# Ollama Test Configuration
# This file controls all Ollama-related settings for tests
# Edit this file to adjust model, temperature, context limits, etc.

ollama:
  # Connection settings
  host: "http://localhost:11434"
  timeout: 120.0  # seconds - extended for LLM operations
  
  # Model settings
  default_model: "gemma3:4b"  # Primary model to use
  fallback_models:  # Fallback models if default unavailable
    - "mistral"
    - "phi3"
    - "llama3.2:3b"
  
  # Generation parameters
  temperature: 0.7  # 0.0 (deterministic) to 2.0 (creative)
  max_tokens: 2048  # Maximum tokens per response
  context_window: 131072  # 128K context (supports gemma3:4b)
  top_p: 0.9
  seed: null  # null for random, integer for reproducibility
  
  # Response length settings
  short_max_tokens: 150  # For query_short()
  long_max_tokens: 16384  # For query_long()
  long_min_tokens: 0
  
  # Test-specific timeouts
  test_timeouts:
    default: 10  # Default pytest timeout (seconds)
    network: 30  # Network-dependent tests
    llm_short: 60  # Short LLM queries
    llm_long: 120  # Long LLM queries
    llm_review: 180  # Literature review generation
    download: 30  # PDF downloads




