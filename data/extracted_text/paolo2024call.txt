arXiv:2402.03824v4  [cs.AI]  13 Sep 2024
A Call for Embodied AI
Giuseppe Paolo 1 Jonas Gonzalez-Billandon 2 Bal ´azs K ´egl 1
Abstract
W e propose Embodied AI (E-AI) as the next fun-
damental step in the pursuit of Artiﬁcial General
Intelligence (AGI), juxtaposing it against current
AI advancements, particularly Large Language
Models (LLMs). W e traverse the evolution of the
embodiment concept across diverse ﬁelds (phi-
losophy, psychology, neuroscience, and robotics)
to highlight how E-AI distinguishes itself from
the classical paradigm of static learning. By
broadening the scope of E-AI, we introduce a the-
oretical framework based on cognitive architec-
tures, emphasizing perception, action, memory,
and learning as essential components of an em-
bodied agent. This framework is aligned with
Friston’s active inference principle, offering a
comprehensive approach to E-AI development.
Despite the progress made in the ﬁeld of AI, sub-
stantial challenges, such as the formulation of a
novel AI learning theory and the innovation of
advanced hardware, persist. Our discussion lays
down a foundational guideline for future E-AI
research. Highlighting the importance of creat-
ing E-AI agents capable of seamless communica-
tion, collaboration, and coexistence with humans
and other intelligent entities within real-world en-
vironments, we aim to steer the AI community
towards addressing the multifaceted challenges
and seizing the opportunities that lie ahead in the
quest for AGI.
1. Introduction
Over recent years, the ﬁeld of artiﬁcial intelligence (AI)
has experienced a signiﬁcant surge, leading to substantial
breakthroughs in areas ranging from computer vision (CV)
and natural language processing (NLP) to neuroscience.
This journey through AI’s development has been marked
1 Noah’s Ark Lab, Huawei T echnologies France, Paris, France
2 London Research Center, London, UK. Correspondence to:
Giuseppe Paolo <giuseppe.paolo@huawei.com>.
Proceedings of the 41 st International Conference on Machine
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024
by the author(s).
by a series of signiﬁcant triumphs interspersed with set-
backs, including the well-documented AI winter of the mid-
1980s. The ambitious goal that has propelled AI research
forward from the beginning was to create intelligence that
either parallels or exceeds human abilities. This quest for
superhuman intelligence, commonly termed Artiﬁcial Gen-
eral Intelligence (AGI), has been seen differently by exper ts
across different disciplines, yet it broadly refers to the a bil-
ity of a system to understand, learn, and apply knowledge
in a wide array of tasks and contexts, mirroring the cogni-
tive ﬂexibility of humans and animals.
The remarkable progress in AI over the past decade can
largely be attributed to three pivotal developments: i) ad-
vancements in deep learning algorithms, ii) the advent of
powerful new hardware, and iii) the availability of exten-
sive datasets for training. A prime illustration of this
advancement is the creation of Large Language Models
(LLMs) like OpenAI’s GPT -4 (
Achiam et al. , 2023) and
Google’s Gemini ( T eam et al. , 2023). The surprising abil-
ities of these LLMs have sparked discussions within the
AI community, with some pondering whether these models
have already achieved nascent forms of AGI. Foundation
models (large networks with billions of parameters trained
on massive datasets) have found success in varied ﬁelds,
ranging from predicting 3D protein structures (
Cramer,
2021) and robotic control ( Brohan et al. , 2023), to generat-
ing images and audio ( Ramesh et al. , 2022; Radford et al. ,
2022). This breadth of achievement supports the hypoth-
esis that continued scaling and reﬁnement of foundation
models could be a viable path toward realizing AGI.
In our paper, we argue that despite the signiﬁcant advances
made by current AI technologies, they represent only the
initial steps towards truly intelligent agents. Despite th eir
impressive capabilities, these large networks are static a nd
unable to evolve with time and experience. They lever-
age large datasets and cutting-edge hardware for scaling,
but they lack the ability to properly care about the truth
(
V ervaeke & Coyne , 2024), which in turn makes it impos-
sible to dynamically adjust their knowledge and actively
search for valuable new information. The two primary
manifestations of this fundamental shortfall are i) the dif -
ﬁculty in effectively aligning LLMs (
Ouyang et al. , 2022),
and ii) their propensity to generate plausible but inaccu-
rate information, a phenomenon known as confabulation
1
A Call for Embodied AI
(Huang et al. , 2023; W ei et al. , 2023). Current strategies to
mitigate these issues, such as post-processing, ﬁne-tunin g,
prompt engineering, and incorporating human feedback,
are undeniably valuable. However, we argue that these
methods address only the superﬁcial aspects of the prob-
lem and fall short in dealing with the core issue at play: the
inherent lack of a deeper, grounded sense of care in LLMs.
Pursuing the development of AGI, we draw upon the in-
sights of
V ervaeke & Coyne (2024) to advocate for design-
ing AI agents that are bound to, observe, interact with,
and learn from the real world (including humans) in a con-
tinuous and dynamic manner. These
Embodied AI (E-AI)
agents ought to prioritize their continued existence and
our bindings to them, thereby learning the value of truth.
They should also be capable of adapting to environmental
changes and evolving without human intervention.
While Large Language Models play a signiﬁcant role in
the development of AI systems, they fall short of captur-
ing the essence of what constitutes an intelligent agent.
Notably, intelligent beings, whether humans or animals,
are characterized by three fundamental components: the
mind, perception, and action capabilities (
Kirchhoff et al. ,
2018). LLMs, or more broadly, foundation models, may
be likened to an aspect of the mind’s reasoning function
(Xi et al. , 2023). Y et, the perceptive and action-oriented di-
mensions of intelligence, along with the pivotal ability to
dynamically revise beliefs and knowledge based on experi-
ences, remain unaddressed. Autoregressive LLMs are not
designed to understand the causal relationships between
events, but rather to identify proximate context and cor-
relations within sequences (
Bariah & Debbah , 2023). In
contrast, a fully embodied agent should have the ability
to grasp the causality underlying events and actions within
its environment, be it digital or physical. By comprehend-
ing these causal relationships, such an agent can make in-
formed decisions that consider both the anticipated out-
comes and the reasons behind those outcomes.
In short, this paper argues that the necessary next step
in our pursuit for truly intelligent and general AIs is
the development and study of Embodied AI. W e pro-
pose that current LLM-based foundation models could
lay the groundwork for designing these agents, but are
just one component of a truly embodied agent. This
approach is akin to how neonates come into the world
equipped with inherent priors to successfully adapt to the
world (
Reynolds & Roth , 2018).
In the next section of this paper, we will deﬁne the concept
of embodiment and what we mean by
E-AI in Sec. 2, ana-
lyzing the literature and various scientiﬁc and philosophi cal
currents. In Sec. 3, we discuss why we believe this is a nec-
essary step towards Artiﬁcial General Intelligence (AGI) .
In Sec. 4 we analyze the main components of a truly embod-
ied agent, and we discuss the major challenges to achieving
this ambitious goal in Sec.
5. Our motivation behind the
need to develop E-AI and its fundamental role in our path
towards AGI is proposed throughout. Finally, Sec. 6 pro-
vides a short recap of our proposition.
2. What is embodied AI
E-AI is s sub-ﬁeld of AI, focusing on agents that inter-
act with their physical environment, emphasizing senso-
rimotor coupling and situated intelligence. As opposed
to mere passive observing,
E-AI agents act on their en-
vironment and learn from the reaction. E-AI is deeply
rooted in embodied cognition (Shapiro, 2011; McNearney,
2011), a perspective in philosophy and cognitive science
that posits a profound coupling between the mind and the
body. This idea, challenging Cartesian dualism — the his-
torically dominant view that distinctly separates the mind
from the body (
Descartes, 2012) — emerged in the early
20th century. Pioneers like Lakoff & Johnson (1979; 1999)
have signiﬁcantly contributed to this paradigm by propos-
ing that reason is not based on abstract laws but is grounded
in bodily experiences. Embodied cognition forms a critical
part of the 4E cognitive science framework (
V arela et al. ,
1991; Clark, 1997; Clark & Chalmers , 1998), encompass-
ing embodied, enactive, embedded, and extended aspects
of cognition. Within
E-AI, the focus is predominantly on
implementing the ‘embodied’ and ‘enactive’ aspects, while
the ‘embedded’ and ‘extended’ components are more perti-
nent to situating AI in a social context and as an augmenta-
tion of human (individual or collective) cognition.
In AI, initial explorations into embodiment emerged in the
1980s, driven by a growing recognition of the inherent lim-
itations in disembodied agents. These limitations were pri -
marily attributed to the absence of rich, high-bandwidth
interactions with the environment (
Pfeifer & Iida , 2004;
Pfeifer & Bongard , 2006). An early advocate for this
paradigm shift was Brooks (1991), who built walking
robots simulating insect-like locomotion. Simultaneousl y,
the ﬁeld of computer vision was undergoing its own trans-
formation. Researchers and practitioners were increasing ly
focusing on enabling agents to interact with their surround -
ings. This emphasis on interaction led to a concentration on
the perceptual elements of embodiment, particularly from
a ﬁrst-person point of view (POV) (
Shapiro, 2021). This
approach aligns with the concept of visual exploration and
navigation (
Ramakrishnan et al. , 2021), where an agent ac-
quires information about a 3D environment through move-
ment and sensory perception, thereby continuously reﬁn-
ing its model of the environment (
Anderson et al. , 2018;
Chen et al. , 2019). Such exploration techniques empower
an agent to discover objects and understand their perma-
nence. As a result of these developments, many contem-
porary benchmarks in
E-AI have emerged predominantly
2
A Call for Embodied AI
from the domains of vision and robotics ( Duan et al. , 2022),
reﬂecting the integral role these disciplines have played i n
advancing the ﬁeld.
That said, the broader deﬁnition of E-AI does not
require vision. Sensorimotor coupling may be imple-
mented using any physical sense (
Pfeifer & Bongard ,
2006). In the living world, many organisms survive
and thrive without vision, using, for example, chemical
or electric sensing (
Bargmann, 2006). Levin (2022)’s
T echnological Approach to Mind Everywhere (T AME)
framework further explores this idea, suggesting that
cognition emerges from the collective intelligence of cell
groups, they themselves deeply embodied within their
environment (the body they comprise). This framework
challenges traditional Cartesian dualism, embedding
cognition within the physical and biological makeup of
an organism. In the
T AME perspective, cognition is not
just an attribute of higher-order organisms; it extends
throughout the ontological hierarchy of living beings, fro m
individual cells, through tissues and organs, to complex
organisms. Each agent demonstrates cognitive capabilitie s
that are inherently connected to its physical structure and
the environmental interactions at its proper level. This
broadened view of cognition and embodiment goes beyond
the conventional focus on vision in robotics and computer
vision. It posits that any entity capable of perceiving, in-
teracting with, and learning from its environment, thereby
adapting to it and inﬂuencing it, qualiﬁes as embodied. A
technological instantiation of this concept is an intellig ent
router in a telecommunication network. This device ‘lives’
in a realm dominated by electromagnetic sensing. It
continuously learns from and adapts to the network trafﬁc,
effectively mapping and managing the ﬂow of informa-
tion. This example underscores the potential of applying
the principles of
E-AI beyond the traditional domains,
embracing a more inclusive and diverse understanding of
intelligence and embodiment.
This broadening of the notion of
E-AI raises the ques-
tion: how close current commercial AI tools are to embod-
iment? Here we examine two such tools: Large Language
Models ( Brown et al. , 2020; Devlin et al. , 2019) and So-
cial Media Content AI Recommendation Systems (SMAI)
(Bakshy et al. , 2015; Covington et al. , 2016; Eirinaki et al. ,
2018).
LLMs operate within a linguistic-symbolic domain, rep-
resenting textual information and generating new text by
completing prompts. Their foundational training is es-
sentially static, relying on datasets meticulously compil ed
and curated by teams of AI engineers. Their goal is su-
pervised: to generate likely tokens following a context.
Their secondary training (ﬁne-tuning) may involve both in-
teractions with their symbolic environment (human users)
and goals to reach (satisfy their human users), but these
interactions are presents some limits due to both techni-
cal (e.g., catastrophic forgetting (
Kirkpatrick et al. , 2017;
Parisi et al. , 2019)) and business (e.g., managing individ-
uated LLMs ( Strubell et al. , 2019; Kaplan et al. , 2020))
reasons. Looking ahead, we anticipate advancements
that might address these limitations, potentially leading
to the emergence of “personal assistant” LLMs. These
would represent a form of embodied agents within a sym-
bolic realm. However, at present, LLMs largely resemble
static
Internet AI (I-AI) (Duan et al. , 2022), differing signif-
icantly from the dynamic, interactive nature characterist ic
of E-AI.
It is intriguing that despite the growing concerns about
the risks and alignment challenges of LLMs highlighted
in recent research (
Bender et al. , 2021), SMAIs have at-
tracted comparatively less scrutiny ( Husz ´ ar et al., 2022;
Ribeiro et al. , 2020). This is noteworthy considering
SMAIs have been around for a longer time and their in-
ﬂuence on society is both wider and more profound. W e
propose that their widespread acceptance and their more in-
tegrated, less intrusive presence in our lives are due to the ir
closer alignment with the principles of embodiment, in con-
trast to LLMs.
What do we mean by SMAIs being closer to embodi-
ment? Firstly, SMAIs are driven by clear objectives: to
captivate our attention and maximize our engagement with
their respective platforms (
Bozdag, 2013; Bod ´ o, 2021).
These goals are fundamentally linked to the business mod-
els of these platforms, which revolve around advertising.
The speciﬁcs of these “engagement” objectives are typi-
cally proprietary, forming the core of the competitive ad-
vantage of these platforms. Although these goals are
initially human-designed and not intrinsically generated
(Covington et al. , 2016), they are subject to evolutionary
pressure and adaptation, and thus they are tied to the ex-
istence and the survival of the SMAI. Secondly, SMAIs
learn almost entirely from the data they collect by inter-
acting with us. This leads to a high level of individua-
tion (adapting to our individual preferences (
Nguyen et al. ,
2014)), and notions of exploration (offering us content not
so much to satisfy us but for the sake of learning what
we like). This creates a user experience that, when well-
executed, resembles interaction with a considerate friend ,
who wants our best, who connects us to things we like,
and who wants to understand us better. The ﬂip side, how-
ever, is the potential for these systems to morph into mech-
anisms that perpetuate addictive behaviors or harmful con-
tent (
Sch ¨ ull, 2012). Nevertheless, since SMAIs connect
and adapt to us in a more intuitive and deeper manner than
LLMs, we often feel a greater sense of control over our in-
teractions with these systems (by, for example, consciousl y
not clicking on content that we know we do not want to see
3
A Call for Embodied AI
in the long run). This control, albeit limited, is reminisce nt
of persuasion more than mechanical manipulation, align-
ing with how we interact with other sentient beings rather
than machines. This type of relationship with AI systems
is a fundamental aspect of
Levin (2022)’s T AME proposal
. Our stance on E-AI suggests that, while systems akin to
SMAIs pose greater risks due to their seamless integration
into our social fabric, they also present more natural op-
portunities for alignment with our values. This alignment
process is procedural, perspectival, and evolutionary in n a-
ture (
V ervaeke et al. , 2012; V ervaeke & Coyne , 2024), con-
trasting with the primarily propositional approaches bein g
applied to LLMs ( Shen et al. , 2023).
W e posit that the potential for more effective and naturally
aligned AI systems is, alone, a compelling reason to priori-
tizing
E-AI in the broader AI research agenda.
In the forthcoming section, we further explore the pivotal
role that well-executed implementations of
E-AI could play
in the quest for AGI.
3. Why embodiement?
In the previous section, we examined how contemporary
theories of embodiment, particularly the T AME framework
(
Levin, 2022), challenge the long-standing Cartesian dual-
ism which posits a distinct separation between mind and
body (
Descartes, 2012). This philosophical stance has sig-
niﬁcantly inﬂuenced the development of current generative
AI models, such as LLMs, which primarily rely on static
data and lack interaction with the physical or even the sym-
bolic world. It is a prevalent belief that simply scaling up
such models, in terms of data volume and computational
power, could lead to AGI. W e contest this view . W e pro-
pose that true understanding, not only propositional truth
but also the value of propositions that guide us how to
act, is achievable only through
E-AI agents that live in the
world and learn of it by interacting with it.
The signiﬁcance of embodiment in cognitive development
was demonstrated by
Held & Hein (1963)’s carousel exper-
iment with kittens. In this study, one kitten could actively
interact with and control a carousel, while the other could
only observe it passively. Despite both kittens receiving
identical visual input, the one engaged in active interac-
tion exhibited normal visual development, unlike its pas-
sively observing counterpart. This seminal experiment un-
derscores the vital role of embodied interaction in shaping
cognitive abilities (
Shenavarmasouleh et al. , 2022). It also
reinforces the observation that all known forms of intelli-
gence, including human intelligence, are inherently embod -
ied (
Smith & Gasser , 2005), suggesting that embodiment
serves as a solid foundation for cognitive learning and de-
velopment. Current AI learns in a very different way from
humans. W e humans learn by seeing, moving, interacting
with the world and speaking with others. W e also learn by
collecting sequential experiences, not by passive observa -
tion of shufﬂed and randomized, even if carefully selected,
data (
Smith & Gasser , 2005; W estho et al. , 2020). W e ad-
vocate for an approach where insights from cognitive sci-
ence and developmental psychology inform the design of
AI systems. Such systems should be designed to learn
through active interaction with their surroundings, mirro r-
ing the embodied learning processes fundamental to human
cognition.
Even advocates of static learning concede that multimodal
learning is the next milestone towards AGI (
Fei et al. , 2022;
Parcalabescu et al. , 2021). In I-AI, multimodal data needs
to be collected and connected painstakingly. In contrast,
E-AI agents, when equipped with multimodal sensors, will
inherently collect and correlate multi-modal data by mere
co-occurrence. For instance, robots will see (CV), com-
municate (NLP), reason (general intelligence), navigate
and interact with their environment (planning and RL),
all simultaneously (
Shenavarmasouleh et al. , 2022). Intel-
ligent routers will observe requests and trafﬁc (sensing),
communicate with other routers, human engineers, absorb
news about their surroundings (NLP), reason (general in-
telligence), and control the trafﬁc (control and RL). De-
spite the impressive progress in these domains, much of
it has relied on the external collection and curation of vast
datasets for algorithmic training. This approach has signi f-
icant drawbacks: i) the collection and preparation of data
demands substantial investments; ii) this data can contain
biases that are hard to detect and rectify (
Li & Deng , 2020;
Balayn et al. , 2021; V erma et al. , 2021). The issue of bi-
ases is particularly pertinent in discussions on AI align-
ment (
Shen et al. , 2023; Ji et al. , 2023). Efforts to align AI
through rule-based and procedural methods (such as RLHF
(Lambert et al. , 2022)) often struggle, producing systems
that feel mechanistic and “dumb”, rather than an agent
which seamlessly acts according to values compatible with
our society.
An embodied agent, designed to interact with and learn
from its environment, fundamentally changes the tradi-
tional approach to data collection and curation in AI de-
velopment. By being inherently integrated with its physi-
cal and social contexts, such an agent bypasses the labor-
intensive processes previously required. This shift not on ly
simpliﬁes the challenge of aligning AI with human values
but also enhances the agent’s learning efﬁciency by utiliz-
ing the unique features of its environment. As a result, the
focus in AI development transitions from data to simula-
tors. These simulators serve a dual purpose: they are both
training grounds for
E-AI and platforms for testing and re-
ﬁning concepts and algorithms ( Duan et al. , 2022). More-
over, the process of aligning these agents with human val-
ues becomes more intuitive as it involves deﬁning goals re-
4
A Call for Embodied AI
ﬂective of those values. This approach does not claim to
fully resolve the alignment challenge, as
E-AI systems will
still necessitate oversight and guidelines to avert unwant ed
behaviors. However, the alignment process becomes inher-
ently more natural. Adjusting and deﬁning goals is a more
straightforward task than the extensive editing and curat-
ing of data. This methodology draws upon our inherent,
non-propositional understanding and instincts about alig n-
ing embodied intelligences—whether it is guiding our own
actions, nurturing children, or training pets.
Another important characteristic of
E-AI, stemming from
the coupling between the agent and its environment, is the
agent’s capacity for ongoing evolution and adaptation. Thi s
adaptability is vital for any agent destined to navigate a
world in perpetual change. It underscores the importance
of continual learning: the process of assimilating new ex-
periences while retaining previously acquired knowledge
(
W ang et al. , 2023a).
Moreover, Ishiguro & Kawakatsu (2004) have shown, both
through theory and practical application in robotics, that
a close and effective integration of control mechanisms
with body dynamics signiﬁcantly enhances energy efﬁ-
ciency.
Ororbia & Friston (2024) elaborates on energy ef-
ﬁciency, proposing that embodied mortal systems, which
are characterized by their inherent lifecycle and eventual
mortality, can optimize energy usage through adaptive pro-
cesses. These processes allow the system to self-organize
and maintain homeostasis by minimizing free energy, in
alignment with the principles of the free energy princi-
ple (
Friston, 2010; Friston et al. , 2023). Coupled systems
lead also to the emergence of intriguing behaviors that can
be hard to explicitly program or learn from disembodied
datasets (
Rosas et al. , 2020), an observation aligning with
the principles of the T AME framework.
Embodiment is also a prerequisite for learning about affor-
dances (
Gibson, 1979). Learning, or more precisely re-
alizing affordances, according to V ervaeke et al. (2012)’s
perspectival learning, is a fundamental capacity of AGI,
as affordances are what “ﬁll our world with meaning”
(Roli et al. , 2022), and are thus necessary for agents that
give meaning to their own world. Affordances emerge
from the dynamic interplay between an agent’s perception,
objectives, abilities, and the characteristics of objects and
contexts within the environment; for example, a chair af-
fords us to sit, a glass to drink and a hand to grasp and
pick up objects.
Roli et al. (2022) argue that the capacity to
comprehend, utilize, and be inﬂuenced by environmental
affordances distinguishes biological intelligence from c ur-
rent artiﬁcial systems. Besides affordances,
E-AI is also in-
dispensable for investigating emergent phenomena such as
qualia (
Locke, 1847; Korth, 2022), consciousness ( Solms,
2019), as well as creativity, empathy ( Perez, 2023), and eth-
ical understanding ( Lake et al. , 2017; Russell, 2021).
Finally, there is the important question of why an intellige nt
agent would do anything in the ﬁrst place ( Pfeifer & Iida ,
2004). What drives it to engage and acquire new knowl-
edge without external prompts? Within well-framed small
worlds, such as a chess game, an agent’s purpose is straight-
forward: deciding the next move. However, when navigat-
ing large, open worlds, the motivations guiding an agent’s
decisions grow increasingly ambiguous. The concepts
of active inference and the free energy principle (
Friston,
2010; Friston et al. , 2023) provide a compelling framework
for understanding the behaviors of intelligent agents. Thi s
principle posits that minimizing surprise and uncertainty is
the core objective of the agents. They achieve this through
the use of internal models to forecast outcomes, continuall y
updating these models with sensory input, and proactively
modifying their surroundings to better match their expec-
tations. This concept resonates within the AI community,
particularly in the design of agents equipped with mecha-
nisms for intrinsic motivation (
Oudeyer & Kaplan , 2007;
Pathak et al. , 2017), which incentivize agents to explore
and acquire new knowledge to reduce uncertainty.
However, what propels an intelligent agent to act, espe-
cially beyond mere survival instincts, continues to be a mat -
ter of debate. W e argue that exploring and developing em-
bodied agents will illuminate this question. Thus,
E-AI not
only shows potential for signiﬁcant breakthroughs toward
achieving AGI, but also has deep implications for our un-
derstanding of cognition in general.
4. Theoretical framework
In previous sections, we have underscored the pivotal role
of
E-AI in advancing toward AGI. Shifting focus, we now
delve into the essential components that, we believe, will
comprise
E-AIs. W e draw heavily on the concept of cog-
nitive architectures designed by cognitive scientists aiming
to model the human mind ( Thagard, 2012). Despite the
promise these architectures hold for enhancing modern ma-
chine learning methods, progress on this has been notably
limited (
Kotseruba & Tsotsos , 2020). The slow advance-
ment is largely due to cognitive architectures being the do-
main of neuroscientists and cognitive scientists, with onl y
a select few within the machine learning community ex-
ploring their potential for AGI. W e advocate for a syner-
gistic strategy that marries cognitive architectures with ma-
chine learning within the
E-AI paradigm, proposing it as
a viable path toward AGI. The emergence of agent-based
LLMs, such as AutoGPT (
FIRA T & Kuleli , 2023), which
pioneers the generation of autonomous agents, and Pan-
guAgent (
Christianos et al. , 2023), an agent-focused lan-
guage model, indicate the potential of this approach.
W e identify four essential components of an E-AI sys-
5
A Call for Embodied AI
tem: (i) perception: the ability of the agent to sense its
environment; (ii) action: the ability to interact with and
change its environment; (iii) memory: the capacity to re-
tain past experiences; and (iv) learning: integrating expe ri-
ences to form new knowledge and abilities. These compo-
nents are notably aligned with the active inference frame-
work of
Friston (2010). In this framework, the agent mod-
els its world through a probabilistic generative model that
infers the causes of its sensory observations (perception) .
This model is hierarchical, forecasting future states in a
top-down manner and reconciling these predictions with
bottom-up sensory data, with discrepancies or errors be-
ing escalated upwards only when they cannot be reconciled
at the initial level. The agent acts to minimize the diver-
gence between its anticipations and reality, thus moving to -
wards states of reduced uncertainty (action). Concurrentl y,
it collects and stores new information about its environmen t
(memory) and reﬁnes its internal model to minimize pre-
dictive errors (learning). In the sections that follow , we
will describe in detail these four components and how they
comprise the
E-AI agent.
4.1. Perception
At the heart of an embodied agent lies the ability to per-
ceive the world in which it exists. Perception is a process
by which raw sensory data is transformed into a structured
internal representation, enabling the agent to engage in co g-
nitive tasks. The range of inputs that inform perception is
vast, encompassing familiar human senses such as vision,
hearing, smell, touch, and taste. It extends to any form
of stimuli an agent might encounter, be it force sensors in
robotics or signal strength indicators in wireless technol -
ogy. The challenge with sensory data is that it is often
not immediately actionable. It typically undergoes a pro-
cess of transformation, a task where recent advances in ma-
chine learning can prove invaluable. The ﬁeld has seen the
development of sophisticated methods for learning feature
and embedding spaces, facilitating the conversion of raw
data into meaningful information (
Golinko & Zhu , 2019;
Sivaraman et al. , 2022). A particularly effective strategy
has been self-supervised learning to learn such representa -
tions. Although much of the research has concentrated on
single modalities, such as vision (
Oquab et al. , 2023), the
principles underlying these techniques are universally ap -
plicable across different sensory inputs ( Orhan et al. , 2022;
Lee et al. , 2019).
4.2. Action
Embodied agents navigate the world by taking actions and
observing the outcomes. Acting can be broken down into
two steps: (i) choosing what action to undertake next, like
deciding to relocate to a speciﬁc spot, and (ii) determining
how to execute this action, such as plotting the course to
that location. Actions can further be categorized into re-
active and goal-directed types. Reactive actions, akin to
human reﬂexes, occur almost instantaneously in response
to stimuli and play a crucial role in an agent’s immediate
self-preservation by maintaining stability. Goal-direct ed ac-
tions, on the other hand, involve strategic planning and are
motivated by high-level objectives. Reactive actions are
important for self-preservation, with model-free reinfor ce-
ment learning methods playing an important role for devel-
oping reactive control policies in tasks like robot walking
(
Rudin et al. , 2022). On the other hand, for an agent to
achieve more complex, high-level objectives, planning is
indispensable, even if efﬁcient planning remains an open
area of research (
Lin et al. , 2022; Shi et al. , 2022). Cen-
tral to the concept of planning is the presence of a “world
model” within the agent, which it can use to predict the con-
sequences of its own actions. Model-based RL has made
signiﬁcant strides in developing algorithms that learn the se
world models and use them for planning (
Silver et al. , 2016;
K ´ egl et al., 2021; Paolo et al. , 2022).
4.3. Memory
Embodied agents learn from their experience, which are
stored in memory. Memory encompasses various dimen-
sions, including its duration (short-term or long-term) an d
its nature (procedural, declarative, semantic, and episod ic).
Importantly, memory is not necessarily represented as ex-
plicit propositional knowledge; it can be implicitly en-
coded into the weights of a
neural network (NN) . T o
navigate cognitive tasks, agents require diverse types of
memory systems, each playing a distinct role. W orking
and short-term memory offer temporary storage to sup-
port the agent’s immediate objectives. Long-term and
episodic memories provide a reservoir for information
over longer time. Episodic memory captures and stores
unique, perspectival experiences, ready to be accessed
when familiar scenarios unfold. Long-term memory, con-
versely, is the repository for broader propositional knowl -
edge. LLMs, for example, implement long-term memory
using
Retrieval-Augmented Generation (RAG) (Gao et al. ,
2024), a technique that reduces hallucinations using an ex-
ternal database. This technique showcases how sophisti-
cated machine learning methods can be synergized with
cognitive architectures.
4.4. Learning
A deﬁning trait of intelligent agents is their ability to
learn. Y et, how to learn, especially in a continuous and
dynamic way, remains a subject of ongoing research and
debate (
W ang et al. , 2023a; Y ifan et al. , 2023). While re-
cent strides in AI have largely been powered by training
on static datasets, the concept of continual learning, es-
sential for adapting over time, faces challenges. These
challenges stem primarily from the inherent limitations of
deep NNs, such as catastrophic forgetting (
Kemker et al. ,
6
A Call for Embodied AI
2018), and the complexities associated with learning from
non-stationary data that result from an agent’s interactio n
with its environment ( Fahrbach et al. , 2023). The embodi-
ment hypothesis suggests that true intelligence is born fro m
such interactions ( Smith & Gasser , 2005), underscoring the
need for dynamic learning methodologies. In this context,
simulators emerge as a vital tool, offering a shift away from
the static learning typical of traditional AI. Instead, the y
enable agents to evolve through ongoing, interactive expe-
riences within simulated environments (
Duan et al. , 2022).
5. Challenges
E-AIs agents will adopt an egocentric perspective, experi-
encing their environment from a ﬁrst-person viewpoint, in
contrast to the allocentric perspective prevalent in curre nt
AI systems. This shift is not only essential for meaning-
ful interaction with the world but also offers an advantage
by allowing the agents to focus on modeling their immedi-
ate surroundings rather than the entirety of the world. On
the other hand,
E-AIs introduces several challenges, includ-
ing extending current learning theories, managing noise in
perception and action effectively and safely, and ensuring
meaningful communication with humans that adheres to
ethical standards. The remainder of this section will cover
these challenges, exploring potential pathways and solu-
tions.
5.1. New learning theory
The principles of
E-AI challenge us to reevaluate tra-
ditional learning theories ( Devroye et al. , 1996; V apnik,
1998), bridging a gap between supervised and reinforce-
ment learning. Supervised learning, while foundational
in AI, assumes that the data is drawn from an unknown
but ﬁxed distribution, collected independently of the lear n-
ing process. This theory gives rise to the classical no-
tions of generalization, over- and underﬁtting, bias and
variance, and asymptotic or ﬁnite-sample statistical cons is-
tency. This framing is obviously highly useful: even those
who are not explicitly doing theory use it transparently as
their lingua technica and cognitive scaffolding when work-
ing with algorithms and analyzing results.
When embodied agents interact dynamically with their en-
vironment, data collection becomes part of the data sci-
ence pipeline (
Pfeifer & Iida , 2004; Thrun et al. , 2005).
Classical supervised learning theory is insufﬁcient to
analyze these cases and to guide algorithm build-
ing. Extensions, like transfer learning (
Pan & Y ang ,
2010), multitask learning ( Caruana, 1997), distribution
shift ( Qui ˜ nonero-Candela et al., 2009), domain adaptation
(Csurka, 2017) or out-of-distribution generalization, have
been proposed to patch basic supervised learning theory,
but most of these cling to the original framing, pretend-
ing that the data is coming from outside the learning pro-
cess, encapsulating the value (business or otherwise) of
the predictive pipeline. Practically, this is obviously no t
the case: the data on which we learn a predictor is often
collected by the data scientist, responsible for the qualit y
of the pipeline (
O’Neil & Schutt , 2013; Provost & Fawcett ,
2013). Furthermore, most of the debates around respon-
sible AI turn around the data, not the learning algorithm
(O’Neil , 2016; Selbst et al. , 2019). Collecting, selecting,
and curating data is obviously part of the pipeline. The text
we use to train LLMs is created by its writers, rather than
drawn from a distribution. In some cases, when collection
and model-retraining are automated, the situation may be
even worse. For example, in click-through-rate prediction
(
Bottou et al. , 2013; Perlich et al. , 2014) or recommenda-
tion systems ( Deldjoo et al. , 2020), the deployed predictor
affects the data for the next round of training, generating a n
often adversarial feedback. A similar phenomenon is hap-
pening in the LLM world: as these AIs become the go-to
tools for creative and business writing, the data collected
for the next round of training will, in large part, be coming
from the previous generation of LLMs.
Reinforcement Learning (
Sutton & Barto , 2018) and re-
lated paradigms (Bayesian optimization ( Mockus, 1989)
or contextual bandits ( Langford & Zhang , 2008)) offer a
closer ﬁt for embodied AI, when the prediction is not the
end-product, rather part of a predictive pipeline that also
includes data collection. RL affords the data scientist to
design a higher-level objective, letting the algorithm opt i-
mize both the predictor and the data it is trained on. Here,
the mismatch between theory and practice is different from
supervised learning. The analysis in RL or bandit theory
often focuses on the convergence of the agent to a theo-
retical optimum, given a ﬁxed but often unknown environ-
ment. RL theory usually does not offer tools to analyze
the data collected during the learning process, especially
when the collection is semi-automatic (includes a human
curator in the loop). RL agents, in practice, usually do not
converge even in a stationary environment, they rather indi -
viduate, making, for example, quite perversely, the random
seed part of the algorithm (
Henderson et al. , 2018). This
is even more pronounced in non-stationary environments
where the agent’s actions alter the environment; a situatio n
which AGI will deﬁnitely ﬁnd itself ( da Silva et al. , 2006;
Zhou et al. , 2024).
A new learning theory for embodied AI must transcend
these limitations. It should account for the dynamic, in-
teractive nature of data in
E-AI, where the agent’s actions
continuously reshape its learning environment. This theor y
should not just aim for optimal performance in a ﬁxed set-
ting but should embrace a spectrum of behaviors suitable
for evolving environments. Moreover, it should provide di-
agnostics to assess the quality and relevance of data gener-
ated through these interactions.
7
A Call for Embodied AI
5.2. Noise and uncertainty
E-AI agents are tasked with navigating the real world, rife
with noise and uncertainty. These elements can drastically
affect both the agent’s perception of its surroundings and
the quality of its decision-making. For example, elevated
noise levels may distort the agent’s interpretation of envi -
ronmental cues, leading to suboptimal decisions. This chal -
lenge is accentuated in an egocentric perspective, where
agents frequently encounter continuous streams of ﬂuctuat -
ing and imprecise data. Sources of noise include the natural
imprecision of sensors and actuators, which might lack ac-
curacy due to manufacturing inconsistencies, degradation
over time, or external disturbances. Additionally, quanti -
zation error, a byproduct of converting analog signals into
digital form (
Widrow & Koll´ ar, 2008), can further compro-
mise data integrity.
As these agents learn and adapt to their environment, they
must also grapple with uncertainty. This uncertainty can
obscure the agent’s understanding of its environment, in-
ﬂuencing its performance. This dilemma is especially
prevalent in RL scenarios dealing with partial observabil-
ity, where decisions must be made with incomplete infor-
mation, leading to uncertainty in predicting the outcomes
of its actions (
Dulac-Arnold et al. , 2021; Hess et al. , 2023;
Pattanaik et al. , 2017). Therefore, managing noise and un-
certainty effectively is paramount for the progress of E-AI.
5.3. Simulators
As we pivot towards E-AI, simulators will assume a fun-
damental role as a key driver of progress, similar to the
role data sets play in the training of traditional
I-AI mod-
els. These simulators offer a controlled, replicable envi-
ronment where AI systems can be rigorously trained and
tested. This setup allows for learning and adapting to di-
verse scenarios prior to deployment, ensuring both safety
and cost-efﬁciency. A notable advantage of simulators, and
requirements, is their speed and ease of parallelization, s ig-
niﬁcantly accelerating training time, making it more feasi -
ble to train sophisticated AI models on multiple scenarios
simultaneously.
Many advanced simulators have been introduced recently,
yet they often demand signiﬁcant computational resources
and are predominantly geared towards robotics applica-
tions (
Li et al. , 2021; Gan et al. , 2020; Y an et al. , 2018;
Puig et al. , 2018; Gao et al. , 2019). For these simulators
to truly serve the needs of E-AI, they must expand their
scope to a broader spectrum of environments. A major
challenge in the use of simulators is bridging the “reality
gap” (
Bousmalis & Levine , 2017): the difference between
simulated conditions and the agent’s eventual real-world o r
virtual deployment context ( Ligot & Birattari , 2020). This
gap can lead to a situation where models that excel in simu-
lations fail in actual application, undermining the effect ive-
ness of the training process. Despite numerous strategies
being put forward to mitigate the reality gap (
Salvato et al. ,
2021; Daza et al. , 2023; Daoudi et al. , 2023; Koos et al. ,
2012; T obin et al. , 2017), it remains an unresolved issue in
the ﬁeld, challenging the applicability of simulated train ing
environments.
5.4. Interaction with humans
A key ambition of
E-AI is to seamlessly interact with and
learn from humans, enhancing AI’s ability to offer person-
alized and impactful solutions. By improving these inter-
actions,
E-AI will also diminish fear and mistrust towards
AI technologies, leading to broader acceptance and inte-
gration. In this endeavor, LLMs stand out as particularly
beneﬁcial, with their ability to comprehend and produce
human-like text, facilitating communication in natural la n-
guage and making engagements with AI more natural and
accessible. The domain of Human-Robot Interaction (HRI)
offers valuable lessons for enhancing AI-human communi-
cation, as researchers in this domain have dedicated effort s
to explore innovative methods for robots to better commu-
nicate with us (
Amirova et al. , 2021; Bonarini, 2020). Y et,
the challenge of ensuring proper and ethical communica-
tion with AI systems persists. The effectiveness of LLMs,
for instance, hinges signiﬁcantly on their training and how
well they are aligned with human intentions and values
(
W ang et al. , 2023b). Integrating human oversight directly
into the AI development process and establishing compre-
hensive guidelines and protocols for AI communication are
among the proposed strategies to address these challenges,
aiming to make AI interactions more meaningful and ethi-
cally sound.
5.5. Generalization
An important issue in AI is generalization. There have
been many attempts at developing systems capable of
quickly generalizing to settings unseen at training time
(
Pourpanah et al. , 2022) in the same fashion living beings
do. Nonetheless this is still an open problem that will likel y
afﬂict embodied AIs as well, as acting in the real world ex-
poses the agent to situations unseen at training time. For
instance, consider a service robot trained in a simulated en -
vironment. When placed in a real household, it may en-
counter novel objects and behaviors not present in its train -
ing data, leading to suboptimal or even erroneous actions.
This illustrates the critical need for AIs that can adapt and
generalize beyond their initial programming. A promising
direction in addressing this problem is the leveraging of
the enormous amount of internet data. LLMs have demon-
strated remarkable zero-shot learning capabilities with m in-
imal ﬁne-tuning (
W ei et al. , 2021). W e can envision that
some form of pretraining on internet datasets can kick-star t
the AI before its embodied phase, enhancing generalization
8
A Call for Embodied AI
and adaptability.
Recent developments in robotics have started exploring
this research direction.
Ahn et al. (2024) used a mixed ap-
proach between I-AI and E-AI to effectively control multi-
ple robots in different settings. However, only relying on
internet data is insufﬁcient. An important aspect is also th e
ability to accurately identify unknown situations and avoi d
overconﬁdence, a common shortcoming of LLMs, that of-
ten produce plausible-sounding response that are factuall y
incorrect (
Xiong et al. , 2024). The ability to assess its own
uncertainty is essential, and can prompt the AI to seek hu-
man assistance, similarly to how infants ask for help in thei r
early development. W e believe that, while the integration
of I-AI and E-AI will prove necessary as foundation for
the development of the next generation of intelligent sys-
tems, the active learning paradigm and precise uncertainty
estimation are vital. Active learning, where the AI activel y
queries for information when uncertain, combined with re-
liable uncertainty estimation, can enable an E-AI agent to
manage novel situations effectively.
Finally, we believe that to properly address the issue of gen -
eralization, the community must ﬁrst clearly deﬁne what
is the meaning of “generalization”. Currently, discussion s
around this issue often rely on vague terms, referring to an
agent’s ability to adapt to unseen settings or data. However ,
without a formal deﬁnition, it is challenging to assess or
improve generalization effectively.
Consider the varying degrees of generalization required in
different scenarios: transferring skills from driving a ca r
to driving a bus represents generalization within a similar
domain, whereas adapting from walking to swimming in-
volves a more profound shift in the type of task. These ex-
amples illustrate the spectrum of generalization challeng es
that embodied AI might face.
T o advance this ﬁeld, it is imperative to develop a precise
deﬁnition of generalization and establish standardized me t-
rics and benchmarks for measuring an AI’s generalization
capabilities (
Kawaguchi et al. , 2017). This necessity ties
back to our discussion in Section 5.1, highlighting the ur-
gent need for a new learning theory that can provide a prin-
cipled approach to developing agents that generalize well.
Addressing these questions will hopefully lead to more pre-
cise and principled approaches in the development of the
ﬁeld.
5.6. Hardware limitations
A signiﬁcant challenge to the broad-scale development and
integration of
E-AI lies in the hardware requirements of
these AI systems. Presently, AI technologies largely de-
pend on GPU clusters, which are, while powerful, not ide-
ally suited for embodied agents due to their high cost, en-
ergy consumption, and extensive heat output. Additionally ,
the physical bulk and heft of GPUs pose logistical chal-
lenges for mobile agents or those operating within spatial
limitations. Addressing these constraints necessitates t he
innovation of new , energy-efﬁcient hardware solutions tha t
can be embedded within the agents. Promising develop-
ments are on the horizon, with Google’s T ensor Processing
Unit (TPU) (
Norrie et al. , 2021; Cass, 2019) and Huawei’s
Ascend chip ( Liao et al. , 2021) leading the charge. These
advancements, coupled with the potential of neuromorphic
computing and the strategic synergy of hardware-software
co-design, signal a new era of hardware capability. More-
over, the development of energy and data-efﬁcient algo-
rithms is critical. Such breakthroughs in hardware and algo -
rithm efﬁciency will have a direct and profound effect on
an AI’s ability to understand, decide, and interact within
its environment, enabling
E-AI agents to operate more au-
tonomously and effectively in a diverse array of settings.
6. Conclusion
In this paper, we have articulated the critical role Embodie d
AI plays on the path toward achieving AGI, setting it apart
from prevailing AI methodologies, notably LLMs. By in-
tegrating insights from a spectrum of research ﬁelds, we
underscored how E-AI’s development beneﬁts from exist-
ing knowledge, with LLMs enhancing the potential for in-
tuitive interactions between humans and emerging AI en-
tities. W e introduced a comprehensive theoretical frame-
work for the development of E-AI, grounded in the princi-
ples of cognitive science, highlighting perception, actio n,
memory, and learning, situating E-AI within the context
of Friston’s active inference framework, thereby offering a
wide-ranging theoretical backdrop for our discussion. De-
spite the outlook, the journey ahead is fraught with chal-
lenges, not least the formulation of a novel learning theory
tailored for AI and the creation of sophisticated hardware
solutions. This paper aims to serve as a roadmap for ongo-
ing and future research into E-AI, proposing directions tha t
could lead to signiﬁcant advancements in the ﬁeld.
Impact Statement
While the development of Embodied AI introduces com-
plexities and challenges, particularly in hardware requir e-
ments, ethical considerations, and safety protocols, the p o-
tential beneﬁts signiﬁcantly outweigh these drawbacks. E-
AI stands to evolve our interaction with technology, imbu-
ing AI with a deeper understanding of and engagement with
both the physical world and human society. This not only
paves the way for more natural and effective human-AI in-
teractions but also enhances AI’s adaptability and applica -
tion across a broad spectrum of ﬁelds.
9
A Call for Embodied AI
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F . L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
Ahn, M., Dwibedi, D., Finn, C., Arenas, M. G., Gopalakr-
ishnan, K., Hausman, K., Ichter, B., Irpan, A., Joshi, N.,
Julian, R., et al. Autort: Embodied foundation models
for large scale orchestration of robotic agents. arXiv
preprint arXiv:2401.12963 , 2024.
Amirova, A., Rakhymbayeva, N., Y adollahi, E.,
Sandygulova, A., and Johal, W . 10 years of human-nao
interaction research: A scoping review . Frontiers in
Robotics and AI , 8:744526, 2021. URL
https://
www.frontiersin.org/articles/10.3389/
frobt.2021.744526/full.
Anderson, P ., Wu, Q., T eney, D., Bruce, J., Johnson, M.,
S ¨ underhauf, N., Reid, I., Gould, S., and V an Den Hen-
gel, A. V ision-and-language navigation: Interpreting
visually-grounded navigation instructions in real envi-
ronments. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pp. 3674–3683,
2018. URL
https://ieeexplore.ieee.org/
document/8578485.
Bakshy, E., Messing, S., and Adamic, L. A. Exposure
to ideologically diverse news and opinion on facebook.
In Proceedings of the National Academy of Sciences ,
volume 112, pp. 5791–5796. National Acad Sciences,
2015. URL
https://www.science.org/doi/
10.1126/science.aaa1160.
Balayn, A., Loﬁ, C., and Houben, G.-J. Manag-
ing bias and unfairness in data for decision sup-
port: a survey of machine learning and data engi-
neering approaches to identify and mitigate bias and
unfairness within data management and analytics sys-
tems. The VLDB Journal , 30(5):739–768, 2021.
URL
https://link.springer.com/article/
10.1007/s00778-021-00671-8.
Bargmann, C. I. Comparative chemosensation from
receptors to ecology. Nature, 444(7117):295–301, 2006.
URL https://www.nature.com/articles/
nature05402.
Bariah, L. and Debbah, M. Ai embodiment through 6g:
Shaping the future of agi. 2023.
Bender, E. M., Gebru, T ., McMillan-Major, A., and
Shmitchell, S. On the dangers of stochastic par-
rots: Can language models be too big? In
Proceedings of the 2021 ACM Conference on F air-
ness, Accountability, and T ransparency , pp. 610–623.
ACM, 2021. URL
https://dl.acm.org/doi/
10.1145/3442188.3445922.
Bod ´ o, B. Selling news to audiences–a qualitative inquiry
into the emerging logics of algorithmic news personal-
ization in european quality news media. In Algorithms,
Automation, and News , pp. 75–96. Routledge, 2021.
URL
https://www.tandfonline.com/doi/
full/10.1080/21670811.2019.1624185.
Bonarini, A. Communication in human-robot interac-
tion. Current Robotics Reports , 1:279–285, 2020.
URL https://link.springer.com/article/
10.1007/s43154-020-00026-1.
Bottou, L., Peters, J., Qui ˜ nonero-Candela, J., Charles,
D. X., Chickering, M., Portugaly, E., Ray, D., Simard,
P ., and Snelson, E. Counterfactual reasoning and learn-
ing systems: The example of computational advertising.
In Journal of Machine Learning Research , volume 14,
pp. 3207–3260, 2013. URL
https://jmlr.org/
papers/v14/bottou13a.html.
Bousmalis, K. and Levine, S. Closing the simulation-to-
reality gap for deep robotic learning. Google Research
Blog, 1, 2017. URL https://blog.research.
google/2017/10/closing-simulation-to-
reality-gap-for.html
.
Bozdag, E. Bias in algorithmic ﬁltering and personal-
ization. Ethics and information technology , 15:209–
227, 2013. URL https://link.springer.com/
article/10.1007/s10676-013-9321-6.
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Chen,
X., Choromanski, K., Ding, T ., Driess, D., Dubey, A.,
Finn, C., Florence, P ., Fu, C., Arenas, M. G., Gopalakr-
ishnan, K., Han, K., Hausman, K., Herzog, A., Hsu, J.,
Ichter, B., Irpan, A., Joshi, N., Julian, R., Kalashnikov,
D., Kuang, Y ., Leal, I., Lee, L., Lee, T .-W . E., Levine,
S., Lu, Y ., Michalewski, H., Mordatch, I., Pertsch, K.,
Rao, K., Reymann, K., Ryoo, M., Salazar, G., San-
keti, P ., Sermanet, P ., Singh, J., Singh, A., Soricut, R.,
Tran, H., V anhoucke, V ., V uong, Q., W ahid, A., W elker,
S., W ohlhart, P ., Wu, J., Xia, F ., Xiao, T ., Xu, P ., Xu,
S., Y u, T ., and Zitkovich, B. Rt-2: V ision-language-
action models transfer web knowledge to robotic con-
trol. In arXiv preprint arXiv:2307.15818 , 2023. URL
https://arxiv.org/abs/2307.15818.
Brooks, R. A. Intelligence without representa-
tion. Artiﬁcial intelligence , 47(1-3):139–159,
1991. URL https://www.sciencedirect.
com/science/article/abs/pii/
000437029190053M
.
10
A Call for Embodied AI
Brown, T ., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P ., Neelakantan, A., Shyam, P ., Sastry, G.,
Askell, A., et al. Language models are few-shot learn-
ers. Advances in neural information processing systems ,
33:1877–1901, 2020. URL https://arxiv.org/
abs/2005.14165.
Caruana, R. Multitask learning. Machine Learning , 28
(1):41–75, 1997. URL https://link.springer.
com/article/10.1023/A:1007379606734.
Cass, S. T aking ai to the edge: Google’s tpu now comes
in a maker-friendly package. IEEE Spectrum , 56(5):
16–17, 2019. URL https://ieeexplore.ieee.
org/document/8701189.
Chen, T ., Gupta, S., and Gupta, A. Learning ex-
ploration policies for navigation. arXiv preprint
arXiv:1903.01959 , 2019. URL https://arxiv.
org/abs/1903.01959.
Christianos, F ., Papoudakis, G., Zimmer, M., Coste, T ., Wu,
Z., Chen, J., Khandelwal, K., Doran, J., Feng, X., Liu, J.,
et al. Pangu-agent: A ﬁne-tunable generalist agent with
structured reasoning. arXiv preprint arXiv:2312.14878 ,
2023. URL
https://arxiv.org/2312.14878.
Clark, A. Being There: Putting Brain, Body, and W orld
T ogether Again . MIT Press, 1997. URL https://
mitpress.mit.edu/9780262531566/being-
there/
.
Clark, A. and Chalmers, D. The extended mind.
Analysis, 58(1):7–19, 1998. URL https://
era.ed.ac.uk/bitstream/handle/1842/
1312/TheExtendedMind.pdf?sequence=1&
isAllowed=y.
Covington, P ., Adams, J., and Sargin, E. Deep neural net-
works for youtube recommendations. In Proceedings of
the 10th ACM Conference on Recommender Systems , pp.
191–198, 2016. URL https://dl.acm.org/doi/
10.1145/2959100.2959190.
Cramer, P . Alphafold2 and the future of structural bi-
ology. Nature structural & molecular biology , 28(9):
704–705, 2021. URL https://www.nature.com/
articles/s41594-021-00650-1.
Csurka, G. Domain adaptation for visual applica-
tions: A comprehensive survey. arXiv preprint
arXiv:1702.05374 , 2017. URL https://link.
springer.com/chapter/10.1007/978-3-
319-58347-1_1
.
da Silva, B. C., Basso, E. W ., Bazzan, A. L. C., and Engel,
P . M. Dealing with non-stationary environments using
context detection. In Proceedings of the 23rd Interna-
tional Conference on Machine Learning , ICML ’06, pp.
217–224, 2006. URL https://dl.acm.org/doi/
10.1145/1143844.1143872.
Daoudi, P ., Prieur, C., Robu, B., Barlier, M., and Santos,
L. D. A trust region approach for few-shot sim-to-real
reinforcement learning, 2023. URL
https://arxiv.
org/abs/2312.15474.
Daza, I. G., Izquierdo, R., Mart´ ınez, L. M., Benderius, O.,
and Llorca, D. F . Sim-to-real transfer and reality gap
modeling in model predictive control for autonomous
driving. Applied Intelligence , 53(10):12719–12735,
2023. URL
https://link.springer.com/
article/10.1007/s10489-022-04148-1.
Deldjoo, Y ., Di Noia, T ., and Merra, F . A. Adver-
sarial machine learning in recommender systems (aml-
recsys). In Proceedings of the 13th International Con-
ference on W eb Search and Data Mining , pp. 869–872,
2020. URL
https://dl.acm.org/doi/abs/10.
1145/3336191.3371877.
Descartes, R. Discourse on method . Hackett Publish-
ing, 2012. URL https://hackettpublishing.
com/discourse-on-method.
Devlin, J., Chang, M.-W ., Lee, K., and T outanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding, 2019. URL
https://arxiv.
org/abs/1810.04805.
Devroye, L., Gy ¨ orﬁ, L., and Lugosi, G. A Probabilis-
tic Theory of P attern Recognition . Springer, 1996.
URL https://link.springer.com/book/10.
1007/978-1-4612-0711-5 .
Duan, J., Y u, S., T an, H. L., Zhu, H., and T an, C. A
survey of embodied ai: From simulators to research
tasks. IEEE T ransactions on Emerging T opics in Com-
putational Intelligence , 6(2):230–244, 2022. URL
https://arxiv.org/abs/2103.04918.
Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J.,
Paduraru, C., Gowal, S., and Hester, T . Challenges
of real-world reinforcement learning: deﬁnitions,
benchmarks and analysis. Machine Learning , 110(9):
2419–2468, 2021. URL
https://link.springer.
com/article/10.1007/s10994-021-05961-
4.
Eirinaki, M., Gao, J., V arlamis, I., and Tserpes, K.
Recommender systems for large-scale social net-
works: A review of challenges and solutions. Future
Generation Computer Systems , 78:413–418, 2018.
URL https://www.sciencedirect.com/
science/article/pii/S0167739X17319684.
11
A Call for Embodied AI
Fahrbach, M., Javanmard, A., Mirrokni, V ., and W orah,
P . Learning rate schedules in the presence of distribu-
tion shift. arXiv preprint arXiv:2303.15634 , 2023. URL
https://arxiv.org/abs/2303.15634.
Fei, N., Lu, Z., Gao, Y ., Y ang, G., Huo, Y ., W en, J., Lu,
H., Song, R., Gao, X., Xiang, T ., et al. T owards arti-
ﬁcial general intelligence via a multimodal foundation
model. Nature Communications , 13(1):3094, 2022. URL
https://arxiv.org/abs/2110.14378.
FIRA T , M. and Kuleli, S. What if gpt4 be-
came autonomous: The auto-gpt project and use
cases. Journal of Emerging Computer T echnologies ,
3(1):1–6, 2023. URL
https://github.com/
Significant-Gravitas/AutoGPT.
Friston, K. The free energy principle: A uniﬁed brain
theory? Nature Reviews Neuroscience , 11(2):127–
138, 2010. URL https://www.nature.com/
articles/nrn2787.
Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzh ¨ offe r,
K., Pavliotis, G. A., and Parr, T . The free energy prin-
ciple made simpler but not too simple. Physics Reports ,
1024:1–29, 2023. URL
https://arxiv.org/abs/
2201.06387.
Gan, C., Schwartz, J., Alter, S., Mrowca, D., Schrimpf,
M., Traer, J., De Freitas, J., Kubilius, J., Bhandwal-
dar, A., Haber, N., et al. Threedworld: A platform
for interactive multi-modal physical simulation. arXiv
preprint arXiv:2007.04954 , 2020. URL
https://
arxiv.org/abs/2007.04954.
Gao, X., Gong, R., Shu, T ., Xie, X., W ang, S., and
Zhu, S.-C. Vrkitchen: an interactive 3d virtual en-
vironment for task-oriented learning. arXiv preprint
arXiv:1903.05757 , 2019. URL https://arxiv.
org/abs/1903.05757.
Gao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai,
Y ., Sun, J., Guo, Q., W ang, M., and W ang, H. Retrieval-
augmented generation for large language models: A sur-
vey, 2024. URL https://arxiv.org/abs/2312.
10997.
Gibson, J. J. The Ecological Approach to V isual P erception .
Houghton Mifﬂin, 1979.
Golinko, E. and Zhu, X. Generalized feature embed-
ding for supervised, unsupervised, and online learning
tasks. Information Systems Frontiers , 21:125–142, 2019.
URL
https://link.springer.com/article/
10.1007/s10796-018-9850-y.
Held, R. and Hein, A. Movement-produced stimulation
in the development of visually guided behavior. Jour-
nal of comparative and physiological psychology , 56
(5):872, 1963. URL https://psycnet.apa.org/
record/1964-03855-001.
Henderson, P ., Islam, R., Bachman, P ., Pineau, J., Precup,
D., and Meger, D. Deep reinforcement learning that mat-
ters. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018. URL
https://arxiv.
org/abs/1709.06560.
Hess, F ., Monfared, Z., Brenner, M., and Durstewitz, D.
Generalized T eacher Forcing for Learning Chaotic Dy-
namics, October 2023. URL
http://arxiv.org/
abs/2306.04406. arXiv:2306.04406 [nlin].
Huang, L., Y u, W ., Ma, W ., Zhong, W ., Feng, Z., W ang, H.,
Chen, Q., Peng, W ., Feng, X., Qin, B., and Liu, T . A
survey on hallucination in large language models: Prin-
ciples, taxonomy, challenges, and open questions, 2023.
URL
https://arxiv.org/abs/2311.05232.
Husz ´ ar, F ., Ktena, S. I., O’Brien, C., Belli, L., Schlaikje r,
A., and Hardt, M. Algorithmic ampliﬁcation of poli-
tics on twitter. Proceedings of the National Academy
of Sciences , 119(1):e2025334119, 2022. doi: 10.1073/
pnas.2025334119. URL https://www.pnas.org/
doi/abs/10.1073/pnas.2025334119.
Ishiguro, A. and Kawakatsu, T . How should con-
trol and body systems be coupled? a robotic case
study. In Embodied Artiﬁcial Intelligence: Interna-
tional Seminar , Dagstuhl Castle, Germany, July 7-11,
2003. Revised P apers , pp. 107–118. Springer, 2004.
URL
https://link.springer.com/chapter/
10.1007/978-3-540-27833-7_8 .
Ji, J., Qiu, T ., Chen, B., Zhang, B., Lou, H., W ang,
K., Duan, Y ., He, Z., Zhou, J., Zhang, Z., et al. Ai
alignment: A comprehensive survey. arXiv preprint
arXiv:2310.19852 , 2023. URL
https://arxiv.
org/abs/2310.19852.
Kaplan, J., McCandlish, S., Henighan, T ., Brown, T . B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J.,
and Amodei, D. Scaling laws for neural language mod-
els. arXiv preprint arXiv:2001.08361 , 2020. URL
https://arxiv.org/abs/2001.08361.
Kawaguchi, K., Kaelbling, L. P ., and Bengio, Y .
Generalization in deep learning. arXiv preprint
arXiv:1710.05468 , 1(8), 2017.
K ´ egl, B., Hurtado, G., and Thomas, A. Model-based
micro-data reinforcement learning: what are the crucial
model properties and which model to choose? arXiv
12
A Call for Embodied AI
preprint arXiv:2107.11587 , 2021. URL https://
arxiv.org/2107.11587.
Kemker, R., McClure, M., Abitino, A., Hayes, T ., and
Kanan, C. Measuring catastrophic forgetting in neural
networks. In Proceedings of the AAAI conference on ar-
tiﬁcial intelligence , volume 32, 2018. URL
https://
arxiv.org/abs/1708.02072.
Kirchhoff, M., Parr, T ., Palacios, E., Friston, K., and
Kiverstein, J. The Markov blankets of life: autonomy,
active inference and the free energy principle. Journal
of The Royal Society interface , 15(138):20170792, 2018.
URL
https://royalsocietypublishing.
org/doi/10.1098/rsif.2017.0792.
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., V eness, J.,
Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ra-
malho, T ., Grabska-Barwinska, A., et al. Overcom-
ing catastrophic forgetting in neural networks. Pro-
ceedings of the National Academy of Sciences , 114
(13):3521–3526, 2017. URL
https://arxiv.org/
abs/1612.00796.
Koos, S., Mouret, J.-B., and Doncieux, S. The trans-
ferability approach: Crossing the reality gap in evolu-
tionary robotics. IEEE T ransactions on Evolutionary
Computation, 17(1):122–145, 2012. URL
https://
ieeexplore.ieee.org/document/6151107.
Korth, M. The purpose of qualia: What if human thinking
is not (only) information processing? arXiv preprint
arXiv:2212.00800 , 2022. URL https://arxiv.
org/abs/2212.00800.
Kotseruba, I. and Tsotsos, J. K. 40 Y ears of Cog-
nitive Architectures: Core Cognitive Abilities and
Practical Applications , volume 53. Springer Nether-
lands, 2020. ISBN 9550141039. doi: 10.1007/
s10462-018-9646-y. URL
https://doi.org/10.
1007/s10462-018-9646-y.
Lake, B. M., Ullman, T . D., T enenbaum, J. B., and Ger-
shman, S. J. Building machines that learn and think
like people. Behavioral and brain sciences , 40:e253,
2017. URL
https://pubmed.ncbi.nlm.nih.
gov/27881212/.
Lakoff, G. and Johnson, M. Metaphors we live by . Univer-
sity of Chicago press, 1979. URL https://press.
uchicago.edu/ucp/books/book/chicago/
M/bo3637992.html
.
Lakoff, G. and Johnson, M. L. Philosophy in the ﬂesh :
the embodied mind and its challenge to western thought.
1999. URL https://api.semanticscholar.
org/CorpusID:16103621.
Lambert, N., Castricato, L., von W erra, L., and Havrilla,
A. Illustrating reinforcement learning from human feed-
back (rlhf). Hugging F ace Blog , 2022. URL https://
huggingface.co/blog/rlhf.
Langford, J. and Zhang, T . The epoch-greedy algorithm
for contextual multi-armed bandits. In Advances
in Neural Information Processing Systems , vol-
ume 20, 2008. URL https://proceedings.
neurips.cc/paper/2007/file/
4b04a686b0ad13dce35fa99fa4161c65-
Paper.pdf.
Lee, M. A., Zhu, Y ., Srinivasan, K., Shah, P ., Savarese, S.,
Fei-Fei, L., Garg, A., and Bohg, J. Making sense of vi-
sion and touch: Self-supervised learning of multimodal
representations for contact-rich tasks. In 2019 Interna-
tional Conference on Robotics and Automation (ICRA) ,
pp. 8943–8950. IEEE, 2019. URL
https://arxiv.
org/1810.10191v2.
Levin, M. T echnological approach to mind everywhere:
an experimentally-grounded framework for understand-
ing diverse bodies and minds. Frontiers in systems
neuroscience, 16:768201, 2022. URL
https://
www.frontiersin.org/articles/10.3389/
fnsys.2022.768201.
Li, C., Xia, F ., Mart´ ın-Mart´ ın, R., Lingelbach, M., Sriva s-
tava, S., Shen, B., V ainio, K., Gokmen, C., Dharan, G.,
Jain, T ., et al. igibson 2.0: Object-centric simulation
for robot learning of everyday household tasks. arXiv
preprint arXiv:2108.03272 , 2021. URL https://
arxiv.org/abs/2108.03272.
Li, S. and Deng, W . A deeper look at facial expression
dataset bias. IEEE T ransactions on Affective Computing ,
13(2):881–893, 2020. URL https://arxiv.org/
1904.11150.
Liao, H., Tu, J., Xia, J., Liu, H., Zhou, X., Y uan, H.,
and Hu, Y . Ascend: a scalable and uniﬁed architecture
for ubiquitous deep neural network computing: Industry
track paper. In 2021 IEEE International Symposium on
High-P erformance Computer Architecture (HPCA) , pp.
789–801. IEEE, 2021. URL
https://ieeexplore.
ieee.org/document/9407221.
Ligot, A. and Birattari, M. Simulation-only experiments
to mimic the effects of the reality gap in the automatic
design of robot swarms. Swarm Intelligence , 14(1):1–
24, 2020. URL
https://link.springer.com/
article/10.1007/s11721-019-00175-w.
Lin, H., Sun, Y ., Zhang, J., and Y u, Y . Model-based re-
inforcement learning with multi-step plan value estima-
tion. arXiv preprint arXiv:2209.05530 , 2022. URL
https://arxiv.org/abs/2209.05530.
13
A Call for Embodied AI
Locke, J. An essay concerning human understanding .
Kay & Troutman, 1847. URL https://www.
gutenberg.org/files/10615/10615-h/
10615-h.htm.
McNearney, S. A Brief Guide to Embodied Cognition:
Why Y ou Are Not Y our Brain, 2011.
Mockus, J. Bayesian Approach to Global Optimization:
Theory and Applications . Kluwer Academic Publish-
ers, 1989. URL https://link.springer.com/
book/10.1007/978-94-009-0909-0 .
Nguyen, T . T ., Hui, P .-M., Harper, F . M., T erveen, L., and
Konstan, J. A. Exploring the ﬁlter bubble: the effect of
using recommender systems on content diversity. In Pro-
ceedings of the 23rd international conference on W orld
wide web , pp. 677–686, 2014. URL
https://dl.
acm.org/doi/10.1145/2566486.2568012.
Norrie, T ., Patil, N., Y oon, D. H., Kurian, G., Li,
S., Laudon, J., Y oung, C., Jouppi, N., and Patter-
son, D. The design process for google’s training
chips: Tpuv2 and tpuv3. IEEE Micro , 41(2):56–63,
2021. URL
https://ieeexplore.ieee.org/
document/9351692.
O’Neil, C. W eapons of Math Destruction: How Big Data
Increases Inequality and Threatens Democracy . Crown
Publishing Group, 2016.
O’Neil, C. and Schutt, R. Doing Data Science . O’Reilly
Media, Inc., 2013.
Oquab, M., Darcet, T ., Moutakanni, T ., V o, H. V .,
Szafraniec, M., Khalidov, V ., Fernandez, P ., Haziza, D.,
Massa, F ., El-Nouby, A., Howes, R., Huang, P .-Y ., Xu,
H., Sharma, V ., Li, S.-W ., Galuba, W ., Rabbat, M., As-
sran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H.,
Mairal, J., Labatut, P ., Joulin, A., and Bojanowski, P . Di-
nov2: Learning robust visual features without supervi-
sion, 2023.
Orhan, P ., Boubenec, Y ., and King, J.-R. Don’t stop
the training: continuously-updating self-supervised al-
gorithms best account for auditory responses in the cor-
tex. arXiv preprint arXiv:2202.07290 , 2022. URL
https://arxiv.org/abs/2202.07290.
Ororbia, A. and Friston, K. Mortal computation: A founda-
tion for biomimetic intelligence, 2024. URL https://
arxiv.org/abs/2311.09589.
Oudeyer, P .-Y . and Kaplan, F . What is intrinsic mo-
tivation? a typology of computational approaches.
Frontiers in neurorobotics , 1:6, 2007. URL https://
www.frontiersin.org/articles/10.3389/
neuro.12.006.2007.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., W ainwright,
C. L., Mishkin, P ., Zhang, C., Agarwal, S., Slama, K.,
Ray, A., Schulman, J., Hilton, J., Kelton, F ., Miller,
L., Simens, M., Askell, A., W elinder, P ., Christiano, P .,
Leike, J., and Lowe, R. Training language models to
follow instructions with human feedback, 2022. URL
https://arxiv.org/abs/2203.02155.
Pan, S. J. and Y ang, Q. A survey on transfer learning.
IEEE T ransactions on knowledge and data engineering ,
22(10):1345–1359, 2010.
Paolo, G., Gonzalez-Billandon, J., Thomas, A., and K ´ egl,
B. Guided safe shooting: model based reinforce-
ment learning with safety constraints. arXiv preprint
arXiv:2206.09743 , 2022. URL
https://arxiv.
org/2206.09743.
Parcalabescu, L., Trost, N., and Frank, A. What is multi-
modality? arXiv preprint arXiv:2103.06304 , 2021. URL
https://arxiv.org/abs/2103.06304.
Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and
W ermter, S. Continual lifelong learning with neural net-
works: A review . Neural Networks , 113:54–71, 2019.
Pathak, D., Agrawal, P ., Efros, A. A., and Darrell, T .
Curiosity-driven exploration by self-supervised predic-
tion. In International conference on machine learning ,
pp. 2778–2787. PMLR, 2017. URL
https://arxiv.
org/1705.05363.
Pattanaik, A., T ang, Z., Liu, S., Bommannan, G.,
and Chowdhary, G. Robust Deep Reinforce-
ment Learning with Adversarial Attacks, Decem-
ber 2017. URL
http://arxiv.org/abs/1712.
03632. arXiv:1712.03632 [cs].
Perez, C. Artiﬁcial Empathy - A Roadmap for Human
Aligned Artiﬁcial General Intelligence . Publisher, 2023.
Perlich, C., Dalessandro, B., Raeder, T ., Stitelman, O., an d
Provost, F . Machine learning for targeted display adver-
tising: transfer learning in action. In Machine learning ,
volume 95, pp. 103–127. Springer, 2014.
Pfeifer, R. and Bongard, J. How the Body Shapes the W ay
W e Think: A New V iew of Intelligence . MIT Press, 2006.
Pfeifer, R. and Iida, F . Embodied artiﬁcial intelligence:
Trends and challenges. Lecture notes in computer
science, pp. 1–26, 2004. URL
https://link.
springer.com/chapter/10.1007/978-3-
540-27833-7_1
.
Pourpanah, F ., Abdar, M., Luo, Y ., Zhou, X., W ang, R.,
Lim, C. P ., W ang, X.-Z., and Wu, Q. J. A review of
14
A Call for Embodied AI
generalized zero-shot learning methods. IEEE transac-
tions on pattern analysis and machine intelligence , 45
(4):4051–4070, 2022.
Provost, F . and Fawcett, T . Data Science for Business:
What Y ou Need to Know about Data Mining and Data-
Analytic Thinking . O’Reilly Media, Inc., 2013.
Puig, X., Ra, K., Boben, M., Li, J., W ang, T ., Fi-
dler, S., and T orralba, A. V irtualhome: Simu-
lating household activities via programs. In Pro-
ceedings of the IEEE Conference on Computer
V ision and P attern Recognition , pp. 8494–8502,
2018. URL
https://openaccess.thecvf.
com/content_cvpr_2018/html/Puig_
VirtualHome_Simulating_Household_
CVPR_2018_paper.html.
Qui ˜ nonero-Candela, J., Sugiyama, M., Schwaighofer, A.,
and Lawrence, N. D. (eds.). Dataset Shift in Machine
Learning. The MIT Press, 2009.
Radford, A., Kim, J. W ., Xu, T ., Brockman, G., McLeavey,
C., and Sutskever, I. Robust speech recognition via large-
scale weak supervision, 2022. URL
https://arxiv.
org/abs/2212.04356.
Ramakrishnan, S. K., Jayaraman, D., and Grauman, K.
An exploration of embodied visual exploration. Inter-
national Journal of Computer V ision , 129:1616–1649,
2021. URL https://arxiv.org/abs/2001.
02192.
Ramesh, A., Dhariwal, P ., Nichol, A., Chu, C., and
Chen, M. Hierarchical text-conditional image generation
with clip latents, 2022. URL https://arxiv.org/
abs/2204.06125.
Reynolds, G. D. and Roth, K. C. The development of
attentional biases for faces in infancy: A developmen-
tal systems perspective. Frontiers in psychology , 9:222,
2018. URL
https://www.frontiersin.org/
articles/10.3389/fpsyg.2018.00222.
Ribeiro, M. H., Ottoni, R., W est, R., Almeida, V . A.,
and Meira Jr, W . Auditing radicalization pathways on
youtube. In Proceedings of the 2020 Conference on F air-
ness, Accountability, and T ransparency , pp. 131–141,
2020.
Roli, A., Jaeger, J., and Kauffman, S. A. How organisms
come to know the world: fundamental limits on artiﬁcial
general intelligence. Frontiers in Ecology and Evolution ,
9:1035, 2022. URL
https://www.frontiersin.
org/articles/10.3389/fevo.2021.806283.
Rosas, F . E., Mediano, P . A., Jensen, H. J., Seth, A. K.,
Barrett, A. B., Carhart-Harris, R. L., and Bor, D.
Reconciling emergences: An information-theoretic
approach to identify causal emergence in multi-
variate data. PLoS computational biology , 16(12):
e1008289, 2020. URL
https://journals.plos.
org/ploscompbiol/article?id=10.1371/
journal.pcbi.1008289
.
Rudin, N., Hoeller, D., Reist, P ., and Hutter, M. Learning to
walk in minutes using massively parallel deep reinforce-
ment learning. In Conference on Robot Learning , pp.
91–100. PMLR, 2022. URL https://arxiv.org/
abs/2109.11978.
Russell, S. Human-compatible artiﬁcial intelligence.
Human-like machine intelligence , pp. 3–23, 2021. URL
http://aima.cs.berkeley.edu/˜russell/
papers/mi19book-hcai.pdf.
Salvato, E., Fenu, G., Medvet, E., and Pellegrino, F . A.
Crossing the reality gap: A survey on sim-to-real trans-
ferability of robot controllers in reinforcement learning .
IEEE Access , 9:153171–153187, 2021.
Sch ¨ ull, N. D. Addiction by Design: Machine Gambling in
Las V egas. Princeton University Press, 2012.
Selbst, A. D., Boyd, D., Friedler, S. A., V enkatasubrama-
nian, S., and V ertesi, J. Fairness and abstraction in so-
ciotechnical systems. ACM Conference on F airness, Ac-
countability, and T ransparency (F AT*) , pp. 59–68, 2019.
Shapiro, L. Embodied Cognition . Routledge, 2011.
Shapiro, L. G. Computer vision: the last ﬁfty years. Uni-
versity of W ashington, Last access , 7(05), 2021.
Shen, T ., Jin, R., Huang, Y ., Liu, C., Dong, W ., Guo,
Z., Wu, X., Liu, Y ., and Xiong, D. Large lan-
guage model alignment: A survey. arXiv preprint
arXiv:2309.15025 , 2023. URL
https://arxiv.
org/abs/2309.15025.
Shenavarmasouleh, F ., Mohammadi, F . G., Amini, M. H.,
and Reza Arabnia, H. Embodied ai-driven operation
of smart cities: A concise review . Cyberphysical
Smart Cities Infrastructures: Optimal Operation and
Intelligent Decision Making , pp. 29–45, 2022. URL
https://arxiv.org/abs/2108.09823.
Shi, L. X., Lim, J. J., and Lee, Y . Skill-based
model-based reinforcement learning. arXiv preprint
arXiv:2207.07560 , 2022. URL https://arxiv.
org/abs/2207.07560.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
V an Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
15
A Call for Embodied AI
Panneershelvam, V ., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
nature, 529(7587):484–489, 2016.
Sivaraman, V ., Wu, Y ., and Perer, A. Emblaze: Illumi-
nating machine learning representations through inter-
active comparison of embedding spaces. In 27th Inter-
national Conference on Intelligent User Interfaces , pp.
418–432, 2022. URL
https://arxiv.org/abs/
2202.02641.
Smith, L. and Gasser, M. The development of embodied
cognition: Six lessons from babies. Artiﬁcial life , 11
(1-2):13–29, 2005. URL https://pubmed.ncbi.
nlm.nih.gov/15811218/.
Solms, M. The hard problem of consciousness and the free
energy principle. Frontiers in Psychology , 2019.
Strubell, E., Ganesh, A., and McCallum, A. Energy and
policy considerations for deep learning in NLP. In Pro-
ceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pp. 3645–3650, 2019.
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An
Introduction. MIT press, 2018.
T eam, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B.,
Y u, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A.,
et al. Gemini: a family of highly capable multimodal
models. arXiv preprint arXiv:2312.11805 , 2023.
Thagard, P . Cognitive architectures. The Cambridge hand-
book of cognitive science , 3:50–70, 2012.
Thrun, S., Burgard, W ., and Fox, D. Probabilistic Robotics .
MIT Press, 2005.
T obin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W .,
and Abbeel, P . Domain randomization for transferring
deep neural networks from simulation to the real world.
In 2017 IEEE/RSJ international conference on intelli-
gent robots and systems (IROS) , pp. 23–30. IEEE, 2017.
V apnik, V . Statistical Learning Theory . Wiley, 1998.
V arela, F . J., Thompson, E., and Rosch, E. The Embodied
Mind: Cognitive Science and Human Experience . MIT
Press, 1991.
V erma, S., Ernst, M., and Just, R. Removing biased
data to improve fairness and accuracy. arXiv preprint
arXiv:2102.03054 , 2021. URL
https://arxiv.
org/2102.03054.
V ervaeke, J. and Coyne, S. Mentoring the Machines .
Hackett Publishing, 2024. URL https://www.
mentoringthemachines.com.
V ervaeke, J., Lillicrap, T . P ., and Richards, B. A. Relevanc e
realization and the emerging framework in cognitive sci-
ence. Journal of Logic and Computation , 22(1):79–99,
2012.
W ang, L., Zhang, X., Su, H., and Zhu, J. A comprehensive
survey of continual learning: Theory, method and appli-
cation. arXiv preprint arXiv:2302.00487 , 2023a. URL
https://arxiv.org/abs/2302.00487.
W ang, Y ., Zhong, W ., Li, L., Mi, F ., Zeng, X., Huang, W .,
Shang, L., Jiang, X., and Liu, Q. Aligning large lan-
guage models with human: A survey. arXiv preprint
arXiv:2307.12966 , 2023b.
W ei, J., Bosma, M., Zhao, V . Y ., Guu, K., Y u, A. W .,
Lester, B., Du, N., Dai, A. M., and Le, Q. V . Finetuned
language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.
W ei, J., W ang, X., Schuurmans, D., Bosma, M., Ichter,
B., Xia, F ., Chi, E., Le, Q., and Zhou, D. Chain-of-
thought prompting elicits reasoning in large language
models, 2023. URL
https://arxiv.org/abs/
2201.11903.
W estho, B., Koele, I. J., and van de Groep, I. H. So-
cial learning and the brain: How do we learn from and
about other people? Everything Y ou and Y our T each-
ers Need to Know About the Learning Brain , pp. 42,
2020. URL
https://kids.frontiersin.org/
articles/10.3389/frym.2020.00095.
Widrow , B. and Koll´ ar, I. Quantization noise: round-
off error in digital computation, signal processing, con-
trol, and communications . Cambridge University Press,
2008.
Xi, Z., Chen, W ., Guo, X., He, W ., Ding, Y ., Hong, B.,
Zhang, M., W ang, J., Jin, S., Zhou, E., Zheng, R., Fan,
X., W ang, X., Xiong, L., Zhou, Y ., W ang, W ., Jiang, C.,
Zou, Y ., Liu, X., Y in, Z., Dou, S., W eng, R., Cheng, W .,
Zhang, Q., Qin, W ., Zheng, Y ., Qiu, X., Huang, X., and
Gui, T . The rise and potential of large language model
based agents: A survey, 2023. URL
https://arxiv.
org/abs/2309.07864.
Xiong, M., Hu, Z., Lu, X., Li, Y ., Fu, J., He, J., and Hooi,
B. Can llms express their uncertainty? an empirical eval-
uation of conﬁdence elicitation in llms, 2024.
Y an, C., Misra, D., Bennnett, A., W alsman, A., Bisk, Y .,
and Artzi, Y . Chalet: Cornell house agent learning envi-
ronment. arXiv preprint arXiv:1801.07357 , 2018. URL
https://arxiv.org/abs/1801.07357.
16
A Call for Embodied AI
Y ifan, C., Y ulu, C., Y adan, Z., and W enbo, L. Continual
learning in an easy-to-hard manner. Applied Intelligence ,
pp. 1–21, 2023. URL https://link.springer.
com/article/10.1007/s10489-023-04454-
2
.
Zhou, Q., Chen, S., W ang, Y ., Xu, H., Du, W ., Zhang, H.,
Du, Y ., T enenbaum, J. B., and Gan, C. HAZARD chal-
lenge: Embodied decision making in dynamically chang-
ing environments, 2024.
17