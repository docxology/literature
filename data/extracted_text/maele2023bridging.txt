Bridging Cognitive Maps: a Hierarchical Active Inference Model of
Spatial Alternation Tasks and the Hippocampal-Prefrontal Circuit
Toon Van de Maele
IDLab, Department of Information Technology, Ghent University - imec, Ghent, Belgium
Bart Dhoedt
IDLab, Department of Information Technology, Ghent University - imec, Ghent, Belgium
Tim Verbelen∗
VERSES Research Lab, Los Angeles, USA
Giovanni Pezzulo∗, †
Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy
July 30, 2024
Abstract
Cognitive problem-solving benefits from cognitive maps aiding navigation and planning. Previous
studies revealed that cognitive maps for physical space navigation involve hippocampal (HC) allocentric
codes, while cognitive maps for abstract task space engage medial prefrontal cortex (mPFC) task-specific
codes. Solving challenging cognitive tasks requires integrating these two types of maps. This is exem-
plified by spatial alternation tasks in multi-corridor settings, where animals like rodents are rewarded
upon executing an alternation pattern in maze corridors. Existing studies demonstrated the HC – mPFC
circuit’s engagement in spatial alternation tasks and that its disruption impairs task performance. Yet,
a comprehensive theory explaining how this circuit integrates task-related and spatial information is
lacking. We advance a novel hierarchical active inference model clarifying how the HC – mPFC circuit
enables the resolution of spatial alternation tasks, by merging physical and task-space cognitive maps.
Through a series of simulations, we demonstrate that the model’s dual layers acquire effective cognitive
maps for navigation within physical (HC map) and task (mPFC map) spaces, using a biologically-inspired
approach: a clone-structured cognitive graph. The model solves spatial alternation tasks through recip-
rocal interactions between the two layers. Importantly, disrupting inter-layer communication impairs
difficult decisions, consistent with empirical findings. The same model showcases the ability to switch
between multiple alternation rules. However, inhibiting message transmission between the two layers
results in perseverative behavior, consistent with empirical findings. In summary, our model provides
a mechanistic account of how the HC – mPFC circuit supports spatial alternation tasks and how its
disruption impairs task performance.
Keywords: cognitive map, hierarchical active inference, spatial alternation tasks, hippocampus, medial
prefrontal cortex, disruption
*These authors contributed equally to this work
†Corresponding author: giovanni.pezzulo@istc.cnr.it
Preprint
1
arXiv:2308.11463v3  [q-bio.NC]  27 Jul 2024
1 Introduction
To solve cognitive problems, such as finding the shortest route to a goal destination in a busy city, we can
use so-called cognitive maps of the situation that affords flexible planning. The concept of the cognitive map
has been initially popularized by Tolman, especially in the context of spatial navigation (Tolman, 1948).
An extensive body of literature has investigated the neuronal underpinnings of cognitive maps for spatial
navigation in humans, rodents, and other animals. This research has established the crucial importance
of structures of the medial temporal lobe and especially of the hippocampal formation in the creation of
codes for allocentric space, such as place cells (in the hippocampus) (O’Keefe and Nadel, 1979; O’Keefe and
Dostrovsky, 1971) and grid cells (in the entorhinal cortex) (Hafting et al., 2005). An emerging perspective to
understand these findings is that cognitive mapping in the hippocampal formation might be described at the
computational level as a sequence learning problem and that place cells, grid cells and other specialized codes
might emerge from such sequence learning (Whittington et al., 2020, 2022; George et al., 2021; Raju et al.,
2022a; Stachenfeld et al., 2017; Stoianov et al., 2022; Chen et al., 2022; Levenstein et al., 2024; Recanatesi
et al., 2021). Furthermore, it has been suggested that the same hippocampal codes that support spatial
navigation might also support navigation in “cognitive” domains (Bellmund et al., 2018; Epstein et al., 2017;
Buzs´ aki and Moser, 2013).
In parallel, the concept of a cognitive map can apply to the abstract state spaces that describe the
sequential stages of a task at hand (for example, buying a gift in one’s preferred shop and then bringing it
to a friend’s house) as opposed to the more fine-grained spatial codes found in the hippocampal formation.
Recent studies reported that prefrontal structures, such as the orbitofrontal cortex, might host such coarse
task codes, which might permit representing (for example) the current and the next stages of the task (Niv,
2019; Schuck et al., 2016) as well as future navigational goals (Basu et al., 2021), as opposed to a physical
location.
Crucially, during goal-directed spatial navigation (and other cognitive problems), it is often necessary to
combine cognitive maps of task space (to find a sequence of coarse-grained actions to progress in task space,
such as ensuring the correct sequence of destinations to buy and deliver a gift) and physical space (to find
a path that reaches the next intended destination) (Ito et al., 2015; Pezzulo et al., 2014b; Verschure et al.,
2014). However, we still lack a comprehensive theory that explains the ways cognitive maps of physical and
task space interact when executing cognitive tasks.
A useful starting point to understand the interaction of task-related and spatial codes is rodent memory-
guided, spatial alternation tasks. A prominent example is the spatial alternation task in the W-maze studied
in (Jadhav et al., 2012). As visualized in Figure 1, the W-maze comprises three corridors. To collect rewards,
the animal has to visit the corridors in the correct order, according to an “alternation rule” (e.g. left, center,
right, center, left, etc) that is initially unknown and has to be learned by trial and error.
A series of experiments showed that solving the spatial alternation task engages coordinated patterns
of neural activity in the hippocampus (HC), which putatively encodes the spatial aspects of the task, and
the prefrontal cortex (mPFC), which putatively learns the alternation rule and uses it to guide behav-
ior (Benchenane et al., 2011; Shin and Jadhav, 2016; Colgin, 2011; Siapas et al., 2005; Khodagholy et al.,
2017). For example, the HC - mPFC interaction is selectively enhanced during epochs requiring spatial work-
ing memory (Jones and Wilson, 2005). Furthermore, the coordination of neural activity in both structures
spans multiple timescales, from theta sequences during navigation to reactivation (replay) activity during
inter-trial periods prior to navigation – and is considered crucial for the information exchange between the
two areas during the task (Tang et al., 2021).
Notably, disrupting awake hippocampal reactivations (sharp wave ripples, SWR) at decision points im-
paired the animal’s performance, but only for the aspects of the alternation task that require spatial working
memory (Jadhav et al., 2012), see Figure 1. Specifically, disrupting awake SWRs impaired outbound de-
cisions (from the central corridor to the left or right corridor), which require memory for immediate past
outer arm location. Rather, the same SWR disruption did not impair inbound decisions (from the left or
right corridor to the central corridor) that do not require memory of the past corridor; nor did it impair
self-localization (Jadhav et al., 2012). The study concludes that impairing SWRs prevents the hippocampus
from providing information about past locations (and/or future options) to the prefrontal cortex, which is
2
(a)
Figure 1: Spatial alternation tasks (LCRC). This figure exemplifies the W-maze used in (Jadhav et al.,
2012) to study spatial alternation and the role of the hippocampal - prefrontal dynamics. The W-maze is
characterized by three separate corridors. The animal can acquire the reward at the end of each corridor,
however, to receive the reward it must do so in the correct order (e.g. Left, Center, Right, Center, Left,
etc). The spatial alternation task implies that when the animal is in the center position, it has to make a
(difficult) outbound decision: it has to go either to the left or the right, depending on where it comes from.
This outbound decision, therefore, requires a spatial memory component. In contrast, when the animal is
in one of the side corridors (left or right), the only correct action is to move to the center. This inbound
decision is considered simpler since it does not require a memory component. Trajectory order is indicated
by the shade where lighter is earlier.
responsible to learn the alternation rule and to use it to guide behavior. In other words, the SWR disruption
impairs communication between the hippocampus and the prefrontal cortex, hence preventing the latter
from correctly inferring the current stage in task space.
The importance of hippocampal-prefrontal communication for memory guided decisions is confirmed by
another study in which animals had to learn and subsequently switch between three spatial alternation rules
in the same three-arm maze (Den Bakker et al., 2022). This study showed that disrupting neural activity
in the mPFC directly following hippocampal sharp-wave ripples (but not after a random delay) impairs the
animal’s ability to switch between learned rules.
Here, we advance a novel computational theory that casts the interactions between the hippocampus (HC)
and the medial prefrontal cortex (mPFC) during spatial alternation tasks, in terms of hierarchical active
inference (Friston, 2010; Friston et al., 2017a; Bogacz, 2017; Buckley et al., 2017; Parr et al., 2022; Smith et al.,
2022; Isomura et al., 2023). Our theory has two main tenets. First, the HC and the mPFC learn cognitive
maps for navigation in physical and task space, respectively, using the same statistical computations modeled
as Partially Observable Markov Decision Processes, POMDP (Kaelbling et al., 1998)), but based on different
inputs (see Section 4). Second, the neural circuit formed by the HC and the mPFC realizes a hierarchical
active inference architecture, which can solve spatial alternation tasks. Hierarchical active inference rests
on reciprocal, bottom-up, and top-down message passing. In the bottom-up pathway, the lower hierarchical
level (encoding the HC map) infers the current location based on sensory information and communicates
it to the higher level (encoding the mPFC map). In turn, the higher level infers the next goal location
(in the mPFC map) and sets it as a goal for spatial navigation (in the HC map), in a top-down manner.
Impairments of the message passing prevent the architecture from correctly solving spatial alternation tasks.
The separation of timescales automatically emerges from the structure of hierarchical active inference. The
lower level passes the bottom-up message when a free-energy threshold is met, indicating it is confident it
has reached its goal.
We exemplify the new theory by presenting two simulations of rodent spatial alternation tasks, with intact
and impaired HC- mPFC interactions (Jadhav et al., 2012; Den Bakker et al., 2022). Our first simulation,
presented in Section 2.2, brings three main conclusions. First, the model is able to learn the spatial structure
of the maze (HC map) and spatial alternation rules (mPFC) by navigating in the environment. Second, when
the HC - mPFC circuit is configured as a hierarchical active inference system, it effectively solves spatial
alternation tasks, through bottom-up and top-down message passing. Third, disrupting the interaction from
the HC to the mPFC breaks the spatial memory and impairs the animal’s ability to make outbound but
3
not inbound decisions, as observed empirically (Jadhav et al., 2012). Our second simulation, presented in
Section 2.2.1, shows two additional features of the model. First, in a task in which three alternative spatial
alternation rules are in play, the HC - mPFC circuit permits inferring the current rule. Second, selectively
inhibiting the mPFC impairs this ability and provokes distinct behavioral patterns, as observed empirically
in the rodent study reported in (Den Bakker et al., 2022).
We provide the code for the simulations at https://github.com/toonvdm/bridging-cognitive-maps.
2 Results
2.1 The W-maze setup and the hierarchical active inference (HAI) agent
In this work, we consider the spatial alternation task in the W-maze of (Jadhav et al., 2012), see Figure 1.
In this task, an animal (or an artificial agent) can acquire rewards at the end of each corridor, providing
that the corridors are visited in the correct order (e.g. left, center, right, center, left).
In this section, we present a hierarchical active inference (HAI) agent (Parr et al., 2022) for solving the
spatial alternation task. Central to this framework is the fact that agents are endowed with a generative
model, describing how observed outcomes are generated from a hidden, unobserved state, which is influenced
by the agent’s actions. The agent infers these hidden states by minimizing the free energy functional with
respect to its belief over the state and acts to minimize its expected free energy over time (Parr et al., 2022).
For more details on these mechanisms, the reader is referred to Section 4.2.
In our simulations, the HAI agent is endowed with a hierarchical generative model of two layers, which is
schematically illustrated in Figure 2a (see Section 4.2 for a formal introduction). In this generative model,
the lowest layer takes up the functional role of the hippocampus, namely encoding the spatial structure of
the maze (O’Keefe and Nadel, 1979). In contrast, the highest layer takes the functional role of the prefrontal
cortex, namely encoding the structure of task space (Schuck et al., 2016). The agent learns both maps
using the same sequence learning algorithm: the clone structured cognitive graphs (CSCG (George et al.,
2021)), see Section 4 for details. The latent states learned by the CSCG at the two levels are schematically
illustrated within the two boxes of Figure 2a.
Crucially, the HAI agent performs inference (of where it is in physical space, i.e., its pose, and in task
space) and planning (of the next goal in physical and task space) through message-passing between the two
hierarchical levels. As shown in Figure 2a, level 1 observes the structural aspects of the environment through
its bottom-up observations, i.e. the three-by-three grid around the agent as shown in Figure 2b and selects
local movements in physical space such as “turn left”, “turn right” or “move forward” to reach any goal
location (which is fed by the highest level, see below).
Once level 1 has navigated towards its local goal, it passes a bottom-up message containing the current
state of the agent to level 2. In turn, level 2 processes this message and infers the next stage of the task to
achieve a reward (as it is endowed with a prior preference to achieve reward at each step, see Section 4.2).
Actions at level 2 are abstract and are matched to the bottom-up received observable states from level 1.
Since level 1 states encode the agent’s pose, these actions essentially mean “move to target location”. We
specifically encode only actions corresponding to target locations at the end of a corridor (i.e. observation
1 in Figure 2b). The selected action is sent to level 1 in a top-down manner and corresponds to the goal
location for level 1. Once level 1 reaches the goal location, this process repeats.
Finally, the right part of Figure 2a illustrates the fact that the lowest level of the model is responsible
for the action-perception loop with the environment – here, an implementation of the W-maze in the Min-
igrid (Chevalier-Boisvert et al., 2018) simulator. Observations in the Minigrid (Chevalier-Boisvert et al.,
2018) are acquired as a square around the agent. In this work, we chose a range of three, yielding three-by-
three observations which are then mapped to a unique one-hot index. For each step, the agent also observes
a binary value indicating the reward presence. The actions the agent can perform are “turn left”, “turn
right” and “move forward”, which are also represented as a one-hot index. See Section 4.2 for more details
on the model implementation
4
(a)
(b)
Figure 2: Illustration of the hierarchical active inference (HAI) agent and the W-maze setup
(a) The hierarchical generative model of the HAI agent. The figure shows that the two layers of the model
include cognitive maps of physical space (level 1) and task space (level 2). These cognitive maps essentially
represent transitions between learned locations and stages of the task, respectively. Both maps are learned
using a probabilistic sequence learning algorithm: the clone structured cognitive graph (CSCG (George et al.,
2021)). Within each block, a subset of states from the CSCG is visualized, namely, the active states when
pursuing the alternation rule (LCRC). For a visualization of the full set of states from the CSCG, we refer
to Figure 6. The dynamics between the physical (level 1) and task space (level 2) CSCG in the hierarchical
model are regulated by bottom-up message passing (that supports inference) and top-down message passing
(that supports planning). Bottom-up inference messages from the physical space model are passed to the
task space model, while top-down actions drive planning from the task-space model to the physical space
model. An essential aspect of these interactions is that the two levels operate on a different temporal scale.
Transitions at the physical level, through the agent’s movement, occur at every time step. Rather, the task
level only transitions when the intermediate goal (of reaching a certain position in the maze) is achieved.
This abstraction allows for hierarchical planning. Finally, the interaction with the environment using an
action-perception loop with the world, the W-maze, is shown on the right. (b) The 20 distinct observations
the agent can encounter in the W-maze created in the Minigrid environment (Chevalier-Boisvert et al., 2018).
5
2.2 Experiment 1: Solving the spatial alternation task
We replicate the task and experiment from a study on rodents (Jadhav et al., 2012) where the animal is
taught this specific alternation task. In this study, disruption of the hippocampal sharp-wave ripple (SWR)
reduces the performance on outbound trajectories, but not on inbound trajectories.
We first allow the HAI agent to learn the two cognitive maps for solving the spatial alternation task,
using two CSCGs (see (George et al., 2021) and Section 4 for details on the learning procedure). The learned
maps for the W-maze are shown within the two boxes of Figure 2a. For ease of visualization, within each
box, only a subset of states from the CSCG is shown, namely, the states that are active when correctly
pursuing the alternation rule (the complete learned maps are shown in Figure 6). We denote the sequence of
this alternation rule as LCRC (for Left, Center, Right, Center). For a visualization of the full set of states
from the CSCG, please see Figure 6.
The hippocampal map on the lowest level organizes the 20 observations that the agent encounters during
navigation (illustrated in Figure 2b) into a coherent graph, which nicely reflects the 3 corridors of the W-
maze. In this map, the circles reflect the states learned by the CSCG, while the transitions reflect the
agent’s spatial actions (e.g., “turn left”, “turn right”, or “move forward”). The nodes are color-coded (and
numbered) according to the observations that they encode. Note that while the agent encounters the same
observations multiple times during navigation (i.e., the observations are aliased), the CSCG correctly reflects
the fact that they are part of different behavioral sequences. For example, observation 1 (that corresponds
to the location in which a reward can be collected) appears three times, at the apex of each of the latent
sequences that represent each of the three corridors.
The prefrontal map on the highest level corresponds to a much smaller graph that encodes the task-specific
sequence of corridors that the agent has to visit to secure rewards. In this graph, the nodes correspond to
(color-coded) corridors in which rewards have been collected and the edges correspond to higher-order actions
(“go to corridor”). Note that there are two nodes of the same color: both correspond to the central corridor
but in different phases in task space (i.e., after the left and the right corridors, respectively). This indicates
that the agent has successfully learned a low-dimensional representation of (or a finite state machine for)
the task rule LCRC.
We then investigate whether the HAI agent equipped with the learned cognitive maps for physical and
task space is able to solve spatial alternation tasks. For this, we endow the HAI agent with a prior preference
for maximizing the reward and place it in a random pose in the W-maze. We then let the agent run for
300 steps, and record whether it selected the correct or the incorrect corridor to find the reward. We repeat
this experiment for 20 trials, to collect statistics. Figure 3a shows that the HAI agent has near-perfect per-
formance during the easiest, inbound decisions and very high (above 80%) performance during the harder,
outbound decisions, closely approximating the empirical results reported in Jadhav et al. (2012). Further-
more, as expected, it significantly outperforms a Random agent that selects the next corridor randomly
(t-test p=0). Figure 3b shows that after a few time steps, once the HAI agent has inferred where it is (in
both physical and task space), it consistently follows the rule and is able to collect rewards at every step.
We performed two control experiments, which reveal that the HAI model is robust to a wide choice of
parameters (Appendix D.2) and to greater levels of uncertainty (Appendix D.3). Our control experiments
also illustrate that in scenarios characterized by greater uncertainty, the expected free energy functional
used by the HAI model to select policies is more advantageous compared to the sole objective of reward
maximization that is common in economic and reinforcement learning settings. The expected free energy
functional effectively balances two components: a pragmatic imperative to maximize reward (e.g., to reach
the next subgoal to secure reward) and an epistemic imperative to gain information (e.g., to reduce uncer-
tainty about the current pose, by going to places where unambiguous observations could be found, e.g., the
T-junction), see Section 4.2. By comparing HAI agents with and without the epistemic imperative, we found
that the pragmatic imperative to secure rewards is sufficient to address a W-maze with low uncertainty, but
adding the epistemic imperative to reduce uncertainty about one’s hidden state (here, the pose) significantly
increases performance in a W-maze with greater uncertainty (Appendix D.3). This is because an agent en-
dowed with epistemic uncertainty can explicitly plan to self-localize (e.g., by visiting unambiguous locations,
such as the bottom of corridors), which is especially advantageous with greater uncertainty (Parr et al., 2022;
6
(a)
 (b)
(c)
Figure 3: Experiment 1: Solving the spatial alternation task with intact and disrupted spatial
memory. (a) The success rate in the spatial alternation task for the HAI agent without disruption, the
HAI agent with disruption (HAI disrupted), and the Random agent, separated for both in- and outbound
trajectories. The success rate is computed over 20 trials per agent, with 300 steps per trial, only recording
the presence of reward at the end of the selected corridors. (b) The reward over time for the HAI agent and
a disrupted HAI agent. The shaded areas in the plot indicate outbound trajectories. (c) The success rate
for the HAI agent with (HAI disrupted) and without (HAI) disruption in the alternative light environment.
The success rate is computed over 20 trials per agent, with 300 steps per trial, only recording the presence
of reward at the end of the selected corridors. The agent is disrupted by disabling the connection from
the transition model of the POMDP, separated for in- and outbound trajectories. We observe that the
performance on inbound trajectories does not change, while for outbound trajectories this drops to random
chance.
7
Schwartenbeck et al., 2019).
2.2.1 Disrupting spatial memory impairs outbound decisions
Having established that the HAI agent correctly solves the spatial alternation task, we next ask whether
an impairment of spatial memory disrupts its performance in outbound decisions, as shown experimentally
by Jadhav et al. (Jadhav et al., 2012). In this study, the experimenters disrupted hippocampal SWR at
decision points and observed that the disruption prevented access to spatial memory – rendering the rodents
unable to make correct outbound decisions – but left hippocampal spatial codes intact. The experimenters,
therefore, concluded that the disruption was caused by a failure of communication or updating at the level
of the PFC.
In analogy with this procedure, we realized a variant of the HAI agent (HAI disrupt) in which we
implement the SWR disruption by preventing the belief updates (through the transition model) at the
second level of the hierarchical model (Figure 7 as explained in Section 4.2). By doing this, we remove the
agent’s spatial memory about task space. However, the agent is still able to receive the bottom-up message
from level 1 and thus knows where it currently is in physical space – in keeping with the finding that spatial
codes are intact after SWR disruptions (Jadhav et al., 2012).
When we compare the HAI agent with and without disruption in the spatial alternation task, we observe
the same behavior as the experimental study: the HAI disrupted agent correctly addresses inbound decisions
but makes outbound decisions at a chance level, see Figure 3a. The simulation shows that the disruption
only impairs the outbound trajectories (please see Appendix D.1 for additional simulations exploring how
the choice of the gamma parameter influences the results). A more detailed example of this pattern of results
can be appreciated in Figure 3b, which shows that the HAI disrupted agent tends to miss rewards, but only
during outbound decisions.
2.2.2 Control experiment: the disruption impairs outbound decisions even when task vari-
ables are observed
Finally, we performed a control simulation to identify more clearly why the disruption impairs the ability
to make the outbound decisions. It is worth noting that in the higher-level (prefrontal) map of the HAI
model, the task variables (color-coded nodes in Figure 2) are hidden (i.e., they do not have corresponding
observations), unlike the spatial states in the hippocampal map that have corresponding observations. Hence,
the agent’s estimate of the hidden state at the level of task space depends entirely on spatial memory, as
encoded in the model’s transition function – which we disrupt.
To assess whether the partial observability of task space is key to explain the effects of the disruption,
we considered an alternative (control) environment where the task variable is observable, however, the
component required for spatial memory is unobserved. We consider the following variant of the W-maze
environment: the maze is augmented with a light signal. Depending on where the agent just collected reward,
a different color is shown. We implement this by adding an additional observation modality containing a
categorical variable, indicating the previous rewarding corridor.
We repeated the same disruption experiment as before and found that the disrupted HAI agent maintains
a high success rate for inbound decisions, but makes outbound decisions at chance level (Figure 3c). This
control experiment therefore indicates that disruption of spatial memory impairs the ability to make the
outbound decisions, even when task variables are observable.
2.3 Experiment 2: learning multiple rules and flexible switching between them
In the next experiment, we investigate whether the HAI agent is able to learn multiple alternation rules as
proposed in (Den Bakker et al., 2022). We consider three spatial alternation rules, where for each rule, a
different corridor serves as the alternation point (LCLR, LCRC, and RCRL). Note that for simplicity, we
use the same structure and notation as in the previous experiment with the W-maze, despite the study of
8
(a)
(b)
(c)
(d)
Figure 4: Experiment 2: Multiple spatial alternation rules (LCLR, LCRC, and RCRL). (a) Rep-
resentation of the states used in the CSCG for the three distinct rules. States again map to the conjunction
of the visited corridor and the presence of reward. Colors in the graph indicate the rule for which the state
is used (red for rule 1, green for rule 2, and blue for rule 3). The gray nodes represent other states, either
without reward or in transition between multiple rules (b) Disruption experiment for the model that learned
three rules, evaluated on a single rule. The success rate is computed over 20 trials per agent, with 300 steps
per trial. (c) Average reward for the agents trained on 3 rules, 1 rules and random selected. The case where
the rule is constant (left), and the rule switches every 300 steps to a randomly selected next rule (right)
is considered. This experiment was repeated over 20 trials of 900 steps. (d) Performance during a single
representative trial of 1000 steps, during which the rule switches randomly, every 150 steps. The top plot
shows the rule that is currently in play. The center plot shows the belief over rule, which is computed by
first extracting states belonging to a single rule, and then measuring the likelihood of being in one of these
states. The bottom plot shows the rewards collected over time.
multiple alternation rules of (Den Bakker et al., 2022) has been conducted on a three-arm radial maze, not
a W-maze.
In this experiment, the HAI agent is endowed with a cognitive map of physical space which is the same
as Experiment 1 (since the maze is always the same). However, it has to learn a new cognitive map of task
space, which now comprises the dynamics of the three rules. For this, we let the agent explore the maze for
8000 steps, with alternating task rules after blocks of 1000 steps (in task space, i.e. visits to corridors). To
aid the learning process, we guide the sequence and select the correct action 75% of the time, and random
otherwise.
The learned cognitive map of task space is visualized in Figure 4a. For ease of visualization, we extract
the states that are active when the agent has correctly inferred the rule, after a warmup period (15 steps)
9
in each block. Similar to Section 2.2, the states of the task space correspond to the distinct colors, however
since the different rules have distinct transition dynamics, these are mapped to unique states. Figure 4a uses
different colors to visualize the states of the three different rules (red for rule 1, green for rule 2, and blue for
rule 3). The smaller dark gray states are states used for transitioning between rules, or encode the corridors
where no reward is found. We observe that for each of the individual rules, four states are active. This is an
indication of correct learning since four is the optimal number required for each alternation problem.
Then, we evaluate whether the HAI agent is able to infer which rule is currently in place – in order
to continue collecting rewards when the rule switches. For this, we test the HAI agent (endowed with a
preference to maximize reward) in a task in which the rule switches every 300 steps.
Figure 4b shows the success rate of various agents for a fixed rule over 20 trials. From the success rate
of the HAI agent trained on the three rules (HAI 3 rules), we can conclude that the agent properly encodes
the rule. When we apply the same disruption as in Experiment 1 (Section 2.2), the success rate for inbound
trajectories is now also affected (HAI 3 rules disrupt). This drop in performance is caused by the fact that
when considering multiple rules, there is also ambiguity over which rule is currently in play, for which the
spatial memory component is crucial.
Figure 4c compares the performance of various agents in two scenarios: in the former, there is a single
rule, which remains fixed throughout the experiment (left plot), whereas in the latter, there are three rules,
which switch randomly every 300 steps, over 20 trials. The left plot shows that both the Active Inference
agents trained on three rules (HAI 3 rules) and on one rule (HAI 1 rule) achieve a very high success rate in
the first scenario and both outperform a random agent (random). This result indicates that learning multiple
rules does not decrease performance when a single rule is in place. The right plot shows that the Active
Inference agent trained on multiple rules (HAI 3 rules) achieves a high success rate in the second scenario,
too, indicating it correctly learned all the rules and successfully alternates them. The Active Inference agent
trained on a single rule (HAI 1 rule) achieves a lower success rate, as expected, but still it outperforms a
random agent. This result shows that learning only a subset of the task rules could lead to a relatively
good performance in the W-maze, but still it is possible to discriminate between agents that learn the task
completely, partially or select randomly from their behavior.
Figure 4d illustrates the behavior of the agent during a single, representative trial. As shown in the
figure, shortly after the rule switches (top panel), the HAI agent is able to correctly update its belief about
the current rule in its plan (center panel) and secure rewards (bottom panel). The inference of the rule
currently in play follows standard Bayesian approach: when the rule changes, the agent receives unexpected
observations (since expected rewards are not delivered) and correctly makes a transition to the subspace of
the task rule map that encodes the most likely rule. In the center panel of Figure 4d, we visualize the belief
over each rule as the probability of being in one of the four states used in a particular rule (the rules are
color-coded as Figure 4a) in or in another phase if the state does not occur in either of the rules (grey line).
When the agent has a high probability of a rule, it no longer misses the reward. In some cases, it misses the
reward because there is a potential overlap between the two rules. For example, around step 200 it could
be either of the rules, as it only picks the right one once, and the wrong one after – which could happen
for each corridor in each of the rules. Note that in this architecture, the belief about the currently active
rule is implicit in the model, but it could be made explicit by adding a hierarchical layer that maintains a
probability distribution over the map or the task one is currently in, as in (Stoianov et al., 2022).
2.3.1 Disrupting spatial memory yields specific patterns
Another important finding of the study of Den Bakker et al. (2022) is that disrupting optogenetically the
medial prefrontal cortex (mPFC) immediately after a hippocampal SWR is detected significantly impairs
performance, by increasing the occurrence of three distinct maladaptive patterns: (i) rotated alternation,
where the agent follows a different rule, (ii) back and forth, where the agent alternates between two corridors,
and (iii) circling behavior, where the agent iterates over all corridors in a cyclic pattern (see Figure 10 for a
graphical illustration of these patterns). The authors reported a proportion of 35% alternation, 20 % rotated
alternation, 15 % back and forth, and 10 % circling.
The behavioral patterns that are generated from the same type of disruption as conducted in experiment
10
Table 1: Behavioral patterns during disruption: Classified observed behavioral patterns according
to Den Bakker et al. (2022) in different scenarios for agents trained on the three rules indicated by (3) and
a single rule indicated by (1).
HAI HAI HAI HAI Random
control (1) disrupt (1) control (3) disrupt (3)
Alternation 86.33% 49.07% 76.00% 16.80% 3.47%
Rotated alternation 0.00% 0.80% 1.07 % 13.55% 8.24%
Back and forth 2.68% 38.40% 2.40 % 38.48% 8.86%
Circling 1.61% 1.60% 8.80 % 17.34% 5.86%
Other 9.38% 10.13% 11.73% 13.82% 73.75%
1 - where the transition model of the POMDP is lesioned - are investigated. We match the patterns observed
in the biological counterpart of this experiment (Den Bakker et al., 2022), and evaluate the trajectories
generated by the agent trained on the three rules, for an agent with and without the spatial memory
disruption from Section 2.2, as well as a random agent. The results are reported in Table 1. This shows
that, when lesioned, our model reproduces all four behavioral patterns seen in lesioned animals (Den Bakker
et al., 2022) (alternation, rotated alternation, back and forth, and circling) although in slightly different
proportions compared to the empirical study (please see Appendix D.1 for a simulation how the choice of
γ affects these proportions). We observe that for the disrupted agent, a large part of the behaviors can be
classified as back-and-forth behavior. The specific proportions of categorization are a function of the agents’
habit policy E. This habit acts as a prior over the selected action (of level 2). When no temporal information
is considered through the lesion, this prior will dominate, and the agent will act according to the statistics
of the training data. This can be observed by looking at HAI disrupt (1), where the agent behavior falls
back to either the alternation or the back-and-forth case. Finally, we also note that the patterns generated
by the random agent are generally classified as the other category, indicating that the behavior generated
by our disruption is not random.
Furthermore, we performed the same experiment for an agent trained on a single rule. We observe that
both rotated alternation and circling behavior drops to near 0% as shown in the table indicated by HAI
control (1) and HAI disrupt (1). This is due to the agent not considering other rules, when falling back to
its habit policy. To the best of our knowledge, the pattern of errors in animals trained with a single rule
have not been systematically studied and hence our results could be considered as a novel prediction of the
HAI model.
2.3.2 Disrupting communication between the two levels yields perseveration behavior
The previous simulations illustrate that disruptions of the HAI model permit reproducing empirical findings
of rodent studies (Den Bakker et al., 2022; Jadhav et al., 2012). However, our model permits realizing
synthetic experiments – for example, other types of disruptions – that have not been studied empirically so
far, but which could be interpreted as novel predictions of the model.
Here, we investigate the potential effects of a disruption of hippocampal - prefrontal communication
that completely impairs the communication between the two levels of the HAI model. Specifically, in
this simulation, we prevent level 1 from sending any bottom-up information to level 2. This is a severe
impairment, since hierarchical inference rests upon the reciprocal messages passing between the two levels –
and level 2 only receives observations from level 1, not from the environment. Figure 5 shows the differences
between the physical trajectory of the intact HAI agent (left) and the disrupted HAI agent (right) in which
communication between the two levels is impaired. The shade of the trajectory indicates the order of visits
(lighter is earlier). The figure shows that while the intact HAI agent follows the correct rule, the disrupted
HAI agent perseveres in choosing the center corridor.
Interestingly, the agent does check at the unambiguous T-junction to self-localize and ensure that it is
still in the correct corridor (which is a feature of the epistemic dynamics of active inference, see Section 4.2),
11
Figure 5: Disruption experiment for rule switching : when communication between the level 1 and
level 2 model is suppressed, perseveration behavior is observed. Trajectory order is indicated by the shade
where lighter is earlier, noise was added to the trajectory to visualize overlapping visits.
but never selects the next goal in the task rule. This is because, without the bottom-up message from level 1,
level 2 is unable to correctly recognize that one phase of the task has been achieved and hence never updates
the top-down plan. This result illustrates the possibility to use the model to generate novel predictions – in
this case, about how disrupting communication between the two levels produces perseveration – which could
be subsequently tested empirically.
3 Discussion
Here, we advanced a novel computational theory of hippocampal (HC) - prefrontal (PFC) interactions during
cognitive tasks that require navigating in both physical and task space – such as spatial alternation tasks.
Empirical studies of spatial alternation have assessed that they depend on the animal’s spatial memory,
which is at least in part maintained by HC-PFC communication (Jones and Wilson, 2005; Spiers, 2008; Shin
and Jadhav, 2016; Patai and Spiers, 2021; Tang et al., 2021; Simons and Spiers, 2003) and that disruption of
this communication can impair difficult (outbound) decisions (Jadhav et al., 2012) and the ability to switch
between multiple rules (Den Bakker et al., 2022).
Our computational model is based on – and unites – two established theories. The former is a theory of
cognitive map formation, based on a statistical sequence learning algorithm: the clone-structured cognitive
graph (CSCG) (George et al., 2021). Previous studies assessed that CSCG are computationally effective
and have biological validity since they are able to successfully reproduce a number of empirical observations
about cognitive map formation in the hippocampus, including for example the emergence of place cell coding
and splitter cells (Raju et al., 2022b; Sun et al., 2023) and orthogonalized state representations (Sun et al.,
2023) (see also (Whittington et al., 2020, 2022) for alternative proposals about the computational principles
that underlie spatial map formation). Here we extended this body of work by showing that CSCGs can
be used in a hierarchical scheme to learn not just cognitive maps for physical space (putatively linked to
hippocampal computations), but also more abstract cognitive maps for task space (putatively linked to
prefrontal computations). Furthermore, we used learned CSCGs as components of a hierarchical generative
model for active inference (Parr et al., 2022). Active inference is a normative framework to model sentient
behavior in terms of free energy minimization and (approximate Bayesian) inference over a generative model,
which is gaining popularity in cognitive neuroscience. We have shown that by combining two learned CSCG
maps (for physical and task space), it is possible to design an effective hierarchical active inference (HAI)
agent able to solve spatial alternation tasks. Interestingly, this scheme affords (hierarchical) planning by only
using local computations – that is, top-down and bottom-up message-passing between the two hierarchical
levels.
Furthermore, by simulating the interruption of HC - PFC communication in our model, we were able to
correctly reproduce impairments of difficult (outbound) decisions (Jadhav et al., 2012) (Experiment 1) and
of correct rule switching, unveiling the same kind of maladaptive behavior – rotated alternation, back and
forth, and circling behavior (Experiment 2) – observed empirically in rodents (Den Bakker et al., 2022).
Our model suggests that the selective impairment of outbound decisions provoked by hippocampal SWR
disruptions (Jadhav et al., 2012) is due to the fact that the SWRs convey messages to higher structures, like
12
the PFC, which are used to update a belief about the current stage of the task (specifically, this message is
key to propagate the belief about task state at level 2 over time). This interpretation is in keeping with the
idea of (Jadhav et al., 2012) that the impairment is at the level of spatial memory, not of hippocampal place
coding, but to our knowledge, ours is the first work that provides a mechanistic model of this theoretical
proposal. Furthermore, our model suggests that the maladaptive behavior found in (Den Bakker et al.,
2022) could be due to the impossibility for the higher, prefrontal component (level 2) to correctly update
its belief, based on bottom-up message passing from the hippocampal component (level 1). This perspective
is coherent with the finding of (Den Bakker et al., 2022) that the only disruption of mPFC that prevents
spatial rule switching is one that directly follows hippocampal SWRs – hence highlighting the importance
of coherent HC - PFC reactivations to solve spatial alternation tasks. Finally, our simulations suggest that
other, more severe interruptions of HC - PFC communication (Section 2.3.2) could produce a specific pattern
of maladaptive behavior – namely, perseverative behavior – that differs from those observed in the above
disruption experiments. This is a novel prediction that remains to be tested empirically.
Besides helping understand HC - PFC interactions, our simulations (Appendix D.3) suggest that looking
at the animal’s epistemic behavior (e.g., the selection of actions to self-localize and reduce uncertainty
about the current pose) during uncertain tasks could be important to elucidate its strategy; in particular,
whether it aims to maximize reward or also to minimize its uncertainty – as predicted by active inference
(Parr et al., 2022; Schwartenbeck et al., 2019; Rens et al., 2023). Future studies might assess whether
epistemic imperatives are important drivers of behavior during spatial alternation or similar tasks, especially
in conditions of high uncertainty, such as when the animal is randomly placed in a maze (as in our simulations)
or when spatial contingencies or task rules change unexpectedly.
It could, in principle, be possible to solve the navigation tasks studied in this article using a non-
hierarchical generative model with a single “map” (and a single CSCG) that encompasses both spatial
and task-related components. However, the hierarchical structure of the HAI generative model used in
this study better reflects the implicit division of labor between HC and PFC circuits, which is well estab-
lished empirically in rodent studies. For example, inactivating prefrontal structures during navigation tasks
tends to disrupt rule-related contingencies and deliberative behavior, while sparing spatial representation
Den Bakker et al. (2022); Schmidt et al. (2019). Therefore, our model reflects the widespread perspective
that goal-directed navigation depends on the coordinated interplay between (inferential) processes at two
levels, which could be associated with HC and PFC structures (Shin and Jadhav, 2016; Tang et al., 2021;
Patai and Spiers, 2021; Ito et al., 2015). The division of labor between HC and PFC is also central to
other prominent accounts, such as the Complementary Learning Systems framework, which highlights that
faster learning in HC facilitates slower learning in neocortex – with the latter integrating across episodes
to extract semantic structure (McClelland et al., 1995). Furthermore, from a computational perspective,
the hierarchical structure of the HAI model affords a useful factorization: learning a novel rule in a known
maze, as we did in Experiment 2, only requires re-train the higher-level CSCG, while leaving the lower-level
CSCG unchanged. This might be more problematic when using a single CSCG that combines spatial and
task-related information.
The current study has several limitations that will need to be addressed in future studies. First, for
efficiency reasons, we learned the CSCGs offline (before embedding them into the HAI), using a simplified
procedure: we used predefined trajectories that exhaustively covered the W-maze as inputs for the cognitive
map of physical space and 75% correct trajectories as inputs for the cognitive map of task space. In the
future, it would be interesting to train CSCG online, similar to (Lazaro-Gredilla et al., 2023), by guiding
the exploration through active inference dynamics (Friston et al., 2017a; Schwartenbeck et al., 2019; Parr
et al., 2022). A second research avenue is to relax the separation of the timescales between the two levels,
by selecting their inputs (e.g., level 1 takes all sensory observations as inputs, whereas level 2 only considers
observations that could be rewarding – and in particular, observation 1 in Figure 2b). In the future, it
would be interesting to explore methods to learn hierarchical models with multiple timescales (Yamashita
and Tani, 2008; Hinton et al., 2006) and effective state spaces for navigation and for task rules in self-
supervised (and/or reward-guided) ways, as shown in prior work (Stoianov et al., 2022, 2018, 2016; Niv,
2019). This might also help understand the reciprocal influences between cognitive map learning at different
13
levels and in different (e.g., prefrontal versus hippocampal) brain structures. A third challenge is to avoid
having the agent learn from scratch each new maze or rule. Recent work in transfer learning shows that it
is possible to reuse existing cognitive maps or latent task representations to learn novel and similar tasks
much faster (Stoianov et al., 2022; Guntupalli et al., 2023; Stoianov et al., 2016; Swaminathan et al., 2023).
Extending our architecture with transfer learning abilities would be important to provide more accurate
models of how animals learn cognitive maps, especially given the strong evidence for the reuse of existing
neural sequences and cognitive maps in the hippocampus (Liu et al., 2019a; Farzanfar et al., 2023).
Another limitation of the model presented in this study is that in the CSCG of the task layer, we made
explicit the fact that the relevant task states are the ends of corridors, because the animals can (only)
acquire rewards in such states. This design choice reflects the animals’ knowledge, given that in the spatial
alternation task (Jadhav et al., 2012), the three arms had reward food wells at their endpoints. However,
future work should consider generative models that learn autonomously what the relevant task states are,
even if they are not explicitly cued as in Jadhav et al. (2012). There is an established literature showing that
latent task states relevant for cognitive control can be learned autonomously using Bayesian (nonparametric)
methods (Collins and Frank, 2013; Stoianov et al., 2016, 2018; Gershman and Blei, 2012) and that task rules
can be learned using deep reinforcement learning (Wang et al., 2018). However, it remains to be assessed in
future research how to integrate these methods in the HAI model.
Future studies might consider how to adapt the HAI agent to robot navigation and planning. There is
increasing interest in applying active inference to robotic domains, given the appeal of its unified approach
to control, state estimation, and world model learning (Lanillos et al., 2021; Da Costa et al., 2022; Taniguchi
et al., 2023). Hierarchical active inference is especially appealing, since it facilitates planning over long time
horizons. Indeed, the computational burden required to calculate the expected free energy of long policies
is high, but it can be substantially reduced by splitting work between (higher-level) policies that select (a
sequence of) subgoal(s) along with (lower-level) policies that select the specific actions to reach each subgoal
(Donnarumma et al., 2016). For example, one study built a hierarchical model for robot navigation, using
multiple layers of recurrent state space models (C ¸ atal et al., 2021). Another study realized a hierarchical
model for next-frame video prediction, using a subjective timescale for the predictions (Zakharov et al.,
2021). In another study (Van de Maele et al., 2023), a single layer CSCG was embedded within the active
inference framework and was able to support navigation in different mazes. An interesting direction of future
work could be adding a (learned) hierarchical layer that maps the high-dimensional observations space to
a discrete state space and stack the proposed hierarchical active inference on top with multiple layers of
CSCGs. This would potentially afford abstract reasoning and planning for complex tasks, directly from
sensory observations such as pixels.
Finally, future studies might consider in more detail the functional role and content of hippocampal
reactivations and replay in the hippocampus and other brain structures, such as the prefrontal cortex and
the ventral striatum (Wilson and McNaughton, 1994; Foster, 2017; Pfeiffer and Foster, 2013; Liu et al., 2019b;
Lansink et al., 2009; Peyrache et al., 2009; Wittkuhn and Schuck, 2021; Liu et al., 2018; Buzs´ aki, 2015, 2019;
Gupta et al., 2010; Nour et al., 2021; Pezzulo et al., 2014a, 2017). Previous studies suggested that replay
might play different roles, which range from memory functions to planning, compositional computation and
the optimization of the brain’s generative models (Stoianov et al., 2022; Shin et al., 2017; Mattar and Daw,
2018; Pezzulo et al., 2019, 2021; Wittkuhn et al., 2021; Kurth-Nelson et al., 2023). However, these works
have mostly considered hippocampal replay, not coordinated replay in the hippocampus and the prefrontal
cortex (and other brain areas). It might be interesting to combine the insights of these studies with the
hierarchical architecture of the HAI agent, to test whether (for example) planning and model optimization
benefit from the combined replay of multiple brain structures, as opposed to local replay in the hippocampus.
4 Methods
In this paper, we develop a hierarchical planning agent by combining two components: we useclone structured
cognitive graphs (CSCG) to learn “cognitive maps” of physical and task space; and hierarchical active
inference to form hierarchical plans. In the next sections, we explain the two key components of our
14
(a)
 (b)
Figure 6: Learned cognitive graphs. The arrows indicate possible transitions from one state to the next.
The node in the center where all nodes point to, is the added and absorbing “dummy” state. (a) The learned
CSCG for the physical space model (level 1). The color in the graph represents distinct observations, only
the states that are active when pursuing the spatial alternation rule (LCRC) are colored, the other states are
marked in gray. (b) The learned CSCG for the task space model (level 2). The color in the graph indicates
the states that were active when pursuing the spatial alternation rule (LCLR). While only four states are
necessary to encode the rule, it can be observed that many more states are added by learning the other paths
that might have been visited during training.
approach: CSCG (Section 4.1) and hierarchical active inference (Sec. 4.2). Finally, we explain how we
combined these (Section 4.3).
4.1 Clone-structured cognitive graphs (CSCG) for learning cognitive maps of
physical and task space
Clone-structured cognitive graphs (CSCGs) are a probabilistic model for representing sequences of data,
e.g. a sequence of action-observation pairs (Gothoskar et al., 2019; George et al., 2021). They are a special
case of Hidden Markov Models (HMM), where each observation maps to a subset of the hidden states: the
so-called clones of this observation. While these states have the same observation likelihood, they differ in
the implied dynamics encoded in their transition model. Through the sequence of action-observation pairs,
specific clones will have a higher likelihood and can therefore disambiguate the aliased observations.
The CSCGs are learned using the expectation-maximization (EM) algorithm for HMMs (the Baum-
Welch algorithm) which maximizes the ELBO as described in George et al. (2021). During the E-step, the
posteriors over states are estimated through smoothing, i.e. the forward-backward algorithm. The M-step
then selects the optimal parameters for the transition model, given this sequence of visited states. For the
update equations, the reader is referred to George et al. (2021).
We consider two cognitive maps, that represent the W-maze task at two distinct levels. The first level
considers the structure of the environment (i.e. where can the agent walk and where are the walls), and the
second level encodes the task rule (i.e. in which order the corridors yields the reward). We learn these two
maps in a sequential fashion, using two distinct CSCGs.
We first collect a sequence of data for learning the spatial structure by exploring the maze. We designed
the exploration sequence to exhaustively cover the W-maze such that a path from each pose to each possible
15
other pose is present. This sequence was used to learn the parameters of a CSCG for the first (physical) level,
using the expectation-maximization mechanism described in (George et al., 2021). We initialize the level 1
CSCG with 20 clone states per observation. After learning, the model is pruned using a Viterbi-decoding
algorithm as described in (George et al., 2021) and mapped to the POMDP of the first level of the hierarchical
generative model (as will be explained in Section 4.3) for engaging in active inference. The learned graph is
visualized in Figure 6a. The nodes in the graph correspond to the learned nodes and their colors represent
the observations encountered in the states, during the correct execution of the spatial alternation task. The
gray nodes represent other states required for transitioning between states or trajectories outside the correct
spatial alternation. Note that the graph shown in Figure 2a only shows the colored nodes, but not the gray
nodes.
We use the learned cognitive map of physical space to learn the cognitive map of task space. We first
extract the states that are distinctly representing the end of each corridor (i.e. matching with observation 1
in Figure 2b) and use them as the actions for the task level. When an action is selected, this means that the
agent is moving toward one of these states, and thus the end of one of the corridors. The observations at this
level are the presence of reward (“1” if the agent is following the rule, “0” otherwise), and the reached level
1 state (physical level). To aid the learning process, we guide the agent to follow the rule 75% of the time.
We now learn a CSCG with 10 clones per observation using the same expectation-maximization scheme for
the single rule scheme, and 32 clones for the scenario with three rules. After learning, the model is pruned
using a Viterbi algorithm as described in (George et al., 2021) and mapped to the POMDP of the second
level of the hierarchical generative model to engage in active inference (see Section 4.3). For a robustness
analysis on the model capacity with respect to amount of rules, rule length and amount of clones per state,
the reader is referred to the appendix. We visualize the learned cognitive graph in Figure 6b, where the
states used when pursuing the rule are colored in red. It can be observed that the spatial alternation rule is
encoded in this graph, using four states, as this is the optimal amount required for the spatial alternation
rule (two states for the center, one for the left, and one for the right corridor). Note that the graph shown
in Figure 2a only shows the colored nodes, but not the gray nodes.
4.2 Hierarchical active inference
Active inference is a normative framework to describe cognitive processing and brain dynamics in living
organisms (Parr et al., 2022). It assumes that action and perception both minimize a common functional:
the minimization of variational free energy (which is an upper bound to the organisms’ surprise). Central to
active inference is the idea that living organisms are equipped with generative models, capturing the causal
relations between observable outcomes, the agent’s actions, and hidden states.
It is important to note the difference between the agent’s generative model and the generative process.
The former represents the internal generative model that the agent uses to attribute consequences to its ac-
tions, while the latter represents the true process from which outcomes are generated in the world. Crucially,
the agent is unable to observe the hidden state of the generative process, as they are separated through a
Markov blanket given the observation and action hidden variables. However, the agent is able to perform
actions and observe the generated outcomes (Parr et al., 2022; Pezzulo et al., 2018, 2015).
4.2.1 The hierarchical generative model
We endow the active inference agent with the hierarchical generative model depicted in Figure 7. In this
section, we provide a high-level overview of the generative model, whereas in Section 4.3 we discuss imple-
mentation details.
The hierarchical generative model illustrated in Figure 7 is split into two distinct levels. The highest
hierarchical level (level 2) reasons at a more abstract level, e.g. about which corridor to visit, while the
lowest level (level 1) considers the step-by-step navigation in the environment. Each of these levels can
be interpreted as an individual partially observable Markov decision process (POMDPs) (Kaelbling et al.,
1998), operating at different timescales: faster for level 1 and slower for level 2. However, crucially, the two
levels interact reciprocally by exchanging messages, as described below.
16
Figure 7: The Hierarchical Generative Model: This generative model consists of two hierarchical levels,
where the top level operates at a slower timescale than the lower level. The policy at the highest level sets
the actions for the top level, which determines temporal transitions between states at level 2. The level
2 state then generates the presence of reward and the level 1 state. At level 1, the states generate the
observations and the policy generates the actions, depending upon the selected level 2 action. Finally, when
a policy is pursued, a message is passed to the upper level, transitioning the upper level to the next state.
This highlights the different timescales, whereas level 1 operates in the timescale of the agent’s movement,
the upper level operates at the timescale of the reached subgoals (visited corridors). Orange circles denote
observed variables, while white circles denote unobserved variables.
The hierarchical model supports the hierarchical selection of abstract policies that move the agent from
one goal to the other, following the task rule(s) (at level 2) and from one spatial location to the other, based
on the spatial map (at level 1). Conceptually, this hierarchical selection starts when the agent’s preferred
observation is set to find the reward (technically, in active inference, this is done by assuming a strong prior
for the reward observation) and it involves both levels.
The most fundamental goal of Level 2 is to select a policy π2 that moves the agent through task space,
by steering abstract actions a2
t that follow the rule learned by the agent (or in the case of multiple rules,
as in Experiment 2, the rule currently inferred to be in place). These actions indicate the location that the
agent will then try to reach in the spatial map (level 1). In this way, the level 2-action conditions the level
1 policy π1. The level 2 state s2
t is a latent representation of the corridor in which the agent is currently
located in, and where it comes from (i.e. where the agent is in rule space). This abstraction is enforced by
the choice of considering the reachable states to be matched to observation 1 (Figure 2b), reflecting the fact
that the agent knows the potential reward locations (which, in the animal experiments, are baited with food
wells). The agent only considers reward on level 2 and thus generates the presence of reward directly from
the level 2 state.
At level 1, the policy π1 is conditioned by level 2, by setting the preferred state as reaching one of the
17
spatial locations encoded in the cognitive graph. The policy π1 generates the low-level actions that navigate
the agent from one pose (position and orientation, encoded in hidden state s1
t ) to the next. Then, the
agent receives observations ot from the environment, which reflect its current pose in the maze and which
permit updating the hidden state estimate. Note that reaching the preferred spatial location requires making
multiple transitions at level 1, but level 2 only makes a transition between states when the preference at level
1 is reached (or equivalently when the level 2 action is “performed”). This creates a separation of timescales
because multiple transitions at level 1 are nested between two subsequent actions at level 2.
4.2.2 Active inference
As shown in Figure 7, the generative model considered in this paper is a hierarchically stacked (two levels)
POMDP. In order to reduce complexity, we first discuss active inference for a single layer POMDP, as
depicted in Figure 8. At time step t, we have observation ot, action at generated from policy π, and state
st. The generative model is factorized as:
P(˜ s, ˜ a, ˜ o) = P(s0)
Y
t=1
P(ot|st)P(st|st−1, at−1)P(at−1), (1)
where the tilde represents a sequence over time. As computing the posterior distribution over the state
is intractable for large state spaces, we resort to variational inference and introduce the approximate poste-
rior Q(˜ s|˜ o, ˜ a), the variational free energy measures the discrepancy between the joint distribution and the
approximate posterior:
F = EQ(˜ s|˜ o,˜ a)[log Q(˜ s|˜ o, ˜ a) − log P(˜ s, ˜ a, ˜ o)] (2)
Active inference agents minimize this quantity through learning (by optimizing the model parameters),
perception (by estimating the most likely state), and planning (by selecting the policy or action sequence
that results in the lowest expected free energy).
The expected free energy G is a quantity that is only used during planning and represents the agent’s
variational free energy expected after pursuing a policy π. It is distinct from the variational free energy,
because it requires considering future observations generated from the selected policy, as opposed to the
current (and past) observations.
Active inference realizes goal-directed behavior by selecting policies that minimize expected free energy
– and that are expected to yield observations that are closer to preferred observations (or prior preferences).
This is done by setting the approximate posterior of the policy proportional to this quantity:
Q(π) = σ(γG(π) + logE), (3)
where σ is the softmax function, γ is a temperature variable, and E is a prior distribution over actions,
or habit. Actions are sampled according to this posterior distribution, where low temperatures yield more
deterministic behavior. At a given timestep τ, G is computed for a given policy according to the following
quantity as formalized in (Heins et al., 2022):
G(π, τ) = −EQ(oτ |π)

DKL[Q(sτ |oτ , π)||Q(sτ |π)]

− EQ(oτ |π)

log P(o)

(4)
+ EQ(oτ |π)[DKL(Q(sτ |oτ )||P(sτ |oτ , π))]| {z }
Expected Approximation Error≥0
(5)
≥ −EQ(oτ |π)

DKL[Q(sτ |oτ , π)||Q(sτ |π)]

| {z }
Epistemic value
−EQ(oτ |π)

log P(o)

| {z }
Pragmatic Value
(6)
This quantity is decomposed into an epistemic value and a pragmatic value (Friston et al., 2017a). When
evaluating G, we compute the upper bound shown in Equation (6). We thus make the assumption that
the expected approximation error (Equation (5)) is zero (Heins et al., 2022). The epistemic value computes
the expected information gain (Bayesian surprise) between the prior Q(sτ |π) and posterior Q(sτ |oτ , π).
18
Intuitively, this quantity represents how much the agent expects the belief over the state to shift when
pursuing this policy. The pragmatic value then measures the expected log probability of observing the
preferred observation under the selected policy, intuitively computing how likely it is that this policy will
drive the agent to its prior preferences. The full expected free energy for a finite time horizon T is computed
as PT
τ=1 G(π, τ).
4.2.3 Hierarchical active inference
Here, we discuss how the active inference scheme introduced above is extended to realize hierarchical per-
ception and planning, through bottom-up and top-down message passing between the two levels of the
hierarchical generative model shown in Figure 7.
During perception, first, the low-level state s1 is inferred given observations ot and level 1 actions a1
t
using the inference mechanism implemented in PyMDP (Heins et al., 2022). When the free energy of the
lowest level reaches a pre-specified threshold, a message containing the most likely level 1 state s1
t the agent
is in is passed to the level above (level 2). This threshold is chosen to be reached when the level 1-preference
is reached, and uncertainty is below a fixed level. When level 2 gets this bottom-up message, it queries the
environment for the presence of reward and observes rτ . This conjunction of observations, together with
high-level action a2 is used to infer the belief over the level 2 state Q(s2), in the same way as done for level
1.
During planning, i.e. generating a sequence of actions that (should) drive the agent toward its goal,
policies are generated in a top-down manner, in the sense that goals set at level 2 determine the prior
preference (and then in turn the policy) at level 1. First, the level 2 policy is selected by sampling from
the approximate posterior over π2 proportional to its expected free energy. The action selected at this level
then conditions the policy at level 1 π1, for which in turn the approximate posterior Q(π1) is computed.
The low-level action a1
t is then sampled according to this distribution, similar to the hierarchical generative
model described by Friston et al. (Friston et al., 2017b).
As depicted in Figure 7, the policy of the lower level sets a limited amount of steps. In particular, when
the preference of the lower level is reached, a message is passed to the upper level, transitioning it into the
next state. In this way, inferring the policy at the lower level only considers one action of the upper level at
a time.
The posterior over the level 2 policy Q(π2) is first inferred, as it has no dependencies from above. To do
this, the expected free energy is computed, for which we look one time step ahead in our implementation
as this could predict the next corridor in which the agent encounters reward. We consider Equation 7, for
which the observations are now a conjunction of reward rτ and level 1-states s1:
G2(π2, τ) ≥ −EQ(rτ ,s1
t=τ |π2)

DKL[Q(s2
τ |rτ , s1
t , π2)||Q(s2
τ |π2)]

− EQ(rτ ,s1
t=τ |π2)

log P(rτ , s1
t=τ )

, (7)
Note that the level 1 state is synchronized with the level 2 state, and only the state observed at this
synchronized time is considered, hence we denoted the time index for the level 1 state by t = τ. In practice,
we used a temperature (γ) value of 0.5 for this level. For the habit policy E, we use a categorical distribution
over action, that is conditioned on the current state of the agent. The action a2
τ is then sampled according
to Q(π2).
The agent, however, is not able to act upon the physical world yet with this level 2 action. The agent
now has to infer the posterior distribution over the policy at level 1 Q(π1). Because we now want to move
towards a specific state, i.e. a disambiguated observation, we set the preference in state space for computing
the expected free energy at level 1. In practice, Equation 6, for level 1 then boils down to:
G1(π1, t) ≥ −EQ(ot|π1)

DKL[Q(s1
t |ot, π1)||Q(s1
t |π1)]

− EQ(s1
t |π1)

log P(s1
t )

, (8)
where the first term yields the expected information gain after pursuing policy π1, and the latter the
expected utility. In other words, how close the expected state is from the preferred state, set by the action of
the level above. This quantity is then used to approximate the posterior over the policy at level 1 from which
19
Figure 8: Factor graph of a POMDP : The conditional dependencies in Hidden Markov Models are
parameterized by a set of matrices. The A matrix parameterizes the likelihood model, i.e. how states st
map to observations ot. The B matrix parameterizes the transition model, i.e. how state st+1 changes at
each timestep, dependent on policy π. The C vector denotes the preferred observation or state, and directly
influences the G factor which conditions π. The policy π depends on the habit E. Orange circles denote
that the variable is observed.
action a1
t is sampled. At this level, we set a temperature ( γ) value of 0.5, and a uniform habit distribution.
Crucially, in order to evaluate G1 at planning depth 1, we set the prior preference ( C matrix) as such that
the preference for each state is proportional to the distance to the goal, i.e. intuitively this means that the
agent can follow a breadcrumb trail towards the goal, given that it properly inferred the current state.
4.3 Casting CSCGs as partially observable Markov decision processes
CSCGs can be used directly to plan (George et al., 2021). However, in this work, we use the two learned
CSCGs to create a hierarchical generative model for active inference – and then use hierarchical active
inference to solve the spatial alternation tasks.
Using the two CSCGs to create a hierarchical generative model for active inference requires mapping them
into two partially observable Markov decision processes (POMDPs) (Kaelbling et al., 1998), the mathematical
framework used in discrete time active inference.
Practically, a POMDP is described by a set of four arrays as shown in Figure 8. We describe these arrays
using the symbol notation typically adopted in discrete time active inference (Parr et al., 2022). The A
matrix encompasses the likelihood model P(o|s), or how states are mapped to observations. The B tensor
entails the transition model P(st+1|st, at), or how states change over time, conditioned on the selected action.
The C vector sets the prior over future observations or states, depending on the implementation. This vector
is used within active inference to embed the preference or goal state/observation into the agent. Finally, the
D vector describes the initial belief over the state P(s).
As mentioned earlier, clone-structured cognitive graphs are a special case of hidden Markov models and
can be therefore easily mapped to POMDPs (which extend HMMs, too). First, we consider the mapping of
the likelihood. This A matrix represents the mapping of observation to state, specifically Ai,j = P(oi|sj).
The matrix can be constructed through the definition of the CSCG, a deterministic mapping of clone state to
its observation is set: P(oi|sj) = 1∀sj ∈ C(oi) and P(oi|sj) = 0∀sj /∈ C(oi), where C(oi) denotes all clone
states for observation oi. When constructing the model we only consider states for which the marginalized
probability P(sj) surpasses a threshold of 0 .0001. Finally, we add an additional “dummy” state to the
additional “dummy” observation, to which unlikely actions are mapped (see below).
To define the transition model, or B tensor, we use the learned parameters of the model. Through the
learning process described in Section 4.1, the CSCG has learned the transition probability P(st+1|st, at).
This is parameterized as a tensor for which Bi,j,k = P(si|sj, ak). We construct this tensor with the learned
20
probabilities, for the same states that are considered in the likelihood matrix. Note that for ease of imple-
mentation, the active inference routines that we used require that any action could be executed from any
state. However, in our task, some actions are not available or highly unlikely in some states (e.g., turning
actions in a corridor). To handle this, we created a “dummy state” in the POMDP, which corresponds to
observation 20 in Figure 2b to which we map the “highly unlikely” actions and to which we assign a large
negative value (see below) to ensure that it is never selected during planning. In practice, we set a transition
probability of 10 −12 from every state to the dummy state (after which we re-normalize the tensor to sum
to one for each action and state). We also set a self-transition of 1 for the dummy state, therefore ensuring
that it is “absorbing”.
We endow the active inference agent with prior preferences to secure rewards in the spatial alternation
tasks in the C matrix. For ease of implementation, we set these prior preferences manually, rather than ex-
tracting them from the learned parameters of the CSCG. For level 2, we set a preference over the conjunction
of observations where the reward is 1 to encourage the agent to follow the learned rule. As described above,
when the level 2 action is selected, it sets the C matrix of level 1 to a large preference for the preferred state
and to a value proportional to the distance (in the learned state space of level 1) from said given state to
the preferred state (except for the dummy state, for which the C vector has a fixed value. This is set to the
lowest value, making the dummy state the least preferred state).
The prior over the state is parameterized by theD matrix. We parameterize this as a uniform distribution
over all the states, except for the dummy state. This reflects the fact that in our simulations, the active
inference agent is placed in the W-maze with a random pose and has to self-localize.
Finally, the habit, or the prior over action is parameterized by the E tensor. For level 1, we parameterize
this as a uniform distribution. For level 2, we model this habit as a Dirichlet distribution, that is conditioned
on the current state of the agent, and is proportional to actions that were rewarding during learning.
Using these tensors, inference at each single level can be implemented as a Bayes filter, iteratively
computing the posterior over state at each timestep using the following update:
qst = σ(A · ot + Bπt−1 · qst−1), (9)
where qst denotes the parameters of the categorical distribution over state, i.e. Q(sτ |oτ , π) = Cat(qs1
τ ), ot
represents the observation as a one-hot vector, σ denotes the softmax function, and the B-tensor is sliced
by the policy πt−1 (Friston et al., 2017a). In the first timestep, qs0 is initialized as the prior matrix D.
The expected free energy G for a considered policy πt can then be evaluated using these tensors for each
future timestep τ. As we specify the preference over state for the first level, and over observation for the
second level, the distinction is made explicitly. For the first level, this boils down to (Parr et al., 2022):
G1
τ = diag(A1⊤ · ln A1) · qs1
τ − qo1
τ · ln qo1
τ + qs1
τ · ln C1, (10)
where the superscript 1 denotes the first level, qs1
τ denotes the parameters of the categorical distribution
over state, i.e. Q(s1
τ |o1
τ , π1
t ) = Cat( qs1
τ ), which is computed as qs1
τ = B1,π2
t
· qs1
t . The parameters of the
categorical over the observation is denoted byqo1
τ , i.e. Q(o1
τ ) = Cat(qo1
τ ), and is computed asqo1
τ = A1·qs1
τ .
The same equation can be used for the expected free energy at the second level, except now the preference
is specified as an expected observation:
G2
τ = diag(A2⊤ · ln A2) · qs2
τ − qo2
τ · ln qo2
τ + qo2
τ · ln C2, (11)
where the superscript 2 denotes the second level. Q(s2
τ |o2
τ , π1
t ) = Cat( qs2
τ ), which is computed as qs2
τ =
B2,π2
t
·qs2
t . The parameters of the categorical over the observation at level 2 is denoted by qo2
τ , i.e. Q(o2
τ ) =
Cat(qo2
τ ), and is computed as qo2
τ = A2 · qs2
τ . The full expected free energy G can then be acquired by
summing over time horizon T: G = PT
τ=t+1 Gτ , and the posterior over the policies is achieved through
Q(π) = σ(γ · G + lnE).
21
Acknowledgements
This research received funding from the European Union’s Horizon 2020 Framework Programme for Research
and Innovation under the Specific Specific Grant Agreements No. 945539 (Human Brain Project SGA3) and
No. 952215 (TAILOR); the European Research Council under the Grant Agreement No. 820213 (ThinkA-
head), the Italian National Recovery and Resilience Plan (NRRP), M4C2, funded by the European Union
– NextGenerationEU (Project IR0000011, CUP B51E22000150006, “EBRAINS-Italy”; Project PE0000013,
CUP B53C22003630006, ”FAIR”; Project PE0000006, CUP J33C22002970002 “MNESYS”), and the PRIN
PNRR P20224FESY. The GEFORCE Quadro RTX6000 and Titan GPU cards used for this research were
donated by the NVIDIA Corporation. The funders had no role in study design, data collection and analysis,
decision to publish, or preparation of the manuscript.
References
Basu, R., Gebauer, R., Herfurth, T., Kolb, S., Golipour, Z., Tchumatchenko, T., and Ito, H. T. (2021). The
orbitofrontal cortex maps future navigational goals. Nature, 599(7885):449–452.
Bellmund, J. L., G¨ ardenfors, P., Moser, E. I., and Doeller, C. F. (2018). Navigating cognition: Spatial codes
for human thinking. Science, 362(6415):eaat6766.
Benchenane, K., Tiesinga, P. H., and Battaglia, F. P. (2011). Oscillations in the prefrontal cortex: a gateway
to memory and attention. Current Opinion in Neurobiology , 21(3):475–485.
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning. Journal
of Mathematical Psychology, 76:198–211.
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). The free energy principle for action and
perception: A mathematical review. Journal of Mathematical Psychology , 81:55–79.
Buzs´ aki, G. (2015). Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and plan-
ning. Hippocampus, 25(10):1073–1188.
Buzs´ aki, G. (2019).The brain from inside out . Oxford University Press.
Buzs´ aki, G. and Moser, E. I. (2013). Memory, navigation and theta rhythm in the hippocampal-entorhinal
system. Nature neuroscience, 16(2):130–138.
Chen, Y., Zhang, H., Cameron, M., and Sejnowski, T. (2022). Predictive sequence learning in the hippocam-
pal formation. bioRxiv, pages 2022–05.
Chevalier-Boisvert, M., Willems, L., and Pal, S. (2018). Minimalistic gridworld environment for gymnasium.
Colgin, L. L. (2011). Oscillations and hippocampal–prefrontal synchrony. Current opinion in neurobiology ,
21(3):467–474.
Collins, A. G. and Frank, M. J. (2013). Cognitive control over learning: creating, clustering, and generalizing
task-set structure. Psychological review, 120(1):190.
Da Costa, L., Lanillos, P., Sajid, N., Friston, K., and Khan, S. (2022). How Active Inference Could Help
Revolutionise Robotics. Entropy, 24(3):361.
Den Bakker, H., Van Dijck, M., Sun, J.-J., and Kloosterman, F. (2022). Sharp-wave ripple associated activity
in the medial prefrontal cortex supports spatial rule switching. preprint, Neuroscience.
Donnarumma, F., Maisto, D., and Pezzulo, G. (2016). Problem solving as probabilistic inference with
subgoaling: explaining human successes and pitfalls in the tower of hanoi. PLoS computational biology ,
12(4):e1004864.
22
Epstein, R. A., Patai, E. Z., Julian, J. B., and Spiers, H. J. (2017). The cognitive map in humans: spatial
navigation and beyond. Nature neuroscience, 20(11):1504–1513.
Farzanfar, D., Spiers, H. J., Moscovitch, M., and Rosenbaum, R. S. (2023). From cognitive maps to spatial
schemas. Nature Reviews Neuroscience, 24(2):63–79.
Foster, D. J. (2017). Replay comes of age. Annual review of neuroscience, 40:581–602.
Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience ,
11(2):127–138.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G. (2017a). Active Inference: A
Process Theory. Neural Computation, 29(1):1–49.
Friston, K. J., Rosch, R., Parr, T., Price, C., and Bowman, H. (2017b). Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews, 77:388–402.
George, D., Rikhye, R. V., Gothoskar, N., Guntupalli, J. S., Dedieu, A., and L´ azaro-Gredilla, M. (2021).
Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps.
Nature Communications, 12(1):2392.
Gershman, S. J. and Blei, D. M. (2012). A tutorial on bayesian nonparametric models. Journal of Mathe-
matical Psychology, 56(1):1–12.
Gothoskar, N., Guntupalli, J. S., Rikhye, R. V., L´ azaro-Gredilla, M., and George, D. (2019). Different clones
for different contexts: Hippocampal cognitive maps as higher-order graphs of a cloned hmm. bioRxiv, page
745950.
Guntupalli, J. S., Raju, R. V., Kushagra, S., Wendelken, C., Sawyer, D., Deshpande, I., Zhou, G., L´ azaro-
Gredilla, M., and George, D. (2023). Graph schemas as abstractions for transfer learning, inference, and
planning. arXiv preprint arXiv:2302.07350 .
Gupta, A. S., Van Der Meer, M. A., Touretzky, D. S., and Redish, A. D. (2010). Hippocampal replay is not
a simple function of experience. Neuron, 65(5):695–705.
Hafting, T., Fyhn, M., Molden, S., Moser, M.-B., and Moser, E. I. (2005). Microstructure of a spatial map
in the entorhinal cortex. Nature, 436(7052):801–806.
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., and Tschantz, A. (2022). pymdp: A
Python library for active inference in discrete state spaces. arXiv:2201.03904.
Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural
computation, 18(7):1527–1554.
Isomura, T., Kotani, K., Jimbo, Y., and Friston, K. J. (2023). Experimental validation of the free-energy
principle with in vitro neural networks. Nature Communications, 14(1):4547.
Ito, H. T., Zhang, S.-J., Witter, M. P., Moser, E. I., and Moser, M.-B. (2015). A prefrontal–thalamo–
hippocampal circuit for goal-directed spatial navigation. Nature, 522(7554):50–55.
Jadhav, S. P., Kemere, C., German, P. W., and Frank, L. M. (2012). Awake Hippocampal Sharp-Wave
Ripples Support Spatial Memory. Science, 336(6087):1454–1458.
Jones, M. W. and Wilson, M. A. (2005). Theta Rhythms Coordinate Hippocampal–Prefrontal Interactions
in a Spatial Memory Task. PLoS Biology, 3(12):e402.
Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). Planning and acting in partially observable
stochastic domains. Artificial intelligence, 101(1-2):99–134.
23
Khodagholy, D., Gelinas, J. N., and Buzs´ aki, G. (2017). Learning-enhanced coupling between ripple oscilla-
tions in association cortices and hippocampus. Science, 358(6361):369–372.
Kurth-Nelson, Z., Behrens, T., Wayne, G., Miller, K., Luettgau, L., Dolan, R., Liu, Y., and Schwartenbeck,
P. (2023). Replay and compositional computation. Neuron, 111(4):454–469.
Lanillos, P., Meo, C., Pezzato, C., Meera, A. A., Baioumy, M., Ohata, W., Tschantz, A., Millidge, B., Wisse,
M., Buckley, C. L., and Tani, J. (2021). Active Inference in Robotics and Artificial Agents: Survey and
Challenges. arXiv:2112.01871.
Lansink, C. S., Goltstein, P. M., Lankelma, J. V., McNaughton, B. L., and Pennartz, C. M. (2009). Hip-
pocampus leads ventral striatum in replay of place-reward information. PLoS biology, 7(8):e1000173.
Lazaro-Gredilla, M., Deshpande, I., Swaminathan, S., Dave, M., and George, D. (2023). Fast exploration
and learning of latent graphs with aliased observations. Publisher: arXiv Version Number: 3.
Levenstein, D., Efremov, A., Eyono, R. H., Peyrache, A., and Richards, B. A. (2024). Sequential predictive
learning is a unifying theory for hippocampal representation and replay. bioRxiv, pages 2024–04.
Liu, K., Sibille, J., and Dragoi, G. (2018). Generative predictive codes by multiplexed hippocampal neuronal
tuplets. Neuron, 99(6):1329–1341.
Liu, K., Sibille, J., and Dragoi, G. (2019a). Preconfigured patterns are the primary driver of offline multi-
neuronal sequence replay. Hippocampus, 29(3):275–283.
Liu, Y., Dolan, R. J., Kurth-Nelson, Z., and Behrens, T. E. (2019b). Human Replay Spontaneously Reorga-
nizes Experience. Cell, 178(3):640–652.e14.
Mattar, M. G. and Daw, N. D. (2018). Prioritized memory access explains planning and hippocampal replay.
Nature neuroscience, 21(11):1609–1617.
McClelland, J. L., McNaughton, B. L., and O’Reilly, R. C. (1995). Why there are complementary learning
systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models
of learning and memory. Psychological review, 102(3):419.
Niv, Y. (2019). Learning task-state representations. Nature neuroscience, 22(10):1544–1553.
Nour, M. M., Liu, Y., Arumuham, A., Kurth-Nelson, Z., and Dolan, R. J. (2021). Impaired neural replay of
inferred relationships in schizophrenia. Cell, 184(16):4315–4328.
O’Keefe, J. and Dostrovsky, J. (1971). The hippocampus as a spatial map. preliminary evidence from unit
activity in the freely-moving rat. Brain Research Volume, 34:171–175.
O’Keefe, J. and Nadel, L. (1979). Pr´ ecis of O’Keefe & Nadel’sThe hippocampus as a cognitive map. Behavioral
and Brain Sciences, 2(4):487–494.
Parr, T., Pezzulo, G., and Friston, K. J. (2022). Active Inference: The Free Energy Principle in Mind,
Brain, and Behavior . The MIT Press.
Patai, E. Z. and Spiers, H. J. (2021). The Versatile Wayfinder: Prefrontal Contributions to Spatial Naviga-
tion. Trends in Cognitive Sciences, 25(6):520–533.
Peyrache, A., Khamassi, M., Benchenane, K., Wiener, S. I., and Battaglia, F. P. (2009). Replay of rule-
learning related neural patterns in the prefrontal cortex during sleep. Nature neuroscience, 12(7):919–926.
Pezzulo, G., Donnarumma, F., Maisto, D., and Stoianov, I. (2019). Planning at decision time and in the
background during spatial navigation. Current opinion in behavioral sciences , 29:69–76.
24
Pezzulo, G., Kemere, C., and Van Der Meer, M. A. (2017). Internally generated hippocampal sequences
as a vantage point to probe future-oriented cognition. Annals of the New York Academy of Sciences ,
1396(1):144–165.
Pezzulo, G., Rigoli, F., and Friston, K. (2015). Active inference, homeostatic regulation and adaptive
behavioural control. Progress in neurobiology, 134:17–35.
Pezzulo, G., Rigoli, F., and Friston, K. J. (2018). Hierarchical active inference: a theory of motivated control.
Trends in cognitive sciences, 22(4):294–306.
Pezzulo, G., Van der Meer, M. A., Lansink, C. S., and Pennartz, C. M. (2014a). Internally generated
sequences in learning and executing goal-directed behavior. Trends in cognitive sciences, 18(12):647–657.
Pezzulo, G., Verschure, P. F., Balkenius, C., and Pennartz, C. M. (2014b). The principles of goal-directed
decision-making: from neural mechanisms to computation and robotics.
Pezzulo, G., Zorzi, M., and Corbetta, M. (2021). The secret life of predictive brains: what’s spontaneous
activity for? Trends in Cognitive Sciences, 25(9):730–743.
Pfeiffer, B. E. and Foster, D. J. (2013). Hippocampal place-cell sequences depict future paths to remembered
goals. Nature, 497(7447):74–79.
Raju, R. V., Guntupalli, J. S., Zhou, G., L´ azaro-Gredilla, M., and George, D. (2022a). Space is a latent
sequence: Structured sequence learning as a unified theory of representation in the hippocampus. arXiv
preprint arXiv:2212.01508.
Raju, R. V., Guntupalli, J. S., Zhou, G., L´ azaro-Gredilla, M., and George, D. (2022b). Space is a latent se-
quence: Structured sequence learning as a unified theory of representation in the hippocampus. Publisher:
arXiv Version Number: 1.
Recanatesi, S., Farrell, M., Lajoie, G., Deneve, S., Rigotti, M., and Shea-Brown, E. (2021). Predictive
learning as a network mechanism for extracting low-dimensional latent space representations. Nature
communications, 12(1):1417.
Rens, N., Lancia, G. L., Eluchans, M., Schwartenbeck, P., Cunnington, R., and Pezzulo, G. (2023). Evidence
for entropy maximisation in human free choice behaviour. Cognition, 232:105328.
Schmidt, B., Duin, A. A., and Redish, A. D. (2019). Disrupting the medial prefrontal cortex alters hip-
pocampal sequences during deliberative decision making. Journal of neurophysiology, 121(6):1981–2000.
Schuck, N., Cai, M., Wilson, R., and Niv, Y. (2016). Human Orbitofrontal Cortex Represents a Cognitive
Map of State Space. Neuron, 91(6):1402–1412.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., and Friston, K. J.
(2019). Computational mechanisms of curiosity and goal-directed exploration. eLife, 8:e41703.
Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Continual Learning with Deep Generative Replay.
arXiv:1705.08690 [cs].
Shin, J. D. and Jadhav, S. P. (2016). Multiple modes of hippocampal–prefrontal interactions in memory-
guided behavior. Current Opinion in Neurobiology , 40:161–169.
Siapas, A. G., Lubenov, E. V., and Wilson, M. A. (2005). Prefrontal phase locking to hippocampal theta
oscillations. Neuron, 46(1):141–151.
Simons, J. S. and Spiers, H. J. (2003). Prefrontal and medial temporal lobe interactions in long-term memory.
Nature reviews neuroscience, 4(8):637–648.
25
Smith, R., Friston, K. J., and Whyte, C. J. (2022). A step-by-step tutorial on active inference and its
application to empirical data. Journal of Mathematical Psychology , 107:102632.
Spiers, H. J. (2008). Keeping the goal in mind: Prefrontal contributions to spatial navigation. Neuropsy-
chologia, 46(7):2106–2108.
Stachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map.
Nature neuroscience, 20(11):1643–1653.
Stoianov, I., Genovesio, A., and Pezzulo, G. (2016). Prefrontal goal codes emerge as latent states in proba-
bilistic value learning. Journal of Cognitive Neuroscience , 28(1):140–157.
Stoianov, I., Maisto, D., and Pezzulo, G. (2022). The hippocampal formation as a hierarchical generative
model supporting generative replay and continual learning. Progress in Neurobiology, 217:102329.
Stoianov, I. P., Pennartz, C. M., Lansink, C. S., and Pezzulo, G. (2018). Model-based spatial navigation
in the hippocampus-ventral striatum circuit: A computational analysis. PLoS computational biology ,
14(9):e1006316.
Sun, W., Winnubst, J., Natrajan, M., Lai, C., Kajikawa, K., Michaelos, M., Gattoni, R., Fitzgerald, J. E.,
and Spruston, N. (2023). Learning produces a hippocampal cognitive map in the form of an orthogonalized
state machine. preprint, Neuroscience.
Swaminathan, S., Dedieu, A., Raju, R. V., Shanahan, M., Lazaro-Gredilla, M., and George, D. (2023).
Schema-learning and rebinding as mechanisms of in-context learning and emergence. Publisher: arXiv
Version Number: 1.
Tang, W., Shin, J. D., and Jadhav, S. P. (2021). Multiple time-scales of decision-making in the hippocampus
and prefrontal cortex. eLife, 10:e66227.
Taniguchi, T., Murata, S., Suzuki, M., Ognibene, D., Lanillos, P., Ugur, E., Jamone, L., Nakamura, T., Ciria,
A., Lara, B., and Pezzulo, G. (2023). World models and predictive coding for cognitive and developmental
robotics: frontiers and challenges. Advanced Robotics, 37(13):780–806.
Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological review, 55(4):189.
Van de Maele, T., Dhoedt, B., Verbelen, T., and Pezzulo, G. (2023). Integrating cognitive map learning and
active inference for planning in ambiguous environments. Publisher: arXiv Version Number: 1.
Verschure, P. F., Pennartz, C. M., and Pezzulo, G. (2014). The why, what, where, when and how of goal-
directed choice: neuronal and computational principles. Philosophical Transactions of the Royal Society
B: Biological Sciences, 369(1655):20130483.
Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D., and
Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience,
21(6):860–868.
Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., and Behrens, T. E. (2020).
The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the
Hippocampal Formation. Cell, 183(5):1249–1263.e23.
Whittington, J. C. R., McCaffary, D., Bakermans, J. J. W., and Behrens, T. E. J. (2022). How to build a
cognitive map. Nature Neuroscience, 25(10):1257–1272.
Wilson, M. A. and McNaughton, B. L. (1994). Reactivation of Hippocampal Ensemble Memories During
Sleep. Science, 265(5172):676–679.
26
Wittkuhn, L., Chien, S., Hall-McMaster, S., and Schuck, N. W. (2021). Replay in minds and machines.
Neuroscience & Biobehavioral Reviews, 129:367–388.
Wittkuhn, L. and Schuck, N. W. (2021). Dynamics of fmri patterns reflect sub-second activation sequences
and reveal replay in human visual cortex. Nature communications, 12(1):1795.
Yamashita, Y. and Tani, J. (2008). Emergence of functional hierarchy in a multiple timescale neural network
model: a humanoid robot experiment. PLoS computational biology, 4(11):e1000220.
Zakharov, A., Guo, Q., and Fountas, Z. (2021). Variational Predictive Routing with Nested Subjective
Timescales. Publisher: arXiv Version Number: 2.
C ¸ atal, O., Verbelen, T., Van De Maele, T., Dhoedt, B., and Safron, A. (2021). Robot navigation as
hierarchical active inference. Neural Networks, 142:192–204.
27
Supplementary materials: Bridging Cognitive Maps: a Hierarchi-
cal Active Inference Model of Spatial Alternation Tasks and the
Hippocampal-Prefrontal Circuit
A Learning the spatial level using a Hidden Markov Model
We investigate the benefit of using the CSCG’s for learning the distinct layers of the hierarchy over using
standard Hidden Markov Models on a navigation task. Specifically, we learn the spatial level using both
approaches on the same dataset: a trajectory of the agent that contains a path from each distinct pose
(position and orientation) to another. We consider the following variants, where the model is used by an
active inference agent implemented in pymdp (Heins et al., 2022): (i) a CSCG agent, as used in the main
text, (ii) a HMM agent trained using the Baum-Welch algorithm (on the same data), and (iii) a HMM fine
tuned by the Viterbi algorithm (on the same data).
The performance of each agent is measured as the success rate for reaching each corridor end from each
considered starting pose in the maze within 30 steps. Since the observations are ambiguous, and the goals
are specified in state space, the corresponding state is first extracted using the following mechanism: an
agent is started in an unambiguous position (the T-junction) and the path to a corridor end is replayed,
while the states are inferred. The resulting state is the state for which the agent has encoded this corridor
end. From the results shown in Table 2, it is clear that only the CSCG is able to properly navigate the
ambiguous W-maze.
We determine the cause for the low performance of the HMM agents to be the inability to disambiguate
the distinct corridor ends. As shown in Figure 9, this fails even when starting from unambiguous positions,
e.g. the bottom row of the maze has some unique identifiers such as the T-junction and the 2 corners. In
contrast, the CSCG encodes each corridor end in a distinct state.
Table 2: Success ratio for spatial navigation: The performance for reaching the three corridor ends as
a goal location using the different models (CSCG, HMM, HMM+Viterbi) starting from each starting pose:
16 positions × 4 directions × 3 goals = 192 trajectories.
success ratio % success
CSCG Agent 192/192 100.0 %
HMM Agent 23/192 11.9%
HMM + Viterbi Agent 32/192 16.7%
B Behavioral patterns observed in the W-maze
The different behavioral patterns from Den Bakker et al. (2022) are depicted in Figure 10, specifically for the
W-maze considered in this paper. There are four distinct patterns: (i) Alternation: the pattern of following
a rule thet is in place. (ii) Rotated alternation: the pattern of following a rule that is currently not in place.
(iii) Back and forth: iterating over two corridors, and (iv) Circling: visiting each corridor in a cyclic pattern.
In both Den Bakker et al. (2022) and our study, these are measured over sub-sequences of four consecutive
corridor visits.
C Model specification
The hyperparameters of the model are depicted in Table 3. The threshold to determine when a message
should be passed to the level above isF1
threshold, and is related to the max-valueCmax used in the construction
of the preference or C matrix. The policy length parameters determine how many steps are considered when
evaluating the expected free energy.
28
(a)
 (b)
(c)
Figure 9: State disambiguation for the W-maze : This figure displays the replayed trajectory from
unambiguous locations in the bottom row of the maze toward each of the corridor ends. The trajectories are
colored by the final inferred state, i.e. the state that encodes the end of the corridor.
(a)
Figure 10: Behavioral patterns for the W-maze: Visualization of multiple behavioral patterns measured
over 4 visits of corridors. The dotted line indicates the trajectory, and the numbers in the corridor ends
indicate the order of the visit. (i) alternation: following the rule, (ii) rotated alternation: following another
rule that is currently not in play, (iii) back-and-forth: iterating between two corridors, and (iv) circling:
cyclic evaluation of the different corridors.
29
Table 3: The hyperparameters of the hierarchical active inference model. The superscript indicates the level
for which this parameter is used (e.g. γ1 is the temperature for level 1.)
Parameter Value
γ1 0.5
γ2 0.5
F1
threshold -14.8
C1
max 15
C2
max 1
policy length1 5
policy length2 2
n clones1 20
n clones2(1 rule) 10
n clones2(3 rules) 32
(a)
 (b)
Figure 11: Sensitivity analysis for γ. (a) The mean reward acquired for different values of γ in both the
spatial and task level. We considered values from 0.5 to 20 in steps of 0.5. (b) Success rate for a hierarchical
active inference (HAI), a disrupted HAI agent, and a random agent trained on a single rule. The values are
computed over 20 trials, of 300 steps.
D Control analyses for robustness
D.1 Sampling temperature γ
We performed a sensitivity analysis on the sampling temperature ( γ parameter) for both levels, as shown
in Figure 11a. We observed that the specific value of γ does not have a large influence on performance, as
assessed by mean reward collected.
We also conducted the same simulation as in Section 2.2, but with a sampling temperature of 16. Fig-
ure 11b reports the mean success rate of visiting consecutive corridors while following the rule for an agent
with and without disruption. This figure indicates that, without disruption, the agent is able to follow the
rule and is near-perfect in collecting reward for both in- and outbound trajectories. When the disruption
is applied, (i.e. the transition model of the highest level is impaired), the agent can still solve the inbound
scenario, but drops to a chance-level performance for the outbound scenario. The higher value of γ ensures
that the agent leverages it’s generative model more (i.e. the distribution over the next action has lower
entropy) and is closer to greedily selecting the lowest expected free energy. This ensures that the agent
will not select the central corridor to visit, as the generative model can predict that this will not yield any
reward.
We note that the choice of temperature γ has a large impact on the generated behavior (Table 4). When
30
Table 4: Behavioral patterns during disruption: Classified observed behavioral patterns according
to Den Bakker et al. (2022) in different scenarios for agents trained on the three rules indicated by (3) and
a single rule indicated by (1). Agent’s policies are sampled using a sampling temperature γ of 16.
HAI HAI HAI HAI Random
control (1) disrupt (1) control (3) disrupt (3)
Alternation 80.48% 61.80% 87.06% 0.00 % 3.47%
Rotated alternation 0.00% 0.00 % 0.00% 0.00 % 8.24%
Back and forth 2.41% 26.26% 4.31% 83.77% 8.86%
Circling 0.00% 0.80 % 4.85% 0.00 % 5.86%
Other 17.11% 11.14% 3.77% 16.23% 73.75%
using a large value of γ, the agent resorts to greedy behavior (with respect to the expected free energy),
while lower values allow the agent to sample according to the distribution over actions. We observe that
with a temperature of 16, most of the generated behavior can be categorized in the back and forth category
(83.77%).
D.2 Control analysis: model capacity
We performed a robustness analysis of the model capacity (amount of parameters, length of the rules, and the
amount of rules) by evaluating the average collected reward. The experiment considers 5 trials of 900 steps
per trial, where the rule switches every 150 steps. The models are trained using the same hyperparameters
from the main text, i.e. the amount of clones from Table 3, and a sequence of 8000 corridor visits. In the
case of multiple rules as in Experiment 2 (LCLR, LCRC, and RCRL), the rule switches every 1000 visits.
In Figure 12a, the average collected reward is visualized in function of the amount of clone states, after
a warmup period of 50 steps to infer the rule. We observe that the performance remains constant when we
vary the amount of clones. We observe that for a single clone, the model does not have the capacity to learn
the rule for both models. We observe that the model is able to learn (an average collected reward of over
80%) the single rule starting at 4 states, and three rules from 8 clone states.
Next, we evaluated the impact of rule length. In the main text, all rules had a fixed length of four. We
now lengthen this rule by adding more corridors (e.g. for the single rule case this becomes: 4: LCRC, 5:
LCRCR, 6: LCRCRC, and 7: LCRCRCL). We observe a drop in performance when the rules become longer,
indicating that the model does not have enough capacity to learn the rules. Note that in rule 5, the (R)ight
corridor can be followed by a (L)eft or (C)enter corridor, while in the other rules this is always (C)enter,
which could cause this change.
Finally, we evaluated the performance of our agent when multiple rules of length four are considered.
For this experiment, we now consider a sequence of 1050 steps, where the rule switches every 150 steps. We
consider an agent with 32 clones and vary the amount of learned rules between one and eight. We observe
that the average collected reward remains similar.
D.3 Control analysis: noisy scenario
We conducted a robustness study that evaluates how robust the active inference model is in case of a noisy
environment. Consider the same W-maze where a single rule is in place, however, now whenever the agent
enters a corridor, there is a 1/3 chance that the agent’s prior beliefs (of both the navigation and prefrontal
model) are reset to a uniform distribution. This means that, after this reset, the agent needs to infer what
rule it is currently in, and where it currently is.
We compared two agents: one that uses the expected free energy functional described in the main text
(Active Inference agent) and another that only uses the utility and thus acts as a greedy agent (Utility
agent). Figure 13a shows that the average success rate (i.e. the ratio of corridor visits that yield reward) of
the two agents trained on the same rule, over 300 trials, is significantly greater for the Active Inference agent
31
(a)
(b)
(c)
Figure 12: Capacity analysis of the hierarchical CSCG. Each point is computed over 5 trials of length
900 steps, where the rule switches every 150 steps if multiple rules are in place. (a) The average collected
reward in function of the number of clones for both the one and three rule case. (b) The average collected
reward in function of rule length. (c) The average collected reward in function of the amount of learned
rules.
32
compared to the Utility agent (p-value 0.0147). This is because while the Utility agent only seeks reward
by reaching the end of corridors, the Active Inference agent seeks unambiguous locations to self-localize.
Figure 13b shows that the Active Inference agent also needs fewer steps to reach rewards, with a mean of
23.48 steps for the Active Inference agent and 26.3 steps for the Utility agent, but this difference does not
reach significance. This is probably because this count also includes the steps required to reach unambiguous
locations, which are greater for the Active Inference than for the Utility agent.
(a)
 (b)
Figure 13: Control Experiment: Robustness of the agent in a noisy scenario where the agent
state is reset with a 33.3% probability upon entering a corridor. (a) The success rate of an active
inference and a purely utility agent in visiting the corridors in the W-maze according to a fixed rule, in
the noisy environment. The active inference agent has higher accuracy in the choice of the correct corridor
according to the rule (t-test p=0.0147). The values are computed over 20 trials of 300 steps. (b) The average
amount of steps between visiting a rewarding corridor. There is no significant difference between the visit
times for both agents (t-test p=0.348) The values are computed over 20 trials of 300 steps.
33