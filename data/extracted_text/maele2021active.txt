ORIGINALRESEARCH
published:05March2021
doi:10.3389/fnbot.2021.642780
Active Vision for Robot Manipulators
Using the Free Energy Principle
ToonVandeMaele*,TimVerbelen,OzanÇatal,CedricDeBoomandBartDhoedt
IDLab,DepartmentofInformationTechnology,GhentUniversity—imec,Ghent,Belgium
Occlusions,restrictedfieldofviewandlimitedresolutionallconstrainarobot’sabilityto
senseitsenvironmentfromasingleobservation.Inthesecases,therobotfirstneedsto
activelyquerymultipleobservationsandaccumulateinformationbeforeitcancomplete
a task. In this paper, we cast this problem of active vision as active inference, which
statesthatanintelligentagentmaintainsagenerativemodelofitsenvironmentandacts
in order to minimize its surprise, or expected free energy according to this model. We
apply this to an object-reaching task for a 7-DOF robotic manipulator with an in-hand
cameratoscantheworkspace.Anovelgenerativemodelusingdeepneuralnetworksis
proposedthatisabletofusemultipleviewsintoanabstractrepresentationandistrained
fromdatabyminimizingvariationalfreeenergy.Wevalidateourapproachexperimentally
for a reaching task in simulation in which a robotic agent starts without any knowledge
aboutitsworkspace.Eachstep,thenextviewposeischosenbyevaluatingtheexpected
free energy. We find that by minimizing the expected free energy, exploratory behavior
emerges when the target object to reach is not in view, and the end effector is moved
to the correct reach position once the target is located. Similar to an owl scavenging
Editedby: for prey, the robot naturally prefers higher ground for exploring, approaching its target
DimitriOgnibene,
oncelocated.
UniversityofMilano-Bicocca,Italy
Reviewedby: Keywords:activevision,activeinference,deeplearning,generativemodeling,robotics
ThomasParr,
UniversityCollegeLondon,
1. INTRODUCTION
UnitedKingdom
YinyanZhang,
JinanUniversity,China Despite recent advances in machine learning and robotics, robot manipulation is still an
*Correspondence: open problem, especially when working with or around people, in dynamic or cluttered
ToonVandeMaele environments(BillardandKragic,2019).Oneimportantchallengefortherobotisbuildingagood
toon.vandemaele@ugent.be representationoftheworkspaceitoperatesin.Inmanycases,asinglesensoryobservationisnot
sufficienttocapturethewholeworkspace,duetorestrictedfieldofview,limitedsensorresolution
Received:16December2020 orocclusionscausedbyclutter,humanco-workers,orotherobjects.Humansontheotherhand
Accepted:03February2021 tacklethisissuebyactivelysamplingtheworldandintegratingthisinformationthroughsaccadic
Published:05March2021
eyemovements(SrihasamandBullock,2008).Moreover,theylearnarepertoireofpriorknowledge
Citation: of typical shapes and objects, allowing them to imagine “what something would look like” from
VandeMaeleT,VerbelenT,ÇatalO, a different point of view. For example, when seeing a coffee mug, we immediately reach for the
DeBoomCandDhoedtB(2021)
handle, even though the handle might not be directly in sight. Recent work suggests that active
ActiveVisionforRobotManipulators
visionandsceneconstructioninwhichanagentusesitspriorknowledgeaboutthesceneandthe
UsingtheFreeEnergyPrinciple.
Front.Neurorobot.15:642780. world can be cast as a form of active inference (Mirza et al., 2016; Conor et al., 2020), i.e., that
doi:10.3389/fnbot.2021.642780 actionsareselectedthatminimizesurprise.
FrontiersinNeurorobotics|www.frontiersin.org 1 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
Active inference is a corollary of the free energy principle, through active inference on observations of 3D coffee cups
which casts action selection as a minimization problem of with and without handles. We evaluate the interpretation of
expected free energy or surprise (Friston et al., 2016). The the uncertainty about the cup from the variance of the latent
paradigmstatesthatintelligentagentsentailagenerativemodel distributions. Finally, we consider a robotic manipulator in a
of the world they operate in (Friston, 2013). The expected simulatedworkspace.Therobotcanobserveitsworkspacebyan
free energy naturally unpacks as the sum of an information- RGBcamerathatismountedtoitsgripperandistaskedtofind
seeking (epistemic) and an utility-driven (instrumental) term, andreachanobjectintheworkspace.Inordertosolvethereach
whichmatcheshumanbehaviorofvisualsearchand“epistemic task,therobotmustfirstlocatetheobjectandthenmovetoward
foraging”(Mirzaetal.,2018).Furthermoreitisalsohypothesized it. We show that exploratory behavior emerges naturally when
that active inference might underpin the neurobiology of the therobotisequippedwithourgenerativemodelanditsactions
visualperceptionsysteminthehumanbrain(ParrandFriston, aredriventhroughactiveinference.
2017). To summarize, the contributions of this paper are
Recent work has illustrated how active vision emerges from three-fold:
activeinference in a number of simulations (Mirza etal., 2016;
• Wedevelop adeepneuralnetwork architectureandtraining
Daucé, 2018; Conor et al., 2020). However, these approaches
methodtolearnagenerativemodelfrompixeldataconsistent
typicallydefinetheagent’sgenerativemodelupfront,intermsof
with the free energy principle, based on Generative Query
small,oftendiscretestateandobservationspaces.Mostsimilaris
Networks(GQN).
the work by Matsumoto and Tani (2020), which also considers
• WeproposeanovelBayesianaggregationstrategyforGQN-
a robot manipulator that must grasp and move an object by
based generative models which leverages the probabilistic
minimizing its free energy. Their approach differs from ours
natureofthelatentdistribution.
in the sense that they use an explicitly defined state space,
• Weshowthatwecanusealearnedgenerativemodeltopartake
containing both the robot state and the object locations. In
in active inference and that natural behavior emerges, first
order to be applicable for real-world robot manipulation, the
searchingbeforeattemptingtoreachit.
generativemodelshouldworkwithrealisticsensoryobservations
suchascamerainputs.Therefore,inthispaper,weexploretheuse This paper is structured as follows: the proposed method is
of deep neural networks to learn expressive generative models, explainedinsection2,wherethegenerativemodel(section2.1)
and evaluate to what extent these can drive active vision using andtheactiveinferenceframework(section2.2)areintroduced
the principles from active inference. We consider the active first. Section 2.3.1 then explains how the approximation of
vision problem of finding and reaching a certain object in a the expected free energy can be achieved using the learned
roboticworkspace. distributions. Section 2.3.2 finally elaborates on how these
While a lot of research on learning generative models of distributions are learned using deep neural networks through
the environment has been performed, most of them only pixel-based data. Section 3 considers the results from applying
consider individual objects (Sitzmann et al., 2019b; Häni et al., the proposed method on numerous scenes of increasing
2020),considersceneswithafixedcameraviewpoint(Kosiorek complexity. First, the proposed model architecture is evaluated
et al., 2018; Kulkarni et al., 2019; Lin et al., 2020) or train on a subset of the ShapeNet dataset (section 3.1). Next, the
a separate neural network for each novel scene (Mildenhall learneddistributionsareevaluatedonwhethertheycanbeused
et al., 2020; Sitzmann et al., 2020). We tackle the problem within the active inference framework on the use case three
of an active agent that can control the extrinsic parameters dimensional cup (section 3.2). Finally the robot manipulator
of an RGB camera as an active vision system. Both camera in simulation is used for the reaching problem (section 3.3).
viewpoint and its RGB observation are therefore available for A discussion on the results, related work and possible future
ourapproach.Toleveragetheavailableinformation,ourlearned prospects is provided in section 4. A conclusion is provided in
generative model is based on the Generative Query Network section5.
(GQN) (Eslami et al., 2018). This is a variational auto-encoder
thatlearnsalatentspacedistributiontoencodetheappearance
oftheenvironmentthroughmultipleobservationsfromvarious 2. METHOD
viewpoints.Whereas,Eslamietal.(2018)integratesinformation
of these different viewpoints by simply adding feature vectors, In this section we first discuss how the artificial agent interacts
weshowthatthisdoesnotscalewellformanyobservations,and with the world through a Markov blanket, and that its internal
proposeanovelBayesianaggregationscheme.Theapproximate generativemodelcanbedescribedbyaBayesiannetwork.Next,
posterior is computed through Gaussian multiplication and we further unpack the generative model and describe how the
resultsinavariancethatproperlyencodesuncertainty. internal belief over the state is updated. In the second section
We evaluate our approach on three specific scenarios. First, the theoretical framework of active vision and how this relates
wevalidateourgenerativemodelandBayesianlatentaggregation to an agent choosing its actions is elaborated on. Finally, we
strategy on plane models of the ShapeNet v2 dataset (Chang show how a learned generative model can be used to compute
et al., 2015). In addition, we provide an ablation study on the the expected free energy to drive the action-perception system
differentaspectsofourmodelarchitectureandcomparedifferent known as active inference. We also go into the details of the
aggregation methods. Second, we evaluate action selection neural network architecture and how it is learned exclusively
FrontiersinNeurorobotics|www.frontiersin.org 2 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
from element 0 until the ith element. The generative model is
factorizedas:
i
P(o 0:i ,s,v 0:i )=P(s) P(o k |v k ,s)P(v k ) (1)
kY =1
As the artificial agent can only interact with the world through
its Markov blanket, the agent has to infer the posterior belief
P(s|o 0:i ,v 0:i ). For high dimensional state spaces, computing
this probability becomes intractable and approximate inference
FIGURE1|Theinternalgenerativemodeloftheartificialagentisrepresented methods are used (Beal, 2003). The approximate posterior Q is
asaBayesiannetwork.Theenvironmentisconsideredunchangingandis introduced, which is to be optimized to approximate the true
describedbyalatentvariables.Theobservationsokdependonboththe
posteriordistribution.Theapproximateposteriorisfactorizedas:
environmentdescribedbysandtheagent’sviewpointdenotedbyvk.Thefirst
iviewpointshavebeenvisitedandareusedtoinferabeliefoverthejoint
distribution.Futureviewpointvi+1hasnotbeenvisitedorobservedyet.
i
Observedvariablesareshowninblue,whileunobservedvariablesareshown
inwhite. Q(s|o 0:i ,v 0:i )= Q(s|o k ,v k ), (2)
kY =0
This approximate posterior corresponds to the internal model
that the agent uses to reason about the world. In the next
from pixel-based observations by minimizing the variational
section, we will discuss how variational methods can be used
freeenergy.
to optimize the approximate posterior by minimizing the
variationalfreeenergy.
2.1. The Generative Model
2.2. The Free Energy Principle
We model the agent as separated from the true world state
throughaMarkovblanket,whichmeansthattheagentcanonly According to the free energy principle, agents minimize their
update its internal belief about the world by interacting with variational free energy (Friston, 2010). This quantity describes
the world through its chosen actions and its observed sensory the difference between the approximate posterior and the true
information (Friston et al., 2016). In the case of active vision, distribution or equivalently, the surprise. The free energy F for
the actions the agent can perform consist of moving toward a thegraphicalmodeldescribedinFigure1canbeformalizedas:
new viewpoint to observe its environment. We thus define the
action space as the set of potential viewpoints the agent can
move to. The sensory inputs of the agents in this paper are a F=E Q(s|o0:i,v0:i) [logQ(s|o 0:i ,v 0:i )−logP(o 0:i ,s,v 0:i )]
simple RGB camera and the observations are therefore pixel- =−logP(o 0:i ,v 0:i )+D KL [Q(s|o 0:i ,v 0:i )||P(s|o 0:i ,v 0:i )]
based.Inthispaper,welimitourselvestoanagentobservingand
Evidence Approximatevstrueposterior
reachingtowardobjectsintheenvironment,butnotinteracting
with them. Hence, we assume the environment is static and its
=E Q|(s|o0:i,v{
0
z:i) [−lo}gP(|o 0:i |v 0:i ,s)] {z }
dynamicsshouldnotbemodeledinourgenerativemodelaswe Accuracy
do not expect an object on the table to suddenly change color, |+D KL [Q(s|o 0:{i ,zv 0:i )||P(s)] }
shape, or move around without external interaction. However,
one might extend the generative model depicted here to also Complexity
includedynamics,similartoÇataletal.(2020). | {z i }
Moreformally,weconsiderthegenerativemodeltotakethe =E Q(s|o0:i,v0:i) − logP(o k |v k ,s)
" #
shape of a Bayesian network (Figure1) in which the agent can Xk=0
notobservetheworldstatedirectly,buthastoinferaninternal +D KL [Q(s|o 0:i ,v 0:i )||P(s)] (3)
beliefthroughsensoryobservationso andchosenviewpointsv .
k k
Theenvironmentorworldwhichcanbeobservedfromdifferent ThisformalizationcanbeunpackedasthesumoftheKullback-
viewpointsisdescribedbythelatentfactors.Whenaviewpoint Leibler divergence between the approximate posterior and the
v isvisited,anobservationo isacquiredwhichdependsonthe truebeliefovers,andtheexpectednegativeloglikelihoodover
k k
chosen viewpoint and environment state s. The agent uses the the observed views o 0:i given their viewpoints v 0:i . It is clear
sequenceofobservationstoinferabeliefabouttheworldthrough thatifbothdistributionsareequal,theKL-termwillevaluateto
thelatentdistributions. zeroandthevariationalfreeenergyF equalstheloglikelihood.
The generative model describes a factorization of the joint Minimizingthefreeenergythereforemaximizestheevidence.
probabilityP(o 0:i ,s,v 0:i )overasequenceofobservationso 0:i , We can further interpret Equation (3) as an accuracy term,
states s and viewpoints v 0:i . In the remainder of this paper, encouraging better predictions for an observation o k given a
the colon notation 0:i is used to represent a sequence going viewpointv andthestates,andacomplexitytermpromoting
k
FrontiersinNeurorobotics|www.frontiersin.org 3 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
“simpler”explanations,i.e.,closertothepriorbeliefovers.The its environment. The instrumental value represents the prior
approximateposteriorcanthenbeacquiredby: likelihood of the future observation. This can be interpreted as
agoalthattheagentwantstoachieve.Forexampleinareaching
task,theagentexpectstoseethetargetobjectinitsobservation.
Q(s|o 0:i ,v 0:i )= argmin F≈P(s|o 0:i ,v 0:i ), (4)
Q(s|o0:i,v0:i) 2.3. Active Vision and Deep Neural
However,theagentdoesnotonlywanttominimizeitssurprise Networks
forpastobservations,butalsoforthefuture.Minimizingthefree To apply active inference in practice, a generative model
energywithrespecttothefutureviewpointswilldrivetheagentto that describes the relation between different variables in the
observethesceneinordertofurthermaximizeitsevidence,and environment, i.e., actions, observations, and the global state, is
canthereforebeusedasanaturalapproachtoexploration.The required.Whenusingthisparadigmforcomplextasks,suchas
nextviewpointstovisitcanhencebeselectedbyevaluatingtheir reachinganobjectwitharobotmanipulator,itisoftendifficult
freeenergy.However,itisimpossibletocomputethisfreeenergy, to define the distributions over these variables upfront. In this
as observations from the future are not yet available. Instead, paper, we learn the mapping of observations and viewpoints
similartoConoretal.(2020),theexpectedfreeenergyGcanbe to a posterior belief directly from data using deep neural
computedforthenextviewpointv i+1 .Thisquantityisdefinedas networks. We model the approximate posterior Q(s|o 0:i ,v 0:i )
thefreeenergyexpectedtoencounterinthefuturewhenmoving and likelihood P(o |s,v ) as separate neural networks that
k k
to a potential viewpoint v i+1 . The probability distribution over are optimized simultaneously, similar to the variational auto-
theconsideredfutureviewpointscanbecomputedwithrespect encoder approach (Kingma and Welling, 2014; Rezende et al.,
toGas: 2014).
TheapproximateposteriorQ(s|o 0:i ,v 0:i )ismodeledthrough
afactorizationoftheposteriorsaftereachobservation.Thebelief
P(v i+1 )=σ(−G(v i+1 )), (5) overscanthenbeacquiredbymultiplyingtheposteriorbeliefs
oversforeveryobservation.Welearnanencoderneuralnetwork
WhereG(v i+1 )istheexpectedfreeenergyforthefuturevisited
viewpoint, σ is the softmax operation which transforms the
withparametersφtolearntheposteriorqφ(s|o
k
,v
k
)oversgiven
a single observation and viewpoint pair (o ,s ). The posterior
expectedfreeenergyGforeveryconsideredviewpointv i+1 intoa k k
distributions over s given each observation and viewpoint pair
categoricaldistributionovertheseviewpoints.Theexpectedfree
arecombinedthroughaGaussianmultiplication.Weacquirethe
energyisthenobtainedbycomputingthefreeenergyforfuture
posterior distribution as a Normal distribution proportional to
viewpointv i+1 :
theproductoftheposteriors:
G(vi+1) i
=E Q(s,oi+1|o0:i,v0:i+1)[logQ(s|o0:i,v0:i+1)−logP(o0:i+1,s|v0:i+1)] Qφ(s|o 0:i ,v 0:i )∝ qφ(s|o k ,v k ). (7)
=E Q(s,oi+1|o0:i,v0:i+1)[logQ(s|o0:i,v0:i+1)−logP(s|o0:i+1,v0:i+1) kY =0
−logP(o0:i+1|v0:i+1)] Secondly, we create a neural network with parameters ψ that
≈−E Q(oi+1|o0:i,v0:i+1) DKL[Q(s|o0:i+1,v0:i+1)||Q(s|o0:i,v0:i)] estimatesthepixelvaluesofanobservationoˆ k ,giventheselected
viewpoint v and a state vector s. The likelihood over the
(cid:2) Epistemicvalue (cid:3) k
observation pψ(oˆ
k
|v
k
,s) is modeled as an image where every
−E| Q(oi+1|o0:i,v0:i+1)[logP(o0:i+ {z 1)] } (6) pixelisanindependentGaussiandistributionwiththepixelvalue
beingthemeanandafixedvariance.
Instrumentalvalue
We jointly train these models using a dataset of tuples
| {z }
{(o ,v )}k=i for a number of environments by minimizing the
k k k=0
freeenergylossfunction:
Thisexpectedfreeenergycanbereformulatedasthesumofan
instrumental and anepistemicterm. Theepistemicvalueisthe
KL-divergencebetweentheposteriorbeliefoversafterobserving
i
the future viewpoint, and before visiting this viewpoint. As the L= ||oˆ k −o k || 2 +D KL [Qφ(s|o 0:i ,v 0:i )||N(0,I)] (8)
trueposteriorisnotavailable,weapproximateP(s|o 0:i+1 ,v 0:i+1 ) Xk=0
usingtheapproximateposteriordistributionQ(s|o 0:i+1 ,v 0:i+1 ).
Pleasenotethatinthefinalstep,theconditionontheviewpoints This loss function is reformulated as a trade-off between
in the instrumental value can be omitted. Which can be a reconstruction term and a regularization term. The
interpreted as an intelligent agent creating a preferred prior in reconstruction term computes the summed mean squared
advancethatisnotdependentonthecorrespondingviewpoints. errorbetweenthereconstructedobservationsoˆ 0:i andground-
Intuitively, this KL-term represents how much the posterior truthobservationso 0:i .Thistermcorrespondswiththeaccuracy
belief over s will change given the new observation. An agent termofEquation(3),asminimizationofthemeansquarederror
that minimizes free energy will thus prefer viewpoints that isequivalenttominimizingloglikelihoodwhenthelikelihoodis
change the belief over s, or equivalently, to learn more about aGaussiandistributionwithafixedvariance.Theregularization
FrontiersinNeurorobotics|www.frontiersin.org 4 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
term is identical to the complexity term of Equation (3) and 2.3.2.ModelandTrainingDetails
computestheKL-divergencebetweenthebeliefoverthestates Bothneuralnetworksaredirectlyoptimizedend-to-endthrough
andachosenprior,whichwechoosetobeanisotropicGaussian pixel data, using a dataset consisting of different scenes. We
withunitvariance. define a scene as a static environment or object in or around
whichtheagent’scameracanmovetodifferentviewpoints.The
agenthasobservedthesetofiobservationsandviewpointsfrom
asceneS = {(o ,v )}k=i.Theviewo isanRGBimagescaled
k k k=0 k
down to a resolution of 64 × 64 pixels and the viewpoint v is
k
2.3.1.ApproximatingtheExpectedFreeEnergyfor representedbyasevendimensionalvectorthatconsistsofboth
ActiveVision thepositioncoordinatesandtheorientationquaternion.
Underactiveinference,theagentchoosesthenextviewpointto The generative model we consider belongs to the family of
visit in order to minimize its expected free energy as described variationalauto-encoders(KingmaandWelling,2014;Rezende
insection2.2.Theagentselectstheviewpointbysamplingfrom et al., 2014). It most resembles the Generative Query Network
thecategoricaldistributionP(v i+1 ).AsdescribedbyEquation(5), (GQN) (Eslami et al., 2018). This variational auto-encoder
this categorical distribution is acquired by computing the variantencodesinformationforeachobservationseparatelyand
expected free energy G for every potential viewpoint v i+1 , and aggregates the acquired latent codes. Similarly to the GQN,
applying the softmax function on the output. The expected ourencodergeneratesalatentdistributionforeachobservation
free energy is computed by separately evaluating the epistemic separately and combines them to form the current scene
and instrumental term from Equation 6. Calculating these representation.Fromthisscenerepresentation,thedecoderhas
expectationsforeverypossibleviewpointisintractable,hencewe torendertheexpectedobservationsgivenatargetviewpoint.
resorttoMonteCarlomethodstoapproximatetheexpectedfree WedeviatefromtheGQNpresentedbyEslamietal.(2018)
energythroughsampling. in two ways. First, whereas GQNs concatenate the viewpoint
A schematic overview of our method is shown in Figure2. parameterssomewhereintheencoderanduseanauto-regressive
For a target viewpoint v i+1 , the epistemic term is the expected decoder architecture, we use convolutional neural networks for
value of the KL divergence between the belief over state s bothencodinganddecoding,anduseFiLMlayers(Perezetal.,
after observing o i+1 (i.e., Q(s|o 0:i+1 ,v 0:i+1 )) and prior to 2018) for conditioning. The encoder is conditioned on the
observing o i+1 (i.e., Q(s|v 0:i ,o 0:i )). The latter distribution is viewpoint parameters and the decoder is conditioned on both
the output after feeding all previous observations o 0:i and the query viewpoint v i+1 and the scene representation vector.
theircorrespondingviewpointsv 0:i throughtheneuralnetwork Secondly,whereasGQNsaggregatetheextractedrepresentations
qφ(s|o 0:i ,v 0:i ).ThisisshownontheleftofFigure2andprovides fromtheencoderbymereaddition,weuseaBayesianinspired
theagentwiththecurrentbeliefovers.Toestimatetheposterior aggregation scheme. We consider the distributions from the
distribution Q(s|o 0:i+1 ,v 0:i+1 ), an imagined observation oˆ i+1 model described in section 2.1. Instead of the addition used in
must be sampled. The likelihood model is used to do this, theGQN,weuseafactorizationoftheposteriorQ(s|v 0:i ,o 0:i )
conditioned on the potential viewpoint v i+1 and a sampled to aggregate the acquired representations through Gaussian
state vector from Q(s|o 0:i ,v 0:i ), an estimate of the observed multiplication. When a new observation o i is available, the
view oˆ is made. Together with the initial observations o 0:i current belief distribution N(µ cur ,σ2 cur I) is updated with the
and viewpoints v 0:i , the imagined view is encoded through outputoftheencodernetworkqφ(o i |v i ),aNormaldistribution
the posterior model to approximate Q(s|o 0:i+1 ,v 0:i+1 ) as N(µ obs ,σ2 obs I),usingGaussianmultiplication:
shown on the right of Figure2. As both prior and posterior
distributions are approximated by a Multivariate Gaussian
σ2 ·µ +σ2 ·µ
with a diagonal covariance matrix, the KL divergence can be µ= cur obs obs cur , (9)
computedanalytically.Toapproximatetheexpectedvalueover σ2 +σ2
cur obs
Q(s|o 0:i ,v 0:i ),werepeatthisprocessformultiplestatesamples
andaveragetheobtainedvalues.
The instrumental term, as described in Equation 6, is the 1 1 1
= + (10)
expected negative log likelihood of the observed view o i+1 for σ2 σ2 cur σ2 obs
the future viewpoint v i+1 . Again, we approximate this value
by sampling from the state distribution, and forwarding this This way of refining belief of the acquired representations is
through the likelihood model. We calculate the negative log equivalenttotheupdatestepfoundinBayesianfilteringsystems
likelihood of each imagined observation oˆ i+1 according to a such as the Kalman filter (Kalman, 1960). As the variance in
priordistributionoverthisobservation.Thisprocessisrepeated each dimension reflects the spread over that state vector, it
for numerous samples from Q(s|o 0:i ,v 0:i ), and the computed can be interpreted as the confidence of the model. The belief
log likelihood is averaged to calculate the instrumental term. over the state is therefore updated based on their uncertainty
In the case of a robotic reaching task, this prior distribution in each dimension. Additionally, using this type of aggregation
takestheformofadesiredgoalobservation,andcomputinglog hasthebenefitthattheoperationismagnitude-preserving.This
likelihoodreducestocomputingthemeansquarederrorbetween results in a robust system that is invariant to the amount of
animaginedobservationoˆ i+1 andareferencegoalobservation. received observations, unlike an addition-based system. For
FrontiersinNeurorobotics|www.frontiersin.org 5 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
FIGURE2|Theflowfollowedwhenevaluatingtheexpectedfreeenergyusingdeepneuralnetworksforagivenpotentialnewviewpointvi+1.Startingontheleftof
thefigure,theencoderneuralnetworkthatapproximatestheposteriorqφ(s|o0:i,v0:i)encodestheobservationso0:iandcorrespondingviewpointsv0:iuntilnowinto
abeliefoverthestates.Fromthisbelief,astatevectorissampledandisusedtogetherwithviewpointvi+1topredicttheimaginedviewfromthisviewpoint.The
instrumentalvalueiscomputedastheloglikelihoodthatthetargetimageisgeneratedfromthedistributionoverthepredictedimage.Thisismarkedbytheredarrow.
Thisimaginedviewoˆ i+1ispassedthroughtheapproximateposteriormodeltoacquiretheexpectedbeliefoversafterselectingviewpointvi+1.Theepistemicvalueis
computedastheKLdivergencebetweentheapproximateposteriorbeforeobservingtheimaginedviewoˆ i+1andafter.Thisismarkedbythebluearrow.Finally,the
expectedfreeenergyisapproximatedbyaveragingoveranumberofsamples.
stabilityreasons,weclipthevarianceoftheresultingdistribution 3. RESULTS
toavalueof0.25.
We parameterize our model as follows. The inputs are Three experiments were designed to evaluate both our model
first expanded by using a 1 × 1 convolution that maps andtheproposedactivevisionsystem.Inafirstexperiment,we
the RGB channels to a higher dimensional space of 64 considerasubsetoftheShapeNetdataset(Changetal.,2015)to
channels. The encoder consists of four convolutional layers evaluate model performance. We conduct an ablation study on
with a stride of 2, a kernel size of 3 × 3 and feature maps differentaggregationmethodsforthestateencodingsproduced
that increase with a factor 2 every layer (16, 32, 64, 128). by the generative model. We show that our model exhibits
They are interleaved with FiLM layers (Perez et al., 2018) performancesimilartootheraggregationstrategies,whilebeing
that learn a transform for the extracted features based on more resistant to the number of observations and better
the viewpoint pose. The extracted feature representation is leveragingtheBayesiancharacteroftheextracteddistributions.
then transformed in two feature vectors that represent the In a second experiment, we consider scenes consisting of a
mean and variance of the latent state s. In each considered 3D coffee cup that potentially has a handle. We investigate the
experiment this latent size is different. The decoder mirrors learnedapproximateposteriordistributionanditsbehaviorwhen
thisarchitecturewithfourconvolutionblocks,eachconvolution observingdifferentviews.Weanalyzethebehaviorthatemerges
blockfirstappliesaconvolutionthathalvestheamountoffeature inourartificialagentwhendrivingviewpointsselectionusingthe
maps, after which a convolution is applied which preserves epistemic term. In the final experiment, we consider a realistic
the amount of feature channels (128, 128, 64, 64, 32, 32). roboticworkspaceinCoppeliaSim(Rohmeretal.,2013).Scenes
Here, the FiLM layers are conditioned on the concatenated arecreatedwithanarbitraryamountofrandomtoyobjectswith
latent code and query pose. Between every convolution block randomcolors.Ataskisdesignedinwhichtherobotmanipulator
in the decoder, the image is linearly upsampled. LeakyReLU must find and reach a target object. First, we investigate the
activationsareusedaftereveryconvolutionallayer.Theoutput exploratory behavior when no preferred state is provided and
of the decoder is finally processed using a 1x1 convolution see that the agent explores the workspace. We then provide
that maps the 64 channels back to RGB channels. For the theagentwithagoalbyspecifyingapreferredobservationand
specifics of the neural network, the reader is referred to computingthefullvalueofG.Weobservethattheagentexplores
SupplementaryMaterial. theworkspaceuntilithasfoundandreacheditstarget.
This model is optimized end-to-end by minimizing the
free energy loss with respect to the model parameters, 3.1. ShapeNet
as described in Equation (8) using Adam (Kingma and
Inthefirstexperimentwewanttoevaluatetheproposedneural
Ba, 2015), a gradient-based optimizer. Additionally, we
networkarchitectureonasubsetoftheShapeNetdataset(Chang
use the constraint-based GECO algorithm (Rezende
et al., 2015). We focus on whether the neural architecture is
and Viola, 2018) that balances the reconstruction and
capable of learning to implicitly encode the three dimensional
regularization term by optimizing Lagrangian multipliers
structure of a scene from purely pixel-based observations by
usingamin-maxscheme.
minimization of the free energy loss function. Additionally,
FrontiersinNeurorobotics|www.frontiersin.org 6 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
we want to validate our novel aggregation strategy which TABLE1|AverageMSEoverallobjectsintheselectedtestsetofShapeNet
uses a factorization of the approximate posterior to combine planesdata.
the extracted representations for all observations. The novel
Model #param MSE(10obs) MSE(30obs) MSE(60obs)
aggregation method ensures that the resulting distribution will
alwaysbeinthesameorderofmagnitude,independentlyofthe GQN 49.5M 0.0143±0.0110 0.0354±0.0228 0.0438±0.0275
numberofobservations,incontrasttotheadditionmethodfrom Ours 3.6M 0.0151±0.0138 0.0148±0.0133 0.0147±0.0133
theoriginalworkbyEslamietal.(2018).Weexpecttoseethatour Addition 3.6M 0.0169±0.0122 0.1222±0.1102 0.2409±0.1599
approachoutperformstheGQNbaselinewhenprovidedwitha ablation
largeamountofobservations. Max-pool 3.6M 0.0175±0.0112 0.0170±0.0110 0.0176±0.0101
Toseparatetheinfluenceoftheoverallnetworkarchitecture ablation
from the used aggregation method to combine extracted latent Mean-pool 3.6M 0.0182±0.0110 0.0175±0.0103 0.0175±0.0094
distributions from all separate observations into a belief over ablation
thestates,weperformanablationstudy.Besidestheproposed
TheboldvalueindicatesthelowestMSEforeverycolumn.
approach, we also introduce three variants to combine latent
distributions,whileusingthesameencoder-decoderarchitecture
with a latent size of 64 dimensions. We compare our approach level as well after 60 observations, and is even able to improve
to the addition method from the original GQN paper (Eslami its reconstruction. Both the max-pool and the mean-pool
etal.,2018),ameanoperation(Garneloetal.,2018),oramax- ablation are less affected after 60 observations, but the overall
pooling (Su et al., 2015) operation. As these ablations do not reconstructionsarelessdetailed.
propose a method to integrate the variance of the individual
reconstructions, the variance of the new observation is set to a 3.2. The Cup
fixedvalueof1foreverydimension.Wealsocomparetheresults In active inference, viewpoints are selected by minimizing the
withtheoriginalGQNarchitecture. agent’s expected free energy. It is essential that the predicted
All models in this experiment are trained on the same data distributions through our learned generative model are well-
using the free energy loss function from Equation (8). The behaved and thus are able to properly represent ambiguity
observations are RGB images with a resolution of 64 × 64. when it has no, or incomplete, information about the scene. In
The viewpoints are a 7-dimensional vector, that correspond to this experiment, we evaluate the distributions produced by the
the position in Euclidean coordinates and the orientation in learned generative model and analyze whether they are able to
quaternionrepresentation.Themodelisoptimizedend-to-endas capture the ambiguity provided by the scene. We expect to see
describedinsection2.3.2.Abatchsizeof100sequencespermini- dubiety in both the reconstructed imagined views of the cup,
batchisused.SimilartotheapproachusedbytheGQN,between as well as in the variance of the produced distributions. We
3and10observationsarerandomlyprovidedduringtrainingto also investigate the behavior that emerges when viewpoints are
enforce independence on the amount of observed data. These selected by minimizing the epistemic term of the expected free
models are then trained until convergence. The GQN baseline energyandexpectexploratorybehaviortosurface.
isoptimizedusingthetraditionalELBOlossasdescribedinthe We consider simple scenes that consist of a 3D model of a
originalpaperbyEslamietal.(2018). coffeecupthatcanvaryinsizeandorientation.Itcanpotentially
Table1showstheaveragemeansquarederror(MSE)ofnovel be equipped with a handle. For each created scene, 50 views of
viewsgeneratedforallobjectsinthetestsetforavaryingnumber 64 × 64 pixels are randomly sampled from viewpoints around
of observations. We observe that our model outperforms the the object. A dataset of 2,000 different scenes containing a cup
others for 30 and 60 observations, whereas GQN has similar were created in Blender (Blender Online Community, 2018),
performanceon10observations.Alsonotethatourmodelhasan of which half are equipped with a handle. One thousand eight
orderofmagnitudefewerparametersthantheGQNmodel.From hundredofthesesceneswereusedtotrainthegenerativemodel.
theablationstudy,wecanindeednotethattheGQNsuffersfrom Theparametersoftheneuralnetworkareoptimizedinadvance
theadditionaggregationmethod.Max-poolingseemstoperform usingthisprerecordeddatasetbyminimizingthefreeenergyover
betterwithmorethantenobservations,buthasanoverallhigher the acquired observations as explained in section 2.3. For each
MSEcomparedtoourapproach.Thesameistrueforthemean- scene,between3and5imagesareprovidedtothemodelduring
poolablation,whichimprovesasmoreobservationsareadded. training.Themodelforthisexperimentisthesameasdescribed
Thisimprovementcanbeattributedtothereductionofnoiseon insection2.3,butwithalatentdimensionsizeof9.Thefollowing
therepresentationvectorbyhavingmoreobservations. experimentswereconductedonscenesofcupsinthevalidation
Examples of the reconstructions generated from the setthatwerenotseenduringtraining.
aggregatedlatentspaceareshowninFigure3.ClearlytheGQN To evaluate whether the generative model is able to capture
achieves the best performance when operating in the trained the ambiguity of a cup when not all information is gathered
range, but when more observations are added the quality of throughobservationsyet,weconsidertwonearlyidenticalcups,
the decoded image decays rapidly and the object is no longer both positioned in the same orientation and scaled with the
recognizable.Thesamebehaviorcanbenoticedfortheaddition samefactor.Theonlydifferencebetweenthesecupsisthatone
ablation. Our model yields comparable reconstructions as the has a handle, while the other one does not. We provide our
GQN for 10 observations, but achieves to uphold this quality learnedmodelwithasingleobservationthatdoesnotresolvethe
FrontiersinNeurorobotics|www.frontiersin.org 7 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
FIGURE3|EvolutionofimaginedobservationsfordifferentmodelsonanunseenShapeNetexamplewhen10,30,or60observationsareprovided.OurObserver
modelmaintainsagoodreconstructionqualityevenifmoreobservationsareconsideredthanduringtraining.
FIGURE4|Generatedobservationsareshowninthisfigure.(A)Anambiguousobservationisprovidedtothegenerativemodel,andthisisreflectedinambiguous
reconstructionsafterobservingthecupfromtheotherside.(B)Themodelisprovidedwithanadditionalunambiguousobservationofahandle.(C)Themodelis
providedwithanadditionalobservationofthecupfromtheothersidewhichdoesnotcontainahandle.
ambiguityaboutthelocationanddoesnotrevealthepresenceof When a new observation from a different viewpoint around
ahandle.Wenowusethelikelihoodmodelovertheobservation the cup is added to the model, the ambiguity can be noticed
o k+1 to generate the expected observation, which is shown in to clearly drop. Figure4B shows the reconstructed cups in
Figure4A.Whenlookingatthesegeneratedcups,itshowsboth case the handle is observed. These reconstructions are sharp
cups with and without handle, with the handle at a random and draw the handle consistently at the same position. This
position.Thiscanbeattributedtothefactthattheorientationof consistency is also reflected by the lower variance of its latent
thecupisnotknown,andthemodelthereforedoesnotknowat distributionshowninFigure5B.Thesameobservationwithout
whatpositiontodrawahandle.Thisambiguityisalsoreflected a handle was provided as a second observation for the cup
in the high variance shown in the extracted latent distribution without a handle. The generated cups of this scene are shown
(Figure5A). in Figure4C. In Figure5C, a lower variance compared to the
FrontiersinNeurorobotics|www.frontiersin.org 8 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
FIGURE5|Thisfigureisarepresentationofthedistributionsovertheapproximateposteriorwithalatentstateofninedimensions.Astheapproximateposterioris
representedbyamultivariateGaussiandistributionwithadiagonalcovariancematrix,eachdimensioncanbeconsideredindependently.(A)Thedistributionafteran
ambiguousobservationisrepresented.(B)Theapproximatedistributionforanunambiguousobservationsofahandleisrepresented.Thevarianceisonaverage9.08
timeslowerineachdimensionthanpriortoobservationin(A).(C)Thesameunambiguousobservationswithoutahandleisprovided.Inthiscase,thevarianceison
average2.83timessmallerineachdimensionthanin(A).
FIGURE6|Theepistemicvaluesareshownforallpotentialviewpoints.ThisvalueiscomputedastheKLdivergencebetweenthebeliefoverlatentstateswiththe
observationsmarkedbyaredobserverandtheexpectedposteriorbeliefwhenchoosingthenextviewpointvi+1.Theepistemicvaluesarenormalizedbythesoftmax
operationwithatemperatureof1,asdescribedinsection2.Theredcolormarkstheobservationviewpointthathasbeenobserved.Thecolorofthecamera
representsthelikelihoodthatthisviewpointisselectednext.(A–C)shownforthreescenarioswithadifferentinitialviewpointin(A)through(C).
oneshowninFigure5Acanagainbenoticed.Wethusconclude are placed on the table within reach of the manipulator. These
thatoptimizingthegenerativemodelthroughaminimizationof objects are randomly chosen and can take the shape of a cube,
expectedfreeenergyresultsinwell-behavedlatentdistributions. a sphere, a cylinder or a bar that could either be standing
Additionally,wewanttoevaluatewhetherusingtheexpected up or laying down. These objects have a Lambertian surface
free energy as a viewpoint selection policy is a valid approach with a uniform color. An example of such a scene is shown in
for active vision. We hypothesize that if the agent observes the Figure7. The agent is able to manipulate the extrinsic camera
cup from one viewpoint, it will prefer policies that move the parametersthroughroboticactuationofthegripper.Itcanthen
agent to observe the cup from the other side, to gain as much observedifferentareasoftheworkspace.Similartotheprevious
informationaspossibleintheleastamountofobservations.The experiment,wefirstlearntheneuralnetworkparametersfroma
potentialviewpointsareuniformlyspacedinacirclearoundthe prerecorded dataset, which is then used in the proposed active
cup at a fixed height, and with an orientation toward the cup. visionschemeforviewpointselection.Themodelarchitectureis
Figure6 shows the probability distribution over the potential identical to the one in the previous experiments, but with 256
viewpoints P(v i+1 ) for three different initial observations. It is latentstatespacedimensions.
clearthatingeneral,theagentwillchooseaviewpointfaraway In order to learn the model parameters, a prerecorded
fromthecurrentobservationtomaximizetheinformationgain datasetwascreatedusingthesameenvironmentinCoppeliaSim.
withrespecttothecup. Up to five randomly selected toy objects are spawned in the
workspace. The orientation and position of the objects within
3.3. Robot Manipulator the workspace are determined randomly by sampling from a
In the final experiment, a robotic environment in uniform distribution. A dataset of 8,000 such scenes is created,
CoppeliaSim(Rohmeretal.,2013)isconsidered.Theworkspace inwhichtherobotend-effectorismovedalongatrajectorythat
is equipped with a robot manipulator on a fixed table, which coverstheentireworkspaceatdifferentheights.Weconstrainthe
has an RGB camera mounted to its gripper. Some toy objects end-effectortolookinadownwardsorientation.Thisfacilitates
FrontiersinNeurorobotics|www.frontiersin.org 9 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
forthepotentialviewpoints,showninFigure8B.Theepistemic
value is computed for all potential viewpoints, and is shown
in Figure8A. The largest epistemic values are located in the
centerofthetable,astheagentbelievesthatobservationsfrom
these locations will allow it to learn more. After moving to the
viewpointwiththehighestepistemicvalue,theagentobservesthe
yellowcubeandtheredball.Thegenerativemodelisthenable
toreconstructtheseobjectscorrectlyatthepotentialviewpoints,
which can be observed in the second plot of Figure8B. We
notice, that after observing these objects, the agent still prefers
to look at these positions for a number of steps. The internal
model of the environment is still being updated, which we can
FIGURE7|AnexamplesceneoftheroboticworkspaceinCoppeliaSim. seeinthesharperreconstructionsinthefirstandsecondrowof
Threerandomobjectsarespawnedatarbitrarypositionsandrotations.This Figure8. This can be attributed to the aggregation strategy for
sceneisusedfortheexperimentsinsection3.3.
the approximate posterior. A single observation of the objects
willnottransformthedistributionentirely,butaweightedmean
and variance is computed. This results in a slower process for
the training process and does not limit performance on this updating the state distribution, and it can result in the agent
use case, as the robot is still able to observe all objects placed trying to observe the same area for a number of steps. Similar
on the workspace from a top view. During training, these totheexperimentinsection3.1,theobservationscanbeseento
observationsareshuffledrandomly,andasubsetbetween3and improveasthelatentdistributionimproves.Afterafewsteps,this
10observationsareselectedandusedasmodelinputs. distributionconvergestoafixedvalueascanalsobenotedbythe
We design two cases for the active vision experiments in decreasingepistemicvaluesshowninFigure8A.Additionally,as
the robotic workspace. In the first case, we put an additional theagentimaginesnonewobjectsattheotherviewpoints,itdoes
constraint on the height of the agent and only allow the agent notbelievetheywillinfluenceitsbeliefovers.Aftertheagenthas
to move in the x and y direction of the workspace, i.e. parallel refineditsinternalmodel,instep7,theviewpointsithasnotyet
withthetable.Wechoosethistolimitthepotentialviewpoints observedresultinahigherepistemicvalue,afterwhichtheagent
oftheagenttoobservetheepistemicandinstrumentalbehavior movestothislocation.Itfinallyobservesthebluecubeinthetop
inmoredetail,withrespecttotheimaginedviews.Inthesecond whichisthenalsoreconstructedintheimaginedviews.
case, we allow the agent to also move along the z-axis. We can In a second experiment, we evaluate the behavior that
now evaluate the global behavior of the agent and observe that emerges when the full expected free energy is used to drive
whenitexploresanewarea,itwillfirstpreferviewpointsfrom viewpointselection.Boththeepistemicandinstrumentalvalues
higher vantage points in which it can observe a large piece of are computed and used to acquire the expected free energy for
the workspace, after which it will move down to acquire more every potential viewpoint. The instrumental value is computed
detailedobservations. as the log likelihood of the expected observation under a
desired goal prior distribution. We choose the distribution of
3.3.1.ActiveVisionWith2DegreesofFreedom this preferred observation as a multivariate Gaussian in which
This experiment considers the case where the artificial agent is each pixel is an independent Gaussian with as mean value
limitedto2degreesoffreedom.Welimitthedegreesoffreedom the target goal observation and a fixed variance of 0.65. We
to make the analysis of the behavior more interpretable. The empirically determined this value for the goal variance which
resultsofthisexperimentareshowninFigure8. yields a good trade-off between the epistemic and instrumental
Even though the generative model is capable of inferring behavior.Inthiscaseweuseanobservationofthebluecubeas
the state and generating an imagined view for any viewpoint goalobservation,namelythefinalobservationfromtheepistemic
in a continuous space of the robotic working area, it would be exploration, and shown in Figure8C. Please note that any
computationallyexpensivetocomputetheexpectedfreeenergy observationcouldbeusedasagoal.
for all potential viewpoints. Instead, we sample a uniform grid When we look at the behavior that emerges in Figure9, we
of potential future viewpoints over the robotic workspace, and notice that initially the agent has no idea where it can observe
evaluatetheexpectedepistemicvalueforthesesamplesusingthe it’spreferredobservation. Thiscanbeobservedbytheuniform
methoddescribedinsection2. instrumentalvalueshowninFigure9Batstep0.Theepistemic
First, only the epistemic value is considered. We look at the value takes the upper hand, and the chosen viewpoint is again
behavior for an active vision agent for the scene visualized in in the center of the table, similar as in the case when only
Figure7.Forresultsonadditionalscenes,thereaderisreferredto the epistemic value was considered. At this new viewpoint, the
SupplementaryMaterial.Theagentstartsinaninitialposition agent observes the yellow cube and the red ball. Notice how
inwhichitcannotobserveanyoftheobjectsthatarelyingon the instrumental value becomes lower at these viewpoints in
the table. Its current observation is shown in the first image of Figure9B.Theagentrealizesthattheseviewpointswillnotaidin
Figure8C.Theagentimaginestheentireworkspacetobeempty thetasktoreachtheblueobject.However,astheepistemicvalue
without objects, this can be seen in the imagined observations atthistimestepislargerthantherangeoftheinstrumentalvalue
FrontiersinNeurorobotics|www.frontiersin.org 10 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
FIGURE8|Asequenceofstepsperformedbyanartificialagentdriventhroughminimizationoftheepistemictermfromtheexpectedfreeenergy.
Information-seekingbehaviorcanbeobserved.(A)Arepresentationoftheepistemicvaluefordifferentpotentialfutureviewpointsindifferentstepsofthesequence.
Thelegendisprovidedontheright,darkervaluesmeanhigherepistemicvalues,andarethusmorelikelytobechosenbytheagent.(B)Theimaginedobservations
forthepotentialviewpointsatdifferentstepsinasequenceexecutedbyanagentdrivenaccordingtotheactiveinferenceparadigm.(C)Thelastobservationthe
agenthasacquiredfromthepreviousstep.Theblacksquaresinthebottomofeachframearethegripperhandles.
at this viewpoint, they contradict each other and the epistemic willflytoahighvantagepointtosearchforitsprey,andmove
valueisstilldominant.Pleasenotethatwhiletheabsolutevalue downwhenithaslocalizedit(Fristonetal.,2016).
oftheinstrumentaltermismuchhigherthantheepistemicterm, We task the robot to find the blue cube from the final
these are relative to each other. The range of the instrumental observationinFigure8again.Thedifferentachievedrobotposes
termisinthesamerangeastheepistemicvalue.Afterobserving and their corresponding observations are shown in Figure10.
afewobservations,theinstrumentaltermfinallytakestheupper In the executed trajectory, we notice that the owl-like behavior
hand and the agent is driven away to further explore the area. emerges through the minimization of expected free energy.
Itfinallyfindsthebluecubeinthetoprightinthe7thstep.As Initially, the agent has no knowledge about the workspace and
the instrumental value is very high for this observation, it now moves its gripper and corresponding camera toward a higher
takestheupperhandandtheagentwillnaturallyremainatthis vantagepointfromwhichitcanobservetheworkspace.Initially,
location.Noticehowtheagenthasfoundtheobjectinlesssteps theagentonlyobservesaredandayellowobject,afterwhichit
thanwhenitwasonlydriventhroughepistemicvalue.Because movesclosertoinspecttheseobjects.Ithasupdateditsinternal
the agent now prefers to search and reach its goal observation, model by observing the object from up close, and it is clear
itwillavoidgettingstuckataspecificlocationaslongasthisis through the instrumental value that the desired observation is
notthepreferredobservation.Itisthereforebetteratfindingthe not at this location. In a similar manner as explained in the
targettoreach,howeveritwillnotnecessarilyexploretheentire experimentwithtwodegreesoffreedom,theagentagainmoves
workspace,asitwouldwhenonlyconsideringtheepistemicterm toahighervantagepoint,butmoretothecenterofthetable.Itis
givenenoughsteps.Itisimportanttonotethattheinstrumental nowabletoobserveboththebluecuboidandtheedgesofthered
value to the right of the target value is low in magnitude. The andyellowobjects.Ithaslocalizedthetargetandmovestoward
modelbelievesitisunlikelythatitwillfindthetargetobservation its preferred state. The agent does not move in the subsequent
here.Thiscanbeattributedtothepixel-wiseloglikelihoodthat steps,showingthatithasreachedthepointthatprovidesitwith
is computed, even though the object is in view, because it is at the lowest expected free energy. We also notice that the agent
differentpixellocations,thiswillbealesslikelyobservationthan hasfoundtheobjectfasterthaninthepreviousexperiment.The
anareaofthetablethatdoesnotcontainobjects.Tocombatthis additionaldegreeoffreedomisimmediatelyexploitedbythefree
characteristic,wesamplethegridofpotentialviewpointswitha energy principle. For the acquired results on additional scenes,
lotofoverlapbetweentheneighboringviews. thereaderisreferredtoSupplementaryMaterial.
3.3.2.ExtendingtoThreeDegreesofFreedom
Finally, we no longer constrain the movement along the z-axis 4. DISCUSSION
for the robot manipulator. The orientation is still in a fixed
downwardsposition.Westillconsiderthesamesceneasinthe In the above experiments, we have shown that it is possible
previous experiments and start the robot gripper in the same to use the active inference paradigm as a natural solution for
initial position without any observations. We evaluate whether active vision on complex tasks in which the distribution over
thisthirddegreeoffreedomimprovesthespeedatwhichthearea the environment is not defined upfront. Similar to prior work
canbeuncovered,andwhetherthechosenactionsmatchesthe onlearningstatespacemodelsforactiveinference(Çataletal.,
biologicalbehaviorencounteredinforexampleanowl.Theowl 2020),welearnourgenerativemodeldirectlyfromdata.
FrontiersinNeurorobotics|www.frontiersin.org 11 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
FIGURE9|Thisfigurerepresentsasequenceofmovementwhendrivingviewpointselectionthroughminimizationoftheexpectedfreeenergy.Theagentisnow
equippedwithapreferredstate.Asapreferredstate,wesetthefinalobservationfromtheepistemicexplorationshowninFigure8C,butinprinciple,anyobservation
couldbeused.(A)Arepresentationoftheepistemicvaluefordifferentpotentialfutureviewpointsindifferentstepsofthesequence.Thelegendisprovidedonthe
right,darkervaluesmeanhigherepistemicvalues,andarethusmorelikelytobechosenbytheagent.(B)Arepresentationoftheinstrumentalvaluefordifferent
potentialfutureviewpointsindifferentstepsofasequence.Thelegendisprovidedontheright,darkervaluesmeanhigherinstrumentalvaluesandarethusmorelikely
tobechosenbytheagent.(C)Thechosenviewpointatthisstepisshownbyablacksquare.Thisisdonebyapplyingthesoftmaxoperationtothefullexpectedfree
energy.(D)Theimaginedobservationsforthepotentialviewpointsfordifferentstepsinasequenceexecutedbyanactiveinferencedrivenagent.(E)Thelast
observationtheagenthasacquiredfromthepreviousstep.Theblacksquaresinthebottomofeachframearethegripperhandles.
FIGURE10|Thechosentrajectoryforanartificialagentwhenviewpointsarechosenbytheminimizationofexpectedfreeenergy.Thepreferredstateischosen
randomlyasthefinalobservationfromtheepistemicexplorationshowninFigure8C.(A)Showstheposeoftherobotateachstep.Step1representstheinitialstate.
(B)Thecorrespondingobservationfromthegripperatthatstep.
We have observed that a sheer epistemic agent will explore andwillmovetowardahighervantagepointtoobservealarger
theenvironmentbymovingtodifferentviewpointsintheworld. area at one time, similar to the behavior of an owl scavenging
When we use the full expected free energy to drive viewpoint forprey.
selection,weobservethatepistemicforagingbehavioremerges, Forrobotstosolvecomplextasks,oneofthefirststepsisto
andtheagentwillexploretheenvironmentwithrandomsaccades perceive the world and understand the current situation. This
FrontiersinNeurorobotics|www.frontiersin.org 12 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
workshowsthatthelearnedgenerativemodeliscapableofbeing the optimal next viewpoint. In follow-up work, the actions of
used in a neurologically inspired solution for perception of the human collaborators can be modeled through their own free
world.Asthistheoreticalframeworkofactiveinferenceisalready energyminimizationschemeandcanbeintegratedintheactive
equippedtodealwithactionsthatperturbtheworld,thissolution inferenceframeworktoselectthenextbestview.Finally,thegoal
can be extended with a more complex generative model that istoevaluatethismethodonareal-liferobot.
is able to estimate the changes the agent, or other autonomous
beingscanmakeintheworld. Related Work
While our approach allows to learn the generative model Therelatedworkfallsintwocategories,i.e.,scenerepresentation
purely from pixel data, this also has a couple of drawbacks. learningandrelatedworkintheareaofactivevision.Thereisa
In our case for instance, the model is trained using a large lotofresearchthatconsiderstheproblemofscenerepresentation
amount of data in a simulation environment with a restricted learningandproposesdifferentneuralnetworkarchitecturesto
number of object shapes and colors. To be applicable for real- aidtheprocessoflearningproperrepresentationmodelsofour
world scenarios, probably an even larger model and dataset neuralnetworkarchitecture.Inthesecondpartweconsiderthe
are required. Also, it is clear that the reconstructions are not domain in active vision, this is an active research domain in
sharp, and blurry objects are reconstructed. This is typical for traditionalcomputervisionproblems,buthasalsobeenapplied
avariationalauto-encoder,andwhilemanyapproachesexistto tomanyreinforcementlearningtasks.
create sharper reconstructions (Makhzani et al., 2015; Heljakka Scene representation learning is a research field in which
et al., 2018, 2020; Huang et al., 2018), we argue that this is not the goal is to learn a good representation of the environment.
necessary for our case. As long as the generated observations A vast amount of work exists that considers representation
are spatially correlated and the object properties such as size learning for separate objects. Multi-View CNN (MVCNN) uses
andcolorarecorrectlyreconstructed,thegenerativemodelwill views from multiple viewpoints to learn a representation for
be capable of working within the active inference framework. classificationandsegmentation(Suetal.,2015).DeepVoxelsuses
Thiscanbecomparedtosomeone tryingtoremember thefine ageometricrepresentationoftheobject,inwhicheachvoxelhas
details of a recently visited building. A person is able to draw aseparatefeaturevector,whichisthenrenderedthroughaneural
the general structure of the building, but will find difficulty to renderer (Sitzmann et al., 2019a). In their follow-up work on
draweachstonecorrectlywiththecorrectshade.However,thisis Scene Representation Networks, this was extend to replace the
notnecessarytofindthedoorandnavigatethroughthebuilding. voxelizedrepresentationbyaneuralnetwork,estimatedthrough
Nevertheless,byusingthemeansquarederrorinpixelspaceto a hypernetwork, that predicts a feature vector for any point in
train the likelihood model, small-sized objects will generate a 3D space. These features are then rendered through a neural
smallgradientsignal,andwillbedifficultforthemodeltoencode. renderer(Sitzmannetal.,2019b).
To mitigate this, one could look at different loss functions, for Object-centric models have also gained a lot of attention
example perceptual loss (Johnson et al., 2016) or contrastive lately. These models stem from the seminal work on Attend
loss(Hadselletal.,2006). InferRepeat(Eslamietal.,2016)inwhichadistinctlatentcode,
Our approach evaluates the expected free energy for a which separately encodes the position and the type of object,
number of considered potentialviewpoints. Thecomputational is predicted per object in the scene. This is done through a
complexityofthisalgorithmscaleslinearlywiththenumberof recurrent neural network that is capable of estimating when all
considered viewpoints. However, given enough GPU memory, objectsarefound.InSQ-AIR,thisworkisextendedtosequences
this algorithm can easily be modified to compute the expected of images, and a discovery and propagation mechanism was
free energy for all potential viewpoints in parallel, making it introduced to track objects through different frames (Kosiorek
an algorithm with constant time complexity. Provided that the etal.,2018).Thesehavebeenextendedtobetterhandlephysical
neuralnetworkcanberunonaGPU,itcanbeusedforreal-time interactions(Kossenetal.,2020)orbemorescalable(Crawford
controlofphysicalrobotmanipulators. and Pineau, 2020; Jiang et al., 2020). These extensions have
Infuturework,wewanttoinvestigatemoreefficientmethods also been combined by Lin et al. (2020). 3D-RelNet is also an
for evaluating the free energy and planning in a complex state object-centric model that predicts a pose for each object and
space. In this case, it was feasible to evaluate the expected their relation to the other objects in the scene (Kulkarni et al.,
free energy for each viewpoint as we sampled a limited grid 2019).Whiletheseapproachesseempromising,intheircurrent
of future viewpoints and only looked at one step in the implementation they only consider video data from a fixed
future. The amount of expected free energy values to compute camera viewpoint. These models do not lend themselves to an
would increase exponentially, as more time steps ahead are activevisionsystem.
considered. Additionally, in future work we would like to add Implicit representation models learn the three dimensional
object interaction, i.e., allowing the robot to move objects in properties of the world directly from observations with no
a specific desired configuration. Moreover, this approach will intermediate representation. A single neural network is then
beincreasinglyimportantincollaboratorysettings.Therobotic created for each scene. Neural Radiance Fields (NeRF) learn to
agent can encounter occlusions and limited field of view for inferthecolorvaluesforeachthreedimensionalpointthrough
multiple reasons such as other humans obstructing objects or adifferentiableraytracerfromasetofobservations(Mildenhall
placingthingsinfrontofthetargetobject.Itisinthesesituations etal.,2020).Thefollow-upworkbyParketal.(2020)adaptsthe
essential to be able to reason about the scene and choosing algorithmforamorerobustoptimizationandtheworkbyXian
FrontiersinNeurorobotics|www.frontiersin.org 13 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
et al. (2020) extends this to deal with video sequences. SIRENs using a data-driven approach that predicts the amount of new
alsobelongtothiscategory,however,thisnetworkisoptimized information gained from a given viewpoint. They learn this
directly from the three dimensional point cloud (Sitzmann directly using supervision with oracle data. Instead of learning
et al., 2020). While these works often result in very sharp autilityfunction,deepneuralnetworksthatdirectlypredictthe
reconstructions with a large amount of detail present in the next-best viewpoint have also been researched (Doumanoglou
scenes, they are difficult to optimize due to the large training et al., 2016; Mendoza et al., 2020). These methods require a
times and do not allow for new observations to be added on ground-truth“best”view,forwhichadatasetiscreatedusingthe
thefly. fullsceneorobjectinformation.
The last category of methods encodes the scene in a latent Biology has inspired work on active vision and perception
vectorthatdescribesthesceneinablackboxapproach.Thelatent as well. An active vision system for robotic manipulators was
vector does not enforce geometric constraints. The Generative proposed that is inspired by the way primates deal with their
QueryNetworkdoesthisbyencodingallobservationsseparately visual inputs (Ognibene and Baldassare, 2014). Rasouli et al.
into a latent vector, which is then summed to acquire a global (2019)proposeaprobabilisticbio-inspiredattention-basedvisual
representation of the scene (Eslami et al., 2018). This latent search system for mobile robotics. Similar to our work, active
vector can be sampled and decoded through an autoregressive inference has already been applied to different active vision
decoder (Gregor et al., 2015), which is then optimized in an settings. Mirza et al. (2016) show that the free energy principle
end-to-end fashion. This work considers full scenes in which canbeusedforvisualforaging.Theydefineaclassificationtask,
the observer can navigate. This has also been extended with wheretheagentmustacquirevisualcuestocorrectlyclassifythe
an attention mechanism to separately encode parts of each scenarioitisin.Follow-upwork(Conoretal.,2020)considersa
observation,inordertobettercapturetheinformation(Burgess hierarchicalsceneinwhichdecisionsaremadeatmultiplelevels.
etal.,2019).OurmodelmostresemblesthisGQNarchitecture, Fovea-based attention to improve perception and recognition
as this is a straightforward implementation that allows for on image data has been performed through the free energy
arbitraryviewpointsandwhichcouldeasilybeextendedwithour principle(Daucé,2018).Whiletheseapproachesshowpromising
Bayesianaggregationstrategy.Otherapproachesresultinsharper results, they all consider designed scenarios for which the state
reconstructions,howevertheyeitheroptimizeaneuralnetwork spacecanbecarefullycraftedinadvance.
perscene,workwithafixedobserverviewpoint,oronlyconsider Our approach closely connects to traditional active vision
separateobjects. systems in which a utility function is evaluated. The expected
Activevisionsystemsarecalledactivesincetheycanchange free energy formulation is used as a utility function in our
the camera extrinsic parameters to improve the quality of work. However, in contrast to these traditional approaches,
the perception (Aloimonos et al., 1988). In most active vision we use a deep neural network to encode the representation
research, the next best viewpoints are selected to improve the of the environment instead of using geometric representations
amountofobservationsneedtoscananarea,forexplorationand or hand-crafting the distributions that are acquired. While
mappingorforreconstructionoftheworld. active vision techniques that use neural networks typically use
Most traditional methods use a frontier-based approach to these models to predict the next best viewpoint directly, or
select the next viewpoint (Yamauchi, 1997; Chen et al., 2011; predict a learned utility function. We reason that the expected
Fraundorferetal.,2012;Forsteretal.,2014;Kriegeletal.,2015; free energy is a natural solution to this problem, as this
Hepp et al., 2018). The frontier is defined as the boundary is the utility function that determine the actions of living
between the observed area and the unobserved area, and thus organisms (Friston, 2013). We use our neural networks to
thesemodelsrequireanexplicitgeometricrepresentationofthe imaginefuturestates,beliefabouttheenvironmentand,similar
world. Typically these methods use a discretized map of the to the work Finn and Levine (2017), use these to plan the
world,anoccupancygridin2D(Yamauchi,1997)oravoxelized agent’sactions.
rasterization in 3D (Fraundorfer et al., 2012). The points on
the frontier are then evaluated through a utility function that
scores the amount of information that will be gained. These 5. CONCLUSION
utility functions are often hand-crafted and uncertainty or
reconstructionbased(Wenhardtetal.,2007;DunnandFrahm, In this paper we investigated whether the active inference
2009; Forster et al., 2014; Kriegel et al., 2015; Isler et al., 2016; paradigm could be used for a robotic searching and reaching
Delmericoetal.,2018;Heppetal.,2018). task. As it is impossible for real-world scenarios to define
Withtheriseofdeeplearning,activevisionproblemshasalso the generative model upfront, we investigated the ability to
been tackled through learning-based approaches. The problem use a learned generative model to this end. We showed
has been cast as a set covering optimization problem in which that we were able to approximate a generative model using
a reinforcement learning agent has to select the least amount deep neural networks and that this can be learned directly
of views to observe the area (Devrim Kaba et al., 2017). This from pixel observations by means free energy minimization.
approach assumes that the area is known in advance, and To this end we expanded the Generative Query Network
that an agent can be trained on this. It does not allow for by aggregating the latent distributions from each observation
unseen environments. Other deep learning techniques have through a Gaussian multiplication. We conducted an ablation
also been proposed. Hepp et al. (2018) learn a utility function study and showed that this model had similar performance
FrontiersinNeurorobotics|www.frontiersin.org 14 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
as other aggregation methods when operating in the training AUTHOR CONTRIBUTIONS
range,andthatthemodeloutperformedothertechniqueswhen
multipleobservationswereconsidered.Inasecondexperiment TVa and TVe conceived and performed the experiments.
we evaluated whether this model was capable of inferring TVa, OÇ, and TVe worked out the mathematical basis for
information about a cup, namely its orientation and whether the experiments. TVa, TVe, OÇ, CD, and BD contributed to
or not it has a handle. We showed that the agent actively the manuscript. BD supervised the experiments. All authors
samples the world from viewpoints that allow itself to reduce contributedtothearticleandapprovedthesubmittedversion.
the uncertainty on its belief state distributions. In the third
case,weshowthatanartificialagentwitharoboticmanipulator FUNDING
explores the environment until it has observed all objects in
the workspace. We showed that if the viewpoints are chosen This research received funding from the Flemish Government
by minimization of the expected free energy when provided (AIResearchProgram).OÇwasfundedbyaPh.D.grantofthe
with a target goal, the agent explores the area in a biologically- FlandersResearchFoundation(FWO).Partofthisworkhasbeen
inspired manner and navigates toward the goal viewpoint supportedbyFlandersInnovation&Entrepreneurship,bywayof
once it has acquired enough information to determine this grantagreementHBC.2020.2347.
specificviewpoint.
SUPPLEMENTARY MATERIAL
DATA AVAILABILITY STATEMENT
The Supplementary Material for this article can be found
The raw data supporting the conclusions of this article will be online at: https://www.frontiersin.org/articles/10.3389/fnbot.
madeavailablebytheauthors,withoutunduereservation. 2021.642780/full#supplementary-material
REFERENCES
Delmerico,J.,Isler,S.,Sabzevari,R.,andScaramuzza,D.(2018).Acomparisonof
volumetricinformationgainmetricsforactive3dobjectreconstruction.Auton.
Aloimonos, J., Weiss, I., and Bandyopadhyay, A. (1988). Active vision. Int. J. Robots42,197–208.doi:10.1007/s10514-017-9634-0
Comput.Vis.1,333–356.doi:10.1007/BF00133571 DevrimKaba,M.,GokhanUzunbas,M.,andNamLim,S.(2017).“Areinforcement
Beal,M.J.(2003).VariationalalgorithmsforapproximateBayesianinference(Ph.D. learningapproachtotheviewplanningproblem,”inProceedingsoftheIEEE
thesis).UniversityCollegeLondon,London,UnitedKingdom. Conference on Computer Vision and Pattern Recognition (Honolulu, HI),
Billard,A.,andKragic,D.(2019).Trendsandchallengesinrobotmanipulation. 6933–6941.doi:10.1109/CVPR.2017.541
Science364:6446.doi:10.1126/science.aat8414 Doumanoglou, A., Kouskouridas, R., Malassiotis, S., and Kim, T.-K.
Blender Online Community (2018). Blender - a 3D Modelling and (2016). “Recovering 6d object pose and predicting next-best-view in the
Rendering Package. Amsterdam: Blender Foundation; Stichting crowd,” in Proceedings of the IEEE Conference on Computer Vision and
BlenderFoundation. Pattern Recognition (Las Vegas, NV), 3583–3592. doi: 10.1109/CVPR.20
Burgess,C.P.,Matthey,L.,Watters,N.,Kabra,R.,Higgins,I.,Botvinick,M.,etal. 16.390
(2019).Monet:Unsupervisedscenedecompositionandrepresentation.arXiv Dunn,E.,andFrahm,J.-M.(2009).“Nextbestviewplanningforactivemodel
[Preprint].arXiv:1901.11390. improvement,”inProceedingsoftheBritishMachineVisionConference,edsA.
Çatal,O.,Wauthier,S.,DeBoom,C.,Verbelen,T.,andDhoedt,B.(2020).Learning Cavallaro,S.Prince,andD.Alexander(BMVAPress).doi:10.5244/C.23.53
generativestatespacemodelsforactiveinference.Front.Comput.Neurosci. Eslami, S. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Hinton, G. E.,
14:103.doi:10.3389/fncom.2020.574372 etal.(2016).“Attend,infer,repeat:fastsceneunderstandingwithgenerative
Chang,A.X.,Funkhouser,T.,Guibas,L.,Hanrahan,P.,Huang,Q.,Li,Z.,etal. models,”inAdvancesinNeural Information ProcessingSystems(Barcelona),
(2015).ShapeNet:AnInformation-Rich3DModelRepository.TechnicalReport, 3225–3233.
StanfordUniversity;PrincetonUniversity;ToyotaTechnologicalInstituteat Eslami,S.M.A.,Rezende,D.J.,Besse,F.,Viola,F.,Morcos,A.S.,Garnelo,M.,etal.
Chicago. (2018).Neuralscenerepresentationandrendering.Science360,1204–1210.
Chen, S., Li, Y., and Kwok, N. M. (2011). Active vision in robotic systems: doi:10.1126/science.aar6170
a survey of recent developments. Int. J. Robot. Res. 30, 1343–1377. Finn,C.,andLevine,S.(2017).“Deepvisualforesightforplanningrobotmotion,”
doi:10.1177/0278364911410755 in2017IEEEInternationalConferenceonRoboticsandAutomation(ICRA)
Conor, H. R., Berk, M. M., Thomas, P., Karl, F., Igor, K., Arezoo, P. (2020). (Singapore),2786–2793.doi:10.1109/ICRA.2017.7989324
Deep active inferenceand sceneconstruction. Front. Artif. Intell.3:509354. Forster, C., Pizzoli, M., and Scaramuzza, D. (2014). “Appearance-based
doi:10.3389/frai.2020.509354 active, monocular, dense reconstruction for micro aerial vehicles,”
Crawford, E., and Pineau, J. (2020). “Exploiting spatial invariance for scalable in Conference: Robotics: Science and Systems (RSS) (Berkely, CA)
unsupervised object tracking,” in The Thirty-Fourth AAAI Conference doi:10.15607/RSS.2014.X.029
on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Fraundorfer,F.,Heng,L.,Honegger,D.,Lee,G.H.,Meier,L.,Tanskanen,P.,etal.
Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth (2012).“Vision-basedautonomousmappingandexplorationusingaquadrotor
AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI MAV,” in 2012 IEEE/RSJ International Conference on Intelligent Robots and
2020 (New York, NY: AAAI Press), 3684–3692. doi: 10.1609/aaai.v34i04. Systems(Vilamoura),4557–4564.doi:10.1109/IROS.2012.6385934
5777 Friston,K.(2010).Thefree-energyprinciple:aunifiedbraintheory?Nat.Rev.
Daucé, E. (2018). Active fovea-based vision through computationally- Neurosci.11,127–138.doi:10.1038/nrn2787
effective model-based prediction. Front. Neurorobot. 12:76. Friston, K. (2013). Life as we know it. J. R. Soc. Interface 10:20130475.
doi:10.3389/fnbot.2018.00076 doi:10.1098/rsif.2013.0475
FrontiersinNeurorobotics|www.frontiersin.org 15 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, Lin, Z., Wu, Y.-F., Peri, S., Fu, B., Jiang, J., and Ahn, S. (2020). Improving
J., and Pezzulo, G. (2016). Active inference and learning. generativeimaginationinobject-centricworldmodels.arXiv:2010.02054.
Neurosci. Biobehav. Rev. 68, 862–879. doi: 10.1016/j.neubiorev.2016. Makhzani,A.,Shlens,J.,Jaitly,N.,Goodfellow,I.,andFrey,B.(2015).Adversarial
06.022 autoencoders.arXiv[Preprint].arXiv:1511.05644.
Garnelo,M.,Schwarz,J.,Rosenbaum,D.,Viola,F.,Rezende,D.J.,Eslami,S.,etal. Matsumoto,T.,andTani,J.(2020).Goal-directedplanningforhabituatedagents
(2018).Neuralprocesses.arXiv[Preprint].arXiv:1807.01622. by active inference using a variational recurrent neural network. Entropy
Gregor,K.,Danihelka,I.,Graves,A.,Rezende,D.,andWierstra,D.(2015).“Draw: 22:564.doi:10.3390/e22050564
arecurrentneuralnetworkforimagegeneration,”inInternationalConference Mendoza,M.,Vasquez-Gomez,J.I.,Taud,H.,Sucar,L.E.,andReta,C.(2020).
onMachineLearning(Lille:PMLR),1462–1471. Supervisedlearningofthenext-best-viewfor3Dobjectreconstruction.Pattern
Hadsell, R., Chopra, S., and Lecun, Y. (2006). “Dimensionality reduction by Recogn.Lett.133,224–231.doi:10.1016/j.patrec.2020.02.024
learning an invariant mapping,” in 2006 IEEE Computer Society Conference Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R.,
on Computer Vision and Pattern Recognition (CVPR’06) (New York, NY), and Ng, R. (2020) “Nerf: Representing scenes as neural radiance fields for
1735–1742.doi:10.1109/CVPR.2006.100 viewsynthesis”,inFrahmComputerVision?ECCV2020.ECCV2020.Lecture
Häni, N., Engin, S., Chao, J.-J., and Isler, V. (2020). “Continuous object NotesinComputerScience,Vol.12346,edsA.Vedaldi,H.Bischof,T.Brox,
representationnetworks:novelviewsynthesiswithouttargetviewsupervision,” and J. M. Frahm (Glasgow, UK; Cham: Springer) doi: 10.1007/978-3-030-
in34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020), 58452-8_24
(Vancouver,BC). Mirza,M.B.,Adams,R.A.,Mathys,C.,andFriston,K.J.(2018).Humanvisual
Heljakka,A.,Solin,A.,andKannala,J.(2018).“Pioneernetworks:progressively explorationreducesuncertaintyaboutthesensedworld.PLoSONE13:e190429.
growing generative autoencoder,” in Asian Conference on Computer Vision doi:10.1371/journal.pone.0190429
(Perth:Springer),22–38.doi:10.1007/978-3-030-20887-5_2 Mirza, M. B., Adams, R. A., Mathys, C. D., and Friston, K. J. (2016). Scene
Heljakka, A., Solin, A., and Kannala, J. (2020). “Towards photographic image construction,visualforaging,andactiveinference.Front.Comput.Neurosci.
manipulationwithbalancedgrowingofgenerativeautoencoders,”inTheIEEE 10:56.doi:10.3389/fncom.2016.00056
WinterConferenceonApplicationsofComputerVision(SnowmassVillage,CO), Ognibene,D.,andBaldassare,G.(2014).Ecologicalactivevision:fourbioinspired
3120–3129.doi:10.1109/WACV45572.2020.9093375 principles to integrate bottom-up and adaptive top-down attention tested
Hepp,B.,Dey,D.,Sinha,S.N.,Kapoor,A.,Joshi,N.,andHilliges,O.(2018). with a simple camera-arm robot. IEEE Trans. Auton. Mental Dev. 7, 3–25.
“Learn-to-score:efficient3dsceneexplorationbypredictingviewutility,”in doi:10.1109/TAMD.2014.2341351
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV)(Munich), Park,K.,Sinha,U.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Seitz,S.M.,etal.
437–452.doi:10.1007/978-3-030-01267-0_27 (2020).Deformableneuralradiancefields.arXiv[Preprint].arXiv:2011.12948.
Huang, H., He, R., Sun, Z., Tan, T., et al. (2018). “Introvae: Introspective Parr, T., and Friston, K. J. (2017). The active construction of the visual
variational autoencoders for photographic image synthesis,” in Advances in world. Neuropsychologia 104, 92–101. doi: 10.1016/j.neuropsychologia.2017.
NeuralInformationProcessingSystems(Montreal,QC),52–63. 08.003
Isler,S.,Sabzevari,R.,Delmerico,J.,andScaramuzza,D.(2016).“Aninformation Perez,E.,Strub,F.,deVries,H.,Dumoulin,V.,andCourville,A.(2018).“Film:
gain formulation for active volumetric 3d reconstruction,” in 2016 IEEE visualreasoningwithageneralconditioninglayer,”inProceedingsoftheAAAI
International Conference on Robotics and Automation (ICRA) (Stockholm), Conference on Artificial Intelligence (New Orleans, LA), Vol. 32. Available
3477–3484.doi:10.1109/ICRA.2016.7487527 onlineat:https://ojs.aaai.org/index.php/AAAI/article/view/11671
Jiang, J., Janghorbani, S., de Melo, G., and Ahn, S. (2020). “SCALOR: Rasouli, A., Lanillos, P., Cheng, G., and Tsotsos, J. K.
generative world models with scalable object representations,” in 8th (2019). Attention-based active visual search for mobile
International Conference on Learning Representations, ICLR 2020 (Addis robots. Auton. Robots 44, 131–146. doi: 10.1007/s10514-019-
Ababa:OpenReview.net).Availableonlineat:https://openreview.net/forum? 09882-z
id=SJxrKgStDH Rezende,D.J.,Mohamed,S.,andWierstra,D.(2014).“Stochasticbackpropagation
Johnson,J.,Alahi,A.,andFei-Fei,L.(2016).“Perceptuallossesforreal-timestyle andapproximateinferenceindeepgenerativemodels,”inProceedingsofthe
transferandsuper-resolution,”inComputerVision–ECCV2016,edsB.Leibe, 31th International Conference on Machine Learning, ICML 2014 (Beijing),
J.Matas,N.Sebe,andM.Welling(Cham:SpringerInternationalPublishing), 1278–1286.
694–711. Rezende,D.J.,andViola,F.(2018).Tamingvaes.CoRR,abs/1810.00597.
Kalman,R.E.(1960).Anewapproachtolinearfilteringandpredictionproblems. Rohmer,E.,Singh,S.P.N.,andFreese,M.(2013).“Coppeliasim(formerlyv-
J.BasicEng.82,35–45.doi:10.1115/1.3662552 rep): a versatile and scalable robot simulation framework,” in Proc. of The
Kingma,D.P.,andBa,J.(2015).“Adam:amethodforstochasticoptimization,” International Conference on Intelligent Robots and Systems (IROS) (Tokyo).
in3rdInternationalConferenceonLearningRepresentations,ICLR2015,edsY. doi:10.1109/IROS.2013.6696520
BengioandY.LeCun(SanDiego,CA). Sitzmann,V.,Martel,J.N.P.,Bergman,A.W.,Lindell,D.B.,andWetzstein,G.
Kingma,D.P.,andWelling,M.(2014).“Auto-encodingvariationalbayes,”in2nd (2020).“Implicitneuralrepresentationswithperiodicactivationfunctions,”in
InternationalConferenceonLearningRepresentations,ICLR2014(Banff,AB, Proc.NeurIPS.
Canada). Sitzmann, V., Thies, J., Heide, F., Nießner, M., Wetzstein, G., and Zollhöfer,
Kosiorek,A.R.,Kim,H.,Posner,I.,andTeh,Y.W.(2018).“Sequentialattend, M. (2019a). “Deepvoxels: Learning persistent 3d feature embeddings,” in
infer,repeat:generativemodellingofmovingobjects,”inAdvancesinNeural Proc. Computer Vision and Pattern Recognition (CVPR) (Long Beach, CA).
InformationProcessingSystems(Montreal,QC). doi:10.1109/CVPR.2019.00254
Kossen, J., Stelzner, K., Hussing, M., Voelcker, C., and Kersting, K. (2020). Sitzmann, V., Zollhöfer, M., and Wetzstein, G. (2019b). “Scene representation
“Structuredobject-awarephysicspredictionforvideomodelingandplanning,” networks: continuous 3d-structure-aware neural scene representations,” in
in International Conference on Learning Representations (Addis Ababa). AdvancesinNeuralInformationProcessingSystems(Vancouver,BC).
Availableonlineat:https://openreview.net/forum?id=B1e-kxSKDH Srihasam, K., and Bullock, D. (2008). Target selection by the frontal
Kriegel, S., Rink, C., Bodenmüller, T., and Suppa, M. (2015). Efficient next- cortex during coordinated saccadic and smooth pursuit eye
best-scan planning for autonomous 3d surface reconstruction of unknown movements. J. Cogn. Neurosci. 21, 1611–1627. doi: 10.1162/jocn.2009.
objects. J. Real-Time Image Process. 10, 611–631. doi: 10.1007/s11554-013- 21139
0386-6 Su, H., Maji, S., Kalogerakis, E., and Learned-Miller, E. G.
Kulkarni, N., Misra, I., Tulsiani, S., and Gupta, A. (2019). “3D-relnet: joint (2015). “Multi-view convolutional neural networks for 3d shape
object and relational network for 3d prediction,” in 2019 IEEE/CVF recognition,” in Proceedings of the IEEE International Conference on
International Conference on Computer Vision (ICCV), 2212–2221. Computer Vision (ICCV) (Santiago), 945–953. doi: 10.1109/ICCV.
doi:10.1109/ICCV.2019.00230 2015.114
FrontiersinNeurorobotics|www.frontiersin.org 16 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
Wenhardt,S.,Deutsch,B.,Angelopoulou,E.,andNiemann,H.(2007).“Active ConflictofInterest:Theauthorsdeclarethattheresearchwasconductedinthe
visualobjectreconstructionusingd-,e-,andt-optimalnextbestviews,”in2007 absenceofanycommercialorfinancialrelationshipsthatcouldbeconstruedasa
IEEEConferenceonComputerVisionandPatternRecognition(Minneapolis, potentialconflictofinterest.
MN),1–7.doi:10.1109/CVPR.2007.383363
Xian,W.,Huang,J.-B.,Kopf,J.,andKim,C.(2020).Space-timeneuralirradiance Copyright©2021VandeMaele,Verbelen,Çatal,DeBoomandDhoedt.Thisisan
fieldsforfree-viewpointvideo.arXiv[Preprint].arXiv:2011.12950. open-accessarticledistributedunderthetermsoftheCreativeCommonsAttribution
Yamauchi,B.(1997).“Afrontier-basedapproachforautonomousexploration,” License(CCBY).Theuse,distributionorreproductioninotherforumsispermitted,
in Proceedings 1997 IEEE International Symposium on Computational providedtheoriginalauthor(s)andthecopyrightowner(s)arecreditedandthatthe
IntelligenceinRoboticsandAutomationCIRA’97.’TowardsNewComputational originalpublicationinthisjournaliscited,inaccordancewithacceptedacademic
Principles for Robotics and Automation’ (Monterey, CA), 146–151. practice.Nouse,distributionorreproductionispermittedwhichdoesnotcomply
doi:10.1109/CIRA.1997.613851 withtheseterms.
FrontiersinNeurorobotics|www.frontiersin.org 17 March2021|Volume15|Article642780
VandeMaeleetal. ActiveVisionforRobotManipulators
APPENDIX
TABLEA1|Neuralnetworkarchitecture. TABLEA2|Neuralnetworkarchitectureofthelikelihoodmodel.
Layer Neurons/filters Layer Neurons/filters
Convolutional(1×1) 64 Linear 4×4×3
LeakyReLU
Convolutional(3×3) 16
LeakyReLU Convolutional(3×3) 128
FiLM(conditionedonvk) 16 LeakyReLU
Convolutional(3×3) 128
Convolutional(3×3) 32
LeakyReLU
Posterior(φ) LeakyReLU
FiLM(conditionedonvkands) 128
FiLM(conditionedonvk) 32
Convolutional(3×3) 64
Convolutional(3×3) 64
LeakyReLU
LeakyReLU
Convolutional(3×3) 64
FiLM(conditionedonvk) 64
LeakyReLU
Convolutional(3×3) 128 Likelihood(ψ) FiLM(conditionedonvkands) 64
LeakyReLU
Convolutional(3×3) 32
FiLM(conditionedonvk) 128
LeakyReLU
Linear 2×latentsize Convolutional(3×3) 32
LeakyReLU
Theposteriormodeldescribestheencoderusedintheneuralnetwork.Thelatentsize FiLM(conditionedonvkands) 32
variesfromexperimenttoexperiment.IntheShapeNetexperiment,thelatentsizeis64,
intheexperimentofthecup,thelatentsizeis9.Inthefinalcase,fortheroboticworkspace, Convolutional(3×3) 16
thelatentsizeis256.Intheposteriormodel,each3×3convolutionusesastrideof2to
LeakyReLU
reducethespatialresolutionofthedata.The1×1convolutionsuseastrideof1.
Convolutional(3×3) 16
LeakyReLU
FiLM(conditionedonvkands) 16
Convolutional(1×1) 3
Thismodelestimatesthepixelvaluesofapotentialviewpoint.Each3×3convolution
isprecededbyalinearlyupsamplestepthatdoublestheimageresolution.The1×1
convolutionsuseastrideof1.
FrontiersinNeurorobotics|www.frontiersin.org 18 March2021|Volume15|Article642780