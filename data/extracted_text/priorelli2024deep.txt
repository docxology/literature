DEEP HYBRID MODELS : INFER AND PLAN IN A DYNAMIC WORLD
Matteo Priorelli
Institute of Cognitive Sciences and Technologies
National Research Council of Italy
Sapienza University of Rome, Italy
matteo.priorelli@gmail.com
Ivilin Peev Stoianov
Institute of Cognitive Sciences and Technologies
National Research Council of Italy ivilinpeev.stoianov@cnr.it
ABSTRACT
To determine an optimal plan for complex tasks, one often deals with dynamic and hierarchical
relationships between several entities. Traditionally, such problems are tackled with optimal control,
which relies on the optimization of cost functions; instead, a recent biologically-motivated proposal
casts planning and control as an inference process.Active inference assumes that action and perception
are two complementary aspects of life whereby the role of the former is to fulfill the predictions
inferred by the latter. Here, we present an active inference approach that exploits discrete and
continuous processing, based on three features: the representation of potential body configurations in
relation to the objects of interest; the use of hierarchical relationships that enable the agent to easily
interpret and flexibly expand its body schema for tool use; the definition of potential trajectories
related to the agent’s intentions, used to infer and plan with dynamic elements at different temporal
scales. We evaluate this deep hybrid model on a habitual task: reaching a moving object after having
picked a moving tool. We show that the model can tackle the presented task under different conditions.
This study extends past work on planning as inference and advances an alternative direction to optimal
control.
1 Introduction
Imagine a baseball player striking a ball with a bat. State-of-the-art approaches to simulate such goal-directed
movements in a human-like manner often rely on optimal control [ 1, 2]. This theory rests upon the formulation of
goals in terms of value functions and their optimization via cost functions. While this approach has advanced both
robotics and our understanding of human motor control, its biological plausibility remains disputed [3]. Moreover, value
functions for motor control might restrict the range of motions an agent can learn [4]. In contrast, complex movements
like handwriting or walking can emerge naturally from generative models encoding goals as prior beliefs about the
environment [5]. The theory of active inference builds upon this premise, proposing that goal-directed behavior results
from a biased internal representation of the world. This bias generates a cascade of prediction errors forcing the agent
to sample those observations that make its beliefs true [6, 7, 8, 9]. In this view, the tradeoff between exploration and
exploitation arises naturally, driving the agent to minimize the uncertainty of its internal model before maximizing
potential rewards [10]. Active inference shares foundational principles with predictive coding [11, 12] and the Bayesian
brain hypothesis, which postulates that the brain makes sense of the world by constructing a hierarchical generative
model that continuously makes perceptual hypotheses and refines them [13].
This perspective offers a promising avenue for advancing current robotics and machine learning, particularly, in a
research that frames control and planning as an inference process [14, 15, 16, 17, 18]. A distinctive feature of active
inference is its ability to model the environment as a hierarchy of causes and dynamic states evolving at different
timescales [19], which is fundamental for biological phenomena such as linguistic communication [ 20], and for
advanced movements such as that of a baseball player. In the latter, several characteristics of the nervous system can be
arXiv:2402.10088v4  [cs.RO]  9 May 2025
well captured by an active inference agent, providing a robust alternative to optimal control. First, the human brain is
assumed to maintain a hierarchical representation of the body that generates the motor commands required to achieve
the desired goal [21]. This representation must be flexible, in that the baseball player should relate the configuration of
the bat to his body, acting as an extension of his hand [22]. Tool-use experiments in non-human primates revealed that
parietal and motor regions rapidly adapt to incorporate tools into the body schema, enabling seamless interaction with
objects [23]. Further, the player’s brain should maintain a dynamic representation of the moving ball in order to predict
its trajectory before and after the hit. The posterior parietal cortex is known to encode multiple objects in parallel during
action sequences, forming visuomotor representations that also account for object affordances [24]. In addition, distinct
neural populations in the dorsal premotor cortex are known to encode multiple reaching options in decision-making
tasks, with one population being activated and the others suppressed as the decision unfolds [25]. In short, the human
brain can maintain multiple potential body configurations and potential trajectories appropriate for specific tasks.
Despite its potential, the development of deep hierarchical models within active inference remains limited. Adaptation
to complex data still relies primarily on neural networks as generative models [26, 27, 28, 29, 30, 31, 32, 33, 34, 35].
One study demonstrated that a deep hierarchical agent, equipped with independent dynamics functions for each degree
of freedom (DoF), could continuously adjust its internal trajectories to align with prior expectations, enabling advanced
control of complex kinematic chains [36]. This ability to learn and act across intermediate timescales offers significant
advantages for solving control tasks. Moreover, simulating real-world scenarios can benefit from so-called hybrid
or mixed models in active inference, which integrate discrete decision-making with continuous motion. However,
state-of-the-art applications of such models have simulated static contexts only [8, 37, 38, 39, 40].
In this work, we address these challenges from a unified perspective. Our key contributions are summarized as follows:
• We present an active inference agent affording robust planning in dynamic environments. The basic unit
of this agent maintains potential trajectories, enabling a high-level discrete model to infer the state of the
world and plan composite movements. The units are hierarchically combined to represent potential body
configurations, incorporating object affordances (e.g., grasping a cup by the handle or with the whole hand)
and their hierarchical relationships (e.g., how a tool can extend the agent’s kinematic chain).
• We introduce a modular architecture designed for tasks that involve deep hierarchical modeling, such as tool
use. Its multi-input and multi-output connectivity resembles traditional neural networks and represents an
initial step toward designing deep structures in active inference capable of generalizing across and learning
novel tasks.
• We evaluate the agent’s performance in a common task: reaching a moving ball after reaching and picking a
moving tool. The results highlight the interplay between the agent’s potential trajectories and the dynamic
accumulation of sensory evidence. We demonstrate the agent’s ability to infer and plan under different
conditions, such as random object positions and velocities.
2 Methods
2.1 Predictive coding
According to predictive coding (PC), the human brain makes sense of the world by constructing an internal generative
model of how hidden states of the environment generate the perceived sensations [11, 12, 41]. This internal model is
continuously refined by minimizing the discrepancy (called prediction error) between sensations and the respective
predictions. More formally, given some prior knowledge p(x) over hidden states x and partial evidence p(y) over
sensations y, the nervous system can find the posterior distribution of the hidden states given the sensations via Bayes
rule:
p(x|y) = p(x, y)
p(y) (1)
However, direct computation of the posterior p(x|y) is unfeasible since the evidence requires marginalizing over every
possible outcome, i.e., p(y) =
R
p(x, y)dx. Predictive coding supposes that, instead of performing exact Bayesian
inference, organisms are engaged in a variational approach [42], i.e., they approximate the posterior distribution with
a simpler recognition distribution q(x) ≈ p(x|y), and minimize the difference between the two distributions. This
2
difference is expressed in terms of a Kullback-Leibler (KL) divergence:
DKL[q(x)||p(x|y)] =
Z
x
q(x) ln q(x)
p(x|y)dx (2)
Given that the denominator p(x|y) still depends on the marginal p(y), we express the KL divergence in terms of the
log evidence and the Variational Free Energy(VFE), and minimize the latter quantity instead. The VFE is the negative
of what in the machine learning community is known as the evidence lower bound or ELBO [43]:
F = E
q(x)

ln q(x)
p(x, y)

= E
q(x)

ln q(x)
p(x|y)

− ln p(y) (3)
Since the KL divergence is always nonnegative, the VFE provides an upper bound on surprise, i.e., F ≥ −ln p(y).
Therefore, minimizing F is equivalent to minimizing the KL divergence with respect to q(x). The more the VFE
approaches 0, the closer the approximate distribution is to the real posterior, and the higher the model evidence (or,
equivalently, the lower the surprise about sensory outcomes will be). The discrepancy between the two distributions also
depends on the specific assumptions made over the recognition distribution: a common one is the Laplace approximation
[42], which assumes Gaussian probability distributions, e.g., q(x) = N(µ, Σ), where µ represents the most plausible
hypothesis - also called belief about the hidden states x - and Σ is its covariance matrix. Now, we factorize the
generative model p(x, y) into likelihood and prior terms, and we further parameterize it with some parameters θ:
p(x, y) = p(y|x, θ)p(x)
p(y|x, θ) = N(g(x, θ), Σy)
p(x) = N(η, Ση)
(4)
where g(x, θ) is a likelihood function and η is a prior. In this way, after applying the logarithm over the Gaussian
terms, the VFE breaks down to the following simple formula:
F = −1
2

Πyε2
y + Πηε2
η + ln 2πΣy + ln 2πΣη

(5)
where we expressed the difference in the exponents of the Gaussian distributions in terms of prediction errors
εy = y − g(µ, θ) and εη = µ − η, and we wrote the covariances in terms of their inverse, i.e., precisions Πy and
Πη. In order to minimize the KL divergence between the real and approximate posteriors, we can minimize the VFE
with respect to the beliefs µ – a process associated with perception – and the parameters θ – generally referred to as
learning. In practice, the update rules follow gradient descent:
µ = arg min
µ
F
˙µ = −∂µF = ∂µgT Πyεy − Πηεη
θ = arg min
θ
F
˙θ = −∂θF = −∂θgT Πyεy
(6)
In order to separate the timescales of fast perception and slow-varying learning, the two phases are treated as the steps
of an EM algorithm, i.e., optimizing the beliefs while keeping the parameters fixed, and then optimizing the parameters
while keeping the beliefs fixed.
Predictive coding, originally rooted in data compression [ 44], has inspired biologically plausible alternatives to
traditional deep learning, such as Predictive Coding Networks (PCNs) [45, 46]. In fact, the predictive coding algorithm
can be scaled up to learn highly complex structures, composed of causal relationships of arbitrarily high depth – in
a similar way to deep neural networks. In particular, we can factorize a generative model into a product of different
distributions, wherein a specific level only depends on the level above:
p(x(0), . . . ,x(l)) = p(x(0))
LY
l=1
p(x(l)|x(l−1))
p(x(l)|x(l−1)) = N(g(x(l−1), θ(l−1)), Σ(l))
(7)
where 0 indicates the highest level, andL is the number of levels in the hierarchy. In this way,x(l) acts as an observation
for level l−1, and g(x(l), θ(l)) acts as a prior for levell+1. As a result, this factorization allows us to express the update
of beliefs and parameters of a specific level only based on the level above and the level below – with a simple VFE
3
minimization as in the previous case. Differently from the backpropagation algorithm of neural networks, the message
passing of prediction errors implements a biologically plausible Hebbian rule between presynaptic and postsynaptic
activities. In fact, the predictions u computed by the likelihood functions are typically a weighted combination of
neurons passed to a nonlinear activation function ϕ:
u(l)
i =
JX
j=1
W(l−1)
i,j ϕ(x(l−1)
j ) (8)
where W(l−1)
i,j are the weights from neuron j at level l − 1 to neuron i at level l. Crucially, what is backpropagated
in PCNs are not signals detecting increasingly complex features, but messages representing how much the model is
surprised about sensory observations (e.g., a prediction equal to an observation means that the network structure is a
good approximation of the real process, and no errors have to be conveyed).
2.2 Hierarchical active inference
The theory of active inference builds on the same assumptions of predictive coding, but with two critical differences.
The first one – which is actually shared with some implementations of predictive coding such as temporal PC [47] –
assumes that living organisms constantly deal with highly dynamic environments, and the internal models that they
build must reflect the changes occurring in the real generative process [48]. This relation is usually expressed in terms
of generalized coordinates of motion (encoding, e.g., position, velocity, acceleration, and so on) [49]; consequently, the
environment is modeled with the following nonlinear system:
˜y = ˜g(˜x) + wy
D˜x = ˜f(˜x, ˜v) + wx
(9)
where ˜x are the generalized hidden states, ˜v are the generalized hidden causes, ˜y are the generalized sensory signals,
D is a differential operator that shifts all the temporal orders by one, i.e.: D˜x = [x′, x′′, x′′′, . . .], and the letter w
indicates (Gaussian) noise terms. The likelihood function ˜g defines how hidden states generate sensory observations (as
in predictive coding), while the dynamics function ˜f specifies the evolution of the hidden states (see [50, 51] for more
details). The associated joint probability is factorized into independent distributions:
p(˜y, ˜x, ˜v) = p(˜y|˜x)p(˜x|˜v)p(˜v) (10)
where each distribution is Gaussian:
p(˜y|˜x) = N(˜g(˜x), ˜Σy)
p(D˜x|˜v) = N( ˜f(˜x, ˜v), ˜Σx)
p(˜v|η) = N(η, ˜Σv)
(11)
As in predictive coding, these distributions are inferred through approximate posteriors q(˜x) and q(˜v), minimizing the
related VFE F. As a result, the updates of the beliefs ˜µ and ˜ν respectively over the hidden states and hidden causes
become:
˙˜µ − D˜µ = −∂µF = ∂˜gT ˜Πy ˜εy + ∂µ ˜fT ˜Πx ˜εx − DT ˜Πx ˜εx
˙˜ν − D˜ν = −∂νF = ∂ν ˜fT ˜Πx ˜εx − ˜Πv ˜εv
(12)
where ˜Πy, ˜Πx, and ˜Πv are the precisions, and ˜εy, ˜εx, and ˜εv are respectively the prediction errors of sensory signals,
dynamics, and priors:
˜εy = ˜y − ˜g(˜µ)
˜εx = D˜µ − ˜f(˜µ, ˜ν)
˜εv = ˜ν − η
(13)
For a full account of free energy minimization in active inference, see [ 8]. Unlike the update rules of predictive
coding, additional terms arise from the inferred dynamics, through which the model can capture the evolution of the
4
environment. Also note that since we are minimizing over (dynamic) paths and not (static) states, an additional termD˜µ
is present: this implies trajectory tracking, during which the VFE is minimized only when the belief of the generalized
hidden states D˜µ matches its instantaneous trajectory ˙˜µ.
The second assumption made by active inference is that our brains not only perceive (and learn) the external generative
process, but also interact with it to reach desired states (e.g., in order to survive, not only one must understand which
cause leads to an increase or decrease in temperature, but also take actions to live in a narrow range around 37 degrees
Celsius). The free energy principle, which is at the core of active inference, states that all living organisms act to
minimize the free energy (or, equivalently, surprise). In fact, in addition to the perceptual inference of predictive
coding, the VFE can be minimized by sampling those sensory observations that conform to some prior beliefs, e.g.,
a = arg mina F, where a are the motor commands. This process is typically called self-evidencing, implying that if I
believe to find myself in a narrow range of temperatures, minimizing surprise via action will lead to finding places with
such temperatures – hence making my beliefs true. One role of the hidden causes is to define the agent’s priors that
ensure survival. These priors generate proprioceptive predictions which are suppressed by motor neurons via classical
reflex arcs [52]:
˙a = −∂aFp = −∂a ˜yp ˜Πp ˜εp (14)
where ∂a ˜yp is an inverse model from (proprioceptive) observations to actions, and ˜εp = ˜yp − ˜gp(˜µ) are the generalized
proprioceptive prediction errors.
As in predictive coding, we can scale up this generative model to capture the causal relationships of several related
entities (e.g., the joints of a kinematic structure), up to a certain depth [37, 19, 53, 36]. Therefore, the prior becomes the
prediction from the layer above, while the observation becomes the likelihood of the layer below:
˙˜µ(l) = D˜µ(l) + ∂˜g(l)T ˜Π(l)
v ˜ε(l)
v + ∂µ ˜f(l)T ˜Π(l)
x ˜ε(l)
x − DT ˜Π(l)
x ˜ε(l)
x
˙˜ν(l) = D˜ν(l) + ∂ν ˜f(l)T ˜Π(l)
x ˜ε(l)
x − ˜Π(l−1)
v ˜ε(l−1)
v
(15)
where:
˜ε(l)
x = D˜µ(l)
x − ˜f(l)(˜µ(l), ˜ν(l))
˜ε(l)
v = ˜µ(l+1)
v − ˜g(l)(˜µ(l))
(16)
and the subscript indicates the index of the level, as before. Here, the role of the hidden causes is to link hierarchical
levels. This allows the agent to construct a hierarchy of representations varying at different temporal scales, critical for
realizing richly structured behaviors such as linguistic communication [ 20] or singing [54]. In this case, the motor
commands minimize the generalized prediction errors of the lowest level of the hierarchy.
This continuous formulation of active inference is highly effective for dealing with realistic environments. However,
minimizing the VFE (which is the free energy of the past and present) does not afford planning and decision-making in
the immediate future. To do this, we construct a discrete generative model in which we discretize the possible future
states and encode their expectations with categorical distributions. We then minimize the Expected Free Energy (EFE)
which, as the name suggests, is the free energy that the agents expect to perceive in the future [55, 8]. This discrete
generative model is similar to the continuous model defined above, with the difference that we condition the hidden
states over policies π (which in active inference are sequences of discrete actions):
p(s, o, π) = p(o|s, π)p(s|π)p(π) (17)
Here, s and o are discrete states and outcomes, which represent past, present, and future states as in Hidden Markov
Models. Then, the EFE is specifically constructed by considering future states as random variables that need to be
inferred:
Gπ = E
q(s,o|π)

ln q(s|π)
p(s, o|π)

≈ E
q(s,o|π)

ln q(s)
q(s|o, π)

− E
q(o|π)
[ln p(o|C)] (18)
where q(o|π) is a recognition distribution that the agent constructs to infer the real posterior of the generative process.
Critically, the probability distribution p(o|C) encodes preferred outcomes, acting similar to a prior over the hidden
causes in the continuous counterpart. The last two terms are respectively called epistemic (uncertainty reducing) and
pragmatic (goal seeking). In practice, this quantity is used by first factorizing the agent’s generative model as in
5
POMDPs:
p(s1:T , o1:T , π) = p(s1) · p(π) ·
TY
τ=1
p(oτ |sτ ) ·
TY
τ=2
p(sτ |sτ−1, π) (19)
Each of these elements can be represented with categorical distributions:
p(s1) = Cat(D)
p(π) = Cat(E)
p(oτ |sτ ) = Cat(A)
p(sτ |sτ−1, π) = Cat(Bπ,τ ) (20)
where D encodes beliefs about the initial state, E encodes the prior over policies, A is the likelihood matrix and Bπ,τ
is the transition matrix. The minimization of EFE in discrete models follows the variational method used by predictive
coding and continuous-time active inference; specifically, the use of categorical distributions breaks down the inference
of hidden states and policies to a simple local message passing:
sπ,τ = σ(ln Bπ,τ−1sπ,τ−1 + lnBT
π,τ sπ,τ+1 + lnAT oτ )
π = σ(ln E − G)
Gπ ≈
X
τ
Asπ,τ (ln Asπ,τ − ln p(oτ |C)) − sπ,τ diag(AT ln A)
(21)
For a complete treatment of how the EFE and the approximate posteriors are computed – along with other insightful
implications of the free energy principle in discrete models – see [50], while [56] provides a more practical tutorial with
basic applications. Equation 21 shows that the update rule for the discrete hidden states at time τ and conditioned over
a policy π is a combination of messages coming from the previous and next discrete time steps, and a message coming
from the discrete outcome. This combination is passed to a softmax function in order to get a proper probability. In
addition, the optimal policy π is found by a combination of a policy prior and the EFE, where the latter is a composition
of epistemic and pragmatic behaviors. Policy inference can be refined by computing the EFE of several steps ahead in
the future – a process called sophisticated inference. Then, at each discrete step τ, the agents selects the most likely
action u under all policies, i.e., ut = arg maxu π · [Uπ,t = u].
As before, we can construct a hierarchical structure able to express more and more invariant representations of hidden
states [57]. In discrete state-space, links between hierarchical levels are usually done between hidden states via the
matrix D – although in some formulations the hidden states of a level condition the policies of the subordinate levels.
The update rules for this hierarchical alternative become:
s(l)
π,τ = σ(ln B(l)
π,τ−1s(l)
π,τ−1 + lnB(l)T
π,τ s(l)
π,τ+1 + lnA(l)T o(l)
τ + lnD(l+1)T s(l+1)
1 )
π(l) = σ(ln E(l) − G(l))
G(l)
π ≈
X
τ
A(l)s(l)
π,τ (ln A(l)s(l)
π,τ − ln p(o(l)
τ |C)) − s(l)
π,τ diag(A(l)T ln A(l))
(22)
This implies that each level takes abstract actions to minimize its EFE, only depending on the levels immediately below
and above.
2.3 Bayesian model comparison
Bayesian model comparison is a technique used to compare a posterior over some data with a few simple hypotheses
known a-priori [58]. Consider a generative model p(y, θ) with parameters θ and data y:
p(θ, y) = p(y|θ)p(θ) (23)
We introduce additional distributions p(y, θ|m) which are reduced versions of the first model if the likelihood of some
data is the same under both models – i.e., p(y|θ, m) = p(y|θ) – and the only difference rests upon the specification of
the priors p(θ|m). We can express the posterior of the reduced models in terms of the posterior of the full model and
the ratios of the priors and the evidence:
p(θ|y, m) = p(θ|y)p(θ|m)p(y)
p(θ)p(y|m) (24)
6
The procedure for computing the reduced posteriors is the following: first, we integrate over the parameters to obtain
the evidence ratio of the two models:
p(y|m) = p(y)
Z
p(θ|y)p(θ|m)
p(θ) dθ (25)
Then, we define an approximate posterior q(θ) and we compute the reduced free energies of each model m:
F[p(θ|m)] ≈ F[p(θ)] + lnE
q
p(θ|m)
p(θ)

(26)
This VFE acts as a hint to how well the reduced representation explains the full model. Similarly, the approximate
posterior of the reduced model can be written in terms of the posterior of the full model:
ln q(θ|m) = ln q(θ) + ln p(θ|m)
p(θ) − ln E
q
p(θ|m)
p(θ)

(27)
The Laplace approximation [ 59] leads to a simple form of the approximate posterior and the reduced free energy.
Assuming the following Gaussian distributions:
p(θ) = N(η, Σ)
p(θ|m) = N(ηm, Σm)
q(θ) = N(µ, C)
q(θ|m) = N(µm, Cm) (28)
the reduced free energy turns to:
F[p(θ|m)] ≈ F[p(θ)] + 1
2 ln |ΠmPCmΣ|
− 1
2(µT Pµ − µT
mPmµm − ηT Πη + ηT
mΠmηm)
(29)
expressed in terms of precision of priors Π and posteriors P. The reduced posterior mean and precision are computed
via Equation 27:
µm = Cm(Pµ − Πη + Πmηm)
Pm = P − Π + Πm
(30)
In this way, two different hypotheses i and j can be easily compared to infer which is the most likely to have generated
the observed data; this comparison has the form of a log-Bayes factor F[p(θ|i)] − F[p(θ|j]. For a more detailed
treatment of Bayesian model comparison (in particular under the Laplace approximation), see [58, 60].
3 Results
3.1 Deep hybrid models
Analyzing the emergence of distributed intelligence, Friston et al. [ 61] emphasized three kinds of depth within the
framework of active inference: factorial, hierarchical, and temporal. Factorial depth assumes independent factors in
the agent’s generative model (e.g., objects and qualities of an environment, or more abstract states), which can be
combined to generate outcomes and transitions. Hierarchical depth introduces causal relationships between levels,
inducing a separation of temporal scales whereby higher levels happen to construct more invariant representations,
while lower levels better capture the rapid changes of sensory stimuli. Temporal depth entails, in discrete terms, a vision
into the imminent future that can be used for decision-making; or, in continuous terms, increasingly precise estimates of
dynamic trajectories.
In the following, we present the main features of a deep hybrid model in terms of factorial, temporal, and hierarchical
depths in the context of flexible behavior, iterative transformations of reference frames, and dynamic planning. By deep
hybrid model, we intend an active inference model composed of hybrid units connected hierarchically. Here, hybrid
means that discrete and continuous representations are encoded within each unit, wherein the communication between
the two domains is achieved by Bayesian model reduction [ 60, 58]. As a technical note, all the internal operations
can be computed through automatic differentiation, i.e., by maintaining the gradient graph when performing each
7
Figure 1: (a) Factor graph of a hybrid unit. Continuous hidden states ˜x generate predictions ˜y through parallel pathways.
Model dynamics is encoded by potential trajectories fm, which are hypotheses of how the world may evolve and are
associated with discrete hidden causes v. (b) Illustrative example of a hybrid unit. In this task, the agent has to infer
which one among two objects (a red circle and a gray square moving along a circular trajectory) is being tracked by
another 1-DoF agent. The time step is shown in the bottom left of each frame. The hidden states ˜x encode the angle
and angular velocity of the arm (generating proprioceptive predictions), as well as the positions and velocities of the
two objects (generating visual predictions). The blue arrow represents the actual hand trajectory, while the red and
green arrows represent the two potential trajectories associated with reaching movements toward the two objects. See
[62] for more details.
forward pass and propagating back the prediction errors. For a detailed treatment of predictive coding, hierarchical
active inference in discrete and continuous state-spaces, and Bayesian model comparison, see Sections 2.1, 2.2, and 2.3,
respectively.
3.1.1 Factorial depth and flexible behavior
Consider the case where one needs to infer which object is being followed by an agent. This can be done through a
hybrid unit U, whose factor graph is depicted in Figure 3.1.1. The variables are: continuous hidden states ˜x = [x, x′],
observations ˜y = [y, y′], and (discrete) hidden causes v. Hidden states and observations comprise two temporal orders
(e.g., x encodes the position and x′ the velocity), and the first temporal order will be indicated as the 0th order – see the
Methods section for more information. The factors are: dynamics functions f, and likelihood functions ˜g = [g, g′].
Note also a prior over the 0th-order hidden states ηx, and a prior over the hidden causes Hv encoding the agent’s goals.
The generative model is the following:
p(˜x, v, ˜y) = p(˜y|˜x)p(x′|x, v)p(x)p(v) (31)
We assume that hidden causes and hidden states have different dimensions, resulting in two factorizations. The hidden
states, with dimension N, are sampled from independent Gaussian distributions and generate predictions in parallel
pathways:
p(x) =
NY
n
N(ηx,n, Ση,x,n) p(˜y|˜x) =
NY
n
N(˜gn(˜xn), ˜Σy,n) (32)
where Ση,x,n and ˜Σy,n are their covariance matrices. In turn, the hidden causes, with dimension M, are sampled from
a categorical distribution:
p(v) = Cat(Hv) (33)
This differs from state-of-the-art hybrid architectures which assume separate continuous and discrete models with
continuous hidden causes (see Section 2.2) [48]. A discrete hidden cause vm concurs, with hidden states x, in generating
8
a specific prediction for the 1st temporal order x′:
p(x′|x, m) = N(fm(x), Σx,m) (34)
This probability distribution entails a potential trajectory, or hypothetical evolution of the hidden states, which the
agent maintains to infer the state of affairs of the world and act. More formally, we consider p(x′|x, m) as being the
mth reduced version of a full model:
p(x′|x, v) = N(η′
x, Σx) (35)
This allows us – using the variational approach for approximating the true posterior distributions – to convert discrete
signals into continuous signals and vice versa through Bayesian model average and Bayesian model comparison,
respectively (see Section 2.3 and [60, 58]). In particular, top-down messages combine the potential trajectories fm(x)
with the related probabilities encoded in the discrete hidden causes v:
η′
x =
MX
m
vmfm(x) (36)
This computes a dynamic path that is an average of the agent’s hypotheses, based on its prior Hv. Conversely,
bottom-up messages compare the agent’s prior surprise −ln Hv with the log evidence lm of every reduced model,
i.e., v = σ(ln Hv + l), where l = [l1, . . . , lM ] and σ is a softmax function. The log evidence is accumulated over a
continuous time T:
lm =
Z T
0
1
2(µ′T
m Px,mµ′
m − fm(x)T Πx,mfm(x) − µ′T Pxµ′ + η′T
x Πxη′
x)dt (37)
where µm, Px,m, and Πx,m are the mean, posterior precision, and prior precision of the mth reduced model. In this
way, the agent can infer which dynamic hypothesis is most likely to have generated the perceived trajectory – see Figure
3.1.1 and [62] for more details about this approach.
A hybrid unit has useful features deriving from the factorial depths of hidden states and causes. Consider the case where
the hidden states encode the agent’s configuration and other environmental objects, while the hidden causes represent
the agent’s intentions. A hybrid unit could dynamically assign the causes of its actions at a particular moment: this is
critical, e.g., in a pick and place operation, during which an object is first the cause of the hand movements – resulting
in a picking action – but then it is the consequence of another cause (i.e., a goal position) – resulting in a placing
action. This approach differs from other solutions [63, 6] that directly encode a target location in the hidden causes.
Further, embedding environmental entities – and not just the self – into the hidden states permits inferring their dynamic
trajectories, which is fundamental for interactions, e.g., in catching objects on the fly [ 64] or in tracking a hidden
target with the eyes [65]. Considering the example in Figure 3.1.1, the hidden states x and x′ may encode the angle
and angular velocity of an agent’s arm, and two hidden causes vcircle and vsquare may be associated with dynamics
functions fcircle and fsquare encoding (potential) reaching movements toward the two objects. The environment
may be inferred by two likelihood functions, one – gp – predicting proprioceptive observations (i.e., arm and angular
velocity), and another one – gv – predicting visual observations (i.e., the hand position and velocity). Since we are
interested in inferring which object is being followed by the agent, we set a uniform discrete prior Hv. Then, Equation
37 compares the actual agent’s trajectory to the two reaching movements toward the two objects, and assigns a higher
probability to the one better resembling the real trajectory.
3.1.2 Hierarchical depth and iterative transformations
Hierarchical depth is critical in many tasks that require learning of modular and flexible functions. Considering motor
control, forward kinematics is repeated throughout every element of the kinematic chain, computing the end effector
position from the body-centered reference frame. Iterative transformations are also fundamental in computer vision,
where camera models perform roto-translations and perspective projections in sequence. How can we express such
hierarchical computations in terms of inference? We design a structure called Intrinsic-Extrinsic (or IE) module,
performing iterative transformations between reference frames [36, 66]. A unit U(i)
e – where the superscript indicates
the ith level – encodes a signal x(i)
e in an extrinsic reference frame (e.g., Cartesian coordinates), while another unit
U(i)
i represents an intrinsic signal x(i)
i (e.g., polar coordinates). At each level i, a likelihood function g(i)
e applies a
transformation to the extrinsic signal provided by the higher level based on the intrinsic information, and returns a new
9
Figure 2: (a) An IE module is composed of two units Ui and Ue, which represent a signal in intrinsic and extrinsic
reference frames, respectively. Different IE modules can be combined in a hierarchical fashion: the extrinsic signal
x(i)
e is iteratively transformed through linear transformation matrices encoded in the extrinsic likelihood function g(i)
e .
Hierarchical levels communicate via the 0th-order hidden states. (b) Illustrative examples of a hierarchical model with
IE modules. In the first task, the agent (a 23-DoF human body) has to avoid a moving obstacle; in the second task, the
agent (a 28-DoF kinematic tree) has to reach four target locations with the extremities of its branches. In both cases,
the module in (a) is repeated for every DoF of the agents, matching their kinematic structures. Proprioceptive and
exteroceptive (e.g., visual) for each DoF are respectively generated by the intrinsic and extrinsic units via appropriate
likelihood functions. See [36] for more details.
extrinsic state:
x(i)
e = g(i)
e (x(i)
i , x(i−1)
e ) + we = T(i)(x(i)
i ) · x(i−1)
e + we (38)
where we is a noise term and T(i) is a linear transformation matrix. This new state acts as a prior for the subordinate
levels in a multiple-output system; indicating with the superscript (i, j) the ith hierarchical level and the jth unit within
the same level, we link the IE modules in the following way:
y(i,j)
e ≡ x(i+1,j)
e x(i−1,j)
e ≡ η(i,j)
e (39)
as displayed in Figure 2; hence, the observation of level i becomes the prior over the hidden states of level i + 1.
Ill-posed problems that generally have multiple solutions – such as inverse kinematics or depth estimation – can be
solved by inverting the agent’s generative model and backpropagating the sensory prediction errors, with two additional
features compared to traditional methods: (i) the possibility of steering the optimization by imposing appropriate priors,
e.g., for avoiding singularities during inverse kinematics; (ii) the possibility of acting over the environment to minimize
uncertainty, e.g., with motion parallax during depth estimation.
Encoding signals in intrinsic and extrinsic reference frames also induces a decomposition over proprioceptive and
exteroceptive predictions, as well as intrinsic and extrinsic dynamics functions, leading to simpler (and yet richer)
attractor states. Figure 3.1.2 shows two examples of goal-directed behavior with complex kinematic structures – a
28-DoF kinematic tree and a 23-DoF human body. In these cases, an IE module of Figure 3.1.2 is employed for
each DoF of the agent. Each IE module encodes the position and velocity of a specific limb, both in an extrinsic
(e.g., Cartesian) – x(i)
e and x(i)′
e – and intrinsic (e.g., polar) – x(i)
i and x(i)′
i – reference frames. These modules are
connected hierarchically, i.e., the trunk position generates predictions for the positions of both arms and legs through
the likelihood function g(i)
e computing forward kinematics. The goal-directed behavior of the kinematic tree is realized
by defining four reaching dynamics functions (toward the red objects) at the last levels of the hierarchy representing
the end effectors. Instead, the behavior of the human body is achieved by defining repulsive dynamics functions for
the extrinsic reference frames of each IE module. The decomposition of independent dynamics is also useful for tasks
requiring multiple constraints in both domains, e.g., when walking with a glass in hand [36], and it has also been applied
10
to controlling robots in 3D environments [67]; or for estimating the depth of an object by moving the eyes [66]. Further,
the factorial depth previously described permits representing hierarchically not only the self, but also the objects in
relation to the self, along with the kinematic chains of other agents [68]. In this way, an agent could maintain a potential
body configuration whenever it observes a relevant entity. This representation also accounts for the affordances of
objects to be manipulated, and can be realized efficiently as soon as necessary.
3.1.3 Temporal depth and dynamic planning
Consider the following discrete generative model:
p(s1:τ , o1:τ , π) = p(s1)p(π)
Y
τ
p(oτ |sτ )p(sτ |sτ−1, π) (40)
where:
p(s1) = Cat(D)
p(π) = σ(−G)
p(oτ |sτ ) = Cat(A)
p(sτ+1|sτ , π) = Cat(Bπ,τ ) (41)
Here, A, B, D are the likelihood matrix, transition matrix, and prior, π is the policy, sτ are the discrete hidden states
at time τ, oτ are discrete observations, and G is the expected free energy (see Section 2.2 for more details).
We can let the likelihood p(oτ |sτ ) directly bias the (discrete) hidden causes of a hybrid unit:
Hτ ≡ Asτ oτ ≡ vτ (42)
Hence, the prior Hτ over the discrete hidden causes becomes the prediction by the discrete model, while the discrete
observation becomes the discrete hidden causes of the hybrid unit. This is an alternative method to the state-of-the-art,
which considers an additional level between discrete observations and static priors over continuous hidden causes [37].
Here, the discrete model can impose priors over trajectories even in the same period τ, thus affording dynamic planning
[62, 68]. If a discrete model is linked to different hybrid units in parallel – as shown in Figure 3 – the discrete hidden
states are inferred by combining multiple evidences:
sπ,τ = σ(ln Bπ,τ−1sπ,τ−1 + BT
π,τ+1sπ,τ+1 +
X
n
ln A(i)T
v(i)
τ ) (43)
where sπ,τ are the discrete hidden states conditioned over policy π at time τ, while the superscript i indicates the ith
hybrid unit. These parallel pathways synchronize the behavior of all low-level units based on the same high-level plan,
allowing, e.g., simultaneous coordination of every limb of the human body.
In Figure 3.1.3, we notice the two kinds of temporal depths, peculiar to hybrid active inference. The first one comes
from the discrete component and unfolds over future states (s1, s2, s3, . . .) over a time horizon defined by the policy
length: it allows the agent to make plans by computing the expected free energy of those states [ 69]. The second
temporal depth derives from the continuous level and unfolds over the temporal derivatives of the hidden states, i.e.,
(x, x′, x′′, . . .): this refines the estimated trajectories with an increasing sampling rate.
In addition to this, the overall deep hybrid model presents two hierarchical depths with different roles. First, a hybrid
scale that separates the slow-varying representation of the discretized task with the fast update of continuous signals.
Here, the temporal predictions from the continuous dynamics are used to infer accurately the discrete variables, so that
the agent can make complex high-level plans even when the surrounding environment is changing frequently, and is
able to revise those plans when new evidence has been accumulated. Second, a continuous scale linking the hybrid
units and inherent to the hierarchical representation of the agent’s kinematic structure, which can be appreciated from
Figure 2. This induces a separation of temporal scales between high and low levels of the hierarchy (e.g., the trunk vs
the hands), as the predictions errors generated from the dynamics of the hand have a less and less impact as they flow
back to the shoulder and trunk dynamics.
Considering the pick-and-place operation shown in Figure 3.1.3, we can encode the three key moments of the task
(start position, ball picked, and ball placed) in terms of discrete hidden states s. At each discrete step τ, these states
make (intrinsic and extrinsic) predictions for the hybrid units representing the agent’s kinematic chain (as in Figure
2). For instance, the second discrete hidden state generates a potential body configuration with the hand closed and at
the ball position. We can use an identity mapping for the likelihood matrices, so that the intrinsic and extrinsic hidden
11
Figure 3: (a) Interface between a discrete model and several hybrid units. The hidden causes v(i) are directly generated,
in parallel pathways, from discrete hidden states sτ via likelihood matrices A(i). (b) Illustrative example with the
hybrid units combined with a discrete model. In this task, the agent (a 4-DoF arm with an additional 4-DoF hand
composed of two fingers) has to pick a moving ball (the red circle) and place it at a goal position (the grey square).
The discrete hidden states sτ encode the agent position (start position, at the ball, or at the goal) and the status of the
hand (open or closed). These are informed by two continuous models encoding intrinsic (joint angles) and extrinsic
(hand and objects positions) information, respectively. The hidden causes v of the intrinsic model are related to hand
opening and closing actions, while the hidden causes of the extrinsic model relate to two reaching movements, as in
the previous case. Note that the object belief (purple circle) is rapidly inferred, and as soon as the picking action is
complete, the belief is gradually pulled toward the goal position, resulting in a second reaching movement. The top
right panel shows the hand-object distance over time, while the bottom right panel displays the dynamics of the discrete
action probabilities used to infer the next discrete state. The vertical dashed lines distinguish five different phases:
a pure reaching movement, an intermediate phase when the agent prepares the grasping action, a grasping phase, a
second reaching movement and, finally, the ball release. The stepped behavior of the action probabilities is due to the
replanning made by the discrete model every 10 continuous time steps. See [64] for more details.
12
Figure 4: (a) Virtual environment of the tool use task. An agent controlling a 4-DoF arm has to grasp a moving tool (in
green) and reach a moving ball (in red) with the tool’s extremity. (b) Agent’s beliefs over the continuous hidden states
of the arm (blue), tool (light green), and ball (light red). The real positions of the tool and ball are represented in dark
green and dark red, respectively. The virtual level is plotted with more transparent colors. (c) Graphical representation
of the agent’s continuous generative model. Every environmental entity is encoded hierarchically by considering the
whole arm’s kinematic structure. For clarity, the three pathways are displayed separately, while lateral connections
and the high-level discrete model are not shown. The end effector’s level encodes intrinsic and extrinsic information
about the end effector, regarding the three configurations (the actual end effector position, the belief over the end
effector at the tool’s origin, or at an appropriate position to reach the ball with the tool’s extremity). Instead, the virtual
level is not present in the actual configuration, since the tool is not part of the agent’s kinematic chain and it is only
used in the generative model for goal-directed behavior – as if it were a new joint. This level encodes intrinsic and
extrinsic information about the tool, regarding the two potential configurations (the belief over the tool’s extremity at
the actual tool’s extremity, and at the actual ball position.) Small purple and yellow circles represent proprioceptive and
exteroceptive observations, respectively.
causes of the hybrid units all have the same decomposition into three steps – i.e., v(i)
i = [v(i)
i,start, v(i)
i,picked, v(i)
i,placed]
and v(i)
e = [v(i)
e,start, v(i)
e,picked, v(i)
e,placed]. Notably, since these hidden causes are related to potential trajectories, the
agent can pick and place the ball even in dynamic contexts, e.g., if the ball is moving.
3.2 A deep hybrid model for tool use
In this section, we show how a deep hybrid model can be used efficiently in a task that requires planning in a dynamic
environment and coordination of all elements of the agent’s body. The implementation details are found hereafter in
Section 3.2.1, while Appendix A illustrates the algorithms for the inference of the discrete model and hybrid units. Then,
in Section 3.3 we analyze model performance and describe the effects of dynamic planning in terms of accumulated
sensory evidence and transitions over discrete hidden states.
Reaching an object with a tool is a complex task that requires all the features delineated in the previous section. First,
the task has to be decomposed into subgoals – reaching the tool and reaching the object – which requires high-level
discrete planning. Second, the agent has to maintain distinct beliefs about its arm, the tool, and the object, all of which
must be inferred from sensory observations if their locations are unknown or constantly changing. Third, if the tool has
to be grasped at the origin while the object has to be reached with the tool’s extremity, the agent’s generative model
should encode a hierarchical representation of the self and every entity, and specify goals (in the form of attractors) at
different levels of the hierarchy.
As shown in the graphical representation of the virtual environment of Figure 3.2, the agent controls an arm of 4
DoF. The agent receives proprioceptive information about its joint angles, and exteroceptive (e.g., visual) observations
13
encoding the positions of its limbs, the tool, and the ball. For simplicity, we assume that the tool sticks to the agent’s
end effector as soon as it is touched. The generative model provides an effective decomposition into three parallel
pathways, displayed in Figure 3.2: one maintaining an estimate of the agent’s actual configuration (indicated by the
subscript 0), and two others representing potential configurations in relation to the tool and the ball (respectively
indicated by the subscripts t and b). In other words, the objects of interest are not just encoded by their qualities or
location, but already define a body configuration appropriate to achieve a specific interaction. In our case, the potential
configuration related to the tool represents not only the estimated tool’s location, but also the estimated end effector’s
location needed to reach the tool. In addition, each body configuration is composed of as many IE modules as the
agent’s DoF, following the hierarchical relationships of the forward kinematics and allowing to express both joint angles
and limb positions. Message passing of extrinsic prediction errors, i.e., differences between the estimated limb positions
and their predictions given by the estimated joint angles, allows to infer the whole body configuration (either actual
or potential) via exteroceptive observations. In addition, every component of a level exchanges lateral messages with
the other components, in the form of dynamics prediction errors. These errors are caused by the potential dynamics
functions described in Section 3.1.1, which define the interactions between entities for goal-directed behavior.
Two crucial aspects arise when modeling entities in a deep hierarchical fashion and related to the self. First, different
configurations are inferred (hence, different movements) depending on the desired interaction with the object considered
– for example, reaching a ball with either the elbow or the end effector. Second, entities could have their own hierarchical
structures: in our application, the tool consists of two Cartesian positions and an orientation, and the agent should
somehow represent this additional link. For these reasons, we consider a virtual level for the tool configuration, attached
to the last IE module (i.e., end effector), as exemplified in Figure 3.2; the visual observations of the tool are then linked
to the last two levels. From these observations, the correct tool angle can be inferred as if it were a new joint angle of
the arm. Additionally, since we want the agent to touch the ball with the tool’s extremity, we model the third (ball)
configuration with a similar structure, in which a visual observation of the ball is attached to the virtual level. The
overall architecture can be better understood from Figure 3.2, showing the agent’s continuous beliefs of all three entities.
As soon as the agent perceives the tool, it infers a possible kinematic configuration as if it had visual access only to its
last two joints (which are actually the tool’s origin and extremity). Likewise, perceiving the ball causes the agent to find
an extended kinematic configuration as if the tool were part of the arm.
With this task formalization, specifying the correct dynamics for goal-directed behavior is simple. First, we define
two sets of dynamics functions implementing every subgoal, one for reaching the tool and another for reaching the
ball with the tool’s extremity. As explained in Section 3.2.1, the second subgoal requires specifying an attractor at the
virtual level, which makes the agent think that the tool’s extremity will be pulled toward the ball. The biased state
generates an extrinsic prediction error that is backpropagated to the previous level encoding the tool’s origin and the
end effector. Notably, defining discrete hidden states and discrete hidden causes related to the agent’s intentions allows
the agent to accumulate evidence over trajectories from different modalities (e.g., intrinsic or extrinsic) and hierarchical
locations (e.g., elbow or end effector), ultimately solving the task via inference. The four main processes of the task –
i.e., perception, dynamic inference, dynamic planning, and action – are summarized in Figure 5.
3.2.1 Implementation details
The agent’s sensory modalities are: (i) a proprioceptive observationyp for the arm’s joint angles; (ii) a visual observation
yv encoding the Cartesian positions of every link of the arm, both extremities of the tool, and the ball; (iii) a discrete
tactile observation ot signaling whether or not the target is grasped.
We decompose intrinsic and extrinsic hidden states of every IE module into three components, the first one corresponding
to the actual arm configuration, and the other two related to potential configurations for the tool and the ball. Hence:
x(i)
i =
h
x(i)
i,0 x(i)
i,t x(i)
i,b
i
x(i)
e =
h
x(i)
e,0 x(i)
e,t x(i)
e,b
i (44)
The end effector’s level and the virtual level (see Figure 3.2) are indicated with the superscripts(4) and (5), respectively.
Regarding the IE module of the virtual level, the intrinsic and extrinsic hidden states only have two components related
to the potential states of the tool and the ball, i.e., x(5)
i = [x(5)
i,t , x(5)
i,b ] and x(5)
e = [x(5)
e,t , x(5)
e,b].
14
Figure 5: Graphical representation of a deep hybrid model for tool use, composed of a discrete model at the top and
several IE modules. Every module is factorized into three elements, related to the observations of the agent’s arm (in
blue), a tool (in green), and a ball (in red). Note that the last (virtual) level only considers the tool’s extremity and the ball.
The computation of the action for a single time step is divided into four main processes. (a) Perception. Proprioceptive
and visual observations yp and ye are compared with the agent’s predictions. The resulting prediction errors are
propagated throughout the hierarchy to infer the actual kinematic configuration, as well as potential configurations
related to the objects. (b) Dynamic inference. The bottom-up messages le from the IE modules inform the discrete
model about the most likely state that may have generated the perceived arm trajectory. This is done by comparing the
latter with potential trajectories fm related to dynamic hypotheses ve (see Equation 57). For instance, if the agent is
reaching the tool and the ball is moving away, the bottom-up messages assign a higher probability to the tool-reaching
hypothesis and a lower probability to the initial steady state. (c) Dynamic planning. The agent infers the next discrete
action to take by minimizing the expected free energy G (see Equation 59). As a result, the agent believes to be at the
next discrete state, corresponding to the ball-reaching hypothesis. In turn, this biased state generates a new combined
trajectory (through the discrete extrinsic prediction Aes in Equation 57), acting as a prior for the continuous hidden
states of the IE modules. (d) Action. The continuous hidden states generate predictions, which are again compared
with the related observations. The proprioceptive prediction errors climb back the hierarchy as before, but they are also
suppressed through movement by motor units (see Equation 56). This second process eventually produces a continuous
action that moves the end effector toward the ball.
15
Figure 6: Sequence of time frames of the simulation. Real ball, tool, and arm are displayed in dark red, dark green, and
dark blue respectively. Beliefs of tool and ball, in terms of potential kinematic configurations, are shown in light green
and light red, respectively. Trajectories of the end effector, tool, and ball are displayed as well. The number of time
steps is shown in the lower-left corner of each frame.
For each entity, the intrinsic hidden states encode pairs of joint angles and limb lengths, e.g.,x(i)
i,0 = [θ(i)
0 , l(i)
0 ] while the
extrinsic reference frame is expressed in terms of the position of a limb’s extremity and its absolute orientation, e.g.,
x(i)
e,0 = [p(i)
0,x, p(i)
0,y, ϕ(i)
0 ]. The likelihood function ge of Equation 38 computes extrinsic predictions independently for
each entity:
ge(x(i)
i , x(i−1)
e ) =
h
T(x(i)
i,0, x(i−1)
e,0 ) T(x(i)
i,t , x(i−1)
e,t ) T(x(i)
i,b, x(i−1)
e,b )
i
(45)
Here, the mapping T(xi, xe) reduces to a simple roto-translation:
T(xi, xe) =


px + lcθ,ϕ
py + lsθ,ϕ
ϕ + θ

 (46)
where xi = [θ, l], xe = [px, py, ϕ], and we used a compact notation to indicate the sine and cosine of the sum of two
angles, i.e., cθ,ϕ = cos(θ) cos(ϕ) − sin(θ) sin(ϕ). Each level then computes proprioceptive and visual predictions
through likelihood functions gp and gv, which in this case are simple mappings that extract the joint angles of the actual
arm configuration and the Cartesian positions of the limbs and objects from the intrinsic and extrinsic hidden states,
respectively:
gp(x(i)
i ) = θ(i)
0
gv(x(i)
e ) =
"
p(i)
0,x p(i)
t,x p(i)
b,x
p(i)
0,y p(i)
t,y p(i)
b,y
#
(47)
Reaching the tool’s origin with the end effector is achieved by a function (related to an agent’s intention) that sets the
first component of the corresponding extrinsic hidden states equal to the second one:
i(4)
e,t (x(4)
e ) =
h
x(4)
e,t x(4)
e,t x(4)
e,b
i
(48)
Then, we define a potential dynamics function by subtracting the current hidden states from this intentional state:
f(4)
e,t (x(4)
e ) = i(4)
e,t (x(4)
e ) − x(4)
e =
h
x(4)
e,t − x(4)
e,0 0 0
i
(49)
Note the decomposition into separate attractors. A non-zero velocity for the first component expresses the agent’s desire
to move the end effector, while a zero velocity for the other two components means that the agent does not intend to
manipulate the objects during the first step of the task. Since a potential kinematic configuration for the tool is already
at the agent’s disposal, in order to speed up the movement similar functions can be defined at every hierarchical level,
16
Figure 7: Representation of the dynamics active during the two steps. (a) The end effector is pulled toward the belief
about the tool’s origin through f(4)
e,t . (b) The tool’s extremity is pulled toward the ball through dynamics f(5)
e,b . This
generates an extrinsic prediction error ε(5)
e that steers the previous level of the potential configuration of the tool.
Concurrently, a second dynamics f(4)
e,b also pulls both actual and tool components of the end effector’s level toward the
potential configuration of the ball.
both in intrinsic and extrinsic reference frames. The second step of the task involves reaching the ball with the tool’s
extremity. Hence, we define two intentional states for the end effector’s and virtual levels, setting every component
equal to the (potential) component related to the ball:
i(4)
e,b(x(4)
e ) =
h
x(4)
e,b x(4)
e,b x(4)
e,b
i
i(5)
e,b(x(5)
e ) =
h
x(5)
e,b x(5)
e,b
i (50)
The attractors encoded in the second set of potential dynamics functions express the agent’s desire to modify the tool’s
location:
f(4)
e,b (x(4)
e ) = i(4)
e,b(x(4)
e ) − x(4)
e =
h
x(4)
e,b − x(4)
e,0 x(4)
e,b − x(4)
e,t 0
i
f(5)
e,b (x(5)
e ) = i(5)
e,b(x(5)
e ) − x(5)
e =
h
x(5)
e,b − x(5)
e,t 0
i (51)
Maintaining this set of dynamics eventually drives the hand into a suitable position that makes the tool’s extremity
touch the ball. A representation of the relations between such dynamics is displayed in Figure 7.
Now, we define the hidden causes of the last two levels (for simplicity, we only describe the extrinsic hidden states):
v(4)
e =
h
v(4)
s v(4)
t v(4)
b
i
v(5)
e =
h
v(5)
s v(5)
b
i (52)
where the subscripts s, t, and b indicate the agent’s intentions to maintain the current state of the world ("stay"), reach
the tool, and reach the ball, respectively. The first hidden cause, related to the following intentional states:
i(4)
e,s(x(4)
e ) = x(4)
e i(5)
e,s(x(5)
e ) = x(5)
e (53)
are needed to ensure that v(4)
e and v(5)
e encode proper probabilities when the discrete model is in the initial state. The
average trajectory is then found by weighting the potential dynamics functions with the corresponding hidden causes.
Hence, having indicated with µ(4)
e the belief of the extrinsic hidden states of the end effector, the generated trajectory is:
η′(4)
x,e = v(4)
s f(4)
e,s (µ(4)
e ) + v(4)
t f(4)
e,t (µ(4)
e ) + v(4)
b f(4)
e,b (µ(4)
e ) (54)
17
The belief is then updated according to the following update rules:
˙µ(4)
e = µ′(4)
e − π(4)
e ε(4)
e + ∂gT
e π(5)
e ε(5)
e + ∂gT
v π(4)
v ε(4)
v + ∂η′(4)T
x,e π(4)
x,eε(4)
x,e
˙µ′(4)
e = −π(4)
x,eε(4)
x,e
(55)
In short, the 0th-order is subject to: (i) a quantity proportional to the estimated trajectory; (ii) an extrinsic prediction
error coming from the elbow, i.e., ε(4)
e = µ(4)
e − ge(µ(4)
i , µ(3)
e ); (iii) a backward extrinsic prediction error coming
from the virtual level, i.e., ε(5)
e = µ(5)
e − ge(µ(5)
i , µ(4)
e ); (iv) a visual prediction error, i.e., ε(4)
v = y(4)
v − µ(4)
e ; (v) a
backward dynamics error encoding the generated trajectory, i.e., ε(4)
x,e = µ′(4)
e − η′(4)
x,e . For a more detailed treatment of
inference and dynamics of kinematic configurations in hierarchical settings, see [36].
The actions a are instead computed by minimizing proprioceptive prediction errors:
˙a = −∂agT
p πpεp (56)
where ∂agp performs an inverse dynamics from proprioceptive predictions to actions.
As concerns the discrete model, its hidden states s express: (i) whether the agent is at the tool position, at the ball
position, or none of the two; (ii) whether the agent has grasped or not the tool. These two factors combine in 6 process
states in total. The first factor generates predictions for the extrinsic hidden causes of the hybrid units through likelihood
matrices, i.e., A(4)
e s and A(5)
e s. This allows the agent to synchronize the behavior of both the tool and end effector;
additional likelihood matrices can be defined to impose priors for the intrinsic hidden states and at different levels of the
hierarchy. The second factor returns a discrete tactile prediction, i.e., Ats.
Finally, we define a discrete action for each step of the task, and a transition matrix B such that the ball can be reached
only when the tool has been grasped. Discrete actions are replanned every 10 continuous time steps, and transitions
between discrete states occur dynamically depending on continuous evidence. In the example of Figure 6, the transition
between reaching the tool and reaching the ball happens after 350 time steps. The extrinsic hidden causes v(4)
e and v(5)
e
are found by Bayesian model comparison, as explained in Section 3.1.1:
v(4)
e = σ(ln A(4)
e s + l(4)
e )
v(5)
e = σ(ln A(5)
e s + l(5)
e )
(57)
As noted above, A(4)
e s and A(5)
e s represent predictions of extrinsic hypotheses made by the discrete model for the end
effector and virtual levels, e.g., a higher value of v(4)
b and v(5)
b means that the discrete model wants to reach the ball.
Conversely, l(4)
e and l(5)
e are the bottom-up messages that accumulate continuous log evidence over some time T, that
is, they provide information whether the end effector is reaching the tool or the ball based on the context. Comparison
between such high-level expectations and low-level evidences permits inferring the discrete hidden states based on
potential trajectories. In fact, the hidden causes act as additional observations for the discrete model, which infers the
states at time τ by combining them with the tactile observation and the hidden states at time τ − 1:
sπ,τ = σ(ln Bπ,τ−1sτ−1 + lnA(4)T
e v(4)
e,τ + lnA(5)T
e v(5)
e,τ + lnAT
t ot) (58)
If we assume for simplicity that the agent’s preferences are encoded in a tensor C in terms of expected states, the
expected free energy breaks down to:
Gπ ≈
X
τ
sπ,τ [ln sπ,τ − ln p(sτ |C)] (59)
Computing the softmax of the expected free energy returns the posterior probability over the policies π, which are used
to infer the new discrete hidden states at time τ + 1.
3.3 Analysis of model performances
Figure 6 illustrates task progress during a sample trial. Although both objects are moving, the discrete model successfully
infers and imposes continuous trajectories allowing the agent to operate correctly and achieve its goal. At the beginning
of a trial, the beliefs of the hidden states are initialized with the actual starting configuration of the arm. The agent infers
18
two potential kinematic configurations for the tool and the ball. While the two observations of the tool constrain the
corresponding inference, the ball belief is only subject to its actual position, thus letting the agent initially overestimate
the length of the virtual level. During the first phase, only the tool reaching intention is active: as a consequence, the
tool belief constantly biases the arm belief, which in turn pulls the real arm. After 350 steps, both these beliefs are in
the same configuration, while the ball belief has inferred the corresponding position. At this point, the tool is grasped,
causing the discrete model to predict a different combination of hidden causes. Now, both the tool and arm beliefs
are pulled toward the ball belief. After about 800 steps, the agent infers the same configuration for all three beliefs,
successfully reaching the ball with the tool’s extremity, and tracking it until the trial ends. Note that even during the
first reaching movement, the agent continuously updates its configuration in relation to the ball; as a result, the second
reaching movement is faster.
The transitions can be better appreciated from Figure 3.3, showing the bottom-up messages (i.e., accumulated evidences)
l(4)
e and l(5)
e for the last two levels of the hierarchy (i.e., end effector’s and virtual levels), and the discrete hidden
states sτ . As evident, the virtual level does not contribute to the inference of the first reaching trajectory, since this
only involves the end effector. Note how the agent is able to dynamically accumulate the evidences over its discrete
hypotheses: during the first phase, the evidence l(4)
e,t (related to the tool belief at the end effector’s level) increases as
soon as the end effector approaches the tool’s origin, while l(4)
e,b and l(5)
e,b (related to the ball belief at the end effector’s
and virtual levels, respectively) decrease as the ball moves away. During the second phase, the latter two rapidly
increase as the end effector approaches the ball; finally, every probability of both levels slowly stabilizes as the extrinsic
beliefs converge to the same value and the errors are minimized. The slow decrease of the initial state and the fast
transition between the two steps are well summarized in the bottom graph. The trajectories of the hidden states show
that the agent can plan new trajectories with a high frequency (in this case, 10 continuous time steps), allowing it to
react rapidly to environmental stimuli.
The relationship between hidden causes and continuous dynamics is summarized in Figure 3.3, showing the extrinsic
potential and estimated dynamics for the end effector’s and virtual levels, as well as the dynamics of the extrinsic hidden
causes. For simplicity, we considered the same discrete hidden causes for every hierarchical level, and the top plot
recapitulates the state of the whole kinematic configuration. Here, we note a similar behavior to the discrete hidden
states (i.e., slow decrease of the stay cause and increase of the first reaching movement, and rapid increase of the second
reaching movement in the middle of the trial). Note that the agent maintains potential dynamics related to the three
intentions for the duration of the whole trial; these dynamics are combined to produce a trajectory that the motor units
accomplish. In fact, two spikes are evident in the dynamics of the end effector’s level, and one in the dynamics of the
virtual level, regarding the ball reaching action.
In order to assess the model performances in dynamic planning tasks, we run three different experiments, each composed
of 150 trials. The first experiment assessed the capacity of picking a moving tool and reaching a static target, hence, for
each trial we varied the tool velocity. The second experiment assessed the capacity of picking a static tool and tracking
a moving target: here, we varied the ball velocity. The third experiment evaluated the performances of the agent in
picking a moving tool and tracking a moving target: we hence varied both tool and ball velocities. In all experiments,
we randomly sampled tool and ball positions, and their directions where relevant. Also, velocity varied from 0 to 8
pixels per time step. The width and height of the virtual environment was 1300x1300 pixels, twice the total arm length
plus the tool length; thus, the ball and tool were out of reach for a significant period of each trial. The duration of each
trial was set to 3000 steps.
The results of the simulations are visualized in Figure 9, showing the task accuracy, the time needed to complete the
task, and the average final error (see the caption for more details). With static or slow-varying environments, the agent
completes the task in all conditions. The first condition (moving tool) achieved good performances even with high tool
velocity, although with low velocities there is a slight decrease in accuracy: since the tool often moves out of reach, the
agent cannot accomplish the tool-picking action. This specific behavior is not present in the second condition (moving
ball), probably due to the increased operational space which allows the agent to move along the whole environment.
However, the performance decreases for high ball velocities also due to the additional difficulty of tracking moving
objects. The third combined condition (moving tool and ball) achieved slightly lower performances than the second
condition, with a time needed to complete the task similar to the first condition. However, the average tracking error
shown in the bottom panel remains restricted even with high ball and tool velocities.
19
Figure 8: (a) Normalized log evidences, for 60 discrete steps τ (composed, in turn, of 10 continuous time steps), of the
end effector’s level l(4)
e (top), and virtual level l(5)
e (middle). Discrete hidden states (bottom). The green and red dashed
lines respectively represent the tool’s extremity-ball distance, and the tool’s origin-end effector distance, normalized to
fit in the plots. As explained in Section 3.2.1, the agent has two discrete states and two hidden causes related to the steps
of the task, i.e., reaching the ball and reaching the tool, with additional stay discrete state and cause. (b) Dynamics
of extrinsic hidden causes ve (top plot). Norm of extrinsic potential dynamics ||f(4)
e,s || (stay), ||f(4)
e,t || (reach tool) and
||f(4)
e,b || (reach ball), along with estimated dynamics ||µ(4)′
e || for end effector’s level (middle plot). Norm of extrinsic
potential dynamics ||f(5)
e,s || (stay) and ||f(5)
e,b || (reach ball), along with estimated dynamics ||µ(5)′
e || for virtual level
(bottom plot). The dynamics are plotted for 600 continuous time steps.
20
Figure 9: Performances of the deep hybrid model during tool use for three conditions: a moving tool (in red), a moving
ball (in green), and moving tool and ball (in blue). Accuracy (top), for which we considered a trial successful if the tool
was picked and the average ball-tool distance for the last 300 steps was less than 100 pixels. Time (middle), measured
as the number of steps needed for the tool to be picked and for the ball-tool distance to be less than 100 pixels. Error
(bottom), measured as the average ball-tool distance for the last 300 steps. For each condition, we aggregated the
measures for 150 trials. The middle and bottom plots also show the 95% confidence interval.
Finally, the dynamic behavior of the extrinsic beliefs can be analyzed from Figure 10, showing the trajectories, for a
sample trial of the third condition, of all the forces that make up the update of Equation 55, for the last two levels and
every environmental entity. The transition between the two phases of the task is here evident: the 1st-order derivative
of the arm belief µ′(4)
e,a (blue line in the top left panel) is non-zero during the whole task, and presents two spikes at
the beginning of each phase, signaling an increased prediction error due to the new intention. The arm movement
during the first phase is the consequence of the non-zero 1st-order derivative of the tool beliefµ′(4)
e,t (blue line in the
middle left panel). The dynamics of the corresponding extrinsic prediction error ε(5)
e,t (green line in the middle left
panel) combines both this derivative and the visual prediction error of the next levelε(5)
v,t (red line in the middle right
panel). Note that this extrinsic prediction error does not exist for the arm belief, and that the backward dynamics error
ε(4)
e,x,a has a smaller impact on the overall update with respect to the 1st-order derivative. The second phase begins with
a spike in the 1st-order derivative of the tool belief at the virtual level µ′(5)
e,t (blue line in the middle right panel), which
is propagated back to the previous level as an extrinsic prediction error. Finally, note that the ball belief is only subject
to its visual observation and the extrinsic prediction error coming from the previous levels.
21
Figure 10: Trajectory of every component of the extrinsic belief updates for the end effector (left panels), and virtual
(right panels) levels. Every environmental entity is shown separately. Blue, orange, green, red, and purple lines
respectively indicate the 1st-order derivative, the extrinsic prediction error from the previous level, the extrinsic
prediction error from the next level, the visual prediction error, and the backward dynamics error.
4 Discussion
We proposed a computational method, based on hybrid active inference, that affords dynamic planning for hierarchical
settings. Our goal was twofold. First, to show the effectiveness of casting control problems as inference and, in
particular, of expressing entities in relation to a hierarchical configuration of the self. While there could be several
ways to combine the units of the proposed architecture, we showed a specific design as a proof-of-concept to solve
a typical task: reaching a moving object with a tool. The agent had to rely on three kinds of depth, i.e., it had to
dynamically infer its intentions for decision-making, and form different hierarchical generative models depending on
the structure and affordances of the entities. The proposed model unifies several characteristics studied in the active
inference literature: the modeling of objects, recently done in the context of active object reconstruction [70, 71, 72];
the analyses of affordances in relation to the agent’s beliefs [73]; the modeling of itinerant movements, with a behavior
similar to the Lotka-V olterra dynamics implemented in [74]; planning and control in extrinsic coordinates [ 75, 6];
inference of discrete states based on continuous signals in dynamic environments, achieved through a different kind of
post-hoc Bayesian model selection [76] or other various approaches such as bio-inspired SLAM [33], dynamic Bayesian
networks [77], recurrent switching linear dynamical systems [78]; the use of tools for solving complex tasks [79].
Our second goal was to show that a (deep) hierarchical formulation of active inference could lend itself to learning and
generalization of novel tasks. Although we used a fixed generative model, we revealed that an advanced behavior is
possible by using likelihood and dynamics functions that could be easily implemented with neural connections, and
by decomposing the model into small units linked together. In [80], a hierarchical kinematic model was used to learn
the limbs of an agent’s kinematic chain, both during perception and action. The same mechanism could be used to
infer the length of tools needed for object manipulation, extending the kinematic chain in a flexible way. Therefore, an
encouraging research direction would be to design a deep hybrid model in the wake of PCNs, and let the agent learn
appropriate structure and internal attractors for a specific goal via free energy minimization. PCNs have demonstrated
robust performance in tasks like classification and regression [ 81, 46], while approximating the backpropagation
algorithm [82, 83, 84, 85]. However, few studies have leveraged the modular and hierarchical nature of predictive
coding to model complex dynamics [86, 47, 87, 88, 89] or enable interactions with the environment [90, 91, 92, 93, 18],
mostly done through RL. Well-known issues of deep RL are data efficiency, explainability, and generalization [45].
22
Instead, the human brain is capable of learning new tasks with a small amount of samples, transferring the knowledge
previously acquired in similar situations. Another common criticism is that deep RL lacks explainability, which is
of greater concern as AI systems rapidly grow. A viable alternative is to learn a model of the environment [94], e.g.,
with Bayesian non-parametrics [18]; however, these approaches are still computationally demanding. Albarracin et
al. described how active inference may find an answer to the black box problem [95], and we further showed how
different elements of an active inference agent have practical and interpretable meanings. In this view, optimization
of parameters in hybrid models could be an effective alternative to deep RL algorithms, or other approaches in active
inference relying on the use of neural networks as generative models.
Besides the fixed generative model, another limitation of the proposed study is that we only used two temporal orders,
while a more complete and effective model would make use of a greater set of generalized coordinates [49]. Nonetheless,
every aspect we introduced can be extended by considering increasing temporal orders. For instance, discrete variables
could depend on the position, velocity, and acceleration of an object, thus inferring a more accurate representation of
dynamic trajectories. Also, flexible behavior could be specified in the 2nd temporal order, resulting in a more realistic
force-controlled system.
An interesting direction of research regards the generation of states and paths, about which useful indications might
come from planning and control with POMDP models [18]. Some implementations of discrete active inference models
used additional connections between policies and between discrete hidden states [96, 61]; hence, it might be beneficial
to design similar connections in continuous and hybrid contexts as well. In this study, a single high-level discrete model
imposed the behavior of every other hybrid unit; an alternative would be to design independent connections between
hidden causes such that a high-level decision would be propagated down to lower levels with local message passing.
This approach may also provide insights into how, by repetition of the same task, discrete policies adapt to construct
composite movements (e.g., a reaching and grasping action) from simpler continuous paths.
Acknowledgments
This research received funding from the European Union’s Horizon H2020-EIC-FETPROACT-2019 Programme for
Research and Innovation under Grant Agreement 951910 to I.P.S. The funders had no role in study design, data
collection and analysis, decision to publish, or preparation of the manuscript.
References
[1] Emanuel Todorov. Optimality principles in sensorimotor control. Nature Neuroscience, 7:907–915, 2004.
[2] Jörn Diedrichsen, Reza Shadmehr, and Richard B Ivry. The coordination of movement: optimal feedback control
and beyond. Trends in cognitive sciences, 14(1):31–39, 2010.
[3] Karl J. Friston, Tamara Shiner, Thomas FitzGerald, Joseph M. Galea, Rick Adams, Harriet Brown, Raymond J.
Dolan, Rosalyn Moran, Klaas Enno Stephan, and Sven Bestmann. Dopamine, affordance and active inference.
PLoS Computational Biology, 8(1):e1002327, January 2012.
[4] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement learning or active inference? PLoS ONE,
4(7), 2009.
[5] Karl Friston. What is optimal about motor control? Neuron, 72(3):488–498, 2011.
[6] Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-energy
formulation. Biological Cybernetics, 102(3):227–260, 2010.
[7] Karl Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11(2):127–138,
2010.
[8] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind, brain, and
behavior. Cambridge, MA: MIT Press, 2021.
[9] Matteo Priorelli, Federico Maggiore, Antonella Maselli, Francesco Donnarumma, Domenico Maisto, Francesco
Mannella, Ivilin Peev Stoianov, and Giovanni Pezzulo. Modeling motor control in continuous-time Active
Inference: a survey. IEEE Transactions on Cognitive and Developmental Systems, pages 1–15, 2023.
23
[10] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of The Royal Society
Interface, 14(136):20170376, November 2017.
[11] Rajesh P.N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of some
extra-classical receptive-field effects. Nature Neuroscience, 2(1):79–87, 1999.
[12] Andy Clark. Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral
and Brain Sciences, 36(3):181–204, 2013.
[13] Jakob Hohwy. The Predictive Mind. Oxford University Press UK, 2013.
[14] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the relationship between
active inference and control as inference. Communications in Computer and Information Science, 1326:3–11,
2020.
[15] Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences, 16(10):485–488,
2012.
[16] Marc Toussaint and Amos Storkey. Probabilistic inference for solving discrete and continuous state Markov
Decision Processes. ACM International Conference Proceeding Series, 148:945–952, 2006.
[17] Marc Toussaint. Probabilistic inference as a model of planned behavior. Künstliche Intelligenz, 3/09:23–29, 2009.
[18] I Stoianov, C Pennartz, C Lansink, and G Pezzulo. Model-based spatial navigation in the hippocampus-ventral
striatum circuit: a computational analysis. Plos Computational Biology, 14(9):1–28, 2018.
[19] Karl Friston. Hierarchical models in the brain. PLoS Computational Biology, 4(11), 2008.
[20] Karl J. Friston, Thomas Parr, Yan Yufik, Noor Sajid, Catherine J. Price, and Emma Holmes. Generative models,
linguistic communication and active inference. Neuroscience I& Biobehavioral Reviews, 118:42–64, November
2020.
[21] Eric R Kandel, James H Schwartz, Thomas M. Jessell, Steven A. Siegelbaum, and A. J. Hudspeth. Principles of
Neuroscience. McGraw-Hill, New York, 5 edition, 2013.
[22] Lucilla Cardinali, Francesca Frassinetti, Claudio Brozzoli, Christian Urquizar, Alice C. Roy, and Alessandro
Farnè. Tool-use induces morphological updating of the body schema. Current Biology, 19(13):478, 2009.
[23] Angelo Maravita and Atsushi Iriki. Tools for the body (schema). Trends in Cognitive Sciences, 8(2):79–86, 2004.
[24] Daniel Baldauf, He Cui, and Richard A. Andersen. The posterior parietal cortex encodes in parallel both goals for
double-reach sequences. Journal of Neuroscience, 28(40):10081–10089, 2008.
[25] Paul Cisek and John F. Kalaska. Neural correlates of reaching decisions in dorsal premotor cortex: Specification
of multiple direction choices and final selection of action. Neuron, 45(5):801–814, March 2005.
[26] Kai Ueltzhöffer. Deep Active Inference. pages 1–40, 2017.
[27] Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical Psychology, 96,
2020.
[28] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlo methods. Advances in Neural Information Processing Systems, 2020-Decem(NeurIPS), 2020.
[29] Thomas Rood, Marcel van Gerven, and Pablo Lanillos. A deep active inference model of the rubber-hand illusion.
2020.
[30] Cansu Sancaktar, Marcel A. J. van Gerven, and Pablo Lanillos. End-to-End Pixel-Based Deep Active Inference
for Body Perception and Action. In 2020 Joint IEEE 10th International Conference on Development and Learning
and Epigenetic Robotics (ICDL-EpiRob), pages 1–8, 2020.
[31] Théophile Champion, Marek Grze´s, Lisa Bonheme, and Howard Bowman. Deconstructing deep active inference.
2023.
[32] Aleksey Zelenov and Vladimir Krylov. Deep active inference in control tasks. In 2021 International Conference
on Electrical, Communication, and Computer Engineering (ICECCE), pages 1–3, 2021.
[33] Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierarchical
active inference. Neural Networks, 142:192–204, 2021.
24
[34] Kai Yuan, Karl Friston, Zhibin Li, and Noor Sajid. Hierarchical generative modelling for autonomous robots.
Research Square, 2023.
[35] Matteo Priorelli and Ivilin Peev Stoianov. Flexible Intentions: An Active Inference Theory. Frontiers in
Computational Neuroscience, 17:1 – 41, 2023.
[36] Matteo Priorelli, Giovanni Pezzulo, and Ivilin Peev Stoianov. Deep kinematic inference affords efficient and
scalable control of bodily movements. PNAS, 120, 2023.
[37] Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference.
1(4):381–414, 2017.
[38] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and
active inference. Neuroscience and Biobehavioral Reviews, 77(November 2016):388–402, 2017.
[39] Thomas Parr and Karl J. Friston. Active inference and the anatomy of oculomotion. Neuropsychologia,
111(January):334–343, 2018.
[40] T. Parr and K. J. Friston. The computational pharmacology of oculomotion. Psychopharmacology (Berl.),
236(8):2473–2484, August 2019.
[41] Jakob Hohwy. New directions in predictive processing. Mind and Language, 35(2):209–223, 2020.
[42] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle. Philosophical Transactions of
the Royal Society B: Biological Sciences, 364(1521):1211–1221, 2009.
[43] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. Machine Learning ,
37(2):183–233, 1999.
[44] H. Kobayashi and L. R. Bahl. Image data compression by predictive coding i: Prediction algorithms. IBM Journal
of Research and Development, 18(2):164–171, 1974.
[45] Beren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz. Predictive Coding:
Towards a Future of Deep Learning beyond Backpropagation? IJCAI International Joint Conference on Artificial
Intelligence, pages 5538–5545, 2022.
[46] Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston,
and Alexander Ororbia. Brain-inspired computational intelligence via predictive coding, 2023.
[47] Beren Millidge, Mahyar Osanlouy, and Rafal Bogacz. Predictive Coding Networks for Temporal Prediction. pages
1–59, 2023.
[48] Thomas Parr and Karl J. Friston. The Discrete and Continuous Brain: From Decisions to Movement—And Back
Again Thomas. Neural Computation, 30:2319–2347, 2018.
[49] Karl Friston, Klaas Stephan, Baojuan Li, and Jean Daunizeau. Generalised filtering. Mathematical Problems in
Engineering, 2010:Article ID 621670, 34 p.–Article ID 621670, 34 p., 2010.
[50] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind, brain, and
behavior. 2022.
[51] Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simple. Physics Reports, 1024:1–29, 2022.
[52] Rick A. Adams, Stewart Shipp, and Karl J. Friston. Predictions not commands: Active inference in the motor
system. Brain Structure and Function, 218(3):611–643, 2013.
[53] Giovanni Pezzulo, Francesco Rigoli, and Karl J. Friston. Hierarchical Active Inference: A Theory of Motivated
Control. Trends in Cognitive Sciences, 22(4):294–306, 2018.
[54] Karl J. Friston and Christopher D. Frith. Active inference, communication and hermeneutics. Cortex, 68:129–143,
July 2015.
[55] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics ,
113(5–6):495–513, September 2019.
[56] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its
application to empirical data. Journal of Mathematical Psychology, 107:102632, 2022.
25
[57] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active
inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99, 2020.
[58] Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian model reduction. pages 1–32, 2018.
[59] Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational free energy
and the Laplace approximation. NeuroImage, 34(1):220–234, 2007.
[60] Karl Friston and Will Penny. Post hoc Bayesian model selection. NeuroImage, 56(4):2089–2099, 2011.
[61] Karl J. Friston, Thomas Parr, Conor Heins, Axel Constant, Daniel Friedman, Takuya Isomura, Chris Fields, Tim
Verbelen, Maxwell Ramstead, John Clippinger, and Christopher D. Frith. Federated inference and belief sharing.
Neuroscience & Biobehavioral Reviews, 156:105500, 2024.
[62] M. Priorelli and I.P. Stoianov. Dynamic inference by model reduction. bioRxiv, 2023.
[63] Léo Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. Active inference and robot control: A case
study. Journal of the Royal Society Interface, 13(122), 2016.
[64] Matteo Priorelli and Ivilin Peev Stoianov. Slow but flexible or fast but rigid? discrete and continuous processes
compared. Heliyon, page e39129, October 2024.
[65] Rick A. Adams, Eduardo Aponte, Louise Marshall, and Karl J. Friston. Active inference and oculomotor pursuit:
The dynamic causal modelling of eye movements. Journal of Neuroscience Methods, 242:1–14, 2015.
[66] M. Priorelli, G. Pezzulo, and I.P. Stoianov. Active vision in binocular depth estimation: A top-down perspective.
Biomimetics, 8(5), 2023.
[67] Corrado Pezzato, Christopher Buckley, and Tim Verbelen. Why learn if you can infer? robot arm control with
hierarchical active inference. In The First Workshop on NeuroAI @ NeurIPS2024, 2024.
[68] Matteo Priorelli and Ivilin Peev Stoianov. Dynamic planning in hierarchical active inference. Neural Networks,
page 107075, January 2025.
[69] Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated inference. Neural
Computation, 33(3):713–763, 2021.
[70] Stefano Ferraro, Toon Van de Maele, Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Disentangling shape and
pose for object-centric deep active inference models, 2022.
[71] Ruben S. van Bergen and Pablo L. Lanillos. Object-based active inference, 2022.
[72] Toon Van de Maele, Tim Verbelen, Ozan undefinedatal, and Bart Dhoedt. Embodied object representation learning
and recognition. Frontiers in Neurorobotics, 16, April 2022.
[73] Francesco Donnarumma, Marcello Costantini, Ettore Ambrosini, Karl Friston, and Giovanni Pezzulo. Action
perception as hypothesis testing. Cortex, 89:45–60, April 2017.
[74] Karl J Friston, Jérémie Mattout, and James Kilner. Action understanding and active inference. Biological
cybernetics, 104(1-2):137–60, feb 2011.
[75] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a humanoid robot.
IEEE Transactions on Cognitive and Developmental Systems, 8920(c):1–10, 2021.
[76] Takuya Isomura, Thomas Parr, and Karl Friston. Bayesian filtering with multiple internal models: Toward a theory
of social intelligence. Neural Computation, 31(12):2390–2431, 2019.
[77] Sheida Nozari, Ali Krayani, Pablo Marin-Plaza, Lucio Marcenaro, David Martin Gomez, and Carlo Regazzoni.
Active inference integrated with imitation learning for autonomous driving. IEEE Access, 10:49738–49756, 2022.
[78] Poppy Collis, Ryan Singh, Paul F Kinghorn, and Christopher L Buckley. Learning in hybrid active inference
models, 2024.
[79] Ajith Anil Meera and Pablo Lanillos. Towards metacognitive robot decision making for tool selection. In
Christopher L. Buckley, Daniela Cialfi, Pablo Lanillos, Maxwell Ramstead, Noor Sajid, Hideaki Shimazaki, Tim
Verbelen, and Martijn Wisse, editors,Active Inference, pages 31–42, Cham, 2024. Springer Nature Switzerland.
[80] Matteo Priorelli and Ivilin Peev Stoianov. Efficient motor learning through action-perception cycles in deep
kinematic inference. In Active Inference, pages 59–70. Springer Nature Switzerland, 2024.
26
[81] Alexander Ororbia and Daniel Kifer. The neural coding framework for learning generative models. Nature
Communications, 13(1), 2022.
[82] James C R Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive
coding network with local hebbian synaptic plasticity. Neural Comput, 29(5):1229–1262, March 2017.
[83] James C.R. Whittington and Rafal Bogacz. Theories of Error Back-Propagation in the Brain. Trends in Cognitive
Sciences, 23(3):235–250, 2019.
[84] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Predictive Coding Approximates Backprop
Along Arbitrary Computation Graphs. Neural Computation, 34(6):1329–1368, 2022.
[85] Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, Rafal Bogacz, and Zhenghua Xu. Predictive coding can
do exact backpropagation on convolutional and recurrent neural networks, 2021.
[86] Ivilin Stoianov, Domenico Maisto, and Giovanni Pezzulo. The hippocampal formation as a hierarchical generative
model supporting generative replay and continual learning. Progress in Neurobiology, 217(102329):1–20, 2022.
[87] Linxing Preston Jiang and Rajesh P. N. Rao. Dynamic predictive coding: A model of hierarchical sequence
learning and prediction in the neocortex. PLOS Computational Biology, 20(2):e1011801, February 2024.
[88] Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based
planning in latent space, 2021.
[89] Mufeng Tang, Helen Barron, and Rafal Bogacz. Sequential memory with temporal predictive coding, 2023.
[90] Beren Millidge. Combining Active Inference and Hierarchical Predictive Coding: a Tutorial Introduction and
Case Study. PsyArXiv, 2019.
[91] Alexander Ororbia and Ankur Mali. Active Predicting Coding: Brain-Inspired Reinforcement Learning for Sparse
Reward Robotic Control Problems. 2022.
[92] Rajesh P. N. Rao, Dimitrios C. Gklezakos, and Vishwas Sathish. Active predictive coding: A unified neural
framework for learning hierarchical world models for perception and planning, 2022.
[93] Ares Fisher and Rajesh P N Rao. Recursive neural programs: A differentiable framework for learning composi-
tional part-whole hierarchies and image grammars. PNAS Nexus, 2(11), October 2023.
[94] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based reinforcement learning:
A survey. 2022.
[95] Mahault Albarracin, Inês Hipólito, Safae Essafi Tremblay, Jason G. Fox, Gabriel René, Karl Friston, and Maxwell
J. D. Ramstead. Designing explainable artificial intelligence with active inference: A framework for transparent
introspection and decision-making. 2023.
[96] Toon Van de Maele, Tim Verbelen, Pietro Mazzaglia, Stefano Ferraro, and Bart Dhoedt. Object-centric scene
representations using active inference, 2023.
27
A Algorithms
Algorithm 1 Compute expected free energy
Input:
length of policies Np
discrete hidden states s
policies π
transition matrix B
preference C
Output:
expected free energy G
G ← 0
for each policy ππ do
sπ,τ ← s
for τ = 0 to Np do
sπ,τ ← Bπ,τ sπ,τ
Gπ ← Gπ + sπ,τ (ln sπ,τ − ln C)
end for
end for
Algorithm 2 Accumulate log evidence
Input:
mean of full prior η
mean of reduced priors ηm
mean of full posterior µ
precision of full prior Π
precision of reduced priors Πm
precision of full posterior P
log evidences Lm
Output:
log evidences Lm
for each reduced model m do
Pm ← P − Π + Πm
µm ← P−1
m (Pµ − Πη + Πmηm)
Lm ← Lm + (µT
mPmµm − ηT
mΠmηm − µT Pxµ + ηT Πη)/2
end for
28
Algorithm 3 Active inference with deep hybrid models
Input:
continuous time T
discrete time T
intrinsic units U(i,j)
i
extrinsic units U(i,j)
e
inverse dynamics ∂agp
proprioceptive precisions Πy,p
learning rate ∆t,
action a
for t = 0 to T do
Get observations
if t mod T = 0 then
Update discrete model via Algorithm 4
end if
for each unit U(i,j)
i and U(i,j)
e do
Update intrinsic unit via Algorithm 5
Update extrinsic unit via Algorithm 6
end for
Get proprioceptive prediction errors εy,p from intrinsic units
˙a ← −∂agT
p Πy,pεy,p
a ← a + ∆t ˙a
Take action a
end for
Algorithm 4 Update discrete model at time τ
Input:
discrete hidden states s
policies π
likelihood matrices A(i,j)
transition matrix B
prior D
accumulated log evidences l(i,j)
Output:
accumulated log evidences l(i,j)
discrete hidden causes v(i,j)
for each unit U(i,j) do
v(i,j) ← σ(ln A(i,j)s + l(i,j))
end for
s ← σ(ln D + P
i,j ln A(i,j)T v(i,j))
Compute expected free energy G via Algorithm 1
π ← σ(−G)
D ← P
π ππ,0Bπ,0s
for each unit U(i,j) do
v(i,j) ← A(i,j)D
l(i,j) ← 0
end for
29
Algorithm 5 Update intrinsic unit
Input:
belief of extrinsic hidden states of previous level µ(i−1)
e
belief of intrinsic hidden states ˜µ(i)
i
intrinsic (discrete) hidden causes v(i)
i
proprioceptive observation y(i)
p
belief of extrinsic hidden states ˜µ(i)
e
intrinsic dynamics (reduced) functions f(i)
i,m
proprioceptive likelihood gp
extrinsic likelihood ge
proprioceptive precision Π(i)
y,p
extrinsic precision Π(i)
y,e
intrinsic dynamics precision Π(i)
x,i
learning rate ∆t
Output:
proprioceptive prediction error ε(i)
y,p
extrinsic prediction error ε(i)
y,e
η(i)′
x,i ← P
m v(i)
i,mf(i)
i,m(µ(i)
i )
ε(i)
y,p ← y(i)
p − gp(µ(i)
i )
ε(i)
x,i ← µ(i)′
x,i − η(i)′
x,i
ε(i)
y,e ← µ(i)
e − ge(µ(i)
i , µ(i−1)
e )
Accumulate log evidence via Algorithm 2
˙µ(i)
i ← µ(i)′
i + ∂igT
p Π(i)
y,pε(i)
y,p + ∂igT
e Π(i)
y,eε(i)
y,e + ∂iη(i)′T
x,i Π(i)
x,iε(i)
x,i
˙µ(i)′
i ← −Π(i)
x,iε(i)
x,i
˜µ(i)
i ← ˜µ(i)
i + ∆t ˙˜µ
(i)
i
30
Algorithm 6 Update extrinsic unit
Input:
extrinsic prediction error ε(i)
y,e
belief of extrinsic hidden states ˜µ(i)
e
extrinsic (discrete) hidden causes v(i)
e
visual observation y(i)
v
extrinsic prediction errors of next levels ε(i+1,l)
y,e
extrinsic dynamics (reduced) functions f(i)
e,m
visual likelihood gv
extrinsic precision Π(i)
y,e
extrinsic precisions of next levels Π(i+1,l)
y,e
visual precision Π(i)
y,v
extrinsic dynamics precision Π(i)
x,e
learning rate ∆t
Output:
belief of extrinsic hidden states ˜µ(i)
e
η(i)′
x,e ← P
m v(i)
e,mf(i)
e,m(µ(i)
e )
ε(i)
y,v ← y(i)
v − gv(µ(i)
e )
ε(i)
x,e ← µ(i)′
x,e − η(i)′
x,e
Accumulate log evidence via Algorithm 2
˙µ(i)
e ← µ(i)′
e − Π(i)
y,eε(i)
y,e + P
l ∂egT
e Π(i+1,l)
y,e ε(i+1,l)
y,e + ∂egT
v Π(i)
y,vε(i)
y,v + ∂eη(i)′T
x,e Π(i)
x,eε(i)
x,e
˙µ(i)′
e ← −Π(i)
x,eε(i)
x,e
˜µ(i)
e ← ˜µ(i)
e + ∆t ˙˜µ
(i)
e
31