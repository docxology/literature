Prior Preference Learning from Experts:
Designing a Reward with Active Inference
Jin young Shin§, Cheolhyeong Kim§, Hyung Ju Hwang∗
Department of Mathematics
POSTECH
Pohang, 37673, Republic of Korea
{sjy6006, tyty4, hjhwang}@postech.ac.kr
December 14, 2021
Abstract
ActiveinferencemaybedefinedasBayesianmodelingofabrainwithabiologicallyplausiblemodeloftheagent.
Its primary idea relies on the free energy principle and the prior preference of the agent. An agent will choose an
action that leads to its prior preference for a future observation. In this paper, we claim that active inference can
be interpreted using reinforcement learning (RL) algorithms and find a theoretical connection between them. We
extend the concept of expected free energy (EFE), which is a core quantity in active inference, and claim that
EFE can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical
connection,weproposeasimplebutnovelmethodforlearningapriorpreferencefromexperts.Thisillustratesthat
the problem with inverse RL can be approached with a new perspective of active inference. Experimental results
of prior preference learning show the possibility of active inference with EFE-based rewards and its application to
an inverse RL problem.
1 Introduction
Active inference [19] is a theory emerging from cognitive science using a Bayesian modeling of the brain function
[11, 12, 14, 16], predictive coding [21, 30], and the free energy principle [17, 36, 18]. It states that the agents choose
actionstominimizeanexpectedfuturesurprise[15,13,20],whichisameasurementofthedifferencebetweenanagent’s
prior preference and expected future. Minimization of an expected future surprise can be achieved by minimizing the
expected free energy (EFE),whichisacorequantityofactiveinference.AlthoughactiveinferenceandEFEhavebeen
inspired and derived from cognitive science using a biologically plausible brain function model, its usage in RL tasks
is still limited owing to its computational issues and prior-preference design. [31, 10]
First,EFErequiresheavycomputationalcost.AprecisecomputationofanEFEtheoreticallyaveragesallpossible
policies, which is clearly intractable as an action space A and a time horizon T increase in size. Several attempts have
been made to calculate the EFE in a tractable manner, such as limiting the future time horizon from t to t+H [47],
and applying Monte-Carlo based sampling methods [10, 53] for the search policies.
Second, it is unclear how the prior preferences should be set. This is the same question as how to design the
rewards in the RL algorithm. In recent studies [10, 53, 49] the agent’s prior preference is simply set as the final goal
of a given environment for every time step. There are some environments in which the prior preference can be set as
timeindependent.However,mostpriorpreferencesinRLproblemsareneithersimplenoreasytodesignbecauseprior
preferences of short and long-sighted futures should generally be treated in different ways.
In this paper, we first claim that there is a theoretical connection between active inference and RL algorithms. We
then propose prior preference learning (PPL), a simple and novel method for learning a prior-preference of an active
inference from an expert simulation. In Section 2, we briefly introduce the basic concepts of RL and active inference.
From the previous definition of the EFE of a deterministic policy, in Section 3, we extend the previous concepts of
active inference and theoretically demonstrate that it can be analyzed in view of the RL algorithm. We extend this
quantity to a stochastic policy network and define an action-conditioned EFE for a given action and a given policy
network. Following [31] using a bootstrapping argument, we show that the optimal distribution over the first-step
action induced from active inference can be interpreted using Q-Learning. Consequently, we show that EFE can be
treated as a negative value function from an RL perspective. From this connection, in Section 4, we propose a novel
inverse RL algorithm for designing EFE-based rewards, by learning a prior preference from expert demonstrations.
Throughsuchexpertdemonstrations,anagentlearnsitspriorpreferencegiventheobservationtoachieveafinalgoal,
which can effectively handle the difference between local and global preferences. It will extend the scope of active
inferencetoinverseRLproblem.OurexperimentsinSection6showtheapplicabilityofactiveinferencebasedrewards
∗CorrespondingAuthor
§Theauthorshaveequallycontributedtothispaper.
1
1202
ceD
31
]GL.sc[
3v73980.1012:viXra
using EFE to an inverse RL problem. We compare our method under various experimental setting including global
preference and expert prior preferece. Moreover, we compare our method with traditional inverse RL algorithms and
test our method with various RL algorithm. It shows our method is comparable to traditional inverse RL algorithm.
2 Preliminaries
2.1 Reinforcement Learning
Reinforcement learning (RL) is a computational approach to learning from interaction, that a way of learning what
to do for a given current situation. [44] Given a current state, when an agent chooses an action, then the state may
change and a signal (either positive or negative) is given to the agent. From this interaction, the agent improves its
choice of actions. The final goal of the RL is to maximize the positive interaction.
In order to analyse the ’positive interaction’ in a computational way, one can identify four main subelements of a
RL system as in [44]: A policy, a reward signal, a value function, and a model of the environment.
• Apolicyπ :S →Aistheagent’sbehaviour,amappingfromasetofstatesS toasetofactionsA.Onedenotes
the policy π as either π(s)=a or π(a|s): A deterministic policy π(s)=a gives an action a from a given state s.
A stochastic policy π(s|a) denotes a probability mass/density function on A given the current state s.
• A reward r is real-valued number, which quantifies a single interaction from the environment and defines the
goal of RL in a short term.
• Avaluefunction(v (s),q (s,a))isapredictionofafuturecumulativereward,whichevaluatesthecurrentstate
π π
s,orapair(s,a)ofthecurrentstateandthefirst-stepactiona.Astatevaluefunctionv (s)denotestheexpected
π
totalamountofrewardfollowingthepolicyπ fromcurrentstates.Anaction-valuefunctionq (s,a)denotesthe
π
expected cumulative reward given a current state s and the first-step action a.
• A model of the environment p(s(cid:48),r|s,a) denotes a probability mass/density function of a future state s(cid:48) and a
reward signal r given a current state s and an action a.
Repeatingtheinteractionbetweentheenvironment,oneobtainsasequenceof(state,actionrewards)as(S ,A ,R ,S ,A ,R ,···),
0 0 1 1 1 2
where S denotes the initial state of the environment. A return G at time t with a discount rate γ is given by
0 t
G = R +γR +γ2R +··· = (cid:80)∞ γkR , the cumulative future reward. The return G can be recur-
t t+1 t+2 t+3 k=0 t+k+1 t
sively written as G = R +γG . Discount rate 0 ≤ γ ≤ 1 determines the evaluation on the future reward.
t t+1 t+1
Note that the state-value function v (s) is an expected future cumulative reward given state s under policy π, thus
π
v (s)=E [G |S =s]. From the recursive relation of the cumulative reward and a Markov assumption on the states
π π t t
S , a Bellman equation [6] for a state-value function v (s) can be written as below.
t π
(cid:88) (cid:88)
v (s)= π(a|s) p(s(cid:48),r|s,a)[r+γv (s(cid:48))]
π π
a s(cid:48),r
the similar argument also holds for action-value function q (s,a).
π
(cid:88) (cid:88)
q (s,a)= p(s(cid:48),r|s,a)[r+γ π(a(cid:48)|s(cid:48))q (s(cid:48),a(cid:48))]
π π
s(cid:48),r a(cid:48)
The agent’s final goal is to find an optimal policy π that maximizes a sum of rewards. For such optimal policy π ,
∗ ∗
the following Bellman optimality equation [6] on the optimal state value function v :=v and action-value function
π∗ ∗
q :=q holds.
π∗ ∗
(cid:88)
v (s)=max p(s(cid:48),r|s,a)[r+γv (s(cid:48))] (1)
∗ ∗
a
s(cid:48),r
(cid:88)
q (s,a)= p(s(cid:48),r|s,a)[r+maxq (s(cid:48),a(cid:48))]
∗ ∗
a(cid:48)
s(cid:48),a
Once the optimal q or v is obtained by solving the Bellman optimality equation, then the corresponding optimal
∗ ∗
policyπ canbeobtained.However,explicitlysolvingtheequationwithoutaknowledgeontheenvironmentp(s(cid:48),r|s,a)
∗
is not applicable in general. One of the breakthrough algorithm on the value-based RL, Q-learning [51], had been
proposedtolearnandapproximatetheoptimalaction-valuefunctionq .Later,modifiedconnectionistQ-learning[40]
∗
had been proposed, which is more widely known as SARSA. (Section 6.4 of [44]) On the other hands, optimizing the
target policy π directly with policy gradient algorithm was proposed by [45].
Emerging these early advances of RL and huge advances on a neural network and computing devices, recent
advances such as expected SARSA [50], double Q-learning [22], deep Q-learning [34], double delayed Q-learning [3],
advantage actor-critic (A2C) and asynchronous actor-critic (A3C) [33], proximal policy optimization (PPO) [43], and
other variants [9, 1, 5, 8, 4, 2] of the early advances for finding optimal q or π had been proposed.
∗ ∗
2
2.2 Active inference
The active inference environment rests on the partially observed Markov decision process settings with an observation
that comes from sensory input o and a hidden state s which is encoded in the agent’s latent space. We will discuss
t t
a continuous observation/hidden state space, a discrete time step, and a discrete action space A: At a current time
t < T with a given time horizon T, the agent receives an observation o . The agent encodes this observation to a
t
hidden state s in its internal generative model, (i.e., a generative model for the given environment in an agent) and
t
thensearchesfortheactionsequencethatminimizestheexpected future surprise basedontheagent’spriorpreference
p˜(o ) of a future observation o with τ >t. (i.e. The agent avoids an action which leads to unexpected and undesired
τ τ
future observations, which makes the agent surprised.)
In detail, we can formally illustrate the active inference agent’s process as follows: s and o are a hidden state
t t
and an observation at time t, respectively. In addition, π =(a ,a ,...,a ) is a sequence of actions. Let p(o ,s ) be
1 2 T 1:T 1:T
a generative model of the agent with its transition model p(s |s ,a ), and q(o ,s ,π) be a variational density.
t+1 t t 1:T 1:T
A distribution over policies q(π) will be determined later. From here, we can simplify the parameterized densities as
trainable neural networks with p(o |s ) as a decoder, q(s |o ) as an encoder, and p(s |s ,a ) as a transition network
t t t t t+1 t t
in our generative model.
First, we minimize the current surprise of the agent, which is defined as −logp(o ). Its upper bound can be
t
interpreted as the well-known negative ELBO term, which is frequently referred to as the variational free energy F
t
at time t in studies on active inference.
−logp(o )≤E [logq(s |o )−logp(o ,s )]=F (2)
t q(st|ot) t t t t t
Minimizing F provides an upper bound on the current surprise, and makes our networks in the generative model
t
well-fittedwithourknownobservationsoftheenvironmentanditsencodedstates.Forthefutureactionselection,the
total EFE G(s ) over all possible policies at the current state s at time t should be minimized.
t t
q(s ,π)
G(s )=E [log t+1:T ] (3)
t q(st+1:T,ot+1:T,π) p˜(s ,o )
t+1:T t+1:T
Focusingonthedistributionq(π),itisknownthatthetotalEFEG(s )isminimizedwhenthedistributionoverpolicies
t
q(π) follows σ(−G (s )), where σ(·) is a softmax over the policies and G (s ) is the EFE under a given state s for a
π t π t t
fixed sequence of actions π at time t. [32]
G (s )= (cid:88) G (τ,s )= (cid:88) E [log q(s τ |π) ] (4)
π t π t q(sτ,oτ|π) p˜(o )q(s |o )
τ τ τ
τ>t τ>t
This means that a lower EFE is obtained for a particular action sequence π; a lower future surprise will be expected
andadesiredbehaviorp˜(o)willbeobtained.Severalactiveinferencestudiesintroduceatemperatureparameterγ >0
such that q (π) = σ(−γG (s )) to control the agent’s behavior between exploration and exploitation. Because we
γ π t
know that the optimal distribution over the policies is q(π)=σ(−G (s )), the action selection problem boils down to
π t
a calculation of the expected free energy G (s ) of a given action sequence π.
π t
The learning process of active inference contains two parts: (1) learning an agent’s generative model with its
trainable neural networks p(o |s ), q(s |o ), and p(s |s ,a ) that explains the current observations and (2) learning
t t t t t+1 t t
to select an action that minimizes a future expected surprise of the agent by calculating the EFE of a given action
sequence.
3 EFE as a negative value: Between RL and active inference
Inthissection,wefirstextendthedefinitionofanEFEtoastochasticpolicy.Wethen,proposeanaction-conditioned
EFE that has a similar role as a negative action-value function in RL. Based on these extensions, we will repeat
the arguments in the active inference and claim that the RL algorithm with EFE as a negative value is equivalent to
controllingthepolicynetworktowardanidealdistributionintheactiveinference.Calculatingtheexpectedfreeenergy
G (s )forallpossibledeterministicpoliciesπ isintractableeveninatoy-exampletaskbecausethenumberofpolicies
π t
rapidly increases as the time horizon T and the number of actions |A| increases. Instead of searching for a number of
deterministic policies, we extend the concept of EFE to a stochastic policy based on a policy network φ = φ(a |s ).
t t
In this case, we also extend q(s |π) to q(s ,a |φ) := p(s |s ,a )φ(a |s ), which is a distribution over
τ τ τ−1 τ τ−1 τ−1 τ−1 τ−1
the states and actions, where each action a is only dependent on state s with φ(a |s ). Plugging in a
τ−1 τ−1 τ−1 τ−1
deterministic policy φ=π yields q(s ,a |π)=p(s |s ,a(cid:48)) with π(s )=a(cid:48), which is the same equation used in
τ τ−1 τ τ−1 τ−1
previous studies on active inference. Suppose we choose an action a based on the current state s and a given action
t t
network φ, its corresponding expected free energy term can then be written as follows. This can be interpreted as an
EFE of a sequence of policies (φ,φ,...,φ) by substituting our extensions for the probabilities in (4).
G φ (s t )=E (cid:81) τ>t q(sτ,oτ,aτ−1|φ) [ (cid:88) log p q ˜( ( o s τ τ ) , q a ( τ s − τ 1 |o |φ τ ) ) ] (5)
τ>t
3
Note that the agent uses the same action network φ(a |s ) for all τ, and we can therefore rewrite this equation in a
τ τ
recursive form.
(cid:34) (cid:35)
G φ (s t )=E (cid:81) τ>t q(sτ,oτ,aτ−1|φ) (cid:88) log p q ˜( ( o s τ τ ) , q a ( τ s − τ 1 |o |φ τ ) )
τ>t
(cid:20)
q(s ,a |φ)
=E (cid:81) τ>t q(sτ,oτ,aτ−1|φ) log p˜(o t+1 )q t ( + s 1 t+1 , t a t |o t+1 )
(cid:35)
+ (cid:88) log q(s τ ,a τ−1 |φ)
p˜(o )q(s |o )
τ τ τ
τ>t+1 (6)
(cid:20)
q(s ,a |φ)
=E log t+1 t
q(st+1,ot+1,at|φ) p˜(o )q(s |o )
t+1 t+1 t+1
(cid:35)
+E (cid:81) τ>t+1 q(sτ,oτ,aτ−1|φ) (cid:88) log p q ˜( ( o s τ τ ) , q a ( τ s − τ 1 |o |φ τ ) )
τ>t+1
(cid:20) (cid:21)
q(s ,a |φ)
=E log t+1 t +G (s )
q(st+1,ot+1,at|φ) p˜(o )q(s |o ) φ t+1
t+1 t+1 t+1
Replacing and fixing the first action a = a, an action-conditioned EFE G (s |a) of a given action a and a policy
t φ t
network φ is then defined as follows:
G (s |a) (7)
φ t
(cid:20) (cid:21)
q(s ,a |a =a)
:=E log t+1 t t +G (s )
q(st+1,ot+1,at|at=a) p˜(o ,s ) φ t+1
t+1 t+1
(cid:20) (cid:21)
p(s |s ,a =a)
=E log t+1 t t +G (s ) (8)
p(st+1|st,at=a)p(ot+1|st+1) p˜(o )q(s |o ) φ t+1
t+1 t+1 t+1
Taking an expectation over φ(a|s ), we obtain the relationship between G (s ) and G (s |a ).
t φ t φ t t
E [G (s |a)]=G (s )+H[φ(a|s )] (9)
φ(a|st) φ t φ t t
We may consider separating the distribution over the first-step action from φ to find an alternative distribution that
minimizestheEFE.Substitutingthedistributionoverthefirst-stepactionasq(a )insteadofφ(a ),weobtainitsone-
t t
stepsubstitutedEFEasindicatedbelow.ThiscanbeinterpretedastheEFEofasequenceofapolicies(q(a ),φ,...,φ)
t
(cid:20)
G 1 φ −step (s t )=E q(at)q(st+1,ot+1|at)(cid:81) τ>t+1 q(sτ,oτ|φ) logq(a t )
(cid:35)
+log q(s t+1 |a t ) + log (cid:89) q(s τ ,a τ−1 |φ)
p˜(o ,s ) p˜(o )q(s |o )
t+1 t+1 τ τ τ
τ>t+1
(cid:20) (cid:21)
=E logq(a )+G (s |a ) (10)
q(at) t φ t t
Undera given φ, the value above dependsonly onthe distributionq(a ). Thus,minimizing thequantityabove will
t
naturallyintroducethedistributionq∗(a )=σ (−G (s |a )),whichisknowntobesimilarintermsofactiveinference
t a φ t t
to the γ =1 case.
(cid:20) (cid:21)
Gφ (s )=E logq(a )+G (s |a )
1−step t q(at) t φ t t
(11)
(cid:88)
=KL(q(a )||q∗(a ))−log exp(−G (s |a ))
t t φ t t
at∈A
Therefore, Gφ (s ) ≥ −log (cid:80) exp(−G (s |a )), and the equality holds if and only if q(a ) = σ (−G (s |a )).
Let us furthe 1 r −s c t o ep nsid t er the temp a e t r ∈ a A ture hype φ rpa t ram t eter γ > 0 such that q∗(a ) = σ (−γG(s |a t )). Si a milar φ ly t t o t the
γ t a t t
4
above, we obtain the following:
(cid:20) (cid:21)
1
Gφ (s )= E γlogq(a )+γG (s |a )
1−step t γ q(at) t φ t t
(cid:20) (cid:21)
1
= E (1−γ)(−logq(a ))+logq(a )−logq∗(a )
γ q(at) t t γ t
1 (cid:88)
− log exp(−γG (s |a ))
γ φ t t (12)
at∈A
1 1
=( −1)H(q(a ))+ KL(q(a )||q∗(a ))
γ t γ t γ t
1 (cid:88)
− log exp(−γG (s |a ))
γ φ t t
at∈A
From the arguments above, we can conclude that (1) Gφ (s ) is minimized when q∗(a ) = σ (−G(s |a )), a
1−step t t a t t
‘natural case’ in which γ =1, which was heuristically set in the experiments described in [31]. (2) Plugging q∗(a )=
γ t
σ (−γG(s |a )) to q(a) in the equation above, we obtain
a t t
Gφ (s )=(1−γ)E [G (s |a )]−logD
1−step t q γ ∗(a) φ t t γ (13)
≈(1−γ)E [G (s |a )]+γ min{G (s |a )}
q∗(a) φ t t φ t t
γ at∈A
(cid:80)
where D = exp(−γG (s |a )), and the last comes from the smooth approximation to the maximum function
γ at∈A φ t t
of log-sum-exp. This approximation becomes accurate when the maximum is much larger than the others.
(cid:88)
−logD =−log exp(−γG (s |a ))
γ φ t t
at∈A (14)
≈−max{−γG (s |a )}=γ min G (s |a )
φ t t φ t t
at∈A at∈A
When γ = 1, the quantity Gφ (s ) is minimized with q∗(a ) = σ (−G(s |a )), and its minimum value can be
1−step t 1 t a t t
approximated as −logD ≈ min {G (s |a )}. When γ → ∞, q∗(a ) can be considered as a deterministic policy
1 at∈A φ t t γ t
seeking the smallest EFE, which leads to the following:
1 1 (cid:88)
Gφ (s )=( −1)H(q∗(a ))− log exp(−γG (s |a ))
1−step t γ γ t γ φ t t
at∈A
≈ min G (s |a ) (15)
φ t t
at∈A
Note that Q-learning with a negative EFE can be interpreted as q∗ with the case of γ = ∞. When γ (cid:38) 0, q∗(a )
γ γ t
convergestoauniformdistributionovertheactionspaceA,andthetemperaturehyperparameterγmotivatestheagent
to explore other actions with a greater EFE. Its weighted sum also converges to an average of the action-conditioned
EFE.
Considering the optimal policy φ∗ that seeks an action with a minimum EFE, we obtain the following:
G (s )=minG (s |a)
φ∗ t φ∗ t
a
(cid:20) p(s |s ,a =a) (cid:21) (16)
=minE log t+1 t t +G (s )
a p(st+1|st,at=a)p(ot+1|st+1) p˜(o t+1 )q(s t+1 |o t+1 ) φ∗ t+1
This equation is extremely similar to the Bellman optimality equation (2). Here, G (s |a) and G (s ) correspond
φ t φ t+1
to the action-value function and the state value function, respectively. Based on this similarity, we can consider the
first term log p(st+1|st,at=a) as a one-step negative reward (because active inference aims to minimize the expected
p˜(ot+1)q(st+1|ot+1)
free energy of the future) and EFE as a negative value function.
4 PPL: Prior Preference Learning from Experts
From the previous section, we verified that using log p(st+1|st,at) as a negative reward can handle EFE with
p˜(ot+1)q(st+1|ot+1)
traditional RL methods. Through a simple calculation, the given term can be decomposed as follows:
p(s |s ,a )
R :=−log t+1 t t (17)
t p˜(o )q(s |o )
t+1 t+1 t+1
p(s |s ,a )
=logp˜(o )+(−log t+1 t t )=R +R (18)
t+1 q(s |o ) t,i t,e
t+1 t+1
5
The first term measures the similarity between a preferred future and a predicted future, which is an intuitive
value. The second term is called the epistemic value, which encourages the exploration of an agent. [14] The epistemic
valuecanbecomputedeitherwithpriorknowledgeontheenvironmentofanagentorwithvariousalgorithmstolearn
a generative model. [23, 24]
The core key to calculating the one-step reward and learning EFE is the prior preference p˜(o ), which includes
t
information about the agent’s preferred observation and goal. Setting a prior preference for an agent is a challenging
pointforactiveinference.Asimplemethodthathasbeenusedinrecentstudies[49,53]istosetapriorpreferenceasa
Gaussiandistributionwithmeanofthegoalposition.However,itwouldbeinefficienttousethesamepriorpreference
for all observations o. We called this type of preference global preference. Taking a mountain-car environment as an
example, to reach the goal position, the car must move away from the goal position in an early time step. Therefore
p˜(o ) must contain information about local preference.
t
To solve the problems, we introduce prior preference learning from experts (PPL). Suppose that an agent can
access expert simulations S ={(o ,...,o )}N , where N is the number of simulations. From the expert simulations
i,1 i,T i=1
S, an agent can learn an expert’s prior p(o |o ) based on the current observation o . Model p(o |o ) captures the
t+1 t t t+1 t
expert’s local preference, which is more effective than the global preference.
Once learning the prior preference p(o |o ), we can calculate R given in (18). We can use any RL alogorithm to
t+1 t t
approximate EFE, G (s ) in (16). In Algorithm 1, we provide our pseudo-code of PPL with Q-learning.
φ∗ t
5 Related work
Active inference on RL. Our works are based on active inference and RL. Active inference was first introduced
in [11], inspired from neuroscience and free energy principle. It explains how a biological system is maintained with a
brain model. Furthermore, [19] treated the relation between active inference and RL. Early studies [12, 21, 14, 13, 41]
dealt with a tabular or model based problem due to computational cost.
Amongthem,theconceptofpreferencelearning appearedin[41],butthegoalofpreferencelearninginourworkand
[41] are clearly different: Our work proposed to design a reward based on active inference by learning prior preference
from the known expert, whereas [41] proposed an active inference algorithm to learn the environment by learning the
prior preference from interacting with the environment. The authors in [41] claimed that the agent’s behaviors can
be learned through active inference and preference learning even in the total absence of reward signals. However, our
mainpurposeistodesignarewardin(18)byincorporatingthetheoryofactiveinferenceandtheknownexperts.This
canbedonebylearningpriorpreferencep˜(o )=p(o |o )fromtheexperts’trajectory.Also,[41]mainlyfocusedona
t t t−1
discrete state-observation setting with a matrix calculation and a recursive update, whereas our work mainly focuses
on a continuous state-observation setting with a neural network. Thus, the preference learning in our work and [41]
can be clearly distinguished.
Recently, [49] introduced deep active inference which utilizes deep neural network to approximate the observation
and transition models on MountainCar environment. This work used EFE as an objective function and its gradient
propagates through the environment dynamics. To handle this problem, [49] used stochastic weights to apply an
evolutionary strategy [42], which is a method for gradient approximation. For a stable gradient approximation, about
104 environments were used in parallel.
[53, 31] introduced end-to-end differentiable models by including environment transition models. Both require
much less interaction with environments than before. [53] used the Monte Carlo sampling to approximate EFE and
used global prior preference. On the other hand [31] used bootstrapping methods and used common RL rewards with
model driven values derived from EFE. [31] verified that the model driven values induce faster and better results on
MountainCar environment. Furthermore, [48, 32] introduced a new KL objective called free energy of expected future
(FEEF) which is related to probabilistic RL [28, 39, 26, 27].
Our work extends the scope of active inference to inverse RL by learning a preferred observation from experts. A
common limitation of previous studies on active inference is the ambiguity of prior preference distribution. Previous
works have done in environments where its prior preference can be naturally expressed, which is clearly not true in
general.PPLisanactiveinferencebasedapproachfortheinverseRLproblemsetting,whichallowsustolearnaprior
preference from expert simulations. This broadens the scope of active inference and provides a new perspective about
a connection between active inference and RL.
Control as inference. [29] proposed control as inference framework, which interprets a control problem as a
probabilistic inference with an additional binary variable O that indicates whether given action is optimal or not.
t
Several studies on the formulation of RL as an inference problem [46, 25] have been proposed. Control as inference
measures a probability that a given action is optimal based on a given reward with p(O = 1|s ,a ) = exp(r(s ,a )).
t t t t t
That is, from the given reward and the chosen probability model, control as inference calculates the probability of
the given action a with the state s to be optimal. In contrast, active inference measures this probability as in (18)
t t
with a prior preference distribution p˜and constructs a reward, viewing EFE as a negative value function. Although
control as inference and active inference as a RL in Section 3 have a theoretical similarity based on a duality of a
controlproblemandaninference,ourproposedPPLcaninterprettherewardr(s ,a )andthemessagefunctionβ(s )
t t t
in [29] as EFE-related quantities. Therefore, learning the expert as a prior preference p˜ immediately constructs its
active inference based reward function and thereby applicable to a reward construction problem and several inverse
6
Algorithm 1: Inverse Q-Learning with Prior Preference Learning
Learning prior preference from expert simulations;
Input: Expert simultaions S =(o ,...o )N
i,1 i,T i=1
Initialize a prior preference network p˜ (o);
θ
while not converge do
Compute loss L (p˜(o ),o );
ppl t t+1
Update θ ←θ−α∇L
ppl
end
Output: p˜ (o)
θ
Learning EFE and forward dynamic of an environment;
Input: Prior preference p˜ (o)
θ
Initialize the forward dynamic p (o|s),T (s |s ,a ),q (s|o), and EFE network G (s ,a ). while not converge
η η t+1 t t η ξ t t
do
Reset environment;
for t←0 do
Select action a =argmax G(s ,a) with (cid:15)-greedy;
t a t
Observe new observation o ;
t+1
Compute R =log T(st+1|st,at) ;
t p˜(ot+1)q(st+1|ot+1)
Compute the environment model loss L ((p◦T ◦q)(o ),o );
model t t+1
Compute the EFE network loss L (G (s ,a ),R +max G (s ,a));
efe ξ t t t a ξ t+1
Update η ←η−α∇L and ξ ←ξ−α∇L fe.
d e
end
end
RL problems.
6 Experiments
In this section, we discuss and compare the experimental results of our proposed algorithm PPL and other inverse RL
algorithms on the several classical control environments. We evaluate our approach with a classic control environment
implemented in Open AI Gym [7]. First, we aim to compare the conventional global preference method to the PPL.
We expect the PPL to be effective in environments where the local and global preferences are different. Second, we
claim that our active inference based approach can achieve a compatible results with current inverse RL algorithms.
Expert simulations were obtained from Open AI RL baseline zoo [38].
6.1 PPL and global preference
First, we compare our proposed algorithm (setting 1 in Table 1) and its variants. Table 1 contains four experimental
settings, where the setting 1 is our proposed method and the others are experimental groups to compare the effects of
an expert batch, the epistemic value R , and PPL.
t,e
Expert Batch. WeuseexpertsimulationsforbatchsamplingwhenlearningEFEandaforwarddynamicmodel.
This allows an EFE network and a dynamic model to be trained even in states that do not reach the early stage of
learning. We use the deep Q-learning algorithm to learn the EFE network. Commonly used techniques in RL, such as
replay memory and target network [35] are used.
Reward. We observe how an epistemic value R in (18) influences the learning process and its performance.
t,e
Global preference. Global preference is a distribution over a state which can be naturally induced from the
agent’s goal, whereas our PPL is a learned prior preference from the expert’s simulations. Roughly, global preference
canbeunderstoodasahard-codedpriorpreferencebasedonthepriorknowledgeoftheenvironment,asa‘goaldirected
behavior’in[49].Detailedhard-codedglobalpreferencesinourexperimentscanbefoundinthebelowsub-subsection.
6.1.1 Environments and experiment details
We tested three classical control environments: Acrobot, Cartpole, and MountainCar. We used 5000 pairs of (o ,o )
t t+1
to train the prior preference p˜(o ). We used the same neural network architecture for all environments, except for
t+1
the number of input and output dimensions. During the training process, we clip the epistemic value to prevent a
gradient explosion while using the epistemic value in settings 1, 2, and 4. Note that we did not run setting 4 in the
Acrobot environment, because Acrobot is ambiguous in defining the global preference of the environment.
7
Table 1: Experment setting for PPL and global preference and its variants
Preference Batch sampling Reward
Setting 1 PPL RM + Experts R +R
t,i t,e
Setting 2 PPL RM R +R
t,i t,e
Setting 3 PPL RM + Experts R
t,i
Setting 4 GP RM + Experts R +R
t,i t,e
∗GP–Global Preference # RM–Replay Memory
Figure 1: Experiment results on three classical control environments: MountainCar-v0, Acrobot-v1, and CartPole-v1.
The curves in the figure were averaged over 50 runs, and the standard deviation of 50 runs is given as a shaded area.
Each policy was averaged out of 5 trials. All rewards of the environment follow the default settings of Open AI Gym.
Figure 2: Inverse RL Experiment results on MountainCar-v0 (left) and CartPole-v1 (right). The curves in the figure
wereaveragedover50runs,andthestandarddeviationof50runsisgivenasashadedarea.Notethatblackandgreen
dashed line on the right are overlapped. All rewards of the environment follow the default settings of Open AI Gym.
(BC : Behavioral Cloning, MaxEnt : Maximum Entropy)
6.1.2 Results and Discussions
Figure 1 shows the experimental results on MountainCar, Acrobot, and Cartpole. Their performances are compared
and benchmarked with the default reward of the environments.
PPL and global preference. Comparing setting 1 (blue line) and setting 2 (red line), it can be seen that PPL
is more efficient than the conventional global preference as expected. In particular, in MountainCar, we can see that
little learning is achieved. This seems to be because the difference between global and expert preferences is greater in
the MountainCar environment. In the Cartpole, setting 2 learned more slowly than setting 1.
Expert Batch. Comparing setting 1 (blue line) and setting 3 (orange line), it can be seen that using the expert
batchishelpfulforcertaintasks.WithAcrobotandMountainCar,theuseofanexpertbatchperformsbetterthanthe
case without an expert batch. However, the results without expert batch are marginally better than those of setting 1
for Cartpole. This is because an agent of Cartpole only moves near the initial position, and thus there is no need for
an expert batch to discover the dynamics of the generative model.
8
Figure3:ExperimentresultsonCartPole-v1usingpropsedPPLrewards.Theproposedrewardswereappliedtothree
RLalgorithms,DQN,A2C,andPPO.Thecurvesinthefigurewereaveragedover50runs,andthestandarddeviation
of 50 runs is given as a shaded area. The rewards presented in the figure are general rewards, not PPL rewards.
Epistemic Value. We found that the epistemic value in the EFE term does not significantly impact the training
process. Comparing setting 1 (blue line) and setting 4 (green line), the results were similar regardless of whether
the epistemic value was used. In Acrobot and MountainCar, standard deviations were marginally smaller, but there
were no significant differences between them. In the result of CartPole-v1, we found that setting 4 with no epistemic
value term learned faster than our proposed setting 1 at the beginning of the training process. We deduce that this
initial performance drop is due to the instability of the epistemic term. At the beginning of the training process, the
generative model is not learned, and thus the related epistemic term becomes unstable. We leave this issue to a future
study.
6.2 PPL and inverse RL algorithms
Second, we check that our proposed PPL is compatible with traditional inverse RL algorithms. We compared PPL
with behavioral cloning (BC, [37]) and maximum entropy inverse RL (MaxEnt, [52]) as benchmark models. We use
setting 1 in Table 1 as our proposed PPL here, and we test on MountainCar-v0 and CartPole-v1. Note that BC does
not need to interact with the environment and the state space was discretized to use original MaxEnt algorithm. We
also tried to run the experiment on Acrobot-v1 for PPL and other benchmarks, but we failed to make the agent learn
with MaxEnt. A discretized state space for MaxEnt becomes larger exponentially to its state dimension. We think it
is due to a larger dimension of its state space compared to the others. Therefore, we only report that PPL and BC
give similar results to Acrobot-v1.
We verified that our method PPL gives compatible results on the MountainCar-v0 and CartPole-v1. Compared to
MaxEnt,PPLshowsbetterresultsthanMaxEntonbothenvironments.NotethatMaxEntneedsmuchmoreepisodes
to converge. Also, PPL obtained almost similar mean rewards to BC on MountainCar-v0, whereas BC gives better
results than PPL on CartPole-v1.
6.3 PPL with various RL algorithms
Third,weapplythePPLrewardstovariousRLalgorithmstocheckvalidityofthepropsedreward.WecompareDQN,
A2C, and PPO in Figure 3. Since A2C and PPO are on-policy alogorithms, no expert batch is required. We verified
that all three algorithms work well with PPL and the propsed reward. Among them, A2c shows the highest average
reward. A2C and PPO had smaller amplitudes than DQN. Otherwise, the standard deviations of three algorithms are
not significantly different.
9
7 Conclusion
Inthispaper,weintroducedtheuseofactiveinferencefromtheperspectiveofRL.Althoughactiveinferenceemerged
from the Bayesian model of cognitive process, we show that the concepts of active inference, especially for EFE, are
highly related to RL using the bootstrapping method. The only difference is that, the value function of RL is based
on a reward, while active inference is based on the prior preference. We also show that active inference can provide
insights to solve the inverse RL problems. Using expert simulations, an agent can learn a local prior preference, which
is more effective than the global preference. Furthermore, our proposed active inference based reward with a prior
preference and a generative model makes the previous invser RL problems free from an ill-posed state. Our work on
active inference is complementary to RL because it can be applied to model-based RL for the design of reward and
model-free RL for learning of generative models. Although, our method is promising and has theoretical background,
but its practicality is still limited. We only tested on relatively simple environments and learning process is unstable
because of the KL term. And it also depends on RL algorithm. If these further issues are addressed in the future, we
think it will be a much more promising method.
Acknowledgments
ThisworkwassupportedbytheNationalResearchFoundationofKorea(NRF)grantfundedbytheKoreagovernment
(MSIT)(No.2017R1E1A1A03070105,NRF-2019R1A5A1028324)andbyInstituteforInformation&Communications
Technology Promotion (IITP) grant funded by the Korea government(MSIP) (No.2019-0-01906, Artificial Intelligence
Graduate School Program (POSTECH)).
References
[1] Bilal H Abed-alguni. Bat q-learning algorithm. Jordanian Journal of Computers and Information Technology
(JJCIT), 3(1):56–77, 2017.
[2] Bilal H Abed-alguni. Action-selection method for reinforcement learning based on cuckoo search algorithm.
Arabian Journal for Science and Engineering, 43(12):6771–6785, 2018.
[3] BilalHAbed-alguniandMohammadAshrafOttom. Doubledelayedq-learning. InternationalJournalofArtificial
Intelligence, 16(2):41–59, 2018.
[4] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,
Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. pages 5048–5058, 2017.
[5] Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement learning. In
International Conference on Machine Learning, pages 449–458. PMLR, 2017.
[6] Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym, 2016.
[8] Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement learning with
quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[9] Adithya M Devraj and Sean P Meyn. Zap q-learning. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 2232–2241, 2017.
[10] Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl J. Friston. Deep active inference agents using
monte-carlo methods. CoRR, abs/2006.04176, 2020.
[11] K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain. Journal of Physiology-Paris,
100:70–87, 2006.
[12] KarlFriston. Thefree-energyprinciple:aunifiedbraintheory? Nature reviews. Neuroscience,11:127–38,022010.
[13] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active infer-
ence: A process theory. Neural Computation, 29(1):1–49, 2017. PMID: 27870614.
[14] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pezzulo.
Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, 2015.
[15] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal control without
cost functions. Biological cybernetics, 106:523–41, 08 2012.
10
[16] KarlFriston,PhilippSchwartenbeck,ThomasFitzgerald,MichaelMoutoussis,TimBehrens,andRaymondDolan.
The anatomy of choice: active inference and agency. Frontiers in Human Neuroscience, 7:598, 2013.
[17] Karl J. Friston. A free energy principle for biological systems. Entropy, 14(11):2100–2121, 2012.
[18] Karl J. Friston. A free energy principle for a particular physics, 2019.
[19] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference? PloS one,
4(7):e6421, 2009.
[20] KarlJ.Friston,MarcoLin,ChristopherD.Frith,GiovanniPezzulo,J.AllanHobson,andSashaOndobaka. Active
inference, curiosity and insight. Neural Computation, 29(10):2633–2683, 2017.
[21] Karl J. Friston, J´er´emie Mattout, and James Kilner. Action understanding and active inference. Biol. Cybern.,
104(1-2):137–160, 2011.
[22] Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23:2613–2621, 2010.
[23] MaximilianIgl,LuisaZintgraf,TuanAnhLe,FrankWood,andShimonWhiteson. Deepvariationalreinforcement
learning for pomdps. arXiv preprint arXiv:1806.02426, 2018.
[24] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,
Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for
atari. arXiv preprint arXiv:1903.00374, 2019.
[25] Bert Kappen, Vicenc¸ G´omez, and Manfred Opper. Optimal control as a graphical model inference problem.
CoRR, abs/0901.0633, 2009.
[26] Hilbert J Kappen, Vicen¸c G´omez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine learning, 87(2):159–182, 2012.
[27] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient
exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.
[28] SergeyLevine. Reinforcementlearningand controlas probabilistic inference:Tutorialandreview. arXiv preprint
arXiv:1805.00909, 2018.
[29] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR,
abs/1805.00909, 2018.
[30] Aliz´ee Lopez-Persem, Philippe Domenech, and Mathias Pessiglione. How prior preferences determine decision-
making frames and biases in the human brain. Neuroscience, 5:e20317, 2016.
[31] Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical Psychology,
96:102348, 2020.
[32] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the expected free energy? CoRR,
abs/2004.08128, 2020.
[33] VolodymyrMnih,Adri`aPuigdom`enechBadia,MehdiMirza,AlexGraves,TimothyP.Lillicrap,TimHarley,David
Silver,andKorayKavukcuoglu. Asynchronousmethodsfordeepreinforcementlearning. InMaria-FlorinaBalcan
andKilianQ.Weinberger,editors,Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings,
pages 1928–1937. JMLR.org, 2016.
[34] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Mar-
tin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.
[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,
IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis. Human-
level control through deep reinforcement learning. Nat., 518(7540):529–533, 2015.
[36] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics, 113(5-
6):495–513, 2019.
[37] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information
processing systems, pages 305–313, 1989.
[38] Antonin Raffin. Rl baselines zoo. https://github.com/araffin/rl-baselines-zoo, 2018.
11
[39] Konrad Cyrus Rawlik. On probabilistic inference approaches to stochastic optimal control. 2013.
[40] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. Citeseer,
1994.
[41] NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. Activeinference:Demystifiedandcompared. Neural
Comput., 33(3):674–712, 2021.
[42] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable
alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
[44] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[45] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Sara A. Solla, Todd K. Leen, and Klaus-Robert Mu¨ller,
editors, Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA,
November 29 - December 4, 1999], pages 1057–1063. The MIT Press, 1999.
[46] Emanuel Todorov. General duality between optimal control and estimation. In Proceedings of the 47th IEEE
Conference on Decision and Control, CDC 2008, December 9-11, 2008, Cancu´n, Mexico,pages4286–4292.IEEE,
2008.
[47] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference. CoRR,
abs/1911.10601, 2019.
[48] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement learning through
active inference. CoRR, abs/2002.12636, 2020.
[49] Kai Ueltzh¨offer. Deep active inference. Biol. Cybern., 112(6):547–573, December 2018.
[50] HarmVanSeijen,HadoVanHasselt,ShimonWhiteson,andMarcoWiering. Atheoreticalandempiricalanalysis
of expected sarsa. In 2009 ieee symposium on adaptive dynamic programming and reinforcement learning, pages
177–184. IEEE, 2009.
[51] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
[52] BrianDZiebart,AndrewLMaas,JAndrewBagnell,andAnindKDey. Maximumentropyinversereinforcement
learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.
[53] O.C¸atal,T.Verbelen,J.Nauta,C.D.Boom,andB.Dhoedt. Learningperceptionandplanningwithdeepactive
inference. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3952–3956, 2020.
12