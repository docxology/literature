Prior Preference Learning from Experts:
Designing a Reward with Active Inference
Jin young Shin§, Cheolhyeong Kim§, Hyung Ju Hwang∗
Department of Mathematics
POSTECH
Pohang, 37673, Republic of Korea
{sjy6006, tyty4, hjhwang}@postech.ac.kr
December 14, 2021
Abstract
Active inference may be deﬁned as Bayesian modeling of a brain with a biologically plausible model of the agent.
Its primary idea relies on the free energy principle and the prior preference of the agent. An agent will choose an
action that leads to its prior preference for a future observation. In this paper, we claim that active inference can
be interpreted using reinforcement learning (RL) algorithms and ﬁnd a theoretical connection between them. We
extend the concept of expected free energy (EFE), which is a core quantity in active inference, and claim that
EFE can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical
connection, we propose a simple but novel method for learning a prior preference from experts. This illustrates that
the problem with inverse RL can be approached with a new perspective of active inference. Experimental results
of prior preference learning show the possibility of active inference with EFE-based rewards and its application to
an inverse RL problem.
1 Introduction
Active inference [19] is a theory emerging from cognitive science using a Bayesian modeling of the brain function
[11, 12, 14, 16], predictive coding [21, 30], and the free energy principle [17, 36, 18]. It states that the agents choose
actions to minimize an expected future surprise [15, 13, 20], which is a measurement of the diﬀerence between an agent’s
prior preference and expected future. Minimization of an expected future surprise can be achieved by minimizing the
expected free energy (EFE), which is a core quantity of active inference. Although active inference and EFE have been
inspired and derived from cognitive science using a biologically plausible brain function model, its usage in RL tasks
is still limited owing to its computational issues and prior-preference design. [31, 10]
First, EFE requires heavy computational cost. A precise computation of an EFE theoretically averages all possible
policies, which is clearly intractable as an action space Aand a time horizon T increase in size. Several attempts have
been made to calculate the EFE in a tractable manner, such as limiting the future time horizon from t to t+ H [47],
and applying Monte-Carlo based sampling methods [10, 53] for the search policies.
Second, it is unclear how the prior preferences should be set. This is the same question as how to design the
rewards in the RL algorithm. In recent studies [10, 53, 49] the agent’s prior preference is simply set as the ﬁnal goal
of a given environment for every time step. There are some environments in which the prior preference can be set as
time independent. However, most prior preferences in RL problems are neither simple nor easy to design because prior
preferences of short and long-sighted futures should generally be treated in diﬀerent ways.
In this paper, we ﬁrst claim that there is a theoretical connection between active inference and RL algorithms. We
then propose prior preference learning (PPL), a simple and novel method for learning a prior-preference of an active
inference from an expert simulation. In Section 2, we brieﬂy introduce the basic concepts of RL and active inference.
From the previous deﬁnition of the EFE of a deterministic policy, in Section 3, we extend the previous concepts of
active inference and theoretically demonstrate that it can be analyzed in view of the RL algorithm. We extend this
quantity to a stochastic policy network and deﬁne an action-conditioned EFE for a given action and a given policy
network. Following [31] using a bootstrapping argument, we show that the optimal distribution over the ﬁrst-step
action induced from active inference can be interpreted using Q-Learning. Consequently, we show that EFE can be
treated as a negative value function from an RL perspective. From this connection, in Section 4, we propose a novel
inverse RL algorithm for designing EFE-based rewards, by learning a prior preference from expert demonstrations.
Through such expert demonstrations, an agent learns its prior preference given the observation to achieve a ﬁnal goal,
which can eﬀectively handle the diﬀerence between local and global preferences. It will extend the scope of active
inference to inverse RL problem. Our experiments in Section 6 show the applicability of active inference based rewards
∗Corresponding Author
§The authors have equally contributed to this paper.
1
arXiv:2101.08937v3  [cs.LG]  13 Dec 2021
using EFE to an inverse RL problem. We compare our method under various experimental setting including global
preference and expert prior preferece. Moreover, we compare our method with traditional inverse RL algorithms and
test our method with various RL algorithm. It shows our method is comparable to traditional inverse RL algorithm.
2 Preliminaries
2.1 Reinforcement Learning
Reinforcement learning (RL) is a computational approach to learning from interaction, that a way of learning what
to do for a given current situation. [44] Given a current state, when an agent chooses an action, then the state may
change and a signal (either positive or negative) is given to the agent. From this interaction, the agent improves its
choice of actions. The ﬁnal goal of the RL is to maximize the positive interaction.
In order to analyse the ’positive interaction’ in a computational way, one can identify four main subelements of a
RL system as in [44]: A policy, a reward signal, a value function, and a model of the environment.
• A policy π: S→A is the agent’s behaviour, a mapping from a set of states Sto a set of actions A. One denotes
the policy π as either π(s) = aor π(a|s): A deterministic policy π(s) = agives an action afrom a given state s.
A stochastic policy π(s|a) denotes a probability mass/density function on Agiven the current state s.
• A reward r is real-valued number, which quantiﬁes a single interaction from the environment and deﬁnes the
goal of RL in a short term.
• A value function (vπ(s),qπ(s,a)) is a prediction of a future cumulative reward, which evaluates the current state
s, or a pair (s,a) of the current state and the ﬁrst-step actiona. A state value functionvπ(s) denotes the expected
total amount of reward following the policy π from current state s. An action-value function qπ(s,a) denotes the
expected cumulative reward given a current state s and the ﬁrst-step action a.
• A model of the environment p(s′,r|s,a) denotes a probability mass/density function of a future state s′ and a
reward signal r given a current state s and an action a.
Repeating the interaction between the environment, one obtains a sequence of (state, action rewards) as (S0,A0,R1,S1,A1,R2,···),
where S0 denotes the initial state of the environment. A return Gt at time t with a discount rate γ is given by
Gt = Rt+1 + γRt+2 + γ2Rt+3 + ··· = ∑∞
k=0 γkRt+k+1, the cumulative future reward. The return Gt can be recur-
sively written as Gt = Rt+1 + γGt+1. Discount rate 0 ≤γ ≤1 determines the evaluation on the future reward.
Note that the state-value function vπ(s) is an expected future cumulative reward given state s under policy π, thus
vπ(s) = Eπ[Gt|St = s]. From the recursive relation of the cumulative reward and a Markov assumption on the states
St, a Bellman equation [6] for a state-value function vπ(s) can be written as below.
vπ(s) =
∑
a
π(a|s)
∑
s′,r
p(s′,r|s,a)[r+ γvπ(s′)]
the similar argument also holds for action-value function qπ(s,a).
qπ(s,a) =
∑
s′,r
p(s′,r|s,a)[r+ γ
∑
a′
π(a′|s′)qπ(s′,a′)]
The agent’s ﬁnal goal is to ﬁnd an optimal policy π∗ that maximizes a sum of rewards . For such optimal policy π∗,
the following Bellman optimality equation [6] on the optimal state value function vπ∗ := v∗and action-value function
qπ∗ := q∗holds.
v∗(s) = max
a
∑
s′,r
p(s′,r|s,a)[r+ γv∗(s′)] (1)
q∗(s,a) =
∑
s′,a
p(s′,r|s,a)[r+ max
a′
q∗(s′,a′)]
Once the optimal q∗or v∗is obtained by solving the Bellman optimality equation, then the corresponding optimal
policy π∗can be obtained. However, explicitly solving the equation without a knowledge on the environmentp(s′,r|s,a)
is not applicable in general. One of the breakthrough algorithm on the value-based RL, Q-learning [51], had been
proposed to learn and approximate the optimal action-value function q∗. Later, modiﬁed connectionist Q-learning [40]
had been proposed, which is more widely known as SARSA. (Section 6.4 of [44]) On the other hands, optimizing the
target policy π directly with policy gradient algorithm was proposed by [45].
Emerging these early advances of RL and huge advances on a neural network and computing devices, recent
advances such as expected SARSA [50], double Q-learning [22], deep Q-learning [34], double delayed Q-learning [3],
advantage actor-critic (A2C) and asynchronous actor-critic (A3C) [33], proximal policy optimization (PPO) [43], and
other variants [9, 1, 5, 8, 4, 2] of the early advances for ﬁnding optimal q∗or π∗had been proposed.
2
2.2 Active inference
The active inference environment rests on the partially observed Markov decision process settings with an observation
that comes from sensory input ot and a hidden state st which is encoded in the agent’s latent space. We will discuss
a continuous observation/hidden state space, a discrete time step, and a discrete action space A: At a current time
t < Twith a given time horizon T, the agent receives an observation ot. The agent encodes this observation to a
hidden state st in its internal generative model, (i.e., a generative model for the given environment in an agent) and
then searches for the action sequence that minimizes the expected future surprise based on the agent’s prior preference
˜p(oτ) of a future observation oτ with τ >t. (i.e. The agent avoids an action which leads to unexpected and undesired
future observations, which makes the agent surprised.)
In detail, we can formally illustrate the active inference agent’s process as follows: st and ot are a hidden state
and an observation at time t, respectively. In addition, π= (a1,a2,...,a T) is a sequence of actions. Let p(o1:T,s1:T) be
a generative model of the agent with its transition model p(st+1|st,at), and q(o1:T,s1:T,π) be a variational density.
A distribution over policies q(π) will be determined later. From here, we can simplify the parameterized densities as
trainable neural networks with p(ot|st) as a decoder, q(st|ot) as an encoder, and p(st+1|st,at) as a transition network
in our generative model.
First, we minimize the current surprise of the agent, which is deﬁned as −log p(ot). Its upper bound can be
interpreted as the well-known negative ELBO term, which is frequently referred to as the variational free energy Ft
at time t in studies on active inference.
−log p(ot) ≤Eq(st|ot)[log q(st|ot) −log p(ot,st)] = Ft (2)
Minimizing Ft provides an upper bound on the current surprise, and makes our networks in the generative model
well-ﬁtted with our known observations of the environment and its encoded states. For the future action selection, the
total EFE G(st) over all possible policies at the current state st at time t should be minimized.
G(st) = Eq(st+1:T,ot+1:T,π)[log q(st+1:T,π)
˜p(st+1:T,ot+1:T)] (3)
Focusing on the distribution q(π), it is known that the total EFEG(st) is minimized when the distribution over policies
q(π) follows σ(−Gπ(st)), where σ(·) is a softmax over the policies and Gπ(st) is the EFE under a given state st for a
ﬁxed sequence of actions π at time t. [32]
Gπ(st) =
∑
τ>t
Gπ(τ,st) =
∑
τ>t
Eq(sτ,oτ|π)[log q(sτ|π)
˜p(oτ)q(sτ|oτ)] (4)
This means that a lower EFE is obtained for a particular action sequence π; a lower future surprise will be expected
and a desired behavior ˜p(o) will be obtained. Several active inference studies introduce a temperature parameter γ >0
such that qγ(π) = σ(−γGπ(st)) to control the agent’s behavior between exploration and exploitation. Because we
know that the optimal distribution over the policies is q(π) = σ(−Gπ(st)), the action selection problem boils down to
a calculation of the expected free energy Gπ(st) of a given action sequence π.
The learning process of active inference contains two parts: (1) learning an agent’s generative model with its
trainable neural networks p(ot|st), q(st|ot), and p(st+1|st,at) that explains the current observations and (2) learning
to select an action that minimizes a future expected surprise of the agent by calculating the EFE of a given action
sequence.
3 EFE as a negative value: Between RL and active inference
In this section, we ﬁrst extend the deﬁnition of an EFE to a stochastic policy. We then, propose an action-conditioned
EFE that has a similar role as a negative action-value function in RL. Based on these extensions, we will repeat
the arguments in the active inference and claim that the RL algorithm with EFE as a negative value is equivalent to
controlling the policy network toward an ideal distribution in the active inference. Calculating the expected free energy
Gπ(st) for all possible deterministic policies π is intractable even in a toy-example task because the number of policies
rapidly increases as the time horizon T and the number of actions |A|increases. Instead of searching for a number of
deterministic policies, we extend the concept of EFE to a stochastic policy based on a policy network φ = φ(at|st).
In this case, we also extend q(sτ|π) to q(sτ,aτ−1|φ) := p(sτ|sτ−1,aτ−1)φ(aτ−1|sτ−1), which is a distribution over
the states and actions, where each action aτ−1 is only dependent on state sτ−1 with φ(aτ−1|sτ−1). Plugging in a
deterministic policy φ= π yields q(sτ,aτ−1|π) = p(sτ|sτ−1,a′) with π(sτ−1) = a′, which is the same equation used in
previous studies on active inference. Suppose we choose an action at based on the current state st and a given action
network φ, its corresponding expected free energy term can then be written as follows. This can be interpreted as an
EFE of a sequence of policies ( φ,φ,...,φ ) by substituting our extensions for the probabilities in (4).
Gφ(st) = E∏
τ>t q(sτ,oτ,aτ−1|φ)[
∑
τ>t
log q(sτ,aτ−1|φ)
˜p(oτ)q(sτ|oτ)] (5)
3
Note that the agent uses the same action network φ(aτ|sτ) for all τ, and we can therefore rewrite this equation in a
recursive form.
Gφ(st) = E∏
τ>t q(sτ,oτ,aτ−1|φ)
[∑
τ>t
log q(sτ,aτ−1|φ)
˜p(oτ)q(sτ|oτ)
]
= E∏
τ>t q(sτ,oτ,aτ−1|φ)
[
log q(st+1,at|φ)
˜p(ot+1)q(st+1,at|ot+1)
+
∑
τ>t+1
log q(sτ,aτ−1|φ)
˜p(oτ)q(sτ|oτ)
]
= Eq(st+1,ot+1,at|φ)
[
log q(st+1,at|φ)
˜p(ot+1)q(st+1|ot+1)
+E∏
τ>t+1 q(sτ,oτ,aτ−1|φ)
∑
τ>t+1
log q(sτ,aτ−1|φ)
˜p(oτ)q(sτ|oτ)
]
= Eq(st+1,ot+1,at|φ)
[
log q(st+1,at|φ)
˜p(ot+1)q(st+1|ot+1) + Gφ(st+1)
]
(6)
Replacing and ﬁxing the ﬁrst action at = a, an action-conditioned EFE Gφ(st|a) of a given action a and a policy
network φ is then deﬁned as follows:
Gφ(st|a) (7)
:= Eq(st+1,ot+1,at|at=a)
[
log q(st+1,at|at = a)
˜p(ot+1,st+1) + Gφ(st+1)
]
= Ep(st+1|st,at=a)p(ot+1|st+1)
[
log p(st+1|st,at = a)
˜p(ot+1)q(st+1|ot+1) + Gφ(st+1)
]
(8)
Taking an expectation over φ(a|st), we obtain the relationship between Gφ(st) and Gφ(st|at).
Eφ(a|st)[Gφ(st|a)] = Gφ(st) + H[φ(a|st)] (9)
We may consider separating the distribution over the ﬁrst-step action from φ to ﬁnd an alternative distribution that
minimizes the EFE. Substituting the distribution over the ﬁrst-step action as q(at) instead of φ(at), we obtain its one-
step substituted EFE as indicated below. This can be interpreted as the EFE of a sequence of a policies (q(at),φ,...,φ )
Gφ
1−step(st) = Eq(at)q(st+1,ot+1|at) ∏
τ>t+1 q(sτ,oτ|φ)
[
log q(at)
+ log q(st+1|at)
˜p(ot+1,st+1) + log
∏
τ>t+1
q(sτ,aτ−1|φ)
˜p(oτ)q(sτ|oτ)
]
= Eq(at)
[
log q(at) + Gφ(st|at)
]
(10)
Under a given φ, the value above depends only on the distribution q(at). Thus, minimizing the quantity above will
naturally introduce the distribution q∗(at) = σa(−Gφ(st|at)), which is known to be similar in terms of active inference
to the γ = 1 case.
Gφ
1−step(st) = Eq(at)
[
log q(at) + Gφ(st|at)
]
= KL(q(at)||q∗(at)) −log
∑
at∈A
exp(−Gφ(st|at))
(11)
Therefore, Gφ
1−step(st) ≥−log ∑
at∈Aexp(−Gφ(st|at)), and the equality holds if and only if q(at) = σa(−Gφ(st|at)).
Let us further consider the temperature hyperparameter γ >0 such that q∗
γ(at) = σa(−γG(st|at)). Similarly to the
4
above, we obtain the following:
Gφ
1−step(st) = 1
γEq(at)
[
γlog q(at) + γGφ(st|at)
]
= 1
γEq(at)
[
(1 −γ)(−log q(at)) + logq(at) −log q∗
γ(at)
]
−1
γ log
∑
at∈A
exp(−γGφ(st|at))
= ( 1
γ −1)H(q(at)) + 1
γKL(q(at)||q∗
γ(at))
−1
γ log
∑
at∈A
exp(−γGφ(st|at))
(12)
From the arguments above, we can conclude that (1) Gφ
1−step(st) is minimized when q∗(at) = σa(−G(st|at)), a
‘natural case’ in which γ = 1, which was heuristically set in the experiments described in [31]. (2) Plugging q∗
γ(at) =
σa(−γG(st|at)) to q(a) in the equation above, we obtain
Gφ
1−step(st) = (1 −γ)Eq∗γ(a)[Gφ(st|at)] −log Dγ
≈(1 −γ)Eq∗γ(a)[Gφ(st|at)] + γ min
at∈A
{Gφ(st|at)} (13)
where Dγ = ∑
at∈Aexp(−γGφ(st|at)), and the last comes from the smooth approximation to the maximum function
of log-sum-exp. This approximation becomes accurate when the maximum is much larger than the others.
−log Dγ = −log
∑
at∈A
exp(−γGφ(st|at))
≈−max
at∈A
{−γGφ(st|at)}= γ min
at∈A
Gφ(st|at)
(14)
When γ = 1, the quantity Gφ
1−step(st) is minimized with q∗
1(at) = σa(−G(st|at)), and its minimum value can be
approximated as −log D1 ≈minat∈A{Gφ(st|at)}. When γ →∞, q∗
γ(at) can be considered as a deterministic policy
seeking the smallest EFE, which leads to the following:
Gφ
1−step(st) = ( 1
γ −1)H(q∗
γ(at)) −1
γ log
∑
at∈A
exp(−γGφ(st|at))
≈min
at∈A
Gφ(st|at) (15)
Note that Q-learning with a negative EFE can be interpreted as q∗
γ with the case of γ = ∞. When γ ↘0, q∗
γ(at)
converges to a uniform distribution over the action spaceA, and the temperature hyperparameterγmotivates the agent
to explore other actions with a greater EFE. Its weighted sum also converges to an average of the action-conditioned
EFE.
Considering the optimal policy φ∗that seeks an action with a minimum EFE, we obtain the following:
Gφ∗(st) = min
a
Gφ∗(st|a)
= min
a
Ep(st+1|st,at=a)p(ot+1|st+1)
[
log p(st+1|st,at = a)
˜p(ot+1)q(st+1|ot+1) + Gφ∗(st+1)
] (16)
This equation is extremely similar to the Bellman optimality equation (2). Here, Gφ(st|a) and Gφ(st+1) correspond
to the action-value function and the state value function, respectively. Based on this similarity, we can consider the
ﬁrst term log p(st+1|st,at=a)
˜p(ot+1)q(st+1|ot+1) as a one-step negative reward (because active inference aims to minimize the expected
free energy of the future) and EFE as a negative value function.
4 PPL: Prior Preference Learning from Experts
From the previous section, we veriﬁed that using log p(st+1|st,at)
˜p(ot+1)q(st+1|ot+1) as a negative reward can handle EFE with
traditional RL methods. Through a simple calculation, the given term can be decomposed as follows:
Rt : = −log p(st+1|st,at)
˜p(ot+1)q(st+1|ot+1) (17)
= log ˜p(ot+1) + (−log p(st+1|st,at)
q(st+1|ot+1) ) = Rt,i + Rt,e (18)
5
The ﬁrst term measures the similarity between a preferred future and a predicted future, which is an intuitive
value. The second term is called the epistemic value, which encourages the exploration of an agent. [14] The epistemic
value can be computed either with prior knowledge on the environment of an agent or with various algorithms to learn
a generative model. [23, 24]
The core key to calculating the one-step reward and learning EFE is the prior preference ˜ p(ot), which includes
information about the agent’s preferred observation and goal. Setting a prior preference for an agent is a challenging
point for active inference. A simple method that has been used in recent studies [49, 53] is to set a prior preference as a
Gaussian distribution with mean of the goal position. However, it would be ineﬃcient to use the same prior preference
for all observations o. We called this type of preference global preference. Taking a mountain-car environment as an
example, to reach the goal position, the car must move away from the goal position in an early time step. Therefore
˜p(ot) must contain information about local preference.
To solve the problems, we introduce prior preference learning from experts (PPL). Suppose that an agent can
access expert simulations S = {(oi,1,...,o i,T)}N
i=1, where N is the number of simulations. From the expert simulations
S, an agent can learn an expert’s prior p(ot+1|ot) based on the current observation ot. Model p(ot+1|ot) captures the
expert’s local preference, which is more eﬀective than the global preference.
Once learning the prior preference p(ot+1|ot), we can calculate Rt given in (18). We can use any RL alogorithm to
approximate EFE, Gφ∗(st) in (16). In Algorithm 1, we provide our pseudo-code of PPL with Q-learning.
5 Related work
Active inference on RL.Our works are based on active inference and RL. Active inference was ﬁrst introduced
in [11], inspired from neuroscience and free energy principle. It explains how a biological system is maintained with a
brain model. Furthermore, [19] treated the relation between active inference and RL. Early studies [12, 21, 14, 13, 41]
dealt with a tabular or model based problem due to computational cost.
Among them, the concept ofpreference learning appeared in [41], but the goal of preference learning in our work and
[41] are clearly diﬀerent: Our work proposed to design a reward based on active inference by learning prior preference
from the known expert, whereas [41] proposed an active inference algorithm to learn the environment by learning the
prior preference from interacting with the environment . The authors in [41] claimed that the agent’s behaviors can
be learned through active inference and preference learning even in the total absence of reward signals. However, our
main purpose is to design a reward in (18) by incorporating the theory of active inference and the known experts. This
can be done by learning prior preference ˜p(ot) = p(ot|ot−1) from the experts’ trajectory. Also, [41] mainly focused on a
discrete state-observation setting with a matrix calculation and a recursive update, whereas our work mainly focuses
on a continuous state-observation setting with a neural network. Thus, the preference learning in our work and [41]
can be clearly distinguished.
Recently, [49] introduced deep active inference which utilizes deep neural network to approximate the observation
and transition models on MountainCar environment. This work used EFE as an objective function and its gradient
propagates through the environment dynamics. To handle this problem, [49] used stochastic weights to apply an
evolutionary strategy [42], which is a method for gradient approximation. For a stable gradient approximation, about
104 environments were used in parallel.
[53, 31] introduced end-to-end diﬀerentiable models by including environment transition models. Both require
much less interaction with environments than before. [53] used the Monte Carlo sampling to approximate EFE and
used global prior preference. On the other hand [31] used bootstrapping methods and used common RL rewards with
model driven values derived from EFE. [31] veriﬁed that the model driven values induce faster and better results on
MountainCar environment. Furthermore, [48, 32] introduced a new KL objective called free energy of expected future
(FEEF) which is related to probabilistic RL [28, 39, 26, 27].
Our work extends the scope of active inference to inverse RL by learning a preferred observation from experts. A
common limitation of previous studies on active inference is the ambiguity of prior preference distribution. Previous
works have done in environments where its prior preference can be naturally expressed, which is clearly not true in
general. PPL is an active inference based approach for the inverse RL problem setting, which allows us to learn a prior
preference from expert simulations. This broadens the scope of active inference and provides a new perspective about
a connection between active inference and RL.
Control as inference. [29] proposed control as inference framework, which interprets a control problem as a
probabilistic inference with an additional binary variable Ot that indicates whether given action is optimal or not.
Several studies on the formulation of RL as an inference problem [46, 25] have been proposed. Control as inference
measures a probability that a given action is optimal based on a given reward with p(Ot = 1|st,at) = exp(r(st,at)).
That is, from the given reward and the chosen probability model, control as inference calculates the probability of
the given action at with the state st to be optimal. In contrast, active inference measures this probability as in (18)
with a prior preference distribution ˜p and constructs a reward, viewing EFE as a negative value function. Although
control as inference and active inference as a RL in Section 3 have a theoretical similarity based on a duality of a
control problem and an inference, our proposed PPL can interpret the reward r(st,at) and the message function β(st)
in [29] as EFE-related quantities. Therefore, learning the expert as a prior preference ˜ p immediately constructs its
active inference based reward function and thereby applicable to a reward construction problem and several inverse
6
Algorithm 1:Inverse Q-Learning with Prior Preference Learning
Learning prior preference from expert simulations;
Input: Expert simultaions S = (oi,1,...oi,T)N
i=1
Initialize a prior preference network ˜pθ(o);
while not converge do
Compute loss Lppl(˜p(ot),ot+1);
Update θ←θ−α∇Lppl
end
Output: ˜pθ(o)
Learning EFE and forward dynamic of an environment;
Input: Prior preference ˜pθ(o)
Initialize the forward dynamic pη(o|s),Tη(st+1|st,at),qη(s|o), and EFE network Gξ(st,at). while not converge
do
Reset environment;
for t←0 do
Select action at = argmaxaG(st,a) with ϵ-greedy;
Observe new observation ot+1;
Compute Rt = log T(st+1|st,at)
˜p(ot+1)q(st+1|ot+1) ;
Compute the environment model loss Lmodel((p◦T ◦q)(ot),ot+1);
Compute the EFE network loss Lefe(Gξ(st,at),Rt + maxaGξ(st+1,a));
Update η←η−α∇Ld and ξ←ξ−α∇Lefe.
end
end
RL problems.
6 Experiments
In this section, we discuss and compare the experimental results of our proposed algorithm PPL and other inverse RL
algorithms on the several classical control environments. We evaluate our approach with a classic control environment
implemented in Open AI Gym [7]. First, we aim to compare the conventional global preference method to the PPL.
We expect the PPL to be eﬀective in environments where the local and global preferences are diﬀerent. Second, we
claim that our active inference based approach can achieve a compatible results with current inverse RL algorithms.
Expert simulations were obtained from Open AI RL baseline zoo [38].
6.1 PPL and global preference
First, we compare our proposed algorithm (setting 1 in Table 1) and its variants. Table 1 contains four experimental
settings, where the setting 1 is our proposed method and the others are experimental groups to compare the eﬀects of
an expert batch, the epistemic value Rt,e, and PPL.
Expert Batch. We use expert simulations for batch sampling when learning EFE and a forward dynamic model.
This allows an EFE network and a dynamic model to be trained even in states that do not reach the early stage of
learning. We use the deep Q-learning algorithm to learn the EFE network. Commonly used techniques in RL, such as
replay memory and target network [35] are used.
Reward. We observe how an epistemic value Rt,e in (18) inﬂuences the learning process and its performance.
Global preference. Global preference is a distribution over a state which can be naturally induced from the
agent’s goal, whereas our PPL is a learned prior preference from the expert’s simulations. Roughly, global preference
can be understood as a hard-coded prior preference based on the prior knowledge of the environment, as a ‘goal directed
behavior’ in [49]. Detailed hard-coded global preferences in our experiments can be found in the below sub-subsection.
6.1.1 Environments and experiment details
We tested three classical control environments: Acrobot, Cartpole, and MountainCar. We used 5000 pairs of ( ot,ot+1)
to train the prior preference ˜p(ot+1). We used the same neural network architecture for all environments, except for
the number of input and output dimensions. During the training process, we clip the epistemic value to prevent a
gradient explosion while using the epistemic value in settings 1, 2, and 4. Note that we did not run setting 4 in the
Acrobot environment, because Acrobot is ambiguous in deﬁning the global preference of the environment.
7
Table 1: Experment setting for PPL and global preference and its variants
Preference Batch sampling Reward
Setting 1 PPL RM + Experts Rt,i + Rt,e
Setting 2 PPL RM Rt,i + Rt,e
Setting 3 PPL RM + Experts Rt,i
Setting 4 GP RM + Experts Rt,i + Rt,e
∗GP–Global Preference # RM–Replay Memory
Figure 1: Experiment results on three classical control environments: MountainCar-v0, Acrobot-v1, and CartPole-v1.
The curves in the ﬁgure were averaged over 50 runs, and the standard deviation of 50 runs is given as a shaded area.
Each policy was averaged out of 5 trials. All rewards of the environment follow the default settings of Open AI Gym.
Figure 2: Inverse RL Experiment results on MountainCar-v0 (left) and CartPole-v1 (right). The curves in the ﬁgure
were averaged over 50 runs, and the standard deviation of 50 runs is given as a shaded area. Note that black and green
dashed line on the right are overlapped. All rewards of the environment follow the default settings of Open AI Gym.
(BC : Behavioral Cloning, MaxEnt : Maximum Entropy)
6.1.2 Results and Discussions
Figure 1 shows the experimental results on MountainCar, Acrobot, and Cartpole. Their performances are compared
and benchmarked with the default reward of the environments.
PPL and global preference.Comparing setting 1 (blue line) and setting 2 (red line), it can be seen that PPL
is more eﬃcient than the conventional global preference as expected. In particular, in MountainCar, we can see that
little learning is achieved. This seems to be because the diﬀerence between global and expert preferences is greater in
the MountainCar environment. In the Cartpole, setting 2 learned more slowly than setting 1.
Expert Batch. Comparing setting 1 (blue line) and setting 3 (orange line), it can be seen that using the expert
batch is helpful for certain tasks. With Acrobot and MountainCar, the use of an expert batch performs better than the
case without an expert batch. However, the results without expert batch are marginally better than those of setting 1
for Cartpole. This is because an agent of Cartpole only moves near the initial position, and thus there is no need for
an expert batch to discover the dynamics of the generative model.
8
Figure 3: Experiment results on CartPole-v1 using propsed PPL rewards. The proposed rewards were applied to three
RL algorithms, DQN, A2C, and PPO. The curves in the ﬁgure were averaged over 50 runs, and the standard deviation
of 50 runs is given as a shaded area. The rewards presented in the ﬁgure are general rewards, not PPL rewards.
Epistemic Value. We found that the epistemic value in the EFE term does not signiﬁcantly impact the training
process. Comparing setting 1 (blue line) and setting 4 (green line), the results were similar regardless of whether
the epistemic value was used. In Acrobot and MountainCar, standard deviations were marginally smaller, but there
were no signiﬁcant diﬀerences between them. In the result of CartPole-v1, we found that setting 4 with no epistemic
value term learned faster than our proposed setting 1 at the beginning of the training process. We deduce that this
initial performance drop is due to the instability of the epistemic term. At the beginning of the training process, the
generative model is not learned, and thus the related epistemic term becomes unstable. We leave this issue to a future
study.
6.2 PPL and inverse RL algorithms
Second, we check that our proposed PPL is compatible with traditional inverse RL algorithms. We compared PPL
with behavioral cloning (BC, [37]) and maximum entropy inverse RL (MaxEnt, [52]) as benchmark models. We use
setting 1 in Table 1 as our proposed PPL here, and we test on MountainCar-v0 and CartPole-v1. Note that BC does
not need to interact with the environment and the state space was discretized to use original MaxEnt algorithm. We
also tried to run the experiment on Acrobot-v1 for PPL and other benchmarks, but we failed to make the agent learn
with MaxEnt. A discretized state space for MaxEnt becomes larger exponentially to its state dimension. We think it
is due to a larger dimension of its state space compared to the others. Therefore, we only report that PPL and BC
give similar results to Acrobot-v1.
We veriﬁed that our method PPL gives compatible results on the MountainCar-v0 and CartPole-v1. Compared to
MaxEnt, PPL shows better results than MaxEnt on both environments. Note that MaxEnt needs much more episodes
to converge. Also, PPL obtained almost similar mean rewards to BC on MountainCar-v0, whereas BC gives better
results than PPL on CartPole-v1.
6.3 PPL with various RL algorithms
Third, we apply the PPL rewards to various RL algorithms to check validity of the propsed reward. We compare DQN,
A2C, and PPO in Figure 3. Since A2C and PPO are on-policy alogorithms, no expert batch is required. We veriﬁed
that all three algorithms work well with PPL and the propsed reward. Among them, A2c shows the highest average
reward. A2C and PPO had smaller amplitudes than DQN. Otherwise, the standard deviations of three algorithms are
not signiﬁcantly diﬀerent.
9
7 Conclusion
In this paper, we introduced the use of active inference from the perspective of RL. Although active inference emerged
from the Bayesian model of cognitive process, we show that the concepts of active inference, especially for EFE, are
highly related to RL using the bootstrapping method. The only diﬀerence is that, the value function of RL is based
on a reward, while active inference is based on the prior preference. We also show that active inference can provide
insights to solve the inverse RL problems. Using expert simulations, an agent can learn a local prior preference, which
is more eﬀective than the global preference. Furthermore, our proposed active inference based reward with a prior
preference and a generative model makes the previous invser RL problems free from an ill-posed state. Our work on
active inference is complementary to RL because it can be applied to model-based RL for the design of reward and
model-free RL for learning of generative models. Although, our method is promising and has theoretical background,
but its practicality is still limited. We only tested on relatively simple environments and learning process is unstable
because of the KL term. And it also depends on RL algorithm. If these further issues are addressed in the future, we
think it will be a much more promising method.
Acknowledgments
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government
(MSIT) (No. 2017R1E1A1A03070105, NRF-2019R1A5A1028324) and by Institute for Information & Communications
Technology Promotion (IITP) grant funded by the Korea government(MSIP) (No.2019-0-01906, Artiﬁcial Intelligence
Graduate School Program (POSTECH)).
References
[1] Bilal H Abed-alguni. Bat q-learning algorithm. Jordanian Journal of Computers and Information Technology
(JJCIT), 3(1):56–77, 2017.
[2] Bilal H Abed-alguni. Action-selection method for reinforcement learning based on cuckoo search algorithm.
Arabian Journal for Science and Engineering , 43(12):6771–6785, 2018.
[3] Bilal H Abed-alguni and Mohammad Ashraf Ottom. Double delayed q-learning. International Journal of Artiﬁcial
Intelligence, 16(2):41–59, 2018.
[4] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,
Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. pages 5048–5058, 2017.
[5] Marc G Bellemare, Will Dabney, and R´ emi Munos. A distributional perspective on reinforcement learning. In
International Conference on Machine Learning , pages 449–458. PMLR, 2017.
[6] Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym, 2016.
[8] Will Dabney, Mark Rowland, Marc G Bellemare, and R´ emi Munos. Distributional reinforcement learning with
quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018.
[9] Adithya M Devraj and Sean P Meyn. Zap q-learning. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 2232–2241, 2017.
[10] Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl J. Friston. Deep active inference agents using
monte-carlo methods. CoRR, abs/2006.04176, 2020.
[11] K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain. Journal of Physiology-Paris ,
100:70–87, 2006.
[12] Karl Friston. The free-energy principle: a uniﬁed brain theory? Nature reviews. Neuroscience, 11:127–38, 02 2010.
[13] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active infer-
ence: A process theory. Neural Computation, 29(1):1–49, 2017. PMID: 27870614.
[14] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pezzulo.
Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, 2015.
[15] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal control without
cost functions. Biological cybernetics, 106:523–41, 08 2012.
10
[16] Karl Friston, Philipp Schwartenbeck, Thomas Fitzgerald, Michael Moutoussis, Tim Behrens, and Raymond Dolan.
The anatomy of choice: active inference and agency. Frontiers in Human Neuroscience, 7:598, 2013.
[17] Karl J. Friston. A free energy principle for biological systems. Entropy, 14(11):2100–2121, 2012.
[18] Karl J. Friston. A free energy principle for a particular physics, 2019.
[19] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference? PloS one ,
4(7):e6421, 2009.
[20] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka. Active
inference, curiosity and insight. Neural Computation, 29(10):2633–2683, 2017.
[21] Karl J. Friston, J´ er´ emie Mattout, and James Kilner. Action understanding and active inference. Biol. Cybern.,
104(1-2):137–160, 2011.
[22] Hado Hasselt. Double q-learning. Advances in neural information processing systems , 23:2613–2621, 2010.
[23] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational reinforcement
learning for pomdps. arXiv preprint arXiv:1806.02426 , 2018.
[24] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,
Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for
atari. arXiv preprint arXiv:1903.00374 , 2019.
[25] Bert Kappen, Vicen¸ c G´ omez, and Manfred Opper. Optimal control as a graphical model inference problem.
CoRR, abs/0901.0633, 2009.
[26] Hilbert J Kappen, Vicen¸ c G´ omez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine learning, 87(2):159–182, 2012.
[27] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov. Eﬃcient
exploration via state marginal matching. arXiv preprint arXiv:1906.05274 , 2019.
[28] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint
arXiv:1805.00909, 2018.
[29] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR,
abs/1805.00909, 2018.
[30] Aliz´ ee Lopez-Persem, Philippe Domenech, and Mathias Pessiglione. How prior preferences determine decision-
making frames and biases in the human brain. Neuroscience, 5:e20317, 2016.
[31] Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical Psychology ,
96:102348, 2020.
[32] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the expected free energy? CoRR,
abs/2004.08128, 2020.
[33] Volodymyr Mnih, Adri` a Puigdom` enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David
Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan
and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016 , volume 48 of JMLR Workshop and Conference Proceedings,
pages 1928–1937. JMLR.org, 2016.
[34] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Mar-
tin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.
[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,
Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-
level control through deep reinforcement learning. Nat., 518(7540):529–533, 2015.
[36] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics, 113(5-
6):495–513, 2019.
[37] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information
processing systems, pages 305–313, 1989.
[38] Antonin Raﬃn. Rl baselines zoo. https://github.com/araffin/rl-baselines-zoo, 2018.
11
[39] Konrad Cyrus Rawlik. On probabilistic inference approaches to stochastic optimal control. 2013.
[40] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems , volume 37. Citeseer,
1994.
[41] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystiﬁed and compared. Neural
Comput., 33(3):674–712, 2021.
[42] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable
alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 , 2017.
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[44] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[45] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Sara A. Solla, Todd K. Leen, and Klaus-Robert M¨ uller,
editors, Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA,
November 29 - December 4, 1999] , pages 1057–1063. The MIT Press, 1999.
[46] Emanuel Todorov. General duality between optimal control and estimation. In Proceedings of the 47th IEEE
Conference on Decision and Control, CDC 2008, December 9-11, 2008, Canc´ un, Mexico, pages 4286–4292. IEEE,
2008.
[47] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference. CoRR,
abs/1911.10601, 2019.
[48] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement learning through
active inference. CoRR, abs/2002.12636, 2020.
[49] Kai Ueltzh¨ oﬀer. Deep active inference. Biol. Cybern., 112(6):547–573, December 2018.
[50] Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and empirical analysis
of expected sarsa. In 2009 ieee symposium on adaptive dynamic programming and reinforcement learning , pages
177–184. IEEE, 2009.
[51] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
[52] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement
learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.
[53] O. C ¸ atal, T. Verbelen, J. Nauta, C. D. Boom, and B. Dhoedt. Learning perception and planning with deep active
inference. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3952–3956, 2020.
12