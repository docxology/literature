ACTIVE INFERENCE ON DISCRETE STATE -SPACES – A
SYNTHESIS
A PREPRINT
Lancelot Da Costa1,2,∗, Thomas Parr2, Noor Sajid2, Sebastijan Veselic2, Victorita Neacsu2, Karl Friston2
1Department of Mathematics, Imperial College London
2Wellcome Centre for Human Neuroimaging, University College London
March 31, 2020
ABSTRACT
Active inference is a normative principle underwriting perception, action, planning, decision-making
and learning in biological or artiﬁcial agents. From its inception, its associated process theory has
grown to incorporate complex generative models, enabling simulation of a wide range of complex
behaviours. Due to successive developments in active inference, it is often difﬁcult to see how its
underlying principle relates to process theories and practical implementation. In this paper, we try
to bridge this gap by providing a complete mathematical synthesis of active inference on discrete
state-space models. This technical summary provides an overview of the theory, derives neuronal
dynamics from ﬁrst principles and relates this dynamics to biological processes. Furthermore, this
paper provides a fundamental building block needed to understand active inference for mixed gener-
ative models; allowing continuous sensations to inform discrete representations. This paper may be
used as follows: to guide research towards outstanding challenges, a practical guide on how to im-
plement active inference to simulate experimental behaviour, or a pointer towards various in-silico
neurophysiological responses that may be used to make empirical predictions.
Keywords: active inference, free energy principle, process theory, variational Bayesian inference, Markov decision
process, explained.
Contents
1 Introduction 2
2 Active inference 4
3 Discrete state-space generative models 5
4 Variational Bayesian inference 7
4.1 Free energy and model evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 On the family of approximate posteriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 Computing the variational free energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 Perception 12
∗Author correspondence: l.da-costa@imperial.ac.uk
arXiv:2001.07203v2  [q-bio.NC]  28 Mar 2020
5.1 Plausibility of neuronal dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6 Planning, decision-making and action selection 13
6.1 Planning and decision-making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6.2 Action selection, policy-independent state-estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Biological plausibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.4 Pruning of policy trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
6.5 Discussion of the action-perception cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7 Properties of the expected free energy 15
8 Learning 16
9 Structure learning 19
9.1 Bayesian model reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
9.2 Bayesian model expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
10 Discussion 22
11 Conclusion 23
A More complex generative models 23
A.1 Learning B and D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.2 Complexifying the prior over policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.3 Multiple state and outcome modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.4 Deep temporal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B Expected free energy 25
C Computing expected free energy 26
C.1 Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.2 Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.3 Novelty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1 Introduction
Active inference is a normative principle underlying perception, action, planning, decision-making and learning in
biological or artiﬁcial agents. It postulates that these processes may all be seen as optimising two complementary
objective functions; namely, a variational free energy, which measures the ﬁt between an internal model and (past)
sensory observations, and an expected free energy, which scores possible (future) courses of action in relation to
prior preferences. Active inference has been employed to simulate a wide range of complex behaviours, including
planning and navigation [1], reading [2], curiosity and abstract rule learning [3], saccadic eye movements [4], visual
foraging [5,6], visual neglect [7], hallucinations [8], niche construction [9,10], social conformity [11], impulsivity [12],
image recognition [13], and the mountain car problem [14–16]. The key idea that underwrites these simulations is that
creatures use an internal forward (generative) model to predict their sensory input, which they use to infer the causes
of these data.
2
Early formulations of active inference employed generative models expressed in continuous space and time (for an
introduction see [17], for a review see [18]), with behaviour modelled as a continuously evolving random dynamical
system. However, we know that some processes in the brain conform better to discrete, hierarchical, representations,
compared to continuous representations (e.g., visual working memory [19, 20], state estimation via place cells [21,
22], language, etc). Reﬂecting this, many of the paradigms studied in neuroscience are naturally framed as discrete
state-space problems. Decision-making tasks are a prime candidate for this, as they often entail a series of discrete
alternatives that an agent needs to choose among (e.g., multi-arm bandit tasks [23–25], multi-step decision tasks [26]).
This explains why – in active inference – agent behaviour is often modelled using a discrete state-space formulation:
the particular applications of which are summarised in Table 1. More recently, mixed generative models [27] –
combining discrete and continuous states – have been used to model behaviour involving discrete and continuous
representations (e.g., decision-making and movement [28], speech production and recognition [29], pharmacologically
induced changes in eye-movement control [30] or reading; involving continuous visual sampling informing inferences
about discrete semantics [27]).
Table 1: Applications of active inference (discrete state-space).
Application Description References
Decision-making under
uncertainty
Initial formulation of active inference on partially observable
Markov decision processes.
[31]
Optimal control Application of KL or risk sensitive control in an engineering
benchmark – the mountain car problem.
[14, 16]
Evidence accumulation Illustrating the role of evidence accumulation in
decision-making through an urns task.
[32, 33]
Psychopathology Simulation of addictive choice behaviour. [34]
Dopamine The precision of beliefs about policies provides a plausible
description of dopaminergic discharges.
[35, 36]
Functional magnetic
resonance imaging
Empirical prediction and validation of dopaminergic
discharges.
[37]
Maximal utility theory Evidence in favour of surprise minimization as opposed to
utility maximisation in human decision-making.
[38]
Social cognition Examining the effect of prior preferences on interpersonal
inference.
[39]
Exploration-exploitation
dilemma
Casting behaviour as expected free energy minimising
accounts for epistemic and pragmatic choices.
[40]
Habit learning and action
selection
Formulating learning as an inferential process and action
selection as Bayesian model averaging.
[41, 42]
Scene construction and
anatomy of time
Mean-ﬁeld approximation for multi-factorial hidden states,
enabling high dimensional representations of the
environment.
[5, 43]
Electrophysiological responses Synthesising various in-silico neurophysiological responses
via a gradient descent on free energy. E.g., place-cell activity,
mismatch negativity, phase-precession, theta sequences,
theta-gamma coupling and dopaminergic discharges.
[44]
Structure learning, curiosity
and insight
Simulation of artiﬁcial curiosity and abstract rule learning.
Structure learning via Bayesian model reduction.
[3]
Hierarchical temporal
representations
Generalisation to hierarchical generative models with deep
temporal structure and simulation of reading.
[2, 45]
Computational
neuropsychology
Simulation of visual neglect, hallucinations, and prefrontal
syndromes under alternative pathological priors.
[7, 46–49]
3
Neuromodulation Use of precision parameters to manipulate exploration during
saccadic searches; associating uncertainty with cholinergic
and noradrenergic systems.
[6, 30, 50, 51]
Decisions to movements Mixed generative models combining discrete and continuous
states to implement decisions through movement.
[27, 28]
Planning, navigation and niche
construction
Agent induced changes in environment (generative process);
decomposition of goals into subgoals.
[1, 9, 10]
Atari games Active inference compares favourably to reinforcement
learning in the game of Doom.
[52]
Machine learning Scaling active inference to more complex machine learning
problems.
[53]
Due to the pace of recent theoretical advances in active inference, it is often difﬁcult to retain a comprehensive overview
of its process theory and practical implementation. In this paper, we hope to provide a comprehensive (mathemati-
cal) synthesis of active inference on discrete state-space models. This technical summary provides an overview of
the theory, derives the associated (neuronal) dynamics from ﬁrst principles and relates these to known biological pro-
cesses. Furthermore, this paper and [18] provide the building blocks necessary to understand active inference on mixed
generative models. This paper can be read as a practical guide on how to implement active inference for simulating
experimental behaviour, or a pointer towards various in-silico neuro- and electro- physiological responses that can be
tested empirically.
This paper is structured as follows. Section 2 is a high-level overview of active inference. The following sections
elucidate the formulation by deriving the entire process theory from ﬁrst principles; incorporating perception, planning
and decision-making. This formalises the action-perception cycle: 1) an agent is presented with a stimulus, 2) it infers
its latent causes, 3) plans into the future and 4) realises its preferred course of action; and repeat. This enactive cycle
allows us to explore the dynamics of synaptic plasticity, which mediate learning of the contingencies of the world at
slower timescales. We conclude in section 9 with an overview of structure learning in active inference.
2 Active inference
To survive in a changing environment, biological (and artiﬁcial) agents must maintain their sensations within a cer-
tain hospitable range (i.e., maintaining homeostasis through allostasis). In brief, active inference proposes that agents
achieve this by optimising two complementary objective functions, a variational free energy and an expected free
energy. In short, the former measures the ﬁt between an internal (generative) model of its sensations and sensory ob-
servations, while the latter scores each possible course of action in terms of its ability to reach the range of “preferred”
states of being.
Our ﬁrst premise is that agents represent the world through an internal model. Through minimisation of variational
free energy, this model becomes a good model of the environment. In other words, this probabilistic model and the
probabilistic beliefs2 that it encodes are continuously updated to mirror the environment and its dynamics. Such a
world model is considered to be generative; in that it is able to generate predictions about sensations (e.g., during
planning or dreaming), given beliefs about future states of being. If an agent senses a heat source (e.g., another agent)
via some temperature receptors, the sensation of warmth represents an observed outcome and the temperature of the
heat source a hidden state; minimisation of variational free energy then ensures that beliefs about hidden states closely
match the true temperature. Formally, the generative model is a joint probability distribution over possible hidden
states and sensory consequences – that speciﬁes how the former cause the latter – and minimisation of variational free
energy enables to "invert" the model; i.e., determine the most likely hidden states given sensations. The variational
free energy is the negative evidence lower bound that is optimised in variational Bayes in machine learning [54, 55].
Technically – by minimising variational free energy – agents perform approximate Bayesian inference [56,57], which
enables them to infer the causes of their sensations (e.g., perception). This is the point of contact between active
inference and the Bayesian brain [58–60]. Crucially, agents may incorporate an optimism bias [61, 62] in their model;
thereby scoring certain “preferred” sensations as more likely. This lends a higher plausibility to those courses of action
2By beliefs we mean Bayesian beliefs, i.e., probability distributions over a variable of interest (e.g., current position). Beliefs
are therefore used in the sense of Bayesian belief updating or belief propagation – as opposed to propositional or folk psychology
beliefs.
4
that realise these sensations. In other words, a preference is simply something an agent (believes it) is likely to work
towards.
To maintain homeostasis, and ensure survival, agents must minimise surprise3. Since the generative model scores pre-
ferred outcomes as more likely, minimising surprise corresponds to maximising model evidence4). In active inference,
this is assured by the aforementioned processes; indeed, the variational free energy turns out to be an upper bound on
surprise and minimising expected free energy ensures preferred outcomes are realised, thereby avoiding surprise on
average.
Active inference can thus be framed as the minimisation of surprise [63–66] by perception and action. In discrete
state models – of the sort discussed here – this means agents select from different possible courses of action (i.e.,
policies) in order to realise their preferences and thus minimise the surprise that they expect to encounter in the future.
This enables a Bayesian formulation of the perception-action cycle [67]: agents perceive the world by minimising
variational free energy, ensuring their model is consistent with past observations, and act by minimising expected free
energy, to make future sensations consistent with their model. This account of behaviour can be concisely framed as
self-evidencing [68].
In contrast to other normative models of behaviour, active inference is a ‘ﬁrst principle’ account, which is grounded
in statistical physics [69, 70]. Active inference describes the dynamics of systems that persist (i.e., do not dissipate)
during some timescale of interest, and that can be statistically segregated from their environment – conditions which
are satisﬁed by biological systems. Mathematically, the ﬁrst condition means that the system is at non-equilibrium
steady-state (NESS). This implies the existence of a steady-state probability density to which the system self-organises
and returns to after perturbation (i.e., the agent’s preferences). The statistical segregation condition is the presence of
a Markov blanket (c.f., Figure 1) [71, 72]: a set of variables through which states internal and external to the system
interact (e.g., the skin is a Markov blanket for the human body). Under these assumptions it can be shown that the
states internal to the system parameterise Bayesian beliefs about external states and can be cast a process of variational
free energy minimisation. This coincides with existing approaches to approximate inference [54,73–75]. Furthermore,
it can be shown that the most likely courses of action taken by those systems are those which minimise expected free
energy – a quantity that subsumes many existing constructs in science and engineering (see section 7).
By subscribing to the above assumptions, it is possible to describe the behaviour of viable living systems as performing
active inference – the remaining challenge is to determine the computational and physiological processes that they
implement to do so. This paper aims to summarise a possible answers to this question, by reviewing the technical
details of a process theory for active inference on discrete state-space generative models, ﬁrst presented in [44]. Note
that it is important to distinguish between active inference as a principle (presented above) from active inference as a
process theory. The former is a consequence of fundamental assumptions about living systems, while the latter is a
hypothesis concerning the computational and biological processes in the brain that might implement active inference.
The ensuing process theories theory can then be used to predict plausible neuronal dynamics and electrophysiological
responses that are elicited experimentally.
3 Discrete state-space generative models
The generative model [54] expresses how the agent represents the world. This is a joint probability distribution over
sensory data and the hidden (or latent) causes of these data. The sorts of discrete state-space generative models
used in active inference are speciﬁcally suited to represent discrete time series and decision-making tasks. These can
be expressed as variants of partially observable Markov decision processes (POMDPs; [76]): from simple Markov
decision processes [77–79] to generalisations in the form of deep probabilistic (hierarchical) models [2, 80, 81]. For
clarity, the process theory is derived for the simplest model that facilitates understanding of subsequent generalisations;
namely, a POMDP where the agent holds beliefs about the probability of the initial state (speciﬁed asD), the transition
probabilities from one state to the next (deﬁned as matrix B) and the probability of states given outcomes (i.e., the
likelihood matrix A); see Figure 2.
3In information theory, the surprise (a.k.a., surprisal) associated with an outcome under a generative model is given by
−log p(o). This speciﬁes the extent to which an observation is unusual and surprises the agent – but this does not mean that
the agent consciously experiences surprise. In information theory this kind of surprise is known as self-information.
4In Bayesian statistics, the model evidence (often referred to as marginal likelihood) associated with a generative model isp(o)
– the probability of observed outcomes according to the model (sometimes this is written as p(o|m), explicitly conditioning upon
a model). The model evidence scores the goodness of the model as an explanation of data that are sampled, by rewarding accuracy
and penalising complexity, which avoids overﬁtting.
5
Table 2: Glossary of terms and notation.
Notation Meaning Type
S Set of all possible (hidden) states. Finite set of cardinality m> 0.
sτ (Hidden) state at time τ. In computations, if sτ
evaluates to the ith possible state, then interpret it as
the ith unit vector in Rm.
Random variable over S.
s1:t Sequence of hidden states s1,...,s t. Random variables over St.
O Set of all possible outcomes. Finite set of cardinality n> 0.
oτ Outcome at time τ. In computations, if oτ evaluates
to the jth possible outcome, then interpret it as the jth
unit vector in Rn.
Random variable over O.
o1:t Sequence of outcomes o1,...,o t Random variables over Ot.
T Number of timesteps in a trial of observation epochs
under the generative model.
Positive integer.
U Set of all possible actions. Finite set.
Π Set of all allowable policies; i.e., sequences of actions. Finite subset of UT.
π Policy. Random variable over Π, or element of
Π depending on context.
Q Approximate posterior distribution. Probability distribution over the latent
variables of the generative model.
F,Fπ Variational free energy and variational free energy
conditioned upon a policy.
Functionals of Q.
G Expected free energy. Function deﬁned on Π.
Cat Categorical distribution; probability distribution over
a ﬁnite set assigning strictly positive probabilities.
Probability distribution over a ﬁnite set
of cardinality kwith parameter space
{x∈Rk|xi >0,∑
ixi = 1}
Dir Dirichlet distribution (conjugate prior of the
categorical distribution); probability distribution over
the parameter space of the categorical distribution,
parameterised by a vector of positive reals.
Probability distribution over
{x∈Rk|xi >0,∑
ixi = 1}, itself
parameterised by an element of (R>0)k.
X•i,Xki ith column and (k,i)th element of matrix X. Matrix indexing convention.
·,⊗,⊙,⊙ Respectively inner product, Kronecker product,
element-wise product and element-wise power.
Following existing active inference literature, we
adopt the convention X·Y := XTY for matrices.
Operation on vectors and matrices.
A Likelihood matrix. The probability of the
state-outcome pair oτ,sτ is given by oτ ·Asτ.
Random variable over the subset of
Mn×m(R) with columns in
{x∈Rk|xi >0,∑
ixi = 1}.
B Matrix of transition probabilities from one state to the
next state given action πτ−1. The probability of
possible state sτ, given sτ−1 and action πτ−1 is
sτ ·Bπτ−1 sτ−1.
Matrix in Mm×m(R) with columns in
{x∈Rk|xi >0,∑
ixi = 1}.
a,a Parameters of prior and approximate posterior beliefs
about A.
Matrices in Mn×m(R>0).
6
a0,a0 Matrices of the same size as a,a, with homogeneous
columns; any of its ith column elements are denoted
by ai0,ai0 and deﬁned by
ai0 = ∑n
j=1 aji,ai0 = ∑n
j=1 aji.
Matrices in Mn×m(R>0).
log,Γ,ψ Natural logarithm, gamma function and digamma
function. By convention these functions are taken
component-wise on vectors and matrices.
Functions.
EP(X)[f(X)] Expectation of random variable f(X) under a
probability density P(X), taken component-wise if
f(X) is a matrix. EP(X)[f(X)] :=
∫
f(X)P(X) dX
Real-valued operator on random
variables.
A A := EQ(A)[A] = a ⊙a⊙(−1)
0 Matrix in Mn×m(R>0).
logA logA := EQ(A)[log A] = ψ(a) −ψ(a0). Note that
logA ̸= log A!
Matrix in Mn×m(R).
σ Softmax function or normalised exponential.
σ(x)k = exk∑
iexi
Function
Rk →{x∈Rk|xi >0,∑
ixi = 1}
H[P] Shannon entropy of a probability distribution P.
Explicitly, H[P] = EP(x)[−log P(x)]
Functional over probability
distributions.
As mentioned above, a substantial body of work justiﬁes describing certain neuronal representations with discrete
state-space generative models (e.g., [19, 20, 87]). Furthermore, it has been long known that – at the level of neu-
ronal populations – computations occur periodically (i.e., in distinct and sometimes nested oscillatory bands). Sim-
ilarly, there is evidence for sequential computation in a number of processes (e.g., attention [88–90], visual percep-
tion [91, 92]) and at different levels of the neuronal hierarchy [2, 93], in line with ideas from hierarchical predictive
processing [94, 95]. This accommodates the fact that visual saccadic sampling of observations occurs at a frequency
of approximately 4Hz [28]. The relatively slow presentation of a discrete sequence of observations enables inferences
to be performed in peristimulus time by (much) faster neuronal dynamics.
Active inference, implicitly, accounts for fast and slow neuronal dynamics. At each time-step the agent observes an
outcome, from which it infers the past, present and future (hidden) states through perception. This underwrites a plan
into the future, by evaluating (the expected free energy of) possible policies. The inferred (best) policies specify the
most likely action, which is executed. At a slower timescale, parameters encoding the contingencies of the world (e.g.,
A), are inferred. This is referred to as learning. Even more slowly, the structure of the generative model is updated
to better account for available observations – this is called structure learning. The following sections elucidate these
aspects of the active inference process theory.
This paper will be largely concerned with deriving and interpreting the inferential dynamics that agents might imple-
ment using the generative model in Figure 2. We leave the discussion of more complex models to Appendix A, since
the derivations are analogous in those cases.
4 Variational Bayesian inference
4.1 Free energy and model evidence
Variational Bayesian inference rests upon minimisation of a quantity called (variational) free energy, which measures
the improbability (i.e., the surprise) of sensory observations, under a generative model. Simultaneously, variational
free energy minimisation is a statistical inference technique that enables the approximation of the posterior distribution
in Bayes rule. In machine learning, this is known as variational Bayes [54, 73–75]. Active inference agents minimise
variational free energy, enabling concomitant maximisation of their model evidence and inference of the latent vari-
ables of their generative model. In the following, we consider a particular time point to be givent∈{1,...,T }, whence
the agent has observed a sequence of outcomes o1:t. The posterior about the latent causes of sensory data is given by
Bayes rule:
7
Figure 1: Markov blankets in active inference.This ﬁgure illustrates the Markov blanket assumption of active
inference. A Markov blanket is a set of variables through which states internal and external to the system interact.
Speciﬁcally, the system must be such that we can partition it into a Bayesian network of internal states µ, external
states η, sensory states oand active states u, (µ, oand uare often referred together as particular states) with prob-
abilistic (causal) links in the directions speciﬁed by the arrows. All interactions between internal and external states
are therefore mediated by the blanket states b. The sensory states represent the sensory information that the body
receives from the environment and the active states express how the body inﬂuences the environment. This blanket
assumption is quite generic, in that it can be reasonably assumed for a brain as well as elementary organisms. For
example, when considering a bacillus, the sensory states become the cell membrane and the active states comprise
the actin ﬁlaments of the cytoskeleton. Under the Markov blanket assumption – together with the assumption that the
system persists over time (i.e., possesses a non-equilibrium steady state) – a generalised synchrony appears, such that
the dynamics of the internal states can be cast as performing inference over the external states (and vice-versa) via
a minimisation of variational free energy [69, 70]. This coincides with existing approaches to inference; i.e., varia-
tional Bayes [54, 73–75]. This can be viewed as the internal states mirroring external states, via sensory states (e.g.,
perception), and external states mirroring internal states via active states (e.g., a generalised form of self-assembly,
autopoiesis or niche construction). Furthermore, under these assumptions the most likely courses of actions can be
shown to minimise expected free energy. Note that external states beyond the system should not be confused with
the hidden states of the agent’s generative model (which model external states). In fact, the internal states are exactly
the parameters (i.e., sufﬁcient statistics) encoding beliefs about hidden states and other latent variables, which model
external states in a process of variational free energy minimisation. Hidden and external states may or may not be
isomorphic. In other words, an agent uses its internal states to represent hidden states that may or may not exist in the
external world.
8
Figure 2: Example of a discrete state-space generative model.Panel 2a, speciﬁes the form of the generative model,
which is how the agent represents the world. The generative model is a joint probability distribution over (hidden)
states, outcomes and other variables that cause outcomes. In this representation, states unfold in time causing an
observation at each time-step. The likelihood matrix Aencodes the probabilities of state-outcome pairs. The policy
π speciﬁes which action to perform at each time-step. Note that the agent’s preferences may be speciﬁed either in
terms of states or outcomes. It is important to distinguish between states (resp. outcomes) that are random variables,
and the possible values that they can take in S (resp. in O), which we refer to as possible states (resp. possible
outcomes). Note that this type of representation comprises a ﬁnite number of timesteps, actions, policies, states,
outcomes, possible states and possible outcomes. In Panel 2b, the generative model is displayed as a probabilistic
graphical model [54, 72, 75, 82] expressed in factor graph form [83]. The variables in circles are random variables,
while squares represent factors, whose speciﬁc form are given in Panel 2a. The arrows represent causal relationships
(i.e., conditional probability distributions). The variables highlighted in grey can be observed by the agent, while
the remaining variables are inferred through approximate Bayesian inference (see Section 4) and called hidden or
latent variables. Active inference agents perform inference by optimising the parameters of an approximate posterior
distribution (see Section 4). Panel 2c speciﬁes how this approximate posterior factorises under a particular mean-
ﬁeld approximation [84], although other factorisations may be used [85, 86]. A glossary of terms used in this ﬁgure
is available in Table 2. The mathematical yoga of generative models is heavily dependent on Markov blankets. The
Markov blanket of a random variable in a probabilistic graphical model are those variables that share a common factor.
Crucially, a variable conditioned upon its Markov blanket is conditionally independent of all other variables. We will
use this property extensively (and implicitly) in the text.
9
P(s1:T,A,π |o1:t) = P(o1:t|s1:T,A,π )P(s1:T,A,π )
P(o1:t) (1)
Computing the posterior distribution requires computing the model evidence P(o1:t) =∑
π∈Π
∑
s1:T∈ST
∫
P(o1:t,s1:T,A,π ) dA, which is intractable for complex generative models embodied by
biological and artiﬁcial systems [93] – a well-known problem in Bayesian statistics. An alternative to computing
the exact posterior distribution is to optimise an approximate posterior distribution over latent causes Q(s1:T,A,π ),
by minimising the Kullback-Leibler (KL) divergence [96] ( DKL) – a non-negative measure of discrepancy between
probability distributions. We can use the deﬁnition of the KL divergence and Bayes rule to arrive at the variational
free energy F, which is a functional of approximate posterior beliefs:
0 ≤DKL[Q(s1:T,A,π )||P(s1:T,A,π |o1:t)]
= EQ(s1:T,A,π)[log Q(s1:T,A,π ) −log P(s1:T,A,π |o1:t)]
= EQ(s1:T,A,π)[log Q(s1:T,A,π ) −log P(o1:t,s1:T,A,π ) + logP(o1:t)]
= EQ(s1:T,A,π)[log Q(s1:T,A,π ) −log P(o1:t,s1:T,A,π )]  
F[Q(s1:T,A,π)]
+ logP(o1:t)
⇒−log P(o1:t) ≤F[Q(s1:T,A,π )]
(2)
From (2), one can see that by varying Qto minimise the variational free energy enables us to approximate the true
posterior, while simultaneously ensuring that surprise remains low. This means that variational free energy minimising
agents, simultaneously, infer the latent causes of their observations and maximise the evidence for their generative
model. To aid intuition, the variational free energy can be rearranged into complexity and accuracy:
F[Q(s1:T,A,π )] = DKL[Q(s1:T,A,π )||P(s1:T,A,π )]  
Complexity
−EQ(s1:T,A,π)[log P(o1:t|s1:T,A,π )]  
Accuracy
(3)
The ﬁrst term of (3) can be regarded as complexity: a simple explanation for observable data Q, which makes few
assumptions over and above the prior (i.e., with KL divergence close to zero), is a good explanation. In other words,
a good explanation is an accurate account of some data that requires minimal movement for updating of prior to
posterior beliefs (c.f., Occam’s principle). The second term is accuracy; namely, the probability of the data given
posterior beliefs about model parameters Q. In other words, how well the generative model ﬁts the observed data.
The idea that neural representations weigh complexity against accuracy underwrites the imperative to ﬁnd the most
accurate explanation for sensory observations that is minimally complex, which has been leveraged by things like
Horace Barlow’s principle of minimum redundancy [97] and subsequently supported empirically [98–101]. Figure 3
illustrates the various implications of minimising free energy.
4.2 On the family of approximate posteriors
The goal is now to minimise variational free energy with respect to Q. To obtain a tractable expression for the
variational free energy, we need to assume a certain simplifying factorisation of the approximate posterior. There
are many possible forms [85, 118, 119] (e.g., mean-ﬁeld, marginal, Bethe), each of which trades off the quality of
the inferences with the complexity of the computations involved. For the purpose of this paper we use a particular
(structured) mean-ﬁeld approximation (c.f., Figure 2):
Q(s1:T,A,π ) = Q(A)Q(π)
T∏
τ=1
Q(sτ|π) (4)
This choice is driven by didactic purposes and since this factorisation has been used extensively in the active in-
ference literature (e.g., [2, 27, 44]). However, the most recent software implementation of active inference (i.e.,
spm_MDP_VB_X.m) employs a marginal approximation [85, 120], which retains the simplicity and biological inter-
pretation of the neuronal dynamics afforded by the mean-ﬁeld approximation, while approximating the more accurate
inferences of the Bethe approximation. For these reasons, the marginal free energy currently stands as the most bio-
logically plausible.
10
Figure 3: Markov blankets and self-evidencing.This schematic illustrates the various interpretations of minimising
variational free energy. Recall that the existence of a Markov blanket implies a certain lack of inﬂuences among
internal, blanket and external states. These independencies have an important consequence; internal and active states
are the only states that are not inﬂuenced by external states, which means their dynamics (i.e., perception and action)
are a function of, and only of, particular states (i.e., internal, sensory and active states); here, the variational (free
energy) bound on surprise. This surprise has a number of interesting interpretations. Given it is the negative log
probability of ﬁnding a particle or creature in a particular state, minimising surprise corresponds to maximising the
value of a particle’s state. This interpretation is licensed by the fact that the states with a high probability are, by
deﬁnition, attracting states. On this view, one can then spin-off an interpretation in terms of reinforcement learning
[77], optimal control theory [102] and, in economics, expected utility theory [103]. Indeed, any scheme predicated on
the optimisation of some objective function can now be cast in terms of minimising surprise – in terms of perception
and action (i.e., the dynamics of internal and active states) – by specifying these optimal values to be the agent’s
preferences. The minimisation of surprise (i.e., self-information) leads to a series of inﬂuential accounts of neuronal
dynamics; including the principle of maximum mutual information [104,105], the principles of minimum redundancy
and maximum efﬁciency [106] and the free energy principle [66]. Crucially, the average or expected surprise (over
time or particular states of being) corresponds to entropy. This means that action and perception look as if they
are minimising entropy. This leads us to theories of self-organisation, such as synergetics in physics [107–109] or
homeostasis in physiology [110–112]. Finally, the probability of any blanket states given a Markov blanket ( m) is,
on a statistical view, model evidence [113, 114]. This means that all the above formulations are internally consistent
with things like the Bayesian brain hypothesis, evidence accumulation and predictive coding; most of which inherit
from Helmholtz’s motion of unconscious inference [115], later unpacked in terms of perception as hypothesis testing
in 20th century psychology [116] and machine learning [117].
11
4.3 Computing the variational free energy
The next sections focus on producing biologically plausible neuronal dynamics that perform perception and learning
based on variational free energy minimisation. To enable this, we ﬁrst compute variational the free energy, using the
factorisations of the generative model and approximate posterior (c.f., Figure 2):
F[Q(s1:T,A,π )] = EQ(s1:T,A,π)[log Q(s1:T,A,π ) −log P(o1:t,s1:T,A,π )]
= EQ(s1:T,A,π)[log Q(A) + logQ(π) +
T∑
τ=1
log Q(sτ|π)
−log P(A) −log P(π) −log P(s1) −
T∑
τ=2
log P(sτ|sτ−1,π) −
t∑
τ=1
log P(oτ|sτ,A)]
= DKL[Q(A)||P(A)] + DKL[Q(π)||P(π)] + EQ(π)[Fπ[Q(s1:T|π)]]
(5)
where
Fπ[Q(s1:T|π)] :=
T∑
τ=1
EQ(sτ|π)[log Q(sτ|π)] −
t∑
τ=1
EQ(sτ|π)Q(A)[log P(oτ|sτ,A)]
−EQ(s1|π)[log P(s1)] −
T∑
τ=2
EQ(sτ|π)Q(sτ−1|π)[log P(sτ|sτ−1,π)]
(6)
is the variational free energy conditioned upon a policy. This is the same quantity that we would have obtained by
omitting Aand conditioning all probability distributions in the numerators of (1) byπ. In the next section, we will see
how perception can be framed in terms of variational free energy minimisation.
5 Perception
In active inference, perception is equated with state estimation [44] (e.g., inferring the temperature from the sensation
of warmth), consistent with the idea that perceptions are hypotheses [116]. To infer the (past, present and future) states
of the environment, an agent must minimise the variational free energy with respect to Q(s1:T|π) for each policy π.
This provides the agent’s inference over hidden states, contingent upon pursuing a given policy. Since the only part of
the free energy that depends on Q(s1:T|π) is Fπ, the agent must simply minimise Fπ. Substituting Q(sτ|π) by their
sufﬁcient statistics (i.e., parameters sπτ), Fπ becomes a function of those parameters. This enables us to rewrite (6),
conveniently in matrix form:
Fπ(sπ1,..., sπT) =
T∑
τ=1
sπτ ·log sπτ −
t∑
τ=1
oτ ·logAsπτ
−sπ1 log D−
T∑
τ=2
sπτ ·log(Bπτ−1 )sπτ−1
(7)
This enables to compute the variational free energy gradients [121]:
∇sπτFπ(sπ1,..., sπT) = ⃗1 + logsπτ −



oτ ·logA + sπτ+1 ·log(Bπτ) + logD if τ = 1
oτ ·logA + sπτ+1 ·log(Bπτ) + log(Bπτ−1 )sπτ−1 if 1 <τ ≤t
sπτ+1 ·log(Bπτ) + log(Bπτ−1 )sπτ−1 if τ >t
(8)
The neuronal dynamics are given by a gradient descent on free energy [44], with state-estimation expressed as a
softmax function of accumulated (negative) free energy gradients. The constant term ⃗1 is generally omitted since the
softmax function removes it anyway. This enables us to equate the gradient with a prediction error.
˙v(sπ1,..., sπT) = −∇sπτFπ(sπ1,..., sπT)
sπτ = σ(v) (9)
12
The softmax function – a generalisation of the sigmoid to vector inputs – is a natural choice as the variational free
energy gradient is a logarithm and the components of sπτ must sum to one.
5.1 Plausibility of neuronal dynamics
The temporal dynamics expressed in (9) unfold at a much faster timescale than the sampling of new observations (i.e.,
within timesteps) and correspond to fast neuronal processing in peristimulus time. This is consistent with behaviour-
relevant computations at frequencies that are higher than the rate of visual sampling (e.g., working memory [122],
visual stimulus perception in humans [91] and macaques [92]).
Furthermore, these dynamics are consistent with predictive processing [123, 124] – since active inference prescribes
dynamics that minimise prediction error – although they generalise it to a wide range of generative models. Note that,
while also a variational free energy gradient, this sort of prediction error is not the same as that given by predictive
coding schemes (which rely upon a different sort of generative model) [17, 18, 125].
Just as neuronal dynamics involve translation from post-synaptic potentials to ﬁring rates, (9) involves translating from
a vector of real numbers ( v), to a vector whose elements are bounded between zero and one ( sπτ); via the softmax
function. As a result, it is natural to interpret the components ofvas the average membrane potential of distinct neural
populations, and sπτ as the average ﬁring rate of those populations, which is bounded thanks to neuronal refractory
periods. This is consistent with mean-ﬁeld formulations of neural population dynamics, in that the average ﬁring rate
of a neuronal population follows a sigmoid function of the average membrane potential [126–128]. Using the fact
that a softmax function is a generalisation of the sigmoid to vector inputs – here the average membrane potentials of
coupled neuronal populations – it follows that their average ﬁring follows a softmax function of their average potential.
In this context, the softmax function may be interpreted as performing lateral inhibition, which can be thought of as
leading to narrower tuning curves of individual neurons and thereby sharper inferences [129]. Importantly, this tells us
that state-estimation can be performed in parallel by different neuronal populations, and a simple neuronal architecture
is sufﬁcient to implement these dynamics (see Figure 6 in [85]).
Lastly, interpreting the dynamics in this way has a degree of face validity, as it enables us to synthesise a wide-range of
biologically plausible electrophysiological responses; including repetition suppression, mismatch negativity, violation
responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-
to-bound dynamics and transfer of dopamine responses [37, 44].
The neuronal dynamics for state estimation coincide with variational message passing [130, 131]: a widely used
algorithm for approximate Bayesian inference. This is an important result, since it shows that variational message
passing emerges under active inference using a particular mean-ﬁeld approximation. If one were to use the Bethe
approximation, the corresponding dynamics coincide with belief propagation [54,83,85,86,118], another widely used
algorithm for approximate inference. This offers a formal connection between active inference and message passing
interpretations of neuronal dynamics [27, 132, 133]. In the next section, we examine planning, decision-making and
action selection.
6 Planning, decision-making and action selection
So far, we have focused on optimising beliefs about hidden states under a particular policy by minimising a variational
free energy functional of an approximate posterior over hidden states, under each policy.
In this section, we explain how planning and decision-making arise as a minimisation of expected free energy – a
function scoring the goodness of each possible future course of action. We brieﬂy motivate how the expected free
energy arises from ﬁrst-principles. This allows us to frame decision-making and action-selection in terms of expected
free energy minimisation. Finally, we conclude by discussing the computational cost of planning into the future.
6.1 Planning and decision-making
At the heart of active inference, is a description of agents that strive to attain a target distribution specifying the range of
preferred states of being, given a sufﬁcient amount of time. To work towards reaching these preferences, agents select
policies Q(π), such that their predicted states Q(sτ,A) at some future time point τ > t(usually, the time horizon of
a policy T) reach the preferred states P(sτ,A), which are speciﬁed by the generative model. These considerations
13
allow us to show in Appendix B that the requisite approximate posterior over policies Q(π) is a softmax function of
the negative expected free energy G5:
Q(π) = σ(−G(π))
G(π) = DKL[Q(sτ,A|π)||P(sτ,A)]  
Risk
−EQ(sτ,A|π)P(oτ|sτ,A)[log P(oτ|sτ,A)]  
Ambiguity
(10)
This means that the most likely (i.e., best) policies minimise expected free energy. This ensures that future courses of
action are exploitative (i.e., risk minimising) and explorative (i.e., ambiguity minimising). In particular, the expected
free energy speciﬁes the optimal balance between goal-seeking and itinerant novelty-seeking behaviour, given some
prior preferences or goals. Note that the ambiguity term rests on an expectation over ﬁctive (i.e., predicted) outcomes
under beliefs about future states. This means that optimising beliefs about future states during perception is crucial
to accurately predict future outcomes during planning. In summary, planning and decision-making respectively cor-
respond to evaluating the expected free energy of different policies, which scores their goodness in relation to prior
preferences and forming approximate posterior beliefs about policies.
6.2 Action selection, policy-independent state-estimation
Approximate posterior beliefs about policies allows to obtain the most plausible action as the most likely under all
policies – this can be expressed as a Bayesian model average
ut = arg max
u∈U
(∑
π∈Π
δu,πtQ(π)
)
(11)
where δ is the Kronecker delta. In addition, we obtain a policy independent state-estimation at any time point τ ∈
{1,...,T }, as a Bayesian model average of approximate posterior beliefs about hidden states under policies:
Q(sτ) =
∑
π∈Π
Q(sτ|π)Q(π)
⇐⇒sτ =
∑
π∈Π
sπτQ(π)
(12)
Note that these Bayesian model averages may be implemented by neuromodulatory mechanisms [42].
6.3 Biological plausibility
Winner take-all architectures of decision-making are already commonplace in computational neuroscience (e.g., mod-
els of selective attention and recognition [134, 135], hierarchical models of vision [136]). This is nice, since the
softmax function in (10) can be seen as providing a biologically plausible [126–128], smooth approximation to the
maximum operation, which is known as soft winner take-all [137]. In fact, the generative model, presented in Figure
2, can be naturally extended such that the approximate posterior contains an (inverse) temperature parameter γmulti-
plying the expected free energy inside the softmax function (see Appendix A.2). This temperature parameter regulates
how precisely the softmax approximates the maximum function, thus recovering winner take-all architectures for high
parameter values (technically, this converts Bayesian model averaging into Bayesian model selection, where the pol-
icy corresponds to a model of what the agent is doing). This parameter, regulating precision of policy selection, has a
clear biological interpretation in terms of conﬁdence encoded in dopaminergic ﬁring [35–37, 44]. Interestingly, Daw
and colleagues [23] uncovered evidence in favour of a similar model employing a softmax function and temperature
parameter in human decision-making.
5A more complete treatment may include priors over policies – usually denoted byE– and the evidence for a policy afforded by
observed outcomes (usually denoted by F). These additional terms supplement the expected free energy, leading to an approximate
posterior of the form σ(−log E−F −G) [2].
14
6.4 Pruning of policy trees
From a computational perspective, planning (i.e., computing the expected free energy) for each possible policy can be
cost-prohibitive, due do the combinatorial explosion in the number of sequences of actions when looking deep into
the future. There has been work in understanding how the brain ﬁnesses this problem [138], which suggests a simple
answer: during mental planning, humans stop evaluating a policy as soon as they encounter a large loss (i.e., a high
value of the expected free energy that renders the policy highly implausible). In active inference this corresponds to
using an Occam window; that is, we stop evaluating the expected free energy of a policy if it becomes much higher
than the best (smallest expected free energy) policy – and set its approximate posterior probability to an arbitrarily low
value accordingly. This biologically plausible pruning strategy drastically reduces the number of policies one has to
evaluate exhaustively.
Although effective and biologically plausible, the Occam window for pruning policy trees cannot deal with large policy
spaces that ensue with deep policy trees and long temporal horizons. This means that pruning can only partially explain
how biological organisms perform deep policy searches. Further research is needed to characterise the processes in
which biological agents reduce large policy spaces to tractable subspaces. One explanation – for the remarkable
capacity of biological agents to evaluate deep policy trees – rests on deep (hierarchical) generative models, in which
policies operate at each level. These deep models enable long-term policies, modelling slow transitions among hidden
states at higher levels in the hierarchy, to contextualise faster state transitions at subordinate levels (see Appendix A).
The resulting (semi Markovian) process can then be speciﬁed in terms of a hierarchy of limited horizon policies that
are nested over temporal scales; c.f., motor chunking [139–141].
6.5 Discussion of the action-perception cycle
Minimising variational and expected free energy are complementary and mutually beneﬁcial processes. Minimisation
of variational free energy ensures that the generative model is a good predictor of its environment; this allows the agent
to accurately plan into the future by evaluating expected free energy, which in turn enables it to realise its preferences.
In other words, minimisation of variational free energy is a vehicle for effective planning and reaching preferences via
the expected free energy; in turn, reaching preferences minimises the expected surprise of future states of being.
In conclusion, we have seen how agents plan into the future and make decisions about the best possible course of
action. This concludes our discussion of the action-perception cycle. In the next section, we examine expected free
energy in greater detail. Then, we will see how active agents can learn the contingencies of the environment and the
structure of their generative model at slower timescales.
7 Properties of the expected free energy
The expected free energy is a fundamental construct of interest. In this section, we unpack its main features and
highlight its importance in relation to many existing theories in neurosciences and engineering.
The expected free energy of a policy can be unpacked in a number of ways. Perhaps the most intuitive is in terms of
risk and ambiguity:
G(π) = DKL[Q(sτ,A|π)||P(sτ,A)]  
Risk
+ EQ(sτ,A|π)[H[P(oτ|sτ,A)]]  
Ambiguity
(13)
This means that policy selection minimises risk and ambiguity. Risk, in this setting, is simply the difference between
predicted and prior beliefs about ﬁnal states. In other words, policies will be deemed more likely if they bring about
states that conform to prior preferences. In the optimal control literature, this part of expected free energy underwrites
KL control [142, 143]. In economics, it leads to risk sensitive policies [144]. Ambiguity reﬂects the uncertainty
about future outcomes, given hidden states. Minimising ambiguity therefore corresponds to choosing future states that
generate unambiguous and informative outcomes (e.g., switching on a light in the dark).
We can express the expected free energy of a policy as a bound on information gain and expected log (model) evidence
(a.k.a., Bayesian risk):
15
G(π) = EQ[DKL[Q(sτ,A|oτ,π)||P(sτ,A|oτ)]]  
Expected evidence bound
−EQ[log P(oτ)]  
Expected log evidence
−EQ[DKL[Q(sτ,A|oτ,π)||Q(sτ,A|π)]]  
Expected information gain
≥− EQ[log P(oτ)]  
Expected log evidence
−EQ[DKL[Q(sτ,A|oτ,π)||Q(sτ,A|π)]]  
Expected information gain
(14)
The ﬁrst term in (14) is the expectation of log evidence under beliefs about future outcomes, while the second ensures
that this expectation is maximally informed, when outcomes are encountered. Collectively, these two terms underwrite
the resolution of uncertainty about hidden states (i.e., information gain) and outcomes (i.e., expected surprise) in
relation to prior beliefs.
When the agent’s preferences are expressed in terms of outcomes (c.f., Figure 2), it is useful to express risk in terms of
outcomes, as opposed to hidden states. This is most useful when the generative model is not known or during structure
learning, when the state-space evolves over time. In these cases, the risk over hidden states can be replaced risk
over outcomes by assuming the KL divergence between the predicted and true posterior (under expected outcomes) is
small:
DKL[Q(sτ,A|π)||P(sτ,A)]  
Risk (states)
= DKL[Q(oτ|π)||P(oτ)]  
Risk (outcomes)
+ EQ(oτ|π)[DKL[Q(sτ,A|oτ,π)||P(sτ,A|oτ)]]  
≈0
≈DKL[Q(oτ|π)||P(oτ)]  
Risk (outcomes)
(15)
This divergence constitutes an expected evidence bound that also appears if we express expected free energy in terms
of intrinsic and extrinsic value:
G(π) = −EQ(oτ|π)[log P(oτ)]  
Extrinsic value
+ EQ(oτ|π)[DKL[Q(sτ,A|oτ,π)||P(sτ,A|oτ)]]  
Expected evidence bound
−EQ(oτ|π)[DKL[Q(sτ|oτ,π)||Q(sτ|π)]]  
Intrinsic value (states) or salience
−EQ(oτ,sτ|π)[DKL[Q(A|oτ,sτ,π)||Q(A)]]  
Intrinsic value (parameters) or novelty
(16)
Extrinsic value is just the expected value of log evidence, which can be associated with reward and utility in be-
havioural psychology and economics, respectively [145–147]. In this setting, extrinsic value is the negative of
Bayesian risk [148], when reward is log evidence. The intrinsic value of a policy is its epistemic value or affor-
dance [40]. This is just the expected information gain afforded by a particular policy, which can be about hidden states
(i.e., salience) or model parameters (i.e., novelty). It is this term that underwrites artiﬁcial curiosity [149].
Intrinsic value is also known as intrinsic motivation in neurorobotics [145, 150, 151], the value of information in eco-
nomics [152], salience in the visual neurosciences and (rather confusingly) Bayesian surprise in the visual search
literature [153–155]. In terms of information theory, intrinsic value is mathematically equivalent to the expected
mutual information between hidden states in the future and their consequences – consistent with the principles of min-
imum redundancy or maximum efﬁciency [105, 106, 156]. Finally, from a statistical perspective, maximising intrinsic
value (i.e., salience and novelty) corresponds to optimal Bayesian design [157] and machine learning derivatives,
such as active learning [158]. On this view, active learning is driven by novelty; namely, the information gain afforded
model parameters, given future states and their outcomes. Heuristically, this curiosity resolves uncertainty about “what
would happen if I did that” [147]. Figure 4 illustrates the compass of expected free energy, in terms of its special cases;
ranging from optimal Bayesian design through to Bayesian decision theory.
8 Learning
In active inference, learning concerns the dynamics of synaptic plasticity, which are thought to encode beliefs about the
contingencies of the environment [44] (e.g., beliefs about B, in some settings, are thought to be encoded in recurrent
16
Figure 4: Expected free energy.This ﬁgure illustrates the various ways in which minimising expected free energy can
be unpacked (omitting model parameters for clarity). The upper panel casts action and perception as the minimisation
of variational and expected free energy, respectively. Crucially, active inference introduces beliefs over policies that
enable a formal description of planning as inference [1, 159, 160]. In brief, posterior beliefs about hidden states of the
world, under plausible policies, are optimised by minimising a variational (free energy) bound on log evidence. These
beliefs are then used to evaluate the expected free energy of allowable policies, from which actions can be selected [44].
Crucially, expected free energy subsumes several special cases that predominate in the psychological, machine learning
and economics literature. These special cases are disclosed when one removes particular sources of uncertainty from
the implicit optimisation problem. For example, if we ignore prior preferences, then the expected free energy reduces
to information gain [113, 157] or intrinsic motivation [145, 150, 151]. This is mathematically the same as expected
Bayesian surprise and mutual information that underwrite salience in visual search [153, 155] and the organisation
of our visual apparatus [104–106, 156]. If we now remove risk but reinstate prior preferences, one can effectively
treat hidden and observed (sensory) states as isomorphic. This leads to risk sensitive policies in economics [144, 161]
or KL control in engineering [143]. Here, minimising risk corresponds to aligning predicted outcomes to preferred
outcomes. If we then remove ambiguity and relative risk of action (i.e., intrinsic value), we are left with extrinsic
value or expected utility in economics [162] that underwrites reinforcement learning and behavioural psychology
[77]. Bayesian formulations of maximising expected utility under uncertainty is also known as Bayesian decision
theory [148]. Finally, if we just consider a completely unambiguous world with uninformative priors, expected free
energy reduces to the negative entropy of posterior beliefs about the causes of data; in accord with the maximum
entropy principle [163]. The expressions for variational and expected free energy correspond to those described in the
main text (omitting model parameters for clarity). They are arranged to illustrate the relationship between complexity
and accuracy, which become risk and ambiguity, when considering the consequences of action. This means that risk-
sensitive policy selection minimises expected complexity or computational cost. The coloured dots above the terms in
the equations correspond to the terms that constitute the special cases in the lower panels.
17
excitatory connections in the prefrontal cortex [47]). The fact that beliefs about matrices (e.g., A, B) may be encoded
in synaptic weights conforms to connectionist models of brain function, as it offers a convenient way to compute
probabilities, in the sense that the synaptic weights could be interpreted as performing matrix multiplication as in
artiﬁcial neural networks, to predict; e.g., outcomes from beliefs about states, using the likelihood matrix A.
These synaptic dynamics (e.g., long-term potentiation and depression) evolve at a slower timescale than action and
perception, which is consistent with the fact that such inferences need evidence accumulation over multiple state-
outcome pairs. For simplicity, we will assume the only variable that is learned is A, but what follows generalises to
more complex generative models (c.f., Appendix A.1. Learning Ameans that approximate posterior beliefs about A
follow a gradient descent on variational free energy. Seeing the variational free energy (5) as a function of a (the
sufﬁcient statistic of Q(A)) we can write:
F(a) = DKL[Q(A)||P(A)] −
t∑
τ=1
EQ(π)Q(sτ|π)Q(A)[oτ ·log(A)sτ] + ···
= DKL[Q(A)||P(A)] −
t∑
τ=1
oτ ·logAsτ + ···
(17)
Here, we ignore the terms in (5) that do not depend on Q(A), as these will vanish when we take the gradient. The
KL-divergence between Dirichlet distributions is [164, 165]:
DKL[Q(A)||P(A)] =
m∑
i=1
DKL[Q(A·i)||P(A·i)]
=
m∑
i=1
(
log Γ(a0i) −
n∑
k=1
log Γ(aki) −log Γ(a0i) +
n∑
k=1
log Γ(aki) + (a•i −a•i) ·(logA)•i
)
=
m∑
i=1
(
log Γ(a0i) −
n∑
k=1
log Γ(aki) −log Γ(a0i) +
n∑
k=1
log Γ(aki)
)
+ (a −a) ·logA
(18)
Incorporating (18) in (17), we can take the gradient of the variational free energy with respect to logA:
∇logAF(a) = a −a−
t∑
τ=1
oτ ⊗sτ (19)
where ⊗is the Kronecker (i.e., outer) product. This means that the dynamics of synaptic plasticity follow a descent
on (19):
˙ρ(a) = −∇logAF(a)
= −a + a+
t∑
τ=1
oτ ⊗sτ
(20)
In computational terms, these are the dynamics for evidence accumulation of Dirichlet parameters at time t. Since
synaptic plasticity dynamics occur at a much slower pace than perceptual inference, it is computationally much cheaper
– in numerical simulations – to do a one-step belief update at the end of each trial of observation epochs. Explicitly,
setting the free energy gradient to zero at the end of the trial gives the following update for Dirichlet parameters:
a = a+
T∑
τ=1
oτ ⊗sτ (21)
After which, the prior beliefs P(A) are updated to the approximate posterior beliefs Q(A) for the subsequent trial.
Note that in particular, the update counts the number of times a speciﬁc mapping between states and observations has
been observed. Interestingly, this is formally identical to associative or Hebbian plasticity.
18
As one can see, the learning rule concerning accumulation of Dirichlet parameters (c.f., (21)) means that the agent
becomes increasingly conﬁdent about its likelihood matrix by receiving new observations (since the matrix which
is added onto aat each timestep is always positive). This is ﬁne as long as the structure of the environment remains
relatively constant. In the next section, we will see how Bayesian model reduction can revert this process, to enable the
agent to adapt quickly to a changing environment. Table 3 summarises the belief updating entailed by active inference,
and Figure 5 indicates where particular computations might be implemented in the brain.
Table 3: Summary of belief updating.
Process Computation Equations
Perception sπτ = σ(v),˙v= −∇sπτFπ (8)
Planning G(π) (43), (44)
Decision-making Q(π) = σ(−G(π)) (10)
Action selection ut = arg maxu∈U
(∑
π∈Π δu,πtQ(π)
)
(11)
Policy-independent
state-estimation
sτ = ∑
π∈Π sπτQ(π) (12)
Learning (end of trial) a = a+ ∑T
τ=1 oτ ⊗sτ (21)
9 Structure learning
In the previous sections, we have addressed how an agent performs inference over different variables at different
timescales in a biologically plausible fashion, which we equated to perception, planning and decision-making. In this
section, we consider the problem of learning the form or structure of the generative model.
The idea here is that agents are equipped (e.g., born) with an innate generative model that entails fundamental prefer-
ences (e.g., essential to survival), which are not updated. For instance, humans are born with prior preferences about
their body temperature around 37◦C and O2, CO2, glucose etc concentrations within a certain range. Mathemati-
cally, this means that the parameters of these innate prior distributions – encoding the agent’s expectations as part
of its generative model – have hyperpriors that are inﬁnitely precise (e.g., a Dirac delta distribution) and thus cannot
be updated in an experience dependent fashion. The agent’s generative model then naturally evolves by minimising
variational free energy to become a good model of the agent’s environment but is still constrained by the survival
preferences hardcoded within it. This process of learning the generative model (i.e., the variables and their functional
dependencies) is called structure learning.
Structure learning in active inference is an active area of research. Active inference proposes that the agent’s generative
model evolves over time to maximise the evidence for its observations. However, a complete set of mechanisms
that biological agents use to do so has not yet been laid out. Nevertheless, we use this section to summarise two
complementary approaches; namely, Bayesian model reduction and Bayesian model expansion [3, 172–174] – that
enable to simplify and complexify the model, respectively.
9.1 Bayesian model reduction
To explain the causes of their sensations, agents must compare different hypotheses about how their sensory data are
generated – and retain the hypothesis or model that is the most valid in relation to their observations (i.e., has the
greatest model evidence). In Bayesian statistics, these processes are called Bayesian model comparison and Bayesian
model selection – these correspond to scoring the evidence for various generative models in relation to available
data and selecting the one with the highest evidence [175, 176]. Bayesian model reduction (BMR) is a particular
instance of structure learning, which formalises post-hoc hypothesis testing to simplify the generative model. This
precludes redundant explanations of sensory data – and ensures the model generalises to new data. Technically, it
involves estimating the evidence for simpler (reduced) priors over the latent causes and selecting the model with the
highest evidence. This process of simplifying the generative model – by removing certain states or parameters –
has a clear biological interpretation in terms of synaptic decay and switching off certain synaptic connections, which
19
Figure 5: Possible functional anatomy.This ﬁgure summarises a possible (coarse-grained) functional anatomy that
could implement belief updating in active inference. The arrows correspond to message passing between different
neuronal populations. Here, a visual observation is sampled by the retina, aggregated in ﬁrst-order sensory thalamic
nuclei and processed in the occipital (visual) cortex. The green arrows correspond to message passing of sensory
information. This signal is then propagated (via the ventral visual pathway) to inferior and medial temporal lobe
structures such as the hippocampus; this allows the agent to go from observed outcomes to beliefs about their most
likely causes in state-estimation (perception), which is performed locally. The variational free energy is computed in
the striatum. The orange arrows encode message passing of beliefs. Preferences C are attributed to the dorsolateral
prefrontal cortex – which is thought to encode representations over prolonged temporal scales [45] – consistent with
the fact that these are likely to be encoded within higher cortical areas [3]. The expected free energy is computed in
the medial prefrontal cortex [44] during planning, which leads to inferences about most plausible policies (decision-
making) in the basal ganglia, consistent with the fact that the basal ganglia is thought to underwrite planning and
decision-making [166–171]. The message concerning policy selection is sent to the motor cortex via thalamocortical
loops. The most plausible action, which is selected in the motor cortex is passed on through the spinal cord to trigger
a limb movement. Simultaneously, policy independent state-estimation is performed in the ventrolateral prefrontal
cortex, which leads to synaptic plasticity dynamics in the prefrontal cortex, where the synaptic weights encode beliefs
about A.
20
is reminiscent of the synaptic mechanisms of sleep (e.g., REM sleep [177, 178]), reﬂection and associated machine
learning algorithms (e.g., wake-sleep [179]).
To keep things concise, letνrepresent a hidden variable in the generative model that is optimised during learning (e.g.
A), and o = o1:t a sequence of observations. The current model has a prior P(ν) and we would like to test whether
a reduced prior (i.e., less complex) ˜P(ν) can provide a more parsimonious explanation for the observed outcomes.
Using Bayes rule, we have the following identities:
P(ν)P(o|ν) = P(ν|o)P(o) (22)
˜P(ν)P(o|ν) = ˜P(ν|o) ˜P(o) (23)
Where P(o) =
∫
P(o|ν)P(ν) dνand ˜P(o) =
∫
P(o|ν) ˜P(ν). Dividing (22) by (23) yields
P(ν)
˜P(ν)
= P(ν|o)P(o)
˜P(ν|o) ˜P(o)
(24)
We can then use (24) in order to obtain the following relations:
1 =
∫
˜P(ν|o) dν = P(o)
˜P(o)
∫ ˜P(ν)P(ν|o)
P(ν) dν = P(o)
˜P(o)
EP(ν|o)
[˜P(ν)
P(ν)
]
(25)
⇒log ˜P(o) −log P(o) = log EP(ν|o)
[˜P(ν)
P(ν)
]
(26)
We can approximate the posterior term in the expectation of (26) with the corresponding approximate posteriorQ(ν),
which simpliﬁes the computation. This allows us to compare the evidence of the two models (reduced and full) and
select the best. If the reduced model has more evidence, it implies the current model is too complex – and redundant
parameters can be removed by adopting the new priors.
In conclusion, BMR allows for computationally efﬁcient and biologically plausible hypothesis testing, to ﬁnd simpler
explanations for the data at hand. It has been used to emulate sleep and reﬂection in abstract rule learning [3], by
simplifying the prior over A at the end of each trial – this has the additional beneﬁt of preventing the agent from
becoming overconﬁdent.
9.2 Bayesian model expansion
Bayesian model expansion is complementary to Bayesian model reduction. It entails adopting a more complex gener-
ative model (by adding, e.g., more states); if, and only if the gain in accuracy in (3) is sufﬁcient enough to outweigh the
increase in complexity. This model expansion allows for generalisation and concept learning in active inference [173].
Note that additional states need not always lead to a more complex model. It is in principle possible to expand a model
in such a way that complexity decreases, as many state estimates might be able to remain close to their priors in place
of a small number of estimates moving a lot. This ‘shared work’ by many parameters could lead to a simpler model.
From a computational perspective, concept acquisition can be seen as a type of structure learning [180,181] – that can
be emulated through Bayesian model comparison. Recent work on concept learning in active inference [173], shows
that a generative model equipped with extra (latent) hidden states can engage these ‘unused’ hidden states, when an
agent is presented with novel stimuli during the learning process. Initially the corresponding likelihood mappings (i.e.,
the corresponding columns ofA) are uninformative, but these are updated when the agent encounters new observations
that cannot be accounted by its current knowledge (e.g., observing a cat when it has only been exposed to birds). This
happens naturally, during the learning process, in an unsupervised way through free energy minimization. To allow
for effective generalization, this approach can be combined with BMR; in which any new concept can be aggregated
with similar concepts, and the associated likelihood mappings can be reset for further concept acquisition, in favour of
a simpler model with higher model evidence. This approach can be further extended by updating the number of extra
hidden states through a process of Bayesian model comparison.
21
10 Discussion
Due to the various recent theoretical advances in active inference, it is easy to lose sight of its underlying principle,
process theory and practical implementation. We have tried to address this by rehearsing – in a clear and concise way
– the assumptions underlying active inference as a principle, the technical details of the process theory for discrete
state-space generative models and the biological interpretation of the accompanying neuronal dynamics. It is useful
to clarify these results; as a ﬁrst step to guide towards outstanding theoretical research challenges, a practical guide to
implement active inference to simulate experimental behaviour and a pointer towards various predictions that may be
tested empirically.
Active inference offers a degree of plausibility as a process theory of brain function. From a theoretical perspective
its requisite neuronal dynamics correspond to known empirical phenomena and extend earlier theories like predictive
coding [64, 123, 124]. Furthermore, the process theory is consistent with the underlying free energy principle, which
biological systems are thought to abide by – namely, the avoidance of surprising states: this can be articulated formally
based on fundamental assumptions about biological systems [69, 70]. Lastly, the process theory has a degree of face
validity as its predicted electrophysiological responses closely resemble empirical measurements.
However, for a full endorsement of the process theory presented in this paper, rigorous empirical validation of the
synthetic electrophysiological responses is needed. To pursue this, one would have to specify the generative model that
a biological agent employs for a particular task. This can be done through Bayesian model comparison of alternative
generative models with respect to empirical (choice) behaviour being measured (e.g., [182]). Once the appropriate
generative model is formulated, evidence for a plausible but distinct implementations of active inference would need
to be compared, which come from various possible approximations to the free energy [85, 86, 118], each of which
yields different belief updates and simulated electrophysiological responses. Note that the marginal approximation to
the free energy currently stands as the most biologically plausible [85]. From this, the explanatory power of active
inference can be assessed in relation to empirical measurements and contrasted with other existing theories.
This means that the key challenge for active inference – and arguably data analysis in general – is ﬁnding the generative
model that best explains observable data (i.e., evidence maximising). A solution to this problem would enable to
ﬁnd the generative model – entailed by an agent – by observing its behaviour. In turn, this would enable one to
simulate its belief updating and behaviour accurately in-silico. It should be noted that these generative models can
be speciﬁed manually for the purposes of reproducing simple behaviour (e.g., agents performing simple tasks needed
for empirical validation discussed above). However, a generic solution to this problem is necessary to account for
complex datasets; in particular, complex behavioural data from agents in a real environment. Moreover, a biologically
plausible solution to this problem could correspond to a complete structure learning roadmap; accounting for how
biological agents evolve their generative model to account for new observations. Evolution has solved this problem
by selecting phenotypes with a good model of their sensory data, therefore, understanding the processes that have
selected generative models that are ﬁt for purpose for our environment might lead to important advances in structure
learning and data analysis.
Discovering new generative models corresponding to complex behavioural data, will demand to extend the current
process theory to these models, in order to provide testable predictions and reproduce the observed behaviour in-
silico. Examples of generative models that do not fall within the current discrete state-space, continuous state-space
[8,18,31,183–186] or mixed [27,28,30] models – currently implemented in active inference – include Markov decision
trees [75, 187] and Boltzmann machines [78, 188, 189].
One challenge that may arise, when scaling active inference to complex models with many degrees of freedom, will be
the size of the policy trees in consideration. Although effective and biologically plausible, the current pruning strategy
is unlikely to reduce the search space sufﬁciently to enable tractable inference in such cases. As noted above, the issue
of scaling active inference may yield to the ﬁrst principles of the variational free energy formulation. Speciﬁcally,
generative models with a high evidence are minimally complex. This suggests that ‘scaling up’, in and of itself, is
not the right strategy for reproducing more sophisticated or deep behaviour. A more principled approach would be
to explore the right kind of factorisations necessary to explain structured behaviour. A key candidate here are deep
temporal or diachronic generative models that have a separation of timescales. This form of factorisation (c.f., mean
ﬁeld approximation) replaces deep decision trees with shallow decision trees that are hierarchically composed.
To summarise, we argue that some important challenges for theoretical neuroscience include ﬁnding process theories
of brain function that comply with active inference as a principle [69, 70]; namely, the avoidance of surprising events.
The outstanding challenge is then to explore and ﬁne grain such process theories, via Bayesian model comparison (e.g.,
using dynamic causal modelling [59,190]) in relation to experimental data. From a structure learning and data analysis
perspective, the main challenge is ﬁnding the generative model with the greatest evidence in relation to available data.
This may be achieved by understanding the processes evolution has selected for creatures with a good model of their
22
environment. Finally, to scale active inference to behaviour with many degrees of freedom, one needs to understand
how biological agents effectively search deep policy trees when planning into the future, when many possible policies
may be entertained at separable timescales.
11 Conclusion
In conclusion, this paper aimed to summarise: the assumptions underlying active inference, the technical details
underwriting its process theory, and how the associated neuronal dynamics relate to known biological processes.
These processes underwrite action, perception, planning, decision-making, learning and structure learning; which
we have illustrated under discrete state-space generative models. We have discussed some important outstanding
challenges: from a broad perspective, the challenge for theoretical neuroscience is to develop increasingly ﬁne-grained
mechanistic models of brain function that comply with the core tenets of active inference [69, 70]. In regards to the
process theory, key challenges relate to experimental validation, understanding how biological organisms evolve their
generative model to account for new sensory observations and how they effectively search large policy spaces when
planning into the future.
Software availability
The belief updating scheme described in this article is generic and can be implemented using standard rou-
tines (e.g., spm_MDP_VB_X.m). These routines are available as Matlab code in the SPM academic software:
http://www.fil.ion.ucl.ac.uk/spm/. Examples of simulations using discrete state-space generative models
can be found via a graphical user interface by typing DEM.
Acknowledgements
LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875). TP is supported by
the Rosetrees Trust (Award number: 173346). NS is funded by the Medical Research Council (Ref: 2088828). SV
was funded by the Leverhulme Doctoral Training Programme for the Ecological Study of the Brain (DS-2017-026).
KF is funded by a Wellcome Trust Principal Research Fellowship (Ref: 088130/Z/09/Z).
A More complex generative models
In this Appendix, we brieﬂy present cases of more complex discrete state-space generative models and explain how
the belief updating can be extended to those cases.
A.1 Learning B and D
In this paper, we have only considered the case whereAis learned, while beliefs about B(i.e., transition probabilities
from one state to the next) and D(i.e., beliefs about the initial state) remained ﬁxed. In general, Band Dcan also be
learnt over time. This calls upon a new (extended) expression for the generative model with priors over Band D:
P(o1:T,s1:T,A,B,D,π ) = P(π)P(A)P(B)P(D)P(s1|D)
T∏
τ=2
P(sτ|sτ−1,B,π )
T∏
τ=1
P(oτ|sτ,A)
P(B) =
∏
u∈U
m∏
i=1
P((Bu)•i) P((Bu)•i) = Dir((bu)•i)
P(D) = Dir(d)
(27)
Here, (Bu)•i and (bu)•i denote the ith columns of the matrix Bu encoding the transition probabilities from one state
to the next state and its corresponding Dirichlet parameter bu. Furthermore, one needs to deﬁne the corresponding
approximate posteriors that will be used for learning:
23
Q(B) =
∏
u∈U
m∏
i=1
Q((Bu)•i) Q((Bu)•i) = Dir((bu)•i)
Q(D) = Dir(d)
(28)
The variational free energy, after having observed o1:t, is computed analogously as in equation (5). The process of
ﬁnding the belief dynamics is then akin to section 8 – we rehearse it in the following: selecting only those terms in the
variational free energy, which depend on Band Dyields:
F[Q(B,D)] = DKL[Q(B)||P(B)] + DKL[Q(D)||P(D)] −EQ(π)Q(s1|π)Q(D)[s1 ·log D]
−
t∑
τ=2
EQ(π)Q(sτ,sτ−1|π)Q(B)[sτ ·log Bπτsτ−1] + ···
= DKL[Q(B)||P(B)] + DKL[Q(D)||P(D)] −s1 ·logD −
t∑
τ=2
EQ(π)[sπτ ·logBπτsπτ−1 ] + ···
(29)
Using the form of the KL divergence for Dirichlet distributions (18) and taking the gradients yields
∇logBuF(bu) = bu −bu −
t∑
τ=2
∑
π∈Π
δu,πtQ(π)(sπτ ⊗sπτ−1) (30)
∇logDF(d) = d −d−s1 (31)
where ⊗denotes the Kronecker product. Finally, it is possible to specify neuronal plasticity dynamics following a de-
scent on (30), (31), which correspond to biological dynamics. Alternatively, we have belief update rules implemented
once after each trial of observation epochs in in-silico agents:
bu = bu +
t∑
τ=2
∑
π∈Π
δu,πτQ(π)(sπτ ⊗sπτ−1) (32)
d = d+ s1 (33)
A.2 Complexifying the prior over policies
In this paper, we have considered a simple prior approximate posterior over policies; namely, σ(−G(π)). This can
be extended to σ(−γG(π)), where γ is an (inverse) temperature parameter that denotes the conﬁdence in selecting a
particular policy. This extension is quite natural in the sense that γcan be interpreted as the postsynaptic response to
dopaminergic input [35,36]. This correspondence is supported by empirical evidence [37] and enables one to simulate
biologically plausible dopaminergic discharges (c.f., Appendix E [44]). Anatomically, this parameter may be encoded
within the substantia nigra, in nigrostriatal dopamine projection neurons [37], which maps well with our proposed
functional anatomy (c.f., Figure 5), since the substantia nigra is connected with the striatum. We refer the reader
to [44] for a discussion of the associated belief updating scheme.
A.3 Multiple state and outcome modalities
In general, one does not only need one hidden state and outcome factor to represent the environment, but many. In-
tuitively, this happens in the human brain as we integrate sensory stimuli from our ﬁve (or more) distinct senses.
Mathematically, we can express this via different streams of hidden states (usually referred to as hidden factors) that
evolve independently of one another that interact to generate outcomes at each time step; e.g., see Figure 9 [75] for a
graphical representation of a multi-factorial hidden Markov model. This means that Abecomes a multi-dimensional
tensor that integrates information about the different hidden factors to cause outcomes. The belief updating is anal-
ogous in this case, contingent upon the fact that one assumes a mean-ﬁeld factorisation of the approximate posterior
on the different hidden state factors (see, e.g., [5, 43]). This means that the beliefs about states may be processed in a
manner analogous to Figure 5, invoking a greater number of neural populations.
24
A.4 Deep temporal models
A deep temporal model is a generative model with many layers that are nested hierarchically and act at different
timescales. These were ﬁrst introduced within active inference in [3]. One can picture them graphically as a POMDP
(c.f., Figure 2) at the higher level where each outcome is replaced by a POMDP at the lower level, and so forth.
There is a useful metaphor for understanding the concept underlying deep temporal models: each layer of the model
corresponds to the hand of a clock. In a two-layer hierarchical model, a ticking (resp. rotation) of the faster hand
corresponds to a time step (resp. trial of observation epochs) at the lower level. At the end of each trial at the lower
level, the slower hand ticks once, which corresponds to a time-step at the higher level, and the process unfolds again.
One can concisely summarise this by saying that a state at the higher level corresponds to a trial of observation epochs
at the lower level. Of course, there is no limit to the number of layers one can stack in a hierarchical model.
To obtain the associated belief updating, one computes free energy at the lower level by conditioning the probability
distributions from Bayes rule by the variables from the higher levels. This means that one performs belief updating at
the lower levels independently of the higher levels. Then, one computes the variational free energy at the higher levels
by treating the lower levels as outcomes. For more details on the speciﬁcities of the scheme see [3].
B Expected free energy
At the heart of active inference is a description of a certain class of systems at non-equilibrium steady-state (NESS)
[69, 70]. An important consequence of NESS is the existence of a steady-state probability distribution P(sτ,A) that
the agent is guaranteed to reach given a sufﬁcient amount of time. Intuitively, this distribution should be thought as
the agent’s preferences over states and model parameters. Practically, this means that the agent selects policies, such
that its predicted states Q(sτ,A) at some future time point τ > t– usually, the time horizon of a policy T – reach
its preferences P(sτ,A), which are speciﬁed by the generative model. In the following, we will show how a speciﬁc
family of distributions Q(π) guarantee an agent to reach its preferences. Then, we will see how NESS enables in fact
to extract one single canonical member of this family: the (softmax negative) expected free energy.
Objective: we seek distributions over policies that imply steady-state solutions; i.e., when the ﬁnal distribution
does not depend upon initial observations. Such solutions ensure that, on average, stochastic policies lead to
a steady-state or target distribution speciﬁed by the generative model. These solutions exist in virtue of condi-
tional independencies, where the hidden states provide a Markov blanket that separates policies from outcomes.
In other words, policies cause ﬁnal states that cause outcomes. In what follows, τ > t is a future time and
Q := Q(oτ,sτ,A,π ) ≈P(oτ,sτ,A,π |o1:t) is the corresponding approximate posterior distribution, given initial
conditions o1:t.
Lemma 1(Steady-state). The surprisal over policies −log Q(π) and the Gibbs energy,
G(π; β) = DKL[Q(sτ,A|π)||P(sτ,A)] −EQ(oτ,sτ,A|π)[βlog P(oτ|sτ,A)] (34)
β := EQ[log Q(π|sτ,A)]
EQ[log P(oτ|sτ,A)] ≥0 (35)
are equal on average under Qif and only if the system reaches steady-state. Explicitly:
EQ[−log Q(π)] = EQ[G(π; β)] ⇐⇒DKL[Q(sτ,A)||P(sτ,A)] = 0 (36)
Here, β ≥0 characterises the steady-state with the relative precision (i.e., negative entropy) of policies and ﬁnal
outcomes, given ﬁnal states. The generative model stipulates steady-state, in the sense that distribution over ﬁnal
states (and outcomes) does not depend upon initial observations. Here, the generative and predictive distributions
simply express the conditional independence between policies and ﬁnal outcomes, given ﬁnal states. Note that when
β = 1, Gibbs energy becomes expected free energy.
Proof. Let us unpack the Gibbs energy expected under Q:
EQ[G(π; β)] = EQ[DKL[Q(sτ,A|π)||P(sτ,A)] −EQ(oτ,sτ,A|π)[βlog P(oτ|sτ,A)]]
= EQ[DKL[Q(sτ,A|π)||P(sτ,A)]]
− EQ[log Q(π|sτ,A)]
EQ[log P(oτ|sτ,A)]EQ[EQ(oτ,sτ,A|π)[log P(oτ|sτ,A)]]
= EQ[log Q(sτ,A|π) −log P(sτ,A) −log Q(π|sτ,A)]
= EQ[−log Q(π) −log P(sτ,A) + logQ(sτ,A)]
= EQ[−log Q(π)] + DKL[Q(sτ,A)||P(sτ,A)]
(37)
25
And the result is immediate.
A straightforward consequence of Lemma 1, is that each distribution
Q(π) = σ(−G(π; β)), β ≥0 (38)
describes a certain kind of system that self-organises to some steady-state distribution. This family of distributions has
interesting interpretations: for example, the case β = 0 corresponds to standard stochastic control, variously known
as KL control or risk-sensitive control [143]:
G(π; 0) = DKL[Q(sτ,A|π)||P(sτ,A)] ≥DKL[Q(sτ|π)||P(sτ)] (39)
In other words, one chooses policies that minimise the KL divergence between the predictive and target distribution.
More generally, when β >0, policies are more likely when they simultaneously minimise the entropy of outcomes,
given states. In other words, β >0 ensures that the system exhibits itinerant behaviour. One can see that KL control
may arise in this case if the entropy of the likelihood mapping remains constant with respect to policies.
Remark 2. It is possible to extend this framework by considering systems that reach their preferences at a collection
of time-steps into the future, say τ1,...,τ n >t. In this case, one can adapt the proof of Lemma 1 to obtain:
EQ
[ n∑
i=1
G(π,τi; β)
]
= EQ[−nlog Q(π)] +
n∑
i=1
DKL[Q(sτi,A)||P(sτi,A)] (40)
where G(π,τi; β) is the Gibbs free energy of Lemma 1, replacing τ by τi. In this case, the canonical choice of
approximate posterior over policies would be:
Q(π) = σ
(
1
n
n∑
i=1
G(π,τi; β)
)
(41)
One perspective – on the distinction between simple and general steady-states – is in terms of uncertainty about
policies. For example, simple steady-states preclude uncertainty about which policy led to a ﬁnal state. This would be
appropriate for describing classical systems (that follow a unique path of least action), where it would be possible to
infer which policy had been pursued, given the initial and ﬁnal outcomes. Conversely, in general steady-state systems
(e.g., mice, Homo sapiens), simply knowing that ‘you are here’ does not tell me ‘how you got here’, even if I knew
where you were this morning. Put another way, there are lots of paths or policies open to systems that attain a general
steady state.
In active inference, we are interested in a certain class of systems that self-organise to general steady-states; namely,
those that move through a large number of probabilistic conﬁgurations from their initial state to their ﬁnal steady-state.
The treatments in [69,70] effectively turn the steady-state lemma on its head by assuming NESS is stipulatively true –
and then characterise the ensuing self-organisation in terms of Bayes optimal policies:
Corollary 3(Active inference [70]). If a system attains a general steady-state, it will appear to behave in a Bayes
optimal fashion – both in terms of optimal Bayesian design (i.e., exploration) and Bayesian decision theory (i.e.,
exploitation). Crucially, the loss function deﬁning Bayesian risk is the negative log evidence for the generative model
entailed by an agent. In short, systems (i.e., agents) that attain general steady-states will look as if they are responding
to epistemic affordances [45].
So far, we have deduced the distribution over policies of systems that reach steady-state. However, recall that reaching
steady-state is only a consequence of NESS. In fact, NESS dynamics under a Markov blanket (c.f., Figure 1) imply
a slightly stronger statement: the most likely trajectories of systems described by active inference are those which
minimise expected free energy [69, 70] – this is exactly the case β = 1 in (38). This is nice, since many existing
theories of cognition and control emerge under this speciﬁc imperative (c.f., Figure 4).
C Computing expected free energy
In this appendix, we present the derivations underlying the analytical expression of the expected free energy that is
used in spm_MDP_VB_X.m. Following [120], we can reexpress the expected free energy in the following form:
26
G(π) = EQ(sτ|π)[H[P(oτ|sτ)]]  
Ambiguity
+ DKL[Q(sτ|π)||P(sτ)]  
Risk (states)
−EP(oτ|sτ)Q(sτ|π)[DKL[Q(A|oτ,sτ)||Q(A)]]  
Novelty
(42)
Here, Q(A|oτ,sτ) denotes approximate posterior beliefs about A if we knew occurence of the state outcome pair
(oτ,sτ). In the following, we show that we can compute the expected free energy in the following way
G(π) ≈H·sπτ  
Ambiguity
+ sπτ ·(log sπτ −log C)  
Risk (states)
−Asπτ ·Wsπτ  
Novelty
H := −diag[A ·log A]
W := 1
2
(
a⊙(−1) −a⊙(−1)
0
)
(43)
when the agent’s preferencesC are expressed in terms of preferences over states. When preferences are expressed in
terms of outcomes (as is currently implemented in spm_MDP_VB_X.m), the risk term instead becomes
(Asπτ) ·(log(Asπτ) −log C)  
Risk (outcomes)
(44)
C.1 Ambiguity
The ambiguity term of (42) is EQ(sτ|π)[H[P(oτ|sτ)]]. By deﬁnition, the entropy inside the expectation is:
H[P(oτ|sτ)] = −
∑
oτ∈O
P(oτ|sτ) logP(oτ|sτ) (45)
The ﬁrst factor inside the sum corresponds to:
P(oτ|sτ) =
∫
P(oτ,A|sτ) dA
=
∫
P(oτ|sτ,A)P(A) dA
=
∫
oτ ·AsτP(A) dA
≈
∫
oτ ·AsτQ(A) dA
= oτ ·EQ(A)[A]sτ
= oτ ·Asτ
(46)
Here we have replaced the prior over model parameters P(A) by the approximate posterior Q(A). This is not nec-
essary, but in numerical simulations since learning occurs once at the end of the trial the two can be interchanged –
furthermore, this allows us to reuse previously introduced notation. In any case, this tells us that the entropy can be
re-expressed as:
H[P(oτ|sτ)] = −
∑
oτ∈O
(oτ ·Asτ)(oτ ·log(A)sτ)
= −
n∑
i=1
(A•isτ)(log(A•i)sτ)
= −
n∑
i=1
(A•i ⊙log(A•i))sτ
= −(A ⊙log A)sτ
= −diag[A ·log A] ·sτ
(47)
27
Finally,
EQ(sτ|π)[H[P(oτ|sτ)]]  
Ambiguity
= H·sπτ
H := −diag[A·log A]
(48)
C.2 Risk
The risk term of (42) is the KL divergence between predicted states following a particular policy and preferred states.
This can be expressed as:
DKL[Q(sτ|π)||P(sτ)]  
Risk (states)
= sπτ ·(sπτ −log C) (49)
Where the vector C ∈ Rm encodes preference over states P(sτ) = Cat(C). However, it is also possible to
approximate this risk term over states by a risk term over outcomes (c.f., (15)), as is currently implemented in
spm_MDP_VB_X.m. In this case, if C ∈Rn denotes the preferences over outcomes P(oτ) = Cat(C):
DKL[Q(oτ|π)||P(oτ)]  
Risk (outcomes)
= (Asπτ) ·(log(Asπτ) −log C) (50)
C.3 Novelty
The novelty term of (42) is EP(oτ|sτ)Q(sτ|π)[DKL[Q(A|oτ,sτ)||Q(A)]] where
Q(A) =
m∏
i=1
Q(A•i), Q (A•i) = Dir(a•i) (51)
Q(A|oτ,sτ) =
m∏
i=1
Q(A•i|oτ,sτ), Q (A•i|oτ,sτ) := Dir(a′
•i) (52)
The KL divergence between both distributions (c.f., (18)) can be expressed as:
DKL[Q(A|oτ,sτ)||Q(A)] =
m∑
i=1
[log Γ(a′
0i) −
n∑
k=1
log Γ(a′
ki) −log Γ(a0i)
+
n∑
k=1
log Γ(aki)] + (a′−a) ·(ψ(a′) −ψ(a′
0))
(53)
where ψ is the digamma function. We now want to make sense of a′. Suppose that at time τ the agents knows the
possible outcome j and possible state k as in Q(A|oτ,sτ) (c.f., Table 2 for terminology). This means that in this
case, beliefs about hidden states correspond to the true state; in other words, sτ = sτ. We can then use the rule of
accumulation of Dirichlet parameters to deduce a′= a + oτ ⊗sτ. In other words, a′
jk = ajk + 1 and the remaining
components are identical. Using the well-known identity:
Γ(x+ 1) = xΓ(x) ⇒log Γ(x+ 1) = logx+ log Γ(x) (54)
we can compute (53):
DKL[Q(A|oτ,sτ)||Q(A)] = log Γ(a0k + 1) −log Γ(a0k) −log Γ(ajk + 1) + log Γ(ajk) + ψ(ajk + 1) −ψ(a0k
= log a0k −log ajk + ψ(ajk + 1) −ψ(a0k + 1)
(55)
28
Using the deﬁnition of the digamma function ψ(x) = d
dx log Γ(x) we obtain:
DKL[Q(A|oτ,sτ)||Q(A)] = log a0k −log ajk + d
dajk
(log Γ(ajk + 1)) − d
da0k
(log Γ(a0k + 1))
= log a0k −log ajk + d
dajk
(log Γ(ajk + 1)) − d
da0k
(log a0k + log Γ(a0k))
= log a0k −log ajk + 1
ajk
− 1
a0k
+ ψ(ajk) −ψ(a0k)
(56)
We can use an asymptotic expansion of the digamma function to simplify the expression:
ψ(x) ≈log x− 1
2x + ···
⇒DKL[Q(A|oτ,sτ)||Q(A)] ≈ 1
2ajk
− 1
2a0k
(57)
Finally, the analytical expression of the novelty term:
EP(oτ|sτ)Q(sτ|π)[DKL[Q(A|oτ,sτ)||Q(A)]] ≈Asπτ ·Wsπτ
W := 1
2
(
a⊙−1 −a⊙−1
0
) (58)
References
[1] Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological Cybernetics,
112(4):323–343, August 2018.
[2] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and
active inference. Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018.
[3] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka.
Active Inference, Curiosity and Insight. Neural Computation, 29(10):2633–2683, October 2017.
[4] Thomas Parr and Karl J. Friston. Active inference and the anatomy of oculomotion. Neuropsychologia,
111:334–343, March 2018.
[5] M. Berk Mirza, Rick A. Adams, Christoph D. Mathys, and Karl J. Friston. Scene Construction, Visual Foraging,
and Active Inference. Frontiers in Computational Neuroscience, 10, June 2016.
[6] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of the Royal Society
Interface, 14(136), November 2017.
[7] Thomas Parr and Karl J. Friston. The Computational Anatomy of Visual Neglect. Cerebral Cortex (New York,
N.Y.: 1991), 28(2):777–790, January 2018.
[8] Rick A. Adams, Klaas Enno Stephan, Harriet R. Brown, Christopher D. Frith, and Karl J. Friston. The Compu-
tational Anatomy of Psychosis. Frontiers in Psychiatry, 4, 2013.
[9] Jelle Bruineberg, Erik Rietveld, Thomas Parr, Leendert van Maanen, and Karl J Friston. Free-energy mini-
mization in joint agent-environment systems: A niche construction perspective.Journal of Theoretical Biology,
455:161–178, October 2018.
[10] A Constant, M Ramstead, Samuel P. L. Veissière, J O Campbell, and K.J. Friston. A variational approach to
niche construction. Journal of The Royal Society Interface, 15(141):20170685, April 2018.
[11] Axel Constant, Maxwell J. D. Ramstead, Samuel P. L. Veissière, and Karl Friston. Regimes of Expectations:
An Active Inference Model of Social Conformity and Human Decision Making. Frontiers in Psychology, 10,
2019.
[12] M. Berk Mirza, Rick A. Adams, Thomas Parr, and Karl Friston. Impulsivity and Active Inference. Journal of
Cognitive Neuroscience, 31(2):202–220, February 2019.
29
[13] Beren Millidge. Implementing Predictive Processing and Active Inference: Preliminary Steps and Results.
Preprint, PsyArXiv, March 2019.
[14] Ozan Çatal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy selection using
active inference. arXiv:1904.08149 [cs], April 2019.
[15] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference? PLoS
ONE, 4(7):e6421, July 2009.
[16] Karl Friston, Rick Adams, and Read Montague. What is value—accumulated reward or evidence? Frontiers in
Neurorobotics, 6, 2012.
[17] Rafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning. Journal of
Mathematical Psychology, 76:198–211, February 2017.
[18] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychology , 81:55–79, December
2017.
[19] S. J. Luck and E. K. V ogel. The capacity of visual working memory for features and conjunctions. Nature,
390(6657):279–281, November 1997.
[20] Weiwei Zhang and Steven J. Luck. Discrete Fixed-Resolution Representations in Visual Working Memory.
Nature, 453(7192):233–235, May 2008.
[21] Howard Eichenbaum, Paul Dudchenko, Emma Wood, Matthew Shapiro, and Heikki Tanila. The Hippocampus,
Memory, and Place Cells: Is It Spatial Memory or a Memory Space? Neuron, 23(2):209–226, June 1999.
[22] J. O’Keefe and J. Dostrovsky. The hippocampus as a spatial map. Preliminary evidence from unit activity in the
freely-moving rat. Brain Research, 34(1):171–175, November 1971.
[23] Nathaniel D. Daw, John P. O’Doherty, Peter Dayan, Ben Seymour, and Raymond J. Dolan. Cortical substrates
for exploratory decisions in humans. Nature, 441(7095):876–879, June 2006.
[24] Paul Reverdy, Vaibhav Srivastava, and Naomi E. Leonard. Modeling Human Decision-making in Generalized
Gaussian Multi-armed Bandits. arXiv:1307.6134 [cs, math, stat], July 2013.
[25] Charley M. Wu, Eric Schulz, Maarten Speekenbrink, Jonathan D. Nelson, and Björn Meder. Generalization
guides human exploration in vast decision spaces. Nature Human Behaviour, 2(12):915–924, December 2018.
[26] Nathaniel D. Daw, Samuel J. Gershman, Ben Seymour, Peter Dayan, and Raymond J. Dolan. Model-Based
Inﬂuences on Humans’ Choices and Striatal Prediction Errors. Neuron, 69(6):1204–1215, March 2011.
[27] Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4):381–414, December 2017.
[28] Thomas Parr and Karl J. Friston. The Discrete and Continuous Brain: From Decisions to Movement—And
Back Again. Neural Computation, 30(9):2319–2347, September 2018.
[29] Karl J. Friston, Noor Sajid, David Ricardo Quiroga-Martinez, Thomas Parr, Cathy J. Price, and Emma Holmes.
Active Listening. bioRxiv, page 2020.03.18.997122, March 2020.
[30] Thomas Parr and Karl J. Friston. The computational pharmacology of oculomotion. Psychopharmacology,
April 2019.
[31] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal control without
cost functions. Biological Cybernetics, 106(8):523–541, October 2012.
[32] Thomas H. B. FitzGerald, Philipp Schwartenbeck, Michael Moutoussis, Raymond J. Dolan, and Karl Friston.
Active inference, evidence accumulation, and the urn task. Neural Computation, 27(2):306–328, February
2015.
[33] Thomas H. B. FitzGerald, Rosalyn J. Moran, Karl J. Friston, and Raymond J. Dolan. Precision and neuronal
dynamics in the human posterior parietal cortex during evidence accumulation. NeuroImage, 107:219–228,
February 2015.
[34] Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, Friedrich Wurst, Martin
Kronbichler, and Karl Friston. Optimal inference with suboptimal models: Addiction and active Bayesian
inference. Medical Hypotheses, 84(2):109–117, February 2015.
[35] Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine, reward learning, and active infer-
ence. Frontiers in Computational Neuroscience, 9, November 2015.
30
[36] Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timothy Behrens, and Ray-
mond J. Dolan. The anatomy of choice: Dopamine and decision-making. Philosophical Transactions of the
Royal Society B: Biological Sciences, 369(1655), November 2014.
[37] Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, and Karl Friston. The
Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes.Cerebral Cortex (New York,
N.Y.: 1991), 25(10):3434–3445, October 2015.
[38] Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, Martin Kronbichler, and Karl
Friston. Evidence for surprise minimization over value maximization in choice behavior. Scientiﬁc Reports,
5:16575, November 2015.
[39] Michael Moutoussis, Nelson J. Trujillo-Barreto, Wael El-Deredy, Raymond J. Dolan, and Karl J. Friston. A
formal model of interpersonal inference. Frontiers in Human Neuroscience, 8, March 2014.
[40] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pez-
zulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, October 2015.
[41] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O’Doherty, and Giovanni
Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, September 2016.
[42] Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl J. Friston. Model averaging, optimal inference, and
habit formation. Frontiers in Human Neuroscience, 8, 2014.
[43] Karl Friston and Gyorgy Buzsáki. The Functional Anatomy of Time: What and When in the Brain. Trends in
Cognitive Sciences, 20(7):500–511, July 2016.
[44] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active
Inference: A Process Theory. Neural Computation, 29(1):1–49, January 2017.
[45] Thomas Parr and Karl J Friston. Working memory, attention, and salience in active inference.Scientiﬁc Reports,
7(1):14678, December 2017.
[46] Thomas Parr, Geraint Rees, and Karl J. Friston. Computational Neuropsychology and Bayesian Inference.
Frontiers in Human Neuroscience, 12, 2018.
[47] Thomas Parr, Rajeev Vijay Rikhye, Michael M Halassa, and Karl J Friston. Prefrontal Computation as Active
Inference. Cerebral Cortex, page bhz118 e0190429 e12112 e37454, July 2019.
[48] David Benrimoh, Thomas Parr, Peter Vincent, Rick A. Adams, and Karl Friston. Active Inference and Auditory
Hallucinations. Computational Psychiatry, 2:183–204, November 2018.
[49] Thomas Parr, David A. Benrimoh, Peter Vincent, and Karl J. Friston. Precision and False Perceptual Inference.
Frontiers in Integrative Neuroscience, 12, 2018.
[50] Anna C. Sales, Karl J. Friston, Matthew W. Jones, Anthony E. Pickering, and Rosalyn J. Moran. Locus
Coeruleus tracking of prediction errors optimises cognitive ﬂexibility: An Active Inference model. bioRxiv,
page 340620, June 2018.
[51] Peter Vincent, Thomas Parr, David Benrimoh, and Karl J. Friston. With an eye on uncertainty: Modelling
pupillary responses to environmental volatility. PLOS Computational Biology, 15(7):e1007126, 05-Jul-2019.
[52] Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active Inference in OpenAI Gym: A Paradigm
for Computational Investigations Into Psychiatric Illness. Biological Psychiatry: Cognitive Neuroscience and
Neuroimaging, 3(9):809–818, September 2018.
[53] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference.
arXiv:1911.10601 [cs, eess, math, stat], November 2019.
[54] Christopher M. Bishop. Pattern Recognition and Machine Learning . Information Science and Statistics.
Springer, New York, 2006.
[55] Yang Xitong. Understanding the Variational Lower Bound. 2017.
[56] Biswa Sengupta and Karl Friston. Approximate Bayesian inference as a gauge theory. In Computational
Biology Workshop, volume 14, page e1002400, March 2016.
[57] Biswa Sengupta, Arturo Tozzi, Gerald K. Cooray, Pamela K. Douglas, and Karl J. Friston. Towards a Neuronal
Gauge Theory. PLOS Biology, 14(3):e1002400, 08-Mar-2016.
[58] Laurence Aitchison and Máté Lengyel. With or without you: Predictive coding and Bayesian inference in the
brain. Current Opinion in Neurobiology, 46:219–227, October 2017.
[59] Karl Friston. The history of the future of the Bayesian brain. NeuroImage, 62(2):1230–1233, August 2012.
31
[60] David C. Knill and Alexandre Pouget. The Bayesian brain: The role of uncertainty in neural coding and
computation. Trends in Neurosciences, 27(12):712–719, December 2004.
[61] Ryan T. McKay and Daniel C. Dennett. The evolution of misbelief. The Behavioral and Brain Sciences ,
32(6):493–510; discussion 510–561, December 2009.
[62] Tali Sharot. The optimism bias. Current Biology, 21(23):R941–R945, December 2011.
[63] Karl Friston. The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, 13(7):293–
301, July 2009.
[64] Karl Friston. The free-energy principle: A uniﬁed brain theory? Nature Reviews Neuroscience, 11(2):127–138,
February 2010.
[65] Karl J. Friston and Klaas E. Stephan. Free-energy and the brain. Synthese, 159(3):417–458, November 2007.
[66] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain.Journal of Physiology-Paris,
100(1-3):70–87, July 2006.
[67] J. M. Fuster. Prefrontal cortex and the bridging of temporal gaps in the perception-action cycle. Annals of the
New York Academy of Sciences, 608:318–329; discussion 330–336, 1990.
[68] Jakob Hohwy. The Self-Evidencing Brain. Noûs, 50(2):259–285, June 2016.
[69] Thomas Parr, Lancelot Da Costa, and Karl J. Friston. Markov blankets, information geometry and stochastic
thermodynamics. Phil. Trans. R. Soc. A., 2019.
[70] Karl Friston. A free energy principle for a particular physics. BioArxiv, page 148, 2019.
[71] Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. The Markov blankets
of life: Autonomy, active inference and the free energy principle. Journal of The Royal Society Interface ,
15(138):20170792, January 2018.
[72] Judea Pearl. Graphical Models for Probabilistic and Causal Reasoning. In Philippe Smets, editor, Quantiﬁed
Representation of Uncertainty and Imprecision, Handbook of Defeasible Reasoning and Uncertainty Manage-
ment Systems, pages 367–389. Springer Netherlands, Dordrecht, 1998.
[73] Matthew James Beal. Variational Algorithms for Approximate Bayesian Inference. page 281, 2003.
[74] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians.
Journal of the American Statistical Association, 112(518):859–877, April 2017.
[75] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An Introduction to Vari-
ational Methods for Graphical Models. In Michael I. Jordan, editor, Learning in Graphical Models , pages
105–161. Springer Netherlands, Dordrecht, 1998.
[76] K J Aström. Optimal Control of Markov Processes with Incomplete State Information.Journal of Mathematical
Analysis and Applications, 10, 1965.
[77] Andrew Barto and Richard Sutton. Reinforcement Learning: An Introduction. 1992.
[78] James V Stone. Artiﬁcial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning .
2019.
[79] C. C. White. Markov decision processes. In Encyclopedia of Operations Research and Management Science,
pages 484–486. Springer US, Boston, MA, 2001.
[80] G E P Box and George C Tiao. Multiparameter problems from a Bayesian point of view. The Annals of
Mathematical Statistics, 1965.
[81] Greg M Allenby, Peter E Rossi, and Robert E McCulloch. Hierarchical Bayes Models: A Practitioners Guide.
2005.
[82] Judea Pearl. Probabilistic Reasoning in Intelligent Systems. 1988.
[83] Hans-Andrea Loeliger. An introduction to factor graphs. IEEE Signal Processing Magazine, 2004.
[84] Toshiyuki Tanaka. A Theory of Mean Field Approximation. page 10, 1999.
[85] Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. Neuronal message passing using Mean-
ﬁeld, Bethe, and Marginal approximations. Scientiﬁc Reports, 9(1):1889, December 2019.
[86] Sarah Schwöbel, Stefan Kiebel, and Dimitrije Markovi ´c. Active Inference, Belief Propagation, and the Bethe
Approximation. Neural Computation, 30(9):2530–2567, September 2018.
[87] James Tee and Desmond P. Taylor. Is Information in the Brain Represented in Continuous or Discrete Form?
arXiv:1805.01631 [cs, math, q-bio], May 2018.
32
[88] Timothy J. Buschman and Earl K. Miller. Shifting the Spotlight of Attention: Evidence for Discrete Computa-
tions in Cognition. Frontiers in Human Neuroscience, 4, 2010.
[89] J. Duncan, R. Ward, and K. Shapiro. Direct measurement of attentional dwell time in human vision. Nature,
369(6478):313–315, May 1994.
[90] Ayelet Nina Landau and Pascal Fries. Attention Samples Stimuli Rhythmically. Current Biology, 22(11):1000–
1004, June 2012.
[91] Simon Hanslmayr, Gregor V olberg, Maria Wimber, Sarang S. Dalal, and Mark W. Greenlee. Prestimulus Os-
cillatory Phase at 7 Hz Gates Cortical Information Flow and Visual Perception. Current Biology, 23(22):2273–
2278, November 2013.
[92] E T Rolls and M J Tovee. Processing speed in the cerebral cortex and the neurophysiology of visual masking.
Proceedings of the Royal Society of London. Series B: Biological Sciences, 257(1348):9–15, July 1994.
[93] Karl Friston. Hierarchical Models in the Brain. PLoS Computational Biology , 4(11):e1000211, November
2008.
[94] Zenas C. Chao, Kana Takaura, Liping Wang, Naotaka Fujii, and Stanislas Dehaene. Large-Scale Cortical
Networks for Hierarchical Prediction and Prediction Error in the Primate Brain. Neuron, 100(5):1252–1266.e3,
May 2018.
[95] Sandra Iglesias, Christoph Mathys, Kay H. Brodersen, Lars Kasper, Marco Piccirelli, Hanneke E. M. den
Ouden, and Klaas E. Stephan. Hierarchical Prediction Errors in Midbrain and Basal Forebrain during Sensory
Learning. Neuron, 80(2):519–530, October 2013.
[96] S. Kullback and R. A. Leibler. On Information and Sufﬁciency. The Annals of Mathematical Statistics ,
22(1):79–86, March 1951.
[97] Horace Barlow. Redundancy reduction revisited. Comput. Neural Syst., page 13, 2001.
[98] Yang Dan, Joseph J. Atick, and R. Clay Reid. Efﬁcient Coding of Natural Scenes in the Lateral Geniculate
Nucleus: Experimental Test of a Computational Theory. Journal of Neuroscience, 16(10):3351–3362, May
1996.
[99] Michael S. Lewicki. Efﬁcient coding of natural sounds. Nature Neuroscience, 5(4):356–363, April 2002.
[100] Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current Opinion in Neurobiology ,
14(4):481–487, August 2004.
[101] Bruno A. Olshausen and Kevin N. O’Connor. A new window on sound. Nature Neuroscience, 5(4):292–294,
April 2002.
[102] Emanuel Todorov and Michael I. Jordan. Optimal feedback control as a theory of motor coordination. Nature
Neuroscience, 5(11):1226–1235, November 2002.
[103] Peter Bossaerts and Carsten Murawski. From behavioural economics to neuroeconomics to decision neuro-
science: The ascent of biology in research on human decision making.Current Opinion in Behavioral Sciences,
5:37–42, October 2015.
[104] L. M. Optican and B. J. Richmond. Temporal encoding of two-dimensional patterns by single units in pri-
mate inferior temporal cortex. III. Information theoretic analysis. Journal of Neurophysiology, 57(1):162–178,
January 1987.
[105] R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information
Theory. Annual Review of Neuroscience, 13(1):257–281, 1990.
[106] H. B. Barlow. Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961.
[107] Hermann Haken. Synergetics: An Introduction Nonequilibrium Phase Transitions and Self-Organization in
Physics, Chemistry and Biology. Springer Series in Synergetics. Springer-Verlag, Berlin Heidelberg, 2 edition,
1978.
[108] Stuart A. Kauffman. The Origins of Order: Self-Organization and Selection in Evolution . Oxford University
Press, 1993.
[109] G. Nicolis and I. Prigogine. Self-Organization in Nonequilibrium Systems: From Dissipative Structures to
Order Through Fluctuations. Wiley-Blackwell, New York, June 1977.
[110] W. R. Ashby. Principles of the Self-Organizing Dynamic System. The Journal of General Psychology ,
37(2):125–128, October 1947.
[111] Claude Bernard. Lectures on the Phenomena of Life Common to Animals and Plants. Thomas, 1974.
33
[112] Roger C Conant and W. R. Ashby. Every good regulator of a system must be a model of that system. Int. J.
Systems Sci., 1(2):89–97, 1970.
[113] David J. C. MacKay. Information Theory, Inference and Learning Algorithms . Cambridge University Press,
Cambridge, UK ; New York, sixth printing 2007 edition edition, September 2003.
[114] David MacKay. A Free Energy Minimization Algorithm for Decoding and Cryptanalysis. In Electronic Letters,
1995.
[115] Hermann von Helmholtz and James P. C Southall. Helmholtz’s Treatise on Physiological Optics.Dover Publi-
cations, New York, 1962. OCLC: 523553.
[116] R. L. Gregory. Perceptions as hypotheses. Philosophical Transactions of the Royal Society of London. Series
B, Biological Sciences, 290(1038):181–197, July 1980.
[117] Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. The Helmholtz Machine. Neural
Computation, 7(5):889–904, September 1995.
[118] J.S. Yedidia, W.T. Freeman, and Y . Weiss. Constructing Free-Energy Approximations and Generalized Belief
Propagation Algorithms. IEEE Transactions on Information Theory, 51(7):2282–2312, July 2005.
[119] T. Heskes. Convexity Arguments for Efﬁcient Minimization of the Bethe and Kikuchi Free Energies. Journal
of Artiﬁcial Intelligence Research, 26:153–190, June 2006.
[120] Thomas Parr. The Computational Neurology of Active Vision . Ph.D., University College London, London,
2019.
[121] Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook. page 72.
[122] Mikael Lundqvist, Jonas Rose, Pawel Herman, Scott L. Brincat, Timothy J. Buschman, and Earl K. Miller.
Gamma and Beta Bursts Underlie Working Memory. Neuron, 90(1):152–164, April 2016.
[123] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of
some extra-classical receptive-ﬁeld effects. Nature Neuroscience, 2(1):79–87, January 1999.
[124] Andre M. Bastos, W. Martin Usrey, Rick A. Adams, George R. Mangun, Pascal Fries, and Karl J. Friston.
Canonical Microcircuits for Predictive Coding. Neuron, 76(4):695–711, November 2012.
[125] Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational free energy
and the Laplace approximation. NeuroImage, 34(1):220–234, January 2007.
[126] André C. Marreiros, Jean Daunizeau, Stefan J. Kiebel, and Karl J. Friston. Population dynamics: Variance and
the sigmoid activation function. NeuroImage, 42(1):147–157, August 2008.
[127] Gustavo Deco, Viktor K. Jirsa, Peter A. Robinson, Michael Breakspear, and Karl Friston. The Dynamic Brain:
From Spiking Neurons to Neural Masses and Cortical Fields. PLoS Computational Biology , 4(8):e1000092,
August 2008.
[128] Rosalyn Moran, Dimitris A. Pinotsis, and Karl Friston. Neural masses and ﬁelds in dynamic causal modeling.
Frontiers in Computational Neuroscience, 7, 2013.
[129] Georg V on Békésy. Sensory Inhibition. Princeton University Press, 1967.
[130] John Winn and Christopher M Bishop. Variational Message Passing. Journal of Machine Learning Research,
page 34, 2005.
[131] Justin Dauwels. On Variational Message Passing on Factor Graphs. In 2007 IEEE International Symposium on
Information Theory, pages 2546–2550, Nice, June 2007. IEEE.
[132] Justin Dauwels, François Vialatte, Tomasz Rutkowski, and Andrzej Cichocki. Measuring Neural Synchrony by
Message Passing. In NIPS, 2007.
[133] Dileep George. Belief Propagation and Wiring Length Optimization as Organizing Principles for Cortical
Microcircuits. 2005.
[134] Gail A. Carpenter and Stephen Grossberg. A massively parallel architecture for a self-organizing neural pattern
recognition machine. Computer Vision, Graphics, and Image Processing, 37(1):54–115, January 1987.
[135] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, November 1998.
[136] Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in cortex. Nature
Neuroscience, 2(11):1019–1025, November 1999.
[137] Wolfgang Maass. On the Computational Power of Winner-Take-All. Neural Computation, 12(11):2519–2535,
November 2000.
34
[138] Quentin J. M. Huys, Neir Eshel, Elizabeth O’Nions, Luke Sheridan, Peter Dayan, and Jonathan P. Roiser.
Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices by Pruning Decision
Trees. PLoS Computational Biology, 8(3):e1002410, March 2012.
[139] Stanislas Dehaene, Florent Meyniel, Catherine Wacongne, Liping Wang, and Christophe Pallier. The Neural
Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees.Neuron,
88(1):2–19, October 2015.
[140] Jordi Fonollosa, Emre Neftci, and Mikhail Rabinovich. Learning of Chunking Sequences in Cognition and
Behavior. PLOS Computational Biology, 11(11):e1004592, 19-Nov-2015.
[141] Masahiko Haruno, Daniel Wolpert, and Mitsuo Kawato. Hierarchical MOSAIC for Movement Generation .
2003.
[142] Emanuel Todorov. General duality between optimal control and estimation. 2008 47th IEEE Conference on
Decision and Control, pages 4286–4292, 2008.
[143] Bart van den Broek, Wim Wiegerinck, and Bert Kappen. Risk sensitive path integral control. UAI, 2010.
[144] W. H. Fleming and S. J. Sheu. Risk-sensitive control and an optimal investment model II.The Annals of Applied
Probability, 12(2):730–767, May 2002.
[145] Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or Surprise? Frontiers in Psychology, 4,
2013.
[146] Emil Kauder. Genesis of the Marginal Utility Theory: From Aristotle to the End of the Eighteenth Century.
The Economic Journal, 63(251):638–650, September 1953.
[147] Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010). IEEE Transac-
tions on Autonomous Mental Development, 2(3):230–247, September 2010.
[148] James O. Berger. Statistical Decision Theory and Bayesian Analysis . Springer Series in Statistics. Springer-
Verlag, New York, 2 edition, 1985.
[149] Jürgen Schmidhuber. Developmental robotics, optimal artiﬁcial curiosity, creativity, music, and the ﬁne arts.
Connection Science, 18(2):173–187, June 2006.
[150] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational ap-
proaches. Frontiers in Neurorobotics, 1, 2009.
[151] Edward Deci and Richard M. Ryan. Intrinsic Motivation and Self-Determination in Human Behavior. Perspec-
tives in Social Psychology. Springer US, 1985.
[152] Ronald A. Howard. Information Value Theory. IEEE Transactions on Systems Science and Cybernetics ,
2(1):22–26, August 1966.
[153] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision research, 49(10):1295–1306,
May 2009.
[154] Philipp Schwartenbeck, Thomas FitzGerald, Raymond J. Dolan, and Karl Friston. Exploration, novelty, sur-
prise, and free energy minimization. Frontiers in Psychology, 4, October 2013.
[155] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration
in Dynamic Environments. arXiv:1103.5708 [cs, stat], March 2011.
[156] H B Barlow. Inductive Inference, Coding, Perception, and Language. Perception, 3(2):123–134, June 1974.
[157] D. V . Lindley. On a Measure of the Information Provided by an Experiment. The Annals of Mathematical
Statistics, 27(4):986–1005, 1956.
[158] David J. C. MacKay. Information-Based Objective Functions for Active Data Selection. Neural Computation,
4(4):590–604, July 1992.
[159] Hagai Attias. Planning by Probabilistic Inference. In 9th Int. Workshop on Artiﬁcial Intelligence and Statistics,
page 8, 2003.
[160] Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences, 16(10):485–488,
October 2012.
[161] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk. Decision, Proba-
bility, and Utility: Selected Readings. Cambridge University Press, New York, NY , US, 1988.
[162] J. V on Neumann and O. Morgenstern. Theory of Games and Economic Behavior . Theory of Games and
Economic Behavior. Princeton University Press, Princeton, NJ, US, 1944.
35
[163] E. T. Jaynes. Information Theory and Statistical Mechanics. Physical Review, 106(4):620–630, May 1957.
[164] Baris Kurt. Kullback-Leibler Divergence Between Two Dirichlet (and Beta) Distributions, 2013.
[165] W.D. Penny. KL-divergence of Normal, Gamma, Dirichlet and Wishart densitites. 2001.
[166] Thomas Parr and Karl J. Friston. The Anatomy of Inference: Generative Models and Brain Structure. Frontiers
in Computational Neuroscience, 12, 2018.
[167] Marjan Jahanshahi, Ignacio Obeso, John C. Rothwell, and José A. Obeso. A fronto-striato-subthalamic-pallidal
network for goal-directed and habitual inhibition. Nature Reviews. Neuroscience, 16(12):719–732, December
2015.
[168] G. S. Berns and T. J. Sejnowski. How the Basal Ganglia Make Decisions. In A. R. Damasio, H. Damasio,
and Y . Christen, editors,Neurobiology of Decision-Making, Research and Perspectives in Neurosciences, pages
101–113. Springer Berlin Heidelberg, Berlin, Heidelberg, 1996.
[169] Long Ding and Joshua I. Gold. The basal ganglia’s contributions to perceptual decision-making. Neuron,
79(4):640–649, August 2013.
[170] Suzanne N. Haber. The primate basal ganglia: Parallel and integrative networks. Journal of Chemical Neu-
roanatomy, 26(4):317–330, December 2003.
[171] Florence Thibaut. Basal ganglia play a crucial role in decision making. Dialogues in Clinical Neuroscience,
18(1):3, March 2016.
[172] Karl Friston and Will Penny. Post hoc Bayesian model selection. NeuroImage, 56(4):2089–2099, June 2011.
[173] Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An active inference model of concept
learning. bioRxiv, page 633677, May 2019.
[174] Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian model reduction. arXiv:1805.07092 [stat], May 2018.
[175] Gerda Claeskens and Nils Lid Hjort. Model Selection and Model Averaging. Cambridge University Press, 2006.
[176] Klaas Enno Stephan, Will D. Penny, Jean Daunizeau, Rosalyn J. Moran, and Karl J. Friston. Bayesian model
selection for group studies. NeuroImage, 46(4):1004–1017, July 2009.
[177] J.A. Hobson and K.J. Friston. Waking and dreaming consciousness: Neurobiological and functional consider-
ations. Progress in Neurobiology, 98(1):82–98, July 2012.
[178] J. Allan Hobson, Charles C.-H. Hong, and Karl J. Friston. Virtual reality and consciousness inference in
dreaming. Frontiers in Psychology, 5, 2014.
[179] G. Hinton, P Dayan, B. Frey, and R. Neal. The "wake-sleep" algorithm for unsupervised neural networks.
Science, 268(5214):1158–1161, May 1995.
[180] Samuel J. Gershman and Yael Niv. Learning latent structure: Carving nature at its joints. Current Opinion in
Neurobiology, 20(2):251–256, April 2010.
[181] D. Gowanlock R. Tervo, Joshua B. Tenenbaum, and Samuel J. Gershman. Toward the neural implementation
of structure learning. Current Opinion in Neurobiology, 37:99–105, April 2016.
[182] M. Berk Mirza, Rick A. Adams, Christoph Mathys, and Karl J. Friston. Human visual exploration reduces
uncertainty about the sensed world. PLOS ONE, 13(1):e0190429, 05-Jan-2018.
[183] Harriet Brown, Rick A. Adams, Isabel Parees, Mark Edwards, and Karl Friston. Active inference, sensory
attenuation and illusions. Cognitive Processing, 14(4):411–427, November 2013.
[184] Rick A. Adams, Stewart Shipp, and Karl J. Friston. Predictions not commands: Active inference in the motor
system. Brain Structure & Function, 218(3):611–643, May 2013.
[185] Karl Friston, Rick Adams, Laurent Perrinet, and Michael Breakspear. Perceptions as Hypotheses: Saccades as
Experiments. Frontiers in Psychology, 3, 2012.
[186] Harriet Brown and Karl J. Friston. Free-Energy and Illusions: The Cornsweet Effect. Frontiers in Psychology,
3, 2012.
[187] Michael I. Jordan, Zoubin Ghahramani, and Lawrence K. Saul. Hidden Markov Decision Trees. 1997.
[188] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann machines.
Cognitive Science, 9(1):147–169, January 1985.
[189] Ruslan Salakhutdinov and Geoffrey Hinton. An efﬁcient learning procedure for deep Boltzmann machines.
Neural Computation, 24(8):1967–2006, August 2012.
[190] K.J. Friston, L. Harrison, and W. Penny. Dynamic causal modelling. NeuroImage, 19(4):1273–1302, August
2003.
36