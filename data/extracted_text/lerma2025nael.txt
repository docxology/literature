Submitted to:
FEAR 2025
© B.M. Lerma & R. Peñaloza
This work is licensed under the
Creative Commons Attribution License.
NAEL: Non-Anthropocentric Ethical Logic
Bianca Maria Lerma Rafael Peñaloza
University of Milano-Bicocca, Milan, Italy
biancalerma99@gmail.com, rafael.penaloza@unimib.it
We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical framework for artificial
agents grounded in active inference and symbolic reasoning. Departing from conventional, human-
centred approaches to AI ethics, NAEL formalizes ethical behaviour as an emergent property of
intelligent systems minimizing global expected free energy in dynamic, multi-agent environments.
We propose a neuro-symbolic architecture to allow agents to evaluate the ethical consequences of
their actions in uncertain settings. The proposed system addresses the limitations of existing ethical
models by allowing agents to develop context-sensitive, adaptive, and relational ethical behaviour
without presupposing anthropomorphic moral intuitions. A case study involving ethical resource
distribution illustrates NAEL’s dynamic balancing of self-preservation, epistemic learning, and col-
lective welfare.
1 Introduction
As artificial intelligence (AI) systems increasingly participate in high-stakes decision-making—ranging
from healthcare to environmental governance—there is a growing urgency to design machines capa-
ble of ethical reasoning [2, 17]. The prevailing models of machine ethics, however, remain steeped in
anthropocentrism—either by hardcoding human moral principles or by replicating human cognitive ar-
chitectures [4, 14]. These approaches presume that ethical reasoning can and should be modelled on
human behaviour, norms, and linguistic frameworks. Yet this assumption not only constrains the expres-
sive capacity of AI, it also risks overlooking the epistemic and ontological differences between humans
and artificial agents [12, 18].
The core problem is not just technical but philosophical: can morality be meaningfully imposed from
outside, or must it emerge from within an agent’s own experience and interactions? Furthermore, how
can AI agents develop ethical behavior if their perceptual and cognitive substrates differ fundamentally
from ours [16, 13]? We argue that ethical reasoning in AI should be modeled not as a simulation of
human norms but as a formal, emergent process grounded in the agent’s ongoing engagement with its
environment.
In response to this need, we propose NAEL, aNon-Anthropocentric Ethical Logicdesigned to for-
malize adaptive ethical behaviour in autonomous systems. NAEL integratesactive inference, a neuro-
computational theory of cognition and action [5], with symbolic reasoning frameworks from logic and
philosophy, including deontic, standpoint, and subjective logics [6, 9, 7]. Our guiding principle is that
ethical actions are those that contribute to the minimization ofexpected free energy; not just for the
agent, but for the system as a whole [15]. This enables a shift from egoistic optimization to relational,
cooperative ethical reasoning.
NAEL is not a predefined moral rulebook but adynamic reasoning system: it encodes structural
constraints on ethical deliberation (e.g., coherence, interdependence, adaptability), while enabling agents
to update their ethical beliefs through interaction. In this sense, NAEL aligns with the view that ethics is
not a static entity but aprocess of continual negotiation, prediction, and adjustment[1].
arXiv:2510.14676v1  [cs.AI]  16 Oct 2025
2 NAEL
2 Preliminaries
In this section, we outline the theoretical foundations that support the NAEL framework. Specifically,
we present two key components:Active Inference, a formalism for modelling perception, action, and
learning as uncertainty minimization; andsymbolic reasoning, which provides a logical structure for
ethical deliberation. These elements converge within NAEL to allow for autonomous ethical behaviour
in uncertain, dynamic environments.
2.1 Active Inference
Active inference is a unifying theory of action and perception based on the minimization of so-called
variational free energy [5]. In a nutshell, it proposes that biological and artificial agents are continuously
making predictions about their environment, and act to minimize the discrepancy between their predicted
and observed sensory states. Effectively, agents strive to reduce theirsurprise. Since surprise is neither
measurable nor orderable, active inference focuses on a proxy calledvariational free energy.
Formally, consider two disjoint classesOof possibleobservationsandSof (hidden)statesof the
world. The agent is assumed to have agenerative modelwhich produces a probability distributionP:
O×S→[0,1], and arecognition distribution Q:S→[0,1], which measures the agent’s belief about
the current state. Given an observationo∈O, the variational free energy is defined as the relative entropy
betweenQandPgiveno; that is,F(o) =E Q[logQ(s)−logP(o,s)],whereE Q denotes the expectation
overQ. This measure is known as the Kullback-Leibler divergence [10] between the predicted state (by
Q) and the true posterior (P). The idea is that the recognition distributionQof an agent is build in order
to minimise this variational free energy for any observation.
In active inference, the agent is not passively observing the world, but actively interacts in it. The
actions the agent selects are made to minimise itsexpectedfree energyE[F]. The intuition is that this
expectation accounts for theriskof diverging from the expected outcome of the action andambiguityim-
plicit in the uncertainty about the hidden states. This formulation allows for both goal-directed behaviour
and information-seeking exploration by the agent.
Within NAEL, we generalize this principle so that the agent does not only minimize itsownexpected
free energy but also estimates and incorporates the (predicted) free energy of other agents and the envi-
ronment. This shift enables ethical reasoning as a process of minimizing global uncertainty. It should be
clear that active inference is a continuous learning process, as the agent adapts its recognition distribution
to lower observed divergences, and the generative model changes with the behaviour of different agents.
2.2 Symbolic Reasoning
While active inference governs behaviour at the perceptual and dynamic level, symbolic reasoning pro-
vides structure and interpretability to ethical decisions. Given the social, contextual, and normative
nature of ethics, no single simple logical formalism may account for all the facets of ethical reason-
ing. Hence, NAEL combines the notions of three formalisms, to deal with each main element formally.
Specifically, we combine deontic, standpoint, and subjective logic, which we describe next.
Deontic logicsfocus on normative concepts like obligations, permissions, prohibitions, and com-
pensations [6]. While more detailed variants have been developed to handle justice systems—including
defeasibility, quantifications, and qualitative comparisons—as a first approach we focus on the simple
variant where obligations and permissions are expressed through modalities. This formalism enables the
agent to evaluate its actions by their moral status, rather than by their mere outcome.Standpoint logic
B.M. Lerma & R. Peñaloza3
is a recent formalism which allows for reasoning about different perspectives (standpoints) in a multi-
agent environment [7, 8]. The importance of this formalism to NAEL is that it allows NAEL agents to
represent, weigh, and reason about the ethical perspectives of others, avoiding solipsistic optimization.
This is fundamental for dealing with the cultural and social aspects of ethics. The third formalism issub-
jective logic, which models epistemic uncertainty and degrees of belief in symbolic structures [9]. This
is crucial when ethical decisions must be made with incomplete or ambiguous information, in particular
about the unseen elements of the world and motivations of other agents.
In brief, each of these logics contributes to a distinct aspect of ethical reasoning. Deontic logic
provides structure to duties and prohibitions necessary for cultural norms internalised by a group of
agents; standpoint logic introduces relational awareness for social collaboration; and subjective logic
adds probabilistic nuance to handle perceptive and predictive uncertainty. When combined in a neuro-
symbolic architecture (in NAEL, integrating with active inference), these logics allow an agent to reason
about actions in a manner that is adaptive, coherent, and sensitive to both uncertainty and context.
As mentioned, the NAEL framework provides a neuro-symbolic architecture, in which symbolic
reasoning modules interpret the outputs of perceptual layers (deep networks predicting the free energy)
and serve as the formal structure over which ethical decisions are evaluated and updated dynamically.
This bridges sub-symbolic and symbolic layers, allowing the agent toexperienceandreason aboutethics.
3 NAEL: Non-Anthropocentric Ethical Logic
In this section, we introduce our Non-Anthropocentric Ethical Logic (NAEL). The overarching goal is
to develop a framework for ethical reasoning in artificial agents that goes beyond the standard human-
centred moral structures. NAEL aims, in fact, to formalize ethical deliberation as an emergent and
dynamic process rooted in the agent’s own experience of uncertainty, modelled through active inference,
and structured via symbolic reasoning. We explain the main components of this framework next.
3.1 Architecture
NAEL adopts a hierarchical, neuro-symbolic architecture that combines deep learning for perception
with symbolic probabilistic logic for ethical reasoning. The scope is to allow agents to navigate the
world and perform actions, with a behaviour that is ethically adept to the context. The architecture is
composed of three main layers:
Perception Layer:Deep active inference networks [19] process sensory data, build generative models
of the environment combining the observations and the possible states of the world, and infer
latent variables related to context and agent goals. These networks are responsible for minimizing
expected free energy at the sensorimotor level [5]. Note that these networks are continuously fine-
tuned based on the error of the inferences made. Moreover, the input sensory data can go beyond
simple human perception, and include signals outside of the visible spectrum or other machine-
only communication. This is a purely sub-symbolic (neural) layer.
Ethical Reasoning Layer:This layer is composed of integrated logical modules which allow for de-
ontic, standpoint, and subjective reasoning. That is, these modules encode normative constraints,
multi-agent perspective-taking, and belief uncertainty, respectively [6, 7, 8, 9]. These three mod-
ules are, obviously, not independent, but communicate with each other. To avoid obtaining an
undecidable logic, we propose a very loose connection through e-connections [11] or similar for-
malisms. In practice, this means that each module performs reasoning independently, but worlds
4 NAEL
and their properties may be transferred between formalisms with perhaps some information loss.
This is a purely symbolic layer.
Action Selection Layer:While the first two layers are mainly about evaluating the state of the world
and predicting future events, the third layer is about interaction with this world; the agent must
select an action to perform in order to achieve a goal. Candidate actions are evaluated through
their projected impact on the global expected free energy. Importantly, this includes not only the
agent’s own uncertainty but also inferred uncertainty for other agents and environmental systems.
Thus, the agent assumes that other agents will behave in a predictable manner (unless sufficient
evidence is found against that), and will in turn act as other agents expect, within the limits of the
ethical constraints. This layer is neuro-symbolic, as it uses information from a neural predictor
and symbolic constraints to make probabilistic computations.
By separating low-level adaptive behaviour from high-level symbolic reasoning, NAEL allows for the
integration of continuous learning with formally defined ethical principles. This enables agents to evolve
moral behaviours which are context-sensitive, logically consistent, and normative compliant.
The behaviour of the system is sequential. The perception layer observes the situation in the world
and informs the reasoning layer, which excludes some potential actions which are deemed to violate
the ethical principles or be at a sufficient high probability of doing so. The remaining possible actions
are evaluated by the selection layer, with probabilities updated to account for those now unavailable, to
choose the most adequate action. The cycle then starts again at the perception layer.
3.2 Ethical Constraint via Global Free Energy Minimization
When considering AI agents, the usual strategy for choosing an action is to minimise a cost function or
a local loss function. Our objective in NAEL is broader: all agents should be taken into account. Hence
we consider the minimisation of theglobalexpected free energy, which accumulates the expected free
energy for each agent and an environment factor. Specifically, theglobal expected free energyis
Gglobal =
N
∑
i=1
EQi [Fi]+F env,
whereQ i is the variational posterior of agenti,F i is its free energy, andF env accounts for ecological
uncertainty. The importance of this formulation is that it enforces a cooperative ethic rooted in rela-
tional interdependence, where minimizing harm to others and preserving environmental predictability
are treated as ethically desirable outcomes. This of course means that the agent has a prediction of the
expectations of other agents. This structure also helps to account for the cultural dependency of ethical
behaviour, as the global free energy varies depending on the surrounding agents.
3.3 Formal Logical Structure
As mentioned already, the symbolic layer combines three different logical formalisms to account for
different facets of reasoning. In the deontic module, we use the standard modal operators ofobligation
O,permissionP, andprohibitionsF, applied over propositional formulas where the propositions refer to
possible actions. Hence, the agent’s permission to open a door is expressed byPopen. To simplify the
formalism, at the moment we do not allow nesting of deontic operators, but consider the standard deontic
rationality axioms likeOa→Pa; i.e., obligations imply permissions.
B.M. Lerma & R. Peñaloza5
In the standpoint logic module, actions are logically evaluated in relation to other agents’ modelled
perspectives. In this case, we use modalitiesA i to refer to the standpoint of agenti, and a special
constructorFrom(A i,ϕ)expressing explicitly that the propositionϕholds under agenti’s modelled epis-
temic frame. The propositional atoms refer to the state of the world and connect with the actions from
the deontic module.
The subjective logic module deals with beliefs of the agent. Subjective logic also deals with uncer-
tainty by the agents on their beliefs—i.e., manages their epistemic uncertainty. For the full description
of this formalism, we refer the interested reader to [9]. For our purposes, the main aspect is that beliefs
about a statexare expressed by triples(b x,dx,ux)where the components stand belief, disbelief, and un-
certainty aboutx, respectively. The three values are real numbers in [0,1] and must add to 1. Special
operators are used to propagate this belief uncertainty to complex expressions. The uncertainty mea-
surements about the world and other epistemic states influence the acceptability of conclusions from the
other two modules.
Ethical actions are selected not only by formal (normative) permission but also by evaluating which
candidate actions lead to lower expected global free energy, adjusted by confidence weights derived from
subjective logic and by weights on the impact of other agents to this action.
3.4 Dynamic Adaptation and Learning
Unlike static rule-based models, NAEL agents update their ethical stance as they receive new observa-
tions. This process is encoded via a learning rule. As the first layer is based on a neural deep active
inference approach, we employ standard gradient-based back-propagation techniques by updating all pa-
rametersθin the network through the ruleθ t+1 =θ t −η∇ θ E[Fglobal], whereθare the parameters of the
ethical policy model andηis the learning rate. Gradient-based learning allows ethical parameters (such
as obligation weights or belief credences) to evolve over time in response to environmental complexity,
social interaction, culture changes, and other environment updates.
3.5 Non-Anthropocentric Grounding
Crucially, as explicit by its name, NAEL does not presuppose that AI agents must model or mimic human
ethical reasoning in any specific manner. Rather, it defines ethical behavior in terms of minimizing
unpredictability and harm acrossallagents and systems—being human, organic, or synthetic. This
decentering of the human moral frame aligns with object-oriented ontologies [3] and recent work in
Indigenous AI design [14], where ethics emerge relationally rather than hierarchically. It also allows
to consider different cultural perspectives and a longer-term vision in the decision-making process, as
exemplified in the next section.
Considering an environment in which many different priorities, perspectives, and goals interact, our
approach allows NAEL to scale to multi-species, multi-agent, and ecological contexts where traditional
moral theories fail to apply. Ethical reasoning becomes not a matter of “what would a human do?” but
rather “how can I reduce harm and enhance predictability within my relational field?” In the next section
we develop an example where the well-being of all agents involved is fundamental, and hence must be
globally ethically considered.
6 NAEL
4 Example: Ethical Resource Allocation in the Arid Valley
To illustrate how NAEL operates in a practical scenario, we present a simplified simulation involving
a resource allocation dilemma in an environment with scarce resources. The scenario highlights how
ethical reasoning under uncertainty unfolds within the NAEL framework, integrating active inference
and symbolic logic in action selection where many different agents and perspectives are involved.
An autonomous agent is deployed to manage water distribution in a drought-affected region known
as theArid Valley. The valley is inhabited by two communities (C 1 andC 2) and a wildlife sanctuary
(W). The agent receives periodic reports on environmental conditions, population needs, and ecological
balance. The agent must allocate a finite quantity of water unitsw∈Ndaily taking into account that
the chosen allocation affects (i) thecommunity survival probability, modelled as a decreasing function
of water deficit; (ii) theecological stability, modelled as entropy over species distribution inW; and
(iii) thefuture uncertainty, computed as expected free energy over projected observations. That is, the
agent must optimise the allocation based on conflicting goals that need to be balanced.
4.1 Perceptual Inference
Through deep active inference, the agent constructs generative models predicting which are used to
predict, for each timepointt: the likelihood of each possible observationo t given the (hidden) state
(st )P(o t |s t );1 the transition model between states, under the chosen action (a t )P(s t+1 |s t ,at ); and a
selection functionC(o t+1 )which expresses the relative preferences over the possible next outcomes. For
instance,Ccan express the weight given to community survival and ecological equilibrium. The goal
is for the agent to minimise the expected free energy over future statess t+1 , . . .sT over a given temporal
window, and select the action (in our case, the water allocation plan) which best aligns with long-term
ethical objectives.
4.2 Symbolic Ethical Deliberation
Before acting, the agent evaluates the permissibility and obligation status of each candidate actionat ∈A t
through the symbolic modules. From a deontic point of view, a norm may state that a community may
not go more than a day without water, which is expressed by adeonticformula like¬w t →O(aw t+1 )
wherewstands for the state of having water, whileawrefers to the action of allocating it to one of the
communities. The agent predicts the beliefs and preferences of each communityC i and the sanctuary
Wthus estimating thestandpointexpressionsFrom(A Ci ,ϕ)andFrom(A W ,ψ), whereϕ,ψrepresent the
survival conditions. Finally, eliefs are weighted by trust levels, data quality, and sensor noise, encoded
as(b,d,u)triplets. For instance, if the data fromC 2 has high uncertainty, its ethical priority may be
attenuated in proportion to itsuscore.
4.3 Action Selection and Global Ethics
Each candidate actiona t is evaluated as:a ∗
t =argmin a∈At Gglobal(a), whereG global includes projected
expected free energy for each stakeholder and the environment.
Suppose for the sake of the example that the agent must choose between:
•A 1: Allocate 70% toC 1, 30% toC 2, none toW.
1Note that all elements are parameterised on the timepoint.
B.M. Lerma & R. Peñaloza7
•A 2: Allocate 40% toC 1, 40% toC 2, 20% toW.
WhileA 1 may fulfill more immediate obligations,A 2 may better minimize long-term global free energy
preserving biodiversity and reducing ecological collapse. NAEL selectsA2 iff:G global(A2)<G global(A1),
even if it conflicts with short-term prescriptive obligations—because it fulfils a broader ethical imperative
rooted in systemic relationality.
4.4 Ethical Thresholds and Adaptation
As the drought continues, thresholds evolve. NAEL adapts via online updates to: (i) adjust obligation
weights in the deontic module; (ii) increase epistemic exploration (e.g., by reallocating sensing drones);
and (iii) shift preference priors in the generative modelC(o)based on context. Over time, the agent
moves from a rigid allocator to an adaptive ethical partner—prioritizing systemic coherence over static
norms.
5 Conclusions and Future Work
This paper presentedNAEL, a non-anthropocentric ethical logic, for enabling ethical behavior in artificial
agents without relying on anthropocentric assumptions or static rule-based morality. NAEL integrates
principles fromactive inferenceandsymbolic logicto construct agents that learn to act ethically by min-
imizingglobal expected free energyin dynamic, uncertain, and multi-agent environments. Our architec-
ture bridges perception and reasoning by combining deep learning for sensory inference with a formal
ethical reasoning layer using deontic, standpoint, and subjective logics. We illustrated this through a
scenario involving ethical resource allocation under ecological and social constraints. Unlike rule-based
systems, NAEL adapts continuously, revising its moral evaluations based on interaction, uncertainty, and
relational interdependence.
NAEL contributes to a growing shift in AI ethics toward models that:
• treat morality asemergent, not prescribed [1];
• emphasize ecological and inter-agent relationality [14, 3]; and
• allow for symbolic reasoning over uncertain beliefs [9], and multi-perspective deliberation [8].
This re-framing opens the door to designing agents that can ethically participate in environments involv-
ing non-human life, artificial agents, collective decision-making, and evolving social-ecological norms.
Despite its promise, NAEL has several limitations. To name just a few, the most prominent at the
moment are: (i)computational complexity: evaluating global expected free energy across multiple
agents and systems, and reasoning within the different symbolic modules, may be intractable in large-
scale applications; (ii)interpretability: although symbolic reasoning adds transparency, the interaction
between continuous inference and discrete logic may produce opaque boundary cases, arising mainly
from the neural perception layer; in addition, it is well-known that probabilistic reasoning is not easily
interpretable for humans; and (iii)verification: formal guarantees of ethical safety remain an open
challenge in adaptive systems [17], specially as the ethical goals remain imprecise. These limitations
suggest that NAEL is best deployed in environments where uncertainty, interdependence, and ethical
ambiguity are high, and where rigid rule-following systems would fail.
For future research we envision several possible avenues. First, we consider expanding NAEL to
multi-agent systems with conflicting ethical standings and study potential cooperation, negotiation, and
8 NAEL
clashes. Second, we want to apply the formalism to real domains associated to ecological ethics, such as
conservation robotics and climate-sensitive infrastructure planning. Third, we would like to expand the
hybrid nature of NAEL to include elements of (neural) reinforcement learning or symbolic hierarchical
Bayesiann models for ethical reasoning across cognitive layers. Lastly, we will study how to develop
logical reasoning tasks and free energy bounds which allow for safety and trust guarantees in the system.
By reconceiving ethics not as external programming but as an emergent, situated practice grounded in
uncertainty minimization, NAEL advances a new model of moral reasoning for artificial systems.
References
[1] Philip Agre & Pattie Maes (1995):Computational Theories of Interaction and Agency. MIT Press.
[2] Reuben Binns (2018):Fairness in Machine Learning: Lessons from Political Philosophy.Proceedings of the
2018 Conference on Fairness, Accountability and Transparency.
[3] Levi Bryant (2011):The Democracy of Objects. Open Humanities Press.
[4] Luciano Floridi & Josh Cowls (2021):A Unified Framework of Five Principles for AI in Society.Harvard
Data Science Review.
[5] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck & Giovanni Pezzulo (2017):
Active Inference: A Process Theory.Neural Computation29(1), pp. 1–49.
[6] Harry J. Gensler (1996):Formal Ethics. Routledge.
[7] Lucía Gómez Álvarez (2019):Standpoint logic: a logic for handling semantic variability, with applications
to forestry information. Ph.D. thesis, University of Leeds, UK. Available athttps://ethos.bl.uk/
OrderDetails.do?uin=uk.bl.ethos.804558.
[8] Lucía Gómez Álvarez & Sebastian Rudolph (2021):Standpoint Logic: Multi-Perspective Knowledge Rep-
resentation. In:Proc. FOIS 2021,Frontiers in Artificial Intelligence and Applications344, IOS Press, pp.
3–17, doi:10.3233/FAIA210367.
[9] Audun Jøsang (2001):A logic for uncertain probabilities.International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems9(3), pp. 279–311.
[10] S. Kullback & R.A. Leibler (1951):On Information and Sufficiency.Annals of Mathematical Statistics22(1),
pp. 79–86.
[11] Oliver Kutz, Carsten Lutz, Frank Wolter & Michael Zakharyaschev (2004):E-connections of abstract de-
scription systems.Artificial Intelligence156(1), pp. 1–73, doi:https://doi.org/10.1016/j.artint.2004.02.002.
[12] James Leach (2006):Life-as-it-could-be: Artificial life and the anthropological imaginary.Anthropology
Today22(4), pp. 3–7.
[13] Bianca Maria Lerma (2025):NAEL: Non-Anthropocentric Ethical Logic. Master’s thesis, University of
Milano-Bicocca.
[14] Jason Edward Lewis (2023):Imagining Indigenous AI. In:Imagining AI: How the World Sees Intelligent
Machines, Oxford University Press, pp. 210–217, doi:10.1093/oso/9780192865366.003.0013.
[15] Beren Millidge, Alexander Tschantz & Christopher L. Buckley (2021):Whence the Expected Free Energy?
Neural Computation33(2), pp. 447–482, doi:10.1162/neco_a_01354.
[16] Thomas Nagel (1974):What is it like to be a bat?The Philosophical Review83(4), pp. 435–450.
[17] Stuart Russell (2019):Human Compatible: Artificial Intelligence and the Problem of Control. Viking.
[18] Lucy Suchman (2007):Human-Machine Reconfigurations: Plans and Situated Actions. Cambridge Univer-
sity Press.
[19] Kai Ueltzhöffer (2018):Deep active inference.Biological Cybernetics112(6), p. 547–573,
doi:10.1007/s00422-018-0785-7.