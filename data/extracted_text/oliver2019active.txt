Active inference body perception and action
for humanoid robots
Guillermo Oliver1, Pablo Lanillos1,2, Gordon Cheng1
Abstract—Providing artificial agents with the same computa- Body perception
tional models of biological systems is a way to understand how
intelligentbehavioursmayemerge.Wepresentanactiveinference
body perception and action model working for the first time in
ahumanoidrobot.Themodelreliesonthefreeenergyprinciple
proposedforthebrain,wherebothperceptionandactiongoalis
tominimisethepredictionerrorthroughgradientdescentonthe
variational free energy bound. The body state (latent variable)
is inferred by minimising the difference between the observed
(visualandproprioceptive)sensorvaluesandthepredictedones.
Simultaneously,theactionmakessensorydatasamplingtobetter
correspond to the prediction made by the inner model. We Action
formalised and implemented the algorithm on the iCub robot
and tested in 2D and 3D visual spaces for online adaptation
to visual changes, sensory noise and discrepancies between the Fig. 1. Body perception and action via active inference. The robot infers
model and the real robot. We also compared our approach with its body (e.g., joint angles) by minimising the prediction error: discrepancy
classical inverse kinematics in a reaching task, analysing the betweenthesensors(visualsvandjointsp)andtheirexpectedvalues(gv(µ)
suitability of such a neuroscience-inspired approach for real- andgp(µ)).Inthepresenceoferroritchangestheperceptionofitsbodyµ
andgeneratesanactionatoreducethisdiscrepancy.Botharecomputedby
world interaction. The algorithm gave the robot adaptive body
optimisingthefreeenergyboundF.Inareachingtask,theobjectisacausal
perception and upper body reaching with head object tracking
variableρthatactsasaperceptualattractor,producinganerrorinthedesired
(toddler-like), and was able to incorporate visual features online
sensorystateandpromotinganactiontowardsthegoal.Theequilibriumpoint
(in a closed-loop manner) without increasing the computational µ˙ =0appearswhenthehandreachestheobject.Theheadkeepstheobject
complexity. Moreover, our model predicted involuntary actions initsvisualfield,improvingthereachingperformance.
in the presence of sensorimotor conflicts showing the path for a
potential proof of active inference in humans.
Index Terms—Active inference, free energy optimization, Bio- This principle accounts for perception, action and learning
inspired Perception, Predictive coding, Humanoid robots, iCub. through the minimization of surprise, i.e., the discrepancy
between the current state and the predicted or desired one.
Accordingtothisapproach,freeenergyisawayofquantifying
I. INTRODUCTION
surpriseanditcanbeoptimisedbychangingthecurrentbeliefs
The medical doctor and physicist Hermann von Helmholtz (perception)orbyactingontheenvironment(action)toadjust
describedvisualperceptionasanunconsciousmechanismthat the difference between reality and prediction [7]. Active in-
infers the world [1]. In other words, the brain has generative ference would allow living beings to infer and simultaneously
models that reconstruct the world from partial information. adapttheirbodytouncertainenvironments[8],somethingjust
Nowadays, there is a scientific mainstream that describes the partially solved in robotics.
inner workings of the brain as those of a Bayesian inference
Hence, inspired by this principle, we present a robust
machine[2],[3].Thisapproachsupportsthatwecanadjustthe
and adaptive body perception and action mathematical model
cues (visual, proprioceptive, tactile, etc.) contribution to our
based on active inference, for a real humanoid robot (Fig. 1).
interpretation in a Bayesian optimal way considering sensors
Our approach describes artificial body perception and action,
and motor uncertainties [4]. This implies that the brain is
similartobiologicalsystems,asaflexibleanddynamicprocess
able to encode uncertainty not only for perception but also
that approximates the robot body latent state using the error
for acting in the world.
between the expected and the observed sensory information.
Optimal feedback control was proposed for modelling mo-
The proposed model enabled the robot to have adaptive body
tor coordination under uncertainty [5]. Alternatively, active
perception and to perform upper-body reaching behaviours
inference [6], defended that both perception and action are
even under high levels of sensor noise and discrepancies
two sides of the same process: the free energy principle.
between the model and the real robot. We further compared
1Institute for Cognitive Systems, Technical University of Munich, Arcis- our approach to standard inverse kinematics and analysed the
strasse21,80333Munich,Germany. relevanceandthechallengesofpredictivecodinginterpretation
2DondersInstituteforBrain,CognitionandBehaviour,theNetherlands. solved through variational free energy methods for humanoid
ThisworkhasbeensupportedbySELFCEPTIONprojectEUHorizon2020
robotics. For reproducibility, the code, with the parameters
Programme,grantagreementn.741941andtheEU’sErasmus+Programme.
Supplementaryvideo:https://youtu.be/rdbbmwo4TY4,code:tobereleased. used, is publicly available: tobereleased.
0202
naJ
92
]OR.sc[
3v22030.6091:viXra
A. Related work II. FREEENERGYOPTIMIZATION
Multisensory body perception has been widely studied in A. Bayesian inference
the literature, from the engineering point of view, and enables
AccordingtotheBayesianinferencemodelforthebrain,the
robots to estimate its body state by combining joint infor-
body configuration x is inferred using available sensory data
mation with other sensors such as images and tactile cues.
s by applying Bayes’ theorem: p(x|s) = p(s|x)p(x)/p(s).
Bayesian estimation (e.g., particle filters) has been proved to
Where this posterior probability, corresponding to the prob-
achieve robust and accurate model-based robot arm tracking
ability of body configuration x given the observed data s, is
[9] even under occlusion [10], or for simultaneous joint offset
obtainedasaconsequenceofthreeantecedents:(1)likelihood,
parameters adaptation and hand pose estimation [11]. Fur-
p(s|x), or compatibility of the observed data s with the
thermore, integrated visuomotor processes enabled humanoid
body configuration x, (2) prior probability, p(x), or current
robots to learn motor representations for robust reaching [12],
belief about the configuration before receiving the sensory
[13],visuotactilemotorrepresentationsforreachingandavoid-
datas,and(3)marginallikelihood,p(s),anormalizationterm
ancebehaviours[14]orenablingvisualrobotbodydistinction
which corresponds to the marginalization of the likelihood of
foraidingthediscoveringofmovableobjects[15].Freeenergy
receiving sensory data s regardless of the configuration.
optimization for multisensory perception in a real robot was
The goal is to find the value of x which maximises p(x|s),
firstly presented in [16] working as an approximate Bayesian
because it is the most likely value for the real-world body
filter estimation. The robot was able to perceive its arm
configuration x taking into account the sensory data obtained
location fusing visual, proprioceptive and tactile information.
s.Thisdirectmethodbecomesintractableforlargestatespaces
Active inference (under the free energy principle) man-
[20]. In this case, marginalization over all the possible body
aged to include the action within the multisensory perception
and world states explodes computationally.
process, explained as a classical spinal reflex arc pathway
triggered by prediction errors. It has been mainly studied in
B. Free energy principle with the Laplace approximation
theoreticalorsimulatedconditions.Fristonetal.[7]presented
this neuroscience-inspired model as a generalization of the Thefreeenergyprinciple[6]providesatractablesolutionto
dynamic expectation-maximization algorithm. Recently, a few thisobstacle,where,insteadofcalculatingthemarginallikeli-
works analysed this approach for robotics in simulated condi- hood, the idea is to minimise the Kullback-Leibler divergence
tions: one degree of freedom vehicle [17], a PR2 robot arm [21]betweenareferencedistributionq(x)andtherealp(x|s),
[18], and two degrees of freedom robot arm with generative inorderforthefirsttobeagoodapproximationofthesecond.
functionlearning[19].However,thesestudiesdidnotvalidate (cid:90) q(x)
the method on real robots bypassing the true difficulties, such D (q(x)||p(x|s))= q(x)ln dx
KL p(x|s)
as non-linear dynamics due to friction, backlash, unmodelled
(cid:90) q(x)
physical constraints, uncertainties in the dynamic model, sen-
= q(x)ln dx+lnp(s)=F +lnp(s)≥0 (1)
sor and model errors, etc. p(s,x)
Inthiswork,weprovidetheactiveinferenceperceptionand
Minimising the first term, F, effectively minimises the
action mathematical construct for the humanoid robot iCub
differencebetweenthesetwodensities,withonlythemarginal
upper-body working in different real setups. Thus, we show
likelihood remaining. Unlike the whole expression of the KL-
the plausibility of real implementations of the free energy
divergence, the first term can be evaluated because it depends
principle in artificial agents, analysing its challenges and
on the reference distribution and the knowledge about the en-
limitations.Furthermore,weintroduceanovelwaytocompute
vironmentwecanassumetheagenthas,p(s,x)=p(s|x)p(x).
the mapping from action to sensory consequences (partial
This term is defined as variational free energy [22].
derivatives of the sensory input with respect to the action)
According to the free energy optimization theory, there
exploiting the properties of velocity control, and simplifying
are two ways to minimise surprise, which accounts for the
the calculus of the action within the free energy optimization
discrepancy between the current state and the predicted or
framework.
desired one (prediction error): changing the belief (perceptual
inference)oractingontheworld(activeinference).Perceptual
B. Paper organization
inferenceandactiveinferenceoptimisethevalueofvariational
First, in Sec. II we explain the general mathematical free free energy, while active inference also optimises the value
energy optimization for perception and action. Afterwards, of the marginal likelihood by acting on the environment and
Sec. III describes the iCub physical model and in Sec. IV changing the sensory data s.
and V we detail the active inference computational model Under the Laplace approximation instead of computing the
applied for object reaching and head object tracking. Finally, wholedistributionwecantrackthemode[23]andsimplifythe
Sec. VI shows the results for several experiments and Sec. calculus of F. Thus, the variational density q(x) is assumed
VII analyses the relevance of implementing active inference to have Gaussian form N(x|µ,Σ). Defining the Laplace-
body perception in humanoids, for both human and robotics encoded energy [22] as L(s,x) = −lnp(s,x), F can be
science. approximated to:
(cid:20) (cid:21)
1
F ≈L(s,µ)− ln|Σ|+nln2π (2)
2
where µ conceptually describes the internal belief or state, s
is the sensory input and n is the size of µ vector.
C. Perceptual and active inference
Perceptual inference is the process of updating the inner
model belief to best account for sensory data, minimising
the prediction error. For instance for body perception, the
agent must infer themost-likely oroptimal valuefor thebody
configuration or state µ. This optimal value is the one that
minimises free energy. To solve it, we can compute it by
gradientdescent overthe freeenergy term.For staticsystems,
this update is performed directly as [16]: µ˙ = −∂F. In
∂µ
dynamic systems, time derivatives should be considered, the
state variable is now a vector µ:
∂F
µ˙ =Dµ− (3)
∂µ
wheretheDistheblock-matrixderivativeoperator.Whenfree
energyisminimised,thevalueofitsderivativeis ∂F =0,and ∂µ
the system is at equilibrium.
Activeinference[7],istheextensionofperceptualinference
to the relationship between sensors and actions, taking into
account that actions can change the world to make sensory
data more accurate with predictions made by the inner model.
The action plays a core role in the optimization and improves
the approximation of the real distribution, therefore reducing
the prediction error by minimising free energy. It also acts
on the marginal likelihood by changing the real configuration
which modifies the sensory data s to obtain new data that is
moreinconcordancewiththeagent’sbelief.Theoptimalvalue
is the one which minimises free energy, and again a gradient
descent approach will be taken to update its value:
∂F
a˙ =− (4)
∂a
III. ROBOTPHYSICALMODEL
q
μ ae3
qe3
v a q e e1 1 q
2
ae2 qe2
v L
ρ vL
R
ρ R Left ρ
a 2 Right
q 1 q 3 v
a a
1 3
μ'
q a 4 4
tnemnorivnE
The robot iCub [24] (v1.4) is a humanoid powered by
electricmotorsanddrivenbytendons.Itisdividedintoseveral
kinematicchainsdefinedthroughhomogeneoustransformation
matricesusingDenavit-Hartenbergconvention.Wefocusedon
two kinematic chains, those with the end-effector being the
hands (without fingers), the head, the eyes and the torso.
As depicted in Fig. 2, each arm model is defined as a
three degree of freedom system: shoulder roll, shoulder yaw
and r elbow. To provide improved reaching capabilities, an
additional degree of freedom is considered for the torso:
torso yaw.Leftandrighteyecamerasobservetheend-effector
position and the world around it, providing stereo vision. The
joints considered for the head are: neck pitch and neck yaw.
Moreover, vertical synchronised motion is performed in both
eyes using: eyes tilt.
IV. ACTIVEINFERENCECOMPUTATIONALMODELFOR
ICUBREACHINGTASK
A. Problem formulation
The body configuration, or internal variables, is defined as
the joint angles g (µ)=µ. The estimated states µ∈IR4 are
p
the belief the agent has about the joint angle position and the
action a ∈ IR4 is the angular velocity of those same joints.
Due to the fact that we use a velocity control for the joints,
first-order dynamics must also be considered µ(cid:48) ∈IR4.
Sensorydatawillbeobtainedthroughseveralinputsensors.
Using binocular disparity between the images obtained from
both eyes, stereo vision reconstruction is performed to obtain
the3Dpositionoftheend-effector,s ∈IR3.Alljointmotors
v
have encoders which provide joint angle position, s ∈IR4.
p
 q 1   v 
q 1
s p =  q 2   s v = v 2  (5)
3 v q 3
4
The likelihood p(s|µ) is made up of proprioception func-
tions in terms of the current body configuration, while the
prior p(µ) will take into account the dynamic model of the
agent that describes how this internal state changes with time.
AdaptingtheLaplace-encodedenergyforourrobotmodeland
assuming independence between different modalities sensory
readings:
3D
ρ v lnp(s,ρ,µ)=lnp(s p |µ)p(s v |µ)p(µ(cid:48)|µ,ρ) (6)
B. Free energy optimization
In order to define the conditional densities for each of the terms, we should define the expressions for the sensory data.
ρ Joint angle position, s p , is obtained directly from the joint
anglesensors.Letusassumethattheinputisnoisyandfollows
a normal distribution with mean at the internal value µ and
a z varianceΣ sp .End-effector2Dor3Dposition,s v ,isdefinedby
x y anon-linearfunctiondependentonthebodyconfigurationand
obtainedusingtheforwardmodelofthearm.Again,theinput
Fig. 2. Model description. (Left) Generative model of the robot. (Right)
is noisy and follows a normal distribution with mean at the
Notation and robot configuration. Shown variables: internal variables µ and
µ(cid:48),jointanglepositionq,jointactionsa,and3Dlocationoftheend-effector value of this function g v (µ)∈IRnv and variance Σ sv , where
v andcausalvariablesρalongwiththeirprojectionsin2Dvisualspace. n is the visual dimensions. The dynamic model for n latent
v p
variables (joints angles) is determined by a function which transforms the attractor vector from target space (3D) to joint
depends on both the current state µ and the causal variables space.Thesystemisvelocitycontrolled,thereforetargetspace
ρ (e.g. 3D position of the object to be reached), with a noisy is a linear velocity vector and joint space is angular velocity.
input following a normal distribution with mean at the value When the Jacobian matrix is rectangular (e.g., 3D space
of this function f(µ,ρ)∈IRnp and variance Σ
sµ
. Therefore, and 4 DOF arm joint space), we used the generalised in-
likelihood functions in Eq. (6) are defined as: verse(Moore-Penrosepseudoinverse)asthemappingfunction:
T(µ)=J+(µ). This matrix is calculated using the singular-
(cid:89)
np
(cid:89)
nv value decomposition (SVD), where J+ =VΣ+UT.
p(s |µ)= N(q |µ ,Σ ) p(s |µ)= N(v |g (µ),Σ )
p i i p v i vi v
D. Active inference
i=1 i=1
p(µ(cid:48)|µ,ρ)= (cid:89) np N(µ(cid:48)|f (µ,ρ),Σ ) (7) Action is set to be an angular velocity magnitude, which
i i sµ corresponds with angular joint velocity in the latent space. In
i=1
order to compute Eq. (9) we need the mapping between the
The optimization of the Laplace-encoded energy is per-
sensor values and the velocities of the joints, expressed in the
formed using gradient descent (Eq. (3) and (4)). The depen- partial derivatives ∂sp and ∂sv.
dency of F with respect to the vector of internal variables ∂a ∂a
We assume that the control action, a, is updated for every
µ can be calculated using the chain rule on the functions that
cycle, and therefore for each interval of time between cycles
dependonthoseinternalvariables.ThedependencyofF with
it has the same value. For each period (cycle time between
respecttothevectorofactionsaiscalculatedconsideringthat
updates), the equation of uniform circular motion is satisfied
the only magnitudes directly affected by action are the values
for each joint position. If this equation is discretised, for each
obtained from the sensors: ∂F = ∂s∂F - see Sec. IV-D.
∂a ∂a ∂s sampling moment, which are T seconds apart, the joint value
will be updated in the following way: q = q + a T.
i+1 i i
− ∂F = 1 (s −µ)+ 1 ∂g v (µ)T (s −g(µ)) Thedependencyofthejointanglepositionwithrespecttothe
∂µ Σ sp p Σ sv ∂µ v control action is therefore defined.
1 ∂f(µ,ρ)T The partial derivatives of joint position s with respect
+ (µ(cid:48)−f(µ,ρ)) (8) p
Σ ∂µ to action, considering there is no cross-influence or coupling
sµ
− ∂F =− (cid:18) 1 ∂s p T (s −µ)+ 1 ∂s v T (s −g (µ)) (cid:19) betweenjointvelocitiesandthatq i anditsexpectedµ i should
∂a Σ ∂a p Σ ∂a v v convergeatequilibrium,aregivenbythefollowingexpression:
sp sv
(9)
(cid:40)
∂q ∂µ T i=j,
Theagentalsoinfersthefirst-orderdynamicsofthebodyµ(cid:48) i = i = (13)
and updates their values using a gradient descent formulation.
∂a
j
∂a
j
0 otherwise.
The dependency of F with respect to µ(cid:48) is limited to the If the dependency of joint position with respect to action is
influence of the dynamic model. known, we can use the chain rule to calculate the dependency
for the visual sensor s . Considering that the values of
v
− ∂ ∂ µ F (cid:48) =− Σ 1 sµ (µ(cid:48)−f(µ,ρ))= Σ 1 sµ (f(µ,ρ)−µ(cid:48)) (10) g d v e i r ( iv µ a ) tiv s e h s ou a l r d e a g l i s v o en co b n y ve th rg e e fo to llo v w i in at g e e q x u p i r l e ib ss ri i u o m n: , the partial
Finally, the differential equations for µ, µ(cid:48) and a are:
∂v ∂v ∂q ∂g (µ)∂µ
i = i j = vi j (14)
∂a ∂q ∂a ∂µ ∂a
j j j j j
∂F ∂F ∂F
µ˙ =µ(cid:48)−
∂µ
µ˙(cid:48) =−
∂µ(cid:48)
a˙ =−
∂a
(11) V. ACTIVEINFERENCECOMPUTATIONALMODELFOR
ICUBHEADOBJECTTRACKINGTASK
A first-order Euler integration method is applied to update the
We extend the arm reaching model for the head to obtain
valuesofµ,µ(cid:48)andaineachiteration.E.g.,µ =µ +µ˙∆t,
i+1 i an object tracking motion behaviour. The goal of this task is
where∆t=T istheperiodofexecutionoftheupdatingcycle.
to maintain the object in the centre of the visual field, thus
C. Perceptual attractor dynamics increasing its reaching working range capabilities.
Sensory data and proprioception for the head is defined by
The reaching goal is defined in the dynamics of the model
internal variables beliefs µ ∈ IR3, actions a ∈ IR3, and
byintroducingaperceptualattractorρinthe2Dor3Dspace: e e
first-order dynamics µ(cid:48) ∈ IR3 and because the end-effector
e
 ρ   g (µ)  are the eyes, there is only joint angle position, s ∈IR3.
1 v1 e
A(µ,ρ)=ρ 4 ρ 2 − g v2 (µ)  (12) Toobtainthedesiredmotion,anattractorisdefinedtowards
ρ g (µ) the centre of the left eye image (c ,c ). Attractor position
3 v3 x y
(ρ ,ρ ) is read from the projected 2D visual input of the left
where ρ defines the object location in the visual space 5 6
1:3 eye and dynamically updates with head motion.
and ρ describes the gain of the attractor.
4
Internal variable dynamics are then defined in terms of the (cid:18)(cid:20) c (cid:21) (cid:20) ρ (cid:21)(cid:19)
A (µ ,ρ)=ρ x − 5 (15)
attractor: f(µ,ρ)=T(µ)A(µ,ρ). T(µ) is the function that e e 4 c ρ
y 6
)slexip(1vetanidrooclausiV
100
eevi nns ccio oon ddeerrss +vision 4
125
150
175 0.14 0.12 0. y 10 0.08 0.06 -0.30 0.14 0.12 0. y 10 0.08 0.06 -0.30
200 3 -0.32 -0.32
-0.34x -0.34x
1
225 -0.36 -0.36
2 0.10 0.10
100 150 Visualcoor 20 d 0 inatev2(pixels) 250 300 z0.05 0.00 vca iat s tlr uca auc lltao str veρdgv(μ) z0.05 0.00 vca iat s tlr uca auc lltao str veρdgv(μ)
(a) Sensoryfusionpath. (b) Visualmarkerdeviationtotheleft. (c) Visualmarkerdeviationtotheright.
)slexip(1vetanidrooclausiV
100
125
N xxx o === n 000 o ,,, i σσσ se === 124 000 °°° 4
150
175 y y
0.20 0.15 0.10 0.05 0.00-0.25 0.20 0.15 0.10 0.05 0.00-0.20
200 3 -0.30 vca iat s tlr uca auc lltao str veρdgv(μ) -0.25
x x
1 -0.35 -0.30
225
2 0.10 -0.40 0.20 -0.35
100 150 Visualcoor 20 d 0 inatev2(pixels) 250 300 0.0 z 5 0.0-00.0-50.10 vca iat s tlr uca auc lltao str veρdgv(μ) 0.1 z 5 0.100.050.00
(d) Encodernoisehandlingpath. (e) Visualmarkerlocatedonfinger. (f) Visualmarkerlocatedonforearm.
Fig. 3. (Left) End-effector averaged paths in the right arm object reaching for (a) sensory fusion and (d) under noisy encoder sensors. Attractor position
isrepresented bya bluedisk. (Right)Results ofadaptive behaviourusing sensoryfusion. Ifthe visualmarker positionis modified,the algorithmproduces
a reaching action that tries to approach this new position to the attractor location. On the top right of each image the left eye camera is shown. Attractor
positionisshowninblue,visualperceptioninredandcalculatedpositioningreen.Theresultingdirectionofmotionobtainedfromthealgorithmisshown
asanarrow.
Internal variable dynamics are then defined in terms of the factor to adjust the power of the attractor. The right arm end-
attractor as: f (µ ,ρ) = T (µ )A (µ ,ρ), f (µ ,ρ) ∈ effector has a visual marker and its 3D position is obtained as
e e e e e e e e
IR3.Withtwopixelcoordinatesandthreedegreesoffreedom, thevaluesofv ,v andv ,alongwithitsprojectionsincamera
1 2 3
the pseudoinverse of the visual Jacobian matrix is used as the space. The relevant parameters of the algorithm are: encoder
mapping matrix in the visual space: T (µ )=J+(µ ). sensor variance Σ , visual perception variance Σ , attractor
e e v e sp sv
dynamicsvarianceΣ andactiongainsk .Theseparameters
VI. RESULTS
were tuned empirical
s
l
µ
y with their physica
a
l meaning in mind
We evaluated the proposed approach for humanoid upper- and remained constant during the experiments unless stated
body reaching in 2D and 3D visual space scenarios. The pro- otherwise. Their values can be found in the code repository.
posed algorithm, in order to reduce the discrepancy between
A. Sensory fusion and robustness under noisy sensors
the current sensory input and the desired state, produces a
reaching behaviour with the arm and torso of the robot while The first experiment was designed to test sensory fusion
the head tracks the object to maintain it in its visual field. (Fig. 3(a)) and the impact of noisy sensors on the algorithm
Several experiments were performed: (1) proof of concept, (Fig.3(d)).Therobotworkedin2Dvisualspaceusingtheleft-
right arm reaching towards a series of 2D locations in the eyecamera(monocularvision)andthetorsomotionislocked.
visual plane with different levels of noise and sensor sources; The robot had to reach 10 times four different static locations
(2) adaptation, the robot adapts its reaching behaviour during in the visual field with the right arm. A location is considered
changesonthevisualfeaturelocation;(3)comparison,motion to be reached when the visual position is inside a disk with a
from the active inference algorithm is compared to inverse radius of five pixels centred at the location position.
kinematics; and (4) dynamics evaluation, body perception Figure 3(a) shows the averaged trajectory of the visual
and action variables are analysed during an arm reaching marker with different sensor modalities (considering only
with active head towards a moving object. Video footage intrinsicnoisefromthesensorsandmodelerrors):visual,joint
of the experiments and some extras can be found in the angleencoders,andbothtogether.Eventhoughthemodelhas
supplementary video: https://youtu.be/rdbbmwo4TY4. beenverifiedandthecameracalibrated,therewasadifference
The iCub robot is placed in a controlled environment and betweentheforwardmodelandtherealrobot,duetopossible
an easily recognizable object is used as a perceptual attractor deviations in parameters and motor backslash, which implies
toproducethemovementoftherobot.In3Dspace,thevalues that the robot had to use visual information to correct its real
ofthecausalvariablesρ ,ρ ,ρ arethecoordinatesofthe3D position. Employing joint angle encoders and vision provided
1 2 3
positionoftheobjectobtainedusingstereovision,whilein2D the best behaviour for the reaching of the fixed locations in
space, the values used for the attractor in (12) are the object the visual field, achieving all positions in the shortest fashion.
pixel location at the left eye. The value of ρ is a weighting Visualperceptionreachedallthepositionsbutitdidnotfollow
4
)m(rorreSMRnoitisoP
0.12 aincvteivreseinkfeinreemncaetics
0.10
0.08
0.06
0.04
0.02
0.000 20 40 60 80
Time(s)
(a) 3Dpositionerror.Activeinference(orange)
andinversekinematics(blue).
)snaidar(noitisopralugnA
1.5 q μ11 μ q22 μ q33 μ q44
1.0
0.5
0.0
-0.5
0 20 40 60 80
Time(s)
(b) Rightarminternalstateµandencoderssp
inactiveinferencealgorithm.
)snaidar(noitisopralugnA
1.5 q1 q2 q3 q4
1.0
0.5
0.0
-0.5
0 20 40 60 80
Time(s)
(c) Rightarmencoderssp ininversekinematics
algorithm.
y y
0.3 0.2 0.1 0.0 -0.1 -0.2 0.3 0.2 0.1 0.0 -0.1 -0.2
-0.3 -0.3
x x
-0.4 -0.4
0.3 -0.5 0.3 -0.5
0.2 0.2
z 0.1 z 0.1
0.0 v c i a s l u c a u l la s tvedg(μ) 0.0 v c i a s l u c a u l la s tvedg(μ)
attractorρ attractorρ
-0.1 -0.1
(d) Experimentdescription. (e) 3Dpathsinactiveinferencealgorithm. (f) 3Dpathsininversekinematics.
Fig. 4. Comparison between active inference and inverse kinematics. (Top left) RMS 3D position error between the real visual position and the attractor
position.(Topcentreandright)Internalstateandencodervaluesoftherightarmforeachofthealgorithms.(Bottomleft)Experimentdescriptionshowing
theinitialarmpositionandtherectangularprism.(Bottomcentreandright)3Dpathstakenforeachofthealgorithms.
the optimum path while using only the encoder values failed Similar behaviour was obtained when the marker was located
to reach all locations. on one of the fingers (Fig. 3(e)) and on the forearm (Fig.
Secondly, in order to test the robustness against sensory 3(f)). In this case, the visual discrepancy with the forward
noise,weinjectednoiseintherobotmotorsencoderss .Four model is much more noticeable and the algorithm is not able
p
conditions were tested: Gaussian noise with a zero mean and offullyreachingthevisualmarkerpositiontotheattractor,but
with standard deviation of 0° (control test), 10°, 20° and 40°. the same opposite action behaviour as in the previous case is
Again, averaged path is shown (Fig. 3(d)). Trajectories with obtained.
no noise and σ = 10° achieved very similar results, with the Thisexperimentshowsthatinthecaseofvisualsensorimo-
first one achieving the objectives slightly faster. Extreme case tor conflict a perception body alteration and action to reduce
withσ =40°causedoscillationsanderroneousapproximation the uncertainty is generated. In essence, this is a simplified
trajectories that produced significant delays in the reaching body illusion that only uses one visual feature. Thus, this is
of the target locations. Intuitively, with low sensor variance extremelyrelevantasthemodeloftherobotispredictingdrifts
Σ < 0.5 the system was not able to perform the task with on the hand localization [25] but also actions.
sp
high sensor noise.
C. Comparison of active inference and inverse kinematics
B. Online adaptive behaviour We compared our algorithm with the inverse kinematics
approach in a reaching task using the right arm, the torso and
We tested the online perception and action loop when there
the head. This comparison was performed by defining a set
was a change in the visual feature location. The visual feature
of 3D waypoints matching the vertices of a rectangular prism
wasinthe3Dspace,andalldegreesoffreedomandbinocular
that the robot must reach, as described in Fig. 4(d).
vision were considered. Figure 3(b,c,e,f) shows the behaviour
The inverse kinematics module from iCub1 was used se-
ofthearmtryingtocorrecttheforwardmodelpredictionwith
quentially to solve each of the eight vertices reaching prob-
thealteredlocationofthevisualmarker.Wesettheperceptual
lems. The robot joints were position-controlled to the desired
attractor position to a constant fixed point in 3D space. The
state and joint speed was adjusted in order for the motion
end-effector visual marker was removed and replaced with a
to have the same time duration as the active inference test.
similarmarkerlocatedonatoolthatwasoperatedbyahuman.
Headangleswerepositionedinawaysuchthattheperceptual
When the marker was moved sideways (Fig. 3(b) and 3(c)),
the algorithm generated an action that moved the end-effector
1iKin uses the nonlinear optimization package Ipopt to obtain the joint
towards the opposite direction to correct the deviation. The
anglepositiontoarriveatacertain3Dpointinspacetakingacertainangle
internal belief was also updated to find an equilibrium point. configurationasthestartingpoint.
)snaidar(noitisopralugnA
1.5
1.0
0.5
0.0
-0.5 q μ11 μ q22 μ q33 μ q44
0 5 10 15 20 25 30 35 Time(s)
(a) Rightarminternalstatesµandencoderssp.
)snaidar(noitisopralugnA
0.4 0.00
q1e q2e q3e μ1e μ2e μ3e y0.1 0 0 .05
0.2 0.15
0.20 0.4
0.0
-0.2 0.3
-0.4 z
0.2
-0.6 v c
a
i a
t
s
t
l
r
u c
a
a u
c
l l
t
a
o
s t
r
ve
ρ
dgv(μ)
-0.8 0.1
0 5 10 15Time(s) 20 25 30 35 -0.2 -0.3 -0 x .4 -0.5 -0.6
(b) Headinternalstatesµe andencodersse. (c) 3Dpathsofsv,gv(µ)andρ.
)snaidar(rorreralugnA
0.4 (((( qqqq 1234 ---- μμμμ 1234 ))))
0.2
0.0
-0.2
-0.4
0 5 10 15 20 25 30 35 Time(s)
(d) Rightarmjointerror(sp−µ).
)snaidar(rorreralugnA
0.06
( ( ( q q q e e e 1 2 3 - - - μ μ μ e e e 1 2 3 ) ) )
0.04
0.02
0.00
-0.02
0 5 10 15 20 25 30 35 Time(s)
(e) Headjointerror(sp−µ).
)m(rorrenoitisoP
0.10 ( ( ( v v v 1 2 3 - - - g g g v v v 1 2 3 ( ( ( μ μ μ ) ) ) ) ) )
0.05
0.00
-0.05
0 5 10 15 20 25 30 35 Time(s)
(f) 3Dend-effectorerror(sv−gv(µ)).
Fig.5. Systemdynamicsforreachingamovingobjectin3D.Internalstatesaredriventowardstheperceptualattractorposition.Thedifferencebetweenthe
valueofinternalstatesandencoders(5(d))andbetween3Dcalculatedpositionandvisuallocationofend-effector(5(f))drivetheactions.
attractor was always inside the visual field. Our approach difference in the discrepancy between the calculated and the
included in addition to the arm and the torso, also active head visual perception position. Inverse kinematics paths show the
objecttracking.Inthiscase,theheadstartedlookingfrontand error between the model and the physical robot (distance
the arm was outside of the visual field. between the orange and the green line of Fig. 4(f)). Our
Each location was considered to be reached once the cal- approach reduced this distance when approaching the target
culated position of the visual marker was inside a sphere of location in a closed-loop manner (Fig. 4(e)), while inferring
r = 0.5cm centred at the attractor location. Setting the 3D its real body configuration and using head object tracking.
position reconstructed from the visual input as the ground
truth for the location of the end-effector, the performance was D. System dynamics when reaching a moving object
assessedusingtherootmeansquare(RMS)errorbetweenthis
We evaluated the algorithm in a reaching task of a moving
location and the target position of each vertex. Figure 4(a)
object manually operated by a human. The experiment started
shows the RMS error between the visual perception position
with the object near the robot and it was progressively moved
s andtheattractorpositionρ.Thedashedlinemarksanerror
v away and in an ascendant vertical direction, to finally be
of 0.5cm. Because the active inference algorithm implements
handed to the robot in an intermediate and lower location.
online sensory fusion (forward model and stereo vision) the
This experiment was performed both in 2D and 3D visual
performance of the reaching task was improved over the
space. In 2D space, the setup used monocular vision from the
inverse kinematics approach. The minimum error obtained
left eye and the torso was blocked. Our model was tested for
for the inverse kinematics algorithm was 1.1cm, while the
both arms and using active head object tracking. In 3D space,
minimumerrorfortheactiveinferencealgorithmwas0.42cm.
we enabled the torso motion but restricted the model for the
Themeanerrorforthereachingofall3Dpointsintheinverse
rightarmandthehead.Figure6showsthevisionoftherobot
kinematics was e = 1.69cm, while the active inference
IK andthefreeenergyvaluesfortheheadandthearmduringthe
algorithm obtained e =0.67cm.
AI reaching experiment in 2D space. F is minimised modifying
Figure 4(b) shows the internal state belief µ and the the arm perception and generating actions towards the object.
encoder values q of the active inference algorithm, while Fig. Theresultingvariabledynamicsofthe3Dspaceexperiment
4(c) shows the encoder values q for the inverse kinematics are shown in Fig. 5. The initial position of the right arm
algorithm. The attractor position change is represented using lied outside of the visual plane (Fig. 5(f) showing zero error).
a dashed line. In the active inference algorithm, each time the When there was no visual input, the free energy optimization
attractor position changes the believed state µ gets pushed to- algorithm only relied on joint measurements and the forward
wardsthenewpositionandthereisarelativelybigdiscrepancy model to produce the reaching motion until the right hand
between µ and q that gets reduced as the goal is reached. appears in the visual plane. Figures 5(a) and 5(b) show both
Figures 4(e) and 4(f) show the paths taken by the active the encoders measurements q and the estimated joint angle
inference and the inverse kinematics algorithm. In both al- µ of the arm and head. Figure 5(c) shows how calculated
gorithms, the robot reached all targets, but there was a clear g (µ) and real s 3D positions of the right arm end-effector
v v
)mrathgir(ygrene-eerF
0.06 3.5 RHiegahdtarm
3.0 0.05
2.5 0.04
2.0
0.03
1.5
0.02
1.0
0.5 0.01
0 10 20 30 40 50
)daeh(ygrene-eerF
energy tackles both sensory noise and model bias. In fact,
the equilibrium point where the algorithm converges reduces
the differences between the model and the real observations.
However, it is prone to local minima. As there are no task
hierarchies, dual arm reaching produced redundant motions in
both arms. A hierarchical version of the proposed approach,
as proposed in [28], could circumvent this limitation by
generating attractors for specialised behaviours.
Time(s)
The engineering analysis needed on iCub specific param-
Fig.6. Reachingamovingobjectin2D.(Left)Robotvision:visualfeature eters and actuation was the biggest limitation for generaliza-
(red), predicted end-effector location (green), object (blue). (Right) Free
tion. Sensor variances, action gains and velocity limits were
energyoptimizationforthearm(orange)andthehead(red).
experimentally tuned. In theory, variances and gains could be
also optimised within the free energy framework [7]. Besides,
follow the perceptual attractor ρ. Although the cameras were we introduced how time discretization in a velocity control
previously calibrated, noise was present in the stereo recon- scheme can be used to simplify the action computation. In
structionofvisualperception.Thisnoisewasmorerelevantin force controlled non-linear systems the mapping between the
the depth estimation (first term in Fig. 5(f)). Stop action for sensoryconsequenceandtheforceappliedmustbelearnt[19]
this experiment was produced by the sense of touch. Contact or explicitly computed through the forward dynamics [18],
in the hand pressure sensors triggers the grasping motion. considerably increasing the complexity of the optimization
framework.
VII. ACTIVEINFERENCEINREALARTIFICIALAGENTS
The proposed algorithm is scalable to integrate features
Being able to develop robots with biologically plausible from different modalities (i.e., adding a new term to the
body perception and action models allow us to validate summation) and handles sensory fusion in both perception
current brain-inspired computational models [26], [27]. How- and action. Furthermore, the needed generative functions can
ever, there is a big gap between the theoretical mathematical be easily computed from the forward kinematics or be even
construct and physical reality. This work reduces this gap replaced by learned ones, as shown in [16]. However, more
and shows the plausibility of implementing the free energy complex sensory input features should be incorporated to be
principle [6] on artificial agents for real-world applications, able to compare with other bio-inspired approaches such as
joining together robotics and computational neuroscience. deep reinforcement learning.
The reproduced robot behaviour, during the system dynamics
VIII. CONCLUSIONS
study (Sec. VI-D), resembled a one-year-old infant. Just by
Thisworkpresentedaneuroscience-inspiredclosed-loopal-
prediction error minimization, in this case, through surprise
gorithmforbodyperceptionandactionworkinginahumanoid
minimization, the robot was able to perform human-like
robot. Our active inference model is the first one validated on
movements in a reaching and object tracking task. The model
a real robot for upper-body reaching and active head object
and the experiments can be reproduced and replicated by
tracking. Robots perceiving and interacting under the active
downloading the open-source code tobereleased.
inferenceapproachisanimportantstepforevaluatingtheplau-
A. Relevant model predictions for human body perception sibilityofthemodelinbiologicalsystemsaswesystematically
observe the behaviours and connect them to the mechanisms
Interestingly, our model predicts the appearance of invol-
behind, and even make predictions. As a proof of concept,
untary actions in a sensorimotor conflict situation, such as
we showed its robustness to sensory noise and discrepancies
the rubber-hand illusion. In the presence of sensory errors
between the robot model and the real body, its adaptive char-
(armlocationexpectationmismatch),weobservedmovements
acteristic to visual online changes and its capacity to embed
towards the visual feature of the arm. In previous work, we
sensory fusion. Body perception was approached as a hidden
showed that the proprioceptive drift (mislocalization of the
Markov model and the unobserved body configuration/state
hand)usingasimilarfreeenergyoptimizationalgorithmcould
wascontinuouslyapproximatedwithvisualandproprioceptive
be modelled [25]. By adding the action term of active infer-
inputs by minimising the free energy bound. The action was
ence, forces towards minimising the prediction error should
modelledwithinperceptionasareflextoreducetheprediction
appear, i.e., in the same direction of the drift. This behaviour
error.WetestedthealgorithmontheiCubrobotin2Dand3D
could be in charge of the online adaptation of movements
using binocular vision. Results showed that our approach was
in the presence of disturbances or not trained conditions,
capableofcombiningdifferentindependentsourcesofsensory
bypassing the voluntary movement pathway.
information without an increase of computational complexity,
B. Technical challenges for a full-fledged body model displaying online adaptation capabilities and performing more
accurate reaching behaviours than inverse kinematics
Closed-loop adaptive perception and action is an important
advantage. The model is assumed to be an approximation REFERENCES
of the reality and the minimization of the variational free [1] H.v.Helmholtz,HandbuchderphysiologischenOptik. L.Voss,1867.
[2] D.C.KnillandA.Pouget,“Thebayesianbrain:theroleofuncertainty [27] M. Hoffmann and R. Pfeifer, “Robots as powerful allies for the study
inneuralcodingandcomputation,”TRENDSinNeurosciences,vol.27, of embodied cognition from the bottom up,” in The Oxford Handbook
no.12,pp.712–719,2004. of4ECognition,2018.
[3] K. Friston, “A theory of cortical responses,” Philos Trans R Soc Lond [28] J.Tani,“Learningtogeneratearticulatedbehaviorthroughthebottom-
B:BiologicalSciences,vol.360,no.1456,pp.815–836,2005. up and the top-down interaction processes,” Neural Networks, vol. 16,
[4] T.ParrandK.J.Friston,“Uncertainty,epistemicsandactiveinference,” no.1,pp.11–23,2003.
JournalofTheRoyalSocietyInterface,vol.14,no.136,p.20170376,
2017.
[5] E.TodorovandM.I.Jordan,“Optimalfeedbackcontrolasatheoryof
motorcoordination,”Natureneuroscience,vol.5,no.11,p.1226,2002.
[6] K.J.Friston,“Thefree-energyprinciple:aunifiedbraintheory?”Nature
Reviews.Neuroscience,vol.11,pp.127–138,022010.
[7] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
behavior: a free-energy formulation,” Biological cybernetics, vol. 102,
no.3,pp.227–260,2010.
[8] M. Kirchhoff, T. Parr, E. Palacios, K. Friston, and J. Kiverstein, “The
markovblanketsoflife:autonomy,activeinferenceandthefreeenergy
principle,” Journal of The royal society interface, vol. 15, no. 138, p.
20170792,2018.
[9] C. Fantacci, U. Pattacini, V. Tikhanoff, and L. Natale, “Visual end-
effector tracking using a 3d model-aided particle filter for humanoid
robotplatforms,”in2017IEEE/RSJInternationalConferenceonIntel-
ligentRobotsandSystems(IROS). IEEE,2017,pp.1411–1418.
[10] C. Garcia Cifuentes, J. Issac, M. Wthrich, S. Schaal, and J. Bohg,
“Probabilistic articulated real-time tracking for robot manipulation,”
IEEERoboticsandAutomationLetters,vol.2,2017.
[11] P.Vicente,L.Jamone,andA.Bernardino,“Onlinebodyschemaadap-
tationbasedoninternalmentalsimulationandmultisensoryfeedback,”
FrontiersinRoboticsandAI,vol.3,p.7,2016.
[12] C.GaskettandG.Cheng,“Onlinelearningofamotormapforhumanoid
robotreaching,”in2ndInt.Conf.oncomputationalinte.,roboticsand
autonomoussystems,Singapore,2003.
[13] L. Jamone, M. Brandao, L. Natale, K. Hashimoto, G. Sandini, and
A.Takanishi,“Autonomousonlinegenerationofamotorrepresentation
of the workspace for intelligent whole-body reaching,” Robotics and
AutonomousSystems,vol.62,no.4,pp.556–567,2014.
[14] A. Roncone, M. Hoffmann, U. Pattacini, L. Fadiga, and G. Metta,
“Peripersonal space and margin of safety around the body: learning
visuo-tactileassociationsinahumanoidrobotwithartificialskin,”PloS
one,vol.11,no.10,p.e0163713,2016.
[15] P. Lanillos, E. Dean-Leon, and G. Cheng, “Yielding self-perception in
robots through sensorimotor contingencies,” IEEE Trans. on Cognitive
andDevelopmentalSystems,no.99,pp.1–1,2016.
[16] P.LanillosandG.Cheng,“Adaptiverobotbodylearningandestimation
throughpredictivecoding,”IntelligentRobotsandSystems(IROS),2018
IEEE/RSJInt.Conf.on,2018.
[17] M.BaltieriandC.L.Buckley,“Anactiveinferenceimplementationof
phototaxis,”2018ConferenceonArtificialLife,no.29,pp.36–43,2017.
[18] L.Pio-Lopez,A.Nizard,K.Friston,andG.Pezzulo,“Activeinference
androbotcontrol:acasestudy,”JRSocInterface,vol.13,2016.
[19] P.LanillosandG.Cheng,“Activeinferencewithfunctionlearningfor
robotbodyperception,”InternationalWorkshoponContinualUnsuper-
visedSensorimotorLearning,ICDL-Epirob,2018.
[20] M.JBealandZ.Ghahramani,“Thevariationalbayesianemalgorithm
forincompletedata,”Statistics,072002.
[21] S. Kullback and R. A. Leibler, “On information and sufficiency,” The
AnnalsofMathematicalStatistics,vol.22,no.1,pp.79–86,031951.
[22] C.L.Buckley,C.S.Kim,S.McGregor,andA.K.Seth,“Thefreeenergy
principleforactionandperception:Amathematicalreview,”Journalof
MathematicalPsychology,2017.
[23] K.Friston,J.Mattout,N.Trujillo-Barreto,J.Ashburner,andW.Penny,
“Variational free energy and the laplace approximation,” Neuroimage,
vol.34,no.1,pp.220–234,2007.
[24] G. Metta, G. Sandini, D. Vernon, L. Natale, and F. Nori, “The icub
humanoidrobot:Anopenplatformforresearchinembodiedcognition,”
PerformanceMetricsforIntelligentSystemsWorkshop,012008.
[25] N.-A.Hinz,P.Lanillos,H.Mueller,andG.Cheng,“Driftingperceptual
patterns suggest prediction errors fusion rather than hypothesis selec-
tion: replicating the rubber-hand illusion on a robot,” arXiv preprint
arXiv:1806.06809,2018.
[26] P. Lanillos, E. Dean-Leon, and G. Cheng, “Enactive self: a study
of engineering perspectives to obtain the sensorimotor self through
enaction,” in Developmental Learning and Epigenetic Robotics, Joint
IEEEInt.Conf.on,2017.