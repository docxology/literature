Active inference body perception and action
for humanoid robots
Guillermo Oliver1, Pablo Lanillos 1,2, Gordon Cheng 1
Abstract—Providing artiﬁcial agents with the same computa-
tional models of biological systems is a way to understand how
intelligent behaviours may emerge. We present an active inference
body perception and action model working for the ﬁrst time in
a humanoid robot. The model relies on the free energy principle
proposed for the brain, where both perception and action goal is
to minimise the prediction error through gradient descent on the
variational free energy bound. The body state (latent variable)
is inferred by minimising the difference between the observed
(visual and proprioceptive) sensor values and the predicted ones.
Simultaneously, the action makes sensory data sampling to better
correspond to the prediction made by the inner model. We
formalised and implemented the algorithm on the iCub robot
and tested in 2D and 3D visual spaces for online adaptation
to visual changes, sensory noise and discrepancies between the
model and the real robot. We also compared our approach with
classical inverse kinematics in a reaching task, analysing the
suitability of such a neuroscience-inspired approach for real-
world interaction. The algorithm gave the robot adaptive body
perception and upper body reaching with head object tracking
(toddler-like), and was able to incorporate visual features online
(in a closed-loop manner) without increasing the computational
complexity. Moreover, our model predicted involuntary actions
in the presence of sensorimotor conﬂicts showing the path for a
potential proof of active inference in humans.
Index Terms—Active inference, free energy optimization, Bio-
inspired Perception, Predictive coding, Humanoid robots, iCub.
I. I NTRODUCTION
The medical doctor and physicist Hermann von Helmholtz
described visual perception as an unconscious mechanism that
infers the world [1]. In other words, the brain has generative
models that reconstruct the world from partial information.
Nowadays, there is a scientiﬁc mainstream that describes the
inner workings of the brain as those of a Bayesian inference
machine [2], [3]. This approach supports that we can adjust the
cues (visual, proprioceptive, tactile, etc.) contribution to our
interpretation in a Bayesian optimal way considering sensors
and motor uncertainties [4]. This implies that the brain is
able to encode uncertainty not only for perception but also
for acting in the world.
Optimal feedback control was proposed for modelling mo-
tor coordination under uncertainty [5]. Alternatively, active
inference [6], defended that both perception and action are
two sides of the same process: the free energy principle.
1Institute for Cognitive Systems, Technical University of Munich, Arcis-
strasse 21, 80333 Munich, Germany.
2Donders Institute for Brain, Cognition and Behaviour, the Netherlands.
This work has been supported by SELFCEPTION project EU Horizon 2020
Programme, grant agreement n. 741941 and the EU’s Erasmus+ Programme.
Supplementary video: https://youtu.be/rdbbmwo4TY4, code: tobereleased.
Action
Body perception
Fig. 1. Body perception and action via active inference . The robot infers
its body (e.g., joint angles) by minimising the prediction error: discrepancy
between the sensors (visual sv and joint sp) and their expected values (gv(µ)
and gp(µ)). In the presence of error it changes the perception of its body µ
and generates an action ato reduce this discrepancy. Both are computed by
optimising the free energy bound F. In a reaching task, the object is a causal
variable ρthat acts as a perceptual attractor, producing an error in the desired
sensory state and promoting an action towards the goal. The equilibrium point
˙µ= 0 appears when the hand reaches the object. The head keeps the object
in its visual ﬁeld, improving the reaching performance.
This principle accounts for perception, action and learning
through the minimization of surprise, i.e., the discrepancy
between the current state and the predicted or desired one.
According to this approach, free energy is a way of quantifying
surprise and it can be optimised by changing the current beliefs
(perception) or by acting on the environment (action) to adjust
the difference between reality and prediction [7]. Active in-
ference would allow living beings to infer and simultaneously
adapt their body to uncertain environments [8], something just
partially solved in robotics.
Hence, inspired by this principle, we present a robust
and adaptive body perception and action mathematical model
based on active inference, for a real humanoid robot (Fig. 1).
Our approach describes artiﬁcial body perception and action,
similar to biological systems, as a ﬂexible and dynamic process
that approximates the robot body latent state using the error
between the expected and the observed sensory information.
The proposed model enabled the robot to have adaptive body
perception and to perform upper-body reaching behaviours
even under high levels of sensor noise and discrepancies
between the model and the real robot. We further compared
our approach to standard inverse kinematics and analysed the
relevance and the challenges of predictive coding interpretation
solved through variational free energy methods for humanoid
robotics. For reproducibility, the code, with the parameters
used, is publicly available: tobereleased.
arXiv:1906.03022v3  [cs.RO]  29 Jan 2020
A. Related work
Multisensory body perception has been widely studied in
the literature, from the engineering point of view, and enables
robots to estimate its body state by combining joint infor-
mation with other sensors such as images and tactile cues.
Bayesian estimation (e.g., particle ﬁlters) has been proved to
achieve robust and accurate model-based robot arm tracking
[9] even under occlusion [10], or for simultaneous joint offset
parameters adaptation and hand pose estimation [11]. Fur-
thermore, integrated visuomotor processes enabled humanoid
robots to learn motor representations for robust reaching [12],
[13], visuotactile motor representations for reaching and avoid-
ance behaviours [14] or enabling visual robot body distinction
for aiding the discovering of movable objects [15]. Free energy
optimization for multisensory perception in a real robot was
ﬁrstly presented in [16] working as an approximate Bayesian
ﬁlter estimation. The robot was able to perceive its arm
location fusing visual, proprioceptive and tactile information.
Active inference (under the free energy principle) man-
aged to include the action within the multisensory perception
process, explained as a classical spinal reﬂex arc pathway
triggered by prediction errors. It has been mainly studied in
theoretical or simulated conditions. Friston et al. [7] presented
this neuroscience-inspired model as a generalization of the
dynamic expectation-maximization algorithm. Recently, a few
works analysed this approach for robotics in simulated condi-
tions: one degree of freedom vehicle [17], a PR2 robot arm
[18], and two degrees of freedom robot arm with generative
function learning [19]. However, these studies did not validate
the method on real robots bypassing the true difﬁculties, such
as non-linear dynamics due to friction, backlash, unmodelled
physical constraints, uncertainties in the dynamic model, sen-
sor and model errors, etc.
In this work, we provide the active inference perception and
action mathematical construct for the humanoid robot iCub
upper-body working in different real setups. Thus, we show
the plausibility of real implementations of the free energy
principle in artiﬁcial agents, analysing its challenges and
limitations. Furthermore, we introduce a novel way to compute
the mapping from action to sensory consequences (partial
derivatives of the sensory input with respect to the action)
exploiting the properties of velocity control, and simplifying
the calculus of the action within the free energy optimization
framework.
B. Paper organization
First, in Sec. II we explain the general mathematical free
energy optimization for perception and action. Afterwards,
Sec. III describes the iCub physical model and in Sec. IV
and V we detail the active inference computational model
applied for object reaching and head object tracking. Finally,
Sec. VI shows the results for several experiments and Sec.
VII analyses the relevance of implementing active inference
body perception in humanoids, for both human and robotics
science.
II. F REE ENERGY OPTIMIZATION
A. Bayesian inference
According to the Bayesian inference model for the brain, the
body conﬁguration x is inferred using available sensory data
s by applying Bayes’ theorem: p(x|s) = p(s|x)p(x)/p(s).
Where this posterior probability, corresponding to the prob-
ability of body conﬁguration x given the observed data s, is
obtained as a consequence of three antecedents: (1) likelihood,
p(s|x), or compatibility of the observed data s with the
body conﬁguration x, (2) prior probability , p(x), or current
belief about the conﬁguration before receiving the sensory
data s, and (3) marginal likelihood, p(s), a normalization term
which corresponds to the marginalization of the likelihood of
receiving sensory data s regardless of the conﬁguration.
The goal is to ﬁnd the value of x which maximises p(x|s),
because it is the most likely value for the real-world body
conﬁguration x taking into account the sensory data obtained
s. This direct method becomes intractable for large state spaces
[20]. In this case, marginalization over all the possible body
and world states explodes computationally.
B. Free energy principle with the Laplace approximation
The free energy principle [6] provides a tractable solution to
this obstacle, where, instead of calculating the marginal likeli-
hood, the idea is to minimise the Kullback-Leibler divergence
[21] between a reference distribution q(x) and the real p(x|s),
in order for the ﬁrst to be a good approximation of the second.
DKL(q(x)||p(x|s)) =
∫
q(x) ln q(x)
p(x|s)dx
=
∫
q(x) ln q(x)
p(s,x)dx+ lnp(s) = F + lnp(s) ≥0 (1)
Minimising the ﬁrst term, F, effectively minimises the
difference between these two densities, with only the marginal
likelihood remaining. Unlike the whole expression of the KL-
divergence, the ﬁrst term can be evaluated because it depends
on the reference distribution and the knowledge about the en-
vironment we can assume the agent has, p(s,x) = p(s|x)p(x).
This term is deﬁned as variational free energy [22].
According to the free energy optimization theory, there
are two ways to minimise surprise, which accounts for the
discrepancy between the current state and the predicted or
desired one (prediction error): changing the belief ( perceptual
inference) or acting on the world (active inference). Perceptual
inference and active inference optimise the value of variational
free energy, while active inference also optimises the value
of the marginal likelihood by acting on the environment and
changing the sensory data s.
Under the Laplace approximation instead of computing the
whole distribution we can track the mode [23] and simplify the
calculus of F. Thus, the variational density q(x) is assumed
to have Gaussian form N(x|µ,Σ). Deﬁning the Laplace-
encoded energy [22] as L(s,x) = −ln p(s,x), F can be
approximated to:
F ≈L(s,µ) −
[1
2 ln |Σ|+ nln 2π
]
(2)
where µconceptually describes the internal belief or state, s
is the sensory input and n is the size of µvector.
C. Perceptual and active inference
Perceptual inference is the process of updating the inner
model belief to best account for sensory data, minimising
the prediction error. For instance for body perception, the
agent must infer the most-likely or optimal value for the body
conﬁguration or state µ. This optimal value is the one that
minimises free energy. To solve it, we can compute it by
gradient descent over the free energy term. For static systems,
this update is performed directly as [16]: ˙µ = −∂F
∂µ. In
dynamic systems, time derivatives should be considered, the
state variable is now a vector µ:
˙µ= Dµ−∂F
∂µ (3)
where the Dis the block-matrix derivative operator. When free
energy is minimised, the value of its derivative is ∂F
∂µ = 0, and
the system is at equilibrium.
Active inference [7], is the extension of perceptual inference
to the relationship between sensors and actions, taking into
account that actions can change the world to make sensory
data more accurate with predictions made by the inner model.
The action plays a core role in the optimization and improves
the approximation of the real distribution, therefore reducing
the prediction error by minimising free energy. It also acts
on the marginal likelihood by changing the real conﬁguration
which modiﬁes the sensory data s to obtain new data that is
more in concordance with the agent’s belief. The optimal value
is the one which minimises free energy, and again a gradient
descent approach will be taken to update its value:
˙a= −∂F
∂a (4)
III. R OBOT PHYSICAL MODEL
q1
a1
q2
a2
q4 a4
q3
a3
qe1
ae1
qe2
ae2
qe3
ae3
vR
ρR
Left
Right
vL
ρL
v
ρ
μ
μ'
q
v
Environment
a
ρ
v
ρ
3D
x
z
y
Fig. 2. Model description. (Left) Generative model of the robot. (Right)
Notation and robot conﬁguration. Shown variables: internal variables µand
µ′, joint angle position q, joint actions a, and 3D location of the end-effector
vand causal variables ρalong with their projections in 2D visual space.
The robot iCub [24] (v1.4) is a humanoid powered by
electric motors and driven by tendons. It is divided into several
kinematic chains deﬁned through homogeneous transformation
matrices using Denavit-Hartenberg convention. We focused on
two kinematic chains, those with the end-effector being the
hands (without ﬁngers), the head, the eyes and the torso.
As depicted in Fig. 2, each arm model is deﬁned as a
three degree of freedom system: shoulder roll, shoulder yaw
and r elbow. To provide improved reaching capabilities, an
additional degree of freedom is considered for the torso:
torso yaw. Left and right eye cameras observe the end-effector
position and the world around it, providing stereo vision. The
joints considered for the head are: neck pitch and neck yaw.
Moreover, vertical synchronised motion is performed in both
eyes using: eyes tilt.
IV. A CTIVE INFERENCE COMPUTATIONAL MODEL FOR
ICUB REACHING TASK
A. Problem formulation
The body conﬁguration, or internal variables, is deﬁned as
the joint angles gp(µ) = µ. The estimated states µ∈I R4 are
the belief the agent has about the joint angle position and the
action a ∈I R4 is the angular velocity of those same joints.
Due to the fact that we use a velocity control for the joints,
ﬁrst-order dynamics must also be considered µ′ ∈I R4.
Sensory data will be obtained through several input sensors.
Using binocular disparity between the images obtained from
both eyes, stereo vision reconstruction is performed to obtain
the 3D position of the end-effector, sv ∈I R3. All joint motors
have encoders which provide joint angle position, sp ∈I R4.
sp =


q1
q2
q3
q4

 sv =


v1
v2
v3

 (5)
The likelihood p(s|µ) is made up of proprioception func-
tions in terms of the current body conﬁguration, while the
prior p(µ) will take into account the dynamic model of the
agent that describes how this internal state changes with time.
Adapting the Laplace-encoded energy for our robot model and
assuming independence between different modalities sensory
readings:
ln p(s,ρ,µ) = ln p(sp|µ)p(sv|µ)p(µ′|µ,ρ) (6)
B. Free energy optimization
In order to deﬁne the conditional densities for each of the
terms, we should deﬁne the expressions for the sensory data.
Joint angle position, sp, is obtained directly from the joint
angle sensors. Let us assume that the input is noisy and follows
a normal distribution with mean at the internal value µ and
variance Σsp. End-effector 2D or 3D position,sv, is deﬁned by
a non-linear function dependent on the body conﬁguration and
obtained using the forward model of the arm. Again, the input
is noisy and follows a normal distribution with mean at the
value of this function gv(µ) ∈I Rnv and variance Σsv, where
nv is the visual dimensions. The dynamic model for np latent
variables (joints angles) is determined by a function which
depends on both the current state µ and the causal variables
ρ(e.g. 3D position of the object to be reached), with a noisy
input following a normal distribution with mean at the value
of this function f(µ,ρ) ∈I Rnp and variance Σsµ. Therefore,
likelihood functions in Eq. (6) are deﬁned as:
p(sp|µ) =
np∏
i=1
N(qi|µi,Σp) p(sv|µ) =
nv∏
i=1
N(vi|gvi(µ),Σv)
p(µ′|µ,ρ) =
np∏
i=1
N(µ′
i|fi(µ,ρ),Σsµ) (7)
The optimization of the Laplace-encoded energy is per-
formed using gradient descent (Eq. (3) and (4)). The depen-
dency of F with respect to the vector of internal variables
µcan be calculated using the chain rule on the functions that
depend on those internal variables. The dependency of F with
respect to the vector of actions ais calculated considering that
the only magnitudes directly affected by action are the values
obtained from the sensors: ∂F
∂a = ∂s
∂a
∂F
∂s - see Sec. IV-D.
−∂F
∂µ = 1
Σsp
(sp −µ) + 1
Σsv
∂gv(µ)
∂µ
T
(sv −g(µ))
+ 1
Σsµ
∂f(µ,ρ)
∂µ
T
(µ′ −f(µ,ρ)) (8)
−∂F
∂a = −
( 1
Σsp
∂sp
∂a
T
(sp −µ) + 1
Σsv
∂sv
∂a
T
(sv −gv(µ))
)
(9)
The agent also infers the ﬁrst-order dynamics of the body µ′
and updates their values using a gradient descent formulation.
The dependency of F with respect to µ′ is limited to the
inﬂuence of the dynamic model.
−∂F
∂µ′ = − 1
Σsµ
(µ′ −f(µ,ρ)) = 1
Σsµ
(f(µ,ρ) −µ′) (10)
Finally, the differential equations for µ, µ′ and aare:
˙µ= µ′ −∂F
∂µ
˙µ′ = −∂F
∂µ′ ˙a= −∂F
∂a (11)
A ﬁrst-order Euler integration method is applied to update the
values of µ, µ′ and ain each iteration. E.g., µi+1 = µi+ ˙µ∆t,
where ∆t= T is the period of execution of the updating cycle.
C. Perceptual attractor dynamics
The reaching goal is deﬁned in the dynamics of the model
by introducing a perceptual attractor ρin the 2D or 3D space:
A(µ,ρ) = ρ4




ρ1
ρ2
ρ3

−


gv1 (µ)
gv2 (µ)
gv3 (µ)



 (12)
where ρ1:3 deﬁnes the object location in the visual space
and ρ4 describes the gain of the attractor.
Internal variable dynamics are then deﬁned in terms of the
attractor: f(µ,ρ) = T(µ)A(µ,ρ). T(µ) is the function that
transforms the attractor vector from target space (3D) to joint
space. The system is velocity controlled, therefore target space
is a linear velocity vector and joint space is angular velocity.
When the Jacobian matrix is rectangular (e.g., 3D space
and 4 DOF arm joint space), we used the generalised in-
verse (Moore-Penrose pseudoinverse) as the mapping function:
T(µ) = J+(µ). This matrix is calculated using the singular-
value decomposition (SVD), where J+ = VΣ+UT.
D. Active inference
Action is set to be an angular velocity magnitude, which
corresponds with angular joint velocity in the latent space. In
order to compute Eq. (9) we need the mapping between the
sensor values and the velocities of the joints, expressed in the
partial derivatives ∂sp
∂a and ∂sv
∂a .
We assume that the control action, a, is updated for every
cycle, and therefore for each interval of time between cycles
it has the same value. For each period (cycle time between
updates), the equation of uniform circular motion is satisﬁed
for each joint position. If this equation is discretised, for each
sampling moment, which are T seconds apart, the joint value
will be updated in the following way: qi+1 = qi + ai T.
The dependency of the joint angle position with respect to the
control action is therefore deﬁned.
The partial derivatives of joint position sp with respect
to action, considering there is no cross-inﬂuence or coupling
between joint velocities and that qi and its expected µi should
converge at equilibrium, are given by the following expression:
∂qi
∂aj
= ∂µi
∂aj
=
{
T i = j,
0 otherwise. (13)
If the dependency of joint position with respect to action is
known, we can use the chain rule to calculate the dependency
for the visual sensor sv. Considering that the values of
gvi(µ) should also converge to vi at equilibrium, the partial
derivatives are given by the following expression:
∂vi
∂aj
= ∂vi
∂qj
∂qj
∂aj
= ∂gvi(µ)
∂µj
∂µj
∂aj
(14)
V. A CTIVE INFERENCE COMPUTATIONAL MODEL FOR
ICUB HEAD OBJECT TRACKING TASK
We extend the arm reaching model for the head to obtain
an object tracking motion behaviour. The goal of this task is
to maintain the object in the centre of the visual ﬁeld, thus
increasing its reaching working range capabilities.
Sensory data and proprioception for the head is deﬁned by
internal variables beliefs µe ∈I R3, actions ae ∈I R3, and
ﬁrst-order dynamics µ′
e ∈I R3 and because the end-effector
are the eyes, there is only joint angle position, se ∈I R3.
To obtain the desired motion, an attractor is deﬁned towards
the centre of the left eye image (cx,cy). Attractor position
(ρ5,ρ6) is read from the projected 2D visual input of the left
eye and dynamically updates with head motion.
Ae(µe,ρ) = ρ4
([ cx
cy
]
−
[ ρ5
ρ6
])
(15)
Visual coordinate v 1 (pixels)
1
2
3
4
encoders+vision
encoders
vision
100 150 200 250 300
100
125
150
175
200
225
Visual coordinate v 2 (pixels)
(a) Sensory fusion path.
-0.36
-0.34
-0.32
-0.30
x
0.060.080.100.120.14
y
0.00
0.05
0.10
z
visual sv
calculated gv(μ)
attractor ρ (b) Visual marker deviation to the left.
-0.36
-0.34
-0.32
-0.30
x
0.060.080.100.120.14
y
0.00
0.05
0.10
z
visual sv
calculated gv(μ)
attractor ρ (c) Visual marker deviation to the right.
Visual coordinate v 1 (pixels)
1
2
3
4No noise
x =0,σ=10°
x =0,σ=20°
x =0,σ=40°
100 150 200 250 300
100
125
150
175
200
225
Visual coordinate v 2 (pixels)
(d) Encoder noise handling path.
-0.40
-0.35
-0.30
-0.25
x
0.000.050.100.150.20
y
-0.10
-0.05
0.00
0.05
0.10
z
visual sv
calculated gv(μ)
attractor ρ (e) Visual marker located on ﬁnger.
-0.35
-0.30
-0.25
-0.20
x
0.000.050.100.150.20
y
0.00
0.05
0.10
0.15
0.20
z
visual sv
calculated gv(μ)
attractor ρ (f) Visual marker located on forearm.
Fig. 3. (Left) End-effector averaged paths in the right arm object reaching for (a) sensory fusion and (d) under noisy encoder sensors. Attractor position
is represented by a blue disk. (Right) Results of adaptive behaviour using sensory fusion. If the visual marker position is modiﬁed, the algorithm produces
a reaching action that tries to approach this new position to the attractor location. On the top right of each image the left eye camera is shown. Attractor
position is shown in blue, visual perception in red and calculated position in green. The resulting direction of motion obtained from the algorithm is shown
as an arrow.
Internal variable dynamics are then deﬁned in terms of the
attractor as: fe(µe,ρ) = Te(µe)Ae(µe,ρ), fe(µe,ρ) ∈
I R3. With two pixel coordinates and three degrees of freedom,
the pseudoinverse of the visual Jacobian matrix is used as the
mapping matrix in the visual space: Te(µe) = J+
v (µe).
VI. R ESULTS
We evaluated the proposed approach for humanoid upper-
body reaching in 2D and 3D visual space scenarios. The pro-
posed algorithm, in order to reduce the discrepancy between
the current sensory input and the desired state, produces a
reaching behaviour with the arm and torso of the robot while
the head tracks the object to maintain it in its visual ﬁeld.
Several experiments were performed: (1) proof of concept ,
right arm reaching towards a series of 2D locations in the
visual plane with different levels of noise and sensor sources;
(2) adaptation, the robot adapts its reaching behaviour during
changes on the visual feature location; (3) comparison, motion
from the active inference algorithm is compared to inverse
kinematics; and (4) dynamics evaluation , body perception
and action variables are analysed during an arm reaching
with active head towards a moving object. Video footage
of the experiments and some extras can be found in the
supplementary video: https://youtu.be/rdbbmwo4TY4.
The iCub robot is placed in a controlled environment and
an easily recognizable object is used as a perceptual attractor
to produce the movement of the robot. In 3D space, the values
of the causal variables ρ1, ρ2, ρ3 are the coordinates of the 3D
position of the object obtained using stereo vision, while in 2D
space, the values used for the attractor in (12) are the object
pixel location at the left eye. The value of ρ4 is a weighting
factor to adjust the power of the attractor. The right arm end-
effector has a visual marker and its 3D position is obtained as
the values of v1, v2 and v3, along with its projections in camera
space. The relevant parameters of the algorithm are: encoder
sensor variance Σsp, visual perception variance Σsv, attractor
dynamics variance Σsµ and action gains ka. These parameters
were tuned empirically with their physical meaning in mind
and remained constant during the experiments unless stated
otherwise. Their values can be found in the code repository.
A. Sensory fusion and robustness under noisy sensors
The ﬁrst experiment was designed to test sensory fusion
(Fig. 3(a)) and the impact of noisy sensors on the algorithm
(Fig. 3(d)). The robot worked in 2D visual space using the left-
eye camera (monocular vision) and the torso motion is locked.
The robot had to reach 10 times four different static locations
in the visual ﬁeld with the right arm. A location is considered
to be reached when the visual position is inside a disk with a
radius of ﬁve pixels centred at the location position.
Figure 3(a) shows the averaged trajectory of the visual
marker with different sensor modalities (considering only
intrinsic noise from the sensors and model errors): visual, joint
angle encoders, and both together. Even though the model has
been veriﬁed and the camera calibrated, there was a difference
between the forward model and the real robot, due to possible
deviations in parameters and motor backslash, which implies
that the robot had to use visual information to correct its real
position. Employing joint angle encoders and vision provided
the best behaviour for the reaching of the ﬁxed locations in
the visual ﬁeld, achieving all positions in the shortest fashion.
Visual perception reached all the positions but it did not follow
Position RMS error(m)
active inference
inverse kinematics
0 20 40 60 800.00
0.02
0.04
0.06
0.08
0.10
0.12
Time(s)
(a) 3D position error. Active inference (orange)
and inverse kinematics (blue).
Angular position(radians)
q1 q2 q3 q4
μ1 μ2 μ3 μ4
0 20 40 60 80
-0.5
0.0
0.5
1.0
1.5
Time(s)
(b) Right arm internal state µand encoders sp
in active inference algorithm.
Angular position(radians)
q1 q2 q3 q4
0 20 40 60 80
-0.5
0.0
0.5
1.0
1.5
Time(s)
(c) Right arm encoders sp in inverse kinematics
algorithm.
(d) Experiment description.
-0.5
-0.4
-0.3
-0.2
x
-0.10.00.10.20.3
y
-0.1
0.0
0.1
0.2
0.3
z
visual sv
calculated g(μ)
attractor ρ (e) 3D paths in active inference algorithm.
-0.5
-0.4
-0.3
-0.2
x
-0.10.00.10.20.3
y
-0.1
0.0
0.1
0.2
0.3
z
visual sv
calculated g(μ)
attractor ρ (f) 3D paths in inverse kinematics.
Fig. 4. Comparison between active inference and inverse kinematics. (Top left) RMS 3D position error between the real visual position and the attractor
position. (Top centre and right) Internal state and encoder values of the right arm for each of the algorithms. (Bottom left) Experiment description showing
the initial arm position and the rectangular prism. (Bottom centre and right) 3D paths taken for each of the algorithms.
the optimum path while using only the encoder values failed
to reach all locations.
Secondly, in order to test the robustness against sensory
noise, we injected noise in the robot motors encoders sp. Four
conditions were tested: Gaussian noise with a zero mean and
with standard deviation of 0° (control test), 10°, 20° and 40°.
Again, averaged path is shown (Fig. 3(d)). Trajectories with
no noise and σ = 10° achieved very similar results, with the
ﬁrst one achieving the objectives slightly faster. Extreme case
with σ= 40° caused oscillations and erroneous approximation
trajectories that produced signiﬁcant delays in the reaching
of the target locations. Intuitively, with low sensor variance
Σsp < 0.5 the system was not able to perform the task with
high sensor noise.
B. Online adaptive behaviour
We tested the online perception and action loop when there
was a change in the visual feature location. The visual feature
was in the 3D space, and all degrees of freedom and binocular
vision were considered. Figure 3(b,c,e,f) shows the behaviour
of the arm trying to correct the forward model prediction with
the altered location of the visual marker. We set the perceptual
attractor position to a constant ﬁxed point in 3D space. The
end-effector visual marker was removed and replaced with a
similar marker located on a tool that was operated by a human.
When the marker was moved sideways (Fig. 3(b) and 3(c)),
the algorithm generated an action that moved the end-effector
towards the opposite direction to correct the deviation. The
internal belief was also updated to ﬁnd an equilibrium point.
Similar behaviour was obtained when the marker was located
on one of the ﬁngers (Fig. 3(e)) and on the forearm (Fig.
3(f)). In this case, the visual discrepancy with the forward
model is much more noticeable and the algorithm is not able
of fully reaching the visual marker position to the attractor, but
the same opposite action behaviour as in the previous case is
obtained.
This experiment shows that in the case of visual sensorimo-
tor conﬂict a perception body alteration and action to reduce
the uncertainty is generated. In essence, this is a simpliﬁed
body illusion that only uses one visual feature. Thus, this is
extremely relevant as the model of the robot is predicting drifts
on the hand localization [25] but also actions.
C. Comparison of active inference and inverse kinematics
We compared our algorithm with the inverse kinematics
approach in a reaching task using the right arm, the torso and
the head. This comparison was performed by deﬁning a set
of 3D waypoints matching the vertices of a rectangular prism
that the robot must reach, as described in Fig. 4(d).
The inverse kinematics module from iCub 1 was used se-
quentially to solve each of the eight vertices reaching prob-
lems. The robot joints were position-controlled to the desired
state and joint speed was adjusted in order for the motion
to have the same time duration as the active inference test.
Head angles were positioned in a way such that the perceptual
1iKin uses the nonlinear optimization package Ipopt to obtain the joint
angle position to arrive at a certain 3D point in space taking a certain angle
conﬁguration as the starting point.
Angular position(radians)
q1 q2 q3 q4
μ1 μ2 μ3 μ4
0 5 10 15 20 25 30 35
-0.5
0.0
0.5
1.0
1.5
Time(s)
(a) Right arm internal states µand encoders sp.
Angular position(radians)
q1e q2e q3e μ1e μ2e μ3e
0 5 10 15 20 25 30 35
-0.8
-0.6
-0.4
-0.2
0.0
0.2
0.4
Time(s) (b) Head internal states µe and encoders se.
-0.6-0.5-0.4-0.3-0.2
x
0.00
0.05
0.10
0.15
0.20
y
0.1
0.2
0.3
0.4
z
visual sv
calculated gv(μ)
attractor ρ (c) 3D paths of sv, gv(µ) and ρ.
Angular error(radians)
(q1 -μ 1)
(q2 -μ 2)
(q3 -μ 3)
(q4 -μ 4)
0 5 10 15 20 25 30 35
-0.4
-0.2
0.0
0.2
0.4
Time(s)
(d) Right arm joint error (sp − µ).
Angular error(radians)
(qe1 -μ e1 )
(qe2 -μ e2 )
(qe3 -μ e3 )
0 5 10 15 20 25 30 35
-0.02
0.00
0.02
0.04
0.06
Time(s) (e) Head joint error (sp − µ).
Position error(m)
(v1 -g v1 (μ))
(v2 -g v2 (μ))
(v3 -g v3 (μ))
0 5 10 15 20 25 30 35
-0.05
0.00
0.05
0.10
Time(s) (f) 3D end-effector error (sv − gv(µ)).
Fig. 5. System dynamics for reaching a moving object in 3D. Internal states are driven towards the perceptual attractor position. The difference between the
value of internal states and encoders (5(d)) and between 3D calculated position and visual location of end-effector (5(f)) drive the actions.
attractor was always inside the visual ﬁeld. Our approach
included in addition to the arm and the torso, also active head
object tracking. In this case, the head started looking front and
the arm was outside of the visual ﬁeld.
Each location was considered to be reached once the cal-
culated position of the visual marker was inside a sphere of
r = 0.5 cm centred at the attractor location. Setting the 3D
position reconstructed from the visual input as the ground
truth for the location of the end-effector, the performance was
assessed using the root mean square (RMS) error between this
location and the target position of each vertex. Figure 4(a)
shows the RMS error between the visual perception position
sv and the attractor position ρ. The dashed line marks an error
of 0.5 cm. Because the active inference algorithm implements
online sensory fusion (forward model and stereo vision) the
performance of the reaching task was improved over the
inverse kinematics approach. The minimum error obtained
for the inverse kinematics algorithm was 1.1 cm, while the
minimum error for the active inference algorithm was 0.42 cm.
The mean error for the reaching of all 3D points in the inverse
kinematics was eIK = 1 .69 cm, while the active inference
algorithm obtained eAI = 0.67 cm.
Figure 4(b) shows the internal state belief µ and the
encoder values qof the active inference algorithm, while Fig.
4(c) shows the encoder values q for the inverse kinematics
algorithm. The attractor position change is represented using
a dashed line. In the active inference algorithm, each time the
attractor position changes the believed state µgets pushed to-
wards the new position and there is a relatively big discrepancy
between µand q that gets reduced as the goal is reached.
Figures 4(e) and 4(f) show the paths taken by the active
inference and the inverse kinematics algorithm. In both al-
gorithms, the robot reached all targets, but there was a clear
difference in the discrepancy between the calculated and the
visual perception position. Inverse kinematics paths show the
error between the model and the physical robot (distance
between the orange and the green line of Fig. 4(f)). Our
approach reduced this distance when approaching the target
location in a closed-loop manner (Fig. 4(e)), while inferring
its real body conﬁguration and using head object tracking.
D. System dynamics when reaching a moving object
We evaluated the algorithm in a reaching task of a moving
object manually operated by a human. The experiment started
with the object near the robot and it was progressively moved
away and in an ascendant vertical direction, to ﬁnally be
handed to the robot in an intermediate and lower location.
This experiment was performed both in 2D and 3D visual
space. In 2D space, the setup used monocular vision from the
left eye and the torso was blocked. Our model was tested for
both arms and using active head object tracking. In 3D space,
we enabled the torso motion but restricted the model for the
right arm and the head. Figure 6 shows the vision of the robot
and the free energy values for the head and the arm during the
reaching experiment in 2D space. F is minimised modifying
the arm perception and generating actions towards the object.
The resulting variable dynamics of the 3D space experiment
are shown in Fig. 5. The initial position of the right arm
lied outside of the visual plane (Fig. 5(f) showing zero error).
When there was no visual input, the free energy optimization
algorithm only relied on joint measurements and the forward
model to produce the reaching motion until the right hand
appears in the visual plane. Figures 5(a) and 5(b) show both
the encoders measurements q and the estimated joint angle
µ of the arm and head. Figure 5(c) shows how calculated
gv(µ) and real sv 3D positions of the right arm end-effector
Free-energy(right arm)
Right arm
Head
0 10 20 30 40 50
0.5
1.0
1.5
2.0
2.5
3.0
3.5
0.01
0.02
0.03
0.04
0.05
0.06
Free-energy(head)
Time(s)
Fig. 6. Reaching a moving object in 2D. (Left) Robot vision: visual feature
(red), predicted end-effector location (green), object (blue). (Right) Free
energy optimization for the arm (orange) and the head (red).
follow the perceptual attractor ρ. Although the cameras were
previously calibrated, noise was present in the stereo recon-
struction of visual perception. This noise was more relevant in
the depth estimation (ﬁrst term in Fig. 5(f)). Stop action for
this experiment was produced by the sense of touch. Contact
in the hand pressure sensors triggers the grasping motion.
VII. A CTIVE INFERENCE IN REAL ARTIFICIAL AGENTS
Being able to develop robots with biologically plausible
body perception and action models allow us to validate
current brain-inspired computational models [26], [27]. How-
ever, there is a big gap between the theoretical mathematical
construct and physical reality. This work reduces this gap
and shows the plausibility of implementing the free energy
principle [6] on artiﬁcial agents for real-world applications,
joining together robotics and computational neuroscience.
The reproduced robot behaviour, during the system dynamics
study (Sec. VI-D), resembled a one-year-old infant. Just by
prediction error minimization, in this case, through surprise
minimization, the robot was able to perform human-like
movements in a reaching and object tracking task. The model
and the experiments can be reproduced and replicated by
downloading the open-source code tobereleased.
A. Relevant model predictions for human body perception
Interestingly, our model predicts the appearance of invol-
untary actions in a sensorimotor conﬂict situation, such as
the rubber-hand illusion. In the presence of sensory errors
(arm location expectation mismatch), we observed movements
towards the visual feature of the arm. In previous work, we
showed that the proprioceptive drift (mislocalization of the
hand) using a similar free energy optimization algorithm could
be modelled [25]. By adding the action term of active infer-
ence, forces towards minimising the prediction error should
appear, i.e., in the same direction of the drift. This behaviour
could be in charge of the online adaptation of movements
in the presence of disturbances or not trained conditions,
bypassing the voluntary movement pathway.
B. Technical challenges for a full-ﬂedged body model
Closed-loop adaptive perception and action is an important
advantage. The model is assumed to be an approximation
of the reality and the minimization of the variational free
energy tackles both sensory noise and model bias. In fact,
the equilibrium point where the algorithm converges reduces
the differences between the model and the real observations.
However, it is prone to local minima. As there are no task
hierarchies, dual arm reaching produced redundant motions in
both arms. A hierarchical version of the proposed approach,
as proposed in [28], could circumvent this limitation by
generating attractors for specialised behaviours.
The engineering analysis needed on iCub speciﬁc param-
eters and actuation was the biggest limitation for generaliza-
tion. Sensor variances, action gains and velocity limits were
experimentally tuned. In theory, variances and gains could be
also optimised within the free energy framework [7]. Besides,
we introduced how time discretization in a velocity control
scheme can be used to simplify the action computation. In
force controlled non-linear systems the mapping between the
sensory consequence and the force applied must be learnt [19]
or explicitly computed through the forward dynamics [18],
considerably increasing the complexity of the optimization
framework.
The proposed algorithm is scalable to integrate features
from different modalities (i.e., adding a new term to the
summation) and handles sensory fusion in both perception
and action. Furthermore, the needed generative functions can
be easily computed from the forward kinematics or be even
replaced by learned ones, as shown in [16]. However, more
complex sensory input features should be incorporated to be
able to compare with other bio-inspired approaches such as
deep reinforcement learning.
VIII. C ONCLUSIONS
This work presented a neuroscience-inspired closed-loop al-
gorithm for body perception and action working in a humanoid
robot. Our active inference model is the ﬁrst one validated on
a real robot for upper-body reaching and active head object
tracking. Robots perceiving and interacting under the active
inference approach is an important step for evaluating the plau-
sibility of the model in biological systems as we systematically
observe the behaviours and connect them to the mechanisms
behind, and even make predictions. As a proof of concept,
we showed its robustness to sensory noise and discrepancies
between the robot model and the real body, its adaptive char-
acteristic to visual online changes and its capacity to embed
sensory fusion. Body perception was approached as a hidden
Markov model and the unobserved body conﬁguration/state
was continuously approximated with visual and proprioceptive
inputs by minimising the free energy bound. The action was
modelled within perception as a reﬂex to reduce the prediction
error. We tested the algorithm on the iCub robot in 2D and 3D
using binocular vision. Results showed that our approach was
capable of combining different independent sources of sensory
information without an increase of computational complexity,
displaying online adaptation capabilities and performing more
accurate reaching behaviours than inverse kinematics
REFERENCES
[1] H. v. Helmholtz, Handbuch der physiologischen Optik . L. V oss, 1867.
[2] D. C. Knill and A. Pouget, “The bayesian brain: the role of uncertainty
in neural coding and computation,” TRENDS in Neurosciences, vol. 27,
no. 12, pp. 712–719, 2004.
[3] K. Friston, “A theory of cortical responses,” Philos Trans R Soc Lond
B: Biological Sciences , vol. 360, no. 1456, pp. 815–836, 2005.
[4] T. Parr and K. J. Friston, “Uncertainty, epistemics and active inference,”
Journal of The Royal Society Interface , vol. 14, no. 136, p. 20170376,
2017.
[5] E. Todorov and M. I. Jordan, “Optimal feedback control as a theory of
motor coordination,” Nature neuroscience, vol. 5, no. 11, p. 1226, 2002.
[6] K. J. Friston, “The free-energy principle: a uniﬁed brain theory?” Nature
Reviews. Neuroscience, vol. 11, pp. 127–138, 02 2010.
[7] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
behavior: a free-energy formulation,” Biological cybernetics , vol. 102,
no. 3, pp. 227–260, 2010.
[8] M. Kirchhoff, T. Parr, E. Palacios, K. Friston, and J. Kiverstein, “The
markov blankets of life: autonomy, active inference and the free energy
principle,” Journal of The royal society interface , vol. 15, no. 138, p.
20170792, 2018.
[9] C. Fantacci, U. Pattacini, V . Tikhanoff, and L. Natale, “Visual end-
effector tracking using a 3d model-aided particle ﬁlter for humanoid
robot platforms,” in 2017 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) . IEEE, 2017, pp. 1411–1418.
[10] C. Garcia Cifuentes, J. Issac, M. Wthrich, S. Schaal, and J. Bohg,
“Probabilistic articulated real-time tracking for robot manipulation,”
IEEE Robotics and Automation Letters , vol. 2, 2017.
[11] P. Vicente, L. Jamone, and A. Bernardino, “Online body schema adap-
tation based on internal mental simulation and multisensory feedback,”
Frontiers in Robotics and AI , vol. 3, p. 7, 2016.
[12] C. Gaskett and G. Cheng, “Online learning of a motor map for humanoid
robot reaching,” in 2nd Int. Conf. on computational inte., robotics and
autonomous systems, Singapore , 2003.
[13] L. Jamone, M. Brandao, L. Natale, K. Hashimoto, G. Sandini, and
A. Takanishi, “Autonomous online generation of a motor representation
of the workspace for intelligent whole-body reaching,” Robotics and
Autonomous Systems, vol. 62, no. 4, pp. 556–567, 2014.
[14] A. Roncone, M. Hoffmann, U. Pattacini, L. Fadiga, and G. Metta,
“Peripersonal space and margin of safety around the body: learning
visuo-tactile associations in a humanoid robot with artiﬁcial skin,” PloS
one, vol. 11, no. 10, p. e0163713, 2016.
[15] P. Lanillos, E. Dean-Leon, and G. Cheng, “Yielding self-perception in
robots through sensorimotor contingencies,” IEEE Trans. on Cognitive
and Developmental Systems , no. 99, pp. 1–1, 2016.
[16] P. Lanillos and G. Cheng, “Adaptive robot body learning and estimation
through predictive coding,” Intelligent Robots and Systems (IROS), 2018
IEEE/RSJ Int. Conf. on , 2018.
[17] M. Baltieri and C. L. Buckley, “An active inference implementation of
phototaxis,” 2018 Conference on Artiﬁcial Life, no. 29, pp. 36–43, 2017.
[18] L. Pio-Lopez, A. Nizard, K. Friston, and G. Pezzulo, “Active inference
and robot control: a case study,” J R Soc Interface , vol. 13, 2016.
[19] P. Lanillos and G. Cheng, “Active inference with function learning for
robot body perception,” International Workshop on Continual Unsuper-
vised Sensorimotor Learning, ICDL-Epirob , 2018.
[20] M. J Beal and Z. Ghahramani, “The variational bayesian em algorithm
for incomplete data,” Statistics, 07 2002.
[21] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The
Annals of Mathematical Statistics , vol. 22, no. 1, pp. 79–86, 03 1951.
[22] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth, “The free energy
principle for action and perception: A mathematical review,” Journal of
Mathematical Psychology, 2017.
[23] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, and W. Penny,
“Variational free energy and the laplace approximation,” Neuroimage,
vol. 34, no. 1, pp. 220–234, 2007.
[24] G. Metta, G. Sandini, D. Vernon, L. Natale, and F. Nori, “The icub
humanoid robot: An open platform for research in embodied cognition,”
Performance Metrics for Intelligent Systems Workshop , 01 2008.
[25] N.-A. Hinz, P. Lanillos, H. Mueller, and G. Cheng, “Drifting perceptual
patterns suggest prediction errors fusion rather than hypothesis selec-
tion: replicating the rubber-hand illusion on a robot,” arXiv preprint
arXiv:1806.06809, 2018.
[26] P. Lanillos, E. Dean-Leon, and G. Cheng, “Enactive self: a study
of engineering perspectives to obtain the sensorimotor self through
enaction,” in Developmental Learning and Epigenetic Robotics, Joint
IEEE Int. Conf. on , 2017.
[27] M. Hoffmann and R. Pfeifer, “Robots as powerful allies for the study
of embodied cognition from the bottom up,” in The Oxford Handbook
of 4E Cognition , 2018.
[28] J. Tani, “Learning to generate articulated behavior through the bottom-
up and the top-down interaction processes,” Neural Networks, vol. 16,
no. 1, pp. 11–23, 2003.