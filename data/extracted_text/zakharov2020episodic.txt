arXiv:2010.01430v1  [cs.LG]  3 Oct 2020
EP I S O D I C ME M O RY F O R LE A R N I N G SU B J E C T I VE -
TI M E S C A L E MO D E L S
Alexey Zakharov∗
Imperial College London &
Emotech Labs
az519@ic.ac.uk
Matthew Crosby
Leverhulme Centre for the Future of Intelligence,
Imperial College London
m.crosby@imperial.ac.uk
Zafeirios Fountas
Emotech Labs &
WCHN, University College London
f@emotech.co
ABSTRACT
In model-based learning, an agent’s model is commonly deﬁne d over transitions
between consecutive states of an environment even though pl anning often requires
reasoning over multi-step timescales, with intermediate s tates either unnecessary,
or worse, accumulating prediction error. In contrast, inte lligent behaviour in bi-
ological organisms is characterised by the ability to plan o ver varying temporal
scales depending on the context. Inspired by the recent work s on human time
perception, we devise a novel approach to learning a transit ion dynamics model,
based on the sequences of episodic memories that deﬁne the ag ent’s subjective
timescale – over which it learns world dynamics and over which future pl anning
is performed. W e implement this in the framework of active in ference and demon-
strate that the resulting subjective-timescale model (STM ) can systematically vary
the temporal extent of its predictions while preserving the same computational ef-
ﬁciency. Additionally, we show that STM predictions are mor e likely to introduce
future salient events (for example new objects coming into v iew), incentivising
exploration of new areas of the environment. As a result, STM produces more
informative action-conditioned roll-outs that assist the agent in making better de-
cisions. W e validate signiﬁcant improvement in our STM agen t’s performance in
the Animal-AI environment against a baseline system, train ed using the environ-
ment’s objective-timescale dynamics.
1 I NTRODUC TI ON
An agent endowed with a model of its environment has the abili ty to predict the consequences of
its actions and perform planning into the future before deci ding on its next move. Models can
allow agents to simulate the possible action-conditioned f utures from their current state, even if the
state was never visited during learning. As a result, model-based approaches can provide agents
with better generalization abilities across both states an d tasks in an environment, compared to their
model-free counterparts (Racani` ere et al., 2017; Mishra et al., 2017) .
The most popular framework for developing agents with inter nal models is model-based rein-
forcement learning (RL). Model-based RL has seen great prog ress in recent years, with a num-
ber of proposed architectures attempting to improve both th e quality and the usage of these mod-
els (Kaiser et al., 2020; Racani` ere et al., 2017; Kansky et a l., 2017; Hamrick, 2019). Nevertheless,
learning internal models affords a number of unsolved probl ems. The central one of them is model-
bias, in which the imperfections of the learned model result in unwanted over-optimism and sequen-
tial error accumulation for long-term predictions (Deisen roth & Rasmussen, 2011). Long-term pre-
dictions are additionally computationally expensive in en vironments with slow temporal dynamics,
∗ Corresponding author.
1
Episodic Memory for Subjective-Timescale Models
given that all intermediary states must be predicted. Moreo ver, slow world dynamics 1 can inhibit the
learning of dependencies between temporally-distant even ts, which can be crucial for environments
with sparse rewards. Finally, the temporal extent of future predictions is limited to the objective
timescale of the environment over which the transition dyna mics has been learned. This leaves little
room for ﬂexible and context-dependent planning over varyi ng timescales which is characteristic to
animals and humans (Clayton et al., 2003; Cheke & Clayton, 20 11; Buhusi & Meck, 2005).
The ﬁnal issue exempliﬁes the disadvantage of the classical view on internal models, in which they
are considered to capture the ground-truth transition dyna mics of the environment. Furthermore,
in more complex environments with ﬁrst-person observation s, this perspective does not take into
account the apparent subjectivity of ﬁrst-person experien ces. In particular, the agent’s learned repre-
sentations of the environment’s transition dynamics impli citly include information about time. Little
work has been done to address the concept of time perception i n model-based agents (Deverett et al.,
2019). Empirical evidence from the studies of human and anim al cognition suggests that intelligent
biological organisms do not perceive time precisely and do n ot possess an explicit clock mechanism
responsible for keeping track of time (Roseboom et al., 2019 ; Sherman et al., 2020; Hills, 2003). For
instance, humans tend to perceive time slower in environmen ts rich in perceptual content (e.g. busy
city), and faster in environments with little perceptual ch ange (e.g. empty ﬁeld). The mechanisms
of subjective time perception still remain unknown; howeve r, recent computational models based on
episodic memory were able to closely model the deviations of human time perception from veridical
perception (Fountas et al., 2020b).
Inspired by this account, in this work we propose subjective-timescale model (STM), an alterna-
tive approach to learning a transition dynamics model, by re placing the objective timescale with a
subjective one. The latter represents the timescale by whic h an agent perceives events in an envi-
ronment, predicts future states, and which is deﬁned by the s equences of episodic memories. These
memories are accumulated on the basis of saliency (i.e. how p oorly an event was predicted by the
agent’s transition model), which attempts to mimic the way h umans perceive time, and resulting in
the agent’s ability to plan over varying timescales and cons truct novel future scenarios.
W e employ active inference as the agent’s underlying cognit ive framework. Active inference is an
emerging framework within computational neuroscience, wh ich attempts to unify perception and
action under the single objective of minimising the free-en ergy functional. Similar to model-based
RL, an active inference agent relies almost entirely on the c haracteristics and the quality of its inter-
nal model to make decisions. Thus, it is naturally susceptib le to the previously mentioned problems
associated with imperfect, objective-timescale models. T he selection of active inference for the pur-
poses of this paper is motivated by its biological plausibil ity as a normative framework for under-
standing intelligent behaviour (Friston et al., 2017a; 200 6), which is in line with the general theme
of this work. Furthermore, being rooted in variational infe rence, the free energy objective generates
a distinct separation between the information-theoretic q uantities that correspond to the different
components of the agent’s model, which is crucial to deﬁne th e memory formation criterion.
W e demonstrate that the resulting characteristics of STM al low the agent to automatically perform
both short- and long-term planning using the same computati onal resources and without any explicit
mechanism for adjusting the temporal extent of its predicti ons. Furthermore, for long-term predic-
tions STM systematically performs temporal jumps (skippin g intermediary steps), thus providing
more informative future predictions and reducing the detri mental effects of one-step prediction error
accumulation. Lastly, being trained on salient events, STM much more frequently imagines futures
that contain epistemically-surprising events, which ince ntivises exploratory behaviour.
2 R ELATED WORK
Model-based RL. Internal models are extensively studied in the ﬁeld of model -based RL. Us-
ing linear models to explicitly model transition dynamics h as achieved impressive results in
robotics (Levine & Abbeel, 2014a; W atter et al., 2015; Bagne ll & Schneider, 2001; Abbeel et al.,
2006; Levine & Abbeel, 2014b; Levine et al., 2016; Kumar et al ., 2016). In general, however, their
application is limited to low-dimensional domains and rela tively simple environment dynamics.
Similarly, Gaussian Processes (GPs) have been used (Deisen roth & Rasmussen, 2011; Ko et al.,
1 W orlds with small change in state given an action
2
Episodic Memory for Subjective-Timescale Models
2007). Their probabilistic nature allows for state uncerta inty estimation, which can be incorporated
in the planning module to make more cautious predictions; ho wever, GPs struggle to scale to high-
dimensional data. An alternative and recently more prevale nt method for parametrising transition
models is to use neural networks. These are particularly att ractive due to their recent proven success
in a variety of domains, including deep model-free RL (Silve r et al., 2017), ability to deal with high-
dimensional data, and existence of methods for uncertainty quantiﬁcation (Blundell et al., 2015;
Gal & Ghahramani, 2016). Different deep learning architect ures have been utilised including fully-
connected neural networks (Nagabandi et al., 2018; Feinber g et al., 2018; Kurutach et al., 2018) and
autoregressive models (Ha & Schmidhuber, 2018; Racani` ere et al., 2017; Ke et al., 2019), showing
promising results in environments with relatively high-di mensional state spaces. In particular, au-
toregressive architectures, such as Long Short-T erm Memor y (LSTM) (Hochreiter & Schmidhuber,
1997), are capable of modelling non-Markovian environment s and of learning temporal dependen-
cies. Nevertheless, LSTMs are still limited in their abilit y to learn relations between temporally-
distant events, which is exacerbated in environments where little change occurs given an action.
Uncertainty quantiﬁcation using ensemble methods (Kalwei t & Boedecker, 2017; Clavera et al.,
2020; Buckman et al., 2018) or Bayesian neural networks (McA llister & Rasmussen, 2016;
Depeweg et al., 2017) have been proposed to tackle model bias and sequential error accumulation.
Other works have focused on techniques to create more accura te long-term predictions. Mishra et al.
(2017) used a segment-based approach to predict entire traj ectories at once in an attempt to avoid
one-step prediction error accumulation. A work by Ke et al. ( 2019) used an autoregressive model
and introduced a regularising auxiliary cost with respect t o the encodings of future observations,
thus forcing the latent states to carry useful information f or long-horizon predictions. In contrast, the
work presented in this paper re-focuses the objective from a ttempting to create better parametrisa-
tion techniques or mitigating methods to simply transformi ng the timescale over which the dynamics
of an environment is learned. As will be seen, our approach ca n lead to more accurate and efﬁcient
long-term predictions without compromising agent’s abili ty to plan over short time-horizons.
Episodic Memory . In neuroscience, episodic memory is used to describe autobi ographical memo-
ries that link a collection of ﬁrst-person sensory experien ces at a speciﬁc time and place (Tulving,
1972). Past studies in the ﬁeld suggest that episodic memory plays an important role in human
learning (Mahr & Csibra, 2017), and may capture a wide range o f potential functional purposes,
such as construction of novel future scenarios (Schacter et al., 2007; 2012; Hassabis et al., 2007),
mental time-travel (Michaelian, 2016) or assisting in the f ormation of new semantic memories
(Greenberg & V erfaellie, 2010). A recent computational mod el of episodic memory (Fountas et al.,
2020b) also relates it to the human ability to estimate time d urations.
The application of episodic memory in reinforcement learni ng has been somewhat limited. Some
works have employed simple forms of memory to improve the per formance of a deep model-free
RL agent via experience replay (Mnih et al., 2015; Espeholt e t al., 2018; Schaul et al., 2016). How-
ever, these methods do not incorporate information about as sociative or temporal dependencies be-
tween the memories (Hansen et al., 2018). Read-write memory banks have also been implemented
alongside gradient-based systems (memory-augmented neur al networks) for assisting in learning
and prediction (Graves et al., 2014; 2016; Hung et al., 2019; Oh et al., 2016; Jung et al., 2018). Fur-
ther, episodic memory has been used for non-parametric Q-fu nction approximation (Blundell et al.,
2016; Pritzel et al., 2017; Hansen et al., 2018; Zhu et al., 20 20). It has also been proposed to be
used directly for control as a faster and more efﬁcient alter native to model-based and model-free
approaches in RL, such as instance-based control (Lengyel & Dayan, 2007; Botvinick et al., 2019;
Gershman & Daw, 2017) and one-shot learning (Kaiser et al., 2 017). In contrast, our paper consid-
ers a novel way of using episodic memories – in deﬁning the age nt’s subjective timescale of the
environment and training a transition dynamics model over t he sequences of these memories.
Active Inference. Until now , most of the work on active inference has been done i n low-dimensional
and discrete state spaces (Friston et al., 2015; 2017b;c;d) . Recently, however, there has been a rising
interest in scaling active inference and applying it to envi ronments with continuous and/or large state
spaces (Fountas et al., 2020a; Tschantz et al., 2019; C ¸ atal et al., 2019; Millidge, 2019; Ueltzh ¨ offer,
2018). Although these works used deep learning techniques, their generative models have so far
been designed to be Markovian and trained over the objective timescale of the environment.
3
Episodic Memory for Subjective-Timescale Models
3 B ASELINE ARCHITEC TU R E
W e take the deep active inference system devised by Fountas e t al. (2020a) as the starting point with
a few architectural and operational modiﬁcations. The gene rative model of this baseline agent is
deﬁned as p(o1:t, s1:t, a1:t; θ), where s t denotes latent states at time t, o t (visual) observations, a t
actions, and θ = {θo, θs} the parameters of the model. s t is assumed to be Gaussian-distributed
with a diagonal covariance, o t follows Bernoulli and a 1:t categorical distributions. For a single
time step, as illustrated in Figure 1A, this generative mode l includes two factors, a transition model
p(st|st−1, at−1; θs) and a latent state decoder p(ot|st; θo) parametrised by feed-forward neural net-
works with parameters θs and θo, respectively. W e modify the transition model from the orig inal
study to predict the change in state , rather than the full state 2 .
The agent also possesses two inference networks, which are t rained using amortized inference: a ha-
bitual network q(at; φa) and observation encoder q(st; φs) parametrised by φa and φs, respectively.
The habitual network acts as a model-free component of the sy stem, learning to map inferred states
directly to actions. Following Fountas et al. (2020a), the v ariational free energy for an arbitrary
time-step t is deﬁned as:
Ft = − Eq(st) [log p(ot|st; θo)] (1a)
+ DKL [q(st; θs)∥p(st|st−1, at−1; θs)] (1b)
+ Eq(st) [DKL [q(at; φa)∥p(at)]] (1c)
where p(a) = ∑
π :a1=a p(π) is the summed probability of all policies beginning with act ion a.
All the divergence terms are computable in closed-form, giv en the assumption about Gaussian- and
Bernoulli-distributed variables. Finally, the expected f ree energy (EFE) of the generative model up
to some time horizon T can be deﬁned as:
G(π) =
T∑
τ =t
G(π, τ ) =
T∑
τ =t
E˜q [log q(sτ , θ|π) − log p(oτ , sτ , θ|π)] , (2)
where ˜q = q(oτ , sτ , θ|π) and p(oτ , sτ , θ|π) =p(oτ |π)q(sτ |oτ , π)p(θ|sτ , oτ , π).
T o make expression 2 computationally feasible, it is decomp osed such that,
G(π, τ ) =− Eq(θ|π )q(sτ |θ,π )q(oτ |sτ ,θ,π ) [log p(oτ |π)]
+ Eq(θ|π )
[
Eq(oτ |θ,π )H(sτ |oτ , π) − H(sτ |π)
]
+ Eq(θ|π )q(sτ |θ,π ) [H(oτ |sτ , θ, π )]
− Eq(sτ |π ) [H(oτ |sτ , π)] ,
(3)
where expectations can be taken by performing sequential sa mpling of θ, sτ and oτ and entropies are
calculated in closed-form using standard formulas for Bern oulli and Gaussian distributions. Network
parameters, θ, are sampled using Monte Carlo (MC) dropout (Gal & Ghahraman i, 2016).
The system also makes use of top-down attention mechanism by introducing variable ω, which
modulates uncertainty about hidden states, promoting late nt state disentanglement and more ef-
ﬁcient learning. Speciﬁcally, the latent state distributi on is deﬁned as a Gaussian such that
s ∼ N (s; µ , Σ /ω), where µ and Σ are the mean and the diagonal covariance, and ω is a decreasing
logistic function over the divergence DKL [q(a; φa)∥p(a)].
Finally, action selection is aided with Monte Carlo tree sea rch (MCTS), ensuring a more efﬁcient
trajectory search. Speciﬁcally, MCTS generates a weighted tree that is used to sample policies from
the current timestep, where the weights refer to the agent’s estimation of the EFE given a state-action
pair, ˜G(s, a). The nodes of the tree are predicted via the transition model , p(st|st−1, at−1; θs). At
the end of the search, MCTS is used to construct the action pri or, p(a) = N(ai, s)/ ∑
j N(aj , s),
where N(a, s) is the number of times action a has been taken from state s.
The baseline agent is trained with prioritised experience r eplay (PER) (Schaul et al., 2016) to mit-
igate the detrimental consequences of on-line learning (wh ich was used in the original paper), and
to encourage better object-centric representations. The d etails of the baseline implementation and
training with PER can be found in Appendices B.1 and B.2, resp ectively.
2 This has largely become common practice in the ﬁeld of model- based RL (Nagabandi et al., 2018), im-
proving algorithm efﬁciency and accuracy especially in env ironments with slow temporal dynamics.
4
Episodic Memory for Subjective-Timescale Models
Figure 1: A. Baseline generative model. B. STM generative model with additional deterministic
hidden states h introduced by an LSTM.
4 S UBJECT I VE -T IM E SC AL E MODEL
W e introduce subjective-timescale model (STM) that record s sequences of episodic memories over
which a new transition model is trained. As such, the system c onsists of a memory accumulation
system to selectively record salient events, a simple actio n heuristic to summarise sequences of
actions between memories, and an autoregressive transitio n model.
W e deﬁne a ground-truth sequence as a sequence of all states experienced in an environment during
a single episode, Sg = {s0, s1, s2, ..., sT }, and an S-sequence (subjective sequence) as a sequence of
states selectively picked by our system, and over which the n ew transition model would be learned,
Se = {sτ1 , sτ2 , sτ3 , ..., sτN }. Each unit in an S-sequence is called an episodic memory and consists
of a set of sufﬁcient statistics, s = {µ s, σ s}, where µ s and σ s are mean and variance vectors of a
Gaussian-distributed state s, respectively. Additionally, each episodic memory contai ns a reference
to its preceding (parent) episodic memory and all actions un til the next one. The process of recording
S-sequences is called memory accumulation .
4.1 M E M O RY ACCU M U L AT IO N
Previous work on time perception and episodic memory (Fount as et al., 2020b) employed saliency
of an event, or the generative model’s prediction error, as t he memory formation criterion. Selection
of this criterion is informed by the experimental evidence f rom neuroscience on episodic memory
(Greve et al., 2017; Jang et al., 2018; Rouhani et al., 2018). Inspired by this account, our memory
accumulation system employs the free energy of the objectiv e-timescale transition model 3 (Eq.1b)
as a measure of event saliency, and forms memories when a pre- deﬁned threshold is exceeded.
T o train STM, an active inference agent moves in the environm ent under a pre-trained generative
model described in Section 3. During this process, each tran sition is evaluated based on the objective
transition model free energy, DKL [q(st; θs)∥p(st|st−1, at−1; θs)], which represents the degree of
surprise experienced by the transition model upon taking an action. If the value of the free energy
exceeds a pre-deﬁned threshold, ǫ, a memory is formed and placed into an S-sequence. At the end
of each episode, the recorded S-sequence is saved for later u se.
W e can categorise the transitions that cause higher values o f transition model free energies into two
main groups: epistemic surprise and model-imperfection su rprise. The former refers to transitions
that the model could not have predicted accurately due to the lack of information about the current
state of the environment (e.g. objects coming into view). Th e latter refers to the main bulk of
these high prediction-error transitions and stems from the inherent imperfections of the learned
dynamics. Speciﬁcally, less frequently-occurring observ ations with richer combinatorial structure
would systematically result in higher compounded transiti on model errors, given that these would
be characteristic of more complex scenes. As will become app arent, the presence of these two
3 Components of the total free energy correspond to a measure of belief update for each of the networks,
and therefore, loosely speaking, quantify the prediction e rror generated by each of the respective system con-
stituents: autoencoder (Eqs.1a, 1b), objective-timescal e transition model (Eq.1b), and habitual network (Eq.1c).
5
Episodic Memory for Subjective-Timescale Models
categories in the recorded S-sequences results in the model ’s ability to vary its prediction timescale
based on the perceptual context and systematically imagine future salient events.
Figure 2: STM pipeline. (A) As the agent moves through the environment, states s that exceeded
a pre-deﬁned threshold are recorded along with all successi ve actions a in an S-sequence. (B) S-
sequences are saved in a buffer at the end of each episode. (C) S-sequences are sampled for training
a subjective-timescale transition model.
A transition dynamics model is necessarily trained with res pect to actions that an agent took to reach
subsequent states. However, STM records memories over an ar bitrary number of steps, thus leaving
action sequences of variable length. For the purposes of thi s paper, we implement a simple heuristic
to summarise agent’s trajectories, which is enough to provi de STM with the necessary information
to learn action-conditioned predictions. W e do it by estima ting the angle between the agent’s initial
position and its ﬁnal position at the time-step of the subseq uent memory. Full details of this heuristic
can be found in Appendix B.4.
4.2 T RA N S IT IO N DY NA M ICS MO D E L
As mentioned, S-sequences are characterised by the presenc e of epistemically-surprising and salient
events squeezed together in the recorded episodes. As a resu lt, training on these sequences is more
conducive for learning temporal dependencies between important states. For this reason, we train
an LSTM model over the S-sequences, which utilises internal memory states to store information
about preceding inputs. In our architecture, an LSTM calcul ates hidden state hτ at subjective time
τ using a deterministic mapping,
hτ = fθh (sτ , aτ , hτ −1) =σ(xτ Wh + hτ −1Uh + bh), (4)
where sτ and aτ are the latent state and action taken at subjective time τ respectively, xτ is the con-
catenated vector of sτ and aτ , and θh = {Wh, Uh, bh} are deterministic LSTM model parameters.
Importantly, function fθs is deterministic and serves only to encode information abou t preceding
steps into the hidden state of the LSTM. This hidden state hτ is then mapped to a latent state sτ +1 at
the next subjective time τ + 1via a feed-forward neural network with random-variable par ameters,
θhs, using p(sτ |hτ −1; θhs) with MC dropout. The parameters of both of the networks are tr ained via
backpropagation with a loss function deﬁned as
L = 1
T
T∑
τ
DKL
[
q(sτ +1; φs)∥p(sτ +1|fθh (sτ , aτ , hτ −1); θhs)
]
(5)
The new generative model of observations is shown in Figure 1 B. Because the mapping of LSTM
is deterministic, the formulation of the variational free e nergy remains intact with the exception of
the second term that now includes the state prediction produ ced by the network p(sτ |hτ −1; θhs)
conditioned on the hidden state of the LSTM,
Fτ = − Eq(sτ ) [log p(oτ |sτ ; θo)]
+ DKL [q(sτ ; φs)∥p(sτ |hτ −1; θhs)]
+ Eq(sτ ) [DKL [q(aτ ; φa)∥p(aτ )]]
(6)
Architectural and training details of the model can be found in Appendix B.3. The source code will
be made available after the review process.
6
Episodic Memory for Subjective-Timescale Models
4h 32min
14h 20min
5h 57min
A B
Figure 3: Experimental Results. (A) Cumulative rewards collected by the agents. It can be seen th at
the STM-MCTS agent shows improved performance even when com pared with the computationally-
expensive Baseline-MPC. (B) Mean number of rewards (spheres) by category. STM-MCTS coll ects
more green and yellow (positive) rewards than the baseline a gents. However, Baseline-MPC collects
fewer (negative) red rewards, which is likely related its ab ility to evaluate actions after every step.
Uncertainty regions and bars indicate one standard deviati on over 5 runs.
5 E XPERIME NT S
The Animal-AI (AAI) environment is a virtual testbed that pr ovides an open-ended sandbox training
environment for interacting with a 3D environment from ﬁrst -person observations (Crosby et al.,
2020; Crosby, 2020). In AAI, an agent is tasked with reaching a green sphere given a particular setup
that may include intermediary rewards (yellow spheres), te rminal negative rewards (red spheres),
obstacles (e.g. walls), etc. For the purposes of this work, w e use a sparsely populated conﬁguration
with single green, red, and yellow spheres, in which a succes sful agent would be forced to perform
both short- and long-distance planning, as well as more exte nsive exploration of the environment.
5.1 E X P E RIM E N TA L RE S U LT S
W e tested the STM agent using 100,000 steps in randomly-gene rated environments (max episode
length of 500) against the baseline system with two differen t planning procedures – MCTS and
model predictive control (MPC). In contrast to MCTS, the MPC agent re-evaluates its plan after
every action. Figure 3 summarises the experimental results . Our STM-MCTS agent outperforms
the baseline systems in acquiring more rewards within the 10 0,000 steps. In particular, we note
that the STM-MCTS agent showed signiﬁcant improvement agai nst the Baseline-MCTS. Similarly,
we show that STM-MCTS model retrieves more cumulative rewar d than the Baseline-MPC agent,
which uses a computationally expensive planning procedure . Speciﬁcally, our agent achieves a
higher cumulative reward in less than half the time, ∼6 hours, compared to ∼14 hours.
5.2 R O L L -O U T IN S P E CT IO N
Inspecting prediction roll-outs produced by the STM-based system provides great insight into its
practical beneﬁts for the agent’s performance. Speciﬁcall y, our agent is capable of varying the
temporal extent of its predictions and imagining future sal ient events.
5.2.1 V A RY IN G PRE D ICT IO N TIM E S CA L E
Much like human perception of time changes depending on the p erceptual content of the surround-
ings, our agent varies the prediction timescale depending o n the context it ﬁnds itself in. Specif-
ically, in the AAI environment the complexity of any given ob servation is primarily driven by the
presence of objects, which may appear in different sizes, co lours, and conﬁgurations. As a result,
our agent consistently predicts farther into the future in t he absence of any nearby objects, and slows
its timescale, predicting at ﬁner temporal rate, when the ob jects are close.
Practically, this has several important implications. Fir st, performing temporal jumps and skipping
unnecessary intermediary steps affords greater computati onal efﬁciency, and reduces the detrimental
effects of sequential error accumulation, as can be seen in F igure 4. Second, while STM is able to
predict far ahead, its inherent ﬂexibility to predict over v arying timescales does not compromise the
7
Episodic Memory for Subjective-Timescale Models
agent’s performance when the states of interest are close. T hus, a separate mechanism for adjusting
how far into the future an agent should plan is not necessary a nd is implicitly handled by our model.
Third, STM allows the agent to make more informed decisions in an environment, as it tends to
populate the roll-outs with salient observations of the sho rt- and long-term futures depending on
the context. As a result, STM effectively re-focuses the cen tral purpose of a transition model from
most accurately modeling the ground-truth dynamics of an en vironment to predicting states more
informative with respect to the affordances of the environm ent.
Figure 4: STM can vary prediction timescale , with large gaps between predictions at the start,
which becomes more ﬁne-grained as the object gets closer. Ob jective-timescale model suffers from
slow-timescale predictions and error accumulation, resul ting in poorly-informative predictions.
5.2.2 I M AG IN IN G SU RP RIS IN G EV E N T S
As mentioned, S-sequences frequently include epistemical ly-surprising transitions, which, in the
context of the AAI environment, constitute events where obj ects come into view . As a result, STM
is signiﬁcantly more likely to include roll-outs with new ob jects appearing in the frame, in contrast
to the baseline that employs the objective-timescale trans ition model.
The ability of the STM to imagine novel and salient future eve nts encourages exploratory behaviour,
which is distinct from the active inference agent’s intrins ic exploratory motivations. W e again stress
that although the predicted futures may be inaccurate with r espect to the ground-truth positions of
the objects, they are nevertheless more informative with respect to the agent’s potential affordances
in the environment. This is in stark contrast with the object ive-timescale model, which imagines
futures in the absence of any objects. As a result, the STM age nt is less prone to get stuck in a
sub-optimal state, which was commonly observed in the basel ine system, and is more inclined to
explore the environment beyond its current position.
Figure 5: STM is able to imagine surprising events.Despite the fact that appearance of objects is a
rare event in the environment, STM frequently predicts them in the roll-outs. In contrast, objective-
timescale model is not capable of that as a direct corollary o f its training procedure.
6 C ONCLUSIO N AND FUTURE WORK
W e proposed STM, a novel approach to learning a transition dy namics model with the use of se-
quences of episodic memories, which deﬁne an agent’s more us eful, subjective timescale. STM
showed signiﬁcant improvement against the baseline agent’ s performance in the AAI environment.
Inspired by the problems of inaccurate and inefﬁcient long- term predictions in model-based RL and
the recent neuroscience literature on episodic memory and h uman time perception, we merged ideas
from the different ﬁelds into one new technique of learning a forward model. W e further emphasised
two important characteristics of the newly-devised model – its ability to vary the temporal extent of
future predictions and to predict future salient events. Th e application of our technique is not limited
to active inference, and can be adapted for use in other model -based frameworks.
8
Episodic Memory for Subjective-Timescale Models
Future work may explore more generalised approaches of acti on summarisation and dynamic thresh-
olding for memory formation. Another enticing direction of research is to investigate the feasibility
of having a single transition model that slowly transitions from training on an objective timescale to
training on a subjective timescale, as the memory formation goes on.
ACK N OW L E D G M E N T S
Matthew Crosby’s contribution to this work was supported by the Leverhulme Centre for the Future
of Intelligence, Leverhulme Trust, under Grant RC-2015-06 7.
REFERENC ES
Mart´ ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay G hemawat, Ian Goodfellow ,
et al. T ensorFlow: Large-scale machine learning on heterog eneous systems, 2015. URL
https://www.tensorflow.org/. Software available from tensorﬂow .org.
Pieter Abbeel, Morgan Quigley, and Andrew Y . Ng. Using inacc urate models in reinforcement
learning. In ICML ’06 , 2006.
J. Andrew Bagnell and Jeff G. Schneider. Autonomous helicop ter control using reinforcement learn-
ing policy search methods. Proceedings 2001 ICRA. IEEE International Conference on Ro botics
and Automation (Cat. No.01CH37164) , 2:1615–1620 vol.2, 2001.
Benjamin Beyret, Jos´ e Hern ´ andez-Orallo, Lucy G Cheke, Ma rta Halina, Murray Shanahan, and
Matthew Crosby. The Animal-AI Environment: Training and te sting animal-like artiﬁcial cogni-
tion. ArXiv, abs/1909.07483, 2019.
Charles Blundell, Julien Cornebise, K. Kavukcuoglu, and Da an Wierstra. W eight uncertainty in
neural networks. ArXiv, abs/1505.05424, 2015.
Charles Blundell, Benigno Uria, Alexander Pritzel, Y azhe L i, A vraham Ruderman, Joel Z. Leibo,
Jack W . Rae, Daan Wierstra, and Demis Hassabis. Model-free e pisodic control. ArXiv,
abs/1606.04460, 2016.
Matthew M Botvinick, Sam Ritter, Jane X. W ang, Zeb Kurth-Nel son, and Demis Hassabis. Rein-
forcement learning, fast and slow . T rends in cognitive sciences , 23 5:408–422, 2019.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevd o, and Honglak Lee. Sample-
efﬁcient reinforcement learning with stochastic ensemble value expansion. In NeurIPS, 2018.
Catalin V . Buhusi and W arren H. Meck. What makes us tick? func tional and neural mechanisms of
interval timing. Nature Reviews Neuroscience , 6:755–765, 2005.
L. Cheke and N. Clayton. Eurasian jays (garrulus glandarius ) overcome their current desires to
anticipate two distinct future needs and plan for them appro priately. Biology Letters , 8:171 – 175,
2011.
Ignasi Clavera, Y ao Fu, and Pieter Abbeel. Model-augmented actor-critic: Backpropagating through
paths. ArXiv, abs/2005.08068, 2020.
Nicola S. Clayton, Timothy J. Bussey, and Anthony Dickinson . Can animals recall the past and plan
for the future? Nature Reviews Neuroscience , 4:685–691, 2003.
Matthew Crosby. Building thinking machines by solving anim al cognition tasks. Minds and Ma-
chines, 2020.
Matthew Crosby, Benjamin Beyret, Murray Shanahan, Jos´ e He rn ´ andez-Orallo, Lucy Cheke, and
Marta Halina. The Animal-AI testbed and competition. volum e 123 of Proceedings of Machine
Learning Research , pp. 164–176. PMLR, 2020.
Marc Peter Deisenroth and Carl E. Rasmussen. Pilco: A model- based and data-efﬁcient approach
to policy search. In ICML, 2011.
9
Episodic Memory for Subjective-Timescale Models
Stefan Depeweg, Jos´ e Miguel Hern ´ andez-Lobato, Finale Do shi-V elez, and Steffen Udluft. Learn-
ing and policy search in stochastic dynamical systems with b ayesian neural networks. ArXiv,
abs/1605.07127, 2017.
Ben Deverett, Ryan Faulkner, Meire Fortunato, Greg W ayne, a nd Joel Z. Leibo. Interval timing in
deep reinforcement learning agents. ArXiv, abs/1905.13469, 2019.
Lasse Espeholt, Hubert Soyer, R´ emi Munos, Karen Simonyan, V olodymyr Mnih, T om W ard, Y otam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, an d Koray Kavukcuoglu. Im-
pala: Scalable distributed deep-rl with importance weight ed actor-learner architectures. ArXiv,
abs/1802.01561, 2018.
Vladimir Feinberg, Alvin W an, Ion Stoica, Michael I. Jordan , Joseph E. Gonzalez, and Sergey
Levine. Model-based value expansion for efﬁcient model-fr ee reinforcement learning. 2018.
Zafeirios Fountas, Noor Sajid, Pedro AM Mediano, and Karl Fr iston. Deep active inference agents
using monte-carlo methods. Accepted to NeurIPS , 2020a.
Zafeirios Fountas, Anastasia Sylaidi, Kyriacos Nikiforou , Anil K. Seth, Murray Shanahan, and W ar-
rick Roseboom. A predictive processing model of episodic me mory and time perception. bioRxiv,
2020b.
Karl J. Friston. A free energy principle for a particular phy sics. arXiv: Neurons and Cognition ,
2019.
Karl J. Friston, James Kilner, and Lee Harrison. A free energ y principle for the brain. Journal of
Physiology-P aris, 100:70–87, 2006.
Karl J. Friston, Francesco Rigoli, Dimitri Ognibene, Chris toph Mathys, Thomas H. B. FitzGerald,
and Giovanni Pezzulo. Active inference and epistemic value . Cognitive Neuroscience , 6:187 –
214, 2015.
Karl J. Friston, Thomas H. B. FitzGerald, F . Rigoli, P . Schwa rtenbeck, J. O’Doherty, and G. Pezzulo.
Active inference and learning. Neuroscience and Biobehavioral Reviews , 68:862 – 879, 2016.
Karl J. Friston, Thomas H. B. FitzGerald, F . Rigoli, P . Schwa rtenbeck, and G. Pezzulo. Active
inference: A process theory. Neural Computation , 29:1–49, 2017a.
Karl J. Friston, Thomas H. B. FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active inference: A process theory. Neural Computation , 29:1–49, 2017b.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni P ezzulo, J. Allan Hobson, and Sasha
Ondobaka. Active inference, curiosity and insight. Neural Computation , 29:2633–2683, 2017c.
Karl J. Friston, Richard E Rosch, Thomas Parr, Cathy J. Price , and Howard Bowman. Deep temporal
models and active inference. Neuroscience and Biobehavioral Reviews , 90:486 – 501, 2017d.
Y arin Gal and Zoubin Ghahramani. Dropout as a bayesian appro ximation: Representing model
uncertainty in deep learning. In ICML, 2016.
Samuel J. Gershman and Nathaniel D. Daw . Reinforcement lear ning and episodic memory in hu-
mans and animals: An integrative framework. Annual Review of Psychology , 68:101–128, 2017.
Alex Graves, Greg W ayne, and Ivo Danihelka. Neural turing ma chines. ArXiv, abs/1410.5401, 2014.
Alex Graves, Greg W ayne, Malcolm Reynolds, Tim Harley, Ivo D anihelka, Agnieszka Grabska-
Barwi ´ nska, Sergio G ´ omez Colmenarejo, Edward Grefenstet te, et al. Hybrid computing using a
neural network with dynamic external memory. Nature, 538:471–476, 2016.
Daniel L. Greenberg and Mieke V erfaellie. Interdependence of episodic and semantic memory:
evidence from neuropsychology. Journal of the International Neuropsychological Society : JINS,
16 5:748–53, 2010.
10
Episodic Memory for Subjective-Timescale Models
Andrea Greve, Elisa Cooper, Alexander Kaula, Michael C. And erson, and Richard N. A. Henson.
Does prediction error drive one-shot declarative learning ? Journal of Memory and Language , 94:
149 – 165, 2017.
David R Ha and J ¨ urgen Schmidhuber. Recurrent world models f acilitate policy evolution. In
NeurIPS, 2018.
Jessica B. Hamrick. Analogues of mental simulation and imag ination in deep learning. Current
Opinion in Behavioral Sciences , 29:8–16, 2019.
Steven Hansen, Pablo Sprechmann, Alexander Pritzel, Andr’ e Barreto, and Charles Blundell. Fast
deep reinforcement learning using online adjustments from the past. In NeurIPS, 2018.
Demis Hassabis, Dharshan Kumaran, Seralynne D. V ann, and El eanor A. Maguire. Patients with
hippocampal amnesia cannot imagine new experiences. Proceedings of the National Academy of
Sciences, 104:1726 – 1731, 2007.
Thomas T . Hills. T owards a uniﬁed theory of animal event timi ng. 2003.
Sepp Hochreiter and J ¨ urgen Schmidhuber. Long short-term m emory. Neural Computation , 9:1735–
1780, 1997.
Chia-Chun Hung, T . Lillicrap, Josh Abramson, Y an Wu, M. Mirz a, F . Carnevale, Arun Ahuja, and
G. W ayne. Optimizing agent behavior over long time scales by transporting value. Nature Com-
munications, 10, 2019.
Anthony I Jang, Matthew R. Nassar, Daniel G. Dillon, and Mich ael J. Frank. Positive reward
prediction errors strengthen incidental memory encoding. bioRxiv, 2018.
Hyunwoo Jung, Moonsu Han, Minki Kang, and Sung Ju Hwang. Lear ning what to remember:
Long-term episodic memory networks for learning from strea ming data. ArXiv, abs/1812.04227,
2018.
Lukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learn ing to remember rare events.
ArXiv, abs/1703.03129, 2017.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej O sinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski , Sergey Levine, Ryan Sepassi,
George Tucker, and Henryk Michalewski. Model-based reinfo rcement learning for atari. ArXiv,
abs/1903.00374, 2020.
Gabriel Kalweit and Joschka Boedecker. Uncertainty-drive n imagination for continuous deep rein-
forcement learning. In CoRL, 2017.
Ken Kansky, T om Silver, David A. M ´ ely, Mohamed Eldawy, Migu el L ´ azaro-Gredilla, Xinghua Lou,
N. Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot
transfer with a generative causal model of intuitive physic s. ArXiv, abs/1706.04317, 2017.
Nan Rosemary Ke, Amanpreet Singh, Ahmed T ouati, Anirudh Goy al, Y oshua Bengio, Devi Parikh,
and Dhruv Batra. Learning dynamics model in reinforcement l earning by incorporating the long
term future. ArXiv, abs/1903.01599, 2019.
Jonathan Ko, Daniel J. Klein, Dieter Fox, and Dirk H ¨ ahnel. G aussian processes and reinforcement
learning for identiﬁcation and control of an autonomous bli mp. Proceedings 2007 IEEE Interna-
tional Conference on Robotics and Automation , pp. 742–747, 2007.
V ikash Kumar, Emanuel T odorov, and Sergey Levine. Optimal c ontrol with learned local models:
Application to dexterous manipulation. 2016 IEEE International Conference on Robotics and
Automation (ICRA) , pp. 378–383, 2016.
Thanard Kurutach, Ignasi Clavera, Y an Duan, A viv T amar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. ArXiv, abs/1802.10592, 2018.
M ´ at´ e Lengyel and Peter Dayan. Hippocampal contributions to control: The third way. In NIPS,
2007.
11
Episodic Memory for Subjective-Timescale Models
Sergey Levine and Pieter Abbeel. Learning neural network po licies with guided policy search under
unknown dynamics. In NIPS, 2014a.
Sergey Levine and Pieter Abbeel. Learning neural network po licies with guided policy search under
unknown dynamics. In NIPS, 2014b.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abb eel. End-to-end training of deep visuo-
motor policies. J. Mach. Learn. Res. , 17:39:1–39:40, 2016.
Johannes B. Mahr and Gergely Csibra. Why do we remember? the c ommunicative function of
episodic memory. The Behavioral and brain sciences , pp. 1–93, 2017.
Rowan McAllister and Carl Edward Rasmussen. Improving pilc o with bayesian neural network
dynamics models. 2016.
Kourken Michaelian. Mental time travel: Episodic memory an d our knowledge of the personal past.
2016.
Beren Millidge. Deep active inference as variational polic y gradients. ArXiv, abs/1907.03876, 2019.
Nikhil Mishra, Pieter Abbeel, and Igor Mordatch. Predictio n and control with temporal segment
models. ArXiv, abs/1703.04070, 2017.
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel V eness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, et al. Human-level control through deep reinforcement
learning. Nature, 518:529–533, 2015.
Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Serg ey Levine. Neural network dy-
namics for model-based deep reinforcement learning with mo del-free ﬁne-tuning. 2018 IEEE
International Conference on Robotics and Automation (ICRA ), pp. 7559–7566, 2018.
Junhyuk Oh, V alliappa Chockalingam, Satinder P . Singh, and Honglak Lee. Control of memory,
active perception, and action in minecraft. ArXiv, abs/1605.09128, 2016.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adri` a Puigdom ` enech Badia, Oriol V inyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell. Neura l episodic control. ArXiv,
abs/1703.01988, 2017.
S´ ebastien Racani` ere, Theophane W eber, David P . Reichert , Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, et al. Imagination-augmented agen ts for deep reinforcement learning.
In NIPS, 2017.
W arrick Roseboom, Z. Fountas, Kyriacos Nikiforou, David Bh owmik, M. Shanahan, and A. Seth.
Activity in perceptual classiﬁcation networks as a basis fo r human subjective time perception.
Nature Communications , 10, 2019.
Nina Rouhani, Kenneth A. Norman, and Y ael Niv. Dissociable e ffects of surprising rewards on
learning and memory. Journal of Experimental Psychology: Learning, Memory, and Cognition,
44:1430–1443, 2018.
Noor Sajid, Philip J. Ball, and Karl J. Friston. Active infer ence: demystiﬁed and compared. arXiv:
Artiﬁcial Intelligence , 2019.
D. Schacter, D. Addis, D. Hassabis, V . C. Mart´ ın, R. N. Spren g, and K. Szpunar. The future of
memory: Remembering, imagining, and the brain. Neuron, 76:677–694, 2012.
Daniel L. Schacter, Donna Rose Addis, and Randy L. Buckner. R emembering the past to imagine
the future: the prospective brain. Nature Reviews Neuroscience , 8:657–661, 2007.
T om Schaul, John Quan, Ioannis Antonoglou, and David Silver . Prioritized experience replay.
CoRR, abs/1511.05952, 2016.
Maxine T Sherman, Zafeirios Fountas, Anil K Seth, and W arric k Roseboom. Accumulation of
salient perceptual events predicts subjective time. bioRxiv, 2020.
12
Episodic Memory for Subjective-Timescale Models
D. Silver, Julian Schrittwieser, K. Simonyan, Ioannis Anto noglou, Aja Huang, A. Guez, T . Hubert,
L. Baker, et al. Mastering the game of go without human knowle dge. Nature, 550:354–359, 2017.
Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Chri stopher L. Buckley. Scaling active
inference. ArXiv, abs/1911.10601, 2019.
Endel Tulving. Episodic and semantic memory. 1972.
Kai Ueltzh ¨ offer. Deep active inference. Biological Cybernetics , 112:547–573, 2018.
Manuel W atter, Jost T obias Springenberg, Joschka Boedecke r, and Martin A. Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. In NIPS, 2015.
Guangxiang Zhu, Zichuan Lin, Guangwen Y ang, and Chongjie Zh ang. Episodic reinforcement
learning with associative memory. In ICLR, 2020.
Ozan C ¸ atal, Johannes Nauta, Tim V erbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy
selection using active inference. ArXiv, abs/1904.08149, 2019.
13
Episodic Memory for Subjective-Timescale Models
A P RELIMINA R IE S
A.1 A CT IV E IN F E RE N CE
Active inference is a corollary of the free-energy principl e applied to action (Friston et al., 2016;
Friston, 2019; Sajid et al., 2019). In this framework, an age nt embedded in an environment aims
to do two things: (i) minimise surprisal from the observatio ns of the environment under the agent’s
internal model of this environment, and (ii) perform action s so as to minimise the expected surprisal
in the future. More formally, an agent is equipped with a gene rative model p(ot, st; θ), where o t
is the agent’s observation at time t, s t is the hidden state of the environment, and θ denotes the
parameters of the generative model. The agent’s surprise at time t is deﬁned as the negative log-
likelihood, − log p(ot; θ).
W e can upper-bound this intractable expression using varia tional inference by introducing an ap-
proximate posterior distribution, q(st), over s t, such that:
− log p(ot; θ) ≤ Eq(st) [log q(st) − log p(ot, st; θ)] =F, (7)
where F is the variational free energy . The minimisation of this quantity realises objective (i)
and is performed by optimising the parameters of the generat ive model, θ. It is also equivalent
to the maximisation of model evidence, which intuitively im plies that the agent aims to perfect its
generative model at explaining the sensory observations fr om the environment. T o realise objective
(ii), the agent must select actions that lead to the lowest expected surprise in the future, which can
be calculated using the expected free energy (EFE), G:
G(π, τ ) =Ep(oτ |sτ )
[
Eq(sτ |π ) [log q(sτ |π) − log p(oτ , sτ |π)]  
variational free energy , F
]
, (8)
where τ > t and π = {at, at+1, ..., aτ −1} is a sequence of actions (policy) between the present
time t and the future time τ. The free-energy minimising system must, therefore, imagi ne the future
observations given a policy and calculate the expected free energy conditioned on taking this policy.
Then, actions that led to lower values of the EFE are chosen wi th higher probability, as opposed to
actions that led to higher values of EFE, such that:
p(π) =σ (−γG(π)) , (9)
where G(π) =∑
τ >t G(π, τ ), γ is the temperature parameter, σ(·) denotes a softmax function, and
t is the present timestep.
B A RCHITEC TU R AL DETAILS AND TRAINING
B.1 B A S E L IN E IM P L E M E N TAT IO N
As mentioned, each component of the generative and inferenc e models is parametrised by feed-
forward neural networks (including fully-connected, conv olutional and transpose-convolutional lay-
ers), whose architectural details can be found in Figure 6. T he latent bottleneck of the autoencoder,
s, was of size 10. The hyperparameters of the top-down attenti on mechanism were: a = 2, b = 0.5,
c = 0.1, and d = 5, chosen to match those in Fountas et al. (2020a). Similarly, we restricted the
action space to just 3 actions – forward, left, right. For tes ting, we optimised the MCTS param-
eters of the baseline agent, setting the exploration hyperp arameter cexplore = 0.1 (see Eq.10), and
performing 30 simulation loops, each with depth of 1. The net works were trained using separate
optimisers for stability reasons. The habitual and transit ion networks are trained with a learning rate
of 0.0001; the autoencoder’s optimiser had a learning rate of 0.001. The batch size was set to 50
and the model was trained for 750k iterations under a green ob servational prior. All of the networks
were implemented using T ensorﬂow v2.2 (Abadi et al., 2015). T ests were performed in Animal-AI
v2.0.1 (Beyret et al., 2019).
Furthermore, following Fountas et al. (2020a), we deﬁne the MCTS upper conﬁdence bound as,
U(s, a) = ˜G(s, a) +cexplore · q(a|s; φa) · 1
N(a, s) + 1. (10)
14
Episodic Memory for Subjective-Timescale Models
Figure 6: Implementation of the baseline system.
As discussed, each network was trained with its correspondi ng loss function, which are the con-
stituent parts of the total variational free energy. In part icular, the autoencoder was trained using
Eqs. 1a and 1b, transition model using Eq. 1b, and habitual ne twork using Eq. 1c.
Furthermore, following the training procedure from Founta s et al. (2020a), we stabilise the conver-
gence of the autoencoder by modifying the loss function to:
Lautoencoder = − Eq(st) [log p(ot|st; θo)] +γDKL [q(st; φs)||p(st|st−1, at−1; θs)]
+ (1− γ)DKL [q(st; φs)||N(000, I)] , (11)
where γ is a hyperparameter that gradually increases from 0 to 0.8 du ring training.
B.2 P RIO RIT IS E D EX P E RIE N CE RE P L AY
As part of the baseline system’s training procedure, we util ise prioritised experience replay (PER)
(Schaul et al., 2016) to mitigate the detrimental effects of on-line learning (which was used in the
original paper by Fountas et al. (2020a)), and to encourage b etter object-centric representations.
In particular, on-line learning has three major issues asso ciated with it. First, training is performed
on correlated data points, which is generally considered to be detrimental for training neural net-
works (Schaul et al., 2016). Second, observations that are rarely encountered in an environment are
discarded in on-line learning and are used for training only when visited again. These are likely to
be the observations for which there is most room for improvem ent. Instead, the agent will often
be training on already well-predicted transitions that it h appens to visit often. Finally, an on-line
learning agent is constrained by its current position in an e nvironment to sample new data and, thus,
has very limited control over the content of its training bat ches.
15
Episodic Memory for Subjective-Timescale Models
Furthermore, as mentioned in Section 4.1, in the Animal-AI e nvironment rare observations are those
that include objects; yet, objects are a central component of this environment – the only way to
interact, get rewards, and importantly, the only means of mi nimising the free energy optimally.
T o encourage our agent to learn better object representatio ns, we employ PER with the objective-
timescale transition model free energy as the priority metric . As discussed, observations with higher
values of this metric tend to constitute more complex scenes , which include objects – as the only
source of complexity in the AAI. See Figure 7 for qualitative evidence of this trend. The use of PER
resulted in a considerable improvement in the baseline’s pe rformance and better ability to reconstruct
observations with objects (See Figure 8).
B.3 STM I M P L E M E N TAT IO N
The STM introduces two additional components: STM habitual network and STM transition model.
The habitual network was trained using the same training pro cedure as described in Appendix B.1.
The transition model was trained on batch size 15 and a learni ng rate of 0.0005. Each batch consisted
of zero-padded S-sequences with length 50. W e use a Masking l ayer to ignore zero-padded parts of
the sequences in the computational graph. The training was s topped at 200k training iterations. For
testing STM-MCTS in Section 5.1, we optimise the MCTS parame ters, setting cexplore = 0.1, and
performing 15 simulation loops, each with depth of 3. The thr eshold (objective-timescale transition
model free energy), ǫ, was manually set to 5 after inspection of the buffer and valu e distribution.
B.4 A CT IO N HE U RIS T IC
T o train the STM transition model in the Animal-AI environme nt, we implement a simple heuristic
that is used to summarise a sequence of actions taken by the ag ent from one memory to reach
the next one. A sequence of actions, A = {aτ1 , aτ1+1, ...aτ1+(N−1)}, takes the agent from a
recorded memory sτ1 to memory sτ2 , where the time between these states τ2 − τ1 = N, and
a ∈ { aforward, aright, aleft }. W e employ polar coordinates relative to the agent’s initia l position in
Cartesian coordinates at time τ1 and perform iterative updates of its position after every ac tion un-
til the time-step of the next episodic memory, τ2, is reached. Given the agent’s orientation in the
environment, θ, the next position of the agent is calculated using,
pt+1 = pt +
[
sin θ
cos θ
]
, where pt =
[
0
0
]
t=τ1
(12)
Finally, we retrieve angle φ, which describes the direction in which the agent has travel led with re-
spect to its initial position and orientation. This angle is used to decide on the action that summarises
the trajectory using
a =



aforward |φ| ≤ 22.5◦
aright 22.5◦ < φ < 180◦
aleft −22.5◦ > φ ≥ − 180◦,
Although this heuristic provided satisfactory results, tr ajectory encoding is one of the most limiting
parts of the STM and is a promising direction for further rese arch.
16
Episodic Memory for Subjective-Timescale Models
Figure 7: Observations sorted by their corresponding recor ded value of the objective-timescale tran-
sition model free energy in descending order. States with hi gher values on average contain more
objects, constituting more complex settings.
17
Episodic Memory for Subjective-Timescale Models
Figure 8: Agent trained with PER is better at generating obse rvations with objects. (A) Prediction
roll-outs showing that baseline trained with PER can better represent objects, thus preserving them in
future predictions. (B) Ground-truth observations (third column) are passed throu gh the autoencoder
of the on-line and PER agents. As can be seen, observations ar e better reconstructed by the baseline
system trained with PER.
Figure 9: Implementation of STM components.
18
Episodic Memory for Subjective-Timescale Models
C A DDITIONAL RESULTS
W e provide additional results of the STM prediction roll-ou ts:
a) Figure 10: random roll-outs generated by the system. Thes e diverse roll-outs demonstrate
that STM is able to: i) make correct action-conditioned predictions, ii) speed up its pre-
diction timescale when objects are far away, iii) slow down the prediction timescale when
objects are nearby.
b) Figure 11: STM consistently imagines objects coming into view . The observations pro-
duced by the model are entirely plausible given the path the a gent is taking and the context
it ﬁnds itself in. This indicates that STM does indeed produc e semantically meaningful
predictions. It is pertinent to note that the roll-outs comp ly with the physics of the environ-
ment, which is crucial, as it potentially refutes the hypoth esis that these imagined objects
were predicted at random.
c) Figure 12: shows the roll-outs produced by the objective- timescale model using the same
starting states as in Figure 11. These roll-outs are in stark contrast to those produced by
STM, exemplifying the baseline’s inability to imagine obje cts that are not present in the
initial frame.
19
Episodic Memory for Subjective-Timescale Models
Figure 10: Random roll-outs generated with the STM transiti on model.
20
Episodic Memory for Subjective-Timescale Models
Figure 11: STM can imagine objects from ‘uninteresting’ sta tes. Arrows indicate roll-outs with
imagined objects.
21
Episodic Memory for Subjective-Timescale Models
Figure 12: In contrast to STM, the objective-timescale tran sition model is not able to imagine ob-
jects, starting with the same initial observations as shown in Figure 11.
22