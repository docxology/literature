0202
tcO
3
]GL.sc[
1v03410.0102:viXra
EPISODIC MEMORY FOR LEARNING SUBJECTIVE-
TIMESCALE MODELS
AlexeyZakharov∗ MatthewCrosby
ImperialCollegeLondon& LeverhulmeCentrefortheFutureofIntelligence,
EmotechLabs ImperialCollegeLondon
az519@ic.ac.uk m.crosby@imperial.ac.uk
ZafeiriosFountas
EmotechLabs&
WCHN,UniversityCollegeLondon
f@emotech.co
ABSTRACT
Inmodel-basedlearning,an agent’smodeliscommonlydefinedovertransitions
betweenconsecutivestatesofanenvironmenteventhoughplanningoftenrequires
reasoningovermulti-steptimescales,withintermediatestateseitherunnecessary,
or worse, accumulatingpredictionerror. In contrast, intelligentbehaviourin bi-
ological organismsis characterised by the ability to plan over varying temporal
scales depending on the context. Inspired by the recent works on human time
perception,wedevisea novelapproachtolearningatransitiondynamicsmodel,
based on the sequences of episodic memories that define the agent’s subjective
timescale–overwhichitlearnsworlddynamicsandoverwhichfutureplanning
isperformed.Weimplementthisintheframeworkofactiveinferenceanddemon-
stratethattheresultingsubjective-timescalemodel(STM)cansystematicallyvary
thetemporalextentofitspredictionswhilepreservingthesamecomputationalef-
ficiency.Additionally,weshowthatSTMpredictionsaremorelikelytointroduce
future salient events (for example new objects coming into view), incentivising
exploration of new areas of the environment. As a result, STM produces more
informativeaction-conditionedroll-outsthatassisttheagentinmakingbetterde-
cisions. WevalidatesignificantimprovementinourSTMagent’sperformancein
theAnimal-AIenvironmentagainstabaselinesystem,trainedusingtheenviron-
ment’sobjective-timescaledynamics.
1 INTRODUCTION
An agentendowedwith a modelof its environmenthas the ability to predictthe consequencesof
its actions and perform planning into the future before deciding on its next move. Models can
allowagentstosimulatethepossibleaction-conditionedfuturesfromtheircurrentstate,evenifthe
state was never visited during learning. As a result, model-based approaches can provide agents
withbettergeneralizationabilitiesacrossbothstatesandtasksinanenvironment,comparedtotheir
model-freecounterparts(Racanie`reetal.,2017;Mishraetal.,2017).
The most popular framework for developing agents with internal models is model-based rein-
forcement learning (RL). Model-based RL has seen great progress in recent years, with a num-
ber of proposedarchitecturesattemptingto improveboth the quality and the usage of these mod-
els(Kaiseretal.,2020;Racanie`reetal.,2017;Kanskyetal.,2017;Hamrick,2019). Nevertheless,
learninginternalmodelsaffordsanumberofunsolvedproblems.Thecentraloneofthemismodel-
bias,inwhichtheimperfectionsofthelearnedmodelresultinunwantedover-optimismandsequen-
tialerroraccumulationforlong-termpredictions(Deisenroth&Rasmussen,2011). Long-termpre-
dictionsareadditionallycomputationallyexpensiveinenvironmentswithslowtemporaldynamics,
∗Correspondingauthor.
1
EpisodicMemoryforSubjective-TimescaleModels
giventhatallintermediarystatesmustbepredicted.Moreover,slowworlddynamics1caninhibitthe
learningofdependenciesbetweentemporally-distantevents,whichcanbecrucialforenvironments
with sparse rewards. Finally, the temporal extent of future predictions is limited to the objective
timescaleoftheenvironmentoverwhichthetransitiondynamicshasbeenlearned.Thisleaveslittle
roomforflexibleandcontext-dependentplanningovervaryingtimescaleswhichischaracteristicto
animalsandhumans(Claytonetal.,2003;Cheke&Clayton,2011;Buhusi&Meck,2005).
Thefinalissueexemplifiesthedisadvantageoftheclassicalviewoninternalmodels,inwhichthey
are considered to capture the ground-truthtransition dynamics of the environment. Furthermore,
in more complex environments with first-person observations, this perspective does not take into
accounttheapparentsubjectivityoffirst-personexperiences.Inparticular,theagent’slearnedrepre-
sentationsoftheenvironment’stransitiondynamicsimplicitlyincludeinformationabouttime.Little
workhasbeendonetoaddresstheconceptoftimeperceptioninmodel-basedagents(Deverettetal.,
2019). Empiricalevidencefromthestudiesofhumanandanimalcognitionsuggeststhatintelligent
biologicalorganismsdonotperceivetimepreciselyanddonotpossessanexplicitclockmechanism
responsibleforkeepingtrackoftime(Roseboometal.,2019;Shermanetal.,2020;Hills,2003).For
instance,humanstendtoperceivetimeslowerinenvironmentsrichinperceptualcontent(e.g. busy
city),andfasterinenvironmentswithlittle perceptualchange(e.g. emptyfield). Themechanisms
ofsubjectivetimeperceptionstillremainunknown;however,recentcomputationalmodelsbasedon
episodicmemorywereabletocloselymodelthedeviationsofhumantimeperceptionfromveridical
perception(Fountasetal.,2020b).
Inspired by this account, in this work we propose subjective-timescale model (STM), an alterna-
tiveapproachto learninga transitiondynamicsmodel,byreplacingtheobjectivetimescale witha
subjectiveone. The latter representsthe timescale bywhich an agentperceiveseventsin an envi-
ronment,predictsfuturestates,andwhichisdefinedbythesequencesofepisodicmemories. These
memoriesareaccumulatedonthebasisofsaliency(i.e. howpoorlyaneventwaspredictedbythe
agent’stransitionmodel),whichattemptstomimicthewayhumansperceivetime,andresultingin
theagent’sabilitytoplanovervaryingtimescalesandconstructnovelfuturescenarios.
We employactiveinferenceastheagent’sunderlyingcognitiveframework. Activeinferenceisan
emerging framework within computational neuroscience, which attempts to unify perception and
actionunderthesingleobjectiveofminimisingthefree-energyfunctional. Similartomodel-based
RL,anactiveinferenceagentreliesalmostentirelyonthecharacteristicsandthequalityofitsinter-
nalmodeltomakedecisions. Thus,itisnaturallysusceptibletothepreviouslymentionedproblems
associatedwithimperfect,objective-timescalemodels.Theselectionofactiveinferenceforthepur-
posesofthispaperis motivatedbyits biologicalplausibilityas a normativeframeworkforunder-
standingintelligentbehaviour(Fristonetal.,2017a;2006),whichisinlinewiththegeneraltheme
ofthiswork.Furthermore,beingrootedinvariationalinference,thefreeenergyobjectivegenerates
a distinct separation between the information-theoreticquantities that correspond to the different
componentsoftheagent’smodel,whichiscrucialtodefinethememoryformationcriterion.
WedemonstratethattheresultingcharacteristicsofSTMallowtheagenttoautomaticallyperform
bothshort-andlong-termplanningusingthesamecomputationalresourcesandwithoutanyexplicit
mechanismforadjustingthetemporalextentofitspredictions. Furthermore,forlong-termpredic-
tions STM systematically performs temporal jumps (skipping intermediary steps), thus providing
moreinformativefuturepredictionsandreducingthedetrimentaleffectsofone-steppredictionerror
accumulation. Lastly,beingtrainedonsalientevents,STMmuchmorefrequentlyimaginesfutures
thatcontainepistemically-surprisingevents,whichincentivisesexploratorybehaviour.
2 RELATED WORK
Model-based RL. Internal models are extensively studied in the field of model-based RL. Us-
ing linear models to explicitly model transition dynamics has achieved impressive results in
robotics (Levine&Abbeel, 2014a; Watteretal., 2015; Bagnell&Schneider, 2001; Abbeeletal.,
2006;Levine&Abbeel,2014b;Levineetal.,2016;Kumaretal.,2016). Ingeneral,however,their
application is limited to low-dimensional domains and relatively simple environment dynamics.
Similarly, Gaussian Processes (GPs) have been used (Deisenroth&Rasmussen, 2011; Koetal.,
1Worldswithsmallchangeinstategivenanaction
2
EpisodicMemoryforSubjective-TimescaleModels
2007). Theirprobabilisticnatureallowsforstateuncertaintyestimation,whichcanbeincorporated
intheplanningmoduletomakemorecautiouspredictions;however,GPsstruggletoscaletohigh-
dimensionaldata. An alternative and recently more prevalentmethod for parametrisingtransition
modelsistouseneuralnetworks.Theseareparticularlyattractiveduetotheirrecentprovensuccess
inavarietyofdomains,includingdeepmodel-freeRL(Silveretal.,2017),abilitytodealwithhigh-
dimensional data, and existence of methods for uncertainty quantification (Blundelletal., 2015;
Gal&Ghahramani,2016). Differentdeeplearningarchitectureshavebeenutilisedincludingfully-
connectedneuralnetworks(Nagabandietal.,2018;Feinbergetal.,2018;Kurutachetal.,2018)and
autoregressivemodels(Ha&Schmidhuber,2018;Racanie`reetal.,2017;Keetal.,2019),showing
promisingresultsin environmentswith relativelyhigh-dimensionalstate spaces. In particular,au-
toregressivearchitectures,suchasLongShort-TermMemory(LSTM)(Hochreiter&Schmidhuber,
1997),arecapableofmodellingnon-Markovianenvironmentsandoflearningtemporaldependen-
cies. Nevertheless, LSTMs are still limited in their ability to learn relations between temporally-
distantevents,whichisexacerbatedinenvironmentswherelittlechangeoccursgivenanaction.
Uncertainty quantification using ensemble methods (Kalweit&Boedecker, 2017; Claveraetal.,
2020; Buckmanetal., 2018) or Bayesian neural networks (McAllister&Rasmussen, 2016;
Depewegetal.,2017)havebeenproposedtotacklemodelbiasandsequentialerroraccumulation.
Otherworkshavefocusedontechniquestocreatemoreaccuratelong-termpredictions.Mishraetal.
(2017) used a segment-basedapproachto predictentire trajectoriesat oncein an attemptto avoid
one-steppredictionerroraccumulation. A work by Keetal. (2019) used an autoregressivemodel
and introduced a regularising auxiliary cost with respect to the encodings of future observations,
thusforcingthelatentstatestocarryusefulinformationforlong-horizonpredictions.Incontrast,the
workpresentedinthispaperre-focusestheobjectivefromattemptingtocreatebetterparametrisa-
tiontechniquesormitigatingmethodstosimplytransformingthetimescaleoverwhichthedynamics
ofanenvironmentislearned. Aswillbeseen,ourapproachcanleadtomoreaccurateandefficient
long-termpredictionswithoutcompromisingagent’sabilitytoplanovershorttime-horizons.
EpisodicMemory. Inneuroscience,episodicmemoryisusedtodescribeautobiographicalmemo-
riesthatlinkacollectionoffirst-personsensoryexperiencesataspecific timeandplace(Tulving,
1972). Past studies in the field suggest that episodic memory plays an important role in human
learning (Mahr&Csibra, 2017), and may capture a wide range of potential functional purposes,
such as construction of novel future scenarios (Schacteretal., 2007; 2012; Hassabisetal., 2007),
mental time-travel (Michaelian, 2016) or assisting in the formation of new semantic memories
(Greenberg&Verfaellie,2010). Arecentcomputationalmodelofepisodicmemory(Fountasetal.,
2020b)alsorelatesittothehumanabilitytoestimatetimedurations.
The applicationof episodic memoryin reinforcementlearninghas been somewhatlimited. Some
workshave employedsimple formsof memoryto improvethe performanceof a deep model-free
RLagentviaexperiencereplay(Mnihetal.,2015;Espeholtetal.,2018;Schauletal.,2016). How-
ever,thesemethodsdonotincorporateinformationaboutassociativeortemporaldependenciesbe-
tweenthememories(Hansenetal.,2018). Read-writememorybankshavealsobeenimplemented
alongside gradient-based systems (memory-augmented neural networks) for assisting in learning
andprediction(Gravesetal.,2014;2016;Hungetal.,2019;Ohetal.,2016;Jungetal.,2018). Fur-
ther,episodicmemoryhasbeenusedfornon-parametricQ-functionapproximation(Blundelletal.,
2016; Pritzeletal., 2017; Hansenetal., 2018; Zhuetal., 2020). It has also been proposed to be
used directly for control as a faster and more efficient alternative to model-based and model-free
approachesinRL,suchasinstance-basedcontrol(Lengyel&Dayan,2007;Botvinicketal.,2019;
Gershman&Daw,2017)andone-shotlearning(Kaiseretal.,2017). Incontrast,ourpaperconsid-
ers a novel way of using episodic memories – in defining the agent’s subjective timescale of the
environmentandtrainingatransitiondynamicsmodeloverthesequencesofthesememories.
ActiveInference.Untilnow,mostoftheworkonactiveinferencehasbeendoneinlow-dimensional
anddiscretestatespaces(Fristonetal.,2015;2017b;c;d).Recently,however,therehasbeenarising
interestinscalingactiveinferenceandapplyingittoenvironmentswithcontinuousand/orlargestate
spaces(Fountasetal.,2020a;Tschantzetal.,2019;C¸ataletal.,2019;Millidge,2019;Ueltzho¨ffer,
2018). Although these works used deep learning techniques, their generative models have so far
beendesignedtobeMarkovianandtrainedovertheobjectivetimescaleoftheenvironment.
3
EpisodicMemoryforSubjective-TimescaleModels
3 BASELINE ARCHITECTURE
WetakethedeepactiveinferencesystemdevisedbyFountasetal.(2020a)asthestartingpointwith
a few architectural and operational modifications. The generative model of this baseline agent is
defined as p(o ,s ,a ;θ), where s denotes latent states at time t, o (visual) observations, a
1:t 1:t 1:t t t t
actions, and θ = {θ ,θ } the parameters of the model. s is assumed to be Gaussian-distributed
o s t
with a diagonal covariance, o follows Bernoulli and a categorical distributions. For a single
t 1:t
timestep,asillustratedinFigure1A,thisgenerativemodelincludestwofactors,atransitionmodel
p(s |s ,a ;θ )andalatentstatedecoderp(o |s ;θ )parametrisedbyfeed-forwardneuralnet-
t t−1 t−1 s t t o
works with parametersθ and θ , respectively. We modifythe transition modelfrom the original
s o
studytopredictthechangeinstate,ratherthanthefullstate2.
Theagentalsopossessestwoinferencenetworks,whicharetrainedusingamortizedinference:aha-
bitualnetworkq(a ;φ )andobservationencoderq(s ;φ )parametrisedbyφ andφ ,respectively.
t a t s a s
Thehabitualnetworkactsasamodel-freecomponentofthesystem,learningtomapinferredstates
directly to actions. Following Fountasetal. (2020a), the variational free energy for an arbitrary
time-steptisdefinedas:
F =−E [logp(o |s ;θ )] (1a)
t q(st) t t o
+D [q(s ;θ )kp(s |s ,a ;θ )] (1b)
KL t s t t−1 t−1 s
+E [D [q(a ;φ )kp(a )]] (1c)
q(st) KL t a t
where p(a) = p(π) is the summed probability of all policies beginning with action a.
π:a1=a
AllthedivergencPetermsarecomputableinclosed-form,giventheassumptionaboutGaussian-and
Bernoulli-distributedvariables. Finally,theexpectedfreeenergy(EFE)ofthegenerativemodelup
tosometimehorizonT canbedefinedas:
T T
G(π)= G(π,τ)= E [logq(s ,θ|π)−logp(o ,s ,θ|π)], (2)
q˜ τ τ τ
Xτ=t Xτ=t
whereq˜=q(o ,s ,θ|π)andp(o ,s ,θ|π)=p(o |π)q(s |o ,π)p(θ|s ,o ,π).
τ τ τ τ τ τ τ τ τ
Tomakeexpression2computationallyfeasible,itisdecomposedsuchthat,
G(π,τ)=−E [logp(o |π)]
q(θ|π)q(sτ|θ,π)q(oτ|sτ,θ,π) τ
+E E H(s |o ,π)−H(s |π)
q(θ|π) q(oτ|θ,π) τ τ τ
(3)
+E (cid:2) [H(o |s ,θ,π)] (cid:3)
q(θ|π)q(sτ|θ,π) τ τ
−E [H(o |s ,π)],
q(sτ|π) τ τ
whereexpectationscanbetakenbyperformingsequentialsamplingofθ,s ando andentropiesare
τ τ
calculatedinclosed-formusingstandardformulasforBernoulliandGaussiandistributions.Network
parameters,θ,aresampledusingMonteCarlo(MC)dropout(Gal&Ghahramani,2016).
The system also makes use of top-down attention mechanism by introducing variable ω, which
modulates uncertainty about hidden states, promoting latent state disentanglement and more ef-
ficient learning. Specifically, the latent state distribution is defined as a Gaussian such that
s∼N(s;µ,Σ/ω),whereµandΣarethemeanandthediagonalcovariance,andωisadecreasing
logisticfunctionoverthedivergenceD [q(a;φ )kp(a)].
KL a
Finally, action selection is aided with Monte Carlo tree search (MCTS), ensuringa more efficient
trajectorysearch.Specifically,MCTSgeneratesaweightedtreethatisusedtosamplepoliciesfrom
thecurrenttimestep,wheretheweightsrefertotheagent’sestimationoftheEFEgivenastate-action
pair, G˜(s,a). Thenodesofthe tree arepredictedvia the transitionmodel,p(s |s ,a ;θ ). At
t t−1 t−1 s
theendofthesearch, MCTSisusedto constructtheactionprior,p(a) = N(a ,s)/ N(a ,s),
i j j
whereN(a,s)isthenumberoftimesactionahasbeentakenfromstates. P
Thebaselineagentistrainedwithprioritisedexperiencereplay(PER)(Schauletal.,2016)tomit-
igatethedetrimentalconsequencesofon-linelearning(whichwasusedintheoriginalpaper),and
to encouragebetterobject-centricrepresentations. The detailsof the baselineimplementationand
trainingwithPERcanbefoundinAppendicesB.1andB.2,respectively.
2Thishaslargelybecome common practice inthefieldof model-based RL (Nagabandietal.,2018), im-
provingalgorithmefficiencyandaccuracyespeciallyinenvironmentswithslowtemporaldynamics.
4
EpisodicMemoryforSubjective-TimescaleModels
Figure 1: A. Baseline generative model. B. STM generative model with additional deterministic
hiddenstateshintroducedbyanLSTM.
4 SUBJECTIVE-TIMESCALE MODEL
Weintroducesubjective-timescalemodel(STM)thatrecordssequencesofepisodicmemoriesover
which a new transitionmodelis trained. As such, the system consists of a memoryaccumulation
system to selectively record salient events, a simple action heuristic to summarise sequences of
actionsbetweenmemories,andanautoregressivetransitionmodel.
Wedefineaground-truthsequenceasasequenceofallstatesexperiencedinanenvironmentduring
asingleepisode,S ={s ,s ,s ,...,s },andanS-sequence(subjectivesequence)asasequenceof
g 0 1 2 T
statesselectivelypickedbyoursystem,andoverwhichthenewtransitionmodelwouldbelearned,
S ={s ,s ,s ,...,s }. EachunitinanS-sequenceiscalledanepisodicmemoryandconsists
e τ1 τ2 τ3 τN
ofasetofsufficientstatistics, s = {µ ,σ },whereµ andσ aremeanandvariancevectorsofa
s s s s
Gaussian-distributedstates,respectively. Additionally,eachepisodicmemorycontainsareference
toitspreceding(parent)episodicmemoryandallactionsuntilthenextone.Theprocessofrecording
S-sequencesiscalledmemoryaccumulation.
4.1 MEMORY ACCUMULATION
Previousworkontimeperceptionandepisodicmemory(Fountasetal.,2020b)employedsaliency
ofanevent,orthegenerativemodel’spredictionerror,asthememoryformationcriterion.Selection
ofthis criterionis informedbythe experimentalevidencefromneuroscienceonepisodicmemory
(Greveetal., 2017;Jangetal., 2018;Rouhanietal., 2018). Inspiredby thisaccount, ourmemory
accumulationsystememploysthefreeenergyoftheobjective-timescaletransitionmodel3 (Eq.1b)
asameasureofeventsaliency,andformsmemorieswhenapre-definedthresholdisexceeded.
To train STM, an active inference agent movesin the environmentunder a pre-trained generative
modeldescribedinSection3.Duringthisprocess,eachtransitionisevaluatedbasedontheobjective
transition model free energy, D [q(s ;θ )kp(s |s ,a ;θ )], which represents the degree of
KL t s t t−1 t−1 s
surpriseexperiencedbythetransitionmodelupontakinganaction. Ifthevalueofthefreeenergy
exceedsapre-definedthreshold,ǫ,amemoryisformedandplacedintoanS-sequence. Attheend
ofeachepisode,therecordedS-sequenceissavedforlateruse.
Wecancategorisethetransitionsthatcausehighervaluesoftransitionmodelfreeenergiesintotwo
maingroups: epistemicsurpriseandmodel-imperfectionsurprise. Theformerreferstotransitions
thatthemodelcouldnothavepredictedaccuratelyduetothelackofinformationaboutthecurrent
state of the environment (e.g. objects coming into view). The latter refers to the main bulk of
these high prediction-error transitions and stems from the inherent imperfections of the learned
dynamics. Specifically, less frequently-occurringobservationswith richer combinatorialstructure
wouldsystematically resultin highercompoundedtransitionmodelerrors, giventhatthese would
be characteristic of more complex scenes. As will become apparent, the presence of these two
3Components ofthetotalfreeenergycorrespond toameasureofbeliefupdateforeachofthenetworks,
andtherefore,looselyspeaking,quantifythepredictionerrorgeneratedbyeachoftherespectivesystemcon-
stituents:autoencoder(Eqs.1a,1b),objective-timescaletransitionmodel(Eq.1b),andhabitualnetwork(Eq.1c).
5
EpisodicMemoryforSubjective-TimescaleModels
categoriesintherecordedS-sequencesresultsinthemodel’sabilitytovaryitspredictiontimescale
basedontheperceptualcontextandsystematicallyimaginefuturesalientevents.
Figure2: STMpipeline. (A)Astheagentmovesthroughtheenvironment,statessthatexceeded
a pre-definedthresholdare recordedalong with all successive actions a in an S-sequence. (B) S-
sequencesaresavedinabufferattheendofeachepisode.(C)S-sequencesaresampledfortraining
asubjective-timescaletransitionmodel.
Atransitiondynamicsmodelisnecessarilytrainedwithrespecttoactionsthatanagenttooktoreach
subsequentstates. However,STMrecordsmemoriesoveranarbitrarynumberofsteps,thusleaving
actionsequencesofvariablelength.Forthepurposesofthispaper,weimplementasimpleheuristic
tosummariseagent’strajectories,whichisenoughtoprovideSTMwiththenecessaryinformation
tolearnaction-conditionedpredictions.Wedoitbyestimatingtheanglebetweentheagent’sinitial
positionanditsfinalpositionatthetime-stepofthesubsequentmemory.Fulldetailsofthisheuristic
canbefoundinAppendixB.4.
4.2 TRANSITION DYNAMICS MODEL
Asmentioned,S-sequencesarecharacterisedbythepresenceofepistemically-surprisingandsalient
eventssqueezedtogetherintherecordedepisodes. Asaresult,trainingonthesesequencesismore
conducivefor learningtemporaldependenciesbetweenimportantstates. For thisreason, we train
an LSTM modelover the S-sequences, which utilises internalmemorystates to store information
aboutprecedinginputs. Inourarchitecture,anLSTMcalculateshiddenstateh atsubjectivetime
τ
τ usingadeterministicmapping,
h =f (s ,a ,h )=σ(x W +h U +b ), (4)
τ θh τ τ τ−1 τ h τ−1 h h
wheres anda arethelatentstateandactiontakenatsubjectivetimeτ respectively,x isthecon-
τ τ τ
catenatedvectorofs anda ,andθ = {W ,U ,b }aredeterministicLSTMmodelparameters.
τ τ h h h h
Importantly, function f is deterministic and serves only to encode information about preceding
θs
stepsintothehiddenstateoftheLSTM.Thishiddenstateh isthenmappedtoalatentstates at
τ τ+1
thenextsubjectivetimeτ +1viaafeed-forwardneuralnetworkwithrandom-variableparameters,
θ ,usingp(s |h ;θ )withMCdropout.Theparametersofbothofthenetworksaretrainedvia
hs τ τ−1 hs
backpropagationwithalossfunctiondefinedas
T
1
L= D q(s ;φ )kp(s |f (s ,a ,h );θ ) (5)
T
KL τ+1 s τ+1 θh τ τ τ−1 hs
Xτ h i
Thenewgenerativemodelofobservationsisshownin Figure1B. Becausethe mappingofLSTM
isdeterministic,theformulationofthevariationalfreeenergyremainsintactwiththeexceptionof
the second term that now includes the state prediction produced by the network p(s |h ;θ )
τ τ−1 hs
conditionedonthehiddenstateoftheLSTM,
F =−E [logp(o |s ;θ )]
τ q(sτ) τ τ o
+D [q(s ;φ )kp(s |h ;θ )] (6)
KL τ s τ τ−1 hs
+E [D [q(a ;φ )kp(a )]]
q(sτ) KL τ a τ
ArchitecturalandtrainingdetailsofthemodelcanbefoundinAppendixB.3. Thesourcecodewill
bemadeavailableafterthereviewprocess.
6
EpisodicMemoryforSubjective-TimescaleModels
A B
5h 57min
14h 20min
4h 32min
Figure3:ExperimentalResults.(A)Cumulativerewardscollectedbytheagents.Itcanbeseenthat
theSTM-MCTSagentshowsimprovedperformanceevenwhencomparedwiththecomputationally-
expensiveBaseline-MPC.(B)Meannumberofrewards(spheres)bycategory.STM-MCTScollects
moregreenandyellow(positive)rewardsthanthebaselineagents.However,Baseline-MPCcollects
fewer(negative)redrewards,whichislikelyrelatedits abilityto evaluateactionsaftereverystep.
Uncertaintyregionsandbarsindicateonestandarddeviationover5runs.
5 EXPERIMENTS
TheAnimal-AI(AAI)environmentisavirtualtestbedthatprovidesanopen-endedsandboxtraining
environment for interacting with a 3D environment from first-person observations (Crosbyetal.,
2020;Crosby,2020).InAAI,anagentistaskedwithreachingagreenspheregivenaparticularsetup
that may include intermediary rewards (yellow spheres), terminal negative rewards (red spheres),
obstacles(e.g. walls),etc. Forthepurposesofthiswork,weuseasparselypopulatedconfiguration
withsinglegreen,red,andyellowspheres,inwhichasuccessfulagentwouldbeforcedtoperform
bothshort-andlong-distanceplanning,aswellasmoreextensiveexplorationoftheenvironment.
5.1 EXPERIMENTALRESULTS
We tested the STM agent using 100,000steps in randomly-generatedenvironments(max episode
length of 500) against the baseline system with two different planning procedures – MCTS and
model predictive control (MPC). In contrast to MCTS, the MPC agent re-evaluates its plan after
every action. Figure 3 summarises the experimentalresults. Our STM-MCTS agent outperforms
the baseline systems in acquiring more rewards within the 100,000 steps. In particular, we note
thattheSTM-MCTSagentshowedsignificantimprovementagainsttheBaseline-MCTS.Similarly,
weshowthatSTM-MCTSmodelretrievesmorecumulativerewardthantheBaseline-MPCagent,
which uses a computationally expensive planning procedure. Specifically, our agent achieves a
highercumulativerewardinlessthanhalfthetime,∼6hours,comparedto∼14hours.
5.2 ROLL-OUTINSPECTION
Inspecting prediction roll-outs produced by the STM-based system provides great insight into its
practical benefits for the agent’s performance. Specifically, our agent is capable of varying the
temporalextentofitspredictionsandimaginingfuturesalientevents.
5.2.1 VARYING PREDICTION TIMESCALE
Muchlikehumanperceptionoftimechangesdependingontheperceptualcontentofthesurround-
ings, our agent varies the prediction timescale dependingon the context it finds itself in. Specif-
ically, in the AAI environmentthe complexityofany givenobservationis primarilydrivenby the
presenceofobjects, whichmayappearindifferentsizes, colours, andconfigurations. As a result,
ouragentconsistentlypredictsfartherintothefutureintheabsenceofanynearbyobjects,andslows
itstimescale,predictingatfinertemporalrate,whentheobjectsareclose.
Practically,thishasseveralimportantimplications. First,performingtemporaljumpsandskipping
unnecessaryintermediarystepsaffordsgreatercomputationalefficiency,andreducesthedetrimental
effectsofsequentialerroraccumulation,ascanbeseeninFigure4. Second,whileSTMisableto
predictfarahead,itsinherentflexibilitytopredictovervaryingtimescalesdoesnotcompromisethe
7
EpisodicMemoryforSubjective-TimescaleModels
agent’sperformancewhenthestatesofinterestareclose. Thus,aseparatemechanismforadjusting
howfarintothefutureanagentshouldplanisnotnecessaryandisimplicitlyhandledbyourmodel.
Third, STM allows the agent to make more informed decisions in an environment, as it tends to
populate the roll-outs with salient observations of the short- and long-term futures depending on
thecontext. Asaresult,STMeffectivelyre-focusesthecentralpurposeofatransitionmodelfrom
most accurately modelingthe ground-truthdynamics of an environmentto predicting states more
informativewithrespecttotheaffordancesoftheenvironment.
Figure 4: STM can vary prediction timescale, with large gaps between predictions at the start,
whichbecomesmorefine-grainedastheobjectgetscloser. Objective-timescalemodelsuffersfrom
slow-timescalepredictionsanderroraccumulation,resultinginpoorly-informativepredictions.
5.2.2 IMAGINING SURPRISING EVENTS
As mentioned, S-sequences frequently include epistemically-surprising transitions, which, in the
contextoftheAAIenvironment,constituteeventswhereobjectscomeintoview. Asaresult,STM
issignificantlymorelikelytoincluderoll-outswithnewobjectsappearingintheframe,incontrast
tothebaselinethatemploystheobjective-timescaletransitionmodel.
TheabilityoftheSTMtoimaginenovelandsalientfutureeventsencouragesexploratorybehaviour,
whichisdistinctfromtheactiveinferenceagent’sintrinsicexploratorymotivations.Weagainstress
thatalthoughthepredictedfuturesmaybeinaccuratewithrespecttotheground-truthpositionsof
theobjects,theyareneverthelessmoreinformativewithrespecttotheagent’spotentialaffordances
in the environment. This is in stark contrast with the objective-timescale model, which imagines
futures in the absence of any objects. As a result, the STM agent is less prone to get stuck in a
sub-optimalstate, which was commonlyobserved in the baseline system, and is more inclined to
exploretheenvironmentbeyonditscurrentposition.
Figure5:STMisabletoimaginesurprisingevents.Despitethefactthatappearanceofobjectsisa
rareeventintheenvironment,STMfrequentlypredictsthemintheroll-outs. Incontrast,objective-
timescalemodelisnotcapableofthatasadirectcorollaryofitstrainingprocedure.
6 CONCLUSION AND FUTURE WORK
We proposedSTM, a novelapproachto learning a transition dynamicsmodel with the use of se-
quences of episodic memories, which define an agent’s more useful, subjective timescale. STM
showedsignificantimprovementagainstthebaselineagent’sperformanceintheAAIenvironment.
Inspiredbytheproblemsofinaccurateandinefficientlong-termpredictionsinmodel-basedRLand
therecentneuroscienceliteratureonepisodicmemoryandhumantimeperception,wemergedideas
fromthedifferentfieldsintoonenewtechniqueoflearningaforwardmodel.Wefurtheremphasised
twoimportantcharacteristicsofthenewly-devisedmodel–itsabilitytovarythetemporalextentof
futurepredictionsandtopredictfuturesalientevents.Theapplicationofourtechniqueisnotlimited
toactiveinference,andcanbeadaptedforuseinothermodel-basedframeworks.
8
EpisodicMemoryforSubjective-TimescaleModels
Futureworkmayexploremoregeneralisedapproachesofactionsummarisationanddynamicthresh-
oldingformemoryformation.Anotherenticingdirectionofresearchistoinvestigatethefeasibility
ofhavingasingletransitionmodelthatslowlytransitionsfromtrainingonanobjectivetimescaleto
trainingonasubjectivetimescale,asthememoryformationgoeson.
ACKNOWLEDGMENTS
MatthewCrosby’scontributiontothisworkwassupportedbytheLeverhulmeCentrefortheFuture
ofIntelligence,LeverhulmeTrust,underGrantRC-2015-067.
REFERENCES
Mart´ınAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL
https://www.tensorflow.org/. Softwareavailablefromtensorflow.org.
Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using inaccurate models in reinforcement
learning. InICML’06,2006.
J.AndrewBagnellandJeffG.Schneider.Autonomoushelicoptercontrolusingreinforcementlearn-
ingpolicysearchmethods. Proceedings2001ICRA.IEEEInternationalConferenceonRobotics
andAutomation(Cat.No.01CH37164),2:1615–1620vol.2,2001.
Benjamin Beyret, Jose´ Herna´ndez-Orallo, Lucy G Cheke, Marta Halina, Murray Shanahan, and
MatthewCrosby. TheAnimal-AIEnvironment:Trainingandtestinganimal-likeartificialcogni-
tion. ArXiv,abs/1909.07483,2019.
Charles Blundell, Julien Cornebise, K. Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neuralnetworks. ArXiv,abs/1505.05424,2015.
Charles Blundell, Benigno Uria, AlexanderPritzel, Yazhe Li, AvrahamRuderman, Joel Z. Leibo,
Jack W. Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. ArXiv,
abs/1606.04460,2016.
MatthewM Botvinick,Sam Ritter, JaneX. Wang, ZebKurth-Nelson,andDemisHassabis. Rein-
forcementlearning,fastandslow. Trendsincognitivesciences,235:408–422,2019.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficientreinforcementlearningwithstochasticensemblevalueexpansion. InNeurIPS,2018.
CatalinV.BuhusiandWarrenH.Meck. Whatmakesustick? functionalandneuralmechanismsof
intervaltiming. NatureReviewsNeuroscience,6:755–765,2005.
L. Cheke and N. Clayton. Eurasian jays (garrulus glandarius) overcome their current desires to
anticipatetwodistinctfutureneedsandplanforthemappropriately.BiologyLetters,8:171–175,
2011.
IgnasiClavera,YaoFu,andPieterAbbeel.Model-augmentedactor-critic:Backpropagatingthrough
paths. ArXiv,abs/2005.08068,2020.
NicolaS.Clayton,TimothyJ.Bussey,andAnthonyDickinson. Cananimalsrecallthepastandplan
forthefuture? NatureReviewsNeuroscience,4:685–691,2003.
Matthew Crosby. Buildingthinkingmachinesbysolving animalcognitiontasks. Minds andMa-
chines,2020.
Matthew Crosby, Benjamin Beyret, Murray Shanahan, Jose´ Herna´ndez-Orallo, Lucy Cheke, and
MartaHalina. TheAnimal-AItestbedandcompetition. volume123ofProceedingsofMachine
LearningResearch,pp.164–176.PMLR,2020.
MarcPeterDeisenrothandCarlE. Rasmussen. Pilco: A model-basedanddata-efficientapproach
topolicysearch. InICML,2011.
9
EpisodicMemoryforSubjective-TimescaleModels
Stefan Depeweg,Jose´ MiguelHerna´ndez-Lobato,Finale Doshi-Velez, andSteffenUdluft. Learn-
ing and policy search in stochastic dynamical systems with bayesian neural networks. ArXiv,
abs/1605.07127,2017.
BenDeverett,RyanFaulkner,MeireFortunato,GregWayne,andJoelZ.Leibo. Intervaltimingin
deepreinforcementlearningagents. ArXiv,abs/1905.13469,2019.
LasseEspeholt,HubertSoyer,Re´miMunos,KarenSimonyan,VolodymyrMnih,TomWard,Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Im-
pala: Scalable distributed deep-rlwith importanceweighted actor-learnerarchitectures. ArXiv,
abs/1802.01561,2018.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-basedvalueexpansionforefficientmodel-freereinforcementlearning. 2018.
ZafeiriosFountas,NoorSajid,PedroAMMediano,andKarlFriston. Deepactiveinferenceagents
usingmonte-carlomethods. AcceptedtoNeurIPS,2020a.
ZafeiriosFountas,AnastasiaSylaidi,KyriacosNikiforou,AnilK.Seth,MurrayShanahan,andWar-
rickRoseboom.Apredictiveprocessingmodelofepisodicmemoryandtimeperception.bioRxiv,
2020b.
Karl J. Friston. A free energy principle for a particular physics. arXiv: Neurons and Cognition,
2019.
KarlJ. Friston, JamesKilner, andLee Harrison. A freeenergyprincipleforthebrain. Journalof
Physiology-Paris,100:70–87,2006.
KarlJ.Friston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasH.B.FitzGerald,
and GiovanniPezzulo. Active inferenceand epistemicvalue. Cognitive Neuroscience, 6:187–
214,2015.
KarlJ.Friston,ThomasH.B.FitzGerald,F.Rigoli,P.Schwartenbeck,J.O’Doherty,andG.Pezzulo.
Activeinferenceandlearning. NeuroscienceandBiobehavioralReviews,68:862–879,2016.
Karl J. Friston, Thomas H. B. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo. Active
inference:Aprocesstheory. NeuralComputation,29:1–49,2017a.
KarlJ.Friston,ThomasH.B.FitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanni
Pezzulo. Activeinference:Aprocesstheory. NeuralComputation,29:1–49,2017b.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha
Ondobaka. Activeinference,curiosityandinsight. NeuralComputation,29:2633–2683,2017c.
KarlJ.Friston,RichardERosch,ThomasParr,CathyJ.Price,andHowardBowman.Deeptemporal
modelsandactiveinference. NeuroscienceandBiobehavioralReviews,90:486–501,2017d.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertaintyindeeplearning. InICML,2016.
SamuelJ. Gershmanand NathanielD. Daw. Reinforcementlearningandepisodicmemoryin hu-
mansandanimals:Anintegrativeframework. AnnualReviewofPsychology,68:101–128,2017.
AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.ArXiv,abs/1410.5401,2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwin´ska, SergioGo´mezColmenarejo,EdwardGrefenstette, etal. Hybridcomputingusinga
neuralnetworkwithdynamicexternalmemory. Nature,538:471–476,2016.
Daniel L. Greenberg and Mieke Verfaellie. Interdependence of episodic and semantic memory:
evidencefromneuropsychology.JournaloftheInternationalNeuropsychologicalSociety: JINS,
165:748–53,2010.
10
EpisodicMemoryforSubjective-TimescaleModels
AndreaGreve,Elisa Cooper,AlexanderKaula, MichaelC. Anderson,andRichard N. A. Henson.
Doespredictionerrordriveone-shotdeclarativelearning? JournalofMemoryandLanguage,94:
149–165,2017.
David R Ha and Ju¨rgen Schmidhuber. Recurrent world models facilitate policy evolution. In
NeurIPS,2018.
Jessica B. Hamrick. Analogues of mental simulation and imagination in deep learning. Current
OpinioninBehavioralSciences,29:8–16,2019.
StevenHansen,PabloSprechmann,AlexanderPritzel,Andr’eBarreto,andCharlesBlundell. Fast
deepreinforcementlearningusingonlineadjustmentsfromthepast. InNeurIPS,2018.
Demis Hassabis, Dharshan Kumaran, Seralynne D. Vann, and Eleanor A. Maguire. Patients with
hippocampalamnesiacannotimaginenewexperiences. ProceedingsoftheNationalAcademyof
Sciences,104:1726–1731,2007.
ThomasT.Hills. Towardsaunifiedtheoryofanimaleventtiming. 2003.
SeppHochreiterandJu¨rgenSchmidhuber.Longshort-termmemory. NeuralComputation,9:1735–
1780,1997.
Chia-ChunHung,T.Lillicrap,JoshAbramson,YanWu,M.Mirza,F.Carnevale,ArunAhuja,and
G.Wayne. Optimizingagentbehavioroverlongtimescalesbytransportingvalue. NatureCom-
munications,10,2019.
Anthony I Jang, Matthew R. Nassar, Daniel G. Dillon, and Michael J. Frank. Positive reward
predictionerrorsstrengthenincidentalmemoryencoding. bioRxiv,2018.
Hyunwoo Jung, Moonsu Han, Minki Kang, and Sung Ju Hwang. Learning what to remember:
Long-termepisodicmemorynetworksforlearningfromstreamingdata. ArXiv,abs/1812.04227,
2018.
Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.
ArXiv,abs/1703.03129,2017.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi,
GeorgeTucker,andHenrykMichalewski. Model-basedreinforcementlearningforatari. ArXiv,
abs/1903.00374,2020.
GabrielKalweitandJoschkaBoedecker. Uncertainty-drivenimaginationforcontinuousdeeprein-
forcementlearning. InCoRL,2017.
KenKansky,TomSilver,DavidA.Me´ly,MohamedEldawy,MiguelLa´zaro-Gredilla,XinghuaLou,
N. Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot
transferwithagenerativecausalmodelofintuitivephysics. ArXiv,abs/1706.04317,2017.
NanRosemaryKe,AmanpreetSingh,AhmedTouati,AnirudhGoyal,YoshuaBengio,DeviParikh,
andDhruvBatra. Learningdynamicsmodelinreinforcementlearningbyincorporatingthelong
termfuture. ArXiv,abs/1903.01599,2019.
JonathanKo,DanielJ.Klein,DieterFox,andDirkHa¨hnel. Gaussianprocessesandreinforcement
learningforidentificationandcontrolofanautonomousblimp. Proceedings2007IEEEInterna-
tionalConferenceonRoboticsandAutomation,pp.742–747,2007.
VikashKumar,EmanuelTodorov,andSergeyLevine. Optimalcontrolwithlearnedlocalmodels:
Application to dexterous manipulation. 2016 IEEE International Conference on Robotics and
Automation(ICRA),pp.378–383,2016.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-regionpolicyoptimization. ArXiv,abs/1802.10592,2018.
Ma´te´ Lengyel and Peter Dayan. Hippocampalcontributionsto control: The third way. In NIPS,
2007.
11
EpisodicMemoryforSubjective-TimescaleModels
SergeyLevineandPieterAbbeel. Learningneuralnetworkpolicieswithguidedpolicysearchunder
unknowndynamics. InNIPS,2014a.
SergeyLevineandPieterAbbeel. Learningneuralnetworkpolicieswithguidedpolicysearchunder
unknowndynamics. InNIPS,2014b.
SergeyLevine,ChelseaFinn,TrevorDarrell,andPieterAbbeel. End-to-endtrainingofdeepvisuo-
motorpolicies. J.Mach.Learn.Res.,17:39:1–39:40,2016.
Johannes B. Mahr and Gergely Csibra. Why do we remember? the communicative function of
episodicmemory. TheBehavioralandbrainsciences,pp.1–93,2017.
Rowan McAllister and Carl Edward Rasmussen. Improving pilco with bayesian neural network
dynamicsmodels. 2016.
KourkenMichaelian.Mentaltimetravel:Episodicmemoryandourknowledgeofthepersonalpast.
2016.
BerenMillidge.Deepactiveinferenceasvariationalpolicygradients.ArXiv,abs/1907.03876,2019.
Nikhil Mishra, Pieter Abbeel, and Igor Mordatch. Prediction and control with temporal segment
models. ArXiv,abs/1703.04070,2017.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,MarcG.Belle-
mare,AlexGraves,MartinA.Riedmiller,etal. Human-levelcontrolthroughdeepreinforcement
learning. Nature,518:529–533,2015.
Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dy-
namics for model-based deep reinforcement learning with model-free fine-tuning. 2018 IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pp.7559–7566,2018.
Junhyuk Oh, Valliappa Chockalingam, Satinder P. Singh, and Honglak Lee. Control of memory,
activeperception,andactioninminecraft. ArXiv,abs/1605.09128,2016.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria` Puigdome`nech Badia, Oriol Vinyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. ArXiv,
abs/1703.01988,2017.
Se´bastien Racanie`re, Theophane Weber, David P. Reichert, Lars Buesing, Arthur Guez,
DaniloJimenezRezende,etal. Imagination-augmentedagentsfordeepreinforcementlearning.
InNIPS,2017.
WarrickRoseboom,Z. Fountas,KyriacosNikiforou,DavidBhowmik,M. Shanahan,andA. Seth.
Activity in perceptual classification networks as a basis for human subjective time perception.
NatureCommunications,10,2019.
Nina Rouhani, Kenneth A. Norman, and Yael Niv. Dissociable effects of surprising rewards on
learningandmemory. JournalofExperimentalPsychology: Learning,Memory, andCognition,
44:1430–1443,2018.
NoorSajid,PhilipJ.Ball,andKarlJ.Friston. Activeinference: demystifiedandcompared. arXiv:
ArtificialIntelligence,2019.
D. Schacter, D. Addis, D. Hassabis, V. C. Mart´ın, R. N. Spreng, and K. Szpunar. The future of
memory:Remembering,imagining,andthebrain. Neuron,76:677–694,2012.
DanielL.Schacter,DonnaRoseAddis,andRandyL.Buckner. Rememberingthepasttoimagine
thefuture:theprospectivebrain. NatureReviewsNeuroscience,8:657–661,2007.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
CoRR,abs/1511.05952,2016.
Maxine T Sherman, Zafeirios Fountas, Anil K Seth, and Warrick Roseboom. Accumulation of
salientperceptualeventspredictssubjectivetime. bioRxiv,2020.
12
EpisodicMemoryforSubjective-TimescaleModels
D.Silver,JulianSchrittwieser,K.Simonyan,IoannisAntonoglou,AjaHuang,A.Guez,T.Hubert,
L.Baker,etal.Masteringthegameofgowithouthumanknowledge.Nature,550:354–359,2017.
Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active
inference. ArXiv,abs/1911.10601,2019.
EndelTulving. Episodicandsemanticmemory. 1972.
KaiUeltzho¨ffer. Deepactiveinference. BiologicalCybernetics,112:547–573,2018.
ManuelWatter,JostTobiasSpringenberg,JoschkaBoedecker,andMartinA.Riedmiller. Embedto
control:Alocallylinearlatentdynamicsmodelforcontrolfromrawimages. InNIPS,2015.
Guangxiang Zhu, Zichuan Lin, Guangwen Yang, and Chongjie Zhang. Episodic reinforcement
learningwithassociativememory. InICLR,2020.
Ozan C¸atal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy
selectionusingactiveinference. ArXiv,abs/1904.08149,2019.
13
EpisodicMemoryforSubjective-TimescaleModels
A PRELIMINARIES
A.1 ACTIVE INFERENCE
Active inference is a corollary of the free-energy principle applied to action (Fristonetal., 2016;
Friston, 2019; Sajidetal., 2019). In this framework, an agent embedded in an environmentaims
todotwothings: (i)minimisesurprisalfromtheobservationsoftheenvironmentundertheagent’s
internalmodelofthisenvironment,and(ii)performactionssoastominimisetheexpectedsurprisal
in the future. More formally, an agent is equipped with a generativemodel p(o ,s ;θ), where o
t t t
is the agent’s observation at time t, s is the hidden state of the environment, and θ denotes the
t
parametersof the generativemodel. The agent’s surprise at time t is defined as the negativelog-
likelihood,−logp(o ;θ).
t
We can upper-boundthis intractable expression using variational inference by introducingan ap-
proximateposteriordistribution,q(s ),overs ,suchthat:
t t
−logp(o ;θ)≤E [logq(s )−logp(o ,s ;θ)]=F, (7)
t q(st) t t t
where F is the variational free energy. The minimisation of this quantity realises objective (i)
and is performed by optimising the parameters of the generative model, θ. It is also equivalent
tothemaximisationofmodelevidence,whichintuitivelyimpliesthattheagentaimstoperfectits
generativemodelatexplainingthesensoryobservationsfromtheenvironment. Torealiseobjective
(ii),theagentmustselectactionsthatleadtothelowestexpected surpriseinthefuture,whichcan
becalculatedusingtheexpectedfreeenergy(EFE),G:
G(π,τ)=E E [logq(s |π)−logp(o ,s |π)] , (8)
p(oτ|sτ) q(sτ|π) τ τ τ
h i
variationalfreeenergy,F
| {z }
where τ > t and π = {a ,a ,...,a } is a sequence of actions (policy) between the present
t t+1 τ−1
timetandthefuturetimeτ. Thefree-energyminimisingsystemmust,therefore,imaginethefuture
observationsgivenapolicyandcalculatetheexpectedfreeenergyconditionedontakingthispolicy.
Then,actionsthatledtolowervaluesoftheEFEarechosenwithhigherprobability,asopposedto
actionsthatledtohighervaluesofEFE,suchthat:
p(π)=σ(−γG(π)), (9)
whereG(π)= G(π,τ),γ isthetemperatureparameter,σ(·)denotesasoftmaxfunction,and
τ>t
tisthepresenttPimestep.
B ARCHITECTURAL DETAILS AND TRAINING
B.1 BASELINE IMPLEMENTATION
As mentioned, each component of the generative and inference models is parametrised by feed-
forwardneuralnetworks(includingfully-connected,convolutionalandtranspose-convolutionallay-
ers),whosearchitecturaldetailscanbefoundinFigure6. Thelatentbottleneckoftheautoencoder,
s,wasofsize10.Thehyperparametersofthetop-downattentionmechanismwere:a=2,b=0.5,
c = 0.1, and d = 5, chosen to match those in Fountasetal. (2020a). Similarly, we restricted the
action space to just 3 actions – forward, left, right. For testing, we optimised the MCTS param-
eters of the baseline agent, setting the explorationhyperparameterc = 0.1 (see Eq.10), and
explore
performing30 simulation loops, each with depth of 1. The networks were trained using separate
optimisersforstabilityreasons.Thehabitualandtransitionnetworksaretrainedwithalearningrate
of 0.0001; the autoencoder’soptimiserhad a learning rate of 0.001. The batch size was set to 50
andthemodelwastrainedfor750kiterationsunderagreenobservationalprior.Allofthenetworks
wereimplementedusingTensorflowv2.2(Abadietal.,2015). TestswereperformedinAnimal-AI
v2.0.1(Beyretetal.,2019).
Furthermore,followingFountasetal.(2020a),wedefinetheMCTSupperconfidenceboundas,
1
U(s,a)=G˜(s,a)+c ·q(a|s;φ )· . (10)
explore a
N(a,s)+1
14
EpisodicMemoryforSubjective-TimescaleModels
Figure6: Implementationofthebaselinesystem.
As discussed, each network was trained with its corresponding loss function, which are the con-
stituent parts of the total variationalfree energy. In particular, the autoencoderwas trained using
Eqs. 1aand1b,transitionmodelusingEq.1b,andhabitualnetworkusingEq.1c.
Furthermore,followingthetrainingprocedurefromFountasetal.(2020a),westabilisetheconver-
genceoftheautoencoderbymodifyingthelossfunctionto:
L =−E [logp(o |s ;θ )]+γD [q(s ;φ )||p(s |s ,a ;θ )]
autoencoder q(st) t t o KL t s t t−1 t−1 s
(11)
+(1−γ)D [q(s ;φ )||N(000,I)],
KL t s
whereγ isahyperparameterthatgraduallyincreasesfrom0to0.8duringtraining.
B.2 PRIORITISED EXPERIENCE REPLAY
Aspartofthe baselinesystem’strainingprocedure,we utilise prioritisedexperiencereplay(PER)
(Schauletal., 2016) to mitigate the detrimentaleffectsof on-linelearning(whichwas used in the
originalpaperbyFountasetal.(2020a)),andtoencouragebetterobject-centricrepresentations.
Inparticular,on-linelearninghasthreemajorissuesassociatedwithit. First,trainingisperformed
on correlated data points, which is generally considered to be detrimental for training neural net-
works(Schauletal.,2016). Second,observationsthatarerarelyencounteredinanenvironmentare
discardedinon-linelearningandareusedfortrainingonlywhenvisitedagain. Thesearelikelyto
be the observations for which there is most room for improvement. Instead, the agent will often
be training on already well-predicted transitions that it happensto visit often. Finally, an on-line
learningagentisconstrainedbyitscurrentpositioninanenvironmenttosamplenewdataand,thus,
hasverylimitedcontroloverthecontentofitstrainingbatches.
15
EpisodicMemoryforSubjective-TimescaleModels
Furthermore,asmentionedinSection4.1,intheAnimal-AIenvironmentrareobservationsarethose
that include objects; yet, objects are a central component of this environment – the only way to
interact, get rewards, and importantly, the only means of minimising the free energy optimally.
To encourageour agentto learnbetter objectrepresentations,we employPER with the objective-
timescaletransitionmodelfreeenergyastheprioritymetric.Asdiscussed,observationswithhigher
valuesof this metric tend to constitute more complexscenes, which include objects– as the only
sourceofcomplexityintheAAI.SeeFigure7forqualitativeevidenceofthistrend.TheuseofPER
resultedinaconsiderableimprovementinthebaseline’sperformanceandbetterabilitytoreconstruct
observationswithobjects(SeeFigure8).
B.3 STM IMPLEMENTATION
TheSTMintroducestwoadditionalcomponents:STMhabitualnetworkandSTMtransitionmodel.
ThehabitualnetworkwastrainedusingthesametrainingprocedureasdescribedinAppendixB.1.
Thetransitionmodelwastrainedonbatchsize15andalearningrateof0.0005.Eachbatchconsisted
ofzero-paddedS-sequenceswithlength50. WeuseaMaskinglayertoignorezero-paddedpartsof
thesequencesinthecomputationalgraph. Thetrainingwasstoppedat200ktrainingiterations. For
testing STM-MCTSin Section5.1, we optimise the MCTS parameters, setting c = 0.1, and
explore
performing15simulationloops,eachwithdepthof3. Thethreshold(objective-timescaletransition
modelfreeenergy),ǫ,wasmanuallysetto5afterinspectionofthebufferandvaluedistribution.
B.4 ACTION HEURISTIC
TotraintheSTMtransitionmodelintheAnimal-AIenvironment,weimplementasimpleheuristic
that is used to summarise a sequence of actions taken by the agent from one memory to reach
the next one. A sequence of actions, A = {a ,a ,...a }, takes the agent from a
τ1 τ1+1 τ1+(N−1)
recorded memory s to memory s , where the time between these states τ − τ = N, and
τ1 τ2 2 1
a ∈ {a ,a ,a }. We employ polar coordinates relative to the agent’s initial position in
forward right left
Cartesiancoordinatesattimeτ andperformiterativeupdatesofitspositionaftereveryactionun-
1
til the time-step of the next episodic memory, τ , is reached. Given the agent’s orientationin the
2
environment,θ,thenextpositionoftheagentiscalculatedusing,
sinθ 0
p =p + , wherep = (12)
t+1 t (cid:20)cosθ(cid:21) t (cid:20)0(cid:21)
t=τ1
Finally,weretrieveangleφ,whichdescribesthedirectioninwhichtheagenthastravelledwithre-
specttoitsinitialpositionandorientation.Thisangleisusedtodecideontheactionthatsummarises
thetrajectoryusing
a |φ|≤22.5◦
forward
a=a 22.5◦ <φ<180◦
right
 a −22.5◦ >φ≥−180◦,
left
Althoughthisheuristicprovidedsatisfactoryresults,trajectoryencodingisoneofthemostlimiting
partsoftheSTMandisapromisingdirectionforfurtherresearch.
16
EpisodicMemoryforSubjective-TimescaleModels
Figure7:Observationssortedbytheircorrespondingrecordedvalueoftheobjective-timescaletran-
sition model free energy in descending order. States with higher values on average contain more
objects,constitutingmorecomplexsettings.
17
EpisodicMemoryforSubjective-TimescaleModels
Figure8: AgenttrainedwithPERisbetteratgeneratingobservationswithobjects. (A)Prediction
roll-outsshowingthatbaselinetrainedwithPERcanbetterrepresentobjects,thuspreservingthemin
futurepredictions.(B)Ground-truthobservations(thirdcolumn)arepassedthroughtheautoencoder
oftheon-lineandPERagents.Ascanbeseen,observationsarebetterreconstructedbythebaseline
systemtrainedwithPER.
Figure9:ImplementationofSTMcomponents.
18
EpisodicMemoryforSubjective-TimescaleModels
C ADDITIONAL RESULTS
WeprovideadditionalresultsoftheSTMpredictionroll-outs:
a) Figure10: randomroll-outsgeneratedbythesystem. Thesediverseroll-outsdemonstrate
that STM is able to: i) make correctaction-conditionedpredictions, ii) speed up its pre-
dictiontimescalewhenobjectsarefaraway,iii)slowdownthepredictiontimescalewhen
objectsarenearby.
b) Figure 11: STM consistently imagines objects coming into view. The observations pro-
ducedbythemodelareentirelyplausiblegiventhepaththeagentistakingandthecontext
it finds itself in. This indicates that STM does indeed produce semantically meaningful
predictions.Itispertinenttonotethattheroll-outscomplywiththephysicsoftheenviron-
ment,whichiscrucial,asitpotentiallyrefutesthehypothesisthattheseimaginedobjects
werepredictedatrandom.
c) Figure12: showstheroll-outsproducedbytheobjective-timescalemodelusingthesame
starting states as in Figure 11. These roll-outsare in stark contrastto those producedby
STM, exemplifyingthe baseline’s inability to imagine objects that are not present in the
initialframe.
19
EpisodicMemoryforSubjective-TimescaleModels
Figure10:Randomroll-outsgeneratedwiththeSTMtransitionmodel.
20
EpisodicMemoryforSubjective-TimescaleModels
Figure 11: STM can imagine objects from ‘uninteresting’ states. Arrows indicate roll-outs with
imaginedobjects.
21
EpisodicMemoryforSubjective-TimescaleModels
Figure12: Incontrastto STM,theobjective-timescaletransitionmodelisnotable toimagineob-
jects,startingwiththesameinitialobservationsasshowninFigure11.
22