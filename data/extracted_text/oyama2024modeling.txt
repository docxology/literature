MODELING AUTONOMOUS SHIFTS BETWEEN FOCUS STATE AND
MIND -WANDERING USING A PREDICTIVE -CODING -INSPIRED
VARIATIONAL RNN M ODEL
Henrique Oyama
Cognitive Neurorobotics Research Unit
Okinawa Institute of Science and Technology Graduate University
Okinawa
henrique.oyama@oist.jp
Jun Tani
Cognitive Neurorobotics Research Unit
Okinawa Institute of Science and Technology Graduate University
Okinawa
jun.tani@oist.jp
ABSTRACT
The current study investigates possible neural mechanisms underling autonomous shifts between
focus state and mind-wandering by conducting model simulation experiments. On this purpose,
we modeled perception processes of continuous sensory sequences using our previous proposed
variational RNN model which was developed based on the free energy principle. The current study
extended this model by introducing an adaptation mechanism of a meta-level parameter, referred to as
the meta-prior w, which regulates the complexity term in the free energy. Our simulation experiments
demonstrated that autonomous shifts between focused perception and mind-wandering take place
when w switches between low and high values associated with decrease and increase of the average
reconstruction error over the past window. In particular, high w prioritized top-down predictions
while low w emphasized bottom-up sensations. This paper explores how our experiment results align
with existing studies and highlights their potential for future research.
Keywords mind-wandering · predictive coding · free energy principle · variational RNN
1 Introduction
During mindfulness practice, such as focusing on sensations like breathing, our attention sometimes spontaneously
deviates to mental imagery or thoughts about the past and future, a phenomenon known as mind-wandering [1, 2, 3].
This shift from a focused state to mind-wandering can occur not only during meditation but also in everyday activities,
such as driving, listening to music, or tasting food.
Mind-wandering tends to occur more frequently during tasks that are either too easy or too difficult. When tasks are
less demanding, such as simply attending to breathing, instances of mind-wandering increase. Conversely, during
more challenging tasks, like reading complex material, our minds are more prone to wander because maintaining focus
becomes difficult over extended periods [4, 5].
An interesting aspect is that the transition from the focused state (FS) to the mind-wandering state (MW) often happens
without conscious awareness, whereas the shift from MW back to FS involves recognizing the mind-wandering episode
consciously [6]. Various studies have investigated the psychological and systematic mechanisms underlying these shifts.
For example, [7] argued that the transition from FS to MW is gradual, as evidenced by increasing response times during
focused tasks. In contrast, [8] suggested that the shift is abrupt, triggered by sudden internal or external stimuli.
arXiv:2412.15620v1  [q-bio.NC]  20 Dec 2024
[9] proposed a model where mental states alternate between FS and MW, with MW episodes ending when individuals
consciously recognize their mind-wandering and return to the task. This “two-stage model” assumes that the probability
of being in FS is higher at the beginning of an episode and decreases over time. However, contrary to this prediction,
[10] found that the probability of FS does not decline within an FS-MW episode in a subject study introducing a probe in
a random timing during the episode. To address this discrepancy, the authors proposed the “multiple sub-event model”,
which hypothesizes that unconscious alternations between FS and MW occur multiple times before an individual
becomes aware of being in MW. Their simulation study suggested that as the number of sub-sequences increases, the
decline in the probability of FS becomes less pronounced.
Although the above mentioned studies clarified some phenomena in the shift from FS to MW from psychological
observation, they have not provided sufficient accounts for the underlying neuronal mechanisms. Recently, some studies
[11, 12] suggested system level neuroscience models incorporated with the concept of the free energy principle (FEP)
[13]. Here, FEP is briefly explained for better understanding of the readers. The FEP is a neuroscience theory that
has attracted large attention. The FEP posits that humans and animals execute various functions such as learning,
perception, and action generation to maximize their chances of survival by minimizing surprises they encounter during
interaction with the environment. According to the FEP, these functions are achieved by optimizing generative models
for predicting the sensation, whereby a common statistical quantity called free energy is minimized. The FEP supports
two frameworks, one is predictive coding and the other is active inference. Predictive coding provides a formalism
accounting for how agents perceive sensations. It suggests that the brain predicts sensory observations in the top-down
pathway, while at the same time updating posterior beliefs about those sensations in the bottom-up pathway whenever
errors arise between predictions and observations [ 14, 13, 15]. By updating posterior beliefs in the direction of
minimizing errors, perceptual inference for the observed sensation can be achieved. On the other hand, active inference
(AIF) provides a theory for action generation by assuming that the brain is embodied deeply and embedded in the
environment, such that acting on it changes future sensory observation. Then, AIF considers that actions should be
selected such that the error between the desired and predicted sensations can be minimized [16, 17].
[11] postures the underlying mechanism of shift from FS to MW using active inference of “mental action” in terms of
attention changes. The proposed model assumes hierarchical probabilistic generative model wherein the hidden meta-
awareness states in the higher level account for "how aware am I of where my attentions is?", the hidden mental states
in the middle level dealing with focus of attention account for "what am I paying attention to?", and the sensorimotor
hidden states in the lowest level do for "what am I perceiving or trying to do?" according to the authors. The states at
each level condition the ones in the next lower level by controlling their precisions or beliefs. Agent’s perceptual and
attentional states are inferred at each time step by means of active inference in minimizing the expected free energy.
The results of simulation experiments show that when the meta-awareness state is manually shifted from high to low,
distracted or MW state is developed more frequently. Under this condition, redirection back to FS by consciously being
aware of the current MW state tends to take more time because of less precision in the attention toward distracted state.
[12] investigated mind-wandering mechanism by conducting a model simulation study on allostasis using hierarchically
organized variation recurrent neural network, so-called the PV-RNN [ 18]. Dynamic behavior of PV-RNN can be
characterized by a meta-level parameter, referred to as meta-prior w, that regulates the complexity term against the
accuracy term in free energy which is minimized in the inference of the posterior probability distribution of the latent
variables. It was shown that high setting of meta-prior w enhances generation of the top-down imagery while low
setting of it enhances the bottom-up sensory perception [19, 20, 21]. Analogous to this, [12] showed that low setting of
w generates stronger sensory bottom-up which leads to FS wherein less change in movement as well as neural activity
are observed. On the other hand, high setting of w generates weak attention to sensation and stronger top-down which
leads to MW wherein more movement as well as neural activity.
The aforementioned FEP-based studies provide valuable insights into macroscopic neural mechanisms, such as
redirecting attention to focused states by inferring one’s attentional state, or generating mind-wandering by balancing
top-down and bottom-up information flows. However, these studies do not provide systematic explanations for how
the shifts between FS and MW could be autonomously generated, since the shift from FS to MW in [11] is caused by
manual change of the meta-aware state from high to low and the one in [12] does this by resetting meta-prior w from
low to high value.
In this regard, the current study speculates that autonomous transition between FS and MW could be generated by
introducing an adaptation mechanism of meta-prior w to PV-RNN in which w is modulated with response to some
macroscopic variables such as an average prediction error. In our study, PV-RNN learns to predict a target sequence of
continuously changing sensory patterns which is generated by means of predetermined probabilistic transitions among
a set of cyclic patterns. In the test phase after the training, given one of the pre-trained cyclic patterns as the target
inputs, the PV-RNN predicts encountering sensory inputs by simultaneously inferring the approximated posterior of the
latent state at each time step by minimizing the free energy while adapting w. Analogous to studies [19, 20, 21, 12],
2
when w modulates to a lower value by reflecting surge of the average reconstruction error, the inference process may
improve by placing greater emphasis on bottom-up sensations. This situation may correspond to FS. On the other
hand, when w modulates to a higher value by responding to decline of the average reconstruction error, the PV-RNN
may generate top-down imagery by following the learned probabilistic transitions of patterns while ignoring the target
sensory inputs. This may correspond to MW. Our simulation study with PV-RNN under various parameter settings will
evaluate this hypothesis. The following section introduces the proposed model, followed by a detailed description of the
simulation experiment setup, the presentation of the results, and a discussion that includes proposals for extensions to
future research work.
2 Materials and Methods
2.1 Overview
This study investigates autonomous shifts between the focused state (FS) and mind-wandering (MW) during a perception
task using sequential sensory input patterns. The predictive coding framework is employed to model this perception
process. Predictive coding assumes a generative model that predicts sensory sequences by learning both the latent state
transition function and the likelihood mapping from latent states to sensory observations. Additionally, this generative
model infers the current latent state through continuous sensory sequence observations.
Both learning and inference processes are achieved by minimizing prediction error or, more specifically, free energy. We
hypothesize that FS is enhanced by strengthening bottom-up inference, while MW becomes more likely by emphasizing
top-down sensory pattern generation. It is also hypothesized that shifts between FS and MW take place autonomously
incorporating with adaptation of meta-level states with response to particular system variables. To test this, we propose
an extended version of a variational recurrent neural network model, referred to as the Predictive Coding Inspired
Variational RNN (PV-RNN) [18]. Details of the original PV-RNN and its extensions are provided in the following
sections.
2.2 Predictive Coding Inspired Variational RNN Model (PV-RNN)
The PV-RNN is based on the free energy principle [13], where learning and inference are achieved by minimizing free
energy (Equation 1) in accordance with Bayes’ theorem:
F = DKL[qϕ(z|X)∥pθ(z)]| {z }
complexity
−Eqϕ(z|X)[log pθ(X|z)]
| {z }
accuracy
(1)
Here, pθ(X) is the marginal likelihood of the sensory observation X, given the generative model pθ parameterized by θ.
The latent variables z and inference model qϕ, parameterized by ϕ, allow for posterior inference through minimization
of free energy. Free energy consists of two terms: the complexity term (a measure of divergence between prior and
posterior distributions) and the accuracy term (log-likelihood of sensory observations) [22]. PV-RNN serves as both a
generative model and an inference model. The generative model predicts future sensory inputs via top-down processes,
while the inference model estimates the approximate posterior from observed sensory sequences through free energy
minimization as bottom-up processes.
The following subsections describe the PV-RNN implementation and the use of the meta-priorw.
2.2.1 Model Implementation
The free energy ˜F for PV-RNN predicting a time series of T steps is given by:
F = w
TX
t=1
Eqϕ(z1:t−1|dt−1,Xt−1:T )

DKL[qϕ(zt|dt−1, Xt:T )∥pθ(zt|dt−1)]

| {z }
complexity
−
TX
t=1
Eqϕ(z1:t−1|dt−1,Xt:T )[log pθ(Xt|dt)]
| {z }
accuracy
(2)
3
PV-RNN introduces two types of latent variables: probabilistic latent variables (z) governed by Gaussian distributions,
and deterministic latent variables (d). Their relationships are shown in Figure 1. In equation 2, a meta-level parameter,
named meta-prior w, is introduced to balance the complexity and accuracy terms during this process. This regulation
is particularly important when the limited amount of training data prevents reliable estimation of latent variable
distributions. Also, dynamic behavior of PV-RNN is largely affected by setting of the meta-prior. It was shown that high
setting of meta-prior w enhances generation of the top-down imagery while low setting of it enhances the bottom-up
sensory perception [19, 20, 21, 12].
Figure 1: A hierarchical two-layer PV-RNN architecture. Solid blue lines represent the generative process, while dotted
red lines indicate the inference process. The shaded area shows an inference window of length 3.
Next, the forward computation of each variable used in PVRNN is described. At each time step t, the internal states of
the l-th layer (hl
t) are recursively computed:
hl
t =

1 − 1
τl

hl
t−1 + 1
τl

Wll
dd˜dl
t−1 + Wll
zdzl
t + Wll+1
dd ˜dl+1
t−1 + Wll−1
dd ˜dl−1
t−1 + bl
h

˜dl
t = tanh(hl
t)
(3)
The PV-RNN structure supports hierarchical information processing using time constantsτl, enabling the differentiation
of temporal dynamics across layers [23, 24].
4
The generative model computes prior distributions (zp
t ) as Gaussian variables parameterized by mean (µp
t ) and standard
deviation (σp
t ):
µp
t = tanh(Wll
dµ˜dt−1 + bp
µ)
σp
t = exp(Wll
dσ˜dt−1 + bp
σ)
zp
t = µp
t + σp
t ∗ ϵt with ϵt ∼ N(0, I)
(4)
bp
µ and bp
σ are bias terms for µp
t and σp
t , respectively. ϵ represents a noise sampled from a standard normal distribution
for usage of the reparameterization trick [25]. Analogous to the computation of the prior distribution, the inference
model qϕ approximates the posterior zq
t as a Gaussian distribution with mean µq
t and standard deviation σq
t .
µq
t = tanh(Wll
dµ˜dt−1 + Aµ
t + bq
µ)
σq
t = exp(Wll
dσ˜dt−1 + Aσ
t + bq
σ)
zq
t = µq
t + σq
t ∗ ϵt with ϵt ∼ N(0, I)
(5)
where bq
µ and bq
σ are bias terms for computing µq
t and σq
t , respectively. Aµ
t and Aσ
t are adaptive variables to be
optimized for inferring the posterior distribution which is parameterized by µq
t and σq
t .
Intuitively, the random variablezp can be regarded as a time-dependent prior/top-down expectation about the encounter-
ing sensation. The adaptive vector A (i.e., zq) can be regarded as the approximate posterior distribution that may or
may not be close to the prior distribution, depending on the setting of meta-prior. zp and zq are used by the generative
and inference model, respectively to compute the latent variable d.
2.2.2 Learning and Inference
The free energy F of PV-RNN can be computed as follows by adapting the original equation 2. Given a PV-RNN with
L layers, predicting a T time series sensory inputs, F can be written as
F =
TX
t=1
" LX
l=1
˜wlDKL[qϕ(zl
t|dl
t−1, Xt:T )∥pθ(zl
t|dl
t−1)]
#
−
TX
t=1
∥Xt − ¯Xt∥2
2 (6)
where ˜wl is w specific to lth layer, and ¯X denotes the prediction output of the PV-RNN. In equation 6, we approximate
the expectation with respect to the approximate posterior by iterative sampling. Also, the accuracy term is replaced by
the squared error, which can be regarded a special case of computation of log-likelihood wherein each dimension of X
and ¯X is independent and follows a Gaussian distribution with standard deviation 1. Since the Kullback-Leibler (KL)
divergence between two one-dimensional Gaussian distributions takes a simple expression, equation 6 is reduced to
F =
TX
t=1
" LX
l=1
˜wl
Rl
zX
r=1
δ(l, r, t)
#
−
TX
t=1
∥Xt − ¯Xt∥2
2 (7)
where
δ(l, r, t) = log σp,l,r
t
σq,l,r
t
+ (µq,l,r
t − µp,l,r
t )2 + (σq,l,r
t )2
2(σp,l,r
t )2
− 1
2 (8)
µp,l,r
t represents rth element of µl
t of the prior, and the same notation is applied to µq,l,r
t , σp,l,r
t , and σq,l,r
t . Rl
z denotes
the dimension of zl
t. Given that the complexity term is summed over all the dimension of z, which is arbitrary to
the network design, and the accuracy term is to all the data dimension, which varies among data, the free energy is
normalized with respect to the dimension of z and the data dimension. Therefore, introducing such normalization, the
free energy of PV-RNN in the study is computed by
F =
TX
t=1
" LX
l
wl
Rlz
δ(l, r, t)
#
| {z }
complexity
− 1
RX
" TX
t=1
∥Xt − ¯Xt∥2
2
#
| {z }
accuracy
(9)
where RX is the data dimension, Rl
z is the number of z variables in each layer, and wl = Rl
z ˜wl.
By minimizing equation 9, the posterior inference is performed during network learning and during the perception
task. Figure 1 shows a schematic illustration of the posterior inference process of a two-layer PV-RNN model used in
the current simulation with an optimization window of three time steps. At every sensory step, an adaptive variable
5
A in the window is optimized through multiple epochs of stochastic gradient descent. In the network learning phase,
weights and bias parameters θ and ϕ of the generative and inference models, including an adaptive variable A for the
approximate posterior zq are jointly optimized. In the perception task phase, network parameters θ and ϕ are fixed, and
free energy is minimized at each time step within a dedicated inference window by optimizing only A parameterizing
the approximate posterior.
2.2.3 Adaptation of Meta-Prior
The meta-prior w is dynamically adapted based on the average prediction error ( ersum) over a fixed length time
window in the past. When the error decreases below a predefined threshold (ThrL), w transitions to a high value (wH),
prioritizing top-down generation, which leads to generating MW. This can be intuitively understood from analogy that
continuing easy or predictable tasks tends to initiate MW [5, 4]. Conversely, when the average prediction error exceeds
an upper threshold (ThrH), w transitions to a low value (wL), enhancing bottom-up inference. The implementation
strategy for autonomous meta-prior switching between FS and MW is described in Algorithm 1. Specifically, the
probabilistic shifting between the two modes is given by equations 10-11, where T empis the temperature, a tunable
parameter that can reflect how stochastic or deterministic the system is (see Section 3.2). It is highly speculated that this
dynamic adaptation should enable autonomous transitions between FS and MW, as will be validated in the simulation
experiments detailed in subsequent sections.
Algorithm 1Autonomous Meta-Prior Switching Between Focus State (FS) and Mind-Wandering (MW)
1: Initialize meta-prior w (either wL or wH)
2: if w == wL then
3: Compute transition probability from FS to MW:
P(F S→ MW ) = sigmoid
−(ersum − T hrL)
T emp

(10)
4: Generate random number r ∼ G(0, 1)
5: if r <P(F S→ MW ) then
6: Set meta-prior to w ← wH
7: end if
8: else ifw == wH then
9: Compute transition probability from MW to FS:
P(MW → F S) = sigmoid
ersum − T hrH
T emp

(11)
10: Generate random number r ∼ G(0, 1)
11: if r <P(MW → F S) then
12: Set meta-prior to w ← wL
13: end if
14: end if
3 Experiments and Results
3.1 Model Training
First, we trained a PV-RNN with 2-dimensional sensory sequence data. The training data comprised 80 sequences, each
containing 2160 time steps. For preparing those trajectories, we designed 2 different 2-dimensional cyclic patterns,
one with periodicity of 40 time steps and the other with periodicity of 27 time steps. Each trajectory was made of
probabilistic switching among these 2 cyclic patterns wherein after one cycle of a particular pattern the same pattern
repeats with a probability of 60% and the pattern transits to the other pattern with a probability of 40% equally. Noise
has been added to individual points at randomly spaced intervals. The intervals between noise points are determined
by drawing from a normal distribution (mean of 1, standard deviation of 10), providing a variable time step size. At
each noise interval, Gaussian noise (mean of 0, standard deviation of 0.001) is added to the current data point, slightly
perturbing its coordinates to simulate natural fluctuations without disrupting the cyclic structure. A part of the training
trajectory is shown in Figure 2.
6
Figure 2: Training trajectory over 400 time steps (top plot) and its representation in X − Y space (bottom plot).
T arget− X and T arget− Y correspond to the first and second dimensions of the training trajectory, respectively.
The network parameters used for training PV-RNN are listed in Table 1. #d, #z, τ, wtr indicates the number of d
neurons, number of z neurons, time constant, and meta-prior during the training phase, respectively.
Table 1: PV-RNN training parameters.
#d #z τ wtr
Layer 1 30 2 1 0 .001
Layer 2 15 1 5 0 .01
The PV-RNN was trained over130, 000 epochs minimizing free energy in Equation 9 using the Adam optimizer [26]
and back-propagation through time (BPTT) [27] with learning rate 0.001 to optimize all network parameters of θ and ϕ
of the generative and inference model, and the adaptive variable A corresponding to each training trajectory.
The trained network was evaluated on the basis of how well probabilistic transitions in the training data were reflected in
the PV-RNN generative process, the so-called prior generation of the PV-RNN, which is conducted without performing
the inference of the latent variables with sensory observation. In prior generation, the prior distributionzp
1 was initialized
with a unit Gaussian (Equation 4) and then latent states were recursively computed to generate network output sequences.
Figure 3 shows an example of the prior generation outputs over 1200 time steps. We can see that the patterns shift
from one to another, where the two patterns used for training appear randomly. In addition, using a categorizer to
discriminate between the two patterns, different prior generation outputs over 50,000 time steps have shown a probability
of 38%-43% of switching to a different pattern and a probability of 57%-62% of staying in the same pattern, which are
close to the training dataset.
7
Figure 3: Prior generation over 1200 time steps under trained model with meta-prior wtr from Table 1 (top plot),
selected activities of the d neurons in the bottom layer of the PV-RNN (middle plot), and a representation inX − Y
space (bottom plot). Output−X and Output−Y correspond to the first and second dimensions of the prior generation
output trajectory, respectively.
3.2 Testing of perception task
The trained PV-RNN was tested by performing the perception task. In the test, the inference process was performed
within the inference window, while one of the trained patterns was used as the target sensory sequence for the inference
of the latent variables. The length of the inference window was set to 400 time steps. The adaptation of meta-prior, w,
during inference with the monitoring of the average prediction error over 300 time steps 1 was carried out using the
parameters listed in Table 2.
Table 2: PV-RNN testing parameters.
wL wH Temp Thr L ThrH
Layer 1 0.001 100 0.01 0.1 0.5
Layer 2 0.01 1000
The mechanistic behavior when w adapted to low and high values are shown in Figures 4 and 5, respectively. The plots
show the output trajectory, the target sensory sequence, the average prediction error, and KL divergence at the PV-RNN
1For the implementation strategy described in Algorithm 1, the length of the inference window and time window for computing
the average reconstruction error can be considered design decisions and do not need to be the same length. This choice may depend
on how the high and low thresholds are defined, which impact the probability of transition from FS to MW (and vice-versa) and,
thus, the expected probabilistic behavior of the system. For instance, an average reconstruction error computed over a small time
window may not reach or may be too far from a desired threshold. In this scenario, the probability of staying in the current state (FS
or MW) would remain large over the entire simulation time.
8
bottom layer for each case. It can be seen in Figure 4 that when w adapted to wL, a pattern used for the target sensory
sequence is generated well during inference while the average prediction error remains low (below 0.03) over the entire
inference window. This indicates that adaptation of w to wL enabled the output to accurately reconstruct the target
sensory sequence. This period is analogous to a situation of FS.
On the other hand, Figure 5 demonstrates a period when w adapted to wH. In this period, the inference trajectory is
generated similarly to the prior generation shown in (Figure 3). In particular, we can observe in Figure 5 that after a few
cycles of one pattern, the inference trajectory generates the other pattern, returns to the previous pattern (which is out
of phase from the target sensory sequence due to the different periodicity between the two cyclic patterns), and then
switches again to the other pattern. As a result, the average reconstruction error increases once the inference trajectory
starts to deviate from the target sensory sequence. This observation is analogous to a situation of MW.
Figure 4: From top to bottom: inference output trajectory with meta-prior wL from Table 2, selected activities of the
d neurons in the bottom layer of the PV-RNN, average reconstructions error over the inference window at time step
542, and KL divergence at the PV-RNN bottom layer.Inference − X and Inference − Y correspond to the first and
second dimensions of the inference output trajectory, respectively.
9
Figure 5: From top to bottom: inference output trajectory with meta-prior wH from Table 2, selected activities of the
d neurons in the bottom layer of the PV-RNN, average reconstructions error over the inference window at time step
283, and KL divergence at the PV-RNN bottom layer.Inference − X and Inference − Y correspond to the first and
second dimensions of the inference output trajectory, respectively.
Selected d activities of the PV-RNN bottom layer during prior generation after training, as well as during inference with
adaptation of w to low and high values, are shown in Figures 3-5. In both cases of the inference, the correspondence
between d activity patterns and the output patterns is analogous to that observed during prior generation after training.
Specifically, the d activities follow a single pattern when w adapted to the low value, while the d activities alternate
between two patterns, closely reflecting the dynamics of the d activities seen in the prior generation when w adapted to
the high value.
Figure 6 shows the overall behavior of autonomous shifts between two distinct periods obtained in the experiments.
The plots show the average reconstruction error during inference and meta-prior values of the PV-RNN bottom layer
over time. It can be observed that when PV-RNN is underwH, the average prediction error increases as close to the
high threshold value, which makes the probability of switching from wH to wL larger according to equation 11. Then,
10
w is switched to the low value (wL). After this shift, the average prediction error continues to decline until it becomes
close to the low threshold value, which increases the probability of switching from wL to wH. w is then switched back
to the high value. The former case corresponds to the shift from MW to FS and the latter case corresponds to the shift
from FS to MW.
Finally, we investigated the effect of changing temperature values on the characteristics of the shifts between FS
and MW. For this purpose, we counted the number of transitions occurred from FS to MW during 1000 steps in
the perception test. The results are shown in Figure 7. It can be seen that the transition frequency from FS to MW
increases when the temperature increases. In particular, for larger temperature values, the transitions from FS to MW
become more frequent (i.e., the system becomes more random) since the probability of switching from FS to MW
becomes closer to 50% due to the argument inside the sigmoid function being closer to zero in equation 10. In contrast,
when the temperature is smaller, the transitions from FS to MW become less frequent (i.e., the system becomes more
deterministic), which primarily happen when the average reconstruction error reaches the low threshold. For the case
study in Figure 6, 0.01 was chosen to be the temperature with a mean of 1.23 transitions from FS to MW per 1000 time
steps.
Figure 6: Reconstruction error over inference window computed at each time step (top plot) and adaptive meta-prior
value (w) of the PV-RNN bottom layer over time (bottom plot).
11
Figure 7: Transition frequency from FS to MW per 1000 time steps under different temperature values. The mean and
standard deviation are displayed for three intermediate cases when temperature is 0.01, 0.10, and 0.50.
4 Discussion
This study explored the neural mechanisms underlying autonomous shifts between the focused state (FS) and mind-
wandering (MW) through simulation experiments using a newly proposed model based on the free energy principle.
The proposed model, an extension of PV-RNN, introduces an adaptation mechanism for a meta-level parameter, the
meta-prior w, which is modulated based on the average reconstruction error over a fixed-size past window. Specifically,
w probabilistically switches to a high value when the average reconstruction error decreases close to a minimal threshold
and to a low value when the average reconstruction error increases near a maximal threshold.
In the simulation experiments, the PV-RNN was first trained to generate probabilistic transitions between two distinct
cyclic patterns. In the perception task phase, latent variables within the inference window were inferred to minimize the
reconstruction error for given target sensory sequence while adapting w. One of the trained cyclic patterns was used as
the target.
When w shifted to a low value, stronger bottom-up sensory perception dominated, regenerating the observed sensory
sequence in the outputs with minimal reconstruction error while allowing larger Kullback-Leibler divergence between
the prior and the approximate posterior. This leads to a focused state. Conversely, when w shifted to a high value, the
approximated posterior is attracted toward the prior by stronger mean of minimizing the Kullback-Leibler divergence
between the prior and the approximated posterior. This allowed stronger top-down processing while less attending to
sensation, generating relatively large reconstruction error in the inference window. This results in a state resembling
mind-wandering.
One limitation of the current study is that the proposed model does not account for the phenomenon of becoming
consciously aware of MW, which enables redirection of attention back to FS. [11] hypothesize that inferring a "true
meta-state" by asking, "How aware am I of where my attention is?" could trigger self-awareness of MW. While
the dynamically changing meta-prior in the current model modulates the balance between top-down and bottom-up
information flow, leading to shifts between FS and MW, it may correspond to the meta-state proposed in [11]. However,
the current model lacks a mechanism for explicitly inferring such a meta-state, making it unable to account for self-
12
awareness of it. Future studies should address this limitation by extending the model to include an inference mechanism
for a meta-state.
How do the current results relate to either the two-stage model [ 9] or the multiple sub-event model [ 10] described
previously? The two-stage model suggests that the probability of remaining in FS decreases over time during an FS-MW
episode, which concludes with conscious awareness of MW. In contrast, the multiple sub-event model posits a lesser
decrease in this probability, speculating that multiple unconscious shifts between FS and MW occur before MW is
consciously noticed. Since the current model does not account for self-awareness of MW, as discussed earlier, it is
challenging to directly align its results with either of these models.
Finally, numerous studies have indicated that MW during the resting state is intricately linked to the functional
organization and dynamics of brain networks, particularly the default network (DN), central executive network (CEN),
and salience network (SN) [28, 29, 30].The current study does not model interactions between such distinct networks.
Extending the model to incorporate dynamic interactions among these networks would provide a tighter connection to
established neuroscientific findings on resting-state phenomena and offer deeper insights into mind-wandering.
References
[1] Jonathan Smallwood and Jonathan W Schooler. The science of mind wandering: Empirically navigating the
stream of consciousness. Annual review of psychology, 66(1):487–518, 2015.
[2] Kalina Christoff, Zachary C Irving, Kieran CR Fox, R Nathan Spreng, and Jessica R Andrews-Hanna. Mind-
wandering as spontaneous thought: a dynamic framework. Nature reviews neuroscience, 17(11):718–731, 2016.
[3] Paul Seli, Evan F Risko, Daniel Smilek, and Daniel L Schacter. Mind-wandering with and without intention.
Trends in cognitive sciences, 20(8):605–617, 2016.
[4] Paul Seli, Mahiko Konishi, Evan F Risko, and Daniel Smilek. The role of task difficulty in theoretical accounts of
mind wandering. Consciousness and Cognition, 65:255–262, 2018.
[5] Catarina I Peral-Fuster, Rhiannon S Herold, Oliver J Alder, Omar Elkelani, Sara I Ribeiro-Ali, Eleanor M Deane,
Alexander PL Martindale, Ziqiao Qi, Carina EI Westling, and Harry J Witchel. Intentional mind wandering is
objectively linked to low effort and tasks with high predictability. In Proceedings of the European Conference on
Cognitive Ergonomics 2023, pages 1–8, 2023.
[6] Jonathan Smallwood and Jessica Andrews-Hanna. Not all minds that wander are lost: the importance of a balanced
perspective on the mind-wandering state. Frontiers in psychology, 4:441, 2013.
[7] Rodrigo A Henríquez, Ana B Chica, Pablo Billeke, and Paolo Bartolomeo. Fluctuating minds: spontaneous
psychophysical variability during mind-wandering. PLoS One, 11(2):e0147174, 2016.
[8] David R Vago and Fadel Zeidan. The brain on silent: mind wandering, mindful awareness, and states of mental
tranquility. Annals of the New York Academy of Sciences, 1373(1):96–113, 2016.
[9] Matthew J V oss, Meera Zukosky, and Ranxiao Frances Wang. A new approach to differentiate states of mind
wandering: Effects of working memory capacity. Cognition, 179:202–212, 2018.
[10] Meera Zukosky and Ranxiao Frances Wang. Spontaneous state alternations in the time course of mind wandering.
Cognition, 212:104689, 2021.
[11] Lars Sandved-Smith, Casper Hesp, Jérémie Mattout, Karl Friston, Antoine Lutz, and Maxwell JD Ramstead.
Towards a computational phenomenology of mental action: modelling meta-awareness and attentional control
with deep parametric active inference. Neuroscience of consciousness, 2021(1):niab018, 2021.
[12] Hayato Idei, Keisuke Suzuki, and Yuichi Yamashita. Awareness of being: A computational neurophenomenological
model of mindfulness, mind-wandering, and meta-attentional control. 2024.
[13] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences, 360(1456):815–836, 2005.
[14] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some
extra-classical receptive-field effects. Nature neuroscience, 2(1):79, 1999.
[15] Andy Clark. Surfing uncertainty: Prediction, action, and the embodied mind. Oxford University Press, 2015.
[16] Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a free-energy formulation.
Biological cybernetics, 102(3):227–260, 2010.
[17] Karl Friston, Jérémie Mattout, and James Kilner. Action understanding and active inference.Biological cybernetics,
104:137–160, 2011.
13
[18] Ahmadreza Ahmadi and Jun Tani. A novel predictive-coding-inspired variational rnn model for online prediction
and recognition. Neural computation, 31(11):2025–2074, 2019.
[19] Wataru Ohata and Jun Tani. Investigation of the sense of agency in social cognition, based on frameworks of
predictive coding and active inference: A simulation study on multimodal imitative interaction. Frontiers in
Neurorobotics, 14:61, 2020.
[20] Hendry F. Chame, Ahmadreza Ahmadi, and Jun Tani. A hybrid human-neurorobotics approach to primary
intersubjectivity via active inference. Frontiers in Psychology, 11:3207, 2020.
[21] Nadine Wirkuttis, Wataru Ohata, and Jun Tani. Turn-taking mechanisms in imitative interaction: Robotic social
interaction based on the free energy principle. Entropy, 25(2):263, 2023.
[22] Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127–38,
2010.
[23] Yuichi Yamashita and Jun Tani. Emergence of functional hierarchy in a multiple timescale neural network model:
a humanoid robot experiment. PLoS computational biology, 4(11), 2008.
[24] Guido Schillaci, Alejandra Ciria, and Bruno Lara. Tracking emotions: Intrinsic motivation grounded on multi-level
prediction error dynamics. 10th Joint IEEE ICDL-EPIROB, pages 1–8, 2020.
[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2014.
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[27] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error
propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
[28] Malia F Mason, Michael I Norton, John D Van Horn, Daniel M Wegner, Scott T Grafton, and C Neil Macrae.
Wandering minds: the default network and stimulus-independent thought. science, 315(5810):393–395, 2007.
[29] Christine A Godwin, Michael A Hunter, Matthew A Bezdek, Gregory Lieberman, Seth Elkin-Frankston, Victoria L
Romero, Katie Witkiewitz, Vincent P Clark, and Eric H Schumacher. Functional connectivity within and between
intrinsic brain networks correlates with trait mind wandering. Neuropsychologia, 103:140–153, 2017.
[30] Ekaterina Denkova, Jason S Nomi, Lucina Q Uddin, and Amishi P Jha. Dynamic brain network configurations
during rest and an attention task with frequent occurrence of mind wandering.Human brain mapping, 40(15):4564–
4576, 2019.
14