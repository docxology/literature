bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
Active Inference as a Framework for Brain-Computer
Interfaces
Syed Hussain Ather*
*Institute of Medical Science, University of Toronto, Toronto, ON M5S
Corresponding Author:
Syed Hussain Ather,
e-mail: s.ather@mail.utoronto.ca
tel: +13177011491
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
Active Inference as a Framework for Brain-Computer
Interfaces
Syed Hussain Ather
Institute of Medical Sciences, University of Toronto
Krembil Centre for Neuroinformatics, Centre for Addiction and Mental
Health, Toronto
hussain.ather@camh.ca
Abstract
As Karl Friston explained during the International Symposium on Ar-
tificial Intelligence and Brain Science 2020, active inference provides a way
of using abstract rule-learning and approximate Bayesian inference to show
how minimizing (expected) free energy leads to active sampling of novel
contingencies. Friston elaborated how there were ways of making an optimal
decision using active inference that can o↵er perspectives to advances in arti-
ficialintelligence. Thesemethodsofoptimizationwithinthecontextofactive
inference can also be used as a framework for improving brain-computer in-
terfaces (BCI). This way, BCIs can give rise to artificial curiosity in the way
Friston had described during his session. Using Friston’s free energy princi-
ple, we can optimize the criterion a BCI uses to infer the intentions of the
user from EEG observations. Under Friston’s criteria for making an optimal
decision, BCIs can expand their framework of optimal decision-making using
active inference.
Key words: Artificial intelligence, Neuroscience, Free energy principle,
Active inference, Computational Neuroscience, Mathematics
1. Introduction
1
1.1. Free energy principle
2
In the search for grand unified theories of the brain, the free energy prin-
3
ciple states that a self-organizing principle at equilibrium with its environ-
4
Preprint submitted to Journal Name January 5, 2021
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
ment must minimize the amount of free energy that it has. Its applica-
5
tions have spanned far and wide in explaining brain structure and function.
6
This self-organizing behavior arises from the defining characteristic of bio-
7
logical systems to resist disorder in dynamic environments[1] stemming from
8
Helmholtz’sideasofperception[2]. Amodelofperceptualinfluenceandlearn-
9
ing built upon these ideas can explain how principles can resolve problems of
10
the inference of the causes underlying sensory input and learning the causal
11
structure that generates them. From there, one can study how inference and
12
learning follow. Friston has used this in the context of Empirical Bayes and
13
hierarchical models of sensory input to show how the free energy principle
14
can understand a range of cortical organization and responses.
15
The free energy principle provides a way for adaptive systems to unify
16
action, perception, and learning[3]. The theory and background of the prin-
17
ciple involves a system using a Markov blanket to minimize the di↵erence
18
between a model of the world and its sense and the perception associated
19
with it. Through continuously correcting the world model of the system, the
20
system changes the world into an expected state while minimizing the free
21
energy of the system. Using the Bayesian idea of the brain as an “inference
22
engine,” the system can actively change the world (active inference) into the
23
expected state and minimize the free energy of the system. This holds true
24
for a wide variety of adaptive systems from animals to brains themselves
25
in understanding mental disorders and artificial intelligence, and other ap-
26
plications of the free energy principle span areas of exploration and novelty
27
seeking[4].
28
Friston outlined the motivation behind using the free energy principle as
29
a unified brain theory using the system’s tendency to resist disorder. When
30
a system resists its tendency to move towards disorder, the physiological
31
and sensory states of a system move towards configurations of low entropy.
32
Given that the number of these states is limited, the system is very likely to
33
be in these states of low entropy. By using a formulation of entropy as the
34
averageamountofself-informationor“surprise”(thenegativelog-probability
35
of a specific outcome), Friston explained how biological agents minimize the
36
long-term average of surprise (or maximize sensory evidence for an agent’s
37
existence) to keep sensory entropy low. By sampling the environment to
38
change configuration and minimize free energy this way, the system changes
39
its expectations[5]. This forms the basis of action and perception, and the
40
system’s state and structure encode an implicit and probabilistic model of
41
the environment. The nervous system in particular maintains order through
42
2
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
these methods, and the specific structural and functional organization is
43
maintained by the environment’s causal structure.
44
One can evaluate free energy as a function of two things to which the
45
agent has access: the sensory states and a recognition density encoded by
46
its internal states (such as neuronal activity and connection strengths)[6]. In
47
terms of the environment’s causal structure, the recognition density would
48
be a probabilistic representation of what caused a particular sensation. The
49
causes underlying sensory input, or the probabilistic representation of what
50
causes sensations, are used as the recognition density. They can vary from
51
an object in one’s field of vision or blood pressure changing the physiological
52
state of organs.
53
1.2. Active Inference
54
A corollary of the free energy principle, active inference, arises from the
55
way natural agents act in the context of these observations. Active inference
56
claims natural agents act to fulfill prior beliefs about preferred observations.
57
By adjusting sensory data (without changing recognition density) to mini-
58
mize free energy, an agent chooses the sensory inputs from a sample based
59
on prior expectations to increase the accuracy, the surprise about sensations
60
expected under a particular recognition density, of an agent’s predictions. In
61
the context of Bayesian inference, one may define the complexity (“Bayesian
62
surprise”) as the di↵erence between the prior density, which encodes beliefs
63
about the state of the world before sensory data are assimilated, and poste-
64
rior beliefs, encoded by the recognition density. In essence, the agent avoids
65
surprising states by making active inferences.
66
AsFristonexplainedduringhissession[7],activeinferenceisself-evidencing
67
in that action and perception can be cast as maximising Bayesian model ev-
68
idence for the generative models of the world[8]. Using a generative model
69
of the underlying causal structure of a system, you can explain how evidence
70
accumulates or how a specific action was chosen. Examples of active infer-
71
ence for Markov decision processes include using Bayes optimal precision to
72
predict activity in dopaminergic areas [9] and using a gradient descent on
73
variational free energy to simulate neuronal processing [10]. This evidence
74
can be decomposed through its accuracy and complexity.
75
One may even describe active inference as a method of explaining action
76
using the idea that the brain has “stubborn predictions” that are resistant
77
to change, such as adaptive body temperature necessary for survival, that
78
causethesystemtobehaveinawaytocausethepredictionstocometrue[11].
79
3
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
Figuring out the etiology of stubbornness would provide insight into ways of
80
how to change these predictions helpful for understanding the relationship
81
between drugs and psychotherapy that have synergistic e↵ects when used
82
together. Other applications of active inference extend to visual foraging[12]
83
and BCIs[13].
84
1.3. Active Inference Python Implementation
85
A good example of a Python implementation of Active Inference can be
86
found in the “Active Inference for Markov Decision Processes” repository
87
[14].
88
Using the grid world environment that has 3 x 3 states, an agent can
89
choose one of five actions for each time step. The agent can move in any
90
of the four directions (up, down, left, right) or remain where it is. After
91
performing inferences about its current location, the agent must figure out
92
which location it prefers to move to and move there. Using a reward value,
93
we can set the reward value to reward the user when choosing the direction
94
leading to a preferred location.
95
REWARD_LOCATION = 7
96
env = GridWorldEnv()
97
env.set_reward_state(REWARD_LOCATION)
98
Init,the“Inferenceandactionexample”illustratestheprocessofcreating
99
a generative model consisting of a likelihood distribution A and an empirical
100
prior distribution B for the environment.
101
likelihood_matrix = env.get_likelihood_matrix()
102
A = Categorical(values=likelihood_matrix)
103
A.remove_zeros()
104
plot_likelihood(A)
105
106
transition_matrix = env.get_transition_matrix()
107
B = Categorical(values=transition_matrix)
108
B.remove_zeros()
109
plot_empirical_prior(B)
110
With the beliefs about the current location:
111
Qs = Categorical(dims=[env.n_states])
112
4
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The beliefs are then plotted from the agent’s preferences:
113
C[REWARD_LOCATION] = 1.
114
plot_beliefs(C, title="Prior preference (C)")
115
By evaluating the policy, one can calculate negative expected free energy
116
for a policy:
117
def evaluate_policy(policy, Qs, A, B, C):
118
# store expected free energy
119
G = 0
120
121
# create copy of our state
122
Qs = Qs.copy()
123
124
# loop over policy
125
for t in range(len(policy)):
126
127
# get action
128
u = int(policy[t])
129
130
# work out expected state
131
Qs = B[u].dot(Qs)
132
133
# work out expected observations
134
Qo = A.dot(Qs)
135
136
# get entropy
137
H = A.entropy()
138
139
# get predicted divergence and uncertainty and novelty
140
divergence = F.kl_divergence(Qo, C)
141
uncertainty = H.dot(Qs)[0, 0]
142
143
G += (divergence + uncertainty)
144
return -G
145
And infer an action:
146
5
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
def infer_action(Qs, A, B, C, n_actions, policy_len):
147
148
# this function generates all possible combinations of policies
149
policies = F.generate_policies(n_actions, policy_len)
150
n_policies = len(policies)
151
152
# negative expected free energy
153
neg_G = np.zeros([n_policies, 1])
154
155
for i, policy in enumerate(policies):
156
neg_G[i] = evaluate_policy(policy, Qs, A, B, C)
157
158
# get distribution over policies
159
Q_pi = F.softmax(neg_G)
160
161
# probabilites of control states
162
Qu = Categorical(dims=n_actions)
163
164
# sum probabilites of controls
165
for i, policy in enumerate(policies):
166
# control state specified by policy
167
u = int(policy[0])
168
# add probability of policy
169
Qu[u] += Q_pi[i]
170
171
# normalize
172
Qu.normalize()
173
174
# sample control
175
u = Qu.sample()
176
177
return u
178
Putting everything together, we can write a loop to infer beliefs about
179
hidden states:
180
# number of time steps
181
T = 10
182
6
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
183
# number of actions
184
n_actions = env.n_actions
185
186
# length of policies we consider
187
policy_len = 4
188
189
# set initial state
190
env.set_initial_state(0)
191
192
# reset environment
193
o = env.reset()
194
195
# infer initial state
196
Qs = F.softmax(A[o, :].log())
197
198
# loop over time
199
for t in range(T):
200
201
# random action
202
a = infer_action(Qs, A, B, C, n_actions, policy_len)
203
204
# perform action
205
o, r = env.step(a)
206
207
# infer new hidden state
208
Qs = F.softmax(A[o,:].log() + B[a].dot(Qs).log())
209
210
# print information
211
print("Time step {}".format(t))
212
env.render()
213
plot_beliefs(Qs, "Beliefs (Qs) at time {}".format(t))
214
A straightforward method such as this one could be applied to BCIs to
215
let them use active inference in optimizing their decision-making.
216
1.4. Artificial Curiosity
217
For artificially intelligent agents to look at their environments and learn
218
from them in a way that a human being would, they develop a kind of artifi-
219
7
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
cial curiosity. They can examine and understand which behavior is rewarded
220
based on the outcomes of their actions. Schmidhuber put forward a simple
221
formal theory of fun and intrinsic motivation based on maximizing intrin-
222
sic reward for active creation or discovery of novel, surprising patterns[15].
223
In this sense, artificially curious agents learn to become bored or tired of
224
predictable patterns or behaviors.
225
Friston’s method of using the free energy principle and predictive coding,
226
the method by which the brain generates and updates a mental model of the
227
environment given sensory input[16][17], doesn’t achieve this, Schmindhuber
228
wrote. By visiting highly predictable states, Friston argued that perception
229
seeks to suppress prediction error by changing predictions and action changes
230
the signals themselves. Instead of learning, Friston’s approach only teaches
231
agents to stabilize and make things predictable. Other methods of using
232
the free energy principle in active inference have included variational Bayes,
233
formal account explaining the relationship between posterior expectations of
234
hidden states, control states and precision[18]. As Friston explained during
235
his session, artificial curiosity falls under a di↵erent form of optimality that
236
uses Hamilton’s principle of least action.
237
2. Body
238
2.1. Brain-Computer Interfaces
239
The BCI is a system that lets a human brain interact directly with an
240
external device for restoring movement [19] or communication[20], for as-
241
sisting and optimizing tasks[21], and for rehabilitating by enabling the self-
242
regulation of brain activity for therapeutic purposes[22]. The BCI works by
243
collecting information through electroencephalography (EEG) to update an
244
internal model of the user, optimizing interactions via inference about the
245
user, and optimizing interactions by acting upon the user. These behaviors
246
parallel that of an Active Inference system. With Active Inference, the BCI
247
can choose desired outcomes based on the noisy, variable EEG input signal
248
through optimization. By minimizing entropy to anticipate future outcomes,
249
one can use an Active Inference framework to model sequential learning and
250
decision-making in the brain guided by an objective function to minimize
251
free energy when adjusting to signal variability during the signal processing
252
pipeline and modeling and acting in accordance with the causal structure the
253
model determines. The model was shown in the context of a P300-speller,
254
a BCI for communication that uses event-related potentials (ERPs) such as
255
8
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
the P300 ERP, an EEG positive deflection around 300 ms after a rare and
256
related event.
257
By inferring the user’s intentions of which letter to spell (the observation)
258
given a 6 x 6 grid of characters (the action), the machine studies how a user
259
focuses their visual attention and how their brain reacts with a P300. This
260
lets the machine detect a particular ERP in response. The machine flashes
261
the items in repetition until it becomes confident about figuring out which
262
stimulus corresponds to a targeted item[23].
263
The P300-based BCI has shown improvement with statistical methods
264
have included Bayesian sensor fusions for event detection[24], a classifier with
265
both Fisher’s and Bayesian linear discriminant analysis[25], and a support
266
vector machine classifier[26].
267
2.2. Brain-Computer Interfaces
268
Figure 1: Active Inference for the P300 speller. With free energy as a function of hidden
states,thestatescanbeeitherlongtermorshorttermreactionstostimuli. Themachine’s
actionsarebasedonthegenerativemodeloftheuser. Thefreeenergyis, then, afunction
of hidden states and actions F (u ,s ).
m t t
With this method of Active Inference, one can use a generative Bayesian
269
modelofpriorsthatencodepredictionsoverfutureoutcomeswithamodelm,
270
a joint probability distribution over hidden states S, control states (actions)
271
U, observations (sensations) O and model parameters S, U, and O. The
272
9
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
hidden states are internal representations of the BCI about the etiology of
273
the observations such as the letter a user thinks of that causes the P300
274
ERP. The Markov model uses the precision parameter   with preferences
275
over future outcomes C.
276
The model works by figuring out which hidden states are most likely to
277
occur by optimizing the expectations with respect to the free energy of the
278
observations. This way, the fully adaptive P300 speller can learn and act
279
optimally in real time by automatically and optimally updating an internal
280
model of the environment. Then, given the speller’s optimized flashing and
281
the corresponding error potentials, the BCI can choose to spell the next
282
probable letter or continue flashing to increase evidence for the target letter.
283
The finite set of hidden states would be: S = s(1),s(2),...,s(n) with S =
284
| |
n ; in whichs maps each trial t onto one element from finite set S such that
285
s(t) = s S, t = 1,...,T.
286 t
2 8
If n represents the number of possible states (cardinality of S at every
287
trial t), T is the final trial, and t is the current one. Only one state out of n
288
possible ones can take place at a time or trial t.
289
By sampling actions from control beliefs and inferring them from observa-
290
tions as well as making the assumption that agents know the realized actions,
291
one can define U, a finite set of control states or actions such as which letter
292
to flash next as U = u(1),u(2),...,u(r), with U = r such that u maps t onto
293
| |
an element from the finite set U. The control states (actions) are then given
294
by u(t) = u U and t = 1,...,T in which r represents the number of
295 t
2 8
possible states (cardinality of U at trial t). This lets the agent choose an
296
action out of r possible ones.
297
Similarly, the observations are, then O = o(1),o(2),...,o(z), with O = z
298
| |
such that o maps the trial t onto an element from O with o(t) = o U, t =
299 t
2 8
1,...,T for z, the number of possible observations (cardinality of O at trial
300
t).
301
The Bayesian model is then:
302
P(o,u,s,  m ) =P(o s,m)P(s,u  ,m)P(  m ) 1) (
| | | |
for o = o ,...,o O,s = s ,...s S,u = u ,...u . U broken down into
303 1 T 1 T 1 T
the likelihood materixe2 Pe(o s,m), proebea 2 bilisticeteransition 8 matrix P(s,u  ,m),
304
| |
and precision parameter P(  m).
305 e e e
|
The likelihood matrix:
306 ee e e
10
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
T
P(o,u,s,  m ) = P(o s , ,m) (2)
i i
| |
i=1
Y
307 with e e e e e
P(o = k s = h) = A ,h (3)
i i k
|
308 where A R zxn so that, for each h = 1,...n states with a probability to
2
get the k = 1,...z observation. With the likelihood, the Bayesian model can
309
predict the probabilities of new observations based on prior experience. The
310
probabilistic transition matrix:
311
t
P(s,u  ,m) = P(u  ,m) P(s +1 s ,u ,m) (4)
t i i i
| | |
i=1
Y
312 with e e e
P(s = w s = q,u ) = B(u ) (5)
t+1 t t t w,q
|
313 for w,q = 1,...n and B(u t ) R nxn for number of hidden states n . The
2
putative action, a specific control state that minimizes expected free energy,
314
u under policy ⇡ 1,...K. A policy indexes a specific sequence of control
315 t
2
states (u ⇡) = (u ,...,u ⇡) :
316 t ⌧
| |
ln(P(u  ,m)) =   Q(⇡) =   (Q(u ⇡)+... +Q(u ⇡)) (6)
e t+1 ⌧
| · · | |
e
Q(u ⇡) = E [ln(P(o m))]+E [D [Q(s o ,⇡) Q(s ⇡)]] (7)
⌧
|
Q(o⌧
|
⇡) ⌧
|
Q(o⌧
|
⇡) KL ⌧
|
⌧
|
⌧
|
withextrinsicvalueE [ln(P(o m))]andepistemicvalueE [D
317 Q(o⌧
|
⇡) ⌧
|
Q(o⌧)
|
⇡ KL
[Q(s o ,⇡) Q(s ⇡)]], probability distribution Q . The precision parameter
318 ⌧ ⌧ ⌧
| | |
  is used to minimize expected free energy, D as the Kullback-Leibler
319 KL
(KL) divergence (relative entropy). E is the expectation of a future
320 Q(o⌧ ⇡)
|
outcome, o , given policy ⇡. The extrinsic value is the maximized preferred
321 tau
finaloutcome, andtheepistemicvalueistheinformationmaximizedachieved
322
by minimizing relative entropy. This way, we maximize information to get
323
to the preferred future outcome.
324
Using C , the prior distribution over the outcomes P(o m), we can de-
325 ⌧ ⌧
|
termine the preference for future outcomes.
326
11
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
The extrinsic value contains P(o m) which is the prior distribution over
327 ⌧
|
future outcomes, referred to as C . So, let C be the preference of future
328 ⌧ ⌧
outcomes o O. As part of extrinsic value, it influences the choice of action
329 ⌧
2
to reach such desired outcomes. If we consider all available observations from
330
set O as future outcomes then o = o(z):
331 ⌧
C =  ([C(o(1)),C(o(2)),...,C(o(z))])T (8)
⌧
For   softmax (normalized exponential function) of final outcomes that
332
can transform observations to prior probabilities of future outcomes.
333
The precision parameter is, then:
334
P(  m ) = (↵, ) (9)
|
where  (↵) is a gamma distribution of scale parameter a and rate param-
335
eter b. If a random variable X follows a gamma distribution then:
336
 ↵x↵
 
1e   x
f(x;↵, ) = (10)
 (↵)
for x > 0 and ↵,  > 0.
337
where  (↵) is the gamma function, as defined:
338
 (↵) = (↵ 1)! (11)
 
The active inference framework can then be implemented using optimal
339
stopping and flashing while classifying error potential for future outcomes.
340
This way, the machine can act the way the user wants to.
341
3. Conclusion
342
Active inference provides a way for BCIs to improve their performance
343
in predicting user actions based on how the user interacts with a genera-
344
tive model of the user and their interactions. The general nature of the
345
free energy principle provides a grand, unifying way of understanding how
346
self-organization emerges. The corollary, active inference, then can be used
347
for calculating the epistemic reward, extrinsic value, and other parameters
348
in minimizing free energy and choosing the optimal decision for how the
349
user wants to act. One can describe evidence of specific behavior for active
350
12
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
inference in terms of simplicity and accuracy. Paraphrasing Einstein, every-
351
thing should be made as simple as possible, but not simpler. Minimizing free
352
energy provides a way of achieving this simplicity.
353
References
354
[1] K. Friston, Life as we know it, Journal of the Royal Society Interface
355
(2013).
356
[2] K. Friston, J. Kilner, L. Harrison, A free energy principle for the brain,
357
Journal of Physiology-Paris (2006).
358
[3] K. Friston, The free-energy principle: a unified brain theory?, Nature
359
Reviews Neuroscience 10 (2010) 127–138.
360
[4] P. Schwartenbeck, T. FitzGerald, R. J. Dolan, K. Friston, Exploration,
361
novelty, surprise, and free energy minimization, Frontiers in Psychology
362
4 (2013).
363
[5] K. Friston, J. Kilner, L. Harrison, Free-energy and the brain, Synthese
364
(2007).
365
[6] K. Friston, Embodied inference: or “i think therefore i am, if i am what
366
i think” (2010).
367
[7] F. K., M. Lin, C. Frith, G. Pezzulo, A. Hobson, J., S. Ondobaka, Ac-
368
tive inference and artificial curiosity, Neural Computation 29 (2017)
369
3434–3445.
370
[8] T. Parr, K. Friston, Generalised free energy and active inference, Bio-
371
logical Cybernetics 113 (2019) 495–513.
372
[9] P. Schwartenbeck, T. H. B. FitzGerald, C. Mathys, R. Dolan, K. Fris-
373
ton, The dopaminergic midbrain encodes the expected certainty about
374
desired outcomes, Cerebral Cortex 10 (2015) 3434–3445.
375
[10] K. Friston, M. Lin, C. Frith, G. Pezzulo, A. Hobson, J., S. Ondobaka,
376
Active inference, curiosity and insight, Neural Computation 10 (2017)
377
2633–2683.
378
[11] D. Yon, F. P. de Lange, C. Press, The predictive brain as a stubborn
379
scientist, Trends in Cognitive Sciences 23 (2019) 6–8.
380
13
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
[12] M. B. Mirza, R. A. Adams, C. Mathys, K. Friston, Scene construc-
381
tion, visual foraging, and active inference, Frontiers in Computational
382
Neuroscience 10 (2016) 1–16.
383
[13] J. Mladenovic, J. Frey, M. Jo ly, E. Maby, F. Lotte, J. Mattout, Active
384
inferenceasaunifying, genericandadaptiveframeworkforap300-based
385
bci, Journal of Neural Engineering 17 (2020).
386
[14] A. Tschantz, C. Heins, B. Klein, Active inference for markov decision
387
processes, https://github.com/jkbren/infer-actively,
388
2019.
389
[15] J. Schmidhuber, Formal theory of creativity, fun,and intrinsic motiva-
390
tion (1990-2010), IEEE Transactions on Autonomous Mental Develop-
391
ment 2 (2010).
392
[16] K. Friston, J. Daunizeau, J. Kilner, S. J. Kiebel, Action and behavior:
393
a free-energy formulation, Biological Cybernetics 102 (2010) 227–260.
394
[17] R. P. N. Rao, D. H. Ballard, Predictive coding in the visual cortex:
395
a functional interpretation of some extra-classical receptive-field e↵ects,
396
Nature Neuroscience 2 (1999) 79–87.
397
[18] K. Friston, P. Schwartenbeck, T. FitzGerald, M. Moutoussis,
398
T.Behrens, R.J.Dolan, Theanatomyofchoice: dopamineanddecision-
399
making, Philosophical Transactions of the Royal Society B: Biological
400
Sciences 369 (2014) 24–29.
401
[19] J. del R. Milan, J. M. Carmena, Invasive or noninvasive: Under-
402
standing brain-machine interface technology [conversations in bme],
403
IEEE Engineering in Medicine and Biology Magazine 29 (2010) 16–22.
404
doi:10.1109/MEMB.2009.935475.
405
[20] L. A. Farwell, E. Donchin, Talking o↵ the top of your head: toward
406
a mental prosthesis utilizing event-related brain potentials, Electroen-
407
cephalography and Clinical Neurophysiology (1988).
408
[21] T.O.Zander, C.Kothe, Towardspassivebrain–computerinterfaces: ap-
409
plying brain–computer interface technology to human–machine systems
410
in general, Journal of Neural Engineering (2011).
411
14
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
[22] J.-M. Batail, S. Bioulac, F. Cabestaing, C. Daudet, D. Drapier,
412
M. Fouillen, T. Fovet, A. Hakoun, R. Jardri, C. Jeunet, F. Lotte,
413
E. Many, J. Mattout, T. Medani, J.-A. Micoulaud-Franchi, J. Mlade-
414
novic, L. Perronet, L. Pillette, T. Ros., F. Vialatte, N. group, Eeg
415
neurofeedback research: A fertile ground for psychiatry?, L’Enc´ephale
416
45 (2019) 245–255.
417
[23] E. W. Sellers, D. J. Krusienski, D. J. McFarland, V. T. M., J. R. Wol-
418
paw, A p300 event-related potential brain-computer interface (bci): the
419
e↵ects of matrix size and inter stimulus interval on performance, Bio-
420
logical Psychology 73 (2006) 242–252.
421
[24] G. Pires, M. Castelo-Branco, U. Nunes, Visual p300-based bci to steer
422
a wheelchair: A bayesian approach, in: 2008 30th Annual International
423
Conference of the IEEE Engineering in Medicine and Biology Society,
424
2008, pp. 658–661. doi:10.1109/IEMBS.2008.4649238.
425
[25] R. C. Panicker, S. Puthusserypady, Y. Sun, Adaptation in p300
426
brain–computer interfaces: A two-classifier cotraining approach, IEEE
427
Transactions on Biomedical Engineering 57 (2010) 2927–2935.
428
[26] M. Thulasidas, C. Guan, J. Wu, Robust classification of eeg signal for
429
brain-computer interface, IEEE Transactions on Neural Systems and
430
Rehabilitation Engineering 14 (2006) 24–29.
431
15
bioRxiv preprint doi: https://doi.org/10.1101/2021.02.02.429272; this version posted February 3, 2021. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
Declaration of interests
☒ The authors declare that they have no known competing financial interests or personal relationships
that could have appeared to influence the work reported in this paper.
☐The authors declare the following financial interests/personal relationships which may be considered
as potential competing interests: