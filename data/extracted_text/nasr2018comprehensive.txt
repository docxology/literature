arXiv:1812.00910v2  [stat.ML]  6 Jun 2020
Comprehensive Privacy Analysis of Deep Learning:
Passive and Active White-box Inference Attacks
against Centralized and Federated Learning
Milad Nasr
University of Massachusetts Amherst
milad@cs.umass.edu
Reza Shokri
National University of Singapore
reza@comp.nus.edu.sg
Amir Houmansadr
University of Massachusetts Amherst
amir@cs.umass.edu
Abstract—Deep neural networks are susceptible to various
inference attacks as they remember information about their
training data. W e design white-box inference attacks to per form
a comprehensive privacy analysis of deep learning models. W e
measure the privacy leakage through parameters of fully tra ined
models as well as the parameter updates of models during
training. W e design inference algorithms for both centrali zed and
federated learning, with respect to passive and active infe rence
attackers, and assuming different adversary prior knowled ge.
W e evaluate our novel white-box membership inference attacks
against deep learning algorithms to trace their training da ta
records. W e show that a straightforward extension of the kno wn
black-box attacks to the white-box setting (through analyz ing the
outputs of activation functions) is ineffective. W e theref ore design
new algorithms tailored to the white-box setting by exploit ing
the privacy vulnerabilities of the stochastic gradient des cent
algorithm, which is the algorithm used to train deep neural
networks. W e investigate the reasons why deep learning mode ls
may leak information about their training data. W e then show
that even well-generalized models are signiﬁcantly suscep tible
to white-box membership inference attacks, by analyzing st ate-
of-the-art pre-trained and publicly available models for t he
CIF AR dataset. W e also show how adversarial participants,
in the federated learning setting, can successfully run act ive
membership inference attacks against other participants, even
when the global model achieves high prediction accuracies.
I. I N T RO D U CT IO N
Deep neural networks have shown unprecedented gener-
alization for various learning tasks, from image and speech
recognition to generating realistic-looking data. This su ccess
has led to many applications and services that use deep learn -
ing algorithms on large-dimension (and potentially sensit ive)
user data, including user speeches, images, medical record s,
ﬁnancial data, social relationships, and location data poi nts.
In this paper, we are interested in answering the following
critical question: What is the privacy risk of deep learning
algorithms to individuals whose data is used for training de ep
neural networks? In other words, how much is the information
leakage of deep learning algorithms about their individual
training data samples?
W e deﬁne privacy-sensitive leakage of a model, about its
training data, as the information that an adversary can lear n
from the model about them, which he is not able to infer from
other models that are trained on other data from the same
distribution. This distinguishes between the information that
we can learn from the model about the data population, and
the information that the model leaks about the particular da ta
samples which are in its training set. The former indicates
utility gain, and the later reﬂects privacy loss. W e design
inference attacks to quantify such privacy leakage.
Inference attacks on machine learning algorithms fall into
two fundamental and related categories: tracing (a.k.a. mem-
bership inference) attacks, and reconstruction attacks [1]. In
a reconstruction attack, the attacker’s objective is to inf er
attributes of the records in the training set [2], [3]. In a
membership inference attack, however, the attacker’s obje ctive
is to infer if a particular individual data record was includ ed in
the training dataset [4], [5], [6]. This is a decisional prob lem,
and its accuracy directly demonstrates the leakage of the mo del
about its training data. W e thus choose this attack as the bas is
for our privacy analysis of deep learning models.
Recent works have studied membership inference attacks
against machine learning models in the black-box setting,
where the attacker can only observe the model predictions [6 ],
[7]. The results of these works show that the distribution of
the training data as well as the generalizability of the mode l
signiﬁcantly contribute to the membership leakage. Partic u-
larly, they show that overﬁtted models are more susceptible to
membership inference attacks than generalized models. Suc h
black-box attacks, however, might not be effective against deep
neural networks that generalize well (having a large set of
parameters). Additionally, in a variety of real-world sett ings,
the parameters of deep learning algorithms are visible to th e
adversaries, e.g., in a federated learning setting where mu ltiple
data holders collaborate to train a global model by sharing t heir
parameter updates with each other through an aggregator.
Our contributions. In this paper, we present a comprehensive
framework for the privacy analysis of deep neural networks,
using white-box membership inference attacks. W e go beyond
membership inference attacks against fully-trained model s.
W e take all major scenarios where deep learning is used
for training and ﬁne-tuning or updating models, with one
or multiple collaborative data holders, when attacker only
passively observes the model updates or actively inﬂuences
the target model in order to extract more information, and
for attackers with different types of prior knowledge. Desp ite
1
differences in knowledge, observation, and actions of the
adversary, their objective is the same: membership inference .
A simple extension of existing black-box membership in-
ference attacks to the white-box setting would be using the
same attack on all of the activation functions of the model.
Our empirical evaluations show that this will not result in
inference accuracy better than that of a black-box attacker .
This is because the activation functions in the model tend to
generalize much faster compared to the output layer. The ear ly
layers of a trained model extract very simple features that
are not speciﬁc to the training data. The activation functio ns
in the last layers extract complex and abstract features, th us
should contain more information about the model’s training
set. However, this information is more or less the same as
what the output leaks about the training data.
W e design white-box inference attacks that exploit the
privacy vulnerabilities of the stochastic gradient descen t
(SGD) algorithm . Each data point in the training set in-
ﬂuences many of the model parameters, through the SGD
algorithm, to minimize its contribution to the model’s trai ning
loss. The local gradient of the loss on a target data record,
with respect to a given parameter, indicates how much and
in which direction the parameter needs to be changed to ﬁt
the model to the data record. T o minimize the expected loss
of the model, the SGD algorithm repeatedly updates model
parameters in a direction that the gradient of the loss over t he
whole training dataset leans to zero. Therefore, each training
data sample will leave a distinguishable footprint on the
gradients of the loss function over the model’s parameters .
W e use the gradient vector of the model, over all parameters,
on the target data point, as the main feature for the attack. W e
design deep learning attack models with an architecture tha t
processes extracted (gradient) features from different la yers of
the target model separately, and combines their informatio n to
compute the membership probability of a target data point.
W e train the attack model for attackers with different types
of background knowledge. Assuming a subset of the training
set is known to the attacker, we can train the attack model
in a supervised manner. However, for the adversary that lack s
this knowledge, we train the attack model in an unsupervised
manner. W e train auto-encoders to compute a membership
information embedding for any data. W e then use a clustering
algorithm, on the target dataset, to separate members from
non-members based on their membership embedding.
T o show the effectiveness of our white-box inference at-
tack, we evaluate the privacy of pre-trained and publicly
available state-of-the-art models on the CIF AR100 dataset.
W e had no inﬂuence on training these models. Our results
show that the DenseNet model—which is the best model on
CIF AR100 with 82% test accuracy—is not much vulnerable
to black-box attacks (with a 54. 5% inference attack accuracy,
where 50% is the baseline for random guess). However, our
white-box membership inference attack obtains a consider-
ably higher accuracy of 74. 3%. This shows that even well-
generalized deep models might leak signiﬁcant amount
of information about their training data, and could be
vulnerable to white-box membership inference attacks .
In federated learning, we show that a curious parameter
server or even a participant can perform alarmingly accurat e
membership inference attacks against other participants. For
the DenseNet model on CIF AR100, a local participant can
achieve a membership inference accuracy of 72. 2%, even
though it only observes aggregate updates through the pa-
rameter server. Also, the curious central parameter server
can achieve a 79. 2% inference accuracy, as it receives the
individual parameter updates from all participants. In fed erated
learning, the repeated parameter updates of the models over
different epochs on the same underlying training set is a key
factor in boosting the inference attack accuracy.
As the contributions (i.e., parameter updates) of an advers ar-
ial participant can inﬂuence the parameters other parties, in the
federated learning setting, the adversary can actively pus h
SGD to leak even more information about the participants’
data. W e design an active attack that performs gradient ascent
on a set of target data points before uploading and updating
the global parameters. This magniﬁes the presence of data
points in others’ training sets, in the way SGD reacts by
abruptly reducing the gradient on the target data points if
they are members. On the Densenet model, this leads to a
76. 7% inference accuracy for an adversarial participant, and
a signiﬁcant 82. 1% accuracy for an active inference attack by
the central server. By isolating a participant during param eter
updates, the central attacker can boost his accuracy to 87. 3%.
II. I N F E RE N CE AT TACK S
W e use membership inference attacks to measure the in-
formation leakage through deep learning models about their
training data. There are many different scenarios in which d ata
is used for training models, and there are many different way s
the attacker can observe the deep learning process. In T able I,
we cover the major criteria to categorize the attacks. This
includes attack observations, assumptions about the adver sary
knowledge, the target training algorithm, and the mode of th e
attack based on the adversary’s actions. In this section, we
discuss different attack scenarios as well as the technique s
we use to exploit deep learning algorithms. W e also describe
the architecture of our attack model, and how the adversary
computes the membership probability.
A. Attack Observations: Black-box vs. White-box Inference
The adversary’s observations of the deep learning algorith m
are what constitute the inputs for the inference attack.
Black-box. In this setting, the adversary’s observation is lim-
ited to the output of the model on arbitrary inputs. For any da ta
point x, the attacker can only obtain f(x; W). The parameters
of the model W and the intermediate steps of the computation
are not accessible to the attacker. This is the setting of machine
learning as a service platforms. Membership inference atta cks
against black-box models are already designed, which explo it
the statistical differences between a model’s predictions on its
training set versus unseen data [6].
2
Criteria Attacks Description
Observation Black-box The attacker can obtain the prediction vector f(x) on arbitrary input x, but cannot access the model parameters, nor
the intermediate computations of f(x).
x f f (x)
White-box The attacker has access to the full model f(x; W), notably its architecture and parameters W, and any hyper-parameter
that is needed to use the model for predictions. Thus, he can a lso observe the intermediate computations at hidden layers
hi(x).
x W1 h1(x) W2 h2(x) · · · Wi f(x)
T arget Stand-alone The attacker observes the ﬁnal target model f, after the training is done (e.g., in a centralized manner) u sing dataset D.
He might also observe the updated model f∆ after it has been updated (ﬁne-tuned) using a new dataset D∆ .
xD
f
D∆
f∆
ﬁne-tune
Federated The attacker could be the central aggregator, who observes i ndividual updates over time and can control the view of the
participants on the global parameters. He could also be any o f the participants who can observe the global parameter
updates, and can control his parameter uploads.
x
Aggregator (global parameters W)
D1
f(x; W{t}
1 )
D2
f(x; W{t}
2 )
DN
f(x; W{t}
N )
· · ·
down=W{t}
up=W{t}
i
Mode Passive The attacker can only observe the genuine computations by th e training algorithm and the model.
Active The attacker could be one of the participants in the federate d learning, who adversarially modiﬁes his parameter upload s
W{t}
i , or could be the central aggregator who adversarially modiﬁ es the aggregate parameters W{t} which he sends
to the target participant(s).
Knowledge Supervised The attacker has a data set D′, which contains a subset of the target set D, as well as some data points from the same
underlying distribution as D that are not in D. The attacker trains an inference model h in a supervised manner, by
minimizing the empirical loss function ∑
d∈D′ (1 − /BD
d∈D)h(d) + /BD
d∈D(1 − h(d)), where the inference model h
computes the membership probability of any data point d in the training set of a given target model f, i.e., h(d) =
Pr(d ∈ D; f).
Data Universe
D
D′
D′
∼ Pr(X = x)
∼ Pr(X = x)
Unsupervised The attacker has data points that are sampled from the same un derlying distribution as D. However, he does not have
information about whether a data sample has been in the targe t set D.
T ABLE I: V arious categories of inference attacks against machine le arning models, based on their prior knowledge, observation , mode of
attack, and the training architecture of the target models.
3
x
target model
W1 W2 ··· Wi
L(f(x; W)), y
y
∂L
∂ W1
∂L
∂ W1
∂L
∂ W2
∂L
∂ W2
···
∂L
∂ Wi
∂L
∂ Wi
LL
h1(x)h1(x) h2(x)h2(x) ··· f(x)f(x)
attack modelattack features
CNN FCN CNN FCN CNN··· FCN FCN FCN
Encoder (FCN)
attack output :
Decoder (FCN)
unsupervised attack component
L
/BD
y=arg max f(x) f(x)y H(f(x))

 ∂L
∂ W


Fig. 1: The architecture of our white-box inference attack. Given
target data (x, y), the objective of the attack is to determine its
membership in the training set D of target model f. The attacker
runs the target model f on the target input x, and computes all the
hidden layers hi(x), the model’s output f(x), and the loss function
L(f(x), y; W), in a forward pass. The attacker also computes the
gradient of the loss with respect to the parameters of each la yer
∂L
∂ Wi
, in a backward pass. These computations, in addition to the o ne-
hot encoding of the true label y, construct the input features of the
inference attack. The attack model consists of convolution al neural
network (CNN) components and fully connected network (FCN)
components. For attacking federated learning and ﬁne-tuni ng, the
attacker observes each attack feature T times, and stacks them before
they are passed to the corresponding attack component. For e xample,
the loss features are composed as L = {L{1}, L{2},· · ·, L{T }}).
The outputs of the CNN and FCN components are appended togeth er,
and this vector is passed to a fully connected encoder. The ou tput
of the encoder, which is a single value, is the attack output. This
represents an embedding of the membership information in a s ingle
value. In the supervised attack setting, this embedding is t rained to
be Pr{(x, y) ∈ D}. In the unsupervised setting, a decoder is trained
to reconstruct important features of the attack input (such as the
model’s output uncertainty H(f(x)), and the norm of its gradients
 ∂L
∂ W

) from the attack output. This is similar to deep auto-encode rs.
All unspeciﬁed attack layers are fully connected. The detai ls of the
architecture of the attack is presented in T able XIV in Appen dix A.
White-box. In this setting, the attacker obtains the model
f(x; W) including its parameters which are needed for pre-
diction. Thus, for any input x, in addition to its output, the
attacker can compute all the intermediate computations of
the model. That is, the adversary can compute any function
over W and x given the model. The most straightforward
functions are the outputs of the hidden layers, hi(x) on the
input x. As a simple extension, the attacker can extend black-
box membership inference attacks (which are limited to the
model’s output) to the outputs of all activation functions o f
the model. However, this does not necessarily contain all th e
useful information for membership inference. Notably, the
model output and activation functions could generalize if t he
model is well regularized. Thus, there might not be much
difference, in distribution, between the activation funct ions of a
model on its training versus unseen data. This can signiﬁcan tly
limit the power of the inference attacks (as we also show in
our evaluations).
What we suggest is to exploit the algorithm used to train
deep learning models: the stochastic gradient descent (SGD )
algorithm. Let L(f(x; W), y ) be the loss function for the
classiﬁcation model f. During the training, the SGD algorithm
minimizes the empirical expectation of the loss function ov er
the training set D:
min
W
E(x,y)∼D
[
L(f(x; W), y )
]
(1)
The SGD algorithm solves this minimization by repeatedly
updating parameters, W, towards reducing the loss on small
randomly selected subsets of D. Thus, for any data record in
the training dataset, the gradient of the loss ∂L
∂ W over the data
record is pushed towards zero, after each round of training.
This is exactly what we can exploit to extract information
about a model’s training data.
For a target data record (x, y ), the adversary can compute
the loss of the model L(f(x; W), y ), and can compute the
gradients of the loss with respect to all parameters ∂L
∂ W
using a simple back-propagation algorithm. Given the large
number of parameters used in deep neural networks (millions
of parameters), the vector with such a signiﬁcantly large
dimension cannot properly generalize over the training dat a
(which in many cases is an order of magnitude smaller in
size). Therefore, the distribution of the model’s gradient s on
members of its training data, versus non-members, is likely
to be distinguishable. This can help the adversary to run
an accurate membership inference attack, even though the
classiﬁcation model (with respect to its predictions) is we ll-
generalized.
Inference model. W e illustrate the membership inference
attack in Figure 1. The signiﬁcance of gradient (as well as
activation) computations for a membership inference attac k
varies over the layers of a deep neural network. The ﬁrst laye rs
tend to contain less information about the speciﬁc data poin ts
in the training set, compared to non-member data record from
the same underlying distribution. W e can provide the gradie nts
and activations of each layer as separate inputs to the attac ker,
4
as the attacker might need to design a speciﬁc attack for each
layer. This enables the inference attack to split the infere nce
task across different layers of the model, and then combine
them to determine the membership. This engineering of the
attack model architecture empowers the inference attack, a s it
reduces the capacity of the attack model and helps ﬁnding the
optimal attack algorithm with less background data.
The distinct inputs to the attack model are the set of
gradients ∂L
∂ W1
, ∂L
∂ W2
, ···, the set of activation vectors for
different layers h1(x), h 2(x), ···, the model output f(x), the
one-hot encoding of the label y, and the loss of the model on
the target data L(f(x; W), y ). Each of these are separately
fed into the attack model, and are analyzed separately using
independent components.
Inference attack components. The attack model is composed
of feature extraction components and an encoder component.
T o extract features from the output of each layer, plus the
one-hot encoding of the true label and the loss, we use fully
connected network (FCN) submodules with one hidden layer.
W e use convolutional neural network (CNN) submodules for
the gradients. When the gradients are computed on fully
connected layers (in the target model), we set the size of the
convolutional kernel to the input size of the fully connecte d
layer, to capture the correlation of the gradients in each
activation function. W e reshape the output of each submodul e
component into a ﬂat vector, and then concatenate the output
of all components. W e combine the outputs of all attack
feature extraction components using a fully connected enco der
component with multiple hidden layers. The output of the
encoder is a single score, which is the output of the attack.
This score (in the supervised attack raining) predicts the
membership probability of the input data.
B. Inference T arget: Stand-alone vs. F ederated Learning
There are two major types of training algorithms for deep
learning, depending on whether the training data is availab le
all in one place (i.e., stand-alone centralized training), or it
is distributed among multiple parties who do not trust each
other (i.e., federated learning) [8]. In both cases, the att acker
could be the entity who obtains the ﬁnal trained model. In
addition to such attack setting, the attacker might observe an
updated version of the model after ﬁne-tuning, for instance ,
which is very common in deep learning. Besides, in the
case of federated learning, the attacker can be an entity who
participates in the training. The settings of ﬁne-tunning and
federated learning are depicted in T able I.
Stand-alone ﬁne-tunning. A model f is trained on dataset
D. At a later stage it is updated to f∆ after being ﬁne-tuned
using a new dataset D∆ . If the attacker observes the ﬁnal
outcome, we want to measure the information leakage of the
ﬁnal model f∆ about the whole training set D∪D∆ . However,
given that two versions of the model exist (before and after
ﬁne-tuning), we are also interested in measuring the extra
information that could be learned about the training data, f rom
the two model snapshots. The attacker might also be interest ed
only in recovering information about the new set D∆ . This is
very relevant in numerous cases where the original model is
trained using some unlabeled (and perhaps public) data, and
then it is ﬁne-tunned using sensitive private labeled data.
The model for inference attacks against ﬁne-tunned models
is a special case of our membership inference model for at-
tacking federated learning. In both cases, the attacker obs erves
multiple versions of the target model.
F ederated learning. In this setting, N participants, who have
different training sets Di, agree on a single deep learning task
and model architecture to train a global model. A central ser ver
keeps the latest version of the parameters W for the global
model. Each participant has a local model, hence a local set
of parameters Wi. In each epoch of training, each participant
downloads the global parameters, updates them locally usin g
SGD algorithm on their local training data, and uploads them
back to the server. The parameter server computes the averag e
value for each parameter using the uploaded parameters by
all participants. This collaborative training continues u ntil the
global model converges.
There are two possibilities for the position of the attacker
in federated learning: The adversary can be the centralized pa-
rameter server, or one of the participants. A curious parame ter
server can receive updates from each individual participan t
over time W {t}
i , and use them to infer information about
the training set of each participant. A malicious parameter
server can also control the view of each participant on the
global model, and can act actively to extract more informati on
about the training set of a participant (as we discuss under
active attacks). Alternatively, the adversary can be one of
the participants. An adversarial participant can only obse rve
the global parameters over time W {t}, and craft his own
adversarial parameter updates W {t}
i to gain more information
about the union of the training data of all other participant s.
In either of these cases, the adversary observes multiple
versions of the target model over time. The adversary can try
to run an independent membership inference attack on each of
these models, and then combine their results. This, however ,
might not capture the dependencies between parameter value s
over time, which can leak information about the training dat a.
Instead, in our design we make use of a single inference model ,
where each attack component (e.g., components for gradient s
of layer i) processes all of its corresponding inputs over the
observed models at once. This is illustrated in Figure 1. For
example, for the attack component that analyzes the loss val ue
L, the input dimension can be 1×T , if the adversary observes
T versions of the target model over time. The output of the
attack component is also T times larger than the case of
attacking a stand-alone model. These correlated outputs, o f all
attack components, are processed all at once by the inferenc e
model.
C. Attack Mode: P assive vs. Active Inference Attack
The inference attacks are mostly passive, where the ad-
versary makes observations without modifying the learning
process. This is the case notably for attacking models after
the training is over, e.g., the stand-alone setting.
5
Active attacks. The adversary, who is participating in the
training process, can actively inﬂuence the target model in
order to extract more information about its training set. Th is
could be the case notably for running inference attacks agai nst
federated learning. In this setting, the central parameter server
or a curious participant can craft adversarial parameter up -
dates for a follow-up inference attack. The inference model
architecture will be the same for passive and active attacks .
The active attacker can exploit the SGD algorithm to run the
active attack. The insight we use to design our attack is that
the SGD algorithm forcefully decreases the gradient of the l oss
on the training data, with the hope that this generalizes to t he
test data as well. The amount of the changes depends on the
contribution of a data point in the loss. So, if a training dat a
point leads to a large loss, the SGD algorithm will inﬂuence
some parameters to adapt themselves towards reducing the lo ss
on this point. If the data point is not seen by the model during
training, the changes in the gradient on this point is gradua l
throughout the training. This is what we exploit in our activ e
membership inference attack.
Let x be a data record, which is targeted by the adversary
to determine its membership. Let us assume the adversary is
one of the participants. The attacker runs a gradient ascent on
x, and updates its local model parameters in the direction of
increasing the loss on x. This can simply be done by adding
the gradient to the parameters,
W ← W + γ ∂Lx
∂W , (2)
where γ is the adversarial update rate. The adversary then
uploads the adversarially computed parameters to the centr al
server, who will aggregate them with the parameter updates
from other participants. The adversary can run this attack o n
a batch of target data points all at the same time.
If the target record x is in the training set of a participant,
its local SGD algorithm abruptly reduces the gradient of the
loss on x. This can be detected by the inference model, and
be used to distinguish members from non-members. Repeated
active attacks, which happens in federated learning, lead t o
high conﬁdence inference attacks.
D. Prior Knowledge: Supervised vs. Unsupervised Inference
T o construct his inference attack model, the adversary need s
to ﬁnd the meaningful mapping between the model’s behavior
on a data point and its membership in the training set. The
most straightforward way of learning such relationship is
through some known members of the training data, and some
data points from the same distribution which are not in the
training data set. This is illustrated in T able I. The advers ary
has a dataset D′ that overlaps with the target dataset D. Given
this dataset, he can train the attack model in a supervised wa y,
and use it to attack the rest of the training dataset.
Let h be the inference attack model. In the supervised
setting, we minimize the (mean square) loss of the attacker
for predicting the membership of the data points in its train ing
set D′:
∑
d∈D′∩D
(h(d) −1)2 +
∑
d∈D′\D
(h(d))2 (3)
If the adversary does not have known samples from the
target training set, there are two possibilities for traini ng
the inference attack models: supervised training on shadow
models [6], and unsupervised training on the target model.
Shadow models are models with the same architecture as the
target model. The training data records for the shadow model s
are generated from the same distribution as the target train ing
data, but do not have a known overlap with the target training
set. The attacker trains the attack model on the shadow model s.
As the behavior of the shadow models on their training data is
more or less similar to the behavior of the target model on its
training data, the attack models trained on the shadow model s
are empirically shown to be effective.
The attack output for (shadow) supervised training setting
is the probability of membership.
h(d) = Pr( d ∈D; f) (4)
Unsupervised training of inference models. W e introduce
an alternative approach to shadow training, which is unsupe r-
vised training of the attack model on the target model. The
assumption for this attack is that the attacker has access to a
dataset D′ which partially overlaps with the target training set
D, however, the adversary does not know which data points
are in D′ ∩D.
Our objective is to ﬁnd a score for each data point that rep-
resents its embedding in a space, which helps us easily separ at-
ing members from non-members (using clustering algorithms ).
The attack’s output should compute such representations. W e
make use of an encoder-decoder architecture to achieve this .
This is very similar to the auto-encoders for unsupervised d eep
learning. As shown in Figure 1, the output of the attack is fed
into a decoder. The decoder is a fully connected network with
one hidden layer.
The objective of the decoder is to reconstruct some key
features of the attack input which are important for member-
ship inference. These include the loss value L, whether the
target model has predicted the correct label
/BD
y=arg max f(x),
the conﬁdence of the model on the correct label f(x)y , the
prediction uncertainty (entropy) of the model H(f(x)), and the
norm of the gradients

 ∂L
∂ W

. As previous work [6] as well
as our empirical results show , these features are strong sig nals
for distinguishing members from non-members. The encoder-
decoder architecture maximizes the information that the at tack
output contains about these features. Thus, it generates a
membership embedding for each data point. Note that after
training the attack model, the decoder plays no role in the
membership inference attack.
The attack in the unsupervised setting is a batch attack,
where the adversary attacks a large set of data records (disj oint
from his background knowledge). W e will use the encoder to
for each target data record, and we compute the embedding
6
value (output of the encoder model). Next, we use a clusterin g
algorithm (e.g., we use the spectral clustering method) to
cluster each input of the target model in two clusters. Note
that the outcome of the clustering algorithm is a threshold, as
the attack output is a single number. W e predict the cluster
with the larger gradient norm as non-members.
III. E X P E RIM E N TA L SE T U P
W e implemented our attacks using Pytorch. 1 W e trained all
of the models on a PC equipped with four Titan X GPU each
with 12 GB of memory.
A. Datasets
W e used three datasets in our experiments: a standard image
recognition benchmark dataset, CIF AR100, and two datasets
Purchase100 and T exas100 [6].
CIF AR100. This is a popular benchmark dataset used to
evaluate image recognition algorithms [9]. It contains 60, 000
color (RGB) images, each 32 × 32 pixels. The images are
clustered into 100 classes based on objects in the images.
Purchase100. The Purchase100 dataset contains the shop-
ping records of several thousand online customers, extract ed
during Kaggle’s “acquire valued shopper” challenge. 2 The
challenge was designed to identify offers that would attrac t
new shoppers. W e used the processed and simpliﬁed version
of this dataset (courtesy of the authors of [6]). Each record
in the dataset is the shopping history of a single user. The
dataset contains 600 different products, and each user has a
binary record which indicates whether she has bought each of
the products (a total of 197, 324 data records). The records
are clustered into 100 classes based on the similarity of the
purchases, and our objective is to identify the class of each
user’s purchases.
T exas100. This dataset includes hospital discharge data
records released by the T exas Department of State Health
Services 3 . The records contain generic information about the
patients (gender, age, and race), external causes of injury (e.g.,
drug misuse), the diagnosis, and patient procedures. Simil ar to
Purchase100, we obtained the processed dataset (Courtesy o f
the authors [6]), which contains 67, 330 records and 6, 170
binary features.
B. T arget Models
W e investigate our attack model on the previously
mentioned three datasets, T exas100, Purchase100 and CI-
F AR100. For the CIF AR100 dataset we used Alexnet [10],
ResNet [11], DenseNet [12] models. W e used SGD opti-
mizer [13] to train the CIF AR100 models with learning rates
of 0. 01, 0. 001, 0. 0001 for epochs 0 −50, 50 −100, 100 −300
accordingly. W e used l2 regularization with weight of 0. 0005.
For the T exas100 and Purchase100 datasets, we used fully
connected models. For Purchase100, we used a model with
1 https://pytorch.org/
2 https://www .kaggle.com/c/acquire-valued-shoppers-challenge/data
3 https://www .dshs.texas.gov/thcic/hospitals/Inpatientpudf.shtm
layer sizes of 600, 1024, 512, 256, 128, 100 (where 100 is the
output layer), and for T exas100, we used layers with size
1024, 512, 256, 128, 100 (where 100 is the output layer). W e
used Adam [13] optimizer with the learning rate of 0. 001
for learning of these models. W e trained each model for 100
epochs across all of our experiments. W e selected the model
with the best testing accuracy across all the 100 epochs.
C. Pre-trained Models
T o demonstrate that our attacks are not limited to our
training algorithm, we used publicly available pre-traine d
CIF AR100 models 4. All of these models are tuned to get the
best testing accuracy using different regularization tech niques.
D. F ederated Learning
W e performed the training for all of the federated learning
experiments. Speciﬁcally, we used the averaging aggregati on
method for the federated scenario [8]. Each training party
sends the parameter updates after every epoch of training to
the central model, and the central server averages the model s’
updates from the parties and sends the updated model to all
parties. In our experiments, we use the same training datase t
size for all parties, and each party’s training data is selec ted
uniformly at random from our available datasets.
E. Attack Models
T able XIV in Appendix A, presents the details of our
attack model architecture. As can be seen, we used ReLU
activation functions, and we initialized the weights using a
normal distribution with mean 0 and standard deviation of
0. 01. The bias values of all layers are initialized with 0. The
batch size of all experiments is 64. T o train the attack model
we use the Adam optimizer with a learning rate of 0. 0001. W e
train attack models for 100 epochs and pick the model with
the highest testing accuracy, across all the 100 epochs.
T ables II and XI present the dataset sizes used for training
the target and attack models. In the supervised setting for
training the attack models, we assume the attacker has acces s
to a fraction of the training set and some non-member samples .
In this case, to balance the training, we select half of each
batch to include member instances and the other half non-
member instances from the attacker’s background knowledge .
Creating the batches in this fashion will prevent the attack
model from a bias towards member or non-member instances.
F . Evaluation Metrics
Attack accuracy The attacker’s output has two classes “ Mem-
ber” and “ Non-member”. Attack accuracy is the fraction of
the correct membership predictions (predicting members as
member and non-members as non-member) for unknown data
points. The size of the set of member and non-member samples
that we use for evaluating the attack are the same.
True/F alse positive For a more detailed evaluation of attack
performance, we also measure the true positive and false
4 W e make use of ResNet, DenseNet, and Alexnet pre-trained mod els,
provided in https://github.com/bearpaw/pytorch-classi ﬁcation
7
T ABLE II: Size of datasets used for training and testing the target
classiﬁcation model and the membership inference model
T arget Model Inference Attack Model
Datasets T raining T est T raining T raining T est T est
Members Non-members Members Non-members
CIF AR100 50,000 10,000 25,000 5,000 5,000 5,000
T exas100 10,000 70,000 5,000 10,000 10,000 10,000
Puchase100 20,000 50,000 10,000 10,000 10,000 10,000
positive rates of the attacker. Positive is associated with the
attacker outputting “member”.
Prediction uncertainty For a classiﬁcation model, we com-
pute its prediction uncertainty using the normalized entro py
of its prediction vector for a given input.
H = 1
log(K)
K∑
i=1
pi log(pi) (5)
where K is the number of all classes and pi is the prediction
probability for the ith class. W e compute the probabilities
using a softmax function as pi = eh(d)(i)
∑ K
k=1 eh(d)(k) .
IV . E X P E RIM E N T S
W e start by presenting our results for the stand-alone
scenario, followed by our results for the federated learnin g
scenario.
A. Stand-Alone Setting: Attacking Fully-T rained Models
W e investigate the case where the attacker has access to the
fully-trained target model, in the white-box setting. Ther efore,
the attacker can leverage the outputs and the gradients of th e
hidden layers of the target model to perform the attack. W e
have used pre-trained CIF AR100 models, and have trained
other target models and the attack models using the dataset
sizes which are presented in T able II.
Impact of the outputs of different layers: T o understand
and demonstrate the impact of different layers’ outputs, we
perform the attack separately using the outputs of individual
layers. W e use a pre-trained Alexnet model as the target model,
where the model is composed of ﬁve convolutional layers and
a fully connected layer at the end. T able III shows the accura cy
of the attack using the output of each of the last three layers .
As the table shows, using the last layers results in the highe st
attack accuracy, i.e., among the layer outputs, the last layer
(model output) leaks the most membership information
about the training data .The reason behind this is twofold. By
proceeding to the later layers, the capacity of the paramete rs
ramps up, which leads the target model to store unnecessary
information about the training dataset, and therefore leak more
information. Moreover, the ﬁrst layers extract simple feat ures
from the input, thus generalize much better compared to the
last layers, which are responsible for complex task of ﬁndin g
the relationship between abstract features and the classes . W e
did not achieve signiﬁcant accuracy gain by combining the
outputs from multiple layers; this is because the leakage fr om
the last layer (which is equivalent to a black-box inference
attack) already contains the membership information that l eaks
from the output of the previous layers.
Impact of gradients: In Section II-A, we discussed why
gradients should leak information about the training datas et. In
T able VIII, we compare the accuracy of the membership attack
when the attacker uses the gradients versus layer outputs,
for different dataset and models. Overall, the results show
that gradients leak signiﬁcantly more membership information
about the training set, compared to the layer outputs .
W e compare the result of the attack on pre-trained
CIF AR100-ResNet and CIF AR100-DenseNet models, where
both are designed for the same image recognition task, both
are trained on the same dataset, and both have similar gener-
alization error. The results show that these two models have
various degrees of membership leakage; this suggests that t he
generalization error is not the right metric to quantify
privacy leakage in the white-box setting. The large capacit y
of the model which enables it to learn complex tasks and
generalize well, leads to also memorizing more information
about the training data. The total number of the parameters
in pre-trained Densenet model is 25. 62M , whereas this is only
1. 7M parameters for ResNet.
W e also investigated the impact of gradients of different
layers on attack accuracy. The results are shown in T able IV
show that the gradient of the later layers leak more
membership information . This is similar to our ﬁndings for
layer outputs: the last layer generalizes the least among al l the
layers in the model, and is the most dependent layer to the
training set. By combining the gradients of all layers, we ar e
able to only slightly increase the attack accuracy.
Finally, T able V shows the attack accuracy when we com-
bine the output layer and gradients of different layers. W e s ee
that the gradients from the last layer leak the most membersh ip
information.
Impact of the training size: T able VI shows attack accuracy
for various sizes of the attacker’s training data. The model s
are tested on the same set of test instances, across all these
scenarios. As expected, increasing the size of the attacker ’s
training dataset improves the accuracy of the membership
inference attack.
Impact of the gradient norm: In this experiment, we
demonstrate that the norm of the model’s gradients is highly
correlated with the accuracy of membership inference, as it
behaves differently for member and non-member instances.
Figure 3 plots the last-layer gradient norms over consecuti ve
training epochs for member and non-member instances (for
the Purchase100 dataset). As can be seen, during training, t he
gradient norms of the member instances decrease over traini ng
epochs, which is not the case for non-member instances.
Figure 4 shows the distribution of last-layer gradient norm s
for members and non-members on three various pretrained
architectures on CIF AR100. Comparing the ﬁgures with T a-
ble VIII, we see that a model leaks more membership infor-
mation when the distribution of the gradient norm is more
distinct for member and non-member instances . For instance,
8
T ABLE III: Attack accuracy using the outputs of individual activa-
tion layers. Pre-trained Alexnet on CIF AR100, stand-alone setting.
Output Layer Attack Accuracy
Last layer (prediction vector) 74.6% (black-box)
Second to last 74.1%
Third to last 72.9%
Last three layers, combined 74.6%
T ABLE IV: Attack accuracy when we apply the attack using
parameter gradients of different layers. (CIF AR100 datase t with
Alexnet architecture, stand-alone scenario)
Gradient w .r .t. Attack Accuracy
Last layer parameters 75.1%
Second to last layer parameters 74.6%
Third to last layer parameters 73.5%
Forth to last layer parameters 72.6%
Parameters of last four layers, combined 75.15%
T ABLE V: Attack accuracy using different combinations of layer
gradients and outputs. (CIF AR100 dataset with Alexnet arch itecture,
stand-alone scenario)
Gradients w .r .t. Output Layers Attack T est Accuracy
Last Layer - 75.10%
Last layer Last layer 75.11%
Last Layer All Layer 75.12%
All Layer All Layer 75.18%
we can see that ResNet and DenseNet both have relatively
similar generalization errors, but the gradient norm distr ibution
of members and non-members is more distinguishable for
DenseNet (Figure 4b) compared to ResNet (Figure 4c). W e
see that the attack accuracy in DenseNet is much higher than
ResNet.
Also, we show that the accuracy of our inference attack is
higher for classiﬁcation output classes (of the target mode l)
with a larger difference in member/non-member gradient
norms. Figure 2a plots the average of last layer’s gradient
norms for different output classes for member and non-
member instances; we see that the difference of gradient nor ms
between members and non-members varies across different
classes. Figure 2b shows the receiver operating characteri stic
(ROC) curve of the inference attack for three output classes
with small, medium, and large differences of gradient norm
between members and non-members (averaged over many
samples). As can be seen, the larger the difference of gradient
norm between members and non-members, the higher the
accuracy of the membership inference attack .
Impact of prediction uncertainty: Previous work [6] claims
that the prediction vector’s uncertainty is an important fa ctor
in privacy leakage. W e validate this claim by evaluating the
attack for different classes in CIF AR100-Alexnet with diff erent
prediction uncertainties. Speciﬁcally, we selected three classes
0 20 40 60 80 100
0
500
1000
1500
Class
Gradient norm
Member instances
Non-member instances
(a) Gradient norm for member and non-member data across all c lasses
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
False Positive
True Positive Small gradient diff (174.33)
Medium gradient diff (860.32)
Large gradient diff (1788.13)
Random guess attack
(b) Attacker accuracy for class members with various differ ences in
their member/non-member gradient norms
Fig. 2: Attack accuracy is different for different output classes ( pre-
trained CIF AR100-Alextnet model in the stand-alone scenar io).
T ABLE VI: Attack accuracy for various sizes of the attacker’s
training dataset. The size of the target model’s training da taset is
50,000. (The CIF AR100 dataset with Alexnet, stand-alone sc enario)
Members Sizes Non-members Sizes Attack Accuracy
10,000 2,000 73.2%
15,000 2,000 73.7%
15,000 5,000 74.8%
25,000 5,000 75.1%
T ABLE VII: Accuracy of our unsupervised attack compared to the
Shadow models approach [6] for the white-box scenario.
Dataset Arch (Unsupervised) (Shadow Models)
Attack Accuracy Attack Accuracy
CIF AR100 Alexnet 75.0% 70.5%
CIF AR100 DenseNet 71.2% 64.2%
CIF AR100 ResNet 63.1% 60.9%
T exas100 Fully Connected 66.3% 65.3%
Purchase100 Fully Connected 71.0% 68.2%
with small, medium, and high differences of prediction unce r-
9
0 20 40 60 80
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Training epoch
Gradient norm
Member instances
Non-member instances
Fig. 3: Gradient norms of the last layer during learning epochs for
member and non-member instances (for Purchase100).
tainties, where the attack accuracies are shown in Figure 6 f or
these classes. Similar to the differences in gradient norms , the
classes with higher prediction uncertainty values leak mor e
membership information .
B. Stand-Alone Setting: Unsupervised Attacks
W e also implement our attacks in an unsupervised scenario,
in which the attacker has data points sampled from the same
underlying distribution, but he does not know their member
and non-member labels. In this case, the attacker classiﬁes the
tested records into two clusters as described in Section II- D.
W e implemented our attack and compared its performance
to Shadow models of Shokri et al. [6] introduced earlier. W e
train our unsupervised models on various datasets based on t he
training and test dataset sizes in T able II. W e train a single
Shadow model on each of T exas100 and Purchase100 datasets
using training sizes according to T able II. The training set s of
the Shadow models do no overlap with the training sets of
the target models. For the CIF AR100 dataset, however, our
Shadow model uses a training dataset that overlaps with the
target model’s dataset, as we do not have enough instances
(we train each model with 50,000 instances out of the total
60,000 available records).
After the training, we use the Spectral clustering algo-
rithm [14] to divide the input samples into two clusters. As
shown earlier (Figure 4), the member instances have smaller
gradient norm values. Therefore, we assign the member label
to the cluster with a smaller average gradient norm, and the
non-member label to the other cluster.
T able VII compares the accuracy of our unsupervised attack
with shadow training [6] on various datasets and architectu res.
W e see that our approach offers a noticeably higher accuracy .
The intuition behind our attack working is that the encoded
values of our unsupervised algorithm present different dis tri-
butions for member and non-member samples. This can be
seen in Figure 5 for various datasets and architectures.
C. Stand-Alone Setting: Attacking Fine-T uned Models
W e investigate privacy leakage of ﬁne-tuned target models.
In this scenario, the victim trains a model with dataset D, then
he uses a dataset D∆ to ﬁne-tune the trained model to improve
its performance. Hence, the attacker has two snapshots of th e
trained model, one using only D, and one for the same model
which is ﬁne-tuned using D∆ . W e assume the attacker has
access to both of the trained models (before and after ﬁne-
tuning). W e are interested in applying the membership infer -
ence attack in this scenario, where the goal of the adversary is
to distinguish between the members of D, D∆ , and ¯D, which
is a set of non-members.
W e use the same training dataset as in the previous experi-
ments (T able II); we used 60% of the train dataset as D and the
rest for D∆ . T able IX shows the train, test, and attack accuracy
for different scenarios. As can be seen, the attacker is able to
distinguish between members (in D or D∆ ) and non-members
( ¯D) with accuracies similar to previous settings. Additional ly,
the attacker can also distinguish between the members of D
and D∆ with reasonably high accuracies.
D. F ederated Learning Settings: P assive Inference Attacks
T able XI shows the dataset sizes used in our federated
attack experiments. For the CIF AR100 experiment with a
local attacker, each participant uses 30,000 instances to t rain,
which overlaps between various participants due to non-
sufﬁcient number of instances. For all the other experiment s,
the participants use non-overlapping datasets. In the foll owing,
we present the attack in various settings.
The Passive Global Attacker: In this scenario, the attacker
(the parameter aggregator) has access to the target model’s
parameters over multiple training epochs (see Section II-B ).
Thus, he can passively collect all the parameter updates
from all of the participants, at each epoch, and can perform
the membership inference attack against each of the target
participants, separately.
Due to our limited GPU resources, our attack observes each
target participant during only ﬁve (non-consecutive) trai ning
epochs. T able XII shows the accuracy of our attack when
it uses different sets of training epochs (for the CIF AR100
dataset with Alexnet). W e see that using later epochs, sub-
stantially increases the attack accuracy . Intuitively, this is
because the earlier training epochs contain information of the
generic features of the dataset, which do not leak signiﬁcan t
membership information, however, the later epochs contain
more membership information as the model starts to learn the
outliers in such epochs [15].
T able X presents the results of this attack on different
datasets. For the Purchase100 and T exas100 datasets we use
the [40, 60, 80, 90, 100] training epochs, and for the CIF AR100
dataset we use epochs [100, 150, 200, 250, 300]. When the
attacker has access to several training epochs in the CIF AR1 00
target models, he achieves a high membership attack accurac y.
In T exas100 and Purchase100 datasets, however, the accurac y
of the attack decreases compare to the stand-alone setting.
10
T ABLE VIII: The attack accuracy for different datasets and different ta rget architectures using layer outputs versus gradients. T his is the
result of analyzing the stand-alone scenario, where the CIF AR100 models are all obtained from pre-trained online repos itories.
T arget Model Attack Accuracy
Dataset Architecture T rain Accuracy T est Accuracy Black-box White-box (Outputs) White-box (Gradients)
CIF AR100 Alexnet 99% 44% 74.2% 74.6% 75.1%
CIF AR100 ResNet 89% 73% 62.2% 62.2% 64.3%
CIF AR100 DenseNet 100% 82% 67.7% 67.7% 74.3%
T exas100 Fully Connected 81.6% 52% 63.0% 63.3% 68.3%
Purchase100 Fully Connected 100% 80% 67.6% 67.6% 73.4%
T ABLE IX: Attack accuracy on ﬁne-tuned models. D is the initial training set, D∆ is the new dataset used for ﬁne-tuning, and ¯D is the
set of non-members (which is disjoint with D and D∆ ).
Dataset Architecture T rain Acc. T est Acc. Distinguish D from D∆ Distinguish D from ¯D Distinguish D∆ from ¯D
CIF AR100 Alexnet 100.0% 39.8% 62.1% 75.4% 71.3%
CIF AR100 DenseNet 100.0% 64.3% 63.3% 74.6% 71.5%
T exas100 Fully Connected 95.2% 48.6% 58.4% 68.4% 67.2%
Purchase100 Fully Connected 100.0% 77.5% 64.4% 73.8% 71.2%
0 500 1000 1500 2000 2500 3000
0.0
0.2
0.4
0.6
0.8
Gradient norm
Fraction
Member
Non-member
(a) CIF AR100-Alexnet
0 50 100 150 200 250 300
0.0
0.2
0.4
0.6
0.8
1.0
Gradient norm
Fraction
Member
Non-member
(b) CIF AR100-Densenet
0 50 100 150 200 250 300
0.0
0.2
0.4
0.6
Gradient norm
Fraction
Member
Non-member
(c) CIF AR100-Resnet
Fig. 4: The distribution of gradient norms for member and non -member instances of different pretrained models.
0 100 200 300 400 500
0.0
0.2
0.4
0.6
0.8
1.0
Encoded value
Fraction
Non-member
Member
(a) DenseNet-CIF AR100
0 100 200 300 400 500
0.0
0.2
0.4
0.6
0.8
1.0
Encoded value
Fraction
Non-member
Member
(b) ResNet-CIF AR100
0 500 1000 1500 2000 2500
0.0
0.2
0.4
0.6
Encoded value
Fraction
Non-member
Member
(c) AlexNet-CIF AR100
Fig. 5: The distribution of the encoded values (i.e., the att ack output) for the member and non-member instances of our
unsupervised algorithm are distinguishable. This is the in tuition behind the high accuracy of our unsupervised attack .
This is due to the fact that averaging in the federated learning
scenarios will reduce the impact of each individual party .
The Passive Local Attacker: A local attacker cannot observe
the model updates of the participants; he can only observe th e
11
T ABLE X: Attack accuracy in the federated learning setting. There ar e 4 participants. A global attacker is the central parameter aggregator,
and the local attacker is one of the participants. The global attacker performs the inference against each individual pa rticipant, and we report
the average attack accuracy . The local attacker performs th e inference against all other participants. The passive att acker follows the protocol
and only observes the updates. The active attacker changes i ts updates, or (in the case of a global attack) isolates one pa rticipant by not
passing the updates of other participants to it, in order to i ncrease the information leakage.
T arget Model Global Attacker (the parameter aggregator) Local Attacker (a participant)
Passive Active Passive Active
Dataset Architecture Gradient Ascent Isolating Isolating Gradient Ascent Gradient Ascent
CIF AR100 Alexnet 85.1% 88.2% 89.0% 92.1% 73.1% 76.3%
CIF AR100 DenseNet 79.2% 82.1% 84.3% 87.3% 72.2% 76.7%
T exas100 Fully Connected 66.4% 69.5% 69.3% 71.7% 62.4% 66.4%
Purchase100 Fully Connected 72.4% 75.4% 75.3% 82.5% 65.8% 69.8%
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
False Positive
True Positive Small uncert diff (0.25)
Medium uncert diff (0.36)
Large uncert diff (0.59)
Random guess
Fig. 6: Attack’s ROC for three different classes of data with
large, medium, and small prediction uncertainty values (pr e-trained
CIF AR100-Alextnet model in the stand-alone scenario).
T ABLE XI: Dataset sizes in the federated learning experiments
Parties’ Datasets Inference Attack Model
Datasets T raining T est T raining T raining T est T est
Members Non-members Members Non-members
CIF AR100 30,000 10,000 15,000 5,000 5,000 5,000
T exas100 8,000 70,000 4,000 4,000 4,000 4,000
Puchase100 10,000 50,000 5,000 5,000 5,000 5,000
T ABLE XII: The accuracy of the passive global attacker in the
federated setting when the attacker uses various training e pochs.
(CIF AR100-Alexnet)
Observed Epochs Attack Accuracy
5,10,15,20,25 57.4%
10,20,30,40,50 76.5%
50,100,150,200,250 79.5%
100,150,200,250,300 85.1%
T ABLE XIII: The accuracy of the passive local attacker for different
numbers of participants. (CIF AR100-Alexnet)
Number of Participants Attack Accuracy
2 89.0%
3 78.1%
4 76.7%
5 67.2%
aggregate model parameters. W e use the same attack model
architecture as that of the global attack. In our experiment s,
there are four participants (including the local attacker) . The
goal of the attacker is to learn if a target input has been a
member of the training data of any other participants. T able X
shows the accuracy of our attack on various datasets. As
expected, a local attack has a lower accuracy compared to the
global attack; this is because the local attacker observes t he
aggregate model parameters of all participants, which limits
the extent of membership leakage. The accuracy of the local
attacker degrades for larger numbers of participants . This is
shown in T able XIII for the CIF AR100 on Alexnet model.
E. F ederated Learning Settings: Active Inference Attacks
T able X shows the results of attacks on federated learning.
The Gradient Ascent Attacker: In this scenario, the attacker
adversarially manipulates the learning process to improve the
membership inference accuracy. The active attack is descri bed
in Section II-C. W e evaluate the attack accuracy on predicti ng
the membership of 100 randomly sampled member instances,
from the target model, and 100 non-member instances. For
all such target instances (whose membership is unknown to
the attacker), the attacker updates their data features tow ards
ascending the gradients of the global model (in case of the
global attack) or the local model (in the case of a local attac k).
Figure 7 compares the last-layer gradient norm of the target
model for different data points. As Figure 7a shows, when
the attacker ascends on the gradients of the target instance s,
the gradient norm of the target members will be very similar
to that of non-target member instances in various training
epochs. On the other hand, this is not true for the non-member
instances as shown in Figure 7b.
Intuitively, this is because applying the gradient ascent
algorithm on a member instance will trigger the target model
to try to minimize its loss by descending in the direction
of the model’s gradient for those instances (and therefore
nullify the effect of the attacker’s ascent). For target non -
member instances, however, the model will not explicitly
change their gradient, as they do not inﬂuence the training
loss function. The attacker repeats gradient ascend algori thm
for each epoch of the training, therefore, the gradient of th e
12
0 20 40 60 80
0
1
2
3
4
5
Epoch
Gradient norm
T arget members
Non-target members
(a)
0 20 40 60 80
0
1
2
3
4
5
Epoch
Gradient norm
T arget non-members
Non-target non-members
(b)
0 20 40 60 80
0
1
2
3
4
5
Epoch
Gradient norm
T arget members
T arget non-members
(c)
Fig. 7: The impact of the global active gradient ascent attac k on the target model’s training process. Figures show the gr adient
norms of various instances (Purchase100 dataset) during th e training phase, while the target instances are under attac k.
model will keep increasing on such non-member instances.
Figure 7c depicts the resulted distinction between the grad ient
norm of the member and non-member target instances. The
active gradient ascend attacker forces the target model to
behave drastically different between target member and tar get
non-member instances which makes the membership inference
attack easier. As a result, compared to the passive global
attacker we see that the active attack can noticeably gain hi gher
accuracy. In the local case, the accuracy is lower than the
global attack due to the observation of aggregated paramete rs
from multiple participants.
The Isolating Attacker: The parameter aggregation in the
federated learning scenario negatively inﬂuences the accu racy
of the membership inference attacks. An active global attac ker
can overcome this problem by isolating a target participant ,
and creating a local view of the network for it. In this
scenario, the attacker does not send the aggregate paramete rs
of all parties to the target party. Instead, the attacker iso lates
the target participant and segregates the target participa nt’s
learning process.
When the attacker isolates the target participant, then the
target participant’s model does not get aggregated with the
parameters of other parties. As a result, it stores more info r-
mation about its training dataset in the model. Thus, simply
isolating the training of a target model signiﬁcantly incre ases
the attack accuracy . W e can apply the isolating method to
the gradient ascent attacker and further improve the attack er
accuracy. See T able X for all the results.
V . R E L AT E D WO RK
Investigating different inference attacks on deep neural
networks is an active area of research.
A. Membership Inference Attacks
Multiple research papers have studied membership inferenc e
attacks in a black-box setting [6], [16], [7]. Homer et al. [4 ]
performed one of the ﬁrst membership inference attacks on
genomic data. Shokri et al. [6] showed that an ML model’s
output has distinguishable properties about its training d ata,
which could be exploited by the adversary’s inference model .
They introduced shadow models that mimic the behavior of
the target model, which are used by the attacker to train
the attack model. Salem et al. [17] extended the attacks of
Shokri et al. [6] and showed empirically that it is possible
to use a single shadow model (instead of several shadow
models used in [6]) to perform the same attack. They further
demonstrated that even if the attacker does not have access
to the target model’s training data, she can use statistical
properties of outputs (e.g., entropy) to perform membershi p in-
ference. Y eom et al. [7] demonstrated the relationship betw een
overﬁtting and membership inference attacks. Hayes et al. [ 18]
used generative adversarial networks to perform membershi p
attacks on generative models.
Melis et al. [19] developed a new set of membership
inference attacks for the collaborative learning. The atta ck
assumes that the participants update the central server aft er
each mini-batch, as opposed to updating after each training
epoch [20], [21]. Also, the proposed membership inference
attack is designed exclusively for models that use explicit
word embeddings (which reveal the set of words used in the
training sentences in a mini-batch) with very small trainin g
mini-batches.
In this paper, we evaluate standard learning mechanisms
for deep learning and standard target models for various
architectures. W e showed that our attacks work even if we
use pre-trained, state-of-the-art target models.
Differential privacy [22], [23] has been used as a strong
defense mechanism against inference attacks in the context of
machine learning [24], [25], [26], [27]. Several works [28] ,
[29], [30] have shown that by using adversarial training, on e
can ﬁnd a better trade-off between privacy and model accurac y.
However, the focus of this line of work is on the membership
inference attack in the black-box setting.
B. Other Inference Attacks
An attacker with additional information about the training
data distribution can perform various types of inference at -
tacks. Input inference [31], attribute inference [32], par ameter
13
inference [33], [34], and side-channel attacks [35] are sev eral
examples of such attacks. Ateniese et al. [36] show that an
adversary with access to the parameters of machine learning
models such as Support V ector Machines (SVM) or Hidden
Markov Models (HMM) [37] can extract valuable information
about the training data (e.g., the accent of the speakers in
speech recognition models).
VI. C O N CL U S IO N S
W e designed and evaluated novel white-box membership
inference attacks against neural network models by exploit ing
the privacy vulnerabilities of the stochastic gradient des cent
algorithm. W e demonstrated our attacks in the stand-alone a nd
federated settings, with respect to passive and active infe rence
attackers, and assuming different adversary prior knowled ge.
W e showed that even well-generalized models are signiﬁcant ly
susceptible to such white-box membership inference attack s.
Our work did not investigate theoretical bounds on the priva cy
leakage of deep learning in the white-box setting, which wou ld
remain as a topic of future research.
ACK N OW L E D G M E N T S
This work was supported in part by the NSF grant CNS-
1525642, as well as the Singapore Ministry of Education
Academic Research Fund Tier 1, R-252-000-660-133. Reza
Shokri would like to acknowledge the support of NVIDIA
Corporation with the donation of a Titan Xp GPU which was
used for this research.
RE F E RE N CE S
[1] C. Dwork, A. Smith, T . Steinke, and J. Ullman, “Exposed! a survey of
attacks on private data, ” 2017.
[2] I. Dinur and K. Nissim, “Revealing information while pre serving pri-
vacy , ” in Proceedings of the twenty-second ACM SIGMOD-SIGACT -
SIGART symposium on Principles of database systems . ACM, 2003,
pp. 202–210.
[3] R. W ang, Y . F . Li, X. W ang, H. T ang, and X. Zhou, “Learning y our
identity and disease from research papers: information lea ks in genome
wide association study , ” in Proceedings of the 16th ACM conference on
Computer and communications security . ACM, 2009, pp. 534–544.
[4] N. Homer, S. Szelinger, M. Redman, D. Duggan, W . T embe,
J. Muehling, J. V . Pearson, D. A. Stephan, S. F . Nelson, and D. W . Craig,
“Resolving individuals contributing trace amounts of dna t o highly
complex mixtures using high-density snp genotyping microa rrays, ” PLoS
genetics, vol. 4, no. 8, p. e1000167, 2008.
[5] C. Dwork, A. Smith, T . Steinke, J. Ullman, and S. V adhan, “ Robust
traceability from trace amounts, ” in F oundations of Computer Science
(FOCS), 2015 IEEE 56th Annual Symposium on . IEEE, 2015, pp.
650–669.
[6] R. Shokri, M. Stronati, C. Song, and V . Shmatikov , “Membe rship
inference attacks against machine learning models, ” in Security and
Privacy (SP), 2017 IEEE Symposium on , 2017.
[7] S. Y eom, I. Giacomelli, M. Fredrikson, and S. Jha, “Priva cy risk in
machine learning: Analyzing the connection to overﬁtting, ” in IEEE
Computer Security F oundations Symposium , 2018.
[8] J. Koneˇ cn ` y, H. B. McMahan, F . X. Y u, P . Richt´ arik, A. T . Suresh, and
D. Bacon, “Federated learning: Strategies for improving co mmunication
efﬁciency , ” arXiv preprint arXiv:1610.05492 , 2016.
[9] A. Krizhevsky , “Learning multiple layers of features fr om tiny images, ”
Citeseer, T ech. Rep., 2009.
[10] A. Krizhevsky , I. Sutskever, and G. E. Hinton, “Imagene t classiﬁcation
with deep convolutional neural networks, ” in Advances in neural infor-
mation processing systems , 2012, pp. 1097–1105.
[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learni ng for image
recognition, ” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778.
[12] G. Huang, Z. Liu, L. V an Der Maaten, and K. Q. W einberger, “Densely
connected convolutional networks. ” in CVPR, vol. 1, no. 2, 2017, p. 3.
[13] I. Goodfellow , Y . Bengio, and A. Courville, Deep learning , 2016, vol. 1.
[14] U. V on Luxburg, “ A tutorial on spectral clustering, ” Statistics and
computing, vol. 17, no. 4, pp. 395–416, 2007.
[15] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. V inyals, “ Understand-
ing deep learning requires rethinking generalization, ” arXiv preprint
arXiv:1611.03530, 2016.
[16] Y . Long, V . Bindschaedler, L. W ang, D. Bu, X. W ang, H. T an g, C. A.
Gunter, and K. Chen, “Understanding membership inferences on well-
generalized learning models, ” arXiv preprint arXiv:1802.04889 , 2018.
[17] A. Salem, Y . Zhang, M. Humbert, M. Fritz, and M. Backes, “ Ml-leaks:
Model and data independent membership inference attacks an d defenses
on machine learning models, ” arXiv preprint arXiv:1806.01246 , 2018.
[18] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Lo gan: evalu-
ating privacy leakage of generative models using generativ e adversarial
networks, ” arXiv preprint arXiv:1705.07663 , 2017.
[19] L. Melis, C. Song, E. De Cristofaro, and V . Shmatikov , “E xploiting
unintended feature leakage in collaborative learning, ” arXiv preprint
arXiv:1805.04049, 2018.
[20] R. Shokri and V . Shmatikov , “Privacy-preserving deep l earning, ” in
Proceedings of the 22nd ACM SIGSAC conference on computer an d
communications security . ACM, 2015, pp. 1310–1321.
[21] H. B. McMahan, E. Moore, D. Ramage, S. Hampson et al. ,
“Communication-efﬁcient learning of deep networks from de centralized
data, ” arXiv preprint arXiv:1602.05629 , 2016.
[22] C. Dwork, F . McSherry , K. Nissim, and A. Smith, “Calibra ting noise
to sensitivity in private data analysis, ” in Theory of Cryptography
Conference. Springer, 2006, pp. 265–284.
[23] C. Dwork, A. Roth et al. , “The algorithmic foundations of differential
privacy , ” F oundations and Trends R⃝ in Theoretical Computer Science ,
vol. 9, no. 3–4, pp. 211–407, 2014.
[24] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differ entially private
empirical risk minimization, ” Journal of Machine Learning Research ,
vol. 12, no. Mar, pp. 1069–1109, 2011.
[25] R. Bassily , A. Smith, and A. Thakurta, “Private empiric al risk mini-
mization: Efﬁcient algorithms and tight error bounds, ” in F oundations
of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on.
IEEE, 2014, pp. 464–473.
[26] M. Abadi, A. Chu, I. Goodfellow , H. B. McMahan, I. Mirono v ,
K. T alwar, and L. Zhang, “Deep learning with differential pr ivacy , ” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer an d
Communications Security . ACM, 2016, pp. 308–318.
[27] N. Papernot, S. Song, I. Mironov , A. Raghunathan, K. T al war, and
´U. Erlingsson, “Scalable private learning with pate, ” arXiv preprint
arXiv:1802.08908, 2018.
[28] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learnin g with
membership privacy using adversarial regularization, ” in Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communicatio ns
Security. ACM, 2018, pp. 634–646.
[29] J. Hamm, “Minimax ﬁlter: learning to preserve privacy f rom inference
attacks, ” The Journal of Machine Learning Research , vol. 18, no. 1, pp.
4704–4734, 2017.
[30] C. Huang, P . Kairouz, X. Chen, L. Sankar, and R. Rajagopa l, “Generative
adversarial privacy , ” arXiv preprint arXiv:1807.05306 , 2018.
[31] M. Fredrikson, S. Jha, and T . Ristenpart, “Model invers ion attacks
that exploit conﬁdence information and basic countermeasu res, ” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer an d
Communications Security . ACM, 2015, pp. 1322–1333.
[32] N. Carlini, C. Liu, J. Kos, ´U. Erlingsson, and D. Song, “The secret
sharer: Measuring unintended neural network memorization & extracting
secrets, ” arXiv preprint arXiv:1802.08232 , 2018.
[33] F . Tram ` er, F . Zhang, A. Juels, M. K. Reiter, and T . Riste npart, “Stealing
machine learning models via prediction apis, ” in USENIX Security , 2016.
[34] B. W ang and N. Z. Gong, “Stealing hyperparameters in mac hine
learning, ” arXiv preprint arXiv:1802.05351 , 2018.
[35] L. W ei, Y . Liu, B. Luo, Y . Li, and Q. Xu, “I know what you see : Power
side-channel attack on convolutional neural network accel erators, ” arXiv
preprint arXiv:1803.05847 , 2018.
[36] G. Ateniese, L. V . Mancini, A. Spognardi, A. V illani, D. V itali, and
G. Felici, “Hacking smart machines with smarter ones: How to extract
14
meaningful data from machine learning classiﬁers, ” International Jour-
nal of Security and Networks , vol. 10, no. 3, pp. 137–150, 2015.
[37] C. Robert, Machine learning, a probabilistic perspective . T aylor &
Francis, 2014.
AP P E N D IX A
ARCH IT E CT U RE O F T H E AT TACK MO D E L
T ABLE XIV: Attack model layer sizes
Name Layers Details
Output Component 2 Fully Connected Layers
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Label Component 2 Fully Connected Layers
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Loss Component 2 Fully Connected Layers
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Gradient Component
Convolutional Layer
Kernels: 1000
Kernel size: 1× Next layer
Stride:1
Dropout: 0.2
2 Fully Connected Layers
Sizes: 128, 64
Activation: ReLU
Dropout: 0.2
Encoder Component 4 Fully Connected Layers
Sizes: 256, 128, 64, 1
Activation: ReLU
Dropout: 0.2
Decoder Component 2 Fully Connected Layers
Sizes: 64, 4
Activation: ReLU
Dropout: 0.2
15