0202
nuJ
6
]LM.tats[
2v01900.2181:viXra
Comprehensive Privacy Analysis of Deep Learning:
Passive and Active White-box Inference Attacks
against Centralized and Federated Learning
Milad Nasr Reza Shokri Amir Houmansadr
University of Massachusetts Amherst National University of Singapore University of Massachusetts Amherst
milad@cs.umass.edu reza@comp.nus.edu.sg amir@cs.umass.edu
Abstract—Deep neural networks are susceptible to various we can learn from the model about the data population, and
inference attacks as they remember information about their the information that the model leaks about the particular data
trainingdata. Wedesign white-boxinferenceattacks to perform samples which are in its training set. The former indicates
a comprehensive privacy analysis of deep learning models. We
utility gain, and the later reflects privacy loss. We design
measuretheprivacyleakagethroughparametersoffullytrained
models as well as the parameter updates of models during inference attacks to quantify such privacy leakage.
training.Wedesigninferencealgorithmsforbothcentralizedand Inference attacks on machine learning algorithms fall into
federated learning, with respect to passive and active inference
two fundamental and related categories: tracing (a.k.a. mem-
attackers, and assuming different adversary prior knowledge.
bership inference) attacks, and reconstruction attacks [1]. In
Weevaluate our novel white-boxmembership inferenceattacks
against deep learning algorithms to trace their training data a reconstruction attack, the attacker’s objective is to infer
records.Weshowthatastraightforward extensionof theknown attributes of the records in the training set [2], [3]. In a
black-boxattackstothewhite-boxsetting(throughanalyzingthe membershipinferenceattack,however,theattacker’sobjective
outputsofactivationfunctions)isineffective.Wethereforedesign
istoinferifaparticularindividualdatarecordwasincludedin
new algorithms tailored to the white-box setting by exploiting
the training dataset [4], [5], [6]. This is a decisional problem,
the privacy vulnerabilities of the stochastic gradient descent
algorithm, which is the algorithm used to train deep neural anditsaccuracydirectlydemonstratestheleakageofthemodel
networks. We investigate the reasons why deep learning models aboutits trainingdata. We thuschoosethis attack asthe basis
may leak information about their training data. We then show for our privacy analysis of deep learning models.
that even well-generalized models are significantly susceptible
to white-box membership inference attacks, by analyzing state- Recent works have studied membership inference attacks
of-the-art pre-trained and publicly available models for the against machine learning models in the black-box setting,
CIFAR dataset. We also show how adversarial participants, wheretheattackercanonlyobservethemodelpredictions[6],
in the federated learning setting, can successfully run active
[7]. The results of these works show that the distribution of
membership inference attacks against other participants, even
the training data as well as the generalizability of the model
when the global model achieves high prediction accuracies.
significantly contribute to the membership leakage. Particu-
I. INTRODUCTION larly,theyshowthatoverfittedmodelsaremoresusceptibleto
membership inference attacks than generalized models. Such
Deep neural networks have shown unprecedented gener-
black-boxattacks,however,mightnotbeeffectiveagainstdeep
alization for various learning tasks, from image and speech
neural networks that generalize well (having a large set of
recognition to generating realistic-looking data. This success
parameters). Additionally, in a variety of real-world settings,
has led to manyapplicationsand servicesthat use deep learn-
the parameters of deep learning algorithms are visible to the
ing algorithms on large-dimension (and potentially sensitive)
adversaries,e.g.,inafederatedlearningsettingwheremultiple
user data, including user speeches, images, medical records,
dataholderscollaboratetotrainaglobalmodelbysharingtheir
financial data, social relationships, and location data points.
parameter updates with each other through an aggregator.
In this paper, we are interested in answering the following
critical question: What is the privacy risk of deep learning Ourcontributions. Inthispaper,wepresentacomprehensive
algorithmsto individualswhose data is used for trainingdeep framework for the privacy analysis of deep neural networks,
neuralnetworks?Inotherwords,howmuchistheinformation usingwhite-boxmembershipinferenceattacks.Wegobeyond
leakage of deep learning algorithms about their individual membership inference attacks against fully-trained models.
training data samples? We take all major scenarios where deep learning is used
We define privacy-sensitive leakage of a model, about its for training and fine-tuning or updating models, with one
training data, as the information that an adversary can learn or multiple collaborative data holders, when attacker only
fromthemodelaboutthem,whichheisnotabletoinferfrom passively observes the model updates or actively influences
other models that are trained on other data from the same the target model in order to extract more information, and
distribution. This distinguishes between the information that for attackers with different types of prior knowledge. Despite
1
differences in knowledge, observation, and actions of the vulnerable to white-box membership inference attacks.
adversary, their objective is the same: membership inference. In federated learning, we show that a curious parameter
A simple extension of existing black-box membership in- server or even a participant can perform alarmingly accurate
ference attacks to the white-box setting would be using the membership inference attacks against other participants. For
same attack on all of the activation functions of the model. the DenseNet model on CIFAR100, a local participant can
Our empirical evaluations show that this will not result in achieve a membership inference accuracy of 72.2%, even
inference accuracy better than that of a black-box attacker. though it only observes aggregate updates through the pa-
This is because the activation functions in the model tend to rameter server. Also, the curious central parameter server
generalizemuchfastercomparedtotheoutputlayer.Theearly can achieve a 79.2% inference accuracy, as it receives the
layers of a trained model extract very simple features that individualparameterupdatesfromallparticipants.Infederated
are not specific to the training data. The activation functions learning, the repeated parameter updates of the models over
in the last layers extract complex and abstract features, thus different epochs on the same underlying training set is a key
should contain more information about the model’s training factor in boosting the inference attack accuracy.
set. However, this information is more or less the same as Asthecontributions(i.e.,parameterupdates)ofanadversar-
what the output leaks about the training data. ialparticipantcaninfluencetheparametersotherparties,inthe
We design white-box inference attacks that exploit the federatedlearningsetting,the adversarycanactivelypush
privacy vulnerabilities of the stochastic gradient descent SGDtoleakevenmoreinformationabouttheparticipants’
(SGD) algorithm. Each data point in the training set in- data.Wedesignanactiveattackthatperformsgradientascent
fluences many of the model parameters, through the SGD on a set of target data points before uploading and updating
algorithm,tominimizeits contributiontothe model’straining the global parameters. This magnifies the presence of data
loss. The local gradient of the loss on a target data record, points in others’ training sets, in the way SGD reacts by
with respect to a given parameter, indicates how much and abruptly reducing the gradient on the target data points if
in which direction the parameter needs to be changed to fit they are members. On the Densenet model, this leads to a
the model to the data record. To minimize the expected loss 76.7% inference accuracy for an adversarial participant, and
of the model, the SGD algorithm repeatedly updates model a significant 82.1%accuracyfor an active inferenceattack by
parametersin a directionthat the gradientof the loss overthe the central server. By isolating a participant during parameter
whole trainingdataset leans to zero. Therefore,each training updates, the central attacker can boost his accuracyto 87.3%.
data sample will leave a distinguishable footprint on the
gradientsofthelossfunctionoverthemodel’sparameters. II. INFERENCEATTACKS
Weusethegradientvectorofthemodel,overallparameters,
We use membership inference attacks to measure the in-
onthetargetdatapoint,asthemainfeaturefortheattack.We
formation leakage through deep learning models about their
design deep learning attack models with an architecture that
trainingdata.Therearemanydifferentscenariosinwhichdata
processesextracted(gradient)featuresfromdifferentlayersof
isusedfortrainingmodels,andtherearemanydifferentways
thetargetmodelseparately,andcombinestheirinformationto
the attacker canobservethe deep learningprocess. In Table I,
compute the membership probability of a target data point.
we cover the major criteria to categorize the attacks. This
We train the attack model for attackers with different types
includesattack observations, assumptionsaboutthe adversary
of background knowledge. Assuming a subset of the training
knowledge,the targettraining algorithm,and the mode of the
set is known to the attacker, we can train the attack model
attack based on the adversary’s actions. In this section, we
in a supervisedmanner. However,for the adversarythat lacks
discuss different attack scenarios as well as the techniques
thisknowledge,wetraintheattackmodelinanunsupervised
we use to exploit deep learning algorithms. We also describe
manner. We train auto-encoders to compute a membership
the architecture of our attack model, and how the adversary
informationembeddingfor anydata.We thenuse a clustering
computes the membership probability.
algorithm, on the target dataset, to separate members from
non-members based on their membership embedding.
A. Attack Observations: Black-box vs. White-box Inference
To show the effectiveness of our white-box inference at-
tack, we evaluate the privacy of pre-trained and publicly Theadversary’sobservationsofthedeeplearningalgorithm
available state-of-the-art models on the CIFAR100 dataset. are what constitute the inputs for the inference attack.
We had no influence on training these models. Our results Black-box. In thissetting, the adversary’sobservationis lim-
show that the DenseNet model—which is the best model on itedtotheoutputofthemodelonarbitraryinputs.Foranydata
CIFAR100 with 82% test accuracy—is not much vulnerable pointx,theattackercanonlyobtainf(x;W).Theparameters
to black-box attacks (with a 54.5% inference attack accuracy, ofthemodelWandtheintermediatestepsofthecomputation
where 50% is the baseline for random guess). However, our arenotaccessibletotheattacker.Thisisthesettingofmachine
white-box membership inference attack obtains a consider- learningas a service platforms.Membershipinferenceattacks
ably higher accuracy of 74.3%. This shows that even well- against black-boxmodels are already designed, which exploit
generalized deep models might leak significant amount the statistical differencesbetweena model’spredictionson its
of information about their training data, and could be training set versus unseen data [6].
2
Criteria Attacks Description
Black-box The attacker can obtain the prediction vector f(x) on arbitrary input x, but cannot access the model parameters, nor
Observation theintermediate computations off(x).
x f f(x)
White-box Theattackerhasaccesstothefullmodelf(x;W),notablyitsarchitectureandparametersW,andanyhyper-parameter
thatisneededtousethemodelforpredictions.Thus,hecanalsoobservetheintermediatecomputationsathiddenlayers
hi(x).
x W1 h1(x) W2 h2(x) ··· W i f(x)
Stand-alone Theattackerobservesthefinaltargetmodelf,afterthetrainingisdone(e.g.,inacentralizedmanner)usingdatasetD.
Target
Hemightalsoobservetheupdatedmodelf∆ afterithasbeenupdated (fine-tuned)usinganewdataset D∆.
fine-tune
f f∆
Dx D∆
Federated Theattackercouldbethecentralaggregator,whoobservesindividualupdatesovertimeandcancontroltheviewofthe
participants on the global parameters. He could also be any of the participants who can observe the global parameter
updates, andcancontrolhisparameteruploads.
Aggregator (globalparameters W)
down=W{t}
up=W{t}
i
f(x;W
1
{t}) f(x;W
2
{t}) f(x;W
N
{t})
···
Dx1 D2 DN
Passive Theattacker canonlyobservethegenuine computations bythetraining algorithm andthemodel.
Mode
Active Theattackercouldbeoneoftheparticipantsinthefederatedlearning,whoadversariallymodifieshisparameteruploads
W{t},orcould bethe central aggregator whoadversarially modifies theaggregate parameters W{t} whichhesends
i
tothetarget participant(s).
Supervised Theattacker hasadatasetD′,whichcontains asubsetofthetargetsetD,aswellassomedatapointsfromthesame
Knowledge
underlying distribution as D that are not in D. The attacker trains an inference model h in a supervised manner, by
minimizing the empirical loss function Pd∈D′(1−
1
d∈D)h(d)+
1
d∈D(1−h(d)), where the inference model h
computes the membership probability of any data point d in the training set of a given target model f, i.e., h(d) =
Pr(d∈D;f).
DataUniverse
∼Pr( X= x) D D′
∼Pr(X
=x)
D′
Unsupervised Theattacker has data points that are sampledfrom thesameunderlying distribution asD.However, hedoes nothave
information aboutwhetheradatasamplehasbeeninthetargetsetD.
TABLE I: Various categories of inference attacks against machine learning models, based on their prior knowledge, observation, mode of
attack, and the training architecture of the target models.
3
x W W ··· W 1 2 i
target model
y,))W;x(f(L
y
hh11 (( xx)) hh22 (( xx)) ··· ff(( xx))
∂∂
∂ W∂
W
LL
11 ∂∂
∂ W∂
W
LL
22
···
∂∂
∂ W∂
W
LL
ii
LL
ledom
kcatta
serutaef
kcatta
unsupervisedattack component
L
Decoder(FCN)
attack output:
Encoder(FCN)
···
CNN FCN CNN FCN CNN FCN FCN FCN
1
White-box. In this setting, the attacker obtains the model
f(x;W) including its parameters which are needed for pre-
y=argmaxf(x) f(x) y H(f(x)) (cid:13)∂ ∂ W L (cid:13) diction. Thus, for any input x, in addition to its output, the (cid:13) (cid:13) attacker can compute all the intermediate computations of
the model. That is, the adversary can compute any function
over W and x given the model. The most straightforward
functions are the outputs of the hidden layers, h (x) on the
i
inputx. As a simple extension,the attacker can extendblack-
box membership inference attacks (which are limited to the
model’s output) to the outputs of all activation functions of
the model. However, this does not necessarily contain all the
useful information for membership inference. Notably, the
model output and activation functions could generalize if the
model is well regularized. Thus, there might not be much
difference,indistribution,betweentheactivationfunctionsofa
modelonitstrainingversusunseendata.Thiscansignificantly
limit the power of the inference attacks (as we also show in
our evaluations).
What we suggest is to exploit the algorithm used to train
deep learning models: the stochastic gradient descent (SGD)
algorithm. Let L(f(x;W),y) be the loss function for the
classificationmodelf.Duringthetraining,theSGDalgorithm
minimizes the empirical expectation of the loss function over
the training set D:
minE (x,y)∼D(cid:2) L(f(x;W),y) (cid:3) (1)
W
The SGD algorithm solves this minimization by repeatedly
updating parameters, W, towards reducing the loss on small
randomly selected subsets of D. Thus, for any data record in
the training dataset, the gradientof the loss ∂L overthe data
∂W
record is pushed towards zero, after each round of training.
This is exactly what we can exploit to extract information
Fig. 1: The architecture of our white-box inference attack. Given
about a model’s training data.
target data (x,y), the objective of the attack is to determine its
For a target data record (x,y), the adversary can compute
membership in the training set D of target model f. The attacker
runs the target model f on the target input x, and computes all the the loss of the model L(f(x;W),y), and can compute the
hidden layers hi(x), the model’s output f(x), and the loss function gradients of the loss with respect to all parameters
∂
∂
W
L
L(f(x),y;W), in a forward pass. The attacker also computes the using a simple back-propagation algorithm. Given the large
gradient of the loss with respect to the parameters of each layer
number of parameters used in deep neural networks (millions
∂L ,inabackwardpass.Thesecomputations,inadditiontotheone-
∂Wi of parameters), the vector with such a significantly large
hot encoding of the true label y, construct the input features of the
inference attack. The attack model consists of convolutional neural dimension cannot properly generalize over the training data
network (CNN) components and fully connected network (FCN) (which in many cases is an order of magnitude smaller in
components. For attacking federated learning and fine-tuning, the size). Therefore, the distribution of the model’s gradients on
attackerobserveseachattackfeatureT times,andstacksthembefore
members of its training data, versus non-members, is likely
theyarepassedtothecorrespondingattackcomponent.Forexample,
the loss features are composed as L = {L{1},L{2},··· ,L{T}}). to be distinguishable. This can help the adversary to run
TheoutputsoftheCNNandFCNcomponentsareappendedtogether, an accurate membership inference attack, even though the
and this vector is passed to a fully connected encoder. The output classification model (with respect to its predictions) is well-
of the encoder, which is a single value, is the attack output. This generalized.
represents an embedding of the membership information in a single
Inference model. We illustrate the membership inference
value. In the supervised attack setting, this embedding is trained to
bePr{(x,y)∈D}.Intheunsupervised setting,adecoderistrained attack in Figure 1. The significance of gradient (as well as
to reconstruct important features of the attack input (such as the activation) computations for a membership inference attack
model’s output uncertainty H(f(x)), and the norm of its gradients variesoverthelayersofadeepneuralnetwork.Thefirstlayers
(cid:13) (cid:13)∂ ∂ W L(cid:13) (cid:13))fromtheattackoutput.Thisissimilartodeepauto-encoders. tend to contain less informationabout the specific data points
All unspecified attack layers are fully connected. The details of the
in thetrainingset, comparedtonon-memberdatarecordfrom
architecture of the attack is presented in Table XIV in Appendix A.
thesameunderlyingdistribution.Wecanprovidethegradients
andactivationsofeachlayerasseparateinputstotheattacker,
4
as the attacker mightneed to design a specific attack for each very relevant in numerous cases where the original model is
layer. This enables the inference attack to split the inference trained using some unlabeled (and perhaps public) data, and
task across different layers of the model, and then combine then it is fine-tunned using sensitive private labeled data.
them to determine the membership. This engineering of the The modelfor inferenceattacks againstfine-tunnedmodels
attack modelarchitectureempowersthe inferenceattack, as it is a special case of our membership inference model for at-
reducesthecapacityof theattack modelandhelpsfindingthe tackingfederatedlearning.Inbothcases,theattackerobserves
optimal attack algorithm with less background data. multiple versions of the target model.
The distinct inputs to the attack model are the set of Federated learning. Inthis setting, N participants,whohave
d g i r f a f d e i r e e n n t t s la ∂ y ∂ W e L r 1 s , h ∂ 1 ∂ W ( L x 2 ) , , · h · 2 · ( , x t ) h , e ··· se , t th o e f m a o c d ti e v l at o i u o t n pu v t e f ct ( o x r ) s , t f h o e r a d n if d fe m re o n d t e t l ra a i r n c i h n i g te s c e tu ts re D to i , t a ra g i r n ee a o g n lo a ba s l in m g o le de d l e . e A p c l e e n ar tr n a i l n s g e t r a v s e k r
one-hotencodingof the label y, and the loss of the modelon keeps the latest version of the parameters W for the global
the target data L(f(x;W),y). Each of these are separately model. Each participant has a local model, hence a local set
fed into the attack model, and are analyzed separately using of parameters W . In each epoch of training, each participant
i
independent components. downloads the global parameters, updates them locally using
Inference attackcomponents. Theattackmodeliscomposed SGD algorithm on their local training data, and uploads them
of feature extraction components and an encoder component. backtotheserver.Theparameterservercomputestheaverage
To extract features from the output of each layer, plus the value for each parameter using the uploaded parameters by
one-hot encoding of the true label and the loss, we use fully all participants. This collaborative training continues until the
connected network (FCN) submodules with one hidden layer. global model converges.
We use convolutional neural network (CNN) submodules for There are two possibilities for the position of the attacker
the gradients. When the gradients are computed on fully infederatedlearning:Theadversarycanbethecentralizedpa-
connected layers (in the target model), we set the size of the rameterserver,oroneoftheparticipants.Acuriousparameter
convolutional kernel to the input size of the fully connected server can receive updates from each individual participant
layer, to capture the correlation of the gradients in each over time W{t}, and use them to infer information about
i
activation function.We reshape the outputof each submodule the training set of each participant. A malicious parameter
component into a flat vector, and then concatenate the output server can also control the view of each participant on the
of all components. We combine the outputs of all attack globalmodel,andcanactactivelytoextractmoreinformation
featureextractioncomponentsusingafullyconnectedencoder about the training set of a participant (as we discuss under
component with multiple hidden layers. The output of the active attacks). Alternatively, the adversary can be one of
encoder is a single score, which is the output of the attack. the participants. An adversarial participant can only observe
This score (in the supervised attack raining) predicts the the global parameters over time W{t}, and craft his own
membership probability of the input data. adversarialparameter updatesW{t} to gain more information
i
about the union of the training data of all other participants.
B. Inference Target: Stand-alone vs. Federated Learning
In either of these cases, the adversary observes multiple
There are two major types of training algorithms for deep
versions of the target model over time. The adversary can try
learning, depending on whether the training data is available
torunanindependentmembershipinferenceattackoneachof
all in one place (i.e., stand-alone centralized training), or it
these models, and then combine their results. This, however,
is distributed among multiple parties who do not trust each
mightnotcapturethe dependenciesbetweenparametervalues
other (i.e., federated learning) [8]. In both cases, the attacker
overtime, whichcanleak informationaboutthetrainingdata.
could be the entity who obtains the final trained model. In
Instead,inourdesignwemakeuseofasingleinferencemodel,
addition to such attack setting, the attacker might observe an
where each attack component (e.g., componentsfor gradients
updated version of the model after fine-tuning, for instance,
of layer i) processes all of its corresponding inputs over the
which is very common in deep learning. Besides, in the
observed models at once. This is illustrated in Figure 1. For
case of federated learning, the attacker can be an entity who
example,fortheattackcomponentthatanalyzesthelossvalue
participates in the training. The settings of fine-tunning and
L,theinputdimensioncanbe1×T,iftheadversaryobserves
federated learning are depicted in Table I.
T versions of the target model over time. The output of the
Stand-alone fine-tunning. A model f is trained on dataset attack component is also T times larger than the case of
D. At a later stage it is updated to f after being fine-tuned
∆ attackingastand-alonemodel.Thesecorrelatedoutputs,ofall
using a new dataset D ∆ . If the attacker observes the final attack components, are processed all at once by the inference
outcome, we want to measure the information leakage of the model.
finalmodelf aboutthewholetrainingsetD∪D .However,
∆ ∆
C. Attack Mode: Passive vs. Active Inference Attack
given that two versions of the model exist (before and after
fine-tuning), we are also interested in measuring the extra The inference attacks are mostly passive, where the ad-
informationthatcouldbelearnedaboutthetrainingdata,from versary makes observations without modifying the learning
thetwomodelsnapshots.Theattackermightalsobeinterested process. This is the case notably for attacking models after
only in recoveringinformationaboutthe new set D . This is the training is over, e.g., the stand-alone setting.
∆
5
Active attacks. The adversary, who is participating in the forpredictingthemembershipofthedatapointsinitstraining
training process, can actively influence the target model in set D′:
order to extract more information about its training set. This
X (h(d)−1)2+ X (h(d))2 (3)
couldbethecasenotablyforrunninginferenceattacksagainst
federatedlearning.Inthissetting,thecentralparameterserver
d∈D′∩D d∈D′\D
or a curious participant can craft adversarial parameter up- If the adversary does not have known samples from the
dates for a follow-up inference attack. The inference model target training set, there are two possibilities for training
architecture will be the same for passive and active attacks. the inference attack models: supervised training on shadow
models [6], and unsupervised training on the target model.
TheactiveattackercanexploittheSGDalgorithmtorunthe
Shadow models are models with the same architecture as the
active attack. The insight we use to design our attack is that
targetmodel.Thetrainingdatarecordsfortheshadowmodels
theSGDalgorithmforcefullydecreasesthegradientoftheloss
are generatedfromthe same distribution as the targettraining
on the training data, with the hope that this generalizesto the
data, butdo nothavea knownoverlapwith the targettraining
test data as well. The amount of the changes depends on the
set.Theattackertrainstheattackmodelontheshadowmodels.
contribution of a data point in the loss. So, if a training data
Asthebehavioroftheshadowmodelsontheirtrainingdatais
point leads to a large loss, the SGD algorithm will influence
more or less similar to the behaviorof the targetmodelon its
someparameterstoadaptthemselvestowardsreducingtheloss
trainingdata,the attack modelstrainedonthe shadowmodels
onthispoint.Ifthedatapointisnotseenbythemodelduring
are empirically shown to be effective.
training, the changes in the gradient on this point is gradual
The attack output for (shadow) supervised training setting
throughoutthe training. This is what we exploit in our active
is the probability of membership.
membership inference attack.
Let x be a data record, which is targeted by the adversary h(d)=Pr(d∈D;f) (4)
to determine its membership. Let us assume the adversary is
oneof the participants.Theattacker runsa gradientascenton Unsupervised training of inference models. We introduce
x, and updates its local model parameters in the direction of an alternative approach to shadow training, which is unsuper-
increasing the loss on x. This can simply be done by adding vised training of the attack model on the target model. The
the gradient to the parameters, assumption for this attack is that the attacker has access to a
datasetD′ whichpartiallyoverlapswith thetargettrainingset
∂Lx
D, however, the adversary does not know which data points
W←W+γ , (2)
∂W are in D′∩D.
Ourobjectiveis to finda scoreforeachdata pointthatrep-
where γ is the adversarial update rate. The adversary then
resentsitsembeddinginaspace,whichhelpsuseasilyseparat-
uploads the adversarially computed parameters to the central
ingmembersfromnon-members(usingclusteringalgorithms).
server, who will aggregate them with the parameter updates
The attack’s output should compute such representations. We
from other participants. The adversary can run this attack on
make use of an encoder-decoder architecture to achieve this.
a batch of target data points all at the same time.
Thisisverysimilartotheauto-encodersforunsuperviseddeep
If the target record x is in the training set of a participant, learning.As shownin Figure1, theoutputof the attackis fed
its local SGD algorithm abruptly reduces the gradient of the intoa decoder.Thedecoderis afullyconnectednetworkwith
loss on x. This can be detected by the inference model, and one hidden layer.
be used to distinguishmembersfromnon-members.Repeated The objective of the decoder is to reconstruct some key
active attacks, which happens in federated learning, lead to features of the attack input which are important for member-
high confidence inference attacks. ship inference. These include the loss value L, whether the
target model has predicted the correct label
D. Prior Knowledge: Supervised vs. Unsupervised Inference
Toconstructhisinferenceattackmodel,theadversaryneeds
tofindthe meaningfulmappingbetweenthe model’sbehavior
on a data point and its membership in the training set. The
most straightforward way of learning such relationship is
through some known members of the training data, and some
data points from the same distribution which are not in the
training data set. This is illustrated in Table I. The adversary
hasadatasetD′ thatoverlapswiththetargetdatasetD.Given
thisdataset,hecantraintheattackmodelinasupervisedway,
and use it to attack the rest of the training dataset.
Let h be the inference attack model. In the supervised
setting, we minimize the (mean square) loss of the attacker
1
y=argmaxf(x) ,
the confidence of the model on the correct label f(x) , the
y
predictionuncertainty(entropy)ofthemodelH(f(x)),andthe
norm of the gradients ∂L . As previous work [6] as well
(cid:13)∂W(cid:13)
asourempiricalresults(cid:13)show(cid:13),thesefeaturesarestrongsignals
for distinguishing membersfrom non-members.The encoder-
decoderarchitecturemaximizestheinformationthattheattack
output contains about these features. Thus, it generates a
membership embedding for each data point. Note that after
training the attack model, the decoder plays no role in the
membership inference attack.
The attack in the unsupervised setting is a batch attack,
wheretheadversaryattacksalargesetofdatarecords(disjoint
from his backgroundknowledge). We will use the encoder to
for each target data record, and we compute the embedding
6
value(outputoftheencodermodel).Next,weuseaclustering layer sizes of 600,1024,512,256,128,100(where 100 is the
algorithm (e.g., we use the spectral clustering method) to output layer), and for Texas100, we used layers with size
cluster each input of the target model in two clusters. Note 1024,512,256,128,100 (where 100 is the output layer). We
that the outcomeof the clustering algorithm is a threshold, as used Adam [13] optimizer with the learning rate of 0.001
the attack output is a single number. We predict the cluster for learning of these models. We trained each model for 100
with the larger gradient norm as non-members. epochs across all of our experiments. We selected the model
with the best testing accuracy across all the 100 epochs.
III. EXPERIMENTALSETUP
C. Pre-trained Models
We implementedour attacks using Pytorch.1 We trained all
of the modelson a PC equippedwith four Titan X GPU each To demonstrate that our attacks are not limited to our
with 12 GB of memory. training algorithm, we used publicly available pre-trained
CIFAR100 models4. All of these models are tuned to get the
A. Datasets besttestingaccuracyusingdifferentregularizationtechniques.
Weusedthreedatasetsinourexperiments:astandardimage
D. Federated Learning
recognition benchmark dataset, CIFAR100, and two datasets
We performed the training for all of the federated learning
Purchase100 and Texas100 [6].
experiments. Specifically, we used the averaging aggregation
CIFAR100. This is a popular benchmark dataset used to
method for the federated scenario [8]. Each training party
evaluate image recognition algorithms [9]. It contains 60,000
sends the parameter updates after every epoch of training to
color (RGB) images, each 32 × 32 pixels. The images are
the centralmodel,andthe centralserveraveragesthe models’
clustered into 100 classes based on objects in the images.
updates from the parties and sends the updated model to all
Purchase100. The Purchase100 dataset contains the shop- parties. In our experiments, we use the same training dataset
ping records of several thousand online customers, extracted size for all parties, and each party’s training data is selected
during Kaggle’s “acquire valued shopper” challenge.2 The uniformly at random from our available datasets.
challenge was designed to identify offers that would attract
E. Attack Models
new shoppers. We used the processed and simplified version
of this dataset (courtesy of the authors of [6]). Each record Table XIV in Appendix A, presents the details of our
in the dataset is the shopping history of a single user. The attack model architecture. As can be seen, we used ReLU
dataset contains 600 different products, and each user has a activation functions, and we initialized the weights using a
binaryrecordwhich indicateswhethershe hasboughteach of normal distribution with mean 0 and standard deviation of
the products (a total of 197,324 data records). The records 0.01. The bias values of all layers are initialized with 0. The
are clustered into 100 classes based on the similarity of the batch size of all experiments is 64. To train the attack model
purchases, and our objective is to identify the class of each weusetheAdamoptimizerwithalearningrateof0.0001.We
user’s purchases. train attack models for 100 epochs and pick the model with
Texas100. This dataset includes hospital discharge data the highest testing accuracy, across all the 100 epochs.
records released by the Texas Department of State Health Tables II and XI present the dataset sizes used for training
Services 3. The recordscontain generic informationabout the the target and attack models. In the supervised setting for
patients(gender,age,andrace),externalcausesofinjury(e.g., training the attack models, we assume the attacker has access
drugmisuse),thediagnosis,andpatientprocedures.Similarto toafractionofthetrainingsetandsomenon-membersamples.
Purchase100, we obtained the processed dataset (Courtesy of In this case, to balance the training, we select half of each
the authors [6]), which contains 67,330 records and 6,170 batch to include member instances and the other half non-
binary features. member instances from the attacker’s backgroundknowledge.
Creating the batches in this fashion will prevent the attack
B. Target Models modelfroma biastowardsmemberornon-memberinstances.
We investigate our attack model on the previously
F. Evaluation Metrics
mentioned three datasets, Texas100, Purchase100 and CI-
FAR100. For the CIFAR100 dataset we used Alexnet [10], Attackaccuracy Theattacker’soutputhastwoclasses“Mem-
ResNet [11], DenseNet [12] models. We used SGD opti- ber” and “Non-member”. Attack accuracy is the fraction of
mizer [13] to train the CIFAR100 models with learning rates the correct membership predictions (predicting members as
of 0.01,0.001,0.0001for epochs 0−50,50−100,100−300 memberandnon-membersas non-member)for unknowndata
accordingly.We used l2regularizationwith weightof 0.0005. points.Thesizeofthesetofmemberandnon-membersamples
For the Texas100 and Purchase100 datasets, we used fully that we use for evaluating the attack are the same.
connected models. For Purchase100, we used a model with True/False positive For a more detailed evaluation of attack
performance, we also measure the true positive and false
1https://pytorch.org/
2https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data 4We make use of ResNet, DenseNet, and Alexnet pre-trained models,
3https://www.dshs.texas.gov/thcic/hospitals/Inpatientpudf.shtm providedinhttps://github.com/bearpaw/pytorch-classification
7
TABLE II: Size of datasets used for training and testing the target attack)alreadycontainsthemembershipinformationthatleaks
classification model and the membership inference model from the output of the previous layers.
TargetModel InferenceAttackModel Impact of gradients: In Section II-A, we discussed why
Training Training Test Test gradientsshouldleakinformationaboutthetrainingdataset.In
Datasets Training Test
Members Non-members Members Non-members TableVIII,wecomparetheaccuracyofthemembershipattack
CIFAR100 50,000 10,000 25,000 5,000 5,000 5,000
when the attacker uses the gradients versus layer outputs,
Texas100 10,000 70,000 5,000 10,000 10,000 10,000
Puchase100 20,000 50,000 10,000 10,000 10,000 10,000 for different dataset and models. Overall, the results show
thatgradientsleaksignificantlymoremembershipinformation
about the training set, compared to the layer outputs.
positive rates of the attacker. Positive is associated with the
We compare the result of the attack on pre-trained
attacker outputting “member”.
CIFAR100-ResNet and CIFAR100-DenseNet models, where
Prediction uncertainty For a classification model, we com- both are designed for the same image recognition task, both
pute its prediction uncertainty using the normalized entropy are trained on the same dataset, and both have similar gener-
of its prediction vector for a given input. alization error. The results show that these two models have
variousdegreesof membershipleakage;this suggests that the
K
1
H= Xp
i
log(p
i
) (5) generalization error is not the right metric to quantify
log(K) privacyleakageinthewhite-boxsetting.Thelargecapacity
i=1
of the model which enables it to learn complex tasks and
where K is the number of all classes and p is the prediction
i
generalizewell,leadstoalsomemorizingmoreinformation
probability for the ith class. We compute the probabilities
using a softmax function as p =
eh(d)(i)
.
about the training data. The total number of the parameters
i PK
k=1
eh(d)(k) inpre-trainedDensenetmodelis25.62M,whereasthisisonly
1.7M parameters for ResNet.
IV. EXPERIMENTS
We also investigated the impact of gradients of different
We start by presenting our results for the stand-alone
layers on attack accuracy. The results are shown in Table IV
scenario, followed by our results for the federated learning
show that the gradient of the later layers leak more
scenario.
membership information. This is similar to our findings for
A. Stand-Alone Setting: Attacking Fully-Trained Models layeroutputs:thelastlayergeneralizestheleastamongallthe
layers in the model, and is the most dependent layer to the
We investigatethecasewheretheattackerhasaccessto the
training set. By combining the gradients of all layers, we are
fully-trainedtargetmodel,inthewhite-boxsetting.Therefore,
able to only slightly increase the attack accuracy.
the attacker can leverage the outputs and the gradients of the
Finally, Table V shows the attack accuracy when we com-
hidden layers of the target model to perform the attack. We
bine the outputlayer and gradientsof differentlayers.We see
have used pre-trained CIFAR100 models, and have trained
thatthegradientsfromthelastlayerleakthemostmembership
other target models and the attack models using the dataset
information.
sizes which are presented in Table II.
Impact of the training size: Table VI shows attack accuracy
Impact of the outputs of different layers: To understand
for various sizes of the attacker’s training data. The models
and demonstrate the impact of different layers’ outputs, we
are tested on the same set of test instances, across all these
perform the attack separately using the outputs of individual
scenarios. As expected, increasing the size of the attacker’s
layers.Weuseapre-trainedAlexnetmodelasthetargetmodel,
training dataset improves the accuracy of the membership
wherethe modelis composedoffive convolutionallayersand
inference attack.
afullyconnectedlayerattheend.TableIIIshowstheaccuracy
of the attack using the output of each of the last three layers. Impact of the gradient norm: In this experiment, we
As the table shows, using the last layers results in the highest demonstrate that the norm of the model’s gradients is highly
attackaccuracy,i.e.,among the layer outputs, the last layer correlated with the accuracy of membership inference, as it
(model output) leaks the most membership information behaves differently for member and non-member instances.
aboutthetrainingdata.Thereasonbehindthisistwofold.By Figure 3 plots the last-layer gradient norms over consecutive
proceeding to the later layers, the capacity of the parameters training epochs for member and non-member instances (for
ramps up, which leads the target model to store unnecessary the Purchase100dataset). As can be seen, duringtraining, the
informationaboutthetrainingdataset,andthereforeleakmore gradientnormsofthememberinstancesdecreaseovertraining
information. Moreover, the first layers extract simple features epochs, which is not the case for non-member instances.
from the input, thus generalize much better compared to the Figure4 showsthe distributionof last-layer gradientnorms
last layers, which are responsible for complex task of finding for members and non-members on three various pretrained
the relationship between abstract features and the classes. We architectures on CIFAR100. Comparing the figures with Ta-
did not achieve significant accuracy gain by combining the ble VIII, we see that a model leaks more membership infor-
outputsfrom multiple layers;this is because the leakage from mation when the distribution of the gradient norm is more
the last layer (which is equivalent to a black-box inference distinct for member and non-member instances. For instance,
8
TABLE III: Attackaccuracy using theoutputs of individual activa-
tion layers. Pre-trained Alexnet on CIFAR100, stand-alone setting.
OutputLayer Attack Accuracy 1500
Lastlayer(prediction vector) 74.6%(black-box)
Secondtolast 74.1% 1000
Thirdtolast 72.9%
Lastthreelayers,combined 74.6%
500
TABLE IV: Attack accuracy when we apply the attack using
parameter gradients of different layers. (CIFAR100 dataset with 0
Alexnet architecture, stand-alone scenario)
0 20 40 60 80 100
Gradient w.r.t. Attack Accuracy Class
Lastlayerparameters 75.1%
Secondtolastlayerparameters 74.6%
Thirdtolastlayerparameters 73.5%
Forthtolastlayerparameters 72.6%
Parameters oflastfourlayers, combined 75.15%
TABLE V: Attack accuracy using different combinations of layer
gradients andoutputs. (CIFAR100dataset withAlexnet architecture,
stand-alone scenario)
Gradients w.r.t. OutputLayers Attack Test Accuracy
LastLayer - 75.10%
Lastlayer Lastlayer 75.11%
LastLayer AllLayer 75.12%
AllLayer AllLayer 75.18%
we can see that ResNet and DenseNet both have relatively
similargeneralizationerrors,butthegradientnormdistribution
of members and non-members is more distinguishable for
DenseNet (Figure 4b) compared to ResNet (Figure 4c). We
see that the attack accuracy in DenseNet is much higher than
ResNet.
Also, we show that the accuracy of our inference attack is
higher for classification output classes (of the target model)
with a larger difference in member/non-member gradient
norms. Figure 2a plots the average of last layer’s gradient
norms for different output classes for member and non-
memberinstances;weseethatthedifferenceofgradientnorms
between members and non-members varies across different
classes. Figure 2b shows the receiver operating characteristic
(ROC) curve of the inference attack for three output classes
with small, medium, and large differences of gradient norm
between members and non-members (averaged over many
samples).Ascanbeseen,thelargerthedifferenceofgradient
norm between members and non-members, the higher the
accuracy of the membership inference attack.
Impact of prediction uncertainty: Previous work [6] claims
that the prediction vector’s uncertainty is an important factor
in privacy leakage. We validate this claim by evaluating the
attackfordifferentclassesinCIFAR100-Alexnetwithdifferent
predictionuncertainties.Specifically,weselectedthreeclasses
mron
tneidarG
Member instances
Non-member instances
(a)Gradientnormformemberandnon-memberdataacrossallclasses
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
FalsePositive
evitisoP
eurT Small gradient diff (174.33)
Medium gradient diff (860.32)
Large gradient diff (1788.13)
Random guess attack
(b) Attacker accuracy for class members with various differences in
their member/non-member gradient norms
Fig. 2:Attackaccuracyisdifferentfordifferentoutput classes(pre-
trained CIFAR100-Alextnet model in the stand-alone scenario).
TABLE VI: Attack accuracy for various sizes of the attacker’s
training dataset. The size of the target model’s training dataset is
50,000. (The CIFAR100 dataset with Alexnet, stand-alone scenario)
Members Sizes Non-members Sizes Attack Accuracy
10,000 2,000 73.2%
15,000 2,000 73.7%
15,000 5,000 74.8%
25,000 5,000 75.1%
TABLE VII: Accuracy of our unsupervised attackcompared tothe
Shadow models approach [6] for the white-box scenario.
(Unsupervised) (ShadowModels)
Dataset Arch
AttackAccuracy AttackAccuracy
CIFAR100 Alexnet 75.0% 70.5%
CIFAR100 DenseNet 71.2% 64.2%
CIFAR100 ResNet 63.1% 60.9%
Texas100 FullyConnected 66.3% 65.3%
Purchase100 FullyConnected 71.0% 68.2%
withsmall, medium,andhighdifferencesofpredictionuncer-
9
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40 60 80
Training epoch
mron
tneidarG
C. Stand-Alone Setting: Attacking Fine-Tuned Models
Member instances
Non-member instances We investigate privacy leakage of fine-tuned target models.
Inthisscenario,thevictimtrainsamodelwithdatasetD,then
heusesadatasetD tofine-tunethetrainedmodeltoimprove
∆
its performance.Hence, the attacker has two snapshots of the
trainedmodel,oneusing onlyD, and oneforthe samemodel
which is fine-tuned using D . We assume the attacker has
∆
access to both of the trained models (before and after fine-
tuning). We are interested in applying the membership infer-
enceattackinthisscenario,wherethegoaloftheadversaryis
to distinguishbetween the membersof D, D , and D¯, which
∆
is a set of non-members.
We use the same training dataset as in the previousexperi-
Fig. 3: Gradient norms of the last layer during learning epochs for ments(TableII);weused60%ofthetraindatasetasDandthe
member and non-member instances (for Purchase100). restforD .TableIXshowsthetrain,test,andattackaccuracy
∆
for different scenarios. As can be seen, the attacker is able to
distinguishbetweenmembers(inD orD )andnon-members
∆
tainties,wheretheattackaccuraciesareshownin Figure6for (D¯) with accuraciessimilar to previoussettings. Additionally,
these classes. Similar to thedifferencesin gradientnorms,the the attacker can also distinguish between the members of D
classes with higher prediction uncertainty values leak more and D ∆ with reasonably high accuracies.
membership information.
D. Federated Learning Settings: Passive Inference Attacks
Table XI shows the dataset sizes used in our federated
B. Stand-Alone Setting: Unsupervised Attacks attack experiments. For the CIFAR100 experiment with a
local attacker, each participant uses 30,000 instances to train,
We alsoimplementourattacksinanunsupervisedscenario,
which overlaps between various participants due to non-
in which the attacker has data points sampled from the same
sufficient number of instances. For all the other experiments,
underlying distribution, but he does not know their member
theparticipantsusenon-overlappingdatasets.Inthefollowing,
andnon-memberlabels.Inthiscase,theattackerclassifiesthe
we present the attack in various settings.
tested records into two clusters as described in Section II-D.
The Passive Global Attacker: In this scenario, the attacker
We implemented our attack and compared its performance
(the parameter aggregator) has access to the target model’s
to Shadow models of Shokri et al. [6] introduced earlier. We
parameters over multiple training epochs (see Section II-B).
trainourunsupervisedmodelsonvariousdatasetsbasedonthe
Thus, he can passively collect all the parameter updates
training and test dataset sizes in Table II. We train a single
from all of the participants, at each epoch, and can perform
ShadowmodeloneachofTexas100andPurchase100datasets
the membership inference attack against each of the target
using trainingsizes accordingto Table II. The trainingsets of
participants, separately.
the Shadow models do no overlap with the training sets of
DuetoourlimitedGPUresources,ourattackobserveseach
the target models. For the CIFAR100 dataset, however, our
target participant during only five (non-consecutive) training
Shadow model uses a training dataset that overlaps with the
epochs. Table XII shows the accuracy of our attack when
target model’s dataset, as we do not have enough instances
it uses different sets of training epochs (for the CIFAR100
(we train each model with 50,000 instances out of the total
dataset with Alexnet). We see that using later epochs, sub-
60,000 available records).
stantially increases the attack accuracy. Intuitively, this is
After the training, we use the Spectral clustering algo-
because the earlier training epochs contain information of the
rithm [14] to divide the input samples into two clusters. As
generic features of the dataset, which do not leak significant
shown earlier (Figure 4), the member instances have smaller
membership information, however, the later epochs contain
gradient norm values. Therefore, we assign the member label
moremembershipinformationas the modelstarts to learn the
to the cluster with a smaller average gradient norm, and the
outliers in such epochs [15].
non-member label to the other cluster.
Table X presents the results of this attack on different
TableVIIcomparestheaccuracyofourunsupervisedattack datasets. For the Purchase100 and Texas100 datasets we use
withshadowtraining[6]onvariousdatasetsandarchitectures. the[40,60,80,90,100]trainingepochs,andfortheCIFAR100
Weseethatourapproachoffersanoticeablyhigheraccuracy. dataset we use epochs [100,150,200,250,300]. When the
The intuitionbehindourattack workingis thatthe encoded attackerhasaccesstoseveraltrainingepochsintheCIFAR100
values of our unsupervised algorithm present different distri- targetmodels,heachievesahighmembershipattackaccuracy.
butions for member and non-member samples. This can be In Texas100and Purchase100datasets, however,the accuracy
seen in Figure 5 for various datasets and architectures. of the attack decreases compare to the stand-alone setting.
10
TABLE VIII: The attack accuracy for different datasets and different target architectures using layer outputs versus gradients. This is the
result of analyzing the stand-alone scenario, where the CIFAR100 models are all obtained from pre-trained online repositories.
Target Model Attack Accuracy
Dataset Architecture Train Accuracy TestAccuracy Black-box White-box (Outputs) White-box (Gradients)
CIFAR100 Alexnet 99% 44% 74.2% 74.6% 75.1%
CIFAR100 ResNet 89% 73% 62.2% 62.2% 64.3%
CIFAR100 DenseNet 100% 82% 67.7% 67.7% 74.3%
Texas100 FullyConnected 81.6% 52% 63.0% 63.3% 68.3%
Purchase100 FullyConnected 100% 80% 67.6% 67.6% 73.4%
TABLE IX: Attack accuracy on fine-tuned models. D is the initial training set, D∆ is the new dataset used for fine-tuning, and D¯ is the
set of non-members (which is disjoint with D and D∆).
Dataset Architecture Train Acc. TestAcc. Distinguish D from D∆ Distinguish D from D¯ Distinguish D∆ from D¯
CIFAR100 Alexnet 100.0% 39.8% 62.1% 75.4% 71.3%
CIFAR100 DenseNet 100.0% 64.3% 63.3% 74.6% 71.5%
Texas100 FullyConnected 95.2% 48.6% 58.4% 68.4% 67.2%
Purchase100 FullyConnected 100.0% 77.5% 64.4% 73.8% 71.2%
0.8
0.6
0.4
0.2
0.0
0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 5 0 5 0 1 1 2 2 3
Gradient norm
noitcarF
1.0
Member
Non-member 0.8
0.6
0.4
0.2
0.0
0 0 0 0 0 0 0 5 0 5 0 5 0 1 1 2 2 3
Gradient norm
(a) CIFAR100-Alexnet
noitcarF
Member
0.6
Non-member
0.4
0.2
0.0
0 0 0 0 0 0 0 5 0 5 0 5 0 1 1 2 2 3
Gradient norm
(b) CIFAR100-Densenet
noitcarF
Member
Non-member
(c) CIFAR100-Resnet
Fig. 4: The distribution of gradient norms for member and non-member instances of different pretrained models.
1.0
0.8
0.6
0.4
0.2
0.0
0 100 200 300 400 500
Encoded value
noitcarF
1.0
Non-member
Member 0.8
0.6
0.4
0.2
0.0
0 100 200 300 400 500
Encoded value
(a) DenseNet-CIFAR100
noitcarF
Non-member
0.6
Member
0.4
0.2
0.0
0 500 1000 1500 2000 2500
Encoded value
(b) ResNet-CIFAR100
noitcarF
Non-member
Member
(c) AlexNet-CIFAR100
Fig. 5: The distribution of the encoded values (i.e., the attack output) for the member and non-member instances of our
unsupervised algorithm are distinguishable. This is the intuition behind the high accuracy of our unsupervised attack.
Thisisduetothefactthataveraginginthefederatedlearning The Passive Local Attacker: A local attacker cannotobserve
scenarios will reduce the impact of each individual party. themodelupdatesoftheparticipants;hecanonlyobservethe
11
TABLE X: Attackaccuracyinthefederatedlearningsetting.Thereare4participants.Aglobalattackeristhecentralparameteraggregator,
andthelocalattackerisoneoftheparticipants.Theglobalattackerperformstheinferenceagainsteachindividual participant,andwereport
theaverageattackaccuracy.Thelocalattackerperformstheinferenceagainstallotherparticipants.Thepassiveattackerfollowstheprotocol
and only observes the updates. The active attacker changes its updates, or (in the case of a global attack) isolates one participant by not
passing the updates of other participants to it, in order to increase the information leakage.
Target Model Global Attacker (theparameter aggregator) Local Attacker (aparticipant)
Passive Active Passive Active
Dataset Architecture Gradient Ascent Isolating Isolating Gradient Ascent Gradient Ascent
CIFAR100 Alexnet 85.1% 88.2% 89.0% 92.1% 73.1% 76.3%
CIFAR100 DenseNet 79.2% 82.1% 84.3% 87.3% 72.2% 76.7%
Texas100 FullyConnected 66.4% 69.5% 69.3% 71.7% 62.4% 66.4%
Purchase100 FullyConnected 72.4% 75.4% 75.3% 82.5% 65.8% 69.8%
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
False Positive
evitisoP
eurT
aggregate model parameters. We use the same attack model
architecture as that of the global attack. In our experiments,
there are four participants (including the local attacker). The
goal of the attacker is to learn if a target input has been a
memberofthetrainingdataofanyotherparticipants.TableX
shows the accuracy of our attack on various datasets. As
expected,a localattack hasa lower accuracycomparedto the
Small uncert diff (0.25)
global attack; this is because the local attacker observes the
Medium uncert diff (0.36)
aggregate model parameters of all participants, which limits
Large uncert diff (0.59)
the extent of membership leakage. The accuracy of the local
Random guess
attacker degrades for larger numbers of participants. This is
shown in Table XIII for the CIFAR100 on Alexnet model.
E. Federated Learning Settings: Active Inference Attacks
Fig. 6: Attack’s ROC for three different classes of data with
large, medium, and small prediction uncertainty values (pre-trained Table X shows the results of attacks on federated learning.
CIFAR100-Alextnet model in the stand-alone scenario).
The Gradient Ascent Attacker: In this scenario, the attacker
adversarially manipulates the learning process to improve the
TABLE XI: Dataset sizes in the federated learning experiments
membershipinferenceaccuracy.Theactiveattackisdescribed
Parties’Datasets InferenceAttackModel inSection II-C. We evaluatetheattack accuracyonpredicting
Training Training Test Test the membership of 100 randomly sampled member instances,
Datasets Training Test
Members Non-members Members Non-members from the target model, and 100 non-member instances. For
CIFAR100 30,000 10,000 15,000 5,000 5,000 5,000
all such target instances (whose membership is unknown to
Texas100 8,000 70,000 4,000 4,000 4,000 4,000
the attacker), the attacker updates their data features towards
Puchase100 10,000 50,000 5,000 5,000 5,000 5,000
ascending the gradients of the global model (in case of the
TABLE XII: The accuracy of the passive global attacker in the globalattack)orthelocalmodel(inthecaseofalocalattack).
federated setting when the attacker uses various training epochs. Figure7comparesthelast-layergradientnormofthetarget
(CIFAR100-Alexnet) model for different data points. As Figure 7a shows, when
the attacker ascends on the gradients of the target instances,
Observed Epochs Attack Accuracy
the gradient norm of the target members will be very similar
5,10,15,20,25 57.4%
to that of non-target member instances in various training
10,20,30,40,50 76.5% epochs.Ontheotherhand,thisisnottrueforthenon-member
50,100,150,200,250 79.5% instances as shown in Figure 7b.
100,150,200,250,300 85.1% Intuitively, this is because applying the gradient ascent
algorithm on a member instance will trigger the target model
TABLEXIII:Theaccuracyofthepassivelocalattackerfordifferent to try to minimize its loss by descending in the direction
numbers of participants. (CIFAR100-Alexnet) of the model’s gradient for those instances (and therefore
nullify the effect of the attacker’s ascent). For target non-
Number of Participants Attack Accuracy
member instances, however, the model will not explicitly
2 89.0%
change their gradient, as they do not influence the training
3 78.1%
loss function. The attacker repeats gradient ascend algorithm
4 76.7%
for each epoch of the training, therefore, the gradient of the
5 67.2%
12
5
4
3
2
1
0
0 20 40 60 80
Epoch
mron
tneidarG
5
Target members
Non-target members 4
3
2
1
0
0 20 40 60 80
Epoch
(a)
mron
tneidarG
5
Target non-members
Non-target non-members 4
3
2
1
0
0 20 40 60 80
Epoch
(b)
mron
tneidarG
Target members
Target non-members
(c)
Fig. 7:The impactof the globalactive gradientascentattack on the targetmodel’straining process. Figuresshow the gradient
norms of various instances (Purchase100 dataset) during the training phase, while the target instances are under attack.
model will keep increasing on such non-member instances. which could be exploited by the adversary’sinference model.
Figure7cdepictstheresulteddistinctionbetweenthegradient They introduced shadow models that mimic the behavior of
norm of the member and non-member target instances. The the target model, which are used by the attacker to train
active gradient ascend attacker forces the target model to the attack model. Salem et al. [17] extended the attacks of
behave drastically differentbetween targetmember and target Shokri et al. [6] and showed empirically that it is possible
non-memberinstanceswhichmakesthemembershipinference to use a single shadow model (instead of several shadow
attack easier. As a result, compared to the passive global modelsused in [6]) to performthe same attack. They further
attackerweseethattheactiveattackcannoticeablygainhigher demonstrated that even if the attacker does not have access
accuracy. In the local case, the accuracy is lower than the to the target model’s training data, she can use statistical
global attack due to the observationof aggregatedparameters propertiesofoutputs(e.g.,entropy)toperformmembershipin-
from multiple participants. ference.Yeometal.[7]demonstratedtherelationshipbetween
The Isolating Attacker: The parameter aggregation in the overfittingandmembershipinferenceattacks.Hayesetal.[18]
federated learning scenario negativelyinfluences the accuracy used generative adversarial networks to perform membership
ofthemembershipinferenceattacks.Anactiveglobalattacker attacks on generative models.
can overcome this problem by isolating a target participant, Melis et al. [19] developed a new set of membership
and creating a local view of the network for it. In this inference attacks for the collaborative learning. The attack
scenario, the attacker does not send the aggregate parameters assumes that the participants update the central server after
of all parties to the target party. Instead, the attacker isolates each mini-batch, as opposed to updating after each training
the target participant and segregates the target participant’s epoch [20], [21]. Also, the proposed membership inference
learning process. attack is designed exclusively for models that use explicit
When the attacker isolates the target participant, then the word embeddings (which reveal the set of words used in the
target participant’s model does not get aggregated with the training sentences in a mini-batch) with very small training
parameters of other parties. As a result, it stores more infor- mini-batches.
mation about its training dataset in the model. Thus, simply In this paper, we evaluate standard learning mechanisms
isolating the training of a target model significantly increases for deep learning and standard target models for various
the attack accuracy. We can apply the isolating method to architectures. We showed that our attacks work even if we
the gradient ascent attacker and further improve the attacker use pre-trained, state-of-the-art target models.
accuracy. See Table X for all the results. Differential privacy [22], [23] has been used as a strong
defensemechanismagainstinferenceattacks in the contextof
V. RELATEDWORK machine learning [24], [25], [26], [27]. Several works [28],
Investigating different inference attacks on deep neural [29], [30] have shown that by using adversarial training, one
networks is an active area of research. canfindabettertrade-offbetweenprivacyandmodelaccuracy.
However,the focusof this line of work is on the membership
A. Membership Inference Attacks inference attack in the black-box setting.
Multipleresearchpapershavestudiedmembershipinference
B. Other Inference Attacks
attacks in a black-box setting [6], [16], [7]. Homer et al. [4]
performed one of the first membership inference attacks on An attacker with additional information about the training
genomic data. Shokri et al. [6] showed that an ML model’s data distribution can perform various types of inference at-
output has distinguishable properties about its training data, tacks. Inputinference[31], attribute inference[32], parameter
13
inference [33], [34], and side-channelattacks [35] are several [11] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
examples of such attacks. Ateniese et al. [36] show that an recognition,”inProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition, 2016,pp.770–778.
adversary with access to the parameters of machine learning
[12] G.Huang,Z.Liu,L.VanDerMaaten,andK.Q.Weinberger,“Densely
models such as Support Vector Machines (SVM) or Hidden connected convolutional networks.” inCVPR,vol.1,no.2,2017,p.3.
MarkovModels(HMM)[37]canextractvaluableinformation [13] I.Goodfellow,Y.Bengio,andA.Courville,Deeplearning,2016,vol.1.
[14] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and
about the training data (e.g., the accent of the speakers in
computing, vol.17,no.4,pp.395–416,2007.
speech recognition models). [15] C.Zhang,S.Bengio,M.Hardt,B.Recht,andO.Vinyals,“Understand-
ing deep learning requires rethinking generalization,” arXiv preprint
VI. CONCLUSIONS arXiv:1611.03530,2016.
[16] Y. Long,V.Bindschaedler, L.Wang,D.Bu, X.Wang,H. Tang,C.A.
We designed and evaluated novel white-box membership Gunter, and K. Chen, “Understanding membership inferences onwell-
generalized learning models,”arXivpreprintarXiv:1802.04889,2018.
inferenceattacksagainstneuralnetworkmodelsby exploiting
[17] A.Salem,Y.Zhang,M.Humbert,M.Fritz,andM.Backes,“Ml-leaks:
the privacy vulnerabilities of the stochastic gradient descent Modelanddataindependentmembershipinferenceattacksanddefenses
algorithm.Wedemonstratedourattacksinthestand-aloneand onmachinelearning models,”arXivpreprintarXiv:1806.01246,2018.
[18] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan: evalu-
federatedsettings, withrespectto passiveandactiveinference
atingprivacy leakageofgenerative modelsusinggenerative adversarial
attackers, and assuming different adversary prior knowledge. networks,”arXivpreprintarXiv:1705.07663,2017.
Weshowedthatevenwell-generalizedmodelsaresignificantly [19] L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting
unintended feature leakage in collaborative learning,” arXiv preprint
susceptible to such white-box membership inference attacks.
arXiv:1805.04049,2018.
Ourworkdidnotinvestigatetheoreticalboundsontheprivacy [20] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
leakageofdeeplearninginthewhite-boxsetting,whichwould Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security. ACM,2015,pp.1310–1321.
remain as a topic of future research.
[21] H. B. McMahan, E. Moore, D. Ramage, S. Hampson et al.,
“Communication-efficient learningofdeepnetworksfromdecentralized
ACKNOWLEDGMENTS data,”arXivpreprintarXiv:1602.05629,2016.
[22] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise
This work was supported in part by the NSF grant CNS- to sensitivity in private data analysis,” in Theory of Cryptography
1525642, as well as the Singapore Ministry of Education Conference. Springer, 2006,pp.265–284.
[23] C. Dwork, A. Roth et al., “The algorithmic foundations of differential
Academic Research Fund Tier 1, R-252-000-660-133. Reza
privacy,” Foundations and Trends(cid:13)R in Theoretical Computer Science,
Shokri would like to acknowledge the support of NVIDIA vol.9,no.3–4,pp.211–407, 2014.
Corporation with the donation of a Titan Xp GPU which was [24] K.Chaudhuri,C.Monteleoni,andA.D.Sarwate,“Differentiallyprivate
empirical risk minimization,” Journal of Machine Learning Research,
used for this research.
vol.12,no.Mar,pp.1069–1109, 2011.
[25] R. Bassily, A. Smith, and A. Thakurta, “Private empirical risk mini-
REFERENCES mization: Efficient algorithms and tight error bounds,” in Foundations
ofComputer Science (FOCS),2014IEEE55thAnnual Symposium on.
[1] C.Dwork,A.Smith,T.Steinke, andJ.Ullman,“Exposed!asurveyof IEEE,2014,pp.464–473.
attacks onprivate data,”2017. [26] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,
[2] I. Dinur and K. Nissim, “Revealing information while preserving pri- K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in
vacy,” in Proceedings of the twenty-second ACM SIGMOD-SIGACT- Proceedings of the 2016 ACM SIGSAC Conference on Computer and
SIGART symposium on Principles of database systems. ACM, 2003, Communications Security. ACM,2016,pp.308–318.
pp.202–210. [27] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and
[3] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou, “Learning your U´. Erlingsson, “Scalable private learning with pate,” arXiv preprint
identityanddiseasefromresearchpapers:informationleaksingenome arXiv:1802.08908,2018.
wideassociationstudy,”inProceedingsofthe16thACMconferenceon [28] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learning with
Computer andcommunications security. ACM,2009,pp.534–544. membershipprivacyusingadversarialregularization,”inProceedingsof
[4] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, the2018ACMSIGSACConference onComputerandCommunications
J.Muehling,J.V.Pearson,D.A.Stephan,S.F.Nelson,andD.W.Craig, Security. ACM,2018,pp.634–646.
“Resolving individuals contributing trace amounts of dna to highly [29] J.Hamm,“Minimax filter: learning topreserve privacy from inference
complexmixturesusinghigh-densitysnpgenotypingmicroarrays,”PLoS attacks,”TheJournalofMachineLearningResearch,vol.18,no.1,pp.
genetics, vol.4,no.8,p.e1000167, 2008. 4704–4734, 2017.
[5] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan, “Robust [30] C.Huang,P.Kairouz,X.Chen,L.Sankar,andR.Rajagopal,“Generative
traceability from trace amounts,” in Foundations of Computer Science adversarial privacy,” arXivpreprintarXiv:1807.05306,2018.
(FOCS), 2015 IEEE 56th Annual Symposium on. IEEE, 2015, pp. [31] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks
650–669. that exploit confidence information and basic countermeasures,” in
[6] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership Proceedings of the 22nd ACM SIGSAC Conference on Computer and
inference attacks against machine learning models,” in Security and Communications Security. ACM,2015,pp.1322–1333.
Privacy(SP),2017IEEESymposium on,2017. [32] N. Carlini, C. Liu, J. Kos, U´. Erlingsson, and D. Song, “The secret
[7] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in sharer:Measuringunintendedneuralnetworkmemorization&extracting
machine learning: Analyzing the connection to overfitting,” in IEEE secrets,”arXivpreprintarXiv:1802.08232,2018.
Computer Security Foundations Symposium,2018. [33] F.Trame`r,F.Zhang,A.Juels,M.K.Reiter,andT.Ristenpart,“Stealing
[8] J.Konecˇny`, H.B.McMahan, F.X.Yu,P.Richta´rik, A.T.Suresh,and machinelearningmodelsviapredictionapis,”inUSENIXSecurity,2016.
D.Bacon,“Federatedlearning:Strategiesforimprovingcommunication [34] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
efficiency,” arXivpreprintarXiv:1610.05492,2016. learning,” arXivpreprintarXiv:1802.05351,2018.
[9] A.Krizhevsky,“Learningmultiplelayersoffeaturesfromtinyimages,” [35] L.Wei,Y.Liu,B.Luo,Y.Li,andQ.Xu,“Iknowwhatyousee:Power
Citeseer, Tech.Rep.,2009. side-channelattackonconvolutionalneuralnetworkaccelerators,”arXiv
[10] A.Krizhevsky,I.Sutskever, andG.E.Hinton,“Imagenetclassification preprintarXiv:1803.05847,2018.
withdeepconvolutional neuralnetworks,”inAdvances inneuralinfor- [36] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and
mationprocessingsystems,2012,pp.1097–1105. G.Felici, “Hacking smartmachines withsmarterones:How toextract
14
meaningful datafrommachinelearning classifiers,”International Jour-
nalofSecurity andNetworks, vol.10,no.3,pp.137–150,2015.
[37] C. Robert, Machine learning, a probabilistic perspective. Taylor &
Francis, 2014.
APPENDIX A
ARCHITECTURE OF THEATTACK MODEL
TABLE XIV: Attack model layer sizes
Name Layers Details
Sizes:128,64
OutputComponent 2FullyConnectedLayers Activation:ReLU
Dropout:0.2
Sizes:128,64
LabelComponent 2FullyConnectedLayers Activation:ReLU
Dropout:0.2
Sizes:128,64
LossComponent 2FullyConnectedLayers Activation:ReLU
Dropout:0.2
Kernels:1000
Kernelsize:1×Nextlayer
ConvolutionalLayer
Stride:1
GradientComponent
Dropout:0.2
Sizes:128,64
2FullyConnectedLayers Activation:ReLU
Dropout:0.2
Sizes:256,128,64,1
EncoderComponent 4FullyConnectedLayers Activation:ReLU
Dropout:0.2
Sizes:64,4
DecoderComponent 2FullyConnectedLayers Activation:ReLU
Dropout:0.2
15