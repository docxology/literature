Complex behavior from intrinsic motivation to occupy future
action-state path space
Jorge Ramírez-Ruiz1, Dmytro Grytskyy1, Chiara Mastrogiuseppe1, Yamen Habib1, and
Rubén Moreno-Bote1,2
1Center for Brain and Cognition, and Department of Information and Communication Technologies, Universitat
Pompeu Fabra, Barcelona, Spain 08005
2Serra Húnter Fellow Programme, Universitat Pompeu Fabra, Barcelona, Spain
February 27, 2024
Abstract
Most theories of behavior posit that agents tend to maximize some form of reward or utility. However,
animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we
abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy
of future paths of actions and states. According to this maximum occupancy principle, rewards are
the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways
of searching for resources so that movement, understood amply, never ends. We find that action-state
path entropy is the only measure consistent with additivity and other intuitive properties of expected
future action-state path occupancy. We provide analytical expressions that relate the optimal policy
and state-value function, and prove convergence of our value iteration algorithm. Using discrete and
continuous state tasks, including a high–dimensional controller, we show that complex behaviors such
as ‘dancing’, hide-and-seek and a basic form of altruistic behavior naturally result from the intrinsic
motivation to occupy path space. All in all, we present a theory of behavior that generates both variability
and goal-directedness in the absence of reward maximization.
1 Introduction
Natural agents are endowed with a tendency to move, explore and interact with their environment [1, 2]. For
instance, human newborns unintentionally move their body parts [3], and 7 to 12-months infants spontaneously
babble vocally [4] and with their hands [5]. Exploration and curiosity are major drives for learning and
discovery through information-seeking [6–8]. These behaviors seem to elude a simple explanation in terms
of extrinsic reward maximization. However, these intrinsic motivations push agents to visit new states by
performing novel courses of action, which helps learning and the discovery of even larger rewards in the long
run [9, 10]. Therefore, it has been argued that exploration and curiosity could arise as a consequence of
seeking extrinsic reward maximization by endowing agents with the necessary inductive biases to learn in
complex and ever-changing natural environments [11, 12].
While most theories of rational behavior do posit that agents are reward maximizers [13–16], very few of us
would agree that the sole goal of living agents is maximizing money gains or food intake. Indeed, expressing
excessive emphasis on these types of goals is usually seen as a sign of psychological disorders [17, 18]. Further,
setting a reward function by design as the goal of artificial agents is, more often than not, arbitrary [14, 19–21],
leading to the recurrent problem faced by theories of reward maximization of defining what rewards are
[22–26]. In some cases, like in artificial games, rewards can be unambiguously defined, such as number of
collected points or wins [27]. However, in most situations defining rewards is task-dependent, non-trivial
and problematic. For instance, a vacuum cleaner robot could be designed to either maximize the weight or
volume of dust collected, energy efficiency, or a weighted combination of them [28]. In more complex cases,
companies can aim at maximizing profit, but without a suitable innovation policy profit maximization can be
self-defeating [29].
1
arXiv:2205.10316v2  [cs.AI]  24 Feb 2024
Here, we abandon the idea that the goal is maximizing extrinsic rewards and that movement over space is a
means to achieve this goal. Instead, we adopt the opposite view, inspired by the nature of our intrinsic drives:
we propose that the objectiveis to maximally occupy action-state path space, understood in a broad sense, in
the long term. We call this principle the maximum occupancy principle (MOP), which posits that the goal
of agents is to generate all sort of behaviors and occupy, on average, as much space (action-state paths) as
possible in the future. According to MOP, extrinsic rewards serve to obtain the energy necessary to move in
order to occupy action-state space, they are not the goals per se. The usual exploration–exploitation tradeoff
[30] therefore disappears: agents that seek to occupy space “solve” this issue naturally because they care about
rewards only as means to an end. Furthermore, in this sense, surviving is only preferred because it is needed
to keep visiting action-state space. Our theory provides a rational account of exploratory and curiosity-driven
behavior where the problem of defining a reward function vanishes, and captures the variability of behavior
[31–36] by taking it as a principle.
In this work, we model a MOP agent interacting with the environment as a Markov decision process
(MDP) where the intrinsic, immediate reward is the occupancy of the next action-state visited, which is
largest when performing an uncommon action and visiting a rare state –there are no extrinsic rewards (i.e., no
task is defined) that drive the agent. We show that (weighted) action-state path entropy is the only measure
of occupancy consistent with additivity per time step, positivity and smoothness. Due to the additivity
property, the value of being in a state, defined as the expected future time-discounted action-state path
entropy, can be written in the form of a Bellman equation, which has a unique solution that can be found with
an iterative map. Following this entropy objective leads to agents that seek variability, while being sensitive to
the constraints imposed by the agent-environment interaction on the future path availability. We demonstrate
in various simulated experiments with discrete and continuous state and action spaces that MOP generates
complex behaviors that, to the human eye, look genuinely goal-directed and playful, such as hide-and-seek in
a prey-predator problem, dancing of a cartpole, a basic form of altruism in an agent-and-pet example, and
rich behaviors in a high-dimensional quadruped.
MOP builds over an extensive literature on entropy-regularized reinforcement learning (RL) [37–47] or
pure entropic objectives [48–53]. This body of work emphasizes the regularization benefits of entropy for
learning, but extrinsic rewards still serve as the major drive of behavior, and arbitrary mixtures of action-state
entropy are rarely considered [47]. Our work also relates to reward-free theories of behavior. These minimize
predictions errors [54–59], seek novelty [60–62], or maximize data compression [63], and therefore the major
behavioral driver depends on the agent’s experience with the world. On the other hand, MOP agents find the
action-states that lead to high future occupancy “interesting”, regardless of experience. There are two other
approaches that sit closer to this description, one maximizing mutual information between actions and future
states (empowerment, MPOW) [20, 64–66], and the other minimizing the distance between the actual and a
desired state distribution (free energy principle, FEP) [67, 68]. We show that both MPOW and FEP tend
to collapse to deterministic policies with little behavioral variability. In contrast, MOP results in lively and
seemingly goal-directed behavior by taking behavioral variability and the constraints of embodied agents as
principles.
2 Maximum occupancy principle
2.1 Entropy measure of path space occupancy
We model an agent as a finite action-state MDP in discrete time. The policyπ describes the probability
π(a|s) of performing actiona, from some setA(s), given that the agent is at states at some time step, and
p(s′|s, a) is the transition probability froms to a successor states′ in the next time step given that actiona
is performed. Starting att = 0 in states0, an agent performing a sequence of actions and experiencing state
transitions τ ≡ (s0, a0, s1, ..., at, st+1, ...) gets a return defined as
R(τ) =
∞X
t=0
γtR(st, at) = −
∞X
t=0
γt ln
 
πα(at|st)pβ(st+1|st, at)

, (2.1)
with action and state weightsα >0 and β ≥ 0, respectively, and discount factor0 < γ <1. A larger return is
obtained when, fromst, a low-probability actionat is performed and followed by a low-probability transition
2
Figure 1: MOP agents maximize action-state path occupancy. (a) A MOP agent (grey triangle) in the middle of two
rooms has the choice between going left or right. When the number of actions (black arrows) in each room is the same,
the agent prefers going to the room with more state transitions (blue arrows indicate random transitions after choosing
moving right or moving left actions, and pink arrow width indicates the probabilities of those actions). (b) When the
states transitions are the same in the two rooms, the MOP agent prefers the room with more available actions. (c) If
there are many absorbing states in the room where many actions are available, the MOP agent avoids it. (d) Even if
there are action and state-transition incentives (in the left room), a MOP agent might prefer a region of state space
where it can reliably get food (right room), ensuring occupancy of future action-state paths. See Supplemental Fig. 8
for a more formal example.
to a statest+1. Therefore, maximizing the return in Eq. (2.1) favors ‘visiting’ action-states(at, st+1) with a
low transition probability. Fromst+1, another low-probability action-state transition is preferred and so on,
such that low-probability trajectoriesτ are more rewarding than high-probability ones. Thus, the agent is
pushed to visit action-states that are rare or ‘unoccupied’, implementing the intuitive notion of MOP. Due to
the freedom to choose actionat given statest and the uncertainty of the resulting next statest+1, apparent
in Eq. (2.1), the term ‘action-states’ used here is more natural than ‘state-actions’. We stress that this return
is purely intrinsic, namely, there is no extrinsic reward that the agent seeks to maximize. We define intrinsic
rewards as any reward signal that depends on the policy or the state transition probability, and therefore it
can change with the course of learning as the policy is improved, or the environment is learnt. An extrinsic
reward is the complementary set of reward signals: any functionR(s, a) that is both policy-independent and
transition probability-independent, and therefore it does not change with the course of improving the policy
or learning the state transition probability of the environment.
The agent is assumed to optimize the policyπ to maximize the state-valueVπ(s), defined as the expected
return
Vπ(s) ≡ Eat∼π,st+1∼p[R(τ)|s0 = s] = Eat∼π,st+1∼p
" ∞X
t=0
γt (αH(A|st) + βH(S′|st, at))
s0 = s
#
(2.2)
given the initial conditions0 = s and following policyπ, that is, the expectation is over theat ∼ π(·|st) and
st+1 ∼ p(·|st, at), t ≥ 0. In the last identity, we have rewritten the expectations of the terms in Eq. (2.1) as a
discounted and weighted sum of action and successor state conditional entropiesH(A|s) = −P
a π(a|s) ln π(a|s)
and H(S′|s, a) = −P
s′ p(s′|s, a) lnp(s′|s, a), respectively, averaged over previous states and actions.
3
We define a MOP agent as the one that optimizes the policy to maximize the state-value in Eq. (2.2).
The entropy representation in Eq. (2.2) of MOP has several implications. First, agents prefer regions of state
space that lead to a large number of successor states (Fig. 1a) or larger number of actions (Fig. 1b). Second,
death (absorbing) states where only one action-state (i.e., “stay”) is available forever are naturally avoided
by a MOP agent, as they promise zero future action and state entropy (Fig. 1c). Therefore, our framework
implicitly incorporates a survival instinct. Finally, regions of state space where there are “rewarding” states
that increase the capacity of the agent to visit further action-states (such as filling an energy reservoir) are
more frequently visited than others (Fig. 1d).
We found that maximizing the discounted action-state path entropy in Eq. (2.2) is the only reasonable
way of formalizing MOP, as it is the only measure of action-state path occupancy in Markov chains consistent
with the following intuitive conditions (Supplemental Sec. A.1): if a pathτ has probabilityp, visiting it
results in an occupancy gainC(p) that (i) decreases withp and (ii) is first-order differentiable. Condition (i)
implies that visiting a low probability path increases occupancy more than visiting a high probability path,
and our agents should tend to occupy ‘unoccupied’ path space; condition (ii) requires that the measure should
be smooth. We also ask that (iii) the occupancy of paths, defined as the expectation of occupancy gains over
paths given a policy, is the sum of the expected occupancies of their subpaths (additivity condition). This
last condition implies that agents can accumulate occupancy over time by keeping visiting low-probability
action-states, but the accumulation should be consistent with the Markov property of the decision process.
These conditions are similar but not exactly the same as Shannon’s information measure [69] (Supplemental
Sec. A.1).
2.2 Optimal policy and state-value function
The state-valueVπ(s) in Eq. (2.2) can be recursively written using the values of successor states through the
standard Bellman equation
Vπ(s) = αH(A|s) + β
X
a
π(a|s)H(S′|s, a) + γ
X
a,s′
π(a|s)p(s′|s, a)Vπ(s′)
=
X
a,s′
π(a|s)p(s′|s, a) (−α ln π(a|s) − β ln p(s′|s, a) + γVπ(s′)) , (2.3)
where the sum is over the available actionsa from state s, A(s), and over the successor statess′ given
the performed action at state s. The optimal policy π∗ that maximizes the state-value is defined as
π∗ = arg maxπ Vπ and the optimal state-value is
V ∗(s) = max
π
Vπ(s), (2.4)
where the maximization is with respect to the{π(·|·)} for all actions and states. To obtain the optimal policy,
we first determine the critical points of the expected returnVπ(s) in Eq. (2.3) using Lagrange multipliers
(Supplemental Sec. A.2). The optimal state-valueV ∗(s) is found to obey the non-linear self-consistency set of
equations
V ∗(s) = α ln Z(s) = α ln
"X
a
exp
 
α−1βH(S′|s, a) + α−1γ
X
s′
p(s′|s, a)V ∗(s′)
!#
, (2.5)
where Z(s) is the partition function, defined by substitution, and the critical policy satisfies
π∗(a|s) = 1
Z(s) exp
 
α−1βH(S′|s, a) + α−1γ
X
s′
p(s′|s, a)V ∗(s′)
!
. (2.6)
We find that the solution to the non-linear system of Eqs. (2.5) is unique and, moreover, the unique solution
is the absolute maximum of the state-values over all policies (Supplemental Sec. A.3).
To determine the actual value function from such non-linear set of equations, we derive an iterative
map, a form of value iteration that exactly incorporates the optimal policy at every step. Definingzi =
4
exp
 
α−1γV (si)

, pijk = p(sj|si, ak) and Hik = α−1βH(S′|si, ak), Eq. (2.5) can be turned into the iterative
map
z(n+1)
i =

X
k
wikeHik
Y
j

z(n)
j
pijk


γ
(2.7)
for n ≥ 0 and with initial conditionsz(0)
i > 0. Here, the matrix with coefficients wik ∈ {0, 1} indicate
whether actionak is available at statesi (wik = 1) or not (wik = 0), andj extends over all states, with the
understanding that if a statesj is not a possible successor from statesi after performing actionak then
pijk = 0. We find that the infinite seriesz(n)
i defined in Eq. (2.7) converges to a finite limitz(n)
i → z∞
i
regardless of the initial condition in the positive first orthant, and thatV ∗(si) = αγ−1 ln z∞
i is the optimal
state-value function, which solves Eq. (2.5) (Supplemental Sec. A.3). Iterative maps similar to Eq. (2.7)
have been studied before [37, 70], subsequently shown to have uniqueness [71] and convergence guarantees
[45, 72] in the absence of state entropy terms. A summary of results and particular examples can be found in
Supplemental Sec. A.4.
We note that in the definition of return in Eq. (2.2) we could replace the absolute action entropy terms
H(A|s) by relative entropies of the form−DKL(π(a|s)||π0(a|s)) = P
a π(a|s) ln(π0(a|s)/π(a|s)), as in KL-
regularization [37, 41, 46, 70], but in the absence of any extrinsic rewards. In this case, one obtains an equation
identical to (2.7) where the coefficientswik are simply replaced byπ0(ak|si), one to one. This apparently minor
variation undercovers a major qualitative difference between absolute and relative action entropy objectives:
as P
k wik ≥ 1, absolute entropy-seeking favors visiting states with a large action accessibility, that is, where
the sumP
k wik and thus the argument of Eq. (2.7) tends to be largest. In contrast, asP
k π0(ak|si) = 1,
maximizing relative entropies provides no preference for statess with large number of accessible actions|A(s)|.
This happens even if the default policy is uniform in the actions, as then the immediate intrinsic return
becomes −DKL(π(a|s)||π0(a|s)) = H(A|s) − ln |A(s)|, instead ofH(A|s). The negative logarithm penalizes
visiting states with large number of actions, which is the opposite goal to occupying action-state path space
(see details in Supplemental Sec. A.6).
3 Results
3.1 MOP agents quickly fill physical space
In very simple environments with high symmetry and little constraints, like open space, maximizing path
occupancy amounts to performing a random walk that chooses at every step any available action with equal
probability. However, in realistic environments where space is not homogeneous, where there are energetic
limitations for moving, or where there are absorbing states, a random walk is no longer optimal. To illustrate
how interesting behaviors arise from MOP in these cases, we first tested how a MOP agent moving in a 4-room
and 4-food-sources environment (Fig. 2a) compares in occupying physical space to a random walker (RW)
and to a reward seeking agent (R agent). The definition of the three agents are identical in most ways. They
have nine possible movement actions, including not moving; they all have an internal state corresponding to
the available energy, which reduces one unit at every time step and gets increased by a fixed amount (food
gain) whenever a food source is visited; and they can move as long as their energy is non-zero. The total
state space is the Cartesian product between physical space and internal energy. The agents differ however in
their objective function. The MOP agent has a reward-free objective and implements MOP by maximizing
path action entropy, Eq. (2.2). In contrast, the R agent maximizes future discounted reward (in this case,
food), and displays stochastic behavior through anϵ-greedy action selection, withϵ matched to the survival of
the MOP agent (Supplemental Sec. A.5 and Fig. 9a). Finally, the random walker is simply an agent that in
each state takes a uniformly random action from the available actions at that state.
We find that the MOP agent generates behaviors that can be dubbed goal-directed and curiosity-driven
(Video 1). First, by storing enough energy in its reservoir, the agent reaches far, entering the four rooms in
the long term (Fig. 2b, left panel), and visiting every location of the arena except when food gain is small
(Fig. 2d, blue line). In contrast, the R agent lingers over one of the food sources for most of the time (Fig. 2b,
middle panel; Video 1). Although itsϵ-greedy action selection allows for brief exploration of other rooms,
the R agent does not on average visit the whole arena (Fig. 2d, orange line). Finally, the random walker
5
Figure 2: Maximizing future path occupancy leads to high occupancy of physical space. (a) Grid-world arena. The
agents have nine available actions (arrows, and staying still) when alive (internal energyE larger than zero) and
away from walls. There are four rooms, each with a small food source in a corner (green diamonds). (b) Probability
of visited spatial states for a MOP agent, anϵ-greedy reward (R) agent that survives as long as the MOP agent,
and a random walker. Food gain= 10units, maximum reservoir energy= 100, episodes of5 × 104 time steps, and
(α, β) = (1, 0) for the MOP agent. All agents are initialized in the middle of the lower left room. (c) Optimal value
function V ∗(s) over locations when energy isE = 5. Black arrows represent the optimal policy given by Eq. 2.6; their
length is proportional to the probability of each action. The size of red dots is proportional to the probability of the
do nothing action. (d) Fraction of locations of the arena visited at least once per episode as a function of food gain.
Error bars correspond to s.e.m over50 episodes. (e) Noisy room problem. The bottom right room of the arena was
noisy, such that agents in this room jump randomly to neighboring locations regardless of their actions. Food gain
equals maximum reservoir energy= 100. Histogram of visited locations for an episode as long as in (b) for a MOP
agent withβ = 0.3 (left) and time fraction spent in the noisy room (right) show that MOP agents withβ >0 can
either be attracted to the room or repelled depending onγ.
dies before it has time to visit a large fraction of the physical space (Fig. 2b, right panel). These differences
hold for a large range of food gains (Fig. 2d). The MOP agent, while designed to generate variability, is also
capable of deterministic behavior: when its energy is low, it moves toward the food sources with little to no
variability, a distinct mark of goal-directedness (Fig. 2c, corner spots show that only one action is considered
by optimal policy).
We next considered a slightly more complex environment where actions in one of the rooms lead to
uniformly stochastic transitions to any of the neighboring locations (noisy room –a spatial version of the noisy
TV problem [57, 73]). A MOP agent withβ >0 (see Eq. (2.2)) has a preference for stochastic state transitions,
and a priori it could get attracted and stuck in the noisy room, where actions do not have any predictable
effect. Indeed, we see that for largerβ, which measures the strength of the state entropy contribution to the
agent’s objective, the attraction to the noisy room increases (Fig. 2e, right panel). However, MOP agents also
care about future states, and thus getting stuck in regions where energy cannot be predictably obtained is
avoided by sufficiently long-sighted agents, as shown by the reduction of the time spent in the noisy room
with increasingγ (Fig. 2e; Supplemental Sec. A.5.3). This shows how MOP agents can tradeoff immediate
with future action-state occupancy.
6
Figure 3: Complex hide-and-seek and escaping strategies in a prey-predator example. (a) Grid-world arena. The agent
has nine available actions when alive and far from walls. There is a small food source in a corner (green diamond).
A predator (red, down triangle) is attracted to the agent (gray, up triangle), such that when they are at the same
location, the agent dies. The predator cannot enter the locations surrounded by the brown border. Arrows show a
clockwise trajectory. (b) Histogram of visited spatial states across episodes for the MOP and R agents. The vector
field at each location indicates probability of transition at each location. Green arrows on R agent show major motion
directions associated with its dominant clockwise rotation. (c) Fraction of clockwise rotations (as in panel (a)) to total
rotations as a function of food gain, averaged over epochs of 500 timesteps. Error bars are s.e.m. (d) Optimal value
functions for different energy levels, and same predator position; black arrows indicate optimal policy, as in Fig. 2c.
3.2 Hide and seek in a prey-predator interaction
More interesting behaviors arise from MOP in increasingly complex environments. To show this, we next
considered a prey and a predator in a grid world with a safe area (a “home”) and a single food source (Fig. 3a).
The prey (a “mouse”, gray up triangle) is the agent whose behavior is optimized by maximizing future action
path entropy, while the predator (a “cat”, red down triangle) acts passively chasing the prey. The state of the
agent consists of its location and energy level, but it also includes the predator’s location being accurately
perceived. The prey can move as in the previous 4-room grid world and it also has a finite energy reservoir.
For simplicity, we only considered a food gain equal to the size of the energy reservoir, such that the agent
fully replenishes its reservoir each time it visits the food source. The predator has the same available actions
as the agent and is attracted to it stochastically, i.e. actions that move the predator towards the agent are
more probable than those that move it away from it (Supplemental Sec. A.5.4).
MOP generates complex behaviors, not limited to visiting the food source to increase the energy buffer and
hide at home. In particular, the agent very often first teases the cat and then performs a clockwise rotation
around the obstacle, which forces the cat to chase it around, leaving the food source free for harvest (Fig.
3a, arrows show an example; Video 2, MOP agent). Importantly, this behavior is not restricted to clockwise
rotations, as the agent performs an almost equal number of counterclockwise rotations to free the food area
(Fig. 3c, MOP agent, blue line). The variability of these rotations in the MOP agent are manifest in the
lack of virtually any preferred directionality of movement in the arena at any single position. Indeed, arrows
pointing toward several directions indicate that on average the mouse moves following different paths to get to
the food source (Fig. 3b, MOP agent). Finally, the optimal value function and optimal policy show that the
MOP agent can display deterministic behaviors as a function of internal state as well as distance to the cat
(Fig. 3d): for instance, it prefers running away from the cat when energy is large (right), and it risks getting
7
Figure 4: Dancing of a MOP cartpole. (a) The cart (brown rectangle) has a pole attached. The cartpole reaches an
absorbing state if the magnitude of the angleθ exceeds 36 deg or its position reaches the borders. There are 5 available
actions when alive: a big and a small force to either side (arrows on cartpole), and doing nothing (full circle). (b)
Time-shifted snapshots of the pole in the reference frame of the cart as a function of time for the MOP (top) and R
(bottom) agents. (c) Position and angle occupation for a2 × 105 time step episode. (d) Here, the right half of the
arena is stochastic, while the left remains deterministic. In the stochastic half, the intended state transition due to an
applied action (force) succeeds with probability1 − η (and thus zero force is applied with probabilityη). (e) Fraction
of time spent on the right half of the arena increases as a function ofβ, regardless of the failure probabilityη. (f)
The fraction has a non-monotonic behavior as a function ofη when state entropy is important for the agent (β = 1),
highlighting a stochastic resonance behavior. When the agents do not seek state entropy (β = 0) the fraction of time
spent by the agent on the right decreases with the failure probability, and thus they avoid the stochastic right side.
γ = 0.99 for panels (e,f).
caught to avoid starvation if energy is small (left), both behaviors starkly opposite to stochastic actions.
The behavior of the MOP agent was compared with an R agent that receives a reward of 1 each time it
is alive and 0 otherwise. To promote variable behavior in this agent as well, we implemented anϵ-greedy
action selection (Supplemental Sec. A.5.4), whereϵ was chosen to match the average lifetime of the MOP
agent (Supplemental Fig. 9b). The behavior of the R agent was strikingly less variable than that of the
MOP agent, spending more time close to the food source (Fig. 3b, R agent). Most importantly, while the
MOP agent performs an almost equal number of clock and counterclockwise rotations, the R agent strongly
prefers the clockwise rotations, reaching90% of all observed rotations (Video 3, R-agent; Fig. 3c, orange line).
This shows that the R agent mostly exploits only one strategy to survive and displays a smaller behavioral
repertoire than the MOP agent.
3.3 Dancing in an entropy-seeking cartpole
In the previous examples, complex behaviors emerge as a consequence of the presence of obstacles, predators
and limited food sources, but the actual dynamics of the agents are very coarse-grained. Here, we considered
a system with physically realistic dynamics, the balancing cartpole [74, 75], composed of a moving cart with
an attached pole free to rotate (Fig. 4a). The cartpole is assumed to reach an absorbing state when either it
hits a border, or when the pole angle exceeds36 degrees. Thus, we consider a broad range of angles that
makes the agents reach a larger state space than in standard settings [76]. We discretized the state space and
used a linear interpolation to solve for the optimal value function in Eq. (2.4), and to implement the optimal
8
Figure 5: Modelling altruism through an optimal tradeoff between own action entropy and other’s state entropy. (a)
An agent (gray up triangle) has access to nine movement actions (gray arrows and doing nothing), and open or close a
fence (dashed blue lines). This fence does not affect its movements. A pet (green, down triangle) has access to the
same actions, and chooses one randomly at each timestep, but is constrained by the fence when closed. Pet location is
part of the state of the agent. (b) Asβ in Eq. (2.2) is increased, the agent tends to leave the fence open for a larger
fraction of time. This helps its pet reach other parts of the arena. Error bars correspond to s.e.m. (c) Occupation
heatmaps for 2000 timestep-episodes forβ = 0(left) andβ = 1(right). In all casesα = 1.
policy in Eq. (2.6), (Supplemental Sec. A.5.5). The MOP agent widely occupies the horizontal position, and
more strikingly it produces a wide variety of pole angles, constantly swinging sideways as if it were dancing
(Video 4, MOP agent; Fig. 4b,c).
We compared the behavior of a MOP agent with that of an R agent that receives a reward of 1 for being
alive and 0 otherwise. The R agent gets this reward regardless of the pole angle and cart position within the
allowed broad ranges, so that behaviors of the MOP and R agents can be better compared without explicitly
favoring in any of them any specific behavior, such as the upright pole position. As expected, the R agent
maintains the pole close to the balanced position throughout most of a long episode (Fig. 4b, bottom),
because it is the furthest to the absorbing states and thus the safest. Therefore, the R agent produces very
little behavioral variability (Fig. 4c, right panel) and no movement that could be dubbed ‘dancing’ (Video 4,
R agent). Although both MOP and R agents use a similar strategy which keeps the pole pointing towards
the center for substantial amounts of time (Fig. 4c, positive angles correlate with positive positions in both
panels), the behavior of the R agent is qualitatively different, and is best described as a bang-bang sort of
control for which the angle is kept very close to zero while the cart is allowed to travel and oscillate around
the origin, which is more apparent in the actual paths of the agent (see trajectories in phase space in Video
5). We also find that the R agent does not display much variability in state space even after using anϵ-greedy
action selection (Supplemental Fig. 10, Video 6), withϵ chosen to match average lifetimes between agents
(Supplemental Fig. 9c). This result showcases that the MOP agent exhibits the most appropriate sort of
variability for a given average lifetime.
We finally introduced a slight variation to the environment, where the right half of the arena has stochastic
state transitions. Here, when agents choose an action (force) to be executed, a state transition in the desired
direction occurs with probability1 − η, and a transition corresponding to zero force occurs with probabilityη
(Fig. 4d). Therefore, a MOP agent that seeks state entropy (β >0) will show a preference for the right side,
where there is in principle higher state entropy resulting from the stochastic transitions over more successor
states than on the left side. Indeed, we find that MOP agents spend more time on the right side asβ increases,
regardless of the probabilityη (Fig. 4e). For fixedγ, spending more time on the right side can bring the life
expectancy to decrease significantly depending onβ and η (Supplemental Fig. 9 d-e). Interestingly, forβ >0
there is an optimal value of the noiseη that maximizes the fraction of time spent on the right side (Fig. 4f),
which is a form of stochastic resonance. Therefore, for differentβ, qualitatively different behaviors emerge as
a function of the noise levelη.
3.4 MOP agents can also seek entropy of others
Next, we considered an example where an agent seeks to occupy path space, which includes another agent’s
location as well as its own. The agent can freely move (Fig. 5a; grey triangle) and open or close a fence by
pressing a lever in a corner (blue triangle). The pet of the agent (green triangle) can freely move if the fence is
9
open, but when the fence is closed the pet is confined to move in the region where it is currently located. The
pet moves randomly at each step, but its available actions are restricted by its available space (Supplemental
Sec. A.5.6).
To maximize action-state path entropy, the agent ought to trade off the state entropy resulting from
letting the pet free with the action entropy resulting from using the open-close action when visiting the lever
location. The optimal tradeoff depends on the relative strength of action and state entropies. In fact, when
state entropy weighs as much as action entropy (α = β = 1), the fraction of time that the agent leaves the
fence open is close to1 (rightmost point in Fig. 5b) so that the pet is free to move (Fig. 5c, right panel;β = 1
MOP agent). However, when the state entropy has zero weight (α = 1, β= 0), the fraction of time that the
fence remains open is close to0.5 (leftmost point in Fig. 5b) and the pet remains confined to the right side
for most of the time (Fig. 5c, left panel;β = 0 MOP agent), the region where it was initially placed. As a
function ofβ the fraction of time the fence is open increases. Therefore, the agent gives more freedom to
its pet, as measured by the pet’s state entropy, by curtailing its own action freedom, as measured by action
entropy, thus becoming more "altruistic".
3.5 MOP compared to other reward-free approaches
One important question is how MOP compares to other reward-free, motivation-driven theories of behavior.
Here we focus on two popular approaches: empowerment and the free energy principle. In empowerment
(MPOW) [20, 64–66] agents maximize the mutual information betweenn-step actions and the successor
states resulting from them [20, 77], a measure of their capability to perform diverse courses of actions with
predictable consequences. MPOW formulates behavior as greedy maximization of empowerment [20, 64], such
that agents move to accessible states with the largest empowerment (maximal mutual information), and stay
there with high probability.
We applied MPOW to the gridworld and cartpole environments (Fig. 6). In the gridworld, MPOW agents
(5-step MPOW, see Supplemental Sec. A.7.1) prefer states from where they can reach many distinct states,
such as the middle of a room. However, due to energetic constraints, they also gravitate towards the food
source when energy is low, and they alternate between these two locationsad nauseam (Fig. 6a, middle;
Video 7). In the cartpole, MPOW agents (3-step MPOW [64], see Supplemental Sec. A.7.1) favour the upright
position because, being an unstable fixed point, it is the state with highest empowerment, as previously
reported [64, 78]. Given the unstable equilibrium, the MPOW agent gets close to it but needs to continuously
adjust its actions when greedily maximizing empowerment (Fig. 6b, middle; Video 8). The paths traversed by
MPOW agents in state space are highly predictable, and they are similar to the ones of the R agent (see Fig.
4c). The only source of stochasticity comes from the algorithm, which approximately calculates empowerment,
and thus a more precise estimation of empowerment leads to even less variability.
In the free energy principle (FEP), agents seek to minimize the minus log probability, called surprise, of
a subset of desired states via the minimization of an upper bound, called free energy. This minimization
reduces behavioral richness by making a set of desired (homeostatic) states highly likely [67, 68], rendering
this approach almost opposite to MOP. In a recent MDP formalization, FEP agents aim to minimize the
(future) expected free energy (EFE) [79], which equals the future cumulative KL divergence between the
probability of states and the desired (target) probability of those states (see Supplemental Sec. A.7.2 for
details). Even though this objective contains the standard exploration entropy term on state transitions
[68, 80], we prove that the optimal policy is deterministic (see Supplemental Sec. A.7.2).
As a consequence, we find that in both the gridworld and cartpole environments, the behavior of the
EFE agent (receding horizonH = 200) is much less variable than the MOP agent in general (Fig. 6a, right
panel for the gridworld, Video 7; and b, right panel, for the cartpole, Video 8). The only source of action
variability in the EFE agent is due to the degeneracy of the expected free energy, and thus behavior collapses
to a deterministic policy as soon as the target distribution is not perfectly uniform (see Supplemental Sec.
A.7.2.2 for details). We finally prove that under discounted infinite horizon, and assuming a deterministic
environment, the EFE agent is equivalent to a classical reward maximizer agent with rewardR = 1 for all
non-absorbing states andR = 0 for the absorbing states (Supplemental Sec. A.7.2). In conclusion, MOP
generates much more variable behaviors than MPOW and FEP.
10
Figure 6: Empowerment (MPOW) and Free Energy Principle (FEP) lack robust occupation of action-states. (a) In the
grid-world environment, MPOW and expected free energy (EFE) only visit a restricted portion of the arena. Initial
position was the center of a room(x, y) = (3, 3). (b) In the cartpole environment, both MPOW and EFE shy away
from large angles, producing a limited repertoire of predictable behaviors.
3.6 MOP in continuous and large action-state spaces
The examples so far can be solved exactly with numerical methods, without relying on function approximation
of the value function or the policy, which could obscure the richness of the resulting behaviors. However, one
important question is whether our approach scales up to large continuous action-state spaces where no exact
solutions are available. To show that MOP generates rich behaviors even in high-dimensional agents, we used
a quadruped from Gymnasium [81] without imposing any explicit fine-tuned reward function (Fig. 7a). The
only externally imposed conditions are the absorbing states, which are reached when either the agent falls
(given by the torso touching the ground), or the torso reaches a maximum height [81].
We first trained the MOP agent by approximating the state-value function, Eq. (2.5), using the soft-actor
critic (SAC) architecture [40] with zero rewards, which corresponds to the caseα = 1 and β = 0. The MOP
agent learns to stabilize itself and walk around, sometimes jumping, spinning and moving up and down the
legs, without any instructions to do so (Video 9). The MOP agent exhibits variable and long excursions over
state space (Fig. 7b,c blue) and displays a broad distribution of speeds (Fig. 7d, blue). We compared the
MOP agent with an R agent that obtains a reward ofR = 1 whenever it is alive andR = 0 when it reaches
an absorbing state. As before, we add variability to the R agent with anϵ-greedy action selection, adjusting
ϵ so that the average lifetime of the R agent matched that of the MOP agent (Supplemental Fig. 11a). In
contrast to the MOP agent, the R agents exhibit much shorter excursions (Fig. 7b,c yellow) and a velocity
distribution that peaks around zero, indicating prolonged periods spent with no translational movement (Fig.
7d, yellow). When visually compared, the behavior for MOP and R agents shows stark differences (Video 9).
While the MOP agent elicits variable behaviors, it is also capable of generating deterministic, goal-directed
behaviors when needed. To show this, we added a food source in the arena and extended the state of the
agent with its internal energy. Now the agent can also die of starvation when the internal energy hits zero
(Fig. 7e). As expected, when the initial location of the MOP quadruped is far from the food source, it directly
moves to the food source to avoid dying from starvation (Fig. 7f). After the food source is reached for the
first time, the MOP quadruped generates random excursions away from the food source. During these two
phases, the agent displays very different speed distributions (Fig. 7g), showing also quantitative differences in
the way it moves (see a comparison with the R agent in Supplemental Fig. 11, and Video 10).
Finally, we modified the environment by adding state transition noise of various magnitudes in one half of
the arena (x >0), while the other half remained deterministic. We find that the agent’s behavior is modulated
by β, which controls the preference of state transition entropy (see details in Supplemental Sec. A.5.7). As
expected, for fixedα and positive noise magnitude, MOP agents show increasing preference toward the noisy
11
Figure 7: MOP in high-dimensional states generates variable and goal-directed behaviors. (a) The quadruped
environment, adapted from Gymnasium, serves as the testing environment. Thex, ydimensions are unbounded. (b)
Trajectories of the center of mass of the torso of the MOP (left panel) and R (right) agents. MOP occupies more
space for approximately the same survival time (see Supplemental Fig. 11a). (c-d) Distribution of the planar distance
d from the origin (c) and planar speedvxy (d) for MOP (blue) and R (yellow) agents. (e) In a new environment, a
food source (green ball) is available so that the MOP agent can replenish its internal energy to avoid starvation. (f)
Trajectories of the MOP agent before (left) and after (right) getting to the food source. Colormap defined by the
energy level of the agent. (g) Distribution of the planar speed showcasing changes before (dark blue) and after (light
blue) the MOP agent reaches the food source for the first time. Distributions computed only on the tests where the
quadruped finds the food source.
side asβ increases (Supplemental Fig. 12). However, as noise magnitude increases, and for fixedβ, MOP
agents tend to avoid the noisy side to prevent them from falling. This shows that MOP agents can exhibit
approach and avoidance behaviors depending on the environment’s stochasticity and theirβ hyperparameter.
4 Discussion
Often, the success of agents in nature is not measured by the amount of reward obtained, but by their
ability to expand in state space and perform complex behaviors. Here we have proposed that a major goal of
intelligence is to ‘occupy path space’. Extrinsic rewards are thus the means to move and occupy action-state
path space, not the goal of behavior. In an MDP setting, we have shown that the intuitive notion of path
occupancy is captured by future action-state path entropy, and we have proposed that behavior is driven
by the maximization of this intrinsic goal –the maximum occupancy principle (MOP). We have solved the
associated Bellman equation and provided a convergent iterative map to determine the optimal policy.
In several discrete and continuous state examples we have shown that MOP, along with the agent’s
constraints and dynamics, leads to complex behaviors that are not observed in other simple reward maximizing
agents. Quick filling of physical space by a moving agent, hide-and-seek behavior and variable escaping
routes in a predator-prey example, dancing in a realistic cartpole dynamical system, altruistic behavior in
an agent-and-pet duo and successful, vigorous movement in a high-dimensional quadruped are all behaviors
that strike as being playful, curiosity-driven and energetic. To the human eye, these behaviors look genuinely
goal-directed, like approaching to the food source when the energy level is low or escaping from the cat when
12
it gets close to the mouse (see Figs. 2c and 3d). Although MOP agents do not have any extrinsically designed
goal, like eating or escaping, they generate these deterministic, goal-directed behaviors whenever necessary so
that they can keep moving in the future and maximize future path action-state entropy (see Supplemental Sec.
A.8). These results show that the presence of internal states (e.g. energy) and absorbing states (e.g, having
zero energy or being eaten) are critical for generating interesting behaviors, as getting close to different types
of absorbing states triggers qualitatively different behaviors. This capability of adapting variability depending
on internal states has been overlooked in the literature and is essential to obtaining the goal-directed behaviors
we have shown here. In parallel, when basic energy and safety conditions are met, behaviors are lively and
somewhat risky, like when the cartpole gets close to the borders of the arena, and therefore our approach can
lead to novel ways of thinking about risk–seeking behaviors [82].
A related set of algorithms, known as empowerment, have also proposed using reward-free objectives as
the goal of behavior [20, 64, 66]. In this approach, the mutual information between a sequence of actions and
the final state is maximized. This makes empowerment agents prefer states where actions lead to large and
predictable changes, such as unstable fixed points [64]. We have shown that one drawback is that empowered
agents tend to remain close to those states without producing diverse behavioral repertoires (see Fig. 6b
and Video 8), as it also happens in causal entropy approaches [83]. Another difference is that empowerment
is not additive over paths because the mutual information of a path of actions with the path of states is
not the sum of the per-step mutual information, and thus it cannot be formalized as a cumulative per-step
objective (Supplemental Sec. A.9) [64, 66, 72, 84], in contrast to action-state path entropy. We note, however,
that an approximation to empowerment having the desired additive property could be obtained from our
framework by puttingβ <0 in Eq. (2.2), such that more predictable state transitions are preferred. Similarly
to empowerment, we have also shown that agents following the free energy principle [67, 79] collapse behavior
to deterministic policies in known environments (see Fig. 6b and Video 8). Other reward-free RL settings
and pure exploration objectives have been proposed in the past [48, 50, 52, 58, 85–88], but this body of
work typically investigates how to efficiently sample MDPs to construct near-optimal policies when reward
functions are introduced in the exploitation phase. More importantly, this work differs from ours in that the
goal-directedness that MOP displays entails behavioral variability at its core, even in known environments
(see examples above). Finally, other overlapping reward-free approaches focus on the unsupervised discovery
of skills, by encouraging diversity [26, 89–91]. While the motivation is similar, they focus on skill-conditioned
policies, whereas our work demonstrates that complex sequences of behaviors are possible working from the
primitive actions of agents, although a possible future avenue for MOP is to apply it to temporally extended
actions [92]. In addition, these works define tasks based on extrinsic rewards, whereas we have shown that
internal state signals are sufficient to let agents define sub-tasks autonomously.
Our approach is conceptually different as well to hybrid approaches that combine extrinsic rewards
with action entropy or KL regularization terms [37, 38, 41, 43, 93] for two main reasons. First, entropy
seeking behavior does not pursue any form of extrinsic reward maximization. But most importantly, using
KL-regularization using a default policyπ0(a|s) in our framework would be self-defeating. This is because
the absolute action entropy termsH(A|s) in the expected return in Eq. (2.2) favor visiting states where a
large set of immediate and future action-states are accessible. In contrast, using relative action entropy (KL)
precludes this effect by normalizing the number of accessible actions, as we have shown above. Additionally,
minimizing the KL divergence with a uniform default policy and without extrinsic rewards leads to an optimal
policy that is uniform regardless of the presence of absorbing states, equivalent to a random walker, which
shows that a pure KL objective does not lead to interesting behaviors (Supplemental Sec. A.6, Supplemental
Fig. 13). The idea of having a variable number of actions that depend on the state is consistent with the
concept of affordance [94]. While we do not address the question of how agents get the information about
the available actions, an option would be to use the notion of affordances as actions [95]. Secondly, while
previous work has studied the performance benefits of either action [40], state [42, 48] or equally weighted
action-state [44, 96] steady-state entropies, our work proposes mixing them arbitrarily through path entropy,
leading to a more general theory without any loss in mathematical tractability [47]. This arbitrary weight
mixing lets us model more diverse phenomena. For example, for the right combination of state entropy weight
β, and lookahead horizon, controlled byγ, MOP agents could get stuck in a noisy TV, consistent with the
observation that humans have a preference for noisy TVs under particular conditions [97]. However, it can
also capture the avoidance of noisy TVs for sufficiently-long-sighted agents (see Fig. 2e).
We have also shown that MOP is scalable to high-dimensional problems and when the state-transition
13
matrix is unknown, using the soft-actor critic architecture [98] to approximate the optimal policy prescribed
by MOP. Nevertheless, several steps remain to have a more complete MOP theory with learning. Previous
related attempts have introduced Z-learning [37, 70] and G-learning [99] using off-policy methods, so our
results could be extended to learning following similar lines. Other possibilities are using transition estimators
using counts or pseudo-counts [60], or hashing [61], for the learning of the transition matrices. One potential
advantage of our framework is that, as entropy-seeking behavior obviates extrinsic rewards, those rewards do
not need to be learned and optimized, and thus the learning problem reduces to transition matrices learning.
In addition, modeling and injecting prior information could be particularly simple in our setting in view
that intrinsic entropy rewards can be easily bounded before the learning process if action space is known.
Therefore, initializing the state-value function to the lower or upper bounds of the action-state path entropy
could naturally model pessimism or optimism during learning, respectively.
All in all, we have introduced MOP as a novel theory of behavior, which promises new ways of understanding
goal-directedness without reward maximization, and that can be applied to artificial agents to discover by
themselves ways of surviving and occupying action-state space.
Acknowledgments
This work is supported by the Howard Hughes Medical Institute (HHMI, ref 55008742), ICREA Academia
2022 and MINECO (Spain; BFU2017-85936-P) to R.M.-B, and MINECO/ESF (Spain; PRE2018-084757) to
J.R.-R. We thank Jose Apesteguia, Luca Bonatti, Ignasi Cos, and Benjamin Hayden for very useful comments.
Code and data availability
The code to generate the results and various figures is available as Python and Julia code along with guided note-
books to reproduce the figures at this public GitHub repository:
https://github.com/jorgeerrz/occupancy_max_paper. All data are in this public repository, except
for the specific data for the ant experiment, which are available upon request.
References
[1] Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic definitions and new
directions. Contemporary educational psychology, 25(1):54–67, 2000. Publisher: Elsevier.
[2] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous
mental development.IEEE transactions on evolutionary computation, 11(2):265–286, 2007. Publisher:
IEEE.
[3] Karen E Adolph and Sarah E Berger. Motor development. Handbook of child psychology, 2, 2007.
Publisher: Wiley Online Library.
[4] Peter F MacNeilage and Barbara L Davis. On the origin of internal structure of word forms.Science,
288(5465):527–531, 2000. Publisher: American Association for the Advancement of Science.
[5] Laura Ann Petitto and Paula F Marentette. Babbling in the manual mode: Evidence for the ontogeny of
language. Science, 251(5000):1493–1496, 1991. Publisher: American Association for the Advancement of
Science.
[6] Arne Dietrich. The cognitive neuroscience of creativity.Psychonomic bulletin & review, 11(6):1011–1026,
2004. Publisher: Springer.
[7] Celeste Kidd and Benjamin Y Hayden. The psychology and neuroscience of curiosity.Neuron, 88(3):
449–460, 2015. Publisher: Elsevier.
[8] Jacqueline Gottlieb, Pierre-Yves Oudeyer, Manuel Lopes, and Adrien Baranes. Information-seeking,
curiosity, and attention: computational and neural mechanisms.Trends in cognitive sciences, 17(11):
585–593, 2013. Publisher: Elsevier.
14
[9] John Gittins, Kevin Glazebrook, and Richard Weber.Multi-armed bandit allocation indices. John Wiley
& Sons, 2011.
[10] Bruno B Averbeck. Theory of choice in bandit, information sampling and foraging tasks. PLoS
computational biology, 11(3):e1004164, 2015. Publisher: Public Library of Science San Francisco, CA
USA.
[11] Bradley B Doll, Dylan A Simon, and Nathaniel D Daw. The ubiquity of model-based reinforcement
learning. Current opinion in neurobiology, 22(6):1075–1081, 2012. Publisher: Elsevier.
[12] Maya Zhe Wang and Benjamin Y Hayden. Latent learning, cognitive maps, and curiosity.Current
Opinion in Behavioral Sciences, 38:1–7, 2021. Publisher: Elsevier.
[13] John Von Neumann and Oskar Morgenstern.Theory of games and economic behavior. Princeton university
press, 2007.
[14] Richard S Sutton, Andrew G Barto, and others. Introduction to reinforcement learning. 1998. Publisher:
MIT press Cambridge.
[15] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. InHandbook
of the fundamentals of financial decision making: Part I, pages 99–127. World Scientific, 2013.
[16] David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artifi-
cial Intelligence, 299:103535, 2021. URL https://www.sciencedirect.com/science/article/pii/
S0004370221000862. Publisher: Elsevier.
[17] Carla J Rash, Jeremiah Weinstock, and Ryan Van Patten. A review of gambling disorder and substance
use disorders. Substance abuse and rehabilitation, 7:3, 2016. Publisher: Dove Press.
[18] Tamás Ágh, Gábor Kovács, Dylan Supina, Manjiri Pawaskar, Barry K Herman, Zoltán Vokó, and David V
Sheehan. A systematic review of the health-related quality of life and economic burdens of anorexia
nervosa, bulimia nervosa, and binge eating disorder.Eating and Weight Disorders-Studies on Anorexia,
Bulimia and Obesity, 21(3):353–364, 2016. Publisher: Springer.
[19] John M McNamara and Alasdair I Houston. The common currency for behavioral decisions.The
American Naturalist, 127(3):358–378, 1986. Publisher: University of Chicago Press.
[20] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-
centric measure of control. In2005 ieee congress on evolutionary computation, volume 1, pages 128–135.
IEEE, 2005.
[21] Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty
alone. Evolutionary computation, 19(2):189–223, 2011. Publisher: MIT Press.
[22] Satinder Singh, Richard L Lewis, and Andrew G Barto. Where do rewards come from. InProceedings of
the annual conference of the cognitive science society, pages 2601–2606. Cognitive Science Society, 2009.
[23] Tony Zhang, Matthew Rosenberg, Pietro Perona, and Markus Meister. Endotaxis: A Universal Algorithm
for Mapping, Goal-Learning, and Navigation.bioRxiv, 2021. Publisher: Cold Spring Harbor Laboratory.
[24] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural
controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to
animats, pages 222–227, 1991.
[25] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward
design. Advances in neural information processing systems, 30, 2017.
[26] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function.arXiv preprint arXiv:1802.06070, 2018.
15
[27] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,
Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, and others. Mastering atari, go, chess
and shogi by planning with a learned model.Nature, 588(7839):604–609, 2020. Publisher: Nature
Publishing Group.
[28] TB Asafa, TM Afonja, EA Olaniyan, and HO Alade. Development of a vacuum cleaner robot.Alexandria
engineering journal, 57(4):2911–2920, 2018. Publisher: Elsevier.
[29] Stephen J Kline and Nathan Rosenberg. An overview of innovation.Studies on science and the innovation
process: Selected works of Nathan Rosenberg, pages 173–203, 2010. Publisher: World Scientific.
[30] Robert C Wilson, Elizabeth Bonawitz, Vincent D Costa, and R Becket Ebitz. Balancing exploration and
exploitation with information and randomization.Current opinion in behavioral sciences, 38:49–56, 2021.
Publisher: Elsevier.
[31] Rubén Moreno-Bote, David C Knill, and Alexandre Pouget. Bayesian sampling in visual perception.
Proceedings of the National Academy of Sciences, 108(30):12491–12496, 2011. Publisher: National Acad
Sciences.
[32] StefanoRecanatesi, UlisesPereira-Obilinovic, MasayoshiMurakami, ZacharyMainen, andLucaMazzucato.
Metastable attractors explain the variable timing of stable behavioral action sequences.Neuron, 110(1):
139–153, 2022. Publisher: Elsevier.
[33] Abel Corver, Nicholas Wilkerson, Jeremiah Miller, and Andrew Gordus. Distinct movement patterns
generate stages of spider web building.Current Biology, 31(22):4983–4997, 2021. Publisher: Elsevier.
[34] Paule Dagenais, Sean Hensman, Valérie Haechler, and Michel C Milinkovitch. Elephants evolved strategies
reducing the biomechanical complexity of their trunk.Current Biology, 31(21):4727–4737, 2021. Publisher:
Elsevier.
[35] Gabriela Mochol, Roozbeh Kiani, and Rubén Moreno-Bote. Prefrontal cortex represents heuristics that
shape choice bias and its integration into future behavior.Current Biology, 31(6):1234–1244, 2021.
Publisher: Elsevier.
[36] Fanny Cazettes, Masayoshi Murakami, Alfonso Renart, and Zachary F Mainen. Reservoir of decision
strategies in the mouse brain.bioRxiv, 2021. Publisher: Cold Spring Harbor Laboratory.
[37] Emanuel Todorov. Efficient computation of optimal actions.Proceedings of the national academy of
sciences, 106(28):11478–11483, 2009. Publisher: National Acad Sciences.
[38] Brian D Ziebart.Modeling purposeful adaptive behavior with the principle of maximum causal entropy.
Carnegie Mellon University, 2010.
[39] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with Deep
Energy-Based Policies, July 2017. URLhttp://arxiv.org/abs/1702.08165. arXiv:1702.08165 [cs].
[40] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. InInternational conference on machine
learning, pages 1861–1870. PMLR, 2018.
[41] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440, 2017.
[42] Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798, 2017.
[43] Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning
an embedding space for transferable robot skills. InInternational Conference on Learning Representations,
2018.
16
[44] Naftali Tishby and Daniel Polani. Information theory of decisions and actions. InPerception-action
cycle, pages 601–636. Springer, 2011.
[45] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value
and policy based reinforcement learning.Advances in neural information processing systems, 30, 2017.
[46] Alexandre Galashov, Siddhant M Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz,
Guillaume Desjardins, Wojciech M Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess.
Information asymmetry in KL-regularized RL.arXiv preprint arXiv:1905.01240, 2019.
[47] Dmytro Grytskyy, Jorge Ramírez-Ruiz, and Rubén Moreno-Bote. A general Markov decision process
formalism for action-state entropy-regularized reward maximization, February 2023. URLhttp://arxiv.
org/abs/2302.01098. arXiv:2302.01098 [cs].
[48] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy
exploration. In International Conference on Machine Learning, pages 2681–2691. PMLR, 2019.
[49] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training.Advances in
Neural Information Processing Systems, 34:18459–18473, 2021.
[50] Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gradient of a
non-parametric state entropy estimate. InProceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 9028–9036, 2021. Issue: 10.
[51] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy
maximization with random encoders for efficient exploration. InInternational Conference on Machine
Learning, pages 9443–9454. PMLR, 2021.
[52] Chuheng Zhang, Yuanying Cai, Longbo Huang, and Jian Li. Exploration by maximizing Rényi entropy for
reward-free RL framework. InProceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 10859–10867, 2021. Issue: 12.
[53] Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A Survey of
Exploration Methods in Reinforcement Learning.arXiv preprint arXiv:2109.00157, 2021.
[54] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018.
[55] Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning.
arXiv preprint arXiv:1703.01732, 2017.
[56] Zafeirios Fountas, Noor Sajid, Pedro Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlo methods. Advances in neural information processing systems, 33:11662–11675, 2020.
[57] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. Large-
Scale Study of Curiosity-Driven Learning. InICLR, 2019.
[58] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. InInternational conference on machine learning, pages 2778–2787. PMLR,
2017.
[59] Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action
and Perception as Divergence Minimization, February 2022. URLhttp://arxiv.org/abs/2009.01791.
arXiv:2009.01791 [cs, math, stat].
[60] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation.Advances in neural information processing
systems, 29, 2016.
17
[61] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-
man, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning.Advances in neural information processing systems, 30, 2017.
[62] Arthur Aubret, Laetitia Matignon, and Salima Hassas. An Information-Theoretic Perspective on Intrinsic
Motivation in Reinforcement Learning: A Survey.Entropy, 25(2):327, February 2023. ISSN 1099-4300.
doi: 10.3390/e25020327. URL https://www.mdpi.com/1099-4300/25/2/327. Number: 2 Publisher:
Multidisciplinary Digital Publishing Institute.
[63] Juergen Schmidhuber. Driven by Compression Progress: A Simple Principle Explains Essential Aspects
of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science,
Music, Jokes, April 2009. URLhttp://arxiv.org/abs/0812.4360. arXiv:0812.4360 [cs].
[64] Tobias Jung, Daniel Polani, and Peter Stone. Empowerment for continuous agent—environment systems.
Adaptive Behavior, 19(1):16–39, 2011. Publisher: SAGE Publications Sage UK: London, England.
[65] Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory in Biosciences, 131(3):139–148, 2012. Publisher: Springer.
[66] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning.Advances in neural information processing systems, 28, 2015.
[67] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
Physiology-Paris, 100(1-3):70–87, July 2006. ISSN 09284257. doi: 10.1016/j.jphysparis.2006.10.001. URL
https://linkinghub.elsevier.com/retrieve/pii/S092842570600060X.
[68] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle
for action and perception: A mathematical review. Journal of Mathematical Psychology, 81:55–79,
December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URLhttps://www.sciencedirect.
com/science/article/pii/S0022249617300962.
[69] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal,
27(3):379–423, 1948. URLhttps://ieeexplore.ieee.org/abstract/document/6773024/. Publisher:
Nokia Bell Labs.
[70] Emanuel Todorov. Linearly-solvable Markov decision problems.Advances in neural information processing
systems, 19, 2006.
[71] Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in MDPs. InDecision
Making with Imperfect Decision Makers, pages 57–74. Springer, 2012.
[72] Felix Leibfried, Sergio Pascual-Díaz, and Jordi Grau-Moya. A Unified Bellman Optimality Principle
Combining Reward Maximization and Empowerment. InAdvances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019. URLhttps://proceedings.neurips.cc/paper_
files/paper/2019/hash/13384ffc9d8bdb21c53c6f72d46f7866-Abstract.html.
[73] Jürgen Schmidhuber. Curious model-building control systems. InProc. international joint conference on
neural networks, pages 1458–1463, 1991.
[74] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):
834–846, 1983. Publisher: IEEE.
[75] Razvan V Florian. Correct equations for the dynamics of the cart-pole system.Center for Cognitive and
Neural Studies (Coneural), Romania, 2007. Publisher: Citeseer.
[76] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym.arXiv preprint arXiv:1606.01540, 2016.
18
[77] R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on
Information Theory, 18(4):460–473, July 1972. ISSN 1557-9654. doi: 10.1109/TIT.1972.1054855.
Conference Name: IEEE Transactions on Information Theory.
[78] Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Keep Your Options Open: An
Information-Based Driving Principle for Sensorimotor Systems.PLOS ONE, 3(12):e4018, December 2008.
ISSN 1932-6203. doi: 10.1371/journal.pone.0004018. URL https://journals.plos.org/plosone/
article?id=10.1371/journal.pone.0004018. Publisher: Public Library of Science.
[79] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. Reward Maximization
Through Discrete Active Inference.Neural Computation, 35(5):807–852, April 2023. ISSN 0899-7667.
doi: 10.1162/neco_a_01574. URLhttps://doi.org/10.1162/neco_a_01574.
[80] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement learning
through active inference.arXiv preprint arXiv:2002.12636, 2020.
[81] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu,
Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré,
Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023.
URL https://zenodo.org/record/8127025.
[82] Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-Sensitive Reinforcement
Learning: Near-Optimal Risk-Sample Tradeoff in Regret. InAdvances in Neural Information Processing
Systems, volume 33, pages 22384–22395. Curran Associates, Inc., 2020. URLhttps://proceedings.
neurips.cc/paper/2020/hash/fdc42b6b0ee16a2f866281508ef56730-Abstract.html.
[83] Alexander D Wissner-Gross and Cameron E Freer. Causal entropic forces.Physical review letters, 110
(16):168702, 2013. Publisher: APS.
[84] Nicola Catenacci Volpi and Daniel Polani. Goal-Directed Empowerment: Combining Intrinsic Motivation
and Task-Oriented Behaviour.IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL
SYSTEMS.
[85] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.
Efficient exploration via state marginal matching.arXiv preprint arXiv:1906.05274, 2019.
[86] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning, pages 4870–4879. PMLR,
2020.
[87] Mirco Mutti and Marcello Restelli. An intrinsically-motivated approach for learning highly exploring and
fast mixing policies. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
5232–5239, 2020. Issue: 04.
[88] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems.
arXiv preprint arXiv:2103.06257, 2021.
[89] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational Intrinsic Control, November
2016. URL http://arxiv.org/abs/1611.07507. arXiv:1611.07507 [cs].
[90] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-Aware
Unsupervised Skill Discovery.International Conference on Learning Representations, April 2020. MAG
ID: 2995736683 S2ID: ae3b2768b0a3c73410bce0d2ae03feaf01f6f864.
[91] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-Aware Unsupervised Skill
Discovery, February 2023. URLhttp://arxiv.org/abs/2302.05103. arXiv:2302.05103 [cs].
[92] Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a
framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):
181–211, August 1999. doi: 10.1016/s0004-3702(99)00052-1. MAG ID: 2109910161 S2ID:
0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d.
19
[93] Jordi Grau-Moya, Felix Leibfried, Tim Genewein, and Daniel A. Braun. Planning with Information-
Processing Constraints and Model Uncertainty in Markov Decision Processes, April 2016. URLhttp:
//arxiv.org/abs/1604.02080. arXiv:1604.02080 [cs].
[94] James J. Gibson. The Ecological Approach to Visual Perception: Classic Edition. Psychology Press,
November 2014. ISBN 978-1-317-57937-3. Google-Books-ID: QReLBQAAQBAJ.
[95] Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina Precup. What can i do
here? a theory of affordances in reinforcement learning. InInternational Conference on Machine Learning,
pages 5243–5253. PMLR, 2020. URLhttps://proceedings.mlr.press/v119/khetarpal20a.html.
[96] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. InProceedings
of the AAAI Conference on Artificial Intelligence, volume 24, pages 1607–1612, 2010. URLhttps:
//ojs.aaai.org/index.php/AAAI/article/view/7727. Issue: 1.
[97] Alireza Modirshanechi, Wei-Hsiang Lin, He A. Xu, Michael H. Herzog, and Wulfram Gerstner. The curse
of optimism: a persistent distraction by novelty, June 2023. URLhttps://www.biorxiv.org/content/
10.1101/2022.07.05.498835v2. Pages: 2022.07.05.498835 Section: New Results.
[98] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic Algorithms
and Applications, January 2019. URLhttp://arxiv.org/abs/1812.05905. arXiv:1812.05905 [cs, stat].
[99] Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates.
arXiv preprint arXiv:1512.08562, 2015.
20
A Appendix
A.1 Entropy measures the occupancy of action-state paths
In this section, we show that entropy is the only measure of action-state path occupancy that obeys some
basic intuitive notions of occupancy. We first list the intuitive conditions in mathematical form, present the
main theorem and then discuss some implications through some corollaries.
We consider a time-homogeneous Markov decision process with finite state setS and finite action setA(s)
for every states ∈ S. Henceforth, the action-statexj = (aj, sj) is any joint pair of one available actionaj
and one possible successor statesj that results from making that action under policyπ ≡ {π(a|s)} from the
action-state xi = (ai, si). By assumption, the availability of actionaj depends on the previous statesi alone,
not onai. Thus, the transition probability fromxi to xj in one time step ispij = π(aj|si)p(sj|si, aj), where
p(sj|si, ai) is the conditional probability of transitioning from statesi to sj given that actionaj is performed.
Although there is no dependence of the previous actionai on this transition probability, it is notationally
convenient to define transitions between action-states. We conceive of rational agents as maximizing future
action-state path occupancy. Any measure of occupancy should obey the intuitive Conditions1-4 listed below.
Intuitive Conditions for a measure of action-state occupancy:
1. Occupancy gain of action-statexj from xi is a function of the transition probabilitypij, C(pij)
2. Performing a low probability transition leads to a higher occupancy gain than performing a high probability
transition, that is,C(pij) decreases withpij
3. The first order derivativeC′(pij) is continuous forpij ∈ (0, 1)
4. (Definition: the action-state occupancy of a one-step path from action-statexi is the expectation over
occupancy gains of the immediate successor action-states,C(1)
i ≡ P
j pijC(pij))
The action-state occupancy of a two-steps path is additive,
C(2)
i ≡ P
jk pijpjkC(pijpjk) = C(1)
i + P
j pijC(1)
j
for any choice of thepij and initialxi
Condition 1 simply states that occupancy gain from an initial action-state is defined over the transition
probabilities to successor action-states in a sample space. Condition2 implies that performing a low probability
transition leads to a higher occupancy of the successor states than performing a high probability transition.
This is because performing a rare transition allows the agent to occupy a space that was left initially unoccupied.
Condition 3 imposes smoothness of the measure.
In Condition4 we have defined the occupancy of the successor action-states (one-step paths) in the Markov
chain as the expected occupancy gain. Condition4 is the central property, and it imposes that the occupancy
of action-states paths with two steps can be broken down into a sum of the occupancies of action-states at
each time step. Note that the action-state path occupancy can be written as
C(2)
i ≡
X
jk
pijpjkC(pijpjk) =
X
j
pijC(pij) +
X
jk
pijpjkC(pjk) =
X
jk
pijpjk (C(pij) + C(pjk)) ,
which imposes a strong condition on the functionC(p). Note also that the sumP
jk pijpjkC(pijpjk) extends
the notion of action-state to a path of two consecutive action-states, each path having probabilitypijpjk due
to the (time-homogeneous) Markov property. The last equality is an identity. While here we consider paths of
length equal to2, further below we show that there is no difference in imposing additivity to paths of any
fixed or random length (Corollary 2).
Theorem 1. C(p) = −k ln p with k >0 is the only function that satisfies Conditions1-4
Corollary 1. The entropyC(1)
i = −k P
j pij ln pij is the only measure of action-state occupancy of successor
action-states xj from xi with transition probabilitiespij consistent with Conditions1-4.
21
Proof. Put p1,1 = 1 and p1,j = 0 for j ̸= 1. Then, Condition4 reads C(1) = C(1) + C(1) when the initial
action-state isx1, which impliesC(1) = 0.
Now, take a Markov chain withp0,0 = 1, p1,0 = 1 − t >0, p1,2 = t >0, p2,0 = p2,1 = 0, p2,j = 1/n for
j = 3, ..., n+ 2 and n >0, andpk,0 = 1 for k = 3, ..., n+ 2. In this chain, the state0 is absorbing and all others
are transient (here action-states are simply referred to as states). Starting from state1, transition to the
transient state2 happens with probabilityt and to the absorbing state0 with probability1 − t. From state2
a transition to statesj = 3, ..., n+ 2 happens with equal probability. From any of those states, a deterministic
transition to0 ensues. (These last transitions can only happen in the third time step, and although it will be
relevant later on, it is no used in the current proof, which only uses additivity on paths of length two.) Then,
Condition 4 with initial state1 reads tC(t/n)+(1 −t)C(1−t) = tC(t)+(1 −t)C(1−t)+ tC(1/n)+(1 −t)C(1),
and henceC(t/n) = C(t) +C(1/n) for any0 < t <1 and integern >0. By Condition3 and taking derivative
with respect tot in both sides, we obtainC′(t/n) = nC′(t), and multiplying in both sides byt we obtain
t
n C′( t
n ) = tC′(t). By replacingt with nt, we gettC′(t) = ntC′(nt), provided thatnt <1.
We will now show thattC′(t) is constant. In the last equation replacet by t/m by integerm >0 to get
the last equivalence intC′(t) = t
m C′( t
m ) = n
m tC′( n
m t) (the first equivalence is obvious). These equivalences
are valid for positivet <1 and n
m t <1. Let 0 < s <1 and n = ⌊ms/t⌋ be the largest integer smaller than
ms/t. Therefore, asm increases n
m t <1 and approachess as close as desired. By Condition3 the function
xC′(x) is continuous, and thereforelimm→∞ n
m tC′( n
m t) = sC′(s). The basic idea is that we can first compress
t as much as needed by the integer factorm and then expand it by the integer factorn so thatnt/m is as
close as desired tos. This shows thatsC′(s) = tC′(t) for s, t∈ (0, 1), and thereforetC′(t) is constant.
Assume thattC′(t) = −k. Then, by integrating we obtainC(t) = −k ln t + a, buta = 0 due toC(1) = 0,
and k >0 due to Condition2. Together with the above, we can now proof the theorem by noticing that the
solution satisfies Condition4 for any choice of thepij.
Remark: We have found that entropy is the measure of occupancy. The famous derivation of entropy as a
measure of information [69] uses similar elements, but some differences are worthy to be mentioned. First, our
proof uses the notion of additivity of occupancy on MDPs of length two (our Condition4), while Shannon’s
notion of additivity uses sequences of random variable of arbitrary length (his Condition3), and therefore his
condition is in a sense stronger than ours. Second, our proof enforces continuous derivative of the measure,
while Shannon enforces continuity of the measure, rendering our Condition3 stronger. Finally, we enforce a
specific form of the measure as an average over occupancy gains (our Condition4 again), because it intuitively
captures the notion of occupancy, while Shannon does not enforce this structure in his information measure.
Corollary 2. Condition 4 can be replaced by the stronger condition that requires additivity of paths of any
finite lengthn with no change in the above proof. We first introduce some notation: the probability of path
i0, i1, ..., in is pi0,i1 pi1,i2 ...pin−1,in, whereit refers to the state visited at stept and i0 is the initial state. Then
the new Condition4 reads in terms of the action-state occupancy of paths of lengthn as
C(n)
i0 =
X
i1,i2,...,in
pi0,i1 pi1,i2 ...pin−1,inC
 
pi0,i1 pi1,i2 ...pin−1,in

=
X
i1
pi0,i1 C(pi0,i1 ) +
X
i1,i2
pi0,i1 pi1,i2 C(pi1,i2 ) + ... +
X
i1,i2,...,in
pi0,i1 pi1,i2 ...pin−1,inC
 
pin−1,in

=
X
i1,i2,...,in
pi0,i1 pi1,i2 ...pin−1,in
 
C(pi0,i1 ) + C(pi1,i2 )... + C(pin−1,in)

,
for any time-homogeneous Markov chain. By choosing the particular chains used in Theorem 1, we arrive
again to the same unique solutionC(p) = −k ln p after usingC(1) = 0 repeated times, which obviously solves
the above equation for any chain and length path. Indeed, note that for the second chain in Theorem 1, from
initial state1 the absorbing state is reached in three time steps with probability one, and thus the above sum
contains allC(1) starting from the third terms, which contribute zero to the sum.
The above entropy measure of action-state path occupancy can be extended to the case where there is a
discount factor0 < γ <1. To do so, we assume now that the paths can have a random lengthn ≥ 1 that
follows a geometric distribution,pn = γn−1(1 − γ). In this case, the occupancy of the paths is
22
Cglobal = (1 − γ)
X
i1
pi0,i1 C(pi0,i1 ) + γ(1 − γ)
X
i1,i2
pi0,i1 pi1,i2 C(pi0,i1 pi1,i2 )
+γ2(1 − γ)
X
i1,i2,i3
pi0,i1 pi1,i2 pi2,i3 C(pi0,i1 pi1,i2 pi2,i3 ) + ... (A.1)
where then-th term in the sum is the expected occupancy gain of paths of lengthn weighted by the probability
of a having a path with exactly such a length.
Equivalently, a path in course can grow one step further with probabilityγ or be extinguished with
probability 1 − γ. Therefore, the occupancy in Eq. (A.1) should also be equal to the sum of the expected
occupancy gains of the local states along the paths, defined as
Clocal =
X
i1
pi0,i1 C(pi0,i1 ) + γ
X
i1,i2
pi0,i1 pi1,i2 C(pi1,i2 ) + γ2 X
i1,i2,i3
pi0,i1 pi1,i2 pi2,i3 C(pi2,i3 ) + ... (A.2)
where the first term is the expected occupancy gain given by the initial condition, the second term is the
expected occupancy gain in the next step weighted by the probability of having a path length of at least two
steps, and so on.
Eqs. (A.1-A.2), after using the Markov chain in Corollary 2, reduce to
Cglobal = (1 − γ)
X
i1
pi0,i1 C(pi0,i1 ) + γ(1 − γ)
X
i1,i2
pi0,i1 pi1,i2 C(pi0,i1 pi1,i2 )
+γ2(1 − γ)
X
i1,i2
pi0,i1 pi1,i2 C(pi0,i1 pi1,i2 ) + ...
= (1 − γ)
X
i1
pi0,i1 C(pi0,i1 ) + γ
X
i1,i2
pi0,i1 pi1,i2 C(pi0,i1 pi1,i2 )
and
Clocal =
X
i1
pi0,i1 C(pi0,i1 ) + γ
X
i1,i2
pi0,i1 pi1,i2 C(pi1,i2 ),
where we have usedpi2,i3 = 1 because all transitions in the third step are deterministic.
Equality of these two quantities leads to Condition 4, specifically, P
i1,i2 pi0,i1 pi1,i2 C(pi0,i1 pi1,i2 ) =P
i1
pi0,i1 C(pi0,i1 ) + P
i1,i2
pi0,i1 pi1,i2 C(pi1,i2 ). Therefore, the only consistent measure of occupancy with
temporal discount is the entropy. Obviously, the equality of global and local time-discounted occupancies
measured by entropy holds for any time-homogeneous or inhomogeneous Markov chain.
A.2 Critical policies and critical state-value functions
Here, the expected return following policyπ in Eq. (A.2), known as the state-value function, is written
recursively using the Bellman equation. Then, we find a non-linear system of equations for the critical policy
and critical state-value function by taking partial derivatives with respect to the policy probabilities (Theorem
2).
Using Eq. (A.2) and Theorem 1 withk = 1, we define the expected return from states under policyπ as
Vπ(s) = −
X
i1
ps,i1 ln ps,i1 − γ
X
i1,i2
ps,i1 pi1,i2 ln pi1,i2 − γ2 X
i1,i2,i3
ps,i1 pi1,i2 pi2,i3 ln pi2,i3 + ... (A.3)
where ps,i1 is the transition probability from states to action-statexi1 = (ai1 , si1 ). Note that in Eq. (A.2) we
have replaced the initial action-statei0 by the initial states alone, as the previous action that led to it does
no affect the transition probabilities in the Markov decision process setting. The expected returns satisfy the
standard recurrence relationship [14]
23
Vπ(s) =
X
a,s′
ps,(a,s′)
 
−ln ps,(a,s′) + γVπ(s′)

=
X
a,s′
π(a|s)p(s′|s, a) (−ln π(a|s)p(s′|s, a) + γVπ(s′)) . (A.4)
Here, we have unpacked the sum over the action-statei1 into a sum over(a, s′), wherea is the action made
in state s and s′ is its successor. The second equation shows, in a more standard notation, the explicit
dependence of the expected return on the policy. It also highlights that the intrinsic immediate reward takes
the formRintrinsic(s, a, s′) = −ln π(a|s)p(s′|s, a), which is unbounded.
From Eq. (A.3) it is easy to see that the expected return exists (is finite) for any policyπ if the Markov
decision process has a finite number of actions and states. Due to the properties of entropy, Eq. (A.3) is a
sum of non-negative numbers bounded byHmax = ln(|A|max|S|) (|A|max is the maximum number of available
actions from any state) weighted by the geometric series, which guarantees convergence of the infinite sum for
−1 < γ <1. An obvious, but relevant, implication of the above is that the expected return is non-negative
and bounded,0 ≤ Vπ(s) ≤ Hmax/(1 − γ), for any state and policy.
While in Eq. (A.4) the immediate intrinsic reward is the sum of the action and state occupancies,
Rintrinsic(s, a, s′) = −ln π(a|s)p(s′|s, a) = −ln π(a|s) − ln p(s′|s, a), we can generalize this reward to consider
any weighted mixture of entropies asRintrinsic(s, a, s′) = −α ln π(a|s) − β ln p(s′|s, a) for any two numbers
α > 0 and β ≥ 0. In particular, for (α, β) = (1 , 1) we recover the action-state occupancy of Eq. (A.4),
and for (α, β) = (1 , 0) and (α, β) = (0 , 1) we only consider action or state occupancy, respectively. The
case (α, β) = (0, 1) is understood as the limit case whereα becomes infinitely small. We note that the case
(α, β) = (1, 0) has often been used along with an external reward with the aim of regularizing the external
reward objective [37, 38, 40, 41, 43]. We also note that the case(α, β) = (1, −1), with negativeβ, constitutes
an approximation to empowerment [20, 64]: the agent tries to maximize action entropy while minimizing
state entropy conditioned to the previous action-state, which favors paths where there is more control on the
resulting states. However, we do not consider this case in this paper.
Under the more general intrinsic reward, the expected return obeys
Vπ(s) =
X
a,s′
π(a|s)p(s′|s, a)
 
−ln πα(a|s)pβ(s′|s, a) + γVπ(s′)

. (A.5)
Our goal is to maximize the expected return over the policy probabilitiesπ = {π(a|s) : a ∈ A(s), s∈ S} to
obtain the optimal policy. Note that forα >0 and β ≥ 0 the expected return is non-negative,Vπ(s) ≥ 0.
Theorem 2. The critical valuesV c(s) of the expected returnsVπ(s) in equation (A.5) with respect to the
policy probabilitiesπ = {π(a|s) : a ∈ A(s), s∈ S} obey
V c(s) = α ln Z(s) = α ln

 X
a∈A(s)
exp
 
α−1βH(S′|s, a) + α−1γ
X
s′
p(s′|s, a)V c(s′)
!
 (A.6)
where H(S′|s, a) = −P
s′ p(s′|s, a) ln p(s′|s, a) is the entropy of the successors ofs after performing actiona,
and Z(s) is the partition function.
The critical points (critical policies) are
πc(a|s) = 1
Z(s) exp
 
α−1βH(S′|s, a) + α−1γ
X
s′
p(s′|s, a)V c(s′)
!
, (A.7)
one per critical value, where the partition functionZ(s) is the normalization constant.
Defining zi = exp
 
α−1γV c(si)

, pijk = p(sj|si, ak) and Hik = α−1βH(S′|si, ak), Eq. (A.6) can be
compactly rewritten as
zγ−1
i =
X
k
wikeHik
Y
j
zpijk
j (A.8)
24
where the matrix with coefficientswik ∈ {0, 1} indicates whether actionak is available at statesi (wik = 1)
or not (wik = 0), and j extends over all states, with the understanding that if a statesj is not a possible
successor from statesi and actionak then pijk = 0.
Note that the we simultaneously optimize|S| expected returns, one per states, each with respect to the
set of probabilitiesπ = {π(a|s) : a ∈ A(s), s∈ S}.
Proof. We first note that the expected return in Eq. (2.2) is continuous and has continuous derivatives with
respect to the policy except at the boundaries (i.e.,π(a|s) = 0 for some action-state(a, s)). Choosing a state
s, we first take partial derivatives with respect toπ(a|s) for eacha ∈ A(s) in both sides of (A.5), and then
evaluate them at a critical pointπc to obtain the condition
λ(s, s) =
X
s′
p(s′|s, a)
 
−ln(πc(a|s))αpβ(s′|s, a) + γV c(s′)

− α + γ
X
b,s′
πc(b|s)p(s′|s, b)λ(s′, s)
= −α ln πc(a|s) − β
X
s′
p(s′|s, a) lnp(s′|s, a) − α
+γ
X
s′
p(s′|s, a)V c(s′) + γ
X
b,s′
πc(b|s)p(s′|s, b)λ(s′, s), (A.9)
where we have defined the partial derivative at the critical point∂Vπ(s′)
∂π(a|s) |πc ≡ λ(s′, s) and used the fact that
this partial derivative should be action-independent. To understand this, note that the critical policy should
lie in the simplexP
a π(a|s) = 1, π(a|s) ≥ 0, and therefore the gradient ofVπ(s′) with respect to theπ(a|s)
at the critical policy should be along the normal to the constraint surface, i.e., the diagonal direction (hence,
action-independent), or be zero. Indeed, the action-independence of theλ(s′, s) also results from interpreting
them as Lagrange multipliers:λ(s′, s) is the Lagrange multiplier corresponding to the state-value function at
s′, Vπ(s′), associated to the constraintP
a π(a|s) = 1, π(a|s) ≥ 0, defining the simplex where the probabilities
{π(a|s) : a ∈ A(s)} lie.
Noticing that the last term of Eq. (A.9) does not depend ona, we can solve for the critical policyπc(a|s)
to obtain equation (A.7). Eq. (A.7) implicitly relates the critical policy with the critical value of the expected
returns from each states. Inserting the critical policy (A.7) into Eq. (A.5), we get (A.6), which is an implicit
non-linear system of equations exclusively depending on the critical values.
It is easy to verify that the partial derivatives ofVπ(s) in Eq. (A.5) with respect toπ(a′|s′) for s ̸= s′ are
λ(s, s′) = γ
X
s′′
p(s′′|s)λ(s′′, s′),
and thus they provide no additional constraint on the critical policy.1
We finally show that the optimal expected returns, as defined from the Bellman optimality equation
V ∗(s) = max
π(·|s)
X
a,s′
π(a|s)p(s′|s, a)
 
−ln πα(a|s)pβ(s′|s, a) + γV ∗(s′)

, (A.10)
obey the same Eq. (A.6) as the critical values of Eq. (A.5) do. To see this, note that after taking partial
derivatives with respect toπ(a|s) for eacha ∈ A(s) on the right-hand side of Eq. (A.10) we get
0 = −α ln π(a|s) − β
X
s′
p(s′|s, a) lnp(s′|s, a) + γ
X
s′
p(s′|s, a)V ∗(s′) − α + λ(s), (A.11)
1This set of equations along with Eq. (A.9) generates a linear system ofS2 equations for theS2 unknowns λ(s, s′). In the
next section we show that the critical valuesV c(s) and critical policyπc(a|s) exists and are unique, and thus the system of
equations forλ(s, s′) is of the typeΛ = γP ⊺Λ + F, with unique matricesΛss′ = λ(s, s′), Ps′s = p(s′|s) ≡ P
a πc(a|s)p(s′|s, a)
and Fs′s is a diagonal matrix withFss = V c(s) − α. Because P is a stochastic matrix, it does not have eigenvalues larger than
one. Therefore the matrixI − γP ⊺ with γ <1 does not have zero eigenvalues, and thus it is invertible. The solution to the
system is then unique and given thenbyΛ = (I − γP ⊺)−1F.
25
where λ(s) is the Lagrange multiplier associated to the constraintP
a π(a|s) = 1. This equation, except for
the irrelevant action-independent Lagrange multipliers, is identical to Eq. (A.9). Eq. (A.6) follows from
inserting the resulting optimal policy into the Bellman optimality equation.
A.3 Unicity of the optimal value and policy, and convergence of the algorithm
We now prove that the critical valueV c(s) is unique, in other words, equation (A.6) admits a single solution
(Theorem 3). We later prove that the solution is the optimal expected return (Theorem 4).
Theorem 3. With the definitions in Theorem 2, the system of equations
zγ−1
i =
X
k
wikeHik
Y
j
zpijk
j (A.12)
with 0 < γ <1, α >0 and β ≥ 0 has a unique solution in the positive first orthantzi > 0, provided that for
all i there exists at least onek such thatwik = 1. The solution satisfieszi ≥ 1.
Moreover, given any initial conditionz(0)
i > 0 for all i, the infinite seriesz(n)
i defined through the iterative
map
z(n+1)
i =

X
k
wikeHik
Y
j

z(n)
j
pijk


γ
(A.13)
for n ≥ 0 converges to a finite limitz∞
i ≥ 1, and this limit is the unique solution of equation (A.12)
Note that the condition that for alli there exists at least onek such thatwik = 1 imposes virtually no
restriction, as it only asks for the presence of at least one available action in each state. For instance, in
absorbing states, the action leads to the same state.
Importantly, proving that the map (A.13) has a single limit regardless of the initial condition in the
positive first orthantz(0)
i > 0 suffices to prove that equation (A.12) has a unique solution in that region, as
then no other fix point of the map can exist. Additionally, since the solution is unique and satisfiesz∞
i ≥ 1,
the critical state-value function that solves equation (A.6) is unique, andV c(si) = αγ−1 ln z∞
i ≥ 0, consistent
with its properties.
The map (A.13) provides a useful value-iteration algorithm used in examples shown in the Results section,
and empirically is found to rapidly converge to the solution.
Proof. We call the seriesz(n)
i with initial conditionz(0)
i = 1 for alli the main series. We first show that the
main series is monotonic non-decreasing.
Forn = 1, we get
z(1)
i =

X
k
wikeHik
Y
j
(1)pijk


γ
≥ 1 = z(0)
i (A.14)
for alli, using that there existsk for which,wik = 1, wik is non-negative for alli and k, Hik ≥ 0 and the
power functionxγ is increasing with its argument.
Assume that for somen >0, z(n)
i ≥ z(n−1)
i for alli. Then
z(n+1)
i =

X
k
wikeHik
Y
j

z(n)
j
pijk


γ
≥

X
k
wikeHik
Y
j

z(n−1)
j
pijk


γ
= z(n)
i (A.15)
using the same properties as before, which proves the assertion for alln by induction.
Now let us show that the main series is bounded. DefineHmax = maxik Hik, and obviouslyHmax ≥ 0.
Forn = 1 we have
z(1)
i =
 X
k
wikeHik
!γ
≤
 
|A|maxeHmax
γ
≡ cγ (A.16)
26
(remember that|A|max is the maximum number of available actions from any state).
Forn = 2,
z(2)
i =

X
k
wikeHik
Y
j

z(1)
j
pijk


γ
≤

X
k
wikeHik
Y
j
cγpijk


γ
=
 X
k
wikeHik cγ
!γ
= cγ2
 X
k
wikeHik
!γ
≤ cγ+γ2
using the standard properties,P
j pijk = 1 and Eq. (A.16).
Assume that for somen >1 we havez(n)
i ≤ cγ+γ2+...+γn
. We have just showed that this is true forn = 2.
Then
z(n+1)
i =

X
k
wikeHik
Y
j

z(n)
j
pijk


γ
≤
 X
k
wikeHik cγ+...+γn
!γ
= cγ2+...+γn+1
 X
k
wikeHik
!γ
≤ cγ+...+γn+1
and therefore it is true for alln ≥ 0 by induction.
Therefore the seriesz(n)
i is bounded byc1/(1−γ). Together with the monotonicity of the series, we have
now proved that the limitz∞
i of the series exists. Moreover,z∞
i ≥ z0
i = 1.
The above results can be intuitively understood: the ‘all ones’ initial condition of the main series corresponds
to an initial guess of the state-value function equal to zero everywhere. The iterative map corresponds to
state-value iteration to a more optimistic value: as intrinsic reward based on entropy is always non-negative,
the z-values monotonically increase after every iteration. Finally, thez-values reach a limit because the
state-value function is bounded.
We now show the central result that the series obtained by using the iterative map starting from any
initial condition in the positive first orthant can be bounded below and above by two series that converge to
the main series. Therefore, by building ‘sandwich’ series we will confirm that any other series has the same
limit as the main series.
Let they(0)
i = ui > 0 be the initial condition of the seriesy(n)
i obeying the iterative map (A.13), and
define umin = mini ui and umax = maxi ui. Obviously,umin > 0 and umax > 0. Applying the iterative map
once, we get
y(1)
i =

X
k
wikeHik
Y
j

y(0)
j
pijk


γ
≤

X
k
wikeHik
Y
j
(umax)pijk


γ
=
 X
k
wikeHik umax
!γ
= uγ
max
 X
k
wikeHik
!γ
= uγ
maxz(1)
i
where in the last step we have used the values of the main series in the first iteration. We can similarly
lower-bound y(1)
i to finally show that it is both lower- and upper-bounded byz(1)
i with different multiplicative
constants,
uγ
minz(1)
i ≤ y(1)
i ≤ uγ
maxz(1)
i (A.17)
Now, assume that
uγn
minz(n)
i ≤ y(n)
i ≤ uγn
maxz(n)
i (A.18)
27
is true for somen >0. Then, forn + 1 we get
y(n+1)
i =

X
k
wikeHik
Y
j

y(n)
j
pijk


γ
≤

X
k
wikeHik
Y
j

uγn
maxz(n)
i
pijk


γ
= uγn+1
max

X
k
wikeHik
Y
j

z(n)
i
pijk


γ
= uγn+1
max z(n+1)
i
by simply extracting the common factor in the fourth expression, remembering thatP
j pijk = 1, and using
the definition of the main series in the last one. By repeating the same with the lower bound, we finally find
that (A.18) holds also forn + 1, and then, by induction, for everyn >0.
The proof concludes by noticing that the limit of bothuγn
max and uγn
min is 1, and therefore using (A.18) the
limit y∞
i of the seriesy(n)
i equals the limit of the main series,y∞
i = z∞
i .
Note that the iterative map (A.13) is not necessarily contractive in the Euclidean metric, as it is possible
that, depending on the values ofumin and umax and the changes in the main series, the bounds in Eq. (A.18)
initially diverge to finally converge in the limit.
Theorem 4. The (unique) critical valueV c(s) is the optimal expected return, that is, the one that attains the
maximum expected return at every state for any policy, and we writeV c(s) = V ∗(s)
Proof. To show thatV c(s) is the optimal expected return, we note that the maximum of the functionsVπ(s)
with respect to policyπ should be at the critical policy or at the boundaries of the simplices defined byP
a π(a|s) = 1 with 0 ≤ π(a|s) ≤ 1 for everya and s, as the expected returnVπ(s) is continuous and has
continuous derivatives with respect to the policy except at the boundaries. At the policy boundary, there
exists a non-empty subset of statessi and a non-empty set of actionsak for whichπ(ak|si) = 0. Computing
the critical value of the expected return along that policy boundary is identical to moving from the original to
a new problem where we replace the graph connectivity matrixwik in Eq. (A.12) by a new onevik such that
vik ≤ wik (remember that at the boundary there should be an actionak that were initially available from
state si, wik = 1, that at the policy boundary is forbidden,vik = 0). We now define the convergent series
z(n)
i and y(n)
i for the original and new problems respectively by using the iterative map (A.13) with initial
conditions equal to all ones. We prove now thatz(n)
i ≥ y(n)
i for alli for n = 1, 2, ..., and thus their limits obey
z∞
i ≥ y∞
i .
Forn = 1, we get
z(1)
i =

X
k
wikeHik
Y
j
(1)pijk


γ
≥

X
k
vikeHik
Y
j
(1)pijk


γ
= y(1)
i (A.19)
for alli, using thatwik ≥ vik and that the power functionxγ is increasing with its argument.
Assuming thatz(n)
i ≥ y(n)
i for alli for somen >0, then
z(n+1)
i =

X
k
wikeHik
Y
j

z(n)
j
pijk


γ
≥

X
k
vikeHik
Y
j

y(n)
j
pijk


γ
= y(n+1)
i (A.20)
using the same properties as before, which proves the assertion for alln by induction.
Remembering that the expected returnV (si) is increasing withzi, we conclude that the expected return
obtained from policies restricted on the boundaries of the simplices is no better than the original critical value
of the expected return.
28
A.4 Particular examples
Here we summarize the main results and specialize them to specific cases. We assume0 < γ <1, α >0
and β ≥ 0 and use the notation zi = exp
 
α−1γV ∗(si)

, where V ∗(s) is the optimal expected return,
pijk = p(sj|si, ak) and Hik = α−1βH(S′|si, ak), whereH(S′|s, a) = −P
s′ p(s′|s, a) lnp(s′|s, a).
A.4.1 Action-state entropy maximizers
Agents that seek to maximize the discounted action-state path entropy follow the optimal policy
π∗(ak|si) = 1
Zi

wikeHik
Y
j
zpijk
j

 (A.21)
with
Zi =
X
k
wikeHik
Y
j
z
pijk′
j (A.22)
The matrix with coefficientswik ∈ {0, 1} indicate whether actionak is available at statesi (wik = 1) or not
(wik = 0).
The expected return (state-value function) in terms of thez variables obeys
zγ−1
i =
X
k
wikeHik
Y
j
zpijk
j (A.23)
A.4.2 Action-only entropy maximizers
Agents that ought to maximize the time-discounted action path entropy correspond to the above case with
β = 0, and therefore the optimal policy reads as
π∗(ak|si) = 1
Zi

wik
Y
j
zpijk
j

 (A.24)
with
Zi =
X
k
wik
Y
j
zpijk
j (A.25)
The state-value function in terms of thez variables obeys
zγ−1
i =
X
k
wik
Y
j
zpijk
j (A.26)
A.4.3 Entropy maximizers in deterministic environments
In a deterministic environmentpi,j(i,k),k = 1 for successor statej = j(i, k), and zero otherwise. In this case,
at every statei we can identify an actionk with its successor statej. Therefore, the optimal policy is
π∗(ak|si) = wijzj
Zi
(A.27)
with
Zi =
X
j
wijzj (A.28)
The state-value function in terms of thez variables reads
zγ−1
i =
X
j
wijzj (A.29)
29
Figure 8: MOP agents determine stochastic policies that maximize occupancy of future action-state paths. In all
panels, the three successive dots indicate that the future looks the same for all the states or actions involved from
that point onwards. (a) At timet, the agent is faced with determining the optimal policy at states. Given that
taking actiona1 can stochastically lead to two distinct statess′
1 and s′
2, the optimal policy gives actiona1 twice the
probability weight than to actiona2 (which only induces a deterministic transition to states′
3). From timet + 1, the
future looks the same from all three statess′
i. (b) If the future does not look the same, and actually there are many
more actions available at states′
3 compared tos′
1 and s′
2, then more weight should be given to actiona2 than if the
future was the same. (c) If, however, all the actions available at states′
3 lead you to an absorbing state, almost zero
weight should be given to actiona2.
A.5 Experiments
In this subsection, we present the details for the numerical simulations performed for the different experiments
in the manuscript. First, we discuss the construction of the MOP and R agents, and afterwards we present
the details of each particular experiment.
A.5.1 MOP agent
In all the experiments presented, we introduce the MOP agent, whose name comes from the usual notation
for using H to denote entropy. Therefore, the objective function that this agent maximizes in general is
Eq. (2.2). As described in section A.4, theα and β parameters control the weights of action and next-state
entropies to the objective function, respectively. Unless indicated otherwise, we always useα = 1, β= 0 for
the experiments. It is important to note, as we have done before, that if the environment is deterministic,
30
then the next-state entropyH(S′|s, a) = −P
s′ p(s′|s, a) ln p(s′|s, a) = 0, and thereforeβ does not change the
optimal policy, Eq. (2.6).
We have implemented the iterative map, Eq. (2.7), to solve for the optimal value, usingz(0)
i = 1 for alli
as initial condition. Theorem (3) ensures that this iterative map finds a unique optimal value regardless of
the initial condition in the first orthant. To determine a degree of convergence, we compute the supremum
norm between iterations,
δ = max
i
|V (n+1)
i − V (n)
i |,
where Vi = α
γ log(zi), and the iterative map stops whenδ <10−3.
A.5.2 R agent
We also introduce a reward-maximizing agent in the usual RL sense. In this case, the reward isr = 1 for
living andr = 0 when dying. In other words, this agent maximizes life expectancy. Additionally, to emphasize
the typical reward-seeking behavior and avoid degenerate cases induced by the tasks, we introduced a small
reward for the Four-room grid world (see below). In all other aspects, the modelling of the R agent is identical
to the MOP agent. To allow for reward-maximizing agents to display some stochasticity, we used anϵ-greedy
policy, the best in the family ofϵ-soft policies [14]. At any given state, a random admissible action is chosen
with probabilityϵ, and the action that maximizes the value is chosen with probability1 − ϵ. Given that
the world modelsp(s′|s, a) are known and the environments are static, thisϵ-greedy policy does not serve
the purpose of exploration (in the sense of learning), but only to inject behavioral variability. Therefore, we
construct an agent with state-independent variability, whose value function satisfies the optimality Bellman
equation for thisϵ-greedy policy,
Vϵ(s) = (1 − ϵ) max
a
X
s′
p(s′|s, a) (r + γVϵ(s′)) + ϵ
|A(s)|
X
a,s′
p(s′|s, a) (r + γVϵ(s′)) , (A.30)
where |A(s)| is the number of admissible actions at states. To solve for the optimal value in this Bellman
equation, we perform value iteration [14]. Theϵ-greedy policy for the R agent is therefore given by
π(a|s) =
(
1 − ϵ + ϵ
|A(s)|, if a = arg maxa′
P
s′ p(s′|s, a′) (r + γVϵ(s′))
ϵ
|A(s)|, otherwise
where ties inarg max are broken randomly. Note that ifϵ = 0, we obtain the usual greedy optimal policy that
maximizes reward.
A.5.3 Four-room grid world
A.5.3.1 Environment The arena is composed of four rooms, each having size5 × 5 locations where the
agent can be in. From each room, the agent can go to two adjacent rooms through small openings, each
located in the middle of the wall that separates the rooms. At each of these rooms, there is a food source
located in the corner furthest from the openings. See Fig. 2 for a graphic description. Unless indicated
otherwise, the discount factor is set toγ = 0.99.
A.5.3.2 States The states are the Cartesian product between(x, y) location and internal stateu, which
is simply a scalar value between a minimum of 0 and a maximum capacity of 100. All states such that
(x, y, u= 0) are absorbing states, independently of the location(x, y). The particular internal stateu = 100 is
the maximum capacity for energy, such that even when at a food source, this internal state does not change.
Therefore, the number of states in this experiment is|S| = 104 external states× 101 internal states= 10504.
A.5.3.3 Actions Theagenthasamaximumof9actions: up, down, left, right, up left, up right,
down left, down right, andnothing. Whenever the agent is close to a wall, the number of available actions
decreases such that the agent cannot choose to go into walls. Finally, whenever the agent is in an absorbing
state, onlynothing is available.
31
Figure 9: Survivability for the experiments considered in the manuscript. (a) Survivability of the various agents tested
in the four-room grid world. At each 5E4 timestep episode, we recorded the survived time and averaged across episodes.
(b) Survivability of the mouse for both MOP and R agents. (c) Survivability for the cartpole (Sec. A.5.5) in the
deterministic arena for the MOP agent and theϵ-greedy R agents,γ = 0.98. (d) Survivability for cartpole (Sec. A.5.5)
in the stochastic arena for theβ = 0and theβ = 1MOP agents. γ = 0.99. (e) Survivability of the cartpole (Sec.
A.5.5) MOP agents as a function ofβ, for various values ofη. γ = 0.99
32
A.5.3.4 Transitions At any transition, there is a cost of 1 unit of energy for being alive. On the other
hand, whenever the agent is located at a food source, there is an increase in energy that we vary parametrically
that we call food gaing. For example, if the agent is in location(2, 1) at timet and moves towards(1, 1)
(where food is located), the change in energy would be∆ut = −1, given that the change in internal energy
depends only on the current state and action. If the agent decides to stay in(1, 1) at time t + 1, then
∆ut+1 = −1 + g.
A.5.3.5 R agent As stated above, in this experiment we introduced an extra reward for the R agent
when it reaches the food source. The magnitude is small compared to the survival reward (1E − 5 smaller)
and it mainly serves to break the degeneracy of the value function. The variability of the R agent is thus
coming purely from theϵ-greedy action selection.
A.5.3.6 Survivability To allow for the maximum uniform variability for the R agent, we tested various
values forϵ and observed the survivability of the agents as a function ofϵ, across all the food gains tested (see
Results section). The value ofϵ for which the R agent still survives as much as the MOP agent isϵ = 0.45
(see Figure 9a).
A.5.3.7 Noisy room In this variation for the experiment, there is a room (the bottom right room) where
transitions are uniformly random for all actions, across all possible neighboring locations. That is, for any
location snr in the noisy room, and anya available at that location, given that it hasn(snr) total neighbours
(including the same location),
p(s′|snr, a) =
(
1
n(snr) for s′ ∈ neighbours
0 otherwise
A.5.4 Predator-prey scenario
Here we provide all details of the simulated experiments. Results are shown in Fig. 3.
A.5.4.1 Environment The environment is similar to that one used for the 4-room grid world described in
A.5.3. Apart from the agent (prey), there is also another moving subject (predator) with a simple predefined
policy. The grid world consists of a “home” area, a rectangle 2x3 where the agent may enter, but the predator
cannot. This home area has a small opening that leads to a bigger 4x7 rectangle arena available for both the
agent and the predator. The only food source is located at the bottom-right corner of the common part of the
arena, so that the agent needs to leave its home to boost its energy. Additionally, there is an obstacle which
separates the arena in two parts with two openings, above and under the obstacle. This obstacle allows the
agent to “hide” from the predator behind it.
A.5.4.2 States The location of the predator is part of the agent’s state, such that a particular state
consists of the position of the agent, the position of the predator and the amount of energy of the agent. For
this case, we set the maximum amount of energyF equal to the food gain. Positions are 2-dimensional, and
therefore the states are 5-dimensional. In the used arena there are33 possible locations for the agent and26
ones for the predator, so that the total number of states ranges from11154 for F = 13 to 17160 for F = 20.
A.5.4.3 Actions The agent has the same actions as in the four-room grid world. The maximum number
of available actions is therefore 9. Moving towards obstacles or walls is not allowed.
A.5.4.4 Transitions The agent loses one unit of energy every time step and increases the amount of
energy up to a given maximum capacity levelF only at the food source. If the position of both the agent and
the predator are the same, then the agent is "eaten" and moves to the absorbing state of death as well as in
the case of energy equal to0. After entering the absorbing state the agent stays there forever.
The predator also moves as the agent (horizontally, vertically, diagonally on one step or to stay still).
Steps of the agent and the predator happen synchronously. The predator is “attracted” to the agent: the
33
probability of moving to some direction is an increasing function on the cosinescos αk of the angleαk between
this direction of motionk and the direction of the radius vector from the predator to the agent. In particular,
this probability is
pc
k = C−1 exp(κ cos αk) (A.31)
where κ is the inverse temperature of the predator andC = P
k exp(κ cos αk) is a normalization factor. These
probabilities are computed only for motions available at the current location of the predator, so that e.g. for
the location at the wall the motions along the wall are taken into account, but not the motion towards the
wall.
A.5.4.5 Goal The goal of the MOP agent is to maximize discounted action entropy, and thus to find
the optimal state-value function using the iterative map in Eq. (2.7) withHik = 0 (β = 0). While using
the iterative map, we take advantage of the fact that given an action the physical transition of the agent is
deterministic, but the physical transition of the predator is stochastic. Therefore, the sum over successor
states j in Eq. (2.7) is simply a sum over the predator successor states.
A.5.4.6 Parameters γ = 0.98, F = 15 (if another value between 13 and 20 not mentioned),κ = 2.
Simulation time is 5000 steps.
A.5.4.7 Counting rotations We define a clockwise (counterclockwise) half-rotation as the event when
the agent came from the left part of the arena to the right part over the field above (under) the wall and
from the right part to the left one over the field under (above) the wall without crossing the vertical line of
the wall in between. One full rotation consists of two half-rotations in the same directions performed one
after another. We counted the number of full rotations in both directions in70 episodes of500 time steps
each for both MOP and R agents for different values of the food gainF. Error bars were computed based on
these 70 repetitions. The fraction of clockwise rotations to total rotations (sum of clockwise and anticlockwise
rotations) for different values ofF is shown at Fig. 3.
A.5.4.8 Survivability The ϵ-greedy R agents display some variability that depends onϵ. To select this
parameter, we matched average lifetimes (measured in simulations of5000 steps length) between the MOP
and R agents, separately for everyF. Lifetimes are plotted in Figure 9b.
A.5.4.9 Videos We have generated one video for the MOP agent (Video 2) and another for the R agent
(Video 3), both forF = 15, κ = 2, andϵ = 0.06 for the R agent so as to match their average lifetimes as
described above. In the videos, green vertical bar indicates the amount of energy by the agent at current
time. When the agent makes at least one full rotation around the wall, it is indicated by the written phrase
“clockwise rotation” or “anticlockwise rotation”. Black vertical arrow indicates direction (‘up’ for clockwise
and ‘down’ for anticlockwise directions) of the half-rotation in the part of arena left from the wall.
A.5.5 Cartpole
A.5.5.1 Environment A cart is placed in a one-dimensional track with boundaries at|x| = 1.8. It has a
pole attached to it, that rotates like an inverted pendulum with its pivot point on the cart.
A.5.5.2 States The dynamical system can be described by a four-dimensional external state(x, v, θ, ω),
where x is the position of the cart,v is its linear velocity,θ is the angle of the pole with respect to the vertical
which grows counterclockwise, andω is its angular velocity. In this case, we model the internal stateu simply
with the binary variablealive, dead, where the agent enters the absorbing statedead if its position exceeds
the boundaries, or if its angle exceeds 36 degrees. This amplitude of angles is larger than that typically
assumed (12 degrees in [76]), and therefore our system is allowed to be more non-linear and unstable. The
state space is[−1.8, 1.8] ×(−∞, ∞) ×[−36, 36] ×(−∞, ∞) ×{0, 1}. To solve for the state value function in Eq.
(2.7), we discretize the state space by setting a maximum value for the velocities. Given all the parameters
(allowed x and θ, magnitude of the forces, masses of cart and pole, length of pole and gravity, below), we
34
Figure 10: Histogram of angles and locations visited for the cartpole, as in Fig. 4 of the main manuscript, for the
MOP agent (left) andϵ-greedy R agent (right), withϵ chosen such that MOP and R agents’ lifetimes are similar (see
Fig. 9c).
empirically set the maximum values for|v| = 6 and |ω| = 3, which the cart actually never exceeds. Therefore,
we computed the state value function in a31 × 31 × 31 × 31 × 2 grid (number of states =1.8 × 106).
A.5.5.3 Actions Any time the agent isalive, it has 5 possible actions: forces of{−40, −10, 0, 10, 40},
where zero force is understood asnothing. If the agent isdead, then onlynothing is allowed.
A.5.5.4 Transitions This dynamical system is a standard task in reinforcement learning, namely the
cartpole-v0 system of the OpenAI gym [76]. The solution of this dynamical system is given in Ref. [75],
where we use a frictionless cartpole. The equations for angular and linear accelerations are thus
¨θ =
−g sin(θ) + cos(θ)
M+m

−F + m ˙θ2l sin(θ)

l

4
3 − m cos2(θ)
M+m
 (A.32)
¨x = 1
cos(θ)
4
3l¨θ − g sin(θ)

. (A.33)
Given a forceF, a deterministic transition can be computed from these dynamical rules, and a real-valued
state transition is observed by the agents.
A.5.5.5 R agent The reward signal is 1 each time the agent is alive and 0 otherwise. To allow for some
variability in the action selection of the R agent, we implement anϵ-greedy action selection as described above.
For exposition purposes, in the manuscript we setϵ = 0.0, but we also compared to an R agent withϵ chosen
such that average lifetimes between MOP and R agents are matched (see Fig. 9c and Fig. 10).
A.5.5.6 Parameters Mass of the cartM = 1, mass of the polem = 0.1, length of the polel = 1,
acceleration due to gravityg = 9.81, time discretization∆t = 0.02. Unless specified differently, the discount
factor was set toγ = 0.98.
A.5.5.7 Value interpolation The observed external state is a continuous four-dimensional variable,
so we need to approximate the value function. In order to do so, we simply discretized the state space as
described above, and use value iteration as described in Eq. (2.5) in these grid points by performing a linear
value interpolation for the successor states at each iteration. During a particular episode, the observed states
might not be the same as the ones in the grid, so in order to compute the optimal policy at these states, we
perform the same type of value interpolation as in the value iteration stage.
35
A.5.5.8 Stochastic arena We introduced a slight variation to the environment, where thex >0 half
of the arena is noisy: agents choose an action (force), but the intended state transition of applying such an
action fails with probabilityη and succeeds with probability1 − η. This is implemented as follows: given
any state-action pair(s, a) for whichx >0, there are two possible successor states, one corresponding to the
intended action (force) chosen, and the other one corresponding to a zero force action:
p(s′|s, a) =



1, if x <0 and s′ ← (s, a)
1 − η, if x >0 and s′ ← (s, a)
η, if x >0 and s′ ← (s, 0)
(A.34)
This stochasticity lets us differentiate between action path occupancy maximizers and action-state path
occupancy maximizers by choosing any positive real value ofβ in Eq. (2.1), becauseβ >0 agents will have a
natural tendency to preferx >0 locations.
A.5.6 Agent-pet scenario
An agent and a pet move in an arena with degrees of freedom that depend on the actions made by the agent,
as explained next in detail.
A.5.6.1 Environment A 5 × 5 arena. The middle column of arena can be blocked by a fence, a vertical
obstacle that the pet cannot cross. The agent can cross it freely regardless of whether it is open or closed.
The agent can open or close the fence by performing the corresponding action when visiting the lever location,
at the left bottom corner.
A.5.6.2 States The system’s state consists of the Cartesian product of agent’s location, pet’s location
and binary state of the fence. So, the number of states is1250. For the sake of simplicity there is no internal
states for the energy, and thus there are not absorbing states. The initial states of the agent and pet at the
start of each episode are the middle of the second column and the right lower corner of the arena, respectively.
A.5.6.3 Actions As in Sec. A.5.4 the agent’s actions are movements to one of the 8 neighbour locations
as well as staying on the current one. Additionally, if the agent is on the “lever” location, an additional action
is available, namely to open or close the fence, depending on its previous state.
A.5.6.4 Transitions The pet has the same available movements as the agent when the fence is open. The
pet performs a random transition to any of the neighbour locations, or stays still, with the same probability.
If the agent closes the fence, then the pet can only move on the side where it lies when closed. For simplicity,
if the fence is closed by the agent when the pet lies in the middle column, then the pet can only move to the
right or left locations such that it will be at one side of the fence in the next time step.
A.5.6.5 Goal The goal of the MOP agent is to maximize discounted action-state entropy using the
iterative map in Eq. (2.7) withα = 1 and β ∈ [0, 1], parameters that measure the weight of action and state
entropies, respectively. As in the prey-predator example, we take advantage of the fact that given an action
the physical transition of the agent is deterministic, while the physical transition of the pet is stochastic.
Thus, the product over successor statesj in Eq. (2.7) is a product over the pet successor states.
A.5.6.6 Simulation details We ran simulations for several values ofβ, from 0 to 1 in 0.1 steps, to
interpolate between pure action entropy (β = 0) and action-state entropy (β = 1). We measured the fraction
of time the gate was open using episodes of2000 steps averaged over70 simulations for eachβ, shown in Fig
5. Heat-maps in that figure correspond to the occupation probability by the pet forβ = 0 (left panel) and
β = 1 (right panel) using an episode of5000 steps.
36
Figure 11: Comparison between the MOP agent and the R agent in the high-dimensional quadruped (ant) environment
from Gymnasium. (a) In both experiments,ϵ for the R agent is chosen as to match the average survival time of the
MOP agent. (b) Both MOP and R agents are able to reach the food source in most of the test runs. (c) Probability
density function of the travel time, defined as the time the agent spends before encountering the food source for the
first time. (d) Probability density function of the projection of all points in a trajectory, for all trajectories, onto the
line perpendicular to the shortest path connecting the origin and the food source during travel time (main diagonal).
(e) Display of 20 randomly chosen trajectories for the R agent before (left) and after (right) finding the food source
for the first time. Colorcode defined by the energy level of the agent. (f) Probability density function of the planar
speed before and after finding the food for the first time for the R agent. Both distributions show a peak at very low
velocities, indicating prolonged periods of time in which the ant performs very little translational movements.
A.5.7 Quadruped-Ant
Our goal is to show that MOP also works in high-dimensional, continuous action-state spaces. We employ the
Ant-v4 environment from OpenAI’s Gymnasium as our testing ground. We benchmark the performance of
our entropy-maximizing agent against agents using anϵ-greedy strategy with rewardsR = 1 for every step
except for absorbing states, whereR = 0 (R agent). All the relevant hyperparameters used to train these
agents are provided in Table 1.
In the first experiment, we study the behavioral variability of our agents and their average lifetime. The
agent begins at the(x, y) coordinate (0, 0) and follows its designated policy algorithm. The agent is considered
"dead" if either it takes a step that results in the z-coordinate of its torso falling outside the range[0.3, 1.0],
or when the episode concludes.
In the second experiment, the agent possesses an energy value, represented as a scalar. The agent dies
once it consumes all its energy. Each step taken by the agent consumes one energy point. It commences
with an initial energy of 200, and its maximum energy capacity is set at 400. A food source is situated at
the (2, 2) coordinate within the arena. Should the agent approach this source within a distance less than
0.5 from the center of its torso, it receives an energy boost of 25. The permissible z-coordinate range for
the agent’s torso remains consistent with the first experiment. In addition to the state vector provided by
the OpenAI Gym environment, we incorporate the agent’s energy level, its absolute position, and the food
source’s position. Interestingly, we found that the MOP agent travels to the food source much faster than
the R agent (Supplemental Fig. 11c, travel time distribution for the MOP agent is shifted towards short
times), seemingly appearing less risky, given the stochastic nature of both agents’ action selection. Even when
37
Table 1: Hyperparameters for Ant environment
Parameter Value
optimizer Adam
learning rate 3 × 10−4
discount (γ) 0.999
replay buffer size 106
number of hidden layers (all networks) 2
number of hidden units per layer 256
number of samples per minibatch 100
number of training epoch 300
steps per epoch 10000
initial random steps 20000
maximum episode length 5000
nonlinearity ReLU
target smoothing coefficient(τ) 0.005
number of agents 5
test runs 100
the MOP agent travels faster, its trajectories are more variable compared to the R agent (Supplemental Fig.
11d, projection of trajectories on a line perpendicular to the straight line that joins the origin and the food
source). In this second experiment, we find one out of five R agents not being able to reach the food source in
a significant percentage of the test runs. For this reason, we excluded that agent from the analyses.
In the third experiment, thex >0 portion of the arena produces state transition noise. In the unperturbed
case (first experiment), given a states and action a, the agent transitions deterministically to a state
sp = step(s, a) (given by the Gymnasium package). For this experiment, we apply discrete noise to the
resulting states′ in the following way: thei-th coordinate of the new states′
i now independently transitions
with probability1/2 to either of two states that are close to the unperturbed transitionsp. Specifically,
p(s′
i|s, a) =



1/2 if x >0 and s′
i = (1 + u)sp
i
1/2 if x >0 and s′
i = (1 − u)sp
i
1 if x ≤ 0 and s′
i = sp
i
, (A.35)
where u is the noise magnitude parameter, which makes the transition noisy with probability1/2 (note that if
u = 0, the transition is deterministic, and corresponds to the unperturbed transition). The perturbations
thus scale with respect to the unperturbed transitionsp, so that each coordinate gets noise proportional to
its magnitude. We apply this noise only to the coordinates given by the 27-dimensional observation vector
provided by Gymnasium, so we do not apply noise to thex, ycoordinates directly (see details at Ref. [81]).
We do not implement an energy constraint in this experiment. The parameterα is set to a constant equal to
1. Note that the intrinsic reward for the next-state transition obtained from being inx >0 is independent of
u >0, as−β log (p(s′|s, a)) = β log 2, given that the stochasticity of the transition does not depend on the
action. Following Eq. (A.35), whenu = 0, this intrinsic reward vanishes.
We found thatβ ≥ 0 MOP agents are sensitive to the added noise, and they all managed to survive
for almost the whole duration of the episodes after training (Fig. 12a). First of all, when there is no noise
(u = 0), the transition is deterministic, and agents do not show any preference to either side of the arena (Fig.
12b, grey line, c, first row). When noise magnitude is finite but small (u = 0.01), β = 0 MOP agents do not
show a significant preference between halves of the arena (Fig. 12b,c). However,β >0 MOP agents show a
preference for the half of the arena that produces state transition noise (Fig. 12b blue line, c second row). If
the noise magnitude is larger (u = 5%, 7%), smallβ MOP agents (includingβ = 0) avoid the noisy half of the
arena, given that noise can more easily cause the agent to fall (Fig. 12b,c). Crucially, with increasingβ >0
we see an increasing preference for the noisy half of the arena (Fig. 12b, increasing curves), without much
effect on survival rates (Fig. 12a).
38
Figure 12: Ant shows flexible preferences for stochastic transitions in the half planex >0 as a function of theβ
parameter, which controls the preference for state transition entropy, for a constantα = 1. Averages are across 1000
episodes for each of the 5 different random seeds. (a) Average survival times for the agents show that all agents learned
to approximately survive 1000 step episodes. (b) Mean ofx position of the ant, as a function of next-state entropy
weight β, for various noise magnitudesu. (c) Position heatmaps of all agents for each combination of parameters.
39
Figure 13: Fixing the number of actions for non-absorbing states in the gridworld environment, instead of having it be
variable across states. We fix this number of actions at 9 for non-absorbing states. (a) MOP agent with fixed actions,
(b) R agent with fixed actions, (c) KL regularization agent with fixed actions.
A.6 Differences with KL regularization
Given the similarity of our objective, Eq. (2.1) to a KL regularization scheme [70], here we contrast predictions
of using a KL divergence (relative entropy) compared to an absolute entropy objective. A relative entropy
objective would look like a maximization of the cumulative immediate reward given by the negative KL
divergence between a behavioral policyπ(a|s) and a default policyπ0(a|s),
−DKL(π(a|s)||π0(a|s)) =
X
a
π(a|s) ln π0(a|s)
π(a|s) (A.36)
= H(π(a|s)) − ln(|A(s)|), (A.37)
where the second equation comes from considering a default policy that is a uniform over actions, conveying
the idea that we want to be as close to a uniform policy as possible. However, the lack of an extrinsic reward
makes this case degenerate, in the sense that all states are equally preferred. We can see this by realizing
that the highest possible immediate intrinsic reward in this case is zero, given that KL divergence is always
non-negative. Thus, the optimal policy at all states is uniform over all available actions in each state. For
instance, for absorbing states, where only one action is available, Eq. (A.37) is zero, making a “KL agent" be
equally attracted to non-absorbing and absorbing states, completely opposite to the motivation of our work,
and illustrated in Supplemental Fig. 13c, where the agent dies very quickly. Furthermore, having a variable
or a fixed number of actions for non-absorbing states is the same for a KL agent, since the relative entropy
regularizes over the number of actions. This is in stark contrast with MOP, which intrinsically prefers states
with a high number of actions. Having a fixed number of actions for non-absorbing states affects the behavior
of MOP agents, which we can see in our gridworld experiment, comparing Fig. 2b and Fig. 13.
Finally, one could imagine setting up a default policy for a KL agent with a different set of available
actions than the behavioral policy. In particular, we can set the action set for the default uniform policy
to be fixed everywhere, including absorbing states. This amounts to shifting the immediate reward by a
scalar everywhere, resulting in an equivalent objective as MOP However, it is hard to see how one can justify
allowing the default policy to have a different set of actions than the behavioral policy, especially because
of the sum over actions in Eq. (A.37) implies that we sum over all (im)possible actions and implicitly set
the probabilityπ(a|s) of the behavioral policy to be zero for actions that are not in its support. In contrast,
MOP does not have to deal with this problem, and can easily handle constant or variable number of actions
for non-absorbing states.
40
A.7 Comparison to Empowerment and Active Inference
A.7.1 Empowerment
In this subsection we compare the behaviors attained by the MOP and empowered (MPOW) agents. We
implemented empowerment for the 4-room gridworld and cartpole experiments.
A.7.1.1 4-room gridworld
For the 4-room gridworld, we implemented empowerment in its original discrete formulation [20]. That is, we
take the definition of empowerment of a particular statest at timet as the channel capacity between the
agent’s n-step actionsan
t = (at, at+1, ..., at+n−1) ∈ An at this state, and the resulting statesst+n,
C(st) = max
p(an
t |st)
X
An,S
p(st+n|st, an
t )p(an
t |st) log
 p(st+n|st, an
t )P
An p(st+n|st, an
t )p(an
t |st)

, (A.38)
where p(an
t |st) is the probability distribution ofn-step actions that mutual information is maximized over,
and p(st+n|st, an
t ) is then-step world model, computed as
p(st+n|st, an
t ) = p(an
t |st)
n−1Y
τ=0
p(st+τ+1|st+τ , at+τ )
.
This maximization procedure is done via the Blahut-Arimoto algorithm [77], with a tolerance of1 × 10−12
for ∥pk+1(an
t |st) − pk(an
t |st)∥, wherek is the iteration number of the algorithm. The initial condition for the
n-step action probabilities is uniform over actions, and for this particular environment, very few iterations
were needed for convergence (typically 3 or 4).
We initialize an agent at a particular location (in the center of a room,(x, y) = (3, 3)), with an internal
energy of E = 30 , so that the initial state iss = ( E, x, y) = (30 , 3, 3). The agent looks ahead at all
possible immediately successor statesst+1, computes their empowerment, and greedily chooses the action
that corresponds to the successor state with highest empowerment (environment is deterministic). In our
particular formulation, we allowed for a stochastic choice of action in case of empowerment ties between
successor states. Note that the behavioral policy (greedy maximization of empowerment) and the probability
of then-step actions over which mutual information is maximized are different [20].
Given the nature of the arena, we implemented5-step empowerment, to give the agent enough lookahead
to consider going into other rooms, while keeping the computations tractable, given the large amount of
1-step actions (9 for center cells). Usually, empowerment assumes a fixed amount of actions across states, and
simply considers inconsequential actions to end in the current state, such as running into a wall resulting in
staying in the same place. We implemented this original formulation of empowerment, although it is possible
to implement state-dependent action sets, as for our formulation of MOP. This would still be meaningful for
empowerment, as having more actions available results in more distinct successor states, producing similar
predictions as in the original formulation of empowerment.
A.7.1.2 Cartpole
For the case of the cartpole experiment, we implemented continuous-state empowerment, as developed in [64],
C(st) = max
p(an
t |st)
X
An
p(an
t |st)
Z
S
p(st+n|st, an
t ) log
 p(st+n|st, an
t )P
An p(st+n|st, an
t )p(an
t |st)

dst+n, (A.39)
where p(st+n|st, an
t ) is now a probability density over successor statesst+n.
In order to have enough lookahead without needing highn, we used3-step empowerment with each action
in the3-step action held constant fork = 10 time steps, in order for the computation of empowerment to be
meaningfully different between states. Following [64], we constructed a Gaussian process from where successor
states st+n can be drawn for each of the actions, only in the computation of empowerment (real dynamics are
still deterministic). The standard deviation of the noise that blurs successor states wasσ = 0.01I4×4, as in
[64], independent of the action. The number of Monte Carlo samples needed to be drawn to approximate the
high dimensional integral in Eq. (A.39) wasNMC = 300. The computation of empowerment is done similarly
41
Figure 14: In our grid-world environment, the expected free energy (EFE) agent only visits a restricted portion of
the arena, as long as the target distribution is not perfectly uniform (see Supplemental Sec. A.7.2.2). In the limit of
infinite temperature (λ = 0), the EFE degenerates to a survival maximization and the stochasticity is due to the Free
Energy degeneracy across actions.
as in the gridworld, through a Blahut-Arimoto algorithm described in [64]. Similarly, the agent looks ahead
at successor states, computes their empowerment and greedily chooses the action that corresponds to the
state with the highest empowerment.
A.7.2 Active Inference
Second, we compared with an active inference approach [79]. Note that our experiments assume full
observability of states, although the partial observability condition has often been studied under active
inference [80]. The Expected Free Energy (EFE) is defined as the quantity
Gπ,t(st) =
X
¯st+1,¯at
pπ(¯st+1, ¯at|st) log p(¯st+1|¯at, st)
q(¯st+1) , (A.40)
which is to be minimized as a function of the policyπ, which is allowed to change as a function of the
state. Here ¯st+1 = (st+1, st+2, ..., sT ) and ¯at = (at, at+1, ..., aT−1), that is, the sequence of future states and
actions respectively from timet up to some finite timeT given that the initial state at timet is st. Thus,
pπ(¯st+1, ¯at|st) and p(¯st+1|¯at, st) refer to the join probability of future states and actions, and their conditional,
respectively, given the initial state. The quantityq(¯st+1) factorizes asq(¯st+1) = QT−1
τ=t q(sτ+1), whereq(s) is
a time-independent probability describing the "desired" states of the agent, capturing the idea that desired
states are independent of time. Note thatGπ,t(st) is the expectation over actions given a policyπ of the KL
divergence betweenp(¯st+1|¯at, st) and q(¯st+1), that is,Gπ,t(st) = E¯at∼πKL(p(¯st+1|¯at, st)||q(¯st+1)). Note that
because the time horizon is finite, here we need to consider time-dependent policies, soπ(at|st) is understood
as the probability of selecting actionat at time t given that the state at timet is st. Time-independent
policies will be suboptimal in general in finite horizon MDPs.
Minimizing the objective in Eq. (A.40) is similar to MOP in that state transition entropy is being
maximized, but it differs in that there is no action entropy and there is a regularizing distributionq(s) towards
which states should converge on the long run. The latter distinction highlights a difference in focus of the
EFE and MOP approaches, but they can be made similar by just takingq(s) to be uniform in state space.
However, the former difference is essential: the optimal policy of the EFE will be deterministic (see Sec.
A.7.2), while the optimal policy of MOP is stochastic. Therefore, one expects to find much larger behavioral
variability under MOP than under EFE with uniform preference over all states.
By virtue of the Markov property, we havepπ(¯st+1, ¯at|st) = QT−1
τ=t π(aτ |sτ )p(sτ+1|sτ , aτ ) and p(¯st|¯at, st) =QT−1
τ=t p(sτ+1|sτ , aτ ). Therefore, the objective in Eq. (A.40) can be recursively written as
Gπ,t(st) =
X
st+1,at
π(at|st)p(st+1|st, at)

log p(st+1|st, at)
q(st+1) + Gπ,t+1(st+1)

(A.41)
for t < T− 1, while the terminal value is
Gπ,T −1(sT−1) =
X
sT ,aT−1
π(aT−1|sT−1)p(sT |sT−1, aT−1) log p(sT |sT−1, aT−1)
q(sT ) , (A.42)
42
Figure 15: (Left) Entropy of distribution of visited state space as a function of horizon shows EFE agent is never as
good as the MOP agent in generating variability. (Right) Increasing the horizon lookahead makes EFE agent similar
to the R agent described in the main manuscript (see Fig. 4)
as at timeT the episode terminates.
Note that the above formalization slightly generalizes EFE [79] by allowing the possibility that the optimal
policy is stochastic. Next we show that the optimal policy is deterministic.
To find the optimal policy, we proceed backwards in time [14]. At time T − 1 the optimal policy is
deterministic because Eq. (A.42) is linear in the policy. The only exception is that there could be ties between
several actions having the same value of the objective, in which case one can be always chosen arbitrarily, or
they can be chosen randomly. Therefore, the optimal action is
a∗
T−1(sT−1) = arg min
a
X
sT
p(sT |sT−1, a) log p(sT |sT−1, a)
q(sT ) (A.43)
and define the optimal return at timeT − 1 as
G∗
T−1(sT−1) =
X
sT
p(sT |sT−1, a∗
T−1(sT−1)) log p(sT |sT−1, a∗
T−1(sT−1))
q(sT ) , (A.44)
Proceeding backwards, with t = T − 2, T− 3, ..., we find that again for all times the optimal policy is
deterministic, and that the optimal action is
a∗
t (st) = arg min
a
X
st+1
p(st+1|st, a)

log p(st+1|st, a)
q(st+1) + G∗
t+1(st+1)

, (A.45)
where the optimal return is recursively computed as
G∗
t (st) =
X
st+1
p(st+1|st, a∗
t (st))

log p(st+1|st, a∗
t (st))
q(st+1) + G∗
t+1(st+1)

. (A.46)
A.7.2.1 Discounted infinite-horizon sophisticated inference is identical to reward maximization
under deterministic dynamics
Here, we show that under an infinite horizon, a discounted expected free energy that considers state-
dependent policies in the future is equivalent to reward maximization under deterministic dynamics. We start
with the same assumption as before that minimizing EFE optimally needs to consider future states where the
agent is minimizing EFE. In a discounted, infinite horizon case, this becomes
Gπ(s) =
X
s′,a
π(a|s)p(s′|s, a)

log p(s′|s, a)
q(s′) + γGπ(s′)

, (A.47)
where γ <1. Under deterministic dynamicsp(s′|s, a) = 1 for only one states′, i.e. s′ = s′(s, a). So we can
rewrite the EFE as
Gπ(s) =
X
a
π(a|s) [−log (q(s′(s, a))) + γGπ(s′(s, a))] . (A.48)
43
Asking to minimizeG is equivalent to maximizing−G, which means that the optimal Bellman equation for
sophisticated active inference in this case turns to
G∗(s) = max
a
[log (q(s′(s, a))) + γG∗(s′(s, a))] . (A.49)
Simply rewritinglog(q(s′(s, a))) = r(s′(s, a)) gives us the typical Belllman equation for MDPs.
In particular, when the preferred distribution is uniform on a finite portion of state space, under the
presence of absorbing states outside this portion, this scheme is identical to survival maximization. This
is because we can define q(s′(s, a)) = 1 /V , where V is the volume of the portion of state space that
is not absorbing, fors′(s, a) that stays in this portion. For states outside this region, we can establish
q(s′(absorbing)) ≪ q(s′(alive)), such thatlog(q) is bounded. Therefore, for long horizons, we expect the
EFE agent to behave identically to our previously defined R agent that maximizes survival. We confirm this
expectation in Supplemental Fig. 15.
A.7.2.2 Details of simulations
One can define a target distributionq(s) through a Boltzmann distribution, instead of a hard maximization
of rewards, as similar to what is done in soft RL [39, 40]. The target distributionq(s) can be defined as
qλ(s) = 1
Zλ
exp(λR(s)), (A.50)
where λ is an inverse temperature, which expresses how motivated the agent is to maximize reward [79].
A.7.2.3 Grid world We takeR = δ for being in the food andR = 0 otherwise. For a large temperature,
λ is small, and thusqλ(s) is very close to an uniform distribution –it has a little bump on the reward location.
Even a tiny bump breaks the symmetry of the EFE agent in deterministic environments such that it absolutely
prefers the food source location, and thus behavior collapses to the occupancy of that single state (see Fig. 6).
A.7.2.4 Cartpole We define the rewards similar to the R agent,R = 1 for non-absorbing states and
R = 0 for absorbing states. This amounts to a uniform target distributionq(s) over non-absorbing states.
A.8 Relationship to Maximum Entropy Reinforcement Learning and goal direct-
edness
The objective of maximizing action-state path entropy in Eq. (2.2) for the special caseβ = 0 can be obtained
from the maximum entropy reinforcement learning (MaxEnt RL) formulation [37, 38, 40]
Vπ(s) = Eπ
" ∞X
t=0
γt (r(st, at) + αH(π(·|st)))
s0 = s
#
, (A.51)
by setting the rewardr(s, a) = 0 for all states and actions, and therefore there is no difference between the two
approaches in this particular case. However, this reduction obscures the fact that we can generate goal-directed
behaviors in H-agentswithout the need of specifying rewards –indeed, this is one of the main accomplishment
of our work. To see this, we first quantify how a MaxEnt RL agent gets reward in the four-room grid world
defined in Supplemental Sec. A.5.3, as a function of the temperature parameterα. In this case, a sensible
goal is “eating food” (that is, definingr(s, a) = 1 at the food locations, and zero everywhere else). Trivially,
when α ≪ 1 in Eq. (A.51), the goal is simply to maximize the future expected reward, equivalent to the
ϵ-greedy R agent defined in Supplemental Sec. A.5.2, forϵ = 0 (Figure 16a, leftmost points). In contrast, for
α ≫ 1, we recover the MOP agent in practice (due to the environment being deterministic). In this case,
the agent mostly focuses on maximizing future expected entropy, and getting small eating rate (Figure 16a,
rightmost points). Therefore, the temperatureα quantifies how “goal directed” the agent should be, where the
goal here is understood as getting food, and the entropy term is understood as a regularizer that promotes
exploration of the arena.
44
Figure 16: Reward is not necessary for "goal-directed" behavior. (a) Eating rate as a function of the temperature
parameter α in Equation (A.51) for a MaxEnt RL agent in the four-room grid world. (b) Eating rate as a function of
the capacity for a MOP agent in the four-room grid world.
To aid in showing our central result that an extrinsic reward is not necessary for “goal directed behavior”,
we take the MOP agent and vary its energy capacity (see Supplemental Sec. A.5.3). For large capacities, the
MOP agent can largely ignore the food most of the time, obtaining small eating rate (Figure 16b, right-most
points). This is because food is conceived as the means to accomplish the goal of maximizing future path
occupancy. In contrast, when capacity is small, the MOP agent needs to get the food much more frequently to
avoid the absorbing state, thus getting much higher eating rates (Figure 16b, leftmost points). The remarkably
strong qualitative similarities between the two panels in the figure show that by reinterpreting the concept of
reward, one can forego the need of specifying a reward function, and focus on more universal principles of
behavior.
A.9 Non-additivity of mutual information and channel capacity
Here we show that mutual information over Markov chains does not obey the additive property. It suffices to
prove our statement for paths of length two. Thus, we ask whether the mutual information between actions
(a0, a1) and states(s1, s2) given initial states0
MIglobal =
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a0, s1, a1, s2|s0)
p(a0, a1|s0)p(s1, s2|s0)
equals the sum of the per-step mutual information
MIlocal =
X
a0,s1
p(a0, s1|s0) ln p(a0, s1|s0)
p(a0|s0)p(s1|s0) +
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a1, s2|s1)
p(a1|s1)p(s2|s1)
where p(a0, s1, a1, s2|s0) = π(a0|s0)p(s1|s0, a0)π(a1|s1)p(s2|s1, a1) and p(a0, s1|s0) = π(a0|s0)p(s1|s0, a0).
Using Bayes’ rule and the Markov property, the above quantities can be rewritten as
45
MIglobal =
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a0, a1|s0, s1, s2)
p(a0, a1|s0)
=
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a0|s0, s1)p(a1|s1, s2)
p(a0, a1|s0)
=
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a0|s0, s1)p(a1|s1, s2)
π(a0|s0)p(a1|s0, a0)
=
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a0|s0, s1)p(a1|s1, s2)
π(a0|s0) P
s π(a1|s)p(s|s0, a0)
=
X
a0,s1
p(a0, s1|s0) ln p(a0|s0, s1)
π(a0|s0) +
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a1|s1, s2)P
s π(a1|s)p(s|s0, a0)
and
MIlocal =
X
a0,s1
p(a0, s1|s0) ln p(a0|s0, s1)
π(a0|s0) +
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln p(a1|s1, s2)
π(a1|s1)
The quantitiesMIglobal and MIlocal are remarkable similar except for the denominator in theln of the last
term in each expression. Therefore, equality between MIglobal and MIlocal holds iff
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) ln
X
s
π(a1|s)p(s|s0, a0) =
X
a0,a1,s1,s2
p(a0, s1, a1, s2|s0) lnπ(a1|s1),
which is not true for all choices of policy and transitions probabilities. To see this, take a Markov chain where
the actiona0 = 0 from s0 = 0 is deterministic, but results in two possible successor statess1 = 1 or s1 = 2
with equal probability1/2. From s1 = 1 the policy takes actionsa1 = 1 and a1 = 2 with probability1/2.
Froms1 = 2 the policy is deterministic, that is,a1 = 3 with probability1. A simple calculation shows that
the left side equals−3
2 ln 2, while the right side equals a different quantity,−1
2 ln 2.
A.10 Video captions
A.10.0.1 Video 1 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the ϵ-greedy R agent for the four-room grid world environment (see main text, Fig. 2, for more details).
A.10.0.2 Video 2 Animation of a portion of an episode of the MOP agent (mouse) behaving in the
predator-prey scenario detailed in Fig. 3.
A.10.0.3 Video 3 Animation of a portion of an episode of the R agent (mouse) behaving in the predator-
prey scenario detailed in Fig. 3.
A.10.0.4 Video 4 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the R agent for the cartpole experiment detailed in Fig. 4.
A.10.0.5 Video 5 Animation of the state space trajectories (in an angle-position projection) traveled by
the MOP and the R agents from Video 4, sped up four times the original frame rate.
A.10.0.6 Video 6 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the lifetime-matchingϵ−greedy R agent for the cartpole experiment detailed in Fig. 10.
A.10.0.7 Video 7 Animation of a portion of an episode comparing the behaviors of the MOP agent
and the MPOW and EFE agents for the four-room grid world environment (see main text, Fig. 6, for more
details).
46
A.10.0.8 Video 8 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the MPOW and EFE agents for the cartpole experiment (corresponding to Fig. 6).
A.10.0.9 Video 9 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the R agent for the quadruped experiment without energetic constraints (corresponding to upper row of Fig.
7).
A.10.0.10 Video 10 Animation of a portion of an episode comparing the behaviors of the MOP agent
and the R agent for the quadruped experiment with energetic constraints (corresponding to lower row of Fig.
7).
47