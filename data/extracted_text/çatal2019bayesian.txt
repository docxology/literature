PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
BAYESIAN POLICY SELECTION
USING ACTIVE INFERENCE
OzanC¸atal,JohannesNauta,TimVerbelen,PieterSimoens,&BartDhoedt
IDLab,DepartmentofInformationTechnology
GhentUniversity-imec
Ghent,Belgium
ozan.catal@ugent.be
ABSTRACT
Learningtotakeactionsbasedonobservationsisacorerequirementforartificial
agentstobeabletobesuccessfulandrobustattheirtask. ReinforcementLearn-
ing(RL)isawell-knowntechniqueforlearningsuchpolicies. However,current
RL algorithms often have to deal with reward shaping, have difficulties gener-
alizing to other environments and are most often sample inefficient. In this pa-
per,weexploreactiveinferenceandthefreeenergyprinciple,anormativetheory
from neuroscience that explains how self-organizing biological systems operate
bymaintainingamodeloftheworldandcastingactionselectionasaninference
problem.WeapplythisconcepttoatypicalproblemknowntotheRLcommunity,
themountaincarproblem,andshowhowactiveinferenceencompassesbothRL
andlearningfromdemonstrations.
1 INTRODUCTION
Activeinferenceisanemergingparadigmfromneuroscience(Fristonetal.,2017),whichpostulates
that action selection in biological systems, in particular the human brain, is in effect an inference
problem where agents are attracted to a preferred prior state distribution in a hidden state space.
Contrarytomanystate-of-theartRLalgorithms,activeinferenceagentsarenotpurelygoaldirected
andexhibitaninherentepistemicexploration(Schwartenbecketal.,2018).Inneuroscience,theidea
ofusingactiveinference(Friston,2010;Fristonetal.,2006)tosolvedifferentcontrolandlearning
taskshasalreadybeenexplored(Fristonetal.,2012;2013;2017). Theseapproacheshowevermake
useofmanuallyengineeredtransitionmodelsandpredefined,oftendiscrete,statespaces.
Inthisworkwemakeafirststeptowardsextrapolatingthisinferenceapproachtoactionselection
by artificial agents by the use of neural networks for learning a state space, as well as models for
posterior and prior beliefs over these states. We demonstrate that by minimizing the variational
freeenergyadynamicsmodelcanbelearnedfromtheproblemenvironment,whichissufficientto
reconstructandpredictenvironmentobservations.Thisdynamicsmodelcanbeleveragedtoperform
inferenceonpossibleactionsaswellastolearnhabitualpolicies.
2 ACTIVE INFERENCE & THE FREE ENERGY PRINCIPLE
Freeenergyisacommonlyusedquantityinmanyscientificandengineeringdisciplines,describing
the amount of work a (thermodynamic) system can perform. Physical systems will always move
towards a state of minimal free energy. In active inference the concept of variational free energy
is utilized to describe the drive of organisms to self-organisation. The free energy principle states
thateveryorganismentertainsaninternalmodeloftheworld, andimplicitlytriestominimizethe
differencebetweenwhatitbelievesabouttheworldandwhatitperceives,thusminimizingitsown
variationalfreeenergy(Friston,2010),oralternativelytheBayesiansurprise.Concretelythismeans
thateveryorganismoragentwillactivelydriveitselftowardspreferredworldstates,akindofglobal
prior,thatitbelievesaprioriitwillvisit. Inthecontextoflearningtoact,thissurpriseminimization
boilsdowntotwodistinctobjectives. Ontheonehandtheagentactivelysamplestheworldtofine
1
9102
rpA
52
]GL.sc[
2v94180.4091:viXra
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
tuneitsinternalmodeloftheworldandbetterexplainobservations.Ontheotherhandtheagentwill
bedriventovisitpreferredstateswhichcarrylittleexpectedfreeenergy.
Formally,anagententertainsagenerativemodelP(o˜,a˜,s˜)oftheenvironment,whichspecifiesthe
jointprobabilityofobservations,actionsandtheirhiddencauses,whereactionsaredeterminedby
somepolicyπ. ThereaderisreferredtoAppendixAforanoverviewoftheusednotation.
IftheenvironmentismodelledasaMarkovDecisionProcess(MDP)thisgenerativemodelfactor-
izesas:
T
(cid:89)
P(o˜,a˜,s˜)=P(π)P(s ) P(o |s )P(s |s ,a )P(a |π) (1)
0 t t t t−1 t t
t=1
ThefreeenergyorBayesiansurpriseisthendefinedas:
F =E [logQ(s˜)−logP(s˜,o˜)]
Q
=D (Q(s˜)(cid:107)P(s˜|o˜))−logP(o˜) (2)
KL
=D (Q(s˜)(cid:107)P(s˜))−E [logP(o˜|s˜)]
KL Q
whereQ(s˜)isanapproximateposteriordistribution.Thesecondequalityshowsthatthefreeenergy
isminimizedwhentheKLdivergencetermbecomeszero,meaningthattheapproximateposterior
becomesthetrueposterior, inwhichcasethefreeenergybecomesthenegativelogevidence. The
thirdequalitythenbecomesthenegativeevidencelowerbound(ELBO),asweknowfromvariational
autoencoders(VAE)(Kingma&Welling,2013;Rezendeetal.,2014).Foracompletederivationthe
readerisreferredtoAppendixB.
Inactiveinferenceagentspickactionsthatwillresultinvisitingstatesoflowexpectedfreeenergy.
Concretely,agentsdothisbysamplingactionsfromapriorbeliefaboutpoliciesaccordingtohow
muchexpectedfreeenergythatpolicywillinduce. AccordingtoSchwartenbecketal.(2018)this
meansthattheprobabilityofpickingapolicyisgivenby
P(π)=σ(−γG(π))
(cid:88) T (3)
G(π)= G(π,τ)
τ
where σ is the softmax function with precision parameter γ, which governs the agents goal-
directedness and randomness in its behavior. G is the expected free energy at future time-step τ
underpolicyπ,whichcanbeexpandedinto:
G(π,τ)=E [logQ(s |π)−logP(o ,s |π)]
Q(oτ,sτ|π) τ τ τ
=E [logQ(s |π)−logP(o |s ,π)−logP(s |π)] (4)
Q(oτ,sτ|π) τ τ τ τ
=D (Q(s |π)(cid:107)P(s ))+E [H(P(o |s ))]
KL τ τ Q(sτ) τ τ
We used Q(o ,s |π) = P(o |s )Q(s |π) and that the prior probability P(s |π) is given by a
τ τ τ τ τ τ
preferredstatedistributionP(s ). Thisresultsintotwoterms: aKLdivergencetermbetweenthe
τ
predictedstatesandthepriorpreferredstates,andanentropytermreflectingtheexpectedambiguity
underpredictedstates. Actionselectioninactiveinferencethusentails:
1. EvaluateG(π)foreachpolicyπ
2. CalculatethebeliefoverpoliciesP(π)
3. InferthenextactionusingP(π)P(a |π)
t+1
However, examples applying this principle are often limited to cases with a discrete number of
predefinedpolicies,asotherwisecalculatingP(π)becomesintractable(Fristonetal.,2017).
3 NEURAL NETWORKS AS DENSITY ESTIMATORS
InordertoovercometheintractabilityofcalculatingP(π)wecharacterizetheapproximateposterior
withaneuralnetworkwithparametersφaccordingtothefollowingfactorization:
T
(cid:89)
Q(s˜)= q (s |s ,a ,o )
φ t t−1 t t
t=1
2
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
Figure1:Thevariouscomponentsoftheagentandtheircorrespondingtraininglosses.Weminimize
thevariationalfreeenergybyminimizingboththenegativeloglikelihoodofobservationsandtheKL
divergencebetweenthestatetransitionmodelandtheobservationmodel. Theinferredhiddenstate
ischaracterizedasamultivariateGaussiandistribution. Policylearningisachievedbyminimizing
theexpectedfreeenergyGbetweenthestatedistributionvisitedbythepolicyaccordingtotheState
TransitionModelandthestatedistributionvisitedbytheexpert.
Similarly we parameterise a likelihood model p (o |s ) and dynamics model p (s |s ,a ) as
ξ t t θ t t−1 t−1
neuralnetworkswithparametersξandθ.ThesenetworksoutputamultivariateGaussiandistribution
withdiagonalcovariancematrixusingthereparametrizationtrickfromKingma&Welling(2013).
Minimizingthefreeenergythenboilsdowntominimizingtheobjective:
∀t:minimize:−logp (o |s )+D (q (s |s ,a ,o )(cid:107)p (s |s ,a )) (5)
ξ t t KL φ t t−1 t t θ t t−1 t
φ,θ,ξ
Thenegativeloglikelihoodtermoftheobjectivepunishesreconstructionerror,forcingallinforma-
tionfromtheobservationsintothestatespace. TheKLtermpullsthepriordistribution,orthestate
transition model, to the posterior model, also known as the observation model, forcing it to learn
to encode state distributions from which observations can be reconstructed without having actual
access to these observations. This can be interpreted as a variational autoencoder (VAE), where
insteadofaglobalprior,thepriorisgivenbythestatetransitionmodel.
Actionselectionisthenrealizedbyusingthestatetransitionmodeltosamplefuturestates,givena
sequenceof(randomlysampled)actions. ForeachactionsequenceweevaluateGandweexecute
the first action of the sequence with minimal G. Any random sampling strategy can be used, for
examplethecrossentropymethodRubinstein(1996).
The cumbersome sampling according to expected free energy can be avoided by using amortized
inference. Inthiscasewealsoinstantiatea“habit”policyP(a |s )asaneuralnetworkthatmaps
t t
states to actions in a deterministic or stochastic way. We can train this neural network using back
propagationbyminimizingG.
Crucially,westillneedtodefinethepreferredstatesorglobalpriorP(s ). Whenwehaveaccessto
τ
arewardsignal,thiscanbeconvertedtoapreferredstatepriorbyputtingmoreprobabilitydensity
onrewardingstates. Otherwise,wecaninitializethesystemwithaflatprior(eachstateisequally
preferred),andincreasetheprobabilityofrewardingstatesaswevisitthem. Whenwehaveaccess
toanexpert,wecanuseexpertdemonstrationstoprovideaprioronpreferredstates,i.e. thestates
visitedbytheexpertpolicy.AnoverviewofthevariousmodelsandtrainlossesisshowninFigure1.
4 EXPERIMENTS
We validate our theoretical framework on the continuous mountain car problem from the OpenAI
Gym (Brockman et al., 2016), adapted to only provide noisy observations (and no access to the
velocitystate). Thisenvironment, althoughquitesimpleintermsofproblemcomplexity, provesa
goodinitialtrialproblemduetogreedyapproachesfailingatit.
3
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
(a)Resultingstatespaceduringarole-out. (b)Predictedobservationsplottedontopoftheground
truthobservations.
Figure2: Resultsofthemodeltrainingstage. InFigure(a)weseethateveryparameteroftheeight
dimensional state space encodes some element contributing to the prediction. Figure (b) shows
thatthemodelsarecapableofpredictinggroundtruthobservations,indicatingthattheyaccurately
learnedtheenvironmentdynamics.
We instantiate the state transition model, observation model and likelihood models as fully con-
nected neural networks with 64 hidden units. The agent’s internal state is parameterised as an 8
dimensional multivariate Gaussian s. We bootstrap the model by training on a random agent and
optimizing Eq. 5. Our random agent samples actions uniformly in [−1,1], with a 90% chance of
repeatingthepreviousaction. Figure2ashowsaplotoftheevolutionofthemeansoftheinternal
statesduringarandomrollout,whilstFigure2bshowsreconstructionsfromboththestatetransition
model and observation model from the same rollout. Note that the state transition model has no
access to any observations. The closeness between ground truth observations and state transition
modelreconstructionsillustratesthattheagenthassuccessfullylearnedaninternalworldrepresen-
tationsufficienttopredicttheworldevolutionfromasingleinitialobservation.
To construct a preferred state prior, we manually execute 5 “expert rollouts” in the environment.
TheserolloutscanbeseeninFigure3a.Fromtheserolloutsapreferredstatedistributionisextracted
(Figure3b). Youcanseethatinthebeginningofthesequence,thereisavarianceonthepreferred
states,whereastowardstheendthepreferredstatedistributionispeakedaroundthestatereflecting
the car being on the top of the mountain. This is equivalent with a sparse reward signal when
reachingthemountaintopattheendofthesequence. Similarly,onecanalsoengineerapreferred
statepriorbasedontherewardsignal,whichwediscussinAppendixC
Wenowusethisstatepolicyforactionselectionaccordingtheactiveinferencescheme. Wesample
randomrolloutsusingthestatetransitionmodelandcalculatetheexpectedfreeenergyGforeach.
ThisindeedselectsrolloutsthatsuccessfullyreachthemountaintopasshowninFigure3c. Next,
we also train a policy by minimizing G at every timestep as defined in Eq 4. After training, this
policyisindeedabletogeneralizetoanystartingposition,consistentlyreachingthemountaintop.
5 RELATED WORK
Model-free RL has been successfully proven to work on many game playing (Silver et al., 2016;
Hesseletal.,2017)androboticsproblems(Yahyaetal.,2017;Kalashnikovetal.,2018). However,
there are still some outstanding challenges, such as the issue of reward engineering (Popov et al.,
2017),generalizingtootherenvironments(Lanctotetal.,2017),andsampleinefficiency(Yu,2018).
Recentlytherehavebeenpromisingadvancementsintheareaofmodel-basedRL.Haetal. Ha&
Schmidhuber(2018)trainavariationalautoencoder(VAE)inconjunctionwitharecurrentdynamics
modeltocreateapredictiveworldmodel. Theyusethispredictivemodeltotrainacontrollerusing
evolutionstrategies(Salimansetal.,2017). StevenBohez(2018)trainjointlyapriorandposterior
model for a robotics navigation task. In MERLIN (Wayne et al., 2018), a neural memory module
4
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
(a) Expert roll-outs from 5 different start posi- (b) Preferred state distribution from expert roll-
tions. outs. Each curve indicates the distribution of a
latentdimensionduringrollout.
(c) Random imaginary rollouts with expected (d) Actual policy rollouts with different starting
freeenergyG,representedinthecolorbar.Lower positions.
Gisbetter.
Figure3: Resultsofthepolicylearning. Fromexpertrollouts(a)wedistillapreferredstatedistri-
bution(b). Samplingimaginaryrolloutsofthestatetransitionmodelresultsinbetterrolloutshaving
lowerexpectedfreeenergyG(c). Wecanusethissamplingtotrainanamortizedactiveinference
policythatsuccessfullysolvestheenvironmentfromanystartingposition(d).
is added on top of a VAE based dynamics model to facilitate learning policies over longer time
windows. Inasimilarvein(Srinivasetal.,2018)learnsabstractrepresentationforplanning.
Instead of using an explicit reward signal, policies can also be learned from demonstrations using
inverse reinforcement learning, basically learning a reward signal from an expert (Ng & Russell,
2000). Anotherapproachforlearningfromdemonstrationsisusingmeta-learningtoquicklydistill
apolicyfromanewdemonstration(Finnetal.,2017).
Active inference combines elements of all these works into a single theoretical framework. The
preferredstatespriorcanbeinterpretedasmoregenericformofvaluefunctioninRL.Itcombines
learningworldmodelswithactionselectionandplanning.Also,theconceptofminimizingexpected
Bayesiansurpriseresemblestheworkonartificialcuriosityforexploration(Grazianoetal.,2011).
6 CONCLUSION
Active inference might underlie the way biological agents perceive and interact with the world.
In this work we show that active inference and the free energy principle can also help artificial
agents interact with the world. Active inference also combines a lot of elements from recent RL
literature,suchasbuildingworldmodels,neuralplanning,artificialcuriosity,etc.Webeliefthisisan
interestingdirectionforfurtherresearch,toapplythisprincipletomorechallengingenvironments,
suchastheATARIdomainorrobotics.
5
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
ACKNOWLEDGMENTS
OzanCatalisfundedbyaPh.D.grantoftheFlandersResearchFoundation(FWO).
REFERENCES
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. CoRR,abs/1606.01540,2016. URLhttp://arxiv.org/
abs/1606.01540.
ChelseaFinn,TianheYu,TianhaoZhang,PieterAbbeel,andSergeyLevine. One-shotvisualimi-
tationlearningviameta-learning. CoRR,abs/1709.04905,2017. URLhttp://arxiv.org/
abs/1709.04905.
KarlFriston. Thefree-energyprinciple: Aunifiedbraintheory? NatureReviewsNeuroscience,11
(2):127–138, 2010. ISSN 1471003X. doi: 10.1038/nrn2787. URL http://dx.doi.org/
10.1038/nrn2787.
Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
PhysiologyParis,100(1-3):70–87,2006.ISSN09284257.doi:10.1016/j.jphysparis.2006.10.001.
Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal
controlwithoutcostfunctions.BiologicalCybernetics,106(8-9):523–541,2012.ISSN03401200.
doi: 10.1007/s00422-012-0512-8.
Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timothy Behrens,
and Raymond J. Dolan. The anatomy of choice: active inference and agency. Fron-
tiers in Human Neuroscience, 7(September):1–18, 2013. ISSN 1662-5161. doi: 10.3389/
fnhum.2013.00598. URLhttp://journal.frontiersin.org/article/10.3389/
fnhum.2013.00598/abstract.
KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanniPezzulo.
Activeinference: AProcessTheory. NeuralComputation,29:1–49,2017. ISSN1530888X. doi:
10.1162/NECO a 00912.
VincentGraziano,TobiasGlasmachers,TomSchaul,LeoPape,GiuseppeCuccu,JuxiLeitner,and
JrgenSchmidhuber. Artificialcuriosityforautonomousspaceexploration. ActaFutura,pp.41–
51,012011.
David Ha and Ju¨rgen Schmidhuber. World Models. arXiv preprint, 2018. doi: 10.5281/zenodo.
1207631. URLhttp://arxiv.org/abs/1803.10122{%}0Ahttp://dx.doi.org/
10.5281/zenodo.1207631.
MatteoHessel,JosephModayil,HadovanHasselt,TomSchaul,GeorgOstrovski,WillDabney,Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining Improvements
in Deep Reinforcement Learning. arXiv preprint, 2017. URL http://arxiv.org/abs/
1710.02298.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen,EthanHolly,MrinalKalakrishnan,VincentVanhoucke,andSergeyLevine.Qt-opt:Scal-
abledeepreinforcementlearningforvision-basedroboticmanipulation. CoRR,abs/1806.10293,
2018. URLhttp://arxiv.org/abs/1806.10293.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2013. URLhttp://arxiv.org/abs/1312.6114.
MarcLanctot,Vin´ıciusFloresZambaldi,AudrunasGruslys,AngelikiLazaridou,KarlTuyls,Julien
Pe´rolat,DavidSilver,andThoreGraepel. Aunifiedgame-theoreticapproachtomultiagentrein-
forcementlearning. CoRR,abs/1711.00832,2017. URLhttp://arxiv.org/abs/1711.
00832.
6
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
AndrewY.NgandStuartJ.Russell. Algorithmsforinversereinforcementlearning. InProceedings
oftheSeventeenthInternationalConferenceonMachineLearning,ICML’00,pp.663–670,San
Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2. URL
http://dl.acm.org/citation.cfm?id=645529.657801.
Ivaylo Popov, Nicolas Heess, Timothy P. Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej
Vecerik,ThomasLampe,YuvalTassa,TomErez,andMartinA.Riedmiller. Data-efficientdeep
reinforcementlearningfordexterousmanipulation. CoRR,abs/1704.03073,2017. URLhttp:
//arxiv.org/abs/1704.03073.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Eric P. Xing and Tony Jebara (eds.), Pro-
ceedingsofthe31stInternationalConferenceonMachineLearning, volume32ofProceedings
of Machine Learning Research, pp. 1278–1286, Bejing, China, 22–24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/rezende14.html.
Reuven Y. Rubinstein. Optimization of computer simulation models with rare events. European
JournalofOperationsResearch,99:89–112,1996.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as
aScalableAlternativetoReinforcementLearning. arXivpreprint,pp.1–13, 2017. ISSN1744-
4292. doi: 10.1.1.51.6328. URLhttp://arxiv.org/abs/1703.03864.
PhilippSchwartenbeck, JohannesPassecker, TobiasHauser, ThomasHBFitzGerald, MartinKro-
nbichler, and Karl J Friston. Computational mechanisms of curiosity and goal-directed explo-
ration. bioRxiv, pp. 411272, 2018. doi: 10.1101/411272. URL https://www.biorxiv.
org/content/early/2018/09/07/411272.
DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
MadeleineLeach,KorayKavukcuoglu,ThoreGraepel,andDemisHassabis. Masteringthegame
of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. ISSN
14764687. doi: 10.1038/nature16961.
AravindSrinivas,AllanJabri,PieterAbbeel,SergeyLevine,andChelseaFinn. UniversalPlanning
Networks. Proceedingsof the35th InternationalConference onMachine Learning, Stockholm,
Sweden,PMLR,2018. ISSN1938-7228. URLhttp://arxiv.org/abs/1804.00645.
Sam Leroux Elias De Coninck Bert Vankeirsbilck Pieter Simoens Bart Dhoedt Steven Bohez,
Tim Verbelen. Robot navigation using a variational dynamics model for state estimation and
robustcontrol. InDeepRLworkshopNeurIPS2018,December2018.
Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-
Barwinska,JackRae,PiotrMirowski,JoelZ.Leibo,AdamSantoro,MevlanaGemici,Malcolm
Reynolds,TimHarley,JoshAbramson,ShakirMohamed,DaniloRezende,DavidSaxton,Adam
Cain, Chloe Hillier, David Silver, Koray Kavukcuoglu, Matt Botvinick, Demis Hassabis, and
TimothyLillicrap. UnsupervisedPredictiveMemoryinaGoal-DirectedAgent. arXivpreprint,
2018. URLhttp://arxiv.org/abs/1803.10760.
AliYahya,AdrianLi,MrinalKalakrishnan,YevgenChebotar,andSergeyLevine. Collectiverobot
reinforcement learning with distributed asynchronous guided policy search. In IEEE Interna-
tionalConferenceonIntelligentRobotsandSystems,volume2017-Septe,pp.79–86,2017.ISBN
9781538626825. doi: 10.1109/IROS.2017.8202141.
YangYu. Towardssampleefficientreinforcementlearning. InProceedingsoftheTwenty-Seventh
InternationalJointConferenceonArtificialIntelligence,IJCAI-18,pp.5739–5743.International
JointConferencesonArtificialIntelligenceOrganization,72018. doi: 10.24963/ijcai.2018/820.
URLhttps://doi.org/10.24963/ijcai.2018/820.
7
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
Appendices
A GLOSSARY
Definition Description
s Stateattimet
t
o Observationattimet
t
a Actionattimet
t
s Expectedstateatafuturetimestepτ
τ
s˜ Sequenceofstates
o˜ Sequenceofobservations
a˜ Sequenceofactions
P(o˜,a˜,s˜) Generativemodeloftheagent
P(o |s ) Likelihoodmodel
t t
π Policy
P(s |s ,a ) Statetransitionmodel
t t−1 t−1
P(a |π) Actionattimetgivenapolicy
t
P(π) Beliefoverpolicies
P(s˜|o˜) True posterior about hidden states given a se-
quenceofobservations
Q(s˜) Approximateposteriorabouthiddenstates
F =E [logQ(s˜)−logP(s˜,o˜)] Freeenergy
Q(s)
G(·) Expectedfreeenergy
σ(z)
j
=ezj/ (cid:80)
k
ezk SoftmaxorBoltzmanndistribution
γ Precision parameter governing goal-directedness
andrandomness
Table1: Glossarytablewithbriefdescriptionoftheusedtermsandtheirdefinitions
8
PublishedintheproceedingsoftheWorkshopon“Structure&PriorsinReinforcementLearning”
atICLR2019
B FREE ENERGY DERIVATION
Startingfromthedefinitionoffreeenergy:
F =E [logQ(s)−logP(s,o)]
Q (cid:101) (cid:101) (cid:101)
whereQ(x)isanapproximateposteriordistribution. WecanexpandthefreeenergyF asfollows:
(cid:101)
F =E [logQ(s)−logP(s,o)]
Q (cid:101) (cid:101) (cid:101)
=E [logQ(s)−logP(s|o)−logP(o)]
Q (cid:101) (cid:101)(cid:101) (cid:101)
=D (Q(s)(cid:107)P(x|o))−logP(o),
KL (cid:101) (cid:101) (cid:101) (cid:101)
Similarly,wecanalsorewritethefreeenergyexpressionas:
F =E [logQ(s)−logP(s,o)]
Q (cid:101) (cid:101) (cid:101)
usingtheidentity P(s,o)=P(o|s)P(s)
(cid:101) (cid:101) (cid:101)(cid:101) (cid:101)
=E [logQ(s)−logP(s)−logP(o|s)]
Q (cid:101) (cid:101) (cid:101)(cid:101)
=D (Q(s)(cid:107)P(s))−E [logP(o|s)]
KL (cid:101) (cid:101) Q (cid:101)(cid:101)
C A REWARD-BASED PREFERRED STATE PRIOR
In a traditional RL setting, an agent only has access to a reward signal, rather than expert demon-
strations. In this case the agent will need to find the preferred state prior that matches the reward
signal(higherprobabilityforstatesthatyieldhigherreward),whichisequivalenttolearningavalue
functionV(·)inRL.Inthemountaincarproblem,onlyasparserewardof+1isgivenwhenreaching
thetopofthemountain.Basedonthisrewardfunction,wecandefinethepreferredstatedistribution
asaGaussiancenteredaroundthestateswherethecarreachesthetopofthemountainaftertimestep
100.Wecanusethispriorinsteadoftheoneinducedbythedemonstrationsforactiveinference. We
seeinFigure4thatagaintrajectoriesreachingthetopresultinthelowestexpectedfreeenergy.
Figure4:RandomimaginaryrolloutswithcorrespondingexpectedfreeenergyGwhenthepreferred
stateisdefinedasthosestatesthatgivereward. LowerGisbetter.
9