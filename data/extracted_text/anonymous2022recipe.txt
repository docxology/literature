This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
6
A Recipe for Designing Active Inference Models
Give me six hours to chop down a tree and I will spend the first four sharpening
the axe.
— Abraham Lincoln
6.1 Introduction
This chapter provides a four-s tep r ecipe to construct an Active Inference
model, discussing the most import ant design choices one has to make to
realize a model and providing some guidelines for t hose choices. It serves as
an introduction to the second part of the book, which will illustrate several
specific computational models using Active Inference and their applica-
tions in a variety of cognitive domains.
As Active Inference is a normative approach, it tries to explain as much as
poss ib le about beh avi or, cognitive, and neural proc esses from first princip les.
Consistently, the design philosophy of Active Inference is top-d own. Unlike
many other approaches to computational neuroscience, the challenge is
not to emulate a brain, piece by piece, but to find the generative model that
describes the probl em the brain is trying to solve. Once the probl em is appro-
priately formalized in terms of a generative model, the solution to the prob-
lem emerges u nder Active Inference—w ith accompanying predictions about
brains and minds. In other words, the generative model provides a complete
description of a system of interest. The resulting beh avi or, inference, and
neural dynamics can all be derived from a model by minimizing free energy.
The generative modeling approach is used in several disciplines for the real-
ization of cognitive models, statistical modeling, experimental data analy sis,
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
106 Chapter 6
and machine learning (Hinton 2007b; Lee and Wagenmakers 2014; Pezzulo,
Rigoli, and Friston 2015; Allen et al. 2019; Foster 2019). H ere, we are pri-
marily interested in designing generative models that engender cognitive
proc esses of interest. We have seen this design methodology in previous
chapters. For example, using a generative model for predictive coding, per-
ception was cast as an inference about the most likely cause of sensations;
using a generative model that evolves in discrete time, planning was cast
as an inference about the most likely course of action. Depending on the
probl em of interest (e.g., planning during spatial navigation or planning
saccades during visual search), one can adapt the form of these generative
models to equip them with diff ere nt structures (e.g., shallow or hierarchi-
cal) and variables (e.g., beliefs about allocentric or egocentric spatial loca-
tions). Importantly, Active Inference may take on many diff ere nt guises
under diff ere nt assumptions about the form of the generative model being
optimized. For example, assumptions about models that evolve in discrete
or continuous time influence the form of the message passing (see chap-
ter 4). This implies that the choice of a generative model corresponds to
specific predictions about both beh avi or and neurobiology.
This flexibility is useful as it allows us to use the same language to describe
proc esses in multiple domains. However, it can also be confusing from a
practical perspective, as there are a number of choices that must be made
to find the appropriate level of description for the system of interest. In
the second part of this book, we w ill try to resolve this confusion through
a series of illustrative examples of Active Inference in silico. This chapter
introduces a general r ecipe for the design of Active Inference models, high-
lighting some of the key design choices, distinctions, and dichotomies that
will appear in the numerical analys is of computational models described in
subsequent chapters.
6.2 Designing an Active Inference Model: A R ecipe in Four Steps
Designing an Active Inference model requires four foundational steps, each
resolving a specific design question:
1. Which system are we modeling? The first choice to make is always the
system of interest. This may not be as s imple as it seems; it rests on
the identification of the bounda ries (i.e., Markov blanket) of that sys-
tem. What counts as an Active Inference agent (generative model), what
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 107
counts as the external environment (generative proc ess), and what is the
interface (sensory data and actions) between them?
2. What is the most appropriate form for the generative model? The first
of the next three practical challenges is deciding w hether it is appropri-
ate to think of a proc ess more in terms of categorical (discrete) inferences
or continuous inferences, motivating the choice between discrete or
continuous-t ime implementations (or a hybrid) of Active Inference. Then
we need to select the most appropriate hierarchical depth, motivating
the choice between shallow versus deep models. Fin ally, we need to con-
sider w hether it is necessary to endow generative models with temporal
depth and the ability to predict action-c ontingent observations to support
planning.
3. How to set up the generative model? What are the generative model’s
most appropriate variables and priors? Which parts are fixed and what
must be learned? We emphasize the importance of choosing the right
sort of variables and prior beliefs; furthermore, we emphasize a sepa-
ration in timescales between the (faster) update of state variables that
occurs during inference and the (slower) update of model para meters
that occurs during learning.
4. How to set up the generative proc ess? What are the elem ents of the
generative proc ess (and how do they differ from the generative model)?
These four steps (in most cases) suffice to design an Active Inference
model. Once completed, the beh avi or of the system is determined by the
standard schemes of Active Inference: the descent of the active and internal
states on the free energy functional associated with the model. From a more
practical perspective, once one has specified the generative model and gen-
erative proc ess, one can use standard Active Inference software routines to
obtain numerical results, as well as to perform data visualization, analys is,
and fitting (e.g., model-b ased data analys is). In what follows, we w ill review
the four design choices in order.
6.3 What System Are We Modeling?
A useful first step in applying the formalism of Active Inference is to iden-
tify the bounda ries of the system of interest b ecause we are interested in
characterizing the interaction between what is internal to a system and the
external world via sensory receptors and effectors (e.g., muscles or glands). As
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
108 Chapter 6
discussed in chapter 3, a formal way to characterize the distinction between
internal states of a system and external variables (and intermediate vari-
ables that mediate their interactions) is in terms of a Markov blanket (Pearl
1988). To reiterate the argument, a Markov blanket may be subdivided into
two sorts of variables (Friston 2013): those that mediate the influence of
the external world on internal states of the system of interest (i.e., sensory
states) and t hose that mediate the influence of internal states of the system
of interest on the external world (i.e., active states). See figure 6.1.
Importantly, t here are many ways in which a boundary between internal
and external may be defined. In most of the simulations we w ill discuss in
the second part of this book, t here w ill be a (Markov blanket) separation
between an agent (roughly, a living organism) and its environment. This cor-
responds to the usual setup of cognitive models, where an agent implements
cognitive proc esses such as perception and action sel ection on the basis of its
internal (e.g., brain) states and is provided with sensors and effectors.
GENERATIVE PROCESS GENERATIVE MODEL
Active states
u
External states Internal states
x µ
Sensory states
y
Blanket states
b = (u, y)
Figure 6.1
Action-p erception loop between an adaptive system ( here, the brain) and the envi-
ronment, along with the Markov blanket (composed of active states and sensory
states) that mediates their interaction. The figure implies that the adaptive system
only affects the environment by performing actions (via active states) and that the
environment only affects the adaptive system by producing observations (via sensory
states). The figure exemplifies the distinction between the adaptive system’s genera-
tive model and the (external) generative proc ess that produces its observations.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 109
However, this is not the only possibility. From the perspective of neuro-
biology, we could draw a Markov blanket around a single neuron, around
the brain, or around the entire body. In the first case, sensory states include
postsynaptic receptor occupancies, and active states include the rate at
which vesicles containing neurotransmitters fuse with the presynaptic
membrane. The internal states of the neuron (e.g., membrane potentials,
calcium concentrations) can then be thought of as inferring the c auses of its
sensory states according to some (implicit) generative model (Palacios, Iso-
mura et al. 2019). This setup treats the external states (that are being mod-
eled) as including the neuronal network in which our neuron participates.
This is very diff ere nt from the inference taking place when we assume our
entire network is internal to the Markov blanket. For example, if we take a
system whose sensory states are the photoreceptors in the reti na and whose
active states are the oculomotor muscles, the inferences performed by the
internal states are about t hings outside the brain. This speaks to the impor-
tance of scale, as the internal states of this Markov blanket include the
internal states from the perspective of a single neuron. The latter internal
states appear to make inferences about t hings within the brain when the
Markov blanket is drawn around a single neuron but not when the blanket
is drawn around the ner vous system.
The above is particularly relevant when dealing with embodied or
extended perspectives on cognition (Clark and Chal mers 1998; Barsalou
2008; Pezzulo, Lw et al. 2011). For example, if we draw the blanket around
the ner vous system, the rest of the body becomes an external state, about
which we must make inferences from interoceptive sensory states (Allen
et al. 2019). Alternatively, we could draw our blanket around the entire
organism. This would make it look as if organs other than the brain were
making inferences about their environment. For example, depression of
the skin in response to an external pressure could be framed as an inference
about the source of the external pressure. The extended cognition perspec-
tive takes this further and says that objects external to the body may be
incorporated into the Markov blanket (e.g., the use of a calculator to assist
in inference implies that the calculator is part of the internal state-s pace
of the inferring system). Fin ally, we could have multiple Markov blankets,
nested within one another (e.g., brains, organisms, communities).
In sum, defining the Markov blanket ensures we know what is being inferred
(external states) and what is d oing the inferring. Indeed, minimization of f ree
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
110 Chapter 6
energy with res pect to a generative model only involves the internal and
active states of a system: t hese only see the sensory states, so they can only
infer the external state of the world vicariously.
6.4 What Is the Most Appropriate Form for the Generative Model?
Once we have dec ided on the internal states of a system and the states that
mediate their interaction with the world outside, we need to specify the
generative model that explains how external states influence sensory states.
As discussed in previous chapters, Active Inference can operate on dif-
fere nt kinds of generative models. Therefore, we need to specify the most
appropriate form of the generative model for the probl em at hand. This
implies making three main design choices. The first is a choice between
models that include continuous or discrete variables (or both). The sec-
ond is a choice between shallow models, in which inference operates on a
single timescale (i.e., all variables evolve at the same timescale), and hier-
archical or deep models, in which inference operates on multiple times-
cales (i.e., diff ere nt variables evolve at diff ere nt timescales). The third is
a choice between models that only consider pres ent observations versus
models having some temporal depth, which consider the consequences of
actions or plans.
6.4.1 Discrete or Continuous Variables (or Both)?
The first design choice is to consider w hether generative models that use dis-
crete or continuous variables are more appropriate. The former include object
identities, alternative action plans, and discretized repres ent at ions of continu-
ous variables. These are modeled through expressing the probability—at each
time step—of one variable transitioning into another type. The latter include
things like position, velocity, muscle length, and luminance and require a
generative model expressed in terms of rates of change.
Computationally, the distinction between the two may not be clear-c ut
because a continuous variable may be discretized, and a discrete variable
may be expressed through continuous variables. However, this distinction
is import ant conceptually, as it underlies specific hypotheses about the
time course (discrete or continuous) of the cognitive proc esses of interest.1
In most current implementations of Active Inference, high-l evel decision
proc esses, such as the choice between alternative courses of actions, are
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 111
modeled using discrete variables, whereas more fine-g rained perception and
action dynamics are implemented using continuous variables; we w ill pro-
vide examples of both in chapters 7 and 8, respectively.
Furthermore, the choice between discrete and continuous variables is
relevant for neurobiology. While each style of modeling appeals to free
energy minimization, the message passing t hese imply take diff ere nt forms.
To the extent that one considers message passing relevant for a proc ess the-
ory (see chapter 5), this implies that the neural dynamics that realize this
minimization are diff ere nt u nder each sort of model. Continuous schemes
underwrite predictive coding—a theory of neural proc essing that relies on
top-d own predictions corrected by bottom-up prediction errors. However,
the analogous proc ess theories for discrete inferences involve messages of a
diff ere nt form. Fin ally, the two types of model may be combined such that
discrete states are associated with continuous variables. This means we can
specify a generative model wherein a discrete state (e.g., object identity)
generates some pattern of continuous variables (e.g., luminance). We w ill
discuss an example of a hybrid or mixed generative model that includes
both discrete and continuous variables in chapter 8.
6.4.2 Timescales of Inference: Shallow versus Hierarchical Models
The second design choice concerns the timescales of Active Inference.
One can select e ither (shallow) generative models, in which all the vari-
ables evolve at the same timescale, or (hierarchical or deep) models, which
include variables that evolve at diff ere nt timescales: slower for higher levels
and faster for lower levels.
While many s imple cognitive models only require shallow models, t hese
are not sufficient when t here is a clear separation of timescales between dif-
fere nt aspects of a cognitive proc ess of interest. One example of this is in lan-
guage proc essing, in which short sequences of phonemes are contextualized
by the word that is spoken and short sequences of words are contextualized
by the current sentence. Crucially, the duration of the word transcends that
of any one phoneme in the sequence and the duration of the sentence tran-
scends that of any one word in the sequence. Hence, to model language pro-
cessing, one can consider a hierarchical model in which sentences, words,
and phonemes appear at diff ere nt (higher to lower) hierarchical levels and
evolve over (slower to faster) timescales that are approximately ind ep en-
dent of one another. This is only an approximate separation, as levels must
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
112 Chapter 6
influence each other (e.g., the sentence influences the next words in the
sequence; the word influences the next phonemes in the sequence).
However, this does not mean we need to attempt to model the entire
brain to develop meaningful simulations of a single level. For example, if
we wanted to focus on word proc essing, we could address some aspects
without having to deal with phoneme proc essing. This means we can treat
input from parts of the brain drawing inferences about phonemes as pro-
viding observations from the perspective of word-p rocessing areas. Phras-
ing this in terms of a Markov blanket, this typically means we treat the
inferences performed by lower levels of a model as part of the sensory states
of the blanket. This means we can summarize the inferences performed at
the timescale of interest without having to specify the details of lower-l evel
(faster) inferential processes—a nd this hierarchical factorization entails
great computational benefits.
Another example is in the domain of intentional action sel ection, where
the same goal (enter your apartment) can be active for an extended period
of time and contextualizes a series of subgoals and actions (find keys, open
door, enter) that are resolved at a much faster timescale. This separation of
timescales, w hether in the continuous or discrete domain, demands a hier-
archical (deep) generative model. In neuroscience, one can assume that cor-
tical hierarchies embed this sort of temporal separation of timescales, with
slowly evolving states at higher levels and rapidly evolving states at lower
levels, and that this recapitulates environmental dynamics, which also
evolve at multiple timescales (e.g., during perceptual tasks like speech recog-
nition or reading). In psyc holo gy, this sort of model is useful in reproducing
hierarchical goal proc essing (Pezzulo, Rigoli, and Friston 2018) and working
memory tasks (Parr and Friston 2017c) of the sort that rely on delay-p eriod
activity (Funahashi et al. 1989).
6.4.3 Temporal Depth of Inference and Planning
The third design choice concerns the temporal depth of inference. It is impor-
tant to draw a distinction between two kinds of generative model: the first
have temporal depth and represent explici tly the consequences of actions or
action sequences (policies or plans), whereas the second lack temporal depth
and only consider pres ent but not f uture observations. T hese two kinds of
model are exemplified in figure 4.3: the dynamic POMDP at the top and the
continuous-t ime model at the bottom.2 The key difference between t hese
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 113
two models is not that they use discrete or continuous variables, respec-
tively, but that only the former (temporally deep) model endows creatures
with the ability to plan ahead and select among poss ib le f utures.
Imagine a rodent who plans a route to a known food location in a maze.
Doing this benefits from a temporally deep model, loosely equivalent to a spa-
tial or cognitive map (Tolman 1948), which encodes contingencies between
pres ent and f uture locations conditioned on actions (e.g., the f uture location
after turning right or left). The animal can use the temporally deep model to
counterfactually consider multiple courses of action (e.g., series of right and
left turns) and select the one expected to reach the food location.
Why is a temporally deep model required for planning? In Active Infer-
ence, planning is realized by calculating the expected f ree energy associated
with diff ere nt actions or policies and then selecting the policy that is associ-
ated with the lowest expected f ree energy. Expected f ree energy is not just
a function of pres ent observations (like variational f ree energy) but also a
functional of f uture observations. The latter cannot be observed (by defini-
tion) but only predicted using a temporally deep model, which describes
the ways in which actions produce f uture observations.
When designing an Active Inference agent it is useful to consider w hether
it should have planning and future-o riented capacities—a nd, in this case, to
select a temporally deep model. Furthermore, it is useful to consider plan-
ning depth—t hat is, how far in the f uture the planning proc ess can look.
Fin ally, one can design generative models that are both hierarchical and
temporally deep, wherein planning proceeds at multiple timescales—f aster
at lower levels, and slower at higher levels.3 The decision w hether to model
alternative f utures, contingent on policy sel ection, is largely tied up with the
choice between discrete and continuous models b ecause the idea of selecting
between alternative f utures, defined by sequences of actions, is more simply
articulated using discrete-t ime models.
6.5 How to Set Up the Generative Model?
When we have specified our system of interest and identified the relevant
forms of the generative model (e.g., continuous or discrete repres ent at ion,
shallow versus hierarchical structure), our next challenges are to specify the
specific variables to include in the generative model and decide which of
these variables remain fixed or change as an effect of learning.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
114 Chapter 6
6.5.1 Setting Up the Variables of the Generative Model
The variables of generative models can be e ither predefined or learned from
data. For illustrative purposes, most models that we discuss in this book
use predefined variables. When designing these models, in practice, the
main challenge is deciding which hidden states, observations, and actions
are most appropriate for the probl em at hand. For example, the perceptual
model able to distinguish frogs from apples in chapter 2 only included two
hidden states (frogs, apples) and two observations (jumps, does not jump).
A more sophisticated model could include additional observations (e.g.,
red, green) as well as actions such as touching, which produce differential
sensory effects (jump or no jump) in the presence of a frog or an apple.
Figure 6.2 schematically illustrates a generative model for the concept
of a jumping frog. The concept is cast as a hierarchical model, where
Interoceptive sensations
(e.g., high heart rate)
Auditory sensations
(e.g., croaking)
Interoceptive percept
Auditory percept
Tactile percept
Tactile sensation
Jumping frog
(e.g., wrinkliness)
Visual percept
Action
(e.g., touching) Visual sensations
(e.g., green, jumping)
Figure 6.2
(Hierarchical) generative model for the concept of a jumping frog uses a simpli-
fied notation compared to chapter 4: nodes within the dotted circle correspond to
hidden states, whereas nodes at the periphery correspond to sensory observations.
Beliefs about hidden states, following inversion of the model, correspond to percepts
that may be tied to a sensory modality (e.g., visual percept) or may be amodal (e.g.,
the jumping frog). Action contingencies are represented as dashed lines. Horizon-
tal dependencies between hidden states in diff ere nt modalities, as well as temporal
dependencies between hidden states (as we saw in the dynamical generative models
of chapter 4), are ignored for the sake of simplicity.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 115
Box 6.1
Var ie t ies of sensory modalities: Exteroceptive, proprioceptive, and interoceptive
In Active Inference, a conceptual distinction is often made between three
kinds of sensory modalities: exteroceptive (e.g., vision and audition), proprio-
ceptive (e.g., the sense of joint and limb positions), and interoceptive (e.g.,
the sense of the internal organs of the body, such as heart and stomach). In
multimodal generative models, one can often factorize parts of the model that
relate to diff ere nt modalities; this permits representing that (for example) sac-
cadic movements have visual but not auditory consequences.
Importantly, the same princip les of Active Inference operate across all the
modalities. For example, in the same way visual proc essing can be described
as the inference about (hidden variables about) a perceptual scene, intero-
ceptive proc essing can be described as the inference about (hidden variables
that report) the internal state of the body. Furthermore, motor actions that
change the perceptual scene and internally directed actions that change the
interoceptive state can be described in a similar way. The former engages spi-
nal reflexes that fulfill proprioceptive predictions, whereas the latter engages
autonomic reflexes that fulfill interoceptive predictions. Such interoceptive
proc essing supports allostasis and adaptive regulation, and its dysfunctions
can have psychopathological consequences (Pezzulo 2013, Seth 2013, Pezzulo
and Levin 2015, Seth and Friston 2016, Allen et al. 2019).
a single (multimodal or supramodal) hidden state at the center of the
figure unfolds in a cascade of (unimodal) hidden states corresponding
to percepts in dif fer ent modalities (exteroceptive, proprioceptive, and
interoceptive; see box 6.1) and ultimately causing sensations in the same
modalities. This arrangement corresponds to casting the jumping frog con-
cept as the common cause of multiple sensory consequences (e.g., something
green and jumping in the visual domain; a croaking sound in the audi-
tory domain), some of which can be action- contingent (e.g., the sight of
something jumping may increase on touching it). The inversion of the
generative model corresponds to a perceptual inference (e.g., the presence
of a jumping frog) from its observed sensory consequences (e.g., the sight
of something green and jumpy), and it integrates information across mul-
tiple modalities.
Once t hese variables of interest have been established, the next exer-
cise is to write down the full generative model. One example is the s imple
generative model for frogs and apples in figure 2.1, which is fully specified
by prior beliefs about hidden states and a (likelihood) mapping between
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
116 Chapter 6
hidden states and observations and whose numerical values can be either
specified by hand or learned from data (see 6.5.2).
Beyond this s imple example, the elem ents that need to be specified are
fully determined by the form of the selected generative model. For exam-
ple, the model for discrete-t ime POMDP shown in figure 4.3 (top) requires
specifying the A, B, C, D, and E matrices; continuous schemes use analogous
(although less alphabetical) elem ents, which w ill be dealt with in chapter 8.
But even in these more complex cases, the exercise is not so dissimilar from
above: namely, specifying prior beliefs about the variables of interest (e.g., in
discrete-t ime implementations, about hidden states at the first time step in
the D-v ector and about observations in the C-m atrix) and their probabilistic
mappings (e.g., likelihood mapping between hidden states and observations
in the A-m atrix). However, in some cases, it is useful to think about factoriza-
tions of the state-s pace of the generative model, which avoids considering
every poss ib le combination of variables if some are unnecessary. In chapter 7,
we w ill discuss a biologically plausible example of factorization that occurs
in perceptual proc essing between “what” and “where” streams (Ungerleider
and Haxby 1994)—n amely, between variables that represent object identities
and locations, respectively, which can be treated ind ep end ently in the model
(hence simplifying it) as they are often invariant to one another.
Deciding which variables are of interest and the ways they are related or
factorized in the model is often the most challenging—b ut also the most
creative—p art of model design. It is an exercise of translating our cogni-
tive hypotheses into a mathematical form that supports Active Inference.
How should we select the “right” variables? Ultimately, this is a question
of specifying plausible alternatives and picking those that have the lowest
free energy (cf. Bayesian model comparison). However, a practically use-
ful perspective for most studies is that the generative model should be as
similar as poss ib le to how we believe data are generated. When appealing
to Active Inference in the setting of cognitive psyc holo gy, this often means
thinking about how experimental psychologists would go about generating
the stimuli they pres ent to their experimental participants. On formalizing
these proc esses in terms of the requisite probability distributions, we arrive
at a generative model whose f ree energy minimizing dynamics naturally
lead to perf orm ance of the task in question.
Here, we can draw an analogy with most Bayesian (or ideal observer)
models of perception, in which the models are designed to mimic (to a
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 117
Box 6.2
Priors and empirical beh avi or
Another perspective on the issue of selecting priors draws from a set of results
known as the complete class theorems (Wald 1947, Daunizeau et al. 2010), which
state that any statistical decision procedure (i.e., beh avi or) may be framed as
Bayes optimal u nder the right set of prior beliefs. This means that if we are
interested in explaining empirical beh avi or, our challenge is to identify the
generative model (comprising prior beliefs) that would reproduce that beh av-
ior as simply as poss ib le. In short, priors are a statement of a hypothesis about
the system in question. If other prior beliefs would be plausible, this offers an
opportunity to put this to empirical data through Bayesian model comparison.
This also has implications for computational phenotyping in clinical popula-
tions. That t here w ill always be a set of prior beliefs that render beh avi or Bayes
optimal implies the key question—in understanding the computational defi-
cits that give rise to psychiatric or neurological syndromes—is what t hese priors
are. This idea is slightly counterintuitive at first. However, the complete class
theorem means that asking w hether a beh avi or is (Bayes) optimal is meaning-
less. The import ant question is, What are the prior beliefs that would make this
optimal? In chapter 9, we w ill see how an appeal to f ree energy minimization
based on our own beliefs as scientists offers a way to answer this question.
large extent) the structure of the task at hand, as in the example of rec-
ognizing a frog or an apple (chapter 2). This idea is sometimes equated
with the good regulator theorem (Conant and Ashby 1970), which says that
to regulate an environment effectively, a creature (w hether biological or
synthetic) must be a good model of that system. From the perspective of
eco-n iche construction, this is sometimes phrased in terms of the (statisti-
cal) fitness (Bruineberg et al. 2018) of a creature’s model to its environment
(and vice versa). However, this does not mean that an agent’s generative
model has to be identical to the generative proc ess that actually generates
data. For most practical applications, it can be simplified or diff ere nt. We
will return to this point l ater in this chapter (6.6).
6.5.2 Which Parts of the Generative Model Are Fixed,
and What Is Learned?
Another design choice is deciding which parts of the generative model are fixed
and which ones are updated over time as an effect of learning. In princip le,
Active Inference allows e very part of the model—a nd even its structure—to be
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
118 Chapter 6
updated (or learned) over time. This renders learning a design choice rather
than something mandatory. In keeping with this, we w ill cover examples of
Active Inference models that are completely designed by hand and examples
in which some parts of the model (e.g., transition probabilities) remain fixed
while o thers (e.g., likelihoods) are updated over time.
In Active Inference, learning is cast as an aspect of inference, as a f ree
energy minimizing proc ess. So far, we have described inference in terms of
an update of beliefs about states of the generative model. In much the same
way, we can describe learning as an update of beliefs about para meters of the
generative model. For this, the generative model has to be endowed with
prior beliefs about para meters of the distributions to be learned, where the
specific para meters depend on probability distribution associated with each
variable (e.g., mean and variance for a Gaussian distribution). T hese prior
values are updated to form posterior beliefs whenever new data are encoun-
tered. As we w ill discuss in chapter 7, the algorithmic form of this update is
the same as the update of state variables.
The fact that both inference and learning use the same kind of Bayesian
belief updates may seem confusing during model design—p artly b ecause
deciding what should be modeled as a state or a par ame t er is not always
straightforward. However, when it comes to cognitive models, there is a
clear difference between inference and learning. Inference describes (fast)
changes of our beliefs about model states—f or example, how we update our
belief that there is an apple in front of us after observing something red.
Learning describes (slow) changes of our beliefs about model para meters—
for example, how we update our likelihood distribution to increase the value
of the apples-r ed mapping a fter observing several occurrences of red apples.
Beliefs about para meters typically vary much more slowly than t hose about
states, and they may only be updated after states have been inferred. From
a neurobiological perspective, it is appealing to map inference to neuro-
nal dynamics and learning to synaptic plasticity. Furthermore, as we will
discuss in chapter 7, holding probabilistic beliefs about model para meters
induces novelty-s eeking beh avi ors so that creatures may select the best data
to learn the causal structure of their worlds. This suggests that endowing
Active Inference models with the ability to learn their par ameters (or even
their structure; see chapter 7) is an effective way to study the behavioral
dynamics of active learning and curiosity-b ased exploration.
Before concluding this section, it is worth noting that in this book we
exemplify rather simple generative models that are defined using tabular
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 119
methods (e.g., with explicit matrices for priors and likelihoods) and that
operate in small state-s paces. In comparison, much more sophisticated
kinds of generative models—a nd associated learning schemes—a re being
developed in fields like machine learning, deep learning, and robotics, such
as, for example, variational autoencoders (Kingma and Welling 2014), gen-
erative adversarial networks (Goodfellow et al. 2014), recursive cortical net-
works (George et al. 2017), and world models (Ha and Schmidhuber 2018).
In princip le, one could borrow any of t hese methods (and many o thers) to
implement one or more parts of Active Inference models (e.g., likelihood
or transition models). By leveraging the most up-t o-d ate machine learning
methods, it would be poss ib le to scale up Active Inference to increasingly
more challenging domains and applications; see, for example, Ueltzhöffer
(2018) and Millidge (2019).
However, t here are some import ant points to consider when designing
Active Inference models that use sophisticated machine learning models,
especially if one is interested in cognitive and neurobiological implications.
One appeal of Active Inference is that it offers an integrative perspective on
cognitive functions by assuming that (for example) perceptual inference,
action planning, and learning all stem from the same f ree energy minimiza-
tion proc ess. This integrative power would be lost if (for example) one juxta -
posed generative models that operate or learn ind ep end ently from one
another. Furthermore, the aforementioned machine learning methods cor-
respond to proc ess models that are distinct from Active Inference and have
diff ere nt cognitive and neurobiological interpretations. Fin ally, when using
machine learning methods, some of the design choices discussed h ere (e.g.,
about the choice of model variables) may be skipped, as they are emergent
properties of learning; however, they may be replaced by diff ere nt design
choices, about (for example) number of layers, para meters, and learning
rates of a deep neural net. T hese design choices potentially have relevant
cognitive and neurobiological implications, which are beyond the scope of
what we address h ere.
6.6 Setting Up the Generative Proc ess
In Active Inference, the generative pro cess describes the dynamics of the world
external to the Active Inference agent, which corresponds to the proc ess
that determines the agent’s observations (see figure 6.1). It may seem bizarre
to have postponed defining the generative proc ess u ntil a fter describing the
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
120 Chapter 6
agent’s generative model. A fter all, a modeler would have some task (and
generative proc ess) in mind from the beginning, so it would make perfect
sense to revert this order and design the generative proc ess before the gen-
erative model, especially in applications where the generative model has to
be learned during situated interactions, as in gamelike or robotic settings
(Ueltzhöffer 2018, Millidge 2019, Sancaktar et al. 2020).
The reason we postponed the design of the generative proc ess is that, in
many practical applications discussed in this book, we simply assume that
the dynamics of the generative proc ess are the same as, or very similar to,
the generative model. In other words, we generally assume that the agent’s
generative model closely mimics the proc ess that generates its observations.
This is not the same as saying that the agent has perfect knowledge of the
environment. Indeed, even if the agent knows the proc ess that generates its
observations, it may be uncertain about (for example) its initial state in the
proc ess, as was the case in the apple versus frog example. In the language of
discrete-t ime Active Inference, one could design a model in which both the
generative model and the generative proc ess are characterized by the same
A-m atrix but in which the agent’s belief about its initial state (D- vector),
which is part of its generative model, is dif fere nt from—or even inconsis-
tent with—t he true initial state of the generative pro cess. One subtle t hing
to notice is that even if both the generative model and the generative pro-
cess are characterized by the same A- and B-m atrices, their semantics are
dif fer ent. The A-m atrix of the generative proc ess is an objective property of
the environment (sometimes called a mea sure ment distribution in Bayesian
models), whereas the A-m atrix of the generative model encodes an agent’s
subjective belief (called a likelihood function in Bayesian models).
Of course, except in the simplest cases, it is not mandatory that the gen-
erative model and generative proc ess are the same. In practical implemen-
tations of Active Inference, one can always specify the generative proc ess
separately from the generative model, e ither using equations that differ
from t hose of the generative model or using other methods, such as game
simulators, which take actions as inputs and provide observations as out-
puts (Cullen et al. 2018), thereby following the usual action-p erception
loop implied by the Markov blanket of figure 6.1.
There are some philosophical implications of designing generative mod-
els that are similar or dissimilar from the generative proc ess (Hohwy 2013;
Clark 2015; Pezzulo, Donnarumma et al. 2017; Nave et al. 2020, Tschantz
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 121
et al. 2020). As discussed above, the good regulator theorem (Conant and
Ashby 1970) says that an effective adaptive creature must have or be a good
model of the system it regulates. However, this can be achieved in vario us
ways. First, as discussed so far, the creature’s generative model can mimic
(at least to a g reat extent) the generative proc ess. Models developed in this
way may be called explicit or environmental models, given the resemblance
between their internal states and the environment’s external states. Second,
the creature’s generative model can be much more parsimonious than (and
even significantly diff ere nt from) the generative proc ess, to the extent that
it correctly manages those aspects of the environment that are useful to act
adaptively in it and achieve the creature’s goals. Models developed in this
way may be called sensorimotor or action oriented, as they mostly encode
action-o bservation (or sensorimotor) contingencies and their primary role
is supporting goal-d irected actions as opposed to providing an accurate
description of the environment.
The difference between explicit and action-o riented models can be
appreciated if we consider diff ere nt ways one can model (for example) a
rodent trying to escape from a maze in which some corridors are dead ends.
An explicit generative model may resemble a cognitive map of the maze
and provide a detailed characterization of external entities, such as spe-
cific locations, corridors, and dead ends. This model may permit the rodent
to escape from the maze using map-b ased navigation. An action-o riented
model may instead encode contingencies between whisker movements and
touch sensations. This latter model would afford the sel ection of contextu-
ally appropriate strategies, such as moving forward (if no touch sensation
is experienced or expected) or changing direction (in the opposite case)—
eventually permitting the rodent to escape from the maze without explic-
itly representing locations, corridors, or dead ends. T hese two kinds of
model prompt diff ere nt philosophical interpretations of Active Inference,
considering generative models as ways to either reconstruct the external
environment (explicit) or afford accurate action control (action oriented).
Fin ally, as discussed in the field of morphological computation (Pfeifer and
Bongard 2006), some aspects of a creature’s or a robot’s control can be out-
sourced to the body and hence do not need to be encoded in its generative
model. One example is the passive dynamic walker: a physical object resem-
bling a h uman body, composed of two “legs” and two “arms,” which is able
to walk an incline with no sensors, motors, or controllers (Collins et al.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
122 Chapter 6
2016). This example implies that at least some aspects of locomotion (or
other abilities) can be achieved with body mechanics that are carefully tuned
to exploit environmental contingencies (e.g., an appropriate body weight or
size to walk without slipping); therefore, t hese contingencies do not need to
be encoded in the creature’s generative model. This suggests an alternative
way to design Active Inference agents (and their bodies) that are—as opposed
to have—g ood models of their environment. Yet all the ways to design Active
Inference models are not mutually alternative but can be appropriately com-
bined, depending on the probl em of interest.
6.7 Simulating, Visualizing, Analyzing, and Fitting Data
Using Active Inference
In most practical applications, once the generative model and generative
proc ess have been defined, one only needs to use the standard procedure
of Active Inference—t he descent of the active and internal states on the
free energy functional associated with the model—to obtain numerical
results. Arguably, modelers’ goals are to simulate, visualize, analyze, and fit
data (e.g., conduct model-b ased data analys is). Standard routines for Active
Inference that provide support for all these functions are freely available
(https:// www . fil . ion . ucl . ac . uk / spm / ); an annotated example of using these
routines is provided in appendix C.
Although in most cases Active Inference procedures function off-t he-
shelf, in some practical applications one may consider specific fine-t unings
or changes. For example, specifying the temporal depth of planning defines
how many f uture states are considered during expected f ree energy compu-
tations. Setting up a l imited temporal depth, along with other approxima-
tions to exhaustive search such as sampling (Fountas et al. 2020), may be
useful in practical applications of Active Inference in large state-s paces.
Another example of adapting the standard functioning of Active Infer-
ence is the selective removal of parts of the expected f ree energy equation.
This ablation may be useful to compare standard Active Inference (that uses
expected f ree energy) with reduced versions, in which some parts of the
expected f ree energy are suppressed to render them formally analogous to
(for example) KL control or utility maximization systems (Friston, Rigoli
et al. 2015). Furthermore, one can also augment Active Inference models
with additional mechanisms, such as habitual learning (Friston, FitzGerald
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
A Recipe for Designing Active Inference Models 123
et al. 2016) or learning rate modulation (Sales et al. 2019), with the caveat
that maintaining the normative character of Active Inference would require
casting t hese additional mechanisms in terms of f ree energy minimization.
Fin ally, other fine-t unings or changes to Active Inference may be useful
to characterize disorders of inference and psychopathological conditions—
for example, to explore the behavioral and neuronal consequences of endow-
ing a creature’s generative model with excessively strong (or weak) priors
via excessively high (or low) levels of neuromodulators. We will provide
some examples of Active Inference models that are relevant for psychopa-
thology in chapter 9.
6.8 Summary
In this chapter, we have outlined the most import ant design choices that
must be made in setting up an Active Inference model. We provided a r ecipe
in four steps and some guidelines to address the usual challenges that model
designers face. Of course, it is not necessary to follow the r ecipe in a rigid man-
ner. Some steps can be inverted (e.g., design the generative proc ess before the
generative model) or combined. But in general, t hese steps are all required.
This sets up the remainder of this book, which puts t hese ideas into practice
through a series of illustrative examples designed to showcase the theoretical
princip les presented in the first half of the book. In everyt hing that follows,
the only differences among the examples rest on the design choices we have
highlighted h ere. Part 2 illustrates systems with diff ere nt bounda ries, with
discrete or continuous dynamics at diff ere nt timescales, for which the choice
of prior beliefs is fundamental in reproducing beh avi or across many diff ere nt
domains—b ut all implementing the same Active Inference.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:ThomasParr,GiovanniPezzulo,KarlJ.Friston
DOI:10.7551/mitpress/12441.001.0001
ISBN(electronic):9780262369978
Publisher:TheMITPress
Published:2022
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025
MIT Press Direct
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided
comments on drafts of this book. The generous work of academic experts is essential
for establishing the authority and quality of our publications. We acknowledge with
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services.
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J.
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior /
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. |
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246583/c005000_9780262369978.pdf by guest on 12 December 2025