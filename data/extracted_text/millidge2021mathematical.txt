A MATHEMATICAL WALKTHROUGH AND DISCUSSION OF THE
FREE ENERGY PRINCIPLE
Beren Millidge
School of Informatics
University of Edinburgh
beren@millidge.name
Anil K Seth
Sackler Center for Consciousness Science
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
A.K.Seth@sussex.ac.uk
Christopher L Buckley
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
C.L.Buckley@sussex.ac.uk
5th October, 2021
ABSTRACT
The Free-Energy-Principle (FEP) is an inÔ¨Çuential and controversial theory which postulates a deep
connection between the stochastic thermodynamics of self-organization and learning through vari-
ational inference. SpeciÔ¨Åcally, it claims that any self-organizing system which can be statistically
separated from its environment, and which maintains itself at a non-equilibrium steady state, can be
construed as minimizing an information-theoretic functional ‚Äì the variational free energy ‚Äì and thus
performing variational Bayesian inference to infer the hidden state of its environment. This principle
has also been applied extensively in neuroscience, and is beginning to make inroads in machine
learning by spurring the construction of novel algorithms by which action, perception, and learning
can all be uniÔ¨Åed under a single objective. While its expansive and often grandiose claims have
spurred signiÔ¨Åcant debates in both philosophy and theoretical neuroscience, the mathematical depth
and lack of accessible introductions and tutorials for the core claims of the theory have often made
productive discussion challenging. Here, we aim to provide a mathematically detailed, yet intuitive
walk-through of the formulation and central claims of the FEP while also providing a discussion
of the assumptions necessary and potential limitations of the theory. Additionally, since the FEP
is a living theory, subject to internal controversy, change, and revision, we also present a detailed
appendix highlighting and condensing current perspectives as well as controversies about the nature,
applicability, and the mathematical assumptions and formalisms underlying the FEP.
arXiv:2108.13343v2  [cs.AI]  1 Oct 2021
A PREPRINT - 5TH OCTOBER , 2021
1 Introduction
The Free-energy principle (FEP) is a grand theory, arising out of theoretical neuroscience, with deep ambitions to
provide a uniÔ¨Åed understanding of the nature of self organisation under the rubric of Bayesian inference (K. Friston,
2010, 2019a; K. Friston & Ao, 2012a; K. Friston, Kilner, & Harrison, 2006). Perhaps the central postulate of this
theory is the ‚ÄòFree Energy Lemma‚Äô which states that one can interpret the internal dynamics of any self organizing
system with a Markov Blanket (to be deÔ¨Åned later), of any type and on any scale, as performing, on average, a kind
of elemental Bayesian inference upon the external environment that surrounds it (K. Friston, 2013, 2019b; K. Friston
& Ao, 2012b). More generally, it claims to provide a recipe, in terms of the statistical dependencies which underpin
a Markov Blanket, to deÔ¨Åne precisely and mathematically what it means to be a system at all (K. Friston, 2019b).
Understanding self-organization through the lens of inference provides a powerful perspective for understanding the
nature of self-organizing systems, as it allows one to immediately grasp the nature of the dynamics which undergird
self-organization, as well as apply the extremely large and powerful literature on Bayesian inference methods and
algorithms to understanding the dynamics of self-organizing systems (Parr, Da Costa, & Friston, 2020; Parr, Sajid,
& Friston, 2020; Yedidia, 2011). Moreover, by framing everything in statistical terms ‚Äì in terms of conditional
independence, generative models, and approximate posterior distributions ‚Äì the free-energy principle provides a novel
and powerful vocabulary to talk about such systems, as well as to ask questions such as ‚Äòwhat kind of generative model
does this system embody?‚Äô. Ultimately, this new statistical and inferential perspective upon dynamics may lead to
signiÔ¨Åcant advances in our understanding of complex systems, both biotic and abiotic.
Historically, this perspective has close relationships with early cybernetic views of control and regulation (Conant &
Ross Ashby, 1970; Kalman, 1960; Wiener, 2019). Philosophically, the FEP can be seen as an extension to Ashby‚Äôs
notion that every good regulator of a system must also become a model of the system (Conant & Ross Ashby, 1970).
The FEP nuances this notion slightly by instead stating that every system that regulates itself against the external
environment, must in some sense embody a generative model of the environment, and also that the dynamics of the
internal states of the system necessarily can be interpreted as performing approximate variational inference upon a
distribution over external variables of their local environment - in other words, such systems appear to form ‚Äòbeliefs‚Äô
about the external sources of incoming Ô¨Çuctuations.
The free-energy principle originated in theoretical neuroscience, as an attempt to understand the mathematical properties
that a self-organising living, biotic system, must possess in order to sustain itself against thermodynamic equilibrium. It
was Ô¨Årst and especially applied to understanding the function of the brain (K. Friston, 2012; K. Friston et al., 2006;
K. J. Friston, Daunizeau, Kilner, & Kiebel, 2010), and has been developed into two main process theories ‚Äì predictive
coding 1 (K. Friston, 2003, 2005, 2008; Rao & Ballard, 1999) and active inference (Da Costa et al., 2020; K. Friston,
FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017a; K. Friston, Rigoli, et al., 2015; K. Friston, Samothrakis, &
Montague, 2012; K. J. Friston, Daunizeau, & Kiebel, 2009; K. J. Friston, Rosch, Parr, Price, & Bowman, 2018)
which have been investigated in a wide variety of paradigms, where it has been used to investigate a wide variety of
phenomena from (K. Friston, Levin, Sengupta, & Pezzulo, 2015; K. Friston, Rigoli, et al., 2015; K. Friston et al., 2014),
information foraging and saccades (Parr, 2019; Parr & Friston, 2017, 2018) exploratory behaviour (K. Friston, Da Costa,
Hafner, Hesp, & Parr, 2020; K. Friston, Rigoli, et al., 2015; K. J. Friston et al., 2017; Schwartenbeck, FitzGerald,
Dolan, & Friston, 2013), concept learning (Schwartenbeck et al., 2019), and a variety of neuropsychiatric disorders
(Benrimoh, Parr, Adams, & Friston, 2019; Cullen, Davey, Friston, & Moran, 2018; Lawson, Rees, & Friston, 2014).
These process theories translate the abstract formulation of the FEP into concrete and practical algorithms by specifying
certain generative models, variational distributions, and inference procedures, and have been shown to be useful both in
providing biologically plausible theories of learning and inference in the brain, and also in developing highly effective
1Predictive coding as a theory has a long history, originating as a method for time-series compression (Spratling, 2017) then
being applied to processing in the brain Ô¨Årst by Srinivasan, Laughlin, and Dubs (1982) and Mumford (1992) and was related formally
to variational inference and the free energy principle by (K. Friston, 2003). For a recent review of predictive coding see (Millidge,
Seth, & Buckley, 2021).
2
A PREPRINT - 5TH OCTOBER , 2021
inference algorithms which have advanced the state of the art in machine learning (Millidge, 2019; Millidge, Tschantz,
Seth, & Buckley, 2020; Parr, Markovic, Kiebel, & Friston, 2019; Tschantz, Millidge, Seth, & Buckley, 2020).
The FEP is (in)famously mathematically deep and broad, incorporating concepts and techniques from a wide range of
disciplines such as advanced statistical methods for Bayesian inference, stochastic thermodynamics, classical physics,
and differential geometry. The level of mathematical sophistication required, as well as sometimes dense expositions of
the theory in key works, has made full comprehension of the core arguments and central results of the theory difÔ¨Åcult to
achieve, and has often resulted in confusion in the literature. In this tutorial, we aim to provide a self-contained and
intuitive walk-through of the key mathematical results underpinning the FEP, relying on a fairly minimal amount of
prerequisite knowledge ‚Äì speciÔ¨Åcally, linear algebra, probability theory and statistics, and basic concepts of differential
equations. We also aim to derive all results in a fully explicit way, with substantial commentary to aid intuitive
understanding and to make clear all assumptions and major logical moves in the argument. Finally, we provide a fairly
detailed discussion on the nature and meaning of the core results of the FEP as well as the potential limitations of the
theory.
Additionally, we also offer a substantive appendix which includes in-depth discussions of all the assumptions and
potential limitations highlighted in the main text, as well as current theoretical debates within the free energy community
over the mathematical formalism and general applicability of the FEP. Thus, the main text presents the FEP in its best
possible light, while the potential controversial assumptions and other mathematical difÔ¨Åculties, which are substantial
and still under signiÔ¨Åcant discussion within the community, are presented in the footnotes to the main text and then
discussed in detail in the appendix. As such, critical readers should make sure to consult the appendix in detail for a
balanced understanding of both the claims, and the potential limitations of the free energy principle
1.1 Related Work
While there have been many excellent tutorials on the process theories arising from the free-energy principle, such as
predictive coding (Bogacz, 2017; Buckley, Kim, McGregor, & Seth, 2017; Millidge et al., 2021), and active inference
(Da Costa et al., 2020; K. Friston et al., 2017a) , there is a general lack in the literature of mathematically detailed
tutorials tackling the central claims of the FEP, especially in its most recent formulation in the K. Friston (2019b). This
monograph currently represents the theoretical state of the art of the Free-Energy-Principle 2, and we aim to recapitulate
its main results in a more direct and accessible manner in this paper.
The FEP also involves deep knowledge from a variety of disciplines. For thorough tutorials on variational Bayesian
inference, we recommend (Beal, 2003; Blei, Kucukelbir, & McAuliffe, 2017; Fox & Roberts, 2012). For an excellent
walk through of differential geometry, information-geometry, and the role of the Fisher Information matrix, we
recommend Caticha (2015). For a detailed treatment of the principles of stochastic thermodynamics which underlie
much of the FEP, we recommend Esposito and Van den Broeck (2010); Seifert (2008, 2012); Van den Broeck and
Esposito (2010). Finally, since the publication of the particular physics monograph (K. Friston, 2019b), the theory of the
FEP has been developed in numerous further publications. A concise overview of the theory is given in Parr, Da Costa,
and Friston (2020), and later developments have been surveyed in Da Costa et al. (2021). Further development of the
operationalization of Markov Blankets is given in Parr, Sajid, and Friston (2020). Discussion upon different inference
algorithms and divergences has been given in Blei et al. (2017); Yedidia (2011). Some further theoretical reÔ¨Ånements to
various aspects of the theory can be found in (K. J. Friston, Wiese, & Hobson, 2020; Ramstead et al., 2020).
For a detailed mathematical critique to many of the arguments and claims of the FEP, especially regarding the technical
conditions required for the free energy lemma, please see Biehl, Pollock, and Kanai (2020), and also the response
K. Friston, Da Costa, and Parr (2020), as well as Aguilera, Millidge, Tschantz, and Buckley (2021) for additional
2Many of the key results are also reprised more concisely in Da Costa, Friston, Heins, and Pavliotis (2021); Parr, Da Costa, and
Friston (2020)
3
A PREPRINT - 5TH OCTOBER , 2021
Langevin Dynamics Non-Equilibrium Steady State
p*(x)
¬∑x = f(x, t) + œâ
Ao Decomposition
¬∑x = (Œì ‚àí Q) ‚àáxln p*(x)
fŒº(œÄ) = ùîºp( ÀúœÄ|œÄ)[ fŒº(x)] = (Œì ‚àí Q) ‚àáŒºln p*(œÄ)
Marginal Flow Lemma Markov Blanket Condition
p(x) = p(Œ∑| b)p(Œº| b)p(b)
fŒº(œÄ) = (Œì ‚àí Q) ‚àáŒº‚Ñ±particular(Œº, s, a)
Particular Free Energy Parametrisation by argmax
Œº(b) = argmax p(Œº| b)
Œ∑(b) = argmax p(Œ∑| b)
Œ∑(b) = œÉ(Œº(b))
Identical true and variational posterior
q(Œ∑; Œ∑) = p(Œ∑| b)
¬∑Œº = (Œì ‚àí Q) ‚àáŒº‚Ñ±(Œº, s, a)
Free Energy Lemma
(Approximate Bayesian Inference) Laplace Approximation
q(Œ∑; Œº) = ùí©(œÉ(Œº), Œ£(Œ∑))
Figure 1: The logical Ô¨Çow of the argument of the FEP from the initial formulation to the crucial approximate Bayesian
inference lemma. We begin with a setting of a random langevin stochastic dynamical system, which possess a non-
equilibrium-steady state. By applying the Ao decomposition, we can understand its dynamics in terms of a gradient
descent upon the surprisal. Upon the addition of a Markov Blanket partition, we can express subsets in terms of their
own marginal Ô¨Çows via the marginal Ô¨Çow lemma. If we then identify the internal states as parametrizing a variational
distribution over the external states, we can interpret the marginal Ô¨Çow on the surprisal as a Ô¨Çow on the variational free
energy, under the Laplace approximation, and thus interpret the internal states of the system as a whole as performing a
simple kind of variational inference upon the external states.
critiques of the markov blanket condition, assumptions around the solenoidal Ô¨Çows, and the some of the technical steps
and assumptions of the free energy lemma.
1.2 History and Logical Structure
Historically, the free energy principle has evolved over the course of about Ô¨Åfteen years. Its intellectual development
can best be seen in two phases. In the Ô¨Årst phase, an intuitive and heuristic treatment emerged with K. Friston et al.
(2006) which stated that the imperative to minimize variational free energy emerged from a necessary imperative of
minimizing the system‚Äôs entropy, or log model evidence, which is upper bounded by variational free energy. This
imperative emerges due to the self-sustaining nature of biological systems such as brains, in that they maintain a set
distribution against the inexorably increasing entropic nature of thermodynamic reality (K. Friston, 2009). In order to
do so, systems must constantly seek to reduce and maintain their entropy across their state space. Since the VFE is
computationally tractable while the entropy itself is not, it was postulated that neural systems maintain themselves by
implicitly minimizing this proxy rather than the actual entropy itself (K. Friston, 2010).
Later, in the second phase (K. Friston, 2013), this heuristic argument and intuition was related more formally to
concepts in stochastic thermodynamics beginning with (K. Friston & Ao, 2012a, 2012b). SpeciÔ¨Åcally, the framework
developed mathematically into a description of stochastic dynamics (as stochastic differential equations) separated into
‚Äòexternal, internal, and blanket‚Äô states by a statistical construct called aMarkov Blanket. This blanket makes precise the
statistical independence conditions required to make sense of talking about a ‚Äòsystem‚Äô as distinct from its ‚Äòenvironment‚Äô.
Moreover, by separating the ‚Äòblanket‚Äô into ‚Äòsensory‚Äô and ‚Äòactive‚Äô states, one can obtain a statistical description of the
core elements of a perception-action-loop, a central concept in cybernetics, control theory, and reinforcement learning.
4
A PREPRINT - 5TH OCTOBER , 2021
Secondly, the theory developed a precise notion of what it means to maintain a stable ‚Äòphenotype‚Äô which is interpreted
mathematically as a non-equilibrium steady-state density over the state-space. This steady state is non-equilibrium due
to the presence of ‚Äòsolenoidal Ô¨Çows‚Äô which are Ô¨Çows orthogonal to the gradient of the NESS density. Mathematically,
such Ô¨Çows do not increase or decrease the entropy of the steady-state-density, but do, however, in contrast to an
equilibrium steady state (ESS), provide a clear arrow of time. Given this, it is claimed, that under certain conditions,
one can draw a relationship between the Ô¨Çow dynamics and the process of variational Bayesian inference through the
minimization of the variational free energy ‚Äì speciÔ¨Åcally, that the dynamics that result from this speciÔ¨Åc kind of Ô¨Çow
under a Markov blanket at the NESS density can be seen as approximating a gradient descent upon the VFE, thus
licensing the interpretation of the system as performing a basic kind of Bayesian inference or, ‚Äòself-evidencing‚Äô (Clark,
2015; Hohwy, Roepstorff, & Friston, 2008)
While the intuitions and basic logical structure of the theory has remained roughly constant since K. Friston (2013),
the mathematical formulation and some of the arguments have been reÔ¨Åned in the most recent K. Friston (2019b)
monograph and related papers (Da Costa et al., 2021; K. Friston, Da Costa, & Parr, 2020; Parr, Da Costa, & Friston,
2020). These papers have drawn close connections between the formulation of free energy principle, and many aspects
of physics including the principle of least action in classical mechanics, and notions of information length and the arrow
of time in stochastic thermodynamics. Additionally, recent work contains a novel information-geometric gloss on the
nature of the Bayesian inference occurring in the system. SpeciÔ¨Åcally, it argues that the internal states of the system
can be seen as points on an statistical manifold that parametrize distributions over the external states, and that thus
the internal states can be described using a ‚Äòdual-aspect information geometry.‚Äô According to this perspective, internal
states evolve in both the ‚Äòintrinsic‚Äô state space of the system‚Äôs physical dynamics, while simultaneously parameterising
a manifold of statistical beliefs about external states - the so-called ‚Äòextrinsic‚Äô information geometry.
While the mathematical depths of the FEP often appears formidably complex to the uninitiated, the actual logical
structure of the theory is relatively straightforward. First, we want to deÔ¨Åne what it means to be ‚Äòa system‚Äô that keeps
itself apart from the outside ‚Äòenvironment‚Äô over a period of time. The FEP answers this question in its own way. We
deÔ¨Åne a ‚Äòsystem‚Äô (according to the FEP) as a dynamical system which has a non-equilibrium steady state (NESS) which
it maintains over an appreciable length of time, and that the dynamics are structured in such a way that they obey the
‚ÄòMarkov Blanket Condition‚Äô. SpeciÔ¨Åcally, having a NESS can be intuitively thought of as deÔ¨Åning dynamics which
produce something like a phenotype ‚Äì i.e. a recognizable pattern of states which persists relatively unchanged for some
period of time. For instance, we can think of the biological systems in such a manner. Biological organisms maintain
relatively steady states, against constant entropic dissipation, for relatively long (by thermodynamic standards) periods
of time. Of course, from a purely thermodynamical perspective, in resisting entropy themselves, biological organisms
are not countering the law of thermodynamics. To achieve their steady state requires a constant inÔ¨Çux of energy ‚Äì
hence it is a non-equilibrium steady state (NESS). From this perspective, we can understand biological organisation to
be the process of creating ‚Äòdissipative structures‚Äô (Kondepudi & Prigogine, 2014; Prigogine & Lefever, 1973) which
only manage to maintain themselves at steady state and reduce their own entropy at the expense of consuming energy
and increasing the entropy production rate of their environment (Prigogine, 2017). Illustrative physical examples of
similar NESS states are Benard convection cells, and the Belousov-Zhabotinsky reaction (Zwanzig, 2001). In practical
terms, we can consider the NESS density to be the ‚Äòphenotype‚Äô of the system. From the perspective of the FEP, we are
not usually concerned with whether a set of dynamics possesses a NESS density, or how convergence to the NESS
density works. Instead we take it as an axiom that we possess a system with a NESS density which the system can
converge to, and are instead concerned with the dynamical behaviour of the system at the NESS density. While this is
clearly a special case, nevertheless dynamical systems at NESS already exhibit rich behaviours to effectively maintain
themselves there. It is these properties, which any system which maintains itself at NESS must possess, that are the
fundamental object of study for the FEP.
Secondly, now that we have a set of dynamics which has a NESS density, and thus exhibits some stability through time,
we also require a statistical way to separate the ‚Äòsystem‚Äô from the ‚Äòenvironment‚Äô. The FEP handles this by stipulating
5
A PREPRINT - 5TH OCTOBER , 2021
that any system it considers must fulÔ¨Ål a set of criteria which we call the Markov Blanket conditions. These conditions,
deriving from the idea of Markov blankets in Bayesian networks (Pearl, 2011, 2014), set forth a set of conditional
independence requirements that allow a system to be statistically separated from its environment 3. SpeciÔ¨Åcally, we
require that the dynamics of the system can be meaningfully partitioned into three sets of states ‚Äì ‚Äòinternal‚Äô states
which belong to the system of study, ‚Äòexternal‚Äô states which correspond to the environment, and ‚Äòblanket states‚Äô which
correspond to the boundary between the system and its environment. SpeciÔ¨Åcally, we require the internal states to be
conditionally independent of the external states given the blanket states, and vice versa. Thus all ‚ÄòinÔ¨Çuence‚Äô of the
environment must travel through the blanket, and cannot directly interact with the internal states of the system which
are ‚Äòshielded‚Äô behind the blanket4
Now that we have a system with a NESS density which obeys the Markov Blanket conditions, so that we can partition it
into external, internal, and blanket states, we then wish to understand the dynamics of the system at the NESS density,
so we can understand the necessary behaviours of the system to allow the NESS to be maintained. Here we use the
Helmholtz (Ao) decomposition (Yuan & Ao, 2012; Yuan, Ma, Yuan, & Ao, 2011; Yuan, Tang, & Ao, 2017) to represent
the dynamics as a gradient Ô¨Çow on the log of the NESS density (which is called the surprisal) with both dissipative
(in the direction of the gradient) and solenoidal (orthogonal to the gradient) components. Now that we can express
the dynamics of the system in terms of gradients of the log NESS density, we then invoke theMarginal Flow Lemma
to write out the dynamics of each component of the partitioned dynamics (i.e. external, internal, and blanket states)
solely in terms of a gradient Ô¨Çow on its own marginal NESS density. This means that we can express, for instance, the
dynamics of the internal states solely in terms of gradient Ô¨Çows on the marginal NESS density over the internal and
blanket states.
Given this marginal partition, we can analyze and understand each of the Ô¨Çows in each partition of the system separately.
SpeciÔ¨Åcally, to understand the Ashbyan notion that ‚Äòevery good regulator of a system is a model of the system‚Äô, we wish
to understand the relationship between the Ô¨Çows of the internal and external states, which are statistically separated
from the blanket. Despite this separation, it is possible to deÔ¨Åne a mapping between the most likely internal state,
given a speciÔ¨Åc conÔ¨Åguration of the blanket states, and the distribution over the most likely external state of the system.
We can use this mapping to interpret internal states as parametrizing a variational or approximate distribution over
the external states. This interpretation sets up the ‚Äòdual-aspect‚Äô information geometry of the internal states, since the
internal dynamics simultaneously represent changes in the parameters of the distribution over internal states (which
can potentially be non-parametric), and changes to the parameters of the variational distribution over external states.
This latter interpretation means that the internal states can be directly mapped to parameters of a distribution over
external states, and thus that these parameters form a statistical manifold equipped with a Fisher information metric
(if the variational distribution is in the exponential family), and in general becomes amenable to the techniques of
information geometry (Amari, 1995; Ollivier, Arnold, Auger, & Hansen, 2017) Finally, given that we can interpret the
internal states as parametrising a distribution over external states, we can reconsider the gradient Ô¨Çow upon the log
NESS density with a new light. SpeciÔ¨Åcally, once the identiÔ¨Åcation of the dynamics of the internal mode to variational
inference is recognised, we can understand the NESS density to represent the implicit generative model of the system
(since it is a joint density over all variables of the dynamical system modelled), and the gradient Ô¨Çow dynamics as
a descent upon the free-energy, with a perfect Bayes-optimal posterior. Alternatively, if we invoke an approximate
posterior distribution over external states, we can represent the gradient Ô¨Çow as an approximate minimization of the
variational free energy (VFE), and thus the internal states of the system can be interpreted as performing approximate
3Whenever we say Markov Blanket, following standard use in the literature, we mean the minimal Markov blanket ‚Äì i.e. the
Markov Blanket which requires the fewest number of blanket states to achieve the required conditional independencies.
4Interestingly, mathematically, the MB condition and all of the FEP is completely symmetrical between ‚Äòinternal‚Äô and ‚Äòexternal‚Äô
states. Thus from the perspective of the system, the ‚Äòexternal states‚Äô are its environment, but from the perspective of the environment,
the ‚Äòexternal states‚Äô are the system. This means that the environment models and performs inference about the system just as the
system models and performs inference on the environment. We can thus think of the environment-system interaction as a duality of
inference, where each tries to model and infer the other in a loop.
6
A PREPRINT - 5TH OCTOBER , 2021
variational Bayes. This is the key result of the FEP. It states, simply, that the internal dynamics of any system that
maintains itself at a non-equilibrium steady state, and possesses a Markov Blanket, can be interpreted as modelling,
and performing approximate variational inference upon the external states beyond its own Markov Blanket. It thus
generalizes and makes precise Ashby‚Äôs notion that every good regulator must in some sense be a model of the system
(Conant & Ross Ashby, 1970). Here we see that in order to maintain a non-equilibrium steady state, to counteract the
dissipative forces inherent in thermodynamics, it is necessary to perform some kind of inference about the environment
beyond the system itself.
2 Formulation
Here we begin the precise mathematical description of the FEP. We aim to provide a consistent notation, and more
detailed derivations of key results than are often presented. The presentation in this tutorial mostly follows the order of
presentation in K. Friston (2019b), although many circumstantial topics are omitted to focus on the main Ô¨Çow of the
argument. We begin with the basic mathematical setting and formulation of the theory. We assume that the dynamics
we wish to describe can be expressed in terms of a Langevin stochastic differential equation (Jaswinski, 1970),
dx
dt = f(x) + œâ (1)
where x= [x0 ...x N] is a vector of states of some dimensionality, and f(x) is an arbitrary nonlinear but differentiable
function of the state vector. SpeciÔ¨Åcally, here we assume already that this process is not history dependent. The dynamics
only depend on the instantaneous values of the states. In practice, history dependent systems can be represented in this
fashion, albeit somewhat unintuitively by adding sufÔ¨Åcient statistics of the history to the state itself. œâis assumed to be
white (zero autocorrelation) Gaussian noise with zero mean such that œâ= N(x; 0,2Œì) where Œì is the variance of the
noise. Zero autocorrelation means that the covariance between the noise at any two time instants, even inÔ¨Åntesimally
close together, is 0 ‚Äì E[œâtœâT
t+Œ¥] = 0. We assume that this noise is added additively to the dynamics. A full list of the
assumptions required in the formulation are outlined and discussed in the Appendix (section 10).
This stochastic differential equation can also be represented not in terms of dynamically changing states, but in terms of
a dynamically changing probability distribution over states. This transformation is achieved through the Fokker-Planck
equation, by which we can derive that the change in the distribution over states can be written as,
dp(x,t)
dt = ‚àí‚àáxf(x)p(x,t) + Œì‚àá2
xp(x,t) (2)
Where p(x,t) is the instantaneous distribution over the states at a given time t. p(x,t) begins with the distribution
p(x0,0) = N(f(x0,0),Œì) due to the initial noise term œâ and the starting state x0. Here ‚àáxf(x,t) is the gradient
function and simply denotes the vector of partial derivatives of the function f with respect to each element of the vector
x. ‚àáxf(x) = [‚àÇf(x,t)
‚àÇx0
,‚àÇf(x,t)
‚àÇxN
,..., ‚àÇf(x,t)
‚àÇxN
]. ‚àá2
xf(x) represents the vector of second partial derivatives of the function.
Next, we presuppose that the dynamics expressed in Equation 1 tend towards a non-equilibrium steady state
limt‚Üí‚àûp(x,t) = p‚àó(x) where we represent the steady state distribution as p‚àó(x). Note that this distribution no
longer depends on time, since it is by deÔ¨Ånition at a steady state. We use p‚àóto make clear that this distribution is at
steady state. By deÔ¨Ånition a steady state distribution does not change with time, so that dp‚àó(x)
dt = 0.
The distinction between an equilibrium steady state and a non-equilibrium steady state (NESS) distribution is subtle and
important. An equilibrium steady state, mathematically, is one where the property of detailed balance holds. This means
that any transition between states at equilibrium is just as likely to go in the ‚Äòforwards‚Äô direction as it is to go in the
‚Äòbackwards‚Äô direction. In effect, the dynamics are completely symmetric to time, and thus there is no notion of an arrow
of time in such systems. Conversely, a non-equilibrium steady state is one where detailed balance does not hold, so there
is a directionality to the dynamics, and thus an arrow of time, even though the actual distribution over states remains
constant. From a thermodynamic perspective, the equilibrium-steady-state is the inexorable endpoint of the second law
7
A PREPRINT - 5TH OCTOBER , 2021
of thermodynamics, since it is the maximum entropy state. Conversely, a NESS is not a maximum entropy solution,
since the directionality of the dynamics means that there is a degree of predictability in the system which could in theory
be exploited to produce work. Non-equilibrium steady states can arise in thermodynamic systems but require an external
source of driving energy as a constant input to the system, which is then dissipated to the external surroundings and
gives the NESS a positive entropy production rate. To take an intuitive example, we can think about the thermodynamic
equilibrium of a cup of coffee with cream added. The equilibrium steady state (ESS) is when the coffee and cream have
completely diffused into one another, so that the cream maintains a constant proportion throughout the entire coffee
cup. This will be the inevitable result (by the second law of thermodynamics) of adding an initially low entropy highly
concentrated cream scoop into the coffee. On the other hand, we can think of the non-equilibrium steady state (NESS)
as to be when the cream and coffee are equally diffused throughout, but somebody is constantly stirring the coffee in
a speciÔ¨Åc direction. Here, we are at steady state because the concentrations of cream and coffee don‚Äôt change over
time, but nevertheless there is a directionality to the dynamics in the direction of the stirring. This directionality is only
maintained due to a constant input of energy 5 to the system (the stirring) 6. The Ô¨Çow caused by the stirring is referred
to as the ‚Äòsolenoidal Ô¨Çow‚Äô and mathematically is necessarily orthogonal to the gradient of the steady state distribution.
This is necessary so that the solenoidal Ô¨Çow does not ascend or descent the gradient of the density, and thus change the
steady state distribution which, as a steady state, by deÔ¨Ånition cannot change. Biological self organizing systems are
often considered to be ‚Äòdissipative structures‚Äô, or non-equilibrium steady states from the perspective of thermodynamics,
since they maintain a relatively steady state over time which requires a constant inÔ¨Çux of energy to maintain.
Given that we presuppose a system with a NESS density, we wish to understand the dynamicsat the NESS density ‚Äì
speciÔ¨Åcally, how does the solenoidal Ô¨Çow help prevent the system from relaxing into an equilibrium-steady-state (ESS)?
To understand this, we utilize the Helmholtz decomposition (K. Friston & Ao, 2012b; Yuan & Ao, 2012; Yuan et al.,
2017) to rewrite the dynamics at the NESS into a form of a dissipative and solenoidal descent upon the gradient of
the log NESS density. The Helmhotlz (or Ao) decomposition is a matheamtical tool which lets us express the ‚ÄòÔ¨Çow‚Äô
‚Äì i.e. dynamics function f ‚Äì of a dynamical system into separable ‚Äòdissipative‚Äô (noise) and ‚Äòsolenoidal‚Äô components
which perform a gradient descent on a scalar potential function which we identify with the NESS. Mathematically, the
Helmholtz decomposition can be written as,
f(x) = (Œì(x) ‚àíQ(x))‚àáxln p‚àó(x) (3)
Where Œì(x) is a dissipative component of the Ô¨Çow which tries to descend the log density. It is the amplitude of the
random Ô¨Çuctuations in the original SDE formulation (R. Jordan, Kinderlehrer, & Otto, 1998; Yuan, Ma, Yuan, & Ao,
2010; Yuan et al., 2011), which in effect are constantly trying to ‚Äòsmooth out‚Äô the NESS density and increase its entropy.
Conversely, the Q(x) represents the solenoidal portion of the Ô¨Çow which, although orthogonal to the gradient of the
log potential, successfully counteracts the dissipative effects of the Œì(x) terms to maintain the dynamics at a steady
state. While Œì(x) and Q(x) can in theory be state-dependent, from here on out we typically assume that they are not ‚Äì
Œì(x) = Œì; Q(x) = Q7 We verify that the Helmholtz decomposition is satisÔ¨Åed at steady state in Appendix 11.1.
5It‚Äôs important to note that here we are using physical intuition and concepts like ‚Äòenergy‚Äô in a purely metaphorical sense.
All results here apply to arbitrary SDEs which do not necessarily follow the same constraints as physical systems ‚Äì i.e. respect
conservation of energy
6Interestingly, physical experience with this analogy would suggest that the solenoidal dynamics leading to NESS would lead to
faster convergence to the NESS density compared to the strictly dissipative dynamics leading to ESS ‚Äì effectively, stirring helps the
cream diffuse faster. This insight has been applied to the design of highly efÔ¨Åcient Markov-Chain-Monte-Carlo samplers in machine
learning (M. J. Betancourt, 2013; Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953; Neal et al., 2011)
7SpeciÔ¨Åcally, the introduction of state-dependent Œì and Q matrices leads to there being an additional ‚Äòhousekeeping‚Äô term in the
Helmholtz free energy as discussed in (K. Friston, Heins, Ueltzh√∂ffer, Da Costa, & Parr, 2021).
8
A PREPRINT - 5TH OCTOBER , 2021
Figure 2: The intuition behind the Markov Blanket partition. The brain (or bacillus) consists of internal states ¬µwhich
are separated from the outside world (external states Œ∑by the blanket states b, which can themselves be partitioned
into sensory states s, representing the sensory epithelia, and which are directly inÔ¨Çuenced by external states, and active
states arepresenting the organisms effectors and which are directly inÔ¨Çuenced by internal states, and act on external
states. We see that perception concerns the minimization of free energy of the internal states, while action concerns the
minimization of the expected free energy of the active states. Figure originally appeared in K. Friston (2019b)
3 Markov Blankets
From these preliminaries, we have a set of dynamics of states x, which possess a NESS density, and, by using the Ao
decomposition, we can express the dynamics at the NESS density in terms of dissipative Œì and a solenoidal QÔ¨Çows
on the gradient of the log density. Now, we begin to explore the statistical structure of these dynamics in terms of a
Markov Blanket. SpeciÔ¨Åcally, we next require that we can partition the states xof the dynamics into three separate units.
External states e, internal states i, and blanket states bsuch that x= [e,i,b ]. Intuitively, the external states represent
the ‚Äòenvironment‚Äô; the internal states represent the ‚Äòsystem‚Äô we wish to describe, and the blanket states represent the
statistical barrier between the system and its environment. For instance, we might wish to describe the dynamical
evolution of a simple biological system such as a bacterium in such a manner. Here, the internal states would describe
the internal cellular environment of the bacterium ‚Äì the cytoplasm, the nucleus, the ribosomes etc. The external states
would be the environment outside the bacterium, while the blanket states would represent the cell membrane, sensory
epithelia, and potentially active instruments such as the Ô¨Çagella which sense and interact with the external environment.
The key intuition behind the FEP is that although all inÔ¨Çuence between external and internal states is mediated by the
blanket states, simply maintaining the non-equilibrium steady state against environmental perturbations requires that
the internal states in some sense model and perform (variational) Bayesian inference on the external states. The Markov
Blanket condition is straightforward. It simply states that the internal and external states must be independent given the
blanket states,
p‚àó(x) = p‚àó(e,i,b ) = p‚àó(e|b)p‚àó(i|b)p‚àó(b) (4)
While in probabilistic terms this factorisation is straightforward, it has more complex consequences for the dynamical
Ô¨Çow of the system. Firstly, we additionally decompose the blanket states into sensory sand active astates such that
9
A PREPRINT - 5TH OCTOBER , 2021
b= [s,a] and thus, ultimately x= [e,i,s,a ]. Sensory states are blanket states that are causal children of the external
states ‚Äì i.e. the states that the environment acts on directly. Active states are those blanket states that are not causal
children of the external states. Essentially, external states inÔ¨Çuence sensory states, which inÔ¨Çuence internal states, which
inÔ¨Çuence active states, which inÔ¨Çuence external states. The circular causality implicit in this loop is what allows the
Markov Blanket condition to represent the perception-action loop 8.
The next step is to understand what the conditional independence requirements put forth in Equation 4 imply for the
dynamics of the Ô¨Çow. SpeciÔ¨Åcally, we obtain the marginal Ô¨Çow lemma (see K. Friston (2019b) for a full derivation),
which states that the Ô¨Çow of a subset of variables averaged under the rest of the variables, is equal to a solenoidal
gradient Ô¨Çow only on the gradient of the potential of marginal NESS density of that subset of variables plus a solenoidal
coupling term. For instance, we can express the Ô¨Çow of just the external statese, averaged under the blanket and internal
states, as simply a solenoidal gradient descent Ô¨Çow on the marginal NESS densities plus a term dependent upon the
solenoidal coupling of the external and blanket states
fe(x) = Ep(e|b)
[
fe(x)] = (Œìe,e ‚àíQe,e)‚àáeln p‚àó(e) ‚àíQe,b‚àábp(e,b)
If we then assume a lack of solenoidal coupling so that Qe,b = 0 then we can neglect this solenoidal coupling
term and treat the marginal Ô¨Çow as completely independent gradient descent. This lets us investigate in detail the
information-theoretic interactions of one set of states with another, and gain intuition and understanding of the core
information-theoretic properties of the perception-action loop. Similarly, using the marginal Ô¨Çow lemma we can express
the Ô¨Çow of autonomous (active and internal) Œ±= (a,i) as,
fŒ±(x) = (ŒìŒ±Œ± ‚àíQŒ±Œ±)‚àáŒ±ln p‚àó(i,s,a ) (5)
So we see that autonomous states follow a gradient descent on the marginal NESS density of the internal, sensory, and
active states, and attempt to suppress their surprisal or, on average, their entropy. We can use a series of mathematical
‚ÄòinÔ¨Çationary devices‚Äô (adding and subtracting the same quantity,so the total is 0, in order to introduce it to the equation)
to express this surprisal in terms of its interaction with the external states beyond the blanket.
‚àíln p‚àó(i,s,a ) = Ep‚àó(e|i,s,a)
[
‚àíln p‚àó(i,s,a )
]
= Ep‚àó(e|i,s,a)
[
ln p‚àó(e|i,s,a ) ‚àíln p‚àó(e,i,s,a )
]
= Ep‚àó(e|i,s,a)
[
ln p‚àó(e|i,s,a ) ‚àíln p‚àó(i,s,a |e) ‚àíln p‚àó(e)
]
= Ep‚àó(e|i,s,a)
[
‚àíln p‚àó(i,s,a |e)
]
Ó¥ô Ó¥òÓ¥ó Ó¥ö
Inaccuracy
+ DKL
[
p‚àó(e|i,s,a )||p(e)
]
Ó¥ô Ó¥òÓ¥ó Ó¥ö
Complexity
(6)
Thus we can see that the Ô¨Çow of autonomous states acts to minimize the inaccuracy (maximize accuracy) and minimize
the complexity of the external states with respect to the ‚Äòparticular states‚Äô of the system in question. The particular
states consist of the sensory, active, and internal states ‚Äì i.e. everything except the external states. Parsed into more
intuitive terms, we can thus see that the Ô¨Çow of ‚Äòsystem‚Äô states (i,s,a ) aim to maximize the ‚Äòlikelihood‚Äô of the internal
states given the external states ‚Äì i.e. perform maximum likelihood inference on themselves (c.f. ‚Äòself evidencing‚Äô
(Hohwy, 2016)) ‚Äì while simultaneously minimizing the complexity ‚Äì or the divergence between the external states
given the internal states, and the ‚Äòprior‚Äô distribution over the external states. In short, by re-expressing the Ô¨Çow
in information-theoretic terms, we can obtain a decomposition of the entropy term into intuitive and interpretable
sub-components which can help us reason about the kinds of behaviours these systems must exhibit.
Further discussion of the nature and necessity of the Markov Blanket conditions, and the various additional constraints
on solenoidal coupling are discussed in detail in the appendix (section 10.3.2)
8There has been some recent controversy in the literature about the meaning and implications of the statistical Markov blanket
condition in terms of what it does or does not imply about the dynamical ‚Äòreal‚Äô connectivity. For more discussion on this please see
the Appendix section 10.3.1 or Aguilera et al. (2021)
10
A PREPRINT - 5TH OCTOBER , 2021
4 Variational Inference
Variational inference is a method for approximating intractable integrals in Bayesian statistics (Feynman, 1998; Fox &
Roberts, 2012; Ghahramani & Beal, 2001; M. I. Jordan, Ghahramani, Jaakkola, & Saul, 1998, 1999; Neal & Hinton,
1998). Typically, a direct application of Bayes-rule to compute posteriors in complicated systems fails due to the
intractability of the log model evidence, which appears in the denominator of Bayes‚Äô rule. While there exist numerical
or sampling-based methods to precisely compute this integral, they typically scale poorly with the dimension of the
problem ‚Äì a phenomenon which is known as the curse of dimensionality (Goodfellow, Bengio, & Courville, 2016).
Variational techniques originated from methods in statistical physics in the 1970s and 1980s (Feynman, 1998), and
were then taken up in mainstream statistics and machine learning in the 1990s (Beal, 2003; Ghahramani & Beal, 2001;
M. I. Jordan et al., 1998) where they have become an inÔ¨Çuential, often dominant approach for approximating posteriors
and Ô¨Åtting complex high-dimensional Bayesian models to data (Beal, 2003; Blei et al., 2017; Dayan, Hinton, Neal, &
Zemel, 1995; Feynman, 1998; Ghahramani, Beal, et al., 2000; M. I. Jordan et al., 1999; Kingma & Welling, 2013).
The core idea of variational inference is to approximate an intractable inference problem with a tractable optimization
problem. Thus, instead of directly computing a posterior distribution p(H|D) where H is some set of hypotheses and
Dis the data, we instead postulate an approximate or variational distribution q(H|D; Œ∏) which is often, although not
always, parametrized with some Ô¨Åxed number of parameters Œ∏. We then seek to optimize the parameters Œ∏to minimize
the divergence between the approximate and true posterior,
Œ∏‚àó= argmin
Œ∏
DKL[q(H|D; Œ∏)||p(H|D)] (7)
Unfortunately, this optimization problem is itself intractable since it contains the intractable posterior as an element.
Instead, we minimize a tractable bound on this quantity called the variational free energy (VFE) F(D,Œ∏),
F(D,Œ∏) = Eq(H|D)[ln q(H|D; Œ∏) ‚àíln p(H,D)]
= DKL[q(H|D; Œ∏)||p(H|D)] ‚àíln p(D)
‚â•DKL[q(H|D; Œ∏)||p(H|D)] (8)
Since the VFE is simply a divergence between the variational distribution and the generative model p(D,H), it is
tractable as we assume we know the generative model that gave rise to the data. By minimizing the VFE, therefore, we
reduce the divergence between the true and approximate posteriors, and thus improve our estimate of the posterior.
Secondly, the variational free energy is simultaneously a bound upon the log model evidenceln p(D), which is precisely
the quantity which was intractable to compute due to the implicit integration over all possible hypotheses (or parameters)
p(D) =
‚à´
dHp(D|H)p(H). However, if we perfectly succeed in matching the variational and true posteriors, then the
free energy simply converges to precisely the log model evidence. It is then possible to use this estimate for model
comparison and selection (K. Friston, Parr, & Zeidman, 2018; Geweke, 2007) since it provides a metric to score the Ô¨Åt
of a model on the data.
ln p(D) = Eq(H|D;Œ∏)[ln q(H|D; Œ∏) ‚àíln p(H,D)] ‚àíF(D,Œ∏)
‚â•‚àíF(D,Œ∏) (9)
The second line follows due to the non-negativity of the KL divergence. The VFE is the foundation of the Free-energy
principle as, we shall show, we can interpret self-organizing systems which maintain themselves at a non-equilibrium-
steady state to be implicitly minimizing the VFE, and thus performing variational Bayesian inference.
11
A PREPRINT - 5TH OCTOBER , 2021
We can gain some intuition for the effects of minimizing the VFE by decomposing it into various constituent terms.
Here we showcase two different decompositions which each give light to certain facets of the objective function,
F(D,Œ∏) = Eq(H|D;Œ∏)[q(H|D; Œ∏)||p(H,D)]
= Eq(H|D;Œ∏)[ln p(H,D)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Energy
‚àíH[q(H|D; Œ∏)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Entropy
(10)
= Eq(H|D;Œ∏)[ln p(D|H)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Accuracy
+ DKL[q(H|D; Œ∏)||p(H)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Complexity
(11)
Here we see that we can decompose the variational free energy into two separate decompositions, each consisting of
two terms. The Ô¨Årst decomposition splits the VFE into an ‚Äòenergy‚Äô term, which effectively scores the likelihood of
the generative model under the variational distribution, while the entropy term encourages the variational distribution
to become maximally entropic. Essentially, this decomposition can be interpreted as requiring that the variational
distribution maximize the joint probability of the generative model (energy), while simultaneously remaining as
uncertain as possible (entropy) 9. The second decomposition ‚Äì into an ‚Äòaccuracy‚Äô and a ‚Äòcomplexity‚Äô term ‚Äì speaks
more to the role of the VFE in inference. Here the accuracy term can be interpreted as driving the variational density
to produce a maximum likelihood Ô¨Åt of the data, by maximizing their likelihood under the variational density. The
complexity term can be seen as a regularizer, which tries to keep the variational distribution close to the prior distribution,
and thus restrains variational inference from pure maximum-likelihood Ô¨Åtting.
5 Intrinsic and Extrinsic information geometries
Now, we wish to understand the relationship between the internal states and the external states, which are separated
by the blanket states. Given the existence of a blanket, the next move is to deÔ¨Åne a mapping, denoted œÉbetween the
most likely internal state and the most likely external state, given a speciÔ¨Åc blanket state. While such a mapping is not
guaranteed to exist in general, it does under certain conditions ‚Äì namely if we assume injectivity between the most
likely internal and blanket states (Parr, Da Costa, & Friston, 2020). For linear OU processes, the œÉmapping always
exists and can be analytically derived relatively straightforwardly (Aguilera et al., 2021; Da Costa et al., 2021).
We deÔ¨Åne the most likely internal and external states given a blanket state as 10,
e(b) = argmax
e
p(e|b)
i(b) = argmax
i
p(i|b) (12)
From this, we can deÔ¨Åne œÉto be the mapping that fulÔ¨Åls the following equation,
e(b) = œÉ(i(b)) (13)
Importantly, we can interpret the output of this function ‚Äì the most likely external states given the blanket states ‚Äì
as parametrizing the mean of a distribution over the external states, as a function of the internal states q(e; e(b)) =
q(e; œÉ(i(b))). This allows us to interpret the Ô¨Çow of the mean of internal states as parametrising a changing distribution
over the external states.
Crucially, we can say that if any given set of internal states parametrizes a distribution over external states, then the space
of internal states effectively represents a space of distributions over external states, parametrized by internal states. This
9Interestingly, this energy-entropy decomposition is precisely why this information-theoretic quantity is named the variational
free energy. The thermodynamic free energy, a central quantity in statistical physics, has an identical decomposition into the energy
and the entropy.
10A small assumption introduced here is that the argmax for the distribution of external or internal states given a blanket state is
unique.
12
A PREPRINT - 5TH OCTOBER , 2021
space of distributions may be, and usually is, curved and non-euclidean in nature. The Ô¨Åeld of information geometry
provides many mathematical tools to allow us to describe and mathematically characterise such spaces correctly (Amari,
1995; Caticha, 2015). A key result in information geometry is that the space of parameters of families of exponential
distributions is a non-euclidean space with the Fisher Information as its metric. A metric is simply a notion of distance
for a given space. For instance, in Euclidean space, the metric is
‚àö‚àëN
i x2
i where N is the dimensionality of the space
and the xis are the coordinate vectors of the space. We can represent general coordinate transformers on spaces with
any metric through the use of a metric tensor G. Essentially, we measure differences between distributions in terms of
the KL divergence, and thus if we want to see how an inÔ¨Ånitesimal change in the parameters of a distribution results in
changes to the distribution itself, we can measure an inÔ¨Ånitesimal change in their KL divergence as a function of the
inÔ¨Ånitesimal change in the parameters. i.e.
‚àÇp(x; Œ∏)
‚àÇŒ∏ = lim
Œ¥Œ∏‚Üí0
DKL[p(x; Œ∏)||p(x; Œ∏+ Œ¥Œ∏)] (14)
In the case of the space of parameters of exponential distributions, the metric tensor is the Fisher information, which
arises as from the Taylor expansion of the inÔ¨Ånitesimal KL divergence between the two distributions. We deÔ¨Åne
Œ∏‚Ä≤= Œ∏+ Œ¥Œ∏. SpeciÔ¨Åcally, since there is only an inÔ¨Åntesimal change, we can Taylor-expand around Œ∏‚Ä≤= Œ∏to obtain,
DKL[p(x; Œ∏)||p(x; Œ∏‚Ä≤)] ‚âàDKL[p(x; Œ∏)||p(x; Œ∏)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
=0
+ ‚àÇDKL[p(x; Œ∏)||p(x; Œ∏‚Ä≤)]
‚àÇŒ∏ |Œ∏=Œ∏‚Ä≤(Œ∏‚àíŒ∏‚Ä≤)
Ó¥ô Ó¥òÓ¥ó Ó¥ö
=0
+‚àÇ2DKL[p(x; Œ∏)||p(x; Œ∏‚Ä≤)]
‚àÇŒ∏2 |Œ∏=Œ∏‚Ä≤(Œ∏‚àíŒ∏‚Ä≤)2
(15)
‚Äò Where the Ô¨Årst two terms vanish, so we need only handle the second term,
DKL[p(x; Œ∏)||p(x; Œ∏‚Ä≤)] ‚âà‚àÇ2DKL[p(x; Œ∏)||p(x; Œ∏‚Ä≤)]
‚àÇŒ∏2 |Œ∏=Œ∏‚Ä≤(Œ∏‚àíŒ∏‚Ä≤)2
=
‚à´
p(x; Œ∏)‚àÇln p(x; Œ∏)
‚àÇŒ∏
‚àÇln p(x; Œ∏)
‚àÇŒ∏ dŒ∏
= I (16)
where Iis the Fisher information. Since the internal states can be interpreted as parametrizing distributions over
external states, as parameters, they lie on an information-geometric manifold with a Fisher information metric. This is
the extrinsic information geometry. Simultaneously, the internal states also parametrize (implicitly) a second (empirical)
distribution over the internal states. This parametrization gives rise to a second information geometry ‚Äì the intrinsic
geometry, since it represents the relationship the internal states have to the distribution over themselves. SpeciÔ¨Åcally,
suppose i deÔ¨Åne the sufÔ¨Åcient statistics of a density over internal states p(i; i), and e = œÉ(i) deÔ¨Åne the sufÔ¨Åcient
statistics of the variational density over external statesq(e; e), then we can see that the internal states in fact parametrize
two densities and thus partake in two simultaneous information geometries. First, there is a metric deÔ¨Åned over the
space of internal densities,
I(i) = ‚àÇ2DKL[p(i; i)||p(i; i + Œ¥i)]
‚àÇi2 |i+Œ¥i=i (17)
which is called the intrinsic information geometry. And secondly, a metric deÔ¨Åned over the space of external densities,
parametrized by internal states,
I(e) = ‚àÇ2DKL[q(e; e)||q(e; e + Œ¥e)]
‚àÇe2 |e+Œ¥e=e (18)
which is called the extrinsic information geometry. These well-deÔ¨Åned intrinsic and extrinsic information geometries,
allow us to interpret the motion of the internal as also representing motion on the intrinsic and extrinsic statistical
manifolds. Crucially, enabling us to make mathematically precise the link between two conceptually distinct ideas ‚Äì
dynamical motion in space, and variational inference on parameters of distributions. Using this underlying information-
geometric framework, in the next section we shall go on to see how we can interpret the dynamics of a non-equilibrium
system at NESS as performing approximate variational Bayesian inference on its external environment.
13
A PREPRINT - 5TH OCTOBER , 2021
6 Self-Organization and Variational Inference
Here we present the key results of the Free-energy principle via the Free-Energy lemma. SpeciÔ¨Åcally, this says, Ô¨Årstly,
that the dynamics of the autonomous states can be interpreted as minimizing a free energy functional over the external
states, and thus can be construed as performing a kind of elemental Bayesian (variational) inference. This section
relies on a fair number of assumptions which are controversial within the FEP community. Here we present the ‚Äòideal
narrative‚Äô where these assumptions are taken as fact. For a more critical discussion of the steps and assumptions in this
section, please see the appendix.
We will Ô¨Årst consider the general case in terms of the ‚Äòparticular‚Äô free energy, which stipulatively assumes that the
system obtains the correct posterior at every time-point, rendering the traditional variational bound superÔ¨Çuous, and thus
demonstrating that in a way self-organizing systems maintaining themselves at NESS can be construed as performing
exact Bayesian inference on the generative model they embody through their NESS density. We thus reach a Ô¨Årst draft
of the key statement of the FEP ‚Äì that the dynamics self-organizing systems that maintain themselves at NESS can be
interpreted as performing exact Bayesian inference on the external states beyond their blanket or, alternatively, they can
be interpreted as approximating approximate (variational) Bayesian inference.
We then introduce the general case of the variational free energy, which is in general a bound upon the marginal NESS
density, and we show in the special case of assuming that the variational distribution over external states which is
parametrized by the internal states can be approximated by the Laplace approximation, that we can interpret the Ô¨Çow
of autonomous states as directly performing a descent upon the variational free energy and thus directly performing
variational Bayesian inference. Since we, as the modeller, can specify the variational distribution in any desired way,
then this means that this interpretation is potentially tenable for a wide range of systems. The Laplace approximation
approximates the variational distribution as a Gaussian where the variance is a function of the curvature at the mean.
Intuitively, this assumption is that the Gaussian is tightly peaked around the mean value. In linear systems, this
approximation is theoretically well-justiÔ¨Åed, due to the underlying Gaussianity of the stochastic noise in the system,
and the likely concentration of the probability mass around the mean, since the noie distribution is unimodal. However,
in nonlinear systems complex multimodal distributions may emerge, even with purely Gaussian noise due to intrinsic
nonlinearities of the dynamics, and so this approximation may perform poorly. On the other hand, the Gaussian
distribution arises regularly in nature whenever averages over large numbers of independent events are taken c.f. the
Central Limit Theorem (CLT), and can thus be considered a natural modelling choice for distribution of the mode of the
external states given the blanket, which likely is composed of contributions from a large number of speciÔ¨Åc external
states.
To recall, we can write the Ô¨Çow of autonomous states Œ±= (i,a) in terms of a gradient descent on the log NESS density
of the particular states ln p(s,i,a ) with both dissipative and solenoidal components via the Helmholtz decomposition.
fŒ± = (Œì ‚àíQ)‚àáŒ±ln p‚àó(s,Œ±) (19)
Then we can deÔ¨Åne the particular free energy as the variational free energy, where the variational distribution over
external states, is stipulatively deÔ¨Åned to be equal to the ‚Äòtrue‚Äô posterior distribution over external states given the
particular states q(e|s,i,a ) = p‚àó(e|s,i,a ) 11. With this assumption, we can deÔ¨Åne the particular free energy using the
11We implicitly assume here that the variational distribution can be stipulated to be of the same family of the true posterior, so that
they can match one another
14
A PREPRINT - 5TH OCTOBER , 2021
standard form for the variational free-energy
Fparticular = DKL[q(e|i,s,a )||p‚àó(e,i,s,a )]
= Eq(e|i,s,a)[ln p‚àó(i,s,a |e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Accuracy
+ DKL[q(e|i,s,a )||p‚àó(e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Complexity
= ln p‚àó(i,s,a )Ó¥ô Ó¥òÓ¥ó Ó¥ö
Evidence
+ DKL[q(e|i,s,a )||p‚àó(e|i,s,a )]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Bound = 0
= ln p‚àó(i,s,a ) (20)
where the last line follows because the bound is always 0 since we have deÔ¨Åned the variational and true posteriors to
be the same. Importantly, we see that the particular free energy is then equal to the log of the NESS density over the
sensory, internal, and active states. As such, we can rewrite the dynamics of the autonomous states directly in terms of
the particular free energy,
dŒ±
dt = (Œì ‚àíQ)‚àáŒ±Fparticular(s,Œ±) (21)
While this may seem like just a mathematical sleight of hand, it demonstrates how systems which maintain the statistical
structure of a Markov Blanket at equilibrium can in fact be interpreted as performing variational Bayesian inference with
a correct posterior distribution. If, conversely, we relax this assumption somewhat, so that, as is typical for variational
inference when the class of distributions represented under the variational density does not include the true posterior,
then we retain an approximate relationship. That is, when q(e|i,s,a ; Œ∏) ‚âàp(e|i,s,a ), we obtain,
F= DKL[q(e|i,s,a )||p‚àó(e,i,s,a )]
= ln p‚àó(i,s,a ) + DKL[q(e|is,a)||p‚àó(e|i,s,a )]
‚âàln p‚àó(i,s,a )
=‚áí dŒ±
dt ‚âà(Œì ‚àíQ)‚àáŒ±F(s,Œ±)
So we can see that in this case, we can interpret the dynamics of the autonomous states as approximating approximate
Bayesian inference. This is perhaps the most general statement of the FEP ‚Äì that the dynamics of a system which
maintains the statistical structure of a Markov Blanket at NESS against external dissipative perturbations, can be
interpreted as performing approximate variational Bayesian inference to optimize a distribution over the external
states of the environment, parametrized by its own internal states. The distinction between variational and particular
free energy, with the particular free energy always using the stipulatively correct posterior, while being somewhat a
mathematical trick, is also a useful philosophical distinction to draw. In effect, we can think of the system as always
performing correct Bayesian inference, simply because the inference is over the system itself, where the generative
model of the system is simply its NESS density. Conversely, we can see the approximation arising from the approximate
variational distribution as being related to the imperfection of our own understanding of the system as an exogenous
modeller. The system is perfectly happy using its Bayes-optimal posterior at all times. A variational distribution distinct
from this posterior must be, in some sense, the creature and creation of the modeller, not of the system, and as such the
approximations to the dynamics that arise from this approximation is due to the approximations implicit in modelling
rather than in the dynamics of the system per-se. It is also important to note that while we have used an approximation
sign, in reality the variational free energy is an upper bound upon the log model evidence or the particular free energy ‚Äì
i.e. F‚â•F particular and the approximate dynamics can be interpreted as driving the system towards the minimization
of this bound, and thus increasing the accuracy of the approximation in a manner analogous to the similar process
inherent in variational inference.
While in the general case above, the relationship between the dynamics of the system and variational inference is only
approximate, if we are only interested in the distribution over the mode of the external states ‚Äì i.e. the most likely
external state conÔ¨Åguration ‚Äì instead of the full distribution, then the approximation becomes exact and we can directly
15
A PREPRINT - 5TH OCTOBER , 2021
see that the dynamics of the system do perform variational inference upon the mode of the external states. Here we can
see that, in a sense, the maximum-a-posteriori (MAP) modes for the internal states precisely track the MAP modes for
the external states and thus, under the Laplace approximation, can be seen as directly performing a minimization of the
variational free energy.
Firstly, recall from previously that we had deÔ¨Åned the smooth mapping between the modes of the external and internal
states given the blanket state, e(b) = œÉ(i(b)). By applying the chain rule to this function, it is straightforward to derive
the dynamics of the external mode with respect to the internal mode,
de(b)
dt = ‚àÇœÉ(i(b))
‚àÇi(b)
di(b)
dt (22)
Then, assuming that the mapping is invertible (requiring that the internal states and external states have the same
dimensionality), or rather in the general case that it has a Moore-Penrose pseudoinverse, we can express the dynamics
of the internal mode in terms of the dynamics of the external mode,
di(b)
dt = ‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1 de(b)
dt (23)
Similarly, we can derive the expression NESS density over the external mode in terms of the mode of the internal states,
which provides a precise mapping, called the synchronization manifold, between the two densities, even though they are
in fact separated by the Markov Blanket,
‚àÇln p(e(b)|b)
‚àÇi = ‚àÇln p(e(b)|b)
‚àÇe(b)
‚àÇœÉ(i(b))
‚àÇi (24)
Combining Equations 24 and 23 and using the fact that the dynamics of the external mode, by the marginal Ô¨Çow lemma
are, de(b)
dt = (Œìe ‚àíQe)‚àáeln p(e(b)|b), we can express the dynamics of the internal mode in terms of the marginal
NESS density over the external states, thus understanding how the internal states probabilistically track changes in their
environment,
di(b)
dt = ‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1 de(b)
dt
= ‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1
(Œìe ‚àíQe)‚àáeln p(e(b)|b)
= ‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1
(Œìe ‚àíQe)‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1 ‚àÇœÉ(i(b))
‚àÇi(b) ‚àáeln p(e(b)|b)
= (ŒìœÉ ‚àíQœÉ)‚àáiln p(œÉ(i(b))) (25)
where (ŒìœÉ ‚àíQœÉ) = ‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1
(Œìe ‚àíQe)‚àÇœÉ(i(b))
‚àÇi(b)
‚àí1
. Crucially, this expression allows us to express the dynamics of
the internal mode as a gradient descent on the NESS density of the external mode given the blanket, with respect to the
mode of the internal states. Fascinatingly, this relationship takes the same general form of the Helmholtz decomposition
with separate dissipative ŒìœÉ and solenoidal QœÉ components which are simply the original dissipative and solenoidal
components with respect to the internal states modulated by the inverse of the mapping function œÉ. In effect, this
implements a coordinate transform between the coordinates of the dynamics of the external states to the coordinates of
the dynamics of the mode of the external states, as a function of internal states.
Now we demonstrate how we can interpret this gradient descent on the NESS density of the mode over external states in
terms of a direct descent on the variational free energy, and thus as directly and exactly performing variational inference.
First, we must deÔ¨Åne our variational distribution q(e|b; i) which is a distribution over the modes of external states, given
the blanket states, parametrized by the mode of the internal states. Since we are only interested now in distributions
over the mode of the external states, a reasonable assumption is that it is approximately Gaussian distributed due to
the central limit theorem. This means that a Laplace approximation, which is a Gaussian approximation where the
16
A PREPRINT - 5TH OCTOBER , 2021
covariance is simply a function of the mean, derived via a second order Taylor-expansion of the density at the mode, is
a good approximation to use here. We thus deÔ¨Åne the variational density as,
q(e|b; i) = N(e; i,Œ£(i))
where Œ£(i) = ‚àÇ2œÉ(i)
‚àÇi2
‚àí1
(26)
Importantly, if we substitute this deÔ¨Ånition of qinto the variational free energy and drop constants unrelated to the
variational parameters i, we obtain,
F= ln p(i,b) + 1
2tr(Œ£(i))‚àÇ2œÉ(i)
‚àÇœÉ2
‚àí1
+ ln|Œ£(i)|
=‚áí ‚àÇF
‚àÇi = ‚àÇln p(i,b)
‚àÇi
The second line follows since this is the only term where iis directly utilized. Then, from this deÔ¨Ånition, we can see
that the variational free energy is actually precisely the gradient term we see in the expression for the dynamics of the
internal state mode, thus allowing us to rewrite it as,
di
dt = (ŒìœÉ ‚àíQœÉ)‚àáiF (27)
After this thicket of mathematics, we thus see a crucial result for the FEP. That, with a Laplace-encoded variational
density, we can see that the mode of the internal states precisely tracks the mode of the external states, and the dynamics
that allows it to do so are precisely those of a gradient descent on the variational free energy, thus enabling an exact
interpretation of the dynamics of the internal states as performing Bayesian inference on the external states. This proof
demonstrates the fundamentally Ashbyan nature of self-organization at non-equilibrium steady state, where systems, in
order to maintain their steady state, and thus existence as distinct systems, are necessarily forced to engage in some
degree of modelling or tracking external states of the environment, in order to counter their dissipative perturbations.
Interestingly, this exact relationship to variational inference only emerges when considering themodes of the system,
not the full distribution over environmental and internal states as was done previously, where we only obtained an
approximation to variational inference. Perhaps this is because, in some sense, the system need not perform inference on
full distributions, but only on modes. This perhaps makes more intuitive sense within the cybernetic Ashbyan paradigm
where, in general, the system is seen as signiÔ¨Åcantly smaller than the environment, and thus simply cannot be expected
to encode a fully accurate model of the entire environment which, in the extreme case, includes the entire rest of the
universe. Instead, the system simply models and tracks coarse-grained environmental variables such as the mode.
An important additional note is that the gradient descent also contains solenoidal terms based upon QœÉ. Since these
terms are orthogonal to the gradient of the free-energy, they do not affect the ultimate minimum of the descent, however
they may alter its rate of convergence, since solenoidal terms encourage additional exploration of the state-space than
a simple gradient descent does. This result also means that strictly speaking, a block diagonal Qmatrix, or even a
state independent Qmatrix is not necessary for this derivation as relaxing these assumptions will simply result in
additional solenoidal coupling terms in equation 27 but will not change the ultimate minimum of the descent so long as
the solenoidal coupling terms remain orthogonal to the gradient of the free energy.
7 The Expected Free Energy and Active Inference
So far, we have only considered the relationship between internal and external states, and observed that the dynamics of
the internal state can be considered to be performing a variational gradient descent on the parameters of the variational
density over external states. The internal state dynamics exactly follow a variational gradient descent if we assume that
the internal states parametrize a Laplacian approximate posterior, or they approximately follow a variational gradient
descent if we assume a broader class of variational posteriors. From this, we can interpret the dynamics of the internal
states as performing some kind of ‚Äòperceptual‚Äô inference about the causes of Ô¨Çuctuations in the blanket states ‚Äì namely,
the external states. But what about the active states? How do they Ô¨Åt into this picture?
17
A PREPRINT - 5TH OCTOBER , 2021
Figure 3: Active Inference and Free Energy Minimization. Top: We see that discrete state space active inference
requires two separate minimizations ‚Äì one of variational free energy for perception, and one of expected free energy
for action selection. Bottom: We see that the EFE functional can be decomposed in various ways to yield a variety of
inÔ¨Çuential objectives which have previously been proposed in the literature. Figure originally in Da Costa et al. (2020)
First, we recall from the approximate Bayesian inference lemma that we can express the dynamics of the autonomous
states (active and internal) in terms of an approximate gradient descent on the variational free energy (Equation 27). By
the marginal Ô¨Çow lemma, if we ignore solenoidal coupling between internal and active states, we can partition this
descent into separate (marginal) descents on the internal and the active states, allowing us to write the dynamics of the
active states as
da
dt ‚âà(Œìaa ‚àíQaa)‚àáaF(s,Œ±) (28)
where Œìaa and Qaa are the block matrices corresponding solely to the interactions between active states in the larger Œì
and Qmatrices. Crucially, if we recall the deÔ¨Ånition of the variational free energy,
F(i,s,a ) = Eq(e;¬µ)[ln q(e; ¬µ) ‚àíln p‚àó(e,i,s,a )]
= Eq(e|¬µ)[‚àíln p‚àó(i,s,a |e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Inaccuracy
+ DKL[q(e; ¬µ)||p‚àó(e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Complexity
(29)
Crucially, the only term in this decomposition that depends on the active states ais the Ô¨Årst inaccuracy term. Thus, we
can straightforwardly write down the dynamics of the active states as,
da
dt ‚âà(Œìaa ‚àíQaa)‚àáaEq(e|¬µ)[‚àíln p‚àó(i,s,a |e)] (30)
Where we can intuitively see that the dynamics of the active states effectively minimize inaccuracy (or maximize
accuracy). In effect, we can interpret the dynamics of the active states at the NESS density to try to ensure that the
variational ‚Äôbeliefs‚Äô encoded by the blanket and internal states of the system are as accurate as possible. Since active
18
A PREPRINT - 5TH OCTOBER , 2021
states can only inÔ¨Çuence external states and not internal states, the way this is achieved is by acting upon the external
states to bring them into alignment with the beliefs represented by the internal states ‚Äì hence active inference.
While this provides a good characterisation of the dynamics of the system at equilibrium, we are often also interested in
the properties of dynamical systems as the self-organize towards equilibrium. SpeciÔ¨Åcally, we wish to characterise the
nature of the active states during this process of self-organization, so that we can understand the necessary kinds of
active behaviour any self-organizing system must evince. To begin to understand the nature of this self-organization we
Ô¨Årst deÔ¨Åne another information theoretic quantity, the Expected Free Energy(EFE) which serves as an upper-bound
on surprisal throughout the entire process of self-organization, with equality only at the equilibrium itself. Since we
have this upper-bound, we can interpret self-organizing systems away from equilibrium, by following their surprisal
dynamics as approximating expected free energy minimization, using logic directly analogous to the approximate
Bayesian inference lemma. Conversely, turning this logic around lets us construct self-organizing systems by deÔ¨Åning
some desired NESS density, and then prescribing dynamics which simply minimize the EFE.
To handle systems away from equilibrium, we deÔ¨Åne some new terminology. We deÔ¨Åne p(et,it,st,at|e0,i0,s0,a0)
to be the probability density over the variables of the system at some time t, which depends on some set of initial
conditions e0,i0,s0,a0. To simplify, we average over the external initial condition and only represent the particular
initial condition œÄ0 = (i0,s0,a0). Next we deÔ¨Åne the expected free energy G(œÄ) similarly to the variational free energy,
but with the current-time predictive density taking the place of the approximate variational posterior, and the NESS
density taking the place of the generative model.
G(œÄ) = Ep(et,œÄt)|œÄt)[ln p(et|œÄt,œÄ0) ‚àíln p‚àó(e,œÄ)]
= Ep(et,œÄt)|œÄt)[‚àíln p‚àó(œÄ|e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Ambiguity
+ DKL[p(et|œÄt,œÄ0)||p‚àó(e)]Ó¥ô Ó¥òÓ¥ó Ó¥ö
Risk
(31)
We see that the EFE mandates the minimization of both ambiguity (i.e. avoiding situations which are heavily uncertain)
and risk (avoiding large divergences between the current state density and the equilibrium state. It is straightforward to
see that the EFE is an upper bound on the expected predictive surprisal at any time-point, by using the fact that the
KL-divergence is always greater than or equal to 0,
DKL[p(et,œÄt|œÄ0)||p‚àó(e,œÄ)] ‚â•0
=‚áí G(œÄt) + Ep(et,œÄt)|œÄt) ln p(œÄt|œÄ0)] ‚â•0
=‚áí G(œÄt) ‚â•‚àíEp(et,œÄt)|œÄt)[ln p(œÄt|œÄ0)]
Similarly, it is straightforward to see that at equilibrium the EFE simply becomes the surprisal.
DKL[p(et,œÄt|œÄ0)||p‚àó(e,œÄ)] = G(œÄt) + Ep(et,œÄt)|œÄt)[ln p(œÄt|œÄ0)] = 0
=‚áí G(œÄt) = ‚àíEp(et,œÄt)|œÄt)[ln p(œÄt|œÄ0)] (32)
Since this is the case, we can understand the EFE as effectively quantifying the discrepancy between the current
predictive density and the equilibrium. Because of this, we can see that the EFE is necessarily a Lyapunov function
of self-organizing dynamics, and it makes sense to interpret self-organizing dynamics under a Markov blanket as
minimizing the EFE. Conversely, if one wants to deÔ¨Åne a set of dynamics that self-organize to some given attractor
p‚àó(e,i,s,a ) then one simply needs to deÔ¨Åne dynamics that minimize the EFE to achieve convergence to the equilibrium
(which may be a local minimum).
Taking this converse approach allows us to move from simply providing an interpretative characterisation of given
dynamics in terms of inference, and move instead to constructing or deÔ¨Åning systems, or agents, which can achieve
speciÔ¨Åc goals. This approach is taken in the literature on active inference process theories (Da Costa et al., 2020;
K. Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017b; K. Friston, Rigoli, et al., 2015; K. Friston et al.,
2012) where instead of simply describing a given stochastic differential equation, we instead consider the NESS density
to be the preferences or desires of the agent often represented as a Boltzmann distribution over environmental rewards
19
A PREPRINT - 5TH OCTOBER , 2021
p‚àó(e,i,s,a ) = exp(‚àír(e)) and the active states (the agent‚Äôs actions) being computed through a minimization of the
EFE, with this minimization either taking place directly as a gradient descent in continuous time and space (K. J. Friston
et al., 2009) or else as an explicit model-based planning algorithm as in the discrete-time and discrete-space formulation
(K. Friston et al., 2017a; Millidge, 2019; Millidge et al., 2020; Tschantz et al., 2020).
8 Philosophical Status of the FEP
It is worth stepping back from the mathematical morass at this point to try to deÔ¨Åne at a high level what kind of
theory, philosophically speaking, the FEP is, and what kind of claims about the world it makes. There have been
numerous debates in the literature about whether the FEP is ‚ÄòfalsiÔ¨Åable‚Äô, or whether it is ‚Äòcorrect‚Äô, and whether or not
it makes any speciÔ¨Åc, empirical claims (Andrews, 2020; Williams, 2020). However often debates on this matter are
obscured or confused by the challenging and deep mathematical background required for a full understanding of the
speciÔ¨Åcs of the FEP. It is clear from the mathematics that the main strand of the FEP offers only an ‚Äòinterpretation‚Äô
of already extant dynamics. In short, FEP presupposes the existence of the kinds of dynamics it wishes to make
sense of ‚Äì dynamical systems which organize themselves into a non-equilibrium steady state, and which maintain the
requisite statistical independency structure of the Markov Blanket condition. Once these conditions are satisÔ¨Åed, the
FEP gives an interpretation of the dynamical evolution of such a system as performing an kind of variational Bayesian
inference whereby the internal states of the system (deÔ¨Åned by the Markov Blanket partition) can be seen as inferring
or representing external states which are otherwise statistically isolated behind the Markov Blanket. Crucially, the
FEP, in its most general formulation does not make any speciÔ¨Åc predictions about the dynamics of the system. It offers
an interpretation only. While systems that implement the FEP can be derived, and several process theories have been
explicitly derived from within the FEP framework (K. Friston, 2005; K. Friston, Rigoli, et al., 2015), all such theories
necessitate making speciÔ¨Åc and ultimately arbitrary modelling choices, such as of the generative model and variational
density. Such choices sit below the level of abstraction that the mathematical theory of the FEP exists at. The FEP, at its
core, only offers a mathematical interpretation of certain dynamical structures.
The FEP is often compared and analogised to the principle of least action in physics (Lanczos, 2012) which allows
one to describe many physical processes (although not all) as minimizing the path integral of a functional called
the ‚Äòaction‚Äô over a trajectory of motion (Sussman & Wisdom, 2015). This argument is often used to claim, in my
opinion correctly, that the FEP is a mathematical ‚Äòprinciple‚Äô or interpretation and therefore cannot be falsiÔ¨Åed or
empirically tested. In my opinion, however, the principle of least action is, in its philosophical status, not directly
analogous to the FEP. While the relationship between the path integral of the action and the dynamics prescribed
by the Euler-Lagrange equations is simply a mathematical truth, the principle of least action itself, as applied to
physics is contains a fundamentally empirical and falsiÔ¨Åable claim ‚Äì that physical systems in the real world can be
well described through its own mathematical apparatus ‚Äì that is of dynamics derived from minimizing an action. This
claim is in principle falsiÔ¨Åable. Not all dynamical systems can be derived from least action principles. If physical
systems predominantly came from the class that cannot be so derived, the principle of least action in physics would be
effectively falsiÔ¨Åed, and the mathematical apparatus underlying it would have become nothing more than an arcane
mathematical curiosity. So far as we know, there is no a-priori reason why much of physics can be so well understood
through action principles, and indeed there are areas of physics ‚Äì such as statistical mechanics and thermodynamics,
and dissipative non-conservative systems in general ‚Äì which cannot (so far) be described in these terms.
It appears a closer physics analogy to the FEP might be one direction of Noether‚Äôs theorem. Noether‚Äôs theorem proves
a direct correspondences between symmetries or invariances in a given system, and conservation laws. For instance,
in physical systems, time-translation symmetry implies the conservation of energy, and rotational symmetry (of the
underlying euclidean space, not any given object within it) implies the conservation of angular momentum. The FEP,
similarly, tries to show a correspondence between the dynamics of a certain kind of system (NESS density, Markov
Blanket conditions) and the dynamics of variational Bayesian inference. Interestingly, while the ‚Äòforward‚Äô direction
20
A PREPRINT - 5TH OCTOBER , 2021
from the NESS density and Markov Blanket conditions handled by the FEP, the reverse direction ‚Äì i.e. whether the
presence of Bayesian inference dynamics implies any kind of statistical structure upon the dynamics of the system
remains unclear, and this is likely a fruitful direction for further theoretical work. Noether‚Äôs theorem, unlike the
principle of least action, matches more closely than the principle of least action since it only speciÔ¨Åes correspondences
between certain kinds of mathematical objects (symmetries and conservation laws) just as the FEP only speciÔ¨Åes a
correspondence between dynamical Ô¨Çows at NESS of a system with a Markov Blanket, and the gradient Ô¨Çows on the
variational free energy.
While its status as a mathematical principle and interpretation only can shield the FEP from the possibility of an
empirical ‚ÄòfalsiÔ¨Åcation‚Äô, this does not mean that the theory is not subject to some kind of implicit intellectual review.
Much of the core motivation behind the FEP has been to try to derive universal properties of the kind of biological
self-organizing systems which give rise to structured behaviour including relatively ‚Äòhigh level‚Äô processes such as
the perception-action-loop, explicit perception and inference about the causes of the external world and, ultimately,
prospective inference and planning. For instance, much of the FEP literature has been focused on and applied to
understanding brain function (K. Friston, 2008; K. Friston et al., 2017a; K. Friston, Rigoli, et al., 2015). This ambition
renders the FEP open to questions about its ‚Äòapplicability‚Äô, if not its falsiÔ¨Åability. The FEP imposes relatively stringent
conditions that dynamical systems must satisfy for the logical steps in the FEP must hold. Although the precise list of
assumptions required is not entirely clear, so far in the literature this appears to include at least:
‚Ä¢ that the system in question can be adequately represented as a Langevin equation (i.e. the system is Markov
and does not depend on history) with additive white Gaussian noise.
‚Ä¢ that the dynamical system as a whole have a well-deÔ¨Åned NESS density (including over the external states)
‚Ä¢ that the system obey the Markov Blanket conditions, which are, in general, relatively restrictive about the
kinds of Ô¨Çows that are possible, and appear to have become more restrictive in K. Friston, Da Costa, and Parr
(2020), which precludes any solenoidal coupling between active and sensory states
‚Ä¢ that there is an injective mapping between the most-likely internal state given the blanket and the mode of the
distribution of external states given the blanket states, which is additionally smooth and differentiable (this is
required for the dual-aspect information geometry, and thus the identiÔ¨Åcation with Bayesian inference)
These conditions are quite strict about the class of systems that the FEP can apply to, and it is unclear if ‚Äòreal systems‚Äô
of the kind of the FEP desires to explain ‚Äì such as biological self-organization, and especially brains, can fulÔ¨Ål them. If
it turns out that such systems Ô¨Çagrantly violate the conditions for the FEP, then the FEP cannot be said to apply to them
and thus cannot be of use in understanding them, even as an interpretatory device. In this case, the FEP would fail the
applicability criterion, and would cease to be particularly useful for its original goals of neuroscience, even if it remains
not technically falsiÔ¨Åed and does, in fact, apply to some obscure mathematical class of dynamical systems. Importantly,
many of the assumptions of the FEP, when interpreted strictly, do not appear to hold in general for complex biological
systems such as brains. For instance, to take extreme but illustrative examples, it is clear that no biological system
is ever in a true non-equilibrium steady state, since eventually all such organisms will eventually age and die, and
indeed eventually the entire universe will likely eventually decay to a thermodynamic equilibrium state. Additionally,
the Markov Blanket assumption is directly violated by things such as x-rays (and indeed gravity) which can directly
interact with ‚Äòinternal states‚Äô of the brain, such as neurons, without Ô¨Årst passing through the Markov Blanket of the
physical boundaries of the brain and the sensory epithelium. As such, for a real physical system, we must take the
assumptions of the FEP to be only approximations, which hold locally, or approximately, but not for all time and with
complete perfection. It remains to be seen, and empirically investigated if possible, the extent to which the mathematical
interpretations and logical statements of the FEP remain robust to such slight relaxations of its core assumptions.
While the FEP provides a mathematical interpretation of certain kinds of dynamics in terms of inference, it also,
largely, remains to be seen whether such an interpretation is useful for spurring new ideas, questions, and developments
within the Ô¨Åelds the FEP hopes to inÔ¨Çuence ‚Äì such as neuroscience, cognitive science, and dynamical systems theory.
21
A PREPRINT - 5TH OCTOBER , 2021
Returning to our anaologies of the least action principle and Noether‚Äôs theorem, while both of these mathematical
results only provides interpretations of known dynamics, by operating at a high level of abstraction they provide
powerful capabilities for generalization. For instance the principle of least action allows for dynamics to be derived,
via the Euler-Lagrange equations, directly from the high level speciÔ¨Åcation of the action. For instance, potentially
new or counterfactual laws of physics can be derived simply by postulating a given Lagrangian or Hamiltonian and
then working through the mathematical machinery of the principle of least action to derive the ensuing dynamics.
Additionally, by investigating equivariances in the action, one can often understand the kinds of invariances and degrees
of freedom that exist in the actually realized dynamics. Similarly, Noether‚Äôs theorem allows one to play with setting up
certain conserved quantities or symmetries a-priori, and then work out precisely the consequences that these entail for
the dynamics.
It is currently unclear to what extent the FEP offers such powerful advantages of abstraction and generalization. This is
largely due to the FEP being immature as a Ô¨Åeld compared to the cornerstones of classical physics, and the majority of
the research effort so far has gone into making the theory precise rather than deriving consequences and generalizations
from it, but there are some promising initial signs which have just begun to emerge in the literature of the power the
FEP perspective offers. From a practical perspective, the FEP appears to offer a number of novel techniques. Firstly,
given a desired NESS density, the free energy lemma provides a straightforward way of deriving dynamics which
will necessarily reach that density, due to the fact that the variational free energy becomes a Lyapunov function of
the system as a whole. This approach has strong potential links to Markov-Chain-Monte-Carlo methods in machine
learning and statistics, which aim to approximate an intractable posterior distribution by the time evolution of a Markov
process (M. Betancourt, 2017; Brooks, Gelman, Jones, & Meng, 2011; Chen, Fox, & Guestrin, 2014; Metropolis et al.,
1953; Neal et al., 2011). The FEP provides a new perspective on such systems as fundamentally performing variational
Bayesian inference, and may in future be used to develop improved algorithms in this domain, akin to the developments
of Hamiltonian (M. J. Betancourt, 2013) and Riemannian MCMC (Girolami & Calderhead, 2011) methods. For
instance, there is much potential in the idea of solenoidal Ô¨Çow speeding up convergence to the desired equilibrium
density (Ma, Chen, & Fox, 2015). Conversely, the FEP, through the Helmholtz decomposition, may additionally provide
tools for inferring the eventual NESS density given a speciÔ¨Åc set of dynamics (K. Friston, 2019b; Ma et al., 2015). This
would allow for an analytical or empirical characterisation of the ultimate fate of a system, and allow for characterising
different kinds of systems purely, such as whether equilibrium or non-equilibrium purely by their dynamics far from
equilibrium.
A second strand of potentially directly useful research which has begun to arise from the FEP is empirical and
statistical methodologies for deÔ¨Åning, computing, and approximating Markov Blankets. This implies the ability to
infer the statistical independency structure of the dynamics either from analytical knowledge of the dynamics or from
observed trajectories. There are already two approaches to achieve this in the literature. One which utilizes graph
theory in the form of the graph Laplacian to infer nodes of the Markov blanket based on the parents, and children of
parents of the largest eigenstates of the Jacobian (K. Friston, 2013; Palacios, Razi, Parr, Kirchhoff, & Friston, 2017).
A second approach directly uses the Hessian of the dynamics to attempt to read off the conditional independency
requirements it implies (K. J. Friston, Fagerholm, et al., 2020). These approaches may have substantial merit and
utility in understanding the effective statistical independency structure of complex dynamical processes, especially
questions regarding functional independence in the brain. This strand of research heavily relates to the question of
abstraction in dynamical systems ‚Äì namely, whether complex systems can or cannot be straightforwardly partitioned
into independent ‚Äòsubsystems‚Äô which can then be abstracted over. For instance, the ideal would be the ability to, given a
complex high-dimensional dynamical system, parse this system into individual ‚Äòentities‚Äô (separated by Markov blankets)
which interact with each other according to another set of (hopefully simpler) dynamical rules. This would allow for an
automatic procedure to transform a high dimensional complex system into a simpler, low-dimensional approximate
system more amenable for analysis and, ultimately understanding.
22
A PREPRINT - 5TH OCTOBER , 2021
Finally, it is clear that the process theories inspired by the FEP, although epistemologically they stand apart from the FEP,
neither requiring not supporting its validity, have had substantial inÔ¨Çuence and impact within theoretical neuroscience,
where they have been productively applied to understanding a wide range of behavioural and neuronal phenomena.
9 Discussion
In this paper, we have aimed to clarify the core logical steps of the FEP, and explain the mathematical apparatus
underlying it. At its core, the FEP is a simple theory, which relates the dynamics of a dynamical system with a
non-equilibrium steady state, to the dynamics of a variational Bayesian inference process which minimizes a variational
free energy functional. The key logic required for this identiÔ¨Åcation is simply the Helmholtz decomposition of the
dynamics into a gradient descent on a potential function, and then the Laplace approximation of the variational density
which enables the identiÔ¨Åcation of the potential with the variational free energy. However, it is important to realize
that the FEP prescribes an interpretation of certain kinds of system behaviour only ‚Äì it cannot naturally be used in a
forward direction to make causal predictions about the evolution of a system, since the dynamics of the system which
would be used to make such a prediction must intrinsically be known before the variational free energy functional can
be identiÔ¨Åed. However, the FEP does provide a straightforward mathematical machinery for the design and creation of
systems which fulÔ¨Åll its postulates, by providing a mathematical mechanism to translate a desired stationary density
with a Markov Blanket conditional independence structure to a set of dynamics which will maintain it.
While throughout the main body of this paper we have largely endeavoured to present the FEP in the most charitable
light, there is still a substantial amount of controversy over the correctness and interpretation of the FEPs mathematical
claims ‚Äì see Biehl et al. (2020) and Aguilera et al. (2021). A crucial limitation of the FEP is that many of the conditions
required for its derivations are quite restrictive and may signiÔ¨Åcantly limit the kinds of system the FEP can reasonably
be applied to. Additionally, many of the claims made in the literature about the power and generality of the FEP
are somewhat overstated given these conditions, as well as the FEPs entirely interpretational theoretical status. An
important question remains as to the degree to which the assumptions required by the FEP can be slightly relaxed, so as
to admit a signiÔ¨Åcantly larger class of potential systems into consideration, without totally destroying the core claims of
the FEP as to systems being able to be interpreted as performing variational inference. By presenting the reader with a
mathematically detailed, yet ideally intuitive and fairly straightforward presentation of the key concepts of the FEP, we
hope to enable them to engage more deeply with the technical literature discussing and debating these questions, as
well as to understand on a deep level simply what the FEP does and does not do.
References
Aguilera, M., Millidge, B., Tschantz, A., & Buckley, C. L. (2021). How particular is the physics of the free energy
principle? arXiv preprint arXiv:2105.11203.
Amari, S.-I. (1995). Information geometry of the em and em algorithms for neural networks. Neural networks, 8(9),
1379‚Äì1408.
Andrews, M. (2020). The math is not the territory: Navigating the free energy principle.
Beal, M. J. (2003). Variational algorithms for approximate bayesian inference(Unpublished doctoral dissertation).
UCL (University College London).
Benrimoh, D., Parr, T., Adams, R. A., & Friston, K. (2019). Hallucinations both in and out of context: an active
inference account. PloS one, 14(8), e0212379.
Betancourt, M. (2017). A conceptual introduction to hamiltonian monte carlo. arXiv preprint arXiv:1701.02434.
Betancourt, M. J. (2013). Generalizing the no-u-turn sampler to riemannian manifolds.arXiv preprint arXiv:1304.1920.
Biehl, M., Pollock, F. A., & Kanai, R. (2020). A technical critique of the free energy principle as presented in" life as
we know it" and related works. arXiv preprint arXiv:2001.06408.
23
A PREPRINT - 5TH OCTOBER , 2021
Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the
American statistical Association, 112(518), 859‚Äì877.
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning. Journal
of mathematical psychology, 76, 198‚Äì211. Retrieved from https://www.sciencedirect.com/science/
article/pii/S0022249615000759
Brooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of markov chain monte carlo. CRC press.
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and perception: A
mathematical review. Journal of Mathematical Psychology, 81, 55‚Äì79.
Caticha, A. (2015). The basics of information geometry. In Aip conference proceedings (V ol. 1641, pp. 15‚Äì26).
Chen, T., Fox, E., & Guestrin, C. (2014). Stochastic gradient hamiltonian monte carlo. In International conference on
machine learning (pp. 1683‚Äì1691).
Clark, A. (2015). SurÔ¨Ång uncertainty: Prediction, action, and the embodied mind. Oxford University Press. Retrieved
from https://books.google.co.uk/books?hl=en&lr=&id=TnqECgAAQBAJ&oi=fnd&pg=PP1&dq=
andy+clark+surfing+uncertainty&ots=aurm4jE3NO&sig=KxeHGJ6YJJdN9tKyr6snwDyBBKg&redir
_esc=y#v=onepage&q=andy%20clark%20surfing%20uncertainty&f=false
Conant, R. C., & Ross Ashby, W. (1970). Every good regulator of a system must be a model of that system.International
journal of systems science, 1(2), 89‚Äì97.
Cullen, M., Davey, B., Friston, K. J., & Moran, R. J. (2018). Active inference in openai gym: A paradigm for computa-
tional investigations into psychiatric illness. Biological psychiatry: cognitive neuroscience and neuroimaging,
3(9), 809‚Äì818.
Da Costa, L., Friston, K., Heins, C., & Pavliotis, G. A. (2021). Bayesian mechanics for stationary processes. arXiv
preprint arXiv:2106.13830.
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V ., & Friston, K. (2020). Active inference on discrete state-spaces:
a synthesis. arXiv preprint arXiv:2001.07203.
Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine. Neural computation, 7(5),
889‚Äì904.
Esposito, M., & Van den Broeck, C. (2010). Three faces of the second law. i. master equation formulation. Physical
Review E, 82(1), 011143.
Feynman, R. (1998). Statistical mechanics: a set of lectures (advanced book classics).
Fox, C. W., & Roberts, S. J. (2012). A tutorial on variational bayesian inference. ArtiÔ¨Åcial intelligence review, 38(2),
85‚Äì95.
Friston, K. (2003). Learning and inference in the brain. Neural Networks, 16(9), 1325‚Äì1352.
Friston, K. (2005). A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences, 360(1456), 815‚Äì836.
Friston, K. (2008). Hierarchical models in the brain. PLoS computational biology, 4(11).
Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in cognitive sciences, 13(7), 293‚Äì301.
Friston, K. (2010). The free-energy principle: a uniÔ¨Åed brain theory? Nature reviews neuroscience, 11(2), 127‚Äì138.
Friston, K. (2012). The history of the future of the bayesian brain. NeuroImage, 62(2), 1230‚Äì1233. Retrieved from
https://www.sciencedirect.com/science/article/pii/S1053811911011657
Friston, K. (2013). Life as we know it. Journal of the Royal Society Interface, 10(86), 20130475.
Friston, K. (2019a). A free energy principle for a particular physics.
Friston, K. (2019b). A free energy principle for a particular physics. arXiv preprint arXiv:1906.10184.
Friston, K., & Ao, P. (2012a). Free energy, value, and attractors. Computational and mathematical methods in medicine,
2012.
Friston, K., & Ao, P. (2012b). Free energy, value, and attractors.Computational and mathematical methods in medicine,
2012.
Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2020). Sophisticated inference. arXiv preprint
24
A PREPRINT - 5TH OCTOBER , 2021
arXiv:2006.04120. Retrieved from https://arxiv.org/abs/2006.04120
Friston, K., Da Costa, L., & Parr, T. (2020). Some interesting observations on the free energy principle. arXiv preprint
arXiv:2002.04501.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017a). Active inference: a process theory.
Neural computation, 29(1), 1‚Äì49.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017b). Active inference: a process theory.
Neural computation, 29(1), 1‚Äì49.
Friston, K., Heins, C., Ueltzh√∂ffer, K., Da Costa, L., & Parr, T. (2021). Stochastic chaos and markov blankets. Entropy,
23(9), 1220.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris,
100(1-3), 70‚Äì87.
Friston, K., Levin, M., Sengupta, B., & Pezzulo, G. (2015). Knowing one‚Äôs place: a free-energy approach to
pattern regulation. Journal of the Royal Society Interface , 12(105), 20141383. Retrieved from https://
royalsocietypublishing.org/doi/full/10.1098/rsif.2014.1383
Friston, K., Parr, T., & Zeidman, P. (2018). Bayesian model reduction. arXiv preprint arXiv:1805.07092.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G. (2015). Active inference and epistemic
value. Cognitive neuroscience, 6(4), 187‚Äì214.
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and agency: optimal control without cost
functions. Biological cybernetics, 106(8-9), 523‚Äì541.
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., & Dolan, R. J. (2014). The anatomy of
choice: dopamine and decision-making. Philosophical Transactions of the Royal Society B: Biological Sciences,
369(1655), 20130481.
Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active inference? PloS one, 4(7).
Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and behavior: a free-energy formulation.
Biological cybernetics, 102(3), 227‚Äì260. Retrieved from https://link.springer.com/article/10.1007/
s00422-010-0364-z
Friston, K. J., Fagerholm, E. D., Zarghami, T. S., Parr, T., Hip√≥lito, I., Magrou, L., & Razi, A. (2020). Parcels and
particles: Markov blankets in the brain. arXiv preprint arXiv:2007.09704. Retrieved from https://arxiv.org/
abs/2007.09704
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active inference, curiosity and
insight. Neural computation, 29(10), 2633‚Äì2683.
Friston, K. J., Rosch, R., Parr, T., Price, C., & Bowman, H. (2018). Deep temporal models and active inference.
Neuroscience & Biobehavioral Reviews, 90, 486‚Äì501.
Friston, K. J., Wiese, W., & Hobson, J. A. (2020). Sentience and the origins of consciousness: From cartesian duality
to markovian monism. Entropy, 22(5), 516.
Geweke, J. (2007). Bayesian model comparison and validation. American Economic Review, 97(2), 60‚Äì64.
Ghahramani, Z., & Beal, M. J. (2001). Propagation algorithms for variational bayesian learning. In Advances in neural
information processing systems (pp. 507‚Äì513).
Ghahramani, Z., Beal, M. J., et al. (2000). Graphical models and variational methods . Advanced mean Ô¨Åeld
methods-theory and practice. MIT Press.
Girolami, M., & Calderhead, B. (2011). Riemann manifold langevin and hamiltonian monte carlo methods. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 73(2), 123‚Äì214.
Goodfellow, I., Bengio, Y ., & Courville, A. (2016).Deep learning. MIT press.
Hohwy, J. (2016). The self-evidencing brain. No√ªs, 50(2), 259‚Äì285.
Hohwy, J., Roepstorff, A., & Friston, K. (2008). Predictive coding explains binocular rivalry: An epistemological
review. Cognition, 108(3), 687‚Äì701.
Jaswinski, A. (1970). Stochastic processes and Ô¨Åltering theory, 1970. Academic Press.
25
A PREPRINT - 5TH OCTOBER , 2021
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1998). An introduction to variational methods for
graphical models. In Learning in graphical models (pp. 105‚Äì161). Springer.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational methods for
graphical models. Machine learning, 37(2), 183‚Äì233.
Jordan, R., Kinderlehrer, D., & Otto, F. (1998). The variational formulation of the fokker‚Äìplanck equation. SIAM
journal on mathematical analysis, 29(1), 1‚Äì17.
Kalman, R. E. (1960). A new approach to linear Ô¨Åltering and prediction problems.
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes.
Kondepudi, D., & Prigogine, I. (2014). Modern thermodynamics: from heat engines to dissipative structures. John
Wiley & Sons.
Lanczos, C. (2012). The variational principles of mechanics. Courier Corporation.
Lawson, R. P., Rees, G., & Friston, K. J. (2014). An aberrant precision account of autism. Frontiers in human
neuroscience, 8, 302. Retrieved from https://www.frontiersin.org/articles/10.3389/fnhum.2014
.00302/full
Ma, Y.-A., Chen, T., & Fox, E. B. (2015). A complete recipe for stochastic gradient mcmc. arXiv preprint
arXiv:1506.04696.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation of state calculations
by fast computing machines. The journal of chemical physics, 21(6), 1087‚Äì1092.
Millidge, B. (2019). Deep active inference as variational policy gradients.
Millidge, B., Seth, A., & Buckley, C. L. (2021). Predictive coding: a theoretical and experimental review. arXiv
preprint arXiv:2107.12979.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). On the relationship between active inference and
control as inference. arXiv preprint arXiv:2006.12964.
Mumford, D. (1992). On the computational architecture of the neocortex. Biological cybernetics, 66(3), 241‚Äì251.
Neal, R. M., & Hinton, G. E. (1998). A view of the em algorithm that justiÔ¨Åes incremental, sparse, and other variants.
In Learning in graphical models (pp. 355‚Äì368). Springer.
Neal, R. M., et al. (2011). Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11), 2.
Ollivier, Y ., Arnold, L., Auger, A., & Hansen, N. (2017). Information-geometric optimization algorithms: A unifying
picture via invariance principles. Journal of Machine Learning Research, 18(18), 1‚Äì65.
Ovchinnikov, I. V . (2016). Introduction to supersymmetric theory of stochastics.Entropy, 18(4), 108.
Palacios, E. R., Razi, A., Parr, T., Kirchhoff, M., & Friston, K. (2017). Biological self-organisation and markov blankets.
BioRxiv, 227181. Retrieved from https://www.biorxiv.org/content/10.1101/227181v1.abstract
Parr, T. (2019). The computational neurology of active vision (Unpublished doctoral dissertation). UCL (University
College London).
Parr, T., Da Costa, L., & Friston, K. (2020). Markov blankets, information geometry and stochastic thermodynamics.
Philosophical Transactions of the Royal Society A, 378(2164), 20190159.
Parr, T., & Friston, K. J. (2017). Uncertainty, epistemics and active inference. Journal of The Royal Society Interface,
14(136), 20170376.
Parr, T., & Friston, K. J. (2018). Active inference and the anatomy of oculomotion. Neuropsychologia, 111, 334‚Äì343.
Parr, T., Markovic, D., Kiebel, S. J., & Friston, K. J. (2019). Neuronal message passing using mean-Ô¨Åeld, bethe, and
marginal approximations. ScientiÔ¨Åc reports, 9(1), 1‚Äì18.
Parr, T., Sajid, N., & Friston, K. J. (2020). Modules or mean-Ô¨Åelds? Entropy, 22(5), 552. Retrieved from
https://www.mdpi.com/1099-4300/22/5/552
Pearl, J. (2011). Bayesian networks.
Pearl, J. (2014). Probabilistic reasoning in intelligent systems: networks of plausible inference. Elsevier.
Prigogine, I. (2017). Non-equilibrium statistical mechanics. Courier Dover Publications.
Prigogine, I., & Lefever, R. (1973). Theory of dissipative structures. In Synergetics (pp. 124‚Äì135). Springer.
26
A PREPRINT - 5TH OCTOBER , 2021
Ramstead, M. J., Hesp, C., Tschantz, A., Smith, R., Constant, A., & Friston, K. (2020). Neural and phenotypic repre-
sentation under the free-energy principle. Neuroscience & Biobehavioral Reviews. Retrieved from https://www
.sciencedirect.com/science/article/pii/S0149763420306643?casa_token=16rC0ManFBUAAAAA:
3mbntn5I7fObnA_Y397rvZbWrnUzkqmALD1LtS88tGrIRxbw9RQvU55XJuH-zKdBi6tPaN9faDM
Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some
extra-classical receptive-Ô¨Åeld effects. Nature neuroscience, 2(1), 79‚Äì87.
Schwartenbeck, P., FitzGerald, T., Dolan, R., & Friston, K. (2013). Exploration, novelty, surprise, and free energy
minimization. Frontiers in psychology, 4, 710.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., & Friston, K. J. (2019). Com-
putational mechanisms of curiosity and goal-directed exploration. , 8, e41703. Retrieved 2019-11-15, from
https://doi.org/10.7554/eLife.41703 doi: 10.7554/eLife.41703
Seifert, U. (2008). Stochastic thermodynamics: principles and perspectives. The European Physical Journal B, 64(3),
423‚Äì431.
Seifert, U. (2012). Stochastic thermodynamics, Ô¨Çuctuation theorems and molecular machines. Reports on progress in
physics, 75(12), 126001.
Spratling, M. W. (2017). A review of predictive coding algorithms. Brain and cognition, 112, 92‚Äì97. Retrieved
from https://www.sciencedirect.com/science/article/pii/S027826261530035X?casa_token=
zzTchZsrFesAAAAA:5bJNguAnRfn4BOjlCtmGvjiQT0Mkk3CE1By9JsrGrDIT0qY-CUKLUwVROkHB9S
_kUx6mtH-nc74
Srinivasan, M. V ., Laughlin, S. B., & Dubs, A. (1982). Predictive coding: a fresh view of in-
hibition in the retina. Proceedings of the Royal Society of London. Series B. Biological
Sciences, 216(1205), 427‚Äì459. Retrieved from https://royalsocietypublishing.org/
doi/abs/10.1098/rspb.1982.0085?casa_token=gdNrGbAlmC8AAAAA%3Ac1xArFgNym4QLB0vI
-dDd0ywIS0ozVZjzjnhogf4CVpFZi2zIW8cMU3OIZwvV8cFCoVqAaDOFo_IFDY
Sussman, G. J., & Wisdom, J. (2015). Structure and interpretation of classical mechanics. The MIT Press.
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement learning through active inference.
arXiv preprint arXiv:2002.12636.
Van den Broeck, C., & Esposito, M. (2010). Three faces of the second law. ii. fokker-planck formulation. Physical
Review E, 82(1), 011144.
Wiener, N. (2019). Cybernetics or control and communication in the animal and the machine. MIT press.
Williams, D. (2020). Is the brain an organ for prediction error minimization?
Yedidia, J. S. (2011). Message-passing algorithms for inference and optimization. Journal of Statistical Physics, 145(4),
860‚Äì890.
Yuan, R., & Ao, P. (2012). Beyond it√¥ versus stratonovich. Journal of Statistical Mechanics: Theory and Experiment,
2012(07), P07010.
Yuan, R., Ma, Y ., Yuan, B., & Ao, P. (2010). Constructive proof of global lyapunov function as potential function.
arXiv preprint arXiv:1012.2721.
Yuan, R., Ma, Y ., Yuan, B., & Ao, P. (2011). Potential function in dynamical systems and the relation with lyapunov
function. In Proceedings of the 30th chinese control conference(pp. 6573‚Äì6580).
Yuan, R., Tang, Y ., & Ao, P. (2017). Sde decomposition and a-type stochastic interpretation in nonequilibrium processes.
Frontiers of Physics, 12(6), 1‚Äì9.
Zwanzig, R. (2001). Nonequilibrium statistical mechanics. Oxford University Press.
27
A PREPRINT - 5TH OCTOBER , 2021
10 Appendix
10.1 General Review of Assumptions Required for the FEP
Here we provide a general overview and short discussion of every assumption required at each stage of the FEP. Many
of these assumptions have more detailed discussions in subsections of this appendix. Ultimately, the overall picture
that emerges is that the FEP requires many assumptions to work, and it is unlikely that all of them can be fulÔ¨Ålled
by the kinds of complex self-organizing systems that the FEP ultimately ‚Äòwants‚Äô to be about ‚Äì such as biological self
organization and, ultimately, brains. However, this does not mean the FEP is useless as many of its assumptions may be
‚Äòapproximately‚Äô, or ‚Äòlocally‚Äô true over small enough time periods. This is not necessarily a bad thing ‚Äì almost all of the
sciences ultimately use simpliÔ¨Åed models to try to understand their ultimate objects of study in a more tractable way.
The FEP is simply continuing that tradition, but if we do this, we need to make explicit the key distinction between the
model and the reality or, more memorably, the map and the territory.
The Ô¨Årst set of key assumptions that the FEP makes comes through the deÔ¨Ånition of the kinds of stochastic dynamical
systems that it works with. SpeciÔ¨Åcally, we make the following assumptions about the form of the dynamics we deal
with,
‚Ä¢ The system as a whole can be modelled as a Langevin SDE of the form dx
dt = f(x) + œâ
‚Ä¢ The noise œâis Gaussian with 0 mean and a covariance matrix 2Œì.
‚Ä¢ The noise is additive to the dynamics
‚Ä¢ Œì does not change with time
‚Ä¢ Œì has no state dependence (no heteroscedastic noise)
‚Ä¢ Œì is a diagonal matrix (each state dimension has independent noise)
‚Ä¢ The dynamics f(x) do not themselves change with time.
We also must make the following assumptions about the system as a whole,
‚Ä¢ The system is ergodic, which means that state and time averages coincide or, alternatively, that there must be
some probability of ultimately reaching every part of the system from every other part.
‚Ä¢ The system possesses a well characterized non-equilibrium-steady state density (NESS), which does not
change over time
‚Ä¢ Once the system reaches this NESS density it cannot escape it ‚Äì there is no metastability or multiple competing
attractors. This does not mean, however, that the NESS attractor cannot be complex itself and can contain
limit cycles.
These assumptions setup the basic formalism we wish to consider. From here, we then apply the Ao decomposition to
rewrite the dynamics in the form of a gradient descent on the log of the potential function with dissipative and solenoidal
components f(x) = (Œì ‚àíQ)‚àáxln p‚àó(x). To be able to implement this decomposition requires,
‚Ä¢ The dynamics function f be smooth and differentiable,
‚Ä¢ There exists a steady state distribution p‚àó(x) to serve as the potential function.
Now, we apply the Markov Blanket conditions at the NESS density,
‚Ä¢ The state space x can be partitioned into a set of four states ‚Äì internal i, external e, active a and sen-
sory swhich, at the NESS density fulÔ¨Åll the following conditional independence relationships: p‚àó(x) =
p‚àó(e|s,a)p‚àó(i|s,a)p‚àó(s,a).
28
A PREPRINT - 5TH OCTOBER , 2021
‚Ä¢ We thus require all partitions to be at NESS, including the external states. This means that the environment
also has to be at steady state, not just the system.
‚Ä¢ We often assume no solenoidal coupling between internal and sensory states (internal states do not directly act
on sensory states ‚Äì only the external states do), nor between active and external states (active states drive the
external state but are not driven by it). Mathematically this corresponds to Qs,i = 0,Qe,a = 0.
Given the Markov Blanket conditions hold, we can then begin to move towards the free energy lemma. To begin with,
we must Ô¨Årst assume,
‚Ä¢ There is a unique argmax e,i exists for both internal and external states for every blanket state b.
‚Ä¢ That there exists a function œÉwhich maps from i to e
‚Ä¢ That œÉis invertible
‚Ä¢ That œÉis differentiable
‚Ä¢ For the particular free energy, we assume that the variational posterior q(e; e) is equal to the true posterior,
and thus that the true posterior can be represented by one sufÔ¨Åcient statistic (e).
These assumptions on œÉare quite restrictive. A more detailed discussion of what these assumptions require can be
found in Appendix 10.4.1.
Finally, to reach the free energy lemma, we must make the following assumptions,
‚Ä¢ The dynamics of the sufÔ¨Åcient statistic of external states e follows the same (Ao-decomposition) dynamics as
the external states themselves
‚Ä¢ The variational distribution q(e; e) is a Laplace distribution (Gaussian) with a Ô¨Åxed covariance Œ£ as a function
of e.
‚Ä¢ The variational covariance Œ£ is not an implicit function of the blanket states
This Ô¨Årst assumption has come under heavy controversy. These additional assumptions pertain to the Laplace approxi-
mation, but the Ô¨Ånal assumption here appears to go beyond what is typically required by variational Laplace.
10.2 Assumptions on the Form of the Langevin Dynamics
The FEP formulation makes reasonably strong assumptions about the nature of the dynamics that it models ‚Äì restricting
them to the form of stochastic dynamics which can be written as a Langevin equation with additive Gaussian noise. While
the assumptions on the dynamics function are not that strong, only requiring differentiability and time-independence,
the restrictions on the noise in the system are quite severe.
Firstly, it is important to note that using additive white noise, while a common modelling assumption due to its
mathematical simplicity, nevertheless imposes some restrictions on the kind of systems that can be modelled ‚Äì especially
as complex self organizing systems typically evince some kind of colored smooth noise, as well as often power-law
noise distributions which are associated with self-organized criticality (Ovchinnikov, 2016).
However, the further assumptions on the Œì covariance matrix ‚Äì that it is diagonal, state-independent, and time-
independent ‚Äì are also strong additional restrictions. SpeciÔ¨Åcally, this means that the noise to every dimension in the
system is completely independent of any other dimension, and that the noise is constant at every point throughout the
state space and throughout time.
10.2.1 Ito vs Ao vs A-type interpretation
A subtle point raised by Manuel Baltieri (private correspondence) is that the formulation of the FEP in K. Friston
(2019b) is inconsistent between a Stratonovich and an Ito interpretation of the relevant SDEs. This has no impact in the
29
A PREPRINT - 5TH OCTOBER , 2021
case of non-state-dependent noise, when the two interpretations coincide, but could potentially impact generalizations
of the theory to include state dependent noise.
10.2.2 Ergodicity and the Ao Decomposition
The Ao decomposition requires both that the dynamics possess a consistent non-equilibrium steady state density
(which forms the potential function) and also that the dynamics are ergodic. Additionally, this ergodicity assumption
is implicitly used in the Bayesian mechanics, which allows expectations of the surprisal to be taken and interpreted
as entropies, and thus to ultimately derive an interpretation of the dynamics in terms of accuracy and complexity. In
general, for many biological and self-organizing systems, ergodicity does not hold and such systems typically exhibit
substantial amounts of path dependence and irreversibility. This means that on a strict reading, for most systems the
FEP desires to model, the ergodicity assumption does not hold. However, it may still be possible to describe ergodicity
as holding ‚Äôlocally‚Äô in the small region of the state space around the NESS density and this may be sufÔ¨Åcient for
approximate version of the FEP to hold, although the resistance of the FEP to slight perturbations of its assumptions
remains unclear.
10.3 The Markov Blanket Condition
10.3.1 Functional vs Statistical Connectivity and the Markov Blanket Condition
A subtle conceptual issue has been recently raised by Aguilera et al. (2021) as to the precise meaning of the Markov
blanket condition for real systems. SpeciÔ¨Åcally, intuitively the Markov blanket is presented as being a kind of boundary
between the internal and external states, and is often presented as a literal boundary ‚Äì for instance the cell wall and
sensory epithelia of a bacterium, or the sensory epithelia of a brain as opposed to its external environment (Da Costa
et al., 2021; K. Friston, 2019b; Parr, Da Costa, & Friston, 2020). However, this intuition subtly conÔ¨Çates two types
of connectivity ‚Äì functional connectivity and statistical connectivity. Functional connectivity is the literal causal
connectivity in the world. For instance, to reach the inside of a bacterial cell, a molecule on the outside must pass
through the cell wall. This functional independence in this way can be represented by sparsity (zeroes) in the dynamics
matrix f. of the whole system. Statistical connectivity, on the other hand, is concerned only with the statistical
independencies between variables given the blanket states and is represented by sparsity in the Hessian of the NESS
density. Importantly, except under very restrictive conditions, it has been demonstrated by Aguilera et al. (2021) that
these two senses of connectivity are independent of one another ‚Äì in that statistical independence does not imply
functional separation and vice versa.
To see why this is the case, imagine the case of water molecules osmosing into the bacterial cell. Here, we assume,
there is no functional direct functional connection between inside and outside the cell wall ‚Äì i.e. water molecules
cannot ‚Äòteleport‚Äô from outside the bacterium directly into it. However, through this process of osmosis, the internal and
external states slowly become correlated with each other, as the concentrations of water molecules on both sides of
the boundary equalize, thus implying in this case that a functional separation does not necessarily imply a statistical
separation. Conversely, imagine taking the densities of two adjacent patches of gas as our variables. If this gas is
at thermodynamic equilibrium, then the density in the two patches will be, on average the same, however any tiny
Ô¨Çuctuations in density will be purely random and uncorrelated. Thus, even though these adjacent patches have a
functional connection (molecules can directly move from one patch to another), they lack a statistical connection.
An important note is that while the FEP generally bases its intuitions about the Markov Blanket in functional terms, the
actual mathematical deÔ¨Ånition is purely statistical. This can lead to confusion about what are the kinds of systems that
the FEP actually models and the dynamics they have.
In general functional connectivity expressly precludes statistical independence because, for any noise in the system,
as long as there is some causal path between two elements, even if they are separated by a blanket states, noise will
propagate through the system and thus internal and external will tend to become correlated with one another. Indeed,
30
A PREPRINT - 5TH OCTOBER , 2021
this development of correlations over time between internal and external states in the absence of functional connectivity
is precisely what we intuitively mean when we think about systems ‚Äòaccumulating knowledge‚Äô about their environments,
and thus seems important for questions about how systems can learn to model, predict, and infer various facts about the
external world.
However, as originally noted by Martin Biehl in private discussion, the Markov Blanket condition appears to explicitly
preclude this kind of knowledge accumulation inside the internal states, requiring that all knowledge be held in the
blanket states. It may indeed turn out to be the case that deÔ¨Åning Markov Blankets in the intuitive sense in terms of
functional independencies instead of statistical independencies may lead to a better description of precisely the kinds of
knowledge accumulating processes that appear to be important in performing inference.
10.3.2 The Real Constraints on Solenoidal Coupling?
While the Markov blanket conditions only explicitly disallow solenoidal coupling directly between the internal and
external states ‚Äì Qi,e = 0, the free energy lemma as stated in Equation 27 appears to require a signiÔ¨Åcantly greater
reduction of solenoidal coupling. SpeciÔ¨Åcally, the free energy lemma requires that, for a straightforward identiÔ¨Åcation
of the surprisal with the free energy, that the form of the dynamics for each marginal subset of states in the partition
take the same form as the dynamics of the full set of states x. SpeciÔ¨Åcally, this means that all solenoidal coupling
between the subsets must be suppressed, since if they were not then, by the marginal Ô¨Çow lemma, there would be
additional solenoidal coupling terms in Equation 27, which would complicate the relation to Free-energy minimization
with additional solenoidal terms. As such, for the free energy lemma, as currently presented, we appear to have the
extremely strong condition of the diagonality of Q, where each subset in the Markov Blanket is only allowed solenoidal
interactions with itself.
It is important to note that this restriction is signiÔ¨Åcantly stronger than those required just by the Markov Blanket
condition, and indeed is stronger even than the Ô¨Çow constraints proposed in K. Friston, Da Costa, and Parr (2020).
While this does not entirely rule out any interactions between different subsets of the Markov blanket, it does mean
that all interactions have to be mediated through the gradient term, since both the Œì and Qmatrices are assumed to be
diagonal.
However, it is important to note that if these stringent implicit assumptions on solenoidal coupling in Equation 27 were
relaxed, there would be additional solenoidal coupling terms in the equation. However, these terms would be orthogonal
to the gradient of the free energy, and thus not materially impact the ultimate minimum of the descent, although they
would alter the dynamics of actually reaching the minimum signiÔ¨Åcantly. SpeciÔ¨Åcally, this means that the FEP would
instead predict a solenoidal ‚Äòswirling‚Äô kind of gradient descent instead of a direct steepest descent for systems with
signiÔ¨Åcant solenoidal couplings.
10.4 Assumptions of the Free-Energy Lemma
10.4.1 The œÉfunction
The existence and general properties of the œÉfunction have also drawn much controversy from within the community.
SpeciÔ¨Åcally, it is not at all clear that this function exists in the general case, for arbitrary dynamics functions f and
conditional NESS distributions p‚àó(e|b) and p‚àó(i|b). In later papers it is assumed to exist under the condition of
injectivity between e and i. In effect, this means that there must be a unique mapping between e and i for all blanket
states ‚Äì i.e. that for every blanket state, if the argmax of the internal states is i, then the argmax of the external states
must be e. Additionally, there must be a corresponding (and separate) external argmax for every internal argmax. There
may, however, be some external argmaxes with no corresponding internal argmaxes (although the converse condition
does not hold). This requires that the dimensionality of the external states but greater than or equal to the dimensinoality
of the internal states ‚Äì which should generally hold for most reasonable systems where we can safely assume that the
environment is larger than the system itself. his injectivity condition also guarantees invertibility in the case that the
31
A PREPRINT - 5TH OCTOBER , 2021
internal and external state spaces are of the same dimension. It is also possible to use the Moore-Penrose pseudoinverse
for the case where the external state space is larger, at the cost of the free energy lemma becoming approximate instead
of exact.
The differentiability of the œÉfunction is a more stringent condition. In many cases this is unlikely to be met, since the
argmax functions which the œÉfunction maps between are generally nondifferentiable. It remains unclear to what extent
differentiable œÉfunctions can exist in systems of interest.
However, it can be straightforwardly shown that in the simple case of linear OU processes, that both the œÉfunction
exists and that it is analytically calculable (Aguilera et al., 2021; Da Costa et al., 2021). Moreover, in the general case, it
is possible to obtain an approximate œÉfunction by running a regression between the internal and external modes which
is often what is done in practice for nonlinear systems (K. Friston, 2013).
10.4.2 The Dynamics of the SufÔ¨Åcient Statistics e
An additional important assumption necessary for the free energy lemma, is that the dynamics of the sufÔ¨Åcient statistics
of the external mode follow the same dynamics as the external states generally ‚Äì see Equation 25. This assumption turns
out to be crucial to the free energy lemma which relies heavily in the fact that the dynamics of the sufÔ¨Åcient statistic e
can be written as a gradient descent on the log surprisal ‚Äì which can then be expressed in terms of a free energy under
the Laplace approximation.
This assumption is also problematic and has been the source of controversy within the community. The extent to which
this assumption is justiÔ¨Åed remains unclear. SpeciÔ¨Åcally, it appears to rule out the use of arbitrary functions Œæ(to be
discussed in the next section) to parametrize the external sufÔ¨Åcient statistic (although not the internal sufÔ¨Åcient statistic).
The assumption effectively holds to the extent to which one can describe the sufÔ¨Åcient statistic as equal to some external
state e(b) ‚âàe, which may occur often for the argmax but not necessarily always. It remains to be seen whether the
argmax is in fact the optimal such function ‚Äì which is dependent on the blanket, but which can identify a consistent eto
identify with and thus partake in the same dynamics.
Interestingly, one can also directly compute the dynamics of the sufÔ¨Åcient statistic e through the chain rule in terms of
the dynamics of the blanket states, which do follow the Ao decomposition. If we do this, we obtain,
Àôe(b) = ‚àÇe(b)
‚àÇb
db
dt
= ‚àÇe(b)
‚àÇb (Qbb ‚àíŒìbb)‚àábln p‚àó(i,s,a )
which is signiÔ¨Åcantly different from the required dynamics as it is a gradient descent on the blanket states brather than
the most likely external state. It is unclear under what conditions we should expect these dynamics to coincide.
10.4.3 Interpretation of Bayesian Inference Lemma in terms of Average Flows
An alternative interpretation of the Bayesian inference lemma, which solves some of the issues raised in the previous
section, while creating others, is that instead of interpreting the FEP as a statement about the dynamics of the average
(since for Gaussian systems the mode and the average coincide) but rather as a statement about the average of the
dynamics. On this view, the most likely state does perform a gradient descent on the free energy, but rather the states of
the system, on average, perform a gradient descent, or at least that the instantaneous Ô¨Çow of the syste is, on average,
pointed in the direction of the free energy gradient. This second interpretation is hinted at in (K. Friston, Da Costa, &
Parr, 2020) and has been developed more formally in private correspondence, as well as analysed in detail in Aguilera
et al. (2021). The general result is that while this new interpretation solves the issue of assuming unrealistic dynamics
for e, it raises two new issues.
The Ô¨Årst, more technical, concerns the œÉfunction which must now be redeÔ¨Åned to be a mapping between the average
Ô¨Çows, rather than between the modes of the internal and external states. Again, this can be achieved in linear Gaussian
32
A PREPRINT - 5TH OCTOBER , 2021
systems where an analytical solution can be found, but in more complex scenarios the existence and uniqueness of such
a mapping cannot be guaranteed. A second, more philosophical issue concerns what this statement about the average
dynamics means. SpeciÔ¨Åcally, the FEP can no longer be taken to offer even an interpretation of the dynamics of any
speciÔ¨Åc system. It can instead only offer an interpretation of the average behaviour over some counterfactual ensemble
of possible systems. Moreover, the average dynamics and the actually realized dynamics can diverge quite strongly in
real systems, and can even diverge for Gaussian linear systems, as demonstrated by (Aguilera et al., 2021). Sometimes,
even if a theory only deals in statistical averages, it can be highly fruitful scientiÔ¨Åcally. For instance, the theory of
evolution only ever makes statements about the average changes in populations, not about the behaviour of any speciÔ¨Åc
individual. However, it is unclear at present whether the revised FEPs statement that, on average, the dynamics of
a system tend towards a solenoidal gradient Ô¨Çow on the NESS density for that system and thus, on average, tend to
minimize variational free energy, is similarly scientiÔ¨Åcally fruitful.
10.4.4 Potential and Optimal ŒæFunctions
A further interesting question concerns the degree to which it is necessary to deÔ¨Åne the sufÔ¨Åcient statistics e and
i through the argmax over the conditional distribution over the external or internal states given the blanket. While
the assumption that the dynamics of e equal the dyanmics of emay impose some constraints for this function for e,
there are no such constraints in the deÔ¨Ånition of i(b), so we could, in theory use an arbitrary function i(b) = Œæ(b)
instead of the argmax. Indeed, we might desire to make this function contain as much information as possible about
the true conditional distribution of the internal states given the external states, so that when theœÉfunction maps this
to the sufÔ¨Åcient statistic of the external density it can be seen as performing inference with the most information
possible between the external and internal states. An additional beneÔ¨Åt of deÔ¨Åning an arbitrary function for Œæinstead of
using Œæ(b) = argmaxp(i|b) is that we can make Œædifferentiable, which alleviates much of the difÔ¨Åculty of making œÉ
differentiable as well.
While this approach brings many beneÔ¨Åts, it also has the drawback of the necessity to choose a suitable function Œæ
which introduces another degree of freedom into the modelling process. One possible condition is that we could chose
the optimal Œæto be the one that contains the most information about the internal state or, alternatively minimizes the KL
between the approximate conditional distribution over the internal states parametrized vy Œæand the true conditional
over the blanket states. That is, we could deÔ¨Åne,
Œæ‚àó= argmin
Œæ
DKL[q(i; Œæ(b))||p(i|b)]
This would reduce the number of degrees of freedom of Œæand provide a valid modelling target, although the actual
computability of this minimization process is potentially a problem, as is whether this objective is actually optimal.
Nevertheless, the use of an arbitrary Œæ function for the sufÔ¨Åcient statistics of the internal states may yet resolve or
ameliorate some of the difÔ¨Åculties with the free energy lemma, and is an interesting inroad to begin understanding
various relaxations or extensions to the current incarnation of the free energy principle.
10.4.5 FEP as a locally valid theory
Overall, we have seen that the derivation of the FEP requires many fairly restrictive assumptions ‚Äì Ô¨Årst in the formulation
of the kinds of systems that the FEP applies to (autonomous Langevin equations with state-independent diagonal
additive white Gaussian noise), and secondly in the formulation of the free energy lemma (no solenoidal coupling
between subsets, existence and differentiability of the œÉfunction, the dynamics of e being the same as the dynamics of
e), as well as
Beyond the purely technical considerations outlined above, there are also crucial, and more intuitive issues which arise
in the modelling of biological systems or indeed complex self-organizing systems are the assumptions of ergodicity
and the NESS density including the environmental states. Both of these assumptions, when taken literally, are almost
33
A PREPRINT - 5TH OCTOBER , 2021
always false. Most interesting systems which self-organize are, almost tautologically, non-ergodic, in that they exhibit a
high degree of path-dependence and never come close to exploring their full state space. Secondly, the requirement that
all subsets of the markov blanket and the system be at non-equilibrium steady state requires that the external states,
which are typically taken to be the environment also be at steady state for the FEP to apply. This condition intuitively
does not intuitively hold for general cases of biotic self organization. For instance, we typically consider the system as
maintaining itself in steady state against environmental Ô¨Çuctuations, not that the environment itself is deÔ¨Åned to be in
steady state with us. In the most obvious case where we deÔ¨Åne the environment to be the whole universe outside of the
system, this is deÔ¨Ånitely false. If we take a more local approach and deÔ¨Åne the environment to be some small ‚Äòbubble‚Äô
around the system, and then simply model the rest of the universe as Gaussian Ô¨Çuctuations impinging on this bubble,
then this assumption may be tenable in some cases, but it nevertheless fails to match our intuitions about real biological
systems ‚Äì such as animals. For instance, it may be the case that my (human) body largely maintains its own homeostatic
steady state against external Ô¨Çuctuations, however it is deÔ¨Ånitely not the case that my environment itself is at steady
state at all times ‚Äì for instance I can get up and go for a walk, or Ô¨Çy around the world for a conference ‚Äì and none of this
should disrupt the internal homeostatic steady state of my body. This is the real question which is what the FEP tries (at
least in its intuitive sales pitch) to answer ‚Äì how can I maintain an internal steady state against an environment which is
not at steady state. By assuming that the external states are also at the steady state, it may be that the FEP is, in some
sense, answering the wrong question and is, in the process, assuming away the true difÔ¨Åculty in answering the right one.
In general, one response to these general issues is to argue that the assumptions of the FEP do not need to hold globally
only locally over some relevant timescale, which is a fair point. This idea of the FEP as a theory which only holds
locally is valid (although it still needs to be empirically or mathematically shown whether such assumptions such as
environmental steady state and ergodicity do in fact hold locally over relevant timescales), and has close analogies
in mathematics and physics where, for instance, linearity assumptions are often used which are only actually true
in the inÔ¨Åntesimal limit. In general, it is likely that, in fact, the FEP is, in this sense, a local theory, and should be
thought of as such. This locality should then inform modelling work, as well as spur new theoretical advances as to the
range of conditions over which the FEP is valid. Understanding and precisely quantifying the limitations and region of
applicability of the theory is ultimately vital for obtaining true understanding, and is a very important area for future
empirical work within the FEP paradigm.
11 Mathematical Appendices
11.1 Helmholtz Decomposition at Steady State
It is straightforward to verify that the Helmholtz decomposition of the dynamics satisÔ¨Åes the steady state condition
dp‚àó(X)
dt = 0 by plugging this form into the Fokker-Planck equation (Equation 2),
dp‚àó(x)
dt = ‚àí‚àáx
[
(Œì ‚àíQ)‚àáxln p‚àó(x)
]
p‚àó(x) + Œì‚àá2
xp‚àó(x)
= ‚àí‚àáx
[
(Œì ‚àíQ)‚àáxp‚àó(x)
p‚àó(x)
]
p‚àó(x) + Œì‚àá2
xp‚àó(x)
= ‚àí‚àáx
[
(Œì ‚àíQ)‚àáxp‚àó(x)
]
+ Œì‚àá2
xp‚àó(x)
= ‚àíŒì‚àá2
xp‚àó(x) + ‚àáxQ‚àáxp‚àó(x) + Œì‚àá2
xp‚àó(x)
= ‚àáxQ‚àáxp‚àó(x) = 0 (33)
Where the last line follows because, by deÔ¨Ånition, the gradient of the solenoidal Ô¨Çow with respect to the gradient of the
log density is 0, since the solenoidal Ô¨Çow must be orthogonal to the gradient of the density, which is represented by the
solenoidal Qmatrix being antisymmetric Q= ‚àíQT.
34