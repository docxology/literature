An analytical model of active inference in the
Iterated Prisoner’s Dilemma
Daphne Demekas∗1,2, Conor Heins†1,3,4,5, and Brennan Klein‡1,5,6
1Network Science Institute, Northeastern University, Boston, Massachusetts, USA
2Wheeler Lab, University of Arizona, Tucson, Arizona, USA
3Department of Collective Behaviour, Max Planck Institute of
Animal Behavior, 78464 Konstanz, Germany
4Department of Biology and the Centre for the Advanced Study of Collective Behaviour,
University of Konstanz, 78464 Konstanz, Germany
5VERSES AI Research Lab, Los Angeles, CA 90016, USA
6The Institute for Experiential AI, Northeastern University, Boston, Massachusetts, USA
August 31, 2023
Abstract
This paper addresses a mathematically tractable model of the Prisoner’s Dilemma
using the framework of active inference. In this work, we design pairs of Bayesian
agents that are tracking the joint game state of their and their opponent’s choices in an
Iterated Prisoner’s Dilemma game. The specification of the agents’ belief architecture in
the form of a partially-observed Markov decision process allows careful and rigourous
investigation into the dynamics of two-player gameplay, including the derivation of
optimal conditions for phase transitions that are required to achieve certain game-
theoretic steady states. We show that the critical time points governing the phase
transition are linearly related to each other as a function of learning rate and the
reward function. We then investigate the patterns that emerge when varying the agents’
learning rates, as well as the relationship between the stochastic and deterministic
solutions to the two-agent system.
1 Introduction
Studies of behavioural science, be it in biology, psychology, or machine learning, often rely
on the concept of rational thinking and decision making [1–4]. Game theory has had wide
∗daphnedemekas@arizona.edu
†cheins@ab.mpg.de
‡b.klein@northeastern.edu
1
arXiv:2306.15494v2  [physics.soc-ph]  29 Aug 2023
success in precisely formulating contexts in which players or agents are challenged to converge
to an optimal yet counter-intuitive strategy that maximises reward. In particular, game
theory models communication among agents that can result in bounded-complex emergent
behaviour [5, 6]. The Iterated Prisoner’s Dilemma (IPD) is a quintessential game, in which
the ‘dilemma’ is that the highest reward is attributed to the action of defection, but the
optimal behaviour in the long run is to cooperate, because of the ‘Shadow of the Future’
phenomenon [7]1. When played iteratively, agents learn each other’s predictable behaviour
and can form an optimal strategy, away from the Nash equilibrium of the one-shot game.
To do so, agents need to be aware of what their opponent is likely to do, which is why the
IPD is widely used to study the evolution of cooperation for selfish agents [9].
This work addresses a computational model of the (memory-one) Iterated Prisoner’s
Dilemma under the framework of active inference (AIF) [10–12]. AIF is an agent-based
modelling framework derived from theoretical neuroscience, where cognitive processes like
action, perception, and learning are seen as solutions to an inference problem. As an explic-
itly model-based, Bayesian framework for simulating behaviour, AIF provides cognitively
‘transparent’ agents, whose posterior beliefs about the world and associated uncertainties
are accessible and interpretable. This enables careful investigation into the Bayesian basis of
behaviour in these simple models, in turn allowing us to identify the conditions under which
optimal behaviour is possible.
When two identical and deterministic AIF agents play against one another, we show that
the equation governing across-trial learning dynamics is mathematically tractable given one
approximation. This enables us to derive functions that model the specific conditions under
which convergence to an optimal strategy—namely the Pavlov Strategy [9]—for the IPD can
occur, given a multi-agent AIF model. The Pavlov strategy is win-stay-lose-change, where
agents will cooperate if the agent’s and opponent’s moves are the same in the previous round
and defect otherwise. We explore how these dynamics vary across different configurations of
the agents’ learning rates, as well as how stochasticity in the agent network determines the
probabilities of agents reaching the optimal outcome.
1.1 Iterated Prisoner’s Dilemma
In the Prisoner’s Dilemma, at each round, both players can either defect or cooperate, leading
to 4 possible outcomes [8] (see Table 1 with different reward levels). The outcome with the
highest reward is if the player defects and its opponent cooperates (DC), which is also the
outcome with the lowest reward for the opponent (CD). The second-best outcome is if both
cooperate (CC), and the third-best outcome for both players is if they defect (DD). In this
model, the four reward levels are respectively [3,1,4,2]. This work specifically models the
memory-one IPD, where each player only considers the previous move of their opponent
when making their decision for the current round.
There are several notable strategies in the IPD, which have been categorised in different
1This is when agents in repeated play—without awareness of when the play will end—will be more
cooperative because they are made to learn about the possibility of being punished and plan accordingly [8].
2
Player 2
Cooperate (C) Defect ( D)
Player 1 C (3, 3) (1, 4)
D (4, 1) (2, 2)
Table 1: Example payout matrix in a Prisoner’s Dilemma game.
ways [13]. First, a dominant strategy produces the best possible payoff for an agent, re-
gardless of the strategies used by opponents. The most commonly cited dominant outcome
is when both players defect (choose to betray) in every round. From an individual player’s
perspective, defecting in every round provides a higher immediate payoff compared to coop-
eration, especially when the other player cooperates. However, defecting in every round is
not socially optimal as it leads to a lower overall payoff compared to mutual cooperation.
The challenge is to find strategies that can foster cooperation and lead to better outcomes
for both players in the long run, rather than succumbing to the dominant outcome of mutual
defection [14].
In order to reach the social optimum of cooperation, new heuristics or bounds on the
agents need to emerge in order for them to look beyond the reward function when deciding
their actions. This makes the IPD a good arena to study bounded rationality, in which
agents do not have access to the full generative process (encompassing both themselves
and their opponent), and therefore must make decisions given a bound on their awareness
or knowledge, of, for instance, the other player, or any external environmental factors.[4].
Agents playing the IPD has been studied in the context of reinforcement learning already
[15, 16], and the idea of bounded rationality serves as a motivation for using active inference
agents to model the IPD, as the AIF is a transparent and interpretable framework in which
agents infer actions and quantify uncertainty under the constraints of their generative model.
Thereareseveralwaystotraintheagentstoconvergetothesocialoptimum, whichwewill
refer to as the cooperative steady state. When agents sample their actions deterministically,
our model shows that active inference agents parameterised with a constrained set of learning
rates can converge to the cooperative steady state by learning the Pavlov Strategy [9], and
it also demonstrates learning rate configurations that get trapped in the Nash equilibrium,
in which agents converge to Unconditional Defection [17, 18].
1.2 Active inference
Activeinference(AIF)agentsareabletoplanandlearnabouttheirstatespaceandtransition
probabilities through observed experience. They infer which actions to take by minimising
the expected free energy anticipated to accrue from their actions [12]. This often allows these
agents to solve complex tasks often seen in reinforcement learning or neuroscience, such as
the Multi-Armed Bandit [19] and other Monte-Carlo based tasks [20].
Advances in the ability to quickly build and scale models of AIF agents, particularly in
3
Variable Name Notation
Hidden States s ∈ {CC, CD, DC, DD}
Observations o ∈ {CC, CD, DC, DD}
Actions u ∈ {uC, uD}
Observation Model P(ot|st; A) =Cat(A)
Transition Model P(st+1|st−1, ut−1; B) =Cat(B)
Transition Model Parameter P(B) =Q
ju P(B•ju), P (B•ju) =Dir(b•ju)
Initial State Prior P(s1; D) =Cat(D)
‘Biased’ State Prior (Reward) ˜P(s; C) =Cat(C), s.t. ln C = [3, 1, 4, 2]
Table 2:Generative model variables and notation.
Python using thepymdp library [21], have allowed for a much more scalable and accessible
means to model these agents in different and flexible environments, as well as to connect them
in networks and allow them to observe each other’s actions. This has allowed researchers
to ask more interesting questions about how relevant AIF is in terms of modelling rational
decision making, such as those observed in game theory. In this paper, we show that not
only can AIF agents effectively learn optimal strategies to the IPD, but the framework of
active inference enables us to derive the exact conditions for when this will occur and have
a layered understanding of the agents’ ‘mental process’ throughout the game.
The agents in this model actively entertain beliefs about the dynamics of the game
and iteratively update their beliefs about the game dynamics (i.e., a ‘transition model’) as
they play multiple rounds against their opponent. In the context of the discrete-time and
-space models used in the present work, this amounts to updating the elements of transition
probability matrices that represent each agent’s beliefs about game states from one trial
to the next. After every trial of iterated play, the agents update these state transition
probability distributions based on their actions and the outcomes that they observed. In
doing so, the agents have the capacity to learn strategies, manifested as patterns of learned
probabilities of transition from each state to each other state.
Our hypothesis is that throughout iterative play, the bounded-rational agents will learn
to infer actions based on learned patterns of their opponent’s behaviour (i.e., the ability
to predict revenge from defection), and this will result in a strategy leading to the social
optimum steady state in which both agents cooperate. Further, given the interpretability of
the AIF, we will be able to analytically derive the process that the agents undergo during
this learning process and thus predict how it might change with different parameters.
2 Simulation Dynamics
Here, we explore the long-term dynamics of the IPD. Agents play in turns for a finite set
of trials, updating their transition model beliefsQ(B; ϕb) at each trial. Unless otherwise
4
Figure 1: Beliefs about transition probabilities over trials. Top:A representation
of Player 1’s beliefs at three phases of the simulation(t = 10, 20, 150). Each box con-
tains a graph representation of the transition probabilities, and histograms of the cooperate-
conditioned (top row) or defection-conditioned (bottom row) transition distributions at the
displayed trial indices. Darker values represent a higher probability.Bottom: The inferred
probabilities of cooperation in each trial. Agents select the action with the highest posterior
probability. The agents begin by continuously defecting, then undergo an oscillatory period
of defection and cooperation, and eventually reach a cooperative steady state. After this
period of training, they will have learned the Pavlov strategy, i.e. they will cooperate if the
agent’s and opponent’s moves are the same in the previous round and defect otherwise [9].
specified, agentsareconfiguredexactlythesame(samepriors, samelearningrate)andsample
their actions deterministically as described in (A.21). In this model, agents always converge
to the cooperative steady state and remain there indefinitely. The magnitude of the learning
rate η affects the rate of convergence by scaling the update to the transition matrix at each
timestep, as shown in (A.25). In Figure 1 we show the simulation dynamics for agents
configured with learning rateη = 0.3, but it’s important to note that at different learning
rates, the nature of these dynamics would not change - rather the critical time points would
only occur either sooner (for largerη) or later (for smallerη). Therefore, the amount of time
taken in order to converge is not representative of the performance of this model, but rather
a parameter that can be tweaked. Given the transparency of this deterministic system, it is
possible to explain exactly how these agents are ‘thinking’, given their posteriors over time.
Agents are initialised with uniform transition matrices as in (A.7). Upon the first obser-
vation, they infer the game state and calculate the expected free energies (EFEs, orG) of
cooperating and defecting. They take the action that has smaller EFE, i.e.,arg minu G0(u).
At first, because of the reward parameterization and the uniformity in the transition prior
P(B; b), defection will minimise the EFE (i.e., predicts the highest reward), according to:
5
G0(u = C, ϕC) =−(BC
0 · ϕC
0 ) · (ln BC
0 · ϕC
0 − ln C) =1
2 ln(C1C2) − ln 1
2 (1)
G0(u = D, ϕD) =−(BD
0 · ϕD
0 ) · (ln BD
0 · ϕD
0 − ln C) =1
2 ln(C3C4) − ln 1
2 (2)
Therefore, aslongas ln(C3C4) < ln(C1C2), theagentalwaysdefectsonthefirsttimestep.
Agents will then continue to defect, because the expected reward from realising the state
DC still outweighs that of any other predicted state. As the agents continue to defect, their
beliefs aboutP(st = DC|st−1 = DD, u= D)will be decreasing with a proportional increase
in P(st = DD|st−1 = DD, u= D), meaning G(u = D) will increase as the probability of
getting their desired reward decreases.
At a critical time, which we denoteτ12, the agents will begin assigning more probability
to cooperation than defectionϕC > ϕD, because the transition probabilities have decreased
sufficiently for the EFE of cooperation to outweigh that of defection. Once the agents begin
cooperating, they undergo an oscillatory period during which their actions fluctuate from
cooperation to defection. This is because atτ1, the transition probabilitiesP(st|st−1 = CC)
are fixed at their initial value, since the agents have yet observed the previous state being
CC. Thus the agents will still be optimistic about realising the highest reward state DC via
the transition probabilityP(st+1 = DC|st = CC, u= D).
The agents will eventually learn that inferring to defect will inevitably lead to observing
DD, and inferring to cooperate will inevitably lead to CC. The oscillatory period is crucial
to this because it teaches the agent that defecting in response to cooperation will only ever
lead to DD. The oscillation continues until the critical time pointτ2, in which the probability
p(st+1 = DC|st = CC, ut = D) becomes smaller than p(st+1 = DD|st = CC, ut = D), at
which point the agents will cooperate for all remaining rounds.
2.1 The analytic transition function
In the above model of AIF agents, an analytic solution for the evolution of each agent’s
beliefs about the transition likelihoodQ(B; ϕ∗
b) is available. This is formulated by deriving
approximations toτ1—the critical trial in which the agents transition to an oscillatory period
between defection and cooperation—andτ2, the second phase transition in which the agents
converge to the cooperative steady state. Given the expressions forτ1 and τ2 in (3), we can
writedowntheevolutionoftheDirichletparametersofthetransitionprobabilitymatrix. The
derivations for the following expressions are in the Appendix (A.4, A.5). Here,C corresponds
to the ‘biased’ state reward prior, and each entry ofC corresponds to the reward value of
that observation(rCC, rCD, rDC, rDD). For the full definition see (A.5).
τ1 ≈ R1(β)
η τ2 ≈ R2(β)
η (3)
2whose solution in terms of generative model parameters we derive in the next section.
6
where
R1 = 2
ln C3
C4
+ 2−
q
(ln C4
C3
− 2)2 − 8(−ln C4
2√C1C2
− 1
5 )
− 1 R2 = 3
2R1 (4)
This means thatτ2, the number of trials it takes the system to reach the steady state,
can be precisely approximated as a linear function ofτ1, the number of trials it takes to
start the oscillatory period (see Figure 3)—i.e. the critical time points governing the phase
transition are linearly related to each other as a function of learning rate and the reward
function. Given these expressions, our analytic form of the transition rule for the posterior
Dirichlet parameters over the transition model is:
ϕC
bt+1 =



bC
0
bC
0 + η
2 sCC ⊗ sDD(t − R1
η )
bC
τ2 + ηsCC ⊗ sCC(t − R2
η )
ϕD
bt+1 =



bD
0 + ηsDD ⊗ sDDt t <R1
η
bD
τ1 + η
2 sDD ⊗ sCC(t − R1
η ) R1
η < t <R2
η
bD
τ2 t >R2
η
(5)
which can be used to exactly replicate the trajectory ofQ(st+1|st, ut) over time (Figure 3).
We conclude by noting that the agents in this model, after undergoing these two phase
transitions and converging to CC, have learned the well-known Pavlov (also known as the
“Win-Stay Lose-Shift”) strategy from IPD literature [9]. Agents learned during0 < t < τ1
that given the observation DD, the best strategy is to cooperate, and duringτ1 < t < τ2
they learned that cooperating is the best outcome given the observation CC—therefore,
having reachedτ2, they continue cooperating. To show that the agents learned the Pavlov
strategy, we performed an experiment where once an agent converged to the steady state, we
disabledadditionallearningandhadthisagentplayagainstanagentthatbehavescompletely
randomly. When playing against this random agent, they observe the new asymmetric states
DC or CD. The desire to maximise expected utility (via the drive to minimise KL risk, a.k.a.,
the expected free energy) will lead them to perform the ‘greedy’ strategy of defection, which
is how their behaviour is consistent with the Pavlov strategy3. Future work will further
characterise the space of learnable strategies under this framework.
3 Generalizing the model
In the previous section, we found an approximate solution for the belief-, action-, and
learning-dynamics, which completely describes the case of two symmetrically-parameterised
agents playing IPD. For any given parameterisation of the prior preferencesC, we derived
the trials at which the critical transitions take place in the two-agent system, steering it
away from the Nash equilibrium and towards the cooperative steady state.
3An agent exhibiting the Pavlov strategy will only cooperate if in the previous trial, both agents performed
the same action (i.e., the state was either CC or DD, otherwise they will defect).
7
Figure 2: Marginalised transition probabilities under differentη. The dotted lines
represent the marginalised probabilities from all states to the highest reward state DC, and
the solid lines represent the marginalised probabilities from all states to the socially optimal
state CC. The transition probabilities to DC decrease initially during the period of defection,
then fluctuate during the period of oscillation and steady out close to 0 once the agents reach
the cooperate steady state, and the probabilities to state CC take the same pattern in the
opposite direction. This happens more rapidly for larger η, because the updates to the
parameters of the transition likelihood distribution are larger at every trial.
Thesimplicityofthismodelisthattheseagentsareconfiguredexactlyalike, andtherefore
there is complete symmetry in the state space. This means that the agents will only ever
observe two out of four possible states in the space. However, this case no longer holds when
either the agents are parameterised with different learning rates, or when they sample their
actions stochastically, according to (A.22). These cases open the space of possible strategies
that the agents can learn, some of which will lead the agents to fall into the Nash equilibrium,
and others which will allow them to reach the optimal outcome.
3.1 Different learning rates
We now assume agents parameterised with differentη and the sameβ, performing actions
deterministically. We denote the agent with larger η1 as a1, and the agent with smaller
η2 as a2. According to (A.38), the critical valueτ1 depends on η, and sinceη1 > η2, this
means τa1
1 < τa2
2 . Thus,a1 will cooperate atτa1
1 = R1
η1
, buta2 will not yet deem cooperation
a better policy than defection (namely, the EFE of defection will remain below that of
8
Figure 3: Simulated vs. derived relation between reward and learning rate.Sim-
ulated and approximatedτs for three values ofβ parameterizing the reward function. On
the left, we approximateτ1 with the equationτ1 = R1
η where R1 depends on the reward pa-
rameter ofβ. On the right, we approximateτ2 with τ2 = τ1 + 1
η R2 where again,R2 depends
on β. With a largerβ, meaning a higher predicted reward for the state DC, the values of
τ increase as it will take more trials for the players to update their transition probabilities
away from having a preference to defect.
cooperation). Therefore, at τa1
1 , the game state will be CD froma1’s perspective and DC
from a2’s perspective. This symmetry-breaking means that the system will not enter into
the typical oscillation phase triggered by mutual cooperation (as is guaranteed whenη1 = η2
and thusτa1
1 = τa2
1 ).
The nonidentical observations imply that afterτa1
1 , a1 believes P(st+1 = CD|DD) is more
probable, thereby being disincentivised to continue cooperating, anda2 believes P(st+1 =
DC|DD) is more probable, being incentivised to continue defecting. The degree of disincen-
tivisation (or incentivisation) will increase in proportion toη1 orη2, respectively, due to a cor-
responding η1-scaled increase inGa1 (u = C) and anη2-scaled decrease inGa2 (u = D). This
growing asymmetry in the agents’ beliefs means that (5) no longer holds. At this point, the
agents will return to continuous defection until another instance ofG(u = D) =G(u = C)
occurs; the duration of this depends onη.
In sum, the conditions under which the joint-agent system converges to the optimal
steady state is determined by whether or not the agents’ learning rates are configured such
that there will be some time pointt less than some thresholdTmax in which both agents
cooperate simultaneously. If this is not the case, then as defection continues, the rate of
increase of G(u = D) slows, and after a certain amount of time (governed byη) it will
become too slow and never catch up toG(u = C) (see Figure 4). In other words, if at any
point, for either agent,Gt(u = D) < Gt(u = C) ∀t ∈ (0, Tmax], the agents are trapped in
9
Figure 4: Relative value of cooperation under different η parameterisations.
Above: Agents are configured with ηs along the tendrils of Figure 5. On the left, the
relative values of cooperation, calculated asG(u = C) −G(u = D), reach zero several times
and converging around 0.75 at the optimal outcome. On the right: the fluctuations in the
individual EFEs. There are periods beforeτ1 and betweenτ1 and τ2 in which one player will
cooperate and the opponent defects; this creates the spikes in the distribution, as one agent
is punished and the other is rewarded.Below: Agents withηs that are not on the tendrils
in Figure 5, meaning that they do not converge to the cooperative steady state. We can see
on the left howGt(u = D) is converging to something less thanGt(u = C).
the Nash equilibrium.
Figure 4 shows EFE trajectories in scenarios where agents converge to the optimal out-
come (above) and where agents get trapped in the Nash equilibrium (below). Convergence
to the Nash equilibrium occurs in the absence of any trial where the relative value of coop-
eration reaches 0 simultaneously for both agents. Instead, the relative values of cooperation
slowly converge to different and nonoverlapping limits4. If the intersection of the condition
in (A.31) does occur, this guarantees that the agents will begin the oscillatory period which
will eventually lead them to convergence to CC (while there may be some instances of CD
and DC in the oscillatory period, this will not prevent eventual cooperation). In general,
when learning rates are close together, the likelihood of convergence to CC is more likely;
however, the actual pattern is more complicated than this. Figure 5 demonstrates the com-
4Note that even after the agents reach a cooperative steady state, the difference in expected free energy
takes time to flatten because the entropy is still decreasing as beliefs become more precise, via learning.
10
Figure 5: Parameter sweeps overη. Top row:Agents sample actions deterministically.
Wherever the average cooperation is nonzero, agents converged to the cooperative steady
state—yellow cells indicate faster cooperation, which is generally associated with higher
overall reward.Bottom row:Agents sample actions stochastically. Cooperation still occurs
most often along the diagonal, tapering off as learning rates become more different.
plex pattern of instances in which the agents converge to the cooperative steady state given
different learning rate combinations, with both the deterministic and stochastic sampling.
3.2 Stochastic sampling
Here, we introduce noise in the action selection such that agents sample actions with some
probability proportional to their (negative) EFE. Action stochasticity can be controlled with
an inverse temperature parameterα according to (A.22). In general, all of the principles
outlined in Section 2 remain; however, now the agents will sometimes perform the suboptimal
11
action. This enables agents to experience the entire state space (different combinations of
defection and cooperation) and therefore estimate transitions between all the combinations
of states.
We can see from Figure 5 that, on average, endowing the agents with stochasticity enables
them to converge to the cooperative steady state for a larger number of combinations of
different learning rates. This makes sense, because it increases the likelihood of ‘escaping’ the
pattern of continuous defection, and therefore learning about the advantages of cooperation.
In terms of the reward, the agents that have most similar learning rates will behave most
similarly and therefore accumulate more reward (along the diagonal).
4 Conclusion
Iterated Prisoners’ Dilemma games have long been the test bed for new developments in
behavioural science and game theory. Because of the relative simplicity of the game’s
structure—and its, at times, surprising experimental results—researchers often use it to de-
velop mathematical frameworks for understanding decision making in social or multi-agent
contexts. In this paper, we demonstrated how active inference can be used to model the
IPD transparently, such that in a simple set-up, we can derive a solution to the evolution of
the agents’ beliefs about the game dynamics, i.e., the transition probabilities. This allows
us to quantitatively reason about why the agents converge to their chosen optimal strategy
and how behaviour changes as a function of different learning rates and stochastic action
selection. While the simple case of similarly-configured agents resulted in both agents ex-
hibiting the Pavlov strategy, once we introduce asymmetry in the generative models, and/or
stochasticity in action sampling, then upon testing, agents are able to learn a variety of
different strategies, including the Pavlov strategy, Unconditional Defection, Unconditional
Cooperation, and Tit for Tat—or some variation of Tit for Tat [22, 23].
Thisfindingisastartingpointforfuturework, inwhichsuchamodelcouldbeextendedto
multiple agents interacting towards a common goal, and investigating the various strategies
that emerge from acting in a network order to minimise free energy. The current model
did not incorporate the information-seeking components that are often leveraged in action-
selection under active inference [24]. In our case, the ambiguity term of the expected free
energy was zero by construction (due to zero observation uncertainty), but future work
could explore the role of parameter information gain (resolving uncertainty aboutB) and
how that changes the multi-agent dynamics in IPD. Overall, in this work we demonstrated
that AIF can offer game theory a novel analytic transparency and simplicity for accounting
for multi-agent dynamics using a first-principles, Bayesian account.
Acknowledgements: The authors thank Wolfram Barfuss and Christoph Riedl for valu-
able feedback and comments that substantially improved the quality of the manuscript.
Funding information: DD, CH, & BK acknowledge the support of a grant from the John
Templeton Foundation (61780). The opinions expressed in this publication are those of the
authors and do not necessarily reflect the views of the John Templeton Foundation.
12
References
[1] Robert Axelrod and William D. Hamilton. “The Evolution of Cooperation”. In:Science
211.4489 (1981), pp. 1390–1396.doi: 10.1126/science.7466396.
[2] Martin A. Nowak. “Five rules for the evolution of cooperation”. In:Science 314.5805
(2006), pp. 1560–1563.doi: 10.1126/science.1133755.
[3] Martin Nowak and Karl Sigmund. “A strategy of win-stay, lose-shift that outperforms
tit-for-tat in the Prisoner’s Dilemma game”. In:Nature 364.6432 (1993), pp. 56–58.
doi: 10.1038/364056a0.
[4] Herbert A. Simon. “Bounded Rationality”. In: Utility and Probability. Ed. by John
Eatwell, Murray Milgate, and Peter Newman. The New Palgrave. London: Palgrave
Macmillan UK, 1990, pp. 15–18.isbn: 978-1-349-20568-4.doi: 10.1007/978-1-349-
20568-4_5.
[5] Mazui A. Niazi Aisha D. Farooqui. “Game theory models for communication between
agents: a review”. In:Complex Adaptive Systems Modeling13 (2016).doi: 10.1186/
s40294-016-0026-7.
[6] Jeromos Vukov, György Szabó, and Attila Szolnoki. “Cooperation in the noisy case:
Prisoner’s dilemma game on two types of regular random graphs”. In:Physical Review
E 73 (6 2006), p. 067103.doi: 10.1103/PhysRevE.73.067103.
[7] Jan B. Heide and Anne S. Miner. “The shadow of the future: Effects of anticipated
interaction and frequency of contact on buyer-seller cooperation”. In:The Academy
of Management Journal 35.2 (1992), pp. 265–291.url: https://www.jstor.org/
stable/256374.
[8] Steven Kuhn. “Prisoner’s Dilemma”. In:The Stanford Encyclopedia of Philosophy. Ed.
by Edward N. Zalta. Winter 2019. Metaphysics Research Lab, Stanford University,
2019. url: https://plato.stanford.edu/archives/win2019/entries/prisoner-
dilemma/.
[9] Velumailum Mohanaraj Martin Dyer. “The Iterated Prisoner’s Dilemma on a Cycle”.
In: arXiv (2018). doi: 10.48550/arXiv.1102.3822.
[10] Maxwell J.D. Ramstead, Dalton A.R. Sakthivadivel, Conor Heins, Magnus Koudahl,
Beren Millidge, Lancelot Da Costa, Brennan Klein, and Karl J. Friston. “On Bayesian
mechanics: A physics of and by beliefs”. In:Interface Focus13.3 (2023), p. 20220029.
doi: 10.1098/rsfs.2022.0029.
[11] Conor Heins, Brennan Klein, Daphne Demekas, Miguel Aguilera, and Christopher L.
Buckley. “Spin glass systems as collective active inference”. In:Active Inference. Ed. by
Christopher L. Buckley, Daniela Cialfi, Pablo Lanillos, Maxwell Ramstead, Noor Sajid,
Hideaki Shimazaki, and Tim Verbelen. Communications in Computer and Information
Science. Cham: Springer Nature Switzerland, 2023, pp. 75–98.doi: 10.1007/978-3-
031-28719-0_6.
13
[12] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior. MIT Press, 2022.
[13] Steven Kuhn. “Strategies for the Iterated Prisoner’s Dilemma”. In:The Stanford En-
cyclopedia of Philosophy. Ed. by Edward N. Zalta. Winter 2019. Metaphysics Research
Lab, Stanford University, 2019. url: https : / / plato . stanford . edu / entries /
prisoner-dilemma/strategy-table.html.
[14] Charles A. Holt and Alvin E. Roth. “The Nash equilibrium: A perspective”. In:Pro-
ceedings of the National Academy of Sciences101.12 (2004), pp. 3999–4002.doi: 10.
1073/pnas.0308738101.
[15] Robert H. Crites Tuomas W. Sandholm. “Multiagent reinforcement learning in the
Iterated Prisoner’s Dilemma”. In: Biosystems 37 (1996), pp. 147–166. doi: https :
//doi.org/10.1016/0303-2647(95)01551-5.
[16] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. “Online learning in Iterated
Prisoner’s Dilemma to mimic human behavior”. In:PRICAI 2022: Trends in Artificial
Intelligence. Ed. by Sankalp Khanna, Jian Cao, Quan Bai, and Guandong Xu. Lecture
Notes in Computer Science. Cham: Springer Nature Switzerland, 2022, pp. 134–147.
doi: 10.1007/978-3-031-20868-3_10 .
[17] William H. Press and Freeman J. Dyson. “Iterated Prisoner’s Dilemma contains
strategies that dominate any evolutionary opponent”. In: Proceedings of the Na-
tional Academy of Sciences 109.26 (2012), pp. 10409–10413. doi: 10 . 1073 / pnas .
1206569109.
[18] Tuomas W. Sandholm and Robert H. Crites. “Multiagent reinforcement learning in
the Iterated Prisoner’s Dilemma”. In:Biosystems 37.1 (1996), pp. 147–166.doi: 10.
1016/0303-2647(95)01551-5.
[19] Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. “An empirical
evaluation of active inference in multi-armed bandits”. In:Neural Networks144 (2021),
pp. 229–246.doi: 10.1016/j.neunet.2021.08.018.
[20] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl J. Friston. “Deep active
inference agents using Monte-Carlo methods”. In:Proceedings of the 34th International
Conference on Neural Information Processing Systems. NIPS’20. Curran Associates
Inc., 2020.doi: 10.5555/3495724.3496702.
[21] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl J. Friston, Iain
D. Couzin, and Alexander Tschantz. “pymdp: A Python library for active inference
indiscrete state spaces”. In: Journal of Open Source Software7.73 (2022), p. 4098.
issn: 2475-9066.doi: 10.21105/joss.04098.
[22] Lorens A. Imhof, Drew Fudenberg, and Martin A. Nowak. “Tit-for-tat or win-stay,
lose-shift?” In: Journal of Theoretical Biology 247.3 (2007), pp. 574–580. doi: 10 .
1016/j.jtbi.2007.03.027.
14
[23] Claus Wedekind and Manfred Milinski. “Human cooperation in the simultaneous and
the alternating Prisoner’s Dilemma: Pavlov versus Generous Tit-for-Tat”. In:Proceed-
ings of the National Academy of Sciences93.7 (1996), pp. 2686–2689.doi: 10.1073/
pnas.93.7.2686.
[24] Karl J. Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas
Fitzgerald, and Giovanni Pezzulo. “Active inference and epistemic value”. In:Cognitive
Neuroscience 6.4 (2015), pp. 187–214.doi: 10.1080/17588928.2015.1020053.
15
A Supplementary Information
A.1 Generative Model
In this section, we describe the Prisoner’s Dilemma game as a two-agent active inference sys-
tem and determine the conditions under which the agents reach the optimal state of constant
cooperative play, avoiding the Nash equilibrium. To enable active inference agents to reach
the cooperative steady state, we invoke the notion of parameter learning; specifically, the
ability of agents to infer likely sequences of game states by updating posterior beliefs about
transition probabilities. These transition probabilities parameterise a likelihood model that
describes transitions between game states (e.g., the transition from the state of ‘cooperate-
cooperate’ to ‘cooperate-defect’). Under active inference, this parameter learning is cast as
a problem of inferring generative model parameters. Usually, parameter inference unfolds
on a slow timescale (hence the term ‘learning’) relative to ‘fast’ inference of hidden states [1]
(See Table A.1 for full description of model parameters).
Variable Name Notation
Hidden States s ∈ {CC, CD, DC, DD}
Observations o ∈ {CC, CD, DC, DD}
Actions u ∈ {uC, uD}
Observation Model P(ot|st; A) =Cat(A)
Transition Model P(st+1|st−1, ut−1; B) =Cat(B)
Transition Model Parameter P(B) =Q
ju P(B•ju), P (B•ju) =Dir(b•ju)
Initial State Prior P(s1; D) =Cat(D)
‘Biased’ State Prior (Reward) ˜P(s; C) =Cat(C), s.t. ln C = [3, 1, 4, 2]
Table A.1:Generative model variables and notation.
The agent’s generative model is a Markov Decision Process [2] that encodes a joint
distribution over sequences of hidden statess1:T observations o1:T , actionsu1:T , and model
parameters A, B, D[3]. Markov Decision Processes assume that the dynamics are shallow,
with single-timestep dependencyP(st+1|st, ut; B); this Markov property means we can write
the generative model as a product of time-dependent distributions:
P(o1:T , s1:T , u1:T , A, B) =P(s1; D)P(π)P(A)P(B)P(D)
T−1Y
t=1
P(ot+1|st+1; A)P(st+1|st, ut; B)
(A.1)
multiplied by initial priors over hidden states, policies, and parameters.
16
The hidden statess consist of a single factor with four possible states or levels, corre-
sponding to the game states (the four combinations of possible two-player choices): CC, CD,
DC, and DD. This game state factor comprises the primary random variable in each agent’s
model.
In our notation, the first letter of each game state corresponds to the focal agent’s choice,
and the second letter corresponds to that of its opponent. In our formulation, agents have
precise knowledge of the current game state, which they technically infer through (unam-
biguous) observation of their and their opponent’s action. Uncertainty comes into the game
insofar as agents mustpredict the subsequent game state and then act based on their pre-
dictions and their desires to maximise utility.
There is one observation modality with four observations, which again correspond directly
to thefour gamestates. Therefore, the four observations areCC, CD,DC,and DD.Note that
the agents will only observe the game state after-the-fact, i.e., each observation corresponds
to the game state in the previous round of iterative play. This is because in the Prisoner’s
Dilemma, the agents perform their actions at any given trial without knowing what their
opponent will do in that trial, but in iterative play, the agents can build a strategy over time
by observing the resulting game states after each trial ends.
A.1.1 Observation Likelihood
The observation modelP(ot|st, A) is a conditional distribution encoding the agent’s beliefs
about the relationship between the current (hidden) game state and its concurrent observa-
tion. Also known as the likelihood model, the agent uses this distribution to infer the most
likely game state, given an observation thereof.
In the simulations presented here, we assume that agents are equipped with a determin-
istic, unambiguous observation model, i.e., observations are deterministic indicators of the
game state. In the discrete state space models common in active inference, likelihoods like
P(ot|st, A) are often represented as multidimensional arrays (e.g., matrices) whose values are
populated by parameters; in the case of the observation model, we represent this likelihood
directly as a matrixA whose entries are given by the likelihood parametersA. Hereafter
we use boldfaceX to indicate a representation of Categorical parameters in terms of vectors
and matrices, and use the standard italic notationX to indicate the random variable in the
generative model (e.g.P(A)). When we have an unambiguous or precise likelihood mapping,
this matrix is the identity matrix, representing the mapping from hidden states (columns)
to observations (rows):
P(ot = i|st = j, [A]ij) =δ(i − j) (A.2)
A =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 = I (A.3)
17
An agent with such a precise likelihood model will infer the game state in the previous
round of iterative play entirely based on the observed game state.
However, one can imagine introducing uncertainty into an agent’s beliefs by adding off-
diagonal, positive values into theA matrix – this would correspond to the agent believing
that game state observations are ambiguous with respect to the true game state. Concretely,
we could imagine that one agent might receive a misleading signal indicating that its oppo-
nent defected when they actually cooperated. A simple way to parameterise this uncertainty
is through an inverse temperature parameterψ, which makes theA matrix totally uninfor-
mative (maximum entropy columns) in the limit ofψ → 0, and infinitely precise in the limits
of ψ → ∞:
A = Iψ
PIψ (A.4)
Finally, it is worth mentioning that we assumeP(A) is infinitely precise and not subject
to learning. Therefore, we emit any parameterisation of the priors over this likelihood, while
we keep them for the transition likelihood parametersB, as we will update these in learning.
A.1.2 Reward
Different game states are assigned different rewards or desirabilities under the Prisoner’s
Dilemma problem formulation. Active inference converts the notion of ‘reward’ into prior
probability by equipping agents with biased prior beliefs about future states or observations
[4]. In the context of planning actions, this biased prior serves the role of a “goal-vector”
or reward function [3]. We denote this as a biased prior over states in our agent’s model
˜P(s; C)5. This special ‘goal prior’ is parameterised by a vector of Categorical parameters
C. Reward and prior probability can be straightforwardly related via the relation˜P(s) ∝
exp(r) [5]; therefore, we typically parameteriseC using relative log probabilities or nats,
i.e., C = ln ˜P(s) +Z. Following from Table 1, the most desirable observation issDC (the
agent defects and the opponent cooperates), followed bysCC (both players cooperate), then
sDD (both players defect), and finallysCD (the agent cooperates and the opponent defects).
Therefore, ourC vector isC = [3, 1, 4, 2].
Note that the values of these numbers have an effect on the desirability of the observations
and therefore will impact the agents action-planning such that they plan actions that they
infer will result in the observation of the most desirable state. Changing the values of these
rewards will change the incentive and behaviour of the agents.
A.1.3 Different reward parameterizations
We can parameterise the reward functionC in terms of a single precision that makes a
single ordered reward function with the constraintsrCD < rDD < rCC < rDC more or less
shallow/steep. We do this using the softmax (normalised exponential transformation):
5Note that in many formulations of active inference this is formulated as a prior over observations˜P(o; C).
18
C = σ




rCC
rCD
rDC
rDD

, β

, where CCC = exp(βrCC)P
i exp(βri)
ln CCC = βrCC − ln
 X
i
exp(βri)
!
=⇒ ln C ∝ β


rCC
rCD
rDC
rDD

 (A.5)
A.1.4 Policies
A policyπ is comprised of individual actions, or control states,π = {u1, u2, ...uH}. At each
trial of iterative play, the agents can either defect or cooperate. This means that the policy
space consists of two control states, namelyuC and uD. Once the action is inferred, the
intersection of both agents’ actions will result in the realised game state.
A.1.5 Transition Likelihood
The transition matrix encodes the beliefs that the agent holds about how game states will
evolve given previous trials and their actions. Because action selection under active inference
depends on model-based planning, this transition model also directly determines the agent’s
strategy. Although in this work we focus on how agents can automatically learn the game’s
dynamics and thus their strategies through experience, we nevertheless begin by constraining
what agents can learn by initialising agents‘ beliefs about transition dynamics, so that they
assume that two game state transitions are always impossible. Agents believe that when they
cooperate, there is zero probability that the next state will be DC or DD, and conversely,
when they defect, they believe there is zero probability that the next state will be CD or
CC. Therefore, the transition matrix encodes the agent’s assumptions about whether the
other will cooperate or defect in the next trial, given the outcome of the current trial and
the agent’s own action.
We use existing formulations of parameter learning under active inference to allow our
agents to update their beliefs about transition model over time based on experience. Tech-
nically, the agents are updating a Dirichlet posterior belief over the Categorical parameters
B that characterise its transition model (a transition probability matrix, mapping from past
to current game states, further conditioned on action). They update this matrix of posterior
Dirichlet parameters at the end of each trial, based on that trial’s outcome.
At the beginning of iterative play, the agent will be initialised with no prior opinion or
knowledge about which of the possible transitions are more likely given its actions (aside
19
from the zero constraints laid out above). These uniform initial transition distributions are
shown in (A.6) and (A.7).
P(st+1|st, ut = C) =


0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5
0 0 0 0
0 0 0 0

 (A.6)
P(st+1|st, ut = D) =


0 0 0 0
0 0 0 0
0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5

 (A.7)
At the conclusion of each trial during a session of iterative play, a given agent observes the
game state of the previous trial and updates its beliefs about transitions based on the realised
states and its actions. As these transition dynamics are learned, the agent is simultaneously
learning a strategy based on planning the most optimal action (cooperate or defect), given
its evolving beliefs.
A.2 Inference
A.2.1 State inference
At each trial of iterative play, the agents first infer the game state by inverting their Marko-
vian (POMDP) generative model using ongoing observationsot.
The agent’s hidden state inference involves optimising a variational posterior over hid-
den states and policiesQ(s1:T , π) as a categorical distribution with parameters˜ϕ that are
factorised ‘mean-field’-style across timesteps [6]:
Q(s1:T , π; ˜ϕ) =Q(π; ϕπ)
Y
1:T
Q(st; ϕs,t)
Where the variational parameters˜ϕ = {ϕπ, ϕs1:T } are themselves segregated into policy-
specific parametersϕπ and hidden-state-specific parametersϕs1:T .
At each timestept, the agent performs inference by optimising the posterior parameters
˜ϕ to minimise the timestep-specific variational free energyFt, which due to the Markovian
factorisation of the generative model and mean-field factorisation of the posterior, can be
expressed in terms of only the generative model of the current timestepP(ot, st, π,A, B, C):
Ft = EQ(st,π;˜ϕt)
h
ln Q(st, π; ˜ϕt) − ln P(ot, st, π,A, B, C)
i
(A.8)
20
The optimal posterior parameters ˜ϕ
∗
are those that minimise the free energy in (A.8)
and can be found by solving exactly for the fixed points ofFt. We begin by solving for the
parameters of the variational beliefs about hidden statesϕst:
∂Ft
∂ϕst
= 0
=⇒ ϕ∗
st = σ
 
ln AT ot + ln(But−1 · ϕ∗
st−1 )

(A.9)
where σ represents the softmax (or normalised exponential) transform of a vector. Theith
entry of the softmaxed output is given by:
σ(x)i ≜ exp(xi)P
j exp(xj) (A.10)
The initial matrix-vector product in the last line of (A.9)ln AT ot represents the con-
tribution of sensory evidence to inference, and can be thought of as picking out the row of
the A matrix that corresponds to the observation at timestept. The second matrix vec-
tor product ln(But−1 · ϕ∗
st−1 ) represents the contribution of prior information to inference.
This simple form is a consequence of the mean-field factorisation of the variational parame-
ters ϕs1:T across timesteps and an ‘empirical prior’ assumption, where the prior term of the
generative modelP(st) = EP(st−1)[P(st|st−1, ut−1, B)] is evaluated at the parameters of the
previous timestep’s variational posterior, in a manner reminiscent of a belief propagation
step or empirical Bayes:
P(st) =EP(st−1) [P(st|st−1, ut−1, B)]
≈ EQ(st−1;ϕst−1 ) [P(st|st−1, ut−1, B)] (A.11)
We can simplify the expression for the parameters of the variational beliefs due to the
unambiguous form of the observation likelihood with infinite precision in (A.3),A = Iψ
PIψ ,
as well as the fact that the agents are taking identical actions at every trial, thus limiting
the state space to {CC, DD} which implies that inference can be solved for exactly for any
trial t >0 as
ϕ∗
st = σ
 
ln
 Iψ
PIψ
T
ot + ln
 
But−1 · ϕ∗
st−1

!
(A.12)
= lim
ψ→∞
σ

ψ ln
 
IT ot

+ ln
 
But−1 · ϕ∗
st

− ln
X
Iψ

(A.13)
= σ
 
ln
 
IT o1

= IT o1 (A.14)
21
A.2.2 Policy inference
Under active inference, action selection and planning are cast as an inference problem, where
policies are treated as a latent variable to be inferred. This has deep homology to contem-
porary approaches to model-based planning in reinforcement learning, such as planning as
inference and control as inference [5, 7–9]. In particular, active inference agents optimise a
variational posterior over policiesQ(π). However, because policies inherently require esti-
mation of future, unobserved states, we use an augmented, ‘predictive’ generative model to
perform this policy inference. This predictive generative model is importantly augmented
with the biased prior distribution over states˜P(s; C). Beliefs about policies, similar to those
about hidden states, are optimised by minimising a free energy functional of beliefs about
the consequences of action under the predictive generative model. This functional is known
as theexpected free energyand exhibits many desirable properties such as a natural balance
between information-seeking (‘exploration’) and goal-directedness (‘exploitation’) [10]. The
approximate posterior over policiesQ(π) is also a Categorical distribution with parameters
ϕu; the optimal setting of these parametersϕ∗
u minimises the expected free energy, leading
to the relationship:
Q(π; ϕu) =σ(−G(π))
G(π) =
HX
τ=1
Gt+τ (ut+τ−1) (A.15)
The second line shows that the expected free energy of a policy is the sum of the expected
free energies that accrue for each action that comprises the policy:π = {u1, u2, ...uH}. For
the present purposes we only consider 1-step ahead policies (H = 1). This means that the
expected free energy of a policy is simply the expected free energy computed one timestep
into the futureGt+1(ut).
The expected free energy can be decomposed into expected ambiguity and risk terms:
Gt+1(ut) =EQ(st+1|ut) [H [P(ot+1|st+1)]] + DKL (Q(st+1|ut) ∥ ln P(st+1|C)) (A.16)
We can write this general expression in terms of sufficient statistics of the variational
distribution over hidden statesϕ∗
st. The ambiguity term of the expected free energy vanishes
because the agent’s likelihood matrix is the identity:
ABt · ϕ∗
st ·
 
ln(ABt · ϕ∗
st) − ln C

(A.17)
= Bt · ϕ∗
st
 
ln Bt · ϕ∗
st − ln C

− (A ln A) · ϕ∗
st
| {z }
=0
(A.18)
22
A.2.3 Action Selection
Having optimised a posterior over policies (which in this context simply reduce to control
states), action selection simply consists of sampling the action at trialt that minimises the
expected free energy, i.e., sampling an action from the posterior marginal over actions.
ϕu = σ(−G) (A.19)
ut+1 ∼ Q(ut+1; ϕu) (A.20)
This can be done either deterministically by selecting the most probable control state at
every timestep:
ut+1 = arg max
u
Q(ut+1; ϕu) (A.21)
Or, this can be done stochastically by sampling from the posterior over actions. The
stochasticity of this sampling can be further tuned by sampling from a transformed action
posterior scaled by a temperature parameterα.
ut+1 ∼ Q(ut+1; ϕ, α) (A.22)
A.2.4 B matrix learning
After every trial of iterative play, each agent updates its posterior beliefs about the transition
model B by optimizing Dirichlet parameters ϕb, which are the sufficient statistics of a
Dirichlet parameterization of the posteriorQ(B; ϕb). This is also known as ‘learning’ in the
active inference literature, and analogised to neuronal processes such as synaptic plasticity,
which typically occurs on a slower timescale than hidden state inference (analogised to rapid
dynamics of neural firing rates) [1]. Dirichlet distributions are used as the parameterizations
of discrete Categorical likelihood matrices, due to their natural role as conjugate priors for
the Categorical distribution.
We supplement the generative model with an additional prior over the parameters of the
transition model, the Dirichlet distributionP(B; b) parameterised by a vector of positive
real hyperparametersb, that can also be interpreted as ‘pseudocounts’, i.e., how many times
has the agent seen this particular transition occur, before the simulation starts. Alongside
this prior we introduce a variational posterior overB that is also a Dirichlet distribution
Q(B; ϕb). This leads to a new expression for the variational free energy at a given time
point, which includes an additional Kullback-Leibler divergence between the variational and
generative model Dirichlet distributions overB [3]:
Ft = EQ(st,ut,B;˜ϕ)
h
ln Q

st, ut, B; ˜ϕ

− ln P (ot, st, ut, A, B, C; A, b, C)
i
= EQst,ut;ϕs,u

ln Q
 
st, ut; ϕs,u

− ln P (ot, st, ut, A, C; A, C)

+ DKL (Q(B; ϕb) ∥ P(B; b))
(A.23)
23
This new expression means that when we minimiseFt with respect to the variational
(Dirichlet) parameters ϕb, we get a closed-form expression for the variational beliefs over
B, which can be expressed in terms of the Dirichlet prior parametersb and the variational
posterior over hidden states at current and previous timestepsϕst and ϕst−1 .
Bt+1 = ϕ∗
b
ϕb,0
(A.24)
ϕ∗
b = b + η(ϕst ⊗ ϕst−1 ) (A.25)
where(A.24)representstheupdatetotheDirichletpriorforthetransitiondistributionduring
learning. This is updated with respect to the learning rateη and the transition probabilities
given the previously performed actionat−1. It is this normalised updated Dirichlet prior that
then becomes the new transition probability distribution for the following trial.
The updates to the transition model are governed by the sequence of game states. We can
imagine a fictive 1-turn sequence (two trials) to imagine how a particular sequence influences
learning. If at one trial, the agents both cooperated, then they will infer that the game state
was CC. Given this belief, they will infer which action to take. If they choose to defect,
hoping that the opponent will cooperate again, the resulting inferred state will be that the
optimal action isut = uD, and after the trial they will observe the resulting state, DD. At
this point, the agents will update their beliefs about likely transitions (encoded in theB
matrix parameters), such that there will be a small incremental increase in the conditional
probability of DD, given a past state of CC and a past action ofuD, i.e.,P(st+1 = DD|st =
CC, ut = uD). The size of this update is determined by a learning rate parameterη.
A.3 Deriving the analytic form of the transition function
When two deterministic agents have the same learning rate, they will perform the same
action at every timestep. This has the consequence that the two-agent system will only ever
explore two out of four states, namely CC and DD.
The posterior belief can be represented as a vector of its parameters, and in the solution
of two identical agents, it can take two possible values, which we denote assCC and sDD.
Because the likelihood distribution is the identity matrix, these will be maximally precise
vectors:
sCC =


1
0
0
0

 sDD =


0
0
0
1

 (A.26)
The initial Dirichlet parameters of the prior distribution over the transition model are,
for the cooperate and defect-conditioned transitions, respectively,
24
bC
0 =


0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5
0 0 0 0
0 0 0 0

 bD
0 =


0 0 0 0
0 0 0 0
0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5

 (A.27)
This means that at each timestep, there are four possible updates to the parameters of
each agent’s variational posterior over the transition modelϕb, given the two variational
beliefs a given agent might have (sCC and sDD):
ϕ∗
bt+1 =



bC
0 + η · (sCC ⊗ sCC)t
bD
0 + η · (sDD ⊗ sCC)t
bC
0 + η · (sCC ⊗ sDD)t
bD
0 + η · (sDD ⊗ sDD)t
(A.28)
When the agents are both defecting (e.g., in the first timestep when the most likely action
is defect), then the update rule for the weights of the Dirichlet parameters of the transition
matrix is governed by:
ϕ∗
bt<τ1
= bD
0 + η
 
sDD ⊗ sDD
t (A.29)
Bt+1<τ1 =
ϕ∗
bt<τ1
ϕ∗
bt<τ1,0
(A.30)
At some critical timeτ1 the probability of cooperation exceeds that of defection, due to
the change in the expected free energies of the two actionsGτ1 (u = C)< Gτ1 (u = D). This
triggers the beginning of the so-called “oscillation period” (see Section 2 in the main text),
where agents periodically oscillate between cooperating and defecting with the same phase.
We can expand this condition according to (A.18) into the following form:
BC
0 · sDD
τ1 ·
 
ln BC
0 · sDD
τ1 − ln C

= BD
τ1 · sDD
τ1 ·
 
ln BD
τ1 · sDD
τ1 − ln C

(A.31)
As shown in Section A.4, the equality in (A.31) can be written in terms ofη, C and τ1:
1
(2 +η2τ1)

ln 1
2(1 +ητ1)C3
+ (1 + 2ητ1) ln 1 + 2ητ1
2(1 +ητ1)C4

= 1
2 ln
 1
4C1C2

, (A.32)
Letting y = 1
2+2ητ1
, which will always be between 0 and 1, we can now rewrite (A.32) as
y ln y − y ln C3 + (1− y) ln(1− y) − (1 − y) lnC4 = 1
2 ln
 1
4C1C2

(A.33)
25
To deriveτ1 in terms ofη, we must make an approximation. We use the fact that when
y is between 0 and 1, it can be approximated byy ≈ Ayb(y −1). This gives us the following
expression as an approximation for (35)
Ayb(y − 1) − y ln C3 − A(1 − y)by − (1 − y) lnC4 = 1
2 ln
 1
4C1C2

(A.34)
The optimal values for the approximation areA = 4774
4563 and b = 3
5 , however, for simplicity,
we letA = 1and b = 1and then the desired root of (A.34) can be solved as:
y = 1
4
 
ln C3
C4
+ 2−
s
(ln C4
C3
− 2)2 − 8(−ln C4
2√C1C2
− 1
5)

(A.35)
Therefore, sincey = 1
2+2ητ1
, we have that
τ1 ≈ R1
η (A.36)
where
R1 = 2
ln C3
C4
+ 2−
q
(ln C4
C3
− 2)2 − 8(−ln C4
2√C1C2
− 1
5 )
− 1 (A.37)
We now have an approximation forτ1 in terms ofη and a constantR1, which depends
on the rewardC which can be parameterised byβ according to (A.5).
τ1 = R1(β)
η (A.38)
for some precisionβ. We can plot this equation for different values ofβ to see how the values
in the reward function influenceτ1 (see Figure A.1).
For τ1 < t < τ2 (i.e., during the period of oscillation dynamics shown in Figure 2), the
update rules then become:
ϕD
bτ1<t<τ2
= bD
τ1 + 1
2η(sDD ⊗ sCC)(t − τ1) (A.39)
ϕC
bτ1<t<τ2
= bC
0 + 1
2η(sCC ⊗ sDD)(t − τ1) (A.40)
The update rule changes from (A.39) to (A.40) at every other trial, from conditioning on
the previous action being D, to being C. The oscillation period persists until some timeτ2.
26
Figure A.1: Dynamics of the expected free energy. Left:The difference of EFE for
cooperation and defection (vertical axis). The roots of this equation are the values ofτ1 for
different values ofβ, parameterizing the values in the reward functionC as per (A.42), with
η = 0.2. It is clear that with a higher value ofβ, it will take agents longer to cooperate,
i.e. τ1 will be larger, demonstrated by the horizontal translations of the curves asβ increases.
Right: Values ofτ1 for different values ofβ parameterizing the reward function, at different
learning rates. Again, we see that asβ increases, τ1 increases. We can also see that larger
η competes with higherβ to decrease τ1, as the agents update their transition probability
distributions at a higher frequency.
At τ2 we will have that, for the first time,G0(u = C, ϕC) < G0(u = C, ϕD). Again, we can
expand this according to (A.18) as:
BC
τ2 · sCC ·
 
ln BC
τ2 · sCC − ln C

= BD
τ2 · sCC ·
 
ln BD
τ1 · sCC − ln C

(A.41)
Rewriting this equation in terms ofη, τ1, τ2, andC leads to the following inequality (for
full derivation, see Section A.5):
1
2 +η(τ2 − τ1)

ln[ 1
C3
( 1
2 +η(τ2 − τ1))] + (1 +η(τ2 − τ1)) ln 1 +η(τ2 − τ1)
C4(2 +η(τ2 − τ1))

= −1
2 ln(4C1C2)
(A.42)
This time, we lety = 1
2+η(τ2−τ1) and we have:
(y − 1) lny − y ln C3 + (1− y) ln(1− y) − (1 − y) lnC4 = −1
2 ln(4C1C2) (A.43)
27
Now, notice that this is the exact same equation as (A.33) above, which we know we can
approximate as (A.34). We can then write our solution in terms ofR1:
τ2 ≈ 1
η(1
y − 2) +τ1 = 1
η(3
2R1) (A.44)
The resulting equation is obtained in terms ofR2, whereR2 = 3
2 R1.
τ2 ≈ R2(β)
η (A.45)
After τ2, agents will cooperate indefinitely according to the final steady state update rule:
ϕ∗
bt>τ2
= bC
τ2 + η
 
sCC ⊗ sCC
t (A.46)
Bt+1>τ2 =
ϕ∗
bt>τ2
ϕ∗
bt>τ2,0
(A.47)
A.4 Full derivation of τ1
Here we deriveτ1 for the following equality from (A.31):
BC
0 · sDD
τ1 ·
 
ln BC
0 · sDD
τ1 − ln C

= BD
τ1 · sDD
τ1 ·
 
ln BD
τ1 · sDD
τ1 − ln C

(A.48)
Using the following:
Bt = ϕbt
ϕbt,0
(A.49)
ϕD
bt<τ1
= bD
0 + η(sDD ⊗ sDD)t (A.50)
ϕC
bt<τ1
= bC
0 (A.51)
bC
0 =


0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5
0 0 0 0
0 0 0 0

 bD
0 =


0 0 0 0
0 0 0 0
0.5 0 .5 0 .5 0 .5
0.5 0 .5 0 .5 0 .5

 (A.52)
sDD = e4, (A.53)
we have:
ϕC
b0
ϕC
b0,0
· sDD ·
 
ln ϕC
b0
ϕC
b0,0
· sDD − ln C
!
=
ϕD
bτ1
ϕD
bτ1,0
· sDD ·
 
ln
ϕD
bτ1
ϕD
bτ1,0
· sDD − ln C
!
(A.54)
28
On the LHS:
ϕC
b0
ϕC
b0,0
· sDD ·
 
ln ϕC
b0
ϕC
b0,0
· sDD − ln C
!
= (bC
0 · e4) · ln bC
0 · e4
C = −1
2 ln(4C1C2) (A.55)
On the RHS:
ϕD
bτ1
ϕD
bτ1,0
· sDD ·
 
ln
ϕD
bτ1
ϕD
bτ1,0
· sDD − ln C
!
=
ϕbDτ1 ,j=4
ϕD
bτ1,0
·
 
ln
ϕbDτ1 ,j=4
ϕD
bτ1,0
− ln C
!
(A.56)
= 1
2
1
1 +ητ1
ln( 1
2(1 +ητ1)C3
) +1
2
1 + 2ητ1
1 +ητ1
ln( 1 +ητ1
2(1 +ητ1)C4
) (A.57)
Our equality is therefore:
1
(2 + 2ητ1)

ln 1
2(1 +ητ1)C3
+ (1 + 2ητ1) ln 1 + 2ητ1
2(1 +ητ1)C4

= −1
2 ln(4C1C2) (A.58)
A.5 Full derivation of τ2
Our condition for derivingτ2 in terms of the expected free energies is
BC
τ2 · sCC ·
 
ln BC
τ2 · sCC − ln C

= BD
τ2 · sCC ·
 
ln BD
τ2 · sCC − ln C

(A.59)
Here ourϕs between trialsτ1 and τ2 are:
ϕD
bτ1<t<τ2
= ϕD
bt<τ1
+ 1
2η(sDD ⊗ sCC)(t − τ1) (A.60)
ϕC
bτ1<t<τ2
= bC
0 + 1
2η(sCC ⊗ sDD)(t − τ1) (A.61)
And to solve forτ2 our inequality is
ϕC
bτ2
ϕC
bτ2 ,0
· sCC ·
 
ln
ϕC
bτ2
ϕC
bτ2 ,0
· sCC − ln C
!
=
ϕD
bτ2
ϕD
bτ2 ,0
· sDD ·
 
ln
ϕD
bτ2
ϕD
bτ2 ,0
· sDD − ln C
!
(A.62)
On the LHS we have:
ϕC
bτ2
ϕC
bτ2 ,0
· sCC ·
 
ln
ϕC
bτ2
ϕC
bτ2 ,0
· sCC − ln C
!
= −1
2 ln(4C1C2) (A.63)
29
On the RHS:
ϕC
bτ2
ϕC
bτ2 ,0
· sDD = 1
2 +η(τ2 − τ1)


0
0
1
1 +η(τ2 − τ1)

 (A.64)
1
2 +η(τ2 − τ1)

ln
 1
C3
( 1
2 +η(τ2 − τ1))

+ (1 +η(τ2 − τ1)) ln 1 +η(τ2 − τ1)
C4(2 +η(τ2 − τ1))

(A.65)
1
2 +η(τ2 − τ1) ln
 C4
C3(1 +η(τ2 − τ1))

+ ln 1 +η(τ2 − τ1)
C4(2 +η(τ2 − τ1)) (A.66)
Finally, our inequality is:
1
2 +η(τ2 − τ1)
"
ln
 1
C3
 1
2 +η(τ2 − τ1)

+
(1 +η(τ2 − τ1)) ln 1 +η(τ2 − τ1)
C4(2 +η(τ2 − τ1))
#
= −1
2 ln(4C1C2) (A.67)
Supplemental References
[1] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John
O’Doherty, and Giovanni Pezzulo. “Active inference and learning”. In:Neuroscience &
Biobehavioral Reviews68 (2016), pp. 862–879.doi: 10.1016/j.neubiorev.2016.06.
022.
[2] Martin L. Puterman. “Markov decision processes”. In: Handbooks in Operations Re-
search and Management Science. Vol. 2. Stochastic Models. Elsevier, 1990, pp. 331–
434. doi: 10.1016/S0927-0507(05)80172-0.
[3] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl J. Friston, Iain
D. Couzin, and Alexander Tschantz. “pymdp: A Python library for active inference
indiscrete state spaces”. In: Journal of Open Source Software7.73 (2022), p. 4098.
issn: 2475-9066.doi: 10.21105/joss.04098.
[4] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement learning or
active inference?” In: PloS One 4.7 (2009), e6421. doi: 10 . 1371 / journal . pone .
0006421.
30
[5] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “On
the relationship between active inference and control as inference”. In:Active Inference.
Ed. by Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom.
Communications in Computer and Information Science. Cham: Springer International
Publishing, 2020, pp. 3–11.doi: 10.1007/978-3-030-64919-7_1 .
[6] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. “Variational inference: A review
for statisticians”. In:Journal of the American Statistical Association112.518 (2017),
pp. 859–877.doi: 10.1080/01621459.2017.1285773.
[7] Hagai Attias. “Planning by probabilistic inference”. In: Proceedings of the Ninth In-
ternational Workshop on Artificial Intelligence and Statistics. Ed. by Christopher M.
Bishop and Brendan J. Frey. Vol. R4. Proceedings of Machine Learning Research. 2003,
pp. 9–16.url: https://proceedings.mlr.press/r4/attias03a.html.
[8] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
Heess, and Martin Riedmiller. “Maximum a posteriori policy optimisation”. In:Inter-
national Conference on Learning Representations. 2018.url: https://openreview.
net/forum?id=S1ANxQW0b.
[9] Matthew Botvinick and Marc Toussaint. “Planning as inference”. In:Trends in Cogni-
tive Sciences16.10 (2012), pp. 485–488.doi: 10.1016/j.tics.2012.08.006.
[10] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. “Whence the ex-
pected free energy?” In:Neural Computation33.2 (2021), pp. 447–482.doi: 10.1162/
neco_a_01354.
31