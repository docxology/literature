arXiv:2008.09927v3  [q-bio.NC]  21 Jan 2021
Bayesian mechanics of perceptual inference and
motor control in the brain
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61 186, Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. The free energy principle (FEP) in the neurosciences stipulates tha t all
viable agents induce and minimize informational free energy in the bra in to ﬁt their
environmental niche. In this study, we continue our eﬀort to make the FEP a more
physically principled formalism by implementing free energy minimization b ased on
the principle of least action. We build a Bayesian mechanics (BM) by cas ting the
formulation reported in the earlier publication (Kim in Neural Comput 3 0:2616–2659,
2018) to considering active inference beyond passive perception. The BM is a neural
implementation of variational Bayes under the FEP in continuous time . The resulting
BM is provided as an eﬀective Hamilton’s equation of motion and subjec t to the
control signal arising from the brain’s prediction errors at the pro prioceptive level. To
demonstrate the utility of our approach, we adopt a simple agent-b ased model and
present a concrete numerical illustration of the brain performing r ecognition dynamics
by integrating BM in neural phase space. Furthermore, we recapit ulate the major
theoretical architectures in the FEP by comparing our approach w ith the common
state-space formulations.
Keywords free energy principle, Bayesian mechanics, recognition dynamics,
continuous state-space models, neural phase space, limit cycles
Bayesian mechanics of perceptual inference and motor control in the brain 2
1. Introduction
The free energy principle (FEP) in the ﬁeld of neurosciences rationa lizes that all viable
organisms cognize and behave in the natural world by calling forth th e probabilistic
models in their neural system — the brain — in a manner that ensures t heir adaptive
ﬁtness (Friston 2010a). The neurobiological mechanism that endo ws an organism’s brain
— the neural observer — with this ability is theoretically framed into an inequality that
weighs two information-theoretical measures: surprisal and info rmational free energy
(IFE) (see, for a review, Buckley and Kim et al. 2017). The surprisa l provides a measure
of the atypicality of an environmental niche, and the IFE is the uppe r bound of the
surprisal. The inequality enables a cognitive agent to minimize the IFE a s a variational
objective function indirectly instead of the intractable surprisal. ; The minimization
corresponds to inferring the external causes of aﬀerent senso ry data, which are encoded
as a probability density at the sensory interface, e.g., sensory org ans. The brain of
an organism neurophysically performs the Bayesian computation of minimizing the
induced variational IFE; this is termed as recognition dynamics (RD) , which emulates,
under the Laplace approximation (Friston et al. 2007), the predict ive coding scheme of
message processing or recognition (Rao and Ballard 1999; Bogacz2 017). The neuronal
self-organization in vitrounder the FEP was studied recently at the level of single neuron
responses (Isomura et al. 2015; Isomura and Friston 2018). Owin g to its explanatory
power of perception, learning, and behavior of living organisms within a framework, it is
suggested a promising uniﬁed biological principle (Friston 2010a; Fris ton 2013; Colombo
and Wright 2018; Ramstead et al. 2018).
The neurophysical mechanisms of the abductive inference in the br ain are yet
to be understood; therefore, researchers mostly rely on inform ation-theoretic concepts
(Elfwing 2016; Ramstead et al. 2019; Kuzma 2019; Shimazaki 2019; Kiefer 2020, Sanders
et al. 2020). The FEP facilitates dynamic causal models in the brain’s g eneralized-state
space (Friston 2008b; Friston et al. 2010b), which pose a mixed disc rete-continuous
Bayesian ﬁltering (Jazwinski 1970; Balaji and Friston 2011). In th is work, we consider
that the brain confronts the continuous inﬂux of stochastic sens ations and conducts the
Bayesian inversion of inferring external causes in the continuous s tate representations.
Biological phenomena are naturally continuous spatiotemporal eve nts; accordingly, we
suggest that the continuous-state approaches used to describ e cognition and behavior are
better suited than discrete-state descriptions for studying the perceptual computation
in the brain.
Recently, we carefully evaluated the FEP while clarifying technical as sumptions
that underlie the continuous state-space formulation of the FEP ( Buckley and Kim
et al. 2017). A full account of the discrete-state formulation com plementary to our
; Free energy (FE) is a notion developed by Hermann von Helmholtz in th ermodynamics; it is a physical
energy measured in joules. The FE in the FEP is an information-theor etic measure deﬁned in terms of
probabilities, which serves as an objective function for variational Bayesian inference. Accordingly, we
call it variational IFE in our formulation.
Bayesian mechanics of perceptual inference and motor control in the brain 3
formulation can be found in (Da Costa et al. 2020a). In a subsequen t paper (Kim
2018), we reported a diﬀerent variational scheme that the Bayes ian brain may utilize
in conducting inference. In particular, by postulating that “surpr isal” plays the role
of a Lagrangian in theoretical mechanics (Landau and Lifshitz 1976 ; Sengupta et al.
2016), we worked a plausible computational implementation of the FE P by utilizing the
principle of least action. We believed that although the FEP relies on Ba yesian abductive
computation, it must be properly formulated conforming to the phy sical principles and
laws governing the matter comprising the brain. To this end, we prop osed that any
process theory of the FEP ought to be based on the full implication o f the inequality
(Kim 2018) ż
dtt´ ln ppϕqu ď
ż
dtFrqpϑq,p pϕ,ϑ qs, (1)
where ϕ and ϑ collectively denote the sensory inputs and environmental hidden st ates,
respectively. The integrand on the left-hand side (LHS) of the pre ceding equation
´ ln ppϕq is the aforementioned surprisal, which measures the “self-information”
contained in the sensory density ppϕq (Cover 2006), and F on the right-hand side (RHS)
is the variational IFE deﬁned as
Frqpϑq,p pϕ,ϑ qs ”
ż
dϑqpϑq ln qpϑq
ppϕ,ϑ q , (2)
which encapsulates the recognition (R-) density qpϑq and the generative (G-) density
ppϑ,ϕ q (Buckley and Kim et al. 2017). While the G-density represents the br ain’s
belief (or assumption) of sensory generation and hidden environme ntal dynamics, the
R-density is the brain’s current estimate of the environmental cau se of the sensory
perturbation. The G- and R- densities together induce variational IFE when receptors
at the brain-environment interface are excited by sensory pertu rbations.
According to Eq. (1), the FEP articulates that the brain minimizes th e upper bound
of the sensory uncertainty, which is a long-term surprisal. We ident ify this bound as an
informational action (IA) within the scope of the mechanical princip le of least action
(Landau and Lifshitz 1976). Then, by complying with the revised FEP , we formulate the
Bayesian mechanics (BM) that executes the RD in the brain. The RD n europhysically
performs the computation for minimizing the IA when the neural obs erver encounters
continuous streams of sensory data. The advantage of our form ulation is that the brain
and the environmental states are speciﬁed using only bare continu ous variables and their
ﬁrst-order derivatives (velocities or equivalent momenta). The mo mentum variables
represent prediction errors, which quantify the discrepancy bet ween an observed input
and its top-down belief of a cognitive agent in the predictive coding lan guage (Huang
and Rao 2011; de Gardelle et al. 2013; Kozunov et al. 2020).
The goal of this work is to cast our previous study to include the age nt’s motor
control, which acts on the environment to alter sensory inputs. § Previously, by utilizing
§ In this work, we use the term control instead of the frequently used term “action” to mean the motion
of a living agent’s eﬀectors (muscles) acting on the environment. Th is is done to avoid any confusion
with the term action appearing in the nomenclature of “the principle of least action”.
Bayesian mechanics of perceptual inference and motor control in the brain 4
the principle of least action, we focused on the formulation of perce ptual dynamics for
the passive inference of static sensory inputs (Kim 2018) without incorporating motor
control for the active perception of nonstationary sensory str eams. Here, we apply
our approach to the problem of active inference derived from the FEP (Friston et al.
2009; Friston et al. 2010c; Friston et al. 2011a), which proposes t hat organisms can
minimize the IFE by altering sensory observations when the outcome of perceptual
inference alone is not in accordance with the internal representat ion of the environment
(Buckley and Kim et al. 2017). Living systems are endowed with the ab ility to adjust
their sensations via proprioceptive feedback, which is attributed t o an inherited trait
of all motile animals embodied in the reﬂex pathways (Tuthill and Azim 20 18). In
this respect, motor control is considered an inference of the cau ses of motor signals
encoded as prediction errors at proprioceptors, and motor infer ence is realized at the
spinal level by classical reﬂex arcs (Friston 2011b; Adams et al. 20 13). Our formulation
evinces time-dependent driving terms in the obtained BM, which arise from sensory
prediction errors, as control (motor) signals. Accordingly, the B M bears resemblance
to the deterministic control derived from Pontryagin’s maximum prin ciple in optimal
control theory (Todorov 2007). In this work, we consider the ag ent’s locomotion for
action inference only implicitly: our formulation focuses on the impleme ntation of the
control signal (or commands) at the neural level of description a nd not at the behavioral
level of biological locomotion; accordingly, the additional minimization mechanism of
the IA inferring optimal control was not explicitly handled, which is lef t as a future
work. There are other systematic approaches that try to relate the active inference
formalism to the existing control theories (Baltieri and Buckley 201 9; Millidge et al.
2020a; Da Costa et al. 2020c).
Technically, a variation in the IA yields the BM that computes Bayesian inversion,
which is given as a set of coupled diﬀerential equations for the brain v ariables and their
conjugate momenta. The brain variables are ascribed to the brain’s representation of
the environmental states, and their conjugate momenta are the combined prediction
errors of the sensory data and the rate of the state represent ations. The neural
computation of active inference corresponds to the BM integratio n and is subject to
nonautonomous motor signals. The obtained solution results in optim al trajectories
in the perceptual phase space, which yields a minimum accumulation of the IFE over
continuous time, i.e., a stationary value of the IA. Our IA is identical t o that of “free
action” deﬁned in the Bayesian ﬁltering schemes (Friston et al. 2008 c). When the
minimization of free action is formulated in the generalized ﬁltering sch eme (Friston et
al. 2010b), two approaches are akin to each other such that both assume the Laplace-
encoded IFE as a mechanical Lagrangian. The diﬀerence lies in their m athematical
realization of minimization: our approach applies the principle of least a ction in classical
mechanics, while generalized ﬁltering uses the gradient descent met hod in the generalized
state space, where generalized states are interpreted as a solen oidal gradient ﬂow in a
nonequilibrium steady state.
The remainder of this paper is organized as follows. In Sect. 2, we un ravel some of
Bayesian mechanics of perceptual inference and motor control in the brain 5
the theoretical details in the formulation of the FEP. In Sect. 3, we formulate the BM
of the sensorimotor cycle by utilizing the principle of least action. The n, in Sect. 4, we
present a parsimonious model for a concrete manifestation of our formulation. Finally,
in Sect. 5, we provide the concluding remarks.
2. Recapitulation of technical developments
Here, we recapitulate theoretical architectures in the continuou s-state formulation under
the FEP while discussing technical features that distinguish our for mulation from
prevailing state-space approaches.
2.1. Perspective on generalized states
The Bayesian ﬁltering formalism of the FEP adopts the concept of th e generalized
motion of a dynamical object by deﬁning its mechanical state beyon d position and
velocity (momentum). The generalized states of motion are genera ted by recursively
taking time derivatives of the bare states. A point in the hyperspac e deﬁned by
the generalized states is interpreted as an instantaneous trajec tory. This notion
provides an essential theoretical basis for ensuring an equilibrium s olution of the RD
in the conventional formulation of the FEP (Kim 2018); it is commonly e mployed by
researchers (Parr and Friston 2018; Baltieri and Buckley 2019).
The motivation behind the generalized coordinates of motion is to des cribe noise
correlation in the generative processes beyond white noise (Wiener process), and thus,
to provide a more detailed speciﬁcation of the dynamical states (Fr iston 2008a; Friston
2008b; Friston et al. 2010b). The mathematical theory of a quasi- Markovian process
undergirds this formulation, which describes general stochastic d ynamics with a colored-
noise correlation with a ﬁnite-dimensional Markovian equation in an ex tended state
space by adding auxiliary variables (Pavliotis 2014). State-space au gmentation in
terms of generalized coordinates may be considered a special realiz ation of the Pavliotis
formalism. The state-extension procedure adopts some speciﬁc a pproximations, such
as the local linearization procedure developed in nonlinear time-serie s analysis (Ozaki
1992).
From the physics perspective, higher-order states possess a diﬀ erent dynamical
status in comparison with Newtonian mechanical states, speciﬁed o nly by position (bare
order) and velocity (ﬁrst order). A change in the Newtonian state s is caused by a force
that speciﬁes acceleration (second order) (Landau and Lifshitz 1 976). Although there
are no “generalized forces” causing the jerk (third order), snap (fourth order) etc.,
the jerk can be measured phenomenologically by observing a change in acceleration.
This induces all higher-order states to the kinematic level. Another perspective is
whether update equations in terms of generalized coordinates are equivalent to the
Pavliotis’ quasi-Markovian description. Auxiliary variables in Pavliotis’ analysis are not
generated by the recursive temporal derivatives of a bare state . The generalized phase
Bayesian mechanics of perceptual inference and motor control in the brain 6
space considered in (Kerr and Graham 2000) is also spanned in terms of canonical
displacement and momentum variables. A further in-depth analysis is required.
Our formulation does not employ the generalized states, but instea d, it follows
the normative rules in specifying generative models (Kim 2018). The d erived BM
performs the brain’s Bayesian inference in terms of only the bare br ain variable and its
conjugate momentum in phase space, and not in an extended state space. Accordingly,
our formulation is restricted to the white noise in the generative pro cesses; however,
it provides a natural approach to determine the equilibrium solutions of the BM (see
Sect. 4). For the general brain models described by many brain var iables, the brain’s BM
can be set up in multi-dimensional phase space, which is distinctive fro m the state-space
augmentation in the generalized coordinate formulation (see Sect. 2.3).
2.2. Continuous state implementation of recognition dynamics (RD)
The conventional FEP employs the gradient-descent minimization of the variational IFE
by the brain’s internal states. To incorporate the time-varying fe ature of sensory inputs,
the method distinguishes the path of a mode and the mode of a path in the generalized
state space (Friston 2008b; Friston et al. 2008c; Friston et al. 20 10b). This theoretical
construct intuitively considers the nonequilibrium dynamics of gener alized brain states
as drift-diﬀusion ﬂows that locally conserve the ensemble density in t he hyperspace of
the generalized states (Friston and Ao 2012b; Friston 2019).
Mathematically, the gradient descent formulation is based on the ge neral idea for a
fast and eﬃcient convergence and it ensures that formulations re ach a sophisticate level
by incorporating the Riemannian metric in information geometry (Ama ri 1998; Surace
et al. 2020); the idea is applied to the FEP (Sengupta and Friston 201 7, Da Costa et
al. 2020b).
In our proposed formulation, we replace the gradient descent sch eme with the
standard mechanical formulation of the least action principle (Kim 20 18). However,
there is a disadvantage in that we incorporate only the Gaussian whit e noise in the
generative processes of the sensory data and environmental dy namics [see Sect. 2.3].
The resulting novel RD described by an eﬀective Hamiltonian mechanic s entails optimal
trajectories but no single ﬁxed points in the canonical state (phas e) space, which provides
an estimate of the minimum sensory uncertainty, i.e., the average su rprisal over a ﬁnite
temporal horizon. The phase space comprises the positions (pred ictions) and momenta
(prediction errors) of the brain’s representations of the causal environment.
Our implementation of the minimization procedure is an alternative to t he gradient
descent algorithms in the FEP. A crucial diﬀerence between the two approaches is that
while the gradient descent scheme searches for an instantaneous trajectory representing
a local minimum on the IFE landscape in the multidimensional generalized state space,
our theory determines an optimal trajectory minimizing the continu ous-time integral of
the IFE in two-dimensional phase space for a single variable problem.
Bayesian mechanics of perceptual inference and motor control in the brain 7
2.3. Treatment of noise correlations
The FEP requires the brain’s internal model of the G-density ppϕ,ϑ q encapsulating
the likelihood ppϕ|ϑq and prior ppϑq. The likelihood density is determined by the
random ﬂuctuation in the expected sensory-data generation, an d the prior density is
determined by that in the believed environmental dynamics. The bra in encounters
sensory signals on a timescale, which is often shorter than the corr elation time of the
random processes (Friston 2008a); accordingly, in general, the n oises embrace a non-
Markovian stochastic process with an intrinsic temporal correlatio n that surmounts the
ideal white-noise stochasticity. Conventional formulations (Frist on 2008b; Friston et al.
2010b) consider that colored noises are analytic (i.e., diﬀerentiable) to allow correlation
between the distinct dynamical orders of the continuous states. In practice, to furnish
a closed dynamics for a ﬁnite number of variables, the recursive equ ations of motion for
the continued generalized states need to be truncated at an arbit rary embedding order.
Our formulation considers the BM in the brain in terms of the standar d Newtonian
(Hamiltonian) construct; the drawback is that our theory does no t explore the nature
of temporal correlation in the assumed Gaussian noises in the gener ative processes.
Accordingly, our generative models assume and account for the wh ite noise describing
the Wiener processes. The delta-correlated white noise is mathema tically singular; they
need to be smoothed to describe fast biophysical processes. The re are approaches in
stochastic theories that formulate non-Markovian processes wit h colored noises without
resorting to generalized states of motion (van Kampen:1981; Fox 1 987; Risken 1989;
Moon and Wettlaufer 2014), which are not discussed here.
Instead, we discuss an approach to extend the phase-space dime nsion for the white
noise processes. At the level of the Hodgkin–Huxley description of the biophysical
brain dynamics, the membrane potential, gating variables, and ionic c oncentrations are
relevant coarse-grained brain variables (Hille 2001). Thus, if one em ploys the ﬂuctuating
Hodgkin-Huxley models with Gaussian white noises as neurophysically p lausible
generative models (Kim 2018), one can proceed with our Lagrangian (equivalently,
Hamiltonian) approach to formulate the RD in an extended phase spa ce. Such a state-
space augmentation is diﬀerent from and alternative to that in term s of the generalized
coordinates of motion [see Sect. 2.1], while accommodating only delta- correlated noises.
2.4. Lagrangian formulation of Bayesian mechanics (BM)
The (classical) “action” is deﬁned as an ordinary time-integral of the Lagrangian
for an arbitrary trajectory (Landau and Lifshitz 1976). Our for mulation of the BM
proposes the Laplace-encoded IFE — an upper bound on the senso ry surprisal — as
an informational Lagrangian and hypothesizes the time-integral o f the IFE — an upper
bound on the sensory Shannon uncertainty — as an informational a ction (IA). By
applying the principle of least action, we minimize the IA to ﬁnd a tight bo und for the
sensory uncertainty and derive the BM that performs the brain’s B ayesian inference of
the external cause of sensory data. In turn, we cast the workin g BM in our formulation
Bayesian mechanics of perceptual inference and motor control in the brain 8
as eﬀective Hamilton’s equations of motion in terms of position and mom entum in phase
space.
Meanwhile, the BM described in (Friston 2019) intuitively adopts the id ea of
Feynman’s path integral formulation (Feynman and Hibbs 2005). Th e Feynman’s
path integral formulation extends the idea of classical action to qu antum dynamics
and provides an approach to determine the “propagator” that sp eciﬁes the transition
probability between initial and ﬁnal states. The propagator is deﬁn ed as a functional
integral of the exponentiated action, which summates all possible trajecto ries connecting
initial and ﬁnal states. The description provided in (Friston 2019) id entiﬁes the
propagator using the probability density over neural states, and it makes the connection
to the Bayesian FEP. In this manner, the surprisal may be identiﬁed as a negative log of
the steady-state density in nonequilibrium ensemble dynamics (Parr et al. 2020), which
is governed by a Fokker-Plank equation. The generalized Bayesian ﬁ ltering scheme
(Friston et al. 2008c; Friston et al. 2010b) provides a continuous- state formulation of
minimizing the surprisal, and it delivers the BM in terms of the generalize d coordinates
of motion using the concept of gradient ﬂow.
In some technical details, the Lagrangian presented in (Friston 20 19), which is
the integrand in the classical action, encloses two terms. They are the quadratic term
arising from the state equation and the term involving a state-deriv ative (divergence
in three dimension) of the force, which appears in the Langevin-typ e state equation.
The former term is included in our Lagrangian but with an additional qu adratic term
from the observation equation. In contrast, the latter is not pre sent in our Lagrangian,
which is known to arise from the Stratonovich convention (Seifert 2 012; Cugliandolo
and Lecomte 2017).
2.5. Closure of the sensorimotor loop in active inference
The conventional FEP facilitates gradient descent minimization for t he mechanistic
implementation of active inference, which makes the motor-contro l dynamics available
in the brain’s RD (Friston et al. 2009; Friston et al. 2010c; Friston et al. 2011a). The
gradient-descent scheme is mathematically expressed as
9a“ ´ ∇aF Ñ ´ BF
Bϕ
dϕ
da, (3)
where a denotes an agent’s motor variable, and F represents the Laplace-encoded
IFE by the biophysical brain variables (Buckley and Kim et al. 2017). A n agent’s
capability of subjecting sensory inputs to motor control is conside red a functional
dependence ϕ “ ϕpaq in the environmental generative processes (Friston et al. 2009).
According to Eq. (3), an agent performs the minimization by eﬀectu ating the sensory
data dϕ{da and obtains the best result for motor inference when 9a “ 0, where the
condition BF{Bϕ “ 0 must be met. Because BF{Bϕ produces terms proportional
to the sensory prediction errors, the fulﬁllment of motor inferenc e is equivalent to
suppressing proprioceptive errors. Thus, motor control attem pts to minimize prediction
Bayesian mechanics of perceptual inference and motor control in the brain 9
errors, while prediction errors convey motor signals for control d ynamics; this forms a
sensorimotor loop. Some subtle questions arise here regarding the dynamical status of
the motor-control variable a: Equation (3) evidently handles a as a dynamical state;
however, the corresponding equation of motion governing its dyna mics is not given in
the environmental processes. Instead, the mechanism of motor control that vicariously
alters the sensory-data generation is presumed (Friston et al. 20 09). In addition,
motor variables are represented as the active states of the brain , e.g., motor-neuron
activities in the ventral horn of the spinal cord (Friston et al. 2010 c); however, they
are treated diﬀerently from other hidden-state representation s. Recall that the internal
state representations are expressed as generalized states, wh ereas the active states are
not.
In the following, we pose a semi-active inference problem that does n ot explicitly
address optimal motor control (motor inference) in the RD but en compasses the motor-
control signal as a time-dependent driving term arising from nonst ationary prediction
errors in the sensory-data cause.
3. Closed-loop dynamics of perception and motor control
The brain is not divided into sensory and motor systems. Instead, it is one inference
machine that performs the closed-loop dynamics of perception and motor control. Here,
we develop a framework of active inference within the scope of the le ast action principle
by employing the Laplace-encoded IFE as an informational Lagrang ian.
The environmental states ϑ undergo deterministic or stochastic dynamics by
obeying physical laws and principles. Here, we do not explicitly conside r their equations
of motion because they are hidden from the brain’s perspective, i.e., the brain as a
neural observer does not possess direct epistemic access. Similar ly, sensory data ϕ
are physically generated by an externally hidden process at a senso ry receptor, which
constitutes the brain-environment interface. However, to emph asize the eﬀect of an
agent’s motor control a on sensory generation, we facilitate the generative process of
sensory data using an instantaneous mapping
ϕ “ hpϑ,a q ` zgp, (4)
where hpϑ,a q denotes the linear or nonlinear map of input generation, and zgp represents
the noise involved. Note that an agent’s motor-control a is explicitly included in the
generative map. However, the neural observer is not aware of ho w the sensory streams
are eﬀectuated by the agent’s motion in the environment (Friston e t al. 2010c).
The FEP circumvents this epistemic diﬃculty by hypothesizing a forma l homology
between external physical processes and the corresponding int ernal models foreseen by
the neural observer (Friston et al. 2010c). Upon receiving senso ry-data inﬂux, the
brain launches R-density qpϑq to infer the external causes via variational Bayes. The R-
density is the probabilistic representation of the environment, who se suﬃcient statistics
are assumed to be encoded by neurophysical brain variables, e.g., n euronal activity or
Bayesian mechanics of perceptual inference and motor control in the brain 10
synaptic eﬃcacy. When a ﬁxed-form Gaussian density is considered for the R-density,
which is called Laplace approximation, only the ﬁrst-order suﬃcient s tatistic, i.e., the
mean µ is needed to specify the IFE eﬀectively (Buckley and Kim et al. 2017). The
brain continually updates the R-density using its internal dynamics, described here as
a Langevin-type equation
dµ
dt “ fpµq ` w, (5)
where fpµq represents the brain’s belief regarding the external dynamics enc oded by
a neurophysical driving mechanism of the brain variables µ, and w is random noise.
The sensory perturbations at the receptors are predicted by th e neural observer via the
instantaneous mapping
ϕpaq “ gpµq ` z, (6)
where the belief gpµq is encoded by the internal variables, and z is the associated noise.
Our sensory generative model provides a mechanism for sampling se nsory data ϕ using
the brain’s active states a, which represent an external motor control embedded in
Eq. (4). Note that Eq. (4) describes the environmental process es that generate sensory
inputs ϕ, while its homolog “Eq. (6)” prescribes the brains’ prior belief of ϕ that can be
altered by the active states a. The instantaneous state of the brain µ, which is speciﬁed
by Eq. (5), selects a particular R-density qpϑq when the brain seeks the true posterior
(the goal of perceptual inference). The motor control fulﬁlls th e prior expectations by
modifying the sensory generation via active-state eﬀectuation at the proprioceptors.
Through Laplace approximation (Buckley and Kim et al. 2017), the G- density
ppϕ,ϑ q is encoded in the brain as p“ ppϕ,µ q, where the sensory stimuli ϕ are predicted
by the neural observer µ via Eq. (6). Here, we argue that the physical sensory-recording
process is conditionally independent of the brain’s internal dynamics ; however, the brain
states must be neurophysically involved in computing the sensory pr ediction. In other
words, from the physics perspective, the sensory perturbation ϕ at the interface is a
source for exciting the neuronal activity µ. This observation renders the set of Eqs. (5)
and (6) to be dynamically coupled, and not conditionally independent. We incorporate
this conditional dependence into our formulation by introducing a st atistical coupling
via the covariance connection between the likelihood ppϕ|µq and prior ppµq that together
furnish the Laplace-encoded G-density.
For simplicity, we consider the stationary Gaussian processes for t he bivariate
variable Z as a column vector
Z ”
˜
w
z
¸
,
where w “ 9µ ´ fpµq and z “ ϕ ´ gpµq, and we specify the Laplace-encoded G-density
ppϕ,µ q “ ppϕ|µqppµq as
ppϕ,µ q “ 1a
p2πq2|Σ |
exp
ˆ
´ 1
2ZT Σ ´ 1Z
˙
, (7)
Bayesian mechanics of perceptual inference and motor control in the brain 11
where |Σ | and Σ ´ 1 are the determinant and the inverse of the matrix Σ, respectively;
ZT is the transpose of Z. The covariance matrix Σ for the above is given as
Σ “
˜
σw φptq
φptq σz
¸
,
where the stationary variances σi (i“ w,z ) and the transient covariance φ are deﬁned,
respectively, as
σwp0q “ x w2y, σzp0q “ x z2y, and φptq “ x wp0qzptqy.
With the prescribed internal model of the brain for the G-density, the Laplace-encoded
IFE can be speciﬁed as Fpϕ,µ q “ ´ ln ppϕ,µ q (for details, see Buckley and Kim et al.
2017). Then, it follows that
Fpϕ,µ ; tq “ 1
2mw p 9µ´ fpµqq2 ` 1
2mz pϕ ´ gpµqq2 (8)
´ ?mwmzρp 9µ´ fpµqq p ϕ ´ gpµqq
` 1
2 ln
`
2πp1 ´ ρ2qσwσz
˘
,
where ρ denotes the correlation function deﬁned as a normalized covarianc e
ρ” φ?σwσz
. (9)
Furthermore, we introduce notations mi pi“ w,z q as
mi ” 1
σip1 ´ ρ2q , (10)
which are precisions, scaled by the correlation in the conventional FEP.
Next, as proposed in (Kim 2018), we identify F as an informational Lagrangian L
within the scope of the principle of least action, and we deﬁne
L” 1
2mw p 9µ´ fpµqq2 ` 1
2mz pϕ ´ gpµqq2
´ ?mwmzρp 9µ´ fpµqq p ϕ ´ gpµqq , (11)
which is viewed as a function of µ and 9µ for the given sensory inputs ϕptq, i.e.,
L “ Lpµ, 9µ; ϕq. Note that we dropped the last term in Eq. (8) when translating F
into L because it can be expressed as a total time-derivative term that d oes not aﬀect
the resulting equations of motion (Landau and Lifshitz 1976). Then , the theoretical
action S that eﬀectuates the variational objective functional under the revised FEP is
set up as
Srµptqs “
ż
Fpµptq, 9µptq; ϕqdt. (12)
The Euler-Lagrange equation of motion, which determines the traj ectory µ “ µptq for
a given initial condition µp0q, is derived by minimizing the action δS” 0.
Equivalently, the equations of motion can be considered in terms of t he position
µ and its conjugate momentum p, instead of the position µ and velocity 9µ. We used
the terms position and velocity as a metaphor to indicate dynamical v ariables µ and
Bayesian mechanics of perceptual inference and motor control in the brain 12
9µ, respectively. For this, we need to convert Lagrangian L into Hamiltonian H by
performing a Legendre transformation
Hpµ,p q “ p9µ´ Lpµ, 9µq,
where pdenotes the canonical momentum conjugate to µ, which is calculated from Las
p“ BL
B 9µ “ mw p 9µ´ fq ´ ?mwmzpϕ ´ gqρ. (13)
After some manipulation, the functional form of H can be obtained explicitly as
Hpµ,p ; ϕq “ Tpµ,p ; ϕq ` Vpµ; ϕq, (14)
where we indicated its dependence on the sensory inﬂux ϕ. In addition, the terms T
and V on the RHS are deﬁned as
T ” p2
2mw
`
ˆ
ρ
c mz
mw
´
ϕ ´ gpµq
¯
` fpµq
˙
p, (15)
V ” 1
2mz
`
ρ2 ´ 1
˘ ´
ϕ ´ gpµq
¯2
. (16)
Here, T and V represent the kinetic and potential energies, respectively, which deﬁne
the informational Hamiltonian of the brain. Similarly, mw and mz represent the neural
inertial masse as a metaphor. Unlike that in standard mechanics, the second term in
the expression for kinetic energy is dependent on linear momentum a nd position.
We generate the Hamilton equations of motion, which are equivalent t o the
Lagrange equation using
9µ “ BH
Bp and 9p“ ´ BH
Bµ .
As described below, Hamilton’s equations are better suited for our p urposes because
they specify the RD as coupled ﬁrst-order diﬀerential equations o f the brain state µ
and its conjugate momentum p. In contrast, the Lagrange equation is a second-order
diﬀerential equation of the state variable (Landau and Lifshitz 197 6). The results are
9µ “ 1
mw
p` fpµq ` α∆ ϕ , (17)
9p “ ´
ˆ Bf
Bµ ´ βBg
Bµ
˙
p´
`
1 ´ γ2˘ Bg
Bµ∆ ϕ , (18)
where parameters α, β, and γ have been respectively deﬁned for notational convenience
as
α ” ρ?mzmw
, β ” mzα, and γ ” ρ?κ, (19)
where κ denotes the tuning parameter to spawn stability. In Eqs. (17) and (18), we
deﬁned the notation ∆ ϕ as
∆ ϕ pµ; tq ” mzpϕpaq ´ gpµqq. (20)
It measures the discrepancy between the adjustable sensory inp ut ϕ by an agent’s motor
control aand the top-down neural prediction gpµq, weighted by the neural inertial mass
mz.
Bayesian mechanics of perceptual inference and motor control in the brain 13
Below, we appraise the BM prescribed by Eqs. (17) and (18) and not e some
signiﬁcant aspects:
(i) The derived RD suggests that both brain activities µ and their conjugate momenta
pare dynamic variables. The instantaneous values of µ and pcorrespond to a point
in the brain’s perceptual phase space, and the continuous solution over a temporal
horizon forms an optimal trajectory that minimizes the theoretica l action, which
represents sensory uncertainty.
(ii) The canonical momentum pdeﬁned in Eq. (13) can be rewritten as p“ mw p 9µ´ fq´
ρ
a
mw{mz∆ ϕ . Accordingly, when the normalized correlation ρ is nonvanishing,
the momentum quantiﬁes combined errors in predicting changing sta tes and
sensory stimuli. Prediction errors propagate through the brain by obeying coupled
dynamics according to Eqs. (17) and (18).
(iii) Terms involving time-dependent ∆ ϕ in Eqs. (17) and (18) are identiﬁed as driving
forces Ci, i“ µ, p,
Cµ ” α∆ ϕ , (21)
Cp ” ´
`
1 ´ γ2˘ Bg
Bµ∆ ϕ . (22)
The sensory prediction error ∆ ϕ deﬁned in Eq. (20) quantiﬁes motor signals
engaging the brain’s nervous control in integrating the RD.
Equations (17) and (18) are the highlights of our formulation, which prescribe
the brain’s BM of semi-actively inferring the external causes of sen sory inputs under
the revised FEP. Note that the motor variable a is not explicitly included in our
derived RD; instead, it implicitly induces nonautonomous sensory inpu ts ϕptq in
the motor signal ∆ ϕ . The motor signal appears as a time-dependent driving force;
accordingly, our Hamiltonian formulation bears a resemblance to the motor-control
dynamics described by the Hamilton-Jacobi-Bellman (HJB) equation in the control
theory (Todorov 2007). If one regards the Lagrangian Eq. (11) as a negative cost rate
and the canonical momentum p as a costate, our IA is equivalent to the total cost
function that generates the continuous-state HJB equations. I n optimal control theory,
the associated Hamiltonian function is further minimized with respect to the control
signal, which we do not explicitly consider in this work. In our formulatio n, the motor
signals are produced by the discrepancy between the sensory str eams ϕptq and those
predicted by the brain. The nonstationary data are presented to a sensorimotor receptor,
whose ﬁeld position in the environment is speciﬁed by the agent’s locom otive motion.
The neural observer continuously integrates the BM subject to a motor signal to perform
the sensory-uncertainty minimization, thereby closing the percep tion and motion control
within a reﬂex arc. When we neglect the correlation ρ between the sensory prediction
modeled by Eq. (6) and the internal dynamics of predicting the neur onal state modeled
by Eq. (5), we can recover the RD reported in the previous publicat ion (Kim 2018),
which demonstrates the consistency of our formulation.
Bayesian mechanics of perceptual inference and motor control in the brain 14
In the present treatment, we consider only a single brain variable µ; accordingly,
the ensuing BM speciﬁed by Eqs. (17) and (18) is described in a two-d imensional phase
space. The extension of our formulation to the general case of th e multivariate brain
is possible by applying the same line of work proposed in (Kim 2018). Und er the
independent-particle approximation, the multivariate Lagrangian t akes the form
L” 1
2
Nÿ
α “ 1
”
mwα
´
9µα ´ fα ptµuq
¯2
` mzα
´
ϕα ´ gα ptµuq
¯2
´ 2ρα
?mwα mzα
´
9µα ´ fα ptµuq
¯´
ϕα ´ gα ptµuq
¯ı
, (23)
where tµu “ p µ1,µ 2, ¨ ¨ ¨ ,µ N q denotes a row vector of N brain states that respond to
multiple sensory inputs tϕu “ p ϕ1,ϕ 2, ¨ ¨ ¨ ,ϕ N q in a general manner. Note that our
proposed multivariate formulation is diﬀerent from the state-spac e augmentation using
the higher-order states [see Sect. 2.1]. In our case, multiple brain s tates are, for instance,
the membrane potential, gating variables, and ionic concentrations , that can be viewed
as the ﬂuctuating variables on a corase-grained time scale, inﬂunce d by Gaussian white
noises [see Sect. 2.3].
Furthermore, the implication of our formulation in the hierarchical b rain can be
achieved in a straightforward manner as in (Kim 2018), which adopts the bidirectional
facet in information ﬂow of descending predictions and ascending pr ediction errors
(Markov and Kennedy 2013; Michalareas et al. 2016). Note that in e nsuing
formulation, both descending predictions and ascending prediction errors will constitute
the dynamical states governed by the closed-loop RD in the functio nal architecture
of the brain’s sensorimotor system. This feature is in contrast to t he conventional
implementation of the FEP, which delivers the backward prediction — b elief propagation
— as neural dynamics and the forward prediction error as an instan t message passing
without causal dynamics (Friston 2010a; Buckley and Kim et al. 2017 ).
4. Simple Bayesian-agent model: Implicit motor control
In this section, we numerically demonstrate the utility of our formula tion using an agent-
based model, which is based on a previous publication (Buckley and Kim e t al. 2017).
Unlike that in the previous study, the current model does not emplo y generalized states
and their motions; instead, the RD is speciﬁed using only position µ and its conjugate
momentum p for incoming sensory data ϕ. Environmental objects invoking an agent’s
sensations can be either static or time dependent, and in turn, the time dependence
can be either stationary (not moving on average) or nonstationar y. According to the
framework of active inference, the inference of static propertie s corresponds to passive
perception without motor control a. Meanwhile, the inference of time-varying properties
renders an agent’s active perception of proprioceptive sensation s by discharging motor
signals ∆ ϕ via classic reﬂex arcs.
In the present simulation, the external hidden state ϑ is a point property, e.g.,
temperature or a salient visual feature, which varies with the ﬁeld p oint x. As the
Bayesian mechanics of perceptual inference and motor control in the brain 15
0 1 2 3 4 5 t
5
10
15
20
φ(t)
Figure 1. Inﬂux of stochastic sensory data ϕ ptq in the blue curve was generated by the
environmental process shown in Eq. (24), which instantly enters t he sensory receptor
located at the ﬁeld point xptq. The dashed curve represents the agent’s position at xptq
as a function of time, with its movement starting from xp0q “ 10. The dotted curve
represents the magnitude of the latent motor variable aptq that controls the agent’s
location. [All curves are in arbitrary units.]
simplest environmental map, we consider hpϑ,a q “ ϑpxpaqq and assume that the sensory
inﬂux at the corresponding receptor is given by
ϕ “ ϑ` zgp, (24)
where zgp denotes the random ﬂuctuation. The external property, e.g., te mperature, is
assumed to display a spatial proﬁle as
ϑpxq “ ϑ0{px2 ` 1q,
where ϑ0 denotes the value at the ﬁeld origin, and the desired environmental niche is
situated at x “ xd, where ϑpxdq “ ϑd. The biological agent that senses temperature
is allowed to navigate through a one-dimensional environment by exp loiting the hidden
property. The agent initiates its motion from xp0q, where the temperature does not
accord with the desired value. In this case, the agent must fulﬁll its allostasis at the
cost of biochemical energy by exploiting the environment based on
xptq “ xp0q `
ż t
0
apt1qdt1 , (25)
where aptq denotes a motor variable, e.g., agent’s velocity. The nonstationary sensory
data ϕptq are aﬀerent at the receptor subject to noise zgp; its time dependence is caused
by the agent’s own motion, i.e., ϕptq “ ϑpxpaptqqq, which is assumed to be latent to the
agent’s brain in the current model. With the prescribed sensorimoto r control, the rate
of sensory data averaged over the noise is related to the control variable as
9ϕ “ Bϕpxq
Bx a.
The neural observer is not aware of how sensory inputs at the pro prioceptor are
aﬀected by the motor reﬂex control of the agent. In the case of saccadic motor control
Bayesian mechanics of perceptual inference and motor control in the brain 16
(Friston et al. 2012b), an agent may stand at a ﬁeld point without ch anging its position;
however, sampling the salient visual features of the environment t hrough a fast eye
movement aptq makes the visual input nonstationary, i.e., ϕptq “ ϑpaptqq.
In Fig. 1, we depict streams of sensory data at the agent’s recept or as a function of
time. For this simulation, the latent motor variable in Eq. (25) is consid ered as
aptq “ ap0q et
p1 ` etq2 ,
which renders the agent’s position in the environment as xptq “ 2xp0q{p1 ` etq with
xp0q “ ´ 2ap0q. For simplicity, we assume that this is hardwired in the agent’s reﬂex
pathway over evolutionary and developmental time scales. The ﬁgu re shows that
the agent, initially located at xp0q “ 10, senses an undesirable stimulus ϑp0q “ 0.2;
accordingly, it reacts by using motor control to determine an acce ptable ambient niche.
For this illustration, we assumed the environmental property at th e origin to be ϑ0 “ 20.
After a period of ∆ t “ 5, the agent ﬁnds itself at the origin x “ 0, where the
environmental state is marked by the value ϑ “ 20.
Having prescribed the nonstationary sensory data, we now set up the BM to be
integrated by applying Eqs. (17) and (18) to the generative models below. We assume
that the agent has already learned an optimal generative model; th erefore, the agent
retains prior expectations regarding the observations and dynam ics. Here, for the
demonstration, we consider the learned generative model in its simp lest linear form
gpµq “ µ, (26)
fpµq “ ´p µ´ ϑdq. (27)
Note that the motor control a is not included in the generative model, and the desired
sensory data ϑd, e.g., temperature, appear as the brain’s prior belief of the hidden s tate.
Accordingly, Eqs. (17) and (18) are reduced to a coupled set of diﬀ erential equations for
the brain variable µ and its conjugate momentum p as
9µ “ ´ p µ´ ϑdq ` 1
mw
p` α∆ ϕ , (28)
9p “ p 1 ` βqp´
`
1 ´ γ2˘
∆ ϕ . (29)
Parameters α, β, and γ are proportional to the correlation ρ; see Eq. (19). Hence, they
become zero when the neural response to the sensory inputs is un correlated with neural
dynamics, which is not the case in general. Time-dependent driving te rms appearing on
the RHS of both equations, namely Eqs. (28) and (29), include the s ensorimotor signal
∆ ϕ pµ; ϕptqq given in Eq. (20). The motor variable a, which drives the nonstationary
inputs ϕptq, is unknown to the neural observer in our implementation.
In the following, for a compact mathematical description, we denot e the brain’s
perceptual state as a column vector
Ψ ”
˜
µ
p
¸
.
Bayesian mechanics of perceptual inference and motor control in the brain 17
10 20 30 40 t
-50
50
(t)
-150 -100 -50
-600
-500
-400
-300
-200
-100
p
Figure 2. Perceptual inference of static sensory data: (Left) Oscillatory brain
variables µ “ µptq in time t developed from a common spontaneous state pµp0q, p p0qq “
p0, 0q by responding to sensory inputs ϕ “ 10 (gray), 15 (red), and 20 (blue). The
horizontal dotted line indicates the agent’s prior belief regarding se nsory input. (Right)
Limit cycles in the perceptual state space from an input ϕ “ 4. 0 for three initial
conditions pµp0q, p p0qq “ p 0, 0q, p´ 20, ´ 100q, and p´ 40, ´ 200q; where the state space
is spanned by continuous brain state µ and its conjugate momentum p variables. The
common ﬁxed point is indicated by an orange bullet at the center of th e orbits, which
predicts the sensory cause incorrectly. [All curves are in arbitrar y units.]
Vector Ψ represents the brain’s current expectation µ and the associated prediction error
p with respect to the sensory causes, as encoded by the neuronal activities performed
when encountering a sensory inﬂux. Therefore, in terms of perce ptual vector Ψ, Eqs. (28)
and (28) are expressed as
dΨ
dt ` RΨ “ S, (30)
where relaxation matrix R is deﬁned as
R “
˜
1 ` β ´ 1
mw
pγ2 ´ 1qmz ´p1 ` βq
¸
, (31)
and source vector S encompassing the sensory inﬂux ϕptq is deﬁned as
S “
˜
ϑd ` βϕ
pγ2 ´ 1qmzϕ
¸
. (32)
Unless it is a pathological case, the steady-state (or equilibrium) so lution ψeq of Eq. (30)
is uniquely obtained as
Ψ eq “ R´ 1S ”
˜
µeq
peq
¸
. (33)
We ﬁnd it informative to consider the general solution Ψ ptq of Eq. (30) with respect to
the ﬁxed point ψeq by setting
ψptq ” Ψ ptq ´ Ψ eq.
To this end, we seek time-dependent solutions for the shifted meas ure ψptq as follows
dψ
dt ` Rψ “ δS,
Bayesian mechanics of perceptual inference and motor control in the brain 18
where δS “ Sptq ´ Sp8q. It is straightforward to integrate the above inhomogeneous
diﬀerential equation to obtain a formal solution, which is given by
ψptq “ e´ Rtψp0q `
ż t
0
dt1e´ Rpt´ t1 qδSpt1 q. (34)
Note that δS becomes zero identically for static sensory inputs; therefore, th e relaxation
admits simple homogeneous dynamics. In contrast, for time-varyin g sensory inputs, the
inhomogeneous dynamics driven by the source term is expected to b e predominant.
However, on time scales longer than the sensory-inﬂux saturation time τ, it can be
shown that δS Ñ 0; for instance, τ “ 5 in Fig. 1. Therefore, for such a time scale, the
inhomogeneous contribution in the relaxation diminishes even for time -varying sensory
inputs, and the homogeneous contribution is dominant for further time-development.
The ensuing homogeneous relaxation can be expressed in terms of e igenvalues λl and
eigenvectors ξplq of the relaxation matrix R as
ψptq “
2ÿ
l“ 1
cle´ λltξplq, (35)
where expansion coeﬃcients cl are ﬁxed by initial conditions ψp0q. The initial conditions
ψp0q represent a spontaneous or resting cognitive state. In Eq. (35) , eigenvalues and
eigenvectors are determined by the secular equation
Rξplq “ λlξplq. (36)
Then, the solution for the linear RD Eq. (30) is given by
Ψ ptq “ Ψ eq `
2ÿ
l“ 1
cle´ λltξplq , (37)
which is exact for perceptual inference, and legitimate for active in ference on timescales
tą τ.
Before presenting the numerical outcome, we ﬁrst inspect the na ture of ﬁxed points
by analyzing the eigenvalues of the relaxation matrix R given in Eq. (31). First, it
can be seen that the trace of R is zero, which indicates that the two eigenvalues have
opposite signs, i.e., λ1 “ ´ λ2. Second, the determinant of R can be calculated as
DetpRq “ ´ p 1 ` βq2 `
`
γ2 ´ 1
˘ mz
mw
.
Therefore, if the correlation φ Ñ 0, it can be conjectured that both eigenvalues are
real. This is because Det pRq “ λ1λ2 Ñ ´ 1 ´ mz{mw ă 0, which yields λ2
1 “ λ2
2 ą 0
using the ﬁrst conjecture. Thus, we can conclude that the two eig envalues are real and
have opposite signs. Therefore, for φ “ 0, the solution is unstable. In contrast, when
the correlation is retained, Det pRq can be positive for a suitable choice of statistical
parameters, namely mw, mz, and φ. In the latter case, the condition λ1λ2 ą 0 renders
λ2
l ă 0 for both l “ 1, 2. Accordingly, λ1 and λ2 that have opposite signs are purely
imaginary, which makes the ﬁxed point Ψ eq a center (Strogatz 2015). If we deﬁne
λ1, 2 ” ˘ iω, the long-time solution of RD with respect to Ψ eq is expressed as
ψptq “ c1eiωt ξp1q ` c2e´ iωt ξp2q ,
Bayesian mechanics of perceptual inference and motor control in the brain 19
which speciﬁes a limit cycle with angular frequency ω. Thus, according to our
formulation, the eﬀect of correlation on the brain’s RD is not a subsid iary but a crucial
component. Below, we consider numerical illustrations with ﬁnite cor relation.
We exploited a wide range of parameters for numerically solving Eqs. ( 28) and
(29) and found through numerical observation that there exists a narrow window in the
statistical parameters σw, σz, and φ, within which a stable trajectory is allowed for a
successful inference. This ﬁnding implies that the agent’s brain mus t learn and hardwire
this narrow parameter range over evolutionary and developmenta l timescales; namely,
generative models are conditioned on an individual biological agent. W e denote the
instantaneous cognitive state as pµptq,p ptqq for notational convenience.
In Fig. 2, we depict the numerical outcome from the perceptual inf erence of static
sensory inputs. To obtain the results, we select a particular set of statistical parameters
as
σw “ 1.0, σz “ 10, and φ “ ´ 2.8,
which specify the neural inertial masses
mw “ 4.6 and mz “ 0.1 ˆ mw
and the coeﬃcients that enter the RD, namely
α “ ´ 0.60,β “ ´ 0.28, and γ “ ´ 2.8 pκ “ 10q.
In Fig. 2Left, we depict the brain variable µ as a function of time, which represents the
cognitive expectation of a registered sensory input under the gen erative model [Eq. (26)]
for three values, namely ϕ “ 10, 15, and 20. For all illustrations, the agent’s prior belief
with regard to the sensory input is set as
ϑd “ 20,
which is indicated by the horizontal dotted line. The blue curve repre sents the case in
which sensory data are in line with the belief. The RD of the perceptua l inference
delivers an exact output pµeq,p eqq “ p 20, 0q; where µeq and peq are the perceptual
outcome of the sensory cause and its prediction error, respectiv ely. Note that µeq and
peq correspond to the temporal averages of µptq and pptq, respectively. The other two
inferences underscore the correct answer. Figure 2Right corre sponds to the case of a
single sensory data ϕ “ 4.0, which the standing agent senses at the ﬁeld point x “ 2.
The ensuing trajectories from all three initial spontaneous state s have their limit cycles
in the state space deﬁned by µ and p. We numerically determined the ﬁxed point to
be pµeq,p eqq “ p´ 65.6, ´306q and the two eigenvalues of the relaxation matrix R to be
pλ1,λ 2q “ p 1.84i, ´1.84iq, which are purely imaginary and have opposite signs. Again,
the perceptual outcome does not accord with the sensory input; it deviates signiﬁcantly.
Next, in Fig. 3, we depict the results for active inference, which wer e calculated
using the same generative parameters used in Fig. 2. The agent is init ially situated at
xp0q “ 2, where it senses the sensory inﬂux ϑp0q “ 4, which does not match the desired
value ϑd “ 20. Therefore, the agent reacts to identify a comfortable enviro nmental
Bayesian mechanics of perceptual inference and motor control in the brain 20
10 20 30 40 t
-20
-10
10
20
30
Figure 3. Active perception: Time-development of the perceptual state inf erring
the external causes of sensory inputs altered by the agent’s mot or control. Blue
and magenta curves depict the brain activity µptq and corresponding momentum pptq,
respectively. In addition, the noisy curve indicates the nonstation ary sensory inputs
ϕ ptq entering the sensory receptor at instant t. For numerical illustration, we used
σw “ 1. 0, σ z “ 10, and φ “ ´ 2. 8. [All curves are in arbitrary units.]
-50 50 100 μ
-300
-200
-100
100
200
300
p
Figure 4. Active inference: Temporal development of trajectories render ing stationary
limit cycles in the perceptual phase space, spanned by continuous n eural state µ and
its conjugate momentum p variables. Data were obtained from the same statistical
parameters used in Fig. 2. The blue, red, and gray curves corresp ond to the three
initial conditions, pµp0q, p p0qq “ p 0, 0q, p0, 100q, and p35, 0q, respectively. The angular
frequency of the limit cycles is the magnitude of the imaginary eigenva lues of the
relaxation matrix R given in Eq. (31). The common ﬁxed point is indicated by a
magenta bullet at the center of the orbits. [All curves are in arbitra ry units.]
niche matching its prior belief, which generates nonstationary sens ory inputs at the
receptors (Fig. 1). The brain variable µ initially undergoes a transient period at tď 5.
The RD commences from the resting condition pµp0q,p p0qq “ p 0, 0q and then develops a
stationary evolution. Furthermore, we numerically conﬁrmed that the brain’s stationary
Bayesian mechanics of perceptual inference and motor control in the brain 21
10 20 30 40 t
-20
20
40
60
Δφ
10 20 30 40 t
-40
-30
-20
-10
10
20
30
Δφ
Figure 5. Motor signals ∆ ϕ pµ; ϕ ptqq as a function of time t evoked by the discrepancy
between the nonstationary sensory stream and its top-down pre diction [Eq. (20)].
Here, we set the prior belief ϑd “ 20 (Left) and ϑd “ 10 (Right). The blue and
red curves represent the results from the initial condition pµp0q, p p0qq “ p 0, 0q and
p0, 100q, respectively. The gray curves represent the corresponding sig nals from the
plain perception of the static sensory input. All data were obtained by setting the
statistical parameters as σw “ 1. 0, σ z “ 10, and φ “ ´ 2. 8. [All curves are in
arbitrary units.]
prediction µeq, which is the brain’s perceptual outcome of the sensory cause, is c lose
to but not in line with the prior belief ϑd. The stationary value peq is estimated to be
approximately 8 .0, which is the average of the stationary oscillation of prediction err or
pptq.
In Fig. 4, the trajectory corresponding to that in Fig. 3 is illustrate d in blue in the
perceptual state space spanned by µ and p, including two other time developments from
diﬀerent choices of initial conditions. All data were calculated using t he same generative
parameters and sensory inputs used for Fig. 3. Regardless of the initial conditions, after
each transient period, the trajectories approach stationary limit cycles about a common
ﬁxed point, as seen in the case of static sensory inputs in Fig. 2Right . The ﬁxed point
Ψ eq and stationary frequency ω of the limit cycles are not aﬀected by initial conditions,
which are solely determined by the generative parameters mw, mz, and φ and the prior
belief ϑd for a given sensory input ϕ [Eqs. (33) and (36)]. In addition, we numerically
observed that the precise location of the ﬁxed points is stochastic , thereby reﬂecting the
noise from the nonstationary sensory inﬂux ϕ.
In the framework of active inference, motor behavior is attribute d to the inference of
the causes of proprioceptive sensations (Adams et al. 2013), and in turn, the prediction
errors convey the motor signals in the closed-loop dynamics of perc eption and motor
control. In Fig. 5, we depict the sensorimotor signals ∆ ϕ pµ; ϕptqq that appear as time-
dependent driving terms in Eqs. (28) and (29). In both ﬁgures, th e agent is assumed
to be initially situated such that it can sense the sensory data ϕp0q “ 4. After an
initial transient period elapses, the motor signals exhibit a stationar y oscillation about
average zero in Fig. 5 (Left), implying the successful fulﬁllment of t he active inference
of nonstationary sensory inﬂux matching the desired belief ϑd “ 20. The amplitude
of the motor signal shown by the blue curve is smaller than that show n by the red
curve, which is also reﬂected in the size of the corresponding limit cyc les in Fig. 4. The
Bayesian mechanics of perceptual inference and motor control in the brain 22
prediction-error signal from the plain perception exhibits an oscillat ory feature in the
gray curve, which arises from the stationary time dependence of t he brain variable µptq.
The amplitude shows a large variation caused by the signiﬁcant discre pancy between the
static sensory input ϕ “ 4 and its prior belief ϑd “ 20. In Fig. 5 (Right), we repeated
the calculation with another value: ϑd “ 10. In this case, the prior belief ϑd regarding
the sensory input does not accord with stationary sensory strea ms. Therefore, the blue
and red signals for active inference oscillate about the negatively sh ifted values from
average zero. In contrast to Fig. 5 (Left), the error-signal am plitude of the static input
is reduced because the diﬀerence between the sensory data and p rior belief decreases.
Next, we consider the role of correlation φ in the brain’s RD, whose value is
limited by the constraint |φ| ď ?σwσz. To this end, we select three values of φ for
the ﬁxed variances σw and σz, and we integrate the RD for active inference. In Fig. 6,
we present the resulting time evolution of the brain states µ for the initial condition
pµp0q,p p0qq “ p 0, 0q. In this ﬁgure, the conjugate momentum variables are not shown.
The noticeable features in the results include the changes in the ﬁxe d point and the
amplitude of the stationary oscillation with correlation. The average value of µptq in
the periodic oscillation corresponds to the perceptual outcome µeq of the sensory data
in the stationary limit. We remark that for all numerical data presen ted in this work,
we selected only negative values for φ. This choice was made because our numerical
inspection revealed that positive correlation does not yield stable so lutions.
In Fig. 7, as the ﬁnal numerical manifestation, we show the tempor al buildup of the
limit cycles in the perceptual phase space; however, this time, we ﬁx σw while varying
σz and φ. To generate the red, blue, and gray curves, the tuning paramet er κ was
selected as κ “ 50, 10, and 100, respectively. The resulting ﬁxed points are located
approximately at the center of each limit cycle, which are not shown. Similar to that
in Fig. 6, it can be observed that the positions of the ﬁxed point and a mplitudes of
oscillation are altered by variations in the statistical parameters. E vidently, a diﬀerent
set of parameters, namely σw, σz, and φ, which are the learning parameters encoded by
the brain, result in a distinctive BM of active inference.
Here, we summarize the major ﬁndings from the application of our fo rmulation to
a simple nonstationary model. The brain’s BM, i.e., Eqs. (28) and (29), employ linear
generative models given in Eqs. (26) and (27).
(i) The steady-state solutions of the RD turn out to be a center ab out which stationary
limit cycles (periodic oscillations) are formed as an attractor (Fristo n and Ao 2012a)
in the perceptual phase space, which constitute the brain’s noneq uilibrium resting
states.
(ii) The nonequilibrium stationarity stems from the pair of purely imagin ary eigenvalues
of the relaxation matrix with opposite signs, given by Eq. (31); the e qual magnitude
speciﬁes the angular frequency ω of the periodic trajectory.
(iii) Centers are determined by generative parameters and the prio r belief for a given
sensory input [Eq. (33)], which represents the outcome of active in ference and the
Bayesian mechanics of perceptual inference and motor control in the brain 23
10 20 30 40 t
-10
10
20
30
μ(t)
Figure 6. Time evolution of the brain variable µ. Here, we vary the correlation φ
for ﬁxed variances σw “ 1. 0 and σz “ 10. The red, blue, and gray curves correspond
to φ “ ´ 3. 0, ´ 2. 8, and ´ 2. 6, respectively. For all data, the agent is environmentally
situated at x “ 2, where it senses the transient sensory inputs ϕ ptq induced by the
motor reﬂexes at the proprioceptive level. The agent’s initial cognit ive state is assumed
to be pµp0q, p p0qq “ p 0, 0q, and the prior belief is set as ϑd “ 20. [All curves are in
arbitrary units.]
entailed prediction error.
(iv) The theoretical assumption of the statistical dependence of two generative noises
describing the brain’s expectation of the external dynamics and se nsory generation
is consequential to ensuring a stationary solution. Furthermore, based on numerical
experience, a negative covariance is necessary for obtaining stab le solutions using
the current model.
5. Concluding remarks
In the present study, we continued our eﬀort to make the FEP a mo re physically
principled formalism based on our previous publication (Kim 2018). We im plemented
the FEP in the scope of the principle of least action by casting the minim ization scheme
to the BM described by the eﬀective Hamiltonian equations in the neur al phase space.
We deconstructed some of the theoretical details in the ﬁrst part , which are embedded
in the formulation of the FEP, while comparing our approach with othe r currently
prevailing approaches. In the second half, we demonstrated our p roposed continuous-
state RD in the Bayesian brain using a simple model, which is biologically rele vant
to sensorimotor regulation such as motor reﬂex arcs or saccadic e ye movement. In our
theory, the time-integral of the induced IFE in the brain, not the in stant variational IFE,
performs as an objective function. In other words, our minimizatio n scheme searches
for the tight bound on the sensory uncertainty (average surpris al) and not the instant
sensory surprisal.
To present the novel aspects of our formulation, this study focu sed on the perceptual
inference of nonstationary sensory inﬂux at the interface. The n onstationary sensory
Bayesian mechanics of perceptual inference and motor control in the brain 24
-5 5 10 15 20 25 μ
-250
-200
-150
-100
-50
50
p
Figure 7. Limit cycles in the perceptual phase space spanned by the brain sta te µ
and its conjugate momentum p. Here, we considered several sets of σz and φ for a
ﬁxed σw “ 1. 0. The red, blue, and gray curves were obtained from pσz , φ q “ p 50, ´ 6. 6q,
p10, ´ 2. 8q, and p100, ´ 9. 5q, respectively. For all data, the agent’s initial cognitive state
is assumed to be pµp0q, p p0qq “ p 0, 0q, and the prior belief is set as ϑd “ 20. The agent
is environmentally situated at x “ 2, where it senses the transient sensory inputs ϕ ptq
induced by the motor reﬂexes at the proprioceptive level. [All curve s are in arbitrary
units.]
inputs were assumed to be unknown or contingent to the neural ob server without
explicitly engaging in motor-inference dynamics in the BM. Instead, w e considered that
the motor signals are triggered by the discrepancies between the s ensory inputs at the
proprioceptive level and their top-down predictions. They appear ed as nonautonomous
source terms in the derived BM, thus completing the sensorimotor d ynamics via
reﬂex arcs or oculomotor dynamics of sampling visual stimuli. This clos ed-loop
dynamics contrasts with the gradient-descent implementation, wh ich involves the double
optimization of the top-down belief propagation and the motor infer ence in message-
passing algorithms. In our present formulation, the sensorimotor inference was not
included; however, a mechanism of motor inference can be included e xplicitly by
considering a Langevin equation for a sensorimotor state. This pro cedure extends the
probabilistic generative model by accommodating the prior density f or motor planning
for active perception, which is similar to what was done in (Bogacz 202 0).
By integrating the Bayesian equations of motion for the considered parsimonious
model, we manifested transient limit cycles in the neural phase space , which numerically
illustrate the brain’s perceptual trajectories performing active p erception of the causes of
nonstationary sensory stimuli. Moreover, we revealed that ensuin g trajectories and ﬁxed
points are aﬀected by the input values of the learning parameters ( both diagonal and oﬀ-
diagonal elements of the covariance matrix) and prior belief regard ing sensory data. The
Bayesian mechanics of perceptual inference and motor control in the brain 25
idea of exploring the eﬀect of noise covariance was purely from the t heoretical insight
without a supporting empirical evidence, which allowed us to drive a st able solution
in perceptual and motor-control dynamics. We did not attempt to explicate in detail
the eﬀect of neural inertial masses (precisions) and correlation ( noise covariance) on the
numerically observed limit cycles. This was because of the numerical lim itation set by
the presented model, which permits stable solutions in a signiﬁcantly n arrow window of
statistical parameters. In neurosciences, it is commonly recogniz ed that neural system
dynamics implement cognitive processes inﬂuencing psychiatric stat es (Durstewitz et al.
2020). We hope that the key features of our manifestation will ser ve to motivate and
guide further investigations on more realistic generative models with neurobiological
and psychological implications.
Finally, we mention the recent research eﬀorts on synthesizing per ception, motor
control, and decision making within the FEP (Friston et al. 2015; Frist on et al. 2017;
Biel et al. 2018; Parr and Friston 2019; van de Laar and de Vries 201 9; Tschantz
et al. 2020; Da Costa et al. 2020a). The underlying idea of these stu dies is rooted
in machine learning (Sutton and Barto 1998) and the intuition from no nequilibrium
thermodynamics (Parr et al. 2020; Friston 2019), and they attem pt to widen the scope
of active inference by incorporating prior beliefs regarding behavio ral policies. The new
trend supplements the instant IFE to the future expected IFE in a time series, and
it formulates the adaptive decision-making processes in action-orie nted models. The
assimilation of this feature needs to be studied in depth (Millidge et al. 2 020b; Tschantz
et al. 2020). We are currently considering a formulation of motor inf erence together
with the assimilation of extended IFEs in the scope of the least action principle.
Acknowledgments
This is a post-peer-review, pre-copyedit version of an article publis hed in
Biological Cybernetics. The ﬁnal authenticated version is available o nline at:
https://doi.org/10.1007/s00422-021-00859-9.
References
[1] Adams RA, Shipp S, Friston KJ (2013) Predictions not commands: active inference in the motor
system. Brain Struct Funct 218:611–643. https://doi.org/10.100 7/s00429-012-0475-5
[2] Amari S (1998) Natural gradient works eﬃciently in learning. Neu ral Comput 10(2): 251–276.
https://doi.org/10.1162/089976698300017746
[3] Balaji B, Friston K (2011) Bayesian state estimation using gener alized coordinates. Proc.
SPIE 8050, Signal Processing, Sensor Fusion, and Target Recogn ition XX, 80501Y.
https://doi.org/10.1117/12.883513.
[4] Baltieri M, Buckley CL (2019) PID control as a process of active inference with linear generative
models. Entropy 21:257
[5] Biehl M, Guckelsberger C, Salge C, Smith SC, Polani D (2018) Expa nding the active inference
landscape: More intrinsic motivations in the perception-action loop. Front Neurorobot 12:45.
doi: 10.3389/fnbot.2018.00045
Bayesian mechanics of perceptual inference and motor control in the brain 26
[6] Bogacz R (2017) A tutorial on the free-energy framework for modelling perception and learning.
J Math Psychol 76(B):198–211. https://doi.org/10.1016/j.jmp.20 15.11.003
[7] Bogacz R (2020) Dopamine role in learning and action inference. eL ife 9:e53262. DOI:
https://doi.org/10.7554/eLife.53262
[8] Buckley CL, Kim CS, McGregor S, Seth AK (2017) The free energy principle
for action and perception: A mathematical review. J Math Psychol 81:55–79.
https://doi.org/10.1016/j.jmp.2017.09.004
[9] Colombo M, Wright C (2018) First principles in the life sciences: The f ree-energy principle,
organicism, and mechanism. Synthese. https://doi.org/10.1007/s 11229-018-01932-w
[10] Cover T, Thomas JA (2006) Elements of Information Theory 2n d ed. Wiley-Interscience, Hoboken
[11] Cugliandolo LF and Lecomte V (2017) Rules of calculus in the path in tegral representation of white
noise Langevin equations: the Onsager–Machlup approach. J Phys A: Math Theor 50:345001
[12] Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (2020 a) Active inference on discrete
state-spaces: A synthesis. J Math Psychol 99:102447. https:// doi.org/10.1016/j.jmp.2020.102447
[13] Da Costa L, Parr T, Sengupta B, Friston K (2020b) Natural se lection ﬁnds natural gradient.
arXiv:200108028 [q-bio]
[14] Da Costa L, Sajid N, Parr T, Friston K, Smith R (2020c) The relat ionship between dynamic
programming and active inference: The discrete, ﬁnite horizon cas e. arXiv:2009.08111v3 [cs.AI]
[15] de Gardelle V, Waszczuk M, Egner T, Summerﬁeld C (2013) Concu rrent repetition enhancement
and suppression responses in extrastriate visual cortex. Cereb ral Cortex 23(9):2235–2244.
https://doi.org/10.1093/cercor/bhs211
[16] Durstewitz D, Huys Q, Koppe G (2020) Psychiatric illnesses as dis orders of network dynamics. Bi-
ological Psychiatry: Cognitive Neurosci Neuroimg. https://doi.org /10.1016/j.bpsc.2020.01.001
[17] Elfwing S, Uchibe E, Doya K (2016) From free energy to expecte d energy: Improving energy-
based value function approximation in reinforcement learning. Neur al Networks 84:17–27.
http://dx.doi.org/10.1016/j.neunet.2016.07.013
[18] Feynman RP and Hibbs AR (2005) Quantum mechanics and path int egrals Emended Edition.
Dover Publication, Mineola
[19] Fox RF (1987) Stochastic calculus in physics. J Stat Phys 46:114 5–1157.
https://doi.org/10.1007/BF01011160
[20] Friston K, Mattout J, Trujillo-Barreto N, Ashburner J, Penny W (2007) Varia-
tional free energy and the Laplace approximation. NeuroImage 34 (1):220–234.
https://doi.org/10.1016/j.neuroimage.2006.08.035.
[21] Friston K (2008a) Hierarchical models in the brain. PLoS Comput Biol 4(11): e1000211.
doi:10.1371/journal.pcbi.1000211
[22] Friston KJ (2008b) Variational ﬁltering. Neuroimage 41:747–76 6
[23] Friston KJ, Trujillo-Barreto N, Daunizeau J (2008c). DEM: a va riational treatment of dynamic
systems. Neuroimage 41(3): 849–885
[24] Friston KJ, Daunizeau J, Kiebel SJ (2009) Reinforcement learn ing or active inference?. PLoS ONE
4(7):e6421. doi:10.1371/journal.pone.0006421
[25] Friston K (2010a) The free-energy principle: a uniﬁed brain the ory? Nature Rev Neurosci 11:127–
138
[26] Friston K, Stephan K, Li B, Daunizeau J (2010b) Generalized ﬁlt ering. Math Problem in Eng
2010:261670.
[27] Friston KJ, Daunizeau J, Kilner J, Kiebel SJ (2010c) Action and b ehavior: a free-energy
formulation. Biol Cybern 102(3):227–260
[28] Friston K, Mattout J, Kilner J (2011a) Action understanding an d active inference. Biol Cybern
104:137–160.
[29] Friston K (2011b) What is optimal about motor control? Neuron 72(3):488–498. DOI
10.1016/j/neuron.2011.10.018
[30] Friston K, Ao P (2012a) Free Energy, value, and attractors. Comput Math Methods Med
Bayesian mechanics of perceptual inference and motor control in the brain 27
2012:937860. https://doi.org/10.1155/2012/937860
[31] Friston K, Adams R, Perrinet L, Breakspear M (2012b) Percep tions as hypotheses: saccades as
experiments. Front in Psychol 3:151. https://doi.org/10.3389/fp syg.2012.00151
[32] Friston K (2013) Life as we know it. J R Soc Interface 10:102013 0475
http://doi.org/10.1098/rsif.2013.0475
[33] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G (2015) Active inference and
epistemic value. Cogn Neurosci 6:187–214. doi: 10.1080/17588928 .2015.1020053
[34] Friston KJ, Parr T, de Vries B (2017) The graphical brain: Belief propagation and active inference.
Network Neurosci 1(4):381–414
[35] Friston K (2019) A free energy principle for a particular physics . arXiv:190610184 [q-bio]
[36] Hille, B. (2001) Ion channels of excitable membranes 3rd Edition. Sinauer Associates, Sunderland
[37] Huang Y, Rao RPN (2011) Predictive coding. WIREs Cogni Sci 2:5 80–593. DOI: 10.1002/wcs.142
[38] Isomura T, Kotani K, Jimbo Y (2015) Cultured cortical neuron s can perform blind source
separation according to the free-energy principle. PLoS Comput B iol 11(12):e1004643.
doi:10.1371/journal.pcbi.1004643
[39] Isomura T and Friston K (2018) In vitro neural networks minimis e variational free energy. Sci Rep
8:16926. https://doi.org/10.1038/s41598-018-35221-w
[40] Jazwinski AH (1970) Stochastic process and ﬁltering theory. Academic Press, New York
[41] Kerr W and Graham A (2000) Generalized phase space version of Langevin equa-
tions and associated Fokker-Planck equations. Eur. Phys. J. B 15 :305–311.
https://doi.org/10.1007/s100510051129
[42] Kiefer AB (2020) Psychophysical identity and free energy. J R Soc Interface 17: 20200370.
http://dx.doi.org/10.1098/rsif.2020.0370
[43] Kim CS (2018) Recognition dynamics in the brain under the free en ergy principle. Neural Comput
30:2616-2659. https://doi.org/10.1162/neco a 01115
[44] Kozunov VV, West TO, Nikolaeva AY, Stroganova TA, Friston KJ (2020) Object recognition is
enabled by an experience-dependent appraisal of visual feature s in the brain’s value system.
Neuroimage 221:117143. https://doi.org/10.1016/j.neuroimage.20 20.117143
[45] Kuzma S (2019) Energy-information coupling during integrative cognitive processes. J Theor Biol
469:180–186. https://doi.org/10.1016/j.jtbi.2019.03.005
[46] Landau LD, Lifshitz EM (1976) Mechanics: Volume 1 (Course of T heoretical Physics S) 3rd
Edition. Elsevier Ltd, Amsterdam
[47] Markov NT, Kennedy H (2013) The importance of being hierarch ical. Curr Opin Neurobiol
23(2):187–194 doi:10.1016/j.conb.2012.12.008
[48] Michalareas G, Vezoli J, van Pelt S, Schoﬀelen JM, Kennedy H, Fr ies P (2016) Alpha-beta and
gamma rhythms subserve feedback and feedforward inﬂuences a mong human visual cortical
areas. Neuron 89(2):384–397 doi:10.1016/j.neuron.2015.12.018
[49] Millidge B, Tschantz A, Seth AK, Buckley CL (2020a) On the Relatio nship between active inference
and control as inference. arXiv:2006.12964v3 [cs.AI]
[50] Millidge B, Tschantz A, Buckley CL (2020b) Whence the expected free energy? arXiv:2004.08128
[cs.AI]
[51] Moon W, Wettlaufer J (2014) On the interpretation of Straton ovich calculus. New J Phys
16:055017. http://dx.doi.org/10.1088/1367-2630/16/5/055017
[52] Ozaki T (1992) A bride between nonlinear time series models and n onlinear stochstic dynamical
systems: Alocal linearization approach. Statistica Sinica 2:113—135
[53] Parr T, Friston KJ (2018) Active inference and the anatomy of oculomotion. Neuropsychologia
111:334–343. https://doi.org/10.1016/j.neuropsychologia.2018.0 1.041
[54] Parr T, Friston KJ (2019) Generalised free energy and active in ference. Biol Cybern 113:495–513.
https://doi.org/10.1007/s00422-019-00805-w
[55] Parr T, Da Costa L, Friston K (2020) Markov blankets, informa tion geometry and stochastic
thermodynamics. Phil Trans R Soc A 37820190159. http://doi.org/ 10.1098/rsta.2019.0159
Bayesian mechanics of perceptual inference and motor control in the brain 28
[56] Pavliotis GA (2014) Stochastic processes and applications: diﬀu sion processes the Fokker-Planck
and Langevin equations. Springer, New York
[57] Ramstead MJD, Badcock PB, Friston KJ (2018) Answering Schr ¨ odinger’s question: A free-energy
formulation. Phys Life Rev 24:1–16. https://doi.org/10.1016/j.plre v.2017.09.001
[58] Ramstead MJD, Constant A, Badcock PB, Friston KJ (2019) Va riational ecology and the physics
of sentient systems. Phys Life Rev 31:188–205. https://doi.org/1 0.1016/j.plrev.2018.12.002
[59] Rao RPN, Ballard DH (1999) Predictive coding in the visual cortex : a functional
interpretation of some extra-classical receptive-ﬁeld eﬀects. N at Neurosci 2(1):79–87.
https://doi.org/10.1038/4580
[60] Risken H (1989) The Fokker-Planck Equation 2nd edition. Spring er-Verlag, Berlin
[61] Sanders H, Wilson MA, Gershman SJ (2020) Hippocampal remapp ing as hidden state inference.
eLife 9:e51140. https://doi.org/10.7554/eLife.51140
[62] Seifert U (2012) Stochastic thermodynamics, ﬂuctuation the orems and molecular machines. Rep
Prog Phys 75:126001. http://dx.doi.org/10.1088/0034-4885/75/ 12/126001
[63] Sengupta B, Tozzi A, Cooray GK, Douglas PK, Friston KJ (2016 ) Towards a neuronal gauge
theory. PLoS Biol 14(3): e1002400. https://doi.org/10.1371/jo urnal.pbio.1002400
[64] Sengupta B, Friston K (2017) Approximate Bayesian inference as a gauge theory.
arXiv:1705.06614v2 [q-bio.NC]
[65] Shimazaki H (2019) The principles of adaptation in organisms and machines I: machine learning,
information theory, and thermodynamics. arXiv:1902.11233
[66] Strogatz SH (2015) Nonlinear dynamics and chaos: with applicat ions to physics, biology, chemistry,
and engineering (Studies in Nonlinearity) 2nd Edition. Westview Press , Cambridge
[67] Surace SC, Pﬁster JP, Gerstner W, Brea J (2020) On the choic e of metric in
gradient-based theories of brain function. PLoS Comput Biol 16(4 ):e1007640.
https://doi.org/10.1371/journal.pcbi.1007640
[68] Sutton RS, Barto AG (1998) Reinforcement learning: An Intro duction. The MIT Press, Cambridge
[69] Todorov E (2007) Optimal control theory. Bayesian brain: pr obabilistic approaches to neural
coding. 269–298. The MIT Press, Cambridge
[70] Tschantz A, Seth AK, Buckley CL (2020) Learning action-orien ted models through active inference.
PLOS Comput Biol 16(4):E1007805. https://doi.org/10.1371/jour nal.pcbi.1007805
[71] Tuthill JC, Azim E (2018) Proprioception. Current Biol 28(5):R1 94–R203
doi:10.1016/j.cub.2018.01.064
[72] van Kampen NG (1981) Itˆ o versus Stratonovich. J Stat Phys 24:175–187.
https://doi.org/10.1007/BF01007642
[73] van de Laar TW, de Vries B (2019) Simulating active inference pro cesses by message passing.
Front Robot and AI 6:20. https://doi.org/10.3389/frobt.2019.000 20