0102
guA
13
]MI.hp-ortsa[
3v8682.4001:viXra
Inference with minimal Gibbs free energy in information field theory
Torsten A. Enßlin and Cornelius Weig
Max-Planck-Institut fu¨r Astrophysik, Karl-Schwarzschild-Str. 1, 85741 Garching, Germany
(Dated: October23, 2018)
Non-linear and non-Gaussian signal inference problems are difficult to tackle. Renormalization
techniquespermitustoconstructgood estimators fortheposterior signalmean withininformation
field theory (IFT), but the approximations and assumptions made are not very obvious. Here we
introduce the simple concept of minimal Gibbs free energy to IFT, and show that previous renor-
malization results emergenaturally. They can beunderstood asbeingtheGaussian approximation
tothefullposterior probability,whichhasmaximal crossinformation with it. Wederiveoptimized
estimators for three applications, to illustrate the usage of the framework: (i) reconstruction of a
log-normal signal from Poissonian data with background counts and point spread function, as it
is needed for gamma ray astronomy and for cosmography using photometric galaxy redshifts, (ii)
inferenceofaGaussiansignalwithunknownspectrumand(iii)inferenceofaPoissonianlog-normal
signal with unknown spectrum, the combination of (i) and (ii). Finally we explain how Gaussian
knowledge states constructed by the minimal Gibbs free energy principle at different temperatures
can be combined into a more accurate surrogate of thenon-Gaussian posterior.
I. INTRODUCTION gle signal estimate. Minimizing the Gibbs free energy
maximizes the entropy within the constraints given by
the internal energy. The latter is understood as the av-
A. Abstract inference problem
erage of the negative logarithm of the joint probability
function of signal and data weighted with the PDF.
Measurements provide information on the signals we
The usage of thermodynamical concepts for inference
are interested in, encoded in the delivered data. How
problems is not new, see e.g. [1, 2]. What is new here,
canthisinformationbebestretrieved? Isthereageneric
is that we develop this for signals which are fields, spa-
and simple principle from which optimal data analysis
tially distributed quantities with an infinite number of
strategies derive? Can an information energy be con-
degrees of freedom, while using an approximate Gaus-
structedwhich–ifminimized–providesuswiththecor-
sian ansatz for the PDF to be inferred. We thereby con-
rect knowledge state given the data and prior informa-
nectinformationfieldtheory(IFT)[3–10],asastatistical
tion? Andifthisexists,howcanthisinformationground
field theory dealing with a huge number of microscopic
state be found at least approximatively?
degrees of freedom, to thermodynamics, as a means to
Aninformationenergy,tobeminimized,wouldbevery
generate simplified, but macroscopic descriptions of our
useful to have, since many of the existing minimization
knowledge. Thereby we find that former IFT results ob-
techniques, analytical and numerical, can then be ap-
tained with complex renormalization schemes in [9, 11]
plied to it. A number of suchfunctions to be extremized
can easily be reproduced, and even be extended to more
to solve inference problems were proposed in the liter-
complicated measurement situations.
ature, like the likelihood, the posterior, or the entropy.
In the remainder of Sect. I we briefly introduce to
The likelihood is the probability that the data has re-
IFT, MAP, and Maximum Entropy. This motivates the
sulted from some signal. The posterior is the reverse, it
minimal Gibbs free energy principle, which we formally
istheprobabilitythatgiventhedatasomesignalwasthe
derive in Sect. II, and show its equivalence to maximal
origin of it. Extremizing either of them certainly makes
cross information. The application of this principle to
sense,butoftenignoresthepresenceofslightlylessprob-
optimize approximations of the posterior of concrete in-
able, but much more numerous possibilities in the signal
ference problems is provided in Sect. III. There, the
phase space. Those have a much larger entropy and are
log-normal Poisson problem (Sect. IIIA) and the prob-
therefore favored by maximum entropy methods. How-
lemtoreconstructwithoutknownsignalpowerspectrum
ever, maximum entropy alone can not be the inference
(Sect. IIIB), as well as their combination (Sect. IIIC)
determining criterion, since it favors states of complete
are addressed. Finally, we show how approximate poste-
lack of knowledge, irrespective of the data. Thus some
riorsobtainedatdifferenttemperaturescanbecombined
counteractingenergyisrequiredwhichprovidestheright
into a better posterior surrogate in Sect. IV before we
amount of force to the inference solution. Here, we ar-
conclude Sect. V.
gue that the ideal information energy is provided by the
Gibbs free energy, which combines both maximum en-
tropy and maximum a posteriori (MAP) principles.
The Gibbs free energy has to be regarded as a func- B. Information field theory
tionaloverthespaceofpossibleprobabilitydensityfunc-
tions (PDF) of the signal given the data. The result of Information theory describes knowledge states with
theminimizationisthereforeaPDFitself,andnotasin- probabilities. If Ω is the complete set of possibilities,
2
and A ⊂ Ω is a subset, then P(A) ∈ [0,1] describes the definesthegroundstateenergy. Thisrelationcane.g.be
plausibility of A being the case, with P(A) = 1 denot- used to check the correctness of an implementation of a
ing A being assumed to be sure, P(A) = 0 denoting A signal phase-space sampling algorithm.
being (assumed to be) impossible, and 0 < P(A) < 1
describing uncertainty about the truth of A. Obviously
P(Ω) = 1 and P(∅) = 0. The usual rules of proba- C. Maximum a posteriori
bility theory apply, and generalize the binary logic of
Aristotle to different degrees of certainty or uncertainty The first guess for a suitable energy to be minimized
[12, 13]. In case the set of possibilities is a continuum, to obtain the information state might be the Hamilto-
it makes sense to introduce a PDF P(ψ) over Ω, so that nian. Minimizing the Hamiltonian with respect to s,
P(A) = dψP(ψ). Each possible state ψ can be a while keeping d at their observed values, is equivalent
A
multi-component vector, containing all aspects of reality to maximizing the joint probability P(d,s) and also the
R
which are in the focus of our inference problem. posteriorP(s|d). Theclassicalfieldemergingfromthisis
We might be interested in a sub-aspect of ψ which called the MAP signal reconstruction in signal process-
we call our signal s = s(ψ). The induced signal PDF ing. For a detailed discussion of the usage of the MAP
is retrieved from a functional or path integral over all principle in IFT see [10]. The MAP field is often a very
the phase spaces of the possibilities of ψ via P(s) = good approximationof the mean field
DψP(ψ)δ(s − s(ψ)). If s is a field, a function over
a physical space V, then s = (s ) might be a vector m=hsi ≡ DssP(s|d), (5)
i R n the Hilbert space Ω ofall L2- x int x e ∈ g V rablefunctions over (s|d) Z
V and P(s) is then a probability density functional. In- whichistheoptimalestimatorofthesignalinastatistical
formationtheoryforsbecomesIFT,whichisastatistical L2 error norm sense:
field theory.
Inference on the signal s from data d is done from the m=argmin s˜ dx(s x −s˜ x )2 . (6)
posterior probability P(s|d), which can be constructed (cid:28)ZV (cid:29)(s|d)
from the joint PDF of signal and data P(d,s) via The MAP estimator on the other hand can be shown
to optimize the statistical L0 norm1, the result of which
P(d,s) e−βH(d,s) may strongly deviate from the mean m, if the posterior
P(s|d)= = , (1)
P(d) Z is highly asymmetric aroundits maximum. Thus we can
β (cid:12)β=1
(cid:12) regardtheMAPestimatorasagoodreferencepoint,but
(cid:12)
where P(d,s) = DψP(d|ψ)δ(s −(cid:12)s(ψ))P(ψ) = not as the solution we are seeking in general. It is, how-
Ω
P(d|s)P(s) and P(d)= DsP(d,s). The second equal- ever, accurate (in the L2 error norm sense) in case the
R
ityin(1)isjustarenamingofthenumeratoranddenomi- posterior aroundits maximum is close to a Gaussian. In
R
natorofthefirstfraction,whichhighlightstheconnection this case, the MAP field can easily be augmented with
to statistical mechanics. Thus we define the information some uncertainty information from the Hessian of the
Hamiltonian Hamiltonian
δ2H(d,s)
H(d,s)=−logP(d,s), (2) H= , (7)
δsδs†
(cid:12)s=m
the partition function including a moment generating (cid:12)
asanapproximationofthe twop(cid:12)ointfunctionofthe sig-
source term J (cid:12)
nal uncertainty
Z (d,J)= Dse−β(H(d,s)+J†s), (3) D ≡ (s−m)(s−m)† . (8)
β
(s|d)
Z
Thus we set D ≈H(cid:10)−1 in (cid:11)
and the inverse temperature β = 1/T as usual in statis-
tical mechanics. Here s† is the transposed and complex P(s|d)≈P˜(s|d)=G(s−m,D), (9)
conjugated signal vector s, leading to a scalar product
j†s = dx¯j s . The ad-hoc notion of temperature is where we introduced the Gaussian
a to s n in ar s r t R o a V w nd ( a fo rd r x T s x im < u 1 la ) t o e r d w a i n d n e e n al ( i f n o g r p T ra > ct 1 ic ) e t . he It e p x e p r l m or i e t d s G(φ,D)≡ |2π 1 D|1 2 e− 2 1φ†D−1φ. (10)
phase space region with respect to the one of the joint
Unfortunately, the MAP estimator can perform subop-
PDF and therefore is a useful auxiliary parameter. We
timally in cases where the Gaussian approximation does
show in Sect. IIA that the wellknownthermodynamical
not hold, see e.g. [11].
equipartition theorem holds:
1
hH(s, d)i −H(m, d)≈ N T. (4)
(s|d) 2 dgf
1 The L0 norm measures the amount of exact agreement via
where N dgf is the number of degrees of freedom and m kfk0 =limε→0 1 ε dxθ(f2(x)−ε2),withθ denoting theHeavi-
is the mean signal field as defined below in (5), which sidefunction.
R
3
D. Maximum Entropy priorstronglysuppresseslargevaluesinthe MiEmap. If
a data feature can be either explained by a single map
1. Image entropy pixelexhibitingapeakvalueorbyseveralpixelsdividing
thatvalueamongthemselves,MiEwillusuallypreferthe
second option, leading to blurred reconstructed images.
Another quantity often extremized in image recon-
We conclude, that the term maximum entropy com-
struction problems is the so-called image entropy (iE)
monlyusedinimagereconstructionisverymisleading. A
[14–25]. In classical maximum (image) entropy (MiE
more accurate term would be minimal dynamical range,
here,usuallyME)methodstheiEisdefinedforastrictly
since the implicitly assumed prior states that pixels car-
positive signal via
ryinglargerthanaveragesignals areextremelyunlikely.
x
S (s)=− dxs log(s /s˜ )≡−s†log(s/s˜), (11)
iE x x x
ZV 2. Physical entropy
where s˜ is the reference image, which is used to model
x
some prior information. In the second equality we have A physical entropy should measure the distribution
defined the component-wise application of functions on spread of a PDF using a phase space integral over its
fields, e.g. (f(s)) x = f(s x ), which we use throughout phasespace. Infact,thelatterisgivenbytheBoltzmann
this work. entropy as given by the negative Shannon information,
Wenote,thattheiEisactuallynotaphysicalentropy.
Usuallyits usageis arguedforbyadhoc assumptionson
S =− DsP(s|d) logP(s|d), (15)
B
the distributionentropyofphotonpackagesinthe image
Z
plane, rather than being a well motivated description of
which is a functional of the signal posterior, S =
the signal prior knowledge (or lack thereof). In the fol- B
S [P(s|d)], and not of the signal map. Inserting (1)
lowingwewillrevealtheimplicitlyassumedpriorofMiE B
yields
methods.
The data enter the MiE method in form of an image
S =hH(d,s)i +log Z (d,0)=U −F, (16)
energy, which is ideally chosen to be the negative log- B (s|d) 1
likelihood,
where we introduced the internal energy U =
hH(d,s)i andtheHelmholtzfreeenergyF =F (d,0)
E(d|s)=−log P(d|s) , (12) (s|d) 1
with
in order to ensure the best im(cid:0)print o(cid:1)f the data on the
1
reconstruction. The entropy is then maximized with the F β (d,J)=− log Z β (d,J). (17)
β
energy constraint given by minimizing
The fully J-dependent Helmholtz free energy provides
E (d,s)=E(d|s)−T S (s) (13)
iE iE the field expectation value via
with respect to s. Here T is some adjustable ∂F (d,J)
β
temperature-like parameter, permitting us to choose the m=hsi = . (18)
(s|d) ∂J
relative weight of image entropy and image energy. Low (cid:12)β=1,J=0
(cid:12)
temperature means that the MiE map follows the data (cid:12)
The entropy is also given in terms o(cid:12)f the free energy via
closely,hightemperaturethatthemapspacewantstobe
more uniformly occupied by the signal reconstruction. ∂F (d,J)
β
S = . (19)
The prior information on the signal, P(s), does not B
∂β
enter the MiE formalismexplicitly. Actually, an implicit (cid:12)β=1,J=0
(cid:12)
prior can be identified, assuming that MiE is actually The entropy as well as the free (cid:12) (cid:12)energy are functionals of
a MAP principle. In that case the implicitly assumed the posterior and not of the signal. Maximizing or min-
Hamiltonian is H iE (d,s) ∼ = E iE (d,s), where ∼ = denotes imizing them does not provide a signal estimator, but
equalityuptoanirrelevant,sinces-independent,additive singles out a PDF. If we restrict the space of PDFs to
constant, and we find the ones we can handle analytically, namely Gaussians
as given in (9) and (10), we might obtain a suitable ap-
P (s)∝eTSiE(s) ∝ s x
−Tsx
. (14) proximationscheme to the full field theoreticalinference
iE
s˜ problem.
x (cid:18) x(cid:19)
Y Maximizing the entropy alone does not lead to a suit-
This is not a general prior, but a very specific PDF. Al- ablealgorithm,sincethemaximalentropystateisthatof
though there is some flexibility to adopt its functional complete lack of knowledge, with a uniform probability
formby choosings˜, T,andthe imagespace (pixelspace, foreverysignalpossibility. Theinternalenergy,however,
Fourier space, wavelet space, etc.) in which (11) holds, favors knowledge states close to the posterior maximum
P (s) can not be regarded as being generic. The MiE and would return the MAP solution if extremized alone.
iE
4
Thus the right combination of entropy and internal en- In this case, the partition function can be calculated
ergy is to be extremized. We would expect a free energy explicitly and reads
of the form U −TS to be this function, in analogy to
B
the energy (13) used in MiE methods. Thermodynamics 2π 1/2 J†DJ
Z˜ (d, J)= D exp +J†m−βH(m, d) .
teaches us that the Gibbs free energy is the quantity to β β 2β
(cid:12) (cid:12) (cid:18) (cid:19)
be minimized (which is identical to the Helmholtz free (cid:12) (cid:12)
energy in case J = 0). Since we are going to calculate With standar(cid:12) (cid:12) d ther(cid:12) (cid:12) modynamics procedure we calculate
thisforanapproximationoftherealPDF,itisnecessary
δ N
togothroughthederivationinordertomakesurewedo hHi ≈− Z˜ (d, J) = dgf T +H(m, d)
(s|d) δβ β 2
this inthe rightfashionandunderstandall implications. (cid:12)J=0
(cid:12) (22)
(cid:12)
where N is the dimension(cid:12) of the signal vector. This
dgf
result is the re-phrased equipartition theorem (4) from
II. THERMODYNAMICAL INFERENCE
classical thermodynamics and further motivates the no-
tion of temperature in IFT.
A. Tempered Posterior
In order to take full advantage of the existing ther- B. Internal, Helmholtz and Gibbs energy
modynamicalmachinerywe wantto constructthe Gibbs
free energy for information problems. To this end, we
ThenextstepistocalculatetheHelmholtzfreeenergy.
introduce a temperature and a source function into the
In case it can be calculated explicitly from (17), the in-
PDFofthesignalposteriorassuggestedbythedefinition
ferenceproblemisbasicallysolved,sinceany(connected)
of the partition function (3) by defining
momentofthesignalposteriorcandirectlybecalculated
fromitbytakingderivativeswithrespecttothemoment
e−β(H(s,d)+J†s) (P(d,s)e−J†s)β
P(s|d,T,J)= = . generating function J, e.g. see (18). This will, however,
Z β (d,J) Ds′(P(d,s′)e−J†s′)β only be the case for a very restricted class of Hamilto-
(20) nians, like the free ones, which are only quadratic in s.
R
Withthetemperaturewecanbroaden(forT >1)ornar- In the more interesting case the Helmholtz free energy
row (for T <1) the posterior. Three temperature values can not be calculated explicitly, we can use the thermo-
are of special importance, namely T = 0, which modi- dynamicalrelationoftheHelmholtz freeenergywiththe
fies the PDF into a delta peak located at the posterior internal energy and entropy.
maximum, T = 1, which returns the original posterior, First, we note that the internalenergyof the modified
andT =∞,leadingtothemaximumentropystateofan posterior is given by
uniform PDF. The source function J permits us to shift
themeanofthePDFtoanypossiblesignalconfiguration U(d,T,J) = hH(s,d)i
(s|d,T,J)
m=m(d,T,J). ≈ hH(s,d)i =U˜(d,m,D), (23)
ThemodifiedPDFwillbeapproximatedbyaGaussian (s|m,D)
with identical mean and variance:
where m and D are still functions of d, T, and J. The
average in the second line has to be understood to be
P(s|d,T,J)≈G(s−m,D)=P˜(s|m,D), (21)
performed over a Gaussian with mean m and dispersion
D: hf(s)i = Dsf(s)G(s−m,D).
where also D =D(d,T,J). (s|m,D)
Further,weneedtocalculatetheentropyforthemod-
We will see, that the width D of this Gaussian ap- R
ified PDF, which for a Gaussian depends only on D:
proximation of the PDF increases with increasing tem-
perature. At low temperature (T ≪ 1) the center of 1
S [G(s−m,D)]= Tr 1+log(2πD) =S˜ (D). (24)
the PDFisprobedandmodeled,while atlargetempera- B 2 B
tures (T ≫1) the focus is on its asymptotic tails. Since
(cid:0) (cid:1)
the Gaussian in (21) is an approximation, it is not even For the full modified posterior, (20), the entropy is cal-
guaranteed that T = 1 provides the best recipe for sig- culated via (15) to be
nal reconstruction. E.g. in [9] a case is shown, where
S =β U +J†m−F , (25)
signal reconstruction using T =0.5 slightly outperforms B
both, T = 0 and T = 1. Since working at multiple tem- where m = m(d,T,J) =(cid:0) hsi , U(cid:1)is given by (23),
peratures can reveal different aspects of the same non- (s|d,T,J)
andF by(17). Solving(25)fortheHelmholtzfreeenergy
Gaussian PDF (i.e. its central or asymptotic behavior),
yields
the question appears how the differently retrievedGaus-
sian approximations can be combined into a single and F (d,J)=U −T S +J†m. (26)
β B
more accurate representation of the original PDF. This
will be addressed in Sect. IV. For the moment we ap- This expresses the Helmholtz free energy in terms of in-
proximate our posterior by a single Gaussian as in (21). ternalenergyandentropy. Unfortunately,thisexpression
5
containsthetermJ†m,wheremdependsonJ implicitly Thus the propagatoris the second moment of the Gaus-
through (18). In order to get rid of this term, we Leg- sian weighted Hamiltonian,
endre transform with respect to J and thereby use (18),
which provides us with the Gibbs free energy φφ†H m (φ) (φ|D)
D = . (33)
(cid:10)U˜(d,m,D)(cid:11)+T
δF
G (d,m)=F −J† =U −T S . (27)
β B
δJ Thisequationseemstosuggestthatthepropagatoreval-
uatedathighertemperatureisnarrower,sinceT appears
The Gibbs energy depends solely on m and not on J. It
in the denominator. However, the opposite is the case
can be constructed approximatively, in case approxima-
due to the presence of D in all terms, as a test with a
tionsoftheinternalenergyandtheentropyareavailable.
free Hamiltonian will show in (44).
For our Gaussian approximation of the modified poste-
rior we therefore write
C. Cross information
G˜ (d,m,D)=U˜(d,m,D)−T S˜ (D). (28)
β B
The Gibbs free energy at T = 1 is directly related to
Weknowfromthermodynamicsthattheminimumofthe
thecrossinformationbetweentheposterioranditsGaus-
Gibbsfreeenergywithrespecttovariationsinmprovides
sian approximation. The cross information (or negative
the expectation value hsi of our field:
(s|d) relative entropy) of a PDF P˜ with respect to another
one P is measured by the so called Kullback-Leibler di-
δG(d,m,D)
=0 (29) vergence [26]:
δm
(cid:12)m=hsi
Thus,theGibbsenergyistheinfo
(cid:12)
(cid:12) (cid:12)rmati
(
o
s
n
|d)
energywewere d KL [P˜,P]= DsP˜(s|d) log
P˜(s|d)
. (34)
P(s|d)
!
looking for in the introduction. Z
Minimizing the Gibbs free energy for a GaussianPDF The Kullback-Leibler divergence characterizes the dis-
with respect to m yields tancebetweenasurrogateandtargetPDFinaninforma-
tiontheoreticalsense. It is anasymmetricdistance mea-
δG˜ δG(s−m,D) sure, reflecting that the roles of the two involved PDF
0 = = DsH(d,s)
δm δm differ. TheequivalenceofGibbsfreeenergyandcrossin-
Z
= −D−1 hφH (d,φ)i , (30) formation with respect to inference problems can easily
m (φ|D)
be seen:
with H m (d,φ)=H(d,m+φ), which implies G˜(m,D) = hH(d,s)+log(G(s−m,D))i
(s|m,D)
hsH(d,s)i hsH(d,s)i G(s−m,D)
m= (s|m,D) = (s|m,D) . (31) = DsG(s−m,D) log
hH(d,s)i
(s|m,D)
U˜(m,D)
Z (cid:18)
P(s,d)
(cid:19)
G(s−m,D)
∼ = DsG(s−m,D) log
The optimal map is therefore the first signal moment of P(s|d)
Z (cid:18) (cid:19)
the full Hamiltonian weighted with the approximating
= d [P˜,P]. (35)
Gaussian. KL
Thermodynamics teachesus further that the propaga- InthesecondlaststepweaddedthetermlogP(d),which
tor, the uncertainty dispersion of the field, is provided is irrelevant here, since m- and D-independent, and in
by the secondderivativeofthe Gibbs free energyaround the last step we introduced the Kullback-Leibler diver-
this location, thanks to the well known relation gence between posterior P(s|d) and its Gaussian sur-
rogate P˜(s|d) = G(s −m,D). Minimal Gibbs free en-
δ2G −1 δ2F
ergythereforeseemstocorrespondstominimalKullback-
=− =βD. (32)
(cid:18) δmδm† (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) m=hsi (s|d) δJδJ† (cid:12) (cid:12) (cid:12) (cid:12) J=0 L m e a H i t b i o l o e w n r e d v o i e f v r t e , h r w e ge e s n u h c r e a r , v o a e g n a o t d n e t l w y he i m t r h e i f n o t i h r m e e i t z e o e x d a m c t a t h x p e i o m G s a t i e l b r c b i r o s o r s . f s re i e nf e o n r - -
This relation closes(cid:12)the set of equations by providing D.
ergy so far with respect to m, the mean field, degrees of
Evaluating(32)withourapproximateGibbsenergy(28)
freedomofourGaussian,notwithrespecttotheonespa-
and using (29) yields
rameterizing the uncertainty dispersion D. We have de-
termined this using the thermodynamical relation (29).
δ2G˜
T D−1 = =−D−1U˜(d,m,D) IfwewantthatoursurrogatePDFhasmaximalcrossin-
δmδm† (cid:12) formation with the posterior with respect to all degrees
(cid:12)m=hsi
(cid:12) (s|d) of freedom of our Gaussian, we also have to minimizing
+ D−1 φφ(cid:12) (cid:12) †H m (d,φ) (φ|D) D−1. the Gibbs energy with respect to D. A short calculation
(cid:10) (cid:11)
6
showsthatthisactuallyyieldsaresultwhichisequivalent We assume that the interaction coefficients Λ(n) are
x1...xn
to the thermodynamical relation (32): symmetricwithrespecttoindexpermutations,sincethey
resulted from a Taylor-Fr´echetexpansion.
δG˜ δG(φ,D) δS˜ B (D) The internal energy can then be calculated via the
0 = = DφH (d,φ) −T
δD m δD δD Wick theorem and the fact that all odd moments of φ
Z
D−1 vanish:
= φφ†H (d,φ) −D U˜(m,D)+T D−1,
2 m (φ|D) ∞
1
h(cid:10) (cid:11) (cid:0) (cid:1)i U˜(m,D) = Λ(n)(φ,...φ) (39)
from which also (33) follows. Thus, we can regard both, n! m (φ|D)
the map m and its uncertainty covariance D, as param- n X =0 D E
n
∞
eters for which the Gibbs energy should be minimized. 1
= Λ(2n)(D⊗···D)
We will refer to this as the maximal cross information 2nn! m
principle. n X =0 z }| {
n k
We further note that the maximal cross information
principle also holds if the Gaussian is replaced by some
∞ Λ(2n+k)(D⊗···D⊗m⊗···m)
= .
othermodel function, G[P˜(s|d)]∼ =d
KL
[P˜,P],a property
n,k=0 z
2
}|
nn!
{
k!
z }| {
we will use later in Sect. IV. X
Note,thattheminimalcrossinformationandthether-
Here, we defined the symmetrized tensor product T ⊗
modynamical relations yield exactly the same results for
m and D only if G˜(m,D) is calculated exactly. In case T′ x1...xn ≡ π∈Sn n 1 ! T xπ(1)...xπ(k) ·T x ′ π(k+1)...xπ(n) by (cid:0) aver-
aging over all permutations in S , the symmetric group.
there are approximations involved, the resulting algo- (cid:1) P n
Having obtained the internal energy with (39), and
rithms differ slightly, and this difference can be used to
entropy with (25) approximatively,we can construct the
monitor the impact of the approximation made. In the
Gibbs free energyaccordingto (28)which weuse for our
following,weusetheminimalcrossinformationprinciple
inference.
for our examples.
D. Calculating the internal energy E. Minimizing
In order to calculate the approximative Gibbs energy, In order to get our optimal Gaussian approximation
we need to estimate the internal energy, for which we to the posterior, we have to minimize G˜ β (m,D) with
have to specify the exact Hamiltonian. We assume that respect to m and D. Minimizing for m is equivalent to
it can be Taylor-Fr´echetexpanded as minimizing the internal energy, since the entropy does
not depend on m. This yields
∞
1
H(d,s)= n! Λ( x n 1 ) ...xn s x1 ···s xn , (36) δU˜(m,D)
n=0 0 = (40)
X Λ(n)(s,...s) δm
n k
| {z }
where repeated coordinatesare thought to be integrated ∞ Λ(2n+k+1)(D⊗···D⊗m⊗···m,·)
or summed over. The approximative internal energy is = . ,
2nn!k!
then n,k=0 z }| { z }| {
X
U˜(m,D) = U[P˜(s|d)]= DsH(d,s)P˜(s|d) whichhasto be solvedform foranygivenD. The prop-
agator derives from (32) or from
Z
∞
1
= Λ(n)(s,...s) . (37) δG˜(m,D)
n! (s|m,D) 0 = ⇒ (41)
n X =0 D E δD
n k
The Gaussiann-point correlationfunctions in this equa-
tion can actually be calculated analytically. For this, we T D−1 = ∞ Λ(2n+k+2)(·,·,D⊗···D⊗m⊗·· . ·m)
again use the shifted field φ = s − m, which has the 2nn!k!
n,k=0 z }| { z }| {
Hamiltonian X
∞ which also depends on m. Thus, (40) and (41) have to
1
H (d,φ) = Λ(n)(φ,...φ), with (38) be solved simultaneously.
m n! m
n=0 A simple example should be in order. The simplest
X
∞ case is that of the original Hamiltonian being quadratic.
1
Λ(n)(φ,...φ) = Λ(n+k)(φ,...φ,m,...m). Theapproximatedoneshouldthenmatchthisexactly. A
m k!
k=0 quadraticorfreeHamiltonianisequivalenttoaGaussian
X n k
| {z } | {z }
7
posterior, P(s|d)=G(s−m ,D ). We get The corresponding Hamiltonian was shown in [9] to be
∗ ∗
H(d,s) ∼ = 2 1 (s−m ∗ )†D ∗ −1(s−m ∗ ) H(d,s) ∼ = 1 2 s†S−1s−d†bs+κ†ebs. (48)
1
∼ = Λ(1)s + Λ(2)s s with (42)
x x 2 xy x y Reconstruction methods for this data model were devel-
Λ(1) = −D−1m , and oped by [9, 35–37].
∗ ∗
TheinternalenergyofourGaussianapproximationcan
Λ(2) = D−1.
∗ be calculated analytically,
Inserting this into (40) and (41) yields
1 1
U˜(m,D) ∼ = m†S−1m+ Tr(DS−1)−d†bm
0 = Λ(1)(·)+Λ(2)(m,·)=D−1(m−m ) 2 2
∗ ∗
⇒ m=m , (43) + κ†ebm+b 2 2 D (49)
∗
T D−1 = Λ(2)(·,·)=D−1
∗ where D denotes the vectorbof diagonal elements of D.
⇒ D =T D ∗ , (44) Minimizing G˜(m,D) = U˜(m,D)−T S˜ (D) with re-
B
which indeed recovers the original coefficients for T =1,
spect tobm and D yields
andanarrowerorwideruncertaintydispersionforT <1
or T >1, respectively. In the following, we will see that m = Sb d−κ , and
m+bD
alsoin case ofinteracting Hamiltonians the minimal free 2
(cid:16) (cid:17) −1
energy principle provides the correct results. We show D = T S−1+b2κ , (50)
mb+bD
thisbyreproducing(andextending)signalestimatorsde- 2
(cid:16) (cid:17)
rivedpreviouslyinIFTusingrenormalizationtechniques.
respectively. Here we have defibned κ
bt
= κexp(bt) and
denote a diagonal matrix by putting a hat onto a vector
of its diagonal elements (λ) = λ δ . This result is
III. APPLICATION EXAMPLES xy x xy
identicalwith the one found in [9]using a lengthy renor-
malization calculation. Thbere it was found by numerical
A. Poissonian log-normal data
experiment, that using T =0.5 in (50) seems to produce
slightly better results than T =0 and T =1.
1. Separable case
Manyinference problems havetodealwith Poissonian 2. Entangled case
noise, like X-ray and γ-ray astronomy as well as recon-
struction of the cosmic large-scalestructure from galaxy
Sofar,weassumedthattheresponseprovidesaoneto
counts. Let us assume that the mean count rate λ of
one correspondencebetweenlocations in signaland data
photons or galaxies is proportional to an exponentiated
space. However, for most measurements this is not ex-
Gaussian random field s with covariance S = ss†
(s) actlytrue. X-andγ-raytelescopestypicallyexhibitpoint
according to
(cid:10) (cid:11) spreadfunctions,whichmapasinglesignalspacelocation
ontoseveraldetectors,ofwhicheachdetects eventscom-
λ(s)=κebs. (45)
ingfromseveralindistinguishabledirections. Alsogalaxy
Here, κ is the expected counts for s = 0, which may de- redshifts do not provide accurate distance information,
pend on the spatial position. The scalar b permits us to sinceredshiftdistortionsandmeasurementerrorsleadto
change conveniently the strength of the non-linearity of effective point spread functions.
the problem without changing the signal statistics. This In the following, we generalize to the case of a known
log-normalmodelforthe cosmiclarge-scalestructuresas and fixed, but non-local measurement response. Fixed
an approximative description is actually supported ob- means, that the response is independent of the signal.
servationally [27, 28] and theoretically [29–34]. Thisexcludesthetreatmentofgalaxyredshiftdistortions
Asastartingpoint,weassumealocalresponse,sothat with this case (e.g. see [38] for this), but still includes
the Poissonstatisticsfortheactualcountsd atlocation photometric redshift errors of galaxy catalogs as well as
x
x are X- and γ-ray telescope data. Such problems have been
approachedin the past via the MAP principle [39–42].
λdx
P(d |λ )= x e−λx, (46) The point spread function is modeled by the response
x x
d x ! matrix R = (R ix ) which describes how emissivity at lo-
cation x is expected to be observed in data channel i.
and the full likelihood is well separable into local ones:
The expected count rate is now
P(d|s)= P(d |λ (s )). (47)
x x x λ(s)=Rebs, (51)
x
Y
8
and the likelihood does not separate any more with re- b2
spect to x − 3r ix r iy exp D ab
 2 
a,b∈X{x,y}
P(d|s)=
i
P(d i |λ i (s)), (52) + 3r
i
†e2 1b2D−  1.  (57)
Y
These coefficients stay small if b2D ≪ 1, which means
since λ (s) entangles the signal from several locations, b
i
thattheexpansioncanbetruncatedifthesignalisknown
whereasin(47)itdepends only onthe localsignalvalue.
within a few ten percent or if non-Gaussianity is small.
WerecovertheformercaseforadiagonalresponseR =
ix
Large uncertainties in the signal strength do not nec-
κ δ . The resulting Hamiltonian
x ix
essarily lead to large coefficients if they are located at
H(s|d) ∼ = 1 s†S−1s+1†Rebs−d†log(Rebs) (53) positions without instrumentalsensitivity (R ix small) or
2 much lower expected count rates (m x small). In both
cases mostly prior information and extrapolation from
reduces to (48) for R being diagonal.
regions with more informative data will determine the
The internal energy of our surrogate Gaussian
solution at such locations.
P˜(s|d)=G(s−m,D) is then
In case some of these coefficients are large, substan-
U˜(m,D) = 1 m†S−1m+ 1 Tr(DS−1)+1†Rebm+b 2 2 D t s i e a n l s s it ig iv n e a m l u u n s c t e b r e ta p in re ty se a n t t. th In e t lo h c is at c i a o s n e s a t n o a w c h cu ic r h at t e h r e e y co a n re -
2 2
struction for these locations can not be expected. Thus,
− d Dφ log R†eb(m+φ) G(φ,D).(54b) if we simplify the Hamiltonian by dropping such terms,
i i
X i Z (cid:16) (cid:17) evenif they are relatively large,the quality ofthe recon-
Ii struction will not suffer too much since only regions are
affected, which are poorly constrained by the data any-
This integral I can|not be calculat{ezd in closedfro}m due
i way. Therefore,truncatingthe expansionshouldalready
to the logarithm in the integrand. We expand the loga-
provide usable algorithms.
rithm around R†em, since we will see that this recovers
i
the result of the separable case most easily for R being
diagonal. We get 3. Zeroth order solution
R†eb(m+φ)
I i = log R i †ebm +
*
log i R†ebm
!+
(.55)
the
To
ap
z
p
e
r
r
o
o
x
th
im
o
a
r
t
d
iv
er
e
,
f
w
re
e
e
i
e
g
n
n
e
o
r
r
g
e
y
all II in -terms and find for
(cid:16) (cid:17) i (φ|D)
1 1
In case R is diagonal, the first term reduces to bm + G˜(m,D) ≈ m†S−1m+ Tr(DS−1)
logR , the second vanishes as hlog(exp(bφ))i = 2 2
i (φ|D)
hbφi (φ|D) = 0, and we recover the Hamiltonian of the + R i †ebm+b 2 2 D−d i log R i †ebm
separable case. X i h (cid:16) (cid:17)i
InthegeneralcaseofanentanglingresponseweTaylor T b
− Tr(1+log(2πD)). (58)
expand the logarithm of the second term 2
Minimizing this with respect to m and D yields
I = log R†ebm
i i
− ∞(cid:16) (− n 1)n (cid:17) r i †ebφ−1 n (φ|D) , with (56) m = Sb i R i ebm R i † d e i bm −e 1 2 b2D !
n X =1 D(cid:16) (cid:17) E X b
IIin = Sb d†r−κ′(m+bD/2) , and
R i ebm | {zR i (x)ebm } (x) (cid:16) (cid:17) −1
r i = R i †ebm orr i (x)= dx′R i (x′)ebm(x′) . D = T S−1+b2κ′(m+bbD/2) , with
(cid:16) (cid:17)
We note that r†1= dxr = R 1 by construction. κ′(t) = R i ebt. b b (59)
i ix i
The expansioncoefficients II can be workedout one X
in
R
by one. We provide here the first few, namely Thisisverysimilarto(50)andreducestoitforadiagonal
response.
II
i1
= r
i
†e2 1b2D−1,
II i2 = r ix r iy e b2 1b2(Dxx+Dyy+2Dxy)−2r i †e2 1b2D+1, 4. First order correction
b2
b
II = r r r exp D First order corrections are included by keeping the
i3 ix iy iz ab
 2 
II -term in the approximative free energy, but ignoring
a,b∈X{x,y,z} i1
 
9
higher terms. The resulting equations are are usually not interested in the background proper-
ties, we marginalize over it. This is especially simple
m = Sb d i 1+r i †e b 2 2 D r i −κ′′(m+bD/2) i P n (s t ′ h |d e ) G ≈ a G u ( s s s ′ ia − n m a ′ p , p D ro ′) x , im wi a t t h io s n ′ = of (s o , u f r ), j m oin ′ t = p ( o s˜, st f˜ e ) r , ior
!
X i (cid:16) (cid:17)
D = T S−1+b2κ′′(m+b b D/2) −1 , with b m ≈ Ds′sG(s′−m′,D′)=s˜, and (62)
Z
(cid:16) (cid:17)
κ′′(t) = R i ebt 1b+ R† d e i bm b ! . (60) D xy ≈ Z Ds′(s−s˜) x (s−s˜) y G(s′−m′,D′)=D x ′ y .
i i
X
Although this does not look too different from the for-
This is a slight modification with respect to (59) in two mula for the case without background, the effect of the
aspects. The map changes a bit, but the sign of the background entered through the joint covariance matrix
changes depends on the details of the point spread func- D′, which mixes the contribution from the signal and
tion, since there are two new terms of similar order, but backgroundevents appropriately.
withoppositesigns. Theuncertaintyvarianceisreduced,
since the term added to the inverse propagatoris always
positive. B. Reconstruction without spectral knowledge
1. Effective theory
5. Observation with background
The reconstruction of the signal in the Poisson log-
The observationmaysuffer froma background,events normal model in the previous section assumed that the
in data space, which do not contribute to our signal
signalcovarianceisknownapriori. Incaseitisunknown,
knowledge. Forexampleγ-rayastronomyhastosuppress
it has to be extracted from the same data used for the
cosmicrayeventsasmuchaspossible,sincechargedpar-
signalinference [43–47]. However,the optimalwaytodo
ticles do not point back to the same sources as neutral
this was usually not derived from first principles, maybe
photons due to cosmic magnetic fields. Fortunately, cos-
except in [48–50]. A rigorous approach to such prob-
micrayshavedifferentsignaturesindataspaceduetothe
lems is given by the computationally expensive Gibbs-
differences in hadronic and electromagnetic interactions.
sampling technique, whichinvestigatesthe jointspace of
However, not for all measured events is the distinction
signal realizations and power spectra [51–54], which can
clearly cut and we have to use prior knowledge to sup-
then easily be marginalized over the power spectra to
press the backgroundevents.
obtainagenericsignalreconstruction. Thisproblemwas
Thereforeweshouldextendourformalismtoalsotake
also addressed approximatively for the case of linear re-
such unwanted backgrounds into account. Actually a
sponse data from a Gaussian signal subject to Gaussian
reinterpretation of the above formula will do. We ex-
noise using the MAP principle as well as by the help of
tend our signal space by the quantity f determining the
parameter uncertainty renormalized estimation by [11].
logarithm of the backgroundcount rate, s→s′ =(s,f). We re-address this problem here using the minimal free
f mightbe afieldoverthe samephysicalspaceass ,or
z x energy approach.
just a single number as a total isotropic cosmic ray flux. We assume the covariance S = ss† of our Gaus-
Inanycase,thex−andz−coordinatesareregardedtobe (s)
sian signal s to be diagonal within some known function
over different spaces, or distinct areas of the joint space (cid:10) (cid:11)
basis O , e.g. the Fourier basis with O = eikx. We
over which f and s live. The joint covariance reads kx kx
model the power spectrum (in this basis) as being a lin-
ear combination of a number of positive basis functions
S 0
S′ = (61) f (k)withdisjointsupports(the spectralbands),sothat
0 F i
(cid:18) (cid:19)
P (k)= p f (k) (63)
duetotheindependenceofsignalandbackground. Here, s i i
F = ff† is the log-backgroundcovariance. The re- X i
(f)
spons (cid:10) eR→ (cid:11) R′ hastobeextendedtomapalsothe back- ispositiveforallk(allcoefficientsofp=(p i ) i arepositive
groundspace into the data space. Whether the response and the spectral bands cover the full k-space domain).
imagesofsignalandbackgroundeventsindataspaceare We define
wellseparatedorwhethertheyoverlapdecidesaboutthe
(S ) =(O†f O) =O f (k)O (64)
background discriminating power of the instrument. i xy i xy kx i ky
The combined map and covariance of signal and log- to be the i-th spectral band matrix and S−1 to be its
background can now be obtained, e.g. from (59) or b i
pseudo-inverse. Thus, we write our signal covariance as
(60) with the appropriatereplacements for S,R,m,D→
S′,R′,m′,D′. Our joint map can be split into a sig- S = p S , (65)
nal and log-background part m′ = (s˜,f˜). Since we i i
i
X
10
with p = (p ) the vector of unknown spectral parame- Here we have introduceda parameterδ to be fixed soon.
i
ters. We further assume that the individual signal-band The first two expansion coefficients are
amplitudes p have an independent prior distribution,
i 1
II = (1−δ)Tr(DS−1)
i1 2 i
P(p)= P(p ), (66)
i
1
Y i II i2 = II2 i1 +Tr mm†+ 2 D S i −1DS i −1 .(72)
with the individualpriorsbeing inverse-gammadistribu- (cid:18)(cid:18) (cid:19) (cid:19)
tions, power-laws with exponential low amplitude cutoff
at q i : 3. Zeroth order solution
1 p
−αi
q
P(p )= i exp − i . (67) To zeroth order we find by minimizing the free energy
i
q i Γ(α i −1) (cid:18) q i(cid:19) (cid:18) p i(cid:19) while ignoring the II-corrections
For α i ≫ 1 this is an informative prior, where q i /α i m = D′j, D =T D′, and
determines the preferred value. A non-informative prior
−1
wouldbegivenbyJeffreyspriorwithα i =1andq i =0.2 D′ = M + p−1S−1 . (73)
For a linear data model i i !
i
X
d=Rs+n, (68) This means that the map is the Wiener filtered data,
where the spectral coefficients are assumed to be
with Gaussian noise with covariance N = nn† , the
(n)
q˜ 1 1
p in a g ra t m o e [1 te 1 r ] marginalized effective Hamilton (cid:10) ian is (cid:11) accord- p i = γ i i δ = γ i δ (cid:18) q i + 2 Tr((mm†+δD)S i −1) (cid:19) . (74)
H(d,s)∼ = 1 2 s†Ms−j†s+ γ i log q i + 2 1 s†S i −1s . F if o M r δ is = (p 0 se t u h d is o) y - i i e n l v d e s rt p i i bl = e. ∞ The an r d esu th lt e i r n e g fo fi r l e te D r p = rov M id − e 1 s
i (cid:18) (cid:19) a noise weighted deconvolution,howeveris unable to ex-
X
(69)
trapolate into unobserved regions of the signal space. It
Here M = R†N−1R, j = R†N−1d, γ = α −1+̺ /2,
i i i is widely used for map making in the field of cosmic mi-
and ̺ = Tr[S−1S ] the number of spectral degrees of
i i i crowave background observations. For δ = 1 we recover
freedom within the band i.
the critical estimator of [11]. Since there it was shown
thatthe latterperformssignificantlybetter thanthe for-
mer,andalsosinceII =0andII isminimalforδ =1,
2. Free energy expansion i1 i2
we adopt this in the following. For Jeffreys prior we find
The internal energy of a Gaussian posterior-ansatz is Tr(B i )
p = , (75)
then i ̺
i
U˜(m,D) ∼ = 1 m†Mm+ 1 Tr(DM)−j†m with B i =(mm†+D)S i −1.
2 2
1
+ γ log q + s†S−1s (.70)
i i 2 i 4. Second order correction
i (cid:28) (cid:18) (cid:19)(cid:29)(s|m,D)
X
Ii
Including higher order corrections should improve the
Again we have to dea|l with a Gaus{szian average ov}er a reconstruction. Thefirstordercorrectionsvanishforδ =
logarithm, which we expand as 1. The second order correction yields
−1
I = log(q˜)− ∞ (−1)k q + 1 s†S−1s−q˜ k , m = D′j, D =T D′−1 − γ i S−1mm†S−1 ,
i i X k=1 k(q˜ i )k *(cid:18) i 2 i i (cid:19) + (s|m,D) " X i q˜ i 2 i i #
−1
IIik D′ = M + γ i X S−1 , (76)
1 q˜ i i
with q˜ i =q i + 2 Tr(|(mm†+δD)S i −{z1). (71)} " X i i #
1 1 1
X = 1+ Tr (mm†+ D)S−1DS−1 − S−1D.
i q˜2 2 i i q˜ i
i (cid:18) (cid:19) i
2 Since this would result in an improperly normalized prior, we The operator D′, which is applied to j to generate the
map, and the uncertainty dispersion D are not identi-
understand thisas αi =1+ǫ,qi=ǫ,andlimǫ→0 attheendof
thecalculation. cal any more. Neither of them can still be expressed as
11
(M+ p−1S−1)−1,duetotheoperatorstructureofthe IV. INFORMATION SYNTHESIS
i i i
S−1D and S−1mm† terms. This was also found in [11].
i P i
However,ifwecanassumethatthisoperatorprocesses A. Multi-temperature posterior
any channelin the i-thband in a similar way,we canre-
placeS−1DS−1 andS−1mm†S−1 bytheirchannelaver- AlthoughtheobtainedGaussianknowledgestatesfrom
i i i i
agedvalues Tr(DS−1)S−1/̺ andTr(mm†S−1)S−1/̺ , minimal free energy estimation are approximative and
i i i i i i
respectively. This permits to identify spectral coeffi- therefore of limited accuracy, they might permit us to
cients of D = (M + p−1S−1)−1 and D′ = (M + construct more accurate models of the posterior. The
i i i
p′−1S−1)−1. For Jeffreys prior they become idea is to combine several Gaussian distributions to a
i i i P more accurate approximation of the true non-Gaussian
P posteriorprobability,andto measure the meanmapand
Tr(B ) 2 Tr mm†S−1 2 −1 its uncertainty dispersion from this combination.
p i = i 1− i , and We recall that Gaussian approximations of the poste-
̺ i  ̺ i (cid:0) Tr(B i ) (cid:1)!  rior obtained at low temperatures (T ≪1) mostly carry
  −1 information on its peak region, while those obtained at
Tr(B ) 2 Tr mm†S−1 Tr DS−1
p′ = i 1+ i i (77,) large temperatures (T ≫ 1) information on its asymp-
i ̺ i " ̺ i (cid:0) (Tr(B (cid:1)i ))2 (cid:0) (cid:1)# totics. Also the canonical T = 1 does not provide a
perfectrepresentationoftheposterior,asaGaussianap-
proximation for a non-Gaussian PDF never can. How-
where m, D, and B =(mm†+D)S−1 all depend on p.
i i ever, by combining such different approximations in an
It is obvious, that the second order correction increases
appropriate way, we should obtain an improved repre-
p by some margin compared to (75), meaning that the
i sentationofthe correctPDF,whichpermits mucheasier
reconstruction uncertainty increases. It is less obvious
calculation of moments like the signal mean and its un-
how p′ develops, since at first glance it seems to be cor-
i certainty variance.
rected downwards. Note however, that an increased p
i Tothisendwepostulatetheexistenceofatemperature
implies an increased Tr(B ), since D grows (spectrally)
i distribution function P(T), such that
with increasing p .
i
∞
Thefactthatwegettwodifferingsetsofspectralcoeffi- P(s|d)= dT G(s−m ,D )P(T) (79)
(d,T) (d,T)
cients, p i and p′ i , reminds us to regardthem as auxiliary Z0
variables of our signal reconstruction algorithm, rather
combines the different Gaussians with means m and
(d,T)
than as optimal spectrum estimates.
dispersionsD to synthesize the rightposteriorprob-
(d,T)
ability. A formalproof of the existence of P(T), and the
necessary conditions for this is beyond the scope of this
work. It should be noted, that e.g. multi-peaked distri-
C. Poisson log-normal distribution with unknown butionscannotaccuratelyberepresentedbyapproximate
spectrum Gaussiansobtainedatdifferenttemperatures. They can,
however, often be well approximated by Gausians cen-
tered on those peaks. The recipes described below do
The combined problem, reconstructing a Poisson log-
not depend on the way the different Gaussians used in
normal signal with unknown spectrum, can now be
the mixture model wereobtained, and thereforecanalso
treated approximatively. The combined free energy for
be used in such cases.
the Gaussian posterior approximationto zeroth order is
Inthe followingwe providea recipeto constructP(T)
in practice. We assume that m = m and D =
G˜(m,D) ≈ R i †ebm+b 2 2 D−d i log R i †ebm D (d,Ti) havebeencomputed fora i numbe ( r d, N Ti T ) oftemp i er-
X i h (cid:16) (cid:17)i atures T i . The temperatures are best chosen to sample
b1 wellthedifferentpartoftheposterior,itspeakbyhaving
+ γ log q + Tr (mm†+D)S−1
i i 2 i some T i ≪1, the bulk of the PDF with T i = 1, and the
X i (cid:18) (cid:0) (cid:1) (cid:19) PDF tails with T i ≫1.
T The surrogate probability function we want to con-
− Tr(1+log(2πD)). (78)
2 struct,andwhichshouldresembletheexactoneasclosely
as possible, is therefore of the form
The resulting map and uncertainty dispersion are pro-
NT
vided by (59) with the addition that S = i p i S i and P˜(s|d)= G(s−m ,D )P . (80)
thep sareprovidedby(74). Higherordercorrectionscan i i i
i
be included in a similar way as in the indi P vidual prob- X i=1
lems. Also background counts with known or unknown P˜(s|d)shouldbeascloseaspossibletoP(s|d)inaninfor-
covariance structure can be included in the same way mation theoretical sense. The natural choice for the dis-
they were treated in Sect. IIIA5. tance measure is the Kullback-Leibler divergence, which
12
measures the cross-informationof P˜(s|d) on P(s|d), and Thisway,G˜(p)canbeapproximated,andminimizedwith
whichispracticallyidenticaltothefreeenergyG˜[P˜(s|d)] a suitable optimization scheme. The sampling points,
ofoursurrogateposterioraccordingto (34). Introducing their Gaussian probabilities G (j) = G (s (j) ), as well as
ki k i
un-normalizedprobabilitiesp i asourdegreesoffreedom, theenergiesU i needonlybecalculatedonce,butthesur-
and setting P i = p i /Z p with Z p = j p j in order to rogate energies U i (p)= logZ p − j log( k p k G k (j i ))/N i
enforce the proper normalization, iP P i =1, this reads have to be updated at any step o
P
f the sch
P
eme.
One might argue, that if we use stochastic methods
p
G˜(p)= i (U i −U˜P i (p))−F. (81) to build P˜(s|d), one could have used a Markov-Chain
Z
i p Monte-Carlo (MCMC) method right from the beginning
X
forthesignalinferenceproblem. However,weexpectthat
We have introduced the here irrelevant, since p-
the here describedposterior synthesis method should re-
independent, free energy F = −logZ of the original
d
problem and the energies U and U˜ (p) with respect to produce the correctposteriorbetter thana sample point
i i
cloud, since we are using well adapted Gaussians as our
the template distributions G (s)=G(s−m ,D ):
i i i
building blocks and not delta functions as the direct
MCMC approach uses. Furthermore, the analytical and
U = hH(d,s)i = Ds G (s)H(d,s)and
i Gi i sampling method can be combined, in that the analyti-
Z
cal estimates are combined with the sampling estimates
U˜ (p) = H˜ (s) , with (82)
i p ofthe contributionsoftheneglectedtermsintheTaylor-
Gi
H˜ (s) = − D log( E p G (s)/Z ). Fr´echetexpansionsof(83). Andfinally,sinceourscheme
p i i p draws samples from Gaussians, it can be trivially paral-
i
X lelized,whichisnoteasilypossiblewithMCMCschemes.
B. Minimizing the Gibbs energy
C. Maps and moments
1. Analytical scheme
Once the minimum of G˜(p) with respect to p is found,
Now one has to minimize G˜(p) with respect to p. The one has synthesized a posterior approximation with a
Gaussian mixture model. From this, any moment of the
problem to calculate the path integrals defining the en-
distributionfunctioncaneasilybe calculated. Themean
ergies was already addressed in this work. A system-
map can be expressed as
atic way is to Taylor-Fr´echet expand the Hamiltonians
aroundthe centersofthe Gaussiansm andthenuse the
i
m≈hsi = P hsi = P m , (86)
known moments of G i (s) to approximate the energies. P˜(s) i Gi(s) i i
For the surrogate energies this yields up to second order X i X i
in φ =s−m
i i as well as the uncertainty dispersion as
1 g
U˜ i (p) = −logg i + 2 g j i i Tr(D j −1D i ) (83) D ≈ (s−m)(s−m)† P˜(s) = P i (D i +m i m† i )−mm†.
j
X i
1 g g (cid:10) (cid:11) X (87)
+ ji ki −δ m† D−1D D−1m ,
2 g g jk ij j i k ik We leave the verificationand applicationof the informa-
X jk i (cid:18) i (cid:19) tion synthesis method for future work.
with
g = p G (m )/Z , and V. CONCLUSIONS
ji j j i p
Nj
g = g , and (84) We have shown that the minimal free Gibbs energy
i ji
principle in information field theory can be used to ob-
j=1
X
m = m −m . tain approximate knowledge states with maximal cross-
ij i j
information to the exact posterior. The construction of
such knowledge states with Gaussian PDF is relatively
2. Monte-Carlo scheme straightforward:
1. The joint PDF of signal and data P(d,s) has to
Alternatively, one can approximate the average
be specified, e.g. by specifying a data likelihood
hX[s]i of a quantity X[s] by sums over N sampling
Gi i P(d|s) and signal prior P(s), and using P(d,s) =
points {s i (j)} j , which can easily be drawn from G i (s): P(d|s)P(s).
hX[s]i ≈ X[s(j)]/N . (85) 2. TheinformationHamiltonianH isthenegativelog-
Gi i i
arithm of this, H(d,s)=−log(P(d,s)).
j
X
13
3. AsuitablyparametrizedPDFasasurrogateforthe thehereproposedmethodforthemorecomplicatedcom-
posterior has to be specified, e.g. a Gaussian with binedcase canbe expectedto work. However,a detailed
its mean and dispersion as degrees of freedom. implementationandverificationofthiswasleftforfuture
work.
4. The internal energy U and entropy S of this
B Finally we have sketched how Gaussian knowledge
PDF have to be calculated as the PDF-average of
states obtained at different thermodynamical tempera-
theHamiltonianandthenegativelog-PDF,respec-
tures can be combined into a more accurate representa-
tively.
tion of the posterior, from which moments of the signal
5. The Gibbs free energy, G=U −T S , has then to uncertainty distributions can easily be extracted.
B
beminimizedwithrespecttoalldegreesoffreedom TheminimalGibbsenergyandmaximalcrossinforma-
of the surrogate PDF. tion principle introduced here to IFT should allow the
construction of novel reconstruction schemes for statis-
6. Any statistical summary like mean and variance
tical inference problems on spatially distributed signals.
can now be extracted from the surrogate PDF.
The thermodynamical language may help to clarify con-
ceptsandtosimplifyapplicationsofIFT,sinceitpermits
Theminimalfreeenergyprincipleisthereforewellsuited
ustotacklenon-linearinverseproblemswithouttheneed
to tacklestatisticalinferenceproblems. We havedemon-
to use diagrammatic perturbation theory and renormal-
strated this with two different problems and their com-
ization schemes.
bination: reconstructing a log-normal field from Poisson
data subject to a point spread function and reconstruc-
tion without prior knowledge on the signal power spec-
trum. Earlier results from renormalization calculations
Acknowledgments
in [9, 11] have been reproduced. The there used renor-
malization schemes can therefore be understood as aim-
ing for a surrogate Gaussian PDF which has maximal We thank Mona Frommert, Jens Jasche, Niels Opper-
cross information to the correct posterior. Since these mann,GerhardB¨orner,andthreerefereesfordiscussions
results were previously shown to reconstruct well, also and comments on the manuscript.
[1] E. T. Jaynes, Physical Review 106, 620 (1957). [20] D. M. Titterington and J. Skilling, Nature 312, 381
[2] E. T. Jaynes, Physical Review 108, 171 (1957). (1984).
[3] J. N. Fry,ApJ289, 10 (1985). [21] J. Skilling and R. K.Bryan, MNRAS 211, 111 (1984).
[4] E. Bertschinger, ApJ323, L103 (1987). [22] R. K. Bryan and J. Skilling, Journal of Modern Optics
[5] W. Bialek and A. Zee, Physical Review Letters 58, 741 33, 287 (1986).
(1987). [23] S. F. Gull, in Maximum Entropy and Bayesian Meth-
[6] W.Bialek andA.Zee,PhysicalReviewLetters61, 1512 ods, edited by J. Skilling (Kluwer Academic Publishers,
(1988). Dordtrecht, 1989), pp.53–71.
[7] W. Bialek, C. G. Callan, and S. P. Strong, Physical Re- [24] S. F. Gull and J. Skilling, The MEMSYS5 User’s Man-
view Letters 77, 4693 (1996), arXiv:cond-mat/9607180. ual (MaximumEntropyDataConsultantsLtd,Royston,
[8] J. C. Lemm, ArXiv Physics e-prints (1999), 1990).
physics/9912005. [25] J.Skilling,inMaximum Entropy and Bayesian Methods,
[9] T. A. Enßlin, M. Frommert, and F. S. Kitaura, editedbyG.J.Erickson,J.T.Rychert,andC.R.Smith
Phys.Rev.D 80, 105005 (2009), 0806.3474. (1998), p.1.
[10] J.C.Lemm,BayesianFieldTheory (JohnsHopkinsUni- [26] S. Kullback and R. Leibler, Annals of Mathematical
versity Press, 2003). Statistics 22 (1), 79 (1951).
[11] T. A. Enßlin and M. Frommert, ArXiv e-prints (2010), [27] E. Hubble,ApJ79, 8 (1934).
1002.2928. [28] F. S.Kitaura, J.Jasche, C. Li,T. A.Enßlin, R.B.Met-
[12] R.T. Cox, American Journal of Physics 14, 1 (1946). calf, B. D. Wandelt, G. Lemson, and S. D. M. White,
[13] R.T. Cox, American Journal of Physics 31, 66 (1963). MNRAS 400, 183 (2009), 0906.3978.
[14] S.F. Gull and G. J. Daniell, Nature 272, 686 (1978). [29] D. Layzer, AJ 61, 383 (1956).
[15] J.Skilling,A.W.Strong,andK.Bennett,MNRAS187, [30] P. Coles and B. Jones, MNRAS248, 1 (1991).
145 (1979). [31] R. K. Sheth, MNRAS 277, 933 (1995), astro-
[16] R.K. Bryan and J. Skilling, MNRAS 191, 69 (1980). ph/9511096.
[17] S.F.Burch,S.F.Gull,andJ.Skilling,ComputerVision [32] I. Kayo, A. Taruya, and Y. Suto, ApJ 561, 22 (2001),
Graphics and Image Processing 23, 113 (1983). arXiv:astro-ph/0105218.
[18] S. F. Gull and J. Skilling, in Indirect Imaging. Measure- [33] R. Vio, P. Andreani, and W. Wamsteker, PASP 113,
mentandProcessing forIndirect Imaging (1983),p.267. 1009 (2001), arXiv:astro-ph/0105107.
[19] S. Sibisi, J. Skilling, R. G. Brereton, E. D. Laue, and [34] M. C. Neyrinck, I. Szapudi, and A. S. Szalay, ApJ 698,
J. Staunton,Nature311, 446 (1984). L90 (2009), 0903.4693.
14
[35] J. Jasche and F. S. Kitaura, ArXiv e-prints (2009), [46] P. Stoica, H. Li, and J. Li, IEEE Signal Processing Let-
0911.2496. ters 6, 205 (1999).
[36] J. Jasche, F.S. Kitaura, C. Li, and T. A.Enßlin, ArXiv [47] P.Stoica,E.G.Larsson,andJ.Li,AJ120,2163(2000).
e-prints(2009), 0911.2498. [48] P. K. Kitanidis, Water Resources Research 22, 499
[37] F. Kitaura, J. Jasche, and R. B. Metcalf, MNRAS 403, (1986).
589 (2010), 0911.1407. [49] G. Rydbeck,ApJ675, 1304 (2008).
[38] C. Weig and T. A. Enßlin, ArXiv e-prints (2010), [50] U. Seljak, ApJ 503, 492 (1998), astro-ph/9710269.
1003.1311. [51] B. D. Wandelt, D. L. Larson, and A. Lakshmi-
[39] T.J.HebertandR.Leahy,IEEETransactionsonSignal narayanan,Phys.Rev.D70,083511(2004),arXiv:astro-
Processing 40, 2290 (1992). ph/0310080.
[40] T. J. Cornwell and K. F. Evans,A&A 143, 77 (1985). [52] H. K. Eriksen, I. J. O’Dwyer, J. B. Jewell, B. D. Wan-
[41] G. Wang, L. Fu, and J. Qi, Physics in Medicine and delt, D. L. Larson, K. M. G´orski, S. Levin, A. J. Ban-
Biology 53, 593 (2008). day, and P. B. Lilje, ApJS 155, 227 (2004), arXiv:astro-
[42] C.OhandB.RoyFrieden,OpticsCommunications282, ph/0407028.
2489 (2009). [53] J. Jewell, S. Levin, and C. H. Anderson, ApJ 609, 1
[43] D. H. Roberts, J. Lehar, and J. W. Dreher, AJ 93, 968 (2004), arXiv:astro-ph/0209560.
(1987). [54] J.Jasche,F.S.Kitaura,B.D.Wandelt,andT.A.Enßlin,
[44] G. B. Rybickiand W. H.Press, ApJ 398, 169 (1992). MNRAS 406, 60 (2010), 0911.2493.
[45] J. Li and P. Stoica, IEEE Transactions on Signal Pro-
cessing 44, 1469 (1996).