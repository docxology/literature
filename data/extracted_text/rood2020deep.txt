A deep active inference model of the
rubber-hand illusion
Thomas Rood, Marcel van Gerven, and Pablo Lanillos
Department of Artiﬁcial Intelligence
Donders Insitute for Brain, Cognition and Behaviour
Montessorilaan 3, 6525 HR Nijmegen, the Netherlands
p.lanillos@donders.ru.nl
Abstract. Understanding how perception and action deal with senso-
rimotor conﬂicts, such as the rubber-hand illusion (RHI), is essential
to understand how the body adapts to uncertain situations. Recent re-
sults in humans have shown that the RHI not only produces a change in
the perceived arm location, but also causes involuntary forces. Here, we
describe a deep active inference agent in a virtual environment, which
we subjected to the RHI, that is able to account for these results. We
show that our model, which deals with visual high-dimensional inputs,
produces similar perceptual and force patterns to those found in humans.
Keywords: Active inference · Rubber-hand illusion · Free-energy opti-
mization · Deep learning.
1 Introduction
The complex mechanisms underlying perception and action that allow seamless
interaction with the environment are largely occluded from our consciousness.
To interact with the environment in a meaningful way, the brain must integrate
noisy sensory information from multiple modalities into a coherent world model,
from which to generate and continuously update an appropriate action [13]. Es-
pecially, how the brain-body deals with sensorimotor conﬂicts [8,16], e.g., con-
ﬂicting information from diﬀerent senses, is an essential question for both cog-
nitive science and artiﬁcial intelligence. Adaptation to unobserved events and
changes in the body and the environment during interaction is a key character-
istic of body intelligence that machines still fail at.
The rubber-hand illusion (RHI) [2] is a well-known experimental paradigm
from cognitive science that allows the investigation of body perception under
conﬂicting information in a controlled setup. During the experiment, human
participants cannot see their own hand but rather perceive an artiﬁcial hand
placed in a diﬀerent location (e.g. 15 cm from their current hand). After a minute
of visuo-tactile stimulation [10], the perceived location of the real hand drifts
towards the location of the artiﬁcial arm and suddenly the new hand becomes
part of their own.
arXiv:2008.07408v2  [cs.AI]  22 Dec 2020
2 T. Rood et al.
We can ﬁnd some RHI modelling attempts in the literature; see [12] for an
overview until 2015. In [18], a Bayesian causal inference model was proposed to
estimate the perceived hand position after stimulation. In [8] a model inspired by
the free-energy principle [5] was used to synthetically test the RHI in a robot. The
perceptual drift (mislocalization of the hand) was compared to that of humans
observations.
Recent experiments have shown that humans also generate meaningful force
patterns towards the artiﬁcial hand during the RHI [1,16], adding the action
dimension to this paradigm. We hypothesise that the strong interdependence
between perception and action can be accounted for by mechanisms underlying
active inference [7].
In this work, we propose a deep active inference model of the RHI, based
on [14,17,19], where an artiﬁcial agent directly operates in a 3D virtual reality
(VR) environment1. Our model 1) is able to produce similar perceptual and
active patterns to human observations during the RHI and 2) provides a scalable
approach for further research on body perception and active inference, as it
deals with high-dimensional inputs such as visual images originated from the 3D
environment.
2 Deep active inference model
We formalise body perception and action as an inference problem [11,3,7,17]. The
unobserved body state is inferred from the senses (observations) while taking
into account its state prior information. To this end, the agent makes use of two
sensory modalities. The visual input sv is described by a pixel matrix (image)
and the proprioceptive information sp represents the angle of every joint of the
arm – See Fig. 1a.
Computation of the body state is performed by optimizing the the variational
free-energy bound [7,17]. Under the mean-ﬁeld and Laplace approximations and
deﬁning µas the brain variables that encode the variational density that approx-
imates the body state distribution and deﬁning a as the action exerted by the
agent, perception and action are driven by the following system of diﬀerential
equations (see [6,4,19] for a derivation):
˙µ= −∂µF = −∂µeT
pΣ−1
p ep −∂µeT
vΣ−1
v ev −∂µeT
fΣ−1
µ ef (1)
˙a= −∂aF = −∂aeT
pΣ−1
p ep (2)
ep = sp −gp(µ) (3)
ev = sv −gv(µ) (4)
ef = −f(µ) (5)
Note that this model is a speciﬁc instance of the full active inference model [5]
tailored to the RHI experiment. We wrote the variational free-energy bound in
1 Code will be publicly available at https://github.com/thomasroodnl/
active-inference-rhi
A deep active inference model of the rubber-hand illusion 3
(a) Deep Active inference
2
µ 1024
FC1
8192
FC2
128 8
R
128 16
CT1
128 16
C1
64 32
CT2
64 32
C2
48 64
CT3
48 64
C3
32 128
CT4
32 128
C4
1 256
CT5
(b) Convolutional decoder
1 256
I
16 256
CV1
MP1
32 128
CV2
MP2
64 64
CV3
MP3
128 32
CV4
MP4
128 16
CV5
MP5
8192
R
4096
FC1
1024
FC2
512
FC3
2
FCµ
2
FClog(σ)
2
z
(c) VAE (encoder only)
Fig. 1: Deep active inference model for the virtual rubber-hand illusion. (a) The
brain variables µthat represent the body state are inferred through propriocep-
tive ep and visual ev prediction errors and their own dynamics f(µ). During the
VR immersion, the agent only sees the VR arm. The ensuing action is driven by
proprioceptive prediction errors. The generative visual process is approximated
by means of a deep neural network that encodes the sensory input into the body
state through a bottleneck. (b,c) Visual generative architectures tested.
terms of the prediction error e and for clarity, we split it into three terms that
correspond to the visual, proprioceptive and dynamical component of the body
state. The variances Σv,Σp,Σµ encode the reliability of the visual, propriocep-
tive and dynamics information, respectively, that is used to infer the body state.
The dynamics of the prediction errors are governed by diﬀerent generative pro-
cesses. Here, gv(µ) is the generative process of the visual information (i.e. the
predictor of the visual input given the brain state variables), gp(µ) is the propri-
oceptive generative process and f(µ) denotes internal state dynamics (i.e. how
the brain variables evolve in time) 2.
Due to the static characteristics of the passive RHI experiment we can sim-
plify the model. First, the generative dynamics model does not aﬀect body up-
date because the experimental setup does not allow for body movement. Second,
we fully describe the body state by the joint angles. This means that the sp and
the body state match. Thus, g(µ) = µ plus noise and the inverse mapping
∂µgp(µ) becomes an all-ones vector. Relaxing these two assumptions is out of
the scope of this paper. We can ﬁnally write the diﬀerential equations with the
2 Note that in Equation (5), the prediction error with respect to the internal dynamics
ef = µ′ −f(µ) was simpliﬁed to ef = −f(µ) under the assumption that µ′ = 0. In
other words, we assume no dynamics on the internal variables.
4 T. Rood et al.
generative models as follows:
˙µ= Σ−1
p (sp −gp(µ)) + ∂µgv(µ)TγΣ−1
v (sv −gv(µ)) (6)
˙a= −∆tΣ−1
p (sp −gp(µ)) (7)
where γ has been included in the visual term to modulate the level of causality
regarding whether the visual information has been produced by our body in the
RHI – see Sec. 2.2. Equation 7 is only valid if the action is the velocity of the
joint. Thus, the sensor change given the action corresponds to the time interval
between each iteration ∂as= ∆t.
We scale up the model to high-dimensional inputs such as images by ap-
proximating the visual generative model gv(µ) and the partial derivative of the
error with respect to the brain variables ∂µev by means of deep neural networks,
inspired by [19].
2.1 Generative model learning
We learn the forward and inverse generative process of the sensory input by
exploiting the representational capacity of deep neural networks. Although in
this work we only address the visual input, this method can be extended to any
other modality. To learn the the visual forward model gv(µ) we compare two
diﬀerent deep learning architectures, that is, a convolutional decoder (Fig. 1b)
and a variational autencoder (VAE, Fig. 1c).
The convolutional decoder was designed in similar fashion to the architecture
used in [19]. After training the relation between the visual input and the body
state, the visual prediction can be computed through the forward pass of the
network and its inverse ∂g(µ)/∂µ by means of the backward pass. The VAE
was designed using the same decoding structure as the convolutional decoder
to allow a fair performance comparison. This means that these models mainly
diﬀered in the way they were trained. In the VAE approach we train using the
full architecture and we just use the decoder to compute the predictions in the
model.
2.2 Modelling visuo-tactile stimulation synchrony
To synthetically replicate the RHI we need to model both synchronous and
asynchronous visuo-tactile stimulation conditions. We deﬁne the timepoints at
which a visual stimulation event and the corresponding tactile stimulation take
place, denoted tv and tt respectively. Inspired by the Bayesian causal model [18],
we distinguish between two causal explanations of the observed data. That is,
C = c1 signiﬁes that the observed (virtual) hand produced both the visual and
the tactile events whereas C = c2 signiﬁes that the observed hand produced the
visual event and our real hand produced the tactile event (visual and tactile input
come from two diﬀerent sources). The causal impact of the visual information
on the body state is represented by
γ = p(c1 |tv,tt) = p(tv,tt |c1)p(c1)
p(tv,tt |c1)p(c1) + p(tv,tt |c2)p(c2) (8)
A deep active inference model of the rubber-hand illusion 5
where p(tv,tt | c1) is deﬁned as a zero-mean Gaussian distribution over the
diﬀerence between the timepoints ( p(tv −tt |c1)) and p(tv,tt |c2) is deﬁned as
a uniform distribution since under c2, no relation between tv and tt is assumed.
This yields the update rule
γt+1 =
{ p(tv,tt|c1)γt
p(tv,tt|γt)·γt+p(tv,tt|c2)(1−γt) if visuo-tactile event
γt ·exp(−(t−max(tv,tt))2
∆−1
t
·rdecay), otherwise (9)
Note that γ is updated only in case of visuo-tactile events. Otherwise, an expo-
nential decay is applied.
Virtual arm: 
Left
Real arm: 
Center
Camera
Visuo-tactile 
stimulation
Fig. 2: Virtual environment and experimental setup modelled in the Unity engine.
3 Experimental setup
We modelled the RHI in a virtual environment created in Unity, as depicted
in Fig. 2. This environment was build to closely match the experimental setup
used in the human study described in [16]. This experiment exposed human
participants to a virtual arm located to the left and right of their real arm, and
applied visuo-tactile stimulation by showing a virtual ball touching the hand
and applying a corresponding vibration to the hand. Here, the agent’s control
consisted of two degrees of freedom: shoulder adduction/abduction and elbow
ﬂexion/extension. The environment provided proprioceptive information on the
shoulder and elbow joint angles to the agent. Visual sensory input to the model
originated from a camera located between the left and the right eye position,
producing 256 ×256 pixel grayscale images. Finally, the ML-Agents toolkit was
used to interface between the Unity environment and the agent in Python [9].
The agent arm was placed in a forward resting position such that the hand was
located 30 cm to the left of the body midline (center position). Three virtual
arm location conditions were evaluated: Left, Center and Right. The Center
condition matched the information given by proprioceptive input. Visuo-tactile
6 T. Rood et al.
stimulation was applied by generating a visual event at a regular interval of two
seconds, followed by a tactile event after a random delay sampled in the range
[0, 0.1) for synchronous stimulation and in the range [0, 1) for asynchronous
stimulation. The initial γ value was set to 0.01 and we ran N = 5 trials each for
30 s (1500 iterations).
L C R
1.0
0.5
0.0
0.5
1.0
Perceptual end effector drift (cm)
 
VAE
 
 
ConvDecoder
 
(a) Perception all
L C R
1.0
0.5
0.0
0.5
1.0 (b) Percept. Sync.
L C R
1.0
0.5
0.0
0.5
1.0 (c) Percept. Async.
L C R
0.4
0.2
0.0
0.2
0.4
Attempted acceleration (ms 2)
 
VAE
 
 
ConvDecoder
 
(d) Action all
L C R
0.4
0.2
0.0
0.2
0.4 (e) Action Sync.
L C R
0.4
0.2
0.0
0.2
0.4 (f) Action Async.
(g) Human recorded forces
/uni00000039/uni00000024/uni00000028/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000058/uni0000004f/uni00000047/uni00000048/uni00000055
/uni00000026/uni00000052/uni00000051/uni00000059/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000058/uni0000004f/uni00000047/uni00000048/uni00000055
/uni00000039/uni00000024/uni00000028/uni00000003/uni00000028/uni0000004f/uni00000045/uni00000052/uni0000005a
/uni00000026/uni00000052/uni00000051/uni00000059/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055/uni00000003/uni00000028/uni0000004f/uni00000045/uni00000052/uni0000005a
(h) Jacobian ∂µg(µ) learnt for both visual models.
Fig. 3: Model results. (a, b, c) Mean perceptual end-eﬀector drift (in cm). (d,e,f)
Mean horizontal end-eﬀector acceleration. (g) Mean forces exerted by human
participants in a virtual rubber-hand experiment (from [16]). (h) Visual repre-
sentation of the Jacobian learnt for the visual models.
A deep active inference model of the rubber-hand illusion 7
4 Results
We observed similar patterns in the drift of the perceived end-eﬀector location
(Fig. 3a) and the end-eﬀector action (Fig. 3). These agree with the behavioural
data obtained in human experiments (Fig. 3g). For the left and right condition,
we observed forces in the direction of the virtual hand during synchronous stim-
ulation (Fig. 3e). However, non-meaningful forces were produced using the con-
volutional decoder for the right condition. For the center condition, both models
produced near-zero average forces. Lastly, asynchronous stimulation produced,
with both models, attenuated forces (Fig. 3f). The learnt visual representation
diﬀered between the VAE and the Convolutional decoder approaches (Fig. 3h).
The VAE obtained smoother and more bounded visual Jacobian values, likely
due to its probabilistic latent space.
5 Conclusion
In this work, we described a deep active inference model to study body per-
ception and action during sensorimotor conﬂicts, such as the RHI. The model,
operating as an artiﬁcial agent in a virtual environment, was able to produce sim-
ilar perceptual and active patterns to those found in humans. Further research
will address how this model can be employed to investigate the construction of
the sensorimotor self [15].
References
1. Asai, T.: Illusory body-ownership entails automatic compensative movement: for
the uniﬁed representation between body and action. Experimental brain research
233(3), 777–785 (2015)
2. Botvinick, M., Cohen, J.: Rubber hands ‘feel’ touch that eyes see. Nature
391(6669), 756–756 (Feb 1998). https://doi.org/10.1038/35784, https://doi.
org/10.1038/35784
3. Botvinick, M., Toussaint, M.: Planning as inference. Trends in cognitive sciences
16(10), 485–488 (2012)
4. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
5. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nature Reviews
Neuroscience 11(2), 127–138 (Feb 2010). https://doi.org/10.1038/nrn2787,https:
//doi.org/10.1038/nrn2787
6. Friston, K., Mattout, J., Trujillo-Barreto, N., Ashburner, J., Penny, W.: Variational
free energy and the laplace approximation. Neuroimage 34(1), 220–234 (2007)
7. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behav-
ior: a free-energy formulation. Biological Cybernetics 102(3), 227–260 (Mar
2010). https://doi.org/10.1007/s00422-010-0364-z, https://doi.org/10.1007/
s00422-010-0364-z
8 T. Rood et al.
8. Hinz, N.A., Lanillos, P., Mueller, H., Cheng, G.: Drifting perceptual patterns
suggest prediction errors fusion rather than hypothesis selection: replicating the
rubber-hand illusion on a robot. arXiv preprint arXiv:1806.06809 (2018)
9. Juliani, A., Berges, V.P., Vckay, E., Gao, Y., Henry, H., Mattar, M., Lange, D.:
Unity: A general platform for intelligent agents (2018)
10. Kalckert, A., Ehrsson, H.H.: The onset time of the ownership sensation
in the moving rubber hand illusion. Frontiers in Psychology 8, 344
(2017). https://doi.org/10.3389/fpsyg.2017.00344, https://www.frontiersin.
org/article/10.3389/fpsyg.2017.00344
11. Kappen, H.J., G´ omez, V., Opper, M.: Optimal control as a graphical model infer-
ence problem. Machine learning 87(2), 159–182 (2012)
12. Kilteni, K., Maselli, A., Kording, K.P., Slater, M.: Over my fake body: body owner-
ship illusions for studying the multisensory basis of own-body perception. Frontiers
in human neuroscience 9, 141 (2015)
13. K¨ ording, K.P., Wolpert, D.M.: Bayesian integration in sensorimotor learning. Na-
ture 427(6971), 244–247 (Jan 2004). https://doi.org/10.1038/nature02169,https:
//doi.org/10.1038/nature02169
14. Lanillos, P., Cheng, G.: Adaptive robot body learning and estimation through pre-
dictive coding. In: 2018 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). pp. 4083–4090. IEEE (2018)
15. Lanillos, P., Dean-Leon, E., Cheng, G.: Enactive self: a study of engineering per-
spectives to obtain the sensorimotor self through enaction. In: Developmental
Learning and Epigenetic Robotics, Joint IEEE Int. Conf. on (2017)
16. Lanillos, P., Franklin, S., Franklin, D.W.: The predictive brain in ac-
tion: Involuntary actions reduce body prediction errors. bioRxiv (2020).
https://doi.org/10.1101/2020.07.08.191304, https://www.biorxiv.org/content/
early/2020/07/08/2020.07.08.191304
17. Oliver, G., Lanillos, P., Cheng, G.: Active inference body perception and action
for humanoid robots. arXiv preprint arXiv:1906.03022 (2019)
18. Samad, M., Chung, A.J., Shams, L.: Perception of body ownership is
driven by bayesian sensory inference. PloS one 10(2), e0117178–e0117178 (Feb
2015). https://doi.org/10.1371/journal.pone.0117178, https://pubmed.ncbi.nlm.
nih.gov/25658822
19. Sancaktar, C., van Gerven, M., Lanillos, P.: End-to-end pixel-based deep active
inference for body perception and action. arXiv preprint arXiv:2001.05847 (2020)