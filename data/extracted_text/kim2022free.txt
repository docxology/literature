arXiv:2203.14194v2  [q-bio.NC]  23 Nov 2022
Free energy and inference in living systems
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61 186, Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. Organisms are nonequilibrium, stationary systems self-organized v ia
spontaneous symmetry breaking and undergoing metabolic cycles w ith broken detailed
balance in the environment. The thermodynamic free-energy princ iple describes
an organism’s homeostasis as the regulation of biochemical work con strained by
the physical free-energy cost. In contrast, recent research in neuroscience and
theoretical biology explains a higher organism’s homeostasis and allos tasis as Bayesian
inference facilitated by the informational free energy. As an integ rated approach to
living systems, this study presents a free-energy minimization theo ry overarching the
essential features of both the thermodynamic and neuroscientiﬁ c free-energy principles.
Our results reveal that the perception and action of animals result from active
inference entailed by free-energy minimization in the brain, and the b rain operates
as Schr¨ odinger’s machine conducting the neural mechanics of minim izing sensory
uncertainty. A parsimonious model suggests that the Bayesian br ain develops the
optimal trajectories in neural manifolds and induces a dynamic bifur cation between
neural attractors in the process of active inference.
Keywords Living system, homeostasis and allostasis, Bayesian brain, free-en ergy
principle, Schr¨ odinger’s machine, neural attractor
24 November 2022
Free energy and inference in living systems 2
1. Introduction
Although there is no standard deﬁnition of life [78, 50, 42, 31, 28, 27 , 57], literature
often states that a living system tends to reduce its entropy, def ying the second law of
thermodynamics to sustain its nonequilibrium (NEQ) existence. Howe ver, conforming
to the second law of thermodynamics, adjudication between the en tropy reduction and
augmentation of an open system must depend on the direction of irr eversible heat ﬂux
at the system-reservoir interface. Organisms are open systems in the environment;
hence, they obey the second law by contributing to the total-entr opy increase in the
universe. The above confusion, perhaps, is rooted in Erwin Schr¨ o dinger’s annotation,
which metaphorically explains living organisms as feeding on negative en tropy [96].
In the same monograph, Schr¨ odinger continues to explicate that a more appropriate
discussion for metabolism is to be addressed in terms of free energy (FE). He made
this clariﬁcation because, in contrast to the Clausius entropy to wh ich he was referring,
thermodynamic FE always decreases during irreversible processes in any open system
[67]. Many studies have been based on Schr¨ odinger’s insight into how biological systems
can be explained by physical laws and principles. We examine the deﬁnit ion of life in
terms of FE minimization.
Organisms maintain biologically essential properties, such as body te mperature,
blood pressure, and glucose levels, which are distinct from ambient s tates. Living
systems continuously exchange heat and material ﬂuxes with the e nvironment by
performing metabolic work, which is subject to the energy balance d escribed by
the ﬁrst law of thermodynamics. The second law posits that the ent ropy of an
isolated macroscopic system increases monotonically with any spont aneous changes.
Organisms and the environment together constitute the biospher e, which is isolated
and macroscopic; thus, metabolic processes in organisms increase the total entropy.
The second law aﬀects organisms by limiting metabolic eﬃciency. The th ermodynamic
free-energy principle (TFEP) encompasses thermodynamic laws an d provides qualitative
and quantitative explanation of how living systems biophysically susta in homeostasis by
minimizing FEs. Recent studies have addressed the modern metabolis m perspective as
energy regulation of multisensory integration across both interoc eptive and exteroceptive
processes [15, 85]. This explains metabolism not only at the level of ind ividual
organisms, but also at the ecosystem and planetary levels [54, 43], a nd emphasizes the
energetics and power eﬃciency in brain performance [100, 6, 69]. In contrast, the ability
of organisms to undergo allostasis, which predictively regulates hom eostasis [105, 97],
or, more generally, their autopoietic properties [72], are unable to b e explained by
the TFEP. Allostatic ability is the main driver of adaptive ﬁtness, the e mergence of
which cannot be solely attributed to a (bio)physical self-organizat ion from a myriad
of emergent possibilities in the primitive circuits of neuronal activities . Organisms
are under environmental constraints, and adaptive ﬁtness, or n atural selection, is
the consequence of survival optimization in speciﬁc environments d uring evolution.
Therefore, the FE minimization scheme requires a top-down or high- level computational
Free energy and inference in living systems 3
mechanism that facilitates hardwiring of the allostatic capability.
The brain-inspired free-energy principle (FEP) in neuroscience and theoretical
biology suggests a universal biological principle in an axiomatic manner , which
provides the informational FE-minimization formalism, that account s for the perception,
learning, and behavior of living systems [33, 34]. This principle has been also applied to
other cognitive systems, such as artiﬁcial intelligence and robots [1 1, 71, 92, 12, 74, 73,
23]; however, our study primarily focuses on living systems and implica tions of the FEP
in a biological context, emphasizing the embodied nature of inferenc e [83]. According
to the informational free-energy principle (IFEP), all life forms ar e evolutionarily self-
organized to minimize surprisal, which is an information-theoretic measure of the
improbability or unexpectedness of the environmental niche of org anisms. Informational
FE (IFE) is a mathematical construct, rather than a physical (th ermodynamic) quantity,
speciﬁed by the temperature, chemical potential, volume, etc. In formational FE
mathematically bounds the surprisal from above; accordingly, the IFEP suggests that
natural selection reﬂects minimization of IFE in an organism as a prox y for surprisal
at all biological time scales. The IFEP employs Helmholtz’s early idea of p erception as
unconscious inference [111]: an organism’s brain possesses an inter nal model of sensory
generation and infers the external causes of sensory data by ma tching them with prior
knowledge. The active-inference framework following from the IFE P encapsulates motor
control and planning beyond Helmholtzian perception as an additiona l inferential scheme
[32, 1]. The brain possesses the probabilistic internal model whose p arameters (suﬃcient
statistics) are encoded by brain variables in the NEQ stationary sta te; however, thus
far, no physical theory has been developed for determining NEQ pr obabilities in the
macroscopic brain. In practice, the IFEP assumes open forms, or some ﬁxed forms,
for the NEQ densities and implements IFE minimization. The Gaussian ﬁx ed-form
assumption can be used to convert the IFE to a sum of discrepancie s between the
predicted and actual signals [10], which is known as prediction error in predictive
coding theory [48]. Commonly, the transformed IFE objective is minim ized by
employing the gradient-descent method widely used in machine learnin g [106]. The
resulting variational-ﬁltering equations compute the Bayesian inver sion of sensory data
by inferring the external sources [5], known as recognition dynamic s (RD) [33]. Recently,
the IFEP was generalized in a manner that minimizes sensory uncertainty , which is a
long-term surprisal over a temporal horizon of an organism’s chan ging environmental
niche [59]. Despite being a promising universal biological principle, the I FEP has led
to controversy regarding its success as the universal principle an d its distance between
biophysical reality and epistemological grounds [29, 62, 13, 87, 93 , 63, 8, 2, 9, 86].
In this study, the two FE approaches are jointly considered to dev elop a uniﬁed
paradigm for living systems: the TFEP does not describe the brain’s a bility to infer
and act in the environment, whereas the brain-inspired IFEP is mainly a purposive
(hypothesis-driven) framework lacking intimate connections to ne uronal substrates and
physical laws. Our goal is to link the two FEPs and propose a biological FEP that
integrates the reductionistic base and top-down teleology in the br ain. In addition,
Free energy and inference in living systems 4
we unveil the attractor dynamics that computes allostatic regulat ion, perception and
motor inference, in the brain, based on our proposed FE-minimizatio n framework. A
similar approach was reported in [35], in which formalisms underwriting s tochastic
thermodynamics and the IFEP were presented without addressing the direct link
between the thermodynamic and informational FE. In addition, a un iﬁed Bayesian and
thermodynamic view attempted to explain the brain’s learning and rec ognition as a
neural engine and proposed the laws of neurodynamics [102]. We also note another recent
work that made the neural manifold models from a symmetry-break ing mechanism in
brain-network synergetics, commensurate with the maximum infor mation principle [56].
In brain architecture, enormous degrees of freedom of neurona l activities pose the
classical negligence in a high-dimensional problem; thus, the underly ing neural dynamics
appears to be stochastic. However, we argue that perception, le arning, and motor-
inference in the brain is low-dimensional at the functional level, obey ing the law of large
numbers; accordingly, RD becomes deterministic, involving a limited nu mber of latent
variables. For instance, a few joint angles suﬃce for the brain to inf er arm movement
in motor control. In contrast, the emergence of deterministic RD is more subtle in
perception and learning, which demands a systematic coarse-grain ing of stochastic
neuronal dynamics. Our investigation facilitates the systematic de rivation of Bayesian-
brain RD in terms of a few eﬀective variables, which we term Bayesian mechanics (BM);
BM regulates the homeostasis and allostasis (that is, adaptive ﬁtne ss) of living systems,
conforming with the proposed biological FEP.
The concept of coarse-graining, or eﬀective description, is ubiquit ous in
computational neurosciences [46, 4, 89, 24, 38, 109, 14]. Here, w e review the
recent research relevant to our work, which motivated the develo pment of BM. Many
previous studies of recorded neurons showed that population dyn amics is conﬁned to
a low-dimensional manifold in empirical neural space, where traject ories are neural
representations of the population activity [22]. In mathematical te rms, the neural
modes were deﬁned as eigen-ﬁelds that span the neural manifold. T he latent variables,
or collective coordinates, were deﬁned as projection of the popula tion activity onto
the neural modes [40, 39]. Other theoretical models support the id ea that long-
term dynamics in recurrent neural networks gives rise to the attr actor manifold [75],
which is a continuous set of ﬁxed points occupying a limited region of ne ural space.
Consequently, the attractor dynamics and switching between diﬀe rent attractors were
manifested [76], indicating a contextual change in neuronal repres entations [113, 55].
Moreover, the manifold hypothesis is widely applied in machine learning t o approximate
high-dimensional data using a small number of parameters [17]. Expe rimental studies
showed that a dynamical collapse occurs in the brain from incoheren t baseline activity
to low-dimensional coherent activity across neural nodes [103, 82 , 110]. Synchronized
patterns emerged when the featured inputs and prediction derive d from prior or stored
knowledge matched; in contrast, when there was a mismatch, the h igh-dimensional
multi-unit activity increased. This observation also provided empirica l evidence that
neural signals reduce prediction errors, thereby minimizing the IFE .
Free energy and inference in living systems 5
Based on the results described above, we suggest that the latent dynamics can
be eﬀectively described by a small number of coarse-grained variab les in the reduced
dimension. In this study, we formulate the BM of inferential regulat ion of homeostasis
in living systems in terms of a few latent variables. The latent variables are determined
as the brain activities and their conjugate momenta that represen t the external,
environmental and motor, states and online prediction errors, re spectively. The sensory
error at the peripheral level acts as a time-dependent driving sou rce in BM, providing
the neural mechanism for sensory, as well as motor, inferences. Our continuous-state
formulation in continuous time may be useful for studying situated- action problems in
which biological systems must make decisions even during ongoing sen sorimotor activity
[16].
The remainder of this paper is organized as follows. In Section 2, we d escribe the
establishment of the TFEP from NEQ ﬂuctuation theorems when app lied to living
systems. Section 3 explains how stochastic dynamics at the neuron al level can be
modeled and how a statistical approach can be used to determine th e NEQ densities
of neural states in the physical brain. In Section 4, we present th e proposed biological
IFEP minimizing long-term surprisal and establish its continuous-sta te implementation
that yields BM in the neural phase space. Next, in Section 5, we nume rically integrate
BM and manifest the attractor dynamics that performs perceptio n and motor inference
in the brain. Finally, we summarize important outcomes of our investig ation and the
conclusions in Section 6. In Appendix, we present the dual closed-lo op circuitry of active
inference resulting from our model.
2. Nonequilibrium ﬂuctuation theorems applied to organism s
Fluctuation theorems (FTs) concisely describe stochastic NEQ pro cesses in terms
of mathematical equalities [52, 20]. Although FTs were initially establish ed for
small systems, where ﬂuctuations are appreciable, they also apply to macroscopic
deterministic dynamics [58]. Here, we present FTs in an appropriate c ontext of biological
problems and propose that the FTs suggest a living organism is an NEQ system that
maintains the housekeeping temperature, T , (average 36 .5 0 C in humans) within its body
and employs metabolism isothermally to act against its environment.
To this end, among the various representations of FTs, we use the NEQ work
relation [52]:
xe´ β pW ´ ∆ F qy “ 1, (1)
where β “ kBT , with kB being the Boltzmann constant and T being the temperature as
described below. The mathematical equality given in Eq. (1) is known a s the Jarzynski
relation [98]. Here, W is the amount of experimental work performed on a small system
immersed in a thermal reservoir and ∆ F is the induced change in the Helmholtz FE of
the system. Accordingly, W ´ ∆ F is the excess energy associated with each irreversible
work process in the system, which is unavailable for a useful conver sion. The bracket,
Free energy and inference in living systems 6
x¨ ¨ ¨y , indicates the average over many work strokes, that is, work dist ribution subject to
a protocol. The average must be considered because the experime ntal work performance
on small systems ﬂuctuates.
The Jarzynski relation can be converted to an expression for ent ropy as follows. By
applying xe´ βW y ě e´ β xW y to Eq. (1), which is known as the Jensen inequality [19], we
obtain the inequality ∆ F ď x W y. This inequality is an alternative expression that can be
used to apply the second law to isothermal irreversible processes of the system initially
prepared in equilibrium with a reservoir [58]. Using the inequality, one ca n consider
the change in the average total entropy: x∆ Stoty “ x ∆ Ssysy ` x ∆ SRy, where ∆ Ssys
is the change in the system entropy and ∆ SR is the change in the reservoir entropy.
The average associated with ∆ SR, which is reversible by deﬁnition, can be further
manipulated to obtain x∆ SRy “ ´x Qsysy{T “ px W y ´ ∆ Uq{T , where QR “ ´ Qsys is
used in the ﬁrst step, and then the thermodynamic ﬁrst law is applied for xQsysy ; U is
the internal energy of the system. Therefore, T x∆ Stoty “ x W y ´ p ∆ U ´ T x∆ Ssysyq “
xW y ´ ∆ F , which leads to the stochastic second law for the combined small sys tem and
reservoir:
x∆ Stoty ě 0. (2)
The possibility of tightening the preceding inequality has been investig ated among
researchers by revealing a nonzero, positive bound, leading to the rmodynamic
uncertainty relations [47, 99]. The unavailable energy associated wit h individual work
processes amounts to the total entropy change, namely, βpW ´ ∆ F q “ k´ 1
B ∆ Stot under
isothermal conditions. By applying the ﬁnal identity to Eq. (1), the Jarzynski equality
is cast to the integral form of entropy ﬂuctuation:
xe´ k´ 1
B ∆ Stot y “ 1. (3)
In the biological context, W is the amount of environmental work involved in the
metabolism of a living system, such as the biological reactions of oxyg enic photosynthesis
and aerobic respiration [3, 107]. The biological work is not controllable and thus,
stochastic. The FTs describe the imbalance between energy intake and expenditure
in an organism while maintaining the housekeeping temperature. The Helmholtz FE
increment in the living system over a metabolic work cycle is limited by the average
environmental work done on the organism. The resulting inequality f rom the Jarzynski
relation can be written in the organism-centric form as
xWy ď ∆ F, (4)
where we set xWy “ ´x W y and ∆ F ” ´ ∆ F , which now states that the work
performance, xWy, of a biological system against the environment (e.g., via metabolism)
is bounded from above by the thermodynamic FE cost, ∆ F. The preceding inequality
reﬂects the limited eﬃciency of metabolic work in living systems. Rare in dividual
processes that violate Eq. (4) may occur in small systems; howeve r, such statistical
deﬂection is not expected in a ﬁnite biological system with macroscop ic degrees of
freedom. The equality in Eq. (4) holds for reversible work cycles in ina nimate matter,
Free energy and inference in living systems 7
attaining thermodynamic eﬃciency at its maximum, but not in the meta bolic processes
of living organisms, which are irreversible. Our consideration of meta bolic work may be
generalized to the multi-level autocatalytic cycles suggested as th e chemical origins of
life [68].
Note here that we considered the temperature appearing in the Ja rzynski relation
as the body temperature of a speciﬁc biological system, unlike the u sual implication
of FTs; in the standard derivation of the Jarzynski relation [53], th e temperature, T ,
appearing in the NEQ equality is, by construction, the reservoir tem perature. The FT is
generally intended for an irreversible process during which the syst em temperature may
not be deﬁned. However, the initial and end states must be in equilibr ium so that the
FE is meaningful. The subtlety lies in the fact that the end-state tem perature may or
may not be the same as the reservoir temperature for experiment s performed in isolation
after the initial equilibrium preparation. Living organisms are in an NEQ stationary
state, maintaining a housekeeping temperature, T , that is distinct from the ambient
temperature, to which they equilibrate only when ceasing to exist. T hus, organisms are
viewed as isothermal systems, which are open to heat and particle exchange with the
environment.
The NEQ work relation expresses the second law of thermodynamics as the
mathematical equality in Eq. (1). The second law, in its biological cont ext, renders the
thermodynamic constraint on living organisms given by the inequality in Eq. (4), which
reveals the inevitable (thermodynamic) FE waste produced during m etabolic cycles.
However, this inequality accounts for neither self-adaptiveness n or brain functions,
such as perception, learning, and behavior. To address these ess ential features of life,
researchers currently employ a hybridizing scheme, which ﬁrst pro poses how the system-
level biological functions operate and then attempts to make conn ections to biophysical
substances. Particularly, the Bayesian mechanism built into the IFE P provides a crucial
component in this promising hybrid explanation of life, which is describe d in detail in
Section 4.
3. Statistical-physical description of the nonequilibriu m brain
The brain is comprised of a myriad of complex neurons; accordingly, it s internal
dynamics at the mesoscopic level must obey some stochastic equat ions of motion on
account of classical indeterminacy. The relevant coarse-grained neural variables are
local-scale population activities, or intra-area brain rhythms. In th e following, we
consider that the brain matter itself constitutes the thermal env ironment at body
temperature for the mesoscopic neural dynamics.
Below, we assume that the neural activity, µ, at the coarse-grained population level
obeys the stochastic dynamics [64]:
dµ
dt “ fpµ; tq ` wptq, (5)
where the inertial term in the Langevin equation was dropped by tak ing the over-
Free energy and inference in living systems 8
damping limit. Here, f may represent both conservative and time-dependent metabolic
forces, and w represents random ﬂuctuation characterized as a delta-correla ted Gaussian
noise satisfying the following conditions:
xwptqy “ 0 and xwptqwpt1qy “ Iδpt ´ t1q,
where I is the noise strength. In one dimension (1D), for simplicity, the envir onmental
perturbation and noise strength are physically speciﬁed, respect ively, as [88]
f “ 1
mγ A and I “ 2kBT
mγ ,
where A is a conservative force acting on a neural unit with mass, m, neglecting time-
dependent driving, T is the body temperature, and γ is the phenomenological frictional
coeﬃcient whose inverse corresponds to momentum relaxation time . The solutions to
Eq. (5) describe the individual trajectories of random dynamical p rocesses.
In general, colored noises can be considered beyond the delta-cor related white noise
by generalizing Eq. (5) to incorporate the non-Markovian memory e ﬀect:
m
ż t
´8
dt1γpt ´ t1q 9µpt1q “ Apµq ` ζ.
To ensure equilibrium at temperature T , the colored Langevin equation must satisfy
the ﬂuctuation-dissipation theorem that accounts for the nonsin gular noise correlation
[114]:
xζptqζpt1qy “ 2kBT γp|t ´ t1 |q.
A standard example of such colored noise is the Orstein-Uhlenbeck m emory kernel given
by γp|t ´ t1 |q “ γτ ´ 1 expp´|t ´ t1 |{τq, where τ is the noise autocorrelation time.
As an alternative to the Langevin equation [Eq. (5)], one may collectiv ely consider
an ensemble of identical systems displaying various values of the sta te, µ, and ask
how the statistical distribution changes over time. After normaliza tion, the ensemble
distribution is reduced to the probability density, say ppµ, tq, so that ppµ, tqdµ speciﬁes
the probability that an individual Brownian particle is found in the rang e pµ, µ ` dµq
at time t. In the Markovian approximation, the change in the probability dens ity is
determined by the probability density at the current time, which is ge nerally described
by the master equation given in the continuous-state formulation a s
Bppµ, tq
Bt “
ż !
wpµ, µ1qppµ1, tq ´ wpµ1, µqppµ, tq
)
dµ1, (6)
where wpµ1, µq is the transition rate of the state change from µ to another µ1. We
further assume that the transition occurs between two inﬁnitesim ally close states, µ and
µ1, where µ1 ´ µ “ x ! 1, so that the transition rate sharply peaks at around x “ 0 to
approximate the value as wpµ1, µq « wpµ; xq. Then, ppµ1q can be expanded about µ to
the second-order in x and all higher-order terms are neglected. Consequently, the mas ter
equation can be converted into the Smoluchowski-Fokker-Planck ( S-F-P) equation [88]:
Bppµ, tq
Bt “ B
Bµ
!
´ D1pµq ` B
BµD2pµq
)
ppµ, tq, (7)
Free energy and inference in living systems 9
where, D1 and D2 correspond to the ﬁrst two expansion coeﬃcients in the Kramers-
Moyal formalism, which are determined in the present case to be
D1 “ f and D2 “ 1
2I.
The S-F-P equation can be expressed in three dimensions (3D) as
Bpp⃗ µ, tq
Bt ` ∇ ¨
!
⃗fp⃗ µq ´ D∇
)
pp⃗ µ, tq “ 0, (8)
where ∇ is the gradient operation with respect to the three-dimensional st ate, ⃗ µ. In
Eq. (8), the drift term, p ⃗f, accounts for conservative potential forces. In addition,
the diﬀusion term, D2, is denoted as D, assuming spatial isotropy, for simplicity and
notational convention.
The S-F-P equation describes local conservation of the probability , pp⃗ µ, tq, in the
state space spanned by the state vector, ⃗ µ, which carries the probability ﬂux, ⃗j, identiﬁed
as
⃗jp⃗ µ, tq “ pp⃗ µ, tq ⃗fp⃗ µq ´ D∇pp⃗ µ, tq.
In steady state (SS), Bpst{Bt “ 0, where pst ” ppµ, 8q; accordingly, the divergence of
the SS ﬂux, ⃗jst ” ⃗jpµ, 8q, must vanish in the S-F-P equation:
∇ ¨ ⃗jst “ 0. (9)
If Brownian particles undergo motion in an isolated or inﬁnite medium, ⃗jst should
disappear on the local boundary because the total ﬂux through t he surface must vanish
to ensure probability conservation. ; Because the ﬂux must be continuous over the entire
space, the SS condition in Eq. (9) imposes ⃗jst ” 0 everywhere, reﬂecting the detailed
balance between the drift ﬂux and dissipative ﬂux. In this case, the system holds in
equilibrium, where life ceases to exist. The equilibrium probability can be obtained
from the condition jst “ 0, giving canonical Boltzmann probability as the result:
peqpµq9 expt´βV pµqu,
where β “ 1{kBT and V pµq is potential energy. The kinetic-energy term does not
appear in peq because the Langevin dynamics we consider are in the over-damping limit.
However, for a ﬁnite open system, such as a living organism, the sys tem’s SS ﬂux
does not necessarily vanish on the local boundary; instead, it must be compensated
by the environmental aﬀerent or eﬀerent ﬂuxes to achieve stead y state. Thus, for a
living system, the detailed balance is not satisﬁed in the steady state [41, 70]; that is,
⃗jst ‰ 0. Instead, the vanishing condition of the divergence of the proba bility ﬂux entails
a necessary balance. The mathematical expression in Eq. (9) admit s a non-vanishing
vector ﬁeld ⃗Bp⃗ µq via
⃗jstp⃗ µq ” ∇ ˆ ⃗Bp⃗ µq, (10)
; In an isolated or inﬁnite medium, the net ﬂux through the entire surf ace must vanish to ensure
probability conservation, i.e., ⃗jnet ”
ű ⃗jst ¨ d⃗ a“ 0, where d⃗ ais the outward, inﬁnitesimal area element.
Accordingly, ⃗jst “ 0 at every point on the surface.
Free energy and inference in living systems 10
which shows that the SS ﬂux is divergenceless or, equivalently, solen oidal [112, 84, 94].
The life ﬂux, ⃗jst, deﬁned in this manner is unchanged when ⃗B is transformed to
⃗B1 “ ⃗B ` ∇Λ, where Λ is a scalar function of the state, ⃗ µ.§ From Eq. (10), the
following generalized balance condition must hold locally on the boundar y:
pstp⃗ µq ⃗fstp⃗ µq “ D∇pstp⃗ µq ` ∇ ˆ ⃗Bp⃗ µq. (11)
The above modiﬁed detailed-balance condition supports the frequent interpretation of
the force ﬁeld, ⃗fst, as the gradient ﬂow of the SS probability, pst [31, 80]:
⃗fstp⃗ µq “ p D ´ Qq∇ ln pstp⃗ µq, (12)
where we introduced the isotropic coeﬃcient, Q, via
Q∇ ln pst ” ´ p´ 1
st ∇ ˆ ⃗B;
for simplicity, the coeﬃcient Q is assumed to be isotropic as it was for the diﬀusion
constant, D. The gradient ﬂow is driven by entropy because the most likely equilibr ium
state of the combined system and environment is achieved by maximiz ing the total
entropy; hence, it is an entropic force, conforming to the second law. Note that Eq. (10)
mimics the Ampere law in magnetism [44]; the eﬀective ﬁeld ⃗B may be construed as
an induced ﬁeld by the static current, ⃗jst. Accordingly, the vector ﬁeld, ⃗Bp⃗ µq, can be
determined by means of
⃗Bp⃗ µq “ 1
4π
ż
⃗jstp⃗ µ1q ˆ p⃗ µ´ ⃗ µ1 q
|⃗ µ´ ⃗ µ1|3 d⃗ µ1. (13)
Note that the modiﬁed detailed-balance condition given in Eq. (11) is o nly a formal
description for determining the NEQ density, pst, given SS ﬂux, ⃗jst, or, equivalently,
the environmental magnetic ﬁeld, ⃗B, in Eq. (13). Precise determination of pst
is an independent research subject, which may be non-Gaussian wit h a colored
autocorrelation.
In general, it is diﬃcult to obtain an analytic expression for the NEQ pr obability
density for open systems, except in low-density and/or linear-res ponse regimes [61, 30].
Because of morphological complexity, it is practically intractable to d erive the NEQ
densities specifying the physical brain states. Accordingly, the ne ural states under
continual sensory perturbation are assumed to be statistically de scribed by time-
dependent Gaussian densities, predicted from Gaussian random no ises imposed on the
Langevin description.
4. Latent dynamics of sensorimotor inference in the brain
Here, we present the BM for conducting Bayesian inversion of sens ory observation in
the brain under the proposed generalized IFEP. This idea was previo usly developed by
considering passive perception [59] and only implicitly including active inf erence [60].
§ The freedom to choose the vector ﬁeld, ⃗B, without aﬀecting the physical quantity, ⃗jst, is known as
gauge symmetry. Recently, researchers attempted to determine the implication an d utility of the gauge
transformation in neuronal dynamics in the brain and emergent fun ctions [101, 91].
Free energy and inference in living systems 11
Here, we advance this formalism by explicitly introducing motor infere nce and planning
in the generative models to fully conform to the active-inference fr amework.
The environmental states, ϑ, generate sensory stimuli, ϕ, at the organism’s
receptors through mechanical, optical, or chemical perturbation s, which are transduced
in the brain’s functional hierarchy in the form of a nervous signal. Th e sensory
perturbations may be altered by the organism’s motor manipulation, and we designate
u to denote the motor variables responsible for such control over t he eﬀectors. A crucial
point here is that the brain has access only to the sensory data and not their causes;
accordingly, from the brain’s perspective, both the environmenta l states, ϑ, and motor
variables, u, are external, that is, hidden. In terms of these relevant variables, we deﬁne
the variational IFE functional, denoted as F:
Frqpϑ, uq, ppϕ; ϑ, uqs ”
ż
dϑ
ż
du q pϑ, uq ln qpϑ, uq
ppϕ; ϑ, uq , (14)
where qpϑ, uq and ppϕ; ϑ, uq are the recognition density (R-density) and generative
density (G-density), respectively. The R-density is the brain’s onlin e estimate
of posterior beliefs about the external causes of the sensory pe rturbation (it
probabilistically represents the environmental states). The G-de nsity encapsulates
the brain’s likelihood in beliefs about sensory-data generation and pr ior beliefs about
the hidden environmental as well as motor dynamics (it probabilistica lly speciﬁes
the internal model of sensory-data generation, environmental dynamics, and motor
feedback). Note that whereas the R-density is the current estim ate, the G-density
contains the stored information in the brain, which can be updated b y learning. In this
study, we generalize the R-density as a bi-modal probability of ϑ and u, and G-density
as a tri-modal probability of ϑ, u, and ϕ. Note that a semicolon is used between the
sensory perturbation, ϕ, and hidden variables ϑ and u in the G-density rather than
a comma to emphasize their diﬀerential role in perception. The explicit inclusion of
the motor variable, u, in the q and p densities is a key advancement over the standard
deﬁnition of IFE [10].
Now, using the product rule, ppϕ; ϑ, uq “ ppϑ, u|ϕqppϕq for the G-density in
Eq. (14), we decompose the IFE to a form applicable in the biological c ontext:
Frqpϑ, uq, ppϕ; ϑ, uqs “ DKL pqpϑ, uq}ppϑ, u|ϕqq ´ ln ppϕq,
where DKL is the Kullback-Leibler divergence [19]. Because DKL is non-negative, the
following inequality holds, which underpins the IFEP described in Sectio n 1:
´ ln ppϕq ď Frqpϑ, uq, ppϕ; ϑ, uqs, (15)
where ´ ln ppϕq is the information-theoretic surprisal. Here, it is important to notice
the resemblance between the preceding inequality and that given in E q. (4) from the
TFEP.
Under the IFEP, the organism’s cognitive goal is to infer the hidden e nvironmental
causes of sensory inputs with feedback from the motor-behavior inference. This goal is
achieved by minimizing F with respect to the R-density, qpϑ, uq, which corresponds to
Free energy and inference in living systems 12
the online adaptation of the sensory and motor modules in the brain. For instance, in
the classic reﬂex arc, the proprioceptive stimulus evokes the activ ity of sensory neurons
in the dorsal root, and the motor variable is engaged by the eﬀecto r’s active states
of the motor neurons in the ventral root. The double procedures are involved in the
minimization scheme to cope with the bi-modal cognitive nature of sen sory and motor
inferences: 1) the internal model is updated to better predict th e sensory perturbation,
and 2) the sensory perturbation is modiﬁed by the agent’s motor en gagement to further
reduce the residual discrepancy with the internal model. The form er is termed as passive
perception and the latter as active perception. However, the two inferential mechanisms
do not separately engage, but act as a whole in the sensorimotor clo sed loop in the
embodied agent, and are therefore jointly termed as active inference under the IFEP
[32, 1].
To draw a connection between the IFE minimization and neuronal cor relates, it
is practically convenient to use the ﬁxed form for the unknown R-de nsity [10], whose
suﬃcient statistics are assumed to be encoded neurophysiologically by brain variables,
that is, neuronal activities. Here, we write the R-density as qpϑ, uq “ qpϑqqpuq by
considering the external variables ϑ and u as conditionally independent. Furthermore,
it is assumed that the factorizing densities, qpϑq and qpuq, are Gaussian; the means of
the environmental states, ϑ, and motor states, u, are encoded by the neuronal variables
µ and a, respectively. Then, by performing technical approximations simila r to those
used in [10], we convert the IFE functional, F, of the R- and G- densities to the IFE
function, F , of the neural representations µ and a, given sensory data, s. The sensory
data or inputs are a neural representation of the evoked pertur bation, ϕ, at the receptors,
detected by the organism’s brain. Here, the homunculus hypothes is, the brain as a neural
observer, is implicit, which assumes teleological homology between th e environmental
processes and brain’s internal dynamics.
The result for the IFE function, up to an additive constant, is given as
F pµ, a; sq “ ´ ln pps; µ, aq; (16)
here, the dependence on the second-order suﬃcient statistics, namely (co)variances
of the R-density, was optimally removed. Consequently, the brain m ust only update
the means in the R-density in conducting the latent RD. The mathema tical procedure
involved in Eq. (16) extends the Laplace approximation delineated in t he review [10].
To complete the Laplace-encoded IFE, one must specify the infere ntial structure in
the encoded G-density, pps; µ, aq. We facilitate probabilistic implementation of the
generative model using the product rule:
pps; µ, aq “ pps|µ, aqppµ, aq, (17)
where the likelihood density, pps|µ, aq, is the brain’s concurrent estimation of the
encoded sensory data, s, from the neuronal response, µ, and motor manipulation, a.
Assuming conditional independence between µ and a, the joint prior ppµ, aq can be
further factorized as
ppµ, aq “ ppµqppaq,
Free energy and inference in living systems 13
where ppµq and ppaq are the brain’s prior beliefs regarding the environmental-state
changes and motor dynamics, respectively. Thus, the Laplace-en coded IFE has been
speciﬁed solely in terms of the neural variables µ and a, which is suitable for biologically-
plausible implementation of active inference in the physical brain.
Sensory states, s, evoked by exogenous stimuli, neurophysically activate the
neuronal population in the brain. The population dynamics is complex a nd high-
dimensional; however, the RD of the perceptual and behavioral inf erences may be well-
described in lower-dimensional neural manifolds. Below, we formulat e the generative
equations of latent neural modes considering classical indetermina cy. First, we assume
that sensory data, s, encoded at the receptors are measured by a neural observer
according to instant mapping:
s “ gpµ, a; θgq ` z, (18)
where g is the generative model of the sensory data and z is the observation noise. The
generative map, gpµ, aq, encapsulates both the perceptual states, µ, and motor states, a,
which conjointly predict the sensory data, s. We consider the sensory generative model
as a continuous process of sensory prediction by µ and error prediction by a via the
eﬀector alteration:
rs ´ g1pµqs ´ g2paq ” s ´ gpµ, aq,
where we set gpµ, aq “ g1pµq ` g2paq. Second, we assume that the neural activity, µ,
obeys internal dynamics as described in Section 3:
dµ
dt “ fpµ; θf q ` w, (19)
where f is the generative model of the neuronal change, and w is the involved random
noise. Third, we assume that the motor state, a, bears the motor-neural dynamics:
da
dt “ πpa; θπ q ` η, (20)
where π is the generative model of the motor-neuronal change and η is the noise in
the process. The generative function, π, in Eq. (20) functions as the policy in machine
learning [106]: the policy, πpa; θπ q, encapsulates the internal model of motor planning
in continuous time. The dependence of the generative models on the parameters
θg, θ f , and θπ enables incorporation of a longer-term neural eﬃcacy, such as sy naptic
plasticity; below, we omit the parameter dependence for notationa l simplicity. For the
neuronal generative equations, the continuous Hodgkin-Huxley m odel [59] or a more
biophysically realistic model can be employed; however, our simple mod el in Section 5
suﬃces to unveil the emergence of BM.
Noises in the neural generative models [Eqs. (18)-(20)] indicate st ochastic
mismatches between the cognitive objectives on the left-hand side (LHS) and their
prediction through the generative functions/map. Accordingly, w e consider that z, w,
and η neurophysically encode the probabilistic generative models pps|µ, aq, ppµq, and
ppaq, respectively, [Eq. (17)] in the neuronal dynamics. Furthermore , we assume that the
Free energy and inference in living systems 14
random noises are continuously distributed according to the norma lized NEQ Gaussian.
Therefore, the Laplace-encoded likelihood, pps|µ, aq, and prior densities, ppµq and ppaq,
in Eq. (17) can be assumed to take the following forms:
pps|µ, aq “ N ps ´ g; 0, σzq,
ppµq “ N p 9µ ´ f; 0, σwq, (21)
ppaq “ N p 9a ´ π; 0, σηq;
here, N px ´ h; 0, σq ” expt´ 1
2σ px ´ hq2u{
?
2πσ denotes a Gaussian density of stochastic
variable x´h with variance σ about the zero mean }, and 9x denotes the time derivative of
x, that is, dx{dt. The generative likelihood and prior densities in Eq. (21) are thought
to be stationary solutions to the S-F-P equation or a more general non-Markovian
extension, the biophysical derivation of which is beyond the scope o f this work. Instead,
we assume the time-dependent Gaussian probabilities eﬀectively at z ero temperature
as physically admissible densities encoding internal models in the brain. Removing the
assumption by deriving physical probability densities is a key theoret ical demand in
future studies.
Next, by substituting the expressions in Eq. (21) into Eq. (16) usin g the
decompositions in Eq. (17), we obtain an explicit expression for the I FE function at
an instant t:
F pµ, a; sq “ 1
2σz
ps ´ gpµ, aqq2 ` 1
2σw
p 9µ ´ fpµqq2 ` 1
2ση
p 9a ´ πpaqq2, (22)
where we dismissed the term 1
2 ln tσzσwση u [59]. Our speciﬁc construct of the IFE
encapsulates motor planning explicitly in continuous time via the policy, πpaq, in the
generative models. Based on the Laplace-encoded IFE, the mathe matical statement for
the biological FEP is given asż
dt t´ ln ppsqu ď
ż
dtF pµ, a; sq, (23)
where the LHS is equivalent to the Shannon uncertainty,
ş
ds t´ ln ppsqu ppsq, under the
ergodic assumption, which is assured by the NEQ stationarity of living systems. The
inequality [Eq. (23)] shows that the upper bound of sensory uncer tainty can be estimated
by minimizing the time integral of F over a temporal horizon. Accordingly, if we regard
the integrand F as a Lagrangian, the systematic framework of the Hamilton principle
can be employed to implement the minimization scheme [66]. Next, we cas t Eq. (22) to
a weighted summation of the quadratic terms: F “ 1
2
ř
i miε2
i pi “ w, z, a q, which can
be expressed as a total time derivative that does not aﬀect the re sulting BM [66]. In
the summation, we deﬁned the notations εi as
εw ” 9µ ´ fpµq,
εη ” 9a ´ πpaq, (24)
εz ” s ´ gpµ, aq,
} Here, we use σ, not σ2, to denote the variances only to be consistent with the notations in an earlier
publication [10].
Free energy and inference in living systems 15
which represent the prediction errors involved in state, motor, and sensory inferences,
respectively. Additionally, the weight factors, mw, m η , and mz, are deﬁned through the
variances as
mw ” 1{σw, m η ” 1{ση , and mz ” 1{σz, (25)
where mi may be considered as a metaphor for the neural inertial masses . The neural
masses correspond to the predictive precisions in the standard terminology [10]; heavier
neural masses lead to more precise predictions. The IFE F as a Lagr angian, conforming
to classical dynamics, can be considered as a function of the instant trajectories of µptq
and aptq, subject to the time-dependent force, s “ sptq.
To exercise the Hamilton principle, we deﬁne the classical Action , S, as the time
integral of arbitrary trajectories µptq and aptq in the conﬁgurational state space:
Srµptq, aptq; tqs “
ż t
t0
dt1 F pµpt1q, apt1q; spt1 qq , (26)
where t0 is the initial time, and τ ” t ´ t0 is the temporal horizon of the relevant
biological process. The initial time can be chosen either in the past, t hat is, t0 Ñ ´8 ,
or at present, that is, t0 “ 0. In the former, t is the present time, whereas in the
latter, t is the future time. Hence, active inference of the living systems mat hematically
corresponds to varying S, subject to the sensory stream, to ﬁnd an optimal trajectory
in the conﬁgurational state space spanned by µ and a.
Further, it is advantageous to consider the brain’s RD in phase spac e rather than
conﬁgurational space; the phase space is spanned by positions an d momenta. This is
because the momentum variables are meaningful prediction errors in the brain’s message
passing algorithms; they are deﬁned via the informational Lagrang ian, F , as
pµ ” BF
B 9µ “ mwp 9µ ´ fq, (27)
pa ” BF
B 9a “ mη p 9a ´ πq, (28)
where pµ and pa are the momentum conjugates corresponding to µ and a, respectively.
Equation (24) reveals that the momenta, pµ and pa, are indeed the prediction errors,
εµ and εη, weighted by the neural masses, mw and mz, respectively. The purposive
Hamiltonian, H, can be obtained by performing the Legendre transformation H ”
pµ 9µ` pa 9a´ F . After straightforward manipulation, we obtain the Hamiltonian fun ction:
Hpµ, a, pµ , pa; sq “ 1
2mw
p2
µ ` 1
2mη
p2
a ` pµ fpµq ` paπpaq ´ 1
2mzε2
z, (29)
which is a generator of time evolution in neural phase space. The fun ction H is speciﬁed
in the cognitive phase space spanned by the four-component colum n vector, Ψ, in the
present single-column formulation, whose components are deﬁned as
Ψ T “ p Ψ 1, Ψ 2, Ψ 3, Ψ 4q ” p µ, a, pµ , paq,
Free energy and inference in living systems 16
where Ψ T is the transpose of Ψ. Having determined the Hamiltonian, the Bayes ian
mechanical equations of motion (termed as BM) can be abstractly w ritten in the
symplectic representation as
9Ψ i “ ´ Jij
BH
BΨ j
, (30)
where the block matrix J is deﬁned as
J ”
˜
0 ´1
1 0
¸
, where 1 “
˜
1 0
0 1
¸
.
Speciﬁcally, we unpack Eq. (30) and explicitly display the outcome:
9µ “ 1
mw
pµ ` fpµq, (31)
9a “ 1
mη
pa ` πpaq, (32)
9pµ “ ´ pµ
Bf
Bµ ´ mzps ´ gq Bg
Bµ, (33)
9pa “ ´ pa
Bπ
Ba ´ mzps ´ gq Bg
Ba, (34)
which are a coupled set of diﬀerential equations that are nonlinear, in general.
The preceding Eqs. (31)–(34) comprise the BM of the brain variable s, which execute
the RD of the Bayesian perception and motor inference in the brain. The BM was
attained by applying the Hamilton principle, for which we adopted the L aplace-encoded
IFE as an informational Lagrangian and derived the Hamiltonian to ge nerate the
equations of motion. Our latent variables are the neural represen tations ( µ,a) and
their conjugate momenta ( pµ ,pa); they span the reduced-dimensional neural manifold.
The momenta represent the prediction errors neurophysiologically encoded by the error
units in the neuronal population. Below, we describe two signiﬁcant f eatures of the
latent dynamics, governed by the BM, subjected to the time-vary ing sensory input,
sptq.
(i) Equations (31)–(34) suggest that the brain mechanistically exe cutes the cognitive
operation, which reﬂects Schr¨ odinger’s suggestion of an organis m as a mechanical
work [96]. Our derived BM addresses the continuous-state implemen tation of IFE
minimization in continuous time, which contrasts common discrete-tim e approaches
[37, 18, 90, 104]. We considered that biological phenomena are natu rally continuous
and, thus, continuous representations better suit perception a nd behavior.
(ii) BM in symplectic form [Eq. (30)] represents the gradient-desce nt (GD) on
the Hamiltonian function. However, under nonstationary sensory inputs, the
multi-dimensional energy landscape is not static, but incurs time dep endence.
Accordingly, the presented BM naturally facilitates fast dynamics b eyond the quasi-
static limit implied by the usual GD methods. In addition, it does not invo ke the
concept of higher-order motions in the conventional framework [3 6]; accordingly,
our theory is not limited by the issue of average ﬂows vs the rate of change of the
average [2].
Free energy and inference in living systems 17
5. Numerical study of BM
In this section, we numerically develop the latent dynamics of the bra in’s sensorimotor
system resulting from the Hamilton principle-based FE minimization for mulation.
For simplicity, we consider a homogeneous, but time-dependent, se nsory input, such
as nonstationary light intensity or temperature, at the receptor s, which emits a
motor output innervating the eﬀectors that alter the sensory ob servation. There
are approximately 150,000 cortical columns in the mammalian neocort ex, and each
cortical column exhibiting a six-laminae structure may be considered as an independent
sensorimotor system [77, 45]. Our simple model features the double closed-loop
microcircuitry delineated in Fig 1 within a single column, which constitute s the basic
computational unit of canonical cortical circuits in an actual large -scale brain network
[7].
The generative map, g, and functions, f and π, are unknown; they may be nonlinear
or even undescribable within ordinary mathematics. Here, we exploit the linear models
assuming the generic structure:
gpµ, a; θgq “ θp0q
g ` θp1q
g µ ` θp2q
g a, (35)
fpµ; θf q “ θp0q
f ` θp1q
f µ, (36)
πpa; θπ q “ θp0q
π ` θp2q
π a, (37)
where θpiq
α (α “ f, g, π ) are the parameters that are to be learned and encoded as long-
term plasticity in the neural circuits. We have included the term θp2q
g a in Eq. (35), which
facilitates the additive motor-inference mechanism of the sensory data; additionally, θp1q
g
and θp2q
g magnify or demagnify sensory prediction and motor emission by the in ternal
state, µ, and motor state, a, respectively; θp0q
g denotes the background error in the
measurements. The constant terms θp0q
f and θp0q
π in Eqs. (36) and (37) specify the prior
beliefs on the state and motor expectations, respectively; the co eﬃcients θp1q
f and θp2q
π
modulate the relaxation times to the targets. In addition to these s even parameters
θpiq
α , there appear three neural masses, mα , in the BM unpacked in Eqs. (31)–(34).
Hence, the proposed parsimonious BM still encloses 10 parameters , which deﬁne a
multidimensional parameter space to explore for learning. The learn ing problem was not
pursued in this study but should be explored in future investigations . Here, we focus on
the active inference problem, assuming that the optimal paramete rs were already learned
or amortized over the developmental and evolutionary time scales; these param eters are
assumed to be shared for generating present and future sensor y data.
By substituting the generative functions given in Eqs. (35)–(37) in to Eqs. (31)–(33),
the BM of the state vector, Ψ, can be concisely expressed as
9Ψ ` RΨ “ I, (38)
Free energy and inference in living systems 18
Figure 1. Spontaneous attractor: For illustrational purposes, we depict t he attractor
in the 3D state space spanned by pRerµs, Reras, Rerpµ sq with instantaneous other
variables; the attractor center, Ψ c, is positioned at p´ 10, 10, ´ 20q. The full attractor
evolves in the hyper space spanned by the eight components of com plex vector, Ψ; in
our model, there are the four types of neuronal units pµ, a, pµ , paq in a single cortical-
column, each of which is allowed to be a complex variable. [Data are in arb itrary
units.]
where the relaxation matrix, R, is
R “
¨
˚
˚
˚
˝
´θp1q
f 0 ´m´ 1
ω 0
0 ´θp2q
π 0 ´m´ 1
η
´mzθp1q
g θp1q
g ´mzθp1q
g θp2q
g θp1q
f 0
´mzθp1q
g θp2q
g ´mzθp2q
g θp2q
g 0 θp2q
π
˛
‹
‹
‹
‚ (39)
and the source term, I, on the right-hand side (RHS) is
Iptq “
¨
˚
˚
˚
˝
θp0q
f
θp0q
π
´mzθp1q
g sptq ` mzθp0q
g θp1q
g
´mzθp2q
g sptq ` mzθp0q
g θp2q
g
˛
‹
‹
‹
‚. (40)
Note that the time-dependence in the source term I occurs through the sensory inputs,
s. The general solution for Eq. (38) can be formally expressed by dir ect integration as
Ψ ptq “ e´ RtΨ p0q `
ż t
0
dt1 e´ Rt1
Ipt ´ t1q. (41)
The ﬁrst term on the RHS of Eq. (41) describes the homogeneous s olution for an initial
condition of Ψ p0q, and the second term is the inhomogeneous solution driven by the
source, Iptq, manifesting the history-dependent feature. The solution repre sents the
brain’s cognitive trajectory in action while continuously perceiving th e sensory inputs,
sptq.
In the long-time limit, t Ñ 8 , we mathematically predict that the trajectory in the
state manifold will fall onto either a ﬁxed point, spiral node or repelle r, satisfying 9Ψ st “ 0
or a limit cycle about a center satisfying 9Ψ st “ ´ iωΨ st, where ω is an angular frequency
Free energy and inference in living systems 19
characterizing stationarity¶. The details of the solution’s approach to a steady-state will
be determined from the eigenvalue spectrum of the matrix R and time-varying feature
of sptq. We denote the eigenvalues and eigenvectors by λp” iωq and φ, respectively, and
set up the eigenvalue problem:
Rφα “ λα φα .
The trace and determinant are invariant under a similarity transfor mation; accordingly,
the ensuing eigenvalues must satisfy:ÿ
α
λα “ trpRq “ 0, (42)
ź
α
λα “ detpRq
“ θp1q
f θp1q
f θp2q
π θp2q
π ` mz
mw
θp1q
g θp1q
g θp2q
π θp2q
π ` mz
mη
θp1q
f θp1q
f θp2q
g θp2q
g . (43)
The eigenvalues form the Lyapunov exponents in the ﬁnite-dimensio nal manifold and
characterize the dynamical behavior of the state vector near an attractor. Because of
the multi-dimensionality of the parameter space, it is not ideal to ext ract the eigenvalue
properties analytically from the trace and determinant conditions. Accordingly,
informative constraints on the parameters must be determined on the heuristic basis.
In this study, we numerically searched for parameters that led to p ure-imaginary
eigenvalues, thereby entailing stationary attractors.
Numerical result I: Spontaneous dynamics
We ﬁrst consider the spontaneous dynamics of the brain evolved from the particular
solution in Eq. (41) with null sensory inputs in our proposed BM. The f ormal
representation for the spontaneous trajectory, Ψ spptq, can be obtained by direct
integration as
Ψ spptq “ Ψ c ´ R´ 1e´ RtIsp, (44)
where the constant vector, Ψ c, is speciﬁed as Ψ c “ R´ 1Isp, where Isp is the
inhomogeneous term solely from the internal driving sources withou t the sensory inputs,
that is, s “ 0 [see Eq. (40)].
In Fig. 1, we depict the trajectories generated assuming a set of p arameters in the
neural generative models [Eqs. (35)–(37)] as `
pθp0q
g , θp0q
f , θp0q
π q “ p 0, 10, 10q,
pθp1q
g , θp1q
f q “ p 2eiπ {2, ´1q,
pθp2q
g , θp2q
π q “ p eiπ {2, eiπ {2q.
¶ In this study, we distinguish between the concept of a stationary s tate and a steady state: a steady
state is the general term indicating the limiting state as t Ñ 8 , whereas a stationary state is the speciﬁc
steady state where an oscillatory time dependence remains.
` These parameter values were selected in an ad hoc manner through numerical inspection to produce a
dynamic attractor; therefore, the latent dynamics of the cognit ive vector, Ψ, is evolved in the extended,
complex-valued phase space in the present manifestation.
Free energy and inference in living systems 20
(a)
-100 -80 -60 -40 -20 20 
-50
50
100
p
■ ■ ■ ■ ■ ■ ■ ■ ■ ■
(b)
20 40 60 80 100 s
50
100
150
200
250
300
350
Cognitive Intensity
Figure 2. Latent dynamics under static sensory inputs: (a) Attractor dev eloped from
a resting state, Ψ p0q, and driven by the static input s “ 100, using the same parameter
values as in Fig. 1; the initial state was chosen from the spontaneou s states in Fig. 1, and
for illustrational purposes, the attractor is depicted in the two-d imensional state space
spanned by pRerΨ 2s, RerΨ 4sq. (b) Cognitive intensity, |Ψ c|2, vs sensory input, s. The
ﬁlled squares are the results from the neural inertial masses pmz, mw, mη q “ p 10, 1, 10q
and open circles are the results from pmz, mw, mη q “ p 1, 1, 1q; the numerical values
for the other generative parameters are the same as those used in Fig. 1. [Data are in
arbitrary units.]
In addition, the neural inertial masses were assumed to have value s of
pmz, mw, mη q “ p 1, 1, 1q.
The major numerical observations are as follows. The brain’s spont aneous trajectory
occupies a limited region in the state space around a center, Ψ c, which describes a
dynamic attractor forming the brain’s resting states before sensory inﬂux occurs. T he
center is speciﬁed by the internal parameters, that is, the gener ative parameters and
neural masses. We numerically checked that the position of Ψ c varies with the values of
neural masses and the brain’s prior belief on the hidden causes of th e sensory input and
motor state. We also conﬁrmed that the size of attractors is aﬀec ted by the generative
parameters and neural masses.
Numerical result II: Passive recognition dynamics
To demonstrate passive perception, we exposed the resting brain to a static sensory
signal; that is, we inserted s “ constant in Eq. (40). In this case, the formal solution
Eq. (41) can be reduced to
Ψ ptq “ e´ RtΨ p0q ` Ψ c ´ R´ 1e´ RtI, (45)
where, on the RHS, the ﬁrst term speciﬁes the homogeneous tran sience of the initial
resting state, Ψ p0q, second term, Ψ c, denotes the center of attractors, and last term
describes the dynamic development from the inhomogeneous sourc e, Ipsq. In contrast
to the spontaneous attractors, the location of center depends on the sensory input, s:
Ψ c “ Ψ cpsq “ R´ 1Ipsq.
We performed numerical integration and obtained the stationary a ttractor in the
presence of static sensory inputs. Thus, we conﬁrmed that the a ttractor behaved
Free energy and inference in living systems 21
(a)
100 200 300 400 500 t0
20
40
60
80
100
120
s(t)
(b)
100 200 300 400 500 t
-20
20
40
60
p(t)
Figure 3. Active dynamics under time-dependent sensory inputs: (a) Salient feature
of streaming perturbation at the receptor state, sptq; we assume a sigmoid shape for
the temporal dependence with the saturated value s8 “ 100, stiﬀness k “ 0.2, and
mid-time tm “ 500. (b) Motor inference of the sensory signals; the BM was integr ated
using the same parameter values as in Fig. 2 for the generative para meters and neural
masses. [All curves are in arbitrary units.]
similarly as in the spontaneous case, but with a shift of the center be cause of the nonzero
sensory stimulus. The outcome is presented in Fig. 2. Figure 2(a) sh ows a typical
attractor in the two-dimensional state space, which is evolved fro m a spontaneous state
shown in Fig. 1. In addition, in Fig. 2(b), we show the change in the cognitive intensity,
|Ψ cpsq|2, with respect to sensory inputs, s, which is deﬁned as
|Ψ cpsq|2 ” Ψ cΨ ˚
c .
Given a sensory stimulus, we numerically observe that the cognitive in tensity is weaker
for a larger inertial mass. The neural inertial masses represent t he inferential precision
in the internal models; accordingly, the result shows that less cogn itive intensity is
required when the internal model is more precise in perceptual infe rence. The cognitive
intensity may be used as a quantitative measure of awareness or at tention in phycology.
Our intensity measure is closely related to neuroimaging analysis [65], w here the neural
response to sensory inputs was analyzed as the energy-level cha nge associated with
information encoding.
Numerical result III: Active recognition dynamics
We considered the nonstationary sensory input, sptq, that renders the time-dependent
driving I [Eq. (40)] in the latent dynamics: the sensory receptors are cont inuously
elicited, and the brain engages in online computation to integrate the BM. For numerical
purposes, we assumed the salient feature of sensory signal, sptq, as a sigmoid temporal
dependence:
sptq “ s8
1 ` e´ kpt´ tmq , (46)
where tm indicates the time when the sensory intensity reaches the midpoint a nd k
adjusts the stiﬀness of transience in approaching the limiting value, sptq Ñ s8. The
sigmoidal sensory inputs are depicted as a function of time in Fig. 3(a ).
Free energy and inference in living systems 22
We numerically integrated Eqs. (31)–(34) assuming the same initial s tate selected
for the data shown in Fig. 2, subject to the sensory stream prese nted in Fig. 2(a). In
Fig. 3(b), we illustrate the imaginary part of the motor state, aptq, in continuous time,
which is the online outcome of active inference of the sensory input. For illustrational
purposes, we adopted the sigmoid shape for the temporal depend ence with a saturated
value of s8 “ 100, stiﬀness of k “ 0.2, and mid-time of tm “ 500. The results suggest
that the motor state aligns with the sensory variation and success fully infers the sharp
change in the sensory input around t “ 250.
In addition, Fig. 4 presents the attractor dynamics at several tim e steps exhibiting
state transition, dynamic bifurcation, from a resting state, Ψ p0q, to a cognitive attractor,
Ψ ptq, over time [51]. The numerical computation reveals the initial develo pment of the
NEQ attractor with passage of time shown in Fig. 4(a) and Fig. 4(b), which corresponds
to the inferential outcome of the lower part of the sigmoid inﬂux dep icted in Fig. 3(a).
The intermediate attractor in Fig. 4(b) repeats the spontaneous attractor presented in
Fig. 1 because the sensory input is nearly null apart from the negligib le ﬂuctuation in the
present model. As time elapses from Fig. 4(b) to Fig. 4(c), the cogn itive state begins to
escape from the ﬁrst attractor and build the second attractor. Eventually, with passage
of time shown in Fig. 4(c) and Fig. 4(d), the dynamic transition betwe en two attractors
completes over a relaxation time period, say, τ. At time t ą τ, the stationary attractor
can be described by the expansion
Ψ ptq “ ¯Ψ c `
ÿ
α
cα e´ iω α tφα , (47)
where iωα ” λα and φα are the eigenvalues and corresponding eigenvectors of the
relaxation matrix, R, respectively. The expansion coeﬃcients, cα , are speciﬁed by the
initial condition, Ψ p0q. The center of mass of the attractor, ¯Ψ c, is speciﬁed by R´ 1I8,
where I8 is the source vector I with the saturated sensory input, s8. The shift of the
center between two stationary attractors is shown in Fig. 4(d).
The concrete example presented above fully accommodates the ac tive inference of a
living agent inferring the sensory signal’s salient feature and perfor ming feedback motor-
inference in the double closed-loop cognitive architecture [See Appe ndix]. Although the
illustration accounts for a single sensorimotor system, our formula tion can also handle
multiple modalities of sensory inputs posing multisensory perception p roblems. Notably,
the time-dependent sensory inﬂux, sptq, makes the linear BM nonconservative, which,
from a dynamical-systems perspective, serves as a bifurcation pa rameter. Our numerical
illustration of the dynamic transition from a resting state to a cognit ive attractor is
relevant to recent studies of cognitive control of behavior in psyc hiatry [21, 79] and
stability of conscious states against external perturbations in pa tients with brain injury
[25, 95].
Free energy and inference in living systems 23
Figure 4. Attractor dynamics inferring the nonstationary sensory inﬂux de picted in
Fig. 3(a): (a) t “ 5, (b) t “ 100, (c) t “ 260, and (d) t “ 500. The trajectory,
Ψ ptq, results from the direct numerical integration of the BM described by Eqs. (31)–
(34); the initial state, Ψ p0q “ p´ 16.9, 21.1, ´ 13.3q, was selected from the spontaneous
attractor given in Fig. 1. For numerical purposes, the attractor evolution is depicted
in the three-dimensional state space spanned by pRerµs, Reras, Rerpµ sq. The numerical
values adopted for all parameters are the same as those in Fig. 3. [D ata are in arbitrary
units.]
6. Summary and conclusion
This study is based on the consensus that living systems are self-or ganized into an
NEQ stationary state that violates the detailed balance while sustain ing physiological
and bodily properties. In a biological context, the thermodynamic s econd law, the
FTs in its modern forms, implies that there is inevitably uncompensate d energy in an
organism’s metabolic processes of maintaining its homeostasis in the e nvironment. More
precisely, the amount of metabolic work is bounded from above by th e thermodynamic
FE expense. Eﬃciency is important in any irreversible phenomena exh ibiting the arrow
of time, and by extension, in brain work. We applied modern FTs to a bio logical agent as
an open system and clariﬁed why the concept of the FE is more appro priate than entropy
when discussing the question of What is life? . The thermodynamic and neuroscientiﬁc
FEPs were evaluated based on their respective mathematical inequ alities, suggesting
the FE bounds as variational objective functions for minimization. C onsequently, we
revealed the drawbacks of both principles in accounting for cognitiv e biological systems
Free energy and inference in living systems 24
and proposed an integrated thermodynamic and Bayesian approac h to the biological
FEP as a self-organizing principle of life.
The brain states of higher organisms can only be realistically describe d in terms
of probability because of the enormous neuronal degrees of free dom and morphological
complexity. And at the core of the biological FEP are the likelihood and prior densities,
making up the G-density, which are thought to be the NEQ probabilitie s of the
physical brain states. This study argues that the brain dynamics a t the mesoscopic,
constitutional level are stochastic because of classical negligenc e, for which time-
asymmetric Langevin equations were employed. The broken time-re versal symmetry was
attributed to biological systems being open to the environment. To statistically describe
the brain states, we further used the Markovian approximation in s tate transitions
and adopted the Smoluchowski-Fokker-Planck equation to determ ine the probability
densities of the continuous brain variables. We viewed the S-F-P equ ation as a local
balance equation for probability and argued that its steady-state solutions furnish
the NEQ densities. The probability ﬂux appearing in the S-F-P equatio n does not
vanish at the brain-environment interface, which reﬂects that a d etailed balance will
not be reached in the SS limit, and thus, no standard ﬂuctuation-dis sipation theorem
is available in the NEQ brain. Instead, the SS ﬂux resembles the Amper e law in
magnetism, resulting from the modiﬁed detailed-balance condition an d supporting the
gradient ﬂow of the NEQ probabilities.
We presented the brain as Schr¨ odinger’s mechanical machine pres iding over
predictive regulation of physiology and adaptive behavior of the bod y. The BM at
the system level is deterministic, indicating that the brain, as a macr oscopic physical
system, obeys the law of large numbers entailing dimensionality reduc tion. In addition,
thermal ﬂuctuations from body temperature do not have signiﬁca nt eﬀects on the brain’s
low-dimensional functions; in other words, the brain is cognitively in it s ground state
at eﬀective zero temperature. The IFE was speciﬁed in terms of th e latent variables
that probabilistically encode the environmental and motor states in the brain. As
aforementioned, the encoded probability densities were assumed t o be SS solutions to
the S-F-P equation or more realistic ones. Central to our study wa s the idea that the
encoded, online IFE in the brain is a Lagrangian, deﬁning the informat ional Action.
Based on Hamilton’s principle, we found that the brain deterministically conducts
allostatic regulation by completing the double closed-loop dynamics of perception and
adaptive motor behavior. We employed a simple model for nonstation ary sensory inﬂux
and illustrated the development of optimal trajectories in the neur al phase space: we
numerically observed that the brain undergoes a dynamic transition from a resting
state to the stationary attractor, which corresponds to the on line inference of the
environmental causes in continuous time. The proposed BM may app ly to any generic
cognitive processes at the interoceptive, exteroceptive, and pr oprioceptive levels.
In conclusion, organisms’ adaptive sustentation cannot be descr ibed within
thermodynamic laws and the ensuing TFEP, for which the brain-inspir ed IFEP provides
a promising avenue. The IFEP, however, utilizes teleological informa tion-theoretic
Free energy and inference in living systems 25
models and then considers the neural bases of those models. To es tablish an integrated
framework of the organizing principle of life, two rationales of FE minim ization and
Bayesian inference were hybridized, and the BM directing the brain’s latent dynamics of
active inference was derived. Consequently, the brain’s perceptio n and motor inference
in higher organisms were revealed to operate eﬀectively as Schr¨ od inger’s mechanical
machine. In addition, we numerically illustrated the attractor dynam ics that develops
online during a sensory stream in the low-dimensional neural space.
Acknowledgements
The author is grateful to J. Kang for providing assistance with the mathematica
programming.
References
[1] R A Adams, S Shipp, and K J Friston. Predictions not commands: ac tive inference in the motor
system.
Brain Struct Funct, 218:611–643, 2013.
[2] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christop her L Buckley. How particular
is the physics of the free energy principle? Physics of Life Reviews, 2021.
[3] E Albarran-Zavala and F Angulo-Brown. A simple thermodynamic an alysis of photosynthesis.
Entropy, 9(4):152–168, 2007.
[4] S Amari. Dynamics of pattern formation in lateral-inhibition type ne ural ﬁelds. Biol Cybern,
27:77–87, 1977.
[5] Bhashyam Balaji and Karl Friston. Bayesian state estimation us ing generalized coordinates.
In Ivan Kadar, editor, Signal Processing, Sensor Fusion, and Target Recognition XX, volume
8050, page 80501Y. International Society for Optics and Photon ics, SPIE, 2011.
[6] Vijay Balasubramanian. Heterogeneity and eﬃciency in the brain. Proceedings of the IEEE,
103(8):1346–1358, 2015.
[7] A M Bastos, W M Usrey, R A Adams, G R Mangun, P Fries, and K J Frist on. Canonical
microcircuits for predictive coding. Neuron, 76(4):695–711, 2012.
[8] Martin Biehl, Felix A Pollock, and Ryota Kanai. A technical critique of some parts of the free
energy principle. Entropy, 23(3):293, 2021.
[9] Jelle Bruineberg, Krzysztof Dolega, Joe Dewhurst, and Manuel Baltieri. The emperor’s new
markov blankets. Behavioral and Brain Sciences, pages 1–63, 2021.
[10] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free energy
principle for action and perception: A mathematical review. Journal of Mathematical
Psychology, 81:55–79, 2017.
[11] Ozan Catal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy
selection using active inference. In Workshop on Structure & Priors in Reinforcement Learning
at ICLR 2019 : proceedings, page 9, 2019.
[12] Ozan Catal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and A dam Safron. Robot
navigation as hierarchical active inference. Neural Networks, 142:192–204, 2021.
[13] M Colombo and C Wright. First principles in the life sciences: the fre e-energy principle,
organicism, and mechanism. Synthese, 198:3463–3488, 2021.
[14] Blake J Cook, Andre D H Peterson, Wessel Woldman, and John R T erry.
Neural Field Models: A mathematical overview and unifying framewor k.
Mathematical Neuroscience and Applications, Volume 2, March 2022 .
[15] Andrew W Corcoran and Jakob Hohwy. Allostasis, interoception , and the free energy principle:
Feeling our way forward, 2017.
Free energy and inference in living systems 26
[16] Ignasi Cos, Giovanni Pezzulo, and Paul Cisek. Changes of mind after movement onset depend
on the state of the motor system. eNeuro, 8(6), 2021.
[17] J A Costa and A O Hero. Geodesic entropic graphs for dimension a nd entropy estimation in
manifold learning. IEEE Transactions on Signal Processing, 52(8):2210–2221, 2004.
[18] L Da Costa, T Parr, N Sajid, S Veselic, V Neacsu, and Karl Fristo n. Active inference on discrete
state-spaces: A synthesis. Journal of Mathematical Psychology, 99:102447, 2020.
[19] T M Cover and J A Thomas. Elements of Information Theory. Wiley-Interscience, New York,
1991.
[20] Gavin E Crooks. Entropy production ﬂuctuation theorem and t he nonequilibrium work relation
for free energy diﬀerences. Phys Rev E, 60:2721–2726, 1999.
[21] Z Cui, J Stiso, G L Baum, and et al. Optimization of energy state tr ansition trajectory supports
the development of executive function during youth. Elife, 27(9):e53060, 2020.
[22] J Cunningham and B Yu. Dimensionality reduction for large-scale n eural recordings. Nat
Neurosci, 17:1500–1509, 2014.
[23] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How active
inference could help revolutionise robotics. Entropy, 24(3):361, 2022.
[24] G Deco, V K Jirsa, P A Robinson, M Breakspear, and K Friston. Th e dynamic brain: From
spiking neurons to neural masses and cortical ﬁelds. PLoS Comput Biol, 4(8):e1000092, 2008.
[25] Gustavo Deco, Josephine Cruzat, Joana Cabral, Enzo Tagliazu cchi, Helmut Laufs, Nikos
Logothetis, and Morten Kringelbach. Awakening: Predicting exter nal stimulation to force
transitions between diﬀerent brain states. Proceedings of the National Academy of Sciences,
116:201905534, 08 2019.
[26] Kenji Doya. Canonical cortical circuits and the duality of baye sian inference and optimal control.
Current Opinion in Behavioral Sciences, 41:160–167, 2021.
[27] Xiaona Fang, Karsten Kruse, Ting Lu, and JinWang. Nonequilibriu m physics in biology. Rev
Mod Phys, 91:045004, 2019.
[28] Keith Douglas Farnsworth. How organisms gained causal indepe ndence and how it might be
quantiﬁed. Biology, 7(3):38, 2018.
[29] C Fiorillo. A neurocentric approach to bayesian inference. Nat Rev Neurosci, 11:605, 2010.
[30] Nahuel Freitas, Gianmaria Falasco, and Massimiliano Esposito. Lin ear response in large
deviations theory: a method to compute non-equilibrium distribution s. New Journal of Physics,
23(9):093003, 2021.
[31] K Friston. Life as we know it. Journal of The Royal Society Interface, 10(86), 2013.
[32] K Friston, J Mattout, and J Kilner. Action understanding and ac tive inference. Biol Cybern,
104:137–160, 2011.
[33] Karl Friston. The free-energy principle: a rough guide to the b rain? Trends Cogn Sci, 13:293–
301, 2009.
[34] Karl Friston. The free-energy principle: a uniﬁed brain theory ? Nat Rev Neurosci, 11:127–138,
2010.
[35] Karl Friston. A free energy principle for a particular physics, 2 019.
[36] Karl Friston and Ping Ao. Free energy, value, and attractors . Computational and Mathematical
Methods in Medicine, 2012:937860, 2012.
[37] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwa rtenbeck, and Giovanni
Pezzulo. Active inference: A process theory. Neural Computation, 29(1):1–49, 2017.
[38] C C A Fung, K Y M Wong, and S Wu. A moving bump in a continuous manif old: a comprehensive
study of the tracking dynamics of continuous attractor neural n etworks. Neural Comput,
22(3):752–792, 2010.
[39] J A Gallego, M G Perich, R H Chowdhury, and et al. Long-term stab ility of cortical population
dynamics underlying consistent behavior. Nat Neurosci, 23:260–270, 2020.
[40] Juan A Gallego, Matthew G Perich, Lee E Miller, and Sara A Solla. Neu ral manifolds for the
control of movement. Neuron, 94(5):978–984, 2017.
Free energy and inference in living systems 27
[41] F S Gnesotto, F Mura, J Gladrow, and C P Broedersz. Broken de tailed balance and non-
equilibrium dynamics in living systems: a review. Reports on Progress in Physics, 81(6):066601,
apr 2018.
[42] Nigel Goldenfeld and Carl Woese. Life is physics: Evolution as a co llective phenomenon far from
equilibrium. Annual Review of Condensed Matter Physics, 2(1):375–399, 2011.
[43] Joshua E Goldford and Daniel Segre. Modern views of ancient me tabolic networks. Current
Opinion in Systems Biology, 8:117–124, 2018.
[44] David J Griﬃths. Introduction to Electrodynamics. Cambridge University Press, 4 edition, 2017.
[45] Jeﬀ Hawkins, Subutai Ahmad, and Yuwei Cui. A theory of how co lumns in the neocortex enable
learning the structure of the world. Frontiers in Neural Circuits, 11:81, 2017.
[46] J J Hopﬁeld. Neural networks and physical systems with emerg ent collective computational
abilities. Proc Natl Acad Sci U S A, 79(8):2554–2558, 1982.
[47] Jordan M Horowitz and Todd R Gingrich. Thermodynamic uncerta inty relations constrain non-
equilibrium ﬂuctuations. Nature Physics, 16:15–20, 2019.
[48] Yanping Huang and Rajesh P N Rao. Predictive coding. WIREs Cognitive Science, 2(5):580–593,
2011.
[49] Takuya Isomura, Hideaki Shimazaki, and Karl J Friston. Canon ical neural networks perform
active inference. Communications biology, 5(1):55, January 2022.
[50] Genrikh R Ivanitskii. 21st century: what is life from the perspec tive of physics? Physics-Uspekhi,
53(4):327–356, 2010.
[51] E Izhikevich. Dynamical systems in neuroscience. MIT Press, page 159, July 2007.
[52] C Jarzynski. Nonequilibrium equality for free energy diﬀerences . Phys Rev Lett, 78:2690–2693,
1997.
[53] Christopher Jarzynski. Equalities and inequalities: Irreversibilit y and the second law of
thermodynamics at the nanoscale. Annual Review of Condensed Matter Physics, 2(1):329–
351, 2011.
[54] Benjamin I Jelen, Donato Giovannelli, and Paul G Falkowski. The ro le of microbial electron
transfer in the coevolution of the biosphere and geosphere. Annual Review of Microbiology,
70(1):45–62, 2016.
[55] K Jezek, E Henriksen, A Treves, and et al. Theta-paced ﬂicker ing between place-cell maps in
the hippocampus. Nature, 478:246–249, 2011.
[56] Viktor Jirsa and Hiba Sheheitli. Entropy, free energy, symmetr y and dynamics in the brain.
Journal of Physics: Complexity, 3(1):015007, 2022.
[57] Stuart Kauﬀman. Answering schr¨ odinger’s “what is life?”. Entropy, 22(8):815, 2020.
[58] C S Kim. Statistical work-energy theorems in deterministic dyna mics. Journal of the Korean
Physical Society, 67:273–289, 2015.
[59] C S Kim. Recognition dynamics in the brain under the free energy p rinciple. Neural
Computation, 30(10):2616–2659, 2018.
[60] C S Kim. Bayesian mechanics of perceptual inference and motor control in the brain. Biol
Cybern, 115:87–102, 2021.
[61] C S Kim and G P Morriss. Local entropy in quasi-one-dimensional h eat transport. Phys. Rev.
E, 80:061137, 2009.
[62] Michael Kirchhoﬀ, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. The markov
blankets of life: autonomy, active inference and the free energy p rinciple. J R Soc Interface,
15(138):20170792, 2018.
[63] T Korbak. Computational enactivism under the free energy pr inciple. Synthese, 198:2743–2763,
2021.
[64] R Kubo, M Toda, and N Hashitsume. Statistical Physics II. Springer, Berlin, 1992.
[65] Strelnikov Kuzma. Energy-information coupling during integrat ive cognitive processes. Journal
of Theoretical Biology, 469:180–186, 2019.
[66] L D Landau and E M Lifshitz. Mechanics: Volume 1 (Course of Theoretical Physics Series) 3rd
Free energy and inference in living systems 28
Edition. Elsevier Ltd, Amsterdam, 1976.
[67] L D Landau and E M Lifshitz. Statistical Physics Part 1: Volume 5 (Course of Theoretical
Physics Series) 3rd Edition. Pergamon Press, Oxford, 1980.
[68] Niles E Lehman and Stuart A Kauﬀman. Constraint closure drove major transitions in the origins
of life. Entropy, 23(1):105, 2021.
[69] William B Levy and Victoria G Calvert. Communication consumes 35 tim es more energy than
computation in the human cortex, but both costs are needed to pr edict synapse number.
Proceedings of the National Academy of Sciences, 118(18):e2008173118, 2021.
[70] Christopher W Lynn, Eli J Cornblath, Lia Papadopoulos, Maxwell A Bertolero, and Danielle S
Bassett. Broken detailed balance and entropy production in the hu man brain. Proceedings of
the National Academy of Sciences, 118(47):e2109889118, 2021.
[71] Takazumi Matsumoto and Jun Tani. Goal-directed planning for h abituated agents by active
inference using a variational recurrent neural network. Entropy, 22(5):564, 2020.
[72] H R Maturana and F J Varela. Autopoiesis and cognition: The realization of the living. D
Reidel Publishing Company, Boston, 1980.
[73] Pietro Mazzaglia, Tim Verbelen, Ozan Catal, and Bart Dhoedt. Th e free energy principle for
perception and action: A deep learning perspective. Entropy, 24(2):301, 2022.
[74] Cristian Meo and Pablo Lanillos. Multimodal vae active inference co ntroller. In 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pages 2693–2699, 2021.
[75] R Monasson and S Rosay. Crosstalk and transitions between mu ltiple spatial maps in an attractor
neural network model of the hippocampus: Phase diagram. Phys Rev E, 87:062813, 2013.
[76] R Monasson and S Rosay. Transitions between spatial attract ors in place-cell models. Phys Rev
Lett, 115(5):098101, 2015.
[77] V B Mountcastle. The columnar organization of the neocortex. Brain, 120(4):701–722, 1997.
[78] P Nurse. What is life? Five great idesa in biology. W W Norton & Company, New York, 2020.
[79] Linden Parkes, Tyler M Moore, Monica E Calkins, Matthew Cieslak, David R Roalf, Daniel H
Wolf, Ruben C Gur, Raquel E Gur, Theodore D Satterthwaite, and D anielle S Bassett. Network
controllability in transmodal cortex predicts positive psychosis spe ctrum symptoms. Biological
Psychiatry, 90(6):409–418, 2021.
[80] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov bla nkets, information geometry and
stochastic thermodynamics. Phil Trans R Soc A, 378:20190159, 2020.
[81] Thomas Parr and Karl J Friston. The Discrete and Continuous B rain: From Decisions to
Movement?And Back Again. Neural Computation, 30(9):2319–2347, 2018.
[82] A Peter, C Uran, J Klon-Lipok, R Roese, S van Stijn, W Barnes, J R Dowdall, W Singer, P Fries,
and M Vinck. Surface color and predictability determine contextual modulation of v1 ﬁring
and gamma oscillations. eLife, 8:e42101, 2019.
[83] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active inf erence, homeostatic regulation
and adaptive behavioural control. Progress in Neurobiology, 134:17–35, 2015.
[84] Hong Qian. A decomposition of irreversible diﬀusion processes wit hout detailed balance. Journal
of Mathematical Physics, 54(5):053302, 2013.
[85] K S Quigley, S Kanoski, W M Grill, L F Barrett, and M Tsakiris. Functio ns of interoception:
From energy regulation to experience of the self. Trends Neurosci, 44(1):29–38, 2021.
[86] Vicente Raja, Dinesh Valluri, Edward Baggs, Anthony Chemero, and Michael L Anderson. The
markov blanket trick: On the scope of the free energy principle and active inference. Physics
of Life Reviews, 39:49–72, 2021.
[87] M J D Ramstead, P B Badcock, and K J Friston. Answering schr¨ o dinger’s question: A free-energy
formulation. Physics of Life Reviews, 24:1–16, 2018.
[88] H Risken. The Fokker-Planck Equation. Springer-Verlag, Berlin, 1989.
[89] P A Robinson, C J Rennie, and J J Wright. Propagation and stability of waves of electrical
activity in the cerebral cortex. Phys Rev E, 56:826–840, 1997.
[90] Noor Sajid, Philip J Ball, Thomas Parr, and Karl J Friston. Active in ference: Demystiﬁed and
Free energy and inference in living systems 29
compared. Neural Computation, 33(3):674–712, 2021.
[91] Dalton A R Sakthivadivel. Towards a geometry and analysis for ba yesian mechanics, 2022.
[92] Cansu Sancaktar, Marcel A J van Gerven, and Pablo Lanillos. En d-to-end pixel-based deep active
inference for body perception and action. In 2020 Joint IEEE 10th International Conference
on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), pages 1–8, 2020.
[93] Javier S´ anchez-Ca˜ nizares. The free energy principle: Good science and questionable philosophy
in a grand unifying theory. Entropy, 23(2):238, 2021.
[94] Yonatan Sanz Perl, Hern´ an Bocaccio, Carla Pallavicini, Ignacio P ´ erez-Ipi˜ na, Steven Laureys,
Helmut Laufs, Morten Kringelbach, Gustavo Deco, and Enzo Tagliaz ucchi. Nonequilibrium
brain dynamics as a signature of consciousness. Phys. Rev. E, 104:014411, Jul 2021.
[95] Yonatan Sanz Perl, Carla Pallavicini, Ignacio P´ erez Ipi˜ na, and et al. Perturbations in dynamical
models of whole-brain activity dissociate between the level and stab ility of consciousness.
PLoS Computational Biology, 17(7):e1009139, July 2021.
[96] Erwin Schr¨ odinger and Roger Penrose. What is Life?: With Mind and Matter and
Autobiographical Sketches. Cambridge University Press, Cambridge, 1992.
[97] Jay Schulkin and Peter Sterling. Allostasis: A brain-centered, p redictive mode of physiological
regulation. Trends in Neurosciences, 42(10):740–752, 2019.
[98] Udo Seifert. Stochastic thermodynamics: principles and persp ectives. Eur Phys J B, 64:423–431,
2008.
[99] Udo Seifert. From stochastic thermodynamics to thermodyna mic inference. Annual Review of
Condensed Matter Physics, 10(1):171–192, 2019.
[100] Biswa Sengupta, Martin B Stemmler, and Karl J Friston. Infor mation and eﬃciency in the
nervous system–synthesis. PLOS Computational Biology, 9:1–12, 2013.
[101] Biswa Sengupta, Arturo Tozzi, Gerald K Cooray, Pamela K Doug las, and Karl J Friston. Towards
a neuronal gauge theory. PLoS Biology, 14, 2016.
[102] Hideaki Shimazaki. The principles of adaptation in organisms and machines ii: Thermodynamics
of the bayesian brain, 2020.
[103] Wolf Singer. Recurrent dynamics in the cerebral cortex: Int egration of sensory evidence with
stored knowledge. Proceedings of the National Academy of Sciences, 118(33):e2101043118,
2021.
[104] Ryan Smith, Karl J Friston, and Christopher J Whyte. A step- by-step tutorial on active inference
and its application to empirical data. Journal of Mathematical Psychology, 107:102632, 2022.
[105] Peter Sterling. Allostasis: A model of predictive regulation. Physiology & Behavior, 106(1):5–15,
2012.
[106] R Sutton and A Barto, editors. Reinforcement learning. MIT Press, Cambridge, MA, 1998.
[107] Nabil Swedan. Photosynthesis as a thermodynamic cycle. Heat Mass Transfer, 56:1649–1658,
2020.
[108] E Todorov. Optimal control theory. In Bayesian Brain: Probabilistic Approaches to Neural
Coding, pages 269–298. The MIT Press, Cambridge, 2006.
[109] Mark K Transtrum, Benjamin B Machta, Kevin S Brown, Bryan C Daniels, Christopher R Myers,
and James P Sethna. Perspective: Sloppiness and emergent theor ies in physics, biology, and
beyond. The Journal of Chemical Physics, 143(1):010901, 2015.
[110] Cem Uran, Alina Peter, Andreea Lazar, William Barnes, Johanna Klon-Lipok, Katharine A
Shapcott, Rasmus Roese, Pascal Fries, Wolf Singer, and Martin Vin ck. Predictive coding of
natural images by v1 activity revealed by self-supervised deep neu ral networks, 2021.
[111] H von Helmholtz and J P C Southall. Helmholtz’steatise on physiological optics, Vol. 3. Courier
Corporation, 2005.
[112] Jin Wang, Li Xu, and Erkang Wang. Potential landscape and ﬂu x framework of nonequilibrium
networks: Robustness, dissipation, and coherence of biochemica l oscillations. Proceedings of
the National Academy of Sciences, 105(34):12271–12276, 2008.
[113] T J Wills, C Lever, F Cacucci, N Burgess, and J O’Keefe. Attract or dynamics in the hippocampal
Free energy and inference in living systems 30
representation of the local environment. Science, 308:873–876, 2005.
[114] R Zwanzig. Nonequilibrium Statistical Mechanics. Oxford Univ Press, Berlin, 2001.
Appendix: Dual structure of perception and motor inference
Here, we describe a signiﬁcant feature of our derived BM capturing the dual nature of
the sensory and motor inference in the neocortex [26], and brieﬂy d iscuss its relevance
to other control theories.
Figure 5 shows the double-loop architecture of the neural circuitr y emerging from
the attained BM. The environmental cause, ϑ, encodes the sensory data, s, at the
peripheral interface (receptors or input layers), and the brain c onducts the variational
Bayesian inference that conjointly integrates the double closed-lo op dynamics of sensory
perception (A) and motor control (B). Note that the neural unit s pµ, pµ , a, paq are
connected by arrows for excitatory driving and by lines guided by ﬁlle d dots for
inhibitory driving. Loop (A): The state unit, µ, in neuronal population predicts the
input, s, based on the internal model, g1pµq. The error signal, ξzpµq “ s ´ g1pµq,
weighted by the accuracy, mz, of the model, innervates the state-error unit, pµ , in the
population. The error unit estimates the state by assimilating the dis crepancy and sends
the feedback signal to the state unit. Then, the state unit updat es its expectation and
predicts the sensory input again, which completes the passive perc eptual loop. Loop
(B): The motor (eﬀector) unit, a, alters the sensory input, s, according to the protocol,
g2paq, to promote accurate sensation of the data. The error signal, mzps ´ g2paqq, acts
as a control command to call for an adjustment in the motor-erro r unit, pa. Then, the
adjusted motor-dynamics transmits the feedback signal to the e ﬀector state to further
modify the sensory data, which completes the active perceptive loo p. The double closed-
loop dynamics concurrently continue until an optimal trajectory, Ψ ptq, is fulﬁlled in the
neural hyper-phase space, which corresponds to optimizing the in formational classical
Action, S, deﬁned in Eq. (26).
Our Hamiltonian formulation renders the sensory-driving term, s ´ gpµ, aq, to
appear explicitly in the BM [see Eqs. (33) and (34)]. Its role is similar to t he
unsupervised updating rule in the reinforcement-learning framewo rk [106]; speciﬁcally,
it resembles the continuous control signal in the optimal control t heory described
by the Hamilton-Jacobi-Bellman equation [108]. The sensory-discrep ancy signal not
only aﬀects the prediction error, pµ , in the state prediction [Eq. (33)], but also the
prediction error, pa, of the motor inference [Eq. (34)]; this interrelation provides the
neural mechanism for adaptive motor feedback via Eq. (32). The m omenta in our
formulation are termed a costates in the deterministic optimal control theory.
Also, the policy, π, in Eq. (32) accounts for the online motor behavior, which
prescribes motor planning and can accommodate a situated decision [16]. In the discrete-
state formulations, the policy is deﬁned as a sequence of actions or decisions in discrete
time [90, 49], where the authors incorporate the necessary time-d ependence directly in
the deﬁnition of FE. On the contrary, our continuous-time theory deﬁnes the policy
Free energy and inference in living systems 31
ϑ
μ
p
pμ

g2()
g1(μ)
mz ξz()
mz ξz(μ)
(A)
(B)

Figure 5. Schematic of the neural circuitry exhibiting the double closed-loop
architecture, which emerges from the Bayesian mechanics prescr ibed by Eqs. (31)–
(34).
as continuous action planning, which we model as the generative fun ction of motor
inference. The time-dependence of policy generates the history- dependent response of
the brain’s cognitive state; see Eq. (41), in which the time, t, can be either at present or
in the future. When the dynamic perception is coupled to categorica l-decision making,
the mixed continuous-discrete approaches may shape the active in ference problems [81].
Finally, we have employed a set of simple and speciﬁc generative models [Eqs. (35)–
(37)] for a concrete numerical illustration. In practice, however , the employed models can
be readily generalized. For instance, one may consider an action-de pendent generative
function, fpµ, a; θf q, which will make the state dynamics [Eq. (19)] subjected to actions .
Further investigations using more realistic models are required to lea rn the implication
and utility of our theory for the dual closed-loop dynamics related t o the standard
control theories.