Group level MEG/EEG source imaging via
optimal transport: minimum Wasserstein
estimates
H. Janati1, T. Bazeille1, B. Thirion1, M. Cuturi2, A. Gramfort1
1 INRIA, CEA Neurospin, France
2 Google and CREST ENSAE
Abstract. Magnetoencephalography (MEG) and electroencephalogra-
phy (EEG) are non-invasive modalities that measure the weak electro-
magnetic fields generated by neural activity. Inferring the location of
the current sources that generated these magnetic fields is an ill-posed
inverse problem known as source imaging. When considering a group
study, a baseline approach consists in carrying out the estimation of
these sources independently for each subject. The ill-posedness of each
problemistypicallyaddressedusingsparsitypromotingregularizations.A
straightforwardwaytodefineacommonpatternforthesesourcesisthen
toaveragethem.Amoreadvancedalternativereliesonajointlocalization
of sources for all subjects taken together, by enforcing some similarity
across all estimated sources. An important advantage of this approach
is that it consists in a single estimation in which all measurements are
pooled together, making the inverse problem better posed. Such a joint
estimation poses however a few challenges, notably the selection of a
valid regularizer that can quantify such spatial similarities. We propose
in this work a new procedure that can do so while taking into account
the geometrical structure of the cortex. We call this procedure Minimum
Wasserstein Estimates (MWE). The benefits of this model are twofold.
First, joint inference allows to pool together the data of different brain
geometries, accumulating more spatial information. Second, MWE are
defined through Optimal Transport (OT) metrics which provide a tool
to model spatial proximity between cortical sources of different subjects,
hence not enforcing identical source location in the group. These benefits
allow MWE to be more accurate than standard MEG source localization
techniques. To support these claims, we perform source localization on
realistic MEG simulations based on forward operators derived from MRI
scans. On a visual task dataset, we demonstrate how MWE infer neural
patternssimilartofunctionalMagneticResonanceImaging(fMRI)maps.
Keywords: Brain · Inverse modeling · EEG / MEG source imaging
1 Introduction
Magnetoencephalography (MEG) measures the components of the magnetic field
surroundingthehead,whileElectroencephalography(EEG)measurestheelectric
9102
beF
31
]LM.tats[
1v21840.2091:viXra
2 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
potential at the surface of the scalp. Both can do so with a temporal resolution
of less than a millisecond. Localizing the underlying neural activity on a high
resolution grid of the cortex, a problem known as source imaging, is inherently
an “ill-posed” linear inverse problem: Indeed, the number of potential sources is
larger than the number of MEG and EEG sensors, which implies that, even in
the absence of noise, different neural activity patterns could result in the same
electromagnetic field measurements.
To limit the set of possible solutions, prior hypotheses on the nature of
the source distributions are necessary. The minimum-norm estimates (MNE)
for instance are based on (cid:96) Tikhonov regularization which leads to a linear
2
solution [11]. An (cid:96) norm penalty was also proposed by [34], modeling the
1
underlyingneuralpatternasasparsecollectionoffocaldipolarsources,hencetheir
name“MinimumCurrentEstimates” (MCE).Thesemethodshaveinspiredaseries
of contributions in source localization techniques relying on noise normalization
[6,28]tocorrectforthedepthbias[1]orblock-sparsenorms[30,10]toleveragethe
spatio-temporal dynamics of MEG signals. While such techniques have had some
success, source estimation in the presence of complex multi-dipole configurations
remainsachallenge.Inthisworkweaimtoleveragetheanatomicalandfunctional
diversity of multi-subject datasets to improve localization results.
Related work. This idea of using multi-subject information to improve statistical
estimation has been proposed before in the neuroimaging literature. In [20] it is
showed that different anatomies across subjects allow for point spread functions
that agree on a main activation source but differ elsewhere. Averaging across
subjects thereby increases the accuracy of source localization. On fMRI data,
[35] proposed a probabilistic dictionary learning model to infer activation maps
jointly across a cohort of subjects. A similar idea led [19] to introduce a Bayesian
framework to account for functional intersubject variability. To our knowledge,
the only contribution formulating the problem as a multi-task regression model
employs a Group Lasso with an (cid:96) block sparse norm [21]. Yet this forces every
21
potential neural source to be either active for all subjects or for none of them.
Contribution. The assumption of identical functional activity across subjects
is clearly not realistic. Here we investigate several multi-task regression models
that relax this assumption. One of them is the multi-task Wasserstein (MTW)
model [15]. MTW is defined through an Unbalanced Optimal Transport (UOT)
metric that promotes support proximity across regression coefficients. However,
applying MTW to group level data assumes that the signal-to-noise ratio is the
same for all subjects. We propose to build upon MTW and alleviate this problem
by inferring estimates of both sources and noise variance for each subject. To do
so, we follow similar ideas that lead to the concomitant Lasso [27,31,25] or the
multi-task Lasso [24].
This paper is organized as follows. Section 2 introduces the multi-task re-
gression source imaging problem. Section 3 presents some background on UOT
metrics and explains how MWE are carried out. Section 4 presents the results of
our experiments on both simulated and MEG datasets.
MWE: Minimum Wasserstein Estimates 3
Notation. Wedenoteby1 thevectorofonesinRp andby q theset{1,...,q}
p
for any integer q ∈N. The set of vectors in Rp with non-neg(cid:74)at(cid:75)ive (resp. positive)
entries is denoted by Rp (resp. Rp ). On matrices, log, exp and the division
+ ++
operator are applied elementwise. We use (cid:12) for the elementwise multiplication
between matrices or vectors. If X is a matrix, X denotes its ith row and X
i. .j
its jth column. We define the Kullback-Leibler (KL) divergence between two
positive vectors by KL(x,y)=(cid:104)x,log(x/y)(cid:105)+(cid:104)y−x,1 (cid:105) with the continuous
p
extensions0log(0/0)=0and0log(0)=0.Wealsomaketheconventionx(cid:54)=0⇒
KL(x|0)=+∞. The entropy of x∈Rn is defined as H(x)=−(cid:104)x,log(x)−1 (cid:105).
p
The same definition applies for matrices with an element-wise double sum.
2 Source imaging as a multi-task regression problem
We formulate in this section the inverse problem of interest in this paper, and
recall how a multi-task formulation can be useful to carry out a joint estimation
of all these parameters through regularization.
Source modeling. Using a volume segmentation of the MRI scan of each subject,
thepositionsofpotentialsourcesareconstructedasasetofcoordinatesuniformly
distributed on the cortical surface of the gray matter. Moreover, synchronized
currents in the apical dendrites of cortical pyramidal neurons are thought to be
mostly responsible for MEG signals [26]. Therefore, the dipole orientations are
usually constrained to be normal to the cortical surface. We model the current
density as a set of focal current dipoles with fixed positions and orientations.
The purpose of source localization is to infer their amplitudes. The ensemble of
possible candidate dipoles forms the source space.
Forward modeling. Let n denote the number of sensors (EEG and/or MEG) and
p the number of sources. Following Maxwell’s equations, at each time instant,
the measurements B ∈ Rn are a linear combination of the current density
x∈Rp :B=Lx. However, we observe noisy measurements Y ∈Rn given by:
Y =B+ε=Lx+ε , (1)
where ε is the noise vector. The linear forward operator L∈Rn×p is called the
leadfield or gain matrix, and can be computed by solving Maxwell’s equations
using the Boundary element method [12]. Up to a whitening pre-processing step,
ε can be assumed Gaussian distributed N(0,σI ).
n
Sourcelocalization. Sourcelocalizationconsistsinsolvinginxtheinverseproblem
(1) which can be cast as a least squares problem:
x(cid:63) =argmin(cid:107)Y−Lx(cid:107)2 . (2)
2
x∈Rp
Since n(cid:28)p, problem (2) is ill-posed and additional constraints on the solution
x(cid:63) are necessary. When analyzing evoked responses, one can promote source
4 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
configurations made of a few focal sources, e.g. using the (cid:96) norm. This regu-
1
larization leads to problem (3) called minimum current estimates (MCE), also
known in the machine learning community as the Lasso [32].
1
x(cid:63) =argmin (cid:107)Y−Lx(cid:107)2+λ(cid:107)x(cid:107) , (3)
x∈Rp 2n 2 1
where λ>0 is a tuning hyperparameter.
Common source space. Here we propose to go beyond the classical pipeline
and carry out source localization jointly for S subjects. First, dipole positions
(features) must correspond to each other across subjects. To do so, the source
spaceofeachsubjectismappedtoahighresolutionaveragebrainusingmorphing
where the sulci and gyri patterns are matched in an auxiliary spherical inflating
of each brain surface [8]. The resulting leadfields L(1),...,L(S) have therefore
the same shape (n×p) with aligned columns.
Multi-task framework. Jointly estimating the current density x(s) of each subject
s can be expressed as a multi-task regression problem where some coupling prior
is assumed on x(1),...,x(S) through a penalty Ω:
S
1 (cid:88)
min (cid:107)Y(s)−L(s)x(s)(cid:107)2+Ω(x(1),...,x(S)) . (4)
x(1),...,x(S)∈Rp 2n 2
s=1
Following the work of [15], we propose to define Ω using an UOT metric.
3 Minimum Wassertein Estimates
We start this section with background material on UOT. Consider the finite
metric space (E,d) where each element of E = {1,...,p} corresponds to a
vertex of the source space. Let M be the matrix where M corresponds to the
ij
geodesic distance between vertices i and j. Kantorovich [16] defined a distance
for normalized histograms (probability measures) on E. However, it can easily
be extended to non-normalized measures by relaxing marginal constraints [4].
Marginal relaxation. Let a,b be two normalized histograms on E. Assuming
that transporting a fraction of mass P from i to j is given by P M , the total
ij ij ij
cost of transport is given by (cid:104)P,M(cid:105)= (cid:80) P M . Minimizing this total cost
ij ij ij
with respect to P must be carried out on the set of feasible transport plans with
marginals a and b. The (normalized) Wasserstein-Kantorovich distance reads:
WK(a,b)= min (cid:104)P,M(cid:105) . (5)
P∈R
+
p×p
P1=a,P(cid:62)1=b
In practice, if a and b are positive and normalized current densities, WK(a,b)
will quantify the geodesic distance between their supports along the curved
MWE: Minimum Wasserstein Estimates 5
geometry of the cortex. This property makes OT metrics adequate for assessing
the proximity of functional patterns across subjects. To allow a,b to be non-
normalized, the marginal constraints in (5) can be relaxed using a KL divergence:
min (cid:104)P,M(cid:105)+γKL(P1|a)+γKL(P(cid:62)1|b) , (6)
P∈R
+
p×p
where γ >0 is a hyperparameter that enforces a fit to the marginals.
Entropy regularization. Entropyregularizationwasintroducedby[5]toproposea
faster and more robust alternative to the direct resolution of the linear program-
mingproblem(5).Formally,thisamountstominimizingtheloss(cid:104)P,M(cid:105)−εH(P)
where ε > 0 is a tuning hyperparameter. This penalized loss function can be
written: εKL(P,e−M ε ) up to a constant [3]. Combining entropy regularization
with marginal relaxation in (6), we get the unbalanced Wasserstein distance as
introduced by [4]:
W(a,b)= min εKL(P|e−M ε )+γKL(P1|a)+γKL(P(cid:62)1|b) , (7)
P∈R
+
p×p
Generalized Sinkhorn. Problem (7) can be solved as follows. Let K=e−M ε and
ψ =γ/(γ+(cid:15)). Starting from two vectors u,v set to 1 and iterating the scaling
operations u←(a/Kv)ψ , v← (cid:0) b/K(cid:62)u (cid:1)ψ until convergence, the minimizer of
(7)canbecomputedasP(cid:63) =(u K v ) .Thisalgorithmisageneralization
i ij j i,j∈ p
of the Sinkhorn algorithm [18]. Since it in(cid:74)v(cid:75)olves matrix-matrix operations, it
benefits from parallel hardware, such as GPUs.
Extension to Rp. We extend next the Wasserstein distance to signed measures.
Weadoptasimilarideatowhatwassuggestedin[23,29,15]usingadecomposition
intopositiveandnegativeparts,x(s) =x(s)+−x(s)− wherex(s)+ =max(x(s)+,0)
and x(s)− =max(−x(s)+,0). For any vectors a,b∈Rp, we define the generalized
Wasserstein distance as:
W(cid:102)(a,b) d = ef W(a+,b+)+W(a−,b−) . (8)
Note that W(0,0)=0 (see [15] for a proof), thus on positive measures W(cid:102) =W.
For the sake of convenience, we refer to W(cid:102) in (8) by the Wasserstein distance,
even though it does not verify indiscernability. In practice, this extension allows
tocomparecurrentdipolesacrosssubjectsaccordingtotheirpolaritywhichcould
be either towards the deep or superficial layers of the cortex.
The MTW model. The multi-task Wasserstein model is the specific case of (4)
with a penalty Ω promoting both sparsity and supports’ proximity:
S
Ω
MTW
(x(1),...,x(S)) d = ef µ
x¯
m
∈
i
R
n
p S
1 (cid:88) W(cid:102)(x(s),x¯)+λ(cid:107)x(s)(cid:107)
1
, (9)
s=1
where µ,λ ≥ 0 are tuning hyperparameters. The OT term in (9) can be seen
as a spatial variance. Indeed, the minimizer x¯ corresponds to the Wasserstein
barycenter with respect to the distance W(cid:102) .
6 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
Algorithm 1 MWE algorithm
Input: σ , µ,(cid:15),γ,λ and cost matrix M. data (Y(s)) (L(s)) .
0 s s
Output: MWE: (x(s)), minimizers of (10).
repeat
for s=1 to S do
Update x(s)+ with proximal coordinate descent to solve (12).
Update x(s)− with proximal coordinate descent to solve (12).
Update σ(s) with (11).
end for
Update left marginals m(1)+,...,m(S)+ and x¯+ with generalized Sinkhorn.
Update left marginals m(1)−...,m(S)− and x¯− with generalized Sinkhorn.
until convergence
Minimum Wasserstein Estimates. One of the drawbacks of MTW is that λ is
common to all subjects. Indeed, the loss considered in MTW implicitly assumes
that the level of noise is the same across subjects. Following the work of [25] on
the smoothed concomitant Lasso, we propose to extend MTW by inferring the
specific noise standard deviation σ(s) along with the regression coefficient x(s) of
each subject. This allows to scale the weight of the (cid:96) according to the level of
1
noise. The Minimum Wasserstein Estimates (MWE) model reads:
(cid:88) S 1 σ(s)
min (cid:107)Y(s)−L(s)x(s)(cid:107)2+ + Ω (x(1),...,x(S)) ,
x(1),...,x(S)∈Rp 2nσ(s) 2 2 MTW
s=1
σ(1),...,σ(S)∈[σ0,+∞]
(10)
where σ is a pre-defined constant. This lower bound constraint avoids numerical
0
issues when λ→0 and therefore the standard deviation estimate also tends to 0.
In practice σ can be set for example using prior knowledge on the variance of
0
the data or as a small fraction of the initial estimate of the standard deviation
σ 0 =αmin s (cid:107)Y √ ( n s)(cid:107). In practice we adopt the second option and set α=0.01.
Algorithm. By combining (7), (8) and (10), we obtain an objective function
takingasarguments(cid:0) (x(s)+) ,(x(s)−) ,(P(s)+) ,(P(s)−) ,x¯+,x¯−,(σ(s)) (cid:1).This
s s s s s
functionrestrictedtoallparametersexcept(σ(s)) isjointlyconvex[15].Moreover,
s
each σ(s) is only coupled with the variable x(s). The restriction on every pair
(x(s),σ(s)) is also jointly convex [25]. Thus the problem is jointly convex in all its
variables. We minimize it by alternating optimization. To justify the convergence
of such an algorithm, one needs to notice that the non-smooth (cid:96) norms in the
1
objective are separable [33]. The update with respect to each σ(s) is given by
solving the first order optimality condition (Fermat’s rule):
(cid:107)Y(s)−L(s)x(s)(cid:107)
σ(s) ← √ 2 ∧σ , (11)
n 0
MWE: Minimum Wasserstein Estimates 7
whichalsocorrespondstotheempiricalestimatorofthestandarddeviationwhen
theconstraintisnotactive.Toupdatetheremainingvariables,wefollowthesame
optimization procedure laid out in [15] and adapted to MWE in Algorithm 1.
Briefly, let m(s)+ d = ef P(s)+1 (resp. m(s)+ d = ef P(s)+1), when minimizing with
respecttoonex(s)+ (resp.x(s)−),theresultingproblemcanbewritten(dropping
the exponents for simplicity):
1 µγ
min (cid:107)Y−Lx(cid:107)2+ ((cid:104)x,1(cid:105)−(cid:104)log(x),m(cid:105))+λσ(cid:107)x(cid:107) , (12)
x∈Rp 2n 2 S 1
+
whichcanbesolvedusingproximalcoordinatedescent[7].Notethattheadditional
inference of a specific σ(s) for each subject allows to scale the Lasso penalty
depending on their particular level of noise. The final update with respect to
((P(s)+) ,(P(s)−) ,x¯+,x¯−) can be cast as two Wasserstein barycenter problems,
s s
carried out using generalized Sinkhorn iterations [4]. Note that we do not need
to compute the transport plans P(s) since inferring every source estimate x only
requires the knowledge of the left marginal m = P1 which does not require
storing P in memory.
4 Experiments
Benchmarks: Dirty models and Multi-level Lasso. As discussed in introduction,
standardsparsesourcelocalizationsolversarebasedonan(cid:96) normregularization,
1
applied to the data of each subject independently. We use the independent
Lasso estimator as a baseline. We compare MWE to the Group-Lasso estimator
[37,2] which was proposed in this context to promote functional consistency
across subjects [21]. It falls in the multi-task framework of (4) where the joint
(cid:113)
penaltyisdefinedthroughan(cid:96) mixednorm(cid:107)X(cid:107) = (cid:80)p (cid:80)S x(s)2 where
21 21 j=1 s=1 j
X = (x(s)) ∈ Rp×S. We also evaluate the performance of more flexible
j (j,s)
block sparse models where only a fraction of the source estimates are shared
across all tasks: Dirty models [14] and the multivel lasso [22]. In Dirty models
source estimates are written as a sum of two parts which are penalized with
different norms. One common to all subjects (penalty (cid:96) ) and one specific for
21
each subject (penalty (cid:96) ). The Multi-level Lasso (MLL) [22] applies the same
1
idea using instead a product decomposition and a Lasso penalty on both parts.
We also compare MWE with MTW to evaluate the benefits of inferring noise
levels adaptively.
Simulation data and MEG/fMRI datasets. We use the public dataset DS117 [36]
which provides MEG, EEG and fMRI data of 16 healthy subjects to whom were
presented images of famous, unfamiliar and scrambled faces. Using the MRI scan
ofeachsubject,wecomputeasourcespaceanditsassociatedleadfieldcomprising
around2500sourcesperhemisphere[9].KeepingonlyMEGgradiometerchannels,
we have n=204 observations per subject.
8 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
Fig.1. Left: 3 labels from
the aparc.a2009s parcellation.
Right: Simulated activations
for S =6 subjects. Each color
corresponds to a subject. Dif-
ferentradiusesareusedtodis-
tinguish overlapping sources.
0.8
0.6
0.4
dleifdaeL
emaS
AUC EMD in cm 1e 3 MSE
6
15
4
10
2 5
0.8
0.6
0.4
2 4 6 8 10 16
# subjects
sdleifdaeL
tnereffiD
1e 3 6
15
4
10
2 5
2 4 6 8 10 16 2 4 6 8 10 16
# subjects # subjects
MTW MLL Dirty GL Lasso MWE
Fig.2. Performance of different models over 30 trials in terms of AUC, EMD
and MSE using the same leadfield for all subjects (randomly selected in each
trial) (top) and specific leadfields (bottom).
For realistic data simulation, we use the actual leadfields from all subjects,
yet restricted to the left hemisphere. We thus have 16 leadfields with p=2500.
We simulate an inverse solution xs with q sources (q-sparse vector) by randomly
selecting one source per label among q pre-defined labels using the aparc.a2009s
parcellation of the Destrieux atlas. To model functional consistency, 50% of the
subjects share sources at the same locations, the remaining 50% have sources
randomlygeneratedinthesamelabels(seeFigure1).Theiramplitudesaretaken
uniformlybetween20and30nAm.TheirsignistakenatrandomwithaBernoulli
distribution (0.5) for each label. We simulate Y using the forward model with a
variancematrixσI .Wesetσ soastohaveanaveragesignal-to-noiseratioacross
n
subjects equal to 4 (SNRd = ef (cid:80)S (cid:107)L(s)x(s)(cid:107)). We evaluate the performance of all
s=1 Sσ
models knowing the ground truth by comparing the best estimates in terms of
three metrics: the mean squared error (MSE) to quantify accuracy in amplitude
estimation, AUC and a generalized Earth mover distance (EMD) to assess
supportsestimation.WegeneralizethePR-AUC(AreaunderthecurvePrecision-
recall) by defining AUC(xˆ,x(cid:63)) = 1PR-AUC(xˆ+,x(cid:63)+) + 1PR-AUC(xˆ−,x(cid:63)−)
2 2
where PR-AUC is computed between the estimated coefficients and the true
MWE: Minimum Wasserstein Estimates 9
Lasso
Fig.3.Supportofsourceestimates.Eachcolorcorrespondstoasubject.Different
radiuses are displayed for a better distinction of sources. The fusiform gyrus is
highlightedingreen.Increasingµpromotesfunctionalconsistencyacrosssubjects.
supports.WecomputeEMDbetweennormalizedvaluesofsources:EMD(xˆ,x(cid:63))=
1WK( xˆ+ , x(cid:63)+ )+ 1WK( xˆ− , x(cid:63)− ). Since M is expressed in centimeters, WK
2 xˆ+1 x(cid:63)+1 2 xˆ−1 x(cid:63)−1
can be seen as an expectation of the geodesic distance between sources. The
mean across subjects is reported for all metrics.
Simulation results. We set the number of sources to 3 and vary the number of
subjects under two conditions: (1) using one leadfield for all subjects, (2) using
individual leadfields. Each model is fitted on a grid of hyperparameters and the
best AUC/MSE/EMD scores are reported. We perform 30 different trials (with
different true activations and noise, different common leadfield for condition (1))
and report the mean within a 95% confidence interval in Figure 2.
Various observations can be made. The Group Lasso performs poorly – even
comparedtoindependentLasso–whichisexpectedsincesourcesarenotcommon
for all subjects. Non-convexity allows MLL to be very effective with less than
2-4 subjects. Its performance yet degrades with more subjects. OT-based models
(MWE and MTW) however benefit from the presence of more subjects by
leveraging spatial proximity. They reduce the average error distance from 4
cm (Lasso) to less than 1 cm and reach an AUC of 0.9. One can also observe
that the estimation of the noise standard deviation in the MTW model does
improve performance. Finally, we can appreciate the improvement of multi-task
models when increasing the number subjects, especially when using different
leadfield matrices. We argue that the different folding patterns of the cortex
acrosssubjectsleadtodifferentdipoleorientationstherebyincreasingthechances
of source identification.
ResultsonMEG/fMRIdata Thefusiformfaceareaspecializesinfacialrecognition
and activates around 170ms after stimulus [17,13]. To study this response, we
perform MEG source localization using Lasso and MWE. We pick the time point
with the peak response for each subject within the interval 150-200 ms after
visual presentation of famous faces. For both models, we select the smallest (cid:96)
1
tuning parameter λ for which less than 10 sources are active for each subject.
Figure 3 shows how UOT regularization favors activation in the ventral pathway
10 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
1.00
0.75
0.50
0.25
%5=
xam
Left hemisphere Right hemisphere
1.00
0.75
0.50
0.25
%01=
xam
1.00
0.75
0.50
0.25
%51=
xam
1.00
0.75
0.50
0.25
0 10 20 30 40 50 60 70 80 90 100
OT regularization
%02=
xam
0 10 20 30 40 50 60 70 80 90 100
OT regularization
Lasso fMRI MWE
Fig.4. Ratio of maximum absolute amplitude in the fusiform gyrus over maxi-
mum absolute amplitude in the hemisphere. The mean across the 16 subjects is
reported for different (cid:96) norm regularization weights λ.
1
time=0.170s time=0.170s
00..0000 00..441177 00..883344 11..2255 11..6677 22..0088 22..5500 22..9922 00..0000 00..117722 00..334433 00..551155 00..668877 00..885588 11..0033 11..2200 00..0000 11..3366 22..7722 44..0088 55..4433 66..7799 88..1155 99..5511
Fig.5. Neural patterns of subject 2. Absolute amplitudes of MEG source esti-
mates (in nAm) given by Lasso (Left) and MWE (Middle). Absolute values of
fMRI Z-scores. (Right). The fusiform gyrus is highlighted in green.
of the visual cortex. The Lasso solutions in Figure 3 show significant differences
between subjects. Since no ground truth exists, one could argue that MWE
promotes consistency at the expense of individual signatures. To address this
concern we compute the standardized fMRI Z-score of the conditions famous
vs scrambled faces. We compare Lasso, MWE and fMRI by computing for each
subjecttheratiolargestvalueinfusiformgyrus /largestabsolutevalue.Wereport
the mean across all subjects in Figure 4. Note that for all subjects, the fMRI
Z-scorereachesitsmaximuminthefusiformgyrus,andthatMWEregularization
leads to more agreement between MEG and fMRI. Figure 5 shows MEG with
MWE and fMRI results for subject 2.
MWE: Minimum Wasserstein Estimates 11
Conclusion
We proposed in this work a novel approach to promote functional consistency
through a convex model defined using an Unbalanced Optimal Transport reg-
ularization. Using a public MEG and fMRI dataset, we presented experiments
demonstratingthatMWEoutperformmulti-tasksparsemodelsinbothamplitude
and support estimation. We have shown in these experiments that MWE can
close the gap between MEG and fMRI source imaging by gathering data from
different subjects.
References
1. Ahlfors, S.P., Ilmoniemi, R.J., Hämäläinen, M.S.: Estimates of visually evoked
cortical currents. Electroencephalography and Clinical Neurophysiology 82(3),
225–236 (2018/11/20 1992)
2. Argyriou,A.,Evgeniou,T.,Pontil,M.:Multi-taskfeaturelearning.In:NIPS(2007)
3. Benamou, J., Carlier, G., Cuturi, M., Nenna, L., Peyré, G.: Iterative Bregman
Projections For Regularized Transportation Problems. Society for Industrial and
Applied Mathematics (2015)
4. Chizat, L., Peyré, G., Schmitzer, B., Vialard, F.X.: Scaling Algorithms for Unbal-
anced Transport Problems. arXiv:1607.05816 [math.OC] (2017)
5. Cuturi, M.: Sinkhorn Distances: Lightspeed Computation of Optimal Transport.
NIPS (2013)
6. Dale, A.M., Liu, A.K., Fischl, B.R., Buckner, R.L., Belliveau, J.W., Lewine, J.D.,
Halgren, E.: Dynamic statistical parametric mapping. Neuron 26(1), 55–67 (2000)
7. Fercoq, O., Richtárik, P.: Accelerated, parallel and proximal coordinate descent.
SIAM Journal on Optimization 25, 1997–2023 (2015)
8. Fischl, B., Sereno, M.I., Dale, A.M.: Cortical surface-based analysis: Ii: Inflation,
flattening, and a surface-based coordinate system. NeuroImage 9, 195 – 207 (1999),
mathematics in Brain Imaging
9. Gramfort, A., Luessi, M., Larson, E., Engemann, D.A., Strohmeier, D., Brodbeck,
C., Parkkonen, L., Hämäläinen, M.: MNE software for processing MEG and EEG
data. NeuroImage 86 (10 2013)
10. Gramfort, A., Strohmeier, D., Haueisen, J., Hämäläinen, M., Kowalski, M.: Time-
frequency mixed-norm estimates: Sparse M/EEG imaging with non-stationary
source activations. NeuroImage 70(0), 410 – 422 (2013)
11. Hämäläinen, M.S., Ilmoniemi, R.J.: Interpreting magnetic fields of the brain: mini-
mumnormestimates.Medical&BiologicalEngineering&Computing32(1),35–42
(Jan 1994)
12. Hämäläinen, M.S., Sarvas, J.: Feasibility of the homogeneous head model in the
interpretation of neuromagnetic fields. Physics in Medicine and Biology 32(1), 91
(1987)
13. Henson, R.N., Wakeman, D.G., Litvak, V., Friston, K.J.: A parametric empirical
bayesian framework for the EEG/MEG inverse problem: Generative models for
multi-subject and multi-modal integration. Frontiers in human neuroscience 5, 76;
76–76 (08 2011)
14. Jalali, A., Ravikumar, P., Sanghavi, S., Ruan, C.: A Dirty Model for Multi-task
Learning. NIPS (2010)
12 H. Janati, T. Bazeille, B. Thirion, M. Cuturi, A. Gramfort
15. Janati,H.,Cuturi,M.,Gramfort,A.:Wassersteinregularizationforsparsemulti-task
regression. Arxiv. preprint (2018)
16. Kantorovic, L.: On the translocation of masses. C.R. Acad. Sci. URSS (1942)
17. Kanwisher, N., McDermott, J., Chun, M.M.: The fusiform face area: A module in
human extrastriate cortex specialized for face perception. Journal of Neuroscience
17(11), 4302–4311 (1997)
18. Knopp, P., Sinkhorn, R.: Concerning nonnegative matrices and doubly stochastic
matrices. . Pacific Journal of Mathematics 1(2), 343–348 (1967)
19. Kozunov, V.V., Ossadtchi, A.: Gala: group analysis leads to accuracy, a novel
approach for solving the inverse problem in exploratory analysis of group MEG
recordings. Frontiers in Neuroscience 9, 107 (2015)
20. Larson,E.,Maddox,R.K.,Lee,A.K.C.:Improvingspatiallocalizationinmeginverse
imagingbyleveragingintersubjectanatomicaldifferences.FrontiersinNeuroscience
8, 330 (2014)
21. Lim,M.,Ales,J.,Cottereau,B.M.,Hastie,T.,Norcia,A.M.:Sparseeeg/megsource
estimation via a group lasso. PLOS (2017)
22. Lozano,A.,Swirszcz,G.:Multi-levelLassoforSparseMulti-taskRegression.ICML
(2012)
23. Mainini, E.: A description of transport cost for signed measures. Journal of Mathe-
matical Sciences 181(6), 837–855 (Mar 2012)
24. Massias, M., Fercoq, O., Gramfort, A., Salmon, J.: Generalized concomitant multi-
task lasso for sparse multimodal regression. Proceedings of Machine Learning
Research, vol. 84, pp. 998–1007. PMLR (09–11 Apr 2018)
25. Ndiaye, E., Fercoq, O., Gramfort, A., Leclère, V., Salmon, J.: Efficient smoothed
concomitant lasso estimation for high dimensional regression. Journal of Physics:
Conference Series 904(1), 012006 (2017)
26. Okada, Y.: Empirical bases for constraints in current-imaging algorithms. Brain
Topography p. 373–377
27. Owen, A.B.: A robust hybrid of lasso and ridge regression. Contemporary Mathe-
matics 443, 59–72 (2007)
28. Pascual-Marqui,R.:Standardizedlow-resolutionbrainelectromagnetictomography
(sloreta): technical details. Methods Find Exp Clin Pharmacol 24, D:5–12 (2002)
29. Profeta, A., Sturm, K.T.: Heat flow with dirichlet boundary conditions via optimal
transport and gluing of metric measure spaces (2018)
30. Strohmeier, D., Bekhti, Y., Haueisen, J., Gramfort, A.: The iterative reweighted
mixed-norm estimate for spatio-temporal MEG/EEG source reconstruction. IEEE
Transactions on Medical Imaging 35(10), 2218–2228 (Oct 2016)
31. Sun,T.,Zhang,C.H.:Scaledsparselinearregression.Biometrika99,879–898(2012)
32. Tibshirani, R.: Regression Shrinkage and Selection via the Lasso. Journal of the
Royal Statistical Society 58(1), 267–288 (1996)
33. Tseng, P.: Convergence of a block coordinate descent method for nondifferentiable
minimization. J. Optim. Theory Appl. 109(3), 475–494 (2001)
34. Uutela, K., Hämäläinen, M.S., Somersalo, E.: Visualization of magnetoencephalo-
graphic data using minimum current estimates. NeuroImage 10(2), 173–180 (1999)
35. Varoquaux, G., Gramfort, A., Pedregosa, F., Michel, V., Thirion, B.: Multi-subject
dictionarylearningtosegmentanatlasofbrainspontaneousactivity.In:Information
Processing in Medical Imaging. vol. 6801, pp. 562–573. Springer (2011)
36. Wakeman, D., Henson, R.: A multi-subject, multi-modal human neuroimaging
dataset. Scientific Data 2(150001) (2015)
37. Yuan, M., Lin, Y.: Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society 68(1), 49–67 (2006)