Highlights
Text as Environment: A Deep Reinforcement Learning Text Read-
ability Assessment Model
Hamid Mohammadi, Seyed Hossein Khasteh, Tahereh Firoozi, Taha Sama-
vati
• Self-attention’s computational load grows quadratically with input size
• Transformerscanbemathematicallymodeledasdynamicintegraltrans-
forms
• Domaindecompositionmethodcanreducetransformer’scomputational
complexity
• Reinforcementlearningcanbeusedfordomaindecompositionfortrans-
formers
3202
tcO
32
]LC.sc[
4v75950.2191:viXra
Text as Environment: A Deep Reinforcement Learning
Text Readability Assessment Model
Hamid Mohammadia, Seyed Hossein Khastehb, Tahereh Firoozic, Taha
Samavatid
aComputer Engineering Department, Amirkabir University of
Technology, Tehran, Tehran, Iran
bComputer Engineering Department, K.N. Toosi University of
Technology, Tehran, Tehran, Iran
cMeasurement, Evaluation, and Data Science (MEDS), University of
Alberta, Edmonton, Alberta, Canada
dSchool of Computer Engineering, University of Science and
Technology, Tehran, Tehran, Iran
Abstract
Evaluating the readability of a text can significantly facilitate the precise
expression of information in written form. The formulation of text read-
ability assessment involves the identification of meaningful properties of the
text regardless of its length. Sophisticated features and models are used to
evaluate the comprehensibility of texts accurately. Despite this, the prob-
lem of assessing texts’ readability efficiently remains relatively untouched.
The efficiency of state-of-the-art text readability assessment models can be
further improved using deep reinforcement learning models. Using a hard
attention-based active inference technique, the proposed approach makes
efficient use of input text and computational resources. Through the use of
semi-supervised signals, the reinforcement learning model uses the minimum
amount of text in order to determine text’s readability. A comparison of the
model on Weebit and Cambridge Exams with state-of-the-art models, such
as the BERT text readability model, shows that it is capable of achieving
state-of-the-art accuracy with a significantly smaller amount of input text
than other models.
Keywords: Text Readability, Deep Reinforcement Learning, Transformer,
Active Inference
Preprint submitted to Pattern Recognition Letters October 24, 2023
1. Introduction
Text, as a prevalent form of communication, has a fundamental role in
conveying knowledge and information between humans. Nevertheless, not
all texts are equally intelligible and understandable for all people. It is,
therefore, vital to measure the readability of written information in order to
ensureitsclarityandunderstandability. Thesignificanceofthismeasurement
is apparent from its applications in different fields such as education [1, 2],
medical instructions [3, 4, 5, 6, 7], social media communications [8, 9, 10],
marketing and advertising [11, 12], and in some related fields of research like
text simplification [13, 14, 15, 7].
As early as the 1940s, attempts were made to quantify the readability
of text manually by reading experts. In order for such an evaluation to be
standardized or globally accurate, researchers like Flesch [16] have developed
formulas for measuring the readability of texts. The readability formulas use
simple and manually calculable properties of the text, such as the number
of syllables, words, or sentences in the text, to assess its readability. Even
today, these formulas have remained very popular and are still used by many
people around the world.
Using heuristics and hand-made mathematical relations, the readability
formulas are designed to assess the level of readability of texts in a partic-
ular language. As a result, they are usually low in accuracy and language-
dependent. To compensate for the deficiencies of readability formulas, ad-
vanced and accurate readability assessment methods use machine learning
techniques. Due to their use of NLP features and machine intelligence, these
models can identify a proper level of readability based on extracted features.
As such, these models represent a significant improvement over traditional
readability formulas, providing much more accurate assessments of text com-
plexity. Models proposed by Vajjala and Meurers [17], Xia et al. [18], and
Mohammadi and Khasteh [19] are examples of state-of-the-art models for
their target languages and target audience. In these models, Support Vector
Machines (SVM) are trained on complex and extensive feature sets extracted
from related datasets. However, due to language-specific NLP features, these
models can be challenging to implement and are highly language-dependent.
As a result, these models require significant pre-processing and language-
specific NLP feature engineering to ensure accurate and successful imple-
mentation. This has led to the use of transformers Vaswani et al. [20] for
natural language understanding and, therefore, text readability assessment.
2
BERT is one of the most commonly used transformers in NLP applications,
enabling state-of-the-art accuracy with minimal training data. Automating
feature extraction with transformers has facilitated SOTA accuracy in vari-
ous NLP applications and enabled multilingual models to be built efficiently.
It is not necessary to consider the entire length of the text when assessing
its readability. For example, reading a fraction of a text can provide a gen-
eral view of the text’s readability for human assessors. It is surprising that
previous approaches have not utilized this possibility to reduce computation
load by efficiently processing the smallest amount of input. An improved
variant of the BERT model is presented here that is based on reinforcement
learning. This model can predict the readability of text using less than half
the text’s length. A reinforcement learning model is combined with a pre-
trained BERT in this model to form active inference. Input text can be
perceived within a window of several adjacent words by the model. To view
further parts of the text, the model’s actions could move the hard attention
window. As a result of the model’s ability to intelligently choose which por-
tion of the text is to be perceived, it is possible to determine the minimal
amount of content to be read to determine the readability of the text.
The structure of the paper is as follows: Section 2 discusses the previ-
ous attempts to automate the readability assessment task in detail. Section
3 presents the proposed model and describes its architecture. Later, sec-
tion 4 reviews the experiment results, and section 5 explains the advantages
and disadvantages of the presented model. The last section states the main
contributions of this study and potential future works.
2. Related Works
The text readability literature can be divided into four main categories:
traditional formulas, machine learning models with hand-crafted features,
deep learning models, and transformer models. The increasing complexity
of models in each category improved the text readability models’ accuracy,
data efficiency, feature richness, and multi-lingual capabilities.
Flesch-Kincaid grade level [16] can be named as one of the English lan-
guage’s earliest and most utilized readability formulas. The Flesch-Kincaid
readability formula uses only the average number of words per sentence and
the average number of syllables per word to evaluate text readability. The
Flesch-Kincaid formula can be seen in Equation 1.
3
Flesch-Kincaid GradeLevel =
|words| |syllables|
0.39· + 11.8· − 15.59 (1)
|sentences| |words|
With advances in automated computations, readability assessment appli-
cations utilize extracted and computer-calculated features. Lexile [21] and
the work of Collins-Thompson and Callan [22] used word-frequency and lan-
guage models, respectively. It has been found that statistical models can
be utilized to improve the accuracy of text readability assessment models.
Formulas using traditional methods are simple to implement and require lim-
ited computational resources. In spite of these advantages, these methods
are low in accuracy and have a significant difference between the results and
human judgments [23, 24, 25, 26]. Due to the fact that these formulas are
specially designed for a particular language, they cannot be applied to assess
the readability of texts in other languages. Short text applications are also
incompatible with these formulas, which are prevalent on social media and
the web these days [27].
Using machine learning models, researchers have created a more accurate
and comprehensive system for assessing text readability that overcomes the
shortcomings of traditional formulas. Assessment of text readability can be
viewedeitherasaregressionproblemorasaclassificationproblem. However,
studies have shown increased accuracy and applicability of text readability
assessment as a classification task [28]. Among the primary advantages of
machine learning models are their use of many features (naive or sophisti-
cated) and their automated ability to learn how the features interact with
readability levels, which makes them more popular than traditional formulas
[29]. Models are only as good as the features they include, which makes
choosing features crucial. Simple features, such as the average number of
characters or syllables in words, the average number of words in sentences,
the number of sentences in a text, and simple statistical language models
were features used in early machine learning models for text readability as-
sessment like works presented in [25, 30]. The use of syntactical features
[31], and cohesive features [32] also supported the realization of models with
higher accuracy. However, by restricting machine learning models to prede-
fined features, it is impossible to extract and use subtle features that were
not designed by NLP experts. To address this limitation, recent works have
4
proposed techniques for automatically extracting features from unstructured
texts, enabling more accurate models and providing a more efficient way to
extract knowledge from text.
”Attention is all you need” is the slogan of the next generation of nat-
ural language understanding models [20]. The BERT model, a prominent
model within the transformer family, incorporates self-attention and unsu-
pervised transfer learning to improve many NLP tasks [33]. Transformer
models are able to understand long-term temporal dependencies by using
soft attention as encoder layers. In addition, the use of arbitrary tasks to
pre-traintransformersincreasestheirzero-shotandfew-shotlearningabilities
[33]. As compared to previous approaches to text analysis, transformers offer
an unattainable advantage over previous approaches as transformers could
benefit from large corpora of unlabeled text for pre-training. Based on the
results of this paper, pre-trained BERTs can be fine-tuned and adapted to
text readability problems. Transformers, however, have a limited input size
as one of their constraints. By utilizing truncation, pre-trained transformers
are applied to texts of different sizes. In addition, truncating a long text to
fit the transformer’s input size limits the perceived information since part of
the text must be removed. Furthermore, in practical GPU implementations
of transformers, applying a model to texts of uneven size wastes computa-
tional resources since excess parameters still need to be stored for efficient
computation. Zero-padding and feature concatenation are naive approaches
to adding flexible input sizes to transformers. Further, it is difficult to de-
termine how much of the text needs to be processed to extract relevant and
sufficient data. This paper proposes a method of optimally processing the
minimum length of text to capture the necessary information required for
text readability assessment.
Reinforcement learning can be considered a semi-supervised approach to
machinelearning. Theabilitytolearnfrompartiallylabeleddatamakesrein-
forcement learning particularly useful for NLP. Hence, there is a trend in us-
ing reinforcement learning models in NLP tasks such as machine translation
[34, 35, 36, 37], sentence simplification [38], text summarization [39, 40, 41],
dialogue generation [42, 43], question answering [44, 45, 46], and text gen-
eration [47, 48]. Moreover, deep reinforcement learning models can help to
fuse the advantages of reinforcement learning and deep learning so as to pro-
duce more accurate and efficient models for NLP tasks. Specifically, deep
reinforcement learning could be used to develop hard-attention abilities. By
incorporating hard-attention abilities, deep reinforcement learning models
5
would be able to effectively identify and exploit the most relevant informa-
tion from a given dataset, resulting in improved performance on NLP tasks.
The main drawback of soft attention is the fast-growing computational load
that makes them inefficient when processing large sequences of text. In
contrast, hard-attention models reduce the size and computations of their
models by decreasing the amount of information they process during each
step [49]. Through active inference, these models are able to intelligently
focus on specific parts of a text that carry more valuable information for
their tasks. Despite some drawbacks of deep reinforcement learning models,
such as training instability, these models can achieve higher efficiency in NLP
tasks in comparison to soft-attention deep learning models.
We utilize reinforcement learning to optimize transformer-based text pro-
cessing for texts of varying lengths. This approach combines the power of
transformers, BERT in particular, with the active inference ability of re-
inforcement learning models. The following sections discuss the proposed
model in detail and compare it with standard approaches using technical
evaluations of the readability datasets.
3. Proposed Approach
The self-attention mechanism computations grow exponentially with in-
creasing input size [50]. To show the relationship between input size and
computational load, transformers can be mathematically modeled as dy-
namic integral transforms [50]. Equation 2 shows the formulation of the
self-attention coefficient.
⊺ √ √
A i· = (cid:80)p e(Q e K (Q / K ⊺ d / ) √ i· d)ij := Softmax(q i K⊺/ d) (2)
j=1
Where A is the attention coefficient computed for the token at the ith
i·
position considering a total of p tokens, the Q and K are query and key
values, and the q is the query computed for the jth token. Finally, d is the
i
embedding dimensions. The time complexity of Equation 2 is O(n4) [50].
The growth rate of the self-attention mechanism makes it costly to apply
to lengthy sequences. An empirical evaluation of the growth of the BERT
model’s latency relative to the input sequence length is presented in Table 1.
The Domain Decomposition Method (DDM) is a method for simplifying
and solving integral equations. Essentially, DDM is a method for dividing
6
Table 1: BERT model latency with different input sequence lengths and batch sizes on
an Intel Xeon Platinum 8275 CPU (48 cores/96 threads) and using Tensorflow framework
[51].
Batch size Seq. Length (token) Latency (ms)
512 5,747
16
256 1,663
128 701
64 473
512 11,800
32
256 3,765
128 1,518
64 747
512 41,311
128
256 13,562
128 6,513
64 3,065
a vector space into sub-spaces so that solvers can calculate the solution to
each sub-problem more quickly. DDM is similar to the divide and conquer
concept in computational algorithms. To reduce the computational require-
ment for accurate text classification, this study applies a DDM-like solution
of dividing the problem domain into manageable sub-problems. According
to [50], the self-attention layer can be roughly described using an integral
transform formulation. The integral transform form for a self-attention layer
in Equation 3 produces a Fredholm-like formulation for self-attention based
on a kernel approximation.
z(x) ≈ (cid:80)d
l=1
(eq 1 iK ⊺ / √ d)
l
(cid:82) ω κ˜(x,x′)v(x′)dµ(x′) (3)
Where κ˜(x,x′) is the kernel approximation of the self-attention coeffi-
cient computed for two input tokens, v(x′) is the linear projection of the
token, and µ(x′) is the Borel function which is required for the conversion
of the self-attention formulation into a continuous space. In this study, a
Reinforcement learning-based Active Inference Transformer (RAIT) model
7
is proposed that uses DDM to reduce the computational cost of text classi-
fication using transformers. RAIT differs from the standard DDM method
in two main ways. Firstly, in DDM, all sub-problems are solved and utilized
in forming the global solution. However, this study uses machine learn-
ing optimization to avoid the computation of all sub-problems by selecting
the minimal set of sub-domains required to solve the global problem. Sec-
ondly, in a standard DDM, the global solution is computed by combining
the sub-domain solutions via an auxiliary coarse problem. Yet, in this study,
the global combination of sub-solutions is achieved using a learned policy
in RL agent training. By converting a problem into an expected reward
formulation, a problem can be framed as a reinforcement learning problem.
Equation 4 illustrates the general framework for expected rewards.
G =R +γR +γ2R +···= (cid:80)∞ γkR (4)
t t+1 t+2 t+3 k=0 t+k+1
In Equation 4, G is the cumulative discounted reward, R is the reward
t t
observed at time-step t, and γ is the discount value. You can assign re-
wards for changing your attention window and selecting a class. Using G
t
general modeling of the desired task output (here: maximum accuracy with
minimal computation), the Bellman equation can present an abstract global
aggregation function for the custom DDM. The modified DDM is formulated
as a state-value estimation problem in Equation 5 and state-action value
estimation problems in Equation 6.
υ (s) = E
(cid:2)(cid:80)∞
γkR |S = z(x )
(cid:3)
(5)
π π k=0 t+k+1 t t+k+1
q (s,a)=E (cid:2)(cid:80)∞ γkR |S =z(x ),A =a) (cid:3) (6)
π π k=0 t+k+1 t t+k+1 t
The z(x) function replaces the S , which is the state representation at the
t
time-step t. In other words, to reduce the problem’s computational complex-
ity, it is converted into a partially observable environment using reinforce-
ment learning-based hard attention. Thus, the global solution is computed
by solving only a small number of sub-domain problems. Evaluations pre-
sented in this paper demonstrate that this method reduces computational
complexity without affecting the model’s end-to-end accuracy.
8
3.1. Active Inference
Text is observed by the RAIT model using a text crop function. Crop
function coordinates are derived from their internal state. As a first step,
the internal state is initialized by focusing on the beginning of the text. The
crop function’s coordinates are updated in the next steps according to the
RL agent’s actions. The crop function’s length (number of words to keep)
could also be modified to adjust the trade-off between computational cost
and accuracy. The evaluations in section 4 show the relationship between
crop size and the model’s accuracy.
The texts are encoded using a BERT encoder [20], and the produced
representations are fed into the RL agent during training. The size of the
crop function determines BERT encoder embedding sequence length. As
demonstrated by Equation 6, reducing the crop size reduces the computation
required by the BERT encoder by reducing the number of words included in
the self-attention correlation matrix. The BERT encoder produces a fixed-
size vector of 768 dimensions. Therefore, the reinforcement learning agent’s
network architecture does not depend on crop size.
The reinforcement learning agent learns to predict text’s category while
observing a minimal set of text-window representations. The deep reinforce-
ment learning agent is trained end-to-end with the BERT encoder using Q
learning loss. Consequently, the negative reward for changing the crop func-
tion coordinates and the positive reward for correctly predicting the text’s
category are propagated using the error gradient through the DRL agent and
the BERT encoder. Due to the discrete nature of the problem’s formulation
and the data efficiency of off-policy methods, a DQN is chosen for this study.
The agent’s actions can be divided into two categories. During the ”see
more” action, the crop function coordinates are changed programmatically
so that the next text window can be loaded. The next cycle of text encoding
and agent decision-making begins with this action. The ”select class” action,
on the other hand, terminates the classification loop. This action, which
consists of multiple output nodes corresponding to the number of possible
classes, selects the text’s category based on the text windows observed up to
this point.
3.2. Model Training
The model is rewarded for its interactions with partially observable tex-
tual environments in two ways. Firstly, negative rewards (-0.1) are given
for changes in window position to encourage the model to take the smallest
9
number of steps. Secondly, the model can observe a positive (+1) or negative
(-1) reward for choosing a readability level for the intended text by picking
one of the readability classes. A visual depiction of model interaction with
the textual environment is shown in Figure 1.
Figure 1: The interactions of the RAIT model with the text readability environment.
BERT representations of words in the attention window act as the RL agent’s observa-
tion. The agent decides to continue the observation or select the text’s class based on
observations and rewards.
Figure 2: The classification process of the RAIT model. The crop text function is initial-
izedtocapturetheleft-mostwindowatfirst. TheRLagent’ssubsequentactionsdetermine
the crop window’s future location. The processing loop is terminated when the RL agent
selects a class for the input text.
At each training step, the loss of the model is calculated using the Q
learning equation (Equations 7 and 8). Since deep reinforcement learning
models are prone to divergence, a method called double Q network learning
[52] is applied to stabilize the process. For each state-action pair, the target
Q value is computed using a frozen instance of the main model to avoid
oscillations. As soon as the predefined training steps have been completed,
the frozen network, which is called the target network, is replaced by a new
copy of the main Q network.
10
(cid:104) (cid:105)
Q(z(x ),a )=Q(z(x ),a )+α r +γmaxQ(z(x ),a)−Q(s ,a ) (7)
t t t t t+1 t+1 t t
a
(cid:16) (cid:17)2
loss= r +γmaxQ(z(x ),a)−Q(z(x ),a ) (8)
t+1 t+1 t t
a
Deep reinforcement learning models demand numerous interactions with
their environment to be sufficiently trained. To overcome this problem and
furtherstabilizethelearningprocess,atechniquecalledexperiencereplay[53]
is used in the current study. During its training process, the DRL model is
fed multiple times with (current−state,action,reward,next−state) tuples
containing its previous interactions with the environment. The reintroduc-
tion of previous experiences to the model increases data usage efficiency and
prevents the model from forgetting older experiences. Additional hyperpa-
rameters are presented in Table 2.
Table 2: Hyperparameters of the RAIT model.
Hyperparameter Value
Main network learning rate 2e−5
Number of training episodes 2.5e5
Target network update frequency 5 episodes
Experience buffer size 1000
Q learning discount factor 0.99
Exploration policy ϵ−greedy
Initial action randomness 100%
Final action randomness 10%
4. Experiments
Using active inference, the proposed method effectively decreases the
computational cost of transformer-based text readability assessment models.
Mathematically, the suggested method reduces computational costs by uti-
lizing a smaller self-attention perception field than a standard transformer.
A comparison of the RAIT model with standard baselines in text readability
11
assessment is presented in this section to demonstrate that it is effective at
reducing the computing requirements of text readability assessment without
significantly adversely affecting accuracy at the end-to-end level.
4.1. Datasets
RAITisevaluatedusingtwotextreadabilitydatasets. Firstly, theWeebit
dataset [17] is used to assess the model’s accuracy in deciding the readability
of English texts for native readers. Weebit is gathered from articles in the
Weekly Reader magazine and BBC-Bitesize, which are targeted at readers of
different ages. There are five different readability levels in the Weebit dataset
arranged by age (8-9, 9-10, 10-12, 12-14, 14-16). More than ten thousand
texts are included in the dataset. In addition to the Weebit dataset, RAIT is
applied to the Cambridge Exams dataset [18] in order to assess its English as
a second language readability. This dataset’s text comes from the Cambridge
English Exams reading section, which targets students at five CEFR levels
(A2 to C2). In comparison to the Weebit dataset, the Cambridge dataset
contains only 331 texts. Details about the datasets can be found on Table 3.
Table 3: Number of texts and length of texts at each readability level before balancing.
Dataset Class # texts Avg. # words/text
Weebit [17] 8-9 629 150
9-10 789 189
10-12 807 289
12-14 646 238
14-16 7615 347
total 10486 311
Cambridge [18] A2 64 139
B1 60 268
B2 71 613
C1 67 768
C2 69 752
total 331 519
According to Table 3, the Cambridge dataset is fairly balanced, while the
Weebit dataset has extreme imbalance in its fifth class (14-16). A large dif-
ference in data point counts between classes can lead to imbalanced learning
12
problems. Therefore, similarly to Weebit’s original paper [17], a total of 3145
texts are used for evaluation purposes.
4.2. Baselines
Common baselines and state-of-the-art models are implemented to eval-
uate the presented model comprehensively. Transformers, led by the BERT
model, have dominated natural language processing. A comparison between
RAIT and the BERT model evaluates the accuracy of the method when
compared to the current state-of-the-art in-text readability models. More-
over, Word2Vec-based ConvLSTM models actas their low-computation, low-
accuracy counterparts to demonstrate the superior accuracy of the proposed
method at a low computation cost.
4.2.1. Convolutional-LSTM
The Word2Vec ConvLSTM [54] is a sequence classification model. The
vector space representation of words in the input text window is combined
to form a 2-dimensional view of the input text (1-dimensional Word2Vec
× 1-dimensional text). In the ConvLSTM model, convolutional layers act
as feature extraction modules, encoding information as a window embedding
from the input. In order to create a text embedding, the window embeddings
are aggregated using LSTM layers. Therefore, ConvLSTM operates as a
Text2Vec model. A supervised cross-entropy loss feeds the convolutional
backbone and LSTM layers with their learning signals. The architecture
of the ConvLSTM-based text readability assessment model can be found in
Figure 3.
Figure 3: The architecture of the ConvLSTM model. The Word2Vec representation se-
quence is fed to the convolutional backbone for feature extraction. With the help of an
LSTM layer, the extracted features for each word are aggregated into a text-level feature.
Finally, an MLP classifies the aggregated features according to their readability.
4.2.2. BERT
Considering the BERT model’s unprecedented and excellent results in
several NLP tasks, it is exciting to see BERT’s accuracy on the text readabil-
ity assessment task. The BERT model was trained on a large-scale corpus,
13
and its deep learning architecture allows it to capture the subtle nuances
of language. Thus, it is expected to provide highly accurate readability
assessments, as it can detect and model the complexities of a text. The
implemented BERT-based text readability assessment model works by clas-
sifying the text representations generated by a fine-tuned BERT model from
the texts in each dataset [33]. Similarly, RAIT also relies on the BERT
model as a backbone, but the fine-tuning process is done by the RL loss
and through the agent’s interaction with the textual environment. In this
study, the BERT base model is used, which has approximately 110 million
parameters [33].
4.3. Results
Datasetsaredividedintotwoparts, 80percentasthetrainingdatasetand
20 percent as the testing dataset. The datasets are bootstrapped to ensure
their validity and stability. Comparative analyses are conducted with differ-
ent window sizes applied to the models in order to demonstrate the model’s
abilitytoaccuratelyandefficientlypredicttextreadability. Benchmarkmod-
els also use limited window sizes of 256, 128, and 64 words in addition to the
full text as input. Each model’s perception is therefore limited to the first N
words in a text. The defined window sizes are used in training and evaluating
all three models. We train models with different hyperparameters, and we
report the results that are obtained with the best hyperparameters. Table 4
compares some of the recently published models for English text readability
on the Weebit [17] and Cambridge [18] readability datasets with BERT.
Table 4: Model accuracy comparison on the Weebit [17] and Cambridge [18] readability
datasets.
Model Weebit [17] Cambridge [18] Design
VajjalaandMeurers[17] 93.3% - SML(SVM)+hand-craftedfeatures
ConvLSTM[54] 74.4% 76.3% ConvolutionalNN+LSTM
Xiaetal.[18] - 80.3% SML(SVM)+hand-craftedfeatures
FujinumaandHagiwara[55] - 79.6% GraphConvolutionalNetwork
Qiuetal.[56] 87.3% 78.5% GraphNN+Transformer
Lietal.[57] 92.7% - Transformer+hand-crafterfeatures
Jianetal.[58] 89.1% - ConvolutionalNN+Pooling
BERT[33] 94.3% 76.1% Transformer
Table 4 demonstrates BERT’s superior accuracy (94.3%) on the Weebit
dataset. The second-best accuracy on this dataset is achieved by the SVM
14
model proposed by Vajjala and Meurers [17] which uses hand-crafted fea-
tures. Additionally, the original model proposed by Xia et al. [18] achived
state-of-the-art accuracy (80.3%) on the Cambridge dataset. However, SVM-
based models require the design and implementation of hand-crafted features
which is costly, and limits the generalizability of the model across languages
and tasks. As a result, despite the speed and accuracy of traditional models,
deeplearning-andtransformer-basedmodelsaremorewidelyusedbecauseof
their feature extraction automation. Automated feature extraction reduces
task- and language-specific biases in the model which in turn increases the
need for more data. The data requirement especially affects deep models’
performance on datasets of smaller sizes (i.e., Cambridge). Use of small and
biased models, more training data, and transfer learning is preferred in these
scenarios.
The RAIT model increases the computational efficiency of the BERT
model by reducing self-attention computations inside the transformer back-
bone. The comparison between the proposed model results and the other
state-of-the-art models is shown in Table 5. According to the reported accu-
racy and latency, a reduction in the input length of the BERT model reduces
its latency while negatively affecting its accuracy. On the other hand, the
RAIT model can avoid drastic accuracy loss by actively adjusting the length
of text required for accurate classification. As a result, the RAIT model can
achieve close to state-of-the-art accuracy with less than half of the original
BERT’s latency. For example, the RAIT models with a window size of 64
achieve an accuracy of 93.8% on the Weebit dataset with a latency of 2.1 ms.
In other works, the 83% reduction in computational cost with only 0.5% loss
in accuracy.
5. Discussion
Based on evaluation results, RAIT maintains state-of-the-art accuracy
while significantly reducing model size and computation load. Similar to
its original BERT counterpart, the full window-size model behaves like a
standard transformer. The ”see more” action is virtually eliminated when
the window size is at its maximum. As a result, the proposed model differs
only in its reinforcement learning loss instead of its multiclass cross-entropy
loss from the BERT model in this scenario. However, the presented method
is more accurate when the window size is limited to a value less than the
maximum possible size. Based on the proposed method results, the 256, 128,
15
Table 5: Comparison between the proposed model and the state-of-the-art models on
differentdatasets. TheexecutiontimesweremeasuredusingtheHuggingface[51]Pytorch
[59] implementation of BERT [33] on an A100 GPU with a batch size of 1.
Model (window-size) Weebit [17] Cambridge [18] Latency (ms)
BERT (full) 0.943 0.761 12.6
BERT (256) 0.916 0.734 3.8
BERT (128) 0.911 0.729 1.6
BERT (64) 0.887 0.712 1.0
RAIT (full) 0.943 0.762 12.8
RAIT (256) 0.942 0.76 4.6
RAIT (128) 0.939 0.758 2.7
RAIT (64) 0.938 0.755 2.1
and 64-word window sizes achieve state-of-the-art accuracy while observing
1.2, 1.7, and 2.1 windows of words, respectively. Conversely, models such
as BERT and ConvLSTM show reduced accuracy when limited to smaller
observation windows.
Active inference allows the proposed method to accurately and effi-
ciently classify partially observed input data based on the model’s represen-
tation. In other words, a dynamic selection of observed windows is used to
maximize the accuracy of each input text and minimize the model’s load. It
isreasonabletousedynamicobservationlengthsduetothefactthatdifferent
texts’ representations have varying marginal errors within their class cluster.
Text representations closer to class cluster boundaries must be more accurate
to avoid misclassification. RAIT can also adjust the global solution aggre-
gation strategy to maintain accuracy close to the state-of-the-art. When the
window size is smaller, the self-attention function is further limited to gen-
erating contextually relevant text representations. However, as presented in
Table 5, the proposed method adapts to the smaller window size by increas-
ing the number of observed windows. The limited self-attention perception
range negatively impacts the quality of learned representations, but the rein-
forcement learning-based global solution aggregation described in Equations
5 and 6 minimizes the negative impact when compared to state-of-the-art
models.
16
6. Conclusion and future work
The readability of a text can be determined precisely by analyzing a small
section of it. This study introduces a reinforcement learning-based hard at-
tention method based on transformers in order to realize this point. This
hard attention method allows for precise readability assessment by focusing
on the most informative parts of the text. RAIT uses the Domain Decom-
position Method (DDM) to divide the computationally expensive attention
mechanism across smaller and more manageable portions of the text. DDM
further reduces computation complexity by calculating the global solution
from a minimal set of sub-problem solutions using reinforcement learning.
As evidenced by the comparison between the newly developed method and
standardbaselines,computationalcostreductionhasminimaleffectonmodel
accuracy.
The next step in this research is to test the RAIT model on additional
NLP tasks, including automated essay scoring, sentiment analysis, and text
classification in general. This will assess the generality of this paper’s as-
sumptions. Combining this method with other deep computation reduction
methods like distillation can further optimize transformer-based text pro-
cessing methods. Currently, the proposed method does not interact with
the input text beyond simple actions by the reinforcement learning agent.
There is no possibility for the agent to change the observation window size
dynamically or jump to different text sections randomly. The global solution
aggregation strategy can be further optimized with these actions.
Acknowledgements
We would like to express my deepest appreciation to Dr. Ekaterina
Kochmar1, Dr. Baosong Yang2, and Dr. Yang Gao3 for their valuable review
and suggestions on this manuscript. We are also grateful to HuggingFace
[51] and StableBaselines3 [60] for providing us with state-of-the-art tools
and frameworks for natural language processing and reinforcement learning.
Their open-source contributions have greatly facilitated our research and ex-
periments.
1Department of Computer Science and Technology, University of Cambridge
2Department of Computer and Information Science, University of Macau
3Department of Computer Science, Royal Holloway, University of London
17
References
[1] R. M. Rawian, Text readability: A snapshot, 2019.
[2] B. T. Cheng, A. B. Kim, A. P. Tanna, Readability of online patient
education materials for glaucoma, Journal of Glaucoma 31 (2022) 438–
442.
[3] A. Devaraj, B. C. Wallace, I. J. Marshall, J. J. Li, Paragraph-level sim-
plification of medical texts, in: Proceedings of the conference. Associa-
tion for Computational Linguistics. North American Chapter. Meeting,
volume 2021, NIH Public Access, 2021, p. 4972.
[4] O. Mac, J. Ayre, K. Bell, K. McCaffery, D. M. Muscat, Comparison
of readability scores for written health information across formulas us-
ing automated vs manual measures, JAMA Network Open 5 (2022)
e2246051–e2246051.
[5] Y. Guo, W. Qiu, Y. Wang, T. Cohen, Automated lay language summa-
rization of biomedical scientific reviews, in: Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, 2021, pp. 160–168.
[6] Y. Zheng, Y. Tang, H.-C. Tseng, T.-H. Chang, L. Li, P. Chen, Y. Tang,
X.-b. Lin, X. Chen, K.-J. Tang, Evaluation of quality and readability
of over-the-counter medication package inserts, Research in Social and
Administrative Pharmacy 18 (2022) 3560–3567.
[7] B. Ondov, K. Attal, D. Demner-Fushman, A survey of automated meth-
ods for biomedical text simplification, Journal of the American Medical
Informatics Association 29 (2022) 1976–1988.
[8] E. Pancer, V. Chandler, M. Poole, T. J. Noseworthy, How readability
shapes social media engagement, Journal of Consumer Psychology 29
(2019) 262–270.
[9] S. Sazzed, Influence of language proficiency on the readability of review
textandtransformer-basedmodelsfordetermininglanguageproficiency,
in: Companion Proceedings of the Web Conference 2022, 2022, pp. 881–
886.
18
[10] D. C. Gkikas, K. Tzafilkou, P. K. Theodoridis, A. Garmpis, M. C.
Gkikas, How do text characteristics impact user engagement in social
media posts: Modeling content readability, length, and hashtags num-
berinfacebook, InternationalJournalofInformationManagementData
Insights 2 (2022) 100067.
[11] A. Curiel, C. Guti´errez-Soto, J.-R. Rojano-C´aceres, An online multi-
source summarization algorithm for text readability in topic-based
search, Computer Speech & Language 66 (2021) 101143.
[12] R. Korniichuk, M. Boryczka, Conversion rate prediction based on text
readability analysis of landing pages, Entropy 23 (2021) 1388.
[13] D. Murphy Odo, The effect of automatic text simplification on l2 read-
ers’ text comprehension, Applied Linguistics (2022).
[14] J. Monteiro, M. Aguiar, S. Arau´jo, Using a pre-trained simplet5 model
for text simplification in a limited corpus, Proceedings of the Working
Notes of CLEF (2022).
[15] L. Cripwell, J. Legrand, C. Gardent, Document-level planning for text
simplification, in: Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Linguistics, 2023, pp.
993–1006.
[16] R. Flesch, Marks of readable style; a study in adult education., Teachers
College Contributions to Education (1943).
[17] S. Vajjala, D. Meurers, On improving the accuracy of readability classi-
ficationusinginsightsfromsecondlanguageacquisition, in: Proceedings
oftheseventhworkshoponbuildingeducationalapplicationsusingNLP,
Association for Computational Linguistics, 2012, pp. 163–173.
[18] M. Xia, E. Kochmar, T. Briscoe, Text readability assessment for sec-
ond language learners, in: Proceedings of the 11th Workshop on Inno-
vative Use of NLP for Building Educational Applications, Association
for Computational Linguistics, San Diego, CA, 2016, pp. 12–22. URL:
https://aclanthology.org/W16-0502. doi:10.18653/v1/W16-0502.
[19] H. Mohammadi, S. H. Khasteh, A machine learning approach to persian
text readability assessment using a crowdsourced dataset, in: 2020 28th
19
Iranian Conference on Electrical Engineering (ICEE), IEEE, 2020, pp.
1–7.
[20] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A.N.Gomez,
L(cid:32) . Kaiser, I. Polosukhin, Attention is all you need, Advances in neural
information processing systems 30 (2017).
[21] A. J. Stenner, Measuring reading comprehension with the lexile frame-
work. (1996).
[22] K. Collins-Thompson, J. Callan, Predicting reading difficulty with sta-
tistical language models, Journal of the American Society for Informa-
tion Science and Technology 56 (2005) 1448–1462.
[23] R. G. Benjamin, Reconstructing readability: Recent developments and
recommendations in the analysis of text difficulty, Educational Psychol-
ogy Review 24 (2012) 63–88.
[24] J. Hartley, Is time up for the flesch measure of reading ease?, Sciento-
metrics 107 (2016) 1523–1526.
[25] S. E. Petersen, M. Ostendorf, A machine learning approach to reading
level assessment, Computer speech & language 23 (2009) 89–106.
[26] S. A. Crossley, D. B. Allen, D. S. McNamara, Text readability and
intuitive simplification: A comparison of readability formulas., Reading
in a foreign language 23 (2011) 84–101.
[27] I. Pila´n, E. Volodina, R. Johansson, Rule-based and machine learn-
ing approaches for second language sentence-level readability, in: Pro-
ceedings of the ninth workshop on innovative use of NLP for building
educational applications, 2014, pp. 174–184.
[28] L. Feng, M. Jansche, M. Huenerfauth, N. Elhadad, A comparison of
features for automatic readability assessment, in: Proceedings of the
23rd international conference on computational linguistics: Posters, As-
sociation for Computational Linguistics, 2010, pp. 276–284.
[29] T. Fran¸cois, E. Miltsakaki, Do nlp and machine learning improve tra-
ditional readability formulas?, in: Proceedings of the First Workshop
on Predicting and Improving Text Readability for target reader popu-
lations, Association for Computational Linguistics, 2012, pp. 49–57.
20
[30] S. E. Schwarm, M. Ostendorf, Reading level assessment using support
vector machines and statistical language models, in: Proceedings of
the 43rd Annual Meeting on Association for Computational Linguistics,
Association for Computational Linguistics, 2005, pp. 523–530.
[31] R. J. Kate, X. Luo, S. Patwardhan, M. Franz, R. Florian, R. J. Mooney,
S. Roukos, C. Welty, Learning to predict readability using diverse lin-
guistic features, in: Proceedings of the 23rd international conference on
computational linguistics, Association for Computational Linguistics,
2010, pp. 546–554.
[32] Y.-T. Sung, J.-L. Chen, J.-H. Cha, H.-C. Tseng, T.-H. Chang, K.-E.
Chang, Constructing and validating readability models: the method of
integrating multilevel linguistic features with machine learning, Behav-
ior research methods 47 (2015) 340–354.
[33] J. D. M.-W. C. Kenton, L. K. Toutanova, Bert: Pre-training of deep
bidirectional transformers for language understanding, in: Proceedings
of NAACL-HLT, 2019, pp. 4171–4186.
[34] X. Ke, English synchronous real-time translation method based on re-
inforcement learning, Wireless Networks (2022) 1–13.
[35] X. Kang, Y. Zhao, J. Zhang, C. Zong, Dynamic context selection for
document-level neural machine translation via reinforcement learning,
arXiv preprint arXiv:2010.04314 (2020).
[36] V. Uc-Cetina, N. Navarro-Guerrero, A. Martin-Gonzalez, C. Weber,
S. Wermter, Survey on reinforcement learning for language processing,
Artificial Intelligence Review 56 (2023) 1543–1575.
[37] T. K. Lam, J. Kreutzer, S. Riezler, A reinforcement learning approach
to interactive-predictive neural machine translation, in: 21st Annual
Conference of the European Association for Machine Translation, ????,
p. 169.
[38] Y. Zhao, L. Chen, Z. Chen, K. Yu, Semi-supervised text simplification
with back-translation and asymmetric denoising autoencoders, in: Pro-
ceedings of the AAAI Conference on Artificial Intelligence, volume 34,
2020, pp. 9668–9675.
21
[39] Z. Li, Z. Peng, S. Tang, C. Zhang, H. Ma, Text summarization method
basedondoubleattentionpointernetwork, IEEEAccess8(2020)11279–
11288.
[40] Y. Keneshloo, N. Ramakrishnan, C. K. Reddy, Deep transfer reinforce-
mentlearningfortextsummarization, in: Proceedingsofthe2019SIAM
International Conference on Data Mining, SIAM, 2019, pp. 675–683.
[41] Z. Liang, J. Du, C. Li, Abstractive social media text summarization
using selective reinforced seq2seq attention model, Neurocomputing 410
(2020) 432–440.
[42] M. Yang, W. Huang, W. Tu, Q. Qu, Y. Shen, K. Lei, Multitask learn-
ing and reinforcement learning for personalized dialog generation: An
empirical study, IEEE transactions on neural networks and learning
systems 32 (2020) 49–62.
[43] A.Saleh, N.Jaques, A.Ghandeharioun, J.Shen, R.Picard, Hierarchical
reinforcement learning for open-domain dialog, in: Proceedings of the
AAAI Conference on Artificial Intelligence, volume 34, 2020, pp. 8741–
8748.
[44] F. Godin, A. Kumar, A. Mittal, Learning when not to answer: a ternary
reward structure for reinforcement learning based question answering,
in: Proceedings of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Industry Papers), 2019, pp. 122–129.
[45] Y.Qiu,Y.Wang,X.Jin,K.Zhang, Stepwisereasoningformulti-relation
question answering over knowledge graph with weak supervision, in:
Proceedings of the 13th international conference on web search and data
mining, 2020, pp. 474–482.
[46] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse,
S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-
assisted question-answering with human feedback, arXiv preprint
arXiv:2112.09332 (2021).
[47] G. H. de Rosa, J. P. Papa, A survey on text generation using generative
adversarial networks, Pattern Recognition 119 (2021) 108098.
22
[48] T. Iqbal, S. Qureshi, The survey: Text generation models in deep learn-
ing, Journal of King Saud University-Computer and Information Sci-
ences 34 (2022) 2515–2528.
[49] T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, C. Zhang, Reinforced
self-attention network: a hybrid of hard and soft attention for sequence
modeling, in: Proceedings of the 27th International Joint Conference
on Artificial Intelligence, 2018, pp. 4345–4352.
[50] S. Cao, P. Xu, D. A. Clifton, How to understand masked autoencoders,
arXiv preprint arXiv:2202.03670 (2022).
[51] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-
tac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von
Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger,
M. Drame, Q. Lhoest, A. Rush, Transformers: State-of-the-art nat-
ural language processing, in: Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing: System Demon-
strations, Association for Computational Linguistics, Online, 2020,
pp. 38–45. URL: https://aclanthology.org/2020.emnlp-demos.6.
doi:10.18653/v1/2020.emnlp-demos.6.
[52] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with
double q-learning, in: Thirtieth AAAI conference on artificial intelli-
gence, 2016.
[53] L.-J.Lin,Reinforcementlearningforrobotsusingneuralnetworks,Tech-
nical Report, Carnegie-Mellon Univ Pittsburgh PA School of Computer
Science, 1993.
[54] A. Hassan, A. Mahmood, Deep learning approach for sentiment anal-
ysis of short texts, in: 2017 3rd international conference on control,
automation and robotics (ICCAR), IEEE, 2017, pp. 705–710.
[55] Y. Fujinuma, M. Hagiwara, Semi-supervised joint estimation of word
and document readability, arXiv preprint arXiv:2104.13103 (2021).
[56] X. Qiu, Y. Chen, H. Chen, J.-Y. Nie, Y. Shen, D. Lu, Learning syn-
tactic dense embedding with correlation graph for automatic readability
assessment, arXiv preprint arXiv:2107.04268 (2021).
23
[57] W. Li, Z. Wang, Y. Wu, A unified neural network model for readabil-
ity assessment with feature projection and length-balanced loss, arXiv
preprint arXiv:2210.10305 (2022).
[58] L.Jian, H.Xiang, G.Le, Englishtextreadabilitymeasurementbasedon
convolutional neural network: a hybrid network model, Computational
Intelligence and Neuroscience 2022 (2022).
[59] A.Paszke, S.Gross, S.Chintala, G.Chanan, E.Yang, Z.DeVito, Z.Lin,
A. Desmaison, L. Antiga, A. Lerer, Automatic differentiation in pytorch
(2017).
[60] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, N. Dor-
mann, Stable-baselines3: Reliable reinforcement learning implemen-
tations, Journal of Machine Learning Research 22 (2021) 1–8. URL:
http://jmlr.org/papers/v22/20-1364.html.
24