Active Inference for Robotic Manipulation
TimSchneider BorisBelousov HanyAbdulsamad
IntelligentAutonomousSystems IntelligentAutonomousSystems IntelligentAutonomousSystems
TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt
64289Darmstadt,Germany 64289Darmstadt,Germany 64289Darmstadt,Germany
tim@robot-learning.de boris@robot-learning.de hany@robot-learning.de
JanPeters
IntelligentAutonomousSystems
TechnicalUniversityofDarmstadt
64289Darmstadt,Germany
mail@jan-peters.net
Abstract
Roboticmanipulationstandsasalargelyunsolvedproblemdespitesignificantadvancesinroboticsandmachinelearning
inthelastdecades. Oneofthecentralchallengesofmanipulationispartialobservability,astheagentusuallydoesnot
knowallphysicalpropertiesoftheenvironmentandtheobjectsitismanipulatinginadvance. Arecentlyemergingtheory
thatdealswithpartialobservabilityinanexplicitmannerisActiveInference. Itdoessobydrivingtheagenttoactina
waythatisnotonlygoal-directedbutalsoinformativeabouttheenvironment. Inthiswork,weapplyActiveInferenceto
ahard-to-exploresimulatedroboticmanipulationtasks,inwhichtheagenthastobalanceaballintoatargetzone. Since
therewardofthistaskissparse,inordertoexplorethisenvironment,theagenthastolearntobalancetheballwithout
anyextrinsicfeedback,purelydrivenbyitsowncuriosity. Weshowthattheinformation-seekingbehaviorinducedby
ActiveInferenceallowstheagenttoexplorethesechallenging,sparseenvironmentssystematically. Finally,weconclude
thatusinganinformation-seekingobjectiveisbeneficialinsparseenvironmentsandallowstheagenttosolvetasksin
whichmethodsthatdonotexhibitdirectedexplorationfail.
Keywords: Model-BasedReinforcementLearning,RoboticManipulation,Ac-
tiveInference
2202
nuJ
1
]OR.sc[
1v31301.6022:viXra
1 IntroductionandRelatedWork
Acommonbeliefincognitivescienceisthattheevolutionofdexterousmanipulation
capabilities was one of the major driving factors in the development of the human
mind[1]. Performingmanipulationiscognitivelyhighlydemanding,forcingtheactor
toreasonnotonlyabouttheimpactofitsactionsonitselfbutalsoabouttheimpacton
itsenvironment. Thisinherentcomplexityleavesautonomousroboticmanipulationa
largelyunsolvedtopic,despitesignificantadvancesinroboticsandmachinelearningin
thelastdecades.
One of the central challenges of manipulation is partial observability. While we are
manipulatinganobject,werarelyknowallofitsphysicalpropertiesinadvance. Instead,
wemustresorttoinferringthosepropertiesbasedonobservationsandtouch. Todeal
withthisissueaseffectivelyaspossible,humanshavedevelopedvariousactivehaptic
explorationstrategiesthattheyconstantlyapplyduringmanipulationtasks[2].
A recently emerging theory from cognitive science that tries to explain this notion of
constantactiveexplorationisActiveInference(AI)[3]. AIformulatesbothactionand Figure 1: Robot using Active
perceptionastheminimizationofasinglefree-energyfunctional,calledtheVariational Inference to solve a challeng-
FreeEnergy(VFE).Indoingso,Fristonetal.[4]deriveanobjectivefunctionthatconsists ingmanipulationtask.
ofanextrinsic,goal-directedtermandanintrinsic,information-seekingterm. Thecom-
binationofthesetwotermsdrivestheagenttoactinawaythatisbothgoal-directedand
informative,inthattheagentlearnsaboutitsenvironmentthroughitsactions.
Inthiswork,weshowhowAIcanbeusedtolearnchallengingroboticmanipulationtaskswithoutpriorknowledge. For
now,weassumethattheenvironmentisfullyobservableandonlyconsiderepistemicuncertainty1. ToimplementAI
inpractice,weuseaneuralnetworkensembleanddeployModelPredictiveControlforactionselection. Weshowthat
agentsdrivenbyAIexploretheirenvironmentsinadirectedandsystematicway. Theseexploratorycapabilitiesallowthe
agentstosolvecomplexsparsemanipulationtasks,onwhichagentsthatarenotexplicitlyinformation-seekingfail.
RelatedtoourapproachisPETS[5],whichalsotrainsensemblemodelsforthetransitionandrewarddistributionsand
selectsactionswithaCross-EntropyMethodplanner. ThekeydifferencetoourapproachisthatPETSdoesnotusean
intrinsictermandinsteadgreedilyselecttheactionstheypredicttoyieldthehighestreward.
AnapproachsimilartooursisTschantzetal.[6],whoalsotackleRLtaskswithAI.Thedifferencetoourapproachisthat
theyuseadifferentfreeenergyfunctionalusedforplanningandchoseadifferentapproximationoftheirintrinsicterm,
whichrequiresthemtomakeamean-fieldassumptionoverconsecutivestates. Theyevaluatetheirapproachonmultiple
RLbenchmarks,includingMountainCarandCupCatch.
2 ActiveInference
According to the Free Energy Principle (FEP) [3], any organism must restrict the states it is visiting to a manageable
amount. Mathematically,AIimplementsthisrestrictionasfollows: Everyagentmaintainsagenerativemodelpofthe
worldandavoidssensationsothataresurprising,hencehavealowmarginallog-probabilitylnp(o). Thus,theobjective
canbewrittenas
min −lnp(o) (1)
π
whereoisgeneratedbysomeexternalprocessthatcanbeinfluencedbychangingthepolicyπ.
Theagent’sgenerativemodelisassumedtoconsistofnotonlyobservationso,butalsocontainhiddenstatesx,giving
(cid:82) (cid:82)
p(o) = p(o,x)dx = p(o|x)p(x)dx. TomakeEq.(1)tractable, weapplyvariationalinferenceandobtaintheELBO
usingJensen’sinequality:
(cid:90) (cid:90) q (x)
−lnp(o)=−ln p(o,x)dx=−ln φ p(o,x)dx≤D [q (x)(cid:107)p(x|o)]−lnp(o)=:F(o,φ)
q (x) KL φ
φ
whereq (x)isthevariationalposterior,parameterizedbyφ,andF(o,φ)istermedtheVariationalFreeEnergy(VFE)in
φ
theAIliterature.
Minimizing F(o,φ) w.r.t. the variational parameters φ corresponds to minimizing the KL divergence between the
variationalposteriorq (x)andthetrueposteriorp(x|o). Inotherwords,byminimizingtheVFEw.r.t.φ,theagentis
φ
solvingtheperceptionproblemofmappingitsobservationstotheirlatentcauses.
1Epistemicuncertaintyistheuncertaintytheagenthasoveritsmodeloftheworld.Incontrast,aleatoricuncertaintyisuncertainty
overtheagent’sstate.
1
Tofacilitateplanningintothefuture,theVFEcanbemodifiedtoincorporateanexpectationoverfuturestates,yielding
theExpectedFreeEnergy(EFE)Fristonetal.[4]:
G (φ)=−E [lnp(o ,x )−lnq (x |π)]
π qφ(ot+1:T,xt+1:T|π) t+1:T t+1:T φ t+1:T
≈−E [D [q (o|x,π)(cid:107)q (o|π)]]−E [lnp(o)]
qφ(x|π) KL φ φ qφ(o|π)
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
intrinsicterm(expectedinformationgain) extrinsicterm
whereweomittedsubscriptsforreadabilityanddefinedq (o|x):=p(o|x),suchthatq andpfollowthesameobservation
φ φ
model.
TheminimizationoftheEFEw.r.t.thepolicyπcausestheagenttoactinawaythatmaximizesbothinformationgainand
theextrinsicterm. Here,theextrinsictermactsasanexternalsignalthatallowsustomaketheagentpreferordisprefer
certainobservations. WhileitiscommoninRLliteraturetousearewardfunctiontogivetheagentanotionof“good”
and“bad”behavior,intheAIframework,wedefineapriordistributionovertargetobservationsp(o)thatwewouldlike
theagenttomake. Notethatbymakingtherewardpartoftheobservationandsettingthemaximumrewardastarget
observation[6],wecantransformanyreward-basedtasktofitintotheAIframework.
3 Method
In this work, we propose a model-based Reinforcement Learning algorithm that uses AI to efficiently explore chal-
lenging state spaces. Therefore, we assume that the environment is fully observable, governed by unknown dynam-
ics P(x |x ,a ) and provides the agent with a reward P(r |x ,a ) in every time step. We model both the dy-
τ τ−1 τ τ τ τ
namics and the reward with neural network conditioned Gaussians p(r |x ,a ,θ) := N (x |µx(x ,a ),σxI) and
τ τ τ τ θ τ−1 τ
p(r |x ,a ,θ):=N (r |µr(x ,a ),σrI),resultinginthefollowinggenerativemodel:
τ τ τ τ θ τ τ
T
(cid:89)
p(x ,a ,r ,θ)=p(x )p(a )p(θ) p(r |x ,a ,θ)p(x |x ,a ,θ)
0:T 1:T 1:T 0 1:T τ τ τ τ τ−1 τ
τ=1
Sincetheenvironmentisfullyobserved,theonlyhiddenvariablesaretheneuralnetworkparametersθ. Thus,weareleft
withthefollowingminimizationproblemforselectingapolicyπ :=a attimet:
t+1:T
(cid:34) T (cid:35)
(cid:88)
minG (φ):=−E [D [p(x ,r |θ,π)(cid:107)q (x ,r |π)]]−E r (2)
π
π qφ(θ|π) KL t+1:T t+1:T φ t+1:T t+1:T qφ(rt+1:T|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) τ=t+1
expectedparameterinformationgain (cid:124) (cid:123)(cid:122) (cid:125)
expectedcumulativereward
wherewedefinedtheobservationpreferencedistributionsuchthatp(o
τ
)∝erτ,anddefinedq
φ
(x,r|θ,π):=p(x,r|θ,π).
Hence,bythisdefinition,qandpdifferonlyinthemarginalprobabilityofthemodelparametersθ.
SimilartoothermethodsutilizingModelPredictiveControl[5],byminimizingthisobjectivefunctionweselectapolicy
thatmaximizestheexpectedcumulativerewardoverafixedhorizon. However,additionallywearemaximizingthe
expectedparameterinformationgain,drivingtheagenttoseekoutstatesthatareinformativeaboutitsmodelparameters
θ. Thistermcausestheagenttobecuriousaboutitsenvironmentandexploreitsystematically,eveninthetotalabsence
ofextrinsicreward. Theoptimizationofthisobjectivecannowtheoreticallybedonebyanyplannerthatiscapableof
handlingcontinuousactionspaces. Inthiswork,similartoChuaetal.[5],weuseavariantoftheCross-EntropyMethod
tofindanopenloopsequenceofactionsa thatmaximizesEq.(2).
t+1:T
AmajorchallengeincomputingG (φ)isthatneithertheintrinsic,northeextrinsictermcanbecomputedinclosedform.
π
WhiletheextrinsictermcanstraightforwardlybeapproximatedwithsufficientaccuracyviaMonteCarlo,theintrinsic
termisknowntobenotoriouslydifficulttocompute[7]. Thus,insteadofmaximizingitdirectly,manymethodsmaximize
avariationallowerboundofit[8]. However,duetothehigh-dimensionalnatureofθ,theseapproachesaretooexpensive
tobeexecutedduringplanninginrealtime.
Hence,insteadweproposetouseaNestedMonteCarloestimatorthatreusessamplesfromtheouterestimatorinthe
innerestimatortoapproximatetheintrinsicterm:
n n
1 (cid:88) 1 (cid:88)
IG((x,r),θ)≈ lnp(x ,r |θ )−ln p(x ,r |θ )
n i i i n i i k
i=1 k=1
k(cid:54)=i
(cid:124) (cid:123)(cid:122) (cid:125)
innerestimator
(cid:124) (cid:123)(cid:122) (cid:125)
outerestimator
Althoughusingthesamesamplesθ ,...,θ intheinnerestimatorasintheouterestimatorviolatesthei.i.d. assumption,
1 n
wefoundthisreuseofsamplestoincreasethesampleefficiencysubstantially. Sincethisestimatoronlyrequiressamples
ofθ,werepresentq (θ)byasetofparticlesθ ,...,θ ,makingourmodelaneuralnetworkensemble.
φ 1 n
2
4 ExperimentalResults
A central feature that sets our method apart from other
purelymodel-basedapproaches[5,9]istheintrinsicterm,
thatexplicitlydrivestheagenttoexploreitsenvironment
inasystematicmanner. Toevaluatetheexploratorycapa-
bilitiesofourmethod,wedesignedtwohard-to-explore
manipulationtasks: TiltedPushingandTiltedPushingMaze.
Inbothtasks,theagenthastopushaballupatiltedtable
intoatargetzonetoreceivereward. Theagentcanmove
thegripperinaplaneparalleltothetableandrotatethe
blackend-effectoraroundtheZ-axis(Z-axisbeingorthog-
onal to the brown table and pointing up). As input, the
agentreceivesthe2Dpositionsandvelocitiesofboththe
gripperandtheball,andtheangularpositionandvelocity
oftheend-effector. Toaddanadditionalchallenge,inthe Figure2:Visualizationofthetwoenvironmentconfigurations
Tilted Pushing Maze task we add holes to the table, that wetestourmethodson:TiltedPushing(left)andTiltedPushing
irrecoverablytraptheballifitfallsin. Foravisualization Maze(right). Thetargetzoneismarkedinred.
ofthesetasks,refertoFig.2.
Therearetwoaspectsmakethesetasksparticularlychallenging: First,therewardissparse,meaningthattheonlywaythe
agentcanlearnabouttherewardatthetopofthetableisbymovingtheballthereandexploringit. Second,balancingthe
ballonthefingerandmovingitaroundrequiresafairamountofdexterity,especiallygiventhelowcontrolfrequencyof4
Hz2 weoperateouragenton. Oncetheagentdropstheball,itcannotberecovered,givingtheagentnochoicebutto
waitfortheepisodetoterminatetocontinueexploring. Bothoftheseaspectsmakesolvingthesetaskswithconventional,
undirectedexplorationmethodslikeBoltzmannexplorationoraddingGaussiannoisetotheactionextremelychallenging.
Consequently,theagenthastolearntobalancetheballwithoutreceivinganyextrinsicreward,purelydrivenbyitsown
curiosity.
AsvisibleinFig.3,ourmethodisabletosolvetheTiltedPushing. BothSAC[10]andourmethodwithoutanintrinsicterm
failtofindtherewardwithin10,000episodes. TheholesofTiltedPushingMazemakethisenvironmentsignificantlyharder
toexplore,astheballhastobemaneuveredaroundtwocornersinordertoreachthetargetzone. Inthisexperiment,only
ourmethodfindstherewardwithin30,000episodes. AscanbeseeninFig.4,thereasonforthebadperformanceofthe
non-intrinsicagentisitsfailuretoexplorethefullstatespace. Whileouragentcontinuestosystematicallymaneuverthe
ballaroundtheholesinunseenlocations,thenon-intrinsicagentrarelypassesthelowerholesandleavestheupperhalfof
thetableunexplored.
40
20
0
0 2,000 4,000 6,000 8,000 10,000
Numberofepisodes
drawerevitalumuC
edosiperep
TiltedPushing TiltedPushingMaze
30 Ours Ours
Non-intr. Non-intr.
SAC SAC
20
10
0
0 10,000 20,000 30,000
Numberofepisodes
Figure3: Cumulativeper-episoderewardfortwodifferentversionsofouragent(onewithintrinsicterm,onewithout)
andSAConbothvariantsofourenvironment. Thisgraphdisplaystheevaluationreward,whichisobtainedbyrolling
outthelearnedmodelwithoutconsideringtheintrinsicreward. Bothnon-intrinsicconfigurationsandSACfailedtofind
theobjectiveandconvergedtolocalminima.
Theseexperimentshowthatourmethodisabletosystematicallyexploreacomplex,contact-richenvironmentwithmany
dead-ends. Withoutanyextrinsicfeedback,ouragentslearnedtobalancetheballontheend-effectorandsystematically
moveitaroundtheenvironmentuntilthetargetzonewasfound. Thesolereasonforthisbehaviortooccurinthefirst
2Thecomputationoftheintrinsictermiscomputationallyheavy,limitingustothisratherlowcontrolfrequency.
3
placeisthatouragentsunderstoodtheycouldonlyexploretheentirestatespaceiftheykeptbalancingtheballandmove
ittounseenlocations.
Ours
Non-intrinsic
Episodes 7000 14000 21000 28000 35000
Figure 4: Comparison of the states visited by our method and an agent using no intrinsic term, relying on Gaussian
explorationinstead. Thebrightnessofeachpixelindicateshowoftentheballhasvisitedtherespectivepointofthetableat
thegivenpointinthetraining. Thecoordinateoriginisatthebottomofeachimage,meaningthattheimagesarerotated
180°comparedtothetop-downviewinFig.2. Eachconfigurationwasrunonce.
5 Conclusion
Inthiswork,wedevelopedamethodcapableofapplyingActiveInferencetocomplexReinforcementLearningtasks. We
evaluatedourmethodintwochallengingroboticmanipulationtask,bothdesignedtobeparticularlyhard-to-explore.
Throughoutourexperiments,weshowedthatourmethodinducessystematicexplorationbehaviorandiscapableof
solvingeventhemostchallengingoftheseenvironments. Neitherthenon-intrinsicconfigurationsnorthemaximum
entropymethodSACmanagedtosolvetheroboticmanipulationtasks. Hence,weconcludethattheinformation-seeking
behaviorofouragentsisbeneficialforsolvingchallengingexplorationproblemswithsparserewards.
Finally,infutureworkweplantoapplyourmethodtoarealrobotandevaluatewhetherActiveInferencecanbeusedin
realroboticmanipulationtasks.
References
[1] RobertMacDougall.“Thesignificanceofthehumanhandintheevolutionofmind”.In:TheAmericanJournalof
Psychology16.2(1905),pp.232–242.
[2] AgnesLacreuseandDorothyMFragaszy.“Manualexploratoryproceduresandasymmetriesforahapticsearch
task:Acomparisonbetweencapuchins(Cebusapella)andhumans”.In:Laterality:AsymmetriesofBody,Brainand
Cognition2.3-4(1997),pp.247–266.
[3] KarlJFristonetal.“Actionandbehavior:afree-energyformulation”.In:Biologicalcybernetics102.3(2010),pp.227–
260.
[4] KarlFristonetal.“Activeinferenceandepistemicvalue”.In:Cognitiveneuroscience6.4(2015),pp.187–214.
[5] KurtlandChuaetal.“Deepreinforcementlearninginahandfuloftrialsusingprobabilisticdynamicsmodels”.In:
arXivpreprintarXiv:1805.12114(2018).
[6] AlexanderTschantzetal.“Reinforcementlearningthroughactiveinference”.In:arXivpreprintarXiv:2002.12636
(2020).
[7] DavidMcAllesterandKarlStratos.“Formallimitationsonthemeasurementofmutualinformation”.In:International
ConferenceonArtificialIntelligenceandStatistics.PMLR.2020,pp.875–884.
[8] BenPooleetal.“Onvariationalboundsofmutualinformation”.In:InternationalConferenceonMachineLearning.
PMLR.2019,pp.5171–5180.
[9] DanijarHafneretal.“Learninglatentdynamicsforplanningfrompixels”.In:InternationalConferenceonMachine
Learning.PMLR.2019,pp.2555–2565.
[10] TuomasHaarnojaetal.“Softactor-critic:Off-policymaximumentropydeepreinforcementlearningwithastochastic
actor”.In:Internationalconferenceonmachinelearning.PMLR.2018,pp.1861–1870.
4