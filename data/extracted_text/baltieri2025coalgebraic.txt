A coalgebraic perspective on predictive processing
Manuel Baltieri 1,2,† ,
 Filippo Torresan 1,2,
 Tomoya Nakai 1
1 Araya Inc., Tokyo, Japan
2 University of Sussex, Brighton, UK
Predictive processing and active inference posit that the brain is a system performing Bayesian
inference on the environment. By virtue of this, a prominent interpretation of predictive processing
states that the generative model (a POMDP) encoded by the brain synchronises with the generative
process (another POMDP) representing the environment while trying to explain what hidden
properties of the world generated its sensory input. In this view, the brain is thought to become
a copy of the environment. This claim has however been disputed, stressing the fact that a
structural copy, or isomorphism as it is at times invoked to be, is not an accurate description
of this process since the environment is necessarily more complex than the brain, and what
matters is not the capacity to exactly recapitulate the veridical causal structure of the world. In
this work, we make parts of this counterargument formal by using ideas from the theory of
coalgebras, an abstract mathematical framework for dynamical systems that brings together work
from automata theory, concurrency theory, probabilistic processes and other fields. To do so, we cast
generative model and process, in the form of POMDPs, as coalgebras, and use maps between them
to describe a form of consistency that goes beyond mere structural similarity, giving the necessary
mathematical background to describe how different processes can be seen as behaviourally, rather
than structurally, equivalent, i.e. how they can be seen as emitting the same observations, and
thus minimise prediction error, over time without strict assumptions about structural similarity. In
particular, we will introduce three standard notions of equivalence from the literature on coalgebras,
evaluating them in the context of predictive processing and identifying the one closest to claims
made by proponents of this framework.
Keywords: active inference, action-oriented models, coalgebras, bisimulations, behavioural
equivalence
1. Introduction
Predictive processing, and its more general formulation known as active inference, have been proposed
as a general computational theory to account for the functions of the nervous system [ 1, 2]. The
proposal’s key claim is that one can understand brain activity in its various forms and manifestations
as resulting from the single imperative of minimising (variational) free energy [ 3]. Thus, active
inference promises to be a unified account of cognition and sentient behaviour, explaining in particular
how key cognitive functions such as perception, action, and learning all emerge from a single principle,
i.e. free energy minimisation [4, 5, 6, 7, 8, 9, 10].
In this view, the brain is described as a prediction machine [ 11, 6], and every living organism
is thought to be constantly trying to match or predict incoming sensory inputs produced by the
environment, described as a generative process, that are relevant to itself. If portions of the sensory data
†Correspondence e-mail: manuel_baltieri@araya.org
arXiv:2508.16877v1  [q-bio.NC]  23 Aug 2025
A coalgebraic perspective on predictive processing
remain unaccounted for, then prediction errors ensue. Variational inference is a general framework
from probabilistic machine learning that, under certain assumptions, reduces to prediction error
minimisation. Variational free energy is a measure quantifying how much unexpected current sensory
inputs are, with respect to the generative model and its updates, encoded by the nervous system (see
the next section for a more formal treatment of these notions). A generative model can be seen as
an approximate, probabilistic representation of the surrounding environment encoded by an agent,
describing how sensations and observations arise for a certain organism depending on context and
behaviour (actions) [ 12, 13, 14, 1]. By encoding updates consistent with an implicit hierarchical
generative model, an agent’s nervous system is thought to implement predictions, combined with a
set of prior beliefs, about the most likely sensory inputs in a certain environment [15, 16, 17].
But what is the precise relation between the generative process and generative model? In the
literature, this is somewhat unclear: standard treatments of active inference and predictive processing
often invoke a notion akin to structural similarity between the two, based on the idea that an agent
ought to recapitulate the (statistical, or at times causal) structure of the environment [ 7, 18, 19, 15,
16, 20], and that the two must somehow synchronise [ 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. Other
works have on the other hand argued that structural similarity is not necessary, in the sense that
one can formulate action-oriented generative models that do not capture the structural richness of
their respective generative processes [11, 6, 31, 32, 33, 34, 35] (see also [26], stating that “[h]idden and
external states may or may not be isomorphic [. . . ]”).
To better explain the perspective invoked by the latter, largely based on informal accounts [11, 6]
and simulation work [31, 32, 33, 34, 35], we reformulate the intuition behind it using the theory of
coalgebras, treating generative process and models as coalgebras, and exploring their relation in terms
of specific maps between them. Coalgebras are a standard tool used in theoretical computer science
and mathematics (especially category theory) [36, 37] for the treatment of general dynamical systems.
Their structural formulation puts an emphasis on objects (dynamical systems) and maps between
them (homomorphisms) that must satisfy certain conditions. Thanks to their abstract definitions, their
expressive power is quite broad and will allow us to easily bring out examples of growing difficulty
for different transition types (with inputs, outputs or both, terminal states, etc.) and branching in
dynamical systems (deterministic, possibilistic and probabilistic), focusing in the end on the particular
implementations relevant for this work: POMDPs in active inference, and behavioural equivalence
between them.
In Section 2, we provide a structural overview of active inference with a focus on its core working
parts and its applications to perception, i.e. predictive processing. Section 3 offers a short, self-
contained introduction to coalgebras and their maps, with an emphasis on definitions of bisimulation
(equivalence), kernel bisimulation and behavioural equivalence and a few standard examples of their
uses. In Section 4, we bring these two parts together, with POMDPs in active inference formulated as
coalgebras, and their relation as a kind of consistency between coalgebras using the aforementioned
definitions.
2. Predictive processing, an overview
While a full treatment of active inference remains outside the scope of the present manuscript, in this
section we introduce the main motivations behind this framework, which will allow us to formally
discuss its main structural components, showing connections between active inference and the theory
of coalgebras. For some technical treatments and reviews, see e.g. [38, 39, 40, 41, 26, 42, 29].
2
A coalgebraic perspective on predictive processing
Fig. 1: Active inference setup. Brain-body-environment factorisation of an agent.
2.1. The structure of active inference problems
In a standard active inference setup, we have an agent actively interacting with an environment. The
agent can be described as a system factored into two components: a brain and body, a setup typical
of embodied approaches to biology and cognitive science [43, 44], see Fig. 1. Note that in principle,
these need not be a “brain” and a “body” in a strict sense; one can for instance imagine an E. Coli’s
signalling pathway [45, 46] to play the role of a brain in this setup.
Environment Focusing on the discrete-time treatments [ 40, 41, 47, 26, 42, 29], the relevant part of
the environment with which an agent interacts, and which generates observations coming through its
sensors, is referred to as generative process. A generative process is usually formulated as a partially
observable Markov decision process [48, 49, 50], and represents the ground truth for an agent:
Definition 1 (Partially observable Markov decision process (POMDP)). A partially observable Markov
decision process is a tuple (S, A, T, O, M), where:
• S is the state space,
• A is the action space,
• T : S × A → P(S) is the transitions dynamics, where P(S) is the set of distributions over S with
finite support such that for a given state st and at, T(st, at) gives a probability distribution of
states an agent can transition to from statest while taking action at, often written as p(st+1 | st, at),
• O is the observation space,
• M : S → P(O) is the observation map where P(O) is the set of distributions over O with finite
support such that for a given state st, M(st) gives a probability distribution on observations p(ot).
Note that this corresponds to a “POMDP without rewards”. A more general definition of POMDP
includes in fact:
• γ ∈ [0, 1) is a discount factor,
• r : S × A → R, a map giving a reward every time a transition is taken.
3
A coalgebraic perspective on predictive processing
This is because generative processes in active inference do not include a reward function (and
thus no discount factor either). One could say that rewards are effectively folded into O, seen as
observation-reward pairs O := O′ × R. However, it is standard practice in active inference to avoid
reward functions, in favour of priors on (desired) outcomes/observations [51, 52, 53, 54, 47, 29]. See
also [55] where, under the assumption that an agent has access to preferences that encode exactly
rewarding states, a formulation compatible with standard (PO)MDPs can be derived.
Body The body is often represented as an interface, equivalent to channels that couple the agent
to the environment, in terms of sensors and actuators, forming what is usually referred to as an
action-perception loop or sensorimotor loop, see for instance [56, 57]. In the active inference literature,
these components constitute the so-called “Markov blanket” of an agent [58, 29, 30], depicted in Fig. 1
as a “Friston blanket” instead, following arguments found in [59] (see also [60, 61, 62, 63]).
Brain The role of the brain in this framework is to perform inference about unknown properties
(states, parameters) of the environment. It acts as a prediction machine, as is often highlighted in the
field of cognitive (neuro)science, generating the same observations it receives from the environment.
This is achieved by describing brain states as parametrising distributions of states and parameters
of a generative model, with their dynamics being consistent with belief updates in a Bayesian frame-
work. Generative models are a standard component of (Bayesian) machine learning setups [64, 65],
consisting of a joint probability distribution of observations emitted by the environment, and hidden
variables/parameters that generated them. We note that, under this interpretation, the brain does
not contain, have, or even is a generative model. Instead, brain states and their dynamics implement
a scheme consistent with update equations that could be interpreted [66, 67] as the brain implicitly
having such a model [68, 69].
In this context, a generative model consists of another POMDP [ 40, 47, 26, 42, 29], whose goal
is to approximate (and in an ideal, perfect model scenario, to match) in some sense the generative
process, another POMDP . This particular POMDP ought to have the same interface as the generative
process POMDP . According to active inference, an agent’s goal is for its generative model to have the
same inputs, i.e. actions generated for the environment, either by having a copy of them (efference
copy) or by inferring them (as in more standard active inference setups, see [10] and Fig. 1), and the
same outputs, i.e. predictions of observations that minimise variational free energy/prediction error
(see Fig. 1).
This setup is consistent with model-based approaches to reinforcement learning [ 50] since the
agent operates on the assumptions induced by the generative model. Computationally, however, the
goals and algorithmic implementations of these two approaches are usually quite different, although
more recently converging to similar ideas [70, 71, 54, 72, 55, 73], mainly the free energy (and expected
free energy) minimisation.
2.2. Free energy minimisation and predictive processing
One of the main goals of an active inference agent is to infer the most likely environment’s config-
uration given a sequence of observations. The task is formalised in terms of approximate Bayesian
inference via variational Bayes. To see the core idea, consider the application of Bayes’ rule to a
POMDP within the active inference framework:
P(S1:T, π, A, B|O1:T) =P(O1:T|S1:T, π, A, B)P(S1:T, π, A, B)
P(O1:T) , ( 1)
4
A coalgebraic perspective on predictive processing
where π is a policy, while A, B are parameters for observation and transition maps, respectively. In
general, this cannot be solved analytically. Therefore, in active inference, the update of the agent’s
probabilistic beliefs is performed via an optimisation procedure involving a quantity called variational
free energy. Variational free energy is defined in terms of a probability distribution known as the varia-
tional or approximate posterior, Q(S1:T, π, A, B), which approximates the true posterior in equation 1,
giving the following:
F

Q(S1:T, π, A, B)
 := EQ
h
log Q(S1:T, π, A, B) − log P(O1:T, S1:T, π, A, B)
i
. ( 2)
It can be shown that minimising variational free energy with respect to the parameters of those
approximate posteriors is equivalent to performing approximate Bayesian inference. As a result, the
variational posteriors become more aligned with the exact posteriors given the generative model that
we were aiming to compute in the first place. This is expressed more concisely by the following:
DKL

Q(S1:T, π, A, B)
 P(S1:T, π, A, B|O1:T)

= F

Q(S1:T, π, A, B)

+ log P(O1:T), ( 3)
showing that minimising variational free is equivalent to minimising the KL divergence between the
variational posterior and the exact (analytical) posterior, at least up to another term: the negative
surprisal, log P(O1:T). We note that the surprisal −log P(O1:T) can play different roles depending on
the setup. In (machine) learning and perception, it is assumed to be constant, given by a fixed set
of observations O1:T that do not change over time. However, when action is introduced, combining
perception and action as in active inference, the surprisal itself can change by selectively sampling
observations O1:T through the choice of particular action policies. In this work, we will focus on
predictive processing, which primarily involves perception and learning processes in which the
surprisal remains constant over time [5, 11, 74, 75], and leave action to future work.
In contrast to the full expression of equation1, free energy is a quantity that can be evaluated, given
the specification of a generative model and the variational posterior, thus allowing an agent to use
collected data/observations to update its probabilistic beliefs. Equation (3) highlights an important
aspect of this variational formulation: the starting point is the minimisation of the KL divergence
between the approximate and real posteriors, which is achieved via minimising free energy under the
assumption that the surprisal is assumed is fixed. However, this does not yet specify what kind of
relation ought to be in place between the (implicit) generative model encoded by the brain, and the
generative process representing the ground truth of the environment.
In particular, minimising the free energy in Eq. (2) only implies that the approximate posterior
ought to be as close as possible to the true posterior obtained from the generative model given
observations O1:T. How do we ensure then that this process of approximate Bayesian inference is, in
some sense, veridical with respect to the underlying generative process, i.e. that a generative model
is appropriate for a generative process? In other words, how do we know that an agent minimising
variational free energy will somehow be successful in a given the environment?
To answer this, we start with a simple observation: if a generative model is in some meaningful
way wrong, then free energy cannot be minimised to a satisfactory level. This means that repeated
attempts to implement processes of perception, learning, planning, and action selection, combined
in a sensorimotor loop that aims to minimise free energy across time scales, are not sufficient for
an agent to fulfil its preferences. Active inference thus posits that further strategies should be in
place in such cases, including, for instance, model selection [76] and structure learning [77], so that a
better generative model can be obtained. The former refers to selecting a generative model from a
5
A coalgebraic perspective on predictive processing
pre-defined space of possible models that better match the requirements of a “good” generative model,
one that allows an agent to fulfil its goals. The latter involves combining expansion and reduction
processes, which respectively create and remove variables in a generative model until it can more
correctly be used to achieve a certain goal.
While the technical details are not particularly relevant in this work, the key message is that a
generative model is only as good as the performance it enables an agent to achieve in the pursuit of its
preferences. In other words, the generative model must be close enough to the generative process to
capture the relevant aspects of the world that affect the agent’s ability to realise its preferences. Only
then does it make sense to speak of minimising free energy for perception, learning, planning, and
action selection. Next we examine how the predictive processing literature has interpreted this idea of
close enough in different ways.
2.3. Synchronisation of generative model and generative process?
There is some consensus in the active inference and predictive processing literature on the idea that
the generative model of the agent recapitulates the (statistical, or at times causal) structure of the
generative process of the environment it refers to [7, 18, 19, 26, 78]. However, the exact meaning of
this statement, and the extent to which this ought to hold, are not entirely clear [79, 31, 34, 80, 59, 62],
partly due to the ambiguity concerning what constitutes causal structure (see, e.g. [81]), and partly
because of the distinction between internal and external states [59, 82]. In an attempt to formalise this
idea, different works [21, 22, 23, 24, 25, 26, 27, 28, 29, 30] have argued that, under certain assumptions,
mainly the existence of a synchronisation map [83], free energy minimisation entails a synchronisation
between an agent’s internal states and external environment states. This synchronisation can be
regarded as a kind of mirroring between internal and external states.
On the other hand, several works have argued [11, 6] and demonstrated [31, 32, 33, 34, 35] that
this structural synchronisation need not, in fact, hold. One can have generative models, often called
action-oriented in this area of research, that do not recapitulate the structural richness of their respective
generative processes. This is also now largely accepted by standard treatments of active inference,
e.g. [26], stating that “[h]idden and external states may or may not be isomorphic [. . . ]” and that
“an agent uses its internal states to represent hidden states that may or may not exist in the external
world”.
To reconcile these seemingly divergent views, namely, that 1) a generative model and its respective
generative process must synchronise, and that 2) the structure of the generative model can differ from
that of the generative process, we take a perspective dual to the structural one: a perspective that puts
behaviour first. The term “behaviour” tends to assume different connotations in different fields. In
psychology, for instance, it is often associated with behavioural research, focusing on the observable
behaviour of a subject, its conditioning, and its interactions with the environment, in contrast to
cognitive approaches that emphasise internal processes such as cognition and emotion. In this work,
we do not engage with this kind of debate, instead, we operationalise observable behaviour as outputs
of a system over time.
This operationalisation stems from the well-known duality between structure and behaviour in
theoretical computer science and mathematics [36, 37], where algebras are taken as a language best
suited to describe structure, while coalgebras as a language for the behaviour of systems. While this is
by no means the only way to conceptualise systems and behaviours, it is a convenient approach that
clearly highlights how to build a behavioural understanding of a system, starting from structural (i.e.
algebraic) notions and adapting them (reversing arrows) by categorical duality [84]. In the next section,
6
A coalgebraic perspective on predictive processing
we will provide a brief overview of coalgebras, and their role as a general language for state-based
systems. This includes both transition type (dynamics and their possible effects such as termination,
outputs, and inputs) and branching (e.g. deterministic, probabilistic, or possibilistic systems) within
the same framework in a convenient and formal way [85].
3. Categories, coalgebras and bisimulations
In this section we provide a brief overview of categorical ideas that lead to an abstract treatment of
dynamical systems and maps between them. This framework will later allow us to apply existing
notions of behavioural consistency to processes of various kind, focusing in our particular setup on
POMDPs. Throughout this work, whenever we refer to a “system” we do so under the assumption
that a system is a coalgebra. While there are other doctrines and theories of systems based on various
other definitions, see for instance [86, 87, 88], these are beyond the scope of the current work and will
not be covered here.
3.1. Categories and functors between them
The first crucial idea involves the definition of category, an abstract collection of objects, and maps
between them that are required to satisfy certain rules [84, 89].
Definition 2 (Category). A category C consists of:
• a class of objects, ob (C), e.g. A, B, C, . . . ,
• a class of maps, arrows or morphisms, hom(C), between objects (sources and targets), e.g.
f , g, h, . . . , usually represented asf : A → B, g : B → C, h : C → D, . . . ,
• for each object of ob (C), an identity morphism idA : A → A,
• a binary operation # representing the composition of morphisms, e.g. given the morphisms f , g
above, f # g = A → C, that satisfies the following1,
– associativity, given f , g, h above, f #(g # h) = (f # g) # h,
– left and right unit laws, for every pair of objects A, B and morphism f : A → B, idA # f =
f = f # idB.
Of particular interest for this work then, as well as for several other results in the field of category
theory, is the concept of functor, a “categorification” (the process of replacing sets with categories to
generalise set-theoretic definitions) of the standard notion of function between sets.
Definition 3 (Functor). Let C and D be categories. A (covariant) functor F from C to D is a mapping
that
• associates each object X in C to an object F(X) in D,
• associates each morphism f : X → Y in C to a morphism F( f ) : F(X) → F(Y) in D such that
the following two conditions hold:
– F(idX) =idF(X) for every object X in C (functors must preserve identity),
– F( f # g) = F( f ) # F( f ) for all morphisms f : X → Y and g : Y → Z in C (functors must
preserve composition of morphisms).
1Note that ◦ is also often use for composition, with f # g = g ◦ f , we decided however to adopt # as we think it helps
following the order of a composition.
7
A coalgebraic perspective on predictive processing
For the theory of coalgebras, which we briefly overview next, we need to consider a special case of
functor, called an endofunctor, i.e. a functor from a category C to itself.
3.2. Processes as coalgebras
Coalgebras, a construction from category theory related to algebras (which are their dual in a categori-
cal sense [84]), have in recent years become a popular approach for the study of general dynamical
systems and automata [36, 37]. Technically, given a category C and an endofunctor F : C → C, we
define an F-coalgebra (or simply coalgebra, when F is understood) as an object S of C together with
a map f S : S → F(S), represented as (S, f S), where F is the type or signature of the coalgebra, S
is the carrier, and f S is the (transition) structure map of the coalgebra. Given a category C and an
endofunctor F, we can build a category of coalgebras with F-coalgebras of the form (S, f S) as objects,
and morphisms between them as maps.
Definition 4 (Category of F-coalgebras). CoalgC(F) is the category of F-coalgebras with objects
F-coalgebras and maps called F-homomorphisms, coalgebra homomorphisms or simply homomor-
phism when the context allows it. Given objects (S, f S), (S′, f S′
) in CoalgC(F), an F-homomorphism
ϕ is a map that makes the following diagram commute:
S S′
F(S) F(S′)
f S
ϕ
f S′
F(ϕ)
(4)
i.e. such that f S # F(ϕ) =ϕ # f S′
. The identity is given by the trivial structure map S
idS
− →S between the
underlying sets. Composition is defined by placing commuting squares side by side, and associativity
is established by verifying that the order in which they commute is not relevant.
In this work, we focus exclusively on coalgebras where the base category is C = Set, the category
whose objects are sets and whose morphisms are functions. Accordingly, rather than using the some-
what bloated CoalgSet (F), we will simplify the notation to Coalg(F) for the category of coalgebras
for the endofunctor F on the category Set. As our treatment largely relies on pre-existing knowledge
and intuitions about sets and functions, most other technical details necessary for a full categorical
account of coalgebras will be skipped.
To develop a more intuitive understanding of coalgebras for the purpose of this work, i.e. discrete
dynamical systems, we next introduce a few standard examples and definitions presented within this
framework. We initially follow introductory treatments at first [36, 90], and later refer to [91] for an
example specifically relevant to the present work (i.e. involving probabilities).
3.2.1. Deterministic systems as coalgebras
Deterministic systems constitute the simplest class of dynamical systems. While these systems have a
trivial branching (i.e. only one possible next state), their transition types can follow different rules.
Here we consider both closed and open systems as illustrative examples of how different interfaces
can be represented.
8
A coalgebraic perspective on predictive processing
Example 5 (Category of closed systems as coalgebras) . The category of closed systems as coalgebras
Coalg(Id), with coalgebra type given by the identity functor Id : Set → Set, has
• as objects, coalgebras of the form (S, f S : S → S), and
• as morphisms between two coalgebras (S, f S) and (S′, f S′
), homomorphisms in the form of functions
ϕ : S → S′ that make the following diagram commute
S S′
S S′
f S
ϕ
f S′
ϕ
(5)
Note that “closed” here refer to the fact that these systems have no inputs/outputs, that is, they
are autonomous (no inputs) and have no observable outputs, and thus cannot communicate with the
outside world (i.e. their interfaces are trivial). This, however, does not prevent us from describing
maps that track consistent updates between different closed systems, maps that preserve transitions,
i.e. (F-)coalgebra homomorphisms.
On the other hand, Moore machines are classical architectures in the automata theory literature,
corresponding to systems with inputs and outputs, or open discrete-time dynamical systems. Formally,
a Moore machine is represented as a quintuple with states, inputs, outputs, transition, and output
functions (S, I, O, β : I × S → S, δ : S → O).
Example 6 (Category of Moore machines as coalgebras). The category of Moore machines as coalgebras
Coalg(Moore), with coalgebra type given by the functor Moore : Set → Set such that Moore(S) =O × SI,
has
• as objects, coalgebras of the form (S, f S
Moore : S → O × SI ), where SI is the set of all functions I → S (see
below for some explanation of this notation), with transitions f S
Moore = ⟨outS
Moore, trS
Moore⟩ given by
outS
Moore : S → O
trS
Moore : S → SI, ( 6)
and
• as morphisms between two coalgebras (S, f S
Moore) and (S′, f S′
Moore) with the same inputs and outputs,
coalgebra homomorphisms in the form of functions ϕ : S → S′ (since inputs and outputs are the same
for the two systems, there are simple identity functions between them) that make the following diagram
commute
S S′
O × SI O × S′I
f S
Moore
ϕ
f S′
Moore
idO×(ϕ)I
(7)
The notation for the set of all functions I → S, SI = {f | f : I → S}, implies that trS
Moore sends an element
s ∈ S to a function f = trS
Moore(s) : I → S assigning a next state ˜s := trS
Moore(s)(i) ∈ S to each input i ∈ I,
9
A coalgebraic perspective on predictive processing
see [90]. Importantly, as noted by [36] and references therein, there is a bijection {h | h : I × S → S} ∼= {g |
g : S → SI } (cf. “currying”) 2, and therefore {h | h : I × S → O × S} ∼= {h | h : S → O × SI }. 3
3.2.2. Non-deterministic systems as coalgebras
Nondeterministic automata provide an example of nondeterministic open discrete-time dynamical
systems. These systems have a non-trivial branching: for each state, there is a set of possible next states,
and this set can be empty, meaning that from a state there are no transition allowed. Their transition
types can also be of different kinds (closed, open, with or without a terminal state, etc.), but here we
focus exclusively on open systems (Moore machines) that include final states (states from which there
are no possible transitions). This example is also relevant for the next section, where we will introduce
yet another type of branching: a probabilistic one. All these examples can be viewed as special cases
of “structured Moore machines” [92], where the transition type (with inputs and outputs, specified by
a fixed functor, F) is the same, while the branching (explained in terms of a monad, T) can be different.
To define possibilistic Moore machines, we recall the definition of the finite powerset functor.4
Definition 7 (Finite powerset functor). The finite powerset functor, Pfin : Set → Set is defined as
Pfin(X) ={U ⊆ X | U is finite}. ( 8)
For a function g : X → Y, the (pushforward) map Pfin(g) : Pfin(X) → Pfin(Y) is defined as
Pfin(g)(a) =g[U], ( 9)
where g[U] ={g(a) | u ∈ U}.
Using this functor, we can give the following definition.
Example 8 (Category of nondeterministic Moore machines as coalgebras) . The category of nonde-
terministic Moore machines as coalgebras Coalg(PfinMoore), with coalgebra type given by the functor
PfinMoore : Set → Set such that PfinMoore(S) =Pfin(O) × Pfin(S)I, has
• as objects, coalgebras of the form (S, f S
PfinMoore : S → Pfin(O) × Pfin(S)I ), with transitions f S
PfinMoore =
⟨trS
PfinMoore, outS
PfinMoore⟩ given by
outS
PfinMoore : S → Pfin(O)
trS
PfinMoore : S → Pfin(S)I, ( 10)
and
• as morphisms between two coalgebras (S, f S
PfinMoore) and (S′, f S′
PfinMoore) with the same inputs and outputs,
coalgebra homomorphisms in the form of functions ϕ : S → S′ (since inputs and outputs are the same
2Note that maps I × S → S are not of type S → F(S), while maps of type S → SI are, and so are well defined coalgebras.
3This is true for Moore machines, but not for all kinds of open systems, e.g. Mealy machines where O also depends on I.
4The finite version of this functor has especially nice properties [37], and is in practice more often adopted in place of its
infinite counterpart.
10
A coalgebraic perspective on predictive processing
for the two systems, there are simple identity functions between them) that make the following diagram
commute
S S′
Pfin(O) × Pfin(S)I Pfin(O) × Pfin(S′)I
f S
PfinMoore
ϕ
f S
PfinMoore
Pfin(idO)×Pfin(ϕ)I
(11)
We note that this formulation is closely related to the idea ofpossibilistic [93] systems, defined for
the nonempty powerset monad given by Pfin
+(X) = Pfin(X) \ {∅}, which exclude the case where
there can be no possible transition from a given state.
3.3. Comparing processes
A core feature of the coalgebraic approach to dynamical systems is its focus on maps between systems,
placing them at the center of the theory’s development. These maps are relevant, for instance, in the
analysis of concurrent processes in theoretical computer science [36, 94], where a primary goal is to
define notions of equivalence for processes based on their observable behaviour, contrasting with
standard concepts of equivalence based on structural similarity of different processes [95]. They are
also of interest in other fields, where they appear in specialised forms such as “homomorphisms” [96,
97], “coarse-grainings” [98], “variable aggregation” [99], “state aggregation” [100], “lumpability” [101],
“model reduction” [102] or “dynamical consistency” [103], among others.
These specialised maps can be traced back to the idea of a model, in particular, to what it means
for a system to model another system. The standard definition of model relies on epimorphisms,
a generalisation of surjectivity for functions between sets. In the case of coalgebras, it tells when a
coalgebra can be specified as a coarse-graining of another.
Definition 9 (Models in coalgebras). Let Coalg(F) be a category of coalgebras. A homomorphism
of coalgebras ϕ : S → S′ from (S, f S : S → F(S)) to (S′, f S′
: S′ → F(S′)) is an epimorphism in the
category Coalg(F) if and only if ϕ is a surjective function between the underlying sets [ 37, Theorem
3.3.4]. Following the convention adopted in [68], an epimorphism of coalgebras ϕ is called a model,
whereas (S, f S) is called the referent, i.e. what the model refers to, and (S′, f S′
) the referrer, i.e. what
refers to the model.
Intuitively, this definition states that different elements of the first coalgebra,(S, f S), are mapped to
the same element of the second coalgebra, (S′, f S′
). More precisely, the existence of a modelϕ : S → S′
implies that for each state s′ ∈ S′, there exists a set of states ϕ−1(s′) ∈ S of the referent (S, f S), called
the fibre of s′, which represents a subset of elements of S that are indistinguishable from the perspective
of the simpler coalgebra, the referrer (S′, f S′
), as they all map to the same element s′ ∈ S′ via the
surjective function ϕ. Furthermore, as s′ ∈ S′ varies along the transition function f ′, this variation is
consistent with the variation described by the function f for each element s of the fibre ϕ−1(s′) of s′.
An equivalent definition, see [37, Theorem 3.3.4], can be given via the use ofbisimulation equivalences,
building on the concepts of spans and relations.
Definition 10 (Spans and relations between sets). A span between the setsX and Y is a triple (V, p1, p2)
where V is a set and p1 : V → X and p2 : V → Y are two functions with the same domain, V. The
pair (x, y) ∈ X × Y is related by (V, p1, p2) if there exists a v ∈ V such that p1(v) =x and p2(v) =y.
11
A coalgebraic perspective on predictive processing
A relation R is a jointly monic span, i.e. a span where p1, p2 are jointly a monomorphism (an injective
function in Set), R
(p1,p2)
− − − →X × Y, given by v 7→ (p1(v), p2(v)), or in other words, given any two
functions f , g : W → V, f # p1 = g # p1 and f # p2 = g # p2 imply that f = g.
Definition 11 (Bisimulation equivalence) . Given an F-coalgebra (S, f S) an equivalence relation
B ⊆ S × S is said to be an F-bisimulation equivalence [36] if there exists an F-coalgebra structure
γB : B → F(B) such that the following diagram commutes
S B S
F(S) F(B) F(S)
γB
πS
f S
F(πS)
πS
F(πS)
f S ∃ (12)
i.e. such that the projection πS is an coalgebra homomorphism. More generally, a bisimulation
equivalence is at times defined as a span of coalgebras, i.e. a span (B, πS, πS) between the underlying
sets, S and S (itself), that makes the above diagram commute [37].
To get an intuition for how this relates to the modelling perspective of Definition 9, recall that
a model is an epimorphism of coalgebras, i.e. a surjective function between the underlying sets of
two coalgebras (S, f S) and (S′, f S′
). Recall also that every surjective function f : A → B induces an
equivalence relation on A, R ⊆ A × A and conversely, every equivalence relation R on A induces a
quotient mapping fR : A → A/R, which is surjective, where A/R is the quotient set with elements
equivalence classes of elements of A. In this sense, an equivalence relation is to a surjective function
what a bisimulation equivalence is to a coalgebra epimorphism: a bisimulation equivalence is an
equivalence relation B on S with extra structure, i.e. an equivalence relation that preserves coalgebra
transitions, and a coalgebra epimorphism is a surjective function between the underlying sets of two
coalgebras, S and S′, with extra structure, i.e. a surjective function that preserves coalgebra transitions.
This interpretation has recently received increasing attention in machine and reinforcement learning,
see for instance [104, 105, 97], where bisimulation equivalences are simply referred to as bisimulations.
Following [106, 107, 37], we extend the above definition to relations (not equivalence relations) of
the form B ⊆ S × S′ between different sets of states S and S′ and build a definition of bisimulation
between different processes formalised as coalgebras.
Definition 12 (Bisimulation). Given two F-coalgebras (S, f S), (S′, f S′
), a relation B is said to be an
F-bisimulation [36] between (S, f S) and (S′, f S′
) if there exists an F-coalgebra structure γB : B → F(B)
such that the following diagram commutes
S B S′
F(S) F(B) F(S′)
γB
πS′
f S′
F(πS′ )
πS
F(πS)
f S ∃ (13)
i.e. such that πS, πS′ are coalgebra homomorphisms. Again, we can generalise this definition to state
that a bisimulation is a span of coalgebras.
12
A coalgebraic perspective on predictive processing
Unlike the case of bisimulation equivalences, i.e. equivalence relations of type R ⊆ A × A
that correspond to models and coalgebra epimorphism given in Definition 9, for relations of type
R ⊆ A × B the correspondence is less obvious. This is primarily because relations of type R ⊆ A × B
do not simply induce a surjective function, see also Discussion and [108].
An alternative approach to understanding the more general notion of bisimulation is through
the lenses of behavioural equivalence. However, to establish a rigorous notion of behavioural equiva-
lence, we first need to define cocongruences, kernel bisimulations and final coalgebras. Although in
some other works, “behavioural equivalence”, “cocongruence” and “kernel bisimulation” are used
interchangeably, here we adopt the formal characterisation given in [109], which distinguishes these
notions by, roughly, stating that behavioural equivalence is a kernel bisimulation that uses a final
coalgebra, and kernel bisimulation is a cocongruence with an associated (pullback) relation. See
also [110, Chapter 3.5] for a detailed breakdown.
The definition of cocongruence turns out to be equivalent to that of bisimulation in our setup, i.e.
using only weak pullback-preserving functors on Set as the base category [111] and [37, Theorem
4.5.3]. Cocongruences in [112] (or behavioural equivalence in [37]) build on the dual5 of a relation (a
so-called “corelation”) and more generally, the dual of a span (a “cospan”).
Definition 13 (Cospans and corelations between sets). A cospan between the sets X and Y is a triple
(U, i1, i2) where U is a set andi1 : X → U and i2 : Y → U are two functions with the same codomain,U.
The pair (x, y) ∈ X × Y is identified by (U, i1, i2) if i1(x) =i2(y). A corelation C is a jointly epic cospan,
i.e. a cospan where i1, i2 are jointly an epimorphism (a surjective function inSet), X + Y
(i1,i2)
− − − →C, given
by x 7→ i1(x) and y 7→ i2(y), or in other words given any two functions h, k : U → T, i1 # h = i1 # k and
i2 # h = i2 # k imply that h = k.
Definition 14 (Cocongruence). Given two F-coalgebras (S, f S), (S′, f S′
), a corelation C is said to
be a cocongruence [ 112, 110] between (S, f S) and (S′, f S′
) if there exists an F-coalgebra structure
γC : C → F(C) such that the following diagram commutes:
S C S′
F(S) F(C) F(S′)
rS
f S γC∃
rS′
f S′
F(rS) F(rS′ )
(14)
i.e. such that rS, rS′ are coalgebra homomorphisms. More generally, cocongruence can be characterised
as a cospan of coalgebras, with the above as a special case.
A kernel bisimulation is, in this context, a relation R associated, when it exists, to a cocongruence.
Definition 15 (Kernel bisimulation). Given a cocongruence between twoF-coalgebras (S, f S), (S′, f S′
),
a relation R ⊆ S × S′ is a kernel bisimulation if it is a pullback of the cospan S → C ← S′, i.e. if there
5Formal duality, in the sense of category theory, i.e.arrow reversal.
13
A coalgebraic perspective on predictive processing
exist morphisms sS : R → S and sS′ : R → S′ such that the following diagram commutes:
R
S C S′
F(S) F(C) F(S′)
sS sS′
rS
f S γC∃
rS′
f S′
F(rS) F(rS′ )
(15)
In a category of coalgebras for a functor preserving weak pullbacks, as is the case for all the functors
considered in this work, with a base category with pullbacks (such as Set), cocongruence and kernel
bisimulations imply each other, i.e. every cocongruence has an associated kernel bisimulation.
A final coalgebra, when it exists, is the final or terminal object in a category Coalg(F).
Definition 16 (Final coalgebra). An F-coalgebra (Ω, ωΩ) is final in the category Coalg(F) if for any
F-coalgebra (S, f S) there exists a unique F-homomorphism behS : (S, f S) → (Ω, ω). Graphically, this
is equivalent to the following diagram commuting:
S Ω
F(S) F(Ω)
behS
f S ωΩ
F(behS)
(16)
A final coalgebra is said to capture the behaviour of a coalgebra [ 36, 37]. More precisely, the
elements of a final coalgebra (when it exists) are the possible observable behaviours of all objects
(including itself) of a given category of coalgebras. We recall from Section 2.3 that, in the theory of
coalgebras, behaviour was informally defined as “outputs of a system over time”. In coalgebraic
terms, however, the rigorous definition of behaviour is more complicated, and depends strictly on the
type of functor used to build coalgebras: behaviours could correspond to traces (repeated applications
of the coalgebra transition map), trees, distributions, etc. For the purposes of this paper, we will only
consider a couple of standard examples relevant to predictive processing in Section4, and refer the
reader to standard treatments such as [36, 37] for a more in depth discussion of final coalgebras and
their semantics.
Example 17. For the category of closed systems in Example 5, the final coalgebra is trivial, it is the one element
set {·} since all systems look the same from an external perspective, i.e. nothing can be observed because these
systems have no outputs.
Next, we look at the case of deterministic Moore machines.
Example 18. Let Coalg(Moore) be the category of Moore machines as coalgebras from Example 6. The final
coalgebra in Coalg(Moore) is given by
(OI∗
, ωOI∗
: OI∗
→ O × (OI∗
)I, ( 17)
14
A coalgebraic perspective on predictive processing
where the notation ∗ is used to represent lists [ 36, 37]. Here, the carrier of the final coalgebra is OI∗
,
and its elements ∈ OI∗
can be understood by defining state transitions from any initial state s0 ∈ S
for a given list of actions of arbitrary length n, ⟨i1, . . ., in⟩ ∈ I∗. Using these, we can take n steps
from the initial state s0, i.e. trMoore(. . .trMoore(s0)(i1)) . . .)(in), and obtain an observation for each list
outMoore(. . .trMoore(s0)(i1)) . . .)(in) [37, Sec. 2.2.3], thus obtaining trees rooted in some initial state s0 with
inputs as edges between nodes represented by the possible outputs given those edges.
We now combine the definitions of kernel bisimulation, which can be built from all cocongruences
in categories of coalgebras using only weak pullback-preserving functors on Set as the base category,
and final coalgebra to obtain the following [109].
Definition 19 (Behavioural equivalence ). Given two F-coalgebras (S, f S), (S′, f S′
), behavioural
equivalence between them is a kernel bisimulation R (Definition 15) for a cocongruence (Ω, γΩ : Ω →
F(Ω)) (Definition 14) where (Ω, γΩ) is the final coalgebra (Definition 16).
Two states s ∈ S, s′ ∈ S′ are behaviourally equivalent, i.e. (s, s′) ∈ R, if for rS : S → C, rS : S′ → C
(see Definitions 14 and 15), rS(s) =rS′ (s′) (see [37, Theorem 3.3.3] for polynomial functors and its
generalisation to include the finite powerset functor and the distribution functor [37, Theorem 4.5.3]).
In the next section, we will use behavioural equivalence to provide a structural description of the
core parts of predictive processing, and free energy minimisation, under a coalgebraic framework. As
we shall see, given the level of abstraction we reached, we will only need to apply a few minor changes
to definitions introduced above to understand the relation that ought to be in place between generative
process and generative model for a successful predictive processing agent. Although we do not focus
on any specific algorithmic implementation, our final discussion will provide a connection to existing
work on both exact and approximate bisimulations, showing how some of the ideas introduced in the
next section could be implemented in future work.
4. Predictive processing in coalgebraic terms
4.1. Generative model and generative process as coalgebras
In Section 2.1, we saw that, in the predictive processing literature, the terms generative process and
generative model have been used as labels for probabilistic processes that represent the ground truth
of the environment, and a model of it whose updates are encoded in the agent’s brain states, respec-
tively [26, 35, 29, 47]. These probabilistic processes are usually presented as POMDPs (see Definition 1),
and in the literature on coalgebras, correspond to a probabilistic version of Moore machines (cf. Exam-
ple 6). As in the case of Example 8, these constitute another example of branching (probabilistic) given
the same transition type (with inputs and outputs) of structured Moore machines [92]. To define them,
we first recall the following.
Definition 20 (Distribution functor). The distribution functor for discrete probability, P : Set → Set
is defined as:
P(X) =
(
p : X → [0, 1]
supp(p) is finite and ∑
x
p(x) =1
)
, ( 18)
where [0, 1] ⊆ Ris the unit interval of real numbers, andsupp(p) ⊆ X is the support of the distribution,
i.e. the finite subset of x ∈ X where p(x) ̸= 0. For a function g : X → Y the map P(g) : P(X) → P(Y)
15
A coalgebraic perspective on predictive processing
(the pushforward of p along g) is defined, for any distribution p ∈ P(X) and any element y ∈ Y, as:
P(g)(p)(y) = ∑
x∈g−1(y)
p(x) =∑
x
{p(x) | x ∈ supp(p) with g(x) =y}. ( 19)
Using this, we can define probabilistic Moore machines and interpret them as POMDPs in coal-
gebraic terms. Note that we adopt a common simplifying assumption stating that observations and
transitions to the next state are independent. For more general treatments not involving coalgebras,
see for instance [67, 113, 97].
Definition 21 (Category of POMDPs as coalgebras (probabilistic Moore machines in [ 91])). The
category of POMDPs as coalgebras Coalg(POMDP), with coalgebra type given by the functor
POMDP : Set → Set such that POMDP(S) =P(O) × P(S)A, has
• as objects, coalgebras of the form (S, f S
POMDP : S → P(O) × P(S)A), with transitions f S
POMDP =
⟨outS
POMDP, trS
POMDP⟩ given by
outS
POMDP : S → P(O)
trS
POMDP : S → P(S)A, ( 20)
and
• as morphisms between two coalgebras (S, f S
POMDP) and (S′, f S
POMDP) with the same inputs/ac-
tions and outputs/observations, coalgebra homomorphisms in the form of functions ϕ : S → S′
(since inputs and outputs are the same for the two systems, there are simple identity functions
between them, indicated by idO for observations, and by having the same A on both coalgebras
for actions) that make the following diagram commute:
S S′
P(O) × P(S)A P(O) × P(S′)A
f S
POMDP
ϕ
f S′
POMDP
P(idO)×P(ϕ)A
(21)
Similarly, we define the category of MDPs below, following in this case previous work [114], but
without including explicit rewards.
Definition 22 (Category of MDPs as coalgebras). The category of MDPs as coalgebras Coalg(MDP),
with coalgebra type given by the functor MDP : Set → Set such that MDP(S) =P(S)A, has
• as objects, coalgebras of the form (S, f S
MDP : S → P(S)A)6, and
• as morphisms between two coalgebras (S, f S
MDP) and (S′, f S′
MDP) with the same inputs, coalgebra
homomorphisms in the form of functions ϕ : S → S′ (since inputs are the same, there is once
6Notice how the relation between partially and fully observable MDPs appears: f S
MDP is of the same type as trS
POMDP.
16
A coalgebraic perspective on predictive processing
again a simple identity between them) that make the following diagram commute:
S S′
P(S)A P(S′)A
f S
MDP
ϕ
f S′
MDP
P(ϕ)A
(22)
4.2. Comparing generative process and generative model: predictive processing as be-
havioural equivalence
In Section 2.3 we saw how the literature on active inference and predictive processing contains
several claims that the process of minimising variational free energy, used to perform approximate
Bayesian inference on the environment’s states that generate sensory inputs, can be understood in
terms of a “synchronisation” between generative model and generative process. This means that, for a
particular task, the dynamics of a generative model, implicitly encoded by brain states representing
(approximate) Bayesian updates given observations over time, becomes a model, ideally a perfect
one, of the generative process. Here, we provide three candidate, formal definitions of this idea
corresponding to three particular forms of behavioural equivalence between generative model and
generative process. We will discuss their implications and possible shortcomings, focusing in the
end on what we believe to be the best candidate to reflect a relation between generative model and
generative process that goes beyond mere structural similarity, consistent with predictive processing.
4.2.1. Comparing POMDPs
To start off, we apply directly the definition of behavioural equivalence given in Definition 19 to
generative process and generative model in the category of POMDPs (Definition 21), which, as we
know, has a final coalgebra, see [ 115, Section 7] and [ 37, Theorem 4.6.9]. As we will see shortly,
this has quite strong and perhaps undesirable implications, which are nevertheless important to
discuss. In what follows, we will make extensive use of Definition 19, but without visualising the
relation R (the kernel bisimulation) in our diagrams, since its existence is always implied by our setup,
see Definition 15.
Definition 23 (Behavioural equivalence of POMDPs). We apply Definition 19 for F = POMDP:
S Ω S′
P(O) × P(S)A P(O) × P(Ω)A P(O) × P(S′)A
f S
POMDP
behS
f Ω
POMDP
P(idO)×P(behS)A
behS′
f S′
POMDP
P(idO)×P(behS′ )A
(23)
This corresponds to the following conditions (for more details see Appendix A), where for any
a ∈ A and ω ∈ Ω we have:
p(o | s) =p(o | s′) (condition 1)
17
A coalgebraic perspective on predictive processing
∑
˜s∈beh−1
S (ω)
p(˜s | s, a) = ∑
˜s′∈beh−1
S′ (ω)
p(˜s′ | s′, a) (condition 2) ( 24)
This definition states that, given the same actions (by assumption), states s and s′ are behaviourally
equivalent only if they emit the same probabilistic observations (by condition 1), while creating
equivalence classes of indistinguishable ground truth states by considering their probabilistic transi-
tions (by condition 2). We believe this is too strict to properly describe predictive processing in all
of its facets, as this requires probabilistic transitions of (equivalence classes of) ground truth states
of the generative process, s ∈ S, to be equal to probabilistic transitions of (equivalence classes of)
ground truth states of the generative model, s′ ∈ S′, while one of the main points of action-oriented
generative models is that they don’t need to recapitulate the veridical, ground truth structure of the
environment [11, 6, 31, 32, 33, 34, 35].
More generally, a definition of behavioural equivalence between probabilistic processes is notori-
ously non-trivial, and the formulation provided above is not the only possible choice (see e.g. [116] for
a review of this and other possible choices). For probabilistic processes, it is in fact often desirable to
focus on probabilistic properties rather than on characteristics of sampled trajectories from ground
truth states as in Definition 23. We argue that this is also the case for predictive processing, which is
based on the minimisation of the difference between distributions encoded by the generative model
and generative process, rather than minimising the difference between trajectories sampled from
them.
4.2.2. Comparing belief MDPs
Next, we will adapt the definition of belief bisimulation equivalence [117, 118], originally restricted
to a single system (hence the term “equivalence” (see Definition 11), to work between different
processes. In other words, we will define a belief bisimulation. This definition corresponds to a
standard bisimulation, that is, a span of coalgebras (see Definition 12) between coalgebras encoding
beliefs, in a Bayesian sense, as we shall see below, of the original processes. This means that there is
a corresponding notion of belief behavioural equivalence (a corelation or more generally a cospan
of coalgebras, see Definition 19), which once again is implied and implies that of bisimulation by
working with well behaved functors and Set as the base category [115].
To apply the definition of belief behavioural equivalence, we start from a description of belief
MDPs [49] associated to, or rather induced by POMDPs. These are related to the separation principle
of control [ 119] (see also [ 120] for a review of related ideas). Belief MDPs have previously been
formulated in a coalgebraic context in [113, 121], although they are not explicitly presented in terms of
MDPs in those works.
Definition 24 (Belief MDP). A belief MDP induced by a POMDP (S, A, T, O, M) is an MDP (Z, A, TZ)
where:
• Z is the space of belief states, sufficient statistics of histories H := (O × A)∗ × O, given by
z : H → P(S),
• A is the space of actions and coincides with the one from the original POMDP ,
• TZ : Z × A → P(Z) is the belief transitions dynamics, defined for zt, zt+1 ∈ Z and at ∈ A as
TZ(zt, at) =p(zt+1 | zt, at)
= ∑
ot∈O
p(zt+1 | zt, ot+1, at)p(ot+1 | zt, at), ( 25)
18
A coalgebraic perspective on predictive processing
where
p(zt+1 | zt, ot+1, at) =
(
1 if τZ(zt, ot+1, at) =zt+1,
0 otherwise, (26)
for τZ : Z × O × A → Z defined by standard Bayesian filtering updates of beliefs zt = p(st | ht)
for ht ∈ Ht, see [49, 97] and Appendix B.
In a belief MDP , beliefs, probability distributions over the possible states, serve as the states of a
standard MDP .Applying this construction to both the generative model and generative process, given
as POMDPs or probabilistic Moore machines in coalgebraic form (see Definition 21), produces two
belief MDPs, describing the associated probability distributions and their transitions (obtained by
currying, see Example 6):
TZ : Z × A → P(Z) ↔ f Z
MDP : Z → P(Z)A (belief generative process)
TZ′ : Z′ × A → P(Z′) ↔ f Z′
MDP : Z′ → P(Z′)A (belief generative model). ( 27)
Using these, a belief behavioural equivalence between them can be defined as follows:
Definition 25 (Belief behavioural equivalence of belief MDPs). This is a direct application of Defini-
tion 19 (once again, without visualising R for simplicity) for F = MDP:
Z Ω Z′
P(Z)A P(Ω)A P(Z′)A
f Z
MDP
behZ
f Ω
MDP
P(behZ)A
behZ′
f Z′
MDP
P(behZ′ )A
(28)
This coincides with the following definition, i.e. condition 2 in Eq. ( 24) with a different state space:
beliefs on states, rather than states, and without observations due to the Bayesian construction
in Definition 24 (this can be obtained as in Appendix A, with trivial observations, i.e. O = 1), where,
for any a ∈ A and ω ∈ Ω, we have:
∑
˜z∈beh−1
Z (ω)
p(˜z | z, a) = ∑
˜z′∈beh−1
Z′ (ω)
p(˜z′ | z′, a). ( 29)
This condition implies that beliefs z and z′ are behaviourally equivalent if the distributions over
the respective next states, belonging to the same equivalence class represented by ω, are equal given
the same actions, but does not make any statement about observations. This is a consequence of the
definition of belief MDPs (Definition 24), in which observations are marginalised in the definition
of belief updates (see Eq. ( 25)). Such a condition could be relevant in situations where we require
the beliefs of two processes to be equivalent: their beliefs evolve in the same way, while the exact
implementations of these beliefs are not important. It is however problematic for another, crucial
reason: the final coalgebra of the category of MDPs is trivial, i.e.Ω is the one-element set. 7 This means
that belief MDPs are actually not observable in the coalgebraic sense [37], since there is no non-trivial
monomorphism (in our setup, an injective map) into the final coalgebra (since it has only one element).
We thus turn to another approach to describe similarity between the generative model and generative
process.
7The authors would like to thank Nathaniel Virgo for pointing this out, see also [106, 122] for related results.
19
A coalgebraic perspective on predictive processing
4.2.3. Comparing lifted POMDPs
In our final attempt to find a behavioural notion of similarity between the generative model and
generative process, we turn to a generalisation of distribution bisimulation equivalence [123, 92, 124,
125] between processes, i.e. distribution bisimulation. Under the assumptions of this work, this also
yields a notion of distribution behavioural equivalence.
Note that work on this type of equivalence is often grouped with what we have described as belief
bisimulation and behavioural equivalence (see, e.g. [116]). However, we wish to emphasise that, while
both are behavioural equivalences on probability distributions, the compared distributions have a
markedly different semantics. In belief equivalence, the distributions are constructed as Bayesian
beliefs on hidden states of a given process. In distribution bisimulations, by contrast, they correspond
to probability distributions on hidden states that are not necessarily updated using Bayes [92]. While
the determinisation of a POMDP is another POMDP , abelification of a POMDP is a (belief) MDP . We
further note that these are also related to the unifilarisation described in [66], but do not explore here
the details.
We start by recalling the generalised determinisation construction of [ 92], focusing only on
POMDPs.
Definition 26 (Generalised determinisation of POMDPs). Given a POMDP as a coalgebra of type
(S, f S
POMDP : S → P(O) × P(S)A), the generalised determinisation of(S, f S
POMDP) is the lifted coalgebra
of type (P(S), f S
POMDP
♯ : P(S) → P(O) × P(S)A) such that the following diagram commute:
S P(S)
P(O) × P(S)A
ηS
⟨outS
POMDP, trS
POMDP⟩
⟨outS
POMDP
♯, trS
POMDP
♯ ⟩
(30)
or in other words, such that:
outS
POMDP = ηS # outS
POMDP
♯
and
trS
POMDP = ηS # trS
POMDP
♯
, ( 31)
where ηS : S → P(S) is a map8 that, given S, returns the (Kronecker) delta distribution of S, δS, and
the lifted maps9 ⟨outS
POMDP
♯, trS
POMDP
♯⟩ are given, for any w ∈ P(S), a ∈ A, and s, ˜s ∈ S by:
outS
POMDP
♯
(w)(o) =∑
s∈S
w(s) outS
POMDP(s)(o) and
trS
POMDP
♯
(w)(a)(˜s) =∑
s∈S
w(s) trS
POMDP(s)(a)(˜s) (32)
or in a more traditional notation:
p(o | w) =∑
s∈S
w(s) p(o | s) and
8For readers familiar with it, this is the unit of distribution monad, since P is not only a functor but a full fledged
monad [84].
9For readers familiar with it, this is just a Kleisli extension, i.e. given the multiplication of the distribution monad µX and
a morphism g, we have g♯ = P(g) # µX.
20
A coalgebraic perspective on predictive processing
p(˜s | w, a) =∑
s∈S
w(s) p(˜s | s, a). ( 33)
Determinisation allows us to work with a coalgebraic structure in which the transition map,
⟨outS
POMDP
♯, trS
POMDP
♯⟩, is from a space of distributions over states, P(S), to the space of distributions
over the next state and observations, instead of being from the state space S itself. As we will see
next, in this way, we can think of the transition dynamics in the POMDP as a transition from one state
distribution to another. Further, based on this notion of determinisation, we can compare POMDPs in
terms of probability distributions over states and observations, without inducing a belief MDP that
marginalises observations, see the belief transition dynamics in Definition 24. To see this, we proceed
to define the notion of lifted POMDP .
Definition 27 (Lifted POMDP). A lifted POMDP induced by a POMDP (S, A, T, O, M) via determini-
sation [92] is a POMDP (W, A, TW, O, MW ) where:
• W is the space of belief states P(S)10,
• A is the space of actions and coincides with the one from the original POMDP ,
• TW : W × A → W is the belief transitions dynamics, defined for wt, wt+1 ∈ W and at ∈ A as:
TW (wt, at) =wt+1 = w(st+1) =p(st+1 | wt, at)
= ∑
st∈S
w(st) p(st+1 | st, at) (34)
and correspond to the lifted transition map in Eq. (33),
• MW : W → P(O) is the belief observation map defined for wt ∈ W, at ∈ A and ot ∈ O as:
MW (wt) =p(ot | wt)
= ∑
st∈S
wt(st) p(ot | st), ( 35)
and correspond to the lifted observation map in Eq. (33).
We note that this is a rather special kind of POMDP , one in which state transitions are deterministic. In
some sense, this is a different generalisation of deterministic Moore machines: probabilistic Moore
machines in Definition 21 make both transition and observation maps stochastic, while here only the
observation map is stochastic.
Applying the generalised determinisation of [92] to the generative process and generative model
as coalgebras yields the following, respectively (by currying, see Example 6):
⟨outW
POMDP, trW
POMDP⟩ : W → P(O) × WA (lifted generative process)
⟨outW′
POMDP, trW′
POMDP⟩ : W′ → P(O) × W′A (lifted generative model). ( 36)
Using these, a distribution behavioural equivalence between them amounts to the following:
10NB: W ̸= Z in general, however their relation won’t be explored further here.
21
A coalgebraic perspective on predictive processing
Definition 28 (Distribution behavioural equivalence of lifted POMDPs). This is a direct application
of Definition 19 for F = POMDP, with POMDPs given as lifted POMDPs from Definition 27 (once
again, without visualising R for simplicity):
W Ω W′
P(O) × WA P(O) × ΩA P(O) × W′A
⟨outW
POMDP,trW
POMDP⟩
behW
⟨outΩ
POMDP,trΩ
POMDP⟩
P(idO)×(behW )A
behW′
⟨outW′
POMDP,trW′
POMDP⟩
P(idO)×(behW′ )A
(37)
which corresponds to the following conditions (see again Appendix A, considering that transitions
are deterministic and hence delta distributions), where for any a ∈ A, w ∈ W and w′ ∈ W′ we have:
p(o | w) =p(o | w′) (condition 1)
behW
 
trW
POMDP(w)(a)

= behW′
 
trW′
POMDP(w′)(a)) (condition 2) ( 38)
Condition 1 states that two beliefs, w and w′, are behaviourally equivalent only if they produce
the same expected probability of observations (see Eq. (33)). In other words, provided that condition
2 also holds, w and w′ are equivalent if their distributions on states “average out” to the same
observations. They may represent different internal (i.e. state) information, yet their expected
observational consequences are identical.
The second condition is recursive in nature, as is typical for bisimulations of deterministic sys-
tems [94]. It states that, for two beliefs w and w′ to be behaviourally equivalent, their predicted future
beliefs must also be equivalent for any given action. This implies that two beliefs are indistinguishable
if they lead to the same beliefs (predictions) about the next state of the world, that is, equivalence
is tested on the belief that results from pure dynamical prediction, without taking into account new
evidence, i.e. observations, which are instead part of condition 1. This contrasts with belief MDPs,
where these two conditions are combined into a condition on Bayesian updates (see Eq. (29)).
5. Discussion
In Section 4.2, we introduced three distinct notions of behavioural equivalence building on Defini-
tion 19: behavioural equivalence of POMDPs (Definition 23), belief behavioural equivalence of belief
POMDPs (Definition 25), and distribution behavioural equivalence of lifted POMDPs (Definition 28).
Translating the conditions of each definition into a more familiar form gives us some background
on their implications and relations to predictive processing. We summarise this high level account
pictorially in Fig. 2.
Eq. (24) suggests that Definition 23 may be too strict, as it requires observations for particular
ground truth states to be equal. While states can be coarse grained based on transition dynamics,
the condition on observations seems too strong. In contrast, Definition 25 requires only Bayesian
beliefs to be equal, under the assumption that they can be coarse grained if their transitions allow
for it (see Eq. (29)). From a coalgebraic perspective, however, this condition is too loose: based on
the definition of behavioural equivalence given in Definition 19, this condition must be satisfied for
elements of the final coalgebra of the category of MDPs (which contain belief MDPs). Such final
coalgebra is however trivial, a one element set, which implies that all belief MDPs can be said to be
22
A coalgebraic perspective on predictive processing
Fig. 2: The three possible descriptions of behavioural equivalence introduced in this work.
behaviourally equivalent under this account, because their states are never exposed to an external
observer (see Section 5.2 for further discussion). The third proposal seemingly hits a sweet spot,
and we believe it can be used to establish some of the relevant connections between coalgebras, their
language to compare systems’ behaviours and predictive processing.
Definition 28 translates some of the core principles of predictive processing and active inference in
a coalgebraic language, clarifying the goal, the mechanism, and the nature of an agent’s generative
model. Condition 1 in Definition 28 defines the goal of predictive processing in terms of prediction
error minimisation, which is expressed as the idea that the agent’s model must generate the same
sensory predictions as the environment. If we take w ∈ W to represent probability distributions over
states of the environment encoded by the generative process, Condition 1 demands that the agent’s
beliefs w′ ∈ W′, which, it should be noted, are distinct from those in a belief MDP , encoded by the
implicit generative model produce the exact same probability distribution over observations o ∈ O
given by the environment. In active inference, an agent that has minimised its free energy to the
greatest possible extent is one that has successfully tuned its model to satisfy this condition.
23
A coalgebraic perspective on predictive processing
Furthermore, an active inference agent never has direct access to the true states of the world s ∈ S,
but only to its own probabilistic beliefs about the world. Distribution behavioural equivalence operates
at the level of these beliefs, providing a mechanism to compare an agent’s beliefs, Q(S1:T, π, A, B),
which correspond (DKL = 0 in the limit) to the exact posterior P(S1:T, π, A, B|O1:T) for agents perform-
ing exact inference (see Eq. (3)), with the environment’s ground truth probabilistic state, rather than
comparing an agent’s internal states with the world’s true hidden states, cf. Definition 23.
Thus, behavioural equivalence well captures what we believe to be the true nature of active
inference, proposing that the brain’s implicit generative models need only be action-oriented, enabling
an agent to fulfil its goals but not necessarily proving to be perfect and exhaustive copies of the
generative processes of the world. In this light, a generative model, with state space S′, can be
vastly simpler than the environment’s generative process with state space S. As long as the two
systems satisfy the conditions in Eq. (38), making the same predictions (condition 1) and evolving
their beliefs in indistinguishable ways (condition 2), they are functionally equivalent. This framework
thus demonstrates when a simpler model can be “good enough”, by formalizing what it means to be
observationally indistinguishable.
5.1. Related work
This perspective is based on a coalgebraic formulation of dynamical systems, describing both a
generative process of the environment and an implicit generative model encoded by brain states. The
application of the coalgebraic language to MDPs is not new, see, for instance [114], which defines a
category of MDPs that includes rewards (cf. Definition 22) and [113, 121]. Coalgebras have also been
applied in the context of predictive processing (see for instance [ 126, 127]). However, these works
make no explicit reference to the structural reading of active inference and predictive processing
proposed here, highlighting the generative model and process as two distinct entities, clarifying
their role within the overall framework, and showing how to use maps of systems, particularly
bisimulations, to understand the purported synchronisation of brain and environment (although [127]
defines a notion of (quasi-)bisimulations to get to a general definition of Bayesian inversions in their
setup).
With the use of a coalgebraic framework, we have also seen how immediate it is to switch
between seemingly different kinds of dynamical systems: deterministic (open) dynamical systems,
or Moore machines, possibilistic Moore machines and probabilistic Moore machines are, in fact all
examples of “structured Moore machines” as defined in [ 92]. That is to say, their transition types,
are the same: given an input, a system combines the current state and the input to obtain the next
(one/possible/distribution of) state(s) and produce (one/possible/a distribution of) output(s). What
changes is simply the content in the brackets, differentiating the kind of branching a system can
have, e.g. deterministic, possibilistic or probabilistic. It is thus, to some extent, not surprising to
see important works such as [128] highlighting the similarities between formulations of predictive
processing and classical automata. However, we should stress that, following [92], one cannot simply
expect to use another kind of structured Moore machine to obtain Turing machines, so the precise
connection to [128], if it exists at all, will be investigated in future work.
We believe that our work based on behavioural equivalence of generative model and generative
process is also related to the “interpretation map” approach in [66, 67], however we do not explore
possible connections here. Our definition of lifted POMDP , see Definition 27, employs a notion of
belief states that is also potentially related to the definition of predictive beliefs in [ 97]. This could
then explain its relation to the beliefs in a belief MDP (Definition 24), which would be their postdictive
24
A coalgebraic perspective on predictive processing
counterpart, however, this remains at present speculative. 11
5.2. Current limitations and future work
Currently, we do not handle continuous probabilities (cf. Giry functor on measurable spaces [ 37]).
While discrete probabilities are a helpful simplification and are also useful for different parts of active
inference and predictive processing [26], they do not cover the whole framework, which also involves
large parts using continuous probabilities [23, 30]. One of the main reasons to leave a similar treatment
for continuous probabilities to future work is the fact that definitions of bisimulation for continuous
probabilities are more difficult to handle and require more careful considerations [37, 129, 108].
We also do not currently discuss several other computational components of active inference,
including learning, policy selection, and action, which are a central part of this framework (for a
review, see [26]) but for which we lack a coalgebraic counterpart. The algorithmic part of active
inference is also currently not discussed, as our focus is on “fixed points”, that is, the behavioural
equivalence of generative model and generative process does not really include phases like the
transients leading to synchronisation. This is in part because the current work is focused on laying
down the foundations for a coalgebraic treatment of some aspects of predictive processing and active
inference, and in part because we are not currently aware of algorithmic implementations compatible
with behavioural equivalence that don’t rely on an underlying bisimulation equivalence. This is
perhaps not a major limitation since, bisimulations between two coalgebras (which are in a one-to-one
correspondence to behavioural equivalences under the right conditions) are equivalent to bisimulation
equivalences12, for which various implementations of learning algorithms (i.e. exhibiting transients)
exist in machine learning and reinforcement learning where bisimulation equivalences are a central
research topic [104, 130, 97]. In these fields, they are used to describe compressions (see their relation
to models in Definition 9) of state or state(-action) spaces of a (PO)MDP , often using approximation
such as “bisimulation metrics” [131].
Finally, our definition of a category of MDPs (Definition 22) can be potentially improved: while
it does have a final coalgebra, this is trivial (i.e. the one-element set), which renders all non-trivial
coalgebras in this category non observable in a coalgebraic sense, that is, their maps into the final
coalgebra are not monomorphisms. This is perhaps counter-intuitive from the perspective of rein-
forcement learning and active inference, where MDPs are traditionally seen as fully observable. This
observation points to a potential inconsistency that may need to be addressed. Technically, we could
change the definition of the category of MDPs so that these coalgebras produce an output, exposing
their states directly, i.e. MDP(S) =S × P(S)A. However, this would produce systems that expose all
possible states S rather than the particular state at a specific time step.13 It would be a problem for
the traditional definition of bisimulation. In that case, we would either need to assume that given
coalgebras (S, f S), (S′, f S′
), their states are equal (because their exposes outputs would need to be
equal, see Definition 23 where in place of P(O) we would have S), or abandon the standard notion
of behavioural equivalence if their outputs are to remain distinct. Alternatively, we could adopt the
11Roughly: predictive beliefs are sufficient statistics built from a Bayesian prediction step, a pushforward of probabilities
along the dynamics of the system before a new observation is collected, while postdictive beliefs are sufficient statistics
generated after the observation is collected from a Bayesian update step.
12Following standard results (see for instance [108, proposition 3.44 in the October 8th 2024 version]), given two coalgebras
S = (S, f S), S′ = (S′, f S′
), a cospan between S and S′ (“external” behavioural equivalence) is equivalent to a cospan
between S + S′, the disjoint union (or coproduct) of the two original coalgebras, andS + S′, i.e. itself (“internal” behavioural
equivalence).
13The authors would like to thank Nathaniel Virgo for pointing this out.
25
A coalgebraic perspective on predictive processing
definition from [114], with coalgebras of the form S → R × P(S)A, where R is used to represent
rewards. This category has a non-trivial final coalgebra because rewards are observable. However,
since active inference and predictive processing do not usually rely on rewards, this would be outside
the scope of this work.
6. Conclusion
In this work we captured important properties of agent-environment coupled systems where agents
are thought to implement a process of free energy minimisation in predictive processing, using the
language of coalgebras [36, 37]. More precisely, we provided a new and more formal perspective on
the idea that an agent’s implicit generative model need not in fact be isomorphic in a structural sense
(cf. algebras vs. coalgebras [ 36, 132, 133, 90]) to the generative process representing relevant parts
of the environment [31, 32, 33, 34, 35]. Instead, we argued that what truly matters is the behaviour of
an agent’s brain, or rather of the generative model as a POMDP that it implicitly encodes, which in
coalgebraic terms corresponds to its outputs over time, i.e. predictions of observations over different
modalities. In particular, it matters that these predictions are compatible with the observations
produced by the environment’s generative process on a distribution level, rather than for ground truth
states of the brain and the environment (Definition 28), while remaining consistent with achieving the
agent’s overall goals.
Acknowledgments
M.B. and T.N. were supported by JST FOREST Program (JPMJFR231V). T.N. was also supported by
JSPS KAKENHI (grant numbers JP24H02172 and JP24H01559). F.T. was supported by JST, Moonshot
R&D, Grant Number JPMJMS2012.
26
A coalgebraic perspective on predictive processing
A. Concrete behavioural equivalence for POMDPs
Given two coalgebras (S, f S
POMDP) and (S′, f S′
POMDP), two states s ∈ S and s′ ∈ S′ are behaviourally
equivalent if there exists a final F-coalgebra (Ω, f Ω
POMDP = (trΩ
POMDP(s), outΩ
POMDP(s))) and a pair of
F-coalgebra morphisms behS : (S, f S
POMDP) → (Ω, f Ω
POMDP) and behS′ : (S′, f S′
POMDP) → (Ω, f Ω
POMDP)
such that behS(s) = behS′ (s′). This can be written in a more familiar form. To see that, let ω0 =
behS(s) =behS′ (s′). The maps behS and behS′ are F-coalgebra morphisms if both:
f Ω
POMDP
 
behS(s)

(a) =
 
P(idO) × P(behS)

f S
POMDP(s)(a)
=
 
P(idO) × P(behS)

outS
POMDP(s), trS
POMDP(s)(a)

=

P(idO)outS
POMDP(s), P(behS)trS
POMDP(s)(a)

=

outS
POMDP(s), P(behS)trS
POMDP(s)(a)

(A.1)
and
f Ω
POMDP
 
behS′ (s′)

(a) =
 
P(idO) × P(behS′ )

f S′
POMDP(s′)(a)
=
 
P(idO) × P(behS′ )

outS
POMDP(s′), trS
POMDP(s′)(a)

=

P(idO)outS
POMDP(s′), P(behS′ )trS
POMDP(s′)(a)

=

outS
POMDP(s′), P(behS′ )trS
POMDP(s′)(a)

(A.2)
hold. Since behS(s) =behS′ (s′) =ω0, the following holds
f Ω
POMDP
 
behS(s)

(a) =f Ω
POMDP
 
behS′ (s′)

(a) (A.3)
and therefore, component wise,
outS
POMDP(s) =outS′
POMDP(s′)
P(behS)trS
POMDP(s)(a) =P(behS′ )trS′
POMDP(s′)(a). ( A.4)
The first condition says that the observations must be equal, while the second one corresponds to equal-
ity of distributions over Ω, i.e. for any element ω ∈ Ω, the probability value P(behS)trS
POMDP(s)(a)(ω)
is equal to the probability value P(behS′ )trS′
POMDP(s′)(a)(ω). Using the definition of the distribution
functor for discrete probability (Definition 20), we then have that:
∑
˜s∈beh−1
S (ω)
trS
POMDP(s)(a)(˜s) = ∑
˜s′∈beh−1
S′ (ω)
trS′
POMDP(s′)(a)(˜s′). ( A.5)
Finally, we re-express the two conditions in Eq. (A.4) in a more traditional notation, obtaining:
P(o | s) =P(o | s′) (condition 1)
∑
˜s∈beh−1
S (ω)
P(˜s | s, a) = ∑
˜s′∈beh−1
S′ (ω)
P(˜s′ | s′, a) (condition 2) ( A.6)
27
A coalgebraic perspective on predictive processing
B. Bayesian filtering updates
A belief at time t, a probability distribution over hidden states at time t, zt := z(st) is defined using
Bayesian filtering updates of type τZ : Z × O × A → Z, given by
zt :=τZ(zt−1, ot, at−1)
=p(st | zt−1, ot, at−1)
=p(st | ht−1, ot, at−1)
 
= p(st | ht)

(zt−1 is a sufficient statistic of ht−1)
=p(st | o0...t−1, a0...t−2, ot, at−1)
=p(st | o0...t, a0...t−1)
= p(ot | st, o0...t−1, a0...t−1)p(st, o0...t−1, a0...t−1)
p(ot, o0...t−1, a0...t−1) (Bayesian filtering)
= p(ot | st, o0...t−1, a0...t−1)p(st | o0...t−1, a0...t−1)
p(ot | o0...t−1, a0...t−1)
= p(ot | st)p(st | o0...t−1, a0...t−1)
p(ot | o0...t−1, a0...t−1) (Markovianity)
=
p(ot | st) ∑st−1 p(st | st−1, a0...t−1)p(st−1 | o0...t−1, a0...t−1)
p(ot | o0...t−1, a0...t−1) (Chapman-Kolmogorov)
=
p(ot | st) ∑st−1 p(st | st−1, at−1)p(st−1 | o0...t−1, a0...t−2)
p(ot | o0...t−1, a0...t−1) (Markovianity)
=
p(ot | st) ∑st−1 p(st | st−1, at−1)zt−1
p(ot | ht−1, at−1) (Definitions of zt−1 and ht−1)
= ∑
st−1
p(st, ot | st−1, at−1)zt−1
p(ot | ht−1, at−1) . ( B.1)
References
[1] Karl J. Friston. Learning and inference in the brain. Neural Networks, 16(9):1325–1352, 2003.
[2] Karl J. Friston. A theory of cortical responses. Philosophical Transactions of the Royal Society of
London. Series B, Biological sciences, 360(1456):815–836, 2005.
[3] Karl J. Friston and Stefan J. Kiebel. Predictive coding under the free-energy principle. Philosophi-
cal Transactions of the Royal Society B: Biological Sciences, 364(1521):1211–1221, May 2009.
[4] Karl J. Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience,
11(2):127–138, 2010.
[5] Andy Clark. Whatever next? Predictive brains, situated agents, and the future of cognitive
science. Behavioral and Brain Sciences, 36(3):181–204, 2013.
[6] Andy Clark. Surfing Uncertainty: Prediction, Action, and the Embodied Mind. Oxford University
Press, 2015.
[7] Jakob Hohwy. The Predictive Mind. Oxford University Press, Oxford, 2013.
[8] Rick A. Adams, Stewart Shipp, and Karl J. Friston. Predictions not commands: Active inference
in the motor system. Brain Structure and Function, 218(3):611–643, 2013.
28
A coalgebraic perspective on predictive processing
[9] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of the
Royal Society Interface, 14(136):20170376, 2017.
[10] Filippo Torresan, Keisuke Suzuki, Ryota Kanai, and Manuel Baltieri. Active inference for
action-unaware agents, August 2025.
[11] Andy Clark. Radical predictive processing. The Southern Journal of Philosophy, 2015.
[12] Thomas Parr and Karl J. Friston. The discrete and continuous brain: From decisions to
movement—and back again. Neural Computation, 30(9):2319–2347, 2018.
[13] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative
Models for Active Vision. Frontiers in Neurorobotics, 15:651432, April 2021.
[14] Andy Clark. Dreaming the Whole Cat: Generative Models, Predictive Processing, and the
Enactivist Conception of Perceptual Experience. Mind, 121(483):753–771, July 2012.
[15] Alex Kiefer and Jakob Hohwy. Content and misrepresentation in hierarchical generative models.
Synthese, 195(6):2387–2415, June 2018.
[16] Alex Kiefer and Jakob Hohwy. Representation in the Prediction Error Minimization Framework.
In Sarah Robins, John Symons, and Paco Calvo, editors, The Routledge Companion to Philosophy
of Psychology, pages 384–409. Routledge, Second edition. | Abingdon, Oxon ; New York, NY :
Routledge, Taylor & Francis Group, 2020., 2 edition, October 2019.
[17] Karl J. Friston, Nelson Trujillo-Barreto, and Jean Daunizeau. DEM: A variational treatment of
dynamic systems. NeuroImage, 41(3):849–885, 2008.
[18] Paweł Gładziejewski. Predictive coding and representationalism. Synthese, 193(2):559–582,
February 2016.
[19] Maxwell J. D. Ramstead, Michael D. Kirchhoff, and Karl J. Friston. A tale of two densities:
Active inference is enactive inference. Adaptive Behavior, page 1059712319862774, 2019.
[20] Maxwell J. D. Ramstead, Casper Hesp, Alexander Tschantz, Ryan Smith, Axel Constant, and
Karl Friston. Neural and phenotypic representation under the free-energy principle.Neuroscience
& Biobehavioral Reviews, 120:109–122, January 2021.
[21] Karl J. Friston. Life as we know it. Journal of the Royal Society Interface, 10(86):20130475, 2013.
[22] Biswa Sengupta and Karl J. Friston. Sentient Self-Organization: Minimal dynamics and circular
causality, May 2017.
[23] Karl J. Friston. A free energy principle for a particular physics, June 2019.
[24] Ensor Rafael Palacios, Takuya Isomura, Thomas Parr, and Karl J. Friston. The emergence of
synchrony in networks of mutually inferring neurons. Scientific Reports, 9(1):6412, April 2019.
[25] Thomas Parr, Lancelot Da Costa, and Karl J. Friston. Markov blankets, information geometry
and stochastic thermodynamics. Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 378(2164):20190159, 2020.
[26] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl J.
Friston. Active inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology,
99:102447, December 2020.
[27] Thomas Parr, Noor Sajid, and Karl J. Friston. Modules or Mean-Fields? Entropy, 22(5):552, May
2020.
29
A coalgebraic perspective on predictive processing
[28] Karl J. Friston, Conor Heins, Kai Ueltzhöffer, Lancelot Da Costa, and Thomas Parr. Stochastic
Chaos and Markov Blankets. Entropy, 23(9):1220, September 2021.
[29] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle in
Mind, Brain, and Behavior. MIT Press, March 2022.
[30] Karl J. Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A.
Pavliotis, and Thomas Parr. The free energy principle made simpler but not too simple. Physics
Reports, 1024:1–29, June 2023.
[31] Manuel Baltieri and Christopher L. Buckley. An active inference implementation of phototaxis.
In European Conference on Artificial Life 2017, pages 36–43. MIT Press, 2017.
[32] Manuel Baltieri and Christopher L. Buckley. Generative models as parsimonious descriptions of
sensorimotor loops. Behavioral and Brain Sciences, 42:e218, 2019.
[33] Manuel Baltieri and Christopher L. Buckley. PID control as a process of active inference with
linear generative models. Entropy, 21(3):257, 2019.
[34] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models
through active inference. PLoS Computational Biology, 16(4):e1007805, April 2020.
[35] Francesco Mannella, Federico Maggiore, Manuel Baltieri, and Giovanni Pezzulo. Active infer-
ence through whiskers. Neural Networks, 144:428–437, 2021.
[36] Jan J. M. M. Rutten. Universal coalgebra: A theory of systems. Theoretical Computer Science,
249(1):3–80, October 2000.
[37] Bart Jacobs. Introduction to Coalgebra: Towards Mathematics of States and Observation. Cambridge
University Press, 1 edition, 2017.
[38] R Bogacz. A tutorial on the free-energy framework for modelling perception and learning.
Journal of Mathematical Psychology, 2017.
[39] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy
principle for action and perception: A mathematical review. Journal of Mathematical Psychology,
81:55–79, December 2017.
[40] Karl J. Friston, Thomas Fitzgerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active Inference: A Process Theory. Neural Computation, 29:1–49, 2017.
[41] Martin Biehl, Christian Guckelsberger, Christoph Salge, Simón C. Smith, and Daniel Polani.
Expanding the Active Inference Landscape: More Intrinsic Motivations in the Perception-Action
Loop. Frontiers in Neurorobotics, 12:45, August 2018.
[42] Karl J. Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated
Inference. Neural Computation, 33(3):713–763, March 2021.
[43] Randall D. Beer. Dynamical approaches to cognitive science. Trends in Cognitive Sciences ,
4(3):91–99, March 2000.
[44] Randall D. Beer. The dynamics of brain–body–environment systems: A status report. In
Handbook of Cognitive Science, pages 99–120. Elsevier, 2008.
[45] Uri Alon, Michael G. Surette, Naama Barkai, and Stanislas Leibler. Robustness in bacterial
chemotaxis. Nature, 397(6715):168–171, 1999.
[46] Burton W. Andrews, Tau-Mu Yi, and Pablo A. Iglesias. Optimal Noise Filtering in the Chemo-
tactic Response of Escherichia coli. PLoS Computational Biology, 2(11):e154, 2006.
30
A coalgebraic perspective on predictive processing
[47] Noor Sajid, Philip J Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystified and
compared. Neural Computation, 33(3):674–712, 2021.
[48] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
[49] Leslie P . Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101(1-2):99–134, May 1998.
[50] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . Adaptive
Computation and Machine Learning Series. The MIT Press, Cambridge, Massachusetts, second
edition edition, 2018.
[51] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference?
PLoS ONE, 4(7):e6421, July 2009.
[52] Karl J. Friston. What is optimal about motor control? Neuron, 72(3):488–498, 2011.
[53] Karl J. Friston and Ping Ao. Free energy, value, and attractors. Computational and Mathematical
Methods in Medicine, 2012, 2012.
[54] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L Buckley. On the relationship
between active inference and control as inference. In International Workshop on Active Inference,
pages 3–11. Springer, 2020.
[55] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl J. Friston, and Ryan Smith. Reward maximiza-
tion through discrete active inference. Neural Computation, 35(5):807–852, 2023.
[56] Jakob Von Uexküll. A stroll through the worlds of animals and men: A picture book of invisible
worlds. Semiotica, 89(4), 1992.
[57] Nihat Ay and Keyan Zahedi. On the Causal Structure of the Sensorimotor Loop. In Mikhail
Prokopenko, editor, Guided Self-Organization: Inception, volume 9, pages 261–294. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2014.
[58] Karl J. Friston, Erik D. Fagerholm, Tahereh S. Zarghami, Thomas Parr, Inês Hipólito, Loïc Ma-
grou, and Adeel Razi. Parcels and particles: Markov blankets in the brain. Network Neuroscience,
5(1):211–251, January 2021.
[59] Jelle Bruineberg, Krzysztof Doł˛ ega, Joe Dewhurst, and Manuel Baltieri. The Emperor’s New
Markov Blankets. Behavioral and Brain Sciences, 45:e183, 2022.
[60] Martin Biehl, Felix A. Pollock, and Ryota Kanai. A Technical Critique of Some Parts of the Free
Energy Principle. Entropy, 23(3):293, February 2021.
[61] Fernando E. Rosas, Pedro A.M. Mediano, Martin Biehl, Shamil Chandaria, and Daniel Polani.
Causal blankets: Theory and algorithmic framework. In International Workshop on Active Inference,
pages 187–198. Springer, 2020.
[62] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. How
particular is the physics of the free energy principle? Physics of Life Reviews, 40:24–50, March
2022.
[63] Nathaniel Virgo, Fernando E. Rosas, and Martin Biehl. Embracing sensorimotor history: Time-
synchronous and time-unrolled Markov blankets in the free-energy principle. Behavioral and
Brain Sciences, 45:e215, 2022.
[64] Christopher M. Bishop. Pattern Recognition and Machine Learning . Information Science and
Statistics. Springer, New York, 2006.
31
A coalgebraic perspective on predictive processing
[65] Kevin P . Murphy.Machine Learning - A Probabilistic Perspective . Adaptive Computation and
Machine Learning. MIT Press, Cambridge, 2014.
[66] Nathaniel Virgo, Martin Biehl, and Simon McGregor. Interpreting Dynamical Systems as
Bayesian Reasoners. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases. Springer, December 2021.
[67] Martin Biehl and Nathaniel Virgo. Interpreting systems as solving POMDPs: A step towards
a formal understanding of agency. In International Workshop on Active Inference, pages 16–31.
Springer, 2022.
[68] Manuel Baltieri, Martin Biehl, Matteo Capucci, and Nathaniel Virgo. A Bayesian Interpretation
of the Internal Model Principle, March 2025.
[69] Nathaniel Virgo, Martin Biehl, Manuel Baltieri, and Matteo Capucci. A "good regulator theorem"
for embodied agents, August 2025.
[70] Alexander Tschantz, Manuel Baltieri, Anil. K. Seth, and Christopher L. Buckley. Scaling Active
Inference. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1–8, Glasgow,
United Kingdom, July 2020. IEEE.
[71] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement
Learning through Active Inference, February 2020.
[72] Abraham Imohiosen, Joe Watson, and Jan Peters. Active inference or control as inference? A
unifying view. In International Workshop on Active Inference, pages 12–19. Springer, 2020.
[73] Parvin Malekzadeh and Konstantinos N. Plataniotis. Active inference and reinforcement
learning: A unified inference on continuous state and action spaces under partial observability.
Neural Computation, 36(10):2073–2135, 2024.
[74] Georg B. Keller and Thomas D. Mrsic-Flogel. Predictive processing: A canonical cortical
computation. Neuron, 100(2):424–435, 2018.
[75] Jakob Hohwy. New directions in predictive processing. Mind & Language, 2020.
[76] Klaas E. Stephan, Will D. Penny, Jean Daunizeau, Rosalyn J. Moran, and Karl J. Friston. Bayesian
model selection for group studies. Neuroimage, 46(4):1004–1017, 2009.
[77] Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An Active Inference
Approach to Modeling Structure Learning: Concept Learning as an Example Case. Frontiers in
Computational Neuroscience, 14:41, May 2020.
[78] Thomas Parr. Inferential dynamics. Physics of Life Reviews, 42:1–3, September 2022.
[79] Andy Clark. Embodied prediction. Open MIND, 2015.
[80] Marco Facchin. Structural representations do not meet the job description challenge. Synthese,
199(3-4):5479–5508, December 2021.
[81] Filippo Torresan and Manuel Baltieri. Disentangled representations for causal cognition. Physics
of Life Reviews, 51:343–381, December 2024.
[82] Jelle Bruineberg, Krzysztof Doł˛ ega, Joe Dewhurst, and Manuel Baltieri. The Emperor Is Naked:
Replies to commentaries on the target article. Behavioral and Brain Sciences, 45, 2022.
[83] Lancelot Da Costa, Karl J. Friston, Conor Heins, and Grigorios A. Pavliotis. Bayesian mechanics
for stationary processes. Proceedings of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 477(2256):20210518, December 2021.
32
A coalgebraic perspective on predictive processing
[84] Saunders Mac Lane. Categories for the Working Mathematician , volume 5 of Graduate Texts in
Mathematics. Springer New York, New York, NY, 1978.
[85] Ichiro Hasuo, Bart Jacobs, and Ana Sokolova. Generic trace semantics via coinduction. Logical
Methods in Computer Science, Volume 3, Issue 4, 2007.
[86] David Jaz Myers. Categorical Systems Theory. 2021.
[87] Matteo Capucci. Notes on categorical systems theory. Technical report, 2024.
[88] Sophie Libkind and David Jaz Myers. Towards a double operadic theory of systems, May 2025.
[89] Emily Riehl. Category Theory in Context. Courier Dover Publications, 2017.
[90] Jan J. M. M. Rutten. The Method of Coalgebra: Exercises in coinduction. Technical report,
Amsterdam: CWI, 2019.
[91] Alexandra Silva, Filippo Bonchi, Marcello M. Bonsangue, and Jan J. M. M. Rutten. Generalizing
the powerset construction, coalgebraically. InIARCS Annual Conference on Foundations of Software
Technology and Theoretical Computer Science (FSTTCS 2010), page 12 pages. Schloss Dagstuhl -
Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany, 2010.
[92] Alexandra Silva, Filippo Bonchi, Marcello Bonsangue, and Jan J. M. M. Rutten. Generalizing
determinization from automata to coalgebras. Logical Methods in Computer Science, Volume 9,
Issue 1:1087, March 2013.
[93] Jean-Pierre Aubin, Alexandre M. Bayen, and Patrick Saint-Pierre.Viability Theory: New Directions.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.
[94] Davide Sangiorgi and Jan J. M. M. Rutten. Advanced Topics in Bisimulation and Coinduction ,
volume 52. Cambridge University Press, 2011.
[95] Davide Sangiorgi. Introduction to Bisimulation and Coinduction. Cambridge University Press, 1
edition, October 2011.
[96] Balaraman Ravindran and Andrew G Barto. SMDP Homomorphisms: An Algebraic Approach to
Abstraction in Semi-Markov Decision Processes. In IJCAI’03: Proceedings of the 18th International
Joint Conference on Artificial Intelligence, 2003.
[97] Fernando Rosas, Alexander Boyd, and Manuel Baltieri. AI in a vat: Fundamental limits of
efficient world modelling for agent sandboxing and interpretability. In Proceedings of the Second
Reinforcement Learning Conference, April 2025.
[98] William G. Noid, Jhih-Wei Chu, Gary S. Ayton, Vinod Krishna, Sergei Izvekov, Gregory A. Voth,
Avisek Das, and Hans C. Andersen. The multiscale coarse-graining method. I. A rigorous bridge
between atomistic and coarse-grained models. The Journal of Chemical Physics, 128(24):244114,
June 2008.
[99] Herbert A. Simon and Albert Ando. Aggregation of variables in dynamic systems. Econometrica:
Journal of the Econometric Society, 29(2):111, 1961.
[100] Zhiyuan Ren and Bruce Krogh. State aggregation in Markov decision processes. In Proceedings
of the 41st IEEE Conference on Decision and Control, volume 4, page 3824 vol.4, January 2003.
[101] John G. Kemeny and J. Laurie Snell. Finite Markov Chains, volume 26. van Nostrand Princeton,
NJ, 1969.
[102] B. Moore. Principal component analysis in linear systems: Controllability, observability, and
model reduction. IEEE Transactions on Automatic Control, 26(1):17–32, February 1981.
33
A coalgebraic perspective on predictive processing
[103] Peter E. Caines and Yuan-Jun Wei. The hierarchical lattices of a finite machine.Systems & Control
Letters, 25(4):257–263, 1995.
[104] Amy Zhang, Zachary C. Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima Anandkumar,
Laurent Itti, Joelle Pineau, and Tommaso Furlanello. Learning Causal State Representations of
Partially Observable Environments, February 2021.
[105] Amy Zhang. State Abstractions for Generalization in Reinforcement Learning. PhD thesis, McGill
University, 2021.
[106] E. P . de Vink and Jan J. M. M. Rutten. Bisimulation for probabilistic transition systems: A
coalgebraic approach. Theoretical Computer Science, 221(1):271–293, June 1999.
[107] Josée Desharnais, Abbas Edalat, and Prakash Panangaden. Bisimulation for Labelled Markov
Processes. Information and Computation, 179(2):163–193, December 2002.
[108] Martín Santiago Moroni and Pedro Sánchez Terraf. A classification of bisimilarities for general
Markov decision processes, October 2024.
[109] Sam Staton. Relating coalgebraic notions of bisimulation. Logical Methods in Computer Science,
2011.
[110] Giorgio Bacci. Generalized Labelled Markov Processes, Coalgebraically. PhD thesis, Università degli
Studi di Udine, 2013.
[111] Luís Soares Barbosa. Coalgebra for the working software engineer. Journal of Applied Logics,
2022.
[112] Alexander Kurz. Logics for Coalgebras and Applications to Computer Science. PhD thesis, Ludwig-
Maximilian University of Munich, 2001.
[113] Nathaniel Virgo. Unifilar machines and the adjoint structure of Bayesian models. In Electronic
Proceedings in Theoretical Computer Science, volume 397, pages 299–317, 2023.
[114] Frank M. V . Feys, Helle Hvid Hansen, and Lawrence S. Moss. Long-Term Values in Markov
Decision Processes, (Co)Algebraically. In Corina Cîrstea, editor, Coalgebraic Methods in Computer
Science, volume 11202, pages 78–99. Springer International Publishing, Cham, 2018.
[115] Lawrence S. Moss and Ignacio D. Viglizzo. Final coalgebras for functors on measurable spaces.
Information and Computation, 204(4):610–636, April 2006.
[116] Filippo Bonchi, Alexandra Silva, and Ana Sokolova. Distribution Bisimilarity via the Power of
Convex Algebras. Logical Methods in Computer Science, Volume 17, Issue 3:6158, July 2021.
[117] Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. Equivalence Relations in Fully
and Partially Observable Markov Decision Processes. In IJCAI 2009 - Proceedings, 2009.
[118] David N. Jansen, Flemming Nielson, and Lijun Zhang. Belief Bisimulation for Hidden Markov
Models: Logical Characterisation and Decision Algorithm. In NASA Formal Methods, volume
7226, pages 326–340, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
[119] Karl J. Åström. Introduction to Stochastic Control Theory. Academic Press, 1970.
[120] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. Approximate In-
formation State for Approximate Planning and Reinforcement Learning in Partially Observed
Systems. Journal of Machine Learning Research, 2022.
[121] Simon McGregor, timorl, and Nathaniel Virgo. Formalising the intentional stance 2: A coinduc-
tive approach, January 2025.
34
A coalgebraic perspective on predictive processing
[122] Ana Sokolova. Probabilistic systems coalgebraically: A survey. Theoretical Computer Science,
412(38):5095–5110, September 2011.
[123] Silvia Crafa and Francesco Ranzato. A Spectrum of Behavioral Relations over LTSs on Probability
Distributions. In Joost-Pieter Katoen and Barbara König, editors, CONCUR 2011 – Concurrency
Theory, volume 6901, pages 124–139, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.
[124] Yuan Feng and Lijun Zhang. When Equivalence and Bisimulation Join Forces in Probabilistic
Automata. In FM 2014: Formal Methods , volume 8442, pages 247–262, Cham, 2014. Springer
International Publishing.
[125] Holger Hermanns, Jan Krˇ cál, and Jan Kˇ retínský. Probabilistic Bisimulation: Naturally on
Distributions. In Paolo Baldan and Daniele Gorla, editors, CONCUR 2014 – Concurrency Theory,
volume 8704, pages 249–265, Berlin, Heidelberg, 2014. Springer Berlin Heidelberg.
[126] Toby St Clere Smithe. Compositional Active Inference II: Polynomial Dynamics. Approximate
Inference Doctrines, August 2022.
[127] Toby St Clere Smithe. Open Dynamical Systems as Coalgebras for Polynomial Functors, with
Application to Predictive Processing. Electronic Proceedings in Theoretical Computer Science ,
380:307–330, August 2023.
[128] Takuya Isomura. Triple equivalence for the emergence of biological intelligence.Communications
Physics, 8(1):1–14, April 2025.
[129] Claudio Hermida, Uday Reddy, Edmund Robinson, and Alessio Santamaria. Bisimulation as a
logical relation. Mathematical Structures in Computer Science, 32(4):442–471, April 2022.
[130] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
Invariant Representations for Reinforcement Learning without Reconstruction, April 2021.
[131] Norman Ferns and Doina Precup. Bisimulation Metrics are Optimal Value Functions. In UAI,
pages 210–219, 2014.
[132] Luís Soares Barbosa. Algebraic and Coalgebraic Structures. Technical report, Universidade do
Minho, 2005.
[133] Yde Venema. Algebras and coalgebras. Studies in Logic and Practical Reasoning, 2007.
35