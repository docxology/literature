Robot Localization and Navigation through
Predictive Processing using LiDAR⋆
D. Burghardt1 and P. Lanillos2
1 Radboud University, Houtlaan 4, 6525 XZ Nijmegen, NL
2 Donders Institute for Brain, Cognition and Behaviour, Department of Artiﬁcial
Intelligence, Radboud University, Nijmegen, NL
Abstract. Knowing the position of the robot in the world is crucial
for navigation. Nowadays, Bayesian ﬁlters, such as Kalman and particle-
based, are standard approaches in mobile robotics. Recently, end-to-end
learning has allowed for scaling-up to high-dimensional inputs and im-
proved generalization. However, there are still limitations to providing re-
liable laser navigation. Here we show a proof-of-concept of the predictive
processing-inspired approach to perception applied for localization and
navigation using laser sensors, without the need for odometry. We learn
the generative model of the laser through self-supervised learning and
perform both online state-estimation and navigation through stochastic
gradient descent on the variational free-energy bound. We evaluated the
algorithm on a mobile robot (TIAGo Base) with a laser sensor (SICK)
in Gazebo. Results showed improved state-estimation performance when
comparing to a state-of-the-art particle ﬁlter in the absence of odometry.
Furthermore, conversely to standard Bayesian estimation approaches our
method also enables the robot to navigate when providing the desired
goal by inferring the actions that minimize the prediction error.
Keywords: Predictive Processing · Robot localization · Robot naviga-
tion · Laser sensor · LiDAR.
1 Introduction
Localization algorithms are part of our daily life and core for robotics. Recursive
Bayesian estimation composes the current state-of-art in the ﬁeld and has been
essential for the development of localization, mapping, navigation and searching
applications [20, 10]. Bayesian ﬁlters [4], e.g., Kalman and particle ﬁlters, are
able to estimate the state of a system from noisy sensor observations formalized
as a hidden Markov model. These approaches are useful also in the case of
non-linear modeled systems and out-of-sequence measurements [2]. The particle
ﬁlter (PF) is an approximate Bayesian method that tractably computes the
posterior distribution of the state of any system given the observations. The
state distribution is represented by individual particles, which are evaluated and
⋆ 2nd International Workshop on Active Inference IWAI2021, European Conference
on Machine Learning (ECML/PCKDD 2021)
arXiv:2109.04139v1  [cs.RO]  9 Sep 2021
2 D. Burghardt and P. Lanillos
weighted recursively. Particles with higher probability get bigger weights and
are re-sampled into more particles in its neighborhood, whereas particles with
smaller weights get fewer new samples close to them [13].
In recent years, novel approaches based on deep neural networks, have been
proposed to improve localization using high-dimensional inputs. Regression so-
lutions, for instance, compute the absolute position of the system from only vi-
sual information [8]. However, these methods have lower accuracy than previous
approaches that exploit prior information, such as geometry [19]. In particular,
LiDAR-based Navigation with representation learning (e.g., using autoencoders)
and reinforcement learning has shown downgraded performance in navigation
tasks [7]. Alternatively to LiDAR-based approaches, [21] and [18] showed how
to apply deep active inference using camera images in robot navigation and
humanoid upper-body reaching respectively.
We describe how the predictive processing (PP) approach to perception [3,
11] can aid in localization and simple navigation tasks [12]. In this work naviga-
tion is performed having the robot move between two points in an unobstructed
environment, which can be further built upon to tackle more complex environ-
ments (e.g. mazes). Under PP, the agent, following the Free Energy Principle
(FEP) [5], tries to minimize the error in the predicted observations by either
performing corrective actions to match the expected internal state or by up-
dating this internal state based on what it has experienced through the senses.
In this work, we present a proof-of-concept based on the Pixel-Active Inference
model [18], proposed for humanoid body perception and action, to perform laser-
based localization and navigation without the need for odometry. This has been
successfully applied to robot manipulator control to improve adaptation [14].
Our approach combines the power of deep networks regression with variational
Bayesian ﬁltering to provide a better reliable state estimation than PFs in our
proof-of-concept environment—See Fig. 1.
2 Methods
2.1 Robot
The TIAGo Base mobile robot uses the SICK TiM571 laser sensor, which has a
0.0m −25m range and a 270 ◦ aperture angle. In all experiments we limited the
robot’s movement to 2 degrees of freedom, i.e., moving forward, backward and
sideways.
2.2 Localization
We deﬁne the true state (position) of the robot at instant k as xk = (x,y) ∈R2
and the position belief of the robot as ˜xk. We further deﬁne the observation
ok as the laser measurements. Estimation is solved by computing the posterior
distribution p(x|o) by optimizing the Variational Free Energy (VFE). The algo-
rithm is sketched in Fig. 2a. Under the mean-ﬁeld and Laplace approximation
Localization and navigation through predictive processing using LiDAR 3
Fig. 1: Proof-of-concept environment designed for the experiments in Gazebo.
In the localization experiment the robot true position is randomly set and the
initial position belief is initialized to the center of the map. Laser range ﬁnder
measurements are displayed as the blue shading.
this is equivalent to minimizing the error between the sensory input ok and the
predicted sensory input ˆok. While regression approaches (Fig. 2b) compute the
pose directly from the visual input, stochastic neural ﬁltering continuously re-
ﬁnes the state through an error signal. We perform state estimation through
perceptual inference, minimizing the VFE as follows:
˜x = argmin
˜x
F(˜x,o) →˜xk+1 = ˜xk + α∂˜xg(˜xk)Σ−1
o (ok −g(˜xk)) (1)
Where α is the step size and ∂˜x denotes the derivative with respect to ˜x.
This is computed iteratively using gradient descent on the prediction error—
sensor measurement ok minus the predicted sensory input g(˜xk)—weighted by
the variance Σo. Both the predicted observations and the partial derivative of
the error are computed by means of a deep neural network forward pass and its
Jacobian [18], respectively.
2.3 Predicting the observations
We compute the sensor likelihood p(ok|˜xk) using a transposed convolutional
neural network (Fig. 2), which augments the dimensionality of the input from
x to the laser-sensor input size (e.g., 2 →622). The input is ﬁrstly fed into two
fully connected layers, and each transposed convolution layer is followed by a
regular convolution layer, based on the work of [18]. At every layer, we used the
ReLU activation function.
4 D. Burghardt and P. Lanillos
Fig. 2: (a) The Predictive Processing algorithm’s architecture. (b) Graphical rep-
resentation of regression approaches to robot state estimation. (c) Generative
model prediction vs. true laser measurement on a test sample.
The network was trained on 13000 normalized random samples collected in
the Gazebo simulation. Each sample consists of the true position of the robot
and the laser-sensor measurements at that location. The training was performed
with 20 batches of 500 samples, using the L1 loss and Adam optimizer [9].
2.4 Navigation
Analogously, our algorithm infers the action in the same way that state estima-
tion is computed, namely performing active inference. Actions also minimize the
VFE in the predicted observations.
a = argmin
a
F(˜x,o(a)) (2)
We deﬁne the goal as the preference or the intention of the agent ˜xgoal to arrive
to a sensory state ogoal [16]. Estimation and control are computed as follows 3:
˜xk+1 = ˜xk + α
[
∂˜xg(˜xk)Σ−1
o (ok −g(˜xk)) + ∂˜xg(˜xk)Σ−1
x β(ok −g(˜xgoal))
]
(3)
ak+1 = ak + γ∂a˜x∂˜xg(˜xk)Σ−1
o (ok −g(˜xk)) (4)
where β weights the goal attractor and γ is the action step size. Note that each
term computes the weighted prediction error mapped to the latent space.
The estimated state ˜x, now biased by the desired goal, generates a new
predicted observation at every new iteration that is transformed into an actiona,
3 This update equation assumes that the Hessian of the goal dynamics is −1 as pro-
posed in [18].
Localization and navigation through predictive processing using LiDAR 5
which minimizes the VFE. Thus, performing a movement in the direction of the
goal. The pseudo-code described in Alg. 1 illustrates the process. The algorithm
converges when the observation ﬁts the predicted laser sensor measurements.
Algorithm 1:FEP localization and navigation algorithm
˜x ←initial belief;
ogoal ←g(˜xgoal); // Generate goal
while true do
ok ←Normalize(laser input);
ˆok ←g(˜x); // Predicted observation
˜x ←Eq. 3;
a ←Eq. 4;
PerformAction(a);
end
3 Results
We evaluated our laser-based Active Inference algorithm against a particle ﬁl-
ter [13] in the Gazebo simulator, using a commercial mobile robot with a laser
rangeﬁnder sensor (TIAGo Base, pmb-2) [1], interfaced with Robot Operating
System (ROS) [17]. All experiments were conducted in a designed corridor-like
map described in Fig. 1. Localization and navigation results are summarized in
Fig. 3.
3.1 Localization and estimation
Firstly, we evaluated the localization accuracy when initializing the robot to a
random position in the space when a map was given. Thus, testing absolute
positing with laser measurements in static situations. For the particle ﬁlter, the
particle initial probabilities were randomly spread in the environment and reset
at the beginning of every trial. In our algorithm, we initialized the initial belief
in the center of the environment. We computed the ground truth positional error
in 100 trials for 50 iterations. Our algorithm converged much more consistently
to the true state over all trials, whereas the PF struggled to deliver consistent
results, as shown in Fig. 3 by the rather large standard deviation in the blue
shaded area. It is important to highlight that the PF is tuned for using the
robot’s odometry. However, for the sake of fair comparison solely laser-sensor
values were used as observations.
Secondly, we evaluated the localization performance when traversing the en-
vironment from one side to the other, by performing small teleports (to override
odometry) to simulate robot movement while keeping the rotation angle con-
stant. Results showed a more stable over time state estimation by our algorithm
6 D. Burghardt and P. Lanillos
Fig. 3: Localization and navigation evaluation. Mean and standard deviation of
the positioning absolute error of our model (FEP) compared against the PF
algorithm. Furthermore, our model is able to perform navigation using the same
Bayesian ﬁltering framework. The green line shows the mean absolute error to
the goal.
when compared to the PF, which seemed to suﬀer from the absence of odometry
information.
3.2 Navigation
For the assessment of the navigation algorithm’s performance, we ran an exper-
iment consisting of 50 trials in which the robot had to navigate from a starting
point to a goal position. The initial belief state was set to the robot’s initial
true position, to evaluate the performance of navigation without the eﬀects of
localization in the ﬁrst iterations. In every trial, both the initial position and
the goal state of the robot were chosen randomly, with the constraint that they
should be at least 12 meters (in Gazebo coordinates) apart from each other. The
task was considered complete when the robot got in a range of 0.8m from the
target. The results are plotted in green in Fig. 3.
We observed that the robot initially quickly approximates the goal, with a
big drop in the distance to the goal in the ﬁrst couple of iterations. As it gets
closer to the goal, the “velocity” of the robot (in the experiment described by
the step sizes) decreases. This is a result of the diminishing gradient in every
step of the algorithm, due to the stochastic gradient descent. Additionally, we
computed the average number of iterations that it took the algorithm to get in
the desired 0.8m range of the target. Over the 50 trials of similar travel distance
(∼11mto ∼13.5m), the average number of iterations was 12.5. This number is
naturally closely related to the optimal step size found.
Localization and navigation through predictive processing using LiDAR 7
4 Conclusions
This work shows a proof-of-concept on how predictive processing, i.e. active in-
ference agents, can perform laser-based localization and navigation tasks. The
results obtained in the localization experiment, where we compared our ap-
proach against a state-of-the-art alternative (particle ﬁlter), show the potential
of predictive stochastic neural ﬁltering in robot localization, and estimation in
general [6, 15]. Furthermore, the navigation experiment showcased how to com-
pute actions as a dual ﬁltering process. Nevertheless, at its current state, the
proposed algorithm suﬀers from a few deﬁciencies, most of which are related to
the learning of the generative model of the world. Besides currently requiring a
large dataset for training, the model is prone to mistake very similar objects in
the environment, given that the estimation of the new state is independent from
the previous. Additionally, because it is a supervised method trained before that
the robot can do any navigation, it is unable to cope with changing environ-
ments. All in all, the environment used in our experiments is rather simplistic
compared to demonstrations of current sota algorithms. Therefore, we foresee
further development and experimentation in terms of integration of odometry
information, the introduction of extra degrees of freedom and connection to the
robot’s non-linear dynamics.
References
1. Tiago base (Oct 2020), http://wiki.ros.org/Robots/TIAGo-base
2. Besada-Portas, E., Lopez-Orozco, J.A., Lanillos, P., De la Cruz, J.M.: Localization
of non-linearly modeled autonomous mobile robots using out-of-sequence measure-
ments. Sensors 12(3), 2487–2518 (2012)
3. Clark, A.: Whatever next? predictive brains, situated agents, and the future of
cognitive science. Behavioral and brain sciences 36(3), 181–204 (2013)
4. Fox, V., Hightower, J., Liao, L., Schulz, D., Borriello, G.: Bayesian ﬁlter-
ing for location estimation. IEEE Pervasive Computing 2(3), 24–33 (2003).
https://doi.org/10.1109/MPRV.2003.1228524
5. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nature reviews neu-
roscience 11(2), 127–138 (2010)
6. Friston, K.J., Trujillo-Barreto, N., Daunizeau, J.: Dem: a variational treatment of
dynamic systems. Neuroimage 41(3), 849–885 (2008)
7. Gebauer, C., Bennewitz, M.: The pitfall of more powerful autoencoders in lidar-
based navigation. arXiv preprint arXiv:2102.02127 (2021)
8. Kendall, A., Grimes, M., Cipolla, R.: Posenet: A convolutional network for real-
time 6-dof camera relocalization. In: Proceedings of the IEEE international con-
ference on computer vision. pp. 2938–2946 (2015)
9. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)
10. Lanillos, P.: Minimum time search of moving targets in uncertain environments.
Ph.D. thesis, PhD thesis (2013)
11. Lanillos, P., Cheng, G.: Adaptive robot body learning and estimation through pre-
dictive coding. In: 2018 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). pp. 4083–4090. IEEE (2018)
8 D. Burghardt and P. Lanillos
12. Lanillos, P., van Gerven, M.: Neuroscience-inspired perception-action in robotics:
applying active inference for state estimation, control and self-perception. arXiv
preprint arXiv:2105.04261 (2021)
13. Liu, B., Cheng, S., Shi, Y.: Particle ﬁlter optimization: A brief introduction pp.
95–104 (2016)
14. Meo, C., Lanillos, P.: Multimodal vae active inference controller. arXiv preprint
arXiv:2103.04412 (2021)
15. Millidge, B., Tschantz, A., Seth, A., Buckley, C.: Neural kalman ﬁltering. arXiv
preprint arXiv:2102.10021 (2021)
16. Oliver, G., Lanillos, P., Cheng, G.: An empirical study of active inference on a
humanoid robot. IEEE Transactions on Cognitive and Developmental Systems
(2021)
17. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.Y., et al.: Ros: an open-source robot operating system. In: ICRA workshop
on open source software. vol. 3, p. 5. Kobe, Japan (2009)
18. Sancaktar, C., van Gerven, M.A.J., Lanillos, P.: End-to-end pixel-based deep ac-
tive inference for body perception and action. 2020 Joint IEEE 10th Interna-
tional Conference on Development and Learning and Epigenetic Robotics (ICDL-
EpiRob) (Oct 2020). https://doi.org/10.1109/icdl-epirob48136.2020.9278105,
http://dx.doi.org/10.1109/ICDL-EpiRob48136.2020.9278105
19. Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L.: Understanding the limitations
of cnn-based absolute camera pose regression. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 3302–3312 (2019)
20. Thrun, S.: Simultaneous localization and mapping. In: Robotics and cognitive ap-
proaches to spatial mapping, pp. 13–41. Springer (2007)
21. C ¸ atal, O., Wauthier, S., Verbelen, T., Boom, C.D., Dhoedt, B.: Deep active infer-
ence for autonomous robot navigation (2020)