Robot Localization and Navigation through
Predictive Processing using LiDAR(cid:63)
D. Burghardt1 and P. Lanillos2
1 Radboud University, Houtlaan 4, 6525 XZ Nijmegen, NL
2 Donders Institute for Brain, Cognition and Behaviour, Department of Artificial
Intelligence, Radboud University, Nijmegen, NL
Abstract. Knowing the position of the robot in the world is crucial
fornavigation.Nowadays,Bayesianfilters,suchasKalmanandparticle-
based,arestandardapproachesinmobilerobotics.Recently,end-to-end
learning has allowed for scaling-up to high-dimensional inputs and im-
provedgeneralization.However,therearestilllimitationstoprovidingre-
liablelasernavigation.Hereweshowaproof-of-conceptofthepredictive
processing-inspired approach to perception applied for localization and
navigation using laser sensors, without the need for odometry. We learn
the generative model of the laser through self-supervised learning and
perform both online state-estimation and navigation through stochastic
gradientdescentonthevariationalfree-energybound.Weevaluatedthe
algorithm on a mobile robot (TIAGo Base) with a laser sensor (SICK)
inGazebo.Resultsshowedimprovedstate-estimationperformancewhen
comparingtoastate-of-the-artparticlefilterintheabsenceofodometry.
Furthermore,converselytostandardBayesianestimationapproachesour
method also enables the robot to navigate when providing the desired
goal by inferring the actions that minimize the prediction error.
Keywords: Predictive Processing · Robot localization · Robot naviga-
tion · Laser sensor · LiDAR.
1 Introduction
Localizationalgorithmsarepartofourdailylifeandcoreforrobotics.Recursive
Bayesian estimation composes the current state-of-art in the field and has been
essentialforthedevelopmentoflocalization,mapping,navigationandsearching
applications [20,10]. Bayesian filters [4], e.g., Kalman and particle filters, are
able to estimate the state of a system from noisy sensor observations formalized
as a hidden Markov model. These approaches are useful also in the case of
non-linear modeled systems and out-of-sequence measurements [2]. The particle
filter (PF) is an approximate Bayesian method that tractably computes the
posterior distribution of the state of any system given the observations. The
statedistributionisrepresentedbyindividualparticles,whichareevaluatedand
(cid:63) 2nd International Workshop on Active Inference IWAI2021, European Conference
on Machine Learning (ECML/PCKDD 2021)
1202
peS
9
]OR.sc[
1v93140.9012:viXra
2 D. Burghardt and P. Lanillos
weighted recursively. Particles with higher probability get bigger weights and
are re-sampled into more particles in its neighborhood, whereas particles with
smaller weights get fewer new samples close to them [13].
In recent years, novel approaches based on deep neural networks, have been
proposed to improve localization using high-dimensional inputs. Regression so-
lutions, for instance, compute the absolute position of the system from only vi-
sualinformation[8].However,thesemethodshaveloweraccuracythanprevious
approaches that exploit prior information, such as geometry [19]. In particular,
LiDAR-basedNavigationwithrepresentationlearning(e.g.,usingautoencoders)
and reinforcement learning has shown downgraded performance in navigation
tasks [7]. Alternatively to LiDAR-based approaches, [21] and [18] showed how
to apply deep active inference using camera images in robot navigation and
humanoid upper-body reaching respectively.
We describe how the predictive processing (PP) approach to perception [3,
11] can aid in localization and simple navigation tasks [12]. In this work naviga-
tionisperformedhavingtherobotmovebetweentwopointsinanunobstructed
environment, which can be further built upon to tackle more complex environ-
ments (e.g. mazes). Under PP, the agent, following the Free Energy Principle
(FEP) [5], tries to minimize the error in the predicted observations by either
performing corrective actions to match the expected internal state or by up-
dating this internal state based on what it has experienced through the senses.
In this work, we present a proof-of-concept based on the Pixel-Active Inference
model[18],proposedforhumanoidbodyperceptionandaction,toperformlaser-
based localization and navigation without the need for odometry. This has been
successfully applied to robot manipulator control to improve adaptation [14].
Our approach combines the power of deep networks regression with variational
Bayesian filtering to provide a better reliable state estimation than PFs in our
proof-of-concept environment—See Fig. 1.
2 Methods
2.1 Robot
The TIAGo Base mobile robot uses the SICK TiM571 laser sensor, which has a
0.0m−25m range and a 270◦ aperture angle. In all experiments we limited the
robot’s movement to 2 degrees of freedom, i.e., moving forward, backward and
sideways.
2.2 Localization
We define the true state (position) of the robot at instant k as x =(x,y)∈R2
k
and the position belief of the robot as x˜ . We further define the observation
k
o as the laser measurements. Estimation is solved by computing the posterior
k
distribution p(x|o) by optimizing the Variational Free Energy (VFE). The algo-
rithm is sketched in Fig. 2a. Under the mean-field and Laplace approximation
Localization and navigation through predictive processing using LiDAR 3
Fig.1: Proof-of-concept environment designed for the experiments in Gazebo.
In the localization experiment the robot true position is randomly set and the
initial position belief is initialized to the center of the map. Laser range finder
measurements are displayed as the blue shading.
this is equivalent to minimizing the error between the sensory input o and the
k
predicted sensory input oˆ . While regression approaches (Fig. 2b) compute the
k
pose directly from the visual input, stochastic neural filtering continuously re-
fines the state through an error signal. We perform state estimation through
perceptual inference, minimizing the VFE as follows:
x˜ =argminF(x˜,o)→x˜ =x˜ +α∂ g(x˜ )Σ−1(o −g(x˜ )) (1)
k+1 k x˜ k o k k
x˜
Where α is the step size and ∂ denotes the derivative with respect to x˜.
x˜
This is computed iteratively using gradient descent on the prediction error—
sensor measurement o minus the predicted sensory input g(x˜ )—weighted by
k k
the variance Σ . Both the predicted observations and the partial derivative of
o
the error are computed by means of a deep neural network forward pass and its
Jacobian [18], respectively.
2.3 Predicting the observations
We compute the sensor likelihood p(o |x˜ ) using a transposed convolutional
k k
neural network (Fig. 2), which augments the dimensionality of the input from
x to the laser-sensor input size (e.g., 2→622). The input is firstly fed into two
fully connected layers, and each transposed convolution layer is followed by a
regular convolution layer, based on the work of [18]. At every layer, we used the
ReLU activation function.
4 D. Burghardt and P. Lanillos
Fig.2:(a)ThePredictiveProcessingalgorithm’sarchitecture.(b)Graphicalrep-
resentation of regression approaches to robot state estimation. (c) Generative
model prediction vs. true laser measurement on a test sample.
The network was trained on 13000 normalized random samples collected in
the Gazebo simulation. Each sample consists of the true position of the robot
andthelaser-sensormeasurementsatthatlocation.Thetrainingwasperformed
with 20 batches of 500 samples, using the L1 loss and Adam optimizer [9].
2.4 Navigation
Analogously, our algorithm infers the action in the same way that state estima-
tioniscomputed,namelyperformingactive inference.Actionsalsominimizethe
VFE in the predicted observations.
a=argminF(x˜,o(a)) (2)
a
We define the goal as the preference or the intention of the agent x˜ to arrive
goal
to a sensory state o [16]. Estimation and control are computed as follows3:
goal
x˜ =x˜ +α (cid:2) ∂ g(x˜ )Σ−1(o −g(x˜ ))+∂ g(x˜ )Σ−1β(o −g(x˜ )) (cid:3) (3)
k+1 k x˜ k o k k x˜ k x k goal
a =a +γ∂ x˜∂ g(x˜ )Σ−1(o −g(x˜ )) (4)
k+1 k a x˜ k o k k
where β weights the goal attractor and γ is the action step size. Note that each
term computes the weighted prediction error mapped to the latent space.
The estimated state x˜, now biased by the desired goal, generates a new
predictedobservationateverynewiterationthatistransformedintoanactiona,
3 This update equation assumes that the Hessian of the goal dynamics is −1 as pro-
posed in [18].
Localization and navigation through predictive processing using LiDAR 5
which minimizes the VFE. Thus, performing a movement in the direction of the
goal. The pseudo-code described in Alg. 1 illustrates the process. The algorithm
converges when the observation fits the predicted laser sensor measurements.
Algorithm 1: FEP localization and navigation algorithm
x˜ ← initial belief;
o ←g(x˜ ); // Generate goal
goal goal
while true do
o ←Normalize(laser input);
k
oˆ ←g(x˜); // Predicted observation
k
x˜ ←Eq. 3;
a←Eq. 4;
PerformAction(a);
end
3 Results
We evaluated our laser-based Active Inference algorithm against a particle fil-
ter [13] in the Gazebo simulator, using a commercial mobile robot with a laser
rangefinder sensor (TIAGo Base, pmb-2) [1], interfaced with Robot Operating
System (ROS) [17]. All experiments were conducted in a designed corridor-like
map described in Fig. 1. Localization and navigation results are summarized in
Fig. 3.
3.1 Localization and estimation
Firstly, we evaluated the localization accuracy when initializing the robot to a
random position in the space when a map was given. Thus, testing absolute
positing with laser measurements in static situations. For the particle filter, the
particle initial probabilities were randomly spread in the environment and reset
at the beginning of every trial. In our algorithm, we initialized the initial belief
inthecenteroftheenvironment.Wecomputedthegroundtruthpositionalerror
in 100 trials for 50 iterations. Our algorithm converged much more consistently
to the true state over all trials, whereas the PF struggled to deliver consistent
results, as shown in Fig. 3 by the rather large standard deviation in the blue
shaded area. It is important to highlight that the PF is tuned for using the
robot’s odometry. However, for the sake of fair comparison solely laser-sensor
values were used as observations.
Secondly, we evaluated the localization performance when traversing the en-
vironmentfromonesidetotheother,byperformingsmallteleports(tooverride
odometry) to simulate robot movement while keeping the rotation angle con-
stant.Resultsshowedamorestableovertimestateestimationbyouralgorithm
6 D. Burghardt and P. Lanillos
Fig.3: Localization and navigation evaluation. Mean and standard deviation of
the positioning absolute error of our model (FEP) compared against the PF
algorithm.Furthermore,ourmodelisabletoperformnavigationusingthesame
Bayesian filtering framework. The green line shows the mean absolute error to
the goal.
whencomparedtothePF,whichseemedtosufferfromtheabsenceofodometry
information.
3.2 Navigation
For the assessment of the navigation algorithm’s performance, we ran an exper-
iment consisting of 50 trials in which the robot had to navigate from a starting
point to a goal position. The initial belief state was set to the robot’s initial
true position, to evaluate the performance of navigation without the effects of
localization in the first iterations. In every trial, both the initial position and
the goal state of the robot were chosen randomly, with the constraint that they
shouldbeatleast12meters(inGazebocoordinates)apartfromeachother.The
task was considered complete when the robot got in a range of 0.8m from the
target. The results are plotted in green in Fig. 3.
We observed that the robot initially quickly approximates the goal, with a
big drop in the distance to the goal in the first couple of iterations. As it gets
closer to the goal, the “velocity” of the robot (in the experiment described by
the step sizes) decreases. This is a result of the diminishing gradient in every
step of the algorithm, due to the stochastic gradient descent. Additionally, we
computed the average number of iterations that it took the algorithm to get in
thedesired0.8mrangeofthetarget.Overthe50trialsofsimilartraveldistance
(∼11m to ∼13.5m), the average number of iterations was 12.5. This number is
naturally closely related to the optimal step size found.
Localization and navigation through predictive processing using LiDAR 7
4 Conclusions
This work shows a proof-of-concept on how predictive processing, i.e. active in-
ference agents, can perform laser-based localization and navigation tasks. The
results obtained in the localization experiment, where we compared our ap-
proach against a state-of-the-art alternative (particle filter), show the potential
of predictive stochastic neural filtering in robot localization, and estimation in
general [6,15]. Furthermore, the navigation experiment showcased how to com-
pute actions as a dual filtering process. Nevertheless, at its current state, the
proposed algorithm suffers from a few deficiencies, most of which are related to
the learning of the generative model of the world. Besides currently requiring a
large dataset for training, the model is prone to mistake very similar objects in
theenvironment,giventhattheestimationofthenewstateisindependentfrom
theprevious.Additionally,becauseitisasupervisedmethodtrainedbeforethat
the robot can do any navigation, it is unable to cope with changing environ-
ments. All in all, the environment used in our experiments is rather simplistic
compared to demonstrations of current sota algorithms. Therefore, we foresee
further development and experimentation in terms of integration of odometry
information, the introduction of extra degrees of freedom and connection to the
robot’s non-linear dynamics.
References
1. Tiago base (Oct 2020), http://wiki.ros.org/Robots/TIAGo-base
2. Besada-Portas,E.,Lopez-Orozco,J.A.,Lanillos,P.,DelaCruz,J.M.:Localization
ofnon-linearlymodeledautonomousmobilerobotsusingout-of-sequencemeasure-
ments. Sensors 12(3), 2487–2518 (2012)
3. Clark, A.: Whatever next? predictive brains, situated agents, and the future of
cognitive science. Behavioral and brain sciences 36(3), 181–204 (2013)
4. Fox, V., Hightower, J., Liao, L., Schulz, D., Borriello, G.: Bayesian filter-
ing for location estimation. IEEE Pervasive Computing 2(3), 24–33 (2003).
https://doi.org/10.1109/MPRV.2003.1228524
5. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?Naturereviewsneu-
roscience 11(2), 127–138 (2010)
6. Friston,K.J.,Trujillo-Barreto,N.,Daunizeau,J.:Dem:avariationaltreatmentof
dynamic systems. Neuroimage 41(3), 849–885 (2008)
7. Gebauer, C., Bennewitz, M.: The pitfall of more powerful autoencoders in lidar-
based navigation. arXiv preprint arXiv:2102.02127 (2021)
8. Kendall, A., Grimes, M., Cipolla, R.: Posenet: A convolutional network for real-
time 6-dof camera relocalization. In: Proceedings of the IEEE international con-
ference on computer vision. pp. 2938–2946 (2015)
9. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)
10. Lanillos, P.: Minimum time search of moving targets in uncertain environments.
Ph.D. thesis, PhD thesis (2013)
11. Lanillos,P.,Cheng,G.:Adaptiverobotbodylearningandestimationthroughpre-
dictivecoding.In:2018IEEE/RSJInternationalConferenceonIntelligentRobots
and Systems (IROS). pp. 4083–4090. IEEE (2018)
8 D. Burghardt and P. Lanillos
12. Lanillos, P., van Gerven, M.: Neuroscience-inspired perception-action in robotics:
applying active inference for state estimation, control and self-perception. arXiv
preprint arXiv:2105.04261 (2021)
13. Liu, B., Cheng, S., Shi, Y.: Particle filter optimization: A brief introduction pp.
95–104 (2016)
14. Meo, C., Lanillos, P.: Multimodal vae active inference controller. arXiv preprint
arXiv:2103.04412 (2021)
15. Millidge, B., Tschantz, A., Seth, A., Buckley, C.: Neural kalman filtering. arXiv
preprint arXiv:2102.10021 (2021)
16. Oliver, G., Lanillos, P., Cheng, G.: An empirical study of active inference on a
humanoid robot. IEEE Transactions on Cognitive and Developmental Systems
(2021)
17. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.Y., et al.: Ros: an open-source robot operating system. In: ICRA workshop
on open source software. vol. 3, p. 5. Kobe, Japan (2009)
18. Sancaktar, C., van Gerven, M.A.J., Lanillos, P.: End-to-end pixel-based deep ac-
tive inference for body perception and action. 2020 Joint IEEE 10th Interna-
tional Conference on Development and Learning and Epigenetic Robotics (ICDL-
EpiRob) (Oct 2020). https://doi.org/10.1109/icdl-epirob48136.2020.9278105,
http://dx.doi.org/10.1109/ICDL-EpiRob48136.2020.9278105
19. Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L.: Understanding the limitations
of cnn-based absolute camera pose regression. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 3302–3312 (2019)
20. Thrun,S.:Simultaneouslocalizationandmapping.In:Roboticsandcognitiveap-
proaches to spatial mapping, pp. 13–41. Springer (2007)
21. C¸atal,O.,Wauthier,S.,Verbelen,T.,Boom,C.D.,Dhoedt,B.:Deepactiveinfer-
ence for autonomous robot navigation (2020)