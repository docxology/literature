arXiv:2302.05776v1  [cs.LG]  11 Feb 2023
Citation M. Prabhushankar and G. AlRegib, ”Stochastic Surprisal: An inferential measurement of
Free Energy in Neural Networks, ” Frontiers in Neuroscience - P erception Science , 17,
2023.
Review Data of Initial Submission : 22 April 2022
Date of First Revision : 13 Dec 2022
Date of Second Revision : 08 Feb 2023
Date of Acceptance: 09 Feb 2023
Codes
https://github.com/olivesgatech/Stochastic-Surprisal
Copyright ©2023 Prabhushankar and AlRegib. This is an open-access art icle distributed under the
terms of the Creative Commons Attribution License (CC BY). T he use, distribution or
reproduction in other forums is permitted, provided the ori ginal author(s) or licensor
are credited and that the original publication in this journ al is cited, in accordance with
accepted academic practice. No use, distribution or reprod uction is permitted which does
not comply with these terms.
Contact mohit.p@gatech.edu OR alregib@gatech.edu
https://ghassanalregib.info/
Stochastic Surprisal: An inferential
measurement of Free Energy in Neural
Networks
Mohit Prabhushankar ∗, Ghassan AlRegib
Omni Lab for Intelligent Visual Engineering and Science (OLIVES), Georgia
Institute of T echnology , Electrical and Computer Engineering, Atlanta, GA, USA
Correspondence*:
Mohit Prabhushankar
mohit.p@gatech.edu
ABSTRACT
This paper conjectures and validates a framework that allow s for action during inference in
supervised neural networks. Supervised neural networks ar e constructed with the objective to
maximize their performance metric in any given task. This is done by reducing free energy and
its associated surprisal during training. However, the bot tom-up inference nature of supervised
networks is a passive process that renders them fallible to n oise. In this paper, we provide a
thorough background of supervised neural networks, both ge nerative and discriminative, and
discuss their functionality from the perspective of free en ergy principle. We then provide a
framework for introducing action during inference. We intr oduce a new measurement called
stochastic surprisal that is a function of the network, the i nput, and any possible action. This
action can be any one of the outputs that the neural network ha s learnt, thereby lending
stochasticity to the measurement. Stochastic surprisal is validated on tw o applications: Image
Quality Assessment and Recognition under noisy conditions . We show that, while noise
characteristics are ignored to make robust recognition, th ey are analyzed to estimate image
quality scores. We apply stochastic surprisal on two applic ations, three datasets, and as a plug-
in on twelve networks. In all, it provides a statistically si gniﬁcant increase among all measures.
We conclude by discussing the implications of the proposed s tochastic surprisal in other areas
of cognitive psychology including expectancy-mismatch an d abductive reasoning.
Keywords: Free Energy Principle, Neural Networks, Stochas tic Surprisal, Image Quality Assessment, Robust Recogniti on, Human
Visual Saliency , Abductive Reasoning, Active Inference
1 INTRODUCTION
The human visual system is the resultant of an evolutionary p rocess inﬂuenced and constrained by
the natural visual stimuli present in the outside environme nt ( Geisler, 2008; Sebastian et al. , 2017).
The free energy principle is an over-arching theory that pro vides a mathematical framework for this
evolutionary process ( Friston, 2009). The principle provides a theory of cognition that can unif y and
discuss relationships among fundamental psychological co ncepts such as memory, attention, value,
reinforcement, and salience ( Friston, 2009). It decomposes the visual system into perception and actio n
modalities and argues that the visual system is an inference engine whose objective is to perceive the
1
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
outside environment as best as it can. If this perception is i nsufﬁcient for making an inference, an action is
taken to achieve the objective by inﬂuencing the outside env ironment. While the action is dependent on the
type of inference that is to be made, perception is dependent on the natural visual stimuli. Hence, a study
of the human visual system warrants a study of the patterns th at it is sensitive to. Broadly, these patterns
are classiﬁed under natural scene statistics ( Geisler, 2008). Color, luminance, spatio-temporal structures
and spectral residues are some statistics that are useful in performing fundamental visual tasks including
image quality assessment ( Zhang and Li , 2012), visual saliency detection ( Hou and Zhang , 2007), and
object detection and recognition ( Sebastian et al. , 2017).
Image quality assessment is the objective assessment of sub jective quality of images. V isual saliency
detection ﬁnds those regions in an image that attract signiﬁ cant human attention. Object recognition
attempts to recognize any given object in an image. Methods l ike ( Hou and Zhang , 2007; Murray et al. ,
2013) use spectral residue to detect salient regions. Hou and Zhang (2007) extend their spectral residue-
based saliency detection algorithm to show that object dete ction is possible. The spectral residual concept
is used in SR-SIM ( Zhang and Li , 2012) and BleSS ( T emel and AlRegib , 2016a) to utilize the frequency
characteristics to quantify residuals for IQA. All three di sparate applications share commonalities in their
spectral residual statistics that are used to show comparab le performance within each application. Hence,
natural scene statistics and their governing visual system principles are building blocks of computational
machine vision systems that attempt to mimic human percepti on.
One such a principle is the consistency in spatial structure s that allows for a sparse set of convolutional
kernels to represent natural scenes. Large-scale neural ne tworks are built on this principle. Neural
networks are empowered to mimic human vision by performing t he same tasks as the human visual
system including image quality assessment ( T emel et al. , 2016), visual saliency detection ( Sun et al. ,
2020), and object recognition ( Krizhevsky et al. , 2012) among others. Recently the generalization
capabilities of neural networks has led to their widespread adoption in a number of computational
ﬁelds. Neural networks have produced state-of-the-art res ults on multifarious data ranging from
natural images ( Krizhevsky et al. , 2012), computed seismic ( Shaﬁq et al. , 2018b,a), and biomedical
images ( Prabhushankar et al. , 2022; Prabhushankar and AlRegib , 2021b). In object recognition on
Imagenet dataset ( Deng et al. , 2009), He et al. (2016) surpassed the top- 5 human accuracy of 94. 9%. In the
application of image quality assessment, Bosse et al. (2017) extracted patch-wise distortion characteristics
from images using deep neural networks before fusing them to obtain an objective quality score. The
authors in Liu et al. (2017) device a sparse representation-based entropic measure of quality that is
inspired by the free energy principle. This is extended in Liu et al. (2019) where the authors use the
free energy principle as a plug-in on top of existing blind im age quality assessment techniques. In both
these works, free energy principle is seen as a technique tha t measures the disparity between an outside
environment and the the expectation of that environment thr ough some biologically plausible mechanism.
Other existing works, including ( Zhai et al. , 2011; Gu et al. , 2014), quantify this disparity to estimate
quality.
Hence, from the perspective of free energy principle, neura l networks act as biologically plausible
mechanisms to perceive the outside environment. This is don e by supervising the networks to learn
particular tasks. Prabhushankar and AlRegib (2021a) describe supervised learning as associative learning
where a set of learned features is associated with any given c lass. This class can be an objective score
in image quality assessment or an object class from recognit ion. The learned features are associated
with a speciﬁc dataset and application, and are not easily tr ansferable ( T emel et al. , 2018). A number
of recent works including ( T emel et al. , 2017; Goodfellow et al. , 2014; Hendrycks and Dietterich , 2019)
Frontiers 2
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
describe the fallibility of neural networks to adversarial noise and slight perturbations in data arising from
acquisition or environmental errors. The feature represen tation space can be altered signiﬁcantly by noise
that is sometimes non-noticeable in data. This is in contras t with the spectral residual feature which is used
to infer both object ( Hou and Zhang , 2007) and image quality ( Zhang and Li , 2012; T emel and AlRegib ,
2016a).
W e posit that these shortcomings of supervised neural netwo rks are a resultant of neural networks
exclusively utilizing the perception modality of free ener gy principle. In other words, the passivity
of neural networks during inference leads to their non-robu st nature. This view is corroborated
by Demekas et al. (2020) who identify three challenges in supervised learning. Fir stly, they claim
that neural networks lack an explicit control mechanism of i ncorporating prior beliefs into predictions.
Secondly, neural networks train via a scalar loss function t hat does not allow for incorporating uncertainty
in action. Lastly, neural networks do not perform any action during inference that would elicit changes in
the input from the outside environment.
In this paper, we tackle the above challenges by introducing a framework for action during inference.
This is opposed to the free energy principle based works in Liu et al. (2017, 2019) where the methodology
does not require actions at inference. Based on the free ener gy principle, we treat any trained neural
network as an inference engine. W e deﬁne a quantity called stochastic surprisal that is a function of a
neural network’s inference and some action performed on thi s inference. Reducing surprisal is generally
seen as a single action that reduces the distributional diff erence between two quantities. However, during
inference, we have access to only a single data point. W e over come this challenge by considering that
all possible actions that the network can undertake are equa lly likely. The term stochastic is derived
based on this assumption of action-randomness. Stochastic surprisal acts on top of any existing neural
networks to address the challenge of passive inference. Exi sting neural networks can either be generative
or discriminative. W e evaluate stochastic surprisal on two applications including image quality assessment
and robust object recognition. In image quality assessment , we evaluate our technique to assess the quality
of distorted images at different levels of distortions. Sim ilarly, in robust object recognition, we recognize
distorted images when the original neural network is only tr ained on pristine images. In other words, we
propose a concept that is able to assess the noise characteri stics in images to assign objective quality, as
well as ignore the same noise characteristics to robustly cl assify images. The contributions of this paper
include,
1.W e unify the concepts of image quality assessment and robu st object recognition. W e show that the
features that are extracted from neural networks simultane ously characterize the scene and context
within the image for recognition as well as the noise perturb ing it’s quality.
2.W e term our features as stochastic surprisal and relate them to the free energy principle. W e provide a
mathematical framework to extract stochastic surprisal fr om both discriminative and generative neural
networks as a function of some action.
3.W e discuss the implications of our proposed method from an abductive reasoning as well as expectancy-
mismatch perspective. Both these concepts lead to separate applications including context and relevance
based contrastive visual explanations and human visual sal iency detection.
W e ﬁrst describe the free energy principle in Section 2.1.1. The free energy principle is then
related to neural networks in Section 2.1.2 before describing stochastic surprisal. The generation of
stochastic surprisal in generative and discriminative net works is described in Sections 2.1.2.1 and 2.1.2.2
respectively. Finally, the applications of image quality a ssessment and robust recognition and our
Frontiers 3
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
methodology is discussed in Section 2.3. The results are provided in Section 3. W e further discuss the
implications of the proposed stochastic surprisal on other cognitive concepts and conclude in Section 4.
2 THEORETICAL OVERVIEW AND METHODOLOGY
In this section, we provide a thorough background of the free energy principle and its application in neural
networks, both generative and discriminative. W e then deﬁn e and detail the framework for the extraction of
stochastic surprisal. This is followed by the application o f stochastic surprisal in image quality assessment
and robust recognition.
2.1 Background
2.1.1 Free Energy Principle
The Free Energy Principle (FEP) proposes a theory to explain the self-organizing capability of any
intelligent and adaptive system (
Friston, 2009). FEP assumes the demarcation of a system that exists
in an environment through a functional Markov Blanket . The Markov Blanket ( Hip ´ olito et al., 2021)
provides statistical independence to the system from its en vironment, thereby imbuing the system with
a sense of self. A consequence of this separation is that the system only exp eriences the environment
through the Markov Blanket based on a limited set of sensory i nputs. These sensory inputs are used to
create a generative model of the outside environment within the system. The system then performs a
limited set of actions affecting the outside environment wh ile updating its internal model of the outside
environment. The FEP provides a mathematically concrete se t of principles to bound the long-term
entropy of the internal generative model that is conﬁned in t he set of all possible sensory inputs and
its possible performative actions. Friston (2019) argues that the assumption of the Markov Blanket and
the ensuing FEP is an overarching theory that provides a tool to study and explain self-organization at any
spatio-temporal scale from inﬁnitesimal quantum mechanic s to generational biological evolution.
In this paper, we are interested in the FEP’s application to v isual processes related to the human brain.
The applicability of FEP across concepts such as memory, att ention, value, and reinforcement ( Friston,
2009) is possible because of the central assumption that the limited sensory inputs from the outside
environment to the brain are also likely sensory inputs. In other words, the human brain only allows f or
a limited set of likely encounters ( Demekas et al. , 2020). The term likely is a function of the expectation
set by the internal generative model within the brain. Hence , the brain is considered to encode a bayesian
recognition density that predicts the sensory inputs based on some hypothesis regarding their cause. This
leads to the proposition that the brain is an inverse generat ive model where it expects to sense only a
limited set of likely inputs from the environment. Any misma tch to this expectation is handled in two
stages. Firstly, the internal model is updated with the mism atched sensory input to improve the perception.
Secondly, an action is performed to change the environment. This way, the environment and the model
are made to ﬁt each other by reducing the mismatched input. A m ismatched input is typically termed as
a surprising event ( Buckley et al. , 2017). Self-organization in the brain creates an imperative to m inimize
the surprisal of any event and the FEP provides a mathematical theory of thi s minimization by providing a
tractable upper bound to the surprisal. Mathematically, av erage surprisal is the entropy of the distribution
of all events. More unlikely an event, more surprisal it creates in the internal model. The free energy
decomposed using surprisal ( Demekas et al. , 2020) is given by,
Free Energy = Divergence + Surprisal. (1)
Frontiers 4
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Figure 1. Block diagram for perception (in blue) and proposed action ( in red) for a sparse autoencoder.
The image x is taken from the CURE-TSR dataset ( T emel et al. , 2017). The training loss function is
J(θ, φ ). The latent representation z = fθ(·) is zg. The reconstructed image is shown as ˆx. This forms the
perception pipeline. The action pipeline is shown in red whe re the action Ag is backpropagated through
the decoder to the latent representation space. The learned blue perception representation space zg changes
to the action space za
g as a consequence of Ag. This change is stochastic surprisal, given by ∂Ag(x, ˆx)
∂φ .
Here, divergence is the difference between the variables re presenting the outside environment that
generate the sensory inputs and the variables in the interna l generative model that mimic the outside
world.
2.1.2 Free Energy Principle in Neural Networks
The assumption of the existence of an internal tractable gen erative model that is an inference engine
has been adopted in the construction of early neural network s. Hinton and Zemel (1993) describe the
Helmholtz free energy that is used to construct autoencoder s as agents that minimize the reconstruction
cost and the code cost. The code cost is a function of the entro py of the probability distribution
given a vector. In FEP , this code cost is the surprisal. V aria tional Autoencoders ( Kingma and W elling ,
2019) minimize V ariational Free Energy (VFE) and consequently s urprisal. VFE is a generalization
of the Helmholtz free energy where the divergence of the appr oximate and true probabilities are
minimized ( Gottwald and Braun , 2020). While the generative models of autoencoders lend themsel ves
directly to the FEP , the discriminative models also train th emselves using some variation of a loss function
that resembles free energy. In this paper, we use both genera tive and discriminative models and we
introduce them in terms of the free energy principle.
2.1.2.1 Generative Networks
In this section, we consider a general autoencoder as our gen erative model. An autoencoder is an
unsupervised learning network which learns a regularized r epresentation of inputs to reconstruct them
as its output (
Hinton and Zemel , 1993; Kwon et al. , 2019). Since Hinton and Zemel (1993), a number
of variations have been proposed to autoencoders to constru ct either application-speciﬁc or property-
speciﬁc networks. These variations generally deal with con straining the latent representations learned
by an autoencoder. For instance, Ng (2011) constrain the latent representation to be sparse, thereby
constructing sparse autoencoders. Kingma and W elling (2013) constrain the latent representation to
follow a Gaussian distribution. These are termed as variati onal autoencoders. These are two instances
Frontiers 5
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
of property-speciﬁc autoencoders. Application-speciﬁc a utoencoders include fully-connected networks
used for image compression ( Gedeon and Harris , 1992), and convolutional autoencoders for image
denoising ( Mao et al. , 2016).
All these autoencoders consist of the same base architectur e as shown in Fig. 1. They consist of
an encoder fθ(·), parameterized by θ to map inputs x to a latent representation zg. These latent
representations zg are used to reconstruct the same input ˆx using a decoder gφ (·). This operation is
mathematically represented as,
z = fθ(x) ˆ x = gφ (z) = gφ (fθ(x)), (2)
For a natural image input, x ∈ RH×W ×C, where H, W, C are height, width, channel of input image,
respectively. The encoder and decoder are trained jointly b y minimizing a loss function J(θ, φ ) deﬁned
as:
J(θ, φ ) = L(x, g φ (fθ(x))) + Ω( zg; θ, φ ), (3)
where L is a reconstruction error which measures the dissimilarity between the input x, and the
reconstructed image ˆx. Ω is a regularization term added to avoid overﬁtting the netwo rk to the training set
and to imbue the required constraints. For a sparse autoenco der, Ω is an l1 sparsity constraint. However,
since the l1 constraint is not differentiable, a practical solution for constructing this sparsity constraint is
to use KL-Divergence on zg. Speciﬁcally, the sum of zg is constrained to either zero or a very small value
using a distance metric like KL-Divergence. This is shown in Fig. 1 in blue.
During training, the network parameters, θ and φ are updated by backpropagating the gradients of J(θ, φ )
w .r.t. the parameters. The update rule is given by,
θ′ = θ − ∂J (θ, φ )
∂θ , φ ′ = φ − ∂J (θ, φ )
∂φ , (4)
The two gradients provide the change in the network paramete rs required to incorporate better perception
capabilities as measured by the loss function J(θ, φ ).
Consider Eq. 3 and compare this against the free energy decomposition in Eq . 1. The L reconstruction
error measures the divergence. The regularization is the su rprisal. T echnically, regularization prevents
the network from reconstructing x exactly. Hence, surprisal is added in generative networks to make
them generalizable. A thorough analysis of regularization for reconstruction and feature transfer of
autoencoders to multiple tasks is provided in Prabhushankar et al. (2018). While regularization impacts
the reconstruction negatively, it enhances the adaptabili ty and usability of features for generalized tasks
and test sets.
2.1.2.2 Discriminative Networks
Discriminative networks are neural networks whose functio n is to assign labels to input data. While
the required training data in generative networks are image s x ∈ RH×W ×C, the training data for
discriminative networks are (x, y ), where x ∈ RH×W ×C and y ∈ [1, N ]. Here, y is an integer label
assigned to x, ranging between 1 and the total number of classes N. The goal of a discriminative
network is to assign the label y, given x at inference. The simplest discriminative network is an ima ge
classiﬁcation network. Consider an L-layered network f(·) trained to classify images on a domain X . For
the task of classiﬁcation, where f(·) is trained to classify between N classes, the last layer is commonly
a fully connected layer consisting of N weights or ﬁlters. During inference, the representation sp ace
Frontiers 6
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
zd = fL−1(x) after the ﬁrst (L − 1) layers are projected independently onto each of the N ﬁlters. The
ﬁlter with the maximum projection is inferred as the class ˆy to which x belongs. Mathematically, zd and
ˆy are related as,
zd = fL−1(x), (5)
˜y = arg max( W T
L zd + bL), ˆy = arg max(˜y) (6)
∀WL ∈ ℜ dL− 1×N ,z ∈ ℜ dL− 1×1, b L ∈ ℜ N×1, ˜y ∈ ℜ N×1, ˆy ∈ [1, N ], (7)
where WL and bL are the parameters of the ﬁnal fully connected layer. Note ou r choice of the variable zd.
This is a similar variable that is used to denote the latent re presentation in Eq. 2. Similar to the decoder
gφ (·) acting on zg in generative networks, we have the ﬁnal fully connected lay er WL and bL acting on zd.
This forms the perception pipeline that classiﬁes x as ˆy. This is shown in blue in Fig. 2.
Training an image classiﬁcation technique requires a loss f unction J(ˆy, y ; θ), where θ are the network
parameters and (x, y ) are the image-label pairs required for training. A common ch oice of J(·) is the
cross-entropy loss. Considering σ(˜y) to be the softmax probability distribution of the output vec tor from
f(·), the cross-entropy loss interms of KL-Divergence and entro py can be expressed as,
J(·) = KL(y||σ(˜y)) −
N∑
i=1
(σ( ˜yi))ln(σ( ˜yi)). (8)
Here, KL (||) refers to the KL-divergence between the probability output of the network and the label
vector y expressed as a one-hot probability distribution. Notice th e similarity between Eqs. 1 and 8. The
divergence in the FEP is the KL divergence and the surprisal i s the entropy given by the second term in
Eq. 8. Unlike the generative networks, surprisal is not introduc ed into the network. Rather, the existing
surprisal is minimized. A number of foundational works in FE P ( Friston, 2009, 2019) use the entropy of
a distribution to describe free energy. The network is then t rained by backpropagating the errors w .r.t θ
similar to Eq. 4.
2.1.2.3 Terminologies
Before describing our contributions, we summarize a few key terminologies that are extensively used
within the FEP setup and how they relate to neural networks.
External state of the world X is the observed distribution of the outside world and each x ∈ X is an
instance of this distribution. When describing discrimina tive systems, data is denoted as (x, y ) where x is
the data point and y is its label. When dealing with generative models, data is x only. When there is some
distortion associated with the outside environment, the sa mpled data is x′ and the distribution is X ′. W e
will see X ′ and x′ in IQA and recognition experiments when input data are disto rted by noise.
System A neural network f(·) trained on a distribution X . A trained system is one that does not take in
any external inputs to change or update its weights. W e consi der that a trained system is at NESS density.
For a discriminative network, f(·) is the entire system and its training data is denoted by (x, y ). For a
generative network, fθ(·) is an encoder trained to produce a latent representation spa ce zg given data
denoted by x and gφ (·) is the decoder trained to reconstruct the image given a laten t representation zg.
Frontiers 7
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Figure 2. Block diagram for perception (in blue) and proposed action ( in red) for a classiﬁcation network.
The image x is taken from the CURE-TSR dataset ( T emel et al. , 2017). The perception pipeline is shown
in blue where the network assigns a class 3 to x. The action pipeline is shown in red where the action
Ad, a = 1 is backpropagated through the ﬁnal fully connected layer to the learned blue perception
manifold zd. zd changes to the action manifold za
d as a consequence of Ad. This change is stochastic
surprisal, given by ∂Ad(ˆy,i ;W )
∂WL
.
Markov Blanket The part of the system that produces the latent representati on z. In a generative system
the markov blanket is the encoder fθ(·) and in a discriminative system, the markov blanket is the ini tial
part of the network from Eq. 5, fL−1(·).
Internal State of the system Let z denote the internal state of the latent representation with in a system.
Given a generative network, the latent representation afte r the encoder, zg = fθ(x) is the internal state.
Given a discriminative network, the internal state is zd = fL−1(x). The internal states of both the networks
are interchangeably referred to as latent representations or as perception manifolds. Note that similar to
external state, if an input x is distorted to x′, its internal state is also distorted and we will use either
z′
d or z′
g to denote the internal state of the system. Given any action, a, the internal state shifts to za to
accommodate this action without necessarily changing x. All these states are shown in Figs. 1 and 2.
2.2 Stochastic Surprisal
During inference, the networks are passive. As discussed in Section 1 and noted by Demekas et al.
(2020), there is no mechanism to include a non-scalar surprisal th at allows for an action during inference.
In this paper, we alleviate this challenge by deﬁning a new qu antity called stochastic surprisal as a
function of a hypothetical action. Consider the difference s in the existing deﬁnitions of surprisal. In
generative networks from Eq. 3, surprisal is the induced regularization that prevents ove rﬁtting and creates
speciﬁc constraints for a latent representation zg. In discriminative networks from Eq. 8, surprisal is
the entropy of the network’s predicted distribution obtain ed from a linear combination on zd. While the
surprisal in Eq. 1 deals with bounding the system’s surprise of the distributi onal divergence between the
internal model and external environment, the regularizati on-based and entropy-based deﬁnitions provide
a mathematically-tractable deﬁnition in neural networks. In this paper, we provide a new mathematically-
tractable deﬁnition of surprisal that is inherently a funct ion of an action A and its effect on the network.
A formal deﬁnition is provided ﬁrst.
Deﬁnition 2.1 (Stochastic Surprisal) . Given a trained neural network fθ(·) parameterized by θ, the
gradient change ∂A
∂θ with respect to the network parameters for all possible acti ons A from the perspective
of fθ(·) is termed stochastic surprisal.
Frontiers 8
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Stochastic surprisal measures the change required in the tr ained perception network to measure any
given action A. It is stochastic since it does not measure the divergence be tween distributions but rather a
single data point’s inﬂuence on the network. It is a non-scal ar value that acts on the network parameters
according to Eq. 4. Note that we do not actually update the network. Rather, we o nly measure the network
update and use it as a surprisal quantity. This update is poss ible based on some action all of which are
considered equally likely. A thorough discussion of the nam ing is provided in Section 4.1.
2.2.1 Action and Stochastic Surprisal
Action is a function of any application. W e ﬁrst deﬁne it in a g eneral fashion for generative and
discriminative networks. In Section 2.3, we deﬁne it speciﬁcally for image quality assessment and ro bust
recognition.
2.2.1.1 Generative Networks
The action in generative networks is straightforward. Give n an image x and its reconstructed image
ˆx, the possible action is to change the weight parameters in a w ay that reduces the disparity between x
and ˆx. In this paper, we quantify this disparity as the Mean Square Error given by ∥x − ˜x∥2
2. However,
as described in Section
2.1.2.1, the surprisal is present in the regularization terms. Henc e, any action
performed has to account for this surprisal. In this paper, w e use the elastic net regularization. The overall
action that induces a change in the network is given by,
Ag = ∥x − ˆx∥2
2 + β
h∑
j=1
KL(zj||ˆρj) + λ∥W ∥2
2. (9)
where Ag is a generative action. ∥x − ˆx∥2
2 is the MSE loss function, and ∥W ∥2
2 is the regularization on
the weights. ∑ h
j=1 KL(zj||ˆρj) is the sparsity constraint denoted as the divergence betwee n the latent
representation and some small value ˆρj, j ∈ [1, h ] where h is the size of the latent representation.
By minimizing the KL divergence, the latent variables zj, j ∈ [1, h ] are made sparse. β and λ are
hyperparameters controlling the regularization.
Stochastic surprisal is the the gradient of this action Ag with respect to the decoder weights. The action
pipeline along with the stochastic surprisal generation is shown in Fig.
1 in red. At inference, a test
image is passed through a trained network and reconstructed . The action from Eq. 9 is calculated and
backpropagated to the latent representation space zd. The change, measured as the gradients, creates a
change in zd and the new action manifold is termed za
d . A toy example of the geometric interpretation of
this change is also shown Fig. 1. The blue perception manifold zg that reconstructs ˆx is acted on by Ag to
obtain a new red action manifold za
d. The decoder can use this space to reconstruct x exactly. In Section 3,
we show how these generated gradients can be used as features for image quality assessment. Note that
we keep the perception pipeline as is and make no changes to th e training process.
2.2.1.2 Discriminative Networks
The action Ad in discriminative networks is more involved than generativ e networks. While in generative
networks, the possible action is to reconstruct the image wi th higher ﬁdelity, in discriminative networks,
the action can take any one of N outcomes. At inference, discriminative networks are given an image x
and asked to predict its label y. Assuming that ˆy is the prediction, the action we use to elicit change in the
Frontiers 9
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
network parameters is by backpropagating an action class a in the loss function J(ˆy, a ; W ), a ∈ [1, N ].
Ad = ∥ai − ˜y∥2
2, i ∈ [1, N ]. (10)
Here ai is the action class deﬁned as a Kronecker delta function give n by,
ai =
{
1, if i = class,
0, otherwise (11)
There is no regularization added to the discriminative acti on since the probability distribution σ(˜y) derived
from ˜y is a function of its surprisal entropy. Note that we use an MSE function for Ad in Eq.
10 similar
to Ag from Eq. 9. An important difference between Eqs. 9 and 10 is the number of possible actions. In
discriminative networks that classify between N classes, there are N possible i in Eq. 10. Hence, there are
N possible actions Ad and N possible surprisals ∂Ai
d
∂WL
, ∀i ∈ [1, N ]. The action pipeline for discriminative
network for a toy example where the predicted class is 3 and the action class is 1 is shown in Fig. 2 in
red. The surprisals are the red gradients from the ﬁnal fully connected layer. W e also show the geometric
interpretation of a given action on the learned representat ion space zd. The blue perception manifold is
acted upon by A1
d through ∂A1
d
∂WL
to obtain the red action manifold. Note that there are N such possible
red za
d due to the N possible actions. This idea of N separate gradients to characterize data is not new .
In Settles et al. (2007), the authors construct positive and negative instance lab els for a given input x in a
binary decision setting. This is done to quantify uncertain ty in an active learning setting. In this paper, we
extend this characterization to N-label settings and use the image-label pairs to extract sto chastic surprisal
from the network.
Notice the difference in the deﬁnitions of action. In FEP , th e generative model acts on the outside world
creating a change that reduces its surprisal. Our deﬁnition in Eq. 10 is the same one that is used in I-
FGSM ( Goodfellow et al. , 2014) adversarial generation technique. Eq. 10 is continuously applied and a
gradient w .r.t. the input, i.e. ∂Ad
∂x , is added to x until the prediction changes adversarially. Changing the
input would be a true action from the FEP sense. However, in th is paper, we do not explicitly change the
outside world or x. Rather, we measure the effect of such a change on the network using ∂Ad
∂WL
without
making said change.
2.3 Methodology
W e validate the effectiveness of stochastic surprisal duri ng inference on two applications: Image Quality
Assessment (IQA) and Robust Classiﬁcation. The action grad ients, ∂A
∂φ are used in two ways. The ﬁrst
approach is to use the surprisal gradients as error directio ns. This is done by projecting images with
and without distortions onto the gradient space and compari ng them. In this case, the surprisal acts as
a measurement between the images and acts as a Full-Referenc e IQA metric. The second approach is
to directly use surprisal gradients as feature vectors. The directional change caused by the actions is
dependent on the network, the input and the action class. By k eeping the network same across action
classes, surprisal becomes a characteristic of the data. Th is approach is explored for the application of
robust classiﬁcation.
2.3.1 Image Quality Assessment
Image quality assessment is a ﬁeld of image processing that o bjectively estimates the
perceptual quality of a degraded image. Multiple methods ha ve been proposed to predict the
Frontiers 10
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Figure 3. Block diagram of the proposed framework of IQA as a plug-in on top of T emel et al. (2016).
subjective quality of images ( Ponomarenko et al. , 2011; W ang et al. , 2004, 2003; W ang and Li , 2011;
Sampat et al. , 2009; Zhang and Li , 2012; Zhang et al. , 2011; Mittal et al. , 2012; T emel and AlRegib ,
2019; Prabhushankar et al. , 2017a, 2018). All these methods extract structure related hand-crafte d features
from both reference and distorted images and compare them to predict the quality. Recently, machine
learning models directly extract features from images T emel et al. (2016); Prabhushankar et al. (2017b);
Bosse et al. (2017). The authors in ( Bosse et al. , 2017) propose to do so in either the presence or absence
of the original pristine image. In Ma et al. (2021), the authors propose a free energy inspired technique
to predict the quality. They use a Generative-Adversarial N etwork as the base perception module and an
additional CNN to model content and degradation dependent c haracteristics. In this paper, we approach
the action module in FEP as a function of the perception modul e itself. W e do so by extracting stochastic
surprisal from the same perception network. Hence, our meth od acts as a plug-in on top of existing quality
estimators. In this paper, we show quantitative results by p lugging-in on top of UNIQUE ( T emel et al. ,
2016) and qualitative results on top of Bosse et al. (2017). W e ﬁrst describe and motivate the usage of
UNIQUE for quantitative results.
UNIQUE: W e choose UNIQUE as the base technique since it follows the ge nerative process described
in Section
2.1.2.1 and Fig. 1. This allows for the generation of stochastic surprisal fro m Eq. 3 based
on the Action in Eq. 9. The authors in T emel et al. (2016) train a sparse autoencoder with a one layer
encoder and decoder and a sigmoid non-linearity on 100, 000 patches of size 8 × 8 × 3 extracted from
ImageNet ( Deng et al. , 2009) testset. The autoencoder is trained with MSE reconstructi on loss. This
network is f(·) from Eq. 3. UNIQUE follows a full reference IQA workﬂow which assumes a ccess to
both reference and distorted images while estimating quali ty. The reference and distorted images are
converted to YGCr color space and converted to 8 × 8 × 3 patches. These patches are mean subtracted and
ZCA whitened before being passed through the trained encode r. The activations of all reference patches in
the latent space are extracted and concatenated. Activatio ns lesser than a threshold of 0. 025 are suppressed
to 0. The choice of threshold 0. 025 is made based on the sparsity coefﬁcient used during trainin g. Similar
procedure is followed for distorted image patches. The supp ressed and concatenated features of both the
reference and distorted images are compared using Spearman correlation. The resultant is the estimated
quality of the distorted image.
Frontiers 11
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
T able 1. Structure of H(·) for different ResNet architectures as f(·).
NE T WORK f(·) ST RUCT URE OF H(·) - A L L L AYE RS S E PARAT E D BY S IGMOID
RE SNE T-18,34 640 × 300 − 300 × 100 − 100 × 10
RE SNE T-50, 101 2560 × 300 − 300 × 100 − 100 × 10
2.3.1.1 Proposed Methodology
W e provide the block diagram for the proposed methodology in Fig. 3. Both the pristine and distorted
images go through the same pre-processing steps detailed in UNIQUE ( T emel et al. , 2016) and are
projected onto the stochastic surprisal gradients of the de coder. The gradients ∂Ag
∂φ are extracted by
backpropagating Eq. 9. In this paper, we use the same hyperparameters β = 3 , λ = 3 e−3, and ρj = 0 . 035
as used in T emel et al. (2016). Once projected, the resultant is passed through an invers e sigmoidal layer
to obtain the latent representation. Note that the latent re presentation is zg for the pristine image and z′
g for
the distorted image. Once passed through the inversion laye r, both the magnitude and phase of each latent
representation is concatenated and their spearman correla tion coefﬁcient is taken to estimate the quality
score of the image.
2.3.2 Robust Classiﬁcation
The goal is to characterize an image x using all N actions. Consider an image x whose class as predicted
by fθ(·) is ˆy. Stochastic surprisal of x against class 1 is provided by backpropagating a loss between ˆy
and 1 and obtaining corresponding gradients. The gradient is pro portional to Ad(ˆy, 1; WL), where W is
the weight parameters and 1 is the action class. Speciﬁcally, it is ∇WL Ad(ˆy, 1; WL) for weights in layer
L and class i ∈ [1, N ]. W e backpropagate over all N classes to obtain the overall surprisal features across
all classes. The ﬁnal feature, rx for an image x, is given by concatenating all individual features and rx is
characteristic of image x. Hence,
ri = ( ∇WL Ad(ˆy, i ; WL))), ∀i ∈ [1, N ],
rx = [ r1, r 2 . . . r N ].
(12)
Given a trained feed-forward network f(·) and image x, we extract gradients using Eq.
12
which serve as our features. Gradients as features are used i n diverse applications including visual
explanations ( Selvaraju et al. , 2017; Prabhushankar et al. , 2020; Prabhushankar and AlRegib , 2021b),
adversarial attacks ( Goodfellow et al. , 2014), anomaly detection ( Kwon et al. , 2020), and image quality
assessment ( Kwon et al. , 2019) among others. In this work, we use gradients as features to c haracterize
data.
MLP ( H(·)): Once rx is obtained for all N classes, the surprisal feature is now analogous to zd from Eq.
5.
However, rx is of dimensionality ℜ(N×dL− 1)×1 since it is a concatenation of N gradients. T o account for
the larger dimension size, we classify rx by training an MLP H(·) on top of rx derived from training data.
In this paper, we use a simple three layered MLP as H(·) with sigmoid activations. The exact structure of
the MLP is dependent on dL−1 of the base f(·) network and is given in T able 1 for ResNets 18,34,50, and
101 ( He et al. , 2016) that are considered in Section 3.
T raining H(·): The concatenated rx features for all training data are extracted and normalized . H(·) is
trained on all training rx using the same training procedure as the perception network f(·). H(·) is trained
Frontiers 12
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
for 200 epochs with SGD optimizer and cross-entropy loss, momentum = 0 . 9, weight decay = 5 e − 4,
and adaptive learning rates of 0. 1, 0. 02, 0. 004 changed at epochs 60, 120, 160 respectively.
T esting using f(·) and H(·): During test time, the proposed framework operates in three s teps. In step 1,
as shown in Eq. 13, the given image passes through the perception network to pr ovide a coarse estimation
ˆy. In step 2, the stochastic surprisal features rx are extracted according to Eq. 14 and concatenated. In step
3, rx is normalized and passed through the MLP H(·) to obtain the ﬁnal prediction ˜y. This is shown in
Eq. 15.
ˆy = arg max f(x), (13)
rx = [( ∇WL MSE(ˆy, δ i
i)), ∀i ∈ [1, N ]], (14)
˜y = H(rx), (15)
Note that we substituted Ad in Eq. 14 with the MSE formulation of action from Eq. 10.
3 RESUL TS
3.1 Image Quality Assessment
W e report the results of the our proposed method in compariso n with commonly cited methods in this
section. W e ﬁrst discuss the the datasets used for compariso n as well as the evaluation metrics. W e ﬁnally
show the results in T able 2 and discuss these results.
Datasets W e compare our proposed quality estimation technique on thr ee datasets - MULTI-LIVE (Jayaraman et al. ,
2012), TID2013 (Ponomarenko et al. , 2015), and DR IQA (Athar and W ang , 2023). W e choose
MULTI-LIVE and TID2013 datasets for two reasons. Firstly, our proposed technique i s a plug-in
approach on top of an existing technique ( T emel et al. , 2016). Hence, it is imperative to compare against
and show results on datasets that were used in T emel et al. (2016). Secondly, the two datasets provide
access to seven categories of distortion among ﬁve levels. T his is useful in comparison against the
recognition experiments discussed in Section 3.2 which follows a similar setup. The complex distortions
can either be a combination of multiple distortions such as d istortions generated in the MUL TI-LIVE
dataset Jayaraman et al. (2012) or the human visual system (HVS) speciﬁc peculiar distorti ons such as
the ones presented in the TID2013 (Ponomarenko et al. , 2015) dataset. A more challenging scenario is
presented in DR IQA dataset, where the authors conjecture a degraded reference setting for image quality
assessment. In this setting, pristine images are unavailab le as a reference. Instead, singly distorted images
are used as reference to construct IQA metrics for multiply d istorted images. In T able 2, we provide results
for DR IQA dataset as DRv1 and DRv2 based on the author’s division of the dataset. Each of DRv1
and DRv2 have 31, 790 multiply distorted images and 1, 122 singly distorted images. Additionally, this
dataset does not have true subjective quality scores from humans but is derived from a s ynthetic quality
benchmark. This synthetic score uses existing Full Referen ce metrics for quality generation including
some of comparisons in T able 2.
Evaluation metrics The performance is validated using outlier ratio (consiste ncy), root mean square
error (accuracy), Pearson correlation (linearity), Spear man correlation (rank), and Kendall correlation
(rank). Arrows next to each metric in T able 2 indicate the desirability of a higher number ( ↑) or a lower
number(↓). Statistical signiﬁcance between correlation coefﬁcien ts is measured with the formulations
suggested in ITU-T Rec. P .1401 ITU-T (2012) and provided below each correlation coefﬁcient. A 0 value
Frontiers 13
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
T able 2. Overall performance of image quality estimators.Databases
PSNR SSIM MS CW IW SR FSIM FSIMc BRIS BIQI BLII Per CSV UNI COHER SUM Proposed
HA SSIM SSIM SSIM SIM QUE NDS2 SIM QUE ENSI MER
Outlier Ratio (OR, ↓)
MUL TI 0.013 0.016 0.013 0.093 0.013 0.000 0.018 0.016 0.067 0.024 0.078 0.004 0.000 0.000 0.031 0.000 0.000
TID13 0.615 0.734 0.743 0.856 0.701 0.632 0.742 0.728 0.851 0.856 0.852 0 .655 0.687 0.640 0.833 0.620 0.620
Root Mean Square Error (RMSE, ↓)
MUL TI 11.320 11.024 11.275 18.862 10.049 8.686 10.866 10.794 15.058 12.744 17.419 9.898 9.895 9.258 14.806 8.212 7.943
TID13 0.652 0.762 0.702 1.207 0.688 0.619 0.710 0.687 1.100 1.108 1.092 0.643 0.647 0.615 1.049 0.630 0.596
DRv1 16.19 17.11 16.17 17.18 14.02 13.64 12.98 13.24 - - - 16.01 15.07 13.59 21.82 16.98 13.85
DRv2 16.47 16.42 15.76 17.48 14.04 13.17 12.82 12.92 - - - 16.23 15.35 13.19 21.57 17.59 13.24
Pearson Linear Correlation Coefﬁcient (PLCC, ↑)
MUL TI 0.801 0.813 0.803 0.380 0.847 0.888 0.818 0.821 0.605 0.739 0 .389 0.852 0.852 0.872 0.622 0.901 0.908
-1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0
TID13 0.851 0.789 0.830 0.227 0.832 0.866 0.820 0.832 0.461 0.449 0 .473 0.855 0.853 0.869 0.533 0.861 0.877
-1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 0 -1 -1
DRv1 0.731 0.693 0.732 0.586 0.800 0.819 0.833 0.830 - - - 0.738 0.738 0.820 0.432 0.698 0.800
-1 -1 -1 -1 0 1 1 1 - - - -1 -1 1 -1 -1
DRv2 0.709 0.702 0.738 0.521 0.799 0.826 0.836 0.833 - - - 0.720 0.720 0.825 0.417 0.658 0.815
-1 -1 -1 -1 -1 0 1 1 - - - -1 -1 0 -1 -1
Spearman’s Rank Correlation Coefﬁcient (SRCC, ↑)
MUL TI 0.715 0.860 0.836 0.631 0.884 0.867 0.864 0.867 0.598 0.611 0.386 0.818 0.849 0.867 0.554 0.884 0.887
-1 0 -1 -1 0 0 0 0 -1 -1 -1 -1 -1 0 -1 0
TID13 0.847 0.742 0.786 0.563 0.778 0.807 0.802 0.851 0.414 0.393 0 .396 0.854 0.846 0.860 0.649 0.856 0.865
-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 0 -1 0
DRv1 0.739 0.702 0.738 0.760 0.798 0.807 0.823 0.820 - - - 0.742 0.769 0.810 0.518 0.706 0.807
-1 -1 -1 -1 -1 0 1 1 - - - -1 -1 0 -1 -1
DRv2 0.720 0.705 0.738 0.755 0.795 0.809 0.819 0.816 - - - 0.727 0.755 0.813 0.525 0.672 0.816
-1 -1 -1 -1 -1 -1 0 0 - - - -1 -1 0 -1 -1
Kendall’s Rank Correlation Coefﬁcient (KRCC, ↑)
MUL TI 0.532 0.669 0.644 0.457 0.702 0.678 0.673 0.677 0.420 0.440 0.268 0.624 0.655 0.679 0.399 0 .698 0.702
-1 0 0 -1 0 0 0 0 -1 -1 -1 -1 0 0 -1 0
TID13 0.666 0.559 0.605 0.404 0.598 0.641 0.629 0.667 0.286 0.270 0 .277 0.678 0.654 0.667 0.474 0.667 0.677
0 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 0 0 0 -1 0
DRv1 0.534 0.505 0.537 0.559 0.597 0.609 0.629 0.626 - - - 0.537 0.563 0.609 0.357 0.503 0.605
-1 -1 -1 -1 0 0 1 1 - - - -1 -1 0 -1 -1
DRv2 0.517 0.509 0.539 0.553 0.595 0.613 0.626 0.623 - - - 0.525 0.594 0.613 0.342 0.475 0.616
-1 -1 -1 -1 -1 0 1 0 - - - -1 -1 0 -1 -1
Frontiers 14
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
corresponds to statistically similar performance, -1 means the method is statistically inferior to proposed
method, and 1 indicates that the method is statistically superior to prop osed method. T wo best performing
methods for each metric are highlighted.
Results W e compare our proposed stochastic surprisal-based UNIQUE against other image
quality estimators based only on handcrafted features and p erception pipeline in T able
2. These
compared full reference estimators include PSNR-HA ( Ponomarenko et al. , 2011), SSIM ( W ang et al. ,
2004), MS-SSIM ( W ang et al. , 2003), CW -SSIM ( Sampat et al. , 2009), IW -SIM ( W ang and Li ,
2011), SR-SIM ( Zhang and Li , 2012), FSIM ( Zhang et al. , 2011), FSIMc ( Zhang et al. , 2011),
PerSIM ( T emel and AlRegib , 2015), CSV ( T emel and AlRegib , 2016b), UNIQUE ( T emel et al. ,
2016). W e also compare against no reference metrics including BR ISQUE ( Mittal et al. , 2012),
BIQI ( Moorthy and Bovik , 2010), and BLIINDS2 ( Saad et al. , 2012). All these techniques were
also comapred against the base UNIQUE algorithm in T emel et al. (2016). In addition to
these, we compare against new estimators including COHEREN SI ( T emel and AlRegib , 2019) and
SUMMER ( T emel and AlRegib , 2019). SUMMER beats UNIQUE among six of the ten categories. Note
that we do not show results for BRISQUE, BIQI, and BLIINDS2 fo r DR IQA dataset since NR methods,
that are generally trained on singly distorted images, exhi bit a large performance gap on multiply distorted
images ( Athar and W ang , 2023).
The proposed stochastic surprisal-based method plugs on to p of UNIQUE and its results are provided
under the last column in T able 2. It is always in the top two methods for MULTI-LIVE and TID2013
datasets in all evaluation metrics. In particular, the prop osed method achieves the best performance for
all the categories except in OR and KRCC in TID2013 dataset. UNIQUE, by itself, does not achieve the
best performance for any of the metrics in MULTI dataset. However, the same network using the proposed
gradient features signiﬁcantly improves the performance a nd achieves the best performance on all metrics.
For instance, UNIQUE is the third best performing method in MULTI dataset in terms of RMSE, PLCC,
SRCC, and KRCC. However, the action-based features improve the performance for those metrics by
1. 315, 0. 036, 0. 020, and 0. 023, respectively and achieve the best performance for all metr ics. This further
reinforces the plug-in capability of the proposed method du ring inference. On DR IQA dataset, FSIM
and FSIMc perform the best across all categories. The author s in Athar and W ang (2023) used FSIMc to
construct DR IQA models. However, the proposed algorithm re mains competitive among all evaluation
metrics. The results are statistically signiﬁcant in 53 of the 78 compared metrics across both DRv1 and
DRv2. Note that a number of these compared FR-IQA metrics hav e been utilized to construct the synthetic
ground truth quality scores.
3.2 Robust Classiﬁcation
Neural networks are sensitive to distortions in test that th e network was not privy to during
training (
T emel et al. , 2017, 2018; Hendrycks and Dietterich , 2019). These distortions include image
acquisition errors, environmental conditions during acqu isition, transmission and storage errors among
others. CIFAR-10C (Hendrycks and Dietterich , 2019) dataset consists of 19 real world distortions each
of which has ﬁve levels of degradation that distort the 10000 images in CIFAR-10 testset. Neural
networks that use perception-only mechanics suffer perfor mance accuracy drops on CIFAR-10C. Current
techniques that alleviate the drop in perception-only accu racy require additional training data. The
authors in V asiljevic et al. (2016) show that ﬁnetuning or retraining networks using distorte d images
increases the performance of classiﬁcation under the same d istortion. However, performance between
different distortions is not generalized well. For instanc e, training on gaussian blurred images does
Frontiers 15
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
T able 3. Stochastic surprisal-based plug-in on top of existing robu stness techniques.
ME T HODS ACCURACY
RE SNE T-18 PE RCE P T ION -ONLY 67.89%
PROP OS E D 71.4 %
DE NOIS ING PE RCE P T ION -ONLY 65.02%
PROP OS E D 68.86 %
ADVE RS ARIAL TRAIN (HE NDRYCKS AND DIE T T E RICH , 2019 ) P E RCE P T ION -ONLY 68.02%
PROP OS E D 70.86 %
SIM CLR ( CHE N E T AL ., 2020 ) P E RCE P T ION -ONLY 70.28%
PROP OS E D 73.32 %
AUGME NT NOIS E (VAS IL JE VIC E T AL ., 2016 ) P E RCE P T ION -ONLY 76.86%
PROP OS E D 77.98 %
AUGMIX (HE NDRYCKS E T AL ., 2019 ) P E RCE P T ION -ONLY 89.85%
PROP OS E D 89.89 %
not guarantee a performance increase in motion blur images ( Geirhos et al. , 2018b). Other proposed
methods include training on style-transferred images ( Geirhos et al. , 2018a), training on adversarial
images ( Hendrycks and Dietterich , 2019), training on simulated noisy virtual images ( T emel et al. , 2017),
and self-supervised methods like SimCLR Chen et al. (2020) that train by augmenting distortions.
Augmix ( Hendrycks et al. , 2019) creates multiple chains of augmentations to train the base network. All
these works require additional training data. Our proposed stochastic surprisal-based technique is a plug-
in on top of any existing method that increases the base netwo rk’s robustness to distortions without any
need for new data.
Experimental setup and dataset: W e use CIFAR-10C (
Hendrycks and Dietterich , 2019) as our dataset
of choice with all its 95 distortions and degradation levels. ResNet-18,34,50, and 101 ( He et al. , 2016)
architectures are used as the base f(·) perception-only networks. These are trained from scratch o n
CIFAR-10 dataset. Following the terminologies established in Secti on 2, X is the training set of
CIFAR-10 and X ′ are the 19 distorted domains in which the testing set of CIFAR-10C reside. Each
of the 19 corruptions have 5 levels of distortions. Higher the level, higher is the disto rtion. The distortions
include blur characteristics like gaussian blur, zoom blur , glass blur, and environmental distortions like
rain, snow , fog, haze among others.
Comparison against existing State of the Art Methods: In T able
3, we compare the T op- 1 accuracy
between perception-only inference and our proposed stocha stic surprisal-based inference. All the state-
of-the-art techniques require additional training data - n oisy images ( V asiljevic et al. , 2016), adversarial
images ( Hendrycks and Dietterich , 2019), self-supervision SimCLR augmentations ( Chen et al. , 2020),
and augmentation chains ( Hendrycks et al. , 2019). W e term these perception-only techniques as f′(·)
and we actively infer on top of them. For all f′(·) other than Augmix, the base network is a ResNet-
18. For Augmix, we use WideResNet architecture following th e authors in Hendrycks et al. (2019).
Another commonly used robustness technique is to pre-proce ss the noisy images to denoise them.
Denoising 19 distortions is, however, not a viable strategy assuming tha t the characteristics of the
distortions are unknown. W e use Non-Local Means ( Buades et al. , 2011) denoising and the results
obtained are lower than the perception-only accuracy by alm ost 3%. However, the proposed technique
on this model increases the results by 3. 84%. W e create untargeted adversarial images using FGSM
attack ( Goodfellow et al. , 2014) and use them to train a ResNet-18 architecture. In the exper imental
Frontiers 16
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Figure 4. V isualization of accuracy gains (in red) of using the propos ed stochastic surprisal-based
inference over perception-only inference (in red) on CIFAR-10C dataset ( Hendrycks and Dietterich ,
2019) for four networks across 19 distortions.
setup of augmenting noise ( V asiljevic et al. , 2016), we augment the training data of CIF AR-10 with six
distortions provided by T emel et al. (2018) to randomly distort 500 CIF AR-10 training images to train
f′(·). For all techniques, the proposed technique plugs on top of f′(·) and increases the accuracy to create
robust networks. Note that in all the perception-only metho ds in T able 3, we do not use the augmented
data to train H(·). The gain obtained is by creating actions on only the undisto rted data. Even when the
augmented network f′(·) gains on non-augmented f(·), the proposed technique plugs on top of f′(·) to
provide additional gains.
Analyzing distortion-wise accuracy gains: The results of all four ResNet architectures for each of the
19 distortions is shown in Fig 4. X-Axis in each plot shows 19 distortions averaged over all 5 distortion
levels. Y-Axis shows T op-1 accuracy. The bars in blue show perception-only inference r esults and the
red region in each bar represents the performance gain obtai ned by stochastic surprisal-based inference.
There is an increase in performance across distortions and n etworks. In 9 of the 19 distortions, the
proposed method averages 4% more than its perception-only counterpart. These include g aussian blur,
gaussian noise, glass blur, impulse noise, motion blur, pix elate, shot noise, speckle noise, and zoom blur.
The highest increase is 8. 22% for glass blur. In 2 of the distortions, brightness and saturate, the results
increase by less than 0. 4% averaged over all levels. This is because of the statistics t hat the distortions
affect. Distortions can change either the local or global st atistics within images. Distortions like saturate,
brightness, contrast, fog, and frost change the low level or global statistics in the image domain. Neural
Frontiers 17
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Figure 5. V isualization of accuracy gains (in red) of using the propos ed stochastic surprisal-based
inference over perception-only inference (in blue) on CIFAR-10C dataset ( Hendrycks and Dietterich ,
2019) for four networks (a) averaged across 19 distortions and 5 levels (b) shown across 5 levels of
distortion.
networks are actively trained to ignore such changes so that their effects are not propagated beyond
the ﬁrst few layers. Hence, gradients derived from the ﬁnal f ully connected layer do not capture the
necessary changes required within f(·) to compensate for these distortions. Therefore, both the pr oposed
and perception-only inference follow each other closely in distortions like brightness and saturate.
Level-wise Recognition on CIF AR-10C: In Fig.
5b, the proposed performance gains for the four
networks are categorized based on the distortion levels. Al l 19 categories of distortion on CIFAR-10C are
averaged for each level and their respective perception-on ly accuracy and stochastic surprisal-based gains
are shown. Note that the levels are progressively more disto rted. Hence, level 1 distribution X ′ is similar
to the training distribution X when compared to level 5 distributions. As the distortion le vel increases,
the proposed method’s accuracy gains also increase. This is because, with a larger distributional shift,
more characteristic is the action required w .r.t. the netwo rk parameters. In Fig. 5a, we show the distortion-
wise and level-wise accuracy gains for each network. Note th at, a stochastic surprisal-based ResNet-18
performs similarly to a perception-only ResNet-50.
4 DISCUSSION
W e conclude this paper by considering the terminology of sto chastic surprisal as well as some of the
broader implications of the proposed technique. These incl ude the abductive reasoning module and
expectancy-mismatch hypothesis in cognitive science.
4.1 Choice of the terminology of Stochastic Surprisal
W e motivate the terminology of stochastic surprisal in two ways:
1.As an analogy to gradient descent and stochastic gradient descent : Gradient descent requires
the gradients from the all available training data to update the weights. However, since this is
computationally infeasible for large neural networks, sto chastic gradient descent allows using a single
Frontiers 18
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
training datapoint to estimate gradients, repeated across all data. In stochastic surprisal , we use the
single data point, available at inference, under all allowa ble actions to estimate surprisal.
2.Meaning of stochastic: The word stochastic implies some r andomness within the setting. This
randomness is derived from the possible set of all actions. I n discriminative networks in Eq. 10,
ai, i ∈ [1, N ] is the set of all possible actions with N being the number of trained classes. This suggests
that we allow a datapoint to be any available class, all of whi ch are equally likely. Similarly, in generative
networks in Eq. 9, we add random perturbations at the output of the autoencode r. Hence, there is an
inherent randomness within the actions that allow for the us age of the word stochastic.
4.2 Abductive Reasoning
The free energy principle postulates that the brain encodes a Bayesian recognition density that predicts
sensory data based upon some hypotheses about their causes. This mode of inference is called inference
to the best explanation. The underlying reasoning model is a bductive reasoning. Abductive reasoning was
introduced by the philosopher Charles Sanders Peirce ( Peirce, 1931), who saw abduction as a reasoning
process from effect to cause ( Paul, 1993). An abductive reasoning framework creates a hypothesis an d
tests its validity without considering the cause. A hypothe sis can be considered as an answer to one
of the three following questions: a causal ‘Why P?’ question, a counterfactual ‘What if?’ question, and a
contrastive ‘Why P , rather than Q?’ question ( AlRegib and Prabhushankar , 2022). Here P is the prediction
and Q is any contrast class. The action considered in this paper is the latter contrastive question of the
form ‘Why P , rather than Q?’ . Stochastic surprisal measures the answer to this question . W e explore this
further in AlRegib and Prabhushankar (2022); Prabhushankar et al. (2020). W e borrow the visualization
procedure from Prabhushankar et al. (2020) to visually analyze stochastic surprise in the applicatio ns of
IQA and recognition in Fig. 6. W e do so to illustrate the broader impact of action at infere nce time. As in
Section 2.3.1.1, we use stochastic surprisal as a plug-in approach.
Figure 6. Stochastic surprisal answers contrastive questions. The h ighlighted regions in each image
provides a visual explanation to the question beneath it. Wh ile Grad-CAM ( Selvaraju et al. , 2017) shows
all the perceived regions in the image, the stochastic surpr isal provides ﬁne-grained answers to contrastive
questions. Best viewed in color.
Frontiers 19
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
For IQA visualizations, we use a trained full-reference met ric DIQaM-FR Bosse et al. (2017) as our
perception model. In Fig. 6a, the pretrained network from Bosse et al. (2017) provides a quality score of
0. 58 to the distorted lighthouse image. Here 0. 58 acts as P in the contrastive question. W e use MSE loss
function as Ad and a real number Q ∈ [0, 1] to calculate stochastic surprisal. Contrastive explanati ons
of Q values including 0. 25, 0. 75, and 1 along with Grad-CAM results are shown in Fig. 6a. Grad-CAM
highlights the entire image indicating that the network est imates the quality based on the whole image.
While this builds trust in the network, it does not help us und erstand the network decision. The stochastic
surprisal, however, provides ﬁne-grained explanations. C onsider the contrastive questions asking why the
quality is neither 1 nor 0. 75. The network estimates this to be primarily due to distortio ns concentrating in
the foreground portion of the image. This explanation is inl ine with previous works in IQA that posit that
distortions in the more salient foreground or edge features cause a larger drop in perceptual quality than
that in color or background ( Prabhushankar et al. , 2017b)(Chandler, 2013). When the contrastive question
asks why the prediction is not 0. 25, the network highlights the sky indicating its good quality for a higher
score of 0. 58.
Fig. 6b shows the contrastive questions answered by the stochasti c surprisal for the application of
recognition. Given an image of a spoonbill from ImageNet dat aset ( Deng et al. , 2009), a VGG-16
network highlights the body, feathers, legs and beak of the b ird in the Grad-CAM ( Selvaraju et al. ,
2017) explanation. Consider a more ﬁne grained contrastive ques tion regarding the difference between a
spoonbill and ﬂamingo. The stochastic surprisal highlight s regions in the neck of the spoonbill indicating
that the contrast between the input spoonbill image and the n etwork’s notion of a ﬂamingo lies in the
spoonbill’s lack of S-shaped neck. Similarly, the contrast between a spoonbill and a crane is in the color
of the spoonbill’s feathers. The contrast between a pig and a spoonbill is in the shape of neck and legs in
the spoonbill which is emphasized. All these visualization s serve to illustrate the stochastic nature of the
proposed method. It is stochastic in the sense that it indivi dually depends on the network, the data, as well
as the action. In this case, the action of not predicting a ﬂam ingo has a different explanation compared to
the action of not predicting a pig.
4.3 Expectancy-Mismatch
The expectancy-mismatch hypothesis in cognitive science i s a way to quantify and analyze human
attention. According to this hypothesis, human attention m echanism suppresses expected messages and
focuses on the unexpected ones (
Summerﬁeld and Egner , 2009; Krebs et al. , 2012; Horstmann et al. ,
2016; Horstmann, 2002; Becker and Horstmann , 2011; Sun et al. , 2020). Becker and Horstmann (2011)
shows that a message which is unexpected, captures human att ention. Then, the human visual system
establishes whether the input matches the observers’ expec tation. If they are conﬂicting, error neurons
in the human brain encode the prediction error and pass the er ror message back to the representational
neurons. The proposed method uses gradients with respect to the network parameters to measure an action.
In both the generative and discriminative networks, this ac tion takes the form of a change in the output
thereby creating a mismatch with the network’s expected res ult. Hence, the proposed method can act as a
framework for exploring expectancy-mismatch in future wor ks.
4.4 Related Learning Paradigms
The proposed stochastic surprisal decomposes the decision making and training process of a neural
network into perception and action phases. A number of other machine learning paradigms including
continual and lifelong learning ( Parisi et al. , 2019), online learning ( Hoi et al. , 2021), and introspective
learning ( Prabhushankar and AlRegib , 2022) also have multiple stages. Online learning assumes an
Frontiers 20
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
exploration and exploitation stage in a neural network’s tr aining process. Hence, the differentiation in
the training stages is based on time rather than the proposed action. Continual and lifelong learning is
a research paradigm that tackles the topic of catastrophic f orgetting when a neural network is trained
to perform multiple tasks. Introspective learning conject ures reasons in the form of counterfactual or
contrastive questions in its two stages to make predictions . Hence, while there are multiple machine
learning paradigms that conjecture decomposition of neura l network’s training and decision processes,
the proposed framework that is based on the FEP is unique in it s decomposition. The ﬁeld of active
learning ( Logan et al. , 2022; Benkert et al. , 2022) involves actions within the training and decision making
processes. However, active learning requires actions from the users while the considered actions in the
proposed methodology are with respect to the neural network .
4.5 Conclusion
In this paper, we examine supervised learning from the persp ective of Free Energy Principle. The
learning process of both generative and discriminative mod els can be decomposed into divergence and
surprisal measures. Surprisal is introduced in generative models via regularization and constraints that
allow a generative aspect to their functionality. While thi s complicates the action itself, the set of possible
actions is still limited. Discriminative networks follow t he traditional route of free energy minimization
by deﬁning surprisal in terms of recognition entropy and min imizing it. This allows the action itself to
be a simple ﬁdelity-based reconstruction error. However, i n discriminative networks, there are N set of
possible actions, N being the number of classes in the recognition density. W e ac count for both these
peculiarities in deﬁning our action space. W e use a ﬁdelity- based MSE loss for both generative and
discriminative networks. In addition, generative network s are reinforced with KL-divergence based elastic
net regularization, and in discriminative networks we back propagate N possible actions. W e measure
this scalar action quantity in terms of a vector quantity cal led stochastic surprisal that is a function
of the network parameters and an individual data point rathe r than a distribution. W e use stochastic
surprisal to assess distortions in image quality assessmen t and disregard distortions in robust recognition.
W e then discuss the implications of stochastic surprisal in other areas of cognitive science including
abductive reasoning and expectancy-mismatch. A computati onal bottleneck within the framework is the
consideration of all N possible actions to estimate the surprisal feature rx. rx scales linearly with N
thereby becoming prohibitive on datasets with a large numbe r of classes. Selecting only a subset of the
most likely actions is one plausible solution to the challen ge of scalability.
REFERENCES
AlRegib, G. and Prabhushankar, M. (2022). Explanatory para digms in neural networks. arXiv preprint
arXiv:2202.11838
Athar, S. and W ang, Z. (2023). Degraded reference image qual ity assessment. IEEE Transactions on
Image Processing
Becker, S. I. and Horstmann, G. (2011). Novelty and saliency in attentional capture by unannounced
motion singletons. Acta Psychologica 136, 290 – 299. doi:https://doi.org/10.1016/j.actpsy .20 10.12.002
Benkert, R., Prabhushankar, M., and AlRegib, G. (2022). For getful active learning with switch events:
Efﬁcient sampling for out-of-distribution data. In 2022 IEEE International Conference on Image
Processing (ICIP) (IEEE), 2196–2200
Bosse, S., Maniry, D., M ¨ uller, K.-R., Wiegand, T ., and Same k, W . (2017). Deep neural networks for
no-reference and full-reference image quality assessment . IEEE Transactions on image processing 27,
206–219
Frontiers 21
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Buades, A., Coll, B., and Morel, J.-M. (2011). Non-local mea ns denoising. Image Processing On Line 1,
208–212
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017 ). The free energy principle for action
and perception: A mathematical review . Journal of Mathematical Psychology 81, 55–79
Chandler, D. M. (2013). Seven challenges in image quality as sessment: past, present, and future research.
ISRN Signal Processing 2013
Chen, T ., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning
of visual representations. arXiv preprint arXiv:2002.05709
Demekas, D., Parr, T ., and Friston, K. J. (2020). An investig ation of the free energy principle for emotion
recognition. Frontiers in Computational Neuroscience 14, 30
Deng, J., Dong, W ., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. (2009). Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recogni tion
(Ieee), 248–255
Friston, K. (2009). The free-energy principle: a rough guid e to the brain? Trends in cognitive sciences 13,
293–301
Friston, K. (2019). A free energy principle for a particular physics. arXiv preprint arXiv:1906.10184
Gedeon, T . and Harris, D. (1992). Progressive image compres sion. In [Proceedings 1992] IJCNN
International Joint Conference on Neural Networks (IEEE), vol. 4, 403–407
Geirhos, R., Rubisch, P ., Michaelis, C., Bethge, M., Wichma nn, F . A., and Brendel, W . (2018a). Imagenet-
trained cnns are biased towards texture; increasing shape b ias improves accuracy and robustness. arXiv
preprint arXiv:1811.12231
Geirhos, R., T emme, C. R., Rauber, J., Sch ¨ utt, H. H., Bethge , M., and Wichmann, F . A. (2018b).
Generalisation in humans and deep neural networks. In Advances in Neural Information Processing
Systems. 7538–7550
Geisler, W . S. (2008). V isual perception and the statistica l properties of natural scenes. Annu. Rev . Psychol.
59, 167–192
Goodfellow , I. J., Shlens, J., and Szegedy, C. (2014). Expla ining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572
Gottwald, S. and Braun, D. A. (2020). The two kinds of free ene rgy and the bayesian revolution. PLoS
computational biology 16, e1008420
Gu, K., Zhai, G., Y ang, X., and Zhang, W . (2014). Using free en ergy principle for blind image quality
assessment. IEEE Transactions on Multimedia 17, 50–63
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual l earning for image recognition. In
Proceedings of the IEEE conference on computer vision and pa ttern recognition . 770–778
Hendrycks, D. and Dietterich, T . (2019). Benchmarking neur al network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261
Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. (2019).
Augmix: A simple data processing method to improve robustne ss and uncertainty. arXiv preprint
arXiv:1912.02781
Hinton, G. E. and Zemel, R. (1993). Autoencoders, minimum de scription length and helmholtz free
energy. Advances in neural information processing systems 6
Hip ´ olito, I., Ramstead, M. J., Convertino, L., Bhat, A., Fr iston, K., and Parr, T . (2021). Markov blankets
in the brain. Neuroscience & Biobehavioral Reviews 125, 88–97
Hoi, S. C., Sahoo, D., Lu, J., and Zhao, P . (2021). Online lear ning: A comprehensive survey.
Neurocomputing 459, 249–289
Frontiers 22
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Horstmann, G. (2002). Evidence for attentional capture by a surprising color singleton in visual search.
Psychological Science 13, 499–505. doi:10.1111/1467-9280.00488. PMID: 1243083 2
Horstmann, G., Becker, S., and Ernst, D. (2016). Perceptual salience captures the eyes on a surprise trial.
Attention, P erception, & Psychophysics 78, 1889–1900
Hou, X. and Zhang, L. (2007). Saliency detection: A spectral residual approach. In 2007 IEEE Conference
on computer vision and pattern recognition (Ieee), 1–8
ITU-T (2012). P .1401: Methods, metrics and procedures for statistical ev aluation, qualiﬁcation and
comparison of objective quality prediction models . T ech. rep., ITU T elecom. Stand. Sector
Jayaraman, D., Mittal, A., Moorthy, A. K., and Bovik, A. C. (2 012). Objective quality assessment of
multiply distorted images. In Asilomar Conf. Sig. Syst. Comp. 1693–1697
Kingma, D. P . and W elling, M. (2013). Auto-encoding variati onal bayes. arXiv:1312.6114
Kingma, D. P . and W elling, M. (2019). An introduction to vari ational autoencoders. arXiv preprint
arXiv:1906.02691
Krebs, R., Fias, W ., Achten, E., and Boehler, C. (2012). Stim ulus conﬂict and stimulus novelty trigger
saliency signals in locus coeruleus and anterior cingulate cortex. In Front. Hum. Neurosci. Conference
Abstract: Belgian Brain Council. doi: 10.3389/conf. fnhum . vol. 114
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Ima genet classiﬁcation with deep convolutional
neural networks. Advances in neural information processing systems 25
Kwon, G., Prabhushankar, M., T emel, D., and AlRegib, G. (201 9). Distorted representation space
characterization through backpropagated gradients. In 2019 IEEE International Conference on Image
Processing (ICIP) (IEEE), 2651–2655
Kwon, G., Prabhushankar, M., T emel, D., and AlRegib, G. (202 0). Backpropagated gradient
representations for anomaly detection. In European Conference on Computer V ision (Springer),
206–226
Liu, Y ., Gu, K., Zhang, Y ., Li, X., Zhai, G., Zhao, D., et al. (2 019). Unsupervised blind image quality
evaluation via statistical measurements of structure, nat uralness, and perception. IEEE Transactions on
Circuits and Systems for V ideo T echnology 30, 929–943
Liu, Y ., Zhai, G., Gu, K., Liu, X., Zhao, D., and Gao, W . (2017) . Reduced-reference image quality
assessment in free-energy principle and sparse representa tion. IEEE Transactions on Multimedia 20,
379–391
Logan, Y .-y., Prabhushankar, M., and AlRegib, G. (2022). De cal: Deployable clinical active learning.
arXiv preprint arXiv:2206.10120
Ma, J., Wu, J., Li, L., Dong, W ., Xie, X., Shi, G., et al. (2021) . Blind image quality assessment with
active inference. IEEE Transactions on Image Processing 30, 3650–3663
Mao, X., Shen, C., and Y ang, Y .-B. (2016). Image restoration using very deep convolutional encoder-
decoder networks with symmetric skip connections. Advances in neural information processing systems
29
Mittal, A., Moorthy, A. K., and Bovik, A. C. (2012). No-refer ence image quality assessment in the spatial
domain. IEEE Trans. Image Proc. 21, 4695–4708
Moorthy, A. K. and Bovik, A. C. (2010). A two-step framework f or constructing blind image quality
indices. IEEE Sig. Proc. Let. 17, 513–516
Murray, N., V anrell, M., Otazu, X., and Parraga, C. A. (2013) . Low-level spatiochromatic grouping for
saliency estimation. IEEE transactions on pattern analysis and machine intellig ence 35, 2810–2816
Ng, A. (2011). Sparse autoencoder. CS294A Lecture notes 72, 1–19
Frontiers 23
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and W ermter , S. (2019). Continual lifelong learning with
neural networks: A review . Neural Networks 113, 54–71
Paul, G. (1993). Approaches to abductive reasoning: an over view . Artiﬁcial intelligence review 7, 109–
152
Peirce, C. S. (1931). Collected papers of charles sanders peirce (Harvard University Press)
Ponomarenko, N., Ieremeiev, O., Lukin, V ., Egiazarian, K., and Carli, M. (2011). Modiﬁed image visual
quality metrics for contrast change and mean shift accounti ng. In Proc. CADSM . 305–311
Ponomarenko, N., Jin, L., Ieremeiev, O., Lukin, V ., Egiazar ian, K., Astola, J., et al. (2015). Image
database tid2013: Peculiarities, results and perspective s. Signal Processing: Image Communication 30,
57–77
Prabhushankar, M. and AlRegib, G. (2021a). Contrastive rea soning in neural networks. arXiv preprint
arXiv:2103.12329
Prabhushankar, M. and AlRegib, G. (2021b). Extracting caus al visual features for limited label
classiﬁcation. In 2021 IEEE International Conference on Image Processing (IC IP) (IEEE), 3697–3701
Prabhushankar, M. and AlRegib, G. (2022). Introspective le arning: A two-stage approach for inference
in neural networks. arXiv preprint arXiv:2209.08425
Prabhushankar, M., Kokilepersaud, K., Logan, Y .-y., Coron a, S. T ., AlRegib, G., and W ykoff, C.
(2022). Olives dataset: Ophthalmic labels for investigati ng visual eye semantics. arXiv preprint
arXiv:2209.11195
Prabhushankar, M., Kwon, G., T emel, D., and AIRegib, G. (201 8). Semantically interpretable and
controllable ﬁlter sets. In 2018 25th IEEE International Conference on Image Processin g (ICIP) (IEEE),
1053–1057
Prabhushankar, M., Kwon, G., T emel, D., and AlRegib, G. (202 0). Contrastive explanations in neural
networks. In 2020 IEEE International Conference on Image Processing (IC IP) (IEEE), 3289–3293
Prabhushankar, M., T emel, D., and AlRegib, G. (2017a). Gene rating adaptive and robust ﬁlter sets using
an unsupervised learning framework. In 2017 IEEE International Conference on Image Processing
(ICIP) (IEEE), 3041–3045
Prabhushankar, M., T emel, D., and AlRegib, G. (2017b). Ms-u nique: Multi-model and sharpness-
weighted unsupervised image quality estimation. Electronic Imaging 2017, 30–35
Saad, M. A., Bovik, A. C., and Charrier, C. (2012). Blind imag e quality assessment: A natural scene
statistics approach in the dct domain. IEEE Trans. on Image Proc. 21, 3339–3352
Sampat, M. P ., W ang, Z., Gupta, S., Bovik, A. C., and Markey, M . K. (2009). Complex wavelet structural
similarity: A new image similarity index. IEEE Trans. Image Proc. 18, 2385–2401
Sebastian, S., Abrams, J., and Geisler, W . S. (2017). Constr ained sampling experiments reveal principles
of detection in natural scenes. Proceedings of the National Academy of Sciences 114, E5731–E5740
Selvaraju, R. R., Cogswell, M., Das, A., V edantam, R., Parik h, D., and Batra, D. (2017). Grad-cam:
V isual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE
international conference on computer vision . 618–626
Settles, B., Craven, M., and Ray, S. (2007). Multiple-insta nce active learning. Advances in neural
information processing systems 20
Shaﬁq, M., Prabhushankar, M., and AlRegib, G. (2018a). Leve raging sparse features learned from
natural images for seismic understanding. In 80th EAGE Conference and Exhibition 2018 (European
Association of Geoscientists & Engineers), vol. 2018, 1–5
Frontiers 24
Mohit Prabhushankar et al. Stochastic Surprisal in Neural Networks
Shaﬁq, M. A., Prabhushankar, M., Di, H., and AlRegib, G. (201 8b). T owards understanding common
features between natural and seismic images. In SEG T echnical Program Expanded Abstracts 2018
(Society of Exploration Geophysicists). 2076–2080
Summerﬁeld, C. and Egner, T . (2009). Expectation (and atten tion) in visual cognition. Trends in Cognitive
Sciences 13, 403 – 409. doi:https://doi.org/10.1016/j.tics.2009. 06.003
Sun, Y ., Prabhushankar, M., and AlRegib, G. (2020). Implici t saliency in deep neural networks. In 2020
IEEE International Conference on Image Processing (ICIP) (IEEE), 2915–2919
T emel, D. and AlRegib, G. (2015). PerSIM: Multi-resolution image quality assessment in the perceptually
uniform color domain. In IEEE Int. Conf. Image Proc. 1682–1686
T emel, D. and AlRegib, G. (2016a). Bless: Bio-inspired low- level spatiochromatic similarity assisted
image quality assessment. In 2016 IEEE International Conference on Multimedia and Expo ( ICME)
(IEEE), 1–6
T emel, D. and AlRegib, G. (2016b). CSV: Image quality assess ment based on color, structure, and visual
system. Sig. Proc.: Image Comm. 48, 92 – 103. doi:http://dx.doi.org/10.1016/j.image.201 6.08.008
T emel, D. and AlRegib, G. (2019). Perceptual image quality a ssessment through spectral analysis of error
representations. Sig. Proc.: Image Comm.
T emel, D., Kwon, G., Prabhushankar, M., and AlRegib, G. (201 7). Cure-tsr: Challenging unreal and real
environments for trafﬁc sign recognition. arXiv preprint arXiv:1712.02463
T emel, D., Lee, J., and AlRegib, G. (2018). Cure-or: Challen ging unreal and real environments for object
recognition. In 2018 17th IEEE International Conference on Machine Learnin g and Applications
(ICMLA) (IEEE), 137–144
T emel, D., Prabhushankar, M., and AlRegib, G. (2016). UNIQU E: Unsupervised image quality estimation.
IEEE Sig. Proc. Let. 23, 1414–1418
V asiljevic, I., Chakrabarti, A., and Shakhnarovich, G. (20 16). Examining the impact of blur on recognition
by convolutional networks. arXiv preprint arXiv:1611.05760
W ang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P . (2 004). Image quality assessment: from error
visibility to structural similarity. IEEE Trans. Image Proc. 13, 600–612
W ang, Z. and Li, Q. (2011). Information content weighting fo r perceptual image quality assessment.
IEEE Trans. Image Proc. 20, 1185–1198
W ang, Z., Simoncelli, E. P ., and Bovik, A. C. (2003). Multisc ale structural similarity for image quality
assessment. In Asilomar Conf. Sig., Syst. & Comp. vol. 2, 1398–1402
Zhai, G., Wu, X., Y ang, X., Lin, W ., and Zhang, W . (2011). A psy chovisual quality metric in free-energy
principle. IEEE Transactions on Image Processing 21, 41–52
Zhang, L. and Li, H. (2012). Sr-sim: A fast and high performan ce iqa index based on spectral residual. In
2012 19th IEEE international conference on image processin g (IEEE), 1473–1476
Zhang, L., Zhang, L., Mou, X., Zhang, D., et al. (2011). Fsim: a feature similarity index for image quality
assessment. IEEE Trans. Image Proc. 20, 2378–2386
Frontiers 25