DEEP ACTIVE INFERENCE
KAI UELTZHO¨FFER
Abstract. Thisworkcombinesthefreeenergyprinciplefromcognitiveneuroscienceand
theensuingactiveinferencedynamicswithrecentadvancesinvariationalinferenceondeep
generative models and evolution strategies as efficient large-scale black-box optimisation
technique,tointroducethe”deepactiveinference”agent. Thisagenttriestominimizea
variationalfreeenergyboundontheaveragesurpriseofitssensations,whichismotivated
byahomeostaticargument. Itdoessobychangingtheparametersofitsgenerativemodel,
together with a variational density approximating the posterior distribution over latent
variables, given its observations, and by acting on its environment to actively sample
input that is likely under its generative model. The internal dynamics of the agent are
implemented using deep neural networks, as used in machine learning, and recurrent
dynamics, making the deep active inference agent a scalable and very flexible class of
active inference agents. Using the mountaincar problem, we show how goal-directed
behaviourcanbeimplementedbydefiningsensiblepriorexpectationsonthelatentstates
intheagent’smodel,thatitwilltrytofulfil. Furthermore,weshowthatthedeepactive
inference agent can learn a generative model of the environment, which can be sampled
fromtounderstandtheagent’sbeliefsabouttheenvironmentanditsinteractionwithit.
1. Introduction
Active Inference (Friston et al., 2006, 2010; Friston, 2012) is a normative theory of
brain function derived from the properties required by active agents to survive in dynamic,
fluctuating environments. This theory is able to account for many aspects of action and
perceptionaswellasanatomicandphysiologicfeaturesofthebrainononehand(Brownand
Friston, 2012; Friston, 2005; Friston et al., 2011; Friston and Kiebel, 2009; Schwartenbeck
et al., 2015; Adams et al., 2013), and encompasses many formal theories about brain
function on the other (Friston, 2010). In terms of its functional form it rests on the
minimisation of an upper variational bound on the agents average surprise. In this way
it is formally very similar to state of the art algorithms for variational inference in deep
generative models (Rezende et al., 2014; Kingma and Welling, 2014; Chung et al., 2015).
However, optimising this bound for active agents introduces a dependency on the true
dynamics of the world, to which the agent usually does not have access, and whose true
Date: October 12, 2018
This is a pre-print of an article published in Biological Cybernetics. The final authenticated ver-
sion is available online at: https://doi.org/10.1007/s00422-018-0785-7
The author has archived a personal copy of the accepted manuscript at his personal homepage:
https://kaiu.me.
1
8102
tcO
11
]CN.oib-q[
5v14320.9071:viXra
2 KAIUELTZHO¨FFER
functionalformdoesnothavetocoincidewiththefunctionalformoftheagent’sgenerative
model. Here we solve these problems using deep neural networks (LeCun et al., 2015)
and recurrent neural networks (Karpathy et al., 2015) as flexible function approximators,
which allow the agent’s generative model to learn a good approximation of the true world
dynamics. Futhermore we apply evolution strategies (Salimans et al., 2017) to estimate
gradients on the variational bound, averaged over a population of agents. This formalism
allows to obtain gradient estimates even for non-differentiable objective functions, which
is the case here; since the agent does neither know the equations of motions of the world,
nor its partial derivatives.
In this way, our approach pairs active inference with state of the art machine learning
techniquestocreateagentsthatcansuccessfullyreachgoalsincomplexenvironmentswhile
simultaneously building a generative model of themselves and their surroundings. In this
workwewanttolayoutthebasicformofthesesocalled”DeepActiveInference”agentsand
illustrate their dynamics using a simple, well known problem from reinforcement learning,
namely the mountain car problem (Moore, 1991). By utilising models and optimisation
techniques that have been applied successfully to real-world data and large-scale problems
(Rezende et al., 2014; Kingma and Welling, 2014; Chung et al., 2015; Salimans et al.,
2017), our agent can be scaled to complex and rich environments. With this paper we
publish the full implementation of the resulting Deep Active Inference agent, together
with all scripts used to create the figures in this publication at https://www.github.com/
kaiu85/deepAI_paper.
In section 2 we will briefly recapitulate the active inference principle. In section 3 we
will describe the mountain car environment and introduce a deep active inference agent
that is able to solve this problem. In section 4 we will show that the agent can solve the
mountaincarproblemwhilesimultaneouslylearningagenerativemodelofitsenvironment.
In section 5 we will discuss possible further directions of this approach and its relation to
other approaches.
2. Active Inference
Active Inference (Friston et al., 2006, 2010; Friston, 2012) rests on the basic assumption
that any agent in exchange with a dynamic, fluctuating environment has to keep certain
innerparameterswithinawelldefinedrange. Otherwise, itwouldsoonerorlaterencounter
a phase transition due to which it would loose its defining characteristics and therefore
disappear. Thus, the agent must restrict itself to a small volume in its state space. This
can be formalised using the entropy of the probability distribution p(s∗) of finding the
agent in a given state s∗ of its state space S∗:
(cid:90)
H(S∗) = ( lnp(s∗))p(s∗)ds∗
−
s∗∈S∗
By minimising this entropy, an agent can counter dispersive effects from its environment
and maintain a stable identity in a fluctuating environment.
However, anagentdoesnothavedirectaccesstoanobjectivemeasurementofitscurrent
state. Instead it only perceives itself and the world around it via its sensory epthelia. This
DEEP ACTIVE INFERENCE 3
canbedescribedbyapotentiallynoisysensorymappingo = g(s)fromstatesstosensations
o. Defining the sensory entropy
(cid:90)
H(O) = ( lnp(o))p(o)do
−
o∈O
over the space O of all possible observations of an agent, one can derive the following
inequality in the absence of sensory noise (Friston et al., 2010)
(cid:90)
H(O) H(S∗)+ p(s∗)ln g s∗ ds
≥ | |
s∗∈S∗
Agents, whose sensory organs did not have a good mapping of relevant physical states
to appropriate sensory inputs, would not last very long. So the mapping between the
agents true states and its sensations is assumed to have an almost constant sensitivity,
in terms of the determinant of the Jacobian g s∗ over the encountered range of states s.
| |
This makes the last term approximately constant and allows upper bounding the entropy
of the agent’s distribution on state space by the entropy of its sensory states plus this
constant term (Friston et al., 2010). Thus, to ensure keep its physiological variables within
well-defined bounds, an agent has to minimize its sensory entropy H(O) 1
Assuming ergodicity, i.e. the equivalence of time- and ensemble-averages, one can write
the sensory entropy as
1 (cid:90) T
H(O) = lim lnp(o(t))dt
T→∞−T
0
From the calculus of variations it follows that an agent can minimize its sensory entropy
by minimising its sensory surprise lnp(o(t)) at all times, in terms of the following Euler-
−
Lagrange-equation:
( lnp(o(t))) = 0
o
∇ −
To be able to efficiently do this, our agent needs a statistical model of its sensory inputs,
to evaluate p(o). Since the world in which we live is hierarchically organised, dynamic,
and features a lot of complex noise, we assume that the agent’s model is a deep, recur-
rent, latent variable model (Conant and Ashby, 1970). Furthermore we assume that this
model is generative, using the observation that we are able to imagine certain situations
and perceptions (like the image of a blue sky over a desert landscape) without actually
experiencing or having experienced them. Thus, we work with a generative model p (o,s)
θ
ofsensoryobservationsoandlatentvariabless, thatrepresentthehiddentruestatesofthe
world, which we can factorise into a likelihood function p (o s) and a prior on the states
θ
|
p (s):
θ
p (o,s) = p (o s)p (s)
θ θ θ
|
Thesetθcomprisestheslowlychangingparametersthattheagentcanchangetoimprove
its model of the world. In the brain this might be the pattern and strength of synapses.
1In more general formulations of active inference, the assumption that the mapping between hidden
states and outcomes is constant can be relaxed (Friston et al., 2015).
4 KAIUELTZHO¨FFER
Given this factorisation, to minimize surprise, the agent has to solve the hard task of
calculating
(cid:90)
p (o) = p (o s)p (s)ds
θ θ θ
|
by marginalising over all possible states that could lead to a given observation. As the
dimensionality of the latent state space S can be very high, this integral is extremely hard
to solve. Therefore a further assumption of the free energy principle is, that agents do
not try to minimize the surprise lnp (o) directly, but rather minimize an upper bound,
θ
−
which is a lot simpler to calculate.
Using the fact that the Kullback-Leibler (KL) Divergence
(cid:90) (cid:18) (cid:19)
p (x)
a
D (p (x) p (x)) = ln p (x)dx
KL a b a
|| p (x)
x∈X b
between two arbitrary distributions p (x) and p (x) with a shared support on a com-
a b
mon space X is always greater or equal to zero, and equal to zero if and only if the two
distributions are equal, we can define the variational free energy as:
F(o,θ,u) = lnp (o)+D (q (s) p(s o)) lnp (o)
θ KL u θ
− || | ≥ −
Here q (s) is an arbitrary, so called variational density over the space of hidden states
u
s, which belongs to a family of distributions parameterised by a time-dependent, i.e. fast
changing, parameter set u. If q (s) was a diagonal Gaussian, u = µ,σ would be the
u
{ }
corresponding means and standard-deviations. This parameter set can be encoded by the
internal states of our agent, e.g. by the neural activity (i.e. the firing pattern of neurons)
in its brain. Thus, the upper bound F(o,θ,u) now only depends on quantities to which our
agenthasdirectaccess. Namelythestatesofitssensoryorganso,thesynapsesencodingits
generative model of the world θ and the neural activity representing the sufficient statistics
u of the variational density.
Using the definition of the Kullback-Leibler divergence, the linearity of the integral,
Bayes’ rule, and manipulation of logarithms, one can derive the following equivalent forms
of the free energy functional:
F(o,θ,u) = lnp (o)+D (q (s) p (s o))
θ KL u θ
− || |
= lnp (o,s) q (s)
(cid:104)− θ (cid:105)qu(s)−(cid:104)− u (cid:105)qu(s)
= lnp (o s) +D (q (s) p (s))
(cid:104)− θ | (cid:105)qu(s) KL u || θ
Here f(s) means calculating the expectation value of f(s) with respect to the
(cid:104) (cid:105)qu(s)
variational density q (s).
u
If the agent was unable to change its environment, i.e. just a passive observer of the
world around it, the only thing it could do to minimize F would be to change the param-
eters θ of its generative model and the sufficient statistics u of its inner representation.
Looking at the first form of the free energy, optimizing u would correspond to minimizing
the Kullback-Leibler divergence between the variational distribution q (s) and the true
u
DEEP ACTIVE INFERENCE 5
posterior distribution p(s o), i.e. the probability over hidden states s given the observa-
|
tionso. Thus, thevariationaldensitycanbeseenasanapproximationofthetrueposterior
density. Therefore, by minimising F, the agent automatically acquires a probabilistic rep-
resentation of an approximate posterior on the hidden states of the world, given its sensory
input. Theoptimizationofthesufficientstatisticsuofthevariationaldensityq istherefore
what we call ”perception”. As q has to be optimized on a fast timescale, quickly changing
with the sensory input o, it is likely represented in terms of neural activity. This might
explain hallmarks of Bayesian computations and probabilistic representations of sensory
stimuli in the brain (Ernst and Banks, 2002; Alais and Burr, 2004; Knill and Pouget, 2004;
Moreno-Bote et al., 2011; Berkes et al., 2011; Haefner et al., 2016).
As the variational free energy upper bounds the surprise lnp (o) , minimising free en-
θ
−
ergywithrespecttotheparametersθ ofthegenerativemodelwillsimultaneouslymaximise
theevidencep (o)fortheagent’sgenerativemodeloftheworld. Theresultingoptimisation
θ
of the parameters θ of the generative model is what we call perceptual learning and what
might be implemented by changing the synaptic connections between neurons in the brain.
The second form is given only to demonstrate, where the name ”free energy” originated
from, since its form is very similar to the Helmholtz free energy of the canonical ensemble
in statistical physics.
The core idea of active inference is now to equip the agent with actuators, that allow it
toactivelychangethestateofitsenvironment. Therebythesensoryobservationsobecome
a function of the current and past states a of the agents actuators (”muscles”), via their
influence on the state of the world generating the sensory inputs. Now the agent can not
only minimize the free energy bound by learning (i.e. optimising the parameters of its
generative model) and perception (i.e. optimising the sufficient statistics of the variational
density), but also by changing the observations it makes.
Considering the third form of the variational free energy
F(o,θ,u) = lnp (o s) +D (q (s) p (s))
(cid:104)− θ | (cid:105)qu(s) KL u || θ
weseethatouragentcanminimizeitbyseekingoutobservationsothathaveahighlike-
lihood p (o s) under its own generative model of the world, averaged over its approximate
θ
|
posterior of the state of the world. Thus, the agent will seek out states that conform to its
expectations, i.e. that have a high likelihood under its generative model of the world. So
one can encode specific goals in the priors of the generative model of the agent: assigning
a higher a priori probability to a specific state, the agent will try to seek out this state
more often. The first term, which our agent will maximise using its actions, is also called
accuracy in Bayesian statistics, while the second term is known as complexity. Complexity
measures the degree to which posteriors have to be adjusted in relation to priors to provide
an accurate account of sensory data or outcomes.
This triple optimisation problem can be formalised using the following dynamics for the
parameters θ of an agent’s generative model, its internal states u, encoding the sufficient
statistics of the variational density q, and the states of its actuators a:
6 KAIUELTZHO¨FFER
(θ,u,a) = argminF(o(a˜),θ˜,u˜)
(θ˜,u˜,a˜)
In this paper, we will consider the variational density or beliefs over hidden states of the
world. In more general formulations, this density would cover both the hidden states and
the parameters of the generative model. Usually, these beliefs are factorised so that there
is a mean field approximation to the true posterior estimates of (time varying) states and
(time invariant) parameters (Friston, 2008).
3. Methods
In this section, we introduce an agent which uses recent advances in stochastic optimi-
sation of deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Chung
et al., 2015) and evolution strategies as efficient, scalable optimisation algorithm for non-
differentiable objective functions (Salimans et al., 2017) to implement active inference. We
showitsabilitytoreachaspecificgoalwhileconcurrentlybuildingagenerativemodelofits
environment, usingthewell-knownmountaincarproblem(Moore,1991). Butwefirststart
with some basics that are well known in the deep learning community, but which - due to
similar but differently used nomenclature - might lead to confusion among computational
neuroscientists.
3.1. Deep Neural Networks. When we talk about deep neural networks, we use this
term in the sense of the deep learning literature (LeCun et al., 2015). In this sense,
neural networks are nothing but very flexible function approximators. An excellent and
comprehensive introduction to the state of the art in deep learning is given in Goodfellow
et al. (2016).
3.1.1. Fully Connected Layer. Deep neural networks are composed of individual layers,
which are functions f
θ
: Rd1 Rd2,x f
θ
(x). Here d
1
represents the number of input
→ (cid:55)→
neurons, i.e. the dimensionality of the input space, and d represents the number of
2
output neurons, i.e. the dimensionality of the output space. The subscript θ represents
the parameters of the function, which can be tuned to approximate or optimise a given
objective function.
The canonical functional form of a so called fully connected layer is:
f (x) = h(θ x+θ )
θ w b
Here h : R R is an elementwise nonlinear transfer function. This could for example
→
be a tanh, sigmoid or a so called rectified linear function relu(x) = max(0,x). The set
of parameters θ = θ ,θ consists of the d d matrix θ , called the weight matrix or
w b 2 1 w
just weights, and th { e bias } vector θ Rd2. Th × is functional form is loosely inspired by the
b
∈
response function of ensembles of simple point neurons (e.g. the mean-field equation for
the average firing rate of a large population of leaky-integrate-and-fire neurons as function
of the mean inputs to the population, c.f. equations 12 - 15 in Wong and Wang (2006) ).
These response functions convert the weighted sum of inputs currents (approximated by
DEEP ACTIVE INFERENCE 7
the weighted sum of the firing rates of the presynaptic neurons) to the firing rate of the
modelled population by a nonlinear, thresholded activation function, which is represented
here using the bias parameters θ and the nonlinear transfer function h.
b
3.1.2. Deep Feedforward Networks. A deep feedforward network just consists of a stack
of individual layers f ,f ,f ,...,f ,f , that are applied to the input x sequentially to
1 2 3 n−1 n
generate an output y:
y = f (f (...f (f (f (x)))...)) = f f ... f f f (x)
n n−1 3 2 1 n n−1 3 2 1
◦ ◦ ◦ ◦ ◦
The output of the function f is called the output layer, the input x is called the input
n
layer, the intermediate values f (x),f (f (x)),...,f (...x) are called the hidden layers,
1 2 1 n−1
since they are not explicitly constrained by the target or objective function. The finite
dimensions of the intermediate outputs f (x),f (f (x)),...,f (...x) can be regarded as
1 2 1 n−1
the number of hidden neurons in each layer.
In contrast to previous approaches to active inference (Friston, 2010) deep feedforward
networks allow us to specify agents, whose generative models of their environment do not
need to be from the same model family as the true generative process. This is due to the
flexibility of deep feedforward networks. It was shown that a feedforward network with a
linear output layer and at least one hidden layer with a nonlinear activation function, such
as a tanh or sigmoid activation function can approximate any Borel measurable function
from one finite-dimensional space to another with arbitrary accuracy, given a sufficient
dimensionality (i.e. a sufficient number of neurons) of the hidden layer (Hornik et al.,
1989).
3.1.3. Recurrent Neural Networks. To enable a learning system to build a memory of its
previous actions and experiences, a simple and elegant way is to enhance a neural network
y = f (x )
t θ t
by feeding its output at the previous timestep back as an additional input, i.e.
y = f (x ,y )
t θ t t−1
Althoughthereisawidevarietyofmorecomplexandpowerfularchitectures(c.f. Karpa-
thy et al. (2015); Goodfellow et al. (2016)), this simple, prototypical recurrent neural net-
work was recently shown to be able to learn long-range temporal dependencies, given a
sensible initialization (Le et al., 2015). We are not using this deterministic type of re-
current neural network directly, but we use the basic idea by feeding back samples from
a distribution on the current network state as inputs to calculate the next network state.
Similar to the approximation theorem for deep feedforward networks, it was shown that re-
current neural networks are able to learn arbitrary algorithms, as they are turing complete
(Siegelmann, 1995).
8 KAIUELTZHO¨FFER
0.04
0.03
0.02
0.01
0.00
0.01
0.02
1.0 0.5 0.0 0.5 1.0
Position x / a.u.
.u.a
/
)x(G
laitnetoP
Figure 1. Potential of the mountain car problem. The agent is a small
cart starting at the bottom of the valley at x = 0.5
−
3.2. The Environment. The agent will act in a discrete time version of the mountain
car world (Moore, 1991). It will start at the bottom x = 0.5 of a potential landscape
−
G(x), which is shown in figure 1.
Theagentitselfisasmallcaronthislandscape,whosetrue,physicalstates∗ = (a ,x ,v )
t t t t
is given by its current position x , its velocity v , and the current state a of its effector
t t t
organs. In the case of humans this would be the state of our muscles, here the state a
t
describes the car’s steering direction and the throttle of the car’s engine. The dynamics
of the environment are given by a set of update equations, which can be summarised
symbolically by
(x ,v ) = R(x ,v ,a )
t+1 t+1 t t t+1
Note that we use bold face to denote vectors.
This equation can be decomposed into a set of equations, as multiple forces will act on
the agent. The downhill force F due to the shape of the landscape depends on its position
g
DEEP ACTIVE INFERENCE 9
x. It is given by
(cid:40)
∂ 0.05( 2x 1), x < 0
F g (x) = −∂x G(x) = 0.05 (cid:0)− (1+ − 5x2)−1/2 x2(1+5x2)−3/2 x4/16 (cid:1) , x 0
− − − ≥
and shown in figure 2.
The agent’s motor can generate a force F , depending on its current action state a
a
F (a) = 0.03tanh(a)
a
As mentioned above, the action state controls the steering direction (positive or negative)
and the throttle of the engine. However, the absolute force that the engine can generate is
limited to the interval ( 0.03,0.03) due to the tanh function.
−
The laminar friction force F depends linearly on the agent’s current velocity v
f
F (v) = 0.25v
f
−
Thus, the total force action on the agent is
F (x ,a ,v ) = F (x )+F (a )+F (v )
total t t+1 t g t a t+1 f t
This leads to the following update equations for velocity
v = v +F (x ,a ,v )
t+1 t total t t+1 t
and position
x = x +v
t+1 t t+1
Initially, the agent is resting at the bottom of the landscape, i.e. s∗ = (x ,v ) =
0 0 0
( 0.5,0.0).
−
We will later set the agent the goal of reaching and staying at x = 1.0. However,
due to the shape of the potential landscape and the resulting force F (x), as shown in
g
figure 2, we notice that the landscape gets very steep at x = 0.0. As the force generated
by the motor is limited to the interval ( 0.03,0.03), the agent is not strong enough to
−
climb the slope at x = 0, which results in a downhill force of F (0) = 0.05, without
g
−
some additional momentum. Thus, to overcome this slope the agent has to move uphill
to the left, away from its target position, at first. In this way, it can acquire the required
additional momentum, which allows it to overcome the opposite slope. In this way, the
mountaincar environment - although very simple - is not completely trivial, as the agent
has to learn that a direct approach of the target position will not succeed and it needs to
acquire a strategy that initially leads away from its target.
3.3. The Deep Active Inference Agent. We will now describe an agent that follows
the active inference principle laid out in section 2 (Friston et al., 2006, 2010). It is adapted
to the mountain car environment by its sensory inputs and motor outputs and its inner
workings are implemented and optimised using recent advances in deep learning (Kingma
and Welling, 2014; Rezende et al., 2014; Chung et al., 2015; Salimans et al., 2017).
10 KAIUELTZHO¨FFER
0.06
0.04
0.02
0.00
0.02
0.04
0.06
1.0 0.5 0.0 0.5 1.0
Position x / a.u.
.u.a
/
)x(
F
ecroF
g
Figure 2. Downhill force F due to the slope of the potential landscape.
g
3.3.1. Sensory Inputs. Our agent has a noisy sense o of its real position x
x,t t
p(o x ) = N(o ;x ,0.01)
x,t t x,t t
|
HereN(x;µ,σ)denotesaGaussianprobabilitydensityoverxwithmeanµandstandard-
deviation σ. To show that the agent can indeed learn a complex, nonlinear, generative
model of its sensory input, we add another sensory channel with a nonlinear, non-bijective
transformation of x :
t
(cid:18)
(x
1.0)2(cid:19)
t
o = exp −
h,t − 2 0.32
·
Note that the agent has no direct measurement of its current velocity v , i.e. it has to
t
infer it from the sequence of its sensory inputs, which all just depend on x .
t
To see if the agent understands its own actions, we also equip it with a proprioceptive
sensory channel, allowing it to observe its action state a :
t
DEEP ACTIVE INFERENCE 11
o = a
a,t t
Note that having a direct sense of its action state a is not necessary for an active
t
inference agent to successfully control its environment (c.f. supplementary figures 9-11).
E.g. the reflex arcs that we use to increase the likelihood of our sensory inputs, given our
generative model of the world, feature their own, closed loop neural dynamics at the level
of the spinal cord. So we do not (and do not have to) have direct access to the action
states of our muscles, when we just lift our arm by expecting it to lift. However, adding
thischannelwillallowuslatertodirectlysampletheagentsproprioceptivesensationsfrom
its generative model, to check if it understands its own action on the environment.
3.3.2. The Agent’s Generative Model of the Environment. First, the agent has a prior over
the hidden states of the world s:
T
(cid:89)
p (s ,s ,...,s ) = p (s s )
θ 1 2 T θ t t+1
|
t=1
with initial state s = (0,...,0). Our agent will use d = 10 neurons to represent the
0 s
current state of the world.
This factorisation means that the next state s in the agents generative model only
t+1
depends on the current state s of its inner model of the world. Thus, the transition
t
distribution p (s s ) encodes the agent’s model of the dynamics of the world, i.e. what
θ t t+1
|
it has learned about the laws of motion of its environment and its influence on it.
In our concrete implementation, we model the distribution of s as diagonal Gaussian,
t+1
where the means and standard deviations are calculated from the current state s using
t
neural networks. I.e.
p (s s ) = N(s ;µt(s ),σt(s ))
θ t+1 t t+1 θ t θ t
|
We use a fully connected single layer network with a tanh nonlinearity to calculate the
meansµt(s )andanotherfullyconnectedsinglelayernetworkwithsoftplus(x) = ln(1+ex)
θ t
as nonlinear transfer function to calculate the standard deviations σt(s ). We use θ to
θ t
encompass all parameters of the generative model, that we are going to optimise. In
practice, thismeansalltheweightsandbiasparametersoftheneuralnetworkstocalculate
the means and standard-deviations. The use of diagonal Gaussians in the prior leads to
hidden, abstract representations within which independent causes of observations are well
separated.
Similarly, the likelihood functions for each of the three observables also factorise
p (o ,...,o ,o ,...,o ,o ,...,o s ,....,s )
θ x,1 x,T h,1 h,T a,1 a,t 1 T
|
T T T
(cid:89) (cid:89) (cid:89)
= p (o s ) p (o s ) p (o s )
θ x,t t θ h,t t θ a,t t
| | |
t=0 t=0 t=0
12 KAIUELTZHO¨FFER
Algorithm 1 Sampling from the agents generative model. Concrete samples and values
of a variable marked using a hat, e.g. ˆs .
t
Initialize n processes with ˆs = (0,...,0).
p 0
for each Process do
for t = 1,...,T do
Draw a single sample ˆs from p (s ˆs )
t θ t t−1
|
Sample single observations oˆ from each likelihood p (o ˆs )
x/a/h θ x/a/h,t t
|
Carry sˆ over to the next timestep.
t
end for
end for
So the likelihood of each observable o ,o ,o for a given time t only depends on the
x,t a,t h,t
current state s . We again use Gaussian distributions, to obtain
t
p (o s ) = N(o ;µx(s ),σx(s ))
θ x,t t t θ t θ t
|
p (o s ) = N(o ;µa(s ),σa(s ))
θ a,t t t θ t θ t
|
p (o s ) = N(o ;µh(s ),σh(s ))
θ h,t t t θ t θ t
|
We calculate the sufficient statistics of these Gaussian distributions from the state s using
t
a deep feedforward network with three hidden layers of d = 10 neurons each, using tanh
h
as nonlinear transfer function. We use a linear output layer to calculate the means of the
Gaussian variables and a second output layer with softplus(x) = ln(1+ex) as nonlinear
transfer function to calculate the standard deviations. Although this structure is on the
first glance different from the hierarchical dynamical models developed by Friston (2008),
thedeepandnonlinearstructureofthefeedforwardnetworkalsoallowsforstructurednoise
to enter at different levels of this hierarchy.
Once the agent has acquired a generative model of its sensory inputs, by optimising
the parameters θ, one can sample from this model by propagating the prior on the state
space and then using the learned likelihood function to generate samples, as described
in algorithm 1. Sampling many processes (n 102 104) in parallel yields a good
p
≈ −
approximation, although only a single sample is drawn from the prior density and from
each factor of the likelihood function per individual process and per timestep.
3.3.3. VariationalDensity. FollowingKingmaandWelling(2014);Rezendeetal.(2014)we
do not explicitly represent the sufficient statistics of the variational posterior at every time
step. This would require an additional, costly optimisation of these variational parameters
at each individual time step. Instead we use an inference network, which approximates the
dependencyofthevariationalposterioronthepreviousstateandthecurrentsensoryinput
and which is parameterised by time-invariant parameters. This allows us to learn these
parameters together with the parameters of the generative model and the action function
(c.f. section 3.3.4), and also allows for very fast inference later on. We use the following
factorisation for this approximation of the variational density on the states s, given the
DEEP ACTIVE INFERENCE 13
agents observations o:
q (s ,...,s o ,...,o ,o ,...,o ,o ,...,o )
θ 1 T x,1 x,T h,1 h,T a,1 a,T
|
T
(cid:89)
= q (s s ,o ,o ,o )
θ t t−1 x,t h,t a,t
|
t=1
with initial state s = (0,...,0), before the agent has interacted with the environment. We
0
again use diagonal Gaussians
q (s s ,o ,o ,o ) = N(s ;µq (s ,o ,o ,o ),σq (s ,o ,o ,o ))
θ t | t−1 x,t h,t a,t t θ t−1 x,t h,t a,t θ t−1 x,t h,t a,t
The sufficient statistics are calculated using a deep feedforward network with two hidden
layers of d = 10 neurons each, using tanh nonlinearities. The means are again calculated
h
using a linear, and the standard deviations using a softplus output layer. While the use
of diagonal Gaussians in the prior leads to hidden, abstract representations within which
independent causes of observations are optimally separated, i.e. which has favourable
properties, here this choice is just due to practical considerations. The fact that we choose
both the variational density and the prior density as diagonal Gaussians, will later allow us
to use a closed formula to calculate the Kullback-Leibler-Divergence between the densities.
However,ifamoreflexibleposteriorisrequired,normalizingflowsallowaseriesofnonlinear
transformations of the diagonal Gaussian used for the variational density, by which it can
approximateverycomplexandmultimodalposteriordistributions(RezendeandMohamed,
2015; Kingma et al., 2016; Tomczak and Welling, 2016).
This device – of learning the mapping between inputs and the sufficient statistics of
(approximate) posterior over hidden states – is known as amortisation. It has the great
advantage of being extremely efficient and fast. On the other hand, it assumes that the
mapping can be approximated with a static non-linear function that can be learned with a
neural network. This somewhat limits the context sensitivity of active inference schemes,
depending upon the parameterisation of the mapping. In short, amortisation enables one
to convert a deep inference or deep deconvolution problem into a deep learning problem –
by finding a static non-linear mapping between (time varying) inputs and (approximate)
posterior beliefs about the states generating those inputs.
3.3.4. ActionStates. Ingeneral,theactionstatea oftheagentcouldbeminimiseddirectly
t
at each timestep, thereby minimizing the free energy by driving the sensory input (which
dependsontheactionstateviathedynamicsoftheworld)towardstheagentsexpectations,
in terms of its likelihood function. However, this would require a costly optimisation for
each timestep (and every single simulated process). Thus, we use the same rationale as
Kingma and Welling (2014); Rezende et al. (2014) used for the variational density q, and
approximate the complex dependence of the action state on the agent’s current estimate of
the world (and via this on the true state of the world) by fixed, but very flexible functions,
i.e. deep neural networks. This yields an explicit functional dependency
p (a s )
θ t+1 t
|
14 KAIUELTZHO¨FFER
whose parameters we include, together with the parameters of the generative model and
the variational density, to the set of parameters θ that we will optimise.
We use a one-dimensional Gaussian form
p (a s ) = N(a ;µa(s ),σa(s ))
θ t+1 t t+1 θ t θ t
|
whose sufficient statistics are calculated using a deep feedforward network with one fully
connected hidden layers of d = 10 neurons, a linear output layer for the mean, and a
h
softplus output layer for the standard deviation.
We now can just optimise the time-invariant parameters θ of these neural networks
together with the parameters of the generative model and the variational density. This
approximation makes learning and propagating the agent very fast, allowing for efficient
simultaneous learning of the generative model, the variational density and the action func-
tion.
The approximation of both, the sufficient statistics of the variational density q and the
action states a by deep neural networks is the reason why we call this class of agents Deep
Active Inference agents.
The causal structure of the complete model is shown in figure 3.
3.3.5. Goal Directed Behavior. If we just propagate and optimise our agent as it is now,
it will look for a stable equilibrium with its environment and settle there. However, to be
practically applicable to real-life problems, we have to instil some concrete goals in our
agent. We can achieve this by defining states that it will expect to be in. Then the action
states will try to fulfil these expectations.
In this concrete case, we want to propagate the agent for 30 timesteps and want it to
be at x = 1.0 for at least the last ten timesteps. As the agent’s priors act on the hidden
states, we introduce a hard-wired state which just represents the agents current position.
We do this by explicitly encoding the agent’s sense of position o to the first dimension
x,t
of the state vector s :
1,t
q (s s ,o ,o ,o ) = N(s ;0.1o ,0.01)
θ 1,t t−1 x,t h,t a,t 1,t x,t
|
This can be seen as a homeostatic, i.e. vitally important, state parameter. E.g. the CO
2
concentration in our blood is extremely important and tightly controlled, as opposed to
the possible brightness perceived at the individual receptors of our retina, which can vary
by orders of magnitude. Though we might not directly change our behavior depending on
visual stimuli, a slight increase in the CO concentration of our blood and the concurring
2
decrease in the pH will trigger chemoreceptors in the carotid and aortic bodies, which in
turn will increase the activity of the respiratory centers in the medulla oblongata and the
pons, leading to a fast and strong increase in ventilation, which might be accompanied
by a subjective feeling of dyspnoea or respiratory distress. These hard-wired connection
between vitally important body parameters and direct changes in perception and action
might be very similar to our approach to encode the goal-relevant states explicitly.
But besides explicitly encoding relevant environmental parameters in the hidden states
of the agent’s latent model, we also have to specify the corresponding prior expectations
DEEP ACTIVE INFERENCE 15
s t−1 s t s t+1
o t−1 a t o t a t+1 o t+1
s∗ s∗ s∗
t−1 t t+1
Figure 3. Graphical representation of causal dependencies. Remember
that s∗ = (a ,x ,v ). The solid lines correspond to the factors of the agent’s
t t t t
generative model, p(s s ) and p(o ,s ). The dashed lines correspond to
t+1 t t t
|
the variational density q(s o ,s ). The dotted lines correspond to the en-
t t t−1
|
vironmental dynamics (x ,v ) = R(x ,v ,a ) and the true generative
t+1 t+1 t t t+1
densities p(o s∗). The wiggly line describes the dependency of the action
t t
|
p(a s ) on the hidden states s .
t+1 t t
|
(such as explicit boundaries for the pH of our blood). We do this by explicitly setting the
prior over the first dimension of the state vector for t > 20:
pt(s s ) = N(s ;0.1,0.01),if t > 20
θ 0,t t−1 0,t
|
While this kind of hard-coded inference dynamics and expectations might be fixed for
individual agents of a class (i.e. species) within their lifetimes, these mappings can be
optimised on a longer timescale over populations of agents by evolution. In fact, evolution
might be seen as a very similar learning process, only on different spatial and temporal
scales (Watson and Szathm´ary, 2016; Baez and Pollard, 2015; Harper, 2009).
3.3.6. The Free Energy Objective. Now we have everything that we need to put our objec-
tive function together. We use the following form of the variational free energy bound (c.f
16 KAIUELTZHO¨FFER
section 2):
F(o,θ) = lnp (o ,...,o ,o ,...,o ,o ,...,o s ,....,s )
(cid:104)− θ x,1 x,T h,1 h,T a,1 a,t | 1 T (cid:105)q
+D (q (s ,...,s o ,...,o ,o ,...,o ,o ,...,o ) p (s ,s ,...,s ))
KL θ 1 T x,1 x,T h,1 h,T a,1 a,T θ 1 2 T
| ||
where ... means the average with respect to the variational density
(cid:104) (cid:105)q
q (s ,...,s o ,...,o ,o ,...,o ,o ,...,o )
θ 1 T x,1 x,T h,1 h,T a,1 a,T
|
Using the above factorisation, the free energy becomes
T (cid:20)
(cid:88)
F(o,θ) = lnp (o s ) lnp (o s ) lnp (o s )
(cid:104)− θ x,t | t − θ h,t | t − θ a,t | t (cid:105)q θ (st|st−1,ox,t,o h,t ,oa,t)
t=1
(cid:21)
+D (q (s s ,o ,o ,o ) p (s s ))
KL θ t t−1 x,t h,t a,t θ t t−1
| || |
To evaluate this expression, we simulate several thousand processes in parallel, which
allowsustoapproximatethevariationaldensityq justbyasinglesampleˆs perprocessand
t
per timestep, analogous to Stochastic Gradient Descent or the Variational Autoencoder,
where only one sample per data point is enough, since the gradients depend on entire
(mini-)batches of datapoints (Kingma and Welling, 2014; Rezende et al., 2014; Chung
et al., 2015). The sampling occurs following algorithm 2, where we use the closed form of
the KL-Divergence for diagonal Gaussians:
D (q (s ˆs ,oˆ ,oˆ ,oˆ ) p (s ˆs )) =
KL θ t t−1 x,t h,t a,t θ t t−1
| || |
(cid:88) n 1 (cid:18) σt (σ q )2+(µ q µt)2 (cid:19)
D (N(s ;µq,σq) N(s ;µt,σt)) = 2ln i + i i − i 1
KL t || t 2 σ q (σt)2 −
i=1 i i
The fact that we run a large number n of processes in parallel allows us to resort to
F
this simple sampling scheme for the individual processes.
The minimisation of the free energy with respect to the parameters θ will improve the
agents generative model p (o,s), by lower bounding the evidence p (o) of the observations
θ θ
o, given the generative model. Simultaneously it will make the variational density q (s o)
θ
|
a better approximation of the true posterior p (s o), as can be seen from the following,
θ
|
equivalent form of the free energy (c.f. section 2):
F(o,θ) = < lnp (o s) > +D (q (s o) p (s))
θ q (s|o) KL θ θ
− | θ | ||
= lnp (o)+D (q (s o) p (s o))
θ KL θ θ
− | || |
Additionally, the parameters of the action function will be optimised, so that the agent
seeksoutexpectedstatesunderitsownmodeloftheworld,minimizing< lnp (o s) > .
θ q (s|o)
− | θ
3.4. Optimization using Evolution Strategies. Without action, the model and the
objective would be very similar to the objective functions of Kingma and Welling (2014);
Rezende et al. (2014); Chung et al. (2015). So one could just do a gradient descent on the
DEEP ACTIVE INFERENCE 17
Algorithm 2 Sampling based approximation of the free energy cost. Concrete samples
and values of a variable marked using a hat, e.g. ˆs . l is the sampling-based approximation
t
of the expected likelihood of the observations under the variational density q, i.e. the
accuracy term. d is the KL-divergence between the variational density and the prior, i.e.
the complexity term.
Initialize the free energy estimate with F = 0
Initialize n agents with ˆs = (0,...,0) and (x ,v ) = ( 0.5,0.0).
F 0 0 0
−
for each Agent do
for t = 1,...,T do
Sample an action aˆ from p (a ˆs )
t θ t t−1
|
Propagate the environment using (x ,v ) = R(x ,v ,aˆ )
t t t−1 t−1 t
Draw single observations oˆ from p (o x )
x,t e x,t t
|
Set observation oˆ = exp(
(xt−1.0)2
)
h,t − 2·0.32
Set observation oˆ = aˆ
a,t t
Draw a single sample ˆs from q (s ˆs ,oˆ ,oˆ ,oˆ )
t θ t t−1 x,t h,t a,t
(cid:80) |
Calculate l = lnp (oˆ ˆs )
− i∈{x,h,a} θ i,t | t
Calculate d = D (q (s ˆs ,oˆ ,oˆ ,oˆ ) p (s ˆs ))
KL θ t t−1 x,t h,t a,t θ t t−1
| || |
Increment free energy F = F + d+l
np
Carry ˆs and (x ,v ) over to the next timestep.
t t t
end for
end for
return F
estimatedfreeenergywithrespecttotheparametersθ,usingagradient-basedoptimisation
algorithmsuchasADAM(KingmaandBa,2014). However, todothishere, wewouldhave
to backpropagate the partial derivatives of the free energy with respect to the parameters
through the dynamics of the world. I.e. our agent would have to know the equations of
motions of its environment or at least their partial derivatives. As this is obviously not the
case, and as many environments are not even differentiable, we have to resort to another
approach.
It was recently shown that evolution strategies allow for efficient, large scale optimisa-
tion of complex, non-differentiable objective functions F(θ) (Salimans et al., 2017). We
apply this algorithm as follows: Instead of searching for a single optimal parameter set θ∗,
we introduce a distribution on the space of parameters, which is called the ”population
density”. We optimise its sufficient statistics to minimize the expected free energy under
this distribution. The population density can be seen as a population of individual agents,
whose average fitness is optimised, hence the name. The expected free energy over the
population as function of the sufficient statistics ψ of the population density p (θ) is:
ψ
η(ψ) = F(θ)
(cid:104) (cid:105)p ψ (θ)
Now we can calculate the gradient of η with respect to the sufficient statistics ψ:
18 KAIUELTZHO¨FFER
η(ψ) = F(θ) lnp (θ)
∇ ψ (cid:104) ∇ ψ ψ (cid:105)p ψ (θ)
Using a diagonal Gaussian for p (θ) = N(θ;µψ,σψ) the gradient with respect to the
ψ
means µψ is:
(cid:28) (cid:29)
1 (cid:16) (cid:17)
η(ψ) = F(θ) θ µψ
∇ µψ (σψ)2 −
p (θ)
ψ
We will also optimise the standard deviations of our population density, using the cor-
responding gradient:
(cid:42) (cid:0)
θ
µψ(cid:1)2 (cid:0) σψ(cid:1)2(cid:43)
η(ψ) = F(θ) − −
∇ σψ (σψ)3
p (θ)
ψ
Drawing samples (cid:15) from a standard normal distribution N((cid:15) ;0,1), we can approximate
i i
samplesfromp (θ)byθ = µψ+σψ(cid:15) . Thus,wecanapproximatethegradientsviasampling
ψ i i
by:
npop
1 (cid:88) (cid:15) i
(1) η(ψ) F(θ )
∇ µψ ≈ n i σψ
pop
i=1
and
1 n (cid:88) pop (cid:15)2 1
η(ψ) F(θ ) i −
∇ σψ ≈ n i σψ
pop
i=1
For reasons of stability we are not optimising σψ directly, but calculate the standard
deviations using:
σψ = softplus(σ˜ψ)+σ ψ
min
with softplus(x) = ln(1+ex). By choosing σ ψ = 10−6 constant and optimising σ˜ψ we
min
prevent divisions-by-zero and make sure that there is no sign-switch during the optimisa-
tion. The chain rule gives:
1 n (cid:88) pop (cid:15)2 1∂σψ 1 n (cid:88) pop (cid:15)2 1 exp (cid:0) σ˜ψ(cid:1)
(2) η(ψ) F(θ ) i − = F(θ ) i −
∇ σ˜ψ ≈ n i σψ ∂σ˜ψ n i σψ 1+exp(σ˜ψ)
pop pop
i=1 i=1
Using these gradient estimates, we now optimise the expected free energy bound under
the population density using ADAM as gradient based optimiser (Kingma and Ba, 2014).
The corresponding pseudocode is shown in algorithm 3.
DEEP ACTIVE INFERENCE 19
Algorithm 3 Optimisation of the free energy bound.
Initializethepopulationdensityontheparametersθusingrandomisedsufficientstatistics
ψ = µψ,σ˜ψ
{ }
while Expected free energy η(ψ) has not converged do
Draw n samples θ = µψ +σψ(cid:15) , (cid:15) N(0,1)
pop i i i
∼
for each Sample do
Approximate the free energy bound F(θ ) using n processes
i F
end for
Approximate the gradients η using equations 1 and 2.
ψ
∇
Perform a parameter update on ψ using ADAM with these gradient estimates.
end while
3.5. Constrained Sampling from the Learned Generative Model. Once the agent
has been optimised, we can not only propagate it through the environment or draw uncon-
strained samples from its generative model of the world, we can also use a Markov-Chain
Monte Carlo algorithm to draw constrained samples from its generative model (Rezende
et al., 2014). This can on one hand be used to impute missing inputs. E.g. by sampling
from
p (o ,o ,...,o o ,o ,...,o ,o ,o ,...,o )
θ h,1 h,2 h,T x,1 x,2 x,T a,1 a,2 a,T
|
we can get the agents estimate on the homeostatic sensory channel, given the proprio-
ceptive and spatial sensory channels. By using multiple samples we can even get sensible
bounds on the uncertainty of these estimates by approximating the full distribution.
On the other hand, we can ask the agent about its beliefs about the course of the world,
given its actions (in terms of its proprioceptive channel o ). Thus, we can get an explicit
a
readout about the agents beliefs about its influence on the world.
p (o ,o ,...,o ,o ,o ,...,o o ,o ,...,o )
θ x,1 x,2 x,T h,1 h,2 h,T a,1 a,2 a,T
|
TherequiredalgorithmisdevelopedinappendixFofRezendeetal.(2014)anddescribed
here as algorithm 4. The basic idea is to use the de-noising properties of autoencoders
due to the learned, abstract and robust representation and their ability to generate low-
dimensional representations capturing the regularity and systematic dependencies within
the observational data. Thus, the workings of this algorithm can be understood as follows:
First, all but the given sensory channels are randomly initialised. These partly random
sensory observations are now encoded using the variational distribution q. The resulting
state tries to represent the observation within the low-dimensional, robust representation
learned by the agent and should thereby be able to remove some of the ”noise” from the
randomly initialised channels, just in line with the classic idea of an autoencoder (Hinton
and Salakhutdinov, 2006). From this variational distribution a sample is drawn, which can
be used to generate new, sensory samples, that are already a bit less noisy. Now the known
observationscanberesettotheirrespectivevaluesandthedenoisedobservationscanagain
be encoded, using the variational density q. By iteratively encoding the denoised samples
20 KAIUELTZHO¨FFER
Algorithm 4 Markov-Chain Monte Carlo algorithm for constrained sampling from the
agent’s generative model. Concrete samples and values of a variable marked using a hat,
e.g. ˆs .
t
Given: Proprioceptive sensory inputs o ,...,o
a,1 a,T
Initialize n processes with ˆs = (0,...,0)
s 0
for each Process do
for t = 1,...,T do
Initialize oˆ0 with sample from N(o0 ;0,0.01)
x,t x,t
Initialize oˆ0 with sample from N(o0 ;0,0.01)
h,t h,t
for i = 1,...,n do
i
Sample ˆsi from q (si ˆs ,oˆi−1,oˆi−1,o )
t θ t | t−1 x,t h,t a,t
Sample new estimates oˆi from each likelihood p (oi ˆsi)
x/h,t θ x/h,t| t
end for
Set ˆs =ˆsni
t t
Set oˆ = oˆ
ni
x,t x,t
Set oˆ = oˆ
ni
h,t h,t
Return oˆ and oˆ
h,t a,t
Carry ˆs over to the next timestep.
t
end for
end for
together with the given sensory inputs, the iterative samples from the abstract, robust
representation will converge to the most probable cause (in terms of the hidden states) for
theactuallyobservedsensationsunderthegenerativemodel. Asthevariationaldensityand
the generative model capture the regularities and dependencies within the observations,
the observations generated from this representation will converge to the distribution of the
unknownobservations, giventheobservedchannels. Rezendeetal.(2014)providedaproof
that this is true, given that the unobserved channels are not initialised too far away from
the actual values.
3.6. Experimental Set Up and Parameters. The experiments were performed on a
desktop PC equipped with a 2013 NVIDIA GTX Titan GPU. The parameter values used
in the optimisation algorithm 3 and the required estimation of the free energy bound using
algorithm 2 are shown in table 1. Note that we approximate the free energy by a single
process (n = 1) for each sample from the population density. This is possible, as we draw
F
many (n = 104) samples from the population density. Using more processes for each
pop
sample, while keeping the total number n = n n of simulated processes constant,
tot pop F
results in a worse convergence behavior, as the coverage of the parameter space has to
be reduced, while the total variability due to the stochastic approximation of the total
bound stays approximately constant. The full code of this implementation and the scripts
to reproduce all figures in this paper can be downloaded here: https://www.github.
DEEP ACTIVE INFERENCE 21
200
150
100
50
0
0 100 200 300 400 500
Steps
ygrenE
eerF
200
150
100
50
0
0 5000 10000 15000 20000 25000 30000
Steps
ygrenE
eerF
Figure 4. Convergence of an active inference agent, using parameters in
table 1. The area shaded in red in the left plot was enlarged in the right
plot.
com/kaiu85/deepAI_paper. The code is written in Python 2.7, using Theano (Theano
Development Team, 2016) for GPU optimised Tensor-Operations.
4. Results
The evolution strategies based optimisation procedure used less than 300 MB of GPU
memory and took less than 0.4 s per iteration. Figure 4 shows the convergence of the free
energy bound within 30,000 training steps (less than 3.5 hours). It quickly converges from
its random starting parameters to a plateau, on which it tries to directly climb the hill and
gets stuck at the steep slope. However, after only about 250 updates of the population
densityitdiscovers, thatitcangethigherbyfirstmovingintheoppositedirection, thereby
gaining some momentum, which it can use to overcome the steep parts of the slope . This
insight leads to a sudden, rapid decline in free energy (Friston et al., 2017). The rapid
development of the agent’s strategy and its quick adoption of the insight, that an initial
movement away from its target position is beneficial, is illustrated in figure 5.
The agent’s trajectory after about 30,000 training steps is shown in figure 6: It takes
a short left swing to gain the required momentum to overcome the steep slope at x = 0,
then directly swings up to its target position x = 1.0, and stays there by applying just the
right force to counteract the non-zero downhill force at the target position x = 1.0.
After 30,000 optimisation steps, the agent has also developed quite some understanding
of its environment. We can compare figure 6, which was generated by actually propagating
22 KAIUELTZHO¨FFER
parameter description value
α learning rate of ADAM optimiser 0.001
exponential decay rates for moment
(β ,β ) (0.9, 0.999)
1 2 estimation of ADAM optimiser
noise parameter
(cid:15) 10−8
of ADAM optimiser
number of processes to
n approximate free energy for each 1
F
sample from the population density
number of samples from
n 104
pop the population density
Table 1. Parameter values used in algorithms 2 and 3.
the agent in its environment, with figure 7, which was generated by sampling from the
agents generative model of its environment, without any interaction with the environment.
We see that the agent did not only learn the timecourse of its proprioceptive sensory
channel o and its sense of position o , but also the - in this setting irrelevant - channel
a x
o , which is just a very nonlinear transformation of its position. Note that each panel of
h
figure 7 shows 10 processes sampled from the generative model as described in algorithm 1.
Note that we are approximating each density just by a single sample per timestep and per
process. Thus, although our estimates seem a bit noisy, they are very consistent with the
actual behavior of the agent and the variability can easily be reduced by averaging over
several processes.
Having learned a generative model of the environment, we can not only propagate it
freely, but we can also use it to test beliefs of the agent, given some a priori assumptions
on the timecourse of certain states or sensory channels, using the algorithm described in
section 3.5. We sampled the agent’s prior beliefs about his trajectory o ,o , given its
x,t h,t
proprioceptive inputs o , i.e. p(o ,...,o ,o ,...,o o ,...,o ) . Using the above
a,t x,1 x,T h,1 h,T a,1 a,T
|
example we took the average timecourse of the proprioceptive channel for the true in-
teraction with the environment and shifted it back 10 timesteps. The results are shown
in figure 8. First, we see that not all of the 10 sampled processes did converge. This
might be due to the Markov-Chain-Monte-Carlo-Sampling approach, in which the chain
has to be initialised close enough to the solution to guarantee convergence. However, for 9
out of 10 processes, the results look very similar to the true propagation (figure 6) and the
unconstrainedsamplesfromthegenerativemodel(figure7),onlyshiftedback10timesteps.
5. Discussion and Outlook
5.1. A Scalable and Flexible Active Inference Implementation. In this paper we
have shown that the free energy objective proposed by the active inference framework
in cognitive neuroscience (Friston et al., 2010) allows an agent to find a solution to the
mountain car environment, while concurrently building a generative model of itself and
DEEP ACTIVE INFERENCE 23
0.2
0.1
0.0
0.1
−
0.2
−
0.3
−
0.4
−
0.5
−
0.6
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
2.5
2.0
1.5
1.0
0.5
0.0
0 5 10 15 20 25 30
t/Steps
o (t)propagated
x
.u.a/ao
230Iterations
o (t)propagated
a
0.6
0.4
0.2
0.0
0.2
−
0.4
−
0.6
−
0.8
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.5
− 0 5 10 15 20 25 30
t/Steps
o (t)propagated
x
.u.a/ao
250Iterations
o (t)propagated
a
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.5
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
4
3
2
1
0
1
−
2
− 0 5 10 15 20 25 30
t/Steps
o (t)propagated
x
.u.a/ao
270Iterations
o (t)propagated
a
Figure 5. Theagent’sperformanceafter230, 250, and270trainingsteps,
using the mean parameters of the population density. It has just realised
that by moving uphill a bit to the left (from t = 15 to t = 20), it can
reach a higher position (around t = 27) than by directly going upwards (c.f.
t = 9). Shown are the agent’s proprioception o (upper row), and its sense
a
of position o (lower row).
x
its environment. By implementing the internal dynamics of our agent using deep neural
networks (LeCun et al., 2015; Goodfellow et al., 2016) and recurrent dynamics (Karpathy
et al., 2015), it is able to approximate the true generative process of its environment by
its own generative model. Furthermore, as we are using an efficient, black-box optimiser
for non-differentiable objective functions, the agent does not require direct access to any
informationregardingontruegenerativeprocessoritsderivatesforanygivenenvironment.
As the implementation and optimisation of this agent uses methods that are applied to
complex, large scale problems in machine learning and artificial intelligence (Chung et al.,
2015; Rezende et al., 2016; Kingma et al., 2016), we hope that this class of agent can be of
24 KAIUELTZHO¨FFER
1.5
1.0
0.5
0.0
0.5
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)propagated
x
1.0
0.8
0.6
0.4
0.2
0.0
0 5 10 15 20 25 30
t/Steps
.u.a/ho
2.0
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.5
− 0 5 10 15 20 25 30
t/Steps
o (t)propagated
h
.u.a/ao
o (t)propagated
a
0.15
0.10
0.05
0.00
0.05
−
0.10
−
0.15
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)propagated
1
Figure 6. The agent’s performance after 30,000 training steps, using the
mean parameters of the population density. It tightly sticks to a strategy,
shown here in terms of the proprioceptive sensory channel o (upper left).
a
Theresultingtrajectory(shownintermsofo ,upperright)firstleadsuphill
x
to the left, away from the target position, to gain momentum and overcome
the steep slope at x = 0. The nonlinearly modified sensory channel o is
h
shown on the lower left and the ”homeostatic” hidden state s on the lower
1
right.
DEEP ACTIVE INFERENCE 25
1.5
1.0
0.5
0.0
0.5
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)sampled
x
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
− 0 5 10 15 20 25 30
t/Steps
.u.a/ho
2.0
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.5
− 0 5 10 15 20 25 30
t/Steps
o (t)sampled
h
.u.a/ao
o (t)sampled
a
0.05
0.00
0.05
−
0.10
−
0.15
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)sampled
1
Figure 7. Ten processes sampled from the agent’s generative model of
the world after 30,000 training steps, using the mean parameters of the
population density. Shown are the prior expectations on the proprioceptive
channel o (upper left), the agent’s sense of position o (upper right), a
a x
nonlinear transformation of the position o , and the agent’s prior expecta-
h
tion on its ”homeostatic” state variable s . Note that each distribution is
1
approximated by a single sample per timestep per process.
26 KAIUELTZHO¨FFER
1.5
1.0
0.5
0.0
0.5
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)constrained
x
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
− 0 5 10 15 20 25 30
t/Steps
.u.a/ho
2.0
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.5
− 0 5 10 15 20 25 30
t/Steps
o (t)constrained
h
.u.a/ao
o (t)constrained
a
0.15
0.10
0.05
0.00
0.05
−
0.10
−
0.15
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)constrained
1
Figure 8. Ten processes sampled from the agent’s generative model of
the world after 30,000 training steps, constrained on a given trajectory of
the proprioceptive input o (upper left), using the mean parameters of the
a
population density. Shown are the constrained expectations on the pro-
prioceptive channel o (upper left), the agent’s sense of position o (upper
a x
right), a nonlinear transformation of the position o , and the agent’s con-
h
strained expectation on its ”homeostatic” state variable s . Note that each
1
distribution is approximated by a single sample per timestep per process.
DEEP ACTIVE INFERENCE 27
further use to demonstrate that active inference is able to solve more complex and realistic
problems, such as the Atari and 3D robotic environments from OpenAI Gym (Brockman
et al., 2016). The Atari environments require an agent to learn to play Atari games from
rawpixelorRAMinput, whilethe3DroboticenvironmentsusetheMujoCophysicsengine
(Todorov et al., 2012), to accurately simulate 3D robotic control problems.
5.2. Comparison to Original Implementation. In contrast to the original implemen-
tation of Friston et al. (2010), our implementation is formulated in discrete timesteps
without an explicit representation of generalised coordinates. I.e. our agent has to learn
how its observations of the position o are related between successive timesteps and to
x
form its own representation of its velocity. Furthermore, the agent’s generative model of
the world has - in contrast to the former work - not the same functional form as the true
generative process. I.e. our agent also has to learn an approximation of the true dynamics
in terms of its own generative model. This is possible due to the implementation of the
agents generative model in terms of a high-dimensional, recurrent neural network. These
structures were shown to be able to efficiently learn and represent the underlying structure
in complex time-series data from real-life applications (Karpathy et al., 2015; Le et al.,
2015; Chung et al., 2015). Our agent does also have no access to the partial derivatives
of its sensory inputs with respect to its actions, as these also depend on a knowledge of
the true generative process, which real agents usually do not possess. By using evolu-
tion strategies (Salimans et al., 2017), we are able to derive stochastic estimates of these
gradients that enable us to train a full active inference agent despite these constraints.
Furthermore, as the structure of our agent and the utilised transfer functions and optimis-
ers are directly adopted from large-scale machine learning (Chung et al., 2015), we hope
that our agent will scale to more realistic and rich environments, showcasing the potential
of active inference in terms of rich emergent behavior and the simultaneous optimisation
of a generative model of the environment.
5.3. Comparison to a recent ”Action-Oriented” Implementation of Active In-
ference. Another implementation of Active Inference was recently introduced by Baltieri
and Buckley (2017). Similar to our approach, the authors do not give their agent access
to the generative process, neither in terms of the functional form of the agent’s generative
model, norbyprovidingitwiththepartialderivativesofitsfullsensoryinputswithrespect
to its actions. However, their work differs in several crucial aspects from ours: Their agent
is formulated in continuous time, using partial differential equations. These equations are
simple enough to explicitly discuss their dynamics. In contrast, we use a discrete time
model with high-dimensional and highly nonlinear transfer functions, precluding us from a
classical analytical treatment of the resulting dynamics. However, the very flexible form of
the generative model in our approach allows our agent to learn an almost perfect approxi-
mation of the true dynamics, as shown in figure 7. Furthermore, they circumvent the lack
of explicit partial derivatives of the sensory inputs with respect to the agent’s actions, by
subdividing its sensory inputs into exteroceptive and proprioceptive channels, where they
use action to only suppress prediction errors in the proprioceptive channels. They call this
an”Action-Oriented”approach. Intermsofourmodelthiswouldbeequivalenttoignoring
28 KAIUELTZHO¨FFER
the partial derivatives of the ”exteroceptive” sensory channels o and o with respect to
x h
the action a and updating the parameters of the action function only using the gradients
of the expected sensory surprise < lnp (o s ) > under the popula-
−
θ a,t
|
t q(st|st−1,ox,t,o
h,t
,oa,t)
tion density. On one hand, this leads to an arbitrary subdivision of sensory channels and
necessitates the proprioceptive channel o , which in our approach is not really necessary
a
to successfully build a generative model of the environment and reach the goals defined by
the agent’s prior expectations (c.f. supplementary figures 9-11). On the other hand, this
might severely hinder or at least delay the agent’s learning in more complex and realistic
environments(c.f. supplementaryfigures12-14). Third,thefullactiveinferenceframework
(Friston et al., 2006, 2010) underlines the crucial role of those partial derivatives and hints
at possible approximations implemented in the brain. For example the retinotopic maps
in the early visual system, e.g. in the superficial and deep layers of the superior colliculus,
allow a quick and simple inversion of sensory input with respect to small eye movements
(Friston et al., 2010). Note that in active inference, action selection per se is generally
restricted to the sorts of inputs that the action system has access to (i.e., proprioceptive
inputs). However, learning the parameters of the action mapping – as in this work – is
clearly accountable to all sorts of inputs. This is effectively what we observe in our setup
(c.f. supplementary figures 12-14).
5.4. Action Function as State-Action Policy. The fixed mapping from the approxi-
mate posterior on states of the world, given the agents observations, to a distribution on
possible actions is known as a state-action policy in control theory. It presupposes a Bayes
optimal action for every state of the world. These are a very common forms of policies;
however, they are not universal policies in the sense that many actions depend upon previ-
ously encountered states. However, as we allow the agent to develop its own, very flexible
representation of the current state of the world, it can basically include a compressed rep-
resentation of the history of previous states, if this information might be required to guide
its action. Indeed, our agent shows this behavior, as it developed a representation of its
current velocity, which can be regarded as difference between its current and its previous
position. Otherwise it would not have been able to successfully solve the mountain car
problem, as the agent had to learn to move left from its initial position, to acquire some
additional momentum, while it later had to accelerate to the right at the exact same po-
sition, but after it had acquired the required additional momentum, which allowed it to
climb the steep slope at x = 0.0.
5.5. Emergent Curiosity. Optimising its generative model of the world gives the agent
a sense of epistemic value (Friston et al., 2015). I.e. it will not only seek out the states,
which it expects, but it will also try to improve its general understanding of the world.
This is similar to recent work on artificial curiosity (Pathak et al., 2017). However, in
contrast to this work, our agent is not only interested in those environmental dynamics
which it can directly influence by its own action, but also tries to learn the statistics of its
environment which are beyond its control.
DEEP ACTIVE INFERENCE 29
5.6. Constrained Sampling for Understanding the Learned Models. Wewereable
todemonstrateconstrainedsamplingfromtheagent’sgenerativemodeloftheworld. Com-
paring the constrained samples (figure 8) to the actual interaction (figure 6) with the en-
vironment and the unconstrained samples (figure 7), they look reasonable, but deviate
from the true dynamics that one would expect given the conditioned time course: We
constrained the sampling on the mean timecourse of o from the agent’s true interaction
a
with the world, shifted back in time by 10 time steps. This lead to samples of the other
variables, which were also shifted back in time. However, during the first 10 time steps the
agent believes that it would stay at its initial position, despite its strong push to the left.
This might be due to the fact that the relative weighting of the agent’s prior expectations
on its position is quite strong. Accordingly, it tightly sticks to the optimal trajectory, as
soon as it has learned it. Thus, the dynamics it infers deviate from the true dynamics,
as it only ever experiences a very small subset of its phase space. However, if you put
the agent in a noisy environment with larger stochastic fluctuations, where it is forced to
explore and encounter a wider variety of dynamic states, it should learn a more complete
andrealisticmodeloftheenvironmentaldynamics. Thishopealsorestsonthefactthatby
taking away its effector organs, our agent can be reduced to a generative recurrent latent
variable model. This class of models has been able to model and generate very complex
time series data, such as spoken language on the basis of its raw audio waveform (Chung
et al., 2015). Thinking about autonomous agents in a real environment (i.e. robots or
autonomous vehicles), constrained sampling from an agent’s generative model might be a
good way to understand its beliefs about how the world might react to given actions or
events, opening up the ”black-box” associated with classical deep reinforcement learning
algorithms.
5.7. Benefits of the Evolution Strategies Optimiser. Evolution strategies allow for
exploration of the environment without requiring the agent’s action functions to be proba-
bilistic: Using standard reinforcement-learning algorithms, such as deep Q-learning (Mnih
et al., 2015), the only way that the agent can discover new states, which do not corre-
spond to its current, local optimum, is by requiring it to stochastically take actions from
time to time. This might be by sampling completely random actions or lower bound-
ing the standard-deviation of its actions. Here, no such artificially forced exploration is
required, as the evolution strategies algorithm explores new solutions on the parameter
space (Salimans et al., 2017). So our agent can (and actually does) settle to an (almost)
fully deterministic policy, if such a policy exists in the respective environment. Moreover,
as the gradient estimates only depend on the full free energy, after the completion of each
individual simulation, they are also resilient against long time horizons and sparse rewards
(Salimans et al., 2017).
5.8. Variational Bayesian Perspective on Evolution Strategies. Although we are
using evolution strategies mainly as a black-box optimiser for our non-differentiable objec-
tive function, namely the sampling based approximation of the free energy bound on the
agent’s surprise, a subtle but important reinterpretation of the population density high-
lights the ubiquitous role of variational Bayes and the following minimisation of variational
30 KAIUELTZHO¨FFER
free energy. In brief, if we rename the population density p (θ) with q (θ) and write the
ψ ψ
likelihood as p(o s,θ) instead of p (o s) and the variational density as q(s o,θ) instead of
θ
| | |
q (s o), then the expected free energy becomes:
θ
|
(cid:68) (cid:69)
η(ψ,o) = lnp(o s,θ) +D (q(s o,θ) p(s θ))
(cid:104)− | (cid:105)q(s|o,θ) KL | || | q (θ)
ψ
If we now add a complexity penalty D (q (θ) p(θ)) on the population density, based on
KL ψ
||
a prior p(θ) on the parameters θ, the sum of the expected variational free energy under the
population density plus this KL divergence becomes
η(ψ,o)+D (q (θ) p(θ))
KL ψ
||
(cid:68) (cid:69)
= lnp(o s,θ) +D (q(s o,θ) p(s θ)) + lnq (θ) lnp(θ)
(cid:104)− | (cid:105)q(s|o,θ) KL | || | q (θ) (cid:104) ψ − (cid:105)q ψ (θ)
ψ
(cid:68) (cid:69)
= lnp(o s,θ) + lnq(s o,θ) lnp(s θ) +lnq (θ) lnp(θ)
(cid:104)− | (cid:105)q(s|o,θ) (cid:104) | − | (cid:105)q(s|o,θ) ψ − q (θ)
ψ
= lnp(o s,θ)+lnq(s o,θ) lnp(s θ)+lnq (θ) lnp(θ))
(cid:104)− | | − | ψ − (cid:105)q(s|o,θ)q ψ (θ)
= lnp(o s,θ)+lnq(s o,θ)q (θ) lnp(s θ)p(θ))
(cid:104)− | | ψ − | (cid:105)q(s|o,θ)q ψ (θ)
This is just the variational free energy under a proposal density q (s,θ o) = q(s o,θ)q (θ)
ψ ψ
| |
and prior p(s,θ) = p(s θ)p(θ) that cover both states and parameters, i.e. full variational
|
Bayes(c.f. (KingmaandWelling,2014),appendixF).Onthisview,thepopulationdynam-
ics afford a general and robust, ensemble based scheme for optimising model parameters
with respect to the variational free energy of beliefs over both model parameters and latent
states, neglecting prior information (or using a noninformative prior) on the parameters.
This is similar to the more general formulation of Friston (2008), who also absorbed deep
inferenceanddeeplearningproblemsunderthesameimperative(i.e., minimisationofvari-
ational free energy) to solve a dual estimation problem. One subtlety here is that the free
energy of the beliefs about parameters (i.e., the population density) minimises the path or
time integral of free energy – during which the parameters are assumed not to change.
5.9. Towards more flexible Expectations. Right now, our very direct way of hard-
coding the expectations of our agent, in terms of explicit prior distributions on the first
dimension of the agent’s latent space, seems very ad-hoc and restricted. However, we could
showthatduetotheflexibilityofouragent’sinternaldynamicsandtherobustoptimisation
strategies, our agent quickly learns to reach these very narrowly defined goal and is able to
buildarealisticmodeloftheenvironmentaldynamics, whichitencounters. Infuturework,
we plan to look into more complex a priori beliefs. This could be done by sampling from
the agent’s generative model as part of the optimisation process. E.g. one could sample
some processes from the generative model, calculate the quantity on which a constraint
in terms of prior expectations of the agent should be placed, and calculate the difference
between the sampled and the target distribution, e.g. using the KL-divergence. Then one
couldaddthisdifferenceaspenaltytothefreeenergy. Toenforcethisconstraint, onecould
use for example the Basic Differential Multiplier Method (Platt and Barr, 1988), which is
similar to the use of Lagrange multipliers, but which can be used with gradient descent
DEEP ACTIVE INFERENCE 31
optimisation schemes. The idea is to add the penalty term P(θ), which is equal to zero if
the constraint is fulfilled, to the function to be minimised F(θ), scaled by a multiplier λ.
The combined objective would look like this:
F (θ) = F(θ)+λP(θ)
constrained
To optimise the objective F(θ) while forcing the penalty term P(θ) to be zero, one can
perform a gradient descent on F (θ) for the parameters θ, but a gradient ascent
constrained
for the penalty parameter λ. This prospective sampling to optimise the goals of the agent
might be actually what parts of prefrontal cortex do when thinking about the future and
how to reach certain goals.
5.10. AfirststepfromArtificialLifetoArtificialGeneralIntelligence. Wepresent
here a flexible and scalable implementation of a general active inference agent, that is able
to learn a generative model of its environment while simultaneously achieving prespecified
goals, in terms of prior expectations on its perceived states of the world. We hope that
this implementation will prove useful to solve a wide variety of more complex and realistic
problems. By this, one could show how general, intelligent behaviour follows naturally
from the free energy principle. This principle, in turn, is derived from necessary properties
of dynamic systems in exchange with changing and fluctuating environments, which allow
them to sustain their identity by keeping their inner parameters within viable bounds,
i.e. a very basic homeostatic argument (Friston, 2012, 2013). Thus, we hope that our
work contributes to more concrete, experimental examples that intelligent behaviour nec-
essary follows and is hard to separate from the basic imperative to survive in and adapt to
changing environments.
6. Acknowledgements
The author would like to thank Karl Friston for his insightful comments on an ear-
lier version of this manuscript and the participants and organisers of the Computational
Psychiatry Course 2016 for stimulating lectures and discussions.
References
Adams, R. A., Stephan, K. E., Brown, H., Frith, C. D., and Friston, K. J. (2013). The
computational anatomy of psychosis. Frontiers in Psychiatry, 4(47).
Alais, D. and Burr, D. (2004). The ventriloquist effect results from near-optimal bimodal
integration. Current Biology, 14(3):257–262.
Baez, J. C. and Pollard, B. S. (2015). Relative entropy in biological systems.
https://arxiv.org/abs/1512.02742.
Baltieri, M. and Buckley, C. L. (2017). An active inference implementation of phototaxis.
https://arxiv.org/abs/1707.01806.
Berkes, P., Orb´an, G., Lengyel, M., and Fiser, J. (2011). Spontaneous cortical activity
reveals hallmarks of an optimal internal model of the environment. Science, 331:83–87.
32 KAIUELTZHO¨FFER
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and
Zaremba, W. (2016). Openai gym. https://arxiv.org/abs/1606.01540.
Brown, H. and Friston, K. J. (2012). Free-energy and illusions: the cornsweet effect.
Frontiers in Psychology, 3(43).
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., and Bengio, Y. (2015). A
recurrent latent variable model for sequential data. https://arxiv.org/abs/1506.02216.
Conant, R. and Ashby, W. (1970). Every good regulator of a system must be a model of
that system. International Journal of Systems Science, 1(2):89–97.
Ernst, M. and Banks, M. (2002). Humans integrate visual and haptic information in a
statistically optimal fashion. Nature, 415:429–433.
Friston, K. J. (2005). A theory of cortical responses. Phil. Trans. R. Soc. B, 360:815–836.
Friston, K. J. (2008). Hierarchical models in the brain. PLoS Computational Biology,
4(11).
Friston, K. J. (2010). The free-energy principle: a unified brain theory? Nature Reviews
Neuroscience, 11(2):127–38.
Friston, K.J.(2012). Afreeenergyprincipleforbiologicalsystems. Entropy, 14:2100–2121.
Friston, K. J. (2013). Life as we know it. Journal of The Royal Society Interface, 10.
Friston, K. J., Daunizeau, J., Kilner, J., and Kiebel, S. J. (2010). Action and behavior: a
free-energy formulation. Biological Cybernetics, 192(3):227–260.
Friston, K. J., Frith, C. D., Pezzulo, G., Hobson, J. A., and Ondobaka, S. (2017). Active
inference, curiosity and insight. Neural Computation, 29:1–51.
Friston, K. J. and Kiebel, S. J. (2009). Predictive coding under the free-energy principle.
Phil. Trans. R. Soc. B, 364:1211–1221.
Friston, K. J., Kilner, J., and Harrison, L. (2006). A free energy principle for the brain.
Journal of Physiology Paris, 100:70–87.
Friston, K. J., Mattout, J., and Kilner, J. (2011). Action understanding and active infer-
ence. Biological Cybernetics, 104:137–160.
Friston, K.J., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., andPezzulo, G.(2015).
Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214.
Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press, http:
//www.deeplearningbook.org.
Haefner, R., Berkes, P., and Fiser, J. (2016). Perceptual decision-making as probabilistic
inference by neural sampling. Neuron, 90(3):649–660.
Harper, M. (2009). The replicator equation as an inference dynamic.
https://arxiv.org/abs/0911.1763.
Hinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with
neural networks. Science, 313.
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are
universal approximators. Neural Networks, 2:359–366.
Karpathy, A., Johnson, J., and Fei-Fei, L. (2015). Visualizing and understanding recurrent
networks. https://arxiv.org/abs/1506.02078.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization.
https://arxiv.org/abs/1412.6980.
DEEP ACTIVE INFERENCE 33
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling,
M. (2016). Improving variational inference with inverse autoregressive flow.
https://arxiv.org/abs/1606.04934.
Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. ICLR.
Knill, D. and Pouget, A. (2004). The bayesian brain: the role of uncertainty in neural
coding and computation. Trends in Neurosciences, 27(12):712–9.
Le,Q.V.,Jaitly,N.,andHinton,G.E.(2015). Asimplewaytoinitializerecurrentnetworks
of rectified linear units. https://arxiv.org/abs/1504.00941.
LeCun, Y., Bengio, Y., and Hinton, G. E. (2015). Deep learning. Nature, 521:436–44.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).
Human-level control through deep reinforcement learning. Nature, 518:529–33.
Moore, A. (1991). Variable resolution dynamic programming: Efficiently learning action
maps in multivariate real-valued state-spaces. In Proceedings of the Eight International
Conference on Machine Learning. Morgan Kaufmann.
Moreno-Bote, R., Knill, D., andPouget, A.(2011). Bayesiansamplinginvisualperception.
Proc. Natl. Acad. Sci. U S A, 108(30):12491–6.
Pathak, D., Pulkit, A., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration
by self-supervised prediction. https://arxiv.org/abs/1705.05363.
Platt, J. C. and Barr, A. H. (1988). Constrained differential optimization. In Neural In-
formation Processing Systems, pages 612–621, New York. American Institute of Physics.
Rezende, D. J., Ali Eslami, S. M., Mohamed, S., Battaglia, P., Jaderberg, M.,
and Heess, N. (2016). Unsupervised learning of 3d structure from images.
https://arxiv.org/abs/1607.00662.
Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing flows.
JMRL, 37.
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and
approximate inference in deep generative models. ICML.
Salimans, T., Ho, J., Chen, X., and Sutskever, I. (2017). Evolution strategies as a scalable
alternative to reinforcement learning. https://arxiv.org/abs/1703.03864.
Schwartenbeck, P., Fitzgerald, T., Mathys, C., Dolan, R., Kronbichler, M., and Friston,
K. J. (2015). Evidence for surprise minimization over value maximization in choice
behavior. Scientific Reports, 5(16575).
Siegelmann, H. T. (1995). Computation beyond the turing limit. Science, 268:545–548.
Theano Development Team (2016). Theano: A Python framework for fast computation of
mathematical expressions. https://arxiv.org/abs/1605.02688.
Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based
control. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS).
Tomczak, J. M. and Welling, M. (2016). Improving variational auto-encoders using house-
holder flow. https://arxiv.org/abs/1611.09630.
34 KAIUELTZHO¨FFER
Watson, R. A. and Szathm´ary, E. (2016). How can evolution learn? Trends in Ecology
and Evolution, 31(2):147–157.
Wong, K.-F. and Wang, X.-J. (2006). A recurrent network mechanism of time integration
in perceptual decisions. The Journal of Neuroscience, 26(4):1314–28.
DEEP ACTIVE INFERENCE 35
7. Supplementary Material
7.1. Performance without an explicit proprioceptive channel. Evenwithoutdirect
feedbackonitsactions, intermsofaproprioceptivesensorychannelo , ouragentisableto
a
successfully learn the goal instilled in terms of its prior expectations, while simultaneously
building a generative model of its exteroceptive sensations. The convergence of the free
energyboundisshowninsupplementaryfigure9. Thetrueinteractionoftheagentwithits
environment after 30,000 training steps is shown in figure 10, and its generative model of
the world in figure 11. The full code can be accessed at http://www.github.com/kaiu85/
deepAI_paper.
200
150
100
50
0
0 100 200 300 400 500
Steps
ygrenE
eerF
200
150
100
50
0
0 5000 10000 15000 20000 25000 30000
Steps
ygrenE
eerF
Figure 9. Convergenceofanactiveinferenceagent,whichdoesnotpossess
a proprioceptive sensory channel, i.e. which gets no direct feedback on his
actions. The area shaded in red in the left plot was enlarged in the right
plot.
7.2. Performance with purely proprioceptive action. If action is used only to di-
rectly suppress proprioceptive surprise, the convergence of the learning process is severely
impaired,asshowninsupplementaryfigure12. Hereweoptimiseanagentwhosestructure,
parameters and objective function are identical to the Deep Active Inference agent in the
main text. However, the updates on the parameters of the agent’s action function only
depend on the gradient of the expectation value over the population density of the sensory
surprise in the proprioceptive channel < lnp (o s ) > with respect
−
θ a,t
|
t q(st|st−1,ox,t,o
h,t
,oa,t)
to the parameters of the action function. This corresponds to an agent which neglects
the direct changes in other sensory modalities due to its actions. One example might be
36 KAIUELTZHO¨FFER
the complex, nonlinear changes in the visual input to the retina, which arise even from
small eye movements. The full code can be accessed at http://www.github.com/kaiu85/
deepAI_paper.
Comparing supplementary figure 12 to figure 3 in the main text or to supplementary
figure 1, which shows the convergence of an active inference agent lacking any propriocep-
tive input, it is obvious that this reduction prevents the agent from successfully achieving
its goals and learning about its environment. This is also seen in the behavior of such an
agent after 30,000 training steps, shown in supplementary figure 13, and its - nonexistent
- generative model of the world shown in supplementary figure 14.
DEEP ACTIVE INFERENCE 37
1.5
1.0
0.5
0.0
0.5
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)propagated x
1.0
0.8
0.6
0.4
0.2
0.0
0 5 10 15 20 25 30
t/Steps
.u.a/ho
2.0
1.5
1.0
0.5
0.0
0.5
−
1.0
−
1.5
− 0 5 10 15 20 25 30
t/Steps
o (t)propagated
h
.u.a/ao
a(t)propagated
0.15
0.10
0.05
0.00
0.05
−
0.10
−
0.15
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)propagated
1
Figure 10. Performance of an agent without a proprioceptive sensory
channel o after 30,000 training steps, using the mean parameters of the
a
population density. The agent has acquired a very efficient strategy to
reach its goal position: it swings a bit to the left and then directly swings
up to its goal position x = 1.0. Shown are the agent’s action a (upper left),
its sense of position o (upper right), its nonlinearly transformed sensory
x
channel o and its ”homeostatic” hidden state s .
h 1
38 KAIUELTZHO¨FFER
1.5
1.0
0.5
0.0
0.5
−
1.0
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)sampled
x
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.2
− 0 5 10 15 20 25 30
t/Steps
.u.a/ho
o (t)
a
1.0
0.8
0.6
NOTPARTOF
GENERATIVEMODEL
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
o (t)sampled
h
0.15
0.10
0.05
0.00
0.05
−
0.10
−
0.15
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)sampled
1
Figure 11. Sample from the generative model acquired by an agent which
does not possess a proprioceptive sensory channel o after 30,000 training
a
steps. Shown are the agent’s sense of position o (upper right), its nonlin-
x
earlytransformedsensorychannelo andits”homeostatic”hiddenstates .
h 1
Note the very nice correspondence to its actual trajectory, when interacting
with the world, as shown in supplementary figure 10
DEEP ACTIVE INFERENCE 39
200
150
100
50
0
0 100 200 300 400 500
Steps
ygrenE
eerF
200
150
100
50
0
0 5000 10000 15000 20000 25000 30000
Steps
ygrenE
eerF
Figure 12. Convergence (or rather the lack of it) of an active inference
agent,whichusesactiononlytodirectlysuppressitsproprioceptivesurprise.
The area shaded in red in the left plot was enlarged in the right plot.
40 KAIUELTZHO¨FFER
0.47
−
0.48
−
0.49
−
0.50
−
0.51
−
0.52
−
0.53
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)propagated
x
0.00000374
0.00000372
0.00000370
0.00000368
0.00000366
0.00000364
0.00000362
0 5 10 15 20 25 30
t/Steps
.u.a/ho
0.0020
−
0.0022
−
0.0024
−
0.0026
−
0.0028
−
0.0030
−
0.0032
−
0.0034
− 0 5 10 15 20 25 30
t/Steps
o (t)propagated
h
.u.a/ao
o (t)propagated
a
0.040
−
0.045
−
0.050
−
0.055
−
0.060
−
0.065
−
0.070
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)propagated
1
Figure 13. Performance of an agent which uses action only to directly
suppress its proprioceptive surprise after 30,000 training steps, using the
mean parameters of the population density. The agent is stuck at its initial
position and shows no clear behavioral strategy. Shown are the agent’s
proprioceptivechannelo (upperleft), itssenseofpositiono (upperright),
a x
itsnonlinearlytransformedsensorychannelo andits”homeostatic”hidden
h
state s .
1
DEEP ACTIVE INFERENCE 41
0.46
−
0.47
−
0.48
−
0.49
−
0.50
−
0.51
−
0.52 −
0.53
− 0 5 10 15 20 25 30
t/Steps
.u.a/xo
o (t)sampled
x
0.004
0.003
0.002
0.001
0.000
0.001
−
0.002
−
0.003
− 0 5 10 15 20 25 30
t/Steps
.u.a/ho
0.004
0.002
0.000
0.002
−
0.004
−
0.006
−
0.008
−
0.010 −
0.012
− 0 5 10 15 20 25 30
t/Steps
o (t)sampled
h
.u.a/ao
o (t)sampled
a
0.3
0.2
0.1
0.0
0.1
−
0.2
−
0.3
−
0.4
− 0 5 10 15 20 25 30
t/Steps
.u.a/1s
s (t)sampled
1
Figure 14. Sample from the generative model acquired by an agent which
uses action only to directly suppress its proprioceptive surprise after 30,000
trainingsteps. Shownaretheagent’sproprioceptivechannelo (upperleft),
a
its sense of position o (upper right), its nonlinearly transformed sensory
x
channel o and its ”homeostatic” hidden state s . Note the lacking corre-
h 1
spondence to its actual trajectory (e.g. in o or s ), when interacting with
h 1
the world, as shown in supplementary figure 13