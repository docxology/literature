Interpreting Deep Neural Networks for Medical Imaging using Concept Graphs
Avinash Kori
koriavinash1@gmail.com
Department of Engineering Design
Indian Institute of Technology, Madras
Parth Natekar
patnat26@gmail.com
Department of Engineering Design
Indian Institute of Technology, Madras
Ganapathy Krishnamurthi
gankrish@iitm.ac.in
Department of Engineering Design
Indian Institute of Technology, Madras
Balaji Srinivasan
sbalaji@iitm.ac.in
Department of Mechanical Engineering
Indian Institute of Technology, Madras
Abstract
The black-box nature of deep learning models prevents
them from being completely trusted in domains like
biomedicine. Most explainability techniques do not cap-
ture the concept-based reasoning that human beings fol-
low. In this work, we attempt to understand the behavior
of trained models that perform image processing tasks
in the medical domain by building a graphical repre-
sentation of the concepts they learn. Extracting such a
graphical representation of the model’s behavior on an
abstract, higher conceptual level would help us to un-
ravel the steps taken by the model for predictions. We
show the application of our proposed implementation
on two biomedical problems - brain tumor segmenta-
tion and fundus image classiﬁcation. We provide an al-
ternative graphical representation of the model by for-
mulating a concept level graphas discussed above, and
ﬁnd active inference trails in the model. We work with
radiologists and ophthalmologists to understand the ob-
tained inference trails from a medical perspective and
show that medically relevant concept trails are obtained
which highlight the hierarchy of the decision-making
process followed by the model. Our framework is avail-
able at https://github.com/koriavinash1/
BioExp.
Introduction
Deep learning models are black boxes and as they are inte-
grated into medical diagnosis, it becomes necessary to give
a clear explanation of the concepts learnt by the model in a
form understandable to medical professionals (Holzinger et
al. 2017a). Clinicians also prefer upfront information about
the global properties of a model, such as its known strengths
and limitations (Cai et al. 2019).
For this, semantic concepts internal to the model and
their relationships need to be identiﬁed and represented
in a human-understandable form. Previous interpretability
techniques are example based or attention based (Molnar
2020), such as attribution, saliency, or feature visualiza-
tion, and do not reﬂect the ’concept-based thinking’ that
human-reasoning shows (Armstrong, Gleitman, and Gleit-
man 1983), neither do they allow us to uncover the model’s
understanding of the relationship between such concepts. In
the related work section we detail where our method stands
in relation to current work in this area.
Graphical models provide a tractable way to depict con-
cepts and the relationships between these concepts. How-
ever, there is a clear tug-of-war between model perfor-
mance and transparency in this context (Holzinger et al.
2017a). Consider, for example, that we build a simple
Bayesian Model for predicting the severity of Diabetic
Retinopathy, where each node in the Bayesian Model is
a human-understandable concept, such as microanuerisms,
dark spots, exudates, and hemorrhages. Assuming we learn
the structure and parameters of such a model, we would have
a completely transparent technique for our task. However, it
is difﬁcult and computationally taxing to achieve the same
level of performance with a Bayesian model as a deep neu-
ral network. This also requires an explicit differentiation or
concept-level labelling of all relevant concepts expected to
be in the Bayesian model, which is generally unavailable.
While Deep Neural Networks provide a much more efﬁ-
cient way to represent and learn from image data, they do
not lend themselves to the simple conceptual analysis that
graphical models like Bayesian Networks do. We propose
a method to repurpose a trained deep learning model into
an equivalent graphical structure at the level of abstract,
human-understandable concepts. This provides us with a
simple, transparent representation of the model’s logic and
allows us to determine the pathway it takes for making a pre-
diction. Such a concept level representation is similar to that
in deep probabilistic models, where the depth of the graph is
considered over concepts instead of the depth of the compu-
tational graph (Goodfellow, Bengio, and Courville 2016).
We posit that such an abstraction is possible in a deep
network since individual ﬁlters may be specialised to learn
individual concepts. In the context of representation learn-
ing, it is hypothesized that deeper representation learning
algorithms tend to discover more disentangled representa-
tions (Bengio 2013). For example, experiments in Network
Dissection show that individual ﬁlters learn disentangled vi-
sual concepts (Bau et al. 2017). This behaviour has also been
shown in the context of brain tumor segmentation models
(Natekar, Kori, and Krishnamurthi 2020). Grouping ﬁlters
which detect the same concept within a layer would then en-
able us to build a graphical representation of such concepts
inherent in the network.
This representation of the model has many advantages. It
can tell us about the model’s biases - for example, if it re-
arXiv:2008.06457v2  [cs.CV]  17 Nov 2020
lies heavily on one concept for one class of predictions. It
also allows us to determine active inference trails inherent
in the model, as we have shown in this work. Our main con-
tributions in this works are the following: (i) A method to
represent a Deep Neural Network as a graphical model over
abstract, high level concepts, encouraging concept-based ex-
plainability, and, (ii) Identiﬁcation of inference trails from
this graphical representation that help us understand the
model’s decision-making logic.
Proposed Framework
This work aims to abstract the model into an equivalent
graphical model representation where concepts learnt by the
network become nodes, and edges depict relationships be-
tween them. We take a clustering based approach to identify
weights which may be detecting similar concepts in the in-
put image. Such a method ensures that our explanations are
independent of the input sample and that our formulations
are computationally practical. Previous experiments show
that for state-of-the-art DNNs trained on large-scale datasets
like ImageNet (Deng et al. 2009), euclidian distance in the
activation space of ﬁnal layers is an effective perceptual sim-
ilarity metric (Zhang et al. 2018). It is not unreasonable that
such behaviour extends to deep learning models in the med-
ical domain. We use the euclidian distance between weight
vectors averaged across the channel dimension as our simi-
larity metric.
We posit that the weight clusters thus identiﬁed are re-
sponsible for detecting individual concepts in the input im-
age, and thus form the concept nodes in the abstracted graph-
ical model. We visualize the concept detected by the clusters
formed using a modiﬁcation of Grad-CAM (Selvaraju et al.
2017). Grad-CAM basically visualizes attention of a weight
layer on the input image. By zeroing out weights from other
clusters and only keeping weights from a particular clus-
ter before obtaining Grad-CAM attention maps, we can ﬁnd
what the weight cluster corresponds to in the input space.
Potential active inference trails are then found from the gen-
erated graphical model using a normalized mutual informa-
tion based approach.
The proposed framework for understanding the deep
learning models consists of the following steps: (i) concept
formation, (ii) concept identiﬁcation, (iii) concept signiﬁ-
cance analysis, (iv) graph formation, and (v) trail estimation.
Figure 1 and 2 provide a detailed overview of the described
framework. Next, we go over each section of this framework
in detail.
Concept Formation
We posit that groups of weight vectors in a layer are re-
sponsible for detecting a particular concept in the input im-
age. Weight clustering has been used before in the con-
text of network compression (Han, Mao, and Dally 2015;
Son, Nah, and Mu Lee 2018). We show that a clustering
based approach can be used to identify weights which are
responsible for detecting a particular concept in the input
image. Weight vectors can be clustered using a suitable met-
ric and their attention over the input image can be used to
determine the concept they are specialized to detect. Such
an analysis can be performed at any level of granularity, for
example one could perform the analysis choosing, say, only
the ﬁrst, ﬁfth, ninth, and eleventh layers of a deep network so
that a high level understanding can be gained of the concepts
learnt by these layers.
Let the trained network be Φ(W,X), and the layers cho-
sen for analysis be {...,l −n,l,l + m,...}. The clusters
{Cl
p,Cl
q,Cl
r,...}are formed as a result of clustering weights
at layer l in the network Φ. Let W = {w1,w2,...,w n}be
the set of weights in a layer, where W ∈Rf×f×inc×outc
and wi ∈ Rf×f×inc. Due to high dimensionality of the
weight tensor, we take the mean of the weight tensor across
the outcdimension to obtain a representative tensor wrep
i =
1
inc
∑
c wc
i ∈ Rf×f . To amplify the difference between
symmetric weights we encode position information (Kori,
Krishnamurthi, and Srinivasan 2018; Palop, Mucke, and
Roberson 2010) along with weights.
Clusters are formed using a hierarchical clustering
method (Johnson 1967) using distance-based threshold-
ing. This provides additional degrees of freedom to group
weights into as many numbers of signiﬁcantly different con-
cepts. After obtaining the clusters, for visual veriﬁcation
we view the ﬂattened weight vector to observe similarity
among the clustered weights. Since direct visual interpreta-
tion is insufﬁcient, to quantify the effectiveness of our clus-
tering method we use E(SilhouetteScore) over all weights
(Rousseeuw 1987) as a metric. Figure 11 in the Appendix
depicts this for a sample layer.
Concept Identiﬁcation
In the Concept identiﬁcation step, we try to associate formed
weight clusters with some region in the input image which
corresponds to a human-understandable concept.
Consider cluster Cl
p. To identify the concept learnt by the
cluster and to depict this in a human understandable fash-
ion, we ﬁrst modify the trained network by dissecting the
network at layer l, the outputs of which are denoted by Φl.
Then, we perform a variation of Grad-CAM (which we will
simply refer to as concept attention maps), using the ﬁlters
in the cluster Cl
p as the outputs for which attention is to be
computed, as described in equation 3.
In practice, this is done as follows. The dissected net-
work Φl is modiﬁed by adding a (1 ×1) convolution at the
end, the weights of which are set to one. We then set the
weights of all ﬁlters in the layer lwhich do not belong to the
cluster pto zero. The effective operation performed by the
added convolutional layer Φl+1 is then equivalent to taking
the mean across the channel dimension of only those ﬁlters
which belong to the cluster, providing a single-channel con-
densation of the cluster which can be used for ﬁnding the
concept-attention map. We denote the output of this layer
by Ek∼idxp Φl,k, where idxp are the set indices in a layer l
belonging to cluster Cl
p, as formulated in equation 1.
Concept identiﬁcation then amounts to ﬁnding the con-
cept attention maps of this output with respect to the activa-
tions of the penultimate layer in the dissected network, i.e.
Φl−1 as described in equation 2.
Figure 1: In the proposed framework, we construct a concept graph for a trained deep model. To generate concept graphs,
we cluster weights in user-deﬁned layers of the network, use them as concepts, and later estimate links based on a mutual
information based metric. For example, trails represented in red and blue show active concept-level inference trails a network
uses to predict the ﬁnal result
Figure 2: The above ﬁgure describes all the steps in the proposed concept-based interpretability framework visualized in Figure
1
yl
p(x) = 1
Z
∑
i
∑
j
(
Ek∼idxp Φl,k(x)
)
(1)
βl
m,p(x) = 1
Z
∑
i
∑
j
∂yl
p(x)
∂Φl−1,m(x) (2)
CAMl
p = ReLU
(∑
m
βl
m,p(x)Φl−1,m(x)
)
(3)
Where, m is the index of a ﬁlter in layer l−1 and k is
index of ﬁlter in layer l, β are the Grad-CAM importance
weights, i,j are the indices for the height and width di-
mensions of the feature map of the additional convolutional
layer, and CAM is the output concept-attention map for
concept pof layer l.
Once the concepts are identiﬁed, we conduct signiﬁcance
tests to ensure that the concepts formed are consistent,
robust, and localized. These procedures are detailed next.
Figures 8, 9, and 10 show the results of the conducted
consistency and robustness tests for our identiﬁed concepts,
which provide further evidence to support our hypothesis
that groups of weight vectors in the model are responsible
for detecting different semantic concepts.
Consistency: To evaluate the consistency of clusters
generated by the proposed method, we examine their
regularity over multiple input samples in our datasets.
Figure 10 illustrates the same, where each row corresponds
to the concept attention map for an identiﬁed cluster over
different images in the input dataset. It can be observed that
identiﬁed clusters have similar concept attention maps for
multiple input samples, irrespective of tumor location or
optic disk location.
Robustness: Here, we try to evaluate the robustness of
the formed clusters. Weights belonging to a speciﬁc layer in
a neural network can be considered as i.i.d (Giryes, Sapiro,
and Bronstein 2016). We posit that after learning, all the
weights belonging to a particular cluster come from an un-
derlying distribution and are i.i.d. We assume a gaussian
generating distribution for weights in the cluster and approx-
imate this using the ﬁrst and second order moment of the
weights in the cluster. Figure 4 depicts this graphically.
Consider an identiﬁed cluster Cl
p ∈Rf×f×inc×n, where
f is the ﬁlter size, incis the number of in-channels, and n
is the number of weights in the cluster. Let wi ∈Cl
p be a
weight belonging to the cluster Cl
p. Then, w ∈Rf×f×inc,
i.e. the cluster Cl
p contains nweight tensors wi of size f ×
f ×inc. We generate a gaussian distribution for each pixel
xj at position jin the ﬂattened weight wi,
Figure 3: Above image describes the process of link formation. Sub-ﬁgure (a) describes how pre-interventional distribution is
formed, sub-ﬁgure (b) describes how post-interventional distribution is formed, (c) exibits the condition for the existence of
edge.
x∼N(µ,σ) (4)
µ= Ei(xj),σ = Ei(xj −Ei(xj)) (5)
We then sample nnumber of weights as detailed above,
replace all n weights in the cluster Cl
p by the sampled
weights, and recompute our concept attention maps. Figures
8 and 9 show the results of this experiment.
We observe that recomputed concept attention maps cor-
respond to the same region in the input space as the original
concept attention maps. We also generate recomputed con-
cept attention maps using a uniform prior over the cluster
weights as well as a gaussian prior taken over the range of
all weights in the layer, and compare this with the results of
using a gaussian prior over only the cluster weights. It can
be observed that concept attention maps (CAMs) formed by
using gaussian priors over only the weights belonging to that
particular cluster are visually similar to the originals for each
sampling run, while CAMs formed using uniform priors or
CAMs formed using gaussian priors over all the weights do
not encode the same concept in the input space and show
high variability for each sampling run. This behaviour is
seen consistently over all input samples. Thus, we empiri-
cally justify that our identiﬁed concepts come from the same
underlying distribution, and that the gaussian is a reasonable
proxy for this distribution.
Network Formation and Information Flow
Once concepts and have been identiﬁed for the given set of
layers, we have the means to construct our equivalent graph-
ical representation.
Given these concepts, we can identify relationships be-
tween them to generate a human-understandable trace of
inference which augments model predictions. In order to
identify the relationship between two concepts, we com-
pute the normalized mutual information between the pre-
interventional and post-interventional feature map distribu-
tion, as described below.
For the directed link between two concepts in layer
p and q, Cp
i → Cq
j , the pre-interventional distribution
P(Φj(x |do(Cp
−i = 0))) is the feature map distribution
obtained on zeroing out the weights belonging to all con-
cepts other than Cp
i in layer p (i.e., do(Cp
−i = 0) , where
the do operator indicates a manual intervention on the ar-
gument to set it to a particular value, which is 0 in this
case). This distribution tells us about information ﬂowing
from Cp
i to all concepts in the succeeding layer q. Similarly,
the post-interventional distribution Q(Φj(x |do(Cp
−i =
0),do(Cq
−j = 0))) is the feature map distribution obtained
at the layer q by zeroing out the weights belonging to all
the clusters other than i in layer p as well as the weights
belonging to all the clusters other than j in layer q (i.e.,
do(Cp
−i = 0) and do(Cq
−j = 0)). This distribution tells us
about the information ﬂowing only from Cp
i to Cq
j . In this
formulation the termspre and post interventional are consid-
ered only with respect to layerq. Figure 3 shows this process
graphically.
Based on our formulation, the directed link Cp
i →Cq
j ,
exists only if equation 6 is satisﬁed.
NMI
(
Q
(
Φj(x|do(Cp
−i = 0),do(Cq
−j = 0))
)
,
P
(
Φj(x|do(Cp
−i = 0))
))
>T
(6)
This basically states that the link exists only if the mu-
tual information between pre and post interventional dis-
tribution is higher than a set threshold. High mutual infor-
mation implies that a signiﬁcant portion of the information
ﬂowing from the concept Cp
i to layer q occurs through that
speciﬁc link Cp
i →Cq
j . This results in the formation of a
concept graph, an example visualization of which is shown
in Figure 4. Note that this graphical model is not intended
to be complete, only representative. Since our graph can be
constructed over any set of layers chosen by the user, there
could be multiple inference trails that denote relationships
between different concepts.
Figure 4: A visual depiction of the constructed graphical representation for the network given the set of layers to analyse. Each
pixel in a concept can be imagined to be drawn from its own gaussian distribution, using the mean and variance of the pixel
over the cluster as parameters. Dotted arrows show the concept is sampled from its corresponding normal distribution. Dark
arrows show links between concepts.
Figure 5: Concepts obtained from various layers of a trained U-net model superposed over the MRI Flair channel. (a) C3
0 :
doesn’t capture any input region, (b) C3
1 : concave edges, (c) C3
2 : linear edges, (d) C5
2 : interior key points. (e) C13
0 : Lateral
left hemispherical brain boundary, (f) C13
3 : Lateral left hemispherical and tumor core brain boundary, (g) C15
2 : Anterior tumor
boundary, (h) C15
3 : Tumor core boundary, (i) C19
2 : Whole tumor boundary, (j) C17
0 : Lateral brain boundary and tumor core
boundary, (k) C21
1 : Diffused tumor core region, (l) C21
2 : Tumor core region.
Trail Estimation
Given our graphical representation and the existence of links
between concepts, we now have a method to track inference
steps taken by the model. The obtained concept graph is a
DAG with depth m, where mis number of layers speciﬁed
by the user for interpretability. The trails are all the paths
running from input to a particular node used in an inference.
The obtained trails encode the ﬂow of concept level infor-
mation used in making a prediction.
For example, consider the sample trailX →C1 →C4 →
C8 →Y in Figure 4. Medical professionals can then high-
light whether or not such an inference trail makes sense from
a biomedical perspective, and understand the model’s biases
and its common logical steps of inference. The next section
details the application of the above framework on bench-
mark biomedical image datasets.
Experiments
We illustrate the working of our proposed framework on
both classiﬁcation and segmentation tasks. For the classiﬁ-
cation task, we considered the Diabetic Retinopathy prob-
lem, and for segmentation, we considered the Brain Tu-
mor Segmentation problem. In both the experiments, the
aim was to explain the building blocks of the model,
and understand the hierarchy of decision making in deep
learning models. All the experiments and results can
be reproduced by using notebooks provided in the code
repository https://github.com/koriavinash1/
BioExp_Experiments.
Brain Tumor Segmentation
In the past decade, there has been signiﬁcant development of
image processing algorithms for segmenting intra-tumoral
structures in brain MRI images (Bakas et al. 2018). Deep
Learning has shown great potential in this context, with the
BraTS challenge (Kamnitsas et al. 2017; Wang et al. 2017;
Myronenko 2018; Kori et al. 2018) setting the benchmark
for research in this area. The BraTS dataset contains nearly
300 brain MRI volumes annotated by experts for tumor re-
gions. Various deep learning algorithms have shown great
performance in segmenting tumor core, enhancing tumor,
and edema regions from these MRI volumes.
We implement our algorithm on a UNet based model for
brain tumor segmentation, which is a popular segmentation
architecture in the medical context (Ronneberger, Fischer,
and Brox 2015). Our model also has residual connections as
per (Kermi, Mahmoudi, and Khadir 2018), and achieves a
dice score of 0.788, 0.743, and 0.649 on whole tumor, tu-
mor core and enhancing tumor segmentation respectively on
a held-out validation set of 48 volumes. Our model is not
meant to achieve state of the art performance. Instead, we
aim to demonstrate our method on a commonly used ar-
chitecture for brain-tumor segmentation. The next sections
detail the concepts and active inference trails obtained as a
result of our framework on this task.
Concepts The E(SilhouetteScore) over all the data-
points is 0.241, indicating the formation of weak but signif-
icant clusters. Figure 5 describes the various concepts iden-
tiﬁed from our model. Initial layers (convolutional layers 3
and 5) correspond to edges in a speciﬁc direction or brain
boundaries. In higher layers, ﬁlters start capturing more lo-
cal information. It can be observed that some concepts cap-
ture brain boundary, while some capture tumor boundary.
Figure 5 contains a description of the various concepts ob-
tained from out network. This behaviour is in line with the
understanding that ﬁlters in shallower layers of brain tu-
mor segmentation models learn simple patterns while deeper
layers learn progressively more complex concepts (Natekar,
Kori, and Krishnamurthi 2020). The brain atlas described in
(Ding et al. 2016) was used to formulate appropriate descrip-
tions.
Trails and Discoveries Figure 6 describes inference trails
involved in predicting the enhancing tumor region (Trails for
other classes are available in the Appendix ). These show
the model’s attention is initially on the outer edges and key-
points of the brain, then moves to the white and grey matter
region, then the tumor boundary, and ﬁnally the internal tu-
mor region. The caption of Figure 6 also provides a descrip-
tion of the visual trails for an image based on the predeﬁned
concept description. In the discussion section, we analyse
these trails with feedback from a certiﬁed radiologist.
Diabetic Retinopathy classiﬁcation
Diabetic Retinopathy (DR) is frequent in individuals suf-
fering from diabetes (Fong et al. 2004). Deep Learning al-
gorithms have shown great promise in detecting the sever-
ity of diabetic retinopathy and have the potential to greatly
simplify diagnosis and detection. We implement our frame-
work on a ResNet50 based network which achieves a Co-
hen Kappa Score of 0.71 on the validation set of the AP-
TOS dataset (Society 2019). The APTOS dataset contains
around 5500 retina images taken using fundus photography.
The severity of diabetic retinopathy has been rated for each
image on a scale of 0 (no DR) to 4 (Proliferative DR). Each
stage of DR is characterized by certain features - such as mi-
croanuerisms, exudates, and hemorrhages. Thus, it becomes
necessary to see whether deep learning models process and
identify these features, and to see the model’s understand-
ing of relationships between these and the predicted severity
of DR. We follow a similar process as that for brain tumor
segmentation, detailed below.
Concepts The E(SilhouetteScore) over all the data-
points is 0.2, which again indicates the formation of weak
but signiﬁcant clusters. Figure 12 describes the identiﬁed lo-
cal and global level concepts, encoding blood vessels, hard
and soft exudates, dot-blot hemorrhages, etc.
Trails and Discoveries Similar to the trails obtained for
the BraTS dataset, we show example inference trails ob-
tained for the APTOS dataset in Figure 7 and Figures 13 and
14 in the Appendix. These describe visual trails involved in
predicting ’Severe’, ’Moderate’, and ’Proliferative’ classes
of diabetic retinopathy respectively. An ophthalmologist’s
feedback was obtained on the concept trails, which is elabo-
rated in the discussion section. Once again, we see the emer-
gence of medically relevant concepts in a hierarchical man-
ner, which may provide additional support to medical pro-
fessionals apart from just the output classiﬁcation.
Related Work
Explainability is generally categorized into post-hoc and
ante-hoc methods, where post-hoc explainability methods
try to analyze and make inferences on trained models (Si-
monyan, Vedaldi, and Zisserman 2013; Zeiler and Fergus
2014; Ustun and Rudin 2014). In contrast, ante-hoc meth-
ods try to build an explainable model while training itself
(Caruana et al. 2015; Holzinger et al. 2017b; ).
Current research directions in post-hoc interpretability
focus mainly on visualizing network attributions or illus-
trative samples in the input space (Selvaraju et al. 2017;
Bau et al. 2017; Olah, Mordvintsev, and Schubert 2017;
Kim et al. 2018). Our work is related to methods involving
disentangled latent representations and concept based expla-
nations. For example, previous experiments on network dis-
section show that deep networks learn disentangled latent
concepts (Bau et al. 2017). Previous concept based inter-
pretability methods (Ghorbani et al. 2019; Kim et al. 2018)
use input patches to identify salient concepts that lead to a
particular output. This has been extended to include a com-
pleteness measure for identiﬁed concepts (Yeh et al. 2019).
However, neither of these methods consider the relationship
between concepts learnt by the model and do not provide a
trace of inference steps. Also, these methods either require
a pre-processed set of input samples as concepts (Kim et
Figure 6: Active inference trail for enhancing tumor (Each row is a trail for one input sample, red regions are high attention):
(I: Input image to a network)− > (C1: Concave edges)− > (C2: White matter region)− > (C3: Tumor boundary)− > C4: (Lateral brain
boundary) − > (C5: Inferior tumor boundary)− > (Enhancing Tumor)
Figure 7: Active inference trail for severe DR (green regions are high attention): (I: Input Image)−>(C1: Optic Cup/Hard
exudates) −>(C2: Hard Exudates)−>(C3: Blood vessels, soft exudates)−>(C4: Blood vessel, soft exudates)−>(C5:
dot-blot Hemorrhages/laser scar marks of retinal photocoagulation)
al. 2018), or automatically segment the input image at var-
ious resolutions to create concepts (Ghorbani et al. 2019).
However, in the medical domain, obtaining such concepts is
difﬁcult - manual concept curation is time consuming and
would require medical experts, while segmenting the input
image may not lead to the formation of coherent anatomi-
cal concepts which add interpretability value, especially in
cases where the task itself is image segmentation. In such
domains, interpretability needs to emerge organically from
the model itself and provide an understanding of the model’s
decision making logic.
Our work introduces a post-hoc interpretability method,
by abstracting the trained model into interpretable concept
graphs, where concepts and their relationships emerge im-
plicitly from the model, doing away with the need for user-
curated input concepts. Our concept graphs allow easy vi-
sualization of the model’s logic on an abstract, human-
understandable level.
Discussion
This work aims to provide concept-based interpretability for
deep neural networks, demonstrating the results on medical
data. We use a clustering technique to extract a graphical
representation of concepts in the network, and visualize the
clustered concepts using a variation of Grad-CAM. We then
use an information-theoretic measure to determine relation-
ships between concepts and build concept level inference
trails within our network. Our results show that consistent,
distinct trails that lead to a particular classiﬁcation made up
of anatomically relevant concepts can be identiﬁed.
While in previous work on interpretability in the med-
ical domain (Natekar, Kori, and Krishnamurthi 2020), the
existence of disentangled concepts is shown in brain-tumor
segmentation networks, in this work we create a concept-
level graph that depicts the relationships between these con-
cepts and provides an understanding of inference trails in the
model. As opposed to previous concept-based approaches
(Ghorbani et al. 2019; Kim et al. 2018), no manual extrac-
tion of concepts from the input dataset is required, which
is a challenging task in the medical domain. In this initial
work, we demonstrate the potential of our technique on two
medical datasets - the BraTS dataset for brain tumor seg-
mentation and the APTOS dataset for diabetic retinopathy
classiﬁcation.
For brain-tumor segmentation, a certiﬁed radiologist’s
comments on the extracted concept trail was solicited. They
noted the lateral to medial and anterior to superior nature of
attention of the model, as well as the hierarchical approach
to segmentation which is in line with a radiologist’s thought
process. They commented that tumour boundary delineation
as seen in Figure 6 concept C3 has value for neurosurgeons
when obtaining biopsy or resecting the tumour since this
helps prevent damage to unaffected brain tissue. They also
noted that a neuroradiologist would be able to immediately
perceive the presence of gliomas in the ﬂair sequence and it
is in general not possible to break down that perception in
terms of the trails obtained from the concept graphs. How-
ever, the visualization of concepts that are focused on tu-
mour boundaries and the tumour core would help in improv-
ing conﬁdence and trust in the deep learning model. The tu-
mor core and characteristics are also deﬁned which will aid
in diagnosis and grading of the tumor.
For Diabetic Retinopathy, an ophthalmologist’s feedback
was obtained on the output trail described in Figure 7. Vari-
ous features, such as hard and soft exudates, dot-blot haem-
orrhages, optic cup, and laser scar marks of retinal photo-
coagulation were identiﬁed. In the case of DR, it is interest-
ing that features like this, which ophthalmologists look at to
classify DR images, emerge implicitly from the model, even
though it has not been explicitly trained to learn these.
Acknowledgments. We would like to acknowledge help
from Dr. Ravikanth Balaji and Dr. Devika Joshi for provid-
ing clinician (radiological and opthalmological) feedback on
the inference trails obtained.
References
Armstrong, S. L.; Gleitman, L. R.; and Gleitman, H. 1983.
What some concepts might not be. Cognition 13(3):263–
308.
Bakas, S.; Reyes, M.; Jakab, A.; Bauer, S.; Rempﬂer, M.;
Crimi, A.; Shinohara, R. T.; Berger, C.; Ha, S. M.; Rozycki,
M.; et al. 2018. Identifying the best machine learning al-
gorithms for brain tumor segmentation, progression assess-
ment, and overall survival prediction in the brats challenge.
arXiv preprint arXiv:1811.02629.
Bau, D.; Zhou, B.; Khosla, A.; Oliva, A.; and Torralba, A.
2017. Network dissection: Quantifying interpretability of
deep visual representations. InProceedings of the IEEE con-
ference on computer vision and pattern recognition, 6541–
6549.
Bengio, Y . 2013. Deep learning of representations: Look-
ing forward. In International Conference on Statistical Lan-
guage and Speech Processing, 1–37. Springer.
Cai, C. J.; Winter, S.; Steiner, D.; Wilcox, L.; and Terry,
M. 2019. ” hello ai”: Uncovering the onboarding needs of
medical practitioners for human-ai collaborative decision-
making. Proceedings of the ACM on Human-computer In-
teraction 3(CSCW):1–24.
Caruana, R.; Lou, Y .; Gehrke, J.; Koch, P.; Sturm, M.; and
Elhadad, N. 2015. Intelligible models for healthcare: Pre-
dicting pneumonia risk and hospital 30-day readmission. In
Proceedings of the 21th ACM SIGKDD international confer-
ence on knowledge discovery and data mining, 1721–1730.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
Fei, L. 2009. ImageNet: A Large-Scale Hierarchical Image
Database. In CVPR09.
Ding, S.-L.; Royall, J. J.; Sunkin, S. M.; Ng, L.; Facer, B. A.;
Lesnar, P.; Guillozet-Bongaarts, A.; McMurray, B.; Szafer,
A.; Dolbeare, T. A.; et al. 2016. Comprehensive cellular-
resolution atlas of the adult human brain. Journal of Com-
parative Neurology524(16):3127–3481.
Fong, D. S.; Aiello, L.; Gardner, T. W.; King, G. L.;
Blankenship, G.; Cavallerano, J. D.; Ferris, F. L.; and Klein,
R. 2004. Retinopathy in diabetes. Diabetes care27(suppl
1):s84–s87.
Ghorbani, A.; Wexler, J.; Zou, J. Y .; and Kim, B. 2019. To-
wards automatic concept-based explanations. In Advances
in Neural Information Processing Systems, 9277–9286.
Giryes, R.; Sapiro, G.; and Bronstein, A. M. 2016. Deep
neural networks with random gaussian weights: A universal
classiﬁcation strategy? IEEE Transactions on Signal Pro-
cessing 64(13):3444–3457.
Goodfellow, I.; Bengio, Y .; and Courville, A. 2016. Deep
learning. MIT press.
Han, S.; Mao, H.; and Dally, W. J. 2015. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149.
Holzinger, A.; Plass, M.; Kickmeier-Rust, M.; Holzinger,
K.; Cris ¸an, G. C.; Pintea, C.-M.; and Palade, V . Interac-
tive machine learning: experimental evidence for the human
in the algorithmic loop. Applied Intelligence 49(7):2401–
2414.
Holzinger, A.; Biemann, C.; Pattichis, C. S.; and Kell, D. B.
2017a. What do we need to build explainable ai systems for
the medical domain? arXiv preprint arXiv:1712.09923.
Holzinger, A.; Plass, M.; Holzinger, K.; Crisan, G. C.;
Pintea, C.-M.; and Palade, V . 2017b. A glass-box in-
teractive machine learning approach for solving np-hard
problems with the human-in-the-loop. arXiv preprint
arXiv:1708.01104.
Johnson, S. C. 1967. Hierarchical clustering schemes. Psy-
chometrika 32(3):241–254.
Kamnitsas, K.; Ledig, C.; Newcombe, V . F.; Simpson, J. P.;
Kane, A. D.; Menon, D. K.; Rueckert, D.; and Glocker, B.
2017. Efﬁcient multi-scale 3d cnn with fully connected crf
for accurate brain lesion segmentation. Medical image anal-
ysis 36:61–78.
Kermi, A.; Mahmoudi, I.; and Khadir, M. T. 2018.
Deep convolutional neural networks using u-net for au-
tomatic brain tumor segmentation in multimodal mri vol-
umes. In International MICCAI Brainlesion Workshop, 37–
48. Springer.
Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C.; Wexler, J.;
Viegas, F.; et al. 2018. Interpretability beyond feature at-
tribution: Quantitative testing with concept activation vec-
tors (tcav). InInternational conference on machine learning,
2668–2677.
Kori, A.; Soni, M.; Pranjal, B.; Khened, M.; Alex, V .; and
Krishnamurthi, G. 2018. Ensemble of fully convolutional
neural network for brain tumor segmentation from magnetic
resonance images. In International MICCAI Brainlesion
Workshop, 485–496. Springer.
Kori, A.; Krishnamurthi, G.; and Srinivasan, B. 2018. En-
hanced image classiﬁcation with data augmentation using
position coordinates. arXiv preprint arXiv:1802.02183.
Molnar, C. 2020. Interpretable Machine Learning. Lulu.
com.
Myronenko, A. 2018. 3d mri brain tumor segmentation
using autoencoder regularization. In International MICCAI
Brainlesion Workshop, 311–320. Springer.
Natekar, P.; Kori, A.; and Krishnamurthi, G. 2020. Demys-
tifying brain tumor segmentation networks: Interpretability
and uncertainty analysis. Frontiers in Computational Neu-
roscience 14:6.
Olah, C.; Mordvintsev, A.; and Schubert, L. 2017. Fea-
ture visualization. Distill. https://distill.pub/2017/feature-
visualization.
Palop, J. J.; Mucke, L.; and Roberson, E. D. 2010. Quanti-
fying biomarkers of cognitive dysfunction and neuronal net-
work hyperexcitability in mouse models of alzheimer’s dis-
ease: depletion of calcium-dependent proteins and inhibitory
hippocampal remodeling. In Alzheimer’s Disease and Fron-
totemporal Dementia. Springer. 245–262.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:
Convolutional networks for biomedical image segmentation.
In International Conference on Medical image computing
and computer-assisted intervention, 234–241. Springer.
Rousseeuw, P. J. 1987. Silhouettes: a graphical aid to the
interpretation and validation of cluster analysis. Journal of
computational and applied mathematics20:53–65.
Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;
Parikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-
tions from deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on com-
puter vision, 618–626.
Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2013.
Deep inside convolutional networks: Visualising image
classiﬁcation models and saliency maps. arXiv preprint
arXiv:1312.6034.
Society, A. P. T.-O. 2019. Asia paciﬁc tele-ophthalmology
society 2019, dataset.
Son, S.; Nah, S.; and Mu Lee, K. 2018. Clustering convo-
lutional kernels to compress deep neural networks. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), 216–232.
Ustun, B., and Rudin, C. 2014. Methods and mod-
els for interpretable linear classiﬁcation. arXiv preprint
arXiv:1405.4047.
Wang, G.; Li, W.; Ourselin, S.; and Vercauteren, T.
2017. Automatic brain tumor segmentation using cascaded
anisotropic convolutional neural networks. In International
MICCAI brainlesion workshop, 178–190. Springer.
Yeh, C.-K.; Kim, B.; Arik, S. O.; Li, C.-L.; Ravikumar, P.;
and Pﬁster, T. 2019. On concept-based explanations in deep
neural networks. arXiv preprint arXiv:1910.07969.
Zeiler, M. D., and Fergus, R. 2014. Visualizing and under-
standing convolutional networks. In European conference
on computer vision, 818–833. Springer.
Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,
O. 2018. The unreasonable effectiveness of deep features as
a perceptual metric. In Proceedings of the IEEE conference
on computer vision and pattern recognition, 586–595.
Appendix I
Here we show additional ﬁgures and examples which result from our primary analysis above. First, the results for cluster
signiﬁcance tests are shown - robustness and consistency. Then we show additional examples for brain-tumor segmentation and
diabetic retinopathy classiﬁcation, as well as other supporting images.
(a) Layer: 19, Gaussian Prior over entire weight layer
(b) Layer: 19, Uniform Prior over only the weight cluster
(c) Concept: C19
2 , Gaussian Prior over only the weight cluster
Figure 8: This ﬁgure illustrates results of robustness experiments on BraTs data, (a) Concept attention maps by assuming
Gaussian distribution over all the weights in a layer, (b) Concept attention maps by assuming Uniform distribution over only
the cluster weights, and (c) Concept attention maps by assuming Gaussian distribution over only the cluster weights. Note that
using a gaussian prior over only the cluster gives most consistent concept attention maps.
(a) Layer: 3d Gaussian Prior over entire weight layer
(b) Layer: 3d Uniform Prior over only the cluster weights
(c) Concept: C3d
4 Gaussian Prior over only the cluster weights
Figure 9: This ﬁgure illustrates results of robustness experiments on APTOS data, (a) Concept attention maps by assuming
Gaussian distribution over all the weights in a layer, (b) Concept attention maps by assuming Uniform distribution over only
the cluster weights, and (c) Concept attention maps by assuming Gaussian distribution over only the cluster weights. Note that
using a gaussian prior over only the cluster gives most consistent concept attention maps.
(a) BraTS Concept: C21
2 Tumor Core region
(b) BraTS Concept: C19
2 Whole Tumor boundary
(c) APTOS Concept: C2a
2 Lateral Eye boundary
(d) APTOS Concept: C3d
4 Major Blood vessels
Figure 10: The above ﬁgure shows the consistency of concept formation; each row indicates shows the concept-attention map
for a cluster for different input samples
Figure 11: Above image describes the effectiveness of clustering. Sub-ﬁgure (a) describes the initial layer weights from
ResNet50 trained on APTOS (Society 2019) data, in the ﬁgure dark blue horizontal bands seperates the weights among multiple
clusters (provided ﬁgure has 3 clusters). Sub-ﬁgure (b) quantiﬁes the effectiveness of clusters obtained as the result of proposed
method using a silhouette plot
Figure 12: This ﬁgure illustrates the concepts obtained from various layers of a trained ResNet50 model. Based on the region
of activation we provide description of the concepts as follows: (a)C1
1 : doesn’t capture any input region, (b)C1
2 : Right lateral
edges, (c) C2a
1 : Lateral edges, (d)C2a
2 : Optic disk + lateral edges, (e)C2c
2 : Optic disk + blood vessels, (f)C3a
2 : All blood vessels
(tiny), (g) C3d
4 : Major blood vessels, (h) C3d
5 : Blood vessels (eroded), (i) C4a
2 : Yellow spots (may be hard exodates), (j) C4f
1 :
Yellow spots (may be hard exodates), (k)C4a
3 : Pale Yellow (may be hard exodates), (l)C5c
2 : Hard/Soft exodates
Figure 13: Active inference trail for Moderate DR (Green regions are high attention): (I: Input Image to a network)−>(C1:
Soft exudates + Optic Cup)−>(C2: Hard exudates)−>(C3: All blood vessels)−>(C4: Optic disk and blood vessels)
−>(C5: Inverted Blood vessel (eroded) Image)−>(C6: Dark spots)
Figure 14: Active inference trail for Proliferative DR (Green regions are high attention):(I: Input Image to a network)−>(C1:
Pale areas, due to attenuated artery endings + macula)−>(C2: Hard exudates)−>(C3: All blood vessels + key points)
−>(C4: Optic disk and blood vessels)−>(C5: Laser scar marks of retinal photocoagulation + blot haemorrhages−>
(C6: Dark spots)
Figure 15: Active inference trail for Edema (Each row is a trail for one input sample, red regions are high attention): (I: Input
Image to a network)− > (C1: Concave edges)− > (C2: White matter)− > (C3: Brain and tumor boundary)− > C4: (Lateral brain
boundary) − > (C5: Lateral tumor boundary and mid brain)− > (Edema region)
Figure 16: Active inference trail for Tumor Core (Each row is a trail for one input sample, red regions are high attention): (I:
Input Image to a network)− > (C1: Concave edges)− > (C2: White matter)− > (C3: Brain and tumor boundary)− > C4: Tumor Core)