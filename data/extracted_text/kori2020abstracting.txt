Interpreting Deep Neural Networks for Medical Imaging using Concept Graphs
AvinashKori ParthNatekar
koriavinash1@gmail.com patnat26@gmail.com
DepartmentofEngineeringDesign DepartmentofEngineeringDesign
IndianInstituteofTechnology,Madras IndianInstituteofTechnology,Madras
GanapathyKrishnamurthi BalajiSrinivasan
gankrish@iitm.ac.in sbalaji@iitm.ac.in
DepartmentofEngineeringDesign DepartmentofMechanicalEngineering
IndianInstituteofTechnology,Madras IndianInstituteofTechnology,Madras
Abstract Graphical models provide a tractable way to depict con-
cepts and the relationships between these concepts. How-
Theblack-boxnatureofdeeplearningmodelsprevents
ever, there is a clear tug-of-war between model perfor-
them from being completely trusted in domains like
biomedicine.Mostexplainabilitytechniquesdonotcap- mance and transparency in this context (Holzinger et al.
turetheconcept-basedreasoningthathumanbeingsfol- 2017a). Consider, for example, that we build a simple
low.Inthiswork,weattempttounderstandthebehavior Bayesian Model for predicting the severity of Diabetic
oftrainedmodelsthatperformimageprocessingtasks Retinopathy, where each node in the Bayesian Model is
in the medical domain by building a graphical repre- a human-understandable concept, such as microanuerisms,
sentationoftheconceptstheylearn.Extractingsucha darkspots,exudates,andhemorrhages.Assumingwelearn
graphicalrepresentationofthemodel’sbehavioronan
thestructureandparametersofsuchamodel,wewouldhave
abstract, higher conceptual level would help us to un-
acompletelytransparenttechniqueforourtask.However,it
ravelthestepstakenbythemodelforpredictions.We
is difficult and computationally taxing to achieve the same
show the application of our proposed implementation
levelofperformancewithaBayesianmodelasadeepneu-
on two biomedical problems - brain tumor segmenta-
tionandfundusimageclassification.Weprovideanal- ralnetwork.Thisalsorequiresanexplicitdifferentiationor
ternativegraphicalrepresentationofthemodelbyfor- concept-level labelling of all relevant concepts expected to
mulatingaconceptlevelgraphasdiscussedabove,and beintheBayesianmodel,whichisgenerallyunavailable.
findactiveinferencetrailsinthemodel.Weworkwith WhileDeepNeuralNetworksprovideamuchmoreeffi-
radiologistsandophthalmologiststounderstandtheob- cient way to represent and learn from image data, they do
tained inference trails from a medical perspective and
not lend themselves to the simple conceptual analysis that
showthatmedicallyrelevantconcepttrailsareobtained
graphical models like Bayesian Networks do. We propose
which highlight the hierarchy of the decision-making
a method to repurpose a trained deep learning model into
processfollowedbythemodel.Ourframeworkisavail-
ableathttps://github.com/koriavinash1/ an equivalent graphical structure at the level of abstract,
BioExp. human-understandable concepts. This provides us with a
simple, transparent representation of the model’s logic and
Introduction allowsustodeterminethepathwayittakesformakingapre-
diction.Suchaconceptlevelrepresentationissimilartothat
Deeplearningmodelsareblackboxesandastheyareinte-
indeepprobabilisticmodels,wherethedepthofthegraphis
gratedintomedicaldiagnosis,itbecomesnecessarytogive
consideredoverconceptsinsteadofthedepthofthecompu-
aclearexplanationoftheconceptslearntbythemodelina
tationalgraph(Goodfellow,Bengio,andCourville2016).
formunderstandabletomedicalprofessionals(Holzingeret
We posit that such an abstraction is possible in a deep
al.2017a).Cliniciansalsopreferupfrontinformationabout
network since individual filters may be specialised to learn
theglobalpropertiesofamodel,suchasitsknownstrengths
individual concepts. In the context of representation learn-
andlimitations(Caietal.2019).
ing, it is hypothesized that deeper representation learning
For this, semantic concepts internal to the model and
algorithms tend to discover more disentangled representa-
their relationships need to be identified and represented
tions(Bengio2013).Forexample,experimentsinNetwork
in a human-understandable form. Previous interpretability
Dissectionshowthatindividualfilterslearndisentangledvi-
techniques are example based or attention based (Molnar
sualconcepts(Bauetal.2017).Thisbehaviourhasalsobeen
2020), such as attribution, saliency, or feature visualiza-
shown in the context of brain tumor segmentation models
tion, and do not reflect the ’concept-based thinking’ that
(Natekar, Kori, and Krishnamurthi 2020). Grouping filters
human-reasoning shows (Armstrong, Gleitman, and Gleit-
whichdetectthesameconceptwithinalayerwouldthenen-
man1983),neitherdotheyallowustouncoverthemodel’s
ableustobuildagraphicalrepresentationofsuchconcepts
understandingoftherelationshipbetweensuchconcepts.In
inherentinthenetwork.
therelatedworksectionwedetailwhereourmethodstands
Thisrepresentationofthemodelhasmanyadvantages.It
inrelationtocurrentworkinthisarea.
can tell us about the model’s biases - for example, if it re-
0202
voN
71
]VC.sc[
2v75460.8002:viXra
lies heavily on one concept for one class of predictions. It determine the concept they are specialized to detect. Such
also allows us to determine active inference trails inherent ananalysiscanbeperformedatanylevelofgranularity,for
inthemodel,aswehaveshowninthiswork.Ourmaincon- exampleonecouldperformtheanalysischoosing,say,only
tributions in this works are the following: (i) A method to thefirst,fifth,ninth,andeleventhlayersofadeepnetworkso
representaDeepNeuralNetworkasagraphicalmodelover thatahighlevelunderstandingcanbegainedoftheconcepts
abstract,highlevelconcepts,encouragingconcept-basedex- learntbytheselayers.
plainability, and, (ii) Identification of inference trails from LetthetrainednetworkbeΦ(W,X),andthelayerscho-
this graphical representation that help us understand the sen for analysis be {...,l − n,l,l + m,...}. The clusters
model’sdecision-makinglogic. {Cl,Cl,Cl,...}areformedasaresultofclusteringweights
p q r
at layer l in the network Φ. Let W = {w ,w ,...,w } be
1 2 n
ProposedFramework the set of weights in a layer, where W ∈ Rf×f×inc×outc
and w ∈ Rf×f×inc. Due to high dimensionality of the
This work aims to abstract the model into an equivalent i
weighttensor,wetakethemeanoftheweighttensoracross
graphicalmodelrepresentationwhereconceptslearntbythe
theoutcdimensiontoobtainarepresentativetensorwrep =
network become nodes, and edges depict relationships be- i
1 (cid:80) wc ∈ Rf×f. To amplify the difference between
tweenthem.Wetakeaclusteringbasedapproachtoidentify inc c i
symmetric weights we encode position information (Kori,
weightswhichmaybedetectingsimilarconceptsinthein-
Krishnamurthi, and Srinivasan 2018; Palop, Mucke, and
putimage.Suchamethodensuresthatourexplanationsare
Roberson2010)alongwithweights.
independent of the input sample and that our formulations
Clusters are formed using a hierarchical clustering
are computationally practical. Previous experiments show
method (Johnson 1967) using distance-based threshold-
thatforstate-of-the-artDNNstrainedonlarge-scaledatasets
ing. This provides additional degrees of freedom to group
like ImageNet (Deng et al. 2009), euclidian distance in the
weightsintoasmanynumbersofsignificantlydifferentcon-
activationspaceoffinallayersisaneffectiveperceptualsim-
cepts. After obtaining the clusters, for visual verification
ilaritymetric(Zhangetal.2018).Itisnotunreasonablethat
we view the flattened weight vector to observe similarity
suchbehaviourextendstodeeplearningmodelsinthemed-
amongtheclusteredweights.Sincedirectvisualinterpreta-
icaldomain.Weusetheeuclidiandistancebetweenweight
tionisinsufficient,toquantifytheeffectivenessofourclus-
vectorsaveragedacrossthechanneldimensionasoursimi-
teringmethodweuseE(SilhouetteScore)overallweights
laritymetric.
(Rousseeuw 1987) as a metric. Figure 11 in the Appendix
We posit that the weight clusters thus identified are re-
depictsthisforasamplelayer.
sponsiblefordetectingindividualconceptsintheinputim-
age,andthusformtheconceptnodesintheabstractedgraph-
ConceptIdentification
icalmodel.Wevisualizetheconceptdetectedbytheclusters
formedusingamodificationofGrad-CAM(Selvarajuetal. IntheConceptidentificationstep,wetrytoassociateformed
2017).Grad-CAMbasicallyvisualizesattentionofaweight weight clusters with some region in the input image which
layerontheinputimage.Byzeroingoutweightsfromother correspondstoahuman-understandableconcept.
clusters and only keeping weights from a particular clus- ConsiderclusterCl.Toidentifytheconceptlearntbythe
p
terbeforeobtainingGrad-CAMattentionmaps,wecanfind cluster and to depict this in a human understandable fash-
what the weight cluster corresponds to in the input space. ion, we first modify the trained network by dissecting the
Potentialactiveinferencetrailsarethenfoundfromthegen- networkatlayerl,theoutputsofwhicharedenotedbyΦ .
l
eratedgraphicalmodelusinganormalizedmutualinforma- Then,weperformavariationofGrad-CAM(whichwewill
tionbasedapproach. simply refer to as concept attention maps), using the filters
The proposed framework for understanding the deep intheclusterCl astheoutputsforwhichattentionistobe
p
learningmodelsconsistsofthefollowingsteps:(i)concept computed,asdescribedinequation3.
formation, (ii) concept identification, (iii) concept signifi- In practice, this is done as follows. The dissected net-
canceanalysis,(iv)graphformation,and(v)trailestimation. workΦ ismodifiedbyaddinga(1×1)convolutionatthe
l
Figure1and2provideadetailedoverviewofthedescribed end, the weights of which are set to one. We then set the
framework.Next,wegoovereachsectionofthisframework weightsofallfiltersinthelayerlwhichdonotbelongtothe
indetail. cluster p to zero. The effective operation performed by the
addedconvolutionallayerΦ isthenequivalenttotaking
l+1
ConceptFormation themeanacrossthechanneldimensionofonlythosefilters
We posit that groups of weight vectors in a layer are re- whichbelongtothecluster,providingasingle-channelcon-
sponsiblefordetectingaparticularconceptintheinputim- densation of the cluster which can be used for finding the
age. Weight clustering has been used before in the con- concept-attention map. We denote the output of this layer
text of network compression (Han, Mao, and Dally 2015; by E k∼idxp Φ l,k , where idx p are the set indices in a layer l
Son, Nah, and Mu Lee 2018). We show that a clustering belongingtoclusterCl,asformulatedinequation1.
p
based approach can be used to identify weights which are Concept identification then amounts to finding the con-
responsible for detecting a particular concept in the input ceptattentionmapsofthisoutputwithrespecttotheactiva-
image.Weightvectorscanbeclusteredusingasuitablemet- tions of the penultimate layer in the dissected network, i.e.
ric and their attention over the input image can be used to Φ asdescribedinequation2.
l−1
Figure 1: In the proposed framework, we construct a concept graph for a trained deep model. To generate concept graphs,
we cluster weights in user-defined layers of the network, use them as concepts, and later estimate links based on a mutual
informationbasedmetric.Forexample,trailsrepresentedinredandblueshowactiveconcept-levelinferencetrailsanetwork
usestopredictthefinalresult
Figure2:Theabovefiguredescribesallthestepsintheproposedconcept-basedinterpretabilityframeworkvisualizedinFigure
1
Consistency: To evaluate the consistency of clusters
yl(x)=
1 (cid:88)(cid:88)(cid:16)
E Φ (x)
(cid:17)
(1) generated by the proposed method, we examine their
p Z k∼idxp l,k regularity over multiple input samples in our datasets.
i j
Figure10illustratesthesame,whereeachrowcorresponds
βl (x)= 1 (cid:88)(cid:88) ∂y p l(x) (2) to the concept attention map for an identified cluster over
m,p Z ∂Φ (x) differentimagesintheinputdataset.Itcanbeobservedthat
l−1,m
i j identified clusters have similar concept attention maps for
(cid:32) (cid:33) multiple input samples, irrespective of tumor location or
CAMl =ReLU (cid:88) βl (x)Φ (x) (3) opticdisklocation.
p m,p l−1,m
m
Robustness: Here, we try to evaluate the robustness of
Where, m is the index of a filter in layer l−1 and k is
theformedclusters.Weightsbelongingtoaspecificlayerin
index of filter in layer l, β are the Grad-CAM importance
aneuralnetworkcanbeconsideredasi.i.d(Giryes,Sapiro,
weights, i,j are the indices for the height and width di-
and Bronstein 2016). We posit that after learning, all the
mensionsofthefeaturemapoftheadditionalconvolutional
weights belonging to a particular cluster come from an un-
layer, and CAM is the output concept-attention map for
derlying distribution and are i.i.d. We assume a gaussian
conceptpoflayerl.
generatingdistributionforweightsintheclusterandapprox-
imate this using the first and second order moment of the
Oncetheconceptsareidentified,weconductsignificance
weightsinthecluster.Figure4depictsthisgraphically.
tests to ensure that the concepts formed are consistent,
ConsideranidentifiedclusterCl ∈ Rf×f×inc×n,where
robust, and localized. These procedures are detailed next. p
f is the filter size, inc is the number of in-channels, and n
Figures 8, 9, and 10 show the results of the conducted
is the number of weights in the cluster. Let w ∈ Cl be a
consistencyandrobustnesstestsforouridentifiedconcepts, i p
which provide further evidence to support our hypothesis weight belonging to the cluster C p l. Then, w ∈ Rf×f×inc,
that groups of weight vectors in the model are responsible i.e.theclusterCl containsnweighttensorsw ofsizef ×
p i
fordetectingdifferentsemanticconcepts. f ×inc.Wegenerateagaussiandistributionforeachpixel
x atpositionj intheflattenedweightw ,
j i
Figure3:Aboveimagedescribestheprocessoflinkformation.Sub-figure(a)describeshowpre-interventionaldistributionis
formed, sub-figure (b) describes how post-interventional distribution is formed, (c) exibits the condition for the existence of
edge.
For the directed link between two concepts in layer
x∼N(µ,σ) (4) p and q, Cp → Cq, the pre-interventional distribution
i j
P(Φ (x | do(Cp = 0))) is the feature map distribution
j −i
obtained on zeroing out the weights belonging to all con-
µ=E i (x j ),σ =E i (x j −E i (x j )) (5) cepts other than Cp in layer p (i.e., do(Cp = 0), where
i −i
We then sample n number of weights as detailed above, the do operator indicates a manual intervention on the ar-
replace all n weights in the cluster Cl by the sampled gument to set it to a particular value, which is 0 in this
p
weights,andrecomputeourconceptattentionmaps.Figures case). This distribution tells us about information flowing
8and9showtheresultsofthisexperiment.
fromCptoallconceptsinthesucceedinglayerq.Similarly,
i
Weobservethatrecomputedconceptattentionmapscor- the post-interventional distribution Q(Φ j (x | do(C − p i =
respondtothesameregionintheinputspaceastheoriginal 0),do(Cq = 0))) is the feature map distribution obtained
−j
concept attention maps. We also generate recomputed con- at the layer q by zeroing out the weights belonging to all
cept attention maps using a uniform prior over the cluster the clusters other than i in layer p as well as the weights
weights as well as a gaussian prior taken over the range of belonging to all the clusters other than j in layer q (i.e.,
allweightsinthelayer,andcomparethiswiththeresultsof do(Cp = 0) and do(Cq = 0)). This distribution tells us
−i −j
using a gaussian prior over only the cluster weights. It can about the information flowing only from Cp to Cq. In this
i j
beobservedthatconceptattentionmaps(CAMs)formedby formulationthetermspreandpostinterventionalareconsid-
usinggaussianpriorsoveronlytheweightsbelongingtothat eredonlywithrespecttolayerq.Figure3showsthisprocess
particularclusterarevisuallysimilartotheoriginalsforeach graphically.
samplingrun,whileCAMsformedusinguniformpriorsor Based on our formulation, the directed link Cp → Cq,
CAMsformedusinggaussianpriorsoveralltheweightsdo i j
existsonlyifequation6issatisfied.
not encode the same concept in the input space and show
high variability for each sampling run. This behaviour is
seen consistently over all input samples. Thus, we empiri- NMI (cid:0)Q(cid:0) Φ (x|do(Cp =0),do(Cq =0)) (cid:1) ,
callyjustifythatouridentifiedconceptscomefromthesame j −i −j
(6)
underlyingdistribution,andthatthegaussianisareasonable P(cid:0) Φ (x|do(Cp =0)) (cid:1)(cid:1) >T
j −i
proxyforthisdistribution.
This basically states that the link exists only if the mu-
NetworkFormationandInformationFlow
tual information between pre and post interventional dis-
Onceconceptsandhavebeenidentifiedforthegivensetof tribution is higher than a set threshold. High mutual infor-
layers,wehavethemeanstoconstructourequivalentgraph- mation implies that a significant portion of the information
icalrepresentation. flowingfromtheconceptCp tolayerq occursthroughthat
i
Given these concepts, we can identify relationships be- specific link Cp → Cq. This results in the formation of a
i j
tween them to generate a human-understandable trace of conceptgraph,anexamplevisualizationofwhichisshown
inference which augments model predictions. In order to in Figure 4. Note that this graphical model is not intended
identify the relationship between two concepts, we com- tobecomplete,onlyrepresentative.Sinceourgraphcanbe
pute the normalized mutual information between the pre- constructedoveranysetoflayerschosenbytheuser,there
interventional and post-interventional feature map distribu- could be multiple inference trails that denote relationships
tion,asdescribedbelow. betweendifferentconcepts.
Figure4:Avisualdepictionoftheconstructedgraphicalrepresentationforthenetworkgiventhesetoflayerstoanalyse.Each
pixel in a concept can be imagined to be drawn from its own gaussian distribution, using the mean and variance of the pixel
over the cluster as parameters. Dotted arrows show the concept is sampled from its corresponding normal distribution. Dark
arrowsshowlinksbetweenconcepts.
Figure 5: Concepts obtained from various layers of a trained U-net model superposed over the MRI Flair channel. (a) C3:
0
doesn’t capture any input region, (b) C3: concave edges, (c) C3: linear edges, (d) C5: interior key points. (e) C13: Lateral
1 2 2 0
lefthemisphericalbrainboundary,(f)C13:Laterallefthemisphericalandtumorcorebrainboundary,(g)C15:Anteriortumor
3 2
boundary, (h) C15: Tumor core boundary, (i) C19: Whole tumor boundary, (j) C17: Lateral brain boundary and tumor core
3 2 0
boundary,(k)C21:Diffusedtumorcoreregion,(l)C21:Tumorcoreregion.
1 2
TrailEstimation details the application of the above framework on bench-
markbiomedicalimagedatasets.
Givenourgraphicalrepresentationandtheexistenceoflinks
betweenconcepts,wenowhaveamethodtotrackinference
Experiments
steps taken by the model. The obtained concept graph is a
DAGwithdepthm,wheremisnumberoflayersspecified We illustrate the working of our proposed framework on
by the user for interpretability. The trails are all the paths both classification and segmentation tasks. For the classifi-
runningfrominputtoaparticularnodeusedinaninference. cation task, we considered the Diabetic Retinopathy prob-
The obtained trails encode the flow of concept level infor- lem, and for segmentation, we considered the Brain Tu-
mationusedinmakingaprediction. mor Segmentation problem. In both the experiments, the
Forexample,considerthesampletrailX →C →C → aim was to explain the building blocks of the model,
1 4
C → Y inFigure4.Medicalprofessionalscanthenhigh- and understand the hierarchy of decision making in deep
8
lightwhetherornotsuchaninferencetrailmakessensefrom learning models. All the experiments and results can
abiomedicalperspective,andunderstandthemodel’sbiases be reproduced by using notebooks provided in the code
anditscommonlogicalstepsofinference.Thenextsection repository https://github.com/koriavinash1/
BioExp_Experiments. ity of diabetic retinopathy and have the potential to greatly
simplifydiagnosisanddetection.Weimplementourframe-
BrainTumorSegmentation work on a ResNet50 based network which achieves a Co-
Inthepastdecade,therehasbeensignificantdevelopmentof hen Kappa Score of 0.71 on the validation set of the AP-
image processing algorithms for segmenting intra-tumoral TOS dataset (Society 2019). The APTOS dataset contains
structures in brain MRI images (Bakas et al. 2018). Deep around5500retinaimagestakenusingfundusphotography.
Learninghasshowngreatpotentialinthiscontext,withthe Theseverityofdiabeticretinopathyhasbeenratedforeach
BraTSchallenge(Kamnitsasetal.2017;Wangetal.2017; imageonascaleof0(noDR)to4(ProliferativeDR).Each
Myronenko 2018; Kori et al. 2018) setting the benchmark stageofDRischaracterizedbycertainfeatures-suchasmi-
forresearchinthisarea.TheBraTSdatasetcontainsnearly croanuerisms,exudates,andhemorrhages.Thus,itbecomes
300brainMRIvolumesannotatedbyexpertsfortumorre- necessarytoseewhetherdeeplearningmodelsprocessand
gions. Various deep learning algorithms have shown great identify these features, and to see the model’s understand-
performance in segmenting tumor core, enhancing tumor, ingofrelationshipsbetweentheseandthepredictedseverity
andedemaregionsfromtheseMRIvolumes. of DR. We follow a similar process as that for brain tumor
WeimplementouralgorithmonaUNetbasedmodelfor segmentation,detailedbelow.
braintumorsegmentation,whichisapopularsegmentation
Concepts The E(SilhouetteScore) over all the data-
architecture in the medical context (Ronneberger, Fischer,
points is 0.2, which again indicates the formation of weak
andBrox2015).Ourmodelalsohasresidualconnectionsas
butsignificantclusters.Figure12describestheidentifiedlo-
per (Kermi, Mahmoudi, and Khadir 2018), and achieves a
calandgloballevelconcepts,encodingbloodvessels,hard
dice score of 0.788, 0.743, and 0.649 on whole tumor, tu-
andsoftexudates,dot-blothemorrhages,etc.
morcoreandenhancingtumorsegmentationrespectivelyon
a held-out validation set of 48 volumes. Our model is not Trails and Discoveries Similar to the trails obtained for
meant to achieve state of the art performance. Instead, we the BraTS dataset, we show example inference trails ob-
aim to demonstrate our method on a commonly used ar- tainedfortheAPTOSdatasetinFigure7andFigures13and
chitecture for brain-tumor segmentation. The next sections 14intheAppendix.Thesedescribevisualtrailsinvolvedin
detail the concepts and active inference trails obtained as a predicting ’Severe’, ’Moderate’, and ’Proliferative’ classes
resultofourframeworkonthistask. of diabetic retinopathy respectively. An ophthalmologist’s
Concepts The E(SilhouetteScore) over all the data- feedbackwasobtainedontheconcepttrails,whichiselabo-
pointsis0.241,indicatingtheformationofweakbutsignif- ratedinthediscussionsection.Onceagain,weseetheemer-
icantclusters.Figure5describesthevariousconceptsiden- genceofmedicallyrelevantconceptsinahierarchicalman-
tified from our model. Initial layers (convolutional layers 3 ner, which may provide additional support to medical pro-
and 5) correspond to edges in a specific direction or brain fessionalsapartfromjusttheoutputclassification.
boundaries.Inhigherlayers,filtersstartcapturingmorelo-
calinformation.Itcanbeobservedthatsomeconceptscap- RelatedWork
ture brain boundary, while some capture tumor boundary.
Figure 5 contains a description of the various concepts ob- Explainability is generally categorized into post-hoc and
tained from out network. This behaviour is in line with the ante-hoc methods, where post-hoc explainability methods
understanding that filters in shallower layers of brain tu- try to analyze and make inferences on trained models (Si-
morsegmentationmodelslearnsimplepatternswhiledeeper monyan, Vedaldi, and Zisserman 2013; Zeiler and Fergus
layerslearnprogressivelymorecomplexconcepts(Natekar, 2014; Ustun and Rudin 2014). In contrast, ante-hoc meth-
Kori,andKrishnamurthi2020).Thebrainatlasdescribedin ods try to build an explainable model while training itself
(Dingetal.2016)wasusedtoformulateappropriatedescrip- (Caruanaetal.2015;Holzingeretal.2017b;).
tions. Current research directions in post-hoc interpretability
focus mainly on visualizing network attributions or illus-
TrailsandDiscoveries Figure6describesinferencetrails trative samples in the input space (Selvaraju et al. 2017;
involvedinpredictingtheenhancingtumorregion(Trailsfor Bau et al. 2017; Olah, Mordvintsev, and Schubert 2017;
other classes are available in the Appendix ). These show Kimetal.2018).Ourworkisrelatedtomethodsinvolving
themodel’sattentionisinitiallyontheouteredgesandkey- disentangledlatentrepresentationsandconceptbasedexpla-
pointsofthebrain,thenmovestothewhiteandgreymatter nations.Forexample,previousexperimentsonnetworkdis-
region,thenthetumorboundary,andfinallytheinternaltu- section show that deep networks learn disentangled latent
morregion.ThecaptionofFigure6alsoprovidesadescrip- concepts (Bau et al. 2017). Previous concept based inter-
tionofthevisualtrailsforanimagebasedonthepredefined pretabilitymethods(Ghorbanietal.2019;Kimetal.2018)
concept description. In the discussion section, we analyse use input patches to identify salient concepts that lead to a
thesetrailswithfeedbackfromacertifiedradiologist. particularoutput.Thishasbeenextendedtoincludeacom-
pletenessmeasureforidentifiedconcepts(Yehetal.2019).
DiabeticRetinopathyclassification
However,neitherofthesemethodsconsidertherelationship
Diabetic Retinopathy (DR) is frequent in individuals suf- betweenconceptslearntbythemodelanddonotprovidea
fering from diabetes (Fong et al. 2004). Deep Learning al- trace of inference steps. Also, these methods either require
gorithms have shown great promise in detecting the sever- a pre-processed set of input samples as concepts (Kim et
Figure6:Activeinferencetrailforenhancingtumor(Eachrowisatrailforoneinputsample,redregionsarehighattention):
(I:Inputimagetoanetwork)−>(C :Concaveedges)−>(C :Whitematterregion)−>(C :Tumorboundary)−>C :(Lateralbrain
1 2 3 4
boundary)−>(C :Inferiortumorboundary)−>(EnhancingTumor)
5
Figure7:ActiveinferencetrailforsevereDR(greenregionsarehighattention):(I:InputImage)− >(C :OpticCup/Hard
1
exudates)− >(C :HardExudates)− >(C :Bloodvessels,softexudates)− >(C :Bloodvessel,softexudates)− >(C :
2 3 4 5
dot-blotHemorrhages/laserscarmarksofretinalphotocoagulation)
al. 2018), or automatically segment the input image at var- Discussion
ious resolutions to create concepts (Ghorbani et al. 2019).
Thisworkaimstoprovideconcept-basedinterpretabilityfor
However,inthemedicaldomain,obtainingsuchconceptsis
deepneuralnetworks,demonstratingtheresultsonmedical
difficult - manual concept curation is time consuming and
data. We use a clustering technique to extract a graphical
would require medical experts, while segmenting the input
representationofconceptsinthenetwork,andvisualizethe
image may not lead to the formation of coherent anatomi-
clusteredconceptsusingavariationofGrad-CAM.Wethen
cal concepts which add interpretability value, especially in
useaninformation-theoreticmeasuretodeterminerelation-
cases where the task itself is image segmentation. In such
ships between concepts and build concept level inference
domains, interpretability needs to emerge organically from
trails within our network. Our results show that consistent,
themodelitselfandprovideanunderstandingofthemodel’s
distincttrailsthatleadtoaparticularclassificationmadeup
decisionmakinglogic.
ofanatomicallyrelevantconceptscanbeidentified.
While in previous work on interpretability in the med-
ical domain (Natekar, Kori, and Krishnamurthi 2020), the
Our work introduces a post-hoc interpretability method, existenceofdisentangledconceptsisshowninbrain-tumor
by abstracting the trained model into interpretable concept segmentation networks, in this work we create a concept-
graphs, where concepts and their relationships emerge im- levelgraphthatdepictstherelationshipsbetweenthesecon-
plicitlyfromthemodel,doingawaywiththeneedforuser- ceptsandprovidesanunderstandingofinferencetrailsinthe
curated input concepts. Our concept graphs allow easy vi- model. As opposed to previous concept-based approaches
sualization of the model’s logic on an abstract, human- (Ghorbani et al. 2019; Kim et al. 2018), no manual extrac-
understandablelevel. tion of concepts from the input dataset is required, which
is a challenging task in the medical domain. In this initial medical practitioners for human-ai collaborative decision-
work,wedemonstratethepotentialofourtechniqueontwo making. Proceedings of the ACM on Human-computer In-
medical datasets - the BraTS dataset for brain tumor seg- teraction3(CSCW):1–24.
mentation and the APTOS dataset for diabetic retinopathy Caruana, R.; Lou, Y.; Gehrke, J.; Koch, P.; Sturm, M.; and
classification. Elhadad, N. 2015. Intelligible models for healthcare: Pre-
For brain-tumor segmentation, a certified radiologist’s dictingpneumoniariskandhospital30-dayreadmission. In
commentsontheextractedconcepttrailwassolicited.They Proceedingsofthe21thACMSIGKDDinternationalconfer-
notedthelateraltomedialandanteriortosuperiornatureof enceonknowledgediscoveryanddatamining,1721–1730.
attentionofthemodel,aswellasthehierarchicalapproach
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
tosegmentationwhichisinlinewitharadiologist’sthought
Fei,L. 2009. ImageNet:ALarge-ScaleHierarchicalImage
process.Theycommentedthattumourboundarydelineation
Database. InCVPR09.
asseeninFigure6conceptC hasvalueforneurosurgeons
3 Ding,S.-L.;Royall,J.J.;Sunkin,S.M.;Ng,L.;Facer,B.A.;
when obtaining biopsy or resecting the tumour since this
Lesnar, P.; Guillozet-Bongaarts, A.; McMurray, B.; Szafer,
helps prevent damage to unaffected brain tissue. They also
A.; Dolbeare, T. A.; et al. 2016. Comprehensive cellular-
notedthataneuroradiologistwouldbeabletoimmediately
resolution atlasof the adulthuman brain. Journal ofCom-
perceivethepresenceofgliomasintheflairsequenceandit
parativeNeurology524(16):3127–3481.
is in general not possible to break down that perception in
terms of the trails obtained from the concept graphs. How- Fong, D. S.; Aiello, L.; Gardner, T. W.; King, G. L.;
ever, the visualization of concepts that are focused on tu- Blankenship,G.;Cavallerano,J.D.;Ferris,F.L.;andKlein,
mourboundariesandthetumourcorewouldhelpinimprov- R. 2004. Retinopathy in diabetes. Diabetes care 27(suppl
ingconfidenceandtrustinthedeeplearningmodel.Thetu- 1):s84–s87.
morcoreandcharacteristicsarealsodefinedwhichwillaid Ghorbani,A.;Wexler,J.;Zou,J.Y.;andKim,B. 2019. To-
indiagnosisandgradingofthetumor. wards automatic concept-based explanations. In Advances
ForDiabeticRetinopathy,anophthalmologist’sfeedback inNeuralInformationProcessingSystems,9277–9286.
wasobtainedontheoutputtraildescribedinFigure7.Vari- Giryes, R.; Sapiro, G.; and Bronstein, A. M. 2016. Deep
ousfeatures,suchashardandsoftexudates,dot-blothaem- neuralnetworkswithrandomgaussianweights:Auniversal
orrhages, optic cup, and laser scar marks of retinal photo- classification strategy? IEEE Transactions on Signal Pro-
coagulationwereidentified.InthecaseofDR,itisinterest- cessing64(13):3444–3457.
ingthatfeatureslikethis,whichophthalmologistslookatto
Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. Deep
classifyDRimages,emergeimplicitlyfromthemodel,even
learning. MITpress.
thoughithasnotbeenexplicitlytrainedtolearnthese.
Han, S.; Mao, H.; and Dally, W. J. 2015. Deep com-
Acknowledgments. We would like to acknowledge help pression: Compressing deep neural networks with pruning,
fromDr.RavikanthBalajiandDr.DevikaJoshiforprovid- trained quantization and huffman coding. arXiv preprint
ingclinician(radiologicalandopthalmological)feedbackon arXiv:1510.00149.
theinferencetrailsobtained. Holzinger, A.; Plass, M.; Kickmeier-Rust, M.; Holzinger,
K.; Cris¸an, G. C.; Pintea, C.-M.; and Palade, V. Interac-
References tivemachinelearning:experimentalevidenceforthehuman
in the algorithmic loop. Applied Intelligence 49(7):2401–
Armstrong,S.L.;Gleitman,L.R.;andGleitman,H. 1983. 2414.
What some concepts might not be. Cognition 13(3):263–
Holzinger,A.;Biemann,C.;Pattichis,C.S.;andKell,D.B.
308.
2017a. Whatdoweneedtobuildexplainableaisystemsfor
Bakas, S.; Reyes, M.; Jakab, A.; Bauer, S.; Rempfler, M.; themedicaldomain? arXivpreprintarXiv:1712.09923.
Crimi,A.;Shinohara,R.T.;Berger,C.;Ha,S.M.;Rozycki,
Holzinger, A.; Plass, M.; Holzinger, K.; Crisan, G. C.;
M.; et al. 2018. Identifying the best machine learning al-
Pintea, C.-M.; and Palade, V. 2017b. A glass-box in-
gorithmsforbraintumorsegmentation,progressionassess-
teractive machine learning approach for solving np-hard
ment,andoverallsurvivalpredictioninthebratschallenge.
problems with the human-in-the-loop. arXiv preprint
arXivpreprintarXiv:1811.02629.
arXiv:1708.01104.
Bau, D.; Zhou, B.; Khosla, A.; Oliva, A.; and Torralba, A. Johnson,S.C. 1967. Hierarchicalclusteringschemes. Psy-
2017. Network dissection: Quantifying interpretability of chometrika32(3):241–254.
deepvisualrepresentations.InProceedingsoftheIEEEcon-
Kamnitsas,K.;Ledig,C.;Newcombe,V.F.;Simpson,J.P.;
ference on computer vision and pattern recognition, 6541–
Kane, A. D.; Menon, D. K.; Rueckert, D.; and Glocker, B.
6549.
2017. Efficientmulti-scale3dcnnwithfullyconnectedcrf
Bengio, Y. 2013. Deep learning of representations: Look- foraccuratebrainlesionsegmentation.Medicalimageanal-
ingforward. InInternationalConferenceonStatisticalLan- ysis36:61–78.
guageandSpeechProcessing,1–37. Springer.
Kermi, A.; Mahmoudi, I.; and Khadir, M. T. 2018.
Cai, C. J.; Winter, S.; Steiner, D.; Wilcox, L.; and Terry, Deep convolutional neural networks using u-net for au-
M. 2019. ”helloai”:Uncoveringtheonboardingneedsof tomatic brain tumor segmentation in multimodal mri vol-
umes. InInternationalMICCAIBrainlesionWorkshop,37– Ustun, B., and Rudin, C. 2014. Methods and mod-
48. Springer. els for interpretable linear classification. arXiv preprint
arXiv:1405.4047.
Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C.; Wexler, J.;
Viegas, F.; et al. 2018. Interpretability beyond feature at- Wang, G.; Li, W.; Ourselin, S.; and Vercauteren, T.
tribution: Quantitative testing with concept activation vec- 2017. Automaticbraintumorsegmentationusingcascaded
tors(tcav).InInternationalconferenceonmachinelearning, anisotropicconvolutionalneuralnetworks. InInternational
2668–2677. MICCAIbrainlesionworkshop,178–190. Springer.
Kori, A.; Soni, M.; Pranjal, B.; Khened, M.; Alex, V.; and Yeh, C.-K.; Kim, B.; Arik, S. O.; Li, C.-L.; Ravikumar, P.;
Krishnamurthi, G. 2018. Ensemble of fully convolutional andPfister,T. 2019. Onconcept-basedexplanationsindeep
neuralnetworkforbraintumorsegmentationfrommagnetic neuralnetworks. arXivpreprintarXiv:1910.07969.
resonance images. In International MICCAI Brainlesion Zeiler,M.D.,andFergus,R. 2014. Visualizingandunder-
Workshop,485–496. Springer. standing convolutional networks. In European conference
Kori,A.;Krishnamurthi,G.;andSrinivasan,B. 2018. En- oncomputervision,818–833. Springer.
hanced image classification with data augmentation using Zhang,R.;Isola,P.;Efros,A.A.;Shechtman,E.;andWang,
positioncoordinates. arXivpreprintarXiv:1802.02183. O. 2018. Theunreasonableeffectivenessofdeepfeaturesas
Molnar, C. 2020. Interpretable Machine Learning. Lulu. aperceptualmetric. InProceedingsoftheIEEEconference
com. oncomputervisionandpatternrecognition,586–595.
Myronenko, A. 2018. 3d mri brain tumor segmentation
usingautoencoderregularization. InInternationalMICCAI
BrainlesionWorkshop,311–320. Springer.
Natekar,P.;Kori,A.;andKrishnamurthi,G. 2020. Demys-
tifying brain tumor segmentation networks: Interpretability
and uncertainty analysis. Frontiers in Computational Neu-
roscience14:6.
Olah, C.; Mordvintsev, A.; and Schubert, L. 2017. Fea-
ture visualization. Distill. https://distill.pub/2017/feature-
visualization.
Palop,J.J.;Mucke,L.;andRoberson,E.D. 2010. Quanti-
fyingbiomarkersofcognitivedysfunctionandneuronalnet-
workhyperexcitabilityinmousemodelsofalzheimer’sdis-
ease:depletionofcalcium-dependentproteinsandinhibitory
hippocampalremodeling. InAlzheimer’sDiseaseandFron-
totemporalDementia.Springer. 245–262.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:
Convolutionalnetworksforbiomedicalimagesegmentation.
In International Conference on Medical image computing
andcomputer-assistedintervention,234–241. Springer.
Rousseeuw, P. J. 1987. Silhouettes: a graphical aid to the
interpretationandvalidationofclusteranalysis. Journalof
computationalandappliedmathematics20:53–65.
Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;
Parikh,D.;andBatra,D. 2017. Grad-cam:Visualexplana-
tionsfromdeepnetworksviagradient-basedlocalization. In
Proceedings of the IEEE international conference on com-
putervision,618–626.
Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2013.
Deep inside convolutional networks: Visualising image
classification models and saliency maps. arXiv preprint
arXiv:1312.6034.
Society,A.P.T.-O. 2019. Asiapacifictele-ophthalmology
society2019,dataset.
Son,S.;Nah,S.;andMuLee,K. 2018. Clusteringconvo-
lutionalkernelstocompressdeepneuralnetworks. InPro-
ceedings of the European Conference on Computer Vision
(ECCV),216–232.
AppendixI
Here we show additional figures and examples which result from our primary analysis above. First, the results for cluster
significancetestsareshown-robustnessandconsistency.Thenweshowadditionalexamplesforbrain-tumorsegmentationand
diabeticretinopathyclassification,aswellasothersupportingimages.
(a)Layer:19,GaussianPrioroverentireweightlayer
(b)Layer:19,UniformPrioroveronlytheweightcluster
(c)Concept:C19,GaussianPrioroveronlytheweightcluster
2
Figure 8: This figure illustrates results of robustness experiments on BraTs data, (a) Concept attention maps by assuming
Gaussiandistributionoveralltheweightsinalayer,(b)ConceptattentionmapsbyassumingUniformdistributionoveronly
theclusterweights,and(c)ConceptattentionmapsbyassumingGaussiandistributionoveronlytheclusterweights.Notethat
usingagaussianprioroveronlytheclustergivesmostconsistentconceptattentionmaps.
(a)Layer:3dGaussianPrioroverentireweightlayer
(b)Layer:3dUniformPrioroveronlytheclusterweights
(c)Concept:C3dGaussianPrioroveronlytheclusterweights
4
Figure 9: This figure illustrates results of robustness experiments on APTOS data, (a) Concept attention maps by assuming
Gaussiandistributionoveralltheweightsinalayer,(b)ConceptattentionmapsbyassumingUniformdistributionoveronly
theclusterweights,and(c)ConceptattentionmapsbyassumingGaussiandistributionoveronlytheclusterweights.Notethat
usingagaussianprioroveronlytheclustergivesmostconsistentconceptattentionmaps.
(a)BraTSConcept:C21TumorCoreregion
2
(b)BraTSConcept:C19WholeTumorboundary
2
(c)APTOSConcept:C2aLateralEyeboundary
2
(d)APTOSConcept:C3dMajorBloodvessels
4
Figure10:Theabovefigureshowstheconsistencyofconceptformation;eachrowindicatesshowstheconcept-attentionmap
foraclusterfordifferentinputsamples
Figure 11: Above image describes the effectiveness of clustering. Sub-figure (a) describes the initial layer weights from
ResNet50trainedonAPTOS(Society2019)data,inthefiguredarkbluehorizontalbandsseperatestheweightsamongmultiple
clusters(providedfigurehas3clusters).Sub-figure(b)quantifiestheeffectivenessofclustersobtainedastheresultofproposed
methodusingasilhouetteplot
Figure12:ThisfigureillustratestheconceptsobtainedfromvariouslayersofatrainedResNet50model.Basedontheregion
ofactivationweprovidedescriptionoftheconceptsasfollows:(a)C1:doesn’tcaptureanyinputregion,(b)C1:Rightlateral
1 2
edges,(c)C2a:Lateraledges,(d)C2a:Opticdisk+lateraledges,(e)C2c:Opticdisk+bloodvessels,(f)C3a:Allbloodvessels
1 2 2 2
(tiny),(g)C3d:Majorbloodvessels,(h)C3d:Bloodvessels(eroded),(i)C4a:Yellowspots(maybehardexodates),(j)C4f:
4 5 2 1
Yellowspots(maybehardexodates),(k)C4a:PaleYellow(maybehardexodates),(l)C5c:Hard/Softexodates
3 2
Figure13:ActiveinferencetrailforModerateDR(Greenregionsarehighattention):(I:InputImagetoanetwork)− >(C :
1
Soft exudates + Optic Cup) − > (C : Hard exudates) − > (C : All blood vessels) − > (C : Optic disk and blood vessels)
2 3 4
−>(C :InvertedBloodvessel(eroded)Image)−>(C :Darkspots)
5 6
Figure14:ActiveinferencetrailforProliferativeDR(Greenregionsarehighattention):(I:InputImagetoanetwork)−>(C :
1
Pale areas, due to attenuated artery endings + macula) − > (C : Hard exudates) − > (C : All blood vessels + key points)
2 3
− > (C : Optic disk and blood vessels) − > (C : Laser scar marks of retinal photocoagulation + blot haemorrhages − >
4 5
(C :Darkspots)
6
Figure15:ActiveinferencetrailforEdema(Eachrowisatrailforoneinputsample,redregionsarehighattention):(I:Input
Imagetoanetwork)− >(C :Concaveedges)− >(C :Whitematter)− >(C :Brainandtumorboundary)− >C :(Lateralbrain
1 2 3 4
boundary)−>(C :Lateraltumorboundaryandmidbrain)−>(Edemaregion)
5
Figure16:ActiveinferencetrailforTumorCore(Eachrowisatrailforoneinputsample,redregionsarehighattention):(I:
InputImagetoanetwork)−>(C :Concaveedges)−>(C :Whitematter)−>(C :Brainandtumorboundary)−>C :TumorCore)
1 2 3 4