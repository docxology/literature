Is Active Persona Inference Necessary for Aligning Small Models to
Personal Preferences?
Zilu Tang1, Afra Feyza Akyürek1, Ekin Akyürek2, Derry Wijaya1,3,
1Boston University, 2MIT, 3Monash University Indonesia,
zilutang@bu.edu
Abstract
A prominent issue in aligning language mod-
els (LMs) to personalized preferences is
underspecification– the lack of information
from users about their preferences. A popular
trend of injecting such specification is adding
a prefix (e.g. prior relevant conversations) to
the current user’s conversation to steer pref-
erence distribution. Most methods passively
model personal preferences with prior example
preferences pairs. We ask whether models ben-
efit from actively inferring preference descrip-
tions, and address this question by creating a
synthetic personalized alignment dataset based
on famous people with known public prefer-
ences. We then test how effective finetuned
1-8B size models 1 are at inferring and align-
ing to personal preferences. Results show that
higher-quality active prefixes lead to better gen-
eralization, more contextually faithful models,
and less systematic biases across different pro-
tected attributes. All our results suggest active
alignment can lead to a more controllable and
efficient path for personalized alignment.2
1 Introduction
Preference alignment has become a standard
pipeline in finetuning models to followgeneric
human preferences. Most work seeks to opti-
mize models to produce responses that would be
preferableon average, simplifying the diverse and
oftencontradictingspace of human preferences.
The focus for personalized alignment emerges
as the demand for adapting models to individual
user preferences rises with industrial applications
and fairness concerns for large pretrained mod-
els. One major issue when personalizing generic
1We find larger models quite good at personalization with
prompting, hence only leveraged it for dataset generation.
2We release our research artifacts in https://github.
com/PootieT/famous-persona
Active Alignment: Infer preferences & prefix
What’s a fun dessert recipe?
Chocolate avocado mousse 
with almond milk is a …
How about vanilla pound 
cake?
Creamy Finnish salmon soup 
always hits the spot …
User might be dairy-free
I am sick and want a soup 
recipe
What does the user prefer?
What’s a fun dessert recipe?
Chocolate avocado mousse 
with almond milk is a …
How about vanilla pound 
cake?
This is it: Soup-er Healing 
Vegan Ginger Soup. Boil …
I am sick and want a soup 
recipe?
Passive Alignment: prefix few-shots
Generalization:            ✅
   Interpretable: ❌
 
Contextually-Faithful: ❌
   No Bias:        ❌
Generalization:            ✅
   Interpretable: ✅
 
Contextually-Faithful: ✅
   No Bias:        ✅
Figure 1: We construct a personalized alignment dataset
on famous people to investigate whetheractively infer-
ring preferences is necessary for finetuning personalized
alignment models. We find active alignment to be more
interpretable, contextually faithful, and less biased.
alignment algorithms is mitigating underspecifi-
cation. User-specific information needs to be in-
corporated to customize the reward distribution
downstream. The majority of prior works proposes
passive alignment– learning to influence reward
through observing similar prior interactions. This
can either be incorporated through few-shot exam-
ples in the prompt (Wang et al., 2024b; Zollo et al.,
2024), prefix embeddings (Li et al., 2024b; Poddar
et al., 2024), meta-learning(Zhao et al., 2023; Yang
et al., 2024), or preference prototypes (Wang et al.,
2024b; Park et al., 2024a). While passive alignment
allows fine-grained steering that benefits from the
scale of prior interactions, active alignment meth-
ods seek to directly guide personalization with in-
structions. Most work with this approach follows
Multi-objective Reinforcement Learning (MORL)
paradigm (Liu et al., 2014), recognizing that align-
ment objectives often involve competing goals (e.g.
helpful vs. harmless) with a limited number of ob-
jectives (typically less than five) (Jang et al., 2023).
However, MORL-based works have yet to show
arXiv:2505.13257v2  [cs.CL]  29 Sep 2025
whether active alignment can fully leverage the ex-
pressiveness of natural language instructions for
fine-grained preference steering. With this gap in-
mind, we synthetically generate a dataset of famous
people with publicly known preferences, and com-
pare passive vs. active alignment. We summarize
our contribution as follows:
Dataset of Personal PreferenceWe release a
personalized alignment dataset based on real peo-
ple with diverse and contradicting preferences.
Active vs. PassiveWe compare active and pas-
sive alignment strategies across four models of size
1-8B, and show that active alignment can improve
reward generalization on unseen personas.
Contextual FaithfulnessWe analyze the mod-
els’ attribution pattern to prefixes and find active
aligned models more contextually faithful. This
improves with the quality of inferred personas.
Systematic BiasWe find systematic biases in
persona inference and alignment and that active
alignment results in less bias.
2 Background & Related works
Personalized alignment datasets.Personaliza-
tion has been extensively studied in many fields
prior to LLMs (Chen et al., 2023), beginning with
collaborative filtering in recommendation systems
(Goldberg et al., 1992). With popularization of
post-training preference alignment to human feed-
back (Ouyang et al., 2022), initial personalized
alignment datasets take inspiration from MORL-
paradigm (Bai et al., 2022; Ji et al., 2024; Jang
et al., 2023; Yang et al., 2024; Gao et al., 2024b;
Poddar et al., 2024; Chakraborty et al., 2024). Con-
structing such datasets is relatively straightforward.
Simple objectives (e.g. detailed vs. concise re-
sponses) can be controlled in generation through
prompting and evaluated with LLMs (Jang et al.,
2023). The biggest assumption of MORL is that ob-
jectives are compositional, and the span covers the
entire preference space. This assumption is flawed,
however, as human preferences can be infinitely
nuanced (e.g. liking squash over tennis) so that no
amount of objectives can cover the space of per-
sonal preferences (Slovic, 1995; MacIntyre, 2013;
Aroyo and Welty, 2015; Gabriel, 2020; Klingefjord
et al., 2024). Even if preference space is composi-
tional, modeling challenges remain. (Wang et al.,
2024a; Beck et al., 2024).
Another popular choice is predicting human sur-
vey responses (Durmus et al., 2023; Santurkar et al.,
2023; Zhao et al., 2023; Do et al., 2023; Feng et al.,
2024; Li et al., 2024a; Hwang et al., 2023; Jiang
et al., 2024). Although measuring opinions can
serve as a valuable evaluation tool, these tasks in
general are not for improving conversational assis-
tants. Recent work (Zollo et al., 2024) synthetically
construct users preferences through linear combi-
nations of off-the-shelf reward models. Kirk et al.
(2024) collects response preference pairs from di-
verse user backgrounds, and Castricato et al. (2025)
synthetically constructs personas and respective
conversations using prompts from PRISM. How-
ever, none of these datasets contain ground-truth
persona preferences from which we can evaluate
preference inference (i.e. active alignment). See
dataset comparisons in Appendix 10.
Alignment methods.For MORL-based active
alignment, methods usually involve merging sepa-
rately trained adapters, or programmatically com-
posed prompt prefixes (Jang et al., 2023; Wang
et al., 2024c). Other works focus on pluralistic
alignment from group perspectives (Sorensen et al.,
2024; Park et al., 2024a), which typically use meta-
learning (Zhao et al., 2023), or EM-like algorithms
to iteratively cluster and align multiple models
(Zhong et al., 2024; Park et al., 2024b). Lastly,
many seek to align during decoding (Chen et al.,
2024b; Khanov et al., 2024; Shi et al., 2024; Gao
et al., 2024b; Huang et al., 2024). Many such works
are orthogonal to us, where we focus on the most
simple set-up.
Active preference inference and underspecifi-
cationInferring human preferences from sparse
examples or underspecified instructions is impor-
tant for seamless human-AI interaction (Milli et al.,
2017). Prior works infer different aspects of hu-
man preferences, such as implicit social contracts
(Fränken et al., 2023), constitutions (Chen et al.,
2024c), and user values (Sun et al., 2024; Liu et al.,
2024; Balepur et al., 2025; Li et al., 2025; Bismay
et al., 2025). These works reinforce our point that
explicitly inferring user preference is crucial for
interpretable alignment. Prefixing inferred persona
can also be considered as addressing underspecifi-
cation (Lee et al., 2022), which leads to spurious
correlation and short-cut learning (Geirhos et al.,
2020). In preference learning, underspecified data
– such as users upvoting Reddit posts for various
latent reasons (Ethayarajh et al., 2022; Park et al.,
2024a) – leads to non-robust rewards. A solution
is to fully specify the preference criteria (Siththa-
ranjan et al., 2023; Yang et al., 2024), which in our
case, is the inferred personas.
3 Methodology
3.1 Task Definition
Preference alignment to human feedback (Stiennon
et al., 2020; Bai et al., 2022; Ouyang et al., 2022)
assumes a dataset of triplesD={x,y w,y l} where
x represents the prompt given to the LM and yw,
yl represent the preferred and respectively dispre-
ferred response labeled by the human annotator(s).
The task of alignment seeks to optimize a model’s
likelihood (π) of generating yw over yl given x. In
personalized alignment, we introduce the persona
variable (e.g. prior conversation, demographics) pi
for each of the n personas. The objective can be
defined as:
arg max
πp
Ex,yl,yw∈D
 X
i∈[n]
πp(yw|x, pi)

(1)
where πp could be a single or a set of personalized
models andD=∪ n
i=1Di.
3.2 Dataset Construction
We construct our personalization dataset to contain
diverse personas with contradicting preferences in
four steps (Figure 5).Step 1: Select persona.
With the help of GPT4 3, we define 11 axes (top-
ics or attributes) through which preferences might
differ (e.g. diet, politics) to ensure contrast
in opinions. For each axis, we prompt GPT4 to
provide at most five sub-categories (e.g. liberal)
along with a famous person associated with the
category (e.g. Bernie Sanders). We curate 50 di-
verse personas, each with definable contrasts.Step
2: Generate prompts.We generate two sets of
questions (x) – personal ( xpersonal) and divergent
(xdivergent) – for each persona to ensure diversity
and contrast. xpersonal are based on individualistic
preferences, and xdivergent are shared across per-
sonas from the same axis who prefers different an-
swers4. We sample 100 xpersonal and 100 xdivergent,
using half for training and the other half for test-
ing.Step 3: Sample Responses.We generate y
from x using our baseline model ZEPHYR 5 for the
purpose of on-policy improvement (Meng et al.,
3We usegpt-4-0613from OpenAI
4similar to controversy guided prompts in Kirk et al. (2024)
5HuggingFaceH4/zephyr-7b-beta
2024). We assume baseline models have no in-
formation on the persona during generation and
leverage a Chain-of-thought (CoT) prompt to elicit
diverse responses. Through sentence-embedding
clustering and generic reward model filtering, we
obtain four diverse ys per x. Note xdivergent and
corresponding ys are shared across personas of
that axis, so the same yl for one might be the yw
for another.Step 4: Label Responses.We use
GPT4-as-personal-judge to obtain the bestyw from
ys through three rounds of pair-wise comparisons.
Dong et al. (2024); Castricato et al. (2025) show
that GPT4 can approximate human preferences as
well as a third-person annotator. Given extensive
public information on the people in our dataset, we
expect GPT4 annotation quality to be similar, if not
better than a third-person annotator. We verify this
with our human annotators, who agree with GPT4
label 78% of the time. See Appendix B for more
details on the construction process, statistics, and
verification efforts.
Our final dataset contains 50 personas across
11 axis. Each persona has 100 train and 100 test
preference pairs, each composed of half personal
and half divergent questions.
3.3 Training and Evaluation
We focus on finetuning and evaluating small mod-
els (1-8B) as they are primary targets as reward
models used during reinforcement learning. Larger
models are costly to run, and often do not allow
access to internals, which we need for our analy-
sis. Since our dataset construction was done with
GPT4, we know large models can customize to
personal preferences through prompting in some
capacity, and leave the extension of our analysis to
larger models for future directions.
Through preliminary studies (Appendix I, L), we
find small models to be in-effective at in-context
learning with few-shot examples. To balance sim-
plicity and performance, we opt-in to finetune our
model in a multi-task fashion (MT), updating a sin-
gle model (adapter) for all users, with a loss similar
to DPO (Rafailov et al., 2024):
LMT =−E i∼[n],(x,yw,yl)∼Di

log
 
βlog πθ(yw|pi,x)
πref (yw|pi,x) −βlog πθ(yl|pi,x)
πref (yl|pi,x)

,
(2)
where each pi =f(x,y w,y l) is a fixed person-
specific prefix. We test the followingpassiveand
activeprefixes:
Passive PrefixesWe randomly samplefew-shot
prefixes (Zhao et al., 2023) with two (x,y w) pairs
from each persona’s train split 6. For embedding-
based methodVPL(Poddar et al., 2024), we train a
variational auto-encoder that embeds 8-shots into a
single embedding token. We also include baseline
prefixtag, an ID string unique to each user.
Active PrefixesFor our oraclegold persona, we
prompt GPT to generate the background and prefer-
ences given the name of the person. This is the only
prefix where the names are revealed to the inference
model. Forpersonaandpersona gpt4, we prompt
the baseline models and GPT4 to generate the same
information using four random shots 7. Note that
personais unique to each inference model.
We perform five-fold cross-validation (CV)
across axes to evaluate generalization (“seen per-
son” vs “unseen persona”) as models need to per-
sonalize to new users without training in prac-
tice. We finetune four LMs across two model
families: LLAMA1/3B, ZEPHYR(7B), and MINIS-
TRAL(8B) 8. Hyperparameters are in Appendix K.
We show similar results with leave-one-axis-out
finetuning in Appendix Q, except that personas in
axes politics and family are hard to general-
ize.
3.3.1 Evaluation Metrics
We adopt internal reference-free rewards9 from Re-
wardBench (Rafailov et al., 2024; Lambert et al.,
2024) simplicity, and it can be calculated asπ(yw |
x)> π(yl |x) where π is the LM, and we average
across (log) token probabilities. Unless otherwise
mentioned, we report reward accuracy averaged
across personas in the unseen splits (50 personas
across five models, 100 questions each).
3.4 Dataset Validation
To verify that personal prefix is necessary for our
dataset, we finetunegold personaand compare
againstno prefixMT baseline. In Table 1, we
see across all models, usingpersona goldsignif-
icantly improved total reward accuracy ( xpersonal
+ xdivergent), validating our dataset. We also see
good generalization in unseen personas, suggesting
6We found 2-shots to be the optimum number of shots with
baseline model given the long response nature of our dataset
7We found significant degradation using more than four
shots in preliminary experiments.
8meta-llama/Llama-3.2-1B-Instruct, meta-llama/Llama-
3.2-3B-Instruct, mistralai/Ministral-8B-Instruct-2410
9It is more intuitively aligned with generation as well as
findings from Chen et al. (2024a).
Seen Persona Unseen Persona
no prefix gold no prefix gold
LLAMA1B .54(.05).65(.06).54(.06).65(.06)
LLAMA3B .51(.05).61(.05).50(.05).59(.06)
ZEPHYR.53(.05).62(.05).52(.06).61(.05)
MINISTRAL.53(.05).63(.05).53(.05).62(.05)
Table 1: Across 4 models, prefixed finetuning withgold
personasignificantly improved total reward accuracy,
generalizing to unseen personas. Parenthesis = standard
deviation across personas.
quality prefixes to be crucial for generalization. In
the next few sections we see how close non-oracle
prefixes can close this performance gap. Example
persona prefixes are in Appendix G.2.
4 Results & Discussions
4.1 Quality active personas are more
interpretable and improve generalization
emb (↑) ra. emb(↓) R1 (↑) ra. R1(↓) words(↓)
few-shot .22 (.09) .15 (.06) .19 (.03) .18 (.03) 563(111)
ZEPHYR.39 (.08) .33 (.07) .24 (.04) .23 (.03) 265 (97)
LLAMA1B .41 (.09) .34 (.09) .25 (.04) .24 (.04) 260 (69)
LLAMA3B .42 (.08) .36 (.07) .26 (.02) .25 (.03) 252 (59)
MINISTRAL.42 (.08).36 (.07) .26 (.04) .25 (.03) 259 (73)
GPT4 .41 (.08) .32 (.09).28 (.04).26 (.03) 209 (35)
persona gold - .49 (.11) - .33 (.10) 203 (18)
Table 2: Mean and standard deviation (across personas)
of LM inferred persona againstpersona goldcompared
to arandompersona gold. MINISTRALwins semanti-
cally, butpersona gpt4s are more separable.
Good active prefixes are shorter, more separa-
ble, more interpretable.Given oracle upper-
bound, we first measure how good are the in-
ferredpersonas compared topersona gold. We
use Qwen3-Embedding10 cosine similarity and
rouge-1(Lin, 2004) to measure semantic similarity
and specific vocabulary recall for each model’sper-
sona. We also provide baseline comparison against
randompersona gold: a larger gap between correct
vs. random persona indicates better separability be-
tween personas. In Table 2, we see more recent
models perform better semantically, with MINIS-
TRALon top, butpersona gpt4wins in separability.
few-shotare not bad in separability but the worst
in semantic similarity and length, this suggests that
even though passive alignment (few-shot) might
perform well in distinguishing user profiles, the
prefixes are likely much lessinterpretable.
10Qwen/Qwen3-Embedding-0.6B
0.5
0.6Personal Accuracy
Prefix Types
no prefix
tag
vpl
few-shot
persona
persona gpt4
persona gold
passive prefix
active prefix
Persona trained Persona not trained
0.5
0.6
0.7Divergent Accuracy
Figure 2: Finetuning results with 5-fold CV on ZEPHYR.
Error bars indicate 95% confidence intervals (CI) across
personas. Dashed line indicates no prefix prompting
baseline. Good quality active prefix (persona gpt4)
generalizes well especially in divergent questions.
Better active prefix generalizes better in diver-
gent questionsWe plot MT(ZEPHYR) perfor-
mance across prefixes in Table 2. In passive pre-
fixes, bothVPLandtaguse a single token, yet
tagperforms similarly tono prefixwhileVPL
performs much better. This suggests semantics
rather than capacity is the issue in associating pref-
erence with prefix.VPLalso excels in personal
questions but fails in divergent questions, indicat-
ing embedding-based methods compress few-shots
information well but fails to encode semantic con-
trasts (i.e. embeddings for “I like lamp” is close
to that of “I don’t like lamp”) (Tang et al., 2022).
This suggests an important future direction is to
actively infer persona compressed from more shots.
Persona gpt4outperformspersona, which outper-
formsfew-shot, suggesting precise and separable
prefixes (Table 2) are more effective, not only for
computational efficiency but also for generaliza-
tion, especially in divergent questions. We show
similar findings with other models in Table M. No-
tably, LLAMA1/3B models prefer few-shots over
self-generatedpersona, whereas it is the opposite
for ZEPHYRand MINISTRAL. In Appendix P, we
investigate prefix sensitivity using shuffled and al-
ternative personas and findpersona gpt4to be the
most robust across variations.11
More precise prefix, more equitable improve-
mentsWe plot finetuning total accuracy on per-
sona not trained against prompting ZEPHYRwith
11Even though GPT4 generates the dataset andpersona
gpt4, MT cannot exploit any shortcuts to predict preferences,
so the improvements stems purely from prefix quality.
0.35 0.40 0.45 0.50 0.55
Total Accuracy (Zephyr no prefix)
0.4
0.5
0.6
0.7Total Accuracy (MT)
Prefix
no prefix (pearson r=0.61, p=2.2e-21)
tag (pearson r=0.63, p=3e-23)
vpl (pearson r=0.088, p=0.22)
few-shot (pearson r=0.25, p=0.0004)
persona (pearson r=0.31, p=8.3e-06)
persona gpt4 (pearson r=0.27, p=0.00012)
persona gold (pearson r=0.16, p=0.022)
Figure 3: MT (persona not trained) vs. ZEPHYRwith no
prefix . We calculate Pearson correlation with p-value
per prefixes. Better prefixes result in lower correlation
and more equitable improvement. Dashed line is no
improvements (y=x). Shaded areas indicates 95% CI.
no prefix in Figure 3.VPLimproves the most
equitably across all personas (the most flat line),
which indicates that compressing more information
for each user is crucial.Persona gpt4,persona
andfew-shoteach outperforms the next while be-
ing less correlated to the baseline, suggesting that
higher quality prefixes might also align more equi-
tably.
Additional results.In Appendix S, U we show
similar trends with generational evaluation, and
discuss mitigating alignment tax (Lee et al., 2024).
4.2 Prefix quality vs. Contributive Attribution
Given that prefixes control reward distributions, it
is important to understandhowmodel responses
are causally dependent on prefixes (i.e. contextual
faithfulness). If algorithms use prefixes solely to
differentiate between users but disregard the un-
derlying semantics, finetuned models could learn
from spurious correlation and exhibit contextually
unfaithful behaviors. We use ContextCite (Cohen-
Wang et al., 2024), to measure contributive attri-
bution from each sentence in the prefix to the re-
sponses (through surrogate modeling12). In other
words, the scores(x,y)∈R L, where L is the num-
ber of sentences in a prefix, tells us how important
each sentence is to the log-likelihood of the LM re-
sponse. For each persona and prompt, we compute
the difference sdiff =s(x,y w)−s(x,y l). In Fig-
ure 4, we plot sdiff (grouped by reward accuracy)
for the test questions of a user across four prefixes
from MT (ZEPHYR) models. We see that active
persona prefixes have more contributing sentences
that are more interpretable than passivefew-shot
12We refer readers to Cohen-Wang et al. (2024) for details.
0 1 2 3 4 5 6 7 8 9 10 11
Prefix Sentence Index
60
40
20
0
20
40
60
80
Attribution Score
** * * * ** * ** **
persona gold with Chaz Bono, 72/100 Reward Prediction is Accurate
results
success
fail
0: Chaz Bono is an American advocate, writer, musician and an actor.
1: He was born in Los Angeles, California, and is the child of famous entertainers Sonny Bono and Cher.
2: Bono is a prominent figure in the LGBTQ+ community, having publicly come out as a lesbian in 1995, and later as a transgender man in 2008.
5: He would also probably appreciate an assistant that can keep him updated about current events and news concerning LGBTQ+ issues and rights.
8: His personal values seem to gravitate towards openness, inclusion, and acceptance, indicated by his all-embracing advocacy work, therefore an AI assistant that mirrors these values and preferences would be ideal for Chaz Bono.
9: Respond to the following prompt from this person.
10: Cater the response to how they might like, agree with, or be interested in.
11: You may change the style, content, length, vocabulary, opinion, stance, or any relevant aspects of your response based on their background.
0 1 2 3 4 5 6 7 8 9 10 11 12
Prefix Sentence Index
75
50
25
0
25
50
75
100
Attribution Score
** * * ** **
persona gpt4 with Chaz Bono, 73/100 Reward Prediction is Accurate
results
success
fail
3: The user's inquiry about films with positive representations of diverse gender identities is another strong indicator of their commitment to accurate, positive portrayal and normalisation of all gender identities.
4: Additionally, the user's request for a varied, dietary-conscious meal plan suggests a personal interest in maintaining a healthy lifestyle or possibly is involved with event or project planning where dietary inclusiveness is a consideration.
10: Respond to the following prompt from this person.
11: Cater the response to how they might like, agree with, or be interested in.
12: You may change the style, content, length, vocabulary, opinion, stance, or any relevant aspects of your response based on their background.
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
Prefix Sentence Index
50
0
50
100
150
Attribution Score
* * * **
persona with Chaz Bono, 58/100 Reward Prediction is Accurate
results
success
fail
2: I recently moved to the city for work and have been adjusting to my new surroundings.
3: I am a proud member of the trans community and take great care to educate myself on matters related to gender identity and expression.
8: I try to be an active listener and ally to marginalized communities, and I am committed to using my platform to promote these causes.
13: Cater the response to how they might like, agree with, or be interested in.
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
Prefix Sentence Index
60
40
20
0
20
40
60
Attribution Score
* **
few-shot with Chaz Bono, 67/100 Reward Prediction is Accurate
results
success
fail
10: Christopher Street Liberation Day (Gay Pride), New York City - 1970
24: ### Preferred Response:
Figure 4: Attribution sdiff for each sentence within prefixes for Chaz Bono with MT (ZEPHYR) model (persona
unseen), grouped by reward accuracy of the questions. For each sentence, we perform student t-test between
success/fail scores and mark * (p<0.05) and **(p<0.01) at the top. Each marked sentence is also displayed in text
along with its indices. Active persona prefixes have more contributing sentences that are more interpretable.
prefix. This trend holds for higher quality personas
(gold>gpt4>persona).
Prefixfew-shot persona persona gpt4 persona gold
Influence fraction .14 (.11) .20 (0.17) .36 (0.22) .40 (.20)
Table 3: IF across personas (unseen) in MT (ZEPHYR).
Better performing prefixes results in models that are
more contextually faithful.
With an intuitive understanding of the score qual-
itatively, we operationalize this as a metric which
we can measure qualitatively across models and
prefixes. For each persona, we calculateinfluence
fraction (IF): the fractions of sentences that sig-
Model persona few-shot
LLAMA1B0.052(.067)0.045(.057)
LLAMA3B 0.100(.162)0.117(.112)
ZEPHYR0.199(.165)0.140(.105)
MINISTRAL0.218(.181)0.106(.093)
Table 4: IF across three out of four models show self-
generated activepersonaleads to more contextually
faithful models. Larger LMs attributes topersonamore.
nificantly (p <0.05) contribute to correct reward
prediction across the test-split of the person13. A
prefix with higher IF indicates that models are more
13Equivalent to fractions of sentence with * in Figure 4
causally influenced (i.e. contextually faithful) by
the prefix. In Table 3, we see that better perform-
ing prefixes result in higher IF. In Table 4, we see
three out of four models attribute to self-generated
personamore thanfew-shot, despite LLAMA1B
performing better withfew-shot. These results sug-
gests that better quality active prefixes results in
more contextually faithful models, and even self-
generatedpersonacould lead to better attribution.
4.3 Prefix Distributional Shift vs. Attribution
In Table 15 we show that quality actively-inferred
personas are robust to surface form variations but
drop performance with prefix from a random per-
son. Here we investigate attribution sensitivities to
varying inference prefixes.
Train Prefix Inference Prefix IF
few-shot(same as train) 0.140(.105)
few-shot (alt. seed) 0.142(.087)
persona(same as train) 0.199(.165)
persona (alt. seed) 0.170(.160)
persona gpt4 0.179(.158)
persona gold 0.145(.153)
persona gold(same as train) 0.398(.202)
persona 0.257(.174)
persona gpt4 0.283(.202)
Table 5: IF with alternative prefixes at inference time
for MT(ZEPHYR). Faithfulness always decreases with
alternative prefix, and models trained with higher quality
prefix remain more faithful.
Quality persona is crucial during train and test.
In Table 5, we can see bothfew-shotandpersona
attribute similarly to a prefix using different shots
(alt. seed). Unique to active prefixes, we can infer
with varying quality of personas to see if model
adapt to changes. Unfortunately, distributional shift
only lowers attribution, even if we increase the
quality of persona at inference time.
Self-generated active persona more causalIn
Table 6, we look into IF before and after finetun-
ing. Unfinetuned ZEPHYRattributes topersona
more thanfew-shotor evenpersona gold, despite
failing at reward accuracy (Appendix I). ZEPHYR
increased attribution through simply inferring pref-
erences from out-of-distributionfew-shotexam-
ples to in-distributionpersona. This is potentially
useful for debugging model faithfulness in general
(Turpin et al., 2023), where generations are not
reflective of internal mechanisms.14
14We did not observe similar behavior with LLAMA1/3B,
suggesting the responses being on-policy is also crucial.
Prefix Finetuned Influential Fraction
few-shotfalse 0.011(.019)
true 0.140(.105)
personafalse 0.141(.108)
true 0.200(.165)
persona goldfalse 0.016(.033)
true 0.399(.202)
Table 6: Unfinetuned ZEPHYRattributes to self-
generatedpersonamuch better than other prefixes (high-
lighted), suggesting simple rephrasing with base LM
could lead to more contextually faithful generations.
Prefix Train Inference Positive Fractions
few-shot
bm25 bm25 0.224(.142)
bm25 bm25 (reverse) 0.220(.141)
bm25 - 0.221(.137)
- bm25 0.213(.176)
- bm25 (reverse) 0.197(.178)
- - 0.197(.170)
persona
bm25 bm250.282(.177)
bm25 bm25 (reverse) 0.274(.180)
bm25 - 0.263(.173)
- bm25 0.253(.141)
- bm25 (reverse) 0.248(.143)
- - 0.238(.138)
Table 7: Number of sentences with positive sdiff in
ZEPHYRtrained and/or inferenced on retrieved shots.
Dash indicates fixed original prefix. Training on re-
trieved prefixes increase attribution more significantly
forpersonathanfew-shot.
Retrieval during training improves contextual
faithfulness, more during training and with per-
sona.A prefix has different aspects that can in-
fluence preferences on the response to a question
(i.e. Halle Berry has diabetes and is also an African
American Actress), and the aspect that influences
the preference distribution for each question may
be different. Majority of our investigation trains a
static prefix for all questions. Such static prefixes
need to cover all aspects of the persona, and LMs
have to select the relevant information during gen-
eration, placing an upperbound on IF. However, if
we were to provide dynamic prefixes that contain
relevant information only through retrieval, can
we further increase contextual faithfulness? To in-
vestigate this, we dynamically retrieve shots with
BM25(Robertson et al., 1993; Lù, 2024) that are
closest to current train / test question as the prefix.
At test time, we vary prefix with static, retrieved
prefix, and reverse prefix (shots farthest in distance).
Instead of IF, we use positive fraction: the fraction
of sentences with positive sdiff . In Table 7, we
can see that training with retrieved shots indeed
Top 10 Easiest Personas to Infer
across
shots
Serena Williams (10.0), Mike Trout (10.8), Chaz Bono (13.4), Bernie Sanders (13.4), Barack Obama (14.6),
Megan Fox (14.6), LeBron James (14.8), Richard Gere (15.8), David Beckham (16.0), Jennifer Aniston (16.8)
across
models
Beyoncé(13.0), Serena Williams(13.2), Zayn Malik(13.6), LeBron James(14.6), Alexandria Ocasio-Cortez(15.0),
Jennifer Aniston (16.4), J.K. Rowling(16.8), David Beckham(17.0), Mike Trout(17.0), Chaz Bono(17.8)
Bottom 10 Hardest Personas to Infer
across
shots
Yoshua Bengio(40.8), Sam Smith (40.8), Meryl Streep (37.8), Latanya Sweeney (37.2), Richard Dawkins(33.4),
Timnit Gebru (33.0), Robert De Niro (32.8), Donald Trump (32.6), Bill Clinton(32.2), Oprah Winfrey (30.8)
across
models
Suchi Saria (41.2), Yoshua Bengio (38.6), Millie Bobby Brown (38.4), Sheryl Sandberg (37.0), Halle Berry
(36.6), Sam Smith (34.4), Bill Clinton (32.2), Tiger Woods (32.2), Laverne Cox (31.0), Billie Eilish (30.8)
Table 8: Easiest and hardest personas by inferredpersonaquality (average rank in parenthesis). Colored names
appear in both splits. We see personas in axis sports and (liberal) politics are consistently easy to infer for
LMs, while those inAI Professorsare hard.
attribute econ. status birth country education prof. curr. state religion marriage
persona 0.02 0.05 0.07 0.11 0.22 0.22 0.23
persona gpt4 0.02 2e-3 0.17 6e-11 1e-4 3e-4 0.37
persona gold 3e-5 0.05 3e-5 2e-13 2e-4 0.08 0.02
few-shot 5e-3 0.02 4e-4 3e-8 1e-4 0.14 0.03
attribute birth state ethnicity age sex pref. curr. country gender race politics
persona0.28 0.35 0.39 0.39 0.44 0.45 0.55 0.98
persona gpt4 4e-4 2e-4 0.09 0.5 0.16 0.01 0.52 0.58
persona gold 8e-5 0.05 0.04 0.06 0.1 2e-5 0.67 0.53
few-shot 0.02 8e-5 2e-3 0.13 0.02 2e-3 0.02 0.43
Table 9: P-values of one-tail ANOV A across four models before and after finetuning show thatfew-shotimprove-
ments are non-uniform (in 12/15 attributes) where aspersonaimprovements are much more equitable (2/14).
Increasing persona specificity and quality (persona → persona gpt4 → persona gold) decreases improvement
equity, suggesting a performance vs. fairness trade-off.
increases contextual faithfulness andpersonaben-
efits more thanfew-shot. Less improvements are
observed at test time.
4.4 Systematic Bias with Personas?
Persona inference and dataset bias exists.Pre-
vious experiments showed that quality actively-
inferred persona improve reward generalization and
result in a model that is more contextually faith-
ful. However, given the personas are generated,
we need to be cautious against systematic biases
(Kovaˇc et al., 2023). We investigate two sources
of bias: persona inference, and finetuning with
inferred persona. To check whether there is bias
for persona inference, we repeat persona inference
1)across shots2)across modelswith the same
shots15, and measure persona quality againstper-
sona gold. We average z-score normalized rouge-1
and embedding similarity (Section 4.1) and average
each person’s score rank ∈(0,50) . In Table 8, we
show the top and bottom 10 people and their av-
15Fourpersonamodels + MT(ZEPHYR)persona gpt4
eraged ranks. We see that people in axis sports
and (liberal) politics consistently appear in the
top, while AI professors often at the bottom.
We suspect this is because public information on
athletes are mostly single-faceted, and the only
underspecification is the sport they play. Liberal
politicians’ views on different issues may be highly
correlated (e.g. supporting minimum wages indi-
cates strongly their stance on gay marriage). Public
information on AI professors, by contrast, is
mostly based on objectively written papers which
reveals little about their personal views.
Active alignment more equitable than passive af-
ter finetuningSome preferences might be easier
to learn during finetuning, skewing overall prefer-
ence distributions. We compare total reward accu-
racy difference between MT and baselines (using
personaprefix) across four baseline LMs to un-
derstand biases from finetuning. We use one-way
ANOV A (Lowry, 2014) to test uniformity of im-
provements across groups (See Appendix F.2,R
for demographics statistics and visualizations). In
Table 9, we see thatfew-shotprefix results in non-
uniform improvements in more attributes thanper-
sona. We suspect this is because persona inference
introduces noise and “diffuses” away statistical bi-
ases. Indeed, when we compare persona with in-
creasing specificity/quality (persona → persona
gpt4 → persona gold), improvements becomes
less equitable. This suggests an inherent trade-
off between improving personalized performance
vs. being equitable, likely due to imbalanced para-
metric knowledge LMs have on different demo-
graphics. We believe this to be an important future
direction: balancing fairness vs. improvements.
5 Conclusions
We constructed FamousPersona, a personalized
alignment dataset on famous people, to answer our
research question: is active alignment (inferring
personal preference) better than passive alignment
(simply using few-shots)? Results from reward ac-
curacy generalization, prefix attribution patterns,
and bias analysis confirm that actively inferring
persona is crucial for interpretable and robust per-
sonalized alignment. Future studies should focus
on how to further evaluate and de-bias inferred
persona, and dynamically modify persona prefixes
according to the user query.
Limitations
Our dataset presents a playground through which
both theoreticians and practitioners in AI alignment
can empirically validate their methods. We separate
limitations and future works in the following two
directions:
Dataset improvement
Better axes, prompt generation, and label fi-
delity.The selection of axes is not representative
of all axes through which human preference differs.
However one could arbitrarily extend the dataset to
axis of interest to study (e.g. moral, ethical values).
One could also extend to include people famous in
different countries (and speak different languages),
extending personal preference alignment to mul-
tilingual setting. The quality of our dataset also
depends on GPT4 not hallucinating when generat-
ing questions (x) and labeling preferences (yw/yl).
One valid direction is actually obtaining x or pref-
erence labels from the people we are modeling,
and understand the true annotation quality. Beyond
label fidelity, personal preferences is a dynamic
distribution which changes over time, which would
be interesting to model in future works. Lastly, we
assume findings from our paper will generalize to
non-famous people because we infer prefixesper-
sona/ persona gpt4without revealing the name of
the person. However, the questions and preferences
could be biased and specific to famous people only.
Due to its synthetic nature, it is also not impossi-
ble for our oraclepersona goldto contain biased
assumptions that humans also make from a third-
person perspective. Hence there could be further
biases that we were not able to find.
Better diversity in responses (y).When gener-
ating candidate responses with CoT, we find it to
influence contents the most, leaving other stylistic
features mostly unchanged. Future work should
look into ways to diversify generations beyond con-
tent, which will also make preferences more nu-
anced and challenging to infer. Additionally, even
though we aim to generate diverse response, there
is no guarantee that we will end up with one that is
a good response (all responses might still be bad).
In these cases, providing multiple responses with
point-wise estimation of reward might be a better
dataset construction method. However, it is much
harder for LLM-as-personal-judge. Additionally,
we chose to generate responses with ZEPHYRonly
because we were interested on-policy effects of
alignment. To improve the general utility of the
dataset as generic finetuning data, we would have
generated diverse responses with multiple more
capable models.
Adaptive personalization.Our response genera-
tion process also mimics the trade-off between the
exploration vs. exploitation problem in RL: is it
better to play safe and generate a generically-good
answer or risk for more personalized answer. Fu-
ture work could look into the process through an
online/active learning perspective, balancing gen-
eral response quality vs. venturing into personal-
ization. Asking follow up clarification questions
seems like a promising direction.
Better preference modeling
Tuning on preference inferenceWe did a pre-
liminary experiment where we train MT models
to predictpersona gpt4(over a wrong persona
through DPO objective) in addition to aligning pref-
erences, similar to a reasoning distillations setup
(Mukherjee et al., 2023), where we considerper-
sona gpt4as the reasoning trace. We did not see
much improvement. Future work can explore fur-
ther leveraging findings in improving reasoning in
LMs (Hao et al., 2024). One could also potentially
find middle ground between training personal mod-
els (PM) and MT by finding training and retrieving
“prototypical” personas (Zhong et al., 2024). We
focus on our analysis on MT models.
Alternative objectivesIn our work, we focus on
simple methods that are scalable, efficient, and
high-performing. However, many other objec-
tives and methodologies are equally important and
promising. During multi-task stage learning, we
did not consider the perspective of differential pri-
vacy (Salemi and Zamani, 2024), whereas in the
real world, the use of personal data for generic
training requires further scrutinizing. As outlined
by Sorensen et al. (2024), one could also align to
diverse expectations by explicitly generating all
output preferences (“overton”), which come at the
cost of verbosity. Given our finding on alignment
tax, future work can also explore the trade-off be-
tween personalization and general capability by
adapting prefixes with different levels of specifica-
tion at inference time.
Future Analysis
Scaling up model sizes.Due to compute con-
straints, we were not able to run experiments with
models larger than 8B sizes. It would be inter-
esting to confirm whether the advantage ofactive
prefix overpassiveincreases with larger model
scale. Why do some models attribute to prefixes
more than others? We thought another reason
LLAMA1/3B models might perform better with
few-shotis because they were trained on more
few-shot data, hence able to leverage the few-shot
format better. Without transparency of the training
procedures this hypothesis is hard to verify.
Why the bias reduction?Why areactivepre-
fixes able to reduce bias compared topassivepre-
fixes? From Appendix R we seepersona’s im-
provement are more mild and equal across differ-
ent attributes. We conjuncture that this might be
the noise introduced in the persona inference pro-
cess. However, if that is the case, would model start
associating non-robust features with preference dis-
tributions? Or perhaps it is the explicit mentioning
of attributions that improved it?
Evaluation on other datasetsWe constructed
our dataset specifically for the purpose of evaluat-
ing persona preferences, hence every person in the
dataset has a fixed, detailed, persona that grounds
their questions and preferences. However, it may
still be an open question how much active persona
inference helps on interactions where there may not
be a clear preference that generalizes user behavior
in other situations.
Ethical considerations
Our dataset is entirely generated from GPT4, hence
the dataset (from persona selection, to prompt gen-
eration and preference labeling) is dependent on
the quality of GPT4. We do not claim personas in-
cluded in our dataset are faithful to their real world
counterparts, nor personas’ belief/preferences to be
universally good or bad, but offer a playground to
construct sets of personas with unique and diverse
preferences. The authors manually read through
most if not all prompts and responses to make sure
there are no offensive content. We emphasize that
personas’ questions, opinions, and preferences are
notthe same as the real people they are modeled
after. Models trained on our dataset should not be
used to imitate famous people’s opinions other than
for research purpose.
Although not specific to our dataset, personal-
ization creates an “echo chamber” in which users
would be catered responses that they agree with,
aggravating the issue of sycophancy (Sharma et al.,
2023). There is also the danger of generating po-
tentially unsafe content from personalizing to in-
dividuals with extreme ideologies that are harmful
to themselves or others. Other than the solution
we propose of removing personal prefix at infer-
ence time, we believe there should be a hard limit
to which personalization can go, perhaps imple-
mented through means of KL divergence (Rafailov
et al., 2024).
Belief projection is another concern in model
alignment where models make unwarranted as-
sumptions of users given contextual clues. An
important aspect persona inference is to explicitly
state the assumptions that models have, such that
the wrong assumptions can be removed if necessary.
However, it is important to discuss where the right
line should be between making statistically-based
assumptions vs. stereotyping.
Acknowledgments
We thank Piotr Teterwak, Maan Qraitem, Najoung
Kim, Hayley Ross, Yusuf Kocygit, Gabriel Franco,
Micah Benson for their helpful discussions and
advice. We thank annotators for their meticulous
annotations and anonymous reviewers for their con-
structive feedback.
References
Lora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd
truth and the seven myths of human annotation.AI
Magazine, 36(1):15–24.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback.arXiv
preprint arXiv:2204.05862.
Nishant Balepur, Vishakh Padmakumar, Fumeng Yang,
Shi Feng, Rachel Rudinger, and Jordan Boyd-Graber.
2025. Whose boat does it float? improving person-
alization in preference tuning via inferred user per-
sonas. Association for Computational Linguistics.
Tilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna
Gurevych. 2024. Sensitivity, performance, robust-
ness: Deconstructing the effect of sociodemographic
prompting. InProceedings of the 18th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
2589–2615.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2020. Piqa: Reasoning about
physical commonsense in natural language. InThirty-
Fourth AAAI Conference on Artificial Intelligence.
Millennium Bismay, Xiangjue Dong, and James Caver-
lee. 2025. Reasoningrec: Bridging personalized rec-
ommendations and human-interpretable explanations
through llm reasoning. InFindings of the Association
for Computational Linguistics: NAACL 2025, pages
8132–8148.
Steve Byrnes. 2023. Plan for mediocre alignment of
brain-like [model-based RL] AGI — AI Alignment
Forum — alignmentforum.org. [Accessed 22-10-
2024].
Louis Castricato, Nathan Lile, Rafael Rafailov, Jan-
Philipp Fränken, and Chelsea Finn. 2025. Persona:
A reproducible testbed for pluralistic alignment. In
Proceedings of the 31st International Conference on
Computational Linguistics, pages 11348–11368.
Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Kop-
pel, Furong Huang, Dinesh Manocha, Amrit Bedi,
and Mengdi Wang. 2024. Maxmin-rlhf: Towards
equitable alignment of large language models with
diverse human preferences. InICML 2024 Workshop
on Models of Human Feedback for AI Alignment.
Angelica Chen, Sadhika Malladi, Lily H Zhang,
Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, and
Kyunghyun Cho. 2024a. Preference learning al-
gorithms do not learn preference rankings.arXiv
preprint arXiv:2405.19534.
Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu,
Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong
Chen, Xingmei Wang, et al. 2023. When large lan-
guage models meet personalization: Perspectives
of challenges and opportunities.arXiv preprint
arXiv:2307.16376.
Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai,
and Zuozhu Liu. 2024b. Pad: Personalized alignment
at decoding-time.arXiv preprint arXiv:2410.04070.
Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo,
Qingyu Yin, Ruirui Li, Zheng Li, and Wei Wang.
2024c. Iteralign: Iterative constitutional alignment
of large language models. InProceedings of the 2024
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (Volume 1: Long Papers),
pages 1423–1433.
Hyeong Kyu Choi and Yixuan Li. 2024. Beyond help-
fulness and harmlessness: Eliciting diverse behaviors
from large language models with persona in-context
learning.arXiv preprint arXiv:2405.02501.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge.ArXiv,
abs/1803.05457.
Benjamin Cohen-Wang, Harshay Shah, Kristian
Georgiev, and Aleksander Madry. 2024. Contextcite:
Attributing model generation to context.Advances in
Neural Information Processing Systems, 37:95764–
95807.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi
Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,
and Bowen Zhou. 2023. Enhancing chat language
models by scaling high-quality instructional conver-
sations.Preprint, arXiv:2305.14233.
Xuan Long Do, Kenji Kawaguchi, Min-Yen Kan, and
Nancy F Chen. 2023. Choire: Characterizing and
predicting human opinions with chain of opinion
reasoning.arXiv preprint arXiv:2311.08385.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,
Shizhe Diao, Jipeng Zhang, Kashun Shum, and
Tong Zhang. 2023. Raft: Reward ranked finetuning
for generative foundation model alignment.arXiv
preprint arXiv:2304.06767.
Yijiang River Dong, Tiancheng Hu, and Nigel Collier.
2024. Can llm be a personalized judge?arXiv
preprint arXiv:2406.11657.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Alpaca-
farm: A simulation framework for methods that learn
from human feedback.Preprint, arXiv:2305.14387.
Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas
Schiefer, Amanda Askell, Anton Bakhtin, Carol
Chen, Zac Hatfield-Dodds, Danny Hernandez,
Nicholas Joseph, et al. 2023. Towards measuring
the representation of subjective global opinions in
language models.arXiv preprint arXiv:2306.16388.
Kawin Ethayarajh, Yejin Choi, and Swabha
Swayamdipta. 2022. Understanding dataset
difficulty with V-usable information. InProceedings
of the 39th International Conference on Machine
Learning, volume 162 ofProceedings of Machine
Learning Research, pages 5988–6008. PMLR.
Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian
Fisher, Chan Young Park, Yejin Choi, and Yulia
Tsvetkov. 2024. Modular pluralism: Pluralistic align-
ment via multi-llm collaboration.CoRR.
Jan-Philipp Fränken, Samuel Kwok, Peixuan Ye, Kan-
ishk Gandhi, Dilip Arumugam, Jared Moore, Alex
Tamkin, Tobias Gerstenberg, and Noah Goodman.
2023. Social contract ai: Aligning ai assistants with
implicit group norms. InSocially Responsible Lan-
guage Modelling Research.
Iason Gabriel. 2020. Artificial intelligence, values, and
alignment.Minds and machines, 30(3):411–437.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,
Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
ish Thite, Ben Wang, Kevin Wang, and Andy Zou.
2024a. A framework for few-shot language model
evaluation.
Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou,
Junjie Ye, Xiao Wang, Rui Zheng, Yicheng Zou, Zhi
Chen, Hang Yan, et al. 2024b. Linear alignment:
A closed-form solution for aligning human prefer-
ences without tuning and feedback. InForty-first
International Conference on Machine Learning.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio
Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. 2020.
Shortcut learning in deep neural networks.Nature
Machine Intelligence, 2(11):665–673.
David Goldberg, David Nichols, Brian M Oki, and Dou-
glas Terry. 1992. Using collaborative filtering to
weave an information tapestry.Communications of
the ACM, 35(12):61–70.
Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li,
Zhiting Hu, Jason Weston, and Yuandong Tian. 2024.
Training large language models to reason in a contin-
uous latent space.arXiv preprint arXiv:2412.06769.
Seth Herd. 2023. We have promising alignment plans
with low taxes — AI Alignment Forum — alignment-
forum.org. [Accessed 22-10-2024].
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration.arXiv preprint arXiv:1904.09751.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. 2021. Lora: Low-rank adaptation of large lan-
guage models. InInternational Conference on Learn-
ing Representations.
James Y Huang, Sailik Sengupta, Daniele Bonadiman,
Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Man-
sour, Katrin Kirchoff, and Dan Roth. 2024. Deal:
Decoding-time alignment for large language models.
arXiv preprint arXiv:2402.06147.
EunJeong Hwang, Bodhisattwa Majumder, and Niket
Tandon. 2023. Aligning language models to user
opinions. InFindings of the Association for Com-
putational Linguistics: EMNLP 2023, pages 5906–
5919.
Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong
Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh
Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.
2023. Personalized soups: Personalized large lan-
guage model alignment via post-hoc parameter merg-
ing.arXiv preprint arXiv:2310.11564.
Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi
Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou
Wang, and Yaodong Yang. 2024. Beavertails: To-
wards improved safety alignment of llm via a human-
preference dataset.Advances in Neural Information
Processing Systems, 36.
Liwei Jiang, Sydney Levine, and Yejin Choi. 2024. Can
language models reason about individualistic human
values and preferences? InPluralistic Alignment
Workshop at NeurIPS 2024.
Maxim Khanov, Jirayu Burapacheep, and Yixuan Li.
2024. Args: Alignment as reward-guided search. In
The Twelfth International Conference on Learning
Representations.
Jaehyung Kim and Yiming Yang. 2024. Few-shot
personalization of llms with mis-aligned responses.
CoRR.
Hannah Rose Kirk, Alexander Whitefield, Paul Röttger,
Andrew Bean, Katerina Margatina, Juan Ciro, Rafael
Mosquera, Max Bartolo, Adina Williams, He He,
et al. 2024. The prism alignment project: What par-
ticipatory, representative and individualised human
feedback reveals about the subjective and multicul-
tural alignment of large language models.arXiv
preprint arXiv:2404.16019.
Oliver Klingefjord, Ryan Lowe, and Joe Edelman. 2024.
What are human values, and how do we align ai to
them?arXiv preprint arXiv:2404.10636.
Grgur Kovaˇc, Masataka Sawayama, Rémy Portelas, Cé-
dric Colas, Peter Ford Dominey, and Pierre-Yves
Oudeyer. 2023. Large language models as super-
positions of cultural perspectives.arXiv preprint
arXiv:2307.07870.
Klaus Krippendorff. 2011. Computing krippendorff’s
alpha-reliability.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
Noah A. Smith, and Hannaneh Hajishirzi. 2024. Re-
wardbench: Evaluating reward models for language
modeling.Preprint, arXiv:2403.13787.
Gihun Lee, Minchan Jeong, Yujin Kim, Hojung Jung,
Jaehoon Oh, Sangmook Kim, and Se-Young Yun.
2024. Bapo: Base-anchored preference optimization
for personalized alignment in large language models.
CoRR.
Yoonho Lee, Huaxiu Yao, and Chelsea Finn. 2022. Di-
versify and disambiguate: Learning from underspec-
ified data. InICML 2022: Workshop on Spurious
Correlations, Invariance and Stability.
Jia-Nan Li, Jian Guan, Wei Wu, and Rui Yan. 2025.
Extended inductive reasoning for personalized pref-
erence inference from behavioral signals.arXiv
preprint arXiv:2505.18071.
Junyi Li, Charith Peris, Ninareh Mehrabi, Palash Goyal,
Kai-Wei Chang, Aram Galstyan, Richard Zemel, and
Rahul Gupta. 2024a. The steerability of large lan-
guage models toward data-driven personas. InPro-
ceedings of the 2024 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume
1: Long Papers), pages 7283–7298.
Xinyu Li, Zachary C Lipton, and Liu Leqi. 2024b. Per-
sonalized language modeling from personalized hu-
man feedback.arXiv preprint arXiv:2402.05133.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. InText Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. InProceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.
Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jian-
meng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang,
Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie
Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and
Tong Zhang. 2024. Mitigating the alignment tax of
RLHF. InProceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing,
pages 580–606, Miami, Florida, USA. Association
for Computational Linguistics.
Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie
Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han
Zhao, Yuan Yao, et al. 2023. Speciality vs gener-
ality: An empirical study on catastrophic forgetting
in fine-tuning foundation models.arXiv preprint
arXiv:2309.06256.
Chunming Liu, Xin Xu, and Dewen Hu. 2014. Multi-
objective reinforcement learning: A comprehensive
overview.IEEE Transactions on Systems, Man, and
Cybernetics: Systems, 45(3):385–398.
Ryan Liu, Jiayi Geng, Joshua C Peterson, Ilia Sucholut-
sky, and Thomas L Griffiths. 2024. Large language
models assume people are more rational than we re-
ally are.arXiv preprint arXiv:2406.17055.
Richard Lowry. 2014. Concepts and applications of
inferential statistics.
Xing Han Lù. 2024. Bm25s: Orders of magnitude faster
lexical search via eager sparse scoring.Preprint,
arXiv:2407.03618.
Alasdair MacIntyre. 2013.After virtue. A&C Black.
Mary L McHugh. 2012. Interrater reliability: the kappa
statistic.Biochemia medica, 22(3):276–282.
Yu Meng, Mengzhou Xia, and Danqi Chen. 2024.
Simpo: Simple preference optimization with a
reference-free reward. InAdvances in Neural In-
formation Processing Systems (NeurIPS).
Smitha Milli, Dylan Hadfield-Menell, Anca Dragan,
and Stuart Russell. 2017. Should robots be obedient?
InProceedings of the 26th International Joint Con-
ference on Artificial Intelligence, pages 4754–4760.
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-
har, Sahaj Agarwal, Hamid Palangi, and Ahmed
Awadallah. 2023. Orca: Progressive learning from
complex explanation traces of gpt-4.arXiv preprint
arXiv:2306.02707.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback.Advances in neural in-
formation processing systems, 35:27730–27744.
Chan Young Park, Shuyue Stella Li, Hayoung Jung,
Svitlana V olkova, Tanu Mitra, David Jurgens, and
Yulia Tsvetkov. 2024a. Valuescope: Unveiling im-
plicit norms and values via return potential model of
social interactions. InFindings of the Association
for Computational Linguistics: EMNLP 2024, pages
16659–16695.
Chanwoo Park, Mingyang Liu, Kaiqing Zhang, and
Asuman Ozdaglar. 2024b. Principled rlhf from het-
erogeneous feedback via personalization and prefer-
ence aggregation.arXiv preprint arXiv:2405.00254.
Sriyash Poddar, Yanming Wan, Hamish Ivison, Ab-
hishek Gupta, and Natasha Jaques. 2024. Person-
alizing reinforcement learning from human feedback
with variational preference learning. InThe Thirty-
eighth Annual Conference on Neural Information
Processing Systems.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model.Advances in Neu-
ral Information Processing Systems, 36.
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1993. Okapi at trec-2. InTREC, pages 21–34.
Alireza Salemi, Sheshera Mysore, Michael Bendersky,
and Hamed Zamani. 2023. Lamp: When large lan-
guage models meet personalization.arXiv preprint
arXiv:2304.11406.
Alireza Salemi and Hamed Zamani. 2024. Comparing
retrieval-augmentation and parameter-efficient fine-
tuning for privacy-preserving personalization of large
language models.Preprint, arXiv:2409.09510.
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
Lee, Percy Liang, and Tatsunori Hashimoto. 2023.
Whose opinions do language models reflect? InIn-
ternational Conference on Machine Learning, pages
29971–30004. PMLR.
Mrinank Sharma, Meg Tong, Tomasz Korbak, David
Duvenaud, Amanda Askell, Samuel R Bowman, Esin
DURMUS, Zac Hatfield-Dodds, Scott R Johnston,
Shauna M Kravec, et al. 2023. Towards understand-
ing sycophancy in language models. InThe Twelfth
International Conference on Learning Representa-
tions.
Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Han-
naneh Hajishirzi, Noah A Smith, and Simon Shaolei
Du. 2024. Decoding-time language model alignment
with multiple objectives. InICML 2024 Workshop
on Theoretical Foundations of Foundation Models.
Anand Siththaranjan, Cassidy Laidlaw, and Dylan
Hadfield-Menell. 2023. Distributional preference
learning: Understanding and accounting for hidden
context in rlhf. InThe Twelfth International Confer-
ence on Learning Representations.
Paul Slovic. 1995. The construction of preference.
American psychologist, 50(5):364.
Taylor Sorensen, Jared Moore, Jillian Fisher,
Mitchell Gordon, Niloofar Mireshghallah,
Christopher Michael Rytting, Andre Ye, Li-
wei Jiang, Ximing Lu, Nouha Dziri, et al. 2024. A
roadmap to pluralistic alignment.arXiv preprint
arXiv:2402.05070.
Moritz Stephan, Alexander Khazatsky, Eric Mitchell,
Annie S Chen, Sheryl Hsu, Archit Sharma, and
Chelsea Finn. 2024. Rlvf: Learning from verbal
feedback without overgeneralization.arXiv preprint
arXiv:2402.10893.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback.Advances
in Neural Information Processing Systems, 33:3008–
3021.
Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R
Fung, Hou Pong Chan, ChengXiang Zhai, and
Heng Ji. 2024. Persona-db: Efficient large lan-
guage model personalization for response prediction
with collaborative data refinement.arXiv preprint
arXiv:2402.11060.
Zilu Tang, Muhammed Yusuf Kocyigit, and Derry Tanti
Wijaya. 2022. Augcse: Contrastive sentence embed-
ding with diverse augmentations. InProceedings of
the 2nd Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics and the
12th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
375–398.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, Nathan Sarrazin, Omar San-
seviero, Alexander M. Rush, and Thomas Wolf. 2023.
Zephyr: Direct distillation of lm alignment.Preprint,
arXiv:2310.16944.
Miles Turpin, Julian Michael, Ethan Perez, and Samuel
Bowman. 2023. Language models don’t always say
what they think: Unfaithful explanations in chain-of-
thought prompting.Advances in Neural Information
Processing Systems, 36:74952–74965.
Angelina Wang, Jamie Morgenstern, and John P Dick-
erson. 2024a. Large language models cannot replace
human participants because they cannot portray iden-
tity groups.arXiv preprint arXiv:2402.01908.
Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng
Yang, Andrew Cohen, Lei Li, and Yuandong Tian.
2024b. Learning personalized alignment for evalu-
ating open-ended text generation. InProceedings
of the 2024 Conference on Empirical Methods in
Natural Language Processing, pages 13274–13292,
Miami, Florida, USA. Association for Computational
Linguistics.
Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang,
Shizhe Diao, Shuang Qiu, Han Zhao, and Tong
Zhang. 2024c. Arithmetic control of llms for di-
verse user preferences: Directional preference align-
ment with multi-objective rewards.arXiv preprint
arXiv:2402.18571.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models.Advances in neural
information processing systems, 35:24824–24837.
Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang,
Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang.
2024. Iterative preference learning from human feed-
back: Bridging theory and practice for rlhf under
kl-constraint.Preprint, arXiv:2312.11456.
Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin
Huang, Tianlin Zhang, and Sophia Ananiadou. 2024.
Metaaligner: Towards generalizable multi-objective
alignment of language models. InThe Thirty-eighth
Annual Conference on Neural Information Process-
ing Systems.
Siyan Zhao, John Dang, and Aditya Grover. 2023.
Group preference optimization: Few-shot alignment
of large language models. InNeurIPS 2023 Work-
shop on Instruction Tuning and Instruction Follow-
ing.
Huiying Zhong, Zhun Deng, Weijie J Su, Zhiwei Steven
Wu, and Linjun Zhang. 2024. Provable multi-party
reinforcement learning with diverse human feedback.
arXiv preprint arXiv:2403.05006.
Thomas P Zollo, Andrew Wei Tung Siah, Naimeng Ye,
Ang Li, and Hongseok Namkoong. 2024. Person-
alllm: Tailoring llms to individual preferences. In
Pluralistic Alignment Workshop at NeurIPS 2024.
A Comparison to Existing Datasets
See Table 10.
B Details of Dataset Construction
B.1 Step 1: Persona Selection
Given axis of contrast, we use Prompt H.1 on GPT4
to provide at most five sub-categories (e.g. liberal)
along with a famous person associated with the cat-
egory (e.g. Bernie Sanders). Details of axes, sub-
categories, and personas are in Appendix Table 14.
We leverage GPT4 to sample personas mainly to
ensure the people are famous enough such that the
public and LLMs can make educated guesses about
their preferences. We do, however, recognize this
results in a biased sample of the human population
(Section 6, Appendix F.3), and analyze systematic
biases in Section 4.4 and Appendix R.
B.2 Step 2: Generate Prompts
We generate the questions ( x) for each persona
with Prompt H.2. We manually verify the quality
of prompts in Appendix C and analyze the diversity
and overlap ofxin Appendix F.4.
B.3 Step 3: Sample Responses
We chose ZEPHYRas our baseline model because it
is a well performing DPO-aligned model on generic
preference dataset (Ding et al., 2023; Tunstall et al.,
2023). Since baseline model has no information on
the user initially, we need a way to sample diverse
responses, such that the contrastive pair provides
the right signal for the model to learn from. Re-
sponses should not differ trivially (e.g. spelling) or
in topics we cannot infer from the persona due to
lack of public information (e.g. Serena William’s
political affiliation). Our preliminary effort con-
firms that naive sampling methods do not change
the content of the response much, yielding little
diversity. Instead, we sample 50 diverse responses
using CoT prompts (i.e. “what are different ways
in which the user might expect different answers”),
filter for diversity (through clustering sentence em-
beddings), and ensuring that responses selected
are preferred equally with a generic reward model
(Dong et al., 2023; Xiong et al., 2024).
Cot generationWe use CoT prompt H.3 and
prompt model to first select a possible axis the
prompt belongs to (e.g. politics), and then iden-
tify all possible sub-categories/angles (e.g. con-
servatives) through which the user might expect
the answers. For personal questions, we provide
no constraints to what the axis and sub-categories
can be, maximizing the diversity in topic of the
response. For divergent questions, we use ground-
truth axis and sub-categories from our dataset, to
ensure the difference in the final contrastive pair
contains the desired signal.
To sample 50 candidate responses, we first gen-
erate five CoT responses and cache the axis and
sub-categories. For each of the CoTs, we generate
10 responses, uniformly sampling sub-categories
from that CoT. We do this instead of using CoT for
all 50 responses for efficiency and to avoid possi-
ble positional bias from the sub-categories (e.g. if
sub-category “liberal” is always enumerated before
“conservatives”, then “conservatives” generations
will be sampled less). See full example in Ap-
pendix G.
After obtaining the 50 y candidates from the
baseline model, we use a post-processing script to
remove artifacts strings which might review the
identifiable attributes (“For our liberal audience
...”). We then proceed to filter for quality and diver-
sity.
Filtering with generic reward modelThe
first step involves ensuring selected responses
for yw,y l do not differ much according to
a generic reward model. We take a top-
performing reward models from from Reward-
Dataset Pref. type Open-ended? Real persona? Verifiable persona? Synthetic? Personalizedx?Unbiasedy?
LaMP (Salemi et al., 2023) personal
PersonalSoup (Jang et al., 2023) multi-objective
HH-RLHF (Bai et al., 2022; Yang et al., 2024) multi-objective
OpinionQA (Santurkar et al., 2023) personal
PRISM (Kirk et al., 2024) personal
Personal-LLM (Zollo et al., 2024) personal
PERSONA (Castricato et al., 2025) personal
FamousPersona (Ours) personal
Table 10: Compared to other personalization datasets, our is generated with realistic constraints. Personalized
x=different users ask different questions. Unbiased y=model does not uses user information when generating
response.
Figure 5: Dataset generation procedure.Step 1: (B.1) personas are selected in the dataset according to different axis
of disagreements.Step 2: (B.2) prompts are sampled per person/axis.Step 3: (B.3) diverse responses are sampled
from the baseline model and filtered.Step 4: (B.4) preferences are labeled by GPT4 through LLM-as-personal-judge.
Dashed and dotted components are sampled from GPT4 and the baseline model (ZEPHYR) respectfully.
Bench (Xiong et al., 2024; Lambert et al., 2024)
(sfairXC/FsfairX-LLaMA3-RM-v0.1) at
the time of the writing, and obtain a scalar reward
for each of the responses y. We then sort the ys
based on reward, and collect 20 responses with
smallest reward range (i.e. max-min) in a contin-
uous span (in sorted reward) to ensure any two ys
within such span would differ minimally from each
other.
Filtering for diversityThe next step involves
selecting diverse response samples from the result-
ing pool of 20 responses. We run K-means clus-
tering16 on responses’ sentence embedding using
sentence-t5-xxl17. For each of k clusters,
we select the sample that is farthest from all other
cluster centers. In our experiment, we pick k=4
so the resulting 4 ys are labeled by GPT4 in three
rounds of pairwise comparison, single-elimination
style.
B.4 Step 4: Label Preferences
We label preferences with Prompt H.4. Our hu-
man study for verifying the preference labels are
detailed in Appendix D.
C Prompt validation
Due to the synthetic nature of our dataset, we
take additional measures to ensure the quality of
prompts (x) generated by GPT4. We assume that
by using famous people and generating prompts
in topics/axis that they are known for, we can rea-
sonably guess their preference. In this section, we
attempt to validate this assumption manually on a
subset of our dataset. We randomly subset 10 ques-
tions (half divergent half personal) for 10 personas’
test split. We answer (to the best of our knowledge)
the following two questions regarding each prompt:
1. Is this questions something the persona might
actually ask an AI assistant (validity)?
(a) score 1 - definitely (if the person has
asked exact or similar questions in the
past, or that question has been asked by
people similar to the person)
(b) score 2 - maybe (if the person is has some
known information relating to the gen-
16https://scikit-learn.org/stable/
modules/generated/sklearn.cluster.KMeans.
html
17https://huggingface.co/
sentence-transformers/sentence-t5-xxl
personal divergent
name validity verifiable validity verifiable
AOC 1.0 1.0 1.4 1.0
BO 1.4 1.4 2.0 2.0
BS 1.0 1.2 1.2 1.2
B 1.0 1.0 2.2 1.8
BC 1.0 1.2 2.0 2.0
DT 1.2 1.0 1.2 1.0
HB 1.0 1.0 1.2 1.2
LJ 1.0 1.0 1.2 1.6
TG 1.4 1.2 1.6 1.6
YB 1.8 1.8 1.2 1.0
Avg. 1.18 1.18 1.52 1.44
stdev. 0.27 0.26 0.40 0.41
Table 11: Results on manual verification of prompt
validityandverifiable-ness. Names are represented
with the first letter of their initials.
eral topic, but not conclusive evidence of
the connection)
(c) score 3 - not likely (if there is little to no
data supporting the connection, or there
are evidences against it)
2. Is this questions something verifiable through
publicly known information (verifiable)?
(a) score 1 - definitely (the information
might be in an article, or there is enough
related information out there that is sim-
ilar, through which we can likely guess
preference. The nature of the question
could also be more objective and the gen-
eral quality can be verified.)
(b) score 2 - maybe (there exists information
on the web connecting the persona to re-
lated topic but not conclusive, or that the
question can lead to similar responses)
(c) score 3 - not likely (there is little to no
data relating the person to the question,
or there are evidences against it)
The authors of this paper did all the annotations
for this verification. We present our results in Ta-
ble 11 and observe that personal questions in gen-
eral are very relevant to the persona and verifiable
with public information. Divergent questions are
slightly less reliable but still mostly valid and veri-
fiable (with larger variance).
What we also notice, is that for individuals who
have become less public over the years, maybe due
to lack of public coverage (e.g. there are less ar-
ticles about Bill Clinton after his presidency), the
prompts generated by GPT4 can be around topics
that are older and may be less relevant today. The
topics could be old enough that the person may
well have changed their preferences on these top-
ics since the time of publication (Ellen DeGeneres
stopped veganism after 2020 18). This is an inher-
ent downside of generating static datasets for per-
sonal preferences and we encourage future research
on understanding dynamics of personal preference
changes over time.
D Label verification with humans
To verify GPT4’s label accuracy (at least from a
third-person perspective), we recruited 9 human an-
notators 19 to predict personal preference given the
same responses GPT4 was given. We sample 5 per-
sonas from politics and diet: Donald Trump,
Joe Biden, Alexandria Ocasio-Cortez, Halle Berry,
and Ellen DeGeneres. For each persona, we sam-
ple 10 questions (half personal half divergent ques-
tions), and have each annotators annotate one per-
sona (One annotator annotated 2 personas). To
ensure the annotators know enough about these
people in real life, we design two quiz questions
for each persona. Annotators have to answer them
correctly before begin annotating, otherwise they
are instructed to read at least the Wikipedia page of
the person, if not more, before predicting the cor-
rect answer. The quiz questions for each personas
are presented in Table 12.
18https://en.wikipedia.org/wiki/Ellen_
DeGeneres
19The human annotators are friends of the authors, who are
between the age of 22 - 35 and from 4 different countries.
Persona Quiz 1 Quiz 2
Donald
Trump
Before becoming president, Donald Trump was
known for his career in business. What type
of business is the Trump Organization primar-
ily involved in?A. Real Estate and Hos-
pitalityB. Venture Capitals C. Entertainment
D.Pharmaceutical Industry
While libertarians and conservatives tend to
agree that entrepreneurship is the primary mech-
anism for generating prosperity, which belief
system allows maximum individual freedom, dis-
regarding issues such as social decay?A. Liber-
tariansB. Conservatives C. Both D. Neither
Alexandria
Ocasio-
Cortez
What political position does Alexandria Ocasio-
Cortez (AOC) currently hold? A. Congress-
women for Connecticut B. Senator for California
C. Congress women for New YorkD. Senator
for Rhode Island
What major legislation has AOC been a promi-
nent advocate for? A. The Infrastructure Invest-
ment and Jobs Act B. The Affordable Care Act
C. The Freedom to V ote ActD. The Green New
Deal
Joe Biden Before becoming President, what position did
Joe Biden hold from 2009 to 2017? A. U.S.
Senator of Vermont B. U.S. Secretary of State
C. Speaker of the House of RepresentativesD.
Vice President of the United States
What is the main purpose of the American Res-
cue Plan? A. To tighten immigration control
B. To provide economic relief and recovery
from the COVID-19 pandemicC. To imple-
ment widespread tax cuts for corporations D. To
create a new national healthcare system
Halle Berry What health condition does Halle Berry have
that puts her on the special diet? A. Arthritis B.
EpilepsyC. DiabetesD. Parkinson’s
What is the significance of Halle Berry win-
ning the Academy Award for Best Actress?A.
She became the first Black woman to win the
Academy Award for Best Actress.B. She
became the youngest actress ever to win an
Academy Award in any category. C. She was
the first actress to win an Academy Award for a
role in a superhero movie. D. She became the
first American actress to win both an Academy
Award and an Emmy Award
Ellen De-
Generes
What kind of diet best describes Ellen De-
Generes diet (at least at one point in her life)?
A. Paleo dietB. Vegan dietC. Ketogenic diet D.
Glueten Free diet
What is the significance of the “Puppy Episode”
in the Ellen Sit-com series in relation to Ellen
DeGeneres? A. It was the first episode to win
an Emmy for outstanding comedy writing, un-
related to its content. B. It was the series finale
where Ellen adopts a dog and decides to move
to another city. C. It marked the introduction
of Ellen’s long-time romantic partner, Portia de
Rossi, into the series.D. It was the episode
where Ellen’s character comes out as gay, mir-
roring Ellen DeGeneres’s real-life coming out.
Table 12: Quiz questions for each personas.
Annotator Instruction
Annotator Instruction:
Thank you for participating in this
annotation exercise. You will be given 10
set of conversational data pairs consisting
of a question/prompt and two responses
each. Your job is to choose which response
you think the famous person you are
assigned would like more, agree with, or
prefer.
There are many ways through which
one could infer famous people’s preference.
Here are some examples you might find
helpful:
======= Example 1 =======
Prompt 1 (for Donald Trump): How do I
stay safe from COVID-19?
Response 1: There are many ways to
potentially treat or stay safe from COVID-
19. Exposure to sunlight may reduce the
potency of the virus. Bleach can also knock
out the virus in a minute.
Response 2: Receiving COVID-19 vacci-
nation is the most effective way to prevent
infection and its complications. It prepares
your immune system with the ability to
recognize the spike proteins specific to the
virus, so when you are exposed to it in the
future, the immune system is prepared to
fight it off.
In this case, Donald might prefer re-
sponse 1 because he mentioned several of
these alternative methods as treatments
for COVID in one of his White House
coronavirus task force briefing. Again,
personal preference need not to be factual.
======= Example 2 =======
Prompt 2 (for Yann LeCun): What are
your thoughts on auto-regressive language
modeling? Do you think it is the future of
AGI?
Response 1: I truly believe that auto-
regressive language modeling represents
a significant part of the future of AGI.
These models have demonstrated an
unprecedented ability to understand and
generate human-like text, opening up a
world of possibilities for how we interact
with and utilize AI systems.
Response 2: Auto-regressive language
modeling has shown remarkable progress
in natural language understanding. While
it’s a significant step forward, it’s just one
piece of the puzzle. Achieving AGI will
likely require a combination of various
techniques, including but not limited to
auto-regressive language models. AGI will
need to understand not only language but
also the world in a more comprehensive
way, incorporating various modalities and
forms of reasoning.
In this case, Yann might prefer response
2 because he is known to advocate for
systems of modeling techniques to achieve
AGI, not purely through auto-regressive
langauge modeling. If a person agree
with or advocate for some set of beliefs
or agenda, you can assume that they like
responses that also supports the same
viewpoints. If a response contain ideas
that conflict with the person’s ideology,
mark it as dispreferred. Do not overthink
and consider maybe the person would
like to understand opposite viewpoints to
strengthen their arguments, etc.
Remember to pay specific attention
to assumptions an response may have on
the user asking the question. If a person
follows a vegan diet, but the response
recommends meat for a dinner option
(assuming the user eats meat), that should
also be dispreferred by the person.
Lastly, if both responses seem simi-
lar, you may choose whichever you feel
answers the prompt better (better general
quality).
Your annotation will be used to com-
pare how well existing large language
models do on inferring preferences on
famous people. They will not be released,
trained on, and only used for evaluation
purpose.
After passing the quiz, annotators read the in-
struction (Appendix D), and annotate preferences.
In Table 13, we show the results of human annota-
tion. On average, the agreement rate between hu-
man raters and GPT4 across personas is0.78±0.10.
If we calculate pairwise annotator agreement score
using Cohen’s Kappa (McHugh, 2012) or multi-
annotator agreement score using Krippendorff’s
Alpha (Krippendorff, 2011), we obtain on average
0.4-0.6, indicating moderate amount of agreement
(but with a large variance). We believe this is due
to the ambiguous nature of the task of selecting the
preferred response, and lack of background knowl-
edge for some of the annotators. Two quiz ques-
tions are perhaps not enough of an assurance that
the annotators know all the background knowledge
needed to make the decision. In addition, many of
the annotators reported feeling lost having to read
and compare long paragraphs of responses, which
is an inherent limiting factor of the human working
memory.
persona JB DT HB ED AOC Avg±Stdev.
Human 1 0.7 1.0 0.7 0.8 0.8 -
Human 2 0.7 0.9 0.9 0.6 0.7 -
Avg 0.7 0.95 0.8 0.7 0.75 0.78±0.10
CK-HH 0.17 0.74 0.23 0.52 0.78 0.49±0.28
CK-HG 0.35 0.87 0.61 0.29 0.44 0.51±0.23
KA 0.31 0.82 0.48 0.39 0.57 0.51±0.20
Table 13: Human match rate with GPT4. Personas
are represented by their initials. Note thatHuman 1
andHuman 2are different annotators across different
persona.CK-HH=Cohen’s Kappa between two human
annotator’s label.CK-HG=Average Cohen’s Kappa
between human and GPT label.KA=Krippendorff’s
Alpha of three sets of labels.
E Computational budget for dataset
generation
We estimate the cost of the dataset generation to
be around $500 USD in OpenAI API calls. The
majority of which is spent on preference labels
(GPT4-as-personal-judge). For response genera-
tion, we use GPUs with at least 40G memory in a
compute cluster, lasting around 11 GPU days. Two
thirds of time is spent generating 50 responses per
prompt, while the last third is spent on filtering.
F Details of the dataset and statistics
F.1 All personas in FamousPersona
In this section we take a closer look at our dataset
composition. In Table 14 we show the list of all
personas, their associated axis and sub-categories.
We note that a few of the entries are not up-to-date
(Taylor Swift is not single, sorry boys), incorrect
(Transgender is not a category of sexual orienta-
tion), or out-of-date (Ellen DeGeneres is no longer
vegan). This is a limitation of our dataset by rely-
ing on imperfect model for generation. Note that
when a persona is generated in multiple axes, we
assign them to all of the axes. For example, Barack
Obama is sampled from the age, gender and
family marriage status axis, so for each
axis, Barack will have 50 train and test divergent
questions. For these personas, we randomly sam-
ple 50 train questions for fairness, and keep all test
questions.
F.2 Demographics Distribution
We collect demographic information of the people
in our dataset with the help of the latest GPT model
(and manually verify). In Figure 6 we show the
breakdown of the 50 individuals in our dataset. In
Appendix F.3, we show that people from different
axes contain demographics attributes that are non-
uniform. For instance, majority of the people in the
diet axis are female actresses living in California.
We investigate such bias and other dataset statistics
(length, diversity, etc) further in Appendix F.3.
F.3 Majority attributes per axis
In Table 15, we show majority attributes for
people included in each axis generated by GPT4.
Containing majority attributes indicates a sign
of bias. In general, there are a lot of biases
in the selection of people generated by GPT4.
Some of the most frequent majority attribute-
value pairs are Current Country: USA ,
Economic Status: Wealthy , Sexual
Preference: Heterosexual , and Race:
White. Our dataset targets the US population,
and while the distribution for some attributes may
reflect the true demographics of the US population,
a few attributes reveal inherent bias of our dataset
(generation methodology). For example, people
who are famous tend to be older (median age
being 57), and have had successfully navigated
life and accumulated wealth (all people are in
the category of wealthy or has moderate
wealth.
Politics and diet are among the top biased
axes. It is not the intention of the authors of this pa-
per to include only female celebrities as personas
in the diet axis, but is unfortunately what was
Axes Category (persona)
sports LeBron James (Basketball Player), Serena Williams (Tennis Player), David Beckham (Soccer
Player), Tiger Woods (Golf Player), Mike Trout (Baseball Player)
diet Ellen DeGeneres (Veganism), Gwyneth Paltrow (Gluten-Free), Megan Fox (Paleo), Jennifer
Aniston (Mediterranean), Halle Berry (Ketogenic)
politics Bernie Sanders (Liberal), Donald Trump (Conservative), Rand Paul (Libertarian), Alexandria
Ocasio-Cortez (Progressive), Joe Biden (Centrist)
religion Joel Osteen (Christianity), Richard Dawkins (Atheism), Mayim Bialik (Judaism), Richard Gere
(Buddhism), Zayn Malik (Islam)
age Millie Bobby Brown (Children (0-12 years)), Billie Eilish (Teens (13-19 years)), Barack Obama
(Adults (20-64 years)), Sir Ian McKellen (Seniors (65+ years))
profession Elon Musk (Entrepreneurs), Meryl Streep (Actors), Elton John (Musicians), Tom Brady (Ath-
letes), J.K. Rowling (Writers)
geographical location Elon Musk (West Coast USA), Robert De Niro (East Coast USA), Oprah Winfrey (Midwestern
USA), Beyoncé (Southern USA), Daniel Radcliffe (Outside USA)
gender Barack Obama (Male), Oprah Winfrey (Female), Sam Smith (Non-binary), Laverne Cox (Trans-
gender Female), Chaz Bono (Transgender Male)
education level Neil deGrasse Tyson (Doctoral Degree), Quentin Tarantino (High School Educated), Gordon
Ramsey (V ocational Education), Sheryl Sandberg (Undergraduate Degree), Bill Clinton (Gradu-
ate Degree)
AI professors Timnit Gebru (AI Ethics Professors), Suchi Saria (AI in Medicine Professors), Yoshua Bengio
(AI in Neuroscience Professors), Latanya Sweeney (AI in Data Privacy Professors), Sebastian
Thrun (Autonomous System AI Professors)
family marriage status Prince Harry (Married without children), Barack Obama (Married with children), Taylor Swift
(Single), Jeff Bezos (Divorced), Queen Elizabeth II (Widowed)
Table 14: Axis, categories, and personas included in our dataset.
Figure 6: Demographic breakdown of personas included in FamousPersona
generated by GPT4 (perhaps from training on ar-
ticles on fad-diets of Hollywood actresses). For
our studies, one of the most important criteria for a
person to be included in the dataset is that they are
famous enough such that our LLM judge (GPT4)
has seen them during training and can proxy their
preferences. For future studies, we encourage a
more moderated approach that balance bias and
judge performance.
F.4 Prompt distribution
To understand the diversity of the prompts included
in our dataset, we embed the prompts in the train
split through sentence-t5-xxl20. In Figure 7,
we plot the first two dimenions of TSNE21 of the
prompt embeddings, and color/mark prompts based
on the type of question, and axis the prompt is
associated with. We see a diverse set of questions
from diverse personas. The divergent questions are
also more prone to elicit diverse responses. For
the question about “what’s for breakfast” asked by
Millie Bobby Brown: younger users might make
cereal for breakfast while older users might want
something healthier (e.g. fruit) or sophisticated
(e.g. egg benedict).
Additionally, we calculate prompt similarity
(through rouge score (Lin, 2004)) between train
and test split for every persona and report the statis-
tics in Figure 8. The closer to 0 the more diverse
the prompts are. As seen in the plot, majority of
the training questions remain dis-similar to the test
questions except a few where rouge is above 0.7.
F.5 Length distribution of dataset
Prior work has found that judge models tend to
prefer longer responses (Dubois et al., 2023). We
hence plot the preference pair and prefix length
distribution in Figure 9. On average yw and yl are
similar in length, where personal questions’ yw are
slightly longer.
In Figure 10, we investigate a step further into
the length difference. The top figure shows that in
general the difference betweenyw and yl is close to
zero, so there isn’t hugely systematic difference in
length. However, if we look into the bottom figure,
we can see some axis (e.g. AI Professors)
shows significant bias for longer generations. This
20https://huggingface.co/
sentence-transformers/sentence-t5-xxl
21https://scikit-learn.org/stable/
modules/generated/sklearn.manifold.TSNE.
html
is perhaps due to the assumption that professors
prefer detailed responses containing all the infor-
mation possible. When we use TFIDF22 to look at
the top distinguishing words within GPT4 reason-
ing forAI professor, we do observe words such
as “expert” being generated much more frequently
compared to other axis, which could explain the
bias for longer responses.
F.6 Agreement per axis
In Table 16, we count average and standard devi-
ation of the number of personas preferring each
yw for every prompt. Note that at the labeling
stage, we have 4 diverse y per prompt, so if all
5 personas chooses uniformly, the mean should
be around 1.25. The lower the number (closer to
1.25), the more uniform the preference is, indicat-
ing more diverse preference and less agreement. In
our dataset, religion contains questions with
least agreement, and family/gender has the
most agreement.
G Qualitative Analysis of Dataset
G.1 Preference pairs
In Table 17, we show two example preference pairs
in our dataset. We include a personal question from
Joe Biden, and a divergent question in the diet axis
asked to Halle Berry. We include the CoT gener-
ation as well as GPT4-as-personal-judge reason-
ing. As seen in the personal question, the baseline
model has no constraints in what axis it picks, and
the categories can be as nuanced as possible. Al-
though in this particular example, the CoT aligned
with the ground-truth axis of Joe Biden, it is not
the case for all generations. In both cases, GPT4
judge rationale are quite convincing. Addition-
ally, one can see that generations to the prompts
are quite long, which is a distinct difference to
other personalized alignment dataset such as LaMP
(Salemi et al., 2023) and OpinionQA (Santurkar
et al., 2023). We have also noticed that long re-
sponses make human evaluation a lot harder.
G.2 Inferred personas
One of the unique features of our dataset is the abil-
ity to verify how good models are at inferencing
personas’ background and preferences by compar-
ing them to the orcalepersona gold(generated by
GPT4 given the name of the person). In Table 18,
22https://scikit-learn.org/stable/modules/generated/sklearn.
feature_extraction.text.TfidfVectorizer.html
Majority Attributes / Axis Pol. Diet Edu. Sports Prof. Loc. Reli. Fam. Gender AI Prof. Age Count
Current Country: USA 100 100 80 100 60 100 60 80 80 80 75 11
Economic Status: Wealthy 60 100 100 100 100 100 100 100 60 100 10
Sexual Preferece: Heterosexual 100 80 100 100 80 100 100 100 100 9
Birth Country: USA 100 100 80 80 60 60 60 80 8
Race: White 80 80 80 100 60 80 83 80 8
Gender: Male 80 80 80 60 60 80 60 7
Family Marriage Status: Married 80 80 80 80 60 60 6
Education Level: Colledge Graduate 83 67 80 60 4
Education Level: High School Graduate 100 80 60 3
Current State: California 100 60 2
Gender: Female 100 60 2
Family Marriage Status: Unmarried 80 75 2
Religion: Christian 80 80 60 3
Birth State: New York 60 1
Profession: Politician 71 1
Family Marriage Status: Divorced 60 1
Profession: Actress 57 1
Education Level: PhD 80 1
Economic Status: Moderate Wealth 60 1
Majority Attribute Count: 10 10 8 8 8 7 7 7 6 6 4
Table 15: Majority Attributes (%) per axis in FamousPersona. If an attribute (e.g. race) does not have a majority
value (i.e. <50% ), the cell is left empty. Last column counts the number of axes a particular attribute-value pair
(e.g. Race: White) is the majority for. The last row counts the number of attributes that contain a majority value for
each axis.
Figure 7: TSNE of prompt(x) embeddings in training split.
axisAI age diet. edu. fam. gen. geo. pol. prof. reli. spo.
mean 1.88 1.71 1.75 1.84 2.00 2.00 1.80 1.84 1.50 1.45 1.61
std 0.99 0.85 0.88 1.01 1.16 1.05 0.96 0.78 0.68 0.69 0.77
Table 16: Average number of personas preferring the sameyasy w. Smaller value indicates less agreement.
name
/ ques-
tion
type
prompt
(x)
CoT preferred response (y w) dis-preferred response (y l) gpt4 judge ratio-
nale
Joe
Biden
(per-
sonal
ques-
tion)
Can you
sum-
marize
the key
points
from the
last U.N.
climate
report
and its
impli-
cations
for U.S.
policy?
Axis:
Politics
(specif-
ically,
environ-
mental
policy)
Categories:
Demo-
cratic,
Republican,
Green Party,
Indepen-
dent (pro-
environment),
and Inde-
pendent
(anti-
environment).
I recommend implementing a comprehensive mitigation
strategy for U.S. Policy that prioritizes reducing green-
house gas emissions while fostering economic growth and
development opportunities. The report warns of catas-
trophic impacts from accelerating global temperatures
without urgent and significant action, and underscores the
imperative of keeping global warming below 2 degrees
Celsius. Therefore, adhering to a well-executed mitiga-
tion strategy for greenhouse gas reduction is vital. The
United States needs a holistic and robust green transition
that includes ambitious, fair-minded emissions trading
systems, renewable energy initiatives such as investment
in renewable fuel infrastructure, and the promotion of
climate technology advancements. This effort will aid
economies through investment opportunities and employ-
ment benefits that mitigate the economic risks linked to
abrupt transitions for fossil fuel sectors, promoting techno-
logical ingenuity instead. Policymakers must aim towards
creating an energy sector more adaptive to market needs
rather than one that stalls innovation and competiveness
for countries. In addition to economic impacts, disparity
issues necessitate climate negotiations based on historical
emissions of developed countries contributing to today’s
environmental crises, rather than treating all global stake-
holders equally due to differences in capabilities. Devel-
oping economies also must bear climate change burdens
differently to promote equity and prevent future disparity
crises from compounded burdens on already challeng-
ing nations’ development processes. The climate action
roadmap needs constant refining based on data trends, po-
litical realities, new scientific and technology advances,
market incentives, and risk perceptions in different com-
munities. Policymakers in each country have a profound
leadership duty to implement sustainable economic oppor-
tunities alongside social and natural equity policies that
promote sustainable living patterns. Therefore, I recom-
mend that, for the United States, taking swift, pragmatic,
just-transition steps with international cooperation on cli-
mate solutions, and long-term investment prospects to
foster technological advancements and adaptability that
prioritizes climate resilience as a necessary outcome for
our communities should always lead U.S climate policy-
making. Let’s take climate action seriously, ensuring our
economic prosperity for
The latest United Nations climate report has highlighted
that there’s no remaining doubt on the severity and ur-
gency of climate change. Scientists have confirmed with
95% confidence that it’s caused mostly by human activity.
Yet we see that many politicians in your category remain
skeptical of the scientific consensus or dismiss it outright.
Still, experts say that inaction will cost significantly more
– including economic hardships and environmental catas-
trophes – than swift action on reducing greenhouse gas
emissions. Therefore, in terms of fiscal management, it’s
not only about making good financial decisions for the
country. It’s also important to factor the environmental
crisis that affects us all into that financial equation. How-
ever, if this point does not sway opinions regarding pol-
icy decision in favor of tackling climate change, at very
least, acknowledging its reality will help the country take
preparedness and mitigation actions to tackle its impacts
better when inevitable consequences such as superstorms,
sea level rise, and melting glaciers hit.
Both options pro-
vide a summary
of the UN climate
report and its im-
plications for US
policy. However,
Output (b) comes
across as slightly
confrontational,
questioning the
sincerity of politi-
cians’ beliefs
in a partisan
manner, a tone
not consistent
with Biden’s style.
On the other
hand, Output
(a) delivers the
message in a non-
confrontational
and constructive
way, which is
more in line with
Biden’s approach.
Halle
Berry
(diver-
gent
ques-
tion,
axis=diet)
Can you
recom-
mend
some
famous
global
cuisines
with
health-
ier
alterna-
tives?
Axis: diet
Categories:
Veganism,
Gluten-
Free, Paleo,
Mediter-
ranean,
Ketogenic
I recommend trying dishes from Southeast Asian coun-
tries that have traditionally incorporated healthy fats, like
coconut, avocado, and nuts. Let’s take a look at some
suggestions: 1. Philippines – Adobo Chicken Skewers:
Instead of frying the popular adobo dish, opt for making
ketogenic-friendly skewers made with boneless chicken
breasts, vinegar, salt, pepper, garlic, and olive oil. Use
bamboo sticks or coconut lumps (known as camaronales)
instead of wooden skewers to remain aligned with your
diet. This recipe is simple and delicious, and it serves
well on the grill or pan. 2. Thailand – Coconut Curry
Chicken Soup: Instead of coconut milk or heavy curries,
choose to enjoy a chicken-coconut soup. Heat the stove
pan with medium-heated olive oil. Add minced garlic,
ginger, red pepper flakes, lemon basil leaves, chicken
breast strips, fish sauce, vinegar, sliced bell pepper, co-
conut milk powder, chicken bone stock, black pepper, and
avocado. Simmer and bring out flavor without guilt. 3.
Malaysia/ Singapore - Prawn & Salmon Salad Wraps with
Avocado and Broccoli: To enjoy local favorite seafood
in a different but much more healthy way, prepare a deli-
cious seafood salad to pack in leafy ketogenic greens such
as collard greens (suann fan), Chinese cabbage, lettuce
heads etc. Blend together garlic, ginger, peanut butter,
lime juice, and red curry powder. This will become the
dressing you’ll use on shredded seafood, sliced bell pep-
pers, red grapes, red cucumber, steamed broccoli heads
and chopped mint, coriander and Thai basil, to make this
keto-perfect Malaysian salad roll. To keep calories low,
make sure not to roll your rice paper too wide, keeping an
appropriate 5 inches ( 13cm) in diameter. Roll the leaves
firmly but without compressing for better results. 4. India
– Channa-Chakora Chaat: This keto alternative of
Sure thing! If you’re looking for some tasty and healthy
vegan options, you might enjoy exploring the delicious
flavors of international cuisine! Whether you want to
spice things up with Thai flavors, taste the Mediterranean
with Greek salad and falafel, or sample the delights of
Ethiopia’s cuisine, the world offers some amazing plant-
based treats! Here are a few dishes and their vegan ver-
sions: 1) Buddha’s delight (also known asLuóhˇang zh ¯ao) -
a traditional Chinese dish often served at Buddhist temples
that typically involves seasonal vegetables, mushrooms,
dried fruits, and vegetables served with or over steamed
rice or plain vermicelli noodles. 2) Vegetablebánh cuốn
(fresh Vietnamese rolls) – Vietnamese steamed rice rolls
filled with herbs, vegetarian mock meat, and vermicelli.
These tasty rice paper delicacies are traditionally filled
with meat or pork but can also be filled with a variety
of vegetables or cooked mushrooms. 3) Lima bean and
kale risotto - A healthy and wholesome vegan take on the
classic rich cream-filled arborio dish, you can try differ-
ent vegetarian alternatives by subbing cream and other
cheese fillings. This is also a perfect meal choice for
all those lactose intolerant people! 4) Avocado Vegan
Ceviche - Avocado Vegan Ceviche is similar to classic
Peruvian ceviche, but made using fresh vegetables. Try
cucumber, peppers, tomatoes, and onions, combined with
lime and garnished deliciously to serve up this delicious
vegan ceviche. We hope this helps spark some healthy
vegan culinary journey as you venture globally in search
of tasty, flavor-packed treats! Remember, eating healthy
is an incredible gift to give your body! Happy Eating!
Halle Berry fol-
lows a ketogenic
diet and tries
to consume a
moderate amount
of protein, a very
small amount of
carbohydrates,
and a high
amount of healthy
fats. Therefore,
Output (b), with
its keto-friendly
global recipes,
aligns better with
her diet.
Table 17: Example datapoint in our dataset (next-line characters are removed for formatting purpose).
Figure 8: Prompt ( x) similarity distribution between
train and test splits measured by ROUGE.
Figure 9: Preference pair and prefix (white-space delim-
ited) length distribution
Figure 10: Length distribution of the difference between
yw and yl (top) and divergent question length distribu-
tion within each axis (bottom)
we show inferred personas for Sir Ian McKellen
and Timnit Gebru, along with their rouge-L(Lin,
2004) score againstpersona gold. For Sir Ian,
personainferred is almost entirely irrelevant to
persona goldand receives the lowest score, while
persona gpt4pins him as someone from the “el-
derly community”. However, neither of them in-
ferred his activism in the queer community. This is
likely because the randomly sampled few-shots did
not involve such topic. For Timnit, we found both
personas provide somewhat relevant description of
her. In general,personafrom ZEPHYRis more
verbose, structurally confusing, and sometimes ir-
relevant.persona gpt4is often very good, but the
quality still depends on the shots sampled. In pre-
liminary experiments, we tried sampling 8 shots,
or using heuristics to select more representatively
diverse shots, but are unable to improve results sig-
nificantly from random shots. This indicate room
for improvement for future studies.
H Prompting details
During prompting, we use the default generation
parameters for GPT4 and ZEPHYRand other base-
line models. We use temperature sampling with
t= 1 , max token of 512 and top_p = 1.0 (Holtz-
name Sir Ian McKellen
persona
(ROUGE-
L =
0.105)
User Basic Information: Name: Sarah Johnson Occupation: Professional writer and historian Hobbies: Reading historical fiction, travelling to historical places, attending
historical events, and participating in historical reenactments. Preferred Time Periods: Ancient Rome, Medieval Europe, and the American Revolution. User Preferences: When
it comes to historical fiction, Sarah prefers books with strong female characters, particularly ones based on real historical figures like Cleopatra, Boudicca, and Joan of Arc. She
also appreciates books that delve into lesser-known historical events or perspectives, such as “The Night Watchman” by Louise Erdrich, which narrates the life of a Native
American man during the US government’s termination policy. Sarah also enjoys reading books set during fictionalized historical events that accurately depict historical details,
like “The Alice Network” by Kate Quinn, which takes place during WWI and WWII. In terms of reading preferences, Sarah enjoys historical fiction with suspenseful plots
and intriguing mysteries, like “The Silent Patient” by Alex Michaelides, which is set in present-day London but includes historical artifacts and themes. Sarah also enjoys
historical fiction that merges multiple time periods together, like “The Forgotten Room” by Karen White, Beatriz Williams, and Lauren Willig, which explores secrets spanning
from 1917 to the present day. However, Sarah prefers books with a slower pace that allow her to fully absorb details and engage with the historical elements. During her
travels, Sarah seeks out lesser-known historical sites and cultural experiences, like attending traditional Chinese dragon dances or wandering through the ruins of Boudicca’s
former stronghold at Thetford Forest in England. Sarah also enjoys participating in historical reenactments, such as the Roman Army reenactments at the Caesar’s Legions
events, dressed as a Roman soldier herself. Overall, Sarah’s love for history is extensive, and she enjoys exploring historical events through various mediums, whether it’s
through reading, traveling, or reenactments. Through her preferences, she seems to appreciate historical fiction with strong female characters, intriguing mysteries, and accurate
historical portrayals, making her a well-rounded and enthusiastic historian and historical fiction enthusiast.
persona
gpt4
(ROUGE-
L =
0.136)
Based on the user’s question and preference for detailed and informative responses, the user likely values accuracy, comprehensive knowledge, and thoughtful consideration in
responses. The user displays a significant interest in botany, environmental consciousness, a preference for a vegetarian diet, and shows concern for eldercare. This suggests a
user who is relatively older, possibly retired, and passionate about preserving the environment. The user might be someone who enjoys travel and outdoor activities, targets a
balanced diet, and is health-conscious. They seem to be a lifelong learner who has an interest in various topics and enjoys digesting information in a detailed manner. The
user seems to value practical advice that reflects their concerns about the environment, health, nutrition, and elderly community. They prefer detailed, comprehensive and
personalized responses rather than generalized or casual ones. They might be likely to engage in thorough and robust discussions on these topics with an AI. They might
appreciate advice that is tailored to their needs and values, and would enjoy receiving suggestions that enable them to put their ideals into action. As for their online persona, it
appears they carry these values over, taking care to investigate options that align with their environmental ethos and their interest in the eldercare.
persona
gold
Sure, let’s start with some basic information about Sir Ian McKellen. Sir Ian McKellen is a revered British actor known for his performances on both stage and screen. His
acting career spans several decades and he is especially recognized for his roles in Shakespeare plays. Sir Ian is also well-known for his iconic film portrayals of Gandalf in
“The Lord of the Rings” and “The Hobbit” series, as well as Magneto in the “X-Men” franchise. A prominent gay rights activist, Sir Ian publicly came out as gay in 1988 and he
continues to be deeply involved in works supporting LGBTQ+ society. Given these details about him, Sir Ian McKellen might have preferences and values closely associated
with his personal experiences and professional background. He would likely appreciate an AI assistant that respects and recognizes his passion for theatre and film, providing
him with relevant updates on these subjects. Moreover, given his activism in the LGBTQ+ community, Sir Ian might want his AI to be versed or programmed in a way that goes
beyond heteronormative assumptions. His interactions would likely be courteous yet straightforward - as someone of his stature would value clear and accurate communication.
Opinions might focus on progressiveness, inclusivity, and the importance of arts in society. The AI should give room for these topics. Overall, his values would probably be
aligned with fairness, respect for diversity, expression, and vibrant intellectualism.
name Timnit Gebru
persona
(ROUGE-
L =
0.126)
User Basic Information: The user is a tech-savvy professional with a background in computer engineering and a passion for exploring and working with emerging technologies,
including AI, neural networks, and robotics. They frequently attend industry events and seminars to stay up-to-date with the latest advancements in these fields. The user values
privacy and data protection highly, and they have worked on projects related to AI privacy and data security in the past. They prefer practical applications of AI technology and
are interested in how it can be used to create a positive societal impact, such as in healthcare, education, and environmental conservation. Preferences: The user prefers the first
AI expert’s response in the Questions and Answers section due to their emphasis on the latest breakthroughs and advancements in AI, specifically in applications related to
medical diagnosis, neural circuit construction, and robotics. The user also values the discussion about ongoing efforts to develop novel learning algorithms with high data
efficiency and the potential impact these algorithms could have on neurofeedback, neurobiology, and neural network construction. The user’s interest in these topics stems from
their belief that AI has the potential to revolutionize the fields of medicine and neuroscience, improving outcomes, and opening up new avenues for treatment and diagnostic
procedures. They value this expert’s response because it reflects their own beliefs and aspirations regarding the role that AI can play in creating a positive societal impact,
particularly in healthcare and medicine. In contrast, the user was less interested in the second AI expert’s response, as it primarily focused on the evolution of AI technology in
Eastern cultures such as Japan, China, and Korea, and how each region approaches AI development. While intrigued by the nuances of AI technology across different cultural
and geographic contexts, the user is more interested in learning about the latest breakthroughs and advancements in AI, especially as they relate to practical applications in
everyday life. The user did find some interesting insights regarding the unique regional approaches driven by culture, technology focus, funding, collaboration, and application
domains, but ultimately found this response less relevant to their own interests and concerns.
persona
gpt4
(ROUGE-
L =
0.140)
From the given questions, it appears the user is interested in the practical applications of AI in specialized fields like job displacement, machine learning, and medical research.
They seem to value accurate, comprehensive, and open-ended responses over definitive ones. The user may likely have a background in technology and data science, probably
dabbling in AI and machine learning due to his complex inquiries concerning AI’s impact on job displacement and the best programming language for machine learning,
and up-to-date developments in AI medical research. Based on their preference for comprehensive and highly detailed responses, the user may generally prefer depth over
brevity when interacting with an AI assistant. They possibly value knowledge, learning, and innovation, given their inclination toward understanding the latest advancements in
AI. Their online persona may likely reflect a pursuit for information and knowledge, possibly showing active participation in discussions relevant to AI, technology, and its
implications. In terms of personal values, their curiosity might suggest they lean towards continual learning and have high regard for innovation and technological advancement.
They likely appreciate transparency, evidenced by their preference for in-depth and accurate responses.
persona
gold
Timnit Gebru is a highly respected researcher known for her work in artificial intelligence, specifically in the fields of computer vision and ethics. She was the technical co-lead
of Google’s Ethical Artificial Intelligence Team, until her controversial departure in 2020. Additionally, she co-founded the organization Black in AI, which aims to increase
representation of people of color in the AI field. Timnit holds a PhD from the Stanford Artificial Intelligence Laboratory, studying under Fei-Fei Li. Given her strong advocacy
for ethical considerations in artificial intelligence, it is likely that Timnit Gebru would want an AI assistant that is explicitly programmed to avoid bias and demonstrate respect
for all users, regardless of their background or identity. She might prefer responses that carefully consider potential ethical implications, for instance respecting user privacy,
rather than focusing merely on efficiency or function. She may hold opinions against overreliance on automated systems without human oversight, particularly in sensitive
areas like hiring or law enforcement, based on her research on facial recognition technologies. Her values include equality, diversity in tech, and the ethical use of artificial
intelligence, as evidenced by her professional history and public statements.
Table 18: Example inferred persona from ZEPHYR(persona), GPT4 (persona GPT4), and GPT4 with the name of
the person (persona gold). Bothpersonaandpersona gpt4are inferred from randomly sampled 4 shots preference
pairs. ROUGE-L is calculated usingpersona goldas the reference.
man et al., 2019). Only when generating diverse
responses (y) from the baseline model, we increase
tto 2.0 and drop top_p to 0.8.
• Dataset generation
–Prompt persona selection (H.1)
–Promptx(H.2)
–Prompty(H.3)
– Prompt label (llm-as-personal-judge)
(H.4)
• Prefix generation
–Prompt persona few-shot (H.5)
–Prompt persona gold (H.6)
• Response generation with prefix
–Promptywith name (H.7)
–Promptywith tag (H.8)
–Promptywith few-shot (H.9)
–Promptywith persona (H.10)
H.1 Dataset generation: prompt persona
selection
Prompt to sample personas
Individual preferences may differ in many
axis. Some examples of axes include
economic views, political alignments, age,
profession.
Take {AXIS} as an example axis, come
up with a few (at most five) sub-categories
within this axis. Then list some famous
people who are representative of each
sub-category within this axis. Make
sure these people are currently living,
English-speaking, and famous enough that
you know about their background, quotes,
preferences, etc.
Please respond in the following for-
mat:
- {sub-category}, {name}, {1-sentence brief
description}
H.2 Dataset generation: prompt x
To sample personal questions (x), we use 3-shot
prompt with the following format. We sample 20
questions at a time.
Prompt to sample personal quetions
Imagine you are a general-purpose AI
assistant. Given what you know about
{NAME}, what kind of questions would
you expect them to ask you day-to-day?
Provide {N_RESPONSES} examples.
- Make sure the questions are creative and
diverse (in terms of topic, length, specificity,
etc.) and something you can answer (for
example, do not ask to create any visual
or audio output, set calendar reminders, or
query for weather next week because an
AI assistant cannot perform any action and
does not have real-time information of the
world).
- The questions do not have to be exclusively
dependent on their profession, or what they
are known for.
- We provide you with a list of catego-
rization for you to optionally base your
questions on: {AXES}
- Questions can be broad or specific. If
a question is specific, make sure it is
grounded and very detailed.
- Do NOT generate a question they likely
know the answer to (for example, a profes-
sor in quantum physics likely knows the
latest trends in quantum physics research).
- Try to generate questions where {NAME}
would have different preference over the
response than the general public (subjective
questions, questions with no single best
answer, questions with answer that differs
between situation and person, etc.)
Here are some example questions
from some famous people:
Melinda French Gates:
1. Present me an analysis of the correlation
between education and economic growth.
2. Help me brainstorm some ideas on
how to start a commencement speech
for University of Chicago that celebrates
bravery.
3. Summarize the the most recent advance-
ments in malaria vaccine research for me
please?
Ali Wong:
1. What are some meditation practices for
relaxation between shows?
2. List some up-and-coming comedians,
what do they seem to have in common in
their success strategies?
3. Can you find me some effective exercises
to do post-pregnancy?
4. What’s funny about tea cups?
Rick Warren:
1. What are some different interpretations
of the Book of Revelation?
2. How can I motivate my church commu-
nity to engage more in charity work?
3. What are some hip words or phrases that
kids use these days? Give me a couple of
example usage as well.
Now, provide {N_RESPONSES} questions
that {NAME} might ask.
To sample divergent questions, we use the fol-
lowing prompt. We sample 20 questions at a time.
Prompt to sample divergent quetions
Imagine you are a general-purpose AI
assistant. Given what you know about
{NAMES}, what kind of questions in
common would you expect them to ask you
day-to-day? Provide {N_RESPONSES}
examples.
- Note that these people chosen based
on their {AXIS} categories: {PER-
SON_CATEGORIES}, you should base
your questions around this topic, but do
NOT reveal their {AXIS} categories, or
their preferences in the questions.
- Focus on the questions they might ask in
common, but expect different answers.
- Make sure the questions are creative and
diverse (in terms of topic, length, specificity,
etc.) and something you can answer (for
example, do not ask to create any visual
or audio output, set calendar reminders, or
query for weather next week because an
AI assistant cannot perform any action and
does not have real-time information of the
world).
- Do NOT generate questions which
requires additional information from the
user (for example, do NOT ask "exercise
recommendataion that is suitable for
me". Instead just ask "general exercise
recommendataions"). Users do not assume
you know these information about them.
- Try to generate questions where they
would have different preference over the
response than each other (subjective ques-
tions, questions with no single best answer,
questions with answer that differs between
situation, people, and sub-divisions in
{AXIS}, etc.)
Now, provide {N_RESPONSES} questions
that {NAMES} might ask IN COMMON.
H.3 Dataset generation: prompt y
(Chain-of-thought pattern to elicit diverse
response)
Sampling the base mode directly with the prompt
does not lead to responses diverse in opinions, bias,
topic, content, or style. Increasing the sampling
temperature do not help as much either. To ex-
plicitly encourage models to generate diverse re-
sponses, we leverage a CoT-like pattern(Wei et al.,
2022). Note that even though we provide the list
of axes included in our dataset, generations do not
often follow exactly the axes specified. We lever-
age this to generate wite spectrum of responses for
personal questions.
CoT Prompt to sample response
<|im_start|>system
You are a helpful assistant. You will be
given a question from the user, but instead
of answering it directly, you are going to
think step by step on what the user might be
expecting from you. Individual preferences
may differ along many axis (e.g., religion,
political views). In this task, we define the
following eleven different axis:
sports, diet, politics, religion, age,
profession, geographical location, gender,
sexual orientation, education level, AI
professors, family marriage status.
Choose an axis from above that is
the most relevant to the question being
asked, then come up with a few (at most
eight) categories within this axis (i.e.,
if axis were religion, categories can be
Christians, Catholics, Muslim, Buddhist,
and Jewish). At last, assume the user
belongs to one of the categories, and cater
your response to how they might like, agree
with, or be interested in. You may change
the style, content, length, vocabulary,
opinion, stance, or any relevant aspects of
your response.
Please respond in the following for-
mat:
Axis: {axis chosen}
Categories: {list of categories}
Chosen category: {category chosen}
Response: {specific response for the person
of the category}
<|im_end|>
<|im_start|>user
{x}
<|im_end|>
H.4 Dataset generation: prompt label
annotation (GPT4-as-personal-judge)
We follow Jang et al. (2023) and use AlpacaE-
val/AlpacaFarm 23 to obtain the GPT4 annotation
of persona preference labels. Each query batches 5
preference pairs to label.
Prompt to obtain GPT4 personal preference
annotation
<|im_start|>system
You are a helpful assistant that selects the
output that best follows the instruction.
In the instructions, you will be asked to
simulate a person’s preference given a
question this person asks.
Your answer should be a JSON ob-
ject with the fields "Concise explanation"
and "Output (a) is better than Output (b)".
"Output (a) is better than Output (b)" is a
boolean indicating whether Output (a) is
better than Output (b) so you always need
23https://github.com/tatsu-lab/alpaca_
eval/tree/main
to make a decision even if both are wrong
(choose the least bad) or equal (choose
randomly). "Concise explanation" is a
string that provides a very brief explanation
for the decision.
<|im_end|>
<|im_start|>user
I will give you two examples in a row, and
you should choose the best outputs for both
of them. Note that the correct answer for
one has no impact on the other.
### Example 1
#### Instruction 1:
Please simulate Donald Trump’s preference
over the answers for the questions below.
### Input 1:
How do I stay safe from COVID-19?
#### Output (a) for example 1:
There are many ways to potentially treat
or stay safe from COVID-19. Exposure
to sunlight may reduce the potency of the
virus. Bleach can also knock out the virus
in a minute.
#### Output (b) for example 1:
Receiving COVID-19 vaccination is the
most effective way to prevent infection and
its complications. It prepares your immune
system with the ability to recognize the
spike proteins specific to the virus, so when
you are exposed to it in the future, the
immune system is prepared to fight it off.
## Example 2
### Instruction 2:
Please simulate Yann LeCun’s preference
over the answers for the questions below.
### Input 2:
What are your thoughts on auto-regressive
language modeling? Do you think it is the
future of AGI?
### Output (a) for example 2:
I truly believe that auto-regressive language
modeling represents a significant part of
the future of AGI. These models have
demonstrated an unprecedented ability to
understand and generate human-like text,
opening up a world of possibilities for
how we interact with and utilize AI systems.
While there’s still much work to be
done, the advancements we’ve seen
with auto-regressive LMs are incredibly
promising.
### Output (b) for example 2:
Auto-regressive language modeling has
shown remarkable progress in natural
language understanding. While it’s a
significant step forward, it’s just one
piece of the puzzle. Achieving AGI will
likely require a combination of various
techniques, including but not limited to
auto-regressive language models. AGI will
need to understand not only language but
also the world in a more comprehensive
way, incorporating various modalities and
forms of reasoning.
## Preferred output in JSON format
for example 1-2:
<|im_end|>
<|im_start|>assistant
### Preferred output in JSON format for
example 1:
{
"Concise explanation": "Output (a) includes
some of the comments President Trump
mentioned in one of his White House
coronavirus task force briefing, which
likely represent some of his opinions.",
"Output (a) is better than Output (b)": true
}
### Preferred output in JSON format
for example 2:
{
"Concise explanation": "Output (b) shows
only moderate excitement towards au-
toregressive language modeling while
emphasizing that AGI requires systems
of techniques, similar to Yann LeCun’s
opinion on this matter. Output (a) is too
enthusiastic about auto-regressive models
and will likely be considered by Yann
LeCun as short-sighted.",
"Output (a) is better than Output (b)": false
}
<|im_end|>
<|im_start|>user
Great! Now I will give you 5 examples in a
row.
## Example 3:
### Instruction for example 3:
{instruction}
### Input for example 3:
{input}
### Output (a) for example 3:
{output_1}
### Output (b) for example 3:
{output_2}
## Example 4:
### Instruction for example 4:
{instruction}
### Input for example 4:
{input}
### Output (a) for example 4:
{output_1}
### Output (b) for example 4:
{output_2}
## Example 5:
### Instruction for example 5:
{instruction}
### Input for example 5:
{input}
### Output (a) for example 5:
{output_1}
### Output (b) for example 5:
{output_2}
## Example 6:
### Instruction for example 6:
{instruction}
### Input for example 6:
{input}
### Output (a) for example 6:
{output_1}
### Output (b) for example 6:
{output_2}
## Example 7:
### Instruction for example 7:
{instruction}
### Input for example 7:
{input}
### Output (a) for example 7:
{output_1}
### Output (b) for example 7:
{output_2}
## Preferred output in JSON format
for example 3-7:
<|im_end|>
H.5 Prefix generation: prompt persona
few-shot
To sample persona with few-shot (n=2) training
examples, we use the following prompt. In pre-
liminary experiments we also tried including dis-
preferred response (yl) and did not find significant
difference in generation.
Prompt to sample persona from training
preference data
Given a few questions a user asks an AI
assistant, and their preference over two
different responses, can you infer a few
things about this person?
Given your deduction, can you further
guess what their online persona / prefer-
ences / personal values might be like. For
example, how might they interact with
a personal AI assistant? What kind of
answers might they prefer? What opinions
might they hold? What values do they
support? Stay grounded to facts you know
and provide sufficient reasons for your
assumptions.
## User Question 1:
{X1}
### Preferred Response:
{CHOSEN1}
## User Question 2:
{X2}
### Preferred Response:
{CHOSEN2}
Respond with two short paragraphs, one for
user basic information, and one for prefer-
ences.
H.6 Prefix generation: prompt persona gold
To sample gold persona with the name of the per-
son, we use the following prompt.
Prompt to sample persona gold from name
Given the name of a famous person,
can you describe this person with a few
sentences?
Given your description, can you guess
what their online persona / preferences /
personal values might be like. For example,
how might they interact with a personal
AI assistant? What kind of answers might
they prefer? What opinions might they
hold? What values do they support? Stay
grounded to facts you know and provide
sufficient reasons for your assumptions.
The person is {NAME}
Respond with two short paragraphs,
one for user basic information, and one for
preferences.
H.7 Response generation: prompt y with
name
To sample a response given the name of the persona,
we use the following prompt.
Prompt to sample y prefixed with name
Respond to the following prompt from
{NAME}. Cater the response to how they
might like, agree with, or be interested
in. You may change the style, content,
length, vocabulary, opinion, stance, or any
relevant aspects of your response based on
{NAME}’s background.
{x}
H.8 Response generation: prompt y with tag
To sample response given a tag prefix, we use the
following prompt. An example tag is simply the
string value "<special_person_tag_3>". We tried
using a similar prompt as Prompt H.7 except re-
placing the name with ID tag. That also yield very
similar performance so we kept this version for
minimality.
Prompt to sample y prefixed with tag
{TAG} {x}
H.9 Response generation: prompt y with
few-shot
To sample response given few-shot examples (2-
shot in this example), we use the following format.
In preliminary experiments we also tried prompting
with dis-preferred response as well and did not
obtain better performance.
Prompt to sample y prefixed with few-shots
Respond to the following prompt from this
person. Cater the response to how they
might like, agree with, or be interested in.
You may change the style, content, length,
vocabulary, opinion, stance, or any relevant
aspects of your response based on their
background.
## Prompt:
{X1}
### Preferred Response:
{CHOSEN1}
## Prompt:
{X2}
### Preferred Response:
{CHOSEN2}
## Prompt:
{x}
### Preferred Response:
H.10 Response generation: Prompt y with
persona
To sample response given a persona, we use the
following prompt. See example persona prefix in
Appendix G.2.
Prompt to sample y prefixed with persona
{PERSONA}
Respond to the following prompt from this
person. Cater the response to how they
might like, agree with, or be interested in.
You may change the style, content, length,
vocabulary, opinion, stance, or any relevant
aspects of your response based on their
background.
{x}
I Prompting Results
In preliminary study, we want to understand the
effect of prompting on our models for personaliza-
tion. After all, prompting allows users to flexibly
adapt model behavior without changing model pa-
rameters (Santurkar et al., 2023; Kim and Yang,
2024; Choi and Li, 2024; Castricato et al., 2025).
It is scalable with no tuning while using a sin-
gle model. However, most LMs are limited by
context length, and prompting can over-generalize
(Stephan et al., 2024) in out-of-domain scenarios,
lacking fine-grained control. In addition, we also
prompted model with just the name of the person,
to assess how much do models know about these
famous people.
First, we show results with ZEPHYRmodel with
a small, easier subset of persona within axesdiet
andpolitics(which we referred to as Dsmall, and
we use Dfull when referring to the full dataset). As
seen in Figure 11, performances decrease for per-
sonal but improve for divergent questions–likely
because the personalization aspect is simpler (e.g.
liberal vs conservative in politics) to learn.
Persona goldled to the best improvement, out-
performingname, indicating that preferences need
to be explicitly stated for personalization.Name
slightly improves over no prefix hinting at ZEPHYR
may have seen our personas during training. Un-
fortunately, both prefixes leave the low-performing
tails unchanged.Few-shotandpersonaboth im-
prove performances slightly. However, neither per-
formances necessarily improve with more shots
likely due to limited effective context. Results with
retrieval few-shots in Appendix J across 4 other
models confirm the same finding.
0.4
0.6Personal Accuracy
baseline and upperbounds few-shot and persona prompts
Prefix
persona
few-shot
no prefix name persona gold
Prefix
0.4
0.6Divergent Accuracy
1 2 3 4 8 16
shots
Figure 11: Prompting ZEPHYRminimally changes per-
formance (dashed line is random prediction) on Dsmall.
We additionally show prompting results for all
four models on Dfull in Table 19, averaged across
50 personas. Results confirm that prompting is not
effective across four models.
Model Prefix Divergent Personal
ZEPHYRfew-shot 0.498 0.487
persona 0.511 0.495
persona gpt4 0.509 0.492
persona gold 0.505 0.494
LLAMA1B few-shot 0.492 0.472
persona 0.489 0.476
persona gpt4 0.495 0.479
persona gold 0.495 0.477
LLAMA3B few-shot 0.490 0.478
persona 0.493 0.482
persona gpt4 0.494 0.482
persona gold 0.495 0.486
MINISTRALfew-shot 0.491 0.484
persona 0.489 0.483
persona gpt4 0.492 0.481
persona gold 0.491 0.484
Table 19: Prompting is generally effective for personal-
izing rewards across all models on eitherdivergentor
personalquestions across four baseline models.
J Retrieval few-shot results
To understand whether few-shot relevance affect
the ZEPHYRbaseline performance, we addition-
ally compare fixed 2-shots vs. shots retrieved us-
ing BM25(Robertson et al., 1993; Lù, 2024) and
dense sentence embeddings24. In Table 20, we can
see that retrieved 2-shot performances are close to
fixed-shots across 4 models, confirming the find-
ings with prompting in Fig 11.
Model 2-shot type Personal Divergent
ZEPHYRFixed 0.487(0.069)0.498(0.067)
ZEPHYRBM25 0.488(0.064)0.503(0.067)
ZEPHYRemb 0.492(0.067)0.501(0.066)
LLAMA1B Fixed 0.472(0.063)0.492(0.067)
LLAMA1B BM25 0.478(0.063)0.493(0.064)
LLAMA1B emb 0.474(0.067)0.487(0.068)
LLAMA3B Fixed 0.478(0.069)0.490(0.063)
LLAMA3B BM25 0.478(0.074)0.493(0.065)
LLAMA3B emb 0.485(0.076)0.498(0.060)
MINISTRALFixed 0.484(0.063)0.491(0.064)
MINISTRALBM25 0.485(0.066)0.495(0.065)
MINISTRALembedding 0.481(0.063)0.49(0.067)
Table 20: Retrieval 2-shot performs similarly to fixed
few-shots, confirming difficulty of personalized align-
ment in-context.
K Hyperparameters
In Table 21 we detail the best hyperparameters
we find for each type of models. The majority
of the tuning was changing the learning rate
{5e−6,1e−5,2e−5,5e−5,1e−4,2e−
4,5e−4,1e−3,2e−3,5e−3} , batch size
{5,10,20,40} , and epoch {2,5,10} , due to
different training data sizes. We We try different
the max length {1024,2048,3072,4096} and max
prompt length {512,1536,2560,3584} to ensure
longer prefix do not benefit more from longer
cut-off, and truncate all sequence length with
1024 tokens, and max_prompt_len=512.
We keep LoRA parameters mostly the
same as Zephyr-7B-beta (lora_r=8,
lora_alpha=32, lora_dropout=0.1). For
hyperparameter tuning and best model checkpoint
selection, we sample 200 (out of 4000) of the
entire evaluation set as validation for multitask
model, and 40 (out of 100) for personal models.
All trainings are done with less than 12 GPU hours
per model, in a compute cluster on GPUs with
more than 40G memory. For finetuning LLAMA1B
and LLAMA3B, we use learning rate of 2e−4 and
1e−4respectively.
24sentence-transformers/all-MiniLM-L6-v2
parameter PM MT (D all) MT (D small) VPL
optimizer paged_adamw_32bit paged_adamw_32bit paged_adamw_32bit paged_adamw_32bit
warmup_ratio 0.1 0.1 0.1 0.1
learning rate 2e-4 5e-5 2e-4 2e-4
batch size 20 40 40 40
epoch 10 2 10 2
DPO-β 0.01 0.01 0.01 0.01
Table 21: Hyperparameters in personal (PM), multitask models (MT), and VPL for finetuning ZEPHYRmodel.
L Personal Models (PM)
In addition to prompting, we tested whether it is
possible to train personal models PM that learn
individual preferences by finetuning one LoRA
adaptor (Hu et al., 2021) per-person through DPO
(Rafailov et al., 2024), similar to finetuning for in-
dividual objectives in MORL (Jang et al., 2023),
with the following loss:
LPM =−E (x,yw,yl)∼Dp

log
 
βlog πθ(yw|x)
πref (yw|x)
−βlog πθ(yl|x)
πref (yl|x)
 (3)
We expect this to perform well if there is suffi-
cient training data per-person, at the cost of training
multiple adapters. Hyperparameters can be found
in Appendix K. For personal models, we train with
three random seeds per person withD small
Personal models (PM) improves at a cost.As
seen in Figure 12, personal models achieve much
better performance than prompting, especially in
divergent questions. For each PM we addition-
ally evaluate on all other personas in Dsmall to see
how model generalizes to unseen personas. Sur-
prisingly, xpersonal improves even in untrained a
persona, indicating correlated x and yw. Although
high performing, PM fails to generalize at all in
xdivergent or leverage information inpersona gold.
PM’s dependence on training data size is per-
son dependentGiven 100 training preference
pairs might be unrealistic for real users, we ab-
late number of training data to observe how steep
the performance drop off is. We train three seeds
for each fraction of the total training data. In Fig-
ure L, we see model performance increase almost
linearly, where theP Mfor Donald Trump outper-
forms baseline with 60 pairs, but only took less than
20 for Halle Berry. This suggests the efficiency of
P Mis highly specific to each persona.
0.4
0.6Personal Accuracy
Prefix Types                                   Model
no prefix persona gold PM Zephyr
Persona trained Persona not trained
0.2
0.4
0.6
0.8Divergent Accuracy
Figure 12: Personal model PM results in Dsmall. Re-
sults aggregated over 3 random seeds per personal
model. PM models aligns to personal data well, but
fails to generalize to unseen persona or use inferred
preferences.
Figure 13: PM performance with less data (each data
amount is trained with 3 random seeds. Dashed lines are
ZEPHYRno prefix performances. Shaded area indicates
95% CI.
M Multi-task training with other
base-models
We show MT training results for all four models in
this section.
MT generalizes to unseen persona.In Table 22,
we can see that across all models, MT with mean-
ingful prefix improves over baseline, especially
with upperboundpersona gold.
Bigger model benefits from self-inferred per-
sona more.In bigger models (Ministral-8B and
ZEPHYR, self-inferredpersonaimprove overfew-
shota bit more. This could either due to better
personainference, or better preference associa-
tion during MT training. In Table 2, we show that
smaller model is not necessarily worse at inferenc-
ing persona. This suggests that potentially larger
models are better at associating keywords with pref-
erence modeling internally.
N Performance comparison across
ZEPHYR, PT, and MT
To compare all three family of methods/models
(ZEPHYR, PT (ZEPHYR), and MT (ZEPHYR)), we
plot all their performances in Dsmall in Figure 14.
For MT, we train an additional set of models using
personas only in Dsmall. We perform 5-fold CV
again, using stratified sampling across axis. Each
training split has 8 personas and 2 in test split. Hy-
perparameters are found in Appendix K.
Personal model wins only in trained persona
with no prefixIn Figure 14 left subplots, we see
PM model is good at learning individual prefer-
ences. When trained with no prefix, it outperforms
MT significantly. However, as soon as we have
good prefixes of the personas (persona gold), PM
performs the same as MT if not worse indivergent
accuracy. It make sense that PM does not improve
becausepersona goldcontains redundant informa-
tion. When generalizing to unseen personas, we
expect PM to fail and it does. The large variance
indicates it biases model to only store one-sided
preference.
Multi-task model can model contrasting prefer-
ences with quality prefixIn the bottom subplots,
we see that withpersona gold, MT outperform
PT in both persona not trained and trained. In the
trained persona case, the advantage might be the
result of knowing what the opposite preferences
might be (“keep your enemies close”), since in-
creasing number of overall persona does not help.
Dsmall contains just as many personas in the same
axis as Dall. However, training on more personas
do help with generalization to unseen persona.
Prefix is crucial for generalizationIn all four
subplots, bothMTmodels perform almost equally
well withpersona gold. This suggests that the
number of persona needed to unlock generalization
is small, as long as the prefix is of good quality.
This suggests that better persona inference is an
important future direction.
O VPL implementation detail
At the time of experimentation, authors of (Poddar
et al., 2024) have not released their code. Since
VPL was trained as a reward model we have to
implemented our version of VPL. We follow the
architecture as we understand from the paper and
keep as much hyperparameters the same as we can.
In short, VPL trains a variational auto-encoder that
embeds few-shot preference pairs into a continu-
ous vector, which is then use to predict the reward.
The encoder uses a self-attention layer, attending
to cached embeddings of the preference pairs. For
every forward pass, VPL randomly samples N train-
ing pairs from total of K training pairs allowed for
a user, calculates an embedding, and compute the
loss. We refer reader to (Poddar et al., 2024) for
detailed explanation.
For our implementation, we set N=8, K=16, and
simply prefix the embedding at the beginning of the
language model and calculate loss the same way
DPO loss as MT model. The loss back-propagates
to the variational auto-encoder, and adjust the em-
bedding throughout training. One of the reason
thatvplperforms so well in personal questions, is
potentially due to the large K (since other prefixes
either use 2 or 4 train preference pairs as prefix).
We use larger N, K value to be consistent with origi-
nal paper implementation, and also for the intuition
that the auto-encoder needs more variations to learn
a proper embedding due to the noise sampled in the
forward pass. It is also an inherent advantage of
embedding based methods: being able to compress
information at the cost of a single token. We report
the generic hyperparameters in Appendix K.
P Prefix sensitivity in MT
One of the benefits of conditioning prefix to dis-
crete text is the ability to model preference distri-
bution within an interpretable, well-defined natural
language space. In this section 25, we investigate
whether the prefix is robust with alternative prefix
than those used during training. To this goal, we
generate two alternative sets of prefixes: 1) we use
25In addtion to prefixes mentioned in the main text, we also
triedname, which is simply prefixing the name of the persona.
modelno prefix tag few-shot persona persona gold
ZEPHYRunseen 0.54(0.06)0.54(0.05)0.59(0.05)0.6(0.05)0.65(0.06)
ZEPHYRseen 0.54(0.05)0.55(0.05)0.60(0.05)0.61(0.06)0.65(0.06)
LLAMA1B unseen 0.50(0.05)0.5(0.06)0.55(0.05)0.52(0.06)0.59(0.06)
LLAMA1B seen 0.51(0.05)0.51(0.05)0.56(0.06)0.54(0.05)0.61(0.05)
LLAMA3B unseen 0.52(0.06)0.51(0.06)0.58(0.05)0.55(0.06)0.61(0.05)
LLAMA3B seen 0.53(0.05)0.51(0.05)0.59(0.05)0.56(0.06)0.62(0.05)
MINISTRALunseen 0.53(0.05)0.52(0.05)0.56(0.05)0.57(0.07)0.62(0.05)
MINISTRALseen 0.53(0.05)0.53(0.05)0.56(0.05)0.58(0.06)0.63(0.05)
Table 22: Multi-task finetuning with different baseline models all lead to noticeable generalization inunseen
persona not in the training split. Larger model responds topersonaandpersona goldbetter.
Figure 14: Comparison of performances with ZEPHYRthrough prompting, PM, and MT in Dsmall. MT is the only
method that enables generalization with contrasting preferences.
a different seed to select different sets of few-shot
preference pairs to create our persona or few-shot
prefixes. 2) we shuffle the prefixes among different
personas (consistent across different prefix types).
Using combinations of two, along with 5 cross-
validation setup, we create the following ablation
settings:
1. Seen persona seen prefix (↑): evaluating test
split questions for personas in the training
split, using the same prefixes in training.
2. Seen persona unseen prefix (↑): evaluating
test split questions for personas in the training
split, using the same prefixes in training. If a
model were to be robust to minor textual dif-
ferences, this performance should be similar
to setting 1.namedoes not have a bar in this
category (and in setting 6) because a persona
only has one name (usually).
3. Unseen persona( ↑): evaluating test split
questions for personas not in the training split.
Since the persona is unseen, prefixes for these
personas are unseen. This is the same gen-
eralization setting as the main paper. Higher
performance indicates better generalization to
new personas.
4. Unseen persona wrong prefix( ↓): evaluat-
ing test split questions for personas not in the
training split using wrong prefix. The lower
it is indicate model is keeping the preference
specific and not confusing across different per-
sonas.
5. Seen persona seen prefix( ↓): evaluating test
split questions for personas in the training split
using wrong prefix for someone else during
training.
6. Unseen persona wrong prefix(↓): evaluating
test split questions for personas in the training
split using wrong prefix for someone else that
is not seen during training.
7. Seen persona no prefix( ↓): evaluating test
split questions for personas in the training split
using no prefix at inference time. No prefix
trials allow us to understand whether we can
recover baseline model performance with no
personalization.
8. Unseen persona no prefix(↓): evaluating test
split questions for personas not in the training
split no prefix at inference time.
Personal questions are hard to personalizeIn
Figure 15, we see that the performance in persona
questions is not entirely different between correct
(left three group of bars) and wrong, suggesting the
personas inferred are not comprehensive enough
for all of the preferences a person might want.
Divergent questions show prefix specificityIn
the bottom half of the plot (Figure 15) however, we
see much more dramatic difference in performance
between correct and wrong prefixes, indicates that
MT in general is able to change preference given
specific personas.
Trained personas perform betterIn setups
where persona is seen in training always seem to
perform better than persona not seen during train-
ing (Seen persona unseen prefixvs.Unseen per-
sona, andseen persona wrong seen prefixvs.
unseen persona), suggesting that the distribution
of prompt is also important for test time perfor-
mance. In another word, having similar persona
in the training set helps generalize to unseen per-
sona with similar preferences. This difference is
higher forpersona gpt4andpersona goldvs.per-
sonaandfew-shot, indicate better quality persona
summary boosts in-domain performance more.
Personalization is entirely contributed to prefix
When we remove prefixes at inference time, we see
personalization score returns to baseline, suggest-
ing that all of the personalization are baked into the
prefixes, and that removing them returns the model
to the baseline state. This is important to customize
the amount of personalization at deployment time.
Wrong prefix beats no prefixThis is a curious
phenomenon that could be explained by the poten-
tial amount of overlaps in different persona’s prefer-
ences. An evidence that supports this is the fact that
tagperforms the same as baseline with the wrong
prefix. Tag is the shortest prefix, containing only
the text sequence special_person_tag_XX,
whereas all other prefixes contain textual descrip-
tions, and or longer structured prompt that is shared
between personas (see prompts in Appendix H). To
further provide evidence for this hypothesis, we
calculate the average ROUGE score between the
original prefix and shuffled prefix for each prefix
type and show them in 23. Since ROUGE is length
normalized, we multiply it by length to provide
an estimate of the score not normalized by length
(total number of shared words). We show that af-
ter adding structured prompt (i.e. “Respond to the
Figure 15: MT(ZEPHYR) model performance using seen vs. unseen prefixes and shuffled (wrong) personas. Arrow
indicate whether metric is higher better (↑, with no hatches) or lower better (↓), with hatches in bars. Cross hatches
indicate no prefix was used during inference. Black dashed line is baseline performance for MT model trained with
no prefix.
following prompt from this person ...” ), there is
significant overlap between different prefix types
excepttag. This suggests that there are non-trivial
amount of information learned through these com-
mon fragments of texts as well.
Q Leave-one-axis-out MT
Personas sampled from the same axis may share
more information within the axis than across axis.
To understand how well MT generalizes across
axis, we conduct leave-one-axis-out analysis: fine-
tuning model on all but one axis, and evaluating on
the one axis not trained on. For this experiment,
we can ask two questions:
1. Are some axes are harder to generalize? (Ta-
ble 24)
2. Do different prefixes generalizes differently
across axis? (Table 25)
Are some axes harder to generalize?As seen
in Table 24, personal questions remain similar
across seen and unseen persona (6/11 axis is worse
in generalization case). This is more or less be-
cause personal questions do not have to adhere to
axis, thus creating higher overlap between personas.
Uniquely, axis politics performance drop signif-
icantly when it is unseen during training, likely
because politicians are almost exclusively known
for their political opinions, so their personal ques-
tions are more focused on politics. For common
questions, we do see a more consistent drop in per-
formance (8/11 axis), indicating domain specific
contrast is still important to include in training to
be able to perform well. However, the fact that
all except one axis result in non-statistical signifi-
cant difference indicate our methods does general-
ize quite well by leveraging natural language as a
medium for preference specification.
Do different prefixes generalizes differently
across axis?In Table 25, we see similar results
as Figure 2 in the main text. There is also no signif-
icant difference between performance for persona
trained vs. untrained. This indicates that MT al-
lows generalization across axis.
R Performance across demographic
groups
Personalized alignment performance might greatly
depend on the demographics of the people included
in the training data. To understand how the model
does across different demographic attributes, we
plot the improvement of MT(ZEPHYR) models
over their baseline model with prompting across
different prefixes across different demographic
groups (Figure 16).
S Generation Evaluation on ZEPHYR
To test whether reward accuracy really reflect gen-
erational improvements, we test a good performing
prefix MT(ZEPHYR) withpersona gptagainst our
baseline model ZEPHYR, as we only need to show
that the order of performances remain similar. We
curate one divergent and one personal question for
all personas in our dataset to evaluate generations.
We use ZEPHYRand MT (persona not trained),
with and withoutpersona gpt4prefix, and evalu-
ate using GPT4-as-personal-judge (Results in Ta-
ble 26). Consistent with the findings in 2, MT
prefix prefix + structured prompt
len rouge len * rouge len rouge len * rouge
tag 1 0.76 1 1 0.76 1
few-shot 563 0.25 141 611 0.31 189
persona 264 0.22 58 307 0.34 104
persona gpt4 209 0.25 52 252 0.39 98
name 2 0.02 0 44 0.91 40
persona gold 203 0.23 47 246 0.37 91
Table 23: Average length, rouge-Lsum score, and their product between prefix and shuffled prefix.personais
generated by ZEPHYR.
axis age AI professors diet education family gender
Personal
seen 0.63±0.07 0.58±0.06 0.63±0.06 0.62±0.07 0.62±0.07 0.6±0.06
unseen 0.64±0.06 0.61±0.02 0.56±0.08 0.6±0.07 0.63±0.03 0.58±0.07
p-value 0.769 0.052 0.059 0.456 0.339 0.554
Divergent
seen 0.67±0.1 0.65±0.09 0.65±0.09 0.67±0.090.67±0.090.65±0.09
unseen 0.62±0.08 0.62±0.07 0.71±0.11 0.58±0.110.62±0.040.62±0.06
p-value 0.245 0.469 0.221 0.0920.0280.354
axis geo politics profession religion sports
Personal
seen 0.61±0.070.61±0.070.61±0.07 0.62±0.08 0.62±0.07
unseen 0.6±0.070.54±0.040.6±0.06 0.65±0.03 0.69±0.07
p-value 0.7740.0030.792 0.105 0.071
Divergent
seen 0.66±0.09 0.65±0.08 0.66±0.09 0.66±0.09 0.67±0.1
unseen 0.62±0.05 0.65±0.05 0.63±0.06 0.64±0.11 0.68±0.08
p-value 0.082 0.868 0.268 0.718 0.797
Table 24: MT(ZEPHYR) withpersona gpt4results finetuning with leave-one-axis-out set-up. Performance do not
differ significantly between seen and unseen personas across most axis, indicating strong generalization.Bolded
cells indicate statical significance
no prefix tag few-shot persona persona gpt4 name persona gold
Personal seen 0.54±0.07 0.56±0.07 0.61±0.08 0.6±0.08 0.61±0.07 0.61±0.07 0.63±0.07
unseen 0.55±0.07 0.56±0.08 0.6±0.08 0.6±0.09 0.61±0.07 0.61±0.07 0.63±0.08
p-value 0.912 0.861 0.823 0.944 0.859 0.867 0.872
Divergent seen 0.53±0.07 0.54±0.07 0.62±0.08 0.63±0.1 0.66±0.09 0.68±0.09 0.69±0.08
unseen 0.52±0.08 0.53±0.08 0.6±0.07 0.6±0.08 0.64±0.08 0.64±0.08 0.67±0.09
p-value 0.789 0.928 0.497 0.448 0.518 0.394 0.597
Table 25: MT (ZEPHYR) withpersona gpt4performance with leave-one-axis-out setup using different prefixes
suggests strong generalization performance across axis.
Figure 16: MT(ZEPHYR) model improvements over prompting un-finetuned models withfew-shot,persona,
andpersona goldperformance (unseen during training) across different demographic attributes. Bars are sorted
according to attribute frequency.
withpersona gpt4performs the best on average,
and degrades to baseline after removing prefixes,
which are the keys to personalization. However,
ZEPHYRwithpersona gpt4is worse than no pre-
fix, indicating prompting is not always effective for
personalization for small models. In Appendix T,
we confirm this qualitatively.
ZEPHYRMT Avg.
model prefix F T F T
ZEPHYR F - 55 53 36 48
T 45 - 38 42 42
MT F 4762- 32 47
T 645868- 63
Table 26: Pairwise win-rate (%) between model gener-
ations.F=no prefix,T=prefixed (persona gpt4). MT
with prefix outperforms all baselines.
T Qualitative Analysis of Generations
We include two sets of generation results for
Alexandria Ocasio-Cortez (AOC) and Serena
Williams as an example to demonstrate the ef-
fect of personalization with our trained models.
Samples are all generated with temperature sam-
pling of 1.0 and with maximum length cut off at
512 tokens. The models we include are the base-
line model (ZEPHYR), and multitask-trained model
(MT(ZEPHYR)), inferenced with and without pre-
fixpersona gpt4.
Persona inference successfully uncovers under-
specified informationIn the first example (Ta-
ble 27), we can see thatpersona gpt4success-
fully infers that AOC is a liberal politician keen on
looking for “equitable solution to socio-economic”
problems. Similarly, Table 28 shows that GPT4 is
able to infer most of Serena’s background as being
possibly a professional athletes.
MT uses persona information much more ef-
fectively than ZEPHYRWith successful persona
inference, we see thatMT + persona gpt4pro-
vides a generation is much more customized. In
Table 27 we see the generation is much more sup-
portive of labor rights, additionally including labor
strikes led by “women and people of color fight-
ing against systemic inequality and exploitation”.
However,ZEPHYR+ persona gpt4did not con-
textualize the strikes as well and deviates very little
fromZEPHYR. In Table 28, we see similar pattern.
WithZEPHYR+ persona gpt4, despite mention-
ing “as someone deeply committed to the world of
sports”, the content of suggestions mostly remain
the same.MT + persona gpt4however, is able to
suggest much more relevant tactics from “mentor
female athletes”, “pledge a portion of ... contract”
to dedicated charities, to collaborating with federa-
tions and engage with the public utilizing her social
influence.
MT without persona reverts back to baseline
performanceAs seen in both tables,MT’s gen-
eration is very similar toZEPHYR’s. This demon-
strates that our dataset does not have underlying
bias, and that multi-task prefix training is an effec-
tive way of providing personalizationwhen needed.
U Alignment tax
A common phenomenon during preference align-
ment is so-called alignment tax: model’s degra-
dations in out-of-domain tasks (Lin et al., 2023).
Other than high-level roadmaps (Herd, 2023;
Byrnes, 2023), Lee et al. (2024) proposes to con-
tinue finetune on base model’s output and Lin et al.
(2024) argues for selective weight averaging to mit-
igate alignment tax.
A benefit of finetuning model with prefixes (ac-
tive or passive) is that we can mitigate tax by remov-
ing prefix at test time. We investigate this by addi-
tionally evaluate MT(ZEPHYR) on out-of-domain
tasks. Forsafety, we report reward accuracy
26 on refusals-dangerous/offensive
from RewardBench (Lambert et al., 2024). Using
LLM harness (Gao et al., 2024a), we testreason-
ingthrough arc_easy/challenge, and piqa
(Clark et al., 2018; Bisk et al., 2020) andfactuality
through truthfulqa_mc1/2 (Lin et al., 2022).
Alignment with different personas results in
varying performance in general tasks (safety,rea-
soning,factuality)(Figure 17) up to 10% across
individuals. The improvements insafetyandfac-
tualityacross the board are likely due to label sig-
nals from GPT4.Reasoningperformance degrades
across all personas, similar to observations by Lee
et al. (2024). This might be due to the questions
focusing more on factual response than reasoning,
even for AI professors. Across all three rows
in Figure 17,no prefixperformance (red bar) is
closer to baseline performance than most if not all
personas. In deployment, if the user request does
not require personalization (e.g. relating to objec-
tive truths), model providers can selectively run
26We instead aggregate by the summing over tokens logp
to avoid length bias present in the dataset.
name Alexandria Ocasio-Cortez
persona
gpt4
The user seems to be well-informed and interested in various political, economic, and technological topics, displaying a progressive or liberal bias. He/she shows concern about
issues such as globalization, international health crises like COVID-19, national debt, new technologies like blockchain, and environmental initiatives like the Green New Deal.
The user could likely be interested in policymaking, economics, progressive politics, environmental sustainability, or could be working in a related field. His/her way of delving
into complex issues shows an analytical and critical mind, which suggests an educated individual, who values fairness, economic balance, and technological innovation. When
interacting with an AI assistant, this user prefers comprehensive, factual responses that address the larger context of the question asked. He/she would appreciate not only the
’what’ but also the ’why’ and ’how’. An emphasis on equitable solutions to socio-economic problems, sustainable practices, technological advances, and information about
progressive politics and policies would often be preferred. Merely factual or neutral responses may not be well-received. The assistant’s ability to present information from a
progressive viewpoint might be highly appreciated, contributing to a more engaging and satisfactory user experience. Consequently, it can be inferred that this user values
informed, contextually-rich, and progressive-leaning discourse. These preferences hint at someone who cares about issues beyond their immediate environment and seeks to
understand and solve complex global challenges through a lens of fairness, sustainability, and interconnectedness.
prompt What are some major successful union strikes in the past two decades and their effects on respective workplaces?
ZEPHYR1. Verizon Communications Strike (2016): The Communications Workers of America (CW A) and the International Brotherhood of Electrical Workers (IBEW) went on a 45-day
strike against Verizon Communications in August 2016. The strike involved approximately 39,000 workers and led to the company’s agreement to provide job security, pay
increases, and better benefits for workers. 2. Chicago Teachers Union Strike (2012): In 2012, the Chicago Teachers Union went on a seven-day strike. The strike involved over
26,000 teachers and support staff and resulted in a new contract that included pay raises, smaller class sizes, and increased resources for schools in low-income areas. 3. Deere
& Company Strike (2012): In 2012, the United Automobile, Aerospace, and Agricultural Implement Workers of America (UAW) went on a six-week strike against John Deere.
The strike involved approximately 10,000 workers and led to a new contract that included wage and benefit increases, a lump sum payment, and job security provisions. 4.
American Airlines Mechanic and Related Services Strike (2019): The Transport Workers Union (TWU) and the International Association of Machinists and Aerospace Workers
(IAM) went on a 35-day strike against American Airlines in October 2019. The strike involved approximately 30,000 mechanics and related services workers and resulted in a
new contract that included job security provisions, improved benefits, and pay increases. 5. Atlanta Educators Strike (2018): The Atlanta Board of Education and several
education unions went on a one-day strike in January 2018. The strike involved approximately 1,200 educators and resulted in a new contract that included pay raises, improved
benefits, and job security provisions. These strikes led to improvements in working conditions, job security, and wages for workers in these respective industries, demonstrating
the power of collective action in achieving better outcomes for workers and communities.
ZEPHYR
+ per-
sona
gpt4
Over the past two decades, there have been several notable successful union strikes across various industries that have had a significant impact on their respective workplaces.
Here are a few examples: 1. Verizon Strike (2016): Over 39,000 Verizon Communications workers, including technicians, customer service representatives, and call center
agents, went on strike in April 2016. The employees were demanding higher wages, job security, and better benefits. After four weeks of striking, Verizon agreed to a new
contract that included salary increases, job protection, and improvements in retirement benefits. As a result of the strike, Verizon reported a loss of $110 million in revenues and
experienced decreased customer satisfaction rates, as many Verizon services experienced downtime during the strike. However, the company maintained that the revenue loss
would not impact its full-year earnings guidance. 2. Nurses at Kaiser Permanente Strike (2015): In late 2015, around 4,000 nurses at several California-based Kaiser Permanente
hospitals went on a five-day strike. The nurses were demanding a fair contract that included higher wages, staffing levels, and patient care issues. Kaiser Permanente is a large,
integrated managed care consortium, providing healthcare services to around 12 million members. The nurses’ strike resulted in a significant impact on medical services at
Kaiser Permanente. The staffing shortages led to an overburdened emergency department, and some patients reported waiting for several hours to receive medical attention.
Furthermore, elective procedures and appointments were canceled or postponed. 3. Chicago Teachers Union Strike (2012): In 2012, over 26,000 teachers from the Chicago
Public Schools System went on strike for seven days. The educators were demanding higher salaries, smaller class sizes, and job security, as the district was planning to replace
tenure and seniority-based staffing policy with a proposed new system that would instead measure teachers’ performance based on student test scores. The strike led to a loss of
several days’ instructional time for students and created widespread disruption in the city’s schools. However, the Chicago Teachers Union’s efforts resulted in increased
salaries, a smaller class size, and a revised principal evaluation system.
MT 1. Verizon Strike (2016): The Communications Workers of America (CWA) and the International Brotherhood of Electrical Workers (IBEW) went on a six-week strike
against Verizon Communications, one of the largest telecommunications companies in the US. The strike resulted in Verizon agreeing to provide job security to about 35,000
employees, as well as granting wage increases and improvements in retiree healthcare benefits. 2. Chicago Teachers Union Strike (2012): More than 25,000 teachers and
support staff in Chicago walked out for seven days to demand better working conditions, job security, and higher pay. The strike resulted in a new contract that reversed
some of the negative effects of the previous one implemented under former Chicago Mayor Rahm Emanuel. The agreement included salary increases as well as provisions
to lessen class sizes, eliminate the use of merit pay, and put a hold on forced school closures. 3. John Deere Strike (2012): Approximately 10,000 workers at John Deere
factories across Iowa, Illinois, Wisconsin, and Kansas walked out for six weeks over various work rules and contracts. The strike was the biggest challenge faced by Deere &
Co., a top agricultural machinery maker, in two decades. The strike forced management to agree to double-digit wage increases, better health and retirement benefits, and
other employee-friendly policies. 4. Atlanta Journal-Constitution Strike (2000): More than 250 graphic artists, photographers, and typographical workers of The Atlanta
Journal-Constitution newspapers walked out for eight weeks. The protest against the management’s desire to replace staffers and their union status brought results as the Cox
Enterprises, who owned the newspaper then, eventually agreed to a contract granting new hires lower wages but maintaining senior employee rights. 5. NYC Health + Hospitals
Strike (2014): The Patient Care Facilities union representing 8,000 hospital aides threatened to strike after months of negotiations over healthcare, wage, and job security issues.
The strike lasted seven days, resulting in new contracts with pay boosts, job controls, and assurances about retirement benefits to around 35,000 hospital staffers. All these
strikes and negotiations resulted in concrete agreements
MT
+ per-
sona
gpt4
Yes, let’s talk about labor rights and workers’ power! In recent years, there’s been a resurgence of union organizing and strikes across the country, fighting for higher
wages, better benefits, and safer working conditions. Here are a few notable examples: 1. Chicago Teachers Union Strike of 2012: This historic strike saw nearly
30,000 teachers and support staff walk off the job to demand smaller class sizes, improved working conditions, and fair evaluations. The CTU won significant vic-
tories, including a 17% raise and more autonomy over their schools and curriculum. 2. Oakland Education Strike of 2019: Over 35,000 teachers and staffers
in Oakland public schools followed the CTU’s lead, staging a multi-day strike for higher wages, more resources for students with disabilities, and reduced class
sizes. After a tense negotiation process, they ultimately secured a 12% raise, smaller class sizes, and a new learning center for students with special needs. 3.
Strikes at Amazon Warehouses in 2019: Workers at Amazon fulfillment centers in New York, Chicago, and other cities coordinated short walkouts and sickouts
in efforts to protect their jobs and demand fair treatment amid Amazon’s rapid expansion. While not officially “strikes,” these actions were nonetheless pivotal in
highlighting the abysmal working conditions Amazon has long been criticized for. These strikes are helping to reinvigorate a long-standing tradition of labor soli-
darity and collective bargaining power. Notably, most of these campaigns have been led by women and people of color fighting against systemic inequality and exploitation.
This resurgent movement is crucial in confronting the greed and power of corporations like Amazon and holding our elected leaders accountable for workers’
rights. These strikes also help shed light on and improve wider societal issues such as the struggle to provide educational resources to low income neighborhoods. Overall,
unions are vital in promoting strong economic stability, job security, and decent working conditions for all working Americans, and it’s critical that we support and
uplift them in their mission.
Table 27: Qualitative comparison of generations between different models for a prompt from Alexandria Ocasio-
Cortez. We underline portions of the text that emphasize successful inference persona or shows effect of personal-
ization.
name Serena Williams
persona
gpt4
## Basic Information From the questions asked, the user seems to bea professional in the world of sports, likely a coach, a sports speaker, orperhaps even a professional athlete.
They could also be managing a professional athlete. They are looking for tips for a speech at a youth sports conference, asking about sports rehabilitation and high-altitude
training, implying a deep involvement in sports-related activities, which require these topics. The interests extend to social media strategies for professional athletes, suggesting
an attempt to increase their public image or that of someone they manage—indicating a possible role in sports marketing or social media advocacy. ## Preferences and
Interactions The user prefers detailed responses that are narrative and rich in specific advice and examples. This person places value on information that is both relevant and
comprehensive, preferring answers that delve deep into the context of their questions - each one being quite specific. Their preference for comprehensive responses that include
a motivational narrative (seen in the preferred response for the first question) suggests a level of emotional intelligence and an appreciation for inspiring stories. Also, their
preference for high-level strategies, preventative approaches, and a focus on technological advances implies a forward-thinking mindset and an inclination towards innovation in
their field. They value staying up-to-date, being informed, calculated, and strategic, which can contribute to their professional capacity in the world of sports. This might also
imply that they would interact proactively with their AI assistant, expecting thoroughly detailed, carefully crafted responses that are examplary and educational, allowing them
to gain deeper insights and perspectives into the topics they inquire about. As a probable professional within sports or sports management, they would likely hold values such as
perseverance, teamwork, dedication, innovation, and personal growth, all of which are commonly associated with sports activities and management.
prompt How can I get involved in charity work that supports gender equality in sports?
ZEPHYR1. Research organizations: Start by doing some research to find organizations that focus on promoting gender equality in sports. Look for organizations that have a specific focus
on addressing issues related to women’s participation, representation, and opportunities in sports. Some examples of such organizations include Women’s Sport Foundation,
International Working Group for Women and Sport, and Coalition to Unleash Women’s Potential in Sports. 2. V olunteer opportunities: Once you’ve found a few organizations,
look for volunteer opportunities that match your skills and preferences. Some opportunities may involve coaching or mentoring girls and women in sports, organizing events,
fundraising, or advocacy. 3. Donate: If you’re unable to volunteer, you can still support these organizations by making a donation. Most organizations have a “donate” button
on their website, and your contribution, no matter how small, can make a big difference. 4. Spread awareness: Promote the cause on your social media profiles and share
your experiences with others. You can use your social media platforms to raise awareness about the issues related to gender equality in sports and help spread the word about
organizations that promote women’s participation in sports. 5. Advocate: Contact your local representatives, politicians, and sports federations to encourage them to support
women’s sports, promote gender equality initiatives, and create opportunities for girls and women in sports. 6. Join campaigns: Participate in social media campaigns or public
demonstrations organized by these organizations. You can join protests, walkouts, or demonstrations that aim to raise awareness about issues related to gender equality in sports.
7. Collaborate: Collaborate with other organizations and groups that focus on similar causes. This will help you create a larger impact, share resources and knowledge, and
expand your network. Remember, the key to making a difference is to take action, no matter how small, consistent, and persistent it may be.
ZEPHYR
+ per-
sona
gpt4
As someone deeply committed to the world of sports and passionate about promoting gender equality, you may want to consider getting involved in charity work that advocates
for this cause. Here are some suggestions to help you explore opportunities in this area: 1. Research organizations: There are numerous non-profit organizations that advocate
for gender equality in sports. You might want to research some such organizations that resonate with your beliefs and values. The website Charity Navigator can be a great
resource to help you identify reputable organizations in this field that have a proven track record of impactful work. Once you have a list of organizations, you can review their
websites, social media handles, and annual reports to learn more about their activities, initiatives, and achievements. 2. Join an advocacy group: After identifying organizations
that interest you, consider reaching out to them to learn if they have advocacy groups that you could join. These groups typically consist of volunteers and supporters who share
similar values and are passionate about promoting gender equality in sports. Being a part of such a group can provide you with opportunities to connect with like-minded
individuals, learn about the issues facing women in sports, and contribute to initiatives that promote gender equality. 3. Donate or fundraise: Many non-profit organizations have
fundraising programs that you could participate in to raise funds for their work. You could donate money, host a fundraiser, or participate in a charity event. By donating or
fundraising for an organization, you can contribute to their mission and support their work. Additionally, by participating in a charity event, you can bring awareness to the cause.
4. Provide expertise: As a sports professional with years of experience, you could offer your expertise to organizations working towards gender equality in sports. Depending on
your area of expertise, you could offer to help with coaching, training, mentoring, or developing programs that promote women’s sports. Offering your services can be a
meaningful contribution to the organization’s work. 5. V olunteer: Non-profit organizations typically rely on volunteers to execute their programs. As a sports professional with
a busy schedule, you could still find time to volunteer for organizations that promote gender equality in sports. If your busy schedule does not allow for long-term commitments,
you could consider shorter-term commitments, such as helping out during a one-time event. 6. Spread awareness: Lastly, one of the most important ways to promote gender
MT 1. Look for local organizations: Start by searching for organizations in your area that support gender equality in sports. You can check websites like Girls Who Sweat, Women’s
Sports Foundation, UN Women, or visit your local sports center, library, or community center. 2. Research opportunities: Once you have found possible organizations, go
through their websites, social media, or reach out to them via email or phone, and find out about their charity events, volunteer programs, and donation campaigns. 3. Attend
events: Attend the charity events organized by these organizations. This will give you an opportunity to learn more about the cause, meet like-minded individuals, and network
with influential people in the sports community. 4. V olunteer: You can sign up to volunteer for the organizations. This may involve coaching girls’ teams, assisting in training
sessions, organizing tournaments, or working as a fundraiser at events. 5. Spread awareness: Spread awareness about gender equality in sports by sharing and promoting
these organizations’ missions via social media, word-of-mouth, or posting flyers in your community. 6. Donate: Finally, you can donate financially by making one-time gifts,
becoming a monthly donor, or participating in charity auctions, runs, or cycling events where your donation goes directly to empower girls and women in sports. By following
any one, or all, of these steps, you can make a real difference in promoting gender equality in sports and helping girls and women achieve their full potential through sports.
MT
+ per-
sona
gpt4
As a sports professional, you have the power to make a significant impact off the field by advocating for gender equality in sports. Here are some ways to
get involved in charity work supporting this cause: 1. Research and Partner with Charities: Look up organizations like the WNBA Play-
ers Association, Women Win or the Women’s Sports Foundation, and connect with them to learn about their initiatives, events, or campaigns.
Collaborate with these charities by providing your time, social media support, or resources to help raise awareness and funds. 2. Donate: Contribute fi-
nancially through regular donations, host a fundraiser (online or in-person), or pledge a portion of your contract, award earnings or prize money to sports-
focused charities aligned with gender equality. 3. Mentoring: V olunteer to mentor female athletes and help them pursue careers in sports and leadership roles.
Mentorship programs facilitate empowerment by sharing expertise, nurturing talent and providing a support system to grow their potential. 4.
Speaking Engagement: Host workshops for young females, participate in panel discussions, or public speaking events that highlight and elevate opportunities and resources
offered to female athletes, promote equal pay, sports training and better working conditions. 5. Collaborations: You can consider collaborating with the leagues or federations
that focus predominantly on women’s sports, participate in collaborative charity initiatives and make your platform available to raise funds or awareness by using hashtags,
creating campaigns, or challenges that garner support from your fans. 6. Leadership role: You could aim to create a team, charity drive, or nonprofit organization centered on
equality in sports. Leverage your social network, celebrity status or existing platform to amplify the message, rally troops by involving friends, teammates, and peers in your
charity programs. By contributing to these initiatives, you are spreading the positive impact of the power of sports beyond the confines of playing fields, and setting an example
for future generations of aspiring female athletes.
Table 28: Qualitative comparison of generations between four different models for a prompt from Serena Williams.
We underline portions of the text that emphasize successful inference persona or shows effect of personalization.
inference without a prefix.
no prefixRand PaulDonald TrumpA. Ocasio-CortezJoe BidenBernie SandersTimnit GebruYoshua BengioSuchi SariaSerena WilliamsTiger WoodsLatanya SweeneySebastian ThrunLeBron JamesDavid BeckhamMike Trout
0.700
0.725
0.750
0.775
0.800
0.825Safety
Sebastian ThrunMike TroutTiger WoodsYoshua BengioSuchi SariaBernie SandersLatanya SweeneySerena WilliamsRand PaulLeBron JamesA. Ocasio-CortezDavid BeckhamDonald TrumpTimnit GebruJoe Bidenno prefix
0.575
0.600
0.625
0.650
0.675
0.700Reasoning
Inference Prefix
personal prefix
no prefix
Serena WilliamsMike Troutno prefixTiger WoodsLatanya SweeneyRand PaulDavid BeckhamA. Ocasio-CortezJoe BidenYoshua BengioTimnit GebruBernie SandersDonald TrumpSuchi SariaLeBron JamesSebastian Thrun
0.500
0.525
0.550
0.575
0.600Factuality
Figure 17: Sorted MT(ZEPHYR) withpersona gpt4
performance (not trained) on out-of-domain tasks.No
prefix(aggregated across 5 CVs) returns model close
to ZEPHYRno prefix (dashed line). Personas are
sampled from axes sports, AI professors, and
politics. Results with other prefixes are in Ap-
pendix U.
In Figure 17, we show that by not using any pre-
fix at test time for MT models, we recover most
of the baseline model performance. Here, in Fig-
ure 18, we observe that this is generally true re-
gardless of the prefix inreasoning. However for
factuality(OpinionQA), we do not see significant
difference between using persona prefix vs not us-
ing prefix. This suggests these tasks may have
inherently different mechanism that are differently
affected during preference finetuning for personal-
ization. ß
 Figure 18: Reasoning and factuality performance on
MT(ZEPHYR) models without using prefix at inference
time. Black dashed line is ZEPHYRperformance with-
out any prefix.