Systems of bounded rational agents with
information-theoretic constraints
1 1
Sebastian Gottwald and Daniel A. Braun
1
InstituteofNeuralInformationProcessing,FacultyofEngineering,ComputerScience
andPsychology,UniversityofUlm.
Keywords: Bounded rationality, multi-agent systems, hierarchical structure, spe-
cialization,FreeEnergyprinciple
Abstract
Specializationandhierarchicalorganizationareimportantfeaturesofefficient
collaboration in economical, artificial, and biological systems. Here, we investi-
gatethehypothesisthatbothfeaturescanbeexplainedbythefactthateachentity
ofsuchasystemislimitedinacertainway. Weproposeaninformation-theoretic
approach based on a Free Energy principle, in order to computationally analyze
systems of bounded rational agents that deal with such limitations optimally. We
find that specialization allows to focus on fewer tasks, thus leading to a more ef-
ficient execution, but in turn requires coordination in hierarchical structures of
specialized experts and coordinating units. Our results suggest that hierarchical
architectures of specialized units at lower levels that are coordinated by units at
higherlevelsareoptimal,giventhateachunit’sinformation-processingcapability
islimitedandconformstoconstraintsoncomplexitycosts.
1 Introduction
The question of how to combine a given set of individual entities in order to perform a
certain task efficiently is a long-lasting question shared by many disciplines, including
economics, neuroscience, and computer science. Even though the explicit nature of a
single individuum might differ between these fields, e.g. an employee of a company,
a neuron in a human brain, or a computer or processor as part of a cluster, they have
one important feature in common that usually prevents them from functioning isolated
by themselves: they are all limited. In fact, this was the driving idea that inspired Her-
bertA.Simonsearlyworkondecision-makingwithineconomicorganizations(Simon,
1943, 1955), which earned him a Nobel prize in 1978. He suggested that a scientific
behavioral grounding of economics should be based on bounded rationality, which has
8102
peS
61
]AM.sc[
1v79850.9081:viXra
remainedanactiveresearchtopicuntiltoday(RussellandSubramanian,1995;Lipman,
1995; Aumann, 1997; Kaelbling et al., 1998; DeCanio and Watkins, 1998; Gigerenzer
andSelten,2001;Jones,2003;Sims,2003;Burnsetal.,2013;OrtegaandBraun,2013;
Acerbi et al., 2014; Gershman et al., 2015). Subsequent studies in management theory
have been built upon Simons basic observation, because “if individual managers had
unlimited access to information that they could process costlessly and instantaneously,
there would be no role for organizations employing multiple managers” (Geanakoplos
and Milgrom, 1991). In neuroscience and biology, similar concepts have been used
to explore the evolution of specialization and modularity in nature (Kashtan and Alon,
2005; Wagner et al., 2007). In modern computer science, the terms parallel computing
and distributed computing denote two separate fields that share the concept of decen-
tralizedcomputing(Radner,1993),i.e. thecombinationofmultipleprocessingunitsin
ordertodecreasethetimeofcomputationallyexpensivecalculations.
Despite of their success, there are also shortcomings of most approaches to the or-
ganization of decision-making units based on bounded rationality: As (DeCanio and
Watkins, 1998) point out, existing agent-based methods (including their own) are not
using an overreaching optimization principle, but are tailored to the specific types of
calculations the agents are capable of, and therefore lack in generality. Moreover, it
is usually imposed as a separate assumption that there are two types of units, special-
ized operational units and coordinating non-operational units, which was expressed by
(Knight,1921)as“workersdo,andmanagersfigureoutwhattodo”.
Here, we use a Free Energy optimization principle in order to study systems of
bounded rational agents, extending the work in (Ortega and Braun, 2011, 2013; Ge-
newein and Braun, 2013; Genewein et al., 2015) on decision-making, hierarchical in-
formation-processing, and abstraction in intelligent systems with limited information-
processing capacity, that has precursors in the economic and game-theoretic literature
(McKelveyandPalfrey,1995;Ochs,1995;MattssonandWeibull,2002;Wolpert,2006;
Spiegler,2011;Howesetal.,2009;Todorov,2009;Still,2009;TishbyandPolani,2011;
Kappen et al., 2012; Edward et al., 2014; Lewis et al., 2014). Note that the Free En-
ergy optimization principle of information-theoretic bounded rationality is connected
to the Free Energy principle used in variational Bayes and Active Inference (Friston
etal.,2015a,b,2017a,b),buthasaconceptuallydistinctinterpretationandsomeformal
differences(seeSection6.3foradetailedcomparison).
Bygeneralizingtheideasin(GeneweinandBraun,2013;Geneweinetal.,2015)on
two-step information-processing to an arbitrary number of steps, we arrive at a general
FreeEnergyprinciplethatcanbeusedtostudysystemsofboundedrationalagents. The
advantagesofourapproachcanbesummarizedasfollows:
(i) There is a unifying Free Energy principle that allows for a multi-scale problem
formulationforanarbitraryamountofagentsdistributedamongthestepsofgen-
eralmulti-stepprocesses(seeSections3.3and4.2).
(ii) The computational nature of the optimization principle allows to explicitly cal-
culate and compare optimal performances of different agent architectures for a
givensetofobjectivesandresourceconstraints(seeSection5).
(iii) The information-theoretic description implies the existance of the two types of
2
unitsmentionedabove,non-operationalunits(selectornodes)thatcoordinatethe
activitiesofoperationalunits. Dependingontheirindividualresourceconstraints,
theFreeEnergyprincipleassignseachunittoaregionofspecializationthatispart
ofanoptimalpartitioningoftheunderlyingdecisionspace(seeSection4.3).
In particular, we find that, for a wide range of objectives and resource limitations
(seeSections5and5),hierarchicalsystemswithspecializedexpertsatlowerlevelsand
coordinatingunitsathigherlevelsgenerallyoutperformotherstructures.
2 Preliminaries
This section serves as an introduction to the terminology required for our framework
presentedinSection3and4.
Notation
We use curly letters, , , , etc. to denote sets of finite cardinality, in particular the
W X A
underlying spaces of the corresponding random variables W, A, X, etc., whereas the
values of these random variables are denoted by small letters, i.e. w , a ,
∈ W ∈ A
and x , respectively. We denote the space of probability distributions on a given
set b ∈ y XP . Given a probability distribution p P , the expectation of a function
f :
X RX
is denoted by f :=
(cid:80)
p(x)f(x). I
∈
f the
X
underlying probability measure
X → (cid:104) (cid:105) p x
isclearwithoutambiguitywejustwrite f .
Forafunctiong withmultipleargum (cid:104) en (cid:105) ts,e.g. forg : R,(x,y) g(x,y),
we denote the function R,x g(x,y) for fixed y X × by Y g → ( ,y) (partia (cid:55)→ l applica-
X → (cid:55)→ ∈ Y ·
tion), i.e. the dot indicates the variable of the new function. Similarly, for fixed y ,
∈ Y
wedenoteaconditionalprobabilitydistributionon withvaluesp(x y)byp( y). This
X | ·|
notation shows the dependencies clearly without giving up the original function names
andthusallowstowritemorecomplicatedexpressionsinaconciseform. Forexample,
(cid:80)
if F is a functional defined on functions of one variable, e.g. F[f] := f(x) for all
x
functions f : R, then evaluating F on the function g in its first variable while
X →
keeping the second variable fixed, is simply denoted by F[g( ,y)]. Here, the dot indi-
·
cates on which argument of g the functional F is acting and at the same time it records
(cid:80)
that the resulting value (which equals g(x,y) in the case of the example) does not
x
dependonaparticularxbutonthefixedy.
2.1 Decision-making
Here, we consider (multi-task) decision-making as the process of observing a world
statew ,sampledfromagivendistributionρ P ,andchoosingacorresponding
action a ∈ W drawn from a posterior policy P( ∈ w) W P . Assuming that the joint
∈ A ·| ∈ A
distribution of W and A is given by p(a,w) := ρ(w)P(a w), then P is the conditional
|
probability distribution of A given W. Unless stated otherwise, the capital letter P
always denotes a posterior, while the small letter p denotes the joint distribution or a
marginalofthejoint(i.e. adependentvariable).
3
A decision-making unit is called agent. An agent is rational, if its posterior policy
P maximizestheexpectedutility
(cid:88) (cid:88)
U = ρ(w) P(a w)U(a,w) (1)
(cid:104) (cid:105) |
w a
∈W ∈A
foragivenutilityfunctionU : R. NotethattheutilityU mayitselfrepresent
W×A →
an expected utility over consequences in the sense of von Neumann and Morgenstern
(1944), where W would serve as a context variable for different tasks. The posterior P
can be seen as a state-action policy that selects the best action a with respect to a
∈ A
utilityfunctionU giventhestatew oftheworld.
∈ W
2.2 Bounded rational agents
In the information-theoretic model of bounded rationality (Ortega and Braun, 2011,
2013;Geneweinetal.,2015),anagentisboundedrationalifitsposteriorP maximizes
(1)subjecttotheconstraint
(cid:10) D (P q) (cid:11) = (cid:88) ρ(w)D (P( w) q) (cid:54) D , (2)
KL KL 0
(cid:107) ·| (cid:107)
w
∈W
for a given bound D > 0 and a prior policy q P . Here, D (p q) denotes the
0 KL
Kullback-Leibler (KL) divergence between two d
∈
istri
A
butions p,q
(cid:107)P
on a set ,
(cid:80) ∈ Y Y
defined by D (p q) := p(y)log(p(y)/q(y)). Note that, for D (p q) to be
KL (cid:107) y KL (cid:107)
well-defined,pmustbeabsolut∈eYlycontinuouswithrespecttoq,sothatq(y) = 0implies
p(y) = 0. When p or q are conditional probabilities, then we treat D (p q) as a
KL
(cid:107)
functionoftheadditionalvariables.
Givenaworldstatew,theinformation-processingconsistsoftransformingapriorq
toaworldstatespecificposteriordistributionP( w). SinceD (P( w) q)measures
KL
·| ·| (cid:107)
by how much P( w) diverges from q, the upper bound D in (2) characterizes the
0
·|
limitation of the agent’s average information-processing capability: If D is close to
0
zero, the posterior must be close to the prior for all world states, which means that
A contains only little information about W, whereas if D is large, the posterior is
0
allowed to deviate from the prior by larger amounts and therefore A contains more
information about W. We use the KL-divergence as a proxy for any resource measure,
as any resource must be monotone in processed information, which is measured by the
KL-divergencebetweenpriorandposterior.
Technically, maximizing expected utility under the constraint (2) is the same as
minimizing expected complexity cost under the constraint of a minimal expected per-
formance,wherecomplexityisgivenbytheexpectedKL-divergencebetweenpriorand
posteriorandperformancebyexpectedutility. Minimizingcomplexitymeansminimiz-
ingthenumberofbitsrequiredtogeneratetheactions.
2.3 Free Energy principle
By the variational method of Lagrange multipliers, the above constrained optimization
problemisequivalenttotheunconstrainedproblem
(cid:16) 1 (cid:10) (cid:11) (cid:17)
max U D (P q) , (3)
KL
P (cid:104) (cid:105)− β (cid:107)
4
where β > 0 is chosen such that the constraint (2) is satisfied. In the literature on
information-theoreticboundedrationality(OrtegaandBraun,2011,2013),theobjective
in(3)isknownastheFreeEnergy ofthecorrespondingdecision-makingprocess. In
F
thisform,theoptimalposteriorcanbeexplicitlyderivedbydeterminingthezerosofthe
functionalderivativeof withrespecttoP,yieldingtheBoltzmann-Gibbsdistribution
F
1 (cid:88)
P(a w) = q(a)eβU(a,w), Z(w) := q(a)eβU(a,w). (4)
| Z(w)
a
∈A
Note how the Lagrange multiplier β (also known as inverse temperature) interpolates
betweenanagentwithzeroprocessingcapabilitythatalwaysactsaccordingtoitsprior
policy (β = 0) and a perfectly rational agent (β ). Note that, plugging (4) back
→ ∞
intotheFreeEnergy(3)gives
1(cid:10) (cid:11)
max [P] = logZ . (5)
P F β
2.4 Optimal prior
The performance of a given bounded rational agent crucially depends on the choice of
the prior policy q. Depending on D and the explicit form of the utility function, it
0
can be advantageous to a priori prefer certain actions over others. Therefore, optimal
bounded rational decision-making includes optimizing the prior in (3). In contrast to
(3),themodifiedoptimizationproblem
(cid:16) 1 (cid:10) (cid:11) (cid:17)
max U D (P q) (6)
KL
P,q (cid:104) (cid:105)− β (cid:107)
does not have a closed form solution. However, since the objective is convex in (P,q),
a unique solution can be obtained iteratively by alternating between fixing one and op-
timizing the other variable (Csisza´r and Tusna´dy, 1984), resulting in a Blahut-Arimoto
typealgorithm(Arimoto,1972;Blahut,1972)thatconsistsofalternatingtheequations
(cid:40)
P(a w) = 1 q(a)eβU(a,w),
| Z(w) (7)
(cid:80)
q(a) = p(a) = ρ(w)P(a w),
w |
with Z(w) given by (4). In particular, the optimal prior policy is the marginal p of the
joint distribution of W and A. In this case, the average Kullback-Leibler divergence
betweenpriorandposteriorcoincideswiththemutualinformationbetweenW andA,
(cid:88) (cid:88) p(w,a) (cid:10) (cid:11)
I(W;A) = p(w,a) log = D (P,p) .
KL
ρ(w)p(a)
w a
∈W ∈A
Itfollowsthatthemodifiedoptimizationprinciple(6)isequivalentto
(cid:16) 1 (cid:17)
max U I(W;A) . (8)
P (cid:104) (cid:105)− β
Due to its equivalence to rate distortion theory (Shannon, 1959) (with a negative
distortion measure given by the utility function), (8) is denoted as the rate distortion
caseofboundedrationalityin(GeneweinandBraun,2013).
5
2.5 Multi-step and multi-agent systems
When multiple random variables are involved in a decision-making process, such a
process constitutes a multi-step system (see Section 3). Consider the case of a prior
over that is conditioned on an additional random variable X with values x , i.e.
q( x A ) P for all x . Remember that we introduced a bounded rational ∈ ag X ent as
·| ∈ A ∈ X
a decision-making unit, that, after observing a world state w, transforms a single prior
policy over a choice space to a posterior policy P( w) P . Therefore, in the case
A ·| ∈ A
ofaconditionalprior,thecollectionofpriorpolicies q( x) canbeconsideredas
x
{ ·| } ∈X
a collection or ensemble of agents, or a multi-agent system, where for a given x ,
the prior q( x) is transformed to a posterior P( x,w) P by exactly one a ∈ ge X nt.
·| ·| ∈ A
Note that a single agent deciding about both, X and A, would be modelled by a prior
oftheformq(x,a)withx anda ,instead.
∈ X ∈ A
Hence, in order to combine multiple bounded rational agents, we are first splitting
the full decision-making process into multiple steps by introducing additional interme-
diaterandomvariables(Section3),whichthenwillbeusedtoassignoneormoreagents
to each of these steps (Section 4). In this view, we can regard a multi-agent decision-
making system as performing a sequence of successive decision steps until an ultimate
actionisselected.
3 Multi-step bounded rational decision-making
3.1 Decision nodes
Let W and A denote the random variables describing the full decision-making process
for a given utility function U : R, as described in Section 2. In order
W × A →
to separate the full process into N > 1 steps, we introduce internal random variables
X ,...,X ,whichrepresenttheoutputsofadditionalintermediateboundedrational
1 N 1
−
decision-makingsteps. Foreachk,let denotethetargetspaceandx apartic-
k k k
X ∈ X
ularvalueofX . Wecallarandomvariablethatispartofamulti-stepdecision-making
k
system a (decision) node. For simplicity, we assume that all intermediate random vari-
ablesarediscrete(justlikeW andA).
Here, we are treating feed-forward architectures originating at X := W and ter-
0
minating in X := A. This allows to label the variables X N according to the
N { k }k=0
information flow, so that X potentially can only obtain information about X if i < j.
j i
Thecanonicalfactorization
p(w,x ,...,x ,a) = ρ(w)p(x w)p(x x ,w) p(a x ,...,x ,w)
1 N 1 1 2 1 N 1 1
− | | ··· | −
of the joint probability distribution of X N therefore consists of the posterior poli-
{ k }k=0
ciesofeachdecisionnode.
3.2 Two types of nodes: inputs and prior selectors
A specific multi-step architecture is characterized by specifying the explicit depen-
dencies on the preceding variables for each node’s prior and posterior, or better the
6
p(x x ) p(x x ,x )
3| 1 −→ 3| 2 1
X
3
X A
1
X
W 4
X
2
Figure 1. Example of a processing node that is part of a multi-step architecture with
N = 5, visualized as a directed graph. Here, X processes the output of X by trans-
3 2
forming a prior policy p(x x ) to a posterior policy P(x x ,x ). The prior of X
3 1 3 2 1 3
| |
being conditioned on the output of X (indicated by the dashed arrow), means that X
1 1
determines which of the prior policies p( x ) is used by X to process a given
{ ·| 1 } x1 ∈X 1 3
outputofX .
2
missing dependencies. For example, in a given multi-step system, the posterior of the
node X might depend explicitly on the outputs of X and X but not on W, so that
3 1 2
P(x x ,x ,w) = P(x x ,x ). If its prior has the form q(x x ), then X has to pro-
3 2 1 3 2 1 3 1 3
cesst | heoutputofX . M | oreover,inthiscase,theactualpriorp | olicyq( x ) P that
2 ·| 1 ∈ X 3
isusedbyX fordecision-makingisselectedbyX (seeFigure1).
3 1
Ingeneral,theinputsX ,...,X thathavetobeprocessedbyaparticularnodeX ,
i j k
aregivenbythevariablesintheposteriorthataremissingfromtheprior,and,ifitsprior
qisconditionedontheoutputsofX ,...,X ,thenthesenodesselectwhichoftheprior
l m
policies q( x ,...,x ) P is used by X for decision-making, i.e.
{ ·| l m } xl∈Xl,...,xm ∈X m ⊂ Xk k
forthetransformation
q(x x ,...,x ) P(x x ,...,x ,x ,...,x ).
k l m k l m i j
| −→ |
We denote the collection of input nodes of X by Xk (:= X ,...,X ) and the prior
k in { i j }
selectingnodesofX byXk (:= X ,...,X ). ThejointdistributionofX ,...,X
k sel { l m } 0 N
isthengivenby
(cid:0) (cid:12) (cid:1) (cid:0) (cid:12) (cid:1)
p(x ,...,x ) = ρ(w)P x (cid:12)x1 ,x1 P x (cid:12)xN ,xN (9)
0 N 1 1 sel in ··· N N sel in
forallx andxk k ,xk k (k = 1,...,N).
k ∈ X k sel ∈ Xsel in ∈ Xin
SpecifyingthesetsXk andXk ofselectorsandinputsforeachnodeinthesystem
sel in
then uniquely characterizes a particular multi-step decision-making system. Note that
wealwayshave(X1 ,X1 ) = ( , X ).
sel in {} { 0 }
Decompositions of the form (9) are often visualized by directed acyclic graphs, so-
called DAGs (see e.g. Bishop, 2006, pp. 360). Here, in addition to the decomposition
ofthejointintermsofposteriors,wehaveaddedtheinformationaboutthepriordepen-
denciesintermsofdashedarrows,asshowninFigure1.
7
3.3 Multi-step Free Energy principle
If P and q denote the posterior and prior of the k-th node of an N-step decision-
k k
process,thentheFreeEnergyprincipletakestheform
(cid:16) (cid:88) N 1 (cid:10) (cid:11) (cid:17)
sup U D (P q ) , (10)
KL k k
(cid:104) (cid:105)− β (cid:107)
P1,q1,...,PN,qN
k=1
k
where, in addition to the expectation over inputs, the average of D (P q ) now also
KL k k
(cid:107)
includestheexpectationwithrespecttoX ,
sel
(cid:10) (cid:11) (cid:88) (cid:0) (cid:1)
D (P q) = p(x ,x )D P( x ,x ) q( x ) .
KL sel in KL sel in sel
(cid:107) ·| (cid:107) ·|
xsel,xin
Since the prior policies only appear in the KL-divergences, and moreover, there is
exactlyoneKL-divergenceperprior,itfollowsasin2.4,thatforeachk = 1,...,N the
optimalprioristhemarginalgivenforallx by
k k
∈ X
1 (cid:88)
q (x x ) = p (x x ) := p(x ,...,x ), (11)
k k sel k k sel 0 N
| | p(x )
sel
{
x0,...,xN}\ (
{
xk}∪ xsel)
wheneverXk = x . Hence,theFreeEnergyprinciplecanbesimplifiedto
sel sel
(cid:16) (cid:88) N 1 (cid:0) (cid:12) (cid:1) (cid:17)
sup U I Xk ;X (cid:12)Xk , (12)
(cid:104) (cid:105)− β in k sel
P1,...,PN
k=1
k
where I(X;Y Z) denotes the conditional mutual information of two random variables
|
X,Y givenathirdrandomvariableZ.
Byoptimizing(12)alternatingly,i.e. optimizingoneposterioratatimewhilekeep-
ingtheothersfixed,weobtainforeachk = 1,...,N,
(cid:0) (cid:1)
p x x (cid:104) (cid:105)
(cid:0) (cid:1) k k sel
P x x ,x = | exp β [P ,...,P ](x ,x ,x ) , (13)
k k sel in k k 1 N k sel in
| Z (x ,x ) F
k sel in
whenever Xk = x and Xk = x . Here, Z (x ,x ) denotes the normaliza-
sel sel in in k sel in
tion constant and [P ,...,P ] denotes the (effective) utility function on which the
k 1 N
decision-making i F n X is based on. More precisely, given X ˜ = (X ,Xk ,Xk ),
k k sel in
it is the Free Energy of the subsequent nodes in the system, i.e. for any value of
x˜ := (x ,x ,x )weobtainfor := [P ,...,P ],
k sel in k k 1 N
F F
1 (cid:88)
(x˜) = p(x ,...,x ) (x ,...,x ), (14)
k 0 N k,loc 0 N
F p(x˜) F
{
x0,...,xN}\ x˜
where
(cid:88) 1 P (x xi ,xi )
(x ,...,x ) := U(x ,x ) log i i | sel in .
F k,loc 0 N 0 N − β p (x xi )
i>k i i i | sel
Here, xi and xi are collections of values of the random variables in Xi and Xi ,
in sel in sel
respectively. The final Blahut-Arimito-type algorithm consists of iterating (13), (11),
and(14)foreachk = 1,...,N untilconvergenceisachieved. Notethat,sinceeachop-
timizationstepisconvex(marginalconvexity),convergenceisguaranteedbutgenerally
notunique(JainandKar,2017),sothat,dependingontheinitialization,onemightend
upinalocaloptimum.
8
3.4 Example: two-step information-processing
Thecasesofserialandparallelinformation-processingstudiedin(GeneweinandBraun,
2013),arespecialcasesofmulti-stepdecision-makingsystemsintroducedabove. Both
cases are two-step processes (N = 2) involving the variables X = W, X = X, and
0 1
X = A. The serial case is characterized by (X2 ,X2 ) = ( , X ), and the parallel
2 sel in {} { 1 }
case by (X2 ,X2 ) = ( X , X ). There is a third possible combination for N = 2,
sel in { 1 } { 0 }
given by (X2 ,X2 ) = ( , X ,X ). However, it can be shown that this case is
sel in {} { 0 1 }
equivalenttothe(one-step)ratedistorationcasefromSection2,becauseifAhasdirect
world state access, then any extra input to the final node A = X , that is not a prior
2
selector,containsredundantinformation.
4 Systems of bounded rational agents
4.1 From multi-step to multi-agent systems
As explained in 2.5 above, a single random variable X that is part of an N-step
k
decision-making system can represent a single agent or a collection of multiple agents,
depending on the cardinality of k , i.e. whether X has multiple priors which are
Xsel k
selected by the nodes in Xk or not. Therefore, an N-step bounded rational decision-
sel
makingsystemwithN > 1representsaboundedrationalmulti-agentsystem(ofdepth
N).
Foragivenk 1,...,N ,eachvaluex k ofXk correspondstoexactlyone
∈ { } ∈ Xsel sel
agent in X . During decision-making, the agents that belong to the nodes in Xk are
k sel
choosing which of the k agents in X is going to receive a given input x (see 4.4
|Xsel| k in
below for a detailed example). This decision is based on how well the selected agent
x will perform on the input x by transforming its prior policy p ( x) into a posterior
in k
·|
policyP ( x,x ),subjecttotheconstraint
k in
·|
D (P p ) (x) := (cid:88) p(x x)D (cid:0) P ( x,x ) p ( x) (cid:1) (cid:54) D , (15)
KL k k in KL k in k x
(cid:104) || (cid:105) | ·| (cid:107) ·|
xin
where D > 0 is a given bound on the agent’s information-processing capability. Sim-
x
ilarly to multi-step systems, this choice is based on the performance measured by the
Freeenergyofthesubsequentagents.
4.2 Multi-agent Free Energy principle
In contrast to multi-step decision-making, the information-processing bounds are al-
lowed to be functions of the agents instead of just the nodes, resulting in an extra La-
grange multiplier for each agent in the Free Energy principle (10). As in (12), optimiz-
ingoverthepriorsyieldsthesimplifiedFreeEnergyprinciple
(cid:16) (cid:88) N (cid:88) p(xk ) (cid:0) (cid:12) (cid:1) (cid:17)
sup U sel I Xk ;X (cid:12)Xk = xk , (16)
(cid:104) (cid:105)− β (xk ) in k sel sel
P1,...,PN k=1xk k k sel
sel∈Xsel
9
whichcanbesolvediterativelyasexplainedintheprevioussection,theonlydifference
being that the Lagrange parameters β now depend on xk . Hence, for the posterior of
k sel
anagentthatbelongstonodek,wehave
(cid:0) (cid:1)
p x x (cid:104) (cid:105)
P (cid:0) x x ,x (cid:1) = k k | sel exp β (xk ) (x ,x ,x ) , (17)
k k | sel in Z (x ,x ) k sel F k k sel in
k sel in
where β (xk ) is chosen such that the constraint (15) is fulfilled for all x xk , and
k sel ∈ sel
isgivenby(14)exceptthatnowwehave
k
F
(cid:88) 1 P (x xi ,xi )
(x ,...,x ) := U(x ,x ) log i i | sel in . (18)
F k,loc 0 N 0 N − β (xi ) p (x xi )
i>k i sel i i | sel
TheresultingBlahut-Arimoto-typealgorithmissummarizedinAlgorithm1.
Algorithm1Blahut-Arimito-typealgorithmfor(16)
1: procedure GETMULTIAGENTSOLUTION(U,ρ, (Xk ,Xk ) N ,β,(cid:15))
{ sel in }k=1
2: initializep(x ,...,x ) x ,...,x ,
0 N 0 N
∀
3: repeat
4: p p
0
←
5: fork = 1,...,N do
6: (x ,x ,x ) (14),(18) x ,x ,x (cid:46)calc.effectiveutility
k k sel in k sel in
F ← ∀
7: P (x x ,x ) (17) x ,x ,x (cid:46)updateposterior
k k sel in k sel in
| ← ∀
8: p(x x ) (11) x ,x (cid:46)updateprior
k sel k sel
| ← ∀
9: p(x ,...,x ) (9) x ,...,x (cid:46)updatejoint
0 N 0 N
← ∀
10: endfor
11: error dist(p,p )
0
←
12: untilerror < (cid:15)
13: returnP ,...,P
1 N
14: endprocedure
4.3 Specialization
Eventhoughagivenmulti-agentarchitecturepredeterminestheunderlyingsetofchoices
foreachagent,onlyasmallpartofsuchasetmightbeusedbyagivenagentintheop-
timized system. For example, all agents in the final step potentially can perform any
action a (see Figure 2 and the Example in 4.4 below). However, depending on
∈ A
their indiviual information-processing capabilities, the optimization over the agents’
priors can result in a (soft) partitioning of the full action space into multiple chunks,
A
where each of these chunks is given by the support of the prior of a given agent x,
supp(p( x)) . Note that the resulting partitioning is not necessarily disjoint,
·| ⊂ A
since agents might still be sharing a number of actions, depending on their available
information-processing resources. If the processing capability is low compared to the
amount of possible actions in the full space, and if there are enough agents at the same
level, then this partitioning allows each agent to focus on a smaller number of options
10
to choose from, provided that the coordinating agents have enough resources to decide
betweenthepartitionsreliably.
Therefore, the amount of prior adaptation of an agent, i.e. by howmuch its optimal
prior p deviates from a uniform prior p over all accessible choices, which is measured
0
by the KL-divergence D (p p ), determines its degree of specialization. More pre-
KL 0
(cid:107)
cisely,wedefinethespecializationofanagentwithpriorpandchoicespace by
X
D (p p ) H[p]
[p] := KL (cid:107) 0 = 1 , (19)
S log − log
|X| |X|
(cid:80)
where H[p] := p(x)logp(x) denotes the Shannon entropy of p. By normalizing
with log , we − obta x in a quantity between 0 and 1, since 0 (cid:54) H(p) (cid:54) log . Here,
|X| |X|
[p] = 0 corresponds to H[p] = log , which means that the agent is completely
S |X|
unspecialized, whereas [p] = 1 corresponds to H[p] = 0, which implies that p has
S
support on a single option x meaning that the agent deterministically performs
∗
∈ X
alwaysthesameactionandthereforeisfullyspecialized.
X1
W A
X2
world
agent
action
input
selection
Figure 2. Example of a hierarchical architecture of 10 agents that are combined via
the 3-step decision-making system (N = 3) shown in the upper left corner (see 4.4 for
details). Here, every node—and therefore every agent—has access to the world states
(bigcircle). X consistsofoneagentthatdecidesaboutwhichofthe = 3agentsin
1 1
|X |
X obtains a given world state as input. The selected agent in X selects which of the
2 2
= 2 agents out of the = 6 agents in A that are connected to it, obtains
2 1 2
|X | |X |·|X |
the world state to perform the final decision about an action a (grey circles on the
∈ A
right). In our notation introduced below, this architecture is labelled by (1,4)
[1,3,(3,2)]
(seeSection5.1).
11
4.4 Example: Hierarchical multi-agent system with three levels
Consider the example of an architecture of 10 agents shown in Figure 2 that are com-
binedviathe3-stepdecision-makingsystemgivenby
(X2 ,X2 ) = ( X , W ), (X3 ,X3 ) = ( X ,X , W ), (20)
sel in { 1 } { } sel in { 1 2 } { }
as visualized in the upper left corner of Figure 2. The number of agents in each node
is given by the cardinality of the target space of the selecting node(s) (or equals one if
therearenoselectors). Hence,X consistsofoneagent,X consistsof agents,and
1 2 1
|X |
A consists of agents. For example, if we have = 3 and = 2, as in
1 2 1 2
|X |·|X | |X | |X |
Figure2,thenthisresultsinahierarchyof1,3and6agents.
Thejointprobabilityofthesystemcharacterizedby(20)isgivenby
p(w,x ,x ,a) = p(w)P (x w)P (x x ,w)P (a x ,x ,w),
1 2 1 1 2 2 1 3 2 1
| | |
andtheFreeEnergyby
(cid:20)
(cid:88) 1 P (x w)
1 1
[P ,P ,P ] = p(w,x ,x ,a) U(a,w) log |
1 2 3 1 2
F − β p (x )
1 1 1
w,x1,x2,a
(cid:21)
1 P (x x ,w) 1 P (a x ,x ,w)
2 2 1 3 2 1
log | log | ,
− β (x ) p (x x ) − β (x ,x ) p (a x ,x )
2 1 2 2 1 3 1 2 3 2 1
| |
wherethepriorsp ,p ,andp aregivenbythemarginals(11),i.e.
1 2 3
(cid:88)
p (x ) = ρ(w)P(x w),
1 1 1
|
w
(cid:88)
p (x x ) = p(w x )P (x x ,w),
2 2 1 1 2 2 1
| | |
w
(cid:88)
p (a x ,x ) = p(w x ,x )P (a x ,x ,w).
3 2 1 1 2 3 2 1
| | |
w
By(13),theposteriorsthatiterativelysolvetheFreeEnergyprincipleare
p 1 (x 1 ) (cid:2) (cid:3)
P (x w) = exp β (w,x ) ,
1 1 1 1 1
| Z(w) F
p 2 (x 2 x 1 ) (cid:2) (cid:3)
P (x x ,w) = | exp β (x ) (w,x ,x ) ,
2 2 1 2 1 2 1 2
| Z(w,x ) F
1
p 3 (a x 2 ,x 1 ) (cid:2) (cid:3)
P (a x ,x ,w) = | exp β (x ,x )U(a,w) ,
3 2 1 3 1 2
| Z(w,x ,x )
1 2
where,by(14)and(18),
(cid:88) (cid:104) 1 P (x x ,w)
(w,x ) := p(x ,a x ,w) U(a,w) log 2 2 | 1
1 1 2 1
F | − β (x ) p (x x )
2 1 2 2 1
x2,a |
1 P (a x ,x ,w)(cid:105)
3 2 1
log | ,
− β (x ,x ) p (a x ,x )
3 1 2 3 2 1
|
(cid:88) (cid:104) 1 P (a x ,x ,w)(cid:105)
(w,x ,x ) := P (a x ,x ,w) U(a,w) log 3 | 2 1 .
2 1 2 3 2 1
F | − β (x ,x ) p (a x ,x )
3 1 2 3 2 1
a |
12
Givenaworldstatew ,theagentinX decidesaboutwhichofthethreeagents
1
∈ W
in X obtains w as an input. This narrows down the possible choices for the selected
2
agent in X to two out of the six agents in A. The selected agent performs the final
2
decisionbychoosinganactiona . Dependingonitsdegreeofspecialization,which
∈ A
is a result of his own and the coordinating agents’ resources, this agent will choose his
actionfromacertainsubsetofthefullspace .
A
5 Optimal Architectures
Here,weshowhowtheaboveframeworkcanbeusedtodetermineoptimalarchitectures
of bounded rational agents. Summarizing the assumptions made in the derivations, the
multi-agentsystemsthatweanalyzemustfulfillthefollowingrequirements:
(i) The information-flow is feed-forward: An agent in X can obtain information
k
directlyfromanotheragentthatbelongstoX onlyifm < k.
m
(ii) Intermediate agents cannot be endpoints of the decision-making process: the
information-flow always starts with the processing of W and always ends with
adecisiona A.
∈
(iii) Asingleagentisnotallowedtohavemultiplepriorpolicies: Agentsarethesmall-
est decision-making unit, in the sense that they transform a prior to a posterior
policyoverasetofactionsinonestep.
The performance of the resulting architectures is measured with respect to the ex-
pected utility they are able to achieve under a given set of resource constraints. To
this end, we need to specify (1) the objective for the full decision-making process, (2)
the number N of decision-making steps in the system, (3) the maximal number n of
agents to be distributed among the nodes, and (4) the individual resource constraints
D ,...,D ofthoseagents.
1 n
{ }
We illustrate the specifications (1)–(4) with a toy example in Section 5.2 by show-
casingandexplicitlyexplainingthedifferencesinperformanceofseveralarchitectures.
Moreover, we provide a broad performance comparison in Section 5.3, where we sys-
tematically vary a set of objective functions and resource constraints, in order to deter-
mine which architectural features most affect the overall performance. For simplicity,
in all simulations we are limiting ourselves to architectures with N (cid:54) 3 nodes and
n (cid:54) 10agents. Inthefollowingsection,westartbydescribinghowwecharacterizethe
architecturesconformingtotherequirements(i)–(iii).
5.1 Characterization of architectures
Type. In view of property (ii) above, we can label any N-step decision-making
processbyatuple(i,j),whichwecallthetypeofthearchitecture,whereicharacterizes
13
( 1,) (0,) (1,) (2,) (0,0)
−
X1
X X X
W A W A
W A W A W A
X2
(0,1) (0,2) (0,3) (0,4) (0,5)
X1 X1 X1 X1 X1
W A W A W A W A W A
X2 X2 X2 X2 X2
(1,0) (1,1) (1,2) (1,3) (1,4)
X1 X1 X1 X1 X1
W A W A W A W A W A
X2 X2 X2 X2 X2
(1,5) (2,1) (2,2) (2,3) (2,4)
X1 X1 X1 X1 X1
W A W A W A W A W A
X2 X2 X2 X2 X2
Figure 3. Overview of the resulting architectures for N (cid:54) 3, each of them being
labelledbyitstype.
the relation between the first N 1 variables W, X , ..., X , and j determines how
1 N 1
− −
thesevariablesareconnectedtoX = A.
N
Forexample,forN (cid:54) 3,weobtainthetypesshowninFigure3,wherei 0,1,2
∈ { }
andj 0,...,5 representthefollowingrelations:
∈ { }
i = 0 : (X2 ,X2 ) = ( , X )
sel in {} { 1 }
i = 1 : (X2 ,X2 ) = ( X , W )
sel in { 1 } { }
i = 2 : (X2 ,X2 ) = ( , W )
sel in {} { }
j = 0 : (X3 ,X3 ) = ( , X )
sel in {} { 2 }
j = 1 : (X3 ,X3 ) = ( , X ,X )
sel in {} { 1 2 }
j = 2 : (X3 ,X3 ) = ( X , X )
sel in { 1 } { 2 }
j = 3 : (X3 ,X3 ) = ( X , X )
sel in { 2 } { 1 }
j = 4 : (X3 ,X3 ) = ( X ,X , W )
sel in { 1 2 } { }
j = 5 : (X3 ,X3 ) = ( X , W ).
sel in { 2 } { }
For example, the architecture shown in Figure 2 has the type (1,4). Correspondingly,
thetwo-stepcasesarelabelledby(i,)fori 0,1,2 ,andtheone-stepratedistoration
∈ { }
case by ( 1,). Note that not every combination of i 0,1,2 and j 0,...,5
− ∈ { } ∈ { }
describes a unique system, e.g. (2,3) is equivalent to (2,2) when replacing X by
1
14
X . Moreover, as mentioned above, (2,) is equivalent to ( 1,), and similarly, (0,1) is
2
−
equivalentto(0,).
(0,3)[1,1,8] (1,0)[1,8,1]
1 1
W 8 W 1
1 8
(1,2)[1,4,4] (1,5)[1,3,6]
1 1
W 4 W 6
4 3
(2,1)[1,1,1] (2,4)[1,1,(2,4)]
1 1
W 1 W 8
1 1
world input
agent selection
action
Figure4. Visualizationofexemplary3-stepmulti-agentarchitecturesspecifiedbytheir
typesandshapes.
Shape. After the number of nodes has been fixed, the remaining property that charac-
terizes a given architecture is the number of agents per node. For most architectures
there are multiple possibilities to distribute a given amount of agents among the nodes,
evenwhenneglectingindividualdifferencesinresourceconstraints. Wecallsuchadis-
tribution a shape, denoted by [n ,n ,...], where n denotes the number of agents in
1 2 k
node k. Note that, not all architectures will be able to use the full amount of available
agents, most immanently the one-step rate distortion case (1 agent), or the two-step
serial-case (2 agents). For these systems, we always use the agents with the highest
availableresourcesinoursimulations.
For example, for N (cid:54) 3 the resulting shapes for a maximum of n = 10 agents are
asfollows:
[1]for( 1,),[1,1]for(0,),and[1,9]for(1,),
• −
15
24
20
16
12
8
4
0
0 4 8 12 16 20 24
responsesa
wsllacenohp
UtilityfunctionU Posteriorofasingleagentwith4.6bit
24 1.0
20
0.8
16
0.6
12
8 0.4
4
0.2
0
0 4 8 12 16 20 24
responsesa
0.0
Figure 5. Utility function for Example 5.2 (left) and posterior policy of a single agent
withaninformationboundof4.6bit(right). ThesetofphonecallsW ispartitionedinto
three separate regions, corresponding to three different topics about which customers
might have complaints or questions. Each of these can be divided into two subcate-
goriesoffourcustomercallseach. Foreachphonecallthereisexactlyoneanswerthat
achieves the best result (U = 1). Moreover, the responses that belong to one subcat-
egory of calls are also suitable for the other calls in that particular subcategory, albeit
slightly less effective (U = 0.85) than the optimal answers. Similarly, the responses
that belong to the same topic of calls are still a lot better (U = 0.7) than responses to
othertopics(U = 0).
[1,1,1]for(0,0)and(2,1),
•
[1,1,8]for(0,2),(0,3),(0,5),(2,2),
•
[1,1,(2,4)]and[1,1,(4,2)]for(0,4)and(2,4),
•
[1,8,1]for(1,0)and(1,1),
•
[1,4,4]for(1,2),
•
[1,2,7],[1,3,6],[1,4,5],[1,5,4],[1,6,3],[1,7,2]for(1,3)and(1,5),
•
[1,2,(2,3)]and[1,3,(3,2)]for(1,4),
•
where a tuple inside the shape means that two different nodes are deciding about the
agents in that spot, e.g. [1,1,(2,4)] means that there are 8 agents in the last node,
labeled by the values (x ,x ) with = 2 and = 4. In Figure 4, we
1 2 1 2 1 2
∈ X ×X |X | |X |
visualize one example architecture for each of the above 3-step shapes, except for the
shapesoftype(1,4)ofwhichoneexampleisshowninFigure2.
Together, the type (i,...) and shape [n ,...] uniquely characterize a given multi-
1
agentarchitecture,denotedby(i,...) .
[n1,...]
5.2 Example: Callcenter
Consider the operation of a company’s callcenter as a decision-making process, where
customer calls (world states) must be answered with an appropriate response (action)
in order to achieve high customer satisfaction (utility). The utility function shown in
Figure 5 on the left can be viewed as a simplistic model for a real-world callcenter of a
bigcompanysuchasacommunicationserviceprovider. Inthissimplification,thereare
16
24 possible customer calls that belong to three separate topics, for example questions
related to telephone, internet, or television, which can be further subdivided into two
subcategories,forexampleconsistingofquestionsconcerningthecontractorproblems
withthehardware. SeethedescriptionofFigure5fortheexplicitutilityvalues.
Handling all possible phone calls perfectly by always choosing the corresponding
response with maximum utility requires log (24) 4.6 bit (see Figure 5). However, in
2 ≈
practice a single agent is usually not capable of knowing the optimal answers to every
single type of question. For our example this means that the callcenter only has access
to agents with information-processing capability less than 4.6 bit. It is then required
to organize the agents in a way so that each agent only has to deal with a fraction of
thecustomercalls. Thisisoftenrealizedbyfirstpassingthephonecallthroughseveral
filtersinordertoforwardittoaspecializedagent. Arrangingtheseselectororfilterunits
inastricthierarchythencorrespondstoarchitecturesoftheformof(1,4)or(1,5)(see
belowforacomparisonofthesetwo),whereateachstageasingleoperatorselectshow
a call is forwarded. In contrast, architectures of the form of (2,4) allow for multiple
independent filters working in parallel, for example realized by multiple trained neural
networks, where each is responsible for a particular feature of the call (for example,
one node deciding about the language of the call, and another node deciding about the
topic). In the following we do not discriminate between human and artificial decision-
makers,sincebothcanqualifyequallywellasinformation-processingunits.
Assume that there are n = 10 bounded rational agents available. Considering the
givenutilityfunction,thearchitectures(1,4) (showninFigure2)and(1,5)
[1,3,(3,2)] [1,3,6]
(showninFigure4)mightbeobviouschoicesastheyrepresentthehierarchicalstructure
of the utility function. With an information bound of 1.6 ( log (3)) bit for the first
≈ 2
agent and 0.1 bit for the rest, the optimal prior policies for (1,5) obtained by our
[1,3,6]
Free Energy principle are shown in Figure 6. We can see that, for this architecture, the
choicex oftheagentatthefirststepcorrespondstothegeneraltopicofthephonecall,
1
the decisions x of the three agents at the second stage correspond to the subcategory
2
on which one of the six agents at the final stage is specialized to, who then makes the
decision about the final response a by picking one of the four actions in the support of
itsprior.
2
1
0
0.0 0.2
p(x1)
ixseciohc
5 5 5
20 20 20 20 20 20
4 4 4
15 15 15 15 15 15
3 3 3
2 2 2 10 10 10 10 10 10
1 1 1 5 5 5 5 5 5
0 0 0
0 0 0 0 0 0
0.0 0.2 0.4 0.0 0.2 0.4 0.0 0.2 0.4 0.0 0.2 0.0 0.2 0.0 0.2 0.0 0.2 0.0 0.2 0.0 0.2
p(x2| x1=0) p(x2| x1=1) p(x2| x1=2) p(a
|
x2=0) p(a
|
x2=1) p(a
|
x2=2) p(a
|
x2=3) p(a
|
x2=4) p(a
|
x2=5)
Figure 6. Optimal prior policies for each agent of the architecture (1,5) with an
[1,3,6]
informationboundof(D ,D ,...,D ) = (1.6,0.1,...,0.1).
1 2 10
17
0.8
0.6
0.4
0.2
0.0
(0,3)[1,1,8] (1,0)[1,8,1] (1,4)[1,3,(3,2)] (1,5)[1,3,6] (2,4)[1,1,(2,4)]
U i h
(D1,D2,...,D10)=(1.6,0.1,...,0.1)[bit] (D1,D2,D3,...,D10)=(1.0,0.5,0.1,...,0.1)[bit]
0.8
0.6
0.4
0.2
0.0
(0,3)[1,1,8] (1,0)[1,8,1] (1,4)[1,3,(3,2)] (1,5)[1,3,6] (2,4)[1,1,(2,4)]
Figure7. Performancecomparisonundertwodifferentinformationbounds.
We can see in Figure 7 on the left that a hierarchical structure as in (1,5) or
[1,3,6]
(1,4) is indeed superior when comparing with the architecture (2,4) ,
[1,3,(3,2)] [1,1,(2,4)]
becausethereisnogoodselectorforthesecondfilter. Wehavealsoaddedtwoarchitec-
turestothecomparisonthathaveabottleneckoftheinformationflowateitherendofthe
decision-making process, (0,3) and (1,0) (see Figure 4 for a visualization),
[1,1,8] [1,8,1]
whichareperformingconsiderablyworsethantheothers: in(0,3) thefirstagentis
[1,1,8]
the only one who has direct contact to the customer and passes the filtered information
on to everybody else, whereas in (1,0) the customer talks to multiple agents, but
[1,8,1]
these cannot take any decisions but pass on the information to a final decision node
who has to select from all possible options. Interestingly, as can be seen on the right
side of Figure 7, when changing the resource bounds such that the first agent only has
D =1 bit instead of 1.6 and the second agent has D =0.5 bit instead of 0.1, then the
1 2
strictly hierarchical architectures (1,5) and (1,4) are outperformed by the
[1,3,6] [1,3,(3,2)]
architecture (2,4) , because their first agent is not able to perfectly distinguish
[1,1,(2,4)]
between the three topics anymore. This is an ideal situation for (2,4) , since
[1,1,(2,4)]
here the total information-processing for filtering the phone calls is split up efficiently
betweenthefirsttwoagentsinthesystem.
Note that (1,4) and (1,5) do not necessarily perform identically (as can be seen on
therightinFigure7),eventhoughthestructureoftheutilityfunctionmightsuggestthat
itisidealfor(1,5) toalwayshavetheoptimalpriorsshowninFigure6. However,
[1,3,6]
this crucially depends on the given information-processing bounds. In Figure 8, we
illustratethedifferencebetweenthetwotypesinmoredetail,byshowingtheprocessed
informationthatcanactuallybeachievedperagentintherespectivearchitectureforan
information bound of D = (0.4,2.6,2.6,2.6,0.4,...,0.4). When the first agent in the
hierarchy has low capacity, then the rigid structure of (1,4) is penalized because the
agentsatthesecondstagecannotcompensatetheerrorsofthefirstagent,irrespectively
U
1.0 h i
2.5
0.8 2.0
0.6 1.5
0.4 1.0
0.2 0.5
0.0 0.0
(1,4)[1,3,(3,2)] (1,5)[1,3,6] 1 2 3 4 5 6 7 8 910
agenti
]tib[)ip
kiP(LKD
(1,4)[1,3,(3,2)]
2.5
2.0
1.5
1.0
0.5
0.0
0 1 2 3 4 5 6 7 8 9
agenti
]tib[)ip
kiP(LKD
(1,5)[1,3,6)]
24
20
16
12
8
4
0
0 4 8 12 16 20 24
responsesa
wsllacenohp
(1,4)[1,3,(3,2)]
24
20
16
12
8
4
0
0 4 8 12 16 20 24
responsesa
wsllacenohp
Performance Processedinformation Effectivepolicyp(aw)
|
(1,5)[1,3,6]
Figure 8. Demonstration of the difference between the two architectures (1,4)
[1,3,(3,2)]
and(1,5) foraninformationboundofD = (0.4,2.6,2.6,2.6,0.4,...,0.4).
[1,3,6]
18
of their capacity. In contrast, for (1,5), the connection between the second stage and
the executing stage can be changed freely, which leads to ignoring the first agent and
letting the three agents in the second stage determine the distribution of phone calls
completely. Inthissense,(1,5)ismorerobusttoerrorsinthefirstfilterthan(1,4).
5.3 Systematic performance comparison
In this section, we move away from an explicit toy example to a broad performance
comparisonofallarchitecturesforN (cid:54) 3,averagedovermultipletypesofutilityfunc-
tions and a large number of resource constraints (as defined below). In Section 6.1,
this is supplemented with an analysis of the architectural features that best explain the
performances.
20
15
10
5
0
0 5 10 15 20
setatsdlrow
U1(diagisosm) U2(diagisomm) U3(diagnonisosm) U4(diagnonisomm)
20 20 20
15 15 15
10 10 10
5 5 5
0 0 0
0 5 10 15 20 0 5 10 15 20 0 5 10 15 20
20
15
10
5
0
0 5 10 15 20
setatsdlrow
U5(samwisosm) U6(samwisomm) U7(samwnonisosm) U8(samwnonisomm)
20 20 20
15 15 15
10 10 10
5 5 5
0 0 0
0 5 10 15 20 0 5 10 15 20 0 5 10 15 20
20
15
10
5
0
0 5 10 15 20
actions
setatsdlrow
U9(maswisosm) U10(maswisomm) U11(maswnonisosm) U12(maswnonisomm)
20 20 20
15 15 15
10 10 10
5 5 5
0 0 0
0 5 10 15 20 0 5 10 15 20 0 5 10 15 20
actions actions actions
Figure9. Utilityfunctionsonwhichtheperformancesaremeasured.
Objectives. We compare all possible architectures for twelve different utility func-
tions, U 12 , defined on a world and action space of = = 20 elements, and
{ k }k=1 |W| |A|
we assume the same cardinality for the range of all hidden variables. Note that the car-
dinalityofthetargetset forselectornodesX X isgivenbythenumberofagents
sel
X ∈
it decides about. In particular, we consider three kinds of utility functions (one-to-one,
many-to-one, one-to-many) that we vary in a 2 2 paradigm, where the first dimension
×
19
0.5
0.4
0.3
0.2
0.1
0.0
− ( 1,),[ ( 1 0 ] ,2),[1 ( , 0 1 , , 4 8 ) ] ,[1 ( , 0 1 , , 4 ( ) 2 , , [1 4 ( , 1 ) 1 ] , , 3 ( ) 4 , , [1 2 ( , 1 ) 2 ] , , 3 7 ) ] ,[1 ( , 1 4 , , 4 5 ) ] ,[1 ( , 1 2 , , 4 ( ) 2 , , [1 3 ( , 1 ) 3 ] , , 5 ( ) 3 , , [1 2 ( , 1 ) 2 ] , , 5 7 ) ] ,[1 ( , 1 3 , , 5 6 ) ] ,[1 ( , 1 4 , , 5 5 ) ] ,[1 ( , 1 5 , , 5 4 ) ] ,[1 ( , 1 6 , , 5 3 ) ] ,[1 ( , 1 7 , , ) 2 ,[ ] 1, ( 9 2 ] ,2),[1 ( , 2 1 , , 4 8 ) ] ,[1,1,(2,4)]
etargninniW
Allconditions
0.4
0.3
0.2
0.1
0.0
(0,4),[1, ( 1 0 , , ( 4 2 ) , , 4 [1 ) , ] ( 1 1 , , ( 3 4 ) , , 2 [1 ) , ] ( 2 1 , , 7 3 ] ),[1, ( 4 1 , , 5 4 ] ),[1, ( 2 1 , , ( 4 2 ) , , 3 [1 ) , ] ( 3 1 , , ( 5 3 ) , , 2 [1 ) , ] ( 2 1 , , 7 5 ] ),[1, ( 3 1 , , 6 5 ] ),[1, ( 4 1 , , 5 5 ] ),[1, ( 5 1 , , 4 5 ] ),[1, ( 6 1 , , 3 5 ] ),[1, ( 7 1 , , 2 ), ] [1,9 ( ] 2,2),[1, ( 1 2 , , 8 4 ] ),[1,1,(2,4)]
etargninniW
Sameconstraintsforallagents
0.5
0.4
0.3
0.2
0.1
0.0
− ( 1,),[1 ( ] 0,2),[1, ( 1 0 , , 8 4 ] ),[1, ( 1 0 , , ( 4 2 ) , , 4 [1 ) , ] ( 1 1 , , ( 3 4 ) , , 2 [1 ) , ] ( 4 1 , , 5 4 ] ),[1, ( 2 1 , , ( 4 2 ) , , 3 [1 ) , ] ( 3 1 , , ( 5 3 ) , , 2 [1 ) , ] ( 2 1 , , 7 5 ] ),[1, ( 3 1 , , 6 5 ] ),[1, ( 4 1 , , 5 5 ] ),[1, ( 6 1 , , 3 5 ] ),[1, ( 7 1 , , 2 ), ] [1,9 ( ] 2,2),[1, ( 1 2 , , 8 4 ] ),[1,1,(2,4)]
etargninniW
Sameconstraintsforallagentsbutone
0.6
0.4
0.2
0.0
− ( 1,),[1 ( ] 0,4),[1, ( 1 0 , , ( 4 2 ) , , 4 [1 ) , ] ( 1 1 , , ( 3 4 ) , , 2 [1 ) , ] ( 4 1 , , 5 4 ] ),[1, ( 2 1 , , ( 4 2 ) , , 3 [1 ) , ] ( 3 1 , , ( 5 3 ) , , 2 [1 ) , ] ( 2 1 , , 7 5 ] ),[1, ( 3 1 , , 6 5 ] ),[1, ( 4 1 , , 5 5 ] ),[1, ( 5 1 , , 4 5 ] ),[1, ( 6 1 , , 3 5 ] ),[1, ( 7 1 , , 2 ), ] [1,9 ( ] 2,2),[1, ( 1 2 , , 8 4 ] ),[1,1,(2,4)]
etargninniW
Wins
Sameconstraintsforallagentsbuttwo
Figure 10. Proportion of conditions where the given architectures had the highest per-
formance, for all conditions, and separately for each of the three different schemes of
resourceconstraints.
is the number of maximum utility peaks (single, multiple) and the second dimension is
the range of utility values (binary, multi-valued). The utility functions are visualized in
Figure9,wherethethreekindsoffunctionscorrespondtothethreerowsoftheplot. A
one-to-one scenario applies to a needle-in-a-haystack situation where each world state
affordsonlyauniqueaction,andviceversaeachoptimalactionallowstouniquelyiden-
tifytheworldstate,forexampleanabsoluteidentificationtask. Amany-to-onescenario
allowsforabstractionsintheworldstates,forexampleincategorizationwhenmultiple
instancesarejudgedtobelongtothesameclass(e.g. vegetablesareboiled,fruitiseaten
raw). A one-to-many scenario allows for abstractions in the action space, for example
in hierarchical motor control when a grasp action can be performed in many different
ways.
Resourcelimitations. Weareconsideringthreeschemesofresourceconstraints:
(i) Sameconstraintsforallagents.
(ii) Same constraints for all agents but one, which has a higher limit than the other
agents.
(iii) Sameconstraintsforallbuttwoagents,whichcanhaveadifferentlimitandhave
higherlimitsthanalltheotheragents.
For (i), we compare 20 sets of constraints D ,D ,... with D equally spaced in
0 1 i
{ }
the range between 0 and 3 bits, for (ii) we compare 39 sets in the same range but the
high resource agent having 1, 2 and 3 bits, and for (iii) we allow 89 sets with similar
constraintsthanin(ii)butadditionalcombinationsforthesecondhigh-resourceagent.
Simulationresults. Theperformanceofanarchitectureisgivenbyitsexpectedutil-
ity with respect to a given objective and a given information bound as defined above.
20
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
− ( 1,),[1] (0,),[1,1] (1,),[1,9] (0,0),[1,1 (0 ,1 , ] 2),[1,1 (0 ,8 , ] 3),[1,1 (0 ,8 , ] 4),[1,1 (0 ,( , 2 4 , ) 4 , ) [1 ] ,1 (0 ,( , 4 5 , ) 2 , ) [1 ] ,1 (1 ,8 , ] 0),[1,8 (1 ,1 , ] 1),[1,8 (1 ,1 , ] 2),[1,4 (1 ,4 , ] 3),[1,2 (1 ,7 , ] 3),[1,3 (1 ,6 , ] 3),[1,4 (1 ,5 , ] 3),[1,5 (1 ,4 , ] 3),[1,6 (1 ,3 , ] 3),[1,7 (1 ,2 , ] 4),[1,2 (1 ,( , 2 4 , ) 3 , ) [1 ] ,3 (1 ,( , 3 5 , ) 2 , ) [1 ] ,2 (1 ,7 , ] 5),[1,3 (1 ,6 , ] 5),[1,4 (1 ,5 , ] 5),[1,5 (1 ,4 , ] 5),[1,6 (1 ,3 , ] 5),[1,7 (2 ,2 , ] 1),[1,1 (2 ,1 , ] 2),[1,1 (2 ,8 , ] 4),[1,1,(2,4)]
erocsegarevA
Averageoverallconditions
1.0
0.9
0.8
0.7
0.6
U1 U2 U3 U4 U5 U6 U7 U8 U9 U10 U11 U12
erocsegarevA
Averageoverallconstraints–foreachutilityfunction
(2,4)[1,1,(2,4)]
(1,5)[1,3,6]
(1,4)[1,3,(3,2)]
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
{0.1,...,0.
}1
{0.2,...,0.
}2
{0.3,...,0.
}3
{0.4,...,0.
}4
{0.5,...,0.
}5
{0.6,...,0.
}6
{0.7,...,0.
}7
{0.8,...,0.
}8
{0.9,...,0.
}9
{1.0,...,1.
}0
{1.1,...,1.
}1
{1.2,...,1.
}2
{1.3,...,1.
}3
{1.4,...,1.
}4
{1.5,...,1.
}5
{1.6,...,1.
}6
{1.7,...,1.
}7
{1.8,...,1.
}8
{1.9,...,1.
}9
{2.0,...,2.
}0
ytilitudetcepxeegarevA
Averageoverallutilityfunctions–sameconstraintforallagents
(2,4)[1,1,(2,4)]
(1,5)[1,3,6]
(1,4)[1,3,(3,2)]
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
{1.0,0. { 1,. 1 .. . , 0 0 , . 0 }1 . { 3,. 1 .. . , 0 0 , . 0 }3 . { 4,. 1 .. . , 0 0 , . 0 }4 . { 6,. 1 .. . , 0 0 , . 0 }6 . { 7,. 1 .. . , 0 0 , . 0 }7 . { 9,. 2 .. . , 0 0 , . 0 }9 . { 1,. 2 .. . , 0 0 , . 0 }1 . { 3,. 2 .. . , 0 0 , . 0 }3 . { 4,. 2 .. . , 0 0 , . 0 }4 . { 6,. 2 .. . , 0 0 , . 0 }6 . { 7,. 2 .. . , 0 0 , . 0 }7 . { 9,. 2 .. . , 0 0 , . 1 }9 . { 0,. 2 .. . , 0 1 , . 1 }0 . { 2,. 2 .. . , 0 1 , . 1 }2 . { 3,. 2 .. . , 0 1 , . 1 }3 . { 4,. 2 .. . , 0 1 , . 1 }4 . { 6,. 2 .. . , 0 1 , . 1 }6 . { 7,. 2 .. . , 0 1 , . 1 }7 . { 9,. 3 .. . , 0 1 , . 0 }9 . { 1,. 3 .. . , 0 0 , . 0 }1 . { 3,. 3 .. . , 0 0 , . 0 }3 . { 4,. 3 .. . , 0 0 , . 0 }4 . { 6,. 3 .. . , 0 0 , . 0 }6 . { 7,. 3 .. . , 0 0 , . 0 }7 . { 9,. 3 .. . , 0 0 , . 1 }9 . { 0,. 3 .. . , 0 1 , . 1 }0 . { 2,. 3 .. . , 0 1 , . 1 }2 . { 3,. 3 .. . , 0 1 , . 1 }3 . { 4,. 3 .. . , 0 1 , . 1 }4 . { 6,. 3 .. . , 0 1 , . 1 }6 . { 7,. 3 .. . , 0 1 , . 1 }7 . { 9,. 3 .. . , 0 1 , . 2 }9 . { 0,. 3 .. . , 0 2 , . 2 }0 . { 2,. 3 .. . , 0 2 , . 2 }2 . { 3,. 3 .. . , 0 2 , . 2 }3 . { 5,. 3 .. . , 0 2 , . 2 }5 . { 6,. 3 .. . , 0 2 , . 2 }6 . { 7,. 3 .. . , 0 2 , . 2 }7 .9,...,2. }9
ytilitudetcepxeegarevA
Averageoverallutilityfunctions–sameconstraintforallagentsbutone
(2,4)[1,1,(2,4)]
(1,5)[1,3,6]
(1,4)[1,3,(3,2)]
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
{1.0{ ,1. 1 0 . , 0{ 0 , . 1 1 . , 1 0 . . , . 0 . { 0 , , . 0 1 3 }. . , 1 1 0 . . , . 0 . { 0 , , . 0 1 4 }. . , 1 3 0 . . , . 0 . { 0 , , . 0 1 6 }. . , 1 4 0 . . , . 0 . { 0 , , . 0 1 7 }. . , 1 6 0 . . , . 0 . { 0 , , . 0 2 9 }. . , 1 7 0 . . , . 0 . { 0 , , . 0 2 1 }. . , 1 9 0 . . , . 0 . { 0 , , . 0 2 3 }. . , 1 1 0 . . , . 0 . { 0 , , . 0 2 4 }. . , 1 3 0 . . , . 0 . { 0 , , . 0 2 6 }. . , 1 4 0 . . , . 0 . { 0 , , . 0 2 7 }. . , 1 6 0 . . , . 0 . { 0 , , . 0 3 9 }. . , 1 7 0 . . , . 0 . { 0 , , . 0 3 1 }. . , 1 9 0 . . , . 0 . { 0 , , . 0 3 3 }. . , 1 1 0 . . , . 0 . { 0 , , . 0 3 4 }. . , 1 3 0 . . , . 0 . { 0 , , . 0 3 6 }. . , 1 4 0 . . , . 0 . { 0 , , . 0 3 7 }. . , 2 6 0 . . , . 0 . { 0 , , . 0 1 9 }. . , 2 7 0 . . , . 0 . { 0 , , . 0 1 1 }. . , 2 9 0 . . , . 0 . { 0 , , . 0 1 3 }. . , 2 1 0 . . , . 0 . { 0 , , . 0 1 4 }. . , 2 3 0 . . , . 0 . { 0 , , . 0 1 6 }. . , 2 4 0 . . , . 0 . { 0 , , . 0 1 7 }. . , 2 6 0 . . , . 0 . { 0 , , . 0 2 9 }. . , 2 7 0 . . , . 0 . { 0 , , . 0 2 1 }. . , 2 9 0 . . , . 0 . { 0 , , . 0 2 3 }. . , 2 1 0 . . , . 0 . { 0 , , . 0 2 4 }. . , 2 3 0 . . , . 0 . { 0 , , . 0 2 6 }. . , 2 4 0 . . , . 0 . { 0 , , . 0 2 7 }. . , 2 6 0 . . , . 0 . { 0 , , . 0 2 9 }. . , 2 7 0 . . , . 0 . { 1 , , . 0 2 0 }. . , 2 9 0 . . , . 0 . { 1 , , . 1 2 2 }. . , 2 0 0 . . , . 0 . { 1 , , . 1 2 3 }. . , 2 2 0 . . , . 0 . { 1 , , . 1 2 4 }. . , 2 3 0 . . , . 0 . { 1 , , . 1 2 6 }. . , 2 4 0 . . , . 0 . { 1 , , . 1 2 7 }. . , 2 6 0 . . , . 0 . { 1 , , . 1 3 9 }. . , 2 7 0 . . , . 0 . { 0 , , . 1 3 1 }. . , 2 9 0 . . , . 0 . { 0 , , . 0 3 3 }. . , 2 1 0 . . , . 0 . { 0 , , . 0 3 4 }. . , 2 3 0 . . , . 0 . { 0 , , . 0 3 6 }. . , 2 4 0 . . , . 0 . { 0 , , . 0 3 7 }. . , 2 6 0 . . , . 0 . { 0 , , . 0 3 9 }. . , 2 7 0 . . , . 0 . { 1 , , . 0 3 0 }. . , 2 9 0 . . , . 0 . { 1 , , . 1 3 2 }. . , 2 0 0 . . , . 0 . { 1 , , . 1 3 3 }. . , 2 2 0 . . , . 0 . { 1 , , . 1 3 4 }. . , 2 3 0 . . , . 0 . { 1 , , . 1 3 6 }. . , 2 4 0 . . , . 0 . { 1 , , . 1 3 7 }. . , 3 6 0 . . , . 0 . { 1 , , . 1 1 9 }. . , 3 7 0 . . , . 0 . { 0 , , . 1 1 1 }. . , 3 9 0 . . , . 0 . { 0 , , . 0 1 3 }. . , 3 1 0 . . , . 0 . { 0 , , . 0 1 4 }. . , 3 3 0 . . , . 0 . { 0 , , . 0 1 6 }. . , 3 4 0 . . , . 0 . { 0 , , . 0 1 7 }. . , 3 6 0 . . , . 0 . { 0 , , . 0 2 9 }. . , 3 7 0 . . , . 0 . { 0 , , . 0 2 1 }. . , 3 9 0 . . , . 0 . { 0 , , . 0 2 3 }. . , 3 1 0 . . , . 0 . { 0 , , . 0 2 4 }. . , 3 3 0 . . , . 0 . { 0 , , . 0 2 6 }. . , 3 4 0 . . , . 0 . { 0 , , . 0 2 7 }. . , 3 6 0 . . , . 0 . { 0 , , . 0 2 9 }. . , 3 7 0 . . , . 0 . { 1 , , . 0 2 0 }. . , 3 9 0 . . , . 0 . { 1 , , . 1 2 2 }. . , 3 0 0 . . , . 0 . { 1 , , . 1 2 3 }. . , 3 2 0 . . , . 0 . { 1 , , . 1 2 4 }. . , 3 3 0 . . , . 0 . { 1 , , . 1 2 6 }. . , 3 4 0 . . , . 0 . { 1 , , . 1 2 7 }. . , 3 6 0 . . , . 0 . { 1 , , . 1 3 9 }. . , 3 7 0 . . , . 0 . { 0 , , . 1 3 1 }. . , 3 9 0 . . , . 0 . { 0 , , . 0 3 3 }. . , 3 1 0 . . , . 0 . { 0 , , . 0 3 4 }. . , 3 3 0 . . , . 0 . { 0 , , . 0 3 6 }. . , 3 4 0 . . , . 0 . { 0 , , . 0 3 7 }. . , 3 6 0 . . , . 0 . { 0 , , . 0 3 9 }. . , 3 7 0 . . , . 0 . { 1 , , . 0 3 0 }. . , 3 9 0 . . , . 0 . { 1 , , . 1 3 2 }. . , 3 0 0 . . , . 0 . { 1 , , . 1 3 3 }. . , 3 2 0 . . , . 0 . { 1 , , . 1 3 4 }. . , 3 3 0 . . , . 0 . { 1 , , . 1 3 6 }. . , 3 4 0 . . , . 0 . { 1 , , . 1 3 7 }. . , 3 6 0 . . , . 0 . { 1 , , . 1 3 9 }. . , 3 7 0 . . , . 0 . { 2 , , . 1 3 0 }. . , 3 9 0 . . , . 0 . { 2 , , . 2 3 2 }. . , 3 0 0 . . , . 0 . { 2 , , . 2 3 3 }. . , 3 2 0 . . , . 0 . { 2 , , . 2 3 5 }. . , 3 3 0 . . , . 0 . { 2 , , . 2 3 6 }. . , 3 5 0 . . , . 0 . 2 , , . 2 3 7 }. . , 6 0 . , .. 2 , . 2 9 }. , 7 ...,2 }.9
Information-processingconstraints[bits]
ytilitudetcepxeegarevA
Averageoverallutilityfunctions–sameconstraintforallagentsbuttwo
(2,4)[1,1,(2,4)]
(1,5)[1,3,6]
(1,4)[1,3,(3,2)]
Figure11. Architectureperformancesaveragedoverallconditions(firstrow),averaged
over all information bounds for each utility function (second row), and averaged over
allobjectivesforeachinformationbound(lastthreerows).
21
In Figure 10, we show which of the architectures won at least one condition, together
with the proportion of conditions won by each of these architectures. We can see that
(2,4) overall outperforms all the other systems (see Figure 4 for a visualiza-
[1,1,(2,4)]
tion). In the case when all agents have the same resource constraints, the architecture
(1,4) isastrongsecondwinner,howeverthisisnotthecaseifoneortwoagents
[1,3,(3,2)]
havemoreresourcesthantherest. Itisnotsurprisingthatinthesesituationstheparallel
casewithonehigh-resourceagentdistributingtheworkamongthelowresourceagents,
andeventhecaseofasingleagentthatdoeseverythingbyhimself,arebothperforming
well.
Acloserlookontheachievedexpectedutilitieshowever,showsthatthereareseveral
architectures that are almost equally well performing for many conditions. In order to
increasecomparabilitybetweenthedifferentutilityfunctions,wemeasureperformance
intermsofarelativescore,which,foragivenutilityfunctionandresourceconstraint,is
givenbythearchitectures’expectedutilitydividedbythemaximumexpectedutilityof
all architectures. The score averaged over all conditions is shown for each architecture
inFigure11inthetoprow. Wecanseethatthebestarchitecturesareprettyclosetoeach
other. As expected, the architecture that won the most conditions also has the highest
overall performance, however there are multiple architectures that are very close. The
topthreearchitecturesare
(2,4) , (1,5) , (1,4) , (21)
[1,1,(2,4)] [1,3,6] [1,3,(3,2)]
whichhavebeenvisualizedabove(Figure2and4).
A better understanding of their performances under different resource constraints
can be gathered from the remaining graphs in Figure 11. In the second row we can
see that the top three overall architectures also perform best for almost all utility func-
tions when averaged over the information bounds. The last three graphs in Figure 11
showtheexpectedutilityofeacharchitectureaveragedoverallutilityfunctionsforeach
informationbound. Wecanseehowtheexpectedutilityincreaseswithhigherinforma-
tion bounds, for some architectures more than for others. The top three architecures
perform differently for most of the bounds, with spans of bounds where each of them
clearlyoutperformstheothers.
6 Discussion
6.1 Analysis of the simulations
Thereareplentyoffactorsthatinfluencetheperformanceofeachofthegivenarchitec-
tures. Here, we attempt to unfold the features that determine their performances in the
clearest way. To this end, we compare the architectures with respect to the following
quantities:
Average specialization of operational agents: the specialization (19) averaged over
allagentsinthefinalstageofthearchitecture.
Hierarchical: boolean value that specifies whether an architecture is hierarchical
or not, meaning that consecutive nodes are occupied by an increasing amount of
agents.
22
Agentswithdirectw-access: thenumberofagentswithdirectworldstateaccess.
operational agents with direct w-access: the number of agents in the last node of
thearchitecture.
Number of w-bottlenecks: the total number of nodes that are missing direct access
totheworldstate.
Scores
1.00
0.75
0.50
0.25
0.00
Averagespecializationofexecutingagents
0.8
0.6
0.4
0.2
0.0
Hierarchical
1.00
0.75
0.50
0.25
0.00
Agentswithdirectw-access
10.0
7.5
5.0
2.5
0.0
Executingagentswithdirectw-access
8
6
4
2
0
Numberofw-bottlenecks
2.0
1.5
1.0
0.5
0.0
(2,4),[1,1 (
,
1,
(2
5
,
),
4
[
)
1
]
,3 (
,
1,
6]
4),[1,3 (
,
1,
(3
5
,
),
2
[
)
1
]
,2 (
,
1,
7]
5),[1,4 (
,
1,
5]
4),[1,2 (
,
1,
(2
5
,
),
3
[
)
1
]
,5 (
,
1,
4]
5),[1,6 (
,
1,
3
)
]
,[1,9] (1,5),[1,7 (
,
0,
2]
4),[1,1 (
,
2,
(4
2
,
),
2
[
)
1
]
,1 (
,
0,
8]
4),[1,1 (
,
-1
(
,
2
)
,
,[
4
1
)
]
]
(0,5),[1,1 (
,
1,
8]
3),[1,2 (
,
1,
7]
3),[1,3 (
,
1,
6]
3),[1,4 (
,
1,
5]
2),[1,4 (
,
1,
4]
3),[1,5 (
,
1,
4]
3),[1,6 (
,
0,
3]
2),[1,1 (
,
1,
8]
3),[1,7 (
,
2,
2]
1),[1,1 (
,
1,
1]
1),[1,8 (
,
0,
1]
3),[1,1 (
,
1,
8]
0),[1,8 (
,
0,
1
)
]
,[1,1] (0,0),[1,1,1]
Figure12. Proposedfeaturestoexplainthearchitectures’performances(see6.1).
AscanbeseenfromFigure12,wefoundthatthesearchitecturalfeaturesexplainthe
differencesinperformancequitewell. Moreprecisely,thearchitecturescanberoughly
grouped into three different categories, indicated by slightly different color saturations
inFigure12): Thepoorestperforminggroupconsistsofarchitecturesthathavebetween
one and two w-bottlenecks, and therefore have only few agents with direct w-access,
in particular none of their operational agents has direct w-access. Moreover, in this
group, most architectures are not hierarchical at all, and their operational agents have
lowspecialization,withtwoexceptionsthatbothhavetwow-bottlenecks.
23
Thearchitectureswithmediumperformancehavemaximallyonew-bottleneckand
many of them are hierarchical. Here, those systems that have operational units with
high specialization are missing direct w-access, and the systems that have operational
unitswithdirectw-accesshavelowspecialization.
All architectures in the top group have many agents with direct world-state access
and they have no w-bottlenecks. Interestingly, the best six architectures are all strictly
hierarchical. Moreover, the order of performance is almost in direct accordance with
theaveragespecializationoftheoperationalagents.
Overall we can say that, it is best to have as many operational units as needed
to discriminate the actions well, as long as the coordinating agents have enough re-
sourcestodiscriminatebetweenthemproperly. Thearchitecture(1,4) haseight
[1,1,(2,4)]
operational agents, which are managed by two coordinating units, which need maxi-
mally two bits (for choosing among four agents) and one bit (for choosing among two
agents) in order to perform well. Both of the other top three architectures, (1,5)
[1,3,6]
and (1,4) , have six operational agents, which are managed by three coordinat-
[1,3,(3,2)]
ingunits,sothateachofthemneedsmaximallyonebit. Butcomparedto(1,4) ,
[1,1,(2,4)]
there are less agents to spare for the operational stage. Hence, if the operational units
havelowresources,itisalwaysatrade-offbetweenthenumberofoperationalunitsand
theresourcesofthecoordinatingones.
Another way to see why the architecture (1,4) overall outperforms all the
[1,1,(2,4)]
other high-ranked systems, might be its lower average choice-per-agent ratio, i.e. the
average number options for the decision of each agent in the system. In (1,4) ,
[1,1,(2,4)]
the second agent also directly observes the world state, and moreover, the choice space
of eight agents at the operational stage is split into two and four choices. Therefore,
there are only 2+4+20 = 2.6 choices per agent on average, whereas for (1,5) and
10 [1,3,6]
(1,4) ,thereare 3+6+20 = 2.9.
[1,3,(3,2)] 10
6.2 Limitations of our analysis
The analysis presented above only provides a rough explanation of the differences in
performance. Which architecture is optimal, depends a lot on the actual information
bounds of each agent. In all of our conditions, we assumed that most agents have the
same processing capabilities, which is why there is a certain bias towards architectures
that perform well under this assumption (low variance in choice-per-agent ratio across
theagents).
Due to the large amount of Lagrange parameters in the Free Energy principle (16),
thedatagenerationwasdonebyrunningtheBlahut-Arimoto-typealgorithmfor10.000
different combinations of parameters for each of the architectures, for each type of
the different types of resource limitations, (i)–(iii) in 5, and for each of the utility
functions defined in 5. For a given information bound, the corresponding parameters
weredeterminedbylookingforthepointswiththehighestFreeEnergythatstillrespect
thebound.Abetterapproachwouldbetoenhancetheglobalparametersearchbyamore
fine-grainedlocalsearch. Anotherpossibilityistouseanevolutionaryalgorithm,where
eachpopulationis givenbymultiplesetsofparameters andtheinformationconstraints
arebuiltinbyamethodsimilarto(Chehourietal.,2016). Thisworkswellbutrequires
significantlymoretimetoprocess.
24
Since the Blahut-Arimoto type algorithm is not guaranteed to converge to a global
maximum, the resulting values for the expected utility and mutual information for a
givensetofparameterscandependontheinitializationofthealgorithm. Inpractice,this
variation is small enough, so that it influences the average performance over multiple
conditions only by a negligable amount. However, direct comparisons of architectures
for a given information bound and utility function should be repeated multiple times to
makesurethattheresultsarestable.
6.3 Relation to Variational Bayes and Active Inference
Above,wedeterminedthearchitecturesthatachievethehighestexpectedutilityundera
given resource constraint. These constraints are fulfilled by tuning the Lagrange multi-
pliersintheFreeEnergyprinciple. IftheLagrangemultipliersthemselvesarefixed,for
instance as exchange rates between information and utility (Ortega and Braun, 2010),
or inverse temperatures in thermodynamics (Ortega and Braun, 2013), then the Free
Energy itself would be an appropriate performance measure. This is done, for example
in Bayesian model selection, which is also known as structure learning and represents
an important problem in Bayesian inference and machine learning. The Bayesian ap-
proachforevaluatingdifferentBayesiannetworkstructures,inordertofindtherelation
of a given set of hidden variables that best explains a dataset , consists in comparing
D
the marginal likelihood or evidence p( S) of the structures S (Friedman and Koller,
D|
2003). This can be seen to be analogous to a performance comparison of different
decision-making architectures measured by the Free Energy. In the simple case of one
observableY andonehiddenvariableX,wehave
(cid:88)
p(y S) = p(x S)p(y x,S) y ,
| | | ∀ ∈ Y
x
∈X
where the likelihood p(y x,S) is assumed to be known. Given a prior p(x S) and, for
| |
simplicity, a single observed datapoint y Y, the posterior distribution of X can be
∈
inferredbyusingBayes’rule,
p(x S)p(y x,S)
p(x y,S) = | | x . (22)
| p(y S) ∀ ∈ X
|
As has been noted before (Ortega and Braun, 2013), when comparing (22) with the
Boltzmann equation (4) we can see that (22) is equivalent to the posterior P of a
bounded rational decision-maker with choice space , prior policy p(x S), Lagrange
X |
parameter β = 1, and utility function given by U(x) := logp(y x,S). Since the
|
marginal likelihood p(y S) is the normalization constant in (22), it follows immedi-
|
ately from (5) that logp(y S) is the optimal Free Energy [P = p( y,S)] of this
var
| F ·|
25
decision-maker,where
(cid:88) (cid:88) P(x)
[P] := P(x)logp(y x,S) P(x)log
var
F | − p(x S)
x x |
(cid:88) p(x y,S)
= P(x)log | +logp(y S)
P(x) |
x
(cid:88) p(x,y S)
= P(x)log | . (23)
P(x)
x
In Bayesian statistics, is known as the variational Free Energy, and the given
var
F
decomposition is often referred to in terms of the difference between accuracy (ex-
pected log-likelihood) and complexity (KL-divergence between prior and posterior).
It is used in the variational characterization of Bayes’ rule, i.e. the approximation of
the exact Bayesian posterior p( y,S) given by (22) in terms of a simpler—for exam-
·|
ple a parametrized—distribution q by minimizing the KL-divergence between q and
p( y,S). Since D (q p( y,S)) = [q] + logp(y,S), this is equivalent to the
KL var
·| (cid:107) ·| −F
maximizationof .
var
F
The same is true for multiple hidden variables. For example, let S be the 3-step
architecture of type (1,4) from Section 4.4 with W = Y and hidden variables X ,X ,
1 2
and X = A. Setting β = β = β = 1 and U(a,x ,x ,y) = logp(y a,x ,x ,S), we
3 1 2 3 1 2 1 2
|
obtain
(y,x ,x ) = logp(y x ,x ,S), (y,x ) = logp(y x ,S),
2 1 2 1 2 1 1 1
F | F |
and
Z(y,x ,x ) = p(y x ,x ,S), Z(y,x ) = p(y x ,S), Z(y) = p(y S).
1 2 1 2 1 1
| | |
Note that, even though so far we always assumed that the utility function only depends
on the world states and actions, the equations in Sections 3, 4, and 4.4 are also valid
in the general case of U depending on all the variables in the system. The total Free
Energyforagiveny thentakestheform
∈ Y
(cid:88) (cid:16) p(x x ,y,S) p(x y,S)(cid:17)
2 1 1
p(x ,x y,S) logp(y x ,x ,S) log | log |
1 2 1 2
| | − p(x x ,S) − p(x S)
2 1 1
x1,x2 | |
(cid:88) (cid:16) p(x y,S)(cid:17)
1
= p(x y,S) logp(y x ,S) log | = logp(y S).
1 1
| | − p(x S) |
1
x1 |
Hence, also in this case, the logarithm of the marginal likelihood is given by the Free
Energy of the corresponding decision-making system. Choosing the multi-step archi-
tecturewiththehighestFreeEnergyisthenanalogoustoBayesianmodelselectionwith
themarginallikelihoodorBayesianmodelevidenceasperformancemeasure.
Another interesting interpretation of (23) is that here the hidden variable X can be
thought of as an action causing observed outcomes y. This is close to the framework
of Active Inference (Friston et al., 2015b, 2017b), where actions directy cause transi-
tions of hidden states, which generate outcomes that are observed by the actor. More
precisely,therethereal-worldprocessgeneratingobservableoutcomesisdistinguished
26
from an internal generative model describing the beliefs about the external generative
process (e.g. a Markov decision process). Observations are generated from transitions
of hidden states, which depend on the decision-maker’s actions. Decision-making is
given by the optimization of a variational Free Energy analogous to (23), where the
log-likelihood is given by the generative model, which describes beliefs about the hid-
den and control states of the generative process. This way utilities are absorbed into a
(desired)prior(OrtegaandBraun,2015). Thereareseveraldifferencestoourapproach.
First, the structure of the Free Energy principle of bounded rationality originates from
the maximization of a given pre-defined external utility function under information
constraints, whereas the Free Energy principle of Active Inference aims to minimize
surprise or Bayesian model evidence, effectively minimizing the divergence between
approximate and true posterior. Second, in Active Inference, utility is transformed into
preferences in terms of prior beliefs, while in bounded rationality prior policies over
actions can be part of the optimization process, which results in specialization and ab-
straction. In constrast, Active Inference compounds utilities and priors into a single
desiredpriorwhichisfixedanddoesnotallowtoseparatelyoptimizeutilityandaction
priors.
7 Conclusion
In this work, we have presented an information-theoretic framework to study systems
of decision-making units with limited information-processing capabilities. It is based
onanoverreachingFreeEnergyoptimizationprinciple,which,ontheonehand,allows
to compute the optimal performances of explicit architectures, and on the other hand,
producesoptimalpartitionsoftheinvolvedchoicespacesintoregionsofspecialization.
Inordertocombineagivensetofboundedrationalagents,firstthefulldecision-making
process is split into multiple decision steps by introducing intermediate decision vari-
ables, and then a given set of agents is distributed among these variables. We have
argued that this leads to two types of agents, non-operational units that distribute the
work among subordinates, and operational units that are doing the actual work in the
sense of choosing a particular action that either serves as an input for another agent
in the system, or represents the final decision of the full process. This “vertical” spe-
cialization is enhanced by optimizing over the agents’ prior policies, which leads to
an optimal soft partitioning of the underlying choice space of each step in the system,
resultingina“horizontal”specializationaswell.
In order to illustrate the proposed framework, we have simulated and analyzed the
performancesunderanumberofdifferentresourceconstraintsandtasksforallpossible
3-steparchitectureswhoseinformationflowstartsbyobservingagivenworldstateand
endswiththeselectionofafinaldecision. Eventhoughtherelativearchitectureperfor-
mances depend crucially on the explict information-processing constraints, the overall
best performing architectures tend to be hierarchical systems of non-operational “man-
ager” units at higher hierarchical levels and operational “worker” units at the lowest
level.
Ourapproachisbasedonearlierworkoninformation-theoreticboundedrationality
27
(Ortega and Braun, 2011, 2013; Genewein and Braun, 2013; Genewein et al., 2015)
(see also the references therein). In particular, the N-step decision-making systems
introduced in Section 3 generalize the two-step processes studied in (Genewein and
Braun, 2013; Genewein et al., 2015). According to Simon (Simon, 1979), there are
threedifferentboundedrationalproceduresthatcantransformintractableintotractable
decision problems: (i) Looking for satisfactory choices instead of optimal ones, (ii)
replacing global goals with tangible subgoals, and (iii) dividing the decision-making
task among many specialists. From this point of view, the decision-making process of
a single agent, given by the one-step case of information-theoretic bounded rationality
(Ortega and Braun, 2011, 2013) described in Section 2, corresponds to (i), while the
bounded rational multi-step and multi-agent decision-making processes introduced in
Section3and4,canbeattributedto(ii)and(iii).
The main advantage of a purely information-theoretic treatment is its universality.
Toourknowledgethisworkisthefirstsystematictheory-guidedapproachtotheorgani-
zation of agents with limited resources in the generality of information theory. In other
approaches, more specific methods are used instead, that are tailored to each particular
focus of study. In particular, bounded rationality has usually a very specific meaning,
often being implemented by simply restricting the cardinality of the choice space. For
example, in management theory the well-known results by Graicunas from the 1930s
(Graicunas, 1933) suggest that managers must have a limited span of control in order
to be efficient. By counting the number of possible relationships between managers
and their subordinates, he concludes that there is an explicit upper bound of five or
six subordinates. Of course, there are many cases of successful companies today that
disagree with Graicunas’ claim, e.g. Apple’s CEO has 17 managers that are reporting
directly to him. However, current management experts think that the optimal number
is somewhere between 5 and 12. The idea of restricting the cardinality of the space of
decision-making is also studied for operational agents. For example in (Camacho and
Persky,1988),CamachoandPerskyexplorethehierarchicalorganizationofspecialized
producerswithafocusonproduction. Eventhoughtheirtreatmentismoreabstractand
more general than many preceeding studies, their take on bounded rationality is very
explicit and based on the assumption that the number of elementary parts that form a
product, as well as the number of possibilities of each part, are larger than a single
individual can handle. Similarly, in most game theoretic approaches that are based on
automaton theory (Neyman, 1985; Abreu and Rubinstein, 1988; Herna´ndez and Solan,
2016),theboundednessofanagent’srationalityisexpressedbyaboundonthenumber
ofstatesoftheautomaton. Mostofthesenon-informationtheoretictreatmentsconsider
caseswhenthereisahardupperboundonthenumberofoptions,buttheyusuallylacka
probabilisticdescriptionofthebehaviourincaseswhenthenumberofoptionsislarger
thanthegivenbound.
The work by Geanakoplos and Milgrom (1991) uses “information” to describe the
limitedattentionofmanagersinafirm. Buthere,thetermisusedmoreinformally,and
notintheclassicalinformation-theoreticalsense. However,oneoftheirresultssuggests
that “firms with more prior information about parameters [...] will employ less able
managers, or give their managers wider spans of control” (Geanakoplos and Milgrom,
1991,p.207). Thisobservationisinlinewithinformation-theoreticboundedrationality,
since by optimizing over priors in the Free Energy principle, the required processing-
28
information is decreased compared to the case of non-optimal priors, so that less able
agentscanperformagiventask,orsimilarly,anagentwithahigherinformationbound
canhavealargerchoicespace.
In neuroscience, the variational Bayes approach explained in Section 6.3 has been
proposed as a theoretical framework to understand brain function in terms of Active
Inference (Friston, 2009, 2010; Friston et al., 2015a,b, 2017a,b), where perception is
modelled as variational Bayesian inference over hidden causes of observations. There,
a processing node (usually a neuron) is limited in the sense that it can only linearly
combine a set of input signals into a single output signal. Decision-making is mod-
elled by approximating Bayes’ rule in terms of these basic operations, and then tuning
the weights of the resulting linear transformations in order to optimize the Free En-
ergy (23). Hence, there, the Free Energy serves as a tool to computationally simplify
Bayesian inference on the neuronal level, whereas our Free Energy principle is a tool
tocomputationallytradeoffexpectedutilityandprocessingcosts,providinganabstract
probabilistic description of the best possible choices when the information-processing
capabilityislimited.
InthegeneralsettingofapproximateBayesianinference,therearemanyinteresting
algorithms and belief update schemes, for example belief propagation in terms of mes-
sagepassingonfactorgraphs(seee.g.Yedidiaetal.,2005). Thesealgorithmsmakeuse
of the notion of the Markov boundary (minimal Markov blanket) of a node X, which
consists of the nodes that share a common factor with X (so-called neighbours). Con-
ditioned on its Markov boundary a given random variable is independent of all other
variablesinthesystem,whichallowstoapproximatemarginalprobabilitiesintermsof
localmessages betweenneighbours. Theseapproximations aregenerallyonly exacton
tree-like factor graphs without loops (Me´zard and Montanari, 2009, Thm. 14.1). This
raises the interesting question of whether such algorithms could also be applied to our
setting. First, it should be noted that variational Bayesian inference constitutes only
a subclass of problems that can be expressed by utility optimization with information
constraints. In this subclass, all random variables have to appear either in utility func-
tions,thatistheyhavetobegivenaslog-likelihoods,ortheyhavetoappearinmarginal
distributions that are kept fixed—see for example the definition of the utility in the in-
ference example above where U(a,x ,x ,y) = logp(y a,x ,x ,S) compared to the
1 2 1 2
|
utility functions of the form U(w,a) used throughout the paper that leave all interme-
diate random variables X ,...,X unspecified. Second, while it may be possible to
1 N 1
−
exploit the notion of Markov blankets by recursively computing free energies between
the nodes in a similar fashion to message-passing, there can also be contributions from
outside the Markov boundary, for example when the action node has to take an expec-
tation over possible world states that lie outside the Markov boundary. Finally, it may
be interesting to study whether message passing algorithms can be extended to deal
with our general problem setting and at least to approximately generate the same kind
of solutions as Blahut-Arimoto, even though in general we do not have tree-structured
graphs.
There are plenty of other possible extensions of the basic framework introduced in
this work. Marschak and Reichelstein (1998) study multi-agent systems in terms of
communication cost minimization, while ignoring the actual decision-making process.
One could combine our model with the information bottleneck method (Tishby et al.,
29
1999) and explicitly include communication costs in order to study more general agent
architectures, in particular systems with non-directed information flow. Moreover, we
have seen in our simulations that specialization of operational agents is an important
feature shared among all of the best performing architectures. In the biological liter-
ature, specialization is often paired with modularity. For example Kashtan and Alon
(2005) and Wagner et al. (2007) show that modular networks are an evolutionary con-
sequence of modularly varying goals. Similarly, it would be interesting to study the
effects of changing environments on specialization, abstraction, and optimal network
architecturesofsystemsofboundedrationalagents.
Acknowledgement
This study was funded by the European Research Council (ERC-StG-2015-ERC Start-
ing Grant, Project ID: 678082, “BRISC: Bounded Rationality in Sensorimotor Coordi-
nation”).
Appendix
7.1 Proof of (13)
The Free Energy functional that is optimized in the Free Energy principle (12) is
F
givenby
(cid:88)
[P ,...,P ] = p(x)F (x),
1 N 0,loc
F
x
wherex := (x ,...,x ),andforallk 0,...,n
0 N
∈ { }
(cid:0) (cid:12) (cid:1) (cid:0) (cid:12) (cid:1)
p(x) = ρ(x )P x (cid:12)x1 ,x1 P x (cid:12)xN ,xN ,
0 1 1 sel in ··· N N sel in
(cid:88) 1 P (x xi ,xi )
(x) = U(x ,x ) log i i | sel in .
F k,loc 0 N − β p (x xi )
i>k i i i | sel
Bywriting
1 P (x xk ,xk )
(x) = (x) log k k | sel in (x ),
F 0,loc F k,loc − β p (x xk ) − R k <k
k k k | sel
wherex := (x ,...,x ),and
<k 0 k 1
−
(cid:88) 1 P (x xi ,xi )
(x ) := log i i | sel in ,
R k <k β p (x xi )
i<k i i i | sel
30
weobtainforanyk 0,...,n ,
∈ { }
(cid:20)
(cid:88) (cid:88) (cid:88)
[P ,...,P ] = p(xk ,xk ) P (x xk ,xk ) p(x˜c x˜) (x)
F 1 N sel in k k | sel in | F k,loc
xk
sel
,xk
in
xk x˜c
(cid:21)
1 (cid:0) (cid:13) (cid:1) (cid:88)
D P ( xk ,xk )(cid:13)p (x xk ) p(x ) (x )
− β KL k ·| sel in k k | sel − <k R k <k
k
x<k
with x˜ = (x ,xk ,xk ) and x˜c := (x ,...,x ) x˜. In this form, we can see that
k sel in 0 N \
optimizing for P yields the Boltzmann distribution (13) with respect to the effective
k
(cid:80)
utility (x˜) = p(x˜c x˜) (x)asdefinedin(14).
F k x˜c | F k,loc
References
Abreu, D. and Rubinstein, A. (1988). The structure of nash equilibrium in repeated
gameswithfiniteautomata. Econometrica,56(6):1259–1281.
Acerbi,L.,Vijayakumar,S.,andWolpert,D.M.(2014). Ontheoriginsofsuboptimality
inhumanprobabilisticinference. PLOSComputationalBiology,10(6):1–23.
Arimoto, S. (1972). An algorithm for computing the capacity of arbitrary discrete
memorylesschannels. IEEETransactionsonInformationTheory,18(1):14–20.
Aumann, R. J. (1997). Rationality and bounded rationality. Games and Economic
Behavior,21(1):2–14.
Bishop,C.M.(2006). PatternRecognitionandMachineLearning(InformationScience
andStatistics). Springer-VerlagNewYork,Inc.,Secaucus,NJ,USA.
Blahut, R. E. (1972). Computation of channel capacity and rate-distortion functions.
IEEETransactionsonInformationTheory,18(4):460–473.
Burns,E.,Ruml,W.,andDo,M.B.(2013).Heuristicsearchwhentimematters.Journal
ofArtificialIntelligenceResearch,47(1):697–740.
Camacho, A. and Persky, J. J. (1988). The internal organization of complex teams:
Bounded rationality and the logic of hierarchies. Journal of Economic Behavior &
Organization,9(4):367–380.
Chehouri,A.,Younes,R.,Perron,J.,andIlinca,A.(2016). Aconstraint-handlingtech-
nique for genetic algorithms using a violation factor. Journal of Computer Sciences,
12(7):350–362.
Csisza´r,I.andTusna´dy,G.(1984). Informationgeometryandalternatingminimization
procedures. StatisticsandDecisions,SupplementIssue,1:205–237.
DeCanio, S. J. and Watkins, W. E. (1998). Information processing and organizational
structure. JournalofEconomicBehavior&Organization,36(3):275–294.
31
Edward, V., Noah, G., L., G. T., and B., T. J. (2014). One and done? optimal decisions
fromveryfewsamples. CognitiveScience,38(4):599–637.
Friedman,N.andKoller,D.(2003). Beingbayesianaboutnetworkstructure.abayesian
approach to structure discovery in bayesian networks. Machine Learning, 50(1):95–
125.
Friston, K. J. (2009). The free-energy principle: a rough guide to the brain? Trends in
CognitiveSciences,13(7):293–301.
Friston,K.J.(2010). Thefree-energyprinciple: aunifiedbraintheory? NatureReviews
Neuroscience,11.
Friston,K.J.,Levin,M.,Sengupta,B.,andPezzulo,G.(2015a). Knowingone’splace:
afree-energyapproachtopatternregulation. JournalofTheRoyalSocietyInterface,
12(105).
Friston, K. J., Lin, M., Frith, C., and Pezzulo, G. (2017a). Active inference, curiosity
andinsight. NeuralComputation,29(10):2633–2683.
Friston,K.J.,Parr,T.,anddeVries,B.(2017b).Thegraphicalbrain: Beliefpropagation
andactiveinference. NetworkNeuroscience,1(4):381–414.
Friston, K. J., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., and Pezzulo, G.
(2015b). Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–
214. PMID:25689102.
Geanakoplos,J.andMilgrom,P.(1991). Atheoryofhierarchiesbasedonlimitedman-
agerial attention. Journal of the Japanese and International Economies, 5(3):205–
225.
Genewein, T. and Braun, D. A. (2013). Abstraction in decision-makers with limited
information processing capabilities. NIPS workshop on Planning with Information
Constraints.
Genewein, T., Leibfried, F., Grau-Moya, J., and Braun, D. A. (2015). Bounded ratio-
nality, abstraction, and hierarchical decision-making: An information-theoretic opti-
malityprinciple. FrontiersinRoboticsandAI,2:27.
Gershman,S.J.,Horvitz,E.J.,andTenenbaum,J.B.(2015).Computationalrationality:
A converging paradigm for intelligence in brains, minds, and machines. Science,
349(6245):273–278.
Gigerenzer, G. and Selten, R. (2001). Bounded Rationality: The Adaptive Toolbox.
MITPress.
Graicunas, V. A. (1933). Relationship in organization. Bulletin of the International
ManagementInstitute,(7):39–42.
Herna´ndez,P.andSolan,E.(2016).Boundedcomputationalcapacityequilibrium.Jour-
nalofEconomicTheory,163:342–364.
32
Howes, A., Lewis, R. L., and Vera, A. (2009). Rational adaptation under task and pro-
cessing constraints: Implications for testing theories of cognition and action. Psy-
chologicalReview,116(4):717–751.
Jain, P. and Kar, P. (2017). Non-convex Optimization for Machine Learning. Now
FoundationsandTrends.
Jones, B. D. (2003). Bounded rationality and political science: Lessons from public
administration and public policy. Journal of Public Administration Research and
Theory: J-PART,13(4):395–412.
Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). Planning and acting in
partiallyobservablestochasticdomains. ArtificialIntelligence,101(1):99–134.
Kappen,H.J.,Go´mez,V.,andOpper,M.(2012). Optimalcontrolasagraphicalmodel
inferenceproblem. MachineLearning,87(2):159–182.
Kashtan, N. and Alon, U. (2005). Spontaneous evolution of modularity and network
motifs. ProceedingsoftheNationalAcademyofSciences,102(39):13773–13778.
Knight,F.(1921). Risk,UncertaintyandProfit. Cambridge,MA:HoughtonMifflin.
Lewis, R. L., Howes, A., and Singh, S. (2014). Computational rationality: Linking
mechanismandbehaviorthroughboundedutilitymaximization. TopicsinCognitive
Science,6(2):279–311.
Lipman, B. L. (1995). Information processing and bounded rationality: A survey. The
CanadianJournalofEconomics/Revuecanadienned’Economique,28(1):42–67.
Marschak, T. and Reichelstein, S. (1998). Network mechanisms, informational effi-
ciency,andhierarchies. JournalofEconomicTheory,79(1):106–141.
Mattsson, L.-G. and Weibull, J. W. (2002). Probabilistic choice and procedurally
boundedrationality. GamesandEconomicBehavior,41(1):61–78.
McKelvey,R.D.andPalfrey,T.R.(1995). Quantalresponseequilibriafornormalform
games. GamesandEconomicBehavior,10(1):6–38.
Me´zard,M.andMontanari,A.(2009). Information,Physics,andComputation. Oxford
GraduateTexts.
Neyman, A. (1985). Bounded complexity justifies cooperation in the finitely repeated
prisoners’dilemma. EconomicsLetters,19(3):227–229.
Ochs,J.(1995). Gameswithunique,mixedstrategyequilibria: Anexperimentalstudy.
GamesandEconomicBehavior,10(1):202–217.
Ortega, P. A. and Braun, D. A. (2010). A conversion between utility and information.
TheThirdConferenceonArtificialGeneralIntelligence,pages115–120.
33
Ortega, P. A. and Braun, D. A. (2011). Information, Utility and Bounded Rationality,
pages269–274. SpringerBerlinHeidelberg,Berlin,Heidelberg.
Ortega,P.A.andBraun,D.A.(2013). Thermodynamicsasatheoryofdecision-making
with information-processing costs. Proceedings of the Royal Society of London A:
Mathematical,PhysicalandEngineeringSciences,469(2153).
Ortega, P. A. and Braun, D. A. (2015). What is epistemic value in free energy models
of learning and acting? a bounded rationality perspective. Cognitive Neuroscience,
6(4):215–216. PMID:25990838.
Radner, R. (1993). The organization of decentralized information processing. Econo-
metrica,61(5):1109–1146.
Russell, S. J. and Subramanian, D. (1995). Provably bounded-optimal agents. Journal
ofArtificialIntelligenceResearch,2(1):575–609.
Shannon, C. E. (1959). Coding theorems for a discrete source with a fidelity criterion.
IREInternationalConventionRecord,7:142–163.
Simon, H. A. (1943). A Theory Of Administrative Decision. PhD thesis, University of
Chicago.
Simon, H. A. (1955). A behavioral model of rational choice. The Quarterly Journal of
Economics,69(1):99–118.
Simon, H. A. (1979). Rational decision making in business organizations. The Ameri-
canEconomicReview,69(4):493–513.
Sims, C. A. (2003). Implications of rational inattention. Journal of Monetary Eco-
nomics, 50(3):665 – 690. Swiss National Bank/Study Center Gerzensee Conference
onMonetaryPolicyunderIncompleteInformation.
Spiegler,R.(2011). BoundedRationalityandIndustrialOrganization. OxfordUniver-
sityPress: Oxford.
Still, S. (2009). Information-theoretic approach to interactive learning. EPL (Euro-
physicsLetters),85(2):28005.
Tishby, N., Pereira, F. C., and Bialek, W. (1999). The information bottleneck method.
pages368–377.
Tishby, N. and Polani, D. (2011). Information theory of decisions and actions. In
Perception-ActionCycle: Models,Architectures,andHardware.Springer.
Todorov, E. (2009). Efficient computation of optimal actions. Proceedings of the Na-
tionalAcademyofSciences,106(28):11478–11483.
vonNeumann,J.andMorgenstern,O.(1944). TheoryofGamesandEconomicBehav-
ior. PrincetonUniversityPress.
34
Wagner,G.P.,Pavlicev,M.,andCheverud,J.M.(2007).Theroadtomodularity.Nature
ReviewsGenetics,8:921.
Wolpert,D.H.(2006). InformationTheory–TheBridgeConnectingBoundedRational
Game Theory and Statistical Physics, pages 262–290. Springer Berlin Heidelberg,
Berlin,Heidelberg.
Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2005). Constructing free-energy ap-
proximations and generalized belief propagation algorithms. IEEE Transactions on
InformationTheory,51(7):2282–2312.
35