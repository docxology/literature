ARTICLE TEMPLATE
Active Exploration based on Information Gain by Particle Filter
for Efficient Spatial Concept Formation
Akira Taniguchiaâˆ—, Yoshiki Tabuchib, Tomochika Ishikawaa,
Lotfi El Hafic, Yoshinobu Hagiwarac, and Tadahiro Taniguchia
aCollege of Information Science and Engineering, Ritsumeikan University, Shiga,
Japan;
bGraduate School of Information Science and Engineering, Ritsumeikan University,
Shiga, Japan;
cResearch Organization of Science and Technology, Ritsumeikan University, Shiga,
Japan
ARTICLE HISTORY
Compiled June 13, 2023
ABSTRACT
Autonomous robots need to learn the categories of various places by exploring
their environments and interacting with users. However, preparing training datasets
with linguistic instructions from users is time-consuming and labor-intensive. More-
over, effective exploration is essential for appropriate concept formation and rapid
environmental coverage. To address this issue, we propose an active inference
method, referred to as spatial concept formation with information gain-based active
exploration (SpCoAE) that combines sequential Bayesian inference using particle
filters and information gain-based destination determination in a probabilistic gen-
erative model. This study interprets the robotâ€™s action as a selection of destinations
to ask the user, â€˜What kind of place is this?â€™ in the context of active inference.
This study provides insights into the technical aspects of the proposed method, in-
cluding active perception and exploration by the robot, and how the method can
enable mobile robots to learn spatial concepts through active exploration. Our ex-
periment demonstrated the effectiveness of the SpCoAE in efficiently determining a
destination for learning appropriate spatial concepts in home environments.
KEYWORDS
Active exploration, active inference, probabilistic generative model, particle
filtering, spatial concept
1. Introduction
Service robots deployed in office and home settings, where they should autonomously
categorize and identify various locations through interactions with their surrounding
environment and users. Semantic mapping, which assigns a label to an environmental
map [1,2], was recently proposed. In contrast, spatial concept-based approaches allow
unsupervised learning frameworks to categorize unknown places and flexible word as-
signments from user-language interactions [3,4]. The spatial concept is defined as the
âˆ—Corresponding author. Email: a.taniguchi@em.ci.ritsumei.ac.jp
arXiv:2211.10934v2  [cs.RO]  12 Jun 2023
Active exploration
Online learning
There is living 
room.
Movement
What kind of 
place is this?
Where can I 
go to learn 
about places 
efficiently?
â€¦..
â€¦..
Figure 1. Active exploration and learning of spatial concepts by the robot. The robot explores a position
that would provide the most information and reduce uncertainty. First, the robot decides which destination
to explore from the candidate destinations in the environment. After moving, the robot asks the user, â€˜What
kind of place is this?â€™ to observe the position and words. Subsequently, the robot learns spatial concepts based
on the observations. Following this, the robot decides where to go next based on the spatial concepts it has
formed. The above process is repeated.
abstracted categorical knowledge of locations derived from the multimodal observa-
tions gathered through spatial perception in robots. However, to collect the training
data, the user follows and manipulates the robot, moves, and speaks multiple times
at each location to be taught. To solve these problems, robots should actively de-
termine and move autonomously toward their destinations. Learning through active
exploration requires the user to speak only when asked by the robot, which reduces
the burden on the user.
Our challenge is task-independent and interactive knowledge acquisition, in which
the robot asks the user questions. At the interface between artificial intelligence
and computational neuroscience, free-energy principle (FEP)-based active inference
(AIF) [5] has gained attention as an approach through which agents actively ex-
plore and acquire knowledge. The expected free energy based on AIF theoretically
encompasses information gain (IG) , which is commonly used in traditional active
perception/learning in robotics, and provides further inspiration. The application of
AIF theories in robotics has gained increasing importance [6,7]. AIF encompasses
active exploration and online learning loops, which serve as the foundation for our
study. In the field of robotics, studies have been conducted on active exploration for
simultaneous localization and mapping (SLAM) [8], that is, active SLAM [9â€“11] and
active perception/learning for multimodal categorization [12,13]. Our study integrated
these approaches, leading to active semantic mapping [14,15]. This study differs from
vision-and-language navigation (VLN) [16â€“18], which uses task-dependent knowledge
acquisition without AIF as its theoretical foundation. We focus on active exploration
inspired by AIF for grounding a spatial lexicon in a mobile robot.
The learning procedure that uses the active exploration approach is illustrated in
Figure 1. While moving in any environment, robots can acquire multimodal data on
places, such as the names spoken by users and their locations. The robot can actively
explore uncertain locations and ask the user questions. This allowed the robot to learn
spatial concepts efficiently. However, learning spatial concepts through active explo-
ration has not been achieved in previous studies [3,4]. In particular, it is important to
2
conduct efficient exploration that leads to appropriate concept formation and quickly
encompasses the environment rather than haphazard exploration.
This study addresses three main aspects: learning accuracy, learning efficiency, and
movement efficiency.
(i) Learning accuracy: Selecting data in an order that allows learning to be sta-
ble and accurate is important. In online unsupervised learning, such as particle
filtering, the order of the observed data affects the estimation accuracy [19].
(ii) Efficiency of learning: In active exploration, it is efficient to adapt to the envi-
ronment quickly. It is required to reach a sufficiently accurate learning result that
encompasses the environment in a few steps. To reduce the uncertainty of learn-
ing results concerning the environment, learning by exploring using informationâ€“
theoretical criteria is expected to be effective [12,13].
(iii) Efficiency of movement: For mobile robots, the costs associated with travel
distance must also be considered. Exploring a distant location requires more time
to move.
This study aims to improve the efficiency of learning spatial concepts through au-
tonomous active exploration using a mobile robot. Therefore, we propose an AIF-
inspired method that combines sequential Bayesian inference based on particle filters
and destination determination based on IG called spatial concept formation with IG-
based active exploration (SpCoAE) . By considering the active exploration of spatial
concept formation as an optimization problem, we realize autonomous decision-making
for the questioning behavior of the robot.
This study also considers aspects of a constructive approach in cognitive develop-
mental robotics and symbolic emergence in robotics [20â€“22]. A probabilistic genera-
tive model (PGM) for spatial concept formation was proposed as a world model [23].
Learning and inferring world models is important for building human-like intelligent
machines [6]. The implementation of AIF based on FEP, which is proposed as a funda-
mental principle of the brain [5,24â€“27], and demonstrating its feasibility is challenging
in the robotics field [7].
The main contributions of this study are as follows.
1. We show that a robot can probabilistically relate flexible location-related words to
a map through place categorization by actively determining the destination and
asking the user there.
2. We realize active inference for efficient spatial concept formation by leveraging the
posterior distribution estimated by particle filter-based online learning for IG-based
active exploration.
3. We demonstrate that the utility function, including the IG and travel distance,
achieves more efficient exploration compared to the baselines, suggesting a corre-
spondence with the expected free energy.
The remainder of this paper is organized as follows: Section 2 presents research on
spatial concept acquisition and active exploration. Section 3 describes the background
of AIF based on FEP. Section 4 describes the proposed method, SpCoAE, and the
online learning of spatial concepts. Section 5 presents experiments performed using a
simulator in multiple home environments. Section 6 discusses experiments performed
in real environments. Finally, Section 7 concludes the paper.
3
2. Related work
As research related to this study, we describe spatial perception, spatial concept forma-
tion and semantic mapping in Section 2.1 and active spatial perception, exploration,
and learning in Section 2.2.
2.1. Spatial perception and semantic mapping
Recently, semantic mapping has been emphasized [1,2]. However, several studies
have provided preset labels for specific map areas. For example, LexToMap [28] as-
signs convolutional neural network-recognized lexical labels to a topological map.
Voxblox++ [29] and Kimera [30] constructed dense metric-semantic maps using RGBD
(or dense-stereo) cameras. By contrast, our approach allows unsupervised learning to
categorize unknown places and flexible word assignments.
The robot must learn the spatial concepts online to select its next destination. In
online spatial concept formation and lexical acquisition with SLAM (SpCoSLAM) [3,4],
online learning is achieved by estimating parameters representing spatial concepts
using particle filters. Incidentally, neuroscientific findings suggest that spatial cognition
and inference take place in the hippocampal formation of the brain [31,32]. Online
learning with models such as spatial concept formation has been suggested to be
consistent with the function of the hippocampal formation [33]. Therefore, similar
to SpCoSLAM, the proposed method learns spatial concepts through online learning
using particle filters.
Considering the burden on the user, the robot was required to explore the envi-
ronment by moving actively. Thus far, the applications developed related to spatial
concepts include action selection for object tidy-up [34], spatial concept-based naviga-
tion by using speech instructions (SpCoNavi) [35], knowledge transfer across multiple
environments [36â€“38] and the relative location concept formation [39]. However, previ-
ous studies on spatial concept learning have used passive learning methods for robots,
in which a user manipulates the robot or the robot moves in the environment by fol-
lowing the user. The IG-based active exploration method proposed in this study has
the potential to be applied to these PGMs.
The VLN is the latest applied research that focuses on the boundary area between
computer vision and natural language processing for mobile robots [16,17]. Typical
conventional VLNs provide detailed linguistic texts that specify a route to a goal.
Accordingly, they performed navigation task-oriented knowledge acquisition through
numerous trial-and-error attempts using benchmark simulators [18,40]. By contrast,
our approach to spatial concept formation allows for non-task-oriented spatial knowl-
edge acquisition, including bottom-up lexical acquisition, even from small amounts of
real-world data.
2.2. Active spatial perception, exploration, and learning
Active SLAM selects the next destination when the robot performs SLAM in an envi-
ronment [9,11]. IG-based active SLAM [9] uses entropy to determine move destinations
such that the uncertainty is reduced with FastSLAM, a particle-filter-based online
SLAM method [41,42]. Owing to the numerous candidate destinations, a frontier ap-
proach [43] was used to limit the candidate search points. In another approach, graph
structures have been proposed for fast exploration [10,44]. Active Neural SLAM [45]
4
and hierarchical AIF-based SLAM [46] have also been used for deep learning-based
SLAM. However, active SLAM does not involve learning place-related words or cate-
gories.
Active semantic mapping is a challenging task that involves efficiently exploring
space while accurately capturing the semantics of the environment [11].Self-supervised
embodied active learning (SEAL) uses perception models trained on internet images
for active exploration policies [15], while our study employs unsupervised learning
without pre-training datasets. Veiga et al. proposed information-reward models based
on partially observable Markov decision processes [14]. Unlike room-specific models,
our study employed a unified model for spatial concept uncertainty. Instead of defin-
ing rewards and policies [14,15], our study directly determines actions using IG based
on PGM. Semantic octree mapping and Shannon mutual information computations
were proposed for autonomous robot operations in unstructured and unknown envi-
ronments [47]. This study is positioned as an active semantic mapping approach that
learns through active exploration to make a linguistic sense of the map, rather than
mapping.
Active robot learning approaches for language acquisition and understanding have
been developed. Efficient natural language understanding through interactive spa-
tial concept learning that considers user instructions and the environment is im-
portant [48,49]. Active learning was introduced to estimate the command ambiguity
in tabletop object manipulation in robot language acquisition [50]. Efficient cross-
situational object-word learning was achieved by actively selecting the order of the
training samples [51]. These studies have implications for constructive models for chil-
dren autonomously acquiring language. This study aims to achieve efficient spatial
language acquisition by robots through active action selection.
Online learning is appropriate for learning through active exploration because
new data are obtained each time. Active perception/learning, in which a robot ac-
tively selects the next piece of information to observe when recognizing object cate-
gories, has been proposed [12,13]. Based on a multimodal hierarchical Dirichlet pro-
cess (MHDP) [52], which is a hierarchical Bayesian model of multimodal categoriza-
tion, a robot selects an action corresponding to a sensory modality such as tactile,
visual, and auditory modalities. However, MHDP-based active perception/learning
methods [12,13] use a batch-learning algorithm based on Gibbs sampling. These con-
ventional methods compute the IG by Monte Carlo approximation with new sampling,
whereas the proposed method uses the results of online estimation using particle filters
to compute IG.
Many active-action selection methods use the IG maximization criterion, which is
known to satisfy submodularity [12]. Obtaining words related to a place from a user at
the position where the IG is maximized is expected to efficiently reduce the uncertainty
in spatial concept formation.
The latest research on active exploration and visual goal navigation leverages se-
mantics as an approach to learning through exploration based on curiosity and other
factors [53,54]. In addition, recent studies on VLN have used deep and reinforcement
learning [16,17]. By contrast, the proposed method is an unsupervised learning ap-
proach based on a PGM; it conducts active exploration using probabilistic inference
based on information-theoretic criteria. The proposed method uses abstracted loca-
tions in the ground language, while ensuring extensibility to vision.
In the field of embodied computer vision, curiosity, novelty, coverage, and recon-
struction have been defined independently for exploration [55]. In contrast, AIF can
naturally combine these indicators into a single principle. In addition, applications of
5
AIF to robots include the estimation and control of body movements [56] and multi-
modal affective human-robot interactions [57]. This study is also significant because
it applies AIF to a robot that collects data in its environment by moving with an
embodied body.
3. Foundational Concepts: Active inference based on free energy principle
with world model
The FEP provides a unified explanation for the mechanisms of action, perception, and
learning [24]. Various theories of the brain, such as the Bayesian brain hypothesis [58]
and motor control, can be explained in a unified manner by the FEP [25]. When
humans estimate the state of the external world based on their perceptions, they
actively select the action most likely to provide the most information regarding that
state (i.e., they may maximize the expected free energy) [26,27]. This is called the
AIF [5], which infers what to do next to resolve the uncertainty.
A robot should acquire knowledge about its environment through active infer-
ence [6]. Forming a representation of the world (i.e., an internal representation of
the perception of the world) is necessary to appropriately promote action generation.
This abstract model of the environment is called the world model [6,23,59â€“61]. In
particular, predictive coding [62], which learns to predict future observations, can ef-
ficiently reduce uncertainty in the knowledge of the environment. In this study, we
constructed a PGM for spatial concept formation as a world model.
FEP is realized by simultaneous or reciprocal iterative inference of perceptual in-
ference and AIF [5]. In the perceptual inference formula proposed by Friston et al.,
variational free energy is defined, and an approximate posterior distribution is ob-
tained by variational inference. In this section, we assume that Z is the set of the
hidden states (latent variables), X is the set of observations, and q(Z) is the varia-
tional approximate distribution. In the FEP, the general equations (Eqs. (1) and (2))
for the variational and expected free energies have been proposed [5,24]. Here, the
probability distributions, p(Z | X) and q(Z), are arbitrary.
The variational free energy for perceptual inference and learning is described as
follows:
F(q, p; X) = DKL [q(Z)âˆ¥p(Z | X)] âˆ’ log p(X). (1)
where DKL [qâˆ¥p] is the Kullbackâ€“Leibler (KL) divergence between the distributions q
and p. It is inferred that moves q(Z) closer to the posterior distribution p(Z | X).
The expected free energy for AIF is stated as follows:
G(Ï€, Ï„) = âˆ’Eq(XÏ„ |Ï€) [DKL [q(ZÏ„ | XÏ„ , Ï€)âˆ¥q(ZÏ„ | Ï€)]]
| {z }
information gain
âˆ’Eq(XÏ„ |Ï€) [log p(XÏ„ )]
| {z }
expected log-evidence
(2)
where policy Ï€ indexes a sequence of control states from current time t to the future.
The future time was Ï„ > t. The notation Eq(X)[f(X)] represents the expected value
of a function f(X) with respect to the distribution q(X).
IG (also called mutual information [11]) is responsible for the epistemic value term
in the expected free energy of the AIF. The epistemic value is also called the value
of information or intrinsic value. Epistemic value can be interpreted as the resolution
6
(i) Determination of
destination by IG
(ii) Movement
(iv) Online learning for
spatial concepts
Active exploration 
(Algorithm 2)
What kind of 
place is this?
(iii) Observation of
multimodal information
There is 
living room. â€¦..
â€¦..
ğ‘¥,ğ‘¦ = (1.8,âˆ’2.0)
Robotâ€™s position ğ‘¥ğ‘›
User utterances 
about place ğ‘†ğ‘›
Section 4.2 
(Algorithm 1)
Section 4.3, Eq. (33)
Algorithm 2 (Line 13)
ğ‘âˆ— = 4
ğ‘ = 1 ğ‘ = 2 ğ‘ = 3
Next step
Ï„ â† Ï„+1
Figure 2. Overview of flow of the proposed method (See Algorithm 2 for details.). (i) The robot determines
next destination aâˆ— using the utility function based on IG (See Section 4.3; especially Eq. (33)). (ii) The robot
moves to the position xaâˆ— of the destination (See Algorithm 2, line 13). (iii) The robot asks the user, â€˜What
kind of place is this?â€™ to observe the position xn and words Sn (Here, n = aâˆ—) as multimodal observation of
the PGM in Figure 3. (iv) The robot learns spatial concepts based on the observations (See Section 4.2 and
Algorithm 1). The above process is repeated until potential destinations are over or until learning is completed
by sufficient exploration. In this scenario, it is assumed that a map has been pre-generated using SLAM,
allowing for accurate self-localization and navigation to the destination.
of uncertainty by motivating curiosity and novelty-seeking actions. The expected log
evidence is responsible for pragmatic value (extrinsic value).
In this study, the robot explored and learned spatial concepts autonomously through
repeated perceptual-based online learning and active exploration. The perceptual infer-
ence and learning of the model parameters correspond to online learning using particle
filters. The variational inferences from Eq. (1) and the particle filter attempt to obtain
the same posterior distribution. This posterior distribution was approximated using
multiple samples (i.e., particles). IG maximization corresponds to policy selection in
AIF. If we assume the pragmatic value, whose prior distribution of the observed data
is uniform, the IG maximization becomes equivalent to the expected free energy mini-
mization. Instead of inferring the distribution of policy Ï€ and then determining action
a based on it as in Friston et al.â€™s formulation, we determine the action directly from
IG maximization.
4. Proposed method: Active Exploration for Spatial Concept Formation
In this study, we propose SpCoAE, a method for a robot to select the next destination
in the online learning of spatial concepts. Figure 2 illustrates the flow of the proposed
method. SpCoAE is an AIF-inspired method that combines online learning with par-
ticle filters (described in Section 4.2) and active exploration based on IG (formulated
in Section 4.3) in a PGM for spatial concept formation (defined in Section 4.1).
Section 4.1 presents model definitions. In this section, we describe the definition of
7
nx
nC ni
ï¡ l
ï¦
ï§ ï¢
ï°
nS
âˆ
lW
0 0,ï«m
0 0,ï®V
k
ï­
kï“
âˆ
ğ‘
ğ‘–àµŒ 1
ğ‘–àµŒ 2
ğ‘–àµŒ 3
ğ‘–àµŒ 4 ğ‘–àµŒ 5
ğ‘–àµŒ 6
ğ‘–àµŒ 0
ğ‘–àµŒ 7
ğ‘–àµŒ 8
ğ¶à¯¡ ğ‘†à¯¡ ğ‘–à¯¡
0 2
1
livingÂ room 
kitchen 8
1 cookingÂ area 8
2 bedroom 1
2 bedroom 7
ãƒ»ãƒ»ãƒ» ãƒ»ãƒ»ãƒ» ãƒ»ãƒ»ãƒ»
MultimodalÂ DirichletÂ processÂ mixture
GaussianÂ mixtureÂ model
ğ¶à¯¡: IndexÂ ofÂ spatialÂ concepts
 ğ‘–à¯¡: IndexÂ ofÂ positionÂ distributions
GaussianÂ distributionÂ 
(positionÂ distribution)ClusteringÂ pairsÂ ofÂ 
wordÂ ğ‘†ğ‘› andÂ placeÂ ğ‘–ğ‘›
Figure 3. Graphical model representation for spatial concept formation. The graphical model represents
conditional dependency between random variables. Gray and white nodes represent observations and unob-
served latent variables, respectively. This model is integrated by a multimodal Dirichlet process mixture whose
emission distributions are multinomial distributions based on a Gaussian mixture model. Table 1 summarizes
the descriptions of the variables in the model.
Table 1. Description of the variables in the generative model.
Symbol Definition
xn Position of robot (( x, y)-coordinates of floor plane)
Sn Words representing place corresponding to position xn (Bag-of-Words)
Cn Latent variable for index of spatial concepts
in Latent variable for index of position distributions
Ï€ Parameters of multinomial distribution for index Cn of spatial concepts
Ï•l Parameters of multinomial distribution for index in of position distribution
Wl Parameters of multinomial distribution for observing Sn
Âµk, Î£k Parameters of Gaussian distribution (position distribution) for observation of xn
Î±, Î², Î³, Hyperparameters of Dirichlet prior distribution
m0, Îº0, V0, Î½0 Hyperparameters of Gaussian and inverse Wishart prior distributions
n Index number of training data ( n âˆˆ {1, 2, . . . , N})
l Index number of spatial concepts ( l âˆˆ {1, 2, . . . , L})
k Index number of position distributions ( k âˆˆ {1, 2, . . . , K})
N Total number of training data
L Upper limit of number of spatial concepts
K Upper limit of number of position distributions
the generative process of random variables in the proposed PGM for spatial concept
formation. Based on this PGM, online learning in Section 4.2 and active exploration in
Section 4.3 are executed. Section 4.2 discusses the learning of spatial concepts. In this
section, we present the mathematical derivation of the particle filter in the proposed
PGM. Section 4.3 describes the destination determination through active exploration.
In this section, we describe the formulation of IG-based active exploration and the
procedure for deriving the algorithm.
4.1. Probabilistic generative model
The capabilities of the proposed PGM are as follows. (i) Categorization is performed
using unsupervised learning based on multimodal observations. (ii) There is many-to-
many correspondence between the words and places. Moreover, the model does not
require a prior manual setting of the vocabulary labels or categories.
Figure 3 shows a graphical representation of spatial concept formation, and Table 1
summarizes the descriptions of the variables in the model. The generative process of
8
the model is as follows.
Ï€ âˆ¼ DP(Î±) (3)
Ï•l âˆ¼ DP(Î³) l = 1, 2, . . . ,âˆ (4)
Wl âˆ¼ Dir(Î²) (5)
Î£k âˆ¼ IW(V0, Î½0) k = 1, 2, . . . ,âˆ (6)
Âµk âˆ¼ N(m0, Î£k/Îº0) (7)
Cn âˆ¼ Cat(Ï€) n = 1, 2, . . . , N (8)
in âˆ¼ Cat(Ï•Cn) (9)
Sn âˆ¼ Mult(WCn) (10)
xn âˆ¼ N(Âµin, Î£in) (11)
where DP() denotes the prior distribution in the Dirichlet process. Dir() is a Dirichlet,
Cat() is a categorical, Mult() is multinomial, IW() is an inverse-Wishart, and N() is
a Gaussian distribution. We refer to the literature on machine learning [63] for specific
formulas of the above probability distributions. In this study, the Dirichlet process
is represented as a stick-breaking process (SBP) [64,65]. We adopted a weak-limit
approximation [66] for SBP. Appropriate numbers of spatial concepts and position
distributions are probabilistically determined by learning based on observations.
4.2. Online learning algorithm
In this study, the robot learns spatial concepts from observations to select the next
move. Therefore, we introduced online learning using a Rao-Blackwellized particle fil-
ter (RBPF) [67] as in SpCoSLAM. This method estimates the posterior distributions
of the parameters of a spatial concept using words and positions as multimodal obser-
vations of a place. The learning algorithm is presented as Algorithm 1.
The joint posterior distributions of all the parameters to be estimated when learning
spatial concepts and their factorization are as follows:
p(Î˜, C1:n, i1:n | x1:n, S1:n, h)
= p(Î˜ | C1:n, i1:n, x1:n, S1:n, h)p(C1:n, i1:n | x1:n, S1:n, h) (12)
where the parameter set of each spatial concept is Î˜ = {{Âµk}, {Î£k}, {Ï•l}, {Wl}, Ï€}
and the set of hyperparameters is h = {Î±, Î², Î³, m0, Îº0, V0, Î½0}. Appendix A.4 explains
the calculation procedure for the posterior distribution of the model parameter Î˜.
4.2.1. Derivation and process in Rao-Blackwellized particle filter
The second term,p(C1:n, i1:n | x1:n, S1:n, h), in Eq. (12) was calculated using the RBPF.
The particle filter algorithm is based on sampling importance resampling. The process
can be summarized by the following steps:
Sampling: The latent variables Cn, in are sampled simultaneously as the proposal
9
Algorithm 1 Online learning algorithm for spatial concepts.
1: Znâˆ’1 = {C[r]
1:nâˆ’1, i[r]
1:nâˆ’1, Î˜[r]
nâˆ’1}R
r=1, X1:n = {x1:n, S1:n, h}
2: procedure Online Learning(Znâˆ’1, X1:n)
3: Â¯Zn = Zn = âˆ… â–· Initialize set of particles
4: for r = 1 to R do
5: C[r]
n , i[r]
n âˆ¼ p(Cn, in | C[r]
1:nâˆ’1, i[r]
1:nâˆ’1, x1:n, S1:n, h) â–· Sampling
6: Ï‰[r]
n = p(xn, Sn | C[r]
1:nâˆ’1, i[r]
1:nâˆ’1, x1:nâˆ’1, S1:nâˆ’1, h) â–· Importance
7: Î˜[r]
n = E[p(Î˜ | C[r]
1:n, i[r]
1:n, x1:n, S1:n, h)] â–· Calculation of posterior
parameters
8: Â¯Zn = Â¯Zn âˆª âŸ¨C[r]
1:n, i[r]
1:n, Î˜[r]
n , Ï‰[r]
n âŸ©
9: end for
10: for r = 1 to R do
11: draw j with probability âˆ {Ï‰[j]
n } â–· Resampling
12: add âŸ¨C[j]
1:n, i[j]
1:n, Î˜[j]
n âŸ© to Zn
13: end for
14: return Zn
15: end procedure
distribution, qn, as follows1,2:
Cn, in âˆ¼ p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h) (13)
âˆ p(xn | x1:nâˆ’1, i1:n, h)p(Sn | S1:nâˆ’1, C1:n, Î²)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, Î±, Î³).
(14)
Importance Weighting: Weight Ï‰n is expressed as follows:
Ï‰[r]
n = p(C[r]
1:n, i[r]
1:n | x1:n, S1:n, h)
q(C[r]
1:n, i[r]
1:n | x1:n, S1:n, h)
= P[r]
n
Q[r]
n
(15)
where r is the particle number, and R is the number of particles. The subsequent
equations were computed for each particle [ r]; however, the subscripts indicating the
particle number were omitted.
Target distribution Pn can be transformed as follows 3:
Pn âˆ p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)
p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)
p(Sn | S1:nâˆ’1, C1:nâˆ’1, h)Pnâˆ’1. (16)
1The intermediate equation is provided in Appendix A.2.
2Details of the simultaneous sampling of Cn and in are presented in Appendix A.3.
3The intermediate formulas are given in Appendix A.1.
10
The proposal distribution Qn can be transformed as follows:
Qn = p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)| {z }
qn
q(C1:nâˆ’1, i1:nâˆ’1 | x1:nâˆ’1, S1:nâˆ’1, h)| {z }
Qnâˆ’1
(17)
= qnQnâˆ’1. (18)
Here, the proposal distribution qn is the marginal distribution of the parameter set Î˜
of latent variables Cn, in.
From Eqs. (15)â€“(18), weight Ï‰n is expressed as follows:
Ï‰n = p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)p(Sn | S1:nâˆ’1, C1:nâˆ’1, h) Pnâˆ’1
Qnâˆ’1| {z }
Ï‰nâˆ’1
. (19)
The new weight terms for n in Eq. (19) are transformed and marginalized for Cn
and in as shown in Eq. (21). The terms in the sum operation are the same as those in
Eq. (14). These values were proportional to the proposal distribution qn. These have
already been computed when Cn, in is sampled from qn. Therefore, all probabilities
were added before normalization.
p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)p(Sn | S1:nâˆ’1, C1:nâˆ’1, h)
âˆ p(xn, Sn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, S1:nâˆ’1, h) (20)
=
X
Cn
X
in
p(xn | x1:nâˆ’1, i1:n, h)p(Sn | S1:nâˆ’1, C1:n, Î²)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, Î±, Î³).
(21)
Resampling: As shown in lines 10â€“13 of Algorithm 1, the particles are sampled
again as R particles, according to their weights Ï‰n. After resampling, all the particles
have the same weights.
4.3. Active exploration algorithm
The proposed method uses IG, an information-theoretic measure, to select and move
towards a position in an environment that is most likely to reduce uncertainty. More-
over, it obtains observations, including user utterances. The algorithm for the proposed
method is presented as Algorithm 2. The details of the derivation of the algorithm are
provided in Sections 4.3.1 and 4.3.2. We also describe a method for including costs
related to travel distance, as presented in Section 4.3.3.
4.3.1. Formulation of how to select destination
This section discusses, in the first half, the conceptual and basic formulations that un-
derlie active exploration and, in the second half, the practical formulation and deriva-
tion for executing the basic formulation with online learning.
Basic formulation of active exploration: The proposed method uses a for-
mulation that minimizes the KL divergence between the two types of posterior dis-
11
Algorithm 2 SpCoAE: Active exploration algorithm for spatial concept formation.
1: Initialize n0 = âˆ…, Z0 = âˆ…, Xn0 = âˆ…
2: for Ï„ = 1 to T do â–· Number of action trials
3: for all {xa âˆˆ RD | free-space in a map } do â–· Number of candidate positions
4: for r = 1 to R do â–· Number of particles
5: for j = 1 to J do â–· Number of pseudo observations
6: X[r,j]
a âˆ¼ p(Xa | Z[r]
Ï„âˆ’1, Xn0 )
7: end for
8: end for
9: IGa =
RX
r=1
JX
j=1
log p(X[r,j]
a | Z[r]
Ï„âˆ’1, Xn0 )
PR
râ€²=1 p(X[r,j]
a | Z[râ€²]
Ï„âˆ’1, Xn0 )
10: end for
11: aâˆ— = argmaxa(IGa âˆ’ Î· TravelCost(a)) â–· Select position xaâˆ—
12: n0 = n0 âˆª {aâˆ—}
13: Move to position xaâˆ—, and observe words Saâˆ— â–· Observe Xn0
14: ZÏ„ = Online Learning(ZÏ„âˆ’1, Xn0 )
15: end for
tributions4. The first is the final posterior distribution when data are observed N
times, p(Î˜, C1:N , i1:N | x1:N , S1:N , h). The second is the current posterior distribu-
tion when the data are observed as the next destination from the current step,
p(Î˜, C1:N , i1:N | xn0âˆªa, Sn0âˆªa, h). This means determining the next observation so that
the distribution at the current observations makes similar to the final posterior dis-
tribution after all observations. This formulation aims to select data a. The index set
{1 : N} of the data points corresponding to multiple possible destinations for asking
the user a question is defined as the action set for a. In each step, as observed data is
obtained, it is minimized as follows:
minimize
a
DKL[p(Î˜, C1:N , i1:N | x1:N , S1:N , h)âˆ¥p(Î˜, C1:N , i1:N | xn0âˆªa, Sn0âˆªa, h)]. (22)
Note that n0 is the set of already observed data indices; that is, the subset of indices
of the plate representation from 1 to N in the graphical model for spatial concept
formation in Figure 3. In the first step, the case where data is unobserved is n0 = âˆ….
However, Eq. (22) cannot be computed in online learning. This problem must be solved
in some manner. Note that neither true {x1:N , S1:N } nor {xa, Sa} can be observed
before moving to the next destination.
Formulation for computational feasibility: As a suitable alternative, we cal-
culated the expected value of the KL divergence using Eqs. (23)â€“(25) 5 to obtain the
index of the candidate point aâˆ— of the data to be observed at the next destination as
4This formulation is similar to that in MHDP-based active perception/learning methods [12,13].
5The transformation details of Eqs. (23)â€“(25); please refer to Appendix B.1. This derivation is based on
MHDP-based active perception/learning methods [12,13].
12
ğ‘âˆ—
ğ‘âˆ—
ğ‘ ğ‘ ğ‘‹1:ğ‘ ğ‘ ğ‘ ğ‘‹ğ‘›0âˆªğ‘
ğ‘ ğ‘ ğ‘‹ğ‘›0âˆªğ‘ ğ‘ ğ‘ ğ‘‹ğ‘›0
ğ·KL
ğ·KL
ğ‘ ğ‘
ğ‘
ğ‘ ğ‘
ğ‘
Figure 4. Relationship diagram between Eqs. (23) and (24). Each ellipse is drawn as a spatial concept. Top
(Eq. (23)) represents an action decision that moves the distribution when an action is taken (upper right) closer
to the final posterior distribution (upper left). Bottom (Eq. (24)) represents an action decision that results in a
distribution (bottom right) that is the furthest away from the current distribution (bottom right). This implies
that these are equivalent.
follows:
aâˆ— = argmin
a
EX{1:N}\n0 |Xn0
[DKL [p(Z | X1:N )âˆ¥p(Z | Xn0âˆªa)]] (23)
= argmax
a
EXa|Xn0
[DKL [p(Z | Xn0âˆªa)âˆ¥p(Z | Xn0 )]] (24)
= argmax
a
IG(Z; Xa | Xn0 ), (25)
where a âˆˆ {1 : N}\n0 is the index of the data point to be observed at the next
destination, excluding the previously observed data n0. Let Z = {C1:N , i1:N , Î˜},
Xn0 = {xn0 , Sn0 , h}. Figure 4 shows the relationship between Eqs. (23) and (24),
respectively. Eq. (24) expresses the maximization of the expected value of the KL
divergence between the posterior distributions after observing the data at the next
destination and the current step. Based on the information theory, the expected value
of the KL divergence in Eq. (24) is defined as IG in Eq. (25), i.e., IG( Z; Xa | Xn0 ) :=
EXa|Xn0
[DKL [p(Z | Xn0âˆªa)âˆ¥p(Z | Xn0 )]]. IG is known as mutual information [11].
4.3.2. Derivation of approximate computation of IG with particle filter
In this section, we describe a computationally efficient way to approximate IGs that
takes advantage of particle filter results in online learning. The formula for IG in
13
Eq. (25) as follows:
IG(Z; Xa | Xn0 )
=
X
Z
X
Xa

p(Z, Xa | Xn0 ) log p(Z, Xa | Xn0 )
p(Z | Xn0 )p(Xa | Xn0 )

(26)
â‰ˆ
RX
r=1
JX
j=1
ï£®
ï£°Ï‰[r]
n0 log p(X[j]
a | Z[r], Xn0 )
PR
râ€²=1
h
p(X[j]
a | Z[râ€²], Xn0 )Ï‰[râ€²]
n0
i
ï£¹
ï£»,
Z[r] âˆ¼ q(Z | Xn0 ), X [j]
a âˆ¼ p(Xa | Z[r], Xn0 ) (27)
where R is the number of particles, J is the number of pseudo observations, and Ï‰[r]
n0
is the particle weight. The equation above is approximated by sampling based on the
predictive distribution p(Xa | Xn0 ). There are two approximation levels.
First, p(Z | Xn0 ) is approximated by the estimated samples in a particle filter based
on the observed values as follows:
p(Xa | Xn0 ) =
X
Z
[p(Xa | Z, Xn0 )p(Z | Xn0 )] (28)
â‰ˆ
RX
r=1
h
p(Xa | Z[r], Xn0 )Ï‰[r]
n0
i
, Z [r] âˆ¼ q(Z | Xn0 ). (29)
Next, p(Xa | Z[r], Xn0 ) in Eq. (29) can be approximated by sampling J pseudo
observations, where X[j]
a denotes the pseudo observations obtained at the next desti-
nation. This distribution is expressed as follows:
p(Xa | Z, Xn0 ) = p(xa, Sa | Cn0 , in0 , xn0 , Sn0 , h). (30)
When sampling the pseudo-observation X[j]
a , assuming that xa is fixed for some a,
only Sa must be sampled. In this case, Eq. (30) is identical to Eq. (21) for weight
calculation in the particle filters.
X[j]
a = Sa | xa âˆ¼ p(xa, Sa | Cn0 , in0 , xn0 , Sn0 , h) (31)
=
X
Ca,ia
p(Sa | Cn0âˆªa, Sn0 , h)p(xa | in0âˆªa, xn0 , h)p(Ca, ia | Cn0 , in0 , h).
(32)
In conventional similarity methods [12,13], the Monte Carlo approximation is per-
formed again. However, the proposed method can use the estimated results of existing
particle filters, as expressed in Eq. (27). As Z[r], Ï‰[r]
n0 has already been estimated by
the online learning algorithm, we can sample X[j]
a from p(Xa | Z[r], Xn0 ). Finally, we
obtain probability p(X[j]
a | Z[r], Xn0 ) based on the sampled values. Weight Ï‰[r]
n0 can be
treated as a constant 1 /R because it is resampled 6. Thus, computational reuse based
on particle filters in learning can increase computational efficiency.
6Note that the weights need to be used if they are not resampled by devices such as effective sample size [68,69].
14
(a) Simulated home environment
âˆ’6 âˆ’4 âˆ’2 0 2 4 6
x
âˆ’4
âˆ’2
0
2
4
y (b) Map created by SLAM
Figure 5. Experimental environment on SIGVerse in Experiment I (Environment 5)
4.3.3. Introduction of travel distance costs
When selecting a movable/reachable destination and controlling the robot at that
destination, IG (Eq. (27)). However, the cost of movement also needs to be considered.
In this study, the criterion of â€˜ utilityâ€™ that includes the travel cost of distance, which
is used in active SLAM [9], is also introduced into the IG of SpCoAE. Therefore, the
utility function for destination a is defined as
aâˆ— = argmax
a
(IG(Z; Xa | Xn0 ) âˆ’ Î· TravelCost(a)) (33)
where Î· is a parameter of a weighting factor that trades off the cost with the IG
and TravelCost(a) is the travel cost, which is the path length estimated by the A â‹†
algorithm from the current position to the target position, xa, on the map.
Thus, the IG and travel costs can be treated as a tradeoff. Consequently, the des-
tination (a) that maximizes the utility, represented by Eq. (33) can be determined as
the optimal destination xaâˆ—, and the robot can move to that point.
The introduction of travel costs has similar implications for the incorporation of
state transitions, that is, movements and time steps. The graphical model of SpCoAE
does not include SLAM. However, the consideration of state transitions is inherently
desirable because the robot moves to different locations on the map. Theoretically, we
expect the travel cost to act as an approximation of the state transition probabilities
in a graphical model such as SLAM. We believe that a graphical model of SpCoAE
can be integrated into SLAM in the future.
5. Experiment I: Simulator environments
We evaluated whether active exploration using the proposed method enables efficient
spatial concept formation.
5.1. Condition
In this experiment, we used a simulator environment with a virtual Toyota human-
support robot (HSR) [70]. We used a map (Figure 5b) created in advance in the home
environment (Figure 5a) constructed using the SIGVerse [71] simulator as the ex-
perimental environment. SIGVerse is a virtual environment platform for robots that
integrates Unity, a game development engine, andRobot Operating System (ROS) [72],
15
a robot software platform. For mapping, we used the ROS package gmapping7, which
was implemented based on the grid-based FastSLAM 2.0 [42] algorithm. The inputs
for the SLAM were the depth sensor data from the lidar on the robot as the measured
values and odometry data as the control values.
Candidate position coordinates for data observation were set at 0.8 m intervals in
the white free space in the map in Figure 5b. Within this space, if there is a wall or an
obstacle within 0.5 m of the candidate positions, the candidates are deleted to provide
space for the HSR with a diameter of 0.43 m to move. In addition, a single exploration
was performed for each candidate point, and once a candidate point was searched, it
was assumed that it would not be searched again. It is assumed that the travel to a
destination by self-localization and path planning is accurate.
Figure 8a shows the ideal form of the spatial concept and the position distribu-
tion set by the tutor. The color of the candidate points where the data are ob-
served represents the index of the spatial concept Cn, and the color of the ellipses
corresponds to the index of the position distribution in. The ideal model parame-
ters are set such that for the spatial concept index Cn, both the index of the posi-
tion distribution in and the word of the place are assigned one-to-one. Specifically,
it is assumed that the same word is observed at all candidate points and indicated
by the same color. The word given at a candidate point corresponding to place is
Living room, Dining room, Kitchen, Bedroom A, Bedroom B, Bedroom C, Corridor,
Toilet, Bathroom, or Entrance. Bedroom A, Bedroom B, and Bedroom C are pseudo-
representations of the unique names called at certain places in the onsite environment,
e.g., Bobâ€™s room or catsâ€™ playroom.
The number of particles was set toR = 1000, and the number of pseudo-observations
was J = 10. The hyperparameters are set as Î± = 1 .0, Î² = 0 .01, Î³ = 0 .1, m0 =
[0.0, 0.0]T, Îº0 = 0.001, V0 = diag(1.5, 1.5), Î½0 = 4.0. The upper limits for the number
of categories were set as K = 10 and L = 10.
5.2. Comparison methods
The following comparison methods were used:
(A) SpCoAE: The proposed method (IG maximization).
(B) SpCoAE with travel cost: The proposed method (maximization of the IG with
the travel cost). The next destination point is determined using Eq. (33) described
in Section 4.3.3. The weighting factor for the travel cost is Î· = 0.005.
(C) Random: A method to randomly select a destination point with a uniform dis-
tribution.
(D) Travel cost: A method to select a destination point in order to decrease travel
cost.
(E) IG min: A method to select a destination point in order of decreasing IG.
5.3. Evaluation metrics
Evaluation of learning performance : As an evaluation metric, we adopted the
adjusted Rand index (ARI) , which measures clustering performance. An ARI value
close to 0.0 indicates random clustering, whereas an ARI value close to 1.0 indicates
high clustering performance. In this experiment, the ARI value between the estimated
7ROS Wiki: gmapping, http://wiki.ros.org/gmapping
16
Table 2. Evaluation results (mean and standard deviation) of ten trials in each of
the ten environments.
ARI Travel distance
Methods Cn in [grid/step]
SpCoAE 0.942 (0.055) 0.920 (0.067) 48.96 (7.90)
SpCoAE with travel cost 0.953 (0.041) 0.938 (0.055) 22.79 (4.36)
Random 0.878 (0.076) 0.829 (0.087) 71.37 (9.82)
Travel cost 0.936 (0.053) 0.919 (0.066) 12.07 (1.09)
IG min 0.832 (0.094) 0.755 (0.116) 40.86 (6.17)
result for each particle and the ideal clustering result was calculated, and the sum of
the values multiplied by the weight of each particle was evaluated as the weighted ARI.
We evaluated both the index of the spatial concept Cn and the index of the position
distribution in. Because the proposed method involves online learning, two metrics
are used: (i) ARI (step-by-step) for the observed data only at each step and (ii) ARI
(predictive padding) for all data, that is, observed and unobserved data. In predictive
padding, the latent variables for unobserved data are predicted and filled based on the
posterior predictive distribution. This distribution uses the spatial concept parameters
Î˜ = {{Âµk}, {Î£k}, {Ï•l}, {Wl}, Ï€} estimated at each step. The ARI (step-by-step) can
be expected to show a value close to 1.0 in the first few steps, owing to its nature;
however, it does not necessarily indicate a high clustering performance.
Evaluation of learning efficiency: Two metrics were used to evaluate the learn-
ing efficiency. (i) Normalized minimum step above threshold (NMS) is the mini-
mum step that results in ARI (predictive padding) â‰§ 0.6, normalized to [0 , 100] for
the final step. NMS indicates that quick and high learning performance has been
achieved. (ii) Learning stability rate (LSR) is the percentage of steps that result in
ARI (predictive padding) â‰§ 0.6 (i.e., the total number of steps that result in ARI
higher than 0.6 / the final step in each environment). LSR indicates that the learning
performance is stable and high for multiple steps.
The lower the NMS and the higher the LSR, the higher the efficiency of quick en-
vironmental adaptation in learning. For spatial concept formation, an ARI of approx-
imately 0.6 or higher is sufficient to perform well on tasks such as navigation [35,73].
Evaluation of movement efficiency: The travel distance at each step was eval-
uated to compare the travel amount and time spent in the environment. The travel
distance is the total number of cells that move on the occupancy grid map. The move-
ment path is assumed to be obtained by path planning using the A â‹† algorithm from
the current self-position to the destination position.
5.4. Results
Evaluation values for each step : Figure 6 shows the mean and error bars for the
standard deviation of each evaluation value for all ten trials 8. In terms of the ARI
(step-by-step), overall, SpCoAE showed higher values than the other methods. This
result is attributed to the fact that SpCoAE can select a destination that allows spatial
concepts to be learned more accurately in each step. In addition, SpCoAE with travel
cost had consistently higher values than SpCoAE in steps 10 â€“ 40. In terms of the
ARI (predictive padding), SpCoAE had slightly inferior values compared with the
random method in the initial steps, whereas it showed higher values in the later steps.
SpCoAE is more effective for learning precise spatial concepts. In addition, SpCoAE
8The spatial concept learning results and evaluation values per environment for the ten environments are
presented in Appendix D.
17
(a) ARI (step-by-step) of Cn
 (b) ARI (predictive padding) of Cn
 (c) Cumulative travel distance
(d) ARI (step-by-step) of in
 (e) ARI (predictive padding) of in
SpCoAE
SpCoAE_TravelCost
Random
TravelCost
IG_min (f) Legends
Figure 6. Evaluation values for each step (Environment 5)
with travel costs exhibited the highest value in the final step, although it was inferior
to SpCoAE in the first half. These results suggest that searching for one place after
another in the environment is advantageous for predicting unexplored regions during
the learning process. In terms of the cumulative travel distance, introducing travel
cost into SpCoAE resulted in a smaller value. The random method was less efficient
for movement because the travel distance was greater.
Evaluation result of learning efficiency : Figure 7 compares the methods for
LER and NMS with Cn and in. The results show that SpCoAE and the random
method have higher learning efficiencies than the other methods. This indicates that
these methods are effective for rapidly covering an entire environment.
Evaluation values at the last step: Table 2 summarizes the evaluation results for
all ten environments. The travel distance was normalized by the number of candidate
search points (i.e., the number of final steps) to adjust for the impact of the scale of
each environment. Consequently, the proposed method shows a higher ARI than the
other methods; in particular, SpCoAE with travel costs has the highest value. Further-
more, in terms of travel distance, the introduction of travel costs is effective, even in
environments with different structures. Thus, overall, IG-based SpCoAE, specifically
SpCoAE with travel costs, is shown to be effective.
Qualitative results: Figure 8 shows the results of learning spatial concepts for
each exploration method in Environment 5. The proposed methods were closer to
the ideal form than the other methods. However, the Random and IG min methods
have a higher tendency to combine multiple locations into a single category than the
proposed methods. For example, in Figure 8d, the bedroom and entrance, as well as
the hallway and bathroom, had the same distribution. In Figure 8f, a large distribution
can be observed across the bathroom, toilet, hallway, and kitchen. This indicates that
the order of exploration affects the learning of spatial concepts. Figure 9 shows an
example of sequential spatial concept formation constructed using SpCoAE. In Steps
25â€“28, the robot searches the three bottom-left locations consecutively. Steps 36â€“40
18
SpCoAE SpCoAE
_TravelCost
Random TravelCost IG_min
20
40
60
80
100Normalized minimum step above threshold C
*
n. s.
*
*
*
n. s.
*
*  p < 0.01
(a) NMSâ†“ of index Cn
SpCoAE SpCoAE
_TravelCost
Random TravelCost IG_min
20
40
60
80
100Normalized minimum step above threshold i
*
n. s.
*
*
*
*
*
*  p < 0.01 (b) NMSâ†“ of index in
SpCoAE SpCoAE
_TravelCost
Random TravelCost IG_min
0.0
0.2
0.4
0.6
0.8
1.0Learning stability rate C
*
n. s.
*
*
*
n. s.
*
*  p < 0.01
(c) LSRâ†‘ of index Cn
SpCoAE SpCoAE
_TravelCost
Random TravelCost IG_min
0.0
0.2
0.4
0.6
0.8
1.0Learning stability rate i
*
n. s.
*
*
*
*
*
*  p < 0.01 (d) LSRâ†‘ of index in
Figure 7. Box-and-whisker plot of learning efficiency constructed using the evaluation values of ten trials
gathered in each of the ten environments. The up or down arrows point in the direction of the desired value.
The statistical significance between the methods was checked by Welchâ€™s t-test with Bonferroniâ€™s adjustment
for multiple comparisons. A p-value of less than 0.01 was considered statistically significant.
focused on the upper-middle room. This indicates that SpCoAE can sufficiently reduce
the uncertainty regarding a location by searching several times in a row in areas where
the location name is not provided and where there is uncertainty. In other words, the
robot can reduce the entropy of the word distribution for a location by obtaining
multiple instructions in close positions. It then searches for a different location. This
means that the robot has obtained enough observations about the location.
Discussion: According to our results, SpCoAE with travel costs achieved a higher
ARI than SpCoAE. This can be attributed to the exploration tendency of pure IG,
which tends to focus on the outer areas of the environment, potentially hindering
the exploration of the central area. By incorporating the travel cost, we consider
that the exploration candidate points were more evenly distributed throughout the
environment, including in the central areas. Based on the AIF framework, the travel
cost can be considered a pragmatic/extrinsic value with a preference, and can be
regarded as an appropriate approximation of the expected free energy. This finding
highlights the importance of considering both IG and distance in exploration and
learning strategies for effective semantic mapping.
19
(a) Ideal
 (b) SpCoAE
 (c) SpCoAE with travel cost
(d) Random
 (e) Travel cost
 (f) IG min
Figure 8. Examples of learning results for each method (Environment 5): These results are for the highest
weighted particle in the final step. Ellipses drawn on map are position distributions {Âµk, Î£k}. Colors of ellipses
indicate indices i1:n of position distributions. Colors of candidate points indicate indices C1:n of spatial con-
cepts. Colors of each ellipse and each candidate point are randomly determined for each trial. Sub-figure (a)
shows the ideal form envisioned by the tutor. This means that clustering results close to (a) are desirable.
6. Experiment II: Real environment
The effectiveness of the proposed method was investigated in a realistic environment.
This experiment relaxed some of the limitations of Experiment I and demonstrated
the method in a more realistic scenario.
6.1. Condition
A room simulating a home environment was used as the experimental environment.
Figure 10 shows the actual environment and the robot. This experiment was set up in
a more difficult setting than Experiment I owing to the increased uncertainty. First,
the linguistic information provided by the user comprises multiword sentences. We
assume that the sentence is the response given by the user when the robot asks, â€˜Tell
me what kind of place is this?â€™ A different sentence is provided for each candidate
point. Examples of these sentences are as follows: â€˜A low table is placed between the
TV and the sofa, where one can place drinks and other refreshments.â€™ â€˜The room
in which you sleep at night is called a bedroom.â€™ â€˜The bathroom is used after first
filling the bathtub with hot water for washing the body.â€™ This linguistic information is
converted into a bag-of-words representation by preprocessing as follows: (i) Nonletters
have been removed. (ii) The hyphens were removed to separate them before and after.
(iii) All uppercase letters were converted to lowercase letters. (iv) Nouns are unified
into singular forms and (v) Stop words were removed.
Secondly, in this experiment, a single candidate was explored several times as a
challenge. This is because visiting a single candidate point once may not provide
sufficient information. In addition, visiting candidate points with a low priority should
20
Step 36 Step 40
Step 25 Step 28
Figure 9. Learning process of spatial concepts and position distributions by active exploration of the pro-
posed method in several steps (Environment 5). Each point indicates the position from which the robot has
acquired data thus far.
not be explored. In this case, the robot explored up to 100 steps because it could
explore infinitely.
The number of particles was set toR = 1000, and the number of pseudo-observations
was set to J = 10. The hyperparameters are set as follows: Î± = 1 .0, Î² = 0 .1, Î³ =
0.01, m0 = [0.0, 0.0]T, Îº0 = 0.001, V0 = diag(1.0, 1.0), Î½0 = 5.0. The upper limits for
the number of categories were set as K = 10 and L = 10.
6.2. Result
Figure 11 illustrates the learning process for active exploration in SpCoAE. For each
position distribution, a place-related word corresponded to each location. In approxi-
mately 25 steps, the exploration of all rooms was completed, and the word distribution
became stable. For example, â€˜bedroomâ€™ and â€˜sleepâ€™ in the lower right distribution and
â€˜shoeâ€™ and â€˜boxâ€™ in the upper left distribution are learned as words that characterize
places. Not only are place labels predefined, but the proposed method can also appro-
priately assign words that are called in the on-site environment and words related to
the place in the environment.
Figure 12 shows the transition of the IG values for each step. The value of IG forms
several peaks, rising and falling, and then converges to almost 0 after step 80. This
suggests that terminating the number of exploration steps according to the IG may
be effective. In other words, this suggests that the robot may be able to determine
whether or not to explore new areas based on the value of IG.
Furthermore, when the results in Figures 11 and 12 are combined, the following
can be inferred: In the initial steps, after IG decreased, an increase in IG values was
observed as a new distribution was created (e.g., steps 9, 22, and 33). After Step
33, the number of position distributions does not increase, and each is estimated as
an existing distribution. Even after the number of distributions stops increasing, the
area indicated by the position distribution tends to cover more of the entire area as
exploration progresses. As the IG values changed, the word distribution varied. This
may be attributable to the exploration of words that better represent places. Steps
21
(a) HSR [70]
 (b) Real environment
Shelf
Sofa
Desk
Bed
SofaSofa
Bathtub
Shelf
Shoe box
Sink
TV
Low table
BookshelfTable
Chair
Chair
Table
Door (c) Floor plan
Figure 10. Real robot and real experimental environment in Experiment II: The size of the environment is
8 m Ã— 9.4 m with an area of approximately 74 m 2. In this environment, six areas are assumed. The number of
candidate search points is 53.
80â€“100 exhibit little change. Because the candidate point is not explored, even in Step
100, a place is sufficiently informative owing to the already learned spatial concept.
In this experiment, the exploration was repeated for candidate points on an ad-
vanced trial basis. This approach may be particularly effective when dealing with
dynamic environments. In other words, when the robot revisits a previously explored
location, and the environment has changed, the entropy of the locationâ€™s distribution
increases, suggesting a greater possibility that the robot will further explore the area.
In conclusion, we demonstrated that SpCoAE works in real-world environments with
multiword utterances.
7. Conclusion
In this study, we proposed an active exploration algorithm for the spatial concept
formation of a robot. The proposed method achieves AIF by selecting candidate points
with the maximum IG and performing online learning from observations obtained
at the destination. Simulation experiments demonstrate the effectiveness of active
exploration in terms of more accurate spatial concept formation. In addition to the
IG, introducing the cost of travel distance was effective. In real-world experiments
in which multiple words were provided, exploration proceeded to clarify the word
corresponding to a place.
We realized AIF with models in which the position and word observations were used
for place categorization. Further valid categorization using visual images available at
the location, as in SpCoSLAM, is possible. The prospects include the application of
active exploration to the PGM of SpCoSLAM. To this end, we plan to study candidate
selection, including image features and integration with active SLAM in unknown and
unmapped environments.
In this study, the robot learned spatial concepts from scratch by asking the question,
â€˜What kind of place is this?â€™. The use of generalized prior knowledge in multiple envi-
ronments [36â€“38] and large-scale language models [74,75] accelerates learning through
active exploration. This is expected to further reduce the number of questions and
diversify the question generation. Integration with such an approach is an emphasized
22
4
 2
 0 2 4 6
x
8
6
4
2
0
2
y
(a) Step 5
4
 2
 0 2 4 6
x
8
6
4
2
0
2
y (b) Step 20
4
 2
 0 2 4 6
x
8
6
4
2
0
2
y
(c) Step 25
4
 2
 0 2 4 6
x
8
6
4
2
0
2
y (d) Step 80
Figure 11. Process of spatial concept formation by active exploration in several steps (Experiment II).
Top five words according to the value of pointwise mutual information (PMI) with word s for each position
distribution k are obtained, i.e., PMI( Sn = s; in = k) = log( p(Sn = s | in = k, Î˜)/p(Sn = s | Î˜)). PMI
is an indicator that weights word distributions such that the words characteristic for a particular position
distribution exhibit higher values.
future task.
It is also closely related to AIF based on the FEP [5,27] and control as an infer-
ence [76]. We will also consider exploring and learning the applications of these prin-
ciples in the future. This is expected to enable an integrated generalized formulation
that includes not only exploits for learning, but also moves for task execution [77].
Funding
This study was supported by JST CREST, grant number JPMJCR15E3; JST Moon-
shot R&D Program, grant number JPMJMS2011; and JSPS KAKENHI, grant num-
bers JP20K19900 and JP23K16975, Japan.
References
[1] Kostavelis I, Gasteratos A. Semantic mapping for mobile robotics tasks: A survey.
Robotics and Autonomous Systems. 2015;66:86â€“103. Available from: http://dx.doi.
org/10.1016/j.robot.2014.12.006.
[2] Garg S, SÂ¨ underhauf N, Dayoub F, Morrison D, Cosgun A, Carneiro G, Wu Q, Chin TJ,
Reid I, Gould S, Corke P, Milford M. Semantics for Robotic Mapping, Perception and
23
0 20 40 60 80 100
Step
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5Information Gain
Mean of Each Step IG
Max IG
Min IG
Std of Each Step IG
Figure 12. Trends in IG values per step (Experiment II); Maximum, minimum, mean, and standard deviation
of IG values obtained for all candidate search points at each step are drawn.
Interaction: A Survey. Foundations and Trends Â® in Robotics. 2020 jan;8(1â€“2):1â€“224.
2101.00443. Available from: http://arxiv.org/abs/2101.00443http://dx.doi.org/
10.1561/2300000059.
[3] Taniguchi A, Hagiwara Y, Taniguchi T, Inamura T. Online Spatial Concept and Lex-
ical Acquisition with Simultaneous Localization and Mapping. In: Proceedings of the
IEEE/RSJ international conference on intelligent robots and systems (IROS). 2017. p.
811â€“818.
[4] Taniguchi A, Hagiwara Y, Taniguchi T, Inamura T. Improved and Scalable Online Learn-
ing of Spatial Concepts and Language Models with Mapping. Autonomous Robots. 2020;
44(6):927â€“946.
[5] Friston KJ. Active Inference : A Process Theory. Neural Computation. 2017;49:1â€“49.
[6] Friston KJ, Moran RJ, Nagai Y, Taniguchi T, Gomi H, Tenenbaum J. World model
learning and inference. Neural Networks. 2021 dec;144:573â€“590.
[7] Lanillos P, Meo C, Pezzato C, Meera AA, Baioumy M, Ohata W, Tschantz A, Millidge B,
Wisse M, Buckley CL, Tani J. Active Inference in Robotics and Artificial Agents: Survey
and Challenges. arXiv. 2021 dec; 2112.01871. Available from: https://arxiv.org/abs/
2112.01871v1http://arxiv.org/abs/2112.01871.
[8] Thrun S, Burgard W, Fox D. Probabilistic Robotics. Cambridge, MA: MIT Press. 2005.
[9] Stachniss C, Grisetti G, Burgard W. Information Gain-based Exploration Using Rao-
Blackwellized Particle Filters. In: Robotics: Science and systems. Vol. 2. 2005. p. 65â€“72.
[10] Mu B, Giamou M, Paull L, Agha-Mohammadi AA, Leonard J, How J. Information-based
Active SLAM via topological feature graphs. In: 2016 IEEE 55th conference on decision
and control, cdc 2016. Cdc. 2016. p. 5583â€“5590. 1509.08155.
[11] Placed JA, Strader J, Carrillo H, Atanasov N, Indelman V, Carlone L, Castellanos JA.
A Survey on Active Simultaneous Localization and Mapping: State of the Art and New
Frontiers. IEEE Transactions on Robotics. 2023 jul;:1â€“20.
[12] Taniguchi T, Yoshino R, Takano T. Multimodal Hierarchical Dirichlet Process-
Based Active Perception by a Robot. Frontiers in Neurorobotics. 2018 may;
12(MAY):22. 1510.00331. Available from: https://www.frontiersin.org/
article/10.3389/fnbot.2018.00022http://arxiv.org/abs/1510.00331https:
//www.frontiersin.org/article/10.3389/fnbot.2018.00022/full.
24
[13] Yoshino R, Takano T, Tanaka H, Taniguchi T. Active Exploration for Unsupervised Ob-
ject Categorization Based on Multimodal Hierarchical Dirichlet Process. In: IEEE/SICE
international symposium on system integrations (SII). 2021.
[14] Veiga TS, Silva M, Ventura R, Lima PU. A hierarchical approach to active semantic
mapping using probabilistic logic and information reward POMDPs. In: Proceedings in-
ternational conference on automated planning and scheduling (ICAPS). Icaps. 2019. p.
773â€“781.
[15] Chaplot DS, Dalal M, Gupta S, Malik J, Salakhutdinov R. SEAL: Self-supervised Em-
bodied Active Learning using Exploration and 3D Consistency. In: Proceedings of the
advances in neural information processing systems (NeurIPS). Vol. 16. 2021. p. 13086â€“
13098. 2112.01001.
[16] Anderson P, Wu Q, Teney D, Bruce J, Johnson M, SÂ¨ underhauf N, Reid I, Gould S, van den
Hengel A. Vision-and-language navigation: Interpreting visually-grounded navigation in-
structions in real environments. In: Proceedings of the IEEE conference on computer
vision and pattern recognition (CVPR). 2018. p. 3674â€“3683.
[17] Chen K, Chen JK, Chuang J, VÂ´ azquez M, Savarese S. Topological Planning with Trans-
formers for Vision-and-Language Navigation. In: Proceedings of the IEEE computer so-
ciety conference on computer vision and pattern recognition. Nashville, TN, USA. 2021.
p. 11271â€“11281. 2012.05292. Available from: http://arxiv.org/abs/2012.05292.
[18] Park SM, Kim YG. Visual language navigation: a survey and open challenges. Artificial
Intelligence Review 2022. 2022 mar;:1â€“63Available from: https://link.springer.com/
article/10.1007/s10462-022-10174-9 .
[19] Ulker Y, GÂ¨ unsel B, Cemgil T. Sequential Monte Carlo samplers for Dirichlet process
mixtures. In: Proceedings of the international conference on artificial intelligence and
statistics (AISTATS). 2010. p. 876â€“883.
[20] Asada M, Hosoda K, Kuniyoshi Y, Ishiguro H, Inui T, Yoshikawa Y, Ogino M, Yoshida C.
Cognitive developmental robotics: a survey. IEEE Transactions on Autonomous Mental
Development. 2009;1(1):12â€“34.
[21] Cangelosi A, Schlesinger M. Developmental Robotics: From Babies to Robots. Intelligent
Robotics and Autonomous Agents Series. MIT Press. 2015. Available from: https://
books.google.co.jp/books?id=AbKPoAEACAAJ.
[22] Taniguchi T, Piater J, Worgotter F, Ugur E, Hoffmann M, Jamone L, Nagai T, Rosman
B, Matsuka T, Iwahashi N, Oztop E. Symbol Emergence in Cognitive Developmental
Systems: A Survey. IEEE Transactions on Cognitive and Developmental Systems. 2019;
11(4):494â€“516. 1801.08829.
[23] Ha D, Schmidhuber J. World models. 2018. Tech Rep. 1803.10122. Available from:
https://worldmodels.github.io.
[24] Friston KJ, Kilner J, Harrison L. A free energy principle for the brain. Journal of
Physiology-Paris. 2006 jul;100(1):70â€“87.
[25] Friston KJ, Adams RA, Perrinet L, Breakspear M. Perceptions as Hypotheses: Sac-
cades as Experiments. Frontiers in Psychology. 2012;3:151. Available from:https://www.
frontiersin.org/article/10.3389/fpsyg.2012.00151.
[26] Friston KJ. The free-energy principle: a unified brain theory? Nature reviews neuroscience.
2010;11(2):127.
[27] Friston KJ, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G. Active inference
and epistemic value. Cognitive Neuroscience. 2015;6(4):187â€“214.
[28] Rangel JC, Martinez-Gomez J, Garcia-Varea I, Cazorla M. LexToMap: lexical-based topo-
logical mapping. Advanced Robotics. 2017;31(5):268â€“281.
[29] Grinvald M, Furrer F, Novkovic T, Chung JJ, Cadena C, Siegwart R, Nieto J. Volumet-
ric Instance-Aware Semantic Mapping and 3D Object Discovery; Volumetric Instance-
Aware Semantic Mapping and 3D Object Discovery. IEEE Robotics and Automation
Letters. 2019;4(3). Available from: http://www.ieee.org/publications_standards/
publications/rights/index.html.
[30] Rosinol A, Violette A, Abate M, Hughes N, Chang Y, Shi J, Gupta A, Carlone L. Kimera:
25
From SLAM to spatial perception with 3D dynamic scene graphs. The International
Journal of Robotics Research. 2021;40:1510â€“1546. 2101.06894v1. Available from: www.
sagepub.com/https://github.com/MIT-.
[31] Tolman EC. Cognitive maps in rats and men. Psychological Review. 1948;55(4):189â€“208.
[32] Oâ€™keefe J, Nadel L. The Hippocampus as a Cognitive Map. Vol. 27. Cambridge University
Press. 1978.
[33] Taniguchi A, Fukawa A, Yamakawa H. Hippocampal formation-inspired probabilis-
tic generative model. Neural Networks. 2022 mar;151:317â€“335. 2103.07356. Avail-
able from: http://creativecommons.org/licenses/by/4.0/http://arxiv.org/abs/
2103.07356.
[34] Taniguchi A, Isobe S, El Hafi L, Hagiwara Y, Taniguchi T. Autonomous planning based on
spatial concepts to tidy up home environments with service robots. Advanced Robotics.
2021;35(8):471â€“489. 2002.03671.
[35] Taniguchi A, Hagiwara Y, Taniguchi T, Inamura T. Spatial Concept-Based Naviga-
tion with Human Speech Instructions via Probabilistic Inference on Bayesian Gener-
ative Model. Advanced Robotics. 2020 sep;34(19):1213â€“1228. Available from: https:
//www.tandfonline.com/doi/full/10.1080/01691864.2020.1817777.
[36] Katsumata Y, Taniguchi A, El Hafi L, Hagiwara Y, Taniguchi T. SpCoMapGAN : Spatial
Concept Formation-based Semantic Mapping with Generative Adversarial Networks. In:
Proceedings of the IEEE/RSJ international conference on intelligent robots and systems
(IROS). Las Vegas, USA: Institute of Electrical and Electronics Engineers Inc.. 2020 oct.
p. 7927â€“7934.
[37] Katsumata Y, Kanechika A, Taniguchi A, El Hafi L, Hagiwara Y, Taniguchi T. Map
completion from partial observation using the global structure of multiple environmental
maps. Advanced Robotics. 2022 mar;00(00):1â€“12. 2103.09071. Available from: https:
//arxiv.org/abs/2103.09071v1.
[38] Hagiwara Y, Taguchi K, Ishibushi S, Taniguchi A, Taniguchi T. Hierarchical Bayesian
model for the transfer of knowledge on spatial concepts based on multimodal informa-
tion. Advanced Robotics. 2022 mar;36(1-2):33â€“53. 2103.06442. Available from: https:
//arxiv.org/abs/2103.06442v1.
[39] Sagara R, Taguchi R, Taniguchi A, Taniguchi T, Hattori K, Hoguro M,
Umezaki T. Unsupervised lexical acquisition of relative spatial concepts us-
ing spoken user utterances. Advanced Robotics. 2021 jun;36(1-2):54â€“70. 2106.
08574. Available from: https://www.tandfonline.com/action/journalInformation?
journalCode=tadr20https://arxiv.org/abs/2106.08574v1.
[40] Wang X, Huang Q, Celikyilmaz A, Gao J, Shen D, Wang YF, Wang WY, Zhang L. Re-
inforced cross-modal matching and self-supervised imitation learning for vision-language
navigation. In: Proceedings of the IEEE conference on computer vision and pattern recog-
nition (CVPR). Vol. 2019-June. 2019. p. 6622â€“6631. 1811.10092.
[41] Montemerlo M, Thrun S, Koller D, Wegbreit B, Others. FastSLAM 2.0: An improved
particle filtering algorithm for simultaneous localization and mapping that provably con-
verges. In: Proceedings of the international joint conference on artificial intelligence (IJ-
CAI). Acapulco, Mexico. 2003. p. 1151â€“1156.
[42] Grisetti G, Stachniss C, Burgard W. Improving grid-based slam with Rao-Blackwellized
particle filters by adaptive proposals and selective resampling. In: Proceedings of the
IEEE international conference on robotics and automation (ICRA). April. IEEE. 2005.
p. 2432â€“2437.
[43] Yamauchi B. Frontier-based Exploration Using Multiple Robots. In: Agents. 1998. p.
47â€“53. Available from: http://doi.acm.org/10.1145/280765.280773.
[44] Placed JA, Castellanos JA. Fast Autonomous Robotic Exploration Using the Underlying
Graph Structure. In: Proceedings of the IEEE/RSJ international conference on intelligent
robots and systems (IROS). Section IV. 2021. p. 6649â€“6656.
[45] Chaplot DS, Gandhi D, Gupta S, Gupta A, Salakhutdinov R. Learning to Explore using
Active Neural SLAM. In: Proceedings of the international conference on learning repre-
26
sentations (ICLR). 2020. arXiv:2004.05155v1.
[46] C Â¸ atal O, Verbelen T, Van de Maele T, Dhoedt B, Safron A. Robot navigation
as hierarchical active inference. Neural Networks. 2021 oct;142:192â€“204. Available
from: https://linkinghub.elsevier.com/retrieve/pii/S0893608021002021https:
//doi.org/10.1016/j.neunet.2021.05.010.
[47] Asgharivaskasi A, Atanasov N. Semantic OcTree Mapping and Shannon Mutual In-
formation Computation for Robot Exploration. IEEE Transactions on Robotics. 2023;
2112.04063. Available from: https://doi.org/10.1109/TRO.2023.3245986.
[48] Paul R, Arkin J, Aksaray D, Roy N, Howard TM. Efficient grounding of abstract spatial
concepts for natural language interaction with robot platforms. International Journal of
Robotics Research. 2018;37(10):1269â€“1299.
[49] Patki S, Daniele AF, Walter MR, Howard TM. Inferring compact representations for
efficient natural language understanding of robot instructions. In: Proceedings of the
IEEE international conference on robotics and automation (ICRA). 2019. p. 6926â€“6933.
[50] Sugiura K, Iwahashi N, Kashioka H, Nakamura S. Active learning of confidence mea-
sure function in robot language acquisition framework. In: Proceedings of the IEEE/RSJ
international conference on intelligent robots and systems (IROS). 2010. p. 1774â€“1779.
[51] Chen Y, Bordes JBB, Filliat D. An experimental comparison between NMF and LDA for
active cross-situational object-word learning. Proceedings of the Joint IEEE International
Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob). 2016;
:217â€“222.
[52] Nakamura T, Araki T, Nagai T, Iwahashi N. Grounding of Word Meanings in Latent
Dirichlet Allocation-Based Multimodal Concepts. Advanced Robotics. 2011;25(17):2189â€“
2206.
[53] Chaplot DS, Jiang H, Gupta S, Gupta A. Semantic Curiosity for Active Visual Learning.
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial In-
telligence and Lecture Notes in Bioinformatics). 2020;12351 LNCS:309â€“326. 2006.09367.
Available from: http://arxiv.org/abs/2006.09367.
[54] Georgakis G, Bucher B, Schmeckpeper K, Singh S, Daniilidis K. Learning to Map for
Active Semantic Goal Navigation. Proceedings of the International Conference on Learn-
ing Representations (ICLR). 2022 mar;2106.15648. Available from: http://arxiv.org/
abs/2106.15648.
[55] Ramakrishnan SK, Jayaraman D, Grauman K. An Exploration of Embodied Visual Ex-
ploration. International Journal of Computer Vision. 2021;129(5):1616â€“1649.2001.02192.
Available from: http://arxiv.org/abs/2001.02192.
[56] Oliver G, Lanillos P, Cheng G. An Empirical Study of Active Inference on a Humanoid
Robot. IEEE Transactions on Cognitive and Developmental Systems. 2022;14(2):462â€“
471. 1906.03022. Available from: http://arxiv.org/abs/1906.03022{%}0Ahttp://
dx.doi.org/10.1109/TCDS.2021.3049907.
[57] Horii T, Nagai Y. Active Inference Through Energy Minimization in Multimodal Affective
Humanâ€“Robot Interaction. Frontiers in Robotics and AI. 2021 nov;8:361.
[58] Doya K, Ishii S, Pouget A, Rao RPN. Bayesian Brain: Probabilistic Approaches to Neural
Coding. MIT Press. 2007.
[59] Hafner D, Lillicrap T, Fischer I, Villegas R, Ha D, Lee H, Davidson J. Learning latent dy-
namics for planning from pixels. Proceedings of the International Conference on Machine
Learning (ICML). 2019;2019-June:4528â€“4547. 1811.04551.
[60] Okada M, Kosaka N, Taniguchi T. PlaNet of the Bayesians: Reconsidering and Im-
proving Deep Planning Network by Incorporating Bayesian Inference. In: Proceedings of
the IEEE/RSJ international conference on intelligent robots and systems (IROS). 2020.
arXiv:2003.00370v1.
[61] Ball PJ, Holder JP, Pacchiano A, Choromanski K, Roberts S. Ready policy one: World
building through active learning. In: 37th international conference on machine learning,
(ICML). Vol. PartF16814. 2020. p. 568â€“578.2002.02693. Available from: http://arxiv.
org/abs/2002.02693.
27
[62] Rao RPN, Ballard DH. Predictive coding in the visual cortex: A functional interpretation
of some extra-classical receptive-field effects. Nature Neuroscience. 1999 jan;2(1):79â€“87.
Available from: https://pubmed.ncbi.nlm.nih.gov/10195184/.
[63] Murphy KP. Machine learning: a probabilistic perspective. Cambridge, MA: MIT Press.
2012.
[64] Sethuraman J. A constructive definition of Dirichlet priors. Statistica Sinica. 1994;4:639â€“
650.
[65] Ishwaran H, James LF. Gibbs sampling methods for stick-breaking priors. Journal of the
American Statistical Association. 2001;96(453):161â€“173.
[66] Fox EB, Sudderth EB, Jordan MI, Willsky AS, Fox BEB, Sudderth EB, Jordan MI,
Willsky AS. A sticky HDP-HMM with application to speaker diarization. The Annals of
Applied Statistics. 2011;5(2A):1020â€“1056. arXiv:0905.2592v4.
[67] Doucet A, De Freitas N, Murphy K, Russell S. Rao-Blackwellised particle filtering for
dynamic Bayesian networks. In: Proceedings of the 16th conference on uncertainty in
artificial intelligence. Morgan Kaufmann Publishers Inc.. 2000. p. 176â€“183. 1301.3853.
[68] Kong A, Liu JS, Wong WH. Sequential Imputations and Bayesian Missing Data Problems.
Journal of the American Statistical Association. 1994 mar;89(425):278.
[69] Liu JS. Metropolized independent sampling with comparisons to rejection sampling
and importance sampling. Statistics and Computing. 1996;6(2):113â€“119. Available from:
http://130.203.136.95/viewdoc/summary?doi=10.1.1.31.6718.
[70] Yamamoto T, Terada K, Ochiai A, Saito F, Asahara Y, Murase K. Development of Human
Support Robot as the research platform of a domestic mobile manipulator. ROBOMECH
Journal. 2019;6(1):4. Available from: https://doi.org/10.1186/s40648-019-0132-3 .
[71] Inamura T, Mizuchi Y. SIGVerse: A Cloud-Based VR Platform for Research on Multi-
modal Human-Robot Interaction. Frontiers in Robotics and AI. 2021 may;8:158.
[72] Quigley M, Conley K, Gerkey BP, Faust J, Foote T, Leibs J, Wheeler R, Ng AY. ROS:
an open-source Robot Operating System. In: Proceedings of the icra workshop on open
source software. Kobe, Japan. 2009.
[73] Taniguchi A, Ito S, Taniguchi T. Spatial Concept-based Topometric Semantic Mapping
for Hierarchical Path-planning from Speech Instructions. arXiv. 2022 mar; 2203.10820.
Available from: https://arxiv.org/abs/2203.10820v1http://arxiv.org/abs/2203.
10820.
[74] Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam
P, Sastry G, Askell A, Agarwal S, Herbert-Voss A, Krueger G, Henighan T, Child R,
Ramesh A, Ziegler DM, Wu J, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S,
Chess B, Clark J, Berner C, McCandlish S, Radford A, Sutskever I, Amodei D. Language
models are few-shot learners. In: Advances in neural information processing systems.
Vol. 2020-December. Neural information processing systems foundation. 2020 may. p.
1877â€“1901. 2005.14165. Available from: https://commoncrawl.org/the-data/https:
//arxiv.org/abs/2005.14165v4.
[75] Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C, Gopalakr-
ishnan K, Hausman K, Herzog A, Ho D, Hsu J, Ibarz J, Ichter B, Irpan A, Jang E, Ruano
RJ, Jeffrey K, Jesmonth S, Joshi NJ, Julian R, Kalashnikov D, Kuang Y, Lee KH, Levine
S, Lu Y, Luu L, Parada C, Pastor P, Quiambao J, Rao K, Rettinghouse J, Reyes D, Ser-
manet P, Sievers N, Tan C, Toshev A, Vanhoucke V, Xia F, Xiao T, Xu P, Xu S, Yan M,
Zeng A. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. In: arxiv
preprint. 2022 apr. 2204.01691. Available from: http://arxiv.org/abs/2204.01691.
[76] Levine S. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and
Review. arXiv preprint arXiv:180500909. 2018;.
[77] Taniguchi T, El Hafi L, Hagiwara Y, Taniguchi A, Shimada N, Nishiura T. Semiotically
adaptive cognition: toward the realization of remotely-operated service robots for the
new normal symbiotic society. Advanced Robotics. 2021;35(11):664â€“674. Available from:
https://www.tandfonline.com/doi/abs/10.1080/01691864.2021.1928552.
28
Appendix A. Formula derivation in online learning of spatial concepts
A.1. Derivation of target distributionPn
The target distribution Pn is transformed as follows:
Pn = p(C1:n, i1:n | x1:n, S1:n, h) (A1)
= p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)p(C1:nâˆ’1, i1:nâˆ’1 | x1:n, S1:n, h) (A2)
Bayesâ€™ rule
= p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)
p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, S1:n, h)p(C1:nâˆ’1, i1:nâˆ’1 | x1:nâˆ’1, S1:n, h)
p(xn | x1:nâˆ’1, S1:n, h) (A3)
Markovian
âˆ p(Cn, iÏ„ n | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)
p(C1:nâˆ’1, i1:nâˆ’1 | x1:nâˆ’1, S1:n, h) (A4)
Bayesâ€™ rule
= p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)
p(Sn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, S1:nâˆ’1, h)p(C1:nâˆ’1, i1:nâˆ’1 | x1:nâˆ’1, S1:nâˆ’1, h)
p(Sn | x1:nâˆ’1, S1:nâˆ’1, h)
(A5)
Markovian
âˆ p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h)p(xn | C1:nâˆ’1, i1:nâˆ’1, x1:nâˆ’1, h)
p(Sn | S1:nâˆ’1, C1:nâˆ’1, Î±, Î²)Pnâˆ’1. (A6)
A.2. Derivation of proposal distributionqn
The proposal distribution qn is transformed as follows:
qn = p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, x1:n, S1:n, h) (A7)
Bayesâ€™ rule
= p(x1:n | C1:n, i1:n, S1:n, h)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, S1:n, h)
p(x1:n | C1:nâˆ’1, i1:nâˆ’1, S1:n, h) (A8)
Markovian
âˆ p(x1:n | i1:n, h)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, S1:n, h) (A9)
Bayesâ€™ rule
= p(x1:n | i1:n, h)p(S1:n | C1:n, i1:n, h)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, h)
p(S1:n | C1:nâˆ’1, i1:nâˆ’1, h) (A10)
Markovian
âˆ p(x1:n | i1:n, h)p(S1:n | C1:n, Î²)p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, Î±, Î³) (A11)
âˆp(xn | x1:nâˆ’1, i1:n, h)p(Sn | S1:nâˆ’1, C1:n, Î²)
p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, Î±, Î³). (A12)
A.3. Simultaneous sampling ofCn and in
Here, each term of Eq. (14). The probability distribution for xn is
p(xn | x1:nâˆ’1, i1:n, h) = St

xn | mk, Vk(Îºk + 1)
Îºk(Î½k âˆ’ d + 1), Î½k âˆ’ d + 1

(A13)
i
where St() is the Studentâ€™s t-distribution. The parameters of the posterior distribution
of Eq. (A13) is calculated as follows:
Â¯xk = 1
t(k)
nâˆ’1
X
xjâˆˆxk
xj (A14)
mk = t(k)
nâˆ’1Â¯xk + Îº0m0
t(k)
nâˆ’1 + Îº0
(A15)
Îºk = t(k)
nâˆ’1 + Îº0 (A16)
Î½k = Î½0 + t(k)
nâˆ’1 (A17)
Vk = V0 +
X
xjâˆˆxk
xjxT
j + Îº0m0mT
0 âˆ’ ÎºkmkmT
k (A18)
where t(k)
nâˆ’1 denotes the index of the data point assigned to the k-th position distribu-
tion when the n âˆ’ 1-th data point is obtained.
The probability distribution for the bag-of-words Sn is as follows:
p(Sn | S1:nâˆ’1, C1:n, Î²)
=
GY
g=1
p(Sn,g | S1:nâˆ’1, C1:nâˆ’1, Cn = l, Î²) (A19)
=
GY
g=1
 
t(l,g)
nâˆ’1 + Î²
PG
gâ€²=1(t(l,gâ€²)
nâˆ’1 + Î²)
!Sn,g
(A20)
where t(l,g)
nâˆ’1 is the count number of the g-th word sg of the l-th word distribution in
S1:nâˆ’1 when the n âˆ’1-th data point is obtained. In addition, G is the number of word
types (the number of dimensions of the word distribution), and Bt is the number of
words in an utterance.
The probability distribution with respect to in and Cn is
p(Cn, in | C1:nâˆ’1, i1:nâˆ’1, Î±, Î³)
= p(in = k | Cn = l, C1:nâˆ’1, i1:nâˆ’1, Î³)p(Cn = l | C1:nâˆ’1, Î±) (A21)
= t(l,k)
nâˆ’1 + Î³/K
t(l)
nâˆ’1 + Î³
t(l)
nâˆ’1 + Î±/L
tnâˆ’1 + Î± (A22)
where t(l)
nâˆ’1 denotes the count number of data assigned to thel-th spatial concept when
the nâˆ’1-th data point is obtained. In addition, t(l,k)
nâˆ’1 denotes the count number of data
assigned to the l-th spatial concept and k-th position distribution when the n âˆ’ 1-th
data are obtained.
ii
From the above, Eq. (14) can be expressed as follows:
qn âˆ St

xn | mk, Vk(Îºk + 1)
Îºk(Î½k âˆ’ d + 1), Î½k âˆ’ d + 1

GY
g=1
 
t(l,g)
nâˆ’1 + Î²
PG
gâ€²=1(t(l,gâ€²)
nâˆ’1 + Î²)
!Sn,g
t(l,k)
nâˆ’1 + Î³/K
t(l)
nâˆ’1 + Î³
t(l)
nâˆ’1 + Î±/L
tnâˆ’1 + Î± . (A23)
A.4. Posterior distribution of model parametersÎ˜
The first term p(Î˜ | C1:n, i1:n, x1:n, S1:n, h) in Eq. (12) is the posterior distribution of
the parameter Î˜, as follows:
p(Î˜ | C1:n, i1:n, x1:n, S1:n, h)
=
KY
k=1
NIW (Âµk, Î£k | mk, Îºk, Vk, Î½k)
" LY
l=1
Dir(Wl | (t(l,g)
n + Î²)) Dir(Ï•l | (t(l,k)
n + Î³/K))
#
Dir(Ï€ | (t(l)
n + Î±/L)) (A24)
where NIW () denotes a Gaussian inverse Wishart distribution. In practice, this set
of parameters Î˜ is obtained for each particle sampled using the RBPF. Each term in
Eq. (A24) can be computed independently for the corresponding parameters and cate-
gories. In addition, each term is a posterior distribution for each parameter, which can
be computed by the conjugacy of the prior distribution and the likelihood function. To
obtain specific estimates for each parameter, the expected value of each posterior dis-
tribution was obtained. Refer to [63] for the specific formulas for the above probability
distributions.
Appendix B. Formula derivation in SpCoAE
B.1. Derivation of IG for destination selection
This section describes the transformation details of Eqs. (23)â€“(25). A similar derivation
has been employed in MHDP-based active perception/learning methods [12,13]. The
index aâˆ— of the candidate position for exploration observed at the next destination can
be obtained using the expected value of the KL divergence as follows:
aâˆ— = argmin
a
EX{1:N}\n0 |Xn0
[DKL[p(Z | X1:N ), p(Z | Xn0âˆªa)]] (B1)
= argmin
a
X
X{1:N}\n0
X
Z

p(X{1:N}\n0 | Xn0 )p(Z | X1:N ) log p(Z | X1:N )
p(Z | Xa, Xn0 )

. (B2)
In Eq. (B2), the numerator of the log function p(Z | X1:N ) is eliminated because it
iii
does not depend on a, and the denominator and numerator are inverted as follows:
Eq. (B2) = argmax
a
X
X{1:N}\n0
X
Z

p(X{1:N}\n0 | Xn0 )p(Z | X1:N ) logp(Z | Xa, Xn0 )

(B3)
= argmax
a
X
X{1:N}\n0
X
Z

p(Z, X{1:N}\n0 | Xn0 ) logp(Z | Xa, Xn0 )

. (B4)
In Eq. (B4), the replacement of {1 : N}\n0 with a can be expressed as follows:
Eq. (B4) = argmax
a
X
Xa
X
Z
[p(Z, Xa | Xn0 ) logp(Z | Xa, Xn0 )] (B5)
= argmax
a
X
Xa
X
Z
[p(Z, Xa | Xn0 ) logp(Z | Xa, Xn0 )] âˆ’
X
Z
[p(Z | Xn0 ) logp(Z | Xn0 )]
| {z }
Add the constant term
(B6)
= argmax
a
"X
Xa
X
Z
[p(Xa | Xn0 )p(Z | Xn0 , Xa) logp(Z | Xa, Xn0 )]
âˆ’
X
Xa
X
Z
[p(Xa | Xn0 )p(Z | Xn0 , Xa) logp(Z | Xn0 )]
#
(B7)
= argmax
a
X
Xa
p(Xa | Xn0 )
X
Z
p(Z | Xn0 , Xa)log p(Z | Xa, Xn0 )
p(Z | Xn0 )
| {z }
KL divergence
(B8)
= argmax
a
X
Xa
p(Xa | Xn0 )DKL[p(Z | Xa, Xn0 )âˆ¥p(Z | Xn0 )] (B9)
= argmax
a
EXa|Xn0
[DKL[p(Z | Xn0âˆªa)âˆ¥p(Z | Xn0 )]] (B10)
= argmax
a
IG(Z; Xa | Xn0 ). (B11)
Based on information-theoretic definitions, the deformation from Eq. (B8) to Eq. (B9)
uses the definition of the KL divergence and deformation from Eq. (B10) to Eq. (B11)
defines the IG. IG is known as mutual information [11].
B.2. Another derivation: entropy-based IG derivation
The algorithm for deriving the IG calculation using an entropy-based procedure similar
to the active SLAM [9] is shown in Algorithm 3. The derivation of the algorithm is
presented in this section.
The IG function IG( Z; Xa | Xn0 ) in terms of the entropy difference is as follows:
IG(Z; Xa | Xn0 ) =H(p(Z | Xn0 )) âˆ’ H(p(Z | Xn0âˆªa)) (B12)
=H(p(Î˜, C1:n, i1:n | xn0 , Sn0 , h))
âˆ’ H(p(Î˜, C1:n, i1:n | xn0âˆªa, Sn0âˆªa, h)) (B13)
iv
Thus, Eq. (25) becomes
Eq. (25) = argmax
a
[H(p(Z | Xn0 ))| {z }
Constant
âˆ’H(p(Z | Xn0âˆªa))] (B14)
= argmin
a
H(p(Z | Xn0 , Xa)). (B15)
Thus, H(p(Z | Xn0 , Xa)) can be expressed using the definition of conditional entropy
as follows:
H(p(Z | Xn0 , Xa)) =
X
Xa
[p(Xa | Xn0 )H(p(Z | Xn0 , Xa))] (B16)
â‰ˆ
JX
j=1
H(p(Z | Xn0 , X[j]
a )), X [j]
a âˆ¼ p(Xa | Xn0 ). (B17)
Therefore, from Eqs. (B17) and (29), H(p(Z | Xn0 , Xa)) is expressed as:
H(p(Z | Xn0 , Xa)) â‰ˆ
RX
r=1
ï£®
ï£°Ï‰[r]
n0
JX
j=1
H(p(Z | Xn0 , X[j]
a ))
ï£¹
ï£»,
Z[r] âˆ¼ q(Z | Xn0 ), X [j]
a âˆ¼ p(Xa | Z[r], Xn0 ). (B18)
In addition, H(p(Z | Xn0 , X[j]
a )) is
H(p(Z | Xn0 , X[j]
a )) = H(p(Î˜, C1:n, i1:n | xn0 , Sn0 , x[j]
a , S[j]
a , h)) (B19)
â‰ˆ H(p(Cn0âˆªa, in0âˆªa | x[j]
n0âˆªa, S[j]
n0âˆªa, h))
+
RX
râ€²=1
Ï‰[râ€²,j]
n0âˆªaH(p(Î˜ | C[râ€²,j]
n0âˆªa, i[râ€²,j]
n0âˆªa, x[j]
n0âˆªa, S[j]
n0âˆªa, h)). (B20)
This suggests that entropy is computed after turning the particle filters using the
pseudo-observation X[j]
a = {x[j]
a , S[j]
a }.
B.3. Exploration of active inference algorithms in two derivations
Algorithm 2 is based on the IG-based active perception methods [12,13], and Algo-
rithm 3 is based on the method proposed for active SLAM [9]. These two algorithms
are identical, except for lines 9 and 11 in Algorithm 2 and lines 7, 10, and 12 in Al-
gorithm 3. In Algorithm 2, the calculation results in line 7 can be reused in line 10.
Compared with Algorithm 3, which requires entropy calculations, Algorithm 2 is easier
to implement and faster. Although Algorithm 3 appears to require more processing
in line 8, the results of the processing in line 7 can be directly reused in the sam-
pling and weight calculations of the online learning algorithm in line 8. In addition, if
Xaâˆ— = {xaâˆ—, Saâˆ—} = X[r,j]
a , the estimated value, Â¯Z[r,j]
Ï„ , can be reused as ZÏ„ .
This study demonstrated the theoretical connectivity between SpCoAE and active
SLAM. In the future, Algorithm 3 will be more compatible with the theoretical integra-
tion of SpCoAE and active SLAM. However, because the two algorithms were derived
v
Algorithm 3 Entropy-based active exploration algorithm for spatial concept forma-
tion.
1: Initialize n0 = âˆ…, Z0 = âˆ…, Xn0 = âˆ…
2: for Ï„ = 1 to T do â–· The number of action trials
3: for all {xa âˆˆ RD | free-space in a map } do â–· The number of candidate
positions
4: for r = 1 to R do â–· The number of particles
5: for j = 1 to J do â–· The number of samples
6: X[r,j]
a âˆ¼ p(Xa | Z[r]
Ï„âˆ’1, Xn0 )
7: Â¯Z[r,j]
Ï„ = Online Learning(ZÏ„âˆ’1, {Xn0 , X[r,j]
a })
8: end for
9: end for
10: Ha =
RX
r=1
JX
j=1
H(p(Z | Xn0 , X[r,j]
a ))
11: end for
12: aâˆ— = argmina Ha â–· Select position xaâˆ—
13: n0 = n0 âˆª {aâˆ—}
14: Move to the position xaâˆ—, and observe words Saâˆ— â–· Observe Xn0
15: ZÏ„ = Online Learning(ZÏ„âˆ’1, Xn0 )
16: end for
from the same IG, an integrated reinterpretation based on Algorithm 2 was possible.
This allows for an almost intact extension of the algorithm and implementation.
Comparing the proposed and conventional methods, the number of samples K in
the Monte Carlo approximation [12,13] corresponds to R when J = 1 in the proposed
algorithm. Therefore, the proposed algorithm is more generalizable. Furthermore, as
described in Section 4.3.2, the Monte Carlo approximation is described in detail [12,13].
However, the proposed method can use the estimation results of the existing particle
filters. In addition, whereas [12,13] assumed batch learning for Gibbs sampling for
each exploration, the proposed method is computationally more efficient than existing
methods because of online learning using particle filters.
Appendix C. Preliminary experiment: Comparison of accuracy and
computation time for spatial concept learning
The purpose of this experiment was to compare the accuracies of spatial concept
learning and computation time when the number of particles and pseudo-observations
in SpCoAE were varied.
C.1. Condition
Table C.1 lists the number of particles R and the number of pseudo-observations
J for each experimental pattern. Ten trials were conducted for each experimental
pattern. Environment 5 was used as the experimental environment in this study. The
conditions related to the candidate points were identical to those in Experiment I. A
total of 103 candidates were available for observation. In all the experimental patterns,
The hyperparameters were set as Î± = 1.0, Î²= 0.01, Î³ = 0.1, m0 = [0.0, 0.0], Îº0 =
vi
Table C1. Experimental condition pattern.
Pattern Number of particles (R) Number of pseudo-observations (J)
A 1000 1
B 100 1
C 10 1
D 1 1
E 1000 10
F 100 10
G 10 10
H 1 10
K 1500 10
L 500 10
0.001, V0 = diag(1 .5, 1.5), Î½0 = 4 .0. The upper limits for the number of categories
were set as K = 10 and L = 10. The simulations were performed on a laptop with
the following specifications: Intel Core i7-7820HK CPU, 32 GB of DDR4 memory, and
Nvidia GeForce GTX 1080 GPU. The robot was operated using the ROS kinetic Kame
running on Ubuntu 16.04 LTS.
The evaluation metrics were the same as those in Experiment I. In addition, the
computation time required for each step and the computation time required for all
steps were evaluated. The computation time excludes the time required to obtain
observations or travel time.
C.2. Results
Figure C1 shows examples of the learning results of the spatial concept index C and
the position distribution for each position coordinate in experimental patterns Aâ€“H.
Additionally, the color changed randomly for each trial because the category numbers
assigned to index C and index i changed randomly. For each experimental pattern,
the mean and standard deviation of each evaluation value for all ten trials are shown
in the graphs below.
Figure C2 shows the results of comparing experimental patterns Eâ€“H with R =
(1000, 100, 10, 1) number of particles and J = 10 pseudo-observations. In terms of
the weighted ARI for indices C and i in experimental patterns Aâ€“H, Figures C2(a)
and C2(b) indicate that the higher the number of particles, the higher the value from
approximately step 30 to the final step. In addition, an example of the learning results
(Figure C1) indicates that the higher the number of particles, the closer the result
is to an ideal position distribution. Figures C2(c) and C2(d) show that the larger
the number of particles, the longer is the computation time required. These results
confirm a tradeoff between the number of particles and the accuracy of spatial concept
learning.
The comparison of experimental results A and E with the number of particles R =
1000 and the number of pseudo-observations J = (1 , 10) are shown in Figure C3.
The results of comparing experimental patterns C and G with the number of particles
R = 10 and J = (1, 10) are shown in Figure C4. These results show that, regardless of
the number of particles, the difference in the number of pseudo-observations has little
effect on the accuracy and computation time of spatial concept learning. This can be
attributed to the present condition setting in which one word is assigned. The effect of
the number of pseudo-observations is more pronounced when there are many-to-many
correspondences between places and words.
Finally, we show the results of the experiments when the number of particles was
1500, 500, based on the experimental pattern with the number of particles R = 1000,
vii
Wednesday 31st May, 2023 Advanced Robotics output
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(a) A ( R= 1000, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (b) B ( R= 100, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (c) C ( R= 10, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (d) D ( R= 1, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(e) E ( R= 1000, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (f) F ( R= 100, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (g) G ( R= 10, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (h) H ( R= 1, J= 10)
Figure C1. Examples of learning results for position distributions and index Cn of spatial concept for each coordinate in
experimental patterns Aâ€“H.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(b) ARI (step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
10
20
30
40
50
60
70
80Execute time of each step (sec)
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(c) Computation time re-
quired for each step
E (p=1000,o=10)F (p=100,o=10) G (p=10,o=10) H (p=1,o=10)
Experiment pattern
0
1000
2000
3000
4000Execute time (sec)
4596.1
459.7
47.2 5.6
(d) Cumulative computation
time required for all steps
Figure C2. Weighted ARI values and computation time in experimental patterns Eâ€“H.
(1000, 100, 10, 1) number of particles and J = 10 pseudo-observations. In terms of the weighted
ARI for indices C and i in experimental patterns Aâ€“H, Figures C2(a) and C2(b) indicate that
the higher the number of particles, the higher the value from approximately step 30 to the ï¬nal
step. In addition, an example of the learning results (Figure C1) indicates that the higher the
number of particles, the closer the result is to an ideal position distribution. Figures C2(c) and
C2(d) show that the larger the number of particles, the longer is the computation time required.
These results conï¬rm a tradeoï¬€ between the number of particles and the accuracy of spatial
concept learning.
The comparison of experimental results A and E with the number of particles R= 1000 and
the number of pseudo-observationsJ = (1, 10) are shown in Figure C3. The results of comparing
experimental patterns C and G with the number of particles R= 10 and J = (1, 10) are shown
in Figure C4. These results show that, regardless of the number of particles, the diï¬€erence in the
number of pseudo-observations has little eï¬€ect on the accuracy and computation time of spatial
concept learning. This can be attributed to the present condition setting in which one word is
assigned. The eï¬€ect of the number of pseudo-observations is more pronounced when there are
many-to-many correspondences between places and words.
Finally, we show the results of the experiments when the number of particles was 1500 ,500,
based on the experimental pattern with the number of particles R = 1000, which was the
most accurate in spatial concept learning in previous experiments. The results of comparing
experimental patterns K, E, and L with R= (1500, 1000, 500) number of particles and J = 10
pseudo-observations are shown in Figure C5. The weighted ARI for experimental patterns K, E,
and L, as shown in Figures C5(a) and C5(b) indicate that the ARI of 500 particles is slightly lower
than that of 1500 or 1000 particles. Combining Figures C5(c) and C5(d), when 1500 particles
were used, the computation time was approximately 1.5 times longer than when 1000 particles
were used, even though the accuracies of spatial concept learning were similar. Therefore, the
vii
Figure C1. Examples of learning results for position distributions and index Cn of spatial concept for each
coordinate in experimental patterns Aâ€“H.
Wednesday 31st May, 2023 Advanced Robotics output
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(a) A ( R= 1000, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (b) B ( R= 100, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (c) C ( R= 10, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (d) D ( R= 1, J= 1)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(e) E ( R= 1000, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (f) F ( R= 100, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (g) G ( R= 10, J= 10)
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (h) H ( R= 1, J= 10)
Figure C1. Examples of learning results for position distributions and index Cn of spatial concept for each coordinate in
experimental patterns Aâ€“H.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(b) ARI (step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
10
20
30
40
50
60
70
80Execute time of each step (sec)
E (p=1000,o=10)
F (p=100,o=10)
G (p=10,o=10)
H (p=1,o=10)
(c) Computation time re-
quired for each step
E (p=1000,o=10)F (p=100,o=10) G (p=10,o=10) H (p=1,o=10)
Experiment pattern
0
1000
2000
3000
4000Execute time (sec)
4596.1
459.7
47.2 5.6
(d) Cumulative computation
time required for all steps
Figure C2. Weighted ARI values and computation time in experimental patterns Eâ€“H.
(1000, 100, 10, 1) number of particles and J = 10 pseudo-observations. In terms of the weighted
ARI for indices C and i in experimental patterns Aâ€“H, Figures C2(a) and C2(b) indicate that
the higher the number of particles, the higher the value from approximately step 30 to the ï¬nal
step. In addition, an example of the learning results (Figure C1) indicates that the higher the
number of particles, the closer the result is to an ideal position distribution. Figures C2(c) and
C2(d) show that the larger the number of particles, the longer is the computation time required.
These results conï¬rm a tradeoï¬€ between the number of particles and the accuracy of spatial
concept learning.
The comparison of experimental results A and E with the number of particles R= 1000 and
the number of pseudo-observationsJ = (1, 10) are shown in Figure C3. The results of comparing
experimental patterns C and G with the number of particles R= 10 and J = (1, 10) are shown
in Figure C4. These results show that, regardless of the number of particles, the diï¬€erence in the
number of pseudo-observations has little eï¬€ect on the accuracy and computation time of spatial
concept learning. This can be attributed to the present condition setting in which one word is
assigned. The eï¬€ect of the number of pseudo-observations is more pronounced when there are
many-to-many correspondences between places and words.
Finally, we show the results of the experiments when the number of particles was 1500 ,500,
based on the experimental pattern with the number of particles R = 1000, which was the
most accurate in spatial concept learning in previous experiments. The results of comparing
experimental patterns K, E, and L with R= (1500, 1000, 500) number of particles and J = 10
pseudo-observations are shown in Figure C5. The weighted ARI for experimental patterns K, E,
and L, as shown in Figures C5(a) and C5(b) indicate that the ARI of 500 particles is slightly lower
than that of 1500 or 1000 particles. Combining Figures C5(c) and C5(d), when 1500 particles
were used, the computation time was approximately 1.5 times longer than when 1000 particles
were used, even though the accuracies of spatial concept learning were similar. Therefore, the
vii
Figure C2. Weighted ARI values and computation time in experimental patterns Eâ€“H.
which was the most accurate in spatial concept learning in previous experiments. The
results of comparing experimental patterns K, E, and L with R = (1500, 1000, 500)
number of particles and J = 10 pseudo-observations are shown in Figure C5. The
weighted ARI for experimental patterns K, E, and L, as shown in Figure C5(a) and
C5(b) indicate that the ARI of 500 particles is slightly lower than that of 1500 or 1000
particles. Combining Figures C5(c) and C5(d), when 1500 particles were used, the
computation time was approximately 1.5 times longer than when 1000 particles were
used, even though the accuracies of spatial concept learning were similar. Therefore,
the number of particles, R = 1000, and the number of pseudo-observations, J = 10
were determined in Experiment I.
The experiment was performed on a single-core CPU to verify the computational
speed. In reality, it is possible to parallelize and implement each sample during Monte
Wednesday 31st May, 2023 Advanced Robotics output
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
A (p=1000,o=1)
E (p=1000,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
A (p=1000,o=1)
E (p=1000,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
10
20
30
40
50
60
70
80Execute time of each step (sec)
A (p=1000,o=1)
E (p=1000,o=10)
(c) Computation time re-
quired for each step
A (p=1000,o=1) E (p=1000,o=10)
Experiment pattern
0
1000
2000
3000
4000Execute time (sec)
4602.1 4596.1
(d) Cumulative computation
time required for all steps
Figure C3. Weighted ARI values and computation time in experimental patterns A and E.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
C (p=10,o=1)
G (p=10,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
C (p=10,o=1)
G (p=10,o=10)
(b) ARI (step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Execute time of each step (sec)
C (p=10,o=1)
G (p=10,o=10)
(c) Computation time re-
quired for each step
C (p=10,o=1) G (p=10,o=10)
Experiment pattern
0
10
20
30
40
50Execute time (sec)
47.4 47.2
(d) Cumulative computation
time required for all steps
Figure C4. Weighted ARI values and computation times in experimental patterns C and G.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
20
40
60
80
100
120Execute time of each step (sec)
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(c) Computation time re-
quired for each step
K (p=1500,o=10) E (p=1000,o=10) L (p=500,o=10)
Experiment pattern
0
1000
2000
3000
4000
5000
6000
7000Execute time (sec)
6927.0
4596.1
2309.0
(d) Cumulative computation
time required for all steps
Figure C5. Weighted ARI values and computation time in experimental patterns of K, E, L.
number of particles, R= 1000, and the number of pseudo-observations, J = 10 were determined
in Experiment I.
The experiment was performed on a single-core CPU to verify the computational speed. In
reality, it is possible to parallelize and implement each sample during Monte Carlo sampling
using particle ï¬lters. Therefore, in an actual operation, parallelization allows for accelerated
calculations.
viii
Figure C3. Weighted ARI values and computation time in experimental patterns A and E.
viii
Wednesday 31st May, 2023 Advanced Robotics output
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
A (p=1000,o=1)
E (p=1000,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
A (p=1000,o=1)
E (p=1000,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
10
20
30
40
50
60
70
80Execute time of each step (sec)
A (p=1000,o=1)
E (p=1000,o=10)
(c) Computation time re-
quired for each step
A (p=1000,o=1) E (p=1000,o=10)
Experiment pattern
0
1000
2000
3000
4000Execute time (sec)
4602.1 4596.1
(d) Cumulative computation
time required for all steps
Figure C3. Weighted ARI values and computation time in experimental patterns A and E.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
C (p=10,o=1)
G (p=10,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
C (p=10,o=1)
G (p=10,o=10)
(b) ARI (step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Execute time of each step (sec)
C (p=10,o=1)
G (p=10,o=10)
(c) Computation time re-
quired for each step
C (p=10,o=1) G (p=10,o=10)
Experiment pattern
0
10
20
30
40
50Execute time (sec)
47.4 47.2
(d) Cumulative computation
time required for all steps
Figure C4. Weighted ARI values and computation times in experimental patterns C and G.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
20
40
60
80
100
120Execute time of each step (sec)
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(c) Computation time re-
quired for each step
K (p=1500,o=10) E (p=1000,o=10) L (p=500,o=10)
Experiment pattern
0
1000
2000
3000
4000
5000
6000
7000Execute time (sec)
6927.0
4596.1
2309.0
(d) Cumulative computation
time required for all steps
Figure C5. Weighted ARI values and computation time in experimental patterns of K, E, L.
number of particles, R= 1000, and the number of pseudo-observations, J = 10 were determined
in Experiment I.
The experiment was performed on a single-core CPU to verify the computational speed. In
reality, it is possible to parallelize and implement each sample during Monte Carlo sampling
using particle ï¬lters. Therefore, in an actual operation, parallelization allows for accelerated
calculations.
viii
Figure C4. Weighted ARI values and computation times in experimental patterns C and G.
Wednesday 31st May, 2023 Advanced Robotics output
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
A (p=1000,o=1)
E (p=1000,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
A (p=1000,o=1)
E (p=1000,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
10
20
30
40
50
60
70
80Execute time of each step (sec)
A (p=1000,o=1)
E (p=1000,o=10)
(c) Computation time re-
quired for each step
A (p=1000,o=1) E (p=1000,o=10)
Experiment pattern
0
1000
2000
3000
4000Execute time (sec)
4602.1 4596.1
(d) Cumulative computation
time required for all steps
Figure C3. Weighted ARI values and computation time in experimental patterns A and E.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
C (p=10,o=1)
G (p=10,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
C (p=10,o=1)
G (p=10,o=10)
(b) ARI (step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Execute time of each step (sec)
C (p=10,o=1)
G (p=10,o=10)
(c) Computation time re-
quired for each step
C (p=10,o=1) G (p=10,o=10)
Experiment pattern
0
10
20
30
40
50Execute time (sec)
47.4 47.2
(d) Cumulative computation
time required for all steps
Figure C4. Weighted ARI values and computation times in experimental patterns C and G.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(a) ARI (step-by-step) of in-
dex Cn
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(b) ARI (Step-by-step) of in-
dex in
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Step
0
20
40
60
80
100
120Execute time of each step (sec)
K (p=1500,o=10)
E (p=1000,o=10)
L (p=500,o=10)
(c) Computation time re-
quired for each step
K (p=1500,o=10) E (p=1000,o=10) L (p=500,o=10)
Experiment pattern
0
1000
2000
3000
4000
5000
6000
7000Execute time (sec)
6927.0
4596.1
2309.0
(d) Cumulative computation
time required for all steps
Figure C5. Weighted ARI values and computation time in experimental patterns of K, E, L.
number of particles, R= 1000, and the number of pseudo-observations, J = 10 were determined
in Experiment I.
The experiment was performed on a single-core CPU to verify the computational speed. In
reality, it is possible to parallelize and implement each sample during Monte Carlo sampling
using particle ï¬lters. Therefore, in an actual operation, parallelization allows for accelerated
calculations.
viii
Figure C5. Weighted ARI values and computation time in experimental patterns of K, E, L.
Carlo sampling using particle filters. Therefore, in an actual operation, parallelization
allows for accelerated calculations.
ix
(a) Environment 1
 (b) Environment 2
 (c) Environment 3
 (d) Environment 4
 (e) Environment 5
(f) Environment 6
 (g) Environment 7
 (h) Environment 8
 (i) Environment 9
 (j) Environment 10
Figure D1. Home environments created with SIGVerse in Experiment I (ten environments)
Appendix D. Results of Experiment I in ten environments
In this section, we present the results of Experiment I using ten simulated home
environments created in SIGVerse as the experimental environment. The experimental
setup is shown in Figure D1. Maps previously created by SLAM were used in all
the environments. Examples of the learning results for all the methods are shown in
Figures D2â€“D11. The index of the spatial concept is colored according to the position
coordinates where the data are observed in each environment. Each ellipse represents
the position distribution. The colors of each ellipse and candidate point were randomly
determined for each trial. Sub-figure (a) shows the ideal form envisioned by the tutor
for reference. Finally, the progress of the results of the evaluation values in the ten
experimental environments is shown in Figure D12.
x
Wednesday 31st May, 2023 Advanced Robotics output
(a) Environment 1
 (b) Environment 2
 (c) Environment 3
 (d) Environment 4
 (e) Environment 5
(f) Environment 6
 (g) Environment 7
 (h) Environment 8
 (i) Environment 9
 (j) Environment 10
Figure D1. Home environments created with SIGVerse in Experiment I (ten environments)
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D2. Examples of learning results in environment 1: Position distributions and indices C1:n of spatial concepts drawn
on the map.
Appendix D. Results of Experiment I in ten environments
In this section, we present the results of Experiment I using ten simulated home environments
created in SIGVerse as the experimental environment. The experimental setup is shown in Fig-
ure D1. Maps previously created by SLAM were used in all the environments. Examples of the
learning results for all the methods are shown in Figures D2â€“D11. The index of the spatial
concept is colored according to the position coordinates where the data are observed in each
environment. Each ellipse represents the position distribution. The colors of each ellipse and
candidate point were randomly determined for each trial. Sub-ï¬gure (a) shows the ideal form
envisioned by the tutor for reference. Finally, the progress of the results of the evaluation values
in the ten experimental environments is shown in Figure D12.
ix
Figure D2. Examples of learning results in environment 1: Position distributions and indices C1:n of spatial
concepts drawn on the map.
Wednesday 31st May, 2023 Advanced Robotics output
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D3. Examples of learning results in environment 2: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D4. Examples of learning results in environment 3: Position distributions and indices C1:n of spatial concepts drawn
on the map.
x
Figure D3. Examples of learning results in environment 2: Position distributions and indices C1:n of spatial
concepts drawn on the map.
xi
Wednesday 31st May, 2023 Advanced Robotics output
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D3. Examples of learning results in environment 2: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D4. Examples of learning results in environment 3: Position distributions and indices C1:n of spatial concepts drawn
on the map.
x
Figure D4. Examples of learning results in environment 3: Position distributions and indices C1:n of spatial
concepts drawn on the map.
Wednesday 31st May, 2023 Advanced Robotics output
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D5. Examples of learning results in environment 4: Position distributions and indices C1:n of spatial concepts drawn
on the map.
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(a) Ideal
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (b) SpCoAE
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (c) SpCoAE with travel cost
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(d) Random
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (e) Travel cost
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (f) IG min
Figure D6. Examples of learning results in environment 5: Position distributions and indices C1:n of spatial concepts drawn
on the map.
xi
Figure D5. Examples of learning results in environment 4: Position distributions and indices C1:n of spatial
concepts drawn on the map.
xii
Wednesday 31st May, 2023 Advanced Robotics output
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D5. Examples of learning results in environment 4: Position distributions and indices C1:n of spatial concepts drawn
on the map.
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(a) Ideal
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (b) SpCoAE
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (c) SpCoAE with travel cost
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y
(d) Random
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (e) Travel cost
8
 6
 4
 2
 0 2 4 6
x
4
2
0
2
4
y (f) IG min
Figure D6. Examples of learning results in environment 5: Position distributions and indices C1:n of spatial concepts drawn
on the map.
xi
Figure D6. Examples of learning results in environment 5: Position distributions and indices C1:n of spatial
concepts drawn on the map.
Wednesday 31st May, 2023 Advanced Robotics output
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y
(a) Ideal
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (b) SpCoAE
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y
(d) Random
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (e) Travel cost
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (f) IG min
Figure D7. Examples of learning results in environment 6: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (f) IG min
Figure D8. Examples of learning results in environment 7: Position distributions and indices C1:n of spatial concepts drawn
on the map.
xii
Figure D7. Examples of learning results in environment 6: Position distributions and indices C1:n of spatial
concepts drawn on the map.
xiii
Wednesday 31st May, 2023 Advanced Robotics output
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y
(a) Ideal
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (b) SpCoAE
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y
(d) Random
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (e) Travel cost
4
 2
 0 2 4 6
x
6
4
2
0
2
4
6
y (f) IG min
Figure D7. Examples of learning results in environment 6: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
4
2
0
2
4
6
y (f) IG min
Figure D8. Examples of learning results in environment 7: Position distributions and indices C1:n of spatial concepts drawn
on the map.
xii
Figure D8. Examples of learning results in environment 7: Position distributions and indices C1:n of spatial
concepts drawn on the map.
Wednesday 31st May, 2023 Advanced Robotics output
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D9. Examples of learning results in environment 8: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D10. Examples of learning results in environment 9: Position distributions and indices C1:n of spatial concepts
drawn on the map.
xiii
Figure D9. Examples of learning results in environment 8: Position distributions and indices C1:n of spatial
concepts drawn on the map.
xiv
Wednesday 31st May, 2023 Advanced Robotics output
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(a) Ideal
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (b) SpCoAE
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (c) SpCoAE with travel cost
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y
(d) Random
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (e) Travel cost
8
 6
 4
 2
 0 2 4
x
4
2
0
2
4
y (f) IG min
Figure D9. Examples of learning results in environment 8: Position distributions and indices C1:n of spatial concepts drawn
on the map.
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D10. Examples of learning results in environment 9: Position distributions and indices C1:n of spatial concepts
drawn on the map.
xiii
Figure D10. Examples of learning results in environment 9: Position distributions and indicesC1:n of spatial
concepts drawn on the map.
Wednesday 31st May, 2023 Advanced Robotics output
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(a) Ideal
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (b) SpCoAE
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (c) SpCoAE with travel cost
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y
(d) Random
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (e) Travel cost
8
 6
 4
 2
 0 2 4
x
6
4
2
0
2
4
6
y (f) IG min
Figure D11. Examples of learning results in environment 10: Position distributions and indices C1:n of spatial concepts
drawn on the map.
xiv
Figure D11. Examples of learning results in environment 10: Position distributions and indices C1:n of
spatial concepts drawn on the map.
xv
Wednesday 31st May, 2023 Advanced Robotics output
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50 60 70 80
Step
0
1000
2000
3000
4000
5000
6000Cumulative Travel Distance
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50 60
Step
0
1000
2000
3000
4000Cumulative Travel Distance
0 20 40
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50
Step
0
500
1000
1500
2000
2500Cumulative Travel Distance
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50 60
Step
0
1000
2000
3000
4000Cumulative Travel Distance
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100
Step
0
2000
4000
6000
8000
10000Cumulative Travel Distance
0 20 40 60 80 100 120
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100 120
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100 120
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100 120
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100 120
Step
0
2000
4000
6000
8000
10000
12000Cumulative Travel Distance
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50 60 70
Step
0
1000
2000
3000
4000
5000
6000Cumulative Travel Distance
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80
Step
0
1000
2000
3000
4000
5000
6000Cumulative Travel Distance
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 10 20 30 40 50 60 70
Step
0
1000
2000
3000
4000
5000Cumulative Travel Distance
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index C
0 20 40 60 80 100
Step
0.0
0.2
0.4
0.6
0.8
1.0Weighted ARI of index i
0 20 40 60 80 100
Step
0
2000
4000
6000
8000
10000Cumulative Travel Distance
SpCoAE SpCoAE_TravelCost Random TravelCost IG_min
Figure D12. Evaluation values in ten environments: from left to right, ARI (step-by-step) values of C and i, ARI (predictive
padding) values of C and i, and cumulative travel distance.
xv
Figure D12. Evaluation values in ten environments: from left to right, ARI (step-by-step) values of C and
i, ARI (predictive padding) values of C and i, and cumulative travel distance.
xvi