January 18, 2023 Advanced Robotics main
To appear in Advanced Robotics
V ol. xx, No. xx, Month 20xx, 1–28
Survey Paper
World Models and Predictive Coding for Cognitive and Developmental Robotics:
Frontiers and Challenges
Tadahiro Taniguchia∗, Shingo Muratab, Masahiro Suzukic, Dimitri Ognibened,e, Pablo Lanillosf ,g, Emre Ugurh,
Lorenzo Jamonei, Tomoaki Nakamuraj, Alejandra Ciriak, Bruno Laral, and Giovanni Pezzulom
aDepartment of Information Science and Engineering, Ritsumeikan University,
1-1-1 Noji-Higashi, Kusatsu, Shiga, Japan
bKeio University, Japan
cThe University of Tokyo, Japan
dUniversit`a Milano-Bicocca, Italy
eUniversity of Essex, UK
f Donders Institute for Brain, Cognition and Behaviour, Netherlands
gCajal International Neuroscience Center, Spanish National Research Council, Spain.
hBogazici University, Turkey
iQueen Mary University of London, UK
jThe University of Electro-Communications, Japan
kNational Autonomous University of Mexico, Mexico
lUniversidad Aut´onoma del Estado de Morelos, Mexico
mInstitute of Cognitive Sciences and Technologies, National Research Council of Italy, Italy
(v1.0 released April 2021)
Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills con-
tinuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim
is to create robots that can continuously develop through interactions with their environment, their learning pro-
cesses should be based on interactions with their physical and social world in the manner of human learning and
cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and
predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest
in artiﬁcial intelligence. Cognitive systems learn world models to better predict future sensory observations and
optimize their policies, i.e., controllers. Alternatively, in neuroscience, predictive coding proposes that the brain
continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment.
Both ideas may be considered as underpinning the cognitive development of robots and humans capable of con-
tinual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics
and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics
has rarely been discussed. Therefore, in this paper, we clarify the deﬁnitions, relationships, and status of current
research on these topics, as well as missing pieces of world models and predictive coding in conjunction with
crucially related concepts such as the free-energy principle and active inference in the context of cognitive and
developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and pre-
dictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive
and developmental capabilities in the future.
Keywords: World model; cognitive robotics; predictive coding; free-energy principle; active inference; deep
generative models
∗Corresponding author. Email: taniguchi@ci.ritsumei.ac.jp
1
arXiv:2301.05832v1  [cs.RO]  14 Jan 2023
January 18, 2023 Advanced Robotics main
1. Introduction
How can we develop robots that can autonomously explore the environment, acquire knowledge, and
learn skills continuously? Creating autonomous cognitive and developmental robots that can co-exist in
our society has been considered an ultimate goal of cognitive and developmental robotics and artiﬁcial
intelligence (AI) since the inception of these ﬁelds. Autonomous robots that can develop in the real world
and collaborate with us may also be called embodied artiﬁcial general intelligence (AGI). The recent suc-
cess of artiﬁcial intelligence depends primarily on large-scale human-annotated data. However, human
infants can acquire knowledge and skills from sensorimotor information through physical interactions
with their environment and social interactions with others (e.g., their parents or caregivers). Importantly,
the aim is to build robots that can continuously develop through embodied interactions, their learning
process must be strongly based on their own sensorimotor experiences. This autonomous learning pro-
cess that occurs throughout development is also referred to as continual or lifelong learning [1–3], and is
considered the foundation for the emergence of both individual and social abilities necessary for robots
with adaptive and collaborative capabilities.
Recently, world models have attracted renewed attention in artiﬁcial intelligence [4–7]. Now, the term
“world” does not indicate the objective world but rather refers to a world understood from a robot’s
point of view 1. This idea corresponds to that of Umwelt proposed by Uexk ¨ull [9]. Umwelt, literally
around-world, meaning environment or surroundings, refers to the self-centered world of an organism
perceived through its species-speciﬁc sensors2. Therefore, notably, the world model is different from the
bird’s-eye model of the world that was aimed to build in good-old-fashioned AI and criticized later [12]3.
A cognitive system learns a world model to predict its future sensory observations better and optimize
its policies, also referred to as controllers. Note that although typically the term “world model” is used
to denote the spatiotemporal dynamics of the external environment, it could also equally apply to bod-
ily dynamics (including interoceptive signals from inside the body) and the social environment. This
world-model view entails previous ideas and results, such as the effect of behavioral feedback on sen-
sory sampling and perceptual learning [14] and the resulting acquisition of self-centered, yet efﬁcient,
representations induced by an active perception strategy on the part of an agent [15].
Predictive coding is another related theory that recently has become more and more inﬂuential [16]. It
is heavily inﬂuenced by Helmholtz’s early theories of perception as a process driven by learning, knowl-
edge, and inference [17]. Predictive coding proposes that the brain infers the external causes of sensations
by continuously predicting its input through top-down signals and adapts to minimize prediction error
[18, 19]. This substantiates the idea that the brain might use an adaptive world model to support per-
ception. The free energy principle (FEP) also proposes a similar vision. It argues that our brain supports
both perception (perceptual inference) and action (active inference) using a form of variational Bayesian
inference; in particular, using (variational) free energy, it assesses the quality of the prediction and its
conformity to prior beliefs [20]. These ideas, which are currently inﬂuential in neuroscience and cogni-
tive science, are also used in cognitive and developmental robotics, neurorobotics [21–23], and artiﬁcial
intelligence to develop neurodynamics realizing adaptive behaviors and social perception [24].
Although such a learning-driven world model-based approach is promising in cognitive and devel-
opmental robotics, the many applications and studies of world models tend to be limited to simulation
studies or adopt an ofﬂine pretrained world model [25]. Meanwhile, many studies based on predictive
coding have been conducted in the ﬁeld of cognitive robotics and neurorobotics. However, the rela-
tionship between the world model-based approach in AI and the predictive coding-based approach in
robotics has rarely been discussed in an integrated manner. We believe that clarifying the deﬁnition,
1This viewpoint may be called a robot’s subjective point of view of the world. Philosophically, however, whether robots can have a “subjective”
point of view remains controversial [8]. Therefore, we describe this point of view simply as “a robot’s point of view”.
2Importantly, the relationship between Umwelt and world modeling was suggested in semiotics. Sebeok pointed out that the closest equivalent
of Umwelt in English is “model” [10]. An Umwelt is created and constructed through a functional cycle, which includes 1) anticipation
of a perceptual cue, 2) perception, 3) working out a relation between the perception and action (either simply executing a habit or using
representation, or modeling anew), and 4) action (operation) [11].
3Also, notably, the world-model approach is different from behavior-based robotics [13], which does not learn world models.
2
January 18, 2023 Advanced Robotics main
Figure 1. Overview of challenges and relationships between topics described in this survey. A robot, similar to a human, receives sensations
x, infers internal states z, exhibits actions a, and affects causes E in the social and physical environment. The initial problem is determining the
models and architecture that a robot uses to efﬁciently and effectively learn latent representations. An approach to this problem uses a neuro-
symbolic predictive model, which combines neural network and symbolic models. The notion ofobject affordance highlights the importance of
object-centric representation learning and the coupling of action and perception. Social interaction with other agents is also an important area
of research. Developing artiﬁcial intelligence for cognitive and developmental autonomous robots based on knowledge of neuroscience, i.e.,
brain-inspiredworld models, is promising. Creating cognitive architecture and developing and sharing software frameworks for this purpose
will also be an important frontier.
relationship, current state of the art, notable research gaps in work on world models, predictive cod-
ing, free-energy principle, and active inference in the context of cognitive and developmental robotics
is important for further progress in this ﬁeld. Based on the current status, we elucidate the frontiers and
challenges toward this holy grail in cognitive and developmental robotics.
In this survey paper, we aim to build bridges and clarify the challenges and frontiers of world models
and predictive coding in cognitive robotics. The remainder of this paper is structured as follows. Section
2 provides a working deﬁnition of each key concept. Section 3 describes prior works related to the
concepts and clariﬁes state of the art. Section 4 describes some notable challenges. Some additional
discussion is provided in Section 5, and we conclude the work in Section 6.
2. Working deﬁnition
2.1 World model
World models describe theinternal models of an agent, which encodes how world states evolve, respond
to agents’ actions, and relate to a given sensory input [4, 26]. The term world model dates back to the
beginnings of artiﬁcial intelligence and robotics [27]. Early research in machine learning studied how
an agent could independently acquire and adapt a world model to [28, 29]. Currently, it usually refers to
predictive models [30], which are mainly encoded using deep neural networks.
In recent years, advancements in the studies on deep neural networks have enabled self-supervised
(or unsupervised) learning 4 of large-scale world models directly from observations (sensory in-
puts) [26, 31, 32], and these models have been applied in various areas of artiﬁcial intelligence, including
4Self-supervised learning is a type of unsupervised learning that aims to accomplish a task by learning to predict or classify any part of
unsupervised data from any other part. In contrast to self-supervised learning, unsupervised learning includes clustering.
3
January 18, 2023 Advanced Robotics main
reinforcement learning (RL). World models allow agents to perform a sample-efﬁcient prediction of the
present state of the world and enable the prediction of future states, which further enables efﬁcient plan-
ning (equivalent to model-based reinforcement learning or control). A compact internal representation
further enables planning in an efﬁcient low-dimensional space.
The key elements of world models areprediction and inference5. Prediction is the probabilistic process
of generating the observation x given the state (or representation) z, whereas inference is the process of
obtaining a state representation z from an observation x in a probabilistic manner. In real-world settings,
observations x are large (high-dimensional, e.g., images) and provide only partial information about the
world (partial observability). At the same time, the latent representation z is assumed to represent the
internal state of the world. In a static case, these can be summarized as the generative processes of
probability distributions as follows.
Prediction: x ∼p(x|z)
Inference: z ∼q(z|x), (1)
where p is a generative model and q is an inference (or recognition) model 6. These models are consid-
ered parameterized in deep neural networks. When these models are trained simultaneously, generative
approaches (e.g., variational autoencoders [38]) are employed [26]. There are also cases where only an
inference model is trained, in which case a discriminative approach (e.g., contrastive learning [39]) is
used7.
In the most common conditions, the state of the environment (and agent body) and the observations
evolve over time in response to the agent’s actions. In such cases, the state z is often assumed to satisfy
Markov conditions. In turn, due to typical sensory limitations such as limited ﬁeld of view or occlusions,
the environment is assumed to follow a partially observable Markov decision process (POMDP) [34].
That is, when the current internal state of the environment iszt (where the subscript represents a discrete
time step), performing an action at causes the internal state to transition to zt+1 and the corresponding
xt+1 is observed. In the POMDP case, the prediction and inference models are given as follows.
Prediction (transition): zt ∼p(zt |zt−1,at−1)
Prediction (generation): xt ∼p(xt |zt )
Inference: zt ∼q(zt |x1:t ,a1:t−1), (2)
where x1:t denotes the set of observations from the ﬁrst step ( x1) to step t (xt ). To learn a state space
model (SSM) on the time interval 1 to T , a variational approach can be adopted that maximizes the
following objective [6, 30, 41]:
log p(x1:T |a1:T −1)
≥
T
∑
t=1
Eq(zt |x1:t ,a1:t−1)[log p(xt |zt )]  
(Negative) prediction error
−Eq(zt−1|x1:t−1,a1:t−2)[DKL[q(zt |x1:t ,a1:t−1)||p(zt |zt−1,at−1)]]  
Regularization
≡
T
∑
t=1
Lt ,
(3)
5The use of these terms is sometimes incongruent in the literature on statistics and machine learning; the wordinference is also used to describe
prediction or substituted by other concepts, such as encoding and decoding in the literature on (variational) autoencoders.
6In classical formulations of control theory [33], AI [34], and probabilistic robotics [35], the inference step is performed by exactly inverting
the prediction probability p using the Bayes theorem. However, this poses several computational challenges and is often intractable. For dealing
with such complexity, sampling-based approximate inference can be performed using Monte Carlo methods [36, 37]. In the context of world
models and predictive coding, variational Bayes approaches are often preferred [20]. Variational Bayes involves the deﬁnition of an approximate
inference distribution ˆq. Amortized inference allows us to approximate the q(x) using a neural network, which is called an inference network,
and obtain an inference model q(x|a) [38].
7However, recently, studies have also shown that contrastive learning can be interpreted as a generative approach [40]. Therefore, the boundary
between generative and discriminative approaches is, to a certain extent, blurred.
4
January 18, 2023 Advanced Robotics main
where the ﬁrst term in Eq. (3) represents the prediction error (or reconstruction error) of the observation,
and the second term represents the regularization for the state representation (so that the transition model
and the inference model yield the same state representation).
2.2 Predictive coding and the free-energy principle
The original predictive coding model provided by Rao and Ballard [18] was proposed as a model of
visual processing in the brain. The model assumes a hierarchically organized neural network, and top-
down and bottom-up interactions at each hierarchical level are considered. In the top-down process,
higher levels generate predictions about lower-level neural activities, and the lowest level generates sen-
sory predictions. In the bottom-up process, residual errors between the predictions and actual activities
(or sensory inputs) are computed and used to correct the originally generated predictions at each level.
Predictive coding models learn spatial and temporal statistical regularities at each level for efﬁcient cod-
ing and to reduce the redundancy of the predicted activity of lower levels [42, 43]. The main principle
behind this hierarchical predictive coding cortical organization in the brain is prediction error minimiza-
tion (PEM) [19, 44, 45]. This idea based on the principle of PEM has been extended to various cognitive
processes, and this framework is usually referred to as predictive processing [16, 46, 47]. This approach
is being recently used also in machine learning to learn robust generative models of data [48].
The principle of PEM can be situated within a more general principle of free-energy minimization
because the amount of variational free energy, the core information measure used in the FEP, can be un-
derstood, under simplifying assumptions, as the amount of prediction error [19, 45, 49]. The variational
free energy Ft , which is an upper bound on the surprise −log p(xt |x1:t−1,a1:t−1), is the negative value of
the evidence lower bound (ELBO) Lt introduced in Eq. (3) as follows.
−Lt = Ft
= −Eq(zt |x1:t ,a1:t−1)[log p(xt |zt )]  
Prediction error
+Eq(zt−1|x1:t−1,a1:t−2)[DKL[q(zt |x1:t ,a1:t−1)||p(zt |zt−1,at−1)]  
Regularization
= DKL[q(zt |x1:t ,a1:t−1)||p(zt |x1:t ,a1:t−1)]  
Divergence
−log p(xt |x1:t−1,a1:t−1)  
Evidence
≥−log p(xt |x1:t−1,a1:t−1)  
Surprise
. (4)
From the second line of Eq. (4), when observations are assumed to follow a Gaussian distribution with
a ﬁxed variance, minimizing the variational free energy is equivalent to minimizing the sum of mean
squared errors and a regularization term.
The FEP is a mathematical formulation of how self-organizing systems, such as biological agents,
brains, and cells, are able to maintain an equilibrium with their environment by means of minimizing
variational free energy, or the surprise associated with sensations 8 [44, 50, 51]. In the FEP, different
cognitive processes such as perception and action can be understood as different ways to minimize the
variational free energy in terms of probabilistic inference called active inference [52–54] as detailed in
the next subsection.
2.3 Active inference and exploration
Active inference is a normative framework that derives from the FEP and provides a unifying account
for perception, control, and learning in terms of minimization of the variational free energy in the past,
8In contrast to states of surprise, free-energy can be measured because it is a function of sensory input and the inferred state [45]
5
January 18, 2023 Advanced Robotics main
present, and future. This uniﬁcation is important in neuroscience as it reﬂects neural mechanisms and
on a computational level because it offers new perspectives and the possibility of sharing algorithmic
solutions between all these functions and transforming them into sophisticated robotic behaviors [23].
For example, perception aims to minimize the variational free energy in the past and present by inferring
the latent representations of observed sensory inputs9, e.g., when an orange appears in the ﬁeld of view
instead of the apple as currently encoded in the internal representation, the representation state can
change toward that of an orange [19]. Conversely, actions try to minimize the variational free energy in
the present by actively sampling sensory inputs, e.g., by moving the gaze away from the orange toward
an apple. In addition to selecting an action in the present, agents can infer a sequence of future actions (or
policy) that elicit the most plausible future states [53–56] by considering the minimization of expected
free energy, as detailed below.
While active inference introduces an important perspective towards an understanding of adaptive and
autonomous behaviors, an obvious behavioral imperative, the exploration-exploitation dilemma, seems
in conﬂict with this idea because exploration, i.e., observing an uncertain aspect of the environment,
would result in obtaining an unpredictable outcome [57, 58]. Indeed exploration and active perception
have a central role in robot control and learning. Several tasks focus on robots’ ability to explore an
unknown environment [59–61]. Furthermore, in social contexts, unobservable factors such as others’
intentions must be actively considered to allow for efﬁcient human-robot collaboration [62–65].
However, in [20, 54, 66], the authors showed that active inference can easily support exploratory
behaviors and that it can provide an elegant formal solution for the exploration-exploitation dilemma.
In fact, we must consider that planning behaviors for an extended period of time require anticipating
future data. More speciﬁcally, to infer the best action sequences (policies), one must also predict the
future observations they would produce. This is realized in the active inference framework by minimizing
the expected free energy over a time interval T . We can express this as the sum of two terms, including
i) the variational information gain term [62, 67–69], or epistemic value [54], deﬁned as the expected
KL divergence between the distribution of the latent states conditioned on the expected observations
q(zt+1:T |xt+1:T ,at:T −1) and the prior distribution on the latent states q(zt+1:T |at:T −1) that represents the
reduction in uncertainty on the latent states zt+1:T provided by the expected observations xt+1:T , and ii)
the extrinsic or pragmatic value log p(xt+1:T |C), where C denotes the agent’s preferences. This results in
the following expression10.
G(at:T −1) =−Eq(zt+1:T ,xt+1:T |at:T −1)[DKL[q(zt+1:T |xt+1:T ,at:T −1)||q(zt+1:T |at:T −1)]]  
E pistemic value
−Eq(xt+1:T |at:T −1)[log p(xt+1:T |C)]  
Pragmatic value
. (5)
The epistemic value term favors obtaining observations that disambiguate the world state such as obtain-
ing the address for the best apple shop in town, versus observations that correspond to multiple (aliased)
state such as corridors in a mall. Without the factor of variational information gain, asking the address
of the shop would not be preferred to any other action that would not immediately result in obtaining
an apple. Thus, minimizing expected free energy corresponds to maximizing the sum of epistemic and
pragmatic values over an extended period and deﬁnes the optimal trade-off between exploration and ex-
ploitation. The similarity between the epistemic value term in Eq. (5) and the divergence term in Eq. (4)
with an inverted sign may be noted. This is due to the different role that observations play in expected
free-energy formulation, where they comprise not observed data but expected observations. Finally, the
9This perception can be regarded as a variational Bayesian version of the original predictive coding that employs a maximum a posteriori
(MAP) estimation [44].
10Note that in the literature on active inference, a sequence of actions at:T −1 is referred to as a policy π. This policy is different from a policy
in reinforcement learning, where it represents a statistical mapping from states to actions ( π(at |st )). Using this notation of the policy π and
mean-ﬁeld approximation, the general formulation of the expected free energy can be described by the following more practical formulation.
G(π) =−∑T
τ=t+1 Eq(xτ |π)[DKL[q(zτ |xτ ,π)||q(zτ |π)]]−Eq(xτ |π)[log p(xτ |C)], where the time step τ > t used here is a future time step.
6
January 18, 2023 Advanced Robotics main
Figure 2. Researches of world models. Left: Dreamer [30] and Dreaming [78]. Right: robot control system using NewtonianV AE [79].
close connection between variational free energy (Eq. (4)), expected free energy (Eq. (5)), used in this
context to deﬁne behaviors with exploration capabilities, and ELBO (Eq. (3)), used to model learning
processes objectives, shows the versatility of this type of formulation, the extension and reﬁnement of
which currently a promising ﬁeld of research that aims to develop an autonomous system with the ability
to efﬁciently acquire and execute complex skills [69]. For a more advanced and detailed presentation,
we refer to [20, 54, 69, 70].
Another important framework that considers behaviors as inference is planning orcontrol as inference
(CaI) [71–75]. The main difference between CaI and active inference is that CaI introduces a binary
optimality variable Ot that represents whether an action at in state zt is optimal (or preferred) [75, 76]. If
the reward for taking actionat in state zt is r (zt ,at ), the conditional distribution of the optimality variable
is deﬁned as follows.
p(Ot = 1 |zt ,at ) ≡exp(r (zt ,at )). (6)
Thus, unlike active inference, CaI can introduce the value of the reward at each time explicitly and
independently of the observation’s generative model11.
CaI aims to obtain the optimal policy p(at |zt ) for inference. If the variational inference is chosen
as a solution to the intractability of exact inference (as with active inference), we seek the policy that
maximizes the following ELBO12.
log p(O1:T ) ≥E∏T
t=1 p(at |zt )p(zt |zt−1,at−1)
[
T
∑
t=1
r (zt ,at )+ H (p(at |zt ))
]
, (7)
where H represents the entropy. This corresponds to the entropy-regularized expected reward, and rein-
forcement learning with this as the objective is called entropy-regularized reinforcement learning [77].
3. Prior works
3.1 World models and model-based reinforcement learning in AI and robotics
In this section, we describe world models used in model-based reinforcement learning in the context of
artiﬁcial intelligence and robotics.
Time-series world models conditioned on behavior have been studied for policy learning for some
time. Schmidhuber proposed learning an agent’s policy (utility) via an RNN-based world model obtained
11Therefore, unlike active inference, CaI does not require the assumption of POMDP.
12Here, MDP is assumed, i.e., state zt is an observed variable rather than a latent variable; if POMDP is assumed, inference and generative
models for observations are added to this ELBO.
7
January 18, 2023 Advanced Robotics main
by self-supervised learning [28]. Based on this idea, Ha et al. introduced a large-scale world model
consisting of a V AE and an RNN that learned directly from observations (time-series images) from the
external world [5]. They showed that the policies of agents trained only on this world model, which
learns a game environment, can work properly in real game environments. Since this study, research
has been conducted on self-supervised learning of models of an environment directly from observations,
along with ideas referred to as “world models”. However, the authors trained spatial compression (V AE)
and temporal transitions (RNN) separately; thus, the perspective of learning state representations was
not considered.
Subsequently, V AE-based models that simultaneously learn time transitions and spatial compression
have been proposed. Kaiser et al. proposed a V AE-based world model with discrete latent variables
designed to predict the next frame and reward from the stacked frames of the previous four steps and
the current action and showed that model-based reinforcement learning using this model performed
adequately in an Atari video game environment with high sample efﬁciency [80]. One limitation of this
model is that it does not include RNNs and cannot account for long-term prediction. Moreover, the
measures are learned from the observation space, so the learned representation is not fully exploited. Ke
et al. showed that learning long-term transitions using a stochastic RNN-based world model contributes
to high performance on tasks that require long-term prediction [81]. All of these models, however, are
autoregressive, requiring the generation of a high-dimensional observation space every step for long-term
prediction, and are unable to transition within the latent space.
Recently, models that learn transitions in latent space without requiring autoregressive generation have
been widely used. Hafner et al. introduced a recurrent state space model (RSSM) that includes RNNs in
SSM and showed that it could be used for long-term prediction and model-based reinforcement learning
with higher performance than model-free learning [41].
While PlaNet [41], the ﬁrst study using RSSM, used an existing model-based planning method (cross-
entropy method) for planning in the latent space13, Dreamer [30], a subsequent method, explicitly mod-
eled the policy and value function in neural networks and learned a world model through gradients in an
actor-critic framework, resulting in a better performance than PlaNet. This model has been further devel-
oped by replacing the latent variables with discrete values, which signiﬁcantly outperformed model-free
performance in Atari game environments (Dreamer V2 [6]), and by using contrastive learning instead of
reconstruction, which resulted in higher performance on tasks that were difﬁcult to reconstruct (Dream-
ing [78], see the left side of Fig. 2). They were also combined and compared (Dreaming V2 [83]).
In terms of obtaining a good state representation for control, enforcing explicit constraints on tran-
sitions is preferable. For example, NewtonianV AE was able to form PD-controllable state space [84].
However, to develop such a model, what kind of state representation the world model should acquire
(as a good representation for control) should be considered, which remains as yet relatively unclear (see
section 4.1 for details).
These world models have been shown to be effective in learning using real robots. Okumura et al.
successfully applied the NewtonianV AE to a robot and enabled it to perform a precise socket insertion
task [79](see the right side of Fig. 2). Wu et al. showed that Dreamer V2 enabled real robots to perform
online learning with very high sample efﬁciency and performance, which includes a pipeline of acquiring
data through interaction with the external world, learning a world model, and controlling the robot using
the model [7]. However, all of these results are for a single environment and task, and what kinds of
world models should be acquired for robot control in diverse environments and tasks remains unclear.
3.2 Predictive coding and active inference in cognitive and developmental robotics
In recent years, an increasing body of research has considered predictive coding models for percep-
tion and action in robotics. Recent comprehensive reviews on active inference and predictive processing
in robotics can be found in [21, 23], respectively. These ideas aim to provide a general mathematical
13PlaNet was extended to be uncertainty-aware on the basis of Bayesian inference [82].
8
January 18, 2023 Advanced Robotics main
Figure 3. Research on predictive coding and active inference in cognitive robotics. Left: adaptation to environmental changes by prediction
error minimization [89]. Right: body perception and action by pixel-based deep active inference [99].
account of behavior. Importantly, they incorporate adaptation and robustness to current methods in cog-
nitive and developmental robotics.
Since the early works of Tani et al. using hierarchically organized RNNs [85], a variety of methods
have been proposed to exploit this idea of prediction-error-minimization or propagation. “Higher levels”
(internal representation) generate predictions about the dynamics of the “lower levels” up to the sen-
sorimotor level. Prediction errors at the sensorimotor level, given the observations, are then propagated
“upwards” in the hierarchy correcting the internal state and thus minimizing the errors. Extensions of
Tani’s approach allow multiple time scales [86–89] (see the left side of Fig. 3), stochasticity [90, 91] and
stochastic latent representations [92]. In particular, a precision-weighting mechanism for the PEM en-
abled robots to extract stochastic or ﬂuctuating structures of temporal sensorimotor sequences and utilize
the extracted structures for their action generation [91]. Interestingly, this mechanism is related to the
precision account in psychiatric disorders [93] (especially autism spectrum disorder [94, 95]) and several
works have proposed cognitive robot models based on aberrant-precision to model unusual perception
and action [96–98].
Aside from hierarchical RNNs, active inference controllers for robotic manipulators [100, 101] and
humanoid robots [22, 99] have also been developed based on Lanillos’s initial work on predictive coding
adaptive perception and learning [102, 103] for both low-dimensional and high-dimensional inputs (see
the right side of Fig. 3). These methods have widespread applications such as object manipulation [104],
imitation [104], language acquisition [105], social interaction [106] and navigation [107].
Cognitive robots beneﬁt from predictive coding mechanisms to infer others’ actions [108]. The reuse
of common circuits for both movement generation and action estimation seems to be a key principle in
the sensorimotor organization. Recently, the authors of [109] proposed deep modality blending networks
(DMBN) designed to create a common latent space from the multi-modal experience of a robot by blend-
ing multi-modal signals with a stochastic weighting mechanism. Using a state-of-the-art skill-encoding
system referred to as Conditional Neural Movement Primitives (CNMPs) [110], they showed that deep
learning could facilitate action recognition and produce structures to sustain anatomical (mirror-like) and
effect-based imitation capabilities when combined with a novel modality-blending scheme.
Current state-of-the-art research is focusing on scaling active inference in planning tasks [51] with
high-dimensional inputs [32, 111] and improving representation learning through multimodal common
latent space [109] or introducing structural inductive biases, such as objects [112]. Whilst active infer-
ence is a promising framework for robotics [113], current works are still limited to a particular aspect
of cognitive and developmental processes. Therefore, in addition to extending the scalability of compu-
tational frameworks, continual or lifelong learning for developing abilities from low-level sensorimotor
skills to higher-order cognitive functions should also be considered.
9
January 18, 2023 Advanced Robotics main
4. Frontiers and Challenges
4.1 Latent representations for action planning
One of the most important challenges in world-model approaches of any kind is that of efﬁciently
performing planning, in the sense of generating meaningful actions to solve a sequential task [114].
Working in the high-dimensional space of the sensorimotor manifold is very computationally expen-
sive and provides local optima solutions [99]. In fact, current approaches in planning use a compressed
encoded representation of the world dynamics, which aids in the process of predicting future states
and in action generation [26]. In reinforcement learning, state representation learning is tied to learned
tasks to achieve high performance because it depends on the actions needed to obtain the maximum
expected reward [30]. However, this sometimes prevents generalization across tasks. Decoupled action-
representation world models are an interesting work-around [39]. In deep active inference [23] amortized
methods have also been considered [32, 115], in addition to contrastive [116] and iterative amortized in-
ference approaches [112].
However, the key question cannot be narrowed down to that what type of architecture or method should
be used. Rather, what type of information should be encoded in the latent representation and how this
information is processed must be a key focus so that information is not uncoupled from the sensorimotor
process, particularly from motor control, which is a key limitation of existing endeavors in robotics.
There has been considerable discussion as to what would comprise an appropriate state representation
of a world model; that is, what inductive bias or prior knowledge should be given [117–119]. Here, we
list the properties of this prior knowledge we consider important.
•Low dimensionality. Observations obtained from the environment are high-dimensional, and
compressing this information into a low-dimensional space is critical for efﬁcient data handling,
abstraction, and planning. This approach is the most frequently considered in state representation
learning. The challenge is how best to represent observations in a low-dimensional encoding while
retaining the necessary task-dependant information. Recent literature focused on generative and
discriminative approaches to tackle this.
•Meaningful abstraction and disentanglement. Low-dimensional representations should have
scene-understanding and task meaning, such as objects [112], locations [120] and temporal
events [121]. Representation disentanglement proposes that factors of variation with different se-
mantics should be separated, contributing to the requirements for sufﬁciency and efﬁciency in state
representation. Object-centric representation learning is related to this hypothesis, [112, 122], in
which every observed object is encoded independently.
•Compositionality. Although disentanglement aims to separate independent factors, the agent
should also acquire their relationships and hierarchy. In the case of object representations, there
should also be relations or implication relations among objects. Currently, methods such as those
using graph neural networks are being considered, but they do not provide an essential solution.
This idea of the compositionality of representation is also relevant to the neuro-symbolic approach.
•Dynamics prediction. These three properties are important not only for learning representations
in static environments but in dynamic worlds, e.g., they consider a transition model that depends
on external factors and agent actions. The best latent representation is one that allows transitions
to be easily predictable for given actions. Many recent models use RNNs to learn transitions,
which incorporate information on long-term dependence [30, 41]. One way to make transitions
more predictable is to incorporate prior knowledge of the physical world (e.g., dynamics follow-
ing Newton’s laws of motion [84]). Furthermore, by learning to separate representations that are
not related to control from state representations, representations that are easier to control can be
acquired [123].
•Values are sufﬁciently encoded.To perform reinforcement learning on the state representation of
the world model, the value of the state representative to the agent must be known. For example,
a recurrent state representation learns to predict the reward from the state so that the reward is
embedded in the representation [30]. However, because the value of the state changes depending
10
January 18, 2023 Advanced Robotics main
on the task, it remains unclear whether this hypothesis should be introduced in a world model that
should acquire a prediction model that is as task-independent as possible. Alternatively, in active
inference approaches, the agent value function cannot be modiﬁed, and it is deﬁned by expected
free energy. Here, the challenge becomes learning the state preferences and being able to predict
the transitions that may yield those preferences.
•Task-agnostic. Representations should be informative to solve narrow problems where the agent
is trained but also sufﬁciently general to be reused in tasks with different kinds of variability or
new tasks that the agent has never encountered.
•Fusion of multiple-types multimodal information. Robots inevitably face a variety of events
with their multimodal sensorimotor systems. Observations given to world models are from mul-
tiple sources (e.g., social non-social, sensorimotor purely sensorial, and linguistic and non-
linguistic). They can have different reliability and volatility and represent various aspects of the
world. Therefore, the world models must properly encode the internal representation in a stable
and efﬁcient manner.
These are some of the elements that we identiﬁed that a latent representation should be fulﬁlled to
provide a smooth connection with real-world interaction and provide power for solving cognitive tasks.
Importantly, abstract representation and disentanglement, such as objects or events encoding, may be
important to achieve efﬁcient planning, reducing the gap for neuro-symbolic solutions. However, the
connection between the low-dimensional (and hierarchical) encoding and the synchronization with the
sensorimotor control remains a major challenge.
4.2 Neuro-symbolic predictive models
In this section, we provide an overview of state-of-the-art techniques in which symbols and rules are
discovered and used by robots through neuro-symbolic approaches. The term symbols, here, refers to
manipulative discrete representations used in symbolic AI and cognitive science. The neuro-symbolic
approach attempts to integrate conventional symbolic and modern neural network-based AIs.
Both biological and artiﬁcial agents beneﬁt from predictive coding mechanisms for reasoning,
decision-making, and planning. Predictive forward models are used to generate plans that involve a
sequence of actions. For example, chimpanzees are known to generate multi-step plans that include
stacking a number of boxes on top of each other, grabbing a long stick, climbing on top of a stack of
boxes, and using the stick to reach the object that was initially out of reach [124, 125]. While the underly-
ing cognitive mechanisms for high-level planning remain unknown, different speciﬁc brain regions have
been shown to become active in inductive and deductive reasoning in humans [126] while predicting the
effects of actions [127]. In artiﬁcial agents, on the other hand, standard search and planning rely heavily
on manually coded or learned state transitions and prediction models [128, Ch. 3–6,10-11].
The seminal work of [129] addressed the learning of discrete representations of predictive models,
i.e., dynamic Bayesian networks, by discretizing the continuous features of the environment to plan
goal-directed arm/hand control. [130] showed the units generated by slow feature analysis with the low-
est eigenvalues resemble symbolic representations that highly correlate with high-level features, which
might be considered precursors for fully symbolic systems. [131, 132] studied methods to discover use-
ful symbols that can be directly utilized in problem and domain deﬁnition language (PDDL) for various
agent settings. In simulation and the real world, the discovered symbols were directly used as predicates
in the action descriptions to generate deterministic and probabilistic symbolic plans. [133] learned sym-
bols in the ego-centric frame of the agent to transfer the learned symbols into novel settings. [134, 135]
discovered symbols in the continuous perceptual space of the robots for PDDL-based manipulation plan-
ning via combining several machine learning algorithms such as X-means clustering and support vector
machine (SVM) classiﬁcation. Although the symbols were discovered by the robot without any human
intervention, the continuous perceptual features were manually encoded by the authors. Towards an end-
to-end framework, [136] used directly raw camera image and pixel values to discover symbols via a novel
deep predictive coding neural architecture. In detail, they proposed a deep encoder-decoder network with
a binary bottleneck layer designed to take a camera image and an action as input and output the action
11
January 18, 2023 Advanced Robotics main
effects in pixel coordinates. The binary activations in the bottleneck layer encode object symbols that
not only depend on the visual input but are also shaped based on action and effect. In other words, the
objects that provide the same affordances [137] were automatically grouped together as object symbols.
To distill the knowledge represented by the neural network into rules useful for symbolic reasoning, a
decision tree was trained to reproduce its decoder function. Probabilistic rules were extracted from the
effect predictor / neural decoder and encoded in the probabilistic PDDL, which can be directly used by
the off-the-shelf AI planners. In follow-up work, [138] used a multi-head attention mechanism to learn
symbols to encode affordances of a varying number of objects.
Asai et al. implemented a neural framework where a state autoencoder with a discrete bottleneck layer
was trained ﬁrst, and preconditions and effects of actions were learned next [139]. In follow-up work,
[140] combined the previous two systems and discovered action preconditions and effects together with
visual symbols. These works were realized in visual environments such as 2-D puzzles and achieved
visualized plan executions. An important aspect of pure visual neuro-symbolic systems and studies on
neuro-symbolic robotics is that in robotics, predictive coding over object symbols takes actions and
effects into account in addition to the features of objects and the environment, which facilitates the
formation of symbols that are likely to capture object affordances [141–144].
However, in the context of world models, methods to integrate prior symbolic knowledge into V AE and
SSM-based world models are still being explored. The bottom-up formation of symbolic representations
is closely related to disentanglement and compositionality discussed in Section 4.1. Moreover, leveraging
linguistic knowledge in world modeling is a challenge in relation to neuro-symbolic predictive models.
4.3 Affordance perception
Affordance perception has often been discussed independently of world models, but in fact, it is closely
related. According to the original deﬁnition provided by Gibson [141, 145, 146], an affordance is an
action possibility offered to the agent by the environment. A stable surface may afford to be traversed; a
stone may afford the possibility of being used as a hammer; a door handle may afford the possibility of
being opened. The concept of affordances and affordance perception has then been further analyzed and
revisited in psychology, neuroscience, cognitive science, artiﬁcial intelligence, and robotics (see [143]
for a recent survey). The ability to perceive affordances is crucial for any biological or artiﬁcial agent to
interact successfully with the environment.
Central to the idea of affordances is that the action possibilities depend on both the agent and the envi-
ronment; the same environment would offer different action possibilities to different agents, depending
on their sensorimotor capabilities. A stable surface affords the possibility of traversal to an agent that
is able to locomote and whose body dimensions ﬁt the size of the surface borders; a stone affords the
possibility of being used as a hammer to an agent who is able to pick it and who has enough force to lift
it; a door handle affords to open to an agent who knows how to open doors, assuming it is well-designed
[147]. Therefore, those affordances must be learned autonomously by the agent. In fact, the agent must
learn how to perceive them. The means by which agents perceive affordances are those of ecological
perception, powerfully illustrated by Eleanor Jack Gibson [148–150]: “narrowing down from a vast
manifold of (perceptual) information to the minimal, optimal information that speciﬁes the affordance
of an event, object, or layout” [150, p.284]. The agent must learn what minimal information is to be
picked; this happens both through evolution and development, leveraging the sensorimotor exploration
of the environment by physical interaction.
Interestingly, while exploring the action possibilities, the agent can learn the effects of those actions
as well. This is crucial for biological agents and turns out to be extremely useful for artiﬁcial systems as
well. In fact, most computational models of affordances in robotics rely on representations that include
not only the action but also the effects (or in other terms, the goal) of the action [108, 110, 151–162];
therefore, the perceived possibilities for actions (and for achieving certain effects) can be used for ac-
tion planning, leading to problem-solving [163]. Such comprehensive models of affordances are, in fact,
world models; they are internal models of how the world behaves “in the eyes” of the learning agent, and
they can be used by the agent to make predictions about how the world will change if certain actions are
12
January 18, 2023 Advanced Robotics main
performed. Therefore, it is not surprising that the computational techniques used for learning affordance
models often overlap with those used for learning world models [164]. It is worth noting that, in world
model approaches, a robot only receives raw sensory information and needs to extract the relevant se-
mantics from such data ﬂow; therefore, to successfully integrate affordance perception in these systems,
the challenges of meaningful abstraction/disentanglement and object-centric representation learning, de-
scribed in Section 4.1, are particularly relevant.
4.4 Social interaction
Robots’ “worlds” do not consist of physical objects alone but also of social entities, i.e., people who give
them social guidance and try to cooperate with them. World models should model and predict social
dynamics involving people’s behaviors, and infer their latent variables, e.g., intentions and emotions, to
cooperate with them, that is, to control social phenomena.
Efﬁcient and safe human-robot collaboration and interaction are some of the main research objec-
tives of robotics and have important practical applications [165–171]. Associating beliefs, intentions,
or mental states to other agents, theory of mind, or, in other words, trying to predict the internal state
of another agent’s world model to understand its activities and context [172], is an essential aspect of
human interaction [173–175] and has attracted attention in robotics [176].
Mutual understanding using a world model in social interaction can play an important role when
complex interactions are challenging the perceptual systems of the agents, inducing a mismatch between
their interpretation of the current context [177, 178]. It is also crucial when different levels of knowledge
and expertise induce different representations of a domain, as well as different points of view, which may
induce conﬂictual interactions [179] or different support strategies [180]. For example, the perspective
of an automotive mechanic and that of an ordinary user differ considerably, so collaboration may be
difﬁcult if one cannot properly infer the internal state of others. A robots’ world model can play a crucial
role in its operation and functionality [181].
The recent progress in machine learning methods has resulted in substantial improvement in action
recognition methodologies [65, 182–184]. However, this approach has often focused on shallow and
purely perceptual representations of the observed activities resulting in limited ﬂexibility in terms of
contexts, tasks, and observed actors demanding a substantial amount of difﬁcult-to-collect data and re-
training time to apply the system in relatively similar conditions [185]. Approaches such as goal recogni-
tion as planning or inverse planning [166, 186–188], that, given a model of the environment, understand
others’ activities by computing plans that would result in the observed actions have shown the ﬂexibility
advantage delivered in intention recognition by a world model. Several works have extended this ap-
proach. The problem of dealing with behaviors generated under partial observability, which may require
inferring both the plan and the beliefs, the mental state [176, 178], of the observed actor, was studied
with both classical planning [189] and Bayesian approaches [177, 190]. The impact of missing obser-
vations for the observer agent has also been analyzed [189]. A further step has been proposed by active
methods for activity recognition [62, 191, 192] that use the same world model both to interpret others’
actions as well as selecting actions that would improve the recognition process, e.g., by giving access to
the most informative observations [63] and allow the completion of a joint task [180]. While even the
initial formulations of this approach were computationally aware [186], their efﬁciency is often affected
by the length of the observed behavior and the environment complexity, resulting in methods that can sel-
dom be applied online on a robot. Several models proposed a pre-compile approach that transformed the
world into a form that would allow efﬁcient plan recognition [185]. The adoption of hierarchical world
model representations has also been considered to constrain the computational and modeling costs of
the process [193–195]. Precomputed and robust local plans, in the form of the same motor controllers
that the robot uses to perform its own actions, have also been adopted to allow active perception for
action recognition and prediction on humanoid robots [62, 172]. One of the main issues of the approach,
also related to computational efﬁciency considerations, is relying on speciﬁc algorithms for planning
that aiming for the optimal plan may misinterpret the bounded rational behaviors that collaborators may
perform. This problem was faced by using online Bayesian inference in [196].
13
January 18, 2023 Advanced Robotics main
The additional ﬂexibility provided by world models in social interaction skills is likely relevant be-
yond activity recognition. It is easy to imagine that purely supervised models may be limited in terms of
perspective-taking and the ability to reason based on the world structure may help to adapt to partners
with different sensory systems [176, 197–199]. Similarly, world models are likely to help with imita-
tion learning by dealing with embodiment mismatch between the observed actor and the learner [200].
Finally, physical cooperation [201, 202] and signaling [203, 204] would also be more ﬂexible when in-
tegrating world and partner models in the equation, for example, to account for the trust of the human
cooperator towards the robot [205]. Finally, a world model may also be learned through socially rich
experiences and sources of information (e.g., imitation [200] or verbal instructions) in addition to the
results of autonomous exploration. However, developing a robust, efﬁcient, and ﬂexible enough repre-
sentation may prove to be one of the main challenges in this effort.
4.5 Brain-inspired world models
In cognitive science, it has long been postulated that the brain learns small-scale models of the world and
uses these models for various cognitive functions, such as perception, planning, and imagination [206].
For example, theories of perception-as-inference described perception as an inferential process, which
works by “inverting” a generative model of how the percepts are generated [207, 208]. As discussed
above, these ideas (and others) have been recently formalized under the label of the Bayesian Brain
[209] and extended by Active Inference from the domain of perception to other domains, such as action
planning and interoception [20].
In parallel, there have been many attempts to describe mathematically and to assess the neuronal
underpinnings of world models and of inference processes empirically (e.g., [210]). One question that
has received a great deal of attention is how the brain might encode internal world models in the neuronal
substrate. Given that the brain models are often assumed to be probabilistic, various formal schemes
have been proposed that describe plausible neuronal implementations of probabilistic variables and of
Bayesian inference over these variables, such as, for example, probabilistic population codes [211] and
sampling schemes [212]. These attempts show that (probabilistic, generative) world models could be
at least potentially implemented in neuronal substrate [213, 214] – and even updated after statistical
learning [215] – but the speciﬁc scheme(s) that the brain might use for this remain to be fully assessed.
Another relevant question is what algorithms the brain might use to perform inference over world
models. A strong candidate in neuroscience is predictive coding [18, 19]. Several studies have aimed
to validate its key empirical predictions, showing that under the appropriate conditions, it is possible to
observe predictions [216], prediction errors [217] and other signatures of inference in brain signals [218]
and that neural activity in lower visual areas in the absence of bottom-up inputs could be explained by the
top-down, feedback dynamics postulated by predictive coding [219]. These and other studies (see [220]
for a recent review) lend some support for predictive coding, but the theory remains under development.
At yet another level, one may ask what the systems-level architecture that supports world models and
whether different parts of the brain might model different aspects of the world is. Anatomical consid-
erations suggest that the brain is not a monolithic entity but rather is composed of several areas and
networks [221]; however, the extent to which these areas or networks are modularized and how they
exactly inﬂuence each other are heavily discussed [222]. One interesting consideration is that cortical
brain areas in humans and monkeys appear to be organized along principal gradients (deﬁned by func-
tional connectivity); in one of these gradients, heteromodal areas (e.g., prefrontal cortex) are placed at
the top, and unimodal areas (e.g., primary visual area) at the bottom, recapitulating the structure of a
putative hierarchical generative model [223]. Another interesting consideration is that there seems to
be a “division of labor” between brain pathways that perform complementary computations, such as
the two visual pathways for processing “what” and “where” information [224]. These anatomical and
functional separations might be potentially interpreted as useful factorizations of the brain generative
models. A whole-brain probabilistic generative model (WB-PGM) approach attempts to build a cog-
nitive architecture for cognitive and developmental robots integrating probabilistic generative model
(PGM)-based modules referring comprehensive knowledge of human and animal brain architectures and
14
January 18, 2023 Advanced Robotics main
their anatomy [225].
The above studies indicate that at a general level, both neuroscience and machine learning / AI con-
ceive world models and inference in similar ways. However, at a more detailed level, there might be
profound differences between the ways these two disciplines use the same concepts. Predictive coding
and other biological schemes proposed in neuroscience exploit top-down dynamics (and recurrences)
in ways that are rarely used in machine learning. Furthermore, brain information processing is heav-
ily based on spontaneous brain dynamics, which are largely absent in machine learning systems; see
[226–228] for a detailed discussion of putative computational roles of spontaneous dynamics. Moreover,
it is plausible to assume that different parts of the brain might be specialized (or might have different
inductive biases) to process different statistical regularities, rendering them able to learn and model (for
example) slower or faster dynamics of the visual scenes, one’s own body, the actions of other agents,
or extended temporal events [229]. It is worth highlighting here that, although prediction errors have
a central role in learning, there are other forms of statistical learning, such as those based on Hebbian
associative learning [230]. It remains to be understood how to best endow our more advanced machine
learning systems with the ability of the brain to perform (apparently) specialized computations but also
orchestrate them coherently. Finally, it is important to remember that the brain is an evolved system, and
our more advanced cognitive abilities are grounded in (the neuronal mechanisms supporting) simpler
sensorimotor skills [231, 232]. Trying to develop advanced cognitive systems without the necessary re-
quirements for embodied interaction and “phylogenetic reﬁnement” might lead to solutions that differ
completely from how the brain works – or that fail altogether.
4.6 Cognitive architectures
Truly cognitive and developmental robots, i.e., embodied AGI, that behave autonomously and ﬂexibly
in the real environment would have a wide range of sensors and exhibit multiple functions. That requires
a large-scale world model that deals with multimodal sensory observations and multilayered state repre-
sentations. Considering the discussion in Section 4.5, such word models may be factorized in a proper
manner from engineering and biological viewpoints. To realize embodied AGIs, further frameworks and
architectures to factorize a total world model into cognitive modules and to integrate individual cogni-
tive capabilities into a cognitive system are required. The idea is related tocognitive architectures, which
have been studied in cognitive science, artiﬁcial intelligence, and robotics [233, 234].
In cognitive science, cognitive functionalities such as memory, perception, and decision-making are
implemented as modules in the cognitive architectures studied, and the speciﬁc task can be solved by
activating these modules coordinately. ACT-R [235] and Soar [236] are representatives of cognitive
architectures. It has been shown that the model implemented by ACT-R can explain the time to solve
the task by humans, and activation patterns of the brain can be predicted by activation patterns of the
modules [237]. Furthermore, Soar has been used for controlling robots [238] and learning games [239].
However, complex machine learning methods that have rapidly advanced in a decade are not introduced
yet. Sigma [240, 241] is a newer cognitive architecture that introduces the generative ﬂow graph, a
generalized probabilistic graphical model. Therefore, the model can be implemented using probabilistic
programming techniques [242–244]. Furthermore, the concept of the standard model of the mind is
discussed through a synthesis across these three cognitive architectures [222]. Particularly, cognitive
architectures based on ﬁrst principles, e.g., with a general computation scheme, such as free energy
minimization [19], are especially attractive. The architecture for social cognition has also been proposed
[245]. The authors point out that these architectures explained above are incomplete in dealing with the
social aspect of cognition and describe the elements of architecture for social cognition. Clarion [246] is
another cognitive architecture based on dual process theory [247]. In this architecture, each subsystem is
composed of explicit and implicit processes, and it is shown that the interaction between implicit-explicit
processes can explain psychological phenomena.
In robotics, several types of cognitive architecture have been proposed. One of them is ArmarX
[248], which has three layers, including a middleware layer, a robot framework layer, and an application
layer. This three-layered structure simpliﬁes the development robotics software easier. (Neuro-)Serket
15
January 18, 2023 Advanced Robotics main
[249, 250] is another approach to integrating cognitive modules 14. In (Neuro-)Serket, modules are de-
scribed by the (deep) PGM and trained mutually by exchanging messages between modules. To make
it easy to develop large-scale models, the modules in Neuro-Serket are weakly connected through the
Serket interface. (Neuro-)SERKET is closely related to the world model-based approach because SER-
KET requires each module to be a PGM, i.e., a model based on prediction and inference as Eq. (1),
and integrate modules into a large PGM. This architecture does not provide any restrictions regard-
ing the functionalities of modules. Therefore, it has high ﬂexibility but brings high dimensional design
space at the same time. To reduce the large degree of freedom in the design space, a brain-inspired
approach, WBA-PGM, was proposed [225]. In this approach, a cognitive model was constructed by con-
necting PGM-based modules utilizing knowledge from neuroscience. By referring to the brain studies,
WBA-PGM constrains the function of modules and their connection and reduces the design space of the
cognitive model.
There are two crucial requirements for cognitive architecture for cognitive and developmental robots,
which can be used along with the approach based on world models and predictive coding. The ﬁrst is
the engineering aspect which is seen in (Neuro-)Serket and ArmarX. The scale of cognitive models that
enables the robots to behave ﬂexibly in the real environment is very large, and many modules must be
connected and work collaboratively. Furthermore, the model needs to introduce machine learning tech-
niques that are not only existing as well as those will be developed in rapid progress. The development
of such a model would require a massive engineering effort, and this is considered a notable obstacle to
realizing such robots. Therefore, architecture is needed to simplify development. Another requirement
is that of the scientiﬁc aspect seen in ACT-R, Soar, WBA-PGM, and Clarion. Developing AGI, which is
human-like intelligence, referring to the knowledge regarding humans obtained in cognitive science and
neuroscience, can accelerate its development. However, meeting these two aspects completely is very
challenging. All machine learning techniques and module connections might necessarily be not reason-
able from the point of view of cognitive science and neuroscience. On the other hand, entire humans
are not understood yet. Therefore, ﬁnding common ground between engineering and science aspects
and developing a novel cognitive architecture is a current challenge. Developing a large-scale cognitive
architecture and overcoming the problems described in the previous subsections is also a challenge.
5. Discussion
As we described, world models and predictive coding are promising approaches in cognitive and devel-
opmental robotics. Before closing this paper, we will mention some remaining issues which have not
been addressed in the main body sufﬁciently.
Language and world models : Umwelts, i.e., worlds from ﬁrst-person views, of biological systems
are not monolithic but have some sort of structure. Notably, language and symbolic systems have syn-
tactic structures. The interaction between high-level cognitive capabilities, e.g., language and reason-
ing, and low-level cognitive capabilities, e.g., perception and action, is essential in world modeling.
Recently, large-scale language models (LLMs) have been replacing many natural language processing
methods [251, 252], including reasoning tasks, which have been conducted solely by symbolic AI by the
end of 2010s [253, 254]. Recently, the use of LLMs in robotics has been attempted, e.g., [255]. It is clear
that language learning and understanding by robots is itself a frontier [256]. To leverage the symbolic
knowledge in LLMs, integration of LLMs and world models will be an important challenge.
This shift from models of artiﬁcial symbols in conventional AI to models of natural language, i.e., a
human symbol system, is resonating with the discussion in symbol emergence in cognitive and devel-
opmental systems [257–259]. An important topic is then considering not only the integration of human
language into robots’ world models in a top-down manner but also the bottom-up formation of symbol
systems, including language in relation to world models.
Policy representations: How should the policies of robots be represented? Conventionally, policies
14Neuro-SERKET is an updated version of SERKET.
16
January 18, 2023 Advanced Robotics main
are described as feedback controllers π(zt ,at ) =p(at |zt ) in reinforcement and imitation learning. Even
though one direction of world model approaches is to explore task agnostic representations (Section
4.1), the decomposition of world modeling and policy learning can be controversial. In a conventional
approach of world models, policies (π = p(at |zt ) or at:T ) and world models ( p(zt+1|zt ,at ) and p(ot |zt ))
are decoupled. In contrast, a series of studies about predictive coding in neuro-robotics have been in-
tentionally entangling policies and world models and making robots directly learn p(ot+1,at+1|o1:t ,a1:t )
and exhibiting many successful results in robotics, e.g.,[24, 260, 261]. As the notion of affordance also
suggests, actions and perceptions are not independent and entangled, generally. The question “to what
extent should we decouple world model and policy representations?” should be investigated.
From artiﬁcial cognition to human cognition : Cognitive and developmental robotics are also con-
structive approaches to human developmental cognition. Not only learning from neuro-, cognitive and
developmental sciences but also provide with scientiﬁc feedback to them is also an important mission.
Building a virtuous circle between studies on human and artiﬁcial studies is a challenge.
The constructive approach may give us a novel approach to scientiﬁc and philosophical hard problems
like self-awareness [262] and consciousness. The relationship between the multimodal world model
and global workspace theory was suggested [263]. Extending the discussion between world models and
consciousness using robots may be an exciting challenge. Moreover, the relationship between predic-
tive coding and emotion is worth exploring to build emotional robots and understand the emotions of
biological systems [264–266].
Software frameworks for implementation: To accelerate the studies on world models and predictive
coding in robotics, the development framework for cognitive and developmental robotics is crucially im-
portant. In robotics, not only AI “software” frameworks but also middle-ware are important. Recently,
ROS has been widely used in the robotics community for bridging hardware and AI software layers. De-
veloping and sharing such software frameworks as a community will be important, e.g., [267]. Moreover,
the world model involves many types of knowledge, and the knowledge can be used for achieving mul-
tiple functions via active inference. The software framework should allow the world model to efﬁciently
organize the knowledge and to perform (cross-modal) active inference. A great initiative is the discrete
state-space active inference python library [268]. However, it can only be used for toy examples due to
scalability issues and to be useful in cognitive and developmental robotics. It needs further development.
For instance, the support for high-dimensional input observations and the possibility of combining dis-
crete and continuous action and state representations are something that has been addressed in robotic
approaches [23].
Data-efﬁcient and autonomous learning: A generalist agent called GATO was developed based on
Transformers and shown to be able to solve various tasks with one neural network [269]. Although the
approach is superﬁcially different, the approach is really related to the world models and the predictive
coding approach. However, the learning system is hugely data-hungry. It is very questionable if the model
can be regarded as a model of human intelligence. Moreover, to train the generalist agent, researchers
need to prepare a large dataset and simulation environment. Human children can autonomously explore
their environment and acquire data through active exploration. Moreover, they use heuristics and biases
in their developmental process. Learning and considering the human developmental process will give us
the inspiration to build real generalist agents. Developing a data-efﬁcient autonomous learning architec-
ture with world models and predictive coding at its core is the key to a truly cognitive and developmental
system.
Emergence of behaviors: Should an agent have a completely internal model of its world? Lastly, we
raise fundamental speculation about the world model-based approach. Behaviors are not externalization
of internally designed trajectories but something to emerge through the interaction between the body and
the environment. For example, it has been proven by passive walking machines that the behavior of walk-
ing emerges only from the interaction between the body and the environment, without any computation
by the brain [270]. About three decades ago, Brooks famously advocated the physical grounding hypoth-
esis together with subsumption architecture, saying the world is its own best model [271]. The robots be-
haved smoothly and ﬂexibly without any explicit world models. This is also referred to as morphological
computation, which means the body itself implicitly processes information dynamically [272, 273]. Soft
17
January 18, 2023 Advanced Robotics main
robotics emphasizes these points nowadays. Combining the viewpoints of the emergence of behaviors
with complex physical dynamics and the world model-based approach is another important challenge.
6. Conclusion
In this survey paper, we have aimed to clarify the frontiers and challenges of world models and predictive
coding in cognitive and developmental robotics. Creating an autonomous robot that can actively explore
the real environment, acquire knowledge, and learn skills continuously is the ultimate goal of cognitive
and developmental robotics. To make the robot continuously develop through active exploration, the
robot’s learning process should be based on sensorimotor information obtained through physical and
social interactions with the physical and social environment. Following the motivation, this paper re-
viewed studies related to world models and predictive coding in cognitive and developmental robotics
and related AI studies. We clariﬁed the deﬁnition of world model and predictive coding in robotics, in
conjunction with those of FEP and active inference, and discussed the relationship between them. We
also introduced state-of-the-art and research gaps of studies on world models and predictive in robotics.
We described six frontiers and challenges, i.e., latent representations for action planning, neuro-symbolic
predictive models, affordance perception, social interaction, brain-inspired world models, and cognitive
architecture. Through the survey and clariﬁcation of challenges, we provided future directions for devel-
oping cognitive and developmental robots based on world models and predictive coding.
Acknowledgements
This work was partially supported by JST Moonshot R&D, Grant Number JPMJMS2033 and JP-
MJMS2011, JST PRESTO Grant Number JPMJPR22C9, the BAGEP Award of the Science Academy,
TUBITAK ARDEB 1001 program (project number: 120E274), the European Union’s Horizon 2020
Framework Programme for Research and Innovation under Speciﬁc Grant Agreements No. 945539 (Hu-
man Brain Project SGA3), No. 952215 (TAILOR), and No. 824153 (POTION), the European Research
Council under the Grant Agreement No. 820213 (ThinkAhead), the Human Brain Project Speciﬁc Grant
Agreement 3 grant (ID 643945539, for the SPIKEFERENCE project), Deepself project under the Pri-
ority Programme “The Active Self” (SPP 2134), the project ‘COURAGE - A social media companion
safeguarding and educating students’ (No. 95563 and No. 9B145), and the V olkswagen Foundation in-
side the initiative Artiﬁcial Intelligence and the Society of the Future.
References
[1] Lungarella M, Metta G, Pfeifer R, Sandini G. Developmental robotics: a survey. Connection science. 2003;
15(4):151–190.
[2] Lesort T, Lomonaco V , Stoian A, Maltoni D, Filliat D, D´ıaz-Rodr´ıguez N. Continual learning for robotics:
Deﬁnition, framework, learning strategies, opportunities and challenges. Information fusion. 2020;58:52–
68.
[3] De Lange M, Aljundi R, Masana M, Parisot S, Jia X, Leonardis A, Slabaugh G, Tuytelaars T. A contin-
ual learning survey: Defying forgetting in classiﬁcation tasks. IEEE transactions on pattern analysis and
machine intelligence. 2021;44(7):3366–3385.
[4] Friston K, Moran RJ, Nagai Y , Taniguchi T, Gomi H, Tenenbaum J. World model learning and inference.
Neural Networks. 2021;144:573–590.
[5] Ha D, Schmidhuber J. World models. arXiv preprint arXiv:180310122. 2018;.
[6] Hafner D, Lillicrap T, Norouzi M, Ba J. Mastering Atari with discrete world models. arXiv preprint
arXiv:201002193. 2020;.
[7] Wu P, Escontrela A, Hafner D, Goldberg K, Abbeel P. Daydreamer: World models for physical robot learn-
ing. arXiv preprint arXiv:220614176. 2022;.
18
January 18, 2023 Advanced Robotics main
[8] Kiverstein J. Could a robot have a subjective point of view? Journal of Consciousness Studies. 2007;
14(7):127–139.
[9] V on Uexk¨ull J. A stroll through the worlds of animals and men: A picture book of invisible worlds. Semiot-
ica. 1992;89(4):319–391.
[10] Sebeok TA. Biosemiotics: Its roots, proliferation, and prospects. Semiotica. 2001;:61–78.
[11] Kull K. Umwelt and modelling. In: The routledge companion to semiotics. Routledge. 2009. p. 65–78.
[12] Brooks R. Intelligence without representation. Artiﬁcial Intelligence. 1991;47(1-3):139–159.
[13] Arkin RC, Arkin RC, et al.. Behavior-based robotics. MIT press. 1998.
[14] Verschure PF, V oegtlin T, Douglas RJ. Environmentally mediated synergy between perception and behaviour
in mobile robots. Nature. 2003;425(6958):620–624.
[15] Ognibene D, Baldassare G. Ecological active vision: four bioinspired principles to integrate bottom–up
and adaptive top–down attention tested with a simple camera-arm robot. IEEE transactions on autonomous
mental development. 2014;7(1):3–25.
[16] Clark A. Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral
and Brain Sciences. 2013;36(3):181–204.
[17] Helmholtz Hv. Handbuch der physiologischen optik, vol. III. Allgemeine Encyklop¨adie der Physik, Leipzig:
Leopold V oss. 1867;.
[18] Rao RP, Ballard DH. Predictive coding in the visual cortex: a functional interpretation of some extra-classical
receptive-ﬁeld effects. Nature Neuroscience. 1999;2(1):79–87.
[19] Friston K. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences. 2005;360(1456):815–836.
[20] Parr T, Pezzulo G, Friston KJ. Active inference: the free energy principle in mind, brain, and behavior. MIT
Press. 2022.
[21] Ciria A, Schillaci G, Pezzulo G, Hafner VV , Lara B. Predictive processing in cognitive robotics: a review.
Neural Computation. 2021;33(5):1402–1432.
[22] Oliver G, Lanillos P, Cheng G. An empirical study of active inference on a humanoid robot. IEEE Transac-
tions on Cognitive and Developmental Systems. 2021;.
[23] Lanillos P, Meo C, Pezzato C, Meera AA, Baioumy M, Ohata W, Tschantz A, Millidge B, Wisse M, Buck-
ley CL, et al.. Active inference in robotics and artiﬁcial agents: Survey and challenges. arXiv preprint
arXiv:211201871. 2021;.
[24] Tani J. Exploring robotic minds: actions, symbols, and consciousness as self-organizing dynamic phenom-
ena. Oxford University Press. 2016.
[25] Ibarz J, Tan J, Finn C, Kalakrishnan M, Pastor P, Levine S. How to train your robot with deep reinforcement
learning: lessons we have learned. The International Journal of Robotics Research. 2021;40(4-5):698–721.
[26] Ha D, Schmidhuber J. Recurrent world models facilitate policy evolution. Advances in neural information
processing systems. 2018;31.
[27] Nilsson NJ, et al.. Shakey the robot. 1984. Tech Rep.
[28] Schmidhuber J. Making the world differentiable: On using self-supervised fully recurrent neural networks
for dynamic reinforcement learning and planning in non-stationary environments. Inst. f¨ur Informatik. 1990.
[29] Sutton RS. Integrated architectures for learning, planning, and reacting based on approximating dynamic
programming. In: Machine learning proceedings 1990. Elsevier. 1990. p. 216–224.
[30] Hafner D, Lillicrap T, Ba J, Norouzi M. Dream to control: Learning behaviors by latent imagination. arXiv
preprint arXiv:191201603. 2019;.
[31] Levine S, Finn C, Darrell T, Abbeel P. End-to-end training of deep visuomotor policies. The Journal of
Machine Learning Research. 2016;17(1):1334–1373.
[32] van der Himst O, Lanillos P. Deep active inference for partially observable mdps. In: Verbelen T, Lanillos
P, Buckley CL, De Boom C, editors. Active inference. Cham: Springer International Publishing. 2020. p.
61–71.
[33] Kalman RE. A new approach to linear ﬁltering and prediction problems. Journal of Basic Engineering
(Transactions of the American Society of Mechanical Engineers). 1960;:35–45.
[34] Cassandra AR, Kaelbling LP, Littman ML. Acting optimally in partially observable stochastic domains. In:
Aaai. V ol. 94. 1994. p. 1023–1028.
[35] Thrun S, Burgard W, Fox D. Probabilistic robotics. MIT Press. 2005.
[36] Chen Z, et al.. Bayesian ﬁltering: From kalman ﬁlters to particle ﬁlters, and beyond. Statistics. 2003;
182(1):1–69.
[37] Thrun S. Particle ﬁlters in robotics. In: Proceedings of the eighteenth conference on uncertainty in artiﬁcial
intelligence. 2002. p. 511–518.
19
January 18, 2023 Advanced Robotics main
[38] Kingma DP, Welling M. Auto-encoding variational bayes. arXiv preprint arXiv:13126114. 2013;.
[39] Laskin M, Srinivas A, Abbeel P. CURL: Contrastive unsupervised representations for reinforcement learn-
ing. In: International Conference on Machine Learning (ICML). 2020. p. 5639–5650.
[40] Nakamura H, Okada M, Taniguchi T. Self-supervised representation learning as multimodal variational
inference. arXiv preprint arXiv:220311437. 2022;.
[41] Hafner D, Lillicrap T, Fischer I, Villegas R, Ha D, Lee H, Davidson J. Learning latent dynamics for planning
from pixels. In: International conference on machine learning (ICML). 2019. p. 2555–2565.
[42] Huang Y , Rao RP. Predictive coding. Wiley Interdisciplinary Reviews: Cognitive Science. 2011;2(5):580–
593.
[43] Hogendoorn H, Burkitt AN. Predictive coding with neural transmission delays: a real-time temporal align-
ment hypothesis. Eneuro. 2019;6(2).
[44] Friston K, Kiebel S. Predictive coding under the free-energy principle. Philosophical Transactions of the
Royal Society B: Biological Sciences. 2009;364(1521):1211–1221.
[45] Friston K. The free-energy principle: a uniﬁed brain theory? Nature Reviews Neuroscience. 2010;11(2):127–
138.
[46] Clark A. Surﬁng uncertainty: Prediction, action, and the embodied mind. Oxford University Press. 2015.
[47] Hohwy J. New directions in predictive processing. Mind & Language. 2020;35(2):209–223.
[48] Ororbia A, Kifer D. The neural coding framework for learning generative models. Nature communications.
2022;13(1):1–14.
[49] Friston K, Kilner J, Harrison L. A free energy principle for the brain. Journal of Physiology-Paris. 2006;
100(1-3):70–87.
[50] Friston K, Mattout J, Kilner J. Action understanding and active inference. Biological cybernetics. 2011;
104(1):137–160.
[51] Buckley C, Kim C, McGregor S, Seth A. The free energy principle for action and perception: A mathemat-
ical review. Journal of Mathematical Psychology. 2017;81:55–79.
[52] Friston KJ, Daunizeau J, Kilner J, Kiebel SJ. Action and behavior: a free-energy formulation. Biological
Cybernetics. 2010;102(3):227–260.
[53] Friston K, Adams RA, Perrinet L, Breakspear M. Perceptions as Hypotheses: Saccades as Experiments.
Frontiers in Psychology. 2012;3(May):151.
[54] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G. Active inference and epistemic value.
Cognitive neuroscience. 2015;6(4):187–214.
[55] Kruglanski A, Jasko K, Friston K. All thinking is “wishful”thinking. Trends in Cognitive Sciences. 2020;.
[56] Friston K, Samothrakis S, Montague R. Active inference and agency: optimal control without cost functions.
Biological Cybernetics. 2012;106(8-9):523–541.
[57] Friston K, Thornton C, Clark A. Free-energy minimization and the dark-room problem. Frontiers in psy-
chology. 2012;:130.
[58] Sun Z, Firestone C. The dark room problem. Trends in Cognitive Sciences. 2020;24(5):346–348.
[59] Chaplot DS, Gandhi D, Gupta S, Gupta A, Salakhutdinov R. Learning to explore using active neural slam.
In: International Conference on Learning Representations (ICLR). 2020.
[60] Ramakrishnan SK, Jayaraman D, Grauman K. Emergence of exploratory look-around behaviors through
active observation completion. Science Robotics. 2019;4(30):eaaw6326.
[61] Ammirato P, Poirson P, Park E, Ko ˇseck´a J, Berg AC. A dataset for developing and benchmarking active
vision. In: IEEE International Conference on Robotics and Automation (ICRA). 2017. p. 1378–1385.
[62] Ognibene D, Demiris Y . Towards active event recognition. In: Ijcai. 2013. p. 2495–2501.
[63] Lee K, Ognibene D, Chang HJ, Kim TK, Demiris Y . Stare: Spatio-temporal attention relocation for multiple
structured activities detection. IEEE Transactions on Image Processing. 2015;24(12):5916–5927.
[64] Donnarumma F, Costantini M, Ambrosini E, Friston K, Pezzulo G. Action perception as hypothesis testing.
Cortex. 2017;89:45–60.
[65] Wang B, Huang L, Hoai M. Active vision for early recognition of human actions. In: IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). 2020.
[66] Schwartenbeck P, Passecker J, Hauser TU, FitzGerald TH, Kronbichler M, Friston KJ. Computational mech-
anisms of curiosity and goal-directed exploration. Elife. 2019;8.
[67] Denzler J, Brown C. Information theoretic sensor data selection for active object recognition and state esti-
mation. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002;24(2):145–157.
[68] Sommerlade E, Reid I. Information-theoretic active scene exploration. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). 2008. p. 1–7.
[69] Hafner D, Ortega PA, Ba J, Parr T, Friston K, Heess N. Action and perception as divergence minimization.
20
January 18, 2023 Advanced Robotics main
arXiv preprint arXiv:200901791. 2020;.
[70] Parr T, Friston KJ. Generalised free energy and active inference. Biological cybernetics. 2019;113(5):495–
513.
[71] Attias H. Planning by probabilistic inference. In: International workshop on artiﬁcial intelligence and statis-
tics. 2003. p. 9–16.
[72] Toussaint M. Robot trajectory optimization using approximate inference. In: International conference on
machine learning (ICML). 2009. p. 1049–1056.
[73] Kappen HJ, G ´omez V , Opper M. Optimal control as a graphical model inference problem. Machine learning.
2012;87(2):159–182.
[74] Botvinick M, Toussaint M. Planning as inference. Trends in cognitive sciences. 2012;16(10):485–488.
[75] Millidge B, Tschantz A, Seth AK, Buckley CL. On the relationship between active inference and control as
inference. In: International workshop on active inference. Springer. 2020. p. 3–11.
[76] Van de Cruys S, Friston K, Clark A. Controlled optimism: Reply to sun and ﬁrestone on the dark room
problem. Trends in Cognitive Sciences. 2020;24(9):1–2.
[77] Haarnoja T, Zhou A, Abbeel P, Levine S. Soft actor-critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor. In: International Conference on Machine Learning (ICML). 2018. p. 1861–
1870.
[78] Okada M, Taniguchi T. Dreaming: Model-based reinforcement learning by latent imagination without re-
construction. In: IEEE International Conference on Robotics and Automation (ICRA). 2021. p. 4209–4215.
[79] Okumura R, Nishio N, Taniguchi T. Tactile-sensitive NewtonianV AE for high-accuracy industrial connector-
socket insertion. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022.
[80] Kaiser L, Babaeizadeh M, Milos P, Osinski B, Campbell RH, Czechowski K, Erhan D, Finn C, Kozakowski
P, Levine S, et al.. Model-based reinforcement learning for atari. arXiv preprint arXiv:190300374. 2019;.
[81] Ke NR, Singh A, Touati A, Goyal A, Bengio Y , Parikh D, Batra D. Learning dynamics model in reinforce-
ment learning by incorporating the long term future. arXiv preprint arXiv:190301599. 2019;.
[82] Okada M, Kosaka N, Taniguchi T. Planet of the Bayesians: Reconsidering and improving deep planning
network by incorporating bayesian inference. In: Ieee/rsj international conference on intelligent robots and
systems (IROS). 2020. p. 5611–5618.
[83] Okada M, Taniguchi T. DreamingV2: Reinforcement learning with discrete world models without recon-
struction. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022.
[84] Jaques M, Burke M, Hospedales TM. NewtonianV AE: Proportional control and goal identiﬁcation from
pixels via physical latent spaces. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 2021. p. 4454–4463.
[85] Tani J. Learning to generate articulated behavior through the bottom-up and the top-down interaction pro-
cesses. Neural Networks. 2003;16(1):11–23.
[86] Yamashita Y , Tani J. Emergence of functional hierarchy in a multiple timescale neural network model: A
humanoid robot experiment. PLoS Computational Biology. 2008;4(11):e1000220.
[87] Nishimoto R, Tani J. Development of hierarchical structures for actions and motor imagery: A constructivist
view from synthetic neuro-robotics study. Psychological Research. 2009;73(4):545–558.
[88] Namikawa J, Nishimoto R, Tani J. A neurodynamic account of spontaneous behaviour. PLoS Computational
Biology. 2011;7(10):e1002221.
[89] Yamashita Y , Tani J. Spontaneous prediction error generation in schizophrenia. PloS One. 2012;7(5):e37843.
[90] Murata S, Namikawa J, Arie H, Sugano S, Tani J. Learning to Reproduce Fluctuating Time Series by In-
ferring Their Time-Dependent Stochastic Properties: Application in Robot Learning Via Tutoring. IEEE
Transactions on Autonomous Mental Development. 2013;5(4):298–310.
[91] Murata S, Yamashita Y , Arie H, Ogata T, Sugano S, Tani J. Learning to Perceive the World as Probabilistic
or Deterministic via Interaction With Others: A Neuro-Robotics Experiment. IEEE Transactions on Neural
Networks and Learning Systems. 2017;28(4):830–848.
[92] Ahmadi A, Tani J. A Novel Predictive-Coding-Inspired Variational RNN Model for Online Prediction and
Recognition. Neural Computation. 2019;31(11):2025–2074.
[93] Lanillos P, Oliva D, Philippsen A, Yamashita Y , Nagai Y , Cheng G. A review on neural network models of
schizophrenia and autism spectrum disorder. Neural Networks. 2020;122:338–363.
[94] Van de Cruys S, Evers K, Van der Hallen R, Van Eylen L, Boets B, De-Wit L, Wagemans J. Precise minds
in uncertain worlds: predictive coding in autism. Psychological review. 2014;121(4):649–75.
[95] Lawson RP, Rees G, Friston KJ. An aberrant precision account of autism. Frontiers in Human Neuroscience.
2014;8(May):1–10.
[96] Idei H, Murata S, Chen Y , Yamashita Y , Tani J, Ogata T. A Neurorobotics Simulation of Autistic Behavior
21
January 18, 2023 Advanced Robotics main
Induced by Unusual Sensory Precision. Computational Psychiatry. 2018;2:164–182.
[97] Idei H, Murata S, Yamashita Y , Ogata T. Homogeneous Intrinsic Neuronal Excitability Induces Overﬁt-
ting to Sensory Noise: A Robot Model of Neurodevelopmental Disorder. Frontiers in Psychiatry. 2020;
11(August):1–15.
[98] Idei H, Murata S, Yamashita Y , Ogata T. Paradoxical sensory reactivity induced by functional disconnection
in a robot model of neurodevelopmental disorder. Neural Networks. 2021;138:150–163.
[99] Sancaktar C, van Gerven M, Lanillos P. End-to-end pixel-based deep active inference for body perception
and action. In: Joint IEEE International Conference on Development and Learning and Epigenetic Robotics
(ICDL-EpiRob). 2020.
[100] Pezzato C, Baioumy M, Corbato CH, Hawes N, Wisse M, Ferrari R. Active inference for fault tolerant
control of robot manipulators with sensory faults. In: International workshop on active inference. Springer.
2020. p. 20–27.
[101] Meo C, Lanillos P. Multimodal vae active inference controller. In: IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). IEEE. 2021.
[102] Lanillos P, Cheng G. Active inference with function learning for robot body perception. International Work-
shop on Continual Unsupervised Sensorimotor Learning, IEEE Developmental Learning and Epigenetic
Robotics (ICDL-Epirob). 2018;.
[103] Lanillos P, Cheng G. Adaptive robot body learning and estimation through predictive coding. In: IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE. 2018. p. 4083–4090.
[104] Ito M, Noda K, Hoshino Y , Tani J. Dynamic and interactive generation of object handling behaviors by a
small humanoid robot using a dynamic neural network model. Neural Networks. 2006;19(3):323–337.
[105] Sugita Y , Tani J. Learning Semantic Combinatoriality from the Interaction between Linguistic and Behav-
ioral Processes. Adaptive Behavior. 2005;13(1):33–52.
[106] Chen Y , Murata S, Arie H, Ogata T, Tani J, Sugano S. Emergence of interactive behaviors between two robots
by prediction error minimization mechanism. In: Joint IEEE International Conference on Development and
Learning and Epigenetic Robotics (ICDL-EpiRob). 2016. p. 302–307.
[107] C ¸ atal O, Verbelen T, Van de Maele T, Dhoedt B, Safron A. Robot navigation as hierarchical active inference.
Neural Networks. 2021;142:192–204.
[108] Imre M, Oztop E, Nagai Y , Ugur E. Affordance-based altruistic robotic architecture for human–robot col-
laboration. Adaptive Behavior. 2019;27(4):223–241.
[109] Seker MY , Ahmetoglu A, Nagai Y , Asada M, Oztop E, Ugur E. Imitation and mirror systems in robots
through deep modality blending networks. Neural Networks. 2022;146:22–35.
[110] Seker MY , Imre M, Piater J, Ugur E. Conditional neural movement primitives. In: Robotics Science and
Systems (RSS). 2019.
[111] Tschantz A, Baltieri M, Seth AK, Buckley CL. Scaling active inference. In: international joint conference
on neural networks (IJCNN). 2020. p. 1–8.
[112] van Bergen RS, Lanillos PL. Object-based active inference. arXiv preprint arXiv:220901258. 2022;.
[113] Da Costa L, Lanillos P, Sajid N, Friston K, Khan S. How active inference could help revolutionise robotics.
Entropy. 2022;24(3):361.
[114] Valenzo D, Ciria A, Schillaci G, Lara B. Grounding context in embodied cognitive robotics. Frontiers in
Neurorobotics. 2022;16.
[115] Fountas Z, Sajid N, Mediano P, Friston K. Deep active inference agents using monte-carlo methods. Ad-
vances in neural information processing systems. 2020;33:11662–11675.
[116] Mazzaglia P, Verbelen T, Dhoedt B. Contrastive active inference. Advances in Neural Information Process-
ing Systems. 2021;34:13870–13882.
[117] B ¨ohmer W, Springenberg JT, Boedecker J, Riedmiller M, Obermayer K. Autonomous learning of state repre-
sentations for control: An emerging ﬁeld aims to autonomously learn state representations for reinforcement
learning agents from their real-world sensor observations. KI-K¨unstliche Intelligenz. 2015;29(4):353–362.
[118] Achille A, Soatto S. A separation principle for control in the age of deep learning. Annual Review of Control,
Robotics, and Autonomous Systems. 2018;1:287–307.
[119] Lesort T, D ´ıaz-Rodr´ıguez N, Goudou JF, Filliat D. State representation learning for control: An overview.
Neural Networks. 2018;108:379–392.
[120] Stoianov I, Maisto D, Pezzulo G. The hippocampal formation as a hierarchical generative model supporting
generative replay and continual learning. Progress in Neurobiology. 2022;217:102329.
[121] Gumbsch C, Butz MV , Martius G. Sparsely changing latent states for prediction and planning in partially
observable domains. Advances in Neural Information Processing Systems. 2021;34:17518–17531.
[122] Greff K, Kaufman RL, Kabra R, Watters N, Burgess C, Zoran D, Matthey L, Botvinick M, Lerchner A.
22
January 18, 2023 Advanced Robotics main
Multi-object representation learning with iterative variational inference. In: International Conference on
Machine Learning (ICML). 2019. p. 2424–2433.
[123] Wang T, Du SS, Torralba A, Isola P, Zhang A, Tian Y . Denoised MDPs: Learning world models better than
the world itself. arXiv preprint arXiv:220615477. 2022;.
[124] K ¨ohler W, Winter E. The mentality of apes. International library of psychology, philosophy, and scientiﬁc
method. K. Paul, Trench, Trubner & Company, Limited. 1925.
[125] Steedman M. Plans, affordances, and combinatory grammar. Linguistics and Philosophy. 2002;25(5-6):723–
753.
[126] Goel V , Gold B, Kapur S, Houle S. Neuroanatomical correlates of human reasoning. Journal of cognitive
neuroscience. 1998;10(3):293–302.
[127] Al N, Nolen-Hoeksema S. Atkinson and hilgard’s introduction to psychology. Cengage Learning. 2014.
[128] Russell SJ, Norvig P. Artiﬁcial intelligence: a modern approach. Pearson Education Limited,. 2016.
[129] Mugan J, Kuipers B. Autonomous learning of high-level states and actions in continuous environments.
IEEE Transactions on Autonomous Mental Development. 2012;4(1):70–86.
[130] Ahmetoglu A, Ugur E, Asada M, Oztop E. High-level features for resource economy and fast learning in
skill transfer. Advanced Robotics. 2022;36(5-6):291–303.
[131] Konidaris G, Kaelbling LP, Lozano-Perez T. Constructing symbolic representations for high-level planning.
In: AAAI Conference on Artiﬁcial Intelligence (AAAI). 2014.
[132] Konidaris G, Kaelbling L, Lozano-Perez T. Symbol acquisition for probabilistic high-level planning. In:
International Joint Conference on Artiﬁcial Intelligence (IJCAI). 2015.
[133] James S, Rosman B, Konidaris G. Learning portable representations for high-level planning. arXiv preprint
arXiv:190512006. 2019;.
[134] Ugur E, Piater J. Bottom-up learning of object categories, action effects and logical rules: From contin-
uous manipulative exploration to symbolic planning. In: IEEE International Conference on Robotics and
Automation (ICRA). 2015. p. 2627–2633.
[135] Ugur E, Piater J. Reﬁning discovered symbols with multi-step interaction experience. In: 2015 ieee-ras 15th
international conference on humanoid robots (humanoids). 2015. p. 1007–1012.
[136] Ahmetoglu A, Seker MY , Piater J, Oztop E, Ugur E. Deepsym: Deep symbol generation and rule learning
from unsupervised continuous robot interaction for planning. Journal of Artiﬁcial Intelligence Research.
2022;.
[137] S ¸ahin E, Cakmak M, Do ˘gar MR, U ˘gur E, ¨Uc ¸oluk G. To afford or not to afford: A new formalization of
affordances toward affordance-based robot control. Adaptive Behavior. 2007;15(4):447–472.
[138] Ahmetoglu A, Oztop E, Ugur E. Learning multi-object symbols for manipulation with attentive deep effect
predictors. arXiv preprint arXiv:220801021. 2022;.
[139] Asai M, Fukunaga A. Classical planning in deep latent space: Bridging the subsymbolic-symbolic boundary.
arXiv preprint arXiv:170500154. 2017;.
[140] Asai M, Muise C. Learning neural-symbolic descriptive planning models via cube-space priors: The voyage
home (to strips). arXiv preprint arXiv:200412850. 2020;.
[141] Gibson JJ. The ecological approach to visual perception. Boston: Houghton Mifﬂin. 1979.
[142] Zech P, Haller S, Lakani SR, Ridge B, Ugur E, Piater J. Computational models of affordance in robotics: a
taxonomy and systematic classiﬁcation. Adaptive Behavior. 2017;25(5):235–271.
[143] Jamone L, Ugur E, Cangelosi A, Fadiga L, Bernardino A, Piater J, Santos-Victor J. Affordances in psy-
chology, neuroscience and robotics: a survey. IEEE Transactions on Cognitive and Developmental Systems.
2016;10(1):4–25.
[144] Renaudo E, Zech P, Chatila R, Khamassi M. Computational models of affordance for robotics. Frontiers in
Neurorobotics. 2022;16:1045355.
[145] Gibson JJ. The senses considered as perceptual systems. Boston: Houghton Mifﬂin. 1966.
[146] Gibson JJ. The theory of affordances. Perceiving, Acting, and Knowing: Toward an ecological psychology.
Eds. Lawrence Erlbaum Associates. 1977.
[147] Norman DA. Affordance, conventions, and design. Interactions. 1999;6(3):38–42.
[148] Gibson EJ. An odyssey in learning and perception. MIT Press. 1994.
[149] Gibson EJ. Perceptual learning in development: Some basic concepts. Ecological Psychology. 2000;
12(4):295–302.
[150] Gibson EJ. The world is so full of a number of things: On speciﬁcation and perceptual learning. Ecological
Psychology. 2003;15(4):283–288.
[151] S ¸ahin E, Cakmak M, Do ˘gar MR, U ˘gur E, ¨Uc ¸oluk G. To afford or not to afford: A new formalization of
affordances toward affordance-based robot control. Adaptive Behavior. 2007;15(4):447–472.
23
January 18, 2023 Advanced Robotics main
[152] Montesano L, Lopes M, Bernardino A, Santos-Victor J. Learning object affordances: From sensory–motor
coordination to imitation. IEEE Transactions on Robotics. 2008;24(1):15–26.
[153] Ugur E, S ¸ahin E, Oztop E. Unsupervised learning of object affordances for planning in a mobile manipula-
tion platform. In: IEEE International Conference on Robotics and Automation (ICRA). 2011. p. 4312–4317.
[154] Krueger N, Geib C, Piater J, Petrick R, Steedman M, Worgotter F, Ude A, Asfour T, Kraft D, Omrcen
D, Agostini A, Dillmann R. Object-action complexes: Grounded abstractions of sensory-motor processes.
Robotics and Autonomous Systems. 2011;59(10):740–757.
[155] Szedmak S, Ugur E, Piater J. Knowledge propagation and relation learning for predicting action effects. In:
Ieee/rsj international conference on intelligent robots and systems (IROS). 2014. p. 623–629.
[156] Gonc ¸alves A, Abrantes J, Saponaro G, Jamone L, Bernardino A. Learning intermediate object affordances:
Towards the development of a tool concept. In: 4th international conference on development and learning
and on epigenetic robotics. 2014. p. 482–488.
[157] Dehban A, Jamone L, Kampff AR, Santos-Victor J. Denoising auto-encoders for learning of objects and
tools affordances in continuous space. In: IEEE International Conference on Robotics and Automation
(ICRA). 2016. p. 4866–4871.
[158] Dehban A, Jamone L, Kampff AR, Santos-Victor J. A deep probabilistic framework for heterogeneous self-
supervised learning of affordances. In: IEEE-RAS 17th International Conference on Humanoid Robotics
(Humanoids). 2017. p. 476–483.
[159] Ugur E, Piater J. Emergent structuring of interdependent affordance learning tasks using intrinsic motiva-
tion and empirical feature selection. IEEE Transactions on Cognitive and Developmental Systems. 2017;
9(4):328–340.
[160] Sarathy V , Scheutz M. A logic-based computational framework for inferring cognitive affordances. IEEE
Transactions on Cognitive and Developmental Systems. 2018;10(1):26–43.
[161] Stramandinoli F, Tikhanoff V , Pattacini U, Nori F. Heteroscedastic regression and active learning for
modeling affordances in humanoids. IEEE Transactions on Cognitive and Developmental Systems. 2018;
10(2):455–468.
[162] Tekden AE, Erdem A, Erdem E, Imre M, Seker MY , Ugur E. Belief regulated dual propagation nets for
learning action effects on groups of articulated objects. In: IEEE International Conference on Robotics and
Automation (ICRA). 2020. p. 10556–10562.
[163] Antunes A, Jamone L, Saponaro G, Bernardino A, Ventura R. From human instructions to robot actions:
Formulation of goals, affordances and probabilistic planning. In: Ieee icra. 2016.
[164] Dehban A, Zhang S, Cauli N, Jamone L, Santos-Victor J. Learning deep features for robotic inference from
physical interactions. IEEE Transactions on Cognitive and Developmental Systems. 2022;:1–1.
[165] Shen Z, Wu Y . Investigation of practical use of humanoid robots in elderly care centres. In: Proceedings of
the fourth international conference on human agent interaction. 2016. p. 63–66.
[166] Albrecht SV , Stone P. Autonomous agents modelling other agents: A comprehensive survey and open prob-
lems. Artiﬁcial Intelligence. 2018;258:66–95.
[167] El Zaatari S, Marei M, Li W, Usman Z. Cobot programming for collaborative industrial tasks: An overview.
Robotics and Autonomous Systems. 2019;116:162–180.
[168] Hentout A, Aouache M, Maoudj A, Akli I. Human–robot interaction in industrial collaborative robotics: a
literature review of the decade 2008–2017. Advanced Robotics. 2019;33(15-16):764–799.
[169] Magrini E, Ferraguti F, Ronga AJ, Pini F, De Luca A, Leali F. Human-robot coexistence and interaction in
open industrial cells. Robotics and Computer-Integrated Manufacturing. 2020;61:101846.
[170] Ognibene D, Foulsham T, Marchegiani L, Farinella GM. Active vision and perception in human-robot col-
laboration. Frontiers in Neurorobotics. 2022;16.
[171] Semeraro F, Grifﬁths A, Cangelosi A. Human–robot collaboration and machine learning: A systematic re-
view of recent research. Robotics and Computer-Integrated Manufacturing. 2023;79:102432.
[172] Ognibene D, Chinellato E, Sarabia M, Demiris Y . Contextual action recognition and target localization with
an active allocation of attention on a humanoid robot. Bioinspiration & biomimetics. 2013;8(3):035002.
[173] Fotopoulou A, Tsakiris M. Mentalizing homeostasis: The social origins of interoceptive inference. Neu-
ropsychoanalysis. 2017;19(1):3–28.
[174] Veissi `ere SP, Constant A, Ramstead MJ, Friston KJ, Kirmayer LJ. Thinking through other minds: A varia-
tional approach to cognition and culture. Behavioral and brain sciences. 2020;43.
[175] Saxe R. Uniquely human social cognition. Current opinion in neurobiology. 2006;16(2):235–239.
[176] Bianco F, Ognibene D. Functional advantages of an adaptive theory of mind for robotics: a review of current
architectures. Computer Science and Electronic Engineering (CEEC). 2019;:139–143.
[177] Baker CL, Jara-Ettinger J, Saxe R, Tenenbaum JB. Rational quantitative attribution of beliefs, desires and
24
January 18, 2023 Advanced Robotics main
percepts in human mentalizing. Nature Human Behaviour. 2017;1(4):1–10.
[178] Bianco F, Ognibene D. Robot learning theory of mind through self-observation: Exploiting the intentions-
beliefs synergy. arXiv preprint arXiv:221009435. 2022;.
[179] Bianchi F, Marelli M, Nicoli P, Palmonari M. Sweat: Scoring polarization of topics across different corpora.
arXiv preprint arXiv:210907231. 2021;.
[180] Ognibene D, Mirante L, Marchegiani L. Proactive intention recognition for joint human-robot search and
rescue missions through monte-carlo planning in POMDP environments. In: International conference on
social robotics. Springer. 2019. p. 332–343.
[181] Heinze C. Modelling intention recognition for intelligent agent systems. DEFENCE SCIENCE AND
TECHNOLOGY ORGANISATION. 2004. Tech Rep. Available from: http://www.dsto.defence.
gov.au/corporate/reports/DSTO-RR-0286.pdf.
[182] Roggen D, Calatroni A, Rossi M, Holleczek T, F ¨orster K, Tr ¨oster G, Lukowicz P, Bannach D, Pirkl G,
Ferscha A, et al.. Collecting complex activity datasets in highly rich networked sensor environments. In:
International conference on networked sensing systems (INSS). 2010. p. 233–240.
[183] Zeng M, Nguyen LT, Yu B, Mengshoel OJ, Zhu J, Wu P, Zhang J. Convolutional neural networks for human
activity recognition using mobile sensors. In: 6th international conference on mobile computing, applica-
tions and services. IEEE. 2014. p. 197–205.
[184] Yang J, Nguyen MN, San PP, Li XL, Krishnaswamy S. Deep convolutional neural networks on multichannel
time series for human activity recognition. In: Twenty-fourth international joint conference on artiﬁcial
intelligence. 2015.
[185] Lee SU, Hofmann A, Williams B. A model-based human activity recognition for human–robot collabo-
ration. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2019. p.
736–743.
[186] Ram ´ırez M, Geffner H. Plan recognition as planning. In: International joint conference on artiﬁcial intelli-
gence (IJCAI). 2009.
[187] Baker CL, Saxe R, Tenenbaum JB. Action understanding as inverse planning. Cognition. 2009;113(3):329–
349.
[188] Sohrabi S, Riabov A V , Udrea O. Plan recognition as planning revisited. In: International joint conference on
artiﬁcial intelligence (IJCAI). New York, NY . 2016. p. 3258–3264.
[189] Ramirez M, Geffner H. Goal recognition over POMDPs: Inferring the intention of a pomdp agent. In: Inter-
national joint conference on artiﬁcial intelligence (IJCAI). 2011.
[190] Baker C, Saxe R, Tenenbaum J. Bayesian theory of mind: Modeling joint belief-desire attribution. In: Pro-
ceedings of the annual meeting of the cognitive science society. V ol. 33. 2011.
[191] Shvo M, McIlraith SA. Active goal recognition. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence. V ol. 34. 2020. p. 9957–9966.
[192] Amato C, Baisero A. Active goal recognition. arXiv preprint arXiv:190911173. 2019;.
[193] Demiris Y , Simmons G. Perceiving the unusual: Temporal properties of hierarchical motor representations
for action perception. Neural Networks. 2006;19(3):272–284.
[194] Cardona-Rivera RE, Young RM. Toward combining domain theory and recipes in plan recognition. In:
Workshops at the thirty-ﬁrst aaai conference on artiﬁcial intelligence. 2017.
[195] Proietti R, Pezzulo G, Tessari A. An active inference model of hierarchical action understanding, learning
and imitation. PsyArXiv. 2021;.
[196] Zhi-Xuan T, Mann J, Silver T, Tenenbaum J, Mansinghka V . Online bayesian goal inference for
boundedly rational planning agents. In: Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H, ed-
itors. Advances in neural information processing systems. V ol. 33. Curran Associates, Inc.. 2020.
p. 19238–19250. Available from: https://proceedings.neurips.cc/paper/2020/file/
df3aebc649f9e3b674eeb790a4da224e-Paper.pdf.
[197] Johnson M, Demiris Y . Perceptual perspective taking and action recognition. International Journal of Ad-
vanced Robotic Systems. 2005;2(4):32.
[198] Pandey AK, Ali M, Alami R. Towards a task-aware proactive sociable robot based on multi-state
perspective-taking. International Journal of Social Robotics. 2013;5(2):215–236.
[199] Fischer T, Demiris Y . Computational modeling of embodied visual perspective taking. IEEE Transactions
on Cognitive and Developmental Systems. 2019;12(4):723–732.
[200] Torabi F, Warnell G, Stone P. Recent advances in imitation learning from observation. arXiv e-prints. 2019;
:arXiv–1905.
[201] M ¨ortl A, Lawitzky M, Kucukyilmaz A, Sezgin M, Basdogan C, Hirche S. The role of roles: Physical cooper-
ation between humans and robots. The International Journal of Robotics Research. 2012;31(13):1656–1674.
25
January 18, 2023 Advanced Robotics main
[202] Li Y , Tee KP, Yan R, Chan WL, Wu Y . A framework of human–robot coordination based on game theory
and policy iteration. IEEE Transactions on Robotics. 2016;32(6):1408–1418.
[203] Pezzulo G, Donnarumma F, Dindo H, D’Ausilio A, Konvalinka I, Castelfranchi C. The body talks: Sensori-
motor communication and its brain and kinematic signatures. Physics of life reviews. 2019;28:1–21.
[204] Ognibene D, Giglia G, Marchegiani L, Rudrauf D. Implicit perception simplicity and explicit perception
complexity in sensorimotor comunication. Physics of life reviews. 2019;28:36–38.
[205] Kok BC, Soh H. Trust in robots: Challenges and opportunities. Current Robotics Reports. 2020;1(4):297–
309.
[206] Craik K. The nature of explanation. Cambridge: Cambridge University Press. 1943.
[207] von Helmholtz H. Handbuch der physiologischen optik. Leipzig: L. V oss. 1867.
[208] Gregory RL. Knowledge in perception and illusion. Philosophical Transactions of the Royal Society B:
Biological Sciences. 1997;352:1121–1128.
[209] Doya K, Ishii S, Pouget A, Rao RPN, editors. Bayesian brain: Probabilistic approaches to neural coding. 1st
ed. The MIT Press. 2007.
[210] Shiffrin RM, Bassett DS, Kriegeskorte N, Tenenbaum JB. The brain produces mind by modeling. Proceed-
ings of the National Academy of Sciences. 2020;117(47):29299–29301.
[211] Ma WJ, Beck JM, Latham PE, Pouget A. Bayesian inference with probabilistic population codes. Nat Neu-
rosci. 2006;9(11):1432–1438. Available from: http://dx.doi.org/10.1038/nn1790.
[212] Buesing L, Bill J, Nessler B, Maass W. Neural dynamics as sampling: a model for stochastic computation
in recurrent networks of spiking neurons. PLoS Comput Biol. 2011;7(11):e1002211.
[213] Sebastian S, Seemiller ES, Geisler WS. Local reliability weighting explains identiﬁcation of partially
masked objects in natural images. Proceedings of the National Academy of Sciences. 2020;117(47):29363–
29370.
[214] Lynn CW, Bassett DS. How humans learn and represent networks. Proceedings of the National Academy of
Sciences. 2020;117(47):29407–29415.
[215] Berkes P, Orban G, Lengyel M, Fiser J. Spontaneous cortical activity reveals hallmarks of an optimal internal
model of the environment. Science. 2011;331(6013):83–87.
[216] Ekman M, Kok P, de Lange FP. Time-compressed preplay of anticipated events in human primary visual
cortex. Nature Communications. 2017;8(1):1–9.
[217] Daw ND, Gershman SJ, Seymour B, Dayan P, Dolan RJ. Model-based inﬂuences on humans’ choices and
striatal prediction errors. Neuron. 2011;69(6):1204–1215.
[218] G ´omez CM, Arjona A, Donnarumma F, Maisto D, Rodr ´ıguez Mart´ınez EI, Pezzulo G. Tracking the time
course of bayesian inference with event related potentials: a study using the central cue posner paradigm.
Frontiers in Psychology. 2019;10:1424.
[219] Muckli L, De Martino F, Vizioli L, Petro LS, Smith FW, Ugurbil K, Goebel R, Yacoub E. Contextual
feedback to superﬁcial layers of v1. Current Biology. 2015;25(20):2690–2695.
[220] Walsh KS, McGovern DP, Clark A, O’Connell RG. Evaluating the neurophysiological evidence for predic-
tive processing as a model of perception. Annals of the new York Academy of Sciences. 2020;1464(1):242–
268.
[221] Bullmore E, Sporns O. Complex brain networks: graph theoretical analysis of structural and functional
systems. Nature reviews neuroscience. 2009;10(3):186–198.
[222] Laird JE, Lebiere C, Rosenbloom PS. A standard model of the mind: Toward a common computational
framework across artiﬁcial intelligence, cognitive science, neuroscience, and robotics. AI Magazine. 2017;
38(4):13–26.
[223] Margulies DS, Ghosh SS, Goulas A, Falkiewicz M, Huntenburg JM, Langs G, Bezgin G, Eickhoff SB,
Castellanos FX, Petrides M, et al.. Situating the default-mode network along a principal gradient of
macroscale cortical organization. Proceedings of the National Academy of Sciences. 2016;113(44):12574–
12579.
[224] Ungerleider L, Haxby J. “what” and “where” in the human brain. Current Opinion in Neurobiology. 1994;
4(2):157–65.
[225] Taniguchi T, Yamakawa H, Nagai T, Doya K, Sakagami M, Suzuki M, Nakamura T, Taniguchi A. A whole
brain probabilistic generative model: Toward realizing cognitive architectures for developmental robots.
Neural Networks. 2022;150:293–312.
[226] Singer W. Recurrent dynamics in the cerebral cortex: Integration of sensory evidence with stored knowledge.
Proceedings of the National Academy of Sciences. 2021;118(33):e2101043118.
[227] Pezzulo G, Zorzi M, Corbetta M. The secret life of predictive brains: what’s spontaneous activity for? Trends
in Cognitive Sciences. 2021;.
26
January 18, 2023 Advanced Robotics main
[228] Gy ¨orgy Buzs´aki M. The brain from inside out. Oxford University Press. 2019.
[229] Pezzulo G, Kemere C, van der Meer M. Internally generated hippocampal sequences as a vantage point to
probe future-oriented cognition. Annals of the New York Academy of Sciences. 2017;1396:144–165.
[230] Nazli I, Ferrari A, Huber-Huber C, de Lange FP. Statistical learning is not error-driven. bioRxiv. 2022;.
[231] Pezzulo G, Cisek P. Navigating the affordance landscape: Feedback control as a process model of behavior
and cognition. Trends in Cognitive Sciences. 2016;20(6):414–424.
[232] Cisek P. Resynthesizing behavior through phylogenetic reﬁnement. Attention, Perception, & Psychophysics.
2019;81(7):2265–2287.
[233] Lieto A, Bhatt M, Oltramari A, Vernon D. The role of cognitive architectures in general artiﬁcial intelligence.
2018.
[234] Kotseruba I, Tsotsos JK. 40 years of cognitive architectures: core cognitive abilities and practical applica-
tions. Artiﬁcial Intelligence Review. 2020;53(1):17–94.
[235] Anderson JR. How can the human mind occur in the physical universe? Oxford University Press. 2009.
[236] Laird JE. Extending the soar cognitive architecture. Frontiers in Artiﬁcial Intelligence and Applications.
2008;171:224.
[237] Anderson JR. Human symbol manipulation within an integrated cognitive architecture. In: Cognitive sci-
ence. Routledge. 2005. p. 313–341.
[238] Puigbo JY , Pumarola A, T ´ellez RA. Controlling a general purpose service robot by means of a cognitive
architecture. In: Ceur workshop proceedings. 2013. p. 45–55.
[239] Mohan S, Laird JE. Learning to play mario. Tech Rep CCA-TR-2009-03. 2009;.
[240] Rosenbloom PS, Demski A, Ustun V . The sigma cognitive architecture and system: Towards functionally
elegant grand uniﬁcation. Journal of Artiﬁcial General Intelligence. 2016;7(1):1–103.
[241] Damgaard MR, Pedersen R, Bak T. Toward an idiomatic framework for cognitive robotics. Patterns. 2022;
3(7):100533.
[242] Bingham E, Chen JP, Jankowiak M, Obermeyer F, Pradhan N, Karaletsos T, Singh R, Szerlip P, Horsfall P,
Goodman ND. Pyro: Deep universal probabilistic programming. The Journal of Machine Learning Research.
2019;20(1):973–978.
[243] Paige B, van de Meent JW, Desmaison A, Goodman N, Kohli P, Wood F, Torr P, et al.. Learning disentangled
representations with semi-supervised deep generative models. Advances in neural information processing
systems. 2017;30.
[244] Tran D, Kucukelbir A, Dieng AB, Rudolph M, Liang D, Blei DM. Edward: A library for probabilistic
modeling, inference, and criticism. arXiv preprint arXiv:161009787. 2016;.
[245] Sandini G, Mohan V , Sciutti A, Morasso P. Social cognition for human-robot symbiosis—challenges and
building blocks. Frontiers in neurorobotics. 2018;12:34.
[246] Sun R. Anatomy of the Mind: Exploring Psychological Mechanisms and Processes with the Clarion Cogni-
tive Architecture. Oxford University Press. 2016.
[247] Kahneman D. A perspective on judgment and choice: mapping bounded rationality. American psychologist.
2003;58(9):697.
[248] Vahrenkamp N, W ¨achter M, Kr ¨ohnert M, Welke K, Asfour T. The robot software framework armarx. it-
Information Technology. 2015;57(2):99–111.
[249] Nakamura T, Nagai T, Taniguchi T. Serket: An architecture for connecting stochastic models to realize a
large-scale cognitive model. Frontiers in Neurorobotics. 2018;12:1–16.
[250] Taniguchi T, Nakamura T, Suzuki M, Kuniyasu R, Hayashi K, Taniguchi A, Horii T, Nagai T. Neuro-serket:
development of integrative cognitive system through the composition of deep probabilistic generative mod-
els. New Generation Computing. 2020;:1–26.
[251] Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:181004805. 2019;.
[252] Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell
A, et al.. Language models are few-shot learners. Advances in neural information processing systems. 2020;
33:1877–1901.
[253] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Zhou D. Self-consistency improves chain of thought reasoning
in language models. arXiv preprint arXiv:220311171. 2022;.
[254] Kojima T, Gu SS, Reid M, Matsuo Y , Iwasawa Y . Large language models are zero-shot reasoners. arXiv
preprint arXiv:220511916. 2022;.
[255] Ahn M, Brohan A, Brown N, Chebotar Y , Cortes O, David B, Finn C, Fu C, Gopalakrishnan K, Hausman
K, Herzog A, Ho D, Hsu J, Ibarz J, Ichter B, Irpan A, Jang E, Ruano RJ, Jeffrey K, Jesmonth S, Joshi N,
Julian R, Kalashnikov D, Kuang Y , Lee KH, Levine S, Lu Y , Luu L, Parada C, Pastor P, Quiambao J, Rao
27
January 18, 2023 Advanced Robotics main
K, Rettinghouse J, Reyes D, Sermanet P, Sievers N, Tan C, Toshev A, Vanhoucke V , Xia F, Xiao T, Xu P,
Xu S, Yan M, Zeng A. Do as i can and not as i say: Grounding language in robotic affordances. In: arxiv
preprint arxiv:2204.01691. 2022.
[256] Tangiuchi T, Mochihashi D, Nagai T, Uchida S, Inoue N, Kobayashi I, Nakamura T, Hagiwara Y , Iwahashi
N, Inamura T. Survey on frontiers of language and robotics. Advanced Robotics. 2019;33(15-16):700–730.
[257] Steels L. The symbol grounding problem has been solved, so what’s next ? In: Symbols and embodiment:
Debates on meaning and cognition. Oxford University Press. 2008. p. 223–244.
[258] Taniguchi T, Ugur E, Hoffmann M, Jamone L, Nagai T, Rosman B, Matsuka T, Iwahashi N, Oztop E, Piater
J, et al.. Symbol emergence in cognitive developmental systems: a survey. IEEE transactions on Cognitive
and Developmental Systems. 2018;11(4):494–516.
[259] Taniguchi T, Nagai T, Nakamura T, Iwahashi N, Ogata T, Asoh H. Symbol emergence in robotics: A survey.
Advanced Robotics. 2016;30(11-12):706–728.
[260] Jung M, Matsumoto T, Tani J. Goal-directed behavior under variational predictive coding: Dynamic organi-
zation of visual attention and working memory. In: IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). 2019. p. 1040–1047.
[261] Ito H, Yamamoto K, Mori H, Ogata T. Efﬁcient multitask learning with an embodied predictive model for
door opening and entry with whole-body control. Science Robotics. 2022;7(65):eaax8177.
[262] Hoffmann M, Wang S, Outrata V , Alzueta E, Lanillos P. Robot in the mirror: toward an embodied computa-
tional model of mirror self-recognition. KI-K¨unstliche Intelligenz. 2021;35(1):37–51.
[263] Juliani A, Arulkumaran K, Sasai S, Kanai R. On the link between conscious function and general intelligence
in humans and machines. arXiv preprint arXiv:220405133. 2022;.
[264] Seth AK. Interoceptive inference, emotion, and the embodied self. Trends in cognitive sciences. 2013;
17(11):565–573.
[265] Bauermeister J, Lanillos P. The role of valence and meta-awareness in mirror self-recognition using hierar-
chical active inference. arXiv preprint arXiv:220813213. 2022;.
[266] Hieida C, Nagai T. Survey and perspective on social emotions in robotics. Advanced Robotics. 2022;36(1-
2):17–32.
[267] El Haﬁ L, Zheng Y , Shirouzu H, Nakamura T, Taniguchi T. Serket-SDE: A Containerized Software Develop-
ment Environment for the Symbol Emergence in Robotics Toolkit. In: IEEE/SICE International Symposium
on System Integration (SII). 2023.
[268] Heins C, Millidge B, Demekas D, Klein B, Friston K, Couzin I, Tschantz A. pymdp: A python library for
active inference in discrete state spaces. arXiv preprint arXiv:220103904. 2022;.
[269] Reed S, Zolna K, Parisotto E, Colmenarejo SG, Novikov A, Barth-Maron G, Gimenez M, Sulsky Y , Kay J,
Springenberg JT, et al.. A generalist agent. arXiv preprint arXiv:220506175. 2022;.
[270] McGeer T, et al.. Passive dynamic walking. Int J Robotics Res. 1990;9(2):62–82.
[271] Brooks RA. Elephants don’t play chess. Robotics and autonomous systems. 1990;6(1-2):3–15.
[272] Pfeifer R, Lungarella M, Iida F. Self-organization, embodiment, and biologically inspired robotics. science.
2007;318(5853):1088–1093.
[273] Pfeifer R, G ´omez G. Morphological computation–connecting brain, body, and environment. In: Creating
brain-like intelligence. Springer. 2009. p. 66–83.
28