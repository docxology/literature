Active Inference and Epistemic Value in
Graphical Models
Thijs van de Laar1, Magnus Koudahl1,2, Bart van Erp1, and Bert de
Vries1,3
1Department of Electrical Engineering, Eindhoven University of
Technology, Eindhoven, The Netherlands
2Nested Mind Solutions, Liverpool, England
3GN Hearing Benelux BV, Eindhoven, The Netherlands
March 8, 2022
Abstract
The Free Energy Principle (FEP) postulates that biological agents
perceive and interact with their environment in order to minimize a Vari-
ational Free Energy (VFE) with respect to a generative model of their
environment. The inference of a policy (future control sequence) according
to the FEP is known as Active Inference (AIF). The AIF literature de-
scribes multiple VFE objectives for policy planning that lead to epistemic
(information-seeking) behavior. However, most objectives have limited
modeling ﬂexibility. This paper approaches epistemic behavior from a
constrained Bethe Free Energy (CBFE) perspective. Crucially, variational
optimization of the CBFE can be expressed in terms of message passing on
free-form generative models. The key intuition behind the CBFE is that
we impose a point-mass constraint on predicted outcomes, which explicitly
encodes the assumption that the agent will make observations in the future.
We interpret the CBFE objective in terms of its constituent behavioral
drives. We then illustrate resulting behavior of the CBFE by planning
and interacting with a simulated T-maze environment. Simulations for
the T-maze task illustrate how the CBFE agent exhibits an epistemic
drive, and actively plans ahead to account for the impact of predicted
outcomes. Compared to an EFE agent, the CBFE agent incurs expected
reward in signiﬁcantly more environmental scenarios. We conclude that
CBFE optimization by message passing suggests a general mechanism for
epistemic-aware AIF in free-form generative models.
Keywords: Free Energy Principle, Active Inference, Variational Optimiza-
tion, Constrained Bethe Free Energy, Message Passing
This is the author’s version of the article that has been accepted for publication in Frontiers in
Robotics and AI.
1
arXiv:2109.00541v2  [stat.ML]  7 Mar 2022
1 Introduction
Free energy can be considered as a central concept in the natural sciences. Many
natural laws can be derived through the principle of least action 1, which rests on
variational methods to minimize a path integral of free energy over time [ 1]. In
neuroscience, an application of the least action principle to biological behavior
is formalized as the Free Energy Principle [ 2]. The Free Energy Principle (FEP)
postulates that biological agents perceive and interact with their environment in
order to minimize a Variational Free Energy (VFE) that is deﬁned with respect
to a model of their environment.
Under the FEP, perception relates to the process of hidden state estimation,
where the agent tries to infer hidden causes of its sensory observations; and action
(intervention) relates to a process where the agent actively tries to inﬂuence
its (predicted) future observations by manipulating the external environment.
Because the future is unobserved (by deﬁnition), the agent includes prior beliefs 2
about desired outcomes in its model and infers a policy that prescribes a sequence
of future controls3. The corollary of the FEP that includes action is referred to
as Active Inference (AIF) [3].
The AIF literature describes multiple Free Energy (FE) objectives for policy
planning, e.g., the Expected FE [ 4], Generalized FE [ 5] and Predicted (Bethe)
FE [6] (among others, see e.g. [ 7, 8, 9]). Traditionally, the Expected Free Energy
(EFE) is evaluated for a selection of policies, and a posterior distribution over
policies is constructed from the corresponding EFEs. The EFE is designed to
balance epistemic (knowledge seeking) and extrinsic (goal seeking) behavior. The
active policy (the sequence of future controls to be executed in the environment)
is then selected from this policy posterior [4].
Several authors have attempted to formulate minimization of the EFE by
message passing on factor graphs [ 10, 5, 11, 12]. These formulations evaluate
the EFE objective with the use of a message passing scheme. In this paper we
revisit this problem and compare the EFE approach with the message passing
interpretation of the variational optimization of a Bethe Free Energy (BFE)
[13, 14, 15]. However, the BFE is known to lack epistemic (information-seeking)
qualities, and resulting BFE AIF agents therefore do not pro-actively seek
informative states [6].
As a solution to the lack of epistemic qualities of the BFE, in this paper
we approach epistemic behavior from a Constrained BFE (CBFE) perspective
[16]. We illustrate how optimization of a point-mass constrained BFE objective
instigates self-evidencing behavior. Crucially, variational optimization of the
CBFE can be expressed in terms of message passing on a graphical representation
of the underlying generative model (GM) [ 17, 18], without modiﬁcation of the
GM itself. The contributions of this paper are as follows:
1In this context, “action” refers to the path integral, and is distinct from “action” in the
context of an intervention.
2We will use “belief” and “distribution” interchangeably.
3We use “controls” refer to quantities in the generative model, and “actions” (in the
intervention sense) to refer to quantities in the external environment.
2
• We formulate the CBFE as an objective for epistemic-aware active inference
(Sec. 2.6) that can be interpreted as message passing on a GM (Sec. 6);
• We interpret the constituent terms of the CBFE objective as drivers for
behavior (Sec. 4);
• We illustrate our interpretation of the CBFE by planning and interacting
with a simulated T-maze environment (Sec. 5).
• Simulations show that the CBFE agent plans epistemic policies multiple
time-steps ahead (Sec. 6.2), and accrues reward for a signiﬁcantly larger
set of scenarios than the EFE (Sec. 7).
The main advantage of AIF with the CBFE objective, is that it allows
inference to be fully automated by message passing, while retaining the epistemic
qualities of the EFE. Automated message passing absolves the need for manual
derivations and removes computational barriers in scaling AIF to more demanding
settings [19].
2 Problem Statement
In this section we will introduce the free energy objectives as used throughout
the paper. We start by introducing the Variational Free Energy (VFE), and
explain how a VFE can be employed in an AIF context for perception and
policy planning. We then introduce the Expected Free Energy (EFE) as a
variational objective that is explicitly designed to yield epistemic behavior in
AIF agents, but also note that the EFE deﬁnition limits itself to (hierarchical)
state-space models. We then introduce the Bethe Free Energy (BFE), and argue
that the BFE allows for convenient optimization on free-form models by message
passing, but note that the BFE lacks information-seeking qualities. We conclude
this section by introducing the Constrained Bethe Free Energy (CBFE), which
equips the BFE with information-seeking qualities on free-form models through
additional constraints on the variational distribution.
Table 1 summarizes notational conventions throughout the paper.
2.1 Variational Free Energy
The Variational Free Energy (VFE) is a principled metric in physics, where a
time-integral over free energy is known as the action functional. Many natural
laws can be derived from the principle of least action, where the action functional
is minimized with the use of variational calculus [1, 20].
3
Table 1: Summary of notational conventions throughout the paper.
Notation Def. Explanation
s Collection of (arbitrary) model variables
sj Individual model variable with index j ∈S
f(s) (1) Factorized model of variables s
fa(sa) (1) Factor (conditional or prior probability distribution) with argument variables
sa and index a∈F
q(s) (2) Variational distribution of (latent) variables s
Uq(s)[f(s)] (2) Average energy
H[q(s)] (2) Entropy
F[q] (2) Variational Free Energy
yk, xk, uk Observation, state and control variable (at time k) respectively
ˆyk Speciﬁc realization for observation or unobserved future (predicted) outcome
ˆuk Speciﬁc control realization
p(yk,xk|xk−1,uk) (7) Generative Model engine
p(yk|xk) (7) Observation model
p(xk|xk−1,uk) (7) Transition model
p(xt−1) (8) State prior
y, x, u Sequence of future observation variables yt:t+T−1, state variables xt−1:t+T−1
and control variables ut:t+T−1, respectively
ˆuj Policy (sequence of speciﬁc future controls), ˆuj ∈C, where index j is usually
omitted
F∗(ˆuj) (12) Optimized Variational Free Energy for policy ˆuj
ˆu∗ (13) Optimal policy ˆu∗∈C
G[q; ˆuj] (14) Expected Free Energy (EFE)
p(y|x) (16a) Aggregate observation model
p(x|u) (16b) Aggregate state transition model, including state prior
˜p(y) (18) Goal prior for sequence of future observation variables
˜p(yk) (18) Goal prior for observation variable at time k
f(y,x|u) (19), (25) Factorized model of future variables (at time t), for EFE and (C)BFE respec-
tively
B[q] (23) Bethe Free Energy (BFE)
HB[q] (24) Bethe entropy
B[q; ˆuj] (26) Bethe Free Energy of future model under policy ˆuj
B[q; ˆuj,ˆy] (28) Constrained Bethe Free Energy (CBFE) of future model under policy ˆuj and
predicted outcomes ˆy
4
The VFE is deﬁned with respect to a factorized generative model (GM).
We consider a GM f(s) with factors {fa|a∈F} and variables {si|i∈S} that
factorizes according to
f(s) =
∏
a∈F
fa(sa) , (1)
where sa collects the argument variables of the factors fa. As a notational
convention, we write collections and sequences in bold script. In the model fac-
torization of (1), the factors fa would correspond with the prior and conditional
probability distributions that deﬁne the GM. The VFE is then deﬁned as a
functional of an (approximate) posterior q(s) over latent variables, as
F[q] = Eq(s)
[
log q(s)
f(s)
]
= Uq(s)[f(s)] −H[q(s)] , (2)
which consists of an average energyUq(s)[f(s)] = −Eq(s)[log f(s)] and an entropy
H[q(s)] = −Eq(s)[log q(s)].
Because the VFE is (usually) optimized with respect to the posterior q with
the use of variational calculus [ 14], the posterior q is also referred to as the
variational distribution. In this paper, we will strictly reserve the q notation for
variational distributions.
We can relate the exact posterior belief with the model deﬁnition through a
normalizing constant Z, as
p(s) = f(s)
Z , (3)
where
Z =
∑
s
f(s) . (4)
Throughout this paper, summation can be replaced by integration in the case of
continuous variables.
In a Bayesian context, the normalizer Z is commonly referred to as the
marginal likelihood or evidence for model f. However, exact summation
(marginalization) of (4) over all variable realizations is often prohibitively diﬃcult
in practice, so that the evidence and exact posterior become unobtainable.
Substituting (3) in the VFE (2) expresses the VFE as an upper bound on
the surprise, that is the negative log-model evidence, as
F[q] = KL[ q(s)∥p(s)]  
posterior divergence
−log Z  
surprise
. (5)
The marginalization problem of (4) is thus converted to an optimization problem
over q. After optimization,
q∗= arg min
q
F[q] , (6)
5
the VFE approximates the surprise, and the optimal variational distribution
becomes an approximation to the true posterior, p(s) ≈q∗(s).
Crucially, we are free to choose constraints on q such that the optimization
becomes practically feasible, at the cost of an increased posterior divergence.
One such approximation is the Bethe assumption, as we will see in Sec. 2.5.
2.2 Inference for Perception
We now return to the model deﬁnition of (1). In the context of AIF, a Generative
Model (GM) comprises of a probability distribution over states xk, observations
yk and controls uk, at each time index k. We will use a hat to indicate speciﬁc
variable realizations, i.e. ˆyk for a speciﬁc outcome and ˆuk for a speciﬁc control
at time k. As a notational convention, we use k as an arbitrary time index, often
used in the context of iterations, and we use t to indicate the current simulation
time index.
We deﬁne a state-space model [ 21] for the generative model engine, which
represents our belief about how observations follow from a given control and
previous state, as
p(yk,xk|xk−1,uk) = p(yk|xk)  
observation
model
p(xk|xk−1,uk)  
transition
model
. (7)
We use a prior belief about past states p(xt−1) together with the generative
model engine (7) to deﬁne a generative model for perception
f(yt,xt,xt−1|ut) = p(xt−1) p(yt,xt|xt−1,ut)
= p(xt−1) p(yt|xt) p(xt|xt−1,ut) , (8)
which, after substitution in (2), results in the VFE objective for perception,
F[q] = Uq(xt,xt−1)[f(ˆyt,xt,xt−1|ˆut)] −H[q(xt,xt−1)] . (9)
At each time t, the process of perception then relates to inferring the opti-
mal variational distribution q∗(xt,xt−1) about latent states, given the current
action ˆut and resulting outcome ˆyt. The resulting variational distribution can
then be used as a prior for the next time-step, such that p(xt) ≜ q∗(xt) =∑
xt−1 q∗(xt,xt−1) .
2.3 Inference for Planning
At each time t, planning is concerned with selecting optimal future controls by
minimizing a Free Energy (FE) objective that is deﬁned with respect to future
variables. We write y = yt:t+T−1, x = xt−1:t+T−1, and u = ut:t+T−1 as the
sequences of future observations, states and controls respectively, for a ﬁxed-time
horizon of T time-steps ahead.
We will refer to a speciﬁc future control sequence ˆu as a policy. The optimal
policy ˆu∗ is then referred to as the active policy, where (local) optimality is
6
indicated by an asterisk. Inference for planning then aims to select the optimal
policy (in terms of FE) from a collection of candidate policies ˆuj ∈C, where
Crepresents the ﬁnite set of all (user-provided) candidate policies, and j the
selected policy index.
When we view the candidate policy ˆuj as a model selection variable, the
problem of policy selection becomes equivalent to the problem of Bayesian
model selection, where we wish to ﬁnd a probabilistic model with the highest
posterior probability among some given candidate models. When there is no
prior preference about models, the optimal model is the one with the highest
marginal likelihood (evidence).
Given a model f(y,x|u) of future observations and states given a future
control sequence, we can express the marginal likelihood (evidence) for a speciﬁc
policy choice, as
Zj =
∑
y
∑
x
f(y,x|ˆuj) . (10)
Using (3), we can then relate the exact posterior belief with the variational
distribution and the policy evidence, as
p(y,x|ˆuj) = f(y,x|ˆuj)
Zj
. (11)
Under optimization of q, the minimal VFE then approximates the surprise (5),
as
F∗(ˆuj) = min
q
F[q; ˆuj] ≥−log Zj. (12)
The optimal policy then minimizes the optimized VFE, as
ˆu∗= arg min
ˆuj∈C
F∗(ˆuj) . (13)
In the following, we omit the explicit indexing of the policy on j for notational
convenience, and simply write ˆu to represent a speciﬁc policy choice.
Because the VFE (2) involves expectations over the full joint variational
distribution, it may become prohibitively expensive to compute for larger models.
Therefore, additional assumptions and constraints on the VFE are often required.
As a result, multiple free energy objectives for policy planning have been proposed
in the literature, e.g., the Expected Free Energy (EFE) [ 4, 22], the Free Energy
of the Expected Future [ 23], the Generalized Free Energy [ 5], the Predicted
(Bethe) Free Energy [6], and marginal approximations [11].
2.4 Expected Free Energy
The Expected Free Energy (EFE) is an FE objective for planning that is
explicitly constructed to elicit information-seeking behavior [ 4]. Because future
7
observations are (by deﬁnition) unknown, the EFE is deﬁned in terms of an
expectation that includes observation variables, as
G[q; ˆu] = Eq(y,x)
[
log q(x)
f(y,x|ˆu)
]
. (14)
Construction of the (Markovian) model for the EFE starts by stringing
together a state prior with the generative model engine of (7) for future times,
as
p(y,x|u) = p(xt−1)
t+T−1∏
k=t
p(yk,xk|xk−1,uk)
= p(xt−1)
t+T−1∏
k=t
p(yk|xk) p(xk|xk−1,uk) , (15)
where the state prior p(xt−1) follows from the perceptual process (Sec. 2.2).
For notational convenience, we often group the observation and state transition
models (including the state prior), according to
p(y|x) =
t+T−1∏
k=t
p(yk|xk) (16a)
p(x|u) = p(xt−1)
t+T−1∏
k=t
p(xk|xk−1,uk) . (16b)
From the future generative model engine (15), the EFE deﬁnes a state
posterior
p(x|y,u) = p(y,x|u)∑
x p(y,x|u) . (17)
Note that our notation diﬀers from [ 4], where posterior distributions are denoted
by q. We strictly reserve the q notation for variational distributions.
We introduce goal priors ˜p(yk) over observation variables. Goal priors encode
prior beliefs about desired observations (or states) [ 24], and are annotated with
a tilde to avoid confusion with the marginal distribution over the same variable.
We then introduce a shorthand notation that aggregates (independent) goal
priors for the future generative model, as
˜p(y) =
t+T−1∏
k=t
˜p(yk) . (18)
Together with the aggregated goal prior, the factorized model for the EFE is
then constructed as
f(y,x|u) = p(x|y,u) ˜p(y) . (19)
There are several things of note about the model of (19):
8
• The model includes variables that pertain to future time-points, t≤k≤
t+ T −1. As a result, the future observation variables y are latent;
• The model includes a state prior that is a result of inference for perception;
• The (informative) goal priors ˜p introduce a bias in the model towards
desired outcomes;
• Candidate policies will be given, as indicated by a conditioning on controls.
Upon substitution of (19), the EFE (14) factorizes into an epistemic and an
extrinsic value term [4], as
G[q; ˆu] = −Eq(y,x)
[
log p(x|y,ˆu)
q(x)
]
  
epistemic value
−Eq(y)[log ˜p(y)]  
extrinsic value
, (20)
where the epistemic value relates to a mutual information between states and
observations. This decomposition is often used to motivate the epistemic qualities
of the EFE.
An alternative decomposition, in terms of ambiguity and observation risk,
can be obtained under the assumptions q(y|x) ≈p(y|x) (approximation of
the observation model), and q(x|y) ≈ p(x|y,u) (approximation of the ex-
act posterior). These assumptions allow us to rewrite the exact relationship
q(y,x) = q(y|x) q(x) = q(x|y) q(y) in terms of the approximations q(y,x) ≈
p(y|x) q(x) ≈p(x|y,u) q(y). As a result, we obtain
G[q; ˆu] ≈Eq(y,x)
[
log q(y)
p(y|x) ˜p(y)
]
≈Eq(x)[H[p(y|x)]]  
ambiguity
+ KL[q(y)∥˜p(y)]  
observation risk
, (21)
where q(x) and q(y) on the r.h.s. are implicitly conditioned on ˆu. This decom-
position is often used to motivate the explorative (ambiguity reducing) qualities
of the EFE.
In the current paper we evaluate the EFE in accordance with [ 4, 25], for
which the procedure is detailed in Appendix A.
Although the EFE leads to epistemic behavior, it does not ﬁt the general
functional form of the VFE (2), where the expectation and numerator deﬁne the
same variational distribution. As a result, EFE minimization by message passing
requires custom deﬁnitions, and limits itself to (hierarchical) state-space models.
Furthermore, note that the EFE involves the state posterior p(x|y,u) as part
of its deﬁnition, which is technically a quantity that needs to be inferred. The
EFE thus conﬂates the deﬁnition of the planning objective with the inference
procedure for planning itself.
9
2.5 Bethe Free Energy
The Bethe Free Energy (BFE) deﬁnes a variational distribution that factorizes
according to the Bethe assumption
q(s) ≜
∏
a∈F
qa(sa)
∏
i∈S
qi(si)1−di , (22)
where the degree di counts how many qa’s contain si as an argument. After
substituting the Bethe assumption (22) in the VFE (2), we obtain the BFE,
B[q] =
∑
a∈F
Uqa(sa)[fa(sa)] −
∑
a∈F
H[qa(sa)] −
∑
i∈S
(1 −di) H[qi(si)] , (23)
as a special case of the VFE. The entropy contributions are often summarized
in the Bethe entropy, as
HB[q] =
∑
a∈F
H[qa(sa)] +
∑
i∈S
(1 −di) H[qi(si)] . (24)
Because the BFE fully factorizes into local contributions in Fand S, it can
be optimized by message passing on the generative model [ 15, 14, 26, 16]. In
the context of AIF, the BFE for a model over future states is also referred to as
the Predicted Free Energy [6].
For a ﬁxed time-horizon T, the factorized model for future states is con-
structed from the generative model engine and goal prior, as
f(y,x|u) = p(y,x|u) ˜p(y)
= p(xt−1)
t+T−1∏
k=t
p(yk,xk|xk−1,uk) ˜p(yk)
= p(xt−1)
t+T−1∏
k=t
p(yk|xk) p(xk|xk−1,uk) ˜p(yk) . (25)
Because the generative model engine and goal priors introduce a simultaneous
constraint on future observations, the model of(25) represents a scaled probability
distribution. The BFE of the future model under policy ˆu then becomes
B[q; ˆu] = Uq(y,x)[f(y,x|u)] −HB[q(y,x)] . (26)
A major advantage of the BFE over the EFE as an objective for AIF is
that message passing implementations can be automatically derived on free
form models, thus greatly enhancing model ﬂexibility. A drawback of the BFE,
however, is that it lacks the epistemic qualities of the EFE [6], see also Sec. 4.
2.6 Constrained Bethe Free Energy
The Constrained Bethe Free Energy (CBFE) that we propose in this paper
combines the epistemic qualities of the EFE with the computational ease and
10
model ﬂexibility of the BFE. The CBFE can be derived from the BFE by
imposing additional constraints on the variational distribution
q(y,x) ≜ q(x) δ(y −ˆy)
= q(x)
t+T−1∏
k=t
δ(yk −ˆyk) , (27)
where δ(·) deﬁnes the appropriate (Kronecker or Dirac) delta function for the
domain of the observation variable yk (discrete or continuous). The point-mass
(delta) constraints of the CBFE are motivated by the following key insight:
although the future is unknown, we know that we will observe something in the
future. However, because future outcomes are by deﬁnition unobserved, the ˆyk
encode potential outcomes that need to be optimized for.
For the model of (25), the CBFE then becomes 4
B[q; ˆu,ˆy] = Uq(x) δ(y−ˆy)[f(y,x|ˆu)] −HB[q(x) δ(y −ˆy)]
= Uq(x)[f(ˆy,x|ˆu)] −HB[q(x)] . (28)
The current paper investigates how point-mass constraints of the form (27)
aﬀect epistemic behavior in AIF agents.
3 Methods
To minimize the (Constrained) Bethe Free Energy, the current paper uses
message passing on a Forney-style factor graph (FFG) representation [ 27] of the
factorized model (1). In an FFG, edges represent variables and nodes represent
the functional relationships between variables (i.e. the prior and conditional
probabilities).
Especially in a signal processing and control context, the FFG paradigm
leads to convenient message passing formulations [ 28, 29]. Namely, inference can
be described in terms of messages that summarize and propagate information
across the FFG. The BFE is well-known for being the fundamental objective of
the celebrated sum-product algorithm [ 15], which has been formulated in terms
of message passing on FFGs [ 30]. Extensions of the sum-product algorithm
to hybrid formulations, such as variational message passing (VMP) [ 17] and
expectation maximization (EM) [ 31] have also been formulated as message
passing on FFGs. More recently, more general hybrid algorithms have been
described in terms of message passing, see e.g. [ 32, 33]. A comprehensive
overview is provided in [ 16], where additional constraints, including point-mass
constraints, are imposed on the BFE and optimized for by message passing on
FFGs.
4For continuous variables we need to additionally assume that the entropy of a Dirac delta
H[δ(·)] = 0 [16].
11
3.1 Forney-Style Factor Graph Example
Let us consider an example model (1) that factorizes according to
f(s1,s2,s3,s4) = fa(s1) fb(s1,s2,s3) fc(s3) fd(s2,s4) . (29)
The FFG representation of (29) is depicted in Fig. 1 (left).
fa fb
fc
fd
s1
s3
s2
s4
fa fb
fc
fd
s1
s3
s2
s4
δ
1
→
2
→
3↑
4
←
5
←
6 ↓
7↑
Figure 1: Forney-style factor graph representation for the example model of (29)
(left) and message passing schedule for the Bethe Free Energy minimization of
(30) (right). Shaded messages indicate variational message updates, and the
solid square node indicates given (clamped) values. The round node indicates a
point-mass constraint for which the value is optimized.
Now suppose we observe s4 and introduce a point-mass constraint on s3. The
variational distribution then factorizes as
q(s1,s2,s3) = q(s1,s2) q(s3)
= q(s1,s2) δ(s3 −ˆs3) , (30)
where s4 is excluded from the variational distribution because it is observed and
therefore no longer a latent variable. Substituting (29) and (30) in (28) yields
the CBFE as4
B[q; ˆs3] = Uq(s1,s2)[f(s1,s2,ˆs3,ˆs4)] −H[q(s1,s2)] , (31)
where we directly substituted the observed value ˆs4 into the factorized model.
In this paper we adhere to the notation in [ 16], and indicate point-mass con-
straints by an unshaded round node with an annotated δ on the corresponding
edge of the FFG. A solid square node indicates a given value (e.g., an action,
observed outcome or given parameter), whereas an unshaded round node indi-
cates a point-mass constraint that is optimized for (e.g. a potential outcome).
Unshaded messages indicate sum-product messages [ 30] and shaded messages
indicate variational messages, as scheduled and computed in accordance with
[17]. The ForneyLab probabilistic programming toolbox [ 18] implements an
automated message passing scheduler and a lookup table of pre-derived message
updates [28, 34, App. A].
Variational optimization of (31) then yields the (iterative) message passing
schedule of Fig. 1 (right), where ˆs3 is initialized. After computation of the
12
messages, the mode of the product between message 6 and 7 becomes the
new value for ˆs3, and the schedule is repeated until convergence. The result-
ing optimization procedure then resembles an expectation maximization (EM)
scheme where 6 acts as a likelihood and 7 as a prior [ 31], and where upon
each iteration the value ˆs3 is updated with the maximum a-posteriori (MAP)
estimate.
4 Value Decompositions
In this section we further investigate the drivers for behavior of the (C)BFE.
We assume that all variational distributions factorize according to the Bethe
assumption (22).
4.1 Conﬁdence and Complexity
We substitute the model of (25) in the CBFE deﬁnition of (28) and combine to
identify three terms, as
B[q; ˆy,ˆu] = Uq(x) δ(y−ˆy)[p(y,x|ˆu)] + Uδ(y−ˆy)[˜p(y)] −HB[q(x)δ(y −ˆy)]
= Uq(x) δ(y−ˆy)[p(y|x)] −HB[δ(y −ˆy)] + Uq(x)[p(x|ˆu)] −HB[q(x)] + Uδ(y−ˆy)[˜p(y)]
= Eq(x)[KL[δ(y −ˆy)∥p(y|x)]]  
negative conﬁdence
+ KL[q(x)∥p(x|ˆu)]  
complexity
− log ˜p(ˆy)  
extrinsic value
.
(32)
Table 2 is provided as an overview, and summarizes the properties of the
individual terms of (32) under optimization.
The extrinsic value induces a preference for extrinsically rewarding future
outcomes.
Minimizing complexity prefers policies that induce transitions that are in
line with state beliefs, and (vice versa) prefers state beliefs that remain close the
policy-induced state transitions.
The conﬁdence expresses the expected diﬀerence (divergence) in information
between the outcomes as predicted by the observation model and the most
likely (expected) outcome. In other words, this term quantiﬁes the information
diﬀerence between predictions and absolute certainty about outcomes. While
the negative conﬁdence term could be interpreted as an ambiguity (deviation
from certainty), we choose this alternative terminology to prevent confusion with
the ambiguity as deﬁned in (21).
Speciﬁcally, the ambiguity (21) and negative conﬁdence (32) are both of
the form4 Uq(y,x)[p(y|x)] = −Eq(y,x)[log p(y|x)]. However, where the ambigu-
ity approximates q(y,x) ≈p(y|x) q(x) (21), the conﬁdence deﬁnes q(y,x) ≜
q(x) δ(y −ˆy) (27). As a result, the ambiguity explicitly accounts for the full
spread of p(y|x), whereas the conﬁdence evaluates p(y|x) at the expected maxi-
mum ˆy.
13
Maximizing conﬁdence prefers outcomes that are in line with predictions, and
simultaneously tries to maximize the precision of state beliefs (Table 2), see also
[35, p. 2093]. Note that all terms act in unison – the precision of state beliefs is
simultaneously inﬂuenced by the complexity, which prevents the collapse of the
state belief to a point-mass.
Table 2: Optima for the individual terms of the CBFE decompositions (32),
(34). Each row varies one quantity (variable or function) in their respective term
while other quantities remain ﬁxed. Shaded cells indicate that the term (row) is
not a function(al) of that speciﬁc optimization quantity (column).
Vary
Optimize Fix ˆy ˆu q(x)
max ex. val. ˆy that maximizes ˜p(ˆy)
min
complexity
q(x) ˆu ∈C that renders p(x|ˆu)
closest to q(x)
ˆu q(x) = p(x|ˆu)
max
conﬁdence
q(x) ˆy that maximizes
expected outcomes 4
Eq(x)[log p(ˆy|x)]
ˆy q(x) = δ(x −ˆx) where ˆx
maximizes the likelihood
p(ˆy|x)
max
intrinsic
value
ˆu ˆy that maximizes the
evidence4 p(ˆy|ˆu)
ˆy ˆu ∈C that renders p(y|ˆu)
closest to δ(y −ˆy)
min
posterior
divergence
q(x), ˆu ˆy that renders p(x|ˆy,ˆu)
closest to q(x)
q(x), ˆy ˆu ∈ C that renders
p(x|ˆy,ˆu) closest to q(x)
ˆy, ˆu q(x) = p(x|ˆy,ˆu)
4.2 Intrinsic and Extrinsic Value
A second decomposition of the CBFE objective follows when we rewrite the
factorized model of (25) using the product rule, as
f(y,x|u) = p(y,x|u) ˜p(y)
= p(x|y,u) p(y|u) ˜p(y) . (33)
14
Substituting (33) in the CBFE deﬁnition (28) and combining terms, then
yields
B[q; ˆy,ˆu] = Uq(x) δ(y−ˆy)[p(y,x|ˆu)] + Uδ(y−ˆy)[˜p(y)] −HB[q(x)δ(y −ˆy)]
= Uq(x)[p(x|ˆy,ˆu)] −HB[q(x)] + Uδ(y−ˆy)[p(y|ˆu)] −HB[δ(y −ˆy)] + Uδ(y−ˆy)[˜p(y)]
= KL[q(x)∥p(x|ˆy,ˆu)]  
posterior divergence
+ KL[δ(y −ˆy)∥p(y|ˆu)]  
negative intrinsic value
− log ˜p(ˆy)  
extrinsic value
. (34)
Table 2 again summarizes the properties of the individual terms of (34) under
optimization.
The second term of (34) expresses the diﬀerence (divergence) in information
between the predicted outcomes and the point-mass constrained (expected)
outcome. In contrast to the extrinsic value (third term), this term quantiﬁes a
(negative) intrinsic value that purely depends upon the agent’s intrinsic beliefs
about the environment (state prior and generative model engine (15)). Under
optimization (Table 2), this term prefers policies that lead to precise predictions
for the outcomes.
The posterior divergence (ﬁrst term) is always non-negative and will diminish
under optimization, which allows us to combine (32) and (34) into 4
log p(ˆy|ˆu)  
predicted
log-evidence
≥Eq(x)[log p(ˆy|x)]  
conﬁdence
−KL[q(x)∥p(x|ˆu)]  
complexity
. (35)
Interestingly, (35) tells us that the intrinsic value of (34) relates to the model
evidence as predicted under the policy and resulting expected outcomes. Inference
for planning with the CBFE then attempts to make precise predictions for
outcomes by maximizing predicted model evidence. In this view, the CBFE
for planning can be considered (quite literally) as self-evidencing [ 36, 3]. As
a result of selected actions, environmental outcomes may still be surprising
under current generative model assumptions. Inference for perception then
subsequently corrects the generative model priors (Sec. 2.2), and the action-
perception loop repeats (see also Alg. 1). Epistemic qualities then emerge from
this continual pursuit of evidence [37].
In short, we note a distinction in the interpretation of epistemic value between
the EFE and the CBFE. In the EFE (20), epistemic value is directly related
with a mutual information term between states and outcomes. In the CBFE,
the epistemic drive appears to result from a self-evidencing mechanism.
4.3 Bethe Free Energy Value Decomposition
The BFE does not permit an interpretation in terms of intrinsic value. When
we substitute (33) in the BFE deﬁnition of (26) and combine terms, we obtain
B[q; ˆu] = Uq(y,x)[p(y,x|ˆu)] + Uq(y)[˜p(y)] −HB[q(y,x)]
= KL[q(y,x)∥p(y,x|ˆu)] −Eq(y)[log ˜p(y)]
15
= Eq(y)[KL[q(x|y)∥p(x|y,ˆu)]]  
expected posterior divergence
+ KL[q(y)∥p(y|ˆu)]  
predictive divergence
− Eq(y)[log ˜p(y)]  
expected extrinsic value
.
(36)
The intrinsic value term of (34) has been replaced by a predictive divergence
in (36). This term expresses the diﬀerence (divergence) in information between
the observations as predicted by the model under policy ˆu, and variational
distribution about outcomes q(y). Under optimization of q(x|y), the posterior
divergence will vanish for all y. Without the additional point-mass constraint
of (27), the predictive divergence then no longer quantiﬁes the information
diﬀerence between uncertainty (predictive distribution) and certainty (predicted
outcomes). As a result, the BFE lacks the self-evidencing qualities and resulting
epistemic drive of the CBFE, as we will further illustrate in Sec. 4.4 and 6.
4.4 Example Application
We illustrate our interpretation of (34) and (36) by a minimal example model.
We consider a two-armed bandit, where an agent chooses between two levers,
u ∈{0,1}. Each lever oﬀers a distinct probability for observing an outcome
y∈{0,1}. Speciﬁcally, choosing ˆu= 0 will oﬀer a 0 .5 probability for observing
ˆy= 0 (ignorant policy), wheres choosing ˆu= 1 will always lead to the observation
ˆy = 0 (informative policy). We do not equip the agent with any external
preference (there is no extrinsic reward). The agent’s factorized model then
becomes
f(y|u) = p(y= ai|u= aj) = Aij, (37)
with a= (0,1)T and the conditional probability matrix
A=
(0.5 1
0.5 0
)
. (38)
The BFE then follows as
B[q; ˆu] = Uq(y)[p(y|ˆu)] −H[q(y)] . (39)
The CBFE additionally constrains q(y) = δ(y−ˆy), and as a result corresponds
directly with the negative intrinsic value term of (34), as 4
B(ˆy,ˆu) = Uδ(y−ˆy)[p(y|ˆu)] −H[δ(y−ˆy)]
= −log p(ˆy|ˆu) . (40)
The FFG for the model deﬁnition of (37) together with the resulting schedule
for optimization of the (C)BFE is drawn in Fig. 2.
The results of Table 3 show that the BFE does not distinguish between
policies. The CBFE however penalizes the ignorant policy ( ˆu= 0), which does
not predict precise outcomes. This mechanism thus induces a preference for
the informative policy ( ˆu = 1), which does predict precise outcomes. In the
following, we will further investigate this behavior in a less trivial setting.
16
T
u↓
A
→
y1 ↓
p(y|u) T
δ
u↓
A
→
y1 ↓
p(y|u)
Figure 2: Message passing schedule for the example model of (37) for the BFE
(left) and CBFE (right). The dashed box summarizes the observation model.
Table 3: Free energies (in bits) per policy for the example application.
Policy BFE CBFE
Ignorant (ˆu= 0) 0 1
Informative (ˆu= 1) 0 0
5 Experimental Setting
A classic experimental setting that investigates epistemic behavior is the T-maze
task [4]. The T-maze environment consists of four positions P= {1,2,3,4}, as
drawn in Fig. 3. The agent starts in position 1, and aims to obtain a reward that
resides in either arm 2 or 3, R= {2,3}. The position of the reward is unknown
to the agent a priori, and once the agent enters one of the arms it remains there.
In order to learn the position of the reward, the agent ﬁrst needs to move
to position 4, where a cue indicates the reward position. At each position, the
agent may observe one of four reward-related outcomes O= {1,2,3,4}:
1. The reward is indicated to reside at location two (left arm);
2. The reward is indicated to reside at location three (right arm);
3. The reward is obtained;
4. The reward is not obtained.
The key insight is that an epistemic policy would ﬁrst inspect the cue at
position 4 and then move to the indicated arm, whereas a purely goal directed
agent would immediately move towards either of the potential goal positions
instead of visiting the cue.
5.1 Generative Model Speciﬁcation
We follow [4], and assume a generative model with discrete statesxk, observations
yk and controls uk. The state xk ∈P×R , represents the agent position at time
k (four positions, Fig. 3) combined with the reward position (two possibilities).
The state vector thus comprises of eight possible realizations. The transition
17
1
4
2 3
Figure 3: Layout of the T-maze. The agent starts at position 1. The reward is
located at either position 2 or 3. Position 4 contains a cue which indicates the
reward position.
between states is aﬀected by the control uk ∈P, which encodes the agent’s
attempted next position in the maze. The observation variables yk ∈O×P
represent the agent position at time k (four positions) in combination with the
reward-related outcome at that position (four possibilities).
The respective state prior, observation model, transition model and goal
priors are deﬁned as
p(xt−1) = Cat(xt−1|dt−1)
p(yk = ai|xk = bj) = Aij
p(xk = bi|xk−1 = bj,uk = ˆuk) = (Bˆuk)ij
˜p(yk) = Cat(yk|ck) ,
where ai ∈O×P , bi ∈P×R , and bj ∈P×R .
The agent plans two steps ahead ( T = 2), for which the FFG is drawn in
Fig. 4.
5.2 Parameter Assignments
We start the simulation at t= 1, and assume that the initial position is known,
namely we start at position 1 (Fig. 3). However, the reward position is unknown
a priori. This prior information is encoded by the initial state probability vector
d0 = (1,0,0,0)T ⊗(0.5,0.5)T ,
where ⊗denotes the Kronecker product.
The transition matrix Buk encodes the state transitions (from column-index
to row-index), as
B1 =


1 0 0 1
0 1 0 0
0 0 1 0
0 0 0 0

⊗I2, B2 =


0 0 0 0
1 1 0 1
0 0 1 0
0 0 0 0

⊗I2,
18
Cat T =
MUX
T
Cat
dt−1 xt−1
ut
B
But
xt
A
yt
ct
T
MUX
T
Cat
ut+1
B
But+1
xt+1
A
yt+1
ct+1
p(xt−1)
p(xt|xt−1,ut) p(xt+1|xt,ut+1)
p(yt|xt) p(yt+1|xt+1)
˜p(yt) ˜p(yt+1)
Figure 4: Forney-style factor graph of the generative model for the T-maze. The
MUX nodes select the transition matrix as determined by the control variable.
Dashed boxes summarize the indicated distributions.
B3 =


0 0 0 0
0 1 0 0
1 0 1 1
0 0 0 0

⊗I2, B4 =


0 0 0 0
0 1 0 0
0 0 1 0
1 0 0 1

⊗I2 .
The control aﬀects the agent position, but not the reward position. Therefore,
Kronecker products with the two-dimensional unit matrix I2 ensure that the
transitions are duplicated for both possible reward positions. Note that positions
2 and 3 (the reward arms) are attracting states, since none of the transition
matrices allow a transition away from these positions. This means that although
it is possible to propose any control at any time, not all controls will move the
agent to its attempted position. We denote the collection of transition matrices
by B = {B1,B2,B3,B4}.
The observed outcome depends on the position of the agent. The position-
dependent observation matrices specify how observations follow, given the current
19
position of the agent (subscripts) and the reward position (columns), as
A1 =


0.5 0 .5
0.5 0 .5
0 0
0 0

, A2 =


0 0
0 0
α 1 −α
1 −α α

, A3 =


0 0
0 0
1 −α α
α 1 −α

, A4 =


1 0
0 1
0 0
0 0

,
(41)
with reward probability α. The columns of these position-dependent observation
matrices represent the two possibilities for the reward position. The position-
dependent observation matrices combine into the complete block-diagonal, 16-
by-8 observation matrix
A= A1 ⊕A2 ⊕A3 ⊕A4 ,
where ⊕denotes the direct sum (i.e. block-diagonal concatenation).
The goal prior depends upon the future time,
c1 = (0.25,0.25,0.25,0.25)T ⊗(0.25,0.25,0.25,0.25)T (42a)
ck = σ
(
(0,0,c, −c)T ⊗(1,1,1,1)T)
for k> 1 , (42b)
with reward utility c, and σ the soft-max function where σ(s)i = exp(si)∑
jexp(sj) . The
ﬂat prior c1 encodes a lack of external preference at t= 1, while ck for k >1
encodes a preference for observing rewards at subsequent times. This eﬀectively
removes the goal prior for the ﬁrst move ( t= 1), while in subsequent moves the
agent is rewarded for extrinsically rewarding states [19].
6 Inference for Planning
In this simulation we compare the behavior of a CBFE agent to the behavior
of a reference BFE agent (without point-mass constraints). We consider given
policies ˆu, and the optimal CBFE as a function of those policies
B∗
ˆy(ˆu) = min
q,ˆy
B[q; ˆy,ˆu] , (43)
where the ˆy subscript indicates the explicit inclusion of point-mass constraints.
The unconstrained BFE represents the objective where the future observation
variables y are not point-mass constrained by their potential outcomes ˆy. The
unconstrained agent will therefore optimize the joint belief over state and future
observation variables rather than potential outcomes, as
B∗(ˆu) = min
q
B[q; ˆu] . (44)
We will evaluate the BFE, CBFE and EFE for all sixteen ( T = 2) possible
candidate policies ˆu ∈U×U . We consider several scenarios with varying reward
probabilities α (41) and reward utilities c (42).
20
In the current section we do not (yet) consider the interaction of the agent
with the environment. In other words, actions from optimal policy ˆu∗are not
(yet) executed; we are purely interested in the inference for planning itself, and
the resulting free energy values as a function of the candidate policies (43).
6.1 Message Passing Schedule for Planning
The message passing schedules for planning are drawn in Fig. 5 (BFE) and
6 (CBFE), where light messages are computed by sum-product (SP) message
passing updates [ 30], and dark messages by variational message passing updates
[17]. An overview of message passing updates for discrete nodes can be found in
[34, App. A].
=
xt−1
ut ↓
xt
yt
ut+1
↓
xt+1
yt+1
1
→
2
→
4↑
5
→
6
→
9↑
10
←
11 ↓
13
←
14
←
12 ↓ 3↑ 7 ↓ 8↑
pt(xt−1)
p(xt|xt−1,ut) p(xt+1|xt,ut+1)
p(yt|xt) p(yt+1|xt+1)
˜pt(yt) ˜pt+1(yt+1)
Figure 5: Message passing schedule for planning in the T-maze with the BFE.
For the CBFE, the posterior beliefs associated with the observation variables
are constrained by point-mass (Dirac-delta) distributions, see (27), and the
corresponding potential outcomes are optimized for. The message passing
optimization scheme is derived from ﬁrst principles in [ 16]. In order to obtain
a new value, e.g. ˆyt, messages 11 and 12 are multiplied. The mode of the
product then becomes the new value ˆyt, which is used to construct the belief
q(yt) = δ(yt−ˆyt). The updated belief is subsequently used in the next iteration to
compute 3 . The resulting iterative expectation maximization (EM) procedure
initializes values for all ˆyk, and is performed using message passing according to
[31]. Interestingly, where optimization of the EFE is performed by a forward-only
procedure (see Appendix A), optimization of the (C)BFE, as illustrated in Fig. 5
and 6, also includes a complete backward (smoothing) pass over the model.
21
=
xt−1
ut ↓
xt
yt
ut+1
↓
xt+1
yt+1
δ δ
1
→
2
→
3↑
4
→
5
→
6↑
7
←
8 ↓
9
←
10
←
11 ↓
12↑
13 ↓
14↑
pt(xt−1)
p(xt|xt−1,ut) p(xt+1|xt,ut+1)
p(yt|xt) p(yt+1|xt+1)
˜pt(yt) ˜pt+1(yt+1)
Figure 6: Message passing schedule for planning in the T-maze with the CBFE.
6.2 Inference Results for Planning
Optimization of the (C)BFE by message passing is performed with ForneyLab 56
version 0.11.4 [ 18]. Free energies for planning, for three diﬀerent agents and
T-maze scenarios, are plotted in Fig. 7. The distinct agents optimize the CBFE,
BFE and EFE, respectively. We summarize the most important observations
below.
The ﬁrst column of diagrams in Fig. 7 shows the results for the CBFE agent,
for varying scenarios.
• The ﬁrst scenario for the CBFE agent (upper left diagram) imposes a likely
reward (α= 0.9) and positive reward utility ( c= 2). In this scenario, the
CBFE agent prefers the informative policies (4,2) and (4,3), where the
agent seeks the cue in the ﬁrst move and the reward in the second move.
An epistemic (information seeking) agent would prefer these policies in
this scenario.
• In the upper left diagram, note the lack of preference between position 2
and 3 in the second move. Because the policy is not yet executed (moves
are only planned), the true reward location remains unknown. Therefore,
both of these informative policies are on equal footing.
The second column of diagrams shows the results for the BFE agent.
5ForneyLab is available at https://github.com/biaslab/ForneyLab.jl.
6Simulation source code is available athttps://biaslab.github.io/materials/epistemic_
search.zip.
22
• In every scenario, the BFE agent fails to distinguish between the majority
of ignorant (ﬁrst move to 1), informative (ﬁrst move to 4) and greedy
policies (ﬁrst move to 2 or 3). These policy preferences do not correspond
with the anticipated preferences of an epistemic agent.
• Comparing the BFE with the CBFE results, we observe that the point-
mass constraint on potential outcomes induces a diﬀerentiation between
ignorant, informative and greedy policies.
• More speciﬁcally, the third scenario (third row of diagrams) removes the
extrinsic value of reward ( c = 0). While the CBFE still diﬀerentiates
between ignorant, informative and greedy policies, the BFE agent exhibits
a total lack of preference.
• The second scenario (second row of diagrams) removes the value of infor-
mation about the reward position ( α= 0.5). This scenario thus renders
the cue worthless. The BFE agent appears insusceptible to a change in
the epistemic α parameter.
Taken together, these observations support the interpretation of the BFE as a
purely extrinsically driven objective (Sec. 4).
The third column of diagrams produces the results for an EFE agent, as
implemented in accordance with [4], see also Appendix A.
• In all scenarios, the EFE agent exhibits a consistent preference for the
(4,4) policy. Compared to the CBFE agent, the EFE agent fails to plan
ahead to obtain future reward after observing the cue.
• As we will see in Sec. 7, the EFE agent only infers a preference for a reward
arm (position 2 or 3) after execution of the ﬁrst move to the cue position.
In contrast, the CBFE agent predicts the impact of information and plans
accordingly.
• The second scenario (middle row) provides an informative cue, but removes
the possibility to exploit that information (α= 0.5). Interestingly, the EFE
agent still moves to the que position (for the sake of getting information),
whereas the CBFE agent expresses ambivalence under an inoperable cue.
23
Figure 7: (Constrained) Bethe Free Energies ((C)BFE) and Expected Free
Energies (EFE) (in bits) for the T-maze policies under varying parameter
settings. Each diagram plots the minimized free energy values for all possible
policies (lookahead T = 2), with the ﬁrst move on the vertical axis and the
second move on the horizontal axis. For example, the cell in row 4, column 3
represents the policy ˆu = (4,3), which ﬁrst moves to position 4 and then to
position 3. The values for the optimal policies ˆu∗ are annotated red with an
asterisk.
24
6.3 Results for CBFE Value Decomposition
Simulated values for the CBFE decomposition (32) in the T-maze application
are shown in Fig. 8, for four diﬀerent T-maze scenarios. We summarize the most
important observations below.
The ﬁrst column of diagrams in Fig 8 represents the conﬁdence (34) of the
CBFE objective for all (planned) policies. The conﬁdence prefers (or ties) the
most informative policy (4,4) for all scenarios.
• In the ﬁrst three scenarios (ﬁrst three rows of diagrams), all policies
other than (4,4) dismiss the opportunity to obtain full information about
outcomes on two occasions ( T = 2). This is reﬂected by a negative
conﬁdence value, which measures the average rejected information in bits.
For example, the policy (1,1) rejects two possibilities to obtain 1 bit of
information, leading to a conﬁdence of −2.
• A change in the external value parameter c does not aﬀect the conﬁdence,
which supports the interpretation of the conﬁdence as an intrinsic quantity
(35).
• In the ﬁnal scenario, the greedy policies (moving ﬁrst to position 2 or 3) are
on equal footing with the informative policies (moving ﬁrst to 4). This is
because in the ﬁnal scenario, visiting position 2 or 3 oﬀers the same amount
of information (namely, complete certainty) about the reward position, as
would visiting the cue position.
The complexity (second column of diagrams) opposes changes in state be-
liefs that are unwarranted by the policy-induced state transitions, and guards
against premature convergence of the state precision (Table 2). As a result, the
complexity prefers (or ties) the most conservative policy (1,1) for all scenarios.
• The complexity is unaﬀected by changes in utility (similar to the conﬁ-
dence), which supports the interpretation of the complexity as an intrinsic
quantity (35).
• In the third scenario (α= 0.5), the greedy policies become tied in complex-
ity with (most of) the ignorant policies. Because neither visiting a reward
arm nor remaining at the initial position oﬀers any useful information
about the reward position, the state belief remains unaltered, and these
policies incur no complexity penalty.
The extrinsic value (third column of diagrams) represents the value of external
reward, and leads the agent to pursue extrinsically rewarding states.
• The extrinsic value is unaﬀected by changes in the epistemic reward
probability parameter α, which supports the interpretation of the extrinsic
value as an externally determined quantity.
• In the second scenario the reward utility vanishes (c= 0), and the extrinsic
value becomes indiﬀerent about policies.
25
Figure 8: Conﬁdence, complexity and extrinsic value contributions (in bits) to
the Constrained Bethe Free Energy (32) for the T-maze policies (lookahead
T = 2) under varying parameter settings. Optimal values are indicated red with
an asterisk.
26
7 Interactive Simulation
In this section we compare the resulting behavior of the CBFE agent with a
traditional EFE agent, in interaction with a simulated environment.
7.1 Experimental Protocol
The experimental protocol governs how the agent interacts with its environment.
In our protocol, the action and outcome at time tare the only quantities that are
exchanged between the agent and the environment (generative process). The task
of the agent is then to plan for actions that lead the agent to desired states. We
adapt the experimental protocol of [ 38] for the purpose of the current simulation.
We write the model ft with a time-subscript to indicate the time-dependent
statistics of the state prior as a result of the perceptual process (Sec. 2.2). The
experimental protocol (Alg. 1) then consists of ﬁve steps per time t.
Algorithm 1Experimental protocol.
Given a model f1 with initial state and goal priors
for t= 1 to N do
ˆu∗
t = plan(ft) # Execute the planning algorithm
ˆu∗
t = act(ˆu∗
t) # Select the ﬁrst action
execute(ˆu∗
t) # Execute the action in the simulated environment
ˆyt = observe() # Observe the new environmental outcome
ft+1 = slide(ˆu∗
t,ˆyt) # Prepare the model for the next iteration
end for
The plan step solves the inference for planning (Sec. 2.3), and returns the
active policy ˆu∗
t that represents the (believed) optimal sequence of future controls.
In the act step, the ﬁrst action ˆu∗
t is picked from the policy. The execute step
then subsequently executes this action in the simulated environment. Execution
will alter the state of the environment. In the observe step, the environment
responds with a new observation ˆyt. Given the action and resulting observation,
the slide step then solves the inference for perception (Sec. 2.2) and prepares
the model for the next step.
Inference for the slide step is illustrated in Fig. 9, where message 3 propa-
gates an observed outcome ˆyt, and where message 5 summarizes the information
contained within in the dashed box. Only the dashed sub-model is relevant
to the slide step, that is, beliefs about the future do not inﬂuence 5 . After
computation, message 5 is normalized, and the resulting state posterior q∗(xt)
is subsequently used as a prior to construct the model ft+1 for the next time-step,
see also [38].
7.2 Results for Interactive Simulation
We initialize an environment with the reward in the right arm (position 3). We
then execute the experimental protocol of Alg. 1, with lookahead T = 2, for
27
=
xt−1
ut ↓
xt
yt
ut+1
xt+1
yt+1
δ
1
→
2
→
3↑
4↑
5
→
pt(xt−1)
p(xt|xt−1,ut)
p(xt+1|xt,ut+1)
p(yt|xt) p(yt+1|xt+1)
˜pt(yt) ˜pt+1(yt+1)
Figure 9: Message passing schedule for the slide step.
N = 2 moves, on a dense landscape of varying reward probabilities αand utilities
c (scenarios). After the ﬁrst move, the environment returns an observations to
the agent, which informs the agent about second move. After the second move,
the expected reward that is associated with the resulting position is reported. We
perform 10 simulations per scenario, and compute the average reward probability.
The results of Fig. 10 compare the average rewards of the CBFE agent and the
EFE agent.
From the results of Fig. 10 it can be seen that the region of zero average
reward (dark region in lower left corner) is signiﬁcantly smaller for the CBFE
agent than for the EFE agent. This indicates that the CBFE agent accrues
reward in a signiﬁcantly larger portion of the scenario landscape than the EFE
agent. In the lower left corner, the resulting CBFE agent trajectory becomes
(4, 4), whereas the EFE agent trajectory becomes (4, 1). Although both agents
observe the cue after their ﬁrst move, they do not visit the indicated reward
position in the second move, which leads to zero average reward. Note that
neither objective is explicitly designed to optimize for average reward; both
deﬁne a free energy instead, where multiple simultaneous forces are at play.
28
CBFE Agent EFE Agent
Figure 10: Average reward landscapes for the Constrained Bethe Free Energy
(CBFE) agent and the Expected Free Energy (EFE) agent.
In the upper right regions, with high reward probability and utility, both
agents consistently execute (4,3). With this trajectory, the cue is observed after
the ﬁrst move, and the indicated (correct) reward position is visited in the second
move, leading to an average reward of α. For reward probabilities close to α= 1
however, the performance of both agents deteriorates. In this upper region, the
informative policies become tied with the greedy policies (see Fig. 8), and there
is no single dominant trajectory. In some trajectories the agent enters the wrong
arm on the ﬁrst move, from which the agent cannot escape, and the average
reward deteriorates.
Greedy behavior is also observed for the CBFE agent when informative priors
ck (42) are set for all k (including k= 1), conforming with the conﬁguration of
[4]. With this conﬁguration, expected reward for the CBFE agent deteriorates
to 0.5 in the otherwise rewarding region. Interestingly, this change of priors does
not aﬀect results for the EFE agent. The resulting change in behavior suggests
that the CBFE agent is more susceptible to temporal aspects of the goal prior
conﬁguration. While this eﬀect may be considered a nuisance in some cases, it
also allows for increased ﬂexibility when assigning explict temporal requirements
about goals. For example, assigning an informative versus a ﬂat prior for k= 1
respectively encodes an urgency in obtaining immediate reward versus a freedom
to explore.
29
8 Discussion
In this paper, we focused on epistemic drivers for behavior. We noted that the
nature of the epistemic drive diﬀers between an EFE and CBFE agent. Namely,
the epistemic drive for the EFE agent stems directly from maximizing a mutual
information term between states and observations (20), while the epistemic drive
for the CBFE agent stems from a self-evidencing mechanism (Sec. 4.2). In order
to better understand the strengths and limitations of the driving forces for the
CBFE, it would be interesting to investigate its behavior in more challenging
setups, including continuous variables, inference for control [ 19], and the eﬀects
point-mass constraints on other model variables.
Recent work by [39, 40] shows that epistemic behavior does not occur when
the goal prior goes to a point-mass. The work of [ 39] points to the entropy of
the observed variables H[q(y)] as a pivotal quantity for epistemic behavior. The
CBFE however does not include an entropy over observations, and still exhibits
epistemic qualities. The diﬀerence in methods lies with the constraint quantity;
namely [39, 25] constrain the goal prior ˜p(y) = δ(y −ˆy), while the current paper
constrains the variational distribution q(y) = δ(y −ˆy) instead. While both
constraints remove H[q(y)] from the resulting FE objective, optimization of ˆy in
the CBFE still induces an epistemic drive (Sec. 4). Our results thus show that
epistemic drives for AIF prove to be more subtle than initially anticipated.
Our presented approach is uniquely scalable, because it employs oﬀ-the-shelf
message passing algorithms. All message computations are local, which makes
our approach naturally amenable to both parallel and on-line processing [ 41].
Especially AIF in deep hierarchical models might beneﬁt from the improved
computational properties of the CBFE. It will be interesting to investigate how
the presented approach generalizes to more demanding (practical) settings.
As a generic variational inference procedure, the CBFE approach applies to
arbitrary models. This allows researchers to investigate epistemics in a much
wider class of models than previously available. One immediate avenue for
further research is the integration of CBFE with predictive coding schemes
[42, 43, 44]. Predictive coding has so far been driven mainly by minimizing free
energy in hierarchical models under the Laplace approximation. Here, the CBFE
approach readily applies as well [ 16], allowing researchers to explore the eﬀects
of augmenting existing predictive coding models with epistemic components.
The derivation of alternative functionals that preserve the desirable epistemic
behavior of EFE optimization is an active research area [ 45, 8]. There have been
several interesting proposals such as the Free Energy of the Expected Future
[23, 7, 9] or Generalized Free Energy [ 5], as well as amortization strategies
[46, 47] and sophisticated schemes [ 22]. Comparing behavior between the CBFE
and other free energy objectives might therefore prove an interesting avenue for
future research.
In the original description of active inference, a policy precision is opti-
mized during policy planning, and the policy for execution is sampled from
a distribution of precision-weighted policies [ 4]. The present paper does not
consider precision optimization, and eﬀectively assumes a large, ﬁxed precision
30
instead. In practice, this procedure consistently selects the policy with minimal
free energy; see also maximum selection (in terms of value) as described by
[6]. To accommodate for precision optimization, the CBFE objective might be
extended with a temperature parameter, mimicking thermodynamic descriptions
of free energy [48]. Optimization of the temperature parameter might then relate
to optimization of the policy precision, as often seen in biologically plausible
formulations of AIF [49].
Another interesting avenue for further research would be the design of a
meta-agent that determines the statistics and temporal conﬁguration of the goal
priors. In our experiments we design the goal priors (42) ourselves, such that the
agent is free to explore in the ﬁrst move and seeks reward on the second move.
The challenge then becomes to design a synthetic meta-agent that automatically
generates an eﬀective lower-level goal sequence from a single higher-level goal
deﬁnition.
9 Conclusions
In this paper we presented mathematical arguments and simulations that show
how inclusion of point-mass constraints on the Bethe Free Energy (BFE) leads to
epistemic behavior. The thus obtained Constrained Bethe Free Energy (CBFE)
has direct connections with formulations of the principle of least action in
physics [1], and can be conveniently optimized by message passing on a graphical
representation of the generative model (GM).
Simulations for the T-maze task illustrate how a CBFE agent exhibits
an epistemic drive, whereas the BFE agent lacks epistemic qualities. The
key intuition behind the working mechanism of the CBFE is that point-mass
constraints on observation variables explicitly encode the assumption that the
agent will observe in the future. Although the actual value of these observation
remains unknown, the agent “knows” that it will observe in the future, and
it “knows” (through the GM) how these (potential) outcomes will inﬂuence
inferences about states.
We dissected the CBFE objective in terms of its constituent drivers for
behavior. In the CBFE framework, in addition to being functionals of the state
beliefs, the conﬁdence and complexity are viewed as functions of the potential
outcomes and policy respectively. Simultaneous optimization of variational
distributions and potential outcomes then leads the agent to prefer epistemic
policies. Interactive simulations for the T-maze showed that, compared to an
EFE agent, the CBFE agent incurs expected reward in a signiﬁcantly larger
portion of the scenario landscape.
We performed our simulations by message passing on a Forney-style factor
graph representation of the generative model. The modularity of the graphical
representation allows for ﬂexible model search, and message passing allows for
distributed computations that scale well to bigger models. Constraining the
BFE and optimizing the CBFE objective by message passing thus suggests a
simple and general mechanism for epistemic-aware AIF in free-form generative
31
models.
Conﬂict of Interest Statement
The authors declare that the research was conducted in the absence of any
commercial or ﬁnancial relationships that could be construed as a potential
conﬂict of interest.
Author Contributions
The original idea was conceived by TvdL. All authors contributed to further
conceptual development of the methods that are presented in this manuscript.
Simulations were performed by TvdL and MK. All authors contributed to writing
the manuscript.
Funding
This research was made possible by funding from GN Hearing A/S. This work is
part of the research programme Eﬃcient Deep Learning with project number
P16-25 project 5, which is (partly) ﬁnanced by the Netherlands Organisation for
Scientiﬁc Research (NWO).
Acknowledgments
The authors gratefully acknowledge stimulating discussions with Dimitrije
Markovi´ c of the Neuroimaging group at TU Dresden. The authors also thank
the two reviewers for their valuable comments.
Abbreviations
The following abbreviations are used in the manuscript:
FEP Free Energy Principle
AIF Active Inference
VFE Variational Free Energy
EFE Expected Free Energy
BFE Bethe Free Energy
CBFE Constrained Bethe Free Energy
GM Generative Model
EM Expectation Maximization
FFG Forney-style Factor Graph
VMP Variational Message Passing
SP Sum-Product
MAP Maximum A-Posteriori
32
References
[1] A. Caticha, Entropic Inference and the Foundations of Physics . EBEB-2012,
the 11th Brazilian Meeting on Bayesian Statistics, 2012.
[2] K. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” Journal of Physiology, Paris , vol. 100, pp. 70–87, Sept. 2006.
[3] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
behavior: a free-energy formulation,” Biological cybernetics, vol. 102, no. 3,
pp. 227–260, 2010.
[4] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and G. Pez-
zulo, “Active inference and epistemic value,” Cognitive Neuroscience, vol. 6,
pp. 187–214, Mar. 2015.
[5] T. Parr and K. J. Friston, “Generalised free energy and active inference,”
Biological cybernetics, vol. 113, no. 5, pp. 495–513, 2019. Publisher: Springer.
[6] S. Schw¨ obel, S. Kiebel, and D. Markovi´ c, “Active Inference, Belief Prop-
agation, and the Bethe Approximation,” Neural Computation , vol. 30,
pp. 2530–2567, Sept. 2018.
[7] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley, “Reinforcement
Learning through Active Inference,” arXiv:2002.12636 [cs, eess, math, stat] ,
Feb. 2020. arXiv: 2002.12636.
[8] N. Sajid, F. Faccio, L. Da Costa, T. Parr, J. Schmidhuber, and K. Friston,
“Bayesian brains and the Renyi divergence,”arXiv preprint arXiv:2107.05438,
2021.
[9] D. Hafner, P. A. Ortega, J. Ba, T. Parr, K. Friston, and N. Heess, “Action
and Perception as Divergence Minimization,” arXiv:2009.01791 [cs, math,
stat], Sept. 2020. arXiv: 2009.01791.
[10] B. de Vries and K. J. Friston, “A Factor Graph Description of Deep Temporal
Active Inference,” Frontiers in Computational Neuroscience, vol. 11, 2017.
[11] T. Parr, D. Markovic, S. J. Kiebel, and K. J. Friston, “Neuronal message
passing using Mean-ﬁeld, Bethe, and Marginal approximations,” Scientiﬁc
Reports, vol. 9, p. 1889, Dec. 2019.
[12] T. Champion, M. Grze´ s, and H. Bowman, “Realising Active Inference
in Variational Message Passing: the Outcome-blind Certainty Seeker,”
arXiv:2104.11798 [cs], Apr. 2021. arXiv: 2104.11798.
[13] A. Caticha, “Relative Entropy and Inductive Inference,” AIP Conference
Proceedings, vol. 707, pp. 75–96, 2004. arXiv: physics/0311093.
33
[14] J. S. Yedidia, W. Freeman, and Y. Weiss, “Constructing free-energy approxi-
mations and generalized belief propagation algorithms,” IEEE Transactions
on Information Theory , vol. 51, pp. 2282–2312, July 2005.
[15] J. Pearl, Probabilistic reasoning in intelligent systems: networks of plausible
inference. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.,
1988.
[16] ˙I. S ¸en¨ oz, T. van de Laar, D. Bagaev, and B. de Vries, “Variational Message
Passing and Local Constraint Manipulation in Factor Graphs,” Entropy,
vol. 23, no. 7, p. 807, 2021. Publisher: Multidisciplinary Digital Publishing
Institute.
[17] J. Dauwels, “On Variational Message Passing on Factor Graphs,” in IEEE
International Symposium on Information Theory , (Nice, France), pp. 2546–
2550, June 2007.
[18] M. Cox, T. van de Laar, and B. de Vries, “A factor graph approach to
automated design of Bayesian signal processing algorithms,” International
Journal of Approximate Reasoning , vol. 104, pp. 185–204, Jan. 2019.
[19] T. van de Laar, “Simulating Active Inference Processes With Message
Passing,” May 2018.
[20] C. Lanczos, The variational principles of mechanics . Courier Corporation,
2012.
[21] D. Koller and N. Friedman, Probabilistic graphical models: principles and
techniques. MIT press, 2009.
[22] K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr, “Sophisticated
Inference,” Neural Computation, vol. 33, pp. 713–763, Mar. 2021.
[23] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the Expected Free
Energy?,” arXiv preprint arXiv:2004.08128 , 2020.
[24] K. Friston, P. Schwartenbeck, T. Fitzgerald, M. Moutoussis, T. Behrens,
and R. J. Dolan, “The anatomy of choice: active inference and agency,”
Frontiers in Human Neuroscience, vol. 7, p. 598, 2013.
[25] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston,
“Active inference on discrete state-spaces: a synthesis,” arXiv:2001.07203
[q-bio], Jan. 2020. arXiv: 2001.07203.
[26] M. J. Wainwright and M. I. Jordan, “Graphical Models, Exponential Fam-
ilies, and Variational Inference,” Foundations and Trends® in Machine
Learning, vol. 1, pp. 1–305, Nov. 2008.
[27] G. Forney, “Codes on graphs: normal realizations,” IEEE Transactions on
Information Theory, vol. 47, pp. 520–548, Feb. 2001.
34
[28] S. Korl, A factor graph approach to signal modelling, system identiﬁcation
and ﬁltering . PhD thesis, Swiss Federal Institute of Technology, Zurich,
2005.
[29] H.-A. Loeliger, J. Dauwels, J. Hu, S. Korl, L. Ping, and F. R. Kschischang,
“The Factor Graph Approach to Model-Based Signal Processing,”Proceedings
of the IEEE , vol. 95, pp. 1295–1322, June 2007.
[30] H.-A. Loeliger, “An introduction to factor graphs,” Signal Processing Mag-
azine, IEEE, vol. 21, pp. 28–41, Jan. 2004.
[31] J. Dauwels, S. Korl, and H.-A. Loeliger, “Expectation maximization as
message passing,” in International Symposium on Information Theory, 2005.
ISIT 2005. Proceedings, pp. 583–586, Sept. 2005.
[32] D. Zhang, W. Wang, G. Fettweis, and X. Gao, “Unifying Message Pass-
ing Algorithms Under the Framework of Constrained Bethe Free Energy
Minimization,” arXiv:1703.10932 [cs, math] , Mar. 2017. arXiv: 1703.10932.
[33] T. van de Laar, I. S ¸en¨ oz, A.¨Oz¸ celikkale, and H. Wymeersch, “Chance-
Constrained Active Inference,” arXiv preprint arXiv:2102.08792 , 2021.
[34] T. van de Laar, Automated Design of Bayesian Signal Processing Algo-
rithms. PhD thesis, Eindhoven University of Technology, Eindhoven, The
Netherlands, 2019.
[35] K. Friston and W. Penny, “Post hoc Bayesian model selection,”Neuroimage,
vol. 56, pp. 2089–2099, June 2011.
[36] J. Hohwy, “The Self-Evidencing Brain,” Noˆ us, vol. 50, pp. 259–285, June
2016.
[37] J. Hohwy, “Conscious self-evidencing,”Review of Philosophy and Psychology,
pp. 1–20, 2021. Publisher: Springer.
[38] T. van de Laar and B. de Vries, “Simulating Active Inference Processes by
Message Passing,” Frontiers in Robotics and AI , vol. 6, p. 20, 2019.
[39] B. Millidge, A. Tschantz, A. Seth, and C. Buckley, “Understanding the origin
of information-seeking exploration in probabilistic objectives for control,”
arXiv preprint arXiv:2103.06859 , 2021.
[40] L. Da Costa, N. Sajid, T. Parr, K. Friston, and R. Smith, “The relation-
ship between dynamic programming and active inference: the discrete,
ﬁnite-horizon case,” arXiv:2009.08111 [cs, math, q-bio] , Sept. 2020. arXiv:
2009.08111.
[41] D. Bagaev and B. de Vries, “Reactive Message Passing for Scalable Bayesian
Inference,” arXiv:2112.13251 [cs], Dec. 2021. arXiv: 2112.13251.
35
[42] R. Bogacz, “A tutorial on the free-energy framework for modelling perception
and learning,” Journal of Mathematical Psychology , vol. 76, pp. 198–211,
Feb. 2017.
[43] K. Friston and S. Kiebel, “Predictive coding under the free-energy principle,”
Philosophical Transactions of the Royal Society of London B: Biological
Sciences, vol. 364, no. 1521, pp. 1211–1221, 2009.
[44] B. Millidge, A. Tschantz, and C. L. Buckley, “Predictive coding approx-
imates backprop along arbitrary computation graphs,” arXiv preprint
arXiv:2006.04182, 2020.
[45] A. Tschantz, M. Baltieri, A. K. Seth, and C. L. Buckley, “Scaling active
inference,” in 2020 International Joint Conference on Neural Networks
(IJCNN), pp. 1–8, IEEE, 2020.
[46] B. Millidge, “Deep Active Inference as Variational Policy Gradients,”
arXiv:1907.03876 [cs], July 2019. arXiv: 1907.03876.
[47] K. Ueltzh¨ oﬀer, “Deep Active Inference,”Biological Cybernetics, Oct. 2018.
[48] P. A. Ortega and D. A. Braun, “Thermodynamics as a theory of decision-
making with information-processing costs,” Proceedings of the Royal Society
of London A: Mathematical, Physical and Engineering Sciences , vol. 469,
p. 20120683, May 2013.
[49] T. H. B. FitzGerald, R. J. Dolan, and K. Friston, “Dopamine, reward
learning, and active inference,” Frontiers in Computational Neuroscience,
p. 136, 2015.
[50] H.-A. Loeliger, “Least Squares and Kalman Filtering on Forney Graphs,” in
Codes, Graphs, and Systems (R. E. Blahut and R. Koetter, eds.), vol. 670,
pp. 113–135, Boston, MA: Springer US, 2002.
Appendix
A. Evaluation of the Expected Free Energy
The standard procedure for evaluating the Expected Free Energy (EFE) collects
instantaneous EFE contributions over time by a forward ﬁltering approach [ 4].
Following [4, 25], the EFE constructs an instantaneous model for each future
time-point τ ≥t, as
f(yτ,xτ|ut:τ) = p(xτ|yτ,ut:τ) ˜p(yτ) , (45)
with ˜p(yτ) the goal prior, and p(xτ|yτ,ut:τ) a state posterior that needs to be
further deﬁned.
36
Using Bayes rule, we can express the state posterior in terms of the observation
model and a posterior predictive for the state, as
p(xτ|yτ,ut:τ) = p(xτ|ut:τ)p(yτ|xτ)
p(yτ|ut:τ) (46a)
= p(xτ|ut:τ)p(yτ|xτ)∑
xτ p(xτ|ut:τ)p(yτ|xτ) . (46b)
The posterior predictive p(xτ|ut:τ) is explicitly conditioned on the policy ut:τ,
from current time t up to and including future time τ, and thus represents the
forward prediction (ﬁltering solution) for the current state belief given preceding
controls (whilst excluding preceding goals). Using the generative model engine
deﬁnition, the posterior predictive for the state then becomes 7
p(xτ|ut:τ) =
∑
yt:τ
∑
xt−1:τ−1
p(xt−1)
τ∏
k=t
p(yk,xk|xk−1,uk) (47a)
=
∑
xt−1:τ−1
p(xt−1)
τ∏
k=t
p(xk|xk−1,uk) . (47b)
The second step of (47) simpliﬁes the expression by marginalizing over yt:τ. The
posterior predictive can then conveniently be computed by message passing on
the generative model [50], using a single forward pass.
We are now prepared to construct the instantaneous EFE (the EFE at time
τ), which is deﬁned as [4]
Gτ(ˆut:τ) = Ep(yτ|xτ) p(xτ|ˆut:τ)
[
log p(xτ|ˆut:τ)
f(yτ,xτ|ˆut:τ)
]
. (48)
Upon substitution of (46a) in (48), the instantaneous EFE factorizes into
ambiguity and risk, as
Gτ(ˆut:τ) = Ep(yτ|xτ) p(xτ|ˆut:τ)
[
log p(yτ|ˆut:τ)
p(yτ|xτ) ˜p(yτ)
]
= −Ep(xτ|ˆut:τ)
[
Ep(yτ|xτ)[log p(yτ|xτ)]
]
+ Ep(yτ|xτ) p(xτ|ˆut:τ)
[
log p(yτ|ˆut:τ)
˜p(yτ)
]
= Ep(xτ|ˆut:τ)[H[p(yτ|xτ)]]  
ambiguity
+ KL[p(yτ|ˆut:τ)∥˜p(yτ)]  
observation risk
. (49)
This decomposition is often used to compute the instantaneous EFE in practice.
The complete EFE of the full policy ˆu then follows by summation of all
instantaneous contributions
G(ˆu) =
t+T−1∑
τ=t
Gτ(ˆut:τ) . (50)
7The deﬁnition of [ 4, p. 192] implicitly deﬁnes this forward prediction as a marginalization
over states, which is made explicit in the deﬁnition of (47). In general, this marginalization
need not be tractable, in which case it can also be approximated by on-line optimization of an
appropriate BFE objective on the generative model engine.
37
To summarize, the procedure for computation of the EFE in practice [ 4, 25]
usually consists of three steps. First, for a given policy ˆu, the posterior predictive
distributions (47) are computed for all t≤τ <t+ T. Then, the instantaneous
EFE’s are (individually) computed. Finally, the instantaneous EFE’s are summed
to produce the full-policy EFE (50).
38