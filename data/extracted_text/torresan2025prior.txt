Prior preferences in active inference agents: soft, hard, and
goal shaping
Filippo Torresan 1,2,
 Ryota Kanai 1,
 Manuel Baltieri 1,2,†
1 Araya Inc., Tokyo, Japan
2 School of Engineering and Informatics, University of Sussex, Brighton, UK
Active inference proposes expected free energy as an objective for planning and decision-making
to adequately balance exploitative and explorative drives in learning agents. The exploitative
drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence be-
tween a variational probability distribution, updated at each inference step, and a preference
probability distribution that indicates what states or observations are more likely for the agent,
hence determining the agent’s goal in a certain environment. In the literature, the questions of
how the preference distribution should be specified and of how a certain specification impacts
inference and learning in an active inference agent have been given hardly any attention. In this
work, we consider four possible ways of defining the preference distribution, either providing the
agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals).
We compare the performances of four agents, each given one of the possible preference distri-
butions, in a grid world navigation task. Our results show that goal shaping enables the best
performance overall (i.e., it promotes exploitation) while sacrificing learning about the environ-
ment’s transition dynamics (i.e., it hampers exploration).
Keywords:active inference, Bayesian inference, POMDP , variational free energy, expected free
energy, prior preferences
1. Introduction
Active inference has become an influential computational framework used to account for several as-
pects of cognition and adaptive behaviour in cognitive science and computational neuroscience [13,
15, 21, 36, 37]. The fundamental idea of active inference is that adaptive agents are continuously
engaged in a process of predicting upcoming sensory observations and inferring the best course of
action to minimize prediction error. This kind of perception-action loop is described at different
spatio-temporal levels as a form of variational Bayesian inference, on the hidden states of the en-
vironment, that relies on a (hierarchical) generative model to minimize (variational) free energy, a
proxy for prediction error [29, 14, 21]. In this Bayesian framework, the minimisation of free energy
and expected free energy enables an agent to infer its current state (perception), to infer the best se-
quence of actions (policies) to reach preferred states or observation (planning/goal-directed decision
making), and to progressively learn the transition dynamics and state-observation mappings in the
environment [7, 8, 38, 6, 27, 3].
In contrast to reinforcement learning [47], the active inference framework tries to dispense with
the notion of reward, so it assumes that agents are endowed with goals in the form of prior pref-
erences to be achieved by performing free energy minimisation. In various works deploying active
†Correspondence e-mail:manuel baltieri@araya.org
arXiv:2512.03293v1  [cs.AI]  2 Dec 2025
Prior preferences in active inference agents: soft, hard, and goal shaping
inference agents in more traditional reinforcement learning environments, it is common to use a
preference distribution over observations implicitly defined by regarding the reward signal in the
environment as the desired observation for the agent [52, 51, 12, 42]. However, despite the technical
and theoretical fundamentals of active inference have been reviewed extensively in the literature [4,
9, 23, 24, 28, 31, 35], the issue of how to specify the preference distribution and its impact on inference
and learning in an active inference agent have been largely overlooked.
In this work, our goal is to offer a thorough analysis of how different specifications of the prefer-
ence distribution over states affect perception, decision-making, and learning in an active inference
agent that has to solve a navigation task in a simple grid world. In particular, we consider preference
distributions that vary along two dimensions, i.e., providing the agent (1) with soft goals vs. hard
goals and (2) with goal shaping or not. The former determines how strongly the agent wants to reach
a certain goal, whereas the latter specifies whether to give an agent a series of intermediate goals to
reach the ultimate one.
In Section2we briefly review the fundamental aspects of the active inference framework, with
a focus on expected free energy and on how preference distributions play a key role in it. With
one experiment, we analyze inference and learning in four active inference agents, characterised by
one of the possible preference distributions (based on the considered dimensions of variations), in
a simple grid-world environment (Section3). We will conclude with a discussion of how agent’s
performance and learning is affected by each of the considered preference distributions as well as
with a few more general considerations about the notions of reward and goal-directedness in active
inference (Section4).
2. Active Inference in discrete state spaces
In the discrete state-space formulation of active inference, an agent’s adaptive behaviour is modelled
as a process of variational Bayesian inference given a generative model of the environment. At each
time step, the agent relies on its updated Bayesian beliefs to execute an action from a policy,π∈Π(a
sequence of actions from a set of allowed sequences), so as to access one or more preferred states or
observations (see SectionS1.1for a summary of the notation used hereafter). By acting this way, the
generative model will partially reflect over time (through learning) the emission and transition maps
that jointly characterise the environment’s generative process, i.e., how observations are generated
from states and how actions affect the transition from one state to another, respectively. In the next
few sections, we provide a brief overview of the main components and steps that characterize this
active inference framework. For a more detailed overview of the framework, see e.g. [9, 48].
2.1. The Generative process and the generative model
Both the generative process and model are specified as discrete-time partially observable Markov
decision processes (POMDPs). Formally, we can define the generative process as follows:
Definition 2.1(POMDP in active inference, the generative process).A POMDP is a six-element tuple,
(S,O,A,T,E,T), where:
•Sis a finite set of states,
•Ois a finite set of observations,
•Ais a finite set of admissible actions,
2
Prior preferences in active inference agents: soft, hard, and goal shaping
•S i,O i,A i, withi∈[1,T], are time-indexed random variables defined over the respective spaces,
where the time indexTrepresents a terminal time step,
•T:S × A →∆(S)is a transition function that maps state-action pairs to a probability distribu-
tion in the set∆(S)of probability distribution defined overS
•E:S →∆(O)is an emission function that maps a state to a probability distribution in the set
∆(O)of probability distribution defined overO 1.
The generative model is specified as a joint probability distribution that can be generated by a
POMDP in the sense of Definition2.1. We then define the generative model as follows:
Definition 2.2(Generative model in active inference).The generative modelMof an active inference
agent is a joint probability distribution over a sequence of state and observation random variables, a
policy random variable for sequences of actions, and parameters stored in matrixA(for the emission
map) and tensorB(for the transition map), that is, a joint that factors as:
P(O1:T ,S 1:T ,π,A,B) =P(π)P(A)P(B)P(S 1)
T
∏
t=2
P(St|St−1,π,B)
T
∏
t=1
P(Ot|St−1,A). (1)
The matrixA∈R n×m stores the categorical probability distributionP(Ot|sj;o j)as thejth column,
specifying the probabilities of the observations produced by stateSt =s j, for all state valuess1, . . . ,sm
(those probabilities are stored by the parameter vectoroj thatcoincideswith thejth column ofA). The
tensorB∈R |A|×m×m stores the action-dependent categorical distributionP(S t|sj
t−1,x;s j)as thejth
column ofB x, wherexindicates the action under consideration, specifying the probabilities of the
next state values given the previous stateS t−1 =s j, for all state valuess 1, . . . ,sm and for each action
(again, those probabilities are stored by the parameter vectors j thatcoincideswith thejth column of
Bx). Each column ofAandB x can be seen as an output of an approximation (learned by the active
inference agent) of the emission mapEand the transition mapT, so the generative model has indeed
the same structure of a POMDP as defined in Definition2.1.
The goal of an active inference agent is then, in an intuitive sense, to enforce asynchronization
between the generative model it parameterises and the generative process of the environment it in-
teracts with, see [2] for a more in depth discussion of this reading of active inference. More in detail,
we can imagine that when an agent starts interacting with an environment, before its goal is achieved,
the generative model has yet to capture the emission and transition maps of the generative process,
i.e., the ground-truth POMDP representing the environment. However, the agent can acquire such
knowledge through experience. At each time step, the agent performs inference on the most likely
states corresponding to an observation, which also means revising its probabilistic beliefs about past
and future consequences of performing different sequences of actions, then plans and decides what
action to perform next. At regular intervals, information about an experienced trajectory in the en-
vironment (i.e., a collection of observations plus probabilistic beliefs about the most likely states) is
used to update the generative model’s emission and transition map. Next, we will provide a few
more technical details on this procedure.
1We note that standard definitions of POMDPs [40, Ch. 16, 34, Ch. 34, 47, Ch. 17] include also a notion ofrewardfor
an agent, here we don’t however include them since active inference normally specifies targets for an agent by means of
a prior probability distribution over goal states or observations (see Section2.4). Formally, however, this can be easily
accommodated in the above definition by stating that our observationsOinclude both observationsYand rewardsRof
standard POMDP definitions:O=Y × R. Active inference works involving high-dimensional state spaces have adopted
this approach in practice (see, e.g., [51, 52, 12]).
3
Prior preferences in active inference agents: soft, hard, and goal shaping
2.2. Variational Bayesian inference for POMDPs
An active inference agent learns to perform actions that will lead to its desired observations and/or
states in the environment. Observations received from the environment are evidence or feedback
that can indicate to the agent whether the generative model captures the environmental dynamics
well enough to yield accurate predictions and goal-conducive actions. Such observations are used
to infer the (1) most likely hidden states generating an observation at each time step, the (2) most
likely policy given some preferred states or observations, and the (3) most likely parameters of the
generative model to make more accurate predictions in the environment.
Given the agent’s generative model (see Definition2.2), this process of inference can be imple-
mented by Bayes’ rule, which in this setting corresponds to the following:
P(S1:T ,π,A,B|O 1:T ) =P(O1:T |S1:T ,π,A,B)P(S 1:T ,π,A,B)
P(O1:T ) , (2)
where the generative modelMappears in the numerator, factorised as the product between the
likelihood and the prior probability distributions. The goal here is to find a posterior joint distribution
(the left-hand side) over the state, policy, and transition and emission maps’ parameters random
variables. However, finding an analytic solution to Eq. (2) is often intractable, so active inference
proposes to implement an approximate Bayesian inference scheme based on the minimisation of
variational free energy. This quantity is defined in relation to the available generative model, so it
can be written as follows:
F

Q(S1:T ,π,A,B)
 :=E Q
h
logQ(S 1:T ,π,A,B)−logP(O 1:T ,S 1:T ,π,A,B)
i
, (3)
whereQ(S 1:T ,π,A,B)is known as thevariational posterior, a probability distribution introduced to
approximate the posterior distribution,P(S 1:T ,π,A,B|O 1:T ), in Eq. (2) (the outcome of Bayesian in-
ference). To minimize the free energy defined in Eq. (3), we make some assumptions about the vari-
ational posterior so that the optimisation procedure described above becomes more tractable. In
discrete-time active inference, it is thus common to adopt amean-fieldapproximation [9], meaning
that the variational posterior is factorised as follows:
Q(S1:T ,π,A,B) =Q(A)Q(B)Q(π)
T
∏
t=1
Q(St|π). (4)
By substituting this expression in Eq. (3) for the variational posterior, and by considering the
factorization of the generative model, we can rewrite the free energy as follows (cf., [9]):
F

Q(S1:T ,π,A,B)

=DKL

Q(A)
 P(A)

+D KL

Q(B)
 P(B)

+D KL

Q(π)
 P(π)

+E Q(πk)
"
T
∑
t=1
EQ(St|πk)
h
logQ(S t|πk)
i
−
τ
∑
t=1
EQ(St|πk)Q(A)
h
logP(o t|St,A)
i
−E Q(S1|πk)
h
logP(S 1)
i
−
T
∑
t=2
EQ(St|πk)Q(St−1 |πk)
h
logP(S t|St−1,π k)
i#
,
(5)
where we have singled out the KL divergences between the posterior probability distributions from
the variational approximation and the prior probability distributions from the generative model
4
Prior preferences in active inference agents: soft, hard, and goal shaping
(first three terms), and grouped together all the terms involving one of the variational posteriors
Q(S1:T |πk),k∈[1,p]wherepis an integer indicating the maximum number of policies, inside the
expectationE Q(πk)[. . .](last term), which computes an average with respect to all policies.
When the expression in Eq. (5) is optimised with respect to the policy-conditioned variational
distributions,Q(S t|πk),∀k∈[1,p], we can simply focus on the argument ofE Q(πk)[. . .]to compute
the associated gradient (since that is the only term that contributes to the gradient and ignoring the
expectation does not change the solution of∇ st F[Q(S t|πk)] =0). That argument defines a policy-
conditioned free energy:
Fπk

Q(S1:T |πk)
 :=
T
∑
t=1
EQ(St|πk)
h
logQ(S t|πk)| {z }
state log-probabilities
i
−
τ
∑
t=1
EQ(St|πk)Q(A)
h
logP(o t|St,A)| {z }
observation log-likelihoods
i
−
−E Q(S1|πk)
h
logP(S 1)| {z }
state log-probabilities
i
−
T
∑
t=2
EQ(St|πk)Q(St−1 |πk)
h
logP(S t|St−1,π k)| {z }
transition log-likelihoods
i
.
(6)
The update rules forQ(S t|πk),∀k∈[1,p], derived by taking the corresponding gradient of the ex-
pression in Eq. (6), define an optimisation/inference scheme calledvariational message passingwhich
makes use of past, present and future information to update, in this case, variational probability dis-
tributions at different time points along a trajectory (see appendices in [9, 48] for details on the update
equations). Following standard treatments in the literature of stochastic processes and (Bayesian)
estimation, it is an example of smoothing, to be contrasted with inference (which uses present infor-
mation only) and filtering (which relies on past and present information), and prediction (which uses
the past only) [25, 44].
2.3. Expected free energy
Similarly, the derivation of the updated probability distribution over policies,Q(π), involves taking
the gradient of the free energy in Eq. (5) but with respect to the parameters ofQ(π)this time. While
we again refer to [9, 48] for the full details of the derivation, we briefly overview below the update
equation so to introduce the notion of expected free energy next.
By indicating withπ ⊺
Q andπ ⊺
P the row vectors of parameters of the variational distributionQ(π)
and the prior distributionP(π), respectively, and withF ⊺
π the row vector of policy-conditioned free
energies (one for each policy, i.e., for each value the policy random variable can take, see Eq. (6)), the
updated parameters forQ(π)are computed as follows:
π⊺
Q =σ(lnπ ⊺
P −F ⊺
π), (7)
where the softmax function,σ(·), is used to obtain a vectorπ ⊺
Q of normalised probabilities from
theunnormalisedprobabilities of the vector lnπ ⊺
Q, after setting the gradient∇ πF[Q(π)]to zero and
rearranging (see again appendices in [9, 48]).
One of the key moves of active inference is to specify the prior parametersπ⊺
P in terms of the vec-
torG H⊺ of total expected free energiesG H (defined below), one for each policy under consideration,
5
Prior preferences in active inference agents: soft, hard, and goal shaping
i.e.,π ⊺
P :=σ(−G H⊺), in order to arrive at the folowing update rule 2:
π⊺
Q =σ(−G H⊺ −F ⊺
π), (8)
where each component ofG H is the total expected free energyG H(π)for a certain policyπ. Specifi-
cally, for a given policyπk,G H(πk)is defined as the sum of expected free energies at future time steps
up to the policy horizon,H, i.e.:
GH(πk) =
H
∑
t=τ+1
Gt(πk), (9)
where the expected free energy at timetfor the same policy is specified as follows:
Gt(πk) :=EQ(St|πk)
h
H

P(Ot|St)
i
| {z }
AMBIGUITY
−E P(Ot|St)Q(St|πk)
h
DKL

Q(A|ot,s t)|Q(A)
i
| {z }
A-NOVELTY
+D KL

Q(St|πk)|P∗(St)

| {z }
RISK
−E P(Ot|St)Q(St|πk)
h
DKL

Q(B|ot,s t)|Q(B)
i
| {z }
B-NOVELTY
.
(10)
In the above expression, the risk term quantifies the divergence between the variational state
distribution and a target distribution (here defined over state random variables, but see below), the
ambiguity terms quantifies the uncertainty related to the observation map, and the two novelty terms
are expected information gains for the parameters of the observation and the transition maps, thus
indicating parts of the generative model that are still inaccurate.
Therefore, according to Eq. (8), policy probabilities are updated at teach time step based on a
combination of negative expected free energies and free energies associated with each policy. Con-
cretely, this means that a policy will become more probable to the extent that it minimises free energy
and expected free energy. A policy that minimises free energy represents a sequence of action that
is most likely associated with the observations collected so far, i.e., there is a good chance that the
policy’s actions produced a sequence of states emitting those observations. A policy that minimises
expected free energy tries to makes sure that the agent reaches its goals (i.e., by having a low risk,
equivalently: a highinstrumentalorextrinsicvalue) and that the agent explores sufficiently enough
the environment to acquire relevant information about emission and transition maps (i.e., by being
associated with low ambiguity and high novelty terms, equivalently: a highepistemicorintrinsic
value).
An informative and correct probability distribution over policiesQ(π)is then a crucial compo-
nent of the decision-making step of an active inference agent, involving the selection of what action to
perform next. There are different ways to specify an action-selection procedure or decision rule, e.g.,
an agent could pick the action the most probable policy suggests at timet. Another option would be
to pick the action associated with the highest sum of probability mass coming from each policy that
suggests that action att. The latter, which is used in the following experiments, is known asBayesian
model average[9] and can be formally stated as follows:
at =arg max
a∈A
 
∑
πk∈Π
δa,c
πk
t
Q(πk)
!
, (11)
2Note that lnσ(−G H), obtained by substituting the definition forπ ⊺
P in Eq. (7), simplifies to−G H because the natural
logarithm is the inverse of raising to the power ofeand because we can ignore the normalizing constant, which would be
anyway absorbed into the second softmax used to obtainπ ⊺
Q.
6
Prior preferences in active inference agents: soft, hard, and goal shaping
where the Kronecker deltaδ a,c
πk
t
compares a potential actiona∈ Aand the actionc πk
t that a policy
πk dictates at time stept, giving 1 if they are equal and 0 otherwise. This decision rule thus picks the
action with the highest marginal probability,P(a) = ∑πk∈Π P(a,π k), at time stept.
A crucial piece of information included in the expression for the expected free energy is the prior
probability distributionP ∗(·), which can defined over observation random variablesO t, in the par-
tially observable case, or over state random variablesS t, in the fully observable case (e.g., when
Ot =S t). Henceforth, we will focus on the latter, because we are interested in agents that need to
learn an environment’s transition model as opposed to the observation map (i.e., theBmatrices in-
stead of theAmatrix), and refer toP ∗(·)as thepreference distribution 3. In the next section, we will
delve into some details about the form of this distribution and on what it accomplishes.
2.4. Preference distributions in active inference agents
In the discrete state-space setting, the agent’s preferences are represented by a categorical distribu-
tion, indicated byP ∗(S)in the fully observable case, that effectively encodes an agent’s goals in terms
of particular instantiations of the random variableS. That is, goal-directedness in active inference is
cast as the concentration of probability mass on some states more than others, for the possiblem
realisations,s 1, . . . ,sm, ofS, as indicated by the parameters’ vector ofP ∗(S).
The specified preference distribution constrains policy and action selection because the computa-
tion of expected free energy depends on it. Specifically, the risk component of expected free energy
at timetis the KL divergence between the variational distributionQ(S t|πk)and the preference dis-
tributionP ∗(St)(see Eq. (10)). This KL divergence encapsulates the contrast between how things are
and how they should be for the agent: the variational distributions at different time steps indicate
to the agent what the most probable states afforded by a policy are (they encode the agent’s current
beliefs about the consequence of a policy, determined by the learned generative model) whereas the
fixed preference distribution indicates what the most probable stateshouldbe (regardless of any pol-
icy). Selecting a policy that minimises risk means to increase the chance that executing its actions
will result in variational beliefs that align with the preference distribution (what the agent wants). To
the extent that the variational beliefs reflect the environment’s dynamics, i.e., where the agents most
probably will be located (and not just where it believes it will), the minimisation of risk will lead
the agent to achieve its goal(s). In other words, risk quantifies the instrumental value of the policy
being evaluated, i.e., whether the agents believes it will lead to the preferred states. If risk is low for
a certain policy, then this policy will have a chance of being selected and bring the agent to one of its
preferred states.
The preference distribution specifies a certain goal (e.g., a particular state), potentially connected
with the solution of a task, and establishes how strongly an agent wants to achieve it. This distri-
bution needs to be specified in advance since in general active inference agents are not able to learn
what they should do (but see [42, 46]). This can be done in different ways, depending on the appli-
cation and on what the agent is supposed to do. There are at least two degrees of freedom in the
specification of the agent’s preferences: (1) how strongly an agent prefers to achieve a certain goal,
and (2) whether the preference distribution specifies different goals at different time steps (potentially
indicating to the agent a trajectory through state-space).
3A similar argument can be made for partially observable scenarios, i.e., when agents have to learn the observation
modelA, with the risk in Eq. (10) written in terms of variational and preference distributions over observation random
variables. In particular, the variational distribution over observations is obtained from the agent’s current observation
model by marginalizing over states random variable, i.e.,Q(O t|πk) =∑st P(Ot|St)Q(St|πk)(see appendix in [9]).
7
Prior preferences in active inference agents: soft, hard, and goal shaping
Table 1:Types of preferences considered in this work.
Preference strength
Time-dependence
Soft preferences with
goal shaping
Hard preferences with
goal shaping
Soft preferences
without goal shaping
Hard preferences
without goal shaping
The first dimension relates to how much the probability mass ofP ∗(St)is concentrated on some
states as opposed to others. In particular, the agent could havehard goalsor preferences, meaning
that most of the probability mass is concentrated on a single state,S t =s j, while for all states the
probabilities are close to 0, thus makingP ∗(St)an approximate delta distribution. Alternatively, the
agent could havesoft goalsor preferences, in the sense that the probability mass is more evenly
distributed among at least two or more states.
The second dimension captures the possible time-dependence of preferences: whether the agent
has a different preference distribution for each time step, providing the agent with a distinct goal
for each time step in a given time interval. To understand this, recall that the expected free energy
defined by Eq. (10) is computed for each future action that a policy suggests therefore eachP∗(St)for
t∈[1,T]could assign probability mass to states differently. Again, there are at least two possibilities.
With preference orgoal shaping, for every future time steptwe specify a different preference dis-
tribution capturing what goal(s) the agent ought to strive towards at each of those time steps. This
means that we have effectively a collection of preference distributions defining a desired trajectory in
state-space for the agent to follow, which can be viewed as a sequence of sub-goals or intermediate
goals the agent needs to obtain before reaching their main goal (e.g., at the last time step).
In contrast,withoutpreference or goal shaping, there is only a single, fixed preference distribution
that is used to compute the risk term at each time step. This distribution could specify hard or soft
goals and represents the “final” distribution over states to which the agent tends. In other words, the
agent is given one or more goals to obtain as soon as possible, but no indication of what intermediate
goals to pursue to get there.
In the active inference literature, the specifications of hard goals and soft goals have often been
used in grid world simulations (see, e.g., [17, 18, 22, 43] for hard goals, and [19] for soft goals).
The presence or absence of goal shaping implicitly appears in active inference works that involve
common reinforcement learning environments, where the agent’s preferences are encoded as states
or observations that yield high rewards, and depends on whether the environment’s reward function
was designed to provide a dense vs. sparse reward signal [49, 52, 51, 5, 32].
In the experiment described in the next section, we consider active inference agents in a grid-
world navigation task, each with one of the four possible preference distributions (determined by
the dimensions described above), and examine how the specification of preference distribution af-
fects their ability to reach their final goal, i.e., being in a particular maze’s tile, that corresponds with
8
Prior preferences in active inference agents: soft, hard, and goal shaping
the final solution of the task. In particular, agents with soft goals and goal shaping have different pref-
erence distributionsP ∗(St), . . . ,P∗(ST), with the probability mass distributed among states, based on
the Manhattan distance of each state from a state representing an intermediate goal (tile position), at
t, or from the state corresponding to the final goal, atT. Similarly, agents with hard goals and goal
shaping have different preference distributions, but this time each one approximates a Dirac delta.
Without goal shaping, agents with soft goals have a single preference distribution,P ∗(S), used to
compute the risk in the expected free energy at each time step in the episode, again with a more even
distribution of probability mass (as explained above). Similarly, agents with hard goals have a single
preference distribution that assigns most of the probability mass to the final goal.
3. Results
We trained action-unaware active inference agents, i.e., agents that have no access to previously
executed actions (see [48] for details), with different preferences in a 3×3 grid world. All agents
start in the top left corner and their goal is to reach the bottom right corner, with no obstacles. We
specified preference distributions in four different ways, based on Table1.
The problem is simplified to be a fully observable MDP (withAdiagonal and known to the
agent), with deterministic but unknown state transitionsB. We trained 10 agents of each kind for 200
episodes of 5 steps each, i.e.,T=5, with a policy horizonH=4, giving us at most 256 policies to
evaluate, to allow for convergence. Of the 256 policies, 6 are task-solving, since they allow an agent
to reach the bottom right corner of the grid via a combination of two↓and two→actions, in any
order. The remaining 250 are task-failing since they don’t lead to the final goal.
We start by comparing the percentage of agent solving the task across episodes in Fig.1. Goal-
shaped agents learn more quickly and perform better overall than agents without goal shaping,
which is to be expected since the former know exactly what intermediate goal to aim for at each
time step, guiding them to reach their final goal (bottom right corner).
With goal-shaping, having hard goals is also a further advantage as all agents are able to find the
path to the final goal in a handful of episodes and they stick to it for the rest of the experiment, i.e.,
a 100% success rate is consistently achieved from episode 8 onwards (see top-right plot in Fig.1).
In contrast, agents with soft goals take longer to find their way to the final goal and the percentage
of successful agents falls below 100% in several episodes throughout the experiment, even after the
main learning phase is over, i.e., from episode 17 onwards (compare top-left and top-right plots
in Fig.1).
Without goal-shaping, both kinds of agent take longer to learn and succeed: after an initial phase
of performance improvements, there is a dramatic drop in the percentage of successful agents at
around episode 50, followed by a quick recovery and with at least 60% of the agents being able to
reach the final goal in most episodes until the end of the experiment (see bottom-left and bottom-
right plots in Fig.1). While the performance drop at around episode 50 is less pronounced for agents
with hard goals, overall they appear to reach the final goal fewer times than agents with soft goals.
The percentages of time the final goal was reached during the experiment, 15% in soft-goal agents
and 14.2% in hard-goal agents, confirm the above observation: without goal-shaping, agents with
soft goal are marginally more successful than agents with hard goals (compare heat maps in Fig.S1).
These results indicate that agents with different kinds of preferences can successfully learn rele-
vant aspects of the transition dynamics and solve the task. Agents that are given more information,
by means of intermediate goals (goal shaping) leading to the final goal have an advantage and can
9
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0%
20%
40%
60%
80%
100%Percentage of agents
Agents solving the task
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0%
20%
40%
60%
80%
100%Percentage of agents
Agents solving the task
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0%
20%
40%
60%
80%
100%Percentage of agents
Agents solving the task
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0%
20%
40%
60%
80%
100%Percentage of agents
Agents solving the task
(hard without goal shaping)
Fig. 1:Percentage of agents reaching the goal state in each episode in the 5-step grid world (10 agents for each
subplot).
complete the task a higher number of times. At the same time, soft goals enable better performance
when goal shaping is not available but hard goals provides a further advantage when combined with
goal shaping.
For a more in-depth understanding of how these differences in performance are brought about by
the choice of preference distribution, we analyse perceptual inference and planning/decision-making
in the simulated agents by considering related metrics, chiefly, policy-conditioned free energies and
expected free energies, respectively.
Perceptual inference over trajectoriesThe successful performance of each type of agent hinges
upon an appropriate inference of its current state from the received observations, corresponding to
the perceptual stage in active inference. This is achieved via free energy minimisation. In episodic
setups, this minimisation happens at different time steps. Here we look at how free energy is min-
imised, across episodes, at the last step of each episode, as done in [48]. At the last step, an agent
has to evaluate the entire sequence of observations associated to a full episode so to infer the most
probable sequence of states that have been visited, conditional on having executed a certain policy.
This thus reveals how agents infer their past and current states. Policies that correspond to state
10
Prior preferences in active inference agents: soft, hard, and goal shaping
trajectories that agree with the received observations will be associated with low values of (policy-
conditioned) free energy.
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16Free energy
Policy-conditioned free energy at step 5
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16Free energy
Policy-conditioned free energy at step 5
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16Free energy
Policy-conditioned free energy at step 5
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16Free energy
Policy-conditioned free energy at step 5
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. 2:Policy-conditioned free energies at step 5 across episodes (showing average of 10 agents).
Plots of the policy-conditioned free energies at step 5, in Fig.2, reveal which policy best accounts
for the observations collected in each episode. For all the figures involving policy-conditioned free
energies, we selected 16 policies, among the 256, including the 6 task-solving policies that lead to
the final goal. These plots show that the presence vs. the absence of goal shaping affects what policy
yields the lowest policy-conditioned free energy, i.e., what policy the agent most likely executed
because it is inferred to be more consistent with the observations, thereby minimizing the associated
policy-conditioned free energy.
In general, we can see that goal-shaped agents learned to pickπ 4, one of the task-solving ones,
as the preferred policy and stick with it throughout the experiment (see top-left and top-right plots
in Fig.2). This yields a consistent minimisation of the corresponding policy-conditioned free energy
11
Prior preferences in active inference agents: soft, hard, and goal shaping
across episodes: agents at the end of each episode correctly infer thatπ4 was the most likely executed
policy. In contrast, for agents without goal shaping there is no policy that consistently minimises
the policy-conditioned free energy, see bottom-left and bottom-right plots in Fig.2. This is due to
the fact that these agents are not constrained by their preference distribution to follow a particular
trajectory in state-space but only to reach the final goal as soon as possible. Therefore, they attempt
different task-solving or task-failing policies, and no clear demarcation between these is visible in the
plots. This is because in different episodes the observation sequence is correctly inferred to be more
consistent with different, task-solving or task-failing, policies.
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0Free energy
Policy-conditioned free energy at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0Free energy
Policy-conditioned free energy at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0Free energy
Policy-conditioned free energy at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0Free energy
Policy-conditioned free energy at step 1
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. 3:Policy-conditioned free energies at step 1 across episodes (showing average of 10 agents).
Policy probabilitiesTo examine how the choice of preference distribution affects planning and
decision-making in each episode, we compare the respective contributions of the policy-conditioned
free energies and expected free energies to the policy probabilities at the beginning of each episode,
12
Prior preferences in active inference agents: soft, hard, and goal shaping
i.e., at step 1. This step was chosen because it is the most indicative of how well the agent has learned
the transition model, since at this step the agent performs perceptual inference with respect to future
state, before any observation is received (with the exception of that from the starting state), and plans
for the entire episode, see also [48]. In general, we found that policy-conditioned free energies and
expected free energies do not always agree, i.e., by jointly increasing the probability of a policy if it is
one that leads to the goal state, revealing a mismatch between perception and planning that depends
on the choice of preference distribution.
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Expected free energy
Expected free energy at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Expected free energy
Expected free energy at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Expected free energy
Expected free energy at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Expected free energy
Expected free energy at step 1
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. 4:Expected free energy for each policy across episodes (showing average of 10 agents). Notice that we
only draw 16 expected free energies, representative of the possible 256.
Policy probabilities are computed as a softmax of the sum of negative policy-conditioned free
energy and negative expected free energy. Therefore, the policy probabilities at step 1 in Fig.5are
directly shaped by the contributions of policy-conditioned free energy and expected free energy at
step 1, illustrated for each agent by Fig.3and Fig.4.
13
Prior preferences in active inference agents: soft, hard, and goal shaping
In particular, we observe that in agents with soft goals and goal shaping, the policy-conditioned
free energy assigned to the task-solving policyπ4 is low, see the top-left plot in Fig.3, and this makes
it more probable. On the other hand, the associated expected free energy is high, see the top-left plot
in Fig.4, and this makes it less probable. Overall, this means that the resulting policy probability is
not significantly different from that of other policies, see top-left plot in Fig.5.
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.01
0.02
0.03
0.04
0.05
0.06Probability mass
First-step policy probability
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
First-step policy probability
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.01
0.02
0.03
0.04
0.05
0.06Probability mass
First-step policy probability
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.01
0.02
0.03
0.04
0.05
0.06Probability mass
First-step policy probability
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. 5:Policies probabilities at the first step of each episode (showing average of 10 agents). Notice we only
draw 16 representative policies out of the possible 256.
In agents without goal shaping the opposite happens for some task-failing policies, which are
characterised by high policy-conditioned free energy values, see the bottom plots in Fig.3. This
brings their probability down. At the same time, their expected free energy is remarkably low, see
the bottom plots in Fig.4, pushing the policy probability up. This means that these task-failing poli-
cies become more probable than some task-solving policies, see bottom plots in Fig.5(see also Sec-
tionS2.2.2for a breakdown of these expected free energies showing how their risk component is
14
Prior preferences in active inference agents: soft, hard, and goal shaping
characterised by a significant decrease over the course of the experiment). Crucially, the policy-
conditioned free energies and expected free energies agree for all policies only in agents with goal
shaping and hard goals. In turn this means that one of the task-solving policies,π 4, is correctly iden-
tified as the most likely, i.e., that is the policy for which both quantities are the lowest (see top-right
plot in Fig.5, and compare top-right plots in Fig.3and Fig.4).
Action selection and action probabilitiesDespite not inferring any of the task-solving policies
as more likely than task-failing ones, some classes of agents appear to anyway reach the final goal
state, albeit with varying degrees of success. We refer in particular to agents without goal-shaping
and agents with soft goals and goal-shaping, see Fig.5. This seemingly unintuitive finding can
be explained by the fact that agents do not pick the action suggested by the most probable policy
at a certain time step but the one with the highest marginal probability, see Eq. (11). This action-
selection mechanism, referred to as Bayesian model average in [9], prevents agents from performing
the potentially wrong action of a task-failing policy that was mistakenly inferred as more probable
than others (perhaps only marginally more probable).
Thanks to this, and as we see in Fig.6, all agents assigned a sufficiently high probability mass
to the six task-solving policies so that actions↓or→became more likely to be picked. Note, as a
reminder, that all agents start from the top left corner of a 3×3 grid and need to read the goal at
the bottom right corner. This means that all the task-solving policies are a combination of two↓and
two→actions, in any order. In this way, most of the time, even agents that didn’t infer any of the
task-solving policies as more probable (agents without goal-shaping and agents with soft goals and
goal-shaping) ended up performing one of the correct sequences of actions to reach the goal state.
This finding highlights the importance that the action-selection mechanism can have depending on
the preference distribution given to the agent, e.g., one that may lead to a task-failing policy being
inferred as more probable.
More in detail, Fig.6shows action probabilities at step 1, obtained by marginalising probabilities
for each action, see Eq. (11). Consistently with the fact that goal-shaped agents perform best, in
general these agents assign the majority of probability mass at step 1 to action→, i.e., the first correct
action of their preferred path to the final goal state (see top-left and top-right plots in Fig.6). In agents
without goal shaping something similar occurs. However, since these agents do not have a preferred
path to the goal state, the probability mass at step 1 is almost evenly assigned to the two correct
actions→and↓of the available paths to the final goal state (see bottom-left and bottom-right plots
in Fig.6). In both kinds of agents, action probabilities at other time steps also reflect this preference
for actions that form one of the correct action sequences to reach the final goal (see plots for the other
steps in SectionS2.2.3).
Since Bayesian model average favours the action backed by most probable policies, these results
are not surprising when one of the task-solving policies is significantly more probable than others,
i.e., in agents with a hard goal and goal shaping. But, in agents where this concentration of prob-
ability mass over policies does not occur, these findings indicates that the agents are able to spread
a sufficiently high degree of probability mass among the task-solving policies. Most of the time the
action selection mechanism can then build the right sequence of action to the goal state. When this
does not occur, it is because agents have overall assigned more probability mass to policies suggest-
ing incorrect actions (for the reasons explained in Section4.2), which are then executed and cause the
performance drops seen in Fig.1. These action-selection mistakes become apparent when examining
the individiual action probabilities for specific agents, as shown by the plots in SectionS2.2.4. For
the two agents without goal shaping, we observe a cyclical increase in the probability of the incorrect
15
Prior preferences in active inference agents: soft, hard, and goal shaping
actions (most clearly at step 1 and 2, see Fig.S7and Fig.S8), eventually leading to some episodes
where these incorrect actions are selected over the correct ones, albeit only by a small margin.
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(hard without goal shaping)
Actions
Fig. 6:Action probabilities at step 1 of each episode (average of 10 agent).
4. Discussion
4.1. Learning and performance elicited by different preference distributions
Simulations in a 3×3 grid world with the four preferences’ combinations showed that agents with
goal shaping and hard goals perform best, at the expense of reducing uncertainty about the ground-
truth transition dynamics, which is to be expected as they are entirely guided by a collection of
preference distributions that specify a distinct trajectory through various intermediate goals (maze
16
Prior preferences in active inference agents: soft, hard, and goal shaping
positions) to the final goal (bottom-right tile).
In more complex environments, e.g., when the specification of a desired trajectory is unfeasible,
agents will have to figure out by themselves the best way of solving a certain task or reaching a
certain goal. Not surprisingly, simulations of this kind of scenario using the same 3×3 grid world
showed that agents without goal shaping make more mistakes but eventually discover all the six
possible trajectories to the goal. This suggests that they might be more robust to perturbations, e.g.,
to one or more paths to the goal becoming obstructed, because they have spent more time learn-
ing about the environment’s transition dynamics (see Fig.S11, and compare ground-truth transition
maps in SectionS2.2.6with learned ones in SectionS2.2.7).
Without goal shaping, agents with soft goals turned out to be marginally more capable of reaching
the final goal state, which can be explained by the fact that properly specified soft preferences (e.g.,
using the Manhattan distance from the final goal state) allow the agent to explore the environment
while still having a (soft) preference for reaching a subset of states that are closer to the final goal
state.
4.2. The Risk of unfamiliar policies
In describing how policy-conditioned free energies and expected free energies combine to deter-
mine policy probabilities at step 1, we noted that agents without goal shaping score task-failing poli-
cies higher than task-solving ones at some point, especially thanks to low risk values (see also Sec-
tionS2.2.2). On the face of it, this seems puzzling because those policies are task-failing, i.e., they do
not lead the agent to its goal state, and yet they are scored favourably in terms of risk.
However, since these policies haven’t been sufficiently explored during the experiment, active
inference agents have unresolved uncertainty as to where they lead, indicated by high-entropy vari-
ational distributions,Q(S 2|πk), . . . ,Q(ST|πk), for everyk∈[1,p], updated at the perceptual inference
stage (in step 1 of each episode). These variational beliefs produce large values of policy-conditioned
free energies (see Fig.3) and at the same time determine a drop of the policy’s risks along the corre-
sponding trajectory (see Fig.S2).
To understand why this drop occurs, recall that risk is the KL divergence between two probabil-
ity distributions, and that a low-entropy (peaked) distribution is “closer” to a high-entropy (flatter)
distribution than toanotherlow-entropy distribution, i.e., a distribution with a peak somewhere else.
Thus, risk will be relatively low when computed between a low-entropy preference distribution,
where probability mass is concentrated all on one goal or a few ones, and a high-entropy variational
distribution,Q(S t|πk), for a task-failing policyπ k that was rarely attempted (i.e., for which the agent
is not certain about the resulting state trajectory). In contrast, risk will be relatively high between
two different low-entropy distributions with different peaks, e.g., when a preference distribution
with concentrated probability mass is compared with the variational distribution of a policyπ k that
was executed more frequently (i.e., for which the agent is certain about the resulting state trajectory).
This situation does not occur in agents with hard goals and goal shaping because in this case
we have a collection of preference distributions that traces a path through state-space and a unique
policy that realises that trajectory, making the corresponding variational beliefs match perfectly the
corresponding preference distributions. Thus, the risk (KL divergence) between matching distribu-
tion will always be lower than that between a high-entropy and low-entropy distribution. Because
of this, the risk of the task-solving policy correctly turns out to be the lowest (see top-right plot
in Fig.S2).
Overall, when we consider agents without goal shaping (and, note, in an environment with de-
17
Prior preferences in active inference agents: soft, hard, and goal shaping
terministic state transitions), increasing the entropy of the preference distribution, i.e., using soft
goals, can be seen as an effective method to limit this (incorrect) assignment of instrumental value to
task-failing policies (see again Fig.5).
The phenomenon just described is somewhat mitigated by the fact that high-entropy variational
beliefs for the trajectory of a policy make the corresponding policy-conditioned free energy soar,
thereby penalising the policy when that quantity is combined with expected free energy to yield
the policy probability via the softmax (see Section3). However, this does not prevent some task-
failing policies from becoming more probable than task-solving ones (see again Fig.5). As remarked
in Section3, these agents do not fail completely only because the action selection procedure does not
involve picking the action from the most probable policy.
At the same time, we note that even if that were the case, the agents would learn the consequences
of those task-failing policies, resolving their uncertainty about the transition dynamics of rarely tried
actions’ sequences (and for some agents this is what occurs, as indicated by the probabilities for
these task-failing policies declining from episode 160 onwards, see especially the bottom-right plot
in Fig.5). In other words, in addition to the two novelty terms of the expected free energy, there is an
implicit epistemic drive arising out of the risk term as well, that compels agents to execute less tried
actions’ sequences, when agents are trained without goal shaping.
4.3. Related work
One of the main distinctive features of active inference is the idea of integrating state estimation,
planning, decision-making, and learning under the same objective, i.e., the minimisation of some
kind of free energy, without the need to introducead-hocexploratory bonuses (as done in some re-
inforcement learning approaches, see [30]) to deal with the exploration-exploitation dilemma (but
see [33]) and without relying on notions of reward and value functions (which are foundational in
reinforcement learning, see [47]).
However, it does not follow that goal-directedness emerges from free-energy minimisationtout
court[45, 1]. The design of an active inference agent requires the specification of a preference dis-
tribution that creates a relevant learning signal for the agent, via expected free energy, so that some
kind of adaptive behaviour is elicited in a certain environment. The specification of this preference
distribution, that indicates what states or observations are preferred by an active inference agent,
effectively corresponds to the design of a reward function in reinforcement learning. Because of this,
similar theoretical considerations can be made, e.g., related to the origin or nature of this preference
distribution,cf. [26]. Indeed, the distinction between soft vs. hard goals corresponds to that between
dense vs. sparse reward signals, and goal shaping can be understood similarly as reward shaping [39,
11].
In this respect, active inference and reinforcement learning appear to have more in common than
what is often claimed [20, 16, 41]. After all, the specification of a preference distribution is a way
to let the agent knows what states or observations should be deemed to be rewarding, hence to be
preferred. This is not just a terminological issue, as in scaled-up applications of active inference it
is common to use the reward from environments used in reinforcement learning as part of the ob-
servations the agent receives, thereby marking explicitly some observations as those to be preferred
because associated with higher reward [50, 51, 32, 53]. Furthermore, recent theoretical work has for-
mally showed that minimising expected free energy implies reward maximisation in finite-horizon
MDPs and POMDPs, withsophisticated inference, a recursive kind of planning based on expected free
energy [10]. This work clarifies how goals can be interpreted as reward maximising states, by for-
18
Prior preferences in active inference agents: soft, hard, and goal shaping
mally defining the preference distribution as a Boltzmann distribution with the reward assigned to a
state as the random variable (see [10, p. 818] for the details).
Ultimately, agents need a learning signal that guides them towards achieving what they are de-
signed to achieve, and whether we call it reward or preferences (for an agent) might matter less than
howthat learning signal is exploited in clever ways. In active inference, that learning signal arises out
of variational free energy and expected free energy. In particular, the latter would combine instru-
mental or extrinsic drives together with epistemic or intrinsic drives. More generally, active inference
invites us to take an agent-centric perspective revolving around the notion of free energy minimisa-
tion: the type of agent we are considering, its available actions and how it can interact with a certain
environment, can inform us of the goal-directed behaviour to be expected, or provide some guidance
on how that behaviour can be designed in a transparent way via the specification of the preference
distribution.
4.4. Limitations and future work
The in-depth analysis of the impact of prior preferences in active inference agents was limited, in
this work, to the case of preferences defined over states, in agents trained in a low-dimensional
grid world. The states in question can be considered as internal to the agent, i.e., indicating its cur-
rent location in the environment in our experiment. However, as remarked, preference distributions
can also be defined over observations received from the environment and reward functions from
reinforcement learning environments can be incorporated into them, i.e., reward is treated as an ob-
servable part of the environment (see [31] for a nice summary of the possibilities). A study of the
trade-offs and peculiarities of these alternative approaches, often used in implementations that rely
on deep learning to train active inference agents in high-dimensional environments, must be left for
future work. In a similar vein, the question of how agent’s preferences could be progressively learned
in different environments, e.g., by introducing a Dirichlet prior for the parameters of the categorical
preference distribution [42], was not taken into consideration here and is an interesting avenue for
future research.
5. Concluding Remarks
In the present work, we investigated how different ways of fixing the preference distribution in
action-unaware active inference agents affects their performance in a toy environment. In partic-
ular, we focussed on the distinction between training agents with or without goal shaping and with
soft of hard preference. Goal shaping refers to the idea of using a collection of preference distribu-
tions to direct the agent towards pursuing a particular trajectory in state-space (e.g., a trajectory of
intermediate goals to reach a main goal); without it, the agent needs to find by trial and error which
trajectories converge onto its main goals, encoded by a single preference distribution. The second
dimension, whether to encode soft or hard goals into the preference distribution, refers to how much
the probability mass is concentrated on a subset of states/observations.
In general, agents with goal-shaping perform better because they are provided with preference
distributions that together indicate a sequence of preferred states, facilitating the selection of the
single policy that will lead the agent through the corresponding path in state-space. In contrast,
agents without goal-shaping have to find that preferred path, relying only on a single preference
distribution that indicates the ultimate state or goal they are supposed to achieve (in our case: being
located in tile 9). As a result, their performance suffers but at the same time they are more free to
19
Prior preferences in active inference agents: soft, hard, and goal shaping
explore the environment, therefore they are able to find all the possible paths to the ultimate goal.
Thus, we can conclude that the absence of goal-shaping determines a more widespread learning of
the environment’s transition dynamics, i.e., of the transition probabilities for each available action
(see again Fig.S11and learned transition maps in SectionS2.2.7).
Acknowledgments
This work was supported by JST, Moonshot R&D, Grant Number JPMJMS2012.
References
[1] Manuel Baltieri and Christopher L. Buckley. “The Dark Room Problem in Predictive Process-
ing and Active Inference, a Legacy of Cognitivism?” In:Proceedings of the 2019 Conference on
Artificial Life: How Can Artificial Life Help Solve Societal Challenges, ALIFE 2019. 2020, pp. 40–47.
DOI:10.1162/isal_a_00137.xml.
[2] Manuel Baltieri, Filippo Torresan, and Tomoya Nakai.A Coalgebraic Perspective on Predictive
Processing. 2025.DOI:10.48550/arXiv.2508.16877. arXiv:2508.16877 [q-bio.NC].
[3] Jelle Bruineberg. “Active Inference and the Primacy of the ‘I Can’”. In:Philosophy and Predictive
Processing. Ed. by Thomas Metzinger and Wanja Wiese. Frankfurt am Main, Germany: MIND
Group, 2017, pp. 1–18.ISBN: 978-3-95857-306-2.
[4] Christopher L. Buckley et al. “The Free Energy Principle for Action and Perception: A Math-
ematical Review”. In:Journal of Mathematical Psychology81 (2017), pp. 55–79.ISSN: 10960880.
DOI:10.1016/j.jmp.2017.09.004.
[5] Ozan C ¸ atal et al. “Bayesian Policy Selection Using Active Inference”. In: (2019), pp. 1–9.
[6] Ozan C ¸ atal et al. “Learning Generative State Space Models for Active Inference”. In:Frontiers
in Computational Neuroscience14 (2020), p. 103.ISSN: 1662-5188.DOI:10.3389/fncom.2020.
574372.
[7] Andy Clark. “Whatever next? Predictive Brains, Situated Agents, and the Future of Cognitive
Science”. In:Behavioral and Brain Sciences36.3 (2013), pp. 181–204.ISSN: 14691825.DOI:10 .
1017/S0140525X12000477.
[8] Andy Clark. “Radical Predictive Processing”. In:The Southern Journal of Philosophy53.S1 (2015),
pp. 3–27.ISSN: 00384283.DOI:10.1111/sjp.12120.
[9] Lancelot Da Costa et al. “Active Inference on Discrete State-Spaces: A Synthesis”. In:Journal of
Mathematical Psychology99 (Dec. 2020), p. 102447.ISSN: 0022-2496.DOI:10.1016/j.jmp.2020.
102447.
[10] Lancelot Da Costa et al. “Reward Maximization Through Discrete Active Inference”. In:Neural
Computation35.5 (Apr. 2023), pp. 807–852.ISSN: 0899-7667.DOI:10.1162/neco_a_01574.
[11] Jonas Eschmann. “Reward Function Design in Reinforcement Learning”. In:Reinforcement Learn-
ing Algorithms: Analysis and Applications. Ed. by Boris Belousov et al. Cham: Springer Interna-
tional Publishing, 2021, pp. 25–33.ISBN: 978-3-030-41188-6.DOI:10.1007/978-3-030-41188-
6_3.
20
Prior preferences in active inference agents: soft, hard, and goal shaping
[12] Zafeirios Fountas et al. “Deep Active Inference Agents Using Monte-Carlo Methods”. In:Ad-
vances in Neural Information Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Asso-
ciates, Inc., 2020, pp. 11662–11675.
[13] Karl Friston. “A Theory of Cortical Responses”. In:Philosophical Transactions of the Royal Society
B: Biological Sciences360.1456 (2005), pp. 815–836.ISSN: 0962-8436.DOI:10.1098/rstb.2005.
1622.
[14] Karl Friston. “Hierarchical Models in the Brain”. In:PLoS Computational Biology4.11 (2008),
pp. 1–24.ISSN: 1553734X.DOI:10.1371/journal.pcbi.1000211.
[15] Karl Friston. “The Free-Energy Principle: A Rough Guide to the Brain?” In:Trends in Cognitive
Sciences13.7 (2009), pp. 293–301.ISSN: 13646613.DOI:10.1016/j.tics.2009.04.005.
[16] Karl Friston. “What Is Optimal about Motor Control?” In:Neuron72.3 (Nov. 2011), pp. 488–498.
ISSN: 0896-6273.DOI:10.1016/j.neuron.2011.10.018.
[17] Karl Friston et al. “Active Inference and Learning”. In:Neuroscience and Biobehavioral Reviews68
(2016), pp. 862–879.ISSN: 18737528.DOI:10.1016/j.neubiorev.2016.06.022.
[18] Karl Friston et al. “Active Inference: A Process Theory”. In:Neural Computation29.1 (2017),
pp. 1–49.DOI:10.1162/NECO_a_00912.
[19] Karl Friston et al. “Sophisticated Inference”. In:Neural Computation33.3 (Feb. 2021), pp. 713–
763.ISSN: 0899-7667.DOI:10.1162/neco_a_01351.
[20] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement Learning or Active Infer-
ence?” In:PLoS ONE4.7 (2009).ISSN: 19326203.DOI:10.1371/journal.pone.0006421.
[21] Karl J. Friston, Thomas Parr, and Bert de Vries. “The Graphical Brain: Belief Propagation and
Active Inference”. In:Network Neuroscience1.4 (2017), pp. 381–414.ISSN: 2472-1751.DOI:10.
1162/NETN_a_00018.
[22] Karl J. Friston et al. “Active Inference, Curiosity and Insight”. In:Neural Computation29.10
(2017), pp. 2633–2683.DOI:10.1162/NECO_a_00999.
[23] Sebastian Gottwald and Daniel A. Braun. “The Two Kinds of Free Energy and the Bayesian
Revolution”. In:PLOS Computational Biology16.12 (2020), pp. 1–32.DOI:10.1371/journal.
pcbi.1008420.
[24] Conor Heins et al. “Pymdp: A Python Library for Active Inference in Discrete State Spaces”. In:
Journal of Open Source Software7.73 (May 2022), p. 4098.ISSN: 2475-9066.DOI:10.21105/joss.
04098.
[25] Andrew H. Jazwinski.Stochastic Processes and Filtering Theory. New York, USA: Academic Press,
1970.ISBN: 978-0-12-381550-7.
[26] Keno Juechems and Christopher Summerfield. “Where Does Value Come From?” In:Trends in
Cognitive Sciences23.10 (2019), pp. 836–850.ISSN: 1879307X.DOI:10.1016/j.tics.2019.07.
012.
[27] Raphael Kaplan and Karl J. Friston. “Planning and Navigation as Active Inference”. In:Biolog-
ical Cybernetics112.4 (2018), pp. 323–343.ISSN: 14320770.DOI:10.1007/s00422-018-0753-2.
[28] Pablo Lanillos et al. “Active Inference in Robotics and Artificial Agents: Survey and Chal-
lenges”. In:arXiv:2112.01871v1 [cs.RO](2021), pp. 1–20. arXiv:2112.01871v1 [cs.RO].
21
Prior preferences in active inference agents: soft, hard, and goal shaping
[29] Tai Sing Lee and David Mumford. “Hierarchical Bayesian Inference in the Visual Cortex”. In:
Journal of the Optical Society of America A20.7 (2003), pp. 1434–1448.ISSN: 1084-7529.DOI:10.
1364/josaa.20.001434.
[30] Sergey Levine. “Reinforcement Learning and Control as Probabilistic Inference: Tutorial and
Review”. In:arXiv:1805.00909v3 [cs.LG](2018), pp. 1–22. arXiv:1805.00909v3 [cs.LG].
[31] Pietro Mazzaglia et al. “The Free Energy Principle for Perception and Action: A Deep Learning
Perspective”. In:Entropy24.2 (2022), pp. 1–22.ISSN: 1099-4300.DOI:10.3390/e24020301.
[32] Beren Millidge. “Deep Active Inference as Variational Policy Gradients”. In:Journal of Mathe-
matical Psychology96 (2020), p. 102348.ISSN: 10960880.DOI:10.1016/j.jmp.2020.102348.
[33] Beren Millidge, Alexander Tschantz, and Christopher L Buckley.Whence the Expected Free En-
ergy?2020.DOI:10.48550/arXiv.2004.08128. arXiv:2004.08128 [cs.AI].
[34] Kevin P . Murphy.Probabilistic Machine Learning: Advanced Topics. Cambridge, Massachusetts:
The MIT Press, 2023.
[35] Samuel William Nehrer et al. “Introducing ActiveInference.Jl: A Julia Library for Simulation
and Parameter Estimation with Active Inference Models”. In:Entropy27.1 (Jan. 2025), p. 62.
ISSN: 1099-4300.DOI:10.3390/e27010062.
[36] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.Active Inference: The Free Energy Principle in
Mind, Brain, and Behavior. The MIT Press, 2022.ISBN: 978-0-262-04535-3.
[37] Giovanni Pezzulo, Thomas Parr, and Karl Friston. “Active Inference as a Theory of Sentient
Behavior”. In:Biological Psychology186 (Feb. 2024), p. 108741.ISSN: 0301-0511.DOI:10.1016/j.
biopsycho.2023.108741.
[38] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. “Active Inference, Homeostatic Regu-
lation and Adaptive Behavioural Control”. In:Progress in Neurobiology134 (2015), pp. 17–35.
ISSN: 18735118.DOI:10.1016/j.pneurobio.2015.09.001.
[39] Jette Randløv and Preben Alstrøm. “Learning to Drive a Bicycle Using Reinforcement Learning
and Shaping”. In:Proceedings of the Fifteenth International Conference on Machine Learning. ICML
’98. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., July 1998, pp. 463–471.ISBN:
978-1-55860-556-5.
[40] Stuart Russell and Peter Norvig.Artificial Intelligence: A Modern Approach. 4th ed. Pearson Series
in Artificial Intelligence. Pearson, 2021.ISBN: 978-1-292-15396-4.
[41] Noor Sajid, Philip J. Ball, and Karl J. Friston. “Active Inference: Demystified and Compared”.
In: (2019).
[42] Noor Sajid et al. “Exploration and Preference Satisfaction Trade-off in Reward-Free Learning”.
In:arXiv: 2106.04316 [cs.AI](2021), pp. 1–23. arXiv:2106.04316 [cs.AI].
[43] Anna C. Sales et al. “Locus Coeruleus Tracking of Prediction Errors Optimises Cognitive Flex-
ibility: An Active Inference Model”. In:PLOS Computational Biology15.1 (2019), pp. 1–24.DOI:
10.1371/journal.pcbi.1006267.
[44] Simo S ¨arkk¨a.Bayesian Filtering and Smoothing. Institute of Mathematical Statistics Textbooks.
Cambridge: Cambridge University Press, 2013.ISBN: 978-1-107-03065-7.DOI:10.1017/CBO9781139344203.
[45] Anil K. Seth. “The Cybernetic Bayesian Brain”. In:Open MIND. Ed. by Thomas K. Metzinger
and Jennifer M. Windt. Vol. 35. Frankfurt am Main: MIND Group, 2015, pp. 1–24.ISBN: 978-3-
95857-010-8.
22
Prior preferences in active inference agents: soft, hard, and goal shaping
[46] Jin Young Shin, Cheolhyeong Kim, and Hyung Ju Hwang. “Prior Preference Learning from Ex-
perts: Designing a Reward with Active Inference”. In:Neurocomputing492 (July 2022), pp. 508–
515.ISSN: 0925-2312.DOI:10.1016/j.neucom.2021.12.042.
[47] Richard S. Sutton and Andrew G. Barto.Reinforcement Learning: An Introduction. Adaptive Com-
putation and Machine Learning. The MIT Press, 2018.ISBN: 978-0-262-03924-6.
[48] Filippo Torresan et al.Active Inference for Action-Unaware Agents. 2025.DOI:10.48550/arXiv.
2508.12027. arXiv:2508.12027 [cs.AI].
[49] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning Action-Oriented
Models through Active Inference”. In:PLOS Computational Biology16.4 (Apr. 2020), e1007805.
DOI:10.1371/journal.pcbi.1007805.
[50] Alexander Tschantz et al. “Scaling Active Inference”. In: (2019), pp. 1–13.
[51] Alexander Tschantz et al. “Reinforcement Learning through Active Inference”. In:International
Conference on Learning Representations. Vol. Bridging AI and Cognitive Science. 2020, pp. 1–16.
[52] Alexander Tschantz et al. “Scaling Active Inference”. In:2020 International Joint Conference on
Neural Networks (IJCNN). Glasgow, United Kingdom: IEEE, July 2020, pp. 1–8.ISBN: 978-1-7281-
6926-2.DOI:10.1109/IJCNN48605.2020.9207382.
[53] Otto van der Himst and Pablo Lanillos. “Deep Active Inference for Partially Observable MDPs”.
In:IWAI 2020. Ed. by Tim Verbelen et al. Vol. 1326. Communications in Computer and Infor-
mation Science. Springer, 2020.DOI:10.1007/978-3-030-64919-7_8.
23
Prior preferences in active inference agents: soft, hard, and goal shaping
Supplementary Material
S1. Mathematical background
S1.1. Notation
Table S1:Summary of notation.
Symbol Meaning
t,τ,Tintegers, i.e., generic, current, and terminal time index, respectively
1 :t, 1 :Tsequences of times steps up totandT, respectively
Hinteger, length of a sequence of actions (i.e., the policy horizon), in generalH≤T
pinteger, the number of action sequences (policies) the agent considers
Xt random variable with support inX, and witht∈[1,T]
X1:T,x1:T sequence of random variables with time index and related values
X:,j jth column of matrixXor random vector associated with that column
P(Xt),P(xt)probability distribution of random variableX t and probability thatXt =xt (when defined)
H[Xt]Shannon entropy of random variableX t
Sfinite set of cardinality|S|, i.e., the set of states
Ofinite set of cardinality|O|, i.e., the set of observations
Afinite set of cardinality|A|, i.e., the set of actions
AH finite set of action tuples (H-fold Cartesian product)
Πsubset of action sequences, i.e.,Π⊆ A H
St,Ot,At,πcategorical random variables with support inS,O,A,Π, respectively, i.e.,St ∼Cat(st), . . .
s,o,at,πk elements inS,O,A,Π, respectively, wherek∈[1,p]andp∈[1,|Π|]
sj,oj particular realisations of random variablesSt,Ot, withj∈[1,|S|]andj∈[1,|O|], respectively
st,ot,π, column vectors of parameters for state, observation, and policy random variables, respectively
st[i],ot[i],π[i]ith element of the parameter vector for state, observation, and policy random variables, respectively
Ttransition map/function
Eemission map/function
P(st|st,at)transition probability distribution (returned byT)
P(ot|st)emission probability distribution (returned byE)
P∗(st),P∗(ot)stationary distributions overSandO, respectively
Mgenerative model (collection of probability distributions)
Amatrix inR n×mstoring parameters ofP(Ot|St−1)(the same for anyt)
Btensor inR |A|×m×mstoring parameters ofP(St|St−1)(the same for anyt)
Ba1, . . . ,Bad state-transition matrices inRm×mfor each available action,d=|A|
Ffree energy
Faτ−1 action-conditioned free energy in vanilla active inference
Fπk policy-conditioned free energy in variational message passing
Gt single-step expected free energy
GH total expected free energy, i.e., sum of expected free energies forHtime steps in the future
∇stFπk gradient of policy-conditioned free energy with respect to vector of parametersst
∇πFgradient of free energy with respect to vector of policy parametersπ
F⊺π row-vector inR1×|Π|of policy-conditioned free energies
GH⊺ row-vector inR1×|Π|of total expected free energy
24
Prior preferences in active inference agents: soft, hard, and goal shaping
S2. Further information on experiments
S2.1. How to Reproduce the Results of the Experiment
The results reported in Section3were obtained by using the active inference implementation avail-
able athttps://github.com/FilConscious/cleanAIFand with the following command line in-
structions.
Soft preferences with goal-shaping:
1main_aif_au -- exp_name aif_au_softgs -- gym_id gridworld -v1 --
env_layout gridw9 -- num_runs 10 -- num_episodes 200 -- num_steps 5 --
inf_steps 10 -- action_selection kd -lB -- num_policies 256 --
pref_type states_manh -- pref_loc all_diff
Hard preferences with goal-shaping:
1main_aif_au -- exp_name aif_au_hardgs -- gym_id gridworld -v1 --
env_layout gridw9 -- num_runs 10 -- num_episodes 200 -- num_steps 5 --
inf_steps 10 -- action_selection kd -lB -- num_policies 256 --
pref_type states -- pref_loc all_diff
Soft preferences without goal-shaping:
1main_aif_au -- exp_name aif_au_soft -- gym_id gridworld -v1 -- env_layout
gridw9 -- num_runs 10 -- num_episodes 200 -- num_steps 5 -- inf_steps
10 -- action_selection kd -lB -- num_policies 256 -- pref_type
states_manh -- pref_loc all_goal
Hard preferences without goal-shaping:
1main_aif_au -- exp_name aif_au_hard -- gym_id gridworld -v1 -- env_layout
gridw9 -- num_runs 10 -- num_episodes 200 -- num_steps 5 -- inf_steps
10 -- action_selection kd -lB -- num_policies 256 -- pref_type states
-- pref_loc all_goal
The plots were obtained using the following command line instructions:
1vis_aif -gid gridworld -v1 -el gridw9 -nexp 1 -rdir
episodic_e200_pol16_maxinf10_learnB_cr_Bparams_softgs -fpi 0 1 2 3
4 -i 4 -v 8 -ti 4 -tv 8 -vl 3 -hl 3 -xtes 20 -ph 4 - selrun 0 -npv
16 -sb 4 -ab 0 1 2 3
With these instructions, one can visualize more metrics than those reported in the main text. We
offer a selection next.
25
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2. Additional figures
S2.2.1. State-acces frequency
21.0 20.0 0.3
0.5 19.3 0.3
0.3 20.0 18.3
State-access frequency
(soft with goal shaping)
0%
20%
40%
60%
80%
100%
Percentage of time steps
20.2 20.0 0.1
0.1 20.0 0.0
0.1 19.8 19.5
State-access frequency
(hard with goal shaping)
0%
20%
40%
60%
80%
100%
Percentage of time steps
23.5 11.0 4.6
11.3 11.7 9.8
4.4 8.8 15.0
State-access frequency
(soft without goal shaping)
0%
20%
40%
60%
80%
100%
Percentage of time steps
25.5 13.3 7.6
11.2 9.0 9.1
4.4 5.7 14.2
State-access frequency
(hard without goal shaping)
0%
20%
40%
60%
80%
100%
Percentage of time steps
Fig. S1:State-access frequency in the experiment (showing average of 10 agents).
S2.2.2. Breakdown of expected free energy at step 1
Risk is the largest component in expected free energy to the point of determining its trend (compare
the expected free energy and risk figures, Fig.4and Fig.S2, respectively, and see Fig.S3forB-
novelty). In agents with soft goals and goal-shaping, all policies converge on risk values between
2 and 4, with no clear distinction betweem task-solving and task-failing policies (see top-left plot
in Fig.S2). This is the case because in an environment characterised by deterministic transitions
(and without any kind of teleportation), there is no policy that can attain a probability distribution
over environment’s states that matches the agent’s prior (soft) preferences at each time step, thereby
minimizing risk. In contrast, in agents with hard goals, the policy minimizing risk isπ 4 because it
brings the agent to visit all the intermediate steps (goals), forming the trajectory to the main goal (see
top-right plot in Fig.S2). In agents without goal shaping, risk evolves similarly: it increases in the
first 60 or so episodes before converging to a stationary value (see bottom-left and bottom-right plots
in Fig.S2). All task-solving policies achieve the same risk value, but it is not the lowest one as a few
26
Prior preferences in active inference agents: soft, hard, and goal shaping
task-failing policies appear to minimize expected free energy even further. This is again due to how
preferences are encoded and on how risk is computed (see Section4.2).
As toB-novelty, its evolution is more in line with expectations: the more a policy has been tried
and executed, the more itsB-novelty will decrease. In agents with goal shaping, that is precisely
what happens to the preferredπ 4 (see top-left and top-right plots in Fig.S3). In agents without
goal shaping, the same reduction inB-novelty occurs for all the task-solving policies as well as few
task-failing (see bottom-left and bottom-right plots in Fig.S3).
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Risk
Risk at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Risk
Risk at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Risk
Risk at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0
2
4
6
8
10
12
14
16
18Risk
Risk at step 1
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. S2:Risk (expected free energy term) for each policy across episodes (showing average of 10 agents).
27
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00B-novelty
B-novelty at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00B-novelty
B-novelty at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00B-novelty
B-novelty at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00B-novelty
B-novelty at step 1
(hard without goal shaping)
Policies
1: , , , 
2: , , , 
3: , , , 
4: , , , 
5: , , , 
6: , , , 
7: , , , 
8: , , , 
9: , , , 
10: , , , 
11: , , , 
12: , , , 
13: , , , 
14: , , , 
15: , , , 
16: , , , 
Fig. S3: B-novelty (expected free energy term) for each policy across episodes (showing average of 10 agents).
28
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2.3. Average action probabilities at steps 2–4
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 2
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(hard without goal shaping)
Actions
Fig. S4:Action probabilities at step 2 of each episode (showing average of 10 agents).
29
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 3
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(hard without goal shaping)
Actions
Fig. S5:Action probabilities at step 3 of each episode (showing average of 10 agents).
30
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 4
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 4
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 4
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Probability mass
Action probabilities at step 4
(hard without goal shaping)
Actions
Fig. S6:Action probabilities at step 4 of each episode (showing average of 10 agent).
31
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2.4. Single-agent actions probabilities at step 1–4
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 1
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 1
(hard without goal shaping)
Actions
Fig. S7:Action probabilities at step 1 of each episode (showing results for one agent).
32
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 2
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 2
(hard without goal shaping)
Actions
Fig. S8:Action probabilities at step 2 of each episode (showing results for one agent).
33
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 3
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 3
(hard without goal shaping)
Actions
Fig. S9:Action probabilities at step 3 of each episode (showing results for one agent).
34
Prior preferences in active inference agents: soft, hard, and goal shaping
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 4
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 4
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.1
0.2
0.3
0.4
0.5
0.6Probability mass
Action probabilities at step 4
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episode
0.0
0.2
0.4
0.6
0.8
1.0Probability mass
Action probabilities at step 4
(hard without goal shaping)
Actions
Fig. S10:Action probabilities at step 4 of each episode (showing results for one agent).
35
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2.5. Comparison of action-dependent KL divergence from ground truth
1 20 40 60 80 100 120 140 160 180 200
Episodes
0
5
10
15
20
25
30Nats
Sum of KL divergences for each action
(soft with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episodes
0
5
10
15
20
25
30Nats
Sum of KL divergences for each action
(hard with goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episodes
0
5
10
15
20
25
30Nats
Sum of KL divergences for each action
(soft without goal shaping)
1 20 40 60 80 100 120 140 160 180 200
Episodes
0
5
10
15
20
25
30Nats
Sum of KL divergences for each action
(hard without goal shaping)
Actions
Fig. S11:Sums of KL divergences between learned and ground-truth transition probabilities (columns ofB-
matrices), i.e., ∑st∈S DKL [P(St|st,a t)|P∗(St|st,a t)], for each available actiona t (showing average of 10
agents). The decrease of this total KL divergence for one action across episodes implies that the agent
has learned the consequences of performing that action from multiple states in the environment.
36
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2.6. Ground truth transition maps
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S12:Ground truth transition maps for action→and←in the grid world.
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S13:Ground truth ransition maps for action↓and↑in the grid world.
37
Prior preferences in active inference agents: soft, hard, and goal shaping
S2.2.7. Learned transition maps
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S14:Transition maps for action→.
38
Prior preferences in active inference agents: soft, hard, and goal shaping
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S15:Transition maps for action↓.
39
Prior preferences in active inference agents: soft, hard, and goal shaping
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S16:Transition maps for action←.
40
Prior preferences in active inference agents: soft, hard, and goal shaping
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard with goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(soft without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
1 2 3 4 5 6 7 8 9
States
1
2
3
4
5
6
7
8
9 States
Transition matrix for action 
(hard without goal shaping)
0.0
0.2
0.4
0.6
0.8
1.0
Probability
Fig. S17:Transition maps for action↑.
41