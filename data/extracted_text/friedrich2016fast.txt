Fast Online Deconvolution of Calcium Imaging Data
Johannes Friedrich1,2, Pengcheng Zhou1,3, Liam Paninski1,4
1Grossman Center for the Statistics of Mind and Department of Statistics,
Columbia University, New York, NY
2Janelia Research Campus, Ashburn, V A
3Center for the Neural Basis of Cognition and Machine Learning Department,
Carnegie Mellon University, Pittsburgh, PA
4Kavli Institute for Brain Science, and NeuroTechnology Center
Columbia University, New York, NY , USA
j.friedrich@columbia.edu, pengchez@andrew.cmu.edu, liam@stat.columbia.edu
Abstract
Fluorescent calcium indicators are a popular means for observing the spiking activ-
ity of large neuronal populations, but extracting the activity of each neuron from
raw ﬂuorescence calcium imaging data is a nontrivial problem. We present a fast
online active set method to solve this sparse non-negative deconvolution problem.
Importantly, the algorithm progresses through each time series sequentially from
beginning to end, thus enabling real-time online estimation of neural activity during
the imaging session. Our algorithm is a generalization of the pool adjacent violators
algorithm (PA V A) for isotonic regression and inherits its linear-time computational
complexity. We gain remarkable increases in processing speed: more than one
order of magnitude compared to currently employed state of the art convex solvers
relying on interior point methods. Unlike these approaches, our method can ex-
ploit warm starts; therefore optimizing model hyperparameters only requires a
handful of passes through the data. A minor modiﬁcation can further improve the
quality of activity inference by imposing a constraint on the minimum spike size.
The algorithm enables real-time simultaneous deconvolution of O(105) traces of
whole-brain larval zebraﬁsh imaging data on a laptop.
1 Introduction
Calcium imaging has become one of the most widely used techniques for recording activity from
neural populations in vivo [1]. The basic principle of calcium imaging is that neural action potentials
(or spikes), the point process signal of interest, each induce an optically measurable transient
response in calcium dynamics. The nontrivial problem of extracting the activity of each neuron
from a raw ﬂuorescence trace has been addressed with several different approaches, including
template matching [ 2] and linear deconvolution [ 3, 4], which are outperformed by sparse non-
negative deconvolution [ 5]. The latter can be interpreted as the maximum a posteriori (MAP)
estimate under a simple generative model (linear convolution plus noise; Fig 1), whereas fully
Bayesian methods [6, 7, 8] can provide some further improvements, but are more computationally
expensive. Supervised methods trained on simultaneously-recorded electrophysiological and imaging
data [9, 10] have also recently achieved state of the art results, but are more black-box in nature;
Bayesian methods based on a well-deﬁned generative model are somewhat easier to generalize to
more complex multi-neuronal or multi-trial settings [11, 12, 13].
The methods above are typically applied to imaging data ofﬂine, after the experiment is complete;
however, there is a need for accurate and fast real-time processing to enable closed-loop experiments, a
A shorter version of this article has been published as part of the proceedings of the 30th Conference on Neural
Information Processing Systems (NIPS 2016), Barcelona, Spain.
arXiv:1609.00639v3  [q-bio.NC]  16 Mar 2017
powerful strategy for causal investigation of neural circuitry [14]. In particular, observing and feeding
back the effects of circuit interventions on physiologically relevant timescales will be valuable for
directly testing whether inferred models of dynamics, connectivity, and causation are accurate in vivo,
and recent experimental advances [15, 16] are now enabling work in this direction. Brain-computer
interfaces (BCIs) also rely on real-time estimates of neural activity. Whereas most BCI systems rely
on electrical recordings, BCIs have been driven by optical signals too [17], providing new insight
into how neurons change their activity during learning on a ﬁner spatial scale than possible with
intracortical electrodes. Finally, adaptive experimental design approaches [18, 19, 20] also rely on
online estimates of neural activity.
Even in cases where we do not require the strict timing/latency constraints of real-time processing,
we still need methods that scale to large data sets as for example in whole-brain imaging of larval
zebraﬁsh [ 21, 22]. A further demand for scalability stems from the fact that the deconvolution
problem is solved in the inner loop of constrained non-negative matrix factorization (CNMF) [13],
the current state of the art for simultaneous denoising, deconvolution, and demixing of spatiotemporal
calcium imaging data.
In this paper we address the pressing need for scalable online spike inference methods. Building on
previous work, we frame this estimation problem as a sparse non-negative deconvolution. Current
algorithms employ interior point methods to solve the ensuing optimization problem and are fast
enough to process hundreds of neurons in about the same time as the recording [ 5], but can not
handle larger data sets such as whole-brain zebraﬁsh imaging in real time. Furthermore, these interior
point methods scale linearly in the length of the recording, but they cannot be warm started [23], i.e.,
initialized with the solution from a previous iteration to gain speed-ups, and do not run online.
Here we note a close connection between the MAP problem and isotonic regression, which ﬁts
data by a monotone piecewise constant function. A classic algorithm for isotonic regression is the
pool adjacent violators algorithm (PA V A) [24, 25], which can be understood as an online active-set
optimization method. We generalized PA V A to derive an Online Active Set method to Infer Spikes
(OASIS); this new approach to solve the MAP problem yields speed-ups in processing time by at least
one order of magnitude compared to interior point methods on both simulated and real data. Further,
OASIS can be warm-started, which is useful in the inner loop of CNMF, and also when adjusting
model hyperparameters, as we show below. Importantly, OASIS is not only much faster, but operates
in an online fashion, progressing through the ﬂuorescence time series sequentially from beginning to
end. The advances in speed paired with the inherently online fashion of the algorithm enable true
real-time online spike inference during the imaging session (once the spatial shapes of neurons in the
ﬁeld of view have been identiﬁed), with the potential to signiﬁcantly impact experimental paradigms.
2 Methods
This section is organized as follows. The ﬁrst subsection introduces the autoregressive (AR(p)) model
for calcium dynamics.
In the second subsection we derive an Online Active Set method to Infer Spikes (OASIS) for an
AR(1) model. The algorithm is inspired by the pool adjacent violators algorithm (PA V A, Alg 1),
which we review ﬁrst and then generalize to obtain OASIS (Alg 2). This algorithm requires some
hyperparameter values; the optimization of these hyperparameters is described next, along with several
computational tricks for speeding up the hyperparameter estimation. We ﬁnally discuss thresholding
approaches for reducing the number of small values returned by the original ℓ1-penalized approach.
The resulting problem is non-convex, and so we lose guarantees on ﬁnding global optima, but we can
easily adapt OASIS to quickly ﬁnd good solutions.
In the third subsection we generalize to AR(p) models of the calcium dynamics and describe a dual
active set algorithm that is analogous to the one presented for the AR(1) case (Alg 2). However, this
algorithm is greedy if p> 1 and yields only a good approximate solution. We can reﬁne this solution
and obtain the exact result by warm-starting an alternative primal active set method we call ONNLS
(Alg 3). Finally, Alg 4 summarizes all of these steps.
2
0 150 300Time
0
1
2Fluorescence
s c y
Figure 1: Generative autoregessive model for calcium dynamics. Spike train sgets ﬁltered to
produce calcium trace c; here we used p = 2 as order of the AR process. Added noise yields the
observed ﬂuorescence y.
2.1 Model for calcium dynamics
We assume we observe the ﬂuorescence signal forT timesteps, and denote by st the number of spikes
that the neuron ﬁred at the t-th timestep, t= 1,...,T , cf. Fig 1. Following [5, 13], we approximate
the calcium concentration dynamics cusing a stable autoregressive process of order p(AR(p)) where
pis a small positive integer, usually p= 1 or 2,
ct =
p∑
i=1
γict−i + st. (1)
The observed ﬂuorescence y∈RT is related to the calcium concentration as [5, 6, 7]:
yt = act + b+ ϵt, ϵ t ∼N(0,σ2) (2)
where ais a non-negative scalar, bis a scalar offset parameter, and the noise is assumed to be i.i.d.
zero mean Gaussian with variance σ2. For the remainder we assume units such that a= 1 without
loss of generality. We begin by assuming b= 0 for simplicity, but we will relax this assumption later.
(We also assume throughout that all parameters in sight are ﬁxed; in case of e.g. drifting baselines b
we could generalize the algorithms discussed here to operate over shorter temporal windows, but we
do not pursue this here.) The parameters γi and σcan be estimated from the autocovariance function
and the power spectral density (PSD) of yrespectively [13]. The autocovariance approach assumes
that the spiking signal scomes from a homogeneous Poisson process and in practice often gives a
crude estimate of γi. We will improve on this below (Fig 3) by ﬁtting the AR coefﬁcients directly,
which leads to better estimates, particularly when the spikes have some signiﬁcant autocorrelation.
The goal of calcium deconvolution is to extract an estimate ˆsof the neural activity sfrom the vector
of observations y. As discussed in [5, 13], this leads to the following non-negative LASSO problem
for estimating the calcium concentration:
minimize
ˆc,ˆs
1
2 ∥ˆc−y∥2 + λ∥ˆs∥1 subject to ˆs= Gˆc≥0 (3)
where the ℓ1 penalty on ˆsenforces sparsity of the neural activity and the lower triangular matrix Gis
deﬁned as:
G=


1 0 ... ... ... ... 0
−γ1 1
. . .
. . .
. . .
. . . 0
...
. . .
. . .
. . .
. . .
. . .
...
−γp ... −γ1 1 0 ... 0
0 −γp ... −γ1 1
. . . 0
...
. . .
. . .
. . .
. . .
. . .
...
0 ... 0 −γp ... −γ1 1


. (4)
The deconvolution matrixGis banded with bandwidthpfor an AR(p) process. Equivalently,s= c∗g
with ga ﬁnite impulse response ﬁlter of order p(p+ 1 ﬁlter taps) and ∗denoting convolution. To
produce calcium trace c, spike train sis ﬁltered with the inverse ﬁlter of g, an inﬁnite impulse
response h, c= s∗h. (Although our main focus is on the autoregressive model, we will discuss
more general convolutional observation models below as well, and touch on nonlinear effects such
as saturation in the appendix.) Following the approach in [5], note that the spike signal ˆsis relaxed
from non-negative integers to arbitrary non-negative values.
3
2.2 Derivation of the active set algorithm
The optimization problem (3) could be solved using generic convex program solvers. Here we derive
the much faster Online Active Set method to Infer Spikes (OASIS). The algorithm is inspired by the
pool adjacent violators algorithm (PA V A) [24, 25], which we review ﬁrst for readers not familiar with
this classic algorithm before generalizing it to the non-negative LASSO problem.
2.2.1 Pool Adjacent Violators Algorithm (PA V A)
The pool adjacent violators algorithm (Alg 1) is a classic exact algorithm for isotonic regression,
which ﬁts data by a non-decreasing piecewise constant function. This algorithm is due to [24] and
was independently discovered by other authors [26, 27] as reviewed in [25, 28]. It can be considered
as a dual active set method [29]. Formally, the (convex) problem is to
minimize
x
∥x−y∥2 subject to x1 ≤...≤xT. (5)
We ﬁrst present the algorithm in a way that conveys its core ideas (see Alg S1 in the Appendix),
then improve the algorithm’s efﬁciency by introducing “pools” of variables (adjacent xt values)
which are updated simultaneously. We introduce temporary values x′and initialize them to the
unconstrained least squares solution, x′= y. Initially all constraints are in the “passive set” and
possible violations are ﬁxed by subsequently adding the respective constraints to the “active set”.
Starting at t = 2 the algorithm moves to the right until a violation of the constraint x′
τ ≥x′
τ−1
at some time τ is encountered. Now the monotonicity constraint is added to the active set and
enforced by setting x′
τ = x′
τ−1. (Supposing the opposite, i.e. x′
τ > x′
τ−1, we could move x′
τ and
x′
τ−1 by some small ϵto decrease the objective without violating the constraints, yielding a proof by
contradiction that the monotonicity constraint should be made “active” here - i.e., the constraint holds
with strict equality.) We update the valuesx′
τ = x′
τ−1 at the two time steps to the best possible ﬁt with
constraints. Minimizing their contribution to the residual (yτ−1 −x′
τ−1)2 + (yτ −x′
τ−1)2 by setting
the derivative with respect to x′
τ−1 to zero, yτ−1 −x′
τ−1 + yτ −x′
τ−1 = 0, amounts to replacing
the values with their average, x′
τ−1 = x′
τ = yτ−1+yτ
2 . However, this updated value can violate the
constraint x′
τ−1 ≥x′
τ−2 and we need to add this constraint to the active set and update x′
τ−2 as well,
x′
τ−2 = x′
τ−1 = x′
τ = yτ−2+yτ−1+yτ
3 , etc. In this manner the algorithm continues to back-average
to the left as needed until we have backtracked to time t′where the constraint x′
t′ ≥x′
t′−1 is already
valid. Solving
minimize
x′
t′
τ∑
t=t′
(x′
t′−yt)2 (6)
by setting the derivative to zero yields an update that corresponds to averaging
x′
t′ = x′
t′+1 = ...= x′
τ =
∑τ
t=t′yt
τ −t′+ 1. (7)
The optimal solution that satisﬁes all constraints up to time τ has been found and the search advances
to the right again until detection of the next violation, backtracks again, etc. This process continues
until the last value xT is reached and having found the optimal solution we return x= x′.
Algorithm 1 Pool Adjacent Violators Algorithm (PA V A) for isotonic regression
Require: data yt ∈y at time of reading
1: initialize set of pools P←{} , data index t←0, pool index i←0
2: for yin y do ⊿read next data point y
3: t←t+ 1
4: P←P∪{ (y,1,t)} ⊿add pool (vi+1,wi+1,ti+1)
5: while i> 0 and vi+1 <vi do ⊿merge pools if necessary
6: Pi ←
(
wivi+wi+1vi+1
wi+wi+1
,wi + wi+1,ti
)
7: remove Pi+1
8: i←i−1
9: i←i+ 1
10: for (v,w,t ) in Pdo ⊿construct solution for all t
11: for τ = 0,...,w −1 do xt+τ ←v
12: return x
4
In a worst case situation a constraint violation is encountered at every step of the forward sweep
through the series. Updating all tvalues up to time tyields overall ∑T
t=2 t= T(T+1)
2 −1 updates
and an O(T2) algorithm. However, note that when a violation is encountered the updated time points
all share the same value (the average of the data at these time points, Eq 7) and it sufﬁces to track
this value just once for all these updated time points [30]. The constraints x′
t ≥x′
t−1 between the
updated time points hold with equality x′
t = x′
t−1, and are part of the active set. In order to obtain
a more efﬁcient algorithm, cf. Algorithm 1 and Video S1, we introduce “pools” or groups of the
form (vi,wi,ti) with value vi, weight wi and event time ti where iindices the groups. Initially the
ordered set of pools is empty. During the forward sweep through the data the next data point yt is
initialized as its own pool (yt,1,t) and appended to the set of pools. Adjacent pools that violate the
constraint vi+1 ≥vi are combined to a new pool (wivi+wi+1vi+1
wi+wi+1
,wi + wi+1,ti). Whenever pools i
and i+ 1are merged, former pool i+ 1is removed. It is easy to prove by induction that these updates
guarantee that the value of a pool is indeed the average of the corresponding data points (see S.3)
without having to explicitly calculate it using Eq (7). The latter would be expensive for long pools,
whereas merging two pools has O(1) complexity independent of the pool lengths. With pooling the
considered worst case situation results in a single pool and only its value and weight are updated at
every step forward, yielding O(T) complexity. Constructing the optimal solution xt for all tin a ﬁnal
effort after the optimal pool partition has been reached is also O(T). At convergence all constraints
have been enforced; further note that convergence to the exact solution occurs after a ﬁnite number of
steps, in contrast to interior point-methods which only approach the optimal solution asymptotically.
2.3 Online Active Set method to Infer Spikes (OASIS)
Now we adapt the PA V A approach to problem (3). PA V A solves a regression problem subject to
the constraint that the value at the current time bin must be greater than or equal to the last. The
AR(1) model posits a more general but very similar constraint that bounds the rate of decay instead
of enforcing monotonicity. The key insight is that problem (3) is a generalization of problem (5): if
p= 1 in the AR model and we set γ = 1 (we skip the index of γfor a single AR coefﬁcient) and
λ= 0 in Eq (3) we obtain Eq (5). Therefore we focus ﬁrst on the p= 1 case and deal with p> 1 and
arbitrary calcium response kernels in the next section.
We begin by inserting the deﬁnition of ˆs(Eq 3). Using that ˆsis constrained to be non-negative yields
for the sparsity penalty
λ∥ˆs∥1 = λ1⊤ˆs= λ
T∑
t=1
T∑
k=1
Gk,tˆct = λ
T∑
t=1
(1 −γ+ γδtT)ˆct =
T∑
t=1
µtˆct = µ⊤ˆc (8)
with µt := λ(1 −γ+ γδtT) (with δdenoting Kronecker’s delta) by noting that the sum of the last
column of Gis 1, whereas all other columns sum to (1 −γ).
Now the problem
minimize
ˆc
1
2
T∑
t=1
(ˆct −yt)2 +
T∑
t=1
µtˆct subject to ˆct+1 ≥γˆct ≥0 ∀t (9)
shares some similarity to isotonic regression with the constraint ˆct+1 ≥ˆct (Eq 5). However, our
constraint ˆct+1 ≥γˆct bounds the rate of decay instead of enforcing monotonicity. Thus we need to
generalize PA V A to handle the additional factorγ.
For clarity we mimic our approach from the last section: we ﬁrst present the algorithm in a way
that conveys its core ideas, and then improve the algorithm’s efﬁciency using pools. We introduce
temporary values c′and initialize them to the unconstrained least squares solution, c′ = y−µ.
Starting at t= 2 one moves forward until a violation of the constraint c′
τ ≥γc′
τ−1 at some time τ is
detected (Fig 2A). Updating the two time steps by minimizing 1
2 (yτ−1 −c′
τ−1)2 + 1
2 (yτ −γc′
τ−1)2 +
µτ−1c′
τ−1 + µτγc′
τ−1 yields an updated value c′
τ−1. However, this updated value can violate the
constraint c′
τ−1 ≥γc′
τ−2 and we need to update c′
τ−2 as well, etc., until we have backtracked some
∆tsteps to time t′= τ −∆twhere the constraint c′
t′ ≥γc′
t′−1 is already valid. At most one needs
to backtrack to the most recent spike, because c′
t′ >γc ′
t′−1 at spike times t′(Eq 1). Solving
minimize
c′
t′
1
2
∆t∑
t=0
(γtc′
t′−yt+t′)2 +
∆t∑
t=0
µt+t′γtc′
t′ (10)
5
A B C D E
F G H
 I
move forward track back move forward move forward move forward
track back track back move forward track back
. . .
Figure 2: Illustration of OASIS for an AR(1) process (see Video S2). Red lines depict true spike
times. The shaded background shows how the time points are gathered in pools. The pool currently
under consideration is indicated by the blue crosses. A constraint violation is encountered for the
second time step (A) leading to backtracking and merging (B). The algorithm proceeds moving
forward (C-E) until the next violation occurs (E) and triggers backtracking and merging (F-G) as
long as constraints are violated. When the most recent spike time has been reached (G) the algorithm
proceeds forward again (H). The process continues until the end of the series has been reached (I).
The solution is obtained and pools span the inter-spike-intervals.
by setting the derivative to zero yields
c′
t′ =
∑∆t
t=0(yt+t′−µt+t′)γt
∑∆t
t=0 γ2t
(11)
and the next values are updated according to c′
t′+t = γtc′
t′ for t = 1,..., ∆t. Note the similarity
of Eq (7) and (11), which differs by weighting the summands by powers of γ due to the altered
constraints, and by subtracting µfrom the data ydue to the sparsity penalty. (Along the way it is
worth noting that, because a spike induces a calcium response described by kernelhwith components
h1+t = γt, c′
t′ could be expressed in the more familiar regression form as
h⊤
1:∆t+1(y−µ)t′:τ
h⊤
1:∆t+1h1:∆t+1
, where
we used the notation vi:j to describe a vector formed by components ito jof v.) Now one moves
forward again (Fig 2C-E) until detection of the next violation (Fig 2E), backtracks again to the most
recent spike (Fig 2G), etc. Once the end of the time series is reached (Fig 2I) we have found the
optimal solution and set ˆc= c′.
While this yields a valid algorithm, it frequently updates each value c′
t and recalculates the full sums
in Eq (11) for each step of backtracking. A similar algorithm has been suggested by [ 31] for the
problem without sparsity penalty. However, it passes through the time series in reverse direction,
from its end to its beginning, and is thus not applicable to online processing. It considers directly the
deconvolved activity ˆsand efﬁciently does not update all time steps but only suspected spike times.
However, their algorithm uses the inefﬁcient updates of Eq 11, rendering it an O(T2) algorithm.
As in PA V A, next we introduce “pools" into the algorithm; these are of critical importance in order to
obtain a true O(T) algorithm. In PA V A these pools serve as sufﬁcient statistics summarizing the data
between jumps in the estimated output xt; here the pools summarize the data between estimated spike
times, where the estimated calcium signal ˆct jumps. Pools are now tuples of the form (vi,wi,ti,li)
with value vi, weight wi, event time ti and pool length li. Here we explicitly track the pool length,
which was identical to its weight for PA V A. Initially the ordered set of pools is empty. During the
forward sweep through the data the next data point yt is initialized as its own pool (yt −µt,1,t, 1)
and appended to the set of pools. During backtracking pools get combined and only the ﬁrst value
vi = c′
ti is explicitly considered, while the other values are merely deﬁned implicitly via c′
t+1 = γc′
t.
The constraint c′
t+1 ≥γc′
t translates to vi+1 ≥γlivi as the criterion determining whether pools need
to be combined. The introduced weights allow efﬁcient value updates whenever pools are merged by
6
avoiding recalculating the sums in Eq (11). Values are updated according to
vi ←wivi + γliwi+1vi+1
wi + γ2liwi+1
(12)
where the denominator is the new weight of the pool and the pool lengths are summed
wi ←wi + γ2liwi+1 (13)
li ←li + li+1 (14)
Whenever pools iand i+ 1are merged, former pool i+ 1is removed. It is easy to prove by induction
that the updates according to Eqs (12-14) guarantee that Eq (11) holds for all values (see section S.3
in the appendix).
Analogous to PA V A, the updates solve Eq (9) not just greedily but optimally, ﬁnding the exact
solution to the convex problem in O(T). One important point (especially relevant for online use) is
that the computation time per observation timestep is not ﬁxed but random, since we might have to
backtrack to update an unpredictable number of pools. We found empirically that, over a wide range
of hyperparameters, in about 80% of the cases 0-1 merge operation was performed per observation
timestep. With less than 0.5% probability four or more merges were necessary; in all our experiments
so far, never more than seven merges were needed.
The ﬁnal algorithm is summarized in Algorithm 2 and illustrated in Fig 2 as well as in Video S2.
Comparing Algorithm 1 with 2 clearly reveals the modiﬁcations made and shows that for γ = 1 and
λ= 0 the algorithm reduces to PA V A.
Algorithm 2 Fast online deconvolution algorithm for AR1 processes with positive jumps
Require: decay factor γ, regularization parameter λ, data yt ∈y at time of reading
1: initialize set of pools P←{} , time index t←0, pool index i←0, solution ˆs ←0
2: for yin y do ⊿read next data point y
3: t←t+ 1
4: P←P∪{ (y−λ(1 −γ+ γδtT),1,t, 1)} ⊿add pool (vi+1,wi+1,ti+1,li+1)
5: while i> 0 and vi+1 <γ livi do ⊿merge pools if necessary
6: Pi ←
(
wivi+γliwi+1vi+1
wi+γ2liwi+1
,wi + γ2liwi+1,ti,li + li+1
)
⊿Eqs (12-14)
7: remove Pi+1
8: i←i−1
9: i←i+ 1
10: for (v,w,t,l ) in Pdo ⊿construct solution for all t†
11: for τ = 0,...,l −1 do ˆct+τ ←γτ max(0,v) ⊿enforce ˆct ≥0 via max
12: if t> 1 then ˆst ←ˆct −γˆct−1
13: return ˆc,ˆs
†For online estimates of ˆc and ˆs the solution can be constructed within the loop over y not just after it.
2.3.1 Dual formulation with hard noise constraint
The formulation above contains a troublesome free sparsity parameter λ(implicit in µ). A more
robust deconvolution approach chooses the sparsity implicitly by inclusion of the residual sum of
squares (RSS) as a hard constraint and not as a penalty term in the objective function [ 13]. The
expected RSS satisﬁes ⟨∥c−y∥2⟩= σ2T and by the law of large numbers ∥c−y∥2 ≈σ2T with
high probability, leading to the constrained problem
minimize
ˆc,ˆs
∥ˆs∥1 subject to ˆs= Gˆc≥0 and ∥ˆc−y∥2 ≤ˆσ2T. (15)
(As noted above, we estimate σusing the power spectral estimator described in [13]; see also [8] for
a similar approach.)
We will solve this problem by increasing λin the dual formulation until the noise constraint is tight.
We start with some small λ, e.g. λ= 0, to obtain a ﬁrst partitioning into pools P, cf. Fig 3A. From
Eqs (11-13) along with the deﬁnition of µ(Eq 8) it follows that given the solution (vi,wi,ti,li),
7
0
2 Correlation: 0.734Truth Estimate Data
0 λ∗ λ
σ2TRSSλ□
0
2 Correlation: 0.737
0
2Fluorescence
Correlation: 0.769
ˆγ∗ ˆγ
σ2TRSS ˆγ□
0
2 Correlation: 0.779
0
2 Correlation: 0.792
10 20 30 40Time [s]
0
2 Correlation: 0.851true γ ˆγ from autocovariance
A
run Alg 2
B
C
run Alg 2
D
E
run Alg 2
F
Iterate B – E
∼3 iterations
to converge
...
Figure 3: Optimizing sparsity parameter λand AR coefﬁcient ˆγ. (A) Running the active set
method, with conservatively small estimate ˆγ, yields an initial denoised estimate (blue) of the data
(gray) roughly capturing the truth (red). We also report the correlation between the deconvolved
estimate and true spike train as a direct measure for the accuracy of spike train inference. (B)
Updating sparsity parameter λaccording to Eq (18) such that RSS = σ2T (left) shifts the current
estimate downward (right, blue). (C) Running the active set method enforces the constraints again
and is fast due to warm-starting. (D) Updating ˆγ by minimizing the polynomial function RSS( ˆγ)
and (E) running the warm-started active set method completes one iteration, which yields already
a decent ﬁt. (F) A few more iterations improve the solution further. The obtained estimate (blue)
is hardly distinguishable from the one obtained with known true γ(yellow dashed trace, plotted in
addition to the traces in A-E, is on top of blue solid line). Note that determining ˆγ based on the
autocovariance (additionally plotted purple trace) yields a crude solution that even misses spikes (at
24.6 s and 46.5 s).
where
vi =
∑li−1
t=0 (yti+t −µti+t)γt
∑li−1
t=0 γ2t
=
∑li−1
t=0 (yti+t −λ(1 −γ+ γδti+t,T))γt
wi
(16)
for some λ, the solution (v′
i,w′
i,t′
i,l′
i) for λ+ ∆λis
v′
i = vi −∆λ
∑li−1
t=0 (1 −γ+ γδti+t,T)γt
wi
= vi −∆λ1 −γli(1 −δiz)
wi
(17)
where z is the index of the last pool and because pools are updated independently we make the
approximation that no changes in the pool structure occur. Inserting Eq (17) into the noise constraint
(Eq 15) results in
z∑
i=1
li−1∑
t=0
((
vi −∆λ1 −γli(1 −δiz)
wi
)
γt −yti+t
)2
= ˆσ2T (18)
and solving the quadratic equation for ∆λyields
∆λ= −β+
√
β2 −4αϵ
2α (19)
with α = ∑
i,tξ2
it, β = 2 ∑
i,tχitξit and ϵ = ∑
i,tχ2
it −ˆσ2T where ξit = 1−γli(1−δiz)
wi
γt and
χit = yti+t −viγt.
The solution ∆λprovides a good approximate proposal step for updating the pool values vi (using
Eq 17). Since this update proposal is only approximate it can give rise to violated constraints (e.g.,
8
negative values of vi. To satisfy all constraints Algorithm 2 is run to update the pool structure, cf.
Fig 3C, but with a warm start: we initialize with the current set of merely zpools P′instead of the T
pools for a cold start (Alg 2, line 4). This step returns a set of vi values that satisfy the constraints
and may merge pools (i.e., delete spikes); then the procedure (update λthen rerun the warm-started
Algorithm 2) can be iterated until no further pools need to be merged, at which point the procedure
has converged. In practice this leads to an increasing sequence of λvalues (corresponding to an
increasingly sparse set of spikes), and no pool-split (i.e., add-spike) moves are necessary. (Note that
it is possible to cheaply detect any violations of the KKT conditions in a candidate solution; if such a
violation is detected, the corresponding pool could be split and the warm-started Algorithm 2 run
locally near the detected violations. However, as we noted, due to the increasing λsequence we did
not ﬁnd this step to be necessary in the examples examined here.)
This warm-starting approach brings major speed beneﬁts: after the residual is updated following a
λupdate, the computational cost of the algorithm is linear in the number of pools z, hence warm
starting drastically reduces computational costs from k1T to k2zwith proportionality constants k1
and k2: if no pool boundary updates are needed then after warm starting the algorithm only needs to
pass once through all pools to verify that no constraint is violated, whereas a cold start might involve
a couple passes over the data to update pools, so k2 is typically signiﬁcantly smaller than k1, and zis
typically much smaller than T (especially in sparsely-spiking regimes).
2.3.2 Additional baseline
For ease of exposition we thus far assumed no offsetting baseline. Adding a known baseline b̸= 0
the problem reads
minimize
ˆc,ˆs
1
2 ∥b1 + ˆc−y∥2 + λ∥ˆs∥1 subject to ˆs= Gˆc≥0. (20)
For known baseline one merely needs to initialize OASIS by subtracting not only the sparsity
parameter µ(λ) from the data y, cf. Eq (11) and Algorithm 2, but also the baselineb. The ﬂuorescence
ˆcdepends only on the sum φ= b1 + µ.
If the baseline is not known, we want to optimize it too by solving the noise constrained dual problem
minimize
ˆb,ˆc,ˆs
∥ˆs∥1 subject to ˆs= Gˆc≥0 and ∥ˆb1 + ˆc−y∥2 ≤ˆσ2T. (21)
We denote all except the differing last component of µby µ = λ(1 −γ) (Eq 8) and of φby
φ= b+ λ(1 −γ). φis the total shift applied to the data (except for the last time step) due to the
baseline and sparsity penalty before running OASIS. We increase φuntil the noise constraint is tight.
φcan be initialized by min yt or better by a small percentile of y, e.g. 15%. Once OASIS has been
run with some φthe baseline ˆbis obtained by minimizing the objective (20) with respect to it, yielding
ˆb= ⟨y−ˆc⟩= 1
T
∑T
t=1(yt −ˆct), and the sparsity parameter is µ= φ−ˆb. Appropriately adding ˆb
to Eq (18)
z∑
i=1
li−1∑
t=0
((
vi −∆φ1 −γli(1 −δiz)
(1 −γ)wi
)
γt −yti+t + ˆb
)2
= ˆσ2T (22)
and plugging the analytic expression ˆb = 1
T
∑T
t=1(yt − ˆct) =
1
T
∑z
j=1
∑lj−1
τ=0
(
ytj+τ −
(
vj −∆φ1−γlj(1−δjz)
(1−γ)wj
)
γτ
)
into Eq (22) to account for the changing
baseline, we obtain an estimate of ∆φ using a block coordinate update of φ and ˆb. Solving the
ensuing quadratic equation for ∆φ, yields
∆φ= −β+
√
β2 −4αϵ
2α (23)
with α = ∑
i,tξ2
it, β = 2 ∑
i,tχitξit and ϵ = ∑
i,tχ2
it −ˆσ2T where ξit = 1−γli(1−δiz)
(1−γ)wi
γt −
∑
j
(1−γlj(1−δjz))
2
T(1−γ)2wj
and χit = yti+t −viγt − 1
T
∑
j,τ(ytj+τ −vjγτ). All pools are updated
according to v′
i = vi −∆φ1−γli(1−δiz)
(1−γ)wi
, cf. Eq (17). To satisfy all constraints Algorithm 2 is run,
warm-started by initializing with the current set of pools.
9
2.3.3 Optimizing the AR coefﬁcient
Thus far the parameter γhas been known or been estimated based on the autocovariance function. We
can improve upon this estimate by optimizing ˆγas well, which is illustrated in Fig 3. After updating
λ(and ˆb) followed by running Algorithm 2, we perform a coordinate descent step in ˆγthat minimizes
the RSS, cf. Fig 3D. The RSS as a function of ˆγis a high order polynomial, cf. Eq (11), and we need
to settle for numerical solutions of
ˆγ = arg min
γ
z∑
i=1
li−1∑
t=0
(
ˆb+
∑li−1
τ=0 (yti+τ −µti+τ)γτ
∑li−1
τ=0 γ2τ
γt −yti+t
)2
. (24)
We used Brent’s method [32] with bounds 0 ≤ˆγ <1 to solve this problem. One iteration consists
now of steps B-E in Fig 3, while for knownγonly B-C were necessary. If optimizing the baseline too,
we obtained better results by minimizing the RSS jointly with respect toˆγand ˆbusing L-BFGS-B [33]
instead of keeping the baseline ˆbﬁxed.
2.3.4 Faster optimization of hyperparameters
We have presented methods to estimate the hyperparameters λ, band γ, which require a handful
of warm-started iterations of OASIS. To gain further speed-ups these parameters can be estimated
on decimated data. When downsampling by a factor k, the average of k subsequent frames is
calculated, the noise ˆσdivided by a factor
√
kand the initial estimate of the AR coefﬁcient scaled to
ˆγk. Alternatively, one could estimate σand γbased on the decimated data. Once the hyperparameters
have been obtained, the corresponding inverse transformations are performed: ˆγ →ˆγ
1
k, ˆb→ˆband
λ→λ 1−ˆγ
1−ˆγ1/k such that the shrinkage µ= λ(1 −ˆγ) due to the penalty term stays invariant. The ﬁnal
run of OASIS on the full data is warm started using the solution obtained on the decimated data. Data
points that are not in the proximity of a spike of the downsampled solution are already combined into
large pools, instead of initializing each data point as its own separate pool. More precisely, if the
deconvolved decimated data has positive values at times {ti}, for deconvolving the full data time
steps ⋃
i{(k−1)ti,..., (k+ 1.5)ti}are initialized as individual pools, while the remaining time steps
are pooled together into bigger pools, separated from each other by the individual ones, with values
given by Eq (11) and weights by its denominator.
In particular the estimation of the AR coefﬁcient γ is computationally burdensome, because it
involves expensive repeated evaluations of the RSS in order to minimize it as function ofˆγ(and ˆb).
The computing time depends linearly on the number of pools zand we gain further speed-ups by
restricting the attention to merely a subset of pools. In particular, because γcan be well estimated
based on large isolated calcium events, we restrict the calculation of the RSS to the pools with largest
product of value and length. A large value indicates a large event and a long pool an isolated event.
We present detailed results in the Results section, indicating that altogether we can save about an
order of magnitude computation with the greatest savings obtained by reducing the optimization of ˆγ
from O(z) to O(1).
It is also worth noting that the hyperparameter estimation discussed above is performed in ‘batch’
mode, not online. However, once good hyperparameter values are obtained on a short initial batch we
can switch into online mode (with the hyperparameters held ﬁxed) and handle the remaining data in a
stream.
2.3.5 Hard shrinkage and ℓ0 penalty
It is well-known that ℓ1 penalization results in “soft-thresholding” [34], in which small values are
zeroed out and large values are shifted to lower values (where the size of this shift is proportional
to the penalty λ). We can perform hard instead of soft thresholding (avoiding this shrinkage of
large values) by replacing the sparsity penalty by a constraint on the minimum spike size smin. The
problem
minimize
ˆc,ˆs
1
2 ∥ˆc−y∥2 subject to ˆs= Gˆc with ˆst ≥smin or ˆst = 0 (25)
is non-convex and we are not guaranteed to ﬁnd the global minimum. However, we obtain a good local
minimum by merely changing the condition to merge pools fromvi+1 <γ livito vi+1 <γ livi+smin,
modifying lines 3 and 5 in Algorithm 2.
10
Now we must choose a value for smin. In many cases we found that simply setting smin as a small
multiple of the noise level led to good results. If the scaling factor a(Eq 2) relating ﬂuorescence to
action potentials was known, we could properly normalize the spike train such thatˆst = 1 corresponds
to one spike and choose smin = 0.5, or a slightly higher value to avoid splitting one spike into two
of size 0.5. However, often the factor is unknown or difﬁcult to estimate, rendering the choice of
smin cumbersome. Analogous to the variation of λ, we can start with smin = 0 and increase it until
the RSS crosses the σ2T threshold by sequentially removing the smallest ‘spike’ and merging the
pools it used to separate. By maximizing smin under the noise constraint we minimize the number of
non-zero values of ˆs. De facto, we try to ﬁnd a parsimonious description of the data by minimizing
the number of non-zero values of ˆs, thus solving a sparsity problem with ℓ0 penalty:
minimize
ˆc,ˆs
∥ˆs∥0 subject to ˆs= Gˆc≥0 and ∥ˆc−y∥2 ≤ˆσ2T (26)
Instead of sequentially removing the smallest ‘spike’ we actually obtained the best performance by
sequentially adding spikes at the highest values of theℓ1-solution ˆsuntil the RSS is smaller than ˆσ2T.
While the updates resemble those of matching pursuit [35], in practice we found that adding spikes at
the positions suggested by the ℓ1-solution yields better results than matching pursuit (which adds
spikes at positions that greedily lead to the highest RSS reduction per step). Speciﬁcally, we found
that often matching pursuit cannot resolve spikes in close proximity, but instead results in erroneous
placement of one big spike as an explanation for all nearby spikes. Instead of merging pools we now
need to split pools. Denoting the time where to add a spike by ts, i.e. the time where the ℓ1-solution
has its highest value after ruling out times where spikes have already been added, one searches for the
pool iin which it falls, i.e. ti <ts <ti+li. Pool igets updated as l′
i = ts−ti, w′
i = ∑l′
i−1
t=0 γ2t, and
v′
i = ∑l′
i−1
t=0 yt+tiγt/w′
i, which follows directly from Eq (11) with µt = 0. All pool indices greater
than iare increased by one and a new pool is inserted after pool iwith l′
i+1 = li −l′
i, t′
i+1 = ts,
w′
i+1 = ∑l′
i+1−1
t=0 γ2t, and v′
i+1 = ∑l′
i+1−1
t=0 yt+tsγt/w′
i+1.
As is the case with all optimized hyperparameters, once we have obtained a decent estimate of smin
on an initial subset of the data we can switch back into online mode. In online mode our algorithm is
typically faster than matching pursuit, since matching pursuit requires updating O(∆) points of the
residual with each update, where ∆ is the length of the calcium transient (in number of frames).
2.4 Generalization beyond the AR(1) case
2.4.1 A greedy solution for the AR( p>1) processes
An AR(1) process models the calcium response to a spike as an instantaneous increase followed by
an exponential decay. This is a good description when the ﬂuorescence rise time constant is small
compared to the length of a time-bin, e.g. when using GCaMP6f [36] with a slow imaging rate. For
fast imaging rates and slow indicators such as GCaMP6s it is more accurate to explicitly model
the ﬁnite rise time. Typically we choose an AR(2) process, though more structured responses (e.g.
multiple decay time constants) can also be modeled with higher values for the order p.
For an AR(p) process the sparsity penalty λ∥ˆs∥1 can again be expressed as µ⊤ˆc, because
λ∥ˆs∥1 = λ1⊤ˆs= λ
T∑
t=1
T∑
k=1
Gk,tˆct = λ
T∑
t=1
(1 −
min(p,T−t)∑
i=1
γi)ˆct =
T∑
t=1
µtˆct = µ⊤ˆc, (27)
with µt := λ(1 −∑min(p,T−t)
i=1 γi), by evaluating the column sums of G. For p> 1 the dynamics
are no longer ﬁrst-order Markov and the next value depends not only on the current but on possibly
multiple previous time steps. Now following along the lines of the previous section just leads to a
greedy, approximate solution; we will present an exact algorithm later. We use matrix- and vector
notation to describe the dynamics of ct. Let the transition matrix A, multi time step calcium vectors
ζt, and vector ebe deﬁned as
A=


γ1 γ2 ... γ p
1 0 ... 0
...
. . .
. . .
...
0 ... 1 0

 ζt =


ct
ct−1
...
ct−p+1

 e=


1
0
...
0

 (28)
11
The calcium dynamics is given by ζt = Aζt−1 + ste. Analogously to the AR(1) case we derive an
algorithm that moves through the time series until it ﬁnds a violation of the constraint c′
τ ≥e⊤Aζ′
τ−1
for some time τ, updates c′
τ and c′
τ−1, and backtracks further until the updates do not violate any
constraints at previous time steps. Note that we also implicitly have constraints on ζt, enforcing the
fact that ζt+1 is mostly a time-shifted version of ζt.
Assuming we need to backtrack by ∆tsteps and introducing again t′= τ −∆t, the objective is to
minimize ∑τ
t=t′(1
2 (c′
t −yt)2 + µtc′
t) with respect to c′
t′ under the active constraints ζt = Aζt−1 for
t= t′+ 1,...,τ . Plugging in the constraints on the dynamics the objective reads
minimize
c′
t′
1
2
∆t∑
t=0
(e⊤Atζ′
t′−yt+t′)2 +
∆t∑
t=0
µt+t′e⊤Atζ′
t′. (29)
Setting the derivative with respect to c′
t′ to zero and solving for c′
t′ yields
c′
t′ =
∑∆t
t=0
(
yt+t′−µt+t′−∑p
k=2(At)1,kc′
t′+1−k
)
(At)1,1
∑∆t
t=0(At)2
1,1
(30)
where (At)2
1,1 denotes the square of the entry in the ﬁrst row and column in the matrix obtained as
t-th matrix power of A. Again, note that these entries describe the calcium kernel hwith components
h1+t = (At)1,1. Eq (30) reduces to Eq (11) for p= 1 where Ais just a 1×1-matrix with entry γ.
The next values are updated according to c′
t′+t = ∑p
k=1 γkc′
t′+t−k for t= 1,..., ∆t.
We derive again an efﬁcient formulation of the algorithm using pools. Considering the denominator
in Eq (30) as a weight in analogy to the AR(1) case and calculating the weighted sum upon merging
of pools is not valid for p> 1 because in general (At)1,1(Au)1,1 ̸= (At+u)1,1. Introducing pools is
still useful as it allows us to keep track of only a small number of pelements in each pool. While for
the case of AR(1) we only kept track of each group’s ﬁrst element, we now keep track of the ﬁrst
as well as the p−1 last elements. In order to speed up the update in Eq (30), we can precompute
the powers of Aand store (At)1,: in memory. Note that only the powers up to the maximal inter-
spike-interval are needed, which can be much smaller than T; of course, for very large values of
t, (At)1,: ≈0, by the stability of A; thus for high powers the entries of (At)1,: can also be well
approximated by a quickly computable exponential function or simply be truncated. Analogous to
the case p= 1, we can also impose a constraint on the minimum spike size smin at the expense of
having to deal with a non-convex problem by merely changing the condition to merge pools from
vi+1 < (Ali)1,1vi + (Ali)1,2ui−1 to vi+1 < (Ali)1,1vi + (Ali)1,2ui−1 + smin where vi and ui
denote the ﬁrst and last value of pool i.
According to Eq (30) the solution is a linear function of µ, and hence of λ. Thus the hard noise
constraint for the RSS ∥c−y∥2 = σ2T is a quadratic equation in λ, that can be solved analytically,
under the assumption of invariant pool structure analogous to above case of AR(1), but involving
more lengthy expressions which we state explicitly in the appendix (section S.5). Updating all pools
independently according to Eq (30) can give rise to violated constraints, requiring us to rerun the
algorithm, warm-started by initializing with the current set of pools, as described above. After 2-3
iterations no pools need to be merged and the ﬁnal solution has been found. We can again interleave
an update step for optimizing the parameters γi, as described above.
2.4.2 Online non-negative least squares (ONNLS)
We noted above that Eq (30) is not ﬁrst-order Markovian: it includes a dependency on p−1 previous
time steps and hence in general the previous pool. In updating only the ﬁrst value within a pool
and using the current values of the p−1 last values of the previous pool within the update Eq (30),
we actually performed greedy updates. These greedy updates can yield remarkably good results, in
particular for long pools, such that the last value is already well constrained by a number of data
points and hardly affected by the next pool. Nonetheless, in some cases these greedy updates lead to
errors in the timing of inferred activity, in particular when the rise time of the calcium response is
slow compared to the frame rate. The method described in this section can be used to correct these
small errors. It is again an active set method that can be run in online mode; however, the method
introduced above is a dual active set method, whereas here we describe a primal active set method.
We begin by reformulating the problem as
minimize
ˆs
1
2 ∥Kˆs−y∥2 + λ∥ˆs∥1 subject to ˆs≥0 (31)
12
Algorithm 3 Fast online deconvolution for arbitrary convolution kernels
Require: kernel h, regularization parameter λ, window size ∆, shift size ∆m, data subset yt:t+∆−1 ⊂y at
time of reading
1: initialize Kt,u ←h1+t−u for 1 ≤u≤t≤∆, y ←y −λK−⊤1, A←K⊤K, t←1
2: while t+ ∆ ≤T do
3: ˆst:t+∆−1 ←NNLS( A,K⊤yt:t+∆−1,ˆst:t+∆−1) ⊿classic NNLS on ˆst:t+∆−1, but warm-started†
4: yt:t+∆−1 ←yt:t+∆−1 −K:,1:∆m ˆst:t+∆m−1 ⊿peel off contribution of previous activity
5: t←t+ ∆m
6: ˆst:T ←NNLS( At+∆−T:∆,t+∆−T:∆,K⊤
1:T−t+1,1:T−t+1yt:T,ˆst:T) ⊿robustness to T−∆
∆m
/∈N
7: return ˆs
†The function NNLS implements a minor variation of the classic algorithm of [37] to solveminˆs∈RT
+
∥y−Kˆs∥2:
K⊤Kand K⊤y are precomputed outside the function, to exploit that NNLS is called several times with the same
K. Further ˆs is warm-started instead of initializing it as 0.
where K = G−1 is the convolution matrix with entries Kt,u = h1+t−u if t≥uelse zero; the kernel
vector hcan be taken as an arbitrary response kernel for most of the development in this section. As
noted earlier, h1+t = (At)1,1 for the special case of an AR process. As we have seen previously, the
effect of the sparsity penalty (together with the non-negative constraint) is to shift the data down by a
vector µ= λK−⊤1, and the problem reduces to a non-negative least squares (NNLS) problem.
minimize
ˆs
1
2 ∥Kˆs−(y−λK−⊤1)∥2 subject to ˆs≥0. (32)
(Note that the gradient of Eq (32) is the same as the gradient of Eq (31),K⊤(Kˆs−(y−λK−⊤1)) =
K⊤(Kˆs−y) + λ1. In addition, Kis triangular with positive numbers on the main diagonal, hence
det K >0 and Kis invertible.)
A classic algorithm for solving a NNLS problem is the active set method of [ 38] and [37]. This
algorithm alternates between normal equation matrix solves involving sub-matrices of K⊤K and
updates of the active set. A naive application of this algorithm would scale cubically with the number
of spikes. Instead, we exploit the locality of the problem (the fact that changing a spike height at
time tdoes not affect the solution at very distant times s) and apply the NNLS algorithm in the inner
loop of a sequential coordinate block descent method. Speciﬁcally, we apply warm-started NNLS
on blocks of size ∆ (where ∆ is the length of the calcium transient), stepping the block in steps of
size ∆m <∆ (we found ∆m = ∆
2 to be effective for ofﬂine applications; for online applications ∆m
would be set smaller) and applying NNLS while holding the values of soutside the block ﬁxed. We
further exploit the Toeplitz structure of Kto precompute the necessary sub-matrices of K⊤K.
The resulting algorithm (Alg 3) runs in O(T) time. It involves solving a least squares problem for the
time points within the considered window where ˆst >0; thus it scales cubically with the number of
spikes per window and depends on the sparsity of ˆs. (In fact, for AR(p) models, the required matrix
solves can be performed using linear-time (not cubic-time) Kalman ﬁlter-smoother methods, but the
matrix sizes were sufﬁciently small in the examples examined here that the Kalman implementation
was not necessary.) Further speedups can be obtained by restricting the set of possible spike times,
for example, by running the AR(1) version of OASIS on a temporally decimated version of the signal
to crudely identify the set of spike times, then never updating ˆsaway from zero on the complement
of this set.
To summarize, we describe in Algorithm 4 how the algorithmic variants introduced here are combined
into a ﬁnal full algorithm that includes hyperparameter optimization, the variants for AR(1) or AR(2),
and soft (ℓ1 penalty) or hard shrinkage (ℓ0 penalty).
3 Results
3.1 Benchmarking OASIS
We generated datasets of 20 ﬂuorescence traces each for p = 1 and 2 with a duration of 100 s at
a framerate of 30 Hz, such that T = 3,000 frames. The spiking signal came from a homogeneous
Poisson process. We used γ = 0.95, σ = 0.3 for the AR(1) model and γ1 = 1.7, γ2 = −0.712,
σ= 1 for the AR(2) model. Figures 4A-C are reassuring that our suggested (dual) active set method
13
Algorithm 4 Full algorithm with hyperparameter optimization
Require: data y, order pof the AR-process, sparsity norm q
1: initialize
2: AR parameters ˆγ1,..., ˆγp using autocorrelation of y
3: noise level ˆσusing PSD of y
4: background ˆbusing percentile of y
5: dual variable λ←0
6: ˜y ←temporally decimate batch of y ⊿for faster hyperparameter optimization
7: rescale hyperparamaters due to decimation
8: while hyperparamaters not converged do ⊿optimize hyperparameters, cf. Fig 3
9: Run warm-started Alg 2 on ˜y with current hyperparameters
10: Update hyperparameters ⊿Eqs (19,23, 24)
11: if q= 0 then determine smin ⊿Sec. ‘Hard shrinkage andℓ0 penalty’
12: rescale hyperparamaters using the inverse transformations of line 7
13: ˆc,ˆs ←run Alg 2 on full data y
14: if p= 1 then
15: return ˆs
16: else
17: ˆs ←run warm-started Alg 3 on full data y
18: return ˆs
yields indeed the same results as other convex solvers for an AR(1) process and that spikes are
extracted well. For an AR(2) process OASIS is greedy and yields good results that are similar to the
one obtained with convex solvers (lower panels in Fig 4B and C), with virtually identical denoised
ﬂuorescence traces (upper panels).
Figures 4D,E report the computation time (±SEM) averaged over all 20 traces and ten runs per trace
on a MacBook Pro with Intel Core i5 2.7 GHz CPU. We compared the run time of our algorithm
to a variety of state of the art convex solvers that can all be conveniently called from the convex
optimization toolbox CVXPY [39]: embedded conic solver (ECOS, [40]), MOSEK [41], splitting
conic solver (SCS, [42]) and GUROBI [43]. ECOS and MOSEK are the most competitive methods;
these are interior-point methods that cannot use warm starts. With a known sparsity parameter λ
(Eq 3), OASIS is about two magnitudes faster than any other method for an AR(1) process (Fig 4D,
blue disks) and more than one magnitude for an AR(2) process (Fig 4E). Whereas several of the
other solvers take almost the same time for the noise constrained problem (Eq 15, Fig 4D,E, green
x), our method takes about three times longer to ﬁnd the value of the dual variable λcompared to
the formulation where the residual is part of the objective; nevertheless it still outperforms the other
algorithms by a huge margin.
We also ran the algorithms on longer traces up to a length ofT = 300,000 frames (Fig 4F), conﬁrming
that OASIS scales linearly with T, where we obtained a proportionality constant of 1 µs/frame. For
an unknown hyperparameter λwe obtained its value not only on the full data but on an initial small
batch (1,000 frames) and kept it ﬁxed, which sped activity inference up by a factor of three once T
is sufﬁciently large (Fig 4F, orange vs green) without compromising quality (correlation between
deconvolved activity and ground truth spike train 0.882 ±0.001 vs 0.881 ±0.002 for T = 300,000).
Our active set method maintained its lead by 1-2 orders of magnitude in computing time. Further,
compared to our active set method the other algorithms required at least an order of magnitude more
RAM, conﬁrming that OASIS is not only faster but much more memory efﬁcient. Indeed, because
OASIS can run in online mode the memory footprint can be O(1), instead of O(T).
We veriﬁed these results on real data as well. Running OASIS with the hard noise constraint and
p= 2 on the GCaMP6s dataset of 14,400 frames collected at 60 Hz from [36, 44] took 0.101±0.005 s
per trace, whereas the fastest other methods required 2.37±0.12 s. Fig 4C shows the real data together
with the inferred denoised and deconvolved traces as well as the true spike times, which were obtained
by simultaneous electrophysiological recordings [36].
We also extracted each neuron’s ﬂuorescence activity using CNMF from an unpublished whole-brain
zebraﬁsh imaging dataset from the M. Ahrens lab. Running OASIS with hard noise constraint and
p= 1 (chosen because the calcium onset was fast compared to the acquisition rate of 2 Hz) on 10,000
traces out of a total of 91,478 suspected neurons took 81.5 s whereas ECOS, the fastest competitor,
14
0
2Fluor.
OASIS CVXPY Truth Data
0 25 500
1Activity
A
0
5Fluor.
0 30Time [s]
0
1Activity
0 30Time [s]
B C
OASIS ECOS MOSEK SCS GUROBI
Solver
0
0.5
1.0Time [s]
 O. E. M. S. G.
0.01
0.1
1
D
OASIS ECOS MOSEK SCS GUROBI
Solver
0
1
2Time [s]
O. E. M. S. G.
0.01
0.1
1 E
102 103 104 105
Number of frames
0
1
2
3
4Time per frame [ µs]
λ given
λ ﬁtted on full data
λ ﬁtted on small batch F
Figure 4: OASIS produces the same high quality results as convex solvers at least an order of
magnitude faster. (A) Raw and inferred traces for simulated AR(1) data, (B) simulated AR(2) and
(C) real data from [ 36] ﬁtted with an AR(2) model. OASIS solves Eq (3) exactly for AR(1) and
just approximately for AR(2) processes, nevertheless well extracting spikes. (D) Computation time
for simulated AR(1) data with given λ(blue circles, Eq 3) or inference with hard noise constraint
(green x, Eq 15). GUROBI failed on the noise constrained problem. The inset shows the same data
in logarithmic scale. (E) Computation time for simulated AR(2) data. (F) Normalized computation
time of OASIS for simulated AR(1) data with given λ(blue circles, Eq 3) and inference with hard
noise constraint on full data (green x, Eq 15) or small initial batch followed by processing in online
mode (orange crosses).
needed 2,818.1 s. For all neurons we would hence expect 745 s for OASIS, which is below the 1,500 s
recording duration (3,000 frames), and over 25,780 s for ECOS and other candidates.
OASIS solves the non-negative deconvolution problem exactly for an AR(1) process; however, as
discussed above, for p> 1 the solution is only a good (greedy) approximation. To obtain the exact
solution we ran the ONNLS algorithm on the simulated AR(2) traces using a window size of 200
frames, which was about ten times larger than the ﬂuorescence decay time, and shifting the window
by 100 frames. We obtained higher accuracy results than all the state of the art convex solvers
we compared to, requiring merely 27.8±0.4 ms per trace for λ= 0 and 20.0±0.4 ms per trace for
λ= 30, the value that ensures that the hard noise constraint is tight. The choice of λregulated the
sparsity of the solution, which affects the run time of ONNLS. The fastest state of the art convex
solver (ECOS) required 305 ±9 ms and was thus an order of magnitude slower. It took merely
8.56±0.04 ms to obtain an approximate greedy solution using OASIS, independent of the choice
of sparsity parameter λ. Though obtaining the exact solution requires more computing time, it is
well within the same order of magnitude. In contrast, running batch NNLS was signiﬁcantly slower,
requiring 2,430±53 ms for λ = 0 and 1,620±37 ms for λ = 30. Solving the noise constrained
15
problem by iterating warm-started ONNLS to obtain the corresponding value of the dual variable λ
took 73±1 ms. However, we can improve on that by ﬁrst running the fast but (forp> 1) approximate
dual method to obtain a good estimate of λas well as s, and then switching to the slower but exact
primal method. Running OASIS and executing warm-started ONNLS just once required collectively
merely 23±1 ms, similarly to cold-started ONNLS with given λ. Running ONNLS not just once, but
until the value of λhas been further tuned such that the noise constraint holds not approximately but
exactly, took altogether 31±1 ms.
3.2 Hyperparameter optimization
We have shown that we can solve Eq (3) and Eq (15) faster than interior point methods. The AR
coefﬁent γwas either known or estimated based on the autocorrelation in the above analyses. The
latter approach assumes that the spiking signal comes from a homogeneous Poisson process, which
does not generally hold for realistic data. Therefore we were interested in optimizing not only the
sparsity parameter λ, but also the AR(1) coefﬁent γ. To illustrate the optimization of both, we
generated a ﬂuorescence trace with spiking signal from an inhomogeneous Poisson process with
sinusoidal instantaneous ﬁring rate (Fig 3). We conservatively initialized ˆγto a small value of 0.9.
The value obtained based on the autocorrelation was 0.9792 and larger than the true value of 0.95.
The left panels in Figures 3B and D illustrate the update of λfrom the previous value λ−to λ∗by
solving a quadratic equation analytically (Eq 18) and the update of ˆγby numerical minimization of
a high order polynomial respectively. Note that after merely one iteration (Fig 3E) a good solution
is obtained and after three iterations the solution is virtually identical to the one obtained when the
true value of γhas been provided (Fig 3F). This holds not only visually, but also when judged by the
correlation between deconvolved activity and ground truth spike train, which was 0.869 compared
to merely 0.773 if ˆγwas obtained based on the autocorrelation. The optimization was robust to the
initial value of ˆγ, as long as it was positive and not, or only marginally, greater than the true value.
The value obtained based on the autocorrelation was considerably greater and partitioned the time
series into pools in a way that missed entire spikes.
After illustrating the hyperparameter optimization we next quantify the computing time and quality
of spike inference for various optimization scenarios. We generated 20 ﬂuorescence traces with
sinusoidal instantaneous ﬁring rate as used in the illustration (Fig 3), again having a duration of 100 s
at a framerate of 30 Hz, such that T = 3,000 frames, however we offset the data by an additional
positive baseline bthat can be present in real data. This baseline can be optimized together with
the sparsity parameter λ, as shown in Methods (subsection “Additional baseline"). The fastest
deconvolution method is to merely estimate all parameters and run OASIS just once, cf. ﬁrst row in
Table 1 which shows the mean (±SEM) for computing time as well as correlation of the inferred
spike train. As a baseline estimate we used the 15% percentile of the ﬂuorescence trace. The sparsity
penalty was set to λ= 0. A better choice of λis actually obtained by optimizing it, such that the
hard noise constraint ∥b1 + ˆc−y∥2 = ˆσ2T holds, cf. second row in Table 1. The next rows show
Table 1: Cost and quality of spike inference with parameter optimization.
optimize accelerate Time [ms] Correlation
- - 3.25 ±0.03 0 .831 ±0.006
λ - 9.2 ±0.1 0 .849 ±0.006
λ,b - 8.4 ±0.2 0 .857 ±0.007
λ,b,γ - 48.4 ±2.3 0 .875 ±0.006
λ,b,γ use 10 pools 16.0 ±0.4 0 .875 ±0.006
λ,b,γ use 5 pools 14.2 ±0.3 0 .875 ±0.006
λ,b,γ decimate 29.4 ±1.3 0 .878 ±0.006
λ,b,γ decimate, use 10 pools 12.1 ±0.2 0 .878 ±0.006
λ,b,γ decimate, use 5 pools 10.6 ±0.2 0 .877 ±0.006
The ﬁrst column shows the quantities that have been optimized, the second methods used to accelerate
the parameter optimization, the third the computing time per trace (±SEM) and the last shows the
performance of spike train inference using the correlation between inferred activity and true spike
train. We used 20 simulated ﬂuorescence traces with a spiking signal coming from an inhomogeneous
Poisson process and a duration of 100 s at a framerate of 30 Hz such thatT = 3,000 frames.
16
0
2Fluor.
L1 Thresh. Truth Data
Truth
Thresh.
L1
0 5 10Time [s]
0
0.5
1.0smin
A
B
C
0
5Fluor.
Truth
Thresh.
L1
0 5 10Time [s]
0
0.5
1.0smin
D
E
F
Figure 5: Thresholding can improve the accuracy of spike inference. (A) Inferred trace using L1
penalty (L1, blue) and the thresholded OASIS (Thresh., green). The data (gray) are simulated with
AR(1) model. (B) Inferred spiking activity. (C) The detected events using thresholded OASIS depend
on the selection of smin. The ground truth is shown in red. (D,E,F), same as (A,B,C), but the data
are simulated with AR(2).
that optimizing bfurther improves the result, as does adding γ. However, the increased number of
optimized parameters results in extra computational cost. The computation time can be reduced by
estimating γ not using the full data but only a limited number of pools, which does not affect the
quality of the result, cf. row ﬁve and six in Table 1. Note that by restricting the optimization to a
ﬁxed number of pools, its computational load does not increase with the duration of the recording,
hence the gain would be even more dramatic for longer time series. Further speed ups are obtained
by estimating the parameters on a decimated version of the data, as the last rows in Table 1 illustrate.
Here we decimated the ﬂuorescence traces by a factor of ten, without harming the inference quality.
3.3 Hard thresholding
OASIS solves a LASSO problem resulting in soft shrinkage. The deconvolved trace ˆstypically has
values smaller than 1 and often shows “partial spikes” in neighboring bins reﬂecting the uncertainty
regarding the exact position of a spike, cf. Fig 4. While this information can be useful, one sometimes
wants to merely commit to one event within a time bin instead and get rid of remaining small values
in ˆs. We ran a slightly modiﬁed version of the algorithm that replaces the sparsity penalty by a
constraint on the minimal spike size smin, yielding sparser solutions but rendering the problem
non-convex. Although we are not guaranteed to ﬁnd the global minimum, we obtained good results,
cf. Fig 5. To quantify directly the similarity between the inferred deconvolved trace and ground
truth spike train we calculated the correlation between the two. The best results were obtained for
smin = 0.5 yielding correlation 0.899 ±0.009 with the true spike train compared to 0.879 ±0.006
for the solution of the problem with hard noise constraint (Eq 15). However, in a practical application
the scaling factor between calcium ﬂuorescence and a single spike, which is 1 for our simulated data,
is often unknown, rendering it impossible to simply set the threshold smin to the half of it. Instead,
we can vary the threshold until the RSS crosses the threshold σ2T. The order in which the pools are
merged or split matters for this non-convex case and sequentially adding spikes at the highest values
of the ℓ1-solution yielded the best performance with correlation 0.888 ±0.007.
Fig 5 also shows results with a constraint on the minimal spike size for an AR(2) process. Adding the
constraint helps when pressed for a binary decision whether to assign a spike or not, yielding visually
excellent results. However, with a ﬁnite rise time of the calcium response the onset detection is
notoriously difﬁcult, because for a low threshold there are a lot of false positives due to noise, whereas
for a high threshold, closer to the peak of the calcium kernel, the onset has already occurred earlier.
Indeed, the greedy method for an AR(2) process tends to register spikes too late, which is further
exacerbated when a threshold on the spike size ( smin = 0.5) is introduced, leading to low values
17
of spike similarity (correlation 0.419 ±0.016) compared to the solution of basis pursuit denoising
(Eq 15) (correlation 0.497 ±0.013). We can incorporate a correction step that whenever a new spike
is added, slightly jitters the previous one and calculates the change in the optimization objective
in order to determine the optimal placement of the spike. For simplicity and low computational
burden, we restrict the consideration of the changing RSS to the pools prior and after the jittered spike,
which improves the spike detection (correlation 0.462 ±0.015) while only marginally increasing
computational cost (from 8.65 ms to 11.65 ms). Further improvements can be obtained by following
up with (O)NNLS. The solution obtained by OASIS with threshold on the minimal spike size and
jittering can be used to restrict (O)NNLS to have non-zero values only in close proximity to the
spikes of the greedily obtained solution. This processing step increased the performance of spike
inference to correlation 0.530 ±0.010, which is better than the already mentioned one obtained for
exactly solving the convex problem (Eq 15). Hence, though imposing a minimal spike size renders
the problem non-convex, a tractable approximate solution to this problem can improve over the exact
solution of the convex basis pursuit denoising problem.
In the AR(2) case the exact solutions (ONNLS with λor ONNLS with support only in the proximity
of the thresholded solution) consistently improved over the faster greedy methods, as measured
by spike train correlation. The performance was hardly affected by whether the penalized or the
thresholded version was chosen. Spike train correlation harshly penalizes spikes that are detected
but at an incorrect time, no matter how close; therefore the activity plots and correlation values
convey somewhat complementary information about the quality of the inference. We attribute the
performance gap between greedy and exact solutions to greedy methods missing the exact time step
more often. However, the optimally attainable time resolution is already limited by low SNR, in
particular if the rise time of the calcium indicator is ﬁnite. Indeed, being more lenient regarding the
exact spike timing we calculated the correlations after convolving the spike trains with a Gaussian
with standard deviation of one bin width. The correlation values increased to 0.731 ±0.008 for the
greedy thresholded solution and to 0.800 ±0.007 if followed up by ONNLS, but did not increase
further for wider Gaussian kernels. This indicates that in the considered SNR regime single time bin
resolution is out of reach, but spike times can be inferred with an uncertainty of about one time bin
width.
3.4 Online spike inference with limited lag
For an exact solution of the non-negative deconvolution problem of an AR(1) process OASIS needs
to backtrack to the most recent spike. (For an AR(2) process the solution is greedy and merely
approximate. ONNLS yields an exact solution in this case but considers an even wider time window.)
Such delays could be too long for some interesting closed loop experiments; therefore we were
interested in how well the method performs if backtracking is limited to just a few frames. We varied
the lag in the online estimator, i.e. the number of future samples observed before assigning a spike at
time zero, for different signal-to-noise ratios (SNR). For each lag we chose the sparsity parameter
λsuch that the noise constraint ∥ˆc−y∥2 ≤σ2T was tight. This yielded increasing values of λ
for smaller lags, compensating for the fact that limiting backtracking to fewer frames also imposes
fewer constraints (ˆct ≥γˆct−1) on the dynamics. In the case of hard thresholding, better results
were obtained with higher smin for smaller lags too, in order to avoid that one spike is split in two.
We used a hand-chosen value of smin = 0.5 + 0.175 e−τ where τ is the lag, that asymptotically
approaches the 0.5 for batch processing. The obtained results are depicted in Fig 6. For realistic SNR
(3-5, though [36] report even higher values, cf. Fig 4C) and sample rates (30 Hz), lags of 2-5 yielded
virtually the same results as ofﬂine estimation. The exact number depends on the noise; however, the
main effect of noise was to reduce the optimal performance attainable even with batch processing, as
the asymptotic values in Fig 6A and B reveal.
4 Conclusion
We presented an online active set method for spike inference from calcium imaging data. We assumed
that the forward model to generate a ﬂuorescence trace from a spike train is linear-Gaussian. Further
development will extend the method to nonlinear models [45] incorporating saturation effects and a
noise variance that increases with the mean ﬂuorescence to better resemble the Poissonian statistics
of photon counts. In the Appendix (section S.2) we already extend our mathematical formulation to
include weights for each time point as a ﬁrst step in this direction.
18
0 5 10Lag [frames]
0.6
0.8
1.0CorrelationNoise0.10.20.3
Correlation with -horizon solutionground truth
A
0 5 10Lag [frames]
0.6
0.8
1.0CorrelationNoise0.10.20.3
Correlation with -horizon solutionground truth
B
0 25 50Time [s]0
1 Inferred activity
Lag 0 1 2 5 10 20
C
Figure 6: Varied lag in the online estimator. (A,B)Performance of spike inference as function of
lag for various noise levels (i.e., inverse SNR) without (A) and with positive thresholdsmin (B). We
used correlation of the inferred spike train as similarity measure and compared to ground truth as
well as to the optimally recoverable activity when the lag is unlimited as in ofﬂine processing. (C)
Inferred trace with positive threshold smin for increasing lag using the data depicted in Fig 4A with
high noise level (σ= 0.3). The gray lines indicate the true spike times.
Our method considered spike inference as a sparse non-negative deconvolution problem. We focused
on the formulation that imposes sparsity using an ℓ1 penalty that renders the problem convex.
Using this problem formulation for spike inference has already long standing success within the
neuroscientiﬁc community. We were able to speed it up by an order of magnitude compared to
previously employed interior point methods and derived an algorithm that lends itself to online
applications. However, recently several investigators [46, 47, 48] have advocated sparser methods,
e.g. by using an ℓq-norm with q <1 instead of q = 1 [46] or by enforcing refractoriness [47] (see
also [13] for some further discussion of sparsening beyond ℓ1 penalization). They report improved
results, however in some cases at the expense of non-convexity, thus losing the guarantee of ﬁnding
the global optimum. We leave it to future work to incorporate refractoriness into the methods
developed here, but we did slightly modify the sparse non-negative deconvolution problem by adding
the constraint that positive spikes need to be larger than some minimal value. A minor modiﬁcation
to our algorithm enabled it to ﬁnd an (approximate) solution of this non-convex problem, which
can be marginally better than the solution obtained with ℓ1 regularizer. The ℓ1-penalized solution
reﬂects the uncertainty regarding the exact position of a spike by distributing it as “partial spikes”
over neighboring bins. The thresholded solution lets go of this potentially useful information and
instead commits to one event within the locally optimal time bin. We leave it up to the user which
approach to choose.
Code Availability
We provide Python and MATLAB implementations of our algorithm online (https://github.com/j-
friedrich/OASIS and linked repositories therein). The code is readily usable on new data and includes
example scripts that produce all ﬁgures and Table 1 of this article.
Here we focused on temporal data, i.e. noisy neural ﬂuorescence data that has been extracted and
demixed from raw pixel data. We further added OASIS as deconvolution subroutine to CaImAn
(https://github.com/simonsfoundation/CaImAn) [49], which implements CNMF for simultaneous
denoising, deconvolution, and demixing of spatio-temporal calcium imaging data.
Acknowledgments
We would like to thank Misha Ahrens and Yu Mu for providing whole-brain imaging data of larval
zebraﬁsh. We thank John Cunningham and Eftychios Pnevmatikakis for helpful conversations as well
as Scott Linderman and Daniel Soudry for valuable comments on the manuscript.
19
Part of this work was previously presented at the Thirtieth Annual Conference on Neural Information
Processing Systems (NIPS, 2016) [50].
Funding for this research was provided by Swiss National Science Foundation Research Award
P300P2_158428 (JF), NIH 2R01MH064537 and R90DA023426 (PZ), Simons Foundation Global
Brain Research Awards 325171 and 365002 (JF,LP), ARO MURI W911NF-12-1-0594, NIH BRAIN
Initiative R01 EB22913 and R21 EY027592, DARPA N66001-15-C-4032 (SIMPLEX), and a Google
Faculty Research award (LP); in addition, this work was supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC)
contract number D16PC00003, D16PC00008 (LP), and D16PC00007 (PZ). The funders had no role
in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial
policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.
Supplementary Material
S.1 Algorithm for isotonic regression without pooling
For ease of exposition Alg S1 shows the pseudocode of the isotonic regression algorithm used to
convey the core idea. However, this naïve implementation lacks pooling, rendering it inefﬁcient. It
repeatedly updates all values xt′,...,x τ during backtracking and calculates the updated value using
Eq (7) without exploiting that part of the sum in the numerator has already been computed as an
earlier result. It is thus merely O(T2), whereas introducing pools addresses both issues and yields an
O(T) algorithm.
Algorithm S1 Isotonic regression algorithm without pools (inefﬁcient O(T2))
Require: data y
1: initialize x ←y
2: for τ in 2,...,T do ⊿move forward until end
3: t′←τ
4: while t′>1 and xt′ <xt′−1 do ⊿track back
5: t′←t′−1
6: for iin t′,...,τ do xi ←
∑τ
t=t′yt
τ−t′+1 ⊿Eq (7)
7: return x
S.2 Weighted regression
For sake of generality we consider the case of weighted regression with weights θ.
minimize
ˆc,ˆs
1
2
∑
t
θt(ˆct −yt)2 + λ
∑
t
ˆst subject to ˆs= Gˆc≥0 (S1)
This generalization is not only of theoretical interest. These weights could be used to give lower
weight to time points with higher variance for heteroscedastic data, for example for the Poissonian
statistics of photon counts where the variance of the ﬂuorescence increases with its mean. Further,
instead of the linear relationship between ﬂuorescence and calcium concentration (Eq 2) we could
have a nonlinear observation model
yt = f(ct) + ϵt (S2)
where the nonlinear function f can include saturation effects. This is often taken to be the Hill
equation, i.e., f(c) = acn
cn+kd
+ b, with Hill coefﬁcient n, dissociation constant kd, scaling factor a
and baseline b[45]. Applying Newton’s algorithm to optimize for ˆs(or equivalently ˆc) results for
each Newton step in a weighted constrained regression problem as in Eq (S1), which can be solved
efﬁciently with OASIS. Hence, incorporating OASIS into Newton’s algorithm enables the algorithm
to handle nonlinear and non-Gaussian measurements.
20
For an AR(1) process introducing weights changes Eq (10) to
minimize
c′
t′
1
2
∆t∑
t=0
θt+t′(γtc′
t′−yt+t′)2 +
∆t∑
t=0
µt+t′γtc′
t′ (S3)
and its solution is a modiﬁcation of Eq (11) by adding the weights
c′
t′ =
∑∆t
t=0(θt+t′yt+t′−µt+t′)γt
∑∆t
t=0 θt+t′γ2t
(S4)
We merely need to initialize each pool as (vt,wt,tt,lt) = (yt −µt
θt
,θt,t, 1) for each time step tand
the updates according to Eqs (12-14) guarantee that Eq (S4) holds for all values vi = c′
ti as we prove
in the next section.
For an AR(p) process introducing weights changes Eq (30) to
c′
t′ =
∑∆t
t=0
(
θt+t′
(
yt+t′−∑p
k=2(At)1,kc′
t′+1−k
)
−µt+t′
)
(At)1,1
∑∆t
t=0 θt+t′(At)2
1,1
(S5)
and the same modiﬁed initialization holds.
S.3 Validity of updates according to equations (12-14)
Theorem 1. The updates according to Eqs (12-14) guarantee that Eqs (11, S4) hold for all values
vi = c′
ti.
Proof. We proceed by induction.
Assumption: Let for the denominator and numerator of Eq (S4) hold
wi =
li−1∑
t=0
θt+tiγ2t (S6)
and
wivi =
li−1∑
t=0
(θt+tiyt+ti −µt+ti) γt (S7)
Base case: Pools are initialized as (vt,wt,tt,lt) = (yt −µt
θt
,θt,t, 1) for each time step tsuch that
Eqs (S6, S7) hold.
Induction step: Consider two pools (vi,wi,ti,li) and (vi+1,wi+1,ti+1,li+1) that satisfy Eqs (S6,
S7) and are merged to pool (v′
i,w′
i,t′
i,l′
i) according to Eqs (12-14).
w′
i = wi + γ2liwi+1 =
li−1∑
t=0
θt+tiγ2t +
li+1−1∑
t=0
θt+ti+1γ2liγ2t
=
li+li+1−1∑
t=0
θt+tiγ2t =
l′
i−1∑
t=0
θt+t′
i
γ2t
where we used the contingency of the pools, ti+1 = ti + li. Thus after the update Eq (S6) holds for
the merged pool too. It remains to show this also for the values:
w′
iv′
i = wivi + γliwi+1vi+1
=
li−1∑
t=0
(θt+tiyt+ti −µt+ti) γt +
li+1−1∑
t=0
(
θt+ti+1yt+ti+1 −µt+ti+1
)
γliγt
=
li+li+1−1∑
t=0
(θt+tiyt+ti −µt+ti) γt =
l′
i−1∑
t=0
(
θt+t′
i
yt+t′
i
−µt+t′
i
)
γt
21
S.4 Initial calcium ﬂuorescence
Thus far we have not explicitly taken account of elevated initial calcium ﬂuorescence levels due to
previous spiking activity. For the case p= 1 positive ﬂuorescence values c1 capture initial calcium
ﬂuorescence that decays exponentially. Positive values c1 lead via s= Gcto a positive spike s1.
Instead of attributing the elevated ﬂuorescence to a spike at time t = 1, a positive s1 more likely
accounts for previous spiking activity. Therefore we remove the initial spike by settings1 = 0 (Alg 2,
line 12).
For p= 2 we can model the effect of prior spiking activity as an exponential decay, too. Because
the validity of the constraint ct ≥∑p
i=1 γict−i can only be evaluated if t > p, for p >1 the ﬁrst
pool stays thus far merely at its initialization (y1 −µ1,y1 −µ1,1,1), and the noisy raw data value
is taken as true c1. Instead, we suggest to use the ﬁrst pool to model the exponential decay due to
previous spiking activity. Given c1 = v1 the ﬂuorescence values ct for t= 1,...,l 1 are then given by
dt−1c1 with decay variable
d= 1
2 (γ1 +
√
γ2
1 + 4γ2) (S8)
as well known in the AR / linear systems literature [51]. The ﬁrst pool is merged with the second one
whenever the constraint v2 ≥dl1v1 is violated.
S.5 Explicit expressions of the hyperparameter updates for AR(2)
We solve the noise constrained problem by increasing λ in the dual formulation until the noise
constraint is tight. We start with some small λ, e.g. λ= 0, to obtain a ﬁrst partitioning into pools P.
We denote all except the differing last two components of µby µ = λ(1 −γ1 −γ2) (Eq 27) and
express the components of µas µt = µκt with
κt =



1
1−γ1−γ2
if t= T
1−γ1
1−γ1−γ2
if t= T −1
1 else.
(S9)
Given some µ(λ), the value of the ﬁrst pool used to model the initial calcium ﬂuorescence is (using
Eq 11)
ˆc1 =
∑l1
t=1(yt −µκt)dt−1
∑l1−1
t=0 d2t
(S10)
with decay factor ddeﬁned in Eq (S8). The other values in this ﬁrst pool are implicitly deﬁned by
ˆct = dˆct−1 for t= 2,...,l 1. (S11)
The values of the other pools are according to Eq (30)
ˆcti =
∑li−1
t=0 (yti+t −µκti+t −(At)1,2ˆcti−1) (At)1,1
∑li−1
t=0 (At)2
1,1
(S12)
The other values in the pool are implicitly deﬁned by
ˆcti+t = γ1ˆcti+t−1 + γ2ˆcti+t−2 for t= 1,...,l i −1. (S13)
Altogether these equations deﬁne ˆc(µ) as function of µ. The solution ˆc′= ˆc(µ′) for an updated value
µ′= µ+ ∆µis linear in ∆µ
ˆc′= ˆc−∆µf (S14)
which plugged in above Eqs (S10-S13) yields that fcan be evaluated using the following equations
by plugging in the numerical values of γ1, γ2, d, κ, Aand {li}
f1 =
∑l1
t=1 κtdt−1
∑l1−1
t=0 d2t
(S15)
ft = dft−1 for t= 2,...,l 1 (S16)
fti =
∑li−1
t=0 (κti+t −(At)1,2fti−1) (At)1,1
∑li−1
t=0 (At)2
1,1
for i= 2,...,z (S17)
fti+t = γ1fti+t−1 + γ2fti+t−2 for t= 1,...,l i −1 (S18)
22
where z is the index of the last pool and because pools are updated independently we make the
approximation that no changes in the pool structure occur. Inserting Eq (S14) into the noise constraint
(Eq 15) and denoting the residual as r= ˆc−yresults in
∥ˆc′−y∥2 = ∥ˆc−∆µf−y∥2 = ∥r−∆µf∥2 = ∥f∥2∆µ2 −2r⊤f∆µ+ ∥r∥2 !
= ˆσ2T (S19)
and solving the quadratic equation for ∆µyields
∆µ= r⊤f+
√
(r⊤f)2 −∥f∥2(∥r∥2 −ˆσ2T)
∥f∥2 . (S20)
If we jointly want to optimize the baseline too, we denote again the total shift applied to the data
(except for the last two time steps) due to baseline and sparsity penalty as φ= b+ µ. We increase φ
until the noise constraint is tight. The optimal baseline ˆbminimizes the objective (20) with respect to
it, yielding ˆb= ⟨y−ˆc⟩= 1
T
∑T
t=1(yt −ˆct). Appropriately adding ˆbto the noise constraint yields
∥ˆb′1 + ˆc′−y∥2 = ∥⟨y−ˆc+ ∆φf⟩1 + ˆc−∆φf−y∥2 (S21)
= ∥ˆb1 + ˆc−y  
r
−∆φ(f−⟨f⟩1  
¯f
)∥2 !
= ˆσ2T (S22)
where we used Eq (S14), the current value of the baseline ˆb = ⟨y−ˆc⟩and the updated value
ˆb′= ⟨y−ˆc+ ∆φf⟩. Solving the quadratic equation for ∆φyields
∆φ= (r⊤¯f+
√
(r⊤¯f)2 −∥ ¯f∥2(∥r∥2 −ˆσ2T)
∥¯f∥2 . (S23)
S.6 Supplementary videos
Video S1
The supplementary video illustrates PA V A. The pool currently under consideration is indicated by
the blue crosses. The algorithm sweeps through the time series and enforces the order constraints
x1 ≤...≤xT.
Video S2
The supplementary video illustrates OASIS for an AR(1) process. As in Figure 2, red lines depict
true spike times and the shaded background shows how the time points are gathered in pools. The
pool currently under consideration is indicated by the blue crosses. The upper panel shows how the
calcium ﬂuorescence trace c′develops while the algorithm runs, cf. Figure 2. The video additionally
shows the deconvolved trace s′= Gc′(Eq. 3) in the lower panel. The algorithm sweeps through the
time series and enforces the constraint s′≥0.
References
[1] C Grienberger and C Konnerth. Imaging calcium in neurons. Neuron, 73(5):862–885, 2012.
[2] B F Grewe, D Langer, H Kasper, B M Kampa, and F Helmchen. High-speed in vivo calcium
imaging reveals neuronal network activity with near-millisecond precision. Nat Methods,
7(5):399–405, 2010.
[3] E Yaksi and R W Friedrich. Reconstruction of ﬁring rate changes across neuronal populations
by temporally deconvolved Ca2+ imaging. Nat Methods, 3(5):377–383, 2006.
[4] T F Holekamp, D Turaga, and T E Holy. Fast three-dimensional ﬂuorescence imaging of
activity in neural populations by objective-coupled planar illumination microscopy. Neuron,
57(5):661–672, 2008.
[5] J T V ogelstein, A M Packer, T A Machado, T Sippy, B Babadi, R Yuste, and L Paninski.
Fast nonnegative deconvolution for spike train inference from population calcium imaging. J
Neurophysiol, 104(6):3691–3704, 2010.
23
[6] J T V ogelstein, B O Watson, A M Packer, R Yuste, B Jedynak, and L Paninski. Spike inference
from calcium imaging using sequential monte carlo methods. Biophys J, 97(2):636–655, 2009.
[7] E A Pnevmatikakis, J Merel, A Pakman, and L Paninski. Bayesian spike inference from calcium
imaging data. Asilomar Conference on Signals, Systems and Computers, 2013.
[8] T Deneux, A Kaszas, G Szalay, G Katona, T Lakner, A Grinvald, B Rózsa, and I Vanzetta.
Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging
of large neuronal populations in vivo. Nat Commun, 7, 2016.
[9] T Sasaki, N Takahashi, N Matsuki, and Y Ikegaya. Fast and accurate detection of action
potentials from somatic calcium ﬂuctuations. J Neurophysiol, 100(3):1668–1676, 2008.
[10] L Theis, P Berens, E Froudarakis, J Reimer, M R Rosón, T Baden, T Euler, A S Tolias,
and M Bethge. Benchmarking spike rate inference in population calcium imaging. Neuron,
90(3):471–482, 2016.
[11] Y Mishchencko, J T V ogelstein, and L Paninski. A bayesian approach for inferring neuronal
connectivity from calcium ﬂuorescent imaging data. Ann Appl Stat, pages 1229–1261, 2011.
[12] M A Picardo, J Merel, K A Katlowitz, D Vallentin, D E Okobi, S E Benezra, R C Clary, E A
Pnevmatikakis, L Paninski, and M A Long. Population-level representation of a temporal
sequence underlying song production in the zebra ﬁnch. Neuron, 90(4):866–876, 2016.
[13] E A Pnevmatikakis, D Soudry, Y Gao, T A Machado, J Merel, D Pfau, T Reardon, Y Mu,
C Laceﬁeld, W Yang, M Ahrens, R Bruno, T M Jessell, D S Peterka, R Yuste, and L Paninski.
Simultaneous denoising, deconvolution, and demixing of calcium imaging data. Neuron,
89(2):285–299, 2016.
[14] L Grosenick, J H Marshel, and K Deisseroth. Closed-loop and activity-guided optogenetic
control. Neuron, 86(1):106–139, 2015.
[15] J P Rickgauer, K Deisseroth, and D W Tank. Simultaneous cellular-resolution optical perturba-
tion and imaging of place cell ﬁring ﬁelds. Nat Neurosci, 17(12):1816–1824, 2014.
[16] A M Packer, L E Russell, H WP Dalgleish, and M Häusser. Simultaneous all-optical manip-
ulation and recording of neural circuit activity with cellular resolution in vivo. Nat Methods,
12(2):140–146, 2015.
[17] K B Clancy, A C Koralek, R M Costa, D E Feldman, and J M Carmena. V olitional modulation
of optically recorded calcium signals during neuroprosthetic learning. Nat Neurosci, 17(6):807–
809, 2014.
[18] J Lewi, R Butera, and L Paninski. Sequential optimal design of neurophysiology experiments.
Neural Comput, 21(3):619–687, 2009.
[19] M Park and J W Pillow. Bayesian active learning with localized priors for fast receptive ﬁeld
characterization. In Adv Neural Inf Process Syst, pages 2348–2356, 2012.
[20] B Shababo, B Paige, A Pakman, and L Paninski. Bayesian inference and online experimental
design for mapping neural microcircuits. In Adv Neural Inf Process Syst, pages 1304–1312,
2013.
[21] M B Ahrens, M B Orger, D N Robson, J M Li, and P J Keller. Whole-brain functional imaging
at cellular resolution using light-sheet microscopy. Nat Methods, 10(5):413–420, 2013.
[22] N Vladimirov, Y Mu, T Kawashima, D V Bennett, C-T Yang, L L Looger, P J Keller, J Freeman,
and M B Ahrens. Light-sheet functional imaging in ﬁctively behaving zebraﬁsh. Nat Methods,
2014.
[23] F A Potra and S J Wright. Interior-point methods. J Comput Appl Math, 124(1):281–302, 2000.
[24] M Ayer, H D Brunk, G M Ewing, W T Reid, and E Silverman. An empirical distribution
function for sampling with incomplete information. Ann Math Stat, 26(4):641–647, 1955.
[25] R E Barlow, D J Bartholomew, J M Bremner, and H D Brunk. Statistical inference under order
restrictions: The theory and application of isotonic regression. Wiley New York, 1972.
[26] C van Eeden. Testing and Estimating Ordered Parameters of Probability Distributions.PhD
thesis, University of Amsterdam, 1958.
[27] R E Miles. The complete amalgamation into blocks, by weighted means, of a ﬁnite set of real
numbers. Biometrika, 46(3/4):317–327, 1959.
24
[28] P Mair, K Hornik, and J de Leeuw. Isotone optimization in R: pool-adjacent-violators algorithm
(PA V A) and active set methods.J Stat Softw, 32(5):1–24, 2009.
[29] M J Best and N Chakravarti. Active set algorithms for isotonic regression; a unifying framework.
Math Prog, 47(1-3):425–439, 1990.
[30] S J Grotzinger and C Witzgall. Projections onto order simplexes. Appl Math Optim, 12(1):247–
270, 1984.
[31] K Podgorski and K Haas. Fast non-negative temporal deconvolution for laser scanning mi-
croscopy. J Biophotonics, 6(2):153–162, 2013.
[32] R P Brent. Algorithms for Minimization Without Derivatives. Courier Corporation, 1973.
[33] J Nocedal and S Wright. Numerical optimization. Springer Science & Business Media, 2006.
[34] D L Donoho. De-noising by soft-thresholding. IEEE Trans Inf Theory, 41(3):613–627, 1995.
[35] S. G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Trans
Signal Processing, 41(12):3397–3415, 1993.
[36] T-W Chen, TvJ Wardill, Y Sun, S R Pulver, S L Renninger, A Baohan, E R Schreiter, R A
Kerr, M B Orger, V Jayaraman, and L L Looger. Ultrasensitive ﬂuorescent proteins for imaging
neuronal activity. Nature, 499(7458):295–300, 2013.
[37] R Bro and S De Jong. A fast non-negativity-constrained least squares algorithm.J Chemometrics,
11(5):393–401, 1997.
[38] C L Lawson and R J Hanson. Solving least squares problems, volume 15. SIAM, 1995.
[39] S Diamond and S Boyd. CVXPY: A Python-embedded modeling language for convex opti-
mization. J Mach Learn Res, 17(83):1–5, 2016.
[40] A Domahidi, E Chu, and S Boyd. ECOS: An SOCP solver for embedded systems. In European
Control Conference (ECC), pages 3071–3076, 2013.
[41] E D Andersen and K D Andersen. The MOSEK interior point optimizer for linear programming:
an implementation of the homogeneous algorithm. In High performance optimization, pages
197–232. Springer, 2000.
[42] B O’Donoghue, E Chu, N Parikh, and S Boyd. Conic optimization via operator splitting and
homogeneous self-dual embedding. J Optim Theory Appl, pages 1–27, 2016.
[43] Gurobi Optimization Inc. Gurobi optimizer reference manual, 2015.
[44] GENIE project, Janelia Research Campus, HHMI; Karel Svoboda (contact). Simultaneous
imaging and loose-seal cell-attached electrical recordings from neurons expressing a variety of
genetically encoded calcium indicators. CRCNS.org, http://dx.doi.org/10.6080/K02R3PMN,
2015.
[45] T A Pologruto, R Yasuda, and K Svoboda. Monitoring neural activity and [Ca 2+] with
genetically encoded Ca2+ indicators. J Neurosci, 24(43):9572–9579, 2004.
[46] T Quan, X Lv, X Liu, and S Zeng. Reconstruction of burst activity from calcium imaging
of neuronal population via Lq minimization and interval screening. Biomed Opt Express,
7(6):2103–2117, 2016.
[47] E L Dyer, C Studer, J T Robinson, and R G Baraniuk. A robust and efﬁcient method to recover
neural events from noisy and corrupted data. In 6th Int IEEE/EMBS Conf Neural Eng (NER),
pages 593–596. IEEE, 2013.
[48] M Pachitariu, C Stringer, S Schröder, M Dipoppa, L F Rossi, M Carandini, and K D Harris.
Suite2p: beyond 10,000 neurons with standard two-photon microscopy. bioRxiv, 2016.
[49] A Giovannucci, J Friedrich, B Deverett, V Staneva, D Chklovskii, and E Pnevmatikakis.
CaImAn: An open source toolbox for large scale calcium imaging data analysis on standalone
machines. Cosyne Abstracts 2017, Salt Lake City USA, 2017.
[50] J Friedrich and L Paninski. Fast active set methods for online spike inference from calcium
imaging. In Adv Neural Inf Process Syst 29, pages 1984–1992, 2016.
[51] P J Brockwell and R A Davis. Time series: theory and methods. Springer Science & Business
Media, 2013.
25