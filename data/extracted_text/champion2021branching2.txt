2202
rpA
11
]IA.sc[
2v70111.1112:viXra
Branching Time Active Inference:
the theory and its generality
Th´eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Lancelot Da Costa l.da-costa@imperial.ac.uk
Imperial College London, Department of Mathematics
London SW7 2AZ, United Kingdom
Wellcome Centre for Human Neuroimaging, University College London
London, WC1N 3AR, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Over the last 10 to 15 years, active inference has helped to explain various brain mechanisms from habit
formation to dopaminergic discharge and even modelling curiosity. However, the current implementations
sufferfromanexponential(spaceandtime)complexityclasswhencomputingtheprioroverallthepossible
policiesuptothetime-horizon. Fountas et al(2020)usedMonteCarlotreesearchtoaddressthisproblem,
leading to impressive results in two different tasks. In this paper, we present an alternative framework
that aims to unify tree search and active inference by casting planning as a structure learning problem.
Two tree search algorithms are then presented. The first propagates the expected free energy forward in
time (i.e., towards the leaves), while the second propagates it backward (i.e., towards the root). Then,
we demonstrate that forward and backward propagations are related to active inference and sophisticated
inference, respectively, thereby clarifying the differences between those two planning strategies.
SubmittedtoNeuralNetworks
Champion et al.
Keywords: Active Inference,Variational Message Passing,TreeSearch,Planning,FreeEnergyPrinciple
1. Introduction
Active inference is at this point a compelling explanatory approach in cognitive neuroscience, and significant
analyses of biologically-realistic implementations in both neural and non-neural communication networks has
been conducted. More specifically, active inference extends the free energy principle to generative models with
actions (Friston et al, 2016; Da Costa et al, 2020a; Champion et al, 2021b) and can be regarded as a form of
planning as inference (Botvinick and Toussaint, 2012). This framework has successfully explained a wide range
of neuro-cognitive phenomena, such as habit formation (Friston et al, 2016), Bayesian surprise (Itti and Baldi,
2009), curiosity (Schwartenbeck et al, 2018), and dopaminergic discharges (FitzGerald et al, 2015). It has also
been applied to a variety of tasks, such as animal navigation (Fountas et al, 2020), robotic control (Pezzato et al,
2020; Sancaktar et al, 2020), the mountain car problem (C¸atal et al, 2020), the game of DOOM (Cullen et al,
2018) and the cart pole problem (Millidge, 2019). Many of those applications require planning several steps into
the future in order to be solved successfully. However, as explained in more depth in appendix H, an exhaustive
searchoverallpossiblesequencesofactionswillquicklybecomeintractable, i.e.,thenumberofsequencestoexplore
grows exponentially with the time horizon of planning. Figure 1 illustrates this exponential growth. Exploring
only a subset of this exponential number of possible sequences using a tree search therefore becomes a compelling
and quite natural alternative.
S
(12)
S
(1)
S
(11)
S
t
S
(22)
S
(2)
S
(21)
Figure 1: Illustration of all possible policies up to two time steps in the future when |U| = 2. The state at the
current time step is denoted by S . Additionally, each branch of the tree corresponds to a possible policy, and
t
each node S is indexed by a multi-index (e.g. I = (12)) representing the sequence of actions that led to this
I
state. This should make it clear that for one time step in the future, there are |U| possible policies, after two
time steps there are |U| times more policies, and so on until the time-horizon T where there are a total of |U|T
possible policies, i.e., the number of possible policies grows exponentially with the number of time steps for which
the agent tries to plan.
2
Branching Time Active Inference
But whatexactly is active inference? Imagine abasketball player at the top of the key (i.e., the area justbelow
the net) ready to take a shot. Intuitively, active inference sees the world as a collection of external states such as
the positions of the net, the player and the ball. The player (or agent) is equipped with sensors (such as the eyes)
which allow for measurements of the external states. The player is also able to perform actions in the world such
as to perform sudden eye movement or simply unfolding his (or her) arms and legs. Furthermore, it is believed
that the agent stores an internal representation of the external states, that we shall refer to as the internal states.
Importantly, theexternalandinternalstates areseparatedfromeachotherbytheMarkov blanket(Kirchhoff et al,
2018), i.e., the sensory information received and actions taken by the agent. In other words, the external states
can only modify the internal states indirectly through the observations (also called sensory information) made by
the agent, and the internal states can only modify the external states indirectly through the actions taken by the
agent.
Moreformally,activeinferencebuildsonasubfieldofBayesianstatisticscalledvariationalinference(Fox and Roberts,
2012), in which the true posterior distribution is approximated with a variational distribution. This method pro-
vides a way to balance the complexity and accuracy of the posterior distribution. The variational approach is
only tractable because some statistical dependencies are ignored during the inference process, i.e., the variational
distribution is generally assumed to fully factorise, leading to the well known mean-field approximation:
Q(X) = Q(X ), (1)
i
i
Y
where X is the set of all hidden variables of the model, X represents the i-th hidden variable, Q(X) is the
i
variational distribution (see below) approximating the posterior P(X|O) where O is the available data, and
Q(X ) is the i-th factor of the variational distribution. In 2005, Winn and Bishop (2005) presented a message-
i
based implementation of variational inference, which has naturally been called variational message passing. And
more recently, Champion et al (2021b) realised an active inference scheme using this variational message passing
procedure. By combining the Forney factor graph formalism (Forney, 2001) with the method of Winn and Bishop
(2005), itbecomespossibletocreate modularimplementations of active inference(van de Laar and de Vries,2019;
Cox et al, 2019) that allows users to define their own generative models without the burden of deriving update
equations.
However, as just stated, there is a major bottleneck to scaling up the active inference framework: the number
of action sequences grows exponentially with the time-horizon (see Appendix H for details). In the reinforcement
learning literature, this explosion is frequently handled using Monte Carlo tree search (MCTS) (Silver et al, 2016;
Browne et al,2012;Schrittwieser et al,2019). Thisapproach has beenapplied to active inferencein several papers
(Fountas et al, 2020; Maisto et al, 2021). Fountas et al (2020) chose to modify the original criterion used during
3
Champion et al.
the node selection step in MCTS. This step returns the node that needs to be expanded, and the reinforcement
learning community uses the upper confidence bound for trees (UCT) introduced by Kocsis and Szepesv´ari (2006)
as a selection criterion:
2lnn
UCT = X¯ +2C , (2)
j j p
s n j
where n is the number of times the current (parent) node has been explored; n stands for the number of times
j
the j-th child node has been explored; C > 0 is the exploration constant and X¯ is the average reward received
p j
by the j-th child, i.e., the sum of all rewards received by the current node and its descendants divided by n . The
j
child node with the largest UCT is selected. In their paper, Fountas et al (2020) replaced this selection criterion
j
by:
1
U(s,a) = −G˜(s,a)+C Q(a|s) (3)
explore
1+N(s,a)
where U(s,a) indicates the utility of selecting action a in state s; N(s,a) is the number of times that action a was
explored in state s; C is an exploration constant equivalent to C in the UCT criterion; Q(a|s) is a neural
explore p
network modelling the posterior distribution over actions, which is trained by minimizing the variational free
energy, and G˜(s,a) is an estimator of the expected free energy (EFE). The EFE is computed from the following
equation:
G(π,τ) =−E lnP(o |π) (4)
Q(θπ)Q(sτ θ,π)Q(oτ sτ,θ,π) τ
| | |
h i
+E E H(s |o ,π)−H(s |π) (5)
Q(θπ) Q(oτ θ,π) τ τ τ
| |
h i
+E H(o |s ,θ,π)−E H(o |s ,π), (6)
Q(θπ)Q(sτ θ,π) τ τ Q(sτ π) τ τ
| | |
where H(x|y) is the entropy of p(x|y). The computation of the EFE is performed by sampling from three distri-
butions whose parameters are predicted by deep neural networks, i.e., the encoder network modelling Q(s ), the
τ
decoder network modelling P(o |s ) and the transition network modelling P(s |s ,a ). Note that Equation
τ τ τ τ 1 τ 1
− −
(2)was developed byKocsis and Szepesv´ari(2006)as acriterion forselecting nodesduringplanning,such thatthe
selected node minimizes the agent’s regret (c.f. Appendix G for additional details). Equation (3) finds its origin
in the Predictor Upper Confidence Bound (PUCB) algorithm introduced by Rosin (2010). The idea of the PUCB
algorithm is to use contextual information to predict the node to select during planning. Equations (2) and (3)
both aim to select the node that minimizes the agent’s regret, and can therefore be used interchangeably. How-
ever, Equation (3) requires contextual information and a model predicting the node to be selected. Fountas et al
(2020) proposed to use the neural network modelling Q(a|s) as a predictor. This has the advantage of making the
predictor very flexible, sinceneuralnetworks areknown to begeneral function approximators, butneuralnetworks
are also expensive to train and lack interpretability.
4
Branching Time Active Inference
To avoid the additional complexity brought by the predictor, this paper makes use of (2), which arises from
the multi-armed bandit literature (Auer et al, 2002). The idea is to minimise the agent’s regret to handle the
trade-off between exploration and exploitation at the tree-level in an optimal manner.
A major novelty of our paper is to think about tree search as a dynamical expansion of the generative model,
where the past and present is modelled as a partially observable Markov decision process (Sondik, 1971) and the
future is modelled by a tree-like generative model. Importantly, our agent treats future states and observations as
latent variables over which posterior beliefs are computed, and those beliefs encode the uncertainty of our agent
over future states. In contrast, Fountas et al (2020) are using a maximum a posteriori (MAP) estimate of the
future hidden states, while performing MCTS. Lastly, the posterior beliefs held by our agent are computed using
variational message passing as presented in (Champion et al, 2021b). In comparison, Fountas et al (2020) perform
amortized inference using an encoder network that predicts the mean and variance of the posterior distribution
over latent states. Then, (during planning) a MAP estimate is used as input for the neural network modelling the
temporal transition. All those neural networks are trained using gradient descent on the variational free energy.
Overall, thekey contribution of ourpaperis to useMCTSto expandor grow theprobabilistic graphical model,
treatfuturestates andobservationsaslatent variables, anddoinferenceusingvariational messagepassing. Indeed,
the definition in (Champion et al, 2021b) of a general message passing procedure for performing active inference
makes it possible to construct graphical active inference models in a modular fashion. In turn, this makes it
possible to incrementally expand an active inference model as required of our MCTS procedure. It is this message
passing procedure that makes our approach possible. To our knowledge, an approach of this kind has never been
studied before.
Inthefollowing, wefirstprovidetherequisitebackgroundconcerningForney factor graphs,variational message
passing, active inference, and Monte Carlo tree search in Sections 2, 3, 4, and 5, respectively. Next, Section 6
introducesourmethodthatframesplanningusingatreeasaformofBayesian modelextension. Usingterminology
from concurrency theory (Bowman, 2005), we call our new formalism Branching Time Active Inference (BTAI).
In this domain, models of systems based upon sequences of actions (the format of policies) are described as
linear time, while models based upon tree and even graph structures are called branching time (Glabbeek, 1990;
van Glabbeek, 1993; Bowman, 2005). Importantly, BTAI does not consider the generative model and the tree
as two different objects, instead, BTAI merges those two objects together into a generative model that can be
dynamically expanded. For a detailed analysis of the properties of BTAI, the reader is referred to our companion
paper (Champion et al, 2021a), which provides an empirical demonstration of the benefits of BTAI over standard
active inference (AcI) in the context of a graph navigation task. This companion paper also supplies a theoretical
comparison of BTAI and standard AcI based upon a complexity class analysis. Briefly, standard AcI has a space
complexity class of O(|π|× T ×|S|), where |π| = |U|T is the number of possible policies, |U| is the number of
5
Champion et al.
available actions, T is the time horizon of planning, and |S| is the number of values that the hidden state can
take. In contrast, the space complexity class of BTAI is O([K +t]×|S|), where t is the current (i.e. present)
time point, and K is the number of expansions of the tree performed during planning. Importantly, even complex
applications such as the game of Go can be solved by expanding only a small number of nodes (Silver et al, 2016;
Schrittwieser et al, 2019). Section 6 is followed by Section 7 that explains the connection between our method
and the planning strategies used in both active inference and sophisticated inference (Friston et al, 2021). Finally,
Section 8 concludes this paper and provides ideas for future research.
2. Forney Factor Graphs
A Forney factor graph (Forney, 2001) uses three kinds of nodes. The nodes representing hidden and observed
variables are depicted by white and gray circles, respectively. And the distribution’s factors are represented using
white squares, which are linked to variable nodes by arrows or lines. Arrows are used to connect factors to their
target variable, while lines link factors to their predictors. Figure 2 shows an example of a Forney factor graph
corresponding to the following generative model:
P(O,S) = P (O|S)P (S). (7)
O S
Generally, factor graphs only describe the model’s structure such as the variables and their dependencies, but
do not specify the definition of individual factors. For example, the definitions of P and P are not given by
O S
Figure 2, and additional information is required to remove the ambiguity, e.g., P (S) = N(S;µ,σ) clarifies that
S
P is a Gaussian distribution.
S
PS
Hidden variable
Line
S
Factor
Arrow
PO Observed variable
O
Figure 2: This figure illustrates the Forney factor graph corresponding to the following generative model:
P(O,S) = P (O|S)P (S). The hidden state is represented by a white circle with the variable’s name at the
O S
center, and the observed variable is depicted similarly but with a gray background. The factors of the generative
model are represented by squares with a white background and the factor’s name at the center. Finally, arrows
connect the factors to their target variable and lines link each factor to its predictor variables.
6
Branching Time Active Inference
3. Variational Message Passing
We now build on Forney factor graphs and provide an overview of the method of Winn and Bishop (2005). For
more details, see Champion et al (2021b), which provided a complete derivation of the equations presented below
from Bayes’ theorem.
3.1 Winn and Bishop method
Variational message passing as developed by Winn and Bishop (2005) is an approach for inference based upon the
mean-field approximation, which assumes that the posterior fully factorises, i.e.
Q(X) = Q(X ), (8)
i
i
Y
where X is the set of all hidden variables of the model and X represents the i-th hidden variable. In this section,
i
we focus on the intuition behind the method, starting with the update equation of an arbitrary hidden state x :
k
lnQ (x ) =hlnP(x |pa )i + hlnP(c |x ,cp )i +C (9)
∗k k k k
∼
Qk j k kj
∼
Qk
cjX∈ chk
where C is a normalizing constant, and h·i is the expectation over all factors but Q (x ). (9) tells us that the
∼
Qk k k
optimal posterior of any hidden states x only depends on its Markov blanket, i.e., x ’s parents pa , children ch
k k k k
and co-parents cp . To make (9) more specific, we assume that each random variable of the model is conjugate
kj
to its parents (i.e., the posterior has the same functional form as the prior) and is distributed according to a
distribution in the exponential family, i.e.,
lnP(x |pa ) =µ (pa )·u (x )+h (x )+z (pa ) (10)
k k k k k k k k k k
where µ (pa ), u (x ), h (x ) and z (pa ) are the parameters, the sufficient statistics, the underlying measure
k k k k k k k k
and the log partition, respectively. Under those two assumptions, (9) can be re-written as:
Q (x ) = exp µ ·u (x )+h (x )+Const (11)
∗k k ∗k k k k k
( )
µ = µ˜ ({hu (i)i } )+ µ˜ (hu (c )i ,{hu (l)i } ) (12)
∗k k i Qi i
∈
pa
k
j
→
k j j Qj l Ql l
∈
cp
kj
cjX∈ chk
7
Champion et al.
whereµ˜ is a re-parameterization of µ (pa )in terms of the expectation of the sufficient statistics of the parents of
k k k
x , and similarly µ˜ is a re-parameterization of µ . Importantly, u (x ) and h (x ) in the optimal posterior
k j k j k k k k k
→ →
(11) are the same as in the prior (10), and only the parameters have changed according to (12).
m1 m2 m3 m4
PZ Z PY Y PX W PW
m5
X
m1 m5 m4
∗
µ Y = µ˜ Y (hu Z (Z)i QZ )+µ˜ X→Y (hu X (X)i QX ,hu W (W)i QW )
m2 m3
Figure 3: This figure illustrates the computation of the optimal posterior parameters as a message passing proce-
dure, which requires the transmission of messages from the parent (m ) and child (m ) factors. Additionally, the
2 3
message from the child factor (m ) requires the computation of messages from the co-parent (m ) and child (m )
3 4 5
variables. Also, the message from the parent factor (m ) requires the computation of a message (m ) from the
2 1
parent variable.
To understand the intuition behind (12), let us suppose that we are given the Forney factor graph illustrated
in Figure 3 and we wish to compute the posterior of Y. Then, the only parent of Y is Z, the only child of Y
is X and the only co-parent of Y with respect to X is W. Therefore, applying (12) to our example leads to the
equation presented in Figure 3 whose components can be interpreted as messages. Indeed, each variable (i.e., X,
Z and W) sends the expectation of its sufficient statistics (i.e., a message) to the square node in the direction of
Y (i.e., either P or P ). Those messages are then combined using a function (i.e., either µ˜ or µ˜ ) whose
X Y Y X Y
→
output (i.e., another set of messages) are summed to obtain the optimal parameters µ . The computation of the
∗Y
optimal parameters (12) can then be understood as a message passing procedure. Also, we provide in Appendix
C a concrete instance of the approach presented above.
4. Active Inference
This section provides a quick overview of the active inference framework, and Appendix H presents a description
of the exponential complexity class that it exhibits. The reader is referred to Appendix F for any notations that
might not be explained here. For a more detailed treatment of the active inference framework, we refer the reader
to (Champion et al, 2021b; Da Costa et al, 2020a; Smith et al, 2021).
8
Branching Time Active Inference
4.1 Generative model
AsillustratedinFigure4,theclassicgenerativemodelrepresentstheworldasasequenceofhiddenstatesgenerating
observations through the matrix A. The prior over the initial states is defined by the vector D and the transition
between time steps is encoded by a 3-tensor B, i.e., one matrix per action. Importantly, the random variable π
represents all possible policies up to a given time horizon T and each policy is defined as a sequence of actions,
i.e., {U ,...,U } where U ∈ {1,...,|U|} ∀τ ∈ {t,...,T − 1}. The prior over the policies is then set such that
t T 1 τ
−
policies with high probability minimise the EFE, which is defined as follows (Parr and Friston, 2019):
expected outcomes priorpreferences
T
G(π) ≈ D [ Q(O |π) || P(O ) ] + E [H[P(O |S )]] (13)
KL τ τ Q(Sτ π) τ τ
" | #
τ=t+1
X z }| {risk z }| { ambiguity
| {z } | {z }
where Q(O |π) =∆ P(O |S )Q(S |π), H[·] is the Shannon entropy, G is a vector containing as many elements
τ Sτ τ τ τ
asthenumberofpPolicies,andthei-thelementofGrepresentsthecostofthei-thpolicy. Thepriorpreferencesover
observationsP(O )representthe(categorical)distributionthattheagentwantsitsobservationstobesampledfrom
τ
and is traditionally encoded by the vector C. Note that this generalises the concept of reward from reinforcement
learning. Indeed, maximising reward can bereformulated as sampling observations from a Dirac delta distribution
over reward maximising states (Da Costa et al, 2020b).
Pγ
γ
PD PB Pπ
D B π
PS0
S0 ... PSt S t ... PST S T
PA
A
PO0 POt
O0 O
t
Figure 4: This figure illustrates the Forney factor graph of the entire generative model presented by Friston et al
(2016). The probability of the initial states is defined by the vector D, and the matrix A defines the probability
of the observations given the hidden states. The B matrices define the transition between any successive pair of
hidden states. This transition depends on the action performed by the agent, i.e., on the policy π. Furthermore,
the prior over the policies has been chosen such that policies minimizing expected free energy are more probable.
Finally, the precision parameter γ (which modulates the confidence over which policies to pursue) is distributed
according to a gamma distribution.
9
Champion et al.
Lastly, the precision parameter γ has been associated to neuromodulators such as dopamine (FitzGerald et al,
2015; Friston et al, 2013) and can be understood as modulating the confidence over the information afforded by
theexpected freeenergy—e.g., smaller values of γ lead to morestochastic decision-making. Finally, theframework
allows A, B and D to be learned by introducing Dirichlet distributions over the columns of these tensors such
that the posterior parameters of A, B and D can be reused in a new trial, as parameters of the prior, giving an
empirical prior. Finally, the classic generative model is defined as follows:
P(O ,S ,π,A,B,D,γ) = P(π|γ)P(γ)P(A)P(B)P(S |D)P(D)
0:t 0:T 0
t T
P(O |S ,A) P(S |S ,π ,B) (14)
τ τ τ τ 1 τ 1
− −
τ=0 τ=1
Y Y
P(π|γ) = σ(−γG) P(γ) = Γ(1,β)
P(A) = Dir(a) P(B) = Dir(b)
P(S |D) = Cat(D) P(D) = Dir(d)
0
P(O |S ,A) = Cat(A) P(S |S ,π ,B) = Cat(B),
τ τ τ τ 1 τ 1
− −
where G is a vector of size |π| whose i-th element corresponds to the expected free energy of the i-th policy, σ(•) is
thesoftmaxfunction,Γ(•),Cat(•)andDir(•)standforagamma,categorical andDirichletdistribution,respectively,
π ∈ {1,...,|U|}istheactionprescribedbypolicyπ attimeτ−1,O isthesetof(randomvariablesrepresenting)
τ 1 0:t
−
observationsbetweentimestep0andt,andS isthesetof(randomvariablesrepresenting)hiddenstatesbetween
0:T
time step 0 and T.
4.2 Variational Distribution
The most widely used variational distribution (Da Costa et al, 2020a; Friston et al, 2016) is not fully factorized,
i.e., the posterior models the influence of the policy on the hidden states, leading to the following factorization:
T
Q(S ,π,A,B,D,γ) = Q(π)Q(A)Q(B)Q(D)Q(γ) Q(S |π) (15)
0:T τ
τ=0
Y
Q(S |π) = Cat(Dˆ ) Q(π) = Cat(πˆ)
τ τ
Q(γ) = Γ(1,βˆ) Q(D) = Dir(dˆ)
Q(A) = Dir(aˆ) Q(B) = Dir(ˆb)
where all variables with a hat correspond to posterior parameters. Notice that the distributions over A, B and D
remain Dirichlet distributions,and the distributionsover γ andS remain a gamma and a categorical distribution,
τ
10
Branching Time Active Inference
respectively. Only the distribution over π changes from a Boltzmann to a categorical distribution but both are
discrete distributions.
Remark 1 By definition the generative model P(O ,S ,π,A,B,D,γ) is a joint probability distribution over
0:t 0:T
both the observed (O ) and latent (S ,π,A,B,D,γ) variables. However, the goal of the variational distribution
0:t 0:T
Q(S ,π,A,B,D,γ) is to approximate the true posterior P(S ,π,A,B,D,γ|O ), which is a distribution over
0:T 0:T 0:t
the latent variables only. Thus, the approximate posterior Q(S ,π,A,B,D,γ) is also a distribution over the
0:T
latent variables only, and does not contain the observed variables.
4.3 Variational Free Energy
By definition, the variational free energy (VFE) is the Kullback-Leibler divergence between the variational distri-
bution and the generative model, i.e.
F = E [lnQ(S ,π,A,B,D,γ)−lnP(O ,S ,π,A,B,D,γ)] (16)
Q 0:T 0:t 0:T
= D [Q(x)||P(x|o)] −lnP(o) (17)
KL
= D [Q(x)||P(x)]−E [lnP(o|x)] (18)
KL Q(x)
complexity accuracy
| {z } | {z }
where x = {S ,π,A,B,D,γ} refers to the model’s hidden variables, and o = {O } refers to the sequence of
0:T 0:t
observationsmadebytheagent. (17)showsthatminimisingfreeenergyinvolvesmovingthevariationaldistribution
Q(x) closer to the true posterior P(x|o) in the sense of KL divergence, and that the variational free energy is an
upper bound on the negative log evidence. (18) shows the trade-off between complexity and accuracy, where the
complexity penalises the divergence of the posterior Q(x) from the prior P(x) and the accuracy scores how likely
the observations are given the generative model and current belief of the hidden states.
To fit the variational distribution as closely as possible to the true posterior, the VFE is minimized w.r.t
each variational factor, e.g., Q(D) and Q(A). The minimization process can be solved by iterating the update
equations of each factor until convergence of the VFE. More details and intuition about those updates are given
by Champion et al (2021b).
4.4 Action selection
In active inference, the simplest strategy to select actions is to compute the evidence for all policies under con-
sideration and then choose the most likely action according to these policies. Mathematically, this amounts to a
11
Champion et al.
Bayesian model average by executing the action with the highest posterior evidence:
π
| |
u = argmax [u = πm]Q(π = m) (19)
∗t t
u
m=1
X
where |π| is the number of policies, πm is the action predicted at the current time step by the m−th policy, and
t
[u = πm] is an indicator function that equals one if u = πm and zero otherwise.
t t
5. Monte Carlo Tree Search
By now, the reader should be familiar with the framework of active inference and how variational message passing
combined with the Forney factor graph formalism can be used to compute posterior beliefs. We now turn to the
last piece of background required to present the method proposed in this paper: Monte Carlo tree search (MCTS),
which is based on the multi-armed bandit literature (c.f. Appendix G for details).
5.1 A four step process
Monte Carlo tree search has been widely used in the reinforcement learning literature as it enables agents to plan
efficiently when the evaluation of every possible action sequence is computationally prohibitive (Silver et al, 2016;
Browne et al,2012;Schrittwieser et al,2019;Fountas et al,2020). Thisalgorithmessentially buildsatreeinwhich
each node corresponds to a future state and each edge represents the action that led to that state. Initially, the
tree is only composed of a root node corresponding to the current state. From here, MCTS is a four step process.
First, a node is selected according to a criterion such as the upper confidence bound for trees (UCT):
2lnn
UCT = X¯ +2C , (20)
j j p
s n j
where n is the number of times the current (parent) node has been explored, n stands for the number of times
j
the j-th child node has been explored, C > 0 is the exploration constant and X¯ is the average reward received
p j
by the j-th child. Note, if the rewards are in [0,1], then C = 1 is known to satisfy the Hoeffding inequality
p √2
(Browne et al, 2012) and the UCT criterion reduces to:
lnn
UCT = X¯ +2 . (21)
j j
n
s j
Importantly, theUCT aims to explore highly rewardingpaths (exploitation in firstterm), whilealso visiting rarely
explored regions (exploration in second term).
As shown in Figure 5, this criterion is first used at the root level leading to the selection of a node from the
root’s children. Then, it is used at the level of the root’s children, and so on until a leaf node is reached. As
12
Branching Time Active Inference
explained by Kocsis and Szepesv´ari (2006), UCT is a direct application of the UCB1 criterion to trees, where at
each level, the allocation strategy must pick a node that is expected to lead to the highest reward, and “picking
the i-th node” can beseen as the i-th action of a multi-armed bandit problem. Once a leaf nodehas been selected,
an expansion step is performed by sampling an action from a distribution and adding the node corresponding to
this action as a child of the leaf node, i.e., the leaf node is expanded.
The third step consists of performing virtual rollouts into the future to estimate the average future reward
obtained from the state corresponding to the newly expanded node. Finally, during the back-propagation step,
the average reward obtained from the newly expanded state is used to re-evaluate the average quality of all its
ancestors, and the visit counts of all nodes (in the branch explored) are increased. Iterating this four-step process
until the time budget has been spent gives a fairly good estimate of the best action to perform next. Figure 5
summarises theMCTS procedure. Inthe nextsection, we presentour approach and show how MCTS can befused
to active inference by performing a dynamical expansion of the generative model.
Selection Expansion Simulation Back-propagation
St St St St
±1
S (1) S (2) S (1) S (2) S (1) S (2) S (1) S (2)
±1
S (11) S (12) S (11) S (12) S (11) S (12) S (11) S (12)
±1
S (121) S (121) S (121)
±1
Figure5: ThisfigureillustratestheMCTSalgorithmasafourstepprocess. First,westartatthenoderepresenting
the current state S and select a node based on the UCT criterion until a leaf node is reached. Second, the tree
t
is expanded to a new node by taking a virtual action from the selected node. Third, the value of this action is
estimated by simulating the expected reward following that action. In the simplest version of MCTS, simulations
are run until a terminal state is reached, e.g., until the game ends in Go or Chess. Fourth, the expected value is
back-propagated to the new node and all of its ancestor nodes. The multi-indices in curly brackets denote action
sequences taken from the root node, indicating the current state of the environment.
6. Branching Time Active Inference (BTAI)
In this section, we present a novel active inference agent that frames planning using a tree as a form of Bayesian
modelextension. Usingterminologyfromconcurrencytheory(Bowman,2005),wecallournewformalism,Branch-
ing Time Active Inference (BTAI). In this domain, models of systems based upon sequences of actions (the format
of policies) aredescribed as linear time, whilemodels based upontree and even graphstructures are called branch-
13
Champion et al.
ing time (Glabbeek, 1990; van Glabbeek, 1993; Bowman, 2005). Importantly, we do not consider the generative
model and the tree as two different objects. Instead, we merge those two objects together into a generative model
that can be dynamically expanded.
Figure 6 illustrates an example of such a model, where for the sake of simplicity, we assume that the matrices
A, B and D are given to the agent. Furthermore, the random variable representing the policies has been replaced
by random variables representing actions and the precision parameter γ has been removed, which is a common
designchoice(Fountas et al,2020). Additionally, wefollow Parr and Friston(2019)byviewingfutureobservations
as latent random variables. Finally, note that the transition between two consecutive hidden states in the future
(S
Ilast
and S
I
where I is a multi-index) will only depend on the matrix B¯
I
= B¯(•,•,I
last
), i.e., the matrix
\
corresponding to action I that led to the transition from S to S . The reader is referred to Table 1 for the
last Ilast I
\
definition of B¯ and more details about multi-indices can be found in Appendix F.
6.1 Prior, Posterior and Target distributions
Since the generative model is fairly different from the standard model, we state here its formal definition:
t
P(O
0:t
,S
0:t
,U
0:t 1
,OI
t
,SI
t
,A,B,D,Θ
0:t 1
) = P(S
0
|D)P(A)P(B)P(D) P(O
τ
|S
τ
,A)
− −
τ=0
Y
t 1 t
−
P(U |Θ )P(Θ ) P(S |S ,U ,B) P(O |S )P(S |S ) (22)
τ τ τ τ τ 1 τ 1 I I I Ilast
− − \
τ Y =0 τ Y =1 I Y∈ I t
where I is the set of all non-empty multi-indices already expanded by the tree search from the current state S ,
t t
the second product (τ from 0 to t−1) models the uncertainty over action, reflecting the focus on actions rather
than policies, and S is the parent state of S . Intuitively, the product over all I ∈ I models the future, while
Ilast I t
\
the rest of the above equation models the past and present. Additionally, we need to define the individual factors:
P(S |D) = Cat(D) P(U |Θ ) = Cat(Θ )
0 τ τ τ
P(O |S ,A) = Cat(A) P(O |S )= Cat(A¯)
τ τ I I
P(S |S ,U ,B) = Cat(B) P(S |S ) =Cat(B¯ )
τ τ 1 τ 1 I Ilast I
− − \
P(D) = Dir(d) P(Θ ) =Dir(θ )
τ τ
P(A) = Dir(a) P(B) = Dir(b)
where A¯ and B¯ are defined in Table 1, B¯
I
= B¯(•,•,I
last
) is the matrix corresponding to I
last
and I
last
is the last
index of the multi-index I, i.e., the last action that led to S . Importantly, A¯ and B¯ should not be confused with
I
A˚and B˚, A¯ is the expectation of A w.r.t. Q(A), while A˚is the expectation of the logarithm of A w.r.t. Q(A).
14
Branching Time Active Inference
We now turn to the definition of the variational posterior. Under the mean-field approximation:
Q(S
0:t
,U
0:t 1
,OI
t
,SI
t
,A,B,D,Θ
0:t 1
) =
− −
t 1 t (23)
−
Q(A)Q(B)Q(D) Q(U )Q(Θ ) Q(S ) Q(O )Q(S )
τ τ τ I I
τ Y =0 τ Y =0 I Y∈ I t
where the individual factors are defined as:
Q(S ) = Cat(Dˆ ) Q(U )= Cat(Θˆ )
τ τ τ τ
Q(O )= Cat(Eˆ ) Q(S ) = Cat(Dˆ )
I I I I
Q(D) = Dir(dˆ) Q(Θ ) = Dir(θˆ )
τ τ
Q(A) = Dir(aˆ) Q(B) = Dir(ˆb),
where Dˆ , Θˆ , Eˆ , Dˆ , dˆ, θˆ , aˆ and ˆb are the parameters of the factors Q(S ), Q(U ), Q(O ), Q(S ), Q(D),
τ τ I I τ τ τ I I
Q(Θ ),Q(A)andQ(B),respectively. Importantly,O appearsinthevariationaldistributionbecauseobservations
τ I
in the future are treated as hidden variables.
Finally, wefollow Millidge et al(2021)inassumingthattheagent aims tominimisetheKLdivergence between
theapproximateposteriordepictingthestateoftheenvironmentandatarget(desired)distribution. Therefore,our
framework allows for the specification of prior preferences over both future hidden states and future observations:
V(OI
t
,SI
t
) = V(O
I
)V(S
I
) (24)
I Y∈ I t
where the individual factors are defined as:
V(O )= Cat(C ), V(S ) = Cat(C ). (25)
I O I S
Importantly, by specifying the value of futureobservations and states, C and C play a similar role to the vector
O S
C in active inference, i.e., they specify which observations and hidden states are rewarding.
To sum up, this framework is defined using three distributions: the prior defines the agent’s beliefs before
sampling any observation; the posterior is an updated version of the prior which takes into account past observa-
tions made by the agent; finally, the target distribution encodes the agent’s prior preferences in terms of future
observations and hidden states.
15
Champion et al.
PS0
S O
0 PO0 0
U
PU0 0 PS...
S O
... PO... ...
U
PU... ... PSt
S O
t POt t
PS(1) PS(2)
O S S O
(1) PO(1) (1) (2) PO(2) (2)
PS(11) PS(12) PS(22)
O S S S
(11) PO(11) (11) (12) (22)
Figure 6: This figure illustrates the new expandable generative model allowing planning under active inference.
The future is now a tree like generative model whose branches correspond to the policies considered by the agent.
As we will see, these branches can be dynamically expanded during planning. Here, the nodes in light gray
represent possible expansions of the current generative model. For the sake of clarity, the random tensor A, B,
Θ and D are not illustrated, i.e., Dirichlet priors over those random tensors are not shown.
τ
6.2 Bayesian belief updates
In this section, we focus on the set of update equations used to perform approximate Bayesian inference. These
update equations rely on variational message passing as presented in Section 3, see Champion et al (2021b) as
well as Winn and Bishop (2005) for details. A key strength of the message passing approach is the capacity to
derive and implement these updates within an automatic and modular toolbox (van de Laar and de Vries, 2019;
Cox et al, 2019), which in a way similar to automatic differentiation alleviates the final user from the burden of
manually deriving complex update equations for each new generative model. To simplify our notation, we use two
operators ⊗ and ⊙ that we call generalized outer and inner product, respectively. The generalized outer product
creates an N dimensional tensor from N vectors, while the generalized inner product performs a weighted average
over one dimension of an N dimensional array, cf Appendix A for details. Using these notations, the first set of
16
Branching Time Active Inference
update equations are given by:
t
Q (D) = Dir dˆ where dˆ= d +Dˆ (26)
∗ 0
τ
(cid:0) (cid:1) X
t
Q (A) = Dir aˆ where aˆ = a+ ⊗ Dˆ ,o (27)
∗ τ τ
(cid:0) (cid:1) X τ=0 h i
t
Q (B) = Dir ˆb where ˆb = b+ ⊗ Dˆ ,Θˆ ,Dˆ (28)
∗ τ 1 τ 1 τ
− −
(cid:0) (cid:1) X τ=1 h i
Q (Θ ) = Dir θˆ where θˆ = θ +Θˆ (29)
∗ τ τ τ τ τ
(cid:0) (cid:1)
where o is the observation made at time τ. Furthermore, this first set of equations count (probabilistically) the
τ
number of times, an initial hidden state has been observed, an action has been performed, a state has generated a
particular observation or an action has led to the transition between two consecutive hidden states. For example,
the posterior parameters aˆ are computed by adding t ⊗[Dˆ ,o ] (i.e., the number of times a state-observation
τ=0 τ τ
pair has been observed during this trial) to the prioPr parameters a (i.e., the number of times this same pair has
been observed during previous trials). The equations for belief updates are given by:
t
Q (O )= σ A˚⊙Dˆ (30)
∗ I I
τ
(cid:0) (cid:1)X
t
Q (S )= σ A˚⊙Eˆ +B˚ ⊙Dˆ + B˚ ⊙Dˆ (31)
∗ I I I Ilast J J
\
(cid:16) J X∈ chI (cid:17)X τ
t
Q (U )= σ Θ˚+B˚⊙[Dˆ ,Dˆ ] (32)
∗ τ τ τ+1
τ
(cid:0) (cid:1)X
t
Q (S ) = σ [τ = 0]D˚ + [τ 6= 0]B˚⊙[Dˆ ,Θˆ ]
∗ τ τ τ 1 τ 1
− −
(cid:16) t X
+A˚⊙o
τ
τ
X
+[τ = t] B˚ ⊙Dˆ + [τ 6= t]B˚⊙[Dˆ ,Θˆ ] (33)
J J τ+1 τ
J X∈ cht (cid:17)
whereσ(•)isthesoftmaxfunction,ch
t
arethechildren(states) ofthecurrentstates S
t
,ch
I
arethechildren(states)
of the states S , [predicate] is an indicator function returning one if the predicate is true and zero otherwise, and
I
the definition of A˚, B˚, D˚and Θ˚ are given in Table 1. Note that thanks to the operators ⊗ and ⊙, the perception
τ
(i.e., state-estimation) equations can be intuitively understood as a sum of messages, where each message from
a factor to a variable is the average over all dimensions except the dimension of the variable, e.g., the message
(A˚⊙o ) from P to S is the vector obtained by weigthing the rows of A˚by the elements of o . Importantly,
0 o0 0 0
the above update equations are almost identical to the ones used in standard active inference, and thus can
17
Champion et al.
be implemented efficiently. Indeed, most of the computation required is about addition of matrices O(n2) and
multiplication of matrices O(n3), or their higher dimensional counterparts.
Notation Meaning
hf(X)i =∆ E [f(X)] The expectation of f(X) over P
PX PX X
ψ(•) The digamma function
Θ˚ (i) = hlnΘ (i)i = ψ θˆ (i) −ψ θˆ (k) The expected logarithm of Θ
τ τ QΘτ τ k τ τ
D˚(i) = hlnD(i)i QD = ψ(cid:0)dˆ(i) (cid:1)−ψ (cid:0)Pk dˆ(k) (cid:1) The expected logarithm of D
A˚(i,j) = hlnA(i,j)i
QA
= ψ(cid:0)aˆ(i,j(cid:1)) −(cid:0)ψP
k
aˆ(k(cid:1),j) The expected logarithm of A
B˚(i,j,u) = hlnB(i,j,u)i
QB
= ψ(cid:0) ˆb(i,j,(cid:1)u) −(cid:0)ψP
k
ˆb(k,(cid:1)j,u) The expected logarithm of B
A¯(i,j) = hA(i,j)i(cid:0)
QA
=
P
aˆ (cid:1) (
aˆ
i,
(
j
k
)
,j)
(cid:0)P (cid:1) The expectation of A
k
ˆ
B¯(i,j,u) = hB(i,j,u)i = b(i,j,u) The expectation of B
QB
P
ˆ
b(k,j,u)
k
Table 1: Update equations notation. Note that Appendix D provides a proof for D.
6.3 Planning as structure learning
In this section, we frame planning as a form of structure learning where the structure of the generative model is
modified dynamically. This method is greatly inspired by the Monte Carlo tree search literature, c.f., Section 5
for details.
6.3.1 Selection of the node to be expanded
The first step of planning is to select a node to be expanded. The selection process starts at the root node, if the
root node still has unexplored children, then one of them is selected. Otherwise, the child node maximizing the
UCT criterion, where the average reward is replaced by minus the average EFE, is selected, i.e., the selected node
maximises:
lnn
UCT = −g¯ + C , (34)
J J p
n
r J
exploitation
exploration
|{z}
| {z }
where J is a multi-index, n is the number of times the root node has been visited, n is the number of times the
J
child corresponding to the multi-index J was selected, and g¯ is the average cost received when selecting the child
J
S . The UCT criterion can be understood as a trade-off between exploitation and exploration at the tree level,
J
which is different to the exploitation and exploration dilemma at the model level. This dilemma is handled by
the EFE. Also, the notion of cost in the above equation can be defined in many ways and will be the subject of
Section 6.3.3. For our purposes, the cost will be equal, or similar, to the expected free energy, which means that
18
Branching Time Active Inference
the expected free energy drives structure learning. When a root’s child is selected, it becomes the new root in the
above procedure, which is iterated until a leaf node is reached.
6.3.2 Dynamical expansion of the generative model
Let S denotes the leaf node selected for expansion. When S has been selected, the structure of the
Ilast Ilast
\ \
generative model needs to be modified by expanding all possible actions from that node. For each action, we
expand the generative model by adding a future hidden state whose prior distribution is given by
P(S |S )= Cat(B¯ ), (35)
I Ilast I
\
whereB¯ is thematrix correspondingto thelast action that led to S . Finally, we expandthe(future)observation
I I
associated with the new hidden state S , whose distribution is:
I
P(O |S ) = Cat(A¯). (36)
I I
To sum up, the expansion step is adding two random variables (S and O ) to the generative model, i.e. the
I I
generative model becomes bigger, and I is added to the set of all non-empty multi-indices already expanded by
the tree search (I ). The prior distributions over those newly added random variables (i.e. S and O ) are defined
t I I
using the matrices B¯ and A¯, which effectively predict the future states and observations. After the expansion
I
step, the posterior distribution over S and O needs to be computed. At least two kinds of inference strategies
I I
can be used. The first—global inference—performs variational message passing over the entire generative model,
while the second—local inference—only iterates the update equations of the newly expanded nodes, i.e., S and
I
O , until convergence to the variational free energy minimum.
I
6.3.3 Cost evaluation of the expanded nodes
After expanding the model structure, we need to compute the cost of the newly expanded node S . As explained
I
in Section 6.3.1, the cost of S will influence the probability of expanding S during future planning iterations. In
I I
active inference, the classic objective of planning is the expected free energy as defined in Section 4.1, i.e.,
gclassic =∆ D [Q(O )||V(O )] + E [H[P(O |S )]] (37)
I KL I I Q(SI) I I
19
Champion et al.
where gclassic trades off risk (first summand) and ambiguity (second summand). Alternatively, one could follow
I
Section 5 of Millidge et al (2021) and define the cost of S using the free energy of the expected future:
I
feef
g = D [Q(O ,S )||V(O ,S )] (38)
I KL I I I I
where V(O ,S ) is the target distribution over states and observations. The target distribution V(O ,S ) gen-
I I I I
eralises the C matrix in Friston’s model by specifying prior preferences over both future observations and fu-
ture states. Also, this formulation of the cost speaks to the notion of KL divergence minimization proposed by
Hafner et al (2020).
Furthermore, due to the mean-field approximation of the posterior (23) and the factorised form of the target
distribution (24), the expression of the cost simplifies to
g pcost =∆ D [Q(S )||V(S )]+D [Q(O )||V(O )]. (39)
I KL I I KL I I
pcost
Intuitively, the pure cost (g ) measures how different the predicted future hidden states and observations
I
are from desired states and observations. In future research, it might be interesting to compare the performance
and behaviour of g pcost , g feef and gclassic empirically and theoretically.
I I I
pcost
Note that in MCTS, the evaluation of a node’s quality is done by performing virtual roll-outs, while g ,
I
g feef and gclassic are not. If we let g be any of those criteria, then we can improve our estimate of the cost, by
I I I
20
Branching Time Active Inference
average
computing g , i.e., the average cost over N roll-outs of size K. Algorithm 1 presents the pseudo code used
I
average
to estimate g .
I
average
Algorithm 1: Estimation of g
I
Input: N the number of virtual roll-outs, K the maximal length of each roll-out.
average
g ← 0 ; // Initialize roll-out estimate to zero
I
repeat N times
grollout ← g ; // Initial cost equals cost of S
I I I
for i ← 1 to K do
sample a random action U uniformly from the set of unexplored actions;
i
perform the expansion of the current node using U (Section 6.3.2);
i
perform inference on the newly expanded nodes (Section 6.2);
grollout ← grollout+g ; // J corresponds to last expanded node
I I J
end
g average ← g average +grollout;
I I I
end
average average
g ← g /N;
I I
6.3.4 Propagation of the node cost
aggr
In this section, we let G be a variable that contains the total cost of the node S , where L could be any
L L
pcost feef
multi-index. According to the previous section, we let g be any of the following evaluation criteria g , g
L L L
and gclassic. Initially, Gaggr equals g . Also, we let S be the node that was selected for expansion, and let S be
L L L K I
an arbitrary hidden state expanded from S . The cost of the newly expanded node(s) can be propagated either
K
forward or backward. The forward propagation (towards the leaves) leads to the following equation:
Gaggr
← g
+Gaggr
, (40)
I I K
where here
Gaggr
is the aggregated cost of the parent of S . Importantly, the symbol ← refers to a programming-
K I
likeassignment(i.e., anincremental update)performedeach timethetreeisexpanded. Thebackward propagation
(towards the root) leads to:
Gaggr ← Gaggr +g ∀J ∈ A (41)
J J I I
whereA correspondstoall ancestors ofthenewlyexpandednodeS . WewillseeinSection 7thatthesestrategies
I I
respectively relate to active inference and sophisticated inference (Friston et al, 2021). Finally, since the agent is
21
Champion et al.
free to choose any action, we can back-propagate the (locally) minimum cost, i.e.,
Gaggr ← Gaggr + min g ∀J ∈ A , (42)
J J K::a I
a 1,...,U
∈{ | |}
where K ::a is a multi-index obtained from K by adding the action a to the sequence of actions described by K.
In all cases, the propagation step updates the counter n associated with each ancestor S of the newly expanded
J J
hidden state S ; this counts the number of times the node S has been explored (exactly as in MCTS). This
I J
counter will be used for action selection, as well as for the computation of the average cost of S —g¯ —that was
J J
left undefined by Section 6.3.1. Formally, g¯ is given by:
J
1
g¯ =
Gaggr
. (43)
J n J
J
Remark 2 The forward propagation of the cost presented above will only be used for theoretical purpose in Section
7. Practical implementation of BTAI should use the backward schemes.
6.4 Action selection
The planning procedure presented in the previous section ends after a pre-specified amount of time has elapsed or
when a sufficiently good policy has been found. When the planning is over, the agent needs to choose an action
to act in its environment. In a companion paper (Champion et al, 2021a) that presents empirical results of BTAI,
g
the actions are sampled from σ(−γ ), where σ(•) is a softmax function, γ is a precision parameter, g is a vector
N
whose elements correspond to the cost of the root’s children and N is a vector whose elements correspond to the
number of visits of the root’s children. Importantly, actions with low average cost are more likely to be selected
than actions with high average cost.
Alternative approaches to action selection (Browne et al, 2012) could be studied. For example, one could
imagine sampling actions from a categorical distribution with parameter σ(N), where N is a vector containing
the n of all children of the root node. Or, we could select the action corresponding to the root’s child with the
J
highest number of explorations n . The fact that it has been visited more often means that is has a lower cost
J
overall. If there were a tie between several actions, the action with the lowest cost would be selected. The study
of these strategies is left to future research.
6.5 Action-perception cycle with tree search
Inactiveinference,theaction-perceptioncyclerealisesanactiveinferenceagentinaninfiniteloop(van de Laar and de Vries,
2019). Each loop iteration begins with the agent sampling an observation from the environment. The observation
is used to perform inference about the states and contingencies of the world, e.g., an impression on the retina
22
Branching Time Active Inference
might beused to reconstruct athree dimensional scene with a representation of theobjects that it contains. Then,
planning is performed by inferring the consequences of alternative action sequences. Importantly, only a subset of
all possible action sequences are evaluated, due to the dynamical expansion of the generative model. Finally, the
agent selects an action to perform in the environment by sampling a softmax function of minus the average cost
weighted by the precision parameter γ, i.e., σ(−γ g ). Therefore, actions with low average cost are more likely to
N
be selected than actions with high average cost. We summarise our method using pseudo-code in Algorithm 2.
Algorithm 2: Action-perception cycle with tree search
while end of trial not reached do
sample an observation from the environment;
perform inference using the observation (Section 6.2);
while maximum planning iteration not reached do
select a node to be expanded (Section 6.3.1);
perform the expansion of the node (Section 6.3.2);
perform inference on the newly expanded nodes (Section 6.2);
evaluate the cost of the newly expanded nodes (Section 6.3.3);
propagate the cost of the nodes through the tree, either forward or backward (Section 6.3.4);
end
select an action to be performed (Section 6.4);
execute the action in the environment leading to a new observation;
end
7. Connection between BTAI, active inference and sophisticated inference
In this section, we explore the relationship between BTAI, active inference (AcI) and sophisticated inference (SI).
We show that BTAI is a class of algorithms that generalizes AcI and is related to SI. To do so, we focus on the
“cost” of a policy for each method. In addition, we need to introduce the notion of localized and aggregated cost.
The localized cost of a node S , denoted Glocal, is the cost of S in and of itself, i.e., without any consideration of
I I I
the cost of past or future states. The aggregated cost of a node S , denoted
Gaggre
, is the cost of S when taking
I I I
into account either the cost of future states that can be reached from S (which is the case in SI) or the cost of
I
the past states that an agent has to go through in order to reach S (which is the case in AcI).
I
7.1 Active inference
The full framework of active inference was described in Section 4. This section focuses on expressing the expected
free energy in a recursive form that highlights the relationship between BTAI and AcI. We start by defining the
23
Champion et al.
notion of localized and aggregated EFE with Definitions 3 and 4, respectively. Then, we show that in active
inference (under some assumptions described below), the aggregated EFE of a policy of size N is given by the
aggregated EFE of a policy of size N −1 plus the localized EFE received at time t+N.
In active inference, a policy is a sequence of actions π = (U ,U ,...,U ), where T is the time horizon of
t t+1 T 1
−
planning, andfor convenience, π denotes apolicy of sizeN, obtained by selecting thefirstN actions of thepolicy
N
π, i.e., π = (U ,U ,...,U ) with N ≤ T −t. Recall from Section 4, that (in active inference) the expected
N t t+1 t+N 1
−
free energy of a policy is given by:
T T
G(π) = G(π,τ) = D [Q(O |π)||P(O )] + E [H[P(O |S )]] . (44)
KL τ τ Q(Sτ π) τ τ
" | #
τ=t+1 τ=t+1
X X
If instead of letting τ range from t+1 to T, we let N range from 1 to T −t, then Equation 44 can be re-written
as:
T t T t
− −
G(π) = G(π,t+N)= D [Q(O |π)||P(O )] + E [H[P(O |S )]] . (45)
KL t+N t+N Q(St+N π) t+N t+N
" | #
N=1 N=1
X X
Additionally, under the assumption that the probability of observations and states are independent of future
actions, i.e., that ∀j ∈ N ,Q(O |π ) ≈ Q(O |π ) and ∀j ∈ N ,Q(S |π ) ≈ Q(S |π ), π can be
>0 t+i i t+i i+j >0 t+i i t+i i+j
replaced by π in the RHS of the above equation, leading to:
N
T t T t
− −
G(π) = G(π ,t+N) = D [Q(O |π )||P(O )] + E [H[P(O |S )]] . (46)
N KL t+N N t+N Q(St+N πN) t+N t+N
" | #
N=1 N=1
X X
Importantly, the elements of the above summation constitute the localized cost presented in Definition 3.
Definition 3 We define the localized cost received at time t+N after selecting policy π as:
N
Glocal = G(π ,t+N)= D [Q(O |π )||P(O )] + E [H[P(O |S )]]. (47)
πN N KL t+N N t+N Q(St+N
|
πN) t+N t+N
Importantly, the localized cost quantifies the amount of risk and ambiguity received by the agent at time step
t+N, assuming that it will follow the policy π . We now turn to the notion of aggregated cost of a policy of size
N
N. Definition 4 states that the aggregated cost of a policy is defined recursively. Indeed, by definition, a policy of
size zero has an aggregated cost of zero, and then, the aggregated cost of a policy π (of size N) is equal to the
N
the aggregated cost of π (of size N −1) plus the localized cost received at time t+N.
N 1
−
24
Branching Time Active Inference
Definition 4 We define the aggregated cost of a policy π of size N as:
N
0 if N = 0
Gaggre = . (48)
πN 
  Ga π g N g − r 1 e +Gl π o N cal otherwise


Equipped with Definitions 3 and 4, we are now ready to state and prove Theorem 5 using the two Lemmas of
Appendix E.
Theorem 5 Undertheassumption that theprobability of observationsandstatesare independent offutureactions,
i.e., ∀j ∈ N ,Q(O |π ) ≈ Q(O |π ) and ∀j ∈ N ,Q(S |π ) ≈ Q(S |π ), the expected free energy can
>0 t+i i t+i i+j >0 t+i i t+i i+j
be written as:
G(π ) ≈ Gaggre = Gaggre+Glocal. (49)
N πN πN−1 πN
Proof Thisproofis basedon twolemmas demonstratedinAppendixE.Note thatinactive inferencetheexpected
free energy is defined as:
T
G(π)= G(π,τ). (50)
τ=t+1
X
Let N denote the size of the policy π, i.e. N = T −t. Note that because π is of size N, then by definition π = π ,
N
and the above equation can be re-written as:
t+N
G(π) = G(π )= G(π ,τ). (51)
N N
τ=t+1
X
Expanding the summation and using Definition 3:
t+N 1
−
G(π ) = G(π ,τ)+G(π ,t+N) (52)
N N N
τ=t+1
X
t+N 1
−
= G(π ,τ)+Glocal. (53)
N πN
τ=t+1
X
If, instead of letting τ range from t+1 to t+N −1, we let i range from 1 to N −1, then the above equation can
be re-written as:
N 1
−
G(π )= G(π ,t+i)+Glocal. (54)
N N πN
i=1
X
25
Champion et al.
Note that ∀i ∈ {1,...,N −1},N > i, and thus there exists a k ∈ N such that ∀i ∈ {1,...,N −1},N = i+k .
i >0 i
Therefore, we replace N by i+k in the above summation:
i
N 1
−
G(π ) = G(π ,t+i)+Glocal. (55)
N i+ki πN
i=1
X
Lemma 11 tells us that under the assumption that the probability of observations and states are independent of
future actions, ∀k ∈N ,G(π ,t+i) ≈ G(π ,t+i), which allows us to remove the k to get:
i >0 i+ki i i
N 1
−
G(π ) ≈ G(π ,t+i)+Glocal. (56)
N i πN
i=1
X
Finally, Lemma 12 states that i N =−1 1G(π i ,t+i) = G a π g N g − r 1 e , and thus:
P
G(π ) ≈ Gaggre+Glocal =∆ Gaggre. (57)
N πN−1 πN πN
The above equation will be used in Section 7.4 to show that BTAI generalizes active inference.
7.2 Sophisticated inference
Sophisticated inference (Friston et al, 2021) is a new type of active inference that defines the EFE recursively
from the time horizon backward. Intuitively, the agent does not simply ask “what would happen if I did that”,
but instead wonders “what would I believe about what would happen if I did that”. In other words, the agent is
exhibiting a form of sophistication, which refers to the fact of having beliefs about one’s own or another’s beliefs.
Friston et al (2021) also replaced variational message passing by an alternative inference scheme called Bayesian
Filtering (Fox et al, 2003). While the change of inference method is of little relevance to us here, the recursive
definition of the EFE is at the core of this section. As explained in Section 4.3 of Da Costa et al (2020b), the
(recursive) EFE of a Markov decision process is given by:
G(U ,S )= D [Q(S |U ,S )||V(S )] (58)
T 1 T 1 KL T T 1 T 1 T
− − − −
G(U ,S )= D [Q(S |U ,S )||V(S )]+E [G(U ,S )] (59)
τ τ KL τ+1 τ τ τ+1 Q(Uτ+1,Sτ+1Uτ,Sτ) τ+1 τ+1
|
26
Branching Time Active Inference
where U and S are the action and state at time τ, and V(S ) is the target (i.e., desired) distribution over states
τ τ τ
at time τ. Using our terminology of localized and aggregated cost, this can be rewritten as:
G(U ,S ) = D [Q(S |U ,S )||V(S )] (60)
T 1 T 1 KL T T 1 T 1 T
− − − −
Gaggre(UT−1,ST−1) Glocal(UT−1,ST−1)
|
G
{
(U
z
,S )
}
= D
|
[Q(S |U
{
,
z
S )||V(S )]+
}E
[ G(U ,S ) ] (61)
τ τ KL τ+1 τ τ τ+1 Q(Uτ+1,Sτ+1Uτ,Sτ) τ+1 τ+1
|
Gaggre(Uτ,Sτ) Glocal(Uτ,Sτ) Gaggre(Uτ+1,Sτ+1)
| {z } | {z } | {z }
Put simply, the aggregated cost of taking action U in state S can be computed by summing the localized cost
τ τ
at time step τ and the expected aggregated cost at time step τ +1, i.e.,
Gaggre(U ,S )= Glocal(U ,S )+E [Gaggre(U ,S )] (62)
τ τ τ τ Q(Uτ+1,Sτ+1Uτ,Sτ) τ+1 τ+1
|
Note that for τ = T −1 the second term vanishes because future states beyond the temporal horizon are ignored,
and thus Gaggre(U ,S ) = Glocal(U ,S ). Also, the above equation will be useful in Section 7.5 to show
T 1 T 1 T 1 T 1
− − − −
that BTAI is related to sophisticated inference.
Remark 6 The recursive aspect of Equation 59 isdeeply related to dynamic programming and the interested reader
is referred to Da Costa et al (2020b) for details about this relationship.
7.3 Branching Time Active Inference (BTAI)
In BTAI, the (localized) cost of the hidden state S is defined as Glocal = g , where g can be equal to gclassic,
I I I I I
feef pcost
g or g , and there are two ways of computing the aggregated cost of S . We can either propagate the
I I I
localized cost towards the leaves (forward):
g ← g +g (63)
I I Ilast
\
where the g is the cost of the parent of S . Alternatively, we can back-propagate the cost towards the root
Ilast I
\
g ← g +g ∀J ∈ A, (64)
J J I
where A corresponds to all ancestors of the newly expanded node S .
I
27
Champion et al.
7.4 BTAI as a generalisation of active inference
To understand the relationship between BTAI and active inference, we need to focus on the forward propagation
of the cost where the cost is given by gclassic. Recall that the update for forward propagation is given by:
I
gclassic ← gclassic+gclassic, (65)
I I Ilast
\
where the g is the cost of the parent of S , i.e., the parent of the newly expanded node. This equation tells
Ilast I
\
us that the aggregated cost of S is equal to the localized cost of S plus the aggregated cost of S , i.e.,
I I Ilast
\
gclassic ← gclassic+gclassic ⇔ Gaggre = Glocal+Gaggre , (66)
I I Ilast I I Ilast
\ \
Ga
I
ggre Gl
I
ocal Ga
I\
g
la
gr
st
e
| {z } | {z } | {z }
but, then, we also recall (49), i.e.,
Gaggre ≈ Gaggre+Glocal ⇔ Gaggre ≈ Glocal+Gaggre. (67)
πN πN−1 πN πN πN πN−1
The only difference between Equations 49 and 66 is notational. Indeed, in BTAI (Eq. 49) a policy is represented
by a multi-index denoting the sequence of actions selected, e.g., I = (1,2) corresponds to a policy of size two
consisting of action one followed by action two. In contrast, in active inference, a policy is a sequence of actions,
e.g., π = (1,2) corresponds to the same policy as the one described by I.
2
7.5 Relationship between BTAI and sophisticated inference
The relationship between BTAI and sophisticated inference is slightly more involved. The backward propagation
equation, i.e.,
g ← g +g ∀J ∈ A, (68)
J J I
tells us that when expanding a node S , we first need to compute its localized cost g and then add g to the
I I I
aggregated cost of its ancestors S where J ∈ A. In other words, we can rewrite the backward propagation
J
equation as the following: the aggregated cost of an arbitrary node S will equal the sum of its localized cost g
J J
and that of its descendants D that have already been evaluated
J
Gaggre = Glocal+ Glocal, (69)
J J K
SKX∈ D J
28
Branching Time Active Inference
where the descendants are the children, children of children, etc. We can further simplify this expression by
grouping the summands by children of S . This leads us to:
J
Gaggre = Glocal+ Glocal+ Glocal (70)
J J  I K 
SIX∈ chJ SKX∈ D I
 
Gaggre
I
= Glocal+ | Gaggre , {z } (71)
J I
SIX∈ chJ
where ch are the children of S and D are the descendants of S . The above equation has clear similarities to
J J I I
Equation (62), which is,
Gaggre(U ,S ) = Glocal(U ,S )+E [Gaggre(U ,S )]. (72)
τ τ τ τ Q(Uτ+1,Sτ+1Uτ,Sτ) τ+1 τ+1
|
However, the second term of the RHS of (62) is an expectation, while the second term of the RHS of (70) is a
summation over the children that have already been expanded. The expectation in (62) is w.r.t.
Q(U ,S |U ,S ) = Q(U |S )Q(S |U ,S ), (73)
τ+1 τ+1 τ τ τ+1 τ+1 τ+1 τ τ
where:
Q(U |S )> 0 ⇔ U ∈ argminsGaggre(U,S ), (74)
τ+1 τ+1 τ+1 τ+1
U U
∈
where U is the set of all possible actions, and argmins is defined as:
M = argminsf(U)⇔ M = U ∈ U f(U)= min f(U ) . (75)
′
U U U′ U
∈ n (cid:12) ∈ o
(cid:12)
This means that an action is assigned positive probability mass if and only if it minimises the aggregated cost
at the next time point Gaggre(U ,S ) and a set is required, because multiple actions could have the same
τ+1 τ+1
minimum cost. Note that if there is a unique minimum, then Q(U |S ) will be a one-hot like distribution
τ+1 τ+1
with a probability of one for the best action.
To conclude, Equations (62) and (70) suggest that BTAI and SI share a similar notion of EFE, where the
immediate (or localized) EFE is added to the future (or aggregated) EFE. Both BTAI and SI propagate the cost
backward, however, in SI the aggregated EFE (i.e., the back-propagated cost) is weighted by the probability of the
next action and states, i.e., Q(U ,S |U ,S ). Intuitively, the weighting terms in SI discounts the impact of
τ+1 τ+1 τ τ
the back-propagated cost for unlikely states and (locally) sub-optimal actions. Importantly, those weighting terms
29
Champion et al.
emerge from the recursive definition of the EFE that relates to the Bellman equation (Da Costa et al, 2020b). In
contrast, there are no such weights in BTAI because BTAI finds its inspiration in active inference.
8. Conclusion and future works
In this paper, we have presented a new approach where planning is cast as structure learning. Simply put, this
approach consists of dynamically expanding branches of the generative model by evaluating alternative futures
underdifferentaction sequences. Thedynamicexpansiontradesoffevaluatingpromising(withrepecttothetarget
distribution) policies with exploring policies whose outcomes are uncertain. We proposed two different tree search
methods: the first in which the nodes’ cost is propagated forward from the root node to the leaves; the second in
which the nodes’ cost is propagated backward from the leaves to the root node. Then, in Section 7 we showed
that forward propagation of the EFE leads to active inference (AcI) under the assumption that the probability of
observations and states are independent of future actions, and that backward propagation relates to sophisticated
inference (SI). This clarifies the link between AcI and BTAI, and helps to understand the relationship between
AcI and SI.
Importantly, by performingacomplexity class analysis, we have shown that whileActive Inferencesuffersfrom
an exponential complexity class, our approach scales nicely (linearly) with the number of tree expansions, c.f.,
Section 3.4.2 of our companion paper(Champion et al, 2021a). Of course, the total numberof possibleexpansions
growsexponentiallybutashasbeenempiricallyshowninthereinforcementlearningliterature, evencomplextasks,
such as chess or the game of go, can be performed efficiently with MCTS (Silver et al, 2016; Schrittwieser et al,
2019), i.e., the efficiency of MCTS-like approaches relies on the ability to guide the expansion procedure using
either powerful heuristics (like the EFE) or neural networks or both.
We also know that humans engage in counterfactual reasoning (Rafetseder et al, 2013), which, in our planning
context, could involve the consideration and evaluation of alternative (non-selected) sequences of decisions. It
may be that, because of the more exhaustive representation of possible trajectories, the classic active inference
can more efficiently engage in counterfactual reasoning. In contrast, branching-time active inference would require
these alternatives to be generated “a fresh” for each counterfactual deliberation. In this sense, one might argue
that there is a trade-off: branching-time active inference provides considerably more efficient planning to attain
current goals, classic active inference provides a more exhaustive assessment of paths not taken.
Now that we have laid out the mathematics of BTAI, many directions of research could be investigated. One
could for example obtain an intuitive understanding of the model’s parameters through experimental study. At
first it might be necessary to restrict oneself to agents without learning, i.e., inference only. This step should help
answer questions such as: How does the number of expansions of the tree and the quality of the prior preferences
30
Branching Time Active Inference
impact the quality of planning? What is the best inference method (i.e., local or global inference) to use during
planning?
Then, one could consider learning of the transition and likelihood matrices as well as the vector of initial
states. This can be done in at least two ways. The first is to add Dirichlet priors over those matrices/vectors
and the second would be to use neural networks as function approximators. The second option will lead to a
deep active inference agent (Sancaktar and Lanillos, 2020; Millidge, 2020) equipped with tree search that could be
directly compared to the method of Fountas et al (2020). Including deep neural networks in the framework will
also enable direct comparison with the deep reinforcement learning literature (Haarnoja et al, 2018; Mnih et al,
2013; van Hasselt et al, 2016; Lample and Chaplot, 2017; Silver et al, 2016). These comparisons will enable the
impact of epistemic terms to be studied when the agent is composed of deep neural networks.
Another, very important direction for future research would be the creation of a biologically plausible imple-
mentation of BTAI. For example, using artificial neural networks to model the various mappings of the framework
may provide a neural-based implementation of BTAI that is closer to biology. This would especially be the case,
if the back-propagation algorithm frequently used for learning is replaced by the (more biologically plausible)
generalized recirculation algorithm (O’Reilly, 1996). Another possible approach would be to use populations of
neurons to encode the update equations of the framework, as was proposed by Friston et al (2017).
Whatever technique is chosen for learning and inference, implementing MCTS in a biologically plausible way
will be challenging. Indeed, MCTS requires a dynamic expansion of the search tree used to explore the space of
possible policies. Each time an expansion is performed, the agent needs to store the associated variables such as:
the number of visits, the aggregated expected free energy, and the posterior beliefs of the newly expanded node.
Given thefastpace atwhich planningmustbeperformedto beuseful,slow mechanisms such as synapticplasticity
and neurogenesis are likely to be unsuitable for the task. A more plausible approach might rely upon a change of
neuronal activation, which can occur within a few hundred milliseconds. One such approach uses a binding pool
(Bowman and Wyble, 2007) and provides a notion of variable. In this framework, a variable is composed of two
parts. First, a token that can be intuitively understood as the variable’s name, and second, a type corresponding
to the variable’s value. The binding pool is then composed of neurons representing the fact that a variable’s name
is bound (or set) to a specific value. A localist realisation of a binding pool could be implemented as a 2D array of
neurons of size “number of tokens” × “number of types”. However, such a representation is quite inefficient and
a more compact (i.e. distributed) representation has been developed (Wyble and Bowman, 2006). If variables are
complex data structures, such as those required by BTAI, they can be realised in a neural substrate.
Finally, in this paper, we focused on the UCT criterion for node selection because it is a standard choice in
the reinforcement learning literature (Silver et al, 2016; Schrittwieser et al, 2019). However, it would be valuable
to consider alternative criteria such as Thompson Sampling (Thompson, 1933; Russo et al, 2018; Sajid et al,
31
Champion et al.
2021) or expected improvement (Brochu et al, 2010; Bergstra et al, 2011). For example, Thompson Sampling has
been shown to improve upon the standard UCT criterion when applied to MCTS (Bai et al, 2013), but requires
additional modelling and we leave this for future research.
Acknowledgments
We would like to thank the reviewers for their valuable feedback, which greatly improved the quality of the
present paper. LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875).
Thispublication is based on work partially supportedby the EPSRCCentre for Doctoral Trainingin Mathematics
of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1).
References
AuerP,Cesa-BianchiN,FischerP(2002)Finite-timeanalysisofthemultiarmedbanditproblem.MachineLearning
47(2):235–256, DOI 10.1023/A:1013689704352, URL https://doi.org/10.1023/A:1013689704352
Bai A, Wu F, ChenX (2013) Bayesian mixturemodelling and inferencebased Thompsonsampling in Monte-Carlo
tree search. In: Proceedings of the Advances in Neural Information Processing Systems (NIPS), Lake Tahoe,
United States, pp 1646–1654
Bergstra J, Bardenet R, Bengio Y, K´egl B (2011) Algorithms for hyper-parameter optimiza-
tion. In: Shawe-Taylor J, Zemel R, Bartlett P, Pereira F, Weinberger KQ (eds) Ad-
vances in Neural Information Processing Systems, Curran Associates, Inc., vol 24, URL
https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf
Botvinick M, Toussaint M (2012) Planning as inference. Trends in Cognitive Sciences 16(10):485 – 488, DOI
https://doi.org/10.1016/j.tics.2012.08.006
Bowman H (2005) Concurrency Theory: Calculi an Automata for Modelling Untimed and Timed Concurrent
Systems. Springer, Dordrecht, URL https://cds.cern.ch/record/1250124
BowmanH,WybleB(2007)Thesimultaneoustype,serialtokenmodeloftemporalattentionandworkingmemory.
Psychological Review 114(1):38–70
Brochu E, Cora VM, de Freitas N (2010) A tutorial on Bayesian optimization of expensive cost functions, with
application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:10122599
32
Branching Time Active Inference
Browne CB, Powley E, Whitehouse D, Lucas SM, Cowling PI, Rohlfshagen P, Tavener S, Perez D, Samothrakis S,
ColtonS(2012)AsurveyofMonteCarlotreesearchmethods.IEEETransactionsonComputationalIntelligence
and AI in Games 4(1):1–43
C¸atal O, Verbelen T, Nauta J, Boom CD, Dhoedt B (2020) Learning perception and planning with deep ac-
tive inference. In: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP
2020, Barcelona, Spain, May 4-8, 2020, IEEE, pp 3952–3956, DOI 10.1109/ICASSP40776.2020.9054364, URL
https://doi.org/10.1109/ICASSP40776.2020.9054364
Champion T, Bowman H, Grze´s M (2021a) Branching time active inference: empirical study. URL
https://arxiv.org/abs/2111.11276, available at https://arxiv.org/abs/2111.11276.
Champion T, Grze´s M, Bowman H (2021b) Realizing Active Inference in Varia-
tional Message Passing: The Outcome-Blind Certainty Seeker. Neural Computation
pp 1–65, DOI 10.1162/neco a 01422, URL https://doi.org/10.1162/neco_a_01422,
https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco_a_01422/1930278/neco_a_01422.pdf
Cox M, van de Laar T, de Vries B (2019) A factor graph approach to automated design of Bayesian
signal processing algorithms. Int J Approx Reason 104:185–204, DOI 10.1016/j.ijar.2018.11.002, URL
https://doi.org/10.1016/j.ijar.2018.11.002
Cullen M, Davey B, Friston KJ, Moran RJ (2018) Active inference in openai gym: A paradigm
for computational investigations into psychiatric illness. Biological Psychiatry: Cognitive Neu-
roscience and Neuroimaging 3(9):809 – 818, DOI https://doi.org/10.1016/j.bpsc.2018.06.010, URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617, computational Methods and
Modeling in Psychiatry
Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (2020a) Active inference on discrete state-spaces:
A synthesis. Journal of Mathematical Psychology 99:102,447, DOI https://doi.org/10.1016/j.jmp.2020.102447,
URL https://www.sciencedirect.com/science/article/pii/S0022249620300857
Da Costa L, Sajid N, Parr T, Friston K, Smith R (2020b) The relationship between dynamic programming and
active inference: the discrete, finite-horizon case. arXiv 2009.08111
FitzGerald THB, Dolan RJ, Friston K (2015) Dopamine, reward learning, and active infer-
ence. Frontiers in Computational Neuroscience 9:136, DOI 10.3389/fncom.2015.00136, URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136
Forney GD (2001) Codesongraphs: normalrealizations. IEEETransactions on InformationTheory47(2):520–548
33
Champion et al.
Fountas Z, Sajid N, Mediano PAM, Friston KJ (2020) Deep active inference agents us-
ing Monte-Carlo methods. In: Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H
(eds) Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, URL
https://proceedings.neurips.cc/paper/2020/hash/865dfbde8a344b44095495f3591f7407-Abstract.html
Fox CW,RobertsSJ(2012) AtutorialonvariationalBayesian inference.ArtificialIntelligence Review38(2):85–95,
DOI 10.1007/s10462-011-9236-8, URL https://doi.org/10.1007/s10462-011-9236-8
Fox V,Hightower J,LiaoL,SchulzD,Borriello G(2003) Bayesian filteringforlocation estimation. IEEEPervasive
Computing 2(3):24–33, DOI 10.1109/MPRV.2003.1228524
Friston K, Schwartenbeck P, Fitzgerald T, Moutoussis M, Behrens T, Dolan R (2013) The anatomy of choice:
active inference and agency. Frontiers in Human Neuroscience 7:598, DOI 10.3389/fnhum.2013.00598, URL
https://www.frontiersin.org/article/10.3389/fnhum.2013.00598
Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Doherty JO, Pezzulo G (2016) Active inference and learning.
Neuroscience & Biobehavioral Reviews 68:862 – 879, DOI https://doi.org/10.1016/j.neubiorev.2016.06.022
Friston K, Da Costa L, Hafner D, Hesp C, Parr T (2021) Sophisticated Inference. Neural Com-
putation 33(3):713–763, DOI 10.1162/neco a 01351, URL https://doi.org/10.1162/neco_a_01351,
https://direct.mit.edu/neco/article-pdf/33/3/713/1889421/neco_a_01351.pdf
Friston KJ, Parr T, de Vries B (2017) The graphical brain: Belief propagation and active inference. Network
Neuroscience 1(4):381–414, DOI 10.1162/NETN\ a\ 00018, URL https://doi.org/10.1162/NETN_a_00018,
https://doi.org/10.1162/NETN_a_00018
van Glabbeek RJ (1993) The linear time — branching time spectrum II. In: Best E (ed) CONCUR’93, Springer
Berlin Heidelberg, Berlin, Heidelberg, pp 66–81
Glabbeek RJv (1990) The linear time-branching time spectrum (extended abstract). In: Proceedings of the The-
ories of Concurrency: Unification and Extension, Springer-Verlag, Berlin, Heidelberg, CONCUR ’90, p 278–297
Haarnoja T, ZhouA, Abbeel P, LevineS (2018) Soft actor-critic: Off-policy maximum entropy deep reinforcement
learningwithastochasticactor.CoRRabs/1801.01290, URLhttp://arxiv.org/abs/1801.01290,1801.01290
HafnerD,OrtegaPA,BaJ,ParrT,Friston KJ,Heess N(2020) Action andperceptionasdivergence minimization.
CoRR abs/2009.01791, URL https://arxiv.org/abs/2009.01791, 2009.01791
34
Branching Time Active Inference
van Hasselt H, Guez A, Silver D (2016) Deep reinforcement learning with double q-learning. In:
Schuurmans D, Wellman MP (eds) Proceedings of the Thirtieth AAAI Conference on Artificial
Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, AAAI Press, pp 2094–2100, URL
http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389
Itti L, Baldi P (2009) Bayesian surprise attracts human attention. Vision Re-
search 49(10):1295 – 1306, DOI https://doi.org/10.1016/j.visres.2008.09.007, URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380, visual Attention: Psy-
chophysics, electrophysiology and neuroimaging
Kirchhoff M, Parr T, Palacios E, Friston K, Kiverstein J (2018) The markov blankets of life: autonomy, ac-
tive inference and the free energy principle. Journal of The Royal Society Interface 15(138):20170,792, DOI
10.1098/rsif.2017.0792, URL https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0792,
https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0792
Kocsis L, Szepesv´ari C (2006) Bandit based Monte-Carlo planning. In: Fu¨rnkranz J, Scheffer T, Spiliopoulou
M (eds) Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin, Germany,
September 18-22, 2006, Proceedings, Springer, Lecture Notes in Computer Science, vol 4212, pp 282–293, DOI
10.1007/11871842\ 29, URL https://doi.org/10.1007/11871842_29
Kocsis L, Szepesv´ari C (2006) Bandit based Monte-Carlo planning. In: Fu¨rnkranz J, Scheffer T, Spiliopoulou M
(eds) Machine Learning: ECML 2006, Springer Berlin Heidelberg, Berlin, Heidelberg, pp 282–293
van de Laar T, de Vries B (2019) Simulating active inference processes by message passing. Front Robotics and
AI 2019, DOI 10.3389/frobt.2019.00020, URL https://doi.org/10.3389/frobt.2019.00020
Lai T, Robbins H (1985) Asymptotically efficient adaptive allocation rules. Adv Appl Math 6(1):4–22, DOI
10.1016/0196-8858(85)90002-8, URL https://doi.org/10.1016/0196-8858(85)90002-8
Lample G, Chaplot DS (2017) Playing FPS games with deep reinforcement learning. In: Singh
SP, Markovitch S (eds) Proceedings of the Thirty-First AAAI Conference on Artificial Intelli-
gence, February 4-9, 2017, San Francisco, California, USA, AAAI Press, pp 2140–2146, URL
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14456
Maisto D,Gregoretti F,Friston KJ,PezzuloG(2021) Active treesearch inlargePOMDPs.CoRRabs/2103.13860,
URL https://arxiv.org/abs/2103.13860, 2103.13860
Millidge B (2019) Combining active inference and hierarchical predictive coding: A tutorial introduction and case
study. URL https://doi.org/10.31234/osf.io/kf6wc
35
Champion et al.
Millidge B (2020) Deep active inference as variational policy gradients. Journal of Math-
ematical Psychology 96:102,348, DOI https://doi.org/10.1016/j.jmp.2020.102348, URL
http://www.sciencedirect.com/science/article/pii/S0022249620300298
Millidge B, Tschantz A, Buckley CL (2021) Whence the Expected Free Energy? Neural Com-
putation 33(2):447–482, DOI 10.1162/neco a 01354, URL https://doi.org/10.1162/neco_a_01354,
https://direct.mit.edu/neco/article-pdf/33/2/447/1896836/neco_a_01354.pdf
Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D, Riedmiller MA (2013) Playing atari with
deep reinforcement learning. CoRR abs/1312.5602, URL http://arxiv.org/abs/1312.5602, 1312.5602
O’Reilly RC (1996) Biologically plausible error-driven learning using local activation differences: The
generalized recirculation algorithm. Neural Comput 8(5):895–938, DOI 10.1162/neco.1996.8.5.895, URL
https://doi.org/10.1162/neco.1996.8.5.895
Parr T, Friston KJ (2019) Generalised free energy and active inference. Biological Cybernetics 113(5):495–513,
DOI 10.1007/s00422-019-00805-w, URL https://doi.org/10.1007/s00422-019-00805-w
Pezzato C, Corbato CH, Wisse M (2020) Active inference and behavior trees for reactive action planning and
execution in robotics. CoRR abs/2011.09756, URL https://arxiv.org/abs/2011.09756, 2011.09756
Rafetseder E, Schwitalla M, Perner J (2013) Counterfactual reasoning: From childhood to adulthood. Journal of
experimental child psychology 114(3):389–404
Rosin CD (2010) Multi-armed bandits with episode context. In: International Symposium on Artificial
Intelligence and Mathematics, ISAIM 2010, Fort Lauderdale, Florida, USA, January 6-8, 2010, URL
http://gauss.ececs.uc.edu/Workshops/isaim2010/papers/rosin.pdf
Russo DJ, Van Roy B, Kazerouni A, Osband I, Wen Z (2018) A tutorial on Thompson sampling. Found Trends
Mach Learn 11(1):1–96, DOI 10.1561/2200000070, URL https://doi.org/10.1561/2200000070
Sajid N, Ball PJ, Parr T, Friston KJ (2021) Active Inference: Demystified and Compared. Neural
Computation 33(3):674–712, DOI 10.1162/neco a 01357, URL https://doi.org/10.1162/neco_a_01357,
https://direct.mit.edu/neco/article-pdf/33/3/674/1889396/neco_a_01357.pdf
SancaktarC,LanillosP(2020) End-to-endpixel-baseddeepactive inferenceforbodyperceptionandaction.ArXiv
abs/2001.05847
Sancaktar C, van Gerven MAJ, Lanillos P (2020) End-to-end pixel-based deep active inference for body per-
ception and action. In: Joint IEEE 10th International Conference on Development and Learning and Epi-
36
Branching Time Active Inference
genetic Robotics, ICDL-EpiRob 2020, Valparaiso, Chile, October 26-30, 2020, IEEE, pp 1–8, DOI 10.1109/
ICDL-EpiRob48136.2020.9278105, URL https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105
Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, Schmitt S, Guez A, Lockhart E, Hassabis D,
Graepel T, Lillicrap TP, Silver D (2019) Mastering Atari, Go, Chess and Shogi by Planning with a Learned
Model. ArXiv abs/1911.08265
Schwartenbeck P, Passecker J, Hauser TU, FitzGerald THB, Kronbichler M, Friston K
(2018) Computational mechanisms of curiosity and goal-directed exploration. bioRxiv
DOI 10.1101/411272, URL https://www.biorxiv.org/content/early/2018/09/07/411272,
https://www.biorxiv.org/content/early/2018/09/07/411272.full.pdf
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, Schrittwieser J, Antonoglou I, Panneer-
shelvam V, Lanctot M, Dieleman S, Grewe D, Nham J, Kalchbrenner N, Sutskever I, Lillicrap TP, Leach M,
Kavukcuoglu K, Graepel T, Hassabis D (2016) Mastering the game of Go with deep neural networks and tree
search. Nature 529(7587):484–489, DOI 10.1038/nature16961, URL https://doi.org/10.1038/nature16961
Smith R, Friston KJ, Whyte CJ (2021) A step-by-step tutorial on active inference and its application to empirical
data. URL https://psyarxiv.com/b4jm6/
Sondik EJ (1971) The optimal control of partially observable markov processes. PhD thesis, Stanford University
URL https://ci.nii.ac.jp/naid/20000916958/en/
Thompson WR (1933) On the likelihood that one unknown probability exceeds another in view of the evidence of
two samples. Biometrika 25(3/4):285–294, URL http://www.jstor.org/stable/2332286
Winn J, Bishop C (2005) Variational message passing. Journal of Machine Learning Research 6:661–694
Wyble B, Bowman H (2006) A neural network account of binding discrete items into working memory using a
distributed pool of flexible resources. Journal of Vision 6(6):33–33a
Appendix A: Generalized inner and outer products
Generalized outer products: Given N vectors Vi, the generalized outer product returns an N dimensional
array W, whose element in position (x ,...,x ) is given by V1 ×...×VN , where Vi is the x -th element of the
1 N x1 xN xj j
i-th vector. In other words:
W = ⊗ V1,...,VN ⇔ W(x ,...,x ) = V1 ×...×VN
1 N x1 xN
h i
∀x ∈{1,...,|Vj|} ∀j ∈ {1,...,N}, (76)
j
37
Champion et al.
where |Vj| is the number of elements in Vj. Also, note that by definition W is a N-tensor of size |V1|×...×|VN|.
Figure 7 illustrates the generalized outer product for N = 3.
dim of
V1
W(i,j,k)=V1 (i)V2 (j)V3 (k)
V1
dim of
V3
V2 V3 dim of
V2
Figure 7: This figure illustrates the generalized outer product W = ⊗ V1,V2,V3 , where W is a cube of values
illustrated in red, whose typical element W(i,j,k) is the product of V1(i), V2(j) and V3(k). Also, the vectors
(cid:2) (cid:3)
Vi ∀i∈ {1,...,3} are drawn in blue along the dimension of the cube they correspond to.
Generalized inner products: Given an N-tensor W and M = N−1 vectors Vi, the generalized inner product
returns a vector Z obtained by performing a weighted average (with weighting coming from the vectors) over all
but one dimension. In other words:
Z = W ⊙ V1,...,VM ⇔ Z(x )= V1 ×...×W(x ,...,x ,...,x )×...×VM
j x1 1 j M xM
h i x1 1X,...,V1
∈{ | |}
...
xM 1 } ,... } ,VM
∈{ | |}
∀x ∈ {1,...,|Z|}, (77)
j
where |Z| denotes the number of elements in Z, and the large summand is over all x for r ∈ {1,...,M}\{j}, i.e.,
r
excluding j. Also, note that if |W| ∀i∈ {1,...,M} is the number of elements in the dimension corresponding to
Vi
Vi, then for W ⊙ V1,...,VM to be properly defined, we must have |W| = |Vi| ∀i ∈ {1,...,M} where |Vi| is
Vi
the number of elem(cid:2)ents in Vi.(cid:3)Figure 8 illustrates the generalized inner product for N = 3.
dimofV1 Z =W ⊙(cid:2)V2,V3 (cid:3)
dimofV1
W
Z
dimofV3 V2 V3 dimofV2 Z(i)=Pj,k V2(j)V3(k)W(i,j,k)
Figure 8: This figure illustrates the generalized inner product Z = W ⊙ V2,V3 , where W is a cube of values
illustrated in red with typical element W(i,j,k). Also, the vectors Z and Vi ∀i ∈ {2,3} are drawn in blue along
(cid:2) (cid:3)
the dimension of the cube they correspond to.
38
Branching Time Active Inference
Naming of the dimensions: Importantly, we should imagine that each side of W has a name, e.g., if W is
a 3x2 matrix, then the i-th dimension of W could be named: “the dimension of V ”. This enables us to write:
i
Z1 = W ⊙V1 and Z2 = W ⊙V2, where Z1 is a 1x2 matrix (i.e., a vector with two elements) and Z2 is a 3x1
matrix (i.e., a vector with three elements). The operator ⊙ knows (thanks to the dimension name) that W ⊙V1
takes the weighted average w.r.t “the dimension of V ”, while W ⊙V2 must take the weighted average over “the
1
dimension of V ”.
2
In the context of active inference, the matrix A has two dimensions that we could call “the observation
dimension” (i.e., row-wise) and “the state dimension” (i.e., column-wise). Trivially, A⊙o will then correspond
τ
to the average of A along the observation dimension and A⊙Dˆ will correspond to the average of A along the
τ
state dimension.
Appendix B: Generalized inner/outer products and other well-known products
In this section, we explore the relationship between our generalized inner and outer products—presented in ap-
pendix A—and other well known products in the literature.
Inner product of two vectors
The inner product of two vectors ~a and~b of the same size is given by:
~a
| |
~a·~b = a b , (78)
i i
i=1
X
where a and b are the elements of the vectors ~a and~b, respectively, and |~a| is the number of elements in~a. This
i i
product is a special case of our generalised inner product, i.e.,
W
| |
Z = W ⊙V ⇔ Z = W V , (79)
i i
i=1
X
where Z is a scalar (a 0-tensor), W and V are two vectors (two 1-tensors) of the same size, and |W| is the number
of elements in W.
Inner product of two matrices (Frobenius inner product)
The inner product of two matrices A and B of same sizes is given by:
A1 A2
| | | |
hA,Bi = a b , (80)
ij ij
i=1 j=1
XX
39
Champion et al.
where |A| is the number of rows in A, |A| is the number of columns in A, and a (b ) are the elements of the
1 2 ij ij
matrices A (respectively B). This product is not a special case of our generalised inner product.
Inner product of two tensors (Frobenius inner product)
The inner product of two tensors A and B of same sizes is given by:
A1 An
| | | |
hA,Bi = ... a(i ,...,i )b(i ,...,i ), (81)
1 n 1 n
i X1=1 i Xn=1
where a(i ,...,i ) and b(i ,...,i ) are the elements of the tensors A and B, respectively, and |A| is the number of
1 n 1 n i
elements in the i-th dimension of A. This product is not a special case of our generalised inner product.
Standard matrix multiplication
Let A be an n×m matrix and~b be a vector of size m. The standard matrix multiplication of A by~b is given by:
m
~c = A~b ⇔~c = A ~b . (82)
i ij j
j=1
X
This is a special case of our generalised inner product, i.e.,
~c= A~b = A⊙~b. (83)
Additionally, let A be an n×m matrix and~a be a vector of size n, then:
n
~c= AT~a ⇔~c = A ~a . (84)
i ji j
j=1
X
This is a special case of our generalised inner product, i.e.,
~c = AT~a = A⊙~a. (85)
Note that because the dimensions are “named” (c.f. Appendix A) the operator performs the transposition implic-
itly.
Outer product of two vectors
Given two vectors ~a and~b, there outer product—denoted~a⊗~b—is a matrix defined as:
40
Branching Time Active Inference
a b ... a b
1 1 1 n
~a⊗~b =  . . . ... . . .  . (86)
 
 
a b ... a b
 m 1 m n
 
 
This outer product is a special case of our generalised outer product, where the operator is applied to only two
vectors, i.e.
~a⊗~b = ⊗ ~a,~b . (87)
h i
Outer product of two tensors
Given two tensors U and V, the outer product of U and V is another tensor W such that:
W(i ,...,i ,j ,...,j )= V(i ,...,i )U(j ,...,j ) ∀i ∈ {1,...,|V | }∀α ∈{1,...,n}
1 n 1 m 1 n 1 m α α
∀j ∈ {1,...,|U| }∀β ∈ {1,...,m}. (88)
β β
Given N vectors Vi ∀i∈ {1,...,N}, our outer product is a sequence of outer tensor products, i.e.,
⊗ V1,...,VN = V1⊗ V2 ⊗ ... ⊗ VN, (89)
tensor tensor tensor
" #
h i h i
where ⊗ and ⊗ are the tensor and generalised outer products, respectively.
tensor
Kronecker product
Given two matrices A and B, the Kronecker product of A and B—denoted ⊗ —is a generalisation of the outer
K
product from vectors to matrices defined as:
a B ... a B
11 1n
A⊗ K B =  . . . .. . . . .  , (90)
 
 a B ... a B
 m1 mn 
 
 
where a are the elements of A. Note that even if the Kronecker product is a generalisation of the outer product,
ij
it is neither a special case nor a generalisation of our generalized outer product.
41
Champion et al.
Hadamard product
Given two matrices A and B of the same size, the Hadamard product of A and B is an element-wise product
defined by:
C = A⊙B ⇔ c =a b ∀i∈ {1,...,|A| }∀j ∈ {1,...,|A| }, (91)
ij ij ij 1 2
where a , b and c are the elements in the i-th row and j-th column of A, B and C, respectively, |A| is the
ij ij ij 1
number of rows in A, and |A| is the number of columns in A. This product is unrelated to both our generalised
2
inner and outer products.
Appendix C: Instance of variational message passing
This appendix provides a concrete instance of the method of Winn and Bishop discussed in Section 3. The
generative model is as follows:
P(S,D) = P(S|D)P(D) (92)
where:
P(S|D) = Cat(D) (93)
P(D) = Dir(d). (94)
Additionally, the variational distribution is given by:
Q(D) = Dir(dˆ), (95)
which means that we assume that S is an observed random variable. Let us start with the definition of the
Dirichlet and categorical distributions written in the form of the exponential family:
d −1 lnD
1 1
   
lnP(D) = ... · ... −lnB(d) (96)
   
d −1   lnD  zD(d)
 S   S 
 | |   | |
| {z }
   
µD(d) uD(D)
| {z } | {z }
42
Branching Time Active Inference
lnD [S = 1]
1
   
lnP(S|D) = ... · ... (97)
   
 lnD   [S = |S|] 
 S   
 | |  
   
µS(D) uS(S)
| {z } | {z }
where · performs an inner product of the two vectors it is applied to, B(d) is the Beta function and |S| is the
number of values a state can take. The first step requires us to re-write Equation 97 as a function of u (D),
D
which is straightforward because µ (D) is just another name for u (D). Using the fact that the inner product is
S D
commutative:
[S = 1] lnD
1
   
lnP(S|D) = ... · ... . (98)
   
 [S = |S|]   lnD 
   S 
   | |
   
µS→D(S) uD(D)
| {z } | {z }
The second step aims to substitute (96) and (98) into the variational message passing equation (9), i.e.
d −1 lnD [S = 1] lnD
1 1 1
       
lnQ ∗ (D) = ... · ... −lnB(d) + ... · ... +Const, (99)
       
Dd −1   lnD  zD(d) E D [S = |S|]   lnD E
 S   S     S 
 | |   | |    | |
| {z }
       
µD(d) uD(D) µS→D(S) uD(D)
| {z } | {z } | {z } | {z }
where h•i refers to h•i
QD
. Note that in the above equation, d
i
are fixed parameters, therefore there is no posterior
∼
over d and the first expectation h·i can be removed. The third step rests on taking the exponential of both
QD
∼
sides, using the linearity of expectation and factorising by u (D) to obtain:
D
d −1+h[S = 1]i
1
 
Q ∗ (D) = exp ... ·u D (D)+Const , (100)
(  )
d
−1+h[S = |S|]i

 S 
 | | 
 
where z (d) have been absorbed into the constant term because it does not depend on D. The fourth step is a
D
re-parameterisation done by observing that h[S = i]i is the i-th element of the expectation of the vector u (S),
S
i.e. hu (S)i = h[S = i]i:
S i
43
Champion et al.
d −1+hu (S)i
1 S 1
 
Q ∗ (D) = exp ... ·u D (D)+Const . (101)
(  )
d
−1+hu (S)i

 S S S 
 | | | |
 
µ˜D(...)+µ˜S→D(...)
| {z }
The last step consists of computing the expectation of hu (S)i for all i. This can be achieved by realising
S i
that the probability of an indicator function for an event is the probability of this event, i.e hu (S)i = h[S =
S i
i]i = Q(S = i) = Dˆ where Dˆ is a one hot representation of the observed value for S. Substituting this result in
i
Equation 101, leads to the final result:
d −1+Dˆ
1 1
 
Q ∗ (D) = exp ... ·u D (D)+Const . (102)
(  )
d −1+Dˆ 
 S S 
 | | | |
 
Indeed, the above equation is in fact a Dirichlet distribution in exponential family form, and can be re-written
into its usual form to obtain the final update equation:
Q (D) = Dir(d+Dˆ). (103)
∗
Appendix D: Expected log of Dirichlet distribution
Definition 7 A probability distribution over x parameterized by µ is said to belong to the exponential family if
its probability mass function P(x|µ) can be written as:
P(x|µ) = h(x)exp µ·T(x)−A(µ) , (104)
h i
where h(x) is the base measure, µ is the vector of natural parameters, T(x) is the vector of sufficient statistics,
and A(µ) is the log partition.
Lemma 8 The log partition is given by:
A(µ) = ln h(x)exp µ·T(x) dx. (105)
Z h i
44
Branching Time Active Inference
Proof Starting with the fact that P(x|µ) integrate to one:
P(x|µ)dx = h(x)exp µ·T(x)−A(µ) dx = 1 (106)
Z Z h i
1
⇔ h(x)exp µ·T(x) dx = 1 (107)
expA(µ)
Z h i
⇔ A(µ) = ln h(x)exp µ·T(x) dx (108)
Z h i
Lemma 9 The gradient of the log partition function is the expectation of the sufficient statistics, i.e.,
∂A(µ)
= E T(x) . (109)
∂µ P(x
|
µ)
h i
Proof Restarting with the derivative of the result of Lemma 8:
∂A(µ) ∂
= ln h(x)exp µ·T(x) dx , (110)
∂µ ∂µ
" #
Z h i
and using the chain rule:
∂A(µ) 1 ∂
= h(x)exp µ·T(x) dx . (111)
∂µ h(x)exp µ·T(x) dx∂µ
" #
Z h i
R (cid:2) (cid:3)
Note that the denominator of the first term is equal to the exponential of A(µ), and we can swap the derivative
and the integral because the limit of integration does not depend on the parameters µ:
∂A(µ) 1 ∂
= h(x)exp µ·T(x) dx (112)
∂µ expA(µ) ∂µ
Z (cid:20) h i (cid:21)
1 ∂
= h(x) exp µ·T(x) dx. (113)
expA(µ) ∂µ
Z (cid:20) h i (cid:21)
Using the chain rule again:
∂A(µ) 1
= h(x)exp µ·T(x) T(x)dx (114)
∂µ expA(µ)
Z h i
= h(x)exp µ·T(x)−A(µ) T(x)dx (115)
Z h i
= P(x|µ)T(x)dx (116)
Z
= E T(x) . (117)
P(xµ)
|
h i
45
Champion et al.
Theorem 10 IfD isdistributedaccording toaDirichletdistributionQ(D) = Dir(D;dˆ), then: D˚= E lnD ⇔
Q(D)
D˚ = ψ(d )−ψ( d ). (cid:2) (cid:3)
i i j j
P
Proof Let µ be equal to dˆ−~1. Taking the exponential of both sides in Equation 96 and using that dˆ= µ+~1,
we obtain:
dˆ −1 lnD
1 1
   
Q(D) = exp ... · ... −lnB(µ+~1) , (118)
(    )
dˆ −1   lnD  A(µ)
 S   S 
 | |   | |
| {z }
   
µ T(D)
| {z } | {z }
where µ is the vector of natural parameters, T(D) is the vector of sufficient statistics, A(µ) is the log partition,
and B(•) is the beta function. Using the result of Lemma 9:
∂A(µ) ∂
D˚=∆ E T(D) = E lnD = = lnB(µ+~1) . (119)
Q(D) Q(D) ∂µ ∂µ
h i h i h i
We now focus on a typical element of D˚:
∂
D˚ = lnB(µ+~1) , (120)
i
∂µ
i
h i
and use the definition of the beta function:
∂ Γ(µ +1)
D˚ = ln k k (121)
i
∂µ Γ( µ +1)
i (cid:20) Q k k (cid:21)
∂
= lnPΓ(µ +1)−lnΓ( µ +1) (122)
∂µ k k k
i
(cid:20) k (cid:21)
X P
∂ ∂
= lnΓ(µ +1) − lnΓ( µ +1) , (123)
∂µ i ∂µ k k
i i
(cid:20) (cid:21) (cid:20) (cid:21)
P
46
Branching Time Active Inference
where Γ(•) is the gamma function. The last step relies on the chain rule:
∂ ∂µ +1 ∂ ∂ µ +1
D˚ = lnΓ(µ +1) i − lnΓ( µ +1) k k (124)
i ∂(µ +1) i ∂µ ∂( µ +1) k k ∂µ
i (cid:20) (cid:21) i k k (cid:20) (cid:21) P i
P
=1 P =1
∂ ∂
= lnΓ(µ +1) |− {z } lnΓ( µ +1) | {z } (125)
∂(µ +1) i ∂( µ +1) k k
i (cid:20) (cid:21) k k (cid:20) (cid:21)
P
= ψ(µ +1)−ψ( µ +1) P (126)
i k k
= ψ(dˆ)−ψ( d Pˆ ), (127)
i k k
P
where we used that dˆ= µ+~1 and the definition of the digamma function, i.e., ψ(x) = ∂lnΓ(x) .
∂x
Appendix E: Relationship between BTAI and active inference (Lemmas)
Lemma 11 Underthe assumption that the probability of observations and states are independent of future actions,
i.e.,
∀j ∈ N , Q(O |π ) ≈ Q(O |π ) and Q(S |π )≈ Q(S |π ), (128)
>0 t+i i t+i i+j t+i i t+i i+j
then:
∀j ∈ N , G(π ,t+i) ≈ G(π ,t+i). (129)
>0 i+j i
Proof The proof is straightforward, we start with the following definition:
G(π ,t+i) = D [Q(O |π )||P(O )] + E [H[P(O |S )]]. (130)
i+j KL t+i i+j t+i Q(St+iπi+j) t+i t+i
|
Then, using the assumption that the probability of observations and states are independent of future actions, i.e.,
∀j ∈ N , Q(O |π )≈ Q(O |π ) and ∀j ∈ N , Q(S |π )≈ Q(S |π ), we get:
>0 t+i i+j t+i i >0 t+i i+j t+i i
G(π ,t+i) ≈ D [Q(O |π )||P(O )] + E [H[P(O |S )]] =∆ G(π ,t+i). (131)
i+j KL t+i i t+i Q(St+iπi) t+i t+i i
|
47
Champion et al.
Lemma 12 The aggregated cost for an arbitrary N is given by:
N
Gaggre = G(π ,t+i), (132)
πN i
i=1
X
Proof The proof is done by induction. The initialisation holds for N = 1, indeed, π = {U } and by definition:
1 t
t+1
G(π ) = G(π ,τ) = D [Q(O |π )||P(O )] + E [H[P(O |S )]], (133)
1 1 KL t+1 1 t+1 Q(St+1π1) t+1 t+1
|
Ga π g 1 gre τ= X t+1 Gl π o 1 cal
| {z } | {z }
⇔ Gaggre = Gaggre+Glocal, (134)
π1 π0 π1
because by definition Gaggre = 0. Then, assuming that Gaggre = N G(π ,t+i) holds for some N, we show
π0 πN i=1 i
that its hold for N +1 as well. By definition: P
Gaggre = Gaggre+Glocal , (135)
πN+1 πN πN+1
and:
Glocal = G(π ,t+N +1). (136)
πN+1 N+1
Using the inductive hypothesis and the above two equations:
N
Gaggre = G(π ,t+i)+G(π ,t+N +1) (137)
πN+1 i N+1
i=1
X
N+1
= G(π ,t+i). (138)
i
i=1
X
Appendix F: Notation
In this appendix, we introduce the notation used throughout this paper. The following sub-sections describe the
notation related to sets of numbers, tensors, probability distributions, global variables, multi-indices and random
variables, respectively.
48
Branching Time Active Inference
Sets of numbers
Definition 13 Let N be the set of all strictly positive integers defined as:
>0
N = {x ∈N | x > 0}, (139)
>0
where N is the set of all natural numbers.
Definition 14 Let R be the set of all strictly positive real numbers defined as:
>0
R = {x ∈R | x > 0}, (140)
>0
where R is the set of all real numbers.
Tensors
Definition 15 An n-tensor is an n-dimensional array of values. Each element of an n-tensor is indexed by an
n-tuple of non-negative integers, i.e., (x ,...,x ) where x ∈ N ∀i∈ {1,2,...,n}.
1 n i >0
Definition 16 Let T be an n-tensor. The element of T indexed by the n-tuple (x ,...,x ) is a real number
1 n
denoted by T(x ,...,x ).
1 n
Example 1 Let T be a 2-tensor defined as:
1 2
T = . (141)
 
3 4
 
 
Then T(1,1) = 1, T(1,2) = 2, T(2,1) = 3 and T(2,2) = 4.
Remark 17 A 0-tensor is a scalar, a 1-tensor is a vector, and a 2-tensor is a matrix.
Definition 18 LetT beann-tensor. The sizeofT isavectorofsize ndenoted |T|whose i-thelementcorresponds
to the size of the i-th dimension of T.
Example 2 Let T be a 2-tensor defined as:
1 2 3
T = . (142)
 
4 5 6
 
 
Then |T| = 2 and |T| = 3.
1 2
49
Champion et al.
Definition 19 Let T be an n-tensor. A 1-sub-tensor of T is a 1-tensor obtained by selecting a 1-dimensional
slice of T, i.e.,
T(x
1
,...,x
i 1
,•,x
i+1
,,...,x
n
), (143)
−
where • represents the selection of a 1-dimensional slice of T, and the values of all x j=i must be set to specific
6
values in {1,...,|T| }. Figure 9 (left) illustrates the notion of a 1-dimensional slice.
j
X3 X3
T T
X1
T(x1,•,x3)
X2 X1
T(x1,•,•)
X2
Figure 9: This figure illustrates the notion of a 1-dimensional slice (on the left) and of a 2-dimensional slice (on
the right).
Definition 20 Let T be an n-tensor and m < n. An m-sub-tensor of T (denoted W) is an m-tensor obtained
by selecting an m-dimensional slice of T, i.e.,
W = T(x
1
,...,x
i1 1
,•,x
i1+1
,......,x
im 1
,•,x
im+1
,...,x
n
), (144)
− −
where i ∈ {1,...,n} ∀k ∈ {1,...,m} are indices representing the dimension being selected. Naturally, the k-th
k
dimension of W corresponds to the i -th dimension of T for k ∈ {1,...,m}. Importantly, the sextuple of dots in
k
the middle of the expression represents that there will be m symbols “•”, i.e., one for each dimension selected.
Figure 9 (right) illustrates the special case of a 2-dimensional slice, i.e., m = 2.
Example 3 Let T be a 3-tensor such that:
• |T| = 2
1
• T(1,x ,x ) = 1, ∀(x ,x ) ∈ {1,...,|T| }×{1,...,|T| }
2 3 2 3 2 3
• T(2,x ,x ) = 2, ∀(x ,x ) ∈ {1,...,|T| }×{1,...,|T| }.
2 3 2 3 2 3
Then T(1,•,•) is a 2-sub-tensor of T full of ones, and T(2,•,•) is a 2-sub-tensor of T full of twos.
50
Branching Time Active Inference
Probability distributions
Definition 21 A random n-tensor is an n-tensor over which we have an n-dimensional probability distribution.
Remark 22 A random variable is a random 0-tensor, a random vector is a random 1-tensor, and a random
matrix is a random 2-tensor.
Definition 23 An n-tensor T is said to represent a joint distribution over a set of n random variables
{X ,...,X } if:
1 n
P(X = x ,...,X = x )= T(x ,...,x ). (145)
1 1 n n 1 n
For conciseness, if T represents P(X ,...,X ) we let:
1 n
P(X ,...,X ) = Cat(T). (146)
1 n
Remark 24 If T represents P(X ,...,X ), then the sum of its elements must equal one.
1 n
Remark 25 In contrast, if T is a random n-tensor, then:
P(X ,...,X |T) = Cat(T), (147)
1 n
which means that the joint probability over X ,...,X is represented by T, and because T is a random tensor
1 n
(taking values in the set of valid n-tensors T , i.e., the set of all n-tensors whose elements sum up to one), we
n
must specify which instance of T ∈ T should be used to define the joint distribution over X ,...,X .
n 1 n
Definition 26 Anm-tensorR issaidtorepresent a conditional distributionoverasetofmrandom variables
{X ,...,X } if:
1 m
P(X =x ,...,X = x |X = x ,...,X = x ) = R(x ,...,x ). (148)
1 1 n n n+1 n+1 m m 1 m
For conciseness, if R represents P(X ,...,X |X ,...,X ) we let:
1 n n+1 m
P(X ,...,X |X ,...,X )= Cat(R). (149)
1 n n+1 m
Remark 27 IfRrepresents P(X
1
,...,X
n
|X
n+1
,...,X
m
), thentheelementsofthen-sub-tensorR(•,...,•,x
n+1
,...,x
m
)
must sum to one ∀x ∈ {1,...,|R| } ∀i∈ {n+1,...,m}.
i i
51
Champion et al.
Remark 28 If R is a random m-tensor, then:
P(X ,...,X |X ,...,X ,R)= Cat(R). (150)
1 n n+1 m
Remark 29 Importantly, definition 26 uses the symbol T to represent P(X ,...,X ) and definition 23 uses the
1 n
symbol R to represent P(X ,...,X |X ,...,X ). Throughout this document, different symbols will be used for
1 n n+1 m
representing joint and conditional distributions.
Definition 30 Let R be a random m-tensor representing P(X ,...,X |X ,...,X ,R) and k = m − n be the
1 n n+1 m
number of variables upon which the variables X ,...,X are conditioned. Having a Dirichlet prior over R means
1 n
that:
Rn+1 Rm
| | | |
P(R) = ... Dir r(i
1
,...,i
k
,•) , (151)
i Y1=1 i Yk=1 (cid:16) (cid:17)
where r is an (m+1)-tensor such that the 1-sub-tensor r(i
1
,...,i
k
,•) contains the parameters of the Dirichlet prior
over P(X ,...,X |X = i ,...,X = i ). For conciseness, we denote the prior over R as:
1 n n+1 1 m k
P(R) = Dir(r). (152)
Remark 31 Definition 30 implicitly means that if V is a 1-tensor then Dir(V) represents a Dirichlet distribution.
However, if V is an m-tensor (with m 6= 1) then Dir(V) represents a product of Dirichlet distributions.
Remark 32 If R isa random m-tensorrepresenting P(X ,...,X |X ,...,X ,R), thenitsprior willbe aproduct
1 n n+1 m
of |X |×...×|X | Dirichlet distributions, where |X | is the number of values that X can take. Additionally,
n+1 m i i
each Dirichlet distribution will have |X |×...×|X | parameters stored in the last dimension of r, where r is the
1 n
tensor storing the parameters of the prior over R, i.e. P(R) = Dir(r).
Example 4 Let A be a random 2-tensor representing P(O|S,A), then the Dirichlet prior over A is given by:
A2
| |
P(A) = Dir(a)=∆ Dir a(i,•) , (153)
Y i=1 (cid:16) (cid:17)
where |A| is the number of values that S can take (i.e., the number of hidden states).
2
52
Branching Time Active Inference
Example 5 Let B be a random 3-tensor representing P(S |S ,U ,B), then the Dirichlet prior over B is given
τ+1 τ τ
by:
B2 B3
| | | |
P(B) = Dir(b) =∆ Dir b(i
1
,i
2
,•) , (154)
i Y1=1i Y2=1 (cid:16) (cid:17)
where |B| is the number of values that S can take (i.e., the number of hidden states) and |B| is the number of
2 τ 3
values that U can take (i.e., the number of actions).
τ
Global labels
Definition 33 The number of actions available to the agent is denoted |U|.
Definition 34 The number of states in the environment is denoted |S|.
Definition 35 The number of observations that the agent can make is denoted |O|.
Definition 36 The number of policies that the agent can pick from is denoted |π|.
Definition 37 The time point representing the present is a natural number denoted t.
Definition 38 The time-horizon (i.e., the time point after which the agent stops modelling the sequence of
hidden states) is a natural number denoted T.
Multi-indices
Definition 39 A multi-index is a sequence of indices denoted by:
I = (i ,...,i ), (155)
1 n
where i ∈ D ∀j ∈ {1,...,n}, and in this paper D = {1,...,|U|}.
j
Remark 40 Multi-indices are used to index random variables such that S is the hidden state obtained after taking
I
the sequence of actions described by I, and O is the random variable representing the observation generated by
I
S .
I
Definition 41 The last index of a multi-index is denoted I , i.e., I is the last element of the sequence I.
last last
Definition 42 The one-hot representation of the action corresponding to I is denoted I~ .
last last
Definition 43 Given a multi-index I, I\last corresponds to the sequence of actions described by I without the
last element.
53
Champion et al.
Remark 44 In Section 6, when a hidden state (i.e., S ) is indexed by I, then S will be the parent of S .
I Ilast I
\
Definition 45 Given an expandable generative model, I is the set of all multi-indices already expanded from
t
the current state S .
t
Remark 46 In Section 6 each time a hidden state (i.e., S ) is added to the generative model, I is added to the
I
set of all multi-indices already expanded I .
t
Random variables and parameters of their distributions
Remark 47 Parameters of the posterior distributions are recognizable by the hat notation, e.g., aˆ, ˆb and dˆwill
be posterior parameters, while a, b and d will be prior parameters.
Remark 48 The expected logarithm of an arbitrary tensor X representing a conditional or a joint distribution is
denoted X¯, e.g. A¯= E [lnA] and B¯ = E [lnB].
Q(A) Q(B)
Definition 49 Let U be a random variable taking values in {1,...,|U|} indexing all possible actions. The prior
τ
distribution over U is a categorical distribution represented by Θ . The posterior distribution over U is a
τ τ τ
categorical distribution represented by Θˆ .
τ
Definition 50 Let S be a random variable taking values in {1,...,|S|} indexing all possible states. The prior
τ
and posterior distributions over S are categorical distributions represented by different tensors depending on the
τ
generative model being considered. Therefore, those distributions are defined in Sections 4 and 6.
Definition 51 Let O be an observed random variable taking values in {1,...,|O|} indexing all possible observa-
τ
tions. The prior distribution over O is a categorical distribution conditioned on S and represented by the random
τ τ
matrix A. There is no posterior distribution over O because O is observed, i.e., realised.
τ τ
Definition 52 Let O be a random variable taking values in {1,...,|O|} indexing all possible observations. The
I
prior distribution over O is a categorical distribution conditioned on S and represented by the random matrix A¯.
I I
Note that O refers to an observation in the future and is therefore a hidden variable. The posterior distribution
I
over O is a categorical distribution represented by Eˆ .
I I
Definition 53 Let S be an random variable taking values in {1,...,|S|} indexing all possible states. The prior dis-
I
tribution over S
I
isa categorical distribution conditioned on S
Ilast
and represented bythe matrix B¯
I
=∆ B¯(•,•,I
last
).
\
The posterior distribution over S is a categorical distribution represented by Dˆ .
I I
Definition 54 Let A be a |O|×|S| random matrix defining the probability of an observation O given the hidden
τ
state S . The prior distribution over A is a product of Dirichlet distributions whose parameters are stored in a
τ
54
Branching Time Active Inference
|S|×|O| matrix a. The posterior distributions over A is also a product of Dirichlet distribution, but the parameters
are stored in a |S|×|O| matrix aˆ.
Definition 55 Let B be a |S|×|S|×|U| random 3-tensor defining the probability of transiting from S to S
τ τ+1
when taking action U . The prior distribution over B is a product of Dirichlet distributions whose parameters are
τ
stored in a |S|×|U|×|S| 3-tensor b. The posterior distribution over B is also a product of Dirichlet distributions,
but the parameters are stored in a |S|×|U|×|S| 3-tensor ˆb.
Definition 56 Let D be a random vector of size |S| defining the probability of the initial state S . The prior
0
distribution over D is a Dirichlet distribution whose parameters are stored in a vector d of size |S|. The posterior
distribution over D is also a Dirichlet distribution, but the parameters are stored in a vector dˆof size |S|.
Definition 57 Let Θ ∀τ ∈ {0,...,t−1} be a random vector of size |U| defining the probability of the action U .
τ τ
The prior distribution over Θ is a Dirichlet distribution whose parameters are stored in a vector θ of size |U|.
τ τ
The posterior distribution over Θ is also a Dirichlet distribution, but the parameters are stored in a vector θˆ of
τ τ
size |U|.
Definition 58 Let γ be a random variable taking values in R . The prior distribution over γ is a gamma
>0
distribution with shape parameter α = 1 and rate parameter β ∈ R . The posterior distribution over γ is a
>0
gamma distribution with shape parameter αˆ = 1 and rate parameter βˆ∈ R .
>0
Definition 59 Let π be a random variable taking values in {1,...,|π|} indexing all possible policies. The prior
distribution over π is a softmax function of the vector G multiplied by minus the precision γ. Note that G is a
vector of size |π| whose i-th element is the expected free energy of the i-th policy. The posterior distribution over
π is a categorical distribution whose parameters are stored in a vector πˆ of size |π|.
Appendix G: Multi-armed bandit problem
In the multi-armed bandit problem, the agent is prompted with K actions (one for each bandit’s arm). Pulling
the i-th arm returns a reward sampled from the reward distribution P (X) associated to this arm. Let µ be the
i i
mean of the i-th reward distribution and T (n) be the number of times the i-th bandit has been selected after n
i
plays. To solve the bandit problem, one needs to come up with an allocation strategy that selects the action that
minimises the agent’s regret defined as:
K
R =µ n− µ E[T (n)], (156)
n ∗ i i
i=1
X
55
Champion et al.
where µ is the average reward of the best action. Note that an upper bound of E[T (n)] is derived by first upper
∗ i
bounding T (n), and then using: the Chernoff-Hoeffing bound, the Bernstein inequality and some properties of
i
p-series, c.f., proof of Theorem 1 in Auer et al (2002) for details. So, if we first assume that,
1 1 2lnn
UCB1 = X¯ + , (157)
i i
1 1 n
r r r i
exploitation exploration
| {z } | {z }
where n is the number of times the i-th action has been selected, and X¯ is the average reward received after
i i
taking the i-th action. Then, the main result of Auer et al (2002) was to show that if an allocation strategy was
usingtheUCB1 criterion to select thenext action, theexpected regret of this allocation strategy willgrow at most
logarithmically in the number of plays n, i.e., O(lnn). In addition, since it is known that the expected regret of
the (best) allocation strategy grows at least logarithmically in n (Lai and Robbins, 1985), we say that the UCB1
criterion resolves the exploration / exploitation trade-off, i.e., the UCB1 criterion ensures that the expected regret
grows as slowly as possible.
Appendix H: The exponential complexity class
In this appendix, we precisely pinpoint the exponential complexity class that is addressed in this paper, but first,
we introduce a multi-index notation. Multi-indices will help us to refer to hidden states in the future. Naturally
enough the indexes inside the multi-indices will correspond to the actions the agent will have to perform to reach
the hidden state, e.g., S corresponds to a hidden state at time t+3 obtained by performing action 1 at time t,
(123)
2 at time t+1 and 3 at time t+2. Using this notation, Figure 10 depicts all the possible policies up to two time
steps in the future and the associated hidden states. Importantly, Figure 10 shows that the number of policies
grows exponentially with the number of time steps for which the agent tries to plan. Therefore, the definition
of the prior over the policies, i.e., P(π|γ) = σ(−γG), exhibits an exponential space and time complexity class
because the agent needs to store and compute the |U|T parameters of P(π|γ), where T is the time-horizon.
56
Branching Time Active Inference
S
(12)
S
(1)
S
(11)
S
t
S
(22)
S
(2)
S
(21)
Figure 10: Illustration of all possible policies up to two time steps in the future when |U| = 2. The state at the
current time step is denoted by S . Additionally, each branch of the tree corresponds to a possible policy, and
t
each node S is indexed by a multi-index (e.g. I = (12)) representing the sequence of actions that led to this
I
state. This should make it clear that for one time step in the future, there are |U| possible policies, after two
time steps there are |U| times more policies, and so on until the time-horizon T where there are a total of |U|T
possible policies, i.e., the number of possible policies grows exponentially with the number of time steps for which
the agent tries to plan.
To show that this exponential explosion is not only a theoretical problem and also appears in practice, we
modified the DEMO MDP maze.m of the SPM1 package in two ways. First, we allowed the agent to plan N
time steps in the future as follows:
% V = Allowable policies (N moves into the future), nu = number of actions
V = [];
for i1 = 1:nu, i2 = 1:nu, .., iN = 1:nu
V(:,end + 1) = [i1;i2; .. ;iN];
end
Second, we used the “tic” and “toc” functions provided by Matlab to track the execution time required to
execute the function “spm maze search” where the parameters of the model are assumed to be known already
by the agent:
tic % Start a timer used to evaluate the execution time of spm_maze_search
MDP = spm_maze_search(mdp,8,START,END,128,1);
toc % Display the time elapsed since the last call to tic
Figure 11 presents the results of our simulations for N from 2 to 6. Under a logarithmic scale on the time axis,
the experimental results show that the graph is almost a perfect line, which provides empirical evidence for an
1. Statistical parametric mapping (SPM) is a software package created by the Wellcome Department of Imaging Neuroscience
at University College London, which was initially developed to carry out statistical analyses of functional neuroimag-
ing data. Today, SPM also contains MatLab simulations implementing active inference agents (among other things), c.f.
https://www.fil.ion.ucl.ac.uk/spm/.
57
Champion et al.
exponential time explosion. Note that the simulation for N = 7 crashed after trying to allocate an array of 9.5GB
(space explosion). In section 5, we presented an approach proposed to deal with the exponential complexity class
that arises during planing, yet is fundamentally similiar to active inference. This effectively means that our paper
shows how standard active inference can be made more efficient and scale to longer time horizons.
103
102
101
100
2 3 4 5 6
N
emiT
Figure 11: This figure shows the time required to execute the function “spm maze search” when the agent is
allowed to plan N time steps in the future (for N from 2 to 6). A logarithmic scale is used on the time axis.
58