Curiosity-Driven Development of Action and
Language in Robots Through Self-Exploration
Theodore J. Tinker1, Kenji Doya1, Jun Tani1âˆ—
1 Okinawa Institute of Science and Technology, Okinawa, Japan.
* To whom correspondence should be addressed; E-mail: jun.tani@oist.jp.
Abstract
Human infants acquire language and action gradually through de-
velopment, achieving strong generalization from minimal experience,
whereas large language models require exposure to billions of training
tokens. What mechanisms underlie such efficient developmental learn-
ing in humans? This study investigates this question through robot sim-
ulation experiments in which agents learn to perform actions associated
with imperative sentences (e.g.,push red cube) via curiosity-driven self-
exploration. Our approach integrates the active inference framework
with reinforcement learning, enabling intrinsically motivated develop-
mental learning. The simulations reveal several key findings: i) Gen-
eralization improves markedly as the scale of compositional elements
increases. ii) Curiosity combined with motor noise yields substantially
better learning than exploration without curiosity. iii) Rote pairing of
sentences and actions precedes the emergence of compositional general-
ization. iv) Simpler, prerequisite-like actions develop earlier than more
complex actions that depend on them. v) When exception-handling
rules were introducedâ€”where certain imperative sentences required
1
arXiv:2510.05013v4  [stat.ML]  4 Dec 2025
executing inconsistent actionsâ€”the robots successfully acquired these
exceptions through exploration and displayed a U-shaped performance
curve characteristic of representational redescription in child language
learning. Together, these results suggest that curiosity-driven explo-
ration and active inference provide a powerful account of how intrinsic
motivation and hierarchical sensorimotor learning can jointly support
scalable compositional generalization and exception handling in both
humans and artificial agents.
Introduction
A central question in both cognitive science and artificial intelligence is how humans and artificial
systems can acquire competencies for language and action developmentally, despite having access
to only limited learning experiences. This question is exemplified in human infants, who achieve
remarkable generalization with sparse input. This is a stark contrast to large-scale models which rely
on massive training corpora to reach similar capabilities. This raises the issue of what mechanisms
enable such efficient developmental learning.
From the perspective of developmental psychology, infants acquire language through rich inter-
action with their embodied environments. Tomaselloâ€™s â€œverb-islandâ€ hypothesis argues that children
initially learn verbs in specific, isolated contexts before generalizing across broader linguistic struc-
tures (1). He also emphasized the importance of embodiment in language acquisition, suggesting
that grounding linguistic symbols in sensorimotor experiences is fundamental to language learn-
ing (1). This view aligns with other studies in developmental psychology highlighting the role of
compositionality and generalization in language acquisition (2, 3, 4).
In linguistic terms, compositionality refers to the ability to construct novel configurations by
systematically combining elements such as verbs, adjectives, and nouns. Generalization enables
infants to apply learned components flexibly, allowing for the production and interpretation of
utterances that have not been directly encountered previously. Although the number of possible
compositions grows multiplicatively with the vocabulary size (i.e., number of verbsÃ—number of
adjectivesÃ—number of nouns), infants achieve generalization after experiencing only a small subset
2
of learning examples. This suggests that the effective sample complexity could be proportional to
the sum of elements rather than their product. This phenomenon is closely related to the â€œpoverty of
the stimulusâ€ problem articulated by Chomsky (5), which asks how learners generalize so effectively
given severely sparse input.
To investigate these mechanisms, one promising approach is to reconstruct developmental
learning processes in machines and robots. The field of developmental robotics has long pursued
this line of research, aiming to replicate human-like learning trajectories in embodied systems
(6, 7, 8, 9). However, relatively few studies have focused on development of language and motor
control under conditions of stimulus poverty. Existing work has primarily examined associative
mappings between linguistic input and motor commands in one-shot or supervised batch learning
schemes (10,11,12,13). These approaches neglect the self-directed, developmental context of infant
learning.
In this study, we propose a self-exploratory learning framework of robots in which reinforcement
learning is incorporated with the active inference framework (14,15,16), enabling curiosity-driven
exploration. Our approach to integrate reinforcement learning with active inference was originally
inspired by the work of Kawahara et al. (17). In our model, originally introduced in (18), motor
commands are reinforced by two intrinsic rewards: curiosity (seeking unpredictable sensory con-
sequences) and motor entropy (seeking random movements). Motor commands are also reinforced
by extrinsic rewards for successfully achieving goal verbs specified by given imperative sentences.
Importantly, our previous experiments in maze navigation demonstrated that the combination of cu-
riosity and motor entropy is crucial for enhancing self-exploration, as agents achieved significantly
improved exploratory behaviors under this dual-intrinsic reward scheme. Our approach aligns with
broader research on self-exploration in machine learning, in which agents are intrinsically rewarded
for taking motor commands that increase unpredictability or information gain (19, 20).
A simulated mobile robot equipped with a manipulator arm, vision sensor, and distributed tactile
sensors learns to generate motor movements in response to imperative sentences presented during
each trial. These sentences are systematically composed of verbs, adjectives, and nouns, enabling
evaluation of generalization performance under different levels of compositional complexity.
Beyond these primary developmental processes, we further explore the capacity forexception
handlingâ€”a hallmark of flexible cognition. In human development, exceptions such as irregular
3
verbs or inconsistent mappings often produce non-monotonic, U-shaped learning trajectories:
children first apply a correct form, then overgeneralize it (producing errors), and finally recover
the correct rule. This pattern has been widely interpreted as evidence of internal representational
reorganization orrepresentational redescription(21). Computationally, such U-shaped performance
has been demonstrated in models of language acquisition and rule learning (22, 23, 24, 25, 26).
Developmentally, these phenomena reflect the tension between rote memorization, generalization,
and the later refinement of exception rules.
To examine whether similar mechanisms can emerge in artificial developmental systems, we
hypothesize that robots trained through curiosity-driven active inference can also acquire exception-
handling rules through exploratory learning. Specifically, we predict that when presented with a
small number of abnormal linguistic rulesâ€”commands requiring reversed or inconsistent ac-
tionsâ€”the robot will exhibit a U-shaped performance curve: initial correct learning, followed by
a decline due to overgeneralization, and eventual recovery as exception-specific representations
are internalized. This pattern would indicate that the system develops not merely through associa-
tive reinforcement, but through representational reorganization analogous to cognitive processes
observed in human infants.
This study therefore tested the following hypotheses through simulation experiments:H1:
Generalization performance improves as the scale of compositionality in the task increases.H2:
Curiosity combined with motor entropy enhances the performance of developmental learning.H3:
In the early phase, actions are generated only for exactly learned imperative sentences, but in later
phases, the system generalizes to novel, unlearned compositions.H4:Primitive actions are acquired
earlier, followed by more complex, prerequisite-dependent actions.H5:Exception-handling rules
can be acquired through exploratory learning, exhibiting U-shaped developmental performance
similar to that observed in human cognition.
4
Results
Task Description
We created a robot like a truck crane in a physics simulator along with a set of objects with 5
different shapes each of which can be with 6 different colors (see Fig. 1). The robot can maneuver
Fig. 1.The simulated robot and a set of objects to act on.(A)The robot has two wheels and an
arm with two joints. The design is similar to a truck crane.(B)Left to right: a red pillar, a green
pole, a blue dumbbell, a cyan cone, and a magenta hourglass. The color yellow is not pictured here.
by controlling velocity of left and right wheels independently, and also can move its arm by
controlling rotation velocity of the yaw and pitch joint angles for acting on the objects. A camera
with 16 x 16 pixels was fixed to the body for visual sensation. 16 touch sensors were distributed
in the body and the arm, respectively, and rotation angles for the yaw and pitch were sensed as
proprioception.
For each trial episode, a task goal was given in terms of an imperative sentence composed with
verb, adjective, and noun. Possible words used for them are shown in Table 1. In the beginning of
each episode, two objects were located at random positions in the arena wherein one object was the
one specified in the imperative sentence and the other was the one with randomly selected color
and shape combination among possible ones.
At each step, the robot receives visual sensation, proprioception for the arm, tactile sensation ,
5
English Words
Verb Adjective Noun
watch red pillar
be near green pole
touch the top blue dumbbell
push forward cyan cone
push left magenta hourglass
push right yellow
Table 1.English words.The English words used for imperative sentences specifying goals.
and two types of voices: the command voice and the tutor-feedback voice. The command voice takes
the format of the imperative sentence described previously, and it comes every step continuously
from the beginning. On the other hand, the feedback voice arrives whenever the robot achieves one
of possible goals even if the achieved goal is not the imperative sentence told by the command
voice, and it informs which goal has been achieved actually in the same format with the command
voice. This potentially enhances the forward model to learn about own action as associated with
linguistic representation. Finally, when the goal specified by the command voice is achieved, a
reward is provided. Each trial episode ran for 30 steps, or terminated when the specified goal is
achieved.
Effects of curiosity: Experiment 1
The experiment examined effects of different levels of curiosity to the developmental learning
processes using the basic setup. In the basic setup, full compositions of words (Table ) were used to
generate the imperative sentences. However, the training was conducted using only 60 imperative
sentences (33 percentage) out of 180 possible sentences. 120 untrained sentences were used for
generalization test. For ten robots with different random seeds, the complete developmental learning
process was iterated for 60000 epochs. The generalization test with unlearned imperative sentences
was conducted for every 50 epochs.
The experiment was conducted by changing the levels of curiosity. Since the random latent
6
variables are computed separately for each sensory modality, the complexity or curiosity can
be computed for each sensory modality. Three levels of curiosity were considered in computing
expected free energyğºwhereinno curiosity: the curiosity terms for all sensory modalities are
not included,sensory-motor curiosity: the curiosity terms only for vision, tactile sensation, and
proprioception are included,all curiosity: the curiosity terms for all sensory modalities including
feedback voice are included.
Fig. 2 shows the development of the generalization test performances in terms of success rate for
goals specified by unlearned imperative sentences which are plotted for different action categories
with different levels of curiosity. Shades areas represent 99% confidence intervals.
The plots shows that the performance was improved significantly as the curiosity level was
increased. Especially, the case of all actions with the all curiosity level shows the average success
rate for unlearned goals reached a quite high value of 85 percentage even though the learning was
conducted only for 33 percentage of all possible compositions.
It can be also seen in Fig. 2 that some action categories developed faster than other action
categories. Especially in the all curiosity case, â€œwatchâ€ developed the fastest, â€œbe nearâ€ did as the
second, â€œtouch the topâ€ as the third, and â€œpush leftâ€ â€œpush rightâ€, and â€œpush forwardâ€ developed
much later. This implies that simpler prerequisite-type actions develop earlier, and more complex
actions requiring those prerequisite actions develop later. Actually, an action of watching an object
should be prerequisite for all other object-targeted actions including an action of moving near by
an object, which should be prerequisite again for actions of directly manipulating an object like
pushing left/right an object or touching the top of it. Our observation accords with this.
Next, Fig. 6(A)shows the success rate comparison between learned and unlearned goals under
the all curiosity condition for each action category. These plots show that the test performance for
learned goals developed significantly faster than the case for the unlearned goals. This indicates
that actions are generated only for exactly learned compositional imperative sentences in the early
phase, but the system generalizes to novel, unlearned ones in the later phase.
Video S1 shows examples of the behaviors of robots with the all curiosity level. It can be seen
that in the intermediate phase of development, the robot often acts with play-like behavior without
achieving specified goals while it quickly and accurately achieves its goals in the final phase of
development.
7
Fig. 2.Rolling success-rates for unlearned goals.Compares agents with different levels of
curiosity.
Further analysis
Some analysis were conducted for the purpose of gaining comprehension of the internal representa-
tion developed. We applied Principle Component Analysis (PCA) to the estimated posterior latent
states corresponding to the command voice input, incorporating the verbs and adjectives stated in
each command. Fig. 3 presents evidence that robots with all curiosity developed a compositional
and generalizable understanding of these goals. At the midpoint of training, the latent representa-
8
tions begin to show consistent grouping of verbs and adjectives. For example, the verbs â€œwatch,â€
â€œbe near,â€ and â€œpush forwardâ€ are tightly grouped, suggesting that these verbs are interpreted as
similar. In contrast, the verb â€œpush rightâ€ appears heavily separated from other verbs, and the verbs
â€œtouch the topâ€ and â€œpush leftâ€ also appear as distinct categories. After training, these clusters
of verbs are more compact and separated. Within these clusters, there is loose sub-structuring by
color: green and yellow tend to be on the left side of the cluster, while blue and magenta are on the
right side of the cluster. Therefore, it can be said that each cluster represents a distinct linguistic
concept while exhibiting relationship with others since the learning of visuo-proprioceptive-motor
also contributes to this structuring. In contrast, Fig. 4 shows the same type of PCA results for a
robot with no curiosity. â€œBe near,â€ â€œpush forward,â€ and â€œpush rightâ€ are heavily entangled with each
other. Thus supports the idea that curiosity aids verb disentanglement and compositional learning.
In the current model, the robotâ€™s knowledge of the environment should develop richer along
the course of exploratory learning. For confirmation of this idea, we examined the capability of
the robots in generating mental plans for achieving goals without accessing the sensory inputs
except the initial step for an episode trial as compared between the half-trained case and the fully
trained case. A robotâ€™s mental planning can be visualized by allowing it to receive real sensory
observation only at the initial step, after which the robot must rely entirely on its own internal
predictions. In this setting, the robot views its predicted sensory observations as if they are true
inputs. This process may be likened to a dreamlike state or hallucinatory simulation, in which the
robot mentally simulates future events based on its internal model of the world. Fig. 5(A)illustrates
a simulation example for the fully trained case. The robot was commanded to touch the top of the
yellow pillar. In that figure, the first row shows the ground truth environment from a view behind
the robotâ€™s shoulder. The second row shows what the robot would truly observe if it were not in this
planning setting. The third row shows the robotâ€™s look ahead predictions for visual observations,
which it will interpret as if they are real. It can be seen that even with only the first step sensory
observation, the robot could generate mostly accurate future look-ahead prediction for sensation as
well as motor command. These predictions are sufficiently accurate for the robot to maintain an
internal conceptualization of the environment and complete its command in the case of the end of
the developmental learning. Fig. 5(B)illustrates the same robot after only half of its training in the
same scenario. In this case, the robotâ€™s predictions are inaccurate, causing it to wander and view an
9
Fig. 3.PCA for language latent variables in the case of all curiosity.PCA of latent variables
corresponding to the command voice.(A)Halfway through development.(B)After complete
development. Clusters with substructures emerged early and became more refined over time.
object which does not actually exist.
Effects of scale in compositions: Experiment 2
Next experiment examines the effects of scales of compositionality in learned examples to the
generalization performance. For this purpose, experiments were conducted using reduced number
of words for generating imperative sentences. While the previous basic setup used sentences
10
Fig. 4.PCA for language latent variables in the case of no curiosity.PCA applied to latent
representations of command voice inputs after complete development.
composed of 6 verbs, 6 adjectives, and 5 object nouns as the full scale case, the middle scale case
was prepared with 5 verbs, 5 adjectives, and 4 object nouns, and the small scale case with 4 verbs,
4 adjectives, and 3 object nouns. The exact words used for each setup are listed in Table 2. For
all scaling cases, again only one third was used for learning examples while remained two third
was used for generalization test. Other experimental conditions were also set as the same as the
Experiment 1.
The experimental results are shown in Fig. 6. Shades areas represent 99% confidence intervals.
It can be seen that although the learned goal test cases show equally high performance for all scales
of compositionality, the generalization test for unlearned goal case shows that the success rate
in the final trial decreases significantly (85 percentage to 25 percentage) as the compositionality
scale decreases. This indicates that the generalization performance severely depends on the scale
of compositionality in learning examples.
11
Fig. 5.Mental plans generated by the robot. This robot is commanded to touch the top of the
yellow pillar. The first row displays the ground truth from a view over the robotâ€™s right shoulder.
The second row displays the visual sequence of the ground truth. The third row shows the visual
sequence of mental planning.(A)The case for the end of complete developmental learning, and
(B)for the case of halfway developed.
Exception rule handling: Experiment 3
This experiment examined how robots can acquire exception handling rules through developmental
learning. While most commandâ€“action mappings were preserved, the commands â€œwatch magenta
pillarâ€ and â€œbe near green poleâ€ were swapped: success required performing theothergoal, not
the one commanded. These mismatches required the robot to override its learned generalized
knowledge.
This simulation experiment was conducted using the same model parameters with the ones used
in the previous experiments with 10 robots with 60,000 epochs of developmental trials. At the end
of development the average success rate among 10 robots was 84 percentage for the learned goals,
76 percentage for unlearned goals, and 50 percentage for the exception handling cases. The average
12
Fig. 6.Rolling success-rates for learned and unlearned goals with different compositionality
scales. (A)Agents trained with all six verbs, all six adjectives, and all five nouns.(B)Agents trained
with five verbs, five adjectives, and three nouns.(C)Agents trained with four verbs, four adjectives,
and three nouns.
success rate for the exception handling cases is not so high most likely due to over-generalization.
PanelAin Fig. 7 shows the rolling success-rate for achieving the exception goals for each of 10
individual robots while PanelBshows the success rate for the same goals but without applying the
exception handling rules. The final success rate for the exception handling case is diverse ranging
from 15 percentage to 90 percentage as can be seen in PanelA. It was also discovered that 7 out
of 10 robots trained with these exceptions exhibit characteristic U-shaped curves: early success,
followed by a drop, and eventual recovery with higher success rate than the earlier one. In contrast,
monotonic increase of success rate can be seen in all 10 individuals in the case of learning without
the exception handling rules. Statistical comparisons (detailed in the Supplementary Text) confirm
that U-shaped patterns are significantly more prevalent in the exception condition than in the control
(ğ‘=.0025).
In Fig. 8, PCA illustrates how the internal representations of of goals in the seventh robot in Fig.
7Aevolve in a manner consistent with the U-shaped success rates. In panelA, early in training,
goal embeddings are muddled without clear structure, reflecting a learning phase with minimal
generalization. In panelB, midway in training, the exception command â€œwatch magenta pillarâ€ is
embedded near other â€œwatchâ€ goals, while â€œbe near green poleâ€ clusters with other â€œbe nearâ€ goals,
13
Name Verbs Adjectives Nouns
Largest Vocabulary
Watch
Be Near
Touch the Top
Push Forward
Push Left
Push Right
Red
Green
Blue
Cyan
Magenta
Yellow
Pillar
Pole
Dumbbell
Cone
Hourglass
Reduced Vocabulary
Watch
Be Near
Push Forward
Push Left
Push Right
Red
Green
Blue
Cyan
Magenta
Pillar
Pole
Dumbbell
Cone
Smallest Vocabulary
Watch
Push Forward
Push Left
Push Right
Red
Green
Blue
Cyan
Pillar
Pole
Dumbbell
Table 2. Verbs, adjectives, and nouns which are used for training agents in three different ways.
despite these associations being incorrect. These observation indicates that overgeneralization has
occurred. In panelC, late in training, â€œwatch magenta pillarâ€ is now embedded near â€œbe nearâ€
goals, and â€œbe near green poleâ€ near â€œwatchâ€ goals, indicating that the robot has correctly handled
these exceptions as swapped pairs. Here, it can be said that the representational redescription took
place in the course of developmental learning of exception handling rules.
Discussion
This study investigated how robots can develop action and language through self-exploration by
integrating active inference with reinforcement learning. The experiments were designed to test
four specific hypotheses, and the results provide clear support for each.
14
Fig. 7.Comparison of the performance curve with and without applying the exception han-
dling rules. (A)The development of success rate in achieving two goals which are swapped as
exceptions. Red vertical lines depict the peaks and valleys of the learning, as defined in supplemen-
tary text.(B)The development of success rate in achieving the same two goals without applying
the exception handling rules.
15
Fig. 8.PCA for language latent variables for an individual developed with the exception
handling rules. (A)Plot at 20000 epochs,(B)plot at 32500 epochs, and(C)plot at 57500 epochs
wherein the circled â€Nâ€ and circled â€Wâ€ denote the sentences applied with the exception handling
.
H1: Generalization is enhanced by compositional scale.The experiments confirmed that
larger vocabularies of verbs, adjectives, and nouns led to greater generalization. Robots trained
with richer compositional repertoires achieved higher success rates on unlearned actions, whereas
smaller vocabularies constrained generalization severely. Our previous study (13) on supervised
16
training of language and action for an arm robot also showed that compositional generalization
improved as the size of verbâ€“noun combinations in training increased. However, that work was
limited in scale, examining only cases from 3Ã—3 to 5Ã—8 verbâ€“noun combinations, where success
rates for unlearned goals improved only from 57% to 71% under the condition of 80% training.
By contrast, the present study examined much broader scaling, ranging from 48 to 180 possible
compositions, while using only 33% of them for training. Under these conditions, generalization
performance improved dramatically from near 25% to 85%. This contrast highlights that scale
plays a critical role in enhancing generalization, and that curiosity-driven developmental learning
provides a more powerful mechanism than supervised schemes under conditions of limited input.
The finding also connects to the classical â€œpoverty of the stimulusâ€ problem raised by Chomsky (5),
as it shows how compositionality enables powerful generalization from sparse training data. Once,
we hypothesize that necessary training size could be proportional to summation of number of words
appeared for each dimension instead of multiplication of it for all dimensions if compositionality
size increases (13). This hypothesis becomes more plausible by the results in the current study
which, however, should be confirmed in much more scaled experiments in the future.
H2: Curiosity combined with motor entropy enhances developmental learning.The second
hypothesis was also confirmed. Robots equipped with both curiosity-driven exploration and motor
entropy consistently outperformed those without, achieving higher success rates in both learned
and unlearned actions. This advantage was particularly pronounced when curiosity extended across
all sensory modalities, including vision, touch, proprioception, and voice feedback. These findings
suggest that the synergy of curiosity (seeking novel, unpredictable outcomes) and motor entropy
(encouraging stochastic exploration) plays a crucial role in accelerating the acquisition of language-
action mappings. This result is consistent with our previous study (18), which showed that combining
curiosity and motor entropy significantly enhanced self-exploration in a maze navigation task. More
broadly, this interpretation aligns with the active inference framework, in which agents minimize
expected free energy by reducing uncertainty through maximizing information gain (16, 27).
H3: Generalization follows rote learning.The results again align with this hypothesis: in
early phases, the robot succeeds only on exactly learned sentenceâ€“action pairs, and it is only over
time that the robot begins to generalize to novel combinations of known elements. This mirrors
developmental patterns in infants, who often begin with rigid pairings before achieving broader
17
generalization (28). Tomaselloâ€™s â€œverb-islandâ€ hypothesis, for example, emphasizes that children
initially acquire verbs in isolated contexts before generalizing across broader structures (1), in the
same way that this robot acquired trained goals first before generalizing to untrained goals. Gerken &
Knight (2015) demonstrated that 10- to 11-month-old infants can generalize from just four linguistic
examples under favorable conditions (29). Moreover, Gerken et al. (2014) provide evidence that
infants may generalize even from a single surprising example, suggesting that hypothesis-driven
generalization can follow minimal exposure (30). These studies lend developmental credence to
our observed progression from rote mapping toward flexible compositional generalization.
H4: Primitive actions precede complex actions.The fourth hypothesis was also validated.
Simpler, prerequisite-like actions such as â€œwatchâ€ or â€œbe nearâ€ emerged earlier, while more com-
plex manipulative actions like â€œpush leftâ€ or â€œtouch the topâ€ developed later. This ordering mirrors
hierarchical dependencies in action acquisition observed in developmental psychology, where in-
fants first master basic motor primitives before acquiring coordinated, goal-directed behaviors.
Such progressive structuring of motor development has been well documented in studies showing
that motor and cognitive skills emerge through iterative interaction between perception, action,
and intrinsic motivation (4, 31). The dynamic systems perspective proposed by Smith and Thelen
emphasizes that complex behaviors self-organize from simpler components through embodied ex-
ploration and adaptation, aligning closely with the hierarchical learning patterns observed in our
robot simulations.
H5: Exception-handling rules exhibit U-shaped development.The results from Experiment 3
provide strong support for this hypothesis. When trained with two swapped commandâ€“action map-
pings, most robots initially produced the correct exceptional actions, subsequently regressed due
to overgeneralization, and finally recovered the appropriate exception-specific mappings. This
non-monotonic, U-shaped performance trajectory mirrors a well-established phenomenon in de-
velopmental psychology, in which children first succeed on irregular forms, later overgeneralize
newly learned rules (e.g., producing â€œgoedâ€ for â€œwentâ€), and ultimately reorganize their internal
representations to master both rules and exceptions. Classic accounts interpret these dynamics as
evidence forrepresentational redescriptionâ€”a restructuring of internal knowledge that enables
more abstract, generative representations (21).
Computational modeling has long shown that such U-shaped learning can emerge naturally from
18
error-driven or distributed representations, including Rumelhart and McClellandâ€™s connectionist
model of English past-tense acquisition (22), the multilayer perceptron models of Plunkett and
Marchman (23, 24), and broader frameworks in computational developmental psychology (26).
Our robot simulations demonstrate that an analogous process arises in curiosity-driven active
inference: the agent first relies on rote pairings, then applies generalized compositional mappings
that overwrite earlier exceptions, and finally reconstructs its latent representation to encode the
exception rules correctly.
Taken together, these findings demonstrate that curiosity-driven exploration, motor entropy,
hierarchical acquisition of actions, and scalable compositional exposure jointly support efficient
developmental learning of language and action. The parallels with infant developmentâ€”rote-to-
generalization progression, prerequisite learning, the role of vocabulary scale, and representation
redescriptionâ€”suggest that the mechanisms implemented here capture essential aspects of devel-
opmental psychology. More broadly, these results strengthen the view that reconstructing develop-
mental processes in robots can offer insights into the â€œpoverty of the stimulusâ€ problem, showing
how powerful generalization can arise from limited input when guided by intrinsic motivation,
structured experience, and the principles of predictive coding and active inference (6, 11, 13, 32).
The current study is still limited in many aspects, and several possible extensions can be
envisioned. One crucial limitation is that our experiments examined only a one-directional com-
munication pathway from tutors to robots, relying on the command and feedback voices to guide
the development of languageâ€“action mappings. In contrast, natural human development is charac-
terized by interactive and bidirectional communication, where infants not only receive instructions
but also actively solicit guidance, clarification, and scaffolding from caregivers.
Future studies should extend the current framework to include interactive communication
between tutors and robots. For example, when a robot cannot successfully execute a command,
it could initiate a communicative act such as â€œTell me how to do itâ€ or â€œAsk me an easier one.â€
Such exchanges would allow tutors to adapt their teaching strategy dynamically, modulating the
complexity of instructions or providing additional cues. This adaptive interaction resonates with
Vygotskian ideas of scaffolding and the â€œzone of proximal development,â€ where caregivers adjust
support according to the learnerâ€™s current abilities (33). It also aligns with research in developmental
psychology emphasizing the role of joint attention, imitation, and social feedback in language
19
learning (1, 34). Incorporating interactive dialogue would thus move the current model closer to
capturing the social nature of language acquisition in human infants. By embedding mechanisms
for robots to both seek help and influence the tutoring process, future work could shed light on how
social scaffolding and communicative feedback accelerate the development of language and action
in natural developmental contexts.
Another promising future direction concerns the development of robotâ€“robot communication
through the evolution of language. Previous research has explored this possibility from different
perspectives: Steels introduced the framework of â€œlanguage gamesâ€ to study the emergence of
shared vocabularies among agents (35), Miikkulainen and colleagues investigated the evolution
of artificial language through evolutionary reinforcement learning (36), and Taniguchi proposed
the emergence of symbols using a collective predictive coding approach (37). While these studies
have demonstrated the possibility of emergent communication in an impressive manner, they still
remain limited in that they mainly achieved the emergence of object labeling or naming, whereas
the evolution of action-related language, such as verbs, has been much less explored. In this
context, the current study based on active inference could be extended to address the evolution of
dynamic linguistic structures, including verbs. Since our model implements active inference within
a variational recurrent neural network, it is naturally suited for capturing temporal and dynamic
aspects of action and language. A future extension of this work toward multi-robot interaction under
the framework of â€œcollective active inferenceâ€ may thus provide novel insights into the evolution
of embodied language, moving beyond static object labeling toward dynamic and action-oriented
communication.
Materials and Methods
In this section, we present the model architecture employed in this study. The current model, as
well as our earlier work (38), extends a study by Kawahara et al. (17). That study demonstrated that
curiosity-driven reinforcement learning can be achieved by incorporating the framework of active
inference (AIF) (16, 27), in which motor behavior is reinforced in the direction which minimizes
expected free energy. More details are shown in the â€œFree energy principle, Active Inference, and
Kawahara Modelâ€ section of the Supplementary Materials, along with a brief introduction of the
20
free energy principle (FEP) and AIF.
The Employed Model
The current model, as well as our previous one (38), extends the approach proposed by Kawahara
et al. (17) by implementing both the forward model and actor-critic using a variational recurrent
neural network (VRNN) (39) in order to deal with temporal complexity and stochasticity inherent
in robotâ€“environment interactions.
The expected free energyğºcan be computed as:
ğºğ‘¡ =âˆ’ğœ‚ğ· ğ¾ğ¿[ğ‘(ğ‘§ğ‘¡|ğ‘œğ‘¡,â„ğ‘¡âˆ’1)||ğ‘(ğ‘§ ğ‘¡|â„ğ‘¡âˆ’1)]|                                     {z                                     }
Curiosity/complexity
âˆ’ğ‘Ÿ(ğ‘  ğ‘¡,ğ‘ğ‘¡)|   {z   }
Extrinsic Reward
âˆ’ğ›¼H(ğœ‹ ğœ™(ğ‘ğ‘¡|â„ğ‘¡âˆ’1))
|                {z                }
Entropy
(1)
This equation is derived by replacingğ‘¤, the probabilistic model learning parameter used in Eq. S9,
withğ‘§, the probabilistic model state. The weighting coefficientsğœ‚andğ›¼are introduced to scale
the contributions of the curiosity and motor entropy terms, respectively. The complexity term is
computed as Kullbackâ€“Leibler divergence (KLD) between the estimated posterior distribution and
the prior distribution over the latent variables at each time step. Both distributions are modeled as
Gaussian distribution with time-dependent means and standard deviations. The estimated posterior
is conditioned on the current sensory observation and the previous hidden state, while the prior is
conditioned only on the previous hidden state. The resulting KLD thus reflects the information gain
from that sensory observation, which is driven by the motor command executed at the previous time
step. Therefore, exploration of more novel situations (i.e., curiosity-driven exploration) tends to
result with higher information gain through larger complexity. The motor entropy in the third term
of Eq. 1 reflects the expected uncertainty of the policy, and is computed as the negative expected
log-probability of generating a motor commandğ‘ğ‘¡ conditioned on the hidden stateâ„ğ‘¡âˆ’1.
By adopting an analogous approach to the Kawahara model, the policy for generating a motor
commandğ‘ğ‘¡ is trained to minimize the expected free energyğº ğ‘¡ (Eq. 1) through RL using the the
Soft Actor Critic (SAC) algorithm (40). Accordingly, theğ‘„ğ‘¡ value is updated as:
ğ‘„ğ‘¡ =ğ‘Ÿğ‘¡ +ğœ‚ğ· ğ¾ğ¿[ğ‘(ğ‘§ğ‘¡|ğ‘œğ‘¡,â„ğ‘¡âˆ’1)||ğ‘(ğ‘§ ğ‘¡|â„ğ‘¡âˆ’1)]+ğ›¼H(ğœ‹ ğœ™(ğ‘ğ‘¡+1 |â„ğ‘¡))
+ğ›¾(1âˆ’ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡)Eğ‘œğ‘¡+1 âˆ¼ğ·,ğ‘ğ‘¡+1 âˆ¼ğœ‹ğœ™ [ğ‘„Â¯ğœƒ(ğ‘œğ‘¡+1,ğ‘ğ‘¡+1)].(2)
21
The first termğ‘Ÿğ‘¡ represents the extrinsic reward. The second termğ·ğ¾ğ¿[ğ‘(ğ‘§ğ‘¡|ğ‘œğ‘¡,â„ğ‘¡âˆ’1)||ğ‘(ğ‘§ ğ‘¡|â„ğ‘¡âˆ’1)]is
the intrinsic reward for curiosity, scaled by a positive coefficientğœ‚. The third termH(ğœ‹ ğœ™(ğ‘ğ‘¡+1 |â„ğ‘¡))
is the intrinsic reward for motor entropy, scaled by a positive coefficientğ›¼. The fourth term is the
bootstrapped estimate of the next stepâ€™s value,dğ‘„ğ‘¡+1, which is weighted by a discount rate parameter
ğ›¾âˆˆ[0,1]. The variableğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡ is zero for all steps except the episodeâ€™s final step, where it is set
to one. This restrains the definition ofğ‘„ ğ‘¡ to steps within the episode. The criticğ‘„ ğœƒ(ğ‘œğ‘¡+1,ğ‘ğ‘¡+1)
is trained to generate cğ‘„ğ‘¡, approximation ofğ‘„ ğ‘¡. The target criticğ‘„ Â¯ğœƒ(ğ‘œğ‘¡+1,ğ‘ğ‘¡+1)is maintained for
stability in the criticâ€™s training. Initially identical to the critic, the target critic is updated via Polyak
averaging such that Â¯ğœƒâ†ğœğœƒ+(1âˆ’ğœ) Â¯ğœƒwithğœâˆˆ[0,1]. The actorğœ‹ ğœ™(ğ‘œğ‘¡)is trained to generate
motor commandsğ‘ğ‘¡ which maximize the criticâ€™s predictions of value. To mitigate positive bias, it is
common to train multiple separate critics (each with its own target critic) (40). The actor is trained
using the minimum predicted value across critics. Our model employs two separate critics.
The forward model is trained dynamically over the course of exploratory learning by optimizing
the model parametersğœ“to minimize the evidence free energyğ¹ ğœ“ (Eq. S3) after each trial episode.
The exact implementation of this process is described in the supplementary material subsection,
Details of the Model Architecture.
Robot Actions
The robot and the objects were simulated in PyBullet, the python physics simulator. Each wheelâ€™s
velocity was bounded within the range of[âˆ’10,10]meters per second. For scale, the robotâ€™s body
is a cube measuring 2 meters along each dimension (length, width, and height). The robotâ€™s arm
features two joints: yaw, which rotates left and right within a range of[âˆ’30Â°,30 Â°], and pitch, which
rotates forward and upward within a range of[0Â°,90 Â°].For smooth movement, the robotâ€™s wheel and
arm velocities were implemented with linear interpolation from the current to the target velocities.
We defined success criteria for each action category, which determined whether or not the robot
earned an extrinsic reward. The distance between the robot and an object was measured from the
objectâ€™s center to the center of the robotâ€™s body. The robot was considered to be â€œfacing the objectâ€
when the angular deviation between the robotâ€™s forward direction the line connection it to the object
was less than 15 degrees.
Watch:The robot faces the object between 6 and 10 meters of distance. This must be maintained
22
for 6 steps in a row.
Be Near:The robot faces the object with distance of less than 6 meters, without touching the
object. This must be maintained for 5 steps in a row.
Touch the Top:The robotâ€™s hand contacts with the object while the center of the hand is at
least 3.75 meters above the floor. This must be maintained for 3 steps in a row.
Push Forward:The robot pushes the object farther than .1 meters, with respect to the robotâ€™s
facing direction. This must be maintained for 3 steps in a row.
Push Left:The robot pushes the object to the robotâ€™s left farther than .2 meters, while the
robotâ€™s wheels have velocities below 5 meters per second (requiring use of the arm). This must be
maintained for 3 steps in a row.
Push Right:Same asPush Left,but in the opposite direction.
There are some constraints in rewarding for actions which are described in the â€œConstraints in
Performing Actionsâ€ subsection in the Supplementary Materials.
Developmental learning with Exception handling rules
To examine characteristics in developmental learning with exception handling rules, we tested an
alternative training setup with a pair of swapped goal-action mappings. Robots were trained such
that the command â€œwatch magenta pillarâ€ was only rewarded if the robot performed â€œbe near green
poleâ€ instead, and vice versa. These exceptions conflicted with the regular compositional mappings.
A matched control group of identical robots was trained without such exceptions. The success-rates
of these robots is depicted in Fig. 7.
23
References and Notes
1. M. Tomasello,Constructing a Language: A Usage-Based Theory of Language Acquisition
(Harvard University Press) (2005).
2. L. R. Gleitman, The structural sources of verb meanings.Language Acquisition1(1), 3â€“55
(1990).
3. P. Bloom,How Children Learn the Meanings of Words(MIT Press) (2000).
4. L. B. Smith, E. Thelen, Development of word learning: An embodied perspective.Develop-
mental Review25(3), 205â€“244 (2005).
5. N. Chomsky,Rules and Representations(Columbia University Press) (1980).
6. M. Asada,et al., Cognitive Developmental Robotics as a New Paradigm for the Design of
Humanoid Robots.Robotics and Autonomous Systems37(2-3), 185â€“193 (2001), doi:10.1016/
S0921-8890(01)00115-4.
7. Y. Kuniyoshi, S. Sangawa, Early motor development from partially ordered neural-body dynam-
ics: experiments with a cortico-spinal-musculo-skeletal model.Biological cybernetics95(6),
589â€“605 (2006).
8. G. Sandini, G. Metta, J. Konczak, Developmental robotics: Insights from developmental psy-
chology on robotic learning.Progress in Brain Research164, 327â€“346 (2007).
9. T. J. Prescott, P. F. Dominey, Synthesizing the temporal self: robotic models of episodic and
autobiographical memory.Philosophical Transactions B379(1913), 20230415 (2024).
10. A. Cangelosi, T. Riga, Simulation of language and action learning in a multi-agent environment.
Proceedings of the IEEE92(3), 396â€“401 (2004).
11. Y. Sugita, J. Tani, Cross-situational learning of words and sentences: A developmental robotics
experiment.Proceedings of the IEEE92(3), 428â€“442 (2005).
24
12. A. Taniguchi, T. Taniguchi, T. Inamura, Spatial concept acquisition for a mobile robot that
integrates self-localization and unsupervised word discovery from spoken sentences.IEEE
Transactions on Cognitive and Developmental Systems8(4), 285â€“297 (2016).
13. R. Vijayaraghavan, D. Roy, A. Cangelosi, Grounding language learning in embodied interac-
tion: A review of approaches and challenges.Frontiers in Robotics and AI8, 625891 (2021).
14. K. Friston, J. Mattout, J. Kilner, Action understanding and active inference.Biological cyber-
netics104(1), 137â€“160 (2011).
15. G. Pezzulo, F. Rigoli, K. J. Friston, Hierarchical active inference: a theory of motivated control.
Trends in cognitive sciences22(4), 294â€“306 (2018).
16. T. Parr, K. J. Friston, Generalised free energy and active inference.Biological Cybernetics
(2019).
17. D. Kawahara, S. Ozeki, I. Mizuuchi, A Curiosity Algorithm for Robots Based on the Free
Energy Principle, in2022 IEEE/SICE International Symposium on System Integration (SII)
(Narvik, Norway) (2022).
18. J. Tinker, K. Doya, J. Tani, Active Inference and Reinforcement Learning for Curiosity-Driven
Developmental Robotics.Adaptive Behavior(2024).
19. P.-Y. Oudeyer, F. Kaplan, V. V. Hafner, Intrinsic motivation systems for autonomous mental
development.IEEE transactions on evolutionary computation11(2), 265â€“286 (2007).
20. J. Schmidhuber, A possibility for implementing curiosity and boredom in model-building
neural controllers.Proceedings of the International Conference on Simulation of Adaptive
Behavior: From Animals to Animatspp. 222â€“227 (1991).
21. A. Karmiloff-Smith,Beyond Modularity: A Developmental Perspective on Cognitive Science
(MIT Press, Cambridge, MA) (1992).
22. D. E. Rumelhart, J. L. McClelland, On Learning the Past Tenses of English Verbs, inPar-
allel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2, J. L.
McClelland, D. E. Rumelhart, Eds. (MIT Press), pp. 216â€“271 (1986).
25
23. K. Plunkett, V. Marchman, U-shaped learning and frequency effects in a multilayer perceptron:
Implications for child language acquisition.Cognition38(1), 43â€“102 (1991).
24. V. Marchman, K. Plunkett, From U-shaped learning to systematicity: A connectionist account
of English past tense acquisition.Cognition48(3), 279â€“304 (1993).
25. J. L. Elman,et al.,Rethinking Innateness: A Connectionist Perspective on Development(MIT
Press) (1996).
26. D. Mareschal, T. R. Shultz, Computational Developmental Psychology.Trends in Cognitive
Sciences5(5), 178â€“185 (2001).
27. K. Friston,et al., Active inference and learning.Neuroscience & Biobehavioral Reviews68,
862â€“879 (2016), doi:10.1016/j.neubiorev.2016.06.022.
28. M. Tomasello,Constructing a Language: A Usage-Based Theory of Language Acquisition
(Harvard University Press, Cambridge, MA) (2003).
29. L. Gerken, S. Knight, Infants generalize from just (the right) four words.Cognition143,
187â€“192 (2015).
30. L. Gerken, C. Dawson, R. Chatila, J. Tenenbaum, Surprise! Infants consider possible bases of
generalization for a single input example.Developmental Science(2014).
31. K. E. Adolph, J. M. Franchak, Learning to move, moving to learn: A quarter century of progress
in studying infant motor development.Child Development Perspectives9(3), 214â€“219 (2015).
32. A. Cangelosi, M. Schlesinger,Developmental Robotics: From Babies to Robots(MIT Press)
(2015).
33. L. S. Vygotsky,Mind in Society: The Development of Higher Psychological Processes(Harvard
University Press) (1978).
34. J. Bruner,Childâ€™s Talk: Learning to Use Language(Oxford University Press) (1983).
35. L. Steels, A self-organizing spatial vocabulary, inArtificial Life IV(MIT Press) (1995), pp.
179â€“184.
26
36. S. Li, R. Miikkulainen, Evolving artificial language using evolutionary reinforcement learning,
inProceedings of the 8th International Conference on the Simulation of Adaptive Behavior
(MIT Press) (2006), pp. 182â€“191.
37. T. Taniguchi, T. Nagai, T. Nakamura, Symbol emergence in cognitive developmental systems:
a survey.IEEE Transactions on Cognitive and Developmental Systems11(4), 494â€“516 (2019).
38. T. J. Tinker, K. Doya, J. Tani, Intrinsic Rewards for Exploration Without Harm From Obser-
vational Noise: A Simulation Study Based on the Free Energy Principle.Neural Computation
36(9), 1854â€“1885 (2024), doi:10.1162/neco a 01690,https://doi.org/10.1162/neco_
a_01690.
39. J. Chung,et al., A recurrent latent variable model for sequential data.Advances in neural
information processing systems28(2015).
40. T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft Actor-Critic: Off-Policy Maximum Entropy
Deep Reinforcement Learning with a Stochastic Actor, inProceedings of the 35th International
Conference on Machine Learning, J. Dy, A. Krause, Eds. (PMLR), vol. 80 ofProceedings of
Machine Learning Research(2018), pp. 1861â€“1870,https://proceedings.mlr.press/
v80/haarnoja18b.html.
41. K. J. Friston, A theory of cortical responses.Philosophical transactions of the Royal Society
B: Biological sciences360(1456), 815â€“836 (2005).
42. R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: a functional interpretation of
some extra-classical receptive-field effects.Nature neuroscience2(1), 79â€“87 (1999).
43. J. Hohwy,The predictive mind(OUP Oxford) (2013).
44. A. Clark,Surfing uncertainty: Prediction, action, and the embodied mind(Oxford University
Press) (2015).
45. C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight uncertainty in neural network,
inInternational Conference on Machine Learning(PMLR) (2015), pp. 1613â€“1622.
27
Acknowledgments
Thank you to the Okinawa Institute of Science and Technology (OIST) and colleagues in OISTâ€™s
Cognitive Neurorobotics Unit and Neural Computation Unit for supporting this work. We also
thank Taro Toyoizumi for valuable discussions on acquiring exception handling rules.
Funding:T.J.T. was funded by OIST graduate school. J.T. was partially funded by the Japan
Society for the Promotion of Science (JSPS) KAKENHI, Transformative Research Area (A):
unified theory of prediction and action [24H02175],
Author contributionsT.J.T. and J.T. designed the model and simulation. T.J.T., K.D., and J.T.
designed the experiments. T.J.T. performed all experiments and data analysis. T.J.T. wrote the paper.
K.D. and J.T. edited the paper. J.T. supervised the study.
Competing interests:Authors declare that they have no competing interests.
Data and materials availability:All files related to this project are available from a repository
at the Okinawa Institute of Science and Technology.
The authors are grateful for the help and support provided by colleagues in OISTâ€™s Cognitive
Neurorobotics Research Unit and Neural Computation Unit.
28
Supplementary Materials for
Curiosity-Driven Development of Action and Language in
Robots Through Self-Exploration
Theodore Jerome Tinker1, Kenji Doya1, Jun Tani1
1 Okinawa Institute of Science and Technology, Okinawa, Japan.
* To whom correspondence should be addressed; E-mail: jun.tani@oist.jp.
This PDF file includes:
Supplementary Text
Figs. S1 to S3
Tables S1 to S8
Other Supplementary Materials for this manuscript:
Video athttps://www.youtube.com/watch?v=Gd1FXeb0ee0
S1
Variable Definition
ğ‘œğ‘¡ Observation at timeğ‘¡
ğ‘œğ‘¡,ğ‘– ğ‘–ğ‘¡â„ part of observationğ‘œğ‘¡
ğ‘œğ‘¡,ğ‘£ Our agentâ€™sğ‘œğ‘¡,0 , vision
ğ‘œğ‘¡,ğ‘¡ğ‘ ğ‘œğ‘¡,1 , touch
ğ‘œğ‘¡,ğ‘ ğ‘œğ‘¡,2 , proprioception
ğ‘œğ‘¡,ğ‘ğ‘¤ ğ‘œğ‘¡,3 , command voice
ğ‘œğ‘¡,ğ‘“ğ‘¤ ğ‘œğ‘¡,4 , feedback voice
ğ‘ğ‘¡ Motor Command
ğ‘Ÿğ‘¡ Extrinsic reward
ğ‘‘ğ‘œğ‘›ğ‘’ğ‘¡ Final step of episode
ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡ Steps inside episode
ğ‘… Recurrent replay buffer
ğœ‹ Actor
ğœ™ Actorâ€™s parameters
ğ‘„ Critic
ğœƒ Criticâ€™s parameter
Â¯ğœƒ Target criticâ€™s parameter
ğœ Criticâ€™s soft update coefficient
Variable Definition
ğ‘“ Forward model
ğœ“ Forward model parameters
ğ›¾ Discount for future rewards
ğ›¼ Importance of motor entropy
ğœ‚ Importance of curiosity
ğœ‚ğ‘– ğœ‚forğ‘– ğ‘¡â„ part of observation
ğ‘(ğ‘§ğ‘¡),ğ‘(ğ‘§ ğ‘¡) Prior, estimated posterior
ğœ‡,ğœ Mean, standard deviation
â„ğ‘¡ RNN hidden state
ğ‘§ğ‘¡ Sample from posterior
ğ‘’ğ‘›ğ‘ğ‘– Encoder forğ‘œğ‘¡,ğ‘–
ğœ“ğ‘’ğ‘›ğ‘
ğ‘– ğ‘“parameters forğ‘’ğ‘›ğ‘ ğ‘–
ğ‘‘ğ‘’ğ‘ğ‘– Decoder forğ‘œğ‘¡,ğ‘–
ğœ“ğ‘‘ğ‘’ğ‘
ğ‘– ğ‘“parameters forğ‘‘ğ‘’ğ‘ ğ‘–
ğ‘€ğ¿ğ‘ƒğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ
ğ‘– Multilayer for prior forğ‘œğ‘¡,ğ‘–
ğ‘€ğ¿ğ‘ƒğ‘ğ‘œğ‘ ğ‘¡
ğ‘– Multilayer for estimated
posterior forğ‘œğ‘¡,ğ‘–
Table S1.Definitions of variables.
Supplementary Text
For future reference, table S1 includes definitions for all relevant variables.
Free energy principle, Active Inference, and Kawahara Model
We begin by describing predictive coding and active inference (AIF), which are grounded in the
free energy principle (FEP) (41). The FEP posits that biological and artificial agents maintain
their existence by minimizing variational free energy, which is an upper bound on sensory sur-
S2
prise. In perception, this process is often instantiated as predictive coding (41, 42, 43, 44), wherein
internal models reconstruct sensory inputs by updating beliefs or latent variables by minimizing
the reconstruction errors. More formally, this is minimizing evidence free energy defined for past
observations. In motor command generation, the FEP framework extends to AIF (14, 16), where
agents minimize the future prediction error (quantified as expected free energy) by optimizing the
latent variables and motor commands in the future. These two processes are tightly coupled and
must be considered jointly in embodied cognition systems.
We next introduce the work of Kawahara et al. (17), who proposed a novel reinforcement
learning (RL) scheme that integrates (AIF).
In the Bayesian framework, the true posterior probability distributionğ‘(ğ‘§ ğ‘¡|ğ‘œğ‘¡)over latent
variablesğ‘§ğ‘¡, conditioned on sensory observationsğ‘œğ‘¡, is given by Bayesâ€™ rule:
ğ‘(ğ‘§ğ‘¡|ğ‘œğ‘¡)= ğ‘(ğ‘œğ‘¡|ğ‘§ğ‘¡)ğ‘(ğ‘§ğ‘¡)âˆ«
ğ‘(ğ‘œğ‘¡,ğ‘§ğ‘¡)ğ‘‘ğ‘§
Here,ğ‘(ğ‘§ ğ‘¡)denotes the prior. The denominator, called the evidence, is usually intractable; to
overcome this, variational Bayes introduces an estimate of the posteriorğ‘(ğ‘§ ğ‘¡). This is optimized
to minimize the Kullback-Leibler divergence (KLD) between the estimated posteriorğ‘(ğ‘§ğ‘¡)and the
true posteriorğ‘(ğ‘§ ğ‘¡|ğ‘œğ‘¡).
ğ·ğ¾ğ¿[ğ‘(ğ‘§ğ‘¡)||ğ‘(ğ‘§ ğ‘¡|ğ‘œğ‘¡)]=
âˆ«
ğ‘(ğ‘§ğ‘¡)log ğ‘(ğ‘§ğ‘¡)
ğ‘(ğ‘§ğ‘¡|ğ‘œğ‘¡)ğ‘‘ğ‘§ğ‘¡
=
âˆ«
ğ‘(ğ‘§ğ‘¡)log ğ‘(ğ‘§ğ‘¡)ğ‘(ğ‘œğ‘¡)
ğ‘(ğ‘§ğ‘¡,ğ‘œğ‘¡) ğ‘‘ğ‘§ğ‘¡
=
âˆ«
ğ‘(ğ‘§ğ‘¡)log ğ‘(ğ‘§ğ‘¡)ğ‘(ğ‘œğ‘¡)
ğ‘(ğ‘§ğ‘¡)ğ‘(ğ‘œğ‘¡|ğ‘§ğ‘¡)ğ‘‘ğ‘§ğ‘¡ (S1)
=ğ¹+logğ‘(ğ‘œ ğ‘¡)(S2)
The termğ¹here is the evidence free energy, equal to
ğ¹ğ‘¡ =ğ· ğ¾ğ¿[ğ‘(ğ‘§ğ‘¡)||ğ‘(ğ‘§ ğ‘¡)]|                  {z                  }
Complexity
âˆ’E ğ‘(ğ‘§ğ‘¡ )[logğ‘(ğ‘œ ğ‘¡+1 |ğ‘§ğ‘¡)]
|                     {z                     }
Accuracy
.(S3)
Sinceğ‘(ğ‘œ ğ‘¡)is constant for a given sensory observation, minimizing KLD is equivalent to minimiz-
S3
ingğ¹ğ‘¡. Therefore, the optimal posterior estimate is:
ğ‘âˆ—(ğ‘§ğ‘¡)=arg min
ğ‘(ğ‘§ğ‘¡ )
ğ¹ğ‘¡ (S4)
In active inference, the agent minimizes expected free energyğºğœ at a future time stepğœâ‰¥ğ‘¡+1.
This is the expected value of the evidence free energy under the predictive distribution of future
outcomes (17).
ğºğœ =E ğ‘(ğ‘œğœ |ğ‘§ğœ)[ğ¹]
=E ğ‘(ğ‘œğœ |ğ‘§ğœ)[
âˆ«
ğ‘(ğ‘§ğœ)log ğ‘(ğ‘§ğœ)
ğ‘(ğ‘œğœ,ğ‘§ğœ)ğ‘‘ğ‘§]
=E ğ‘(ğ‘œğœ |ğ‘§ğœ)[Eğ‘(ğ‘§ğœ)[log ğ‘(ğ‘§ğœ)
ğ‘(ğ‘§ğœ|ğ‘œğœ)âˆ’logğ‘(ğ‘œ ğœ)]].(S5)
Recalling thatğ‘(ğ‘§ ğœ|ğ‘œğœ)ğ‘(ğ‘œğœ)=ğ‘(ğ‘œ ğœ,ğ‘§ğœ), we approximate:
ğºğœ â‰ˆE ğ‘(ğ‘œğœ,ğ‘§ğœ)[log ğ‘(ğ‘§ğœ)
ğ‘(ğ‘§ğœ|ğ‘œğœ)âˆ’logğ‘(ğ‘œ ğœ)]
=âˆ’E ğ‘(ğ‘œğœ,ğ‘§ğœ)[log ğ‘(ğ‘§ğœ|ğ‘œğœ)
ğ‘(ğ‘§ğœ) ]âˆ’E ğ‘(ğ‘œğœ)[logğ‘(ğ‘œ ğœ)]
=âˆ’E ğ‘(ğ‘œğœ)[
Bayesian Surprise
z                       }|                       {
ğ·ğ¾ğ¿[ğ‘(ğ‘§ğœ|ğ‘œğœ)||ğ‘(ğ‘§ ğœ)]]
|                                  {z                                  }
Epistemic Value or Mutual Information
âˆ’E ğ‘(ğ‘œğœ)[logğ‘(ğ‘œ ğœ)]
|                {z                }
Extrinsic Value
.(S6)
The first term,ğ¼(ğ‘§ ğœ,ğ‘œğœ)=E ğ‘(ğ‘œğœ)[ğ·ğ¾ğ¿[ğ‘(ğ‘§ğœ|ğ‘œğœ]||ğ‘(ğ‘§ ğœ)]], is the mutual information (or Bayesian
surprise). This depicts expected information gain based on new sensory observationğ‘œğœ, and can be
expressed as:
ğ¼(ğ‘§ğœ,ğ‘œğœ)=ğ»(ğ‘§ ğœ)|{z}
Shannon Entropy
âˆ’ğ»(ğ‘§ ğœ|ğ‘œğœ)|     {z     }
Conditional Entropy
.
The second term,ğ‘(ğ‘œ ğœ), represents log-likelihood of the preferred sensory observation. This is
specified as the extrinsic reward designed by the experimenters. For the intrinsic value to reflect
mutual information or information gain, and the extrinsic value to reflect expected free energy, is
the same as the way shown by Fristonâ€™s group in the study of active inference (16, 27). Separating
ğ‘œğ‘¡ intoğ‘œğ‘¡ andğ‘ğ‘¡, we rewrite the expected free energy as:
S4
ğºğœ =âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ,ğ‘§ğœ)[log ğ‘(ğ‘§ğœ|ğ‘œğœ,ğ‘ğœ)
ğ‘(ğ‘§ğœ) ]âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[logğ‘(ğ‘œ ğœ,ğ‘ğœ)]
=âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ,ğ‘§ğœ)[log ğ‘(ğ‘§ğœ,ğ‘ğœ|ğ‘œğœ)
ğ‘(ğ‘§ğœ)ğ‘(ğ‘ğœ|ğ‘œğœ)]âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[logğ‘(ğ‘œ ğœ,ğ‘ğœ)]
â‰ˆâˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ,ğ‘§ğœ)[log ğ‘(ğ‘§ğœ|ğ‘œğœ)ğ‘(ğ‘ğœ|ğ‘œğœ,ğ‘§ğœ)
ğ‘(ğ‘§ğœ)ğ‘(ğ‘ğœ|ğ‘œğœ) ]âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[logğ‘(ğ‘œ ğœ,ğ‘ğœ)]
=âˆ’E ğ‘(ğ‘ğœ |ğ‘œğœ,ğ‘§ğœ)ğ‘(ğ‘œğœ)[ğ·ğ¾ğ¿[ğ‘(ğ‘§ğœ|ğ‘œğœ)||ğ‘(ğ‘§ ğœ)]]
âˆ’E ğ‘(ğ‘œğœ,ğ‘§ğœ)[ğ·ğ¾ğ¿[ğ‘(ğ‘ğœ|ğ‘œğœ,ğ‘§ğœ)||ğ‘(ğ‘ ğœ|ğ‘œğœ)]]
âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[logğ‘(ğ‘œ ğœ,ğ‘ğœ)].(S7)
Kawahara et al. developed a forward modelğ‘“ ğ‘¤(ğ‘œğœ,ğ‘ğœ)â†’bğ‘œğœ+1 which learns to predict the
future sensory observationğ‘œ ğœ+1 basedğ‘œ ğœ andğ‘ ğœ using a Bayesian Neural Network (BNN) (45).
In this type of model, the network parametersğ‘¤ ğœ are treated as random variables defined with
gaussian distribution. These parameters serve as latent causes of observed sensory transitions and
can be interpreted as random latent variables for the generative model. Therefore,ğ‘¤ğœ corresponds
toğ‘§ğœ.
Let the approximate posterior be defined asğ‘ğœ“ =N(ğ‘¤ ğœ|ğœ‡,ğœ), with parametersğœ“={ğœ‡,ğœ}. In
this setting, the actorğœ‹ ğœ™ of a SAC can be trained to approximateğœ‹ ğœ™(ğ‘ğœ|ğ‘œğœ)â‰ˆğ‘(ğ‘ ğœ|ğ‘œğœ,ğ‘¤ğœ). This
allows rewriting the expected free energy as:
ğº(ğ‘œğœ,ğ‘ğœ)=âˆ’E ğ‘(ğ‘ğœ |ğ‘œğœ,ğ‘¤ğœ)ğ‘(ğ‘œğœ)[ğ·ğ¾ğ¿[ğ‘(ğ‘¤ğœ|ğ‘œğœ)||ğ‘(ğ‘¤ ğœ)]]
âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[ğ·ğ¾ğ¿[ğœ‹ğœ™(ğ‘ğœ|ğ‘œğœ)||ğ‘(ğ‘ ğœ|ğ‘œğœ)]]
âˆ’E ğ‘(ğ‘œğœ,ğ‘ğœ)[logğ‘(ğ‘œ ğœ,ğ‘ğœ)].(S8)
Let us interpret the prior preference logğ‘(ğ‘œ ğœ,ğ‘ğœ)as the extrinsic rewardğ‘Ÿ(ğ‘  ğœ,ğ‘ğœ), whereğ‘  ğœ is the
true environmental state. Bring focus to the current time step by settingğœ=ğ‘¡. Because the forward
model trains to predictğ‘œğ‘¡+1, we can further rewrite the expected free energy as:
S5
ğº(ğ‘œğ‘¡,ğ‘ğ‘¡)=âˆ’ğ· ğ¾ğ¿[ğ‘ğœ“(ğ‘¤ğ‘¡|ğ‘œğ‘¡+1)||ğ‘ğœ“(ğ‘¤ğ‘¡)]âˆ’logğ‘(ğ‘œ ğ‘¡,ğ‘ğ‘¡)
âˆ’ğ·ğ¾ğ¿[ğœ‹ğœ™(ğ‘ğ‘¡|ğ‘œğ‘¡)||ğ‘(ğ‘ ğ‘¡|ğ‘œğ‘¡)]
=âˆ’ğ· ğ¾ğ¿[ğ‘ğœ“(ğ‘¤ğ‘¡|ğ‘œğ‘¡+1)||ğ‘ğœ“(ğ‘¤ğ‘¡)]âˆ’logğ‘(ğ‘œ ğ‘¡,ğ‘ğ‘¡)
âˆ’
âˆ«
ğœ‹ğœ™(ğ‘ğ‘¡|ğ‘œğ‘¡)logğœ‹ ğœ™(ğ‘ğ‘¡|ğ‘œğ‘¡)ğ‘‘ğ‘ğ‘¡ +
âˆ«
ğœ‹ğœ™(ğ‘ğ‘¡|ğ‘œğ‘¡)logğ‘(ğ‘ ğ‘¡|ğ‘œğ‘¡)ğ‘‘ğ‘ğ‘¡
=âˆ’ğ· ğ¾ğ¿[ğ‘ğœ“(ğ‘¤ğ‘¡|ğ‘œğ‘¡+1)||ğ‘ğœ“(ğ‘¤ğ‘¡)]
|                              {z                              }
Curiosity
âˆ’ğ‘Ÿ(ğ‘  ğ‘¡,ğ‘ğ‘¡)|   {z   }
Extrinsic Reward
âˆ’H(ğœ‹ ğœ™(ğ‘ğ‘¡|ğ‘œğ‘¡))
|           {z           }
Entropy
âˆ’E ğœ‹ğœ™ (ğ‘ğ‘¡ |ğ‘œğ‘¡ )[logğ‘(ğ‘ âˆ—
ğ‘¡|ğ‘œğ‘¡)]
|                        {z                        }
Imitation
(S9)
Becauseğ‘¤ ğœ represents the robotâ€™s probabilistic knowledge of their environment, the first term of
Eq. S9 can be said to represent the robotâ€™s gain in knowledge based on information acquired in a
new sensory observation.
In summary, the forward model is trained to minimize the evidence free energyğ¹(Eq. S3)
by accurately reconstructing sensory observations and minimizing posterior complexity based on
past experiences. Meanwhile, the actor-critic pair is trained to minimize expected free energy
ğº, which includes an inverted complexity term (i.e., curiosity) and motor entropy to encourage
exploration. This leads to emergent tension in an adversarial relationship: the actor is encouraged
to maximize information gain by increasing the KL divergence between prior and posterior, which
the forward model trains to minimize that same term. This establishes a dynamic push-pull effect,
driving self-organized exploration. Please note that the imitation term in Eq. S9 depends on
external demonstrations or expert policies; this term is ignored in our study, which focuses on
self-exploration.
From this formulation of expected free energy, theğ‘„-value can be updated as:
ğ‘„(ğ‘¡)=ğ‘Ÿ ğ‘¡ +ğœ‚ğ· ğ¾ğ¿[ğ‘ğœ“(ğ‘¤ğ‘¡|ğ‘œğ‘¡+1)||ğ‘ğœ“(ğ‘¤ğ‘¡)]+
ğ›¾(1âˆ’ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡)Eğ‘œğ‘¡+1 âˆ¼ğ·,ğ‘ğ‘¡+1 âˆ¼ğœ‹ğœ™ [ğ‘„Â¯ğœƒ(ğ‘œğ‘¡+1,ğ‘ğ‘¡+1)]+ğ›¼H(ğœ‹ ğœ™(ğ‘ğ‘¡+1 |ğ‘œğ‘¡+1))(S10)
Here,ğœ‚ >0 andğ›¼ >0 are hyperparameter weighting the intrinsic reward based on the curiosity
and the motor entropy, respectively.
S6
In our experiments, each episode ended after 30 steps, or terminated earlier if the agent success-
fully executed the command. Completed episodes are stored in a recurrent replay buffer, which can
hold up to 256 episodes. When the buffer is full, the buffer discards the oldest episodes to accom-
modate new episodes. To ensure uniform episode length, all episodes were padded to 30 steps with
empty transitions. Hence, transitions are stored with the form{ğ‘œğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡,ğ‘œğ‘¡+1,ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡,ğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡}, where
ğ‘šğ‘ğ‘ ğ‘˜ğ‘¡ =1 for real transitions, andğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡ =0 for empty transitions added for padding. After each
episode, a batch of 32 episodes was sampled from the buffer and used to train the forward model,
actor, and critics. During training, loss terms were multiplied byğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡, removing the influence of
empty transitions.
Details of the Model Architecture
This subsection explains further details about the model architecture employed in this current study.
As noted earlier, the present architecture extends our previous model (38), which is described in
the â€œFree energy principle, Active Inference, and Kawahara Modelâ€ section of the Supplementary
Materials. The primary extension involves the use of separate random latent variables, encoders,
and decoders for each sensory modality. This design allows the model to process multiple types
of sensation independently, including vision, tactile input, proprioception, command voice, and
feedback voice. In addition, our model uses an encoder for the 4-dimensional motor command,
which includes motor velocities for two the robotâ€™s wheels and two joint angles in its arm. The full
architecture of the proposed model is shown in Fig. S1.
Computation in this architecture proceeds as follows:
1. The 4-dimensional motor command from the previous time step is fed into the motor command
encoder, producing an encoded motor command vector.
2. The prior distribution for the current time step is computed using the encoded motor command
vector and the previous hidden state.
3. The sensory observation for each modality is fed through its corresponding encoder, com-
puting its modality-specific encoded vector.
S7
Fig. S1.The details of the proposed model architecture.
4. The estimated posterior distribution for each modality is estimated using its sensory encoded
vector, encoded motor command vector, and the previous hidden state.
5. All posterior vectors from the current time step are concatenated across all modalities, then
sampled and combined with the previous hidden state to compute the current hidden state.
6. The motor command for the current time step is generated from the current hidden state using
S8
the actor (policy network).
7. The model predicts the next sensory observation for each modality using the current hidden
state and the current motor command, passed through the corresponding sensory decoders.
8. Theğ‘„ ğ‘¡ value is updated according to Eq. 2.
9. If the episode terminates at this step, the episodeâ€™s data is saved in a recurrent replay buffer. A
batch of information is sampled from the buffer to train the forward model, actor, and critic.
See the Supplementary Materials for more details.
Details of the encoders and decoders of each sensory modality (e.g., vision, tactile sensation,
etcetera), as well as the motor command encoder, are described in the â€œImplementation detailsâ€
section of the Supplementary Materials.
Vision
The robot visually senses the environment in the direction the robot faces with a 16ğ‘¥16ğ‘¥4 image,
with the four channels being red, green, blue, and distance. See Fig. S2.
Fig. S2.The agentâ€™s vision,o ğ’•,ğ’— .The robot is facing a magenta cone and a green pillar. The robot
also sees part of its hand. The image on the left depicts the red, green, and blue channels. The image
on the right depicts the distance.
In our proposed model, in order to make the estimated posterior for visual sensations, images
are flattened and encoded using a linear neural network with Parametric Rectified Linear Unit
activation (PReLU). To generate a prediction of the next image,â„ğ‘
ğ‘¡ andğ‘ğ‘’ğ‘›ğ‘
ğ‘¡ are concatenated and
decoded with another linear neural network, shaped into a 16ğ‘¥16ğ‘¥4 tensor, and finished with a
convolutional layer. See details in table S2.
S9
Layer Type Activation Details
Encoder,ğ‘’ğ‘›ğ‘ğ‘£
1 Flatten Shape (16, 16, 4) to shape (1024).
2 Linear PReLU To shape (128).
Decoder,ğ‘‘ğ‘’ğ‘ ğ‘£
1 Linear BatchNorm2d, PReLU From shape (264) to shape (8 * 8 * 64).
2 Reshaping To shape (8, 8, 64).
3 CNN Tanh
Kernel size 3, reflective padding 1.
To shape (8, 8, 8).
4 Pixel Shuffle To shape (16, 4, 4).
Table S2.Encoder and decoder of agentâ€™s visual sensations,o ğ’•,ğ’— .
Touch
The second part of the sensory observation is the tactile sensation of touch. This is represented by
a one value between 0 and 1 for each of the 16 sensors. Each value is equal to the fraction of time
in the previous step during which the respective sensor was in contact with an object. See Fig. S3.
Fig. S3.The agentâ€™s sensors for tactile sensations of touch,oğ’•,ğ’•ğ’‚ .The robot has 16 sensors, which
are planes on the surface of the robotâ€™s body, arm, and hand. The camera and wheels are marked
just for clarity.
In our proposed model, in order to make the estimated posterior for tactile sensation, the tensor
is encoded using a linear neural network with PReLU. To generate a prediction of the next tactile
S10
sensation,â„ğ‘
ğ‘¡ andğ‘ğ‘’ğ‘›ğ‘
ğ‘¡ are concatenated and decoded with another linear neural network. See details
in table S3.
Layer Type Activation Details
Encoder,ğ‘’ğ‘›ğ‘ğ‘¡ğ‘
1 Linear BatchNorm2d, PReLU From shape (16) to shape (20).
Decoder,ğ‘‘ğ‘’ğ‘ğ‘¡ğ‘
1 Linear BatchNorm2d, TanH
From shape (264) to shape (16).
Result added to 1 and divided by 2
for values between 0 and 1.
Table S3.Encoder and decoder of agentâ€™s tactile sensations,o ğ’•,ğ’•ğ’‚ .
Proprioception
The third part of the sensation is the angle and velocity of the armâ€™s joints. (The velocity of the joint
may not match the robotâ€™s motor commands, because collisions with objects may restrain it.) This
consists of a tensor with four values between 0 and 1: two joint angles and two joint velocities. Each
value is the normalized proportion of the respective variable between its minimum and maximum
range.
In our proposed model, in order to make the estimated posterior for sensation of proprioception,
the tensor is encoded using a linear neural network with PReLU. To generate a prediction of the
next proprioception,â„ğ‘
ğ‘¡ andğ‘ğ‘’ğ‘›ğ‘
ğ‘¡ are concatenated and decoded with another linear neural network.
See details in table S4.
Voices
The fourth and fifth parts of the sensation are the command voice and the tutor-feedback voices,
which were described briefly in the Results section. Both voices are sequences of one-hot vectors.
Table S5 displays the 18 terms (including silence) and their indexes in the one-hot vectors. For
example, the command â€œWatch the Red Pillarâ€ is represented by
S11
Layer Type Activation Details
Encoder,ğ‘’ğ‘›ğ‘ ğ‘ğ‘œ
1 Linear BatchNorm2d, PReLU From shape (4) to shape (4).
Decoder,ğ‘‘ğ‘’ğ‘ ğ‘ğ‘œ
1 Linear BatchNorm2d, TanH
From shape (264) to shape (4).
Result added to 1 and divided by 2
for values between 0 and 1.
Table S4.Encoder and decoder of agentâ€™s sensation of proprioception,o ğ’•, ğ’‘.
[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]
[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0].
(S11)
If the robot has not performed any action, then the feedback voice is only one one-hot vector
indicating silence:
[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]. (S12)
The robotâ€™s forward modelâ€™s encoding of these two voices has two parts. The first part of the
encoding is an embedding and recurrent neural network. This part is identical for the command
voice and the feedback voices, ensuring that tokens are interpreted consistently across sources.
Note that this RNN is â€œnestedâ€ within the forward modelâ€™s RNN, such that each of the robotâ€™s steps
includes three steps of interpreting voices. See Fig. S4. In the second part of the encoding, outputs
from the first part of the encoding are processed with unique linear layers to produce separate
estimated posteriors. To generate a prediction of the next voices,â„ğ‘
ğ‘¡ andğ‘ğ‘’ğ‘›ğ‘
ğ‘¡ are concatenated and
decoded using two separate recurrent neural networks for the command voice and feedback voice.
See details in table S6.
S12
English Word Indexes
Index Word
0 (Silence)
1 Watch
2 Be Near
3 Touch the Top
4 Push Forward
5 Push Left
6 Push Right
Index Word
7 Red
8 Green
9 Blue
10 Cyan
11 Magenta
12 Yellow
13 Pillar
14 Pole
15 Dumbbell
16 Cone
17 Hourglass
Table S5.English words and indexes. The English words used and their positions in one-hot
vectors.
Fig. S4.Recurrent step shared by command voice and feedback voice.
S13
Layer Type Activation Details
Encoder part one,ğ‘’ğ‘›ğ‘ ğ‘¤ (shared by command voice and feedback voice)
1 Embedding PReLU
From shape (Sequence-length, 18)
to shape (Sequence-length, 8).
2 Linear PReLU To shape (Sequence-length, 64).
3 GRU PReLU To shape (64).
4 Linear PReLU To shape (256).
Decoders,ğ‘‘ğ‘’ğ‘ ğ‘ğ‘¤ andğ‘‘ğ‘’ğ‘ ğ‘“ğ‘¤
1 Linear BatchNorm2d, PReLU From shape (264) to shape (192).
2 Reshaping To shape (3, 64).
3 GRU PReLU To shape (3, 64).
4 Linear To shape (3, 17).
Table S6.Encoder and decoder of agentâ€™s voice sensation,o ğ’•,ğ’„ğ’˜ ando ğ’•, ğ’‡ ğ’˜.
Motor Command Encoder
For usage in the forward model, the robotâ€™s motor commandsğ‘ğ‘¡ are encoded intoğ‘ğ‘’ğ‘›ğ‘
ğ‘¡ with a linear
neural network with PReLU. See details in table S6.
Layer Type Activation Details
Encoder,ğ‘’ğ‘›ğ‘ğ‘
1 Linear PReLU From shape (4) to shape (8).
Table S7.Encoding motor command for forward model.
Constraints in Performing Actions
In each step, the robot can only perform one of the six actions. This is implemented using definitions
of actions and action prioritization. The actions Watch, Be Near, and Touch the Top cannot be
performed simultaneously because of requirements regarding distance from the object and touching
the object. The actions Push Left and Push Right cannot be performed simultaneously because of
S14
the directions of movements. If the robot satisfies the requirements for Touch the Top, we reject the
actions Push Forward, Push Left, or Push Right. If the robot is performing Push Forward and Push
Left or Push Right, we allow only the actions with the greatest distance pushed.
Details of Experiment Design
10 robots trained in each way described in the Results section: no curiosity, sensory-motor curiosity,
and all curiosity. The robots trained for 60000 epochs. In each epoch, the robot performed one
episode which was saved in its recurrent replay buffer. Then the robot trained with a batch of 32 of
its saved episodes.
Experiment 1
Experiment 1 tests the effects of curiosity. We trained robots with three levels of curiosity: no
curiosity, sensory-motor curiosity, and all curiosity. In table S8, we share the value of theğœ‚
hyperparameters for each of the four parts of the sensory observation which may be explored.
These represent the relative importance of each part of the sensory observation in the robotâ€™s
curiosities.
Name ğœ‚ğ‘£ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğœ‚ğ‘¡ğ‘œğ‘¢ğ‘â„ ğœ‚ğ‘ğ‘Ÿğ‘œğ‘ğ‘Ÿğ‘–ğ‘œğ‘ ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğœ‚ğ‘“ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜
No Curiosity 0 0 0 0
Sensory-Motor Curiosity .05 2 .1 0
Complete Curiosity .05 2 .1 .3
Table S8.Hyperparameters for three types of agents.
We measured the success-rates of these three types of robots in the six types of actions. The
plots in Fig. 2 show the rolling average of success-rates of the three types robots from the beginning
of training to the end of training after 60000 epochs, with 99% confidence intervals. Specifically,
the plots show results of the robots regarding the goals with combinations of verb, adjective, and
noun which the robots were not shown in training, testing for the ability to generalize vocabulary
and syntax to unlearned combinations.
S15
As we predicted in hypothesisğ‘–, robots with no curiosity performed the worst, with approxi-
mately 25% success-rate; robots with curiosity for sensory-motor observations performed better,
with approximately 75% success-rate; and robots with curiosity for sensory-motor observations and
the feedback voice are the best, with approximately 90% success-rate. As we predicted in hypothesis
ğ‘–ğ‘–, the robotâ€™s ability to perform simpler actions develop earliest, and the robotâ€™s ability to perform
more complex actions develop later, having required the simpler actions as prerequisites. Merely
watching the object appears to be the simplest, developing earliest, while pushing object the object
to the left or right appear to be the most complex, developing later.
Experiment 2
Experiment 2 tests the relationship between success-rates with learned goals and unlearned goals,
specifically by robots using all curiosity. See Fig. 6. The left column shows success-rate plots
of robots with learned actions, while the right column shows success-rate plots of robots with
unlearned actions. The first row shows results for robots using the complete vocabulary: 6 verbs,
6 adjectives, and 5 nouns. The second and third row show results for robots trained with smaller
vocabularies. In each of the three situations, the robots are trained with one third of the possible
goals, and tested with the other two thirds.
As we predicted in hypothesisğ‘–ğ‘–ğ‘–, the robotâ€™s success-rates with learned actions initiates earlier
than its success-rates with unlearned actions. This suggests pairing sentences of words precedes
generalization with compositionality. As we predicted in hypothesisğ‘–ğ‘£, larger vocabularies lead
to faster generalization. All three collections of robots had success-rates of approximately 100%
with learned actions. Robots which were trained with 60 of the 180 possible goals with 6 verbs,
6 adjectives, and 5 nouns had success-rates of approximately 90% with unlearned actions. Robot
which were trained with 25 of the 75 possible goals with 5 verbs, 5 adjectives, and 3 nouns had
success-rates of approximately 50% with unlearned actions. And robots which were trained with 16
of the 48 possible goals with 4 verbs, 4 adjectives, and 3 nouns has success-rates of approximately
30% with unlearned actions. The ability to generalize quickly is enhanced with the size of the
vocabulary in use.
Movie S1. Example of training.Compares robot with curiosity mid-training and after training.
S16
Statistical Analysis of U-Shaped Patterns
To quantify U-shaped learning in exception handling, we scored the U-shaped structure of success-
rateâ€™s trajectories identifying non-monotonic developmental patterns consistent with representa-
tional redescription (21). The method combines robust smoothing, normalized scaling, and piece-
wise isotonic regression to fit a two-phase model with a central valley.
Consider one robotâ€™s rolling-average success rate over training epochs for goals which are
exceptions. The U-shape score is computed as follows:
1.Burn-in removal.The first 10% of training data is removed to avoid initialization noise.
2.Smoothing.The curve is smoothed using a Savitzkyâ€“Golay filter with a window length of
approximately 3% of the series, reducing spurious local fluctuations.
3.Normalization.The smoothed curve is linearly scaled to the[0,1]range using the 5th and
95th percentiles to ensure robustness across success-rate ranges.
4.Valley localization.The minimum pointğ‘– ğ‘€ is located between 20% and 80% of the sequence
length.
5.Piecewise isotonic regression.For each candidate split pointğ‘˜near the valley (withinÂ±25%
of the series), the left segment is fit with a decreasing isotonic regression and the right
segment with an increasing isotonic regression. A cost function is minimized:
Cost(ğ‘˜)=MSE(ğ‘˜)+ğœ†Â·(drift from valley) 2 Â·MSE base,
whereğœ†=2.0 penalizes drifting too far from the identified valley. Indices ofğ‘˜define the left
peakğ‘–ğ¿ and right peakğ‘–ğ‘….
6.Score calculation.If the best split passes depth and width criteria (minimum 3% depth, 6%
width), a composite U-score is computed:
U-score=0.6Â·improvement+0.25Â·depth+0.15Â·width,
where:
â€¢Improvementis the fractional MSE reduction relative to the best monotonic baseline fit.
S17
â€¢Depthis the drop from the valley to the 90th percentile of the surrounding peaks.
â€¢Widthis the relative proportion of the sequence before/after the valley.
7.Index reporting.Indices of the left peakğ‘– ğ¿, valleyğ‘–ğ‘€, and right peakğ‘–ğ‘… are marked with red
vertical lines in Figure 7.
To compare robots training with exceptions and without exceptions, we computed U-shape
scores for each robot individually and compared the two groups using a one-tailed Welchâ€™sğ‘¡-test
(unequal variances):
ğ‘¡= Â¯ğ‘¥1 âˆ’Â¯ğ‘¥2âˆšï¸‚
ğ‘ 2
1
ğ‘›1
+
ğ‘ 2
2
ğ‘›2
.
The resulting test statistic confirmed that robots trained with exceptions exhibited significantly
stronger U-shaped profiles than those without, withğ‘=0.0025.
S18