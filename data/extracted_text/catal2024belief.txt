Belief sharing: a blessing or a curse
Ozan Çatal1, Toon Van de Maele1, Riddhi J. Pitliya1,2, Mahault Albarracin1,3,
Candice Pattisapu1, and Tim Verbelen1
1 VERSES Research Lab, Los Angeles, California, 90016, USA
2 Department of Experimental Psychology, University of Oxford, Oxford, UK
3 Department of Computer Science, Université du Québec à Montréal, Montréal,
Canada
ozan.catal@verses.ai
Abstract. When collaborating with multiple parties, communicating
relevant information is of utmost importance to efficiently completing
the tasks at hand. Under active inference, communication can be cast as
sharing beliefs between free-energy minimizing agents, where one agent’s
beliefs get transformed into an observation modality for the other. How-
ever, the best approach for transforming beliefs into observations remains
an open question. In this paper, we demonstrate that naively sharing
posterior beliefs can give rise to the negative social dynamics of echo
chambers and self-doubt. We propose an alternate belief sharing strat-
egy which mitigates these issues.
Keywords: Active inference· Belief sharing· Multi-agent systems.
1 Introduction
Communication and the emergence of language have been a cornerstone in the
development of human intelligence, as they enable human collaboration at mul-
tiple communal scales [1]. This collaborative capability hinges significantly on
how agents share and process information, particularly requiring their internal
beliefs about the world to be aligned [2,3]. Active inference provides a compelling
framework for understanding and designing such collaborative interactions in the
interest of building ecosystems of intelligence [4,5,6]. In this paradigm, agents
minimize variational free energy [7], as each agent maintains a generative model
of the world which it uses to make inferences about hidden states and to plan
actions. Then, communication between agents at the lowest level can be concep-
tualized as sharing these internal beliefs, transforming the beliefs of one agent
into observable data for another [8], these beliefs can be shared directly as we will
demonstrate in this paper, but could also be present in the environment more
permanently in the form of scripts[9] and texts [10,11]. This belief-sharing mech-
anism is intended to facilitate a more coherent and efficient joint exploration of
the environment.
However, translating and sharing beliefs has its challenges, as the messages
shared are typically colored by one’s personal priors and biases [12]. We found
arXiv:2407.02465v1  [cs.AI]  2 Jul 2024
2 O. Çatal et al.
that naively sharing posterior beliefs can inadvertently lead to detrimental social
dynamics, such as echo chambers, in which agents reinforce each other’s biases,
and self-doubt, in which agents discount their observations to favor shared, yet
incorrect, beliefs. These phenomena can significantly impair the collective per-
formance of the agents, highlighting the need for more sophisticated strategies
in belief communication. In this paper, we explore these dynamics in depth. We
begin by modeling the communication between agents as belief sharing under
the active inference framework, demonstrating the pitfalls of straightforward be-
lief sharing. We then propose an alternative strategy that mitigates these issues
by adjusting how beliefs are communicated. Specifically, we advocate for sharing
likelihood information rather than posterior beliefs, treating other agents’ ob-
servations as additional independent sources of information. This approach aims
to harness the benefits of collaborative inference while avoiding the pitfalls of
misleading belief reinforcement.
Our contributions are threefold: (i) We provide a detailed analysis of how
naive belief-sharing can lead to echo chambers and self-doubt. (ii) We propose a
novel communication strategy that mitigates these issues by sharing likelihoods.
(iii) We validate our approach through simulations, demonstrating improved
performance and robustness in collaborative tasks. The following sections out-
line our active inference model for communication, describe the experimental
setup used to test our hypotheses, present our findings on echo chambers and
self-doubt, and discuss our proposed solution and its implications for designing
collaborative AI systems.
2 An active inference model for communication
In this section, we provide a summary overview of active inference, and how it
can be adopted to model communication between agents. For a more in depth
overview of active inference we refer the reader to [7].
2.1 Perception and planning as inference
Active inference posits that agents entertain a generative model of the environ-
ment they operate in, and casts perception and action as Bayesian inference [7].
In general, the agent’s generative model can be written as the joint probability
distribution over statess, observations o and actions a, with tilde denoting a
time sequence of those over timestepst:
P(˜s, ˜o, ˜a) =P(s0)
Y
t
P(ot|st)P(st|st−1, at−1)P(at−1) (1)
Perception now becomes inferring the posterior distributions of states given
the performed actions and observations. As this is typically intractable, agents
resort to variational Bayesian inference, where an approximate posteriorQ(˜s|˜o)
is optimized instead, by minimizing the variational Free Energy:
Belief sharing: a blessing or a curse 3
F = DKL[Q(˜s|˜o))||P(˜s|˜a, ˜o)]| {z }
posterior approximation
− log P(˜o)| {z }
log evidence
= DKL[Q(˜s|˜o))||P(˜s, ˜a)]| {z }
complexity
−EQ(˜s|˜o)[log P(˜o|˜s)]| {z }
accuracy
(2)
It is clear that minimizing variational Free Energy is equivalent with maxi-
mizing a bound on the (log) evidence or ELBO [13], and encourages the model
to maximize accuracy with minimal complexity.
To interact with the environment, an agent also needs to select a sequence
of actions or policyπ = {at, at+1, ...} to execute. In active inference, planning is
also treated as inference, assuming that agents will prefer policies that minimize
expected Free EnergyG. More specifically, policies are selected from
P(π) =σ(−G(π)), with
G(π) =
TX
τ=t+1
EQ(oτ|π)

DKL[Q(sτ |oτ , π)||Q(sτ |π)]

| {z }
(negative) Information Gain
−EQ(oτ|π)

log P(oτ )

| {z }
Utility
(3)
Here, σ denotes the softmax function, and the expected Free Energy balances
information gain with some prior preference distribution over future outcomes
or utility.
2.2 Communication as belief sharing
When agents share a common world and world model, they can benefit from
sharing beliefs among each other [8]. The most straightforward way to realize
this would be to share the agent’s respective posterior beliefs on some shared
modality. In order to achieve this, the agents generative model is expanded as
shown in Fig. 1. In particular, any agent, for example., the primary focal agent,
assumes other agents with a similar generative model will communicate infor-
mation about its beliefs of the world. To do so, we equip the focal agent with an
extra observation modalityos
t . Instead of being observed from the environment,
os
t is an observation generated by another agent based on its internal beliefss′
t.
This approach is the kind of model posited in earlier work [8].
To realize posterior belief-sharing, agents require a likelihood mapping be-
tween posterior beliefs about latent states that are shareable among agents, i.e.,
st, and this observation modalityos
t . In natural systems, these can be a very
complex likelihood mapping, e.g., language [14] or birdsong [15]. However, in
the case of AI agents, we can decide on the communication channel ourselves.
One particular naive choice is to directly share the sufficient statistics of one’s in-
ternal beliefss′
t and integrate these with an identity likelihood mapping, as used
in [8]. However, in the remainder of this paper, we will demonstrate the fallacies
of using this approach and propose a different format for shared messages.
4 O. Çatal et al.
...
...
...
...
Other agent
Focal agent
Fig.1: Two active inference agents sharing beliefs.The generative model
of each agent is a POMDP, where observations are generated from a hidden state
st. Actions at, generated from a policyπ, transition this state. In addition to
the typical observationot at each timestept, each agent also receives a shared
observation os
t that is generated from the other agent’s internal beliefss′
t. Blue
variables are observed from the perspective of the focal agent, i.e. they observe
their own actions, observations and the observations shared with the other agent.
Belief sharing: a blessing or a curse 5
0
1
2 3
4
5
?
?
?
Agent 1
Agent 2
(a)
 (b)
Fig.2: Illustration of the graph environment and the agent’s factor
graph. (a) agents are located on a connected graph of locations and need to find
a rewarding object that might be present at one of the locations. (b) a factor
graph representation of the agent’s generative model. Two latent state factors
that model the agent’s location and the object’s location respectively, give rise
to two sensory modalities through a likelihood factor: the agent’s location (A1)
and whether the object is visible (A2). In addition, agents can share beliefs
about the object location through belief sharing (A3). The agent’s location can
change conditioned on move actions (B1), whereas the object is kept static in
our experiments (B2 = I).
3 Experimental setup
To demonstrate multi-agent belief sharing, we simulate an object-finding task,
where multiple agents search for a rewarding object in the same environment,
and can potentially share beliefs on where they think the object is. The setup and
generative model for this task is depicted in Figure 2. The world is represented
by a graph ofN locations that can be visited by the agents, and agents can
move between connected nodes in the graph. Each agent has two state factors,
a Categorical(N) variable sl
t which is the belief about the agent’s location, and
a Categorical(N) variable so
t which is the belief about the object location. An
agent can perform one ofN move actionsa, modeled using the three-dimensional
dynamics tensorB1.
Bi,j,a
1 =



1.0, if a = i ∧ connected(i, j)
1.0, if i = j
0.0, otherwise
6 O. Çatal et al.
We assume the object is static, i.e. its dynamics B2 are modeled by the
identity matrixI. The agent has three observation modalities. First, it observes
its location ol
t, with a near identity likelihood mapping to the location state
factor:
Ai,j
1 =
(
0.99, if i = j
0.01, otherwise
Second, it observesov
t whether the object is visible or not. This is governed by
a three-dimensional likelihood mappingA2 where
Av,i,j
2 =



0.2, if i = j ∧ v = not visible
0.8, if i ̸= j ∧ v = not visible
0.8, if i = j ∧ v = visible
0.2, if i ̸= j ∧ v = visible
Finally, there is the belief-sharing observationos
t which contains the shared in-
formation from the other agent.
The agents are initialized with a prior onsl
t set to their starting location,
and a prior onso
t set to the initial belief on where to find the object. This is
typically set uniform to (a subset of) the available locations to foster searching
behavior. The preferenceC is to have the object visible outcome.
4 Echo chambers
In a model which shares the posterior beliefs of one agent as observations of an-
other, Bayesian model updating reinforces redundant priors shared among them.
The consequent simulated behavior mirrors the “echo chamber effect" wherein
messages communicated by like-minded agents are amplified and returned. Psy-
chological interpretations of the echo chamber effect are illustrated in the conse-
quences of social media feed algorithms, which are often engineered to encourage
user engagement with sympathetic posts [16]. An algorithmically curated social
media curriculum increases engagement by anticipating a user’s expected social
media observations and fulfilling those expectations. Promoted content thereby
constructs homophilic interaction networks which facilitate the construction and
reinforcement of shared narratives among users. In worst case scenarios, the re-
sult is the unimpeded flow of misinformation on social media platforms. More
generally, shared narratives facilitated by social media feed algorithms result
in an increase in confidence of posterior beliefs even when external evidence is
absent or intentionally excluded.
Ignoring new evidence can be adaptive, such as when this strategy facilitates
in-group cooperation [17]. However, negative consequences also result, such as
when outside sources are discredited or distrusted.
Corresponding to the echo chamber effect, one pitfall of belief sharing is that
when agents have established even small prior beliefs on their goal, if those priors
Belief sharing: a blessing or a curse 7
are shared, then an echo chamber forms. The agents, however, reinforce their
prior beliefs through the communication method, resulting in ever-increasing
beliefs on the goal location even when there is no new evidence to support this
belief. Fig. 3 shows a simulation triggering this situation. The agents start with
a small belief that the object will be present at two locations within the graph
to simulate a longer-running experiment where such a situation might naturally
occur. We have restricted the movement of the agents and prohibited the agents
from accumulating more evidence about the object’s actual location. However,
the agents keep increasing their belief on the object being present at the a priori
believed location because of the constant sharing of beliefs. Once the agents
create such an echo chamber, more often than not, they are stuck in this faulty
belief unless they all sufficiently change their belief at the same time.
5 Self-doubt
In an echo chamber, a belief sharing agent’s confidence about their priors is
reinforced in the absence of external evidence. Conversely, self-doubt refers to
a scenario in which the agent’s self-confidence is degraded as a result of belief
sharing. In our simulations, the paradigmatic example is when multiple agents
are fixated on a search task with complimentary plans for exploring the envi-
Fig.3: Simulation of an echo-chamber. We initialize both agents with a small
prior belief that the object will be present at location 11 or 13. Then, we let the
agents share their beliefs. Note that this reinforces the belief that the object will
be at either one of the locations. The next columns show the evolution of both
agents where they keep observing the environment. We see that in the transition
from time 1 to time 2 the agents increase the belief that the object is at the
a priori believed location even though there is no new evidence to support this
belief. Agent location is depicted using the blue dots in both panels.
8 O. Çatal et al.
Fig.4: Simulation of self-doubt for 4 agents.Again, each panel displays
the evolution of the posterior belief as a function of time, where darker colors
indicates a higher degree of belief. Blue dots indicate the agent location. All
agents are initialized with a strong prior belief that the object will be at location
1, as indicated by the dark shaded area. This reflects a potential situation where
all agents have been acting in the environment for a long time, accruing faulty
evidence. In this case the communication mechanism prohibits the agents from
discovering that the object is not there, even after observing its absence multiple
times.
ronment. Naturally, siblings faulty beliefs and ignore all evidence that points
to the contrary. This can occur after an echo chamber is formed but could also
occur independently. In this scenario, the agents again reinforce each other’s
beliefs in such a strong way that the agents “doubt” their observations origi-
nating from the environment. Fig. 4 visually overviews a simulation showcasing
this phenomenon. The agents are initialized with a strong belief on the object
location. In this particular simulation, the agents have a single peaked belief on
the object location (location 1 in the graph), which could occur after an echo
chamber situation. The agents are unrestricted in their movement as long as
they follow the underlying graph structure. We see that even though eventually
all agents visit location 1, they cannot correctly eliminate that location as a
possible object location. The incoming beliefs of the other agents overrule their
sensory observations.
6 To share or not to share?
Given that sharing agents’ beliefs can give rise to the aforementioned issues, it
is worth wondering what information can best be shared to allow multi-agent
cooperation in active inference agents. In particular, the update rule for so
t ,
Belief sharing: a blessing or a curse 9
written in variational message passing notation [18], is of the form
so
t = σ(µ2
→B2 + µ2
↑A2 + µ2
↑A3 ),
µf
↑Ag
= og
t ⊙ φ(ag) ⊙i∈pa(g)\f si
t,
(4)
where σ is the softmax function,µf
↑Ag
the message from observation modality
g to state factor f and φ(ag) the digamma function of the Dirichlet counts
corresponding to the parameters of the likelihood model [8]. Effectively, the
update message is comprised of a part coming from a prior given by our beliefs
on the previous timestepµ2
→B2 , a part based on the latest observationµ2
↑A2 ,
and a part communicated by the otherµ2
↑A3 . When we communicate the other’s
posterior parameters directly through an identity likelihood mapping, we have
µ2
↑A3 = µ2,other
→B2 + µ2,other
↑A2
. This shows that, indeed, when agents have similar
priors, this gets double counted in the belief update.
To address this, we will now instead share the other’s likelihood message only,
i.e. µ2
↑A3 = µ2,other
↑A2
. This scheme leaves out agents’ prior beliefs about the state
and only shares the agent’s interpretation of the observation, treating the other
agents as extra independent observers for the exact latent cause in the world.
We call this scheme ‘likelihood sharing‘.
Fig.5: Illustration of the lack of echo-chamber like behaviour when
sharing likelihoods.In this figure, the situation leading to an echo chamber is
recreated. Both agents are initialized with the same prior belief that the object is
most likely at locations 11 and 13; however, because of the sharing of likelihoods,
they do not get stuck in an echo chamber and do not increase the beliefs when
there is no new evidence.
In the particular scenario of object-finding agents in a graph world, the agents
share their current location and visibility observation as passed through their
10 O. Çatal et al.
object location A-tensor; each receiving agent can integrate this observation
quite easily into their own posterior belief by using Bayes rule. In effect, each
agent treats the other agents as an extra “pair of eyes” in the search for the
object, inferring their posterior update if they observed what the other agent
had observed.
In Fig. 5 and Fig 6, the same simulations from earlier are reprised but using
the new likelihood-sharing mechanism. The echo chamber and self-doubt phe-
nomena are no longer present even when providing the same initial conditions.
Finally, Figure 7 compares likelihood sharing to belief sharing and not com-
municating at all. All methods are tested over all possible combinations of agent
starting locations and object locations in the environment, and each configura-
tion is repeated five times. The trials are evaluated on the percentage of times
the object was found. From the experiments, the likelihood-sharing agents are on
par with the naive belief-sharing agents. At the same time, they both outperform
the random agents and the non-communicating agents.
Fig.6: Alleviation of the self-doubt behavior under likelihood-sharing.
As with the previous depiction of this scenario, the agents are initialized with
a strong belief that the object will be at location 1; nonetheless, due to the
different communicated belief, the agent no longer ignores the evidence for the
object’s absence.
7 Discussion
The results presented in this paper highlight the potential pitfalls of belief-
sharing in multi-agent systems under the framework of active inference. Our
findings suggest that naive sharing of posterior beliefs can lead to undesirable
social dynamics such as echo chambers and self-doubt, severely hampering the
agents’ performance in collaborative tasks.
Belief sharing: a blessing or a curse 11
Fig.7: Overview of the average object find rate for each type of agent.
We measured the percentage of finding the object by any of the two agents over
all possible start configuration in the environment. This is repeated five times
to account for variability in the action selection process.
Echo chambers in human societies are well-documented phenomena where
groups of individuals reinforce their preconceptions, often without external val-
idating evidence. In our simulations, a similar effect occurs when agents con-
tinuously share their posterior beliefs. Initial biases can get amplified through
repetitive belief sharing, leading to overly confident but potentially erroneous
shared beliefs. This results in the agents becoming overconfident in incorrect
hypotheses, thereby hampering their search or exploration processes. Similarly,
self-doubt arises when agents’ observations contradict the reinforced shared be-
liefs, leading them to disregard their sensory inputs. This mirrors real-world
psychological effects where individuals question their perceptions in the face of
strong peer influence. In our simulations, agents maintained strong incorrect be-
liefs about the object’s location despite direct evidence to the contrary due to
the influence of shared but incorrect posterior beliefs.
Our proposed strategy of sharing likelihoods rather than posterior beliefs
mitigates the issues of echo chambers and self-doubt. By sharing interpreted ob-
servations rather than fully formed beliefs, agents can integrate new information
without being overwhelmed by the potentially erroneous priors of others. This
approach allows agents to utilize each other as additional sensing mechanisms,
providing independent evidence that can be more robustly combined with their
observations. By treating other agents’ observations as additional data points
rather than beliefs, the system remains more flexible and resilient to individual
errors. The proposed approach was only validated in simulated experiments but
might also apply to more general and complex scenarios with further modifica-
tions.
Our work suggests that the type of information shared among active inference
agents must be carefully considered to avoid counterproductive dynamics. Future
research can build on these insights by exploring other belief-sharing strategies
and their impacts on system performance. Additionally, exploring these dynam-
ics in more complex and varied environments, including those with adversarial
12 O. Çatal et al.
elements, could provide deeper insights into the performance of different com-
munication strategies. As our approach assumed full and honest collaboration
between the agents, another future avenue of research would be to investigate
the impact of dishonesty and, consequently, the discounting of communications
between untrusted agents.
In conclusion, while belief sharing among active inference agents can en-
hance collaborative performance, it also risks reinforcing incorrect beliefs and
undermining individual observations. Our proposed likelihood-sharing mecha-
nism offers a promising solution by leveraging the strengths of collective sensing
while mitigating the pitfalls of echo chambers and self-doubt without significant
changes to the underlying model. Such strategies will be essential for developing
robust, efficient, and adaptive collaborative agents as multi-agent active infer-
ence systems are designed.
References
1. J. Henrich,The Secret of Our Success: How Culture Is Driving Human Evolution,
Domesticating Our Species, and Making Us Smarter. Princeton University Press,
Oct. 2015.
2. B. Bahrami, K. Olsen, P. E. Latham, A. Roepstorff, G. Rees, and C. D. Frith,
“Optimally interacting minds,”Science, vol. 329, p. 1081–1085, Aug. 2010.
3. A. Constant, M. J. D. Ramstead, S. P. L. Veissière, and K. Friston, “Regimes of
expectations: An active inference model of social conformity and human decision
making,” Frontiers in Psychology, vol. 10, Mar. 2019.
4. K. J. Friston, M. J. Ramstead, A. B. Kiefer, A. Tschantz, C. L. Buckley, M. Al-
barracin, R. J. Pitliya, C. Heins, B. Klein, B. Millidge, D. A. Sakthivadivel,
T. St Clere Smithe, M. Koudahl, S. E. Tremblay, C. Petersen, K. Fung, J. G.
Fox, S. Swanson, D. Mapes, and G. René, “Designing ecosystems of intelligence
from first principles,”Collective Intelligence, vol. 3, Jan. 2024.
5. K. J. Firston and C. D. Firth, “Active inference, communications and hermeneu-
tics,” Cortex, vol. 68, pp. 129–143, 2015.
6. R. Tison and P. Poirier, “Active inference and cooperative communication: An
ecological alternative to the alignment view,” Frontiers in Psychology, vol. 12,
2021.
7. T. Parr, G. Pezzulo, and K. J. Friston,Active Inference: The Free Energy Principle
in Mind, Brain, and Behavior. The MIT Press, 03 2022.
8. K. J. Friston, T. Parr, C. Heins, A. Constant, D. Friedman, T. Isomura, C. Fields,
T. Verbelen, M. Ramstead, J. Clippinger, and C. D. Frith, “Federated inference
and belief sharing,”Neurosci. Biobehav. Rev., p. 105500, Dec. 2023.
9. M. Albarracin, A. Constant, K. J. Friston, and M. J. D. Ramstead, “A variational
approach to scripts,”Frontiers in Psychology, 2021.
10. S. Gallagher and M. Allen, “Active inference, enactivism and the hermeneutics of
social cognition,” Synthese, vol. 195, no. 6, pp. 2627–2648, 2018.
11. N. Bouizegarene, M. Ramstead, A. Constant, K. Friston, and L. Kirmayer, “Nar-
rative as active inference,”Frontiers in Psychology, vol. 15, 2024.
12. M. Albarracin, D. Demekas, M. J. D. Ramstead, and C. Heins, “Epistemic com-
munities under active inference,”Entropy, vol. 24, p. 476, mar 2022.
Belief sharing: a blessing or a curse 13
13. C. M. Bishop, Pattern Recognition and Machine Learning (Information Science
and Statistics). Springer, 1 ed., 2007.
14. K. J. Friston, T. Parr, Y. Yufik, N. Sajid, C. J. Price, and E. Holmes, “Generative
models, linguistic communication and active inference,”Neuroscience &; Biobe-
havioral Reviews, vol. 118, p. 42–64, Nov. 2020.
15. K. J. Friston and C. D. Frith, “Active inference, communication and hermeneutics,”
Cortex, vol. 68, p. 129–143, July 2015.
16. M. Cinelli, G. De Francisci Morales, A. Galeazzi, W. Quattrociocchi, and
M. Starnini, “The echo chamber effect on social media,”Proceedings of the Na-
tional Academy of Sciences, vol. 118, Feb. 2021.
17. M. Kim, B. Park, and L. Young, “The psychology of motivated versus rational
impression updating,”Trends in Cognitive Sciences, vol. 24, p. 101–111, Feb. 2020.
18. J. Winn and C. M. Bishop, “Variational message passing,”Journal of Machine
Learning Research, vol. 6, no. 23, pp. 661–694, 2005.