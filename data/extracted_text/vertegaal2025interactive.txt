Interactive Inference: A Neuromorphic Theory of
Human-Computer Interaction
Roel Vertegaal
Radboud University
The Netherlands
Timothy Merritt
Aalborg University
Denmark
Saul Greenberg
University of Calgary
Canada
Aneesh P. Tarun
Toronto Metropolitan University
Canada
Zhen Li
Huawei Technologies, Ltd.
Canada
Zafeirios Fountas
Huawei Technologies, Ltd.
UK
Abstract
Neuromorphic Human-Computer Interaction (HCI) is a theoreti-
cal approach to designing better user experiences (UX) motivated
by advances in the understanding of the neurophysiology of the
brain. Inspired by the neuroscientific theory of Active Inference,
Interactive Inference is a first example of such approach. It offers a
simplified interpretation of Active Inference that allows designers
to more readily apply this theory to design and evaluation. In Inter-
active Inference, user behaviour is modeled as Bayesian inference
on progress and goal distributions that predicts the next action. We
show how the error between goal and progress distributions, or
Bayesian surprise, can be modeled as a simple mean square error
of the signal-to-noise ratio (SNR) of a task. The problem is that the
userâ€™s capacity to process Bayesian surprise follows the logarithm
of this SNR. This means errors rise quickly once average capacity
is exceeded. Our model allows the quantitative analysis of perfor-
mance and error using one framework that can provide real-time
estimates of the mental load in users that needs to be minimized
by design. We show how three basic laws of HCI, Hickâ€™s Law, Fittsâ€™
Law and the Power Law can be expressed using our model. We
then test the validity of the model by empirically measuring how
well it predicts human performance and error in a car following
task. Results suggest that driver processing capacity indeed is a
logarithmic function of the SNR of the distance to a lead car. This
result provides initial evidence that Interactive Interference can be
useful as a new theoretical design tool.
Introduction
There have been many efforts to develop psychological theories of
human behaviour as it pertains to user interaction. Frameworks
such as GOMS [11] and ACT-R [1] have been successful in allowing
predictions on time performance when provided with a model user
and a model interface. Similarly, models of human error [45] have
been used to predict error as it relates to the design of interactive
systems. However, there has not been a model that has comprehen-
sively explained the relationship between human performance and
error in a way that is generalizable. We believe developing such a
model is important because it is a key to measuring cognitive load
of users in real time. Current methods for measuring cognitive load
fall short because they either rely on a questionnaire that can be
difficult to administer during tasks (e.g., the NASA TLX [24]), or on
physiological metrics that can be difficult to interpret (e.g., Heart
Rate Variability, Electrodermal Activity or Pupillometry).
In this chapter, we discuss a theory of human performance as it
relates to error that allowsdirect measurementof performance by
first defining the signal-to-noise ratio (SNR) of a task. It incorpo-
rates the semantics of a task by modeling goals as predictions using
Bayesian statistics. We set out to develop an approach to modeling
tasks called Interactive Inference that examines both performance
and error using an information-theoretical perspective. We will
show how this approach might allow for wider comparisons of in-
formation processing capacity and error of users between otherwise
incompatible tasks. While inspired by Active Inference, we note that
our approach simplifies many of its key components and should be
viewed as a practical approach rather than a strict implementation
of the theory.
Key Takeaways
(1) Interactive Inference model: We introduce a simple neuromor-
phic framework called Interactive Inference that integrates
human performance and error using information theoretical
constructs. The model uses Bayesian statistics to interpret user
goals as predictions, providing a way to measure performance
by quantifying the Bayesian surprise processed per signal-to-
noise ratio of tasks.
(2) Cognitive load measurement: The model enables real-time es-
timation of cognitive load without relying on traditional self-
reported questionnaires or physiological metrics. By modeling
task complexity externally, it offers a more practical approach
for evaluating user experience during interactions.
(3) Derivation of HCI Laws: Through known mathematical theo-
rems and logic, we show how foundational principles of HCI â€”
Hickâ€™s Law, Fittsâ€™ Law, and the Power Law of Practiceâ€”emerge
naturally from our framework.
(4) Empirical study: To verify whether Interactive Inference could
successfully model a task that does not yet have a known model,
we demonstrate through an empirical study of a car following
task that user performance follows the logarithm of the SNR of
the distance to a lead car.
(5) Research agenda: By discussing the limitations, benefits, and
open research questions of this work, we invite further research
to extend the theory and explore its practical limitations. This
includes further empirical studies that test the validity of the
theory as applied to different task-modeling scenarios.
arXiv:2502.05935v6  [cs.HC]  2 Dec 2025
Vertegaal et al.
Background
Information Theory and Three Laws of HCI
Theories of the way the human brain processes information are as
old as information theory itself. The very idea that humans might
act like a computing circuit arrived shortly after the introduction of
the microprocessor. Although most computing logic is deterministic,
Shannon [50] defined self-information as probabilistic: the lower
the probability of a message, the higher its information content.
One Shannon bit of information is defined as the negative binary
logarithm of this probability (negative log probability):
ğ¼(ğ‘¥)=âˆ’log 2 (ğ‘ƒ(ğ‘¥))(1)
Shannon went on to define that the information capacity ğ¶ of
the transmission of a message through an analog channel has a
maximum that is the (binary) logarithm of the ratio between the
signal strengthğ‘†and the amount of Gaussian noiseğ‘in the channel,
multiplied by some constantğ‘, as expressed by the Shannon-Hartley
Theorem:
ğ¶=ğ‘Â·log 2
ğ‘†+ğ‘
ğ‘

=ğ‘Â·log 2
 ğ‘†
ğ‘ +1

(2)
Where ğ‘is some empirically derived bandwidth parameter. This
equation describes the capacity of information sent through the
channel in terms of the signal-to-noise ratio. Note that without the
negative sign, the SNR represents the reciprocal of the probability
of a transmission. A high signal allows for more uncertainty to
be carried, as does a lower noise. This means the very nature of
information is statistical, with underlying equations based on (e.g.,
normal) distributions. This is important because humans, unlike
traditional computer algorithms, operate in a probabilistic environ-
ment. That is, their exact behaviour cannot be computed, however,
it can be modeled using (normal) probability distributions. Using
this framework, Card, Moran and Newell [11] developed the Model
Human Processor (MHP), a comprehensive set of heuristics for pre-
dicting human performance in simple computer tasks. Statistical
elements to these models were derived from empirical observations
such as those performed by Hick [25] and Fitts [19]. They demon-
strated that human performance in certain tasks indeed seems to
follow a similar pattern to the Shannon-Hartley Theorem: a binary
logarithm of the signal-to-noise ratio in the task. E.g., Hickâ€™s Law
describes the response time ğ‘…ğ‘‡ for selecting from a number of
alternativesğ‘›, plus 1:
ğ‘…ğ‘‡=ğ‘Â·log 2 (ğ‘›+1)(3)
Here, ğ‘ is some empirically derived constant that models the
time per unit of difficulty, the log component of the task. Response
time is derived by multiplying the amount of information in the
task by this empirical constant. Fitts [19] derived a very similar law
describing the movement timeğ‘€ğ‘‡ of a hand towards a target along
a single dimensional trajectory ğ´with a target width ofğ‘Š (specific
equation from [36]):
ğ‘€ğ‘‡=ğ‘Â·log 2
 ğ´
ğ‘Š +1

(4)
One can think of the power of the signal in this equation as the
amplitude (one-dimensional distance) to the target, and the noise as
the width of the target. Again, theğ‘is a constant that is empirically
derived. It describes an average time per bit of difficulty of the
task that when multiplied with the difficulty gives the movement
time prediction. It is important to note the difference between the
Shannon-Hartley Theorem and these equations: these equations
predict time.
The third law was derived from Snoddy [53] and describes the
rate of learning of a model human. Time ğ‘‡ to complete a task
follows a power law as a function of the number of trials of that
taskğ‘¥[15]:
ğ‘‡=ğ‘Â·ğ‘¥ âˆ’ğ‘ +ğ‘(5)
Again, here,ğ‘,ğ‘andğ‘are empirically determined constants.
Predictive Coding and the Bayesian Brain Hypothesis
One of the drawbacks of classic information theoretical approaches
to human performance modeling is that classic information theory
does not describe the presence of memory of messages transmitted
through a channel, nor proactive anticipation of those messages.
According to Shannon, a bit of information is simply the negative
logarithm of the probability of a message (Equation 1). Only if a
message is less probable, does it contain more information. The
semantics of how the human interprets that message is not mod-
eled, nor whether the human already knows the message. This
is problematic because knowing the message, or anticipating the
next message, reduces its information content to zero, because
this renders the uncertainty about the message zero. Instead of
framing human processing as a sequential computer processor, Mc-
Clelland and Rumelhartâ€™s parallel processing model [37] described
perception as an interaction of top-down (cognitive/semantic) and
bottom-up (sensory) elements. Rao and Ballard [44] further eluci-
dated this approach by suggesting that the brain builds up statistical
knowledge of its environment, i.e., incoming messages, over time,
and that this model is used generatively, i.e., top-down, topredict
the subsequent messages. The error between the prediction and the
messages would lead to an update of the prediction. Prediction is
convenient for more than one reason: not only does it allow humans
to plan actions in their environment based on relevant information,
it serves to compress the information content of their environment
by processing only those messages that are surprising: i.e., not, or
insufficiently predicted. Predictive coding theory explains how the
human brain can work so efficiently and explains the mechanisms
behind attention and the scheduling of cognitive resources based
on the amount of surprise in their predictions [2]. According to Rao
and Ballard [44], Bayesian inference [3] is a natural candidate for
a generative statistical model. Within this context, Bayesian infer-
ence estimates the probability that a particular stateğ‘ (representing
a meaning or cause) exists, given an observation ğ‘œ. This is called
the posterior probability ğ‘ƒ(ğ‘ |ğ‘œ), i.e., the chance of state ğ‘  after, or
given observation ğ‘œ. It does so by multiplying two probabilities:
the likelihood ğ‘ƒ(ğ‘œ|ğ‘ ) of the observation ğ‘œ given state ğ‘ , and the
ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘Ÿ probability ğ‘ƒ(ğ‘ ) of state ğ‘  beforethe observation. The result
is subsequently normalized by dividing by the overall probability
of the observationğ‘ƒ(ğ‘œ)(see Figure 1):
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
Figure 1: Bayes Theorem. Distributions of the Likelihood P(o|s) of observations, in green, given a Prior P(s) that describes the
current state, in blue. In black is the calculated Posterior P(s|o) that describes the chance of the state given the observation.
ğ‘ƒ(ğ‘ |ğ‘œ)= ğ‘ƒ(ğ‘œ|ğ‘ )Â·ğ‘ƒ(ğ‘ )
ğ‘ƒ(ğ‘œ) (6)
Unlike the frequentist approach to statistics, well known to the
scientific community as hypothesis testing, Bayesian theory thus
incorporates prior statistical knowledge in its testing of a hypothe-
sis. The posterior ğ‘ƒ(ğ‘ |ğ‘œ) serves as the subsequent prior for the next
observation and as such, serves to improve the prediction of subse-
quent observations. This yields a different measure of uncertainty
than Shannon information: the difference in information between
the prior and posterior, or Bayesian surprise. Because Bayesian sur-
prise incorporates the statistical relationship with prior knowledge,
it is semantic, describing the information content of the meaning of
an observation under prior knowledge. When we consider the prior
and posterior to be (normal) distributions, this Bayesian surprise is
given by the Kullback-Leibler divergence (KL Divergence, [30]). It
provides the relative entropy between the two distributions, or the
information gained from one distribution to the other, in bits:
ğ¾ğ¿(ğ‘ƒ(ğ‘ )||ğ‘ƒ(ğ‘ |ğ‘œ))=
âˆ«
ğ‘ƒ(ğ‘ )Â·ğ‘™ğ‘œğ‘” 2
 ğ‘ƒ(ğ‘ )
ğ‘ƒ(ğ‘ |ğ‘œ)

ğ‘‘ğ‘ (7)
Entropy and Active Inference
Friston et al. [21] further improved upon predictive coding theory
by suggesting a mathematical framework for approximating the
amount of Bayesian surprise (i.e., relative entropy) processed by
the brain. In their framework, called Active Inference, learning and
action are unified under a single principle: both use the difference
between prior and (an estimate of) the posterior to make decisions.
Minimizing the difference informs when to learn or take action.
When learning, the difference is mainly minimized by updating
the prior to the posterior, thus updating statistical knowledge in
the brain. When taking action, the difference is mainly minimized
by altering the world, thereby updating the posterior to be closer
to the prior knowledge of what the state of the world should be.
Since we are concerned with the relative entropy between prior
and posterior, the direction of updates does not principally matter.
Relation to Physical Entropy.To better understand the concept of
relative entropy in information systems, let us use an analogy with
the concept of entropy in the physical world. Physical entropy mea-
sures the logarithm of the number of microscopic states possible
in a system [9]. Because particle interactions are mostly random,
systems naturally evolve toward states with higher entropy, which
can be loosely described as greater disorganization. That is simply
because high-entropy states are overwhelmingly more probable.
This process is governed by the second law of thermodynamics [12]:
In a closed system, entropy (analogous to disorganization) increases
over time and approaches equilibrium; it does not spontaneously de-
crease. Eventually, energy such as heat tends to become uniformly
distributed throughout the system. Engines exploit free-energy gra-
dients, which correspond to differences in entropy, to perform work.
Random interactions thus explain the natural tendency of heat to
dissipate from your morning coffee into the environment. This
illustrates the second law of thermodynamics: in closed systems,
the entropy, which you can loosely think of as spread, increases.
Living organisms must counteract this natural tendency towards
disorganization by locally staying organized [20]. In return, they
export entropy to their local environment through waste products,
including heat [ 43]. They maintain their organization by using
free energy imported from outside the system, from food that con-
centrates solar energy. Organisms must seek food while avoiding
environments that may accelerate entropy increases. For this pur-
pose, they evolved sensorimotor systems and neural circuitry that
enable them to observe, learn, and act on their environment. In
humans, these systems support homeostasis much like physical
systems minimize free energy: they minimize the variational free
energy in their inferences, which bounds prediction error while
avoiding needlessly complicated explanations [20]. They learn to
make correct predictions about their next state by reducing the
Bayesian surprise in their predictions. Bayesian surprise, then, is
Vertegaal et al.
the relative entropy between the prior and posterior beliefs about
the environment. It encodes the missing information between pre-
diction prior to observation and after observation. The equations
governing the statistical properties of particles and Bayesian sur-
prise are thus closely related.
Action Mirrors Learning.There is one complication in the calcu-
lation of Bayesian surprise between prior and posterior: that is
estimating the probability of observations, the model evidenceğ‘ƒ(ğ‘œ).
For this reason, and because the number of known states in the
brain is extremely large, Active Inference uses a variational rather
than an exact Bayesian approach to calculating the posterior [4].
So, Active Inference uses a minimization of Variational Free Energy
(VFE) to reduce uncertainty about its predictions in a way that is
mathematically analogous to the Feynman variational principle for
physical free energy minimization in statistical mechanics [18]. It
minimizes VFE because it is tractable, and from it an approximation
of Bayesian surprise is obtained if the approximate posterior is
close to the true posterior. In our Interactive Inference interpre-
tation, we try to calculate Bayesian surprise directly, as it is the
measure of how much our beliefs change when we learn. When
learning, sequential Bayesian updates of the prior allow humans to
reduce subsequent Bayesian surprise. When acting, similar sequen-
tial Bayesian updates of movements occur [ 31]. When learning,
the prior is modeled as moving to the posterior, when acting, the
posterior as moving to the prior. Once Bayesian surprise is minimal,
learning or action attenuates and the goal is reached. So, Interac-
tive Inference differs from Active Inference in that we try to avoid
approximating VFE by obtaining parameters that determine exact
Bayesian surprise. To briefly summarize Active Inference before
we continue, VFE is computed from what is tractable in Equation 6:
the likelihood p(o|s) and prior p(s), which give a negative log joint
probability:
âˆ’ğ‘™ğ‘œğ‘”2ğ‘ƒ(ğ‘œ,ğ‘ )=âˆ’ğ‘™ğ‘œğ‘” 2ğ‘ƒ(ğ‘œ|ğ‘ )âˆ’ğ‘™ğ‘œğ‘” 2ğ‘ƒ(ğ‘ )(8)
If distributions are normal, this produces a conveniently parabolic
error function in which gradient descent [ 14] can be used to ap-
proximate the minimum free energy to subsequently approximate
Bayesian surprise in a computationally efficient way, while avoiding
explicit computation of the information contained in the intractable
model evidence ğ‘ƒ(ğ‘œ). This gradient descent is similar to that used
for back propagation in neural networks [46]. Unlike classical neu-
ral networks, however, VFE allows unsupervised learning because
it has an inherent reward or loss function: the minimization of
free energy, which does not require ground truths. In terms of the
userâ€™s experience: the reward is inherent when a system performs
the function that the user predicted it to perform after some in-
put. VFE is minimized, and Bayesian surprise is correspondingly
reduced. In the remainder of this paper, we will focus on how we
might calculate Bayesian surprise more easily and more directly
using the signal-to-noise ratio of a task, while remaining within the
general conceptual framework of Active Inference. We emphasize
that Interactive Inference simplifies many aspects of Active Infer-
ence. Particularly, we do not implement model evidence terms or
expected free energy calculations, focusing instead on a tractable
framework suitable for HCI applications.
The Concept of â€œTaskâ€ as a Reduction in Relative
Entropy
Interactive Inference implies that the core concept of HCI, the task,
can be elegantly modeled as the minimizing of Bayesian surprise
between prior knowledge of a desired outcome (the prior or goal)
and the posterior (the progress towards a goal given current obser-
vations).
Reducing Physical Relative Entropy
So we have a goal distribution, and a progress distribution, the
relative entropy between which the brain is trying to reduce. Let
us reconsider how, when we apply this concept to tasks in the real
world, this can align with a reduction in thephysicalentropy of
the userâ€™s environment.
Let us consider a seemingly simple everyday task: organizing
socks in a drawer. Because the number of possible places where
your socks might end up other than the drawer is infinitely large,
random interactions make it much more likely that your socks are
anywhere but inside the drawer. The task, then, is to reduce the rel-
ative disorganization in the location of your socks, or their physical
entropy, relative to the location of the drawer. The goal distribution
describes the probability of locations where socks might be stored
inside the drawer. The progress distribution describes the proba-
bility of locations where socks may be found in the home. We are
going to assume both are normal distributions. To execute this task
and reduce the physical entropy in the organization of the socks,
we must expend physical energy (walk around and pick up socks,
place in drawer), i.e., perform work.
According to Interactive Inference, the information used by the
userâ€™s brain follows a similar process: one that predicts any left-over
physical relative entropy between socks and drawer. The brain is
merely minimizing the statistical difference in information about
the location of socks and drawer that also defines their relative
physical entropy, which is in and of itself a statistical property of
objects. It means that the brain, in the process of acting to mini-
mize the relative physical entropy is also minimizing the relative
information entropy of the task, the Bayesian surprise. To do so,
it calculates the statistical likelihood that an observation of a task
object corresponds to the outcome of the task, after which it can es-
timate the chance that the task is complete using Bayesian inference
[21].
Ten Postulates on Capacity and Error in Tasks
This leads to the first of ten postulates of Interactive Inference:
(1) Entropy Reduction. The aim of any task is to minimize the rela-
tive entropy in the space of possible task outcomes. Conversely,
any errors in the execution of the task lead to increases in rel-
ative entropy. The processing that is required for the userâ€™s
brain to execute a task leads to a reduction in relative infor-
mation entropy, i.e., Bayesian surprise. In our example, this
reduction is theinformationabout the distribution of socks in
the household. The aim of the task is to reduce the difference
between where the user predicts the socks to be and where
they actually are. When executing the task, the userâ€™s brain is
computing the difference between the probability distribution
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
of locations of socks outside the drawer relative to the prob-
ability distribution of the goal: the available locations inside
the drawer. In doing so it can, e.g., estimate the time and effort
required to complete the task, which leads to postulates 2 and
3:
(2) Bayesian Surprise. The reduction in relative information entropy
required for a user to process a task is the reduction in Bayesian
surprise. Bayesian surprise thus equals the amount of work that
needs to be performed by the brain to perform the task, in bits.
(3) Scale of Task Outcomes. Relative entropy is measured along a
scale of possible task outcomes ğ‘ . This scale can beanyfeature
of the task space that measures success or failure using some
ratio level scale. In tasks in the physical world relative location
(i.e., distance) is often used as a scale of task outcomes. However,
an example in the world of information might be a test score.
(4) Difficulty. The difficulty of the task equals the number of bits
of relative information entropy remaining to be processed. A
fully accomplished task has the minimum bits of relative en-
tropy to be processed (preferably zero). Converting bits to time
estimates requires a multiplication of this difficulty with an
empirically derived distribution of time performance. E.g., the
prior indicates prior statistical knowledge, built up over time,
about where socks should be in a second task: that of obtain-
ing a new pair of socks. After the task of organizing socks is
complete, when the user wants to obtain a pair of socks, the
probability of obtaining them in the prior location is 100%. The
posterior distribution of socks corresponds 100% to the prior,
the location inside the drawer. By reducing the relative entropy
in the distribution of socks in the first task to fit the goal, the
user has been able to make the currently estimated location of
socks, theposteriororprogress distribution P, correspond to the
memorizedprior, thegoal distribution G.
(5) Goal and Progress Distributions. Goal and progress of a task
are defined as probability distributions on the scale of task
outcomes ğ‘ . Thegoaldistribution ğº describes the prior, the
probability of positive task outcomes on this scale. Theprogress
distribution ğ‘ƒ describes the posterior, the probability of be-
ing in a current position along the scale of task outcomes ğ‘ .
Each distribution has a mean value ğœ‡on this scale. For the goal
distribution, the mean ğœ‡ğ‘” signifies the value with the highest
probability of a correct task outcome. For the progress distribu-
tion, the mean ğœ‡ğ‘ denotes the value with the highest probability
of the current task outcome. Each distribution also has an as-
sociated variance. The variance ğœ2
ğ‘” or standard deviation ğœğ‘” of
the goal distribution ğº describes the tolerance for error that is
allowed in achieving goal ğº. The reciprocal of this variance 1
ğœ2ğ‘”
is the precision with which the task is to be completed. The vari-
ance ğœ2
ğ‘ or standard deviation ğœğ‘ of the progress distribution
ğ‘ƒ describes the speed with which the task outcome is moving
towards the goal distribution: the speed of progress on the task.
The reciprocal of this variance 1
ğœ2ğ‘
is the precision with which
the task is being executed. From postulate 5 it is becomes ap-
parent that, in order for the goal and progress distributions
to match, the user must not just reduce the distance between
the means of progress and goal distributions to zero, but also
match the precision (or variance) of both distributions. That
is, a more precise goal will require, at some point during the
task, a more precise movement on the scale of possible task
outcomes towards that goal. In other words, to avoid error, the
user will, at some point, need to move more slowly on the scale
of possible task outcomes. This is typically towards the end of
the task, when the user is closer to the goal. It also apparent
that a not so precise goal definition allows faster progress, i.e.,
users can be more sloppy in the execution of the task. As such,
the ratio between the difference of means and the standard
deviation reflects a precision/accuracy trade-off (mistakenly
known as speed/accuracy trade-off) that is inherent to all tasks.
Because the two distributions will need to match on average,
the individual variances of goal and progress distributions can
be simplified as equal. This brings us to the next postulate:
(6) Signal and Noise. We call the distance between the means of the
goal and progress distributions on variable ğ‘  theSignalto the
userâ€™s brain. It describes how far, and in which direction, the
user needs to travel along the scale of task outcomes to reach
the goal distribution, on average. The stronger the signal, the
more stimulus the task provides. However, this also means the
more work has yet to be performed by the brain. The reciprocal
of this signal represents the ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ of the task. The standard
deviation of the goal distribution is the tolerance to noise and
describes the variation allowed in the goal distribution of the
task. We therefore call this theNoisein the task, because it
describes how much noise is allowed in the movement towards
the goal for distributions to still match. The larger the noise
tolerance, the less precise the user needs to be in executing
the task. As such, the reciprocal of this noise represents the
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› of the task. If we return to the example of organizing
your socks, the larger the average distance of socks, theSignal,
to the drawer, the harder it is to complete the task. The larger
the drawer, orNoisetolerance, the easier it will be to complete
the task. Fittsâ€™ Law naturally follows from this, but note we are
working in bits rather than time. From this also follows that
the relative entropy or difficulty of a task is a function of the
ratio of the mean distance S to and standard deviation N of the
goal. Note that this SNR is different from the one introduced
by Shannon as it describes a relative signal to a goal:
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
ğœğ‘”
= ğ‘†
ğ‘ (9)
(7) KL Divergence. Relative information entropy measures the re-
duction in Bayesian Surprise, in bits, when the user is trying to
match progress and goal distributions. This relative entropy is
calculated as a Kullback-Leibler Divergence between the goal
distribution G and progress distribution P, in bits. Mathemati-
cally, this is performed by first converting to bits, subtracting
the two distributions (a fraction inside a log is a subtraction),
then calculating the surface area using an integral. To describe
error, we include an empirical variableğ‘in this equation that re-
lates the KL divergence (where ğ‘= 1) to an observed empirical
error rate (see Postulate 10):
Vertegaal et al.
ğ¾ğ¿(ğ‘ƒ(ğ‘ )||ğº(ğ‘ ))=ğ‘Â·
âˆ«
ğ‘ƒ(ğ‘ )Â·ğ‘™ğ‘œğ‘” 2
 ğ‘ƒ(ğ‘ )
ğº(ğ‘ )

ğ‘‘ğ‘ (10)
(8) Equal Variance Condition. The KL Divergence can be greatly
simplified in cases where the goal and progress distributions
are normal, with identical variance. Bayesian surprise and thus
difficulty can be calculated using a simple version of the KL
divergence where we take the square of the signal-to-noise
ratio, where the signal is the distance between the means of the
goal and progress distributions and the noise is the variance in
the goal distribution. The mathematical proof of the relative
entropy of two Gaussians with equal variance is, in fact, the
motivation for the use of squared error in statistics and machine
learning [41]:
ğ‘ƒ(ğ‘ )
ğº(ğ‘ ) =
âˆšï¸ƒ
ğœ2ğ‘”
âˆšï¸ƒ
ğœ2ğ‘
exp
 
âˆ’(ğ‘ âˆ’ğœ‡ ğ‘)2
2ğœ2ğ‘
+(ğ‘ âˆ’ğœ‡ ğ‘”)2
2ğœ2ğ‘”
!
andğœğ‘ =ğœğ‘”
log2
ğ‘ƒ(ğ‘ )
ğº(ğ‘ ) =log 2
ğœğ‘”
ğœğ‘”

+log 2 exp
 
âˆ’(ğ‘ âˆ’ğœ‡ ğ‘)2
2ğœ2ğ‘”
+(ğ‘ âˆ’ğœ‡ ğ‘”)2
2ğœ2ğ‘”
! (11)
The integral of Gaussianğ‘ƒ(ğ‘ )multiplied with a functionğ‘™ğ‘œğ‘”2

ğ‘ƒ(ğ‘ )
ğº(ğ‘ )

equals the expectation underğ‘ƒ(ğ‘ ):
ğ¾ğ¿ (ğ‘ƒ(ğ‘ )||ğº(ğ‘ ) )=E ğ‘
"
âˆ’ ğ‘ âˆ’ğœ‡ ğ‘
2 + ğ‘ âˆ’ğœ‡ ğ‘”
2
2 ln(2)ğœ2ğ‘”
#
(12)
The expectation underğ‘ƒ(ğ‘ )equals:
Eğ‘

(ğ‘ âˆ’ğ‘) 2
=(ğœ‡ ğ‘ âˆ’ğ‘)2 +ğœ2
ğ‘
Eğ‘
h
âˆ’ ğ‘ âˆ’ğœ‡ ğ‘
2i
=âˆ’

(ğœ‡ğ‘ âˆ’ğœ‡ğ‘)2 +ğœ2
ğ‘

=âˆ’ğœ2
ğ‘
Eğ‘
h ğ‘ âˆ’ğœ‡ ğ‘”
2i
=(ğœ‡ ğ‘ âˆ’ğœ‡ğ‘”)2 +ğœ2
ğ‘
(13)
Substituting expectations:
=
 ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
2 +ğœ2
ğ‘ âˆ’ğœ2
ğ‘
2 ln(2)ğœ2ğ‘”
(14)
We substitute the constant 1
2ğ‘™ğ‘›(2) with an empirical variable ğ‘
that relates the KL divergence (whereğ‘= 1
2ğ‘™ğ‘›(2)) to an observed
empirical error rate:
ğ¾ğ¿ (ğ‘ƒ(ğ‘ )||ğº(ğ‘ ) )=ğ‘Â·
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
ğœğ‘”
2
=ğ‘Â·
 ğ‘†
ğ‘
2
(15)
We note that the metric
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
ğœğ‘” is mathematically identical to
Cohenâ€™s d [13], a well-established statistical measure of effect
size between two distributions. Thus, our framework essentially
connects this measure to modeling of task difficulty in HCI
contexts.
(9) Performance. Human performance, or capacity to process rel-
ative entropy, follows the reduction in KL divergence as the
user moves along the scale of task outcomes. This capacity to
process Bayesian surprise, in bits, follows Shannon-Hartleyâ€™s
theorem (Equation 2) but with relative entropy (KL divergence)
between prior and posterior, rather than absolute entropy of
signals. This equals the logarithm of our relative signal-to-noise
ratio plus 1.
ğ¶=ğ‘Â·ğ‘™ğ‘œğ‘” 2
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
ğœğ‘”
+1

=ğ‘Â·ğ‘™ğ‘œğ‘” 2
 ğ‘†
ğ‘ +1

(16)
Where ğ‘ is some empirically derived constant describing the
average performance. Human performance and error can thus
both be described on the same scale of signal-to-noise ratio
(SNR). Note that this is a different scale then the task outcome
scale. It describes the ratio of the distance between means and
standard deviation of the goal. Since performance is a logarith-
mic function of the SNR, while the amount of Bayesian surprise
in a task is a squared function of SNR, users will run out of
capacity to process Bayesian surprise as the SNR increases. The
userâ€™s capacity to process Bayesian surprise can be measured,
in real time, by calculating the average reduction in KL Di-
vergence per unit of SNR. This capacity equates the amount
of cognitive load and/or sensorimotor load induced by a task
because it describes the amount of Bayesian surprise processed
by the brain. As per Shannon-Hartleyâ€™s theorem [50], optimal
performance is defined by the slope or average capacity of the
logarithmic function on the SNR scale. Note that we can also
express the SNR scale in bits if we take its binary logarithm, in
which case capacity becomes a linear function of log SNR.
(10) Error. Human error equals the Bayesian surprise left unpro-
cessed and is defined as the KL divergence ( ğ‘Â·SNR squared)
after remaining capacity (ğ‘âˆ’ğ‘Â·ğ‘™ğ‘œğ‘” 2 (ğ‘†ğ‘ğ‘…+ 1)) becomes zero.
The ğ‘ are empirically determined constants. When capacity
runs out error cannot be kept arbitrarily low: it starts follow-
ing the KL divergence and increases with the square of the
SNR. Error can thus be measured and predicted in bits in real
time, using the same scale as capacity: that of SNR. Note that
error can be modeled empirically by calculating the negative
log probability of the errors per unit of SNR. Also note that
the empirical error rate ğ‘in Equation 15 is expressed in bits of
uncertainty rather than probability.
The ultimate purpose of any user interface design, then, is to allow
the user to achieve Postulate 1 while avoiding Postulate 10. Together,
our postulates give us a mathematical tool to measure the capacity
of the user as it relates to potential error, in real time. The easiest
way to do this is to measure the mean distance to ğœ‡ğ‘” over some
time window, dividing this by the std. dev., and squaring the result.
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
Figure 2: Bayesian Fittsâ€™ Law. Distribution of the Posterior probability P(s|o) of the cursor being within the Goal, from top
to bottom: in the middle of a trial, at 2/3s of the trial and at the end of a trial. Goal is the 96% area of the Prior probability
distribution P(s), on both sides for a reciprocal trial. Note how the velocity of the cursor tracks the uncertainty in cursor
position.
Three Laws of HCI Revisited
The three laws of Human-Computer Interaction can be recast in
terms of the above equations. That includes Hickâ€™s Law, Fittsâ€™ Law,
the Power Law of Practice and, potentially, others that are yet to
be discovered using our method.
Fittsâ€™ Law
Kording and Wolpertâ€™s work [31, 62] strongly suggests that senso-
rimotor control is indeed guided by Bayesian prediction. Wu et al.
[65] and Williamson et al. [59] also suggest that Fittsâ€™ Law [19, 36]
is the result of the sensorimotor system performing Bayesian in-
ference on movement. Figure 2 shows this process. The goal in a
Fittsâ€™ Law task is to move the cursor (or hand) reciprocally between
two targets or goals at distance A and click within width W. The
goal can be modeled as a Prior probability distribution ğ‘ƒ(ğ‘ ). The
96% area under this probability distribution is sectioned off as a
width W that constitutes the visual target. This width W, then,
corresponds to a4 ğœ variance in the goal. In Fittsâ€™ Law experiments,
both the distance A between the means of the priors and their
standard deviations, i.e., and width W, are systematically varied
[36]. The prior distribution is evidenced over successive trials as
a histogram of the locations of clicks [57]. The figure also shows
a cursor. The velocity of this cursor reflects the uncertainty in the
position of the cursor relative to the goal ğ‘ƒ(ğ‘ ). This uncertainty
is our progress or posterior distribution ğ‘ƒ(ğ‘ |ğ‘œ) and needs to be
estimated from observation of the distance of the cursor to the goal
given a tolerance. Figure 2 also shows how, when the cursor moves,
this velocity and thus the variance in the posterior increases to a
maximum uncertainty, after which it decreases to a minimal veloc-
ity or uncertainty once the target is reached. On average, however,
velocities and thus the variance in the movement must match the
variance or size of the priors so as not to click outside the target.
In Interactive Inference, movement is modeled by a step-wise it-
erative update of the posterior to the prior(s), the size of which is
governed by the remaining bits of difficultyduringthe movement.
The calculation of a new estimate of cursor location is modeled as a
Bayesian update, in which a new progress distribution is iteratively
calculated based on the probability distribution of observations of
the hand given the goal distribution. Indeed, Welford suggested
early-on that performance is a function of a signal-to-noise ratio
in the brain and that there is a maximum capacity at which the
brain can process a Fittsâ€™ Law task due to theâ€œneural noise blurring
or distorting signalsâ€[ 57]. MacKenzie [36] suggested the following
equation for describing movement time in a Fittsâ€™ Law task:
ğ‘€ğ‘‡=ğ‘+ğ‘Â·ğ‘™ğ‘œğ‘” 2
 ğ´
ğ‘Š +1

(17)
Vertegaal et al.
Here, ğ‘€ğ‘‡ equals movement time, and ğ‘and ğ‘are empirically de-
termined constants. The equation is modeled after the Shannon-
Hartley theorem in Equation 2. We can instead define a KL diver-
gence in the form of Equation 15 in which errorğ¸is minimized:
ğ¸=ğ‘
ğ‘’+ğ‘
2ğ‘
2
(18)
Where ğ‘ is the size of the noise tolerance in the cursor (or hand)
movement (4ğœ of the goal distribution) and ğ‘’ the distance from
current position to the nearest edge of the goal distribution (see
Figure 2). If we rewrite this as a gradient descent (i.e., derivative
of Equation 18), we obtain an affine update rule that halves the
distance of the progress distribution at each step, with identical
time intervalsğ‘¡:
ğ‘’ğ‘¡+1 = 1
2 ğ‘’ğ‘¡ âˆ’1
2 ğ‘(19)
The stop condition is when the cursor finds the near edge ğ‘’ğ‘¡ â‰¤0.
This corresponds to a binary search for the target in which velocity
of the cursor is controlled by halving the distance to thefaredge of
the target, at each time interval, until thenearedge is found. Equa-
tion 18 describes the control function that encodes the potential
energy underlying the planning of the cursor (or hand) movement.
Equation 19 models the corresponding force field. Modeling actual
movement requires converting its potential energy into kinetic en-
ergy. This can be achieved by introducing a heavy-ball method, a
form of gradient descent that accounts for the input deviceâ€™s inertia
(mass) and the musclesâ€™ viscoelastic stiffness (damping), which is
beyond the scope of this chapter. In a reciprocal tapping task be-
tween the centers of two targets, the above models produce the
logarithmic time result found in Equation 17 [ 28]. In Interactive
Inference we formulate this result, instead, in terms of bits updated:
ğ¶=ğ‘+ğ‘Â·ğ‘™ğ‘œğ‘” 2
 ğ‘†
ğ‘ +1

(20)
Here, ğ¶equals the capacity of the human sensorimotor system (a
loop that includes pointing device and cursor) in bits, with ğ‘and ğ‘
empirically determined constants in bits. ğ‘† is the signal in this task,
and is given by the distance between the center of the goals. Because
Equation 19 describes the movement control function, rather than
the exact end of movement, over many reciprocal tapping trials the
expected end location clusters around the centerâ€“toâ€“center distance
ğ‘†, with a slight bias towards the near edges. ğ‘ describes the noise
tolerance of the cursor (or hand) in this task, and is equal to the
width of the goal distributions. The main benefit of Equation 19 is
that it can be used to evaluate Bayesian surprise, in bits,duringthe
pointing task. Empirical validation of this Bayesian underpinning
of Fittsâ€™ Law remains ongoing. Schmidt et al. [ 47] did show the
variance in motor impulse in a similar task to be linearly related to
the variance in the resulting distribution, i.e., the prior. Theirğ‘Šğ‘’
metric may correspond to theğ‘in Equation 20.
Another advantage of the above formulation is that capacity can
more easily be compared to error, if errors are converted to bits
using negative log probability. A third advantage is that it may
allow the comparison of the capacity of Fittsâ€™ Law tasks with that
of other tasks. This is important in cases where the overall capacity
of the userâ€™s actions needs to be calculated, in bits of unresolved
Bayesian surprise. From our earlier discussion follows that once a
user runs out of capacity, errors should track a KL divergence after
the user runs out of capacity:
ğ¸=ğ‘Â·
 ğ‘†
ğ‘
2
(21)
Here, ğ‘ is an empirically determined constant describing the er-
ror rate in bits. There is some evidence for such error curve [16].
Welford [57] did demonstrate a method for posthoc correction
of goal distributions, so as to adjust for differences in error rates
between different devices. His work appears consistent with the
above discussion. Wobbrock et al. [60] also modeled the probabil-
ity of an error, with good fit. Our model predicts that user error
only tracks the KL divergence once capacity is reached, while their
model describes the probability of error rather than the negative
log probability. Finally, Guiard et al. [23] analyzed the error in dif-
ferent conditions of a Fittsâ€™ Law experiment. Their RVE metric also
appears to be consistent with Equation 21.
Hick-Hyman Law
Hick-Hyman Law [25, 26] can also be derived from our Bayesian
model. Hick-Hyman Law describes user response time when the
task is to select one from a number of choices. Hicks [ 25] found
that the response time was a logarithm of the number of choices
ğ‘›+1:
ğ‘…ğ‘‡=ğ‘Â·ğ‘™ğ‘œğ‘” 2 (ğ‘›+1)(22)
Here, ğ‘is some empirically derived time constant that can be used
to convert the difficulty of the task to a time estimate. Unlike Fittsâ€™
Law, we model Hick-Hyman Law as a learning task in which the
user performs Bayesian inference to update a prior of possible
targets with an observation of a single highlighted candidate to
create a posterior of only one candidate. This means the role of
the prior and posterior distributions are not reversed: in this task
we update the prior to the posterior. Hyman [26] extended Hickâ€™s
experiment to include unequal probabilities for each choice. He
found that results were a function of the entropy of the task:
ğ‘›âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘– log2
 1
ğ‘ğ‘–

(23)
Where ğ‘ğ‘– is the probability of each choice in the set. Note that
as the set gets smaller, the chance of a correct answer increases. As
the set gets larger, it decreases by the number of elements in the
set. If we examine Equation 7 it is evident that this is equal to a
discrete KL divergence for the task, in which the distributions are
probability mass functions instead of proper density functions. Here,
the probability of the posterior is set to one because there is one
item to which the prior needs to be updated. The prior probability
is set to ğ‘ğ‘–. Conveniently, the binary logarithm of 1 reduces to zero
making this relative entropy equal to the total entropy of the task.
This means the only relevant parameter in this law is the noise in
the prior probabilities: the knowledge the user has at the beginning
of the task. According to Wu et al. [64], evidence from MRI scans of
the brain indeed suggests that the narrowing of the selection from
the prior to the observation follows step-wise Bayesian posterior
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
updates. According to them, the cognitive control network in the
brain eliminates half the options at each Bayesian update, yielding
a binary search with a logarithmic response time. This means Hick-
Hyman Law can be rewritten as a capacity function where noise
ğ‘ is a discrete probability that is the reciprocal of the number of
choicesn:
ğ‘Â·ğ‘™ğ‘œğ‘” 2 (ğ‘›+1)=ğ‘Â·ğ‘™ğ‘œğ‘” 2 (1
1
ğ‘›
+1)=ğ‘Â·ğ‘™ğ‘œğ‘” 2 (1
ğ‘ +1)(24)
Where ğ‘ is some empirically determined constant, in bits, de-
scribing information processing capacity per unit of 1 to noise
(rather than time per decision). The SNR of this task thus equals
the reciprocal of the noise in the prior, i.e., its precision. This means
any errors in the selection process should track our simplified KL
divergence after logarithmic user capacity runs out:
ğ‘Â·( 1
ğ‘)2 =ğ‘Â·ğ‘› 2 (25)
We see this as one of the predictions made by our model that
requires further empirical investigation.
Power Law of Practice
The third law of HCI, the Power Law of Practice [ 15], governs
the learning curve that arises as users are learning a new task.
Again, this is a learning task in which the prior is updated to the
posterior. The theoretical basis for the Power Law of Practice was
not explained: it is based on an empirical finding that was confirmed
in many experiments [15]. These suggest that the amount of time
ğ‘‡ spent performing a trial of a task is a negative power function of
the number of trialsğ‘¥performed:
ğ‘‡=ğ‘Â·ğ‘¥ âˆ’ğ‘ +ğ‘(26)
Where ğ‘ and ğ‘ are empirically derived constants. Within the
context of Interactive Inference, this Power Law is explained as a
reduction in the KL divergence between prior and posterior that
occurs when users are learning a task. This reduction is a function
of the signal-to-noise ratio improving with every trial. In deep
learning, this is known as minimizing the negative log likelihood
function as a function of SNR [6]:
ğ»=ğ‘âˆ’ğ‘Â·ğ‘™ğ‘œğ‘” 2
 ğ‘†
ğ‘

(27)
Where ğ‘and ğ‘are empirically derived constants. This equation
describes the reduction in entropy, in bits, as an improvement in
the signal-to-noise ratio of the task. The time taken per bit then
provides the Power Law of Practice.
The Case of Unequal Variances
There are some cases in which the simplification of the KL diver-
gence to a square function of the SNR does not work. Consider the
example of the distribution of socks that need to be placed in the
drawer. We note that we could consider this task a compound task
in which we sum the individual KL divergences of the movement of
each individual sock. In this example, we are simply going to take
the average of these, in an attempt to describe the overall relative
entropy of the task with one KL metric.
We recognize that the difficulty of this task is not just governed
by the size of the drawer and average distance of socks, but also by
the number and distribution of socks in the room or house. This
means that there is a discrepancy between the size of the distribu-
tion of socks, our posterior, and the goal, our prior. This discrepancy
also increases the KL divergence describing the difficulty of the
task. To calculate the KL divergence, we need to capture this differ-
ence in distributions between goal and progress. While the original
integral does this for any distribution, including non-Gaussians,
it is computationally expensive and not formulated in terms of
SNR. There exists a more general function that is mathematically
identical to Equation 7 that describes the KL divergence in cases of
Gaussian distributions with unequal variances [38]:
log2
ğ‘ƒ(ğ‘ )
ğº(ğ‘ ) =log 2
ğœğ‘”
ğœğ‘

+log 2 exp
 
âˆ’(ğ‘ âˆ’ğœ‡ ğ‘)2
2ğœ2ğ‘
+(ğ‘ âˆ’ğœ‡ ğ‘”)2
2ğœ2ğ‘”
!
ğ¾ğ¿ (ğ‘ƒ(ğ‘ )||ğº(ğ‘ ) )=log 2
ğœğ‘”
ğœğ‘

+E ğ‘
"
(ğ‘ âˆ’ğœ‡ ğ‘”)2
2 ln(2)ğœ2ğ‘”
#
âˆ’E ğ‘
"
(ğ‘ âˆ’ğœ‡ ğ‘)2
2 ln(2)ğœ2ğ‘
#
(28)
The expectation underğ‘ƒ(ğ‘ )equals:
Eğ‘

(ğ‘ âˆ’ğ‘) 2
=(ğœ‡ ğ‘ âˆ’ğ‘)2 +ğœ2
ğ‘
Eğ‘
h ğ‘ âˆ’ğœ‡ ğ‘”
2i
=(ğœ‡ ğ‘ âˆ’ğœ‡ğ‘”)2 +ğœ2
ğ‘
Eğ‘
h ğ‘ âˆ’ğœ‡ ğ‘
2i
=(ğœ‡ ğ‘ âˆ’ğœ‡ğ‘)2 +ğœ2
ğ‘ =ğœ2
ğ‘
(29)
Substituting expectations:
ğ¾ğ¿ (ğ‘ƒ(ğ‘ )||ğº(ğ‘ ) )=log 2
ğœğ‘”
ğœğ‘

+
 ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
2 +ğœ2
ğ‘
2 ln(2)ğœ2ğ‘”
âˆ’
ğœ2
ğ‘
2 ln(2)ğœ2ğ‘
(30)
To describe error, we include an empirical variableğ‘in this equa-
tion that relates the KL divergence (where ğ‘= 1) to an observed
empirical error rate:
ğ¾ğ¿ (ğ‘ƒ(ğ‘ )||ğº(ğ‘ ) )=ğ‘Â·
 
log2
ğœğ‘”
ğœğ‘

+
 ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
2 +ğœ2
ğ‘
2ğ‘™ğ‘›(2)Â·ğœ 2ğ‘”
âˆ’ 1
2ğ‘™ğ‘›(2)
!
(31)
Here, ğœ‡ğ‘” and ğœğ‘” signify the mean and standard deviation of the
goal distribution. ğœ‡ğ‘ and ğœğ‘ signify the mean and standard deviation
of the progress distribution. The natural logarithms are constants,
and are introduced by base conversion. Including these allows for
an exact result in bits of relative entropy. The main difference
between Equation 15 and Equation 31 is the addition of the log
variability ratio [49] of goal and progress distributions. As such,
Equation 31 models themismatchin variances as an increase in the
KL divergence that causes the average entropy to increase, raising
the entire loss function.
Precision/Speed Trade-off
Variance ğœ2
ğ‘” represents the tolerance in the goal, while variance ğœ2
ğ‘
describes the speed of progress towards that goal, meaning this
Vertegaal et al.
equation captures the full speed/accuracy trade-off. However, note
that the word accuracy is incorrectly used here, and should be
worded speed/precision trade-off. The relationship to Fittsâ€™ Law is
evident: if the hand moves slower or faster than the size of the tar-
get allows, a non-optimal result is produced. Stated more generally,
when the user moves outside the tolerance in the task outcome, the
task is executed sub-optimally, and by extension, is performed in
non-optimal time or error conditions. In terms of Interactive Infer-
ence, the posterior update towards the prior is not of an appropriate
size. On average, therefore, ğœ2
ğ‘ should match ğœ2
ğ‘” once the goal is
approached. Hence, the simplified Equation 15 suffices when con-
sidering optimal performance as both variances, on average, need
to be equal for an optimal speed/precision trade-off. This appears to
be the case in Fittsâ€™ Law. Sinceğœ2
ğ‘ directly represents velocity when
ğ‘ is a spatial distance metric, this also suggests a linear relationship
between velocity and target width [47].
Empirical Study
We now turn to a case study, where we empirically examine whether
our general model can predict human performance in a particu-
lar situation. If it can, it provides evidence that the general model
is worthy of further exploration. In particular, we want to verify,
via an empirical study, whether the above postulates could suc-
cessfully model a task that does not yet have a known model, and
function as a real-time measurement tool. We used our simplified
KL Divergence to measure the information processing capacity
of participants in a simulated targetavoidancetask: maintaining
distance while driving behind a car. Our attempt to develop a better
model of a driverâ€™s capacity is more than an academic exercise, as
such a model can help modern car interfaces better recognize and
mitigate problems of driver distraction [22].
Study Literature Review
In-depth analysis of crash data shows that nearly one third of all
fatal and injury crashes involve driver inattention [66]. Lee et al.
identified a 30% (310ms) increase in reaction time when a speech-
based email system is used, with implications for safety [33]. Strayer
et al. demonstrated that inattentional blindness can be caused by
concurrent cell phone usage while driving, significantly increas-
ing the risk of an accident [55]. In a study involving adult drivers,
Strayer and Drews observed an 18% slower reaction time while
conversing on a cell phone [54]. Systematic reviews of distracted
driving, such as those by Simmons et al. [51, 52] and Caird et al. [10]
overwhelmingly demonstrate that tasks that require the driversâ€™
visual attention, such as dialing, texting, and in-car infotainment
touch screen use generate more risk than tasks that do not require
visual attention, as these direct the driverâ€™s attentive resources
away from the road and thus the primary task [ 40]. Researchers
have shown particular interest in the car following task due to
its prevalence in driving. Wolfe et al. â€™sinformation acquisition the-
orymodeled the information a driver acquires through peripheral
vision and eye movements [ 61]. Boer pioneered the use of a be-
havioral entropy measure as an index of workload, and suggested
that information entropy (the average amount of information) is
the basis for driving behaviour [7]. Boer et al. used the sample en-
tropy algorithm to gauge performance of drivers, as demonstrated
through a driving simulator study [8]. Senders did pioneering work
onocclusion distance: the distance a driver feels comfortable driving
with their eyes shut [48]. Kujala developed this metric further as
a way to measure visual demand while driving [29]. Building on
Wildeâ€™srisk homeostasis theory[ 58], Lu et al. developed adesired
safety marginmodel as a quantified index of risk perception when
car following [35]. That model is based on the leading and follow-
ing car speeds, and the relative distance between them. Zhu et al.
analyzed the driverâ€™s choice of headway (the time interval between
two cars at a given speed) in various traffic conditions using a large
database [67]. Wu et al. proposed a risk repulsion factorFfor car-
following as determined by the time headway, relative speed, and
safe spatial headway [63]. The driverâ€™s cognitive load has also been
assessed and captured as a metric when car-following [56]. This is
important especially when secondary tasks consume some or all of
that load. Subjective questionnaires are often carried out to collect
the participantsâ€™ assessment of task difficulty, risk, comfort and
effort. For example, Lewis-Evans et al. identified a negative relation-
ship between the time headway in a car following task and ratings
of task difficulty in a simulator experiment [ 34]. However, there
are many issues with subjective assessments, including but not
limited to bias caused by participantsâ€™ preferences towards socially
desirable responses [ 32]. Others have proposed various physio-
logical measurements as indicators of the subjectâ€™s mental effort,
such as heart rate variability, electromyogram, and facial tempera-
ture [17, 27, 39]. Yet such portable physiological sensing solution
are unreliable, while trajectory-based risk assessment methods are
too complex [42].
KL Divergence of Overlap
We used our framework to model the car following task probabilis-
tically, as illustrated in Figure 3. The task is represented using two
normal distributions that describe the uncertainty in the lead and
following carâ€™s position. In our model, these are the goal distribu-
tion (of the front car) and a progress distribution (of the following
car). The task for the driver of the following car is to ensure the
progress distribution avoids overlap with the goal distribution. The
overlap between the two distributions follows a KL divergence that
indicates the amount of bits of processing by the following driver
to perform this task. Our simplified KL divergence models this as
follows:
ğ»=ğ‘Â·
 ğœğ‘”
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”
2
=ğ‘Â·
ğ‘
ğ‘†
2
(32)
Where ğœ‡ğ‘” is the mean of the goal distribution, ğœ‡ğ‘ is progress
distribution. Recognize that what matters here is the mean relative
distance between cars (ğœ‡ğ‘âˆ’ğœ‡ğ‘”)as it relates to variability in the goal,
the deceleration behaviour of the front car ğœğ‘” that needs matching.
Note also that since we are avoiding rather than meeting the goal,
we need to take the reciprocal of the signal-to-noise ratio, i.e., the
noise becomes the signal.
ğ» is the amount of Bayesian surprise, in bits, that the driver
would need to process in a car following task. However, recall that
the capacity C of the driverâ€™s ability to process this information
is only logarithmic. We can establish parameters ğ‘ and ğ‘ of this
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
Figure 3: Two cars with normal distributions ğº(ğ‘¥) and ğ‘ƒ(ğ‘¥) representing uncertainty in their position. The percentage of
overlap between distributions equals the probability of a collision. The negative logarithm of this probability gives the number
of bits of difference.
logarithm by measuring the amount of information ğ» using Equa-
tion 32 and by performing a regression on the result (we discuss
how we operationalized this in Section ):
ğ¶=ğ‘+ğ‘Â·ğ‘™ğ‘œğ‘” 2
 ğœğ‘”
ğœ‡ğ‘ âˆ’ğœ‡ğ‘”

=ğ‘+ğ‘Â·ğ‘™ğ‘œğ‘” 2
ğ‘
ğ‘†

(33)
Task
We designed a controlled car following task in a driving simulator
to collect driving data in target avoidance conditions with vary-
ing noise-to-signal ratios. Our goal with this experiment was to
model the driversâ€™ information load using our equations to gauge
the amount of information used to maintain a distance ğ‘† when
following a lead car with variation ğ‘ (see Figure 3). We focused
our experimental analysis on deceleration events that may lead to
collision.
ğ‘† and ğ‘ were varied between conditions. As in real driving,
ğ‘ was not static within a condition: the lead car accelerated and
decelerated to different speeds, which added a normally distributed
noise ğ‘ to each distance condition. This allowed us to capture
participantsâ€™ handling of the car in a range of driving difficulty
levels defined by the constituting ğ‘/ğ‘† ratios. We hypothesized
that driverâ€™s information capacity during decelerations would be
logarithmic with the noise-to-signal ratio, with a mean capacityğ‘, in
bits of information per bit of difficulty. Because driver information
capacity is logarithmic, but the amount of information generated
is quadratic, at capacity ğ‘, the amount of information will outstrip
the driverâ€™s ability to process. We hypothesized that this will lead
to an error statistic that follows the KL divergence in Equation 32.
Participants
We recruited fifteen participants (5 female, 10 male) from our orga-
nization, with a mean participant age of 27. Four of the participants
had low driving experience (one year or less).
Experimental Setup
We presented a simulated driving task with simulated road condi-
tions using BeamNG [5], a car simulator video game chosen for
its realism, physics engine, low cost, and programmability. Each
participant driver was seated in a simulator in front of a curved 49-
inch monitor with a 32:9 screen ratio. The monitor was positioned
0.6 m from the participant at eye level to provide an immersive
simulator view of a car dashboard and windscreen (see Figure 4). A
Logitech G29 steering wheel and pedal unit with three pedals were
mounted under the simulator rig. Participants were asked to adjust
their seat position so that the steering wheel and accelerator/brake
pedals were at a comfortable distance. The pedal clutch was not
used. Every participant used the same custom car model with the
same physics characteristics in BeamNG.
We constituted our conditions through custom scripts in BeamNG.
We also wrote a Python program to interface with BeamNG to ad-
minister the study and to capture driving data at a 450 Hz sampling
rate: vehicle positions and speeds of the driverâ€™s car and lead car,
pedal data, and collision data. Our custom Python scripts filtered
and analyzed the data.
During the experiment, participants had a clear view of the
lead car and road through their simulated windscreen. Because a
2D simulator does not provide 3D depth cues, participants were
shown a floating fixed-width rectangular bar in the center of their
windscreen, positioned below the bumper of the lead car. That barâ€™s
width reflected what the perceived width of the lead carâ€™s bumper
should be at the optimal following distance. The driverâ€™s task was
to maintain an optimal following distance, where the driver would
try to match the barâ€™s size to the perceived size of the bumper of the
lead car. For example, in Figure 4, the bar is wider than the bumper,
meaning the driver should move closer to the lead car.
Experiment Design
We used a within-subject experiment where the task was to follow
the lead car at a specified distance ğ‘†. We then varied the speed
Vertegaal et al.
Figure 4: Participantâ€™s experimental setup. The white bar
in the center of the screen helped participants perceive the
correct distance ğ‘†. They needed to match the size of this bar
with the lead carâ€™s bumper while driving. Here, it is wider
than the lead carâ€™s bumper indicating that the participant
needs to accelerate.
of the lead car with variation ğ‘. Each participant experienced 12
different task conditions with two factorsğ‘†andğ‘.
â€¢ Factor ğ‘† comprised four levels of fixed distances to the lead car
(2.84 m, 4.84 m, 6.84 m, and 8.84 m).
â€¢ Factor ğ‘ comprised three levels centered around a mean lead
car speed of 100 km/h. Each level represented a 4.1ğœ of a normal
distribution of varying distances to the lead car, as generated by
increasing the range of the lead carâ€™s allowable speeds (1.93 m:
100Â±12 km/h; 4.19 m: 100Â±18 km/h; and 7.21 m: 100Â±24 km/h).
That is, the levels represented increasing variability and thus
noise in how the car in front accelerated and decelerated. To
explain these numbers, at level 1 the speed was allowed to vary
between 88 â€“ 112 km/h (i.e., 100Â±12 km/h), where 96% of the
variation in distance is captured by 1.93 m.
To constitute each distribution, for each level of ğ‘, the lead car
randomly decelerated 20 times and randomly accelerated 20 times,
although always keeping within the levelâ€™s range of speed values.
Each trial within a level of ğ‘ comprised a new speed adjustment,
which occurred approximately every two seconds. We random-
ized the presentation order of conditions. We also randomized the
presentation order of acceleration or deceleration trials, with accel-
erations never occurring within a deceleration trial or vice versa.
We measured the instantaneous noise-to-signal ratio (change
in distance by lead car Î”ğ‘†, i.e., some sample of ğ‘; mean distance
to lead car ğ‘†, per trial), and pedal activity by the driver. As will
be detailed further, our dependent variables were the amount of
information processed during a trial (KL divergence), and error
(collisions). Together, the 40 trials created a condition simulating a
continuous driving task of 2 minutes.
Procedure
Participants were instructed to hold but not use the steering wheel.
They were instructed to only use the brake and accelerator pedals to
maintain the distanceğ‘†, as the lead car varied its speed, by matching
the bar with the bumper of the lead car. The road simulation was
straight and without obstacles, allowing cars to travel in a straight
line. Participants were given a 5 minute training task to familiarize
themselves with the controls and task. Each condition began with
both cars automatically accelerating to 100 km/h, with the specified
distance ğ‘†. Participants were then given a visual alert to take control
of the car. After each condition ended, we asked the participants to
rate the driving difficulty of that condition using a 7-point Likert
scale [34]. Trials that resulted in collision were repeated, with any
collisions only counting once per trial, per participant.
Ecological Validity
Our task was designed such that participants would be in a position
to experience error. A choice for longer distances and lower speeds
would mean the chances of a collision would be too low to be
measurable and compared with the driverâ€™s remaining capacity.
This meant having relatively low headways at high absolute speeds
that could be criticized for their limited ecological validity. We note
however that the important parameter is not the absolute speed
at which the cars are driving, but the variation in their average
relative distance over time. This applies equally to parking a car
with respect to a solid barrier at low speeds, as it does to avoiding
a lead car at 100 km/h. We also chose for the simulator rendering
to have most environmental cues removed, making it difficult for
participants to estimate an absolute speed. This choice made the
experiment more generalizable by avoiding potential confounding
factors generated by environmental renderings. The choice not to
have participants steer was inspired by the potential for introducing
confounding variables as well.
Results
We first describe our data analysis, and then present the results of
our empirical evaluation of the models.
Measurement and Data Analysis
In our experiment, lead car accelerations never resulted in a colli-
sion. Given our interest in the relationship between target avoid-
ance and the risk of collision (error), we constrained our analysis
to trials where participants responded to a deceleration of the lead
car by braking or by releasing the accelerator pedal. We used 3
different levels of precision for noise-to-signal measurements in
our analysis. Each used a different confidence interval for defining
ğ‘.
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
(1) At the condition level of precision, ğ‘ was defined by 4.1ğœ of
the distribution of relative backwards distances traveled by the
lead car during trials in that condition;
(2) At the trial level of precision, ğ‘ was defined by the actual
relative backwards distance traveled by the lead car during
each trial; and
(3) Within each trial, ğ‘ was defined by the standard deviation
(ğœ)of a one-second time window of instantaneous distance
measurements, as detailed below.
Empirical Factors Noise (N) and Signal (S).Although we used prede-
termined levels of factorğ‘†, we administered factorğ‘ by altering the
speed of the lead vehicle, which was affected by the physics engine.
As such, we measured the actual negative relative distance traveled
by the lead car (Î”ğ‘†) per trial, from the moment of deceleration of
the lead car to the lead car reaching the desired speed. In line with
Welfordâ€™s statistical analysis of Fittsâ€™ law target widthğ‘Š [57], we
then calculated ğ‘ as 4.1ğœ of the distribution of the Î”ğ‘†s actually
administered per level of conditionğ‘ (a confidence interval of 96%).
Empirical Noise-to-Signal Ratio.As each trial involved a braking
event that resulted in processing of information given an individu-
ally administered Î”ğ‘†, many similar ğ‘/ğ‘† ratios were created across
conditions. We therefore used these more detailed empirical ob-
servations of noise-to-signal ratios for our log-linear regression
analysis, pooled across all trials. Here, our definition of noise-to-
signal ratio was Î”ğ‘† per trial divided by the measured mean ğ‘† (ğ‘†)
per trial, yielding the metric Î”ğ‘†/ğ‘†. We also used this definition
for our regression analysis of error rates, as this yielded directly
comparable results of error rates vs. capacity, in bits. This definition
was also used for our Index of Difficulty (see Equation 36). Note
that the only effect of the use of sampled Î”ğ‘†values instead of some
confidence intervalğ‘is a scaling effect onğ‘/ğ‘†ratios.
Measuring Information Processed.Our first dependent variable was
the amount of information processed by the driver during decel-
erations. To measure the instantaneous amount of information
processed within a trial, we used a more detailed 1-second rolling
time window to sample the distribution of distances ğ‘† to the lead
car at 450 Hz. Our definition of noise ğ‘ here was the standard
deviation (ğœ) of the distribution of ğ‘† obtained in this time window,
while our definition of signal ğ‘† was the mean ğ‘† (ğ‘†) of this time
window. First, the noise-to-signal ratio was sampled using this type
of one second window at the beginning of each trial, ending at the
moment the driver responded to the stimulus. We then calculated
the amount of information in the stimulus by entering this first
noise-to-signal ratio into Equation 32. A one-second window again
sampled the noise-to-signal ratio at the end of each trial, prior to
(i.e., ending at) a new deceleration or acceleration event. We calcu-
lated the remaining amount of information by entering this second
noise-to-signal ratio into Equation 32.
Subtracting the remaining amount of information from the amount
of information in the stimulus gave us the amount of information
processed by the driver during each deceleration trial. We subse-
quently aggregated measurements across all conditions, sorting
results using the Î”ğ‘†/ğ‘† from Section , and binning measurements
that were within 0.05 units of Î”ğ‘†/ğ‘†. While this means our defi-
nitions for signal-to-noise were different between y and x axes,
Table 1: Mean KL (s.e.) for consumed information (in bits)
for levels of factorsğ‘andğ‘†.
S
N 2.84 m 4.84 m 6.84 m 8.84 m
1.93 m0.11 (0.07) 0.06 (0.04) 0.03 (0.02) 0.01 (0.02)
4.19 m0.16 (0.09) 0.09 (0.04) 0.05 (0.03) 0.04 (0.04)
7.20 m0.17 (0.09) 0.10 (0.03) 0.08 (0.04) 0.05 (0.02)
we note that our instantaneous KL measurements of information
(y-axis) are orthogonal to and independent from the definition of
empirical signal-to-noise ratioper trialused for the index of diffi-
culty.
Measuring Error.Our second dependent variable was error. Error
was defined, across conditions, as the occurrence of a collision with
the lead car. All trials that ended in error were repeated. Since any
instantaneous measure of ğ‘/ğ‘† during a collision would yield infin-
ity during the 1 second period leading up to the collision, we used
the Î”ğ‘†/ğ‘† of the repeat of the trial as our signal-to-noise metric.
This allowed comparison of error and capacity metrics using the
same scale. We could not establish any measure for unprocessed
information for trials that ended in collision. We could, however,
convert error rates into bits of information in a manner that pro-
vided insights into the relationship between driver capacity and
error rates. After binning errors within 0.5 Î”ğ‘†/ğ‘†, we calculated the
error probability ğ‘(ğ¸)by dividing the number of errors per bin by
the total number of errors across all conditions. We then converted
the complement of this probability to bits ğ¸ğ‘ (the non-surprise of a
collision) using a negative log probability transform [50]:
ğ¸ğ‘ =âˆ’log 2 (1âˆ’ğ‘(ğ¸))(34)
Effects of Factors N and S
Table 1 shows the mean KL divergence values for consumed in-
formation against levels of factors ğ‘ and ğ‘†. Since the residuals
in our ANOVA model of the dependent variable differed signif-
icantly from a normal distribution (KS test, p>0.05), we used a
non-parametric Friedman test. The Friedman test showed highly
significant effects of both factorsğ‘(ğœ’ 2 (2)=17.73,ğ‘<0.001) and
ğ‘†(ğœ’2 (3)= 40.84,ğ‘< 0.001) on the number of bits consumed by the
driver (note that a two-way ANOVA produced the same results).
Car Following Model
Log-linear regression of binned KL divergence (overlap) measure-
ments onto noise-to-signal measurements Î”ğ‘†/ğ‘† yielded a logarith-
mic relationship(ğ‘Ÿ 2 =0.86,ğ¹(1,26)=164.2,ğ‘<0.001):
ğ¶=âˆ’0.02+0.25 log 2
Î”ğ‘†
ğ‘†
+1

(35)
Figure 5 shows this relationship between driver capacity and ag-
gregated signal-to-noise measurements on a Î”ğ‘†/ğ‘† scale across con-
ditions, as well as on a logarithmic scale (below the x-axis). Mean
capacity is visualized as a dotted line. Subtracting driver capacity
from this mean capacity yields remaining driver capacity (black
solid line). In subsequent sections, we will show how this remaining
Vertegaal et al.
0.6 1 1.3
Noise/Signal
1.6 1.8 2
Information (bits)
IndexofDifficulty
(Î”S/S)
Figure 5: Driver information capacity plotted against noise-to-signal ratio ( Î”ğ‘†/ğ‘†, index of difficulty (ID) on a logarithmic scale
below x-axis). Capacity followed a logarithmic function (blue solid), with a slope of 0.25 bits (green dash). Black solid line
indicates remaining capacity.
capacity was used to drive the simulatorâ€™s user interface. The loga-
rithmic scale at the bottom of Figure 5 shows the index of difficulty
(ID) of this task, in bits:
ğ¼ğ·=log 2
Î”ğ‘†
ğ‘†
+1

(36)
The mean capacity (bandwidth ğ‘) of our car following task was
measured at ğ‘= 0.25bits/bit, while the Index of Performance
(throughput1 /ğ‘) was 3.96 bits/bit. The y axis indicates bits of con-
sumed information, while the x axis indicates noise-to-signal ratio
Î”S/ğ‘†, or bits ofğ¼ğ· if the bottom log scale is used. Note that the only
difference in plotting Figure 5 on a log-linear scale against ğ¼ğ·, as
is customary when plotting with Fittsâ€™ law, is that lines would be
straight, with the bottom scale as the primary x-axis. Mean capacity
is expressed as bits of information consumed per bit of difficulty,
while the Index of Performance (1/b) is expressed as the bits of
difficulty processed per bit of consumed information.
Error (Collision) Model
Regression of error metric ğ¸ğ‘ (non-surprise of a collision, in bits)
onto noise-to-signal measurements ( Î”ğ‘†/ğ‘†) yielded a significant
relationship with the KL divergence metric (Equation 32) ( ğ‘Ÿ2 =
0.94,ğ¹(1,5)=80.5,ğ‘<0.001):
ğ¸ğ‘ =0.03+0.13
Î”ğ‘†
ğ‘†
2
(37)
Figure 6 shows the relationship between error (red solid) and aggre-
gated signal-to-noise measurements on aÎ”ğ‘†/ğ‘†scale across condi-
tions, as well as on a logarithmic scale (ğ¼ğ·, bottom), compared with
driver capacity (blue solid) and remaining capacity (black solid).
The total number of collisions across all trials was 90. Error rates
below or equal to a noise-to-signal ratio or ğ¼ğ· of 1 trended, on
average, slightly below the expected 4% (at 0.05 bits or 3.4%, be-
tween 2 and 4 collisions across trials), while those above 1 increased
quadratically, averaging 0.38 bits (23%), with a maximum of 0.82
bits (43% or 39 collisions across trials) atğ¼ğ·=1.85.
Qualitative Results
Figure 7 shows participantsâ€™ questionnaire responses ranking dif-
ficulty per conditional ğ‘/ğ‘†, sorted by ğ¼ğ·. These represent mean
responses to the question â€œThis driving task was difficultâ€, on a scale
from 1 (Strongly Disagree) to 7 (Strongly Agree) [34]. A Friedman
non-parametric analysis of variance showed participants rated the
car following task as significantly more difficult with increasing
ğ¼ğ· (ğœ’2 (11)= 116.4,ğ‘< 0.0001). Note that all participants found
the task to be easy to interpret and execute, while none reported
distraction by the floating bar graphic during the task.
Discussion
We will now discuss the results from our empirical study, relate
them to our Interactive Inference model, and provide some sugges-
tions for future directions.
Empirical Study
The above results are a first step towards empirical validation of
both our capacity and error models. Results show a good to excellent
fit between between predicted and observed models of the task.
Driver information processing capacity during deceleration events
in a simulated car following task can be measured in real time using
the simplified KL divergence of the noise-to-signal ratio of the
distance to the car followed, over a time window. Our hypotheses
that both distance ğ‘† and its variance ğ‘ to the lead car significantly
affect the amount of information consumed by the driver during
such events were confirmed, with more information consumed at
lowerğ‘†s and higherğ‘s.
Error rates also appear to independently confirm the effect of
ğ‘/ğ‘† ratios on driver information capacity when car-following. As
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
0.6 1 1.3 1.6 1.8Index of Difficulty
Noise/Signal
Information (bits)
(Î”S/S)
Figure 6: Error (non-surprise, in bits) plotted against noise-to-signal ratio ( Î”ğ‘†/ğ‘†, index of difficulty (ID) on a logarithmic scale
below x-axis). Error followed a KL divergence (red dots). Blue solid lines shows driver capacity and black solid line remaining
capacity.
Figure 7: Perceived task difficulty for each condition: participantsâ€™ responses to the question â€œThis driving task was difficultâ€
on a scale of 1 (Strongly Disagree) to 7 (Strongly Agree).
expected, the (negative log of the complement of) error rate fol-
lowed a KL divergence. When driver capacity exceeds mean capac-
ity, error rates appear to increase with the square of the noise-to-
signal ratio. We attribute these trends to â€œleft-overâ€ Bayesian sur-
prise, generated by high noise-to-signal ratios, that is not consumed
by the driver. Note that we used mean capacity as our capacity limit
because the theoretical maximum channel capacity is, in practice,
never reached (without significant error) [50]. Our results also sug-
gest bit rates are quite low. We presume this is because information
processing is throttled by the eye/foot/pedal processing loop.
Qualitative ratings by participants were in line with the above.
They rated the task as more difficult with higher noise-to-signal
ratios (and thus ğ¼ğ·s), albeit with bipolar ratings of middle ğ¼ğ·s. We
attribute this to participants perceiving distant targets (ğ‘†> 4.8 ğ‘š)
as easier to process than close targets with similar signal-to-noise
ratios, perhaps due to the 2D nature of our simulator setup. We again
note that our results are limited to modeling driver responses to lead
car decelerations in a simulated car following task using specific
road conditions, car physics, and a sample set of participants.
Interactive Inference Model and Future Directions
Our results provide good support for the Interactive Inference
model, which shows how capacity for performance can be related
to error, both in bits. We model average capacity as a logarithmic
Vertegaal et al.
function of SNR, scaled by a slope that is empirically determined.
We can consider this slope to be analogous to the bandwidth in the
Shannon-Hartley theorem. Because capacity beyond the limits of
this model cannot occur without error, and because both metrics
can be converted to bits, one of the predictions from our model
is that error can be analysed in view of leftover capacity. This is
because the average capacity is, in fact, the maximum capacity
without error. Error subsequently tracks Bayesian surprise as a KL
divergence. Figure 6 shows this relationship, with the KL diver-
gence in red fit to some empirically observed error rate (converted
to negative log probability) visualized in red dots. Capacity is mod-
eled as a logarithmic function (in blue), which is an empirical fit
to Bayesian surprise processed at different SNRs of the task. By
subtracting the logarithmic function from this value, we can arrive
at a leftover capacity metric (in black). Where this line reaches zero
capacity, errors increase and start tracking the KL divergence: the
square of the signal-to-noise ratio. The empirical evaluation of this
relationship is considered a future direction.
Left-over capacity would correspond directly to the cognitive or
sensorimotor load of the task. By correlating empirical results with
Heart Rate Variability (HRV) data and other measures of cognitive
load evidence for this relationship could be examined. One impor-
tant topic of investigation is to determine if cognitive load could
be measured during tasks, in real time. E.g., users could complete a
task modeled via our method while wearing a heart rate monitor.
HRV should be lower during moments in the task where there is
significant left-over surprise. The correlation between HRV and
cognitive load suggests it may be possible that left-over Bayesian
surprise also tracks stress levels in participants. The relationship
between our models and experienced stress levels of participants
during tasks is therefore an interesting area of future study. E.g.,
users could complete a task modeled via our method and then fill
out a questionnaire about their stress levels. Left-over surprise met-
rics should correlate with questionnaire results if our hypothesis is
correct. Because our metrics are in bits, rather than time, it should
be possible to compare both capacity and error between different
tasks. Analyzing different tasks for a signal-to-noise ratio param-
eter that can be measured, and comparing results from empirical
investigations of the relationship between SNR and both capacity
and error, would further evidence this assertion. E.g., users could
complete different tasks while wearing HRV monitors. Left-over
capacity metrics should correlate not just with HRV data, but also
between tasks, if our hypothesis is correct. We consider such em-
pirical work a future direction and invite others to help investigate
this promising area of research.
Limitations
This work presents an exploratory framework inspired by Active
Inference principles that does not require implementation of its full
mathematical machinery (such as model evidence and expected free
energy). Our use of the term Bayesian surprise refers specifically
to the KL divergence between our operationally-defined goal and
progress distributions, not a formal construct from Active Infer-
ence theory. In addition, the connections we draw to Fittsâ€™ Law,
Hickâ€™s Law, and the Power Law are observational. Finally, our em-
pirical validation is limited to a single driving task, and further
research is needed to test the generalization of this approach to
other interaction contexts.
Conclusions
Neuromorphic Human-Computer Interaction aims to improve user
interfaces by exploring how concepts from neuroscience might in-
form design. Interactive Inference offers a simplified interpretation
of the neuroscience theory of Active Inference in user interface
design. It postulates that users perform Bayesian inference on cur-
rent progress and goal distributions to learn tasks and predict their
next action. The Bayesian surprise (relative entropy) between goal
and progress distributions can be modeled as a square function.
However, human capacity to process Bayesian surprise follows a
logarithmic function. This explains why errors occur when the user
runs out of capacity. We believe our model may allow the quan-
titative analysis of performance and error in a single framework
that allows estimation of the mental load of a task and showed how
three basic laws of HCI, Hickâ€™s Law, Fittsâ€™ Law and the Power Law
of Practice can be derived from Interactive Inference. We evalu-
ated the use of the model in an empirical study. In our simulated
car following task, driver information processing capacity during
braking events was linearly related, with good fit, to a logarithmic
model of the noise-to-signal ratio of the distance to a lead car, plus
1 (in bits). We found a significant effect of both distance ğ‘† and
variation ğ‘ on capacity, with a mean driver Index of Performance
of almost 4 bits of difficulty (as defined by ğ‘™ğ‘œğ‘”2 (ğ‘/ğ‘†+ 1))per bit
of information. Participantsâ€™ qualitative ratings followed the same
trend, with significantly higher ratings of difficulty with higher
ğ‘/ğ‘† ratios. We used our model to derive a remaining capacity for
the driver, by subtracting the log-linear curve from mean capac-
ity, in bits. We also fit error (collision) rates, with excellent fit, to
the square of the signal-to-noise ratio of the distance to the lead
car, in bits. We hypothesize that collisions are due to unprocessed
information where driver capacity is exceeded. Overall, the empiri-
cal results provide support for our Interactive Interference model
of HCI. While car-following is just one case study, it provides a
framework for conducting further empirical research in Interactive
Inference, and evidence that such research is well worth doing.
We invite other researchers to test and apply this model to other
tasks, to see how well these models predict human performance,
and to confirm or refute the general applicability of Neuromorphic
Human-Computer Interaction in general, and Interactive Inference
in particular.
References
[1] John R. Anderson, Michael Matessa, and Christian Lebiere. 1997. ACT-R: A theory
of higher level cognition and its relation to visual attention.Humanâ€“Computer
Interaction12, 4 (1997), 439â€“462.
[2] Pierre Baldi and Laurent Itti. 2010. Of bits and wows: A Bayesian theory of
surprise with applications to attention.Neural Networks23, 5 (2010), 649â€“666.
[3] Thomas Bayes. 1763. An essay towards solving a problem in the doctrine of
chances.Phil. Trans. of the Royal Soc. of London53 (1763), 370â€“418.
[4] Matthew James Beal. 2003.Variational Algorithms for Approximate Bayesian
Inference. Ph. D. Dissertation. University of London, London, UK. http:
//mlg.eng.cam.ac.uk/zoubin/papers/beal03.pdf PhD thesis, supervised by Zoubin
Ghahramani.
[5] BeamNG. 2013. BeamNG.drive. WebPage. Retrieved Aug 1, 2023 from https:
//www.beamng.com/game/
[6] Christopher M Bishop and Hugh Bishop. 2023.Deep learning: Foundations and
concepts. Springer Nature.
Interactive Inference: A Neuromorphic Theory of Human-Computer Interaction
[7] Erwin R. Boer. 2000. Behavioral entropy as an index of workload.Proceedings
of the Human Factors and Ergonomics Society Annual Meeting44, 17 (July 2000),
125â€“128. doi:10.1177/154193120004401702
[8] Erwin R Boer, Michael E. Rakauskas, Nicholas J. Ward, and Michael A. Goodrich.
2005. Steering entropy revisited. InDriving Assessment Conference, Vol. 3.
Public Policy Center, Rockport, Maine, USA, 25â€“32. https://doi.org/10.17077/
drivingassessment.1139
[9] Ludwig Boltzmann. 1866.Ãœber die mechanische Bedeutung des zweiten Hauptsatzes
der WÃ¤rmetheorie:(vorgelegt in der Sitzung am 8. Februar 1866). Staatsdruckerei.
[10] Jeff Caird, Kate Johnston, Chelsea Willness, Mark Asbridge, and Piers Steel.
2014. A meta-analysis of the effects of texting on driving.Accident Analysis &
Prevention71 (Oct. 2014), 311â€“318. https://doi.org/10.1016/j.aap.2014.06.005
[11] Stuart K. Card, Thomas P. Moran, and Allen Newell. 1983.The Psychology of
Human-computer Interaction. L. Erlbaum Associates. https://books.google.nl/
books?id=lt9QAAAAMAAJ
[12] Rudolf Clausius. 1879.The mechanical theory of heat. Macmillan.
[13] Jacob Cohen. 1988.Statistical power analysis for the behavioral sciences(2 ed.).
Lawrence Erlbaum Associates, Hillsdale, NJ.
[14] Richard Courant. 1943. Variational methods for the solution of problems of
equilibrium and vibrations.Bull. Amer. Math. Soc.49, 1 (1943), 1 â€“ 23.
[15] E. R. F. W. Crossman. 1959. A Theory of the Acquisition of Speed-skill.Ergonomics
2, 2 (1959), 153â€“166. arXiv:https://doi.org/10.1080/00140135908930419 doi:10.
1080/00140135908930419
[16] Hugo Loeches de la Fuente. 2014.Ã‰tude multi-niveaux du contrÃ´le dâ€™un pÃ©-
riphÃ©rique dâ€™interaction de type joystick. ThÃ¨se de doctorat en Sciences du Mou-
vement Humain. Aix-Marseille UniversitÃ©, Marseille, France. https://theses.fr/
2014AIXM4060
[17] Dick de Waard. 1996.The Measurement of Driversâ€™ Mental Workload. Ph. D. Dis-
sertation. University of Groningen. Advisor(s) Rothengatter, J.A., Meijman, T.F.
and Brookhuis, Karel. https://research.rug.nl/en/publications/the-measurement-
of-drivers-mental-workload
[18] Richard P. Feynman. 1972.Statistical Mechanics: A Set of Lectures. W. A. Benjamin,
Reading, MA. Introduces the Feynman variational principle for free energy.
[19] Paul M. Fitts. 1954. The information capacity of the human motor system in
controlling the amplitude of movement.Journal of Experimental PSychology74
(1954), 381â€“391.
[20] Karl Friston. 2012. A Free Energy Principle for Biological Systems.Entropy14,
11 (2012), 2100â€“2121. doi:10.3390/e14112100
[21] Karl Friston, James Kilner, and Lee Harrison. 2006. A free energy principle for
the brain.Journal of physiology-Paris100, 1-3 (2006), 70â€“87.
[22] John A Groeger. 2000.Understanding Driving: Applying Cognitive Psychology to
a Complex Everyday Task(1st ed.). Routledge, London. https://doi.org/10.4324/
9780203769942
[23] Yves Guiard, Halla B Olafsdottir, and Simon T Perrault. 2011. Fittâ€™s law as an
explicit time/error trade-off. InProceedings of the SIGCHI Conference on Human
Factors in Computing Systems. 1619â€“1628.
[24] Sandra G. Hart. 1988. Development of NASA-TLX (Task Load Index): Results of
empirical and theoretical research.Human mental workload/Elsevier(1988).
[25] William E. Hick. 1952. On the Rate of Gain of Information.
Quarterly Journal of Experimental Psychology4, 1 (1952), 11â€“
26. arXiv:https://doi.org/10.1080/17470215208416600 doi:10.1080/
17470215208416600
[26] Ray Hyman. 1953. Stimulus information as a determinant of reaction time.Jour-
nal of experimental psychology45 3 (1953), 188â€“96. https://api.semanticscholar.
org/CorpusID:17559281
[27] Shinji Kajiwara. 2014. Evaluation of driverâ€™s mental workload by facial tempera-
ture and electrodermal activity under simulated driving conditions.International
Journal of Automotive Technology15 (2014), 65â€“70. https://doi.org/10.1007/
s12239-014-0007-9
[28] Donald E. Knuth. 1998.The Art of Computer Programming, Volume 3: Sorting and
Searching(2nd ed.). Addison-Wesley, Reading, MA.
[29] Tuomo Kujala, Jakke MÃ¤kelÃ¤, Ilkka Kotilainen, and Timo Tokkonen. 2016. The
attentional demand of automobile driving revisited: Occlusion distance as a
function of task-relevant event density in realistic driving scenarios.Human
Factors58, 1 (2016), 163â€“180. https://doi.org/10.1177/0018720815595901
[30] Solomon Kullback. 1997.Information Theory and Statistics. Dover Publications.
https://books.google.nl/books?id=05LwShwkhFYC
[31] Konrad P. KÃ¶rding and Daniel M. Wolpert. 2006. Bayesian decision theory
in sensorimotor control.Trends in Cognitive Sciences10, 7 (2006), 319â€“326.
doi:10.1016/j.tics.2006.05.003 Special issue: Probabilistic models of cognition.
[32] Timo Lajunen and Heikki Summala. 2003. Can we trust self-reports of driving?
Effects of impression management on driver behaviour questionnaire responses.
Transportation Research Part F: Traffic Psychology and Behaviour6, 2 (June 2003),
97â€“107. doi:10.1016/S1369-8478(03)00008-1
[33] John D. Lee, Brent Caven, Steven Haake, and Timothy L. Brown. 2001. Speech-
based interaction with in-vehicle computers: The effect of speech-based e-mail
on driversâ€™ attention to the roadway.Human Factors: The Journal of Human
Factors and Ergonomics Society43, 4 (2001), 631 â€“ 640. https://doi.org/10.1518/
001872001775870340
[34] Ben Lewis-Evans, Dick De Waard, and Karel A. Brookhuis. 2010. Thatâ€™s close
enoughâ€”A threshold effect of time headway on the experience of risk, task
difficulty, effort, and comfort.Accident Analysis & Prevention42, 6 (Nov. 2010),
1926â€“1933. doi:10.1016/j.aap.2010.05.014
[35] Guangquan Lu, Bo Cheng, Yunpeng Wang, and Qingfeng Lin. 2013. A car-
following model based on quantified homeostatic risk perception.Mathematical
Problems in Engineering2013 (2013), 1â€“13. https://doi.org/10.1155/2013/408756
[36] I. Scott MacKenzie. 1992. Fittsâ€™ law as a research and design tool in human-
computer interaction.Hum.-Comput. Interact.7, 1 (mar 1992), 91â€“139. doi:10.
1207/s15327051hci0701_3
[37] James L. McClelland and David E. Rumelhart. 1981. An interactive activation
model of context effects in letter perception: I. An account of basic findings.
Psychological review88, 5 (1981), 375.
[38] Pierre Moulin and Patrick R. Johnstone. 2014. Kullback-Leibler divergence
and the central limit theorem. InUCSD Information Theory and Applications
Workshop.
[39] Lindy J. Mulder. 1992. Measurement and analysis methods of heart rate and
respiration for use in applied environments.Biological Psychology34 (1992),
205â€“236. doi:10.1016/0301-0511(92)90016-N
[40] Jianwei Niu, Xiai Wang, Xingguo Liu, Dan Wang, Hua Qin, and Yunhong Zhang.
2019. Effects of mobile phone use on driving performance in a multiresource
workload scenario.Traffic Injury Prevention20, 1 (2019), 37â€“44. https://doi.org/
10.1080/15389588.2018.1527468
[41] Robert Novak. 2015. Lecture 7: Hypothesis Testing and KL Divergence.Lecture
Notes(2015).
[42] Monika Petelczyc, Jan Gieraltowski, Barbara Å»ogaÅ‚a Siudem, and Grzegorz Siu-
dem. 2020. Impact of observational error on heart rate variability analysis.
Heliyon6, 5 (2020), 4 pages. doi:10.1016/j.heliyon.2020.e03984
[43] Ilya Prigogine. 1977.Self-Organization in Nonequilibrium Systems: From Dissipa-
tive Structures to Order through Fluctuations. Wiley, New York.
[44] Rajesh P. N. Rao and Dana H. Ballard. 1999. Predictive coding in the visual cortex:
a functional interpretation of some extra-classical receptive-field effects.Nature
neuroscience2, 1 (1999), 79â€“87.
[45] James Reason. 1990.Human Error. Cambridge University Press.
[46] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning
representations by back-propagating errors.nature323, 6088 (1986), 533â€“536.
[47] Richard A. Schmidt, Howard Zelaznik, Brian Hawkins, James S. Frank, and John T.
Quinn Jr. 1979. Motor-output variability: a theory for the accuracy of rapid motor
acts.Psychological review86, 5 (1979), 415.
[48] John W. Senders, Alfred B. Kristofferson, William H. Levison, Charles W. Dietrich,
and J. L. Ward. 1967. The attentional demand of automobile driving.Highway
Research Record195, 2 (1967), 15â€“33. http://onlinepubs.trb.org/Onlinepubs/hrr/
1967/195/195-002.pdf
[49] Alistair M. Senior, Wolfgang Viechtbauer, and Shinichi Naka-
gawa. 2020. Revisiting and expanding the meta-analysis of varia-
tion: The log coefficient of variation ratio, lnCVR.bioRxiv(2020).
arXiv:https://www.biorxiv.org/content/early/2020/01/07/2020.01.06.896522.full.pdf
doi:10.1101/2020.01.06.896522
[50] Claude Elwood Shannon. 1948. A mathematical theory of communication.The
Bell System Technical Journal27, 3 (1948), 379â€“423. doi:10.1002/j.1538-7305.1948.
tb01338.x
[51] Sarah M. Simmons, Jeff K. Caird, and Piers Steel. 2017. A meta-analysis
of in-vehicle and nomadic voice-recognition system interaction and driving
performance.Accident Analysis and Prevention106 (2017), 31â€“43. https:
//doi.org/10.1016/j.aap.2017.05.013
[52] Sarah M. Simmons, Anne Hicks, and Jeff K. Caird. 2016. Safety-critical event risk
associated with cell phone tasks as measured in naturalistic driving studies: A
systematic review and meta-analysis.Accident Analysis and Prevention87 (2016),
161â€“169. doi:10.1016/j.aap.2015.11.015
[53] George S. Snoddy. 1926. Learning and stability: a psychophysiological analysis of
a case of motor learning with clinical applications.Journal of Applied Psychology
10, 1 (1926), 1.
[54] David L. Strayer and Frank Drews. 2004. Profiles in driver distraction: effects
of cell phone conversations on younger and older drivers.Human Factors: The
Journal of Human Factors and Ergonomics Society46 (2004), 640â€“649. doi:10.1518/
hfes.46.4.640.56806
[55] David L. Strayer, Jason M. Watson, and Frank A. Drews. 2011. Chapter Two â€“
Cognitive Distraction While Multitasking in the Automobile. InAdvances in
Research and Theory, Brian H. Ross (Ed.). Psychology of Learning and Motivation,
Vol. 54. Academic Press, USA, 29â€“58. doi:10.1016/B978-0-12-385527-5.00002-4
[56] Heikki Summala. 2000. Brake reaction times and driver behavior analysis.Trans-
portation Human Factors2, 3 (Sept. 2000), 217â€“226. doi:10.1207/STHF0203_2
[57] Alan T. Welford. 1968.Fundamentals of Skill. Methuen CO Ltd, London. https:
//books.google.ca/books?id=2tNOAAAAMAAJ
[58] Gerald J. S. Wilde. 1982. The theory of risk homeostasis: Implications for safety
and health.Risk Analysis2 (1982), 209â€“225. https://doi.org/10.1111/j.1539-
6924.1982.tb01384.x
Vertegaal et al.
[59] J. H. Williamson, A. Oulasvirta, P. O. Kristensson, and N. Banovic. 2022. An
Introduction to Bayesian Methods for Interaction Design. InBayesian Methods
for Interaction and Design. Cambridge University Press, 3â€“80.
[60] Jacob O. Wobbrock, Edward Cutrell, Susumu Harada, and I. Scott MacKenzie.
2008. An error model for pointing based on Fittsâ€™ law. InProceedings of the
SIGCHI conference on human factors in computing systems. 1613â€“1622.
[61] Benjamin Wolfe, Ben D. Sawyer, and Ruth Rosenholtz. 2022. Toward a theory
of visual information acquisition in driving.Human Factors: The Journal of the
Human Factors and Ergonomics Society64, 4 (June 2022), 694â€“713. doi:10.1177/
0018720820939693
[62] Daniel M. Wolpert. 2007. Probabilistic models in human sensorimotor control.
Hum. Mov. Sci.26, 4 (Aug. 2007), 511â€“524.
[63] Bing Wu, Yan Yan, Daiheng Ni, and Linbo Li. 2021. A longitudinal car-following
risk assessment model based on risk field theory for autonomous vehicles.In-
ternational Journal of Transportation Science and Technology10, 1 (March 2021),
60â€“68. doi:10.1016/j.ijtst.2020.05.005
[64] Tingting Wu, Alexander J. Dufford, Laura J. Egan, Melissa-Ann Mackie, Cong
Chen, Changhe Yuan, Chao Chen, Xiaobo Li, Xun Liu, Patrick R. Hof, and Jin
Fan. 2018. Hickâ€“Hyman law is mediated by the cognitive control network in the
brain.Cerebral Cortex28, 7 (2018), 2267â€“2282.
[65] Wei Wu, Yun Gao, Elie Bienenstock, John P. Donoghue, and Michael J. Black.
2006. Bayesian population decoding of motor cortical activity using a Kalman
filter.Neural computation18, 1 (2006), 80â€“118.
[66] Lisa Wundersitz. 2019. Driver distraction and inattention in fatal and injury
crashes: Findings from in-depth road crash data.Traffic Injury Prevention20, 7
(2019), 696â€“701.
[67] Meixin Zhu, Xuesong Wang, and Xiaomeng Wang. 2016. Car-following
headways in different driving situations: A naturalistic driving study. InCI-
CTP 2016. American Society of Civil Engineers, Shanghai, China, 1419â€“1428.
doi:10.1061/9780784479896.128