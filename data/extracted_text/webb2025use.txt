The Use of Gaze-Derived Confidence of Inferred
Operator Intent in Adjusting Safety-Conscious
Haptic Assistance
Jeremy D. Webb, Michael Bowman, Songpo Li, and Xiaoli Zhang ∗, Member, IEEE
Abstract—Humans directly completing tasks in dangerous or
hazardous conditions is not always possible where these tasks
are increasingly be performed remotely by teleoperated robots.
However, teleoperation is difficult since the operator feels a
disconnect with the robot caused by missing feedback from
several senses, including touch, and the lack of depth in the video
feedback presented to the operator. To overcome this problem, the
proposed system actively infers the operator’s intent and provides
assistance based on the predicted intent. Furthermore, a novel
method of calculating confidence in the inferred intent modifies
the human-in-the-loop control. The operator’s gaze is employed
to intuitively indicate the target before the manipulation with
the robot begins. A potential field method is used to provide a
guiding force towards the intended target, and a safety boundary
reduces risk of damage. Modifying these assistances based on the
confidence level in the operator’s intent makes the control more
natural, and gives the robot an intuitive understanding of its
human master. Initial validation results show the ability of the
system to improve accuracy, execution time, and reduce operator
error.
I. I NTRODUCTION
A. Context and Motivation
Remotely operated robotic procedures performed has con-
tinued to increase each year. These procedures include scenar-
ios where a human may find it difficult to achieve a task, such
as telesurgery, and environments where it is dangerous for a
human to be present such as in environments contaminated by
chemical, radioactive, or explosive hazards. The environmental
challenges where these robotic systems are used include bomb
disposal/mine clearing robots [1], robots for making repairs
in space, robots for handling nuclear material [2], hazardous
waste handling robots [3].
Use of these robots greatly improves the safety and comfort
of the humans performing the tasks, yet also adds complexity
and difficulty in achieving the goal. The reason for the
difficulty is mainly due to the “disembodiment” problem [4].
Where this problem describes the fact that the operator is not
physically performing the tasks in the environment yet must
Jeremy D. Webb is an engineer with Apple Inc.(e-mail: jew-
ebb@mines.edu.)
Michael Bowman is a Postdoctoral Fellow in Perelman School of
Medicine at University of Pennsylvania, Philadelphia, PA 19096 USA (e-mail:
michael.bowman@pennmedicine.upenn.edu).
Songpo Li is an Associate Scientist with Honda Research Institute US (e-
mail: songpo li@honda-ri.com).
Xiaoli Zhang is an Associate Professor in the Department of Mechan-
ical Engineering at Colorado School of Mines, Golden, CO 80401 USA
(∗corresponding author, phone: 303-384-2343; fax: 303-273-3602; email:
xlzhang@mines.edu).
mentally accomplish the task. The correspondence issue is
inherent in the system as the operator lacks sufficient sensory
feedback. Specifically, in a typical setup, an operator views a
screen and controls a robot with a joystick; however, feedback
from touch and sound as well as depth into the screen are
all missing. Not being able to distinguish the Z-order of
objects can create erroneous complications. For example, in
surgery, unexpected tissue damage, longer operating times,
and increased stress for the surgeon can all result from a lack
of depth information. Likewise, for a bomb disposal robot,
errors caused by the operator’s unclear understanding in depth
could result in a bomb denotating prematurely. It also has
been shown that depth perception is paramount in successfully
performing grasping tasks for human’s using their own hands,
where performance degrades as the depth perception becomes
inaccurate [5]. Furthermore, trying to determine the depth of
an object can distract a teleoperator. Since the operator is not
using their own arm to complete the task, they will not have
a good intuitive understanding of the dynamic behavior of the
robot being controlled. These issues can cause mistakes that
lead to unintentionally harming the surrounding environment
which has the potential to be far more costly than failing to
complete the teleoperation task.
Existing interfaces for teleoperated robots attempt to solve
these issues in various ways, but many are difficult to operate.
One interface uses fixed targets as reference points and an
oscillating camera towards and away from these points to give
an operator a better sense of the environment [6].The largest
problem with this approach is the difficulty for the operator
to accomplish their task with a constantly moving camera.
Another suggested method to help the operator understand
depth in a teleoperation scenario is to reconstruct a virtual
3D environment by using stereoscopic video [7], [8]. Other
solutions to aid the operator determine depth is to change
the lighting conditions of the environment and provide visual
cues [9],] and provide a target object’s pose using machine
vision [10]. It should be noted this last method will fail
whenever new objects are encountered. Another approach
is to immerse the operator in the environment by using a
head-mounted display, which allows the user to look around
naturally [11], but this requires more complex equipment.
There is therefore a need for an intuitive control interface
that can restore some of the sensory feedback lost during
teleoperation and increase the accuracy of task completion.
Such a system should take into account the operator’s intent
to provide accurate assistance for real-time control applica-
arXiv:2504.03098v1  [cs.RO]  4 Apr 2025
tions, and cooperate in a way that is comfortable for the
operator [12]. In this case, the operator’s gaze can serve to
indicate their intent, or the final goal of a manipulation, and
this information can be used to guide the operator’s hand
to the target. This can be accomplished through the use
of haptic forces, which function as a partial restoration of
sensory feedback. Furthermore, the provided assistance should
be adjusted based on the system’s confidence in the operator’s
intent [13]. Since the system cannot be 100% sure of the
predicted operator’s intent, the confidence level in the intent
should be used to moderate the strength of the provided haptic
forces. This will ensure the system is robust and provides
accurate assistance.
The proposed system takes advantage of the natural visuo-
motor behavior of human beings. Several human visuomotor
and cognitive behavior studies indicate that one’s gaze leads
their hands during execution of a grasping or reaching task.
Specifically, when a human decides to pick up an object,
he first looks at the object, then focuses on the part of
the object to be grasped, and finally executes the reaching
movement. Typically, the gaze fixates on an object before
interacting with it and stays fixed on the object until the task
is completed [14], [15]. The average lead time for a grasping
task has been found to be 3 seconds [16]. A human’s eye
gaze has been shown to focus on certain parts of an object
depending on the current task [17], however, initially the
gaze is focused on the object’s center [18]. Furthermore, in
a comprehensive review [19] describes how the human brain
maintains a model of the “eye-head-shoulder system” and
treats gaze as a feedforward mechanism when reaching for
an object. Even on 2D displays, eye-movements indicate the
user’s intention and thus, can be used for “highly-intuitive”
computers [20]. ]. These anticipatory fixations also happen in
teleoperation [21]. Therefore, incorporating the operator’s gaze
into the teleoperation control interface to specify the center of
the haptic assistance is a natural extension of normal human
behavior. There is a need for such a system, as described by
[22] which speaks of the need for human perception models
in haptic teleoperation to improve human-in-the-loop control.
B. Previous Work
Incorporating haptic feedback into a teleoperation system
can restore some of the sensory feedback that is lacking.
Haptic feedback, in this case, refers to applying forces to
the operator that are dependent on the system’s state. Us-
ing haptic feedback in teleoperation has been explored in a
variety of studies, especially in remote surgery applications.
Researchers have shown that using a “computerized force
feedback endoscopic surgical grasper” in minimally invasive
surgery leads to significant performance gains over using a
regular endoscopic grasper [23]. Similarly, using force feed-
back in blunt dissection reduces tissue damage and the force
used in robotic surgery [24]. Haptic feedback has been shown
to improve teleoperation control in general, as well. One study
investigated using haptic feedback for training one’s hands to
follow a certain trajectory, demonstrating that haptic feedback
does improve the training [25]. This indicates that haptics can
be used to teach a more straightforward path to the goal. Other
studies have shown the use of potential fields in haptics to
guide the operator by pushing their hand away from objects
and/or towards the goal (termed guidance virtual fixtures)
[26]–[28]. One such project developed a potential field to
control unmanned aerial vehicles [29].
Often, providing force feedback in terms of a virtual fixture
only solves one issue with teleoperation, accurately reaching
the target. For many teleoperation tasks, such as minimally
invasive surgery (MIS), some damage to the surrounding
environment is unavoidable [30]. However, this damage can be
minimized by ensuring the operator only moves the robot in
allowable regions. This is enforceable using haptic forbidden-
region virtual fixtures. A number of forbidden-region virtual
fixtures have been demonstrated in various research, especially
those concerned with MIS. One shows that forbidden-region
virtual fixtures that move with a portion of the environment,
such as a beating heart, can increase user precision [31]. A
method which assists the user performing MIS by placing con-
ical forbidden-region virtual fixtures at a set of predetermined
locations has also been developed [28]. Others have built
methods to automatically generate forbidden-region virtual
fixtures based on the output of RGB-D cameras to protect
sensitive areas [32], [33].
The drawback to the approaches described above is that they
rely mainly on situational context alone to determine how to
implement the virtual fixtures. Instead, as noted earlier, the
system should incorporate the operator’s intent and confidence
in the prediction of that intent into the control loop to provide
intuitive and accurate assistance. One way to do this is to use
the operator’s gaze.
Gaze as a control input has been used in a variety of assis-
tive mechanisms to help the teleoperator visualize the robot
workspace [12], [34]–[36], and to direct robot navigation. In
particular, researchers have demonstrated successful use of
gaze gestures to control teleoperated drones [37], and gaze
contingent regions, or “hot-spots”, have been used to specify
a robot’s direction of movement [38], [39]. These approaches
do not use the operator’s gaze as an indication of intent, instead
the user must consciously focus on a particular area to provide
input for the signal. This can cause fatigue for the operator
and distract them from completing the goal.
Using the operator’s gaze to infer their intent can provide
a more natural control scheme. This is demonstrated in “pre-
dicting a driver’s intent to change lanes”, which used head
motion and eye data to train a discriminative classifier to
perform the prediction [40]. As explained, use of the user’s
intent has been shown in other applications, but using intent
prediction in haptics is a new area that has not been explored.
Additionally, the shortcoming in the previous approaches are
that they provide only a binary output. For example, the user
intends to change lanes, or does not intend to. In this case, an
important aspect of the system’s ability to make decisions has
been left out: the probability that the predicted intent is correct.
This component is necessary to ensure robust control and
decision making. For example, undesired behavior could be
encountered if the system attempts to assist with lane changing
when the intent prediction is hovering between intent and no
intent. Instead the system should provide assistance based on
its confidence in the predicted intent. Another reason for this
is because the user’s gaze is really an observation mechanism,
not a control input (known as the Midas Touch problem [41]).
Therefore, to reduce inference error the system should take
into account the likelihood that the user actually has an intent.
C. Research Contributions
The proposed system aims to reduce risk and enhance per-
formance in realtime teleoperation through three approaches:
gently guide the operator’s hand toward the goal point; prevent
unwanted destruction of the surrounding teleoperation environ-
ment; and ensure control is natural and intuitive by modifying
the previous two approaches based on the system’s confidence
in the operator’s inferred intent. The intent inference restores a
teleoperator’s eye-hand coordination through incorporation of
the operator’s natural visuomotor behavior by using their gaze
to determine the reaching target before the process begins.
A force then gently pushes the operator’s hand towards the
target. Simultaneously, a safety boundary prevents harm to the
environment by restricting joystick movement to a small area
around the target point. The size and strength of both of these
virtual fixtures is adjusted based on the system’s confidence
in the inferred intent and the specific task. As discussed
in [42], including awareness of the environment and task in a
teleoperation control scheme can give great improvements in
performance. Additionally, the biggest challenge when using
virtual fixtures is determining the appropriate strength of
the fixture [42]. As noted, this system deals with this issue
by dynamically assigning the strength based on the current
situation and the probability that the predicted operator’s intent
is correct.
The contributions of this system include:
1) development of haptic virtual fixtures which are based
on the operator’s inferred intent to ensure control is
instinctive and improves performance
2) a novel gaze-driven method for determining the level of
confidence in the predicted human intent
3) real time adjustment of the haptic virtual fixtures based
on the operator’s predicted intent and confidence in that
prediction, which reduces risk and increases success rate
in teleoperated tasks
4) evaluation of the effectiveness of the intent-driven haptic
assistance with confidence adjustment
The operator’s gaze is used to indicate the final goal of the
joystick motion because, for executing grasping tasks, the hand
of the operator follows their gaze. Additionally, the confidence
in the intent is computed using features inherent in the
operator’s gaze. Using this approach, the system can predict
the operator’s intention and determine its own confidence in
the prediction without the need for extra effort by the operator.
Adjusting the strength of guidance and safety boundary based
on the system’s confidence in the inferred intent allows the
user to teleoperate the robot as normal. The shape of the
safety boundary is chosen to minimize the risk associated
with completing a given task. The guidance force is computed
using a potential field method. Using a potential field allows
Fig. 1: System overview
the spatial uncertainty in the predicted target location to be
taken into account. Errors in gaze tracking, target location
determination, and robot end-effector location all contribute
to the spatial uncertainty.
The proposed system will increase precision, safety, and
ease the use of teleoperation, thus improve task performance
by reducing the time it takes to complete a task and increase
the comfort of remote operators.
II. S YSTEM OVERVIEW
The overall system predicts the goal position from the
operator’s gaze, determines the confidence in the prediction,
guides the operator’s hand to the target using a force based on
the output of the potential hybrid control, and places a safety
boundary (forbidden-region virtual fixture) around the goal
point. In this system, shown in in Fig. 1, the video feedback
shows the operator’s gaze location, which is the goal position,
while a 6 DOF joystick gives the operator the manual control
input for orientation and position. The guidance force and the
safety boundary is provided by the joystick, which is also a
haptic device. Most of the time, the operator should not feel
the boundary at all. Their hand will only come into contact
with it if they attempt to command the robot to a position too
far from the target.
The overall control flow with a more detailed view is shown
in Fig. 2. The operator’s fixation location is determined by
acquiring data from the eye tracker and filtering it as further
discussed in section III. The confidence in the operator’s
intent is also calculated using the gaze data. By combining
the gaze information from the robot environment, a fully
specified spatial position of the target is determined. The safety
boundary is placed with its center at the target position and its
parameters are adjusted based on the confidence level of the
intent. Simultaneously, the target position is blended with the
position of the joystick using the novel potential field method
to determine the force to apply to the operator’s hand. The pose
of the joystick is then fed into the controller for the robot.
As shown in Fig. 2, this system is considered a closed
loop through the user viewing video feedback from the robot
and adjusting the joystick position or orientation accordingly.
Additionally, the guidance force pushes the operator’s hand
Intent Confidence 
Calculation
Filtering
 Intent 
Location
Safety 
Boundary
Boundary 
Adjustment
Operator
 Robot
Guidance 
Force
Force 
Adjustment
Gaze
Data
Robot Position
Haptic Forces
Intent Confidence
Commanded
Position
Visual Feedback
Environment
Data
Fig. 2: Overall control flow which illustrates the role of the
operator’s gaze in the hybrid joystick-gaze control method.
The blue lines represent the control flow for traditional tele-
operation.
towards the target. The operator’s gaze assists with this be-
cause it incorporates the operator’s intention into the control
by indicating the target position. In summary, the operator’s
gaze location indicates their intended target and the system
assists them in reaching this target by actively guiding their
hand towards this position.
III. M OTION INTENT EXTRACTION FROM EYE MOVEMENT
Humans eyes naturally make involuntary movements and
motions such as blinking, rolling, and microsaccades. There-
fore, it is necessary for a method to filter the raw gaze data
to determine the operator’s fixation location. The filter which
was chosen to combat this was an adaptive-length sliding
window [12].
When gaze is used to control a system, it becomes necessary
to determine a way to distinguish an intentional command
from an unintentional one. Because the gaze is always “ac-
tive”, this complicates the problem which is referred to as
the Midas touch problem [41]. One approach to overcoming
this distinction is to use the dwell time method. This method
considers a command to be confirmed when the gaze stays
on a location for a set amount of time. Alternatively, an
option could be to require a certain number of blinks to
confirm a command. In this system, a method of determining
the likelihood that a predicted intention is correct has been
developed. The following section describes the approach.
The confidence in the predicted intent derived from the
operator’s gaze is determined by using a naive Bayes classifier
fitted to three processed gaze features. These features are:
the maximum euclidean distance of the gaze points to the
gaze center, the average distance to the gaze center, and the
number of gaze points that are closer to the center than the
average distance to the center. In this case, the gaze center
refers to the average point taken for all gaze points over the
data segment considered. These features are shown in Fig. 3.
Before computing each gaze feature, the data was smoothed
by running it through a five point moving average filter.
Several different features for the classifier were investigated.
Although the ones selected are not independent, they provide
a good estimate of the reliability of the predicted intent based
on how focused the gaze is.
Fig. 3: Example of the gaze features used to determine the
intent confidence. X and Y are the screen coordinates in pixels.
Training data for the classifier was gathered by recording
all the eye data from the eye tracker while different volunteers
looked at a screen filled with colored numbers. Each volunteer
indicated their intent by clicking the space bar while looking at
a number of their choosing. This caused the gazed-at-number
to move to the center of the screen and labeled two seconds
worth of the preceding valid data points with the class “intent”.
At anytime other time, gathered data was labeled with “no
intent”. Valid data includes all data where both eyes are fully
tracked by the gaze tracker.
After the model has been trained, during actual intent
prediction, each segment of data is taken from the last two
seconds of valid collected gaze data and the gaze features are
calculated. The classifier is then run on the input data.
The prediction output from the classifier includes the poste-
rior probabilities of belonging to the classes “intent” and “no
intent”. These correspond to the operator beginning an action,
or just observing the situation. Since there are only two classes,
a posterior probability of over 50% for “intent” indicates that it
is most likely that the operator has a valid intention. However,
a value of just over 50% indicates that the intent is just barely
likely. Therefore, the posterior probability for intent will be
linearly rescaled to a range from 0.5 to 1.0:
ci =
(
0 pi < 0.5
1
0.5 (pi − 0.5) pi ≥ 0.5 (1)
pi = p (i|G1, G2, G3)
where ci is the confidence in the predicted intent and
p (i|G1, G2, G3) is the probability of an intent given the three
gaze features described. Additionally, if the operator’s eyes are
not tracked for over 0.75 seconds, the intent confidence is set
to zero.
IV. I NTENT -BASED HAPTIC ASSISTANCE
Two different haptic virtual fixtures are employed to provide
assistance to the operator. Both are centered at the gaze-
derived intent location. A guidance force pushes the operator’s
hand towards the target position with its strength based on
distance from the target. Scaling the force this way ensures
the system respects the operator’s control. Even though the
predicted intent location may be correct, the operator may need
to navigate around obstacles so the guidance force should be
relatively weak until the operator begins to move towards the
target. Similarly, the safety boundary prevents destruction of
the environment by preventing movement outside of a region
close to the target.
A. Haptic Guidance Force
The purpose of the guidance force is to gently push the op-
erator’s hand towards the gaze-indicated target position. This
will help overcome the lack of sensory feedback, especially
in the depth direction. To this end, the profile of the guidance
force is based on a method described in our previous work,
termed potential hybrid control [43].
The potential hybrid controller takes two position inputs,
a target (the gaze target, ⃗ pg ∈ R3) and a manually-defined
control position( joystick control location, ⃗ pj ∈ R3). It then
combines them in a way that takes into account the target
uncertainty and the behavior of the operator. In particular, if
the manually-defined control position is far, or very close, to
the target position, then the potential hybrid controller does
not affect the output very much. In the case of the manually-
defined control position being far from the target, the operator
should have complete control over the robot position. On the
other hand, when the manually-defined control position is
close to the target, the operator does not need much assistance
because the target position has already been reached. It is only
in the travel between these two extremes that assistance is
needed. The potential hybrid controller follows this approach.
Fig. 4: An illustration of the potential hybrid control method
for a two-dimensional control space. pg is the goal point, pj
is the joystick point, c is the combined point, and σ is the size
of the field in each direction.
1) Potential Hybrid Controller Method:The target location,
derived from the operator’s gaze, is combined with motion
commands from the joystick through a potentially weighted
influence method shown in Fig. 4. This approach uses the
distance from ⃗ vj to ⃗ pg along a potential field to determine the
influence of a resulting point, ⃗ c. The method is represented by
the following equations:
⃗ c= (⃗ pj)( ⃗1.0 − W ⃗d) + (⃗ pg)W ⃗d (2)
Where ⃗ c∈ R3 is the final combined location. W is the weight
calculated from the potential field, and ⃗d is a 3x1 vector of
coordinate weights,
⃗d =


dx
dy
dz


From Eq. 2, W is the amount of influence that the target
point ⃗ pg has on the resultant ⃗ c. It is calculated from the
potential field and bounded by [0,1] where a higher value of
W approaching 1 means the resultant ⃗ cwill correspond to the
target location, while a lower value of W will mean it follows
the hand position. ⃗d is a weighted vector that controls how the
potential field affects the final combination in each direction.
For example, a value of dx = 0.9 would give the potential field
a 90% influence on the final combination for the x coordinate
only.
2) Potential Field: A potential field describes how a body
interacts with an entity that exerts an influence on that body.
For example, in physics, there is a potential field description
for the gravitational pull exerted by a planet. This potential
field gives a representation of what forces another body would
feel (due to the planet) when placed at any given location in the
field. Analogously, the potential field in this method describes
the effect the ⃗ pg has on the final position of the robot, ⃗ c.
The peak of the potential field is the gaze-indicated target
position, this take advantage of the operator’s visuomotor
behavior. By placing the potential field centered there, the
robot’s end-effector is drawn towards the intended target. The
potential field provides a smooth combination or transition
from the joystick position and the target position, which has
its maximum effect at the intended location, however, this does
not impact the robot position very much when the joystick is
far from the target. This attribute of the potential field ensures
the control follows the operator’s intent.As shown in Eq. 3, a
Gaussian curve was used for the potential field. While other
potential fields could be used including parabolic, cubic, etc.
[28], a Gaussian curve was selected because it is a smooth,
continuous function and the shape is easy to manipulate by
adjusting its parameters. The shape determines how quickly
the influence of the field increases in each direction.
W = exp

−1
2 (⃗ pj − ⃗ pg)T Σ−1 (⃗ pj − ⃗ pg)

(3)
Σ−1 =


σ2
x 0 0
0 σ2
y 0
0 0 σ2
z

 (4)
In the above equation ⃗ pg represents the center of the field
and Σ represents the covariance matrix. The covariance matrix
controls the tightness of the field. The smaller the variance in
each directional component the tighter the field becomes. In
this case, since the off-diagonal elements are zero so only
the variance are left in the covariance matrix. When this
occurs, each coordinate direction in the field is independently
controlled from one another.
There are a few reasons for using the method to combine
the control inputs. One is the simplicity and ease of combining
multiple inputs. Additionally, the method is intuitive from a
physical sense. The potential field represents the probability
the target location determined from the gaze is correct. As
the operator joystick command motion approaches the target
position, the system becomes more confident of its own
guess at the target location and continues to increase its own
influence over the robot end-effector. If, on the other hand, the
robot end-effector is far away from the gaze-selected target
then the system has a lower confidence in the target location
so it affects the final end-effector location less. This also
models the way humans naturally behave. If one is looking at a
particular location in space, they do not want their hand moved
there automatically, but if they are focusing on something
intently (for example when threading a needle) their hand
moves to wherever they are looking.
3) Potential Hybrid Control Approach Guidance Force:
After determining the location of ⃗ cin the previous section,
the guidance force for the haptic feedback must be determined.
By basing the strength of the guidance force on the potential
hybrid control, all the advantages of the method noted above,
are gained. The strength of this force is proportional to the
degree of influence given by the potential hybrid control for
a given direction. It is calculated using:
⃗gf = ⃗ c− ⃗ pj
gfmax
(5)
where ⃗gf ∈ R3 is the normalized, dimensionless strength of
the guidance force in Cartesian space, ⃗ cis found using (2), ⃗ pj
is the normalized joystick position, and gfmax is the maximum
value of ⃗ c− ⃗ pj. gfmax is pre-calculated using numerical
methods. gf is then scaled to an appropriate level for the haptic
device to give the actual guidance force. The scaling ensures
the joystick will not pull itself out of a user’s hand. A size of
σ = 0.4 was used for all coordinate directions, as suggested
in our previous work. In the simplest case, the direction of
the force points directly towards the target. However, just like
the amount of influence for each direction could be controlled
with the potential hybrid control approach described in section
IV-A1, the strength of the force in each direction can be
controlled independently. In most cases, the strength in the
depth direction should be much larger than the other directions
since that is the direction lacking feedback for the operator.
The effect of the guidance force is to provide a gentle push
in the direction of the target when the joystick is moving
towards the target. Similar to the potential hybrid control
influence, if the joystick is far from the target, or close to
the target, then the force is small, but otherwise the force is
larger. An illustration of the magnitude of the force (strength
in each direction combined together) is presented in Fig. 5 in
two dimensions for the case where the strengths, dX and dY ,
in each direction are equal. The force profile is the same from
every direction of approach. This is also illustrated in Fig. 6,
which shows the force magnitude for a single direction, or can
be thought of as a section view of Fig. 5.
Figure 7 shows the normalized strength when the force is
only applied in the depth direction, Y . Moving along the depth
direction at the point where X is equal to the target position,
X = 0.5, gives the same profile as the magnitude plot in Fig.
5. This is helpful to the operator, because the depth direction
is the only direction in this situation that requires assistance.
Fig. 5: Magnitude of the guidance force in two dimensions
for a combination with equal strengths in both X and Y . The
target position is placed at (0.5, 0.5). The profile of the force
is the same for all directions of approach.
Fig. 6: Magnitude of the guidance force in one dimension.
The target position is placed at Y = 0.5.
At other points, at values of X increasingly farther from the
target, the peak strength of the guidance force decreases. This
allows the operator to control the robot with less opposition
if the target position is incorrect, or the approach path needs
to deviate from its current trajectory, for example due to an
obstacle. Along the line Y = 0 .5 the force is completely
zero. This is because the target position in the depth direction
has already been reached, so minimal additional assistance is
necessary.
B. Safety Boundary
An alternative to the guidance force, a haptic safety
boundary is considered. The purpose of the safety boundary
(forbidden-region virtual fixture) is to minimize collateral
damage by restricting movement of the joystick to a small area
surrounding ⃗ pg Various different shapes for this boundary are
examined in the following sections. The shape of the boundary
is very important because the boundary has the potential to
make teleoperation safer, but it could also prevent the operator
from controlling the joystick effectively if the shape is not
chosen carefully, which would lead to more mistakes and
environmental damage.
Fig. 7: Strength of the guidance force in two dimensions
for a combination where only the depth is applied. Y is the
depth direction in this case and the only direction with force
assistance. The target position is placed at (0.5, 0.5).
1) Boundary Design: The shape of the safety boundary,
shown as a section-view in Fig. 8, was chosen to provide
minimum intrusion to the operator’s standard operating man-
ner while still restricting access to areas unnecessary for
completing the task. The full shape can be created by rotating
the profile in Fig. 8 by pi radians about its center axis.
The upper plane prevents unintended damage during general
motion, while the cone allows the robot room to move in and
complete the task at ⃗ pg.
S
H
θ
Fig. 8: Safety boundary shape with parameters.
There are three parameters that govern the shape of the
boundary: S, H, and θ. S is the radius of the flat bottom, H
is the height limit of the cone, and θ is the angle of the cone.
In general, a small S, and a large H and θ will create the most
restrictive boundary. For the purposes of this system, S was
limited from 1 centimeter to 7 centimeters, H was limited from
0 to 15 centimeters, and θ from 5◦ to 85◦. These limits were
chosen based on the physical limitations of the haptic device
and on empirical experience of the necessary room needed to
maneuver to complete a task.
2) Parameter Selection: Selecting the correct set of pa-
rameters for the safety boundary is important because of the
competing aims of adjustment. Tightening the safety boundary,
which corresponds to increasing H, decreasing S, and increas-
ing θ can reduce the risk of collateral damage by preventing
access to areas of the workspace farther from ⃗ pg. However,
while this may reduce the risk of damage, it increases the risk
of failure. With such a tight boundary, there may not be enough
room to maneuver. At the extreme, if θ = 90◦, S = 0, and
H > 0 then the workspace is not accessible at all. Opening
up the boundary, decreasing H, increasing S, and decreasing
θ can lead to the opposite problem: high risk of collateral
damage, but a high likelihood of success.
The set of parameters for each task should be chosen to
minimize the overall risk associated with the task, including
the risk of damage and the risk of failure. There is no clear
set of parameters to achieve this, and the selection is task
dependent. Therefore, it is necessary to investigate how the
variation of parameters affects the risks. This was done by
measuring the failure rate of each task while using different
boundary parameters. Failure rate considered both failures due
to not completing the task, and failures due to damaging
the surroundings. The set of parameters with the minimum
failure rate was selected as the initial set for each task. Two
different tasks were measured, grasping and cutting. These are
explained in more detail in section VI-B.
Eight different combinations of the parameters were tested.
The combinations tested were selected by dividing the range
of each parameter into thirds and using the cutoffs between
each segment. This was done to test each parameter at a high
and low point, and also to leave room for adjustment later.
The specific combinations of parameters measured for each
task are shown in Table I. A mouse click was used to set the
center of the boundary on the target position for each task.
TABLE I: S AFETY BOUNDARY PARAMETER SETS
Set θ [deg] H [cm] S [cm]
1 30 5 3
2 30 5 5
3 30 10 3
4 30 10 5
5 60 5 3
6 60 5 5
7 60 10 3
8 60 10 5
Eight volunteers were tested, with each one performing each
task under every set described in Table I for two to three
trials. All the volunteers were aged 18-28 and were able-
bodied. Three of the volunteers had prior experience using
the system, but all were given as much time as they needed
to familiarize themselves with how it worked. This was done
by practicing picking up a tennis ball with no boundary until
they felt comfortable with the system. The order of the sets
was randomized for every participant to prevent acclimation
to the system, which would affect the results. The failure rate
was then averaged for each trial and over all the participants
to determine the overall failure rate for every parameter set.
The results of the tests are shown in Table II. As expected,
the average failure rate for the cutting task, 59%, was higher
than for the grasping task, 54%. This is simply because the
cutting task is more difficult and requires higher precision
while manipulating the robot arm. For the cutting task, there
are multiple sets which have equal failure rates; sets 2, 6, and
7 have the lowest failure rate of 50%. These sets correspond
to the least restrictive boundary (set 2) and the most restrictive
boundary (set 7), as well as a less restrictive boundary (set 6).
This could be explained by the difficulty of the task. Since the
task was challenging, it was easiest to accomplish with low
restrictions, which would give the most room to maneuver, or
with high restrictions which would provide the most protection
from collateral damage. Further testing would likely help to
differentiate between these sets. Any one of these sets can be
chosen to minimize the risk. Since the cutting task is difficult,
set 2 was chosen to proceed with because it will give the
operator more control. For the grasping task, set 5 had the
minimum failure rate.
TABLE II: SAFETY BOUNDARY PARAMETER TEST RESULTS
SHOWING THE FAILURE RATE OF EACH TASK . T HE HIGH -
LIGHTED CELL (S) IN EACH COLUMN INDICATE THE PA-
RAMETER SET(S) WITH THE MINIMUM FAILURE RATE
Set Cutting Task Grasping Task
1 64% 65%
2 50% 42%
3 64% 58%
4 64% 73%
5 71% 38%
6 50% 65%
7 50% 48%
8 57% 44%
V. A DJUSTMENT BASED ON INTENT CONFIDENCE
Both the guidance force and the safety boundary are ad-
justed from their initial settings based on the level of confi-
dence in the operator’s intent. Specifically, the safety boundary
will become less restrictive and the guidance force weaker
when the intent confidence is low while the opposite will
occur when the intent confidence is high. The reason for this
is that the operator should not be restricted when the system is
not confident in its prediction of the intent. This would cause
frustration, errors, and possible damage to the surrounding
environment as the operator has to fight the system to get
to where she actually wants to go. On the other hand, if the
system is highly confident in the predicted intent, then the
strength of the haptic assistance should be increased to guide
the operator to the target position and minimize the risk of
damaging the environment.
Therefore, the safety boundary will open up when the
confidence level is low, and tighten when the confidence level
is high. Specifically, S and θ will be increased while H is
decreased when the confidence is low. This relationship is
shown in (6).
sci =
(
ci−ithresh
1−ithresh ci ≥ ithresh
ci−ithresh
ithresh ci < ithresh (6a)
paramAdjust = sci ∗ paramMaxAdjustAmount (6b)
where sci is the scaled confidence in the intent, ci is the con-
fidence in the predicted intent calculated using (1), ithresh is
the confidence threshold level, paramAdjust is the amount to
adjust one of the parameters, and paramMaxAdjustAmount
is the maximum amount that the parameter can be adjusted.
The confidence threshold level is the confidence level at which
the intent prediction is high enough to begin to make the safety
boundary more restrictive. It is suggested that this be set to a
value over 50% because that is the point at which the system
is more confident in the intent than not. For this method,
ithresh was set to 60%. Once the scaled confidence, sci, is
computed, each parameter is adjusted from its initial value
based on the paramMaxAdjustAmount. The value of this
parameter for S, H, and θ was chosen as a third of the range
for each parameter, as described in section IV-B. In particular,
SMaxAdjustAmount = −2 cm, HMaxAdjustAmount =
5 cm, and θMaxAdjustAmount = 25◦. This will allow each
parameter to be scaled to the maximum or minimum of its
range depending on the intent confidence. For example, S
will scale down to its lowest value of 3 centimeters when
the intent confidence is 100%. This process enables the safety
boundary to dynamically adjust to the confidence level in order
to decrease the risk of damaging the environment.
Additionally, the guidance force strength is also adjusted
based on the confidence level. In this case, it is applied as a
simple linear scaling, meaning that if the scaled confidence
level is 0 then no guidance force is applied to the operator,
and if the scaled confidence level is 1, then the full strength
of the guidance force is exerted on the operator.
VI. E XPERIMENTAL VALIDATION
In order to evaluate the effectiveness of the proposed
approach, each component was tested with the setup described
in the following section. Both the guidance force and the safety
boundary were tested separately to gain an understanding of
how each affected the teleoperation performance.
A. Experiment Setup
For validation, a system was built following Fig. 1. The
joystick used is a Geomagic Touch created by 3D Systems.
This is a haptic device with 6 degrees of freedom that was
configured to output the pose in space represented by the
stylus. Additionally, a small amount of constant friction was
applied to stabilize motion of the stylus and make it easier
for the operator to produce precise adjustments. The eye-
tracking portion of the project was based on the Tobii Rex
eye tracker. This eye tracker is a video-based remote system
which can track the user’s eyes from 40-90 cm away and
allows significant head movement as long as it stays inside
the trackable volume. To determine the fully specified target
position, a structured light sensor, the Microsoft Kinect, was
used. The Kinect provides a depth image at a resolution of
640x480 that is used to determine the depth of the target
position. The Kinect also supplies the video feedback looking
straight-on to the scene. The robot arm used was a three-
fingered Mico robot from Kinova. Opening and closing of the
robot’s fingers was controlled by a button on the Geomagic
Touch. Updates to change the desired position or state of the
fingers were sent to the robot at approximately 20 Hz, unless
the joystick was immobile and the fingers were not being
controlled. The experimental setup can be seen in Fig. 9.
B. Validation Process
Two different tasks were tested in the experiment, cutting
and grasping. For the grasping task, volunteers were asked to
(a) Operator-side setup
 (b) Robot-side setup
Fig. 9: Experimental setup which shows both the operator-side and the robot-side setup.
use the teleoperation system to pick up a tennis ball. The task
was considered a failure if the tennis ball was knocked off its
stand, or if any of the surrounding obstacles were disturbed.
The setup for this task is shown in Fig. 10a. For the cutting
task, volunteers were asked to cut a strip of paper in a special
marked area using the teleoperation system. This is illustrated
in Fig. 10b. Failure occurred during this task if the strip was
cut in the wrong location or the surrounding area was harmed.
This task simulated an action similar to one that might be
required in telesurgery.
The testing procedure began by calibrating the eye tracker
for each volunteer and verifying its accuracy. Each volunteer
was then given as much time as they needed to become
comfortable with the system, or re-familiarize themselves with
it if they had already used it. No force feedback was applied
during this part, and practice was done on the grasping task
with no obstacles. Once they were ready, each task was tested
with both the guidance force and the safety boundary in a
randomized order to ensure results were not skewed by a
learning curve. Additionally, both tasks were tested without
any haptic assistance (using the joystick only) to provide a
baseline for comparison. Explicitly, the system was tested in
the combinations laid out in Table III. Two to three trials were
performed for each combination and the success rate, as well
as the joystick and robot trajectory were recorded. The target,
either the tennis ball or paper strip, was randomly placed for
each trial. The target position was continuously acquired from
the gaze and before each trial the Geomagic Touch was placed
into a starting position that the robot mirrored.
Four volunteers were tested. The ages of those who partici-
pated in the testing were in the range 18 to 25 and two of the
volunteers wore glasses. One was left-handed and three had
prior experience using the system.
VII. R ESULTS AND DISCUSSION
During each trial, the target location, indicated by the gaze,
and the trajectory of the Geomagic Touch and the robot arm
was recorded. Additional information recorded for each trial
included the number of times the participant tried to grasp the
ball by closing the fingers and whether or not they were finally
successful. The results are broken down into two separate
tasks, cutting and grasping. For each we will evaluate three
TABLE III: Combinations tested for the validation of the
haptic assistance.
Test Task Haptic Assistance
1 cutting safety boundary
2 cutting guidance force
3 grasping safety boundary
4 grasping guidance force
5 cutting no assistance
6 grasping no assistance
criteria: success rate, completion time, and attempts by the
operator to close the scissors/fingers.
A. Cutting Task
1) Success Rate: Due to the number of trials obtained, a
Laplace estimate is used to determine the best success rate
for each condition. Further a 95% adjusted-Wald Interval is
used to compare the theoretical bounds on the success rate
observed. The confidence intervals with the Laplace estimate
are observed in Figure 11. For ease of reading, the joystick
success rate has been duplicated on the figure for both control
modes. In both cases, the assistance improves the success rate
over the joystick only control. The boundary assistance out-
performs the force guidance assistance. The intent adjustment
appears to help the force guidance mode perform better (likely
due to impacting the magnitude of the force more intuitively).
Yet, the intent adjustment in the boundary assistance does not
see improvement, although the success rates are rather similar.
This is likely due to the users perceiving this change as subtle
adaption. An N-1 Chi-Square test was conducted to determine
statistical significance between any proportions. No statistical
significance was found.
2) Execution Time: Time based evaluations are notorious
for being positively skewed [44], and for this reason the
analysis is done by log-transforming the data. The geometric
means and 95% confidence intervals are presented in Figure
12. The boundary assistance does better than joystick control.
The boundary assistance also outperforms the force guidance.
The intent adjustment helps both the boundary and guidance
force improve the speed to complete the task. However, the
improvement is more noticeable in the force guidance. A
(a) Grasping task setup.
 (b) Cutting task setup.
Fig. 10: Task setups used for testing the haptic assistance.
Fig. 11: Confidence intervals for the success rates for each
combination of haptic assistance per task.
two-sample t-test was conducted on each condition, and no
statistical significance was found. Although no significance
was found, the confidence intervals reinforce that the boundary
approach is a better control strategy. It has the smallest bounds
while accomplishing the least amount of time. The force
guidance appears to be a hindrance to users as if it requires
effort to resist undesired movements. However, the extra time
observed from the guidance force may be a result incorrect
depth registering as evident by the cutting attempts.
Fig. 12: Confidence intervals for execution time for each
combination of haptic assistance per task.
3) Cutting Attempts: A standard arithmetic mean and 95%
confidence interval was obtained for the cutting attempts
of each control strategy. They are displayed in Figure 13.
The confidence intervals of the boundary assistance are as
low or lower than the joystick control. The guidance force
required users to make more attempts to cut. This surge in
attempts is most likely responsible for the time increase. The
intent adjustment reduces the number of attempts needed to
accomplish the task. A two-sample t-test was conducted for
intent adjusted vs not intent adjusted control modes for a fair
comparison. No statistical significance was found.
Fig. 13: Confidence intervals for the cutting attempts for each
combination of haptic assistance per task.
B. Grasping Task
For the grasping task, only the assistance modes are com-
pared. The goal of this task is to see if the intent adjustment
is different from no intent adjustment. Issues from this task
occurred when the robot hand would occasionally be the
inferred gaze target.
1) Success Rate: Due to the number of trials obtained, a
Laplace estimate is used to determine the best success rate
for each condition. Further a 95% adjusted-Wald Interval is
used to compare the theoretical bounds on the success rate
observed. The confidence intervals with the Laplace estimate
are observed in Figure 14. The intent adjustment does not have
a positive influence on the success rate. In the force guidance
case, it appears to lower the success. An N-1 Chi-Square test
was conducted to determine statistical significance between
any proportions. No statistical significance was found.
Fig. 14: Confidence intervals for the grasping success for each
combination of haptic assistance per task.
2) Execution Time: The geometric means and 95% confi-
dence intervals are presented in Figure 15. The completion
time for the grasping task leads to mixed results. The intent
adjustment helps the boundary approach; however, it does not
help the guidance force assistance. For the no intent adjustment
cases, the force guidance does better than the boundary. For
intent adjustment, the boundary outperforms the guidance
force. A two-sample t-test was conducted on each condition.
Despite the variations of the confidence intervals no statistical
significance was found.
Fig. 15: Confidence intervals for the grasping execution time
for each combination of haptic assistance per task.
3) Grasping Attempts: A standard arithmetic mean and
95% confidence interval was obtained for the grasping at-
tempts of each control strategy. They are displayed in Figure
16. The intent adjustment forces more attempts to occur to
grasp the tennis ball. The best scenario is the boundary without
intent adjustment. A two-sample t-test was conducted for
adjusted vs not adjusted control modes for a fair comparison.
No statistical significance was found.
C. Summary of Results
In summary, the presented results show that this system
improves teleoperation control by assisting the operator in
reaching the correct target depth and preventing collateral
Fig. 16: Confidence intervals for the grasping attempts for each
combination of haptic assistance per task.
damage. In addition, the intent confidence is a valuable ad-
dition to the approach which allows the system to respond to
the operator’s focus, and provides more natural control for the
operator.
The results also reveal some details about how each form of
haptic assistance affects each task. While the safety boundary
with intent confidence appears to be quite helpful, the same
is not true of the guidance force. For maximum success, the
intent confidence adjustment should not be used with the
guidance force on the grasping task as this addition decreased
the success rate. Furthermore, it seems that the guidance
force for the cutting task is not as helpful. It may be a
better idea to use the safety boundary with intent adjustment
and use a strategy such as the potential hybrid control for
position investigated in our previous work. This will provide
the operator with partial visual feedback and partial haptic
feedback. In this case, the operator will still close the loop and
have full control over the system, but will not be distracted
by the guidance force.
VIII. C ONCLUSION
The presented haptic assistance adjusted based on the sys-
tem’s confidence in the gaze-derived operator’s intent for tele-
operation increases the control performance in teleoperation.
It is natural and easy to use because it takes advantage of
a natural characteristic of the operator’s behavior. It prevents
collateral damage through the use of a safety boundary which
also helps the operator approach the correct depth. The results
in section VII show that users are faster and more accurate
when using this system.
Future work will involve increasing the accuracy of the in-
ferred intent and the confidence in this intent. This necessarily
requires more information than the operator’s eye movements
alone. The reason for this is that the gaze is really an
observational mechanism and was not intended to be a control
input. In order for control to be truly natural, the operator’s
gaze has to be used in such a way as to not interrupt their
regular behavior. However, the operator will not just look at the
target during completion of the teleoperation task. He will also
look at the robot end-effector, at the surrounding obstacles, or
other distractions depending on the environment. Separating
these eye movements, which have little to do with the final
goal of the teleoperation task, from the “valid” fixations on the
target position is very difficult without additional information.
This is especially true in a more general setting where the tasks
may not be related to reaching. Therefore, eye-movement data
alone is likely not sufficient for a highly accurate determination
of the operator’s intent.
Introducing context into the intent inference process could
go a long way to solving the aforementioned issues. For
example, since the location of the robot hand is known,
fixations on the robot end-effector can automatically be filtered
from the intent inference process. Additional steps may be to
consider the actual structure of the environment being gazed at.
If there is no object at the fixation location, or the object is not
graspable, then the gazed-at location must not be the intended
goal of the action. Furthermore, taking into account gaze
history could provide additional insight into the true intent
of the operator. These considerations will improve the intent
inference, confidence level, and overall control significantly.
REFERENCES
[1] Y . Baudoin and M. K. Habib, Using Robots in Hazardous Environments:
Landmine Detection, De-mining and Other Applications. Cambridge:
Woodhead Publishing Limited, 2011.
[2] B. Brumson, “Chemical and hazardous material handling
robotics,” Jan. 2007. [Online]. Available: http://www.
robotics.org/content-detail.cfm/Industrial-Robotics-Industry-Insights/
Chemical-and-Hazardous-Material-Handling-Robotics/content id/614
[3] M. Fachot, “International electrotechnical commission,” Jul. 2011.
[Online]. Available: http://www.iec.ch/etech/2011/etech 0711/ind-1.htm
[4] Y . P. Rybarczyk, E. Colle, and P. Hoppenot, “Contribution of nuero-
science to the teleoperation of rehabilitation robot,” in Systems, Man
and Cybernetics, Hammamet, Tunisia, 2002.
[5] D. Y . P. Henriques, W. P. Medendorp, C. C. A. M. Gielen, and J. D.
Crawford, “Geometric computations underlying the eye-hand coordina-
tion: Orientations of the two eyes and the head,” Experimental Brain
Research, vol. 152, pp. 70–78, 2003.
[6] J. Gomer, C. Dash, K. Moore, and C. Pagano, “Using radial outflow
to provide depth information during teleoperation,” Presence, vol. 18,
no. 4, pp. 304–320, 2009.
[7] D. Drascic, “Skill acquisition and task performance in teleoperation
using monoscopic and stereoscopic video remote viewing,” Proceedings
of the Human Factors and Ergonomics Society Annual Meeting, vol. 35,
no. 19, pp. 1367–1371, 1991.
[8] G. P. Mylonas, A. Darzi, and G.-Z. Yang, Medical Imaging and
Augmented Reality. Springer Berlin Heidelberg, 2004.
[9] T. Brooks and I. Ince, “Operator vison aids for telerobot assembly and
servicing in space,” in IEEE International Conference on Robotics and
Automation, Nice, 1992.
[10] W. A. Hoff, L. B. Gatrell, and J. R. Spofford, “Machine-vison-based
teleoperation aid,” Telematics and Informatics, vol. 8, no. 4, pp. 403–
423, 1991.
[11] H. Martins and R. Ventura, “Immersive 3-d teleoperation of a search
and rescue robot using a head-mounted display,” in IEEE Conference
on Emerging Technologies and Factory Automation, Mallorca, 2009.
[12] S. Li, X. Zhang, F. J. Kim, R. D. da Silva, D. Gustafson, and
W. R. Molina, “Attention-aware robotic laparoscope based on fuzzy
interpretation of eye-gaze patterns,” Journal of Medical Devices, vol. 9,
no. 4, 2015.
[13] A. D. Dragan, S. S. Srinivasa, and K. C. T. Lee, “Teleoperation
with intelligent and customizable interfaces,” Journal of Human-Robot
Interaction, vol. 2, no. 2, pp. 33–57, 2013.
[14] J. B. Pelz and R. Canosa, “Oculomotor behavior and perceptual strate-
gies in complex tasks,” Vision Research, vol. 41, no. 25-26, pp. 3587–
3596, 2001.
[15] M. F. Land and M. Mayhoe, “In what ways do eye movements contribute
to everyday activities?” Vision Research, vol. 41, no. 25-26, pp. 3559–
3565, 2001.
[16] N. Mennie, M. Hayhoe, and B. Sullivan, “Look-ahead fixations: Antic-
ipatory eye movements in natural tasks,” Experimental Brain Research,
vol. 179, no. 3, pp. 427–442, 2006.
[17] A. Belardinelli, O. Herbort, and M. V . Butz, “Goal-oriented gaze
strategies afforded by object interaction,” Vision Research, vol. 106, pp.
47–57, 2015.
[18] L. van der Linden, S. Math ˆot, and F. Vitu, “The role of object affordances
and center of gravity in eye movements toawrd isolated daily-life
objects,” Journal of Wisdom, vol. 15, no. 5, 2015.
[19] J. Crawford, W. Medendorp, and J. Marotta, “Spatial transformations for
eye-hand coordination,” Journal of Neurophysiology, vol. 92, pp. 10–14,
2004.
[20] A. Belardinelli and M. V . Butz, “Gaze strategies in object identification
and manipulation,” in CogSci, Berlin, 2013.
[21] Y . P. Rybarczyk, O. Ait-Aider, P. Hoppenot, and E. Colle, “Remote
control of a biometrics robot assistance system for disabled persons,”
AMSE Modelling, Measurement, and Control, vol. 63, 2002.
[22] S. Hirche and M. Buss, “Human-oriented control for haptic teleopera-
tion,” Proceedings of the IEEE, vol. 100, no. 3, pp. 623–647, 2012.
[23] J. Rosen, B. Hannaford, M. P. MacFarlane, and M. N. Sinanan, “Force
controlled and teleoperated endoscopic grasper for minimally invasive
surgery - experimental performance evaluation,” IEEE Transactions on
Biomedical Engineering, vol. 46, no. 10, pp. 1212–1221, 2009.
[24] C. R. Wagner, R. D. Howe, and N. Stylopoulos, “The role of force
feedback in surgery: Analysis of blunt dissection,” in Haptic Interfaces
for Virtual Environment and Teleoperator Systems, Orlando, 2002.
[25] D. Feygin, M. Keehner, and F. Tendick, “Haptic guidance: Experimental
evaluation of a haptic training method for a perceptual motor skill,” in
Haptic Interfaces for Virtual Environment and Teleoperator Systems,
Orlando, 2002.
[26] J. J. Abbot, P. Marayong, and A. M. Okamura, Robotics Research.
Springer Berlin Heidelberg, 2007, vol. 28.
[27] N. Turro, O. Khatib, and E. Coste-Maniere, “Haptically augmented
teleoperation,” in IEEE International Conference on Robotics and Au-
tomation, Seoul, 2001.
[28] G. P. Mylonas, K.-W. Kwok, A. Darzi, and G.-Z. Yang, “Gaze-
contingent motor channeling and haptic constraints for minimally in-
vasive robotic surgery,” in Medical Image Computing and Computer-
Assisted Intervention, New York, 2008.
[29] T. M. Lam, H. W. Boschloo, M. Mulder, and M. M. van Paassen,
“Artificial force field for haptic feedback in uav teleoperation,” IEEE
Transactions on Systems, Man, and Cybernetics - Part A: Systems and
Humans, vol. 39, no. 6, pp. 1316–1330, 2009.
[30] N. Famaey, E. Verbeken, S. Vinckier, B. Willaert, P. Herijgers, and J. V .
Sloten, “In vivo soft tissue damage for applications in surgery,” Medical
Engineering & Physics, vol. 32, no. 5, pp. 437–443, 2010.
[31] T. L. Gibo, L. N. Verner, and D. Okamura, “Design considerations
and human-machine performance of moving virtual fixtures,” in IEEE
International Conference on Robotics and Automation, Kobe, 2009.
[32] F. Ryden, S. N. Kosari, and H. J. Chizeck, “A computer vision approach
to virtual fixtures in surgical robotics,” 2012. [Online]. Available:
http://automation.berkeley.edu/RSS2012Workshop/abstract2.pdf
[33] F. Ryden, H. J. Chizeck, S. N. Kosari, H. King, and B. Hannaford,
“Using kinect and a haptic interface for implementation of real-time
virtual fixtures,” in Robotics Sciences and Systems, Workshop on RGB-
D: Advanced Reasoning with Depth Cameras, Los Angeles, 2011.
[34] F. Despinoy, J. L. Torres, M. Vitrani, and B. Herman, “Toward remote
teleoperation with eye and hand: A first experimental study,” in 3rd Joint
Workshop on New Technoogies for Computer/Robot Assisted Surgery,
2013.
[35] K. Fujii, Gaze Contingent Robotic Control in Minimally Invasive
Surgery. London: Imperial College of London, 2014.
[36] A. Pandya, L. A. Reisner, B. King, N. Lucas, A. Composto, M. Klein,
and R. D. Ellis, “A review of camera viewpoint automation in robotic
and laparoscopic surgery,” Robotics, vol. 3, no. 3, pp. 310–329, 2014.
[37] M. Yu, Y . Lin, D. Schmidt, X. Wang, and Y . Wang, “Human-robot
interaction based on gaze gestures for the drone teleoperation,” Eye
Movement Research, vol. 7, no. 4, pp. 1–14, 2014.
[38] H. O. Latif, “Mobile robot teleoperation through eye-gaze (telegaze),”
Ph.D. dissertation, Nottingham Trent University, 2010.
[39] Z. Ahmed and A. Shahzad, “Mobile robot navigation using gaze
contingent dynamic interface,” Master’s thesis, Blekinge Institute of
Technology, 2010.
[40] A. Doshi and M. M. Trivedi, “On the roles of eye gaze and head dy-
namics in predicting driver’s intent to change lanes,” IEEE Transactions
on Intelligent Trasportation Systems, vol. 10, no. 3, pp. 453–462, 2009.
[41] S. Nilsson, T. Gustafsson, and P. Carleberg, “Hands free interaction with
virtual information in a real environment: Eye gaze as an interaction tool
in an augmented reality system,” PsychNology Journal, vol. 7, no. 2, pp.
175–196, 2009.
[42] C. Passenberg, A. Peer, and M. Buss, “A survey of environment-
, operator-, and task-adapted controllers for teleoperation systems,”
Mechatronics, vol. 20, no. 7, pp. 787–801, 2010.
[43] J. D. Webb, S. Li, and X. Zhang, “Using visuomotor tendencies to
increase control performance in teleoperation,” in American Controls
Conference, Boston, 2016.
[44] J. Sauro and J. R. Lewis, “Average task times in usability tests:
What to report?” in Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, ser. CHI ’10. New York, NY , USA:
Association for Computing Machinery, 2010, p. 2347–2350. [Online].
Available: https://doi.org/10.1145/1753326.1753679