SUCCESSOR REPRESENTATION ACTIVE INFERENCE
Beren Millidge∗
MRC Brain Networks Dynamics Unit,
University of Oxford, UK,
Verses Research Lab,
Los Angeles, California, USA
beren@millidge.name
Christopher L Buckley
Sussex AI Group, Department of Informatics,
University of Sussex, UK
Verses Research Lab,
Los Angeles, California, USA
C.L.Buckley@sussex.ac.uk
ABSTRACT
Recent work has uncovered close links between between classical reinforcement learning al-
gorithms, Bayesian ﬁltering, and Active Inference which lets us understand value functions in terms
of Bayesian posteriors. An alternative, but less explored, model-free RL algorithm is the successor
representation, which expresses the value function in terms of a successor matrix of expected future
state occupancies. In this paper, we derive the probabilistic interpretation of the successor repres-
entation in terms of Bayesian ﬁltering and thus design a novel active inference agent architecture
utilizing successor representations instead of model-based planning. We demonstrate that active in-
ference successor representations have signiﬁcant advantages over current active inference agents
in terms of planning horizon and computational cost. Moreover, we demonstrate how the successor
representation agent can generalize to changing reward functions such as variants of the expected
free energy.
1 Introduction
Active Inference (AIF) is an unifying theory of action selection in theoretical neuroscience K. Friston et al. (2015);
K. Friston, Samothrakis and Montague (2012); K. J. Friston, Daunizeau and Kiebel (2009). It suggests that action
selection, like perception, is fundamentally a problem of inference and that agents select actions by maximizing
evidence under a biased generative model Da Costa, Parr et al. (2020); K. Friston, FitzGerald, Rigoli, Schwartenbeck
and Pezzulo (2017a). Active inference operates under the aegis of the Bayesian brain hypothesis Doya, Ishii, Pouget
and Rao (2007); Knill and Pouget (2004) and free energy principles Aguilera, Millidge, Tschantz and Buckley (2021);
Buckley, Kim, McGregor and Seth (2017); K. Friston (2019); K. Friston and Ao (2012); K. Friston, Kilner and
Harrison (2006); Millidge, Seth and Buckley (2021) and possesses several neurobiological process theories K. Friston
et al. (2017a); Parr, Markovic, Kiebel and Friston (2019).
∗Corresponding author.
arXiv:2207.09897v1  [cs.AI]  20 Jul 2022
Recent work Millidge, Tschantz, Seth and Buckley (2020) has uncovered close links between active inference and
the framework of control as inference, which shows how many classical reinforcement learning algorithms can be
understood as performing Bayesian inference to infer optimal actions Attias (2003); Levine (2018); Rawlik (2013);
Toussaint (2009). These works, as well as the related duality between control and inference in linearly solvable MDPs
Todorov (2008, 2009) has allowed us to understand classical objects in reinforcement learning such as Q-functions
and value functions in terms of Bayesian ﬁltering posteriors. Similarly, close connections between active inference
and reinforcement learning methods have also been demonstrated Millidge (2019a, 2019b); Tschantz, Millidge, Seth
and Buckley (2020b). It has been shown that deep active inference agents can be derived that can perform actor-
critic algorithms Millidge (2019b) as well as model-based reinforcement learning Fountas, Sajid, Mediano and Friston
(2020); Tschantz, Millidge, Seth and Buckley (2020a); Tschantz et al. (2020b), while the fundamental difference
between them has been found to be related to the encoding of value into the generative model Millidge (2021); Millidge
et al. (2020). Moreover, it has become obvious that active inference can be treated in a model-free (Bellman-equation)
paradigm with simply a distinct reward function (the expected free energy) Da Costa, Sajid, Parr, Friston and Smith
(2020); Millidge (2019b). However, while much of this work has focused on understanding value functions and model-
based RL, another fundamental object in model-free reinforcement learning is the successor representation, which has
received much less attention overall. The successor representation (SR) Dayan (1993) provides an alternative way to
estimate value functions. Instead of estimating the value function with the Bellman backup, a successor matrix of
long-term discounted state transitions is estimated instead and then dynamically combined with the reward function
to yield the value function for a ﬁxed policy. Compared to estimating the value function directly, the SR requires
more memory to store the successor matrix but grants the ability to dynamically recompute the value function as the
reward function changes as well as providing a compressed form of a ‘cognitive map’ of the environment which can be
directly used for exploration and option discovery Machado, Bellemare and Bowling (2020); Machado et al. (2017);
Momennejad (2020). moreover, from a neuroscientiﬁc perspective, the SR has been closely linked to representations
in the hippocampus Momennejad et al. (2017); Stachenfeld, Botvinick and Gershman (2017) which are concerned
with representing abstract (usually spatial) relations Behrens et al. (2018); Whittington, McCaffary, Bakermans and
Behrens (2022); Whittington et al. (2020).
In this work, applying the probabilistic interpretation of the SR, we showcase how the SR can be directly integrated
with standard methods in active inference, resulting in the successor-representation active inference (SR-AIF) agent.
We show how SR-AIF has signiﬁcant computational complexity beneﬁts over standard AIF and that, moreover, the
explicit generative model in AIF enables the SR to be computed instantly without requring substantial experience in
the environment. Additionally, we show how SR methods can ﬂexibly represent the value function of the EFE and can
be used to dynamically trade-off exploration and exploitation at run-time.
2 Active Inference
Discrete-state-space active inference possesses a large literature and several thorough tutorials Da Costa, Parr et al.
(2020); K. Friston, FitzGerald, Rigoli, Schwartenbeck and Pezzulo (2017b); Smith, Friston and Whyte (2022), so we
only provide the essentials here. AIF considers agents acting in POMDPs with observations o, states x, and actions
u. The agent optimizes over policies π = [u1,u2,... ] which are simply sequences of actions. The agent is typically
2
assumed to be equipped with a generative model of the environment p(o1,T,x1:T) which describes how observations
and states are related over time. This generative model can be factorized into two core components: a likelihood model
p(ot|xt) which states how observations are generated from states and is represented by a likelihood matrix denoted
A, and a transition model p(xt|xt−1,ut−1) which states how states change depending on the previous state and action
and is represented by a transition matrix denoted B(u). The rewards or goals of the agents are encoded as strong
priors in the generative model and are encoded in a ‘goal vector’ denoted C. Since AIF considers agents embedded
in a POMDP it has to solve both state inference and action selection problems. State inference is performed using
variational inference with a categorical variational distribution q(xt) which is obtained by minimizing the variational
free energy for a speciﬁc timestep t,
q∗(xt) =argmin
q
Ft = argmin
q
Eq(xt|ot)[log q(xt|ot) −log p(ot,xt|xt−1,ut−1)] (1)
AIF uses a unique objective function called the Expected Free Energy (EFE) which combines utility or reward max-
imization with an information gain term which promotes exploration. AIF agents naturally perform both reward-
seeking and information-seeking behaviour K. Friston et al. (2015); Millidge, Tschantz and Buckley (2021); Millidge,
Tschantz, Seth and Buckley (2021); Parr and Friston (2019). The EFE is deﬁned as,
Gt(ot,xt) =Eq(ot,xt)[log q(xt) −log ˜p(ot,xt|xt−1,ut−1)]
= Eq(ot,xt)[log ˜p(ot)]  
Expected Utility
−Eq(ot)
[
KL[q(xt|ot)||q(xt)]
]
  
Expected Information Gain
(2)
Where ˜pis a ‘biased’ generative model which contains the goal prior vectorC. As can be seen, the EFE can be decom-
posed into a reward-seeking and exploratory component which underlies the ﬂexible uncertainty-reducing behaviour
of AIF agents. To select actions, AIF samples from the prior over policies q(π) which is deﬁned as the softmax over
the path integral of the EFE into the future for each timestep. Typically, future policies are evaluated up to a time
horizon T. This path integral can be expressed as,
q(π) =σ(
T∑
t
Gπ
t (ot,xt)) (3)
where σ(x) = e−x
∑
x e−x is the softmax function. Evaluating this path integral exactly for each policy is typically
computationally extremely expensive and has exponential complexity due to the exponentially branching number of
possible futures to be evaluated. This causes AIF agents to run slowly in practice and has encouraged research into
alternative ‘deep’ active inference agents which estimate this path integral in other more efﬁcient (but only approxim-
ate) ways K. Friston, Da Costa, Hafner, Hesp and Parr (2021) Here, we present a novel approach based on successor
representations.
3 Successor Representation
The Successor Representation Dayan (1993) provides an alternative way to compute the value function of a state.
Instead of directly learning the value function (or Q function), for instance by temporal difference (TD) learning,
the successor representation learns the successor matrix, which is the discounted long term sum of expected state
occupancies, from which the value function can be dynamically computed by simply multipling the successor matrix
with the reward function. This allows the SR to instantly adapt behaviour to changing reward functions online without
explicit model-based planning.
3
The value function can be deﬁned as the expected long term sum of rewards,
Vπ(x) =r(x) +γBπVπ(x)
= r(x) +γBπ[r(x) +γ2Bπ[r(x) +γ3Bπ[···]]] (4)
Where we assume a ﬁxed policy πfor the value function and transition matrixBand where γis a scalar discount rate.
Due to this ﬁxed policy assumption, the max operator in the Bellman equation disappears, so the Bellman equation
becomes linear. We can rearrange,
Vπ(x) =r(x) +γBπ[r(x) +γ2Bπ[r(x) +γ3Bπ[···]]]
= (I+ γBπ + γ2BπBπ + ···)r(x) =Mπr(x) (5)
Where Mπ is the successor matrix and can be thought of as encoding the long-run probability that state xtransitions
to state x′.
4 Successor Representation as Inference
In a special class of MDPs known as linearly solvable MDPs there is a general duality between control and infer-
ence Todorov (2008, 2009) such that control can be cast as a Bayesian ﬁltering problem where the value function
corresponds to the posterior. To see this, consider the optimal Bellman equation,
V∗(xt) =argmax
u
[
r(xt) +c(u) +Ep(xt+1|xt,u)[γV∗(xt+1)]
]
(6)
where we have added an additional control cost c(u). The fundamental challenge is the nonlinearity of the Bellman
equation due to the argmax operation. Todorov (2009) noticed that if the dynamics are considered to be completely
controllable and set by the action p(xt+1|xt,u) = u(·|xt), while the original dynamics instead take the form of a
‘dynamics prior’ in the control cost which is set to KL[u(·|xt)||p(xt+1|xt,u)] which penalizes divergence from the
prior dynamics, then the argmax is analytically solvable. By deﬁning the ‘desirability function’ z(x) =e−V∗(x) and
exponentiating, we can obtain a linear equation in z,
z(x) =e−r(x)Ep(xt+1|xt)[γz(xt+1)] (7)
which can be solved easily. Crucially, however, this equation takes the same form as the Bayesian ﬁltering recursion
p(xt|ot) ∝p(ot|xt)Ep(xt|xt−1 p(xt−1|ot−1) when we make the identiﬁcation of the ‘desirability’ z(xt) with the pos-
terior p(xt|ot) and the exponentiated reward e−r(xt) with the likelihood p(ot|xt). Interestingly, this same relationship
between exponentiated reward and probability is also used heuristically in the control as inference literature Levine
(2018). An additional subtle point is that control is about the future instead of the past so the sum telescopes forward
instead of backwards in time. By factoring Equation 6 as in Equation 5, it is straightforward to observe that,
M =
T∑
τ=t
γτ
( T∏
i=t
∑
xi
p(xi|xi−1)
)
=
T∑
τ=t
γτp(xτ|xt) (8)
In effect, we can think of M as representing the discounted sum of the probabilities of all the possible times to reach
state xover the time horizon. A similar and novel result can be derived for the case of general (not linearly solvable)
MDPs but with a ﬁxed policy except here we derive an upper bound on the SR instead of an equality. We begin by
4
taking the log of the backwards Bayesian ﬁltering posterior and then repeatedly applying Jensen’s inequality to obtain,
log p(xt|ot) = logp(ot|xt) + logEp(xt+1|xt)[log p(xt+1|ot+1)]
≤log p(ot|xt) +Ep(xt+1|xt)
[[
log p(ot|xt) +Ep(xt+2|xt+1)[log ... ]
]]
(9)
Which has the same recursive structure as the linear Bellman Equation for a ﬁxed policy (Equation 4) so long as we
maintain the equivalence between the value function and the log posterior and the reward and the log likelihood and
implicitly set the discount factor γto 1. The technique in Equation 5 can then be applied to give the same probabilistic
interpretation of the SR as Equation 8. In sum, we have shown how optimal control can be associated with ﬁltering
and Bayesian posteriors exactly in the case of linear MDPs and the Bayesian posterior as an upper bound in the case
of a ﬁxed policy. These results provide a sound probabilistic and Bayesian interpretation of the SR, which has hiterto
been missing in the literature, and let us design a principled active inference agent based upon the SR.
5 Successor Representation Active Inference
Using the probabilistic interpretation of the SR and the equations of discrete state-space AIF, we can construct an AIF
agent which utilizes the SR to compute value functions of actions instead of model-based planning. That is, the policy
posterior path integral q(π) = σ(G) = σ(∑T
t Gt) can be considered as a value function and dynamically computed
using the SR. The fact that in discrete AIF the generative model transition matrix B(u) is given allows us to dispense
with learning the successor matrix from experience. However, to apply the SR, we need to choose which policy π
we wish to compute the value function under. This choice is important since the default policy must assign enough
probability mass to all parts of the state-space to be able to provide an accurate value estimate there. Heuristically, we
set the default policy to be uniform over the action space p(u) = 1
A where Ais the cardinality of the action space.
This lets us deﬁne the default transition matrix,
˜B = Ep(u)[B(u)] = 1
A
∑
i
B[:,:,ui] (10)
Given ˜B, we can analytically calculate the SR using the inﬁnite series result,
Mπ = (I+ γ˜B+ γ2 ˜B2 ···) = (I−γ˜B)−1 (11)
This means that as long as the generative model is known, the EFE value function q(π) can be computed exactly
without any interaction with the environment by ﬁrst computing Mπ as in Equation 11 and then multiplying by the
reward function which is the EFE G= MπGπ(x). From this EFE value function actions can be sampled from the
posterior over actions as,
u∼q(π) =σ(G) (12)
A slight complication is that while the SR is deﬁned for MDPs, AIF typically assumes a POMDP structure with
observations othat do not fully specify the hidden state but are related through the likelihood matrix A. We address
this by computing observation value functions as the expected state posterior under the state posterior distribution,
Vπ(o) =Eq(x|o)[Vπ(x)] =qMπGπ (13)
where q = [q1,q2 ···] is the categorical variational posterior. The SR-AIF algorithm can thus be summarized as fol-
lows: we are given a generative model containing theAand Bmatrices and a set of desired states C. At initialization,
5
the agent computes the successor matrix Mπ using the default policy with Equation 10. For each action in a given
state, SR-AIF computes the EFE value function Gπ for that action and then actions are sampled from the policy pos-
terior which is the softmax over the EFE action-value functions. In a POMDP environment exactly the same process
takes place except instead of action-state we have action-observation value functions which are computed as Equation
13.
5.1 Computational Complexity
In theory the computational complexity of SR-AIF is superior than standard AIF as standard-AIF uses model-based
planning which evaluates the EFE value function Gby exhaustively computing all possible future trajectories for
different policies. This has a cost that grows exponentially in the time horizon due to the branching of possible
futures. If we denote the number of actions A, the dimension of the state-space Xand the time-horizon T, we can
approximately say that the computational complexity of standard AIF is of order O(XT2 ·AT) since the number
of possible trajectories is approximately AT where evaluating each step of a trajectory costs of order X and we
must repeat this for each timestep. This is exponential in the time-horizon and renders AIF unsuitable for long term
planning. Several heuristic methods have been proposed to handle this, usually by pruning obviously unsuccessful
policies K. Friston et al. (2021). However, this does not remove the exponential complexity but only reduces it by
a constant factor. In practice, this exponential explosion is also handled by simply reducing the time-horizon or the
policy space to be searched, which renders the evaluation of Gapproximate and makes the AIF agents myopic to long
term reward contingencies.
By contrast, SR-AIF analytically computes an approximation to the EFE value function Gdirectly from the known
transition dynamics by Equation 11. This means that no exhaustive future simulation is required for each action but
instead only a one-time cost is incurred at initialization. The main cost is the matrix inverse of approximately X3.
Then an action must be selected which costs of order A. This means the total complexity of SR-AIF is of order
O(X3 + AT). SR-AIF thus reduces the computational complexity of AIF from exponential to cubic and hence,
in theory, allows discrete-state-space active inference to be applied to substantially larger problems than previously
possible.
6 Experiments
We empirically demonstrate the superior computational complexity and ultimate performance of SR-AIF as the state-
space and time-horizon grows on a series of grid-world environments. These provide a simple test-bed environment
for evaluating computational complexity in practice without the confounding factors introduced by a more complex
environment. The agent is initialized randomly in anN×N grid and must reach a reward located in the bottom corner
of the grid. On average, as the grid size increases, both the state-space size and the planning horizon required to ﬁnd
this reward increase.
We implemented the AIF agent using the pymdp library for discrete state-space AIF Heins et al. (2022). We found
that for larger grid-sizes the matrix inverse used to compute the successor matrix often became numerically unstable.
Heuristically, we countered this by increasing the ‘discount rate’γin Equation 11 to be greater than 1 (we used 5 for
larger grid-sizes). Otherwise γfor SR-AIF and standard AIF was set to 0.99. Intuitively this can be seen as weighting
6
Figure 1: Top Row: A: Schematic of the grid-world task. The AIF agent is initialized in a random square and must
make it to the bottom corner to obtain reward. B: The total reward obtained on average for SR-AIF and AIF agents. Due
to a limited planning horizon, the AIF agent cannot solve larger gridworlds and hence incurs large negative rewards.
C: Computational cost (measured in compute time per episode) for SR-AIF. For small gridworlds, SR-AIF is more
expensive since the matrix inversion cost dominates while for larger gridworlds the cost of standard AIF increases
exponentially. Bottom Row: Visualization of the default policy matrix ˜B, Successor matrix M, and estimated value
function Vπ for a 3 ×3 gridworld.
future states more than the present and had the numerical effect of increasing the differences in the value function
between far away states which would otherwise collapse to a negligible value. This, however, is a heuristic device and
removes the probabilistic interpretation of the discount rate as a probability of agent survival as in Levine (2018). For
the active inference agent, to keep computation times managable, we used an planning horizon of 7 and policy length
of 7.
This task had no epistemic contingencies but only involved reward maximization. A key aspect of active inference
though is its native handling of uncertainty through the EFE objective. Here, we demonstrate that the SR representation
can adapt to uncertainty and dynamically change the balance of exploration and exploitation. To demonstrate this, we
introduce an uncertainty and exploration component into the gridworld task by setting some squares of the grid to be
‘unknowable’ such that if the agent is on these squares, it is equally likely to receive an observation from any other
square in the same row of the grid. This is done by setting columns of the A matrix to uniform distributions for each
‘unknowable’ square in the grid. We show that if equipped with the EFE objective function, SR-AIF is able to instantly
recompute the value function based on this information in the A matrix without having to change the successor matrix
M. Moreover, due to the property that the value function can be recomputed for each reward function, this allows a
dynamic weighting of the utility and information gain components of the EFE to take place at runtime.7
Figure 2: Effect of introducing observation uncertainty into the model. A: the A matrix with two ‘unknowable’
squares resulting in a uniform distribution in two columns. B: the corresponding entropy of the state-space with the
two ‘unknowable’ squares having high entropy. C and D: The value function computed using the EFE which responds
positively to regions of high uncertainty since there is the potential for information gain compared to the standard
reward function. SR-AIF is able to correctly combine both utility and epistemic drives at runtime.
7 Discussion
In this paper, we have derived a probabilistic interpretation of the SR and related it to control as inference and linear
RL. We then constructed an SR-AIF algorithm which exhibits superior signiﬁcant performance and computational
complexity beneﬁts to standard AIF due to its amortization of policy selection using a successor matrix which can be
computed analytically at initialization.
It is important to note that while the SR-AIF has substantially better computational complexity, this comes at the cost
of a necessary approximation. The successor matrix is computed only for a ﬁxed default policy π and the choice of
this policy can have signiﬁcant effects upon the estimated value function and hence upon behaviour. The choice of the
default policy is thus important to performance and was here chosen entirely on heuristic grounds. Principled ways
of estimating or bootstrapping better default policies would be important for improving the performance of SR-AIF in
practice. This is especially important due to the reﬂexivity of RL environments whereby the default exploration policy
determines what data will be sampled from the environment which is then used to further train the model and reﬁne
the SR. Alternatively, the MDP itself could be regularized so that it becomes linear as in Todorov (2009) such that the
optimal policy can be solved for directly. This approach has been applied in a neuroscience context Piray and Daw
(2021) but the extension to active inference remains to be investigated.
Another point of extension is that here we have considered AIF and SR-AIF in the context of a single sensory modality
and a single-factor generative model. However, many tasks modelled by AIF use multiple factors and modalities to
express more complex relationships and contingencies. The extension of SR-AIF to multiple modalities and factors is
straightforward algebraically, but has subtle implementation details and is left to future work.
8
8 Code Availability
Code to reproduce all experiments and ﬁgures can be found at:
https://github.com/BerenMillidge/Active Inference Successor Representations.
9 Acknowledgements
Beren Millidge is supported by the BBSRC grant BB/S006338/1 and by Verses Research. CLB is supported by
BBRSC grant number BB/P022197/1 and by Joint Research with the National Institutes of Natural Sciences (NINS),
Japan, program No. 01112005.
References
Aguilera, M., Millidge, B., Tschantz, A. & Buckley, C. L. (2021). How particular is the physics of the free energy
principle? Physics of Life Reviews.
Attias, H. (2003). Planning by probabilistic inference. In Aistats.
Behrens, T. E., Muller, T. H., Whittington, J. C., Mark, S., Baram, A. B., Stachenfeld, K. L. & Kurth-Nelson, Z.
(2018). What is a cognitive map? organizing knowledge for ﬂexible behavior. Neuron, 100(2), 490–509.
Buckley, C. L., Kim, C. S., McGregor, S. & Seth, A. K. (2017). The free energy principle for action and perception:
A mathematical review. Journal of Mathematical Psychology, 81, 55–79.
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V . & Friston, K. (2020). Active inference on discrete state-spaces:
a synthesis. arXiv preprint arXiv:2001.07203.
Da Costa, L., Sajid, N., Parr, T., Friston, K. & Smith, R. (2020). The relationship between dynamic programming and
active inference: The discrete, ﬁnite-horizon case. arXiv preprint arXiv:2009.08111.
Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representation. Neural
Computation, 5(4), 613–624.
Doya, K., Ishii, S., Pouget, A. & Rao, R. P. (2007). Bayesian brain: Probabilistic approaches to neural coding. MIT
press.
Fountas, Z., Sajid, N., Mediano, P. & Friston, K. (2020). Deep active inference agents using monte-carlo methods.
Advances in neural information processing systems, 33, 11662–11675.
Friston, K. (2019). A free energy principle for a particular physics. arXiv preprint arXiv:1906.10184.
Friston, K. & Ao, P. (2012). Free energy, value, and attractors.Computational and mathematical methods in medicine,
2012.
Friston, K., Da Costa, L., Hafner, D., Hesp, C. & Parr, T. (2021). Sophisticated inference.Neural Computation, 33(3),
713–763.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. (2017a). Active inference: a process theory.
Neural computation, 29(1), 1–49.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. (2017b). Active inference: a process theory.
Neural computation, 29(1), 1–49.
Friston, K., Kilner, J. & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris,
100(1-3), 70–87.
9
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T. & Pezzulo, G. (2015). Active inference and epistemic
value. Cognitive neuroscience, 6(4), 187–214.
Friston, K., Samothrakis, S. & Montague, R. (2012). Active inference and agency: optimal control without cost
functions. Biological cybernetics, 106(8-9), 523–541.
Friston, K. J., Daunizeau, J. & Kiebel, S. J. (2009). Reinforcement learning or active inference? PloS one, 4(7).
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I. & Tschantz, A. (2022). pymdp: A python
library for active inference in discrete state spaces. arXiv preprint arXiv:2201.03904.
Knill, D. C. & Pouget, A. (2004). The bayesian brain: the role of uncertainty in neural coding and computation.
TRENDS in Neurosciences, 27(12), 712–719.
Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint
arXiv:1805.00909.
Machado, M. C., Bellemare, M. G. & Bowling, M. (2020). Count-based exploration with the successor representation.
In Proceedings of the aaai conference on artiﬁcial intelligence (V ol. 34, pp. 5125–5133).
Machado, M. C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G. & Campbell, M. (2017). Eigenoption discovery
through the deep successor representation. arXiv preprint arXiv:1710.11089.
Millidge, B. (2019a). Combining active inference and hierarchical predictive coding: A tutorial introduction and case
study.
Millidge, B. (2019b). Deep active inference as variational policy gradients. arXiv preprint arXiv:1907.03876.
Millidge, B. (2021). Applications of the free energy principle to machine learning and neuroscience. arXiv preprint
arXiv:2107.00140.
Millidge, B., Seth, A. & Buckley, C. L. (2021). A mathematical walkthrough and discussion of the free energy
principle. arXiv preprint arXiv:2108.13343.
Millidge, B., Tschantz, A. & Buckley, C. L. (2021). Whence the expected free energy? Neural Computation, 33(2),
447–482.
Millidge, B., Tschantz, A., Seth, A. & Buckley, C. (2021). Understanding the origin of information-seeking explora-
tion in probabilistic objectives for control. arXiv preprint arXiv:2103.06859.
Millidge, B., Tschantz, A., Seth, A. K. & Buckley, C. L. (2020). On the relationship between active inference and
control as inference. arXiv preprint arXiv:2006.12964.
Momennejad, I. (2020). Learning structures: predictive representations, replay, and generalization. Current Opinion
in Behavioral Sciences, 32, 155–166.
Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D. & Gershman, S. J. (2017). The successor
representation in human reinforcement learning. Nature human behaviour, 1(9), 680–692.
Parr, T. & Friston, K. J. (2019). Generalised free energy and active inference. Biological cybernetics, 113(5-6),
495–513.
Parr, T., Markovic, D., Kiebel, S. J. & Friston, K. J. (2019). Neuronal message passing using mean-ﬁeld, bethe, and
marginal approximations. Scientiﬁc reports, 9(1), 1–18.
Piray, P. & Daw, N. D. (2021). Linear reinforcement learning in planning, grid ﬁelds, and cognitive control. Nature
Communications, 12(1), 1–20.
Rawlik, K. C. (2013). On probabilistic inference approaches to stochastic optimal control.
10
Smith, R., Friston, K. J. & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its application to
empirical data. Journal of mathematical psychology, 107, 102632.
Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. (2017). The hippocampus as a predictive map. Nature
neuroscience, 20(11), 1643–1653.
Todorov, E. (2008). General duality between optimal control and estimation. In2008 47th ieee conference on decision
and control (pp. 4286–4292).
Todorov, E. (2009). Efﬁcient computation of optimal actions. Proceedings of the national academy of sciences ,
106(28), 11478–11483.
Toussaint, M. (2009). Probabilistic inference as a model of planned behavior. KI, 23(3), 23–29.
Tschantz, A., Millidge, B., Seth, A. K. & Buckley, C. L. (2020a). Control as hybrid inference. arXiv preprint
arXiv:2007.05838.
Tschantz, A., Millidge, B., Seth, A. K. & Buckley, C. L. (2020b). Reinforcement learning through active inference.
arXiv preprint arXiv:2002.12636.
Whittington, J. C., McCaffary, D., Bakermans, J. J. & Behrens, T. E. (2022). How to build a cognitive map: insights
from models of the hippocampal formation. arXiv preprint arXiv:2202.01682.
Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N. & Behrens, T. E. (2020). The tolman-
eichenbaum machine: unifying space and relational memory through generalization in the hippocampal forma-
tion. Cell, 183(5), 1249–1263.
11