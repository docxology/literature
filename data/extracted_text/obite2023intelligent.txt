Intelligent Resource Allocation for UA V-Based
Cognitive NOMA Networks: An Active Inference
Approach
Felix Obite1,2, Ali Krayani 1, Atm S. Alam 2, Lucio Marcenaro 1, Arumugam Nallanathan 2, Carlo Regazzoni 1
1DITEN, University of Genova, Italy
2EECS, Queen Mary University of London, United Kingdom
emails:felix.obite@edu.unige.it, ali.krayani@ieee.org {lucio.marcenaro, carlo.regazzoni}@unige.it, {a.alam, a.nallanathan}@qmul.ac.uk
Abstractâ€”Future wireless networks will need to improve
adaptive resource allocation and decision-making to handle
the increasing number of intelligent devices. Unmanned aerial
vehicles (UA Vs) are being explored for their potential in real-time
decision-making. Moreover, cognitive non-orthogonal multiple
access (Cognitive-NOMA) is envisioned as a remedy to address
spectrum scarcity and enable massive connectivity. This paper
investigates the design of joint subchannel and power allocation
in an uplink UA V-based cognitive NOMA network. We aim to
maximize the cumulative sum rate by jointly optimizing the
subchannel and power allocation based on the UA Vâ€™s mobility
at each time step. This is often formulated as an optimization
problem with random variables. However, conventional optimiza-
tion algorithms normally introduce significant complexity, and
machine learning methods often rely on large but partially rep-
resentative datasets to build solution models, assuming stationary
testing data. Consequently, inference strategies for non stationary
events are often overlooked. In this study, we introduce a novel
active inference-based learning approach, rooted in cognitive
neuroscience, to solve this complex problem. The framework
involves creating a training dataset using random or iterative
methods to find suboptimal resource allocations. This dataset
trains a mobile UA V offline, enabling it to learn a generative
model of discrete subchannels and continuous power allocation.
The UA V then uses this model for online inference. The method
incrementally derives new generative models from training data
by identifying dynamic equilibrium conditions between required
actions and variables, represented within a unique dynamic
Bayesian network. The proposed approach is validated through
numerical simulations, showing efficient performance compared
to suboptimal baseline schemes.
Index Termsâ€”Active Inference, UA V , NOMA, Resource Allo-
cation.
I. I NTRODUCTION
As the world advances towards the sixth generation (6G)
wireless networks, which demand complete autonomy, nu-
merous applications now necessitate the use of Unmanned
Aerial Vehicles (UA Vs) in complex and dynamic surround-
ings. In these scenarios, UA Vs must rely solely on their
onboard sensors to comprehend the environment they navigate
through and efficiently accomplish their objectives [1]. UA Vs
are utilized as flying aerial stations to serve ground users,
facilitating data access, extending coverage, and enhancing
communication rates [2]. In contrast to conventional terrestrial
wireless communication infrastructures, UA Vs possess the
capability to adapt their positions dynamically, ensuring they
maintain accurate channel conditions. Cognitive Radio (CR)
technology is a promising solution to the problem of spectrum
scarcity. The basic idea of the overlay CR paradigm is to
efficiently utilize the available radio spectrum by allowing
secondary users (SUs) to access and share the spectrum
opportunistically when the primary users (PUs) are not using
it. Due to its ability to offer superior spectral efficiency
and support huge connectivity, non-orthogonal multiple access
(NOMA) has been chosen as a novel technology to sub-
stantially enhance the throughput of wireless networks. This
approach involves exploiting the power domain to perform
superposition coding at the transmitter and utilizing successive
interference cancellation (SIC) at the receiver to distinguish
signals from multiple users [3]. Hence, the concept of UA V-
enabled Cognitive NOMA is perceived as a strong candidate
to improve the performance of future wireless networks.
Nevertheless, to fully harness the benefits promised by
future networks, a major challenge is how to maximize the
sum rate of mobile users through joint sub-channel and power
allocation. Additionally, the UA Vâ€™s trajectory significantly
influences the channel gains of mobile users. It is worth noting
that optimal joint subchannel and power allocation in NOMA
have been proven to be NP-hard [4]. The existing works
have relied on the UA Vâ€™s perfect knowledge of the positions
of ground users to derive the channel gain explicitly from
a particular radio propagation model, and to solve mobility
problems using traditional optimization techniques or machine
learning (ML) models. In practice, this assumption is often not
realistic, because of their inherent variability and complexity.
While traditional optimization schemes have demonstrated
outstanding performance, they lack inherent adaptability and
typically rely on fixed objectives [2]. On the other hand,
deep learning (DL) requires a large amount of labeled data
for training, which is challenging to generate in a complex
radio environment. Reinforcement learning (RL) algorithms
come with several limitations that can hinder their practical
applicability. They often demand extensive interactions with
the environment to learn effective policies, which can be
impractical in real-world scenarios, requiring significant trial
and error [5]. Also, RL models usually struggle to generalize
knowledge from one environment to another. Learning in a
specific scenario may not easily transfer to new and different
arXiv:2310.11070v1  [eess.SP]  17 Oct 2023
situations [6].
Conversely, active inference, an emerging approach from
theoretical neuroscience, offers a comprehensive brain theory
that unifies action, perception, and inference for adaptive
systems [7]. Biological agents exhibit preferred states (or prior
preferences) that impact their interactions with the external
world and can update under random conditions [8]. Over
the past few years, active inference has been employed in
numerous applications, including decision-making in uncertain
situations, structure learning, and navigation. An extensive
summary of these applications can be found in [9].
In this paper, motivated by [10], we propose a new active
inference-based learning method called Active Generalized
Dynamic Bayesian Network (Active-GDBN) to intelligently
maximize the sum rate of a UA V-based cognitive NOMA
network. The main novel contribution of Active-GDBN, which
sets it apart from traditional optimization techniques and ML
models, lies in its adaptive or real-time belief updating. Active-
GDBN allows an agent to continuously update its beliefs
and actions based on incoming sensory inputs, which aligns
with the way biological systems operate. This formulation is
inspired by the general interpretation of adaptive behaviour,
where the UA V can adjust its perception of the environment
and preferences in response to new information, allowing it to
allocate resources efficiently based on its mobility.
The remaining content in this paper is outlined as follows:
Section II introduces the related work. Section III describes the
system model and problem formulation. The proposed method
for intelligent resource allocation is explained in Section IV.
Section V presents the simulation results and analysis. Lastly,
Section VI concludes the paper.
II. R ELATED WORK
Most research efforts have focused on convex or non-convex
optimization schemes for UA V trajectory design to maximize
wireless network throughput [11]â€“[13]. These efforts have
been directed towards both stationary and flying UA Vs. In the
case of a static UA V scenario, the study [13] concentrated
on optimizing the UA Vâ€™s altitude to achieve the highest
coverage probability for ground users. Meanwhile, in [14],
authors optimized the positions of several UA V base stations
to maximize the throughput of ground users. Conversely, in the
flying UA V setup, a joint design of the UA V-relayâ€™s trajectory
and power allocation was presented in [15] to optimize the
end-to-end throughput.
In addition to optimization-based approaches, in recent
times, reinforcement learning (RL) has been applied to com-
munication networks, especially in UA V networks [16]â€“[18].
Typically, these studies utilize RL to address offline optimiza-
tion tasks, training the UA V to follow a path repeatedly under
stationary conditions. A recent study [2] addressed a more
complex scenario and proposed an enhanced RL algorithm
that incorporates expert knowledge of the wireless channel to
optimize a UA Vâ€™s dynamic maneuver, aiming to maximize the
sum rate of mobile users. However, the study in [2] assumes
specific CSI that might not generalize to scenarios with
time-varying CSI. Different from traditional optimization and
existing studies (see [9]), where active inference is examined
in the context of predefined discrete state spaces, this study
considers a more complex and continuous dynamic scenario
by jointly optimizing subchannels and power allocation, taking
into account UA Vâ€™s mobility.
III. S YSTEM MODEL AND PROBLEM FORMULATION
A. System Model
As shown in Fig. 1, we investigate an uplink NOMA setup
where a mobile UA V provides service to N randomly moving
SUs within a cell area. In practical applications, a single UA V
communication is useful for emergency service recovery and
delay-tolerant tasks like periodic data collection from ground
sensors, which can be sufficient and cost-effective [19]. There
is a primary channel comprising a primary base station (PBS)
that serves PUs. The overall bandwidth is evenly divided
into K orthogonal subchannels, which eliminates interference
between subchannels. The set of SUs and subchannels are
denoted as N = {1, 2, Â·Â·Â· , N} and K = {1, 2, Â·Â·Â· , K},
respectively. At each time step, the positions of the mobile
SUs are randomly updated within the 3D space. Each user
moves independently and randomly in all three dimensions
(x, y, and z) due to the random perturbation applied to their
initial positions. The UA Vâ€™s trajectory follows a straight line
path based on randomly generated and normalized direction.
The UA Vâ€™s position is then updated based on this direction
and the maximum velocity at each time step. We adopted
the third-generation partnership project (3GPP) simulation
standardization, where drones initial movement start from
randomly chosen positions within the network. Subsequently,
they travel at a constant speed and height, moving in straight
lines while following uniformly random directions throughout
the entire simulation period (see [20]).
Secondary Link
Primary Link
UAV
x
y
z
ğğ”ğŸ
ğğ”ğŸ
ğğ”ğ’
ğ’ğ”ğ’
ğ’ğ”ğŸ
ğ’ğ”ğŸ
Y
Superimposed SU Signals
PBS
ğ‘¯
UAV Trajectory
Fig. 1. Illustration of system model with uplink NOMA.
Using NOMA principles, it is possible for multiple users to
be supported simultaneously on each subchannel. Let bk,n(t)
represent the subchannel assignment index in each time slot
(t), where bk,n(t) = 0 denotes a vacant subchannel k that
can be assigned to secondary user (SU) n in time slot t. On
the other hand, if bk,n(t) = 1 it means that the subchannel is
occupied by a PU. We use pk,n(t) to represent the transmit
power of SU n in each time slot t. During the uplink
transmission, each SU n sends its QPSK-modulated signal to
the UA V with a transmit power of pk,n and a channel gain of
gk,n on subchannel k. QPSK modulation is a suitable choice
for NOMA systems as opposed to higher-order modulation
schemes because it can maintain a good level of performance
while minimizing interference from other users. This is impor-
tant because NOMA systems allow multiple users to share the
same resources, which can lead to interference if the signals
from the different users are not properly managed. The UA V is
considered to fly at a constant height of H above ground level,
as mandated by the regulatory authority to ensure safety [2]. To
simplify the system, we assume that both the UA V and all SUs
use a single antenna. We represent the horizontal projection
of the UA V trajectory asq(t) = [x(t), y(t)]T . Hence, the time
changing distance between the UA V and the ground SUs can
be formulated as
d(t) =
p
H2 + âˆ¥q(t)âˆ¥2, 0 â‰¤ t â‰¤ T. (1)
Assume that the initial horizontal position of the UA V is pre-
defined as q(1) = (xi, yi). During each time slot t, the UA V
can alter its position. For the sake of simplicity, we make the
assumption that the communication channel follows a line-of-
sight (LoS) path and utilize the free-space path loss (FSPL)
model similar to [21]. As a result, the channel power gain
from SU n to the UA V in each time slot t is represented as
gk,n(t) = Âµk,n(t)Î¾k,n(t)Î²k,n(t)dk,n(t)âˆ’Î±, (2)
where Î± denotes the path-loss exponent, Âµk,n(t) addresses
the small-scale fading, Î¾k,n(t) indicates the shadowing factor,
Î²k,n(t) represents the reference link power gain. Thus, the
received signal at the UA V in each time slot t over k is
expressed as:
yk(t) =
NX
n=1
bk,n(t)gk,n(t)
q
pk,n(t)dk,n(t) + Î½k(t), (3)
where the first segment of the equation represents the trans-
mitted signals from n SUs on subchannel k and Î½k(t) accounts
for the presence of additive white Gaussian noise. Each SU
must be assigned a distinct QPSK constellation to prevent
signal interference between the signals from various SUs.
This ensures a minimal distance between the constellations
for accurate SIC decoding at the receiver side.
The uplink SIC is executed in a descending order based on
the channel gain. Initially, it decodes the SU with the strongest
signal first, then removes it from the superposed signal, and
subsequently decodes the remaining users with weaker signals.
Therefore, in each time slot, the attainable data rate Rk,n of
SU n on sub-channel k as expressed by Shannon capacity:
Rk,n â‰œ bk,n log2

1 +
pk,ngk,n
P|Uk|
j=Ïƒâˆ’1
k (n)+1 pÏƒk(j)gÏƒk(j) + Î·k,n

.
(4)
The objective is to collectively optimize subchannel selec-
tion bk,n(t), and power allocation pk,n(t) to maximize the total
sum rate in the cell. Mathematically, we express the problem
of maximizing the sum rate as follows:
max
{q(t),bk,n(t),pk,n(t)}
KX
k=1
NX
n=1
Rk,n (5a)
s.t. C1:
KX
k=1
bk,n(t)pk,n(t) â‰¤ pmax, âˆ€k âˆˆ K, nâˆˆ N,
(5b)
C2: pk,n(t) â‰¥ 0, âˆ€k âˆˆ K, nâˆˆ N, (5c)
C3: bk,n(t) âˆˆ {0, 1}, âˆ€k âˆˆ K, nâˆˆ N, (5d)
C4:
NX
n=1
bk,n(t) â‰¤ M, kâˆˆ K, (5e)
C5: q(1) = Ë†qi (5f)
C6:
TX
t=1
âˆ†(t) â‰¤ Tmax (5g)
where constraints C1 and C2 enforce that the transmit power
of each SU n does not exceed the maximum power limit
Pmax and must be non-negative, respectively. C3 implies that
each subchannel can be either allocated or remain unassigned.
Due to SIC decoding complexity, constraint C4 imposes a
maximum of M SUs to be multiplexed per subchannel.
Constraint C5 is the UA Vâ€™s initial position. Constraint C6
ensures that the UA V completes its tasks within a predefined
maximum duration Tmax. This is essential to avoid prolonged
operations that may not be practical or feasible in real-world
scenarios.
Solving the optimization problem presented in equation (5a)
is challenging due to its nonconvexity and NP-hard nature.
Achieving the globally suboptimal solution requires using
either random or iterative search schemes or other optimization
methods. However, our work aims to solve the objective func-
tion by using a conventional optimization method (optimizer)
during the offline stage. The UA V then uses the solutions
provided by this method to learn a dynamic generative model
that represents the wireless environment and the optimizerâ€™s
decision-making processes to solve a set of training examples.
Throughout the online phase, the generative model enables
the UA V to anticipate the future evolution of the wireless
environment, deduce the optimizerâ€™s intended actions, and
rectify actions when it comes across new radio situations
that may deviate from the ones it was trained on. This is
especially critical for intelligent radios (i.e., UA V in our sce-
nario), as traditional optimization methods are unsuitable for
online decision-making, lack online adaptive flexibility, and
require high computational resources. The following sections
explore an active inference-based method to efficiently learn a
representation of the wireless environment where the agentâ€™s
preferences are encoded based on the solutions provided by
the optimizer. This enables a mobile UA V to adapt to new
situations and find an optimal subchannel assignment and
power allocation policy.
IV. P ROPOSED METHOD FOR INTELLIGENT TRAJECTORY
AND RESOURCE ALLOCATION
In this section, we introduce a unique representation of the
optimization problem in (5a) by converting the problem into
abnormality minimization. The framework employs a partially
observable Markov decision process (POMDP) to describe the
set of variables that constitute the optimal solution. This allows
the agent to detect non-stationarity (anomalies) and adapt to
new conditions by obtaining a new suboptimal model, guided
by the principle of free energy minimization.
A. Problem Transformation Based on Active-GDBN
We formalize Active-GDBN within the framework of a
POMDP. At each time step t, the actual state of the en-
vironment ËœSt âˆˆ Rds changes stochastically according to a
transition function ËœSt âˆ¼ Pr(ËœSt|ËœStâˆ’1, A), which is influenced
by the actions A âˆˆ Rda taken by the UA V . Since the actual
state of the environment is typically hidden from the UA V ,
it can only infer through observations ËœZt âˆˆ Rdz , defined as
ËœZt âˆ¼ Pr(ËœZt|ËœSt). Consequently, the UA V relies on its beliefs
about the actual state of the environment ËœSt. Also, applying the
Bayesian principle given a prior Pr(ËœXt|ËœZtâˆ’1) and likelihood
Pr(ËœZt|ËœXt), the posterior Pr(ËœXt|ËœZt) can be obtained as follows
[22]:
Pr(ËœXt|ËœZt) = P(ËœZt|ËœXt)Pr(ËœXt|ËœZtâˆ’1)
Pr(ËœZt|ËœZtâˆ’1)
. (6)
As indicated in (6), the posterior Pr(ËœXt|ËœZt), is characterized
by three primary elements: the prior Pr(ËœXt|ËœZtâˆ’1), which
represents the prior knowledge of the UA V; the likelihood
Pr(ËœZt|ËœXt), which represents the probability of the UA V
observing the evidence or data given a hidden state; the
observation Pr(ËœZt|ËœZtâˆ’1), which denotes the probability of
the UA V observing the data across all possible values of the
hidden states.
In the Active-GDBN framework, the interaction between
UA V and the environment can be described as a 6-element
tuple. ( ËœSt, ËœXt, A, Tpu
Ï„ , Î a
Ï„ , ËœZt), where ËœSt and ËœXt are sets
of environmental hidden states that include the SU positions,
discrete subchannels, and the relative positions between the
UA V and each SU. A = {A[f], A[p], A[u]} is the action
space containing all the possible sub-channel decisions bk,n(t),
power allocation pk,n(t), and the possible UA Vâ€™s trajectories
q(t). Tpu
Ï„ represents the dynamic transition model of PUs
over time. Î a
Ï„ denotes the active inference table capturing
state-action pairs, and ËœZt comprises a sequence of K sensory
signals.
1) Radio Environment Representation: The
UA V can perceive K sensory signals denoted as:
ËœZt={ËœZt,1, ËœZt,2, . . . ,ËœZt,K}, corresponding to K sub-channels.
Furthermore, we use a generalized hierarchical state-space
model to characterize the radio environment with the
following constituents:
ËœS(e)
t,k = f(ËœS(e)
tâˆ’1,k) + wt,k, (7)
ËœX(e)
t,k = CËœX(e)
tâˆ’1,k + DUËœS(e)
t,k
+ wt,k, (8)
ËœZt,k = H
 ËœX(1)
t,k + Â·Â·Â· + ËœX(M)
t,k + ËœX(pu)
t,k

+ vt,k. (9)
In equation (7), ËœS(e)
t,k represents the discrete random variables
signifying discrete state clusters of the physical signal, the sub-
channel transmitting the signal, and its power level. Similarly,
f(.) denotes a nonlinear function that illustrates the evolution
of ËœS(e)
t,k over time based on ËœS(e)
tâˆ’1,k, and wt,k represents the
noise, given as wt,kâˆ¼N(0, Î£wt,k ). Equation (8) is the dynamic
model equation, which describes the temporal evolution of the
Generalized States (GS) ËœX(e)
t,k influenced by both ËœX(e)
tâˆ’1,k and
ËœS(e)
t,k where e âˆˆ {no, pu, c}, no, pu, and c represent noise,
PU and the superimposed NOMA signals, respectively. C and
D are matrices that represent the dynamic and control rules,
respectively, while UËœS(e)
t,k
is the control vector. Equation (9) is
the observation model (sensory signals), which depends on the
GS. Fig. 2 displays the proposed graphical models representing
the Active-GDBN framework.
The process includes an offline phase of perceptual learning,
during which we equip the UA V with an interactive coupled-
state switching GDBN at discrete and continuous levels, as
shown in Fig. 2-a. This is done to effectively capture the tem-
poral dependencies between the UA V trajectory and the SUsâ€™
mobility. Coupled state-switching models [23] are designed
to address multiple observation sequences, where underlying
state variables interact with each other. In this context, a hidden
discrete state ËœS(1)
t,k is influenced by its own previous state,
ËœS(1)
tâˆ’1,k, as well as the previous state of the other hidden chain,
ËœS(2)
tâˆ’1,k. In a similar manner, a continuous hidden state ËœX(1)
t,k is
influenced by its own previous state, ËœX(1)
tâˆ’1,k, and the previous
state of the other hidden chain, ËœX(2)
tâˆ’1,k. Fig. 2-b is the online
active inference stage where the UA V needs to make a joint
decision for all three actions, by finding the best combination
of subchannel assignment bk,n(t), power levels pk,n(t), and
UA V trajectoryq(t) to maximize the sum rate.
2) Offline Perceptual Learning: At the initial learning
phase, the UA V starts with an initial model identical to the
Unmotivated Kalman Filter (UKF), assuming static environ-
mental state evolution. The UA Vâ€™s memory generates initial
generalized errors (GEs), which are then used for incremental
learning of new models. The goal of the offline perception
stage is to equip the UA V with the ability to learn different
vocabularies representing the noise model, PU model, and the
superimposed SUsâ€™ model. Note that the superimposed SUsâ€™
signal is generated using a matrix that encodes the relation-
ships between all possible subchannel selection indices, power
allocations, and all the possible distances between the UA V
trajectory and user positions.
We consider I distinct observations {ËœZ(i)
t }T
t=1, each de-
pending on hidden environment state sequence at discrete
and continuous states {ËœS(i)
t }T
t=1, {ËœX(i)
t }T
t=1, i = 1 , . . . , I.
à·©ğ—ğ’•,ğ’Œ
ğ’Šà·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğ’Š
à·¨ğ’ğ’•,ğ’Œ
ğ’Šà·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğ’Š
à·¨ğ™ğ’•âˆ’ğŸ
ğ’Š à·¨ğ™ğ’•
ğ’Š
ğœ‹ à·¨St,ğ‘˜
Pr à·©Xt,k á‰šà·¨St,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜ ğœ† à·©Xt,ğ‘˜
ğœ† à·¨St,ğ‘˜
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
Discrete 
level
Continuous 
level
Continuous 
level
à·©ğ—ğ’•,ğ’Œ
ğŸ
à·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·¨ğ’ğ’•,ğ’Œ
ğŸà·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·¨ğ™ğ’•âˆ’ğŸ
ğŸ à·¨ğ™ğ’•
ğŸ
ğœ‹ à·¨St,ğ‘˜
Pr à·©Xt,k á‰šà·¨St,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜ ğœ† à·©Xt,ğ‘˜
ğœ† à·¨St,ğ‘˜
â‹¯
â‹¯ à·©ğ—ğ’•,ğ’Œ
ğŸ
à·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·¨ğ’ğ’•,ğ’Œ
ğŸà·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·¨ğ™ğ’•âˆ’ğŸ
ğŸ à·¨ğ™ğ’•
ğŸ
ğœ‹ à·¨St,ğ‘˜
Pr à·©Xt,k á‰šà·¨St,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜
ğœ‹ à·©Xt,ğ‘˜ ğœ† à·©Xt,ğ‘˜
ğœ† à·¨St,ğ‘˜
â‹¯
â‹¯
UA Vâ€™s mobility SUsâ€™ mobility
(a)
ğœ‹ ğ’ğ‘¡âˆ’1,ğ‘˜
1
ğœ† Sğ‘¡,ğ‘˜
ğ‘€ğœ† Sğ‘¡âˆ’1,ğ‘˜
ğ‘€
à·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
à·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğŸà·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğŸ
ğ‘¨ğ’•âˆ’ğŸ
ğ© ,[ğ’–]
ğ‘¨ğ’•âˆ’ğŸ
[ğŸ]
à·©ğ—ğ’•âˆ’ğŸ,ğ’Œ
ğ‘´
à·¨ğ’ğ’•âˆ’ğŸ,ğ’Œ
ğ‘´
à·¨ğ™tâˆ’1
à·©ğ—ğ’•,ğ’Œ
ğŸ
à·©ğ—ğ’•,ğ’Œ
ğŸ
à·¨ğ’ğ’•,ğ’Œ
ğŸà·¨ğ’ğ’•,ğ’Œ
ğŸ
ğ‘¨ğ’•âˆ’ğŸ
ğ© ,[ğ’–]
ğ‘¨ğ’•âˆ’ğŸ
[ğŸ]
à·©ğ—ğ’•,ğ’Œ
ğ‘´
à·¨ğ’ğ’•,ğ’Œ
ğ‘´
ğœ‹ ğ’ğ‘¡,ğ‘˜
1
à·¨ğ™t
(b)
Fig. 2. The proposed graphical representations of Active-GDBN: (a) Of-
fline Perception (Coupled-GDBN), (b) Online Active Inference (Active-
GDBN).The graph is characterized by a hierarchical structure that represents
sequences of hidden states (discrete and continuous) over time. This allows
the UA V to track the evolution of messages at discrete and continuous states,
which it uses to estimate the posterior distribution over hidden states. The blue
arrows represent messages from prior states, while the red arrows signify
messages from future states. Fundamentally, the previous and future states
are constantly represented, and the UA V is able to update its beliefs based on
incoming new data. As displayed in sub-figure 2-b in green arrows, the current
states ËœSt,k, ËœXt,k at time t on sub-channel k determines the current observation
ËœZt, which is influenced by previous states ËœStâˆ’1,k, ËœXtâˆ’1,k and previous joint
actions( A[f]
tâˆ’1 for subchannel assignment), ( A[p,u]
tâˆ’1 for continuous power
allocation and UA V trajectory) .
The hidden states variables interact at discrete and continuous
states. The transition probabilities relating to the discrete and
continuous state vectors are given by:
Pr(ËœSt|ËœStâˆ’1) = Pr(ËœS(1)
t , . . . ,ËœS(I)
t |ËœS(1)
tâˆ’1, . . . ,ËœS(I)
tâˆ’1) (10)
Pr(ËœXt|ËœXtâˆ’1) = Pr(ËœX(1)
t , . . . ,ËœX(I)
t |ËœX(1)
tâˆ’1, . . . ,ËœX(I)
tâˆ’1) (11)
In this case (I = 2), the UA Vâ€™s mobility and the SUsâ€™ mobility
in each time step.
We applied the unsupervised clustering algorithm called
Growing Neural Gas (GNG) [24] to train the coupled GDBN
model. The GNG algorithm is a self-organizing neural network
that can adaptively learn the structure of the input data through
cooperative and incremental learning. This model takes in the
GEs and produces clusters of discrete states and a set of
continuous generalized states.
3) Active Inference Stage: The UA Vâ€™s trajectory is divided
into multiple time slots, which allows it to adapt and optimize
its actions in response to changing conditions over time. The
decision-making process of the UA V relies on the state-action
pair encoded in a time-varying matrix Î [f]
Ï„ , which encodes
the probabilistic dependencies between states and discrete
actions (subchannel assignment), while Î [p]
Ï„ and Î [u]
Ï„ are
time-varying matrices encoding the probabilistic dependencies
between states and continuous actions, power levels, and UA V
trajectories, respectively.
a) Action selection: At the beginning of each time slot,
the UA V enters a specific state defined by its current position
and channel conditions. By leveraging the learned vocabularies
during the offline perception stage, the UA V observes the PU
activities and the positions of SUs. Initially, the UA V employs
random sampling to select joint actions, with each possible
action having an equal chance of being selected. The UA V
selects the initial joint actions for the current time slot, which
include subchannel assignment and power allocation based
on the UA Vâ€™s current state. In the subsequent iterations, the
environment responds to the UA Vâ€™s actions by generating new
observations, updating the environmental states, and altering
the positions of SUs and the UA V . As the UA V interacts with
the environment it learns from the outcomes of its actions. By
iteratively updating its generative model and action policies,
the UA V adapts its decision-making strategies to improve its
performance over time. Concurrently, the UA V can predict the
future behavior of PUs using Tpu
Ï„ and anticipate resources that
are likely to be occupied by PUs.
b) Perception and joint State-Prediction: After select-
ing the joint actions, which include the UA Vâ€™s trajectory,
subchannel assignments, and power allocation, the UA V can
correctly estimate and predict the effect of its actions by
using a modified Markov Jump Particle Filter (M-MJPF) [25].
The M-MJPF incorporates a switching model and employs
particle filtering for discrete state prediction and updating,
along with Kalman filtering for continuous state prediction
and updating. time-dependent inter-slice top-down predictive
messages Ï€(ËœXt,k) and Ï€(ËœSt,k) relies on information learned
from the dynamic model. Conversely, the intra-slice bottom-up
inference is established in the likelihood function and involves
the transmission of backward-propagated messages Î»(ËœXt,k)
and Î»(ËœSt,k) moving to the discrete level. The discrete level
influences the prediction at the continuous level. For every
particle propagated within the discrete level, a KF is activated
to estimate the corresponding continuous level ËœXt,k. The PF
generates L particles with equal weighting, guided by the
proposal density coded in the transition matrix Î k.
c) Abnormality measurements and action evaluation:
The Bhattacharyya distance is employed in the measurement
of continuous state abnormalities to compute the difference
between two messages that reach node ËœXt,k, given by, Ï€(ËœXt,k)
and Î»(ËœXt,k). This is done to evaluate how well the obser-
vations match the predictions made by the model, as shown
below:
Î¥ËœXt,k
= âˆ’ln

BC
 
Ï€(ËœXt,k), Î»(ËœXt,k)

=
Z q
Ï€(ËœXt,k)Î»(ËœXt,k)dËœXt,k,
(12)
where BC denotes the Bhattacharyya coefficient. Bhat-
tacharyya distance measures the divergence between two prob-
ability distributions. A larger distance signifies a higher dif-
ference between observations and model predictions, resulting
in higher abnormalities. On the other hand, a smaller distance
indicates a closer match between observations and predictions,
suggesting a lower abnormality or an accurate prediction.
d) Update of the action selection and incremental model:
Action selection: In exploitation, given the UA Vâ€™s present
model and present observation, the UA V selects actions that
minimize available model free energy. During exploration, the
UA V selects actions to minimize the expected free energy,
which is the new model resulting from the chosen action
Model incremental update: The UA V updates the POMDP
for the new actions that it has learned.
V. S IMULATION RESULTS
In this section, we provide simulation results to validate the
effectiveness of the proposed Active-GDBN.
Data Generation : Random initial positions for multiple
SUs are generated within 0 and 1 for each user and their spatial
3D (x, y, and z). The user positions are updated at each time
step and stored in a user mobility matrix. The UA Vâ€™s initial
position and direction are randomly generated. A loop iterates
through time steps, updating the UA Vâ€™s position based on its
speed and direction. The final position is computed based on
its trajectory after the specified time steps and stored in a
UA V trajectory matrix. At every time step, power allocations
are randomly generated within the specified maximum power
(Pmax). These are stored in a power allocation matrix, where
each row represents a user and each column represents a
time step. The subchannel assignment matrix is populated
with random integers within the range 1 to 6 for all possible
subchannels. We then generate a distance matrix encoding all
possible distances between the UA V trajectory and user posi-
tions. Table I presents an overview of the network parameters,
while for illustrative purposes, Figs. 3-a and 3-b depict the
simulation setup.
TABLE I
SIMULATION PARAMETERS
Cell radius 100 m
UA V flight height(H) 50 m
UA V speed 5 m/s
Number of UA V time steps 10
Modulation scheme of PUs BPSK
Modulation scheme of SUs QPSK
Path loss model Free-space-path-loss [11]
Noise power âˆ’174dBm/Hz
System Bandwidth,Bw 1.4 MHz
Number of sub-channelsK 6
Power budget of SUsPmax 20 W [21]
Maximum number of SUs per sub-channelM M= [1,2,3,4,5,6],
Power difference thresholdPth 1
Learning rate of GNG clustering 0.01
To assess the effectiveness of the proposed Active-GDBN,
we benchmark with suboptimal baselines such as q-learning
[2], convex [11], and random schemes. As evident from Fig. 4,
the proposed approach performs better than the benchmark
schemes by achieving an improved and consistent sum rate
in fewer episodes. This is because Active-GDBN inherently
balances exploration and exploitation by seeking to minimize
abnormalities or prediction errors in each episode to update
0
200
50
100
100
100
150
50
200
00
-50
-100 -100
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
Subchannels
UAV Trajectory
SU 1
SU 2
SU 3
SU 4
SU 5
Initial UAV Position
Final UAV Position
(a) UA V trajectory, SU mobility, and subchannels.
Power Allocations
1 2 3 4 5 6 7 8 9 10
Time Step
0
2
4
6
8
10
12
14
16
18
20Power Allocation (W)
SU 1
SU 2
SU 3
SU 4
SU 5
(b) Power allocation in each time step .
Fig. 3. Illustration of the simulation environment with M = 5, H = 50m,
Pmax = 20Watts.
its beliefs. The UA V continuously updates its internal model
online to align with new sensory inputs. On the other hand, Q-
learning necessitates manual adjustment of exploration strate-
gies and extensive trial and error to achieve enhanced sum
rate performance. The low sum rate performance of convex
and random schemes is due to a lack of inherent adaptability
and online learning to adapt to time-varying environments.
0 20 40 60 80 100 120 140 160 180 200
Episodes
0
100
200
300
400Cumulative Sum Rate (bps/Hz)
 Active-GDBN
Q-learning
Convex scheme
Random scheme
Fig. 4. Cumulative sum rate comparison of the proposed Active-GDBN with
benchmark schemes when M = 2, H = 50m, Pmax = 20Watts.
Fig. 5 demonstrates the convergence performance of Active-
GDBN using a varying number of multiplexed SUs per
subchannel ( M). When the value of M is 1, the situation
becomes orthogonal multiple access (OMA). We observed
that stable convergence is achieved within a range of zero
to forty episodes. As anticipated, the sum rate for each
SU increases monotonically, and the proposed Active-GDBN
achieved a maximum of 5 SUs per subchannel. However, this
monotonic behavior is not guaranteed when M = 4 . This
deviation could be attributed to the presence of reduced LoS
conditions, arising from the UA Vâ€™s random trajectory. This
randomness introduces unexpected interference for a specific
M configuration, resulting in a drop in the cumulative sum
rate. Similarly, the algorithm exhibits performance degradation
as we increase the value of M to 6. This is because, at a certain
limit, due to power control, simultaneous transmission from
superimposed SUs begins to overlap, leading to a decrease in
the sum rate.
0 20 40 60 80 100 120 140 160 180 200
Episodes
0
100
200
300
400
500Cumulative Sum Rate (bps/Hz)
OMA, M = 1
Active-GDBN, M=2
Active-GDBN, M=3
Active-GDBN, M=4
Active-GDBN, M=5
Active-GDBN, M=6
Fig. 5. Convergence of Active-GDBN with varying numbers of multiplexed
SUs M, H = 50m, Pmax = 20Watts.
The GNG learning rate determines the speed at which the
UA V adjusts to the sensory data. As clearly indicated in Fig. 6,
we fix the learning rate at 0.01 for all simulation settings to
attain a faster and enhanced sum rate.
0 20 40 60 80 100 120 140 160 180 200
Episodes
100
150
200
250
300
350
400Cumulative Sum Rate (bps/Hz)
Learning rate = 0.01
Learning rate = 0.05
Learning rate = 0.1
Fig. 6. Effect of the GNG learning rate on convergence during offline
perception training when M = 3, H = 50m, Pmax = 20Watts.
VI. C ONCLUSION
In this study, we present a novel framework based on
active inference for maximizing the cumulative sum rate
of a UA V-based cognitive NOMA network. By leveraging
active inference, we introduce a more robust approach that
can handle the complexities of dynamic wireless networks.
The UA V constantly updates a generative model online to
optimize subchannel assignment, power allocation, and UA V
trajectories. Simulation results have shown that the proposed
Active-GDBN can achieve an improved cumulative sum rate
compared to suboptimal baseline techniques. Future research
will explore the effects of incorporating multiple UA Vs, nu-
merous ground users, and conducting real-world experiments.
REFERENCES
[1] T. Elmokadem and A. V . Savkin, â€œTowards fully autonomous uavs: A
survey,â€ Sensors, vol. 21, no. 18, p. 6223, 2021.
[2] Y . Huang, X. Mo, J. Xu, L. Qiu, and Y . Zeng, â€œOnline maneuver design
for uav-enabled noma systems via reinforcement learning,â€ in2020 IEEE
Wireless Communications and Networking Conference (WCNC) , 2020,
pp. 1â€“6.
[3] C. He, Y . Hu, Y . Chen, and B. Zeng, â€œJoint power allocation and channel
assignment for noma with deep reinforcement learning,â€ IEEE Journal
on Selected Areas in Communications , vol. 37, no. 10, pp. 2200â€“2210,
2019.
[4] L. Sala Â¨un, C. S. Chen, and M. Coupechoux, â€œOptimal joint subcarrier
and power allocation in noma is strongly np-hard,â€ in 2018 IEEE
International Conference on Communications (ICC) . IEEE, 2018, pp.
1â€“7.
[5] A. Kurenkov, â€œReinforcement learningâ€™s foundational flaw,â€ The Gradi-
ent, 2018.
[6] O. C Â¸ atal, S. Wauthier, C. De Boom, T. Verbelen, and B. Dhoedt,
â€œLearning generative state space models for active inference,â€ Frontiers
in Computational Neuroscience , vol. 14, p. 574372, 2020.
[7] K. J. Friston, T. Parr, and B. de Vries, â€œThe graphical brain: Belief
propagation and active inference,â€ Network neuroscience, vol. 1, no. 4,
pp. 381â€“414, 2017.
[8] N. Sajid, P. Tigas, and K. Friston, â€œActive inference, preference learning
and adaptive behaviour,â€ in IOP Conference Series: Materials Science
and Engineering, vol. 1261, no. 1. IOP Publishing, 2022, p. 012020.
[9] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V . Neacsu, and K. Friston,
â€œActive inference on discrete state-spaces: A synthesis,â€ Journal of
Mathematical Psychology, vol. 99, p. 102447, 2020.
[10] A. Krayani, A. S. Alam, L. Marcenaro, A. Nallanathan, and C. Regaz-
zoni, â€œA Novel Resource Allocation for Anti-Jamming in Cognitive-
UA Vs: An Active Inference Approach,â€ IEEE Communications Letters ,
vol. 26, no. 10, pp. 2272â€“2276, Oct 2022.
[11] Q. Wu, Y . Zeng, and R. Zhang, â€œJoint trajectory and communication
design for multi-uav enabled wireless networks,â€ IEEE Transactions on
Wireless Communications, vol. 17, no. 3, pp. 2109â€“2121, 2018.
[12] P. Li and J. Xu, â€œFundamental rate limits of uav-enabled multiple access
channel with trajectory optimization,â€ IEEE Transactions on Wireless
Communications, vol. 19, no. 1, pp. 458â€“474, 2019.
[13] A. Al-Hourani, S. Kandeepan, and S. Lardner, â€œOptimal lap altitude for
maximum coverage,â€ IEEE Wireless Communications Letters , vol. 3,
no. 6, pp. 569â€“572, 2014.
[14] J. Lyu, Y . Zeng, R. Zhang, and T. J. Lim, â€œPlacement optimization
of uav-mounted mobile base stations,â€ IEEE Communications Letters ,
vol. 21, no. 3, pp. 604â€“607, 2016.
[15] Y . Zeng, R. Zhang, and T. J. Lim, â€œThroughput maximization for uav-
enabled mobile relaying systems,â€ IEEE Transactions on communica-
tions, vol. 64, no. 12, pp. 4983â€“4996, 2016.
[16] H. Bayerlein, P. De Kerret, and D. Gesbert, â€œTrajectory optimization
for autonomous flying base station via reinforcement learning,â€ in 2018
IEEE 19th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC). IEEE, 2018, pp. 1â€“5.
[17] U. Challita, W. Saad, and C. Bettstetter, â€œInterference management for
cellular-connected uavs: A deep reinforcement learning approach,â€ IEEE
Transactions on Wireless Communications , vol. 18, no. 4, pp. 2125â€“
2140, 2019.
[18] Y . Zeng and X. Xu, â€œPath design for cellular-connected uav with rein-
forcement learning,â€ in 2019 IEEE Global Communications Conference
(GLOBECOM). IEEE, 2019, pp. 1â€“6.
[19] Y . Zeng and R. Zhang, â€œEnergy-efficient uav communication with tra-
jectory optimization,â€ IEEE Transactions on wireless communications ,
vol. 16, no. 6, pp. 3747â€“3760, 2017.
[20] M. Banagar and H. S. Dhillon, â€œ3gpp-inspired stochastic geometry-based
mobility model for a drone cellular network,â€ in 2019 IEEE Global
Communications Conference (GLOBECOM) . IEEE, 2019, pp. 1â€“6.
[21] Q. Wu, Y . Zeng, and R. Zhang, â€œJoint trajectory and communication
design for multi-uav enabled wireless networks,â€ IEEE Transactions on
Wireless Communications, vol. 17, no. 3, pp. 2109â€“2121, 2018.
[22] Z. Chen et al., â€œBayesian filtering: From kalman filters to particle filters,
and beyond,â€ Statistics, vol. 182, no. 1, pp. 1â€“69, 2003.
[23] J. Pohle, R. Langrock, M. v. d. Schaar, R. King, and F. H. Jensen, â€œA
primer on coupled state-switching models for multiple interacting time
series,â€ Statistical Modelling, vol. 21, no. 3, pp. 264â€“285, 2021.
[24] I. J. Sledge and J. M. Keller, â€œGrowing neural gas for temporal clus-
tering,â€ in 2008 19th International Conference on Pattern Recognition ,
2008, pp. 1â€“4.
[25] A. Krayani, M. Baydoun, L. Marcenaro, A. S. Alam, and C. Regaz-
zoni, â€œSelf-learning bayesian generative models for jammer detection
in cognitive-uav-radios,â€ in GLOBECOM 2020 - 2020 IEEE Global
Communications Conference, 2020, pp. 1â€“7.