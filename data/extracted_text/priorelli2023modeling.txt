Modeling motor control in continuous-time
Active Inference: a survey
Matteo Priorelli1,†, Federico Maggiore2,3,†, Antonella Maselli2,
Francesco Donnarumma2, Domenico Maisto2, Francesco
Mannella2, Ivilin Peev Stoianov1, and Giovanni Pezzulo∗2
1ISTC-CNR, National Research Council, Padua, Italy
2ISTC-CNR, National Research Council, Rome, Italy
3Department of Engineering, Roma Tre University, Rome, Italy
†These two authors contributed equally
Abstract
The way the brain selects and controls actions is still widely debated.
Mainstream approaches based on Optimal Control focus on stimulus-
response mappings that optimize cost functions. Ideomotor theory and
cybernetics propose a different perspective: they suggest that actions are
selected and controlled by activating action effects and by continuously
matching internal predictions with sensations. Active Inference offers a
modern formulation of these ideas, in terms of inferential mechanisms and
prediction-error-based control, which can be linked to neural mechanisms
of living organisms. This article provides a technical illustration of Active
Inference models in continuous time and a brief survey of Active Inference
models that solve four kinds of control problems; namely, the control of
goal-directed reaching movements, active sensing, the resolution of multi-
sensory conflict during movement and the integration of decision-making
and motor control. Crucially, in Active Inference, all these different facets
of motor control emerge from the same optimization process - namely,
the minimization of Free Energy - and do not require designing separate
cost functions. Therefore, Active Inference provides a unitary perspec-
tive on various aspects of motor control that can inform both the study
of biological control mechanisms and the design of artificial and robotic
systems.
Keywords— Active Inference; motor control; active sensing; predictive brain;
ideomotor theory; cybernetics
∗Corresponding author: Giovanni Pezzulo, ISTC-CNR Via S. Martino della Battaglia 44,
00185 Rome, Italy, giovanni.pezzulo@istc.cnr.it
1
arXiv:2310.05144v1  [q-bio.NC]  8 Oct 2023
1 Introduction
A central question in computational motor control is how the brain selects and controls
actions. A common assumption in formal frameworks such as Optimal Control [1, 2]
and reinforcement learning [3] is that the building blocks of action control are stimulus-
response mappings, or policies. Policies can be selected using either cheap-but-rigid,
habitual mechanisms (i.e., based on the history of previous reinforcements), or costly-
but-flexible, deliberative mechanisms (i.e., based on the value of action outcomes) [4].
Then, when a policy is selected, its execution follows stimulus-response control rules
and could be accompanied by a process of prediction of action effects (via a so-called
forward model) that helps manage delayed feedback [5].
An alternative to this stimulus-response perspective is the ideomotor view that
movements are selected and controlled on the basis of their effects (or outcomes), not
of stimuli [6–10]. The general idea of the ideomotor theory is that the brain might
learn the statistical (bidirectional) relations between actions and their effects and then
use the learned action-effect codes both to predict action consequences (in the forward
direction, from actions to effects) and to select and plan actions that achieve the in-
tended effects (in the backward direction, from effects to actions). Stimuli could be
part of this picture, leading to stimulus-action-effect codes, but they would not be the
main responsible for action planning, selection, and control. Various empirical findings
support the claim of ideomotor theory that action effects influence the selection and
control of actions. For example, one study required participants to press one of four
horizontally arranged buttons in response to color stimuli [11]. After each keypress,
an effect stimulus was shown in one of four horizontally arranged locations. Crucially,
responses to color stimuli were faster when there was a correspondence between the lo-
cations of the effect stimulus and the pressed button. This occurred despite color stim-
uli appeared before effect stimuli, indicating that anticipated action effects influenced
actions. This influence would not be present if action selection was stimulus-response,
because effects only occur after actions are completed. Other subsequent studies sug-
gest that the influence of effects over actions regards various processes, such as action
planning, selection, preparation, initiation, and control [12–15] and they could already
be present in infancy [16].
Another framework that highlights the centrality of action effects (or outcomes)
rather than stimulus-response codes is cybernetics [17]. For example, the early TOTE
(test, operate, test, exit) model assumes that the brain continuously tests whether
there is a mismatch between an internally defined event (roughly, a goal, outcome,
or setpoint) and the currently sensed event; and in case it detects a mismatch, it
triggers a corrective action that reduces it [18]. A simple illustration of this “closed-
loop” control mechanism is the functioning of a thermostat: a discrepancy between
a desired temperature or set point (say, 37 degrees) and a sensed temperature (say,
35 degrees) triggers an action (say, switch a heating device on) until there is no more
mismatch. While the TOTE scheme did not natively include sophisticated planning
or control mechanisms, it exemplifies a mechanism by which it is an internal matching
operation, not a stimulus, which triggers actions. Similarly, in perceptual control
theory, the central goal of a system is to continuously monitor that some internally
represented perceptual variable has the desired value (e.g., that the number indicated
by a speedometer in a car is 80 km/h) and if not, trigger corrective actions (accelerate
or decelerate) that cancel out discrepancies from the desired value [19,20].
The two above (ideomotor and cybernetic) schemes require the brain to internally
represent action effects and other expected (or intended) events and to continuously
2
perform matching operations to calculate discrepancies between expected and sensed
events. Recently, these internal matching operations and discrepancies have become
the focus of a large set of theoretical and empirical studies [21–25], which are typically
conceptualized in terms of predictive coding and Active Inference theories [23]. These
theories assume that the brain maintains a statistical model of the regularities of the
environment and uses it to continuously generate predictions about present and future
events, including action effects. Crucially, the brain models can include some preferred
states (e.g., desired values of physiological parameters or setpoints for motor control)
that regulate the control of externally directed actions such as overt movements [26]
and internal regulatory actions [27,28]. This is because the brain continuously predicts
the desired values (e.g., of body posture or temperature) and monitors discrepancies
with sensed stimuli. Any discrepancy is registered as a prediction error that triggers a
corrective action that minimizes the error (or alternatively, depending on the context,
leads to model revision and learning) that ultimately ensures that the system remains
within its preferred states.
In keeping with ideomotor and cybernetics accounts of motor behavior, Active In-
ference formalizes the problem of motor control by assuming that agents act on the
surrounding environment in a goal-directed manner, to achieve a desired state. Ac-
tive Inference agents monitor the state of the system (which may include the external
environment and their own bodily configuration) through the senses (i.e., perception)
and continuously predict how the state of the system will evolve in time. This predic-
tive processing is granted by an internal representation of the system dynamics, which
is assumed to be learned through exposure to the statistical regularities that govern
the environment and the body (the laws of physics, kinematic regularities, etc.), both
during the lifespan and throughout evolution. Furthermore, Active Inference agents
continuously formulate sensory predictions (e.g., about action outcomes) and com-
pare them with sensory events gathered through the senses. The resulting sensory
prediction errors are considered within the state estimation process, which strives to
minimize the errors. To minimize the prediction errors, the (brain of the) agent has
two ways. First, it can change the model that generated the predictions in the first
place: this amounts to a process of belief revision and learning in theories of predictive
coding, DEM and generalized filtering [29,30], [31,32]. Second, the agent can minimize
prediction errors by acting upon the system and changing its state, in such a way that
the system-produced events (registered as sensory events by the agent) become more
similar to its sensory predictions. This second way to minimize prediction errors –
by acting – is key to Active Inference and permits formalizing early ideomotor and
cybernetic ideas, by linking them to the biologically plausible scheme of predictive
coding [23].
While several surveys and tutorials can be found in the literature about Active
Inference in discrete state spaces [33, 34], the continuous framework has received rel-
atively less attention. Discrete models are critical to perform planning and decision-
making, but handling continuous signals is key when interacting with the external
environment. Given the growing interest in predictive processing, here we provide an
overview of how motor control is modeled.
In the rest of this article, we provide a short formal introduction to Active Infer-
ence; we discuss various examples of Active Inference models of motor control; finally,
we discuss the unique features of this framework.
3
Figure 1: Generative process (in blue) and generative model (in red) in Active inference.
The generative process includes the dynamics fGP (x, a) that governs the temporal evolution
of the hidden states x, and the sensory mapping gGP (x) that maps the hidden states into
the sensory input y that the agent receives from the environment, along with sensory noise
w. The generative model includes an internal representation f about the system dynamics
generated from a belief µ over the hidden states, and an internal model g that maps this
belief into sensory predictions. The latter are used to compute prediction errors εy which
contribute to perceptual inference and the generation of actions a via the corresponding Free
Energy minimization terms. Eventually, actions affect the generative process by entering the
system dynamics. Note that f does not include an explicit representation of the actions and
is instead driven by internal representations of intentions encoded into the hidden causes ν.
2 Active Inference in continuous time
Active Inference has been used to model a large variety of problems of motor con-
trol, decision-making, planning and rule learning that are relevant for both biological
organisms and robots [26, 35–38]. This section provides a concise formal introduc-
tion to Active Inference in continuous time; a more detailed treatment comprising
discrete-time formulations can be found in [23].
Active Inference is built upon the Free Energy Principle (FEP), which assumes that
all living organisms strive to minimize the “surprise associated with sensory exchanges
with the world”, allowing them to resist a natural tendency to disorder [23]. The
Variational Free Energy (VFE) – or Free Energy, for brevity – F is introduced as a
mathematically treatable upper bound on surprise; it is a functional that is widely
used in statistics as part of Variational Bayes methods [39], and it is analogous to
the evidence lower bound (ELBO) used in machine learning. Active Inference appeals
to the minimization of Free Energy to model the action-perception loop of living
organisms and assumes that both action and perception minimize the same (Free
Energy) quantity, as will become clear later.
Any implementation of an Active Inference agent requires specifying two inter-
acting systems, as shown in Fig. 1. The first one is the “generative process”, which
describes how the physical system which the agent interacts with (e.g., the environ-
ment and/or the agent’s body) evolves in time, and how it maps into the sensory inputs
observed by the agent. The second one is the “generative model”, which describes the
internal model that the agent holds about how the system is expected to evolve in
time and to map into sensory states. As illustrated in Fig. 1, the two systems interact
bidirectionally: the generative process determines the sensory inputs that the agent
receives and processes (e.g., to compute prediction errors), while the generative model
produces actions that influence the dynamics of the generative process.
4
The action-perception loop of Active Inference can be summarized as follows. Con-
sider an agent immersed in a dynamic environment and receiving observations y gen-
erated from hidden variables u, which generally consist of hidden states x and hidden
causes v (but they could also include other variables, like parameters evolving on dif-
ferent time scales) [40]. The variational method approximates the intractable posterior
P(u|y) = P(u, y)/P(y) through the definition of an auxiliary, approximate posterior
distribution Q(u), sometimes called recognition density [39, 41]. The approximation
is achieved by minimizing the Kullback-Leibler (KL) divergence between these two
distributions. However, since this quantity still depends on the intractable marginal
P(y), it is replaced by the (formally equivalent) minimization of VFE F. The latter
provides an upper bound on log evidence (or surprise) and its minimization is tractable
because it depends on two quantities that the agent knows or has inferred: the joint
probability P(u, y) and the approximate posterior Q(u).
Since the optimization of F for an arbitrary Q(u) is often complex, it is common
to make some additional (biologically plausible) assumptions. A standard assumption
made in Active Inference is the Laplace approximation [42], which implies that the
approximate posterior is a multivariate Gaussian distribution, i.e., in the simple case
of u = {x, v}, Q(u) = N({µ, ν}, Π−1), where µ is the best guess or belief about
the hidden states, ν is the belief about the hidden causes, and Π is their precision
or inverse covariance matrix. A second common assumption used to simplify the
recognition density is the mean-field approximation [40], which renders some variables
of the model conditionally independent (for example, hidden state variables and other
model parameters that we did not consider here). Given the above assumptions, it is
possible to transform the functional F into a function and evaluate it, up to constant
quantities that do not affect the minimization process:
F ≈ −ln P(u, y)|x=µ,v=ν (1)
Hence, under the FEP, everything is reduced to a process of Free Energy min-
imization; however, this requires specifying a generative model - which is our next
topic.
2.1 Generative model
Designing the generative model implies additional assumptions regarding how an agent
represents the system dynamics and the mapping into its sensory inputs. A generative
model can be described in terms of a joint probability density P(u, y) = P(y|u)P(u),
which highlights the separation between two components: the observation (or like-
lihood) model and the prior about the hidden variables. The latter can be further
factorized into the joint density P(u) = P(x|v)P(v). In general, the hidden causes
v are quantities that act as causal variables (or priors) over the hidden states x used
to describe the environment, thus enriching the representation of the system dynam-
ics. In some Active Inference implementations, hidden causes are used to encode the
agent’s goals (as will become clear when we discuss specific examples). This is because,
in keeping with ideomotor and cybernetic formulations, any deviation from the hidden
causes is registered as a prediction error that the agent tries to minimize.
The dynamic environment represented by an agent is usually modeled with the
following stochastic equations:
y = g(x, v) + ωy ˙x = f(x, v) + ωx v = η + ωv (2)
5
where the function g converts latent variables x and v into observed states y, f
encodes the evolution of the hidden states over time, η is the mean of the prior
distribution over the hidden causes, while wy, wx and wv are noise terms describing
system uncertainty, here assumed to belong to multivariate normal distributions with
zero mean and precisions Πy, Πx, and Πv.
This leads to the VFE:
F ≈1
2
h
εT
y Πyεy + εT
x Πxεx + εT
v Πvεv
i
εy = y − g(µ, ν)
εx = µ′ − f(µ, ν)
εv = ν − η
(3)
where µ and µ′ are respectively the internal representations (beliefs) about the 0th
and 1st temporal orders of the hidden states x and ˙x. Hence, the VFE takes the form
of a sum of quadratic forms of prediction errors: a sensory prediction error εy, a state
or model prediction error εx, and a prior prediction error εv.
To effectively represent complex dynamics of the generative process, it is possible
to improve the agent’s model by using generalized coordinates of motion [32,43] beyond
the 1st order. For instance, suppose that the brain represents beliefs about the position
of an object. Under a generalized coordinates model, it would also maintain beliefs
about its velocity, acceleration, jerk, and so on. All these time derivatives are then
concatenated to form a generalized belief, denoted by a vector ˜µ ≡ [µ, µ′, µ′′, . . .].
The same notation is used for the time derivatives of the other variables (i.e., ˜y ≡
[y, y′, y′′, . . .]).
Applying a local linearization [43] to the system dynamics, and then eliminating
the cross terms in the derivatives, it is possible to express the generative model by the
following set of equations:
y = g(x) + ωy
y′ = ∂g(x)
∂x x′ + ω′
y
y′′ = ∂g(x)
∂x x′′ + ω′′
y
...
x′ = f(x, v) + ωx
x′′ = ∂f(x, v)
∂x x′ + ω′
x
x′′′ = ∂f(x, v)
∂x x′′ + ω′′
x
...
(4)
which can be expressed in a compact form as ˜y = ˜g(˜x) + ˜ωy and D˜x = ˜f(˜x, v) + ˜ωx.
Here, the D operator maps each element of the generalized coordinates to its time
derivative: D˜µ = [µ′, µ′′, µ′′′, . . .]. Note that using generalized coordinates permits
dealing not only with white but also colored noise. In general, these coordinates have
been introduced to deal with non-Markovian processes [40,44], adopting a Stratonivich
interpretation with continuous stochastic variables having finite, non-zero autocorre-
lation functions. This is usually done using a temporal covariance matrix acting as
a Gaussian filter between noise terms that modify generalized precision matrices, de-
noted as ˜Π. This leads to a Free Energy that has the same (quadratic) form of
prediction errors [43].
6
2.2 Free Energy minimization
Active Inference assumes that the Free Energy is minimized in two complementary
ways. One, shared with predictive coding [30], consists of modifying the agent’s inter-
nal beliefs, to produce predictions that match the current observations. In particular,
it has been proposed [40] that the intrinsic dynamics of neural activity evolve in such
a way as to implement a (modified) gradient descent scheme:
˙˜µ − D˜µ = −kµ
∂F
∂ ˜µ ˙ν = −kν
∂F
∂ν (5)
where kµ and kν are tunable learning rates.
In biological terms, this means that agents are constantly engaged in an inferential
process to capture the hierarchical relationships between what is perceived at every
instant and what causes those perceptions. Importantly, the instant derivative of a
particular order of the generalized belief does not necessarily correspond to the belief
over that derivative (i.e., ˙µ ̸= µ′); the difference between those two terms provides an
additional error-term to minimize. As evident in Eq. 5, it is only when the Free Energy
is minimized that the generalized belief captures the real instantaneous trajectory of
the environment. Furthermore, since the Free Energy consists of a sum of quadratic
forms, its partial derivatives lead to simple update equations – proportional to the
prediction errors weighted by their respective precisions – which are similar to those
originally derived in the predictive coding model of [30]:
˙˜µ ∝ ∂˜µ˜gT ˜Πy˜εy + ∂˜µ ˜f
T ˜Πx˜εx − DT ˜Πx˜εx (6)
Thus, the overall update for the generalized belief over hidden states is subject to
three different forces: a likelihood component proportional to the sensory prediction
error; forward and backward components of the state prediction errors coming from
the previous and next temporal orders.
In Active Inference, there is however a second way to minimize the Free Energy: by
acting in the environment, the agent produces sensory observations that match its pre-
dictions. This action-related way to minimize the Free Energy is especially appealing
to model biological organisms that strive to realize their goals (or the prior preferences
encoded in their generative models). For example, if an organism is endowed with a
prior over a desired body temperature and senses a different value, it can maintain
its integrity by acting (e.g., by moving to a place having the desired temperature) –
whereas only changing its belief would probably lead to death in the long term. In
short, the predictions are fulfilled by acting, rather than corrected by changing mind.
Figure 2: Comparison between motor control schemes in Optimal Control (left) and Active
Inference (right). See the main text for a detailed discussion.
7
Formally, minimizing the Free Energy with respect to the actions a results in a
gradient descent scheme similar to the previous case:
˙a = −ka
∂F
∂a = −ka
∂F
∂y
∂y
∂a (7)
where ka is a learning rate.
Note that in the last gradient of Eq. 7, the presence of the term ∂ay is key – and
points to the agent’s “implicit” knowledge of (simple) sensory outcomes of actions.
Here, “implicit” is used in the sense that the action variable a is not considered
to be part of the generative model, but at the interface between generative model
and process (see Fig. 1). This knowledge does not correspond to a sophisticated
“inverse” model, but to simple and short-term action consequences, which could be
associated with reflex arcs. For example, the “inverse” model for a velocity-controlled
scheme could be simply approximated by a time constant ∆t [45]. As will be explained
below, in the Active Inference framework reflex arcs are key to motor execution via
the minimization of proprioceptive prediction errors induced by top-down modulatory
signals from motor areas [26].
In sum, the above discussion highlights that the interaction between the agent and
the environment is characterized by a closed loop, during which the agent minimizes
the Free Energy (or under some simplifying assumptions, prediction errors). The agent
could minimize prediction errors through belief updates, which renders the generative
model closer to the generative process, therefore creating a good representation of
the environment. Alternatively, it could generate actions that render the generative
process closer to the generative model. Whether the former (belief updating) or the
latter (action) process is selected simply depends on the relative balance between
prediction errors and their relative precisions. For example, an agent endowed with
an extremely precise prior would never update it in the light of novel evidence and
hence would always try to minimize the Free Energy by acting. Conversely, an agent
endowed with an imprecise prior would be more willing to update its beliefs in the
light of novel evidence. This implies that the design of the generative model is a
crucial choice to determine the agent’s behavior. Two agents that deal with the same
situation but are endowed with different generative models (e.g., with different priors)
could produce completely different patterns of behavior.
2.3 Neural underpinnings of motor control in Active In-
ference
It is useful to briefly summarize the key biological assumptions of motor control in
Active Inference and compare them with classical theories such as Optimal Control;
see [46] for an extensive treatment of these differences and of forward and inverse
models in the brain. As we highlight in Fig. 2, both Active Inference and Optimal
Control [1, 2] use similar processes and variables, but arrange them differently. The
main difference is that in Active Inference, the forward (generative) models convey
proprioceptive predictions down to the spinal cord to compute motor commands at
the level of the reflex arcs. Instead, in Optimal Control, the forward models are
coupled with inverse models at a higher level to compute motor control signals.
Thus, in Active Inference action is made possible through low-level suppression of
prediction errors, which not only climb up the hierarchy but also exert forces on the
muscle states. It is worth noting that while the mathematical formulation of Active
8
Figure 3: An example of goal-directed reaching, from [37]. Left panel: the stimulated robot.
Right panels: Reaching trajectories of the robot’s 7 DoF arm in three dimensions, from a
start location (bottom) to a goal location (green dot) in four scenarios: (subpanel a) without
noise; (subpanel b) with noisy proprioception; (subpanel c) with noisy vision; (subpanel d)
with noisy proprioception and vision. The blue trajectory is the mean of the 20 trajectories,
shown in gray.
Inference allows involving any sensory modality in action processes, in biological treat-
ments it is often assumed that motor control is realized by minimizing proprioceptive
- and not exteroceptive - prediction errors [26], whereas autonomic control is realized
by minimizing interoceptive prediction errors [27, 47]. The reason for assuming that
only proprioceptive prediction errors are involved in motor control is the observation
that the efferents of the somatomotor system share crucial similarities with top-down
projections of other brain areas, thus seeming to encode proprioceptive predictions
rather than motor commands [26]. In this perspective, proprioceptive prediction er-
rors are computed in the spinal cord through the reflex arcs and backpropagated by
afferents throughout the cortical hierarchy, whereas exteroceptive prediction errors are
generated locally in their respective functional areas. It is thus unlikely that direct
somatomotor efferents convey pure exteroceptive signals from the respective sensory
areas to the muscles, or that a difficult inversion is realized in the spinal cord between
motor and exteroceptive domains. This makes it unlikely that exteroceptive sensations
are directly used for action execution and marks another significant difference with Op-
timal Control. However, movements driven only by proprioceptive contributions raise
a few concerns regarding multisensory conflict resolution, as will be explained later.
3 Examples of Active Inference models of motor
control
Here we review some examples of Active Inference models that target four kinds of
problems: goal-directed motor control, active sensing, multisensory conflict resolution,
and decision-making in dynamic environments. The list of selected models and func-
tions is not exhaustive, but provides an overview of the scope of Active Inference in
continuous time.
3.1 Goal-directed reaching: four examples
As explained above, action follows prediction errors that may result from a discrepancy
between a proprioceptive observation and a proprioceptive prediction. This situation
can be exemplified in the case of goal-directed reaching actions, where the start and
the goal positions of the agent’s arm initially differ. One example of Active Inference
implementation of a goal-directed reaching task was proposed in [37]. Here, a simulated
7 Degrees of Freedom (DoF) robotic arm had to reach a static target (see Fig. 3, left
9
panel). The agent maintained a belief over the arm’s joint angles, and was endowed
with a proprioceptive model producing predictions in the same domain and a visual
model generating the end effector’s position. The goal states were embedded into
the 1st-order dynamics function, where the trajectory of each joint was modeled by
Newtonian dynamics with parameters λ, κ and m representing elasticity, viscosity,
and mass. The role of the dynamics function is to make the agent perceive a force
proportional to the desired one, (or better, “think that it will perceive” the force,
since the force does not have any counterpart in the generative process). This force is
computed by performing a kinematic inversion of the error between the end effector
and the target’s position, and by taking into account all the possible singularities. The
right panels of Fig. 3 show the different trajectories that the agent performs under
various conditions in which the noise of one or more information sources is introduced.
These results highlight the advantages of relying on precise multisensory information
(panel a) compared to situations in which proprioceptive (panel b), visual (panel c),
or both sources (panel d) are noisy.
Another implementation for a similar reaching task is described in [48]. Here, the
agent had to perform two tasks: continuously tracking a moving target and realizing
multi-step movements toward two different object locations. In this case, the high-level
belief consisted not only of the arm’s joint angles, but also of as many components as
every object to interact with. Such components were encoded in the proprioceptive
domain and they could be interpreted as particular affordances that the agent wanted
to realize. To address the first (target tracking) task, the belief dynamics uses a
custom “intention” that manipulates the current belief to produce a possible future
configuration. For example, if the belief consists of three components corresponding
to joint configurations of an arm, a target, and a previously memorized home button -
i.e., µ = [µa, µt, µh] - an intention to reach the target is built through a function that
sets the first component equal to the second one, i.e., µ∗
t = [µt, µt, µh]. This future
prediction is then subtracted from the current belief and embedded into the 0th-order
dynamics:
f(t) = λ(µ∗
t − µ) = λ · [µt − µa, 0, 0] (8)
where λ is an attractive gain. Since the target configuration is continuously inferred
through visual predictions (here, generated by the decoder of a Variational Autoen-
coder (VAE) [49], the agent is able to reach and track moving objects, as shown in
Fig. S1. Furthermore, [48] generalized the above approach by considering multiple
intentions that operate simultaneously, thus allowing one to realize more complex
movements or multi-step tasks, such as the one represented in Fig. 4. For example,
if a home button reaching intention is constructed in the same way as the first one,
called µ∗
h, the overall attractive force is:
˙µ′ = −πx,tεx,t − πx,hεx,h (9)
where εx,t and εx,h are the state prediction errors of the two intentions, with precisions
πx,t and πx,h. A multi-step behavior may then be achieved by dynamically modulating
the latter, e.g., through a belief over tactile sensations [50]. Note that while the above
examples suggest that the presence of a goal state or a proprioceptive prediction error
always results in an immediate movement, this is not always the case. A simple
demonstration is in delayed reaching tasks where motor intentions are already present
in the preparatory phase but the onset of action execution depends on a sensory cue
that is presented at a particular time. The precisions modulation can then also be used
to separate the two phases of action preparation and execution (see Fig. 4), as done
10
Figure 4: Visual representation of the two-step delayed reaching task of [48], composed
of two distinct phases. Real and estimated arms are displayed in blue and green, real and
estimated targets in red and purple.
in [48]. In short, a delicate balance is in place between high- and low-level precisions:
indeed, movement in Active Inference is possible through sensory attenuation, i.e., by
reducing the precisions of sensory generative models, so that the belief can be free
to change by its own dynamics, ultimately affecting what sensations will be sampled
next [51].
A more realistic implementation of reaching (and other advanced) movements is
illustrated in [52]. This study introduces a block calledIE model that consists of intrin-
sic (e.g., joint angles) and extrinsic (e.g., Cartesian positions) beliefs, linked through
a generative kinematic model. While the previous examples required inverse models
(either in the dynamics function – as a pseudoinverse or a Jacobian transpose – or in
the implicit backward pass of the VAE), in [52] the inversion arises naturally, through
inference. The extrinsic goal is defined at a lower level compared to the intrinsic state,
following the causal relations encoded into the generative process. Maintaining an ex-
trinsic belief has critical benefits, since it permits to easily design complex movements
(e.g., circular or linear trajectories), without worrying about intrinsic transformations.
Furthermore, different blocks can be combined to encode intrinsic and extrinsic infor-
mation for each DoF of the kinematic chain separately. This scheme affords more
efficient control, permitting to simulate whole-body kinematics during (for example)
sophisticated reaching and obstacle avoidance tasks. See Figures S2 and S3 for some
examples.
Beyond reaching, Active Inference has been used also to simulate the control of
eye movements [53–55]. For example, the study of [56] uses a hierarchical model that
includes a belief over the target in absolute coordinates and a belief over the vergence-
accommodation angles. The model consists of two parallel pathways – one for each
eye – that perform a perspective projection of the target into the eye planes. This
approach permits: (i) inferring the depth of the target by averaging the contributions
from both eyes; (ii) fixating a target, by imposing an attractor in the projective space
of the eyes; and (iii) performing concurrent depth estimation and target fixation –
or active vision – through action-perception cycles – which is particularly useful to
counteract the nonuniform fovea resolution of the eyes (see Figure S4).
Taken together, the above examples show that in Active Inference, it is possible
to define goals for movement as priors over the internal representation of the system
dynamics. These priors then lead to a goal-directed control through Free Energy
minimization, rather than appealing to stimulus-response mappings and cost functions
as in Optimal Control [1, 2] and reinforcement learning [3] (see [46] for a detailed
discussion of the differences between the roles of cost functions in Optimal Control
and Active Inference). As explained before, priors play a similar role as setpoints
for movement control in cybernetics. The above examples also help illustrate the
differences between the generative process encoding the real environmental dynamics,
11
and the agent’s generative model – and the fact that they are reciprocally connected in
an action-perception loop. Finally, they illustrate that there are various ways in which
movement can be generated, e.g., it is possible to impose different kinds of priors that
determine different behaviors; we will return to this point in the Discussion.
3.2 Active sensing
Active sensing refers to the ability of an agent to adapt its perception using self-
generated energy to sample the environment [57–59]. Several Active Inference models
implement active sensing routines to support visual processing [23, 60] and whisker
movements [61], among other examples.
The model of [35] combines motor prediction – the reuse of the motor system to
predict perceived movements – and an active sensing (or hypothesis testing) strategy:
the use of saccadic eye movements to disambiguate among alternative hypotheses.
The architecture embeds a generative model of how (arm and hand) actions are per-
formed to generate hypothesis-specific visual predictions, and directs saccades to the
most informative (or diagnostic) visual locations to test them. The model follows
the hierarchical form for generalized predictive coding, as shown in Fig. 5A. Internal
states encode a representation of the center of oculomotor fixation and the probability
that each hypothesis is the cause of the visual input. Hidden controls determine the
location that attracts the gaze. The model is tested by evaluating the salience of sam-
pling dynamic visual locations under two competing hypotheses (power grasp versus
precision grip) in two conditions: with and without preshape.
The model reproduces the differences observed empirically between the observation
of goal-directed grasping actions, with or without informative cues (i.e., when the hand
of the actor is “preshaped” to grasp one of the two possible objects, big or small, versus
when there is no preshape). The study of [62] reported that during the observation
of goal-directed grasping actions without informative cues (e.g., without preshape),
visual saccades tend to follow the observed arm. Rather, when informative (preshape)
cues are present, people make anticipatory saccades to the object to be grasped.
The simulation results show significant differences between a reactive hand-following
gaze strategy, which emerges in the no-preshape condition, and an anticipatory gaze
strategy, which emerges in the preshape condition, shortly after the beginning of a trial
- analogous to the empirical study of [62]. The crucial model component that affords
this active sensing strategy is a saliency map that assigns salience to the elements of
the visual scene that afford information gain about the to-be-inferred grasping move-
ments (hand shape) or their destination (objects) - in such a way that the objects only
become salient when the uncertainty about the grasping movement has been resolved.
See [35] for details.
The authors assume that a hierarchically organized “action observation” brain
network computes both the expected hand position (at lower hierarchical levels) and
the probability of the two competing hypotheses (at higher hierarchical levels). Fig. 5B
shows the two competing hypotheses considered, which are not only about final states
(hand on big versus small object), but encompass the whole action unfolding in time.
In practice, they correspond to sequences of (superimposed) images of hand trajectories
(here, 6 time frames). As evident in the figure, the hypothesis that the actor is reaching
a small (or big) object entails that the hand will be configured in a precision grip (or
power grasp) during action execution – and it is this sort of hypothesis that the agent
tests by performing saccades to the most informative locations. Fig. 5C-L shows the
results of two example trials during which the agent observes an actor performing a
12
Figure 5: The active sensing model of [35]. (A) The model describes which visual stimuli
should be expected under two perceptual hypotheses (e.g., if the action target is the big/small
object, when doing a saccade to the next hand position I should see a power/precision grasp)
and generates saccades to check if the expectations are correct and to revise the probability of
the two hypotheses. In the first hierarchical layer of the architecture, proprioceptive and visual
signals yp and yq are generated, which are then used to compute the respective expectation
˜µx,p and ˜µx,q through message passing of prediction errors εx,p and εx,q. (B) Schematic
illustration of the two competing hypotheses, corresponding to sequences of images. (C-L)
Simulation results of two representative trials, during which the agent observes an actor that
grasps the small object without hand preshape (left) or with hand preshape (right); see the
main text.
13
precision grip toward the small object, without (left) or with preshape (right). In
particular, the panels show the expected probability of the two competing hypotheses
during an example trial (C); the location of the saccades in the video frame at six time
frames (D); the corresponding saliency maps, with white locations corresponding to
the locations to which the model assigns greater salience, hence the best candidates
for the next saccade (E); the hidden (oculomotor) states computed by the model (F);
the content of what is sampled by a saccade in the (filtered) map (G); the posterior
beliefs about the “true” hypothesis, where expectations (expected log probabilities)
are plotted in blue and the associated uncertainty (90% confidence interval) in gray
(H); the observations of the model i.e., the mixture of the viable hypotheses weighted
by the posterior expectation, represented as a weighted superposition of all the frames
during the simulation steps (I); and the sequence of saccades that the model performs
during the experiment (L).
Without preshape (left panels), the gaze follows a reactive, hand-following strat-
egy and the action is disambiguated fairly late in the trial. Rather, with preshape
information (right panels), the clues present in the hand movement afford a faster
disambiguation of the correct hypothesis and anticipatory saccades to the inferred
objects: the eyes land on the small object before the hand arrives. This example
illustrates that the same Active Inference mechanism that we discussed above in the
context of goal-directed actions (e.g., reaching an external target) can also model active
and higher-level oculomotor control, where the objective is to sample the sensorium
for hypothesis testing. Both forms of behavior stem from Free Energy minimization
under different generative models; while we illustrate them separately, they could also
emerge simultaneously in the same model [23].
3.3 Unintentional actions driven by multisensory conflict
So far, we discussed Active Inference models of goal-directed movement (e.g., reaching
or following a target) and active sensing. However, movement can also arise uninten-
tionally, with little or no awareness. While the study of unintentional motor behavior
found so far little space in the motor control literature, recent evidence for the sys-
tematic induction of unintentional actions comes from embodiment studies, in which
subjects undergo an illusory experience in which fake bodies (e.g., virtual avatars) or
body parts (e.g., rubber hands) are perceived as being (part of) their own body [63–66].
During these body ownership illusions, subjects process the seen body (parts) as the
same causal entity generating somatosensory sensations [67–69], and to some extent it
is possible to introduce multisensory conflicts about the body configuration without
breaking the illusion [70,71]. For example, in the Rubber Hand Illusion (RHI), when
the rubber hand is placed next to the real (occluded) hand, a visuo-proprioceptive
conflict about the hand location is in place. This conflict has been associated with a
proprioceptive recalibration of the perceived hand location since the very first report
of the RHI [63], and consists of a shift of perceived hand location in the direction of
the visual hand. Interestingly, later works have associated an active component to the
illusion; namely, the tendency to unconsciously exert a force in the direction of the
visual hand and to move along with it if no restrictions are in place, in some cases
even when subjects are explicitly instructed to stay still [72–74]. This behavior has
been associated with an active strategy for suppressing prediction errors associated
with the perceived location of the hand. Additionally, movements may arise because
the subject tries to minimize model uncertainty, e.g., to understand if the perceived
and the real hand positions match. The latter process has been successfully repro-
14
duced in Active Inference implementations of the action-perception loops during body
ownership illusions [36,72,74].
The study of [72] introduced an Active Inference model of the active strategies that
emerge during ownership illusions to suppress multisensory (self-perception) conflicts.
The model is tailored to the classic RHI in its virtual version (using a virtual hand, not
a rubber hand), in which subjects are not allowed to move their hand. In line with this,
the model computes the actions but these do not enter the computation of the hand
dynamics. Indeed, actions are here computed as a proxy for the force that participants
exerted while experiencing the illusions with their arms restrained. In this respect,
the model has the intrinsic limitation of being uncoupled with the arm dynamics.
This limitation was addressed in another study [36], which proposed a unified model
that can account for both goal-directed motor behavior (i.e., reaching actions) and
unintentional motor adjustments arising from multisensory conflicts associated with
self-perception, and for their interaction.
The model of [36] implements an agent that continuously infers its own bodily
configuration and could be set to have a goal of reaching a given target. If no goal to
reach is instantiated, which corresponds to no intention to move, the agent is set to
fulfill the requirement to keep its current configuration. A schematic summary of the
model is given in Fig. 6A. An important novelty with respect to previous implemen-
tations of arm control is the possibility to simulate the case in which the agent has
no goal (i.e., no intention to move), enabled by keeping the internal representation of
system dynamics used for reaching tasks (i.e., a damped oscillator) and setting the
attractor (i.e., the desired state) to the current arm configuration. Despite its simplic-
ity, this extension is key, because it allows inspecting subtle aspects of movement such
as how unintentional motor adjustments arise as a byproduct of self-perception, and
how these adjustments interfere with goal-directed behavior. This is exemplified by
some of the results of the model that we discuss in the following. A second difference
from most previous implementations is that action is computed by concurrently mini-
mizing prediction errors in the proprioceptive and the exteroceptive (visual) domains.
This is essential to correctly model visually-guided action operated under multisensory
conflict about the body state.
Fig. 6B shows results comparing standard reaching, unintentional alignment of
the physical hand to its visual counterpart during an ownership illusion, and reaching
under bodily multisensory conflict (columns from left to right respectively). The three
simulations use the same agent exposed to different combinations of tasks and sensory
inputs. In the first, the agent is assigned a standard reaching task; in the other two,
the agent undergoes an ownership illusion over a rubber/virtual hand, and sensory
input about limb state is streamed by the fake the real hand in the visual and the
proprioceptive domain, respectively. In one case, the static rubber hand is displaced
with respect to the real hand and the agent does not have a task assigned besides
observing (and inferring) its own state. In the other case, the agent has to reach a
target but the velocity of the virtual hand is set to 1.3 times the one of the real hand,
so that during the task execution the two hands get progressively displaced from one
another. The model assumes that the ownership illusion is in place by treating the
visual input from the fake hand as if generated by the inferred arm configuration, which
is encoded by the agent in the proprioceptive domains ( yθ). Thus sensory predictions
takes the form: yµ = [ yµP , yµV ], with yµP = µθ and yµV = [ µPRH,x, µPRH,y ] (see
Fig. 6A for more details). In the simulation of the static RHI, the internal model has
been adapted by tuning two of its parameters: (i) the gain of the action component
driven by vision, set to zero to account for the fact that the fake hand is static and
15
Figure 6: Schematic implementation and results from the Active Inference model that si-
multaneously accounts for intentional (goal-directed) and unintentional movements, described
in [36]. (A) The model implements a 1-DoF agent, whose configuration is uniquely described
by its elbow’s joint angle and angular velocity – x = [ θ, θ′] – and who receives informa-
tion about its own configuration and the environment through proprioception and vision –
y = [yP , yV ]. The dynamics of the real arm – ˙x = fx(x, a) – represents a damped system that
may be subject to internal forces generated through actions (here, formalized as joint angular
accelerations), while the internal model of the arm dynamics fµ(µθ, µθT ) by a damped oscil-
lator where the attractor is either set to the arm configuration µθT in which the hand is on
target (for reaching actions), or to the current state when the agent has no intention to move.
(B) Results from three simulations: standard reaching (left column), classic rubber hand il-
lusion (central column), and reaching under visuo-proprioceptive conflict (right column). In
each column, panels from top to bottom show the temporal evolution of the real and inferred
joint angles and joint angular velocity, of the prediction errors, and of the contributions to
action arising from the minimization of proprioceptive and visual prediction errors.
16
not under the control of the agent, and (ii) the internal estimate of the sensory noise
in the visual domain, increased to roughly account for the fact that (as it happens in
the real world) while undergoing an ownership illusion the agent is still aware that the
seen hand is fake and therefore “less reliable” as a source of information about its own
bodily state; see [36] for more details.
The results from the three simulations shown in Fig. 6B demonstrated that the
proposed implementation could account for intentional motor behavior, as in the ex-
emplification of standard reaching, and for unintentional motor adjustments driven by
multisensory conflict in self-body processing, as observed in experimental settings of
ownership illusions. In addition, the model successfully reproduces the motor behav-
ior observed for visually-guided actions in presence of multisensory conflicts, like in
the case of reaching under visuomotor rotations or with aberrant velocity mappings.
Importantly, this case would indicate that the action is driven by both visual and
proprioceptive prediction errors; in fact, keeping action exclusively associated with
the minimization of proprioceptive prediction errors would lead to the implausible re-
sult of the agent overshooting the visual target (see [36] for details). Interestingly, a
previous Active Inference model of a similar visuomotor rotation task has shown that
modulating the relative weighting of the internal estimates of the (visual and propri-
oceptive) sensory noises can mimic attentional effects akin to those observed in the
laboratory [75].
Together, these results lead to two main insights. First, in the case of intentional
reaches, prediction errors are initially dominated by model errors driven by the internal
dynamics under the effect of the target attractors; then, sensory prediction errors
arise as a consequence of the model errors on perceptual inference. In the case of the
RHI, model errors are absent (as no motor intention is instantiated) and the sensory
prediction errors – thus the actions – arise as a byproduct of the visuo-proprioceptive
conflicts associated with self-body perception. An important consequence of the lack
of model errors is that the agent does not update the internal estimate of the joint
angular velocity, which stays null as if no movement has occurred (providing that the
agent has no direct access to joint velocity through proprioceptive receptors). This has
been suggested as a possible explanation for the lack of motor awareness that typically
characterizes subtle unintentional motor adjustments.
Second, when simulating reaching under visuo-proprioceptive conflicts, a better
fit with experimental data can be obtained by also allowing exteroceptive (not only
proprioceptive) prediction errors to drive the action. Given the spatial misalignment
that emerges from the aberrant velocity mapping between real and fake hands, the
inferred posture ( µθ) is biased toward the visual hand. If action were only driven by
proprioceptive prediction errors, both the virtual and the real hand would overshoot
the target, which runs against the empirical observation that the task is accomplished
once the visual hand correctly reaches the target.
3.4 Mixed models for sensorimotor decisions
Despite the traditional literature views decision-making and action control systems as
separated cognitive processes, a more recent tendency considers them as two interact-
ing levels of the same integrated system [76–78]. Accomplishing various skilled tasks
engages a series of decisions to establish the sequence of movements to make, and to
guide the sensorimotor behavior. At the same time, actions can determine changes in
the real world that force us to modify the goal of the executed task and to revise the
plan previously imagined.
17
From a modeling perspective, the variables employed in this sensorimotor decision-
making system are of different natures: decision-making typically involves discrete
variables that select the sequence of actions composing a motor behavior, while the
execution of motor behavior induces a dynamic variation of some continuous variables
(e.g., contracting muscles or decreasing the body temperature). To integrate both
discrete and continuous time variables within the same Active Inference model, it is
possible to adopt so-called “mixed” models [79]. Mixed models inherit the architecture
of hierarchical generative models, where the prediction of a level acts as a prior for the
level below, which in turn computes a prediction error that is then used as a likelihood
signal for the higher level. In a typical mixed model having two layers, the higher layer
consists of a discrete Partially Observable Markov Decision Process (POMDP); in this
article, we did not focus on this discrete-time Active Inference, but a full treatment
can be found in [23]. Rather, the lower layer of the typical mixed model implements
exactly the Active Inference in continuous time that has been the focus of this article.
The higher layer generates sequences of discrete outcomes, which constitute the priors
– or fixed-point attractors – on the hidden causes, guiding the sensorimotor process
controlled by the lower layer. Since the two layers consist of variables of different
natures, they are connected by a particular interface described in [79] that propagates
the beliefs from one layer to another, via descending and ascending messages.
In mixed models, inference progresses by determining probabilities π about se-
quences of control states (i.e., policies) at the higher layer. Each policy ππ generates
a transition between hidden discrete states sπ,τ , which corresponds to a sequence of
predicted outcomes oπ,τ . By performing a Bayesian model average of the outcomes
of all policies, a posterior predictive distribution oτ = P
π ππ · oπ,τ is obtained and
sent as a descending message to the lower continuous layer. Each component oτ,m
can be understood as a particular model, steering the lower-level continuous dynamics
in a specific direction. The mapping of an outcome model into the continuous space
is denoted as ηm, encoding a fixed empirical prior; a second Bayesian model average
defines the actual prior η over the hidden causes ν, i.e., η = P
m ηm · oτ,m. Having
sampled continuous observations, the lower layer returns an ascending posterior esti-
mation for the belief of each discrete outcome through a model evidence accumulated
by the dynamical system over some time T:
E(t)m = −ln oτ,m −
Z T
0
L(t)mdt (10)
where L(t)m = ln P(˜y(t)|ηm) − ln P(˜y(t)|η), with ln P(˜y(t)|ηm) and ln P(˜y(t)|η)
denoting the log evidences about continuous observations ˜y(t) = [y(t), y′(t), y′′(t), . . .]
regarding a single model m and the full set of models, respectively. In other words,
L(t)m is a post-hoc Bayesian comparison between two (Gaussian) probability densities
used to sample the outcomes, under the empirical reduced (ηm) and the full (η) priors
[80]. It can be shown that if ηm = η, then L(t)m = 0; see [79] for a demonstration.
On the other hand, E(t)m expresses the Free Energy of competing outcome models
defined as the sum between their descending prior surprise −ln oτ,m and the log evi-
dence L(t)m of their ascending posterior integrated over time. Note that when T = 0,
the ascending posterior reduces to the descending prior. Thus, E(t)m assigns a score
to the sampled continuous outcomes in relation to the predicted discrete models. To
convert this score for each model back to the discrete layer, E(t) is passed through a
softmax function to give a posterior over each outcome model, so that it can be used
as discrete observation into the POMDP inference process.
18
In the last few years, various studies have used mixed models to target scenar-
ios requiring both discrete and continuous variables. For example, [81] proposed a
mixed generative model to sample visual information from the environment, which
shares some resemblances with the active sensing model of [35] introduced above, but
integrates both discrete and continuous variables. The discrete layer of the model
implements a POMDP to build a sequence of saccade targets and to decide where to
look. Such decisions are translated into movements of the oculomotor system by the
continuous layer that implements how to look, i.e., the realization of the saccades by
controlling the anatomical effectors. Successively, the sampled observations are fed
back to the discrete layer to evaluate the goodness of the saccade sequence.
A similar mixed generative model was used to study the interaction between phar-
maceuticals and oculomotor behaviors, by focusing on the influence of cholinergic
and GABAergic agents upon the choice of the target to fixate and the speed of sac-
cades [54]. The authors simulated an oculomotor task introduced in [82], in which a
cue for a given saccade location is presented and after its disappearance, a saccade to
the target is executed. In the mixed model used to simulate the oculomotor task, the
discrete layer generates predictions about the fixation locations, which constitute the
attractor points for the oculomotor system dynamics in the continuous state-space.
The effects of neuromodulators are simulated by changing various parameters of the
mixed model; namely, the precision of the hidden states transitions (noradrenaline),
the mapping between hidden states and sensory data (acetylcholine), the belief about
the best saccade to select (dopamine), or the empirical priors that control the saccade
peak velocity (GABA).
Another application of mixed models is the “active listening” model [83], which
simulates the parsing of meaningful words from auditory perception. Following some
insights borrowed from active vision, the generative model segments the continuous
stream of acoustic signals by placing word boundaries in accordance with some prior
constraints. For example, the offset of one word should precede, in some plausible
time range, the onset of the subsequent word; in a speech, signal segmentations are
more likely to contain words than non-words; chosen a specific language, there exists
a prior knowledge on the possible produced words; etc. The active listening model
then proceeds by identifying several plausible boundary intervals, which provide the
greatest evidence for the prior beliefs about the words.
Another interesting application is illustrated in [84]. This study shows that Active
Inference can simulate several neurological conditions and that some reflexes might
naturally emerge under the appropriate generative model. Furthermore, this imple-
mentation shows that linking a continuous model of the arm dynamics and a discrete
decision model permits performing multi-step reaching movements, by planning from
a high-level goal. A similar model was used to solve a dynamic pick-and-place opera-
tion [85]. In this case, two novelties are introduced. First, the discrete model generates
and integrates predictions simultaneously from the intrinsic and extrinsic modalities
also used in [52]. Second, the reduced priors of the agent are updated at each dis-
crete step, allowing it to grasp moving objects. A similar grasping task was simulated
in [86], but using an unconventional approach. In this case, the hidden causes were
sampled by a categorical distribution, while the reduced priors were generated by in-
dependent dynamics functions of the hidden states. This allowed to impose and infer
static and discrete intentions corresponding to dynamic trajectories in the continuous
environment.
Yet another example of mixed models - this time, in the domain of interoceptive
processing and autonomic (not action) control - is provided by [28]. The authors
19
describe the mechanisms of adaptive physiological regulation using three generative
models of increasing complexity, which are able to simulate homeostatic, allostatic,
and goal-directed regulation of bodily and interoceptive parameters, such as temper-
ature, thirst, and hunger. While the first two generative models use only continuous
variables, the latter generative model (for goal-directed control) is a mixed model, in
which the higher layer implements a POMDP process to select among discrete policies
(e.g., to run with or without a bottle of water), while the lower layer is a continuous
time system that regulates interoceptive data (e.g., body temperature) via autonomic
reflexes (which might be considered largely analogous to motor reflexes, but operate
on interoceptive streams [47,87]). In order to estimate the long-term consequences of a
certain policy, the model maps discrete outcomes at the higher layer into prior beliefs
for specific interoceptive observations at the lower layer. Conversely, the lower layer
model provides evidence about the discrete outcomes used as hypotheses about the
expected prediction errors, so that it contributes evaluating the policies at the higher
layer. Models of this kind can be used to support computationally guided investiga-
tions of interoceptive processing and its dysfunctions that are possibly associated with
psychopathological conditions [88–94].
4 Discussion
How does the brain control movements toward goals? There is a view of motor control
– pioneered by ideomotor theory and cybernetics – according to which actions are
inextricably linked to their effects, rather than stemming as responses to stimuli. In
these theories, action starts with some internal image of an intended effect – sometimes
called a preference, a goal, or a setpoint – and the movement is the consequence of
filling the gap between the intended effect and the sensed environmental condition. In
other words, these theories assign a role to action effects, or the discrepancy between
action effects and sensory events, in the selection and the control of movements. Active
Inference formalizes key intuitions of these theories, in terms of priors, predictions,
and prediction errors, therefore linking to a large body of studies about predictive
processing and Bayesian inference in biological organisms [22–25, 78] and in robotics
[38,95–99].
Here, we provided a brief illustration of Active Inference in continuous time and
discussed specific models that targeted various aspects of motor control; namely, the
execution of goal-directed reaching actions, active sensing, the resolution of multi-
sensory conflicts, and the integration of discrete (decision-related) and continuous
(perception- and action-related) processes. Each of the example models that we have
briefly reviewed can be evaluated by its own merits, such as by its capability to accu-
rately account for empirical data. However, taken together, these models (and others)
show that Active Inference can address a large variety of motor control processes. Im-
portantly, all the motor control phenomena illustrated by our examples stem from the
same process of Free Energy minimization, rather than requiring separate objective
functions. This feature makes Active Inference appealing both as a general theory of
biological systems and as a technical framework to advance AI and robotics research.
Despite the appeal of the models that we have reviewed, Active Inference accounts
of motor control are still relatively young compared to other frameworks, such as
Optimal Control [1, 2]. Several open issues need to be clarified to develop Active
Inference accounts of motor control that are more mature from both biological and
robotic perspectives. Below we briefly discuss some of the most important open issues
20
that need to be addressed in future research.
One open issue concerns the sensory modalities involved in motor control. As dis-
cussed in Section 2.3, some biological considerations suggest that motor control could
be realized by minimizing proprioceptive, not exteroceptive prediction errors [26].
However, as highlighted in [36], driving action with exteroceptive errors would seem
necessary to correctly reproduce visually-guided reaching behavior in the presence of
visuomotor conflict. From a practical perspective, including exteroceptive modalities
in the Free Energy minimization through action offers various advantages [45,100]. For
example, the reaching model of [101] uses proprioceptive and visual sensory modali-
ties for perception and action. An advantage of this approach is that the agent can
perform smooth and accurate movements even in the presence of high proprioceptive
noise, given that the visual input is more stable. As noted in [48], the increased stabil-
ity derives not only from the fact that there is less noise in the action update, but also
because both the action and the high-level belief are updated with the same informa-
tion, and the effect is more prominent as the visual precision increases. Further studies
are thus needed to understand the actual role of visual predictions in action execu-
tion or, more clearly, how visually-guided movements can be correctly realized, from
a biologically plausible perspective, in presence of noise or conflicts between different
sensory modalities.
This open issue implies that there might be a tension between standard formu-
lations of Active Inference that focus on biological aspects, and studies that realize
efficient robotic implementations. Besides multisensory integration, an Active Infer-
ence agent may also act by minimizing increasing temporal orders of the prediction
error, as in [102], where an agent is controlled by both position and velocity, resulting
in increased stability and additional control over the environment, if an appropriate at-
tractor is embedded at high orders of the belief dynamics. Design differences exist also
about the temporal order primarily affected by the attractor: while this is usually em-
bedded into the 1st-order dynamics function, some models encode it in the 2nd-order
to achieve a more stable control, especially when the robot is force-controlled [37,103].
Finally, different models use different kinds of errors as the attractive force for motor
control. As discussed in Section 2, the belief update depends on three components: a
likelihood error from lower hierarchical levels, a backward error coming from the next
temporal order, and a forward error from the previous order. Generally, the attractive
role is fulfilled by the backward error, which however requires the computation of the
gradient of the dynamics function [100]. Other studies use instead the forward error
(which is simpler to compute) as the main attractive force [48]. Finally, an alternative
strategy consists of including control costs in the Free Energy expression, to remove
estimation biases and afford optimal action [104]. The pros and cons of the different
approaches and their biological plausibility remain to be systematically investigated.
Another dimension that is important to consider in Active Inference studies is the
way the generative model is designed or learned - since the generative model implicitly
defines the agent’s behavior. One crucial design choice regards the extent to which
the generative model and the generative process are similar or dissimilar. For the sake
of simplicity, many Active Inference studies use generative models that are almost
identical to the respective generative processes, with few quantitative differences. In
these studies, the generative model is usually aligned to the generative process, in
three ways. First, the internal state variables are modeled as explicit representations
of features of the physical environment or the body, so that the generative model
already incorporates explicit task-related variables such as speed, pressure, position
in the allocentric space, etc. Second, the internal prior dynamics are designed as a
21
copy of the simulated world dynamics, in the sense that the sets of differential equa-
tions implementing the changes of the state variables are the same. Third, motor
commands are built as inverse models of physical world/body features, so that ac-
tions are direct changes in speed, pressure, positions in the allocentric space or other
physical entities. For example, [51] shows an Active Inference model of the behavior
in a force-matching task, where subjects have to match a reference force by pressing
directly on themselves. In this case, physics is simulated with two coupled differential
equations defining the dynamics of the self-generated and the external force. Sensory
(proprioceptive and somatosensory) observations are simple linear mappings of these
hidden variables with the only exception that while proprioception is a linear map-
ping of self-generated forces alone, touch is a raw sum of self-generated and external
force. The prior dynamics of the generative model are then built in strict relation to
the generative process, as a set of differential equations which is quite similar to the
one described above, with the only exception that a causal variable takes the place of
the action. Accordingly, all mappings to the sensory predictions also have the same
features as the ones generating the described observations from the simulated physics.
A similar example of the similarity between generative model and generative process
is offered by a model of the accommodation of delays in oculomotor control [53]. In
the model, the generative process consists of a set of ordinary differential equations,
which describes the dynamics of the current oculomotor displacement and the target
location; and the mappings generating sensory observations (displacement and target
position) are linear combinations of the hidden variables. As in the previous example,
the generative model is simply a copy of the generative process, except for the fact
that the latter includes a contribution from the action.
Nevertheless, Active Inference does not necessarily require that the two systems
are the same. What is important is that the generative model affords adaptive motor
control, by translating the internal dynamics into commands to the motor actuators,
to change the environment in a predictable way. This is in keeping with the “good
regulator theorem”, which states that a good controller needs to either include or be
(embody) a model of a system [105–107]. One possibility is using generative mod-
els that only generate predictions at the level of the proximal (e.g., proprioceptive)
features, which are the closest consequences of motor commands, rather than distal
features. An example is a model of the active control of whisking behavior, in which the
generative model only predicts the (somatosensory and proprioceptive) consequences
of whisker movements. The model does not include any internal variable that directly
represents the distance from external objects or their identity, yet it is able to estimate
them implicitly [61]. The estimation strategy is based on the active control of whiskers.
Namely, whisker amplitude is continuously adapted to fit the (expected) distance to
objects and at convergence, it could be used as an implicit inference of animal-object
distance, as shown empirically [57,108]. Designing or learning appropriate generative
models is a key prerequisite to accurately model motor control (or other) tasks. Cur-
rent advances in machine learning permit inferring models from data, but it remains
to be investigated how to better incorporate them into Active Inference models [38].
Finally, another key issue that deserves further investigation is the link between
the biologically motivated aspects of the theory and the computational models used
in practical implementations e.g., deep neural networks. From a biological viewpoint,
hierarchical Active Inference assumes a temporally deep model based on predictive
coding, which uses local message passing of predictions and prediction errors across
brain areas. In principle, an architecture of this kind would allow the formation of
effective and increasingly more invariant representations of the sensory input at higher
22
levels of the cortical hierarchy, via a biologically motivated scheme [109]. However, in
practical implementations, it is common to use (deep) neural networks as generative
models [100] rather than hierarchical predictive coding. While using deep networks is
effective, it does not take advantage of the local message passing of predictive coding
such as in the hierarchical kinematic model of [52]. Furthermore, rather than assuming
prediction error minimization at every level of the hierarchy, deep networks often
only pass their final gradient to beliefs encoded at high hierarchical layers. A similar
argument could be made for precision control, which links to learning and attention
in Active Inference. While the precision of signals at every level should be inferred by
minimizing Free Energy, this is rarely done in practice. For example, in the studies
illustrated in Chapter 3, the precision matrices of latent states were fixed. In principle,
allowing Active Inference models to change the precision of signals at every hierarchical
level should make them more adaptive and effective, but this possibility remains to be
fully investigated in future studies.
References
[1] E. Todorov and M. I. Jordan, “Optimal feedback control as a theory of motor
coordination,” Nat Neurosci, vol. 5, no. 11, pp. 1226–1235, 2002.
[2] J. Diedrichsen, R. Shadmehr, and R. B. Ivry, “The coordination of movement:
optimal feedback control and beyond,”Trends Cogn Sci, vol. 14, no. 1, pp. 31–39,
2010.
[3] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cam-
bridge MA: MIT Press, 1998.
[4] N. D. Daw, Y. Niv, and P. Dayan, “Uncertainty-based competition between pre-
frontal and dorsolateral striatal systems for behavioral control,” Nat Neurosci,
vol. 8, no. 12, pp. 1704–1711, 2005.
[5] D. M. Wolpert and M. Kawato, “Multiple paired forward and inverse models for
motor control,” Neural Netw., vol. 11, no. 7-8, pp. 1317–1329, 1998.
[6] A. G. Greenwald, “Sensory feedback mechanisms in performance control: With
special reference to the ideomotor mechanism,”Psychol. Rev., vol. 77, pp. 73–99,
1970.
[7] J. Hoffmann, “Anticipatory behavioral control,” in Anticipatory Behavior in
Adaptive Learning Systems: Foundations, Theories, and Systems . M. V. Butz,
O. Sigaud, and P. Gerard, Eds. Berlin Heidelberg: Springer-Verlag, 2003, pp.
44–65.
[8] B. Hommel, J. Musseler, G. Aschersleben, and W. Prinz, “The theory of event
coding (tec): a framework for perception and action planning,” Behav. Brain
Sci, vol. 24, no. 5, pp. 849–78, 2001.
[9] H. R. Lotze, Medicinische Psychologie oder Physiologie der Seele . Leipzig:
Weidmannsche Buchhandlung, 1852.
[10] A. Wohlschlaeger, M. Gattis, and H. Bekkering, “Action generation and action
perception in imitation: An instance of the ideomotor principle,” Philos. Trans.
R. Soc. Lond., vol. 358, pp. 501–515, 2003.
[11] W. Kunde, “Response-effect compatibility in manual choice reaction tasks,” J.
Exp. Psychol. Hum. Percept. Perform. , vol. 27, pp. 387–394, 2001.
23
[12] B. Elsner, B. Hommel, C. Mentschel, A. Drzezga, W. Prinz, B. Conrad, and
H. Siebner, “Linking actions and their perceivable consequences in the human
brain,” NeuroImage, vol. 17, no. 1, pp. 364–372, 2002.
[13] B. Hommel, “The cognitive representation of action: Automatic integration of
perceived action effects,” Psychol. Res, vol. 59, pp. 176–186, 1996.
[14] W. Kunde, I. Koch, and J. Hoffmann, “Anticipated action effects affect the
selection, initiation, and execution of actions,” Q. J. Exp. Psychol. A , vol. 57,
no. 1, pp. 87–106, 2004.
[15] T. Melcher, M. Weidema, R. M. Eenshuistra, B. Hommel, and O. Gruber, “The
neural substrate of the ideomotor principle: an event-related fmri analysis,”
NeuroImage, vol. 39, no. 3, pp. 1274–1288, 2008.
[16] M. Paulus, S. Hunnius, M. van Elk, and H. Bekkering, “How learning to shake a
rattle affects 8-month-old infants’ perception of the rattle’s sound: electrophys-
iological evidence for action-effect binding in infancy,” Dev. Cogn. Neurosci. ,
vol. 2, no. 1, pp. 90–96, 2012.
[17] N. Wiener, Cybernetics: or Control and Communication in the Animal and the
Machine. Cambridge, MA: The MIT Press, 1948.
[18] G. A. Miller, E. Galanter, and K. H. Pribram, Plans and the Structure of Be-
havior. New York: Holt, Rinehart and Winston, 1960.
[19] W. Mansell, “Control of perception should be operationalized as a fundamental
property of the nervous system,” Top. Cogn. Sci , vol. 3, no. 2, pp. 257–261,
2011.
[20] W. T. Powers, Behavior: The control of perception . Hawthorne, NY: Aldine,
1973.
[21] A. Clark, Surfing Uncertainty: Prediction, Action, and the Embodied Mind. Ox-
ford University Press, 2016.
[22] J. Hohwy, The predictive mind. Oxford University Press, 2013.
[23] T. Parr, G. Pezzulo, and K. J. Friston, Active inference: the free energy principle
in mind, brain, and behavior. Cambridge, Massachusetts: The MIT Press, 2022.
[24] G. Pezzulo, Tracing the roots of cognition in predictive processing. Open Mind,
2017.
[25] K. S. Walsh, D. P. McGovern, A. Clark, and R. G. O’Connell, “Evaluating the
neurophysiological evidence for predictive processing as a model of perception,”
Ann. N. Y. Acad. Sci , vol. 1464, no. 1, pp. 242–268, 2020.
[26] R. Adams, S. Shipp, and K. J. Friston, Predictions not commands: active infer-
ence in the motor system . Brain Struct. Funct, 2012.
[27] G. Pezzulo, F. Rigoli, and K. J. Friston, “Active inference, homeostatic regu-
lation and adaptive behavioural control,” Prog. Neurobiol., vol. 136, pp. 17–35,
2015.
[28] A. Tschantz, L. Barca, D. Maisto, C. L. Buckley, A. K. Seth, and G. Pezzulo,
“Simulating homeostatic, allostatic and goal-directed forms of interoceptive con-
trol using active inference,” Biological Psychology, vol. 169, p. 108266, 2022.
[29] K. J. Friston, “A theory of cortical responses,” Philos Trans R Soc Lond B Biol
Sci, vol. 360, no. 1456, pp. 815–836, 2005.
24
[30] R. P. N. Rao and D. H. Ballard, “Predictive coding in the visual cortex: a func-
tional interpretation of some extra-classical receptive-field effects,” Nat. Neu-
rosci., vol. 2, no. 1, pp. 79–87, 1999.
[31] A. Anil Meera and M. Wisse, “Dynamic expectation maximization algorithm for
estimation of linear systems with colored noise,” Entropy, vol. 23, no. 10, 2021.
[32] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised filtering,” Math.
Probl. Eng, vol. 2010, pp. 1–34, 2010.
[33] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston, “Ac-
tive inference on discrete state-spaces: A synthesis,” Journal of Mathematical
Psychology, vol. 99, 2020.
[34] R. Smith, K. J. Friston, and C. J. Whyte, “A step-by-step tutorial on active
inference and its application to empirical data,” Journal of Mathematical Psy-
chology, vol. 107, p. 102632, 2022.
[35] F. Donnarumma, M. Costantini, E. Ambrosini, K. Friston, and G. Pezzulo,
“Action perception as hypothesis testing,” Cortex, vol. 89, pp. 45–60, 2017.
[36] A. Maselli, P. Lanillos, and G. Pezzulo, “Active inference unifies intentional and
conflict-resolution imperatives of motor control,” PLOS Comput. Biol , vol. 18,
no. 6, 2022.
[37] L. Pio-Lopez, A. Nizard, K. Friston, and G. Pezzulo, “Active inference and robot
control: a case study,” J. R. Soc. Interface , vol. 13, p. 122, 2016.
[38] T. Taniguchi, S. Murata, M. Suzuki, D. Ognibene, P. Lanillos, E. Ugur, L. Ja-
mone, T. Nakamura, A. Ciria, B. Lara, and G. Pezzulo, “World models and
predictive coding for cognitive and developmental robotics: Frontiers and chal-
lenges,” arXiv, vol. 14, 2023.
[39] M. J. Beal, “Variational algorithms for approximate bayesian inference,” Uni-
versity of London , 2015.
[40] K. Friston, “Hierarchical models in the brain,” PLoS Comput. Biol, vol. 4, no. 11,
2008.
[41] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth, “The free energy
principle for action and perception: A mathematical review,” J. Math. Psychol.,
vol. 81, pp. 55–79, 2017.
[42] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, and W. Penny, “Vari-
ational free energy and the laplace approximation,” NeuroImage, vol. 34, no. 1,
pp. 220–234, 2007.
[43] K. J. Friston, N. Trujillo-Barreto, and J. Daunizeau, “Dem: A variational treat-
ment of dynamic systems,” NeuroImage, vol. 41, no. 3, pp. 849–885, 2008.
[44] A. Jazwinski, Stochastic processes and filtering theory , ser. Mathematics in sci-
ence and engineering. New York, NY [u.a.]: Acad. Press, 1970, no. 64.
[45] G. Oliver, P. Lanillos, and G. Cheng, “Active inference body perception and
action for humanoid robots,” IEEE Trans. Cogn. Dev. Syst. , vol. 14, no. 2, pp.
462–471, 2022.
[46] K. Friston, “What is optimal about motor control?” Neuron, vol. 72, no. 3, pp.
488–498, 2011.
[47] A. K. Seth and K. J. Friston, “Active interoceptive inference and the emotional
brain,” Phil Trans R Soc B , vol. 371, no. 1708, 2016.
25
[48] M. Priorelli and I. P. Stoianov, “Flexible intentions: An active inference theory,”
bioRxiv, vol. 8, 2022.
[49] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”ArXiv Prepr.,
vol. 13126114, 2013.
[50] M. Priorelli and I. P. Stoianov, “Intention modulation for multi-step tasks in
continuous time active inference,” presented at the 3rd International Workshop
on Active Inference, 2022.
[51] H. Brown, R. A. Adams, I. Parees, M. Edwards, and K. Friston, “Active in-
ference, sensory attenuation and illusions,” Cogn. Process, vol. 14, no. 4, pp.
411–427, 2013.
[52] M. Priorelli, G. Pezzulo, and I. P. Stoianov, “Deep kinematic inference affords
efficient and scalable control of bodily movements,” bioRxiv, pp. 1–33, 2023.
[53] L. Perrinet, R. A. Adams, and K. J. Friston, “Active inference, eye movements
and oculomotor delays,” Biol. Cybern. Model, vol. 106, no. 8, pp. 777–801, 2014.
[54] T. Parr and K. J. Friston, “The computational pharmacology of oculomotion,”
Psychopharmacology (Berl.), vol. 236, no. 8, pp. 2473–2484, 2019.
[55] T. Parr and K. Friston, “Active inference and the anatomy of oculomotion,”
Neuropsychologia, vol. 111, no. January, pp. 334–343, 2018.
[56] M. Priorelli, G. Pezzulo, and I. Stoianov, “Active vision in binocular depth
estimation: a top-down perspective,” bioRxiv, 2023.
[57] E. Ahissar and E. Assa, “Perception as a closed-loop convergence process,”eLife,
vol. 5, 2016.
[58] B. Morillon, T. A. Hackett, Y. Kajikawa, and C. E. Schroeder, “Predictive motor
control of sensory dynamics in auditory active sensing,” Curr. Opin. Neurobiol.,
vol. 31, pp. 230–238, 2015.
[59] N. O. Zweifel and M. J. Z. Hartmann, “Defining ‘active sensing’ through an anal-
ysis of sensing energetics: homeoactive and alloactive sensing,” J. Neurophysiol.,
vol. 124, no. 1, pp. 40–48, 2020.
[60] K. Friston, R. A. Adams, L. Perrinet, and M. Breakspear, “Perceptions as hy-
potheses: saccades as experiments,” Front. Psychol., vol. 3, p. 151, 2012.
[61] F. Mannella, F. Maggiore, M. Baltieri, and G. Pezzulo, “Active inference through
whiskers,” Neural Networks, vol. 144, pp. 428–437, 2021.
[62] E. Ambrosini, M. Costantini, and C. Sinigaglia, “Grasping with the eyes,” J.
Neurophysiol., vol. 106, no. 3, pp. 1437–1442, 2011.
[63] M. Botvinick and J. Cohen, “Rubber hands ‘feel’ touch that eyes see,” Nature,
vol. 391, no. 6669, p. 756, 1998.
[64] H. H. Ehrsson, C. Spence, and R. E. Passingham, “That’s my hand! activity in
premotor cortex reflects feeling of ownership of a limb,” Science, vol. 305, no.
5685, pp. 875–877, 2004.
[65] A. Maselli and M. Slater, “The building blocks of the full body ownership illu-
sion,” Front. Hum. Neurosci., vol. 7, p. 83, 2013.
[66] V. Petkova and H. Ehrsson, If I were you: perceptual illusion of body swapping .
PloS One, 2008.
26
[67] M. Chancel, H. H. Ehrsson, and W. J. Ma, “Uncertainty-based inference of a
common cause for body ownership,” eLife, vol. 11, p. e77221, 2022.
[68] K. Kilteni, A. Maselli, K. P. Kording, and M. Slater, “Over my fake body: body
ownership illusions for studying the multisensory basis of own-body perception,”
Front. Hum. Neurosci., vol. 9, 2015.
[69] M. Samad, A. J. Chung, and L. Shams, “Perception of body ownership is driven
by bayesian sensory inference,” Plos One, vol. 10, no. 2, 2015.
[70] D. M. Lloyd, “Spatial limits on referred touch to an alien limb may reflect
boundaries of visuo-tactile peripersonal space surrounding the hand,” Brain and
cognition, vol. 64, no. 1, pp. 104–109, 2007.
[71] A. Maselli, K. Kilteni, J. L´ opez-Moliner, and M. Slater, “The sense of body
ownership relaxes temporal constraints for multisensory integration,” Scientific
reports, vol. 6, no. 1, p. 30628, 2016.
[72] P. Lanillos, S. Franklin, A. Maselli, and D. W. Franklin, “Active strategies for
multisensory conflict suppression in the virtual hand illusion,” Sci. Rep, vol. 11,
no. 1, pp. 1–14, 2021.
[73] T. Asai, “Illusory body-ownership entails automatic compensative movement:
for the unified representation between body and action,” Exp. Brain Res , vol.
233, no. 3, pp. 777–785, 2015.
[74] M. Gonzalez-Franco, B. Cohn, D. Burin, and A. Maselli, “The self-avatar fol-
lower effect in virtual reality,” IEEE Conf. Virtual Real. 3D User Interfaces ,
p. 8, 2020.
[75] J. Limanowski and K. Friston, “Active inference under visuo-proprioceptive con-
flict: Simulation and empirical results,” Scientific reports, vol. 10, no. 1, p. 4010,
2020.
[76] J. P. Gallivan, C. S. Chapman, D. M. Wolpert, and J. R. Flanagan, “Decision-
making in sensorimotor control,” Nat. Rev. Neurosci., vol. 19, no. 9, 2018.
[77] N. F. Lepora and G. Pezzulo, “Embodied choice: how action influences percep-
tual decision making,” PLoS Comput Biol , vol. 11, no. 4, 2015.
[78] G. Pezzulo and P. Cisek, “Navigating the affordance landscape: Feedback control
as a process model of behavior and cognition,” Trends Cogn. Sci, vol. 20, no. 6,
pp. 414–424, 2016.
[79] K. J. Friston, T. Parr, and B. de Vries, “The graphical brain: Belief propagation
and active inference,” Netw. Neurosci., vol. 1, no. 4, pp. 381–414, 2017.
[80] K. Friston and W. Penny, “Post hoc bayesian model selection,” Neuroimage,
vol. 56, no. 4, pp. 2089–2099, 2011.
[81] T. Parr and K. J. Friston, “The discrete and continuous brain: From decisions
to movement - and back again,” Neural Comput., vol. 30, no. 9, pp. 2319–2347,
2018.
[82] S. Funahashi, C. J. Bruce, and P. S. Goldman-Rakic, “Mnemonic coding of
visual space in the monkey’s dorsolateral prefrontal cortex,” J. Neurophysiol.,
vol. 61, no. 2, pp. 331–349, 1989.
[83] K. J. Friston, N. Sajid, D. R. Quiroga-Martinez, T. Parr, C. J. Price, and
E. Holmes, “Active listening,” Hear. Res., vol. 399, p. 107998, 2021.
27
[84] T. Parr, J. Limanowski, V. Rawji, and K. Friston, “The computational neurology
of movement under active inference,”Brain, vol. 144, no. 6, pp. 1799–1818, 2021.
[85] M. Priorelli and I. P. Stoianov, “Slow but flexible or fast but rigid? discrete and
continuous processes compared,” bioRxiv, 2023.
[86] M. Priorelli and I. Stoianov, “Dynamic inference by model reduction,” bioRxiv,
2023.
[87] L. F. Barrett and W. K. Simmons, “Interoceptive predictions in the brain,” Nat.
Rev. Neurosci., vol. 16, no. 7, pp. 419–429, 2015.
[88] L. Barca and G. Pezzulo, “Interoceptive inference in anorexia nervosa,” 2018,
open Science Framework, Preprint.
[89] K. J. Friston, K. E. Stephan, R. Montague, and R. J. Dolan, “Computational
psychiatry: the brain as a phantastic organ,” Lancet Psychiatry, vol. 1, no. 2,
pp. 148–158, 2014.
[90] S. S. Khalsa, R. Adolphs, O. G. Cameron, H. D. Critchley, P. W. Davenport,
J. S. Feinstein, J. D. Feusner, S. N. Garfinkel, R. D. Lane, W. E. Mehling, A. E.
Meuret, C. B. Nemeroff, S. Oppenheimer, F. H. Petzschner, O. Pollatos, J. L.
Rhudy, L. P. Schramm, W. K. Simmons, M. B. Stein, K. E. Stephan, O. V.
den Bergh, I. V. Diest, A. von Leupoldt, and M. P. Paulus, “Interoception and
mental health: A roadmap,” Biol. Psychiatry Cogn. Neurosci. Neuroimaging ,
vol. 3, no. 6, pp. 501–513, 2018.
[91] D. Maisto, L. Barca, O. V. den Bergh, and G. Pezzulo, Perception and misper-
ception of bodily symptoms from an Active Inference perspective: Modelling the
case of panic disorder . Psychological Review, 2021.
[92] M. P. Paulus, J. S. Feinstein, and S. S. Khalsa, “An active inference approach
to interoceptive psychopathology,” Annu. Rev. Clin. Psychol., vol. 15, no. 1, pp.
97–122, 2019.
[93] G. Pezzulo, D. Maisto, L. Barca, and O. V. den Bergh, “Symptom perception
from a predictive processing perspective,” Clinical Psychology in Europe, 2019.
[94] O. V. den Bergh, M. Witth”oft, S. Petersen, and R. J. Brown, “Symptoms and
the body: Taking the inferential leap,” Neurosci. Biobehav. Rev. , vol. 74, pp.
185–203, 2017.
[95] O. C ¸ atal, T. Verbelen, T. V. de Maele, B. Dhoedt, and A. Safron, “Robot
navigation as hierarchical active inference,” Neural Netw., vol. 142, pp. 192–204,
2021.
[96] A. Ciria, G. Schillaci, G. Pezzulo, V. V. Hafner, and B. Lara, “Predictive pro-
cessing in cognitive robotics: A review,” Neural Comput. , vol. 33, no. 5, pp.
1402–1432, 2021.
[97] P. Lanillos, C. Meo, C. Pezzato, A. A. Meera, M. Baioumy, W. Ohata,
A. Tschantz, B. Millidge, M. Wisse, C. L. Buckley, and J. Tani, “Active in-
ference in robotics and artificial agents: Survey and challenges,” arXiv, vol. 03,
2021.
[98] T. Matsumoto and J. Tani, “Goal-directed planning for habituated agents by
active inference using a variational recurrent neural network,” Entropy, vol. 22,
no. 5, 2020.
28
[99] P. Mazzaglia, T. Verbelen, O. C ¸ atal, and B. Dhoedt, “The free energy principle
for perception and action: A deep learning perspective,” Entropy, vol. 24, no. 2,
2022.
[100] C. Sancaktar, M. van Gerven, and P. Lanillos, “End-to-end pixel-based deep
active inference for body perception and action,” Jt. IEEE 10th Int. Conf. Dev.
Learn. Epigenetic Robot. ICDL-EpiRob, vol. 2020, pp. 1–8, 2020.
[101] K. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and behavior: a
free-energy formulation,” Biol Cybern, vol. 102, no. 3, pp. 227–260, 2010.
[102] M. Baioumy, P. Duckworth, B. Lacerda, and N. Hawes, “Active inference for
integrated state-estimation, control, and learning,” in 2021 IEEE International
Conference on Robotics and Automation (ICRA) , 2021, pp. 4665–4671.
[103] K. Friston, J. Daunizeau, and S. J. Kiebel, “Reinforcement learning or active
inference?” PLoS One, vol. 4, no. 7, 2009.
[104] M. Baioumy, C. Pezzato, R. Ferrari, and N. Hawes, “Unbiased active inference
for classical control,” in 2022 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) . IEEE, 2022, pp. 12 787–12 794.
[105] R. C. Conant and W. R. Ashby, “Every good regulator of a system must be a
model of that system,” Intl J Syst. Sci , pp. 89–97, 1970.
[106] G. Pezzulo, F. Donnarumma, P. Iodice, D. Maisto, and I. Stoianov, “Model-
based approaches to active perception and control,” Entropy, vol. 19, no. 6, p.
266, 2017.
[107] A. K. Seth, The Cybernetic Bayesian Brain. Open MIND. Frankfurt am Main:
MIND Group, 2014.
[108] J. Voigts, D. H. Herman, and T. Celikel, “Tactile object localization by antici-
patory whisker motion,” J. Neurophysiol., vol. 113, no. 2, pp. 620–632, 2015.
[109] A. Ororbia and D. Kifer, “The neural coding framework for learning generative
models,” Nat. Commun., vol. 13, no. 1, 2022.
Acknoledgments
This research received funding from the European Research Council under the Grant
Agreement No. 820213 to GP, the European Union’s Horizon 2020 Framework Pro-
gramme for Research and Innovation under the Specific Grant Agreements 945539 to
GP and 951910 to I.S., and the Italian Ministry for Research under Grant Agreements
2017KZNZLN to IS, 2020529PCP to FD and PE0000013-FAIR and IR0000011–EBRAINS-
Italy to GP.
29
Supplementary
Figure S1: Illustration of the performance of the Active Inference model of [48] during a
target tracking task. The left and right panels show the performance of the model during
the reaching movement and target estimation. Each line corresponds to a trial. L2 distance
between the real hand and target over time (left), and error between the real and estimated
target positions over time (right). Both decrease in most trials, as the arm successfully reaches
the target (the dotted line indicates the minimum distance from the target to consider a trial
successful).
Figure S2:Controlling a simplified humanoid body composed of 23-DoF. The goal is reaching
3 different target locations, with the left knee and the two arms.
30
Figure S3: Controlling a simplified humanoid body composed of 23-DoF. The task consists
of avoiding a dynamic obstacle with the whole body.
Figure S4: Sequence of time frames of a depth estimation task with simultaneous target
fixation. The agent uses alternating action-perception phases to avoid being stuck during the
minimization process. The eyes are represented in blue, and the real and estimated target
position in red and orange. The fixation trajectory (when vergence occurs) is represented
in cyan. Each frame is composed of three images: a view of the overall task (top), and the
projection of the target to the two camera planes of the eyes (bottom left and bottom right).
31