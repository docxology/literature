arXiv:1806.08083v1  [cs.AI]  21 Jun 2018
Expanding the Active Inference Landscape: More Intrinsic
Motivations in the Perception-Action Loop
Martin Biehl 1, ∗ Christian Guckelsberger 2 Christoph Salge 3, 4
Sim´ on C. Smith4, 5 Daniel Polani 4
1Araya Inc., Tokyo, Japan
2Computational Creativity Group, Department of Computing, Golds miths, University
of London, London, UK
3Game Innovation Lab, Department of Computer Science and Engine ering, New York
University, New York City, NY, USA
4Sepia Lab, Adaptive Systems Research Group, Department of Com puter Science,
University of Hertfordshire, Hatﬁeld, UK
5Institute of Perception, Action and Behaviour, School of Inform atics, The University of
Edinburgh, UK
Abstract
Active inference is an ambitious theory that treats percept ion, inference and action selection of
autonomous agents under the heading of a single principle. I t suggests biologically plausible explana-
tions for many cognitive phenomena, including consciousne ss. In active inference, action selection is
driven by an objective function that evaluates possible fut ure actions with respect to current, inferred
beliefs about the world. Active inference at its core is inde pendent from extrinsic rewards, resulting
in a high level of robustness across e.g. diﬀerent environme nts or agent morphologies. In the liter-
ature, paradigms that share this independence have been sum marised under the notion of intrinsic
motivations. In general and in contrast to active inference , these models of motivation come without
a commitment to particular inference and action selection m echanisms. In this article, we study if
the inference and action selection machinery of active infe rence can also be used by alternatives to
the originally included intrinsic motivation. The percept ion-action loop explicitly relates inference
and action selection to the environment and agent memory, an d is consequently used as foundation
for our analysis. We reconstruct the active inference appro ach, locate the original formulation within,
and show how alternative intrinsic motivations can be used w hile keeping many of the original fea-
tures intact. Furthermore, we illustrate the connection to universal reinforcement learning by means
of our formalism. Active inference research may proﬁt from c omparisons of the dynamics induced
∗martin@araya.org
1
by alternative intrinsic motivations. Research on intrins ic motivations may proﬁt from an additional
way to implement intrinsically motivated agents that also s hare the biological plausibility of active
inference.
Contents
1 Introduction
3
2 Related Work 6
3 Structure of this Article 7
4 Notation 7
5 Perception-Action Loop 8
5.1 PA-loop Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6 Inference and Complete Posteriors 11
6.1 Generative Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6.2 Bayesian Complete Posteriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Connection to Universal Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 17
6.4 Approximate Complete Posteriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
7 Action Selection Based on Intrinsic Motivations 23
7.1 Intrinsic Motivation and Action-Value Functions . . . . . . . . . . . . . . . . . . . . . . . 23
7.2 Deterministic and Stochastic Action Selection . . . . . . . . . . . . . . . . . . . . . . . . . 25
7.3 Intrinsic Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.3.1 Free Energy Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.3.2 Free Energy Principle Specialised to Friston et al. (2015) . . . . . . . . . . . . . . 30
7.3.3 Empowerment Maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
7.3.4 Predictive Information Maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . 34
7.3.5 Knowledge Seeking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
8 Active Inference 35
9 Applications and Limitations 40
10 Conclusion 42
A Posterior Factor 44
B Approximate Posterior Predictive Distribution 45
2
C Notation Translation Tables 46
1 Introduction
Active inference ( Friston et al. , 2012), and a range of other formalisms usually referred to as intrinsic mo -
tivations ( Storck et al. , 1995; Klyubin et al. , 2005; Ay et al. , 2008), all aim to answer a similar question:
Under minimal assumptions, how should an agent act?. More practic ally, they relate to what would be a
universal way to generate behaviour for an agent or robot that a ppropriately deals with its environment,
i.e. acquires the information needed to act and acts towards an intr insic goal. To this end, both the free
energy principle and intrinsic motivations aim to bridge the gap betwee n giving a biologically plausible
explanation for how real organism deal with the problem and providin g a formalism that can be imple-
mented in artiﬁcial agents. Additionally, they share a range of prop erties, such as an independence of a
priori semantics and being deﬁned purely on the dynamics of the age nt environment interaction, i.e. the
agent’s perception-action loop.
Despite these numerous similarities, as far as we know, there has no t been any uniﬁed or comparative
treatment of those approaches. We believe this is in part due to a lac k of an appropriate unifying math-
ematical framework. To alleviate this, we present a technically comp lete and comprehensive treatment
of active inference, including a decomposition of its perception and a ction selection modes. Such a de-
composition allows us to relate active inference and the inherent mot ivational principle to other intrinsic
motivation paradigms such as empowerment ( Klyubin et al. , 2005), predictive information ( Ay et al. ,
2008), and knowledge seeking ( Storck et al. , 1995; Orseau et al. , 2013). Furthermore, we are able to
clarify the relation to universal reinforcement learning ( Hutter, 2005). Our treatment is deliberately
comprehensive and complete, aiming to be a reference for readers interested in the mathematical funda-
ment.
A considerable number of articles have been published on active infer ence (e.g. Friston et al. , 2012,
2015, 2016b,a, 2017a,b; Linson et al. , 2018). Active inference deﬁnes a procedure for both perception
and action of an agent interacting with a partially observable environ ment. The deﬁnition of the method,
in contrast to other existing approaches (e.g. Hutter, 2005; Doshi-Velez et al. , 2015; Leike, 2016), does
not maintain a clear separation between the inference and the actio n selection mechanisms, and the
objective function. Most approaches for perception and action s election are generally formed of three
steps: The ﬁrst step involves a learning or inference mechanism to u pdate the agent’s knowledge about
the consequences of its actions. In a second step, these conseq uences are evaluated with respect to an
agent-internal objective function. Finally, the action selection me chanism chooses an action depending
on the preceding evaluation.
In active inference, these three elements are entangled. On one h and, there is the main feature of
active inference: the combination of knowledge updating and action selection into a single mechanism.
This single mechanism is the minimisation of a “variational free energy” (Friston et al. , 2015, p.188). The
“inference” part of the name is justiﬁed by the formal resemblanc e of the method to the variational free
energy minimisation (also known as evidence lower bound maximisation) used in variational inference.
3
Variational inference is a way to turn Bayesian inference into an opt imisation problem which gives rise
to an approximate Bayesian inference method ( Wainwright and Jordan , 2007). The “active” part is
justiﬁed by the fact that the output of this minimisation is a probabilit y distribution over actions from
which the actions of the agent are then sampled. Behaviour in active inference is thus the result of a
variational inference-like process. On the other hand, the funct ion (i.e. expected free energy) that induces
the objective function in active inference is said to be “of the same f orm” as the variational free energy
(Friston et al. , 2017a, p.2673) or even to “follow” from it ( Friston et al. , 2016b, p.10). This suggests that
expected free energy is the only objective function compatible with active inference.
In summary, perception and action in active inference intertwines f our elements: variational approx-
imation, inference, action selection, and an objective function. Be sides these formal features, active
inference is of particular interest for its claims on biological plausibility and its relationship to the ther-
modynamics of dissipative systems. According to Friston et al. (2012, Section 3), active inference is a
“corollary” to the free energy principle. Therefore, it is claimed, ac tions must minimise variational free en-
ergy to resist the dispersion of states of self-organising systems (see also Friston, 2013b; Allen and Friston ,
2016). Active inference has also been used to reproduce a range of neu ral phenomena in the human brain
(Friston et al. , 2016b), and the overarching free energy principle has been proposed as a “uniﬁed brain
theory” Friston (2010). Furthermore, the principle has been used in a hierarchical formu lation as the-
oretical underpinning of the predictive processing framework ( Clark, 2015, pp. 305-306), successfully
explaining a wide range of cognitive phenomena. Of particular interes t for the present special issue, the
representation of probabilities in the active inference framework is conjectured to be related to aspects
of consciousness ( Friston, 2013a; Linson et al. , 2018).
These strong connections between active inference and biology, s tatistical physics, and consciousness
research make the method particularly interesting for the design o f artiﬁcial agents that can interact
with- and learn about unknown environments. However, it is curren tly not clear to which extent active
inference allows for modiﬁcations. We ask: how far do we have to com mit to the precise combination of
elements used in the literature, and what becomes interchangeable ?
One target for modiﬁcations is the objective function. In situation s where the environment does
not provide a speciﬁc reward signal and the goal of the agent is not directly speciﬁed, researchers often
choose the objective function from a range of intrinsic motivations . The concept of intrinsic motivation
was introduced as a psychological concept by ( Ryan and Deci , 2000), and is deﬁned as “the doing of an
activity for its inherent satisfactions rather than for some separ able consequence”. The concept helps
us to understand one important aspect of consciousness: the as signment of aﬀect to certain experiences,
e.g. the experience of fun ( Dennett, 1991) when playing a game. Computational approaches to intrinsic
motivations ( Oudeyer and Kaplan , 2009; Schmidhuber, 2010; Santucci et al. , 2013) can be categorised
roughly by the psychological motivations they are imitating, e.g. driv es to manipulate and explore,
the reduction of cognitive dissonance, the achievement of optimal incongruity, and ﬁnally motivations
for eﬀectance, personal causation, competence and self-dete rmination. Intrinsic motivations have been
used to enhance behaviour aimed at extrinsic rewards ( Sutton and Barto , 1998), but their deﬁning
characteristic is that they can serve as a goal-independent motiva tional core for autonomous behaviour
4
generation. This characteristic makes them good candidates for t he role of value functions for the design
of intelligent systems ( Pfeifer et al. , 2005). We attempt to clarify how to modify active inference to
accommodate objective functions based on diﬀerent intrinsic motiv ations. This may allow future studies
to investigate whether and how altering the objective function aﬀe cts the biological plausibility of active
inference.
Another target for modiﬁcation, originating more from a theoretic al standpoint, is the variational
formulation of active inference. As mentioned above, variational in ference formulates Bayesian inference
as an optimisation problem; a family of probability distributions is optimis ed to approximate the di-
rect, non-variational Bayesian solution. Active inference is formu lated as an optimisation problem as
well. We consequently ask: is active inference the variational formu lation of a direct (non-variational)
Bayesian solution? Such a direct solution would allow a formally simple for mulation of active inference
without recourse to optimisation or approximation methods, at the cost of sacriﬁcing tractability in most
scenarios.
To explore these questions, we take a step back from the establish ed formalism, gradually extend
the active inference framework, and comprehensively reconstru ct the version presented in Friston et al.
(2015). We disentangle the four components of approximation, inferenc e, action selection, and objective
functions that are interwoven in active inference.
One of our ﬁndings, from a formal point of view, is that expected fr ee energy can be replaced by other
intrinsic motivations. Our reconstruction of active inference then yields a uniﬁed formal framework that
can accommodate:
• Direct, non-variational Bayesian inference in combination with stan dard action selection schemes
known from reinforcement learning as well as objective functions in duced by intrinsic motivations.
• Universal reinforcement learning through a special choice of the e nvironment model and a small
modiﬁcation of the action selection scheme.
• Variational inference in place of the direct Bayesian approach.
• Active inference in combination with objective functions induced by in trinsic motivations.
We believe that our framework can beneﬁt active inference resear ch as a means to compare the dy-
namics induced by alternative action selection principles. Furthermo re, it equips researchers on intrinsic
motivations with additional ways for designing agents that share th e biological plausibility of active
inference.
Finally, this article contributes to the research topic: Consciousne ss in Humanoid Robots, in several
ways. First, there have been numerous claims on how active inferen ce relates to consciousness or related
qualities, which we outlined earlier in the introduction. The most recen t work by Linson et al. (2018), also
part of this research topic, speciﬁcally discusses this relation, par ticularly in regards to assigning salience.
Furthermore, intrinsic motivations (including the free energy princ iple for this argument) have a range of
properties that relate to or are useful to a range of classical app roaches recently summarised as as Good
Old-Fashioned Artiﬁcial Consciousness (GOFAC, Manzotti and Chella , 2018). For example, embodied
5
approaches still need some form of value-function or motivation ( Pfeifer et al. , 2005), and beneﬁt from
the fact that intrinsic motivations are usually universal yet sensitiv e in regards to an agent’s embodiment.
The enactive AI framework ( Froese and Ziemke , 2009), another candidate for GOFAC, proposes further
requirements on how value underlying motivation should be grounded in constitutive autonomy and
adaptivity. Guckelsberger and Salge (2016a) present tentative claims on how empowerment maximisation
relates to these requirements in biological systems, and how it could contribute to realising them in
artiﬁcial ones. Finally, the idea of using computational approaches for intrinsic motivation goes back
to developmental robotics ( Oudeyer et al. , 2007), where it is suggested as way to produce a learning
and adapting robot, which could oﬀer another road to robot consc iousness. Whether these Good Old-
Fashioned approaches will ultimately be successful is an open quest ion, and Manzotti and Chella (2018)
asses them rather critically. However, extending active inference to alternative intrinsic motivations in
a uniﬁed framework allows to combine features of these two approa ches. For example it may bring
together the neurobiological plausibility of active inference and the constitutive autonomy aﬀorded by
empowerment.
2 Related Work
Our work is largely based on
Friston et al. (2015) and we adopt the setup and models from it. This
means many of our assumptions are due to the original paper. Rece ntly, Buckley et al. (2017) have
provided an overview of continuous-variable active inference with a focus on the mathematical aspects,
rather than the relationship to thermodynamic free energy, biolog ical interpretations or neural correlates.
Our work here is in as similar spirit but focuses on the discrete formula tion of active inference and
how it can be decomposed. As we point out in the text, the case of dir ect Bayesian inference with
separate action selection is strongly related to general reinforce ment learning ( Hutter, 2005; Leike, 2016;
Aslanides et al. , 2017). This approach also tackles unknown environments with- and in late r versions also
without externally speciﬁed reward in a Bayesian way. Other work fo cusing on unknown environments
with rewards are e.g. Ross and Pineau (2008) and Doshi-Velez et al. (2015). We would like to stress
that we do not propose agents using Bayesian or variational infere nce as competitors to any of the
existing methods. Instead, our goal is to provide an unbiased inves tigation of active inference with a
particular focus on extending the inference methods, objective f unctions and action-selection mechanisms.
Furthermore, these agents follow almost completely in a straightfo rward (if quite involved) way from the
model in Friston et al. (2015). A small diﬀerence is the extension to parameterisations of enviro nment
and sensor dynamics. These parameterisations can be found in Friston et al. (2016b).
We note that work on planning as inference ( Attias, 2003; Toussaint, 2009; Botvinick and Toussaint ,
2012) is generally related to active inference. In this line of work the prob ability distribution over actions
or action sequences that lead to a given goal speciﬁed as a sensor v alue is inferred. Since active inference
also tries to obtain a probability distribution over actions the approa ches are related. The formalisation
of the goal however diﬀers, at least at ﬁrst sight. How exactly the two approaches relate is beyond the
scope of this publication.
6
3 Structure of this Article
Going forward, we will ﬁrst outline our mathematical notation in Sect ion
4. We then introduce the
perception-action loop, which contains both agent and environmen t in Section 5. In Section 6 we intro-
duce the model used by Friston et al. (2015). We then show how to obtain beliefs about the consequences
of actions via both (direct) Bayesian inference (Section 6.2) and (approximate) variational inference (Sec-
tion 6.4). These beliefs are represented in the form of a set of complete po steriors. Such a set is a common
object but usually does not play a prominent role in Bayesian inferenc e. Here, it turns out to be a con-
venient structure for capturing the agent’ knowledge and descr ibing intrinsic motivations. Under certain
assumptions that we discuss in Section 6.3 the direct Bayesian case specialises to the belief updating of
the Bayesian universal reinforcement learning agent of Aslanides et al. (2017). We then discuss in Sec-
tion 7 how those beliefs (i.e. the set of complete posteriors) can induce ac tion-value functions (playing
the role of objective functions) via a given intrinsic motivation funct ion. We present standard (i.e. non-
active inference) ways to select actions based on such action-valu e functions. Then we look at diﬀerent
instances of intrinsic motivation functions. The ﬁrst is the “expect ed free energy” of active inference.
For this we explicitly show how our formalism produces the original exp ression in Friston et al. (2015).
Looking at the formulations of other intrinsic motivations it becomes clear that the expected free energy
relies on expressions quite similar or identical to those that occur in o ther intrinsic motivations. This
suggests that, at least in principle, there is no reason why active inf erence should only work with ex-
pected free energy as an intrinsic motivation. Finally, in Section 8 formulate active inference for arbitrary
action-value functions which include those induced by intrinsic motiva tions. Modifying the generative
model of Section 6.1 and looking at the variational approximation of its posterior comes c lose but does
not correspond to the original active inference of Friston et al. (2015). We explain the additional trick
that is needed.
In the appendices we provide some more detailed calculations as well a s notation translation tables
(Appendix C) from our own to those of Friston et al. (2015) and Friston et al. (2016b).
4 Notation
We will explain our notation in more detail in the text, but for readers that mostly look at equations
we give a short summary. Note that, Appendix C comprises a translation between Friston et al. (2015,
2016b) and the present notation. Mostly, we will denote random variables by upper case letters e.g.
X, Y, A, E, M, S, ... their state spaces by calligraphic upper case letters X , Y, A, E, M, S... , speciﬁc values
of random variables which are elements of the state spaces by lower case letters x, y, a, e, m, s, ... . An
exception to this are random variables that act as parameters of p robability distributions. For those, we
use upper case Greek letters Ξ , Φ , Θ , ... , for their usually continuous state spaces we use ∆ Ξ , ∆ Θ , ∆ Φ , ...
and for speciﬁc values the lower case Greek letters ξ, φ, θ, ... . In cases where a random variable plays the
role of an estimate of another variable X, we write the estimate as ˆX, its state space as ˆX and its values
as ˆx.
7
We distinguish diﬀerent types of probability distributions with letters p, q, r and d. Here, p corre-
sponds to probability distributions describing properties of the phy sical world including the agent and
its environment, q identiﬁes model probabilities used by the agent int ernally, r denotes approximations
of such model probabilities which are also internal to the agent, and d denotes a probability distribution
that can be replaced by a q or a r distribution. We write conditional pr obabilities in the usual way, e.g.
p(y|x). For a model of this conditional probability parameterised by θ, we write q(ˆy|ˆx, θ ).
5 Perception-Action Loop
E1 E2
S1 S2
A1 A2
M1 M2
E0
S0
Figure 1: First two time steps of the Bayesian network representin g the perception-action loop (PA-loop).
All subsequent time steps are identical to the one from time t = 1 to t = 2.
In this section we introduce an agent’s perception-action loop (PA- loop) as a causal Bayesian network.
This formalism forms the basis for our treatment of active inferenc e. The PA-loop should be seen as
specifying the (true) dynamics of the underlying physical system t hat contains agent and environment
as well as their interactions. In Friston’s formulation, the environm ent dynamics of the PA-loop are
referred to as the generative process . In general these dynamics are inaccessible to the agent itself.
Nonetheless, parts of these (true) dynamics are often assumed to be known to the agent in order to
simplify computation (see e.g. Friston et al. , 2015). We ﬁrst formally introduce the PA-loop as causal
Bayesian network, and then state speciﬁc assumptions for the re st of this article.
5.1 PA-loop Bayesian Network
Figure
1 shows an agent’s PA-loop, formalised as causal Bayesian network. The network describes the
following causal dependencies over time: At t = 0 an initial environment state e0 ∈ E leads to an initial
sensor value s0 ∈ S . This sensor value inﬂuences the memory state m1 ∈ M of the agent at time t = 1.
Depending on this memory state, action a1 ∈ A is performed which inﬂuences the transition of the
environment state from e0 to e1 ∈ E . The new environment state leads to a new sensor value s1 which,
together with the performed action a1 and the memory state m1, inﬂuence the next memory state m2.
The loop then continues in this way until a ﬁnal time step T .
8
We assume that all variables are ﬁnite and that the PA-loop is time-ho mogeneous1. We exclude the
ﬁrst transition from t = 0 to t = 1 from the assumption of time-homogeneity in order to avoid having to
pick an arbitrary action which precedes the investigated time-fram e. The ﬁrst transition is thus simpliﬁed
to p( m1|s0, a 0) := p( m1|s0). Under the assumption of time-homogeneity and the causal depe ndencies
expressed in Figure 1, the joint probability distribution over the entire PA-loop is deﬁned b y:
p(e0:T , s 0:T , a 1:T , m 1:T ) =
( T∏
t=1
p(at|mt) p(mt|st−1, a t−1) p(st|et) p(et|at, e t−1)
)
p(s0|e0) p(e0) (1)
where e0:T is shorthand for states ( e0, e 1, ..., e T ). In order to completely determine this distribution
we therefore have to specify the state spaces E, S, A, and M as well as the following probabilities and
mechanisms for all e0, e t, e t+1 ∈ E ; s0, s t ∈ S ; at, a t+1 ∈ A ; m1, m t, m t+1 ∈ M for t > 0:
• initial environment distribution: p( e0),1
• environment dynamics: p( et+1|at+1, e t),2
• sensor dynamics: p( st|et),3
• action generation: p( at|mt), 4
• initial memory step p( m1|s0), 5
• memory dynamics: p( mt+1|st, a t, m t). 6
In the following we will refer to a combination of initial environment dist ribution, environment dy-
namics, and sensor dynamics simply as an environment. Similarly, an agent is a particular combination
of initial memory step, memory dynamics, and action generation. Th e indexing convention we use here
is identical to the one used for the generative model (see Section 6.1) in Friston et al. (2015).
Also, note the dependence of Mt on St−1, Mt−1, and additionally At−1 in Figure 1. In the literature,
the dependence on At−1 is frequently not allowed ( Ay et al. , 2012; Ay and L¨ ohr, 2015). However, we
assume an eﬀerence-like update of the memory. Note that this dep endence in addition to the dependence
on mt−1 is only relevant if the actions are not deterministic functions of the m emory state 2. If action
selection is probabilistic, knowing the outcome at−1 of the action generation mechanism p( at−1|mt−1) will
convey more information than only knowing the past memory state mt−1. This additional information
can be used in inference about the environment state and fundame ntally change the intrinsic perspective
of an agent. We do not discuss these changes in more detail here bu t the reader should be aware of the
assumption.
In a realistic robot scenario, the action at, if it is to be known by the agent, can only refer to the
“action signal” or “action value” that is sent to the robot’s physical actuators. These actuators will
usually be noisy and the robot will not have access to the ﬁnal eﬀect of the signal it sends. The (noisy)
conversion of an action signal to a physical conﬁguration change o f the actuator is here seen as part of
the environment dynamics p( et|at, e t−1). Similarly, the sensor value is the signal that the physical sensor
1This means that all state spaces and transition probabiliti es are independent of the time step, e.g. Mt = Mt−1 and
p(st|et) = p(st−1|et−1).
2In the deterministic case there is a function f : M → A such that p(mt|st−1, a t−1, m t−1) =
p(mt|st−1, f (mt−1), m t−1) = p(mt|st−1, m t−1).
9
of the robot produces as a result of a usually noisy measurement, s o just like the actuator, the conversion
of a physical sensor conﬁguration to a sensor value is part of the s ensor dynamics p( st|et) which in turn
belongs to the environment. As we will see later, the actions and sen sor values must have well deﬁned
state spaces A and S for inference on an internal model to work. This further justiﬁes this perspective.
5.2 Assumptions
For the rest of this article we assume that the environment state s pace E, sensor state space S as well as
environment dynamics p( et+1|at+1, e t) and sensor dynamics p( st|et) are arbitrarily ﬁxed and that some
initial environmental state e0 is given. Since we are interested in intrinsic motivations, our focus is n ot
on speciﬁc environment or sensor dynamics but almost exclusively on action generation mechanisms of
agents that rely minimally on the speciﬁcs of these dynamics.
In order to focus on action generation, we assume that all the age nts we deal with here have the same
memory dynamics. For this, we choose a memory that stores all pas t sensor values s≺t = ( s0, s 1, ..., s t−1)
and actions a≺t = ( a1, a 2, ..., a t−1) in the memory state mt. This type of memory is also used in
Friston et al. (2015, 2016b) and provides the agent with all existing data about its interactions with the
environment. In this respect, it could be called a perfect memory. A t the same time, whatever the agent
learned from s≺t and a≺t that remains true based on the next time step’s s⪯ t+1 and a⪯ t+1 must be
relearned from scratch by the agent. A more eﬃcient memory use m ight store only a suﬃcient statistic
of the past data and keep reusable results of computations in memo ry. Such improvements are not part
of this article (see e.g. Fox and Tishby , 2016, for discussion).
Formally, the state space M of the memory is the set of all sequences of sensor values and actio ns
that can occur. Since there is only a sensor value and no action at t = 0, these sequences always begin
with a sensor value followed by pairs of sensor values and actions. Fu rthermore, the sensor value and
action at t = T are never recorded. Since we have assumed a time-homogeneous m emory state space M
we must deﬁne it so that it contains all these possible sequences fro m the start. Formally, we therefore
choose the union of the spaces of sequences of a ﬁxed length (simila r to a Kleene-closure):
M = S ∪
( T −1⋃
t=1
S × (S × A )t
)
. (2)
With this we can deﬁne the dynamics of the memory as:
p(m1|s0) : =



1 if m1 = s0
0 else.
(3)
p(mt|st−1, a t−1, m t−1) : =



1 if mt = mt−1st−1at−1
0 else.
(4)
This perfect memory may seem unrealistic and can cause problems if t he sensor state space is large
(e.g. high resolution images). However, we are not concerned with t his type of problem here. Usually,
10
the computation of actions based on past actions and sensor value s becomes a challenge of eﬃciency
long before storage limitations kick in: the necessary storage spac e for perfect memory only increases
linearly with time, while, as we show later, the number of operations fo r Bayesian inference increases
exponentially.
For completeness we also note how the memory dynamics look if action s are a deterministic function
f : M → A of the memory state. Recall that in this case we can drop the edge f rom At−1 to Mt in the
PA-loop in Figure 1 and have at = f(mt) so that we can deﬁne:
p(m1|s0) : =



1 if m1 = s0
0 else.
(5)
p(mt|st−1, m t−1) : =



1 if mt = mt−1st−1f(mt−1)
0 else.
(6)
Given a ﬁxed environment and the memory dynamics, we only have to d eﬁne the action generation
mechanism p( at|mt) to fully specify the perception-action loop. This is the subject of t he next two
sections.
In order to stay as close to
Friston et al. (2015) as possible, we ﬁrst explain the individual building
blocks that can be extracted from Friston’s active inference as de scribed in Friston et al. (2015). These
are the variational inference and the action selection. We then sho w how these two building blocks are
combined in the original formulation. We eventually leverage our sepa ration of components to show how
the action selection component can be modiﬁed, and thus extend th e active inference framework.
6 Inference and Complete Posteriors
Ultimately, an agent needs to select actions. Inference based on p ast sensor values and actions is only
needed if it is relevant to the action selection. Friston’s active infere nce approach promises to perform
action selection within the same inference step that is used to updat e the agent’s model of the environment.
In this section, we look at the inference component only and show ho w an agent can update a generative
model in response to observed sensor values and performed actio ns.
The natural way of updating such a model is Bayesian inference via B ayes’ rule. This type of inference
leads to what we call the complete posterior . The complete posterior represents all knowledge that the
agent can obtain about the consequences of its actions from its pa st sensor values and actions. In
Section
7 we discuss how the agent can use the complete posterior to decide w hat is the best action to
take.
Bayesian inference as straightforward recipe is usually not practic al due to computational costs. The
memory requirements of the complete posterior update increases exponentially with time and so does
the number of operations needed to select actions. To keep the co mputational tractable, we have to
limit ourselves to only use parts of the complete posterior. Further more, since the direct expressions
(even of parts) of complete posteriors are usually intractable, ap proximations are needed. Friston’s active
11
inference is committed to variational inference as an approximation technique. Therefore, we explain
how variational inference can be used as an approximation techniqu e. Our setup for variational inference
(generative model and approximate posterior) is identical to the o ne in Friston et al. (2015), but in this
section we ignore the inference of actions included there. We will look at the extension to action inference
in Section 7.
In the perception-action loop in Figure 1, action selection (and any inference mechanism used in the
course of it) depends exclusively on the memory state mt. As mentioned in Section 5, we assume that
this memory state contains all past sensor values s≺t and all past actions a≺t. To save space, we write
sa≺t := ( s≺t, a ≺t) to refer to both sensor values and actions. We then have:
mt = sa≺t. (7)
However, since it is more intuitive to understand inference with resp ect to past sensor values and actions
than in terms of memory, we use sa≺t explicitly here in place of mt.
6.1 Generative Model
ˆE1 ˆE2
ˆS1 ˆS2
ˆA1 ˆA2
ˆE0
ˆS0
Θ 3
Θ 2
Θ 1
Ξ 3
Ξ 2
Ξ 1
Figure 2: Bayesian network of the generative model with paramete rs Θ = ( Θ 1, Θ 2, Θ 3) and hyperpa-
rameters Ξ = (Ξ 1, Ξ 2, Ξ 3). Hatted variables are models / estimates of non-hatted counter parts in the
perception-action loop in Figure 1. An edge that splits up connecting one node to n nodes (e.g. Θ 2 to
ˆE1, ˆE2, ... ) corresponds to n edges from that node to all the targets under the usual Bayesian network
convention. Note that in contrast to the perception-action loop in Figure 1, imagined actions ˆAt have
no parents. They are either set to past values or, for those in the future, a probability distribution over
them must be assumed.
The inference mechanism, internal to the action selection mechanis m p( a|m), takes place on a hi-
erarchical generative model (or density, in the continuous case) . “Hierarchical” means that the model
has parameters and hyperparameters, and “generative” indicat es that the model relates parameters and
latent variables , i.e. the environment state, as “generative” causes to sensor va lues and actions as data
in a joint distribution. The generative model we investigate here is a p art of the generative model used
in Friston et al. (2015). For now, we omit the probability distribution over future actions a nd the “pre-
12
cision”, which are only needed for active inference and are discusse d later. The generative models in
Friston et al. (2016a,b, 2017a) are all closely related.
Note that we are not inferring the causal structure of the Bayes ian network or state space cardinalities,
but deﬁne the generative model as a ﬁxed Bayesian network with th e graph shown in Figure 2. It
is possible to infer the causal structure (see e.g. Ellis and Wong , 2008), but in that case, it becomes
impossible to represent the whole generative model as a single Bayes ian network ( Ortega, 2011).
The variables in the Bayesian network in Figure 2 that model variables occurring outside of p( a|m)
in the perception-action loop (Figure 1), are denoted as hatted versions of their counterparts. More
precisely:
• ˆs ∈ ˆS = S are modelled sensor values,
• ˆa ∈ ˆA = A are modelled actions,
• ˆe ∈ ˆE are modelled environment states.
To clearly distinguish the probabilities deﬁned by the generative mode l from the true dynamics, we use
the symbol q instead of p. In accordance with Figure 2, and also assuming time-homogeneity, the joint
probability distribution over all variables in the model until some ﬁnal modelled time ˆT is given by:
q(ˆe0:T , ˆs0:T , ˆa1:T , θ 1, θ 2, θ 3, ξ 1, ξ 2, ξ 3) :=
( T∏
t=1
q(ˆst|ˆet, θ 1) q(ˆet|ˆat, ˆet−1, θ 2) q(ˆat)
)
q(ˆs0|ˆe0, θ 1) q(ˆe0|θ3)
( 3∏
i=1
q(θi|ξi) q(ξi)
) (8)
Here, θ1, θ 2, θ 3 are the parameters of the hierarchical model, and ξ1, ξ 2, ξ 3 are the hyperparameters. To
save space, we combine the parameters and hyperparameters by writing
θ := ( θ1, θ 2, θ 3) (9)
ξ := ( ξ1, ξ 2, ξ 3). (10)
To fully specify the generative model, or equivalently a probability dist ribution over Figure
2, we have
to specify the state spaces ˆE, ˆS, ˆA and:
• q(ˆs|ˆe, θ 1) the sensor dynamics model,7
• q(ˆe′|ˆa′, ˆe, θ 2) the environment dynamics8
model,9
• q(ˆe0|θ3) the initial environment state model,10
• q(θ1|ξ1) the sensor dynamics prior,11
• q(θ2|ξ2) the environment dynamics prior,12
• q(θ3|ξ3) the initial environment state prior,13
• q(ξ1) sensor dynamics hyperprior, 14
• q(ξ2) environment dynamics hyperprior, 15
• q(ξ3) initial environment state hyperprior, 16
• ˆT last modelled time step, 17
• q(ˆat) for all t ∈ { 1, , ..., ˆT } the probability dis- 18
tribution over the actions at time t. 19
13
The state spaces of the parameters and hyperparameters are d etermined by the choice of ˆE, ˆS, ˆA. We
will see in Section 6.2 that ˆS = S and ˆA = A should be chosen in order to use this model for inference
on past sensor values and actions. For ˆE it is not necessary to set it equal to E for the methods described
to work. We note that if we set ˆE equal to the memory state space of Equation ( 2) the model and its
updates become equivalent to those used by the Bayesian universa l reinforcement learning agent Hutter
(2005) in a ﬁnite (environment and time-interval) setting (see Section 6.3).
The last modelled time step ˆT can be chosen as ˆT = T , but it is also possible to always set it to
ˆT = t + n, in which case n speciﬁes a future time horizon from current time step t. Such an agent would
model a future that goes beyond the externally speciﬁed last time s tep T . The dependence of ˆT on t
(which we do not denote explicitly) within p( a|m) is possible since the current time step t is accessible
from inspection of the memory state mt which contains a sensor sequence of length t.
The generative model assumes that the actions are not inﬂuenced by any other variables, hence we
have to specify action probabilities. This means that the agent does not model how its actions come
about, i.e. it does not model its own decision process. Instead, the agent is interested in the (parameters
of) the environment and sensor dynamics. It actively sets the pro bability distributions over past and
future actions according to its needs. In practice, it either ﬁxes t he probability distributions to particular
values (by using Dirac delta distributions) or to values that optimise s ome measure. We look into the
optimisation options in more detail later.
Note that the parameters and hyperparameters are standard r andom variables in the Bayesian net-
work of the model. Also, the rules for calculating probabilities accord ing to this model are just the rules
for calculating probabilities in this Bayesian network.
In what follows, we assume that the hyperparameters are ﬁxed as Ξ 1 = ξ1, Ξ 2 = ξ2, Ξ 3 = ξ3. The
following procedures (including both Bayesian and variational infere nce) can be generalised to also infer
hyperparameters. However, our main reference ( Friston et al. , 2015) and most publications on active
inference also ﬁx the hyperparameters.
6.2 Bayesian Complete Posteriors
During action generation (i.e. within p( a|m)) at time t, the agent has retained all its previously perceived
sensor states and its previously performed actions in memory. The “experience” or data contained in
its memory is thus mt = sa≺t. This data can be plugged into the generative model to obtain poste rior
probability distributions over all non-observed random variables. A lso, the model can estimate the not
yet observed sensor values ˆ st: ˆT , past and future unobservable environment states ˆ e0: ˆT , parameters θ and
hyperparameters ξ. These estimations are done by setting:
ˆAτ = aτ , for τ < t (11)
and
ˆSτ = sτ , for τ < t. (12)
14
ˆE1 ˆE2
s1 ˆS2
a1 ˆA2
ˆE0
s0
Θ 3
Θ 2
Θ 1
ξ3
ξ2
ξ1
Figure 3: Internal generative model with plugged in data up to t = 2 with ˆS0 = s0, ˆS1 = s1 and
ˆA1 = a1 as well as from now on ﬁxed hyperparameters ξ = ( ξ1, ξ 2, ξ 3). Conditioning on the plugged in
data leads to the posterior distribution q(ˆ st: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ). Predictions for future sensor values
can be obtained by marginalising out other random variables e.g. to pr edict ˆS2 we would like to get
q(ˆs2|s0, s 1, a 1, ξ ). Note however that this requires an assumption for the probabilit y distribution over
ˆA2.
as shown in Figure 3 for t = 2. For these assignments to be generally possible, we need to choo se ˆA and
ˆS equal to A and S respectively. The resulting posterior probability distribution over a ll non-observed
random variables is then, according to standard rules of calculating probabilities in a Bayesian network:
q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ) : = q(s≺t, ˆst: ˆT , ˆe0: ˆT , a ≺t, ˆat: ˆT , θ, ξ )∫∑
ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT
q(s≺t, ˆst: ˆT , ˆe0: ˆT , a ≺t, ˆat: ˆT , θ, ξ ) dθ. (13)
Eventually, the agent needs to evaluate the consequences of its f uture actions. Just as it can update
the model with respect to past actions and sensor values, the age nt can update its evaluations with
“contemplated” future action sequences ˆat: ˆT . For each such future action sequence ˆ at: ˆT , the agent obtains
a distribution over the remaining random variables in the model:
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) : = q(s≺t, ˆst: ˆT , ˆe0: ˆT , a ≺t, ˆat: ˆT , θ, ξ )∫∑
ˆst: ˆT , ˆe0: ˆT
q(s≺t, ˆst: ˆT , ˆe0: ˆT , a ≺t, ˆat: ˆT , θ, ξ ) dθ. (14)
We call each such distribution a Bayesian complete posterior . We choose the term complete posterior
since the “posterior” by itself usually refers to the posterior distr ibution over the parameters and latent
variables q( θ, ˆet−1|sa≺t, ξ ) (we here call this a posterior factor , see Equation ( 16)) and the posterior
predictive distributions marginalise out the parameters and latent v ariables to get q(ˆ st: ˆT |ˆat: ˆT , sa ≺t, ξ ).
The complete posteriors are probability distributions over all rando m variables in the generative model
including parameters, latent variables, and future variables. In th is sense the set of all (Bayesian)
complete posteriors represents the complete knowledge state of the agent at time t about consequences
of future actions after updating the model with past actions and o bserved sensor values sa≺t. At each
time step the sequence of past actions and sensor values is extend ed from sa≺t to sa≺t+1 (i.e. mt goes
to mt+1) and a new set of complete posteriors is obtained.
15
All intrinsic motivations discussed in this article evaluate future actio ns based on quantities that can
be derived from the corresponding complete posterior.
It is important to note that the complete posterior can be factoris ed into a term containing the
inﬂuence of past sensor values and actions (data). This factorisa tion can be made on the parameters
θ and ξ, the environment states ˆ e≺t, predicted future environment states ˆ et: ˆT and sensor values ˆ st: ˆT
depending on the future actions ˆ at: ˆT , and the estimated environment state ˆ et−1 and θ. Using the
conditional independence
SA≺t ⊥ ⊥ˆSt: ˆT , ˆEt: ˆT | ˆAt: ˆT , ˆEt−1, Θ , Ξ , (15)
which can be identiﬁed (via d-separation ( Pearl, 2000)) from the Bayesian network in Figure 3, we can
rewrite this as:
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) = q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ). (16)
This equation represents the desired factorisation. This formulat ion separates complete posteriors into
a predictive and a posterior factor. The predictive factor is given a s part of the generative model
(Equation ( 8))
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) =
ˆT∏
r=t
q(ˆsr|ˆer, θ 1) q(ˆer|ˆar, ˆer−1, θ 2) (17)
and does not need to be updated through calculations at diﬀerent t ime steps. This factor contains
the dependence of the complete posterior on future actions. This dependency reﬂects that, under the
given generative model, the consequences of actions for each com bination of Θ and ˆEt−1 remain the
same irrespective of experience. What changes when a new action a nd sensor value pair comes in is the
distribution over the values of Θ and ˆEt−1 and with them the expectations over consequences of actions.
On the other hand, the posterior factor must be updated at ever y time step. In Appendix A, we sketch
the computation which shows that it involves a sum over |E|t elements. This calculation is intractable
as time goes on and one of the reasons to use approximate inferenc e methods like variational inference.
Due to the above factorisation, we may only need to approximate th e posterior factor q(ˆe≺t, θ |sa≺t, ξ )
and use the exact predictive factor if probabilities involving future s ensor values or environment states
are needed.
This is the approach taken e.g. in Friston et al. (2015). However, it is also possible to directly approx-
imate parts of the complete posterior involving random variables in bo th factors , e.g. by approximating
q(ˆe0: ˆT , θ 1|ˆat: ˆT , sa ≺t, ξ ). This latter approach is taken in Friston et al. (2016b) and we see it again in
Equation ( 43) but in this publication the focus is on the former approach.
In the next section, we look at the special case of universal reinfo rcement learning before we go on
to variational inference to approximate the posterior factor of t he (Bayesian) complete posteriors.
16
6.3 Connection to Universal Reinforcement Learning
In this section, we relate the generative model of Equation (
8) and its posterior predictive distribution
to those used by the Bayesian universal reinforcement learning ag ent. Originally, this agent is deﬁned
by Hutter (2005). More recent work includes Leike (2016) and (for the current purpose suﬃcient and
particularly relevant) Aslanides et al. (2017).
Let us set ˆE = M with M as in Equation ( 2) and let the agent identify each past sa≺t with a state
of the environment, i.e.
ˆet−1 = sa≺t. (18)
Under this deﬁnition the next environment state ˆ et is just the concatenation of the last environment
state sa≺t with the next next action selected by the agent ˆ at and the next sensor value ˆ st:
ˆet = ˆsˆa⪯ t = sa≺t ˆsˆat. (19)
So given a next contemplated action ¯
ˆat the next environment state ˆ et is already partially determined.
What remains to be predicted is only the next sensor value ˆ st. Formally, this is reﬂected in the following
derivation:
q(ˆet|¯
ˆat, ˆet−1, θ 2) : = q(ˆst, ˆat, ˆsˆa≺t|¯
ˆat, sa ≺t, θ 2) (20)
= q(ˆst|ˆat, ˆsˆa≺t, ¯
ˆat, sa ≺t, θ 2) q(ˆat, ˆsˆa≺t|¯
ˆat, sa ≺t, θ 2) (21)
= q(ˆst|ˆat, ˆsˆa≺t, ¯
ˆat, sa ≺t, θ 2)δ¯
ˆat (ˆat)δsa≺t (ˆsˆa≺t) (22)
= q(ˆst|¯
ˆat, sa ≺t, θ 2)δ¯
ˆat (ˆat)δsa≺t (ˆsˆa≺t). (23)
This shows that in this case the model of the next environment stat e (the left hand side) is determined
by the model of the next sensor value q(ˆ st|¯
ˆat, sa ≺t, θ 2).
So instead of carrying a distribution over possible models of the next environment state such an agent
only needs to carry a distribution over models of the next sensor va lue. Furthermore, an additional
model q(ˆs|ˆe, θ 1) of the dependence of the sensor values on environment states p arameterised by θ1 is
superﬂuous. The next predicted sensor value is already predicted by the model q(ˆ st|ˆat, sa ≺t, θ 2). It is
therefore possible to drop the parameter θ1.
The parameter θ3, for the initial environment state distribution, becomes a distribut ion over the
initial sensor value since ˆ e0 = ˆs0:
q(ˆe0|θ3) = q(ˆs0|θ3). (24)
We can then derive the posterior predictive distribution and show th at it coincides with the one given
17
in Aslanides et al. (2017). For the complete posterior of Equation ( 16) we ﬁnd:
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) = q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) ( 16 revisited)
= q(ˆet: ˆT |ˆst: ˆT , ˆat: ˆT , ˆet−1, θ ) q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) (25)
= q(ˆst: ˆT |ˆat: ˆT , sa ≺t, θ ) q(θ|sa≺t, ξ )
t∏
τ =0
δsa≺τ (ˆeτ )
ˆT∏
τ =t+1
δsa≺t ˆsˆat:τ (ˆeτ ). (26)
To translate this formulation into the notation of Aslanides et al. (2017) ﬁrst drop the representation of
the environment state which is determined by the sensor values and actions anyway. This means that
the complete posterior only needs to predict future sensor values and parameters. Formally, this means
the complete posterior can be replaced without loss of generality:
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) → q(ˆst: ˆT |ˆat: ˆT , sa ≺t, θ ) q(θ|sa≺t, ξ ). (27)
To translate notations let θ → ν; ˆa, a → a; ˆs, s → e. Also, set ˆT → t because only one step futures are
considered in universal reinforcement learning (this is due to the us e of policies instead of future action
sequences). Then, the equation for the posterior predictive dist ribution
q(ˆst|ˆat, sa ≺t, ξ ) =
∫
q(ˆst|ˆat, sa ≺t, θ ) q(θ|sa≺t, ξ ) dθ, (28)
is equivalent to Aslanides et al. (2017, Eq. (5)) (the sum replaces the integral for a countable ∆ Θ ):
ξ(e|ae≺t, a ) =
∑
ν
p(e|ν, ae ≺t, a )p(ν|ae≺t) (29)
⇔ ξ(e) =
∑
ν
p(e|ν)p(ν), (30)
where we dropped the conditioning on ae≺t, a from the notation in the second line as done in the original
(where this is claimed to improve clarity). Also note that ξ(e) would be written q( e|ξ) in our notation.
In the universal reinforcement learning literature parameters like θ (or ν) and ξ are sometimes directly
used to denote the probability distribution that they parameterise .
Updating of the posterior q( θ|sa≺t, ξ ) in response to new data also coincides with updating of the
18
weights p(ν):
q(θ|sa⪯ t, ξ ) = q(θ, s t|at, sa ≺t, ξ )
q(st|at, sa ≺t, ξ ) (31)
= q(st|at, sa ≺t, θ, ξ ) q(θ|at, sa ≺t, ξ )
q(st|at, sa ≺t, ξ ) (32)
= q(st|at, sa ≺t, θ ) q(θ|sa≺t, ξ )
q(st|at, sa ≺t, ξ ) (33)
= q(st|at, sa ≺t, θ )
q(st|at, sa ≺t, ξ ) q(θ|sa≺t, ξ ). (34)
The ﬁrst two lines are general. From the second to third we used
St ⊥ ⊥Ξ |At, SA ≺t, Θ (35)
and
Θ ⊥ ⊥At|SA≺t, Ξ (36)
which follow from the Bayesian network structure Figure 2. In the notation of Aslanides et al. (2017)
Equation ( 34) becomes
p(ν|e) = p(e|ν)
p(e) p(ν). (37)
This shows that assuming the same model class ∆ Θ the predictions and belief updates of an agent
using the Bayesian complete posterior of Section 6.2 are the same as those of the Bayesian universal
reinforcement learning agent. Action selection can then be perfor med just as in Aslanides et al. (2017)
as well. This is done by selecting policies. In the present publication we in stead select action sequences
directly. However, in both cases the choice maximises the value pred icted by the model. More on this
in Section 7.2.
6.4 Approximate Complete Posteriors
As mentioned in the last section, the complete posterior can be appr oximated via variational inference
(see Attias, 1999; Winn and Bishop , 2005; Bishop, 2011; Blei et al. , 2017). There are alternative methods
such as belief propagation, expectation propagation ( Minka, 2001; Vehtari et al. , 2014), and sampling-
based methods ( Lunn et al. , 2000; Bishop, 2011), but active inference commits to variational inference by
framing inference as variational free energy minimisation ( Friston et al. , 2015). Variational free energy
(Equation ( 45)) is just the negative evidence lower bound (ELBO) of standard va riational inference
(e.g. Blei et al. , 2017). In the following, we show how the complete posterior can be appro ximated via
variational inference.
The idea behind variational inference is to use a simple family of probab ility distributions and identify
19
ˆE1ˆE0Θ 3
Θ 2
Θ 1
Φ 3
Φ 2
Φ 1
Φ E0 Φ E1
Figure 4: Bayesian network of the approximate posterior factor a t t = 2. The variational parameters
Φ 1, Φ 2, Φ 3 and Φ E≺t = (Φ E0 , Φ E1 ) are positioned so as to indicate what dependencies and nodes they
replace in the generative model in Figure 2.
ˆE1 ˆE2
ˆS2
ˆa2
ˆE0Θ 3
Θ 2
Θ 1
Φ 3
Φ 2
Φ 1
Φ E0 Φ E1
Figure 5: Bayesian network of the approximate complete posterior of Equation ( 40) at t = 2 for the future
actions ˆat: ˆT . Only ˆEt−1, Θ 1, Θ 2 and the future action ˆ at: ˆT appear in the predictive factor and inﬂuence
future variables. In general there is one approximate complete po sterior for each possible sequence ˆ at: ˆT
of future actions.
20
the member of that family which approximates the true complete pos terior best. This turns inference
into an optimisation problem. According to Wainwright and Jordan (2007) this reformulation as an
optimisation problem is the essence of variational methods. If the f amily of distributions is chosen
such that it includes the complete posterior then the optimisation will eventually lead to the same
result as Bayesian inference. However, one advantage of the for mulation as an optimisation is that it
can also be performed over a family of probability distributions that is simpler than the family that
includes the actual complete posterior. This is what turns variation al inference into an approximate
inference procedure. Usually, the (simpler) families of probability dis tributions are chosen as products
of independent distributions.
Recalling Equation ( 16), the complete posterior as a product of a predictive and a poster ior factor is:
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) = q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ). (16 revisited)
This product is the main object of interest. We want to approximate the formula with a probability
distribution that lets us (tractably) calculate the posteriors requ ired by a given intrinsic motivation,
which can consequently be used for action selection.
As mentioned before, to approximate the complete posterior we he re approximate only the posterior
factor and use the given generative model’s predictive factor as is d one in Friston et al. (2015)3 The
approximate posterior factor is then combined with the exact pred ictive factor to get the approximate
complete posterior. Let us write r(ˆ e≺t, θ |φ) for the approximate posterior factor (Figure 4), deﬁned as:
r(ˆe≺t, θ |φ) : = r(ˆe≺t|φE≺t ) r(θ|φ) (38)
: =
t−1∏
τ =0
r(ˆeτ |φEτ )
3∏
i=1
r(θi|φi). (39)
As we can see it models each of the random variables that the poster ior factor ranges over as independent
of all others. This is called a mean ﬁeld approximation. Then, the approximate complete posterior
(Figure
5) is:
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ) : = q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) r(ˆe≺t, θ |φ). (40)
Note that the variational parameter absorbs the hyperparamet er ξ as well as the past sensor values and
actions sa≺t. The parameter does not absorb future actions which are part of the predictive factor.
The dependence on future actions needs to be kept if we want to se lect actions using the approximate
complete posterior.
3A close inspection of Friston et al. (2015, Eq. (9)) shows that the approximate complete posterior tha t ends up being
evaluated by the action-value function is the one we discuss in Equation ( 40). It uses the predictive factor to get the
probabilities r(ˆet: ˆT |ˆat: ˆT , ˆet−1, φ ) of future environment states. However, the approximate po sterior in Friston et al. (2015,
Eq.(10)) uses a factorisation of all future environment sta tes like the one we give in Equation ( 43). The probabilities of
future environment states in that posterior are not used any where in Friston et al. (2015). In principle, they could be used
as is done in Friston et al. (2016b, Eq. (2.6)) where the complete posterior of Equation ( 43) is used in the action-value
function. Both approaches are possible.
21
We have:
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ) ≈ q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) (41)
if
r(ˆe≺t, θ |φ) ≈ q(ˆe≺t, θ |sa≺t, ξ ). (42)
This approximation can be achieved by standard variational inferen ce methods.
For those interested more in the approximation of the complete pos terior as in Friston et al. (2016b),
we provide the used family of factorised distributions. It must be no ted that the agent in this case
carries a separate approximate posterior for each possible comple te action sequence ˆa0:T . For predictions
of environment states, it does not use the predictive factor, but instead looks at the set of generative
models compatible with the past. For each of those, the agent cons iders all environment states at diﬀerent
times as independent. The approximate posteriors, compatible with a past sequence of actions a≺t, are
of the form:
r(ˆst: ˆT , ˆe0: ˆT , θ 1|ˆat: ˆT , a ≺t, φ 1) = q(ˆst: ˆT |ˆet: ˆT , θ 1)
ˆT∏
τ =0
r(ˆeτ |ˆat: ˆT , a ≺t, φ Eτ ) r(θ1|φ1). (43)
Note also that the relation between sensor values and environment states is still provided by the generative
models’ sensor dynamics q(ˆst: ˆT |ˆet: ˆT , θ 1). In this article however, we focus on the approach in Friston et al.
(2015) which requires only one approximate posterior at time t since future actions only occur in the
predictive factors which we do not approximate.
We deﬁne the relative entropy (or KL-divergence) between the ap proximate and the true posterior
factor:
KL[r( ˆE≺t, Θ |φ)|| q( ˆE≺t, Θ |sa≺t, ξ )] :=
∑
ˆe≺t
∫
r(ˆe≺t, θ |φ) log r(ˆe≺t, θ |φ)
q(ˆe≺t, θ |sa≺t, ξ ) dθ. (44)
Note that, we indicate the variables that are summed over by capita lising them. The KL-divergence quan-
tiﬁes the diﬀerence between the two distributions. It is non-negat ive, and only zero if the approximate
and the true posterior factor are equal (see e.g. Cover and Thomas , 2006).
The variational free energy, also known as the (negative) evidenc e lower bound (ELBO) in variational
inference literature, is deﬁned as:
F[ξ, φ, sa ≺t] : =
∑
ˆe≺t
∫
r(ˆe≺t, θ |φ) log r(ˆe≺t, θ |φ)
q(s⪯ t, ˆe≺t, θ |a≺t, ξ ) dθ (45)
= − log q(s≺t|a≺t, ξ ) + KL[r( ˆE≺t, Θ |φ)|| q( ˆE≺t, Θ |sa≺t, ξ )] (46)
The ﬁrst term in Equation ( 46) is the surprise of negative log evidence. For a ﬁxed hyperparamet er ξ
it is a constant. Minimising the variational free energy therefore dir ectly minimises the KL-divergence
between the true and the approximate posterior factor given sa≺t and ξ.
22
In our case, variational inference amounts to solve the optimisatio n problem:
φ∗
sa≺t,ξ := arg min
φ
F[φ, sa ≺t, ξ ]. (47)
This optimisation is a standard problem. See
Bishop (2011); Blei et al. (2017) for ways to solve it.
The resulting variational parameters φ∗
sa≺t ,ξ = ( φE0
sa≺t,ξ , ..., φ Et−1
sa≺t ,ξ , φ 1
sa≺t,ξ , φ 2
sa≺t,ξ , φ 3
sa≺t,ξ ) deﬁne
the approximate posterior factor. The variational parameters, together with the exact predictive factors,
allow us to compute the approximate complete posteriors for each s equence of future actions ˆ at: ˆT :
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ∗
sa≺t ,ξ ) = q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) r(ˆe≺t, θ |φ∗
sa≺t ,ξ ) (48)
≈ q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ). (49)
In the next section, we look at action selection as the second compo nent of action generation. To this
end, we show how to evaluate sequences of future actions ˆ at: ˆT by evaluating either Bayesian complete
posteriors or the approximate complete posteriors.
7 Action Selection Based on Intrinsic Motivations
7.1 Intrinsic Motivation and Action-Value Functions
The previous section resulted in sets of Bayesian or approximate co mplete posteriors. Independently
of whether a complete posterior is the approximate or the Bayesian version, it represents the entire
knowledge of the agent about the consequences of the sequence of future actions ˆ at: ˆT that is associated
with it. In order to evaluate sequences of future actions the agen t can only rely on its knowledge which
suggests that all such evaluations should depend solely on complete posteriors. One could argue that the
motivation might also depend directly on the memory state containing sa≺t. We here take a position
somewhat similar to the one proposed by
Schmidhuber (2010) that intrinsic motivations concerns the
“learning of a better world model”. We consider the complete poster ior as the current world model
and assume that intrinsic motivations depend only on this model and n ot on the exact values of past
sensor values and actions. As we will see this assumption is also enoug h to capture the three intrinsic
motivations that we discuss here. This level of generality is suﬃcient for our purpose of extending the
free energy principle. Whether it suﬃcient for a ﬁnal and general in trinsic motivation deﬁnition is beyond
the scope of this publication.
Complete posteriors are essentially conditional probability distribut ions over ˆS ˆT −t+1 × ˆE ˆT +1 × ∆ Θ
given elements of ˆA ˆT −t+1. A necessary (but not suﬃcient) requirement for intrinsic motivat ions in
our context (agents with generative models) is then that they are functions on the space of such
conditional probability distributions. Let ∆ ˆS ˆT −t+1× ˆE ˆT +1×∆ Θ | ˆA ˆT −t+1 be the space of conditional prob-
ability distributions over ˆS ˆT −t+1 × ˆE ˆT +1 × ∆ Θ given elements of ˆA ˆT −t+1. Then an intrinsic moti-
vation is a function M : ∆ ˆS ˆT −t+1× ˆE ˆT +1×∆ Θ | ˆA ˆT −t+1 × ˆA ˆT −t+1 → R taking a probability distribution
d(., ., . |. ) ∈ ∆ ˆS ˆT −t+1× ˆE ˆT +1×∆ Θ | ˆA ˆT −t+1 and a given future actions sequence ˆ at: ˆT ∈ ˆA ˆT −t+1 to a real value
23
M(d(., ., . |. ), ˆat: ˆT ) ∈ R. We can then see that the Bayesian complete posterior q(ˆ st: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ )
for a ﬁxed past sa≺t written as q( ., ., . |., sa ≺t, ξ ) provides such conditional probability distribution. Sim-
ilarly, every member of the family of distributions used to approximat e the Bayesian complete posterior
via variational inference r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ) written as r( ., ., . |., φ ) also provides such a conditional prob-
ability distribution. It will become important when discussing active inf erence that the optimised value
φ∗
sa≺t,ξ of the variational parameters as well as any other value of the var iational parameters φ deﬁne
an element with the right structure to be evaluated together with a set of future actions by an intrinsic
motivation function.
Using intrinsic motivation functions we then deﬁne two kinds of induce d action-value functions. These
are similar to value functions in reinforcement learning.
4 The ﬁrst is the Bayesian action-value function
(or functional):
ˆQ(ˆat: ˆT , sa ≺t, ξ ) := M(q(., ., . |., sa ≺t, ξ ), ˆat: ˆT ). (50)
In words the Bayesian action-value function ˆQ(ˆat: ˆT , sa ≺t, ξ ) infers the set of Bayesian complete posteriors
of past experience sa≺t and then evaluates the sequence of future actions ˆ at: ˆT according to the intrinsic
motivation function M.
The variational action-value function is deﬁned as 5:
ˆQ(ˆat: ˆT , φ ) := M(r(., ., . |., φ ), ˆat: ˆT ). (51)
So the variational action-value function ˆQ(ˆat: ˆT , φ ) directly takes the conditional probability distribution
deﬁned by variational parameter φ and evaluates the sequence of future actions ˆ at: ˆT according to M.
Unlike in the Bayesian case no inference takes place during the evalua tion of ˆQ(ˆat: ˆT , φ ).
At the same time, after variational inference, if we plug in φ∗
sa≺t,ξ for φ we have:
ˆQ(ˆat: ˆTa
, φ ∗
sa≺t,ξ ) ≈ ˆQ(ˆat: ˆTa
, sa ≺t, ξ ). (52)
Note that the reason we have placed a hat on ˆQ is that, even in the Bayesian case, it is usually
not the optimal action-value function but instead is an estimate bas ed on the current knowledge state
represented by the complete posteriors of the agent.
Also note that some intrinsic motivations (e.g. empowerment) evalua te e.g. the next n actions by using
predictions reaching n + m steps into the future. This means that they need all complete post eriors for
ˆat:t+n+m−1 but only evaluate the actions ˆ at:t+n−1. In other words they cannot evaluate actions up to
their generative model’s time-horizon ˆT but only until a shorter time-horizon ˆTa = ˆT − m for some
natural number m. When necessary we indicate such a situation by only passing shorte r future action
sequences ˆat: ˆTa
to the action-value function, in turn, the intrinsic motivation funct ion. The respective
posteriors keep the original time horizon ˆT > ˆTa.
4The main diﬀerence is that the action-value functions here e valuate sequences of future actions as opposed to policies.
This is the prevalent practice in active inference literatu re including Friston et al. (2015) and we therefore follow it here.
5We abuse notation here by reusing the same symbol ˆQ for the variational action-value function as for the Bayesi an
action-value function. However, in this publication the ar gument ( sa≺t, ξ or φ ) always indicates which one is meant.
24
7.2 Deterministic and Stochastic Action Selection
We can then select actions simply by picking the ﬁrst action in the sequ ence ˆat: ˆT that maximises the
Bayesian action-value function:
ˆa∗
t: ˆT (mt) := ˆa∗
t: ˆT (sa≺t) := arg max
ˆat: ˆT
ˆQ(ˆat: ˆT , sa ≺t, ξ ) (53)
and set
ˆa∗(mt) := ˆa∗
t (mt). (54)
or for the variational action value function:
ˆa∗
t: ˆT (mt) := ˆa∗
t: ˆT (φ∗
sa≺t ,ξ ) := arg max
ˆat: ˆT
ˆQ(ˆat: ˆT , φ ∗
sa≺t,ξ ). (55)
and set
ˆa∗(mt) := ˆa∗
t (mt). (56)
This then results in a deterministic action generation p( a|m):
p(at|mt) := δˆa∗(mt)(at).
We note here that in the case of universal reinforcement learning t he role of ˆQ(ˆat: ˆT , sa ≺t, ξ ) is played
by V π
ξ (sa≺t). There π is a policy that selects actions in dependence on the entire past sa≺t and ξ
parameterises the posterior just like in the present publication. Th e arg max in Equation (
53) selects a
policy instead of an action sequence and that policy is used for the ac tion generation.
A possible stochastic action selection that is important for active inf erence is choosing the action
according to a so called softmax policy ( Sutton and Barto , 1998):
p(at|mt) :=
∑
ˆat+1: ˆT
1
Z(γ, sa ≺t, ξ )eγ ˆQ(ˆat: ˆT ,sa ≺t,ξ ) (57)
where:
Z(γ, sa ≺t, ξ ) :=
∑
ˆat: ˆT
eγ ˆQ(ˆat: ˆT ,sa ≺t,ξ ) (58)
is a normalisation factor. Note that we are marginalising out later act ions in the sequence ˆ at: ˆT to get a
25
distribution only over the action ˆ at. For the variational action-value function this becomes:
p(at|mt) :=
∑
ˆat+1: ˆT
1
Z(γ, φ ∗
sa≺t,ξ )eγ ˆQ(ˆat: ˆT ,φ ∗
sa≺t,ξ ) (59)
where:
Z(γ, φ ∗
sa≺t,ξ ) :=
∑
ˆat: ˆT
eγ ˆQ(ˆat: ˆT ,φ ∗
sa≺t,ξ ). (60)
Since it is relevant for active inference (see Section
8), note that the softmax distribution over future
actions can also be deﬁned for arbitrary φ and not only for the optimised φ∗
sa≺t,ξ . At the same time, the
softmax distribution for the optimised φsa≺t,ξ clearly also approximates the softmax distribution of the
Bayesian action-value function.
Softmax policies assign action sequences with higher values of ˆQ higher probabilities. They are
often used as a replacement for the deterministic action selection t o introduce some exploration. Here,
lower γ leads to higher exploration; conversely, in the limit where γ → ∞ the softmax turns into the
deterministic action selection. From an intrinsic motivation point of vie w such additional exploration
should be superﬂuous in many cases since many intrinsic motivations t ry to directly drive exploration by
themselves. Another interpretation of such a choice is to see γ as a trade-oﬀ factor between the processing
cost of choosing an action precisely and achieving a high action-value . The lower γ, the higher the cost
of precision. This leads to the agent more often taking actions that do not attain maximum action-value.
We note that the softmax policy is not the only possible stochastic ac tion selection mechanism. An-
other option discussed in the literature is Thompson sampling (
Ortega and Braun , 2010, 2014; Aslanides et al. ,
2017). In our framework this corresponds to a two step action selectio n procedure where we ﬁrst sample
an environment and parameter pair ( ¯
ˆet−1, ¯θ) from a posterior factor (Bayesian or variational)
(¯
ˆet−1, ¯θ) ∼ d( ˆEt−1, Θ |sa≺t, ξ ) (61)
then plug the according predictive factor q(ˆ st: ˆT , ˆet: ˆT |ˆat: ˆT , ¯
ˆet−1, ¯θ) into the action value function
ˆQ(ˆat: ˆT , sa ≺t, ξ ) := M(q(., . |., ¯
ˆet−1, ¯θ), ˆat: ˆT ). (62)
This allows intrinsic motivations that only evaluate the probability distr ibution over future sensor values
ˆSt: ˆT and environment states ˆEt: ˆT . However, it rules out those that evaluate the posterior probabilit y of
environment parameters Θ because we sample a speciﬁc ¯θ.
7.3 Intrinsic Motivations
Now, we look at some intrinsic motivations including the intrinsic motivat ion part underlying Friston’s
active inference.
In the deﬁnitions, we use d( ., ., . |. ) ∈ ∆ ˆS ˆT −t+1× ˆE ˆT +1×∆ Θ | ˆA ˆT −t+1 as a generic conditional probability
26
distribution. The generic symbol d is used since it represents both B ayesian complete posteriors and
approximate complete posteriors. In fact, the deﬁnitions of the in trinsic motivations are agnostic with
respect to the method used to obtain a complete posterior. In the present context, it is important that
these deﬁnitions are general enough to induce both Bayesian and v ariational action-value functions. We
usually state the deﬁnition of the motivation function using general expressions (e.g. marginalisations)
derived from d( , ., . |. ). Also, we look at how they can be obtained from Bayesian complete p osteriors to
give to the reader an intuition for the computations involved in applica tions. The approximate complete
posterior usually makes these calculations easier and we will present an example of this.
7.3.1 Free Energy Principle
Here, we present the non-variational Bayesian inference version s for the expressions that occur in the
“expected free energy” in
Friston et al. (2015, 2017a). These papers only include approximate expressions
after variational inference. Most of the expressions we give here can be found in Friston et al. (2017b).
The exception is Equation ( 74), which can be obtained from an approximate term in Friston et al. (2017a)
in the same way that the non-variational Bayesian inference terms in Friston et al. (2017b) are obtained
from the approximate ones in Friston et al. (2015).
In the following, we can set ˆTa = ˆT , since actions are only evaluated with respect to their immediate
eﬀects.
According to Friston et al. (2017b, Eq. (A.2) supplementary material), the “expected free energy” is
just the future conditional entropy of sensor values 6 given environment states. Formally, this is (with
a negative sign to make minimising expected free energy equivalent to maximising the action-value
function):
M(d(., ., . |. ), ˆat: ˆT ) : =
∑
ˆet: ˆT
d(ˆet: ˆT |ˆat: ˆT )
∑
ˆst: ˆT
d(ˆst: ˆT |ˆet: ˆT ) log d(ˆst: ˆT |ˆet: ˆT ) (63)
= −
∑
ˆet: ˆT
d(ˆet: ˆT |ˆat: ˆT ) Hd( ˆSt: ˆT |ˆet: ˆT ) (64)
= − Hd( ˆSt: ˆT | ˆEt: ˆT , ˆat: ˆT ). (65)
Note that, we indicate the probability distribution d used to calculate entropies H d(X) or mutual infor-
mations I d(X : Y ) in the subscript. Furthermore,we indicate the variables that are s ummed over with
capital letters and those that are ﬁxed (e.g. ˆ at: ˆT above) with small capital letters.
In the case where d( ., ., . |. ) is the Bayesian complete posterior q( ., ., . |., sa ≺t, ξ ), it uses the predictive
distribution of environment states q(ˆ et: ˆT |ˆat: ˆT , sa ≺t, ξ ) and the posterior of the conditional distribution
of sensor values given environment states q(ˆ st: ˆT |ˆet: ˆT , sa ≺t, ξ ). As we see next, both distributions can be
obtained from the Bayesian complete posterior.
The former distribution is a familiar expression in hierarchical Bayesia n models and corresponds to
a posterior predictive distribution or predictive density (cmp. e.g. Bishop, 2011, Eq.(3.74)) that can be
6The original text refers to this as the “expected entropy of o utcomes”, not the expected conditional entropy of outcomes .
Nonetheless, the associated Equation (A.2) in the original is identical to ours.
27
calculated via:
q(ˆet: ˆT |ˆat: ˆT , sa ≺t, ξ ) =
∫ ∑
ˆst: ˆT , ˆe≺t
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) dθ (66)
=
∫ ∑
ˆst: ˆT , ˆe≺t
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) dθ (67)
=
∫ ∑
ˆet−1
q(ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆet−1, θ |sa≺t, ξ ) dθ, (68)
where we split the complete posterior into the predictive and poster ior factor and then marginalised out
environment states ˆe≺t−1 since the predictive factor does not depend on them. Note that in p ractice,
this marginalisation corresponds to a sum over |E|t−1 terms and therefore has a computational cost
that grows exponential in time. However, if we use the approximate complete posterior such that
d(., ., . |. ) = r( ., ., . |., φ ), we see from Equation ( 40), that q(ˆe≺t, θ |sa≺t, ξ ) is replaced by r(ˆ e≺t, θ |φ) which
is deﬁned as (Equation ( 38)):
r(ˆe≺t, θ |φ) :=
t−1∏
τ =0
r(ˆeτ |φEτ )
3∏
i=1
r(θi|φi). (69)
This means that r(ˆ et−1, θ |φ) is just r(ˆ et−1|φEt−1 ) r(θ|φ), which we obtain directly from the variational
inference without any marginalisation. If Bayesian inference increa ses in computational cost exponen-
tially in time, this simpliﬁcation leads to a signiﬁcant advantage. This for mulation leaves an integral
over θ or, more precisely, a triple integral over the three θ1, θ 2, θ 3. However, if the q( θi|ξi) are chosen
as conjugate priors to q(ˆ s|ˆe, θ 1), q(ˆe′|ˆa′, ˆe, θ 2), q(ˆe0|θ3) respectively, then these integrals can be calcu-
lated analytically (compare the similar calculation of q(ˆ e≺t, θ |sa≺t, ξ ) in Appendix A). The remaining
computational problem is only the sum over all ˆ et−1.
The latter term (the posterior conditional distribution over senso r values given environment states)
can be obtained via
q(ˆst: ˆT |ˆet: ˆT , sa ≺t, ξ ) = q(ˆst: ˆT |ˆet: ˆT , ˆat: ˆT , sa ≺t, ξ ) (70)
= q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , sa ≺t, ξ )
q(ˆet: ˆT |ˆat: ˆT , sa ≺t, ξ ) . (71)
Here, the ﬁrst equation holds since
ˆSt: ˆT ⊥ ⊥ˆAt: ˆT | ˆEt: ˆT , SA ≺t. (72)
Both numerator and denominator can be obtained from the complet e posterior via marginalisation as for
the former term. This marginalisation also shows that the intrinsic mo tivation function, Equation ( 63),
is a functional of the complete posteriors or d( ., ., . |. ).
In most publications on active inference the expected free energy in Equation ( 63) is only part of what
28
is referred to as the expected free energy. Usually, there is a sec ond term measuring the relative entropy
to an externally speciﬁed prior over future outcomes (also called “predictive distribution encoding goals”
Friston et al. 2015 ), i.e. a desired probability distribution p d(ˆst: ˆT ). The relative entropy term is formally
given by:
KL[d( ˆSt: ˆT |ˆat: ˆT )|| pd( ˆSd
t: ˆT )] =
∑
ˆst: ˆT
d(ˆst: ˆT |ˆat: ˆT ) log d(ˆst: ˆT |ˆat: ˆT )
pd(ˆst: ˆT ) . (73)
Clearly, this term will lead the agent to act such that the future dist ribution over sensor values is similar
to the desired distribution. Since this term is used to encode extrins ic value for the agent, we mostly
ignore it in this publication. It could included into any of the following intr insic motivations.
In Friston et al. (2017a) yet another term, called “negative novelty” or “ignorance”, occ urs in the
expected free energy. This term concerns the posterior distribu tion over parameter θ1. It can be slightly
generalised to refer to any subset of the parameters θ = ( θ1, θ 2, θ 3). We can write it as a conditional
mutual information between future sensor values and parameter s (the “ignorance” is the negative of
this):
Id( ˆSt: ˆT : Θ |ˆat: ˆT ) =
∑
ˆst: ˆT
d(ˆst: ˆT |ˆat: ˆT )
∫
d(θ|ˆst: ˆT , ˆat: ˆT ) log d(θ|ˆst: ˆT , ˆat: ˆT )
d(θ) dθ. (74)
This is identical to the information gain used in knowledge seeking agen ts. The necessary posteriors in
the Bayesian case are q(ˆ st: ˆT |ˆat: ˆT , sa ≺t, ξ ), q( θ|ˆst: ˆT , ˆat: ˆT , sa ≺t, ξ ) and q( θ|sa≺t, ξ ) with
q(ˆst: ˆT |ˆat: ˆT , sa ≺t, ξ ) =
∫ ∑
ˆe≺t
q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) dθ (75)
a straightforward (if costly) marginalisation of the complete poste rior. Just like previously for q(ˆ et: ˆT |ˆat: ˆT , sa ≺t, ξ ),
the marginalisation is greatly simpliﬁed in the variational case (see App endix B for a more explicit cal-
culation). The integrals can be computed if using conjugate priors. The other two posteriors can be
obtained via
q(θ|ˆst: ˆT , ˆat: ˆT , sa ≺t, ξ ) = 1
q(ˆst: ˆT |ˆat: ˆT , sa ≺t, ξ )
∑
ˆe0: ˆT
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ). (76)
and
q(θ|sa≺t, ξ ) = q( θ|ˆat: ˆT , sa ≺t, ξ ) (77)
=
∑
ˆst: ˆT , ˆe0: ˆT
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ). (78)
29
In the latter equation we used
ˆAt: ˆT ⊥ ⊥Θ |SA≺t. (79)
The marginalisations grow exponentially in computational cost with ˆT . In this case, the variational
approximation only reduces the necessary marginalisation over ˆ e≺t−1 to one over ˆet−1, but the marginal-
isation over future environment states ˆet: ˆT and sensor values ˆst: ˆT remains the same since we use the exact
predictive factor. In practice the time horizon into the future ˆT −t must then be chosen suﬃciently short,
so that marginalising out ˆ et: ˆT and ˆSt: ˆT is feasible. Together with the variational approximation the re-
quired marginalisations over past and future are then constant ov er time which makes the implementation
of agents with extended lifetimes possible.
The combination of the conditional entropy term and the informatio n gain deﬁnes the (intrinsic part)
of the action-value function of Friston’s active inference (or free energy principle):
MF EP (d(., ., . |. ), ˆat: ˆT ) = − Hd( ˆSt: ˆT | ˆEt: ˆT ) + I d( ˆSt: ˆT : θ|ˆat: ˆT ) (80)
In the active inference literature this is usually approximated by a su m over the values at individual
timesteps:
MF EP (d(., ., . |. ), ˆat: ˆT ) =
ˆT∑
τ =t
− Hd( ˆSτ | ˆEτ ) + I d( ˆSτ : Θ |ˆat: ˆT ). (81)
7.3.2 Free Energy Principle Specialised to Friston et al. (2015)
Using Appendix C, we show how to get the action-value function of Friston et al. (2015, Eq. (9)) in
our framework. In Friston et al. (2015), the information gain of Equation ( 74) is not included, but the
extrinsic term of Equation ( 73) is. Furthermore, the sum over timesteps in Equation ( 81) is used. This
leads to the following expression:
MF EP (d(., ., . |. ), ˆat: ˆT ) =
ˆT∑
τ =t
− Hd( ˆSτ | ˆEτ ) − KL[d( ˆSτ |ˆat: ˆT )|| pd( ˆSτ )]. (82)
If we plug in an approximate complete posterior, we get:
MF EP (r(., ., . |. ), ˆat: ˆT ) =
ˆT∑
τ =t
− Hr( ˆSτ | ˆEτ ) − KL[r( ˆSτ |ˆat: ˆT )|| pd( ˆSτ )]. (83)
with
− Hr( ˆSτ | ˆEτ ) =
∑
ˆeτ
r(ˆeτ |ˆat: ˆT , ˆet−1, φ )
∑
ˆsτ
r(ˆsτ |ˆeτ , φ ) log r(ˆsτ |ˆeτ , φ ), (84)
30
and
KL[r( ˆSτ |ˆat: ˆT )|| pd( ˆSτ )] =
∑
ˆsτ
r(ˆsτ |ˆat: ˆT , φ ) log r(ˆsτ |ˆat: ˆT , φ )
pd(ˆsτ ) . (85)
For the particular approximate posterior of Equation ( 40), with its factorisation into exact predictive
and approximate posterior factor, the individual terms can be fur ther rewritten.
r(ˆeτ |ˆat: ˆT , ˆet−1, φ ) =
∑
ˆst: ˆT , ˆeτ +1: ˆT ˆet:τ −1 ˆe0:t−2
∫
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ) dθ (86)
=
∑
ˆst: ˆT , ˆeτ +1: ˆT ˆet:τ −1 ˆe0:t−2
∫
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) r(ˆe≺t, θ |φ) dθ (87)
=
∑
ˆst: ˆT , ˆeτ +1: ˆT ˆet:τ −1 ˆe0:t−2
∫
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ )
t−1∏
r=0
r(ˆer|φEr )
3∏
i=1
r(θi|φi) dθ (88)
=
∑
ˆet:τ −1
∫
q(ˆet:τ −1|ˆat: ˆT , ˆet−1, θ 2) r(ˆet−1|φEt−1 ) r(θ2|φ2) dθ2 (89)
=
Ñ
∑
ˆet:τ −1
∫ τ∏
r=t
q(ˆer|ˆar, ˆer−1, θ 2) r(θ2|φ2) dθ2
é
r(ˆet−1|φEt−1 ). (90)
In Friston et al. (2015), the environment dynamics q(ˆer|ˆar, ˆer−1, θ 2) are not inferred and are therefore
not parameterised:
q(ˆer|ˆar, ˆer−1, θ 2) = q(ˆer|ˆar, ˆer−1) (91)
and are set to the physical environment dynamics:
q(ˆer|ˆar, ˆer−1) = p(ˆer|ˆar, ˆer−1). (92)
This means the integral over θ2 above is trivial and we get:
r(ˆeτ |ˆat: ˆT , ˆet−1, φ ) =
∑
ˆet:τ −1
τ∏
r=t
q(ˆer|ˆar, ˆer−1) r(ˆet−1|φEt−1 ) (93)
(94)
In the notation of Friston et al. (2015) (see Appendix C for a translation table), we have
q(ˆer|ˆar, ˆer−1) = B(ˆar)ˆer ˆer−1 (95)
where B(ˆar) is a matrix, and
r(ˆet−1|φEt−1 ) = ( Ûst−1)ˆet−1 (96)
31
where ( Ûst−1) is a vector, so that
r(ˆeτ |ˆat: ˆT , ˆet−1, φ ) = ( B(ˆaτ ) · · · B(ˆat) · Ûst−1)ˆeτ (97)
=: ( Ûsτ (ˆat: ˆT ))ˆeτ (98)
Similarly, since the sensor dynamics in Friston et al. (2015) are also not inferred, we ﬁnd
r(ˆsτ |ˆeτ , φ ) = q(ˆsτ |ˆeτ ) = p(ˆsτ |ˆeτ ). (99)
Friston et al. writes:
q(ˆsτ |ˆeτ ) =: Aˆsτ ˆeτ (100)
with A a matrix. So that,
r(ˆsτ |ˆat: ˆT , φ Et−1 ) = A · Ûsτ (ˆat: ˆT ) (101)
=: Ûoτ (ˆat: ˆT ). (102)
Then
Hr( ˆSτ | ˆEτ ) = −1 · (A × log A) · Ûsτ (ˆat: ˆT ) (103)
where × is a Hadamard product and 1 is a vector of ones. Also,
KL[r( ˆSτ |ˆat: ˆT )|| pd( ˆSτ )] = Ûoτ (ˆat: ˆT ) · (log Ûoτ (ˆat: ˆT ) − log Cτ ) (104)
where ( Cτ )ˆsτ = p d(ˆsτ ). Plugging these expressions into Equation ( 83), substituting ˆ at: ˆT → π, and
comparing this to Friston et al. (2015, Eq. (9)) shows that 7:
MF EP (r(., ., . |. ), π ) = 1 · (A × log A) · Ûsτ (ˆat: ˆT ) − Ûoτ (ˆat: ˆT ) · (log Ûoτ (ˆat: ˆT ) − log Cτ ) (105)
= Q(π). (106)
This veriﬁes that our formulation of the action-value function spec ialises to the “expected (negative) free
energy” Q(π).
7.3.3 Empowerment Maximisation
Empowerment maximisation (
Klyubin et al. , 2005) is an intrinsic motivation that seeks to maximise the
channel capacity from sequences of the agent’s actions into the s ubsequent sensor value. The agent,
equipped with complete knowledge of the environment dynamics, can directly observe the environment
7There is a small typo in Friston et al. (2015, Eq. (9)) where the time index of Ûst−1 in (Ûsτ (ˆat: ˆT )) = ( B(ˆaτ ) · · ·B(ˆat)·Ûst−1)
is given as t instead of t − 1.
32
state. If the environment is deterministic, an empowerment maximis ation policy leads the agent to a
state from which it can reach the highest number of future states within a preset number of actions.
Salge et al. (2014) provide a good overview of existing research on empowerment max imisation. A
more recent study relates the intrinsic motivation to the essential dynamics of living systems, based on
assumptions from autopoietic enactivism Guckelsberger and Salge (2016b). Several approximations have
been proposed, along with experimental evaluations in complex stat e / action spaces. Salge et al. (2018)
show how deterministic empowerment maximisation in a three-dimensio nal grid-world can be made more
eﬃcient by diﬀerent modiﬁcations of UCT tree search. Three recen t studies approximate stochastic em-
powerment and its maximisation via variational inference and deep ne ural networks, leveraging a varia-
tional bound on the mutual information proposed by Barber and Agakov (2003). Mohamed and Rezende
(2015) focus on a model-free approximation of open-loop empowerment, and Gregor et al. (2016) pro-
pose two means to approximate closed-loop empowerment. While the se two approaches consider both
applications in discrete and continuous state / action spaces, Karl et al. (2017) develop an open-loop,
model-based approximation for the continuous domain speciﬁcally. T he latter study also demonstrates
how empowerment can yield good performance in established reinfor cement learning benchmarks such
as bipedal balancing in the absence of extrinsic rewards. In recent years, research on empowerment has
particularly focused on applications in multi-agent systems. Coupled empowerment maximisation as
a speciﬁc multi-agent policy has been proposed as intrinsic drive for e ither supportive or antagonistic
behaviour in open-ended scenarios with sparse reward landscapes Guckelsberger et al. (2016b). This
theoretical investigation has then been backed up with empirical ev aluations on supportive and adver-
sarial video game characters Guckelsberger et al. (2016a, 2018). Beyond virtual agents, the same policy
has been proposed as a good heuristic to facilitate critical aspects of human-robot interaction, such
as self-preservation, protection of the human partner, and res ponse to human actions Salge and Polani
(2017).
For empowerment, we select ˆTa = t + n and ˆT = t + n + m, with n ≥ 0 and m ≥ 1. This means
the agent chooses n + 1 actions which it expects to maximise the resulting m-step empowerment. The
according action-value function is:
MEM (d(., ., . |. ), ˆat: ˆT ) : = max
d(ˆa ˆTa+1: ˆT )
Id( ˆA ˆTa+1: ˆT : ˆS ˆT |ˆat: ˆTa
) (107)
= max
d(ˆa ˆTa+1: ˆT )
∑
ˆa ˆTa+1: ˆT , ˆs ˆT
d(ˆa ˆTa+1: ˆT ) d(ˆs ˆT |ˆat: ˆT ) log d(ˆs ˆT |ˆat: ˆT )
d(ˆs ˆT |ˆat: ˆTa
). (108)
Note that in the denominator of the fraction, the action sequence only runs to t : ˆTa and not to t : ˆT as
in the numerator.
In the Bayesian case, the required posteriors are q(ˆ s ˆT |ˆat: ˆT , sa ≺t, ξ ) (for each ˆa ˆTa+1: ˆT ) and q(ˆs ˆT |ˆat: ˆTa
, sa ≺t, ξ ).
The former distribution is a further marginalisation over ˆ st+1: ˆT −1 of q(ˆst: ˆT |ˆat: ˆT , sa ≺t, ξ ). The variational
approximation only helps getting q(ˆ st: ˆT |ˆat: ˆT , sa ≺t, ξ ), not the further marginalisation. The latter distri-
33
bution is obtained for a given q(ˆ a ˆTa+1: ˆT ) from the former one via
q(ˆs ˆT |ˆat: ˆTa
, sa ≺t, ξ ) =
∑
ˆa ˆTa+1: ˆT
q(ˆs ˆT , ˆa ˆTa+1: ˆT |ˆat: ˆTa
, sa ≺t, ξ ) (109)
=
∑
ˆa ˆTa+1: ˆT
q(ˆs ˆT |ˆa ˆTa+1: ˆT , ˆat: ˆTa
, sa ≺t, ξ ) q(ˆa ˆTa+1: ˆT ) (110)
since the empowerment calculation imposes
q(ˆa ˆTa+1: ˆT |ˆat: ˆTa
, sa ≺t, ξ ) = q(ˆa ˆTa+1: ˆT ). (111)
7.3.4 Predictive Information Maximisation
Predictive information maximisation, (
Ay et al. , 2008), is an intrinsic motivation that seeks to maximise
the predictive information of the sensor process. Predictive infor mation is the mutual information be-
tween past and future sensory signal, and has been proposed as a general measure of complexity of
stochastic processes ( Bialek and Tishby , 1999). For applications in the literature see Ay et al. (2012);
Martius et al. (2013, 2014). Also, see Little and Sommer (2013) for a comparison to entropy minimisa-
tion.
For predictive information, we select a half time horizon k = ⌊(t : ˆT − t + 1) / 2⌋ where k > 0 for
predictive information to be deﬁned (i.e. t : ˆT − t > 0). Then, we can deﬁne the expected mutual
information between the next m sensor values and the subsequent m sensor values as the action-value
function of predictive information maximisation. This is similar to the tim e-local predictive information
in Martius et al. (2013):
MP I (d(., ., . |. ), ˆat: ˆT ) : = I d( ˆSt:t+k−1 : ˆSt+k:t+2k−1|ˆat: ˆT ). (112)
We omit writing out the conditional mutual information since it is deﬁne d in the usual way. Note that
it is possible that t + 2 k − 1 < t : ˆT so that the action sequence ˆ at: ˆT might go beyond the evaluated
sensor probabilities. This displacement leads to no problem since the s ensor values do not depend on
future actions. The posteriors needed are: q(ˆ st:t+k−1|ˆat: ˆT , sa ≺t, ξ ), q(ˆst+k:t+2k−1|ˆst:t+k−1, ˆat: ˆT , sa ≺t, ξ ),
and q(ˆst+k:t+2k−1|ˆat: ˆT , sa ≺t, ξ ). The ﬁrst and the last are again marginalisations of q(ˆ st: ˆT |ˆat: ˆT , sa ≺t, ξ )
seen in Equation ( 75). The second posterior is a fraction of such marginalisations.
7.3.5 Knowledge Seeking
Knowledge seeking agents (
Storck et al. , 1995; Orseau et al. , 2013) maximise the information gain with
respect to a probability distribution over environments. The inform ation gain we use here is the relative
entropy between the belief over environments after actions and s ubsequent sensor values and the belief
over environments (this is the KL-KSA of Orseau et al. 2013 , “KL” for Kullback-Leibler divergence).
In our case the belief over environments can be identiﬁed with the po sterior q( θ|sa≺t, ξ ) since every
θ = ( θ1, θ 2, θ 3) deﬁnes an environment. In principle, this can be extended to the p osterior q( ξ|sa≺t, ξ )
34
over the hyperprior ξ, but we focus on θ here. This deﬁnition is more similar to the original one. Then,
we deﬁne the knowledge seeking action-value function using the info rmation gain of Equation ( 74):
MKSA (d(., ., . |. ), ˆat: ˆT ) : = I d( ˆSt: ˆT : Θ |ˆat: ˆT ). (113)
We have discussed the necessary posteriors following Equation ( 74).
After this overview of some intrinsic motivations, we look at active inf erence. However, what should be
clear is, that, in principle, both the posteriors needed for the intrin sic motivation function of the original
active inference ( Friston et al. , 2015) and the posteriors needed for alternative inferences overlap. T his
overlap shows that the other intrinsic motivations mentioned here a lso proﬁt from variational inference
approximations. There is also no indication that these intrinsic motiva tions cannot be used together
with the next discussed active inference.
8 Active Inference
Now, we look at active inference. Note that this section is independe nt of the intrinsic motivation
function underlying the action-value function ˆQ.
In the following we ﬁrst look at and try to explain a slightly simpliﬁed vers ion of the active inference
in
Friston et al. (2015). Afterwards we also state the full version.
As mentioned in the introduction, current active inference version s are formulated as an optimisation
procedure that, at least at ﬁrst sight, looks similar to the optimisat ion of a variational free energy
familiar from variational inference. Recall that, in variational infer ence the parameters of a family of
distributions are optimised to approximate an exact (Bayesian) pos terior of a generative model. In the
case we discussed in Section 6.4 the sought after exact posterior is the posterior factor of the g enerative
model of Section 6.1. One of our questions about active inference is whether it is a straig htforward
application of variational inference to a posterior of some generat ive model. This would imply the
existence of a generative model whose standard updating with pas t actions and sensor values leads to
an optimal posterior distribution over future actions. Note that, this does not work with the generative
model in of Section 6.1 since the future actions there are independent of the past senso r values and
actions. Given the appropriate generative model, it would then be na tural to introduce it ﬁrst and then
apply a variational approximation similar to our procedure in Section 6.
We were not able to ﬁnd in the literature or construct ourselves a ge nerative model such that varia-
tional inference leads directly to the active inference as given in Friston et al. (2015). Instead we present
a generative model that contains a posterior whose variational ap proximation optimisation is very similar
to the optimisation procedure of active inference. It is also closely r elated to the two-step action gener-
ation of ﬁrst inferring the posterior and then selecting the optimal actions. This background provides
some intuition for the particularities of active inference.
One diﬀerence of the generative model used here is that its struct ure depends on the current time
step in a systematic way. The previous generative model of Section 6.1 had a time-invariant structure.
35
In Section 6, we showed how the generative model, together with either Bayesia n or variational
inference, can provide an agent with a set of complete posteriors. Each complete posterior is a conditional
probability distribution over all currently unobserved variables ( ˆSt: ˆT , ˆE0:T ) and parameters (Θ and more
generally also Ξ) given past sensor values and actions sa≺t and a particular sequence of future actions ˆ at: ˆT .
Inference means updating the set of posteriors in response to ob servations sa≺t. Active inference should
then update the distribution over future actions in response to ob servations. This means the according
posterior cannot be conditional on future action sequences like th e complete posterior in Equation ( 16).
Since active inference promises belief or knowledge updating and act ion selection in one mechanism the
posterior should also range over unobserved relevant variables like future sensor values, environment
states, and parameters. This leads to the posterior of Equation ( 13):
q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ). (13 revisited)
If this posterior has the right structure, then we can derive a fut ure action distribution by marginalising:
q(ˆat: ˆT |sa≺t, ξ ) =
∑
ˆst: ˆT , ˆe0: ˆT
∫
q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ) dθ. (114)
Actions can then be sampled from the distribution obtained by margin alising further to the next action
only:
p(at|mt) :=
∑
ˆat+1: ˆT
q(ˆat: ˆT |sa≺t, ξ ). (115)
This scheme could justiﬁably be called (non-variational) active infere nce since the future action distribu-
tion is directly obtained by updating the generative model.
However, as we mentioned above, according to the generative mod el of Figure 2, the distribution over
future actions is independent of the past sensor values and action s:
q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ) = q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) q(ˆat: ˆT ) (116)
since
q(ˆat: ˆT |sa≺t, ξ ) = q(ˆat: ˆT ). (117)
Therefore, we can never learn anything about future actions fro m past sensor values and actions using
this model. In other words, if we intend to select the actions based o n the past, we cannot uphold this
independent model. The inferred actions must become dependent o n the history and the generative
model has to be changed for a scheme like the one sketched above t o be successful.
In Section 7.2, we have mentioned that the softmax policy based on a given action- value function ˆQ
36
could be a desirable outcome of an active inference scheme such as t he above. Thus, if we ended up with
q(ˆat: ˆT |sa≺t, ξ ) = 1
Z(γ, sa ≺t, ξ ) eγ ˆQ(ˆat: ˆT ,sa ≺t,ξ ) (118)
as a result of some active inference process, that would be a viable s olution. We can force this by building
this conditional distribution directly into a new generative model. Not e that this conditional distribution
determines all future actions ˆ at: ˆT starting at time t and not just the next action ˆ at. In the end however
only the next action will be taken according to Equation ( 115) and at time t + 1 the action generation
mechanism starts again, now with ˆ at+1: ˆT inﬂuenced by the new data sat in addition to sa≺t. So the
model structure changes over time in this case with the dependenc y of actions on pasts sa≺t shifting
together with each time-step. Keeping the rest of the previous Ba yesian network structure intact we
deﬁne that at each time t the next action ˆAt depends on past sensor values and actions sa≺t as well as
on the hyperparameter ξ (see Figure 6):
q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ) := q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆat: ˆT |sa≺t, ξ ) q(θ, ˆe≺t|sa≺t, ξ ). (119)
On the right hand side we have the predictive and posterior factors left and right of the distribu-
tion over future actions. We deﬁne this conditional future action d istribution to be the softmax of
Equation ( 118). This means that the mechanism-generating future actions uses the Bayesian action-
value function ˆQ(ˆat: ˆT , sa ≺t, ξ ). The Bayesian action-value function depends on the complete pos terior
q(ˆst: ˆT , ˆet: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) calculated using the old generative model of Figure 2 where actions do not
not depend on past sensor values and actions. This is a complex cons truction with what amounts to
Bayesian inference essentially happening within an edge (i.e. ˆS ˆA≺t → ˆAt: ˆT ) of a Bayesian network. How-
ever, logically there is no problem since the posterior q(ˆ st: ˆT , ˆet: ˆT , θ |ˆat: ˆT , sa ≺t, ξ ) for each ˆat: ˆT to be well
deﬁned really only needs sa≺t, ξ, and the model structure. Here we see the model structure as “ hard
wired” into the mechanism, since it is ﬁxed for each time step t from the beginning.
We now approximate the posterior of Equation ( 118) using variational inference. Like in Section 6.4
we do not approximate the predictive factor. Instead we only appr oximate the product of posterior factor
q(θ, ˆe≺t|sa≺t, ξ ) and future action distribution q(ˆat: ˆT |sa≺t, ξ ). By construction these are two independent
factors but with an eye to active inference which treats belief or kn owledge updating and action generation
together we also treat them together. For the approximation we a gain use the approximate posterio factor
of Equation ( 38) and combine it with a distribution over future actions r(ˆ at: ˆT |π) parameterised by π:
r(ˆat: ˆT , ˆe≺t, θ |π, φ ) := r(ˆat: ˆT |π) r(ˆe≺t, θ |φ) (120)
:= r(ˆat: ˆT |π) r(ˆe≺t|φE≺t ) r(θ|φ). (121)
37
Ξ 2
Ξ 3
Ξ 1
Θ 2
Θ 3
Θ 1
ˆE0
ˆS0
ˆE1
ˆS1
ˆA1
ˆE2
ˆS2
ˆA2
ˆE3
ˆS3
ˆA3
Figure 6: Generative model including q(ˆ at: ˆT |sa≺t, ξ ) at t = 2 with ˆS ˆA≺2 inﬂuencing future actions ˆA2: ˆT .
Note that, only future actions are dependent on past sensor valu es and actions, e.g. action ˆA1 has no
incoming edges. The increased gap between time step t = 1 and t = 2 is to indicate that this time step
is special in the model. For each time step t there is an according model with the particular relation
between past ˆS ˆA≺t and ˆAt: ˆT shifted accordingly.
The variational free energy is then:
F[π, φ, sa ≺t, ξ ] : =
∑
ˆat: ˆT , ˆe≺t
∫
r(ˆat: ˆT |π) r(ˆe≺t, θ |φ) log r(ˆat: ˆT |π) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT , ˆe≺t, θ |a≺t, ξ ) dθ (122)
=
∑
ˆat: ˆT , ˆe≺t
∫
r(ˆat: ˆT |π) r(ˆe≺t, θ |φ) log r(ˆat: ˆT |π) r(ˆe≺t, θ |φ)
q(ˆat: ˆT |sa≺t, ξ ) q(ˆe≺t, θ |sa≺t, ξ ) q(s≺t|a≺t, ξ ) dθ
(123)
= F[φ, sa ≺t, ξ ] + KL[r( ˆAt: ˆT |π)|| q( ˆAt: ˆT |sa≺t, ξ )]. (124)
Where F[φ, sa ≺t, ξ ] is the variational free energy of the (non-active) variational inf erence (see Equa-
tion ( 45)). Variational inference then minimises the above expression with r espect to parameters φ and
π:
φ∗
sa≺t ,ξ , π ∗
sa≺t,ξ : = arg min
φ,π
F[π, φ, sa ≺t, ξ ] (125)
= arg min
φ
F[φ, sa ≺t, ξ ] + arg min
π
KL[r( ˆAt: ˆT |π)|| q( ˆAt: ˆT |sa≺t, ξ )]. (126)
We see that the minimisation in this case separates into two minimisation problems. The ﬁrst is just
the variational inference of Section
6.4 and the second minimises the KL-divergence between the parame-
terised action distribution r(ˆat: ˆT |π) and the softmax q(ˆat: ˆT |sa≺t, ξ ) of the Bayesian action-value function.
38
It is instructive to look at this KL-divergence term closer:
KL[r( ˆAt: ˆT |π)|| q( ˆAt: ˆT |sa≺t, ξ )] = − Hr( ˆAt: ˆT |π) −
∑
ˆat: ˆT
r(ˆat: ˆT |π) log q(ˆat: ˆT |sa≺t, ξ ) (127)
= − Hr( ˆAt: ˆT |π) −
∑
ˆat: ˆT
r(ˆat: ˆT |π) ˆQ(ˆat: ˆT , sa ≺t, ξ ) + log Z(γ, sa ≺t, ξ ). (128)
We see that the optimisation of π leads towards high entropy distributions for which the expectation
value of the action-value function ˆQ(ˆat: ˆT , φ ) is large. Action selection could then happen according to
p(at|mt) :=
∑
ˆat+1:T
r(ˆat: ˆT |π∗
sa≺t,ξ ). (129)
So the described variational inference procedure, at least forma lly, leads to a useful result. However, this
is not the active inference procedure of Friston et al. (2015). As noted above the minimisation actually
splits into two completely independent minimisations here. The result o f the minimisation with respect
to φ in Equation ( 126) is actually not used for action selection and since action selection is a ll that
matters here is mere ornament. However, there is a way to make us e of it. Recall that plugging φ∗
sa≺t ,ξ
into the variational action-value function ˆQ(ˆat: ˆT , φ ) means that it approximates the Bayesian action
value function (see Equation (
52)). This means that if we deﬁne a softmax distribution r(ˆ at: ˆT |φ) of the
variational action-value function parameterised by φ as:
r(ˆat: ˆT |φ) = 1
Z(γ, φ )eγ ˆQ(ˆat: ˆT ,φ ). (130)
Then this approximates the softmax of the Bayesian action-value f unction:
r(ˆat: ˆT |φ∗
sa≺t ,ξ ) ≈ q(ˆat: ˆT |sa≺t, ξ ). (131)
Consequently, once we have obtained φ∗
sa≺t,ξ from the ﬁrst minimisation problem in Equation (
126) we
can plug it into r(ˆat: ˆT |φ) and then minimise the KL-divergence of r(ˆ at: ˆT |π) to this distribution instead of
the one to q(ˆat: ˆT |sa≺t, ξ ). In this way the result of the ﬁrst could be reused for the second minimisation.
This remains a two part action generation mechanism however. Activ e inference combines these two
steps into one minimisation by replacing q(ˆ at: ˆT |sa≺t, ξ ) in the variational free energy of Equation ( 122)
with r(ˆat: ˆT |φ). Since r(ˆat: ˆT |φ) thereby becomes part of the denominator it is also given the same s ymbol
(in our case q) as the generative model. So we deﬁne:
q(ˆat: ˆT |φ) := r(ˆat: ˆT |φ). (132)
In this form the softmax q(ˆ at: ˆT |φ) is a cornerstone of active inference. In brief, it can be regarded as a
prior over action sequences. To obtain purposeful behaviour it sp eciﬁes prior assumptions about what
sorts of actions an agent should take when its belief parameter tak es value φ. Strictly speaking the
expression resulting from the replacement q( ˆAt: ˆT |sa≺t, ξ ) → q(ˆat: ˆT |φ) in Equation ( 122) is then not a
39
variational free energy anymore since the variational parameter s φ occur in both the numerator and
the denominator. Nonetheless, this is the functional that is minimise d in active inference as described
in Friston et al. (2015). So active inference is deﬁned as the optimisation problem (cmp. Friston et al. ,
2015, Eq.(1)):
φ∗
sa≺t ,ξ , π ∗
sa≺t,ξ = arg min
φ,π
∑
ˆat: ˆT , ˆe≺t
∫
r(ˆat: ˆT |π) r(ˆe≺t, θ |φ) log r(ˆat: ˆT |π) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT , ˆe≺t, θ |φ, a ≺t, ξ ) dθ (133)
= arg min
φ,π
Ä
F[φ, sa ≺t, ξ ] + KL[r( ˆAt: ˆT |π)|| q( ˆAt: ˆT |φ)]
ä
. (134)
This minimisation does not split into the two independent parts anymor e since both the future action
distribution q( ˆAt: ˆT |φ) of the generative model and the approximate posterior factor in the variational
free energy F[φ, sa ≺t, ξ ] are parameterised by φ. This justiﬁes the claim that active inference obtains
both belief update and action selection through a single principle or op timisation.
Compared to Friston et al. (2015), we have introduced a simpliﬁcation of active inference. In the
original text, additional distributions over γ (with according random variable Γ) are introduced to the
generative model as q( γ|ξγ ) (which is a ﬁxed prior) and to the approximate posterior as r( γ|φγ ). For
the sake of completeness, we show the full equations as well. Since γ is now part of the model, we
write q(ˆat: ˆT |γ, φ ) instead of q(ˆ at: ˆT |φ). The basic procedure above stays the same. The active inferenc e
optimisation becomes:
φ∗
sa≺t,ξ ,φ γ ∗
sa≺t,ξ , π ∗
sa≺t,ξ
= arg min
φ,φ γ ,π
∑
ˆat: ˆT , ˆe≺t
∫∫
r(ˆat: ˆT |π) r(γ|φγ ) r(ˆe≺t, θ |φ) log r(ˆat: ˆT |π) r(γ|φγ ) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT , γ, ˆe≺t, θ |φ, a ≺t, ξ ) dθ dγ. (135)
Note that here, by construction, the denominator can be written as:
q(s≺t, ˆat: ˆT , γ, ˆe≺t, θ |φ, a ≺t, ξ ) = q(ˆat: ˆT |γ, φ ) q(γ|φγ ) q(ˆe≺t, θ |sa≺t, ξ ) q(s≺t|a≺t, ξ ). (136)
Which allows us to write Equation ( 135) with the original variational free energy again:
φ∗
sa≺t,ξ ,φ γ ∗
sa≺t ,ξ , π ∗
sa≺t,ξ = arg min
φ,φ γ ,π
Ä
F[φ, sa ≺t, ξ ] + KL[r( ˆAt: ˆT , Γ |π, φ γ )|| q( ˆAt: ˆT , Γ |φ, ξ γ )]
ä
. (137)
9 Applications and Limitations
An application of the active inference described here to a simple maze task can be found in
Friston et al.
(2015). Active inference using diﬀerent forms of approximate posterior s can be found in Friston et al.
(2016b,b). Here, Friston et al. (2017a) also includes a knowledge seeking term in addition to the con-
ditional entropy term. In the universal reinforcement learning fr amework Aslanides et al. (2017) also
implement a knowledge seeking agent. These works can be quite direc tly translated into our framework.
40
For applications of intrinsic motivations that are not so directly relat ed to our framework see also
the references in the according Sections 7.3.3 to 7.3.5.
A quantitative analysis of the limitations of the diﬀerent approaches we discussed is beyond the scope
of this publication. However, we can make a few observations that m ay help researchers interested in
applying the discussed approaches.
Concerning the computation of the complete posterior by direct Ba yesian methods is not feasible
beyond the simplest of systems and even then only for very short t ime durations. As mentioned in
the text it contains a sum over | ˆE|t elements. If the time horizon into the future is ˆT − t then the
predictive factor consists of ˆS ˆT −t × ˆE ˆT −t × ˆA ˆT −t entries. This means predicting far into the future is
also not feasible. Therefore ˆT − t will usually have to be ﬁxed to a small number. Methods that also
approximate the predictive factor (e.g. Friston et al. , 2016b, 2017a) may be useful here. However, to our
knowledge, their scalability has not been addressed yet. Since in the se approaches the predictive factor
is approximated in a similar way as the posterior factor here, we would expect that it is similar to the
scalability of approximating the posterior factor.
Employing variational inference reduces the computational burde n for obtaining a posterior factor
considerably. The sum over all possible past environment histories ( the | ˆE|t elements) is approximated
within the optimisation. Clearly, by employing variational inference we inherit all shortcomings of this
method. As mentioned also in Friston et al. (2016b) variational inference approximations are known
to become overconﬁdent i.e. the approximate posterior tends to ig nore values with low probabilities
(see e.g. Bishop, 2011). In practice this can of course lead to poor decision making. Furth ermore, the
convergence of the optimisation to obtain the approximate poster ior can also become slow. As time t
increases the necessary computations for each optimisation step in the widely used coordinate ascent
variational inference algorithm ( Blei et al. , 2017) grow with t2. Experiments suggest that the number of
necessary optimisation steps also grows over time. At the moment, we do not know how fast but this
may also lead to problems. A possible solution would be to introduce som e form of forgetting such that
the considered past does not grow forever.
Ignoring the problem of obtaining a complete posterior, we still have to evaluate and select actions.
Computing the information theoretic quantities needed for the men tioned intrinsic motivations and their
induced action-value functions is also computationally expensive. In this case ﬁxing the future time
horizon ˆT − t can lead to constant computational requirements. These grow ex ponentially with the
time horizon which makes large time horizons impossible without furthe r approximations. Note that the
action selection mechanisms discussed here also require the comput ation of the action-value functions
for each of the future action sequences.
Active inference is not a standard variational inference problem an d therefore standard algorithms
like the coordinate ascent variational inference may fail in this case . Other optimisation procedures like
gradient descent may still work. As far as we know there have been no studies of the scalability of the
active inference scheme up to now.
41
10 Conclusion
We have reconstructed the active inference approach of Friston et al. (2015) in in a formally consistent
way. We started by disentangling the components of inference and action selection. This disentanglement
has allowed us to also remove the variational inference completely an d formulate the pure Bayesian
knowledge updating for the generative model of Friston et al. (2015). We have shown in Section 6.3
that a special case of this model is equivalent to a ﬁnite version of th e model used by the Bayesian
universal reinforcement agent ( Hutter, 2005). We then pointed out how to approximate the pure Bayesian
knowledge updating with variational inference. To formalise the not ion of intrinsic motivations within
this framework, we have introduced intrinsic motivation functions t hat take complete posteriors and
future actions as inputs. These induce action-value functions simila r to those used in reinforcement
learning. The action-value functions can then be used for both, th e Bayesian and the variational agent,
in standard deterministic or softmax action selection schemes.
Our analysis of the intrinsic motivations Expected Free Energy Maximisation , Empowerment Maximi-
sation, Predictive Information Maximisation , and Knowledge Seeking indicates that there is signiﬁcant
common structure between the diﬀerent approaches and it may be possible to combine them. At the
time of writing, we have already made ﬁrst steps towards using the p resent framework for a systematic
quantitative analysis and comparison of the diﬀerent intrinsic motiva tions. Eventually, such studies will
shed more conclusive light on the computational requirements and e mergent dynamics of diﬀerent mo-
tivations. An investigation of the biological plausibility of diﬀerent mot ivations might lead to diﬀerent
results and this is of equal interest.
Beyond the comparison of diﬀerent intrinsic motivations within an act ive inference framework, the
present work can thus contribute to investigations on the role of in trinsic motivations in living organisms.
If biological plausibility of active inference can be upheld, and maintain ed for alternative intrinsic moti-
vations, then experimental studies might be derived to test diﬀere ntiating predictions. If active inference
was key to cognitive phenomena such as consciousness, it would be in teresting to see how the cognitive
dynamics would be aﬀected by alternative intrinsic motivations.
Conﬂict of Interest Statement
CG, CS, SS, and DP declare no competing interests. In accordance with Frontiers policy MB declares
that he was employed by company Araya Incorporated, Tokyo, Ja pan.
Author Contributions
MB, CG, CS, SS, and DP conceived of this study, discussed the conc epts, revised the formal analysis,
and wrote the article. MB contributed the initial formal analysis.
42
Funding
CG is funded by EPSRC grant [EP/L015846/1] (IGGI). CS is funded b y the EU Horizon 2020 programme
under the Marie Sklodowska-Curie grant 705643. DP is funded in par t by EC H2020-641321 socSMCs
FET Proactive project.
Acknowledgments
MB would like to thank Yen Yu for valuable discussions on active inferen ce.
43
A Posterior Factor
Here we want to calculate the posterior factor q(ˆ e≺t, θ |sa≺t, ξ ) of the complete posterior in Equation (
16)
without an approximation (i.e. as in direct, non-variational Bayesian inference).
q(ˆe≺t, θ |sa≺t, ξ ) = 1
q(s≺t|a≺t, ξ ) q(s≺t, ˆe≺t, θ |a≺t, ξ ) (138)
= 1
q(s≺t|a≺t, ξ ) q(s≺t|ˆe≺t, θ 1) q(ˆe≺t|a≺t, θ 2, θ 3) q(θ|ξ) (139)
= 1
q(s≺t|a≺t, ξ )
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2) q(ˆe0|θ3)
3∏
i=1
q(θi|ξi). (140)
We see that the numerator is given by the generative model. The den ominator can be calulated according
to:
q(s≺t|a≺t, ξ ) =
∫
∆ Θ
q(s≺t|a≺t, θ ) q(θ|ξ) dθ (141)
=
∫
∆ Θ
Ñ
∑
ˆe≺t
q(ˆe0|θ3)
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2)
é
3∏
i=1
q(θi|ξi) dθ (142)
=
∑
ˆe≺t
∫
∆ Θ
q(ˆe0|θ3)
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2)
3∏
i=1
q(θi|ξi) dθ (143)
=
∑
ˆe≺t
( ∫
q(ˆe0|θ3) q(θ3|ξ3) dθ3
∫ t∏
τ =0
q(sτ |ˆeτ , θ 1) q(θ1|ξ1) dθ1
×
∫ t∏
r=1
q(ˆer|ar, ˆer−1, θ 2) q(θ2|ξ2) dθ2
) (144)
The three integrals can be solved analytically if q( θi|ξi) are chosen as conjugate priors to q( sτ |ˆeτ , θ 1), q(ˆer|ar, ˆer−1, θ 2), q(ˆe0|θ3)
respectively. However, the sum is over |E|t terms and therefore untractable as time increases.
44
B Approximate Posterior Predictive Distribution
Here, we calculate the (variational) approximate predictive poster ior distribution of q(ˆ st: ˆT |ˆat: ˆT , sa ≺t, ξ )
from a given approximate complete posterior. This expression plays a role in multiple intrinsic motivation
functions like empowerment maximisation, predictive information max imisation, and knowledge seeking.
For an arbitrary φ we have:
r(ˆst: ˆT |ˆat: ˆT , φ ) : =
∑
ˆe≺t
∫
q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ ) r(ˆe≺t, θ |φ) dθ (145)
=
∑
ˆet−1
∫
q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ ) r(ˆet−1, θ |φ) dθ (146)
=
∑
ˆet−1
( ∫
q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ )
3∏
i=1
r(θi|φi) dθ
)
r(ˆet−1|φEt−1 ) (147)
=
∑
ˆet−1
Ñ
∑
ˆet: ˆT
∫
q(ˆst: ˆT |ˆet: ˆT , θ 1) r(θ1|φ1) dθ1×
×
∫
q(ˆet: ˆT |ˆat: ˆT , ˆet−1, θ 2) r(θ2|φ2) dθ2
é
r(ˆet−1|φEt−1 )
(148)
=
∑
ˆet−1
Ñ
∑
ˆet: ˆT
∫ ˆT∏
τ =t
q(ˆsτ |ˆeτ , θ 1) r(θ1|φ1) dθ1×
×
∫ ˆT∏
τ =t
q(ˆeτ |ˆaτ , ˆer−1, θ 2) r(θ2|φ2) dθ2
é
r(ˆet−1|φEt−1 )
(149)
=
∑
ˆet−1
∑
ˆet: ˆT
r(ˆst: ˆT |ˆet: ˆT , φ 1) r(ˆet: ˆT |ˆat: ˆT , ˆet−1, φ 2) r(ˆet−1|φEt−1 ) (150)
From ﬁrst to second line we usually have to marginalize q(ˆ e≺t, θ |sa≺t, ξ ) to q(ˆ et−1, θ |sa≺t, ξ ) with a
sum over all |E|t−1 possible environment histories ˆ e≺t−1. Using the approximate posterior, we can use
r(ˆet−1|φEt−1 ) directly without dealing with the intractable sum. From third to four th line, r( θ3|φ3) drops
out since it can be integrated out (and its integral is equal to one). Note that during the optimisation
Equation (
47) r( θ3|φ3) does play a role so it is not superﬂuous.From ﬁfth to last line, we perf orm the
integration over the parameters θ1 and θ2. These integrals can be calculated analytically if we choose
the models r( θ1|φ1) and r( θ2|φ2) as conjugate priors to q( s|e, θ 1) and q( e′|a′, e, θ 2). Variational inference
prediction of the next n = ˆT − t − 1 sensor values requires the sum and calculation of | ˆE|n terms for | ˆS|n
possible futures.
45
C Notation Translation Tables
A table to translate between our notation and the one used in Friston et al. (2015). The translation is
also valid in many cases for Friston et al. (2016b,a, 2017a). Some of the parameters shown here only
show up in the latter publications.
This article Friston et al. (2015) Note
et ∈ E Actual environment states
ˆet ∈ ˆE st ∈ S Estimated/modelled environment states
st ∈ S ot ∈ Ω Actual/observed sensor or outcome values
ˆst ∈ ˆS = S ot ∈ Ω Estimated/modelled (usually future) sensor or out-
come values. Note that the index τ instead of t
often indicates an estimated future sensor value in
Friston et al. (2015).
at ∈ A at ∈ A Actions
ˆat ∈ ˆA = A ut ∈ U Contemplated (usually future) actions
mt ∈ M Agent memory state
ˆat: ˆT π, ˜u π and ˜u both uniquely specify future action se-
quences
θ θ Generative model parameters
q(ˆs|ˆe, θ 1) = q(ˆs|ˆe) P (o|s) = Aos Model sensor dynamics, not parameterised in
Friston et al. (2015), A is a matrix representation
q(ˆe′|ˆa′, ˆe, θ 2) = q(ˆe′|ˆa′, ˆe) P (s′|s, u ) = B(u)s′s Model environment dynamics, not parameterised
in
Friston et al. (2015), B(u) is a matrix represen-
tation for each possible action u
q(ˆe0|θ3) P (s0|m) = Ds0 Modelled initial environment state, not parame-
terised in
Friston et al. (2015), D is a vector repre-
sentation. Note, the parameter m is a ﬁxed hyper-
parameter
ξ = ( ξ1, ξ 2, ξ 3) m Generative model hyperparam. or model parameter
that subsumes all hyperparameters
ξ1 sensor dynamics hyperparam.
ξ2 Environment dynamics hyperparam.
ξ3 Initial environment state hyperparam.
ξγ (α, β ) Precision hyperparam.
(φ, φ γ ) µ Variational param.
φE0: ˆT Ûs Environment states variational param.,
φEτ Ûsτ for each timestep τ
φ1 Sensor dynamics variational param.
φ2 Environment dynamics variational param.
46
φ3 Initial environment state variational param.
π Ûπ Future action sequence variational param.
φγ Ûγ Precision variational param.
ˆQ(ˆat: ˆT , φ ) Q(π) = Q(˜u|π) Variational action-value function. The dependence
of Q(˜u|π) on Ûst is omitted
p(s⪯ t, e ⪯ t, a ≺t) R(˜o, ˜s, ˜a) Our physical environment corresponds to the gen-
erative process
q(ˆs⪯ t, ˆe⪯ t, ˆat: ˆT , γ |a≺t, ξ ) P (˜o, ˜s, ˜u, γ |˜a, m ) The generative model for active inference including
γ (which we mostly omit)
r(ˆe0: ˆT , ˆat: ˆT , γ |π, φ, φ γ ) Q(˜s, ˜u, γ |µ) Approximate complete posterior for active infer-
ence
pd(ˆsτ ) P (oτ |m) Prior over future outcomes.
Since our treatment is more general than that of Friston et al. (2015) and quite similar (though not
identical) to the treatment in Friston et al. (2016b,a, 2017a) we also give the relations to variables in
those publications. We hope this will help interested readers to unde rstand the latter publications even
if some aspects of those are diﬀerent. A discussion of those diﬀere nces is beyond the scope of the present
article.
This article Friston et al. (2016b) Note
et ∈ E Actual environment states
ˆet ∈ ˆE st ∈ S Estimated/modelled environment
states
st ∈ S ot ∈ Ω Actual/observed sensor or outcome
values
ˆst ∈ ˆS = S ot ∈ Ω Estimated/modelled (usually fu-
ture) sensor or outcome values.
Note that the index τ instead of t
often indicates an estimated future
sensor value in
Friston et al. (2015).
at ∈ A ut ∈ A Actions
ˆat ∈ ˆA = A ut ∈ Υ Contemplated (usually future) ac-
tions
mt ∈ M Agent memory state
ˆa0: ˆT π, action sequences
θ θ Generative model parameters
θ1 A Sensor dynamics param.
θ2 B Environment dynamics param.
θ3 D Initial environment state param.
47
ξ η Generative model hyperparam. or
model parameter that subsumes all
hyperparameters
ξ1 a sensor dynamics hyperparam.
ξ2 b Environment dynamics hyper-
param.
ξ3 d Initial environment state hyper-
param.
ξγ β Precision hyperparam.
(φ, φ γ ) η Variational param.
φE0: ˆT s0:T Environment states variational
param.
q(ˆeτ |ˆat: ˆT , a 0:t−1, φ Eτ ) ( sπ
τ )ˆeτ For each sequence of actions and for
each timestep there is a parameter
sπ
τ . Since a categorical distribution
is used, the parameter is a vector of
probabilities whose entry ˆeτ is equal
to the probability of ˆeτ if we set ˆE =
{1, ..., | ˆE|}
φ1 a Sensor dynamics variational param.
φ2 b Environment dynamics variational
param.
φ3 d Initial environment state variational
param.
π π Future action sequence variational
param.
φγ β Precision variational param.
ˆQ(ˆat: ˆT , φ ) −G(π) Variational action-value function.
The dependence of G(π) on sπ
0:T is
omitted
p(s⪯ t, e ⪯ t, a ≺t) R(˜o, ˜s, ˜a) Our physical environment corre-
sponds to the generative process
q(ˆs⪯ t, ˆe0: ˆT , ˆa0: ˆT , γ, θ, ξ ) P (˜o, ˜s, π, γ, A, B, D|a, b, d, β ) The generative model for active in-
ference
r(ˆe0: ˆT , ˆa0: ˆT , γ, θ |π, φ γ , φ ) Q(˜s, π, A, B, D, γ |sπ
0: ˆT , π , a, b, d, β ) Approximate complete posterior for
active inference
pd(ˆsτ ) P (oτ ) = σ(Uτ ) Prior over future outcomes.
48
References
Allen, M. and Friston, K. J. (2016). From Cognitivism to Autopoiesis: Towards a Computational
Framework for the Embodied Mind. Synthese, pages 1–24.
Aslanides, J., Leike, J., and Hutter, M. (2017). Universal Reinforc ement Learning Algorithms: Survey
and Experiments. In Proceedings of the 26th International Joint Conference on A rtiﬁcial Intelligence ,
pages 1403–1410.
Attias, H. (1999). A Variational Bayesian Framework for Graphica l Models. In Solla, S., Leen, T., and
M¨ uller, K., editors,Proceedings Advances in Neural Information Processing Sys tems 12 , pages 209–215,
Cambridge, MA, USA. MIT Press.
Attias, H. (2003). Planning by Probabilistic Inference. In Proceedings 9th International Workshop on
Artiﬁcial Intelligence and Statistics .
Ay, N., Bernigau, H., Der, R., and Prokopenko, M. (2012). Informa tion-Driven Self-Organization: The
Dynamical System Approach to Autonomous Robot Behavior. Theory in Biosciences , 131(3):161–179.
Ay, N., Bertschinger, N., Der, R., Gttler, F., and Olbrich, E. (2008). Predictive Information and
Explorative Behavior of Autonomous Robots. The European Physical Journal B-Condensed Matter
and Complex Systems , 63(3):329–339.
Ay, N. and L¨ ohr, W. (2015). The Umwelt of an Embodied Agenta Mea sure-Theoretic Deﬁnition. Theory
in Biosciences , 134(3-4):105–116.
Barber, D. and Agakov, F. (2003). The IM Algorithm: A Variational Approach to Information Maximiza-
tion. In Thrun, S., Saul, L. K., and Schlkopf, B., editors, Proceedings Advances in Neural Information
Processing Systems 16 , pages 201–208. MIT Press.
Bialek, W. and Tishby, N. (1999). Predictive Information. arXiv preprint cond-mat/9902341 .
Bishop, C. M. (2011). Pattern Recognition and Machine Learning . Information Science and Statistics.
Springer, New York.
Blei, D. M., Kucukelbir, A., and McAuliﬀe, J. D. (2017). Variational Inf erence: A Review for Statisticians.
Journal of the American Statistical Association , 112(518):859–877.
Botvinick, M. and Toussaint, M. (2012). Planning as Inference. Trends in Cognitive Sciences , 16(10):485–
488.
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). The Free Energy Principle for Action
and Perception: A Mathematical Review. Journal of Mathematical Psychology , pages 55–79.
Clark, A. (2015). Surﬁng Uncertainty: Prediction, Action, and the Embodied M ind. Oxford University
Press.
49
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory . Wiley-Interscience, Hoboken,
N.J.
Dennett, D. C. (1991). Consciousness Explained . Penguin Books.
Doshi-Velez, F., Pfau, D., Wood, F., and Roy, N. (2015). Bayesian No nparametric Methods for Partially-
Observable Reinforcement Learning. IEEE Transactions on Pattern Analysis and Machine Intellig ence,
37(2):394–407.
Ellis, B. and Wong, W. H. (2008). Learning Causal Bayesian Network Structures From Experimental
Data. Journal of the American Statistical Association , 103(482):778–789.
Fox, R. and Tishby, N. (2016). Minimum-information lgq control par t ii: Retentive controllers. In 2016
IEEE 55th Conference on Decision and Control (CDC) , pages 5603–5609.
Friston, K. (2010). The free-energy principle: A uniﬁed brain theo ry? Nature Reviews Neuroscience ,
11(2):127–138.
Friston, K. (2013a). Consciousness and Hierarchical Inference . Neuropsychoanalysis, 15(1):38–42.
Friston, K. (2013b). Life as We Know It. Journal of The Royal Society Interface , 10(86).
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, J., and Pezzulo, G. (2016a). Active
Inference and Learning. Neuroscience & Biobehavioral Reviews , 68(Supplement C):862–879.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo , G. (2016b). Active Inference: A
Process Theory. Neural Computation , 29(1):1–49.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., and Pez zulo, G. (2015). Active Inference
and Epistemic Value. Cognitive Neuroscience, 6(4):187–214.
Friston, K., Samothrakis, S., and Montague, R. (2012). Active Inf erence and Agency: Optimal Control
Without Cost Functions. Biological Cybernetics, 106(8-9):523–541.
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., and Ond obaka, S. (2017a). Active
Inference, Curiosity and Insight. Neural Computation , 29(10):2633–2683.
Friston, K. J., Parr, T., and de Vries, B. (2017b). The Graphical Br ain: Belief Propagation and Active
Inference. Network Neuroscience , 1(4):381–414.
Froese, T. and Ziemke, T. (2009). Enactive artiﬁcial intelligence: I nvestigating the systemic organization
of life and mind. Artiﬁcial Intelligence , 173(3–4):466–500.
Gregor, K., Rezende, D. J., and Wierstra, D. (2016). Variational I ntrinsic Control. arXiv preprint
arXiv:1611.07507.
50
Guckelsberger, C. and Salge, C. (2016a). Does empowerment max imisation allow for enactive artiﬁcial
agents? In Proceedings of the Fifteenth International Conference on t he Synthesis and Simulation of
Living Systems (Alife 2016) , page 8. The MIT Press.
Guckelsberger, C. and Salge, C. (2016b). Does Empowerment Max imisation Allow for Enactive Artiﬁcial
Agents? In Proceedings 15th International Conference on Synthesis an d Simulation of Living Systems
(ALIFE).
Guckelsberger, C., Salge, C., and Colton, S. (2016a). Intrinsically M otivated General Companion NPCs
via Coupled Empowerment Maximisation. In Proceedings Conference on Computational Intelligence
in Games .
Guckelsberger, C., Salge, C., Saunders, R., and Colton, S. (2016b) . Supportive and Antagonistic Be-
haviour in Distributed Computational Creativity via Coupled Empower ment Maximisation. In Pro-
ceedings 7th International Conference on Computational Cr eativity.
Guckelsberger, C., Salge, C., and Togelius, J. (2018). New And Surp rising Ways to be Mean: Adver-
sarial NPCs with Coupled Empowerment Minimisation. In Proceedings Conference on Computational
Intelligence in Games .
Hutter, M. (2005). Universal Artiﬁcial Intelligence: Sequential Decisions B ased on Algorithmic Proba-
bility. Texts in Theoretical Computer Science. An EATCS Series. Springe r-Verlag, Berlin Heidelberg.
Karl, M., Soelch, M., Becker-Ehmck, P., Benbouzid, D., van der Smagt , P., and Bayer, J. (2017). Unsu-
pervised Real-Time Control through Variational Empowerment. arXiv preprint arXiv:1710.05101 .
Klyubin, A., Polani, D., and Nehaniv, C. (2005). Empowerment: A Unive rsal Agent-Centric Measure of
Control. In The 2005 IEEE Congress on Evolutionary Computation, 2005 , volume 1, pages 128–135.
Leike, J. (2016). Nonparametric General Reinforcement Learnin g. arXiv:1611.08944 [cs] .
Linson, A., , A., Ramamoorthy, S., and Friston, K. (2018). The Activ e Inference Approach to Ecological
Perception: General Information Dynamics for Natural and Artiﬁ cial Embodied Cognition. Frontiers
in Robotics and AI , 5:21.
Little, D. Y.-J. and Sommer, F. T. (2013). Maximal mutual informat ion, not minimal entropy, for
escaping the Dark Room. Behavioral and Brain Sciences , 36(3):220–221.
Lunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000). WinB UGS - A Bayesian Modelling
Framework: Concepts, Structure, and Extensibility. Statistics and Computing , 10(4):325–337.
Manzotti, R. and Chella, A. (2018). Good old-fashioned artiﬁcial co nsciousness and the intermediate
level fallacy. Frontiers in Robotics and AI , 5:39.
Martius, G., Der, R., and Ay, N. (2013). Information Driven Self-Or ganization of Complex Robotic
Behaviors. PLoS ONE , 8(5).
51
Martius, G., Jahn, L., Hauser, H., and Hafner, V. V. (2014). Self-E xploration of the Stumpy Robot with
Predictive Information Maximization. In del Pobil, A. P., Chinellato, E., Martinez-Martin, E., Hallam,
J., Cervera, E., and Morales, A., editors, From Animals to Animats 13: 13th International Conference
on Simulation of Adaptive Behavior, SAB 2014, Castell´ on, S pain, July 22-25, 2014. Proceedings , pages
32–42. Springer.
Minka, T. P. (2001). Expectation Propagation for Approximate Ba yesian Inference. In Proceedings
of the Seventeenth Conference on Uncertainty in Artiﬁcial I ntelligence, UAI’01, pages 362–369, San
Francisco. Morgan Kaufmann Publishers Inc.
Mohamed, S. and Rezende, D. J. (2015). Variational Information Maximisation for Intrinsically Moti-
vated Reinforcement Learning. In Cortes, C., Lawrence, N. D., Le e, D. D., Sugiyama, M., and Garnett,
R., editors, Proceedings Advances in Neural Information Processing Sys tems 28 , pages 2125–2133. Cur-
ran Associates, Inc.
Orseau, L., Lattimore, T., and Hutter, M. (2013). Universal Know ledge-Seeking Agents for Stochastic
Environments. In Jain, S., Munos, R., Stephan, F., and Zeugmann, T ., editors, Algorithmic Learning
Theory, number 8139 in Lecture Notes in Computer Science, pages 158–17 2. Springer Berlin Heidelberg.
Ortega, P. A. (2011). Bayesian Causal Induction. arXiv preprint arXiv:1111.0708 .
Ortega, P. A. and Braun, D. A. (2010). A Minimum Relative Entropy P rinciple for Learning and Acting.
Journal of Artiﬁcial Intelligence Research , 38(1):475–511.
Ortega, P. A. and Braun, D. A. (2014). Generalized Thompson Sam pling for Sequential Decision-Making
and Causal Inference. Complex Adaptive Systems Modeling , 2:2.
Oudeyer, P.-Y. and Kaplan, F. (2009). What is intrinsic motivation? a typology of computational
approaches. Frontiers in Neurorobotics , 1:6.
Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. (2007). Intrinsic Mot ivation Systems for Autonomous
Mental Development. IEEE Transactions on Evolutionary Computation , 11(2):265–286.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference . Cambridge University Press.
Pfeifer, R., Iida, F., and Bongard, J. (2005). New Robotics: Design Principles for Intelligent Systems.
Artiﬁcial Life , 11(1-2):99–120.
Ross, S. and Pineau, J. (2008). Model-Based Bayesian Reinforcem ent Learning in Large Structured
Domains. Proceedings 24th Conference on Uncertainty in Artiﬁcial In telligence, 2008:476–483.
Ryan, R. M. and Deci, E. L. (2000). Intrinsic and Extrinsic Motivatio ns: Classic Deﬁnitions and New
Directions. Contemporary Educational Psychology , 25(1):54–67.
Salge, C., Glackin, C., and Polani, D. (2014). Empowerment–an Intro duction. In Guided Self-
Organization: Inception , pages 67–114. Springer.
52
Salge, C., Guckelsberger, C., Canaan, R., and Mahlmann, T. (2018). Accelerating Empowerment Com-
putation with UCT Tree Search. In Proceedings Conference on Computational Intelligence in G ames.
IEEE.
Salge, C. and Polani, D. (2017). Empowerment as Replacement for t he Three Laws of Robotics. Frontiers
in Robotics and AI , 4:25.
Santucci, V. G., Baldassarre, G., and Mirolli, M. (2013). Which Is the B est Intrinsic Motivation Signal
for Learning Multiple Skills? Frontiers in Neurorobotics , 7:22.
Schmidhuber, J. (2010). Formal Theory of Creativity, Fun, and I ntrinsic Motivation (1990-2010). IEEE
Transactions on Autonomous Mental Development , 2(3):230–247.
Storck, J., Hochreiter, S., and Schmidhuber, J. (1995). Reinforc ement Driven Information Acquisition in
Non-Deterministic Environments. In Proceedings of the International Conference on Artiﬁcial N eural
Networks, volume 2, pages 159–164.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction . MIT Press.
Toussaint, M. (2009). Probabilistic inference as a model of planned behavior. K¨ unstliche Intelligenz,
3/09:23–29.
Vehtari, A., Gelman, A., Sivula, T., Jylnki, P., Tran, D., Sahai, S., Blomste dt, P., Cunningham, J. P.,
Schiminovich, D., and Robert, C. (2014). Expectation Propagation as a Way of Life: A Framework
for Bayesian Inference on Partitioned Data. arXiv:1412.4869 [stat] .
Wainwright, M. J. and Jordan, M. I. (2007). Graphical Models, Exp onential Families, and Variational
Inference. Foundations and Trends in Machine Learning , 1(12):1–305.
Winn, J. and Bishop, C. M. (2005). Variational Message Passing. Journal of Machine Learning Research ,
6(Apr):661–694.
53