8102
nuJ
12
]IA.sc[
1v38080.6081:viXra
Expanding the Active Inference Landscape: More Intrinsic
Motivations in the Perception-Action Loop
Martin Biehl1,∗ Christian Guckelsberger2 Christoph Salge3,4
Simo´n C. Smith4,5 Daniel Polani4
1Araya Inc., Tokyo, Japan
2Computational Creativity Group, Department of Computing, Goldsmiths, University
of London, London, UK
3Game Innovation Lab, Department of Computer Science and Engineering, New York
University, New York City, NY, USA
4Sepia Lab, Adaptive Systems Research Group, Department of Computer Science,
University of Hertfordshire, Hatfield, UK
5Institute of Perception, Action and Behaviour, School of Informatics, The University of
Edinburgh, UK
Abstract
Active inference is an ambitious theory that treats perception, inference and action selection of
autonomousagentsundertheheadingofasingleprinciple. Itsuggestsbiologically plausibleexplana-
tionsfor manycognitivephenomena,includingconsciousness. Inactiveinference, action selection is
drivenbyanobjectivefunctionthatevaluatespossiblefutureactionswithrespecttocurrent,inferred
beliefs about theworld. Activeinferenceat its coreis independentfrom extrinsicrewards, resulting
in a high level of robustness across e.g. different environments or agent morphologies. In the liter-
ature, paradigms that share this independence have been summarised under the notion of intrinsic
motivations. Ingeneralandincontrasttoactiveinference,thesemodelsofmotivationcomewithout
a commitment to particular inference and action selection mechanisms. In this article, we study if
the inference and action selection machinery of active inference can also be used by alternatives to
the originally included intrinsic motivation. The perception-action loop explicitly relates inference
and action selection to the environment and agent memory, and is consequently used as foundation
forouranalysis. Wereconstructtheactiveinferenceapproach,locatetheoriginalformulationwithin,
and show how alternative intrinsic motivations can be used while keeping many of the original fea-
turesintact. Furthermore,weillustratetheconnectiontouniversalreinforcementlearningbymeans
of our formalism. Active inference research may profit from comparisons of the dynamics induced
∗martin@araya.org
1
byalternativeintrinsicmotivations. Researchonintrinsicmotivationsmayprofitfromanadditional
way to implement intrinsically motivated agents that also share the biological plausibility of active
inference.
Contents
1 Introduction 3
2 Related Work 6
3 Structure of this Article 7
4 Notation 7
5 Perception-Action Loop 8
5.1 PA-loop Bayesian Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6 Inference and Complete Posteriors 11
6.1 Generative Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6.2 Bayesian Complete Posteriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Connection to Universal Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 17
6.4 Approximate Complete Posteriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
7 Action Selection Based on Intrinsic Motivations 23
7.1 Intrinsic Motivation and Action-Value Functions . . . . . . . . . . . . . . . . . . . . . . . 23
7.2 Deterministic and Stochastic Action Selection . . . . . . . . . . . . . . . . . . . . . . . . . 25
7.3 Intrinsic Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.3.1 Free Energy Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.3.2 Free Energy Principle Specialised to Friston et al. (2015) . . . . . . . . . . . . . . 30
7.3.3 Empowerment Maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
7.3.4 Predictive Information Maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . 34
7.3.5 Knowledge Seeking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
8 Active Inference 35
9 Applications and Limitations 40
10 Conclusion 42
A Posterior Factor 44
B Approximate Posterior Predictive Distribution 45
2
C Notation Translation Tables 46
1 Introduction
Activeinference(Friston et al.,2012),andarangeofotherformalismsusuallyreferredtoasintrinsicmo-
tivations (Storck et al., 1995; Klyubin et al., 2005; Ay et al., 2008), all aim to answer a similar question:
Underminimalassumptions,howshouldanagentact?. Morepractically,theyrelatetowhatwouldbea
universalwayto generatebehaviourforanagentorrobotthatappropriatelydealswith its environment,
i.e. acquires the informationneeded to act and acts towardsan intrinsic goal. To this end, both the free
energy principle and intrinsic motivations aim to bridge the gap between giving a biologically plausible
explanation for how real organism deal with the problem and providing a formalism that can be imple-
mented in artificial agents. Additionally, they share a range of properties, such as an independence of a
priorisemantics andbeing defined purelyonthe dynamics ofthe agentenvironmentinteraction,i.e. the
agent’s perception-action loop.
Despitethesenumeroussimilarities,asfarasweknow,therehasnotbeenanyunifiedorcomparative
treatment of those approaches. We believe this is in part due to a lack of an appropriate unifying math-
ematical framework. To alleviate this, we present a technically complete and comprehensive treatment
of active inference, including a decomposition of its perception and action selection modes. Such a de-
compositionallowsus torelateactiveinference andthe inherentmotivationalprinciple tootherintrinsic
motivation paradigms such as empowerment (Klyubin et al., 2005), predictive information (Ay et al.,
2008), and knowledge seeking (Storck et al., 1995; Orseau et al., 2013). Furthermore, we are able to
clarify the relation to universal reinforcement learning (Hutter, 2005). Our treatment is deliberately
comprehensive and complete, aiming to be a reference for readers interested in the mathematical funda-
ment.
A considerable number of articles have been published on active inference (e.g. Friston et al., 2012,
2015, 2016b,a, 2017a,b; Linson et al., 2018). Active inference defines a procedure for both perception
andactionofanagentinteractingwithapartiallyobservableenvironment. Thedefinitionofthemethod,
in contrast to other existing approaches (e.g. Hutter, 2005; Doshi-Velez et al., 2015; Leike, 2016), does
not maintain a clear separation between the inference and the action selection mechanisms, and the
objective function. Most approaches for perception and action selection are generally formed of three
steps: The first step involves a learning or inference mechanism to update the agent’s knowledge about
the consequences of its actions. In a second step, these consequences are evaluated with respect to an
agent-internal objective function. Finally, the action selection mechanism chooses an action depending
on the preceding evaluation.
In active inference, these three elements are entangled. On one hand, there is the main feature of
active inference: the combination of knowledge updating and action selection into a single mechanism.
Thissinglemechanismistheminimisationofa“variationalfreeenergy”(Friston et al.,2015,p.188). The
“inference” part of the name is justified by the formal resemblance of the method to the variational free
energy minimisation (also known as evidence lower bound maximisation) used in variational inference.
3
Variational inference is a way to turn Bayesian inference into an optimisation problem which gives rise
to an approximate Bayesian inference method (Wainwright and Jordan, 2007). The “active” part is
justified by the fact that the output of this minimisation is a probability distribution over actions from
which the actions of the agent are then sampled. Behaviour in active inference is thus the result of a
variationalinference-likeprocess. Ontheotherhand,thefunction(i.e.expectedfreeenergy)thatinduces
the objective function in active inference is said to be “of the same form” as the variational free energy
(Friston et al.,2017a,p.2673)orevento“follow”fromit(Friston et al.,2016b,p.10). Thissuggeststhat
expected free energy is the only objective function compatible with active inference.
In summary, perception and action in active inference intertwines four elements: variational approx-
imation, inference, action selection, and an objective function. Besides these formal features, active
inference is of particular interest for its claims on biological plausibility and its relationship to the ther-
modynamics of dissipative systems. According to Friston et al. (2012, Section 3), active inference is a
“corollary”tothefreeenergyprinciple. Therefore,itisclaimed,actionsmustminimisevariationalfreeen-
ergytoresistthedispersionofstatesofself-organisingsystems(seealsoFriston,2013b;Allen and Friston,
2016). Activeinferencehasalsobeenusedtoreproducearangeofneuralphenomenainthehumanbrain
(Friston et al., 2016b), and the overarching free energy principle has been proposed as a “unified brain
theory” Friston (2010). Furthermore, the principle has been used in a hierarchical formulation as the-
oretical underpinning of the predictive processing framework (Clark, 2015, pp. 305-306), successfully
explaining a wide range of cognitive phenomena. Of particular interest for the present special issue, the
representation of probabilities in the active inference framework is conjectured to be related to aspects
of consciousness (Friston, 2013a; Linson et al., 2018).
These strong connections between active inference and biology,statisticalphysics, andconsciousness
research make the method particularly interesting for the design of artificial agents that can interact
with- and learn about unknown environments. However, it is currently not clear to which extent active
inference allows for modifications. We ask: how far do we have to commit to the precise combination of
elements used in the literature, and what becomes interchangeable?
One target for modifications is the objective function. In situations where the environment does
not provide a specific reward signal and the goal of the agent is not directly specified, researchers often
choose the objective function from a range of intrinsic motivations. The concept of intrinsic motivation
was introduced as a psychologicalconcept by (Ryan and Deci, 2000), and is defined as “the doing of an
activity for its inherent satisfactions rather than for some separable consequence”. The concept helps
us to understand one important aspect of consciousness: the assignmentof affect to certain experiences,
e.g. the experience of fun (Dennett, 1991) when playing a game. Computational approaches to intrinsic
motivations (Oudeyer and Kaplan, 2009; Schmidhuber, 2010; Santucci et al., 2013) can be categorised
roughly by the psychological motivations they are imitating, e.g. drives to manipulate and explore,
the reduction of cognitive dissonance, the achievement of optimal incongruity, and finally motivations
for effectance, personal causation, competence and self-determination. Intrinsic motivations have been
used to enhance behaviour aimed at extrinsic rewards (Sutton and Barto, 1998), but their defining
characteristic is that they can serve as a goal-independent motivational core for autonomous behaviour
4
generation. Thischaracteristicmakesthemgoodcandidatesforthe roleofvaluefunctions forthedesign
of intelligent systems (Pfeifer et al., 2005). We attempt to clarify how to modify active inference to
accommodateobjectivefunctions basedondifferentintrinsic motivations. This mayallowfuture studies
to investigatewhether andhowalteringthe objectivefunctionaffects the biologicalplausibility ofactive
inference.
Another target for modification, originating more from a theoretical standpoint, is the variational
formulationof activeinference. As mentionedabove,variationalinference formulates Bayesianinference
as an optimisation problem; a family of probability distributions is optimised to approximate the di-
rect, non-variational Bayesian solution. Active inference is formulated as an optimisation problem as
well. We consequently ask: is active inference the variational formulation of a direct (non-variational)
Bayesian solution? Such a direct solution would allow a formally simple formulation of active inference
withoutrecoursetooptimisationorapproximationmethods,atthecostofsacrificingtractabilityinmost
scenarios.
To explore these questions, we take a step back from the established formalism, gradually extend
the active inference framework, and comprehensively reconstruct the version presented in Friston et al.
(2015). We disentangle the four components of approximation, inference, action selection, and objective
functions that are interwoven in active inference.
Oneofourfindings,fromaformalpointofview,isthatexpectedfreeenergycanbereplacedbyother
intrinsic motivations. Our reconstructionofactive inference then yields a unified formalframeworkthat
can accommodate:
• Direct, non-variational Bayesian inference in combination with standard action selection schemes
known from reinforcement learning as well as objective functions induced by intrinsic motivations.
• Universal reinforcement learning through a special choice of the environment model and a small
modification of the action selection scheme.
• Variational inference in place of the direct Bayesian approach.
• Active inference in combination with objective functions induced by intrinsic motivations.
We believe that our framework can benefit active inference research as a means to compare the dy-
namics induced by alternativeactionselectionprinciples. Furthermore,it equips researchersonintrinsic
motivations with additional ways for designing agents that share the biological plausibility of active
inference.
Finally, this article contributes to the researchtopic: Consciousness in Humanoid Robots, in several
ways. First,there have beennumerous claimsonhow active inference relatesto consciousnessorrelated
qualities,whichweoutlinedearlierintheintroduction. ThemostrecentworkbyLinson et al.(2018),also
partofthisresearchtopic,specificallydiscussesthisrelation,particularlyinregardstoassigningsalience.
Furthermore,intrinsicmotivations(includingthefreeenergyprincipleforthisargument)havearangeof
properties that relate to or are useful to a range ofclassicalapproachesrecently summarisedas as Good
Old-Fashioned Artificial Consciousness (GOFAC, Manzotti and Chella, 2018). For example, embodied
5
approaches still need some form of value-function or motivation (Pfeifer et al., 2005), and benefit from
thefactthatintrinsicmotivationsareusuallyuniversalyetsensitiveinregardstoanagent’sembodiment.
The enactive AI framework(Froese and Ziemke, 2009), another candidate for GOFAC, proposesfurther
requirements on how value underlying motivation should be grounded in constitutive autonomy and
adaptivity. Guckelsberger and Salge(2016a)presenttentativeclaimsonhowempowermentmaximisation
relates to these requirements in biological systems, and how it could contribute to realising them in
artificial ones. Finally, the idea of using computational approaches for intrinsic motivation goes back
to developmental robotics (Oudeyer et al., 2007), where it is suggested as way to produce a learning
and adapting robot, which could offer another road to robot consciousness. Whether these Good Old-
Fashioned approacheswill ultimately be successful is an open question, and Manzotti and Chella (2018)
asses them rather critically. However, extending active inference to alternative intrinsic motivations in
a unified framework allows to combine features of these two approaches. For example it may bring
together the neurobiological plausibility of active inference and the constitutive autonomy afforded by
empowerment.
2 Related Work
Our work is largely based on Friston et al. (2015) and we adopt the setup and models from it. This
means many of our assumptions are due to the original paper. Recently, Buckley et al. (2017) have
provided an overview of continuous-variable active inference with a focus on the mathematical aspects,
ratherthantherelationshiptothermodynamicfreeenergy,biologicalinterpretationsorneuralcorrelates.
Our work here is in as similar spirit but focuses on the discrete formulation of active inference and
how it can be decomposed. As we point out in the text, the case of direct Bayesian inference with
separateaction selectionis strongly relatedto generalreinforcementlearning(Hutter, 2005; Leike, 2016;
Aslanides et al.,2017). Thisapproachalsotacklesunknownenvironmentswith-andinlaterversionsalso
without externally specified reward in a Bayesian way. Other work focusing on unknown environments
with rewards are e.g. Ross and Pineau (2008) and Doshi-Velez et al. (2015). We would like to stress
that we do not propose agents using Bayesian or variational inference as competitors to any of the
existing methods. Instead, our goal is to provide an unbiased investigation of active inference with a
particularfocusonextendingtheinferencemethods,objectivefunctionsandaction-selectionmechanisms.
Furthermore,theseagentsfollowalmostcompletelyinastraightforward(ifquite involved)wayfromthe
model in Friston et al. (2015). A small difference is the extension to parameterisations of environment
and sensor dynamics. These parameterisations can be found in Friston et al. (2016b).
We note that work on planning as inference (Attias, 2003; Toussaint, 2009; Botvinick and Toussaint,
2012)isgenerallyrelatedtoactiveinference. Inthislineofworktheprobabilitydistributionoveractions
oractionsequencesthatleadtoagivengoalspecifiedasasensorvalueisinferred. Sinceactiveinference
also tries to obtain a probability distribution overactions the approachesare related. The formalisation
of the goal however differs, at least at first sight. How exactly the two approaches relate is beyond the
scope of this publication.
6
3 Structure of this Article
Going forward, we will first outline our mathematical notation in Section 4. We then introduce the
perception-action loop, which contains both agent and environment in Section 5. In Section 6 we intro-
ducethemodelusedbyFriston et al.(2015). Wethenshowhowtoobtainbeliefsabouttheconsequences
ofactionsviaboth(direct)Bayesianinference(Section6.2)and(approximate)variationalinference(Sec-
tion6.4). Thesebeliefsarerepresentedintheformofasetofcompleteposteriors. Suchasetisacommon
object but usually does not play a prominent role in Bayesian inference. Here, it turns out to be a con-
venientstructurefor capturingthe agent’knowledgeanddescribingintrinsicmotivations. Undercertain
assumptions that we discuss in Section 6.3 the direct Bayesian case specialises to the belief updating of
the Bayesian universal reinforcement learning agent of Aslanides et al. (2017). We then discuss in Sec-
tion 7 how those beliefs (i.e. the set of complete posteriors) can induce action-value functions (playing
the role of objective functions) via a given intrinsic motivation function. We present standard (i.e. non-
active inference) ways to select actions based on such action-value functions. Then we look at different
instances of intrinsic motivation functions. The first is the “expected free energy” of active inference.
For this we explicitly show how our formalism produces the original expression in Friston et al. (2015).
Lookingatthe formulationsofotherintrinsic motivationsitbecomes clearthatthe expectedfree energy
relies on expressions quite similar or identical to those that occur in other intrinsic motivations. This
suggests that, at least in principle, there is no reason why active inference should only work with ex-
pectedfreeenergyasanintrinsicmotivation. Finally,inSection8formulateactiveinferenceforarbitrary
action-value functions which include those induced by intrinsic motivations. Modifying the generative
model of Section 6.1 and looking at the variational approximation of its posterior comes close but does
not correspond to the original active inference of Friston et al. (2015). We explain the additional trick
that is needed.
In the appendices we provide some more detailed calculations as well as notation translation tables
(Appendix C) from our own to those of Friston et al. (2015) and Friston et al. (2016b).
4 Notation
We will explain our notation in more detail in the text, but for readers that mostly look at equations
we give a short summary. Note that, Appendix C comprises a translation between Friston et al. (2015,
2016b) and the present notation. Mostly, we will denote random variables by upper case letters e.g.
X,Y,A,E,M,S,...theirstatespacesbycalligraphicuppercaselettersX,Y,A,E,M,S...,specificvalues
of random variables which are elements of the state spaces by lower case letters x,y,a,e,m,s,.... An
exceptionto this arerandomvariablesthat actas parametersof probabilitydistributions. For those,we
use upper case Greek letters Ξ,Φ,Θ,..., for their usually continuous state spaces we use ∆ ,∆ ,∆ ,...
Ξ Θ Φ
andfor specific values the lowercase Greek letters ξ,φ,θ,.... In caseswhere a randomvariable playsthe
roleofanestimate ofanothervariableX,we writethe estimateasXˆ,its state spaceasXˆ andits values
as xˆ.
7
We distinguish different types of probability distributions with letters p,q,r and d. Here, p corre-
sponds to probability distributions describing properties of the physical world including the agent and
its environment, q identifies model probabilities used by the agent internally, r denotes approximations
ofsuchmodelprobabilities whicharealsointernalto the agent,andddenotes aprobability distribution
that can be replaced by a q or a r distribution. We write conditional probabilities in the usual way, e.g.
p(y|x). For a model of this conditional probability parameterised by θ, we write q(yˆ|xˆ,θ).
5 Perception-Action Loop
E E E
0 1 2
S S S
0 1 2
A A
1 2
M M
1 2
Figure1: FirsttwotimestepsoftheBayesiannetworkrepresentingtheperception-actionloop(PA-loop).
All subsequent time steps are identical to the one from time t=1 to t=2.
Inthissectionweintroduceanagent’sperception-actionloop(PA-loop)asacausalBayesiannetwork.
This formalism forms the basis for our treatment of active inference. The PA-loop should be seen as
specifying the (true) dynamics of the underlying physical system that contains agent and environment
as well as their interactions. In Friston’s formulation, the environment dynamics of the PA-loop are
referred to as the generative process. In general these dynamics are inaccessible to the agent itself.
Nonetheless, parts of these (true) dynamics are often assumed to be known to the agent in order to
simplify computation (see e.g. Friston et al., 2015). We first formally introduce the PA-loop as causal
Bayesian network, and then state specific assumptions for the rest of this article.
5.1 PA-loop Bayesian Network
Figure 1 shows an agent’s PA-loop, formalised as causal Bayesian network. The network describes the
following causal dependencies over time: At t=0 an initial environment state e ∈E leads to an initial
0
sensor value s ∈S. This sensor value influences the memory state m ∈M of the agent at time t=1.
0 1
Depending on this memory state, action a ∈ A is performed which influences the transition of the
1
environment state from e to e ∈E. The new environment state leads to a new sensor value s which,
0 1 1
together with the performed action a and the memory state m , influence the next memory state m .
1 1 2
The loop then continues in this way until a final time step T.
8
We assume that all variables are finite and that the PA-loop is time-homogeneous1. We exclude the
first transitionfromt=0 to t=1 fromthe assumption oftime-homogeneity in orderto avoidhaving to
pickanarbitraryactionwhichprecedestheinvestigatedtime-frame. Thefirsttransitionisthussimplified
to p(m |s ,a ) := p(m |s ). Under the assumption of time-homogeneity and the causal dependencies
1 0 0 1 0
expressed in Figure 1, the joint probability distribution over the entire PA-loop is defined by:
T
p(e ,s ,a ,m )= p(a |m )p(m |s ,a )p(s |e )p(e |a ,e ) p(s |e )p(e ) (1)
0:T 0:T 1:T 1:T t t t t−1 t−1 t t t t t−1 0 0 0
!
t=1
Y
where e is shorthand for states (e ,e ,...,e ). In order to completely determine this distribution
0:T 0 1 T
we therefore have to specify the state spaces E,S,A, and M as well as the following probabilities and
mechanisms for all e ,e ,e ∈E;s ,s ∈S;a ,a ∈A;m ,m ,m ∈M for t>0:
0 t t+1 0 t t t+1 1 t t+1
1 • initial environment distribution: p(e 0 ), • action generation: p(a t |m t ), 4
2 • environment dynamics: p(e t+1 |a t+1 ,e t ), • initial memory step p(m 1 |s 0 ), 5
3 • sensor dynamics: p(s t |e t ), • memory dynamics: p(m t+1 |s t ,a t ,m t ). 6
In the following we will refer to a combination of initial environment distribution, environment dy-
namics, and sensor dynamics simply as an environment. Similarly, an agent is a particular combination
of initial memory step, memory dynamics, and action generation. The indexing convention we use here
is identical to the one used for the generative model (see Section 6.1) in Friston et al. (2015).
Also,note the dependence ofM onS , M , andadditionally A inFigure 1. Inthe literature,
t t−1 t−1 t−1
the dependence on A is frequently not allowed (Ay et al., 2012; Ay and Lo¨hr, 2015). However, we
t−1
assumeanefference-likeupdateofthememory. Notethatthisdependenceinadditiontothedependence
on m is only relevant if the actions are not deterministic functions of the memory state2. If action
t−1
selectionisprobabilistic,knowingtheoutcomea oftheactiongenerationmechanismp(a |m )will
t−1 t−1 t−1
convey more information than only knowing the past memory state m . This additional information
t−1
canbeusedininferenceabouttheenvironmentstateandfundamentallychangetheintrinsicperspective
of an agent. We do not discuss these changes in more detail here but the reader should be aware of the
assumption.
In a realistic robot scenario, the action a , if it is to be known by the agent, can only refer to the
t
“action signal” or “action value” that is sent to the robot’s physical actuators. These actuators will
usually be noisy and the robot will not have access to the final effect of the signal it sends. The (noisy)
conversion of an action signal to a physical configuration change of the actuator is here seen as part of
theenvironmentdynamicsp(e |a ,e ). Similarly,thesensorvalueisthesignalthatthephysicalsensor
t t t−1
1This means that all state spaces and transition probabilities areindependent of the time step, e.g. Mt =Mt−1 and
p(st|et)=p(st−1|et−1).
2In the deterministic case there is a function f : M → A such that p(mt|st−1,at−1,mt−1) =
p(mt|st−1,f(mt−1),mt−1)=p(mt|st−1,mt−1).
9
oftherobotproducesasaresultofausuallynoisymeasurement,sojustliketheactuator,theconversion
of a physicalsensor configurationto a sensor value is part of the sensor dynamics p(s |e ) which in turn
t t
belongs to the environment. As we will see later, the actions and sensor values must have well defined
state spaces A and S for inference on an internal model to work. This further justifies this perspective.
5.2 Assumptions
For the restof this article we assumethat the environmentstate space E, sensorstate space S aswellas
environment dynamics p(e |a ,e ) and sensor dynamics p(s |e ) are arbitrarily fixed and that some
t+1 t+1 t t t
initial environmental state e is given. Since we are interested in intrinsic motivations, our focus is not
0
on specific environment or sensor dynamics but almost exclusively on action generation mechanisms of
agents that rely minimally on the specifics of these dynamics.
Inordertofocusonactiongeneration,weassumethatalltheagentswedealwithherehavethesame
memorydynamics. Forthis,wechooseamemorythatstoresallpastsensorvaluess =(s ,s ,...,s )
≺t 0 1 t−1
and actions a = (a ,a ,...,a ) in the memory state m . This type of memory is also used in
≺t 1 2 t−1 t
Friston et al. (2015, 2016b) and provides the agentwith all existing data about its interactions with the
environment. Inthis respect,itcouldbe calledaperfectmemory. Atthe sametime, whateverthe agent
learned from s and a that remains true based on the next time step’s s and a must be
≺t ≺t (cid:22)t+1 (cid:22)t+1
relearned from scratch by the agent. A more efficient memory use might store only a sufficient statistic
of the past data and keep reusable results of computations in memory. Such improvements are not part
of this article (see e.g. Fox and Tishby, 2016, for discussion).
Formally, the state space M of the memory is the set of all sequences of sensor values and actions
that can occur. Since there is only a sensor value and no action at t =0, these sequences always begin
with a sensor value followed by pairs of sensor values and actions. Furthermore, the sensor value and
actionat t=T are never recorded. Since we haveassumed a time-homogeneous memory state space M
we must define it so that it contains all these possible sequences from the start. Formally, we therefore
choose the union of the spaces of sequences of a fixed length (similar to a Kleene-closure):
T−1
M=S∪ S×(S×A)t . (2)
!
t=1
[
With this we can define the dynamics of the memory as:
1 if m =s
1 0
p(m |s ):= (3)
1 0

0 else.

1 if m
t
=m
t−1
s
t−1
a
t−1
p(m |s ,a ,m ):= (4)
t t−1 t−1 t−1

0 else.

This perfect memory may seem unrealistic and can cause problems if the sensor state space is large
(e.g. high resolution images). However, we are not concerned with this type of problem here. Usually,
10
the computation of actions based on past actions and sensor values becomes a challenge of efficiency
long before storage limitations kick in: the necessary storage space for perfect memory only increases
linearly with time, while, as we show later, the number of operations for Bayesian inference increases
exponentially.
Forcompleteness we alsonote how the memory dynamics look if actions area deterministic function
f :M→A of the memory state. Recall that in this case we can drop the edge from A to M in the
t−1 t
PA-loop in Figure 1 and have a =f(m ) so that we can define:
t t
1 if m =s
1 0
p(m |s ):= (5)
1 0

0 else.

1 if m
t
=m
t−1
s
t−1
f(m
t−1
)
p(m |s ,m ):= (6)
t t−1 t−1

0 else.

Given a fixed environment and the memory dynamics, we only have to define the action generation
mechanism p(a |m ) to fully specify the perception-action loop. This is the subject of the next two
t t
sections.
In order to stay as close to Friston et al. (2015) as possible, we first explain the individual building
blocks that can be extracted from Friston’s active inference as described in Friston et al. (2015). These
are the variational inference and the action selection. We then show how these two building blocks are
combinedinthe originalformulation. We eventuallyleverageourseparationofcomponentsto showhow
the action selection component can be modified, and thus extend the active inference framework.
6 Inference and Complete Posteriors
Ultimately, an agent needs to select actions. Inference based on past sensor values and actions is only
needed if it is relevant to the action selection. Friston’s active inference approach promises to perform
actionselectionwithinthesameinferencestepthatisusedtoupdatetheagent’smodeloftheenvironment.
Inthissection,welookatthe inference componentonlyandshowhowanagentcanupdate agenerative
model in response to observed sensor values and performed actions.
ThenaturalwayofupdatingsuchamodelisBayesianinferenceviaBayes’rule. Thistypeofinference
leads to what we call the complete posterior. The complete posterior represents all knowledge that the
agent can obtain about the consequences of its actions from its past sensor values and actions. In
Section 7 we discuss how the agent can use the complete posterior to decide what is the best action to
take.
Bayesianinference as straightforwardrecipe isusually notpracticaldue to computationalcosts. The
memory requirements of the complete posterior update increases exponentially with time and so does
the number of operations needed to select actions. To keep the computational tractable, we have to
limit ourselves to only use parts of the complete posterior. Furthermore, since the direct expressions
(evenofparts)ofcompleteposteriorsareusuallyintractable,approximationsareneeded. Friston’sactive
11
inference is committed to variational inference as an approximation technique. Therefore, we explain
howvariationalinferencecanbeusedasanapproximationtechnique. Oursetupforvariationalinference
(generative model and approximate posterior) is identical to the one in Friston et al. (2015), but in this
sectionweignoretheinferenceofactionsincludedthere. Wewilllookattheextensiontoactioninference
in Section 7.
In the perception-action loop in Figure 1, action selection (and any inference mechanism used in the
course of it) depends exclusively on the memory state m . As mentioned in Section 5, we assume that
t
this memory state contains all past sensor values s and all past actions a . To save space, we write
≺t ≺t
sa :=(s ,a ) to refer to both sensor values and actions. We then have:
≺t ≺t ≺t
m =sa . (7)
t ≺t
However,sinceitismoreintuitivetounderstandinferencewithrespecttopastsensorvaluesandactions
than in terms of memory, we use sa explicitly here in place of m .
≺t t
6.1 Generative Model
Ξ2 Θ2
Ξ3 Θ3 Eˆ Eˆ Eˆ
0 1 2
Sˆ Sˆ Sˆ
0 1 2
Aˆ Aˆ
1 2
Ξ1 Θ1
Figure 2: Bayesian network of the generative model with parameters Θ = (Θ1,Θ2,Θ3) and hyperpa-
rameters Ξ = (Ξ1,Ξ2,Ξ3). Hatted variables are models / estimates of non-hatted counterparts in the
perception-action loop in Figure 1. An edge that splits up connecting one node to n nodes (e.g. Θ2 to
Eˆ ,Eˆ ,...) corresponds to n edges from that node to all the targets under the usual Bayesian network
1 2
convention. Note that in contrast to the perception-action loop in Figure 1, imagined actions Aˆ have
t
no parents. They are either set to past values or, for those in the future, a probability distribution over
them must be assumed.
The inference mechanism, internal to the action selection mechanism p(a|m), takes place on a hi-
erarchical generative model (or density, in the continuous case). “Hierarchical” means that the model
has parametersand hyperparameters,and “generative”indicates that the model relates parameters and
latent variables, i.e. the environment state, as “generative” causes to sensor values and actions as data
in a joint distribution. The generative model we investigate here is a part of the generative model used
in Friston et al. (2015). For now, we omit the probability distribution over future actions and the “pre-
12
cision”, which are only needed for active inference and are discussed later. The generative models in
Friston et al. (2016a,b, 2017a) are all closely related.
NotethatwearenotinferringthecausalstructureoftheBayesiannetworkorstatespacecardinalities,
but define the generative model as a fixed Bayesian network with the graph shown in Figure 2. It
is possible to infer the causal structure (see e.g. Ellis and Wong, 2008), but in that case, it becomes
impossible to represent the whole generative model as a single Bayesian network (Ortega, 2011).
The variables in the Bayesian network in Figure 2 that model variables occurring outside of p(a|m)
in the perception-action loop (Figure 1), are denoted as hatted versions of their counterparts. More
precisely:
• sˆ∈Sˆ=S are modelled sensor values,
• aˆ∈Aˆ=A are modelled actions,
• eˆ∈Eˆare modelled environment states.
To clearly distinguish the probabilities defined by the generative model from the true dynamics, we use
the symbol q instead of p. In accordance with Figure 2, and also assuming time-homogeneity, the joint
probability distribution over all variables in the model until some final modelled time Tˆ is given by:
q(eˆ ,sˆ ,aˆ ,θ1,θ2,θ3,ξ1,ξ2,ξ3):=
0:T 0:T 1:T
T 3 (8)
q(sˆ|eˆ,θ1)q(eˆ|aˆ ,eˆ ,θ2)q(aˆ ) q(sˆ |eˆ ,θ1)q(eˆ |θ3) q(θi|ξi)q(ξi)
t t t t t−1 t 0 0 0
! !
t=1 i=1
Y Y
Here, θ1,θ2,θ3 are the parameters of the hierarchicalmodel, and ξ1,ξ2,ξ3 are the hyperparameters. To
save space, we combine the parameters and hyperparameters by writing
θ :=(θ1,θ2,θ3) (9)
ξ :=(ξ1,ξ2,ξ3). (10)
To fully specify the generative model, or equivalently a probability distribution over Figure 2, we have
to specify the state spaces Eˆ,Sˆ,Aˆand:
7 • q(sˆ|eˆ,θ1) the sensor dynamics model, • q(ξ1) sensor dynamics hyperprior, 14
8 • q(eˆ′|aˆ′,eˆ,θ2) the environment dynamics • q(ξ2) environment dynamics hyperprior, 15
9 model,
10 • q(eˆ 0 |θ3) the initial environment state model, • q(ξ3) initial environment state hyperprior, 16
11 • q(θ1|ξ1) the sensor dynamics prior, • Tˆ last modelled time step, 17
12 • q(θ2|ξ2) the environment dynamics prior, • q(aˆ t )forallt∈{1,,...,Tˆ}theprobabilitydis- 18
13 • q(θ3|ξ3) the initial environment state prior, tribution over the actions at time t. 19
13
Thestatespacesofthe parametersandhyperparametersaredeterminedbythe choiceofEˆ,Sˆ,Aˆ. We
will see in Section 6.2 that Sˆ=S and Aˆ=A should be chosen in order to use this model for inference
onpastsensorvaluesandactions. ForEˆitisnotnecessarytosetitequaltoE forthemethodsdescribed
to work. We note that if we set Eˆ equal to the memory state space of Equation (2) the model and its
updates become equivalent to those used by the Bayesianuniversalreinforcement learning agent Hutter
(2005) in a finite (environment and time-interval) setting (see Section 6.3).
The last modelled time step Tˆ can be chosen as Tˆ = T, but it is also possible to always set it to
Tˆ =t+n,in whichcase n specifies a future time horizonfromcurrenttime step t. Suchan agentwould
model a future that goes beyond the externally specified last time step T. The dependence of Tˆ on t
(which we do not denote explicitly) within p(a|m) is possible since the current time step t is accessible
from inspection of the memory state m which contains a sensor sequence of length t.
t
The generative model assumes that the actions are not influenced by any other variables, hence we
have to specify action probabilities. This means that the agent does not model how its actions come
about,i.e.itdoes notmodelits owndecisionprocess. Instead,the agentis interestedinthe (parameters
of) the environment and sensor dynamics. It actively sets the probability distributions over past and
futureactionsaccordingtoitsneeds. Inpractice,iteitherfixestheprobabilitydistributionstoparticular
values (by using Dirac delta distributions) or to values that optimise some measure. We look into the
optimisation options in more detail later.
Note that the parameters and hyperparameters are standard random variables in the Bayesian net-
workof the model. Also, the rules for calculating probabilitiesaccordingto this modelare just the rules
for calculating probabilities in this Bayesian network.
In what follows, we assume that the hyperparameters are fixed as Ξ1 = ξ1,Ξ2 = ξ2,Ξ3 = ξ3. The
following procedures (including both Bayesianand variationalinference) canbe generalisedto also infer
hyperparameters. However, our main reference (Friston et al., 2015) and most publications on active
inference also fix the hyperparameters.
6.2 Bayesian Complete Posteriors
Duringactiongeneration(i.e.withinp(a|m))attimet,theagenthasretainedallitspreviouslyperceived
sensor states and its previously performed actions in memory. The “experience” or data contained in
its memory is thus m = sa . This data can be plugged into the generative model to obtain posterior
t ≺t
probability distributions over all non-observed random variables. Also, the model can estimate the not
yetobservedsensor values sˆ , past andfuture unobservable environmentstates eˆ , parametersθ and
t:Tˆ 0:Tˆ
hyperparameters ξ. These estimations are done by setting:
Aˆ =a ,for τ <t (11)
τ τ
and
Sˆ =s ,for τ <t. (12)
τ τ
14
ξ2 Θ2
ξ3 Θ3 Eˆ Eˆ Eˆ
0 1 2
s
0
s
1
Sˆ
2
a
1
Aˆ
2
ξ1 Θ1
Figure 3: Internal generative model with plugged in data up to t = 2 with Sˆ = s ,Sˆ = s and
0 0 1 1
Aˆ = a as well as from now on fixed hyperparameters ξ = (ξ1,ξ2,ξ3). Conditioning on the plugged in
1 1
data leads to the posterior distribution q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ). Predictions for future sensor values
t:Tˆ 0:Tˆ t:Tˆ ≺t
can be obtained by marginalising out other random variables e.g. to predict Sˆ we would like to get
2
q(sˆ |s ,s ,a ,ξ). Note however that this requires an assumption for the probability distribution over
2 0 1 1
Aˆ .
2
as shownin Figure 3 for t=2. For these assignments to be generallypossible, we need to choose Aˆand
Sˆ equal to A and S respectively. The resulting posterior probability distribution over all non-observed
random variables is then, according to standard rules of calculating probabilities in a Bayesiannetwork:
q(s ,sˆ ,eˆ ,a ,aˆ ,θ,ξ)
q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ):=
≺t t:Tˆ 0:Tˆ ≺t t:Tˆ
. (13)
t:Tˆ 0:Tˆ t:Tˆ ≺t
q(s ,sˆ ,eˆ ,a ,aˆ ,θ,ξ)dθ
sˆ t:Tˆ,eˆ 0:Tˆ,aˆ t:Tˆ ≺t t:Tˆ 0:Tˆ ≺t t:Tˆ
R P
Eventually, the agent needs to evaluate the consequences of its future actions. Just as it can update
the model with respect to past actions and sensor values, the agent can update its evaluations with
“contemplated”futureactionsequencesaˆ . Foreachsuchfutureactionsequenceaˆ ,theagentobtains
t:Tˆ t:Tˆ
a distribution over the remaining random variables in the model:
q(s ,sˆ ,eˆ ,a ,aˆ ,θ,ξ)
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ):=
≺t t:Tˆ 0:Tˆ ≺t t:Tˆ
. (14)
t:Tˆ 0:Tˆ t:Tˆ ≺t
q(s ,sˆ ,eˆ ,a ,aˆ ,θ,ξ)dθ
sˆ t:Tˆ,eˆ 0:Tˆ ≺t t:Tˆ 0:Tˆ ≺t t:Tˆ
R P
We call each such distribution a Bayesian complete posterior. We choose the term complete posterior
since the “posterior” by itself usually refers to the posterior distribution over the parameters and latent
variables q(θ,eˆ |sa ,ξ) (we here call this a posterior factor, see Equation (16)) and the posterior
t−1 ≺t
predictive distributions marginalise out the parameters and latent variables to get q(sˆ |aˆ ,sa ,ξ).
t:Tˆ t:Tˆ ≺t
The complete posteriors are probability distributions over all random variables in the generative model
including parameters, latent variables, and future variables. In this sense the set of all (Bayesian)
complete posteriors represents the complete knowledge state of the agent at time t about consequences
of future actions after updating the model with past actions and observed sensor values sa . At each
≺t
time step the sequence of past actions and sensor values is extended from sa to sa (i.e. m goes
≺t ≺t+1 t
to m ) and a new set of complete posteriors is obtained.
t+1
15
Allintrinsic motivations discussedinthis articleevaluate future actionsbasedonquantities thatcan
be derived from the corresponding complete posterior.
It is important to note that the complete posterior can be factorised into a term containing the
influence of past sensor values and actions (data). This factorisation can be made on the parameters
θ and ξ, the environment states eˆ , predicted future environment states eˆ and sensor values sˆ
≺t t:Tˆ t:Tˆ
depending on the future actions aˆ , and the estimated environment state eˆ and θ. Using the
t:Tˆ t−1
conditional independence
SA ⊥⊥Sˆ ,Eˆ |Aˆ ,Eˆ ,Θ,Ξ, (15)
≺t t:Tˆ t:Tˆ t:Tˆ t−1
which can be identified (via d-separation (Pearl, 2000)) from the Bayesian network in Figure 3, we can
rewrite this as:
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)=q(sˆ ,eˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ). (16)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t:Tˆ t−1 ≺t ≺t
This equation represents the desired factorisation. This formulation separates complete posteriors into
a predictive and a posterior factor. The predictive factor is given as part of the generative model
(Equation (8))
Tˆ
q(sˆ ,eˆ |aˆ ,eˆ ,θ)= q(sˆ |eˆ ,θ1)q(eˆ |aˆ ,eˆ ,θ2) (17)
t:Tˆ t:Tˆ t:Tˆ t−1 r r r r r−1
r=t
Y
and does not need to be updated through calculations at different time steps. This factor contains
the dependence of the complete posterior on future actions. This dependency reflects that, under the
given generative model, the consequences of actions for each combination of Θ and Eˆ remain the
t−1
same irrespective of experience. What changes when a new action and sensor value pair comes in is the
distribution over the values of Θ and Eˆ and with them the expectations over consequences of actions.
t−1
Ontheotherhand,theposteriorfactormustbeupdatedateverytimestep. InAppendixA,wesketch
the computation which shows that it involves a sum over |E|t elements. This calculation is intractable
as time goes on and one of the reasons to use approximate inference methods like variational inference.
Duetotheabovefactorisation,wemayonlyneedtoapproximatetheposteriorfactorq(eˆ ,θ|sa ,ξ)
≺t ≺t
and use the exact predictive factor if probabilities involving future sensor values or environment states
are needed.
Thisistheapproachtakene.g.inFriston et al.(2015). However,itisalsopossibletodirectlyapprox-
imate parts of the complete posteriorinvolvingrandomvariablesin both factors, e.g.by approximating
q(eˆ ,θ1|aˆ ,sa ,ξ). This latter approach is taken in Friston et al. (2016b) and we see it again in
0:Tˆ t:Tˆ ≺t
Equation (43) but in this publication the focus is on the former approach.
In the next section, we look at the special case of universal reinforcement learning before we go on
to variational inference to approximate the posterior factor of the (Bayesian) complete posteriors.
16
6.3 Connection to Universal Reinforcement Learning
In this section, we relate the generative model of Equation (8) and its posterior predictive distribution
to those used by the Bayesian universal reinforcement learning agent. Originally, this agent is defined
by Hutter (2005). More recent work includes Leike (2016) and (for the current purpose sufficient and
particularly relevant) Aslanides et al. (2017).
Let us set Eˆ=M with M as in Equation (2) and let the agent identify each past sa with a state
≺t
of the environment, i.e.
eˆ =sa . (18)
t−1 ≺t
Under this definition the next environment state eˆ is just the concatenation of the last environment
t
state sa with the next next action selected by the agent aˆ and the next sensor value sˆ:
≺t t t
eˆ =sˆaˆ =sa sˆaˆ . (19)
t (cid:22)t ≺t t
So given a next contemplated action a¯ˆ the next environment state eˆ is already partially determined.
t t
Whatremainsto be predictedisonlythe nextsensorvalue sˆ. Formally,this is reflectedinthe following
t
derivation:
q(eˆ|a¯ˆ ,eˆ ,θ2):=q(sˆ,aˆ ,sˆaˆ |a¯ˆ ,sa ,θ2) (20)
t t t−1 t t ≺t t ≺t
=q(sˆ|aˆ ,sˆaˆ ,a¯ˆ ,sa ,θ2)q(aˆ ,sˆaˆ |a¯ˆ ,sa ,θ2) (21)
t t ≺t t ≺t t ≺t t ≺t
=q(sˆ t |aˆ t ,sˆaˆ ≺t ,a¯ˆ t ,sa ≺t ,θ2)δ a¯ˆt (aˆ t )δ sa≺t (sˆaˆ ≺t ) (22)
=q(sˆ t |a¯ˆ t ,sa ≺t ,θ2)δ a¯ˆt (aˆ t )δ sa≺t (sˆaˆ ≺t ). (23)
This shows that in this case the model of the next environment state (the left hand side) is determined
by the model of the next sensor value q(sˆ|a¯ˆ ,sa ,θ2).
t t ≺t
Soinsteadofcarryingadistributionoverpossiblemodelsofthenextenvironmentstatesuchanagent
only needs to carry a distribution over models of the next sensor value. Furthermore, an additional
model q(sˆ|eˆ,θ1) of the dependence of the sensor values on environment states parameterised by θ1 is
superfluous. The next predicted sensor value is already predicted by the model q(sˆ|aˆ ,sa ,θ2). It is
t t ≺t
therefore possible to drop the parameter θ1.
The parameter θ3, for the initial environment state distribution, becomes a distribution over the
initial sensor value since eˆ =sˆ :
0 0
q(eˆ |θ3)=q(sˆ |θ3). (24)
0 0
We can then derive the posterior predictive distribution and show that it coincides with the one given
17
in Aslanides et al. (2017). For the complete posterior of Equation (16) we find:
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)=q(sˆ ,eˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ) (16 revisited)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t:Tˆ t−1 ≺t ≺t
=q(eˆ |sˆ ,aˆ ,eˆ ,θ)q(sˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ) (25)
t:Tˆ t:Tˆ t:Tˆ t−1 t:Tˆ t:Tˆ t−1 ≺t ≺t
t Tˆ
=q(sˆ |aˆ ,sa ,θ)q(θ|sa ,ξ) δ (eˆ ) δ (eˆ ). (26)
t:Tˆ t:Tˆ ≺t ≺t sa≺τ τ sa≺tsˆaˆt:τ τ
τ=0 τ=t+1
Y Y
To translate this formulation into the notation of Aslanides et al. (2017) first drop the representationof
the environment state which is determined by the sensor values and actions anyway. This means that
the complete posterior only needs to predict future sensor values and parameters. Formally, this means
the complete posterior can be replaced without loss of generality:
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)→q(sˆ |aˆ ,sa ,θ)q(θ|sa ,ξ). (27)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ ≺t ≺t
To translate notations let θ → ν; aˆ,a → a; sˆ,s → e. Also, set Tˆ → t because only one step futures are
considered in universal reinforcement learning (this is due to the use of policies instead of future action
sequences). Then, the equation for the posterior predictive distribution
q(sˆ|aˆ ,sa ,ξ)= q(sˆ|aˆ ,sa ,θ)q(θ|sa ,ξ)dθ, (28)
t t ≺t t t ≺t ≺t
Z
is equivalent to Aslanides et al. (2017, Eq. (5)) (the sum replaces the integral for a countable ∆ ):
Θ
ξ(e|ae ,a)= p(e|ν,ae ,a)p(ν|ae ) (29)
≺t ≺t ≺t
ν
X
⇔ξ(e)= p(e|ν)p(ν), (30)
ν
X
wherewedroppedtheconditioningonae ,afromthenotationinthesecondlineasdoneintheoriginal
≺t
(where this is claimed to improve clarity). Also note that ξ(e) would be written q(e|ξ) in our notation.
In the universal reinforcement learning literature parameters like θ (or ν) and ξ are sometimes directly
used to denote the probability distribution that they parameterise.
Updating of the posterior q(θ|sa ,ξ) in response to new data also coincides with updating of the
≺t
18
weights p(ν):
q(θ,s |a ,sa ,ξ)
t t ≺t
q(θ|sa ,ξ)= (31)
(cid:22)t
q(s |a ,sa ,ξ)
t t ≺t
q(s |a ,sa ,θ,ξ)q(θ|a ,sa ,ξ)
t t ≺t t ≺t
= (32)
q(s |a ,sa ,ξ)
t t ≺t
q(s |a ,sa ,θ)q(θ|sa ,ξ)
t t ≺t ≺t
= (33)
q(s |a ,sa ,ξ)
t t ≺t
q(s |a ,sa ,θ)
t t ≺t
= q(θ|sa ,ξ). (34)
≺t
q(s |a ,sa ,ξ)
t t ≺t
The first two lines are general. From the second to third we used
S ⊥⊥Ξ|A ,SA ,Θ (35)
t t ≺t
and
Θ⊥⊥A |SA ,Ξ (36)
t ≺t
which follow from the Bayesian network structure Figure 2. In the notation of Aslanides et al. (2017)
Equation (34) becomes
p(e|ν)
p(ν|e)= p(ν). (37)
p(e)
This shows that assuming the same model class ∆ the predictions and belief updates of an agent
Θ
using the Bayesian complete posterior of Section 6.2 are the same as those of the Bayesian universal
reinforcement learning agent. Action selection can then be performed just as in Aslanides et al. (2017)
as well. This is done by selecting policies. In the present publication we instead select action sequences
directly. However, in both cases the choice maximises the value predicted by the model. More on this
in Section 7.2.
6.4 Approximate Complete Posteriors
As mentioned in the last section, the complete posterior can be approximated via variational inference
(see Attias,1999;Winn and Bishop,2005;Bishop,2011;Blei et al.,2017). Therearealternativemethods
such as belief propagation, expectation propagation (Minka, 2001; Vehtari et al., 2014), and sampling-
basedmethods(Lunn et al.,2000;Bishop,2011),butactiveinferencecommitstovariationalinferenceby
framing inference as variational free energy minimisation (Friston et al., 2015). Variational free energy
(Equation (45)) is just the negative evidence lower bound (ELBO) of standard variational inference
(e.g. Blei et al., 2017). In the following, we show how the complete posterior can be approximated via
variational inference.
Theideabehindvariationalinferenceistouseasimplefamilyofprobabilitydistributionsandidentify
19
Φ2 Θ2 ΦE0 ΦE1
Φ3 Θ3 Eˆ Eˆ
0 1
Φ1 Θ1
Figure 4: Bayesian network of the approximate posterior factor at t = 2. The variational parameters
Φ1,Φ2,Φ3 and ΦE≺t = (ΦE0,ΦE1) are positioned so as to indicate what dependencies and nodes they
replace in the generative model in Figure 2.
Φ2 Θ2 ΦE0 ΦE1
Φ3 Θ3 Eˆ Eˆ Eˆ
0 1 2
Sˆ
2
aˆ
2
Φ1 Θ1
Figure5: BayesiannetworkoftheapproximatecompleteposteriorofEquation(40)att=2forthefuture
actions aˆ . Only Eˆ ,Θ1,Θ2 and the future action aˆ appear in the predictive factor and influence
t:Tˆ t−1 t:Tˆ
future variables. In general there is one approximate complete posterior for each possible sequence aˆ
t:Tˆ
of future actions.
20
the member of that family which approximates the true complete posterior best. This turns inference
into an optimisation problem. According to Wainwright and Jordan (2007) this reformulation as an
optimisation problem is the essence of variational methods. If the family of distributions is chosen
such that it includes the complete posterior then the optimisation will eventually lead to the same
result as Bayesian inference. However, one advantage of the formulation as an optimisation is that it
can also be performed over a family of probability distributions that is simpler than the family that
includes the actual complete posterior. This is what turns variational inference into an approximate
inference procedure. Usually, the (simpler) families of probability distributions are chosen as products
of independent distributions.
RecallingEquation(16), the complete posteriorasa productofa predictive andaposteriorfactoris:
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)=q(sˆ ,eˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ). (16 revisited)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t:Tˆ t−1 ≺t ≺t
This product is the main object of interest. We want to approximate the formula with a probability
distribution that lets us (tractably) calculate the posteriors required by a given intrinsic motivation,
which can consequently be used for action selection.
As mentioned before, to approximatethe complete posterior we here approximate only the posterior
factor and use the given generative model’s predictive factor as is done in Friston et al. (2015)3 The
approximate posterior factor is then combined with the exact predictive factor to get the approximate
complete posterior. Let us write r(eˆ ,θ|φ) for the approximate posterior factor (Figure 4), defined as:
≺t
r(eˆ
≺t
,θ|φ):=r(eˆ
≺t
|φE≺t)r(θ|φ) (38)
t−1 3
:= r(eˆ |φEτ) r(θi|φi). (39)
τ
τ=0 i=1
Y Y
Aswecanseeitmodelseachoftherandomvariablesthattheposteriorfactorrangesoverasindependent
of all others. This is called a mean field approximation. Then, the approximate complete posterior
(Figure 5) is:
r(sˆ ,eˆ ,θ|aˆ ,φ):=q(sˆ ,eˆ |aˆ ,eˆ ,θ)r(eˆ ,θ|φ). (40)
t:Tˆ 0:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ t−1 ≺t
Note that the variationalparameter absorbs the hyperparameter ξ as well as the past sensor values and
actions sa . The parameter does not absorb future actions which are part of the predictive factor.
≺t
The dependence on future actions needs to be kept if we want to select actions using the approximate
complete posterior.
3AcloseinspectionofFristonetal.(2015,Eq. (9))showsthattheapproximatecompleteposteriorthatendsupbeing
evaluated by the action-value function is the one we discuss in Equation (40). It uses the predictive factor to get the
probabilitiesr(eˆt:Tˆ|aˆt:Tˆ,eˆt−1,φ)offutureenvironmentstates. However,theapproximateposteriorinFristonetal.(2015,
Eq.(10)) uses a factorisation of all future environment states like the one we give in Equation (43). The probabilities of
futureenvironmentstatesinthatposteriorarenotusedanywhereinFristonetal.(2015). Inprinciple,theycouldbeused
as is done in Fristonetal. (2016b, Eq. (2.6)) where the complete posterior of Equation (43) is used in the action-value
function. Bothapproaches arepossible.
21
We have:
r(sˆ ,eˆ ,θ|aˆ ,φ)≈q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ) (41)
t:Tˆ 0:Tˆ t:Tˆ t:Tˆ 0:Tˆ t:Tˆ ≺t
if
r(eˆ ,θ|φ)≈q(eˆ ,θ|sa ,ξ). (42)
≺t ≺t ≺t
This approximationcan be achieved by standard variational inference methods.
Forthose interestedmore inthe approximationofthe complete posterioras inFriston et al. (2016b),
we provide the used family of factorised distributions. It must be noted that the agent in this case
carriesaseparateapproximateposteriorforeachpossiblecompleteactionsequenceaˆ . Forpredictions
0:T
of environment states, it does not use the predictive factor, but instead looks at the set of generative
modelscompatiblewiththepast. Foreachofthose,theagentconsidersallenvironmentstatesatdifferent
times as independent. The approximate posteriors, compatible with a past sequence of actions a , are
≺t
of the form:
Tˆ
r(sˆ
t:Tˆ
,eˆ
0:Tˆ
,θ1|aˆ
t:Tˆ
,a
≺t
,φ1)=q(sˆ
t:Tˆ
|eˆ
t:Tˆ
,θ1) r(eˆ
τ
|aˆ
t:Tˆ
,a
≺t
,φEτ)r(θ1|φ1). (43)
τ=0
Y
Notealsothattherelationbetweensensorvaluesandenvironmentstatesisstillprovidedbythegenerative
models’sensordynamicsq(sˆ |eˆ ,θ1). Inthisarticlehowever,wefocusontheapproachinFriston et al.
t:Tˆ t:Tˆ
(2015) which requires only one approximate posterior at time t since future actions only occur in the
predictive factors which we do not approximate.
We define the relative entropy (or KL-divergence) between the approximate and the true posterior
factor:
r(eˆ ,θ|φ)
KL[r(Eˆ ,Θ|φ)||q(Eˆ ,Θ|sa ,ξ)]:= r(eˆ ,θ|φ)log ≺t dθ. (44)
≺t ≺t ≺t ≺t
q(eˆ ,θ|sa ,ξ)
X
eˆ≺t Z
≺t ≺t
Notethat,weindicatethevariablesthataresummedoverbycapitalisingthem. TheKL-divergencequan-
tifies the difference between the two distributions. It is non-negative, and only zero if the approximate
and the true posterior factor are equal (see e.g. Cover and Thomas, 2006).
Thevariationalfreeenergy,alsoknownasthe(negative)evidencelowerbound(ELBO)invariational
inference literature, is defined as:
r(eˆ ,θ|φ)
≺t
F[ξ,φ,sa ]:= r(eˆ ,θ|φ)log dθ (45)
≺t ≺t
q(s ,eˆ ,θ|a ,ξ)
X
eˆ≺t Z
(cid:22)t ≺t ≺t
=−logq(s |a ,ξ)+KL[r(Eˆ ,Θ|φ)||q(Eˆ ,Θ|sa ,ξ)] (46)
≺t ≺t ≺t ≺t ≺t
The first term in Equation (46) is the surprise of negative log evidence. For a fixed hyperparameter ξ
it is a constant. Minimising the variational free energy therefore directly minimises the KL-divergence
between the true and the approximate posterior factor given sa and ξ.
≺t
22
In our case, variational inference amounts to solve the optimisation problem:
φ∗ :=argminF[φ,sa ,ξ]. (47)
sa≺t,ξ ≺t
φ
This optimisation is a standard problem. See Bishop (2011); Blei et al. (2017) for ways to solve it.
The resulting variational parameters φ∗ = (φE0 ,...,φ Et−1 ,φ1 ,φ2 ,φ3 ) define
sa≺t,ξ sa≺t,ξ sa≺t,ξ sa≺t,ξ sa≺t,ξ sa≺t,ξ
the approximateposteriorfactor. Thevariationalparameters,togetherwiththe exactpredictivefactors,
allow us to compute the approximate complete posteriors for each sequence of future actions aˆ :
t:Tˆ
r(sˆ ,eˆ ,θ|aˆ ,φ∗ )=q(sˆ ,eˆ |aˆ ,eˆ ,θ)r(eˆ ,θ|φ∗ ) (48)
t:Tˆ 0:Tˆ t:Tˆ sa≺t,ξ t:Tˆ t:Tˆ t:Tˆ t−1 ≺t sa≺t,ξ
≈q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ). (49)
t:Tˆ 0:Tˆ t:Tˆ ≺t
Inthenextsection,welookatactionselectionasthesecondcomponentofactiongeneration. Tothis
end, we show how to evaluate sequences of future actions aˆ by evaluating either Bayesian complete
t:Tˆ
posteriors or the approximate complete posteriors.
7 Action Selection Based on Intrinsic Motivations
7.1 Intrinsic Motivation and Action-Value Functions
The previous section resulted in sets of Bayesian or approximate complete posteriors. Independently
of whether a complete posterior is the approximate or the Bayesian version, it represents the entire
knowledge of the agent about the consequences of the sequence of future actions aˆ that is associated
t:Tˆ
with it. In order to evaluate sequences of future actions the agent can only rely on its knowledge which
suggeststhatallsuchevaluationsshoulddependsolelyoncompleteposteriors. Onecouldarguethatthe
motivation might also depend directly on the memory state containing sa . We here take a position
≺t
somewhat similar to the one proposed by Schmidhuber (2010) that intrinsic motivations concerns the
“learning of a better world model”. We consider the complete posterior as the current world model
and assume that intrinsic motivations depend only on this model and not on the exact values of past
sensor values and actions. As we will see this assumption is also enough to capture the three intrinsic
motivations that we discuss here. This level of generality is sufficient for our purpose of extending the
freeenergyprinciple. Whetheritsufficientforafinalandgeneralintrinsicmotivationdefinitionisbeyond
the scope of this publication.
Complete posteriors are essentially conditional probability distributions over
SˆTˆ−t+1×EˆTˆ+1×∆
Θ
given elements of
AˆTˆ−t+1.
A necessary (but not sufficient) requirement for intrinsic motivations in
our context (agents with generative models) is then that they are functions on the space of such
conditional probability distributions. Let ∆ be the space of conditional prob-
SˆTˆ−t+1×EˆTˆ+1×∆Θ|AˆTˆ−t+1
ability distributions over
SˆTˆ−t+1
×
EˆTˆ+1
× ∆ given elements of
AˆTˆ−t+1.
Then an intrinsic moti-
Θ
vation is a function M : ∆ × AˆTˆ−t+1 → R taking a probability distribution
SˆTˆ−t+1×EˆTˆ+1×∆Θ|AˆTˆ−t+1
d(.,.,.|.) ∈∆ and a given future actions sequence aˆ
∈AˆTˆ−t+1
to a real value
SˆTˆ−t+1×EˆTˆ+1×∆Θ|AˆTˆ−t+1 t:Tˆ
23
M(d(.,.,.|.),aˆ )∈R. We can then see that the Bayesiancomplete posterior q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)
t:Tˆ t:Tˆ 0:Tˆ t:Tˆ ≺t
for a fixed past sa written as q(.,.,.|.,sa ,ξ) provides such conditional probability distribution. Sim-
≺t ≺t
ilarly, every member of the family of distributions used to approximate the Bayesiancomplete posterior
viavariationalinferencer(sˆ ,eˆ ,θ|aˆ ,φ)writtenasr(.,.,.|.,φ)alsoprovidessuchaconditionalprob-
t:Tˆ 0:Tˆ t:Tˆ
ability distribution. It will become important when discussing active inference that the optimised value
φ∗ of the variational parameters as well as any other value of the variational parameters φ define
sa≺t,ξ
an element with the right structure to be evaluated together with a set of future actions by an intrinsic
motivation function.
Usingintrinsicmotivationfunctionswethendefinetwokindsofinducedaction-valuefunctions. These
aresimilartovaluefunctionsinreinforcementlearning. 4 ThefirstistheBayesian action-value function
(or functional):
Qˆ(aˆ ,sa ,ξ):=M(q(.,.,.|.,sa ,ξ),aˆ ). (50)
t:Tˆ ≺t ≺t t:Tˆ
InwordstheBayesianaction-valuefunctionQˆ(aˆ ,sa ,ξ)infersthesetofBayesiancompleteposteriors
t:Tˆ ≺t
of past experience sa and then evaluates the sequence of future actions aˆ according to the intrinsic
≺t t:Tˆ
motivation function M.
The variational action-value function is defined as5:
Qˆ(aˆ ,φ):=M(r(.,.,.|.,φ),aˆ ). (51)
t:Tˆ t:Tˆ
So the variationalaction-value function Qˆ(aˆ ,φ) directly takes the conditionalprobability distribution
t:Tˆ
defined by variational parameter φ and evaluates the sequence of future actions aˆ according to M.
t:Tˆ
Unlike in the Bayesian case no inference takes place during the evaluation of Qˆ(aˆ ,φ).
t:Tˆ
At the same time, after variational inference, if we plug in φ∗ for φ we have:
sa≺t,ξ
Qˆ(aˆ ,φ∗ )≈Qˆ(aˆ ,sa ,ξ). (52)
t:Tˆ
a
sa≺t,ξ t:Tˆ
a
≺t
Note that the reason we have placed a hat on Qˆ is that, even in the Bayesian case, it is usually
not the optimal action-value function but instead is an estimate based on the current knowledge state
represented by the complete posteriors of the agent.
Alsonotethatsomeintrinsicmotivations(e.g.empowerment)evaluatee.g.thenextnactionsbyusing
predictions reaching n+m steps into the future. This means that they need all complete posteriors for
aˆ but only evaluate the actions aˆ . In other words they cannot evaluate actions up to
t:t+n+m−1 t:t+n−1
their generative model’s time-horizon Tˆ but only until a shorter time-horizon Tˆ = Tˆ −m for some
a
natural number m. When necessary we indicate such a situation by only passing shorter future action
sequences aˆ to the action-value function, in turn, the intrinsic motivation function. The respective
t:Tˆ
a
posteriors keep the original time horizon Tˆ >Tˆ .
a
4Themaindifferenceisthattheaction-valuefunctionshereevaluatesequences offutureactionsasopposedtopolicies.
ThisistheprevalentpracticeinactiveinferenceliteratureincludingFristonetal.(2015)andwethereforefollowithere.
5We abuse notation here by reusing the same symbol Qˆ for the variational action-value function as for the Bayesian
action-valuefunction. However,inthispublicationtheargument(sa≺t,ξ orφ)alwaysindicates whichoneismeant.
24
7.2 Deterministic and Stochastic Action Selection
We can then select actions simply by picking the first action in the sequence aˆ that maximises the
t:Tˆ
Bayesian action-value function:
aˆ∗ (m ):=aˆ∗ (sa ):=argmaxQˆ(aˆ ,sa ,ξ) (53)
t:Tˆ t t:Tˆ ≺t t:Tˆ ≺t
aˆ t:Tˆ
and set
aˆ∗(m ):=aˆ∗(m ). (54)
t t t
or for the variational action value function:
aˆ∗ (m ):=aˆ∗ (φ∗ ):=argmaxQˆ(aˆ ,φ∗ ). (55)
t:Tˆ t t:Tˆ sa≺t,ξ t:Tˆ sa≺t,ξ
aˆ t:Tˆ
and set
aˆ∗(m ):=aˆ∗(m ). (56)
t t t
This then results in a deterministic action generation p(a|m):
p(a t |m t ):=δ aˆ∗(mt) (a t ).
We notehere thatinthe caseofuniversalreinforcementlearningthe roleofQˆ(aˆ ,sa ,ξ)is played
t:Tˆ ≺t
by Vπ(sa ). There π is a policy that selects actions in dependence on the entire past sa and ξ
ξ ≺t ≺t
parameterises the posterior just like in the present publication. The argmax in Equation (53) selects a
policy instead of an action sequence and that policy is used for the action generation.
A possible stochastic action selection that is important for active inference is choosing the action
according to a so called softmax policy (Sutton and Barto, 1998):
1
p(a
t
|m
t
):= eγQˆ(aˆ t:Tˆ,sa≺t,ξ) (57)
Z(γ,sa ,ξ)
≺t
aˆ tX+1:Tˆ
where:
Z(γ,sa
≺t
,ξ):= eγQˆ(aˆ t:Tˆ,sa≺t,ξ) (58)
a X ˆ t:Tˆ
is a normalisation factor. Note that we are marginalising out later actions in the sequence aˆ to get a
t:Tˆ
25
distribution only over the action aˆ . For the variational action-value function this becomes:
t
p(a t |m t ):= Z(γ,φ 1 ∗ ) e γQˆ(aˆ t:Tˆ,φ∗ sa≺t,ξ ) (59)
aˆ tX+1:Tˆ sa≺t,ξ
where:
Z(γ,φ∗
sa≺t,ξ
):= e γQˆ(aˆ t:Tˆ,φ∗ sa≺t,ξ ). (60)
a X ˆ t:Tˆ
Since it is relevant for active inference (see Section 8), note that the softmax distribution over future
actions canalsobe definedfor arbitraryφand notonly for the optimisedφ∗ . Atthe same time, the
sa≺t,ξ
softmax distribution for the optimised φ clearly also approximates the softmax distribution of the
sa≺t,ξ
Bayesian action-value function.
Softmax policies assign action sequences with higher values of Qˆ higher probabilities. They are
often used as a replacement for the deterministic action selection to introduce some exploration. Here,
lower γ leads to higher exploration; conversely, in the limit where γ → ∞ the softmax turns into the
deterministic action selection. From an intrinsic motivation point of view such additional exploration
shouldbesuperfluousinmanycasessincemanyintrinsicmotivationstrytodirectlydriveexplorationby
themselves. Anotherinterpretationofsuchachoiceistoseeγ asatrade-offfactorbetweentheprocessing
cost of choosing an action precisely and achieving a high action-value. The lower γ, the higher the cost
ofprecision. Thisleadstothe agentmoreoftentakingactionsthatdonotattainmaximumaction-value.
We note that the softmax policy is not the only possible stochastic action selection mechanism. An-
otheroptiondiscussedintheliteratureisThompsonsampling(Ortega and Braun,2010,2014;Aslanides et al.,
2017). Inour frameworkthis correspondsto a twostep actionselectionprocedure where we firstsample
an environment and parameter pair (e¯ˆ ,θ¯) from a posterior factor (Bayesian or variational)
t−1
(e¯ˆ ,θ¯)∼d(Eˆ ,Θ|sa ,ξ) (61)
t−1 t−1 ≺t
then plug the according predictive factor q(sˆ ,eˆ |aˆ ,e¯ˆ ,θ¯) into the action value function
t:Tˆ t:Tˆ t:Tˆ t−1
Qˆ(aˆ ,sa ,ξ):=M(q(.,.|.,e¯ˆ ,θ¯),aˆ ). (62)
t:Tˆ ≺t t−1 t:Tˆ
Thisallowsintrinsicmotivationsthatonlyevaluatethe probabilitydistributionoverfuturesensorvalues
Sˆ and environment states Eˆ . However, it rules out those that evaluate the posterior probability of
t:Tˆ t:Tˆ
environment parameters Θ because we sample a specific θ¯.
7.3 Intrinsic Motivations
Now, we look at some intrinsic motivations including the intrinsic motivation part underlying Friston’s
active inference.
In the definitions, we use d(.,.,.|.) ∈ ∆ as a generic conditional probability
SˆTˆ−t+1×EˆTˆ+1×∆Θ|AˆTˆ−t+1
26
distribution. The generic symbol d is used since it represents both Bayesian complete posteriors and
approximate complete posteriors. In fact, the definitions of the intrinsic motivations are agnostic with
respect to the method used to obtain a complete posterior. In the present context, it is important that
these definitions are generalenough to induce both Bayesianand variationalaction-value functions. We
usually state the definition of the motivation function using general expressions (e.g. marginalisations)
derived from d(,.,.|.). Also, we look at how they can be obtained from Bayesian complete posteriors to
givetothe readeranintuitionforthe computationsinvolvedinapplications. The approximatecomplete
posterior usually makes these calculations easier and we will present an example of this.
7.3.1 Free Energy Principle
Here, we present the non-variational Bayesian inference versions for the expressions that occur in the
“expectedfreeenergy”inFriston et al.(2015,2017a). Thesepapersonlyincludeapproximateexpressions
after variational inference. Most of the expressions we give here can be found in Friston et al. (2017b).
TheexceptionisEquation(74),whichcanbeobtainedfromanapproximateterminFriston et al.(2017a)
in the same waythat the non-variationalBayesianinference terms in Friston et al. (2017b)are obtained
from the approximate ones in Friston et al. (2015).
In the following,we can set Tˆ =Tˆ, since actions are only evaluated with respect to their immediate
a
effects.
Accordingto Friston et al. (2017b, Eq. (A.2) supplementary material),the “expectedfree energy”is
just the future conditional entropy of sensor values6 given environment states. Formally, this is (with
a negative sign to make minimising expected free energy equivalent to maximising the action-value
function):
M(d(.,.,.|.),aˆ ):= d(eˆ |aˆ ) d(sˆ |eˆ )logd(sˆ |eˆ ) (63)
t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ
X eˆ t:Tˆ X sˆ t:Tˆ
=− d(eˆ |aˆ )H (Sˆ |eˆ ) (64)
t:Tˆ t:Tˆ d t:Tˆ t:Tˆ
X eˆ t:Tˆ
=−H (Sˆ |Eˆ ,aˆ ). (65)
d t:Tˆ t:Tˆ t:Tˆ
Note that, we indicate the probability distribution d used to calculate entropies H (X) or mutual infor-
d
mations I (X : Y) in the subscript. Furthermore,we indicate the variables that are summed over with
d
capital letters and those that are fixed (e.g. aˆ above) with small capital letters.
t:Tˆ
In the case where d(.,.,.|.) is the Bayesiancomplete posterior q(.,.,.|.,sa ,ξ), it uses the predictive
≺t
distribution of environment states q(eˆ |aˆ ,sa ,ξ) and the posterior of the conditional distribution
t:Tˆ t:Tˆ ≺t
of sensor values given environment states q(sˆ |eˆ ,sa ,ξ). As we see next, both distributions can be
t:Tˆ t:Tˆ ≺t
obtained from the Bayesiancomplete posterior.
The former distribution is a familiar expression in hierarchical Bayesian models and corresponds to
a posterior predictive distribution or predictive density (cmp. e.g. Bishop, 2011, Eq.(3.74)) that can be
6Theoriginaltextreferstothisasthe“expectedentropyofoutcomes”,nottheexpectedconditionalentropyofoutcomes.
Nonetheless,theassociatedEquation(A.2)intheoriginalisidenticaltoours.
27
calculated via:
q(eˆ |aˆ ,sa ,ξ)= q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)dθ (66)
t:Tˆ t:Tˆ ≺t t:Tˆ 0:Tˆ t:Tˆ ≺t
Z sˆ t:XTˆ,eˆ≺t
= q(sˆ ,eˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ)dθ (67)
t:Tˆ t:Tˆ t:Tˆ t−1 ≺t ≺t
Z sˆ t:XTˆ,eˆ≺t
= q(eˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ)dθ, (68)
t:Tˆ t:Tˆ t−1 t−1 ≺t
Z eˆ Xt−1
where we split the complete posterior into the predictive and posterior factor and then marginalisedout
environment states eˆ since the predictive factor does not depend on them. Note that in practice,
≺t−1
this marginalisation corresponds to a sum over |E|t−1 terms and therefore has a computational cost
that grows exponential in time. However, if we use the approximate complete posterior such that
d(.,.,.|.)=r(.,.,.|.,φ), we see from Equation (40), that q(eˆ ,θ|sa ,ξ) is replaced by r(eˆ ,θ|φ) which
≺t ≺t ≺t
is defined as (Equation (38)):
t−1 3
r(eˆ
≺t
,θ|φ):= r(eˆ
τ
|φEτ) r(θi|φi). (69)
τ=0 i=1
Y Y
This means that r(eˆ
t−1
,θ|φ) is just r(eˆ
t−1
|φEt−1)r(θ|φ), which we obtain directly from the variational
inference without any marginalisation. If Bayesian inference increases in computational cost exponen-
tially in time, this simplification leads to a significant advantage. This formulation leaves an integral
over θ or, more precisely, a triple integral over the three θ1,θ2,θ3. However, if the q(θi|ξi) are chosen
as conjugate priors to q(sˆ|eˆ,θ1),q(eˆ′|aˆ′,eˆ,θ2),q(eˆ |θ3) respectively, then these integrals can be calcu-
0
lated analytically (compare the similar calculation of q(eˆ ,θ|sa ,ξ) in Appendix A). The remaining
≺t ≺t
computational problem is only the sum over all eˆ .
t−1
The latter term (the posterior conditional distribution over sensor values given environment states)
can be obtained via
q(sˆ |eˆ ,sa ,ξ)=q(sˆ |eˆ ,aˆ ,sa ,ξ) (70)
t:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t:Tˆ ≺t
q(sˆ ,eˆ |aˆ ,sa ,ξ)
=
t:Tˆ t:Tˆ t:Tˆ ≺t
. (71)
q(eˆ |aˆ ,sa ,ξ)
t:Tˆ t:Tˆ ≺t
Here, the first equation holds since
Sˆ ⊥⊥Aˆ |Eˆ ,SA . (72)
t:Tˆ t:Tˆ t:Tˆ ≺t
Bothnumeratoranddenominatorcanbeobtainedfromthecompleteposteriorviamarginalisationasfor
the former term. This marginalisation also shows that the intrinsic motivation function, Equation (63),
is a functional of the complete posteriors or d(.,.,.|.).
InmostpublicationsonactiveinferencetheexpectedfreeenergyinEquation(63)isonlypartofwhat
28
is referredto as the expected free energy. Usually, there is a second term measuringthe relative entropy
to anexternallyspecifiedprior over future outcomes (alsocalled“predictivedistributionencoding goals”
Friston et al.2015), i.e.adesiredprobabilitydistributionpd(sˆ ). Therelativeentropytermis formally
t:Tˆ
given by:
d(sˆ |aˆ )
KL[d(Sˆ |aˆ )||pd(Sˆd )]= d(sˆ |aˆ )log t:Tˆ t:Tˆ . (73)
t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ pd(sˆ )
X sˆ t:Tˆ
t:Tˆ
Clearly,this termwillleadthe agenttoactsuchthatthe future distributionoversensorvaluesissimilar
to the desired distribution. Since this term is used to encode extrinsic value for the agent, we mostly
ignore it in this publication. It could included into any of the following intrinsic motivations.
In Friston et al. (2017a) yet another term, called “negative novelty” or “ignorance”, occurs in the
expectedfree energy. This termconcernsthe posteriordistributionoverparameterθ1. Itcanbe slightly
generalised to refer to any subset of the parameters θ = (θ1,θ2,θ3). We can write it as a conditional
mutual information between future sensor values and parameters (the “ignorance” is the negative of
this):
d(θ|sˆ ,aˆ )
I (Sˆ :Θ|aˆ )= d(sˆ |aˆ ) d(θ|sˆ ,aˆ )log t:Tˆ t:Tˆ dθ. (74)
d t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ t:Tˆ
d(θ)
X sˆ t:Tˆ Z
This is identical to the information gain used in knowledge seeking agents. The necessary posteriors in
the Bayesian case are q(sˆ |aˆ ,sa ,ξ), q(θ|sˆ ,aˆ ,sa ,ξ) and q(θ|sa ,ξ) with
t:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ ≺t ≺t
q(sˆ |aˆ ,sa ,ξ)= q(sˆ |aˆ ,eˆ ,θ)q(eˆ ,θ|sa ,ξ)dθ (75)
t:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t−1 ≺t ≺t
Z
X
eˆ≺t
astraightforward(ifcostly)marginalisationofthecompleteposterior. Justlikepreviouslyforq(eˆ |aˆ ,sa ,ξ),
t:Tˆ t:Tˆ ≺t
the marginalisation is greatly simplified in the variational case (see Appendix B for a more explicit cal-
culation). The integrals can be computed if using conjugate priors. The other two posteriors can be
obtained via
1
q(θ|sˆ ,aˆ ,sa ,ξ)= q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ). (76)
t:Tˆ t:Tˆ ≺t
q(sˆ |aˆ ,sa ,ξ)
t:Tˆ 0:Tˆ t:Tˆ ≺t
t:Tˆ t:Tˆ ≺t
e X ˆ 0:Tˆ
and
q(θ|sa ,ξ)=q(θ|aˆ ,sa ,ξ) (77)
≺t t:Tˆ ≺t
= q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ). (78)
t:Tˆ 0:Tˆ t:Tˆ ≺t
sˆ t:XTˆ,eˆ 0:Tˆ
29
In the latter equation we used
Aˆ ⊥⊥Θ|SA . (79)
t:Tˆ ≺t
The marginalisations grow exponentially in computational cost with Tˆ. In this case, the variational
approximationonlyreducesthe necessarymarginalisationovereˆ to oneovereˆ ,butthe marginal-
≺t−1 t−1
isationoverfutureenvironmentstateseˆ andsensorvaluessˆ remainsthesamesinceweusetheexact
t:Tˆ t:Tˆ
predictivefactor. InpracticethetimehorizonintothefutureTˆ−tmustthenbechosensufficientlyshort,
so that marginalising out eˆ and Sˆ is feasible. Together with the variational approximation the re-
t:Tˆ t:Tˆ
quiredmarginalisationsoverpastandfuturearethenconstantovertimewhichmakestheimplementation
of agents with extended lifetimes possible.
Thecombinationoftheconditionalentropytermandthe informationgaindefinesthe(intrinsicpart)
of the action-value function of Friston’s active inference (or free energy principle):
MFEP(d(.,.,.|.),aˆ )=−H (Sˆ |Eˆ )+I (Sˆ :θ|aˆ ) (80)
t:Tˆ d t:Tˆ t:Tˆ d t:Tˆ t:Tˆ
In the active inference literature this is usually approximated by a sum over the values at individual
timesteps:
Tˆ
MFEP(d(.,.,.|.),aˆ )= −H (Sˆ |Eˆ )+I (Sˆ :Θ|aˆ ). (81)
t:Tˆ d τ τ d τ t:Tˆ
τ=t
X
7.3.2 Free Energy Principle Specialised to Friston et al. (2015)
Using Appendix C, we show how to get the action-value function of Friston et al. (2015, Eq. (9)) in
our framework. In Friston et al. (2015), the information gain of Equation (74) is not included, but the
extrinsic term of Equation (73) is. Furthermore, the sum over timesteps in Equation (81) is used. This
leads to the following expression:
Tˆ
MFEP(d(.,.,.|.),aˆ )= −H (Sˆ |Eˆ )−KL[d(Sˆ |aˆ )||pd(Sˆ )]. (82)
t:Tˆ d τ τ τ t:Tˆ τ
τ=t
X
If we plug in an approximate complete posterior, we get:
Tˆ
MFEP(r(.,.,.|.),aˆ )= −H (Sˆ |Eˆ )−KL[r(Sˆ |aˆ )||pd(Sˆ )]. (83)
t:Tˆ r τ τ τ t:Tˆ τ
τ=t
X
with
−H (Sˆ |Eˆ )= r(eˆ |aˆ ,eˆ ,φ) r(sˆ |eˆ ,φ)logr(sˆ |eˆ ,φ), (84)
r τ τ τ t:Tˆ t−1 τ τ τ τ
X
eˆτ
X
sˆτ
30
and
r(sˆ |aˆ ,φ)
KL[r(Sˆ |aˆ )||pd(Sˆ )]= r(sˆ |aˆ ,φ)log τ t:Tˆ . (85)
τ t:Tˆ τ τ t:Tˆ pd(sˆ )
τ
X
sˆτ
For the particular approximate posterior of Equation (40), with its factorisation into exact predictive
and approximate posterior factor, the individual terms can be further rewritten.
r(eˆ |aˆ ,eˆ ,φ)= r(sˆ ,eˆ ,θ|aˆ ,φ)dθ (86)
τ t:Tˆ t−1 t:Tˆ 0:Tˆ t:Tˆ
sˆ t:Tˆ,eˆ τ+1:XTˆeˆt:τ−1eˆ0:t−2Z
= q(sˆ ,eˆ |aˆ ,eˆ ,θ)r(eˆ ,θ|φ)dθ (87)
t:Tˆ t:Tˆ t:Tˆ t−1 ≺t
sˆ t:Tˆ,eˆ τ+1:XTˆeˆt:τ−1eˆ0:t−2Z
t−1 3
= q(sˆ
t:Tˆ
,eˆ
t:Tˆ
|aˆ
t:Tˆ
,eˆ
t−1
,θ) r(eˆ
r
|φEr) r(θi|φi)dθ (88)
sˆ t:Tˆ,eˆ τ+1:XTˆeˆt:τ−1eˆ0:t−2Z r
Y
=0 i
Y
=1
= q(eˆ
t:τ−1
|aˆ
t:Tˆ
,eˆ
t−1
,θ2)r(eˆ
t−1
|φEt−1)r(θ2|φ2)dθ2 (89)
eˆtX:τ−1Z
τ
= q(eˆ |aˆ ,eˆ ,θ2)r(θ2|φ2)dθ2 r(eˆ |φEt−1). (90)
r r r−1 t−1
ÑeˆtX:τ−1Z r
Y
=t é
InFriston et al.(2015),theenvironmentdynamicsq(eˆ |aˆ ,eˆ ,θ2)arenotinferredandaretherefore
r r r−1
not parameterised:
q(eˆ |aˆ ,eˆ ,θ2)=q(eˆ |aˆ ,eˆ ) (91)
r r r−1 r r r−1
and are set to the physical environment dynamics:
q(eˆ |aˆ ,eˆ )=p(eˆ |aˆ ,eˆ ). (92)
r r r−1 r r r−1
This means the integral over θ2 above is trivial and we get:
τ
r(eˆ |aˆ ,eˆ ,φ)= q(eˆ |aˆ ,eˆ )r(eˆ |φEt−1) (93)
τ t:Tˆ t−1 r r r−1 t−1
eˆtX:τ−1r
Y
=t
(94)
In the notation of Friston et al. (2015) (see Appendix C for a translation table), we have
q(eˆ |aˆ ,eˆ )=B(aˆ ) (95)
r r r−1 r eˆreˆr−1
where B(aˆ ) is a matrix, and
r
r(eˆ
t−1
|φEt−1)=(s
t−1
)
eˆt−1
(96)
Û
31
where (s ) is a vector, so that
t−1
r(eˆ |aˆ ,eˆ ,φ)=(B(aˆ )···B(aˆ )·s ) (97)
Û τ t:Tˆ t−1 τ t t−1 eˆτ
=:(s (aˆ )) (98)
τ t:Tˆ eˆτ
Û
Similarly, since the sensor dynamics in Friston et al. (2015) are also not inferred, we find
Û
r(sˆ |eˆ ,φ)=q(sˆ |eˆ )=p(sˆ |eˆ ). (99)
τ τ τ τ τ τ
Friston et al. writes:
q(sˆ |eˆ )=:A (100)
τ τ sˆτeˆτ
with A a matrix. So that,
r(sˆ |aˆ ,φEt−1)=A·s (aˆ ) (101)
τ t:Tˆ τ t:Tˆ
=:o (aˆ ). (102)
τ t:Tˆ
Û
Then
Û
H (Sˆ |Eˆ )=−1·(A×logA)·s (aˆ ) (103)
r τ τ τ t:Tˆ
where × is a Hadamard product and 1 is a vector of ones. Also,
Û
KL[r(Sˆ |aˆ )||pd(Sˆ )]=o (aˆ )·(logo (aˆ )−logC ) (104)
τ t:Tˆ τ τ t:Tˆ τ t:Tˆ τ
where (C ) = pd(sˆ ). Plugging these expressions into Equation (83), substituting aˆ → π, and
τ sˆτ τ Û Û t:Tˆ
comparing this to Friston et al. (2015, Eq. (9)) shows that7:
MFEP(r(.,.,.|.),π)=1·(A×logA)·s (aˆ )−o (aˆ )·(logo (aˆ )−logC ) (105)
τ t:Tˆ τ t:Tˆ τ t:Tˆ τ
=Q(π). (106)
Û Û Û
Thisverifiesthatourformulationoftheaction-valuefunctionspecialisestothe“expected(negative)free
energy” Q(π).
7.3.3 Empowerment Maximisation
Empowermentmaximisation(Klyubin et al., 2005) is anintrinsicmotivationthat seeksto maximisethe
channel capacity from sequences of the agent’s actions into the subsequent sensor value. The agent,
equipped with complete knowledge of the environment dynamics, can directly observe the environment
7ThereisasmalltypoinFristonetal.(2015,Eq. (9))wherethetimeindexofst−1in(sτ(aˆt:Tˆ))=(B(aˆτ)···B(aˆt)·st−1)
isgivenastinsteadoft−1.
Û Û Û
32
state. If the environment is deterministic, an empowerment maximisation policy leads the agent to a
state from which it can reach the highest number of future states within a preset number of actions.
Salge et al. (2014) provide a good overview of existing research on empowerment maximisation. A
more recent study relates the intrinsic motivation to the essential dynamics of living systems, based on
assumptionsfromautopoieticenactivismGuckelsberger and Salge(2016b). Severalapproximationshave
beenproposed,alongwith experimentalevaluationsin complexstate / actionspaces. Salge et al. (2018)
showhowdeterministicempowermentmaximisationinathree-dimensionalgrid-worldcanbemademore
efficient by different modifications of UCT tree search. Three recent studies approximate stochastic em-
powerment and its maximisation via variational inference and deep neural networks, leveraging a varia-
tionalboundonthemutualinformationproposedbyBarber and Agakov(2003). Mohamed and Rezende
(2015) focus on a model-free approximation of open-loop empowerment, and Gregor et al. (2016) pro-
pose two means to approximate closed-loop empowerment. While these two approaches consider both
applications in discrete and continuous state / action spaces, Karl et al. (2017) develop an open-loop,
model-based approximation for the continuous domain specifically. The latter study also demonstrates
how empowerment can yield good performance in established reinforcement learning benchmarks such
as bipedal balancing in the absence of extrinsic rewards. In recent years, research on empowerment has
particularly focused on applications in multi-agent systems. Coupled empowerment maximisation as
a specific multi-agent policy has been proposed as intrinsic drive for either supportive or antagonistic
behaviour in open-ended scenarios with sparse reward landscapes Guckelsberger et al. (2016b). This
theoretical investigation has then been backed up with empirical evaluations on supportive and adver-
sarial video game characters Guckelsberger et al. (2016a, 2018). Beyond virtual agents, the same policy
has been proposed as a good heuristic to facilitate critical aspects of human-robot interaction, such
as self-preservation, protection of the human partner, and response to human actions Salge and Polani
(2017).
For empowerment, we select Tˆ = t+n and Tˆ = t+n+m, with n ≥ 0 and m ≥ 1. This means
a
the agent chooses n+1 actions which it expects to maximise the resulting m-step empowerment. The
according action-value function is:
MEM(d(.,.,.|.),aˆ ):= max I (Aˆ :Sˆ |aˆ ) (107)
t:Tˆ
d(aˆ Tˆa+1:Tˆ)
d Tˆ a+1:Tˆ Tˆ t:Tˆ
a
d(sˆ |aˆ )
= max d(aˆ )d(sˆ |aˆ )log
Tˆ t:Tˆ
. (108)
d(aˆ Tˆa+1:Tˆ)
aˆ Tˆa+X1:Tˆ,sˆ Tˆ
Tˆ a+1:Tˆ Tˆ t:Tˆ
d(sˆ Tˆ |aˆ t:Tˆ
a
)
Note that in the denominator of the fraction, the action sequence only runs to t:Tˆ and not to t:Tˆ as
a
in the numerator.
IntheBayesiancase,therequiredposteriorsareq(sˆ |aˆ ,sa ,ξ)(foreachaˆ )andq(sˆ |aˆ ,sa ,ξ).
Tˆ t:Tˆ ≺t Tˆ a+1:Tˆ Tˆ t:Tˆ
a
≺t
Theformerdistributionisafurthermarginalisationoversˆ ofq(sˆ |aˆ ,sa ,ξ). Thevariational
t+1:Tˆ−1 t:Tˆ t:Tˆ ≺t
approximation only helps getting q(sˆ |aˆ ,sa ,ξ), not the further marginalisation. The latter distri-
t:Tˆ t:Tˆ ≺t
33
bution is obtained for a given q(aˆ ) from the former one via
Tˆ a+1:Tˆ
q(sˆ |aˆ ,sa ,ξ)= q(sˆ ,aˆ |aˆ ,sa ,ξ) (109)
Tˆ t:Tˆ
a
≺t Tˆ Tˆ a+1:Tˆ t:Tˆ
a
≺t
aˆ TˆXa+1:Tˆ
= q(sˆ |aˆ ,aˆ ,sa ,ξ)q(aˆ ) (110)
Tˆ Tˆ a+1:Tˆ t:Tˆ
a
≺t Tˆ a+1:Tˆ
aˆ TˆXa+1:Tˆ
since the empowerment calculation imposes
q(aˆ |aˆ ,sa ,ξ)=q(aˆ ). (111)
Tˆ a+1:Tˆ t:Tˆ
a
≺t Tˆ a+1:Tˆ
7.3.4 Predictive Information Maximisation
Predictiveinformationmaximisation,(Ay et al.,2008), isanintrinsicmotivationthatseekstomaximise
the predictive information of the sensor process. Predictive information is the mutual information be-
tween past and future sensory signal, and has been proposed as a general measure of complexity of
stochastic processes (Bialek and Tishby, 1999). For applications in the literature see Ay et al. (2012);
Martius et al. (2013, 2014). Also, see Little and Sommer (2013) for a comparison to entropy minimisa-
tion.
For predictive information, we select a half time horizon k = ⌊(t:Tˆ −t+1)/2⌋ where k > 0 for
predictive information to be defined (i.e. t:Tˆ − t > 0). Then, we can define the expected mutual
information between the next m sensor values and the subsequent m sensor values as the action-value
function of predictive informationmaximisation. This is similar to the time-localpredictive information
in Martius et al. (2013):
MPI(d(.,.,.|.),aˆ ):=I (Sˆ :Sˆ |aˆ ). (112)
t:Tˆ d t:t+k−1 t+k:t+2k−1 t:Tˆ
We omit writing out the conditional mutual information since it is defined in the usual way. Note that
it is possible that t+2k−1 < t:Tˆ so that the action sequence aˆ might go beyond the evaluated
t:Tˆ
sensor probabilities. This displacement leads to no problem since the sensor values do not depend on
future actions. The posteriors needed are: q(sˆ |aˆ ,sa ,ξ), q(sˆ |sˆ ,aˆ ,sa ,ξ),
t:t+k−1 t:Tˆ ≺t t+k:t+2k−1 t:t+k−1 t:Tˆ ≺t
and q(sˆ |aˆ ,sa ,ξ). The first and the last are again marginalisations of q(sˆ |aˆ ,sa ,ξ)
t+k:t+2k−1 t:Tˆ ≺t t:Tˆ t:Tˆ ≺t
seen in Equation (75). The second posterior is a fraction of such marginalisations.
7.3.5 Knowledge Seeking
Knowledge seeking agents (Storck et al., 1995; Orseau et al., 2013) maximise the information gain with
respectto a probabilitydistribution overenvironments. The informationgainwe use here is the relative
entropy between the belief over environments after actions and subsequent sensor values and the belief
over environments (this is the KL-KSA of Orseau et al. 2013, “KL” for Kullback-Leibler divergence).
In our case the belief over environments can be identified with the posterior q(θ|sa ,ξ) since every
≺t
θ = (θ1,θ2,θ3) defines an environment. In principle, this can be extended to the posterior q(ξ|sa ,ξ)
≺t
34
over the hyperprior ξ, but we focus on θ here. This definition is more similar to the original one. Then,
we define the knowledge seeking action-value function using the information gain of Equation (74):
MKSA(d(.,.,.|.),aˆ ):=I (Sˆ :Θ|aˆ ). (113)
t:Tˆ d t:Tˆ t:Tˆ
We have discussed the necessary posteriors following Equation (74).
Afterthisoverviewofsomeintrinsicmotivations,welookatactiveinference. However,whatshouldbe
clearis,that, inprinciple,boththe posteriorsneededfortheintrinsicmotivationfunctionoftheoriginal
active inference (Friston et al., 2015) and the posteriors needed for alternative inferences overlap. This
overlap shows that the other intrinsic motivations mentioned here also profit from variational inference
approximations. There is also no indication that these intrinsic motivations cannot be used together
with the next discussed active inference.
8 Active Inference
Now, we look at active inference. Note that this section is independent of the intrinsic motivation
function underlying the action-value function Qˆ.
Inthe followingwe firstlook atandtry to explaina slightly simplified versionofthe active inference
in Friston et al. (2015). Afterwards we also state the full version.
Asmentionedintheintroduction,currentactiveinferenceversionsareformulatedasanoptimisation
procedure that, at least at first sight, looks similar to the optimisation of a variational free energy
familiar from variational inference. Recall that, in variational inference the parameters of a family of
distributions are optimised to approximate an exact (Bayesian) posterior of a generative model. In the
case we discussed in Section 6.4 the sought after exact posterior is the posterior factor of the generative
model of Section 6.1. One of our questions about active inference is whether it is a straightforward
application of variational inference to a posterior of some generative model. This would imply the
existence of a generative model whose standard updating with past actions and sensor values leads to
an optimal posterior distribution over future actions. Note that, this does not work with the generative
model in of Section 6.1 since the future actions there are independent of the past sensor values and
actions. Given the appropriate generativemodel, it would then be naturalto introduce it first and then
apply a variational approximationsimilar to our procedure in Section 6.
We were not able to find in the literature or construct ourselves a generative model such that varia-
tionalinference leadsdirectlyto the activeinferenceasgiveninFriston et al.(2015). Insteadwe present
agenerativemodelthatcontainsaposteriorwhosevariationalapproximationoptimisationisverysimilar
to the optimisation procedure of active inference. It is also closely related to the two-step action gener-
ation of first inferring the posterior and then selecting the optimal actions. This background provides
some intuition for the particularities of active inference.
One difference of the generative model used here is that its structure depends on the current time
step in a systematic way. The previous generative model of Section 6.1 had a time-invariant structure.
35
In Section 6, we showed how the generative model, together with either Bayesian or variational
inference,canprovideanagentwithasetofcompleteposteriors. Eachcompleteposteriorisaconditional
probabilitydistributionoverallcurrentlyunobservedvariables(Sˆ ,Eˆ )andparameters(Θandmore
t:Tˆ 0:T
generallyalsoΞ)givenpastsensorvaluesandactionssa andaparticularsequenceoffutureactionsaˆ .
≺t t:Tˆ
Inference means updating the set ofposteriorsin responseto observationssa . Active inference should
≺t
then update the distribution over future actions in response to observations. This means the according
posterior cannot be conditional on future action sequences like the complete posterior in Equation (16).
Since active inference promises belief or knowledge updating and action selection in one mechanism the
posterior should also range over unobserved relevant variables like future sensor values, environment
states, and parameters. This leads to the posterior of Equation (13):
q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ). (13 revisited)
t:Tˆ 0:Tˆ t:Tˆ ≺t
Ifthis posteriorhasthe rightstructure,thenwe canderivea future actiondistributionby marginalising:
q(aˆ |sa ,ξ)= q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ)dθ. (114)
t:Tˆ ≺t t:Tˆ 0:Tˆ t:Tˆ ≺t
sˆ t:XTˆ,eˆ 0:TˆZ
Actions can then be sampled from the distribution obtained by marginalising further to the next action
only:
p(a |m ):= q(aˆ |sa ,ξ). (115)
t t t:Tˆ ≺t
aˆ tX+1:Tˆ
This scheme couldjustifiably be called(non-variational)active inference sincethe future actiondistribu-
tion is directly obtained by updating the generative model.
However,aswementionedabove,accordingtothegenerativemodelofFigure2,thedistributionover
future actions is independent of the past sensor values and actions:
q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ)=q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ)q(aˆ ) (116)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ
since
q(aˆ |sa ,ξ)=q(aˆ ). (117)
t:Tˆ ≺t t:Tˆ
Therefore, we can never learn anything about future actions from past sensor values and actions using
this model. In other words, if we intend to select the actions based on the past, we cannot uphold this
independent model. The inferred actions must become dependent on the history and the generative
model has to be changed for a scheme like the one sketched above to be successful.
In Section 7.2, we have mentioned that the softmax policy based on a given action-value function Qˆ
36
couldbe adesirableoutcomeofanactiveinferenceschemesuchasthe above. Thus,ifweendedupwith
1
q(aˆ
t:Tˆ
|sa
≺t
,ξ)=
Z(γ,sa ,ξ)
eγQˆ(aˆ t:Tˆ,sa≺t,ξ) (118)
≺t
asaresultofsomeactiveinferenceprocess,thatwouldbeaviablesolution. Wecanforcethisbybuilding
thisconditionaldistributiondirectlyintoanewgenerativemodel. Notethatthisconditionaldistribution
determines all future actions aˆ starting at time t and not just the next action aˆ . In the end however
t:Tˆ t
only the next action will be taken according to Equation (115) and at time t+1 the action generation
mechanism starts again, now with aˆ influenced by the new data sa in addition to sa . So the
t+1:Tˆ t ≺t
model structure changes over time in this case with the dependency of actions on pasts sa shifting
≺t
together with each time-step. Keeping the rest of the previous Bayesian network structure intact we
define that at each time t the next action Aˆ depends on past sensor values and actions sa as well as
t ≺t
on the hyperparameter ξ (see Figure 6):
q(sˆ ,eˆ ,aˆ ,θ|sa ,ξ):=q(sˆ ,eˆ |aˆ ,eˆ ,θ)q(aˆ |sa ,ξ)q(θ,eˆ |sa ,ξ). (119)
t:Tˆ 0:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ t:Tˆ t−1 t:Tˆ ≺t ≺t ≺t
On the right hand side we have the predictive and posterior factors left and right of the distribu-
tion over future actions. We define this conditional future action distribution to be the softmax of
Equation (118). This means that the mechanism-generating future actions uses the Bayesian action-
value function Qˆ(aˆ ,sa ,ξ). The Bayesian action-value function depends on the complete posterior
t:Tˆ ≺t
q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ) calculated using the old generative model of Figure 2 where actions do not
t:Tˆ t:Tˆ t:Tˆ ≺t
not depend on past sensor values and actions. This is a complex construction with what amounts to
Bayesianinferenceessentiallyhappeningwithinanedge(i.e.SˆAˆ →Aˆ )ofaBayesiannetwork. How-
≺t t:Tˆ
ever, logically there is no problem since the posterior q(sˆ ,eˆ ,θ|aˆ ,sa ,ξ) for each aˆ to be well
t:Tˆ t:Tˆ t:Tˆ ≺t t:Tˆ
defined really only needs sa , ξ, and the model structure. Here we see the model structure as “hard
≺t
wired” into the mechanism, since it is fixed for each time step t from the beginning.
We now approximate the posterior of Equation (118) using variationalinference. Like in Section 6.4
wedonotapproximatethepredictivefactor. Insteadweonlyapproximatetheproductofposteriorfactor
q(θ,eˆ |sa ,ξ)andfutureactiondistributionq(aˆ |sa ,ξ). Byconstructionthesearetwoindependent
≺t ≺t t:Tˆ ≺t
factorsbutwithaneyetoactiveinferencewhichtreatsbelieforknowledgeupdatingandactiongeneration
togetherwealsotreatthemtogether. Fortheapproximationweagainusetheapproximateposteriofactor
of Equation (38) and combine it with a distribution over future actions r(aˆ |π) parameterised by π:
t:Tˆ
r(aˆ ,eˆ ,θ|π,φ):=r(aˆ |π)r(eˆ ,θ|φ) (120)
t:Tˆ ≺t t:Tˆ ≺t
:=r(aˆ
t:Tˆ
|π)r(eˆ
≺t
|φE≺t)r(θ|φ). (121)
37
Ξ2 Θ2
Ξ3 Θ3 Eˆ Eˆ Eˆ Eˆ
0 1 2 3
Sˆ Sˆ Sˆ Sˆ
0 1 2 3
Aˆ Aˆ Aˆ
1 2 3
Ξ1 Θ1
Figure 6: Generative modelincluding q(aˆ |sa ,ξ) att=2 with SˆAˆ influencing future actionsAˆ .
t:Tˆ ≺t ≺2 2:Tˆ
Note that, only future actions are dependent on past sensor values and actions, e.g. action Aˆ has no
1
incoming edges. The increased gap between time step t=1 and t =2 is to indicate that this time step
is special in the model. For each time step t there is an according model with the particular relation
between past SˆAˆ and Aˆ shifted accordingly.
≺t t:Tˆ
The variational free energy is then:
r(aˆ |π)r(eˆ ,θ|φ)
F[π,φ,sa ,ξ]:= r(aˆ |π)r(eˆ ,θ|φ)log
t:Tˆ ≺t
dθ (122)
≺t t:Tˆ ≺t
q(s ,aˆ ,eˆ ,θ|a ,ξ)
aˆ t:XTˆ,eˆ≺t Z ≺t t:Tˆ ≺t ≺t
r(aˆ |π)r(eˆ ,θ|φ)
= r(aˆ |π)r(eˆ ,θ|φ)log
t:Tˆ ≺t
dθ
t:Tˆ ≺t
q(aˆ |sa ,ξ)q(eˆ ,θ|sa ,ξ)q(s |a ,ξ)
aˆ t:XTˆ,eˆ≺t Z t:Tˆ ≺t ≺t ≺t ≺t ≺t
(123)
=F[φ,sa ,ξ]+KL[r(Aˆ |π)||q(Aˆ |sa ,ξ)]. (124)
≺t t:Tˆ t:Tˆ ≺t
Where F[φ,sa ,ξ] is the variational free energy of the (non-active) variational inference (see Equa-
≺t
tion (45)). Variational inference then minimises the above expression with respect to parameters φ and
π:
φ∗ ,π∗ :=argminF[π,φ,sa ,ξ] (125)
sa≺t,ξ sa≺t,ξ ≺t
φ,π
=argminF[φ,sa ,ξ]+argminKL[r(Aˆ |π)||q(Aˆ |sa ,ξ)]. (126)
≺t t:Tˆ t:Tˆ ≺t
φ π
We see that the minimisation in this case separates into two minimisation problems. The first is just
thevariationalinferenceofSection6.4andthesecondminimisestheKL-divergencebetweentheparame-
terisedactiondistributionr(aˆ |π)andthesoftmaxq(aˆ |sa ,ξ)oftheBayesianaction-valuefunction.
t:Tˆ t:Tˆ ≺t
38
It is instructive to look at this KL-divergence term closer:
KL[r(Aˆ |π)||q(Aˆ |sa ,ξ)]=−H (Aˆ |π)− r(aˆ |π)logq(aˆ |sa ,ξ) (127)
t:Tˆ t:Tˆ ≺t r t:Tˆ t:Tˆ t:Tˆ ≺t
a X ˆ t:Tˆ
=−H (Aˆ |π)− r(aˆ |π)Qˆ(aˆ ,sa ,ξ)+logZ(γ,sa ,ξ). (128)
r t:Tˆ t:Tˆ t:Tˆ ≺t ≺t
a X ˆ t:Tˆ
We see that the optimisation of π leads towards high entropy distributions for which the expectation
value of the action-value function Qˆ(aˆ ,φ) is large. Action selection could then happen according to
t:Tˆ
p(a |m ):= r(aˆ |π∗ ). (129)
t t t:Tˆ sa≺t,ξ
aˆtX+1:T
Sothedescribedvariationalinferenceprocedure,atleastformally,leadstoausefulresult. However,this
is not the active inference procedure of Friston et al. (2015). As noted above the minimisation actually
splits into two completely independent minimisations here. The result of the minimisation with respect
to φ in Equation (126) is actually not used for action selection and since action selection is all that
matters here is mere ornament. However, there is a way to make use of it. Recall that plugging φ∗
sa≺t,ξ
into the variational action-value function Qˆ(aˆ ,φ) means that it approximates the Bayesian action
t:Tˆ
value function (see Equation (52)). This means that if we define a softmax distribution r(aˆ |φ) of the
t:Tˆ
variational action-value function parameterised by φ as:
1
r(aˆ
t:Tˆ
|φ)=
Z(γ,φ)
eγQˆ(aˆ t:Tˆ,φ). (130)
Then this approximates the softmax of the Bayesian action-value function:
r(aˆ |φ∗ )≈q(aˆ |sa ,ξ). (131)
t:Tˆ sa≺t,ξ t:Tˆ ≺t
Consequently, once we have obtained φ∗ from the first minimisation problem in Equation (126) we
sa≺t,ξ
canplugitintor(aˆ |φ)andthenminimisetheKL-divergenceofr(aˆ |π)tothisdistributioninsteadof
t:Tˆ t:Tˆ
the one to q(aˆ |sa ,ξ). In this waythe result ofthe firstcould be reusedfor the secondminimisation.
t:Tˆ ≺t
This remains a two part action generation mechanism however. Active inference combines these two
steps into one minimisation by replacing q(aˆ |sa ,ξ) in the variational free energy of Equation (122)
t:Tˆ ≺t
with r(aˆ |φ). Since r(aˆ |φ) thereby becomes partofthe denominatoritis alsogiventhe samesymbol
t:Tˆ t:Tˆ
(in our case q) as the generative model. So we define:
q(aˆ |φ):=r(aˆ |φ). (132)
t:Tˆ t:Tˆ
In this form the softmax q(aˆ |φ) is a cornerstone of active inference. In brief, it can be regarded as a
t:Tˆ
prior over action sequences. To obtain purposeful behaviour it specifies prior assumptions about what
sorts of actions an agent should take when its belief parameter takes value φ. Strictly speaking the
expression resulting from the replacement q(Aˆ |sa ,ξ) → q(aˆ |φ) in Equation (122) is then not a
t:Tˆ ≺t t:Tˆ
39
variational free energy anymore since the variational parameters φ occur in both the numerator and
the denominator. Nonetheless, this is the functional that is minimised in active inference as described
in Friston et al. (2015). So active inference is defined as the optimisation problem (cmp. Friston et al.,
2015, Eq.(1)):
r(aˆ |π)r(eˆ ,θ|φ)
φ∗ ,π∗ =argmin r(aˆ |π)r(eˆ ,θ|φ)log t:Tˆ ≺t dθ (133)
sa≺t,ξ sa≺t,ξ t:Tˆ ≺t q(s ,aˆ ,eˆ ,θ|φ,a ,ξ)
φ,π aˆ t:XTˆ,eˆ≺t Z ≺t t:Tˆ ≺t ≺t
=argmin F[φ,sa ,ξ]+KL[r(Aˆ |π)||q(Aˆ |φ)] . (134)
≺t t:Tˆ t:Tˆ
φ,π
Ä ä
This minimisation does not split into the two independent parts anymore since both the future action
distribution q(Aˆ |φ) of the generative model and the approximate posterior factor in the variational
t:Tˆ
free energy F[φ,sa ,ξ] are parameterised by φ. This justifies the claim that active inference obtains
≺t
both belief update and action selection through a single principle or optimisation.
Compared to Friston et al. (2015), we have introduced a simplification of active inference. In the
original text, additional distributions over γ (with according random variable Γ) are introduced to the
generative model as q(γ|ξγ) (which is a fixed prior) and to the approximate posterior as r(γ|φγ). For
the sake of completeness, we show the full equations as well. Since γ is now part of the model, we
write q(aˆ |γ,φ) instead of q(aˆ |φ). The basic procedure above stays the same. The active inference
t:Tˆ t:Tˆ
optimisation becomes:
φ∗ ,φγ∗ ,π∗
sa≺t,ξ sa≺t,ξ sa≺t,ξ
=argmin r(aˆ |π)r(γ|φγ)r(eˆ ,θ|φ)log r(aˆ t:Tˆ |π)r(γ|φγ)r(eˆ ≺t ,θ|φ) dθdγ. (135)
t:Tˆ ≺t
q(s ,aˆ ,γ,eˆ ,θ|φ,a ,ξ)
φ,φγ,π aˆ t:XTˆ,eˆ≺t ZZ ≺t t:Tˆ ≺t ≺t
Note that here, by construction, the denominator can be written as:
q(s ,aˆ ,γ,eˆ ,θ|φ,a ,ξ)=q(aˆ |γ,φ)q(γ|φγ)q(eˆ ,θ|sa ,ξ)q(s |a ,ξ). (136)
≺t t:Tˆ ≺t ≺t t:Tˆ ≺t ≺t ≺t ≺t
Which allows us to write Equation (135) with the original variational free energy again:
φ∗ ,φγ∗ ,π∗ =argmin F[φ,sa ,ξ]+KL[r(Aˆ ,Γ|π,φγ)||q(Aˆ ,Γ|φ,ξγ)] . (137)
sa≺t,ξ sa≺t,ξ sa≺t,ξ
φ,φγ,π
≺t t:Tˆ t:Tˆ
Ä ä
9 Applications and Limitations
An applicationofthe activeinference describedhere to a simple maze task canbe found inFriston et al.
(2015). Active inference using different forms of approximate posteriors can be found in Friston et al.
(2016b,b). Here, Friston et al. (2017a) also includes a knowledge seeking term in addition to the con-
ditional entropy term. In the universal reinforcement learning framework Aslanides et al. (2017) also
implement a knowledge seeking agent. These works can be quite directly translated into our framework.
40
For applications of intrinsic motivations that are not so directly related to our framework see also
the references in the according Sections 7.3.3 to 7.3.5.
Aquantitativeanalysisofthelimitationsofthedifferentapproacheswediscussedisbeyondthescope
of this publication. However, we can make a few observations that may help researchers interested in
applying the discussed approaches.
Concerning the computation of the complete posterior by direct Bayesian methods is not feasible
beyond the simplest of systems and even then only for very short time durations. As mentioned in
the text it contains a sum over |Eˆ|t elements. If the time horizon into the future is Tˆ − t then the
predictive factor consists of
SˆTˆ−t×EˆTˆ−t ×AˆTˆ−t
entries. This means predicting far into the future is
also not feasible. Therefore Tˆ−t will usually have to be fixed to a small number. Methods that also
approximatethe predictivefactor(e.g. Friston et al., 2016b, 2017a)maybe useful here. However,toour
knowledge, their scalability has not been addressed yet. Since in these approaches the predictive factor
is approximated in a similar way as the posterior factor here, we would expect that it is similar to the
scalability of approximating the posterior factor.
Employing variational inference reduces the computational burden for obtaining a posterior factor
considerably. The sum over all possible past environment histories (the |Eˆ|t elements) is approximated
within the optimisation. Clearly, by employing variational inference we inherit all shortcomings of this
method. As mentioned also in Friston et al. (2016b) variational inference approximations are known
to become overconfident i.e. the approximate posterior tends to ignore values with low probabilities
(see e.g. Bishop, 2011). In practice this can of course lead to poor decision making. Furthermore, the
convergence of the optimisation to obtain the approximate posterior can also become slow. As time t
increases the necessary computations for each optimisation step in the widely used coordinate ascent
variationalinference algorithm(Blei et al., 2017) growwith t2. Experiments suggestthatthe number of
necessary optimisation steps also grows over time. At the moment, we do not know how fast but this
may also lead to problems. A possible solution would be to introduce some form of forgetting such that
the considered past does not grow forever.
Ignoring the problem of obtaining a complete posterior, we still have to evaluate and select actions.
Computingtheinformationtheoreticquantitiesneededforthementionedintrinsicmotivationsandtheir
induced action-value functions is also computationally expensive. In this case fixing the future time
horizon Tˆ − t can lead to constant computational requirements. These grow exponentially with the
time horizonwhichmakeslargetime horizonsimpossiblewithoutfurtherapproximations. Note thatthe
action selection mechanisms discussed here also require the computation of the action-value functions
for each of the future action sequences.
Active inference is not a standard variational inference problem and therefore standard algorithms
like the coordinate ascent variational inference may fail in this case. Other optimisation procedures like
gradient descent may still work. As far as we know there have been no studies of the scalability of the
active inference scheme up to now.
41
10 Conclusion
We have reconstructed the active inference approach of Friston et al. (2015) in in a formally consistent
way. Westartedbydisentanglingthecomponentsofinferenceandactionselection. Thisdisentanglement
has allowed us to also remove the variational inference completely and formulate the pure Bayesian
knowledge updating for the generative model of Friston et al. (2015). We have shown in Section 6.3
that a special case of this model is equivalent to a finite version of the model used by the Bayesian
universalreinforcementagent(Hutter,2005). WethenpointedouthowtoapproximatethepureBayesian
knowledge updating with variational inference. To formalise the notion of intrinsic motivations within
this framework, we have introduced intrinsic motivation functions that take complete posteriors and
future actions as inputs. These induce action-value functions similar to those used in reinforcement
learning. The action-value functions can then be used for both, the Bayesian and the variational agent,
in standard deterministic or softmax action selection schemes.
OuranalysisoftheintrinsicmotivationsExpected Free Energy Maximisation, Empowerment Maximi-
sation, Predictive Information Maximisation, and Knowledge Seeking indicates that there is significant
common structure between the different approaches and it may be possible to combine them. At the
time of writing, we have already made first steps towards using the present framework for a systematic
quantitative analysis and comparisonof the different intrinsic motivations. Eventually, such studies will
shed more conclusive light on the computational requirements and emergent dynamics of different mo-
tivations. An investigation of the biological plausibility of different motivations might lead to different
results and this is of equal interest.
Beyond the comparison of different intrinsic motivations within an active inference framework, the
presentworkcanthuscontributetoinvestigationsontheroleofintrinsicmotivationsinlivingorganisms.
If biological plausibility of active inference can be upheld, and maintained for alternative intrinsic moti-
vations,thenexperimentalstudiesmightbederivedtotestdifferentiatingpredictions. Ifactiveinference
was key to cognitive phenomena such as consciousness, it would be interesting to see how the cognitive
dynamics would be affected by alternative intrinsic motivations.
Conflict of Interest Statement
CG, CS, SS, and DP declare no competing interests. In accordance with Frontiers policy MB declares
that he was employed by company Araya Incorporated, Tokyo, Japan.
Author Contributions
MB, CG, CS, SS, and DP conceived of this study, discussed the concepts, revised the formal analysis,
and wrote the article. MB contributed the initial formal analysis.
42
Funding
CGisfundedbyEPSRCgrant[EP/L015846/1](IGGI).CSisfundedbytheEUHorizon2020programme
under the Marie Sklodowska-Curie grant 705643. DP is funded in part by EC H2020-641321 socSMCs
FET Proactive project.
Acknowledgments
MB would like to thank Yen Yu for valuable discussions on active inference.
43
A Posterior Factor
Herewewanttocalculatetheposteriorfactorq(eˆ ,θ|sa ,ξ)ofthecompleteposteriorinEquation(16)
≺t ≺t
without an approximation (i.e. as in direct, non-variational Bayesianinference).
1
q(eˆ ,θ|sa ,ξ)= q(s ,eˆ ,θ|a ,ξ) (138)
≺t ≺t ≺t ≺t ≺t
q(s |a ,ξ)
≺t ≺t
1
= q(s |eˆ ,θ1)q(eˆ |a ,θ2,θ3)q(θ|ξ) (139)
≺t ≺t ≺t ≺t
q(s |a ,ξ)
≺t ≺t
t t 3
1
= q(s |eˆ ,θ1) q(eˆ |a ,eˆ ,θ2)q(eˆ |θ3) q(θi|ξi). (140)
τ τ r r r−1 0
q(s |a ,ξ)
≺t ≺t
τ=0 r=1 i=1
Y Y Y
Weseethatthenumeratorisgivenbythegenerativemodel. Thedenominatorcanbecalulatedaccording
to:
q(s |a ,ξ)= q(s |a ,θ)q(θ|ξ)dθ (141)
≺t ≺t ≺t ≺t
Z∆Θ
t t 3
= q(eˆ |θ3) q(s |eˆ ,θ1) q(eˆ |a ,eˆ ,θ2) q(θi|ξi)dθ (142)
0 τ τ r r r−1
Z∆ΘÑ
X
eˆ≺t τ
Y
=0 r
Y
=1 é
Y
i=1
t t 3
= q(eˆ |θ3) q(s |eˆ ,θ1) q(eˆ |a ,eˆ ,θ2) q(θi|ξi)dθ (143)
0 τ τ r r r−1
X
eˆ≺t Z∆Θ τ
Y
=0 r
Y
=1 i
Y
=1
t
= q(eˆ |θ3)q(θ3|ξ3)dθ3 q(s |eˆ ,θ1)q(θ1|ξ1)dθ1
0 τ τ
X
eˆ≺t Z Z τ
Y
=0
(144)
t
× q(eˆ |a ,eˆ ,θ2)q(θ2|ξ2)dθ2
r r r−1
!
Z r=1
Y
Thethreeintegralscanbesolvedanalyticallyifq(θi|ξi)arechosenasconjugatepriorstoq(s |eˆ ,θ1),q(eˆ |a ,eˆ ,θ2),q(eˆ |θ3)
τ τ r r r−1 0
respectively. However, the sum is over |E|t terms and therefore untractable as time increases.
44
B Approximate Posterior Predictive Distribution
Here, we calculate the (variational) approximate predictive posterior distribution of q(sˆ |aˆ ,sa ,ξ)
t:Tˆ t:Tˆ ≺t
fromagivenapproximatecompleteposterior. Thisexpressionplaysaroleinmultipleintrinsicmotivation
functions like empowermentmaximisation,predictive informationmaximisation,and knowledgeseeking.
For an arbitrary φ we have:
r(sˆ |aˆ ,φ):= q(sˆ |aˆ ,eˆ ,θ)r(eˆ ,θ|φ)dθ (145)
t:Tˆ t:Tˆ t:Tˆ t:Tˆ t−1 ≺t
X
eˆ≺t Z
= q(sˆ |aˆ ,eˆ ,θ)r(eˆ ,θ|φ)dθ (146)
t:Tˆ t:Tˆ t−1 t−1
eˆ Xt−1Z
3
= q(sˆ |aˆ ,eˆ ,θ) r(θi|φi)dθ r(eˆ |φEt−1) (147)
t:Tˆ t:Tˆ t−1 t−1
!
eˆ Xt−1 Z i Y =1
= q(sˆ |eˆ ,θ1)r(θ1|φ1)dθ1×
t:Tˆ t:Tˆ
eˆ Xt−1 Ñ X eˆ t:Tˆ Z (148)
× q(eˆ
t:Tˆ
|aˆ
t:Tˆ
,eˆ
t−1
,θ2)r(θ2|φ2)dθ2 r(eˆ
t−1
|φEt−1)
Z é
Tˆ
= q(sˆ |eˆ ,θ1)r(θ1|φ1)dθ1×
τ τ
eˆ Xt−1 Ñ X eˆ t:Tˆ Z τ Y =t (149)
Tˆ
× q(eˆ
τ
|aˆ
τ
,eˆ
r−1
,θ2)r(θ2|φ2)dθ2 r(eˆ
t−1
|φEt−1)
Z τ=t é
Y
= r(sˆ
t:Tˆ
|eˆ
t:Tˆ
,φ1)r(eˆ
t:Tˆ
|aˆ
t:Tˆ
,eˆ
t−1
,φ2)r(eˆ
t−1
|φEt−1) (150)
eˆ Xt−1X eˆ t:Tˆ
From first to second line we usually have to marginalize q(eˆ ,θ|sa ,ξ) to q(eˆ ,θ|sa ,ξ) with a
≺t ≺t t−1 ≺t
sum over all |E|t−1 possible environment histories eˆ . Using the approximate posterior, we can use
≺t−1
r(eˆ
t−1
|φEt−1)directlywithoutdealingwiththeintractablesum. Fromthirdtofourthline,r(θ3|φ3)drops
out since it can be integrated out (and its integral is equal to one). Note that during the optimisation
Equation (47) r(θ3|φ3) does play a role so it is not superfluous.From fifth to last line, we perform the
integration over the parameters θ1 and θ2. These integrals can be calculated analytically if we choose
the modelsr(θ1|φ1)andr(θ2|φ2)asconjugatepriorstoq(s|e,θ1)andq(e′|a′,e,θ2). Variationalinference
predictionofthe next n=Tˆ−t−1sensorvalues requiresthe sumandcalculationof|Eˆ|n terms for |Sˆ|n
possible futures.
45
C Notation Translation Tables
A table to translate between our notation and the one used in Friston et al. (2015). The translation is
also valid in many cases for Friston et al. (2016b,a, 2017a). Some of the parameters shown here only
show up in the latter publications.
This article Friston et al. (2015) Note
e ∈E Actual environment states
t
eˆ ∈Eˆ s ∈S Estimated/modelled environment states
t t
s ∈S o ∈Ω Actual/observedsensor or outcome values
t t
sˆ ∈Sˆ=S o ∈Ω Estimated/modelled(usuallyfuture)sensororout-
t t
come values. Note that the index τ instead of t
often indicates an estimated future sensor value in
Friston et al. (2015).
a ∈A a ∈A Actions
t t
aˆ ∈Aˆ=A u ∈U Contemplated (usually future) actions
t t
m ∈M Agent memory state
t
aˆ π,u˜ π and u˜ both uniquely specify future action se-
t:Tˆ
quences
θ θ Generative model parameters
q(sˆ|eˆ,θ1)=q(sˆ|eˆ) P(o|s)=A Model sensor dynamics, not parameterised in
os
Friston et al. (2015), A is a matrix representation
q(eˆ′|aˆ′,eˆ,θ2)=q(eˆ′|aˆ′,eˆ) P(s′|s,u)=B(u) s′s Model environment dynamics, not parameterised
in Friston et al. (2015), B(u) is a matrix represen-
tation for each possible action u
q(eˆ |θ3) P(s |m)=D Modelled initial environment state, not parame-
0 0 s0
terisedin Friston et al. (2015), D is a vector repre-
sentation. Note, the parameter m is a fixed hyper-
parameter
ξ =(ξ1,ξ2,ξ3) m Generativemodelhyperparam.ormodelparameter
that subsumes all hyperparameters
ξ1 sensor dynamics hyperparam.
ξ2 Environment dynamics hyperparam.
ξ3 Initial environment state hyperparam.
ξγ (α,β) Precision hyperparam.
(φ,φγ) µ Variational param.
φE 0:Tˆ s Environment states variational param.,
φEτ s
τ
for each timestep τ
φ1 Sensor dynamics variational param.
Û
φ2 Environment dynamics variational param.
Û
46
φ3 Initial environment state variational param.
π π Future action sequence variational param.
φγ γ Precision variational param.
Qˆ(aˆ ,φ) Q(π)=Q(u˜|π) Variationalaction-valuefunction. The dependence
t:Tˆ Û
of Q(u˜|π) on s is omitted
Û t
p(s ,e ,a ) R(o˜,s˜,a˜) Our physical environment corresponds to the gen-
(cid:22)t (cid:22)t ≺t
erative process
Û
q(sˆ ,eˆ ,aˆ ,γ|a ,ξ) P(o˜,s˜,u˜,γ|a˜,m) Thegenerativemodelforactiveinferenceincluding
(cid:22)t (cid:22)t t:Tˆ ≺t
γ (which we mostly omit)
r(eˆ ,aˆ ,γ|π,φ,φγ) Q(s˜,u˜,γ|µ) Approximate complete posterior for active infer-
0:Tˆ t:Tˆ
ence
pd(sˆ ) P(o |m) Prior over future outcomes.
τ τ
Since our treatment is more general than that of Friston et al. (2015) and quite similar (though not
identical) to the treatment in Friston et al. (2016b,a, 2017a) we also give the relations to variables in
those publications. We hope this will help interested readers to understand the latter publications even
ifsomeaspectsofthosearedifferent. Adiscussionofthosedifferences isbeyondthe scopeofthe present
article.
This article Friston et al. (2016b) Note
e ∈E Actual environment states
t
eˆ ∈Eˆ s ∈S Estimated/modelled environment
t t
states
s ∈S o ∈Ω Actual/observed sensor or outcome
t t
values
sˆ ∈Sˆ=S o ∈Ω Estimated/modelled (usually fu-
t t
ture) sensor or outcome values.
Note that the index τ instead of t
often indicates an estimated future
sensorvalueinFriston et al.(2015).
a ∈A u ∈A Actions
t t
aˆ ∈Aˆ=A u ∈Υ Contemplated (usually future) ac-
t t
tions
m ∈M Agent memory state
t
aˆ π, action sequences
0:Tˆ
θ θ Generative model parameters
θ1 A Sensor dynamics param.
θ2 B Environment dynamics param.
θ3 D Initial environment state param.
47
ξ η Generative model hyperparam. or
model parameter that subsumes all
hyperparameters
ξ1 a sensor dynamics hyperparam.
ξ2 b Environment dynamics hyper-
param.
ξ3 d Initial environment state hyper-
param.
ξγ β Precision hyperparam.
(φ,φγ) η Variational param.
φE 0:Tˆ s 0:T Environment states variational
param.
q(eˆ
τ
|aˆ
t:Tˆ
,a
0:t−1
,φEτ) (sπ
τ
)
eˆτ
Foreachsequenceofactionsandfor
each timestep there is a parameter
sπ. Since a categorical distribution
τ
is used, the parameteris a vector of
probabilitieswhoseentryeˆ isequal
τ
totheprobabilityofeˆ ifwesetEˆ=
τ
{1,...,|Eˆ|}
φ1 a Sensor dynamics variationalparam.
φ2 b Environment dynamics variational
param.
φ3 d Initialenvironmentstatevariational
param.
π π Future action sequence variational
param.
φγ β Precision variational param.
Qˆ(aˆ ,φ) −G(π) Variational action-value function.
t:Tˆ
The dependence of G(π) on sπ is
0:T
omitted
p(s ,e ,a ) R(o˜,s˜,a˜) Our physical environment corre-
(cid:22)t (cid:22)t ≺t
sponds to the generative process
q(sˆ ,eˆ ,aˆ ,γ,θ,ξ) P(o˜,s˜,π,γ,A,B,D|a,b,d,β) The generative model for active in-
(cid:22)t 0:Tˆ 0:Tˆ
ference
r(eˆ ,aˆ ,γ,θ|π,φγ,φ) Q(s˜,π,A,B,D,γ|sπ ,π,a,b,d,β) Approximate complete posterior for
0:Tˆ 0:Tˆ 0:Tˆ
active inference
pd(sˆ ) P(o )=σ(U ) Prior over future outcomes.
τ τ τ
48
References
Allen, M. and Friston, K. J. (2016). From Cognitivism to Autopoiesis: Towards a Computational
Framework for the Embodied Mind. Synthese, pages 1–24.
Aslanides, J., Leike, J., and Hutter, M. (2017). Universal Reinforcement Learning Algorithms: Survey
and Experiments. In Proceedings of the 26th International Joint Conference on Artificial Intelligence,
pages 1403–1410.
Attias, H. (1999). A Variational Bayesian Framework for Graphical Models. In Solla, S., Leen, T., and
Mu¨ller,K.,editors,ProceedingsAdvancesinNeuralInformationProcessingSystems12,pages209–215,
Cambridge, MA, USA. MIT Press.
Attias, H. (2003). Planning by Probabilistic Inference. In Proceedings 9th International Workshop on
Artificial Intelligence and Statistics.
Ay, N., Bernigau, H., Der, R., and Prokopenko, M. (2012). Information-Driven Self-Organization: The
DynamicalSystemApproachtoAutonomousRobotBehavior. Theory in Biosciences, 131(3):161–179.
Ay, N., Bertschinger, N., Der, R., Gttler, F., and Olbrich, E. (2008). Predictive Information and
Explorative Behavior of Autonomous Robots. The European Physical Journal B-Condensed Matter
and Complex Systems, 63(3):329–339.
Ay,N.andLo¨hr,W.(2015). TheUmweltofanEmbodiedAgentaMeasure-TheoreticDefinition. Theory
in Biosciences, 134(3-4):105–116.
Barber,D.andAgakov,F.(2003).TheIMAlgorithm: AVariationalApproachtoInformationMaximiza-
tion. InThrun, S.,Saul, L.K.,andSchlkopf,B., editors, Proceedings Advances in Neural Information
Processing Systems 16, pages 201–208.MIT Press.
Bialek, W. and Tishby, N. (1999). Predictive Information. arXiv preprint cond-mat/9902341.
Bishop, C. M. (2011). Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, New York.
Blei,D.M.,Kucukelbir,A.,andMcAuliffe,J.D.(2017).VariationalInference: AReviewforStatisticians.
Journal of the American Statistical Association, 112(518):859–877.
Botvinick,M.andToussaint,M.(2012).PlanningasInference. TrendsinCognitiveSciences,16(10):485–
488.
Buckley, C. L., Kim, C. S., McGregor,S., and Seth, A. K.(2017). The Free EnergyPrinciple for Action
and Perception: A Mathematical Review. Journal of Mathematical Psychology, pages 55–79.
Clark, A. (2015). Surfing Uncertainty: Prediction, Action, and the Embodied Mind. Oxford University
Press.
49
Cover,T. M. and Thomas, J. A. (2006). Elements of Information Theory. Wiley-Interscience, Hoboken,
N.J.
Dennett, D. C. (1991). Consciousness Explained. Penguin Books.
Doshi-Velez,F.,Pfau,D.,Wood,F.,andRoy,N.(2015). BayesianNonparametricMethodsforPartially-
ObservableReinforcementLearning.IEEETransactionsonPatternAnalysisandMachineIntelligence,
37(2):394–407.
Ellis, B. and Wong, W. H. (2008). Learning Causal Bayesian Network Structures From Experimental
Data. Journal of the American Statistical Association, 103(482):778–789.
Fox, R. and Tishby, N. (2016). Minimum-information lgq control part ii: Retentive controllers. In 2016
IEEE 55th Conference on Decision and Control (CDC), pages 5603–5609.
Friston, K. (2010). The free-energy principle: A unified brain theory? Nature Reviews Neuroscience,
11(2):127–138.
Friston, K. (2013a). Consciousness and Hierarchical Inference. Neuropsychoanalysis, 15(1):38–42.
Friston, K. (2013b). Life as We Know It. Journal of The Royal Society Interface, 10(86).
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,O’Doherty,J.,andPezzulo,G.(2016a). Active
Inference and Learning. Neuroscience & Biobehavioral Reviews, 68(Supplement C):862–879.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck,P., and Pezzulo, G. (2016b). Active Inference: A
Process Theory. Neural Computation, 29(1):1–49.
Friston,K.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.,andPezzulo,G.(2015).ActiveInference
and Epistemic Value. Cognitive Neuroscience, 6(4):187–214.
Friston, K., Samothrakis, S., and Montague, R. (2012). Active Inference and Agency: Optimal Control
Without Cost Functions. Biological Cybernetics, 106(8-9):523–541.
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., and Ondobaka, S. (2017a). Active
Inference, Curiosity and Insight. Neural Computation, 29(10):2633–2683.
Friston, K. J., Parr, T., and de Vries, B. (2017b). The Graphical Brain: Belief Propagation and Active
Inference. Network Neuroscience, 1(4):381–414.
Froese,T.andZiemke,T.(2009). Enactiveartificialintelligence: Investigatingthesystemicorganization
of life and mind. Artificial Intelligence, 173(3–4):466–500.
Gregor, K., Rezende, D. J., and Wierstra, D. (2016). Variational Intrinsic Control. arXiv preprint
arXiv:1611.07507.
50
Guckelsberger, C. and Salge, C. (2016a). Does empowerment maximisation allow for enactive artificial
agents? In Proceedings of the Fifteenth International Conference on the Synthesis and Simulation of
Living Systems (Alife 2016), page 8. The MIT Press.
Guckelsberger,C.andSalge,C.(2016b). DoesEmpowermentMaximisationAllowforEnactiveArtificial
Agents? In Proceedings 15th International Conference on Synthesis and Simulation of Living Systems
(ALIFE).
Guckelsberger,C., Salge, C., and Colton, S. (2016a). Intrinsically Motivated GeneralCompanion NPCs
via Coupled Empowerment Maximisation. In Proceedings Conference on Computational Intelligence
in Games.
Guckelsberger, C., Salge, C., Saunders, R., and Colton, S. (2016b). Supportive and Antagonistic Be-
haviour in Distributed Computational Creativity via Coupled Empowerment Maximisation. In Pro-
ceedings 7th International Conference on Computational Creativity.
Guckelsberger, C., Salge, C., and Togelius, J. (2018). New And Surprising Ways to be Mean: Adver-
sarialNPCs with Coupled EmpowermentMinimisation. In Proceedings Conference on Computational
Intelligence in Games.
Hutter, M. (2005). Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Proba-
bility. Texts in Theoretical Computer Science. An EATCS Series. Springer-Verlag,Berlin Heidelberg.
Karl, M., Soelch, M., Becker-Ehmck,P., Benbouzid, D., van der Smagt, P., and Bayer,J. (2017). Unsu-
pervised Real-Time Control through Variational Empowerment. arXiv preprint arXiv:1710.05101.
Klyubin, A., Polani,D., andNehaniv, C. (2005). Empowerment: A UniversalAgent-CentricMeasure of
Control. In The 2005 IEEE Congress on Evolutionary Computation, 2005, volume 1, pages 128–135.
Leike, J. (2016). Nonparametric General Reinforcement Learning. arXiv:1611.08944 [cs].
Linson,A., ,A.,Ramamoorthy,S.,andFriston,K.(2018). TheActiveInferenceApproachtoEcological
Perception: General Information Dynamics for Natural and Artificial Embodied Cognition. Frontiers
in Robotics and AI, 5:21.
Little, D. Y.-J. and Sommer, F. T. (2013). Maximal mutual information, not minimal entropy, for
escaping the Dark Room. Behavioral and Brain Sciences, 36(3):220–221.
Lunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000). WinBUGS - A Bayesian Modelling
Framework: Concepts, Structure, and Extensibility. Statistics and Computing, 10(4):325–337.
Manzotti, R. and Chella, A. (2018). Good old-fashioned artificial consciousness and the intermediate
level fallacy. Frontiers in Robotics and AI, 5:39.
Martius, G., Der, R., and Ay, N. (2013). Information Driven Self-Organization of Complex Robotic
Behaviors. PLoS ONE, 8(5).
51
Martius,G.,Jahn,L.,Hauser,H.,andHafner,V.V.(2014). Self-Explorationofthe StumpyRobotwith
PredictiveInformationMaximization. IndelPobil,A.P.,Chinellato,E.,Martinez-Martin,E.,Hallam,
J.,Cervera,E.,andMorales,A.,editors,From Animals to Animats 13: 13th International Conference
on Simulationof AdaptiveBehavior, SAB2014, Castello´n, Spain, July22-25, 2014. Proceedings,pages
32–42.Springer.
Minka, T. P. (2001). Expectation Propagation for Approximate Bayesian Inference. In Proceedings
of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI’01, pages 362–369, San
Francisco. Morgan Kaufmann Publishers Inc.
Mohamed, S. and Rezende, D. J. (2015). Variational Information Maximisation for Intrinsically Moti-
vatedReinforcementLearning. InCortes,C.,Lawrence,N.D.,Lee,D.D.,Sugiyama,M.,andGarnett,
R.,editors,Proceedings Advances in NeuralInformation Processing Systems28,pages2125–2133.Cur-
ran Associates, Inc.
Orseau, L., Lattimore, T., and Hutter, M. (2013). Universal Knowledge-Seeking Agents for Stochastic
Environments. In Jain, S., Munos, R., Stephan, F., and Zeugmann, T., editors, Algorithmic Learning
Theory,number8139inLectureNotesinComputerScience,pages158–172.SpringerBerlinHeidelberg.
Ortega, P. A. (2011). BayesianCausal Induction. arXiv preprint arXiv:1111.0708.
Ortega,P.A.andBraun,D.A.(2010). AMinimumRelativeEntropyPrincipleforLearningandActing.
Journal of Artificial Intelligence Research, 38(1):475–511.
Ortega,P.A.andBraun,D.A.(2014). GeneralizedThompsonSamplingforSequentialDecision-Making
and Causal Inference. Complex Adaptive Systems Modeling, 2:2.
Oudeyer, P.-Y. and Kaplan, F. (2009). What is intrinsic motivation? a typology of computational
approaches. Frontiers in Neurorobotics, 1:6.
Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. (2007). Intrinsic Motivation Systems for Autonomous
Mental Development. IEEE Transactions on Evolutionary Computation, 11(2):265–286.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press.
Pfeifer, R., Iida, F., and Bongard, J. (2005). New Robotics: Design Principles for Intelligent Systems.
Artificial Life, 11(1-2):99–120.
Ross, S. and Pineau, J. (2008). Model-Based Bayesian Reinforcement Learning in Large Structured
Domains. Proceedings 24th Conference on Uncertainty in Artificial Intelligence, 2008:476–483.
Ryan, R. M. and Deci, E. L. (2000). Intrinsic and Extrinsic Motivations: Classic Definitions and New
Directions. Contemporary Educational Psychology, 25(1):54–67.
Salge, C., Glackin, C., and Polani, D. (2014). Empowerment–an Introduction. In Guided Self-
Organization: Inception, pages 67–114.Springer.
52
Salge, C., Guckelsberger, C., Canaan, R., and Mahlmann, T. (2018). Accelerating Empowerment Com-
putation with UCT Tree Search. In Proceedings Conference on Computational Intelligence in Games.
IEEE.
Salge,C.andPolani,D.(2017).EmpowermentasReplacementfortheThreeLawsofRobotics. Frontiers
in Robotics and AI, 4:25.
Santucci, V. G., Baldassarre,G., and Mirolli, M. (2013). Which Is the Best Intrinsic Motivation Signal
for Learning Multiple Skills? Frontiers in Neurorobotics, 7:22.
Schmidhuber, J. (2010). FormalTheory of Creativity,Fun, andIntrinsic Motivation(1990-2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230–247.
Storck,J.,Hochreiter,S.,andSchmidhuber,J.(1995). ReinforcementDrivenInformationAcquisitionin
Non-Deterministic Environments. In Proceedings of the International Conference on Artificial Neural
Networks, volume 2, pages 159–164.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Toussaint, M. (2009). Probabilistic inference as a model of planned behavior. Ku¨nstliche Intelligenz,
3/09:23–29.
Vehtari, A., Gelman, A., Sivula, T., Jylnki, P., Tran, D., Sahai, S., Blomstedt, P., Cunningham, J. P.,
Schiminovich, D., and Robert, C. (2014). Expectation Propagation as a Way of Life: A Framework
for Bayesian Inference on Partitioned Data. arXiv:1412.4869 [stat].
Wainwright, M. J. and Jordan, M. I. (2007). Graphical Models, Exponential Families, and Variational
Inference. Foundations and Trends in Machine Learning, 1(12):1–305.
Winn,J.andBishop,C.M.(2005). VariationalMessagePassing. Journalof Machine LearningResearch,
6(Apr):661–694.
53