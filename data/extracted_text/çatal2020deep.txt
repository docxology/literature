Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
DEEP ACTIVE INFERENCE
FOR AUTONOMOUS ROBOT NAVIGATION
OzanC¸atal,SamuelWauthier,TimVerbelen,CedricDeBoom,&BartDhoedt
IDLab,DepartmentofInformationTechnology
GhentUniversity–imec
Ghent,Belgium
ozan.catal@ugent.be
ABSTRACT
Activeinferenceisatheorythatunderpinsthewaybiologicalagent’sperceiveand
act in the real world. At its core, active inference is based on the principle that
thebrainisanapproximateBayesianinferenceengine, buildinganinternalgen-
erativemodeltodriveagentstowardsminimalsurprise. Althoughthistheoryhas
showninterestingresultswithgroundingincognitiveneuroscience,itsapplication
remainslimitedtosimulationswithsmall,predefinedsensorandstatespaces.
Inthispaper,weleveragerecentadvancesindeeplearningtobuildmorecomplex
generative models that can work without a predefined states space. State repre-
sentationsarelearnedend-to-endfromreal-world,high-dimensionalsensorydata
suchascameraframes. Wealsoshowthatthesegenerativemodelscanbeusedto
engageinactiveinference.Tothebestofourknowledgethisisthefirstapplication
ofdeepactiveinferenceforareal-worldrobotnavigationtask.
1 INTRODUCTION
Activeinferenceandthefreeenergyprincipleunderpinsthewayourbrain–andnaturalagentsin
general–work. Thecoreideaisthatthebrainentertainsa(generative)modeloftheworldwhich
allowsittolearncauseandeffectandtopredictfuturesensoryobservations.Itdoessobyconstantly
minimisingitspredictionerroror“surprise”,eitherbyupdatingthegenerativemodel,orbyinferring
actionsthatwillleadtolesssurprisingstates. Assuch, thebrainactsasanapproximateBayesian
inferenceengine,constantlystrivingforhomeostasis.
Thereisampleevidence(Friston,2012;Fristonetal.,2013a;2014)thatdifferentregionsofthebrain
activelyengageinvariationalfreeenergyminimisation. Theoreticalgroundsindicatethateventhe
simplestoflifeformsactinafreeenergyminimisingway(Friston,2013).
Althoughthereisalargebodyofworkonactiveinferenceforartificialagents(Fristonetal.,2006;
2009;2017;2013b;Cullenetal.,2018),experimentsaretypicallydoneinasimulatedenvironment
with predefined and simple state and sensor spaces. Recently, research has been done on using
deep neural networks as an implementation of the active inference generative model, resulting in
theumbrellaterm“deepactiveinference”. However,sofaralloftheseapproacheswereonlytested
onfairlysimple,simulatedenvironments(Ueltzho¨ffer,2018;Millidge,2019;C¸ataletal.,2019). In
thispaper,weapplydeepactiveinferenceonarobotnavigationtask,withhigh-dimensionalcamera
observationsanddeployitonamobilerobotplatform. Tothebestofourknowledge,thisisthefirst
timethatactiveinferenceisappliedonareal-worldrobotnavigationtask.
IntheremainderofthispaperwewillfirstintroducetheactiveinferencetheoryinSection2. Next,
weshowhowweimplementactiveinferenceusingdeepneuralnetworksinSection3,anddiscuss
initialexperimentsinSection4.
2 ACTIVE INFERENCE
Active inference is a process theory of the brain that utilises the concept of free energy (Friston,
2013) to describe the behaviour of various agents. It stipulates that all agents act in order to min-
1
0202
raM
6
]IA.sc[
1v02230.3002:viXra
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
imise their own uncertainty of the world. This uncertainty is expressed as Bayesian Surprise, or
alternatively the variational free energy. In this context this is characterised by the difference be-
tweenwhatanagentimaginesabouttheworldandwhatithasperceivedabouttheworld(Friston,
2010). Moreconcretely,theagentbuildsagenerativemodelP(o˜,s˜,a˜),linkingtogethertheagents
internalbeliefstatesswiththeperceivedactionsaandobservationsointheformofajointdistri-
bution. Weuseatildetodenoteasequenceofvariablesthroughtime. Thisgenerativemodelcanbe
factorisedasinEquation1.
T
(cid:89)
P(o˜,s˜,a˜)=P(a˜)P(s ) P(o |s )P(s |s ,a ) (1)
0 t t t t−1 t−1
t=1
ThefreeenergyorBayesiansurpriseisthendefinedas:
F =E [logQ(s˜)−logP(o˜,s˜,a˜)]
Q
=D (Q(s˜)(cid:107)P(s˜,a˜|o˜))−logP(o˜) (2)
KL
=D (Q(s˜)(cid:107)P(s˜,a˜))−E [logP(o˜|s˜)]
KL Q
Here, Q(s˜) is an approximate posterior distribution. The second equality shows that free energy
is equivalent to the (negative) evidence lower bound (ELBO) (Kingma & Welling, 2013; Rezende
etal.,2014). Thefinalequationframestheproblemoffreeenergyminimisationasexplainingthe
world from the agents beliefs whilst minimising the complexity of accurate explanations (Friston
etal.,2016).
Crucially,inactiveinferenceagentswillactaccordingtothebeliefthattheywillkeepminimising
surpriseinthefuture. Thismeansagentswillinferpoliciesthatyieldminimalexpectedfreeenergy
in the future, with a policy π being the sequence of future actions a starting at current time
t:t+H
steptwithatimehorizonH. ThisprincipleisformalisedinEquation3withσ beingthesoftmax
functionwithprecisionparameterγ.
P(π)=σ(−γG(π))
t (cid:88) +H (3)
G(π)= G(π,τ)
τ=t
ExpandingtheexpectedfreeenergyfunctionalG(π,τ)wegetEquation4. Usingthefactorisation
ofthegenerativemodelfromEquation1weapproximateQ(o ,s |π)≈P(o |s )Q(s |π).
τ τ τ τ τ
G(π,τ)=E [logQ(s |π)−logP(o ,s |π)]
Q(oτ,sτ|π) τ τ τ
=E [logQ(s |π)−logP(o |s ,π)−logP(s |π)] (4)
Q(oτ,sτ|π) τ τ τ τ
=D (Q(s |π)(cid:107)P(s ))+E [H(P(o |s ))]
KL τ τ Q(sτ) τ τ
Notethat,inthefinalequality,wesubstituteP(s |π)byP(s ),aglobalpriordistributionontheso-
τ τ
called“preferred”statesoftheagent.Thisreflectsthefactthattheagenthaspriorexpectationsabout
thestatesitwillreach. Hence,minimisingexpectedfreeenergyentailsbothrealisingpreferences,
whileminimisingtheambiguityofthevisitedstates.
3 DEEP ACTIVE INFERENCE
Incurrenttreatmentsofactiveinferencethestatespacesaretypicallycompletelyfixedupfront(Fris-
tonetal.,2009;Millidge,2019)orpartially(Ueltzho¨ffer,2018). However,thisdoesnotscalewell
formorecomplextasksasitisoftendifficulttodesignmeaningfulstatespacesforsuchproblems.
Therefore we allow for the agent to learn by itself what the exact parameterisation of its belief
space should be. We enable this by using deep neural networks to generate the various necessary
probabilitydistributionsforouragent.
WeapproximatethevariationalposteriordistributionforasingletimestepQ(s |s ,a ,o )with
t t−1 t−1 t
anetworkq (s |s ,a ,o ). SimilarlyweapproximatethelikelihoodmodelP(o |s )withthe
φ t t−1 t−1 t t t
network p (o |s ) and the prior P(s |s ,a ) with the network p (s |s ,a ). Each of
ξ t t t t−1 t−1 θ t t−1 t−1
2
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
Figure1: Thevariouscomponentsoftheagentrolledouttroughtime. Weminimisethevariational
freeenergybyminimisingboththenegativeloglikelihoodofobservationsandtheKLdivergence
betweenthestatetransitionmodelandtheobservationmodel. Theinferredhiddenstateischarac-
terisedasamultivariateGaussiandistribution.
thenetworksoutputamultivariatenormaldistributionwithadiagonalcovariancematrixusingthe
reparameterisation trick (Kingma & Welling, 2013). These neural networks cooperate in a way
similartoaVAE,wherethefixedstandardnormalpriorisreplacedwiththelearnablepriorp ,the
θ
decoderbyp andfinallytheencoderbyq ,asvisualisedinFigure1.
ξ φ
These networks are trained end-to-end using the free energy formula from the previous section as
anobjective.
∀t:minimise:−logp (o |s )+D (q (s |s ,a ,o )(cid:107)p (s |s ,a )) (5)
ξ t t KL φ t t−1 t−1 t θ t t−1 t−1
φ,θ,ξ
AsinaconventionalVAE(Kingma&Welling,2013)thenegativeloglikelihood(NLL)terminthe
objectivepunishesreconstructionerrorforcingthemodeltolearnrelevantinformationonthebelief
state to be captured in the posterior output, while the KL term pulls the prior output towards the
posterioroutput,forcingthepriorandposteriortoagreeonthecontentofthebeliefstateinaway
thatstillallowsthelikelihoodmodeltoreconstructthecurrentobservation.
Wecannowusethelearnedmodelstoengageinactiveinference,andinferwhichactiontheagent
has to take next. This is done by generating imagined trajectories for different policies using p
θ
andp ,calculatingtheexpectedfreeenergyGandselectingtheactionofthepolicythatyieldsthe
ξ
lowestG.Thesepoliciestoevaluatecanbepredefined,orgeneratedthroughrandomshooting,using
cross-entropymethod(Boeretal.,2005)orbybuildingasearchtree.
4 EXPERIMENTS
Wevalidateourdeepactiveinferenceapproachonarealworldroboticsnavigationtask. First, we
collectadatasetconsistingoftwohoursworthofrealworldaction-observationsequencesbydriving
aKukaYoubotbaseplatformupanddowntheaislesofawarehouselab. Cameraobservationsare
recorded with a front mounted Intel Realsense RGB-D camera, without taking into account the
depthinformation. Thex,yandangularvelocitiesarerecordedasactionsatarecordingfrequency
of 10Hz. The models are trained on a subsampled version of the data resulting in a train set with
datapointsevery200ms.
Next, we instantiate neural networks q and p as a convolutional encoder and decoder network,
φ ξ
and p using an LSTM. These are trained with Adam optimizer using the objective function from
θ
Equation5for1Miterations. Weuseaminibatchsizeof128andasequencelengthof10timesteps.
Adetailedoverviewofallhyperparametersisgiveninappendix.
WeutilisethesameapproachasinC¸ataletal.(2020)forourimaginarytrajectoriesandplanning.
Theagenthasaccesstothreebasepoliciestopickfrom: drivestraight,turnleftandturnright. Ac-
tionsfromthesepoliciesarepropagatedtothelearnedmodelsatdifferenttimehorizonsH =10,25
3
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
(a)Preferredstate. (b)Startstate
(c)Imaginaryfuturetrajectoriesfordifferentpolicies,i.e. goingstraightahead(top),turningright(middle),
turningleft(bottom).
(d)Actuallyfollowedtrajectory.
Figure2: Experimentalresults: Figure(a)showsthetargetobservationinimagined(reconstructed)
space. (b)Thestartobservationofthetrial. Figure(c)showsdifferentimaginaryplanningresults,
whilst(d)showstheactuallyfollowedtrajectory.
or55. Foreachresultingimaginarytrajectory,theexpectedfreeenergyGiscalculated. Finallythe
trajectorywithlowestGispicked,andthefirstactionofthechosenpolicyisexecuted,afterwhich
theimaginaryplanningrestarts. Therobot’spreferencesaregivenbydemonstration,usingthestate
distributionoftherobotwhiledrivinginthemiddleoftheaisle. Thisshouldencouragetherobotto
navigateintheaisles.
At each trial the robot is placed at a random starting position and random orientation and tasked
tonavigatetothepreferredposition. Figure2presentsasingleexperimentasanillustrativeexam-
ple. Figure2ashowsthereconstructedpreferredobservationfromthegivenpreferredstate, while
Figure 2b shows the trial’s start state from an actual observation. Figure 2c shows the imagined
resultsofeitherfollowingthepolicy“alwaysturnright”,“alwaysgostraight”or“alwaysturnleft”.
Figure2distheresultofutilisingtheplanningmethodexplainedabove. Additionalexamplescan
befoundinthesupplementarymaterial.
Therobotindeedturnsandkeepsdrivinginthemiddleoftheaisle,untilitreachestheendandthen
turnsaround1. Whenoneperturbstherobotbypushingit,itwillagainrecoverandcontinuetothe
middleoftheaisle.
5 CONCLUSION
Inthispaperwepresenthowwecanimplementagenerativemodelforactiveinferenceusingdeep
neural networks. We show that we are able to successfully execute a simple navigation task on a
realworldrobotwithourapproach.Asfutureworkwewanttoallowtherobottocontinuouslylearn
from past autonomous behaviour, effectively “filling the gaps” in its generative model. Also how
1Amoviedemonstratingtheresultsisavailableathttps://tinyurl.com/smvyk53
4
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
todefinethe“preferredstate”distributionsandwhichpoliciestoevaluateremainsanopenresearch
challengeformorecomplextasksandenvironments.
REFERENCES
Pieter-Tjerk Boer, Dirk Kroese, Shie Mannor, and Reuven Rubinstein. A tutorial on the cross-
entropy method. Annals of Operations Research, 134:19–67, 02 2005. doi: 10.1007/
s10479-005-5724-z.
Ozan C¸atal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy
selectionusingactiveinference. InWorkshoponStructure&PriorsinReinforcementLearning
atICLR2019: proceedings,pp. 9,2019.
OzanC¸atal,TimVerbelen,JohannesNauta,CedricDeBoom,andBartDhoedt.Learningperception
andplanningwithdeepactiveinference. InIEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing,ICASSP,Barcelona,Spain,pp.InPress,2020.
MaellCullen,BenDavey,KarlJ.Friston,andRosalynJ.Moran. Activeinferenceinopenaigym:
Aparadigmforcomputationalinvestigationsintopsychiatricillness. BiologicalPsychiatry: Cog-
nitiveNeuroscienceandNeuroimaging,3(9):809–818,2018. ISSN2451-9022. doi: https://doi.
org/10.1016/j.bpsc.2018.06.010. URL http://www.sciencedirect.com/science/
article/pii/S2451902218301617. Computational Methods and Modeling in Psychi-
atry.
KarlFriston. Thefree-energyprinciple: Aunifiedbraintheory? NatureReviewsNeuroscience,11
(2):127–138, 2010. ISSN 1471003X. doi: 10.1038/nrn2787. URL http://dx.doi.org/
10.1038/nrn2787.
Karl Friston. A free energy principle for biological systems. Entropy, 14(11):2100–2121, 2012.
ISSN1099-4300. doi: 10.3390/e14112100. URLhttps://www.mdpi.com/1099-4300/
14/11/2100.
Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
PhysiologyParis,100(1-3):70–87,2006.ISSN09284257.doi:10.1016/j.jphysparis.2006.10.001.
Karl Friston, Philipp Schwartenbeck, Thomas Fitzgerald, Michael Moutoussis, Tim Behrens, and
Raymond Dolan. The anatomy of choice: active inference and agency. Frontiers in Human
Neuroscience,7:598,2013a. ISSN1662-5161. doi: 10.3389/fnhum.2013.00598. URLhttps:
//www.frontiersin.org/article/10.3389/fnhum.2013.00598.
Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timothy Behrens,
and Raymond J. Dolan. The anatomy of choice: active inference and agency. Fron-
tiers in Human Neuroscience, 7(September):1–18, 2013b. ISSN 1662-5161. doi: 10.3389/
fnhum.2013.00598. URLhttp://journal.frontiersin.org/article/10.3389/
fnhum.2013.00598/abstract.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O’Doherty,
and Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Re-
views, 68:862 – 879, 2016. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev.
2016.06.022. URL http://www.sciencedirect.com/science/article/pii/
S0149763416301336.
KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanniPezzulo.
Activeinference: AProcessTheory. NeuralComputation,29:1–49,2017. ISSN1530888X. doi:
10.1162/NECO a 00912.
KarlJFriston. Lifeasweknowit. JournaloftheRoyalSocietyInterface,2013.
KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference?
PLOS ONE, 4(7):1–13, 07 2009. doi: 10.1371/journal.pone.0006421. URL https://doi.
org/10.1371/journal.pone.0006421.
5
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
Karl J Friston, Philipp Schwartenbeck, Thomas F. Fitzgerald, Michael Moutoussis, Timothy W.
Behrens, and Raymond J. Dolan. The anatomy of choice: dopamine and decision-making. In
PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences,2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2013. URLhttp://arxiv.org/abs/1312.6114.
BerenMillidge.Deepactiveinferenceasvariationalpolicygradients.CoRR,abs/1907.03876,2019.
URLhttp://arxiv.org/abs/1907.03876.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Eric P. Xing and Tony Jebara (eds.), Pro-
ceedingsofthe31stInternationalConferenceonMachineLearning, volume32ofProceedings
of Machine Learning Research, pp. 1278–1286, Bejing, China, 22–24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/rezende14.html.
Kai Ueltzho¨ffer. Deep active inference. Biological Cybernetics, 112(6):547–573, Dec 2018.
ISSN 1432-0770. doi: 10.1007/s00422-018-0785-7. URL https://doi.org/10.1007/
s00422-018-0785-7.
6
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
Supplementary Material
A NEURAL ARCHITECTURE
Layer Neurons/Filters activationfunction
roiretsoP
Convolutional 8 LeakyReLU
Convolutional 16 LeakyReLU
Convolutional 32 LeakyReLU
Convolutional 64 LeakyReLU
Convolutional 128 LeakyReLU
Concat N.A. N.A.
Linear 2x128states Softplus
doohilekiL
Linear 128x8x8 LeakyReLU
Convolutional 128 LeakyReLU
Convolutional 64 LeakyReLU
Convolutional 32 LeakyReLU
Convolutional 16 LeakyReLU
Convolutional 8 LeakyReLU
roirP
LSTMcell 400 LeakyReLU
Linear 2x128states Softplus
Table1:Neuralnetworkarchitectures.Allconvolutionallayershavea3x3kernel.Theconvolutional
layersintheLikelihoodmodelhaveastrideandpaddingof1toensurethattheypreservetheinput
shape. Upsampling is done by nearest neighbour interpolation. The concat step concatenates the
processedimagepipelinewiththevectorinputsaands.
7
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
B HYPERPARAMETERS
Parameter Value
gninraeL
learningrate 0.0001
batchsize 128
trainiterations 1M
sequencelength 10
gninnalP
γ 100
D(C¸ataletal.,2020) 1
K(C¸ataletal.,2020) 10,25,55
N(C¸ataletal.,2020) 5
ρ(C¸ataletal.,2020) 0.001
Table2: Overviewofthemodelhyperparemeters.
8
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
C DETAILED PLANNING EXAMPLE
Amoviedemonstratingtheresultsisavailableathttps://tinyurl.com/smvyk53.
Figure3: Trialpreferredstate
9
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
Figure4: Shorttermplanning
10
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
gninnalpmretgnolelddiM
:5erugiF
11
Publishedasaworkshoppaperat“BridgingAIandCognitiveScience”(ICLR2020)
gninnalpmretgnoL
:6erugiF
12