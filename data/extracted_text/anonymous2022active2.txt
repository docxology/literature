This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
7
Active Inference in Discrete Time
What I cannot create, I do not understand.
— Richard Feynman
7.1 Introduction
So far, we have discussed the princip les of Active Inference at a relatively
abstract level. This chapter deals with specific examples—a nd how they may
be specified in a practical setting. We focus on models of categorical vari-
ables in discrete time. Through a series of examples, building in complexity,
we illustrate models of perceptual proc essing, decision-m aking, information
seeking, learning, and hierarchical inference. T hese examples are chosen to
highlight as simply as poss ib le emergent properties—i ncluding meas ura ble
physiology and beh avi or—of Active Inference schemes.
7.2 Perceptual Pro cessing
We begin by considering perceptual proc essing and the inversion of the
sort of discrete-t ime models introduced in chapter 4. L ater in this chapter,
we build to a full partially observable Markov decision proc ess (POMDP).
However, we start with a special case of a POMDP in which we can ignore
choices and beh avi or: a hidden Markov model (HMM), which may be used
for perceptual inference of a sequential and categorical sort (see figure 7.1).
To motivate this, we w ill appeal to a s imple example. Imagine listening to
a perf orm ance of a short piece of m usic. The sequence of notes that are
written in the score may be thought of as hidden (unobserved) states, while
the sequence of notes we actually hear are the (observable) outcomes. If
the performer is a professional musician, the correspondence between the
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
126 Chapter 7
D s τ–1 B s τ B s τ+1 D P(s 1 )
A A A A P(o | s)
τ τ
o τ–1 o τ o τ+1 B P(s τ+1 | s τ )
Figure 7.1
This hidden Markov model uses the same notation introduced in chapter 4 to express
a sequence of states (s) that evolve through time. At each time, they give rise to an
observable outcome (o). The state at one time depends only on the state at the previ-
ous time (with this dep end ency expressed in B). The first state in the sequence has
prior probability D. The generation of outcomes from states depends on the likeli-
hood distribution (A). This specification of an HMM is generic, with specific generative
models depending on specific choices for A, B, and D.
hidden states and the outcomes may be very close. However, if an a mateur,
there may be an additional degree of stochasticity in the (likelihood) map-
ping from the note that should be played to that which is heard. In this sce-
nario, it may still be poss ib le to infer which note should have been heard,
given prior beliefs about the probability that each note is preceded or suc-
ceeded by another.
The example of listening to the amateur musician may be formalized in
the following way. First, we decide on how reliably our musician actually
plays the note (outcome) she intends to (hidden state). We can express this
through the A-m atrix, whose elem ents indicate the probability of an outcome
(rows) given a state (columns). In our toy example, we set this as follows:
⎡ 7 1 1 1 ⎤
⎢ ⎥
1 1 7 1 1
A= ⎢ ⎥ (7.1)
10⎢ 1 1 7 1 ⎥
⎢ 1 1 1 7 ⎥
⎣ ⎦
This says that 70 percent of the time, our musician hits her intended note.
We then specify the transition probabilities in the B-m atrix, which account
for the probability of the next state (rows) given the current state (columns):
⎡ 1 1 1 97 ⎤
⎢ ⎥
1 97 1 1 1
B= ⎢ ⎥ (7.2)
100⎢ 1 97 1 1 ⎥
⎢ 1 1 97 1 ⎥
⎣ ⎦
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 127
This says that t here is a 97 percent probability of the first note being fol-
lowed by the second, the second by the third, and so on. If we know that
the sequence always begins with the first note, we set the prior probability:
⎡1⎤
⎢ ⎥
0
D= ⎢ ⎥ (7.3)
⎢0⎥
⎢ ⎥
⎣⎢0⎦⎥
Together, equations 7.1–7.3 completely specify the HMM generative
model shown in figure 7.1. In other words, they provide a description of our
beliefs about how the m usic we hear is generated by our amateur musician.
Using equation 4.12 and substituting in our generative model, we can simu-
late the dynamics of the Bayesian belief updating induced by a sequence
of outcomes. This is shown in figure 7.2. Note the increase in confidence
shown in the upper-l eft plot as more data are accumulated over time, except
for the third time step, where an unexpected outcome has occurred. This
outcome could be explained in two ways. First, it may be that the intended
note r eally was an unusual note u nder our prior beliefs in equation 7.2. This
is made less likely by the rarity of such transitions u nder the B- matrix of
this model. The alternative, more plausible explanation is that the musician
played the wrong note by m istake. As shown in the third column of the
upper-r ight plot, this is the explanation that our simulated listener s ettles
on. However, a nonzero probability is assigned to the possibility that it was
the right note a fter all. The capacity to report this sort of uncertainty is a key
feature of the Bayesian perspective afforded by Active Inference.
The model shown h ere may be made more sophisticated in many ways,
but perhaps the simplest relies on the factorization of the state-s pace (Mirza
et al. 2016). An example might be the pitch and dynamics of the note (with
a similar distinction in the outcomes). In a visual inference task, the fac-
torization may be into what and where, which has a great deal of currency
in neurobiology (Ungerleider and Haxby 1994). In subsequent sections, we
will appeal to this sort of factorization to separate t hose states that can be
influenced by the creature in question from those that cannot. For further
reading on this sort of model (without actions in play) and the kinds of
neuronal message passing scheme that might be used to invert it through
minimizing f ree energy, see Parr, Markovic et al. (2019).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
128 Chapter 7
1
0.8
0.6
0.4
0.2
0
1 2 3 45 s 1 s 2 s 3 s 4 s 5
0.15
0.1
0.05
0
–0.05
1 2 3 45 o o o o o
1 2 3 4 5
Time step
s
ε
τ
τ
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Figure 7.2
These simulated perceptual inference plots illustrate the belief-u pdating proc ess in
an example trial based on the generative model outlined in the main text. Upper-
left: Beliefs (posterior probabilities) about each note in the sequence at each time
step. Upper- right: As the numerical values of t hese beliefs are difficult to track the
beliefs at the end of the sequence, having heard each note (i.e., retrospective beliefs)
are shown. Each column shows (retrospective) beliefs about the hidden states at a
given time step. Each row represents an alternative hypothesis for that hidden state.
The darker the shading, the more probable that note is considered to have been
(with black indicating a probability of one and white a probability of zero). Lower-
left: (Negative) f ree energy gradients (i.e., prediction errors) over time. The rate of
change of the beliefs in the upper-l eft plot is determined by the value of t hese errors
at each time step. Lower- right: Sequence of musical notes presented to our synthetic
agent (i.e., the observations he receives during time steps 1 to 5). Note that while
at the third time step (o) the listener heard the second note (third column of the
3
lower-r ight plot), he infers the third note with higher probability (third column of
the upper-r ight plot).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 129
7.3 Decision-M aking and Planning as Inference
The HMM used above illustrates a very s imple form of categorical inference
based on a sequence of outcomes. However, the sort of (sessile) creature
that this describes is rather uninteresting. Autonomous creatures are clearly
more than passive recipients of sensory data. Instead, they actively change
their environment and engage in a bidirectional exchange with their senso-
rium. This speaks to the importance of converting an HMM into a POMDP,
whereby we must infer not only how our environment is changing but also
how our chosen course of action changes it and which course of action to
choose.
Figure 7.3 shows a POMDP generative model. This is the same as that
introduced in chapter 4, where the details of inference in this sort of model
are unpacked. Note the similarity of this structure to the HMM in figure 7.1
G
A P(om | s1, s2,…,sn,…)
τ τ τ τ
(cid:31)
B P(sn | sn, (cid:31))
τ+1 τ
D s τ–1 B s τ B s τ+1 C P(o τ m)
A A A D P(s 1 n)
o o o G P((cid:31) | C, E)
τ–1 τ τ+1
Figure 7.3
POMDP from figure 4.3, unpacking the probability distributions in terms of hidden
state f actors and outcome modalities. (Figure 7.1 is a special case of this structure.)
Three points of note: First, the factorization of the hidden states now means that the
distribution encoded by A has (potentially) many state f actors in its conditioning
set and can no longer be encoded by a matrix. Instead, this becomes a tensor object,
in which each index corresponds to a state f actor. Second, the separation of the
outcomes into diff ere nt modalities means t here w ill be a separate A tensor for each
modality. Third, while C and E appear in the panel on the right, they do not appear
in the f actor graph on the left b ecause they only get into the generative model via
prior beliefs about policies. For an alternative perspective on this, see Parr and Friston
(2018d) and van de Laar and de Vries (2019).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
130 Chapter 7
and the addition of an extra variable (π ), on which the transition probabili-
ties (B) are conditioned. This means we can entertain alternative hypotheses
about the dynamics of states. These hypotheses may be interpreted as plans
that a creature may select between. This perspective equates policy evalua-
tion with model comparison and says that a policy is simply an explanatory
variable for an observed sequence of (self-g enerated) sensations.
The model in figure 7.3 differs subtly from that introduced in chapter 4: it
allows factorization of states (superscript n) and of outcomes (superscript m).
The utility of this is obvious when we consider the factorization of the visual
world into where an object is and what it is. Clearly, it would be extremely
inefficient (and incur a high complexity cost) to represent every poss ib le
combination of location and identity, when identity is (normally) invariant
to location and vice versa. A similar argument may be used for factorization
of time from identity and location (Friston and Buzsaki 2016). The benefit
of introducing this factorization at this stage is that we can separate t hose
states of the world over which a creature has control from those that it does
not. While the transition probabilities governing the former w ill be diff ere nt
under each policy, the latter w ill be invariant to this.
With t hese preliminaries in place, we now outline a simple example of
a task (Friston, FitzGerald et al. 2017) that requires planning and illustrates
some of the key aspects of active inference using POMDPs. This involves a
rat in a T-m aze containing an aversive stimulus in one arm, an attractive
stimulus in another, and a cue that indicates the location of the two stimuli
in the final arm. This setup means that the rat can behave in (broadly) two
ways. It could choose to go straight to one of the two arms that might con-
tain the attractive stimulus, risking the aversive stimulus. Alternatively, it
could choose to seek out the informative cue and then go to the arm most
likely to contain the attractive stimulus.
This choice speaks to the classical exploration-e xploitation dilemma in
psyc holo gy: a dilemma that is resolved under Active Inference. The resolu-
tion stems from the minimization of expected free energy mandated by
prior beliefs about policies. To review this briefly (see chapter 4 for details),
the most probable policies (for a creature who minimizes its variational f ree
energy) are t hose that lead to the lowest expected free energy. The expected
free energy has the following form:
G(π)=E
!Q#(s!|#π)
[
#
H
#
[P
#
(
#
o!|
"
s!)
#
]]−
#
H
#
[Q
#
(
#
o!|
#
π
$
)]−E
"Q$(o!$|π$)
[l
#
n
$
P(
$
o!|
$
C
%
)]
(7.4)
Negativeepistemicvalue (−I(π)) Pragmaticvalue
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 131
This decomposition of the expected f ree energy into epistemic and prag-
matic value highlights the (epistemic) drive t oward information gathering
and the (pragmatic) drive t oward realizing prior beliefs (C in figure 7.3). We
will attempt to provide a deeper intuition for the epistemic value in the next
section, but it can be thought of simply as the amount of information we
stand to gain under a specific policy. The form of the pragmatic value effec-
tively treats the probability of outcomes, averaged over all policies, as if it
were a prior. In d oing so, t hose policies with consequences consistent with
this prior become more probable, as they are associated with lower expected
free energy. To put this in more intuitive terms, if we consider a certain sort
of observation to be very probable, we w ill act to fulfill our belief that we
will encounter t hese. Therefore, the log probability of outcomes may be
thought of as equivalent to a utility function in other formalisms, such as
optimal control theory and reinforcement learning. The fact that utility
and the value of information emerge as two components of the expected
free energy means that we do not need to worry about balancing explora-
tion and exploitation. Both are in ser vice of optimizing the same function.
To see how this unfolds in the T-m aze example, we need to formalize the
generative model in the same way as with the HMM above. Figures 7.4–7.6
illustrate the likelihood and transition probabilities that comprise the gen-
erative model for the T-m aze. We w ill go through this in some detail, as this
minimal example provides the building blocks from which readers can con-
struct their own generative models. The first t hing to do is to decide on the
number of outcome modalities that represent the (sensory) data our model is
supposed to explain. This tells us the number of A-m atrices we must specify.
Here, we have two modalities that represent exteroceptive data pertaining to
where the rat is in the maze (A1) and a what modality that may be the intero-
ceptive data the rat experiences when it has found the attractive (edible) stim-
ulus (A2). The levels in t hese modalities (i.e., the alternative observations that
could be made in each) determine the rows of each A-m atrix. The next deci-
sion is the number of hidden state f actors that may be used to explain t hese
data; this is the number of B-m atrices we require. We consider two f actors
here: the position of the rat in the maze, and the context (attractive stimu-
lus on left or right). T hese have four and two levels, respectively. We now
must specify, for each combination of hidden states, the probability of each
outcome. Context 1 is shown in figure 7.4; context 2 is shown in figure 7.5.
For the first modality, our A1 associates each location with an outcome with
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
132 Chapter 7
1 O O O
O O O O L
1
A O 1 O O R
O O 1 O
O O O 1
1 1 0 0
R A 2 0 0 2 100 98 100
98 2
0 0 100 100
Figure 7.4
Likelihood in context 1. Left: T-m aze configuration of cues and stimuli: the attractive
stimulus is on the right and the aversive stimulus is on the left. Right: Likelihood or
observation model specifies the probabilistic mapping from location to exteroceptive
cues (A1) and to interoceptive cues (A2). Each elem ent of t hese matrices is the prob-
ability of the outcome illustrated at the end of the row, conditioned on the context
being one, and on being in the location indicated by the row. The exteroceptive
outcomes are visual or proprioceptive input associated with each location, whereby
the cue location can give rise to a rightward or a leftward cue. The interoceptive
outcomes are absent (circle with dashed outline), attractive (filled circle), or aversive
(unfilled circle).
probability one. The cue location may be associated with a left or a right
cue, depending on the context. The interoceptive modality (A2) associates
a neutral outcome with the start and cue locations and a 98 percent chance
of finding the attractive outcome when the context matches the arm of the
maze the rat has entered. Technically, t hese A-m atrices are tensor quantities,
because their elem ents are specified by three numbers (outcome, location,
and context), while a matrix is only specified by two (row and column).
We then need to specify transition probabilities. The B- matrices specify
the probability of transitioning from a state (column) to another state (row),
depending on the choice of policy (π ). T hese specify the transitions pertain-
ing to the position of the rat in the maze (B1) and transitions in the context
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 133
1 O O O
O 1 O O L
1
A O O O O R
O O 1 O
O O O 1
1 1 0 0
L A 2 0 0 98 100 2 100
2 98
0 0 100 100
Figure 7.5
Likelihood in context 2. Nearly identical to figure 7.4—in this context, the aversive
and attractive stimuli have been swapped. This is reflected in the probability of the
exteroceptive outcomes in the cue location and the probabilities of the interoceptive
outcomes in the right and left arms of the maze.
(B2). Figure 7.6 shows the controllable B1-t ransitions. Each matrix shows
the probabilities u nder a diff ere nt action choice (subscripted). T hese allow
a move from any location to any other location, except for from the two
arms of the maze, which are absorbing states. This means that once t here,
the rat must stay t here, regardless of the actions it chooses. In contrast, the
rat has no control over the context (i.e., w hether it is in context 1, shown
in figure 7.4, or context 2, shown in figure 7.5). Context stays constant over
time and can be represented as an identity matrix:
⎡1 0⎤
B π 2 =⎢ 0 1 ⎥ (7.5)
⎣ ⎦
Here each column (and row) refers to a state indexing either figure 7.4
or figure 7.5. This means that whichever context we start in stays con-
stant (transitions to itself ) over time. This is true regardless of the policy
selected. The C1-v ector shows prior preferences for each of the outcomes in
this modality, with uniform preferences except for a slight aversion (−1) to
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
134 Chapter 7
1 1 0 0 0 0 0 0
0 0 0 0 0 0 0 0
B1 (cid:31)(τ) = 1 = B1 (cid:31)(τ) = 3 =
0 0 1 0 1 1 1 0
0 0 0 1 0 0 0 1
0 0 0 0 0 0 0 0
1 1 0 0 0 0 0 0
B1 = B1 =
(cid:31)(τ) = 2 (cid:31)(τ) = 4
0 0 1 0 0 0 1 0
0 0 0 1 1 1 0 1
Figure 7.6
Controllable transition probabilities for moving between the diff ere nt locations. Each
of the four matrices corresponds to an alternative action the rat can choose. T hese
allow for a move from any state (except the right and left arm) to any other state. The
right and left arms are absorbing states, in which the rat must stay once entered.
the start location. The C2-v ector specifies preferences (+6) for the attractive
stimulus and aversion (−6) to the aversive stimulus. The absence of e ither is
considered neutral (0).
C1=σ([−1,0,0,0,0]T)
(7.6)
C2 =σ([0,6,−6]T)
The order of elem ents in t hese vectors corresponds to the order of rows
in the corresponding A-m atrices. The softmax function (σ ) allows us to
specify preferences in terms of positive and negative values (corresponding
to unnormalized log probabilities), which are then converted to probabili-
ties. This preserves the difference in log probabilities (or the relative prob-
ability) while ensuring normalization. Practically, this formulation means
the attractive stimulus is considered e6 (≈ 400) times more probable than
the neutral stimulus u nder the rat’s generative model. This is a very strong
preference that means the rat believes its actions are much more likely to
lead to the attractive outcome. This constraint on inference about action is
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 135
crucial for the beh avi or that follows. Fin ally, the D-v ectors specify the prior
probabilities for the initial states:
D1=[1,0,0,0]T
(7.7)
D2 = 1[1,1]T
2
The order of elem ents in t hese vectors matches t hose of the B- matrices. The
D1-v ector indicates a confident belief in starting at the center of the maze.
The D2-v ector indicates that the two contexts (figure 7.4 or 7.5) are consid-
ered equally probable at the start.
Figure 7.7 shows what happens when we invert the generative model of
figures 7.4–7.6. The upper row illustrates what we would see if observing
the rat’s beh avi or. It starts in the center and then goes to the informative
cue. This is due to the high epistemic value associated with this location (i.e.,
the observations made at this location have the potential to resolve uncer-
tainty about the context). On seeing the cue that indicates a left context
(context 1), the rat chooses the left arm of the maze and finds the reward-
ing stimulus. This move is driven by the high pragmatic value attributed
to this location. The lower plots illustrate the belief updating that occurs
during this s imple trial. As in figure 7.2, this is shown in the form we might
expect to observe in an idealized rat if we w ere meas uri ng neuronal activity
(i.e., firing rates and local field potentials [LFPs]). Note the rapid change in
beliefs at the second time step, when the rat reaches the informative cue
location, and associated LFP (dashed line).
7.4 Information Seeking
The simulation in section 7.2 illustrates a simple example of an exploration-
exploitation trade-o ff, which is solved by foraging for information u ntil
uncertainty is resolved, then exploiting what has been inferred to fulfill
prior preferences. In this section, we unpack the concept of epistemic value
in greater detail. As we saw in equation 7.4, this comprises two terms:
I(π) = H[Q(o!|π)] −E [H[P(o!|s!)]]
! !#"#$ !Q#(s!|π#)#"###$
Epistemic value Post. pred. entropy Expected ambiguity
=D [P(o!|s!)Q(s!|π)||Q(o!|π)Q(s!|π)]
!K#L######"#######$
(7.8)
Mutual information
P(o!|s!)Q(s!|π)
=E [D [Q(s!|π,o!)||Q(s!|π)]]; Q(s!|π,o!)!
!Q#(o!|#π)##KL##"######$ Q(o!|π)
Information gain, salience, Bayesian surprise
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
τ = 1 τ = 2 τ = 3
L
L L L
1
0.8
0.6
0.4
0.2
0
1 2 3
0.08
0.06
0.04
L
0.02
0.00
–0.02
–0.04
R
–0.06
1 2 3
Time step
τS
S1 S1 S1
1 2 3
S2 S2 S2
1 2 3
τε
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Figure 7.7
Simulated epistemic and pragmatic beh avi or of a rat foraging in a T- maze. The rat
starts in the central location but then chooses to sample the informative cue in the
lower arm of the maze. This location is associated with the greatest epistemic value,
as observing the cue in this location reveals the context (reward right or left) that
the rat finds itself in. On observing the cue, the rat undergoes rapid belief updating
(s), inducing an LFP (ε ). With no more uncertainty to resolve, the rat selects the prag-
matically valuable option and goes to the left arm of the maze. The two plots on the
right show the beliefs held by the rat at the end of the trial about all previous times
(i.e., t hese are retrospective beliefs and not the beliefs of the rat at the moment of
the decision). It believes (correctly) that it started in the central location, went to the
cue arm, and then went to the left arm. For the context hidden state factor, the rat
believes that the context was the left context throughout.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 137
These are the posterior predictive entropy and the expected ambiguity, respectively.
Below t hese, we highlight the correspondence between t hese and other rear-
rangements. To unpack t hese in an intuitive way, we w ill frame this in terms
of a visual paradigm, where alternative saccades (π ) lead to diff ere nt transi-
tions between fixation locations (s). In addition to fixation locations, the hid-
den states include the identity of a stimulus at each location. A combination
of stimulus and fixation generate visual and proprioceptive consequences (o).
With this in mind, we can interpret the posterior predictive entropy as the
dispersion (or uncertainty) associated with “what I would see if I performed
this eye movement.” From the perspective of a scientist, this quantifies how
uncertain we might be about the data we would obtain on performing a given
experiment. U nder this perspective, it makes sense that we should select
those saccades (or experiments) that are associated with the greatest posterior
predictive entropy, as t hese offer the greatest potential for uncertainty reso-
lution. We would gain nothing by performing an experiment if we already
knew what the results would be with a high degree of confidence.
However, the predictive entropy only tells us the total amount of uncer-
tainty. It does not tell us how much uncertainty is actually resolvable. We
will always be uncertain about the next number in a sequence of randomly
generated numbers, but we w ill never resolve our uncertainty about the
proc ess generating them by fixating on t hese. This is where the expected
ambiguity comes in. This quantifies the degree to which observations and
states are ind ep end ent of one another. If states always generate the same
observation, this quantity w ill be zero. It w ill be maximal if, as in the ran-
dom number generator, t here is no association between states and outcomes.
In the visual domain, this implies that the best saccade will be that toward a
well-l it stimulus, where t here is l ittle ambiguity about “what I would see if I
looked at this stimulus.” Taken together, this says that the best saccades (i.e.,
perceptual experiments) are t hose for which t here is the greatest uncertainty
to resolve (posterior predictive entropy) but only if that uncertainty can be
resolved (negative ambiguity). Interestingly, this has exactly the same form
as expressions developed in statistics to score experimental design in terms
of information gain (Lindley 1956).
Figure 7.9 illustrates what happens in a saccadic paradigm (Parr and
Friston 2017b) when we simulate manipulations to the ambiguity and pos-
terior predictive entropy. This shows four stimuli (squares), each of which
may change color from moment to moment. Superimposed on t hese is a
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
138 Chapter 7
simulated eye-t racking trace, as if we w ere meas uri ng where an experimental
participant was looking. Crucially, we specify prior beliefs about outcomes
to be uniform (i.e., pragmatic value to be absent), precluding any preference-
based choices. This means each saccade is selected to maximize epistemic
value. When the generative model treats all four stimuli as equivalent (left
image), all are sampled with approximately the same frequency. However,
we can modulate the uncertainty associated with each stimulus (see box
7.1). If we set one stimulus to have a greater ambiguity (by increasing the
value of off-d iagonal elem ents of the corresponding A-m atrix), this square
is ignored (m iddle image). This is an example of the famous “streetlight”
effect (Demirdjian et al. 2005), which takes its name from the metap hor of
Box 7.1
Uncertainty and precision
The example in figure 7.7 appeals to the concept of precision—an import ant
idea in this book. Precision is the inverse of variance and scores our confi-
dence in a given probability distribution. This is closely related to the negative
entropy (negentropy) of a distribution:
−H[P(s)]=EP(s) [lnP(s)]
A s imple way to par ame t erize a distribution such that it can be made more
or less precise is to use a Gibbs form with an inverse temperature pa ram e ter (ω ).
This has the following form:
P(s|ω)=Cat(σ(ωlnD))
Note that the precision multiplies the log prior, so it can be interpreted as a
gain-c ontrol device (amplifying as opposed to adding to neural signals). The
plots in figure 7.8 show how the probability distribution (each column repre-
senting the probability of an alternative state) changes for a given D when we
vary ω. Note the increasing confidence with increasing precision.
This sort of par ame t erization may be applied to any of the distributions
used in a POMDP. In addition, we can define priors over the precision and infer
ω = 0.1 ω = 1 ω = 10 ω = 100
Figure 7.8
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 139
Box 7.1 (continued)
this just as we infer other latent variables (i.e., through f ree energy minimiza-
tion). Assuming the prior has a Gamma distribution (precluding negative val-
ues of the precision), we get the following updates (see appendix B for details):
P(ω)=Γ(1,β )
ω
Q(ω)=Γ(1,β )
ω
.
⇒β
ω
=(Dβω−1−s)ilnD+β
ω
−β
ω
There is an increasing recognition that the biological substrate of t hese
precision para meters may be the neuromodulatory systems that set the gain of
neural responses. Chapter 5 discusses the evidence relating t hese para meters
to specific neurochemicals.
Figure 7.9
Simulated epistemic visual search paradigm (Parr and Friston 2017b) with the syn-
thetic eye-t racking trace superimposed on the four stimulus locations. Each stimu-
lus (shaded square) is associated with a transition matrix that may be more or less
predictable and a likelihood matrix that may be more or less ambiguous. Left: When
transitions and likelihoods are equally predictable for all four locations, all locations
are sampled with about the same frequency. Middle: The viewer shows aversion to
the upper-l eft square when it is specified with a less precise (more ambiguous) like-
lihood mapping. Right: The lower-l eft square is epistemically attractive when the
transition probabilities are specified as more uncertain.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
140 Chapter 7
people who have lost their keys late at night. The first place they might look
is u nder the streetlight—n ot b ecause the keys are most likely to be t here, but
because it is the best place to find high-q uality, unambiguous, uncertainty-
resolving information. The simulation shows how the ambiguous (e.g.,
poorly lit) square is ignored, reproducing an in silico streetlight effect.
In contrast, the right image in figure 7.8 shows what happens when we
make the transitions less predictable for the lower-l eft square. We accumu-
late uncertainty about this location very quickly, ensuring a high posterior
predictive entropy with no change to the ambiguity. As we can see, this
leads to more frequent fixation on this location, as t here is always new
uncertainty to resolve h ere. Intuitively, if I know something has very pre-
dictable dynamics, I do not have to look at it very often to be confident
about its state. In contrast, if something may have changed in the time that
I have been looking at something e lse, it is worth looking back at to check.
These simulations are designed to offer an intuition for the two parts of the
epistemic value, to see how minimization of expected f ree energy ensures
we actively select our sensory data to find out about the world.
7.5 Learning and Novelty
Sections 7.2–7.4 set out everyt hing that is required for the majority of prac-
tical applications of Active Inference. However, we have assumed that the
generative model is already known and does not change as an effect of learn-
ing. In some practical applications, we may want to consider how one or
more parts of the generative model (e.g., the A- or B-m atrix) are learned
during an experiment or, more broadly, how we optimize the structure of
the generative model itself, given some data (Friston, FitzGerald et al. 2016).
In doing so, Active Inference extends to active learning, and the salience
(equation 7.5) describing information gain about states is complemented
by novelty, which deals with resolution of uncertainty about (for example)
the elem ents of the A matrix shown in equation 7.1, the B matrix shown in
equation 7.2, or any other para meters of the generative model. T hese beliefs
can now vary with time rather than being fixed, as assumed so far (Schwart-
enbeck et al. 2019). To get to this, we first have to extend the generative
model as in Figure 7.10 to include beliefs about t hese model para meters.
Conceptually, including beliefs about para meters in the generative model
permits treating learning as another form of Bayesian inference— namely,
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 141
c e
C E
G
d
(cid:31)
D
b
a B s τ–1 B s τ B s τ+1
B
A A A
A
o o o
τ–1 τ τ+1
Figure 7.10
This generative model for learning uses the same POMDP structure as in figure 7.3,
but the priors for each of the hidden states now depend on variables (in circles),
which themselves now come equipped with prior beliefs. T hese have the form of
Dirichlet distributions, which are conjugate (see box 7.2) to the categorical distribu-
tions considered thus far. The model shows how the likelihood of outcomes given
states now also depends on a variable A (which is the same for all time-p oints), the
transition probabilities are now conditioned on a variable B, the preferences depend
on C, the initial states depend on D, and the fixed form policy prior depends on E. By
making prior beliefs about the para meters of the generative model explicit, this figure
emphasizes that both inference and learning are f ree energy minimizing proc esses,
but they are distinct. In short, inference describes the optimization of beliefs about
the state of the world as it is (s), including beliefs about the way in which we are act-
ing (π ). In contrast, learning describes optimization of beliefs about the relationships
between these variables (A, B, C, D, or E ). The latter vary much more slowly than the
former and may only be learned when the states have been inferred. We w ill return to
this separation of timescales below when we consider hierarchical generative models.
as the passage from prior to posterior beliefs about model para meters. This
highlights the fundamental similarity of perception and learning: in the
same way that perception can be described as the inversion of a generative
model to infer hidden states from observations, learning can be described
as the inversion of a generative model to include beliefs about par ameters
(although normally this inversion may operate on a slower timescale).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
142 Chapter 7
Box 7.2
Conjugate priors
When setting up a generative model of the form in figure 7.10, it is import ant to
carefully select the appropriate distribution for prior beliefs. Typically, these will
be the conjugate prior distribution associated with the likelihood. A conjugate
prior belief means that, when used to perform Bayesian inference, the posterior
belief w ill be the same type of distribution. For example, using Bayes’ rule:
P(D | s) ∝ P(D) P(s | D)
If P(s | D) is a categorical distribution, when we choose a Dirichlet distribution
(conjugate to categorical ) for P(D), we can guarantee that P(D | s) is also a Dirich-
let distribution. Put formally:
P(D)=Dir(d) ⎫
⎬⇒P(D|s)=Dir(d)
P(s|D)=Cat(D)⎭
The simplest way to choose the right kind of prior is to look up the con-
jugate prior for whate ver form the likelihood distribution takes. For the cat-
egorical distributions used h ere, a Dirichlet distribution is the appropriate
choice for beliefs about para meters (see box 7.2). Having included t hese addi-
tional prior beliefs, we can now optimize posterior beliefs about the structure
of the generative model. This means incorporating t hese into the f ree energy
(as we did for states in chapter 4) and finding the f ree energy minima.
θ=(A,B,C,D,E)
(7.9)
F=EQ(π,θ) [F(π,θ)]+D
KL
[Q(θ)||P(θ)]+D
KL
[Q(π)||P(π)]
Dirichlet distributions are par ame t erized by counts (or pseudo-c ounts)
that index the number of times a given categorical variable has been seen
(or, in the case of the priors, as if it had been seen that number of times).
For the derivation of the update rules for t hese para meters, see appendix B.
For now, we summarize the update rule and key properties of a Dirichlet
distribution, focusing on the a and a concentration para meters associated
with the prior and posterior over A.
a=a+∑ s ⊗o
τ τ τ
a
EQ [A ij ]=A ij ≈ a 0 ij j (7.10)
EQ [lnA ij ]=lnA ij =ψ(a ij )−ψ(a 0j )
a !∑ a
0j k kj
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 143
The first line h ere expresses the update from prior to posterior concen-
tration para meters following a series of observations, with beliefs about the
states that caused them. The cross in the circle indicates a Kronecker ten-
sor product (or outer product in the case of two vectors), h ere giving rise
to a matrix in which each elem ent is the product of a pair of elem ents in
s and o. This update rule may be interpreted simply as a form of activity-
τ τ
dependent plasticity. When an outcome is observed in combination with a
posterior belief that a part icu l ar state caused it, the elem ent of the matrix
representing the relationship between the two is incremented. The second
line of the equation highlights the interpretation of the Dirichlet concen-
tration para meters in terms of counts. For a given state (column), each ele-
ment of a is the number of times the corresponding outcome has been seen.
Dividing by the sum of the elem ents in the column (total number of obser-
vations or pseudo-o bservations) gives the probability of each outcome given
that state. To understand why this (pseudo) counting method makes intui-
tive sense, consider the amateur musician example from the beginning of
this chapter. If one counts how many times the musician hits the first note
when she intends to do so (first row and column), how many times she hits
the second note when she intends to do so (second row and column), and
so on, and divides t hese by the total number of times she intends each note,
one w ill eventually converge to the correct numerical values of the A- matrix
shown in equation 7.1—n amely, that the musician hits all her intended
notes 70 percent of the time. The counting method has another import ant
consequence that we w ill return to: The number of counts or pseudo-c ounts
preceding an observation tells us how likely we are to update our beliefs on
making the observation. Imagine flipping a coin five times and getting five
heads in a row. This might lead us to update our beliefs to favor the hypothe-
sis that this is an unfair coin. However, if this had been preceded by 100 flips
with 50 heads and 50 tails, the final five heads would do l ittle to influence
our beliefs about w hether this is a fair coin. The third line of equation 7.10
shows a useful identity associated with Dirichlet distributions: the expected
log of the random variable is given by the difference in two digamma func-
tions (derivative of a gamma function).
The inferential approach to learning highlights an import ant difference
between Active Inference and most other approaches to computational
neuroscience and machine learning, which incorporate vario us learning
rules (e.g., Hebbian rules or error backpropagation) that are considered
biologically realistic or computationally efficient. In Active Inference, the
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
144 Chapter 7
update rules that govern learning are derived from statistical consider-
ations, yet they turn out to be remarkably similar to biologically motivated
rules for activity-d ependent plasticity (see the above considerations on the
first line of equation 7.10). This exemplifies one of the appeals of norma-
tive approaches, which start from first princip les to explain what we know
about brains and beh avi or—a nd t hings that we did not know.
A further difference between Active Inference and most machine learn-
ing approaches is that learning is naturally described as an active proc ess, in
which creatures autonomously select the most appropriate data to improve
their generative models. This becomes evident if one considers that when
including beliefs about para meters in the model, the expected f ree energy
acquires an additional term:
G(π)=
"
D
K$L
[
$
Q(
$
o!
$
|π
#
)
$
||P
$
(
$
o!|C
$%
)]+E
"Q$(s!|π$)$
[H
#
[
$
P(
$
o!|
$
s!
%
)]]
Risk Ambiguity
+E
"Q! $(o!,$s!,θ$|π)$
[ln
$
Q
$
(
#
θ)
$
−
$
ln
$
P
$
(θ
$
|o!
$
,s
%
!)]
Parameter information gain (7.11)
=−E
!Q#(o!|π#)
[
#
D
#KL
[
#
Q
#
(
"
s!|π
#
,o
#
!)
#
||Q
#
(s
#
!|π
#$
)]]
Salience
−E
!Q! #(o!,#s!|π#)
[D
# K # L #
[Q
"
(θ
#
|o
#
!,s!
#
)|
#
|Q
#
(θ
#$
)]]−E
"Q$(o!|$π$)
[l
#
n
$
P(
$
o!|
$
C
%
)]
Novelty Pragmatic value
The salience and pragmatic value terms w ere already in place in equation 7.4,
but the novelty term is new. The final equality h ere shows an arrangement
that highlights the relationship between salience and novelty. In short,
salience is to inference what novelty is to learning. Both are expressions
of the change in beliefs anticipated once a perceptual experiment (i.e., an
action in a policy) is performed. As with scientific experiments, the greater
the change in beliefs following data collection, the better the experiment.
Returning to the analogy of flipping a coin and accumulating counts, this
tells us something useful. If we have two coins and can choose to flip e ither
one, we can elicit the greatest change in beliefs by flipping the coin we had
flipped only five times previously rather than the coin with 100 previous
flips. T here is greater novelty associated with flipping the former (less famil-
iar) coin. Similarly, if we have confident prior beliefs as if we had observed
something many times, policies that interrogate t hese variables are associ-
ated with less novelty than those about which we have less confident beliefs.
To illustrate how this works in practice, imagine we have a very myopic
creature standing on a tiled floor. This creature can only see the color of
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 145
the tile it is standing on and can only move one tile at a time. For any suit-
ably large landscape with many tiles, it is very computationally expensive
to represent the color of each tile as a diff ere nt hidden state. However, a
simpler form of model is available. If we associate hidden states only with
location, and colors only with outcomes, we can efficiently represent beliefs
about “what I would see if I went over there” in the A matrix that generates
colored tiles from locations. By accumulating Dirichlet para meters (equation
7.10), our creature can optimize t hese beliefs on the basis of observations. We
might interpret this as a form of synaptic memory as opposed to the main-
tenance of pers ist ent activity in neurons representing beliefs about the color
of a given tile. Given this sort of generative model, wherein all of the uncer-
tainty is in the para meters of the likelihood distribution, it is int ere st ing to
see what happens in the absence of any preferences (i.e., when the novelty
term of equation 7.11 dominates policy sel ection). Figure 7.11 shows a sim-
ulation of a s imple environment comprising 64 black or white tiles. As each
tile is visited, beliefs about the likelihood of observing black or white in
that location are updated through accumulation of Dirichlet para meters. As
large Dirichlet para meters preclude large belief updates, the drive to novelty
resolution given by expected f ree energy minimization leads our simulated
creature to avoid any previously visited locations.
The same princip les could be applied to a range of other paradigms (e.g.,
if we reinterpret the path taken by our creature as a saccadic scan path,
this could be applied to active visual sampling). In the domain of active
vision, this has been used to simulate the kinds of visual search beh avi or
induced by target cancellation tasks (Parr and Friston 2017a). Subsequently,
evidence for the short-t erm plasticity required in accumulating Dirichlet
para meters in this setting has been demonstrated (Parr, Mirza et al. 2019).
Just as we can extend ideas about inference to learning, it is poss ib le to
go (at least) one step further and think about structure learning: the proc ess
of not just optimizing the para meters in the model but selecting between
diff ere nt models with more or fewer para meters in play. Box 7.3 sets out a
way of d oing this that involves efficient post hoc comparisons of alterna-
tive hyp ot heti c al models. This has been used as a metap hor for sleep (Fris-
ton, Lin et al. 2017) and resting spontaneous activity (Pezzulo, Zorzi, and
Corbetta 2020), where no new data are collected but the structure of the
model may still be refined and simplified.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
146 Chapter 7
Path Likelihood
emiT
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Figure 7.11
Active learning is demonstrated by a synthetic creature exploring a s imple world of
black and white tiles (Bruineberg et al. 2018, Kaplan and Friston 2018). Left: Path
taken by the creature, showing which tiles are white and which are black (dots cor-
respond to visited locations). Right: A matrix of the creature and the beliefs (in terms
of normalized Dirichlet counts) the creature has about what it would see on g oing
to diff ere nt locations. Cells in the A matrix are white (or black) if the creature has a
strong belief that the corresponding tile is white (or black); they are grey if the crea-
ture is uncertain about color. Crucially, t hese beliefs influence which path it takes via
the novelty term of the expected free energy. T hose locations about which it has con-
fident beliefs afford relatively l ittle opportunity for uncertainty resolution, so it does
not revisit them. In other words, the phenomenon of “inhibition of return” (Posner
et al. 1985) emerges naturally from the minimization of expected f ree energy.
7.6 Hierarchical or Deep Inference
In the previous section, we saw one method for hierarchical extension of
the original generative model based on defining priors over the par ameters
of the generative model. Figure 7.12 shows a second form of hierarchy that
speaks to the nesting of temporal scales. This generative model for hier-
archical or deep inference can be conceived of as a hierarchical extension
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 147
Box 7.3
Structure learning and model reduction
The discussion in section 7.4 deals with an import ant, but l imited, form of
(parametric) learning. The next level of sophistication—in learning about the
structure of the world—g oes beyond the optimization of model para meters
and asks w hether we should expand or prune the model structure. This can
be cast as a question of model comparison (Friston, Lin et al. 2017). In other
words, would my f ree energy increase or decrease if I w ere to (for example)
eliminate elem ents of a likelihood matrix? By comparing models with and
without t hese elem ents, we can answer this question. However, it may be very
costly to have to explici tly invert multiple models. Fortunately, an efficient
method for d oing this—k nown as Bayesian model reduction (Friston, Litvak
et al. 2016; Friston, Parr, and Zeidman 2018)—is available and only requires
inversion of a single full model. In a general setting, comparison between a
full model and one with alternative priors (indicated by ~) can be achieved
through the following formulae:
ΔF=F[P!(θ)]−F[P(θ)]=lnEQ(θ)
⎡
⎣ ⎢ P
P
!(
(θ
θ
)
)
⎤
⎦ ⎥
Q!(θ)∝exp(lnQ(θ)+lnP!(θ)−lnP(θ)+ΔF)
For the Dirichlet priors used in section 7.5, this takes the form (where B is
the multivariate beta function):
ΔF=lnΒ(d!)−lnΒ(d)+lnΒ(d)−lnΒ(d!)
d!=d+d!−d
This form of model reduction may be import ant in understanding offline
model optimization, of the sort that may occur during sleep. We w ill briefly
revisit Bayesian model reduction in chapter 8, when considering the optimiza-
tion of hierarchical models with both discrete and continuous components.
of the shallow model shown in figure 7.3: it includes a series of POMDP
models at the lower level that are the same as in Figure 7.3 (one example
is outlined with the dashed box), contextualized by a higher-l evel POMDP.
Importantly, this generative model includes variables that evolve at dif-
fere nt timescales: slower for higher levels and faster for lower levels (Friston
2008; Friston, Rosch et al. 2017; Pezzulo, Rigoli, and Friston 2018). This
becomes evident if one considers that the POMDP models at level 1 evolve
over three time steps, but each of t hese short trajectories of states and out-
comes depends on a single state at the higher level (level 2) that persists
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
148 Chapter 7
throughout the entire trajectory at the lower level. In other words, for e very
time step from the perspective of the higher level, t here are multiple (h ere,
three) time steps for the lower level.
To gain some intuition for this separation of timescales—w hich under-
writes deep temporal inference—it is worth thinking about a s imple exam-
ple of hierarchy in everyday life: reading. We draw inferences about words
that combine to form sentences. Sentences combine to form paragraphs,
pages, books, libraries, and so on. If we imagine that each state at the lower
level of figure 7.12 is a word, each state at the higher level can be thought
of as a sentence. Crucially, the duration of the sentence transcends that of
any one word in the sequence.
The reading example is illustrated in more detail in figure 7.13, which is
based on the example from Friston, Rosch et al. (2017), to which we refer
G
D
G G G
D B B D B B B B B
A AA A A A A A A
2
LEVEL
1
LEVEL
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
(cid:31)
s τ–1 B s τ B s τ+1
A A A
o o o
τ–1 τ τ+1
(cid:31) (cid:31) (cid:31)
s s s s s s s s s τ–1 τ τ+1 τ–1 τ τ+1 τ–1 τ τ+1
o o o o o o o o o
τ–1 τ τ+1 τ–1 τ τ+1 τ–1 τ τ+1
Figure 7.12
We can extend the (shallow) generative model set out in figure 7.3 so that it affords
hierarchical or deep inference, which evolves over multiple timescales. The full gen-
erative model includes a slowly changing context (at level 2) that generates a series
of short trajectories at the lower, faster level 1. The form of the POMDP is the same at
the higher level as at the lower level (one of the POMDPs is outlined with the dashed
box). The only difference is that it is stretched out in time (horizontally) and that the
outcomes it generates are not directly observed. Instead, they form empirical priors
for the lower level, which generates observable outcomes.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 149
for more details. The model is structured as in figure 7.12 and represents
sentences (at the higher level) and words (at the lower level) drawn from
a very s imple language. This language comprises three poss ib le words ( flee,
feed, wait) that may be arranged into six poss ib le four-w ord sentences. If the
sentence is “flee, wait, feed, wait,” the higher level predicts the word flee for
the first of the lower-l evel POMDPs, wait for the second, and so on. At the
lower level, we start with an empirical prior (D) based on the higher level,
which tells us which words are most plausible. For example, if we started
with a uniform distribution over the sentences shown in the upper panel of
figure 7.13, we see that the first word is wait in two-t hirds of the sentences
and flee in the other third. This means that at the first time step of the first
low-l evel POMDP, our D-v ector should ascribe t hese probabilities to t hese
words.
The words at the lower level then generate observations, visual inputs
based on which part of the word is currently foveated. Much as in the
example of figure 7.9, the POMDP allows for sel ection of diff ere nt foveal
targets to accumulate evidence for or against each hyp ot heti c al word. This
appeals to the same expected f ree energy minimizing proc esses outlined
above; therefore, we w ill not detail the specific foveations made here, but
we note that with each time step at the lower level, there is an increase in
confidence about the word in play. In the sequence shown in figure 7.13,
we see that evidence is accumulated for the word flee at the lower level over
the first few time steps (over the fast scale, τ (1)). This inference is propa-
gated back up to the higher level, where it provides evidence for the first
and fourth sentences (each of which start with this word). Over subsequent
time steps the evidence accumulated at the lower level is consistent with
both sentences. At the fourth step (at the slow scale, τ (2)), we would predict
wait u nder the first sentence and flee u nder the second. On inferring wait
at the fast timescale, the first sentence is inferred at the slow scale. At the
final step, the simulation selects the correct sentence and is rewarded with
correct feedback. The resulting belief updating is seen in the LFP plot in the
lower part of figure 7.12.
Deep temporal models of this sort have been used to simulate reading
(Friston, Rosch et al. 2017), delay-p eriod working memory tasks (Parr and
Friston 2017c), and computation of empirical priors for visual inference (Parr,
Benrimoh et al. 2018). In addition, they have been leveraged in theoretical
accounts of motivation and control (Pezzulo, Rigoli, and Friston 2018). In
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
150 Chapter 7
“flee, wait, feed, wait”
“wait, wait, wait, feed”
sτ (2) “wait, flee, wait, feed”
“flee, wait, feed, flee”
“wait, wait, wait, flee”
“wait, flee, wait, flee”
“flee”
sτ (1) “feed”
“wait” “wait”
.
Vτ
τ(2) = 1 , 2, 3, 4, 5
τ(1) = 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3
Figure 7.13
Belief updating occurs over multiple timescales by inverting a simulated hierarchical
inference model. This relies on a generative model with a separation of timescales
(shown as a slow timescale—τ (2)—a nd a fast timescale—τ (1)). Belief updating at the
higher level (s(2) ), representing sentences, is slower than at the lower level (s(1)), rep-
resenting words. Lower panel: LFPs, i.e., the rate of change of the log expectations—
which is proportional to the prediction errors (ε ) shown in previous figures.
princip le, t hese models can be extended to an arbitrary number of levels,
accounting for a deeply structured world with dynamics that play out over
many diff ere nt temporal scales.
We can draw an int ere sti ng parallel between hierarchical models of
the sort in figures 7.12 and 7.13 and learning models of the sort in fig-
ure 7.10. Learning models can be considered hierarchical generative mod-
els, which highlight a separation of timescales between faster inferential
dynamics (updates of beliefs about states) and slower learning dynamics
(updates of beliefs about para meters). The models shown in figures 7.10
and 7.12 may be also combined to arbitrary levels of complexity, wherein
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference in Discrete Time 151
the relationships between variables on diff ere nt levels may themselves be
learned. This permits designing increasingly sophisticated generative mod-
els that address systems-l evel cognitive and neurobiological questions.
7.7 Summary
In this chapter, we saw some of the ways that discrete-t ime generative models
may be constructed to address a range of cognitive and neurobiological prob-
lems, such as perceptual inference, decision-m aking and planning, balancing
exploration and exploitation, parametric and structure learning, and novelty
seeking. This is far from an exhaustive summary of applications of discrete
models in Active Inference, but it serves to illustrate the key princip les of
this sort of modeling. The models outlined above may be combined hierar-
chically, with added priors over para meters, and with context-s ensitive pri-
ors for policies or preferences. Importantly, inference using both s imple and
more complex generative models can always proceed through f ree energy
minimization, which illustrates the generality of the approach. The fact that
diff ere nt aspects of Active Inference become apparent under distinct genera-
tive models (e.g., novelty seeking with priors over model para meters) opens
up the possibility of exploring an open-e nded set of cognitive and biological
probl ems by designing the appropriate generative models.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:ThomasParr,GiovanniPezzulo,KarlJ.Friston
DOI:10.7551/mitpress/12441.001.0001
ISBN(electronic):9780262369978
Publisher:TheMITPress
Published:2022
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025
MIT Press Direct
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided
comments on drafts of this book. The generous work of academic experts is essential
for establishing the authority and quality of our publications. We acknowledge with
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services.
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J.
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior /
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. |
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246584/c005900_9780262369978.pdf by guest on 12 December 2025