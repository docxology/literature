On Solving a Stochastic Shortest-Path Markov
Decision Process as Probabilistic Inference
Mohamed Baioumy, Bruno Lacerda, Paul Duckworth, and Nick Hawes
Oxford Robotics Institute, University of Oxford
{mohamed, bruno, pduckworth, nickh}@robots.ox.ac.uk
Abstract. Previousworkonplanningasactiveinferenceaddressesfinite
horizon problems and solutions valid for online planning. We propose
solving the general Stochastic Shortest-Path Markov Decision Process
(SSP MDP) as probabilistic inference. Furthermore, we discuss online
and offline methods for planning under uncertainty. In an SSP MDP,
the horizon is indefinite and unknown a priori. SSP MDPs generalize
finite and infinite horizon MDPs and are widely used in the artificial
intelligencecommunity.Additionally,wehighlightsomeofthedifferences
betweensolvinganMDPusingdynamicprogrammingapproacheswidely
used in the artificial intelligence community and approaches used in the
active inference community. F
1 Introduction
Acoreprobleminthefieldofartificialintelligence(AI)isbuildingagentscapable
of automated planning under uncertainty. Problems involving planning under
uncertaintyaretypicallyformulatedasaninstanceofaMarkovDecisionProcess
(MDP). At a high level, an MDP comprises 1) a set of world states, 2) a set
of actions, 3) a transition model describing the probability of transitioning to
a new state when taking an action in the current state, and 4) an objective
function(e.g.minimizingcostsoverasequenceoftimesteps).AnMDPsolution
determinestheagent’sactionsateachdecisionpoint.AnoptimalMDPsolution
is one that optimizes the objective function. These are typically obtained using
dynamic programming algorithms 1 [17,26].
Recentworkbasedontheactiveinferenceframework[13]posestheplanning
problem as a probabilistic inference problem. Several papers have been pub-
lished showing connections between active inference and dynamic programming
to solve an MDP [15,8,7]. However, the planning problem being solved in the
two communities is not equivalent.
First,dynamicprogrammingapproachesusedtosolveanMDP,suchaspolicy
iteration, are valid for finite, infinite and indefinite horizons. Indefinite horizons
are finite but of which the length is unknown a priori. For instance, consider
1 Linear programming approaches are also popular methods for solving MDPs
[2,12,22,9]. Additionally, other methods exist in the reinforcement learning com-
munity such as policy gradient methods [28,14,27].
1202
peS
31
]GL.sc[
1v66850.9012:viXra
2 M. Baioumy et al.
an agent navigating from a starting state to a goal state in a grid world where
the outcome is uncertain (e.g. the 4×4 grid world in Figure 1 shown in the
appendix). Before starting to act in the environment, there is no way for the
agent to know how many time steps it will take to reach the goal. Algorithms
based on dynamic programming, such as policy iteration, are valid for such
settings. They can solve the Stochastic Shortest-Path Markov decision process
(SSP MDP)[17,2]. However, work from active inference is only formulated for
finite horizons [8].
Second, the optimal solution to an SSP MDP is a stationary deterministic
policy [17]. This refers to a mapping from states to actions independent of time.
Computing this optimal policy can be done offline (without interaction with
the environment) or online (while interacting). In the active inference literature
however, solving the planning problem is performed by computing a stochastic
plan (a sequence of actions given the current state). This is only valid during
online planning. Additionally, the solution is only optimal given a certain hori-
zon, which is specified a priori. If the horizon chosen is too short, the agent will
not find a solution to reach the target. If it is too long, the solution will be
sub-optimal.
Themaincontributionofthispaperispresentinganovelalgorithmtosolvea
Stochastic Shortest-Path Markov Decision Process using probabilistic inference.
ThisisanMDPwithanindefinitehorizon.Additionally,highlightingtheseveral
gaps between solving an MDP in the AI community and the active inference
community.
Section 2 discusses the SSP MDP and section 3 presets an approach for
solving an SSP MDP as probabilistic inference. The equivalence between the
twomethodsisshowninSection4.1.Furthermore,thedifferencebetweenworld
states and temporal state is highlighted in Section 4.2. Policies, plans and prob-
abilistic plans are discussed in Section 4.3. Finally, a discussion on online vs
offline planning can be found in Section 5.
2 Stochastic Shortest Path MDP
AnSSPMDPisdefinedasatupleM=(S,A,C,T,G).S isthesetofstates,Ais
thesetofactions,C :S×A×S →Risthecostfunction,andT :S×A×S →[0,1]
isthetransitionfunction.G⊂S isthesetofgoalstates.Eachgoalstates ∈G
g
isabsorbingandincurszerocost.Theexpectedcostofapplyingactionainstate
sisC¯(s,a)= (cid:80) T (s,a,s(cid:48))·C(s,a,s(cid:48)).Theminimumexpectedcostatstate
s(cid:48)∈S
s is C¯∗(s)=min C¯(s,a). A policy maps a state to a distribution over action
a∈A
choices. A policy is deterministic if it chooses a single action at each step. A
policy π is proper if it reaches s ∈G starting from any s with probability 1. In
g
an SSP MDP, the following assumptions are made [17]: a) there exists a proper
policy, and b) every improper policy incurs infinite cost at all states where it is
improper.
Thegoalistofindtheanoptimal policy π∗ withtheminimumexpectedcost
C¯∗(s) and can be computed as
SSP MDP as Probabilistic Inference 3
(cid:34) (cid:35)
(cid:88)
π∗(s)=argmin T (s,a,s(cid:48))[C(s,a,s(cid:48))+V∗(s(cid:48))] .
a∈A
s(cid:48)∈S
V∗(s) is referred to as the optimal value for a state s and is defined as:
(cid:34) (cid:35)
(cid:88)
V∗(s)=min T (s,a,s(cid:48))[C(s,a,s(cid:48))+V∗(s(cid:48))] .
a∈A
s(cid:48)∈S
Crucially, the optimal policy π∗ corresponding to the optimal value func-
tion is Markovian (only dependant on the current state) and deterministic [17].
Solving an SSP MDP means finding a policy that minimizes expected cost, as
opposedtoonethatmaximizesreward.Thisdifferenceispurelysemanticasthe
problems are dual. We can define a reward function R=−C and move to a re-
wardmaximizationformulation.Amorefundamentaldistinctionisthepresence
of a special set of (terminal) goal states, in which staying forever incurs no cost.
Solving an SSP MDP can be done using standard dynamic programming
algorithms such as policy iteration. Policy iteration can be divided in two steps,
policyevaluationandimprovement.Inpolicyevaluation,forapolicyπ,thevalue
function V (s) is recursively evaluated until convergence as
π
(cid:88)
V (s)← T (s,π(s),s(cid:48))[C(s,π(s),s(cid:48))+V (s(cid:48))].
π π
s(cid:48)∈S
In the policy improvement step, the state-action value function Q(s,a) is
computed as:
(cid:88)
Q(s,a)= T(s,a,s(cid:48))[C(s,a,s(cid:48))+V(s(cid:48))].
s(cid:48)∈S
Then we compute a new policy as π(cid:48) = argmin Q(s,a) for every state
a∈A
in S. Iterating between these two steps guarantees convergence to an optimal
policy.
Properties of an SSP MDP. AnSSPMDPcanbeshowntogeneralizefinite,
infinite and indefinite horizon MDPs [3,17]. Thus algorithms valid for an SSP
MDParealsovalidforthefiniteandinfinitehorizonMDPs.Additionally,itcan
be proven that each SSP MDP has an optimal deterministic policy independent
of time. Therefore, the claims made in [8] about active inference being more
general since it computes stochastic policies are unjustified when solving an
MDP. However, these results do not hold in partially observable cases or in
the presence of uncertain models. But in the SSP MDP defined above (which
is commonly used in the AI community), there always exists a deterministic
optimal policy. Note that there is an infinite number of stochastic policies but
onlyafinitenumberofdeterministicpolicies(|S||A|).Thisgreatlyspeedsupthe
algorithms while still guaranteeing optimality.
4 M. Baioumy et al.
3 Solving an SSP MDP as probabilistic inference
In this section we discuss a novel approach for solving an SSP MDP as prob-
abilistic inference. We use an inference algorithm that exactly solves an SSP
MDP as defined in the previous section. This approach is inspired by work from
[32,31]whichsolvesanMDPwithanindefinitehorizon.Thisapproachhasbeen
successfullyappliedtosolveproblemsofplanningunderuncertainty,e.g.[18,30].
3.1 Definitions
ThedefinitionofanSSPMDPincludesasetofworldstatesS andactionsA.In
probabilistic inference we instead reason about temporal states and actions. A
temporalstates isarandomvariabledefinedoverallworldstates.Conceptually
t
it represents the state that the agent will visit at the time-step t. For the grid
world in Figure 1, there are 16 world states but the number of temporal states
is unknown a priori since the horizon is unknown.
The transition probability is defined as a probability distribution over tem-
poral states and actions as P(s |a ,s ). If the random variables are fixed
t+1 t t
to specific world states i and j and an action a, the transition probability
P(s = j|a = a,s = i) would be equivalent to the transition function T
t+1 t t
defined for an SSP MDP. The probability of taking a certain action in a state is
parameterized by a policy π as P(a =a|s =i;π)=π . This policy is defined
t t ai
exactly the same as in the case of an SSP MDP.
The cost function P(c |s ,a ) is defined differently. The temporal cost vari-
t t t
ables c are defined as binary random variables c ∈{0,1}. Translating an arbi-
t t
trary cost function to temporal costs can be done by scaling the cost function
C(s,a) (as defined in the previous section) between the minimum cost (min(C))
and maximum cost (max(C)) as:
C(a,s)−min(C)
P(c =1|a =a,s =s)= .
t t t max(C)−min(C)
Any expression with P(c =1) can be though of as ‘the probability of a cost
t
beingmaximal’.Thus,theprobabilityofacostbeingmaximalgivenastateand
an action is P(c = 1 | a = a,s = s). Now we can reason about the highest
t t t
possiblecostforastatesandactionaasonewhereP(c =1|a =a,s =s)=1
t t t
and the lowest possible cost as P(c = 1 | a = a,s = s) = 0. Any other cost
t t t
will have a probability in-between, according to its magnitude.
Finally,wemodelthehorizonasarandomvariable.Thetemporalstatesand
actions are considered up to the end of the horizon T. However, the horizon is
generally unknown. We thus model T itself as a random variable. Combining all
this information we can define the SSP MDP using a probabilistic model.
3.2 Mixture of finite MDPs
In this section we define the SSP MDP in terms of a mixture of finite MDPs
with only a final cost variable. Given every horizon (for instance T = 1) the
SSP MDP as Probabilistic Inference 5
finite MDP can be given as P (c,s ,a |T;π). Note that we dropped the
0:T 0:T
time-index for c since there is only one cost variable now. This model can be
t
factorized as
P (c,s ,a |T;π)=P (c|a ,s )P (a |s ;π)
0:T 0:T T T 0 0
P (s )·
(cid:81)T
P (a |s ;π)P (s |a ,s )
0 t=1 t t t t−1 t−1
To reason about the full MDP, we consider the mixture model of the joint
given by the joint probability distribution
P (c,s ,a ,T;π)=P (c,s ,a |T;π)P(T)
0:T 0:T 0:T 0:T
where P(T) is a prior over the total time, which we choose to be a flat prior
(uniform distribution).
3.3 Computing an optimal policy
Our objective is to find a policy that minimizes the expected cost. Similarly
to policy iteration, we do not assume any knowledge about the initial state.
Expectation-Maximization2 can be used to find the optimal parameters of our
model:thepolicyπ.TheE-stepwill,foragivenπ,computeaposterioroverstate-
action sequences. The M-step then adapts the model parameters π to optimize
the expected likelihood with respect to the quantities calculated in the E-step.
E-step: a backwards pass in all finite MDPs. Weusethesimplernotation
p(j |a,i)≡P (s =j |a =a,s =i)andp(j |i;π)≡P (s =j |s =i;π)=
t+1 t t t+1 t
(cid:80)
p(j |a,i)π . Further, as a ‘base’ for backward propagation, we define
a ai
(cid:88)
β (i)=P (c=1|x =i;π)= P (c=1|a =a,x =i)π .
0 T T T ai
a
Thisistheimmediatecostwhenfollowingapolicyπ.Itistheexpectedcostif
there is only one time-step remaining. Then, we can recursively compute all the
other backward messages. We use the index τ to indicate a backwards counter.
This means that τ +t=T, where T is total (unknown) horizon length. This is
computed as
(cid:88)
β (i)=P (c=1|x =i;π)= p(j |i;π)β (j).
τ T−τ τ−1
j
Intuitively,thebackwardmessagesaretheexpectedcostifoneincursacostat
thelasttimesteponly.So,β ,istheexpectedcostiftheagentfollowsthepolicy
2
π for two time-steps and only incurs a cost after that. Using these messages, we
can compute a value function dependent on time, actions and states given as:
2 An expectation-maximization algorithm can be viewed as performing free-energy
minimization [21,16]. In the E-step, the free-energy is computed and the M-step
updates the parameters to minimize the free-energy.
6 M. Baioumy et al.
q (a,i)=P (c=1|a =a,s =i,T =t+τ;π)
τ t t
(cid:26)(cid:80)
p(j |i,a)β (j) τ >1
= j τ−1
P (c=1|a =a,s =i)τ =0.
T T
Marginalizing out time, we get the state-action value-function
1 (cid:88)
P (c=1|a =a,s =i;π)= P(T =t+τ)q (a,i)
t t C τ
τ
where C is a normalization constant. This quantity is the probability of getting
a maximum cost given a state and action. It is similar to the Q(s,a) function
computed in policy iteration.
M-step: the policy improvement step. The standard M-step in an EM-
algorithm maximizes the expected complete log-likelihood with respect to the
new parameters π(cid:48). Given that the optimal policy for an MDP is deterministic,
agreedyM-stepcanbeused.However,ourgoalistominimizethelog-likelihood
inthiscaseasitreferstoatheprobabilityofreceivingamaximalcost.Thiscan
done as
π(cid:48) =argmin(P (c=1|a t =a,s t =i;π)) (1)
a
This update converges much faster than in a standard M-step. Here an M-
step can be used to to obtain a stochastic policy. However, this is unnecessary
since the optimal policy is deterministic. Note that there is an infinite number
of stochastic policies but a finite number of deterministic ones. In conclusion, a
greedy M-step is faster to converge but still guarantees an optimal policy.
4 Connections between the two views
4.1 Exact relationship between policy iteration and planning as
probabilistic inference
Themessagesβ computedduringbackwardpropagationareexactlyequaltothe
value functions for a single MDP of finite time. The full value function is can
therefore be written as the sum of the βs,
(cid:88)
V (i)= β (i)
π T
T
since the prior over time P(T) is a uniform prior. If P(T) is not a uniform
distribution,thiswouldresultinamixtureratherthanasum.Thesameapplies
to the relationship between the Q-value function:
(cid:88)
Q (a,i)= q (a,i).
π T
T
SSP MDP as Probabilistic Inference 7
Hence, the E-step essentially performs a policy evaluation which yields the
classical value function. Given this relation to policy evaluation, the M-step
performsanoperationexactlyequivalenttostandardpolicyimprovement.Thus,
the EM-algorithm using exact inference is equivalent to Policy Iteration but
computes the necessary quantities differently.
Oneunansweredquestioniswhentostopcomputingthebackwardmessages.
In [32] messages are computed up to a number T . From this perspective, the
max
planningasinferencealgorithmpresentedisequivalenttotheso-calledtruncated
policy iteration algorithm as opposed to the more common (cid:15)-greedy version.
In the policy evaluation step, one iterates through the state space to update
the value v (s) for every state until a termination criterion is met. An (cid:15)-greedy
π
criterion means that we stop iterating though the state space once the maxi-
mum difference in V (s) for any s is smaller than a positive small number (cid:15).
π
In truncated policy iteration, however, we iterate through the state space T
max
timesandthenperformthepolicyimprovementstep.Theprobabilisticinference
algorithm presented in this paper is equivalent to truncated policy iteration if
we restrict the maximum number of β messages to be computed.
4.2 World states vs temporal states
In dynamic programming, one reasons over the world states. In the grid world
exampleinFigure1,thisreferstoagridcell.Thisgridworldhas16worldstates.
In probabilistic inference, one reasons about a temporal state. This is a random
variable over all world states. The number of temporal states is dependent on
howmanytime-stepstheagentsactsintheenvironment(whichisoftenunknown
beforehand). An illustration of the difference is given in Fig. 2.
4.3 Policies, plans and probabilistic plans
A classical planning algorithm computes a plan: a sequence of actions. An algo-
rithm like A* or Dijkstra’s algorithm [5] can be used to find the optimal path
from a stating state to a goal state, given a deterministic world. Crucially, this
solutioncanbecomputedoffline(withoutinteractionswiththeenvironment).In
a stochastic world, this does not work since the agent can not predict in which
states it will end up. However, one can use deterministic planning algorithms
for stochastic environments if the path is re-planned online at every time-step.
Determinization-based methods have found success in solving planning under
uncertainity problems such as the famous FF-replan algorithm [34]
Active inference approaches computes a probabilistic plan. The active infer-
ence literature calls this a policy; however, we use a different term to avoid
confusion.3 In active inference, the agents computes a finite plan while interact-
ing with the environment. However, rather than assuming a deterministic world
3 The distinction between a plan and a policy when using active inference has been
brieflydiscussedin[20].Additionally,othermethodscomputingplansasprobabilis-
tic inference have been proposed before active inference in [1,33]
8 M. Baioumy et al.
(likeFF-replan[34]),theprobabilitiesaretakenintoaccount.Thiscanbeshown
to compute the optimal solution to an MDP (when planning online). We thus
refer to it as a probabilistic plan, a plan that was computed while taking the
transition probabilities into account.
Finally,apolicyisamappingfromstatestoactions,i.e.theagenthasapre-
ferredactiontotakeforeverystate.Policiescanbestochasticortime-dependent;
however, for an SSP MDP the optimal policy is deterministic and independent
of time. An agent can compute a policy offline and use it online without need-
ing any additional computation while interacting with the environment. The
difference between a plan and policy is illustrated in Fig. 1.
To summarize, a plan or a probabilistic plan can only be used for online
planning. Since the outcome of an action is inherently uncertain. Probabilistic
plans (as used in active inference) find an optimal solution when used to plan
online. A policy also provides an optimal solution and can be computed offline
or online.
5 Discussion
In this paper we present a novel approach to solve a stochastic shortest path
Markov decision process (SSP MDP) as probabilistic inference. The SSP MDP
generalizes many models, including finite and infinite MDPs. Crucially, the dy-
namicprogrammingalgorithms(suchaspolicyiteration)classicallyusedtosolve
an SSP MDP are valid for indefinite horizons (finite but of unknown length);
this is not the case for active inference approaches.
The exact connections between solving an MDP using policy iteration and
the presented algorithm are discussed. Afterwards, we discussed the gap be-
tween solving an MDP in active inference and the approaches in the artificial
intelligence community. This included the difference between world states and
temporal states, the difference between plans, probabilistic plans and policies.
An interesting question now is, which approach is more appropriate? This de-
pends on the problem at hand and whether it can be solved online or offline.
Online and offline planning. AsdiscussedinSection4.3,apolicyismapping
fromstatestoactionsandcanbeusedforofflineandonlineplanning.Computing
a policy is somewhat computationally expensive; however, a look-up is very
cheap. Thus if one operates in an environment where the transition and cost
function do not change, it is best to compute an optimal policy offline then
useitonline(whileinteractingwiththeenvironment).Thisisthecaseformany
planningandschedulingproblems,suchasasetofelevatorsoperatinginsync[6],
task-level planning in robotics [19], multi-objective planning [23,11] and playing
games [4,25]. The challenges in these problems are often that the state-space is
incredibly large and thus approximations are needed. However, the problem is
fully observable and the cost and transition models are static; the rules of chess
do not change half way, for instance.
SSP MDP as Probabilistic Inference 9
Ifthetransitionorcostfunctionsvarywhileinteractingwiththeenvironment
(e.g. [24,10]), an offline solution is not optimal. In this case, the agent can plan
online by re-evaluating a policy or computing probabilistic plans (as done in
active inference). Computing the latter is cheaper and requires less memory.
This is because a probabilistic plan is a distribution over actions p(a ) up to a
t
time horizon T while a (finite policy) is a conditional distribution p(a |s ) over
t t
all world states in S. For any time-step, the posterior over the action is related
(cid:80)
to the policy such that p(a )= p(a |s )p(s ).
t s t t t
Considertheworkin[29,24].Inbothcasesarobotoperatesinanenvironment
susceptibletochanges.Iftheenvironmentchanges,theagentcaneasilyconstruct
anewmodelbyvaryingthecostortransitionfunctionbutneedstorecomputea
solution. In [29] the authors recompute a policy at every time-step while in [24]
a probabilistic plans is computed using active inference. Since in both cases the
solution is recomputed at every time-step, active inference is preferred since it
requires less memory and can be computationally cheaper. On the other hand,
if the environment only changes occasionally, computing a policy might remain
preferable.
To conclude, if the transition and cost functions (T and C) are static, it is
preferable to compute a policy offline. If T and C change occasionally, one may
stillcomputeanofflinepolicyandrecomputeapolicyonlywhenachangeoccurs.
However, if the environment is dynamic, computing a probabilistic plan (using
active inference) is preferable to recomputing a policy at every time-step.
References
1. Attias, H.: Planning by probabilistic inference. In: AISTATS (2003)
2. Bertsekas, D.P., Tsitsiklis, J.N.: An analysis of stochastic shortest path problems.
Mathematics of Operations Research 16(3), 580–595 (1991)
3. Bertsekas, D.P., Tsitsiklis, J.N.: Neuro-dynamic programming: an overview. In:
Proceedings of 1995 34th IEEE conference on decision and control. vol. 1, pp.
560–564. IEEE (1995)
4. Campbell, M., Hoane, A.J., Hsu, F.: Deep blue. Artif. Intell. 134, 57–83 (2002)
5. Cormen,T.H.,Leiserson,C.E.,Rivest,R.L.,Stein,C.:Introductiontoalgorithms.
MIT press (2009)
6. Crites, R.H., Barto, A.G., et al.: Improving elevator performance using reinforce-
ment learning. Advances in neural information processing systems pp. 1017–1023
(1996)
7. DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.,Friston,K.:Activeinfer-
ence on discrete state-spaces: a synthesis. arXiv preprint arXiv:2001.07203 (2020)
8. DaCosta,L.,Sajid,N.,Parr,T.,Friston,K.,Smith,R.:Therelationshipbetween
dynamicprogrammingandactiveinference:Thediscrete,finite-horizoncase.arXiv
preprint arXiv:2009.08111 (2020)
9. d’Epenoux, F.: A probabilistic production and inventory problem. Management
Science 10(1), 98–108 (1963)
10. Duckworth, P., Lacerda, B., Hawes, N.: Time-bounded mission planning in time-
varying domains with semi-mdps and gaussian processes (2021)
10 M. Baioumy et al.
11. Etessami, K., Kwiatkowska, M., Vardi, M.Y., Yannakakis, M.: Multi-objective
modelcheckingofmarkovdecisionprocesses.In:InternationalConferenceonTools
andAlgorithmsfortheConstructionandAnalysisofSystems.pp.50–65.Springer
(2007)
12. Forejt,V.,Kwiatkowska,M.,Norman,G.,Parker,D.:Automatedverificationtech-
niquesforprobabilisticsystems.In:Internationalschoolonformalmethodsforthe
design of computer, communication and software systems. pp. 53–113. Springer
(2011)
13. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active in-
ference: a process theory. Neural computation 29(1), 1–49 (2017)
14. Grondman, I., Busoniu, L., Lopes, G.A., Babuska, R.: A survey of actor-critic
reinforcementlearning:Standardandnaturalpolicygradients.IEEETransactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42(6),
1291–1307 (2012)
15. Kaplan, R., Friston, K.J.: Planning and navigation as active inference. Biological
cybernetics 112(4), 323–343 (2018)
16. Koller,D.,Friedman,N.:Probabilisticgraphicalmodels:principlesandtechniques.
MIT press (2009)
17. Kolobov,A.:PlanningwithMarkovDecisionProcesses:AnAIPerspective,vol.6.
Morgan & Claypool Publishers (2012)
18. Kumar, A., Zilberstein, S., Toussaint, M.: Probabilistic inference techniques for
scalablemultiagentdecisionmaking.JournalofArtificialIntelligenceResearch53,
223–270 (2015)
19. Lacerda, B., Faruq, F., Parker, D., Hawes, N.: Probabilistic planning with formal
performance guarantees for mobile service robots. The International Journal of
Robotics Research 38(9), 1098–1123 (2019)
20. Millidge,B.,Tschantz,A.,Seth,A.K.,Buckley,C.L.:Ontherelationshipbetween
active inference and control as inference. In: International Workshop on Active
Inference. pp. 3–11. Springer (2020)
21. Murphy, K.P.: Machine learning: a probabilistic perspective. MIT press (2012)
22. Nazareth, J.L., Kulkarni, R.B.: Linear programming formulations of markov deci-
sion processes. Operations research letters 5(1), 13–16 (1986)
23. Painter,M.,Lacerda,B.,Hawes,N.:Convexhullmonte-carlotree-search.In:Pro-
ceedings of the International Conference on Automated Planning and Scheduling.
vol. 30, pp. 217–225 (2020)
24. Pezzato, C., Hernandez, C., Wisse, M.: Active inference and behavior trees for
reactiveactionplanningandexecutioninrobotics.arXivpreprintarXiv:2011.09756
(2020)
25. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanc-
tot, M., Sifre, L., Kumaran, D., Graepel, T., et al.: Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint
arXiv:1712.01815 (2017)
26. Sutton, R.S., Barto, A.G., et al.: Introduction to reinforcement learning, vol. 135.
MIT press Cambridge (1998)
27. Sutton,R.S.,McAllester,D.A.,Singh,S.P.,Mansour,Y.:Policygradientmethods
for reinforcement learning with function approximation. In: Advances in neural
information processing systems. pp. 1057–1063 (2000)
28. Thomas, P.S., Brunskill, E.: Policy gradient methods for reinforcement learn-
ing with function approximation and action-dependent baselines. arXiv preprint
arXiv:1706.06643 (2017)
SSP MDP as Probabilistic Inference 11
29. Tomy,M.,Lacerda,B.,Hawes,N.,Wyatt,J.L.:Batterychargeschedulinginlong-
life autonomous mobile robots via multi-objective decision making under uncer-
tainty. Robotics and Autonomous Systems 133, 103629 (2020)
30. Toussaint,M.,Charlin,L.,Poupart,P.:Hierarchicalpomdpcontrolleroptimization
by likelihood maximization. In: UAI. vol. 24, pp. 562–570 (2008)
31. Toussaint, M., Harmeling, S., Storkey, A.: Probabilistic inference for solving (po)
mdps. University of Edinburgh, School of Informatics Research Report EDI-INF-
RR-0934 (2006)
32. Toussaint, M., Storkey, A.: Probabilistic inference for solving discrete and contin-
uous state markov decision processes. In: Proceedings of the 23rd international
conference on Machine learning. pp. 945–952. ACM (2006)
33. Verma,D.,Rao,R.P.:Goal-basedimitationasprobabilisticinferenceovergraphical
models. In: Advances in neural information processing systems. pp. 1393–1400
(2006)
34. Yoon, S.W., Fern, A., Givan, R.: Ff-replan: A baseline for probabilistic planning.
In: ICAPS. vol. 7, pp. 352–359 (2007)
A Appendix: Illustrations
Fig.1.Anillustration ofa 4×4grid world (right).The initialstate isblueand goal
state is green. An illustration for a policy (left) and a plan (middle).
Fig.2. Annotated world states (left) and a posterior over a temporal state (right).