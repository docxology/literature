Planning with Information-Processing Constraints and
Model Uncertainty in Markov Decision Processes
JordiGrau-Moya1,2,3,FelixLeibfried1,2,3,TimGenewein1,2,3,andDanielA.Braun1,2
1 MaxPlanckInstituteforIntelligentSystems,Tu¨bingen,Germany,
jordi.grau@tuebingen.mpg.de,
2 MaxPlanckInstituteforBiologicalCybernetics,Tu¨bingen,Germany,
3 GraduateTrainingCentreforNeuroscience,Tu¨bingen,Germany
Abstract. Information-theoreticprinciplesforlearningandactinghavebeenpro-
posedtosolveparticularclassesofMarkovDecisionProblems.Mathematically,
such approaches are governed by a variational free energy principle and allow
solvingMDPplanningproblemswithinformation-processingconstraintsexpressed
in terms of a Kullback-Leibler divergence with respect to a reference distribu-
tion.HereweconsiderageneralizationofsuchMDPplannersbytakingmodel
uncertainty into account. As model uncertainty can also be formalized as an
information-processing constraint, we can derive a unified solution from a sin-
gle generalized variational principle. We provide a generalized value iteration
schemetogetherwithaconvergenceproof.Aslimitcases,thisgeneralizedscheme
includesstandardvalueiterationwithaknownmodel,BayesianMDPplanning,
androbustplanning.Wedemonstratethebenefitsofthisapproachinagridworld
simulation.
Keywords: boundedrationality,modeluncertainty,robustness,planning,Markov
DecisionProcesses
1 Introduction
The problem of planning in Markov Decision Processes was famously addressed by
Bellman who developed the eponymous principle in 1957 [1]. Since then numerous
variantsofthisprinciplehaveflourishedintheliterature.Hereweareparticularlyinter-
estedinageneralizationoftheBellmanprinciplethattakesinformation-theoreticcon-
straintsintoaccount.IntherecentpasttherehasbeenaspecialinterestintheKullback-
Leiblerdivergenceasaconstrainttolimitdeviationsoftheactionpolicyfromaprior.
This can be interesting in a number of ways. Todorov [2,3], for example, has trans-
formedthegeneralMDPproblemintoarestrictedproblemclasswithoutexplicitaction
variables,wherecontroldirectlychangesthedynamicsoftheenvironmentandcontrol
costsaremeasuredbytheKullback-Leiblerdivergencebetweencontrolledanduncon-
trolleddynamics.ThissimplificationallowsmappingtheBellmanrecursiontoalinear
algebra problem. This approach can also be be generalized to continuous state spaces
leading to path integral control [4,5]. The same equations can also be interpreted in
termsofboundedrationaldecision-makingwherethedecision-makerhaslimitedcom-
putational resources that allow only limited deviations from a prior decision strategy
6102
rpA
7
]IA.sc[
1v08020.4061:viXra
2 Grau-Moyaetal.
(measured by the Kullback-Leiber divergence in bits) [6]. Such a decision-maker can
also be instantiated by a sampling process that has restrictions in the number of sam-
ples it can afford [7]. Disregarding the possibility of a sampling-based interpretation,
theKullback-Leiblerdivergenceintroducesacontrolinformationcostthatisinteresting
initsownrightwhenformalizingtheperceptionactioncycle[8].
While the above frameworks have led to interesting computational advances, so
far they have neglected the possibility of model misspecification in the MDP setting.
Model misspecification or model uncertainty does not refer to the uncertainty arising
due to the stochastic nature of the environment (usually called risk-uncertainty in the
economicliterature),butreferstotheuncertaintywithrespecttothelatentvariablesthat
specifytheMDP.InBayes-AdaptiveMDPs[9],forexample,theuncertaintyoverthe
latentparametersoftheMDPisexplicitlyrepresented,suchthatnewinformationcan
beincorporatedwithBayesianinference.However,Bayes-AdaptiveMDPsarenotro-
bustwithrespecttomodelmisspecificationandhavenoperformanceguaranteeswhen
planning with wrong models [10]. Accordingly, there has been substantial interest in
developing robust MDP planners [11,12,13]. One way to take model uncertainty into
account is to bias an agent’s belief model from a reference Bayesian model towards
worst-case scenarios; thus avoiding disastrous outcomes by not visiting states where
thetransitionprobabilitiesarenotknown.Conversely,thebeliefmodelcanalsobebi-
asedtowardsbest-casescenariosasameasuretodriveexploration—alsoreferredinthe
literatureasoptimisminfaceofuncertainty[14,15].
When comparing the literature on information-theoretic control and model uncer-
tainty,itisinterestingtoseethatsomenotionsofmodeluncertaintyfollowexactlythe
same mathematical principles as the principles of relative entropy control [3]. In this
paper we therefore formulate a unified and combined optimization problem for MDP
planningthattakesboth,modeluncertaintyandboundedrationalityintoaccount.This
newoptimizationproblemcanbesolvedbyageneralizedvalueiterationalgorithm.We
provide a theoretical analysis of its convergence properties and simulations in a grid
world.
2 BackgroundandNotation
IntheMDPsettingtheagentattimetinteractswiththeenvironmentbytakingaction
a ∈ A while instate s ∈ S. Thenthe environmentupdates thestate ofthe agentto
t t
s ∈ S accordingtothetransitionprobabilitiesT(s |a ,s ).Aftereachtransition
t+1 t+1 t t
the agent receives a reward Rst+1 ∈ R that is bounded. For our purposes we will
st,at
consider A and S to be finite. The aim of the agent is to choose its policy π(a|s) in
ordertomaximizethetotaldiscountedexpectedrewardorvaluefunctionforanys∈S
(cid:34)T−1 (cid:35)
(cid:88)
V∗(s)=max lim E γtRst+1
π T→∞
st,at
t=0
with discount factor 0 ≤ γ < 1. The expectation is over all possible trajectories ξ =
s ,a ,s ... ofstateandactionpairsdistributedaccordingtop(ξ) =
(cid:81)T−1π(a
|s )
0 0 1 t=0 t t
T(s |a ,s ). It can be shown that the optimal value function satisfies the following
t+1 t t
Information-ProcessingConstraintsandModelUncertaintyinMDPs 3
recursion
V∗(s)=max (cid:88) π(a|s)T(s(cid:48)|a,s) (cid:104) Rs(cid:48) +γV∗(s(cid:48)) (cid:105) . (1)
s,a
π
a,s(cid:48)
Atthispointtherearetwoimportantimplicitassumptions.Thefirstisthatthepol-
icyπ canbechosenarbitrarilywithoutanyconstraintswhich,forexample,mightnot
be true for a bounded rational agent with limited information-processing capabilities.
The second is that the agent needs to know the transition-model T(s(cid:48)|a,s), but this
model is in practice unknown or even misspecified with respect to the environment’s
truetransition-probabilities,speciallyatinitialstagesoflearning.Inthefollowing,we
explainhowtoincorporatebothboundedrationalityandmodeluncertaintyintoagents.
2.1 Information-TheoreticConstraintsforActing
Consider a one-step decision-making problem where the agent is in state s and has
to choose a single action a from the set A to maximize the reward Rs(cid:48) , where s(cid:48)
s,a
is the next the state. A perfectly rational agent selects the optimal action a∗(s) =
argmax (cid:80) T(s(cid:48)|a,s)Rs(cid:48) . However, a bounded rational agent has only limited re-
a s(cid:48) s,a
sources to find the maximum of the function (cid:80) T(s(cid:48)|a,s)Rs(cid:48) . One way to model
s(cid:48) s,a
such an agent is to assume that the agent has a prior choice strategy ρ(a|s) in state
s before a deliberation process sets in that refines the choice strategy to a posterior
distribution π(a|s) that reflects the strategy after deliberation. Intuitively, because the
deliberation resources are limited, the agent can only afford to deviate from the prior
strategybyacertainamountofinformationbits.Thiscanbequantifiedbytherelative
entropyD (π||ρ)= (cid:80) π(a|s)logπ(a|s) thatmeasurestheaverageinformationcost
KL a ρ(a|s)
ofthepolicyπ(a|s)usingthesourcedistributionρ(a|s).Foraboundedrationalagent
thisrelativeentropyisboundedbysomeupperlimitK.Thus,aboundedrationalagent
hastosolveaconstrainedoptimizationproblemthatcanbewrittenas
max
(cid:88)
π(a|s)
(cid:88) T(s(cid:48)|a,s)Rs(cid:48)
s.t. D (π||ρ)≤K
s,a KL
π
a s(cid:48)
Thisproblemcanberewrittenasanunconstrainedoptimizationproblem
F∗(s)=max (cid:88) π(a|s) (cid:88) T(s(cid:48)|a,s)Rs(cid:48) − 1 D (π||ρ) (2)
π s,a α KL
a s(cid:48)
=
1
log
(cid:88) ρ(a|s)eα(cid:80) s(cid:48)T(s(cid:48)|a,s)R
s
s
,
(cid:48)
a. (3)
α
a
where F∗ is a free energy that quantifies the value of the policy π by trading off the
average reward against the information cost. The optimal strategy can be expressed
analyticallyinclosed-formas
π∗(a|s)=
ρ(a|s)eα(cid:80) s(cid:48)T(s(cid:48)|a,s)R
s
s
,
(cid:48)
a
Z (s)
α
4 Grau-Moyaetal.
(cid:16) (cid:17)
with partition sum Z (s) = (cid:80) ρ(a|s)exp α (cid:80) T(s(cid:48)|a,s)Rs(cid:48) . Therefore, the
α a s(cid:48) s,a
maximumoperatorin(2)canbeeliminatedandthefreeenergycanberewrittenasin
(3).TheLagrangemultiplierαquantifiestheboundednessoftheagent.Bysettingα→
∞werecoveraperfectlyrationalagentwithoptimalpolicyπ∗(a|s) = δ(a−a∗(s)).
Forα=0theagenthasnocomputationalresourcesandtheagent’soptimalpolicyisto
actaccordingtothepriorπ∗(a|s)=ρ(a|s).Intermediatevaluesofαleadtoaspectrum
ofboundedrationalagents.
2.2 Information-TheoreticConstraintsforModelUncertainty
InthefollowingweassumethattheagenthasamodeloftheenvironmentT (s(cid:48)|a,s)
θ
that depends on some latent variables θ ∈ Θ. In the MDP setting, the agent holds a
beliefµ(θ|a,s)regardingtheenvironmentaldynamicswhereθisaunitvectoroftran-
sitionprobabilitiesintoallpossiblestatess(cid:48).Whileinteractingwiththeenvironmentthe
agentcanincorporatenewdatabyformingtheBayesianposteriorµ(θ|a,s,D),where
D is the observed data. When the agent has observed an infinite amount of data (and
assumingθ∗(a,s)∈Θ)thebeliefwillconvergetothedeltadistributionµ(θ|s,a,D)=
δ(θ−θ∗(a,s))andtheagentwillactoptimallyaccordingtothetruetransitionprobabil-
ities,exactlyasinordinaryoptimalchoicestrategieswithknownmodels.Whenacting
underalimitedamountofdatatheagentcannotdeterminethevalueofanactionawith
thetruetransitionmodelaccordingto (cid:80) T(s(cid:48)|a,s)Rs(cid:48) ,butitcanonlydeterminean
s(cid:48) s,a
expectedvalueaccordingtoitsbeliefs (cid:82) µ(θ|a,s) (cid:80) T (s(cid:48)|a,s)Rs(cid:48) .
θ s(cid:48) θ s,a
The Bayesian model µ can be subject to model misspecification (e.g. by having a
wronglikelihoodorabadprior)andthustheagentmightwanttoallowdeviationsfrom
itsmodeltowardsbest-case(optimisticagent)orworst-case(pessimisticagent)scenar-
iosuptoacertainextent,inordertoactmorerobustlyortoenhanceitsperformancein
a friendly environment [16]. Such deviations can be measured by the relative entropy
D (ψ|µ) between the Bayesian posterior µ and a new biased model ψ. Effectively,
KL
thisallowsformathematicallyformalizingmodeluncertainty,bynotonlyconsidering
the specified model but all models within a neighborhood of the specified model that
deviatenomorethanarestrictednumberofbits.Then,theeffectiveexpectedvalueof
anactionawhilehavinglimitedtrustintheBayesianposteriorµcanbedeterminedfor
thecaseofoptimisticdeviationsas
F∗(a,s)=max (cid:90) ψ(θ|a,s) (cid:88) T (s(cid:48)|a,s)Rs(cid:48) − 1 D (ψ||µ) (4)
ψ θ s(cid:48) θ s,a β KL
forβ >0,andforthecaseofpessimisticdeviationsas
F∗(a,s)=min (cid:90) ψ(θ|a,s) (cid:88) T (s(cid:48)|a,s)Rs(cid:48) − 1 D (ψ||µ) (5)
ψ θ s(cid:48) θ s,a β KL
forβ <0.Conveniently,bothequationscanbeexpressedasasingleequation
1
F∗(a,s)= logZ (a,s)
β β
Information-ProcessingConstraintsandModelUncertaintyinMDPs 5
(cid:16) (cid:17)
with β ∈ R and Z (s,a) = (cid:82) µ(θ|a,s)exp β (cid:80) T (s(cid:48)|a,s)Rs(cid:48) when inserting
β θ s(cid:48) θ s,a
theoptimalbiasedbelief
(cid:32) (cid:33)
ψ∗(θ|a,s)= 1 µ(θ|a,s)exp β (cid:88) T (s(cid:48)|a,s)Rs(cid:48)
Z (a,s) θ s,a
β
s(cid:48)
intoeitherequation(4)or(5).Byadoptingthisformulationwecanmodelanydegree
oftrustinthebeliefµallowingdeviationtowardsworst-caseorbest-casewith−∞ ≤
β ≤ ∞. For the case of β → −∞ we recover an infinitely pessimistic agent that
considers only worst-case scenarios, for β → ∞ an agent that is infinitely optimistic
andforβ →0theBayesianagentthatfullytrustsitsmodel.
3 ModelUncertaintyandBoundedRationalityinMDPs
Inthissection,weconsideraboundedrationalagentwithmodeluncertaintyinthein-
finitehorizonsettingofanMDP.Inthiscasetheagentmusttakeintoaccountallfuture
rewardsandinformationcosts,therebyoptimizingthefollowingfreeenergyobjective
T−1 (cid:32) (cid:33)
F∗(s)=maxext lim E (cid:88) γt Rst+1 − 1 log ψ(θ t |a t ,s t ) − 1 log π(a t |s t )
π ψ T→∞ st,at β µ(θ t |a t ,s t ) α ρ(a t |s t )
t=0
(6)
where the extremum operator ext can be either max for β > 0 or min for β < 0,
0 < γ < 1 is the discount factor and the expectation E is over all trajectories ξ =
s ,a ,θ ,s ,a ,...a ,θ ,s withdistributionp(ξ)=
(cid:81)T−1π(a
|s )ψ(θ |a ,s )
0 0 0 1 1 T−1 T−1 T t=0 t t t t t
T (s |a ,s ).Importantly,thisfreeenergyobjectivesatisfiesarecursiverelationand
θt t+1 t t
therebygeneralizesBellman’soptimalityprincipletothecaseofmodeluncertaintyand
boundedrationality.Inparticular,equation(6)fulfillstherecursion
(cid:34)
1 π(a|s)
F∗(s)=maxextE − log +
π ψ π(a|s) α ρ(a|s)
(cid:20)
1 ψ(θ|a,s)
E − log +
ψ(θ|a,s) β µ(θ|a,s)
(cid:104)
(cid:105)(cid:21)(cid:35)
E Rs(cid:48) +γF∗(s(cid:48)) . (7)
Tθ(s(cid:48)|a,s) s,a
Applying variational calculus and following the same rationale as in the previous
sections [6], the extremum operators can be eliminated and equation (7) can be re-
expressedas
1 (cid:20) (cid:104) (cid:16) (cid:104) (cid:105)(cid:17)(cid:105)α(cid:21)
F∗(s)= logE E exp βE Rs(cid:48) +γF∗(s(cid:48)) β (8)
α ρ(a|s) µ(θ|a,s) Tθ(s(cid:48)|a,s) s,a
6 Grau-Moyaetal.
because
(cid:20) (cid:21)
1 1 π(a|s)
F∗(s)=maxE logZ (a,s)− log (9)
π π(a|s) β β α ρ(a|s)
(cid:20) (cid:18) (cid:19)(cid:21)
1 α
= logE exp logZ (a,s) , (10)
α ρ(a|s) β β
where
(cid:20) (cid:104) (cid:105) 1 ψ(θ|a,s) (cid:21)
Z (a,s)=extE E Rs(cid:48) +γF∗(s(cid:48)) − log (11)
β ψ ψ(θ|a,s) Tθ(s(cid:48)|a,s) s,a β µ(θ|a,s)
(cid:16) (cid:104) (cid:105)(cid:17)
=E exp βE Rs(cid:48) +γF∗(s(cid:48))
µ(θ|a,s) Tθ(s(cid:48)|a,s) s,a
withtheoptimizingarguments
1 (cid:16) (cid:104) (cid:105)(cid:17)
ψ∗(θ|a,s)= µ(θ|a,s)exp βE Rs(cid:48) +γF(s(cid:48))
Z (a,s) Tθ(s(cid:48)|a,s) s,a
β
(cid:18) (cid:19)
1 α
π∗(a|s)= ρ(a|s)exp logZ (a,s) (12)
Z (s) β β
α
andpartitionsum
(cid:20) (cid:18) (cid:19)(cid:21)
α
Z (s)=E exp logZ (a,s) .
α ρ(a|s) β β
Withthisfreeenergywecanmodelarangeofdifferentagentsfordifferentαand
β.Forexample,bysettingα→∞andβ →0wecanrecoveraBayesianMDPplanner
and by setting α → ∞ and β → −∞ we recover a robust planner. Additionally, for
α→∞andwhenµ(θ|a,s)=δ(θ−θ∗(a,s))werecoveranagentwithstandardvalue
functionwithknownstatetransitionmodelfromequation(1).
3.1 FreeEnergyIterationAlgorithm
Solving the self-consistency equation (8) can be achieved by a generalized version of
value iteration. Accordingly, the optimal solution can be obtained by initializing the
freeenergyatsomearbitraryvalueF andapplyingavalueiterationschemeBi+1F =
BBiF wherewedefinetheoperator
(cid:34)
1 π(a|s)
BF(s)=maxextE − log +
π ψ π(a|s) α ρ(a|s)
(cid:20)
1 ψ(θ|a,s)
E − log +
ψ(θ|a,s) β µ(θ|a,s)
(cid:104)
(cid:105)(cid:21)(cid:35)
E Rs(cid:48) +γF(s(cid:48)) (13)
Tθ(s(cid:48)|a,s) s,a
Information-ProcessingConstraintsandModelUncertaintyinMDPs 7
withB1F =BF,whichcanbesimplifiedto
1 (cid:20) (cid:104) (cid:16) (cid:104) (cid:105)(cid:17)(cid:105)α(cid:21)
BF(s)= logE E exp βE Rs(cid:48) +γF(s(cid:48)) β
α ρ(a|s) µ(θ|a,s) Tθ(s(cid:48)|a,s) s,a
InAlgorithm(1)weshowthepseudo-codeofthisgeneralizedvalueiterationscheme.
Givenstate-dependentpriorpoliciesρ(a|s)andtheBayesianposteriorbeliefsµ(θ|a,s)
and the values of α and β, the algorithm outputs the equilibrium distributions for the
actionprobabilitiesπ(a|s),thebiasedbeliefsψ(θ|a,s)andestimatesofthefreeenergy
value function F∗(s). The iteration is run until a convergence criterion is met. The
convergenceproofisshowninthenextsection.
Algorithm1:Iterativealgorithmsolvingtheself-consistencyequation(8)
Input: ρ(a|s),µ(θ|a,s),α,β
Initialize: F ←0,F ←0
old
whilenotconvergeddo
forallthes∈Sdo
(cid:20) (cid:104) (cid:16) (cid:104) (cid:105)(cid:17)(cid:105)α(cid:21)
F(s)← α 1 logE ρ(a|s) E µ(θ|a,s) exp βE Tθ(s(cid:48)|a,s) R s s , (cid:48) a +γF old (s(cid:48)) β
end
F ←F
old
end
(cid:16) (cid:17)
π(a|s)← 1 ρ(a|s)exp αlogZ (a,s)
Zα(s) β β
(cid:16) (cid:104) (cid:105)(cid:17)
ψ(θ|a,s)← Zβ( 1 a,s) µ(θ|a,s)exp βE Tθ(s(cid:48)|a,s) R s s , (cid:48) a +γF(s(cid:48))
returnπ(a|s),ψ(θ|a,s),F(s)
4 Convergence
Here,weshowthatthevalueiterationschemedescribedthroughAlgorithm1converges
toauniquefixedpointsatisfyingEquation(8).Tothisend,wefirstprovetheexistence
of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the
convergenceofthevalueiterationschemepresupposingthatauniquefixedpointexists
(Theorem2)following[19].
Theorem1. AssumingaboundedrewardfunctionRs(cid:48) ,theoptimalfree-energyvector
s,a
F∗(s) is a unique fixed point of Bellman’s equation F∗ = BF∗, where the mapping
B :R|S| →R|S|isdefinedasinequation(13)
Proof. Theorem1isproventhroughProposition1and2inthefollowing.
8 Grau-Moyaetal.
Proposition1. ThemappingT :R|S| →R|S|
π,ψ
(cid:34)
1 π(a|s)
T F(s)=E − log +
π,ψ π(a|s) α ρ(a|s)
(cid:20)
1 ψ(θ|a,s)
E − log +
ψ(θ|a,s) β µ(θ|a,s)
(cid:104)
(cid:105)(cid:21)(cid:35)
E Rs(cid:48) +γF(s(cid:48)) . (14)
Tθ(s(cid:48)|a,s) s,a
converges to a unique solution for every policy-belief-pair (π,ψ) independent of the
initialfree-energyvectorF(s).
Proof. ByintroducingthematrixP (s,s(cid:48))andthevectorg (s)as
π,ψ π,ψ
(cid:20) (cid:21)
P (s,s(cid:48)):=E E [T (s(cid:48)|a,s)] ,
π,ψ π(a|s) ψ(θ|a,s) θ
(cid:34) (cid:20) (cid:104) (cid:105) 1 ψ(θ|a,s) (cid:21) 1 π(a|s) (cid:35)
g (s):=E E E Rs(cid:48) − log − log ,
π,ψ π(a|s) ψ(θ|a,s) Tθ(s(cid:48)|a,s) s,a β µ(θ|a,s) α ρ(a|s)
Equation (14) may be expressed in compact form: T F = g +γP F. By ap-
π,ψ π,ψ π,ψ
plyingthemappingT aninfinitenumberoftimesonaninitialfree-energyvectorF,
π,ψ
thefree-energyvectorF ofthepolicy-belief-pair(π,ψ)isobtained:
π,ψ
i−1
(cid:88)
F := lim Ti F = lim γtPt g + lim γiPi F,
π,ψ π,ψ π,ψ π,ψ π,ψ
i→∞ i→∞ i→∞
t=0 (cid:124) (cid:123)(cid:122) (cid:125)
→0
which does no longer depend on the initial F. It is straightforward to show that the
quantityF isafixedpointoftheoperatorT :
π,ψ π,ψ
i−1
(cid:88)
T F =g +γP lim γtPt g
π,ψ π,ψ π,ψ π,ψ π,ψ π,ψ
i→∞
t=0
i
(cid:88)
=γ0P0 g + lim γtPt g
π,ψ π,ψ π,ψ π,ψ
i→∞
t=1
i−1
(cid:88)
= lim γtPt g + lim γiPi g =F .
π,ψ π,ψ π,ψ π,ψ π,ψ
i→∞ i→∞
t=0 (cid:124) (cid:123)(cid:122) (cid:125)
→0
Furthermore,F isunique.AssumeforthispurposeanarbitraryfixedpointF(cid:48) such
π,ψ
thatT F(cid:48) =F(cid:48),thenF(cid:48) =lim Ti F(cid:48) =F .
π,ψ i→∞ π,ψ π,ψ
Proposition2. Theoptimalfree-energyvectorF∗ =max ext F isauniquefixed
π ψ π,ψ
pointofBellman’sequationF∗ =BF∗.
Information-ProcessingConstraintsandModelUncertaintyinMDPs 9
Proof. Theproofconsistsoftwopartswhereweassumeext=maxinthefirstpartand
ext = min in the second part respectively. Let ext = max and F∗ = F , where
π∗,ψ∗
(π∗,ψ∗)denotestheoptimalpolicy-belief-pair.Then
Induction
F∗ =T F∗ ≤maxmaxT F∗ =:T F∗ ≤ F ,
π∗,ψ∗ π,ψ π(cid:48),ψ(cid:48) π(cid:48),ψ(cid:48)
π ψ
(cid:124) (cid:123)(cid:122) (cid:125)
=BF∗
where the last inequality can be straightforwardly proven by induction and exploiting
thefactthatP (s,s(cid:48)) ∈ [0;1].ButbydefinitionF∗ = max max F ≥ F ,
π,ψ π ψ π,ψ π(cid:48),ψ(cid:48)
hence F∗ = F and therefore F∗ = BF∗. Furthermore, F∗ is unique. Assume
π(cid:48),ψ(cid:48)
for this purpose an arbitrary fixed point F(cid:48) = F such that F(cid:48) = BF(cid:48) with the
π(cid:48),ψ(cid:48)
correspondingpolicy-belief-pair(π(cid:48),ψ(cid:48)).Then
Induction
F∗ =T F∗ ≥T F∗ ≥ F =F(cid:48),
π∗,ψ∗ π(cid:48),ψ(cid:48) π(cid:48),ψ(cid:48)
andsimilarlyF(cid:48) ≥F∗,henceF(cid:48) =F∗.
Letext = minandF∗ = F .BytakingacloserlookatEquation(13),itcan
π∗,ψ∗
beseenthattheoptimizationoverψdoesnotdependonπ.Then
Induction
F∗ =T F∗ ≥minT F∗ =:T F∗ ≥ F .
π∗,ψ∗ π∗,ψ π∗,ψ(cid:48) π∗,ψ(cid:48)
ψ
ButbydefinitionF∗ = min F ≤ F ,henceF∗ = F .Thereforeitholds
ψ π∗,ψ π∗,ψ(cid:48) π∗,ψ(cid:48)
thatBF∗ = max min T F∗ = max T F∗ andsimilartothefirstpartofthe
π ψ π,ψ π π,ψ∗
proofweobtain
Induction
F∗ =T F∗ ≤maxT F∗ =:T F∗ ≤ F .
π∗,ψ∗ π,ψ∗ π(cid:48),ψ∗ π(cid:48),ψ∗
π
(cid:124) (cid:123)(cid:122) (cid:125)
=BF∗
But by definition F∗ = max F ≥ F , hence F∗ = F and therefore
π π,ψ∗ π(cid:48),ψ∗ π(cid:48),ψ∗
F∗ =BF∗.Furthermore,F isunique.Assumeforthispurposeanarbitraryfixed
π∗,ψ∗
pointF(cid:48) =F suchthatF(cid:48) =BF(cid:48).Then
π(cid:48),ψ(cid:48)
Induction Induction
F(cid:48) =T F(cid:48) ≤T F(cid:48) ≤ F ≤ T F∗ ≤T F∗ =F∗,
π(cid:48),ψ(cid:48) π(cid:48),ψ∗ π(cid:48),ψ∗ π(cid:48),ψ∗ π∗,ψ∗
andsimilarlyF∗ ≤F(cid:48),henceF∗ =F(cid:48).
Theorem2. Let (cid:15) be a positive number satisfying (cid:15) < η where γ ∈ (0;1) is the
1−γ
discountfactorandwhereuandlaretheboundsoftherewardfunctionRs(cid:48) suchthat
s,a
l ≤ Rs(cid:48) ≤ u and η = max{|u|,|l|}. Suppose that the value iteration scheme from
s,a
Algorithm 1 is run for i = (cid:100)log (cid:15)(1−γ)(cid:101) iterations with an initial free-energy vector
γ η
F(s)=0foralls.Then,itholdsthatmax |F∗(s)−BiF(s)|≤(cid:15),whereF∗refersto
s
theuniquefixedpointfromTheorem1.
10 Grau-Moyaetal.
Proof. We start the proof by showing that the L -norm of the difference vector be-
∞
tweentheoptimalfree-energyF∗andBiF exponentiallydecreaseswiththenumberof
iterationsi:
max (cid:12) (cid:12)F∗(s)−BiF(s) (cid:12) (cid:12)=: (cid:12) (cid:12)F∗(s∗)−BiF(s∗) (cid:12) (cid:12)
s
Eq = .(9) (cid:12) (cid:12) (cid:12) (cid:12) m π axE π(a|s∗) (cid:20) β 1 logZ β (a,s∗)− α 1 log π ρ( ( a a | | s s ∗ ∗ ) ) (cid:21)
−m π axE π(a|s∗) (cid:20) β 1 logZ β i(a,s∗)− α 1 log π ρ( ( a a | | s s ∗ ∗ ) ) (cid:21)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:20) (cid:21)(cid:12)
≤m π ax (cid:12) (cid:12) (cid:12) E π(a|s∗) β 1 logZ β (a,s∗)− β 1 logZ β i(a,s∗) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
≤m a ax (cid:12) (cid:12) (cid:12)β 1 logZ β (a,s∗)− β 1 logZ β i(a,s∗) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
=: (cid:12) (cid:12) (cid:12)β 1 logZ β (a∗,s∗)− β 1 logZ β i(a∗,s∗) (cid:12) (cid:12) (cid:12)
Eq = .(11) (cid:12) (cid:12) (cid:12) (cid:12) e ψ xtE ψ(θ|a∗,s∗) (cid:20) E Tθ(s(cid:48)|a∗,s∗) (cid:2) R s s , (cid:48) a +γF∗(s(cid:48)) (cid:3) − β 1 log ψ µ( ( θ θ | | a a ∗ ∗ , , s s ∗ ∗ ) ) (cid:21)
− e ψ xtE ψ(θ|a∗,s∗) (cid:20) E Tθ(s(cid:48)|a∗,s∗) (cid:2) R s s , (cid:48) a +γBi−1F(s(cid:48)) (cid:3) − β 1 log ψ µ( ( θ θ | | a a ∗ ∗ , , s s ∗ ∗ ) ) (cid:21)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:20) (cid:21)(cid:12)
≤m
ψ
ax (cid:12) (cid:12)
(cid:12)
E ψ(θ|a∗,s∗) E Tθ(s(cid:48)|a∗,s∗) (cid:2) γF∗(s(cid:48))−γBi−1F(s(cid:48)) (cid:3) (cid:12) (cid:12)
(cid:12)
≤γmax (cid:12) (cid:12)F∗(s)−Bi−1F(s) (cid:12) (cid:12) Re ≤ cur. γimax|F∗(s)−F(s)|≤γi η ,
s s 1−γ
whereweexploitthefactthat|ext f(x)−ext g(x)|≤max |f(x)−g(x)|andthat
x x x
thefree-energyisboundedthroughtherewardboundslanduwithη = max{|u|,|l|}.
For a convergence criterion (cid:15) > 0 such that (cid:15) ≥ γi η , it then holds that i ≥
1−γ
log (cid:15)(1−γ) presupposingthat(cid:15)< η .
γ η 1−γ
5 Experiments:GridWorld
This section illustrates the proposed value iteration scheme with an intuitive example
whereanagenthastonavigatethroughagrid-world.TheagentstartsatpositionS∈S
withtheobjectivetoreachthegoalstateG ∈ S andcanchooseoneoutofmaximally
fourpossibleactionsa∈{↑,→,↓,←}ineachtime-step.Alongtheway,theagentcan
encounterregulartiles(actionsmovetheagentdeterministicallyonestepinthedesired
direction),wallsthatarerepresentedasgraytiles(actionsthatmovetheagenttowards
the wall are not possible), holes that are represented as black tiles (moving into the
holecausesanegativereward)andchancetilesthatareillustratedaswhitetileswitha
questionmark(thetransitionprobabilitiesofthechancetilesareunknowntotheagent).
Reaching the goal G yields a reward R = +1 whereas stepping into a hole results in
a negative reward R = −1. In both cases the agent is subsequently teleported back
Information-ProcessingConstraintsandModelUncertaintyinMDPs 11
to the starting position S. Transitions to regular tiles have a small negative reward of
R =−0.01.Whensteppingontoachancetile,theagentispushedstochasticallytoan
adjacenttilegivingarewardasmentionedabove.Thetruestate-transitionprobabilities
ofthechancetilesarenotknownbytheagent,buttheagentholdstheBayesianbelief
N(s)
µ(θ s,a |a,s)=Dirichlet (cid:0) Φ s s (cid:48) 1,a ,...,Φ s s , (cid:48) N a (s) (cid:1) = (cid:89) (θ s s , (cid:48) ia )Φ s s, (cid:48) i a−1
i=1
wheretransitionmodelisdenotedasT θs,a (s(cid:48)|s,a)=θ s s , (cid:48) a andθ s,a = (cid:0) θ s s , (cid:48) 1a ...θ s s , (cid:48) N a (s) (cid:1)
andN(s)isthenumberofpossibleactionsinstates.Thedataisincorporatedintothe
modelasacountvector (cid:0) Φ s s (cid:48) 1,a ,...,Φ s s (cid:48) N ,a (s) (cid:1) whereΦs s, (cid:48) a representsthenumberoftimes
thatthetransition(s,a,s(cid:48))hasoccurred.Thepriorρ(a|s)fortheactionsateverystate
issettobeuniform.Animportantaspectofthemodelisthatinthecaseofunlimited
observationaldata,theagentwillplanwiththecorrecttransitionprobabilities.
We conducted two experiments with discount factor γ = 0.9 and uniform pri-
ors ρ(a|s) for the action variables. In the first experiment, we explore and illustrate
theagent’splanningbehaviorunderdifferentdegreesofcomputationallimitations(by
varying α) and under different model uncertainty attitudes (by varying β) with fixed
uniformbeliefsµ(θ|a,s).Inthesecondexperiment,theagentisallowedtoupdateits
beliefsµ(θ|a,s)andusetheupdatedmodeltore-planitsstrategy.
5.1 TheRoleoftheParametersαandβonPlanning
Figure 1 shows the solution to the variational free energy problem that is obtained by
iterationuntilconvergenceaccordingtoAlgorithm1underdifferentvaluesofαandβ.
Inparticular,thefirstrowshowsthefreeenergyfunctionF∗(s)(Eq.(8)).Thesecond,
thirdandfourthrowshowheatmapsofthepositionofanagentthatfollowstheoptimal
policy(Eq.(12))accordingtotheagent’sbiasedbeliefs(plan)andtotheactualtransi-
tionprobabilitiesinafriendlyandunfriendlyenvironment,respectively.Inchancetiles,
themostlikelytransitionsinthesetwoenvironmentsareindicatedbyarrowswherethe
agentisteleportedwithaprobabilityof0.999intothetileindicatedbythearrowand
withaprobabilityof0.001toarandomotheradjacenttile.
In the first column of Fig. 1 it can be seen that a stochastic agent (α = 3.0) with
highmodeluncertaintyandoptimisticattitude(β =400)hasastrongpreferenceforthe
broadcorridorinthebottombyassumingfavorabletransitionsfortheunknownchance
tiles. This way the agent also avoids the narrow corridors that are unsafe due to the
stochasticityofthelow-αpolicy.InthesecondcolumnofFig.1withlowα = 3and
highmodeluncertaintywithpessimisticattitudeβ = −400,theagentstronglyprefers
the upper broad corridor because unfavorable transitions are assumed for the chance
tiles.ThethirdcolumnofFig.1showsaverypessimisticagent(β =−400)withhigh
precision(α=11)thatallowtheagenttosafelychoosetheshortestdistancebyselect-
ingtheuppernarrowcorridorwithoutriskinganytileswithunknowntransitions.The
fourthcolumnofFig.1showsaveryoptimisticagent(β =400)withhighprecision.In
thiscasetheagentchoosestheshortestdistancebyselectingthebottomnarrowcorridor
thatincludestwochancetileswithunknowntransition.
12 Grau-Moyaetal.
ygrenE
eerF
α=3 β=400
S G
? ?
? ?
? ?
nalP
S G
? ?
? ?
? ?
tnemnorivnE
yldneirF
S G
0
1
2
3
4
5
6
7
8
0 1 2 3 4 5 6 7 8
tnemnorivnE
yldneirfnU
α=3 β= 400 α=11 β= 400 α=11 β=400
− −
1
S G S G S G
? ? ? ? ? ? 0
? ? ? ? ? ?
? ? ? ? ? ?
1
S G S G S G
? ? ? ? ? ?
? ? ? ? ? ?
? ? ? ? ? ?
S G S G S G
S G S G S G S G
ygrenE
eerF
1.0
0.0
ytisneD
1.0
0.0
ytisneD
1.0
0.0
ytisneD
Fig.1.Thefourdifferentrowsshowfreeenergyvaluesandheat-mapsofplannedtrajectoriesac-
cordingtotheagent’sbeliefsoverstate-transitionsinchancetiles,heat-mapsofrealtrajectories
in a friendly environment and in an unfriendly environment respectively. The Start-position is
indicatedbySandthegoalstateisindicatedbyG.Blacktilesrepresentholeswithnegativere-
ward,graytilesrepresentwallsandchancetileswithaquestionmarkhavetransitionprobabilities
unknowntotheagent.Thewhitetileswithanarrowrepresentthemostprobablestate-transition
inchancetiles(asspecifiedbytheenvironment).Verysmallarrowsineachcellencodethepol-
icyπ(a|s)(thelengthofeacharrowencodestheprobabilityofthecorrespondingactionunder
thepolicy,highestprobabilityactionisindicatedasaredarrow).Theheatmapisconstructed
bynormalizingthenumberofvisitsforeachstateover20000steps,whereactionsaresampled
fromtheagent’spolicyandstate-transitionsaresampledaccordingtooneofthreeways:thesec-
ondrowaccordingtotheagent’sbeliefoverstate-transitionsψ(θ|a,s),inthethirdandfourth
rowaccordingtotheactualtransitionprobabilitiesofafriendlyandanunfriendlyenvironment
respectively.Differentcolumnsshowdifferentαandβcases.
Information-ProcessingConstraintsandModelUncertaintyinMDPs 13
5.2 UpdatingtheBayesianPosteriorµwithObservationsfromtheEnvironment
Similar to model identification adaptive controllers that perform system identification
while the system is running [20], we can use the proposed planning algorithm also in
areinforcementlearningsetupbyupdatingtheBayesianbeliefsabouttheMDPwhile
executingalwaysthefirstactionandreplanninginthenexttimestep.Duringthelearn-
ing phase, the exploration is governed by both factors α and β, but each factor has a
differentinfluence.Inparticular,lowerα-valueswillcausemoreexplorationduetothe
inherentstochasticityintheagent’sactionselection,similartoan(cid:15)-greedypolicy.Ifα
iskeptfixedthroughtime,thiswillofcoursealsoimplya“suboptimal”(i.e.bounded
optimal) policy in the long run. In contrast, the parameter β governs exploration of
stateswithunknowntransition-probabilitiesmoredirectlyandwillnothaveanimpact
on the agent’s performance in the limit, where sufficient data has eliminated model
uncertainty.Weillustratethiswithsimulationsinagrid-worldenvironmentwherethe
agent is allowed to update its beliefs µ(θ|a,s) over the state-transitions every time it
entersachancetileandreceivesobservationdataacquiredthroughinteractionwiththe
environment—compareleftpanelsinFigure2.Ineachstep,theagentcanthenusethe
updatedbelief-modelsforplanningthenextaction.
40
35
0 0 30
1 1 25
20
2 SSS GGG 2 SSS GGG 15
3 3 10
5
4 4 00 50 100 150 200 250 300
0 1 2 3 4 0 1 2 3 4 Step
Environment Optimal
0 ? 0 ?
1 1
2 S ? ? G 2 S ? ? G
3 3
4 ? 4 ?
0 1 2 3 4 0 1 2 3 4 Pessimistic Optimistic
deriuqca
stniop
ataD
α fixed
0.10
0.05
0.00
0.05
0.10
0.150 50 100 150 200 250 300 Step
drawer
egarevA
35
30
25
20
15
10
5
00 50 100 150 200 250 300
Step
α=12.0 β=0.2
α=12.0 β=5.0
α=12.0 β=20.0
deriuqca
stniop
ataD
β fixed
0.10
0.05
0.00
0.05
0.10
0.150 50 100 150 200 250 300 Step
drawer
egarevA
α=5.0 β=0.2
α=8.0 β=0.2
α=12.0 β=0.2
Fig.2.Theeffectofαandβwhenupdatingbeliefsover300interactionstepswiththeenviron-
ment.Thefourpanelsontheleftshowthegrid-worldenvironmentandthepertainingoptimal
policyiftheenvironmentisknown.Thelowerleftpanelsshowpathsthattheagentcouldtake
dependingonitsattitudetowardsmodeluncertainty.Thepanelsontherightshowthenumberof
acquireddatapoints,thatisthenumberoftimesachancetileisentered,andtheaveragereward
(bottompanels)forfixedα(varyingβ)orfixedβ(varyingα).Theaveragerewardateachstep
iscomputedasfollows.Eachtimetheagentobservesastate-transitioninachancetileandup-
datesitsbeliefmodel,10runsoflength2000stepsaresampled(usingtheagent’scurrentbelief
model).Theaveragereward(boldlines)andstandard-deviation(shadedareas)acrossthese10
runsareshowninthefigure.
14 Grau-Moyaetal.
Figure2(rightpanels)showsthenumberofdatapointsacquired(eachtimeachance
tileisvisited)andtheaveragerewarddependingonthenumberofstepsthattheagent
has interacted with the environment. The panels show several different cases: while
keepingα = 12.0fixedwetestβ = (0.2,5.0,20.0)andwhilekeepingβ = 0.2fixed
wetestα=(5.0,8.0,12.0).Itcanbeseenthatlowerαleadstobetterexploration,butit
canalsoleadtolowerperformanceinthelongrun—seeforexamplerightmostbottom
panel. In contrast, optimistic β values can also induce high levels of exploration with
theaddedadvantagethatinthelimitnoperformancedetrimentisintroduced.However,
highβ valuescaningeneralalsoleadtoadetrimentalpersistencewithbadpolicies,as
canbeseenforexampleinthesuperiorityofthelow-β agentattheverybeginningof
thelearningprocess.
6 DiscussionandConclusions
In this paper we are bringing two strands of research together, namely research on
information-theoreticprinciplesofcontrolanddecision-makingandrobustnessprinci-
plesforplanningundermodeluncertainty.Wehavedevisedaunifiedrecursionprinci-
plethatextendspreviousgeneralizationsofBellman’soptimalityequationandwehave
shown how to solve this recursion with an iterative scheme that is guaranteed to con-
vergetoauniqueoptimum.Insimulationswecoulddemonstratehowsuchacombina-
tionofinformation-theoreticpolicyandbeliefconstraintsthatreflectmodeluncertainty
canbebeneficialforagentsthatactinpartiallyunknownenvironments.
MostoftheresearchonrobustMDPsdoesnotconsiderinformation-processingcon-
straintsonthepolicy,butonlyconsiderstheuncertaintyinthetransitionprobabilitiesby
specifyingasetofpermissiblemodelssuchthatworst-casescenarioscanbecomputed
in order to obtain a robust policy [11,12]. Recent extensions of these approaches in-
cludemoregeneralassumptionsregardingthesetpropertiesofthepermissiblemodels
andassumptionsregardingthedatagenerationprocess[13].Ourapproachfallsinside
thisclassofrobustnessmethodsthatusearestrictedsetofpermissiblemodels,because
we extremize the biased belief ψ(θ|a,s) under the constraint that it has to be within
some information bounds measured by the Kullback-Leibler divergence from a refer-
enceBayesianposterior.Contrarytothesepreviousmethods,ourapproachadditionally
considersrobustnessarisingfromthestochasticityinthepolicy.
Information-processing constraints on the policy in MDPs have been previously
considered in a number of studies [3,21,22,17], however not in the context of model
uncertainty. In these studies a free energy value recursion is derived when restricting
the class of policies through the Kullback-Leibler divergence and when disregarding
separateinformation-processingconstraintsonobservations.However,asmallnumber
of studies has considered information-processing constraints both for actions and ob-
servations.Forexample,PolaniandTishby[8]andOrtegaandBraun[6]combineboth
kindsofinformationcosts.Thefirstcostformalizesaninformation-processingcostin
thepolicyandthesecondcostconstrainsuncertaintyarisingfromthestatetransitions
directly(butcruciallynottheuncertaintyinthelatentvariables).Inbothinformation-
processing constraints the cost is determined as a Kullback-Leibler divergence with
respecttoareferencedistribution.Specifically,thereferencedistributionin[8]isgiven
Information-ProcessingConstraintsandModelUncertaintyinMDPs 15
by the marginal distributions (which is equivalent to a rate distortion problem) and in
[6]isgivenbyfixedpriors.TheKullback-Leiblerdivergencecostsfortheobservations
inthesecasesessentiallycorrespondtoarisk-sensitiveobjective.Whilethereisarela-
tionbetweenrisk-sensitiveandrobustMDPs[23,24,25],theinnovationinourapproach
isatleasttwofold.First,itallowscombininginformation-processingconstraintsonthe
policywithmodeluncertainty(asformalizedbyalatentvariable).Second,itprovides
anaturalsetuptostudylearning.
ThealgorithmpresentedhereandBayesianmodelsingeneral[9]arecomputation-
allyexpensiveastheyhavetocomputepossiblyhigh-dimensionalintegralsdepending
on the number of allowed transitions for action-state pairs. However, there have been
tremendouseffortsinsolvingunknownMDPsefficiently,especiallybysamplingmeth-
ods[26,27,28].Aninterestingfuturedirectiontoextendourmethodologywouldthere-
fore be to develop a sampling-based version of Algorithm 1 to increase the range of
applicability and scalability [29]. Moreover, such sampling methods might allow for
reinforcement learning applications, for example by estimating free energies through
TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic
methodsforadaptivecontrol[20].
Acknowledgments ThisstudywassupportedbytheDFG,EmmyNoethergrantBR4164/1-
1.ThecodewasdevelopedontopoftheRLPylibrary[33].
References
1. RichardBellman. DynamicProgramming. PrincetonUniversityPress,Princeton,NJ,USA,
1edition,1957.
2. Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural
informationprocessingsystems,pages1369–1376,2006.
3. Emanuel Todorov. Efficient computation of optimal actions. Proceedings of the national
academyofsciences,106(28):11478–11483,2009.
4. DanielABraun,PedroAOrtega,EvangelosTheodorou,andStefanSchaal. Pathintegral
control and bounded rationality. In Adaptive Dynamic Programming And Reinforcement
Learning(ADPRL),2011IEEESymposiumon,pages202–209.IEEE,2011.
5. BartvandenBroek,WimWiegerinck,andHilbertJ.Kappen. Risksensitivepathintegral
control. InUAI,2010.
6. PedroAOrtegaandDanielABraun. Thermodynamicsasatheoryofdecision-makingwith
information-processingcosts. InProc.R.Soc.A,volume469,page20120683.TheRoyal
Society,2013.
7. Pedro A Ortega and Daniel A Braun. Generalized thompson sampling for sequential
decision-makingandcausalinference. ComplexAdaptiveSystemsModeling,2(1):2,2014.
8. NaftaliTishbyandDanielPolani.Informationtheoryofdecisionsandactions.InPerception-
actioncycle,pages601–636.Springer,2011.
9. MichaelO’GordonDuff. OptimalLearning:ComputationalproceduresforBayes-adaptive
Markovdecisionprocesses. PhDthesis,UniversityofMassachusettsAmherst,2002.
10. ShieMannor,DuncanSimester,PengSun,andJohnNTsitsiklis. Biasandvarianceapprox-
imationinvaluefunctionestimates. ManagementScience,53(2):308–322,2007.
11. Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with
uncertaintransitionmatrices. OperationsResearch,53(5):780–798,2005.
16 Grau-Moyaetal.
12. Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research,
30(2):257–280,2005.
13. WolframWiesemann,DanielKuhn,andBerc¸ Rustem. Robustmarkovdecisionprocesses.
MathematicsofOperationsResearch,38(1):153–183,2013.
14. Istva´n Szita and Andra´s Lo˝rincz. The many faces of optimism: a unifying approach. In
Proceedingsofthe25thinternationalconferenceonMachinelearning,pages1048–1055.
ACM,2008.
15. Istva´nSzitaandCsabaSzepesva´ri. Model-basedreinforcementlearningwithnearlytight
exploration complexity bounds. In Proceedings of the 27th International Conference on
MachineLearning(ICML-10),pages1031–1038,2010.
16. LarsPeterHansenandThomasJSargent. Robustness. Princetonuniversitypress,2008.
17. JonathanRubin,OhadShamir,andNaftaliTishby. Tradingvalueandinformationinmdps.
InDecisionMakingwithImperfectDecisionMakers,pages57–74.Springer,2012.
18. DPBertsekasandJNTsitsiklis. Neuro-dynamicprogramming. 1996.
19. Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite
mdps:Pacanalysis. TheJournalofMachineLearningResearch,10:2413–2444,2009.
20. KarlJA˚stro¨mandBjo¨rnWittenmark. Adaptivecontrol. CourierCorporation,2013.
21. HilbertJKappen.Lineartheoryforcontrolofnonlinearstochasticsystems.Physicalreview
letters,95(20):200201,2005.
22. JPeters,KMu¨lling,YAltun,FoxDPoole,etal. Relativeentropypolicysearch. InTwenty-
FourthNationalConferenceonArtificialIntelligence(AAAI-10),pages1607–1612.AAAI
Press,2010.
23. YunShen,MichaelJTobia,TobiasSommer,andKlausObermayer.Risk-sensitivereinforce-
mentlearning. Neuralcomputation,26(7):1298–1328,2014.
24. TakayukiOsogami. Robustnessandrisk-sensitivityinmarkovdecisionprocesses. InAd-
vancesinNeuralInformationProcessingSystems,pages233–241,2012.
25. Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust
decision-making: a cvar optimization approach. In Advances in Neural Information Pro-
cessingSystems,pages1522–1530,2015.
26. Ste´phane Ross, Joelle Pineau, Brahim Chaib-draa, and Pierre Kreitmann. A bayesian ap-
proach for learning and planning in partially observable markov decision processes. The
JournalofMachineLearningResearch,12:1729–1770,2011.
27. ArthurGuez,DavidSilver,andPeterDayan.Efficientbayes-adaptivereinforcementlearning
usingsample-basedsearch. InAdvancesinNeuralInformationProcessingSystems,pages
1025–1033,2012.
28. ArthurGuez,DavidSilver,andPeterDayan.Scalableandefficientbayes-adaptivereinforce-
mentlearningbasedonmonte-carlotreesearch. JournalofArtificialIntelligenceResearch,
pages841–883,2013.
29. PedroAOrtega,DanielABraun,andNaftaliTishby. Montecarlomethodsforexact&effi-
cientsolutionofthegeneralizedoptimalityequations. InRoboticsandAutomation(ICRA),
2014IEEEInternationalConferenceon,pages4322–4327.IEEE,2014.
30. RoyFox,AriPakman,andNaftaliTishby. G-learning:Tamingthenoiseinreinforcement
learningviasoftupdates. arXivpreprintarXiv:1512.08562,2015.
31. PedroAOrtegaandDanielABraun.Aminimumrelativeentropyprincipleforlearningand
acting. JournalofArtificialIntelligenceResearch,pages475–511,2010.
32. PedroAOrtegaandDanielABraun. Abayesianruleforadaptivecontrolbasedoncausal
interventions.In3dConferenceonArtificialGeneralIntelligence(AGI-2010).AtlantisPress,
2010.
33. AlborzGeramifard,ChristophDann,RobertHKlein,WilliamDabney,andJonathanPHow.
Rlpy:Avalue-function-basedreinforcementlearningframeworkforeducationandresearch.
JournalofMachineLearningResearch,16:1573–1578,2015.