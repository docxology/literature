Planning with Information-Processing Constraints and
Model Uncertainty in Markov Decision Processes
Jordi Grau-Moya1,2,3, Felix Leibfried1,2,3, Tim Genewein1,2,3, and Daniel A. Braun1,2
1 Max Planck Institute for Intelligent Systems, T¨ubingen, Germany,
jordi.grau@tuebingen.mpg.de,
2 Max Planck Institute for Biological Cybernetics, T¨ubingen, Germany,
3 Graduate Training Centre for Neuroscience, T¨ubingen, Germany
Abstract. Information-theoretic principles for learning and acting have been pro-
posed to solve particular classes of Markov Decision Problems. Mathematically,
such approaches are governed by a variational free energy principle and allow
solving MDP planning problems with information-processing constraints expressed
in terms of a Kullback-Leibler divergence with respect to a reference distribu-
tion. Here we consider a generalization of such MDP planners by taking model
uncertainty into account. As model uncertainty can also be formalized as an
information-processing constraint, we can derive a uniﬁed solution from a sin-
gle generalized variational principle. We provide a generalized value iteration
scheme together with a convergence proof. As limit cases, this generalized scheme
includes standard value iteration with a known model, Bayesian MDP planning,
and robust planning. We demonstrate the beneﬁts of this approach in a grid world
simulation.
Keywords: bounded rationality, model uncertainty, robustness, planning, Markov
Decision Processes
1 Introduction
The problem of planning in Markov Decision Processes was famously addressed by
Bellman who developed the eponymous principle in 1957 [1]. Since then numerous
variants of this principle have ﬂourished in the literature. Here we are particularly inter-
ested in a generalization of the Bellman principle that takes information-theoretic con-
straints into account. In the recent past there has been a special interest in the Kullback-
Leibler divergence as a constraint to limit deviations of the action policy from a prior.
This can be interesting in a number of ways. Todorov [2,3], for example, has trans-
formed the general MDP problem into a restricted problem class without explicit action
variables, where control directly changes the dynamics of the environment and control
costs are measured by the Kullback-Leibler divergence between controlled and uncon-
trolled dynamics. This simpliﬁcation allows mapping the Bellman recursion to a linear
algebra problem. This approach can also be be generalized to continuous state spaces
leading to path integral control [4,5]. The same equations can also be interpreted in
terms of bounded rational decision-making where the decision-maker has limited com-
putational resources that allow only limited deviations from a prior decision strategy
arXiv:1604.02080v1  [cs.AI]  7 Apr 2016
2 Grau-Moya et al.
(measured by the Kullback-Leiber divergence in bits) [6]. Such a decision-maker can
also be instantiated by a sampling process that has restrictions in the number of sam-
ples it can afford [7]. Disregarding the possibility of a sampling-based interpretation,
the Kullback-Leibler divergence introduces a control information cost that is interesting
in its own right when formalizing the perception action cycle [8].
While the above frameworks have led to interesting computational advances, so
far they have neglected the possibility of model misspeciﬁcation in the MDP setting.
Model misspeciﬁcation or model uncertainty does not refer to the uncertainty arising
due to the stochastic nature of the environment (usually called risk-uncertainty in the
economic literature), but refers to the uncertainty with respect to the latent variables that
specify the MDP. In Bayes-Adaptive MDPs [9], for example, the uncertainty over the
latent parameters of the MDP is explicitly represented, such that new information can
be incorporated with Bayesian inference. However, Bayes-Adaptive MDPs are not ro-
bust with respect to model misspeciﬁcation and have no performance guarantees when
planning with wrong models [10]. Accordingly, there has been substantial interest in
developing robust MDP planners [11,12,13]. One way to take model uncertainty into
account is to bias an agent’s belief model from a reference Bayesian model towards
worst-case scenarios; thus avoiding disastrous outcomes by not visiting states where
the transition probabilities are not known. Conversely, the belief model can also be bi-
ased towards best-case scenarios as a measure to drive exploration—also referred in the
literature as optimism in face of uncertainty [14,15].
When comparing the literature on information-theoretic control and model uncer-
tainty, it is interesting to see that some notions of model uncertainty follow exactly the
same mathematical principles as the principles of relative entropy control [3]. In this
paper we therefore formulate a uniﬁed and combined optimization problem for MDP
planning that takes both, model uncertainty and bounded rationality into account. This
new optimization problem can be solved by a generalized value iteration algorithm. We
provide a theoretical analysis of its convergence properties and simulations in a grid
world.
2 Background and Notation
In the MDP setting the agent at time tinteracts with the environment by taking action
at ∈A while in state st ∈S. Then the environment updates the state of the agent to
st+1 ∈S according to the transition probabilities T(st+1|at,st). After each transition
the agent receives a reward Rst+1
st,at ∈ Rthat is bounded. For our purposes we will
consider Aand Sto be ﬁnite. The aim of the agent is to choose its policy π(a|s) in
order to maximize the total discounted expected reward or value function for anys∈S
V∗(s) = max
π
lim
T→∞
E
[T−1∑
t=0
γtRst+1
st,at
]
with discount factor 0 ≤γ <1. The expectation is over all possible trajectories ξ =
s0,a0,s1 ... of state and action pairs distributed according to p(ξ) =∏T−1
t=0 π(at|st)
T(st+1|at,st). It can be shown that the optimal value function satisﬁes the following
Information-Processing Constraints and Model Uncertainty in MDPs 3
recursion
V∗(s) = max
π
∑
a,s′
π(a|s)T(s′|a,s)
[
Rs′
s,a + γV∗(s′)
]
. (1)
At this point there are two important implicit assumptions. The ﬁrst is that the pol-
icy π can be chosen arbitrarily without any constraints which, for example, might not
be true for a bounded rational agent with limited information-processing capabilities.
The second is that the agent needs to know the transition-model T(s′|a,s), but this
model is in practice unknown or even misspeciﬁed with respect to the environment’s
true transition-probabilities, specially at initial stages of learning. In the following, we
explain how to incorporate both bounded rationality and model uncertainty into agents.
2.1 Information-Theoretic Constraints for Acting
Consider a one-step decision-making problem where the agent is in state s and has
to choose a single action a from the set Ato maximize the reward Rs′
s,a, where s′
is the next the state. A perfectly rational agent selects the optimal action a∗(s) =
argmaxa
∑
s′T(s′|a,s)Rs′
s,a. However, a bounded rational agent has only limited re-
sources to ﬁnd the maximum of the function ∑
s′T(s′|a,s)Rs′
s,a. One way to model
such an agent is to assume that the agent has a prior choice strategy ρ(a|s) in state
s before a deliberation process sets in that reﬁnes the choice strategy to a posterior
distribution π(a|s) that reﬂects the strategy after deliberation. Intuitively, because the
deliberation resources are limited, the agent can only afford to deviate from the prior
strategy by a certain amount of information bits. This can be quantiﬁed by the relative
entropy DKL(π||ρ) =∑
aπ(a|s) logπ(a|s)
ρ(a|s) that measures the average information cost
of the policy π(a|s) using the source distribution ρ(a|s). For a bounded rational agent
this relative entropy is bounded by some upper limitK. Thus, a bounded rational agent
has to solve a constrained optimization problem that can be written as
max
π
∑
a
π(a|s)
∑
s′
T(s′|a,s)Rs′
s,a s.t. DKL(π||ρ) ≤K
This problem can be rewritten as an unconstrained optimization problem
F∗(s) = max
π
∑
a
π(a|s)
∑
s′
T(s′|a,s)Rs′
s,a −1
αDKL(π||ρ) (2)
= 1
αlog
∑
a
ρ(a|s)eα∑
s′T(s′|a,s)Rs′
s,a. (3)
where F∗is a free energy that quantiﬁes the value of the policy π by trading off the
average reward against the information cost. The optimal strategy can be expressed
analytically in closed-form as
π∗(a|s) =ρ(a|s)eα∑
s′T(s′|a,s)Rs′
s,a
Zα(s)
4 Grau-Moya et al.
with partition sum Zα(s) = ∑
aρ(a|s) exp
(
α∑
s′T(s′|a,s)Rs′
s,a
)
. Therefore, the
maximum operator in (2) can be eliminated and the free energy can be rewritten as in
(3). The Lagrange multiplier αquantiﬁes the boundedness of the agent. By settingα→
∞we recover a perfectly rational agent with optimal policy π∗(a|s) = δ(a−a∗(s)).
For α= 0the agent has no computational resources and the agent’s optimal policy is to
act according to the priorπ∗(a|s) =ρ(a|s). Intermediate values ofαlead to a spectrum
of bounded rational agents.
2.2 Information-Theoretic Constraints for Model Uncertainty
In the following we assume that the agent has a model of the environment Tθ(s′|a,s)
that depends on some latent variables θ ∈Θ. In the MDP setting, the agent holds a
belief µ(θ|a,s) regarding the environmental dynamics where θis a unit vector of tran-
sition probabilities into all possible statess′. While interacting with the environment the
agent can incorporate new data by forming the Bayesian posterior µ(θ|a,s,D ), where
D is the observed data. When the agent has observed an inﬁnite amount of data (and
assuming θ∗(a,s) ∈Θ) the belief will converge to the delta distributionµ(θ|s,a,D ) =
δ(θ−θ∗(a,s)) and the agent will act optimally according to the true transition probabil-
ities, exactly as in ordinary optimal choice strategies with known models. When acting
under a limited amount of data the agent cannot determine the value of an actionawith
the true transition model according to ∑
s′T(s′|a,s)Rs′
s,a, but it can only determine an
expected value according to its beliefs
∫
θµ(θ|a,s) ∑
s′Tθ(s′|a,s)Rs′
s,a.
The Bayesian model µcan be subject to model misspeciﬁcation (e.g. by having a
wrong likelihood or a bad prior) and thus the agent might want to allow deviations from
its model towards best-case (optimistic agent) or worst-case (pessimistic agent) scenar-
ios up to a certain extent, in order to act more robustly or to enhance its performance in
a friendly environment [16]. Such deviations can be measured by the relative entropy
DKL(ψ|µ) between the Bayesian posterior µ and a new biased model ψ. Effectively,
this allows for mathematically formalizing model uncertainty, by not only considering
the speciﬁed model but all models within a neighborhood of the speciﬁed model that
deviate no more than a restricted number of bits. Then, the effective expected value of
an action awhile having limited trust in the Bayesian posteriorµcan be determined for
the case of optimistic deviations as
F∗(a,s) = max
ψ
∫
θ
ψ(θ|a,s)
∑
s′
Tθ(s′|a,s)Rs′
s,a −1
βDKL(ψ||µ) (4)
for β >0, and for the case of pessimistic deviations as
F∗(a,s) = min
ψ
∫
θ
ψ(θ|a,s)
∑
s′
Tθ(s′|a,s)Rs′
s,a −1
βDKL(ψ||µ) (5)
for β <0. Conveniently, both equations can be expressed as a single equation
F∗(a,s) = 1
β log Zβ(a,s)
Information-Processing Constraints and Model Uncertainty in MDPs 5
with β ∈R and Zβ(s,a) =
∫
θµ(θ|a,s) exp
(
β∑
s′Tθ(s′|a,s)Rs′
s,a
)
when inserting
the optimal biased belief
ψ∗(θ|a,s) = 1
Zβ(a,s)µ(θ|a,s) exp
(
β
∑
s′
Tθ(s′|a,s)Rs′
s,a
)
into either equation (4) or (5). By adopting this formulation we can model any degree
of trust in the belief µallowing deviation towards worst-case or best-case with −∞≤
β ≤ ∞. For the case of β → −∞we recover an inﬁnitely pessimistic agent that
considers only worst-case scenarios, for β →∞ an agent that is inﬁnitely optimistic
and for β →0 the Bayesian agent that fully trusts its model.
3 Model Uncertainty and Bounded Rationality in MDPs
In this section, we consider a bounded rational agent with model uncertainty in the in-
ﬁnite horizon setting of an MDP. In this case the agent must take into account all future
rewards and information costs, thereby optimizing the following free energy objective
F∗(s) = max
π
ext
ψ
lim
T→∞
E
T−1∑
t=0
γt
(
Rst+1
st,at−1
β log ψ(θt|at,st)
µ(θt|at,st) −1
αlog π(at|st)
ρ(at|st)
)
(6)
where the extremum operator ext can be either max for β > 0 or min for β < 0,
0 < γ <1 is the discount factor and the expectation E is over all trajectories ξ =
s0,a0,θ0,s1,a1,...a T−1,θT−1,sT with distributionp(ξ) =∏T−1
t=0 π(at|st) ψ(θt|at,st)
Tθt(st+1|at,st). Importantly, this free energy objective satisﬁes a recursive relation and
thereby generalizes Bellman’s optimality principle to the case of model uncertainty and
bounded rationality. In particular, equation (6) fulﬁlls the recursion
F∗(s) = max
π
ext
ψ
Eπ(a|s)
[
−1
αlog π(a|s)
ρ(a|s) +
Eψ(θ|a,s)
[
−1
β log ψ(θ|a,s)
µ(θ|a,s) +
ETθ(s′|a,s)
[
Rs′
s,a + γF∗(s′)
]]]
. (7)
Applying variational calculus and following the same rationale as in the previous
sections [6], the extremum operators can be eliminated and equation (7) can be re-
expressed as
F∗(s) = 1
αlog Eρ(a|s)
[
Eµ(θ|a,s)
[
exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γF∗(s′)
])]α
β
]
(8)
6 Grau-Moya et al.
because
F∗(s) = max
π
Eπ(a|s)
[1
β log Zβ(a,s) −1
αlog π(a|s)
ρ(a|s)
]
(9)
= 1
αlog Eρ(a|s)
[
exp
(α
β log Zβ(a,s)
)]
, (10)
where
Zβ(a,s) = ext
ψ
Eψ(θ|a,s)
[
ETθ(s′|a,s)
[
Rs′
s,a + γF∗(s′)
]
−1
β log ψ(θ|a,s)
µ(θ|a,s)
]
(11)
= Eµ(θ|a,s) exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γF∗(s′)
])
with the optimizing arguments
ψ∗(θ|a,s) = 1
Zβ(a,s)µ(θ|a,s) exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γF(s′)
])
π∗(a|s) = 1
Zα(s)ρ(a|s) exp
(α
β log Zβ(a,s)
)
(12)
and partition sum
Zα(s) =Eρ(a|s)
[
exp
(α
β log Zβ(a,s)
)]
.
With this free energy we can model a range of different agents for different αand
β. For example, by settingα→∞and β →0 we can recover a Bayesian MDP planner
and by setting α →∞ and β →−∞ we recover a robust planner. Additionally, for
α→∞ and when µ(θ|a,s) =δ(θ−θ∗(a,s)) we recover an agent with standard value
function with known state transition model from equation (1).
3.1 Free Energy Iteration Algorithm
Solving the self-consistency equation (8) can be achieved by a generalized version of
value iteration. Accordingly, the optimal solution can be obtained by initializing the
free energy at some arbitrary value F and applying a value iteration scheme Bi+1F =
BBiF where we deﬁne the operator
BF(s) = max
π
ext
ψ
Eπ(a|s)
[
−1
αlog π(a|s)
ρ(a|s) +
Eψ(θ|a,s)
[
−1
β log ψ(θ|a,s)
µ(θ|a,s) +
ETθ(s′|a,s)
[
Rs′
s,a + γF(s′)
]]]
(13)
Information-Processing Constraints and Model Uncertainty in MDPs 7
with B1F = BF, which can be simpliﬁed to
BF(s) = 1
αlog Eρ(a|s)
[
Eµ(θ|a,s)
[
exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γF(s′)
])]α
β
]
In Algorithm (1) we show the pseudo-code of this generalized value iteration scheme.
Given state-dependent prior policiesρ(a|s) and the Bayesian posterior beliefs µ(θ|a,s)
and the values of αand β, the algorithm outputs the equilibrium distributions for the
action probabilities π(a|s), the biased beliefs ψ(θ|a,s) and estimates of the free energy
value function F∗(s). The iteration is run until a convergence criterion is met. The
convergence proof is shown in the next section.
Algorithm 1:Iterative algorithm solving the self-consistency equation (8)
Input: ρ(a|s),µ(θ|a,s),α,β
Initialize: F ←0, Fold ←0
while not converged do
forall thes∈S do
F(s) ←1
α log Eρ(a|s)
[
Eµ(θ|a,s)
[
exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γFold(s′)
])]α
β
]
end
Fold ←F
end
π(a|s) ← 1
Zα(s) ρ(a|s) exp
(
α
β log Zβ(a,s)
)
ψ(θ|a,s) ← 1
Zβ(a,s) µ(θ|a,s) exp
(
βETθ(s′|a,s)
[
Rs′
s,a + γF(s′)
])
return π(a|s), ψ(θ|a,s), F(s)
4 Convergence
Here, we show that the value iteration scheme described through Algorithm 1 converges
to a unique ﬁxed point satisfying Equation (8). To this end, we ﬁrst prove the existence
of a unique ﬁxed point (Theorem 1) following [17,18], and subsequently prove the
convergence of the value iteration scheme presupposing that a unique ﬁxed point exists
(Theorem 2) following [19].
Theorem 1. Assuming a bounded reward functionRs′
s,a, the optimal free-energy vector
F∗(s) is a unique ﬁxed point of Bellman’s equation F∗ = BF∗, where the mapping
B : R|S|→R|S|is deﬁned as in equation (13)
Proof. Theorem 1 is proven through Proposition 1 and 2 in the following.
8 Grau-Moya et al.
Proposition 1. The mapping Tπ,ψ : R|S|→R|S|
Tπ,ψF(s) =Eπ(a|s)
[
−1
αlog π(a|s)
ρ(a|s) +
Eψ(θ|a,s)
[
−1
β log ψ(θ|a,s)
µ(θ|a,s) +
ETθ(s′|a,s)
[
Rs′
s,a + γF(s′)
]]]
. (14)
converges to a unique solution for every policy-belief-pair (π,ψ) independent of the
initial free-energy vector F(s).
Proof. By introducing the matrix Pπ,ψ(s,s′) and the vector gπ,ψ(s) as
Pπ,ψ(s,s′) :=Eπ(a|s)
[
Eψ(θ|a,s) [Tθ(s′|a,s)]
]
,
gπ,ψ(s) :=Eπ(a|s)
[
Eψ(θ|a,s)
[
ETθ(s′|a,s)
[
Rs′
s,a
]
−1
β log ψ(θ|a,s)
µ(θ|a,s)
]
−1
αlog π(a|s)
ρ(a|s)
]
,
Equation (14) may be expressed in compact form: Tπ,ψF = gπ,ψ + γPπ,ψF. By ap-
plying the mapping Tπ,ψ an inﬁnite number of times on an initial free-energy vector F,
the free-energy vector Fπ,ψ of the policy-belief-pair (π,ψ) is obtained:
Fπ,ψ := lim
i→∞
Ti
π,ψF = lim
i→∞
i−1∑
t=0
γtPt
π,ψgπ,ψ + lim
i→∞
γiPi
π,ψF
  
→0
,
which does no longer depend on the initial F. It is straightforward to show that the
quantity Fπ,ψ is a ﬁxed point of the operator Tπ,ψ:
Tπ,ψFπ,ψ = gπ,ψ + γPπ,ψ lim
i→∞
i−1∑
t=0
γtPt
π,ψgπ,ψ
= γ0P0
π,ψgπ,ψ + lim
i→∞
i∑
t=1
γtPt
π,ψgπ,ψ
= lim
i→∞
i−1∑
t=0
γtPt
π,ψgπ,ψ + lim
i→∞
γiPi
π,ψgπ,ψ
  
→0
= Fπ,ψ.
Furthermore, Fπ,ψ is unique. Assume for this purpose an arbitrary ﬁxed point F′such
that Tπ,ψF′= F′, then F′= limi→∞Ti
π,ψF′= Fπ,ψ.
Proposition 2. The optimal free-energy vectorF∗= maxπextψFπ,ψ is a unique ﬁxed
point of Bellman’s equationF∗= BF∗.
Information-Processing Constraints and Model Uncertainty in MDPs 9
Proof. The proof consists of two parts where we assumeext = maxin the ﬁrst part and
ext = minin the second part respectively. Let ext = maxand F∗ = Fπ∗,ψ∗, where
(π∗,ψ∗) denotes the optimal policy-belief-pair. Then
F∗= Tπ∗,ψ∗F∗≤max
π
max
ψ
Tπ,ψF∗
  
=BF∗
=: Tπ′,ψ′F∗Induction
≤ Fπ′,ψ′,
where the last inequality can be straightforwardly proven by induction and exploiting
the fact that Pπ,ψ(s,s′) ∈[0; 1]. But by deﬁnition F∗ = maxπmaxψFπ,ψ ≥Fπ′,ψ′,
hence F∗ = Fπ′,ψ′ and therefore F∗ = BF∗. Furthermore, F∗ is unique. Assume
for this purpose an arbitrary ﬁxed point F′ = Fπ′,ψ′ such that F′ = BF′ with the
corresponding policy-belief-pair (π′,ψ′). Then
F∗= Tπ∗,ψ∗F∗≥Tπ′,ψ′F∗Induction
≥ Fπ′,ψ′ = F′,
and similarly F′≥F∗, hence F′= F∗.
Let ext = minand F∗= Fπ∗,ψ∗. By taking a closer look at Equation (13), it can
be seen that the optimization over ψdoes not depend on π. Then
F∗= Tπ∗,ψ∗F∗≥min
ψ
Tπ∗,ψF∗=: Tπ∗,ψ′F∗Induction
≥ Fπ∗,ψ′.
But by deﬁnition F∗= minψFπ∗,ψ ≤Fπ∗,ψ′, hence F∗= Fπ∗,ψ′. Therefore it holds
that BF∗= maxπminψTπ,ψF∗= maxπTπ,ψ∗F∗and similar to the ﬁrst part of the
proof we obtain
F∗= Tπ∗,ψ∗F∗≤max
π
Tπ,ψ∗F∗
  
=BF∗
=: Tπ′,ψ∗F∗Induction
≤ Fπ′,ψ∗.
But by deﬁnition F∗ = maxπFπ,ψ∗ ≥ Fπ′,ψ∗, hence F∗ = Fπ′,ψ∗ and therefore
F∗= BF∗. Furthermore, Fπ∗,ψ∗ is unique. Assume for this purpose an arbitrary ﬁxed
point F′= Fπ′,ψ′ such that F′= BF′. Then
F′= Tπ′,ψ′F′≤Tπ′,ψ∗F′Induction
≤ Fπ′,ψ∗
Induction
≤ Tπ′,ψ∗F∗≤Tπ∗,ψ∗F∗= F∗,
and similarly F∗≤F′, hence F∗= F′.
Theorem 2. Let ϵ be a positive number satisfying ϵ < η
1−γ where γ ∈(0; 1)is the
discount factor and where uand lare the bounds of the reward functionRs′
s,a such that
l ≤Rs′
s,a ≤u and η = max{|u|,|l|}. Suppose that the value iteration scheme from
Algorithm 1 is run for i = ⌈logγ
ϵ(1−γ)
η ⌉iterations with an initial free-energy vector
F(s) = 0for all s. Then, it holds that maxs|F∗(s) −BiF(s)|≤ ϵ, where F∗refers to
the unique ﬁxed point from Theorem 1.
10 Grau-Moya et al.
Proof. We start the proof by showing that the L∞-norm of the difference vector be-
tween the optimal free-energy F∗and BiF exponentially decreases with the number of
iterations i:
max
s
⏐⏐F∗(s) −BiF(s)
⏐⏐=:
⏐⏐F∗(s∗) −BiF(s∗)
⏐⏐
Eq. (9)
=
⏐⏐⏐⏐max
π
Eπ(a|s∗)
[1
β log Zβ(a,s∗) −1
αlog π(a|s∗)
ρ(a|s∗)
]
−max
π
Eπ(a|s∗)
[1
β log Zi
β(a,s∗) −1
αlog π(a|s∗)
ρ(a|s∗)
]⏐⏐⏐⏐
≤max
π
⏐⏐⏐⏐Eπ(a|s∗)
[1
β log Zβ(a,s∗) −1
β log Zi
β(a,s∗)
]⏐⏐⏐⏐
≤max
a
⏐⏐⏐⏐
1
β log Zβ(a,s∗) −1
β log Zi
β(a,s∗)
⏐⏐⏐⏐
=:
⏐⏐⏐⏐
1
β log Zβ(a∗,s∗) −1
β log Zi
β(a∗,s∗)
⏐⏐⏐⏐
Eq. (11)
=
⏐⏐⏐⏐ext
ψ
Eψ(θ|a∗,s∗)
[
ETθ(s′|a∗,s∗)
[
Rs′
s,a + γF∗(s′)
]
−1
β log ψ(θ|a∗,s∗)
µ(θ|a∗,s∗)
]
−ext
ψ
Eψ(θ|a∗,s∗)
[
ETθ(s′|a∗,s∗)
[
Rs′
s,a + γBi−1F(s′)
]
−1
β log ψ(θ|a∗,s∗)
µ(θ|a∗,s∗)
]⏐⏐⏐⏐
≤max
ψ
⏐⏐⏐⏐Eψ(θ|a∗,s∗)
[
ETθ(s′|a∗,s∗)
[
γF∗(s′) −γBi−1F(s′)
]]⏐⏐⏐⏐
≤γmax
s
⏐⏐F∗(s) −Bi−1F(s)
⏐⏐
Recur.
≤ γimax
s
|F∗(s) −F(s)|≤ γi η
1 −γ,
where we exploit the fact that |extxf(x) −extxg(x)|≤ maxx|f(x) −g(x)|and that
the free-energy is bounded through the reward bounds land uwith η = max{|u|,|l|}.
For a convergence criterion ϵ > 0 such that ϵ ≥ γi η
1−γ, it then holds that i ≥
logγ
ϵ(1−γ)
η presupposing that ϵ< η
1−γ.
5 Experiments: Grid World
This section illustrates the proposed value iteration scheme with an intuitive example
where an agent has to navigate through a grid-world. The agent starts at positionS ∈S
with the objective to reach the goal state G ∈S and can choose one out of maximally
four possible actions a∈{↑,→,↓,←}in each time-step. Along the way, the agent can
encounter regular tiles (actions move the agent deterministically one step in the desired
direction), walls that are represented as gray tiles (actions that move the agent towards
the wall are not possible), holes that are represented as black tiles (moving into the
hole causes a negative reward) and chance tiles that are illustrated as white tiles with a
question mark (the transition probabilities of the chance tiles are unknown to the agent).
Reaching the goal G yields a reward R = +1 whereas stepping into a hole results in
a negative reward R = −1. In both cases the agent is subsequently teleported back
Information-Processing Constraints and Model Uncertainty in MDPs 11
to the starting position S. Transitions to regular tiles have a small negative reward of
R= −0.01. When stepping onto a chance tile, the agent is pushed stochastically to an
adjacent tile giving a reward as mentioned above. The true state-transition probabilities
of the chance tiles are not known by the agent, but the agent holds the Bayesian belief
µ(θs,a|a,s) =Dirichlet
(
Φs′
1
s,a,...,Φ
s′
N(s)
s,a
)
=
N(s)∏
i=1
(θs′
i
s,a)Φ
s′
is,a−1
where transition model is denoted as Tθs,a(s′|s,a) =θs′
s,a and θs,a =
(
θs′
1
s,a...θ
s′
N(s)
s,a
)
and N(s) is the number of possible actions in state s. The data is incorporated into the
model as a count vector
(
Φs′
1
s,a,...,Φ
s′
N(s)
s,a
)
where Φs′
s,a represents the number of times
that the transition (s,a,s ′) has occurred. The prior ρ(a|s) for the actions at every state
is set to be uniform. An important aspect of the model is that in the case of unlimited
observational data, the agent will plan with the correct transition probabilities.
We conducted two experiments with discount factor γ = 0.9 and uniform pri-
ors ρ(a|s) for the action variables. In the ﬁrst experiment, we explore and illustrate
the agent’s planning behavior under different degrees of computational limitations (by
varying α) and under different model uncertainty attitudes (by varying β) with ﬁxed
uniform beliefs µ(θ|a,s). In the second experiment, the agent is allowed to update its
beliefs µ(θ|a,s) and use the updated model to re-plan its strategy.
5.1 The Role of the Parametersα and β on Planning
Figure 1 shows the solution to the variational free energy problem that is obtained by
iteration until convergence according to Algorithm 1 under different values ofαand β.
In particular, the ﬁrst row shows the free energy function F∗(s) (Eq. (8)). The second,
third and fourth row show heat maps of the position of an agent that follows the optimal
policy (Eq. (12)) according to the agent’s biased beliefs (plan) and to the actual transi-
tion probabilities in a friendly and unfriendly environment, respectively. In chance tiles,
the most likely transitions in these two environments are indicated by arrows where the
agent is teleported with a probability of 0.999 into the tile indicated by the arrow and
with a probability of 0.001 to a random other adjacent tile.
In the ﬁrst column of Fig. 1 it can be seen that a stochastic agent ( α = 3.0) with
high model uncertainty and optimistic attitude (β = 400) has a strong preference for the
broad corridor in the bottom by assuming favorable transitions for the unknown chance
tiles. This way the agent also avoids the narrow corridors that are unsafe due to the
stochasticity of the low-αpolicy. In the second column of Fig. 1 with low α = 3and
high model uncertainty with pessimistic attitude β = −400, the agent strongly prefers
the upper broad corridor because unfavorable transitions are assumed for the chance
tiles. The third column of Fig. 1 shows a very pessimistic agent (β = −400) with high
precision (α= 11) that allow the agent to safely choose the shortest distance by select-
ing the upper narrow corridor without risking any tiles with unknown transitions. The
fourth column of Fig. 1 shows a very optimistic agent (β = 400) with high precision. In
this case the agent chooses the shortest distance by selecting the bottom narrow corridor
that includes two chance tiles with unknown transition.
12 Grau-Moya et al.
Free Energy
S G
? ?
? ?
? ?
α= 3 β= 400
PlanS G
? ?
? ?
? ?
Friendly Environment
S G
0 1 2 3 4 5 6 7 8
0
1
2
3
4
5
6
7
8Unfriendly Environment
S G
S G
? ?
? ?
? ?
α= 3 β= − 400
S G
? ?
? ?
? ?
S G
S G
S G
? ?
? ?
? ?
α= 11 β= − 400
S G
? ?
? ?
? ?
S G
S G
S G
? ?
? ?
? ?
α= 11 β= 400
S G
? ?
? ?
? ?
S G
S G
1
0
1
Free Energy
0.0
1.0
Density
0.0
1.0
Density
0.0
1.0
Density
Fig. 1.The four different rows show free energy values and heat-maps of planned trajectories ac-
cording to the agent’s beliefs over state-transitions in chance tiles, heat-maps of real trajectories
in a friendly environment and in an unfriendly environment respectively. The Start-position is
indicated by S and the goal state is indicated by G. Black tiles represent holes with negative re-
ward, gray tiles represent walls and chance tiles with a question mark have transition probabilities
unknown to the agent. The white tiles with an arrow represent the most probable state-transition
in chance tiles (as speciﬁed by the environment). Very small arrows in each cell encode the pol-
icy π(a|s) (the length of each arrow encodes the probability of the corresponding action under
the policy, highest probability action is indicated as a red arrow). The heat map is constructed
by normalizing the number of visits for each state over 20000 steps, where actions are sampled
from the agent’s policy and state-transitions are sampled according to one of three ways: the sec-
ond row according to the agent’s belief over state-transitions ψ(θ|a,s), in the third and fourth
row according to the actual transition probabilities of a friendly and an unfriendly environment
respectively. Different columns show differentαand βcases.
Information-Processing Constraints and Model Uncertainty in MDPs 13
5.2 Updating the Bayesian Posteriorµ with Observations from the Environment
Similar to model identiﬁcation adaptive controllers that perform system identiﬁcation
while the system is running [20], we can use the proposed planning algorithm also in
a reinforcement learning setup by updating the Bayesian beliefs about the MDP while
executing always the ﬁrst action and replanning in the next time step. During the learn-
ing phase, the exploration is governed by both factors α and β, but each factor has a
different inﬂuence. In particular, lower α-values will cause more exploration due to the
inherent stochasticity in the agent’s action selection, similar to an ϵ-greedy policy. If α
is kept ﬁxed through time, this will of course also imply a “suboptimal” (i.e. bounded
optimal) policy in the long run. In contrast, the parameter β governs exploration of
states with unknown transition-probabilities more directly and will not have an impact
on the agent’s performance in the limit, where sufﬁcient data has eliminated model
uncertainty. We illustrate this with simulations in a grid-world environment where the
agent is allowed to update its beliefs µ(θ|a,s) over the state-transitions every time it
enters a chance tile and receives observation data acquired through interaction with the
environment—compare left panels in Figure 2. In each step, the agent can then use the
updated belief-models for planning the next action.
0 1 2 3 4
0
1
2
3
4
S GS GS G
Environment
0 1 2 3 4
0
1
2
3
4
S G?
?
?
?
Pessimistic
Optimal
0 1 2 3 4
0
1
2
3
4
S G?
?
?
?
Optimistic
0 1 2 3 4
0
1
2
3
4
S GS GS G
0 50 100 150 200 250 300
Step
0
5
10
15
20
25
30
35
40Data points acquired
α fixed
0 50 100 150 200 250 300
Step
0.15
0.10
0.05
0.00
0.05
0.10
Average reward α= 12.0 β= 0.2
α= 12.0 β= 5.0
α= 12.0 β= 20.0
0 50 100 150 200 250 300
Step
0
5
10
15
20
25
30
35Data points acquired
β fixed
0 50 100 150 200 250 300
Step
0.15
0.10
0.05
0.00
0.05
0.10
Average reward α= 5.0 β= 0.2
α= 8.0 β= 0.2
α= 12.0 β= 0.2
Fig. 2.The effect of αand βwhen updating beliefs over 300 interaction steps with the environ-
ment. The four panels on the left show the grid-world environment and the pertaining optimal
policy if the environment is known. The lower left panels show paths that the agent could take
depending on its attitude towards model uncertainty. The panels on the right show the number of
acquired data points, that is the number of times a chance tile is entered, and the average reward
(bottom panels) for ﬁxed α(varying β) or ﬁxed β(varying α). The average reward at each step
is computed as follows. Each time the agent observes a state-transition in a chance tile and up-
dates its belief model, 10 runs of length 2000 steps are sampled (using the agent’s current belief
model). The average reward (bold lines) and standard-deviation (shaded areas) across these 10
runs are shown in the ﬁgure.
14 Grau-Moya et al.
Figure 2 (right panels) shows the number of data points acquired (each time a chance
tile is visited) and the average reward depending on the number of steps that the agent
has interacted with the environment. The panels show several different cases: while
keeping α = 12.0 ﬁxed we test β = (0.2,5.0,20.0) and while keeping β = 0.2 ﬁxed
we test α= (5.0,8.0,12.0). It can be seen that lowerαleads to better exploration, but it
can also lead to lower performance in the long run—see for example rightmost bottom
panel. In contrast, optimistic β values can also induce high levels of exploration with
the added advantage that in the limit no performance detriment is introduced. However,
high βvalues can in general also lead to a detrimental persistence with bad policies, as
can be seen for example in the superiority of the low- β agent at the very beginning of
the learning process.
6 Discussion and Conclusions
In this paper we are bringing two strands of research together, namely research on
information-theoretic principles of control and decision-making and robustness princi-
ples for planning under model uncertainty. We have devised a uniﬁed recursion princi-
ple that extends previous generalizations of Bellman’s optimality equation and we have
shown how to solve this recursion with an iterative scheme that is guaranteed to con-
verge to a unique optimum. In simulations we could demonstrate how such a combina-
tion of information-theoretic policy and belief constraints that reﬂect model uncertainty
can be beneﬁcial for agents that act in partially unknown environments.
Most of the research on robust MDPs does not consider information-processing con-
straints on the policy, but only considers the uncertainty in the transition probabilities by
specifying a set of permissible models such that worst-case scenarios can be computed
in order to obtain a robust policy [11,12]. Recent extensions of these approaches in-
clude more general assumptions regarding the set properties of the permissible models
and assumptions regarding the data generation process [13]. Our approach falls inside
this class of robustness methods that use a restricted set of permissible models, because
we extremize the biased belief ψ(θ|a,s) under the constraint that it has to be within
some information bounds measured by the Kullback-Leibler divergence from a refer-
ence Bayesian posterior. Contrary to these previous methods, our approach additionally
considers robustness arising from the stochasticity in the policy.
Information-processing constraints on the policy in MDPs have been previously
considered in a number of studies [3,21,22,17], however not in the context of model
uncertainty. In these studies a free energy value recursion is derived when restricting
the class of policies through the Kullback-Leibler divergence and when disregarding
separate information-processing constraints on observations. However, a small number
of studies has considered information-processing constraints both for actions and ob-
servations. For example, Polani and Tishby [8] and Ortega and Braun [6] combine both
kinds of information costs. The ﬁrst cost formalizes an information-processing cost in
the policy and the second cost constrains uncertainty arising from the state transitions
directly (but crucially not the uncertainty in the latent variables). In both information-
processing constraints the cost is determined as a Kullback-Leibler divergence with
respect to a reference distribution. Speciﬁcally, the reference distribution in [8] is given
Information-Processing Constraints and Model Uncertainty in MDPs 15
by the marginal distributions (which is equivalent to a rate distortion problem) and in
[6] is given by ﬁxed priors. The Kullback-Leibler divergence costs for the observations
in these cases essentially correspond to a risk-sensitive objective. While there is a rela-
tion between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach
is at least twofold. First, it allows combining information-processing constraints on the
policy with model uncertainty (as formalized by a latent variable). Second, it provides
a natural setup to study learning.
The algorithm presented here and Bayesian models in general [9] are computation-
ally expensive as they have to compute possibly high-dimensional integrals depending
on the number of allowed transitions for action-state pairs. However, there have been
tremendous efforts in solving unknown MDPs efﬁciently, especially by sampling meth-
ods [26,27,28]. An interesting future direction to extend our methodology would there-
fore be to develop a sampling-based version of Algorithm 1 to increase the range of
applicability and scalability [29]. Moreover, such sampling methods might allow for
reinforcement learning applications, for example by estimating free energies through
TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic
methods for adaptive control [20].
Acknowledgments This study was supported by the DFG, Emmy Noether grant BR4164/1-
1. The code was developed on top of the RLPy library [33].
References
1. Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA,
1 edition, 1957.
2. Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural
information processing systems, pages 1369–1376, 2006.
3. Emanuel Todorov. Efﬁcient computation of optimal actions. Proceedings of the national
academy of sciences, 106(28):11478–11483, 2009.
4. Daniel A Braun, Pedro A Ortega, Evangelos Theodorou, and Stefan Schaal. Path integral
control and bounded rationality. In Adaptive Dynamic Programming And Reinforcement
Learning (ADPRL), 2011 IEEE Symposium on, pages 202–209. IEEE, 2011.
5. Bart van den Broek, Wim Wiegerinck, and Hilbert J. Kappen. Risk sensitive path integral
control. In UAI, 2010.
6. Pedro A Ortega and Daniel A Braun. Thermodynamics as a theory of decision-making with
information-processing costs. In Proc. R. Soc. A , volume 469, page 20120683. The Royal
Society, 2013.
7. Pedro A Ortega and Daniel A Braun. Generalized thompson sampling for sequential
decision-making and causal inference. Complex Adaptive Systems Modeling, 2(1):2, 2014.
8. Naftali Tishby and Daniel Polani. Information theory of decisions and actions. InPerception-
action cycle, pages 601–636. Springer, 2011.
9. Michael O’Gordon Duff. Optimal Learning: Computational procedures for Bayes-adaptive
Markov decision processes. PhD thesis, University of Massachusetts Amherst, 2002.
10. Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approx-
imation in value function estimates. Management Science, 53(2):308–322, 2007.
11. Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with
uncertain transition matrices. Operations Research, 53(5):780–798, 2005.
16 Grau-Moya et al.
12. Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research ,
30(2):257–280, 2005.
13. Wolfram Wiesemann, Daniel Kuhn, and Berc ¸ Rustem. Robust markov decision processes.
Mathematics of Operations Research, 38(1):153–183, 2013.
14. Istv ´an Szita and Andr ´as L ˝orincz. The many faces of optimism: a unifying approach. In
Proceedings of the 25th international conference on Machine learning , pages 1048–1055.
ACM, 2008.
15. Istv ´an Szita and Csaba Szepesv ´ari. Model-based reinforcement learning with nearly tight
exploration complexity bounds. In Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 1031–1038, 2010.
16. Lars Peter Hansen and Thomas J Sargent. Robustness. Princeton university press, 2008.
17. Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in mdps.
In Decision Making with Imperfect Decision Makers, pages 57–74. Springer, 2012.
18. DP Bertsekas and JN Tsitsiklis. Neuro-dynamic programming. 1996.
19. Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in ﬁnite
mdps: Pac analysis. The Journal of Machine Learning Research, 10:2413–2444, 2009.
20. Karl J ˚Astr¨om and Bj¨orn Wittenmark. Adaptive control. Courier Corporation, 2013.
21. Hilbert J Kappen. Linear theory for control of nonlinear stochastic systems. Physical review
letters, 95(20):200201, 2005.
22. J Peters, K M ¨ulling, Y Altun, Fox D Poole, et al. Relative entropy policy search. In Twenty-
Fourth National Conference on Artiﬁcial Intelligence (AAAI-10) , pages 1607–1612. AAAI
Press, 2010.
23. Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforce-
ment learning. Neural computation, 26(7):1298–1328, 2014.
24. Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. In Ad-
vances in Neural Information Processing Systems, pages 233–241, 2012.
25. Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust
decision-making: a cvar optimization approach. In Advances in Neural Information Pro-
cessing Systems, pages 1522–1530, 2015.
26. St ´ephane Ross, Joelle Pineau, Brahim Chaib-draa, and Pierre Kreitmann. A bayesian ap-
proach for learning and planning in partially observable markov decision processes. The
Journal of Machine Learning Research, 12:1729–1770, 2011.
27. Arthur Guez, David Silver, and Peter Dayan. Efﬁcient bayes-adaptive reinforcement learning
using sample-based search. In Advances in Neural Information Processing Systems , pages
1025–1033, 2012.
28. Arthur Guez, David Silver, and Peter Dayan. Scalable and efﬁcient bayes-adaptive reinforce-
ment learning based on monte-carlo tree search. Journal of Artiﬁcial Intelligence Research,
pages 841–883, 2013.
29. Pedro A Ortega, Daniel A Braun, and Naftali Tishby. Monte carlo methods for exact & efﬁ-
cient solution of the generalized optimality equations. In Robotics and Automation (ICRA),
2014 IEEE International Conference on, pages 4322–4327. IEEE, 2014.
30. Roy Fox, Ari Pakman, and Naftali Tishby. G-learning: Taming the noise in reinforcement
learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.
31. Pedro A Ortega and Daniel A Braun. A minimum relative entropy principle for learning and
acting. Journal of Artiﬁcial Intelligence Research, pages 475–511, 2010.
32. Pedro A Ortega and Daniel A Braun. A bayesian rule for adaptive control based on causal
interventions. In 3d Conference on Artiﬁcial General Intelligence (AGI-2010). Atlantis Press,
2010.
33. Alborz Geramifard, Christoph Dann, Robert H Klein, William Dabney, and Jonathan P How.
Rlpy: A value-function-based reinforcement learning framework for education and research.
Journal of Machine Learning Research, 16:1573–1578, 2015.