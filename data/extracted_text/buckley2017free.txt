The free energy principle for action and perception: A
mathematical review
Christopher L. Buckleya,1,˚, Chang Sub Kim b,1, Simon McGregora, Anil K.
Setha
aSchool of Engineering and Informatics, Evolutionary and Adaptive Systems Group,
University of Sussex, Brighton, BN1 9QJ, UK
bDepartment of Physics, Chonnam National University, Gwangju 61186, Republic of Korea
Abstract
The ‘free energy principle’ (FEP) has been suggested to provide a uniﬁed the-
ory of the brain, integrating data and theory relating to action, perception, and
learning. The theory and implementation of the FEP combines insights from
Helmholtzian ‘perception as inference’, machine learning theory, and statistical
thermodynamics. Here, we provide a detailed mathematical evaluation of a sug-
gested biologically plausible implementation of the FEP that has been widely
used to develop the theory. Our objectives are (i) to describe within a single
article the mathematical structure of this implementation of the FEP; (ii) pro-
vide a simple but complete agent-based model utilising the FEP; (iii) disclose
the assumption structure of this implementation of the FEP to help elucidate
its signiﬁcance for the brain sciences.
Keywords: Free energy principle, perception, action, inference, Bayes
1. Introduction
The brain sciences have long searched for a ‘uniﬁed brain theory’ capable
of integrating experimental data relating to, and disclosing the relationships
among, action, perception, and learning. One promising candidate theory that
has emerged over recent years is the ‘free energy principle’ (FEP) [1, 2]. The5
˚Corresponding author
1Joint contribution
Preprint submitted to Journal of Mathematical Psychology May 26, 2017
arXiv:1705.09156v1  [q-bio.NC]  24 May 2017
FEP is ambitious in scope and attempts to extend even beyond the brain science
to account for adaptive biological processes spanning an enormous range of time
scales, from millisecond neuronal dynamics to the tens of millions of years span
covered by evolutionary theory [3, 1].
The FEP has an extensive historical pedigree. Some see its origins starting10
with Helmholtz’ proposal that perceptions are extracted from sensory data by
probabilistic modelling of their causes [4]. Helmholtz also originated the notion
of thermodynamic free energy, providing a second key inspiration for the FEP2.
These ideas have reached recent prominence in the ‘Bayesian brain’ and ‘predic-
tive coding’ models, according to which perceptions are the results of Bayesian15
inversion of a causal model, and causal models are updated by processing of
sensory signals according to Bayes’ rule [5, 6, 7, 8]. However, the FEP naturally
accommodate and description of both action and perception within the same
framework [9] thus other see it’s origins in 20th-century cybernetic principles of
homeostasis and predictive control [10].20
A recognisable precursor to the FEP as applied to brain operation was de-
veloped by Hinton and colleagues, who showed that a function resembling free
energy could be used to implement a variation of the expectation-maximization
algorithm [11], as well as for training autoencoders [12] and learning population
codes [13]. Because these algorithms integrated Bayesian ideas with a notion of25
free energy, Hinton named them as ‘Helmholtz machines’ [14]. The FEP builds
on these insights to provide a global uniﬁed theory of cognition. Essentially,
this work generalizes these results by noting that all (viable) biological organ-
isms resist a tendency to disorder as shown by their homeostatic properties (or,
more generally, their autopoietic properties), and must therefore minimize the30
occurrence of events which are atypical (‘surprising’) in their habitable envi-
ronment. For example, successful ﬁsh typically ﬁnd themselves surrounded by
water, and very atypically ﬁnd themselves out of water, since being out of wa-
2Thermodynamic free energy describes the macroscopic properties of nature, typically in
thermal equilibrium where it takes minimum values, in terms of a few tractable variables.
2
ter for an extended time will lead to a breakdown of homeostatic (autopoietic)
relations. Because the distribution of ‘surprising’ events is in general unknown35
and unknowable, organisms must instead minimise a tractable proxy, which ac-
cording to the FEP turns out to be ‘free energy’. Free energy in this context
is an information-theoretic construct that (i) provides an upper bound on the
extent to which sensory data is atypical (‘surprising’) and (ii) can be evaluated
by an organsim, because it depends eventually only on sensory input and an40
internal model of the environmental causes of sensory input. While at its most
general this theory can arguably be applied to all life-processes [15], it provides
a particularly appealing account of brain function. Speciﬁcally it describes how
neuronal processes could implement free energy minimisation either by changing
sensory input via action on the world, or by updating internal models via per-45
ception, with implications for understanding the dynamics of, and interactions
among action, perception, and learning. These arguments have been developed
in a series of papers which have appeared over the course of the last several
years [16, 17, 18, 19, 20, 21, 22, 23, 9, 24, 25, 26, 27, 28, 29].
The FEP deserves close examination because of the claims made for its50
explanatory power. It has been suggested that the FEP discloses novel and
straightforward relationships among fundamental psychological concepts such
as memory, attention, value, reinforcement, and salience [2]. Even more gen-
erally, the FEP is claimed to provide a “mathematical speciﬁcation of ‘what’
the brain is doing” [[2], p.300], to unify perception and action [9], and to pro-55
vide a basis for integrating several general brain theories including the Bayesian
brain hypothesis, neural Darwinism, Hebbian cell assembly theory, and optimal
control and game theory [1]. The FEP has even been suggested to underlie
Freudian constructs in psychoanalysis [25].
Our purpose here is ﬁrst to supply a mathematical appraisal of the FEP,60
which we hope will facilitate evaluation of claims such as those listed above;
note that we do not attempt to resolve any such claims here. A mathematical
appraisal is worthwhile because the FEP combines advanced concepts from sev-
eral ﬁelds, particularly statistical physics, probability theory, machine learning,
3
and theoretical neuroscience. The mathematics involved is non-trivial and has65
been presented over diﬀerent stages of evolution and using varying notations.
Here we ﬁrst provide a complete technical account of the FEP, based on a his-
tory of publications through which the framework has been developed. Second
we provide a complete description of simple agent based model working under
this formulation. While we note that several other agent based models have70
been presented they have often made use of existing toolboxes which, while
powerful, have perhaps clouded a fuller understanding of the FEP. Lastly we
use our account to identify the assumption structure of the FEP, highlighting
several instances in which non-obvious assumptions are required.
In the next section we provide a brief overview of the FEP followed by75
detailed guide to the technical content covered in the rest of the paper.
2. An overview of the FEP
Broadly the FEP is an account of cognition derived from the consideration of
how biological organisms maintain their state away from thermodynamic equi-
librium with their ambient surroundings. The argument runs that organisms
are mandated, by the very fact of their existence, to minimize the dispersion of
their constituent states. The atypicality of an event can be quantiﬁed by the
negative logarithm of the probability of its sensory data, which is commonly
known in information theory as ‘surprise’ or ‘self-information’ and the overall
atypicality of an organism’s exchanges with its environment can be quantiﬁed
as a total lifetime surprise [1, 2]. The term surprise has caused much confusion
since it is distinct from the subjective psychological phenomenon of surprise.
Instead, it is a measure of how atypical a sensory exchange is. This kind of sur-
prise can be quantiﬁed using the standard information-theoretic log-probability
measure
´ln ppϕq
where ppϕqis the probability of observing some particular sensory data ϕ in a
typical (habitable) environment. Straightforwardly this quantity is large if the
4
probability of the observed data is small and zero if the data is fully expected,80
i.e., probability 1. To avoid confusion with the common-sense meaning of the
word ‘surprise’ we will refer to it as “surprisal” or “sensory surprisal”.
2.1. R- and G- Densities
The FEP argues organisms cannot minimise surprisal directly, but instead
minimise an upper bound called ‘free energy’. To achieve this it is proposed that85
all (well adapted) biological organisms maintain a probabilistic model of their
typical (habitable) environment (which includes their body), and attempt to
minimize the occurrence of events which are atypical in such an environment as
measured by this model. Two key probability densities are necessary to evaluate
free energy. First it is suggested that organisms maintain an implicit account of90
a best guess at the relevant variables that comprise their environment (i.e. those
variables which cause its sensory data). This account is in the form of a prob-
ability distribution over all possible values of those variables, like a Bayesian
belief; this model is instantiated, and parameterised, by physical variables in
the organism’s brain such as neuronal activity and synaptic strengths, respec-95
tively. When an organism receives sensory signals, it updates this distribution
to better reﬂect the world around it, allowing it to eﬀectively model its envi-
ronment. In other words, the organism engages in a process similar to Bayesian
inference regarding the state of its environment, based on sensory observations.
This internal model of environmental states is called the “recognition density”100
or the R-density. In order to update the R-density appropriately, the organ-
ism needs some implicit assumptions about how diﬀerent environmental states
shape sensory input. These assumptions are presumed to be in the form of a
joint probability density between sensory data and environmental variables, the
“generative density”, or G-density. This density is also presumed to be encoded105
within the organsims brain. As we will see, following a Bayesian formalism, this
joint density is calculated as the product of two densities; a likelihood describ-
ing the probability of sensory input given some environmental state and a prior
describing the organisms current ”beliefs” of the probability distribution over
5
environmental states.110
2.2. Minimising Free Energy
Free energy is a (non-negative) quantity formed from the Kullback-Leibler
divergence between the R- and G-densities. Consequently, it is not a directly
measurable physical quantity: it depends on an interpretation of brain variables
as encoding notional probability densities. Note: the quantity ’free energy’115
is distinct from thermodynamic free energy thus here we will refer to it as
informational free energy (IFE).
Minimisation of IFE has two functional consequences. First it provides an
upper bound on sensory surprisal. This allows organisms to estimate the disper-
sion of their constituent states and is central to the interpretation of FEP as an120
account of life processes [1]. However, IFE minimisation also plays a central role
in a Bayesian approximation method. Speciﬁcally ideal (exact) Bayesian infer-
ence, in general, involves evaluating diﬃcult integrals and thus a core hypothesis
of the FEP framework is that the brain implements approximate Bayesian in-
ference in an analogous way to a method known as variational Bayes. It can125
be shown that minimising IFE makes the R-density a good approximation to
posterior density of environmental variables given sensory data. Under this in-
terpretation the surprisal term in the IFE becomes more akin to the negative
of log model evidence deﬁned in more standard implementations of variational
Bayes [30].130
2.3. The Action-Perception Cycle
Minimising IFE by updating the R-density provides an upper-bound on sur-
prisal but cannot minimise it directly. The FEP suggests that organisms also
act on their environment to change sensory input, and thus minimise surprisal
indirectly [1, 2]. The mechanism underlying this process is formally symmet-135
ric to perceptual inference, i.e., rather than inferring the cause of sensory data
an organism must infer actions that best make sensory data accord with an
internal environmental model [9]. Thus, the mechanism is often referred to as
6
active inference. Formally, action allows an organisms to avoid the dispersion of
its constituent states and is suggested to underpin a form of, homoeostasis, or140
perhaps more precisely homeorhesis [10]. However, equivalently, one can view
action as satisfying hard constraints encoded in the organisms environmental
model [9]. Here expectations in the organism’s G-density (its ”beliefs” about
the world) cannot be met directly by perception and thus an organism must act
to satisfy them. In eﬀect these expectations eﬀectively encode the organism’s145
desires on environmental dynamics. For example, the organisms model may
prescribe it maintains a desired local temperature; we will see an example of
this in Section 7. Here action is seen as more akin to control [10] where be-
haviour arises from a process of minimising deviations between the organisms
actual and a desired trajectory [9]. Note: an implicit assumption here is that150
these constraints are conducive to the organisms survival [1, 2], perhaps arrived
at by an evolutionary process. Other diﬀerent roles for action within the FEP
have also been suggested, e.g., action as a process of experimentation with the
goal to disambiguate competing environmental models [31, 10]. However, here
we only consider action as a source of control [9, 10].155
2.4. Predictive Coding
There at least two general ways to view most FEP-based research. First the
central theory [17] which oﬀers a particular explanation of cognition in terms
of Bayesian inference. Second a biologically plausible process theory of how the
relevant probability densities could be parameterised by variables in the brain160
(i.e. a model of what it is that brain variables encode), and how the variables
should be expected to change in order to minimize IFE. The most commonly
used implementation of the FEP, and the one we focus on here, is strongly anal-
ogous with the predictive coding framework [6]. Speciﬁcally predictive coding
theory constitutes one plausible mechanism whereby an organism could update165
its environmental model (R-density) given a belief of how its environment works
(G-density). The concept of predictive coding overturns classical notions of per-
ception (and cognition) as a largely bottom-up process of evidence accumulation
7
or feature detection driven by impinging sensory signals, proposing instead that
perceptual content is determined by top-down predictive signals arising from170
multi-level generative models of the environmental causes of sensory signals,
which are continually modiﬁed by bottom-up prediction error signals commu-
nicating mismatches between predicted and actual signals across hierarchical
levels (see [8] for a nice review). In the context of the FEP the R-density is
updated using a hierarchical predictive coding (see Section 8). This has several175
theoretical beneﬁts. Firstly, under suitable assumption IFE becomes formally
equivalent to prediction error (weighted by conﬁdence terms), which can read-
ily be computed in neural hardware. Hierarchical coding also provides a very
generic prior which allows high-level abstract sensory features to be learned from
the data, in a manner similar to deep learning nets [32]. Finally, the sense in180
which the brain models the environment can be conceptualised in a very direct
way as the prediction of sensory signals. We will also see in Section 8 that this
implementation suggests that we do not even need to know what environmen-
tal features the R- and G-densities constitute a model of. Given appropriate
assumptions, the formalism can be rewritten to depend only on predictions of185
sensory data, along with recursive predictions of the brain variables which en-
code those predictions.
2.5. A technical guide
In the rest of this work we review the FEP in detail but ﬁrst we provide a
detailed guide to each section. Most of what we present is related to standard190
concepts and techniques in statistical mechanics and machine learning. How-
ever, here we present these ideas in detail to make clear their role for the FEP
as theory of biological systems.
In Section 3 we describe the core technical concepts of FEP including the
R-density, G- density, and IFE. We show how minimising IFE has two conse-195
quences. First, it makes the R-density a better estimate of posterior beliefs
about environmental state given sensory data, thus implementing approximate
Bayesian inference. Second, it makes the IFE itself an upper-bound on sensory
8
surprisal.
In Section 4 we discuss the approximations that allow the brain to explicitly200
instantiate the R-density and thus specify IFE. Speciﬁcally, we make the ap-
proximation that the R-density take Gaussian form, the Laplace approximation,
and that brain states, e.g. neural activity, represent the suﬃcient statistics of
this distribution (mean and variance). Utilising this form for the R-density and
various other approximations we derive an expression for the IFE in terms of the205
unknown G-density only; we refer to this approximation as the Laplace encoded
energy. The derivations in this section are done for the univariate Gaussian
case, but we give an expression for the full multivariate case at the end of the
section.
In Section 5 we look at diﬀerent forms for the G-density. We start by specify-210
ing simple generative models which comprise the brain’s model of how the world
works, i.e., how sensory data is caused by environmental (including bodily) vari-
ables. We utilise these generative models to specify the brain’s expectation on
environmental states given sensory data in terms of a Gaussian distribution
parametrised by expected means and variances (inverse precisions) on brain215
states. Combining this with the result of the last section allows us to write an
expression for Laplace encoded-energy as a quadratic sum of prediction errors
(diﬀerence between expected and actual brain states given sensory data) modu-
lated by expected variances (or inverse precisions), in line with predictive-coding
process theories. Initially we show this for a static generative model but extend220
it to include dynamic generative models by introducing the concept of gener-
alised motion. Again we derive the results for the univariate case but provide
expressions for the multivariate case.
In Section 6 we show how the brain could dynamically minimises IFE. Specif-
ically, we describe how brain states are optimised to minimise IFE through225
gradient descent. We discuss complications of this method when considering
dynamical generative models.
Section 7 demonstrates how action can be implemented as a similar gradi-
ent descent scheme. Speciﬁcally we show how, given a suitable inverse model,
9
actions are chosen to change sensation such that they minimise IFE. We ground230
this idea, and the mechanisms for perception described in prior sections, in a
simple agent based simulation. We show how an agent with an appropriate
model of the environment, can combine action and perception to minimise IFE
constrained both by the environment and its own expectations on brain states.
In Section 8 we extend the formalism to include learning. Speciﬁcally we235
show how the brain could modify and learn the G-density. To facilitate this we
describe notion of hierarchical generative models which involve empirical priors.
We lastly describe a gradient descent schemes which allows the brain to infer
parameters and hyperparameters of the IFE and thus allow the brain to learn
environmental dynamics based on sensory data.240
Finally, Section 9 summarizes the FEP and discusses the implications of its
assumption structure for the brain sciences.
3. Informational free energy
We start by considering a world that consists of a brain and its surrounding
body/environment. For the rest of the presentation we refer to the body and245
environment as simply the environment and use this to refer to all processes
outside of the brain. The brain is distinguished from its environment by an in-
terface which is not necessarily a physical boundary but rather may be deﬁned
functionally; thus the boundary could reside at the sensory and motor surfaces
rather than, for example, at the limits of the cranial cavity. The environment is250
characterized by states, denoted collectively as tϑu, which include well-deﬁned
characteristics like temperature or the orientation of a joint but also unknown
and uncontrollable states, all evolving according to physical laws. The envi-
ronmental states, as exogenous stimuli, give rise to sensory inputs for which
the symbols tϕuare designated collectively. These sensory inputs are assumed255
to reside at the functional interface distinguishing the brain from the environ-
ment, and we assume a many-to-one (non-bijective) mapping between tϑ}and
tϕu[33]. We further assume that the brain, in conjunction with the body, can
10
perform actions to modify sensory signals.
We assume that the important states of the environment cannot be directly
perceived by an organism but instead must be inferred by a process of Bayesian
inference. Speciﬁcally, the goal of the agent is to determine the probability
of environmental states given its sensory input. To achieve this we assume
organism’s encodes prior beliefs about these states characterized by the joint
density ppϑ,ϕqor G-density. Where the G-density can be factorized into (with
respect to ϑ), the prior ppϑq(corresponding to the organism’s ”beliefs” about the
world before sensory input is received) and a likelihood ppϕ|ϑq(corresponding to
the organism’s assumptions about how environmental dynamics cause sensory
input),
ppϑ,ϕq“ ppϕ|ϑqppϑq. (1)
Give an observation, ϕ “ φ (e.g. some particular sensory data), a posterior260
belief about the environment can then be written as ppϑ|ϕ“φq. This quantity
can be calculated using the prior and likelihood using Bayes theorem as,
ppϑ|φq“ 1
ppϕ“φqppφ|ϑqppϑq“ ppφ|ϑqppϑqş
ppφ|ϑqppϑqdϑ. (2)
All the probability densities are assumed to be normalized as
ż
dϑ
ż
dϕ ppϑ,ϕq“
ż
dϑ ppϑq“
ż
dϕ ppϕq“ 1,
where ppϑqand ppϕqare the reduced or marginal probability-densities conform-
ing to
ppϑq“
ż
dϕ ppϑ,ϕq and ppϕq“
ż
dϑ ppϑ,ϕq. (3)
To calculate the posterior probability it is necessary to evaluate the marginal
integral,
ş
ppφ|ϑqppϑqϑ, in the denominator of equation (2). However, this is
often diﬃcult, if not practically intractable. For example, when continuous
functions are used to approximate the likelihood and prior, the integral may
be analytically intractable. Or in the discrete case, when this integral reduces
to a sum, the number of calculations may grow exponentially with the number
of states. Variational Bayes (sometimes known as ‘ensemble learning’) is a
11
method for (approximately) determining ppϑ|ϕqwhich avoids the evaluation of
this integral, by introducing an optimization problem [20]. Such an approach
requires an auxiliary probability density, representing the current ‘best guess’
of the causes of sensory input. This is the recognition density, or R-density,
introduced in the overview. Again the R-density is also normalised as:
ż
qpϑqdϑ“1. (4)
We can construct a measure of the diﬀerence between this density and the true
posterior in terms of an information-theoretic divergence, e.g., the Kullback-
Leibler divergence [34], i.e.,
DKLpqpϑq||ppϑ|ϕqq“
ż
dϑ qpϑqln qpϑq
ppϑ|ϕq (5)
An R-density that minimises this divergence would provide a good approxi-
mation to the true posterior. But obviously we cannot evaluate this quantity
because we still do not know the true posterior. However, we can rewrite this
equation as,
DKLpqpϑq||ppϑ|ϕqq“ F `ln ppϕq (6)
where we have deﬁned F as the informational free energy (IFE),
F ”
ż
dϑ qpϑqln qpϑq
ppϑ,ϕq. (7)
Note here we have introduced the G-density to the denominator on the right-
hand side. In contrast to equation (5) we can evaluate IFE directly because it
depends only on the R-density, which we are free to specify, and the G-density,265
i.e., a model of the environmental causes of sensory input. Furthermore, the
second term on the right-hand side in equation (6) only depends on sensory
input and is independent of the form of the R-density. Thus, minimising equa-
tion (7) with respect to the R-density will also minimise the Kullback-Leibler
divergence between the R-density and the true posterior. Thus, the result of270
this minimisation will make the R-density approximate the true posterior.
The minimisation of IFE also suggests an indirect way to estimate surprisal.
12
Speciﬁcally according to Jensen’s inequality [34], the Kullback-Leibler diver-
gence is always greater than zero. This implies the inequality,
F ě´ ln ppϕq. (8)
which means that the IFE also provides an upper bound on the surprisal as
described in Section 1. However, note the IFE is equal to surprisal only when
the R-density qpϑqbecomes identical with the posterior density ppϑ|ϕq; i.e., it
is this condition that speciﬁes when IFE provides a tight bound on surprisal275
(see Section 2). Furthermore, while this process furnishes the organism with
an approximation of surprisal it does not minimise it. Instead the organism
can minimise IFE further by minimising surprisal indirectly by acting on the
environment and changing sensory input, see Section 7.
Note: formally ppϕq, which describes the agent’s internal (implicit) prob-280
abilistic predictions of sensory inputs, should be written as as ppϕ|mq. This
follows a convention in Bayesian statistics to indicate that a reasoner must be-
gin with some arbitrary prior before it can learn anything; ppϕq indicates the
prior assigned to p ab initio by agent m. However, this notation is unwieldly
and does not change the derivations that follow thus we will omit this for the285
rest of the presentation.
There are several analogies between the terms in the formalism above and
the formulation of Helmoltz’ thermodynamic free energy. These terms can serve
as useful substitutions in the derivation to come and, thus, we describe them
here. Speciﬁcally when the G-density is unpacked in equation (7), the IFE splits
into two terms,
F “
ż
dϑ qpϑqEpϑ,ϕq`
ż
dϑ qpϑqln qpϑq (9)
where, formally speaking, the ﬁrst term in equation (9) is an average of the
quantity
Epϑ,ϕq”´ ln ppϑ,ϕq (10)
over the R-density qpϑqand the second term is essentially the negative entropy
associated with the recognition density. By analogy with Helmoltz’ thermody-
13
namic free energy the ﬁrst term in equation (9) is called average energy [Ac-
cordingly, Epϑ,ϕq itself may be termed the energy] and the second term the290
negative of entropy [35].
In summary, minimising IFE with respect to the R-density, given an ap-
propriate model for the G-density ppϑ,ϕq in which the sensory inputs are en-
capsulated, allows one to approximate the Bayesian posterior. Furthermore
minimising IFE through perception also gives a lower bound on the sensory295
surprisal.
Table 1 provides a summary of the mathematical objects associated with the
IFE.
4. The R-density: How the brain encodes environmental states
To implement the method described above the brain must explicitly encode300
the R-density. To achieve this it is suggested that neuronal quantities (e.g.,
neural activity) parametrize suﬃcient statistics (e.g., means and variances, see
later) of a probability distribution. More precisely the neuronal variables encode
a family of probability densities over environmental states,ϑ. The instantaneous
state of the brain µ then picks out a particular density qpϑ; µq(the R-density)305
from this family; the semicolon in qpϑ; µqindicates that µis a parameter rather
than a random variable.
Finding the optimal qpϑ; µq that minimises IFE in the most general case
is intractable and thus further approximations about the form of this density
are required. Two types of approximation are often utilised. First, an assump-310
tion that the R-density qpϑq can be factorised into independent sub-densities
q1pθ1qˆ¨¨¨ qNpθNq. Under this assumption the optimal R-density still cannot
be expressed in closed form but an approximate solution (of general form) can
be improved iteratively [36]. This leads to a formal solution in which the sub-
densities aﬀect each other only through mean-ﬁeld quantities. Approaches that315
utilise this form of the R-density are often referred to an ensemble learning .
This approach is not the focus of the work presented here but for completeness
14
we provide a treatment of unconstrained ensemble learning in Appendix A.
A more common approximation is to assume that the R-density take Gaus-
sian form, the so called Laplace approximation [20]. In this scenario, the suf-
ﬁcient statistics of the Gaussian become parameters which can be optimized
numerically to minimize IFE. For example the R-densities take the form
qpϑq” Npϑ; µ,ζq“ 1?2πζ exp
␣
´pϑ´µq2{p2ζq
(
(11)
where µ and ζ are the mean and variance values of a single environmental
variable ϑ. Substituting this form for the R-density into equation (7), and
carrying out the integration produces a vastly simpliﬁed expression for the IFE.
In following we examine this derivation in detail. For the clarity of presentation
we pursue it in the univariate case which captures all the relevant assumptions
for the multivariate case. We write the formulation for the multivariate case at
the end of the section. For notational ease we deﬁne
Z ”
a
2πζ and Epϑq”p ϑ´µq2{p2ζq, (12)
to arrive at
qpϑ; µ,ζq“ 1
Ze´Epϑq, (13)
where here we have drawn on terminology from statistical physics in which the
normalization factor Zis called the partition function and Epϑqthe energy of the320
subsystem tϑu[37]. Substituting this equation into equation (9) and carrying
out the integration leads to a much simpliﬁed expression for IFE :
F “
ż
dϑ qpϑqp´ln Z´Eq`
ż
dϑ qpϑqEpϑ,ϕq
“ ´ ln Z´
ż
dϑ qpϑqEpϑq
`
ż
dϑ qpϑqEpϑ,ϕq (14)
where we have used the normalization condition, equation (4) in the second step.
The Gaussian integration involved in the ﬁrst and second terms in equation (14)
can be evaluated straightforwardly. Speciﬁcally, utilising equation (12), the ﬁrst
15
term in equation (14) can be readily manipulated into
´ln Z “´ 1
2 pln 2πζq.
Using equation (12) the second term in equation (14) becomes
´1
2ζ
ż
dϑ qpϑqpϑ´µq2 Ñ´ 1
2.
The ﬁnal term demands further technical consideration because the energy
Epϑ,ϕq is still unspeciﬁed. However, further simpliﬁcations can be made by
assuming that the R-density, equation (13) is sharply peaked at its mean value325
µ (i.e., the Gaussian bell-shape is squeezed towards a delta function) and that
Epϑ,ϕqis a smooth function of ϑ. Under these assumptions we notice that the
integration is appreciably non-zero only at the peaks. One can then use a Taylor
expansion of Epϑ,ϕqaround ϑ“µwith respect to a small increment, δϑ. Note:
while these assumptions permit a simple analytic model of the FEP, they have330
non-trivial implications for the interpretation of brain function so we return to
this issue at the end of this section and in the Discussion. This assumption
brings about,
ż
dϑ qpϑqEpϑ,ϕq,
«
ż
dϑ qpϑq
#
Epµ,ϕq`
„dE
dϑ

µ
δϑ`1
2
„d2E
Bϑ2

µ
δϑ2
+
.
Now substituting back δϑ“ϑ´µ we get,
«Epµ,ϕq`
„BE
Bϑ

µ
ż
dϑ qpϑqpϑ´µq
`1
2
„d2E
dϑ2

µ
ż
dϑ qpϑqpϑ´µq2.
Here the second term in the third line is zero identically because the integral
equates to the mean. Furthermore recognising the expression for the variance
in the third term allows us to write
«Epµ,ϕq` 1
2
„d2E
dϑ2

µ
ζ. (15)
16
Where we identify Epµ,ϕqas the Laplace-encoded energy. Substituting all terms
derived so far into equation (14) furnishes an approximate expression for the
IFE,
F “Epµ,ϕq` 1
2
˜„d2E
dϑ2

µ
ζ´ln 2πζ ´1
¸
(16)
which is now written as a function (i.e., not a functional) of the Gaussian means335
and variances, and sensory inputs, i.e. F “Fpµ,ζ,ϕ q. To simplify further we
remove the dependence of the IFE on the variances by taking derivative of
equation (16) with respect ζ as follows:
dF “ 1
2
#
d
dζ
˜„d2E
dϑ2

µ
ζ
¸
´1
ζ
+
dζ
“ 1
2
#„d2E
dϑl

µ
´1
ζ
+
dζ.
Minimising by demanding that dF ”0 one can get
ζ˚ “
„d2E
dϑ2
´1
µ
(17)
where the superscript in ζ˚ indicates again that it is an optimal variance (i.e.,
it is the variance which optimizes the IFE). Substituting equation (17) into
equation (16) gives rise to the form of the IFE as
F “Epµ,ϕq´ 1
2 ln t2πζ˚u. (18)
The beneﬁt of this process has been to recast the IFE in terms of a joint density
ppµ,ϕq over sensory data ϕ and the R-density’s suﬃcient statistics µ, rather340
than a joint density over some (unspeciﬁed) environmental features ϑ. Note:
this joint density amounts to an approximation of the G-density described in
equation (1); we shall examine the implementation of this density in detail in
the next section. Furthermore, under these assumptions the IFE only depends
on Gaussian means (ﬁrst-order Gaussian statistics) and sensory inputs, and not345
on variances (second-order Gaussian statistics), which considerably simpliﬁes
the expression. It is possible to pursue an analogous derivation for the full
multivariate Gaussian distribution under the more general assumption that the
17
environment states only weakly covary, i.e., both the variance of, and covari-
ances between, variables are small. Under this assumption the full R-density350
distribution is still tightly peaked and the Taylor expansion employed in equa-
tion (15) is still valid.
To get rid of the constant variance term in equation (18), we write the
Laplace-encoded energy for the full multivariate case, as an approximation for
the full IFE as
Eptµαu,tϕαuq“´ ln pptµαu,tϕαuq, (19)
where we deﬁne tµαuand tϕαuas vectors of brain states and sensory data re-
spectively, corresponding to environmental variables tϑαuwith α“1,2,¨¨¨ ,N
indexing N variables. This equation for the Laplace-encoded energy serves as a355
general approximation for the IFE which we will use in the rest of this study.
Conceptually this expression suggests the brain represents only the most
likely environmental causes of sensory data and not the details of their distribu-
tion per se .However, as we will see later, the brain also encodes uncertainties
through (expectations about) variances (inverse variances) in the G-density.360
Table 2 provides a glossary of mathematical objects involved in the Laplace
encoding of the environmental states in the brain.
5. The G-density: Encoding the brains beliefs about environmental
causes
In the previous section we constructed an approximation of the IFE, which365
we called the Laplace-encoded energy, in terms of the approximate G-density
ppµ,ϕq where the environmental states ϑ have been replaced by the suﬃcient
statistics µ of the R-density. In this section we consider how the brain could
specify this G-density, and thus evaluate IFE. We start specifying a generative
model of the environmental causes of sensory data (informally, a description of370
causal dependencies in the environment and their relation to sensory signals).
We then show how to move from these generative models to speciﬁcation of the
G-density, in terms of brain states and their expectations, and ﬁnally construct
18
expressions for the IFE. We develop various speciﬁcations of G-densities for both
static and dynamic representations of the environment and derive the diﬀerent375
expressions for IFE they imply.
Table 3 provides a summary of the mathematical objects associated with
the G-density in the simplest model and also its extension to the dynamical
generative model.
5.1. The simplest generative model380
We ﬁrst consider a simpliﬁed situation corresponding to an organism that
believes in an environment comprising of a single variable and a single sensory
channel. To represent this environment the agent utilise a single brain state µ
and sensory input ϕ. We then write the organisms belief about the environment
directly in terms of a generative mapping between brain states and sensory data.
Note these equations will have a slightly strange construction because in reality
sensory data is caused by environmental, not brain, states. However, writing the
organism beliefs in this way will allow us to easily construct a generative density,
see below. Speciﬁcally we assume the agent believe its sensory is generated as
ϕ“gpµ; θq` z (20)
where g is a linear or nonlinear function, parametrized by θ and z is a random
variable with zero mean and varianceσz. Thus the organism believes its sensory
data is generated as non-linear mapping between environmental states (here
denoted in terms of its belief about environmental state µ) with added noise.
Similarly we specify the organism beliefs about how environmental state are
generated as
µ“¯µ`w, (21)
where ¯µ is some ﬁxed parameter and w is random noise drawn from a Gaus-
sian with zero mean and variance σw. In other words, the organism takes the
environment’s future states to be history-independent, ﬂuctuating around some
mean value ¯µ which is given a priori to the organism. There is a potential
confusion here because equation equation (21) describes a distribution over the385
19
brain state variableµ, which itself represents the mean of some represented envi-
ronmental state ϑ. Speciﬁcally it is worth reiterating that ¯µand σw are distinct
from the suﬃcient statistics of the R-density µ and ζ [see equation (11)]. The
variables ¯µ represent the organism’s belief about the future state of the envi-
ronment as encoded in the G-density and σw encodes the organism’s conﬁdence390
in its estimate of those future states. By contrast µ,ζ belong to the R-density,
encoding the organism’s uncertain beliefs about its current environment ϑ. As
we will see in Section 7, there is conﬂict here because the organism’s best esti-
mate µ(the mean of its subjective distribution over ϑ) may not be in line with
its expectation ¯µ stemming from its model of environmental dynamics.395
To construct the generative density we assume that the noise z is given as
Gaussian, r1{?2πσzsexp
␣
´z2{p2σzq
(
. Then, rewriting equation (20) as z “
ϕ´gpµ; θq, the functional form of the likelihood ppϕ|µqcan be written as
ppϕ|µq“ 1?2πσz
exp
!
´pϕ´gpµ; θqq2 {p2σzq
)
. (22)
Assuming similar Gaussian noise for the random deviation w“µ´¯µ, in equa-
tion (34), the prior density ppµqcan be written as
ppµq“ 1?2πσw
exp
!
´pµ´¯µq2 {p2σwq
)
(23)
where σw is the variance.
Thus far, we have speciﬁed the likelihood and the prior of µwhich together
determine the G-density ppµ,ϕqaccording to the identity,
ppµ,ϕq“ ppϕ,µq“ ppϕ|µqppµq.
Next, we construct the Laplace-encoded energy by substituting the likelihood
and prior densities obtained above into equation (19) to get, up to a constant,
Epµ,ϕq “ ´ ln ppϕ|µq´ ln ppµq (24)
“ 1
2σz
ε2
z ` 1
2σw
ε2
w `1
2 ln pσzσwq, (25)
where the auxiliary notations have been introduced as
εz ”ϕ´gpµ; θq and εw ”µ´¯µ.
20
which comprise a residual error or a prediction error in the predictive coding
terminology [6]. The quantity εz is a measure of the discrepancy between actual400
ϕ and the outcome of its prediction gpµ; θq. While εw describes the extent to
which µ itself deviates from its expectation ¯ µ. The former describes sensory
prediction errors, εz, while the latter describe model predictions, εw , (i.e.,
how brain states deviate from their expectation). Each erro term is multiplied
by the inverse of variance which weight the relative conﬁdence of these term,405
i.e., how they contribute to the Laplace-encoded energy. We note in other
works the inverse of variance, know as a precision, is used in these equations
perhaps to highlight that these terms weight the conﬁdence, or preciseness, of
the prediction. However in this presentation we stick to more standard notation
involving variances.410
The above calculation can be straightforwardly extended to the multivariate
case. Speciﬁcally, we represent tµαuas a row vector of N brain states, and write
their expectations as
µα “¯µα `wα.
Here twαuis a row vector describing correlated noise sources, thus generally the
ﬂuctuations of each variable are not independent, which all have zero mean and
covariance Σw. We can the write a set of N sensory inputs tϕαuwhich depend
on combination of these brain states in some nonlinear way such that
ϕα “gαpµ0,µ1,...,µ Nq` zα. (26)
Again tzαuare noise sources with zero mean and covariance Σ z and thus each
sensory input may receive statistically correlated noise. Then, the prior over
brain states may be represented as the multivariate correlated Gaussian density,
pptµαuq“ 1a
p2πqN|Σw|
exp
ˆ
´1
2tµα ´¯µαuΣ´1
w tµα ´¯µαuT
˙
, (27)
where tµα ´¯µαuT is the transpose of vector tµα ´¯µαu; |Σw|and Σ´1
w are the
determinant and the inverse of the covariance matrix Σw, respectively. Similarly,
21
we can write down the multivariate likelihood as
pptϕαu|tµαuq“ 1a
p2πqN|Σz|
exp
ˆ
´1
2tϕα ´gαpµquΣ´1
z tϕα ´gαpµquT
˙
.
(28)
Now substituting these expressions into equation (19) we can get an expression
of the Laplace-encoded energy as, up to an overall constant,
Eptϕαu,tµαuq “ 1
2tµα ´¯µαuΣ´1
w tµα ´¯µαuT `1
2 ln |Σw|
` 1
2tϕα ´gαpµquΣ´1
z tϕα ´gαpµquT `1
2 ln |Σz|. (29)
The above equation (29) contains non-trivial correlations among the brain vari-
ables and sensory data. It is possible to pursue the full general case, e.g., see
[38] for a nice tutorial on this, but we do not consider this here. Instead we can415
simplify on the assumption of statistical independence between environmental
variables and between sensory inputs. Under this assumption the prior and
likelihood are factorised into the simple forms, respectively,
pptµαuq“
Nź
α“1
ppµαq, (30)
pptϕαu|tµαuq“
Nź
α“1
ppϕα|tµαuq, (31)
where probability densities are the uncorrelated Gaussians,
pptµαuq“
Nź
α“1
1?2πσαz
exp
␣
´rµα ´¯µαs2{p2σα
wq
(
,
pptϕαu|tµαuq“
Nź
α“1
1?2πσαw
exp
␣
´rϕα ´gαpµqs2{p2σα
zq
(
.
This gives the Laplace-encoded energy as
Eptϕαu,tµαuq“
Nÿ
α“1
„pεα
wq2
2σαw
`1
2 ln σα
w

`
Nÿ
α“1
„pεα
zq2
2σαz
`1
2 ln σα
z

, (32)
where the variances σα
w and σα
z are diagonal elements of the covariance matrices420
Σw and Σ z, respectively. In equation (32) we have again used the auxiliary
variables
εα
w “ µα ´¯µα,
εα
z “ ϕα ´gα,
22
The structure of equation (32) suggests that the Laplace-encoded energy, which
is an approximation for the IFE, is a quadratic sum of the prediction-errors,
modulated by the corresponding inverse variances, and an additional sum of425
the logarithm of the variances.
5.2. A dynamical generative model
In the previous section we considered a simple generative model where an or-
ganism understood the environment to be eﬀectively static. Here we extend the
formulation to dynamic generative models which have the potential to support
inference in dynamically changing environments. Again we start by examining
a single sensory input ϕ and a univariate brain state µ. Here we assume that
the agent’s model of environmental dynamics (again expressed in terms of brain
states) follows not equation (21), but rather a Langevin-type equation [39]
dµ
dt “fpµq` w (33)
where f is a function of µ and w is a random ﬂuctuation. A dynamical gener-
ative model can then be obtained by combining the simple generative model,
equation (20), with equation (33).430
The FEP utilizes the notions of generalized coordinates and higher-order
motion [20] to incorporate general forms of dynamics into the G-density. Gen-
eralised coordinates involve representing the state of a dynamical system in
terms of increasingly higher order derivative of its state variables. For example,
generalized coordinates of a position variable may correspond to bare ‘position’435
as well as its (unbounded) higher-order temporal derivatives (velocity, accel-
eration, jerk, and so on) allowing a more precise speciﬁcation of a system’s
state [20]. To obtain these coordinates we simply take recursively higher order
derivatives of both equation (20) and equation (33).
23
For the sensory data:440
ϕ “ gpµq` z
ϕ1 “ Bg
Bµµ1`z1 (34)
ϕ2 “ Bg
Bµµ2`z2
...
where we have used the notations,
ϕ1 ”dϕ{dt, µ 1 ”dµ{dt, µ 2 ”d2µ{dt2, etc.
and where z,z1,... are the noises sources at each dynamic order. Here nonlinear
derivative terms such asµ12, µ1µ2, etc, have been neglected under alocal linearity
assumption [18] and only linear terms have been collected. In some treatments
of the FEP it is assumed that the noise sources are correlated [20]. However,
here, for the clarity of the following derivations, we follow more standard state445
space models and assume each dynamical order receives independent noise, i.e,
we assume the covariance between noise sources is zero.
Similarly, the Langevin equation, equation (33), is generalized as
µ1 “ fpµq` w
µ2 “ Bf
Bµµ1`w1 (35)
µ3 “ Bf
Bµµ2`w2
...
where again we have applied the local linearity approximation and we assume
each dynamical order receives independent noise denoted as w,w1,.... Here, it
is convenient to denote the multi-dimensional sensory-data ˜ϕ as
˜ϕ“pϕ,ϕ1,ϕ1,¨¨¨q”p ϕr0s,ϕr1s,ϕr2s,¨¨¨q
and states ˜µ as
˜µ“pµ,µ1,µ2,¨¨¨q”p µr0s,µr1s,µr2s,¨¨¨q , (36)
24
both being row vectors; where the nth-components are deﬁned to be
ϕrns ” dn
dtnϕ“ϕ1
rn´1s and µrns ” dn
dtnµ“µ1
rn´1s.
The generalized coordinates, equation (36), span the generalized state-space
in mathematical terms. In this state-space, a point represents an inﬁnite-
dimensional vector that encodes the instantaneous trajectory of a brain variable
[21]. By construction, the time-derivative of the state vector ˜µ becomes
˜µ1 ”D˜µ” d
dtpµ,µ1,µ2,¨¨¨q“p µ1,µ2,µ3¨¨¨q”p µr1s,µr2s,µr3s,¨¨¨q .
The ﬂuctuations in the generalized coordinates are written as
˜z“pz,z1,z2,¨¨¨q”p zr0s,zr1s,zr2s,¨¨¨q ,
˜w“pw,w1,w2,¨¨¨q”p wr0s,wr1s,wr2s,¨¨¨q .
In addition, we denote the vectors associated with time-derivatives of the gen-
erative functions as
˜g”pgr0s,gr1s,gr2s,¨¨¨q and ˜f ”pfr0s,fr1s,fr2s,¨¨¨q
where the components are given as gr0s ”gpµqand fr0s ”fpµq, and for ně1
as
grns ” Bg
Bµµrns and frns ” Bf
Bµµrns.
In terms of these constructs the inﬁnite set of coupled equations (34) and (35)
can be written in a compact form as450
˜ϕ “ ˜g`˜z (37)
D˜µ “ ˜f ` ˜w (38)
The generalized map, equation (37), describes how the sensory data ˜ ϕ are in-
ferred by the representations of their causes ˜µat each dynamical order. Accord-
ing to this map, the sensory data at a particular dynamical order n, i.e. ϕrns,
engages only with the same dynamical order of the brain states, i.e. µrns. The
25
generalized equation of motion, equation (38), speciﬁes the coupling between455
adjacent dynamical orders.
As before, in order to obtain the G-density we need to specify the likelihood
of the sensory data pp˜ϕ|˜µq and the prior pp˜µq. The statistical independence
of noise at each dynamical order means that we can write the likelihood as a
product of conditional densities, i.e.,460
pp˜ϕ|˜µq “ ppϕr0s,ϕr1s,ϕr2s,¨¨¨| µr0s,µr1s,µr2s,¨¨¨q
“
8ź
n“0
ppϕrns|µrnsq. (39)
Assuming that the ﬂuctuations at all dynamics orders, zrns, are induced by
Gaussian noise, the conditional likelihood-density ppϕrns|µrnsqis speciﬁed as
ppϕrns|µrnsq“ 1a2πσzrns
exp
”
´
␣
ϕrns´grns
(2
{
`
2σzrns
˘ı
.
Similarly, the postulate of the conditional independence of the generalized noises
wrns leads to a prior in the form
pp˜µq“ ppµr0s,µr1s,µr2s,¨¨¨q“
8ź
n“0
ppµrn`1s|µrnsq (40)
The form of the prior density at dynamical order n is ﬁxed by the assumption
of Gaussian noise, which is then given as
ppµrn`1s|µrnsq“ 1a2πσwrns
exp
”
´
␣
µrn`1s´frns
(2
{
`
2σwrns
˘ı
.
Utilizing equations (39) and (40), the G-density is speciﬁed as
pp˜ϕ,˜µq“
8ź
n“0
ppϕrns|µrnsqppµrn`1s|µrnsq. (41)
Given the G-density, the Laplace-encoded energy can be calculated (equation (19))
to give, up to a constant,
Ep˜µ, ˜ϕq “
8ÿ
n“0
" 1
2σzrns
rεzrnss2 `1
2 ln σzrns
*
`
8ÿ
n“0
" 1
2σwrns
rεwrnss2 `1
2 ln σwrns
*
(42)
26
where εzrns and εwrns are nth component of the vectors ˜εz and ˜εw, respectively,
which have been deﬁned to be
εzrns ”ϕrns´grns and εwrns ”µrn`1s´frns.
As before, the auxiliary variables,εzrnsand εwrns, encode prediction errors: εzrns
is the error between the sensory data ϕrns and its prediction grns at dynamical
order n. Likewise, εwrns measures the discrepancy between the expected higher-
order output µrn`1s and its generation frns from dynamical order n. Typically
only dynamics up to ﬁnite order are considered. This can be done by setting
the highest order term to random ﬂuctuations, i.e.,
µrnmaxs “wrnmaxs
where wrnmaxs has large variance; thus, the corresponding error term in equa-
tion (42) will be close to zero and eﬀectively eliminated from the expression for
the Laplace-encoded energy. In eﬀect it means that the order below is uncon-465
strained, and free to change in a way that best ﬁts the incoming sensory data.
This is related to the notion of empirical priors as discussed in Section 8.1. Thus
we have expressed Laplace-encoded energy for dynamics environment, which is
an approximation for the IFE, is a quadratic sum of the sensory prediction-error,
εwrns, and model prediction errors, εwrns, across diﬀerent dynamical orders.470
Again each error term is modulated by the corresponding variances describing
the degree of certainty in those predictions.
Again its is straightforward we can generalise this to the multivariate case.
We set t˜ϕαuand t˜µαuas vectors of brain states and rewrite equations (37) and
(38) as475
˜ϕα “ ˜gα `˜zα (43)
D˜µα “ ˜fα ` ˜wα, (44)
27
where α runs from 1 to N. Thus, equation (42) becomes
Ept˜µαu,t˜ϕαuq “
Nÿ
α“1
8ÿ
n“0
#
1
2σα
zrns
rεα
zrnss2 `1
2 ln σα
zrns
+
`
Nÿ
α“1
8ÿ
n“0
#
1
2σα
wrns
rεα
wrnss2 `1
2 ln σα
wrns
+
(45)
where we have again used the auxiliary variables
εα
zrns ” ϕαrns´gαrns (46)
εα
wrns ” µαrn`1s´fαrns. (47)
Thus this constitutes an approximation of IFE for a multivariate system across
arbitrary number of dynamical orders.
6. IFE minimisation: How organisms infer environmental states480
In the previous section we demonstrated how to go from a generative model,
specifying the organism’s beliefs about the environment, to a generative density
given expectations on brain states, and ﬁnally to an expression for the IFE.
In this section we discuss how organisms could minimises IFE to make the R-
density a good approximation of the posterior and thus we begin to outline a full485
biologically plausible process theory. In particular, here, we focus on how this
minimisation could be implemented in neuronal dynamics of the brain outlining
one particular process theory.
Under the FEP it is proposed that the innate dynamics of the neural ac-
tivity evolves in such a way as to minimise the IFE. Speciﬁcally, it is sug-
gested that brain states change in such way that they implement a gradient
descent scheme on IFE referred to as recognition dynamics. Under the proposed
gradient-descent scheme, a brain state µα is updated between two sequential
steps t and t`1 as
µt`1
α “µt
α ´κˆµα ¨∇µαEptµαu,tϕαuq
28
where κ is the learning rate and ˆµα is the unit vector along µα. This process
recursively modiﬁes brain states in a way that follows the gradient of Laplace-
encoded energy. In the continuous limit the update µα
t`1 ´µα
t may be converted
to a diﬀerential form as
µt`1
α ´µt
α ” 9µα.
Then, the above discrete updating-scheme can be transformed into a spatio-
temporal diﬀerential equation,
9µα “´κˆµα ¨∇µαEptµαu,tϕαuq. (48)
The essence of the gradient descent method, as described in equation (48), is
that the minima of the objective function E, i.e., the point where ∇µE “ 0,490
occur at the stationary solution when 9µα vanishes. Thus the dynamics of the
brain states settle at a point where the Laplace-encoded energy is minimized.
To update dynamical orders of the brain state µα, equation (48) must be
further generalized to give
µαrn`1s´µαrns ”´κˆµαrns¨∇˜µαEpt˜µαu,t˜ϕαuq
where ˆµαrns is the unit vector along µαrns, nth-component of the generalized
brain state ˜µα (Section 5.2). Also, as before (equation (48)) the sequential
dynamical order pn,n `1qcan be recast into a diﬀerential form to give
µ1
αrns “´κˆµαrns¨∇˜µαEpt˜µαu,t˜ϕαuq. (49)
Note that in order to be consistent with the deﬁnition of the generalised co-
ordinates we have used the distinctive notation for the temporal derivative of
dynamic update from the parametric update, equation (48). Here, we face a
complication because the temporal derivative of the dynamical order µαrns is
already contained within the generalized coordinates, i.e., µ1
αrns “µαrn`1s, in
virtue of the deﬁnition of the latter. Consequently, it is not possible to make
µ1
αrnsvanish at any order, meaning that the gradient descent procedure is unable
to construct a stationary solution at which the gradient of the Laplace-encoded
29
energy vanishes. However, it can be argued that the motion of a point (velocity),
i.e. 9˜µα, in the generalized state-space is distinct from the ‘trajectory’ encoded
in the brain (ﬂow velocity) [20, 36, 21]. The latter object is denoted by D˜µα
where Dimplies also a time-derivative operator which, when acted on ˜µ, results
in (see Section 5.2)
D˜µα ”pµ1
αr0s,µ1
αr1s,µ1
αr2s¨¨¨q”p µ1
α,µ2
α,µ3
α ¨¨¨q ,
but is by this assumption distinct from the usual time-derivative9˜µα, i.e. µ1
αrns ‰
9µαrns. The term ‘velocity’ here has been adapted by analogy with velocity in
mechanics in the sense that 9˜µα denotes ﬁrst order time-derivative of ‘position’,
namely the bare variable ˜µα. Prepared with this extra theoretical construct,
the gradient descent scheme is restated in the FEP as
9µαrns´Dµαrns “´κˆµαrns¨∇˜µαEpt˜µαu,t˜ϕαuq (50)
where Dµαrns “ µ1
αrns. According to this formulation, E is minimized with
respect to the generalized state ˜ µα when the ‘path of the mode’ (generalized
velocity) is equal to the ‘mode of the path’ (average velocity), in other words495
the gradient of E vanishes when 9˜µα “D˜µα. It is worth noting that in ‘static’
situations where generalized motions are not required (see section 8.4), the
concept of the ‘mode of the path’ is not needed, i.e. D˜µα ”0 by construction.
In such situations we consider the relevant brain variablesµα to reach the desired
minimum when there is no more temporal change in µα in the usual sense, i.e.500
when 9µα “0.
In sum, these equations specify sets of ﬁrst order ordinary diﬀerential equa-
tions that could be straightforwardly integrated by neuronal processing, e.g.,
they are very similar equations for ﬁring rate dynamics in neural networks (e.g,
see [40]). Continuously integrating these equations in the presence of stream505
of sensory data would make brain states continuously minimise IFE and thus
implement approximate inference on environmental states. Furthermore with
additional assumption about there implementation [41] they become strongly
analogous to the predictive coding framework [6].
30
7. Active inference510
A central appeal of the FEP is that it suggests not only an account of
perceptual inference but also an account of action within the same framework:
active inference. Speciﬁcally while perception minimises IFE by changing brain
states to better predict sensory data, action instead acts on the environment to
alter sensory input to better ﬁt sensory predictions. Thus action minimises IFE515
indirectly by changing sensations.
In this section we describe a gradient-descent scheme analogous to that in
the previous section but for action. To ground this idea for action, and combine
it with the framework for perceptual inference discussed in previous sections,
we present an implementation of a simple agent-based model.520
Under the FEP action does not appear explicitly in the formulation of IFE
but minimises IFE by changing sensory data. To evaluate this the brain must
have a inverse model [42] of how sensory data change with action [9]. Speciﬁcally,
for a single brain state variable µ we write this as ϕ“ϕpaqwhere a represents
the action and ϕ is a single sensory channel ϕ. Given this relationship we can
then write the gradient of the Laplace-encoded energy with respect to action
using the chain rule as,
dEpµ,ϕq
da ” dϕ
da
BEpµ,ϕq
Bϕ . (51)
Thus we can write the same gradient decent scheme outlined in the last section
to calculate the actions that minimise the Laplace-encoded energy as
9a“´κa
dϕ
da
dEpµ,ϕq
dϕ (52)
where κa is the learning rate associated with action.
It is straightforward to write this gradient descent scheme for a vector of
brain states in generalised coordinates as
9a“´κa
ÿ
α
d˜ϕα
da ¨∇˜ϕα Ept˜µαu,t˜ϕαuq. (53)
The idea that brains innately possess a inverse model, at ﬁrst glance, seems
somewhat troublesome. However, under the FEP the execution of motor control
31
depends only on predictions about proprioceptors (internal sensors) which can
be satisﬁed by classic reﬂex arcs [43, 9]. On this reading exteroceptive, and525
perhaps interoceptive [10], sensations are only indirectly minimised by action.
While a full assessment of this idea’s implications is outside the remit of this
work, it provides an interesting alternative to conventional notions of motor
control, or behaviour optimisation, that rest on maximising or minimising value
[43].530
To give a concrete example of how perceptual and active inference work
we present an implementation of a simple agent-based model. Speciﬁcally we
present a model that comprises a mobile agent that must move to achieve some
desired local temperature, Tdesire. The agent’s environment, or generative pro-
cess [9], consists of a 1-dimensional plane and a simple temperature source. The
agent’s position on this plane is denoted by the environmental variable ϑ and
the agent’s temperature depends on its position in the following manner,
Tpϑq“ T0
ϑ2 `1, (54)
where T0 is the temperature at the origin, i.e., this equation gives the true
dynamics of the agents’ environment (the environmental causes of its sensory
signals). The corresponding temperature gradient is readily given by,
dT
dϑ “´T0
2ϑ
pϑ2 `1q2 ”Tϑ.
The temperature proﬁle is depicted by the black line in Fig. 1a. We allow the
agent to sense both the local temperature and the temporal derivative of this
temperature
ϕ “ T `zgp (55)
ϕ1 “ Tϑϑ1`z1
gp (56)
where zgp and z1
gp are normally distributed noise in the sensory readings. Note
that the subscript gp reminds us that this noise is a part of the agent’s environ-535
ment (rather than its brain model) described by the generative process.
32
In this model the agent is presumed to sit on a ﬂat frictionless plane and,
thus, in the absence of action the agent is stationary. We allow the agent to set
its own velocity by setting it equal to the action variable a as,
ϑ1 “a. (57)
The agent has brain state µ which represents the agents estimate of its tem-
perature in the environment. Following equations (35), we write a generative
model for the agent, up to third order, as
µ1 “ fpµq` w where fpµq”´ µ`Tdesire
µ2 “ ´µ1`w1
µ3 “ w2,
where the third order term is just random ﬂuctuations with large variance and540
thus is eﬀectively eliminated from the expression for the Laplace-encoded energy,
see Section 5.2. Following equation (34), we write the agent’s belief about it’s
sensory data only to ﬁrst order as,
ϕ “ gpµq` z where gpµq” µ
ϕ1 “ µ1`z1
Note the actual environment is not dynamic but the agent’s belief about the
environment is. Indeed, examining the agent’s generative model we easily see545
that it possesses a stable equilibrium point atTdesire. In eﬀect the agent believes
in a environment where the forces it experiences naturally move it to its desired
temperature, see Section 2 and [9]. However, these dynamics are diﬀerent to
those that describe the environment thus the agent must take action to make
the environment conform.550
We can write the Laplace-encoded energy, equation (45), for this model, as
Ep˜µ, ˜ϕq“ 1
2
„ 1
σzr0s
pεzr0sq2 ` 1
σzr1s
pεzr1sq2 ` 1
σwr0s
pεwr0sq2 ` 1
σwr1s
pεwr1sq2

,
(58)
33
where the various error terms are given as
εzr0s “ ϕ´µ
εzr1s “ ϕ1´µ1
εwr0s “ µ1`µ´Tdesire
εwr1s “ µ2`µ1.
Also, σzr0s, σzr1s, σwr0s, and σwr1s in equation (58) are the variances correspond-
ing to the noise termsz, z1, w, and w1, respectively. In addition we have dropped
logarithm of variance terms, see equation (24) because they play no role when
we minimise these equations with respect to the brain variable µ.555
Note, that the noise terms in the agents internal model are distinct from
those in equation (56) and represent the agents beliefs about the noise on en-
vironmental states and sensory data rather than the actual noise on these vari-
ables. As we will see these terms eﬀectively represent the conﬁdence of the agent
in its own sensory input.560
Using the gradient decent scheme described in equation (50) we write the
recognition dynamics as
9µ “ µ1´κa
„
´εzr0s
σzr0s
` εwr0s
σwr0s

9µ1 “ µ2´κa
„
´εzr1s
σzr1s
` εwr0s
σwr0s
` εwr1s
σwr1s

(59)
9µ2 “ ´κa
εwr1s
σwr1s
.
Here we have considered generalised coordinates up to second order only. To
allow the agent to perform action we must provide it with an inverse model,
which we assume is hard-wired [9]. Replacing the agent’s velocity with the
action variable a in equation (56) we specify this as
dϕ1
da “ d
da
`
aTϑ `z1
gp
˘
“Tϑ. (60)
Eﬀectively the agent believes that action changes the temperature in a way that
is consistent with it’s beliefs about the temperature gradient. Given this inverse
34
model we can write down the minimisation scheme for action as.
9a“´κa
„dϕ
da
BE
Bϕ `dϕ1
da
BE
Bϕ1

“´κaTϑ
εzr1s
σzr1s
. (61)
Thus, equations (59) through (61) describe the complete agent-environment
system and can be straightforwardly integrated (see Appendix B for details).
Fig. 1 shows the behaviour of the agent in the absence of action, i.e., when565
the agent is unable to move. We examine two conditions. In a ﬁrst condition
the agent’s sensory variances σzr0s, σzr1s are several orders of magnitude smaller
than model variances σwr0s and σwr1s. Thus the agent has higher conﬁdence (see
Section 5.1) in sensory input than in its internal model. Under this condition
the agent successfully infers both the local temperature and its corresponding570
derivatives, see Fig. 1b black lines. In eﬀect the agent ignores its internal model
and the gradient decent scheme is equivalent to a least mean square estimation
on the sensory data, see supplied code in Appendix B. In a second condition,
see Fig. 2 red lines, we equally balance internal model and sensory variances
(σzris “σwris, i“0,1). Now minimisation of IFE cannot satisfy both sensory575
perception and predictions of the agent’s internal model, i.e., what the agent
perceives is in conﬂict with what it desires. Thus the inferred local temperature
sits somewhere between its desired and sensed temperature, see Fig. 1b.
In Fig. 2, after an initial period, the agent is allowed to act according to
equation (61). It does so by changing the environment to bring it in line with580
with sensory predictions and the desires encoded within its dynamic model, i.e.,
the agent moves toward the desired temperatures.
The reduction of surprisal can be quantiﬁed as the diﬀerence between the
Laplace-encoded energy (and thus IFE) in presence and absence of action, i.e.,
the diﬀerence between black and red traces in Fig. 2e, respectively. Speciﬁcally,585
it is the portion of the IFE that must be minimised by acting on the environment
rather than through optimisation of the agent’s environment model. We leave
a more explicit quantiﬁcation of the dynamics of surprisal for future work.
In summary we have presented an example of an agent performing a very
simple task under the FEP. The model demonstrates how the minimisation590
35
Position ϑ 
0246
Temperature
0
20
40
60
80
100
0 5 10
µ
0
5
10
15
20
time
0 5 10
µ''
-1
0
1
2
3
time
0 5 10
IFE
0
500
1000
1500
2000
2500
0 5 10
µ'
-3
-2
-1
0
1
c
ba
--- Desired pos and temp
--- Starting  pos and temp
Figure 1: Perceptual inference: The agent’s environment comprises a simple temper-
ature gradient ( a), the blue and magenta lines give the actual and desired positions
of the agent, respectively. The agent performs simple perceptual inference ( b), the
dynamics of three generalized coordinates, µ, µ1 and µ2, are given in the top, middle
and bottom panels, respectively. Two conditions are shown, when the conﬁdence in
the sensory input is high (i.e. σzris is small in comparison to σwris), black line, and
when conﬁdence is equal between the internal model and sensory input, red line, re-
spectively. IFE in both conditions monotonically decreases ( c): black and red traces,
respectively. The tension between sensory input and internal model manifests a rel-
atively high value of IFE ( c) (red curve), compared to the case where sensation has
much higher conﬁdence than the internal model (black curve).
36
0 20 40 60 80 1000
10
20 T
# 
0 20 40 60 80 100-10
0
10
20 7 
7 '
7 ''
0 20 40 60 80 100-10
0
10
20
30
' 
' ' 
0 20 40 60 80 100
a
-0.2
0
0.2
0.4
time
0 20 40 60 80 100
IFE
0
1000
2000
a
b
c
d
e
Figure 2: Perceptual and active inference: An agent with equal conﬁdence in its
internal model and sensory input σzris “σwris “1 is allowed to act at t “25. The
agent acts, see ( d), to alter its position, see ( a: orange line), to bring down its initial
temperature (T “20) to the desired temperature (T “Tdesire “4), see (a: blue line).
It does this by bringing its sensory data (c) in line with its desire, i.e., ϕ“Tdesire and
thus the brain state becomes equal to its desired state, see ( b). IFE was calculated in
the presence and absence of the onset of action at t“25, see e, black and red lines,
respectively. First IFE is reduced by inference ( tă25), then later through action ( e:
black line).
37
of IFE can underpin both perception and action. Furthermore, it shows how
a tension between desires and perception can be reconciled through action.
Many other agent based implementations of the FEP have been presented in
the literature, .e.g. [9], which can be constructed in a similarly simplistic way.
8. Hierarchical Inference and Learning595
In the previous sections we developed the FEP for organisms given simple
dynamical generative models. We then investigated the emergence of behaviour
in a simulated organism (agent) furnished with an appropriate generative model
of a simple environment. The assumption here was that organisms possess
some knowledge or beliefs of about how the environment works a priori, in600
the form of a pre-speciﬁed generative model. However, another promise of
the FEP is the ability to learn and infer arbitrary environmental dynamics
[21]. To achieve this it is suggested that brain starts out with a very general
hierarchical generative model of environmental dynamics which is moulded and
reﬁned through experience. The advantage of using hierarchical models, as we605
will see, is that they suggest a way of avoiding specifying an explicit and ﬁxed
prior, and thus can implement empirical Bayes [44]. In this section we ﬁrst
provide a description of a hierarchical G-density which is capable of empirical
Bayes [44]. We then combine this with dynamical generative model described
in equation (45) to deﬁne what we shall call the full construct . We go on to610
describe how appropriate parameters and hyperparameters of the G-density for
given world could be discovered through learning. We ﬁnish this section by
showing how action could be described in this construct.
8.1. Hierarchical generative model
A key challenge for Bayesian inference models is how to specify the priors.615
Hierarchical models provide a powerful response to this challenge, in which
higher levels can provide empirical priors or constraints on lower levels [45].
In the FEP, hierarchical models are mapped onto the hierarchical organisation
38
of the cortex [46, 47], which requires extension of the simple generative model
described above.620
We denote µpiq as a brain state at hierarchical level i and we assume M
cortical levels, with i“1 the lowest level and i“M as the highest. Then, the
hierarchical model may be written explicitly as [21]
ϕ “ gp1qpµp1qq` zp0q
µp1q “ gp2qpµp2qq` zp1q
µp2q “ ¨¨¨
...
µpMq “ zpMq
which can be written compactly as
µpiq “gpi`1qpµpi`1qq` zpiq (62)
where i runs through 1 ,2,¨¨¨ M. We further assume that the sensory data ϕ
reside exclusively at the lowest cortical level µp1q and dynamics at the highest
level µpMq are governed by a random ﬂuctuation zpMq, i.e:
µp0q ”ϕ and gpM`1q ”0. (63)
The hierarchy equation (62) speciﬁes that a cortical state µpiq is connected to
higher level µpi`1q through the generative function gpi`1q. The ﬂuctuations zpiq625
exist at each level, in particular zp0q designating the observation noise at the
sensory interface, and are assumed to be statistically independent.
Having deﬁned the hierarchical model, one can write the corresponding G-
density as
ppϕ,µq “ ppµp0q|µp1q,µp2q,¨¨¨ ,µpNqqppµp1q,µp2q,¨¨¨ ,µpMqq
” ppµp0q|µp1qqppµp1q|µp2qq¨¨¨ ppµpM´1q|µpMqqppµpMqq. (64)
The second step in equation (64) assumes that the transition probabilities from
higher levels to lower levels are Markovian. Consequently, equation (64) asserts
39
that the likelihood of a level, for instance ppµpiq|µpi`1qq, serves as a prior density
for the level immediately below, i´1. The prior at the highest level ppµpMqq
contains information only with respect to its spontaneous noise, which may be
given by a Gaussian form
ppµpMqq“ 1b
2πσpMq
z
exp
!
´rµpMqs2{
´
2σpMq
z
¯)
(65)
where the mean has been assumed to be zero and σpMq
z is the variance. We shall
further assume that the Gaussian noises are responsible for the (statistically
independent) ﬂuctuations at all hierarchical levels. Accordingly, the likelihoods
ppµpiq|µpi`1qqare given as
ppµpiq|µpi`1qq“ 1b
2πσpiq
z
exp
„
´
!
µpiq´gpi`1qpµpi`1qq
)2
{
!
2σpiq
z
)
. (66)
and the G-density reduces to
ppϕ,µq“
»
–
Mź
i“0
1b
2πσpiq
z
ﬁ
ﬂexp
˜
´
Mÿ
i“0
1
2σpiq
z
rεpi`1qs2
¸
(67)
where the auxiliary variables εpiq have been introduced as
εpiq ”µpi´1q´gpiqpµpiqq. (68)
The quantity εpiq measures the discrepancy between the prediction (estimation)630
at a given level µpiq via gpiq and µpi´1q at a lower-level, which comprises a
prediction error.
Finally, by substituting the G-density, constructed in equation (67), into
equation (19), after a simple manipulation, the Laplace-encoded energy E is
given up to a constant as
Epµ,ϕq“
Mÿ
i“0
" 1
2σpiq
z
rεpi`1qs2 `1
2 ln σpiq
z
*
. (69)
The variance of the noise at the top level of hierarchy is typically assumed to
be large and thus the corresponding term in the Laplace-encoded energy equa-
tion (69) is approximately zero. As with the higher dynamical orders discussed635
40
above Section 5.2 this means that the level below is eﬀectively unconstrained
(has no prior) and thus this type of inference constitutes an example of empirical
Bayes [44].
Table 4 itemizes the mathematical objects associated with the hierarchical
generative model.640
8.2. Combining hierarchical and dynamical models: The full construct
We now combine the dynamical structure and the multivariate brain states in
a single expression. First we note that under the FEP brain states representing
neuronal activity µα are divided into the hidden states xα and the causal states
vα,
µα “pxα,vαq.
Then, the full FEP implementation can be derived formally by extending equa-
tions (43) and (44) (equation (62))
˜vpiq
α “ ˜gpi`1q
α p˜xpi`1q
α ,˜vpi`1q
α q` ˜zpiq
α , i “0,1,¨¨¨ .M (70)
D˜xpiq
α “ ˜fpiq
α p˜xpiq
α ,˜vpiq
α q` ˜wpiq
α , i “1,2.¨¨¨ ,M (71)
where the brain-state index runs through α “1,2,¨¨¨ ,N and ˜vp0q
α designates
the sensory data at the lowest cortical layer, i “ 1. Inter-layer hierarchical
links are made through the causal states and intra-hierarchical layer dynamics
through the hidden states. The generalized coordinates of neuronal brain state
α in hierarchical layer i are given by the inﬁnite-dimensional vectors
˜xpiq
α ”pxpiq
αr0s,xpiq
αr1s,xpiq
αr2s,¨¨¨q and ˜ vpiq
α ”pvpiq
αr0s,vpiq
αr1s,vpiq
αr2s,¨¨¨q
where the components are labelled by the subscripts rns, n“0,1,¨¨¨ ,8. Note
that we have introduced diﬀerent notations in the vector components: The
subscript α for brain states at a given hierarchical level, the superscript piqfor
the hierarchical indices, and the subscript rnsfor the dynamical orders. Recall
that the n-th component of the vector ˜xpiq
α and ˜vpiq
α are time-derivatives of order
n, namely
xpiq
αrns ” dn
dtnxpiq
α and vpiq
αrns ” dn
dtnvpiq
α .
41
The other mathematical quantities in equations (70) and (71) are given explicitly
as:
D˜xpiq
α “pxpiq
αr1s,xpiq
αr2s,xpiq
αr3s,¨¨¨q ,
˜zpiq
α ”pzpiq
αr0s,zpiq
αr1s,zpiq
αr2s,¨¨¨q , and ˜ wpiq
α ”pwpiq
αr0s,wpiq
αr1s,wpiq
αr2s,¨¨¨q .
The generative functions appearing in equations (70) and (71) are speciﬁed for
ně1, under the local-linearity assumption, as
gαrnspxpi`1q
αrns ,vpi`1q
αrns q” Bg
Bvpi`1q
αrns
vpi`1q
αrns ”gpi`1q
αrns
and
fαrnspxpiq
αrns,vpiq
αrnsq” Bf
Bxpiq
αrns
xpiq
αrns ”fpiq
αrns.
For the lowest dynamical order of n“0,
gpi`1q
αr0s “gpxpi`1q
αr0s ,vpi`1q
αr0s q and fpiq
αr0s “fpxpiq
αr0s,vpiq
αr0sq.
It is evident from equation (70) that the causal states ˜vpiq
α at one hierarchical
layer are predicted from states at one level higher in the hierarchy ˜ vpi`1q
α via645
the map ˜gpi`1q
α : ˜zpiq
α speciﬁes the ﬂuctuations associated with these inter-layer
links. Equation (71) asserts that the dynamical transitions of the hidden states
˜xpiq
α are induced within a given hierarchical layer via ˜fpiq
α : The corresponding
ﬂuctuations are given by ˜wpiq
α . In order to describe these transitions more trans-
parently, we spell out equations (70) and (71) explicitly:650
˜vp0q
α “˜gp1q
α p˜xp1q
α ,˜vp1q
α q` ˜zp0q
α 9˜xp1q
α “ ˜fp1q
α p˜xp1q
α ,˜vp1q
α q` ˜wp1q
α
˜vp1q
α “˜gp2q
α p˜xp2q
α ,˜vp2q
α q` ˜zp1q
α 9˜xp2q
α “ ˜fp2q
α p˜xp2q
α ,˜vp2q
α q` ˜wp2q
α
... ...
˜vpM´1q
α “˜gpMq
α `˜zpM´1q
α 9˜xpM´1q “ ˜fpM´1q` ˜wpM´1q
˜vpMq
α “˜zpMq
α 9˜xpMq
α “ ˜fpMq
α ` ˜wpMq
α
where we have set that
˜ϕα ”˜vp0q
α and ˜ gpM`1q
α ”0.
42
Note that the sensory data ˜ ϕα reside at the lowest hierarchical layer and are
to be inferred by the causal states ˜vp1q
α at the corresponding dynamical orders.
At the highest cortical layer M the causal states ˜vpMq
α are described by the
spontaneous ﬂuctuations ˜zpMq
α around their means (which have been set to be
zero without loss of generality). Note that the generalized motions of hidden655
states are still present at the highest cortical level, in just the same way that
they manifest at all the other hierarchical levels: the corresponding spontaneous
ﬂuctuations are given by ˜wpMq
α .
Separating brain states into causal and hidden states, we can now express
the G-density by generalizing equation (64) as660
pp˜ϕ,˜µq “
Nź
α“1
pp˜ϕα,˜µαq“
Nź
α“1
pp˜µpMq
α q
M´1ź
i“0
pp˜µpiq
α |˜µpi`1q
α q
ñ
Nź
α“1
pp˜xpMq
α ,˜vpMq
α q
M´1ź
i“0
pp˜xpiq
α ,˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q
“
Nź
α“1
pp˜xpMq
α ,˜vpMq
α q
M´1ź
i“0
pp˜xpiq
α |˜vpiq
α qpp˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q (72)
where in the second step we have used ˜ µpiq
α “ p˜xpiq
α ,˜vpiq
α q and only the causal
states ˜vpiq
α are involved in the inter-layer transitions in the third step. Also,
it must be understood that pp˜xp0q
α |˜vp0q
α q ”1 in equation (72), which appears
solely for a mathematical compactness. The intra-layer conditional probabilities
pp˜xpiq
α |˜vpiq
α qare given as665
pp˜xpiq
α |˜vpiq
α q “ ppxpiq
αr0s,xpiq
αr1s,¨¨¨| vpiq
αr0s,vpiq
αr1s,¨¨¨q
“
8ź
n“0
ppxpiq
αrns|vpiq
αrnsq (73)
where in the second step we have made use of the assumption of statistical
independence among the generalized states at diﬀerent dynamical orders. The
quantity ppxpiq
αrns|vpiq
αrnsqspeciﬁes the conditional density at the dynamical order
nwithin the hierarchical layer i, where the corresponding ﬂuctuations wpiq
αrns are
assumed to take Gaussian form as
ppxpiq
αrns|vpiq
αrnsq” 1b
2πσαpiq
wrns
exp
„
´
´
xpiq
αrn`1s´fpiq
αrns
¯2
{
´
2σαpiq
wrns
¯
. (74)
43
The conditional densities pp˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q appearing in equation (72) link
two successive causal states in the cortical hierarchy which are speciﬁed by a
similar Gaussian ﬂuctuation for zpiq
αrns via equation (70) as
pp˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q”
8ź
n“0
1b
2πσαpiq
zrns
exp
„
´
´
vpiq
αrns´gpi`1q
αrns
¯2
{
´
2σαpiq
zrns
¯
.
(75)
What is left unspeciﬁed in constructing the G-density fully, i.e. equation (72),
is the prior density pp˜xpMq
α ,˜vpMq
α qat the highest cortical layer. It is given here
explicitly as
pp˜xpMq
α ,˜vpMq
α q ”
8ź
n“0
1b
2πσαpMq
wrns
exp
!
´rxpMq
αrn`1s´fpMq
αrnss2{
´
2σαpMq
wrns
¯)
ˆ
8ź
n“0
1b
2πσαpMq
zrns
exp
!
´rvpMq
αrnss2{
´
2σαpMq
zrns
¯)
. (76)
The prior in the highest cortical layer, equation (76), comprises the lateral gen-
eralized motions of the hidden states and the spontaneous, random ﬂuctuations670
associated with the causal states. It is assumed that both causal and hidden
states ﬂuctuate about zero means.
Next, the Laplace-encoded energy Ecan be written explicitly by substituting
equation (72) into equation (19) and incorporating the likelihood and prior
densities, equations (74), (75), and (76), at all hierarchical layers and dynamical675
orders. After a straightforward manipulation, we obtain the Laplace-encoded
energy for a speciﬁc brain variable µα as
Eαp˜µα, ˜ϕαq “
8ÿ
n“0
$
&
%
1
2ΩαpMq
wrns
´
xpMq
αrn`1s´fpMq
αrns
¯2
`1
2 ln ΩαpMq
wrns
,
.
-
`
8ÿ
n“0
$
&
%
1
2ΩαpMq
zrns
´
vpMq
αrns
¯2
`1
2 ln ΩαpMq
zrns
,
.
-
`
M´1ÿ
i“1
8ÿ
n“0
$
&
%
1
2Ωαpiq
wrns
´
xpiq
αrn`1s´fpiq
αrns
¯2
`1
2 ln Ωαpiq
wrns
,
.
-
`
M´1ÿ
i“0
8ÿ
n“0
$
&
%
1
2Ωαpiq
zrns
´
vpiq
αrns´gpi`1q
αrns
¯2
`1
2 ln Ωαpiq
zrns
,
.
-
44
where the ﬁrst and second terms are from prior-densities at the highest layer,
equation (76), the third term is from equation (74), and last term from equa-
tion (75). A quick inspection reveals that the ﬁrst and second terms can be ab-680
sorbed into the third and fourth terms, respectively. Then, the Laplace-encoded
energy for multiple brain variables is written compactly as
Ep˜µ, ˜ϕq “
Nÿ
α“1
Eαp˜µα, ˜ϕαq
“ 1
2
Nÿ
α“1
8ÿ
n“0
Mÿ
i“1
$
&
%
1
σαpiq
wrns
´
εαpiq
wrns
¯2
`ln σαpiq
wrns
,
.
-
` 1
2
Nÿ
α“1
8ÿ
n“0
Mÿ
i“0
$
&
%
1
σαpiq
zrns
´
εαpi`1q
zrns
¯2
`ln σαpiq
zrns
,
.
-. (77)
where we have deﬁned the prediction errors
εαpiq
zrns ” vpi´1q
αrns ´gpiq
αrns
´
xpiq
αrns,vpiq
αrns
¯
(78)
εαpiq
wrns ” xpiq
αrn`1s´fpiq
αrns
´
xpiq
αrns,vpiq
αrns
¯
. (79)
Thus, it turns out that the Laplace-encoded energy is expressed essentially
as a sum of the prediction-errors squared and their associated variances. It685
appears in equation (77) that the structure of the ﬁrst term diﬀers from the
second term: In the ﬁrst term the hierarchical index runs from i “ 1 which
indicates the lowest cortical layer, while the second term includes additional
i“0 in the hierarchical sum which designates the sensory data, ˜ϕ”˜vp0q. Note
also in equation (78) that εαpM`1q
zrns “vpMq
αrns because the highest hierarchical layer690
is at i“M, accordingly gpM`1q
αrns ”0 by construction.
Table 5 provides the glossary of the mathematical objects involved in the
G-density in the full construct for a single brain activity µα.
To summarize, the ‘full construct’ incorporates into the G-density, both
multi-layer hierarchies corresponding to cortical architecture, and multi-scale695
dynamics in each layer via generalized coordinates. The G-density is expressed
as the sequential product of the priors and the likelihoods, cascading down
the cortical hierarchy to the lowest layer where the sensory data are registered
45
(mediated by causal states), and taking into account the intra-layer dynam-
ics, mediated by hidden states. The ﬁnal form of the Laplace-encoded energy,700
equation (77), has been derived from equation (19) which speciﬁes the Laplace-
encoded energy as the (negative) logarithm of the generative density constructed
for the hidden and causal brain states.
8.3. The full-construct recognition dynamics and neuronal activity
We now describe recognition dynamics incorporating the full construct (sec-705
tion 8.2), given the Laplace-encoded energy Ep˜µ, ˜ϕq, equation (77). In the full
construct, the brain states ˜µα are decomposed into the causal states ˜vα which
link the cortical hierarchy and the hidden states ˜ xα which implement the dy-
namical ordering within a cortical layer.
Distinguishing the ‘path of the modes’ from the ‘modes of the path’, see
Section 6, the learning algorithm for the dynamical causal states on the cortical
layer i can be constructed from
9vpiq
αrns´Dvpiq
αrns ”´κzˆvpiq
αrns¨∇˜vαEp˜µ, ˜ϕq (80)
where κz is the learning rate and ˆvpiq
αrns is the unit vector along vpiq
αrns. As men-710
tioned in Section 6, the crucial assumption here is that when the path of modes
becomes identical to the modes of the path, i.e. 9˜vpiq
α ´D˜vpiq
α Ñ0, the Laplace-
encoded energy E takes its minimum, and vice versa. The gradient operation
in the RHS of equation (80) can be made explicit to give
ˆvpiq
αrns¨∇˜vαEp˜µ, ˜ϕq
“ B
Bvpiq
αrns
»
– 1
2σαpi´1q
zrns
!
εαpiq
zrns
)2
` 1
2σαpiq
zrns
!
εαpi`1q
zrns
)2
` 1
2σαpiq
wrns
!
εαpiq
wrns
)2
ﬁ
ﬂ
“ 1
σαpi´1q
zrns
εαpiq
zrns
Bεαpiq
zrns
Bvpiq
αrns
` 1
σαpiq
zrns
εαpi`1q
zrns
Bεαpi`1q
zrns
Bvpiq
αrns
` 1
σαpiq
wrns
εαpiq
wrns
Bεαpiq
wrns
Bvpiq
αrns
(81)
where one can further see that
Bεαpiq
zrns
Bvpiq
αrns
“´
Bgαpiq
zrns
Bvpiq
αrns
,
Bεαpi`1q
zrns
Bvpiq
αrns
“1, and
Bεαpiq
wrns
Bvpiq
αrns
“´
Bfpiq
αrns
Bvpiq
αrns
.
46
The additional auxiliary variables are introduced:
ξαpiq
zrns ”εpiq
zrns{σαpi´1q
zrns ”Λαpi´1q
zrns
!
vpi´1q
αrns ´gpiq
αrns
´
xpiq
αrns,vpiq
αrns
¯)
, (82)
ξαpiq
wrns ”εpiq
wrns{σpiq
wrns ”Λαpiq
wrns
!
xpiq
αrn`1s´fpiq
rns
´
xpiq
αrns,vpiq
αrns
¯)
, (83)
where Λαpiq
zrns and Λαpiq
wrns are the inverse of the variances,
Λαpiq
zrns ”1{σαpiq
zrns and Λ αpiq
wrns ”1{σαpiq
wrns, (84)
which are called the precisions. Note that the precisions reﬂect the magnitude715
of the prediction errors.
Its is proposed that the auxiliary variables ξαpiq
zrns and ξαpiq
wrns represent error
units and that the brain states, vpiq
αrns and xpiq
αrns, similarly represent state units
or, equivalently, representation units, within neuronal populations [23, 1].
In terms of predictive coding or (more generally) hierarchical message pass-720
ing in cortical networks[21], equation (82) implies that the error-units ξαpiq
zrns
receive signals from causal states vpi´1q
αrns lying in immediately lower hierarchical
layer and also from causal and hidden states in the same layer, vpiq
αrns and xpiq
αrns,
via the generative function gpiq
αrns. Similarly, equation (83) implies that the error-
units ξαpiq
wrns specify prediction-error in the within-layer (lateral) dynamics: ξαpiq
wrns725
designates prediction error between the objective hidden-state xpiq
αrn`1s and its
estimation from one-order lower causal- and hidden-states vpiq
αrns and xpiq
αrns, via
the diﬀerent generative function fpiq
αrns.
With the help of equation (81), one can recast the learning algorithm equa-
tion (80) to give the dynamics of the causal states as
9vpiq
αrns “Dvpiq
αrns`κz
Bgαpiq
rns
Bvpiq
αrns
ξαpiq
zrns´κzξαpi`1q
zrns `κz
Bfpiq
αrns
Bvpiq
αrns
ξαpiq
wrns (85)
which shows clearly how hierarchical links are made among nearest-neighbor
cortical layers. Speciﬁcally, the representation units of causal states vpiq
αrns are730
updated by the error units ξαpi`1q
zrns which reside in the layer immediately above,
and also by the error-units ξαpiq
zrns and ξαpiq
wrns in the same hierarchical layer, all at
the same dynamical order.
47
The intra-layer dynamics of hidden states are generated similarly as
9xpiq
αrns ” Dxpiq
αrns´κwˆxpiq
αrns¨∇˜xαEp˜µ, ˜ϕq
“ Dxpiq
αrns´κwξαpiq
wrn´1s`κw
Bfpiq
αrns
Bxpiq
αrns
ξαpiq
wrns`κw
Bgpiq
αrns
Bxpiq
αrns
ξαpiq
zrns (86)
where κw is the leaning rate. In passing to the second line in equation (86), one735
needs to evaluate
ˆxpiq
αrns¨∇˜xαEp˜µ, ˜ϕq
Ñ 1
σαpiq
wrn´1s
εαpiq
wrn´1s
Bεαpiq
wrn´1s
Bxpiq
αrns
` 1
σαpiq
wrns
εαpiq
wrns
Bεαpiq
wrns
Bxpiq
αrns
` 1
σαpi´1q
zrns
εαpiq
zrns
Bεαpiq
zrns
Bxpiq
αrns
,
and an explicit evaluation of the derivatives of the prediction errors, equa-
tions (78) and (79). The hidden-state learning algorithm, equation (86), speciﬁes
how the representation-units xpiq
αrns are driven by the error-units in the current
layer i at both the immediately lower dynamical order ξαpiq
wrn´1s and the same740
dynamical order ξαpiq
wrns, and also by the error units ξαpiq
zrns in the current layer at
the same dynamical order.
To summarize, the hierarchical, dynamical causal structure of the genera-
tive model is fully implemented in the mathematical constructs given by equa-
tions (82) and (83) (specifying prediction errors), and equations (85) and (86)745
(specifying update rules for state-units).
According to these equations, the state units come to encode the condi-
tional expectations of the environmental causes of sensory data, and the error
units measure the discrepancy between these expectations and the data. Er-
ror units are driven by state units at the same layer and from the layer below,750
whereas state units are driven by error units at the same layer and the layer
above. Thus, prediction errors are passed up the hierarchy (bottom-up) and pre-
dictions (conditional expectations) are passed down the hierarchy (top-down),
fully consistent with predictive coding [6].
48
8.4. Parameters and hyper-parameters: Synaptic eﬃcacy and gain755
Thus far we have discussed how environmental variables can be inferred
given an appropriate G-density. In this section we discuss how the G-density
itself can be learned. It is proposed that the dynamics of neural systems is
captured by three time-scales, τµ ă τθ ă τγ. The ﬁrst, τµ, represents the
timescale of the dynamics of suﬃcient statistics of the encoded in the R-density
i,.e µ ” px,vq as described above. In contrast τθ and τγ represent the slow
timescale of synaptic eﬃcacies and gains which are parameterised implicitly in
equation (77) through the generative functions, f and g, and the variances σ
(or the precisions Λ, equation (84)), respectively. Under the FEP slow variables
are assumed to be approximately ‘static’ or ‘time-invariant’ in contrast to the
‘time-varying’ neuronal states µ [23]. Second, changes in θ and γ (with respect
to a small δt) have a much smaller eﬀect on the Laplace-encoded energy (or
IFE) than do changes in µ, i.e.
BF
Bθ
δθ
δt ! BF
Bµ
δµ
δt.
The latter point implies that, from the perspective of gradient-descent, what is
relevant for θ and γ is not the IFE F but the accumulation, more precisely the
integration of F over time [19]
SrFs”
ż
dtFp˜µ, ˜ϕ; θ,γq (87)
where the time-dependence of F is implicit through the arguments. To distin-
guish their diﬀerent roles, θpiq
α are called parameters and γpiq
α are called hyper-
parameters, corresponding to brain state µα, in each hierarchical layer i. Equa-
tions (82) and (83) can now be generalized to include these parameters and
hyper-parameters as
ξαpiq
zrns “Λαpi´1q
zrns pγpi´1q
α q
!
vpi´1q
αrns ´gpiq
αrns
´
xpiq
αrns,vpiq
αrns; θpiq
α
¯)
, (88)
ξαpiq
wrns “Λαpiq
wrnspγpiq
α q
!
xpiq
αrn`1s´fpiq
αrns
´
xpiq
αrns,vpiq
αrns; θpiq
α
¯)
. (89)
49
The Laplace-encoded energy including θ and γ may therefore be written as
Ep˜µ, ˜ϕ; θ,γq “ 1
2
Nÿ
α“1
8ÿ
n“0
Mÿ
i“1
!
εαpiq
wrnsξαpiq
wrns´ln Λαpiq
wrns
)
` 1
2
Nÿ
α“1
8ÿ
n“0
Mÿ
i“0
!
εαpi`1q
zrns ξαpi`1q
zrns ´ln Λαpiq
zrns
)
. (90)
We are now in a position to write down the recognition dynamics for the
slow synaptic eﬃcacy θand for the slower synaptic gain γ. Speciﬁcally, gradient
descent for the parameters θpiq
α is applied using the time-integral of F, given in
equation (87), assuming a static model (i.e., without dynamical order indices),
as
9θpiq
α “´κθˆθpiq
α ¨∇θS
which, when temporal diﬀerentiation is repeated on both sides, gives rise to
:θpiq
α “´κθˆθpiq
α ¨∇θEp˜µ, ˜ϕ; θ,γq. (91)
After explicitly carrying out the gradient on the RHS of equation (91), one ob-
tains an equation to minimise θpiq
α corresponding to brain variable µα at cortical
layer i
:θpiq
α “
8ÿ
n“0
»
–κθ
Bgpiq
αrns
Bθpiq
α
ξαpiq
zrns`κθ
Bfpiq
αrns
Bθpiq
α
ξαpiq
wrns
ﬁ
ﬂ (92)
where the summation over the dynamic index nreﬂects the generalized motion
over causal as well as hidden states. According to Equation (92) synaptic eﬃcacy
is inﬂuenced by error-units only in the same cortical layer.
Similarly, the learning algorithm for the hyper-parameters γ, speciﬁcally for
γpiq
α associated with brain’s representation of environmental statesµα at cortical
layer i, is given from
9γpiq
α “´κγˆγpiq
α ¨∇γS
which results in760
:γpiq
α “ ´1
2
8ÿ
n“0
»
–κγ
BΛαpiq
wrns
Bγpiq
α
!
ξαpiq
wrns
)2
´κγ
B
Bγpiq
α
ln Λαpiq
wrns
ﬁ
ﬂ
´1
2
8ÿ
n“0
»
–κγ
BΛαpiq
zrns
Bγpiq
α
!
ξαpi`1q
zrns
)2
´κγ
B
Bγpiq
α
ln Λαpiq
zrns
ﬁ
ﬂ. (93)
50
According to this equation, synaptic gains are inﬂuenced by error units in the
same layer ξpiq
w and also by error units in one-layer above ξpi`1q
z .
Note that the equations forθand γ, equations (92) and (93), areby construc-
tion second-order diﬀerential equations, unlike the corresponding equations for
state-units µ[equations (85) and (86)], which are ﬁrst-order in time [20]. Table765
6 provides the summary of mathematical symbols appearing in the recognition
dynamics in the dynamical construct and also in the static construct.
To summarize the FEP prescribes recognition dynamics by gradient descent
with respect to the suﬃcient statistics ˜µ, parameters θ, and hyper-parameters
γ on the Laplace-encoded energy Ep˜µ, ˜ϕ; θ,γq, given the sensory input ˜ϕ. At770
the end of this process, an optimal ˜µ˚ is speciﬁed which represents the brain’s
posterior expectation of the environmental cause of the observed sensory data.
In theory the second term in the IFE F, equation (18), can be ﬁxed according
to equation (17) thereby completing the minimization of the IFE, although
in practice this is rarely done and the focus is on approximating the means,775
parameters and hyper-parameters.
This whole minimization process is expressed abstractly as
˜µ˚ “arg min
˜µ
Fp˜µ, ˜ϕq (94)
where ˜µ˚ is the minimizing (optimal) solution and the conditional dependence
m is expressed explicitly. The resulting minimized IFE can be calculated by
substituting the optimizing ˜µ˚ for ˜µ as
F˚ “Fp˜µ˚, ˜ϕ q.
The only remaining task is to specify the generative functions f and g,
which will depend on the particular system being modelled. We have utilised a
concrete model in our calculation in Section 7. Examples of various generating
functions have already been provided[21, 9, 29, 28, 48], to which we refer the780
reader.
51
8.5. Active inference on the full construct
The IFE also accounts for an active inference by minimising the IFE with
respect to action, for which a formal procedure can be written as
a˚ “arg min
a
Fp˜µ, ˜ϕpaqq (95)
where a˚ is the minimizing solution. Similarly with equation (52) we can write
down the gradient descent scheme for the minimisation in the full construct for
action corresponding to brain’s representation µα as
9aα “´κaˆaα ¨∇aαEp˜µ, ˜ϕpaqq (96)
where equation (90) is to be used for the Laplace-encoded energy. Then, af-
ter the gradient operation is completed, the organism’s action is implemented
explicitly in the brain as
9aα “´κa
8ÿ
n“0
d˜ϕαrns
daα
Λαp0q
zrnsεαp1q
zrns (97)
where εαp1q
zrns “ϕαrns´gp1q
αrnspxp1q
αrns,vp1q
αrns; θp1q
α qis the prediction-error associated
with learning of the sensory data on the dynamical order nat the lowest cortical
layer and Λ αp0q
zrns “ Λαp0q
zrnspγp0q
α q is the precision of the sensory noise. To our785
knowledge most existing models of active inference under the FEP require one
to provide an explicit world model. Thus an important goal for for future work
will be to develop agent based models that work with the full construct.
9. Discussion
The FEP framework is an ambitious project, spanning a chain of reasoning790
from fundamental principles of biological self-maintenance essential for sustain-
able life, to a mechanistic brain theory that proposes to account for a startling
range of properties of perception, cognition, action and learning. It draws con-
clusions about neurocognitive mechanisms from extremely general statistical
considerations regarding the viability of organism’s survival in unpredictable795
environments. Under certain assumptions - which we discuss in more detail
52
below - it entails a hierarchical predictive processing model geared towards the
inference and control of the hidden causes of sensory inputs, which both sheds
new light on existing data about functional neuroanatomy and motivates a num-
ber of speciﬁc hypotheses regarding brain function in health and in disease. At800
the same time, the current status of much of the research under the rubric of
the FEP does depend on, to diﬀerent degrees, a variety of assumptions and ap-
proximations, both at the level of the overarching theory and with regard to the
speciﬁc implementation (or process theory) the theory proposes. In this section,
we discuss the consequences of some of more important of these assumptions and805
approximations, with respect to the framework and implementation described
in the body of this paper.
A central assumption in this (representative) exposition of the FEP is that
the brain utilizes properties of Gaussian distributions in order to carry out
probabilistic computation. Speciﬁcally, the Laplace approximation assumes a810
Gaussian functional form for the R-density and G-density which are encoded by
suﬃcient statistics, see Section 4. Additionally, it is assumed that the R-density
is tightly peaked, i.e., the variance and covariance are small, see Section 4. This
assumption implies that an organism only represents the expectation value of
environmental variables, and not a distribution over states, see [38, 49, 50] for a815
nice descriptions of this assumption. At ﬁrst glance this assumption may appear
troublesome, because it suggests that organisms do not directly represent the
uncertainty of environmental variables (hidden causes of sensory signals). This
worry is misplaced, however, since representations of uncertainty enter into the
FEP formalism via precisions on the expectations of brain states that comprise820
the G-density, see equation (32). Intuitively this means that organisms do not
encode uncertainty about world states per se, but rather uncertainties about
their model of how hidden causes relate to each other and to sensory signals.
The main advantage of adopting Gaussian assumptions is that they vastly
simplify the implementation of the FEP, and make it formally equivalent to the825
more widely known predictive coding framework [51, 8, 41], see the Introduction.
Furthermore, it can be argued this implementation is compatible with a plausi-
53
ble neuronal functional architecture in terms of message passing in cortical hier-
archies [16]. Speciﬁcally, inferred variables (hidden causes) can be represented in
terms of neural ﬁring rates; the details of generative models encoded as patterns830
of synaptic connectivity, and the process of IFE minimisation by the relaxation
of neuronal dynamics [52]. The concept of hierarchical generative models, see
Section 8, also maps neatly onto the hierarchical structure of cortical networks,
at least in the most frequently studied perceptual modalities like vision. Here,
the simple idea is that top-down cortical signalling conveys predictions while835
bottom-up activity returns prediction errors [52]. However, it remains an open
question whether representing the world in terms of Gaussian distributions is
suﬃcient given the complexities of real-world sensorimotor interactions. For
example, standard robotics architectures have long utilized practical strategies
for representing more complex distributions [53] including (for example) multi-840
modal peaks [54].Other authors have proposed that brains engage in Bayesian
sampling rather than the encoding of probability distributions, suggesting that
sampling schemes parsimoniously explain classic cognitive reasoning errors [50].
Whether these alternate schemes can be used to construct more versatile and
behaviourally powerful implementations of the FEP, and whether they remain845
compatible with neuronally plausible process theories, remains to be seen.
The minimisation of IFE, for both inference and learning, is assumed to
be implemented as a gradient descent scheme. While this has the major ad-
vantage of transforming diﬃcult or infeasible inference problems into relatively
straightforward optimization problems, it is not clear whether the propose gra-850
dient descent schemes always have good convergence properties. For example,
the conditions under which gradient descent will become stuck in local minima,
or fail to converge in an appropriate amount of time, are not well understood.
Furthermore, parameters such as learning rate will be crucial for the timely
inference of the dynamics of variables, as well as central to the dynamics of855
control, see Fig. 1 and 2. Parameters like these, which play important roles in
the estimation of but not speciﬁcation of the IFE, can be incorporated into
process theories in many ways, with as yet no clear consensus (though see, for
54
one proposal [55]).
The implementation described in this paper supports inference in dynamical860
environments. This is based on the concept of generalised motions, whereby
it is assumed that the brain infers not only the current value of environmental
variables (e.g.,position) but also their higher-order derivatives (i.e., velocity,
acceleration, jerk, etc.). This involve both that the relevant sensory noise is
diﬀerentiable, and, that interactions between derivatives are linear [18]. The865
extent to which these assumptions are justiﬁable remains unclear, as does the
utility of encoding generalized motions in practical applications. It is likely,
for example, that signal magnitudes after the second derivative will be small
and carry considerable noise, thus practical usefulness of including higher order
derivatives is unclear, although this may be justiﬁable in some cases [56].870
Under active inference, prediction errors are minimised by acting on the
world to change sensory input, rather than by modifying predictions. Active in-
ference therefore depends on the ability to make conditional predictions about
the sensory consequences of actions. To achieve this the FEP assumes that
agents have a model of the relationship between action and sensation, in the875
form of an inverse model, in addition to their generative model [57, 29]. In the
general case the speciﬁcation of an inverse model is non-trivial [42], which at ﬁrst
glance seems like a strong assumption. However, the FEP suggests generation
of motor actions are driven through the fulﬁlment of proprioceptive predictions
only, where relations between actions and (proprioceptive) sensations are as-880
sumed to be relatively simple such that minimisation of prediction error can be
satisﬁed by simple reﬂex arcs [9, 43]. On this view, action only indirectly aﬀects
exteroceptive or interoceptive sensations, obviating the need for complicated in-
verse models like those described in the motor control literature [42, 43]. In the
implementation of the FEP given in this paper there is no distinction between885
diﬀerent types of sensory input.
In Section 7 we showed that behaviour is extremely sensitive to precisions.
This is often presented as an advantage of the framework, allowing an agent
to balance sensory inputs against internal predictions in an optimal and con-
55
text sensitive manner, through precision weighting (which is associated with890
attention) [8]. Supposedly the appropriate regulation of precision should also
emerge as a consequence of the minimisation of free energy, see Section 8 for a
description of this. But how the interplay between brain states and precisions
will unfold in an active agent involved in a complex behaviour is far from clear.
Where do the priors come from? This is an intuitive way to put a key895
challenge for models involving Bayesian inference [45]. To some extent the
FEP circumvents this problem via the concept of hierarchical models, which
maps neatly onto the framework of ‘empirical Bayes’ [44]. In this view, the
hierarchical structure allows priors at one level to be supplied by posteriors at
a higher level. Sensory data are assumed to reside only at the lowest level in900
the hierarchy, and the highest level is assumed to generate only spontaneous
random ﬂuctuations. While this is a powerful idea within formal frameworks,
its practicality for guiding inference in active agents remains to be established.
These discussion points merely scratch the surface of the promises and pit-
falls of the FEP formalism, a formalism which is rapidly advancing both in its905
theoretical aspects and in its various implementations and applications. Never-
theless, research directed towards addressing these issues should further clarify
both the explanatory power and the practical utility of this increasingly inﬂuen-
tial framework. In this paper, we have focused on encapsulating within a single
presentation the essential mathematical aspects of the FEP and its implemen-910
tation. In doing so we hope to clarify the scientiﬁc contributions of the FEP,
facilitate discussions of some of the core issues and assumptions underlying it,
and motivate additional research to explore how far the grand ambitions of the
FEP can be realized in scientiﬁc practice.
10. Acknowledgements915
The work of C.S.K. was supported by a special fund granted by the Chon-
nam National University. C.S.K. is grateful to the hospitality of the School
of Engineering aand Informatics at the University of Sussex where he spent a
56
sabbatical. Support is also gratefully acknowledged from the Dr. Mortimer and
Theresa Sackler Foundation.920
References
References
[1] K. Friston, The free-energy principle: a uniﬁed brain theory?, Nat Rev
Neurosci 11 (2010) 127–138.
[2] K. Friston, The free-energy principle: a rough guide to the brain?, Trends925
Cogn Sci 13 (2009) 293–301.
[3] K. Friston, Some free-energy puzzles resolved: response to Thornton,
Trends Cogn Sci 14 (2010) 54.
[4] H. von Helmholz, Treatise on physiological optics, 3rd Edition, Voss, Ham-
burg, 1909.930
[5] D. C. Knill, A. Pouget, The Bayesian brain: the role of uncertainty in
neural coding and computation, Trends Neurosci 27 (2004) 712–719.
[6] R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-ﬁeld eﬀects, Nat Neurosci 2
(1999) 79–87.935
[7] A. Bubic, D. Y. von Cramon, R. I. Schubotz, Prediction, cognition and the
brain, Front Hum Neurosci 4 (2010) 25–25.
[8] A. Clark, Whatever next? predictive brains, situated agents, and the future
of cognitive science, Behavioral and Brain Sciences 36 (03) (2013) 181–204.
[9] K. J. Friston, J. Daunizeau, J. Kilner, S. J. Kiebel, Action and behavior:940
a free-energy formulation, Biol Cybern 102 (3) (2010) 227–260.
[10] A. K. Seth, The cybernetic bayesian brain: From interoceptive inference to
sensorimotor contingencies, Open MIND. Frankfurt A. M: MIND Group.
57
[11] R. Neal, G. Hinton, A view of the em algorithm that justiﬁes incremental,
sparse, and other variants, in: M. Jordan (Ed.), Learning in Graphical945
Models, MIT Press, Cambridge, MA, 1998, pp. 355–368.
[12] G. Hinton, R. Zemel, Autoencoders, minimum description length, and
helmholtz free energy., in: G. T. J. D. Cowan, J. Alspector (Eds.), Ad-
vances in Neural Information Processing Systems 6, Morgan Kaufmann,
San Mateo, CA, 1994, pp. 3–10.950
[13] R. Zemel, G. Hinton, Learning population codes by minimizing description
length, Neural Computation 7 (1995) 549–564.
[14] P. Dayan, G. E. Hinton, R. M. Neal, R. S. Zemel, The Helmholtz machine,
Neural Comput 7 (1995) 889–904.
[15] K. Friston, Life as we know it, Journal of The Royal Society Interface955
10 (86) (2013) 20130475.
[16] K. Friston, A theory of cortical responses, Philos Trans R Soc Lond B Biol
Sci 360 (2005) 815–836.
[17] K. Friston, J. Kilner, L. Harrison, A free energy principle for the brain, J
Physiol Paris 100 (2006) 70–87.960
[18] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, W. Penny, Vari-
ational free energy and the Laplace approximation, Neuroimage 34 (2007)
220–234.
[19] K. J. Friston, K. E. Stephan, Free-energy and the brain, Synthese 159
(2007) 417–458.965
[20] K. J. Friston, N. Trujillo-Barreto, J. Daunizeau, DEM: a variational treat-
ment of dynamic systems, Neuroimage 41 (2008) 849–885.
[21] K. Friston, Hierarchical models in the brain, PLoS Comput Biol 4 (2008)
e1000211–e1000211.
58
[22] K. J. Friston, J. Daunizeau, S. J. Kiebel, Reinforcement learning or active970
inference?, PLoS One 4 (2009) e6421–e6421.
[23] K. Friston, S. Kiebel, Cortical circuits for perceptual inference, Neural Netw
22 (2009) 1093–1104.
[24] K. Friston, S. Kiebel, Predictive coding under the free-energy principle,
Philos Trans R Soc Lond B Biol Sci 364 (2009) 1211–1221.975
[25] R. L. Carhart-Harris, K. J. Friston, The default-mode, ego-functions and
free-energy: a neurobiological account of Freudian ideas, Brain.
[26] R. A. Adams, S. Shipp, K. J. Friston, Predictions not commands: active
inference in the motor system, Brain Structure and Function 218 (3) (2013)
611–643.980
[27] K. Friston, K. Stephan, B. Li, J. Daunizeau, Generalised ﬁltering, Mathe-
matical Problems in Engineering 2010.
[28] G. Pezzulo, F. Rigoli, K. Friston, Active inference, homeostatic regulation
and adaptive behavioural control, Progress in neurobiology 134 (2015) 17–
35.985
[29] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzulo, Active
inference: A process theory, Neural Computation.
[30] G. E. Hinton, R. S. Zemel, Autoencoders, minimum description length, and
helmholtz free energy, Advances in neural information processing systems
(1994) 3–3.990
[31] K. Friston, R. A. Adams, L. Perrinet, M. Breakspear, Perceptions as hy-
potheses: saccades as experiments, Frontiers in psychology 3.
[32] G. E. Hinton, Learning multiple layers of representation, Trends in cogni-
tive sciences 11 (10) (2007) 428–434.
[33] K. Friston, Is the free-energy principle neurocentric?, Nat Rev Neurosci.995
59
[34] T. M. Cover, J. A. Thomas, Elements of information theory, Wiley-
Interscience, New York, 1991.
[35] A. Adkins, Equilibrium thermodynamics, 3rd Edition, Cambridge Univer-
sity Press, Cambridge, 1983.
[36] K. Friston, Variational ﬁltering, NeuroImage 41 (2008) 747–766.1000
[37] K. Huang, Statistical mechanics, John Wiley and Sons, New York, 1987.
[38] R. Bogacz, A tutorial on the free-energy framework for modelling
perception and learning, Journal of Mathematical Psychology (2015)
–doi:http://dx.doi.org/10.1016/j.jmp.2015.11.003.
URL http://www.sciencedirect.com/science/article/pii/1005
S0022249615000759
[39] R. Zwanzig, Nonequilibrium statistical mechanics, Oxford University Press,
Oxford, 2001.
[40] S. Haykin, N. Network, A comprehensive foundation, Neural Networks
2 (2004).1010
[41] K. Friston, S. Kiebel, Predictive coding under the free-energy principle,
Philosophical Transactions of the Royal Society B: Biological Sciences
364 (1521) (2009) 1211–1221.
[42] D. M. Wolpert, Computational approaches to motor control, Trends in
cognitive sciences 1 (6) (1997) 209–216.1015
[43] K. Friston, What is optimal about motor control?, Neuron 72 (3) (2011)
488–498.
[44] G. Casella, R. Berger, Statistical inference, Duxbury, Paciﬁc Grove, 2002.
[45] R. E. Kass, D. Steﬀey, Approximate Bayesian inference in conditionally in-
dependent hierarchical models (parametric empirical Bayes models), Jour-1020
nal of the American Statistical Association 407 (1989) 717–726.
60
[46] S. Zeki, S. Shipp, The functional logic of cortical connections, Nature 335
(1988) 311–317.
[47] D. J. Felleman, D. C. Van Essen, Distributed hierarchical processing in the
primate cerebral cortex, Cerebral Cortex 1 (1991) 1–47.1025
[48] L. Pio-Lopez, A. Nizard, K. Friston, G. Pezzulo, Active inference and robot
control: a case study, Journal of The Royal Society Interface 13 (122) (2016)
20160616.
[49] A. Pouget, J. M. Beck, W. J. Ma, P. E. Latham, Probabilistic brains:
knowns and unknowns, Nature neuroscience 16 (9) (2013) 1170–1178.1030
[50] D. C. Knill, A. Pouget, The bayesian brain: the role of uncertainty in
neural coding and computation, TRENDS in Neurosciences 27 (12) (2004)
712–719.
[51] P. Elias, Predictive coding–i, IRE Transactions on Information Theory 1 (1)
(1955) 16–24.1035
[52] A. M. Bastos, W. M. Usrey, R. A. Adams, G. R. Mangun, P. Fries, K. J.
Friston, Canonical microcircuits for predictive coding, Neuron 76 (4) (2012)
695–711.
[53] S. Thrun, W. Burgard, D. Fox, Probabilistic robotics, MIT press, 2005.
[54] M. Otworowska, J. Kwisthout, I. van Rooij, Counter-factual mathematics1040
of counterfactual predictive models, Frontiers in psychology 5.
[55] M. Joﬃly, G. Coricelli, Emotional valence and the free-energy principle,
PLoS Comput Biol 9 (6) (2013) e1003094.
[56] B. Balaji, K. Friston, Bayesian state estimation using generalized coordi-
nates, in: SPIE Defense, Security, and Sensing, International Society for1045
Optics and Photonics, 2011, pp. 80501Y–80501Y.
61
[57] A. K. Seth, A predictive processing theory of sensorimotor contingencies:
Explaining the puzzle of perceptual presence and its absence in synesthesia,
Cognitive neuroscience 5 (2) (2014) 97–118.
Appendix A. Variational Bayes: Ensemble learning1050
Here, we present an alternative approach to how the brain may achieve
true posterior density, which makes no assumptions about how the R-density
is encoded in the brain’s state; namely the Laplace approximation for the R-
density is dispensed with. Technically the above method is termed ‘Generalized
Filtering’ in [27] and the present one ‘Variational Filtering’ in [36].1055
According to equation (7) the IFE is a functional of the R-densityqpϑqwhere
the variable ϑdenotes the environmental states collectively. The environmental
sub-states ϑα, α “1,2,¨¨¨ ,N, must vary on distinctive time-scale, τ1 ăτ2 ă
¨¨¨ă τN, where τα is associated with ϑα, conforming to physics laws, in general.
Then, the sub-densities may be assumed to be statistically-independent to allow
the factorization approximation for qpϑqas
qpϑq”
Nź
i“1
qαpϑαq. (A.1)
Equation (4) gives rise to the individual normalization condition:
ż
dϑ qpϑq“
Nź
α“1
ż
dϑα qαpϑαq“ 1
which asserts that ż
dϑα qαpϑαq“ 1. (A.2)
When the factorization approximation, equation (A.1) is substituted into equa-
tion (7), the IFE is written as
F “
ż ź
α
rdϑαqαpϑαqs
#
Epϑ,ϕq`
ÿ
σ
ln qσpϑσq
+
” Frqpϑq; ϕs
where the last expression indicates explicitly that the IFE is to be treated as
a functional of the R-density. We now optimize the IFE functional by taking
62
the variation of F with respect to a particular R-density qβpϑβq. We treat
the remainder of the ensemble densities as constant and use the normalization
constraint, equation (4), in the form
λ
˜ź
α
ż
dϑα qαpϑαq´ 1
¸
“0 (A.3)
where λ is a Lagrange multiplier.
A straightforward manipulation brings about
δβF “
ż
dϑβ
#ż ź
α‰β
dϑαqαpϑαq
˜
Epϑ,ϕq`
ÿ
σ
ln qσpϑσq
¸
`1 `λ
+
δqβ
where δβ represents a functional derivative with respect to qβpϑβq. Next, by
imposing δβF ”0 it follows that the integration must vanish identically for any
change in δqβ,
ż ź
α‰β
dϑαqαpϑαq
˜
Epϑ,ϕq`
ÿ
σ
ln qσpϑσq
¸
`1 `λ“0
which is to be solved for qβpϑβq. The result brings out the optimal density for
the sub-state ϑβ as
q˚
β “exp
#
´pλ`1q´
ÿ
σ‰β
ż ź
α‰β
dϑαqαpϑαqln qσpϑσq´ Eβpϑβ,ϕq
+
(A.4)
where use has been made of the deﬁnition
Eβpϑβ,ϕq”
ż ź
α‰β
dϑαqαpϑαqEpϑ,ϕq (A.5)
which is the partially-averaged energy [17, 18]. Here, it is worthwhile to note
that the following relation holds
ż
dϑβ qβpϑβqEβpϑβ,ϕq“
ż
dϑ qpϑqEpϑ,ϕq,
which states that the expectation of the partially-averaged energy Eβpϑβ,ϕq
under qβpϑβq is the average energy, i.e. the ﬁrst term in equation (9). The
undetermined Lagrange multiplier is now ﬁxed by the normalization constraint,
equation (A.2), which results in
„ż
dϑβ e´Eβpϑβ,ϕq

exp
#
´pλ`1q´
ÿ
σ‰β
ż ź
α‰β
dϑαqαpϑαqln qσpϑσq
+
“1,
63
which is to be solved for λ. When the determined λ is substituted back into
equation (A.4), the resulting ensemble-learned R-density can be expressed for-
mally as 3
q˚
βpϑβq“ 1
Zβ
e´Eβpϑβ,ϕq (A.6)
where Zβ has been deﬁned to be
Zβ ”
ż
dϑβ e´Eβpϑβ,ϕq. (A.7)
The superscript ˚ appearing in q˚
β indicates that it is the solution which op-
timizes the IFE. The functional form of equation (A.6) is reminiscent of the1060
equilibrium canonical ensemble in statistical physics in which the normalization
factor Zβ is called the partition function of the subsystem tϑβu[37].
Under the factorization approximation, by substituting equation (A.6) into
equation (A.1), the R-density becomes
q˚pϑq“ 1
ZT
e´ETpϑ,ϕq (A.8)
where
ETpϑ,ϕq”
Nÿ
α“1
Eαpϑα,ϕq and ZT ”
Nź
α“1
Zα “
ż
dϑe´ETpϑ,ϕq.
In equation (A.8) ZT may be called the ‘total’ partition function of the envi-
ronmental states and ET is the sum of the partially-averaged energies. Note
that, as a consequence of the ensemble-learning, the optimizing R-density ap-1065
proximates the posterior density ppϑ|ϕq(see Section 3 and below). In principle,
the optimizing R-density, equation (A.8), completes the ensemble-learning of
the sensory data. However, it does not provide a functionally ﬁxed-form for
the optimal R-density. This is because the partially-averaged energy appearing
on the RHS of equation (A.8) is a functional of the R-density itself (see equa-1070
tion (A.5)). One possible way to obtain a closed form of q˚pϑ,ϕq is to seek
3Note that the minus sign arises in the exponent because we have deﬁned the energy as
equation (10) diﬀerently from other papers on the free energy principle. We have made this
choice because our deﬁnition resembles the Boltzmann factor in the canonical ensemble in
statistical physics.
64
a self-consistent solution : One starts with an educated guess (an ‘ansatz’) for
the optimal R-density to evaluate the partially-averaged energy, equation (A.5)
and uses the outcome to update the R-density, equation (A.6). This iterative
process is to be continued until a convergence reaches between estimation and1075
evaluation of the R-densities.
We now exploit what actually the optimal R-density, q˚
βpϑβqgiven in equa-
tion (A.6), is. The partially averaged-energy appearing inq˚
β can be manipulated
as
Eβpϑβ,ϕq “
ż ź
α‰β
dϑαqαpϑαqEpϑ,ϕq
“ ´
ÿ
σ
ż ź
α‰β
dϑαqαpϑαqln ppϑσ,ϕσq, (A.9)
where we have used the factorization approximation for the G-density appearing
in the energy E “´ ln ppϑ,ϕqas
ppϑ,ϕq“
ź
σ
ppϑσ,ϕσq“
ź
σ
ppϑσ|ϕσqppϕσq. (A.10)
Next, one can separate out the environmental sub-state ϑβ among summation
on the RHS of equation (A.9) to cast it into
Eβpϑβ,ϕq“´ ln ppϑβ,ϕβq´
ÿ
σ‰β
ż ź
α‰β
dϑαqαpϑαqln ppϑσ,ϕσq. (A.11)
Then, it follows from equation (A.6) that
q˚
βpϑβq“ e´Eβpϑβ,ϕq
ş
dϑβ e´Eβpϑβ,ϕq Ñ ppϑβ,ϕβqş
dϑβppϑβ,ϕβq “ppϑβ|ϕβq,
where the last step can be obtained by noticing the identity, ppϑβ,ϕβq “
ppϑβ|ϕβqppϕβq, and
ş
dϑβppϑβ,ϕβq“ ppϕβq. Finally, the ensemble-learned R-
density, equation (A.8), is given by
q˚pϑq“
ź
α
q˚
αpϑαq“
ź
α
ppϑα|ϕαq“ ppϑ|ϕq. (A.12)
Equation (A.12) states that the R-densityqpϑqis directed to the posterior ppϑ|ϕq1080
when the IFE, equation (7) is minimized, conforming to the idea of variational
Bayes.
65
By substituting the optimal R-density, equation (A.12), into expression for
IFE given in equation (7), we can also obtain the minimized IFE as
F˚ “
ż
dϑ q˚pϑqln q˚pϑq
ppϑ,ϕq
“
ż
dϑ q˚pϑqln ppϑ|ϕq
ppϑ|ϕqppϕq
“ ´ ln ppϕq
ż
dϑ q˚pϑq
“ ´ ln ppϕq. (A.13)
where we have used equation (A.12) in moving to second line and the normal-1085
ization condition for q˚pϑqin the last step. Note that we have made it explicit
that the sensory density ppϕqis conditioned on the biological agent m. Thus,
we have come to a conclusion that the minimum IFE provides a tight bound on
surprisal.
In summary, the variation of the IFE functional with respect to the R-density1090
(ensemble-learning) has allowed us to specify an optimal (ensemble-learned) R-
density, q˚pϑ,ϕq, selected among an ensemble of R-densities. The speciﬁed
R-density is the brain’s solution to statistical inference of the posterior density
about the environmental states given sensory inputs. The minimum IFE, ﬁxed
in this way, is identical to the surprisal. To fulﬁll this it was assumed that dis-1095
tinctive independent time-scales characterize environmental sub-states (the fac-
torization approximation). The ensemble-learned R-density of each partitioned
variable set ϑβ, q˚
βpϑβq, is speciﬁed by the corresponding partially-averaged en-
ergy (see equation (A.6)). The inﬂuence from other environmental sets tϑσu
(σ ‰ β) occurs as their average eﬀect: Their complicated interactions have1100
been averaged out in equation (A.5). In this sense, ϑβ may be regarded as a
‘mean-ﬁeld’ of the environmental states. Accordingly, the procedure described
in the above is sometimes referred to as a mean-ﬁeld approximation [20, 36].
Appendix B. Dynamic Bayesian Thermostat
% A Simple Bayesian Thermostat1105
66
% A f r e e energy p r i n c i p l e f o r a c t i o n and perception s c i e n c e s : A mathematical e v a l u a t i o n
% Christopher L . Buckley , Chang Sub Kim, Simon M. McGregor and Anil K. Seth
c l e a r ;
rng ( 6 ) ;
%simulation params1110
simTime=100; dt =0.005; time =0: dt : simTime ;
N =length ( time ) ;
a c t i o n =true ;
%Generetaive Model Parameters
Td = 4 ; %d e s i r e d temperature1115
%Time we trun on a c t i o n
actionTime =simTime /4;
%i n i t i a l i s e s e n s o r s1120
rho 0 (1) =0;
rho 1 (1)=0;
%sensory v a r i a n c e s
Omega z0 =0.1;1125
Omega z1 =0.1;
%hidden s t a t e v a r i a n c e s
Omega w0 =.1;
Omega w1 =.1;
1130
%Params f o r g e n e r a t i v e p r o c e s s
T0 = 100; %temperature at x=0
%i n t i a l i s e brain s t a t e v a r i a b l e s
mu 0 (1)=0;1135
67
mu 1 (1)=0;
mu 2 (1)=0;
% sensory n o i s e in the n e g e r a t e i v e pro es s
zgp 0 = randn (1 ,N) ∗. 1 ;1140
zgp 1 = randn (1 ,N) ∗. 1 ;
%I n i t i a l i s e a c t i o n v a r a i b l e
a (1) =0;
1145
%I n i t i a l i s e g e n e r a t i v e p r o c e s s
x dot (1) = a ( 1 ) ;
x (1) = 2 ;
T(1) = T0/( x (1)ˆ2+1);
Tx(1)= ´2∗T0∗x ( 1 )∗( x(1)ˆ2+1)ˆ ´2;1150
T dot (1) = Tx( 1 ) ∗( x dot ( 1 ) ) ;
%I n i t i a l i s e sensory input
rho 0 (1) = T( 1 ) ;
rho 1 (1) = T dot ( 1 ) ;1155
%I n t i a l i s e e r r o r terms
e p s i l o nz 0 = ( rho 0 (1)´mu 0 ( 1 ) ) ;
e p s i l o nz 1 = ( rho 1 (1)´mu 1 ( 1 ) ) ;
1160
e p s i l o nw 0 = ( mu 1(1)+mu 0(1) ´Td ) ;
e p s i l o nw 1 = ( mu 2(1)+mu 1 ( 1 ) ) ;
%I n t i a l i s e V a r i a t i o n a l Energy
IFE (1) = 1/Omega z0 ∗e p s i l o nz 0 ˆ2/2 . . .1165
68
+ 1/Omega z1 ∗e p s i l o nz 1 ˆ2/2 . . .
+1/Omega w0∗e p s i l o nw 0 ˆ2/2 . . .
+1/Omega w1∗e p s i l o nw 1 ˆ2/2 . . .
+1/2 ∗log (Omega w0∗Omega w1∗Omega z0 ∗Omega z1 ) ;
1170
%Gradient descent l e a r n i n g params
k =.1; %f o r i n f e r e n c e
ka =.01; %f o r l e a r n i n g
1175
f o r i =2:N
%The g e n e r a t i v e p r o c e s s
x dot ( i ) = a ( i ´1);% a c t i o n
x ( i ) = x ( i ´1)+dt ∗( x dot ( i ) ) ;1180
T( i ) = T0/( x ( i )ˆ2+1);
Tx( i )= ´2∗T0∗x ( i )∗( x ( i )ˆ2+1)ˆ ´2;
T dot ( i ) = Tx( i ) ∗( x dot ( i ) ) ;
rho 0 ( i ) = T( i ) + zgp 0 ( i ) ; %c a l c l a u t e sensory input1185
rho 1 ( i ) = T dot ( i ) + zgp 1 ( i ) ;
%The g e n e r a t i v e model
e p s i l o nz 0 = ( rho 0 ( i´1)´mu 0 ( i´1));% e r r o r terms
e p s i l o nz 1 = ( rho 1 ( i´1)´mu 1 ( i´1));1190
e p s i l o nw 0 = ( mu 1 ( i´1)+mu 0 ( i´1)´Td ) ;
e p s i l o nw 1 = ( mu 2 ( i´1)+mu 1 ( i´1));
IFE ( i ) = 1/Omega z0 ∗e p s i l o nz 0 ˆ2/2 . . .1195
69
+1/Omega z1 ∗e p s i l o nz 1 ˆ2/2 . . .
+1/Omega w0∗e p s i l o nw 0 ˆ2/2 . . .
+1/Omega w1∗e p s i l o nw 1 ˆ2/2 . . .
+1/2 ∗log (Omega w0∗Omega w1∗Omega z0 ∗Omega z1 ) ;
1200
mu 0 ( i ) = mu 0 ( i´1) . . .
+dt ∗( mu 1 ( i´1)´k∗(´e p s i l o nz 0 /Omega z0 . . .
+e p s i l o nw 0 /Omega w0 ) ) ;
mu 1 ( i ) = mu 1 ( i´1) +dt ∗( mu 2 ( i´1)´ k∗(´e p s i l o nz 1 /Omega z1 . . .1205
+e p s i l o nw 0 /Omega w0+e p s i l o nw 1 /Omega w1 ) ) ;
mu 2 ( i ) = mu 2 ( i ´1 ) . . .
+dt ∗´k ∗( e p s i l o nw 1 /Omega w1 ) ;
1210
i f ( time ( i ) >25)
a ( i ) = a ( i ´1) +dt ∗´ka ∗Tx( i ) ∗e p s i l o nz 1 /Omega z1 ; %a c t i v e i n f e r e n c e
e l s e
a ( i ) = 0 ;1215
end
end
f i g u r e ( 1 ) ; c l f ;
1220
f i g u r e ( 1 ) ;
subplot ( 5 , 1 , 1 )
p l o t ( time ,T) ; hold on ;1225
70
p l o t ( time , x ) ; hold on ;
legend ( ’T’ , ’ x ’ )
subplot ( 5 , 1 , 2 )
p l o t ( time , mu0 , ’ k ’ ) ; hold on ;1230
p l o t ( time , mu1 , ’m’ ) ; hold on ;
p l o t ( time , mu2 , ’ b ’ ) ; hold on ;
legend ( ’ \mu’ , ’ \mu’ , ’ \mu’ ) ;
1235
subplot ( 5 , 1 , 3 )
p l o t ( time , rho 0 , ’ k ’ ) ; hold on ;
p l o t ( time , rho 1 , ’m’ ) ; hold on ;
legend ( ’ \rho ’ , ’ \rho ’ ) ;1240
subplot ( 5 , 1 , 4 )
p l o t ( time , a , ’ k ’ ) ;
y l a b e l ( ’ a ’ )
1245
subplot ( 5 , 1 , 5 )
p l o t ( time , IFE , ’ k ’ ) ; x l a b e l ( ’ time ’ ) ; hold on ;
y l a b e l ( ’ IFE ’ )
71
Table 1. Mathematical Objects in the IFE
Symbol Name Description
ϑ Environmental states These refer to all states outside of the brain and in-
clude both environmental and bodily variables.
ϕ Sensory data Signals caused by the environment.
qpϑq R-density Organism’s (implicit) probabilistic representation of
environmental states which cause sensory data.
ppϕ,ϑq G-density Joint probability density, encoded in the brain relat-
ing sensory data to environmental states. Assumed
to be encoded in a form which makes ppϕ|ϑqand ppϑq
accessible, but not ppϑ|ϕqor ppϕq.
ppϑq Prior density Organism’s prior beliefs, encoded in the brain’s state,
about environmental states.
ppϕ|ϑq Likelihood density Organism’s implicit beliefs about how environmental
states map to sensory data.
ppϑ|ϕq Posterior density The inference that a perfectly rational agent (with in-
complete knowledge) would make about the environ-
ment’s state upon observing new sensory information,
given the organism’s prior assumptions.
ppϕq Sensory density Probability density of the sensory input, encoded in
the brain’s state, which cannot be directly quantiﬁed
given sensory data alone.
´ln ppϕq Surprisal Surprise or self-information in information-theory
terminology, which is equal to the negative of log
model evidence in Bayesian statistics.
F Information-theoretic free
energy (IFE)
The quantity minimised under the FEP which forms
an upper bound on surprisal allows the approximation
of the posterior density.
72
Table 2. Mathematical objects in the Laplace encoding
Symbol Name Description
Frqpϑq; ϕs Variational IFE A functional (higher-order function) of the R-
density qpϑq and a function of the sensory data
ϕ.
Npϑ; µ,ζq (Gaussian) ﬁxed-form R-
density
An ‘ansatz’ for unknownqpϑq(the Laplace approx-
imation)
µ, ζ Parameters for the R-
density
Suﬃcient statistics (expectation and variance) of
the ﬁxed-form R-density, encoded in the brain’s
state.
ζ˚ Optimal variance Analytically derivable optimal ζ, removing an ex-
plicit dependence of F on ζ.
ppϕ,µq Laplace-encoded G-
density
The G-density in which dependence on ϑhas been
replaced with a dependence on µ.
Epµ,ϕq Laplace-encoded energy Mathematical construct deﬁned to be ´ln ppµ,ϕq.
73
Table 3. Mathematical glossary in the generative models
Symbol Name & Description
Simple model ppϕ,µq“ ppϕ|µqppµq
gpµ; θq Generative mapping between the brain states µand the observed data
ϕ, paramterised by θ
z, w Random ﬂuctuations represented by Gaussian noise
σz, σw That variance of these ﬂuctuations (the inverse of precisions)
ppϕ|µq, ppµq Likelihood, prior of µ, which together determine ppϕ,µq
Dynamical model ppϕ,µq“ ś8
n“0 ppϕrns|µrnsqppµrn`1s|µrnsq
˜µ Brain states in generalized coordinates; an inﬁnite vector whose com-
ponents are given by successive time-derivatives, ˜µ”pµ,µ1,µ2,¨¨¨q”
pµr0s,µr1s,µr2s,¨¨¨q .
˜ϕ Sensory data, similarly deﬁned as ˜ϕ“pϕ,ϕ1,ϕ2,¨¨¨q .
ϕrns “grns`zrns Generalized mapping between the observed dataϕand the brain states
µ at the dynamical order n
µrn`1s “frns`wrns Generalized equations of motion of the brain state µat the dynamical
order n
grns, frns Generative functions in the generalized coordinates
ppϕrns|µrnsq Likelihood of the generalized state µrns, given the data ϕrns
ppµrn`1s|µrnsq Gaussian prior of the generalized state µrns
74
Table 4. Mathematical constructs in the hierarchical generative model
Symbol Name & Description
Hierarchical model ppϕ,µq“ ppµpMqqśM´1
i“0 ppµpiq|µpi`1qq
µpiq Brain states at cortical layer i(i“1,2,¨¨¨ ,M); µp0q ”ϕdenotes the
sensory data which reside at the lowest cortical layer.
gpiqpµpiqq Generative map (or function) of the brain state µpiq to estimate one-
level lower stateµpi´1qin the cortical hierarchy viaµpi´1q “gpiqpµpiqq`
zpi´1q; where zpi´1q is Gaussian noise.
ppµpiq|µpi`1qq Likelihood of µpiq given a value for µpi`1q; which acts as a prior for
ppµpi´1q|µpiqqin the cortical hierarchy.
ppµpMqq Probabilistic representation of brain states at the highest layer, which
forms the highest prior.
75
Table 5. Mathematical constructs in the full generative model
Symbol Name & Description
Full construct pp˜ϕα,˜µαq“ pp˜xpMq
α ,˜vpMq
α qśM´1
i“0 pp˜xpiq
α |˜vpiq
α qpp˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q
˜µpiq
α Brain state α in cortical layer iin generalized coordinates, whose nth
component is denoted as µpiq
αrns.
˜xpiq
α , ˜vpiq
α Two distinctive neuronal representations, ˜µpiq
α “p˜xpiq
α ,˜vpiq
α q; designated
as hidden and causal states, respectively.
˜gpiq
α Generative map of the causal state ˜ vpiq
α to learn the state one layer
below, ˜vpi´1q
α “˜gpiq
α p˜xpiq
α ,˜vpiq
α q` ˜zpi´1q
α .
˜fpiq
α Generative function which induces the Langevin-type equation of mo-
tion of the hidden state ˜xpiq
α , 9˜xpiq
α “ ˜fpiq
α p˜xpiq
α ,˜vpiq
α q` ˜wpiq
α .
˜zpiq
α , ˜wpiq
α Random ﬂuctuations treated as Gaussian noise.
pp˜xpMq
α ,˜vpMq
α q Prior density of the brain state ˜µα at the highest cortical layer pMq.
pp˜xpiq
α |˜vpiq
α q Probabilistic representation of the intra-layer dynamics of hidden
states ˜xpiq
α conditioned on the causal state ˜vpiq
α via ˜fpiq
α ; dynamic tran-
sition from order nto n`1 is hypothesized as the Gaussian ﬂuctuation
of wpiq
αrns “xpiq
αrn`1s´fpiq
αrns.
pp˜vpiq
α |˜xpi`1q
α ,˜vpi`1q
α q Likelihood density of the causal state ˜vpiq which serves as a prior for
one layer lower density, representing statistically the inter-layer map
between two successive causal states, zpiq
αrns “ vpiq
αrns ´gpi`1q
αrns , by the
Gaussian ﬂuctuation.
76
Table 6. Mathematical objects for recognition dynamics
Symbol Name & Description
∇˜µEp˜µ, ˜ϕq ‘Gradient’ of the Laplace encoded-energy: Multi-dimensional deriva-
tive of the scalar function E; which vanishes at an optimum ˜µ˚.
Dynamical construct 9˜µpiq
α ´D˜µpiq
α “´κα∇˜µpiq
α
Ep˜µ, ˜ϕq, ˜ µpiq
α “p˜xpiq
α ,˜vpiq
α q
˜µpiq
α Generalized brain states: A point in the generalized state space to
represent fast ‘time-dependent’ neuronal activity µα on each cortical
layer i [see equations (70) and (71)].
9˜µpiq
α , D˜µpiq
α 9˜µpiq
α is the ‘path of the mode’; D˜µpiq
α is the ‘mode of the path’. 9˜µpiq
α
represents the rate of change of a brain state in generalized state space,
while D˜µpiq
α represents the encoded motion in the brain; when the
two become identical, i.e. 9˜µpiq
α “ D˜µpiq
α , in the course of recognition
dynamics, E reaches its minimum.
Static construct :µpiq
β “´κβˆµpiq
β ¨∇µβEp˜µ, ˜ϕ; θ,γq, µβ “θ,γ
˜Λαpiq
z , ˜Λαpiq
w Precisions: Inverse variances in the generalised coordinates [see equa-
tion (84)].
θpiq
α , γpiq
α Parameters, hyper-parameters: The slow brain states are treated
‘static’ and are associated with θpiq
α and γpiq
α , respectively, on each
cortical layer; where θpiq
α appear as parameters in the generative func-
tions gpiq
α and fpiq
α , and γpiq
α are hyper-parameters in the precisions
Λαpiq
z and Λαpiq
w .
˜ξαpiq
z , ˜ξαpiq
w Prediction errors; measuring the discrepancy between the observation
and the evaluation [e.g. equations (88), (89)]
77