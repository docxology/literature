This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Appendix B: The Equations of Active Inference
B.1 Introduction
In this appendix, we provide a mathematical summary of Active Inference.
This supplements the equations in the mainchapters with details about
where they come from and aims to fill in some of the intermediate steps
omitted t here. This builds directly on the mathematical background of
appendix A and deals with inference in partially observed Markov decision
(POMDP) proc esses and predictive coding architectures, and it touches on
questions of structure learning and model reduction alluded to in the main
text. Our aim is for this to be relatively self- contained, with part icu l ar focus
on topics that frequently cause confusion. Readers should be reassured that
it is not necessary to understand everyt hing in this appendix to be able to
usefully apply Active Inference; this is more for t hose who want greater
technical detail.
B.2 Markov Decision Proc esses
B.2.1 State Inference
When solving a POMDP probl em, our aim is to select the appropriate course
of action, or policy. U nder Active Inference, this is framed as an inference
probl em, in which we must find a posterior probability distribution over
alternative policies. To calculate a posterior probability, we need two things:
the prior probability of policies (addressed in section B.2.2) and the likeli-
hood of observations given a policy. This section focuses on the latter.
The likelihood of observations given a policy is not straightforward to
compute. This is b ecause a POMDP probl em is structured so that policies (π)
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
244 Appendix B
influence trajectories (indicated by ~) of states (s) that influence outcomes
(o) without a direct influence of policies on outcomes. The probl em then
involves a sum over trajectories of states to marginalize these out and find
a marginal likelihood of observations given policies:
P(o!|π)=∑ P(o!|s!)P(s!|π) (B.1)
s!
For any nontrivial state-s pace, this summation can be very challenging
to compute, from a computational perspective. However, as we see in chap-
ter 2, we can approximate marginal likelihoods of this sort using a free
energy functional. Chapters 2–4 describe f ree energy as a functional of two
things: approximate posterior beliefs (Q ) and a generative model (P). This
lets us express the f ree energy for a given policy as follows:
F(π)=EQ(s!|π) [lnQ(s!|π)−lnP(o!,s!|π)]≥−lnP(o!|π)
(B.2)
Q(s!|π)=argminF(π)⇒F(π)≈−lnP(o!|π)
Q
Equation B.2 tells us something s imple but import ant. To be able to infer
what to do, we need to approximate a marginal likelihood of a policy. To
find a good approximation of this marginal likelihood, we need to optimize
our beliefs about states u nder that policy. In short, perceptual inference is
mandated for planning to proceed. So how do we solve this probl em practi-
cally? The answer is to appeal to the methods outlined in section A.4.2. By
choosing explicit forms for the probability distributions in equation B.1, we
can find a s imple expression for the f ree energy:
Q(s!|π)=∏ Q(s |π): Q(s |π)=Cat(s )
τ τ τ πτ
P(o!|s!)=∏ P(o |s ): P(o |s )=Cat(A)
τ τ τ τ τ (B.3)
P(s!|π)=P(s )∏ P(s |s ,π): P(s |s ,π)=Cat(B )
1 τ τ+1 τ τ+1 τ πτ
P(s )=Cat(D)
1
Briefly, the first line of equation B.3 defines beliefs about states in terms of
a mean-f ield approximation (see equation A.41), factorized over time. Each
time-p oint is associated with a belief about what the state would be on
pursuing a policy, given by the vector s , whose elem ents are the probabili-
πτ
ties of each alternative state. The trajectory of observations in the second
line depends on a trajectory of hidden states, with the matrix (or tensor, if
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 245
the states are further factorized) A indicating the distribution over observa-
tions for each state. Similarly, the prior trajectory of states u nder a model
comprises the transition probabilities u nder that policy (B ) and the initial
πτ
state probabilities (D). Substituting t hese into the f ree energy expression
in Equation B.2, we arrive at the following expression for the f ree energy
under a policy:
F
π
=s π1i(lns
π1
−lnAio
1
−lnD)+∑s πτi(lns
πτ
−lnAio
τ
−lnB
πτ
s
πτ−1
)(B.4)
τ=2
Note that the dot product of a probability vector with another quantity
is equivalent to the expectation operation. See section A.2.1 if this is not
clear. Equation B.4 treats the outcomes as if they w ere probability vectors,
but with a one in the elem ent corresponding to the observed outcome and
zeros elsewhere (sometimes called one-h ot encoding or 1-i n-k vector). The
challenge now is to minimize the f ree energy with res pect to our beliefs
about states (s ) to ensure the f ree energy becomes a good approximation
πτ
to a marginal likelihood. We could do this as in section A.4.2 and minimize
with res pect to each f actor of our beliefs one at a time, iterating through
until they converge. However, as we are interested in more biologically
plausible schemes, we can instead construct a dynamical system that con-
verges on the same solution. This approach is known as a gradient descent,
as we follow the f ree energy gradients downward u ntil we arrive at the
minimum.
To update beliefs about states, we take the gradient of this with res pect
to current beliefs about states. We then define an auxiliary variable (v) that
plays the role of the log posterior and set this to perform a gradient descent
on the f ree energy. This log posterior is then passed through a softmax
function1 (σ ) to convert it to a normalized probability distribution. This
proc ess ensures that beliefs about the states change such that they decrease
free energy.
s =σ(v )
πτ πτ
.
v πτ=−∇
sπτ
F
π
(B.5)
∇
sπτ
F
π
=lns
πτ
−lnAio
τ
−lnB
πτ
s
πτ−1
−lnB πτ+1is
πτ+1
Equation B.5 has the same solution to the variational message passing
scheme outlined in equation A.42. It allows for efficient computation of
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
246 Appendix B
posterior beliefs using only locally derived information (in this case, from
sensory data, beliefs about the immediate past and beliefs about the imme-
diate future). However, it is worth noting that the mean- field approxi-
mation used here (factorization over time) often leads to overconfident
posteriors. In practice, this may be countered using a modified scheme called
marginal message passing (Friston, FitzGerald et al. 2017; Parr, Markovic
et al. 2019):
.
v πτ=ε
πτ
ε
πτ
=lnAio
τ
+
2
1( ln(B
πτ
s
πτ−1
)+ln(B
π
†
τ+1
s
πτ+1
) ) −lns
πτ
(B.6)
B† ∝BT
πτ πτ
This leads to more conservative inferences, with greater uncertainty ascribed
to posterior beliefs. Other alternatives have been explored, including the
Bethe approximation (Schwöbel et al. 2018). However, at the time of writ-
ing, the most widely used implementation of Active Inference employs
marginal message passing.
B.2.2 Planning as Inference
The above section deals with inference about states conditioned on some
policy to minimize a f ree energy conditioned on the policy. This f ree energy
plays the role of a negative log marginal likelihood (model evidence), wherein
each policy is treated as a model. Equipping this with prior and posterior
beliefs about the most likely policy, we can express the f ree energy as a func-
tional of beliefs about policies.
F=EQ(π) [lnQ(π)−lnP(π,o!)]
≈EQ(π) [lnQ(π)+F(π)−lnP(π)]
P(π)=Cat(π ) (B.7)
o
Q(π)=Cat(π)
π
o
=σ(lnE−G)
The approximate equality in the second line comes from equation B.2.
Here, E is a vector of fixed beliefs about policies (this may be thought of as
a bias, or habit, term), while G is the expected free energy for each policy.
As before, we can now write the f ree energy in terms of sufficient statistics:
F = π · (ln π − ln E + F + G) (B.8)
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 247
Here, F is a vector whose elem ents are F as defined in equation B.4. Taking the
π
gradients, we find the optimal update for beliefs about policies (i.e., planning):
∇ F=0⇔
π (B.9)
π=σ(lnE−F−G)
B.2.3 Learning
To enable learning, we need to incorporate prior beliefs about the par-
ameters of the probability distributions that comprise the generative
model. As t hese are expressed as categorical distributions, the appropriate
(conjugate) choice of prior is a Dirichlet distribution. Taking the prior over
initial states as an example, the terms in the f ree energy that depend on the
expected (log) prior include the following:
F=...+D
KL
[Q(D)||P(D)]−EQ(s1)Q(D) [lnP(s
1
|D)]
=...+(d−d)iEQ(D) [lnD]−s 1iEQ(D) [lnD]
EQ(D) [lnD]=ψ(d)−ψ(d 0 )
(B.10)
d =∑ d
0 i i
Q(D)!Dir(d)
P(D)!Dir(d)
Equation B.10 highlights in the third equality a useful identity. The expec-
tation of the log of a Dirichlet distributed variable is the difference between
two digamma functions (ψ )—w here the digamma function is the derivative
of a gamma function. We can use equation B.10 to find the f ree energy
minimum:
∇ F=d−d−s =0⇔d=d+s (B.11)
E[lnD] 1 1
This gives a s imple scheme that may be used to update prior Dirichlet par-
ameters to their posterior values. Very similar update rules apply for the other
probability distributions that comprise the generative model:
a=a+∑ o ⊗s
τ τ τ
b =b +∑ s ⊗s
πτ πτ τ πτ πτ−1
(B.12)
c=c+∑ o
τ τ
d=d+s
1
e=e+π
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
248 Appendix B
These simply say that when the t hing predicted by the relevant term in the
probability distribution comes to pass (which may be a combination of two
things for conditional probabilities), we simply augment that elem ent of
the probability array to signal that it is more likely to happen again in the
future.
B.2.4 Precision
In some settings, it may be conv en ient to par ame t erize the generative model
in a slightly diff ere nt way. One option h ere is to use a Gibbs meas ure, where
probability distributions are equipped with an inverse temperature par am-
et er that plays the role of a precision. Most commonly, this is done for the
precision (γ ) over policies:
P(π|γ)=Cat(π )
0 (B.13)
π
0
=σ(−γG)
For simplicity, we omit the E vector for this section. In what follows, we
will also consider a precision for the likelihood (ζ ) and for transitions (ω ).
The prior distribution over precision par ameters is assumed to be a gamma
distribution:
P(ζ)∝β exp( −βζ )
ζ ζ
P(ω)∝β exp(−βω) (B.14)
ω ω
P(γ)∝β exp( −βγ )
γ γ
The approximate posterior distributions have the same (gamma distri-
bution) form, and we will use a bold beta hyper-p arameter to distinguish
between the sufficient statistics of the posterior and prior above. A useful
property of the gamma distribution, when par ame t erized in this way, is the
following:
ζ=EQ(ζ) [ζ]=β
ζ
−1
ω=EQ(ω) [ω]=β
ω
−1 (B.15)
γ =EQ(γ) [γ]=β
γ
−1
Having defined t hese distributions, we can write the variational f ree energy:
F=EQ [F(π,ζ,ω)+D
KL
[Q(π)||P(π|γ)]]
(B.16)
+D
KL
[Q(γ)||P(γ)]+D
KL
[Q(ω)||P(ω)]+D
KL
[Q(ζ)||P(ζ)]
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 249
This can be expressed in terms of its sufficient statistics (omitting constants):
F=πi(F+lnπ+γ iG+lnZ(γ))+lnβ
γ
+lnβ
ω
+lnβ
ζ
−lnβ γ −lnβ ω −lnβ ζ +γβ γ +ωβ ω +ζβ ζ (B.17)
F π ≈−∑ τ s πτi ( ζlnAio τ +ωlnB πτ s πτ−1 −lnZ(ζ)io τ −lnZ(ω)s πτ−1 )
In equation B.17, Z represent partition functions (i.e., normalizing con-
stants) given by the following:
Z(ζ) =∑ (A )ζ
j i ij
Z(ω) =∑ (B )ω
j i πτij
Z(γ)=∑
π
exp(−γ iG
π
)
⇒
∂
ζ
lnZ(ζ)s
τ
=o
τ
ζ ilnA (B.18)
∂
ω
lnZ(ω)s
πτ−1
=s
π
ω τilnB
π
∂
γ
lnZ(γ)=−π
0
iG
oζ !σ(ζlnA)s
τ τ
sω !σ(ωlnB )s
πτ πτ πτ−1
π !σ(−γG)
0
Taking the partial derivative2 with re spect to the expected precisions gives
this:
⎧∂ ζ F ⎫ ⎧β ζ ⎫
⎧
⎪
∑
τ
(o
τ
ζ −o
τ
)ilnA+β
ζ
⎫
⎪
⎪ ⎪ ⎪ ⎪ ⎪ ⎪
⎨∂ ω F⎬=0⇔⎨β ω⎬=⎨ ∑ τ πi(s π ω τ −s πτ )ilnB π s πτ−1 +β ω ⎬ (B.19)
⎪ ⎪ ⎪ ⎪ ⎪ ⎪
⎩ ∂ γ F ⎭ ⎩β γ ⎭ ⎩ ⎪(π−π 0 )iG+β γ ⎭ ⎪
Expressing t hese updates as biologically plausible gradient descents gives
the resulting equations:
⎧ ⎪ β . ζ ⎫ ⎪ ⎧ ⎪ ∑ τ (o τ ζ −o τ )ilnA+β ζ −β ζ ⎫ ⎪
⎪ . ⎪ ⎪⎪ ⎪⎪
⎨β ω ⎬=⎨ ∑ τ πi(s π ω τ −s πτ )ilnB π s πτ−1 +β ω −β ω⎬ (B.20)
⎪ . ⎪ ⎪ ⎪
⎪β ⎪ ⎪(π−π 0 )iG+β γ −β γ ⎪
⎩ γ ⎭ ⎩⎪ ⎭⎪
Note that the dimensionality implies a (row) vector of precisions for A, where
each state (column of A) is associated with its own precision par ame t er.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
250 Appendix B
B.2.5 Expected F ree Energy
Expected f ree energy is discussed extensively in the main text of the book.
In this section, we supplement this discussion with two things. First, we
offer a brief outline of current thinking as to why this is the appropriate
quantity to define prior beliefs about policies. Second, we touch on the
implementational details for computing this quantity.
While numerical simulations (of the sort illustrated in chapter 7) have
established that expected f ree energy is useful, the question of why it is
useful is still an active research area. Anticipating that this discussion w ill
continue to evolve, h ere we set down a brief summary of the most parsi-
monious explanation at the time of writing (Da Costa et al. 2020; Friston,
Da Costa et al. 2020). The starting point is to stipulate that a system attains
some steady state (see section A.5.2) or, equivalently, fulfills its preferences
(defined h ere in relation to latent states) at some f uture time (τ ):
Q(s
τ
)=EQ(π) [Q(s
τ
|π)]=P(s
τ
|C) (B.21)
Our challenge is to find the Q(π) that satisfies equation B.21. To do so,
we note that equation B.21 implies the following:
D [Q(π|s )Q(s )||Q(π|s )P(s |C)]=0
KL τ τ τ τ
⇒ (B.22)
EQ(π,sτ) [lnQ(π,s τ )]=EQ(π,sτ) [lnQ(π|s τ )+lnP(s τ |C)]
We next factorize the left-h and side so as to isolate the Q(π ) term we are
interested in:
EQ(π) [lnQ(π)]=EQ(π,sτ) [lnQ(π|s τ )+lnP(s τ |C)−lnQ(s τ |π)] (B.23)
We define a variable α that represents the ratio of two entropies:
α= EQ(sτ) ⎡ ⎣ H[Q(π|s τ )]⎤ ⎦ (B.24)
EQ(sτ,π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦
Heuristically, α expresses the relative range of behavioral outputs (i.e., poli-
cies) that are plausible in a given state, compared to the range of outcomes
expected in that same state. If very large, this might describe a creature
whose beh avi or bears l ittle relationship to the state of their world, despite
highly precise sensory observations being generated by that world. When
very small, this might describe a creature who always behaves the same way
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 251
when it knows the state of the world but is rarely offered precise data about
that world. H ere, we w ill stipulate that we are interested in systems whose
α = 1—i mplying a relatively balanced and symmetrical exchange with their
world. This renders the two entropies in equation B.24 equal. Returning to
equation B.23:
EQ(π) [lnQ(π)]=−EQ(sτ) ⎡ ⎣ H[Q(π|s τ )]⎤ ⎦
+EQ(π,sτ) [lnP(s
τ
|C)−lnQ(s
τ
|π)]
( B.25 )
=−EQ(sτ,π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦
−EQ(π) ⎡ ⎣ D KL [Q(s τ |π)||P(s τ |C)]⎤ ⎦
The second line follows from the first and from equation B.24 with α = 1.
We now see that equation B.25, and therefore equation B.21, is satisfied by
choosing the following:
lnQ(π)=−E !Q#(sτ#|π)# ⎡ ⎣ # H " [P # (o #τ | # s τ# ) $ ]⎤ ⎦− ! D K#L [ # Q # (s τ# |π " ) # || # P( # s τ | # C $ )] (B.26)
Expectedambiguity Risk
Our final step is to note the relationship between the quantity on the
right-h and side and the expected f ree energy—w ith preferences defined in
terms of observations in place of states:
EQ(sτ|π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦+D KL [Q(s τ |π)||P(s τ |C)]
=EQ(sτ|π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦+D KL [Q(s τ |π)||P(s τ |C)]
+E !Q#(sτ#|π)#P(o#τ|sτ#) [ # ln # P( # o τ# |s τ## )] " − # EQ#(sτ#|π)#P(o#τ|sτ#) [ # ln # P( # o τ# |s #τ$ )]
=0
=EQ(sτ|π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦+D KL [Q(o τ ,s τ |π)||P(o τ ,s τ |C)] (B.27)
=EQ(sτ|π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦+D KL [Q(o τ |π)||P(o τ |C)]
+EQ(oτ|π) ⎡ ⎣ D KL [Q(s τ |o τ ,π)||P(s τ |o τ ,C)]⎤ ⎦
≥EQ(sτ|π) ⎡ ⎣ H[P(o τ |s τ )]⎤ ⎦+D KL [Q(o τ |π)||P(o τ |C)]=G(π)
The steps in equation B.27 have (perhaps a l ittle tediously) been included in
some detail as our experience is that p eople often strugg le with this result.
The ine quality in the final line arises from the omission of the (nonnega-
tive) KL-D ivergence in the previous line. The key result here is that the
ambiguity and risk minimized for the most plausible policies in equation
B.26 acts as an upper bound on the expected f ree energy used throughout
this book.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
252 Appendix B
We now turn to the question of computational implementation. H ere we
can simply appeal to the linear algebraic identities we saw in section A.2. It
is straightforward, if we express preferences as a vector of prior probabilities
(C), to express the pragmatic term of the expected f ree energy as follows:
EQ(oτ|π) [lnP(o τ |C)]=o πτilnC τ
Q(o |π)=Cat(o ) (B.28)
τ πτ
o
πτ
=As
πτ
The expected information gain associated with hidden states (i.e., salience,
epistemic value, or Bayesian surprise) is expressed in terms of the difference
between two entropies:
H[Q(o
τ
|π)]−EQ(sτ|π) [H[P(o
τ
|s
τ
)]]
=−o πilno
π
−His
πτ
(B.29)
H! −diag(AilnA)
See Section A.2.3 for an explanation of the last line. Putting equations B.28
and B.29 together, the expected f ree energy is this:
G =∑ G
π τ πτ (B.30)
G πτ =His πτ +o πτi (lno πτ −lnC τ )
When we need to account for active learning, we supplement this
with par ame t er information gain. The information gain associated with
para meters of the generative model (i.e., novelty) may be derived as fol-
lows. Using the KL-D ivergence between two Dirichlet distributions, we
can express the information gain that would occur following a given state-
outcome combination:
W !D [P(A |o=i,s=j)||P(A )]
ij KL ij ij
= (lnΓ(a )−lnΓ(a +1)) + (lnΓ(a +1)−lnΓ(a ))
!###ij #"###ij #$ !###0j#"####0$j
−lnaij +lna0j (B.31)
+ψ(a +1)−ψ(a +1)
ij 0j
a !∑ a
0j i ij
Here we have used the fact that if we knew a given state-o utcome combina-
tion had occurred, we would add 1 to the associated Dirichlet par ame t er.
This lets us use a standard identity of a log gamma function (as indicated
by the underbraces) to simplify the expression:
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 253
a ∂ Γ(a +1) ∂ Γ(a +1)
W =ln 0j + aij ij − a0j 0j
ij a a Γ(a ) a Γ(a )
ij ij ij 0j 0j
(B.32)
=ln a 0j + 1 + ∂ aij Γ(a ij ) − 1 − ∂ a0j Γ(a 0j )
a a Γ(a ) a Γ(a )
ij ij ij 0j 0j
Here we have used the identity xΓ(x)=Γ(x+1) and an application of the
product rule. We next use the identity ψ(x)Γ(x)=∂ Γ(x) and the approxi-
x
mation ψ(x)≈lnx−(2x)−1 to simplify this:
1 1 a
= − +ln 0j +ψ(a )−ψ(a )
a a a ij 0j
ij 0j ij (B.33)
1 1
≈ −
2a
ij
2a
0j
The expected information gain is then as follows:
EQ(oτ,sτ|π) [D KL [P(A|o τ ,s τ )||P(A)]]≈o πτiWs πτ (B.34)
This simple expression then augments the expected free energy to ensure
novelty-s eeking beh avi or in addition to pragmatic and salient choices.
B.2.6 Bayesian Model Reduction
In chapter 7, we briefly touch on the idea of structure learning and model
reduction. We take the opportunity to unpack the princip les in a little
more depth here. Bayesian model reduction is a technique used to com-
pare alternative models that differ only in their priors. Through Bayes’
theorem (see chapter 2), we can express the ratio of the joint probability of
data ( y) and some par ameters (θ ) between two alternative models in two
diff ere nt ways:
P(y,θ) P(y|θ)P(θ) P(θ|y)P(y)
= = (B.35)
P!(y,θ) P!(y|θ)P!(θ) P!(θ|y)P!(y)
If the only difference between the two models is the prior (i.e., P(y|θ)=P!(y|θ)
P(y|θ)=P!(y|θ)), then we can cancel the likelihood terms. On rearranging, this gives
an expression for the posterior probability u nder alternative (reduced) pri-
ors in terms of the posterior probability u nder the original (full) priors:
P(θ|y)P(y)P!(θ)
P!(θ|y)= (B.36)
P!(y)P(θ)
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
254 Appendix B
Integrating both sides with res pect to the para meters gives this:
P(y) ⎡P!(θ)⎤
1= P!(y) EP(θ|y) ⎣ ⎢ P(θ)⎦ ⎥⇒
(B.37)
⎡P!(θ)⎤
lnP!(y)=lnP(y)+lnEP(θ|y)
⎣
⎢
P(θ)⎦
⎥
Substituting back into equation B.36 gives this:
⎡P!(θ)⎤
lnP!(θ|y)=lnP(θ|y)+lnP!(θ)−lnP(θ)−lnEP(θ|y)
⎣
⎢
P(θ)⎦
⎥ (B.38)
Together, equations B.37 and B.38 mean that we can find the model evi-
dence and posterior we would have got, had we used a given reduced prior,
using the results from inverting a full model. We can reexpress these equa-
tions in terms of the variational quantities introduced in chapter 4:
⎡P!(θ)⎤
F[P(θ)]−F[P!(θ)]=lnEQ(θ)
⎣
⎢
P(θ)⎦
⎥
(B.39)
⎡P!(θ)⎤
lnQ!(θ)=lnQ(θ)+lnP!(θ)−lnP(θ)−lnEQ(θ)
⎣
⎢
P(θ)⎦
⎥
For reference, we offer the form of equation B.39 u nder two diff ere nt kinds
of prior. The first is a normal distribution:3
P(θ)=N(η,∑)
P!(θ)=N(η!,∑ ∼ )
Q(θ)=N(µ,C)
Q!(θ)=N(µ!,C!)
(B.40)
C!−1=P! =P+Π! −Π
µ! =C!(Pµ+Π!η!−Πη)
ΔF=−
2
1ln|Π!PC!Σ|+
2
1(µiPµ+η!iΠ!η!−ηiΠη−µ! iP!µ!)
Practically, this is used in the setting of mixed models, with a continuous
and a categorical component. If each categorical outcome of the latter is
associated with a continuous prior, we can efficiently evaluate the evidence
for each of t hese priors (and therefore categorical outcomes) without hav-
ing to invert each model in turn. See chapter 8 for an example.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 255
More often, in a purely POMDP setting, we may be interested in com-
paring hypotheses about Dirichlet prior distributions. This has been used
to simulate pruning of elem ents in a probability matrix as a metap hor for
synaptic pruning during sleep (Friston, Lin et al. 2017). The form of Bayes-
ian model reduction for Dirichlet distributions is as follows:
P(θ)=Dir(a)
P!(θ)=Dir(a!)
Q(θ)=Dir(a)
Q!(θ)=Dir(a!) (B.41)
a! =a+a!−a
ΔF=lnΒ(a)−lnΒ(a!)+lnΒ(a!)−lnΒ(a)
In this expression, B denotes a beta function. Similar results can be derived
for a range of distributions (see Friston, Parr, and Zeidman 2018), but nor-
mal and Dirichlet priors are the most commonly encountered in Active
Inference.
B.3 (Active) Generalized Filtering
We now move from the categorical inferences u nder a POMDP model to the
continuous domain. This is where some of the preliminaries from appendix
A r eally start to pay off. We will exploit the Laplace approximation and
generalized coordinates of motion, both presented in section A.3. In addi-
tion, we w ill need to construct precision matrices including diff ere nt o rders
of generalized motion, as we saw in section A.5.2. From equations A.33
and A.34 we can write the free energy under the Laplace approximation
as follows:
1
F[q,y!]≈− ln(2π)k ∑! −lnp(y!,µ!)
2
q(x!)=N(µ!,∑!−1) (B.42)
∑!−1=−∇
x!
(∇
x!
lnp(y!,x!))T
x!=µ!
Here we have expressed the f ree energy, using the Laplace assumption, for
a model defined in generalized coordinates. U nder the Laplace assumption,
the only term in the first line that varies with μ is the last one. This is the
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
256 Appendix B
term expressing the generative model. Our next step is to specify the form
of the generative model:
p(y!,x!,v!)= p(y!|x!,v!)p(x!|v!)p(v!)
p(y!|x!,v!)=N(g!(x!,v!),∏! )
y
p(x!|v!)=N(Dif!(x!,v!),∏!
x
)
(B.43)
p(v!)=N(η!,∏! )
x
Dx!=f!(x!,v!)+ω!
x
y!= g!(x!,v!)+ω!
y
Note we now have two hidden variables, x and v. The difference is that the
former depends on an equation of motion ( f ), while the latter depends on
a static prior. The D operator in the penultimate line is a matrix with ones
above the leading diagonal. In generalized coordinates, this is equivalent
to taking a temporal derivative, as each elem ent in the vector of temporal
derivatives is shifted up by one. The generalized precisions are constructed
as in section A.5.3. Substituting the quantities of equation B.43 into B.42,
we have the following:
F[q,y!]=
!2
1ε!
#y"
i∏!
#y$
ε!
y
+
!2
1ε!
#x"
i∏!
#x
ε
$
!
x
+
!2
1ε!
#v"
i∏!
#v$
ε!
v
−lnp(y!|µ!x,µ!v) −lnp(µ!x|µ!v) −lnp(µ!v)
ε% y !y"− g!(µ! x ,µ! v ) (B.44)
ε! !Dµ" −f!(µ! ,µ! )
x x x v
ε! !µ" −η!
v v
We have omitted all constants with res pect to μ. From equation B.44, we
can find the gradients of the free energy (using identities introduced in sec-
tion A.2.2):
∇
µ!x
F[q,y!]=−∇
µ!x
g!i∏!
y
ε!
y
+Di∏!
x
ε!
x
−∇
µ!x
f! i∏!
x
ε!
x
(B.45)
∇
µ!v
F[q,y!]=−∇
µ!v
g!i∏!
y
ε!
y
−∇
µ!v
f! i∏!
x
ε!
x
+∏!
v
ε!
v
We could now specify a gradient descent to find the values of μ that min-
imize f ree energy. However, this would imply that when the f ree energy is
minimized, μ becomes static. Clearly this is suboptimal if we believe higher
orders of motion to be nonzero. To account for this, we can express a gradi-
ent descent in a moving frame of reference, such that when the f ree energy
is minimized, μ continues to move with velocity μ′:
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The Equations of Active Inference 257
.
µ∼
.
x
−Dµ!
x
=∇
µ!x
g!i∏!
y
ε!
y
−Di∏!
x
ε!
x
+∇
µ!x
f! i∏!
x
ε!
x
(B.46)
µ∼
v
−Dµ!
v
=∇
µ!v
g!i∏!
y
ε!
y
+∇
µ!v
f! i∏!
x
ε!
x
−∏!
v
ε!
v
Equation B.46 specifies a predictive coding scheme, in which prediction
errors drive updates in expectations, resolving t hose errors. T hese schemes
may be extended to include multiple hierarchical levels by duplicating the
equations of B.46 for an additional level but replacing y with v from the
lower level:
.
µ∼
.
(
x
i)−Dµ!
x
(i)=∇
µ!x (i)
g!i∏!(
v
i−1)ε!
v
(i−1)−Di∏!(
x
i)ε!
x
(i)+∇
µ!x (i)
f!(i) i∏!(
x
i)ε!
x
(i)
µ∼(
v
i)−Dµ!
v
(i)=∇
µ!v (i)
g!(i) i∏!(
v
i−1)ε!
v
(i−1)+∇
µ!v (i)
f!(i) i∏!(
x
i)ε!
x
(i)+∏!(
v
i)ε!
v
(i)
(B.47)
ε!(i)!µ"(i)− g!(i+1)(µ!(i+1),µ!(i+1))
v v x v
ε!(i)!Dµ!(i)−f!(i)(µ!(i),µ!(i))
x x x v
Under Active Inference, the f ree energy is minimized by perception but also
by action. As the only t hing action changes is the sensory input ( y), most
of the terms in equation B.44 are irrelevant for action. Minimizing the f ree
energy with res pect to action gives this:
.
u=−∇
u
y!(u)i∏!
y
ε!
y
(B.48)
Together, equations B.47 and B.48 provide a very general description of
Active Inference for continuous state-s pace models. We w ill not discuss the
issue of learning or mixed models in this section, as t hese are summarized
in boxes 8.2 and 8.3, respectively.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:ThomasParr,GiovanniPezzulo,KarlJ.Friston
DOI:10.7551/mitpress/12441.001.0001
ISBN(electronic):9780262369978
Publisher:TheMITPress
Published:2022
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025
MIT Press Direct
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided
comments on drafts of this book. The generous work of academic experts is essential
for establishing the authority and quality of our publications. We acknowledge with
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services.
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J.
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior /
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. |
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246591/c010400_9780262369978.pdf by guest on 12 December 2025