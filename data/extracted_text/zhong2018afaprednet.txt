8102
rpA
11
]OR.sc[
1v62830.4081:viXra
AFA-PredNet: The action modulation within predictive coding
Junpei Zhong
1,2∗
and Angelo Cangelosi
2
and Xinzheng Zhang
3
and Tetsuya Ogata
1,4
Abstract—The predictive processing (PP) hypothesizes that lower levels to predict the upcomingneural activities on the
the predictive inferenceof our sensorimotor system is encoded lowerlevel.Inturn,thiskindofpredictingneuralpopulations
implicitly in the regularities between perception and action.
can be suppressedor inhibitedby the predictionerrorwhich
Wepropose aneural architecturein whichsuch regularitiesof
is transmitted in a bottom-up way. In this way, the internal
active inferenceareencoded hierarchically.Wefurthersuggest
that this encoding emerges during the embodied learning world model in the brain has to be shaped by the statistical
process when the appropriate action is selected to minimize structure of the world which is perceived by the bottom-up
the prediction error in perception. Therefore, this predictive flow. The world model infers the posterior of the next state
stream in the sensorimotor loop is generated in a top-down
or event following another based on the current or previous
manner. Specifically, it is constantly modulated by the motor
states. This hypothesis was firstly proposed by Helmholtz
actions and is updated by the bottom-up prediction error
signals. In this way, the top-down prediction originally comes ([9]),whoclaimedthattheperceptioniscastas a processof
from the prior experience from both perception and action unconscious inference, wherein perception is determined by
representing the higher levels of this hierarchical cognition. bothsensoryinputsandourpriorexperiencewiththeworld.
In our proposed embodied model, we extend the PredNet
Based on the PP framework,the PredNet model [10] was
Network, a hierarchical predictive coding network, with the
considered to be the first practical learning model that can
motor action units implemented by a multi-layer perceptron
network (MLP) to modulate the network top-down prediction. be utilized into a realapplication,in whichthe video stream
Two experiments, a minimalistic world experiment, and a duringdrivingcanbepredictedbythemodel.However,only
mobilerobotexperimentareconductedtoevaluatetheproposed the perception (video stream in this case) was considered
model inaqualitativeway. Intheneuralrepresentation,it can
in this PP framework. On the other hand, the execution
beobservedthatthecausalinferenceofpredictiveperceptfrom
of voluntary movements is also another factor while our
motoractionscanbealsoobservedwhiletheagentisinteracting
with the environment. mind is doing prediction.Within the synergistic relationship
of perception and action, what we perceive (or think we
I. INTRODUCTION
perceive) is heavily determined by what we know and what
Predictive process (PP) ([1, 2, 3, 4]) asserts that our we expect and execute, and what we know (or think we
sensorimotorloopworksasapredictivemachine.Itprovides expect) is continuously modulated by our proprioception as
constantly an active inference based on both active action well. Therefore, in a real-world PP model, the world model
and predictive perception from the agent to minimize the should also emerge from the active execution of certain
prediction error. Specifically, such error which the machine sensorimotor skills, rather than an internal representation
attempts to minimize is the difference between the posterior merely from sensory signals. This should be also beneficial
estimation and the truth, by changing its internal learning from different application areas, e.g. autonomous driving.
model (“perceptual inference” (see also [5] and [6]) or by
the action execution (“active inference”, see also [7] and
II. RELATEDWORKS
[8]). As such, perceiving the world (perceptual inference) Giventhe multi-modalaspectofthesensorimotormodels,
andactingonit(activeinference)aretwoaspectsthataimat the construction of embodied predictive models usually em-
thesametarget:tominimizethepredictionerrorbyadjusting phasizes the embodiedand the situated nature of the agents,
the internal models or the external world in the hierarchical to learn from interacting with the world [11]. The predic-
prediction. tive function of the internal model can range from short-
This adjustment is an integrative process follows a bi- and mid-term time-scale prediction/delay compensation to
directional learning mechanism on each level of our hierar- relatively long-term planning behaviors which emerge from
chical brain. After learning, the neuronal representations on theshort-termsimulations.Theshort-termpredictivemodels
the higher level may generate the predictions based on the are mostly related to sensorimotor control, especially the
understandingoftheupcomingworldmodel,andthesubsets consistencyofvisuomotorcoordination(e.g.[12,13])orfast
of such prediction representations will be transmitted to the reaction (e.g. [14, 15]).
Some longer-term behaviors can emerge from such kind
*Corresponding author:zhong@junpei.eu
ofshort-termneuralpredictionaswell.[16]and[17]studied
1Artificial Intelligence Research Center, National Institute ofAdvanced
how to apply an internal model to control the actual motor
Industrial Science andTechnology, Tokyo,Japan
2Centre for Robotics and Neural Systems, Plymouth University, Ply- actions. [18] also extended these models to learn imitation
mouth,PL48AA,UnitedKingdom behaviors All of the three models built a forward predictive
3SchoolofElectrical Engineering, JinanUniversity, Zhuhai,ChinaP.R.
4Lab for Intelligent Dynamics and Representation, Waseda University, model to control the robot and acquire certain behaviors
Tokyo,Japan Similarly, a long-term planning behavior can also emerge
from internal simulation when the prediction is executed generatethelocalpredictionintheimageregion.Weemploy
constantly (e.g. [19, 20]). [21] reported experiments with a number of independent recurrent units on one layer of
a mobile robot implementing a two-level recurrent archi- theGU unit. During training with various perception-action
tecture to accomplish the linguistic and sensorimotor task. pairing occasions, each of these units implicitly memorizes
An extension model has also been examined in a symbolic different possibilities of the prediction (e.g. the moving
understanding tasks [22]. direction)withrespecttothemotoractioninaself-organized
Ifweregardtheunificationofdifferenttime-scalesofpre- way.
diction, the Multiple Timescale Neural Network (MTRNN) The DU network discriminates the errors by calculating
[23] offers a compressive model of such phenomena. The the difference between the convolutional output of the pre-
model is able to represent different temporal scales of dictedsignalfromGUaswellasthe bottom-upsignalasan
sensorimotor information into the hierarchical structure of errorrepresentation,EL,whichissplitintoseparaterectified
the sensorimotor sequences, such as the language learning positiveandnegativeerrorpopulations.Theerror,EL,isthen
[24,25]andobjectfeatures/movements[26].Asanextension passed forward through a convolutionallayer to become the
oftheMTRNNmodelwithmultiplemodalities,themultiple input to the next layer.
spatio-temporal scales RNN (MSTRNN) [27] integrates the
A. Neural Dynamics
MTRNN and convolutional neural networks [28, 29]. It
In the following section, we denote the indices of these
includes two modalities: both the temporal properties as
well as the spatial receptive field sizes in different levels.
perception input image as it, and the target of the network
prediction at the lowest level is set to the actual percept at
The PredNet [10] also holds a similar concept of using the
convolutional network to capture the local features of the
the next time-step it+1 . We directly put the image as the
input of the lowest layer, layer 0, so the input of the layer
visual streams, but the temporal constraints are implicitly
hidden. Moreover, both models use only the information
0, X
0
, equals to the actual image data X
0
t =it.
The targets for higher layers at time-step t is denoted as
from the visual stream for recognition/predictionbut do not
incorporate any action-guided predictions. This is the main
Xl(t). Exceptlayer0, Xl(t)t is obtainedby the higherlevel
representationofthedeepconvolutionallayer,whichfollows
motivation we are proposing for a new action modulated
a usual calculation process of the convolutional network as
predictive model.
shown in Eq. 1: the convolution kernel, the rectified linear
III. MODEL unit(ReLU)calculationandthemax-poolingaresequentially
Compared with the PredNet [10], the AFA-PredNet (Ac- used.Thisbottom-upprocessusingconvolutionalnetworkto
tion FormulAted Predictive-coding Network) architecture extract the local features of the error.
(Fig. 1) further integrates the motor action as an additional At the GU unit, the generative process is determined by
signal which modulatesthe top-downgenerativeprocess via the representation from the recurrent connection (i.e. from
an attention mechanism. This modulation role is similar the previous time-step) X, the bottom-up error El(t−1 as
to the integration process, with perception prediction while well as the top-down prediction Rl+1(t). Such a prediction
having the active motor action as a consideration. in a convolutionalLSTM is calculated as Eq. 4: a deconvo-
Similartothehierarchicalarchitectureinthesensorimotor lution is used to reconstruct a larger size of the (predicted)
integration and the deep learning architecture, the AFA- representation Aˆ after a Rectifield unit calculation (ReLU)
PredNet network consists of a series of repeated stacked (Eq. 2).
modules in a hierarchical way, which attempt to make local To avoid the drawback of the ReLU which only capture
predictionsofthevisualinputs.Ingeneral,theAFA-PredNet only the positive and negative error, the error representation
isfunctionallyorganizedasanintegrationwithtwonetworks: El(t)iscalculatedfromthepositiveandnegativeerrors(Eq.
the left part is equivalent to a generative recurrent network, 3), as the original PredNet does. The modulation of motor
while the right part is a standard convolutional network. actionsarerepresentedasamultiplelayerperceptron(MLP)
Each layer of the network consists of four basic parts: a here, whose output explicitly represents as the movement
generative unit (GU, green) containing the recurrent convo- factors of multiple recurrent units (GU) of the higher level
lutionalnetworkswiththemotormodulatedunit(MM,gray), (Eq. 5), which are further multiplied by all the possible re-
a discriminative unit (DU, blue) containing convolutional currentGUunits. In the futurework,such motormodulated
networks(CNN)andtheerrorrepresentationlayer(ER,red). prediction may be further replaced by the context of the
The generative unit,GU, is usually a recurrentnetwork that perception,suchthatthepredictedperceptioncanbeaddedin
generatesa predictionof the nexttime-step fromthe current order to build a closed loop in the sensorimotor integration.
input.Here,theconvolutionalLSTM[30,31]isemployedto
Fig. 1: A 2-layer AFA-PredNet
i(t), if l =0,
Xl(t)= (1)
(MAXPOOL(f(Conv(El−1(t)))), l >0
Xˆ l(t)=f(Conv(Rl(t))) (2)
El(t)=[f(Xl(t)−Xˆ l(t));f(Xˆ l(t)−Xl(t))] (3)
R
l
d (t)=ConvLSTM(El(t−1),Rl(t−1),DevConv(Rl+1(t))) (4)
Rl(t)=MLP(a(t))×R
l
d (t) (5)
where f(·) is an activation function of the neurons, which IV. EXPERIMENTRESULTS
we apply ReLu function to ensure a faster learning in back-
propagation, X(·) t is the neural representation of the level A. The Minimalistic World
l
l at time t. The representation on the EL layer l is E(·)l. Our first experiment was started by using a set of artifi-
The MAXPOOL, Conv, ConvLSTM and MLP are the cially generated visual input data which mimics a moving
corresponding neural algorithms. object perceived from our visual system, i.e. its position
changes quickly at every time-step. In such scenario, the
external movements of an object are manipulated by the
voluntaryactivemotoraction,e.g.therobotmovesanobject
The overall algorithm for learning a whole sequence is toward left or right. So the motor commands caused the
showed in Algorithm 1. changes in the visual perception in this case. This minimal-
Data: i(t)&a(t)∈data location [4,6] are shown below (Fig. 2 and Fig. 4):
while error >threshold or Toexamineanothermovementdirection,wesetthemotor
iteration>maximum iteration do actionvectortobe[0,1].Wealsopickupaseriesoforiginal
for t←0 to T do imagesfromlocation[4,6]asshowninFig. 4.Thepredicted
for l←0 to L do images are shown in Fig. 5.
if l ==L then To further investigate the modulating functions of the
R l d (t)= actionunitstothemultipleGUunits(twoGUsinourcase),
ConvLSTM(El(t−1),Rl(t−1);
wealsoillustratetheneuraloutputsofthemultiplerecurrent
R l d (t)=ConvLSTM(El(t− units GU on each layer. The reason for doing so is to see
1),Rl(t−1),DevConv(Rl+1(t)));
what do their neural activities represent in the embodied
else context, i.e. given the action vector a and a sequence of
Rl(t)=MLP(a(t))×R l d (t); images i. Furthermore, from those representations, we can
end also infer the functions of the MM network. In order to do
end this, we feed the network with a sequence of the pre-trained
/* Generative (top-down) Process imagesandsettheactionunittobe[0,1].Thenwevisualized
*/ the neural representation of the GU from [4,6]. As shown
for l←L to 0 do in Fig. 6 and Fig. 7, while we set the action unit to [0,1],
Xˆ l(t)=f(Conv(Rl(t))); El(t)= the two GU representations on two layers look similar to
[f(Xl(t)−Xˆ l(t));f(Xˆ l(t)−Xl(t));
each other, but the generated output (Fig. 6c and 7c) to the
/* Discriminative lower layer is different, which indicates that the modulated
(bottom-up) Process */ role from the MM unit.
end
B. Line Tracer Robot
end
end To examinethe network performancein a robotic system,
Algorithm 1: AFA-PredNet Computation we recorded the simulation data about the line tracer robot
carfromtheVRepsimulator[32].Inthisscenario,therobot
car equips three vision sensors as well as three Line Finder
istic set up sketches a tracking scenario which is usually sensors. With these sensors, the robot was able to adjust the
perceived from the visual receptors. velocities of its wheels to follow the line. Using VRep as a
tool, we
In this dataset, the size of the input space of the visual
field is 8×12 and only one object appears at one unique 1) collected wheel velocity data and camera data; and
positioninanytime-step.Thetrainingdatasetcomprisestwo 2) used this data to train and verify the network offline.
directional movements (horizontally or vertically) covering Therefore,with the proposedAFA-PredNet,we were able
all of the possible sequences of all objects. The direction to predict the images which will appear in the vision sensor
of the movement, either toward the right or the bottom, is according to the velocity output of the two wheels at the
determined by an action vector containing two neurons. For next time-step. To gather the data, we captured the grey-
instance, the Fig. 2 and Fig. 4 contain an activation moving scale images with the size of 8×12 pixels from the middle
toward the right and toward the bottom. vision sensor every 0.02s. Fig. 9 shows the sample images,
A 2-layer AFA-PredNet was utilized for training. In the which the white shades are the line on the ground followed
training process, the target data was the one-step-ahead by the robot. Furthermore,inputsa of the MM unit are the
predictionoftheinputdata.Intheexperiment,themaximum velocities of the robot car.
iterationwassetto be100,000,learningrate wasη =0.001 Training of the AFA-Prednet for the line tracer robot
, the number of hidden neurons in the action MLP was 4. followedasimilarprocedureasthepreviousexperiment.The
Other parameters of the CNN are shown in Tab. I. target data was one time-step ahead of the input image (i.e.
the next image in around 0.02 second). We used a 3-layer
Parameters Value AFA-PredNet, with 4 hidden units in the MLP network.
Kernel 3×3
Aftertraining,wefedthesequenceoftheobservedimages
Padding 1
Pooling 2×2 totheinputandthesequenceofthewheelvelocitiestomotor
action units. The Fig. 10 illustrates the predicted images
TABLE I: CNN parameters
correspondingtotheoriginalinputs,inwhichwecanobserve
that the AFA-PredNet could generate a distinguishable one
After training, to testify its prediction, we manually set time-steppredictionforthevisionsystem ofthe LineTracer
the action vector to be [1,0], which indicates that the motor robot.
action is from left toward right, and [0,1], which indicates
V. DISCUSSIONS AND SUMMARY
thattheobjectmovesfromthetoptowardthebottom.While
we assume the object movement is from the left toward The feedback affecting sensory input can be regarded as
the right, the original images we selected from the central a kind of predictive information retrieved from the internal
(a) (b) (c)
Fig. 2: Original Images (movement from left to right)
(a) (b) (c)
Fig. 3: Predicted Images (movement from left to right)
(a) (b) (c)
Fig. 4: Original Images (movement from top to bottom)
(a) (b) (c)
Fig. 5: Predicted Images (movement from top to bottom)
memory [33]. Based on PP, in the hierarchical architecture, not independent; instead they are processes that happen at
thefeedbacksignals(especiallythetop-downsignals)predict the same time and integrate with each other. They are per-
the forthcoming sensory input, while the sensory-driven formed with the similar Bayesian inference and are always
bottom-up signals only deliver the error of the estimation. interchanging prior knowledge on the cognitive processes
These functions of top-down and bottom-up processes are level.
(a)1stGU,Layer0 (b)2ndGU,Layer0 (c)Generated GU,Layer0
Fig. 6: Representation of Generative Units (Layer 0)
(a)1stGU,Layer1 (b)2ndGU,Layer1 (c)Generated GU,Layer1
Fig. 7: Representation of Generative Units (Layer 1)
Fig. 8: Data Collected from VRep Simulation
(a)0second (b)0.8seconds (c)1.6seconds (d)2.4seconds (e)2.8seconds
Fig. 9: Image Samples from the Middle Vision Sensor
Similarly, on the cognitive process level, if such kind multi-modality. It captures the structural regularities in the
of prediction lasts as a closed-loop and long-time in a modality, spatial and temporal spaces ([34]), to accomplish
hierarchical way, it plays as a mental simulation about the the tasks of decisionmakingand planning.As such, the dif-
long-term future events. Such a prediction is also about ferencebetweenthesensorimotorpredictionandtheplanning
(a)0second (b)0.8seconds (c)1.6seconds (d)2.4seconds (e)2.8seconds
Fig. 10: Predicted Images from the Same Sequence
behaviour is a matter of difference in time-scale. implementation of AFA-PredNet can be found on Github1
As specified at [35], such a planning process inherited
REFERENCES
from the predictive process only exists, when:
[1] A. Clark. “Whatever next? Predictive brains, situated
1) the specific goalis alreadydeterminedatthe veryfirst
agents, and the future of cognitive science”. In: Be-
beginning;
havioral Brain Sciences (2012), pp. 1–86.
2) at a short- or mid-term planning problem.
[2] R. P. Rao and D. H. Ballard. “Predictive coding
For more complex planning problems, such as the multi-
in the visual cortex: a functional interpretation of
objective optimization problem (e.g. Traveller Salesman
someextra-classicalreceptive-fieldeffects”.In:Nature
Problem, TSP), it needs a higher level of cognitive compu-
neuroscience 2.1 (1999), pp. 79–87.
tational power and time. Nevertheless, from the engineering
[3] K. Friston. “Learning and inference in the brain”. In:
perspective, the short- and mid-term planning is sufficient
Neural Networks 16.9 (2003), pp. 1325–1352.
in some mid-term planning applications, e.g. autonomous
[4] K. Friston. “A theory of cortical responses”. In:
driving, where the original PredNet model was already
Philosophical Transactions of the Royal Society B:
examined to predict the next frame of the vehicle camera.
Biological Sciences 360.1456 (2005), pp. 815–836.
To sum up, the top-down prediction may happen through
[5] E. M. Segal and T. G. Halwes. “The influence of
the whole the brain from the cognitive function to the
frequency of exposure on the learning of a phrase
sensorimotorprocessesisessentialastheyhavethefollowing
structural grammar”. In: Psychonomic Science 4.1
benefit on the lower-level peripheral perception functions:
(1966), pp. 157–158.
1) The target of the feedback pathways in perception is [6] K. Friston and S. Kiebel. “Cortical circuits for per-
applied in sensory prediction.It is realized by extract- ceptual inference”. In: Neural Networks 22.8 (2009),
ing cues from the multimodal or amodal perception pp. 1093–1104.
via feature extraction (e.g. by the early visual system) [7] K. Friston, J. Mattout, and J. Kilner. “Action under-
which becomes a prior. Then, the posterior estimation standing and active inference”. In: Biological cyber-
is applied to the next predictive perception. netics 104.1 (2011), pp. 137–160.
2) Ifthereisadifferencebetweentheposteriorestimation [8] G. Pezzulo, F. Rigoli, and K. Friston. “Active Infer-
and the current receptor signals, the percept may be ence,homeostaticregulationandadaptivebehavioural
derived from a combination of the two to avoid the control”. In: Progress in Neurobiology 134 (2015),
fluctuation caused by neuronal or receptor noise. On pp. 17–35.
the other hand, the error signals are also transmitted [9] H. Von Helmholtz. “Concerning the perceptions in
from bottom-up signals to further act as a prior to the general”. In: Treatise on physiological optics, (1866).
perception cues. [10] W. Lotter, G. Kreiman, and D. Cox. “Deep predictive
3) Compared with the original PredNet, our proposed coding networks for video prediction and unsuper-
AFA-PredNet incorporates the additional motor mod- vised learning”. In: arXiv preprint arXiv:1605.08104
ulated unit (MM) which uses MLP to convert the (2016).
motor information to object movement information to [11] R.A.Brooks.“Howtobuildcompletecreaturesrather
modulate the predictive sensorimotor signals. than isolated cognitive simulators”. In: Architectures
The qualitative experiments were conducted to evaluate for intelligence (1991), pp. 225–239.
the short-term prediction of perception given the visual [12] E. von Holst and H. Mittelstaedt. “The reafference
sequences and the motor actions. We also examined some principle: Interaction between the central nervous
intriguing representation in the GU units to prove the mod- system and the peripheral organs. Selected Papers
ulated role of the MM unit. of Erich von Holst: The Behavioural Physiology of
Animals and Man”. In: (1950).
ACKNOWLEDGMENT
1https://github.com/jonizhong/afa_prednet.git
TheresearchwassupportedbyNewEnergyandIndustrial
Technology Development Organization (NEDO). A Pytorch
[13] R. C. Miall and D. M. Wolpert. “Forward models for [28] Y. LeCun et al. “Gradient-based learning applied to
physiologicalmotorcontrol”.In:Neural networks9.8 document recognition”. In: Proceedings of the IEEE
(1996), pp. 1265–1279. 86.11 (1998), pp. 2278–2324.
[14] N. L. Cerminara, R. Apps, and D. E. Marple-Horvat. [29] J. Donahue et al. “Long-term recurrent convolutional
“An internal model of a moving visual target in the networks for visual recognition and description”. In:
lateral cerebellum”. In: The Journal of physiology Proceedings of the IEEE conference on computer
587.2 (2009), pp. 429–442. vision and pattern recognition. 2015, pp. 2625–2634.
[15] J. Zhong, C. Weber, and S. Wermter. “A Predictive [30] S. Hochreiter and J. Schmidhuber. “Long short-
NetworkArchitectureforaRobustandSmoothRobot term memory”. In: Neural computation 9.8 (1997),
Docking Behavior”. In: Paladyn. Journal of Behav- pp. 1735–1780.
ioral Robotics 3.4 (2012), pp. 172 –180. [31] X. Shi et al. “Convolutional LSTM network: A ma-
[16] D. M. Wolpert, Z. Ghahramani, and M. I. Jordan. chinelearningapproachforprecipitationnowcasting”.
“An internal model for sensorimotor integration”. In: In: Advances in neural information processing sys-
Science (1995), pp. 1880–1880. tems. 2015, pp. 802–810.
[17] D. M. Wolpert and M. Kawato. “Multiple paired [32] E. Rohmer, S. P. Singh, and M. Freese. “V-REP: A
forward and inverse models for motor control”. In: versatile and scalable robot simulation framework”.
Neural Networks 11.7-8 (1998), pp. 1317–1329. In: Intelligent Robots and Systems (IROS), 2013
[18] Y. Demiris and B. Khadhouri. “Hierarchical attentive IEEE/RSJ International Conference on. IEEE. 2013,
multiple models for execution and recognition of pp. 1321–1326.
actions”. In: Robotics and autonomous systems 54.5 [33] J. R. Anderson and L. J. Schooler. “The adaptive
(2006), pp. 361–369. nature of memory.” In: (2000).
[19] H. Hoffmann. “Perception through visuomotor antic- [34] M. Toussaint. “Probabilistic inference as a model of
ipation in a mobile robot”. In: Neural Networks 20.1 planned behavior.” In: KI 23.3 (2009), pp. 23–29.
(2007), pp. 22–33. [35] D. Basso. “Planning, prospective memory, and
[20] R. Mo¨ller and W. Schenck. “Bootstrapping cognition decision-making:threechallengesforhierarchicalpre-
frombehavioracomputerizedthoughtexperiment”.In: dictive processing models”. In: Frontiers in psychol-
Cognitive Science 32.3 (2008), pp. 504–542. ogy 3 (2013), p. 623.
[21] Y. Sugita and J. Tani. “Learning semantic combina-
toriality from the interaction between linguistic and
behavioral processes”. In: Adaptive Behavior 13.1
(2005), p. 33. ISSN: 1059-7123.
[22] J. Zhong, A. Cangelosi, and S. Wermter. “Towards a
self-organizing pre-symbolic neural model represent-
ing sensorimotor primitives”. In: Frontiers in Behav-
ioral Neuroscience 8 (2014), p. 22.
[23] Y.YamashitaandJ.Tani.“Emergenceoffunctionalhi-
erarchyinamultipletimescaleneuralnetworkmodel:
a humanoid robot experiment”. In: PLoS Computa-
tional Biology 4.11 (2008), e1000220.
[24] T. Ogata and H. G. Okuno. “Integration of behav-
iors and languages with a hierarchal structure self-
organized in a neuro-dynamical model”. In: Robotic
Intelligence In Informationally Structured Space (Ri-
iSS),2013IEEEWorkshopon.IEEE.2013,pp.89–95.
[25] J. Zhong, A. Cangelosi, and T. Ogata. “Toward Ab-
straction from Multi-modal Data: Empirical Studies
on Multiple Time-scale Recurrent Models”. In: arXiv
preprint arXiv:1702.05441(2017).
[26] J. Zhong et al. “Sensorimotor Input as a Language
GeneralisationTool:ANeuroroboticsModelforGen-
eration and Generalisation of Noun-Verb Combina-
tions with Sensorimotor Inputs”. In: arXiv preprint
arXiv:1605.03261(2016).
[27] H.Lee,M.Jung,andJ.Tani.“Recognitionofvisually
perceived compositional human actions by multiple
spatio-temporalscales recurrent neural networks”. In:
arXiv preprint arXiv:1602.01921(2016).