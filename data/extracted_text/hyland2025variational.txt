On the Variational Costs of Changing Our Minds
David Hyland1 and Mahault Albarracin2
1 University of Oxford, Oxford, United Kingdom
2 VERSES AI Research Lab, Los Angeles, CA 90016, USA
david.hyland@cs.ox.ac.uk
mahault.albarracin@verses.ai
Abstract.The human mind is capable of extraordinary achievements,
yet it often appears to work against itself. It actively defends its cher-
ished beliefs even in the face of contradictory evidence, conveniently in-
terprets information to conform to desired narratives, and selectively
searches for or avoids information to suit its various purposes. Despite
these behaviours deviating from common normative standards for belief
updating, we argue that such ‘biases’ are not inherently cognitive flaws,
but rather an adaptive response to the significant pragmatic and cogni-
tive costs associated with revising one’s beliefs. This paper introduces a
formal framework that aims to model the influence of these costs on our
belief updating mechanisms.
Wetreatbeliefupdatingasamotivatedvariationaldecision,whereagents
weigh the perceived ‘utility’ of a belief against the informational cost re-
quired to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform com-
putational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate common-
place human behaviours, including confirmation bias and attitude polar-
isation. In doing so, we suggest that this framework makes steps toward
a more holistic account of the motivated Bayesian mechanics of belief
change and provides practical insights for predicting, compensating for,
and correcting deviations from desired belief updating processes.
Keywords:Belief Change·Motivated Reasoning·Active Inference·
Cognitive Effort
“[H]uman reason is both biased and lazy. Biased because it overwhelmingly finds
justifications and arguments that support the reasoner’s point of view, lazy be-
cause reason makes little effort to assess the quality of the justifications and
arguments it produces.” (Mercier and Sperber, 2017, p. 9) [35].
1 Introduction
Humanity faces an increasingly paradoxical epistemic problem. Never before
have people been able to obtain so much information so quickly, yet at the
same time, many societies have become increasingly polarised and paralysed
arXiv:2509.17957v1  [cs.AI]  22 Sep 2025
2 D. Hyland and M. Albarracin
by conflicting narratives. A feature that is common to several manifestations
of this predicament, including public health crises and conspiracy theorising, is
the presence of actors who tenaciously defend beliefs long after the balance of
evidence has shifted. What, exactly, makes changing our minds so hard?
A natural approach to answering this question may begin by supposing a nor-
mative standard or benchmark against which to compare the actual processes
of human belief change. The primary normative model for rational belief updat-
ing is Bayesian reasoning. According to this model, probabilistic beliefs should
be adjusted proportionally to the strength of evidence according to Bayes’ rule.
However, the persistent discrepancy between the Bayesian standard and actual
human belief updating raises questions about whether our epistemic processes
are inherently irrational or if something is missing from the traditional pic-
ture [34].
The Bayesian paradigm largely remains silent onwhyhumans fall short of its
ideals, primarily due to its assumptions that belief-revision is cost-free and that
the driver of epistemic processes should be probabilistic coherency [26]. In prac-
tice, revising one’s beliefs incurs metabolic costs, cognitive effort, and crucially,
pragmatic risks and opportunities. A scientist retracting a cherished hypothesis,
a politician breaking ranks with their party, or a public figure admitting error
each pay tangible costs that a cost-free Bayesian calculus does not account for.
Without a principled way to model the effect of such costs on belief revision, ap-
parent “irrationalities” including confirmation bias [36], motivated reasoning [29],
attitude polarisation [33], and belief persistence [15] seem like fundamental flaws
in human cognition.
We argue that such apparent deviations from Bayesian norms are adaptive re-
sponses of agents operating under motivational considerations and real resource
constraints. In particular, we formalise cognitive belief-change costs using the KL
divergence to quantify informational distances between belief states, represent-
ing the ‘informational work’ required for belief state transitions. Our approach
also integrates social and pragmatic factors. Beliefs are influenced by identity,
social status, and interpersonal dynamics; fears of ostracism or admitting errors
can increase resistance to change. Our hope is that through such modelling, we
can take steps toward developing general frameworks that explain not just iso-
lated sources of non-Bayesian belief updating, but also the inherent trade-offs
between competing considerations associated with changing our minds.
1.1 Contributions and Paper Structure
Ourprimarycontributionistheproposalofavariationalcostfunctionalforbelief
revision that models the influence of pragmatic affordances and cognitive costs
on human belief updating. Secondly, we present results from simplified com-
putational experiments demonstrating how varying conservatism and likelihood
weighting parameters qualitatively exhibit phenomena such as confirmation bias,
On the Variational Costs of Changing Our Minds 3
evidence search asymmetries, and attitude polarisation. Finally, we discuss the
implications of our model and promising future directions.
2 Related Literature
“There is considerable evidence that people are more likely to arrive at conclu-
sions that they want to arrive at, but their ability to do so is constrained by
their ability to construct seemingly reasonable justifications for these conclusions.
(Kunda, 1990) [29].
2.1 Decision-Theoretic Models of Belief Updating
Drawing on frameworks of decision making, human belief revision is increasingly
being treated as a value-based decision [28,47,51]. On this view, beliefs are
updated not purely based on their accuracy, but are associated with autility.
The utility of holding a belief is derived from the outcomes it leads to, which can
be internal (emotional comfort, positive feelings) or external (acceptance within
a community, job opportunities) [51]. This is supported by several arguments
highlighting the centrality of affect in decision-making and belief-updating [13,
27,50].Certainbeliefsmaygiverisetoutilityinproportiontohowwelltheytrack
orpredictreality,inwhichcase,thereisanincentivefortheagenttoseektruthful
beliefs. Other beliefs may demand that the agent confabulates an elaborate yet
tenuous narrative that coincidentally supports their desired conclusion. In other
words, a belief’s usefulness can be orthogonal to its truthfulness.
2.2 Cognitive Costs
In addition to the pragmatic incentives that shape belief updating, there are un-
avoidable costs that any agent must pay to change their beliefs. These costs have
been studied from the perspective of bounded/resource/computational rational-
ity, where the presence of some form of cost associated with cognition is explicitly
modelled in an agent’s decision-making [21,30,31,39–41,54,64]. Belief updating
can be understood as a thermodynamic process involving transitions between
mental states, where each transition incurs unavoidable dissipative costs [17].
These costs arise from fundamental physical principles governing information
processing in biological systems at the level of neural computation and metabolic
energy expenditure [62,63].
The transition between belief states involves both work-like and heat-like com-
ponents. The work-like component corresponds to the directed shift of the belief
state, while heat dissipation occurs in the form of entropy production during the
transitions between belief states in finite amounts of time [3]. The total dissi-
pation produced by belief updating can be quantified as the difference between
the reversible work theoretically possible and the actual work captured during
the transition process. This represents the unavoidable cost of finite-time belief
changes [43].
4 D. Hyland and M. Albarracin
This thermodynamic perspective helps to explain why, other considerations be-
ing equal, rapid belief changes tend to be more costly and inefficient compared
to gradual updates. The system must balance the speed of belief revision against
the increased dissipative costs of rapid change. This intuition can be made more
precise using concepts from finite-time thermodynamics [3]. The total entropy
production in a sequence of step-equilibrations is bounded by∆Su ≥ L2
2K , where
Lis the thermodynamic length of the belief change pathway andKis the num-
ber of intermediate equilibration steps [48]. Thus, increasing the number of steps
decreases the lower bound on total entropy production, permitting more efficient
pathways of belief change. The brain appears to possess several remarkable fea-
turesthataidinminimisingthesecosts.Forinstance,theefficientcodinghypoth-
esis suggests that neural representations of sensory information are structured to
minimise the number of neuronal spikes required to transmit a given signal [4].
Understanding these fundamental thermodynamic constraints provides insight
into why belief change can be so difficult even in the presence of contradictory
evidence. The brain must carefully balance the energetic and informational costs
of updating against the potential benefits. It is unclear precisely how significantly
the thermodynamic costs of belief change contribute to this effect, and it would
be worthwhile empirically investigating how such considerations can contribute
to and explain belief inertia.
2.3 Social Costs
Humanbeliefsservenotonlyasinternalmodelsoftheworldbutalsoassocialsig-
nals and commitments. In active inference and variational learning frameworks,
agents update beliefs to minimise surprise or prediction error, yet these updates
occur in a social context where beliefs fulfil both epistemic (truth-seeking) and
social-coordination functions [2,6,9,35,57,58,61]. Believing (or disbelieving) cer-
tain propositions can grant individuals emotional comfort or group acceptance,
independent of the belief’s accuracy [51]. This dual role means that an agent’s
posterior after observing new evidence is not determined by epistemic consid-
erations alone, but also by the expected social and personal utilities associated
with holding particular beliefs [1,22,29,51]. Consequently, standard Bayesian
updates, which are focused purely on data and prior likelihood, are often tem-
pered by an additional motive: to align with valued identities and norms that
confer utility on the belief state [9,22,35,61]. This insight echoes the idea that
all thinking is “wishful” thinkingto some extent, with motivational imperatives
modulating inferential processes [28]. The free energy minimised during belief
updating thus implicitly includes not just accuracy-related (surprisal) terms but
also pragmatic terms capturing the work required to overcome cognitive inertia
and social repercussions [2,8].
Changing one’s mind can threaten group affiliations and invite real or perceived
social sanctions (e.g., loss of status, trust, or membership) [6]. Beliefs often func-
tion as markers of group identity, so revising a key belief may signal disloyalty
On the Variational Costs of Changing Our Minds 5
or value misalignment, incurring social costs like ostracism or ridicule. Antici-
pation of such costs creates a strong deterrent to belief revision, especially for
identity-linked beliefs maintained by tight-knit communities and normative ex-
pectations [2,35]. Indeed, social norms enforce a kind of epistemic conformity:
individuals internalize the expectation that they “ought” to hold certain beliefs to
remain in good standing [6,22]. From a decision-analytic perspective, the utility
of a belief therefore includes not only its truth-tracking benefits but also its social
payoff. A false or unfounded belief might persist if it brings social acceptance or
emotional relief, whereas a truthful belief might be resisted if it carries stigma
or existential dread. Accordingly, belief change in social contexts resembles a
form of motivated reasoning: agents are inclined to arrive at the conclusions
they want (or need) to reach, as long as they can justify them to themselves
and others [29]. Here, “wants” are not arbitrary whims but structured by social
identity and normative pressures—what one wants to believe is often what one’s
group wants one to believe. An agent will unconsciously search for justifications
to retain beliefs that serve valued social goals (e.g. solidarity, consistency, pride)
and discount evidence that threatens those goals. The free energy landscape is
warped by social potential energy. Certain directions of belief change appear
steep (costly) due to the interpersonal consequences associated with them.
Empirical research supports these principles. For instance, people consistently
overestimate the severity of the social sanctions they will face for changing a
politically charged belief, leading to excessive self-censorship and public confor-
mity [55]. In one set of their studies, U.S. partisans expected far more backlash
from their in-group if they voiced a dissenting opinion than what actually ma-
terialised, with an average overestimation effect size ofd≈0.87. These inflated
expectations of ostracism or punishment (sometimes stemming from an egocen-
tric bias in social perspective-taking) make belief revision seem riskier than it
truly is. Accordingly, individuals often stick to publicly defending their prior
attitudes, even when privately grappling with contrary evidence. Social psychol-
ogists refer to this pattern as identity-protective cognition, wherein reasoning
processes bend to protect the agent from the social identity costs of admitting
error. The effect can become self-reinforcing. If everyone fears speaking up or
changing their mind, the apparent unanimity of belief within the group remains
unchallenged, further raising the perceived cost of dissent. Yet research also
shows that these perceived social costs are malleable. Prompting individuals to
reflect on their past loyalty and contributions to the group can reassure them
that a change of mind will not irrevocably brand them as “disloyal,” thereby
significantly reducing their concern about sanction and encouraging more open
expression of revised beliefs.
Beliefs are multi-functional cognitive tools that balance accuracy, utility, and in-
ertia. They must at once represent the world (epistemic accuracy), support our
emotionalneedsand moral values, andcoordinate withour social milieu(utility),
all while minimising drastic revisions that incur cognitive and social “work” (iner-
tia). This perspective prepares us to interpret classic phenomena—confirmation
6 D. Hyland and M. Albarracin
bias, selective exposure to information, and attitude polarisation, not as inexpli-
cable failures of rationality, but as strategic trade-offs given the agent’s objec-
tives. An agent facing high costs for belief change will rationally exhibit a kind of
conservatism. Agents will favour information that confirms existing beliefs and
avoids provoking costly updates. Indeed, a confirmation bias in information-
seeking can be seen as an adaptive strategy to preserve high-utility beliefs by
selectively attending to congruent evidence and filtering out challenges. Experi-
mental studies of selective exposure document that people spend more time with
news and arguments that align with their preexisting attitudes than with those
that contradict them, even when source credibility is controlled [60]. By skim-
ming “friendly” evidence, individuals reduce the likelihood of encountering data
that would demand painful social readjustments or internal value conflicts. Simi-
larly, communities may become polarised when each side’s beliefs carry their own
social rewards—members of opposing groups double down on group-consistent
narratives, bolstering internal cohesion at the expense of cross-group accuracy.
Over time, this self-reinforcing selection and interpretation of evidence drives
group attitudes further apart, as each group lives in a bubble where maintaining
their version of reality is pragmatically advantageous [2]. The following sections
will explore how confirmation bias in evidence appraisal, asymmetrical informa-
tion search, and polarisation dynamics emerge naturally once we acknowledge
that changing one’s mind is not “free.” It incurs variational costs, paid in both
cognitive effort and social capital, which a resource-rational mind navigates by
carefully weighing when belief change is truly worth the price.
2.4 Confirmation Bias
Confirmation bias manifests through selective attention mechanisms, as shown
in recent experimental work [46,56]. Westerwick et al. demonstrated that when
selecting political information online, participants spent more time with content
matching their existing views, regardless of source quality [60]. This bias emerged
from participants’ choices rather than the content itself. Building on this, [56]
revealed that making a categorical choice selectively enhanced sensitivity to
subsequent evidence consistent with that choice, similar to attentional cueing
effects. [46] proposed a neural mechanism for this bias, suggesting that choices
direct feature-based attention to amplify processing of choice-consistent sensory
evidence while suppressing inconsistent information. Together, these findings in-
dicate that confirmation bias operates through early attentional selection rather
than solely in later-stage decision processes.
2.5 Motivated Reasoning
Confirmation bias is a type of motivated reasoning, a process where information
processing is biased toward achieving desired outcomes rather than accuracy
alone [29]. Motivations can be accuracy-driven, encouraging unbiased reason-
ing, or directional, prompting strategies that reinforce existing beliefs, identity,
or preferred conclusions. However, motivated reasoning remains constrained by
On the Variational Costs of Changing Our Minds 7
plausibility; people select cognitive processes, such as memory retrieval and in-
terpretation, that justify favoured conclusions rather than inventing implausible
beliefs [14,24,44].
Individuals revise beliefs asymmetrically, giving more weight to confirmatory
or emotionally favourable evidence than to equally informative negative evi-
dence [32]. Motivated reasoners also selectively trust or avoid sources based on
alignment with their views, effectively assigning lower reliability to disconfirm-
ing information. This acts like a biased Bayesian filter, reducing the impact of
contradictory evidence on belief updating [45].
Consequently, shared evidence can polarise rather than unify groups with op-
posing and even similar priors. When interpreting balanced evidence through a
biased lens, individuals’ initial beliefs often become more extreme, exacerbating
attitude polarisation [2,5].
2.6 Biased Reasoning
Two recent frameworks have been proposed to explain some of the biases that
occur in reasoning: coherence-based reasoning (CBR) [53] and belief-consistent
information-processing (BCIP) [37]. Coherence-based reasoning posits that in-
dividuals strive to maintain a consistent and interconnected set of beliefs, min-
imising cognitive dissonance. More specifically, in CBR, a constraint-satisfaction
network settles into an attractor by bidirectionally reshaping both beliefs and in-
coming information to maximise overall coherence. Crucially, strongly activated
priors are harder to dislodge, so they often anchor the attractor state. Simi-
larly, belief-consistent information processing describes the tendency to favour
information that aligns with pre-existing beliefs, a process that is less cogni-
tively demanding than evaluating and integrating contradictory evidence. BCIP
is a special case of CBR’s coherence construction under conditions of dominant
priors [38].
These two frameworks can be reconciled with our account through the lens
of cognitive economy. Our variational cost framework formalises this principle
by suggesting that altering one’s beliefs incurs a cognitive cost, quantified by
the KL divergence, which measures the informational distance between prior
and posterior beliefs. Moreover, the weight of other firmly held beliefs can be
explained by the presence of costs associated with revising more strongly held
beliefs. For example, the expected cost associated with modifying a fundamental
belief such as “I make correct assessments of the world” would be increased levels
of doubt about the reliability of one’s assessments, potentially leading to greater
general levels of uncertainty and the accompanying negative affect that often
arises.
In this sense, both coherence-based reasoning and belief-consistent information
processing can be viewed as cognitive strategies that minimise the costs that we
describe. By maintaining coherence and selectively processing information, indi-
viduals reduce the “informational work” required to update their mental models
8 D. Hyland and M. Albarracin
of the world, thereby avoiding the significant pragmatic and cognitive expen-
ditures associated with belief revision. In essence, these frameworks highlight
different facets of the same underlying drive to manage cognitive resources ef-
ficiently, where the perceived utility of a belief is weighed against the inherent
costs of mental reorganisation.
3 A Motivated Variational Belief Change Model
As a starting point, we take inspiration fromvariational inference[7,59], which
underpins the mathematical formalism of the Free-Energy Principle and Active
Inference [18,19]. In the standard Bayesian paradigm, the goal is to infer a
posterior beliefp(s|o)about the state of the worldsgiven an observation
o. According to Bayes’ rule, finding this posterior requires one to compute the
model evidence or marginal likelihoodp(o), which is an intractable problem in
general. Variational inference aims to reformulate this problem by recasting it as
anoptimisation problemover a variational familyQof probability distributions.
The objective function of this optimisation problem is the negativeevidence
lower bound(ELBO) orvariational free energy(VFE), and is given by
F[q(s), o] =−Eq(s)[logp(o|s)]| {z }
Accuracy
+D KL [q(s)||p(s)]| {z }
Complexity
(1)
=−E q(s)[logp(s, o)]| {z }
Energy
−H[q(s)]| {z }
Entropy
.(2)
The decomposition in Equation 1 highlights a key tension between the expected
log likelihood of the observation (accuracy) and the KL divergence from the prior
to the variational posterior (complexity). In particular, this complexity acts as
aregulariseron the agent’s posterior beliefs, penalising models that differ more
from the prior.
Core to the description of any agent is a description of its boundary, also com-
monly known as itsMarkov blanket. The Markov blanket of an object describes
the interface via which it is coupled to its environment. According to the Free
Energy Principle (FEP), the internal paths of systems possessing a Markov blan-
ket can be viewed as probabilistic beliefs about external paths, and the internal
and active paths of the system appear to minimise its VFE [20]. When moving
to descriptions of agents, however, the system’s internal states embody not only
a predictive model of the world, but alsopreferencesover possible configurations
of the agent. This is whereactive inference(AIF) comes into the picture.
AIF extends the FEP to recognise the role of theactionsthat agentic systems
can perform to influence the environment and, vicariously, their observations [10,
12,42]. Cast here in the variational perspective, the objective function that is
posited to drive decision-making in AIF is theexpected free energy(EFE), which
is a functional of a policy (sequence of actions)π, and is given by
On the Variational Costs of Changing Our Minds 9
Fig.1: A depiction of the causal influences of different components of the agent-
environment pair on each other. External pathssrepresent states of the world external
to a system or agent under consideration. This system possesses a Markov blanket,
which separates internal from active paths and can itself be divided into sensory and
active paths. We further assume that internal paths consist of two distinct components:
beliefs and preferences, which mutually influence each other, are influenced by sensory
paths, and in turn influence active paths.
G(π) =−E q(s,o|π) [DKL [q(s|o, π)||q(s|π)]]| {z }
Epistemic value
−Eq(o|π) [log ˜p(o)]| {z }
Pragmatic value
,(3)
where˜p(o)is a probability distribution representing the agent’s preferences over
their own observations, commonly known as aprior preference[10]. Here, we
propose to extend this picture by considering the implications of assuming that
agents havepreferences about their own beliefs, and develop a mathematical
framework for describing the ensuing implications for belief updating. In other
words, we suggest that theCmatrix, which is used in the active inference lit-
erature to parameterise the preference prior [10,23,42], can be extended to be
defined over the agent’s own beliefs as well, rather than only observations/states.
For the purposes of this study, we focus on the mechanisms that drive belief
change in agents. In particular, we are interested in the mapping from sensory
states to belief states. We investigate the consequences of assuming that this
mappingiscomprisedoftwokeycomponents.Thefirstcomponentisapreference
satisfaction component, represented here by an "expected utility" term, which
can be related to prior preferences through a softmax transformation [11]. The
second component is a direct cost for belief updating, which is quantified by the
KL divergence from the agent’s prior beliefs to their posterior beliefs.
We will further assume that agents’ preferences are grounded only in particular
paths, and not directly on external paths, which is in concordance with an affect-
driven view on motivation [49,52]. Under these assumptions, an agent’s prefer-
10 D. Hyland and M. Albarracin
Fig.2: Schematic depicting the components involved in our proposed model of belief
updating. Prior beliefs and observations act as inputs to a process that optimises a
balance between a belief utility functional and a complexity term, measuring the KL
divergence from prior to the ultimate posterior beliefs.
ences can be described mathematically by autility functionalU:Q × O →R
defined over observations andbeliefs, but not external states.
Given this, we model an agent’s belief updating processes as a variational opti-
misation process that maximises the following functional of beliefs and observa-
tions:
F[q(s), o] =U[q(s), o]| {z }
belief utility
−λ DKL [q(s)||p(s)]| {z }
complexity
,(4)
whereλ≥0is a parameter determining the relative strength of the cost of
belief updating to the belief utility term. The belief utility term may or may not
depend on the accuracy of the model, and depending on its form, can give rise
to different belief updating behaviours. Under this model, takingλ→0induces
a belief update that is purely driven by belief utility, which is akin to assuming
that the agent is able to instantaneously and effortlessly convince themselves of
whatever they wish to believe. On the other hand, takingλ→ ∞increases the
cost of updating to the point that the agent is no longer able to change their
mind, under any circumstances.
Importantly, one particular form for the belief utility that we investigate is a
linear combination of what we term anaffective utilityand a weighted expected
log-likelihood oraccuracyterm, which takes the following form:
U[q(s), o] =U[q(s), o]| {z }
affective utility
+αE q(s)[logp(o|s)]| {z }
accuracy
,(5)
whereα≥0is alikelihood weightingparameter, which determines the extent
to which the agent’s final belief distribution explains the data it has observed.
A higher value ofαcan be interpreted as a stronger desire to arrive at be-
liefs that explain the observed data well. Moreover, for constant affective utility
functions andα=λ= 1, we recover the VFE as a special case of “accuracy-
On the Variational Costs of Changing Our Minds 11
Fig.3: Plots depicting how agents with different conservatism parametersλand likeli-
hood weight parametersαrespond to evidence that confirms or contradicts their belief
preferences to varying degrees. Left: Final belief of the probabilityq(s= 0)of state 0
occurring as the evidence strength (in the form of a likelihoodp(o|s= 0)) varies from
0 to 1 for different values ofλ. Right: Final beliefq(s= 0)as evidence strength varies
for different values ofα.
motivated” belief updating. In the following section, we study the predictions
made by adopting the belief utility functional given in Equation 5.
4 Experiments and Results
To study the implications of our proposed model on how motivated agents up-
date their beliefs, we conducted a series of minimal experiments using categorical
distributions3. In all simulations, we consider how a single piece of evidence pre-
sented in the form of a likelihood distribution may be selected and subsequently
influence the belief updating process. In particular, we demonstrate that un-
der our model, several key features of human belief updating are qualitatively
recovered. Moreover, our model can serve as a framework to generate testable
predictions and simulations of human behaviour in various scenarios.
4.1 How do different agents react to differing degrees of good vs
bad news?
In our first set of experiments, we aim to understand and illustrate the effects of
varyingthestrengthofevidence,theconservatismparameterλandthelikelihood
weight parameterαon belief updating. In this scenario, an agent begins with an
initial prior over the outcomes of a Bernoulli random variable (i.e., a biased coin
flip) specified byp(s= 0) = 0.3, and receives evidence of varying strengths in
the form of a likelihoodp(o|s). For Bernoulli random variables, we will assume
that the hidden statesmay take the values0or1. The agent updates their
3 The code for generating the experimental results can be found athttps://github.
com/dkhyland/motivated-variational-belief-updating
12 D. Hyland and M. Albarracin
Fig.4: Plots of the objective value for Scenarios 1 and 2 with different combinations of
evidence. In both scenarios, we fixα= 2.0and use a linear affective utility functional
withU[q(s), o] =q(s= 0). Left: Scenario 1, where Evidence A has high utility but high
KL from the prior and Evidence B has low utility but low KL from the prior. Right:
Scenario 2, where Evidence A has high utility and low KL, whereas Evidence B has
low utility and high KL.
beliefs to minimise the objective in Equation 4 under the belief utility given in
Equation 5.
In Figure 3, we plot the final belief in state 0 as we vary the evidence strength
p(o|s= 0)between 0 and 1 along thexaxis and the values ofλ(left) and
α(right) as a spectrum forλ∈[1,10]andα∈[1,10], along with the Bayesian
update. From the left plot, we observe that higher values ofλlead to updates
that are closer to the prior, whereas lower values ofλlead to updates that are
more sensitive to the affective utility. From the right plot, the opposite effect
is observed – higher likelihood weights lead to more sensitivity to the evidence,
and lower likelihood weights increase sensitivity to the affective utility.
4.2 How do the relative strengths of belief utility and conservatism
affect the selection of evidence?
In this study, we demonstrate the presence of a form of confirmation bias in our
model, and seek to understand how different components of the model affect the
selection of evidence in our motivated agent. In particular, recall that a crucial
tenet within active inference is that agents are active sense-makers, selecting
evidence to resolve uncertainty in both specific and non-specific manners in order
to develop a better model of the world and achieve their ultimate objectives. In
this experiment, we extend this notion to include the motivated selection of
evidence to either confirm or disconfirm an agent’s preferences.
We consider what happens when an agent with a linear affective utility who
prefers to believe thatp(s= 0) = 1is presented with two pieces of evidence.
We studied two different scenarios for what these pieces of evidence may be. In
Scenario 1, the first piece of evidence (Evidence A) is‘confirmatory’, in the sense
that it provides evidence for the agent’s desired belief, but is further (induces
On the Variational Costs of Changing Our Minds 13
updates with a larger KL divergence) from the agent’s prior compared to the
second piece of evidence (Evidence B). Evidence B is‘contradictory’, in the
sense that it is evidence against the agent’s desired belief but is closer to the
agent’s prior. In Scenario 2, Evidence A has both a higher affective utility and
induces updates with a lower KL from the prior to the posterior. In Figure 4,
the objective value is plotted as we sweep across values ofλ∈[0.1,100]. In
Scenario 1, we observe a threshold at which the agent switches from selecting
confirmatory evidence to selecting contradictory evidence, whereas this does not
occur in Scenario 2. Intuitively, this is because for low values ofλ, the utility
term dominates belief updating, but for higher values ofλ, the cognitive cost
term dominates. In contrast, when both the utility component is higher and
cognitive costs are lower for one piece of evidence over the other, there is never
a reason for the agent to choose to observe disconfirmatory evidence.
4.3 How do belief conservatism and likelihood weighting affect
attitude polarisation?
Fig.5: Plots of attitude polarisation effects between two agents who begin with the
same prior beliefs and observe the same evidence, but have different affective utilities.
Agent 1 linearly prefers to believe thatq(s= 0) = 1, whereas Agent 2 linearly prefers
to believe thatq(s= 0) = 0. Left: Final beliefs as we varyλfrom 0 to 10. Right: Final
beliefs as we varyαfrom 0 to 10.
In our final experiment, we simulated a basic attitude polarisation scenario,
where two agents, Agents 1 and 2, began with the same prior belief about the
probabilityq(s= 0), and observed the same evidence in the form of a likelihood.
Both agents were endowed with linear affective utility functions, but Agent 1 had
a preference for believing thatq(s= 0) = 1and Agent 2 had a preference for
believing thatq(s= 0) = 0. Plotting the final beliefs after updating according to
our model as we variedλandαindependently in Figure 5, we observe that for
low values of both parameters, the two agents’ final beliefs differed significantly,
demonstrating a basic form of attitude polarisation. However, as we increase the
14 D. Hyland and M. Albarracin
two parameters, the agents’ final beliefs converged toward similar (though not
necessarily Bayes rational) beliefs.
5 Discussion
Though much work needs to be done in empirically validating instantiations of
the framework, our findings lend plausibility to the idea that realistic belief up-
dating is subject to significant inertia and bias, driven largely by the constraints
imposed on human agents by both internal cognitive limitations and external
structures.
Realistic belief revision is rarely drastic, especially in the presence of cognitive
costs for updating. From a cognitive standpoint, rapid updates necessitate more
complex neural rewiring and a higher cognitive load, which can overwhelm lim-
ited cognitive resources. Agents would tend to avoid such costly leaps. Incremen-
tal steps across belief space reduce immediate costs but may also cumulatively
result in lower total energetic and informational expenditure.
Under this view, we hypothesise that gradual transitions are typically more sus-
tainable and preferable overall. Such considerations could explain why individu-
als are naturally inclined to resist abrupt changes in their belief systems despite
potentially strong contradictory evidence, reinforcing conservative patterns of
information integration.
Strategies for effective belief updatingOur basic model suggests several
strategic insights for promoting more effective belief updating. Given the high
cost of large leaps in belief space, strategies should prioritise incrementalism.
This involves structuring information exposure in manageable segments that
progressively lead individuals towards desired beliefs, thereby reducing the ener-
getic,cognitive,andsocialresistancetodramaticchanges.Socialnetworksshould
be leveraged strategically: encouraging cross-cutting social ties and diversity in
informational environments can reduce the perceived social risks associated with
belief change.
5.1 Future Directions
In considering future avenues for research based on our current findings, several
promising directions are worth exploring.
Completing the Action-Perception LoopSo far, our model has focused
on the processes involved in the updating of beliefs, taking into account sensory
evidence.Ourpreliminarydataselectioninvestigationtakesthisastepfurtherby
demonstrating how decisions about what data to observe can influence decision-
making. However, further work is required to fully integrate motivation into the
perception-action loop.
On the Variational Costs of Changing Our Minds 15
Addition of temporal considerationsSo far, our model has not explicitly
incorporated the temporal aspect of belief updating, which we believe to be sig-
nificant in modelling the various costs that must be taken into consideration.
Indeed, several works have posited a central role ofrates of changein free en-
ergy/prediction errors as crucial to understanding affect [16,25]. Extending the
model to account for the role of time would allow a more detailed analysis of
how belief trajectories could be optimised, rather than single updates.
Extensions to group dynamicsFurther work could more explicitly incor-
porate group dynamics, particularly focusing on how social networks influence
belief inertia and revision costs. Future work could explore the degree to which
group identity and perceived social costs shape belief stability, potentially repli-
cating frameworks similar to those presented by [2] on epistemic communities.
By examining how belief updates propagate through structured networks and
assessing how identity-protective reasoning reinforces certain belief states, we
can quantify the inertia inherent within closely knit communities. Moreover,
evaluating the relative weight of belief confidence levels and their susceptibility
to drift could provide deeper insights into the dynamics of belief evolution in
social contexts. Such extensions may also clarify how networked beliefs reinforce
each other, creating feedback loops that stabilise misinformation.
Further empirical validationEmpirical validation remains essential for con-
firming and refining our theoretical propositions. Future empirical work will
rigorously test model predictions using controlled laboratory experiments, field
studies, and simulation analyses. For instance, quantifiable predictions derived
from our framework—such as the relationship between KL divergence, belief
revision speed, and associated cognitive or social costs—could be tested experi-
mentally by monitoring physiological or neural responses during belief updating
tasks. Longitudinal field studies examining belief trajectories within real-world
social groups could also provide valuable validation, providing insights into how
incremental versus rapid belief changes correlate with tangible social and cogni-
tive outcomes.
Acknowledgments.The authors would like to thank Lancelot Da Costa and Tomáš
Gavenčiak for helpful discussions and feedback.
References
1.Albarracin, M., Bouchard-Joly, G., Sheikhbahaee, Z., Miller, M.,
Pitliya, R. J., and Poirier, P.Feeling our place in the world: An active
inference account of self-esteem.Neuroscience of Consciousness 2024, 1 (2024),
niae007.
2.Albarracin, M., Demekas, D., Ramstead, M. J., and Heins, C.Epistemic
communities under active inference.Entropy 24, 4 (2022), 476.
3.Andresen, B.Currenttrendsinfinite-timethermodynamics.Angewandte Chemie
International Edition 50, 12 (2011), 2690–2704.
16 D. Hyland and M. Albarracin
4.Barlow, H. B., et al.Possible principles underlying the transformation of
sensory messages.Sensory communication 1, 01 (1961), 217–233.
5.Bartels, L. M.Beyond the running tally: Partisan bias in political perceptions.
Political Behavior 24, 2 (2002), 117–150.
6.Bicchieri, C., and Mercier, H.Norms and Beliefs: How Change Occurs. Ox-
ford University Press, Oxford, 2014.
7.Blei, D. M., Kucukelbir, A., and McAuliffe, J. D.Variational inference:
A review for statisticians.Journal of the American statistical Association 112, 518
(2017), 859–877.
8.Bouizegarene, N., Ramstead, M. J. D., Constant, A., Friston, K. J.,
and Kirmayer, L. J.Narrative as active inference: an integrative account of
cognitive and social functions in adaptation.Frontiers in Psychology 15(2024),
1345480.
9.Constant, A., Ramstead, M. J. D., Veissière, S. P. L., and Friston, K. J.
Regimes of expectations: An active inference model of social conformity and human
decision making.Frontiers in Psychology 10(2019), 679.
10.Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., and Friston,
K.Active inference on discrete state-spaces: A synthesis.Journal of Mathematical
Psychology 99(2020), 102447.
11.Da Costa, L., Sajid, N., Parr, T., Friston, K., and Smith, R.Reward
maximization through discrete active inference.Neural Computation 35, 5 (2023),
807–852.
12.Da Costa, L., Tenka, S., Zhao, D., and Sajid, N.Active inference as a model
of agency.arXiv preprint arXiv:2401.12917(2024).
13.Deane, G., Mago, J., Fotopoulou, A., Sacchet, M., Carhart-Harris,
R., and Sandved-Smith, L.The computational unconscious: Adaptive narrative
control, psychopathology, and subjective well-being, Jan 2024.
14.Ditto, P. H., Pizarro, D. A., and Tannenbaum, D.Motivated moral rea-
soning.Psychology of learning and motivation 50(2009), 307–338.
15.Ecker, U., Lewandowsky, S., Cook, J., Schmid, P., Fazio, L., Brashier,
N., Kendeou, P., Vraga, E., and Amazeen, M.The psychological drivers of
misinformation belief and its resistance to correction.Nature Reviews Psychology
1(01 2022), 13–29.
16.Fernandez Velasco, P., and Loev, S.Affective experience in the predictive
mind: a review and new integrative account.Synthese 198, 11 (2021), 10847–10882.
17.Fields, C., Goldstein, A., and Sandved-Smith, L.Making the thermody-
namic cost of active inference explicit.Entropy 26, 8 (2024), 622.
18.Friston, K.The free-energy principle: a unified brain theory?Nature reviews
neuroscience 11, 2 (2010), 127–138.
19.Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzhöffer, K., Pavli-
otis, G. A., and Parr, T.The free energy principle made simpler but not too
simple.Physics Reports 1024(2023), 1–29.
20.Friston, K., Da Costa, L., Sakthivadivel, D. A., Heins, C., Pavliotis,
G. A., Ramstead, M., and Parr, T.Path integrals, particular kinds, and
strange things.Physics of Life Reviews(2023).
21.Gershman, S. J., Horvitz, E. J., and Tenenbaum, J. B.Computational
rationality: A converging paradigm for intelligence in brains, minds, and machines.
Science 349, 6245 (2015), 273–278.
22.Guénin-Carlut, A., and Albarracin, M.On embedded normativity: An ac-
tive inference account of agency beyond flesh. InActive Inference, vol. 1630 of
On the Variational Costs of Changing Our Minds 17
Communications in Computer and Information Science. Springer Nature, Cham,
Switzerland, 2024, pp. 91–105.
23.Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I.,
and Tschantz, A.pymdp: A python library for active inference in discrete state
spaces.arXiv preprint arXiv:2201.03904(2022).
24.Jain, S. P., and Maheswaran, D.Motivated reasoning: A depth-of-processing
perspective.Journal of Consumer Research 26, 4 (2000), 358–371.
25.Joffily, M., and Coricelli, G.Emotional valence and the free-energy principle.
PLoS computational biology 9, 6 (2013), e1003094.
26.Jones, M., and Love, B. C.Pinning down the theoretical commitments of
bayesian cognitive models.Behavioral and Brain Sciences 34, 4 (2011), 215–231.
27.Kiverstein, J., Miller, M., and Rietveld, E.Desire and motivation in pre-
dictive processing: An ecological-enactive perspective.Review of Philosophy and
Psychology(2024), 1–21.
28.Kruglanski, A. W., Jasko, K., and Friston, K.All thinking is ‘wishful’
thinking.Trends in Cognitive Sciences 24, 6 (2020), 413–424.
29.Kunda, Z.The case for motivated reasoning.Psychological bulletin 108, 3 (1990),
480.
30.Lewis, R. L., Howes, A., and Singh, S.Computational rationality: Linking
mechanism and behavior through bounded utility maximization.Topics in cogni-
tive science 6, 2 (2014), 279–311.
31.Lieder, F., and Griffiths, T. L.Resource-rational analysis: Understanding
human cognition as the optimal use of limited computational resources.Behavioral
and brain sciences 43(2020), e1.
32.Little, A. T.How to distinguish motivated reasoning from bayesian updating.
Political Behavior(2025), 1–25.
33.Lord, C., Ross, L., and Lepper, M.Biased assimilation and attitude polar-
ization: The effects of prior theories on subsequently considered evidence.Journal
of Personality and Social Psychology 37(11 1979), 2098–2109.
34.Mandelbaum, E.Troubleswithbayesianism:Anintroductiontothepsychological
immune system.Mind & Language 34, 2 (2019), 141–157.
35.Mercier, H., and Sperber, D.The enigma of reason. Harvard University Press,
2017.
36.Nickerson, R. S.Confirmation bias: A ubiquitous phenomenon in many guises.
Review of General Psychology 2, 2 (1998), 175–220.
37.Oeberst, A., and Imhoff, R.Toward parsimony in bias research: A proposed
common framework of belief-consistent information processing for a set of biases.
Perspectives on Psychological Science 18, 6 (2023), 1464–1487.
38.Oeberst, A., Mischkowski, D., and Imhoff, R.Belief-consistent information
processing or coherence-based reasoning: Integrating two parsimonious frameworks
for biases.European Journal of Social Psychology(2025).
39.Ortega, P. A., and Braun, D. A.Thermodynamics as a theory of decision-
making with information-processing costs.Proceedings of the Royal Society A:
Mathematical, Physical and Engineering Sciences 469, 2153 (2013), 20120683.
40.Ortega, P. A., Braun, D. A., Dyer, J., Kim, K.-E., and Tishby,
N.Information-theoretic bounded rationality.arXiv preprint arXiv:1512.06789
(2015).
41.Parr, T., Holmes, E., Friston, K. J., and Pezzulo, G.Cognitive effort and
active inference.Neuropsychologia 184(2023), 108562.
42.Parr, T., Pezzulo, G., and Friston, K. J.Active inference: the free energy
principle in mind, brain, and behavior. MIT Press, 2022.
18 D. Hyland and M. Albarracin
43.Parrondo, J. M., Horowitz, J. M., and Sagawa, T.Thermodynamics of
information.Nature physics 11, 2 (2015), 131–139.
44.Patterson, R., Operskalski, J. T., and Barbey, A. K.Motivated explana-
tion.Frontiers in human neuroscience 9(2015), 559.
45.Pilgrim, C., Sanborn, A., Malthouse, E., and Hills, T. T.Confirmation
bias emerges from an approximation to bayesian reasoning.Cognition 245(2024),
105693.
46.Prat-Ortega, G., and de la Rocha, J.Selective attention: A plausible mech-
anism underlying confirmation bias.Current Biology 28, 19 (2018), R1151–R1154.
47.Priniski, J. H., Solanki, P., and Horne, Z.A bayesian decision-theoretic
framework for studying motivated reasoning, Oct 2022.
48.Salamon, P., Andresen, B., Nulton, J., Roach, T. N., and Rohwer, F.
Morestagesdecreasedissipationinirreversiblestepprocesses.Entropy 25,3(2023),
539.
49.Sennesh, E., and Ramstead, M.An affective-taxis hypothesis for alignment
and interpretability.arXiv preprint arXiv:2505.17024(2025).
50.Sharot, T., and Garrett, N.Forming beliefs: Why valence matters.Trends
in Cognitive Sciences 20, 1 (2016), 25–33.
51.Sharot, T., Rollwage, M., Sunstein, C. R., and Fleming, S. M.Why and
when beliefs change.Perspectives on Psychological Science 18, 1 (2023), 142–151.
52.Shenhav, A.The affective gradient hypothesis: An affect-centered account of
motivated behavior.Trends in Cognitive Sciences(2024).
53.Simon, D., and Read, S. J.Toward a general framework of biased reasoning:
Coherence-based reasoning.Perspectives on Psychological Science 20, 3 (2025),
421–459.
54.Simon, H. A.Theories of bounded rationality.Decision and Organization(1964),
161–176.
55.Spelman, T., Elnakouri, A., Kteily, N., and Finkel, E. J.Overestimating
thesocialcostsofpoliticalbeliefchange.Journal of Experimental Social Psychology
105(2023), 104115.
56.Talluri, B. C., Urai, A. E., Tsetsos, K., Usher, M., and Donner, T. H.
Confirmation bias through selective overweighting of choice-consistent evidence.
Current Biology 28, 19 (2018), 3128–3135.
57.Vasil, J., Badcock, P. B., Constant, A., Friston, K. J., and Ramstead,
M. J. D.A world unto itself: Human communication as active inference.Frontiers
in Psychology 11(2020), 417.
58.Veissière, S. P. L., Constant, A., Ramstead, M. J. D., Friston, K. J.,
and Kirmayer, L. J.Thinking through other minds: A variational approach to
cognition and culture.Behavioral and Brain Sciences 43(2020), e90.
59.Wainwright, M. J., Jordan, M. I., et al.Graphical models, exponential
families, and variational inference.Foundations and Trends®in Machine Learning
1, 1–2 (2008), 1–305.
60.Westerwick, A., Kleinman, S. B., and Knobloch-Westerwick, S.Con-
firmation bias in online searches: Impacts of selective exposure before an election
on political attitude strength and shifts.Journal of Communication 67, 4 (2017),
660–684.
61.Williams, D.Socially adaptive belief.Philosophical Studies 178, 3 (2021), 785–
804.
62.Wolpert, D. H.The free energy requirements of biological organisms; implica-
tions for evolution.Entropy 18, 4 (2016), 138.
On the Variational Costs of Changing Our Minds 19
63.Wolpert, D. H., Korbel, J., Lynn, C. W., Tasnim, F., Grochow, J. A.,
Kardeş, G., Aimone, J. B., Balasubramanian, V., De Giuli, E., Doty, D.,
et al.Is stochastic thermodynamics the key to understanding the energy costs
of computation?Proceedings of the National Academy of Sciences 121, 45 (2024),
e2321112121.
64.Zhu, J.-Q., Sanborn, A., Chater, N., and Griffiths, T.Computation-
limited bayesian updating. InProceedings of the Annual Meeting of the Cognitive
Science Society(2023), vol. 45.
20 D. Hyland and M. Albarracin
A Analytical Solution for Optimal Belief Updates in the
Linear Affective Utility Case
In this appendix, we derive the closed–form optimal posterior that minimises the
variational objective introduced in 3. Throughout, letSbe a finite set of latent
statess∈S,q(s)the candidate posterior, andp(s)the fixed prior. Observed
data are denoted byowith likelihoodp(o|s). The objective functional to be
minimised is
F[q(s), o] :=U[q(s), o]| {z }
affective utility
+αE q

logp(o|s)

| {z }
accuracy
−λ DKL [q(s)||p(s)]| {z }
complexity
.(6)
HereU[q(s), o]represents theaffectiveutility of a belief stateq(s)and obser-
vationo, andα, λmodulate respectively the weight assigned to the epistemic
evidence and the inertia (or cost) of deviating from the prior.
For the case of linear affective utilities, we have
U[q(s), o] =
X
s∈S
cs q(s),(7)
with coefficientsc s ∈Rcapturing the valence of believing states.
We maximise (6) under the normalisation constraintP
s q(s) = 1. Introducing
a Lagrange multiplierη∈Rgives the augmented Lagrangian
L(q, o, η) =
X
s∈S
cs q(s) +αEq[logp(o|s)]−λ D KL [q(s)||p(s)] +η

1−
X
s
q(s)

.
(8)
Stationarity with respect to eachq(s)yields
0 = ∂L
∂q(s) =c s +αlogp(o|s)−λ
h
1 + logq(s)
p(s)
i
−η.(9)
Solving (9) forq(s)and exponentiating, we obtain
q(s) =p(s) exp
1
λ

cs +αlogp(o|s)−η

−1

(10)
∝p(s) exp
h
1
λ
 
cs +αlogp(o|s)
i
.(11)
Normalising with the partition function
Z(o) :=
X
s′∈S
p(s′) exp
h
1
λ
 
cs′ +αlogp(o|s ′)
i
,(12)
we arrive at the optimal variational posterior
q⋆(s) =p(s) exp

λ−1 
cs +αlogp(o|s)

Z(o) .(13)
On the Variational Costs of Changing Our Minds 21
B Additional Figures
Fig.6: Heatmaps depicting the variational objective and final belief landscapes for
different(λ, α)pairs. The upper two panels depict the objective and belief landscapes
(left and right, respectively) for evidence in the form of a likelihood wherep(o|s= 0) =
0.3, and the bottom two represent the same but for evidencep(o|s= 0) = 0.7. For
disconfirmatory evidence (top),
22 D. Hyland and M. Albarracin
Fig.7: Contoured heatmaps depicting various quantities as we vary both the conser-
vatism parameterλand the likelihood weight parameterαin Scenario 1 of wour second
experiment (Section 4.2). Top left: heatmap of the objective landscape under confir-
matory evidence, i.e., evidence that aligns with the agent’s desired belief. Top right:
heatmap of the objective landscape under contradictory evidence, i.e., evidence that
contradicts the agent’s desired belief. Bottom left: heatmap of the final beliefq(s= 0)
after evidence selection. The white line depicts the boundary at which the agent se-
lects Evidence A (left of boundary) over Evidence B (right of boundary). Bottom right:
heatmap showing the difference in the objectives for confirmatory and contradictory
evidence. The black line again depicts the boundary between choosing Evidence A over
Evidence B.