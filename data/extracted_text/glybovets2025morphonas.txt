MorphoNAS: Embryogenic Neural Architecture
Search Through Morphogen-Guided Development
Mykola Glybovets, Sergii Medvid
Faculty of Informatics
National University of Kyiv-Mohyla Academy, Ukraine
{glib, smedvid}@ukma.edu.ua
Abstract—While biological neural networks develop from com-
pact genomes using relatively simple rules, modern artificial
neural architecture search methods mostly involve explicit and
routine manual work. In this paper, we introduce MorphoNAS
(Morphogenetic Neural Architecture Search), a system able to
deterministically grow neural networks through morphogenetic
self-organization inspired by the Free Energy Principle, reaction-
diffusion systems, and gene regulatory networks. In MorphoNAS,
simple genomes encode just morphogens dynamics and threshold-
based rules of cellular development. Nevertheless, this leads to
self-organization of a single progenitor cell into complex neural
networks, while the entire process is built on local chemical
interactions.
Our evolutionary experiments focused on two different do-
mains: structural targeting, in which MorphoNAS system was
able to find fully successful genomes able to generate predefined
random graph configurations (8–31 nodes); and functional per-
formance on the CartPole control task achieving low complexity
6-7 neuron solutions when target network size minimization
evolutionary pressure was applied. The evolutionary process
successfully balanced between quality of of the final solutions
and neural architecture search effectiveness. Overall, our findings
suggest that the proposed MorphoNAS method is able to grow
complex specific neural architectures, using simple developmental
rules, which suggests a feasible biological route to adaptive and
efficient neural architecture search.
I. I NTRODUCTION
Biological brains are not assembled region-by-region, it
undergoes development stages using a compact genomic
blueprint. Tony Zador’s genomic bottleneck hypothesis [1]
implies, that the genome, likely, does not encode a full
diagram of the brain, rather a compressed cluster of develop-
ment instructions; mostly rules, constraints, and the conditions
these rules must satisfy. This results in the evolution of
efficient and flexible architectures that are far more adaptable
than existing artificial systems.
Moreover, Zador suggests [1] that the general machine
learning community has largely deviated from the original
biological inspiration. Zador argues that young animal brains
are capable of effective learning without enormous numbers
of labeled examples used by both supervised and unsupervised
machine learning nowadays, and points to another important
implication of genomic bottleneck: it suggests a path toward
artificial neural networks capable of rapid learning [1].
The Free Energy Principle (FEP) , introduced by Karl
Friston [2], brings theoretical foundation, extending Zador’s
idea by arguing that all living systems, from cells to brains,
must minimize variational free energy. This results in emer-
gent self-organization into predictable, low-entropy states via
perception (inference) and action (development or adapta-
tion). In morphogenesis [3] context, this results in embryonic
structures emerging from local interactions , driven by mor-
phogens dynamics and gene regulatory networks (GRNs) .
By merging these principles into an evolutionary computa-
tional system, it becomes possible to evolve neural architec-
tures not as explicit graphs, but as embryogenically grown
phenotypes, ruled by a compact genome which encodes
morphogen-based rules and is guided by FEP-inspired self-
organization. This implies bottom-up approach to building
cybernetic organism, a novel artificial neural network in this
case, complementing traditional top-down approaches from
neuroscience and cybernetics [4].
In this work, we introduce an evolutionary computational
system, MorphoNAS, capable of generating neural networks
through morphogenetic self-organization. The developmental
model utilizes the Free Energy Principle, reaction-diffusion
systems, and gene regulatory networks.
This approach was supported based on experiments we per-
formed in two domains: the generation of neural networks with
pre-defined topological properties (graph targeting), where all
randomly generated configuration targets were achieved in
100% of cases; and functional control task in CartPole environ-
ment, where minimal but capable neural network architectures
were evolved under evolutionary pressure involving network
size constraints.
We hypothesize that complex neural architectures capable of
solving tasks may emerge from simple, biologically relevant,
developmental rules. We provide examples of how networks
grown using the MorphoNAS method can be applied to real
functional optimization tasks.
These results suggest that the proposed evolutionary-
developmental approach has potential as a generic implemen-
tation for automated neural architecture search, suitable for a
wide range of applied domains.
II. T HEORETICAL FOUNDATIONS
A. Embryogenesis and FEP
Embryogenesis – the process by which a fertilized egg de-
velops into a complex multicellular organism – is a remarkable
example of self-organization in biology. John Gurdon won
Nobel Prize for demonstrating that all cells in a multicellular
arXiv:2507.13785v1  [cs.NE]  18 Jul 2025
organism have the same genome [5]. Still, somehow, trillions
of cells self-organize into complex organs, including brain,
from a single fertilized cell. This is a striking example of the
emergence principle [6], [7]. Understanding how organisms
reliably establish their body plan and form during embryonic
development remains a grand challenge in developmental
biology [4].
1) Free Energy Principle (FEP): In [2], Karl Friston
introduced concept of free energy, which he derives from
Expectation Maximization (EM) algorithm [8]. An important
conclusion is its connection to physics: the EM algorithm
maximizes function that corresponds to negative free energy
in physics [2]. In a later work [9], Friston and colleagues
proposed FEP as a framework to explain how living systems
self-organize and maintain low entropy by minimizing free
energy or “surprise”. Originally proposed as a brain theory
[10], the FEP is now being applied beyond neuroscience to
biological pattern formation and embryogenesis [11] [2], spe-
cific physical models [12], and even generic quantum systems
[13]. Moreover, Karl Friston argues that formation of life itself
is an inevitable emergent property of any random dynamical
system basing this claim on a simulation of primordial soup
[14].
The FEP states that any self-organizing system at steady-
state with its environment must minimize its internal varia-
tional free energy [10], which is a measure of the difference
between the system’s model of the world and its sensory
inputs. Organisms avoid surprising states and tend toward con-
figurations that match their expectations (i.e. they minimize
surprise [10] or prediction error). By doing so, the system
resists disorder and maintains its integrity, actively violating
the second law of thermodynamics [10]. This principle has
been described by some authors as a “survival principle” for
living organisms [15], since minimizing informational free
energy is mathematically equivalent to maximizing the evi-
dence for the organism’s internal model of its environment. In
the context of embryogenesis , this suggests that developing
cells might actively drive themselves toward more predictable,
lower-surprise anatomical states.
2) Entropy Control in Development: The FEP involves
entropy minimization (or control) because minimizing sur-
prise constrains the system’s states to a limited set of states.
In his book, What is life , Schr ¨odinger famously noted that
“living matter evades the decay to equilibrium” by “feeding
on ‘negative entropy”’ [16, pp. 74–75]. FEP provides a formal
explanation of this idea by formulating how organisms resist
entropy through constant information exchange and update of
their internal state. In the context of embryogenesis, theoret-
ical models suggest [10] that as development proceeds, the
system’s entropy is kept within bounds. A developing embryo
actively limits external randomness and organizes its internal
state along reliable scenarios – a perspective that unifies
developmental robustness with principles of self-organization.
B. Reaction-Diffusion Systems through the FEP Lens
Reaction-diffusion (RD) models , first proposed by Alan
Turing [3], show how interacting diffusive chemicals (mor-
phogens) can spontaneously break symmetry and form spatial
patterns. These patterns emerge from morphogens’ dynamics,
mutual interaction and inhibition, a purely bottom-up mecha-
nism. When viewed through the FEP lens, morphogen gradi-
ents and RD dynamics can be interpreted as part of the sensory
and signaling landscape that cells use to infer their location.
So, in a FEP context, morphogens are not just passive chemical
cues but are actively regulated signals that cells expect to sense
if the pattern is correct [17]. Each cell has beliefs about the
morphogen concentrations it “should” detect given its position
in the embryo, and it can act (e.g. by secreting or absorbing
morphogens, moving, or changing receptor expression) to
reduce any deviation from that expected profile [17]. As a
result, what looks like a Turing pattern may also be seen as an
emergent outcome of active inference: cells receive morphogen
signals (sensory data) and take actions to drive those signals
toward an expected pattern, minimizing free energy along the
way.
Another study highlighting the link between RD models
and FEP comes indirectly from neuroscience. Atasoy et al.
showed [18] that reaction-diffusion systems can generate a
wide range of biologically relevant patterns, such as those
seen in morphogenesis or brain activity, through simple in-
teractions between spreading excitation and inhibition. These
patterns emerge naturally from the system’s geometry and the
relative rates of diffusion, without the need to have detailed
assumptions or fine-tune parameters for each specific pattern.
This principle is further developed in another fundamental
neuroscience study by Safron [19] which directly maps it to
FEP.
By analogy, in embryogenesis, the chemical pre-patterns
of morphogens might be seen as solutions to a variational
principle (extremizing some information metric or minimizing
a potential). Indeed, Zhang et al show that thermodynamic en-
ergy dissipation enhances spatial accuracy and robustness of
Turing patterns [20], broadening the parameter ranges where
patterns form reliably. This suggests an energetic optimality
playing role in natural pattern formation. The FEP reframes
it in informational terms: an embryo’s RD-mediated pattern is
robust because it is stationary with respect to a free-energy
functional [11]. The authors directly relate it to the least
action principle, which is itself a classic example of variational
principle. In summary, viewing reaction-diffusion systems
through the FEP lens provides a goal-directed interpretation
of morphogenetic patterning – morphogens become part of
an inferential feedback loop ensuring that spatial chemical
distributions converge to those predicted by the collective
developmental program.
C. Gene Regulatory Networks (GRNs)
Such collective developmental programs are realized bio-
logically through Gene Regulatory Networks (GRNs) which
interpret and respond to signals defined by morphogen con-
centrations, governing cell fate decisions. Traditionally, GRNs
are modeled by regulatory logic circuits [21] or differential
equations [22] capturing transcription factor interactions and
morphogen inputs. The FEP perspective offers a higher-level
interpretation: GRNs implement the generative model inside
each cell, mapping environmental signals (e.g. morphogen
concentrations) to internal states (gene expression, which mod-
ulates morphogen secretion) in a way that maximizes the cell’s
certainty about its identity or position [4]. In other words, one
can view a GRN as a Bayesian network that encodes prior
expectations of gene expression patterns appropriate for each
position in the embryo, and that updates expression based on
actual morphogen inputs to minimize discrepancy. Friston et
al. [4] explicitly suggested that genetic codes parameterize
a generative model predicting the signals a cell should
sense in the correct morphology. Epigenetic and signaling
processes then perform inference – adjusting gene activity
until the cell’s “beliefs” (gene expression profile) match the
local signals, reaching a stable fate [4]. This approach views
cell differentiation as a Bayesian optimization process rather
than a hardwired behavior directly encoded in the genome.
Instead, the genome provides a solution space of possible
patterns, and the intercellular interactions (via morphogen
signaling and GRNs) navigate that space to find the pattern
with minimal free energy (which corresponds to maximum
consistency between genes and signals).
A remarkable example of linking GRNs with an optimiza-
tion principle is the recent work by Sokolowski, Tka ˇcik,
Bialek, and Gregor [23]. They approached the classic gap
gene network in the early Drosophila (fruit fly) embryo – a
well-studied GRN that reads a maternal morphogen gradient
(Bicoid) and establishes segmental gene expression stripes.
Instead of manually fitting this network, they asked: Can
the GRN be derived from an optimality principle? Using
an information-theoretic approach, they optimized a detailed
model of the gap gene system’s 50 parameters to maximize
the positional information that the gene expression levels
provide about nuclear position in the embryo [23]. Remark-
ably, the optimized network spontaneously recapitulated
both the regulatory architecture and the spatial expression
patterns of the real embryo’s gap genes. The authors rationalize
[23] how the known Drosophila GRN emerges as the solution
to a variational problem here maximizing information, which
is conceptually related to minimizing uncertainty (free energy).
This result strongly supports the idea that developmental
GRNs are shaped by and operate under optimality principles.
D. Key Mappings from Theory to Model
Based on the theoretical foundations described above, let
us now introduce a computational model that implements
morphogenetic development using self-organizing local inter-
actions to develop neural architectures. The key mappings
from theory to the proposed model are the following:
1) Free Energy Principle: The behavior of the individ-
ual cells is driven by ”surprise” or variational free energy
minimization, corresponding to the fate decisions (division,
differentiation, and axon growth) a cell must make in reaction
to its local morphogenetic environment. The cells operate to
achieve homeostasis or predictability in their morphogenetic
locality, which is the source of the action(s) they take that are
solely dependent on local morphogen concentrations with the
decisions implemented as threshold-based transitions.
2) Reaction-Diffusion Systems: The spatial distribution and
dynamics of morphogens is modeled with a reaction-diffusion
process. A morphogen is released by a cell, which then dif-
fuses throughout the developmental field, potentially inhibiting
other morphogens. Together, this creates concentration gradi-
ents which provide positional information for the cells. The
morphogen concentration gradients reflect Turing patterns and
are responsible for symmetry breaking and spatial organization
[3].
3) Gene Regulatory Networks (GRNs): The genome G
represents the parameters that encode morphogen secretion
rates, diffusion profiles, inhibition interactions, and cellular
response thresholds. This is corresponding to the regulatory
logic of GRNs which control how cells deal with morpho-
genetic signals and their responses.
III. M ORPHO NAS F RAMEWORK
The MorphoNAS framework presented in this paper is based
on the following four components: progenitors, which are
similar to stem cells, can divide and become differentiated;
neurons, which are differentiated cells capable of growing
axons and forming synapse-type connections; morphogens,
chemical fields that direct cell growth, behavior, and decision
making via concentration fields; and axon growth , which
acts in a chemotactic-type manner, having axons grow in the
direction of increasing morphogen concentration, representing
a simplified form of neuronal connectivity.
A. Morphogenetic Development Model
MorphoNAS can be defined as a framework to simulate
neural architectural formation via biologically inspired devel-
opmental processes. The structure is defined formally as
MorphoNAS = (G, Λ, C, M, S, D, I, T)
where each of the components captures an integral aspect of
the morphogenetic developmental process:
Developmental Field ( Λ) is the spatial environment in
which the neural growth occurs:
Λ = {(i, j) | 1 ≤ i ≤ Lx, 1 ≤ j ≤ Ly}
adopting a toroidal topology (connecting opposite edges),
serving to model continuous spatial development.
C denotes the cell types : C = {Progenitor, Neuron}. Pro-
genitor cells are defined as undifferentiated cells that can
divide and differentiate to form neurons. Neuron cells are
defined as differentiated cells that can grow axons, and make
synapse-like connections.
Genome (G) encodes the developmental program as a
compact set of morphogenetic rules, which include: Gdim =
(Lx, Ly) ∈ N2, describing the spatial dimensions of the
developmental field Λ; Gmorph, which defines the morphogen
specifications – each specification describing the secretion
rates for the appropriate cell type, the diffusion properties
of each morphogen (encoded as a tensor), and the parame-
ters concerning cross-inhibition; Gfates describes the threshold
values that govern progenitor cell division, progenitor cell
differentiation, and neuron axon growth; Gaxon, describes the
axon extension parameters to be taken – driving the connection
thresholds, and maximum lengths of axons; and Giter ∈ N, that
represents development time – in discrete time steps.
Morphogens ( M) comprise chemical signals that guide
development: M = {m1, m2, . . . , mk}, and diffuse spatially
across the developmental field, providing positional informa-
tion.
Cellular State ( S): S : Λ → R|M| × (C ∪ {∅}) defines
local morphogen concentrations, and a resident cell (if any),
for each location.
Diffusion Dynamics ( D): D : M →R(m(µ)×n(µ)) defines
how each morphogen µ ∈ Mdiffuses through the develop-
mental field, using diffusion tensors.
Inhibition Dynamics ( I): I : M × M →R defines the
cross-inhibition of morphogen behaviour, which can lead to
complex patterns through competitive interactions.
Developmental Dynamics ( T ): T : S(Λ) → S(Λ) defines
the temporal change of the system, implementation of the self-
organizing growth process at each time-step.
An example of neural morphogenesis using the MorphoNAS
system is shown in Figure 1. It demonstrates the self-
organizing development of a neural network on a 10 × 10
developmental field after 2, 10 and, 200 simulated time steps
resulting in 10 neurons and 36 connections. The gradients in
the background of these simulations reflect the morphogen
landscape achieved by the superposition of three chemicals.
Fig. 1. Example of neural morphogenesis in the MorphoNAS system.
The corresponding neural network structures at each stage
of development are shown in Figure 2. At Step 2, the neural
structure contains a single progenitor cell (not shown in the
plot) and a single neuron. At Step 10, there are 4 neurons
and 6 connections in total. There is also an isolated neuron
disconnected from the cluster in the central region of growth
field. By Step 200, there are 10 neurons and 36 connections
developed.
Fig. 2. Neural network structure at different stages of growth.
IV. M ORPHOGENETIC DEVELOPMENT DYNAMICS
A. Initialization
At time t = 0 , the developmental field Λ is initialized
with minimal structure: a single progenitor cell is placed at
position (i0, j0), like a neural stem cell; and all morphogen
concentrations are set to zero in the entire developmental field.
B. Iteration Steps
At each iteration t ∈ 0, 1, . . . , Giter − 1, the following
sequence is executed:
1) Morphogen Secretion : Cells release chemical signals
into their local environment. For a cell of type c at
position (i, j), morphogen concentrations are updated
as: cm(i, j) ← cm(i, j) + σ(m)
c , where σ(m)
c is the
secretion rate of morphogen m for cell type c.
2) Diffusion: Morphogen concentrations are updated based
on diffusion matrices according to one of several formu-
lations typically using convolution or a numerical inte-
gration scheme (e.g., discrete Laplacian approximation):
cm(i, j) ← DiffusionStep(cm, D(m))
3) Inhibition: Local interactions will modify concentra-
tions according to the inhibition rules: cm(i, j) ←
cm(i, j) Q
n̸=m(1 − αmncn(i, j)), where αmn =
I(m, n) is the inhibition coefficient of morphogen n
inhibiting morphogen m.
4) Cell Fate Decisions : Each cell will evaluate its local
morphogen environment in order to assess its devel-
opmental fate. When progenitor cells need to make
decisions, they can exhibit three possible behaviors.
They can (1) divide, in this case producing a new
progenitor cell in an immediate neighbouring position;
this happens if the concentration of morphogens that
induce cell division exceeds a defined threshold; or (2)
differentiate into a neuron, if the concentration of mor-
phogens that are responsible for differentiation exceeds
another predefined threshold; or (3) continue being in
their current state if no cells or differentiation need to
occur. Neuron cells will follow the morphogen gradients
in the developmental environment to extend out axons
towards neighbouring cells (other neurons) based on the
chemical signals in their local environment.
5) Axon Growth and Connection : Axons grow in a step-
wise fashion, following the gradient ascent in mor-
phogen fields. When a growing axon encounters another
neuron it will form a stable connection if the local mor-
phogen concentration exceeds the connection threshold
θaxon ∈ Gaxon.
6) Weight Assignment: When the connection is formed, an
initial synaptic weight w is assigned from morphogen
concentration c at the target neuron and the distance
d between the source neuron and target neuron. The
weight calculation is:
w = max

0.01, c
1 + d

The weights range between 0.01 and 1.0. The minimum
weight of 0.01 ensures that the edge exists, while a weight of
0 indicates that no edge exists in the model.
Once edges are established, weights can change based on
competitive scaling of the local concentrations of morphogens.
Let clocal be the concentration of morphogen at the location of
a neuron, while ctotal is the total concentration of morphogen
in the local neighborhood of this neuron. The new weight can
be calculated as:
wnew = max

0.01, clocal
ctotal

This creates a competitive situation where even relatively
distant neurons with a high density of the axon-guiding
morphogen will create more connections to other neurons than
close neurons with a low density of morphogen.
C. Termination
This will continue for Giter iterations until the developmental
process is terminated.
Thus, the MorphoNAS system simulates the neural develop-
ment over discrete time steps; where each location in the devel-
opment field contains morphogen concentrations and possibly
a cell (either progenitor or neuron). The system evolves step-
wise and continues the procedure through secretion, diffusion,
inhibition, cell fate decisions, and axon growth, according
to local interactions and development rules encoded in the
genome. Beginning with only one progenitor cell, the system
self-organizes into a neural network through morphogen and
chemical gradient signaling. The complete process is detailed
in Algorithm 1.
V. E VOLUTIONARY TARGETING OF SELECTED GRAPH
PROPERTIES
Having formally defined the MorphoNAS framework, pa-
rameterized with RD system characteristics and a set of
simple GRN rules triggered solely by morphogen thresholds
applied locally, let’s assess its potential to produce graphs with
predefined topological properties.
For this assessment a full isomorphic equality to a target
graph is not feasible. First, despite recent developments, graph
isomorphism is still a difficult computational problem [24].
Algorithm 1: Morphogenetic Development Dynamics
Data: Initial developmental field Λ with a single
progenitor cell; genome G
Result: Final developmental state and neural network
1 Initialize all morphogen concentrations to zero;
2 for each time step t = 1 to Giter do
3 for each cell c in Λ do
4 Secrete morphogens at c according to genome
rules;
5 for each morphogen m do
6 Diffuse m across Λ using diffusion matrix
D(m);
7 for each position (i, j) do
8 Apply inhibition between morphogens at (i, j);
9 for each cell c in Λ do
10 if c is a progenitor then
11 if division conditions are met then
12 Divide progenitor and place new
progenitor in adjacent position;
13 else if differentiation conditions are met
then
14 Differentiate progenitor into neuron;
15 else if c is a neuron then
16 Grow axon toward local morphogen
gradient;
17 if axon reaches another neuron and
connection threshold is satisfied then
18 Form connection and initialize synaptic
weight;
19 return final developmental state and neural network
Second, biological systems rarely evolve toward exact single
structure [25], rather exploring general properties that ensure
functionality and robustness [26].
While in the proposed MorphoNAS one cell naturally self-
organizes into complex graph-like structures, each genome
deterministically defines a single graph. Evolutionary Algo-
rithms (EAs) offer a powerful framework to explore the space
of possible genomes, which will let us select for those that
guide MorphoNAS process to form graphs that meet specific
properties.
A. Problem definition
To evaluate the generative capacity of MorphoNAS without
relying on exact graph matching (isomorphism), let’s define a
toy problem that specifies the following global properties of
the resulting graph:
• Number of nodes : The resulting graph should contain
exactly Ntarget nodes (neurons)
• Number of edges : It should contain exactly Etarget edges
(synaptic connections)
• Number of sources : another easy-to-count property of
the target graph is number of nodes with zero in-degree;
the number of such nodes should match Starget
While the toy problem is limited to these 3 properties,
other potential metrics could be explored in future research.
Examples of such problems solvable in polynomial time com-
pared to more complex graph isomorphism problem include
mean shortest path length, graph density, presence of a giant
component, and others.
B. Fitness Function Design
Now we can define a fitness function that will represent
how closely the grown graph matches the target properties:
F(G) = exp

−wN · |N(G) − Ntarget|
tolN

× exp

−wE · |E(G) − Etarget|
tolE

× exp

−wS · |S(G) − Starget|
tolS

× P(G)
where N(G), E(G), S(G) are the actual numbers of nodes,
edges, and sources; tol N , tolE, tolS are tolerances that specify
the degree of deviation; wN , wE, wS are weights indicating
how much importance to place on each term.
P(G) is a penalty for lack of weak connectivity defined as
P(G) =
(
1, if G is weakly connected
γ ≤ 1, otherwise
where γ is a penalty factor, e.g. .1 or .5; if γ=1 then there
is no penalty.
Some key features of the fitness function are: it returns a
value of 1 when there is an exact match to target values; small
deviations result in small reductions in fitness; as deviations
increase, the penalties catch up exponentially. Finally, if the
graph is not weakly connected, its fitness is multiplied by the
factor γ making it more difficult for disconnected graphs to
survive
C. Genetic Algorithm Setup
For the experiment setup, a steady-state genetic algorithm
[27] is employed.
The initial population consists of P individuals, each
having a genome set up as defined in chapter III-A. All
initial individuals define developmental field of the same size
(Lx, Ly), the same number of morphogens NM, and the same
maximum of Giter CA growth iterations. A fixed random seed
ensured reproducibility of initial population.
Parent selection is being performed using tournament
selection [28] with a tournament size of Tsize. The algorithm
selection pressure is set to psel for the entire run. As part
of steady-state setup, Preplace individuals get replaced in each
generation. Elitism [27, p. 101] is used to preserve top Pelite
individuals without modification.
Algorithm 2: Genetic Algorithm for Evolving Mor-
phoNAS
Data: Initial population of P genomes with fixed
random seed
Result: Best evolved genome matching target graph
properties
1 Initialize population with random genomes;
2 Evaluate fitness of all individuals in parallel;
3 while generation < Gmax and not converged do
4 Select Pparents parents using tournament selection
(tournament size Tsize);
5 Compute convergence ratio ρ = favg/fbest;
6 Update mutation multiplier
µ = µmin + (µmax − µmin) × ρ;
7 for Preplace offspring do
8 Randomly select two parents;
9 if parents have same morphogen count then
10 Inherit simple parameters randomly;
perform element-wise crossover on
secretion rates; perform row-wise
crossover on inhibition matrix; inherit
diffusion patterns from one parent;
11 else
12 Randomly choose one parent’s morphogen
count and associated parameters;
randomly inherit other parameters;
13 With probability pmut × µ, apply mutation;
14 if mutation occurs then
15 Select mutation type according to
probability vector vmut: grid size mutation
(±3), growth steps mutation ( ±20),
parameter mutation (90% scale float by
[0.5, 2.0], 10% increment integer by ±2),
morphogen number mutation ( ±2), or
matrix mutation (modify inhibition or
diffusion patterns), 5% chance of radical
mutation
16 Preserve Pelite elite individuals;
17 Replace Preplace individuals with new offspring;
18 Evaluate fitness of all new individuals (use fitness
cache);
19 Update best fitness and convergence statistics;
20 return Best evolved genome
Crossover applies to the selected parents by randomly
combining simple parameters from each genome. Morphogen
secretion rates are mixed using element-wise crossover, inhibi-
tion matrices are recombined row-wise, and diffusion matrices
are inherited in whole from one parent. When parents differ
in morphogen count, the entire morphogen-specific parameters
set is inherited by each offspring from one randomly selected
parent, keeping other parameters mixed from both parents.
Mutation is applied with pmut probability to each produced
offspring, selected randomly according to vmut probabilities
vector from five mutation types: developmental field size mu-
tation, growth steps mutation, parameter mutation, morphogen
number mutation, and matrix mutation. Parameter mutations
target either floating-point parameters, scaling them by a
random factor between 0.5 and 2.0 (90% of cases), or integer
parameters, changing them by ±1 (10% of cases).
A ”radical mutation” with 5% chance is also introduced:
when it occurs, the number of morphogens set to any in
[3..13] range which results in all morphogen-related param-
eters randomized; the Giter growth steps set to random ones
in [100..1000] range.
An adaptive mutation strategy was used for balancing
exploration and exploitation. The effective mutation rate was
scaled dynamically according to the population’s convergence
ratio, defined as the average fitness divided by the best fitness.
The mutation rate multiplier µ was computed as follows:
µ = µmin + (µmax − µmin) × ρ
where µmin and µmax are the minimum and maximum
scaling factors, respectively, and ρ represents convergence
ratio defined as the ratio of the average fitness favg to the
best fitness fbest in the population: ρ = favg
fbest
.
In each experiment run, the evolution process was running
for up to Gmax generations or until convergence.
D. Assessing the Suggested Evolutionary Framework
To assess the suggested evolutionary framework, we will
define K target properties sets {Ntarget, Etarget, Starget} to define
the desired number of neurons, edges, and sources per set of
target properties.
The target properties are created by sampling of K random
directed graphs created with Erd ˝os-R´enyi G(n, m) method
[29] with a fixed seed target. The n and m parameters are
sampled randomly while ensuring there is a minimum average
outdegree ¯kout ≥ kmin and a maximum density of dmax.
For each target properties set, R independent evolutionary
runs were performed, each initialized with a different ran-
dom seed, passed to Algorithm 2 represented in pseudocode
above. For the seeding process we create a sequence sruns =
{s1, s2, . . . , sR}, built with another fixed seedruns to allow each
of the K target properties set reuse this same sequence. Each
of the evolutionary runs will be executing until a maximum
number of generations Gmax is reached, or until the fitness
threshold has been achieved, or until the average population
fitness has converged to at least 95% of the best fitness.
TABLE I
SUMMARY OF TARGETED GRAPH PROPERTIES EXPERIMENT SETUP
Initial Genome Parameters
Symbol Value Description
(Lx, Ly) (20, 20) Developmental field dimensions (width ×
height)
NM 3 Number of morphogens
Giter 200 Max growth iterations
Genetic Algorithm Parameters
P 2000 Population size
Tsize 7 Tournament size
Pparents 300 Parents per generation
Preplace 200 Replaced individuals
Pelite 10 Elite individuals preserved
Gmax 1000 Max number of generations
pmut 0.4 Baseline mutation probability
µmin 1.0 Minimum mutation multiplier
µmax 2.5 Maximum mutation multiplier
vmut
[0.15,
0.15,
0.45,
0.10,
0.15] Mutation type probabilities (grid size, steps,
param, morphogens, matrix)
prad 0.05 Probability of radical mutation
Experimental Setup Parameters
K 20 Number of target property sets
R 20 Evolutionary runs per target
kmin 1.5 Min avg. out-degree
dmax 0.3 Max graph density
seedtarget 54210 Seed for target graph generation
seedruns 65420 Seed for run initialization
Performance of each run is defined by the best fitness
recorded in that run, calculating the mean and standard de-
viation, while reporting the proportion of runs that achieve
the fitness threshold.
Table I contains the specific parameters for the experiment.
Table II shows the success rates of the evolutionary algo-
rithm on 20 different target configurations, where each config-
uration was defined by a triplet {Ntarget, Etarget, Starget}, defin-
ing the searched numbers of neurons, edges and source nodes.
Success rate is the number of runs (out of R) that achieved
a perfect fitness (the evolved graph specifically matched the
properties that we required in our target configuration for the
graph).
It is important to note that at least one valid solution was
found for each target configuration, indicating the generality
and robustness of our framework given a wide array of
structural constraints. Several target sets (K06, K17, K19)
did have lower success rates, but the algorithm still found a
suite of valid successful solutions. This means that while the
degree of search complexity has relatively increased, it has not
failed the representational capability of genes, i.e. it has still
been confirmed that these target configurations do have their
respective genomes, despite they were found harder to locate
for the EA. In addition, several configurations (K01, K05,
K07) reliably produced 100% of representations indicating
TABLE II
SUCCESS RATES ACROSS TARGET SETS SAMPLED FROM RANDOM GRAPHS .
Target Set Ntarget Etarget Starget Success Rate ( R = 20)
K01 11 30 1 1.00
K02 27 75 2 1.00
K03 14 42 1 0.95
K04 8 14 1 0.95
K05 22 92 1 1.00
K06 30 232 0 0.25
K07 31 124 1 1.00
K08 27 103 2 1.00
K09 19 87 0 0.95
K10 20 54 2 1.00
K11 10 18 1 0.80
K12 14 76 0 0.60
K13 9 35 0 1.00
K14 17 34 1 1.00
K15 18 119 0 0.80
K16 31 87 2 1.00
K17 23 223 0 0.40
K18 27 142 1 1.00
K19 28 189 0 0.35
K20 15 32 1 1.00
Giter: 215         Lattice Size: 20 x 15
     DM0                 DM1                D M2
0.062 0.350 0.000    0.046 0.181 0.049    0.168 0.159 0.055
0.154 0.068 0.075    0.141 0.093 0.051    0.100 0.159 0.055
0.005 0.105 0.181    0.170 0.096 0.174    0.019 0.117 0.169
    Inhibition:              Gfates:
  M0    M1    M2             Division:           0.599
M0 0.000 0.000 0.292          Differentiation:    0.570      
M1 0.000 0.000 0.000          Axon Growth:        0.278
M2 0.000 0.000 0.000
Secretion Rates:                        Gaxon:
 Progenitor: 0.429, 0.424, 0.104       Axon Connect: 0.753
 Neuron:     0.158, 0.441, 0.183       Max Axon Length: 10
Fig. 3. Genome of a successful individual for target K16 (R07).
the ease of convergence and the strength of the evolutionary
algorithm stabilizing in those configurations.
In figure 3, we present the genome G of a competent
individual that was evolved to target K16 (run R07); K16
specifies a graph with the following properties: 31 neurons, 87
edges, and 2 inputs. The small parameter set represents growth
dynamics, morphogen actions, cell fates and axon connectivity
in a 20 × 15 developmental field.
This genome defines three morphogens, each with three
distinct diffusion profiles: (DM0, DM1, DM2), secretion rates
for progenitor cells and neuron cells, and one non-zero cross-
inhibition term. Cell behaviours are defined by thresholds in
Gfates for division, differentiation, and axon initiation. Axonal
growth is managed by two parameters in Gaxon: probability of
connection, and maximum axon reach.
In Figure 4, we show the final grown graph state (step
215) in a 2D projection based on the toroidal topology of
the developmental field.
For such a low dimensional structure, this genome has
produced a valid offspring structure (neural graph) that
has functional compliance (i.e. complex K16 specification),
thereby illustrating the expressive power and generality of the
Fig. 4. Final state of the grown graph at step 215 for target K16 (R07), shown
in a 2D projection of the toroidal developmental field topology.
Fig. 5. Evolved graph for target K16 (R07)
evolutionary-developmental processes described here.
Figure 5 provides the connectivity structure of the evolved
graph that satisfies the K16 target specification. Each node
represents a neuron, each directed edge is associated with a
network axon connection. The two source nodes, shown at the
bottom of the image serve as input neurons to the rest of the
network and they have outgoing connectivity to other neurons
in the network. Overall, the topology is a complex but coherent
wiring defined solely by the developmental process and the
spatial and chemical constraints of the developmental field.
The graph is the direct result of the morphogen driven growth
process as illustrated in the previous figure in the toroidal
developmental field.
E. Discussion
The experiments showed that the proposed MorphoNAS
Framework is capable of achieving its fundamental goal of
defining simple RD- and GRN-only based genomes that yield
neural networks with precise and specific structural targets.
The fact that successful solutions were obtained across all
20 target configurations highlights not only the viability of
the methods incorporated into this process, but also their
generality for exploring different architectural constraints.
Evolutionary Effectiveness: The variability in success rates
across targets (from more complex targets such as K06, K17,
K19 compared to a flat out 100% successful solutions like
K01, or K05) is aligned with the inherent complexities of the
target specifications, and is not a fundamental limitation of
the framework. The fact that valid solutions could be found
across all target configurations suggests that the proposed
evolutionary-developmental framework has been able to ex-
plore the developmental programs effectively.
Developmental Expressiveness: The successful evolution
on the example of target K16 (31 neurons, 87 edges, 2
source nodes) demonstrates the expressiveness of the pro-
posed compact representation of a genome. The expressive
power emerges from a low-dimensional parameter represen-
tation of the morphogen dynamics, cellular behaviours, and
axonal growth rules, producing a reasonably complex network
through self-organized developmental processes. This suggests
that the proposed evolutionary-developmental principles, based
on biological systems, can be useful generative models for
neural architecture search.
Scalability Indicators: The ability of the framework to
tackle varied target specifications which differed significantly
in terms of not only scale, but also network connectivity den-
sity and structural complexity, suggest scalability is likely to
be a promising area. Additionally, the spatial organization that
emerges from morphogen-guided growth provides mechanism
able to create structured connectivity patterns, which could be
beneficial for more complex control tasks.
Methodological Foundation: These results provide evi-
dence that the proposed process is capable of consistently
converting high-level architectural specifications into working
developmental programs. This capability forms a method-
ological foundation for functional validation that will be
demonstrated in the next experiment described below, in which
the evolved networks must both develop to achieve target
structural properties and perform functional computations.
The broad success across target configurations confirms
that the proposed evolutionary-developmental framework can
be regarded as a viable mechanistic approach for automated
neural architecture generation, and is now ready to tackle more
advanced functional optimization problems.
VI. A PPLYING GROWN RNN S FOR PROBLEM SOLVING
Let’s describe how the graphs grown using the proposed
MorphoNAS framework can be applied to concrete problem-
solving tasks.
After completing Giter iterations, the morphogenetic process
yields a directed weighted graph G = (V, E, W), where: V
is the set of neurons (nodes); E ⊆ V ×V is the set of directed
edges (axon connections); and W : E → [0.01, 1.0] assigns a
synaptic weight to each edge.
This graph defines a Recurrent Neural Network (RNN)
structure on which it’s required to deterministically define
input and output neurons, applying propagation dynamics
afterwards.
A. Defining Input and Output Neurons
First, let’s define two sets of input and output neurons Vin ⊆
V and Vout ⊆ V . The number of such neurons is determined
by the dimensionality of the problem din and dout.
To make the neuron selection process deterministic, input
neurons are assigned as the first din nodes of the grown graph
sorted by nodes’ in-degree deg−(v):
Vin = {v1, v2, . . . , vdin } ⊂Vsorted,
where Vsorted = sort(V, by deg−(v)).
Independently, the last dout nodes are selected from the
original grown graph order of V :
Vout = {v|V |−dout+1, . . . , v|V |}.
The following constraints are applied in the process:
• din + dout ≤ |V |, so the emergent graph must be at least
size of the problem’s input plus output dimensions, and
• Vin ∩ Vout = ∅, i.e. input and output neurons do not
intersect.
B. RNN Propagation Dynamics
Now, we will formalize the flow of information over the
emergent directed graph. The graph is intended to be seen as
a Recurrent Neural Network (RNN), in which neurons act as
processing units and axon connections determine how infor-
mation flows among neurons in the network. The propagation
dynamics are modeled after [30]. The flow of information
progress through the network is discrete over a series of time
steps and can be computed with matrix multiplications along
with non-linear activation functions, over the graph structure.
Let
x0 ∈ R|V | be the initial state of activation of the neurons,
instantiated as follows:
• Inputs are injected into the appropriate input neurons
Vin ⊆ V , as defined above.
• All other neuron activations are initiated to the value of
zero.
W ∈ R|V |×|V | be the sparse weight matrix constructed
from synaptic weights W of the original graph:
Wij =
(
W(i, j) if (i, j) ∈ E
0 otherwise
f : R → R be the neuron activation function (take, for
example, tanh), which would be computed for each element.
Let us now define the propagation of the network over T
discrete time steps (where T is the diameter of G, and optional
additional timesteps are allowed):
For each time step t ≥ 0:
xt+1 = Update(xt, f(Wxt))
where Update can operate in either of two modes: accu-
mulation mode implies xt+1 = xt + f(Wxt) or replacement
mode implies xt+1 = f(Wxt).
This mode of propagation has several characteristics. First
of all, the process is completely deterministic and follows the
structure of the grown graph. Also, an important feature is that
the network can be run in a computationally efficient manner
on both CPUs and GPUs, considering the sparsity of the
dynamics. At the same time, the system supports both discrete
and continuous input and output modalities. All this leads to
the sustained state xt changing in relation to the network
topology and its weights, allowing for rich and expressive
dynamic behavior.
VII. E VOLVING RNN C ONTROLLERS FOR GYMNASIUM
ENVIRONMENT
A. Problem Setting
To confirm functional completeness, we will use the stan-
dard CartPole task from the Gymnasium environment [31] as
the most simple RL standard benchmark. In CartPole, the goal
is to balance a vertical pole on a moving cart by applying
a force to the left or right. The state of the system can be
described by 4 numbers: cart position, cart speed, pole angle,
and pole angular velocity. The agent can choose between just
two discrete actions: apply left force and apply right force.
The best (ideal) reward is 500 in this environment.
The recurrent neural network (RNN) grown with Mor-
phoNAS method will act as the cart controller. It accepts
environtment state at its input, using the 4 input neurons Vin,
and generates actions to stabilize the CartPole system using
the 2 output neurons Vout.
Fig. 6. CartPole from the Gymnasium environment
B. Fitness Function Design
Let’s define a fitness evaluation approach able to work with
any standard environments from the Gymnasium library [31].
The fitness function will determine how well a RNN grown
by MorphoNAS performs on a set of random Gymnasium
episodes. The performance a series of episodes is averaged
and this score is normalized to give consistent fitness function
outputs.
The evaluation procedure can be executed on Gymnasium
environments such as CartPole, LunarLander, MountainCar,
and others. Each environment defines a predefined passing
score which helps to perform normalization of the actual
received score into a fitness function value.
The environment defines an observation space O ⊆Rdin
and an action space A: either continuous Rdout or discrete
{0, 1, . . . , k− 1}dout .
The evaluation process will be defined for N independent
rollouts (episodes). For each rollout:
1) Input and output neurons are defined in the RNN ,
as described in section VI-A above. The network may
be so small it will trigger a fitness score of 0:
F(G) = 0, if din + dout > |V |
2) Inject observations. At time step t, the environment
provides observation ot ∈ O. The input neurons are
initialized as x0[vi] = ot[i], for each vi ∈ Vin while
all other neurons are set to zero.
3) Propagate. The network propagates as defined in section
VI-B: xt+T = Propagate(x0, W, T).
4) Extract an action. The action at is extracted by the
output neurons at[i] = xt+T [vi], for each vi ∈ Vout.
Then depending on whether the action space is con-
tinuous or discrete, some postprocessing may be done:
for discrete action spaces the actions are decided by
arg max or discretization, while for continuous action
spaces the activations should be scaled to the valid action
boundaries.
5) Step the environment. Apply action at, receive reward
rt, and get the next observation ot+1.
6) Accumulate rewards. Finally, the rewards are accumu-
lated across all steps in the episode:
R(i) =
T(i)
episodeX
t=0
r(i)
t
The average reward across Ner episode rollouts is:
¯R = 1
Ner
NerX
i=1
R(i)
The average reward ¯R is then converted to a fitness score
F(G) ∈ [0, 1] using a sigmoid transformation:
F(G) = 1
1 + e−k( ¯R−S) ,
where S is the passing score for the given environment and
k > 0 is the scaling factor that controls the slope of the
sigmoid.
C. Penalizing Excessive Networks
Let’s add an optional penalization to the fitness function so
we can penalize excessive networks. It will cause the fitness
to decrease down to the value of α (e.g. 0.8) as the number of
TABLE III
EXPERIMENTS PARAMETERS DIFFERENCE
Symbol Targeting Graphs RNN-Controller
RNN-Controller
(min)
F(G) TargetGraphFitness CartpoleFitness CartpoleFitness
(Lx, Ly) (20, 20) (10, 10) (10, 10)
P 2000 500 500
Pparents 300 100 100
Preplace 200 50 50
pmut 0.4 0.3 0.3
Excessive Networks Penalization
α – – 0.8
θC – – 50
θhalf – – 1000
connections increases. Let NC be the number of connections
in the RNN, θC be the maximum number of unpenalized
connections, and θhalf be the half-decay threshold (that is, the
point where the fitness falls halfway between 1 and α, i.e. to
0.9 if α = 0.8). The decay rate is defined as λ = ln(2)
θhalf−θC
.
The connection penalization factor can then be defined as
Pconn(NC) = α + (1 − α) × exp(−λ × max(0, NC − θC)).
And the penalized fitness function is then computed as
Feff(G) = F(G) × Pconn(NC).
This discourages graphs with more than θC edges as it
imposes a penalty on the fitness function which asymptotes
back towards α as the number of connections increase.
D. Genetic Algorithm Setup
The setup for the evolution of RNN solving Gymnasium
environments closely follows the parameterization described
in Table I for the Targeted Graph Properties experiment.
The main difference lies in the fitness function, where the
Gymnasium environment is employed as discussed in chapter
VII-B. Other parameter modifications are provided in Table III.
E. Evaluation of Results
For the first RNN-Controller experiment, a fixed seed,
65420, was used to generate sruns sequence of 100 seeds to
randomize each run. An independent evolutionary run was
launched for all seeds in sruns.
It is worth to note that the ideal fitness for the CartpoleFit-
ness function was found in the initial generation for 94 out of
100 runs; that is, no evolutionary steps were taken. The results
are summarized in Table IV, outlining number of generations
to reach optimal fitness in all runs.
In these 100 runs, the resulting RNNs contained fewer than
10 neurons in 8 cases, between 34 and 70 neurons in 3 cases,
and exactly 100 neurons in the remaining 89 cases. It is shown
on Figure 7.
For smaller search spaces, such as the Cartpole task from the
Gymnasium suite, the MorphoNAS-based generation process
typically yields RNNs achieving the ideal fitness with the max-
imum number of neurons, 100, for the developmental field of
TABLE IV
NUMBER OF RUNS REACHING THE IDEAL CARTPOLE FITNESS VALUE WITH
A POPULATION SIZE OF 500
Time To Ideal Fitness Number of Runs
Initial population (0 generations) 94
1 generation 3
4 generations 1
5 generations 2
Fig. 7. Sizes of RNN networks successfully operating CartPole found within
Experiment B1
10x10 size used in this experiment. For the Cartpole controller
RNN, large networks (over 90 neurons) were observed in 89%
of populations.
In the RNN-Controller (min) version, we added a penalty
into the fitness function to find smaller RNNs that could con-
trol CartPole with smaller numbers of nodes and connections.
This shifted the distribution of found RNN CartPole controller
networks with ideal fitness. The comparison is shown on the
table. Intriguingly, 39% of all runs in this experimental version
yielded 6-neuron RNNs with ideal fitness when controlling
CartPole while 33% yielded 7-neuron RNNs.
The full distribution of RNN sizes for the RNN-controller
(min) is shown in Figure 8. The values correspond to the
number of evolutionary runs that had valid solutions. The
heatmap shows that the compact solutions were not only oc-
curring frequently, but also demonstrated consequtive patterns
of evolutionary convergence, having highest concentration of
successful solutions with 6 and 7 neurons. This clustering
indicates that the network size-penalized fitness function pro-
vides enough evolutionary pressure to minimize the capacity
TABLE V
SUMMARY STATISTICS OF RNN C ARTPOLE CONTROLLER EXPERIMENTS
Metric RNN-Controller
RNN-Controller
(min)
Small networks ( ≤ 8 neurons) 8% 78%
Medium networks (9-50 neurons) 3% 20%
Large networks ( ≥ 90 neurons) 89% 2%
Fig. 8. Distribution of successful evolutionary runs across network size and
generation count. The values correspond to the number of runs resulting in
finding a valid solution.
of architecture while it is still able to accomplish the same
function. It is interesting to note that most successful compact
solutions required additional selection pressure to yield smaller
neural networks.
F . Discussion
The results of RNN Controller experiment success-
fully demonstrate effectiveness of the proposed MorphoNAS
method to produce functional control networks. High success
rate, when 94 of 100 runs achieve optimal CartPole controller
solustions in the initial population, suggests that the generation
process effectively chooses viable network architectires.
It’s important to point out that Oller et al. [32] have demon-
strated that over 3% of randomly weighted neural networks are
able to operate CartPole successfully, meaning that CartPole
is a relatively simple control task. However, this does not
affect the evaluation of the proposed experimental design:
CartPole serves as a suitable proof-of-concept environment to
test whether the MorphoNAS method can generate functional
architectures, before we go up the complexity ladder.
Key Findings : Our method demonstrates the ability to
evolve different architectures depending on selective pressure.
To underline, the initial RNN Controller experiment evolved
functionally viable control networks that frequently had size of
100 neurons, maximum possible for the given developmental
field size, while in the size penalised RNN Controller (min)
experiment, 72% of successful runs evolved a control network
with 6-7 neurons, i.e., demonstrating that the size of the
architecture was explicitly being considered by MorphoNAS.
Correspondingly, there is a trade-off between solution quality
and efficiency of architecture that the evolutionary process is
clearly able to adapt to.
The emergence of compact, functional networks under
selective evolutionary pressure indicates strong scalability
prospects. The planned future experimental applications that
involve more complex control tasks, including separate tasks
for continuous physical control and partially observable envi-
ronments, will help supply a rigorous experimental platform
capable of providing sufficient assessment of the method’s
output capacity beyond this initial and descriptive verification.
The construction of both maximal and minimal viable
architectures suggests that MorphoNAS-based processes can
reliable grow functional neural networks within known solu-
tion spaces, thus laying the groundwork prior to entering and
addressing much more complex problems.
VIII. C ONCLUSION
In conclusion, this paper has established that MorphoNAS
can produce neural networks with specified structural spec-
ifications and functional capabilities. The proposed biologi-
cally inspired alternative enhances standard neural architecture
search by combining the Free Energy Principle, reaction-
diffusion systems, and gene regulatory networks into one
computational approach.
The experiment on Graph Targeting, in particular, demon-
strated the generality of the system and produced valid solu-
tions across random target configurations. Despite some of the
configurations were more difficult to search than others, the
successful solutions were found for all of them, showing po-
tential for vaibility of the method across various architectural
constraints.
The RNN-controller experiment illustrated functional com-
petence of the system; where, at the population level, 94%
of the random populations contained networks that could
successfully solve the CartPole task, demonstrating effective
architecture sampling. At the same time, when network size
penalties were added to the fitness function, evolution went
on to find smaller and no less effective controllers where
percentage of solutions having compact architecture with only
6-7 neurons was 72%.
Integrating relevant principles from developmental biology
and evolutionary computation lays the groundwork for auto-
mated generation of neural architectures. The key conclusion
is that simple morphogen- and GRN-based rules can create
complex neural architecture through self-organization.
In addition, this work demonstrates how morphogenetic
systems can generate functional computational structures using
a reduced set of biologically inspired developmental rules. It
may also yield insights on the natural processes that occur
in development of the neural tissue, and could potentially be
beneficial to models in computational neuroscience.
While the validation was limited to simple control tasks
with relatively small networks ( ≤ 100 neurons), the principles
of spatial organization in the proposed framework promise
scalability potential. Future work would engage computational
optimization, expand the researched tasks to include more
complex problem domains, and investigate hierarchical devel-
opmental processes for larger target architectures.
By incorporating plasticity mechanisms at development
time or after development, MorphoNAS may be capable of
producing networks that could then undergo further adaptation,
potentially yielding a closer resemblance to biological neural
systems. Also, a systematic study that compares MorphoNAS
to the established neural architecture search methods would
also be beneficial to clarify strengths, weaknesses, and appro-
priate domains of application.
As AI systems become more complex, bio-inspired devel-
opmental approaches may play significant role to discover
neural architectures that maintain effectiveness, adaptability,
and robustness of biological neural systems.
This preliminary validation marks MorphoNAS as an excit-
ing approach in neural architecture search toolbox, and restates
a principled pathway in pursuit of creating more biologically
plausible artificial neural networks.
IX. C ODE AND DATA AVAILABILITY
The source code for MorphoNAS, including all experi-
mental setups and evolved genomes, will be made publicly
available at https://github.com/sergemedvid/MorphoNAS fol-
lowing publication. The repository will include: complete
MorphoNAS framework implementation, evolutionary algo-
rithm code, experimental configurations and seeds for repro-
ducibility, example evolved genomes and visualizations.
REFERENCES
[1] Anthony M. Zador. A critique of pure learning and what artificial
neural networks can learn from animal brains. Nature Communications,
10(1):3770, August 2019. Publisher: Nature Publishing Group.
[2] Karl J. Friston. Learning and inference in the brain. Neural Networks,
November 2003.
[3] A. M. Turing. The Chemical Basis of Morphogenesis. Bulletin of
Mathematical Biology, January 1990.
[4] Karl J. Friston, M. Levin, B. Sengupta, and G. Pezzulo. Knowing one’s
place: a free-energy approach to pattern regulation. Journal of The Royal
Society Interface, April 2015.
[5] J. Gurdon. The developmental capacity of nuclei taken from intestinal
epithelium cells of feeding tadpoles. Journal of embryology and
experimental morphology, December 1962.
[6] Stuart A Kauffman. The Origins of Order: Self-Organization and
Selection in Evolution . Oxford University Press, June 1993.
[7] John H. Holland. Emergence: From Chaos to Order. January 1998.
[8] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from
incomplete data via the EM - algorithm plus discussions on the paper.
September 1977.
[9] Karl J. Friston, James M. Kilner, and Lee H. Harrison. A free energy
principle for the brain. Journal of Physiology - Paris , July 2006.
[10] Karl J. Friston. The free-energy principle: a unified brain theory? Nature
Reviews Neuroscience, February 2010.
[11] Franz Kuchling, Karl J. Friston, Georgi Georgiev, and Michael Levin.
Morphogenesis as Bayesian inference: A variational approach to pattern
formation and control in complex biological systems. Physics of Life
Reviews, July 2020.
[12] Karl J. Friston. A free energy principle for a particular physics, June
2019.
[13] Chris Fields, Karl J. Friston, James F. Glazebrook, Michael Levin, Chris
Fields, Karl J. Friston, James F. Glazebrook, and Michael Levin. A free
energy principle for generic quantum systems., May 2022.
[14] Karl J. Friston. Life as we know it. Journal of the Royal Society
Interface, September 2013.
[15] Yasunari Miyagi, Y . Mio, K. Yumoto, R. Hirata, T. Habara, and
N. Hayashi. Kinetic Energy and the Free Energy Principle in the Birth
of Human Life. Reproductive Medicine, May 2024.
[16] Erwin Schr ¨odinger. What is life? : the physical aspect of the living cell
; & Mind and matter . Cambridge : University Press, 1967.
[17] L ´eo Pio-Lopez, Franz Kuchling, Angela Tung, G. Pezzulo, and
M. Levin. Active inference, morphogenesis, and computational psy-
chiatry. Frontiers in Computational Neuroscience , November 2022.
[18] Selen Atasoy, Isaac Donnelly, and Joel Pearson. Human brain networks
function in connectome-specific harmonic waves. Nature Communica-
tions, January 2016.
[19] A. Safron. An Integrated World Modeling Theory (IWMT) of Con-
sciousness: Combining Integrated Information and Global Neuronal
Workspace Theories With the Free Energy Principle and Active Infer-
ence Framework; Toward Solving the Hard Problem and Characterizing
Agentic Causation. Frontiers in Artificial Intelligence , June 2020.
[20] Dongliang Zhang, Chenghao Zhang, Q. Ouyang, and Y . Tu. Free energy
dissipation enhances spatial accuracy and robustness of self-positioned
Turing pattern in small biochemical systems. Journal of the Royal
Society Interface, July 2023.
[21] Ren ´e Thomas. Boolean formalization of genetic control circuits. Journal
of Theoretical Biology , December 1973.
[22] A. Polynikis, Stephen John Hogan, and M. di Bernardo. Comparing
different ODE modelling approaches for gene regulatory networks.
Journal of Theoretical Biology , December 2009.
[23] Thomas R. Sokolowski, Thomas Gregor, William Bialek, and Ga ˇsper
Tkaˇcik. Deriving a genetic regulatory network from an optimization
principle., January 2025.
[24] L ´aszl´o Babai. Graph isomorphism in quasipolynomial time. In
Proceedings of the forty-eighth annual ACM symposium on Theory of
Computing, STOC ’16, pages 684–697, New York, NY , USA, June 2016.
Association for Computing Machinery.
[25] Gerald M. Edelman and Joseph A. Gally. Degeneracy and complexity in
biological systems. Proceedings of the National Academy of Sciences ,
98(24):13763–13768, November 2001. Publisher: Proceedings of the
National Academy of Sciences.
[26] Joanna Masel and Meredith V . Trotter. Robustness and evolvability.
Trends in genetics : TIG , 26(9):406–414, September 2010. Place:
England.
[27] Kenneth Alan De Jong. An analysis of the behavior of a class of genetic
adaptive systems. PhD thesis, University of Michigan, 1975.
[28] David E. Goldberg and Kalyanmoy Deb. A Comparative Analysis of
Selection Schemes Used in Genetic Algorithms. In GREGORY J. E.
Rawlins, editor, Foundations of Genetic Algorithms , volume 1, pages
69–93. Elsevier, January 1991.
[29] P. Erd ˝os and A. R´enyi. On random graphs I. Publicationes mathematicae
(Debrecen), 6:290–297, 1959.
[30] Elias Najarro, Shyam Sudhakaran, and S. Risi. Towards Self-Assembling
Artificial Neural Networks through Neural Developmental Programs,
July 2023.
[31] Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U. Balis, Gi-
anluca De Cola, Tristan Deleu, Manuel Goul ˜ao, Andreas Kallinteris,
Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierr ´e,
Sander Schulhoff, Jun Jet Tai, Hannah Tan, and Omar G. Younis. Gym-
nasium: A Standard Interface for Reinforcement Learning Environments,
November 2024. arXiv:2407.17032 [cs].
[32] Declan Oller, Tobias Glasmachers, and Giuseppe Cuccu. Analyzing
Reinforcement Learning Benchmarks with Random Weight Guessing.
In Proceedings of the International Conference on Autonomous Agents
and Multiagent Systems (AAMAS) , pages 975–982, 2020.