arXiv:2502.15820v2  [cs.AI]  3 Mar 2025
Universal AI maximizes V ariational Empowerment
Y usuke Hayashi∗ Koichi T akahashi†
Abstract
This paper presents a theoretical framework unifying AIXI— a model of univer-
sal AI—with V ariational Empowerment as an intrinsic drive for exploration. W e
build on the existing framework of Self-AIXI [1]—a universa l learning agent that
predicts its own actions—by showing how one of its establish ed terms can be in-
terpreted as a variational empowerment objective. W e furth er demonstrate that
universal AI’s planning process can be cast as minimizing ex pected variational
free energy (the core principle of Active Inference ), thereby revealing how uni-
versal AI agents inherently balance goal-directed behavio r with uncertainty reduc-
tion curiosity). Moreover, we argue that power-seeking ten dencies of universal
AI agents can be explained not only as an instrumental strate gy to secure future
reward, but also as a direct consequence of empowerment maxi mization—i.e. the
agent’s intrinsic drive to maintain or expand its own contro llability in uncertain
environments. Our main contribution is to show how these int rinsic motivations
(empowerment, curiosity) systematically lead universal A I agents to seek and sus-
tain high-optionality states. W e prove that Self-AIXI asym ptotically converges to
the same performance as AIXI under suitable conditions, and highlight that its
power-seeking behavior emerges naturally from both reward maximization and
curiosity-driven exploration. Since AIXI can be view as a Ba yes-optimal math-
ematical formulation for Artiﬁcial General Intelligence ( AGI), our result can be
useful for further discussion on AI safety and the controlla bility of AGI.
1 Introduction
Designing an autonomous reinforcement learning (RL) agent that is both Bayes-optimal and ex-
ploratory poses signiﬁcant theoretical and practical challenges. Hu tter’s seminal AIXI frame-
work [2] represents the gold standard of universal intellig ence: given a suitable prior over all com-
putable environments, AIXI maximizes reward in a provably optimal sense. However, it uses exhaus-
tive planning and Solomonoff induction, which are computat ionally intractable. Realistic agents
must therefore learn approximate models to scale beyond tri vial tasks.
A key question then arises: how can a learning-based agent (an approximation to AIXI) en sure
sufﬁciently robust exploration so that it does not miss the o ptimal policy? While AIXI’s exhaus-
tive lookahead implicitly solves exploration by evaluating all possible future traje ctories, a practical
agent cannot feasibly do the same. Without a principled mech anism for seeking sufﬁciently diverse
experiences, the agent may get stuck in suboptimal regions o f the environment.
V ariational empowerment [3, 4, 5, 6] has recently emerged as a powerful intrinsic motivation to
drive exploration. It encourages an agent to maximize the mu tual information between its actions
(or latent codes) and resulting states, thereby pushing the agent to discover states where it has high
control and optionality. Intriguingly, maximizing empowe rment often manifests as power-seeking in
the environment, prompting parallels to resource-acquiri ng or inﬂuence-driven behaviors in human
∗ AI Alignment Network (ALIGN). hayashi@aialign.net
† AI Alignment Network (ALIGN); Advanced General Intelligen ce for Science Program, RIKEN; Graduate
School of Media and Governance, Keio University . ktakahashi@riken.jp
Preprint. Under review .
organizations. While this can be an asset for efﬁcient exploration , it also highlights potential safety
concerns: a sufﬁciently advanced agent might over-optimiz e this drive in ways that conﬂict with
human interests [7, 8, 9].
In this paper, we extend the recently proposed Self-AIXI framework [1] by showing that its exist-
ing mixture-policy regularization term can be reinterpret ed as a variational empowerment bonus.
Additionally, we provide two new theoretical contribution s:
(1) W e demonstrate explicitly, through two key equations, t hat AIXI’s decision criterion is
mathematically equivalent to minimizing expected variati onal free energy , the core
objective in Active Inference [10, 11, 12]. This shows that AIXI-like Bayes-optimal plan-
ning inherently includes a drive to reduce uncertainty abou t the environment (i.e. curiosity),
thus unifying AIXI with the “goal-directed + information-s eeking” paradigm of Active In-
ference.
(2) W e formally argue that power-seeking can be explained not only as an instrumental
pursuit of ﬁnal reward , but also as a direct result of empowerment maximization (i.e.
curiosity-driven exploration). Even absent an immediate reward advantage, t he agent ac-
quires power (i.e. broad control over states and options) as a natural consequence of seek-
ing to reduce uncertainty and maintain high optionality. Th is stands in contrast to prior
accounts, e.g. Turner et al. [8] and Cohen et al. [9], which fo cus on ﬁnal reward maximiza-
tion as the source of an agent’s incentive to obtain power.
Though our study is primarily theoretical (no empirical exp eriments are presented), these results
provide a fresh perspective on how intrinsic motivation can ﬁll the gap between purely planning-
based universal agents and tractable learning agents.
1.1 Background on AIXI and Self-AIXI
Bayesian Optimal Reinforcement Learning. Hutter’s AIXI [2] is a Universal Bayesian RL agent
that, in principle, can optimally maximize cumulative rewa rd in any computable environment. It
maintains a mixture (the universal semimeasure) over all po ssible environment hypotheses, updates
these hypotheses upon observing new data, and plans by expec timax over all action sequences. For-
mally, if h<t denotes the history (observations, actions, and rewards) u p to time t, AIXI selects the
action
at = arg max
a
∑
ν ∈M
w(ν | h<t) Q∗
ν (h<t, a ), (1)
where each ν is an environment in a suitable class of computable Markov de cision processes, w(ν |
h<t) is the posterior weight of ν, and Q∗
ν is the optimal Q-value under environment ν. Also,
π∗
ξ (at | h<t)
def
= arg max
a
Q∗
ξ (h<t, a ) (2)
where Q∗
ξ (h<t, a ) represents the optimal action-value function (Q-value) un der the environment
model ξ. Speciﬁcally,
Q∗
ξ (h<t, a t) =
∑
et∈E
ξ(et|h<t, a t)
(
rt + γV ∗
ξ (h<t)
)
, (3)
with
V ∗
ξ (h<t) = max
a
Q∗
ξ (h<t, a ). (4)
AIXI thus selects the action maximizing this Q-value.
Softmax Policy Interpretation. Sometimes, we transform the arg maxover Q∗
ξ into a softmax
over actions:
p (a | h<t)
def
=
exp
(
Q∗
ξ (h<t, a )
)
∑
a′∈A exp
(
Q∗
ξ (h<t, a ′)
) , (5)
2
so that the log-likelihood of action a is:
ln p
(
a | h<t
)
= Q∗
ξ
(
h<t, a
)
− ln
∑
a′
exp
(
Q∗
ξ
(
h<t, a ′) )
, (6)
and
arg max
a
ln p(a | h<t) = arg max
a
Q∗
ξ
(
h<t, a
)
. (7)
Hence maximizing Q∗
ξ is equivalent to maximizing log-likelihood of a. W e can rewrite an “ AIXI
objective” as a likelihood:
LAIXI
def
= −Ea∼p [ln p (a | h<t)] . (8)
Although provably optimal in a Bayesian sense, AIXI is compu tationally infeasible: it sums over
inﬁnitely many models and searches over all possible action sequences. Nonetheless, the theory
behind AIXI is highly inﬂuential: it shows that if the true en vironment µ has nonzero prior proba-
bility, AIXI eventually behaves optimally in µ. It also satisﬁes the self-optimizing property in many
environments and achieves the maximal Legg-Hutter intelli gence score [13].
Self-AIXI as a Learning-Centric Approximation. Self-AIXI, introduced in [1], is a learning-
based approach to approximate AIXI’s policy without exhaus tive search. Instead of planning over
all future action sequences, Self-AIXI predicts its own fut ure behavior given the current policy.
Concretely, it maintains a Bayesian mixture of policies, ζ, with posterior updates based on how
accurately each candidate policy predicts the agent’s acti ons:
ζ(at | h<t) =
∑
π ∈P
ω (π | h<t) π(at | h<t), (9)
where ω (π | h<t) is updated via Bayes’ rule after each action. The Q-values ar e estimated via
experience rather than full expectimax. Formally, one can w rite a self-consistent objective
πS (at | h<t)
def
= arg max
a
{
Qζ
ξ (h<t, a ) − λ ln π∗(a | h<t)
ζ(a | h<t)
}
, (10)
where
Qζ
ξ (h<t, a t)
def
=
∑
π ∈P
ω (π|h<t)
∑
ν ∈M
w(ν|h<t)Qπ
ν (h<t, a t), (11)
where Qζ
ξ combines environment predictions ξ with the mixture policy ζ, and ln π ∗(a|h<t)
ζ(a|h<t) is a
regularization measure encouraging ζ to approach the optimal policy π∗. Note that the KL term
serves as a regularization that nudges ζ toward π∗. This formulation generalizes the simpler case
in [1] by allowing λ > 0 (See A.2). If λ = 0, we recover the original Self-AIXI objective without
explicit KL regularization.
Softmax Policy Interpretation. W e transform the arg maxover Qζ
ξ into a softmax over actions:
qφ (a | h<t)
def
=
exp
(
Qζ
ξ (h<t, a )
)
∑
a′∈A exp
(
Qζ
ξ (h<t, a ′)
) , (12)
so that the log-likelihood of action a is:
ln qφ (a | h<t) = Qζ
ξ
(
h<t, a
)
− ln
∑
a′
exp
(
Qζ
ξ (h<t, a ′)
)
, (13)
and
arg max
a
ln qφ (a | h<t) = arg max
a
Qζ
ξ
(
h<t, a
)
. (14)
Finally, we can rewrite an “Self-AIXI objective” as a likeli hood:
LSelf-AIXI
def
= −Ea∼qφ
[
ln qφ (a | h<t)
]
+ λDKL (π∗ ∥ ζ) . (15)
3
These unify the planning perspective (max Qζ
ξ ) and a probabilistic policy perspective. By self-
predicting its action distributions, Self-AIXI can increm entally reﬁne Q-value estimates, akin to
TD-learning, while still retaining a universal Bayesian fo undation (assuming the environment is
in the model class). Prior work [1] shows that, under suitabl e assumptions, Self-AIXI converges
to the same optimal value as AIXI, but it must still ensure ade quate exploration to gather correct
world-model data.
1.2 V ariational Empowerment for Intrinsic Exploration
While AIXI implicitly explores via its unbounded search ove r hypotheses, any tractable approxima-
tion (like Self-AIXI) requires an explicit exploration mec hanism. W e adopt V ariational Empower-
ment [3, 4, 5, 6] as an intrinsic reward to drive the agent toward high-control states. This perspec tive
aligns with recent work [14] in which empowerment is used not only for exploration but also as a
mechanism for discovering useful latent representations o r skills, potentially complementing goal-
based RL approaches.
1.2.1 Formal Deﬁnition of Empowerment
Empowerment is often deﬁned as the maximal mutual information between an agent’s actions and
future states. For a horizon k, let zk
def
= at:t+k−1 be a sequence of actions and ht<t+k the resulting
state; then
I(zk; h<t+k)
def
= max
p
I (zk; ht<t+k | h<t) , (16)
= max
p
Ez,h ∼p
[
ln p (zk | h<t+k)
p (zk | h<t)
]
. (17)
The agent is empowered in states ht where it can produce a wide variety of distinguishable futur e
outcomes through its choice of action-sequence. Exact comp utation is generally intractable in large
state spaces, so one uses a variational approximation. For i nstance, we introduce a parameterized
distribution qφ that approximates the posterior qφ (zk | h<t+k), and then maximize [6]:
Eφ (zk; h<t+k)
def
= max
qφ
Eφ (zk; ht<t+k | h<t) , (18)
= max
qφ
Ez,h ∼p
[
ln qφ (zk | h<t+k)
p (zk | h<t)
]
. (19)
Using Eqs. (7) and (14), we have:
p (zk | h<t+k) =
k−1∏
i=0
π∗ (at+i | h<t+i) , (20)
qφ (zk | h<t+k) =
k−1∏
i=0
ζ (at+i | h<t+i) , (21)
Ez,h ∼p
[
ln qφ (zk | h<t+k)
p (zk | h<t+k)
]
= Eh∼p
[ k−1∑
i=0
−DKL (π∗
i ∥ ζi)
]
. (22)
The right-hand side of Eq. (22), DKL (π∗
i ∥ ζi), is a regularization term in the Self-AIXI framework
that pushes the agent’s mixture policy ζi
def
= qφ (at+i | h<t+i) to imitate or approach the optimal
policy π∗
i
def
= p(at+i | h<t+i). As the agent learns from experience, it reduces this diverg ence,
effectively self-optimizing its policy.
Hence, Eqs. (17) and (22) allow us to rewrite the V ariational Empowerment as:
Eφ (zk; h<t+k) = max
qφ
Ez,h ∼p
[
ln qφ (zk | h<t+k)
p (zk | h<t+k) + ln p (zk | h<t+k)
p (zk | h<t)
]
, (23)
= max
qφ
Eh∼p
[ k−1∑
i=0
−DKL (π∗
i ∥ ζi)
]
+ I (zk; h<t+k) . (24)
4
1.3 Connecting to Free-Energy Minimization and Active Infe rence
Bayesian RL as Active Inference. Bayesian RL connects closely to Active Inference [10, 11, 12],
where an agent maintains a prior over latent variables and up dates its posterior after observing re-
wards or other feedback. Under a Free Energy Principle (FEP), one often writes a free-energy
functional:
Fφ (zk; h<t+k)
def
= DKL(p(zk, h t<t+k | h<t) ∥ qφ (zk, h t<t+k | h<t)) , (25)
≈ − Eh∼p [ln qφ (ht<t+k | zk, h <t)]  
Predictive Error (Surprise)
+ Ez,h ∼p
[
− ln qφ (zk | h<t+k)
p (zk | h<t)
]
  
FEP’s Regularization
. (26)
Here, −Eh∼p[ln qφ (ht<t+k | zk, h <t)] is the predictive error (surprise), and the remaining term
measures how far qφ (zk | h<t+k) diverges from p(zk | h<t).
Decomposition of Regularization term. Under suitable rearrangements or sign conventions, we
can identify a Regularization part that can be maximized rather than minimized, yielding e mpower-
ment:
Ez,h ∼p
[
− ln qφ (zk | h<t+k)
p (zk | h<t)
]
  
FEP’s Regularization
= Ez,h ∼p
[
− ln qφ (zk | h<t+k)
p (zk | h<t+k) − ln p (zk | h<t+k)
p (zk | h<t)
]
, (27)
= Eh∼p
[ k−1∑
i=0
DKL (π∗
i ∥ ζi)
]
  
Self-AIXI’s Policy Regularization
− I (zk; ht<t+k | h<t)  
Mutual Information
. (28)
Hence, turning the regularization term “upside down” (from negative to positive) motivates V aria-
tional Empowerment :
Eφ (zk; h<t+k)  
V ariational Empowerment
= − min
qφ
Ez,h ∼p
[
− ln qφ (zk | h<t+k)
p (zk | h<t)
]
  
FEP’s Regularization
, (29)
= max
qφ
Eh∼p
[ k−1∑
i=0
−DKL (π∗
i ∥ ζi)
]
  
(Negative) Self-AIXI’s Policy Regularization
+ I (zk; h<t+k)  
Empowerment
. (30)
mirroring the deﬁnitions in Eq. (24) above.
2 Universal AI maximizes V ariational Empowerment
In the Universal Artiﬁcial Intelligence (UAI) framework [2], an agent is considered universal if
it can, given sufﬁcient time, match or surpass any other poli cy’s performance in all computable
environments (with nonzero prior). AIXI achieves this in th eory. Self-AIXI aims to achieve it in
practice, provided it can explore effectively. Below , we su mmarize how our empowered Self-AIXI
ﬁts these formal criteria.
2.1 Asymptotic Equivalence, Legg-Hutter Intelligence, an d Self-Optimizing Property
Prior work [1] proves that if the Self-AIXI agent’s policy cl ass and environment prior are sufﬁciently
expressive (i.e. the true environment is in the hypothesis c lass with nonzero probability), then the
agent’s behavior converges to that of AIXI’s optimal policy in the limit of inﬁnite interaction. For-
mally, for any environment µ in the model class,
lim
t→∞
Eπ s
µ
[
V ∗
ξ
(
h<t
)
− V π s
ξ
(
h<t
) ]
= 0. (31)
5
which implies that, asymptotically, the agent’s expected r eturn under πs matches that of the optimal
policy V ∗
ξ . Intuitively, as the agent’s world-model becomes more accu rate, it exploits the optimal
policy; hyperparameters (such as λ in an empowerment term) can be tuned or annealed so that
extrinsic reward eventually dominates.
From the perspective of Legg-Hutter intelligence [13], which associates an agent’s “intelligence”
with its expected performance across a suite of weighted env ironments, this result is especially
signiﬁcant. Because Self-AIXI asymptotically reproduces AIXI’s policy, it inherits maximal Legg-
Hutter intelligence within that class of environments. Mor eover, in a wide class of self-optimizing
environments, the agent will ultimately achieve the same re turns as an optimal agent with perfect
knowledge would achieve, under the same conditions in Eq. (3 1). These guarantees illustrate that
the enhanced exploration mechanisms—such as empowerment- driven strategies—do not compro-
mise eventual performance. Instead, they help ensure the ag ent uncovers the environment’s true
optimal actions without becoming trapped in suboptimal beh aviors due to insufﬁcient data. Conse-
quently, the agent retains AIXI’s universal optimality in t he limit while mitigating early exploration
challenges.
2.2 Self-Optimization leads Empowerment Maximization
The agent’s process of improving its policy (often referred to as self-optimization) naturally leads to
an increase in V ariational Empowerment. In fact, as reinforcement learning progresses, both AIXI’ s
objective function LAIXI and Self-AIXI’s objective function LSelf-AIXI gradually converge, and they
coincide in the limit t → ∞ .
The difference between these two objectives can be expresse d through the policy regularization term
DKL(π∗ ∥ ζ). Formally, we have:
lim
t→∞
|LAIXI − L Self-AIXI | = lim
t→∞
λ D KL(π∗ ∥ ζ) = 0. (32)
This result implies that DKL(π∗ ∥ ζ) goes to 0 as t → ∞ , which is equivalent to the Self-AIXI’s
variational empowerment Eφ (zk; h<t+k) being maximized. In other words, the universal AI agent
AIXI, which behaves in a Bayes-optimal way with respect to th e environment, also emerges as an
agent that maximizes empowerment.
Concretely, the relationship between the empowerment obje ctive and the policy regularization is
succinctly captured by the following equality:
Eφ (zk; h<t+k)
 
V ariational Empowerment
= max
qφ
Eh∼p
[ k−1∑
i=0
−DKL (π∗
i ∥ ζi)
]
  
(Negative) Self-AIXI’s Policy Regularization
+ I (zk; h<t+k)  
Empowerment
. (33)
Self-optimization refers to the iterative improvement of t he agent’s policy based on observed re-
wards and outcomes. In Self-AIXI, reducing the policy regul arization term DKL (π∗ ∥ ζ) directs ζ
closer to π∗. Because the left-hand side of the second formula above equa ls the variational empower-
ment Eφ (zk; h<t+k), each step that lowers DKL (π∗ ∥ ζ) raises Eφ (zk; h<t+k). Empowerment here
signiﬁes how many high-control or high-optionality states are accessible to the agent. Consequently,
maximizing reward often requires seeking out exactly those states in which the agent can maintain
or expand control—thus also maximizing Eφ (zk; h<t+k). As ζ becomes more similar to π∗, the
agent naturally discovers strategies that grant more contr ol and ﬂexibility. Therefore, under Self-
AIXI, improving the policy toward optimal behavior simulta neously yields higher external rewards
and ampliﬁes the agent’s own empowerment.
3 Conclusions
In this work, we reinterpreted a term in Self-AIXI as variational empowerment —intrinsic explo-
ration bonus. W e have argued that:
• Empowerment naturally complements Bayesian RL in univers al settings, providing a struc-
tured incentive to discover controllable states and gather broad experience.
6
• Even with an empowerment bonus, the agent asymptotically r ecovers AIXI’s Bayes-
optimal policy, inheriting the same universal intelligenc e and self-optimizing properties
in the limit.
One of our main observations is that the agent’s pursuit of hi gh-empowerment states often manifests
as a power-seeking tendency. Traditionally, many authors (e.g., Turner et al. [8]) interpret power-
seeking as purely instrumental: an agent acquires resource s, avoids shutdown, or manipulates the
reward channel to better guarantee high external returns. H owever, we show that power-seeking
can also arise intrinsically from a drive to reduce uncertai nty and maintain a wide range of feasible
actions (i.e., “keeping options open”). Imagine an agent ch oosing between a high-control region
(with many possible actions and partial knowledge) and a low -control region (with fewer actions
and less information). If both yield the same short-term rew ard, a purely extrinsic approach might
be indifferent. By contrast, an empowerment-seeking agent prefers the high-control region, as it
offers greater potential for discovering valuable future s trategies. Over time, as the agent learns
more about its environment, these beneﬁts accumulate.
When not moderated, power-seeking behaviors may conﬂict wi th human interests. For instance,
maximizing control can lead to manipulative or exploitativ e outcomes if the agent’s intrinsic or
extrinsic goals are misaligned with social values. From a Universal AI standpoint, understanding
that power-seeking can stem from both instrumental and intr insic (empowerment-based) motives
is crucial to designing mechanisms—e.g., safe exploration techniques or alignment constraints—to
ensure that an agent’s inﬂuence remains beneﬁcial. It is imp ortant to note that these concerns apply
even to AI agents with apparently benign objectives, such as an AI scientist pursuing scientiﬁc truth
purely out of intellectual curiosity.
Our results are primarily conceptual and rest on idealized a ssumptions: (1) the environment is in
the agent’s hypothesis class with nonzero prior; (2) we assu me unbounded computational resources
and memory; (3) the agent can tractably approximate empower ment. In reality, computing exact
empowerment or using universal priors is challenging. Empi rical methods to approximate these
ideas (e.g., neural networks [4]) remain an active area of re search.
Acknowledgments
The architects of Self-AIXI [1] provided groundwork that em boldened our own inquiry. W e thank
the AI safety community, particularly those who involved in our AI Alignment Network. Special
gratitude goes to davidad and Hiroshi Y amakawa, whose incis ive feedback steered us toward a more
lucid exposition. Part of this work was supported by RIKEN TR IP initiative (AGIS).
References
[1] Elliot Catt, Jordi Grau-Moya, Marcus Hutter, Matthew Ai tchison, Tim Genewein, Gregoire Deletang, Li
Kevin W enliang, and Joel V eness. Self-Predictive Universa l AI. In 37th Conference on Neural Information
Processing Systems (NeurIPS’23) , pages 1–18, New Orleans, USA, 2023.
[2] Marcus Hutter. Universal Artiﬁcial Intelligence: Sequential Decisions b ased on Algorithmic Probability .
EA TCS Series. Springer, Berlin, 2005. ISBN: 3-540-22139-5 ; ISBN-online: 978-3-540-26877-2.
[3] Alexander S Klyubin, Daniel Polani, and Chrystopher L Ne haniv . Empowerment: A universal agent-
centric measure of control. In 2005 IEEE Congress on Evolutionary Computation , pages 128–135. IEEE,
2005.
[4] Shakir Mohamed and Danilo J Rezende. V ariational inform ation maximisation for intrinsically motivated
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIP S), 2015.
[5] Karol Gregor, Danilo J Rezende, and Daan Wierstra. V aria tional intrinsic control. In International Con-
ference on Learning Representations (ICLR) , 2017.
[6] Jongwook Choi 1,† , Archit Sharma 2,† , Honglak Lee 1,3, Sergey Levine 4,5, and Shixiang Shane Gu 4 V aria-
tional empowerment as representation learning for goal-ba sed reinforcement learning. In Proceedings of
the 38th International Conference on Machine Learning (ICM L), 2021.
[7] Nick Bostrom. Superintelligence: P aths, Dangers, Strategies . Oxford University Press, 2014.
[8] Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew C ritch, and Prasad T adepalli. Optimal policies
tend to seek power. In Advances in Neural Information Processing Systems (NeurIP S), 2021. Also
available at: https://arxiv.org/abs/1912.01683.
7
[9] Michael Cohen, Badri V ellambi, and Marcus Hutter. Asymp totically unambitious artiﬁcial general intel-
ligence. Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, a nd Society (AIES) , 2020. Also
available at: https://arxiv.org/abs/1905.12186.
[10] Karl Friston. The free-energy principle: a uniﬁed brai n theory? Nature Reviews Neuroscience , 11(2):127–
138, 2010.
[11] Karl Friston, Biswa Sengupta, and Gennaro Auletta. Cog nitive dynamics: From attractors to active
inference. Proceedings of the IEEE , 102(4):427–445, 2014.
[12] Karl Friston. A free energy principle for a particular p hysics. Neural Computation , 29(6):129–155, 2019.
[13] Shane Legg and Marcus Hutter. Universal intelligence: A deﬁnition of machine intelligence. Minds and
Machines, 17(4):391–444, 2007.
[14] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learn-
ing skills without a reward function. In International Conference on Learning Representations (IC LR),
2019.
A Appendix
A.1 Notation and Further Details
W e summarize the main notation in T able 1 for reference.
Symbol Meaning
h<t History (observations, actions, rewards) up to time t
ξ, ν, µ Environment hypotheses (in a computable class M)
π, ζ Policies (e.g. mixture policy ζ)
w(ν | h<t) Posterior weight of environment ν given history h<t
Q∗
ν , Q ζ
ξ (Optimal) Q-values under environment ν or mixture/policy ζ
I (zk; h<t+k) Empowerment in state h<t+k
Eφ (zk; h<t+k) V ariational empowerment in state h<t+k, approximated by parameter φ
λ Hyperparameters weighting empowerment or KL regularizati on
T able 1: Key notation used in the main text.
A.2 Self-AIXI’s self-consistent objective
In this subsection, we investigate how introducing a Kullba ck–Leibler (KL) divergence-based regu-
larization term into Self-AIXI affects its convergence pro perties and whether it preserves the agent’s
ability to reach the optimal policy π∗ asymptotically. Speciﬁcally, we consider the effect of add ing
a penalty that measures how far the current mixture policy ζ deviates from π∗.
Recall that for a given history h<t, the KL divergence between the optimal policy π∗ and the current
mixture policy ζ is deﬁned as:
DKL (π∗∥ζ) =
∑
a′∈A
π∗ (a′ | h<t) lnπ∗ (a′ | h<t)
ζ (a′ | h<t) . (34)
W e then propose a policy update rule that augments the standa rd Self-AIXI greedy step with a log-
likelihood ratio term:
πS (at | h<t)
def
= arg max
a
{
Qζ
ξ (h<t, a ) − λ ln π∗ (a | h<t)
ζ (a | h<t)
}
, (35)
where Qζ
ξ denotes the estimated value of taking action a in history h<t under the Bayesian mixture
environment ξ and current policy ζ. Here, λ > 0 is included to penalize large deviations from π∗
whenever the agent’s mixture policy ζ differs substantially from the (unknown) optimal policy π∗.
8
KL regularization and recovery of the standard update. In many practical scenarios, such as
when π∗ is deterministic or assigns a high probability to a single ac tion, part of the KL term can be
constant with respect to a. Under those conditions, the penalty term
λ ln π∗ (a′ | h<t)
ζ (a′ | h<t) (36)
does not vary across actions, and the update rule simpliﬁes t o
πS
(
at | h<t
)
= arg max
a
{
Qζ
ξ
(
h<t, a
) }
, (37)
which recovers the conventional (un-regularized) Self-AI XI greedy update.
Impact on learning dynamics and convergence. Although the added term changes the action
selection criterion, it does not alter the identity of the optimal policy in the underlying en vironment.
Intuitively, the new rule can be viewed as performing a more c onservative or “trust-region”-like
update, since actions that the agent’s current policy ζ overestimates relative to π∗ will be penalized
more strongly. Conversely, if ζ assigns too little probability to actions that π∗ actually favors, the
negative logarithm of their ratio produces a smaller (or pos itive) correction. Hence, the agent is
nudged toward π∗.
Crucially, the regularization term does not disrupt Self-A IXI’s standard convergence guarantees,
assuming the original conditions hold (e.g., that the true e nvironment is in the Bayesian mixture
class ξ and π∗ is in the agent’s policy class). From a theoretical perspect ive, π∗ remains a stable
ﬁxed point under this augmented objective. Once ζ converges to π∗, the log ratio
ln π∗ (a | h<t)
ζ (a | h<t) (38)
vanishes for actions with nonzero probability under π∗, so no extra penalty is incurred, and the
update aligns with the optimal policy’s greedy choice.
Regularization coefﬁcient and transient behavior . The coefﬁcient λ < 0 determines the strength
of the penalty term:
• Small, moderate penalty ( |λ|small ). A suitably chosen, relatively small magnitude for |λ|
works as a gentle regularizer, smoothing the agent’s update s by discouraging drastic shifts
in policy. This can stabilize learning and reduce oscillati ons without harming ﬁnal conver-
gence. Indeed, theoretical analyses of KL-based regulariz ation in reinforcement learning
show that while such shaping modiﬁes the transient policy up dates, the optimal policy re-
mains the same in the limit.
• Overly large penalty ( |λ|large). If the KL term is emphasized too strongly, the agent may
stick too closely to its current guess of π∗ and under-explore other actions. Early in learn-
ing—when ζ is still inaccurate—this could delay or even misdirect poli cy improvement.
However, as Self-AIXI continuously updates its environmen t belief (via ξ) and revises ζ,
the agent still accumulates evidence about which actions ar e actually optimal, making it dif-
ﬁcult to remain indeﬁnitely biased toward a suboptimal poli cy. Practical implementations
often tune λ to ensure that exploration is maintained.
9