Interpreting systems as solving POMDPs: a step
towards a formal understanding of agency⋆
Martin Biehl1[0000−0002−1670−6855] and Nathaniel Virgo2[0000−0001−8598−590X]
1 Cross Labs, Cross Compass, Tokyo 104-0045, Japan
martin.biehl@cross-compass.com
2 Earth-Life Science Institute, Tokyo Institute of Technology, Tokyo 152-8550, Japan
Abstract. Under what circumstances can a system be said to have be-
liefs and goals, and how do such agency-related features relate to its
physical state? Recent work has proposed a notion ofinterpretation map,
a function that maps the state of a system to a probability distribution
representing its beliefs about an external world. Such a map is not com-
pletely arbitrary, as the beliefs it attributes to the system must evolve
over time in a manner that is consistent with Bayes’ theorem, and conse-
quently the dynamics of a system constrain its possible interpretations.
Here we build on this approach, proposing a notion of interpretation not
just in terms of beliefs but in terms of goals and actions. To do this we
make use of the existing theory of partially observable Markov decision
processes (POMDPs): we say that a system can be interpreted as a solu-
tion to a POMDP if it not only admits an interpretation map describing
its beliefs about the hidden state of a POMDP but also takes actions
that are optimal according to its belief state. An agent is then a system
together with an interpretation of this system as a POMDP solution. Al-
though POMDPs are not the only possible formulation of what it means
to have a goal, this nevertheless represents a step towards a more general
formal definition of what it means for a system to be an agent.
Keywords: Agency · POMDP · Bayesian filtering · Bayesian inference
1 Introduction
This work is a contribution to the general question of when a physical system
can justifiably be seen as an agent. We are still far from answering this question
in full generality but employ here a set of limiting assumptions / conceptual
commitments that allow us to provide an example of the kind of answer we are
looking for.
The basic idea is inspired by but different from Dennett’s proposal to use
so-called stances [4], which says we should interpret a system as an agent if
⋆ This project was made possible through the support of Grant 62229 from the John
Templeton Foundation. The opinions expressed in this publication are those of the
authors and do not necessarily reflect the views of the John Templeton Foundation.
Work on this project was also supported by a grant from GoodAI.
arXiv:2209.01619v2  [cs.AI]  11 Jul 2025
2 M. Biehl, N. Virgo
taking the intentional stance improves our predictions of its behavior beyond
those obtained by the physical stance (or the design stance, but we ignore this
stance here). Taking the physical stance means using the dynamical laws of the
(microscopic) physical constituents of the system. Taking the intentional stance
means ignoring the dynamics of the physical constituents of the system and
instead interpreting it as a rational agent with beliefs and desires. (We content
ourselves with only ascribing goals instead of desires.) A quantitative method to
perform this comparison of stances can be found in [12].
In contrast to using a comparison of prediction performance of different
stances we propose to decide whether a system can be interpreted as an agent by
checking whether its physical dynamics are consistent with an interpretation as
a rational agent with beliefs and goals. In other words, assuming that we know
what happens in the system on the physical level (admittedly a strong assump-
tion), we propose to check whether we can consistently ascribe meaning to its
physical states, such that they appear to implement a process of belief updating
and decision making.
A formal example definition of what it means for an interpretation to be
consistent was recently published in [16]. This establishes a notion of consistent
interpretation as a Bayesian reasoner, meaning something that receives inputs
and uses them to make inferences about some hidden variable, but does not take
actions or pursue a goal.
Briefly, such an interpretation consists of a map from the physical / internal
states of the system to Bayesian beliefs about hidden states (that is, probability
distributions over them), as well as a model describing how the hidden states
determine the next hidden state and the input to the system. To be consistent,
if the internal state at time t is mapped to some belief, then the internal state
at time t + 1 must map to the Bayesian posterior of that belief, given the input
that was received in between the two time steps.
In other words, the internal state parameterizes beliefs and the system up-
dates the parameters in a way that makes the parameterized belief change ac-
cording to Bayes law. A Bayesian reasoner is not an agent however. It lacks
both goals and rationality since it neither has a goal nor actions that it could
rationally take to bring the goal about.
Here we build on the notion of consistent interpretations of [16] and show
how it can be extended to also include the attribution of goals and rationality.
For this we employ the class of problems called partially observable Markov
decision processes (POMDPs), which are well suited to our purpose. These pro-
vide hidden states to parameterize beliefs over, a notion of a goal, and a notion
of what it means to act optimally, and thus rationally, with respect to this goal.
Note that both the hidden states and the goal (which will be represented by
rewards) are not assumed to have a physical realization. They are part of the
interpretation and therefore only need to exist in the mathematical sense. Infor-
mally, the hidden state is assumed by the agent to exist, but need not match a
state of the true external world.
Interpreting systems as solving POMDPs 3
We will see that given a pair of a physical system (as modelled by a stochastic
Moore machine) and a POMDP it can in principle be checked whether the system
does indeed parameterize beliefs over the hidden states and act optimally with
respect to the goal and its beliefs (definition 5). We then say the system can be
interpreted as solving the POMDP, and we propose to call the pair of system
and POMDP an agent. This constitutes an example of a formal definition of a
rational agent with beliefs and goals .
To get there however we need to make some conceptual commitments /
assumptions that restrict the scope of our definition. Note that we do not make
these commitments because we believe they are particularly realistic or useful
for the description of real world agents like living organisms, but only because
they make it possible to be relatively precise. We suspect that each of these
choices has alternatives that lead to other notions of agents. Furthermore, we do
not argue that all agents are rational, nor that they all have beliefs and goals.
These are properties of the particular notion of agent we define here, but there
are certainly other notions of agent that one might want to consider.
The first commitment is with respect to the notion of system. Generally, the
question of which physical systems are agents may require us to clarify how we
obtain a candidate physical system from a causally closed universe and what the
type of the resulting candidate physical system is. This can be done by defining
what it means to be an individual and / or identifying some kind of boundary.
Steps in this direction have been made in the context of cellular automata e.g.
by [1,2] and in the context of stochastic differential equations by [5,7].
We here restrict our scope by assuming that the candidate physical system
is a stochastic Moore machine (definition 2). A stochastic Moore machine has
inputs, a dynamic and possibly stochastic internal state, and outputs that deter-
ministically depend on the internal state only. This is far from the most general
types of system that could be considered, but it is general enough to represent
the digital computers controlling most artificial agents at present. It it also sim-
ilar to a time and space discretized version of the dynamics of the internal state
of the literature on the free energy principle (FEP) [7].
Already at this point the reader may expect that the inputs of the Moore
machine will play the role of sensor values and the outputs that of actions and
this will indeed be the case. Furthermore, the role of the “physical constituents”
or physical state (of Dennett’s physical stance) will be played by the internal
state of the machine and this state will be equipped with a kind of consistent
Bayesian interpretation. In other words, it will be parameterizing/determining
probabilistic beliefs. This is similar to the role of internal states in the FEP.
For our formal notion of beliefs we commit to probability distributions that
are updated in accordance with Bayes law.
The third commitment is with respect to a formal notion of goals and ra-
tionality. As already mentioned, for those we employ POMDPs. These provide
both a formal notion of goals via expected reward maximization and a formal
notion of rational behavior via their optimal policy.
4 M. Biehl, N. Virgo
Combining these commitments we want to express when exactly a system
can be interpreted as a rational agent with beliefs and goals.
Rational agents take the optimal actions with respect to their goals and be-
liefs. The convenient feature of POMDPs for our purposes is that the optimal
policies are usually expressed as functions of probabilistic beliefs about the hid-
den state of the POMDP. For this to work, the probabilistic beliefs must be
updated correctly according to Bayesian principles. It then turns out that these
standard solutions for POMDPs can be turned into stochastic Moore machines
whose states are the (correctly updated) probabilistic beliefs themselves and
whose outputs are the optimal actions.
This has two consequences. One is that it seems justified to interpret such
stochastic Moore machines as rational agents that have beliefs and goals. An-
other is that there are stochastic Moore machines that solve POMDPs. Accord-
ingly, our definition of stochastic Moore machines that solve POMDPs (defini-
tion 5) applies to these machines.
In addition to such machines, however, we want to include machines whose
states only parameterize (and are not equal to) the probabilistic beliefs over hid-
den states and who output optimal actions. 3 We achieve this by employing an
adapted notion of a consistent interpretation (definition 3). A stochastic Moore
machine can then be interpreted as solving a POMDP if it has this kind of con-
sistent interpretation with respect to the hidden state dynamics of the POMDP
and outputs the optimal policy.
We also show that the machines obeying our definition are optimal in the
same sense as the machines whose states are the correctly updated beliefs, so we
find it justified to interpret those machines as rational agents with beliefs and
goals as well.
Before we go on to the technical part we want to highlight a few more aspects.
The first is that the existence of a consistent interpretation (either in terms of
filtering or in terms of agents) only depends on the stochastic Moore machine
that’s being interpreted, and not on any properties of its environment. This
is because a consistent interpretation requires an agent’s beliefs and goals to
be consistent, and this is different from asking whether they are correct. An
agent may have the wrong model, in that it doesn’t correspond correctly to the
true environment. Its conclusions in this case will be wrong, but its reasoning
can still be consistent; see [16] for further discussion of this point. In the case
of POMDP interpretations this means that the agent’s actions only need to be
optimal according to its model of the environment, but they might be suboptimal
according to the true environment.
This differs from the perspective taken in the original FEP literature con-
cerned with the question of when a system of stochastic differential equations
contain an agent performing approximate Bayesian inference [5,6,14,3,7]. 4 This
3 These machines are probably equivalent to the sufficient information state processes
in [9, definition 2] but establishing this is beyond the scope of this work.
4 The FEP literature includes both publications on how to construct agents that
solve problems (e.g. [8]) and publications on when a system of stochastic differential
Interpreting systems as solving POMDPs 5
literature also interprets a system as modelling hidden state dynamics, but there
the model is derived from the dynamics of the actual environment (the so called
“external states”), and hence cannot differ from it. We consider it helpful to be
able to make a clear distinction between the agent’s model of its environment
and its true environment. The case where the model is derived from the true
environment is an interesting special case of this, but our framework covers the
general case as well. To our knowledge, the possibility of choosing the model
independently from the actual environment in a FEP-like theory was first pro-
posed in [16], and has since also appeared in a setting closer to the original FEP
one [13].
We will see here (definition 3) that the independence of model from actual
environment extends to actions in some sense. Even a machine without any out-
puts can have a consistent interpretation modelling an influence of the internal
state on the hidden state dynamics even though it can’t have an influence on
the actual environment. Such “actions” remain confined to the interpretation.
Another aspect of using consistent interpretations of the internal state and
thus the analogue of the physical state / the physical constituents of the system
is that it automatically comes with a notion of coarse-graining of the internal
state. Since interpretations map the internal state to beliefs but don’t need to
do so injectively they can include coarse-graining of the state.
Also note, all our current notions of interpretation in terms of Bayesian beliefs
require exact Bayesian updating. This means approximate versions of Bayesian
inference or filtering are outside of the scope. This limits the scope of our example
definition in comparison with the FEP which, as mentioned, also uses beliefs
parameterized by internal states but considers approximate inference. On the
other hand this keeps the involved concepts simpler.
Finally, we want to mention that [11] recently proposed an agent discovery
algorithm. This algorithm is based on a definition of agents that takes into
account the creation process of the system. An agent discovery algorithm based
on the approach presented here would take as input a machine (definition 1) or a
stochastic Moore machine (definition 2) and try to find a POMDP interpretation
(definition 5). The creation process of the machine (system) would not be taken
into account. This is one distinction between our notion of an agent and that of
[11]. A more detailed comparison would be interesting but is beyond the scope
of this work.
The rest of this manuscript presents the necessary formal definitions that
allow us to precisely state our example of an agent definition.
2 Interpreting stochastic Moore machines
Throughout the manuscript we write PX for the set of all finitely supported
probability distributions over a set X. This ensures that all probability distri-
equations contain an agent performing approximate Bayesian inference. Only the
latter literature addresses a question comparable to the one addressed in the present
manuscript.
6 M. Biehl, N. Virgo
butions we consider only have a finite set of outcomes that occur with non-zero
probability. We can then avoid measure theoretic language and technicalities.
For two sets X, Y a Markov kernel is a function ζ : X →PY. We write ζ(y|x)
for the probability of y ∈ Yaccording to the probability distribution ζ(x) ∈ PY.
If we have a function f : X → Ywe sometimes write δf : X →PY for the
Markov kernel with δf(x)(y) (which is 1 if y = f(x) and 0 else) then defining the
probability of y given x.
We give the following definition, which is the same as the one used in [16], but
specialised to the case where update functions map to the set of finitely supported
probability distributions and not to the space of all probability distributions.
Definition 1. A machine is a tuple (M, I, µ) consisting of a set M called inter-
nal state space; a set I called input space; and a Markov kernel µ : I×M →PM
called machine kernel, taking an input i ∈ Iand a current machine state m ∈ M
to a probability distribution µ(i, m) ∈ PM over machine states.
The idea is that at any given time the machine has a state m ∈ M. At each
time step it recieves an input i ∈ I, and updates stochastically to a new state,
according to a probability distirbution specified by the machine kernel. If we add
a function that specifies an output given the machine state we get the definition
of a stochastic Moore machine.
Definition 2. A stochastic Moore machine is a tuple (M, I, O, µ, ω) consisting
of a machine with internal state space M, input space I, and machine kernel
µ : I × M →PM; a set O called the output space; and a function ω : M → O
called expose function taking any machine state m ∈ Mto an output ω(m) ∈ O.
Note that the expose function is an ordinary function and not stochastic.
We need to adapt the definition of a consistent Bayesian filtering interpre-
tation [16, Definition 2]. For our purposes here we need to include models of
dynamic hidden states that can be influenced. In particular we need to interpret
a machine as modelling the dynamics of a hidden state that the machine itself
can influence. This suggests that the interpretation includes a model of how the
state of the machine influences the hidden state. We here call such influences
“actions” and the function that takes states to actions action kernel.
Definition 3. Given a machine with state space M, input space I and machine
kernel µ : I × M →PM, a consistent Bayesian influenced filtering interpre-
tation (H, A, ψ, α, κ) consists of a set H called the hidden state space ; a set A
called the action space; a Markov kernel ψ : M →PH called interpretation map
mapping machine states to probability distributions over the hidden state space; a
function α : M → Acalled action function mapping machine states to actions 5;
and a Markov kernel κ : H × A →P(H × I) called the model kernel mapping
pairs (h, a) of hidden states and actions to probability distributions κ(h, a) over
pairs (h′, i) of next hidden states and an input.
5 We choose actions to be deterministic functions of the machine state because the
stochastic Moore machines considered here also have deterministic outputs. Other
choices may be more suitable in other cases.
Interpreting systems as solving POMDPs 7
These components have to obey the following equation. First, in string dia-
gram notation (see appendix A of [16] for an introduction to string diagrams for
probability in a similar context to the current paper):
M M
ψ
α I
H
A κ
µ
I
H
=
M M
ψ
α I
H
A κ I
µ
ψ H
,
(1)
Second, in more standard notation, we must have for each m ∈ M, h′ ∈ H,
i ∈ I, and m′ ∈ M:
 X
h∈H
X
a∈A
κ(h′, i|h, a)ψ(h|m)δα(m)(a)
!
µ(m′|i, m) =
ψ(h′|m′)
 X
h∈H
X
a∈A
X
h′′∈H
κ(h′′, i|h, a)ψ(h|m)δα(m)(a)
!
µ(m′|i, m).
(2)
In appendix A we show how to turn eq. (2) into a more familiar form.
Note that we defined consistent Bayesian influenced filtering interpretations
for machines that have no actual output but that it also applies to those with
outputs. If we want an interpretation of a machine with outputs we may choose
the action space as the output space and the action kernel as the output kernel,
but we don’t have to. Interpretations can still be consistent.
Also note that when A is a space with only one element we recover the
original definition of a consistent Bayesian filtering interpretation from [16].
3 Interpreting stochastic Moore machines as solving
POMDPs
Definition 4. A partially observable Markov decision process (POMDP) can
be defined as a tuple (H, A, S, κ, r) consisting of a set H called the hidden state
space; a set A called the action space; a set S called the sensor space; a Markov
kernel κ : H × A →P(H × S) called the transition kernel taking a hidden state
h and action a to a probability distribution over next hidden states and sensor
values; and a function r : H × A →R called the reward function returning a
real valued reward depending on the hidden state and an action.
To solve a POMDP we have to choose a policy (as defined below) that max-
imizes the expected cumulative reward either for a finite horizon or discounted
with an infinite horizon. We only deal with the latter case here.
POMDPs are commonly solved in two steps. First since the hidden state is
unknown, probability distributions b ∈ PH (called belief states) over the hidden
state are introduced and an updating function f : PH × A × S →PH for these
belief states is defined. This updating is directly derived from Bayes rule [10]:
b′(h′) = f(b, a, s)(h′) :=P r(h′|b, a, s) :=
P
h∈H κ(h′, s|h, a)b(h)P
¯h,¯h′∈H κ(¯h′, s|¯h, a)b(¯h). (3)
8 M. Biehl, N. Virgo
(Note that an assumption is that the denominator is greater than zero.) Then
an optimal policy π∗ : PH → Amapping those belief states to actions is derived
from a so-called belief state MDP (see appendix D for details). The optimal
policy can be expressed using an optimal value function V ∗ : PH →R that
solves the following Bellman equation [9]:
V ∗(b) = max
a∈A


X
h∈H
b(h)r(h, a) + γ
X
s∈S
h,h′∈H
κ(h′, s|h, a)b(h)V ∗(f(b, a, s))

. (4)
The optimal policy is then [9]:
π∗(b) = arg max
a∈A


X
h∈H
b(h)r(h, a) + γ
X
s∈S
h,h′∈H
κ(h′, s|h, a)b(h)V ∗(f(b, a, s))

. (5)
Note that the belief state update function f determines optimal value function
and policy.
Define now fπ∗(b, s) := f(b, π∗(b), s). Then note that if we consider PH a
state space, S an input space, A an output space, δfπ∗ : PH × S →P PH a
machine kernel, and π∗ : PH → Aan expose kernel, we get a stochastic Moore
machine.6
This machine solves the POMDP and can be directly interpreted as a rational
agent with beliefs and a goal. The beliefs are just the belief states themselves,
the goal is expected cumulative reward maximization, and the optimal policy
ensures it acts rationally with respect to the goal.
Our definition of interpretations of stochastic Moore machines as solutions to
POMDPs includes this example and extends it to machines whose states aren’t
probability distributions / belief states directly but instead are parameters of
such belief states that get (possibly stochastically) updated consistently.
We now state this main definition and then a proposition that ensures that
our definition only applies to stochastic Moore machines that parameterize be-
liefs correctly as required by eq. (3). This ensures that the optimal policy ob-
tained via eq. (5) is also the optimal policy for the states of the machine.
Definition 5. Given a stochastic Moore machine (M, I, O, µ, ω), a consistent
interpretation as a solution to a POMDP is given by a POMDP (H, O, I, κ, r)
and an interpretation map ψ : M →PH such that (i) (H, O, ψ, ω, κ) is a consis-
tent Bayesian influenced filtering interpretation of the machine part (M, I, µ) of
the stochastic Moore machine; and (ii) the machine expose function ω : M → O
(which coincides with the action function in the interpretation) maps any ma-
chine state m to the action π∗(ψ(m)) specified by the optimal POMDP policy for
the belief ψ(m) associated to machine state m by the interpretation. Formally:
ω(m) =π∗(ψ(m)). (6)
6 If the denominator in eq. (3) is zero for some values ∈ Sthen define e.g.fπ∗(b, s) = b.
Interpreting systems as solving POMDPs 9
Note that the machine never gets to observe the rewards of the POMDP we
use to interpret it. An example of a stochastic Moore machine together with an
interpretation of it as a solution to a POMDP is given in appendix C.
Proposition 1. Consider a stochastic Moore machine (M, I, O, µ, ω), together
with a consistent interpretation as a solution to a POMDP, given by the POMDP
(H, O, I, κ, r) and Markov kernel ψ : M →PH. Suppose it is given an input i ∈
I, and that this input has a positive probability according to the interpretation.
(That is, eq. (14) is obeyed.) Then the parameterized distributions ψ(m) update
as required by the belief state update equation (eq. (3)) whenever a = π∗(b)
i.e. whenever the action is equal to the optimal action. More formally, for any
m, m′ ∈ Mwith µ(m′|i, m) > 0 and i ∈ Ithat can occur according to the
POMDP transition and sensor kernels, we have for all h′ ∈ H
ψ(h′|m′) = f(ψ(m), π∗(ψ(m)), i)(h′). (7)
Proof. See appendix B.
With this we can see that if V ∗ is the optimal value function for belief states
b ∈ PH of eq. (4), then ¯V ∗(m) := V ∗(ψ(m)) is an optimal value function on the
machine’s state space with optimal policy ω(m) = π∗(ψ(m)).
4 Conclusion
We proposed a definition of when an stochastic Moore machine can be inter-
preted as solving a partially observable Markov decision process (POMDP). We
showed that standard solutions of POMDPs have counterpart machines that this
definition applies to. Our definition employs a newly adapted version of a consis-
tent interpretation. We showed that with this our definition includes additional
machines whose state spaces are parameters of probabilistic beliefs and not such
beliefs directly. We suspect these machines are closely related to information
state processes [9] but the precise relation is not yet known to us.
References
1. Beer, R.D.: The cognitive domain of a glider in the game of life. Artificial Life
20(2), 183–206 (2014). https://doi.org/10.1162/ARTL a 00125
2. Biehl, M., Ikegami, T., Polani, D.: Towards information based spatiotemporal
patterns as a foundation for agent representation in dynamical systems. In: Pro-
ceedings of the Artificial Life Conference 2016. pp. 722–729. The MIT Press (Jul
2016). https://doi.org/10.7551/978-0-262-33936-0-ch115, https://mitpress.mit.
edu/sites/default/files/titles/content/conf/alife16/ch115.html
3. Da Costa, L., Friston, K., Heins, C., Pavliotis, G.A.: Bayesian Mechanics for
Stationary Processes. arXiv:2106.13830 [math-ph, physics:nlin, q-bio] (Jun 2021),
http://arxiv.org/abs/2106.13830, arXiv: 2106.13830
10 M. Biehl, N. Virgo
4. Dennett, D.C.: True Believers : The Intentional Strategy and Why It Works. In:
Heath, A.F. (ed.) Scientific Explanation: Papers Based on Herbert Spencer Lec-
tures Given in the University of Oxford, pp. 53–75. Clarendon Press (1981)
5. Friston, K.: Life as we know it. Journal of The Royal Society Inter-
face 10(86) (Sep 2013). https://doi.org/10.1098/rsif.2013.0475, http://rsif.
royalsocietypublishing.org/content/10/86/20130475
6. Friston, K.: A free energy principle for a particular physics. arXiv:1906.10184 [q-
bio] (Jun 2019), http://arxiv.org/abs/1906.10184, arXiv: 1906.10184
7. Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzh¨ offer, K., Pavliotis,
G.A., Parr, T.: The free energy principle made simpler but not too simple
(Jan 2022). https://doi.org/10.48550/arXiv.2201.06387, http://arxiv.org/abs/
2201.06387, number: arXiv:2201.06387 arXiv:2201.06387 [cond-mat, physics:nlin,
physics:physics, q-bio]
8. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., Pezzulo, G.:
Active inference and epistemic value. Cognitive Neuroscience6(4), 187–214 (2015).
https://doi.org/10.1080/17588928.2015.1020053
9. Hauskrecht, M.: Value-Function Approximations for Partially Observable Markov
Decision Processes. Journal of Artificial Intelligence Research 13, 33–94
(Aug 2000). https://doi.org/10.1613/jair.678, http://arxiv.org/abs/1106.0234,
arXiv:1106.0234 [cs]
10. Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in
partially observable stochastic domains. Artificial Intelligence 101(1–2), 99–
134 (May 1998). https://doi.org/10.1016/S0004-3702(98)00023-X, http://www.
sciencedirect.com/science/article/pii/S000437029800023X
11. Kenton, Z., Kumar, R., Farquhar, S., Richens, J., MacDermott, M., Everitt, T.:
Discovering Agents (Aug 2022). https://doi.org/10.48550/arXiv.2208.08345,http:
//arxiv.org/abs/2208.08345, arXiv:2208.08345 [cs]
12. Orseau, L., McGill, S.M., Legg, S.: Agents and Devices: A Relative Definition
of Agency. arXiv:1805.12387 [cs, stat] (May 2018), http://arxiv.org/abs/1805.
12387, arXiv: 1805.12387
13. Parr, T.: Inferential dynamics: Comment on: How particular is the physics of the
free energy principle? by Aguilera et al. Physics of Life Reviews 42, 1–3 (Sep
2022). https://doi.org/10.1016/j.plrev.2022.05.006, https://www.sciencedirect.
com/science/article/pii/S1571064522000276
14. Parr, T., Da Costa, L., Friston, K.: Markov blankets, information geometry and
stochastic thermodynamics. Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences378(2164), 20190159 (Feb 2020).
https://doi.org/10.1098/rsta.2019.0159, https://royalsocietypublishing.org/
doi/full/10.1098/rsta.2019.0159
15. Sondik, E.J.: The Optimal Control of Partially Observable Markov Processes Over
the Infinite Horizon: Discounted Costs. Operations Research 26(2), 282–304 (Mar
1978), http://www.jstor.org/stable/169635
16. Virgo, N., Biehl, M., McGregor, S.: Interpreting Dynamical Systems as Bayesian
Reasoners. arXiv:2112.13523 [cs, q-bio] (Dec 2021), http://arxiv.org/abs/2112.
13523, arXiv: 2112.13523
A Consistency in more familiar form
One way to turn eq. (2) into a probably more familiar form is to introduce some
abbreviations and look at some special cases. We follow a similar strategy to [16].
Interpreting systems as solving POMDPs 11
Let
ψH,I(h′, i|m) :=
X
h∈H
X
a∈A
κ(h′, i|h, a)ψ(h|m)δα(m)(a) (8)
and
ψI(i|m) :=
X
h′∈H
ψH,I(h′, i|m). (9)
Then consider the case of a deterministic machine and choose the m′ ∈ Mthat
actually occurs for a given input i ∈ Isuch that µ(m′|i, m) = 1 or abusing
notation m′ = m′(i, m). Then we get from eq. (2):
ψH,I(h′, i|m) = ψ(h′|µ(i, m))ψI(i|m). (10)
If we then also consider an input i ∈ Ithat is subjectively possible as defined in
[16] which here means that ψI(i|m) > 0 we get
ψ(h′|m′(i, m)) = ψH,I(h′, i|m)
ψI(i|m) . (11)
This makes it more apparent that in the interpretation the updated machine
state m′ = m′(i, m) parameterizes a belief ψ(h′|m′(i, m)) which is equal to the
posterior distribution over the hidden state given inputi. In the non-deterministic
case, note that when µ(m′|i, m) = 0 the consistency equation imposes no condi-
tion, which makes sense since that means the machine state m′ can never occur.
When µ(m′|i, m) > 0 we can divide eq. (2) by this to also get eq. (10). The
subsequent argument for m′ = m′(i, m) then must hold not only for this one
possible next state but instead for every m′ with µ(m′|i, m). So in this case (if s
is subjectively possible) any of the possible next states will parameterize a belief
ψ(h′|m′) equal to the posterior.
B Proof of proposition 1
For the readers’s convenience we recall the proposition:
Proposition 2. Consider a stochastic Moore machine (M, I, O, µ, ω), together
with a consistent interpretation as a solution to a POMDP, given by the POMDP
(H, O, I, κ, r) and Markov kernel ψ : M →PH. Suppose it is given an input i ∈
I, and that this input has a positive probability according to the interpretation.
(That is, eq. (14) is obeyed.) Then the parameterized distributions ψ(m) update
as required by the belief state update equation (eq. (3)) whenever a = π∗(b)
i.e. whenever the action is equal to the optimal action. More formally, for any
m, m′ ∈ Mwith µ(m′|i, m) > 0 and i ∈ Ithat can occur according to the
POMDP transition and sensor kernels, we have for all h′ ∈ H
ψ(h′|m′) = f(ψ(m), π∗(ψ(m)), i)(h′). (12)
12 M. Biehl, N. Virgo
Proof. By assumption the machine part ( M, I, µ) of the stochastic Moore ma-
chine has a consistent Bayesian influenced filtering interpretation (H, O, ψ, ω, κ).
This means that the belief ψ(m) parameterized by the stochastic Moore
machine obeys eq. (2). This means that for every possible next state m′ (i.e.
µ(m′|s, m) > 0) we have
X
h∈H
X
a∈A
κ(h′, i|h, a)ψ(h|m)δω(m)(a) =
ψ(h′|m′)
 X
h∈H
X
a∈A
X
h′′∈H
κ(h′′, i|h, a)ψ(h|m)δω(m)(a)
! (13)
and for every subjectively possible input, that is, for every input i ∈ Iwith
X
h∈H
X
a∈A
X
h′′∈H
κ(h′′, i|h, a)ψ(h|m)δω(m)(a) > 0 (14)
(see below for a note on why this assumption is reasonable) we will have:
ψ(h′|m′) =
P
h∈H
P
a∈A κ(h′, i|h, a)ψ(h|m)δω(m)(a)P
h∈H
P
a∈A
P
h′′∈H κ(h′′, i|h, a)ψ(h|m)δω(m)(a) (15)
=
P
h∈H κ(h′, i|h, ω(m))ψ(h|m)P
h∈H
P
h′′∈H κ(h′′, i|h, ω(m))ψ(h|m). (16)
Now consider the update function for which the optimal policy is found eq. (3):
f(b, a, s)(h′) :=
P
h∈H κ(h′, s|h, a)b(h)P
¯h,¯h′∈H κ(¯h′, s|¯h, a)b(¯h) (17)
and plug in the belief b = ψ(m) parameterized by the machine state, the optimal
action π∗(ψ(m)) specified for that belief by the optimal policy π∗, and the s = i:
f(ψ(m), π∗(m), i)(h′) :=
P
h∈H κ(h′, i|h, π∗(ψ(m)))ψ(m)(h)P
¯h,¯h′∈H κ(¯h′, i|¯h, π∗(ψ(m)))ψ(m)(¯h). (18)
Also introduce κ and write ψ(h|m) for ψ(m)(h) as usual
f(ψ(m), π∗(m), i)(h′) :=
P
h∈H κ(h′, i|h, π∗(ψ(m)))ψ(h|m)P
¯h,¯h′∈H κ(¯h′, i|¯h, π∗(ψ(m)))ψ(¯h|m) (19)
=ψ(h′|m′). (20)
Which is what we wanted to prove.
Note that if eq. (14) is not true and the probability of an input i is impossible
according to the POMDP transition function, the kernel ψ, and the optimal
policy ω then eq. (13) puts no constraint on the machine kernel µ since both
sides are zero. So the behavior of the stochastic Moore machine in this case
is arbitrary. This makes sense since according to the POMDP that we use to
interpret the machine this input is impossible, so our interpretation should tell
us nothing about this situation.
Interpreting systems as solving POMDPs 13
C Sondik’s example
We now consider the example from [15]. This has a known optimal solution. We
constructed a stochastic Moore machine from this solution which has an inter-
pretation as a solution to Sondik’s POMDP. This proves existence of stochastic
Moore machines with such interpretations.
Consider the following stochastic Moore machine:
– State space M := [0, 1]. (This state will be interpreted as the belief proba-
bility of the hidden state being equal to 1.)
– input space I = {1, 2}
– machine kernel µ : I × M →PM defined by deterministic function g :
I × M → M:
µ(m′|s, m) := δg(s,m)(m′) (21)
where
g(S = 1, m) :=
(
15
6m+20 − 1
2 if 0 ≤ m ≤ 0.1188
9
5 − 72
5m+60 if 0.1188 ≤ m ≤ 1. (22)
and
g(S = 2, m) :=
(
2 + 20
3m−15 if 0 ≤ m ≤ 0.1188
−1
5 − 12
5m−40 if 0.1188 ≤ m ≤ 1. (23)
– output space O := {1, 2}
– expose kernel ω : M → Odefined by
ω(m) :=
(
1 if 0 ≤ m <0.1188
2 if 0.1188 ≤ m ≤ 1. (24)
A consistent interpretation as a solution to a POMDP for this stochastic Moore
machine is given by
– The POMDP with
• state space H := {1, 2}
• action space equal to the output space O of the machine above
• sensor space equal to the input space I of the machine above
• model kernel κ : H × O → H × Idefined by
κ(h′, s|h, a) := ν(h′|h, a)ϕ(s|h′, a) (25)
where ν : H × O →PH and ϕ : H × O →PI are shown in table 1
• reward function r : H × O →R also shown in table 1.
– Markov kernel ψ : M →PH given by:
ψ(h|m) := mδ1(h)(1 − m)δ2(h). (26)
14 M. Biehl, N. Virgo
Action a ∈ Oν(h′|h, A= a) ϕ(s|h′, A= a) r(h, A= a)
1


1/5 1/2
4/5 1/2




1/5 3/5
4/5 2/5


 
4
−4
!
2


1/2 2/5
1/2 3/5




9/10 2/5
1/10 3/5


 
0
−3
!
Table 1.Sondik’s POMDP data.
To verify this we have to check that ( H, O, ψ, ω, κ) is a consistent Bayesian
influenced filtering interpretation of the machine ( M, I, µ). For this we need to
check eq. (2) with δα(m)(a) := δω(m)(a). So for each each m ∈ [0, 1], h′ ∈ {1, 2},
i ∈ {1, 2}, and m′ ∈ [0, 1] we need to check:
 X
h∈H
X
a∈A
κ(h′, i|h, a)ψ(h|m)δω(m)(a)
!
µ(m′|i, m) =
ψ(h′|m′)
 X
h∈H
X
a∈A
X
h′′∈H
κ(h′′, i|h, a)ψ(h|m)δω(m)(a)
!
µ(m′|i, m).
(27)
This is tedious to check but true. We would usually also have to show that ω is
indeed the optimal policy for Sondik’s POMDP but this is shown in [15].
D POMDPs and belief state MDPs
Here we give some more details about belief state MDPs and the optimal value
function and policy of eqs. (4) and (5). There is no original content in this section
and it follows closely the expositions in [9,10].
We first define an MDP and its solution and then discuss then add some
details about the belief state MDP associated to a POMDP.
Definition 6. A Markov decision process (MDP) can be defined as a tuple
(X, A, ν, r) consisting of a set X called the state space, a set A called the action
space, a Markov kernel ν : X × A →P(X) called the transition kernel, and a
reward function r : X × A →R. Here, the transition kernel takes a state x ∈ X
and an action a ∈ Ato a probability distribution ν(x, a) over next states and the
reward function returns a real-valued instantaneous reward r(x, a) depending on
the hidden state and an action.
A solution to a given MDP is a control policy. As the goal of the MDP we
here choose the maximization of expected cumulative discounted reward for an
infinite time horizon (an alternative would be to consider finite time horizons).
This means an optimal policy maximizes
E
" ∞X
t=1
γt−1r(xt, at)
#
. (28)
Interpreting systems as solving POMDPs 15
where 0 < γ <1 is a parameter called the discount factor. This specifies the
goal.
To express the optimal policy explicitly we can use the optimal value function
V ∗ : X →R. This is the solution to the Bellman equation [10]:
V ∗(x) = max
a∈A
 
r(x, a) + γ
X
x′∈X
ν(x′|a, x)V ∗(x′)
!
. (29)
The optimal policy is then the function π∗ : X → Athat greedily maximizes the
optimal value function [10]:
π∗(x) = arg max
a∈A
 
r(x, a) + γ
X
x′∈X
ν(x′|a, x)V ∗(x′)
!
. (30)
D.1 Belief state MDP
The belief state MDP for a POMDP (see definition 4) is defined using the belief
state update function of eq. (3). We first define this function again here with an
additional intermediate step:
f(b, a, s)(h′) :=P r(h′|b, a, s) (31)
=P r(h′, s|b, a)
P r(s|b, a) (32)
=
P
h∈H κ(h′, s|h, a)b(h)P
¯h,¯h′∈H κ(¯h′, s|¯h, a)b(¯h). (33)
The function f(b, a, s) returns the posterior belief over hidden states h given
prior belief b ∈ PH, an action a ∈ Aand observation s ∈ S. The Markov kernel
δf : PH×S ×A →P PH associated to this function can be seen as a probability
of the next belief state b′ given current belief state b, action a and sensor value
s:
P r(b′|b, a, s) = δf(b,a,s)(b′). (34)
Intuitively, the belief state MDP has as its transition kernel the probability
P r(b′|b, a) expected over all next sensor values of the next belief state b′ given
that the current belief state is b the action is a and beliefs get updated according
to the rules of probability, so
P r(b′|b, a) =
X
s
P r(b′|b, a, s)P r(s|b, a) (35)
=
X
s∈S
δf(b,a,s)(b′)
X
h,h′∈H
κ(h′, s|h, a)b(h). (36)
This gives some intuition behind the definition of belief state MDPs.
16 M. Biehl, N. Virgo
Definition 7. Given a POMDP (H, A, S, κ, r) the associated belief state Markov
decision process (belief state MDP) is the MDP (PH, A, β, ρ) where
– the state space PH is the space of probability distributions beliefs over the
hidden state of the POMDP. We write b(h) for the probability of a hidden
state h ∈ Haccording to belief b ∈ PH.
– the action space A is the same as for the underlying POMDP
– the transition kernel κ : PH ×A → PH is defined as [10, Section 3.4]
β(b′|b, a) :=
X
s∈S
δf(b,a,s)(b′)
X
h,h′∈H
κ(h′, s|h, a)b(h). (37)
– the reward function ρ : PH × A →R is defined as
ρ(b, a) :=
X
h∈H
b(h)r(h, a). (38)
So the reward for action a under belief b is equal to the expectation under
belief b of the original POMDP reward of that action a.
D.2 Optimal belief-MDP policy
Using the belief MDP we can express the optimal policy for the POMDP.
The optimal policy can be expressed in terms of the optimal value function
of the belief MDP . This is the solution to the equation [9]
V ∗(b) = max
a∈A
 
ρ(b, a) + γ
X
b′∈PH
β(b′|a, b)V ∗(b′)
!
(39)
V ∗(b) = max
a∈A

ρ(b, a) + γ
X
b′∈PH
X
s∈S
δf(b,a,s)(b′)
X
h,h′∈H
κ(h′, s|h, a)b(h)V ∗(b′)


(40)
V ∗(b) = max
a∈A

ρ(b, a) + γ
X
s∈S
X
h,h′∈H
κ(h′, s|h, a)b(h)V ∗(f(b, a, s))

. (41)
This is the expression we used in eq. (4). The optimal policy for the belief MDP
is then [9]:
π∗(b) = arg max
a∈A

ρ(b, a) + γ
X
s∈S
X
h,h′∈H
κ(h′, s|h, a)b(h)V ∗(f(b, a, s))

. (42)
This is the expression we used in eq. (5).