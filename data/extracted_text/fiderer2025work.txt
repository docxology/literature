The Work Capacity of Channels with Memory:
Maximum Extractable Work in Percept-Action Loops
Lukas J. Fiderer, ∗ Paul C. Barth, Isaac D. Smith, and Hans J. Briegel
Universit¨ at Innsbruck, Institut f¨ ur Theoretische Physik, Technikerstraße 21a, A-6020 Innsbruck, Austria
Predicting future observations plays a central role in machine learning, biology, economics, and
many other fields. It lies at the heart of organizational principles such as the variational free energy
principle and has even been shown—based on the second law of thermodynamics—to be necessary
for reaching the fundamental energetic limits of sequential information processing. While the use-
fulness of the predictive paradigm is undisputed, complex adaptive systems that interact with their
environment are more than just predictive machines: they have the power to act upon their envi-
ronment and cause change. In this work, we develop a framework to analyze the thermodynamics of
information processing in percept-action loops—a model of agent–environment interaction—allowing
us to investigate the thermodynamic implications of actions and percepts on equal footing. To this
end, we introduce the concept of work capacity—the maximum rate at which an agent can expect
to extract work from its environment. Our results reveal that neither of two previously estab-
lished design principles for work-efficient agents—maximizing predictive power and forgetting past
actions—remains optimal in environments where actions have observable consequences. Instead,
a trade-off emerges: work-efficient agents must balance prediction and forgetting, as remembering
past actions can reduce the available free energy. This highlights a fundamental departure from the
thermodynamics of passive observation, suggesting that prediction and energy efficiency may be at
odds in active learning systems.
I. INTRODUCTION
Percept-action loops—cycles in which an agent per-
ceives its environment, processes and stores information,
and acts to influence future perception—underlie adap-
tive behavior in both biological and artificial systems.
Such loops can be observed across various domains, from
humans learning chess, to animals foraging, to artificial
intelligence models engaging in dialogue. Despite the di-
verse range of examples, certain principles governing the
energetics of these processes are shared across domains.
Energetic considerations in biology have been linked
to a wide range of animal behaviors and physiological
processes. An example from the former includes the
energy-saving flight patterns of albatrosses [1] and from
the latter information processing in the brain, where en-
ergy consumption associated with neural signaling is min-
imized through efficient coding strategies [2, 3]. At the
molecular level, ribosomes have been shown to perform
simple decoding computations at energy costs within an
order of magnitude of Landauer’s bound—significantly
outperforming even the most advanced supercomputers
[4]. Indeed, in artificial intelligence, the energetic cost of
supercomputers is becoming an increasing concern, par-
ticularly in the training of large neural networks [5], re-
sulting in performance-power trade-offs in large language
models [6].
This raises fundamental questions: What are the ener-
getic limits of adaptive information processing in percept-
action loops? And how should efficient agents be de-
signed?
These questions can be tackled by reducing the prob-
∗ lukasjfiderer@gmail.com
FIG. 1. Tape setting (a) and percept-action loop setting (b).
In the tape setting, (a), an agent processes symbols St from
a pre-existing tape. Outgoing symbols At do not influence
future inputs. In the percept-action loop setting, (b), the
agent interacts with an environment (Env.) in rounds. In
round t, the agent provides an action symbol At and receives
a percept symbol St from the environment. Both the agent
and environment can have memory, allowing future percepts
to depend on past actions.
lem to an information-theoretic model of percept-action
loops. By abstracting away implementation-dependent
details, we derive energetic bounds that arise solely from
the intrinsic cost of information processing, as analyzed
through nonequilibrium thermodynamics [7, 8]. Indeed,
inspired by Maxwell’s demon, nonequilibrium thermody-
namics has been applied to investigate energetics in the
tape setting (see Figure 1), where agents sequentially pro-
cess and modify symbols on a pre-existing tape [9–22].
In this framework, predictable correlations in the input
serve as an energetic resource, while generating correla-
tions in the output incurs an energy cost. However, exist-
ing works typically assume stationary input patterns and
exclude feedback between agent and environment, leav-
arXiv:2504.06209v1  [cs.LG]  8 Apr 2025
2
ing the thermodynamics of genuine percept-action loops
largely unexplored (but see [23] for a recent exception,
investigating quantum processes with feedback).
In this work, we model both agent and environment
as hidden Markov channels. By combining results from
stochastic thermodynamics [7, 8] with the information
theory of hidden Markov models [24], we obtain a frame-
work which goes beyond prior work situated in the tape
setting by relaxing the assumption of stationary input
patterns and incorporating feedback between agent and
environment. This framework is the primary contribu-
tion of this work.
The central quantity of this work is the work
capacity—the optimal rate of energy production achiev-
able by any agent—which, analogous to communication
capacity, is an intrinsic information-theoretic property of
the environment channel.
The investigation of work capacity in the framework
developed here leads to two key results: (i) in the ab-
sence of feedback, where the agent’s percepts are not
influenced by its actions, we extend prior results [17]
beyond the stationary regime, showing that agents can
reach the work capacity of the environment if and only
if they are maximally predictive of their percepts while
choosing actions randomly, without retaining memory of
them; (ii) in the presence of feedback, maximally pre-
dictive agents are generally inefficient. This counterintu-
itive result highlights crucial distinctions between cyclic
information processing in percept-action loops and linear
information processing on a tape.
In the following sections, we first introduce the
percept-action loop framework (Section II) and define
what it means for an agent to be maximally predictive
(Section III). We then present our results on the work ca-
pacity of channels (Section IV) and the design principles
of work-efficient agents (Section V). Finally, we discuss
directions for future research and conclude by situating
our findings in a broader context (Section VI).
II. FRAMEWORK
We consider a classical (as opposed to quantum) agent
interacting with a classical environment in discrete time
steps (in the following called rounds) indexed by t ∈ N0,
where N0 denotes the nonnegative integers. In each
round t, the agent selects an action At and subsequently
receives a percept St (see Figure 1b). Since both the
agent and environment may be stochastic, At and St are
random variables taking values in finite alphabets A and
S. Embedding the smaller alphabet into the larger, lets
us set A = S, which will be assumed in the following.
Throughout this work, random variables are denoted
by capital letters, their realizations by lowercase let-
ters, their alphabets—such as the sets of possible actions
and percepts—by calligraphic letters, and sequences of
random variables—interpreted as random variables on a
product space—by At:n = (At, At+1, . . . , An−1). Infinite
sequences, known as stochastic processes, are written as
A = A0:∞, with analogous notation for their realizations,
at:n = (at, at+1, . . . , an−1) ∈ An and a = a0:∞ ∈ AN0 .
The environment and agent are described by channels
(conditional probability distributions) νenv
S|A and ηagt
A|S,
which stochastically map actions A to percepts S and
vice versa. We assume these channels are causal (re-
specting time ordering such that future outputs cannot
influence past inputs) and admit a finite memory imple-
mentation. The finite-memory assumption is both prac-
tical and ensures well-behaved asymptotics in percept-
action loops. These constraints define what is commonly
referred to in the literature as a finite-state [25] or hidden
Markov channel [24] (see Supplemental Material C for a
more in-depth exposition).
Definition 1. A channel νenv
S|A is an environment
channel, denoted as
env := νenv
S|A, (1)
if there exists a finite set of states Z, a distribution pZ0
over Z, and a transition matrix Φ = (ϕ(j|i))j,i with i ∈
A × Zand j ∈ S × Zsuch that
νenv
S|A(s|a) =
X
z
penv
Z0 (z0)
∞Y
t=0
ϕenv (st, zt+1|at, zt) (2)
where the sum runs over all z ∈ ZN0 . Then, the tuple
envM := (Φenv, penv
Z0 ) (3)
is called a (hidden Markov) environment model of
νenv
Y |X and z ∈ Zthe hidden states of the model.
While a channel describes only the input-output be-
havior, a hidden Markov model provides an explicit
memory-based mechanism that generates temporal corre-
lations. Agents are defined analogously, with the key dis-
tinction that the agent initiates the percept-action loop
by selecting a first action A0 (see Figure 2):
Definition 2. A channel ηagt
A|S is an agent channel,
denoted as
agt := ηagt
A|S, (4)
if there exists a finite set of states M, a distribution
pagt
A0M0 over A × M, and a transition matrix Θagt =
(θ(j|i))j,i with i ∈ S × Mand j ∈ A × Msuch that
ηagt
A|S(a|s) =
X
m
pagt
A0M0 (a0, m0)
∞Y
t=0
θagt (at+1, mt+1|st, mt) ,
(5)
where the sum runs over all m ∈ MN0 . Then, the tuple
agtM := (Θagt, pagt
A0M0 ) (6)
is called a (hidden Markov) agent model, of agt and
m ∈ Mthe memory states of the model.
3
FIG. 2. Circuit representation of percept-action loops, with
time flowing from left to right. (a) The agent and environment
are modeled as channels with memory. (b) The agent and
environment are represented by their hidden Markov models,
characterized by finite adaptive memories Mt and Zt. The
transition matrices Θ and Φ remain fixed over time.
An agent can be understood as possessing two types
of memory: (i) algorithmic memory, which remains fixed
for all times and stores the agent’s transition matrix Φ,
effectively representing the agent’s algorithm (analogous
to DNA in a biological context), and (ii) adaptive mem-
ory M, which stores information about the past percept-
action sequence and, through the action of Φ, influences
future actions.
With the definitions of agent and environment chan-
nels, as well as their hidden Markov models, we define
percept-action loops as tuples consisting of an agent and
an environment. To highlight that the agent and environ-
ment mutually interact, these are denoted asagt →←env or
agtM →←env, depending on whether the agent is described
by its channel or its model (similarly,env can be replaced
with envM).
Each percept-action loop model corresponds to an as-
sociated stochastic process. For instance, agt →←env de-
termines the input-output behavior of the agent and en-
vironment, thereby defining the percept-action process
AS = ((A0, S0), (A1, S1), . . .) with distribution
pAS = νenv
S|Aηagt
A|S,
see also Figure 2a. Importantly, the stochastic process
corresponding to agtM →←envM, which has a distribution
pMASZ including both the agent’s and the environment’s
hidden memory, can be shown to form a finite-state
Markov chain. This constitutes the global Markov chain
of a percept-action loop (see Supplemental Material D
for a proof):
M0A0S0Z0 → M1A1S1Z1 → ···.
This Markovian property allows us to leverage exist-
ing results on finite-state Markov chains, ensuring that
the asymptotic dynamics of percept-action loops are well-
behaved [26, 27] (see Supplemental Material B for an
overview).
III. MAXIMALLY PREDICTIVE AGENTS
For a given input-output behavior of the agent and en-
vironment, agt →←env, what does it mean for an agent
to be as predictive as possible of its future percepts?
To approach this question, it is helpful to begin with
the following observation. In order to endow the agent
with knowledge that reduces uncertainty about future
percepts, a natural first step is to encode agtM →←env =
(Θagt, pagt
A0M0 , νenv
S|A) into its fixed algorithmic memory. In
what follows, we assume this is always the case.
With this setup, the agent has access to the distribu-
tion of the underlying process, pMAS , which results in
an uncertainty H(St) about percept St, where H(St) de-
notes Shannon entropy in units of bits. If, in addition, the
agent takes its memory Mt into account before observing
St, this memory reduces the agent’s expected (with re-
spect to memory states) uncertainty to H(St|Mt). This
reduction in uncertainty,
I [Mt; St] = H(St) − H(St|Mt), (7)
is simply the mutual information I [Mt; St] between St
and Mt, quantifying how much Mt enables the agent to
predict St (see Supplemental Material A for some back-
ground on information measures).
To enhance its predictive capabilities, the agent can
store information from past percepts S0:t and actions
A0:t+1 in its memory. Since the information that
S0:tA0:t+1 provides about St is given by I [S0:tA0:t+1; St],
we arrive at the following
Definition 3. Let agt →←env be a percept-action loop. A
model agtM for agt is said to be maximally predictive,
or for short predictive, of percept St in round t if
I [A0:t+1S0:t; St|Mt] = 0, (8)
and an agent model is said to be asymptotically mean
(a.m.) predictive if
⟨I [A0:t+1S0:t; St|Mt]⟩t = 0, (9)
where
⟨•⟩t := lim
n→∞
1
n
n−1X
t=0
• (10)
denotes the Ces´ aro limit, the limit of the arithmetic
mean.
Note that eq. (8) expresses that the agent’s memory
Mt contains at least all the information from the past,
S0:tA0:t+1, which helps predicting the next percept, St,
(see Figure 3), while eq. (9) requires this condition to
hold asymptotically on average [28].
Interestingly, although environments, as per Defini-
tion 1, are constrained to a finite number of hidden
states, an agent may require a countably infinite num-
ber of memory states to be predictive as t → ∞. This
4
Mt
At St
Zt
Mt+1
At+1
Zt+1
Mt−1
At−1 St−1
Zt−1
FIG. 3. Bayesian network for a percept-action loop. Shown
is a fragment for rounds t − 1, t, and the beginning of round
t + 1. This type of Bayesian network plays an important
role in the information-theoretic framework underlying our
results (see Supplemental Material E for details). Note that
to faithfully represent the dynamics of the agent and envi-
ronment, auxiliary nodes (gray and reduced in size) are in-
cluded. The colorized nodes illustrate the condition for an
agent to be maximally predictive in roundt: the agent’s mem-
ory (blue) must store all information from past actions and
percepts S0:tA0:t+1 (red) that is relevant for predicting the
current percept St (green).
is because the agent’s memory, which can be seen as a
function of the past, A0:t+1S0:t, must serve as a sufficient
statistic for St for eq. (8) to vanish [29]. Computational
mechanics shows that there are channels that admit suf-
ficient statistics only with a countably infinite number of
states [30]. In this work, instead of allowing agents infi-
nite memory, we consider so-called unifilar environment
channels [25, 31], for which there always exist predictive
and a.m. predictive agents.
Definition 4. An environment model envM = (Φ , pZ0 )
is said to be unifilar if
• pZ0 is a delta distribution and
• H(Zt+1|At, St, Zt) = 0 for all t ∈ N0.
An environment channel env is said to be unifilar if there
exists a unifilar model for it.
Unifilar models have the useful property that the val-
ues of At, St, and Zt fully determine the value of Zt+1
for all rounds t, enabling an agent, given the initial value
of Z0, to perfectly track the hidden state of the environ-
ment. The following theorem is based on this insight:
Theorem 1. Let agt →←env be any percept-action loop.
If the environment channel is unifilar, then there exists
an a.m. predictive agent model agtM for agt.
See Supplemental Material F for a proof. Before pre-
senting our results on work capacity, we first demonstrate
that Definition 3 for predictive agents recovers the defini-
tion previously used in the context of stationary processes
in the tape setting [17]. Note that a process X is sta-
tionary if pXn:m = pXn+t:m+t for all n, t∈ N0 and m > n.
The tape setting can be embedded within the percept-
action loop framework [32] by making the environment
channel effectively generate the tape pattern, i.e., it acts
as a finite-state source of percepts unaffected by actions:
νS|A(s|a) = νS|A(s|a′) for all a, a′ ∈ AN0 . Channels
with this property are also known as product channels
[33].
The following theorem reveals a remarkable property
of predictive agents in the stationary regime: being
a.m. predictive of the next percept is equivalent to be-
ing predictive of all future percepts at all times.
Theorem 2. Let agtM →←env be such that the joint pro-
cess MAS of actions, percepts, and agent memory is
stationary. Then, agtM is a.m. predictive, i.e.,
⟨I[A0:t+1S0:t; St|Mt]⟩t = 0 (11)
if and only if
I[A0:t+1S0:t; St:∞|Mt] = 0 ∀t ∈ N0. (12)
If in addition env is a product channel, agtM is
a.m. predictive if and only if
I[S0:t; St:∞|Mt] = 0 ∀t ∈ N0. (13)
See Supplemental Material F for a proof utilizing the
Markov conditions of the underlying Bayesian network
(see Figure 3). The second part of the theorem connects
our definition of a.m. predictive agents to the one by Boyd
et al. [17] who define predictive agents via eq. (13) and
another condition which is automatically fulfilled for the
type of channels considered in this work (see [34] for a
different notion of predictive agents) [35].
IV. WORK CAPACITY OF CHANNELS
So far, we have treated agents and environments as
abstract information-processing systems. However, as
Landauer famously quipped, information is physical [36]:
any implementation of an agent must ultimately rely
on physical memory and dynamics subject to thermo-
dynamic laws. To analyze the energetic limits of such
implementations, we adopt a framework from stochas-
tic thermodynamics that models the agent’s information
processing—described by its transition matrix Φ agt—as
a physical process acting on memory [7, 8]. We briefly
outline its assumptions.
In this framework, memory is represented by a phys-
ical system coupled to a thermal reservoir at temper-
ature T. The system possesses a few degrees of free-
dom, the information-bearing degrees of freedom, which
are assumed to be meta-stable, i.e., their equilibration
time τinfo is much larger than that of the system’s other
degrees of freedom, τothers. Information processing on
the information-bearing degrees of freedom is carried out
through an isothermal protocol, i.e., a protocol executed
at constant temperature T, with a time scale such that
5
FIG. 4. An agent agt interacting with the cascade of two
environment channel env1 and env2.
τothers ≪ τprotocol ≪ τinfo. The protocol has access to a
work reservoir for storing (or retrieving) work.
Under these assumptions, it can be shown [7, 8] that,
similar to equilibrium thermodynamics, the second law
of thermodynamics sets an upper bound on the ex-
pected amount of work extractable from a system with
information-bearing degrees of freedom X. This upper
bound is a state function, known as the nonequilibrium
free energy, F = U − kBT ln 2H(X), where U is the
memory system’s internal energy, kB is the Boltzmann
constant. Note that we refer to the work as expected be-
cause it is the work that can be expected to be extracted
on average based on the available knowledge about the
input state pX.
In addition, in order to focus on the energetic limits of
information processing alone, we assume that the inter-
nal energy landscape over information-bearing degrees of
freedom is flat and remains unchanged before and after
executing the isothermal protocol, i.e., the internal en-
ergy U does not contribute to the extractable work. Such
a memory model is also known as an information reser-
voir [37]. Then, the second law yields an upper bound
on the expected extractable work W:
W ≤ H (Xout) − H (Xin) , (14)
from implementing Φ on X, mapping Xin to Xout. Here
and throughout, all work expressions are understood to
be in units of kBT ln 2.
The upper bound in eq. (14), imposed by the second
law, is tight in the sense that it can, in principle, be sat-
urated with protocols under idealized conditions. Con-
crete examples of such protocols are given in [16, 17, 19].
While our work is primarily concerned with the fun-
damental limits imposed by the second law, it should
be noted that more realistic and resource-constrained
assumptions can be incorporated [38–41]. Within our
framework, this is most easily achieved when the ex-
tractable work can still be expressed through a state
function, such as in [40], by replacing F with the new
state function.
With this, the work rate, i.e., the asymptotically ex-
pected work per round that an agent model agtM can
extract using the environment channel env is (see Sup-
plemental Material G 1 for a derivation)
W(agtM →←env) = ⟨(H (At|Mt) − H (St|Mt))⟩t , (15)
with the Ces´ aro limit⟨•⟩t defined in eq. (10). The ex-
istence of work rate is not guaranteed for arbitrary pro-
cesses, as it is possible that the Ces´ aro limit in eq. (15)
Environment Channel env Cwork(env)
Noiseless 0
Memoryless Invariant max
pA0

H(S0) − H(A0)

Unifilar Product log |A| −h(S)
TABLE I. Work capacity for different classes of environment
channels (see eq. (18) for a definition of h(S) and Supplemen-
tal Material G 3 for a proof).
does not exist [42]. Note, however, that here the limit
exists because the global Markov chain of the percept-
action loop is asymptotically well-behaved (see Supple-
mental Material B for details). We then arrive at the
following
Definition 5. The work capacity Cwork of an envi-
ronment channel env is defined as
Cwork(env) := max
agtM∈A→←env
W(agtM →←env). (16)
where A→←env denotes the set of all agent models which can
interact with env.
Intuitively, the work capacity captures the maximum
rate at which an agent—optimally tailored to the envi-
ronment channel—can expect to extract work, based on
the second law of thermodynamics. The existing pro-
tocols for implementing transition matrices [16, 17, 19]
can be leveraged to construct optimal protocols for the
agent model agtM which maximizes eq. (16), making it,
in principle, saturable (see Supplemental Material G 2 for
details).
Returning to the question posed at the beginning of
this section, the energetic limits of agents, in terms of
work rate, are determined by the work capacity of the
environment channel.
Next we will provide some general properties of work
capacity:
Theorem 3. For any environment channel env = νenv
S|A,
work capacity Cwork(env) has the following properties:
(i) (Existence) Cwork(env) exists,
(ii) (Bounds) 0 ≤ Cwork(env) ≤ ln |S|,
(iii) (Subadditivity under channel cascade, see Figure 4)
Cwork(env2 ◦ env1) ≤ Cwork(env1) + Cwork(env2).
See Supplemental Material G 3 for a proof. Note that
the bounds in Theorem 3 follow from the canonical
bounds on Shannon entropy.
Due to the Ces´ aro limit, work capacity is generally
difficult to compute. However, for special classes of envi-
ronment channels, the expression for work capacity sim-
plifies, as shown in Table I.
6
FIG. 5. A memoryless invariant environment with binary per-
cept and action alphabets, A = S = {0, 1}. The transition
labels follow the scheme percept|action : transition proba-
bility. The transition on the left (right) corresponds to action
“0” (respectively, “1”).
Fornoiseless environment channels [43], whereAt = St
for all t ∈ N0, the agent can predict a percept precisely
to the extent that it has remembered its previous action,
turning the tradeoff between actions and percepts into
a zero-sum situation: H (At|Mt) − H (St|Mt) = 0, and
work capacity vanishes.
For memoryless invariant environment channels,
where νS|A(s|a) = Q∞
t=0 ϕ (st|at) with the same ϕ for all
t ∈ N0, we show that the absence of memory in the envi-
ronment allows one to reduce the optimization over agent
models in eq. (16) to an optimization over a single action.
For example, consider an environment env, as displayed
in Figure 5, with binary percept and action alphabets,
A = S = {0, 1}, and with transition matrix Φ env given
by its coefficients ϕenv(j|0) = δ0,j and ϕenv(j|1) = 1 /2
for j = 0, 1. For this environment, we find
Cwork(env) = 1
2 ln
3
4 + 1√
2

≃ 0.272 bits, (17)
which, in units of kBT ln 2, is the work capacity of env.
It can be reached by a memoryless agent which in every
round takes action 0 with probability 1 /
√
2.
For unifilar product environment channels, percepts
are not influenced by actions. Consequently, to maximize
the expression in eq. (15), the optimal strategy is to max-
imize H(At|Mt), which corresponds to choosing actions
that are independent, identically distributed, and uni-
formly random. Crucially, the agent must not retain any
information about its action in its memory. This results
in H(At|Mt) = log |A|. The second term in the work
capacity expression, as shown in Table I, is the entropy
rate of the percept process S,
h (S) := lim
n→∞
H (S0:n)
n , (18)
which was introduced by Shannon as the average uncer-
tainty per symbol in a stochastic process [44]. It is also
known, from the information-processing second law [45],
that this entropy rate (in units of kBT ln 2) represents
the maximum rate of expected extractable work from a
stochastic process [46].
V. WORK-EFFICIENT AGENT MODELS
Finding agents that achieve work capacity is challeng-
ing, as it requires solving a nonlinear optimization prob-
lem (eq. (16)). However, for certain classes of environ-
ments, design principles for work-efficient agent models
FIG. 6. Set diagrams illustrating the relationships between
different classes of agent models: those with maximum-
entropy actions (mea), those which are predictive (pred), and
those that are work-efficient (eff). (a) (b) Applies to the mem-
oryless invariant environment channel shown in Figure 5 (see
Theorem 5). Unlike the tape setting, this environment in-
volves feedback, forming a genuine percept-action loop.
can be established. For a given environment env, three
subsets of the set of all agent models play a central role:
• A→←env
mea : the set of random-action agent models. In
the Ces` aro limit, these agents randomize their ac-
tions without retaining memory of them, yielding
⟨H(At|Mt)⟩t = ln |A|. The subscript mea stands
for maximum entropy actions .
• A→←env
pred : the set of a.m. predictive agent models,
which satisfy the a.m. predictive criterion (see Def-
inition 3).
• A→←env
eff : the set of work-efficient agent models, whose
work rate equals the work capacity (eq. (16)) of the
environment.
We now extend the results of Boyd et al. for station-
ary processes [17] by utilizing our framework—along with
the definitions of a.m. predictive and work-efficient agent
models—to encompass all processes that can be gener-
ated by an environment, not just stationary ones:
Theorem 4. For any unifilar product environment chan-
nel env,
A→←env
eff = A→←env
mea ∩ A→←env
pred . (19)
See Figure 6a for a set diagram illustrating the the-
orem. The proof (see Supplemental Material G 4) re-
lies on the expression for work capacity in unifilar prod-
uct environments given in Table I. The assumption of
unifilarity ensures, by Theorem 1, that (finite) predictive
agent models exist. This result establishes two design
principles for work-efficient agents interacting with a po-
tentially nonstationary percept process: (i) randomizing
actions without retaining memory of them and (ii) em-
ploying predictive memory. These two principles can be
directly linked to the two terms of the work rate, eq. (15),
where the first principle ensures that H(At|Mt) is maxi-
mized and the second principle ensures that H(St|Mt) is
minimized.
A natural question then arises: what happens when
actions influence future percepts—that is, in genuine
7
percept-action loops? Based on Theorem 4, one might
expect the same design principles to apply in these sce-
narios. In particular, it is not immediately clear why
an efficient agent model should not be a.m. predictive, as
predicting its percepts reduces the uncertaintyH(St|Mt),
which contributes negatively to the work rate. However,
the following theorem demonstrates that there exist en-
vironments where neither of these previously identified
design principles is compatible with work-efficient agent
models.
Theorem 5. There exist environment channels env such
that the sets A→←env
pred , A→←env
mea , and A→←env
eff are all nonempty
and mutually exclusive.
See Supplemental Material G 4 for a proof, and refer
to Figure 6b for a set diagram illustrating the theorem.
This result underscores a fundamental distinction be-
tween the tape setting and the percept-action loop set-
ting. In order to be maximally predictive of its percepts,
an agent must, for some environments, retain informa-
tion about past actions that carry predictive information
about future percepts. While doing so reduces the per-
cept entropy H(St|Mt), thereby increasing the work rate
(see eq. (15)), remembering actions reduces the action
entropy H(At|Mt), thereby decreasing the work rate.
Conversely, randomizing actions without retaining
memory of them increases H(At|Mt), but may drive
the environment into a less predictable regime, such
that H(St|Mt) increases. Crucially, there exist
environments—such as the memoryless invariant envi-
ronment shown in Figure 5—for which the energetic costs
outweigh the benefits of implementing either of the two
design principles.
Consequently, the two design principles for work-
efficient agents in the tape setting can no longer be pur-
sued independently in percept-action loops. Instead, a
tradeoff emerges between predictive memory and action
forgetfulness, generally rendering both strategies subop-
timal.
VI. DISCUSSION AND FUTURE DIRECTIONS
Predicting future observations is a central theme across
various fields, including Bayesian and active inference
[47], predictive analytics [48], computational mechanics
[49], and chaos theory [50]. It also plays a crucial role
in modern machine learning, particularly in transformer
models and large language models, which are designed to
predict future states in a sequence [51].
However, as we show in this work by analyzing the
fundamental limits of information processing in percept-
action loops, the mere act of remembering the past to
predict the future has thermodynamic consequences. To
investigate this, we developed a framework for studying
the stochastic thermodynamics of information processing
in percept-action loops. Within this framework, we de-
fine the work capacity of an environment channel as the
maximal rate of expected work extraction by an agent.
Similar to communication capacity, work capacity is an
intrinsic information theoretic property of a channel. Ac-
cording to previously established design principles for
work-efficient agents—derived in the context of linear in-
formation processing on a tape—an agent’s actions, from
its own perspective, should appear maximally random,
while its percepts should be as predictable as possible.
Surprisingly, we find that neither of these two prin-
ciples remains valid in general. Most notably, maximal
predictability of percepts is no longer optimal.
This phenomenon arises specifically in percept-action
loops with genuine feedback. In such settings, when pre-
dicting percepts requires remembering past actions, a
trade-off emerges and the goals of prediction and work-
efficiency diverge: as we prove in this work, there exist
environments in which any agent that maximizes work ef-
ficiency must necessarily forget certain aspects of its past
actions—and, therefore, cannot be maximally predictive.
Building on the results established in this work, several
natural directions for future research emerge:
• Agents with goals—In this work, we considered
classes of agents with implicit objectives, such as
maximizing work rate or predictive power. A nat-
ural next step is to investigate agents with specific
goals within our framework. One approach is to
fix a desired percept-action behavior, which corre-
sponds to specifying an agent channel. Then, the
energetic limits of the agent’s behavior can be de-
termined by optimizing over all models that im-
plement this channel (see related ideas in the tape
setting [20, 22]). Alternatively, one could emulate
a reinforcement learning scenario by encoding re-
wards as predictable (i.e., low-entropy) percepts. In
this case, an agent aiming to maximize its work rate
could be guided toward desired behaviors through
suitable reward design.
• Dissipation in percept-action loops —If one
considers that both agent and environment thermo-
dynamically implement their respective channels,
the agent’s positive work rate implies a correspond-
ing work cost for the environment. In such a set-
ting, the environment converts work into structured
correlations, while the agent converts those corre-
lations back into work. For memoryless channels,
this conversion can happen without dissipation,
with the energetic cost of implementing the envi-
ronment channel—known in the quantum context
as the thermodynamic capacity [52, 53]—equaling
the work capacity. However, for channels with
memory, it remains an open question whether for
any agt →←env the agent’s maximum work rate can
match the environment’s minimum work cost. Any
gap between these values would imply intrinsic en-
tropy production in percept-action loops.
• Quantum work capacity —A natural extension
of this work is to explore quantum generalizations
8
of work capacity. Our framework admits a quanti-
zation by replacing classical channels with quantum
combs, enabling analysis of percept-action loops
in the quantum domain. This allows for study-
ing fundamental quantum limits on work extrac-
tion and the design of quantum-enhanced agents
[20–23, 54, 55].
More broadly, our work opens the door to a search for
new energetic design principles tailored to percept–action
loops with feedback. Such considerations may inform
novel organizational principles for biological and artificial
agents [56], moving beyond the predictive paradigm [19,
30, 57].
VII. ACKNOWLEDGMENTS
This research was funded in whole or in part by the
Austrian Science Fund (FWF) [SFB BeyondC F7102,
10.55776/F71]. For open access purposes, the author
has applied a CC BY public copyright license to any au-
thor accepted manuscript version arising from this sub-
mission. We gratefully acknowledge support from the
European Union (ERC Advanced Grant, QuantAI, No.
101055129). The views and opinions expressed in this
article are however those of the author(s) only and do
not necessarily reflect those of the European Union or
the European Research Council - neither the European
Union nor the granting authority can be held responsi-
ble for them. LJF acknowledges support by the Austrian
Research Promotion Agency (FFG) and the European
Union via NextGeneration EU under Contract Number
FO999921407 (HDcode). LJF thanks Benjamin Morris
and Andrew Garner for early discussions that helped
shape the direction of this work.
[1] G. Sachs, J. Traugott, A. P. Nesterova, G. Dell’Omo,
F. K¨ ummeth, W. Heidrich, A. L. Vyssotski, and
F. Bonadonna, Flying at No Mechanical Energy Cost:
Disclosing the Secret of Wandering Albatrosses, PLoS
ONE , 628 (2012).
[2] P. Lennie, The Cost of Cortical Computation, Current
biology 13, 493 (2003).
[3] L. Yu and Y. Yu, Energy-efficient neural information
processing in individual neurons and neuronal networks,
Journal of Neuroscience Research 95, 2253 (2017).
[4] C. P. Kempes, D. Wolpert, Z. Cohen, and J. P´ erez-
Mercader, The thermodynamic efficiency of computa-
tions made in cells across the range of life, Philosophi-
cal Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences 375, 20160343 (2017).
[5] N. C. Thompson, K. Greenewald, K. Lee, G. F. Manso,
et al. , The Computational Limits of Deep Learning,
arXiv:2007.05558 10 (2020).
[6] J. McDonald, B. Li, N. Frey, D. Tiwari, V. Gade-
pally, and S. Samsi, Great Power, Great Responsibil-
ity: Recommendations for Reducing Energy for Train-
ing Language Models, in Findings of the Association
for Computational Linguistics: NAACL 2022 , edited by
M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz (As-
sociation for Computational Linguistics, 2022) pp. 1962–
1970.
[7] U. Seifert, Stochastic thermodynamics, fluctuation the-
orems and molecular machines, Reports on progress in
physics 75, 126001 (2012).
[8] J. M. Parrondo, J. M. Horowitz, and T. Sagawa, Thermo-
dynamics of information, Nature physics 11, 131 (2015).
[9] D. Mandal and C. Jarzynski, Work and information pro-
cessing in a solvable model of Maxwell’s demon, Pro-
ceedings of the National Academy of Sciences 109, 11641
(2012).
[10] D. Mandal, H. Quan, and C. Jarzynski, Maxwell’s Re-
frigerator: An Exactly Solvable Model, PRL 111, 030602
(2013).
[11] A. C. Barato and U. Seifert, An autonomous and re-
versible Maxwell’s demon, Europhysics Letters 101,
60001 (2013).
[12] A. Barato and U. Seifert, Unifying Three Perspectives on
Information Processing in Stochastic Thermodynamics,
PRL 112, 090601 (2014).
[13] J. Hoppenau and A. Engel, On the energetics of informa-
tion exchange, Europhysics Letters 105, 50002 (2014).
[14] N. Merhav, Sequence complexity and work extraction,
Journal of Statistical Mechanics: Theory and Experi-
ment 2015, P06037 (2015).
[15] A. B. Boyd, D. Mandal, and J. P. Crutchfield,
Correlation-powered information engines and the ther-
modynamics of self-correction, PRE 95, 012152 (2017).
[16] A. J. Garner, J. Thompson, V. Vedral, and M. Gu, Ther-
modynamics of complexity and pattern manipulation,
PRE 95, 042140 (2017).
[17] A. B. Boyd, D. Mandal, and J. P. Crutchfield, Thermo-
dynamics of modularity: Structural costs beyond the lan-
dauer bound, PRX 8, 031036 (2018).
[18] A. J. Garner, The fundamental thermodynamic bounds
on finite models, Chaos: An Interdisciplinary Journal of
Nonlinear Science 31, 063131 (2021).
[19] A. B. Boyd, J. P. Crutchfield, and M. Gu, Thermody-
namic machine learning through maximum work produc-
tion, NJP 24, 083040 (2022).
[20] T. J. Elliott, M. Gu, A. J. Garner, and J. Thomp-
son, Quantum Adaptive Agents with Efficient Long-Term
Memories, PRX 12, 011007 (2022).
[21] R. C. Huang, P. M. Riechers, M. Gu, and
V. Narasimhachar, Engines for predictive work ex-
traction from memoryful quantum stochastic processes,
Quantum 7, 1203 (2023).
[22] J. Thompson, P. M. Riechers, A. J. Garner, T. J. Elliott,
and M. Gu, Energetic advantages for quantum agents in
online execution of complex strategies, arXiv:2503.19896
10.48550/arXiv.2503.19896 (2025).
[23] G. Zambon and G. Adesso, Quantum processes as ther-
modynamic resources: the role of non-markovianity,
arXiv:2411.05559 10.48550/arXiv.2411.05559 (2024).
9
[24] Y. Ephraim and N. Merhav, Hidden Markov processes,
IEEE Transactions on information theory 48, 1518
(2002).
[25] R. G. Gallager, Information theory and reliable commu-
nication, Vol. 588 (Springer, 1968).
[26] R. B. Ash, Basic Probability Theory (Courier Corpora-
tion, 2008).
[27] M. Iosifescu, Finite Markov Processes and Their Appli-
cations (Courier Corporation, 2014).
[28] For example, eq. (9) is satisfied if eq. (8) holds true for
all times, or it can be satisfied when the summands in
eq. (9) decay sufficiently quickly.
[29] T. M. Cover and J. A. Thomas, Elements of Information
Theory (Wiley, 2005).
[30] N. Barnett and J. P. Crutchfield, Computational me-
chanics of input–output processes: Structured transfor-
mations and the epsilon-transducer, J. Stat. Phys. 161,
404 (2015).
[31] R. B. Ash, Information Theory , Interscience Tracts in
Pure and Applied Mathematics No. 19 (John Wiley &
Sons).
[32] Technically, this also requires allowing for an infinite
number of hidden states in the environment to generate
all percept processes allowed in the tape setting.
[33] R. M. Gray, Probability, random processes, and ergodic
properties, Vol. 1 (Springer, 2009).
[34] S. Still, D. A. Sivak, A. J. Bell, and G. E. Crooks, Ther-
modynamics of Prediction, PRL 109, 120604 (2012).
[35] In fact, the other condition is I[S0:t; Mt | St:∞] = 0,
which corresponds to a d-separation in the Bayesian net-
work underlying the percept–action loop (see Supplemen-
tal Material E for details on d-separation).
[36] R. Landauer, Information is Physical, Physics Today 44,
23 (1991).
[37] S. Deffner and C. Jarzynski, Information Processing
and the Second Law of Thermodynamics: An Inclusive,
Hamiltonian Approach, PRX 3, 041003 (2013).
[38] A. Kolchinsky and D. H. Wolpert, Dependence of dis-
sipation on the initial distribution over states, Journal
of Statistical Mechanics: Theory and Experiment 2017,
083202 (2017).
[39] N. Shiraishi, K. Funo, and K. Saito, Speed Limit for Clas-
sical Stochastic Processes, PRL 121, 070601 (2018).
[40] A. Kolchinsky and D. H. Wolpert, Work, Entropy Pro-
duction, and Thermodynamics of Information under Pro-
tocol Constraints, PRX 11, 041024 (2021).
[41] D. H. Wolpert, J. Korbel, C. W. Lynn, F. Tasnim, J. A.
Grochow, G. Karde¸ s, J. B. Aimone, V. Balasubramanian,
E. De Giuli, D. Doty,et al., Is stochastic thermodynamics
the key to understanding the energy costs of computa-
tion?, Proceedings of the National Academy of Sciences
121, e2321112121 (2024).
[42] An illustrative example of a sequence ( at)t where the
Ces´ aro limit⟨at⟩t fails to exist is 0110000 . . ., where one
zero is followed by twice as many ones, followed again
by twice as many zeros, and so on. This results in an
oscillating arithmetic mean 1 /n Pn
t=1 at as n → ∞.
[43] The term noiseless channel is inherited from communica-
tion theory, where it refers to an ideal channel that trans-
mits input symbols without alteration—that is, without
introducing noise.
[44] C. E. Shannon, A mathematical theory of communica-
tion, The Bell system technical journal 27, 379 (1948).
[45] A. B. Boyd, D. Mandal, and J. P. Crutchfield, Identifying
functional thermodynamics in autonomous Maxwellian
ratchets, NJP 18, 023049 (2016).
[46] If the percept process is a stationary finite-state Markov
chain, a closed-form expression for the entropy rate exists
[44].
[47] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference:
The Free Energy Principle in Mind, Brain, and Behavior
(MIT Press, 2022).
[48] D. T. Larose, Data Mining and Predictive Analytics
(John Wiley & Sons, 2015).
[49] J. P. Crutchfield, Between order and chaos, Nature
Physics 8, 17 (2012).
[50] S. Boccaletti, C. Grebogi, Y.-C. Lai, H. Mancini, and
D. Maza, The control of chaos: theory and applications,
Physics reports 329, 103 (2000).
[51] T. Lin, Y. Wang, X. Liu, and X. Qiu, A survey of trans-
formers, AI open 3, 111 (2022).
[52] M. Navascu´ es and L. P. Garc´ ıa-Pintos, Nonthermal
Quantum Channels as a Thermodynamical Resource,
PRL 115, 010405 (2015).
[53] P. Faist, M. Berta, and F. Brand˜ ao, Thermodynamic Ca-
pacity of Quantum Processes, PRL 122, 200601 (2019).
[54] M. Gu, K. Wiesner, E. Rieper, and V. Vedral, Quantum
mechanics can reduce the complexity of classical models,
Nat. Commun. 3, 762 (2012).
[55] V. Dunjko, J. M. Taylor, and H. J. Briegel, Quantum-
enhanced machine learning, PRL 117, 130501 (2016).
[56] A. Rupe and J. P. Crutchfield, On principles of emergent
organization, Physics Reports 1071, 1 (2024).
[57] K. Friston, Life as we know it, Journal of the Royal So-
ciety Interface 10, 20130475 (2013).
[58] A. S. Klyubin, D. Polani, and C. L. Nehaniv, Represen-
tations of Space and Time in the Maximization of In-
formation Flow in the Perception-Action Loop, Neural
computation 19, 2387 (2007).
[59] Note that the labeling convention is such that the value
left of the colon in the subscript is included in the se-
quence, while the value to the right is not.
[60] R. M. Gray, Entropy and information theory , first, cor-
rected ed. (Springer Science & Business Media, 2011).
[61] D. Hankerson, G. A. Harris, and P. D. Johnson Jr, In-
troduction to Information Theory and Data Compression
(CRC press, 2003).
[62] R. W. Yeung, A First Course in Information Theory
(Springer US, 2002).
[63] A. Kolchinsky, A novel approach to the partial informa-
tion decomposition, Entropy 24, 403 (2022).
[64] R. W. Yeung, A new outlook on Shannon’s information
measures, IEEE transactions on information theory 37,
466 (1991).
[65] H. K. Ting, On the Amount of Information, Theory of
Probability & Its Applications 7, 439 (1962).
[66] H. C. Tijms, A First Course in Stochastic Models (John
Wiley and sons, 2003).
[67] R. G. Gallager, Discrete Stochastic Processes (Springer,
1996).
[68] R. E. Edwards, Fourier Series: A Modern Introduction
Volume 1, 2nd ed. (Springer, 1979).
[69] J. R. Munkres, Topology: Pearson New International
Edition (Pearson Higher Ed, 2013).
[70] J. C. Kieffer and M. Rahe, Markov Channels are Asymp-
totically Mean Stationary, SIAM Journal on Mathemat-
ical Analysis 12, 293 (1981).
[71] R. G. James, J. R. Mahoney, and J. P. Crutchfield, In-
10
formation trimming: Sufficient statistics, mutual infor-
mation, and predictability from effective channel states,
PRE 95, 060102 (2017).
[72] I. Csisz´ ar, The Method of Types, IEEE Transactions on
Information Theory 44, 2505 (1998).
[73] J. Pearl, Bayesian Networks: A Model of Self-Activated
Memory for Evidential Reasoning, Proceedings of the
Annual Meeting of the Cognitive Science Society 7
(1985).
[74] J. Pearl, Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference (Morgan Kaufmann
Publishers Inc., 1988).
[75] T. Verma and J. Pearl, Causal Networks: Semantics
and Expressiveness, in Machine Intelligence and Pattern
Recognition, Uncertainty in Artificial Intelligence, Vol. 9,
edited by R. D. Shachter, T. S. Levitt, L. N. Kanal, and
J. F. Lemmer (North-Holland) pp. 69–76.
[76] D. Janzing and B. Sch¨ olkopf, Causal Inference Using the
Algorithmic Markov Condition, IEEE Transactions on
Information Theory 56, 5168 (2010).
[77] S. Lauritzen, Graphical Models (Clarendon Press, 1996).
[78] In fact, V can be any sufficient statistic of WX about
Y Zbut for our purposes setting V = Y Zis the simplest.
This solution was also pointed out in[58, figure 13a].
[79] R. J. Evans, Graphs for margins of bayesian networks,
Scandinavian Journal of Statistics 43, 625 (2016).
[80] N. Tishby and D. Polani, Information Theory of Deci-
sions and Actions, in Perception-Action Cycle: Models,
Architectures, and Hardware , edited by V. Cutsuridis,
A. Hussain, and J. G. Taylor (Springer, New York, 2010)
pp. 601–636.
[81] C. Salge, C. Glackin, and D. Polani, Empowerment–
An Introduction, in Guided Self-Organization: Inception,
edited by M. Prokopenko (Springer, Berlin, 2014) pp. 67–
114.
[82] N. Ay and K. Zahedi, On the Causal Structure of the
Sensorimotor Loop, in Guided Self-Organization: Incep-
tion (Springer, 2014) pp. 261–294.
[83] A memoryless agent which chooses actions with pAt(0) =
1/
√
2 and pAt(1) = 1−1/
√
2 for all t achieves Cwork(env).
11
SUPPLEMENTAL MATERIAL
This Supplemental Material provides the full mathematical framework in a self-contained way, allowing it to be
read from start to finish like a technical paper. The main text motivates and further explains the results, and puts
them in the context of existing literature.
Contents
CONTENTS
I. Introduction 1
II. Framework 2
III. Maximally Predictive Agents 3
IV. Work capacity of channels 4
V. Work-Efficient Agent Models 6
VI. Discussion and Future Directions 7
VII. Acknowledgments 8
References 8
Supplemental Material 11
Contents 11
A. Some background on probability and information theory 11
1. Notation for random variables and stochastic processes 11
2. Information theory 12
a. Basic definitions 12
b. Information diagrams 13
c. Entropy rate 14
B. Finite-state Markov chains 14
C. Finite-alphabet finite-state hidden Markov channels 17
D. Percept-action loops 18
E. Markov conditions for percept-action loops 20
1. Bayesian networks and d-separation 20
2. d-separation conditions for percept-action loops 21
3. Existing approaches to the information theory of percept-action loops 25
F. Maximally predictive agent models 26
G. The extractable work in percept-action loops 32
1. Derivation of work capacity 32
2. Existence of Landauer-efficient agents 33
3. Definition and properties of work capacity 34
4. Efficient agent models 38
Appendix A: Some background on probability and information theory
1. Notation for random variables and stochastic processes
In this section, we establish some of the notation relating to random variables and stochastic processes used
throughout the sequel. Random variables will be denoted by capital letters in standard font, i.e. X, Y, Z,etc. The set
of values that each random variable can take, also called an alphabet, will be denoted by capital letters in calligraphic
font, i.e. the alphabet for X is X. In this work, we consider finite alphabets and occasionally also countably infinite
products of finite alphabets. The elements of these alphabets, at times referred to as symbols, will be denoted by
lower-case letters, i.e. the random variable can take value x ∈ X. Probability distributions associated to the random
variable X will be denoted by pX with pX(x) denoting the probability that X takes value x. The subscript may be
omitted when the variable to which the distribution refers is clear.
Given two alphabets Y and Z related to random variables Y and Z respectively, we can consider a new random
variable X that takes values in X := Y × Z. That is, X takes values x = (y, z) where y ∈ Yand z ∈ Z. Composite
random variables of this type can be constructed from any number of constituent variables, which will be useful for,
e.g., the treatment of stochastic processes below.
For the purposes of this work, a discrete-time stochastic process is given by a set of variables {Xt|t ∈ N0} where,
for each t, the variable Xt takes values in the same (finite) alphabet X. This assumption simplifies measure-theoretic
treatments such as those in, e.g., [33]. Pursuant to the paragraph above, we can associate a new random variable, de-
noted X, to the stochastic process, which takes values in XN0 :=×t∈N0
X. That is, X takes values that are sequences
(x0, x1, ...) with each xt ∈ X. It will also be convenient to consider random variables associated to subsequences in
the following way. Let l, m∈ N0 such that l < m. We then define Xl:m to be the random variable that takes values in
12
Xm−l :=×
m
i=l X, that is, values that are tuples xl:m := (xl, xl+1, ..., xm−1).[59] If m = l + 1, then Xl:m is equivalent
to a single variable from {Xt|t ∈ N0}, so we simply write Xl and xl. The variable X can be considered as the limiting
case where l = 0 and m goes to infinity.
The notation for distributions associated to the stochastic process X and to sequences of variables Xl:m follow the
same conventions: pX denotes a distribution for X and pX(x) denotes the probability that X takes as its value the
sequence x := (x0, x1, ...) ∈ XN0 ; similarly for pXl:m and pXl:m(xl:m). In the cases where l and m are “close”, we
sometimes represent the tuples of variable explicitly. For example, in the case of m = l + 2, instead of pXl:m, we write
pXl,Xl+1 . In particular, this allows us to consider the distribution over one part of the subsequence, when the other
part takes some value. For example, if pXl,Xl+1 is known, we can consider the distribution over Xl that results if Xl+1
takes the value xl+1, which we denote by pXl,Xl+1=xl+1 .
2. Information theory
For an introductory treatment on information theory, see [29], for a measure-theoretic treatment, see [60].
a. Basic definitions
Let X be a random variable with distribution pX. The Shannon entropy quantifies the uncertainty associated with
pX as
Hp (X) :=
X
i∈X
pX(i)sp(i) (A1)
where sp(i) := −log2 pX(i) is known as the surprise of obtaining outcome X = i [44] (see [61] for an elegant axiomatic
derivation), and the sum runs over all i ∈ Xsuch that pX(i) ̸= 0. If it is clear from context which distribution p is
used to compute entropy, we drop the index and simply write H (X).
Let X and Y be random variables with joint distribution pXY . The conditional entropy of X given Y is defined as
H (X|Y ) :=
X
i∈X,j∈Y
pXY (i, j) sp(i|j), (A2)
where sp(i|j) := −log pX|Y (i|j) denotes the conditional surprise of obtaining outcome x given that y has been
observed.
Entropy obeys the chain rule of entropy
H (X0:n) =
n−1X
t=0
H (Xt|X0:t) (A3)
where, if t = 0, H (Xt|X0:t) is given by H (X0).
The mutual information I [X; Y ] of random variables X and Y is defined as
I [X; Y ] := H (X) − H (X|Y ) , (A4)
which, with the chain rule of entropy eq. (A3), can be written in the symmetric form
I [X; Y ] = H (XY ) − H (Y |X) − H (X|Y ) . (A5)
The Conditional mutual information is then simply defined via the conditional entropy as
I [X; Y |Z] := H (X|Z) − H (X|Y Z) , (A6)
or equivalently in a symmetric form:
I [X; Y |Z] = H (XY |Z) − H (Y |XZ ) − H (X|Y Z) . (A7)
We say that X and Y are conditionally independent if I [X; Y |Z] = 0. In fact, I [X; Y |Z] = 0 iff
pXY |Z = pX|ZpY |Z. (A8)
13
The conditional mutual information inherits a chain rule from entropy, which can be written as
I [X0:n; Y |Z] =
n−1X
t=0
I [Xt; Y |ZX0:t] . (A9)
The chain rule for mutual information is obtained by dropping Z on both sides. Often, we will use the chain rule for
a single step:
I[W; XY |Z] = I[W; X|Z] + I[W; Y |XZ ]. (A10)
The measures of information defined so far are all nonnegative and can be interpreted based on (conditional) surprise,
respectively its averaged version, (conditional) entropy. For a consistent treatment of multiple random variables, it is
convenient to extend the definition of (conditional) mutual information to more than two arguments. The so-called
multivariate mutual information or interaction information [62, 63] can be defined inductively via
I[Xi; . . .; Xj; Xk] := I[Xi; . . . Xj] − I[Xi; . . .; Xj|Xk], (A11)
and similarly conditional interaction information via
I[Xi; . . .; Xj; Xk|Xl] := I[Xi; . . . Xj|Xl] − I[Xj; . . .; Xj|XlXk]. (A12)
However, it should be noted that multivariate mutual information of three or more variables can assume negative
values which makes it difficult to interpret [63].
b. Information diagrams
The properties of Shannon’s basic measures of information such as entropy and mutual information bear a resem-
blance to set theory. It has been shown that one can establish a one-to-one correspondence between these measures of
information and a (signed) measure on sets [64, 65]. We write X1, . . . , Xn to denote random variables, and ˜X1, . . . ,˜Xn
for the corresponding sets. The union of sets ˜Xi∪···∪ ˜Xj corresponds to the joint entropyH (Xi, . . . , Xj), the intersec-
tion of sets ˜Xi∩···∩ ˜Xj corresponds to the multivariate mutual informationI [Xi; . . .; Xj], and the set difference ˜Xi\ ˜Xj
corresponds to the conditional entropy H (Xi|Xj). Conditional mutual information I [Xi . . . Xj; Sk . . . Sl|Cn . . . Cm]
corresponds then to

( ˜Xi ∪ ··· ∪˜Xj) ∩ ( ˜Sk ∪ ··· ∪˜Sl)

\ ( ˜Cn ∪ ··· ∪˜Cm). This correspondence allows us to represent
the relations between measures of information in terms of Venn diagrams, whose primary sets correspond to the
entropies of single random variables. One example of such an information diagram is given in Figure 7.
H(X|Y, Z) H(Y |X, Z)
H(Z|X, Y)
X Y
Z
I[X; Z|Y ] I[Y ; Z|X]
I[X; Y |Z]
I[X; Y ; Z]
FIG. 7. An example of an information diagram. An information diagram illustrates the relations between (conditional)
entropies and (conditional) mutual information.
14
c. Entropy rate
Let X be a stochastic process with distribution pX. Then, the entropy rate, a process’s degree of intrinsic random-
ness, is defined as
h (X) := lim
n→∞
H (X0:n)
n (A13)
if the limit exists. The entropy rate exists for a broad class of processes known as asymptotically mean stationary
processes [60] which contains stationary processes and, as we will see, also processes which are generated by finite-state
hidden Markov models.
Before we proceed, we introduce the following notation for the Ces` aro limit, the limit of the arithmetic mean, which
will be used throughout this work [66]:
⟨f(t)⟩t := lim
n→∞
1
n
n−1X
t=0
f(t). (A14)
The Ces` aro limit is linear in the sense that
⟨af(t) + bg(t)⟩t = a ⟨f(t)⟩t + b ⟨g(t)⟩t (A15)
whenever ⟨f(t)⟩t and ⟨g(t)⟩t exist, where a, b∈ R.
Using the Ces` aro limit, we can state a chain rule for entropy rate as follows:
⟨H (Xt|X0:t)⟩t = h (X) . (A16)
This chain rule is a consequence of eq. (A3).
When working on the information theory of stochastic processes, expressions which contain a infinite number of
random variables, such as I[Xn:∞; Y |Z], are commonly encountered. It should be noted that such expressions are
always defined via a limit, that is, I[Xn:∞; Y |Z] = lim
m→∞
I[Xn:m; Y |Z]. In particular, using the chain rule, one can
show that
Lemma 1. For any n ∈ N0, I[Xn:∞; Y |Z] is finite, where Y , Z, and Xt for all t are finite random variables.
Proof. Using the chain rule of mutual information, eq. (A9), we find
I[Xn:∞; Y |Z] = lim
m→∞
mX
t=n
I[Xt; Y |ZXn:t]. (A17)
Note that, for finite random variablesA, D, Sn:t for t ∈ {n, ..., m}, all partial sums Pm
t=n I[A; St|DSn:t] = I[A; Sn:m|D]
are upper bounded by H(A), and every summand is nonnegative, so the monotone convergence theorem ensures that
the limit exists.
Appendix B: Finite-state Markov chains
This appendix reviews some results on finite-state Markov chains from the literature. For a more complete treatment
of finite-state Markov chains, we refer the reader to [27, 67].
Let X be a stochastic process with distribution p. A (first-order) Markov chain is a stochastic process X such that
pXt|X0:t (xt|x0:t−1, xt−1) = pXt|X0:t
 
xt|x′
0:t−1, xt−1

(B1)
for all xt−1, xt ∈ Xand for any t ≥ 1 and x0:t−1, x′
0:t−1 ∈ Xt−1.
The Markov chain is said to befinite-state if X is finite, and it is said to be homogeneous if eq. (B1) does not depend
on time t, that is, pXt|Xt−1 (j|i) = pXt′|Xt′−1 (j|i) for all i, j∈ Xand t, t′ ∈ N. We then write ϕ(j|i) := pXt|Xt−1 (j|i)
and call ϕ(j|i) the transition probability. For the remainder of this appendix, it is assumed that all finite-state Markov
chains are homogeneous.
Finite-state Markov chains are thus conveniently characterized by their initial distribution p(X0) and a |X| × |X|
stochastic matrix Φ = (ϕ(j|i))j,i∈X . The matrix Φ is also called a transition matrix. We use the convention that Φ is
15
a right stochastic matrix, i.e., that each row of Φ must sum to one. This convention is more common in the physical
literature, while the mathematical literature such as [27] often uses the convention that Φ is left stochastic. Results
can be translated from one convention to the other by a simple transposition.
In what follows, we introduce some theory of finite-state Markov chains which will be needed to understand in what
sense finite-state Markov chains are well-behaved in the asymptotic time limit t → ∞. The probability to reach state
j after n steps starting from state i is given by (Φn)j,i. If i is a return state, i.e., (Φn)i,i > 0 for some n ≥ 1, we define
its period di as the greatest common divisor of all natural numbers n such that (Φn)i,i > 0 [27, p. 81]. Further, the
first passage time to state j is defined as
Tfirst
j := min{t ≥ 1|Xt = j}. (B2)
where Tfirst
j takes values in N ∪ {∞}. Note that the first passage time is a random variable. Define f(n, i, j) as
the probability p{Tfirst
j = n|X0 = i} that Tfirst
j = n given that the chain started in state i. Then, ( f(n, i, j), n=
1, 2, . . . ,∞) is the probability distribution of Tfirst
j given that the Markov chain started in state i [27, p.86].
Let
f(i, j) :=
∞X
n=1
f(n, i, j) = p{Tfirst
j < ∞|X0 = i}. (B3)
A state i is said to be recurrent if f(i, i) equals one, i.e., the chain is guaranteed to return to i eventually with
probability one. Otherwise, state i is called transient, i.e., there is a nonzero probability that the chain will never
return to i [27, p.88].
Let m(i, i) be the mean recurrence time of state i,
m(i, i) :=
∞X
n=1
nf(n, i, i), (B4)
where m can take values in [1, ∞]. Note that m <∞ for recurrent states and m = ∞ for transient states.
Further, let fr(i, j) be the probability that Xt = j occurs at least for one t = r(mod dj) given that the chain started
in i:
fr(i, j) :=
X
m≥0
f(mdj + r, i, j). (B5)
We are now in the position to state the following result which characterizes the asymptotic behavior of arbitrary
homogeneous finite-state Markov chains.
Lemma 2. [adapted from [27, p.153]] Let X be a homogeneous finite-state Markov chain over an alphabet X with
transition matrix Φ. Then, for any state i ∈ Xand any transient state j we have
lim
n→∞
(Φn)j,i = 0. (B6)
Further, for any state i ∈ Xand any recurrent state j with period dj we have for all rj ∈ {1, 2, . . . , dj},
lim
n→∞
 
Φndj+rj

j,i = frj (i, j)dj
m(j, j) . (B7)
For a proof see [27, p.153-154]. Note that when compared to [27, Thm 5.1, p.153], we treat the case of transient
states (eq. (B6)) separately because for transient states j it can happen that dj is not well defined if there is no n ∈ N
for which (Φn)i,j > 0. Lemma 2 states that for each starting state i, the probability for the chain to be in a transient
state goes to zero as n → ∞while the probability for the chain to be in a recurrent state j is periodic with some
finite period dj as n → ∞.
The following corollary, which is adapted from [27, p.154], summarizes some useful consequences of Lemma 2. We
again make use of the notation ⟨•⟩t = lim
n→∞
1
n
Pn−1
t=0 •.
Corollary 1. Let X with distribution pX be a homogeneous finite-state Markov chain over an alphabet X with
transition matrix Φ. For any recurrent state i ∈ X, let di be its period, and let d be the least common multiple of all
di. Then,
16
(i) [d convergent subsequences] for all r ∈ {1, 2, . . . , d} the limit
Φ(r)
∞ := lim
n→∞
Φnd+r (B8)
exists and in particular Φ(r)
∞ = ΦrΦ(d)
∞ ,
(ii) [Ces` aro limit]the matrix
Π =

Φt
t (B9)
exists and its coefficients are given by πj,i = f(i,j)
m(j,j) ,
(iii) [continuous function of Ces` aro limit]for any continuous function g : T →R where T denotes the set of |X|×|X|
transition matrices,

g
 
Φt
t (B10)
exists, and is given by ⟨g (Φt)⟩t = Pd
r=1 g

Φ(r)
∞

/d.
Proof.
(i) Existence follows from lemma 2 and Φ (r)
∞ = ΦrΦ(d)
∞ from lim
n→∞
 
ΦrΦnd
= Φr lim
n→∞
 
Φnd
.
(ii) Follows from the fact that by corollary 1(i) the sequence (Φ t)t has d convergent subsequences each of which has
a convergent Ces` aro limit by the Ces` aro limit theorem [68, 5.3.1], and
1
dj
djX
r=1
fr(i, j)dj
m(j, j) = f(i, j)
m(j, j). (B11)
(iii) A function g is continuous if and only if for a convergent sequence Π n → Π the sequence g(Πn) converges to
g(Π) [69, Thm. 21.3]. It follows then from corollary 1(i), that the sequence g(Φt) has d convergent subsequences,
and therefore converges in the Ce` saro limit to
⟨g (Φt)⟩t = 1
d
dX
r=1
g

Φ(r)
∞

. (B12)
Finally, it should be noted that not only the per-step distributions of Markov chains are asymptotically well behaved
(as a consequence of corollary 1), but also the entropy rate as defined in eq. (A13). Entropy rate exists even for broader
classes of processes such as deterministic functions of Markov chains: Let X be a finite-state Markov chain. We say
that the process Y over a finite alphabet Y is a deterministic function of X if Yt = f(Xt) for all t ∈ N0 where
f : X → Yis a deterministic function. (Note that the class of deterministic functions of finite-state Markov chains
is equivalent to finite-state finite-alphabet hidden Markov chains [24] in the sense that any deterministic function of
a Markov chain can be described as a finite-alphabet hidden Markov chain, and any finite-alphabet hidden Markov
chain can be described as a deterministic function of Markov chain with an augmented state space [24].)
Lemma 3. Let X and Y be finite alphabets, f : X → Ya map, X a finite-state Markov chain on X, and Y =
(f(X0), f(X1), f(X2), . . .). Then,
⟨H(Y0:t+1)⟩t (B13)
exists.
Proof. This follows from [70, theorem 9] and the entropy ergodic theorem [60, theorem 3.1.1]. □
17
Appendix C: Finite-alphabet finite-state hidden Markov channels
This appendix defines hidden Markov channels in general as well as some special classes of hidden Markov channels.
For a review on hidden Markov processes see [24].
Let X and Y denote the finite input and output alphabets, respectively. A discrete-time, finite-alphabet channel
is defined as a function from input sequences x ∈ XN0 to distributions over the channel’s output process, Y . This
function can be represented as a conditional distribution, denoted νY |X. Thus, for a fixed input sequence X = x, a
channel assigns probabilities νY |X(y|x) for all output sequences y ∈ YN0 .
In the simplest case, νY |X’s inputs are distributed as pX such that the joint distribution becomes pXY = νY |XpX.
However, note that in general,νY |X’s inputs may depend on (some of)νY |X’s outputs. Therefore, the joint probability
distribution over the joint process of inputs and outputs, XY is in general given by
pXY = νY |XηX|Y (C1)
where ηX|Y (x|y) is another channel which specifies how the νY |X’s inputs are distributed, depending on νY |X’s
outputs. In such cases, the distribution
pY |X(y|x) = pXY (x, y)
pX(x) (C2)
= νY |X(y|x)ηX|Y (x|y)P
y∈YN0 νY |X(y|x)ηX|Y (x|y) (C3)
can be different from νY |X(y|x). This difference is also the reason we denote the channel by ν and reserve the symbol
p for the joint distribution pXY (and distributions which can be obtained from it by marginalizing or conditioning,
such as pY |X and pX).
The conditional probability νY |X thus characterizes the behavior intrinsic to the channel while the conditional
probability pY |X also takes into account how the channel’s inputs are prepared.
In this work, we focus on a subclass of discrete-time finite-alphabet channels commonly known as finite-state [25,
p.97] or hidden Markov channels [24].
Definition 6. A (discrete-time finite-alphabet) channel νY |X is a finite-state hidden Markov channel if there exists
a distribution pZ0 over a finite set of states Z and a transition matrix Φ = (ϕ(j|i))j,i with i ∈ X × Zand j ∈ Y × Z
such that
νY |X(y|x) =
X
z
pZ0 (z0)
∞Y
t=0
ϕ (yt, zt+1|xt, zt) , (C4)
where the sum runs over all z ∈ ZN0 . Then, the tuple (Φ, pZ0 ) is called a hidden Markov model of νY |X and z ∈ Z
the hidden states of the Markov model.
In particular, since any such Markov model defines a hidden Markov channel and any hidden Markov channel by
definition has a Markov model, eq. (C4) defines a many-to-one correspondence between Markov models and channels.
Further, hidden Markov channels are causal channels [30, definition 4] in the sense that
νY0:n|X(y0:n|x0:nxn:∞) = νY0:n|X(y0:n|x0:nx′
n:∞) (C5)
for all n ∈ N and for all future input sequences xn:∞, x′
n:∞ ∈ XZ0 , where νY0:n|X(y0:n|x) = P
yn:∞ νY |X(y|x). This
means that for a complete description of the channel’s behavior for the first n rounds (channel uses) it is sufficient to
know its input past x0:n. In particular, hidden Markov channels can be understood as those causal channels which
admit an implementation using only finite memory resources as represented by the finite set of hidden states Z.
The transition matrix Φ stores as its coefficients the conditional probability assignments ϕ (yt, zt+1|xt, zt) which are
independent of t (and hence Φ generates a homogeneous Markov chain).
Given that one knows the transition matrix Φ, the current hidden state z, as well as the current input x and output
y, the obtainable knowledge about the next hidden state z′ of the Markov model is represented by a distribution
determined by Φ which, up to normalization, is given by (ϕ (y, z′|x, z))z′∈Z. Markov models for which this distribution
is a delta distribution, are said to be unifilar [24, 25, 31]. Unifilar Markov models represent an important class of
Markov models because, given the current hidden state, input, and output, for unifilar Markov models it is possible
to infer the next hidden state with certainty.
18
Definition 7. A Markov model (Φ, pZ0 ) of a hidden Markov channel νY |X is said to be unifilar if
(i) pZ0 (z) = 1 for some z and zero otherwise, and
(ii) H(Zt+1|XtYtZt) = 0 for all t ∈ N0.
A hidden Markov channel is said to be unifilar if there exists a unifilar Markov model for it.
That is a Markov model is unifilar if pZ0 is a delta distribution and Xt, Yt, and Zt determine the next hidden state
Zt+1 for all steps t. It should be noted that while there exists a systematic method to construct unifilar models from
non-unifilar ones [71], some hidden Markov channels only admit unifilar Markov models if one allows for an infinite
number of hidden states. However, since Markov models as defined in Definition 6 have only a finite number of hidden
states, the set of unifilar hidden Markov channels is a strict subset of the set of hidden Markov channels. For an
example of a nonunifilar hidden Markov channel see [30, section 13].
Note that it follows from the definition of unifilar Markov models that it is always possible to construct a (deter-
ministic) function funi : X × Y × Z → Z, in the following called a unifilarity map, such that ϕ (y, z′|x, z) ̸= 0 only if
z′ = funi(x, z, y). Then, given the transition matrix Φ and the initial state Z0 = z, one can infer the exact hidden
state z at any time t by observing the input and output processes X0:t and Y0:t and by iteratively using the function
funi.
Unifilarity was first introduced in the context of finite-state sources [31, p. 187], and under the name Markov source
in [25, Section 3.6]. Definition 7 extends unifilarity to Markov models of hidden Markov channels. In the context of
stationary input-output processes, unifilarity is one of the properties of ϵ-transducers [30]. Unifilarity often simplifies
the mathematical treatment of Markov models considerably, see for example [72].
Important classes of channels, which we consider in this work, are the following:
Definition 8. A channel νY |X is said to be
• noiseless if νY |X(y|x) = δx,y and X = Y where δx,y is a Kronecker delta.
• memoryless invariant if there exists a |X| × |Y|stochastic matrix Φ such that νY |X(y|x) = Q∞
t=0 ϕ (yt|xt).
• a product channel if νY |X(y|x) = νY |X(y|x′) for all x, x′ ∈ XN0 .
The output behavior of product channels is fully characterized without knowing their inputs. Thus, they can be
understood as an information source which produces a (hidden Markov) process over outputs [24]. Product channels
are also called completely random channels in the literature [60, chapter 9.4.2].
Appendix D: Percept-action loops
This appendix defines a model for percept-action loops and proves, based on this model, that the global process
(involving agent and environment) is Markov. In the following, we refer to the hidden Markov channel of interest as
the environment, abbreviated as env:
env := νenv
S|A. (D1)
The input random variables At are called action variables taking values a ∈ Aand the output random variables St are
called percept variables taking values s ∈ S(S like state or sensory input is common nomenclature in reinforcement
learning and related fields). For simplicity, we assume that the finite input and output alphabets of env are identical,
A = S. In terms of expressivity of the model, this assumption is not restrictive, as any Markov channel with distinct
input and output alphabets can be trivially extended to a channel with a common alphabet for inputs and outputs
by embedding both to the larger of the two.
A Markov model of the channel νenv
S|A (see Definition 6), denoted as
envM :=
 
Φenv, penv
Z0

, (D2)
is called a (Markov) model of env.
Hidden Markov product channels (see Definition 8) represent a special class of environments which we will call
product environment channel.
Protocols used to interact with environments are called agents. In full generality, agents, abbreviated as agt, can
be represented as a channel ηagt
A|S from percepts to actions. Similarly to environments, we assume that agents respect
19
a causal ordering and that they admit an implementation with finite memory. However, there is a small asymmetry
between agent and environment: the agent must produce the very first actionA0 without being prompted by a percept
(in contrast, the environment is prompted with an action before it produces the first percept). On a formal level, this
is easily taken into account by defining agents as a hidden Markov channel from percepts S to actions A1:∞ where
the initial distribution over hidden states is replaced by a suitable joint distribution over hidden states and action A0.
For clarity, we suitably restate Definition 6:
Definition 9. A channel ηagt
A|S is an agent channel, denoted as
agt := ηagt
A|S, (D3)
if there exists a finite set of states M, a distribution pagt
A0M0 over A × M, and a transition matrix Θagt = (θ(j|i))j,i
with i ∈ S × Mand j ∈ A × Msuch that
ηagt
A|S(a|s) =
X
m
pagt
A0M0 (a0, m0)
∞Y
t=0
θagt (at+1, mt+1|st, mt) ,
where the sum runs over all m ∈ MN0 . Then, the tuple
agtM := (Θagt, pagt
A0M0 ) (D4)
is called a (hidden Markov) agent model, of agt and m ∈ Mthe memory states of the model.
For any given environment channel env, let A→←env denote the set of agent models with matching action-percept alphabet.
As before, eq. (D4) defines a many-to-one mapping correspondence between agent models and agents.
(a)
S1S0 S1 S2 S3A0 A1 A2 A3 A4
(b)
A0
M0
S1
Θ Θ Θ Θ
S0 S1 S2 S3A1
M1
A2
M2
A3
M3
A4
M4
Φ
Z1
Φ
Z2
Φ
Z3
Φ
Z4
Φ
Z0
FIG. 8. Percept-action loops. a: Agent and environment are represented through channels such that the environment’s inputs
At (outputs St) are the agent’s actions (percepts). b: Agent and environment are represented through Markov models with
hidden memory Mand Z, respectively.
The two channels defining an agent and its environment are called percept-action loop, denoted by
agt →←env :=

ηagt
A|S, νenv
S|A

, (D5)
with the associated joint process AS, called the percept-action process, having distribution
pAS = ηagt
A|Sνenv
S|A (D6)
see also Figure 8a. Alternatively, it is possible to specify a Markov model for the agent and/or environment. For
instance,
agtM →←envM =
 
Θagt, pagt
M0A0 , Φenv, penv
Z0

, (D7)
20
denotes the percept-action loop where both Markov models are specified, with the associated process MASZ , called
the global process, having distribution over action, percept, and hidden states
pMASZ (m, a, s, z) = pagt
A0M0 (a0, m0)penv
Z0 (z0)
∞Y
t=0
θagt(at+1, mt+1|st, mt)ϕenv(st, zt+1|at, zt), (D8)
see also Figure 8b. The models agt →←envM and agtM →←env are defined correspondingly.
Lemma 4 (Global Markov chain) . Let agtM →←envM be a percept-action loop global process distribution given in
eq. (D8). Then, the stochastic process U, where
Ut = (Mt, At, St, Zt), (D9)
is a homogeneous finite-state Markov chain which will be called the global Markov chain of the percept-action loop.
Proof.
First we check the Markov property, that is,
p(un|u0:n−1un−1) = p(un|u′
0:n−1un−1), (D10)
for any n ≥ 1 and u0:n−1, u′
0:n−1, where un = (xn, an, sn, zn). Note that for better readability, we drop p’s index.
This is a direct consequence of the Markov property of the Markov models for agent and environment which can
be seen as follows. For any n ≥ 1 we have by the definition of conditional probability
p(un|u0:n) = p(u0:n+1)
p(u0:n) , (D11)
where, by marginalizing the global distribution of a percept-action loop, eq. (D8) and writing un as (mn, an, sn, zn):
p(u0:n+1) = pagt
A0M0 (a0, m0)penv
Z0 (z0)

X
zn+1
ϕenv(sn, zn+1|an, zn)


n−1Y
t=0
θagt(at+1, mt+1|st, mt)ϕenv(st, zt+1|at, zt). (D12)
Due to the product structure of eq. (D12), most terms cancel out when we compute eq. (D11) and we are left with
p(un|un−1, un−2, . . .) =
hP
zn+1 ϕenv(sn, zn+1|an, zn)
i
θagt(an, mn|sn−1, mn−1)ϕenv(sn−1, zn|an−1, zn−1)
P
z′n
ϕenv(sn−1, z′n|an−1, zn−1) . (D13)
Since the right-hand side depends only on variables with time index n and n − 1, we have shown the Markov
chain property, eq. (D10). Further, since the right-hand side is determined by the transition matrices of agent and
environment, the Markov chain is homogeneous, and with this the lemma is proven. □
Appendix E: Markov conditions for percept-action loops
Bayesian networks are graphical models that represent probabilistic relationships among random variables using di-
rected acyclic graphs [73–75]. They allow for efficient reasoning about conditional independence through d-separation.
d-separation is a key concept in Bayesian networks that determines whether two sets of variables are independent
given a third set, based on the structure of the graph. It provides a formal criterion for understanding how information
flows through the network. This appendix introduces Bayesian networks in general and shows how to use them for
percept-action loops.
1. Bayesian networks and d-separation
Let {V1, . . . , Vn} be a set of n random variables and let G be a directed acyclic graph (DAG) such that for each
random variable in {V1, . . . , Vn} there is precisely one node in G. Let PAj be the set of parents of Vj and NDj the set
of non-descendants of Vj except itself. If B, C, Dare sets of random variables, I[B; C|D] is the conditional mutual
information with respect to the joint random variables constituting the sets, and I[B; C|D] = 0 means that B is
statistically independent of C, given D.
In the following, a path is defined as a sequence of nodes connected by edges, regardless of the direction of the
edges. The following definition is adapted from [76].
21
Definition 10 (d-separation). A path p in a DAG is said to be d-separated (or blocked) by a set of nodes D if at least
one of the following conditions holds:
(i) p contains a chain X → Y → Z or fork X ← Y → Z such that the middle node Y is in D, or
(ii) p contains an inverted fork (or collider) X → Y ← Z such that the middle node Y is not in D and such that
no descendant of Y is in D.
A set D is said to d-separate B from C if and only if D blocks every path from a node in B to a node in C.
Lemma 5 (Equivalent Markov conditions, [77, Theorem 3.27], see also [76, Lemma 1]). Let p(V1, . . . , Vn) be the joint
distribution of random variables V1, . . . , Vn (as always, in this work, with respect to a product measure). Then the
following three statements are equivalent:
(i) Recursive form: p(V1, . . . , Vn) admits the factorization
p(V1, . . . , Vn) =
nY
j=1
p(Vj|PAj), (E1)
where the notation p(Vj|PAj) is understood as p(Vj) if PAj is empty.
(ii) Local (or parental) Markov condition: for every node Vj we have
I[Vj; NDj |PAj] = 0, (E2)
i.e., it is conditionally independent of its non-descendants (except itself), given its parents.
(iii) Global Markov condition:
I[B; C|D] = 0 (E3)
for all three sets B, C, Dof nodes for which B and C are d-separated by D.
In the following, we will make extensive use of the notion of compatibility of a distribution with a Bayesian network,
which we define as follows.
Definition 11. Let p be a distribution over a set of variables W, and let G be a Bayesian network with nodes V such
that W ⊆ V . Then, the distribution p is said to be compatible with G if
I[B; C|D] = 0 (E4)
for all three sets B, C, D⊆ W of nodes for which B and C are d-separated by D.
Note that the conditions given by eq. (E4) are those global Markov conditions with respect to G which only involve
variables of p. Compatibility of p with G thus means that the Markov conditions implied by G for the variables of p
hold.
2. d-separation conditions for percept-action loops
One may initially be tempted to think that the Bayesian network depicted in Figure 9b is compatible with any
Φ = (ϕ(j|i))j,i with i ∈ W ×Mand j ∈ Y ×Z(see Figure 9a) in the sense that all local Markov conditions implied by
Figure 9b hold for any distribution pWXY Z(w, x, y, z) = ϕ(y, z|w, x)pWX (w, x). Figure 9b implies (by d-separation)
conditional independence of Y and Z given their parents, that is,
I[Y ; Z|W, X] = 0. (E5)
This condition, however, is easily shown to be violated by a channel which produces correlation independent of the
values of W and X. For instance, let ϕ(0, 0|w, x) = ϕ(1, 1|w, x) = 1/2 for all w ∈ Wand x ∈ Mand where 0, 1 ∈ Y,
0, 1 ∈ Z. Then, eq. (E5) is clearly not fulfilled.
The problem related to Figure 9b can be solved with a little sleight of hand: We introduce an additional variable
V = Y Z, defined as the joint channel output, as depicted in Figure 9c [78]. This is by no means the only way to
22
W
X Z
Y
Φ
(a)
W
X Z
Y
(b)
W
X Z
Y
V
(c)
FIG. 9. On finding a compatible Bayesian network of a Markov channel with two inputs and two outputs. (a): Circuit diagram
of a memoryless channel with input ( W, X) and output (Y, z) described by transition matrix Φ, (b): a Bayesian network which
is not compatible with arbitrary Φ, and (c): a Bayesian network with an auxiliary variable V = Y Zwhich is compatible with
arbitrary Φ.
address the problem with Figure 9b (for instance [79, Lemma 1]), but it is a particularly simple solution which, as we
will see, allows one to use d-separation for percept-action loops. With this choice of V , we can write
pY ZV|WX = pY |V pZ|V pV |WX (E6)
where, since V = Y Z, the conditional distribution pV |WX is given by the transition matrix Φ, pV |WX (y, z|w, x) =
ϕ(v|w, x) with v = (y, z), and pY |V , pZ|V are delta distributions since
pY |V (y|v) = pY |Y Z(y|y′, z′) = δy,y′, (E7)
and similarly for pZ|V .
We recover the original channel from eq. (E6) through marginalization:
X
v∈V
pY ZV|WX (y, z, v|w, x) =
X
v∈V
pY |V (y|v)pZ|V (z|v)pV |WX (v|w, x) (E8)
=
X
y′∈Y,z′∈Z
pY |Y Z(y|y′, z′)pZ|Y Z(z|y′, z′)ϕ(y′, z′|w, x) (E9)
=
X
y∈Y,z∈Z
δy,y′δz,z′ϕ(y′, z′|w, x) (E10)
= ϕ(y, z|w, x). (E11)
M0
A0 S0
Z1Z0
W0
V0
M1
A1 S1
Z2
W1
V1
M2
A2 S2
Z3
W2
V2
M3
A3 S3
Z4
W3
V3
M4
A4 S4
Z5
W4
V4
FIG. 10. Bayesian network of a general percept-action loop.
Applying the Bayesian network representation Figure 9c to the Markov channels of agent and environment in
agtM →←envM leads us to the following
Given a distribution qV over a set of variables V compatible with a Bayesian network B, for any subset W ⊆ V ,
let G(q, W) denote the set of Markov conditions
I[S; T|R] = 0 (E12)
with respect to q for all three sets S, T, R⊂ W of nodes for which S and T are d-separated by R.
23
Lemma 6. For any agtM →←envM, the total distribution, of the form as given in eq. (D8), is compatible with the
Bayesian network in Figure 10.
The proof proceeds by constructing a distribution over all variables in Figure 10
(i) which admits a recursive form in the sense of Lemma 5(i) and thus, by Lemma 5, the global Markov conditions
must hold, and
(ii) such that the distribution of agtM →←envM is recovered through marginalization.
Proof. By Lemma 5, a distribution over the variables in the Bayesian network shown in Figure 10 fulfills the global
Markov conditions if and only if it factorizes as
pMASZV W = pV0 pZ0
∞Y
t=0
pMt|Vt pAt|Vt pVt+1|StMt pZt+1|Wt pSt|Wt pWt|AtZt. (E13)
Let pMASZV W be of the form in eq. (E13). Then, in particular those d-separations which involve only variables Mt,
At, St, and Zt, t ∈ N0, must hold. All that is left to show is that the distribution of any agtM →←envM, as given in
eq. (D8), can be recovered through marginalization from a distribution of the form in eq. (E13).
For all t ∈ N0, we set Vt = AtMt and Wt = StZt+1, and let
agtM →←envM =
 
Θagt, pagt
M0A0 , Φenv, penv
Z0

(E14)
be any percept action loop. Then, let pZ0 = penv
Z0 , and for all t ∈ N0 define those conditional distributions in eq. (E13),
which do not reduce to a delta distribution, to be
pVt+1|StMt(vt+1|st, mt) = θagt(at+1, mt+1|st, mt) for all vt+1 = (at+1, mt+1), and (E15)
pWt|AtZt(wt|at, zt) = ϕenv(st, zt+1|at, zt) for all wt = (st, zt+1). (E16)
For each t ∈ N, we consider all terms on the right-hand side of eq. (E13) which contain Vt and marginalize:
X
vt∈V
pMt|Vt(mt|vt) pAt|Vt(at|vt) pVt|St−1Mt−1 (vt|st−1mt−1) = θagt(at, mt|st−1, mt−1) (E17)
which follows from Vt = AtMt, and thus pMt|Vt and pAt|Vt are delta distributions, and eq. (E15). For each t ∈ N0, a
similar calculation for all terms on the right-hand side of eq. (E13) containing Wt yields ϕenv(st, zt+1|at, zt). Finally,
we consider all terms on the right-hand side of eq. (E13) which contain V0 and marginalize:
X
v0∈V
pV0 (v0)pM0|V0 (m0|v0)pA0|V0 (a0|v0) = pA0M0 (a0, m0), (E18)
which follows from V0 = A0M0. Finally, let pV0 be such that pA0M0 = pagt
A0M0 .
We thus constructed a distribution pMASZV W such that marginalizing out V and W yields eq. (D8). □
The following corollary shows that a simplified Bayesian network can be used when the environment is memoryless.
Recall that for a memoryless environment envMmemless, there exists a |A| × |S|stochastic matrix Φ env such that
νS|A(s|a) = Q∞
t=0 ϕenv (st|at) and, thus, the total distribution of the any agtM →←envmemless is of the form
pMAS (m, a, s) = pagt
A0M0 (a0, m0)
∞Y
t=0
θagt(at+1, mt+1|st, mt)ϕenv(st|at). (E19)
Corollary 2. For any envmemless, the total distribution, of the form as given in eq. (E19), is compatible with the
Bayesian network in Figure 11.
Proof. The corollary is a special case of lemma 6 where the environment is taken care of by setting pAt|St(at|st) =
ϕenv(st|at) for all t ∈ N.
Let env be a product environment channel. Then, the distribution of any agtM →←env =

Θagt, pagt
M0A0 , νenv
S|A

takes
the form
pMAS (m, a, s) = νenv
S|A(s|a)pagt
A0M0 (a0, m0)
∞Y
t=0
θagt(at+1, mt+1|st, mt), (E20)
and we have the following
24
M0
A0 S0
V0
M1
A1 S1
V1
M2
A2 S2
V2
M3
A3 S3
V3
M4
A4 S4
V4
FIG. 11. Bayesian network of an agent interacting with a memoryless environment channel.
M0
A0 S0
Z1Z0
W0
V0
M1
A1 S1
Z2
W1
V1
M2
A2 S2
Z3
W2
V2
M3
A3 S3
Z4
W3
V3
M4
A4 S4
Z5
W4
V4
FIG. 12. Bayesian network of an agent receiving percepts from a source. This is an edge case of a percept-action loop where
the environment is modeled as a product environment channel.
Lemma 7. Let env be a product environment channel.Then, for any percept-action loop agtM →←env with a total
distribution pMAS over the variables (M, A, S), that is of the form in eq. (E20), the Bayesian network in Figure 12
is compatible with pMAS .
The proof is similar to the proof of Lemma 6.
Proof. By Lemma 5, a distribution over the variables in the Bayesian network shown in Figure 12 fulfills the global
Markov conditions if and only if it factorizes as
pMASZV W = pV0 pZ0
∞Y
t=0
pMt|Vt pAt|Vt pVt+1|StMt pZt+1|Wt pSt|Wt pWt|Zt. (E21)
Let pMASZV W be of the form in eq. (E21). Then, in particular those global Markov which involve only variables
Mt, At, and St , t ∈ N0, must hold.
Further, Since product environment channels are hidden Markov channels, by Definition 9 for any product environ-
ment channel there must exist a Markov model (Φ env, penv
Z0 ) such that
νenv
S|A(s|a) =
X
z
penv
Z0 (z0)
∞Y
t=0
ϕenv (st, zt+1|at, zt) . (E22)
Further, by the definition of product environment channels (Definition 8) we have νenv
S|A(s|a) = νenv
S|A(s|a′) for all
a, a′ ∈ MN0 . Thus, for product environment channels, eq. (E22) must still hold if one sets all actions on the
right-hand side in eq. (E22) to some a ∈ A. In this case, we obtain
νenv
S|A(s|a) =
X
z
penv
Z0 (z0)
∞Y
t=0
˜ϕenv (st, zt+1|zt) , (E23)
where we defined a new |S × Z| × |Z|transition matrix ˜Φenv with coefficients
˜ϕenv (st, zt+1|zt) = ϕenv (st, zt+1|a, zt) . (E24)
25
Plugging eq. (E23) into eq. (E20) yields
pMAS (m, a, s) =
X
z
penv
Z0 (z0)pagt
A0M0 (a0, m0)
∞Y
t=0
˜ϕenv (st, zt+1|zt) θagt(at+1, mt+1|st, mt), (E25)
for the global distribution.
All that is left to show is that the distribution in eq. (E25) can be recovered through marginalization from a
distribution of the form in eq. (E21).
For all t ∈ N0, we set Vt = AtMt and Wt = StZt+1 and
pVt+1|StMt(vt+1|st, mt) = θagt(at+1, mt+1|st, at) for all vt+1 = (at+1, mt+1), and (E26)
pWt|Zt(wt|zt) = ˜ϕenv(st, zt+1|zt) for all wt = (st, zt+1). (E27)
For each t ∈ N, we consider all terms on the right-hand side of eq. (E21) which contain Vt and marginalize:
X
vt∈V
pMt|Vt(mt|vt) pAt|Vt(at|vt) pVt|St−1Mt−1 (vt|st−1mt−1) = θagt(at, mt|st−1, at−1) (E28)
which follows from Vt = AtMt, and thus pMt|Vt and pAt|Vt are delta distributions, and eq. (E26). Similarly, for each
t ∈ N0, we consider all terms on the right-hand side of eq. (E21) which contain Wt and marginalize:
X
wt∈W
pZt+1|Wt(zt+1|wt)pSt|Wt(st|wt)pWt|Zt(wt|zt) = ˜ϕenv(st, zt+1|zt) (E29)
which follows from Wt = StZt+1 and eq. (E27).
Finally, we consider all terms on the right-hand side of eq. (E21) which contain V0 and marginalize:
X
v0∈V
pV0 (v0)pM0|V0 (m0|v0)pA0|V0 (a0|v0) = pA0M0 (a0, m0), (E30)
which follows from V0 = A0M0. Finally, let pV0 be such that pA0M0 = pagt
A0M0 .
We thus constructed a distribution pMASZV W such that marginalizing out V , W, and Z yields eq. (E25). □
In Bayesian networks of percept-action loops, there can in general be infinitely many paths between two nodes X
and Y , as the total process MASZ extends to the infinite future. However, note that paths that go through nodes
that lie in the future of both X and Y must necessarily contain a collider. Those paths are therefore d-separated if
the collider and all of its children are not part of the separating set.
3. Existing approaches to the information theory of percept-action loops
In the previous section, we introduced a Bayesian network (Figure 10) for a general class of percept-action loops.
Existing information-theoretic treatments of percept-action loops such as [58, 80–82] also provide Bayesian networks,
see for example [58, figure 1], [80, equation 11], [81, figure 4.1b], and [82, figure 4]. These Bayesian networks mainly
deviate from our network in how the agent dynamics is modeled.
The difference between our network and the ones from the literature can be understood as follows. Since we model
the environment (respectively the agent) with a Markov channel on an input-output and a hidden-state register (see
Figure 9) we focus on incoming and outgoing random variables of this channel while being agnostic to its inner
workings. In comparison, from the perspective of our framework, existing approaches model variables inside the
channel (such as V in Figure 9c). For example, we recover the Bayesian network in [80, equation 11] from Figure 10
by considering variables Wt and Vt as the agent’s memory while ignoring variables Mt and Zt. While our approach
requires the introduction of auxiliary hidden variables Vt and Wt to obtain a compatible Bayesian network, we only
need a single transition matrix to model the agent (in [58, 80–82] two transition matrices are necessary). Accordingly,
our model is suitable in those contexts where one wishes to model the environment (respectively the agent) with a
single Markov channel on an input-output and a memory register.
26
Appendix F: Maximally predictive agent models
In computational mechanics, the concept of a maximally predictive Markov model is based on the idea that in order
to optimally predict the future, the model’s memory must store all relevant information from the past. A commonly
studied scenario involves a fixed input process X, which is transformed by a channel into an output process Y . In
this context, a Markov model with memory states M is defined as maximally predictive at time t if [17, 30]:
I[X0:t; Xt:∞|Mt] = 0 (F1)
and
I[Mt; Xt:∞|X0:t] = 0. (F2)
The first condition captures the notation of a maximally predictive memory Mt while the second condition states that
the Markov model cannot predict the inputs beyond their correlations with the past. Assuming the channel is causal,
as we do in this work, the latter simply corresponds to a d-separation.
However, it is important to notice that the above definition of maximally predictive Markov models was made in
the context of stationary ergodic processes without feedback (see e.g., [30]), i.e., where outputs do not influence future
inputs. It turns out that, in order to lift these assumptions, we need to suitably generalize the definition of maximally
predictive Markov models. As we will show, for the special case of stationary processes without feedback we recover
eq. (F1).
In the following, we use the convention that, for variables W, X, Xn:t, Y, Z, Zn:t with n, t∈ N0,
I [W; Xn:tY |Z] = I [W; Y |Z] (F3)
and
I [X; Y |Zn:t] = I [X; Y ] (F4)
if t ≤ n.
Definition 12. Let agt →←env be a percept-action loop. A model agtM for agt is said to be maximally predictive, or
for short predictive, of percept St in round t if
I [A0:t+1S0:t; St|Mt] = 0, (F5)
and an agent model is said to be asymptotically mean (a.m.) predictive if
⟨I [A0:t+1S0:t; St|Mt]⟩t = 0. (F6)
In the following, A→←env
pred denotes the set of agent models which are a.m. predictive for an environment channel env.
Note that Ces` aro limit in eq. (F6) exists since conditional mutual information can be rewritten as a sum of (positive
and negative) entropy rates, each of which converges by Lemma 3.
An agent model which is predictive at time t must encode in its memory Mt all information from past percepts S0:t
and actions A0:t+1 (including the current action) which helps predicting the current percept St.
By Equation (F6), an agent is a.m. predictive if eq. (F5) holds asymptotically in the Ces` aro sense. There are
multiple ways this condition can be satisfied. One possibility is that the agent is predictive for sufficiently many
rounds (e.g., on a subset of N0 with unit natural density). Alternatively, an agent would also be a.m. predictive if the
summands in eq. (F6), I [A0:n+1S0:n; Sn|Mn], decay sufficiently fast — say as 1 /n as n → ∞. Arguably the simplest
case (which already received some attention in the literature [19]) is when the agent is predictive at all times, meaning
that eq. (F5) holds for all n ∈ N0. In that case, there is an equivalent condition for a Markov model to be predictive.
Based on this equivalence, we will show that our definition of a.m. predictive Markov models reduces to the condition
in eq. (F1) from [30] when applied to stationary processes.
Lemma 8. Let agt →←env be a percept-action loop. An agent model agtM is predictive of the next percept at all times,
i.e.,
I [A0:t+1S0:t; St|Mt] = 0 ∀t ∈ N0, (F7)
if and only if it is predictive of all future percepts at all times,
I [A0:t+1S0:t; St:∞|Mt] = 0 ∀t ∈ N0. (F8)
27
Proof. (⇐) Suppose that I [A0:t+1S0:t; St:∞|Mt] = 0 for all t ∈ N0. By using the single-step chain rule of mutual
information (eq. (A9)), with W = A0:t+1S0:t, X = St, Y = St+1:∞, and Z = Mt, we can write
I[A0:t+1S0:t; St:∞|Mt] = I[A0:t+1S0:t; St|Mt] + I[A0:t+1S0:t; St+1:∞|MtSt] (F9)
for all t ∈ N0. Since the left-hand side vanishes by assumption (eq. (F8)), the nonnegativity of mutual infor-
mation implies that both terms on the right-hand side must independently vanish. In particular, that means
I[A0:t+1S0:t; St|Mt] = 0 for all t ∈ N0.
(⇒) The proof proceeds in two steps. First, we will show that
I [A0:t+1S0:t; Aj+1Sj|MtAt+1:j+1St:j] = 0 (F10)
for an arbitrary t ∈ N0, and for j ∈ {t, t+ 1, . . .}. Second, the proof is concluded by an application of the chain rule
of mutual information.
In order to show eq. (F10), first consider the case j = t: Using the chain rule of mutual information in the form of
eq. (A10) with W = A0:j+1S0:j, X = Sj, Y = Aj+1 and Z = Mj gives
I [A0:j+1S0:j; Aj+1Sj|Mj] = I [A0:j+1S0:j; Sj|Mj] + I [A0:j+1S0:j; Aj+1|MjSj] . (F11)
However, both terms on the right-hand side vanish, the first by assumption (eq. (F7)) and the second due to d-
separataion (see Figure 13), leaving us with
I [A0:j+1S0:j; Aj+1Sj|Mj] = 0 (F12)
for j ∈ N0. But eq. (F12) is just eq. (F10) with t = j.
Mj
Aj Sj
Zj
Mj+1
Aj+1
Zj+1
Sj+1
Mj−1
Aj−1 Sj−1
Zj−1
Mj+2
Aj+2
Zj+2
Sj+2
Mj−2
Aj−2 Sj−2
Zj−2
Mj+3
Aj+3
Zj+3
Mj−3
Aj−3 Sj−3
Zj−3
FIG. 13. Bayesian network for a percept-action loop (lemma 6), used in the proof of Lemma 8. Here blue nodes d-separates
red and green nodes.
What is left to show is the case where j > t. First note that eq. (F12) still holds in that case. Additionally
we will make use of several other conditions involving the random variables A0:j+1S0:j, Mt, At+1:j+1St:j, Mj and
Aj+1Sj. Relations between those random variables can be represented by the information diagram in Figure 14. For
example, eq. (F10) then corresponds to two information atoms in the diagram, l+f. Altogether we have the following
conditions:
I [A0:j+1S0:j; Aj+1Sj|Mj] = 0 = a + b + c + d + e + f, (F13)
I [Aj+1Sj; Mt|MjA0:j+1S0:j] = 0 = k, (F14)
I[A0:t+1S0:t; Mj|MtAt+1:j+1St:j] = 0 = m + l, (F15)
I[A0:t+1S0:t; Mj|MtAt+1:j+1St:jAj+1Sj] = 0 = m, (F16)
where the last equality in each line expresses the condition through the information atoms defined in Figure 14.
The first condition, eq. (F13), is just eq. (F12). The conditions in eqs. (F14) to (F16) follow from d-separation (see
Figure 15 where for visualization purposes we set t to j − 2).
From the information diagram in Figure 14 we see that eq. (F13) and eq. (F14) allow us to write
I [MtA0:j+1S0:j; Aj+1Sj|Mj] = a + b + c + d + e + f + k = 0. (F17)
28
Mt Mj
At+1:j+1St:j
A0:t+1S0:t Aj+1Sj
FIG. 14. Information diagram used in the proof of Lemma 8. Relevant information atoms are labeled.
Rewriting the left-hand side using the chain rule for mutual information in the form of eq. (A10) with W = Aj+1Sj,
X = MtAt+1:j+1St:j, Y = A0:t+1S0:t and Z = Mj gives
I [MtA0:j+1S0:j; Aj+1Sj|Mj] = I [MtAt+1:j+1St:j; Aj+1Sj|Mj] + I[A0:t+1S0:t; Aj+1Sj|MtMjAt+1:j+1St:j]. (F18)
Since the left-hand side vanishes (eq. (F17)), the nonnegativity of mutual information implies that both terms on the
right-hand side must independently vanish; in particular, I[A0:t+1S0:t; Aj+1Sj|MtMjAt+1:j+1St+j] = f = 0. Further,
since eq. (F15) and eq. (F16) imply l = 0, we can then write
I [A0:t+1S0:t; Aj+1Sj|MtAt+1:j+1St:j] = f + l = 0 (F19)
for all j > t. Together, this then completes the proof of eq. (F10) for all j ≥ t.
Applying the chain rule of mutual information (eq. (A9)) to eq. (F10) yields
∞X
j=t
I [A0:t+1S0:t; Aj+1Sj|MtAt+1:j+1St:j] = I [A0:t+1S0:t; At+1:∞St:∞|Mt] = 0. (F20)
Further, by the chain rule of mutual information ( eq. (A10)) we have
0 = I [A0:t+1S0:t; At+1:∞St:∞|Mt] (F21)
= I [A0:t+1S0:t; St:∞|Mt] + I [A0:t+1S0:t; At+1:∞St:∞|MtAt+1:∞] . (F22)
Now, by the nonnegativity of mutual information, each summand ion the right-hand side must vanish individually.
In particular, (I [A0:t+1S0:t; St:∞|Mt] = 0 which concludes the proof of the lemma.
The previous lemma can be used to show that definition 12 reduces to the condition given in eq. (F1) in the case
where the global process is stationary and the environment is modeled by a product environment channel. A stochastic
process is said to be stationary if its distribution pX admits [33, p.87]
pXn:m = pXn+t:m+t (F23)
for all n, t∈ N0 and m > nwhere pXn:m is obtained from pX through marginalization.
Theorem 6. Let agtM →←env be such that the joint process MAS of actions, percepts, and agent memory is stationary.
Then, agtM is a.m. predictive, i.e.,
⟨I[A0:t+1S0:t; St|Mt]⟩t = 0 (F24)
if and only if
I[A0:t+1S0:t; St:∞|Mt] = 0 ∀t ∈ N0. (F25)
If in addition env is a product channel (definition 8), agtM is a.m. predictive if and only if
I[S0:t; St:∞|Mt] = 0 ∀t ∈ N0. (F26)
29
(eq. (F14))
Mj
Aj Sj
Zj
Mj+1
Aj+1
Zj+1
Sj+1
Mj−1
Aj−1 Sj−1
Zj−1
Mj+2
Aj+2
Zj+2
Sj+2
Mj−2
Aj−2 Sj−2
Zj−2
Mj+3
Aj+3
Zj+3
Mj−3
Aj−3 Sj−3
Zj−3
(eq. (F15))
Mj
Aj Sj
Zj
Mj+1
Aj+1
Zj+1
Sj+1
Mj−1
Aj−1 Sj−1
Zj−1
Mj+2
Aj+2
Zj+2
Sj+2
Mj−2
Aj−2 Sj−2
Zj−2
Mj+3
Aj+3
Zj+3
Mj−3
Aj−3 Sj−3
Zj−3
(eq. (F16))
Mj
Aj Sj
Zj
Mj+1
Aj+1
Zj+1
Sj+1
Mj−1
Aj−1 Sj−1
Zj−1
Mj+2
Aj+2
Zj+2
Sj+2
Mj−2
Aj−2 Sj−2
Zj−2
Mj+3
Aj+3
Zj+3
Mj−3
Aj−3 Sj−3
Zj−3
FIG. 15. Bayesian networks for a percept-action loop (lemma 6) with colorized d-separations (blue d-separates red and green)
used in the proof of Lemma 8.
Proof. For the first part of the theorem, we rewrite eq. (F24) as
lim
N→∞
cN = 0 (F27)
where we define
cN :=
N−1X
t=0
bt
N (F28)
bt := I[A0:t+1S0:t; St|Mt]. (F29)
First, we will show that bt is nonnegative, bounded and monotone increasing as t → ∞. Clearly, nonnegativity is
given since conditional mutual information is nonnegative, and the expression for bt is upper bounded by log |Y|. In
30
Mt
At St
Zt
Mt+1
At+1
Zt+1
St+1
Mt−1
At−1 St−1
Zt−1
Mt+2
At+2
Zt+2
St+2
Mt−2
At−2 St−2
Zt−2
Mt+3
At+3
Zt+3
Mt−3
At−3 St−3
Zt−3
FIG. 16. Bayesian network for an product environment channel (lemma 7) with colored d-separarion (blue d-separates red and
green) used in the proof of Theorem 6.
order to show that (bt) is monotone increasing, we use the chain rule for mutual information in the form of eq. (A10)
with W = St+j, X = Aj+1:t+j+1Sj:t+j, Y = A0:jS0:j and Z = Mt+j:
I[A0:t+j+1S0:t+j; St+j|Mt+j] = I[Aj:t+j+1Sj:t+j; St+j|Mt+j] + I[A0:jS0:j; St|MtAj:t+j+1Sj:t+j]. (F30)
Using stationarity (eq. (F25)) of the process MAS, we find pM0:t+1A0:t+1S0:t+1 = pMj:t+j+1Aj:t+j+1Sj:t+j+1 for any
t, j∈ N0, which can be marginalized to the statement pMtA0:t+1S0:t = pMt+jAj:t+j+1Sj:t+j . Thus, I[A0:t+1S0:t; St|Mt] =
I[Aj:t+j+1Sj:t+j; St+j|Mt+j]. Plugging this into eq. (F30) yields
I[A0:t+j+1S0:t+j; St+j|Mt+j] = I[A0:t+1S0:t; St|Mt] + I[A0:jS0:j; St|MtAj:t+j+1Sj:t+j]. (F31)
From this, using the nonnegativity of mutual information, we obtain
I[A0:t+j+1S0:t+j; St+j|Mt+j] ≥ I[A0:t+1S0:t; St|Mt] ∀t, j∈ N0, (F32)
or equivalently bt+j ≥ bt, which proves that (bt) is monotone increasing.
Further, since cN is defined as the arithmetic mean of b0, b1, . . . , bN−1, we have that cN is bounded and monotone
increasing as N → ∞.
We are now in the position to prove the first part of the theorem. By the monotone convergence theorem and the
properties of cN , the limit lim
N→∞
cN exists and equals the supremum. Therefore, eq. (F24) holds true if and only if cN
is zero for all N ∈ N0 which, in turn, is the case if and only if bt is zero for all t ∈ N0. Further, by lemma 8, this is
equivalent to eq. (F26) which concludes the proof of the first part of the theorem.
For the second part of the theorem, we need to show that, given the assumption that the environment channel is
also a product channel, eq. (F25) is equivalent to eq. (F26). Using the single-step chain rule of mutual information
(eq. (A10)), we can split up eq. (F25) as
I[A0:t+1S0:t; St:∞|Mt] = I[S0:t; St:∞|Mt] + I[A0:t+1; St|MtS0:t] ∀t ∈ N0 (F33)
The second term on the right-hand side vanishes for product environment channels due to d-separation (see Figure 16)
and the first term corresponds to eq. (F26) which concludes the proof.
The next theorem provides a condition for the existence of predictive agent models:
Theorem 7. Let agt →←env be any percept-action loop. If the environment channel is unifilar, then there exists an
a.m. predictive agent model agtM for agt.
The proof is based on the idea that the agent’s memory can be extended to store and update the hidden state
of the unifilar environment model. Knowledge of the hidden states of an environment model makes the agent predictive.
Proof. The proof proceeds by construction.
Let
• agtM′ = (Θagt, pM′
0A0 ) be a Markov model for agt with memory states M′;
• envM = (Φenv, pZ0 ) be a unifilar Markov model for env on some hidden-state alphabet Z.
31
We will now construct a transition matrix Θ MY on M × Y, where Y is the input-output alphabet of agt →←env and
M = M′ × Y′ × Z′ (F34)
where Y′ and Z′ are copies of Y and Z, respectively, and M′ is the hidden-state alphabet of agtM′.
Let ΘMY decompose as shown in the following circuit diagram, Figure 17. that is,
M′
Y
Y′
Z′
ΘMY
M′
Y
Y′
Z′
=
u(s, a, z)
UY′Z′Y
Θagt
M′Y
copy
ΓYY′
FIG. 17. Circuit diagram for the decomposition of Θ MY. Time flows from left to right, wires correspond to alphabets, boxes
to operations on the respective alphabets, bullets on wires indicate that the alphabet value controls another operation (above,
the unifilarity map u and a copy operation, respectively) but does not change itself.
ΘMY = (ΓYY′ ⊗ 1MZ′)
 
Θagt
M′Y ⊗ 1Y′Z′

(UY′Z′Y ⊗ 1M′) . (F35)
For clarity, indices indicate the memories on which the respective transition matrices act, in particular (from right to
left)
• 1 is the identity matrix on the memories indicated as indices,
• Θagt
M′Y = Θagt is the transition matrix of agtM’,
• UY′Z′Y is a deterministic transition matrix which acts as the identity on Y′Y and which sets the state of Z′
to u(y′, y, z′) where y′, y, and z′ are the current symbols on memories Y′, Z′, and Y, respectively, and u is a
unifilarity map (see the discussion below definition 7) of the unifilar environment model envM,
• ΓYY′ is a deterministic transition matrix which copies the symbol of memoryY to Y′ while leaving Y unchanged.
By construction, each of the three factors on the right-hand side of eq. (F35) is a valid transition matrix mapping
M × Y= M′ × Y′ × Z′ × Yto itself and thus Θ MY is also.
Define δi,j to be one if i = j and zero otherwise. Define the distribution pM0 = pM′
0 pY ′
0 pZ′
0 where pM′
0 is from agtM’,
pY ′
0 (y) = δy,y0 where y0 is the initial action, and pZ′
0 (z) = δz,z0 where z0 is the initial hidden state of envM (recall
that by unifilarity there exists a definite initial state). Further, define agtM = (ΘMY, pM0 , pA0 ).
By eq. (F35), the transition matrix of agtM first applies the transition matrix of agtM’, then updates the Z′ memory
using the unifilarity map, and updates the Y′ memory by copying Y to Y′.Thus, the only term which can lead to a
change of the Y and M′ memories is Θagt
M′Y. Further, pM0 and pM′
0 coincide on M′. Therefore, agtM and agtM′ both
model agt.
What is left to show is that agtM is a.m. predictive. For this, note that Mt = (M′
t, Y′
t , Z′
t) is initialized such that
Z′
0 = Z0 and Y ′
0 = A0. Further, by construction, agtM updates the Z′ and Y′ memories such that Z′
t = Zt and
Y ′
t = Yt for all times. We then have
I [A0:n+1S0:n; Sn|Mn] = I [A0:n+1S0:n; Sn|M′
nY ′
nZ′
n] (F36)
= I [A0:n+1S0:n; Sn|M′
nAnZn] (F37)
= 0, (F38)
where in eq. (F37) we used that Z′
n = Zn and Y ′
n = Yn and eq. (F38) follows from d-separarion in the Bayesian
network of agtM →←envM, see Figure 19. □
Corollary 3. Let agt →←env be any percept-action loop with env a unifilar source environment. Then there exists an
a.m. predictive agent model agtM for agt.
32
However, for future reference we point out that the proof and in particular the construction of the predictive
agent model can be simplified since, for unifilar source environments, there exist unifilar models whose hidden state
z′ = u(s, z) are a function of the current percept s and the current hidden state z but not of the action. Thus, the
decomposition of Θ in Figure 17 can be replaced by the simpler decomposition in Figure 18 and, in analogy to the
proof of theorem 7, one can show that agtM is predictive.
M′
Y
Z′
ΘMY
M′
Y
Z′
=
u(s, z)
UZ′Y
Θagt
M′Y
FIG. 18. Circuit diagram for the decomposition of Θ MY for unifilar source environments.
Mj
Aj Sj
Zj
Mj+1
Aj+1
Zj+1
Sj+1
Mj−1
Aj−1 Sj−1
Zj−1
Mj+2
Aj+2
Zj+2
Sj+2
Mj−2
Aj−2 Sj−2
Zj−2
Mj+3
Aj+3
Zj+3
Mj−3
Aj−3 Sj−3
Zj−3
FIG. 19. Bayesian network for a percept-action loop (lemma 6) with colorized d-separarion (blue d-separates red and green)
used in the proof of Theorem 7.
Appendix G: The extractable work in percept-action loops
In this framework, memory is represented by a physical system coupled to a thermal reservoir at temperature T.
The system possesses a few degrees of freedom, the information-bearing degrees of freedom, which are assumed to
be meta-stable, i.e., their equilibration time τinfo is much larger than that of the system’s other degrees of freedom,
τothers. Information processing on the information-bearing degrees of freedom is carried out through an isothermal
protocol, i.e., a protocol executed at constant temperature T, with a time scale such that τothers ≪ τprotocol ≪ τinfo.
The protocol has access to a work reservoir for storing (or retrieving) work.
1. Derivation of work capacity
Let X be a finite set of information-bearing degrees of freedom of an information reservoir [37], pXin an arbitrary
initial distribution over X, and Φ = ( ϕ(j|i))j,i an arbitrary transition matrix where i, j∈ X. Then, given that the
available knowledge of about the information-bearing degrees of freedom X is pXin , the work W which one can expect
(with respect to pXin ) to extract by implementing an isothermal process realizing Φ on X is upper-bounded by the
second law of thermodynamics as [7, 8]
W ≤ H (Xout) − H (Xin) (G1)
where W is in units of kBT ln 2, and Xout is distributed as
pXout (xout) =
X
xin∈X
ϕ(xout|xin)pXin (xin), (G2)
33
called the output distribution. Note that the upper bound in eq. (G1) can be positive, negative, or zero. In particular,
if the expected extracted work W is negative, realizing the isothermal process requires work, if it is positive, work
can be gained.
If an agent implements an isothermal process such that the expected extracted work equals the upper bound in
eq. (G1), we say that the agent is Landauer efficient, in reference to Landauer’s bound on the erasure of one bit,
which is a special case of eq. (G1).
Based on the assumption that eq. (G1) holds, we will derive an upper bound on the work an agent agtM can expect
to extract by undergoing a percept-action loop with an environment env.
Let agtM →←env =

Θagt, pagt
M0A0 , νenv
S|A

be a percept-action loop with identical action and percept alphabets A = S
and memory alphabet M of the agent. Then, based on eq. (G1), the work an agent can expect to extract by
implementing Θagt in between rounds (channel uses) t and t + 1 is upper bounded by
Wt→t+1 ≤ H (At+1, Mt+1) − H (St, Mt) . (G3)
Taking the Ces` aro limit (for a definition, see eq. (A14)), we find an upper bound on the expected extracted work per
round:
⟨Wt→t+1⟩t ≤ ⟨H (At+1, Mt+1) − H (St, Mt)⟩t . (G4)
It is convenient to regroup terms in the Ces` aro sum on the right-hand side of this expression:
⟨H (At+1, Mt+1) − H (St, Mt)⟩t = lim
n→∞
1
n
n−1X
t=0
[H (At+1, Mt+1) − H (St, Mt)] (G5)
= lim
n→∞
1
n
 
H (A0, M0) +
n−1X
t=0
[H (At+1, Mt+1) − H (St, Mt)]
!
(G6)
= lim
n→∞
1
n
 n−1X
t=0
[H (At, Mt) − H (St, Mt)]
!
(G7)
= ⟨H (At, Mt) − H (St, Mt)⟩t (G8)
where in eq. (G6) we added H (A0, M0) inside the Ces` aro limit which does not change the result because it vanishes
as n → ∞.
Then, we can rewrite the argument of the Ces` aro limit using twice the definition of conditional entropy:
H (At, Mt) − H (St, Mt) = H (At|Mt) + H (Mt) − H (St|Mt) − H (Mt) (G9)
= H (At|Mt) − H (St|Mt) . (G10)
We define
Wt(agtM →←env) := H (At|Mt) − H (St|Mt) (G11)
as the extractable work for round t and
W(agtM →←env) := ⟨H (At|Mt) − H (St|Mt)⟩t (G12)
as the work rate, the a.m. extractable work (both in units of kBT ln 2).
2. Existence of Landauer-efficient agents
The bound on expected extracted work for a single isothermal implementation of a transition matrix, eq. (G1), can
be reached using efficient protocols. These protocols typically have idealized requirements such as arbitrary energy
functions or infinite timescales; see for example [19] for a protocol based on over-damped Brownian motion in a
controllable energy landscape.
In the following we will outline, for any percept-action loop agtM →←env and provided that such idealized protocols
are available, how an implementation for agtM can be found which extracts all a.m. extractable work, eq. (G12) using
only finite memory. Such agents will be called Landauer efficient.
34
FIG. 20. An agent interacting with the cascade of two environment channel env1 and env2.
To this end, recall that any agtM →←env can be represented through a finite-state global Markov process of some
agtM →←envM which models agtM →←env, see lemma 4. By corollary 1, this process is asymptotically periodic with a
finite period in the sense of corollary 1. Let d be this period. That is, there are only d asymptotically expected
input distributions for the agent’s transition matrix, lim
n→∞
pMdn+cSdn+c for c ∈ {1, 2, . . . , d}, which repeat in the same
periodic manner. We will now exploit this to construct a Landauer-efficient agent.
Let us extend the agent agtM by a separate deterministic counter c which starts at 1 and, with every round, if c < d
counts up or if c = d resets to 1. This additional counter memory is fully deterministic and thus has zero entropy for
all times. It therefore does not contribute to the extractable work.
Now, consider a protocol implementing the agent which, conditioned on the counter c, implements one of d efficient
protocols optimized for the asymptotically expected distribution lim
n→∞
pMdn+cSdn+c. Thereby, we have constructed a
protocol which implements agtM in a Landauer-efficient way using only finite memory.
3. Definition and properties of work capacity
For a given environment, the extracted work in eq. (G12) depends not only on an agent’s input-output behavior,
as characterized by an agent’s channel agt, but also on the agent’s memory usage, as specified by a model agtM. The
environment’s capacity to do work is then defined as the supremum of the work rate with respect to all agent models
agtM.
Definition 13. The work capacity Cwork of channel env = νenv
A|S is defined as
Cwork(env) := max
agtM∈A→←env
W(agtM →←env) (G13)
where W(agtM →←env) := ⟨H (At|Mt) − H (St|Mt)⟩t is the work rate. An agent model agtM is said to be efficient
with respect to an environment channel env if W(agtM →←env) = Cwork(env) and the set of efficient agents is denoted
A→←env
eff .
In the following we will prove various properties of work capacity.
Theorem 8. For any percept-action loop agtM →←env with action-percept alphabet Y, the channel capacity Cwork(env)
has the following properties:
(i) (Existence) The limit in the definition of Cwork(env) exists,
(ii) (Bounds) 0 ≤ Cwork(env) ≤ ln |Y|,
(iii) (Subadditivity under channel cascade) Let env1 = penv1
S|A and env2 = penv2
S|A be two hidden Markov channels.
Define the cascade env2 ◦ env1 = penv2◦env1
S|A of env1 and env2 as
penv2◦env1
S|A (s|a) =
X
i∈YN0
penv2
S|A′(s|i)penv1
S′|A(i|a), (G14)
see also Figure 20. Then,
Cwork(env2 ◦ env1) ≤ Cwork(env1) + Cwork(env2). (G15)
35
Before we prove the theorem, the following definition is made.
Definition 14. For any environment channel env, let A→←env
mea denote the set of agent models which interact with env
such that
⟨H(At|Mt)⟩t = ln |A|, (G16)
i.e., the a.m. entropy over actions given the agent’s memory is maximal.
The index mea stands for maximum entropy actions .
Proof of (i):
By Lemma 4, the global process U = (Ut)∞
t=0 = (Mt, At, St, Zt)∞
t=0 is a homogeneous Markov chain. Let Λ be its
transition matrix. Then, work capacity, as given in eq. (G13), can be rewritten as
Cwork(env) = max
agtM

gpU0
 
Λt
t , (G17)
where gpU0
is a function from the set of transition matrices to the real numbers:
gpU0
 
Λt
= H (Mt, At) − H (Mt, St) , (G18)
where pMtAt and pMtSt are obtained from pUt = ΛtpU0 through marginalization. Since g is continuous, existence
follows from corollary 1(iii). □
Proof of (ii):
We first prove the upper bound.
For all t ∈ N0, the summands H (St|Mt) − H (At|Mt) in the expression for work capacity, eq. (G13), are bounded
from above as
H (St|Mt) − H (At|Mt) ≤ log |Y| −0, (G19)
where the upper bound of conditional entropy, H (St|Mt) ≤ H (St) ≤ log(|Y|), was used to obtain an upper bound
for the first term, and the nonnegativity of conditional entropy, 0 ≤ H (At|Mt), was used to obtain an upper bound
for the second term. (Note that An takes values in Y.) The upper bound in eq. (G19) depends only on the dimension
of the action-percept alphabet and thus is independent of the choice of the agent Markov model. Applying eq. (G19)
to each summand in eq. (G13) yields the upper bound on work capacity.
What is left is the proof for the lower bound.
The proof proceeds by showing that for any env there exists an agent model agtM which has zero extracted
work in each step. Consider an agent which implements the identity map from percept St to action At+1, that is
pAt+1|St(at+1|st) = δat+1,st for all t ∈ N0. This implies
H (St) = H (At+1) (G20)
for all t ∈ N0. Further, note that since agt only employs the identity map there exists a memoryless agtM (i.e., with
|M| = 1) which models it. We thus have
H (St|Mt) = H (St) (G21)
H (At+1|Mt) = H (At+1) . (G22)
Plugging eqs. (G20) to (G22) into the expression for a.m. extracted work (eq. (G12)) yields zero.
We have thus shown that for any environment there exists an agent with zero extracted work. Since the definition
of work capacity involves a maximum with respect to agents, this proves nonnegativity of work capacity for all
environments. □
Proof of (iii):
Let env12 be the channel which is obtained by alternating between env1 and env2 every round, see Figure 21. That
is, the action and percept processes of env12 are
A12 = A1
0A2
0A1
1A2
1 ··· (G23)
S12 = S1
0S2
0S1
1S2
1 ··· (G24)
36
FIG. 21. An agent which alternates between using channels env1 and env2.
where Ak
t (respectively Sk
t ) are the inputs (respectively outputs) of envk where k = 1, 2.
Then, the work capacity of env12 is by definition
Cwork(env12) = max
agtM
lim
N→∞
PN−1
t=0
 
H
 
A1
t |X1
t

− H
 
S1
t |X1
t

+ H
 
A2
t |X2
t

− H
 
A2
t |X2
t

N , (G25)
where the notation X = X1
0 X2
0 X1
1 X2
1 was used for the agent’s memory process in order to match the indexing of
eqs. (G23) and (G24). Then, by replacing the supremum over a sum of terms with a supremum over individual terms,
we obtain an upper bound:
Cwork(env12) ≤ max
agtM
lim
N→∞
PN−1
t=0
 
H
 
A1
t |X1
t

− H
 
S1
t |X1
t

N + (G26)
+ max
agtM
lim
N→∞
PN−1
t=0
 
H
 
A2
t |X2
t

− H
 
A2
t |X2
t

N (G27)
= Cwork(env1) + Cwork(env2). (G28)
Further, note that the a.m. extracted work of agents which implement an identity channel from outputs of env1 to
inputs of env2 is upper bounded by Cwork(env2 ◦ env1). However, since restricting the set of agents can only lead to
a smaller a.m. extracted work we have:
Cwork(env12) ≥ Cwork(env2 ◦ env1). (G29)
Then, eq. (G15) follows by combining eq. (G28) and eq. (G29). □
The following lemma provides simplified expressions for the work capacity (in units of kBT ln 2) of environment
channel env for the classes of channels defined in Definition 8.
Lemma 9.
Cwork(env) =



0 if env is noiseless,
max
pA0
[H (S0) − H (A0)] if env is memoryless invariant,
log|A| −h(S) if env is a unifilar product channel.
(G30)
Proof.
(i) Let env be a noiseless channel.
By Definition 8, S = A for a noiseless environment channel. Setting St = At in the expression for work capacity,
eq. (G13) yields Cwork (env) = 0.
(ii) Let env be a memoryless invariant channel.
Cwork(env) = max
pA0
(H (S0) − H (A0)) will be proven by showing the respective inequalities
Cwork(env) ≤ max
pA0
[H (S0) − H (A0)] (G31)
Cwork(env) ≥ max
pA0
[H (S0) − H (A0)] . (G32)
37
≤: In general, an upper bound on work capacity can be obtained by optimizing each summand of the work capacity
separately:
Cwork = max
agtM
⟨H (At, Mt) − H (St, Mt)⟩t (G33)
≤

max
agtM
[H (At, Mt) − H (St, Mt)]

t
. (G34)
This upper bound simplifies further for memoryless invariant environments as follows. Note that memoryless
invariant environments admit a description with a |Y| × |Y|transition matrix Φ such that the global process at
any time t is given by
pUt(ut) = pMtAtSt(mt, at, st) = ϕ(st|at)pMtAt(mt, at). (G35)
Thus, the maximization in eq. (G34) reduces to a maximization overpMtAt. In fact, this is the same optimization
problem for all t since Φ does not depend on t. Thus, the upper bound in eq. (G34) simplifies to
Cwork ≤ max
pA0M0
[H (A0M0) − H (S0M0)] . (G36)
Further, we find
H (A0, M0) − H (S0, M0) = H (A0) + H (M0) − I [A0; M0] − H (S0) − H (M0) + I [S0; M0] (G37)
where we used H (X, Y) = H(X) + H(Y ) − I[X; Y ] which is easily checked with an information diagram, see
Supplemental Material A. Using I [X1, Y] − I [X2, Y] = I [X1, Y|X2] − I [X2, Y|X1], eq. (G37) becomes
H (A0, M0) − H (S0, M0) = H (A0) − I [A0; M0|S0] − H (S0) + I [S0; M0|A0] . (G38)
Note that I [S0; M0|A0] = 0 due to d-separation (see Figure 22). Then, by the nonnegativity of conditional
M0
A0 S0
M1
A1 S1
M2
A2
FIG. 22. Bayesian network for a memoryless environment channel (corollary 2) with colorized d-separation (blue d-separates
red and green) used in the proof of Lemma 9.
mutual information, we find
H (A0, M0) − H (S0, M0) ≤ H (A0) − H (S0) (G39)
which proves the upper bound.
≥: Consider a memoryless agent model which, for all t, prepares its action in arg max pA0
[H (A0) − H (S0)], i.e.,
its extracted work is given by max
pA0
[H (S0) − H (A0)]. Since any agent’s extracted work is a lower bound on the
work capacity, this proves the lower bound.
Equations (G31) and (G32) imply equality.
(iii) We start by deriving an expression for the a.m. work production under the assumption that the environment
is modeled by a unifilar product environment channel. The a.m. work production in units of kBT ln 2 is given by
eq. (G12),
W(agtM →←env) = ⟨H (At|Mt) − H (St|Mt)⟩t . (G40)
Rewriting the second term in the Ces` aro limit using twice the definition of conditional mutual information eq. (A6)
we find
H (St|Mt) = H (St|MtS0:tA0:t+1) + I [S0:tA0:t+1; St|Mt] (G41)
= H (St|S0:t) − I [St; MtA0:t+1|S0:t] + I [S0:tA0:t+1; St|Mt] . (G42)
38
Mt
At St
Zt
Mt+1
At+1
Zt+1
St+1
Mt−1
At−1 St−1
Zt−1
Mt+2
At+2
Zt+2
St+2
Mt−2
At−2 St−2
Zt−2
Mt+3
At+3
Zt+3
Mt−3
At−3 St−3
Zt−3
FIG. 23. Bayesian network for an product environment channel (lemma 7) with colorized d-separation (blue d-separates red
and green) used in the proof of Theorem 9.
The term I [St; MtA0:t+1|S0:t] vanishes because of the d-separatoin shown in Figure 23. Using linearity of the Ces` aro
limit and the chain rule of entropy rate (eq. (A16)), we find for the a.m. work production:
W(agtM →←env) = ⟨H (At|Mt)⟩t − h (S) − ⟨I [S0:tA0:t+1; St|Mt]⟩t . (G43)
In particular, we see that eq. (G43) is upper bounded by setting the first term to its upper bound (log |Y|) and the
last term to its upper bound (zero):
W(agtM →←env) < log |Y| −h (S) . (G44)
Work capacity equals this upper bound if there exist an agent model which saturates it.
Consider now a class of agent models with memory states denoted by M′ which distributes their actions At
uniformly and independently from its inputsSt−1, M′
t−1 and its output memory M′
t, i.e., H (At|M′
t) = H
 
At|M′
t−1

=
H (At|St−1) = log |A|. This means, we have
pM′
tAt|M′
t−1St−1 = pAtpM′
t|M′
t−1St−1 (G45)
for all t ∈ N0 which results in a simplification in the Bayesian network of the percept-action loop, see Figure 24.
Further, since the environment is unifilar, by corollary 3 for any such agent agtM′ there exists a predictive agent
model agtM with memory states denoted by M constructed as in Figure 18. For predictive agent models, the last
term in eq. (G43) is zero (definition 12). What is left to show is that H (At|Mt) = log |A| for agtM. By construction
(Figure 18), we have Mt = M′
tZt and thus
H (At|Mt) = H (At|M′
tZt) (G46)
and by the definition of conditional mutual information:
H (At|M′
tZt) = H (At|M′
t) − I [At; Zt|M′
t] . (G47)
The first term on the right-hand side equals log |A| by the assumptions made for agtM′ and the second term vanishes
due to d-separation (actions are independent from all other variables, see 24).
Thus, work capacity equals the right-hand side in eq. (G44). □.
4. Efficient agent models
Theorem 9. For any unifilar product environment channel env,
A→←env
eff = A→←env
mea ∩ A→←env
pred , (G48)
with A→←env
eff the set of efficient agent models (Definition 13), A→←env
mea the set of agent models with a.m. maximum entropy
actions (Definition 14), and A→←env
pred the set of predictive agent models (Definition 12).
39
Mt
At St
Zt
Mt+1
At+1
Zt+1
Mt−1
At−1 St−1
Zt−1
FIG. 24. Bayesian network for an product environment channel (lemma 7) and agent with independently and uniformly
distributed actions (see eq. (G45)) used in the proof of Lemma 9.
Proof: Recall eq. (G43), the expression for work rate for a product environment channel:
W(agtM →←env) = ⟨H (At|Mt)⟩t − h (S) − ⟨I [S0:tA0:t+1; St|Mt]⟩t . (G49)
First assume that agtM ∈ A→←env
mea ∩ A→←env
pred . By Definition 14, agents in A→←env
mea fulfill
⟨H (At|Mt)⟩t = log |A|, (G50)
and, by Definition 12 agents in A→←env
pred fulfill
0 = ⟨I [S0:tA0:t+1; St|Mt]⟩t . (G51)
Plugging eqs. (G50) and (G51) into eq. (G49) yields W(agtM →←env) = log|Y| −h(S) which equals the work capacity
of unifilar product environment channels according to Lemma 9, and thus agtM ∈ A→←env
eff .
For the other direction, assume agtM ∈ A→←env
eff . Then,
0 = Cwork(env) − W(agtM →←env) (G52)
= log |A| − ⟨H (At|Mt)⟩t − ⟨I [S0:tA0:t+1; St|Mt]⟩t (G53)
where for the second line we used the expressions for work capacity of product environment channels (Lemma 9) and
extractable work of agents using a product environment channel (eq. (G49)).
Note that −⟨I [S0:tA0:t+1; St|Mt]⟩t is upper bounded by zero and ⟨H (At|Mt)⟩t is upper bounded by log |A|. The
expression in eq. (G53) is thus upper bounded by zero. Thus, agtM must be such that both upper bounds are reached.
By Definition 14, the set of agents which reach the upper bound for⟨H (At|Mt)⟩t is A→←env
mea , and, by Definition 12, the
set of agents which reach the upper bound for−⟨I [S0:tA0:t+1; St|Mt]⟩t is A→←env
pred . It follows that agtM ∈ A→←env
mea ∩A→←env
pred .□
Theorem 9 shows that efficient agents should be constructed such that they are predictive whenever the environment
is modeled by a unifilar product environment channel. This, however, is no longer true for general environment
channels. We first prove the following lemma which shows properties for a particular memoryless environment channel.
Lemma 10. Let environment env be a memoryless environment channel and such that At and St take values in an
alphabet A = S = {0, 1}. Let the environment’s transition matrix Φenv = (ϕenv(j|i))j,i with j, i∈ Abe such that
ϕenv(j|0) = δ0,j and ϕenv(j|1) = 1/2 for j = 0, 1. Then, for any agtM →←env we have
⟨I [At; St|Mt]⟩t = 0 ⇔ ⟨H (At|Mt)⟩t = 0. (G54)
Proof.
First note that if an agent model agtM admits ⟨H (At|Mt)⟩t = 0, then
⟨I [At; St|Mt]⟩t = ⟨H (At|Mt)⟩t − ⟨H (At|MtSt)⟩t = 0, (G55)
where we used the definition of mutual information (eq. (A6)), and the conclusion follows from the nonnegativity of
conditional mutual information and conditional entropy, proving one direction of eq. (G54).
40
For the other direction, for the environment env under consideration, by definition 6 there exists a Markov
model agtM on some state space Z and thus, by lemma 4, there also exists a global Markov chain. Let Γ be
the transition matrix and pM0A0S0Z0 the initial distribution of such a global Markov chain. By corollary 1)(i),
the global Markov chain must consist of convergent subsequences Γ (r)
∞ = lim
n→∞
Γnd+r with r ∈ {1, 2, . . . , d} and d
some finite integer. Let Γ (r)
∞ =

γ(r)
∞ (j|i)

j,i
and let Mr, Ar, Sr, and Zr be random variables with distribution
pArSrMrZr
(j) = P
i γ(r)
∞ (j|i)pM0A0S0Z0 (i) with i, j∈ M × A × S × Z. Then, according to corollary 1(iii), we have
⟨I [At; St|Mt]⟩t = 1
d
dX
r=1
I

Ar; Sr|Mr

, (G56)
and similarly
⟨H (At|Mt)⟩t = 1
d
dX
r=1
H
 
Ar|Mr

. (G57)
Using the definition of mutual information, we find for each summand in eq. (G56)
I

Ar; Sr|Mr

= H
 
Ar|Mr

− H
 
Ar|MrSr

. (G58)
We now want to show that, for any r ∈ {1, 2, . . . , d}, I

Ar; Sr|Mr

= 0 implies H
 
Ar|Mr

= 0.
The proof proceeds by contraction. Assume that I

Ar; Sr|Mr

= 0 but H
 
Ar|Mr

> 0.
First, using basic properties of conditional entropies, we have H
 
Ar|Mr

= P
m∈M pMr
(m)H
 
Ar|Mr = m

where
H
 
Ar|Mr = m

= 0 iff pAr|Mr=m is a delta distribution.
Then, due to H
 
Ar|Mr

> 0, there exists a memory state m′
r ∈ Mwith pMr
(m′
r) > 0 such that pAr|Mr
(0|m′
r) > 0
and pAr|Mr
(1|m′
r) > 0. We have
I
 
Ar; Sr|Mr

=
X
mr∈M
pMr
(mr)I

Ar, Sr|Mr = mr

(G59)
where I

Ar, Sr|Mr = mr

is the mutual information I

Ar, Sr

with Ar, Sr distributed as pArSr|Mr=mr
. The expan-
sion in eq. (G59) can be obtained by writing out mutual information, eq. (A6), in terms of probabilities.
Now, by the nonnegativity of mutual information, for left-hand side of eq. (G59) to vanish, each summand
on the right-hand side of eq. (G59) must vanish individually. In particular, for the summand corresponding to
Mr = m′
r to vanish, I

Ar, Sr|Ar = m′
r

must be zero. Further, using basic properties of mutual information,
I

Ar, Sr|Mr = m′
r

= 0 iff pArSr|Mr=m′r
is a product distribution. However, note that for percept-action loops
with memoryless environment channel we have
pArSr|Mr=m′r
= pSr|Ar
pAr|Mr=m′r
(G60)
where pSr|Ar
(s|a) = ϕenv(s|a) is given by the memoryless environment which is chosen such thatϕenv(s|0) ̸= ϕenv(s|1)
for all s ∈ Sand, thus, pArSr|Mr=m′
r
is not a product distribution. By this contradiction, we have shown, for any
r ∈ {1, 2, . . . , d}, that I

Ar; Sr|Mr

= 0 implies H
 
Ar|Mr

= 0. By eqs. (G56) and (G57) it then follows that
⟨I [At; St|Mt]⟩t = 0, implies ⟨H (At|Mt)⟩t = 0. □
Theorem 10. There exist environment channels env such that the sets A→←env
pred , A→←env
mea , and A→←env
eff are all nonempty and
mutually exclusive.
Proof. We start with noticing that A→←env
eff and A→←env
mea are not empty for any environment. Further, a.m. predictive
agents (definition 12) must fulfill
0 = ⟨I [A0:t+1S0:t; St|Mt]⟩t (G61)
= ⟨I [At; St|Mt]⟩t + ⟨I [A0:tS0:t; St|MtAt]⟩t (G62)
where the second line follows from the chain rule of mutual information (eq. (A10)). Further, here and in the following
we make repeated use of the fact that the Ces` aro limit is linear for terms which converge individually.
41
Mt
At St
Mt+1
At+1 St+1
Mt−1
At−1 St−1
Mt+2
At+2 St+2
Mt−2
At−2 St−2
Mt+3
At+3
Mt−3
At−3 St−3
FIG. 25. Bayesian network for a memoryless environment channel (corollary 2) with colorized d-separation (blue d-separates
red and green) used in the proof of Theorem 10.
From now on, let env be the memoryless environment considered in lemma 10. Then, the second term vanishes
because of d-separation, I [A0:tS0:t; St|MtAt] = 0, depicted in Figure 25. Further, for the environment under consid-
eration we have by lemma 10,
⟨I [At; St|Mt]⟩t = 0 ⇔ ⟨H (At|Mt)⟩t = 0. (G63)
In particular, we have just seen that the left-hand side of eq. (G63) is the condition for an agent to be a.m. predictive.
Then, since there exist agents which remember their actions perfectly in the Ces` aro sense, i.e., they fulfill the right-
hand side of eq. (G63), such agents are also a.m. predictive. For example, take Mt = At for all t ∈ N0. Thus,
A→←env
pred ̸= ∅.
Using the expression for work capacity derived for memoryless environments, see lemma 9, after some straightfor-
ward algebra we obtain for the env under consideration [83]:
Cwork(env) = 1
2 ln
3
4 + 1√
2

> 0. (G64)
Further, the extractable work of any a.m. predictive agent is (by eq. (G12) and the linearity of the Ces` aro limit)
W(agtMpred →←env) = ⟨H (At|Mt)⟩t − ⟨H (St|Mt)⟩t (G65)
= −⟨H (St|Mt)⟩t ≤ 0. (G66)
Since Cwork(env) > 0, it follows that A→←env
eff ∩ A→←env
pred = ∅.
Next, we show that A→←env
eff ∩ A→←env
mea = ∅ for the particular environment channel under consideration. For all agent
models in A→←env
mea we have
W(agtMmea →←env) = ⟨H(At|Mt) − H(St|Mt)⟩t (G67)
= ⟨H(At|Mt)⟩t − ⟨H(St|Mt)⟩t (G68)
= log |A| − ⟨H(St|Mt)⟩t , (G69)
In the following we will determine⟨H(St|Mt)⟩t by showing that⟨H(St|Mt)⟩t = ⟨H(St)⟩t which then is easily computed
for the environment under consideration.
First note that we have ⟨I [St; At; Mt]⟩t ≥ 0 since
⟨I [At; Mt; St]⟩t = ⟨I [Mt; St] − I [Mt; St|At]⟩t (G70)
= ⟨I [Mt; St]⟩t (G71)
≥ 0, (G72)
since I [Mt; St|At] = 0 is a d-separation (shown for t = 0 in Figure 22). Further, since for all agent models in
A→←env
mea ⟨H(At|Mt)⟩t = log |A| takes its maximum value and since H(At|Mt) ≤ H(At) ≤ log |A| (see Supplemental
Material A2), we have ⟨H(At|Mt)⟩t = ⟨H(At)⟩t and thus ⟨I [At; Mt]⟩t = 0. Then, we have
0 = ⟨I [At; Mt]⟩t (G73)
= ⟨I [At; Mt|St]⟩t + ⟨I [At; Mt; St]⟩t . (G74)
The first term is nonnegative by the nonnegativity of conditional mutual information, the second term by eq. (G72).
Thus, both terms must vanish individually. With this, using a decomposition into information atoms we find
⟨H(St|Mt)⟩t = ⟨H(St) − I [St; At; Mt] − I [St; Mt|At]⟩t (G75)
= ⟨H(St)⟩t − ⟨I [St; At; Mt]⟩t (G76)
= ⟨H(St)⟩t . (G77)
42
For the environment under consideration and since agent models in A→←env
mea actions are uniformly distributed,
this is easily computed and found to be ln[256 /27]/ ln[16], which results in a work rate (in units of kBT ln 2) of
1 − ln[256/27]/ ln[16] for all agent models in A→←env
mea . Since this is smaller than the work capacity, eq. (G64), it follows
that A→←env
mea ∩ A→←env
eff = ∅.
What is left to show is that A→←env
mea ∩ A→←env
pred = ∅. Above, we showed that for all predictive agent models for the
environment under consideration, we have ⟨H (At|Mt)⟩t = 0 which contradicts the definition of agent models in A→←env
mea
which concludes the proof. □