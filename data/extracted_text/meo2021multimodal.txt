Multimodal VAE Active Inference Controller
Cristian Meo1 and Pablo Lanillos2
Abstract—Active inference, a theoretical construct inspired than a state-of-the-art model reference adaptive controller.
by brain processing, is a promising alternative to control Latterly, we presented a pixel-based deep AIF controller [7]
artificial agents. However, current methods do not yet scale
that incorporated generative model learning using convolu-
to high-dimensional inputs in continuous control. Here we
tional neural networks but limited to one sensor modality
presentanovelactiveinferencetorquecontrollerforindustrial
arms that maintains the adaptive characteristics of previous (visual) and velocity control. Thus, this work investigates
proprioceptive approaches but also enables large-scale multi- how to design a brain-inspired controller, with the adapt-
modal integration (e.g., raw images). We extended our previ- ability and robustness properties of AIF, that: 1) scales to
ous mathematical formulation by including multimodal state
high-dimensional inputs, 2) allows multisensory integration
representation learning using a linearly coupled multimodal
and 3) commands in torque.
variationalautoencoder.Weevaluatedourmodelonasimulated
7DOF Franka Emika Panda robot arm and compared its A. Contribution
behavior with a previous active inference baseline and the
Panda built-in optimized controller. Results showed improved WeproposeaMultimodalVartiationalAutoencoderActive
tracking and control in goal-directed reaching due to the Inference (MVAE-AIF) torque controller that combines free
increased representation power, high robustness to noise and energy optimization [14] with generative model learning
adaptability in changes on the environmental conditions and
(Fig. 1).
robot parameters without the need to relearn the generative
models nor parameters retuning.
Index Terms—Active inference, Bio-inspired perception and
action, Learning and adaptive systems, Free energy principle.
I. INTRODUCTION
Active inference (AIF) is prominent in neuroscientific
literature as a general mathematical framework of the brain
at the computational level [1]. According to this theory,
the brain learns by interaction a generative model of the
world/body that is used to perform state estimation (percep-
tion) as well as to execute actions, optimizing one single
objective: Bayesian model evidence. This approach, which
grounds on hierarchical variational inference and dynamical
systemsestimation[2],hasstrongconnectionswithBayesian
filtering [3] and control as inference [4], as it both estimates (a) Architecture
the system state and computes the control commands as a
result of the inference process.
Inthelastyears,someproof-of-conceptstudiesinrobotics
have shown that AIF may be a powerful framework to
address key challenges in robotics, such as adaptation,
robustness and abstraction in goal-directed tasks [5], [6],
[7], [8], [9], [10], [11], [12], [13]. For instance, a state
estimation algorithm and an AIF-based reaching controller
for humanoid robots were proposed in [5] and [6] respec- (b) Camera input I (c) Predicted input g v (z)
tively, showing robust sensory fusion (visual, proprioceptive
Fig. 1: MVAE-AIF architecture. (a) Abstract description of
and tactile) and adaptability to unexpected sensory changes.
the algorithm and environmental setup. State representation
However, they could only handle low-dimensional inputs.
learningisgivenbythemultimodalVAE,andestimationand
Meanwhile, AIF-based joint torque control was investigated
controlareprovidedbytheAIFframework.ThePandarobot
using proprioceptive inputs [8], showing better performance
had access to (noisy) proprioceptive input q and a camera
placedinfrontI.(b)Camerainput.(c)Predictedvisualinput
1Cristian Meo is with the Faculty of Mechanical Engineering, Depart-
ment of Cognitive Robotics, Delft University of Technology, Delft, The from the generative model.
Netherlandsc.meo@student.delft.nl
2Pablo Lanillos is with the Donders Institute for Brain, Cognition
We extended our previous AIF formulation [6], [7] to
and Behavior, Department of Artificial Intelligence, Radboud University,
Nijmegen,TheNetherlands.p.lanillos@donders.ru.nl work with high-dimensional multimodal input at the torque
1202
raM
7
]OR.sc[
1v21440.3012:viXra
level. Inspired by [15], [7] and [16], we designed a VAE where r ∼ N(0,Σ ) and w ∼ N(0,Σ ) are the
x z
that provides light coupled multimodal state representation sensory and process noise respectively. Σ and Σ are
x z
learning [17]. One of the main advantages of our approach the covariance matrices that represent the controller’s
is that the robot only has to learn the kinematic forward confidence about each sensory input and about its
mapping and then is the inference process that produces the dynamics respectively.
right torques for achieving the goal, even in the presence of
Variational Free Energy (VFE). TheVFEistheoptimiza-
unmodeled situations or external forces.
tion objective for both estimation and control. We use
We studied, on a simulated1 7DOF Franka Emika Panda
the definition of the F based on [14], where the action
industrial arm,
isimplicitwithintheobservationmodelx(a).Usingthe
1) How multimodal encoding improves representation
KL-divergence the VFE is:
power and thus, state estimation and control accuracy;
2) How the variational inference nature of our approach
F =KL[q(z)||p(z|x)]−logp(x) (3)
provides strong adaptation and robustness against un-
=KL[q(z)||p(z,x)]=−ELBO
modeled dynamics, environment parameter variations
and sensory noise.
State estimation using gradient optimization:
Results show its improved performance by qualitative and
statisticalcomparisonwithtwobaselines:proprioceptiveAIF
˜z˙ =Dz˜−k ∇ F(x˜,z˜) (4)
[8] and the Panda industrial arm built-in controller. For z z
reproducibility, the code is publicly available at https:
Control using gradient optimization:
//github.com/Cmeo97/MAIF.
II. AIFGENERALFORMULATIONANDNOTATION (cid:88)dx˜
a˙ =−k ·∇ F(x˜,z˜) (5)
Here we introduce the standard equations and concepts a da x˜
x
from the AIF literature [1], and the notation used in this
paper, framed for estimation and control of robotic sys- The VFE has a closed form under the Laplace and
Mean-field approximations [18], [6], [19] and it is
tems[6].Theaimoftherobotistoinferitsstate(unobserved
defined as:
variable) by means of noisy sensory inputs (observed). For
that purpose, it can refine its state using the measurements
F(z˜,x˜)(cid:39)−lnp(z˜,x˜)=−lnp(x˜|z˜)−lnp(z˜) (6)
or perform actions to fit the observed world to its internal
(cid:39)(x−g(x))TΣ−1(x−g(x))
model.Thisisduallycomputedbyoptimizingthevariational x
free energy, a bound on the Bayesian model evidence.
+(Dz−f(z))TΣ−
z
1(Dz−f(z))
1 1
System variables. State, observations, actions and their n- + ln|Σ |+ ln|Σ | (7)
2 x 2 z
order time derivatives (generalized coordinates).
x˜ =[x,x(cid:48),x(cid:48)(cid:48),...,xn] , sensors The first term of Eq. (7) is the sensor prediction error
and the second term is the dynamics prediction error.
z˜=[z,z(cid:48),z(cid:48)(cid:48),...,zn] , multimodal system state
µ˜ =[µ,µ(cid:48),µ(cid:48)(cid:48),...,µn] , proprioceptive state
III. MVAE-AIFCONTROLLER
˜r=[r,r(cid:48),r(cid:48)(cid:48),...,rn] , sensory noise
w˜ =[w,w(cid:48),w(cid:48)(cid:48),...,wn] , state fluctuations Themainnoveladditionpresentedinthisworkistheintro-
a={a ,...,a } , action (m actuators) duction of high-dimensional representation learning within
1 m
the AIF optimization framework to provide at the same
Where x(cid:48) = dx/dt. Depending on the formulation the
time multimodal state estimation and torque control. We
action a can be force, torque, acceleration or velocity.
first describe the architecture, the generative models and
In this work action refers to torque. We further define
the training. Second, we detail the estimation and control
the time-derivative of the state vector Dz˜ as:
equations and finally, we summarize the algorithm.
d
Dz˜= (z,z(cid:48),...,zn)=[z(cid:48),z(cid:48)(cid:48),...,zn+1]
dt
A. Architecture and design
Generative models. Two generative models govern the
robot; the mapping function between the robot’s state Our proposed multimodal variational autoencoder active
and the sensory input g(z˜) (e.g., forward kinematics) inference (MVAE-AIF) architecture is depicted in Fig. 1a.
and the dynamics of the internal state f(z˜). Figures 1b and 1c shows the camera input and the predicted
sensory input from the learnt generative model.
x˜ =g(z˜)+˜r (1)
1) Sensors: We assume that the robot has (noisy) propri-
Dz˜=f(z˜)+w˜ (2)
oceptive sensors that provide the joint angles q, velocities
q˙ and accelerations q¨. Furthermore, there is a camera that
1Plannedonsiteexperimentswiththerealplatformwerenotpossibleto
finishduetouniversityaccessrestrictions. provides images I of size (w×h) of the robot.
2) Robot generative models: The generative models of 5) Goal definition: We define the goal of the system fol-
the agent are approximations of the real generative models lowingasimilarapproachas[6].Thedesiredgoal,described
of the world and are composed of a sensory generative in the sensory space ρ = x , is introduced in the internal
d
functiong(z),whichmapstheinternalstatetothevisualand dynamics as follows:
proprioceptive spaces and a dynamics function f(z), which
∂g(z)
describes the evolution the system. f(z,ρ=x )=T(z)(x −g(z))= (x −g(z))
d d ∂z d
(15)
x =g (z)+r visual (8)
v v v
x =g (z)+r joints (9) Where T(z) is a function that maps the error in the sensory
q q q
Dz=f(z,ρ)+w state dynamics (10) space to the latent space. In this case, the partial derivative
z
of g(x) with respect to z. The desired goal in this work is
where x v and x q are the visual and proprioceptive predic- defined by the final image and joints angles x d ={I d ,q d }.
tions respectively. The processes noise r , r and w are
q v z
assumedtobedrawnfromamultivariatenormaldistribution B. Estimation and control
with zero mean and covariance matrices Σ q , Σ v and Σ z The online estimation and control problem is solved by
respectively. Note that we included ρ into the dynamics optimizing the VFE through gradient optimization, comput-
function. This variable will describe the desired goal and ing Equations (4) and (5).
acts as a prior that drives the system.
State estimation uses the Laplace approximation of F,
3) Generative models and state representation: The de-
described in Eq. (7). We update the multimodal state z by
fined sensory generative models are approximated by a means of the partial derivative of the VFE with respect to z
multimodalvariationalautoencoder(MVAE)whichmapsthe (z˙ =−k∂ F):
z
internalstaterepresentationtobothproprioceptiveandvisual
∂g ∂g
sensoryspacesintoacommonlatentspace.Wedesignedthe z˙ =k v ∂z IΣ− v 1(x v −g I (z))+k q ∂z qΣ− q 1(µ−g q (z))
MVAE using two couples of parallel encoders and decoders. ∂f ∂f
+k IΣ−1(I −f (z,ρ))+k qΣ−1(q −f (z,ρ))
TheMVAElatentspacedescribesthestateofthesystemand v ∂z v d I q ∂z q d q
the output of the decoders are the predicted image x and (16)
v
joint values x . The state of the system z is defined as:
q As we do not have access to the high order generalized
coordinates of the latent space z(cid:48),z(cid:48)(cid:48), we track both the
z=encoder (q)+encoder (I) (11)
q v multimodal shared latent space z and the higher orders of
the proprioceptive (joints) state µ(cid:48),µ(cid:48)(cid:48). This is very useful
As a result, modalities have a low correlation, allowing
aswecanusetheangularvelocityandaccelerationwithinthe
sensory integration without reciprocal degradation. In other
controller. Thus, we update the proprioceptive state velocity
words,ifoneofthetwomodalitiesisnotwellreconstructed, and acceleration by computing the partial derivative of F
the other one can still be inferred. For instance, in the with respect to µ(cid:48),µ(cid:48)(cid:48), as it was a unimodal proprioceptive
caseofvisualocclusion,proprioceptivereconstructionisstill controller [8], but the joint angles are predicted by the
MVAE.
possible.Therobotcaninferthesensoryinputgiventhestate
usingthedecoders,whichactasthegenerativelearntmodels: µ=g (z) (17)
q
x v =g v (z)=decoder v (z) (12)
µ˙(cid:48) =µ(cid:48)(cid:48)+k
µ
(cid:0) Σ−
q˙
1(q˙ −µ(cid:48))
x q =g p (z)=decoder q (z) (13) −Σ− µ 1(µ(cid:48)+µ−q d )−Σ µ − ˙ 1(µ(cid:48)(cid:48)+µ(cid:48)) (cid:1) (18)
µ˙(cid:48)(cid:48) =−k Σ−1(µ(cid:48)(cid:48)+µ(cid:48)) (19)
µ µ˙
As we only need the sensory generative models, the
encoders are just employed for the training. Then, in the whereΣ−1,Σ−1aretheprecision(inversevariance)matrices
µ µ˙
AIF control loop, only the decoders are used. The detailed related to joint angles and velocities beliefs.
description of the network layers can be found in Table I.
The action (torque) is computed by optimizing the VFE
For algorithmic purposes we also define the proprioceptive using Eq. (5). Here we only consider the proprioceptive
state(jointanglesbelief)asµ={g (z),µ(cid:48),µ(cid:48)(cid:48),µ(cid:48)(cid:48)(cid:48)},where errors.IntheAIFliteraturethismodelsthereflexarc,where
q
the first component is the predicted joint angles and the top-down proprioceptive predictions coming from higher
cortical levels produce the motoric reflexes [20]. Thus, the
others are the higher-order derivatives.
torquecommandsareupdatedwiththefollowingdifferential
4) Generative models learning: The MVAE was trained equation:
with a dataset containing 500000 images with size (128×
128)andtheassociatedjointsanglevalueswhichwecreated a˙ =−k a (cid:0) ∂ a µΣ− q 1(q−µ)+∂ a µ(cid:48)Σ− µ˙ 1(q˙ −µ(cid:48)) (cid:1) (20)
performing motor babbling on the robot arm. To accelerate =−k (Σ−1(q−µ)+Σ−1(q˙ −µ(cid:48))) (21)
a q µ˙
the training, we included a precision mask Π , computed
xv
Although we can compute the action inverse models
by the variance of all images and highlighting the pixels
∂ µ,∂ µ(cid:48) through online learning using regressors [21], we
with more information. The augmented reconstruction loss a a
let the adaptive controller absorb the non-linearities of the
employed was:
relation between the torque and the joint velocity. Thus, as
L=MSE(x +Π x , I)+MSE(x ,q) (14) described by [8] we just consider the sign of the derivatives.
v xv v q
C. Algorithm accuracy and robustness to sensory noise against the
baselines PAIF and BPC.
Algorithm 1 describes the proposed controller. The first
3) Adaptation study (Sec. IV-E). We evaluated the re-
stagesolvesthefreeenergyoptimizationusingtheprediction
sponse of the system to unmodeled dynamics and
errorsbetweenthegenerativemodelandthesensormeasure-
ments. We compute z˙ ,z˙ ,z˙ and z˙ as the four terms in environment variations by altering: the world gravity
q v qd vd
(Jupiter experiment), the compliance of the robot, and
Eq. 16. Since these terms are equivalent to backpropagation
thesensorynoise.Herewealsocomparedourapproach
operations over the respective errors, as we proposed in [7],
theyarecomputedbydefininganodewithz˙ onthenetwork with the PAIF and the BPC.
virtualtreeandbackpropagatingerrorsthroughthedecoders. In the experiments, we used the following evaluation
Once we have the contributions from the sensory and dy- metrics.
namics prediction errors, we update the robot multimodal • Joints perception error. Error between the inferred
stateandtheproprioceptivehigher-orderderivatives.Finally, (belief)andtheobservedjointangle.Themoreaccurate
we compute the action to minimize the inverse variance are the predictions, the lower will be the perception
weighted proprioceptive prediction errors. error.
• Jointsgoalerror.Errorbetweenthecurrentjointangles
Algorithm 1 MVAE-AIF and the desired ones (goal).
Require: x ={I ,q } • Jointsbelief-goalerror.Errorbetweentheinferredjoint
desired d d
while ¬goal do angle and the desired joints goal. It shows how the
perceived joints approach the final goal.
q,I← Sensors (joint values, camera)
• Imagereconstructionerror.Errorbetweenthepredicted
Computepredictionerrorsinzspace, Eq.16 visual input and the observed image. It describes the
x v =g v (z) Decoder visual goodness of the visual generative model.
x q =g q (z) Decoder joints • End-effector error. Error between the current end-
z˙ q =g q .backward(Σ− q 1(q−x q )) effectorlocationandthedesiredone(goal).Itprovidesa
z˙ v =g v .backward(Σ− v 1(I−x v )) single metric to evaluate the precision of the controller.
z˙ =g .backward(Σ−1(q −x ))
qd q q d q Summarizing, joints perception and image reconstruction
z˙ =g .backward(Σ−1(I −x ))
vd v v d v errors measure how well the state is estimated, while joints
Updatestate goal,jointsbelief-goalandend-effectorerrorsgiveameasure
z˙ =z +z +z +z of how well the control task is executed.
q v qd vd
µ˙(cid:48),µ˙(cid:48)(cid:48) ←proprioceptivestateEq.18, 19
B. Experimental setup and parameters
Activeinference ExperimentswereperformedontheGazebosimulatorand
a˙ =−k a Σ− q 1(q−x q )+Σ− q˙ 1(q˙ −µ(cid:48)) the 7DOF Franka Panda robot arm [22] using ROS [23] as
interface and Pytorch [24] for the MVAE. A camera model
Euler integration
[z,µ(cid:48),µ(cid:48)(cid:48)]+=δ (cid:2) z˙,µ˙(cid:48),µ˙(cid:48)(cid:48)(cid:3) was used to acquire visual grey scaled images with size
t
1x128x128 pixels. The camera was placed in front of the
end while
robot arm with a distance of 1.2 m. The average frequency
ofthe AIFalgorithm was 110 Hz.Proprioceptive andVisual
IV. RESULTS sensors have publishing frequencies of 1000 Hz and 10 Hz
respectively.Duetothisdiscrepancy,theyweresynchronized
A. Experiments and evaluation measures
updating them once per AIF loop iteration.
We performed three different experimental analyses and
The tuning parameters for the AIF controller are:
compared our approach (MAIF) against two state-of-the-art
controllers:aproprioceptivetorqueAIFcontroller(PAIF)[8] • Σ q ,Σ v : Variances representing the confidence about
sensory data were set as the variances of the training
and the built-in Panda controller (BPC) with the factory
dataset (joints and images).
parameters. Video footage of the experiments can be found
in the supplementary video: https://github.com/ • Σ µ =2.5,Σ µ(cid:48) =1.0: Variances representing the confi-
dence of internal belief about the states;
Cmeo97/MAIF
1) Qualitative analysis in sequential reaching (Sec. IV- • k µ =11.67,k q =0.6,k v =0.1,k a =900: The learning rates
for state update and control actions respectively were
C). We evaluated the algorithm accuracy and behavior
manually tuned in the ideal settings experiment.
usingseveralmetrics,aswellasstudiedtheeffectofin-
Table I details the specification of the encoders and de-
cludingthevisualmodality.Precisely,weanalyzedhow
codersusedintheMVAE.FortrainingtheMVAEwesetthe
the multimodal representation power affects accuracy.
number of training epochs to 100, and we used ADAM [25]
Besides, we show how our algorithm can work in an
astheoptimizationalgorithm.Allexperimentsaresimulated
imaginary regime by mentally simulating the behavior.
onacomputerwithCPU:AMDryzen93900X,GPU:Nvidia
2) Statistical evaluation in 1000 random goals (Sec. IV-
GeForce RTX 3080 msi.
D). We compared our proposed controller’s average
during the experiment, are showed in Fig. 4. Switching to a
new goal generated torques that minimize prediction errors.
VisualEnc. PropEnc. VisualDec. PropDec.
Input-I Input-q Input-z Input-z
Conv(1,128,3) FC(7,64) upConv(1,16,4) FC(256,128)
Conv(128,64,3) FC(64,512) upConv(16,32,4) FC(128,64)
Conv(64,32,3) FC(512,4096) upConv(32,16,4) FC(64,7)
upConv(32,32,3) Conv(16,1,2) upConv(16,1,4) Output-xp 2
Conv(32,16,2) Output-z Output-xv
upConv(16,16,4) 1
Conv(16,1,2)
Output-z 0
TABLE I: Network layers specifications. Type: Convolution -1
(Conv), Trasposed Convolution (upConv), Fully connected
-2
(FC). All layers use ReLu activation function, except for the
visual decoder output that uses tanh(ReLu). MaxPool and 0 2.5 5 7.51012.51517.520
time [s]
AvgPool are used to increase network generalization.
C. Qualitative analysis in a sequential reaching task
To analyse the controller behavior, we designed a sequen-
tial reaching task with five desired goals defined by the final
joint angles {q ,q ,q ,q ,q }: d1 d2 d3 d4 d5
• q d1 =[ 1.0, 0.5, 0.0, −2.0, 0.0, 2.5, 0.0] rad
• q d2 =[ 0.0, 0.2, 0.0, −1.0, 0.0, 1.2, 0.9] rad
• q d3 =[−1.0, 0.5, 0.0, −1.2, 0.0, 1.6, 0.0] rad
• q d3 =q d2
• q d5 =q d1
andthedesiredvisualinput{I ,I ,I ,I =I ,I =I },
d1 d2 d3 d4 d2 d5 d1
depictedinFig.2.Inallexperimentstherobotstartsinhome
position (q =0 rad), corresponding to the arm straight
home
up.
(a) Home (b) I (c) I (d) I
d1 d2 d3
Fig. 2: Robot home position and visual goals.
1) Sequential reaching: Fig. 3 describes the evolution
of the evaluation metrics. The joint errors (Fig. 3a) show
synchronous convergence without overshooting or steady-
stateerrors.Fig.3cand3dshowperceptionandjointsbelief
error respectively. The delay between the convergence of
joints beliefs and the real ones is due to the AIF frame-
work. The robot updates its internal belief by approximating
the conditional density, maximizing the likelihood of the
observed sensations and then it generates an action that
results in a new sensory state, which is consistent with
the current internal representation. Fig. 3b shows the visual
generative model image reconstruction errors through the
simulation. From one goal to the next one the error drops
down.However,somegoalscanbebetterreconstructedthan
others, resulting in different steady errors. The reason is
that many z solutions lead to similar images. Fig. 3e shows
somevisualpredictionsinthe20secondssimulation.Finally,
the generated actions (torques) using MAIF, for the 7 joints
]dar[
rrE
tnioJ
J_0 J_3 J_5
J_1 J_4 J_6
J_2
25
20
15
10
5
0
0 2.5 5 7.51012.51517.520
time [s]
(a) Joints goal error
]lexiP[
rrE_lexiP
(b) Image reconstruction error
0.75
0.5
0.25
0
-0.25
-0.5
-0.75
-1
0 2.5 5 7.51012.51517.520 time [s]
]dar[
rrE
tnioJ
J_0 J_3 J_5
J_1 J_4 J_6
J_2
2
1
0
-1
-2
0 2.5 5 7.51012.51517.520 time [s]
(c) Joints perception error
]dar[
rrE
tnioJ
J_0 J_3 J_5
J_1 J_4 J_6
J_2
(d) Joints belief-goal errors
(e) Sequence of the MVAE predicted visual input g (z).
v
Fig. 3: Qualitative analysis of the error measures in the
sequential reaching of five goals. All errors present peaks
when the new goal is set. (a) Each line represents the
error between the i-th joint angle and the desired one. (b)
Imagereconstructionerror.(c)Eachlinerepresentstheerror
between the i-th joint belief and the ground truth. (d) Each
line represents the error between the i-th joint belief and the
desired joint angle. (e) Sequence of the predicted images by
the generative model along the trajectory.
2) Visual modality study: We evaluated the effect of
adding the visual modality. To this end, we tested the
algorithm in the presence of visual noise and in the absence
of visual input (camera occlusion or broken) by clamping
the image to zeros. We compared our algorithm with the
proprioceptive-AIF(PAIF)baseline.Visualnoisewasimple-
mentedasadditivenoisesampledfromaNormaldistribution
r ∼N(0,Σ =0.25).
I I
Fig. 5 shows the end-effector location errors using four
different algorithm settings: MAIF, MAIF with visual noise,
MAIF with visual occlusion and PAIF. MAIF had the best
response while PAIF scored the lowest. In the case of visual
occlusion, the MAIF approached the PAIF response, and in
the case of noisy visual input, accuracy fell in the middle of
MAIF and MAIF with visual occlusion.
20
10
0
-10
-20
-30
-40
-50
0 2.5 5 7.5 1012.51517.520
time [s]
]mN[
seuqrot
a_0 a_3 a_5
a_1 a_4 a_6
a_2
Fig. 4: Torque commands for the 7-DOF robot arm using
our MAIF in sequential reaching of five goals.
0.4
0.3
0.2
0.1
0
0 2 4 6 8 10 12 14 16
time [s]
]m[
rotceffE
dnE
rrE
2
1
0
-1
-2
0 2.5 5 7.51012.51517.520
time [s]
PAIF MAIF-noise_v
MAIF-occlusion_v MAIF
Fig. 5: Visual influence in sequential reaching in ideal
conditions. Lines describe the end-effector location RMSE
to goal. Peaks are present when the new goal is set.
3) Mental simulation: Unlike most of the controllers
presentinliterature,agreatadvantageofusingourapproach
is the possibility to perform simulations on the robot’s brain
exploiting the generative nature of the MVAE. In other
words,givenx ,theentireexperimentcanbementally
desired
simulated. Since sensory data are not available, the state
update law becomes:
z˙ =z˙ +z˙ (22)
qd vd
As a result, decoding the new internal state, the updated
{I,q} can be computed and the new errors can be back-
propagated again, creating a loop that allows the system to
do imaginary simulations.
Fig.6aand6bshowrespectivelyimaginedjointserrorand
images reconstruction error through the entire simulation.
These results show that the errors converge faster to zero
]dar[
rrE
tnioJ
J_0 J_3 J_5
J_1 J_4 J_6 25
J_2
20
15
10
5
0
0 2.5 5 7.51012.51517.520
time [s]
(a) Joints errors
]lexiP[
rrE_lexiP
(b) Image reconstruction error
Fig. 6: Mental simulation of sequential reaching of five
goals. The goal is updated on time steps where peaks are
present.(a)Jointserrorsofanimaginedsimulation.Eachline
representstheerrorofthei-thjoint.(b)Imagereconstruction
errors of an imagined simulation.
than in the normal regime (Fig. 3d) as it does not need to
accommodate the real dynamics of the robot.
D. Statistical analysis
A statistical comparison between our MAIF, PAIF and
BPCwasperformedtoevaluatethecontrollers’performances
onreachingtasksundernormalandnoisyconditions.Tothis
end, we created a list of 1000 random desired goals that
we executed in each controller. Then, we compared their
average joint errors. In every execution, the robot started
1
0.75
0.5
0.25
0
0 1 2 3 4 5
time [s]
]m[
ESMR
BPC
PAIF 1
MAIF
0.75
0.5
0.25
0
0 1 2 3 4 5
time [s]
(a) No noise
]m[
ESMR
BPC
PAIF
MAIF
(b) Proprioceptive noise
Fig. 7: Statistical comparison of our MAIF (green), PAIF
(orange) and the BPC (blue) over 1000 executions with
different random goals in ideal conditions (a) and with pro-
prioceptive noise (b). Lines represent the Root Mean Square
Error(RMSE)oftheend-effectorandshadingsdescribetheir
standard deviation.
from the home position. Fig. 7 shows the root-mean-square
error (RMSE) between the end-effector position and the
desired pose over all executions for the three controllers. 0.4
In perfect conditions (Fig.7a), BPC was the fastest and
had the lowest RMSE on average. MAIF had high accuracy 0.3
and presented a small overshoot when reaching the goal.
PAIF was much slower and had lower accuracy. However, 0.2
when injecting Gaussian noise (r ∼ N(0,Σ = 0.5)) in
q q
thejointsensormeasurements(Fig.7b),ourproposedMAIF 0.1
scored the lowest RMSE and standard deviation. Hence,
MAIF presented higher robustness to proprioceptive noise 0
with more stable behavior than the BPC and the PAIF. BPC
0 2.5 5 7.5 1012.51517.520
had the highest standard deviation, showing poor robustness
time [s]
tosensorynoise,especiallyweapproachingthedesiredgoal.
By visual inspection of the robot behavior, we observed
strong oscillations in the BPC. The robustness of MAIF is
due to the algorithm structure. The free energy optimization
scheme and the decoder interpolation make that the sensory
noise has a low effect on the state estimation.
E. Adaptation
Toinvestigateourapproachadaptabilitytounmodeleddy-
namics and environment variations we tested the controllers
in three experiments. First, we changed the gravity of the
world as if the robot was in Jupiter. Second, we altered the
motors stiffness parameter of the Panda robot. Third, we
reevaluated the controller in the presence of sensory noise
focusing on the robot behavior. Again, we compared our
algorithm (MAIF) with the built-in Panda controller (BPC)
and the proprioceptive controller (PAIF). All controllers
parameters were the same as in the previous experiments.
Thus, no retuning was done.
1) Jupiter experiment: This experiment aims to show
the controller performance in case of gravity changes. The
reasoningbehindthisexperimentistoevaluateMAIFperfor-
manceunderunexpectedexternalconditions,suchasexternal
forces. To do that, we performed the simulation with Jupiter
gravity, which is g = 24.79m on the planet’s surface. Fig.
s2
8 illustrates joint angle absolute errors for each controller.
MAIFwastheleastaffectedbythechangeofgravity.Under
BPC the robot arm oscillated around the desired pose and
had steady-state errors. PAIF response was also affected by
the gravity change. MAIF registered a faster response than
PAIFduetomultisensoryintegration:imagesarenotaffected
by gravity.
2) Compliant experiment: The goal of this experiment
is the evaluation of MAIF adaptation under changes on
the robot’s physical parameters. We altered the robot arm’s
compliance by changing the motors stiffness values from
0.61Nm to 0.01Nm. Figure 9 shows the absolute joints
rad rad
errors for each controller. As in the previous experiment,
the BPC makes the robot arm oscillate around some goal
poses,whilebothMAIFandPAIFdonot.Withlowstiffness,
MAIFandPAIFslightlyovershotwhenthedesiredgoalwas
far away. However, since MAIF updates his state from both
proprioceptive and visual sensory data, it had much lower
overshooting and faster response than PAIF.
]m[
rotceffE
dnE
rrE
BPC PAIF MAIF
Fig.8:Jupiterexperiment.Thelinesdescribetheend-effector
error to the desired goal under Jupiter gravity in a reaching
taskwithfivesequentialgoals.MAIF(green),PAIF(orange)
and BPC (blue). All end-effector errors present peaks when
the new goal is set.
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0 2.5 5 7.5 1012.51517.520
time [s]
]m[
rotceffE
dnE
rrE
BPC PAIF MAIF
Fig. 9: Compliant experiment. Sequential reaching of five
goals when motors stiffness is changed from 0.61Nm to
rad
0.01Nm. All end-effector location absolute errors present
rad
peaks when the new goal is set.
3) Sensory noise experiment: We reevaluated the con-
troller behavior in the presence of proprioceptive noise but
in a sequential reaching task, focusing on the adaptation
capabilities of the three controllers. Figure 10 shows that
MAIF was the most adaptive, presenting the smoothest
behavior.PAIFalsoadaptedtothenoisebutpresentedhigher
oscillations than MAIF. By contrast, BPC had the worst
behavior, showing strong oscillations around the desired
goals.
V. CONCLUSION
We presented an improved adaptive torque controller for
robot manipulators based on active inference. Our approach
makes use of the alleged adaptability and robustness of AIF,
takingadvantageofpreviousworksandoverwhelmingsome
related limitations. We solved state estimation by combining
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0 2.5 5 7.5 1012.51517.520
time [s]
]m[
rotceffE
dnE
rrE
BPC PAIF MAIF [6] G. Oliver, P. Lanillos, and G. Cheng, “An empirical study of active
inferenceonahumanoidrobot,”IEEETransactionsonCognitiveand
DevelopmentalSystems,2021.
[7] C.Sancaktar,M.A.vanGerven,andP.Lanillos,“End-to-endpixel-
based deep active inference for body perception and action,” in
2020JointIEEE10thInternationalConferenceonDevelopmentand
LearningandEpigeneticRobotics(ICDL-EpiRob). IEEE,2020,pp.
1–8.
[8] C.Pezzato,R.Ferrari,andC.H.Corbato,“Anoveladaptivecontroller
forrobotmanipulatorsbasedonactiveinference,”IEEERoboticsand
AutomationLetters,vol.5,no.2,pp.2973–2980,2020.
[9] L.Pio-Lopez,A.Nizard,K.Friston,andG.Pezzulo,“Activeinference
and robot control: a case study,” Journal of The Royal Society
Interface,vol.13,no.122,p.20160616,2016.
[10] M. Jung, T. Matsumoto, and J. Tani, “Goal-directed behavior under
variationalpredictivecoding:Dynamicorganizationofvisualattention
andworkingmemory,”IROS,2019.
[11] M. Baioumy, P. Duckworth, B. Lacerda, and N. Hawes, “Active
inferenceforintegratedstate-estimation,control,andlearning,”arXiv
Fig. 10: Sensory noise experiment. Sequential reaching of preprintarXiv:2005.05894,2020.
five goals under proprioceptive noise. All end-effector abso- [12] A. A. Meera and M. Wisse, “Free energy principle based state and
inputobserverdesignforlinearsystemswithcolorednoise,”in2020
lute errors present peaks when the new goal is set.
AmericanControlConference(ACC). IEEE,2020,pp.5052–5058.
[13] P. Lanillos, J. Pages, and G. Cheng, “Robot self/other distinction:
active inference meets neural networks learning in a mirror,” in
Proceedingsofthe24thEuropeanConferenceonArtificialIntelligence
representation learning (with a multimodal VAE) with free
(ECAI),2020,pp.2410–2416.
energy optimization, improving the representational power [14] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
andadaptability.Asaresult,wederivedaschemaforonline behavior:afree-energyformulation,”Biologicalcybernetics,vol.102,
no.3,pp.227–260,2010.
proprio-visual torque control, which does not require any
[15] Y. Shi, N. Siddharth, B. Paige, and P. H. Torr, “Variational mixture-
dynamic or kinematic model of the robot, is less sensitive of-experts autoencoders for multi-modal deep generative models,”
to unmodeled dynamics, and can handle high-dimensional NeurIPS,2019.
[16] T. Rood, M. van Gerven, and P. Lanillos, “A deep active inference
inputs. Although in this work we used images it can be
model of the rubber-hand illusion,” in Active Inference, T. Verbelen,
generalized to any sensor modality. P.Lanillos,C.L.Buckley,andC.DeBoom,Eds. Cham:Springer
Results showed that our proposed algorithm outperforms InternationalPublishing,2020,pp.84–91.
[17] T. Lesort, N. D´ıaz-Rodr´ıguez, J.-F. Goudou, and D. Filliat, “State
in terms of adaptation the built-in Panda controller and a
representation learning for control: An overview,” Neural Networks,
state-of-the-art torque AIF baseline. Moreover, it was more vol.108,pp.379–392,2018.
accurateinthepresenceofsensorynoise.OurAIFcontroller [18] K.Friston,J.Mattout,N.Trujillo-Barreto,J.Ashburner,andW.Penny,
“Variationalfreeenergyandthelaplaceapproximation,”NeuroImage,
was highly adaptive and robust to different contexts, such
vol.34,no.1,pp.220–234,Jan.2007.
as sensory noise, visual occlusions, changes in the world [19] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth, “The free
dynamics (i.e., gravity) and changes in the robot properties energy principle for action and perception: A mathematical review,”
JournalofMathematicalPsychology,vol.81,pp.55–79,2017.
(i.e.motorstiffness).Besides,ourapproachnativelyallowed
[20] O.vanderHimstandP.Lanillos,“Deepactiveinferenceforpartially
mental simulation of the planned trajectory. observable mdps,” in International Workshop on Active Inference.
Future work will finalize the physical robot experiments Springer,2020,pp.61–71.
[21] P.LanillosandG.Cheng,“Activeinferencewithfunctionlearningfor
that were not possible due to university restrictions and
robot body perception,” in Proc. Int. Workshop Continual Unsuper-
will investigate strongly coupled cross-modal learning and visedSensorimotorLearn.,2018,pp.1–5.
hierarchical schemes for sequential decision making. [22] “Franka emika panda robot arm.” [Online]. Available: https:
//www.franka.de/
[23] A.Koubaa,RobotOperatingSystem(ROS):TheCompleteReference
ACKNOWLEDGMENT
(Volume2),1sted. SpringerPublishingCompany,Incorporated,2017.
We would like to thank Professor Martijn Wisse for his [24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,A.Desmaison,A.Kopf,
fruitful comments on the elaboration of this work.
E.Yang,Z.DeVito,M.Raison,A.Tejani,S.Chilamkurthy,B.Steiner,
L.Fang,J.Bai,andS.Chintala,“Pytorch:Animperativestyle,high-
REFERENCES performancedeeplearninglibrary,”inAdvancesinNeuralInformation
Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer,
[1] K.Friston,“Thefree-energyprinciple:aunifiedbraintheory?”Nature
F. d’ Alche´-Buc, E. Fox, and R. Garnett, Eds. Curran Associates,
reviewsneuroscience,vol.11,no.2,pp.127–138,2010.
Inc.,2019,pp.8024–8035.
[2] F. K.J, T.-B. N., and Daunizeau, “Dem: a variational treatment of
[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
dynamicsystems,”NeuroImage,41,pp.849-885,2008.
tion,”2017.
[3] S.Sa¨rkka¨,Bayesianfilteringandsmoothing. CambridgeUniversity
Press,2013,no.3.
[4] B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley, “On the
relationship between active inference and control as inference,” in
InternationalWorkshoponActiveInference. Springer,2020,pp.3–
11.
[5] P. Lanillos and G. Cheng, “Adaptive robot body learning and esti-
mation through predictive coding,” in 2018 IEEE/RSJ International
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2018,
pp.4083–4090.