A framework for the use of generative modelling in non-equilibrium statistical
mechanics
Karl J Friston, 1 Maxwell J D Ramstead, 1 and Dalton A R Sakthivadivel 2,âˆ—
1Queen Square Institute of Neurology, University College London, London, UK
2Department of Mathematics, CUNY Graduate Center, New York, NY 10016, USA
(Dated: 15th October 2025)
We discuss an approach to mathematically modelling systems made of objects that are
coupled together, using generative models of the dependence relationships between states
(or trajectories) of the things comprising such systems. This broad class includes open or
non-equilibrium systems and is especially relevant to self-organising systems. The ensuing
variational free energy principle (FEP) has certain advantages over using random dynamical
systems explicitly, notably, by being more tractable and offering a parsimonious explanation
of why the joint system evolves in the way that it does, based on the properties of the coupling
between system components. The FEP is a method whose use allows us to build a model of
the dynamics of an object as if it were a process of variational inference, because variational
free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that
using generative models to represent and track relations amongst subsystems leads us to
a particular statistical theory of interacting systems. Conversely, this theory enables us to
construct nested models that respect the known relations amongst subsystems. We point
out that the fact that a physical object conforms to the FEP does not necessarily imply that
this object performs inference in the literal sense; rather, it is a useful explanatory fiction
which replaces the â€˜explicitâ€™ dynamics of the object with an â€˜implicitâ€™ flow on free energy
gradientsâ€”a fiction that may or may not be entertained by the object itself.
CONTENTS
1. Introduction 3
2. Summary remarks on the free energy principle 5
2.1. Fundamentals 5
2.2. Analysis and interpretation 8
3. Empirical validation 10
âˆ— dsakthivadivel@gc.cuny.edu
arXiv:2406.11630v4  [cond-mat.stat-mech]  14 Oct 2025
2
3.1. Cellular morphogenesis 10
3.2. Periodically-firing cells 13
4. Some preliminary philosophical considerations 22
5. A map of that part of the territory that behaves as if it were a map 24
6. Conclusion 26
References 27
Acknowledgements
This paper more fully develops the theory outlined in a previous manuscript [1], and we are
grateful to those listed there for their contributions to preliminary discussions on the topic. The
account here is largely based on a series of talks given by the third named author at the Department
of Philosophy of the University of Vienna, the Laufer Center for Physical and Quantitative Biology,
the 2024 Conference on Complex Systems, and the informal discussions that followed. We thank the
organisers and audiences of both. DARS acknowledges support from the Einstein Chair programme
at the Graduate Centre of the City University of New York. KJF and DARS acknowledge support
from the VERSES Research Lab.
Funding declaration
There is no specific funding to be acknowledged.
Author contributions
The list of authors has been ordered alphabetically by surname. The CRediT contributions are
as follows: KJF (formal analysis, software, writingâ€”review and editing), MJDR (conceptualisation,
writingâ€”review and editing), DARS (conceptualisation, formal analysis, software, visualisation,
writingâ€”original draft).
3
1. INTRODUCTION
In statistics, generative models are joint probability distributions that model the relationships
between different variablesâ€”most often, observations and their causes, or data and a set of labels.
In the study of open and interacting systems, the use of statistical modelsâ€”capable of describing
such relationshipsâ€”seems to be particularly apt. One approach to making such models, using
variational (approximate Bayesian) inference [2â€“4], describes how a physical object reflects the
properties of its environment andvice versa. In this paper, we argue that generative models
provide an especially useful foundation in scientific modelling generally, and especially in the study
of systems exhibiting adaptation, morphogenesis, and other non-equilibrium phenomena. More
specifically, we argue that generative modellingâ€”premised on the use of variational inference to
model the causes of the signals that a coupled system is receiving [5]â€”is particularly apt for the
practice of modelling the activity of modelling itself; for instance, in modelling sophisticated self-
organising systems, where we write down a representation of the model that we assume is in play
in prediction and action. We use the ensuing variational free energy principle to illustrate how
implementing models which (i) formalise how one object keeps track of another, and (ii) codify
these relationships, can increase the fidelity of our mathematical modelling.
Our proposed free energy principle (FEP) has been explored in some depth in prior literature,
from the viewpoint of modelling control systems using the property that they generally look as if
they track or infer the statistics of perturbations to their setpoints. In this paper we will make a
more general case for its utility in statistical mechanics at large. After reviewing prior work on the
FEP, we identify a particular issue that the FEP is able to uniquely address: the FEP keeps track
of how a model depends on aspects of the thing that is modelled in a particularly insightful way,
offering advantages in terms of tractability and explanation, by using gradients of variational free
energy to describe the dynamics of interacting systems.
One of the points this paper will emphasise is that using this framework doesnotnecessarily
presuppose that a system actually performs inference on its environment. It is a truism that phys-
ical systems need not explicitly calculate their trajectories of motion, to be modelled as pursuing
such trajectories. We need not assume that the particle itself is literally performing inference when
we build models under our framework. What is at stake is an â€˜as ifâ€™ description: we simply as-
sume that there exists a quantity that varies systematically with the dynamics of the system; it
so happens thatsurprisal(or, equivalently, variational free energy) is a good choice of quantity
for physical reasons, and that the minimisation of this quantity is mathematically equivalent to
4
inference, in the sense of being an estimator. The fact that it is our model of the system which
does inference and not necessarily the system itself is both a boon and a bane. It enables us to
model the system using the more tractable gradient of variational free energy and the mathematics
of statistical inference, rather than studying the possibly highly non-linear coupling between the
dynamics of the two systems. However, it is not necessarily a faithful representation of the system
in the literalist sense. (See also [6].)
To that end, after describing the advantages of this FEP we will then argue that there is no
conflation of the modelled system and our scientific model of itâ€”in fact, we describe how they
are carefully distinguished. This argument gives way to a more general theme of the FEP, which
is that capturing interactions and dependence relations between things using generative models is
insightful. We discuss how using the FEP gives us a flexible model of the properties of the object
that look like they perform inference, but crucially, without inherently reifying the inferential
aspects of the model, attributing them to the real-world object. That is to say, a model of an
object can always bewrittenas if it were modelling or inferring the structure of its environment.
To make this argument, we appeal to two distinctions: one between the generative model (which
is our scientific model) and the physical object being modelled (the real â€˜thingâ€™ in the world), and
the other, between the generative model and the variational density.
The structure of the paper is as follows. We first present a conceptual introduction to the
FEPâ€”and the way it leverages the relationships encoded by a generative model to build ascientific
modelof the dynamics of physical objects using free energy gradients. This is possible by modelling
the property of physical systems that theylook as if they engage in modellingto an external
observer. To sketch the argument: the FEP provides us with tools from dynamical systems and
information theory, allowing us to model any coupled random dynamical systems asstatistical
estimators,i.e., as engaging in the statistical estimation of some quantity pertaining to the set of
objects with which it interacts. Under the FEP, one object that is coupled to any other can thus
be read as a representation of the coupled or estimated object, albeit of a deflationary, qualified
sort (discussed in [7]; see also [3, 8]). More formally, the FEP applies to generative models that
contain a partition of the states or paths of a joint system (a so called â€œparticularâ€ partition,i.e.,
into objects or particles). This partition induces a statistical boundary called a Markov blanket
[9], which captures conditional independence relations amongst subsets of the system [2, 10]. The
FEP says that, when such Markov blanketed subsets of the system exist, those subsystems track
each othersâ€™ statistical properties (dynamically) across the blanket [11]. Essentially, the FEP is a
modelling framework which captures the following fact: models of coupled particles or factorised
5
systems can be written as parameter estimators, and estimating the parameters of a distribution
is a form of Bayesian inference.
We then argue that there is no ambiguity between model and modelled real-world system in
this formalism. We can think of the FEP, metaphorically, as a map of the part of the territory that
behaves as if it were a mapâ€”or looks as such to an external observer. Accounts such as [6] have
encouraged practitioners of the FEP to be explicit about this point. Viewing the theory this way,
not only is there no conflation between our FEP-theoretic model of the system and the system
itself, but in fact we arrive at a novel modelling method for coupled random dynamical systems. In
particular, we do not claim that the objects in the system themselves must literally be performing
inference. Rather, we describe the dynamics as doing inference, in the sense of being a statistical
estimator whose states parameterise a variational density.
In summary we propose that the FEP provides a map of any possible map whatsoeverâ€”of, or
held byâ€”a physical system. (This includes the use of the FEP itself by a modeller.) We suggest
that this leads to constraints on all possible maps, or models of physical things, arising from
what it means to be a map or model of a physical process at all. In the fashion of Jaynes, who
proposed that doing statistical mechanics is doing inference about the probabilities of microstates
given knowledge of macrostates [12, 13], the FEP is a natural expression of statistical physics as a
process of making models. More broadly we claim that this echoes the sentiments of Wittgenstein,
providing a constraint on modelling in terms of logically necessary preconditions for sense-making
and the parsing of sensory streams [7].
For those already familiar with the FEP, this paper can be thought of as going back to ba-
sics, emphasising its foundation in inverting generative models (written as SDEs in generalised
coordinates), using a path integral scheme for minimising variational free energy [14].
2. SUMMARY REMARKS ON THE FREE ENERGY PRINCIPLE
2.1. Fundamentals
Here we will describe the proposed free energy principle (also directing the reader to prior
literature such as [3, 4, 11] for a more extended look). The FEP aims to describe the physical
properties of a system in terms of dynamics on a manifold of probability distributions (i.e., as
variational inference) [15], resting on the useful property that physical objects generally look as if
they track or infer the statistics of things to which they are coupled [3, 11, 16]. Formulated most
6
generally, the FEP says that the average path taken by an object that is sparsely coupled to its
environment is characterised by a minimisation of a variational free energy functional.
We consider a particle (also referred to as a â€˜systemâ€™) with statesÂµcoupled to another system
with statesÎ·(henceforth, the â€˜environmentâ€™) through a set of variablesBwith valuesb, and a
steady state densityp(Î·, b, Âµ) such thatÂµis conditionally independent ofÎ·givenB. After Pearl
(see [9]) we refer toBas theMarkov blanketof the system. Typically,Bwill itself consist of two
different kinds of statesâ€”sensory states, providing the system signals about its environment, and
active states, providing interventions of the system on the environment. The coupling at play puts
us in a non-equilibrium situation, where a system can be driven by fluxes of energy or matter from
other systems in its environment [17].
The FEP begins by saying that if there exists a boundary in the form of a Markov blanket, then
there exists a functionÏƒrelating (the dynamics of) internal states to the environment across the
boundary, and in particular, such thatÏƒ(Ë†Âµ b) = Ë†Î·b, where Ë†Â·b denotes the conditional expectation
given the blanket stateb[16, Lemma 4.3]. Following that, the aim of the FEP is to understand
what the generative modelp(Î·, b, Âµ) says about the dynamics of the subsystem with statesÂµ,
beyond just its marginal distribution. In particular: if we assume that Ë†Î· b parametrisesp(Î·|b),
then we have madeÂµinto anestimatorof the statistics of the environment.
The FEP is simplest when we have a non-equilibrium steady state density, in which case the
following claim holds: if the system remains on some attractor of states, it looks like it minimises
variational free energy. This is true in the following sense. It is possible to express the dynamics
of a random variable with a non-equilibrium steady state density
dXt =f(X t) dt+D(X t) dWt
in terms of a gradient flow on the surprisal weighted by a particular matrix field [18â€“21], having
the form
dXt =âˆ’(Q(X t)âˆ’Î“(X t))âˆ‡x logp âˆ—(Xt) dt+D(X t) dWt (2.1.1)
whereQ(x) is skew-symmetric everywhere, Î“(x) is positive (semi-)definite everywhere and satisfies
2Î“(x) =D(x)D âŠ¤(x), andp âˆ—(x) is the non-equilibrium steady state density. This decomposition
holds whenever there exists aQ(x) such that
âˆ‡ Ã—[Q(x) + Î“(x)]âˆ’1f(x) = 0
and Î“ is divergence freeâ€”otherwise, in the ItÂ¯ o picture, a correction is necessary due to the diver-
gence of Î“. For an equilibrium steady state,Q(x) is identically zero andfis a gradient on the
7
nose, with steady state proportional to exp{âˆ’âˆ‡f(x)}.
Moreover, the log-probability is able to describe the probability of all fluctuations of the system
in terms of their distance from the most likely state, as well as physically-derived quantities [17,
22, 23], making this a meaningful equation. (See [24] where this is applied to reproduce certain
aspects of stochastic thermodynamics.)
Let us make the Laplace assumption such that the most likely state is the expectation. Then
our generic SDE (2.1.1) tends towards the fixed point solution
Ë†Âµb = arg max logp(Âµ, b).
To make use of this first observation, we nowpostulatea conditional density over external states,
q(Î·; Ë†Î·b), where we have parameterised the variational density by the conditional mode Ë†Î· b of the
environment. Under the Laplace assumption, in a neighbourhood of its peak the posterior is
approximately Gaussian, and hence uniquely determined by its mean and variance. This guarantees
that finding the right value of Ë†Î· b is sufficient forqto equal the posterior almost surely. More
generally, under the maximum entropy assumption of a Gibbs density, any constraint function
suffices. Once again, this implies that the systemâ€™s average behaviour encodes an inference about
its environment, in the sense of parameter estimation.
Inference by parameter estimation implies the KL divergence between the posterior and the
true density vanishes as the parameter varies. We will write
F(Âµ, b):=
Z
q(Î·;Ïƒ(Âµ)) logq(Î·;Ïƒ(Âµ)) dÎ·âˆ’
Z
q(Î·;Ïƒ(Âµ)) logp(Î·|b) dÎ·âˆ’logp(Âµ, b).
Now observe that since the divergence term is non-negative,F(Âµ, b) is an upper bound on the
surprisalâˆ’logp(Âµ, b),
F(Âµ, b) =DKL(q(Î·;Ïƒ(Âµ))âˆ¥p(Î·|b))âˆ’logp(Âµ, b)â©¾âˆ’logp(Âµ, b) (2.1.2)
with equality precisely when the KL term vanishes. We conclude that the minimisation of surprisal
implies the vanishing of the divergence term. Let us apply the Laplace approximation [25] such that
the expected state is approximately the mode of the density. It follows that the most probable state
is the least surprising one. Hence, for physical reasons, we expect the surprisal to be minimised.
From that reasoning, (2.1.1) has the equivalent form
dÂµt =âˆ’(Qâˆ’Î“)âˆ‡ ÂµF(Âµ, b) + dWt .(2.1.3)
Evidently, when the inference gap is minimised, we have
âˆ’(Qâˆ’Î“)âˆ‡ Âµ logp(Âµ, b) + dWt =âˆ’(Qâˆ’Î“)âˆ‡ ÂµF(Âµ, b) + dWt
8
on the nose. Our reasoning so far implies this suffices as an approximate description of all Markov
blanketed systems.
What we have done is used the fact that the dynamics of a (steady state) system minimise
surprisal on average, and argued that whenever equipped with a Markov blanket they also minimise
variational free energy; and hence, the dynamics of a system with a particular partition can always
be written as a gradient flow on variational free energy. This gradient flow is what we identify
as actually being the process (i.e., the performance) of inference. Explicit examples of this can
be found in [11, 24, 26]. The particular partition becomes crucial here, since this is what we
leverage in our model to incorporate some unknown quantity (external states beyond the blanket)
that the system can do inferences about at all. Variational free energy, being a tractable (i.e.,
easily computed) upper bound on surprisal, already makes this a useful tool to haveâ€”however we
will go further and claim supposing the existence of a minimised variational free energy, with its
interpretation as inferring a target density by approximating it with a variational density, has a
use in interpreting models in statistical physics in a new way.
We will remark that much of this story can be told for the trajectories of a system and distribu-
tions over path spaces, but with many modifications [4, 11], none of which are necessary to make
the point sought in this paper.
2.2. Analysis and interpretation
We have, in the previous subsection, introduced a vanishing KL divergence term to our SDE-
based model of a physical system, which we have then stipulated to vanish. Though seemingly
vacuous, this move opens a world of interesting interpretations for models built according to this
reasoning, by connecting the physically-given dynamics of the mode of the system to inference
performed by the mode of an estimator. For example: by existing a particular way inside the joint
state space, the particle can be seen asencoding(viaits mode) an inference about a generative
model over the entire joint state space, resulting from carving out some â€˜nicheâ€™ within it. That is to
say, the environment must likely be some certain way for the system to exist the way it does. On
the other hand, if free energy (or surprisal) remains irreducibly high, it is likely that the particle
occupies surprising (i.e., implausible or uncharacteristic) states; this would render it non-existent
over some timescale, in the sense that it would be occupying states that were not characteristic of
the thing in question (e.g., a fish out of water). Correspondingly, its inference about the generative
model would be incorrect, concentrating mass in the wrong areas of the joint state space. These
9
statements tie together the reasoning that things which exist in an environment reflect the statistics
of their environment in a particular way, based on the way they are coupled to their environment;
or, they do not exist that way.
Here, variational free energy is a Lyapunov functionâ€”a function that decreases monotonically
along the trajectories of a (deterministic) dynamical system. The utility of Lyapunov functions
is that they guarantee stability, allowing us to understand the dynamical system as flowing on
the gradient of the Lyapunov function. Variational free energy arises as a Lyapunov function for
the straightforward reason that the most probable state is the least surprising one, and that the
log-probability is able to describe the probability of all fluctuations of the system in terms of their
distance from that most likely state [17, 22, 23]â€”combined with the inequality in (2.1.2). Since
the dynamics towards the mode induce a gradient flow on a divergence between the estimated
parametric distribution and the true distribution, the FEP moreover proposes that these dynamics
(in virtue of the coupling) can be modelled as a variational inference process, following a gradient
flow on the variational free energy. Said another way, the crucial idea is that the dynamics of
a system do inference when written as minimising a free energy functional, which is itself the
consequence of reading surprisal as a Lyapunov function for those dynamics, followinge.g.Freidlinâ€“
Wentzell, and noting that the KL divergence isstipulatedto be minimised by a near-tautology that
the system will be at its average state on average. In that case, we can map these parameters to
Ïƒ(Ë†Âµb), meaning that for as long as the particle is at state Ë†Âµ b, theparticlemodels its environment
by estimating the parameters of its distribution.
Ultimately this simply means that, in virtue of being coupled (viablanket states) to its envir-
onment, the particle reflects data about its environmentâ€”or, what it believes that its environment
is like. Under the assumption that a particle which exists in an environment models its envir-
onment, free energy is implicitly a measurement of coherence. If the particle stops modelling its
environment in a particular way (i.e., for a particularÏƒ), it must have ceased to exist with the
mean thatÏƒwas constructed for; conversely, if a particle ceases to exist, it will stop modelling its
environment. The surprisal being considered is, much like probability, surprisal given a model, in
that it quantifies how surprising it would be for an observerâ€”equipped with a modelâ€”to find the
system (such as itself) in some non-system-like state or configuration. Minimising surprisal implies
that the variational free energy is minimised, (i.e., the object-as-model is a good estimator of its
environment [11]), and likewise, minimising the variational free energy places an upper bound on
the surprisal.
Similarly, note thedirectionof the implication in our claims. In the context of modelling
10
persistent systems, the FEP is a sort of inversion of the typical reasoning one sees: instead of
saying an object persists (as a thing) because it is stable, we say an object is stable (as what it
is) because it persists (as that thing) [16]. Westipulatethat the KL divergence vanishesâ€”because
it is a tautology thatpshould equalqalmost surelyâ€”and because physics only â€˜worksâ€™ in a given
regime if this is the case (see [11,Â§IIB] where this argument is spelt out). This happens in the
background, so to speak. Then we can use the FEP to describe the dynamics as a gradient flow on
surprisalâ€”which can be done by a simple mathematical fact and independently of the FEPâ€”as a
process whichhas performedinferencein virtue of existing. Adding this â€˜inflationaryâ€™ KL term, the
non-equilibrium steady state density of the system can be understood as a model of the environment
under this stipulation. All this is made possible by the existence of a particular partition. Hence,
we arrive at a deflationary account of intelligence through an inflationary mathematical technique.
Howeverâ€”note that when the Markov blanket consists of sensory states and active states, this
inference can immediately be written as part of a control problem [27â€“33], lending it an even more
sophisticated interpretation.
From the viewpoint of model building, since internal states are by construction unobservable,
we can in principle tell any story that we like about themâ€”provided that this story is supported
well enough by observable evidence. One such story is that those internal states look as if they
parameterise a variational density over external states. Taking this explanation seriously, and
leveraging it as an approach to modelling physical systems,isapplying the FEP.
3. EMPIRICAL VALIDATION
In this section we bolster the claim that modelling physical systems with variational free en-
ergy is advantageous by reviewing some simulations of self-organisation and self-assembly. Two
toy models will be studiedâ€”cellular morphogenesis, and an ensemble of excitable cells. In these
simulations we will see that introducing the generative model allows us to model the system self-
organising as if it â€˜knew its placeâ€™ within a larger systemâ€”inferring the unknowable external states
by the minimisation of variational free energy.
3.1. Cellular morphogenesis
Starting from an ensemble of undifferentiated cells at the same location, we wanted to simulate
their migration and differentiation using the minimisation of variational free energy. The simulation
11
here exploits the fact one can express a Markov blanketed systemâ€™s marginal Lyapunov function in
terms of a variational free energy function, such that its attracting set can be prescribed in terms
of the generative model that defines the free energy. In essence, the systems dynamics rest upon
each cell inferring its unique identity (in relation to all others) and behaving in accord with those
inferences.1
A group of eight cells was simulated with states consisting of the physical locations on a two-
dimensional grid (Ïˆ x) of each cell and the cell type of each cell (measured by its cell signalling
profile, a four-dimensional vector of signals expressedÏˆ c). For celli, (Ïˆ xi, Ïˆci) is an internal state
Âµ, and the remaining tuples (Ïˆ xj, Ïˆcj) constitute external statesÎ·.
Each cell was equipped with active states (a x, ac) corresponding to modulations of its position
and chemotactic signalling profile, respectively. We assume the active states fluctuate quickly
enough that from the viewpoint of the environment their dynamics are negligible,i.e.Ïˆâ‰ˆa,
constituting an adiabatic approximation
Ï„âˆ‚tÏˆ=aâˆ’Ïˆ
with large time constantÏ„. In this way the cellâ€™s active states become the external states offered
to other cells. (Note that this assumption is purely for convenience, simplifying the analytic
expressions for certain quantities; it is by no means necessary in the following discussion, and the
example to follow will not use it. What is at stake in terms of the realism of the simulation is: the
adiabatic assumption excludes the possibility of modelling cell migration or signal expression as a
function of action.)
We will additionally assume an exponential decay in space of extracellular signal concentration
as a function of the cellâ€™s own signalling. This means the cell can sense
Î»i(Ïˆx, Ïˆc) =C
X
j
Ïˆcj e|Ïˆxiâˆ’Ïˆxj|, Câˆˆ[0,1].
Each cell also has sensory states detecting the cellâ€™s current location and the concentration of
chemical signals produced by the cell. We will assume these are simply noisy estimates of (Ïˆxi, Ïˆci)
with noise distributed like a Gaussian of precision Î  (1) =e âˆ’16, for the full sensory state
si = [Ïˆxi +Ï‰, Ïˆci +Ï‰, Î»i(Ïˆx, Ïˆc) +Ï‰].
In summary, the active states are (i) where a cell chooses to be and (ii) what a cell chooses to release.
Those actions are the external states being offered to other cells under the adiabatic assumption,
and through the kernel above they superpose into a shared field which every cell samples.
1 These simulations were based on [34] and use the DEM toolbox in the MATLAB software SPM12 (https://www.
fil.ion.ucl.ac.uk/spm/). They can be reproduced using the morphogenesis feature and the data provided there.
12
A point in the state space (Ïˆ âˆ—
x, Ïˆâˆ—
c ) was designated as a target morphology, giving prescribed
locations and cell types of all cells. This preferred morphology was encoded in a prior and specified
desired positions and signal expressions for each clone or cell in terms of the cellâ€™s expectations
regarding sensory signals and spatial configurations. This point consisted of the physical form of a
head, body, and tail, and with different cell types in each component of the organism. The target
sensation underÎ»is plotted in Figure 1a. In real situations one can imagine a genetic encoding
of the target morphology, with epigenetic dynamics being active inference. A possible example of
this is given in Figure 1b where the target is encoded in binary codons for location and expression.
We then equipped each cell with a generative model, specifying it as follows. The mean was given
in terms of how environmental states are mapped to sensory states,
g(Ïˆ)i = exp(Ïˆi)P
j exp(Ïˆj)
ï£®
ï£¯ï£¯ï£¯ï£°
Ïˆâˆ—
x
Ïˆâˆ—
c
Î»(Ïˆâˆ—
x, Ïˆâˆ—
c )
ï£¹
ï£ºï£ºï£ºï£»
with the softmax function returning the expected identity of each cell. In Figure 1c we show the
softmax function, interpreted as the posterior beliefs that each cell (column) occupies a particular
place in the ensemble (rows). A precision Î (2) =e âˆ’2 was also set over external states. Aprediction
errorfor how the generative model matched with sensations was given in the form
Îµi = [si âˆ’g(Ïˆ) i]
The model was operationalised by following the procedure in [14]. A Taylor expansion of tra-
jectories was taken and the statistics of each order of approximation (Ïˆ, Ïˆâ€², Ïˆâ€²â€², . . .) were modelled
at each point in time. A derivative operatorDwas defined for ordinary differential equations on
this space. The entire trajectory was given a coordinate in function space
ËœÏˆ= [Ïˆ, Ïˆâ€², Ïˆâ€²â€², . . .].
Now by the decomposition (2.1.1), we have the equations of motion
dËœÂµt =âˆ’(Q ÂµÂµ âˆ’Î“ ÂµÂµ)âˆ‡ËœÂµlogp âˆ—(Ëœs,Ëœa,ËœÂµ) dt+ dËœWÂµ
t ,
dËœat =âˆ’(Q aa âˆ’Î“ aa)âˆ‡Ëœalogp âˆ—(Ëœs,Ëœa,ËœÂµ) dt+ dËœWa
t
(3.1.1)
for each cell. 2 This is where the interpretation as inference which we supply as modellers comes
into play. We cannot compute the solution to (3.1.1) since the surprisal term yields an intractable
2 This follows from the marginal flow lemma of [35].
13
integral. However, under the generative model we have equipped our system with, we can instead
treat the dynamics of the cell under (3.1.1) as maximising log-evidence for the sensations of the
cell, and hence as a variational problemâ€”as posed throughoutÂ§2. We will introduce the variational
distribution
q( ËœÏˆ|ËœÂµ) =N
 
ËœÂµ;âˆ’âˆ‡ËœÂµËœÂµlogp(ËœÂµ,Ëœs,Ëœa,ËœÂµ)

,
a Gaussian with expectation being the coordinate of the internal trajectory in function space ËœÂµ
and variance being the curvature of the surprisalâ€”computed at ËœÏˆ= ËœÂµâ€”and free energy
F(Ëœa,Ëœs,ËœÏˆ) =âˆ’logp(Ëœs,Ëœa,ËœÂµ) +DKL
 
q( ËœÏˆ|ËœÂµ)âˆ¥p( ËœÏˆ|Ëœs,Ëœa,ËœÂµ)

.
Treating (3.1.1) as a Bayesian filter of trajectories, from the derivation outlined in [14, equation
3.6] the final drift terms for the equations of motion for thei-th cell given the generative model
were obtained as (again denoting the internal stateÏˆ i asÂµ)
fËœÂµ(Ëœai,Ëœsi,ËœÂµ) = (QÂµÂµ âˆ’Î“ ÂµÂµ)âˆ‡ËœÂµF(Ëœai,Ëœsi,ËœÂµ) =DËœÂµâˆ’ âˆ‡ËœÂµËœÎµi Â·Î  (1) ËœÎµi âˆ’Î  (2) ËœÂµ
fËœai (Ëœai,Ëœsi,ËœÂµ) = (Qaiai âˆ’Î“ aiai )âˆ‡Ëœai F(Ëœai,Ëœsi,ËœÂµ) =DËœai âˆ’ âˆ‡Ëœai Ëœsi Â·Î  (1) ËœÎµi
(3.1.2)
with the following explicit equations for (positional and signalling, respectively) active states
âˆ‚tËœax =DËœax âˆ’ âˆ‡xËœsx Â·Î  (1)
x ËœÎµx +âˆ‡ xËœsÎ» Â·Î  (1)
Î» ËœÎµÎ»
âˆ‚tËœac =DËœac âˆ’Î  (1)
c ËœÎµc + Î (1)
Î» ËœÎµÎ».
As before, if the inference gap is minimised, for instance, in the neighbourhood of the mode, then
(3.1.2) is equal to (3.1.1) on the nose.
We then simulated the dynamics of each cell. In Figure 2 we have shown the expected positions
and releases of each cell over time, extracted from the solution to (3.1.2), and the decrease in free
energy as the cells evolve towards the target state.
In summary, the cell carries a generative model that predicts what signals it should sense if it
were the â€˜correctâ€™ type of cell at the â€˜correctâ€™ place in the target morphology. Those predictions
are built from the target locations and target signal combinations, and the model maps internal
beliefs about identity through the generative model to predicted sensory consequences. The final
outcome of this process is shown in Figure 3.
3.2. Periodically-firing cells
A similar but more complicated example is a ring of excitable, gap-junction-coupled cells whose
generative model encodes a periodic target waveform. Each cellâ€™s internal states infer the current
14
(a)
(b)
cell
2 4 6 8
cell
1
2
3
4
5
6
7
8
(c)
Figure 1: Plots of the target extracellular gradients (1a), encoding of the target signal in the cells
(1b) and softmax expectations of the identities of each cell (1c). In Figure 1a colours denote
different signal expressions and black dots denote no data, with the final locations of the cells
starred.
phase of that cycle from its membrane sensors, whilst active states like ion-channel gating act to
make sensed signals match the predicted periodic trajectory. This model is more expressive because
the prior over hidden states is oscillatory, such that the free energy minimum is a limit cycle: the
population converges to a stable circulating orbit in state space rather than to a fixed point.
We will define externalÎ· t as a stimulus in the world,s t as a sensory state,Âµ t as the internal
state estimating the phase of the cell, anda t as the state of some actuator or motor acting on the
environment to make observations conform to predictions. For simplicty we will assumes t tracks
15
time
0 5 10 15 20 25 30
location
-4
-3
-2
-1
0
1
2
3
4
(a)Migration of cells in time.As the cells
differentiate they are marked with red, yellow, green,
or blue, according to the signal they follow, with the
gradient denoting the level of how strongly a signal is
expressed.
time
5 10 15 20 25 30
free energy
-340
-320
-300
-280
-260
-240
(b)Decrease of free energy in time. A decrease
of free energy in time is noted as the cells migrate to
the final states corresponding to the sensed target
signal.
Figure 2: Here morphogenesis is demonstrated by cells migrating to align with expectations
based on sensory input, modelled by a concomitant decrease of free energy in time.
Î·t and that sensor noise is standard white in time,i.e.that the likelihood satisfies
p(s|Î·)âˆexp

âˆ’(sâˆ’Î·) 2
2Ïƒ2s

at steady state. We will also introduce a likelihood over outcomes of actions
p(Î·|a)âˆexp

âˆ’(Î·âˆ’a) 2
2Ïƒ2Î·

in place of the simplifying adiabatic assumption used in the previous example.
The systemâ€™s predictions of sensations given a phase is the cosine functions(Âµ) =AcosÂµand
we introduce a predictive control prior making actions prefer to create the encoded preference for
the waveform,
p(a|Âµ)âˆexp

âˆ’(aâˆ’AcosÂµ) 2
2Ïƒ2a

.
There will finally be a uniform prior onÂµas a circle-valued variable.
In total we have a one-step generative model (conditioned on the internal phase):
s|Î·âˆ¼ N(Î·, Ïƒ2
s), Î·|aâˆ¼ N(a, Ïƒ 2
Î·), a|Âµâˆ¼ N(AcosÂµ, Ïƒ 2
a).
16
location
location
-4 -3 -2 -1 0 1 2 3 4
-4
-3
-2
-1
0
1
2
3
4
Figure 3:Dynamics of each cell, with final locations starred.The gradients here match
those of Figure 2a. It can be seen that (speaking somewhat heuristically) each cell moves to fulfil
itsexpectationsabout the signals it should encounter, whilst expressing the signals associated
with its current beliefs about its place in the target ensemble.
The generative model is linear in (s, Î·, a) and mildly nonlinear inÂµthrough the cosine function.
To keep this example briefâ€”and to make the surprisal analytically soluble, such that some direct
comparisons can be made between (2.1.1) and (2.1.3)â€”we will not create any hidden states to
make inferences about beyond the environmental state, so our generative model is not hierarchical
and generalised coordinates of motion need not be used.
In that case, using conditional independence, we can write at steady state (up to additive
17
constants)
âˆ’logp(x) = (sâˆ’Î·) 2
2Ïƒ2s
+ (Î·âˆ’a) 2
2Ïƒ2Î·
+ (aâˆ’AcosÂµ) 2
2Ïƒ2a
.
Now specify a parametrisation ofQand Î“,
Q=
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
0q Âµs qÂµa 0
âˆ’qÂµs 0 0 0
âˆ’qÂµa 0 0q aÎ·
0 0âˆ’q aÎ· 0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
,Î“ = diag(Î³ Âµ, Î³s, Î³a, Î³Î·).
Notice there are no couplings in the flow over the state space betweenÂµandÎ·. This is an indication
of the Markov blanket.
Since the generative model is somewhat simple, it is straightforward to write the equations of
motion:
âˆ‚tÂµt =q Âµsâˆ‚sL+q Âµaâˆ‚aLâˆ’Î³ Âµâˆ‚ÂµL+Î¾ Âµ
t
=q Âµs
sâˆ’Î·
Ïƒ2s
+q Âµa
aâˆ’Î·
Ïƒ2Î·
+ aâˆ’AcosÂµ
Ïƒ2a

âˆ’Î³ Âµ
AsinÂµ
Ïƒ2a
(aâˆ’AcosÂµ) +Î¾ Âµ
t
âˆ‚tst =âˆ’q Âµsâˆ‚ÂµLâˆ’Î³ sâˆ‚sL+Î¾ s
t
=âˆ’q Âµs
AsinÂµ
Ïƒ2a
(aâˆ’AcosÂµ)âˆ’Î³ s
sâˆ’Î·
Ïƒ2s
+Î¾ s
t
âˆ‚tat =âˆ’q Âµaâˆ‚ÂµL+q aÎ·âˆ‚Î·Lâˆ’Î³ aâˆ‚aL+Î¾ a
t
=âˆ’q Âµa
AsinÂµ
Ïƒ2a
(aâˆ’AcosÂµ) +q aÎ·
Î·âˆ’s
Ïƒ2s
+ Î·âˆ’a
Ïƒ2Î·

âˆ’Î³ a
aâˆ’Î·
Ïƒ2Î·
+ aâˆ’AcosÂµ
Ïƒ2a

+Î¾ a
t
âˆ‚tÎ·t =âˆ’q aÎ·âˆ‚aLâˆ’Î³ Î·âˆ‚Î·L+Î¾ Î·
t
=âˆ’q aÎ·
aâˆ’Î·
Ïƒ2Î·
+ aâˆ’AcosÂµ
Ïƒ2a

âˆ’Î³ Î·
Î·âˆ’s
Ïƒ2s
+ Î·âˆ’a
Ïƒ2Î·

+Î¾ Î·
t
(3.2.1)
whereÎ¾ i
t are independent Brownian motions. Notice the solenoidal couplings between sensory
states and environmental states preclude any adiabatic assumption.
Using properties of marginals of jointly Gaussian random variables we can compute the steady
state
pâˆ—(s, a, Âµ)âˆ
Z
eâˆ’L(Âµ,s,a,Î·) dÎ·= 1
2Ï€N
 
a;AcosÂµ, Ïƒ2
a

N
 
s;a, Ïƒ2
s +Ïƒ 2
Î·

i.e.i.e. a uniform phase prior, a Gaussian action prior aroundAcosÂµ, and a sensor likelihood
concentrated around the action with varianceÏƒ 2
s +Ïƒ 2
Î·. The interpretation of this density is that
18
the triple (Âµ, s, a) forms a ribbon-shaped non-equilibrium steady state inS1 Ã—R 2 aligned with the
subspace{a=AcosÂµ, s=a}. The solenoidal component makes the probability circulate along
this ribbon; however, the shape of the ribbon is fixed by the generative terms specified above.
The system can treated as a linear Gaussian chain, with Gaussian likelihoods fors|Î·and
Î·|s, a, providing an exact expression for the posterior
p(Î·|s, a)âˆexp

âˆ’(sâˆ’Î·) 2
2Ïƒ2s
âˆ’ (Î·âˆ’a) 2
2Ïƒ2Î·

.
Completing the square inÎ·yieldsp(Î·|s, a) =N(m post, vpost) where
vpost =
 1
Ïƒ2s
+ 1
Ïƒ2Î·
âˆ’1
, m post =v post
 s
Ïƒ2s
+ a
Ïƒ2Î·

so that the posterior is parametrised by a variance of twice the inverse precision. Now let the
internal states parametrise a Gaussian posterior expressing beliefs about the world,q(Î·;Âµ, Ï„ 2) =
N(Î·;AcosÂµ, Ï„ 2). The free energy can be expressed analytically as
F(s, a, Âµ) =Eq[logq(Î·;Âµ, Ï„2)]âˆ’E q[logp(Î·|s, a)]âˆ’logp(s, a, Âµ)
=âˆ’ 1
2 log 2Ï€Ï„2 âˆ’ 1
2 + 1
2 log 2Ï€vpost + 1
2vpost
 
Ï„2 + (AcosÂµâˆ’m post)2
âˆ’logp(s, a, Âµ)
or, collecting like terms,
1
2

log vpost
Ï„2 + Ï„2 + (AcosÂµâˆ’m post)2
vpost
âˆ’1

âˆ’logp(s, a, Âµ)
so that whenÏ„ 2 =v post andm post =AcosÂµ, the KL term vanishes.
Taking gradients of the free energy yields the equations of motion
âˆ‚tÂµt =q Âµsâˆ‚sF+q Âµaâˆ‚aFâˆ’Î³ Âµâˆ‚ÂµF+Î¾ Âµ
t
=q Âµs
sâˆ’AcosÂµ
Ïƒ2s
+q Âµa(aâˆ’AcosÂµ)
 1
Ïƒ2Î·
+ 1
Ïƒ2a

âˆ’Î³ ÂµAsinÂµ
sâˆ’AcosÂµ
Ïƒ2s
+ aâˆ’AcosÂµ
Ïƒ2Î·
+ aâˆ’AcosÂµ
Ïƒ2a

+Î¾ Âµ
t
âˆ‚tat =âˆ’q Âµaâˆ‚ÂµFâˆ’Î³ aâˆ‚aF+Î¾ a
t
=âˆ’q ÂµaAsinÂµ
sâˆ’AcosÂµ
Ïƒ2s
+ aâˆ’AcosÂµ
Ïƒ2Î·
+ aâˆ’AcosÂµ
Ïƒ2a

âˆ’Î³ a(aâˆ’AcosÂµ)
 1
Ïƒ2Î·
+ 1
Ïƒ2a

+Î¾ a
t .
(3.2.2)
We then simulated the system as a Kalman update for a linear Gaussian state space channel
Î·â†’swith inputa. 3 In practice, this means when simulating the system forwards, we set the
3 The python code for this simulation is available athttps://github.com/DARSakthi/periodically_firing_
cells/.
19
parameters of the variational density to those of the posterior such that the variational densityq
became the (instantaneous) Kalman posterior over the latent world stateÎ·and the instantaneous
precision-weighted prediction errors drove the gradient descent onF.
In Figure 4 sampled trajectories ofÂµ t under (3.2.1) and (3.2.2) are compared (Figure 5,a t, re-
spectively). Close correlation is observed between the two flow regimes. A corresponding tendency
towards zero of the sample mean of the KL term is observed in Figure 6. Plotted per-timepoint
sample means of the free energy under both flow regimes are contained in Figure 7. By adding the
precision-weighted prediction error (KL) term to the internal updates,Fhas a faster decrease and
a greater magnitude of decrease under (3.2.2); this is evidently from the prediction error driving
the flow ofâˆ‚ ÂµFto alignq(Î·;Âµ) withp(Î·|s, a).
Figure 4:Evolution of the internal state underLand underF.The dynamics of internal
states in two regimes, plotted on a polar spiral showing the evolution in the phase in time. The
dynamics under a gradient descent onLand onFshow excellent agreement over all timescales.
20
Figure 5:Evolution of the active state underLand underF.The dynamics of the active
state in two regimes were also plotted; again, excellent agreement is observed.
Figure 6:Decrease of the KL divergence under the flow onF.The sample mean
(N= 200) of the KL divergence along trajectories of (3.2.2) is shown.
21
Figure 7:Decrease of the free energy under trajectories in both flow regimes.The
sample mean (N= 200) of the variational free energy along trajectories of (3.2.1) and along
trajectories of (3.2.2) are compared. Both drive the free energy towards zero, however, the flow
underFdecreases more rapidly and attains a lesser minimum closer to zero.
The reason an inferential interpretation is useful here is in the coding of stimuli in neurones
and neuronal ensembles. Minimising variational free energy, and the resultant model of internal
and active states behaving as if they were performing Bayesian inference, endows the phase of
the oscillation with the interpretation of a posterior belief about expected incoming signals in the
environment and the Kalman-like updates as modulations of gain to minimise precision-weighted
prediction error, rather than merely a position on a limit cycle. Under the relationship to stochastic
thermodynamics forged ine.g.[24], quantitative predictions can be made about the cost of main-
taining such beliefsâ€”and with an explicit low-dimensional representation of such signals given by
the concentration of the non-equilibrium steady state density, we have an organically emerging
neural manifold from those beliefs.
22
4. SOME PRELIMINARY PHILOSOPHICAL CONSIDERATIONS
We will now enter a more philosophical discussion concerning theuseof the FEP, guiding
its application and interpretation. The preceding technical arguments establish the FEP as an
explanatory principle that can be applied to any thing (particle, person, or population) around
which one can draw a boundary or Markov blanket. This means that the FEP is what it says on
the tin: it is a principle. Like the principles of least action or of maximum entropy, the FEP is a
machinery that produces models [3].
Throughout, â€˜as ifâ€™ marks an interpretive equivalence: internal flows that minimise variational
free energy can be read as variational inference without presupposing explicit, propositional mod-
els inside the system. Where specific mechanisms are identified (e.g., precision-weighted message
passing), this reading coincides with literal implementation; otherwise, it remains an explanatory
stance. How might one apply the FEP to make a model of a particular particle, person or popula-
tion? The answer is straightforward: the observer is trying to infer the generative modelâ€”which
contains unobservable internal states of an objectâ€”that best explains the observed dynamics of
some thing. What is observed is simply the blanket states of some â€˜thingâ€™ that shield internal
states from direct observation. If a generative model that best explains an objectâ€™s behaviour can
be identified, then by construction the FEP affords the modeller a complete (but not over-complete)
description of the objectâ€™s internal dynamics, even though they can never be observed. As noted
above, technical term for this relationship between real-world system and generative model is â€œen-
tailmentâ€: a system that conforms to the dependence structure of the generative model is said to
entailthat generative model, with non-entailment of a known generative model being surprising.
If we now move to a meta-theoretical perspective and consider one particle (e.g., a scientist or
philosopher) observing another particle, we have the interesting situation where the modeller may
impute a particular generative model that best explains the mechanics of the observed particle. Is
this meta-model a map or a territory?4 Using a generative model to capture the formal structure of
the environmentâ€”and to distinguish it from the model that is carried by particles (the variational
density)â€”can be read as a new take on thenouvelle AIidea that physical systems are their own best
models; or as a new take on the good regulator theorem. For instance, in work on robotics following
the tradition of embodied cognition [38, 39], practitioners have eschewed the construction of agents
4 This meta-theoretical move is commonplace in practical applications of the FEP and is sometimes referred to
as meta-Bayesian inference or, more simply, observing the observer [36]. Practically, it involves optimising the
parameters of a generative model, such that under the FEP (the ideal Bayesian observer assumption) the observable
behaviour of the particle is rendered the most likely. This application of the FEP is often described as computational
phenotyping [37].
23
with internal representations of some environment. In these systems, the physics and geometry of
the situation were sufficient to endow these agents with the capacity to couple to an environment.
The world is, on this view, its own best representation. In our view, this approach dovetails nicely
with FEP-theoretic modelling, where the generative model in FEP-theoretic constructions is just
a joint probability density defined over all the states (or paths) of a system. In other words, the
generative model is just our representation of the world as we believe it to be; and this model
need not be encoded directly in the particle or agent. Given a generative model or Lagrangian
of the appropriate sort, we can show that the system evinces a particular partition (i.e., contains
particles); and we can show that subsets of the system track each other, where tracking means
inferring or becoming the sufficient statistics of probabilistic beliefs about their external states.
We interpret this tracking as a form of inference, namely, variational or approximate Bayesian
inference under a generative model (i.e., the map).
In the context of this meta-question, one may ask how an observer could come to consciously
think an observed system is manipulating a model if the observerâ€™s own brain is simply flowing
down a free-energy gradient. Here, an observerâ€™s conscious report that â€˜X is using an internal
modelâ€™ may arise from some higher-order generative model over another agentâ€™s (or oneâ€™s own)
internal states,i.e., beliefs about the other systemâ€™s blanket-to-internal mappings (what we have
called â€˜observing the observerâ€™). Conscious thought in this context is a higher-level subset of those
internal states which summarises beliefs about others (and the self) within a hierarchical model.
Mechanistically, this report is implemented by the observerâ€™s own free-energy-minimising dynamics.
In short, if we equate the territory with the full joint system (crucially, including the unobserved
internal states or dynamics of the observed thing), then the generative model plays the role of a
map that best describes that (partially observed) territory. Note here, that the map only exists in
relation to the observerâ€”and, crucially, that the observer could be observing itself. In addition,
applications of the FEP enables one to map out the parts of a territory that are unobservable in
another sense; namely, in the sense that one cannot observe every state a given thing has been, or
will be in. Even if one could, the internal states would be forever sequestered behind blanket states.
Applications of the FEP place the generative model centre-stage as explanations for the behaviour
of thingsâ€”in terms of a map of some territory that can only be observed through its impressions
on its Markov blanket. Here both observed and observer are described at the mechanistic level
by flows on free energy flows; at the informational level, the observerâ€™s higher-order model licenses
attributions like â€˜the observed system is manipulating an internal modelâ€™.
24
5. A MAP OF THAT PART OF THE TERRITORY THAT BEHAVES AS IF IT WERE
A MAP
Interestingly, because it incorporates this information about things that are coupled and their
coupling, it has been suggested that our FEP conflates the metaphorical map (our mathematical
model of the thing modelled) and the territory (the thing modelled). The so-calledmodel reification
ormap-territoryfallacy is a general critique of the practice of scientific modellingâ€”in physics,
principally due to Cartwright [40, 41]. Some have referenced this fallacy in relation to prior work
on the FEP [42, 43]. In particular, it has been suggested that FEP-theoretic modelling conflates
the metaphorical â€œmapâ€â€”(i.e., the scientific model that scientists use to make sense of some real-
world phenomenon)â€”and â€œterritory,â€i.e., the actual physical system that is being modelled. (See
[3, 6, 44, 45] for related critical discussions of such claims.) According to this line of thinking,
using the FEP to claim that we can model objects as themselves engaging in inference about
the statistics of their environment constitutes a case of model reification. The allegation is that,
in describing the dynamics of physical objects as implementing a form of inferenceâ€”as opposed
to considering the inferential aspects of the explanation as pertaining to ourscientific modelsof
those objectsâ€”the FEP theorist mistakenly conflates their metaphorical â€œmapâ€ of the territory
(i.e., the scientific, FEP-theoretic model) with the â€œterritoryâ€ (the real-world system itself). This
constitutes a reification of our model by assuming some aspect of the model or â€œmapâ€ is a real
feature of the physical world or â€œterritoryâ€ [6, 44]. By clarifying that this potential difficulty does
not directly and necessarily apply to FEP-theoretic modelling, we are able to turn the story on its
head: namely, we point out that the FEP provides a set of ultimate explanatory constraints on
what counts as a model (or map) of anythingthat exists in the physical world.
We argue that the FEP can be understood, metaphorically, a â€œmapâ€ of sorts: a map of that
part of the territory which behaves or looks as if it were a map. In other words, the FEP provides
us with tools to understand the mathematically striking property of self-organising systems: that
they look as if they infer, track, reflect, or represent the statistics of their environment. Even more
simply, it gives us tools to model systems that look as if they are modelling the world.
How does FEP-theoretic technology allow us to model self-organising systems as modelling their
world? The generative model or joint probabilityp(Î·, b, Âµ) plays a central role. This generative
model can be read as a probabilistic specification of the states or paths characteristically occupied
by the joint particle-environment system. In other words, our scientific model (or map) includes
the generative model entailed by the system (i.e., the territory). In this sense, the generative model
25
we hypothesiseâ€”to account for observations of an objectâ€”allows us to represent the constraints
on the kinds of states the joint system can be found in. In other words, the generative model is a
modellerâ€™s map of the whole â€œterritoryâ€ that characterises the observed system and its environment
(including the modeller, as an observer in the environment).
The technology of the FEP allows us to say that certain components of the system being
considered look as if they track other components, and that this is a feature of our map contained
in a generative model under a particular partition. In providing us with a calculus allowing us
to model things as modelling other things, the FEP provides us with a map (a scientific model)
of any possible map or tracking relationship between things (namely, a mapping between internal
and external states, mediated by blanket states). 5
Does our ability to always describe some dynamics as a form of inference imply that those
dynamics are literally a form of inference, as opposed to being a manner of describing those
dynamics? Here, it is important to defuse a possible (but in our view naÂ¨ Ä±ve) objection. It is
a truism that physical systems need not explicitly calculate their trajectories of motion, to be
modelled as pursuing such trajectories. In developing an inferential account of the dynamics of
systems, we need not assume that the particle itself is literally performing inference. 6 What is at
stake with the FEP is an â€˜as ifâ€™ description. We simply take the fact that surprisal varies with the
dynamics of the system, which implies free energy is minimised; the minimisation of this quantity
is mathematically equivalent to inference, in the sense of being an estimator.
In summary, our scientific, FEP-theoretic model is, metaphorically, a map that allows us to say
that some (internal) subset of the system looks as if it possesses a map; which we interpret formally
as tracking the statistics of another (external) subset.Ourmap (as scientists and modellers) can
be identified in an unproblematic way as the generative model of the system, which FEP-theoretic
technology enables us to write down. It is by construction that our model is a model of the
modelling capacities of subsets of the real-world system: that is, our map is precisely a map of how
objects behave as if they were maps, allowing us to construct scientific models of the maps encoded
(or looking as if they were encoded) by the internal subset of the system considered. This is an
answer to allegations that FEP-theoretic modelling commits the map-territory fallacy: namely, it
shows that there is no conflation of the map and the territory, in the sense of the map wepostulate
as being embodied by the system within the mathematical machinery of the FEP. We remark that
5 Here, we are assuming representationalist accounts of scientific practice. For alternatives, see [6].
6 Ways of defending stronger, increasingly literal versions of this positions are available as well. For a defence of
the claim that physical systems are quite literally in the business of inference, see [46, 47]. Related to this, views
of physical systems implementing computations has been articulated [48, 49], as well as views where systems
self-organise by computing predictions [23, 26, 50, 51].
26
we donotclaim to invalidate the map-territory fallacy itself. That is, we do not suggest that the
map and the territory are never conflated; they obviously are in cases of genuine model reification
(as discussed cogently by [44]). Models built using the FEP are still scientific models, and the
properties of FEP-theoretic models also run the risk of reification. Instead, we are addressing
the claim that the FEP is limited in its effectiveness or engages in model reification because real
physical things cannot be modelled as themselves modelling their environments by minimising free
energy.
Taking all of the above, we might say that the FEP ultimately provides us with a map ofany
possible map whatsoeverof, or held by, a physical system. This echoes seminal work in philosophy
by Wittgenstein [52]. In his famousTractatus Logico-Philosophicus, Wittgenstein set out to delimit
the domain of sensible propositions by determining the general form of a proposition, an utterance
able to carry a truth value. This set ultimate constraints on what is sensible, and bounded what
is meaningful â€œfrom insideâ€ of language itself. Similarly, the FEP provides ultimate constraints
on possible maps or modelling relations, which arise from what it means to be a map or model
of a physical process at all. Indeed, any modelling process consistent with the laws of physics
must conform to the FEP, such that the FEP sets ultimate limits on what can count as a map or
modelâ€”starting from within the technology of mapping or modelling itself.
In sum, the generative model is a kind of scientific model, which harnesses what we know about
the dependency structure of the system. A variational density, on the other hand, is a probability
density over external states or paths, implemented in terms of the sufficient statistics of external
states given blanket states. It is an explicitly inferential representation of the manner in which
subsets of a system with a particular partition track each other: in which case, we are describing
a part of the territory that, as seen in our map, also behaves as if it were a map.
6. CONCLUSION
We argued that, in describing â€˜thingsâ€™ or â€˜particlesâ€™ as estimators or (deflated) representations
of the systems to which they are coupled, the FEP-theoretic apparatus does not commit the
map territory fallacy,i.e., it is false that applications the FEP necessarily reify aspects of the
metaphorical map (i.e., our scientific model), mistakenly taking them to be part of the territory
(i.e., the real-world system that we wish to model); although this remains possible in principle.
We have further argued that this allegation itself constitutes amap-territory fallacy fallacy. In
distinguishing the generative model and the variational densityâ€”and the generative model and
27
the generative process from the real-world system that we aim to modelâ€”the technology of FEP-
theoretic modelling allows us to construct a map of that part of the territory whichlooks to an
external observer as iforbehaves as if it were a map, without committing model reification. The
FEP is thus, metaphorically, a map of any possible map whatsoever of, or held by, a physical
system. One ought to celebrate this â€˜territory-map mapping,â€™ and the FEP is ultimately, at its
core, a principled approach to the formalisation of this mapping.
[1] Maxwell J D Ramstead, Dalton A R Sakthivadivel, and Karl J Friston. On the map-territory fallacy
fallacy. 2022. Preprint arXiv:2208.06924.
[2] Karl J Friston. A free energy principle for a particular physics. 2019. Preprint arXiv:1906.10184.
[3] Maxwell J D Ramstead, Dalton A R Sakthivadivel, Conor Heins, Magnus Koudahl, Beren Millidge,
Lancelot Da Costa, Brennan Klein, and Karl J Friston. On Bayesian mechanics: a physics of and by
beliefs.Interface Focus, 13(3):20220029, 2023.
[4] Karl Friston, Lancelot Da Costa, Dalton A R Sakthivadivel, Conor Heins, Grigorios A Pavliotis, Max-
well Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things.Physics of Life
Reviews, 47:35â€“62, 2023.
[5] Matthew J Beal.Variational algorithms for approximate Bayesian inference. PhD thesis, University
of London, University College London, 2003.
[6] Mel Andrews. The math is not the territory: navigating the free energy principle.Biology & Philosophy,
36(3):30, 2021.
[7] Maxwell J D Ramstead, Karl J Friston, and InË† es HipÂ´ olito. Is the free-energy principle a formal theory
of semantics? From variational density dynamics to neural and phenotypic representations.Entropy,
2020.
[8] Maxwell J D Ramstead, Michael D Kirchhoff, and Karl J Friston. A tale of two densities: active
inference is enactive inference.Adaptive Behavior, 28(4):225â€“239, 2020.
[9] Judea Pearl.Graphical Models for Probabilistic and Causal Reasoning, pages 367â€“389. Springer, 1998.
[10] Dalton A R Sakthivadivel. Weak Markov blankets in high-dimensional, sparsely-coupled random dy-
namical systems. 2022. Preprint arXiv:2207.07620.
[11] Dalton A R Sakthivadivel. A worked example of the Bayesian mechanics of classical objects. InActive
Inference: Third International Workshop, IWAI 2022, Grenoble, France, September 19, 2022, Revised
Selected Papers, pages 298â€“318. Springer, 2023.
[12] Edwin T Jaynes. Information theory and statistical mechanics.Physical Review, 106(4):620, 1957.
[13] Edwin T Jaynes.Probability Theory: The Logic of Science. Cambridge University Press, 2003.
[14] Karl Friston, Klaas Stephan, Baojuan Li, and Jean Daunizeau. Generalised filtering.Mathematical
Problems in Engineering, 2010(1):621670, 2010.
28
[15] Lancelot Da Costa, Karl J Friston, Conor Heins, and Grigorios A Pavliotis. Bayesian mechanics for
stationary processes.Proceedings of the Royal Society A, 477(2256), 2021.
[16] Dalton A R Sakthivadivel. Towards a geometry and analysis for Bayesian mechanics. 2022. Preprint
arXiv:2204.11900.
[17] Udo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines.Reports on
Progress in Physics, 75(12):126001, 2012.
[18] Ping Ao. Potential in stochastic differential equations: novel construction.Journal of Physics A:
Mathematical and General, 37(3):L25, 2004.
[19] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. InAdvances
in Neural Information Processing Systems, volume 28, pages 2917â€“2925, 2015.
[20] Ruoshi Yuan, Ying Tang, and Ping Ao. SDE decomposition and A-type stochastic interpretation in
nonequilibrium processes.Frontiers of Physics, 12:1â€“9, 2017.
[21] Lancelot Da Costa and Grigorios A Pavliotis. The entropy production of stationary diffusions.Journal
of Physics A: Mathematical and Theoretical, 56(36):365001, 2023.
[22] Mark I Freidlin and Alexander D Wentzell.Random Perturbations of Dynamical Systems, volume 260
ofGrundlehren der mathematischen Wissenschaften. Springer, 1998.
[23] Udo Seifert. From stochastic thermodynamics to thermodynamic inference.Annual Review of Con-
densed Matter Physics, 10:171â€“192, 2019.
[24] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, information geometry and
stochastic thermodynamics.Philosophical Transactions of the Royal Society A, 378(2164):20190159,
2020.
[25] Karl Friston, JÂ´ erÂ´ emie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational
free energy and the Laplace approximation.NeuroImage, 34(1):220â€“234, 2007.
[26] Kai UeltzhÂ¨ offer, Lancelot Da Costa, Daniela Cialfi, and Karl J Friston. A drive towards thermodynamic
efficiency for dissipative structures in chemical reaction networks.Entropy, 23(9):1115, 2021.
[27] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: optimal control
without cost functions.Biological Cybernetics, 106:523â€“541, 2012.
[28] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active inference, homeostatic regulation and
adaptive behavioural control.Progress in Neurobiology, 134:17â€“35, 2015.
[29] LÂ´ eo Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. Active inference and robot control:
a case study.Journal of The Royal Society Interface, 13(122):20160616, 2016.
[30] Giovanni Pezzulo, Francesco Rigoli, and Karl J Friston. Hierarchical active inference: a theory of
motivated control.Trends in Cognitive Sciences, 22(4):294â€“306, 2018.
[31] Manuel Baltieri and Christopher L Buckley. PID control as a process of active inference with linear
generative models.Entropy, 21(3):257, 2019.
[32] Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the relationship
between active inference and control as inference. InActive Inference: First International Workshop,
29
pages 3â€“11. Springer, 2020.
[33] Mohamed Baioumy, Paul Duckworth, Bruno Lacerda, and Nick Hawes. Active inference for integ-
rated state-estimation, control, and learning. In2021 IEEE International Conference on Robotics and
Automation, pages 4665â€“4671. IEEE, 2021.
[34] Karl Friston, Michael Levin, Biswa Sengupta, and Giovanni Pezzulo. Knowing oneâ€™s place: a free-energy
approach to pattern regulation.Journal of the Royal Society Interface, 12(105):20141383, 2015.
[35] Karl J Friston. A free energy principle for a particular physics. 2019. Preprint arXiv:1906.10184.
[36] Jean Daunizeau, Hanneke E M Den Ouden, Matthias Pessiglione, Stefan J Kiebel, Klaas E Stephan,
and Karl J Friston. Observing the observer (I): meta-Bayesian models of learning and decision-making.
PLoS ONE, 5(12):e15554, 2010.
[37] Philipp Schwartenbeck and Karl J Friston. Computational phenotyping in psychiatry: a worked ex-
ample.eNeuro, 3(4), 2016.
[38] Randall D Beer. Computational and dynamical languages for autonomous agents. InMind as Motion:
Explorations in the Dynamics of Cognition, pages 121â€“147. 1996.
[39] Rodney A Brooks. Intelligence without representation.Artificial Intelligence, 47(1-3):139â€“159, 1991.
[40] Nancy Cartwright.How the Laws of Physics Lie. Oxford University Press, 1984.
[41] Nancy Cartwright.Natureâ€™s Capacities and their Measurement. Oxford University Press, 1989.
[42] Jelle Bruineberg, Krzysztof Dolega, Joe Dewhurst, and Manuel Baltieri. The emperorâ€™s new Markov
blankets.Behavioral and Brain Sciences, pages 1â€“63, 2020.
[43] Thomas van Es. Living models or life modelled? On the use of models in the free energy principle.
Adaptive Behavior, 2020.
[44] Mel Andrews. Making reification concrete: a response to Bruineberg et al.Behavioral and Brain
Sciences, 45:e186, 2022.
[45] Michael D Kirchhoff, Julian Kiverstein, and Ian Robertson. The literalist fallacy and the free en-
ergy principle: model-building, scientific realism, and instrumentalism.The British Journal for the
Philosophy of Science, 2022.
[46] Alex B Kiefer. Literal perceptual inference. InPhilosophy and Predictive Processing, 2017.
[47] Alex B Kiefer. Psychophysical identity and free energy.Journal of the Royal Society Interface,
17(169):20200370, 2020.
[48] Clare Horsman, Susan Stepney, Rob C Wagner, and Viv Kendon. When does a physical system
compute?Proceedings of the Royal Society A, 470(2169):20140182, 2014.
[49] Scott Aaronson. Guest column: NP-complete problems and physical reality.ACM SIGACT News,
36(1):30â€“52, 2005.
[50] Susanne Still, David A Sivak, Anthony J Bell, and Gavin E Crooks. Thermodynamics of prediction.
Physical Review Letters, 109(12):120604, 2012.
[51] Nikolay Perunov, Robert A Marsland, and Jeremy L England. Statistical physics of adaptation.Physical
Review X, 6(2):021036, 2016.
30
[52] Ludwig Wittgenstein.Tractatus Logico-Philosophicus. 1922. 2013 edition, Routledge.