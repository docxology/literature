A Novel Adaptive Controller for Robot Manipulators
Based on Active Inference
Corrado Pezzato1, Riccardo Ferrari 2 and Carlos Hern ´andez Corbato3
Abstract— More adaptive controllers for robot manipulators
are needed, which can deal with large model uncertainties.
This paper presents a novel active inference controller (AIC)
as an adaptive control scheme for industrial robots. This
scheme is easily scalable to high degrees-of-freedom, and it
maintains high performance even in the presence of large
unmodeled dynamics. The proposed method is based on active
inference, a promising neuroscientiﬁc theory of the brain, which
describes a biologically plausible algorithm for perception and
action. In this work, we formulate active inference from a
control perspective, deriving a model-free control law which
is less sensitive to unmodeled dynamics. The performance and
the adaptive properties of the algorithm are compared to a
state-of-the-art model reference adaptive controller (MRAC)
in an experimental setup with a real 7-DOF robot arm. The
results showed that the AIC outperformed the MRAC in terms
of adaptability, providing a more general control law. This
conﬁrmed the relevance of active inference for robot control.
Index Terms— Biologically-Inspired Robots, Adaptive Con-
trol of Robotic Systems, Industrial Robots, Active Inference,
Free-energy Principle
I. I NTRODUCTION
Traditional control approaches for industrial manipulators
rely on an accurate model of the plant. However, there is
an increasing demand in industry for robot controllers that
are more ﬂexible and adaptive to run-time variability. Often,
robot manipulators are placed in dynamically changing sur-
rounding, and they are subject to noisy sensory input and
unexpected events. In these new applications, obtaining such
a model is a major problem. For example, in pick and place
tasks, the dynamics of the robot manipulators can change un-
predictably while handling unknown objects. Recent research
has focused on the use of machine learning methods to obtain
accurate inverse dynamic models [1], [2]. In general, learning
models using Neural Networks (NN) requires experts for
deﬁning the best topology for a particular problem [3]. Even
though it is possible to exploit the physical knowledge of the
system to simplify and improve the learning performance
[4], the need of large amount of training data and several
iterations for learning, still remains a problem and hard to
generalise [5], [6]. Controllers that can dynamically adapt are
required, but existing solutions in adaptive control either need
an accurate model, or are difﬁcult to tune and scale to higher
*This research was supported by Ahold Delhaize. All content represents
the opinion of the author(s), which is not necessarily shared or endorsed by
their respective employers and/or sponsors.
1,3Corrado Pezzato and Carlos Hern ´andez Corbato are with the Cog-
nitive Robotics Department, TU Delft, 2628 CD Delft, The Netherlands
c.pezzato@tudelft.nl, and c.h.corbato@tudelft.nl
2Riccardo Ferrari is with the Department of Systems and Control, TU
Delft, 2628 CD Delft, The Netherlands r.ferrari@tudelft.nl
number of DOFs. In this paper, we present a novel adaptive
controller for robot manipulators, inspired by a recent theory
of the brain, which does not require accurate plant dynamics,
and that is less sensitive to large parameters variation.
The proposed control scheme is based on the general free-
energy principle proposed by Karl Friston [7], and redeﬁned
in engineering terms [8], [9]. The main idea at the basis of
Friston’s neuroscientiﬁc theory, is that the brain’s cognition
and motor control functions could be described in terms
of energy minimization. It is supposed [10] that humans
have a set of sensory data and a speciﬁc internal model to
characterize how the sensory data could have possibly been
generated. Then, given this generative model, the causes of
sensory data are inferred. Usually, the environment acts on
humans to produce sensory impression, and humans can act
on the environment to change it. In this view, the motor
control of human body can be considered as the fulﬁllment
of a prior expectation about proprioceptive sensations [11].
Although the general active inference framework is math-
ematically well deﬁned, its application to robotics remains
a challenge. Active inference has mainly been applied to
neuronal simulations (for handwriting [7] for instance), sup-
posing to know the true dynamical process. However, this is
not the case in robotics. Even if the neuronal simulations are
a strong proof of concept for the neuroscientiﬁc theory, in
the present form their extension to realistic robotic scenarios
[12], [13] does not provide advantages over other classical
controllers. The main problems are the computational load
and the deﬁnition of meaningful generative models. With our
work we overcome these limitations, using active inference to
derive a model-free control law. Instead of modeling the true
unknown dynamical process, we deﬁne a reference model
that active inference has to follow. The main contributions
of this paper are twofold:
• Derivation of an online active inference control law for
a generic n-DOF robot manipulator in joint space.
• Comparison of the adaptability of the AIC with a state-
of-the-art model reference adaptive controller.
The contributions have been experimentally validated in a
7-DOF collaborative industrial manipulator.
A. Related work
At present, the use of active inference for robot control
is still limited. In [12], the authors simulated a PR2 robot
controlled in Cartesian space for a reaching task. The solu-
tion was ofﬂine, computationally expensive, open-loop, and
it relied on an additional position controller. This makes the
approach not suitable for online tasks. A recent MSc thesis
arXiv:1909.12768v2  [cs.RO]  13 Apr 2021
[13], based on [12], derived an ofﬂine closed-loop scheme
of active inference. The feedforward torque commands for a
simulated 7-DOF manipulator are computed ofﬂine, relying
on additional controllers for feedback control. The scheme
failed to control the robot in presence of gravity since the
feedforward torques did not include the gravitational effect.
Both [12] and [13] were based on the Statistical Parametric
Mapping (SPM) by Friston. This toolbox is suitable for
several ofﬂine applications, but it is too computationally
heavy for online control. In [13], each iteration is reported to
take about one second. Another recent work [14] formalised
the use of the free-energy for static state estimation, using a
real UR5 robot arm equipped with proprioceptive and visual
sensors. Even though the results of the state estimation were
promising, no control actions were included. The same au-
thors presented in [15] the body estimation and control in the
joint space of a simulated 2-DOF robot arm through active
inference. This solution included state-of-the art regressors to
estimate online the generative models. However, during the
simulations, the estimation of the acceleration was unreliable
and substituted with the ground truth. Regardless of the
fact that only forward dynamics models had to be learned,
the authors pointed out how this approach is not simpler
compared with classical inverse dynamics techniques. In a
parallel, related work on active inference [16], the authors
successfully controlled a real 3-DOF robot arm using veloc-
ity commands. In our approach we formulate an AIC for
online closed loop control of industrial robots, using low-
level torque commands. We also provide a comparison with
a state-of-the-art adaptive controller, and insights for design
and tuning. On the other hand, the adaptive control branch of
control theory [17], offers solutions to deal with manipulators
subject to parameters variation and abrupt changes in the
dynamics. Within adaptive controllers, two main categories
can be identiﬁed: the model reference adaptive systems, and
the self-tuning regulators [18]. The ﬁrst technique being
studied for robot manipulators was the model reference
adaptive control (MRAC) [19]. The idea behind this tech-
nique is to derive a control signal to be applied to the
robot actuators which will force the system to behave as
speciﬁed by a chosen reference model. Furthermore, the
adaptation law is designed to guarantee stability using either
Lyapunov theory or hyperstability theory [20]. The other
most common approach for robot control is the self-tuning
adaptive control [21], [22]. The main difference between this
technique and the MRAC is that the self-tuning approach
represents the robot as a linear discrete-time model and it
estimates online the unknown parameters, substituting them
in the control law. Adaptive control of robot manipulators
is required in presence of uncertain dynamics and varying
payloads, however, the complexity of the controller usually
increases with increasing number of DOFs. Among all the
possible adaptive controllers, in this paper we choose the
MRAC with hyperstability theory [20] for comparison. This
approach provides adaptability to abrupt changes in the robot
dynamics, and it does not require the kinematic or dynamic
description of the manipulator, similarly to the AIC.
B. Paper structure
The paper is organised as follows: In Sec. II we present
the free-energy principle and active inference in control
engineering terms. In Sec. III we derive a novel AIC for a
7-DOF robot manipulator, and we explain the model assump-
tions and simpliﬁcations. In Sec IV the MRAC is presented
for comparison. In Sec. V we compare the adaptability of
the AIC and MRAC in a simulated pick and place task,
validating the results in the real setup. We also discuss
the advantages of our AIC and the open questions. Finally,
Sec. VI provides a summary and directions for future work.
II. T HE ACTIVE INFERENCE FRAMEWORK
In this section we report the free-energy principle and
active inference from [8], [10], rewriting only the necessary
concepts in control terms, to understand the derivation of our
novel AIC in Sec. III.
A. The free-energy principle
The free-energy principle is formulated in terms of
Bayesian inference [23]. In this view, body perception for
state estimation is framed using Bayes rule:
p(x|y) = p(y|x)p(x)
p(y) (1)
where p(x|y) is the probability of being in the n-dimensional
state x given the current m-dimensional sensory input y.
Instead of exactly inferring the posterior, which often in-
volves intractable integrals, an auxiliary probability distri-
bution rd(x), called recognition density, is introduced. By
minimizing the Kullback-Leibler divergence (DKL) between
the true posterior p(x|y) and rd(x), the most probable state
given a sensory input is inferred [8]. DKL is deﬁned as:
DKL(rd(x)||p(x|y)) =
∫
rd(x) ln rd(x)
p(x|y)dx= F+ln p(y)
(2)
In the equation above, the scalar F is the so called free-
energy. By minimizing F, DKL is also minimized and the
recognition density approaches the true posterior. Accord-
ing to the Laplace approximation [24], the controller only
parametrises the sufﬁcient statistics (e.g. mean and variance)
of the recognition density. rd(x) is assumed Gaussian and
sharply peaked at its mean value µ. This approximation
allows to simplify the expression for Fwhich results:
F≈− ln p(µ,y) (3)
The mean µ is the internal belief about the true states x.
Minimizing F, the controller is continuously adapting the
internal belief µ about the states x based on the current
sensory input y.
B. Free-energy equation
Equation (3) is still general and it has to be further
speciﬁed to numerically evaluate F. To do so, the joint
probability p(µ,y) has to be deﬁned. This is done by
introducing two generative models, one to predict the sensory
data y, according to the current belief µ, and another to
describe the dynamics of the evolution of the belief µ.
1) Generative model of the sensory data: The sensory
data is modeled using the following expression [8]:
y= g(µ) + z (4)
where g(µ) represents the non-linear mapping between sen-
sory data and states of the environment, and z is Gaussian
noise z∼(0,Σy). The covariance matrix Σy also represents
the controller’s conﬁdence about each sensory input.
2) Generative model of the state dynamics:In presence of
time varying states x, the controller has to encode a dynamic
generative model of the evolution µ′ of the belief µ. This
generative model is deﬁned as [8]:
dµ
dt = µ′= f(µ) + w (5)
where f is a generative function dependant on the belief
about the states µand wis Gaussian noise w∼(0,Σµ).
3) Generalised motions: To describe the dynamics of the
states, or better the belief about these dynamics, we have
to introduce the concept of generalised motions [25]. Gener-
alised motions are used to represent the states of a dynamical
system, using increasingly higher order derivatives of the
states of the system itself. They apply to sensory inputs as
well, meaning that the generalised motions of a position
measurement, for example, correspond to its higher order
temporal derivatives (velocity, acceleration, and so on). The
use of generalised motions allows a more accurate descrip-
tion of the system’s states. More precisely, the generalised
motions ˜µ of the belief under local linearity assumptions
[24] are, up to the second order:
µ′ = µ(1) = f(µ) + w
µ′′ = µ(2) = ∂f
∂µµ′+ w′ (6)
In general, we indicate the generalised motions of the states
up to order nd1 as ˜µ= [µ, µ′, µ′′, µ′′′, ..., µ(nd)].
Similarly, the generalised motions of the sensory input are:
y = y(0) = g(µ) + z
y′ = y(1) = ∂g
∂µµ′+ z′ (7)
We indicate the generalised motions of the sensory input up
to order nd as ˜y= [y, y′, y′′, y′′′, ..., y(nd)].
4) General free-energy expression:With the extra theoret-
ical knowledge about the generalised motions, we can deﬁne
an expression for the free-energy for a multivariate case in
a dynamically changing environment:
F= −ln p(˜µ,˜y) (8)
The joint probability p(˜µ,˜y) has to be speciﬁed. According
to [8] and to the deﬁnitions previously given, the noise
at each dynamical order is considered uncorrelated. Then,
according to the generalised sensory input, the sensory data
at a particular order relates only with the states at the same
1Generalised motions can extend up to inﬁnite order but the noise at high
orders is predominant, thus we can limit the chosen order to nd [26].
dynamical order. Similarly, for the state dynamics, the state
at a certain dynamical order are related only with those which
are one order below. Then, using the chain rule, it results:
p(˜µ,˜y) =
nd−1∏
i=0
p(y(i)|µ(i))p(µ(i+1)|µ(i)) (9)
Using the Laplace assumption, and thus considering Gaus-
sian distributed probability densities, we can write:
p(µ(i+1)|µ(i)) = 1
|Σµ(i) |n√
2π exp
{
−1
2 ε(i)⊤
µ Σ−1
µ(i) ε(i)
µ
}
p(y(i)|µ(i)) = 1
|Σy(i) |n√
2πexp
{
−1
2 ε(i)⊤
y Σ−1
y(i) ε(i)
y
}
(10)
where ε(i)
y = (y(i) −g(i)(µ)) and ε(i)
µ = (µ(i+1) −f(i)(µ))
are respectively the sensory and state model prediction errors.
Furthermore it holds:
g(i) = ∂g
∂µµ(i), f(i) = ∂f
∂µµ(i), g(0) = g, f(0) = f
(11)
Substituting (9) in (8) leads to:
F= −
nd−1∑
i=0
[
ln p(y(i)|µ(i)) + lnp(µ(i+1)|µ(i))
]
(12)
Finally, according to (10), F can be expressed up to a
constant as a weighted sum of squared prediction errors:
F= 1
2
nd−1∑
i=0
[
ε(i)⊤
y Σ−1
y(i) ε(i)
y + ε(i)⊤
µ Σ−1
µ(i) ε(i)
µ
]
+ K (13)
where nd is the number of generalised motions chosen and
K is a constant term resulting from the substitution. The
minimisation of this expression can be done by reﬁning the
internal belief, thus performing state estimation, but also
computing the control actions to fulﬁll the prior expectations
and achieve a desired motion. The constant term K is
neglected in the sequel since it plays no role into the
minimisation problem. The next two subsections describe
the approach proposed by Friston [10], [27] to minimise F,
using gradient descent.
C. Belief update for state estimation
The belief update law for state estimation is determined
from the gradient of the free-energy, with respect to each
generalised motion [8], [25]:
˙˜µ= d
dt˜µ−κµ
∂F
∂˜µ (14)
The learning rate κµ, can be seen from a control perspective
as a tuning parameter for the state update.
D. Control actions
In the free-energy principle the control actions play a
fundamental role in the minimisation process. In fact, the
control input uallows to steer the system to a desired state
while minimising the prediction errors in F. This is done
using gradient descent. Since the free-energy is not a function
of the control actions directly, but the actions ucan inﬂuence
Fby modifying the sensory input, we can write [8]:
∂F(˜µ,˜y(u))
∂u = ∂˜y(u)
∂u
∂F(˜µ,y(u))
∂˜y(u) (15)
Dropping the dependencies for a more compact notation, the
dynamics of the control actions can be written as:
˙u= −κa
∂˜y
∂u
∂F
∂˜y (16)
where κa is the tuning parameter to be chosen.
III. R OBOT ARM CONTROL WITH ACTIVE INFERENCE
In this section we derive the ﬁrst model-free, computa-
tionally lightweight, online torque controller for joint space
control using active inference. The established theory of
Sec. II is adapted to deﬁne a novel control scheme for a
generic n-DOF manipulator. The challenging problem of
ﬁnding suitable generative models f(·) and g(·), and the
relation ∂˜y/∂uin such a complex scenario is solved.
Assumption 1: The robot manipulator is equipped with
position and velocity sensors, which respectively provide the
two variables yq, y˙q ∈Rn.
Assumption 2: Since only the position and velocity mea-
surements are available, we will consider the generalised
motions up to order two, so nd = 2.
Assumption 3: The Gaussian noise affecting the different
sensory channels is supposed uncorrelated [8], [24]. The
covariance matrices for sensory input and state belief are:
Σy(0) = σqIn, Σy(1) = σ˙qIn, (17)
Σµ(0) = σµIn, Σµ(1) = σµ′ In (18)
where we supposed that the controller associates four differ-
ent variances to describe its conﬁdence about sensory input
and internal belief.
Assumption 4: The states of the environment x are set
as the joint positions of the robot arm. Doing so, we can
control the robot arm in joint space through free-energy
minimization, and simplify the equations for states update
and control actions.
A. Generative models andFfor a robot manipulator
In order to numerically evaluate the free-energy as in (13),
the two functions g(µ) and f(µ) have to be chosen.
1) Generative model of the sensory data:g(µ) indicates
the relation between the sensed values and the states. Since
we chose the states to be the joint positions and the sensory
data provides directly the noisy values yq and y˙q, it holds:
gq(µ) = µ, ∂ gq/∂µ= 1 (19)
2) Dynamic generative model of the world: Instead of
modelling the true dynamics of the manipulator, we propose
to deﬁne a reference model to specify the desired behaviour
of the robot [8]. In particular, the world dynamics are chosen
such that the robot is steered to a desired position µd. In
other words, the controller believes that the states will evolve
in such a way that they will reach the goal µd with the
dynamics of a ﬁrst order system with unitary time constant:
f(µ) = µd −µ (20)
The value µd is a constant ∈Rn corresponding to the desired
set-point for the joints of the manipulator. Substituting (19)
and (20) in (7) and (6), it results:
{
µ′ = µd −µ+ w
µ′′ = −µ′+ w′
{
yq = µ+ z
y˙q = µ′+ z′ (21)
According to (21) and (13), the free-energy expression for a
generic robot manipulator under the assumptions given is:
F = 1
2(yq −µ)⊤Σ−1
y(0) (yq −µ)
+ 1
2(y˙q −µ′)⊤Σ−1
y(1) (y˙q −µ′)
+ 1
2(µ′+ µ−µd)⊤Σ−1
µ(0) (µ′+ µ−µd)
+ 1
2(µ′′+ µ′)⊤Σ−1
µ(1) (µ′′+ µ′) (22)
B. Belief update and state estimation for a manipulator
According to the free-energy principle, the states of the
robot manipulator can be estimated using a gradient descent
scheme. Applying (14), having deﬁned Fas in (22), leads
to the following state update law:
˙µ = µ′+ κµΣ−1
y(0) (yq −µ) −κµΣ−1
µ(0) (µ′+ µ−µd)
˙µ′ = µ′′+ κµΣ−1
y(1) (y˙q −µ′) −κµΣ−1
µ(0) (µ′+ µ−µd)
− κµΣ−1
µ(1) (µ′′+ µ′)
˙µ′′ = −κµΣ−1
µ(1) (µ′′+ µ′) (23)
Note that κµ is the tuning parameter for state estimation.
C. Control actions for a robot manipulator
The ﬁnal step in order to be able to steer the joints of a
robot manipulator to a desired value µd, is the deﬁnition of
the control actions.
1) General considerations: The general actions update is
expressed by (16). The partial derivatives of (22) with respect
to the generalised sensory input are given by:
∂F
∂yq
= Σ−1
y(0) (yq −µ), ∂F
∂y˙q
= Σ−1
y(1) (y˙q −µ′) (24)
Having said that, the actions update is expressed as:
˙u= −κa
[
∂yq
∂uΣ−1
y(0) (yq −µ) + ∂y˙q
∂uΣ−1
y(1) (y˙q −µ′)
]
(25)
Active inference requires then to deﬁne the change in the
sensory input with respect to the control actions, namely
∂yq/∂u and ∂y˙q/∂u. This is usually a hard forward dynamic
problem, which constituted a major complication in past
control strategies. One approach to compute these relations is
through online learning using high-dimensional space regres-
sors. However, this increases the complexity of the overall
scheme and can produce unreliable results, as shown by the
authors in [15]. In this paper we propose to approximate
the partial derivatives relying on the high adaptability of the
active inference controller against unmodeled dynamics, as
suggested in the conclusive remarks in [15].
2) Approximation of the true relation betweenuand ˜y:
Let us ﬁrst analyse the structure of the partial derivative
matrices in (25). The control action is a vector of n torques
applied to the njoints of the robot manipulator. Each torque
has a direct effect only on the corresponding joint to which it
is applied. This allows us to conclude that ∂yq/∂uand ∂y˙q/∂u
are diagonal matrices. Furthermore, considering the second
Newton’s law, the total torque applied to a rotational joint
equals the moment of inertia times the angular acceleration.
The diagonal terms of the partial derivatives matrices are then
time varying positive values which depend on the current
robot conﬁguration. In other words, this means that a positive
torque applied to a joint will always result in a positive
contribution for both position and velocity of that speciﬁc
joint. In this control scheme we propose to approximate the
true positive time-varying relation with a positive constant,
making use of the learning rate κa as tuning parameter to
achieve a sufﬁciently fast actions update. The control update
law is ﬁnally given by:
˙u= −κa
[
CqΣ−1
y(0) (yq −µ) + C˙qΣ−1
y(1) (y˙q −µ′)
]
(26)
∂yq
∂u ≈Cq, ∂y˙q
∂u ≈C˙q (27)
The positive deﬁnite diagonal constant matrices Cq, C˙q are
then set to the identity, meaning that we only encode the
sign of the relation between uand the change in ˜y.
3) Tuning parameters AIC:The tuning parameters for the
active inference controller are:
• σq, σ˙q, σµ, σµ′ : the standard deviations representing
the conﬁdence of the controller regarding its sensory
input and internal belief about the states;
• κµ, κa: the learning rates for state update and control
actions respectively.
Algorithm 1 reports the pseudo-code of our AIC. For state
and actions update, ﬁrst-order Euler integration is used.
Algorithm 1 AIC for robot control
Initialization
Par ←σq, σ˙q, σµ, σµ′ , κµ, κa ⊿ Set AIC parameters
µ= yq ∈Rn ⊿ Initialise belief
µ′= y˙q ∈Rn
µ′′= 0 ∈Rn
u= 0 ∈Rn ⊿ Initialise torque commands
µd ∈Rn ⊿ Set prior, desired goal
Control Loop ⊿ At high frequency
yq, y˙q ⊿ Retrieve sensory input
˙˜µ= d
dt˜µ−κµ∂F
∂˜µ ⊿ Belief dynamics (14)
˜µ= ˜µ+ ∆t ˙˜µ ⊿ Belief update, integration
˙u= −κa
∂˜y
∂u
∂F
∂˜y ⊿ Action dynamics (16)
u= u+ ∆t ˙u ⊿ Action update, integration
return u ⊿ Commanded torque
IV. M ODEL REFERENCE ADAPTIVE CONTROLLER
The controller chosen for comparison is an MRAC. This
adaptive controller allows to obtain decoupled joint dynam-
ics, forcing every single joint i = 1 ,...,n to respond as a
second order linear system with transfer function:
Gi(s) = ω2
i
s2 + 2ζωis+ ω2
i
qri(s) (28)
The control architecture is taken from [20], where the control
is speciﬁed in terms of feedforward and feedback adaptive
gain matrices. These time-varying gain matrices are adjusted
by means of adaptation laws to guarantee closed loop stabil-
ity in case of large parameters perturbations. Supposing zero
initial conditions for the gains, and neglecting the derivative
terms as described in [20], it holds:
K0(t) = E01 ¯qe(t)q(t)⊤+ E02
∫ T
0
¯qe(τ)q(τ)dτ (29)
K1(t) = E11 ¯qe(t) ˙q(t)⊤+ E12
∫ T
0
¯qe(τ) ˙q(τ)dτ (30)
Q0(t) = F01 ¯qe(t)qr(t)⊤+ F02
∫ T
0
¯qe(τ)qr(τ)dτ (31)
Q1(t) = F11 ¯qe(t) ˙qr(t)⊤+ F12
∫ T
0
¯qe(τ) ˙qr(τ)dτ (32)
f(t) = α1 ¯qe(t) + α2
∫ T
0
¯qe(τ)dτ (33)
The variables qr and ˙qr are the desired references to track.
The diagonal matrices Ejk and Fjk ∈Rn×n, and the vector
αk ∈Rn with j = {0,1}and k = {1,2}, are the tuning
parameters for the proportional-integral adaptation law. The
term ¯qe is called modiﬁed joint angle error vector [20]:
¯qe = P2[qr(t) −q(t)] + P3[ ˙qr(t) −˙q(t)] (34)
with P2 and P3 diagonal weighting matrices. The MRAC,
similarly to the AIC, does not need the dynamic description
of the robot manipulator, and it is scalable to high DOF.
However, the number of the tuning parameters increases with
the degrees of freedom, unlike for the AIC.
V. E XPERIMENTAL EVALUATION
The adaptive properties of AIC and MRAC are now
compared: The controllers are tuned in simulation using an
approximated model of the robot, and then transferred to
the real system. The tests performed are based on a pick
and place cycle using the Franka Emika Panda 7-DOF robot
manipulator, as in Fig. 1, with different payloads.
Fig. 1. Simulated and real robot for pick and place cycle.
A. Remarks about the tuning procedure for the controllers
Before presenting the simulations and experimental re-
sults, we provide some observations regarding the number
of parameters and the different tuning procedures for the
AIC and MRAC.
1) Number of tuning parameters:The number of tuning
parameters for the MRAC equals the number of DOFs times
the number of weighting terms. According to Sec. IV, this
results in 17×nparameters to be tuned. Regarding the AIC,
instead, the number of tuning parameters is independent from
the DOFs and it equals 6, following the formulation pre-
sented in Sec. III. The lower number of parameters resulted
in an overall easier tuning procedure for the active inference
controller. As a ﬁnal remark, to modify the behaviour of the
step response for the AIC, such as rise time and settling time,
one should change the internal reference model f(µ) instead
of ﬁne tuning the controller’s parameters.
2) AIC tuning procedure: To obtain a satisfactory re-
sponse for the AIC, we performed the following steps:
1) We set the controller conﬁdence about sensory input and
internal belief to one; 2) We disabled the control actions and
incremented the learning rate κµ until the state estimation in
a static situation was fast enough; 3) We included the control
actions and increased the learning rate κa until the robot was
steered to the desired position, showing signiﬁcant oscilla-
tions; 4) We dampened the oscillatory behaviour decreasing
the sensory conﬁdence about the most noisy sensors and the
internal belief about velocities.
B. Simulations with approximated model
The performance of AIC and MRAC in simulation are
now presented. The task is a pick and place cycle where the
desired joint values are chosen such that the arm simulates
the pick and place of an object from one bin to the other,
positioning the end-effector in A, B or C, see Fig 1. This
is achieved giving every 6 [ s] a set-point in joint space
following the sequence: qA, qB, qC, qB, qA, where:
• qA = [1, 0.5, 0, −2, 0, 2.5, 0] [rad]
• qB = [0, 0.2, 0, −1, 0, 1.2, 0] [rad]
• qC = [−1, 0.5, 0, −1.2, 0, 1.6, 0] [rad]
The controllers have been tuned using a considerably inac-
curate model of the robot arm on purpose. The links have
been approximated as cuboids, and 20% random uncertainty
in each link’s mass has been assumed. This will allow to
evaluate later on the adaptability performance while applying
the controllers to the real manipulator. The joint values and
control actions using AIC and MRAC, are depicted in Fig. 2.
Note that, for the MRAC, saturation of the control input at
±85Nm is reached for some of the joints, after providing
the new goal position.
C. Experiments on the real setup
The same controllers tuned in simulation using the ap-
proximated model of the 7-DOF robot arm are now applied
to control the real manipulator. Two tests are performed:
ﬁrst, the pick and place cycle of the previous section is
repeated in the real robot, without re-tuning the controllers.
Fig. 2. Response and control actions for the 7-DOF robot arm controlled
through AIC and MRAC with approximated dynamics.
Second, the AIC and MRAC are re-tuned in the real robot
and used to pick and place different objects. The real setup
is controlled using a standard laptop running Ubuntu 16.04
with RT kernel, 8-cores Intel i7-4710MQ 2.50GHz.
1) Pick and place cycle on the real robot:We applied the
MRAC and AIC from simulation to the real 7-DOF Franka
Emika Panda. It is important to notice that, besides having
different physical parameters, the real setup is already gravity
compensated. The AIC and MRAC are simply applied on
top of this intrinsic controller. This is already a considerable
change in the system’s dynamics, but to further increase the
level of uncertainties, an end-effector is attached to the robot.
From a modeling point of view, the system used for tuning
the controllers in simulation is completely different from
the real one. Usually, a controller tuned in simulation will
not directly work on a real setup, especially if the initial
model was not accurate. This was indeed the case for the
MRAC which, when transferred to the real robot, could
not control the setup leading to an immediate safety stop.
Nonetheless, this was not the case for our novel AIC: its
strong capabilities to cope with unmodeled dynamics allowed
to transfer the controller from the simulation to the real setup
without re-tuning. For clarity, we only report the response of
the AIC during the initial part of the pick and place cycle
(qB →qA →qB) in Fig. 3. Joint 7 is not reported to limit
redundant information, since no motion was required. As can
be seen the AIC can successfully control the manipulator,
however, the effect of the large uncertainties introduced
for the tuning, resulted in some initial jittering, especially
in joint 6 2. In other words, the AIC tuned in simulation
2https://youtu.be/Vsb0MzOp TY
Fig. 3. AIC on real setup without re-tuning from simulation. Focus on initial part of the pick and place cycle ( qB → qA → qB) to highlight jittering
Fig. 4. (A) Lift and place of empty bottle. (B) Difference of trajectories between empty and ﬁlled bottle during lift, place, and release
resulted too aggressive for the real robot. This is because in
simulation the AIC had to compensate also for gravity, thus a
faster torque update was required. The learning rate κa is the
same for every joint but the jittering effect is mostly visible
in joint 6. This is because in the last part of the kinematic
chain, the resulting inertia acting on a joint is lower, and so
it is its reluctance to changes in velocities. To completely
remove the jittering, one can simply reduce the learning rate
κa to lower the torque update rate. The AIC and MRAC
have been tested against large external disturbances such as
a human pushing the robot during motion. AIC resulted more
compliant than MRAC, showing at the same time a faster and
less oscillatory disturbance rejection.
2) Pick and place with different payloads: In order to
use the MRAC on the real robot, a severe re-tuning of 63
parameters had to be performed, to stabilise the response due
to the large unmodeled dynamics. For the AIC, κa has been
reduced to eliminate the jittering, as well as σq, σ˙q to give
more importance to the measurements and further reduce
oscillations. The two controllers are used to perform a pick
and place of an almost empty water bottle ( ≈0.1 [kg]) and
a full water bottle ( ≈0.7 [kg]), as in Fig. 1. In Fig. 4A we
show the responses of AIC and MRAC in case of the almost
empty bottle, during lifting and placing. As can be seen, the
AIC presents a faster convergence to the set-point, as well
as smoother trajectories with less oscillations. To achieve a
satisfactory response, we had to increase the stiffness of the
MRAC, while the AIC could be kept compliant. Furthermore,
in Fig. 4B we show the difference of the trajectories in
joint space between the case with empty and full bottle,
considering lifting, placing and releasing. Both controllers
adapt to the heavier payload, making the trajectory converge
to the one with lightweight bottle. AIC behaves similarly
to the MRAC, yet it presents considerably less oscillations
which reﬂected in smoother placing of the heavy object.
The bigger error appearing at around 16 [ s] is due to the
releasing of the heavy object. The effect is more visible in
the AIC since the robot is more compliant. In a sense, the
AIC behaves similarly to a human arm, when an unexpected
weight is dropped. This is an additional evidence of the bio-
inspired character of the controller. The AIC can also be
tuned to be stiffer if this effect is not desired.
D. Discussion and implementation notes
Our novel AIC showed high adaptability, allowing to
transfer from simulation to real robot without re-tuning. Fur-
thermore, the AIC showed superior performance with respect
to the MRAC in pick and place scenarios. The AIC is com-
pliant while allowing to compensate for large perturbations.
However, even though there is a strong evidence of stability
and robustness of the AIC for a complex non-linear system,
ﬁnding a formal stability proof is still an open question.
Similarly to a linear case, one should determine a set of
learning rates which guarantees convergence. Intuitively, ac-
tive inference is a gradient descent on a quadratic and convex
function thus, for some set of learning rates, the algorithm
should converge to the global minima. A possible approach
to a formal proof is to use Lyapunov theory as for the back-
propagation algorithm in neural networks. Active inference
is, in a sense, back-propagating the sensitivity of the control
input with respect to the free-energy, to minimiseF. Properly
addressing this proof mathematically would require a deep
analysis which is out of the scope of the current paper.
Another remark relates to the computational load of AIC.
According to Algorithm 1, our novel AIC has a computa-
tional complexity of O(n) where n is the number of DOFs.
Given the structure of the generative models and covariance
matrices chosen, the AIC reduces to 16 sums of vectors and
15 scalar-vector multiplications with n-dimensional vectors.
On the other hand, the complexity of the MRAC is O(n3).
Another optimised computed torque algorithm such as LGP
[28], which relies on learning dynamical models, has a cost
of O(N2) for online learning, where N is the number of
data points (i.e. N ≈300). Finally, the Franka Emika Panda
requires the control signals to be ready within 300 [µs] to
guarantee a functioning frequency of 1 [kHz]: Our AIC can
perform at such a high loop rate without any package loss; is
straightforward to implement; and extremely simple to tune.
The source code for simulations 3 and experiments4 is freely
available on GitHub.
VI. C ONCLUSION
In this paper we derived the ﬁrst active inference torque
controller for online joint space control of robot manipula-
tors. Our approach makes use of the alleged adaptability of
active inference, to introduce simpliﬁcations for the gener-
ative models, obtaining a model-free scheme which is less
sensitive to unmodeled dynamics, is easily scalable to high
DOF and is computationally inexpensive. With the proposed
controller structure we overcame the complexity barrier of
previous approaches, making possible control loops at high
frequency with active inference. Simulations and experiments
in a real setup with a 7-DOF robot arm showed that our AIC
is suitable for tasks in which the dynamic model of the plant
is unknown or subject to large changes. The performance
of our novel AIC has been compared with that of a state-
of-the-art MRAC, in different pick and place scenarios. The
AIC shows better adaptability properties, allowing to transfer
from simulation to real setup without re-tuning. In addition,
the AIC resulted easier to tune and implement. With this
work we conﬁrmed the value of active inference to develop
more adaptive control of robot manipulators. This is only
the ﬁrst step in this direction, future work should proof the
closed-loop stability of active inference, deﬁne generative
models to account for dynamic requirements and motion
constraints, and be extended to other control modalities, such
as control in Cartesian space or impedance control.
ACKNOWLEDGMENT
The authors would like to thank Prof. Dr. Martijn Wisse
for the helpful discussions, together with the whole group
3https://github.com/cpezzato/panda simulation
4https://github.com/cpezzato/active inference
working on active inference at the Cognitive Robotics de-
partment.
REFERENCES
[1] S. Vijayakumar and S. Shaal, “Locally weighted projection regression:
Incremental real time learning in high dimensional space,” in Proc. of
Int. Conf. on Machine Learning (ICML), 2000, pp. 1079–1086.
[2] D. Nguyen-Tuong, J. Peters, and M. Seeger, “Local gaussian process
regression for real time online model learning,” in Proc of Neural
Information Processing Systems (NIPS2008), 2008, pp. 1193–1200.
[3] M. Matteucci, “Elearnt: Evolutionary learning of rich neural network
topologies,” Carnegie Mellon University, 2006, technical repository
No. CMU-CALD-02.
[4] F. Ledezma and S. Haddadin, “First-order-principles-based construc-
tive network topologies: An application to robot inverse dynamics,” in
IEEE-RAS 17th Int. Conf. on Humanoid Robotics (Humanoids), 2017.
[5] D. Keppler, F. Peters, N. Ratliff, and S. Shaal, “A new data source
for inverse dynamics learning,” in Proc of IEEE/RJS Conference on
Intelligent Robots and Systems, 2017.
[6] L. Jamone, B. Damas, and J. Santos-Victor, “Incremental learning of
context-dependent dynamic internal models for robot control,” inProc.
of the IEEE Int. Symposium on Intelligent Control (ISIC), 2014.
[7] K. J. Friston, J. Mattout, and J. Kilner, “Action understanding and
active inference,” Biological cybernetics, vol. 104(1-2), 2011.
[8] C. Buckley, C. Kim, S. McGregor, and A. Seth, “The free energy
principle for action and perception: A mathematical review,” Journal
of Mathematical Psychology, vol. 81, pp. 55–79, 2017.
[9] R. Bogacz, “A tutorial on the free-energy framework for modelling
perception and learning,” Journal of mathematical psychology, 2015.
[10] K. J. Friston, “The free-energy principle: a uniﬁed brain theory?”
Nature Reviews Neuroscience, vol. 11(2), pp. 27–138, 2010.
[11] K. J. Friston, J. Daunizeau, and S. Kiebel, “Action and behavior: a
free-energy formulation,” Biological cybernetics, vol. 102(3), 2010.
[12] L. Pio-Lopez, A. Nizard, K. Friston, and G. Pezzulo, “Active inference
and robot control: a case study,” Journal of The Royal Society
Interface, vol. 13(122), 2016.
[13] A. C. Mercad ´e, “Robot manipulator control under the active inference
framework,” (Unpublished MSc thesis), TU Delft, 2018.
[14] P. Lanillos and G. Cheng, “Adaptive robot body learning and estima-
tion through predictive coding,” in (IROS), 2018.
[15] P. Lanillos and G.Cheng, “Active inference with function learning
for robot body perception,” in International Workshop on Continual
Unsupervised Sensorimotor Learning (ICDL-Epirob), 2018.
[16] G. Oliver, P. Lanillos, and G. Cheng, “Active inference body percep-
tion and action for humanoid robots,” arXiv:1906.03022v2, 2019.
[17] K. Astrom, “Theory and applications of adaptive control - a survey,”
Automatica, vol. V ol. 19, No. 5, pp. 471–486, 1983.
[18] T. Hsia, “Adaptive control of robot manipulators - a review,” in Proc
of IEEE Int. conf. on robotics and automation (ICRA), 1986.
[19] D. D. Zhang and B. Wei, “A review on model reference adaptive
control of robotic manipulators,” Annual Reviews in Control, vol. 43,
2017.
[20] M. Tarokh, “Hyperstability approach to the synthesis of adaptive
controllers for robot manipulators,” in Proc of IEEE international
conference on robotics and automation (ICRA), 1991.
[21] R. Walters and M. Bayoumi, “Application of a self-tuning pole-
placement regulator to an industrial manipulator,” inProc of 21st IEEE
Conference on Decision and Control, 1991, pp. 323–329.
[22] A. Koivo and T. Guo, “Adaptive linear controller for robotic manip-
ulators,” IEEE Transactions and Automatic Control, vol. AC-28, pp.
162–171, 1983.
[23] D. Lindley, “Bayesian statistics, a review,” SIAM, vol. 2, 1972.
[24] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, and W. Penny,
“Variational free energy and the laplace approximation,” Neuroimage,
vol. 34(1), pp. 220–234, 2007.
[25] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised ﬁlter-
ing,” Mathematical Problems in Engineering, 2010.
[26] K. Friston, “Hierarchical models in the brain,” PLoS computational
biology, vol. 4(11), e1000211, 2008.
[27] K. Friston, J. Daunizeau, and S. Kiebel, “Reinforcement learning or
active inference?” PloS one, vol. 4(7), e6421, 2009.
[28] D. Nguyen-Tuong and J. Peters, “Learning robot dynamics for com-
puted torque control using local gaussian processes regression,” in
Symp. on Learning and Adaptive Behaviors for Robotic Systems, 2008.