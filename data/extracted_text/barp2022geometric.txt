Geometric Methods for Sampling, Optimisation,
Inference and Adaptive Agents
Alessandro Barp1,2,†, Lancelot Da Costa3,4,†, Guilherme França5,†, Karl Friston4,
Mark Girolami1,2, Michael I. Jordan5,6, and Grigorios A. Pavliotis3
1Department of Engineering, University of Cambridge, Cambridge, UK
2The Alan Turing Institute, The British Library, London, UK
3Department of Mathematics, Imperial College London, London, UK
4Wellcome Centre for Human Neuroimaging, University College London, London, UK
5Computer Science Division, University of California, Berkeley, USA
6Department of Statistics, University of California, Berkeley, USA
†Equal contribution
Abstract
In this chapter, we identify fundamental geometric structures that underlie the problems of
sampling, optimisation, inference and adaptive decision-making. Based on this identiﬁcation,
we derive algorithms that exploit these geometric structures to solve these problems eﬃciently.
We show that a wide range of geometric theories emerge naturally in these ﬁelds, ranging
from measure-preserving processes, information divergences, Poisson geometry, and geometric
integration. Speciﬁcally, we explain how(i) leveraging the symplectic geometry of Hamiltonian
systems enable us to construct (accelerated) sampling and optimisation methods,(ii) the theory
of Hilbertian subspaces and Stein operators provides a general methodology to obtain robust
estimators, (iii) preserving the information geometry of decision-making yields adaptive agents
that perform active inference. Throughout, we emphasise the rich connections between these
ﬁelds; e.g., inference draws on sampling and optimisation, and adaptive decision-making assesses
decisions by inferring their counterfactual consequences. Our exposition provides a conceptual
overview of underlying ideas, rather than a technical discussion, which can be found in the
references herein.
Keywords: information geometry; Hamiltonian Monte Carlo; Stein’s method; reproducing kernel;
variational inference; accelerated optimisation; dissipative systems; decision theory; active inference.
arXiv:2203.10592v3  [stat.ML]  25 Jul 2022
Contents
1 Introduction 2
2 Accelerated optimisation 4
2.1 Principle of geometric integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Conservative ﬂows and symplectic integrators . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Rate-matching integrators for smooth optimisation . . . . . . . . . . . . . . . . . . . 7
2.4 Manifold and constrained optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.5 Gradient ﬂow as a high friction limit . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.6 Optimisation on the space of probability measures . . . . . . . . . . . . . . . . . . . 12
3 Hamiltonian-based accelerated sampling 13
3.1 Optimising diﬀusion processes for sampling . . . . . . . . . . . . . . . . . . . . . . . 14
3.2 Hamiltonian Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4 Statistical inference with kernel-based discrepancies 19
4.1 Topological methods for MMDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.2 Smooth measures and KSDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.2.1 The canonical Stein operator and Poincaré duality . . . . . . . . . . . . . . . 21
4.2.2 Kernel Stein discrepancies and score matching . . . . . . . . . . . . . . . . . . 23
4.3 Information geometry of MMDs and natural gradient descent . . . . . . . . . . . . . 24
4.3.1 Minimum Stein discrepancy estimators . . . . . . . . . . . . . . . . . . . . . . 24
4.3.2 Likelihood-free inference with generative models . . . . . . . . . . . . . . . . . 25
5 Adaptive agents through active inference 25
5.1 Modelling adaptive decision-making . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.1.1 Behaviour, agents and environments . . . . . . . . . . . . . . . . . . . . . . . 26
5.1.2 Decision-making in precise agents . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.1.3 The information geometry of decision-making . . . . . . . . . . . . . . . . . . 27
5.2 Realising adaptive agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.2.1 The basic active inference algorithm . . . . . . . . . . . . . . . . . . . . . . . 29
5.2.2 Sequential decision-making under uncertainty . . . . . . . . . . . . . . . . . . 30
5.2.3 World model learning as inference . . . . . . . . . . . . . . . . . . . . . . . . 30
5.2.4 Scaling active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
1 Introduction
Diﬀerential geometry plays a fundamental role in applied mathematics, statistics, and computer
science, including numerical integration [1–5], optimisation [6–11], sampling [12–16], statistics on
spaces with deep learning [17,18], medical imaging and shape methods [19,20], interpolation [21],
and the study of random maps [22], to name a few. Of particular relevance to this chapter is
information geometry, i.e., the diﬀerential geometric treatment of smooth statistical manifolds,
whose origin stems from a seminal article by Rao [23] who introduced the Fisher metric tensor on
parametrised statistical models, and thus a natural Riemannian geometry that was later observed to
correspond to an inﬁnitesimal distance with respect to the Kullback–Leibler (KL) divergence [24].
The geometric study of statistical models has had many successes [25–27], ranging from statistical
inference, where it was used to prove the optimality of the maximum likelihood estimator [28], to
2
the construction of the category of mathematical statistics, generated by Markov morphisms [29,30].
Our goal in this chapter is to discuss the emergence of natural geometries within a few important
areas of statistics and applied mathematics, namelyoptimisation, sampling, inference, andadaptive
agents. We provide a conceptual introduction to the underlying ideas rather than a technical
discussion, highlighting connections with various ﬁelds of mathematics and physics.
The vast majority of statistics and machine learning applications involve solvingoptimisation
problems. Accelerated gradient-based methods [31,32], and several variations thereof, have became
workhorses in these ﬁelds. Recently, there has been great interest in studying such methods from
a continuous-time limiting perspective; see, e.g., [33–40] and references therein. Such methods can
be seen as 1st order integrators to a classicalHamiltonian system with dissipation. This raises the
question on how to discretise the system such that important properties are preserved, assuming
the system has fast convergence to critical points and desirable stability properties. It has been
known for a long time that the class ofsymplectic integratorsis the preferred choice for simulating
physical systems [1,2,41–48]. These discretisation techniques, designed to preserve the underlying
(symplectic) geometry of Hamiltonian systems, also form the basis ofHamiltonian Monte Carlo
(HMC) (or hybrid Monte Carlo) methods [13,49]. Originally, such a theory of geometric integration
was developed with conservative systems in mind while, in optimisation, the associated system is
naturally a dissipative one. Nevertheless, symplectic integrators were exploited in this context [6–8].
More recently, it has been proved that a generalisation of symplectic integrators to dissipative
Hamiltonian systems is indeed able to preserve rates of convergence and stability [9], which are the
main properties of interest for optimisation. Followup work [10] extended this approach, enabling
optimisation onmanifolds and problems withconstraints. There is also a tight connection between
optimisation on the space of measures and sampling which dates back to [50,51]; we will revisit
these ideas in relation to dissipative Hamiltonian systems.
Sampling methods are critical to the eﬃcient implementation of many methodologies. Most
modern samplers are based on Markov Chain Monte Carlo methods, which include slice sam-
plers [52,53], piecewise-deterministic Markov chains, such as bouncy particle and zig-zag sam-
plers [54–59], Langevin algorithms [60–62], interacting particle systems [63] and the class of HMC
methods [12–14,49,64,65]. The original HMC algorithm was introduced in physics to sample dis-
tributions on gauge groups for lattice quantum chromodynamics [13]. It combined two approaches
that emerged in previous decades, namely the Metropolis-Hastings algorithm and the Hamiltonian
formulation of molecular dynamics [66–68]. Modern HMC relies heavily on symplectic integrators
to simulate a deterministic dynamic, responsible for generating distant moves between samples and
thus reduce their correlation, while at the same time preserving important geometric properties.
This deterministic step is then usually combined with a corrective step (originally a Metropolis-
Hastings acceptance step) to ensure preservation of the correct target, and with a stochastic process,
employed to speed up convergence to the target distribution. We will ﬁrst focus on the geometry
of measure-preserving diﬀusions, which emerges from ideas formulated by Poincaré and Volterra,
and form the building block of many samplers. In particular, we will discuss ways to “acceler-
ate” sampling using irreversibility and hypoellipticity. We will then introduce HMC focusing on
its underlying Poisson geometry, the important role played by symmetries, and its connection to
geometric integration.
We then discuss the problem ofstatistical inference, whose practical implementation usually
relies upon sampling and optimisation. Given observations from a target distribution, many esti-
mators belong to the family of the so-calledM and Z estimators [69], which are obtained by ﬁnding
the parameters that maximises (or are zeros of) a parametrised set of functions. These include
the maximum likelihood and minimum Hyvärinen score matching estimators [70,71], which are
also particular instances of the minimum score estimators induced by scoring rules that quantify
3
the discrepancy between a sample and a distribution [72]. The Monge–Kantorovich transportation
problem [73] motivates another important class of estimators, namely the minimum Kantorovich
and p-Wasserstein estimators, whose implementation use the Sinkhorn discrepancy [74–76]. Our
discussion of inference builds upon the theory of Hilbertian subspaces and, in particular, reproduc-
ing kernels. These inference schemes rely on the continuity of linear functionals, such as probability
and Schwartz distributions, over a class of functions to geometrise the analysis of integral probabil-
ity metrics which measure the worse case integration error. We shall explain how maximum mean,
kernelised, and score matching discrepancies arise naturally from topological considerations.
Models ofadaptive agentsare the basis of algorithmic-decision-making under uncertainty. This
is a diﬃcult problem that spans multiple disciplines such as statistical decision theory [77], game
theory [78], control theory [79], reinforcement learning [80], and active inference [81]. To illustrate a
generic use case for the previous methodologies we consider active inference, a unifying formulation
of behaviour—subsuming perception, planning and learning—as a process of inference [81–84]. We
describedecision-makingunderactiveinferenceusinginformationgeometry, revealingseveralspecial
cases that are established notions in statistics, cognitive science and engineering. We then show how
preserving this information geometry in algorithms enables adaptive algorithmic decision-making,
endowing robots and artiﬁcial agents with useful capabilities, including robustness, generalisation
and context-sensitivity [85,86]. Active inference is an interesting use case because it has yet to be
scaled—to tackle high dimensional problems—to the same extent as established approaches, such
as reinforcement learning [87]; however, numerical analyses generally show that active inference
performs at least as well in simple environments [88–94], and better in environments featuring
volatility, ambiguity and context switches [91,92].
2 Accelerated optimisation
We shall be concerned with the problem of optimisation of a functionV : M→ R, i.e., ﬁnding
a point that maximisesV(q), or minimises −V(q), over a smooth manifoldM. We will assume
this function is diﬀerentiable to construct algorithms that rely on the ﬂows of smooth vector ﬁelds
guided by the derivatives ofV(q).
Many algorithms in optimisation are given as a sequence of ﬁnite diﬀerences, represented by
iterations of a mappingΨδt : M→M , where δt >0 is a step size. The analysis of such ﬁnite
diﬀerence iterations is usually challenging, relying on painstaking algebra to obtain theoretical gua-
rantees; such as convergence to a critical point, stability, and rates of convergence to a critical point.
Even when these algorithms are seen as discretisations of a continuum system, whose behaviour is
presumably understood, it is well-known that most discretisations break important properties of
the system.
2.1 Principle of geometric integration
Fortunately, here comes into play one of the most fundamental ideas ofgeometric integration: many
numerical integrators are very close—exponentially in the step size—to a smooth dynamics gener-
ated by ashadow vector ﬁeld(a perturbation of the original vector ﬁeld). This allows us to analyse
the discrete trajectory implemented by the algorithm using powerful tools from dynamical systems
and diﬀerential geometry, which are a priori reserved to smooth systems. Crucially, while numer-
ical integrators typically diverge signiﬁcantly from the dynamics they aim to simulate,geometric
integrators respect the main properties of the system. In the context of optimisation this means re-
specting stability and rates of convergence. This was ﬁrst demonstrated in [9] and further extended
in [10]; our following discussion will be based on these works.
4
The simplest way to construct numerical methods to simulate the ﬂow of a vector ﬁeldX arises
when it is given by a sum,X = Y + Z, and the ﬂows of the individual vector ﬁeldsY and Z
are—analytically or numerically—tractable. In such a case, we can approximate the exact ﬂow
ΦX
δt = eδtX, for step sizeδt> 0, by composing the individual ﬂowsΦY
δt = eδtY and ΦZ
δt = eδtZ. The
simplest composition is given byΨX
δt ≡ΦY
δt ◦ΦZ
δt. The Baker–Campbell–Hausdorﬀ (BCH) formula
then yields
eδtY ◦eδtZ = eδt˜X, ˜X = (Y + Z) + 1
2[Y,Z]δt+ 1
12 ([Y,[Y,Z]] −[Z,[Y,Z]]) δt2 + ··· , (1)
where [Y,Z] = YZ −ZY is the commutator betweenY and Z. Thus, the numerical method itself
can be seen as a smooth dynamical system with ﬂow mapΨX
δt = eδt˜X. The goal of geometric
integration is to construct numerical methods for which˜X shares withX the critical properties of
interest; this is usually done by requiring preservation of some geometric structure.
Recall that a numerical mapΨX
δt is said to be of orderr≥1 if
⏐⏐ΨX
δt −ΦX
δt
⏐⏐= O(δtr+1); we abuse
notation slightly and let|·| denote a well-deﬁned distance over manifolds (see [95] for details).
Thus, the expansion (1) also shows that the error in the approximation is
⏐⏐ΨX
δt −ΦX
δt
⏐⏐= O(δt2), i.e.,
we have an integrator of orderr= 1. One can also consider more elaborate compositions, such as
ΨX
δt ≡ΦY
δt/2 ◦ΦZ
δt ◦ΦY
δt/2, (2)
which is more accurate since the ﬁrst term in (1) cancels out, yielding an integrator of orderr= 2.1
2.2 Conservative ﬂows and symplectic integrators
As a stepping stone, we ﬁrst discuss the construction of suitableconservative ﬂows, namely ﬂows
along which some functionf : X →R is constant, where X is the phase space manifold of the
system, i.e., the space in which the dynamics evolves. Such ﬂows, which are amongst the most
well-studied due to their importance in physics, will enable us to obtain our desired “rate-matching”
optimisation methods and will also be central in our construction of geometric samplers.
To construct vector ﬁelds along the derivative off we shall needbrackets. Geometrically, these
are morphismsX∗→X, also known as contravariant tensors of rank2 in physics, whereX∗is the
dual space ofX. Note that on Riemannian manifolds (e.g.,X= Rn) both spaces are isomorphic. In
Euclidean space,x∈X = Rn, we deﬁne suchB-vector ﬁelds in terms of a state-dependent matrix
B = B(x) as2
XB
f (x) ≡Bij(x)∂if(x)∂j. (3)
Any vector ﬁeld that depends linearly and locally onf may be written in this manner. Notice that a
decomposition f = ∑
afa induces a decompositionXB
f = ∑
aXB
fa that is amenable to the splitting
integrators previously mentioned. Importantly, vector ﬁelds that preservef correspond tobracket
vector ﬁeldsin whichB is antisymmetric [96]. Constructing conservative ﬂows is thus straightfor-
ward. Unfortunately, it is a rather more challenging task to construct eﬃcient discretisations that
retain this property; most well-known procedures, namely discrete-gradient and projection meth-
ods, only give rise to integrators that require solving implicit equations at every step, and they may
break other important properties of the system.
1Higher-order methods are constructed by looking for appropriate compositions that cancel ﬁrst terms in the BCH
formula [43]. However, methods forr >2 tend to be expensive numerically, with not so many beneﬁts (if any) over
methods of orderr= 2.
2We denote byxi the ith component ofx and ∂i ≡∂/∂xi. We also use Einstein’s summation convention, i.e.,
repeated upper and lower indices are summed over.
5
For a particular class of conservative ﬂows, it is possible to construct splitting integrators that—
exactly—preserve another function ˜f that remains close to f. Indeed, going back to the BCH
formula (1), we see that if we were to approximate a conservative ﬂow ofXB
f = XB
f1 + XB
f2 by
composing the ﬂows ofXB
f1 and XB
f2, and, crucially, if we had a bracket for which the commutators
can be written as [
XB
f1,XB
f2
]
= XB
f3,
for some functionf3, and so on for all commutators in (1), then the right-hand side of the BCH
formula would itself be an expansion in terms of a vector ﬁeldXB
˜f for someshadow function ˜f =
f+ f3δt+ f4δt2 + ···. In particular, ˜f would inherit all the properties off, i.e., properties common
to B-vector ﬁelds. This is precisely the case for Poisson brackets, written B ≡ Π, which are
antisymmetric brackets for which the Jacobi identity holds:
[
XΠ
f ,XΠ
g
]
= XΠ
{f,g}, {f,g}≡ ∂ifΠij∂jg, (4)
where {f,g}is the Poisson bracket between functionsf and g. The BCH formula then implies
˜f = (f1 + f2) + 1
2{f1,f2}δt+ 1
12 ({f1,{f1,f2}}+ {f2,{f2,f1}}) δt2 + ··· . (5)
Such an integrator can thus be seen as a Poisson system itself, generated by the above asymptotic
shadow ˜f, which is exactly preserved.
Poisson brackets and their dynamics are the most important class of conservative dynamical
systems, describing many physical systems, including all of ﬂuid and classical mechanics. The
two main classes of Poisson brackets are constant antisymmetric matrices on Euclidean space,
and symplectic brackets for which Πij(x) is invertible at every point x. Its inverse is denoted
by Ωij =
(
Π−1)
ij and called asymplectic form. In this case, the functionf is called a Hamiltonian,
denoted f = H. The invertibility of the Poisson tensorΠij implies that such a bracket exists only
on even-dimensional spaces. Darboux theorem then ensures the existence of local coordinatesx≡
(q1,...,q d,p1,...,p d) in which the symplectic form can be represented asΩ =
( 0 I
−I 0
)
. Dynamically,
this corresponds to the fact that these are 2nd order diﬀerential equations, requiring not only a
position q ∈ Mbut also a momentum p ∈ T∗
qM.3 Note that if H = pi then XH = ∂/∂qi,
and conversely ifH = qi then XH = −∂/∂pi. Thus, a change in coordinate qi is generated by
its conjugate momentumpi, and vice-versa. Thus, the only way to generate dynamics onMin
this case is by introducing a Hamiltonian depending on both position and momentum. From a
numerical viewpoint, the extended phase space introduces extra degrees of freedom that allow us to
incorporate “symmetries” in the Hamiltonian, which facilitate integration. Indeed, in practice, the
Hamiltonian usually decomposes into a potential energy, associated to position and independent of
momentum, and a kinetic energy, associated to momentum and invariant under position changes,
both generating tractable ﬂows. Thanks to this decomposition, we are able to construct numerical
methods through splitting the vector ﬁeld. Note also that, for symplectic brackets, the existence of
a shadow Hamiltonian can be guaranteed beyond the case of splitting methods, e.g., for variational
integrators—which use a discrete version of Hamilton’s principle of least action—and more generally
for most symplectic integrators in which the symplectic bracket is preserved up to topological
considerations described by the 1st de Rham cohomology of phase space.
3More precisely, the dynamics evolve on the cotangent bundleX= T∗M, with coordinatesx= (q,p); momentum
p∈T∗
qMand velocityv = dq/dt ∈TqMare equivalent on the Riemannian manifolds that are used in practice.M
is called the conﬁguration manifold with coordinatesq.
6
2.3 Rate-matching integrators for smooth optimisation
Having obtained a vast family of smooth dynamics and integrators that closely preservef, we can
now apply these ideas to optimisation. Vector ﬁelds for which a Hamiltonian functionf = H
dissipates can be written as a bracket vector ﬁeldXB
H for some negative semi-deﬁnite matrixB [96].
Let us consider a concrete example inX= R2d in the form of a (generalised)conformal Hamiltonian
system [8,9,97]. Consider thus the Hamiltonian
H(q,p) = 1
2pigijpj + V(q), (6)
where gij is a constantsymmetric positive deﬁnitematrix with inversegij. The associated vector
ﬁeld is XB
H = gijpj∂qi −
[
∂qiV + γ(t)pi
]
∂pi, with γ(t) > 0 being a “damping coeﬃcient.” This is
associated to thenegative deﬁnite matrix
B ≡
(0 −I
I 0
)
  
conservative
−γ(t)
(0 0
0 I
)
  
dissipative
. (7)
The equations of motion are
dqi
dt = gijpj, dpi
dt = −∂V
∂qi −γ(t)pi, (8)
and obey
dH
dt = −γ(t)pigijpj ≤0, (9)
so the system isdissipative. Suppose V(q) has a minimizerq⋆ ≡arg minqV(q) in some region of
interest and, without loss of generality, has valueV⋆ ≡V(q⋆) ≡0. Then H >0 and dH/dt <0
outside such a critical point, implying thatH is also a (strict)Lyapunov function; the existence of
such a Lyapunov function implies that trajectories starting in the neighborhood ofq⋆ will converge
to q⋆. In other words, the above system provably solves the optimisation problem
min
q∈Rd
V(q). (10)
Two common choices for the damping are the constant case,γ(t) = γ, and the asymptotic
vanishing case,γ(t) = r/t for some constantr ≥3 (other choices are also possible). WhenV(q) is
a convex function (resp. strongly convex function with parameterµ> 0) it is possible to show the
following convergence rates [37]:
convex µ-strongly convex damping
V(q(t)) −V⋆ O
(
t−1)
O
(
exp
{
−
√
µ/λ2
1(g)t
})
γ(t) = const.
O
(
λ2
1(g)t−2)
O
(
t−2r/3)
γ(t) = r/t
(11)
where λ1(g) is the largest eigenvalue of the metricg. The convergence rates of this system are
therefore known under such convexity assumptions. Ideally, we want to design optimisation methods
that preserve these rates, i.e., are “rate-matching”, and are also numerically stable. As we will see,
such geometric integrators can be constructed by leveraging the shadow Hamiltonian property of
7
symplectic methods on higher-dimensional conservative Hamiltonian systems [9] (see also [98,99]).
This holds not only onR2d but on general settings, namely on arbitrary smooth manifolds [9,10].
In the conformal Hamiltonian case, the dissipation appears explicitly in the equations of motion.
It is however theoretically convenient to consider an equivalentexplicit time-dependentHamiltonian
formulation. Consider the following coordinate transformation into system (8):
p↦→e−η(t)p, H (q,p) ↦→eη(t)H
(
q,e−η(t)p
)
, η (t) ≡
∫
γ(t)dt. (12)
It is easy to see that (8) is equivalent to standard Hamilton’s equations,
dqi
dt = ∂H
∂pi
, dpi
dt = −∂H
∂qi,
with the explicit time-dependent Hamiltonian
H(t,q,p ) = 1
2e−η(t)pigijpj + eη(t)V(q). (13)
The rate of change ofH along the ﬂow now satisﬁes
dH
dt = ∂H
∂t ̸= 0, (14)
so the system isnonconservative; this equation is equivalent to (9).
Going one step further, let us now promotetto a new coordinate and introduce its (conjugate)
momentum u. Consider thus the higher-dimensional Hamiltonian
K(t,q,u,p ) ≡1
2e−η(t)pigijpj + eη(t)V(q) + u. (15)
Note that t and u are two arbitrary canonical coordinates. Denoting the time parameter of this
system bys, Hamilton’s equations read
dt
ds = 1, du
ds = −∂K
∂t , dqi
ds = e−η(t)gijpj, dpi
ds = −eη(t) ∂V
∂qi. (16)
This system isconservative since dK/ds= 0. Now, if we ﬁx coordinates as
t= s, u (s) = −H(s,q(s),p(s)), (17)
the conservative system (16) reduces precisely the original dissipative system (13); the 2nd equation
in (16) reproduces (14), and the remaining equations are equivalent to the equations of motion
associated to (13), which in turn are equivalent to (8) as previously noted. Formally, what we have
done is to embed the original dissipative system with phase spaceR2d into a higher-dimensional
conservative system with phase spaceR2d+2. The dissipative dynamics thus lies on a hypersurface
of constant energy, K = 0 , in high dimensions; see [9] for details. The reason for doing this
procedure, called symplectiﬁcation, is purely theoretical: since the theory of symplectic integrators
only accounts for conservative systems, we can now extend this theory to dissipative settings by
applying a symplectic integrator to (13) and then ﬁxing the relevant coordinates (17) in the resulting
method. Geometrically, this corresponds to integrating the time ﬂow exactly [9,98]. In [9] such a
procedure was deﬁned under the name ofpresymplectic integrators, and these connections hold not
only for the speciﬁc example above but also for general non-conservative Hamiltonian systems.
8
We are now ready to explain why this approach is suitable to construct practical optimisation
methods. Let Ψδs : R2d+2 →R2d+2 be a symplectic integrator of orderr ≥1 applied to system
(15). Denote by(tk,qk,uk,pk) the numerical state, obtained byk= 0,1,... iterations ofΨδs. Time
is simulated over the gridsk = (δs)k, with step sizeδt> 0. Because a symplectic integrator has a
shadow Hamiltonian we have
˜K(tk,qk,uk,pk) = K(t(sk),q(sk),u(sk),p(sk)) + O
(
δsr)
.
Enforcing (17), the coordinatetk becomes simply the time discretizationsk, which is exact, and so
is uk = u(tk) since it is a function of time alone; importantly,udoes not couple to any of the other
degrees of freedom so it is irrelevant whether we have access tou(s) or not. Replacing (15) into the
above equation we conclude:
˜H(tk,qk,pk) = H(tk,q(tk),p(tk)) + O(δtr), (18)
where we now denotetk = (δt)k, fork= 0,1,... . Hence, the time-dependent Hamiltonian also has
a shadow, thanks to the cancellation of the variableu. In particular, if we replace the explicit form
of the Hamiltonian (13) we obtain4
V(qk) −V⋆
  
numerical rate
= V(q(tk)) −V⋆
  
continuum rate
+ O
(
e−η(tk)δtr)
  
small error
. (19)
Therefore, theknownrates(11)forthecontinuumsystemarenearlypreserved—andsowouldbeany
ratesofmoregeneraltime-dependent(dissipative)Hamiltoniansystems. Moreover, asaconsequence
of (18), the original time-independent Hamiltonian (6) of the conformal formulation is also closely
preserved, i.e., within the same bounded error in δt—recall transformation (12). However, this
is also a Lyapunov function, hence the numerical method respects thestability properties of the
original system as well.5
In short, as a consequence of having a shadow Hamiltonian, such geometric integrators are able
to reproduce all the relevant properties of the continuum system. These arguments are completely
general; namely, they ultimately rely on the BCH formula, the existence of bracket vector ﬁelds
and the symplectiﬁcation procedure. Under these basic principles, no discrete-time analyses were
necessarytoobtainguaranteesforthenumericalmethod; whichmaynotbeparticularlyenlightening
from a dynamical systems viewpoint and are only applicable on a (painful) case-by-case basis.
Let us now present an explict algorithm to solve the optimisation problem (10). Consider a
generic (conservative) HamiltonianH(q,p), evolving in times. The well-knownleapfrogor Störmer-
Verlet method, the most used symplectic integrator in the literature, is based on the composition
(2) and reads [2]
pk+1/2 = pk −(δs/2)∂qH(qk,pk+1/2),
qk+1 = qk −(δs/2)
[
∂pH(qk,pk+1/2) + ∂pH(qk+1,pk+1/2)
]
,
pk+1 = pk+1/2 −(δs/2)∂qH(qk+1,pk+1/2).
4The kinetic part only contributes to the small error sinceg is positive deﬁnite and|pk −p(tk)|= O(δtr). There
are several technical details we are omitting, such as Lipschitz conditions on the Hamiltonian and on the numerical
method, which we refer to [9] for details.
5Naturally, all these results hold for suitable choices of step size, which can be determined by a linear stability
analysis of the particular numerical method under consideration.
9
According to our prescription, replacing the higher-dimensional Hamiltonian (15), imposing the
gauge ﬁxing conditions (17), and recalling thatu cancels out, we obtain the following method:6
pk+1/2 = pk −(δt/2)eη(tk)∂qV(qk),
qk+1 = qk −(δt/2)
[
e−η(tk) + e−η(tk+1)]
g−1pk+1/2,
pk+1 = pk+1/2 −(δt/2)eη(tk+1)∂qV(qk+1),
(20)
where we recall that δt > 0 is the step size and tk = ( δt)k, for iterations k = 0 ,1,... . This
method, which is a dissipative generalisation of the leapfrog, was proposed in [9] and has very good
performance when solving unconstrained problems (10). In a similar fashion, one can extend any
(known) symplectic integrator to a dissipative setting; the above method is just one such example.
2.4 Manifold and constrained optimisation
Following [10], we brieﬂy mention how the previous approach can be extended in great generality,
i.e., to an optimisation problem
min
q∈M
V(q), (21)
where Mis an arbitrary Riemannian manifold. There are essentially two ways to solve this problem
through a (dissipative) Hamiltonian approach. One is to is to simulate a Hamiltonian dynamics
on T∗Mby incorporating the metric ofMin the kinetic part of the Hamiltonian. Another is to
consider a Hamiltonian dynamics onRn and embedMinto Rn by imposing several constraints,7
ψa(q) = 0, a = 1,...,m. (22)
This constrained case turns out to be particularly useful since we typically are unable to compute
the geodesic ﬂow onM, but are able to construct robust constrained symplectic integrators for it.
As an example of the ﬁrst approach, considerM= Gbeing aLie group, with Lie algebrag and
generators {Ti}. The analogous of Hamiltonian (13) is given byH = −1
4ge−η(t) Tr(P2) +eη(t)V(Q),
where g >0 is a constant,Q∈G and P ∈g (they can be seen as matrices). The method (20) can
be adapted to this setting, resulting in the following algorithm [10] (recall footnote 6):
Pk+1/2 = e−∆ηk {Pk −(δt/2) Tr [∂QV(Qk) ·Qk ·Pk] Pk},
Qk+1 = Qkexp
[
δtcosh(∆ηk)g−1Pk+1/2
]
,
Pk+1 = e−∆ηkPk+1/2 −(δt/2) Tr
[
∂QV(Qk+1) ·Qk+1 ·Pk+1/2
]
Pk,
(23)
6 In a practical implementation, it is convenient to make the change of variablespk ↦→eη(tk)pk into (20); recall
the transformations (12). In this case the method reads
pk+1/2 = e−∆ηk [pk −(δt/2)∂qV(qk)] ,
qk+1 = qk −δtcosh(∆ηk)g−1pk+1/2,
pk+1 = e−∆ηkpk+1/2 −(δt/2)∂qV(qk+1),
where ∆ηk ≡η(tk+1/2)−η(tk) =
∫ tk+1/2
tk
γ(t)dt. Note that only a half-step diﬀerence ofη(t) appears in these updates.
The algorithm is thus written in the same variables as the conformal representation (8). The advantage is that we do
not have large or small exponentials, which can be problematic numerically. Furthermore, when solving optimisation
problems, it is convenient to set the matrixg= (δt)I; this was noted in [8] but can also be understood from the rates
(11) since then the step sizeδt disappears from some of these formulas.
7Theoretically, there is no loss of generality since Nash or Whitney embedding theorems tells us that any smooth
manifold Mcan be embedded intoRn for suﬃciently largen.
10
where
(
∂QV(Q)
)
ij = ∂V/∂Qji is a matrix.
As an example of the second approach, one can constrain the integrator onRn to deﬁne a
symplectic integrator on Mvia the discrete constrained variational approach [5] by introducing
Lagrange multipliers, i.e., by considering the HamiltonianH + eη(t) ∑
aλaψa(q), where H is the
Hamiltonian (13). In particular, the method (20) can be constrained to yield [10]
pk+1/2 = e−∆ηkΛg(qk) [pk −(δt/2)∂qV(qk)] ,
¯pk+1/2 = pk+1/2 −(δt/2)e−∆ηk [∂qψ(qk)]⊤λ,
qk+1 = qk −δtcosh(∆ηk)g−1 ¯pk+1/2,
0 = ψa(qk+1) ( a= 1,...,m ),
pk+1 = Λg(qk+1)
[
e−∆ηk¯pk+1/2 −(δt/2)∂qV(qk+1)
]
,
(24)
where we have the projectorΛg(q) ≡I −R−1
g (q)∂qψ(q)g−1 with Rg(q) ≡∂qψ(q)g−1∂qψ(q)⊤, and
(∂qψ)ij ≡∂ψi/∂qj is the Jacobian matrix of the constraints;λ = ( λ1,...,λ m)⊤ is the vector of
Lagrange multipliers and∆ηk ≡
∫tk+1/2
tk γ(t)dtaccounts for the damping. In practice, the Lagrange
multipliers are determined by solving the (nonlinear) algebraic equations for the constraints, i.e.,
the 2nd to 4th updates above are solved simultaneously. The above method consists in a dissipative
generalisation of the well-known RATTLE integrator from molecular dynamics [100–103].
It is possible to generalise any other (conservative) symplectic method to this (dissipative)
optimisation setting on manifolds. In this general setting, there still exists a shadow Hamiltonian
so that convergence rates and stability are closely preserved numerically [10] (similarly to (18) and
(19)). In particular, one can also consider diﬀerent types of kinetic energy, beyond the quadratic
case discussed above, which may perform better in speciﬁc problems [8]. This approach therefore
allows one to adapt existing symplectic integrators to solve optimisation problems on Lie groups
and other manifolds commonly appearing in machine learning, such as Stiefel, Grassmanians, or to
solve constrained optimisation problems onRn.
2.5 Gradient ﬂow as a high friction limit
Let us provide some intuition why simulating 2nd order systems is expected to yield faster algo-
rithms. It has been shown that several other accelerated optimisation methods8 are also discretisa-
tions of system (8) [38]. Moreover, in the large friction limit,γ →∞, this system reduces to the 1st
order gradient ﬂow,dq/dt= −∂qV(q) (assuming g∝I), which is the continuum limit of standard,
i.e., nonaccelerated methods [38]. The same happens in more general settings; when the damping
is too strong, the second derivative becomes negligible and the dynamics is approximately 1st order.
As an illustration, consider Figure 1 (left) where a particle immersed in a ﬂuid falls under the
inﬂuence of a potential force−∂qV(q), that plays the role of “gravity”, and is constrained to move
on a surface. In the underdamped case, the particle is under water, which is not so viscous, so
it has acceleration and moves fast (even oscillate). In theoverdamped case, the particle is in a
highly viscous ﬂuid, such as honey, and the drag force−γp is comparable or stronger to−∂qV(q),
thus the particle moves slowly since it cannot accelerate; during the same elapsed timeδt, an
accelerated particle would travel a longer distance. We can indeed verify this behaviour numerically.
In Figure 1 (right) we run algorithm (23) in the underdamped and overdamped regimes when solving
8Besides accelerated gradient based methods, accelerated extensions of important proximal-based methods such
as proximal point, proximal-gradient, alternating direction method of multipliers (ADMM), Douglas-Rachford, Tseng
splitting, etc., are implicit discretizations of (8); see [38] for details.
11
 
Riemannian Gradient Descent
Lie Group(overdamped)
Lie Group(underdamped)
0 20 40 60 80 100
10-5
10-4
0.001
0.010
0.100
1
10
k
V-V *
γ →∞ γ = O(1)
overdamped (honey)
1st order
underdamped (water)
2nd order
δt
δt
−∂qV
↓
−γp↑
Figure 1: Why simulating 2nd order systems yields accelerated methods. Left:Constrained particle falling
in ﬂuids of diﬀerent viscosity. When the drag force is strong the particle cannot accelerate and has a 1st
order dynamics (see text).Right: Simulation of algorithm (23) whereV(Q) is the energy of a spherical spin
glass (Lie groupSO(n), withn= 500) [10]. In the overdamped regime the method is close to Riemannian
gradient descent [104], which is a 1st order dynamics; (23) is much faster in the underdamped regime.
an optimisation problem on then-sphere, i.e., on the Lie groupSO(n).9 We can see that, in the
overdamped regime, this method has essentially the same dynamics as the Riemannian gradient
descent [104], which is nonaccelerated and corresponds to a 1st order dynamics; all methods use the
same step size, only the damping coeﬃcient is changed.
2.6 Optimisation on the space of probability measures
There is a tight connection between sampling and optimisation on the space of probability measures
which goes back to [50,51]. Let P2(Rn) be the space of probability measures onRn with ﬁnite
second moments, endowed with a Wasserstein-2 metricW2. The gradient ﬂow of a functionalF[µ]
on the space of probability measures is the solution to the partial diﬀerential equation∂tµ(q,t) =
−∇W2F[µ(q,t)], which, under suﬃcient regularity conditions, is equivalent to [50,51,105]
∂tµ= ∂·
(
µ∂δF[µ]
δµ
)
, (25)
where ∂ ≡∂q and ∂·are the derivative and the divergence operators onRn, respectively. The
evolution of this system solves the optimisation problem
ρ≡arg min
µ∈P2(Rn)
F[µ], (26)
i.e., µ(q,t) →ρ(q) as t→∞ in the sense of distributions. We can consider the analogous situation
with a dissipative ﬂow induced by the conformal Hamiltonian tensor (7) on the space of probability
measures; we set g = I and γ(t) = γ = const. for simplicity. Thus, instead of (25), we have a
conformal Hamiltonian system in Wasserstein spacegiven by the continuity equation
∂tµ= ∂·
(
µB∂δF[µ]
δµ
)
, (27)
9The details are not important here, but this problem minimises the Hamiltonian of a spherical spin glass (see [10]
for details). The same behaviour is seen with the constrained method (24) as well.
12
where now∂ ≡(∂q,∂p) and µ is a measure overP2(R2d). Let F be thefree energydeﬁned as
F[µ] ≡U[µ] −β−1 S[µ], U [µ] ≡Eµ[H], S[µ] ≡Eµ[−log µ], (28)
where U is the (internal) energy,H is the Hamiltonian (6),S is the Shannon entropy, andβ is the
inverse temperature. The functional derivative of the free energy equals
δF
δµ = H+ β−1 log µ= 1
2∥p∥2 + V(q) + β−1 log µ. (29)
In particular, the minimiser ofF is the stationary density
ρ(q,p) = Z−1
β e−βH(q,p), Z β ≡Eµ
[
e−βH(q,p)]
. (30)
Note also that the free energy (28) is nothing but theKL divergence (up to a constant which is the
partition function):
KL[µ|ρ] ≡Eµ[log(µ/ρ)] = βF[µ] −log Zβ.
Therefore, the evolution ofµ as given by the conformal Hamiltonian system (27) minimises the
divergence from the the stationary density (30). Replacing (29) into (27) we obtain
∂tµ= −∂q ·[µp] + ∂p ·[µ∂qV(q) + γµp] + γβ−1∂2
pµ,
which is nothing but the Fokker-Planck equation associated to theunderdampedLangevin diﬀusion
dqt = ptdt, dp t = −∂qV(qt)dt−γptdt+
√
2γβ−1dwt, (31)
where wt is a standard Wiener process. Thus, the underdamped Langevin can be seen as performing
acceleratedoptimisation on the space of probability measures. A quantitative study of its speed of
convergence is given by the theory of hypocoercivity [106–108].
The above results provide a tight connection between sampling and optimisation. Interestingly,
by the same argument as used in section 2.5 (see Figure 1), the high friction limit,γ →∞, of the
underdamped Langevin diﬀusion (31) yields theoverdamped Langevin diﬀusion[38,107]
dqt = −∇V(qt)dt+
√
2β−1dwt, (32)
which corresponds precisely to the gradient ﬂow (25) on the free energy functionalF[µ] [51,105],
where nowµ= µ(q,t) ∈P2(Rd). Thus, in the same manner that a 2nd order damped Hamiltonian
system may achieve accelerated optimisation compared to a 1st order gradient ﬂow, the under-
damped Langevin diﬀusion (31) may achieve accelerated sampling compared to the overdamped
Langevin diﬀusion (32). Such an acceleration has indeed been demonstrated [109] in continuous-
time and for a particular discretisation.
3 Hamiltonian-based accelerated sampling
The purpose of sampling methods is to eﬃciently draw samples from a given target distributionρ
or, more commonly, to calculate expectations with respect toρ:
∫
X
f dρ≈1
n
n−1∑
k=0
f(xk) . (33)
13
However, generating i.i.d. samples {xk}is usually practically infeasible, even for ﬁnite sample
spaces, as in high dimensions probability mass tends to concentrate in small regions of the sample
space, while regions of high probability mass tend to be separated by large regions of negligible
probability. Moreover,ρis usually only known up to a normalisation constant [110]. To circumvent
this issue, MCMC methods rely on constructing ergodic Markov chains {xn}n∈N that preserve
the target distributionρ. If we run the chain long enough (n →∞), Birkhoﬀ’s ergodic theorem
guarantees that the estimator on the right-hand side of (33) converges to our target integral on
the left-hand side almost surely [111]. An eﬃcient sampling scheme is one that minimises the
variance of the MCMC estimator. In other words, fewer samples will be needed to obtain a good
estimate. Intuitively, good samplers are Markov chains that converge as fast as possible to the
target distribution.
3.1 Optimising diﬀusion processes for sampling
As many MCMC methods are based on discretising continuous-time stochastic processes, the anal-
ysis of continuous-time processes is informative of the properties of eﬃcient samplers.
Diﬀusion processes possess a rich geometric theory, extending that of vector ﬁelds, and have
been widely studied in the context of sampling. They are Markov processes featuring almost surely
continuous sample paths (i.e., no jumps) and correspond to the solutions of stochastic diﬀerential
equations (SDEs). While a deterministic ﬂow is given by a ﬁrst order diﬀerential operator—namely,
a vector ﬁeld X as used in §2—diﬀusions require specifying a set of vector ﬁeldsX,Y1,...,Y N,
where X represents the (deterministic) drift andYi the directions of the (random) Wiener processes
wi
t, and are characterised by a second order diﬀerential operator of the formL≡ X+ Yi◦Yi, known
as the generator of the process. Equivalently, diﬀusions can be written as Stratonovich SDEs:
dxt = X(xt)dt+ Yi(xt) ◦dwi
t.
For a smooth positive target measureρ, thecomplete family ofρ-preserving diﬀusions is given
by (up to a topological obstruction contribution) [112]
dxt = curlρ(A)dt+ 1
2divρ(Yi)Yidt+ Yi ◦dwi
t, (34)
for a choice of antisymmetric bracketA. Here curlρ is a diﬀerential operator on multi-vector ﬁelds,
generalising the divergence on vector ﬁeldsdivρ ofρ, and is induced via an isomorphismρ♯ deﬁned by
ρ which allows to transfer the calculus of twisted diﬀerential forms to ameasure-informed calculus
on multi-vector ﬁelds [112]. The ergodicity of (34) is essentially characterised by Hörmander’s
hypoellipticity condition; i.e., whether the Lie algebra of vector ﬁelds generated by{Yi,[X,Yi]}N
i=1
spans the tangent spaces at every point [107,113,114]. On Euclidean space the above complete class
of measure preserving diﬀusions can be given succinctly by Itô SDEs [115]:
dxt = −(A+ S) (xt) ∂V (xt) dt+ ∂·(A+ S) (xt) dt+
√
2S(xt)dwt, (35)
where S,A reduce to symmetric and antisymmetric matrix ﬁelds andV is the negative Lebesgue
log-density ofρ.
There are two well-studied criteria describing sampling eﬃciency in Markov processes: 1) the
worst-case asymptotic variance of the MCMC estimator (33) over functions inL2(ρ), and 2) the
spectral gap. The spectral gap is the lowest non-zero eigenvalue of the (negative) generator−L
on L2(ρ). When it exists, it is an exponential convergence rate of the density of the process to
the target density [107,116,117]. Together, these criteria yield conﬁdence intervals on the non-
asymptotic variance of the MCMC estimator, which determines sampling performance [118].
14
A fundamental criterion for eﬃcient sampling is non-reversibility [116,119,120]. A process is
non-reversible if it is statistically distinguishable from its time-reversal when initialised at the target
distribution [107]. Measure-preserving diﬀusions are non-reversible precisely whenA̸≡0 [121]. In-
tuitively, non-reversible processes backtrack less often and thus furnish more diverse samples [122].
Furthermore, non-reversibility leads to mixing, which accelerates convergence to the target mea-
sure. It is well known that removing non-reversibility worsens the spectral gap and the asymptotic
variance of the MCMC estimator [116,119,120]. In diﬀusions with linear coeﬃcients, one can con-
struct the optimal non-reversible matrixAto optimise the spectral gap [123,124] or the asymptotic
variance [120]. However, there are no generic guidelines on how to optimise non-reversibility in
arbitrary diﬀusions. This suggests a two-step strategy to construct eﬃcient samplers: 1) optimise
reversible diﬀusions, and 2) add a non-reversible perturbationA̸≡0 [125].
Diﬀusions on manifolds are reversible whenA≡0, and thus have the formdxt = 1
2 divρ(Yi)Yidt+
Yi ◦dwi
t, which on Euclidean space reads
dxt = −S(xt) ∂V (xt) dt+ ∂·S(xt) dt+
√
2S(xt)dwt. (36)
The spectral gap and the asymptotic variance of the MCMC estimator are the same optimality
criteria in reversible Markov processes [126]. WhenS is positive deﬁnite everywhere, it deﬁnes a
Riemannian metricg on the state space. The generator is then the elliptic diﬀerential operator
L= ∇g + ∆g, (37)
where ∇g is the Riemannian gradient and∆g is the Laplace-Beltrami operator, i.e., the Riemannian
counterpart of the Laplace operator. Thus, reversible (elliptic) diﬀusions (37) are the natural gen-
eralisation of the overdamped Langevin dynamics (32) to Riemannian manifolds [127]. Optimising
S to improve sampling amounts to endowing the state space with a suitable Riemannian geometry
that exploits the structure of the target density. For example, sampling is improved by directing
noise along vector ﬁelds that preserve the target density [128]. When the potentialV is strongly
convex, the optimal Riemannian geometry is given byg ≡∂2V [129,130]. Sampling can also be
improved in hypoelliptic diﬀusions with degenerate noise (i.e., whenS is not positive deﬁnite). In-
tuitively, the absence of noise in some directions of space leads the process to backtrack less often
and thus yield more diverse samples. For instance, in the linear case, the optimal spectral gap is
attained for an irreversible diﬀusion with degenerate noise [131]. However, degenerate diﬀusions can
be very slow to start with, as the absence of noise in some directions of space make it more diﬃcult
for the process to explore the state space [131].
Underdamped Langevin dynamics (31) combines all the desirable properties of an eﬃcient sam-
pler: it is irreversible, has degenerate noise, and achieves accelerated convergence to the target
density [132]. We can optimise the reversible part of the dynamics (i.e., the frictionγ) to improve
the asymptotic variance of the MCMC estimator [133]. Lastly, we can signiﬁcantly improve under-
damped Langevin dynamics by adding additional non-reversible perturbations to the drift [134].
One way to obtain MCMC algorithms is to numerically integrate diﬀusion processes. As vir-
tually all non-linear diﬀusion processes cannot be simulated exactly, we ultimately need to study
the performance of discrete algorithms instead of their continuous counterparts. Alarmingly, many
properties of diﬀusions can be lost in numerical integration. For example, numerical integration
can aﬀect ergodicity [135]. An irreversible diﬀusion may sample more poorly than its reversible
counterpart after integration [136]. This may be because numerical discretisation can introduce, or
otherwise change, the amount of non-reversibility [136]. The invariant measure of the diﬀusion and
its numerical integration may diﬀer, a feature known as bias. We may observe very large bias even
in the simplest schemes, such as the Euler-Maruyama integration of overdamped Langevin [60].
15
Luckily, there are schemes whose bias can be controlled by the integration step size [137]; yet, this
precludes using large step sizes. Alternatively, one can remove bias by supplementing the integra-
tion step with a Metropolis-Hastings corrective step; however, this makes the resulting algorithm
reversible. In conclusion, designing eﬃcient sampling algorithms with strong theoretical guarantees
is a non-trivial problem that needs to be addressed in its own right.
3.2 Hamiltonian Monte Carlo
Constructing measure-preserving processes, in particular diﬀusions, is relatively straightforward.
A much more challenging task consists of constructing eﬃcientsampling algorithms with strong
theoretical guarantees. We now discuss an important family of well-studied methods, known as
Hamiltonian Monte Carlo (HMC), which can be implemented on any manifold, for any smooth
fully supported target measure that is known up to a normalising constant. Some of these methods
can be seen as an appropriate geometric integration of the underdamped Langevin diﬀusion, but it
is in general simpler to view them as combining a geometrically integrated deterministic dynamics
with a simple stochastic process that ensures ergodicity.
The conservative Hamiltonian systems previously discussed provide a natural candidate for the
deterministic dynamics. Indeed, given a target measure ρ ∝ e−VµM, with µM a Riemannian
measure (such as the Lebesgue measuredq on M= Rd), if we interpret the negative log-density
V(q) as a potential energy, i.e., a function depending onposition q, one can then plug in the potential
within Newton’s equation to obtain a deterministic proposal that is well-deﬁned on any manifold,
as soon as the acceleration and derivative operators have been replaced by their curved analogues
m¨q= −∂V(q)  
ﬂat Newton
−→
acceleration
∇˙q
dt =
direction of
greatest decrease
  
−∇V(q)
  
Riemannian Newton
, (38)
with given initial conditions for the positionq and velocity v = dq/dt. This is a 2nd order system
which evolves in the tangent bundle,(q,v) ∈ TM, which is TM = Rd ×Rd when M = Rd.
The resulting ﬂow is conservative since it corresponds to a Hamiltonian system as discussed in
section §2.2, with HamiltonianH(q,v) ≡ 1
2 ∥v∥2
q + V(q), where ∥v∥2
q is the Riemannian squared-
norm, whichisvTg(q)vwhenM= Rd andg(q) istheRiemannianmetric; thisisthemanifoldversion
of the Hamiltonian (6). This system preserves the symplectic measureµΩ(q,v) = det g(q)dqdv, and
thus also thecanonical distributionµ∝e−H(q,v)µΩ, which is the product of the target distribution
over position with the Gaussian measures on velocity (with covarianceg). For instance, onM= Rd,
µ(q,v) ∝ρ(q) ×N
(
0,g−1(q)
)
(v) ∝e−V(q)√
det g(q)dq×
√
det g(q)e−1
2 vTg(q)vdv.
Moreover, the pushforward under the projection Proj: (q,v) ↦→q is precisely the target measure:
Proj∗µ= ρ. Concretely, the samples generated by moving according to Newton’s law, after ignoring
their velocity component, haveρ as their law. The main critical features and advantages in using
Hamiltonian systems arises from their numerical implementation. Indeed, the ﬂow of (38) is only
tractable for the simplest target measures, namely those possessing a high degree of symmetry. In
order to proceed, we must devise suitable numerical approximations which, unfortunately, not only
break such symmetries but may lose key properties of the dynamics such as stationarity (typically
not retainedby discretisations). However, as we sawin section §2.2, mostsymplectic integratorshave
a shadow Hamiltonianand thus generate discrete trajectories that are close to the associated bona
ﬁde (shadow) Hamiltonian dynamics, that in particular preserve the shadow canonical distribution.
16
Most integrators used in sampling, such as the leapfrog, aregeodesic integrators. These are
splitting methods(seesection§2.1)obtainedbysplittingtheHamiltonian H(q,v) = H1(q,v)+H2(q),
where H1(q,v) = 1
2 ∥v∥2
q and H2(q) = V(q) are to be treated as independent Hamiltonians in their
own right. Both of these Hamiltonians generate dynamics that might be tractable: the Riemannian
component H1, associated to the Riemannian reference measure, induces the geodesic ﬂow, while
the target density componentH2 gives rise to a vertical gradient ﬂow, wherein the velocity is shifted
by the direction of maximal density change, i.e.,(q,v) ↦→(q,v −δt∇qV(q)). The Jacobi identity and
the BCH formula imply these integrators do possess a shadow Hamiltonian˜H, and reproduce its
dynamics. Such a shadow can in fact be explicitly obtained from (5) by computing iterated Poisson
brackets; e.g., onM= Rd and forH(q,v) = (1/2)vTgv+ V(q), the three-stage geodesic integrator
ΦH1
bδt ◦ΦH2
aδt ◦ΦH1
(1−1
2 b)δt ◦ΦH2
(1−2a)δt ◦ΦH1
(1−1
2 b)δt ◦ΦH2
aδt ◦ΦH1
bδt, with parametersa,b ∈R, yields [138]
˜H(q,v) = H(q,v) + δt2 [
c1∂V(q)Tg−1∂V(q) + c2vT∂2V(q)v
]
+ O(δt4),
for some constantsc1 and c2. As an immediate consequence, these symplectic integrators preserve
the reference symplectic measureµΩ and can be used as a (deterministic) Markov proposal, which
when combined with the Metropolis-Hastings acceptance step that depends only on the target den-
sity, gives rise to a measure-preserving process. Moreover, the existence of the shadow Hamiltonian
ensures that the acceptance rate will remain high for distant proposals, allowing small correlations.
However, since Hamiltonian ﬂows are conservative, they remain stuck within energy level sets, which
prevents ergodicity. It is thus necessary to introduce another measure-preserving process, known
as the heat bath or thermostat, that explores diﬀerent energy levels; the simplest such process cor-
responds to sampling a velocity from a Gaussian distribution. Bringing these ingredients together,
we thus have the following HMC algorithm: givenzn = (q,v), computezn+1 according to
1. Heat bath:sample a velocity according to a Gaussian,v†∼N(0,g−1(q)).
2. Shadow Hamiltonian dynamics: move along the Hamiltonian ﬂow generated by the geodesic
integrator, z∗= Ψδt(z†), wherez†= (q,v†).
3. Metropolis correction:accept z∗with probabilitymin
{
1,e−∆H}
, where∆H = H(z⋆)−H(z†).
If accepted then setzn+1 = z∗, otherwise setzn+1 = (q,−v†).
The above rudimentary HMC method (originally known as Hybrid Monte Carlo) was proposed for
simulations in lattice quantum chromodynamics withMbeing the special unitary group,SU(n),
and used a Hamiltonian dynamics ingeniously constructed from the Maurer-Cartan frame to com-
pute the partition function of discretised gauge theories [13]. This method has later been applied
in molecular dynamics and statistics [12,49,64,139,140].
While the above discussion provides a justiﬁcation for the use of Hamiltonian mechanics, a more
constructive argument from ﬁrst principles can also be given. From the family of measure-preserving
dynamics, which as we have seen can be written ascurlµ(A) (recall (34)), we want to identify those
suited to practical implementations (hereµ could be any distribution on some spaceFhaving the
target ρ has a marginal). Only for the simplest distributionsµ we can hope to ﬁnd bracketsA for
which the ﬂow ofcurlµ(A) is tractable. Instead, the standard approach to geometrically integrate
this ﬂow relies as before on splitting methods, which eﬀectively decomposeµ ∝e−∑HℓµF into
simpler components by decomposing the reference measure from the density and taking advantage
of any product structure of the density, so thatcurlµ(A) = curlµF(A) + ∑
ℓXA
Hℓ.
There are three critical properties underpinning the success of HMC in practice. The ﬁrst two
are the preservation of the reference measure and the existence of a conserved shadow Hamiltonian
17
for the numerical method. These imply that we remain close to preservingµ along the ﬂow, and
in particular leads to Jacobian-free Metropolis corrections with good acceptance rates for distant
proposals (see [141] for examples of schemes with Jacobian corrections). Achieving these properties
yield strong constraints on the choice ofA [142]; the shadow property is essentially exclusive to
Poisson systems, for which the conservation of a common reference measure is equivalent to the
triviality of the modular class in the ﬁrst Poisson cohomology group [143]. In particular, Poisson
brackets that admit such aninvariant measure have been carefully analysed and are known as
unimodular; the only unimodular Poisson bracket that can be constructed on general manifolds
seems to be precisely the symplectic one.
Thethirdcriticalpropertyistheexistenceofsplittingsmethodsforwhichallthecomposingﬂows
are either tractable or have adequate approximations, namely the geodesic integrators. Indeed, as we
haveseen, theﬂow ΦH2—inducedbythepotential H2(q) = V(q)—isalwaystractable, independently
ofthecomplexityofthetargetdensity; thisispossiblemainlyduetotheextra“symmetries” resulting
from implementing the ﬂow on a higher-dimensional spaceTMrather thanM. On the other hand,
one key consideration for the tractability of the the geodesic ﬂowΦH1
δt —induced by the kinetic
energy H1(q,v)—is the choice of Riemannian metric; for most cases, it is numerically hard to
implement ΦH1
δt since several implicit equations need to be solved. In general, it is desirable to use
a Riemannian metric that reﬂects the intrinsic symmetries of the sample space, mathematically
described by a Lie group action. Indeed, by using aninvariant Riemannian metric, one greatly
simpliﬁes the equations of motion of the geodesic ﬂow, reducing the usual2nd order Euler-Langrange
equations to the 1st order Euler-Arnold equations [144–146], with tractable solutions in many cases
of interest, e.g., for naturally reductive homogeneous spaces; includingRd, the space of positive
deﬁnite matrices, Stiefel manifolds, Grassmannian manifolds, and many Lie groups. In such cases,
it is possible to ﬁnd a Riemannian metric whose geodesic ﬂow is known and given by the Lie group
exponential [16,147,148]. For the other main class of spaces, namely those given by constraints,
if one chooses the restriction of the Euclidean metric, then the RATTLE scheme discussed in
optimisation (see section §2.4) is a suitable symplectic integrator [103,149–152] (perhaps up to a
reversiblity check). Occasionally, it may be suitable to use a Riemannian metric associated to the
target distribution rather than the sample space; e.g., when it belongs to a statistical manifold. In
that case, any choice of (information) divergence gives rise to an information tensor that may be
used in the HMC algorithm. Notably, this is the case in Bayesian statistics, wherein attempting
to ﬁnd a Riemannian metric that locally matches the Hessian of the posterior motivates the use of
the Fisher information tensor summed with the Hessian of the prior, giving rise to the Riemannian
HMC [127,153]. When a Riemannian metric whose geodesic ﬂow is unknown is chosen, one can
use the trick of increasing the dimension of the phase space to add symmetries to derive explicit
symplectic integrators [154,155].
Once we have an integrator for the geodesic ﬂow, another important consideration is the con-
struction and tuning of the overall integrator, i.e., the speciﬁc composition ofΦH1
δt and ΦH2
δt . Tradi-
tional numerical integrators are tuned to provide highly accurate approximations for the trajectories
in the limitδt→0; for instance, a forth order Runge-Kutta method. However, samplers aim to have
the largest possible step sizeδt in order to reduce correlations. One approach consists in tuning the
integrator to obtain good density preservation in the Gaussian case. Another approach consists in
tuning the integrator to ensure the shadow Hamiltonian˜H agrees withH up to the desired order;
see [156–161]. We note that when the target density contains two components, one computationally
expensive and the other computationally cheap, it may be desirable to further split the potential
H2(q) = V(q) to obtain higher acceptance rates [162–164]. In order to achieve ergodicity in HMC
methods, it is usually suﬃcient to randomise the trajectory length of the integrator [165–167]. How-
ever, deriving guarantees on the rate of convergence of HMC is diﬃcult, though recent work have
18
established suﬃcient conditions for geometric ergodicity [15,168].
Let us also brieﬂy mention some useful upgrades that have been proposed in recent years. First,
whenever the Metropolis step rejects the proposed sample, the (expensive) computation of the
numerical trajectory is wasted, and several modiﬁcations have been proposed to address this issue,
for example by granting the method extra integration steps when the proposal is rejected [169,170],
or using a dynamic integration with a termination criterion that aims to ensure the motion is long
enough to avoid random walks, but short enough that we do not waste computational eﬀort, such
as the No-U-Turn sampler [171].
Second, the Metropolis algorithm gives rise to areversible method which, as discussed above,
usually has slower convergence properties. Modern HMC methods bypass this issue by replacing
the heat bath by an Ornstein-Uhlenbeck process, which ensures the overall algorithm isirreversible.
In this case, the overall HMC method can be viewed as a geometric integration of the underdamped
Langevin diﬀusion [106,172,173]. The connection between HMC and Langevin diﬀusion originates
from the desire to replace the Gaussian heat bath with a partial momentum refreshment, yielding
a more accurate simulation of dynamical properties and higher acceptance rates [174].
Third, many modiﬁcations of the rudimentary HMC algorithm only provide improvements when
the acceptance rate is suﬃciently high. A third class of upgrades improves the acceptance rate
by using the fact that the shadow Hamiltonian is exactly preserved by the integrator. These
shadow HMCmethods sample from a biased target distribution, deﬁned by the (truncated) shadow
Hamiltonian, and correct the bias in the samples via an importance sampler [138,175,176].
Finally, the Metropolis step can be replaced with a multinomial correction that uses the entire
numerical trajectory, accepting a given point along it according to the degree by which it distorts the
target measure [64]. Some methods entirely skip the accept/reject step, in particular those relying
on appproximate gradients and surrogates [177,178], such as thestochastic HMC methods; such
methods approximate the potentialV(q) and its derivative when they are given by a sum over data
points, V(q) = ∑
iVi(q), by a cheaper sum over a uniformly sampled minibatches [179] (these are
commonly called stochastic gradients in machine learning). However, this may break the shadow
property and reduce the scalable and robust properties of HMC methods [180].
4 Statistical inference with kernel-based discrepancies
The problem of parameter inference consists of estimating an elementθ∗ ∈Θ using a sequence
of random functions (or estimators)ˆθn : Ω →Θ, with ˆθn determined by a set of measurements
{q1,...,q n}representing the available experimental data. In the statistical context, we search for
the optimal approximationµθ∗ of the target measureρwithin a statistical model{µθ : θ∈Θ}, with
respect to adiscrepancy D : P×P→ [0,∞] over the set of probability measuresP. A common
choice of discrepancy is theKL-divergence, and the resulting inference problem can be implemented
via the asymptotically optimal maximum likelihood estimators [69]. As in many applications we are
interested in computing expectations, a particularly suitable notion of discrepancies are theintegral
probability pseudometrics(IPM) [181], which quantify the worse-case integration error with respect
to a family of functionsF
dF(ρ,µ) ≡sup
f∈F
⏐⏐⏐⏐
∫
fdρ −
∫
fdµ
⏐⏐⏐⏐.
An apparent diﬃculty arises with IPMs in that we need to compute a supremum, which will be
intractable for most choices ofF. Observe, however, that ifFwere the unit ball of a normed vector
space H, and integration with respect toρ and µ was a continuous linear functional onH, then
dF(ρ,µ) would correspond to the distance betweenρ and µ in the dual norm over the dualH∗,
19
i.e., dF(ρ,µ) = ∥ρ−µ∥∗. Conveniently, reproducing kernel Hilbert spaces (RKHS) are precisely
Hilbert spaces over which the Dirac distributionsδx : f ↦→f(x) act continuously [182–184], and,
more generally, the probability distributions that act continuously by integration on a RKHSH
are exactly those for which all elements ofHare integrable [185]. Denoting byPH the set of such
probability measures, so that by deﬁnitionδx ∈PH, we can deﬁne theMaximum Mean Discrepancy
(MMD) as
MMD : PH×PH→[0,∞), MMD[ρ|µ] = ∥ρ−µ∥,
where we further used the Riesz representation isomorphism to viewρ,µ ∈ PH ⊂ H∗ ∼= Has
elements of H. The map PH →H is usually referred to as themean embedding [186,187]. The
angles between the mean embedding of Dirac distributions play a central role in the study of RKHS,
and indeed characterise them. They deﬁne thereproducing kernel
k: M×M→ R, k (x,y) ≡⟨δx,δy⟩,
with ⟨·,·⟩denoting the inner product onH, from which we can obtain a practical expression for the
squared MMD:
MMD2[ρ|µ] =
∫∫
k(x,y)(ρ−µ)(dy)(ρ−µ)(dx).
4.1 Topological methods for MMDs
A key feature of RKHS, as identiﬁed by Laurent Schwartz, is the fact they areHilbertian subspaces,
i.e., Hilbert spaces continuously embedded within a topological vector space T, denoted H ↪→
T [188]. In this context, by composing the transpose of the inclusion H ↪→ Twith the Riesz
isomorphism, we can deﬁne a (generalised) mean embedding as the weakly-continuous positive map
φ: T∗↪→H∗→H.
This mapping allows us to transfer structures betweenHand T∗, an example of which is the MMD,
which is nothing else than the pullback of the Hilbert space metric fromHto T∗. Some important
examples ofT are C0, C∞
c and RM(with their canonical topologies), whose duals are the spaces of
ﬁnite Radon measures, Schwartz distributions, and measures with ﬁnite support, respectively [189].
In particular a RKHS, as deﬁned above, is any Hilbert space satisfyingH↪→RM. More generally,
when T is continuously embedded in the space ofRn-valued functions onM—as in the examples
above, which haven = 1—then Hinherits (and can be characterised in terms of) a reproducing
kernel K : M×M→ Rn×n, deﬁned∀v,u ∈Rn by
v⊤K(x,y)u= δv
x
[
φ(δu
y)
]
,
where δu
x : h ↦→u·h(x); but this need not be the case in general, and we will employ Hilbertian
subspaces with no reproducing kernel to construct the score-matching discrepancy.
This geometric description of RKHS and MMD allows us to swiftly apply topological methods
in their analysis. For example, in order forMMD2 to be a valid notion of statistical divergence, it
should accurately discriminate distinct distributions, in the sense thatMMD[ρ |µ] = 0 iﬀ ρ = µ.
By construction, MMD will becharacteristic to a subset ofT∗, that is be able to distinguish its
elements, iﬀ φ is injective. The Hahn–Banach theorem further shows that this is equivalent to
the denseness of Hin T, reducing the matter to a topological question [186,189,190]. In many
applications, we typically would likeT∗ to be the set of probability measures, but the latter is
not even a vector space. Instead, just as is commonly done to deﬁne (statistical) manifolds, it is
20
desirable to embedPwithin a more structured space, such as the space of ﬁnite Radon measures
C∗
0 . Characteristicness toC∗
0 is also known asuniversality in learning theory, since such RKHS are
dense inL2(µ) for anyµ∈P, which enables the method to learn the target function independently
of the data-generating distribution [191]. However, in many important cases, we are interested in
analysing the denseness ofHin a space other thanC0. For instance, in the case of unbounded
reproducing kernels, we cannot aim to separate all ﬁnite distributions, since the RKHS will contain
unbounded functions and the MMD will only be deﬁned on a subset ofP. In the particular case
of the KSDs discussed below, which are given by transforming a base RKHS into a Stein RKHS
via a diﬀerential operator, the characteristiness of the Stein RKHS to a set of probability measures
is equivalent to the characteristiness of the base RKHS to more general spacesT∗ of Schwartz
distributions [185].
Moreover, the ability of MMD to discriminate distributions is also useful to ensure it further
metrises, or at least controls, weak convergence, and thus provide a suitable quantiﬁcation of the dis-
crepancy between unequal distributions. Indeed, on non-compact locally compact Hausdorﬀ spaces
such asRd, whenH↪→C0, thenMMD will metrise weak convergence (of probability measures) iﬀ
the kernelk is continuous andHis characteristic to the space of ﬁnite Radon measures [192]. The
fact that the RKHS must separate all ﬁnite measures in order to metrise weak convergence results
from the fact that otherwise MMD cannot in general prevent positive measures from degenerating
into the null measure on non-compact spaces, beyond the family of translation-invariant kernels,
for which characteristicness to the sets of probability measures or that of ﬁnite measures are in fact
equivalent [189]. It is also possible to prevent probability mass from escaping to inﬁnity—when the
topology of the sequence of distributions is relatively compact with respect to the weak topology
on the space of distributions—since, in that case, standard topological arguments relate MMD and
weak convergence via characteristicness toP[193]. For example, by Prokhorov’s theorem we may
use the tightness of a sequence of distributions to ensure characteristic MMDs detect any loss of
mass, and thus control weak convergence [194].
4.2 Smooth measures and KSDs
MMD have a computationally tractable expression wheneverρ,µ are discrete measures, or at least
tractable U-statistics when their samples are readily available. Many applications involve distribu-
tions that are smooth and fully supported, but hard to sample from. Recalling the deﬁnition of
dF(ρ,µ), it would be useful to construct a MMD for which the setFconsists of functions whose
integral underµ is tractable, for example equal to zero; the MMD would then reduce to a double
integration with respect toρ. To achieve this, we will leverage ideas fromStein’s method[195,196],
and apply Stein operators to a given RKHS so as to construct aStein RKHS whose elements have
vanishing expectation under a distribution of interest.
4.2.1 The canonical Stein operator and Poincaré duality
To gain intuition on Stein operators, we begin by considering the integral with respect toµ as a
linear operator on test functions,µ : C∞
c (M) →R, with µf ≡
∫
fdµ, and we are interested in
generating test functions in the kernel of this operator (i.e., with vanishing expectations). There
are two fundamental theorems that help us understand the integral-diﬀerential geometry of the
manifold: de Rham’s theorem and Poincaré duality. The former relates the topology of the manifold
to information on the solutions of diﬀerential equations deﬁned over the manifold [197]. The latter
(which contains the fundamental theorem of calculus) describes the properties of the integral pairing
(α,β) ↦→
∫
α∧β of diﬀerential forms, which include the pairing of test functions with smooth
21
measures (f,µ) ↦→
∫
fdµ. While these results are canonical statements about the manifold, we can
turn them into measure-theoretic statements by means of the isomorphismµ♯. In particular, when
Mis connected, there is an isomorphism between the top compactly supported twisted de Rham
cohomology group Hn
c (M) (which depends on the topology ofM) and R given by integration,
ω ↦→
∫
Mω. Applying the transformationµ♯ to this isomorphism yields the isomorphism of vector
spaces
µ: C∞
c (M)/Im(divµ|c) →R,
where divµ|c : Xc(M) →C∞
c (M) is the divergence operator restricted to the set of compactly
supported vector ﬁeldsXc(M). Hence, ifh,f ∈C∞
c (M), then
∫
fdµ =
∫
hdµ ⇐⇒ f = h+ divµ(X) for someX ∈Xc(M).
Consequently,
µ−1({0}) = {divµ(X) : X ∈Xc(M)}.
Thus, the test functions that integrate to zero are precisely those that can be written as the diver-
gence of compactly supported vector ﬁelds. In particular, on compact manifolds, there is a canonical
Stein operator,divµ, which turns vector ﬁelds into functions with vanishing expectations. For other
types of manifolds, one can obtain similardualities by using other classes of diﬀerential forms, such
as the square-integrable ones, or by allowing boundaries. For our purposes, the above is suﬃcient
to motivate calling
Sµ ≡divµ|Xµ : Xµ →C∞(M)
the canonical Stein operator, whose domain Xµ, called the Stein class, is any set of vector ﬁelds
satisfying the desired property thatEµ[Sµ(X)] ≡
∫
Sµ(X)dµ= 0, for allX ∈Xµ.
If we have a bracketB on M, we can turn the canonical Stein operator on vector ﬁelds into a
2nd order diﬀerential operator acting on functions, theB-Stein operator on the Stein class
C∞
µ ≡{f ∈C∞(M) : XB
f ∈Xµ},
by
SB
µ : C∞
µ →C∞(M), SB
µf ≡divµ(XB
f ).
If µ= e−HµMthen we have the following useful decomposition:
dive−HµM(XB
f ) = divµM(XB
f ) −XB
f (H).
Let us give some important examples of bracket Stein operators. WhenB ≡A is antisymmetric,
the A-Stein operator is simply a 1st order diﬀerential operator, namely theµ-preserving curl vector
ﬁeld SA
µ = curlµ(A). When B is Riemannian, andµMis the Riemannian measure, then
Sg
µ(f) = ∇·Xg
f −⟨∇f,∇H⟩= ∆f −⟨∇f,∇H⟩= ∆f + ⟨∇f,∇log dµ/dµM⟩,
where ∇,∆,∇·,⟨·,·⟩are the Riemannian gradient, Laplacian, divergence, and metric, respectively;
theg−1-SteinoperatorbecomestheRiemannianSteinoperator[21,198,199]. Hence, theRiemannian
Stein operator is the restriction of the canonical Stein operator to gradient vector ﬁelds. In general,
decomposing the bracket into its symmetric and antisymmetric parts,B ≡S+ A, we obtain the
following useful decomposition of theB-Stein operator:
SS+A
µ (f) = divµ(XS
f ) + curlµ(A)(f). (39)
22
In particular, if we restrict ourselves to a symmetric positive semi-deﬁniteB, associated to a set
of vector ﬁelds{Yi}, XB
f ≡Yi(f)Yi for any functionf, then (39) corresponds to the generator of
a µ-preserving diﬀusion. A suitable Stein class is then the domain of the generator, since for any
function in that domain Eµ
[
SS+A
µ f
]
vanishes by the Fokker-Planck equation. The construction
of Stein operators via measure-preserving diﬀusions is known as the Barbour approach [200]. In
fact, the brackets allow us to deﬁne a more general notion of Stein operator acting on 1-forms
{α : XB
α ∈Xµ}, and, on ﬂat Euclidean space,SB
µ(α) ≡divµ(XB
α) recovers the “diﬀusion” Stein
operator [201].
4.2.2 Kernel Stein discrepancies and score matching
Once we have a Stein operator, we need to construct a Stein class for it, i.e., a setV of vector ﬁelds
(or more general tensor ﬁelds) whose imageF under the operator has mean zero underµ. The
resulting IPM is then known as aStein Discrepancy:
dSµ(V )(µ,ρ) = sup
X∈V
⏐⏐⏐⏐
∫
Sµ(X)dρ
⏐⏐⏐⏐.
The expression
∫
Sµ(X)dρ is precisely the rate of change of theKL divergence along measures
satisfying the continuity equation; an observation that leads toStein variational gradient descent
(SVGD) methods to approximate distributions [198,202]. Speciﬁcally, in SVGD the target measure
is approximated using a ﬁnite distribution ∑
ℓδxℓ, where the location of the particles {xℓ}ℓ is
updated by moving along the direction that maximises the rate of change ofKL within a space of
vector ﬁelds isomorphic to a RKHS (e.g., the space of gradients of functions in a RKHS).
When Sµ is the canonical Stein operator, there is a canonical Stein class, provided by Stokes’
theorem, which essentially only depends on the manifold: for a connected manifoldM, viewing
integration as an operator on smoothµ-integrable functions, then
∫
fdµ = 0 ⇐⇒
∫
dα= 0, where
f = divµ(µ♯(α)). Unfortunately, Stokes’ theorem usually does not provide a practical description
of the diﬀerential forms that satisfy
∫
dα = 0 , aside from the compactly supported case. There
are, however, several choices of Stein class constructed from Hilbertian subspaces that lead to
computationally tractable Stein discrepancies. One route consists in constructing a RKHS of mean-
zero functions as the image of another RKHS under a Stein operator. In this case, we can useSµ
to map a given RKHS ofRd-valued functionsH, with (matrix-valued) reproducing kernelK, into
a Stein RKHS of R-valued functionsSµ(H) associated to aStein reproducing kernelkµ, given by
(here q is the Lebesgue density ofµ)
kµ(x,y) = 1
q(x)q(y)∂y ·∂x ·(q(x)K(x,y)q(y)) .
The resulting Stein discrepancy can be thought of as an MMD that depends only onρand is known
as kernel Stein discrepancy[203]:
KSD[ρ]2 ≡MMD[ρ|µ]2 =
∫ ∫ 1
q(x)q(y)∂y ·∂x ·(q(x)K(x,y)q(y)) dρ(y)dρ(x).
Another class of discrepancies relies on a choice of bracketB together with a corollary from Stokes’
theorem:
∫
SB
µ(α)dρ=
∫
α(XB∗
H−K)dρ, wheree−H and e−K are the densities ofρand µwith respect
to a common smooth measure (below the Riemannian one), while B∗ is the dual bracket (the
transpose ofB). We can thus re-write the Stein discrepancy as
sup
α∈A
⏐⏐⏐⏐
∫
α(XB∗
H−K)dρ
⏐⏐⏐⏐ (40)
23
over some family of 1-forms A . As we did previously, we can “remove” the supremum by re-
writing the above as a supremum over some unit ball of a continuous linear functional. This can
be achieved once we have a Riemannian metric⟨·,·⟩, which induces a natural inner product that is
central to the theory of Harmonic forms, namely(α,β)µ ≡
∫
⟨α,β⟩dµ. In particular, taking asA
thesmoothcompactlysupported1-formsintheunitballof L2(T∗M,µ)—theHilbertspaceofsquare
µ-integrable 1-forms—the Stein discrepancy recovers a generalisation of the score matching [71]:
SMB[ρ|µ] =
∫
∥XB∗
H−K∥2dρ= Eρ
[
∥XB∗
H−K∥2
]
. (41)
It is worth noting that, whileL2(T∗M,µ) is not a RKHS, and does not have a reproducing kernel,
it remains a Hilbertian subspace of the space of de Rham currents. WhenB is Riemannian we
recover the Riemannian score matching [21]
SMG[ρ|µ] =
∫
∥∇H−∇K∥2dρ,
while in Euclidean space (41) yields the diﬀusion score matching [204].
4.3 Information geometry of MMDs and natural gradient descent
MMDs and Stein discrepancies have proved to be important tools in a wide range of contexts, from
hypothesis testing and training generative neural networks to measuring sample quality [128,205–
209]. In the context of statistical inference, once we have chosen a suitable discrepancy,D, and a
statistical model,{µΘ}, our aim is to ﬁnd the best approximation of the target distribution within
the model; this corresponds to solving the optimisation problemθ∗ ∈arg minθ∈Θ D[ρ |µθ]. As
mentioned previously, computing the value of the discrepancyD[ρ |µθ] is computationally chal-
lenging. Fortunately, we can often obtain robust Stein discrepancy estimators for smooth statistical
models, whose distributions have a smooth positive Lebesgue density, as well as MMD estimators
for generative model that are easy to sample from but have intractable model densities.
In either case, once we have an estimatorˆDm based onmsamples from the target, we must solve
the approximate optimisation problemθ∗
m ∈arg minθ∈Θ ˆDm[ρ|µθ]. When the functionˆDm[ρ|µθ] is
smooth, this may be done via the accelerated Hamiltonian-based optimisation methods previously
discussed (section §2). If D is a divergence function, one can also usually improve the speed of
convergence by following thenatural gradient descent, associated with the information Riemannian
metric gθ induced byD [210–213]. In practice, this leads to implementing the update
ˆθt+1 = ˆθt −γtˆg−1
θt ∂θt
ˆDm[ρ|µθ],
where {γt}is an appropriate sequence of step sizes, andˆg−1
θ is the inverse of a regularised estimate
of the information tensor [214]. Finally, note that there is a deep connection between divergences
and the geometric mechanics discussed in sampling and optimisation, as any divergence may be
interpretedasa discrete Lagrangian, andhencegeneratesasymplecticstructureandintegrator[215].
4.3.1 Minimum Stein discrepancy estimators
When the model{µθ}consists of smooth measures with positive densities{qθ}, and we have access
to samples{xℓ}from the target, the Stein discrepancies oﬀer a ﬂexible family of inference methods.
For SM we can use the estimator
ˆSMm[ρ|µθ] = 1
m
m∑
ℓ=1
(
∥BT∂xlog qθ∥2
2 + 2∂x ·(BBT∂xlog qθ)
)
(xℓ)
24
combined with the following expression for the information tensor:
(gθ)ij =
∫
BT∂x∂θi log qθ ·BT∂x∂θj log qθdµθ.
For KSD it is convenient to choose a family of matrix kernelsKθ(x,y) = Bθ(x)k(x,y)Bθ(y)T, for
some scalar kernelk, and parameter-dependent matrix functionBθ. Denoting the associated Stein
reproducing kernel bykµθ,θ, we have the unbiased estimator
ˆKSDm[ρ] = 1
m(m−1)
m∑
i̸=j
kµθ,θ(xi,xj),
and information tensor
(gθ)ij =
∫∫
(∂x∂θj log qθ)T Bθ(x)k(x,y)BT
θ (y)∂x∂θi log qθdµθ(x)dµθ(y).
The parametersB and k, and the choice of statistical model, can often be adjusted to achieve char-
acteristiness, consistency, bias-robustness, and obtain central limit theorems; see [204] for details,
and for numerical experiments showing an acceleration induced by the information Riemannian
metric.
4.3.2 Likelihood-free inference with generative models
For many applications of interests, the densities of the model,{µθ}, cannot be evaluated or diﬀer-
entiated. We thus need density-free inference methods. This is the case, for instance, in the context
of generative models whereinµθ is the pushforward of a distributionµ, from which we can sample
eﬃciently, with respect to a generator functionTθ. Then, the minimum Stein discrepancy estimators
based onKSD and SM, or other discrepancies that rely on the scores, are intractable. The MMDs
are suited to this case since they depend on the target and model only through integration, which
can be straightforwardly estimated using the samples. The associated information tensor is
(gθ)ij =
∫∫
∂θTθ(u)T∂x∂yk(x,y)|(Tθ(u),Tθ(v))∂θTθ(v)dµ(u)dµ(v).
Under appropriate choices of kernels and models one can derive theoretical guarantees, such as
concentration/generalisation bounds, consistency, asymptotic normality, and robustness; see, e.g.,
[209,216,217]. Moreover, many approaches to kernel selection in a wide range of contexts have been
studied, which include the median heuristic or maximising the power of hypothesis tests, and in
practice mixtures of Gaussian kernels are often employed [209,216,218–221].
5 Adaptive agents through active inference
The previous sections have established some of the mathematical fundaments of optimisation, sam-
pling and inference. In this ﬁnal section, we close with a generic use case calledactive inference.
Active inference is a general framework for describing and designing adaptive agents that uniﬁes
many aspects of behaviour—including perception, planning and learning—as processes of inference.
Active inference emerged in the late 2000s as a unifying theory of human brain function [82–84,222],
and has since been applied to simulate a wide range of behaviours in neuroscience [81,223], ma-
chine learning [88,94,224], and robotics [85,86]. In what follows, we derive the objective functional
25
overarching decision-making in active inference and describe its information geometric structure,
revealing several special cases that are established notions in statistics, cognitive science and engi-
neering. Finally, we exploit this geometric structure in a generic framework for designing adaptive
agents.
5.1 Modelling adaptive decision-making
5.1.1 Behaviour, agents and environments
We deﬁne behaviour as the interaction between an agent and its environment. Together the agent
and its environment form asystem that evolves over time according to a stochastic processx. This
deﬁnition entails a notion of timeT, which may be discrete or continuous, and a state spaceX,
which should be a measure space (e.g., discrete space, manifold, etc.). A stochastic processx is a
time-indexed collection of random variablesxt on the state space. More concisely, it is a random
variable over trajectories on the state spaceT →X:
x: Ω →(T →X), ω↦→x(ω) ⇐⇒ xt : Ω →X, ω↦→x(ω)(t) ∀t∈T .
We denote by P the probability density of x on the space of paths T → Xwith respect to a
pre-speciﬁed base measure.
Typically, systems comprising an agent and its environment have three sets of states:external
states are unknown to the agent and constitute the environment; theobservable states are those
agent’s states that the agent sees but cannot directly control; ﬁnally, theautonomous states are
those agent’s states that the agent sees and can directly control. This produces a partition of
the state spaceX into states external to the agentSand states belonging to the agentΠ, which
themselves comprise observableOand autonomous statesA. As a consequence, the systemx can
be decomposed into externals, observableo, and autonomousa processes:
X≡S× Π ≡S×O×A =⇒ x≡(s,π) ≡(s,o,a ),
here written as random interacting trajectories on their respective spaces (see Figure 2 for an
illustration).
5.1.2 Decision-making in precise agents
The description of behaviour adopted so far could, in principle, describe particles interacting with a
heat bath [107] as well as humans interacting with their environment (see Figure 2). We would like a
description that accounts for purposeful behaviour [81,225–229]. So what distinguishes people from
small particles? An obvious distinction is that human behaviour is subject to classical as opposed
to statistical mechanics. In other words, people are precise agents, with conservative dynamics.
Deﬁnition 5.1(Precise agent). An agent is precise when it evolves deterministically in a (possibly)
stochastic environment, i.e., whenP(π|s) is a Dirac measure for anys. For example,
dst = f(st,πt)dt+ dwt, dπ t = g(st,πt)dt.
At any moment in timet, the agent has access, at most, to its past trajectoryπ≤t, and has
agency over its future autonomous trajectory a>t. We deﬁne a decision to be a choice of au-
tonomous trajectory in the future given available knowledgea>t |π≤t. We interpretP(s,o |π≤t)
as expressing the agent’spreferencesover external and observable trajectories given available data,
and P(s,o |a>t,π≤t) as expressing the agent’spredictions over external and observable paths given
26
Figure 2: Partitions and agents.This ﬁgure illustrates a human (agentπ) interacting with its environment
(external processs), and the resulting partition into externals, observableo, and autonomousa processes.
The external states are the environment, which the agent does not have direct access to, but which is sampled
through the observable states. These could include states of the sensory epithelia (e.g., eyes and skin). The
autonomous states constitute the muscles and nervous system that factor available information into decisions.
In the example of human behaviour, the environment causes observations (i.e., sensations), which informs a
nervous and muscular response, which in turn inﬂuences the environment. In general, autonomous responses
may be informed by all past agent statesπ≤t = (o≤t,a≤t) (the information available to the agent at timet),
which means that the systems we are describing are typically non-Markovian.
a decision. Crucially, decision-making in (precise) agents is a functional of the agent’s predictions
and preferences10
−log P(a>t |π≤t) = EP(s,o|a>t,π≤t)[−log P(a>t |π≤t)] = E[log P(s,o |a>t,π≤t) −log P(s,o,a >t |π≤t)]
= E[log P(s|a>t,π≤t) −log P(s,o |π≤t) + logP(o|s,a>t,π≤t) −log P(a>t |s,o,π ≤t)  
=0
]
⇒−log P(a>t |π≤t) = EP(s,o|a>t,π≤t)[log P(s|a>t,π≤t) −log P(s,o |π≤t)]. (EFE)
This functional is known as anexpected free energy(EFE) [81,84,231] because it resembles the
expectation of the free energy functional (a.k.a. evidence lower bound [232]) used in approximate
Bayesian inference [84,226]. We deﬁneactive inference as Hamilton’s principle of least action on
expected free energy11 that expresses the most likely decisiona>t, where
a>t ≡arg min
a>t
−log P(a>t |π≤t). (AIF)
5.1.3 The information geometry of decision-making
Interestingly, active inference (AIF) looks like it describes agents that engage in purposeful be-
haviour. Indeed, we can rearrange the expected free energy (EFE) in several ways, each of which
reveals a fundamental trade-oﬀ that underwrites decision-making. This allows us to relate active
inference to information theoretic formulations of decision-making that predominate in statistics,
10Under the precise agent assumption (Deﬁnition 5.1) it is straightforward to show thatEP(s,o|a>t,π≤t)[log P(o |
s,a>t,π≤t) −log P(a>t |s,o,π ≤t)] = 0 when the path spaceT →Xis countable [230]. Presumably, this equality
can be extended to more general path spaces by a limiting argument.
11As a negative log density over paths, the expected free energy is an action in the physical sense of the word.
27
Figure 3:Decision-making under active inference. This ﬁgure illustrates various imperatives that underwrite
decision-making under active inference in terms of several constructs that predominate in statistics, cognitive
science and engineering. These formulations are disclosed when one removes certain sources of uncertainty.
For example, if we remove ambiguity, decision-making minimises risk, which corresponds to aligning pre-
dictions with preferences about the external course of events. This aligns with prospect theory of human
choice behaviour in economics [233] and underwrites modern approaches to control as inference [234–236],
variously known as Kalman duality [237,238], KL control [239] and maximum entropy reinforcement learn-
ing [240]. If we further remove preferences, decision-making maximises the entropy of external trajectories.
This maximum entropy principle [241,242] allows one to least commit to a pre-speciﬁed external trajectory
and therefore keep options open. If we reintroduce ambiguity, but ignore preferences, decision-making max-
imisesintrinsicvalueorexpectedinformationgain[243]. ThisunderwritesBayesianexperimentaldesign[244]
and active learning in statistics [245], intrinsic motivation and artiﬁcial curiosity in machine learning and
robotics [246–250]. This is mathematically equivalent to optimising expected Bayesian surprise and mutual
information, whichunderwritesvisualsearch[251,252]andtheorganisationofourvisualapparatus[253–255].
Lastly, if we remove intrinsic value, we are left with maximising extrinsic value or expected utility. This
underwrites expected utility theory [78], game theory, optimal control [256,257] and reinforcement learn-
ing [80]. Bayesian formulations of maximising expected utility under uncertainty are also known as Bayesian
decision theory [77]. To ease notation, we omitted to condition every distribution in the ﬁgure byπ≤t.
cognitive science and engineering (see Figure 3). For example, decision-making minimises both risk
and ambiguity:
−log P(a>t |π≤t) = KL
[
predicted paths
  
P(s|a>t,π≤t) |
preferred paths
  
P(s|π≤t)
]
  
risk
+ EP(s,o|a>t,π≤t)
[
−log P(o|s,π≤t)
]
  
ambiguity
.
(42)
Risk refers to the KL divergence between the predicted and preferred external course of events.
Minimising risk entails making predicted (external) trajectories fulﬁl preferred external trajectories.
In a nutshell,ambiguity refers to the expected entropy of future observations, given future external
trajectories. An external trajectory that can lead to various distinct observation trajectories is
highly ambiguous—and vice-versa. Thus, minimising ambiguity leads to sampling observations
28
that enable to recognise the external course of events. This leads to a type of observational bias
commonly known as the streetlight eﬀect [258]: when a person loses their keys at night, they initially
search for them under the streetlight because the resulting observations (“I see my keys under the
streetlight” or “I do not see my keys under the streetlight”) accurately disambiguate external states
of aﬀairs.
Similarly, decision-making maximises extrinsic and intrinsic value [259]:
−log P(a>t |π≤t) = EP(o|a>t,π≤t)
[
KL [P(s|o,a>t,π≤t) |P(s|o,π≤t)]  
≥0
]
−EP(o|a>t,π≤t)
[
log
preferred paths
  
P(o|π≤t)
]
  
extrinsic value
−EP(o|a>t,π≤t)
[
KL [P(s|o,a>t,π≤t) |P(s|a>t,π≤t)]
]
  
intrinsic value
.
Extrinsic value refers to the (log) likelihood of observations under the model of preferences. This
corresponds to an expected utility or expected reward in behavioural economics, control theory and
reinforcement learning [78,80]. In short, maximising extrinsic value leads to sampling observations
that are likely under the model of preferences.Intrinsic value refers to the amount of information
gained about external courses of events. This measures the expected degree of belief updating
about external trajectories under a decision, with versus without future observations. Making de-
cisions to maximise information gain leads to a goal-directed form of exploration [231], driven to
answer “What would happen if I did that?” [247]. Interestingly, this decision-making procedure
underwrites Bayesian experimental design in statistics [244], which describes optimal experiments
as those that maximise expected information gain. In summary, decision-making weighs the im-
peratives of maximising utility and information gain, which suggests a principled solution to the
exploration-exploitation dilemma [260].
5.2 Realising adaptive agents
We now show how active inference aﬀords a generic recipe to generate adaptive agents.
5.2.1 The basic active inference algorithm
Active inference speciﬁes an agent by aprediction modelP(s,o |a), expressing the distribution of
external and observable paths given autonomous paths, and apreference modelP(s,o), expressing
the preferred external and observable trajectories. To aid intuition, we will refer to autonomous
states as actions. At any timet, the agent knows past observations and actionsπ≤t = (o≤t,a≤t),
and must make a decisiona>t. In discrete time, active inference proceeds by assessing the expected
free energy of each possible decision and then executing the best one:
1. Preferential inference:infer preferences about external and observable trajectories, i.e.,
Approximate P(s,o |π≤t) by Q(s,o). (43)
2. For each possible sequence of future actionsa>t:
(a) Perceptual inference:infer external and observable paths under the action sequence, i.e.,
Approximate P(s,o |a>t,π≤t) by Q(s,o |a>t). (44)
29
(b) Planning as inference: assess the action sequence by evaluating its expected free en-
ergy (EFE), i.e.,
−log Q(a>t) ≡EQ(s,o|a>t)
[
log Q(s|a>t) −log Q(s,o)
]
. (45)
3. Decision-making: execute the most likely decisionat+1 according to
at+1 = arg maxQ(at+1), Q (at+1) =
∑
a>t
Q(at+1 |a>t)Q(a>t). (46)
5.2.2 Sequential decision-making under uncertainty
A common model of sequential decision-making under uncertainty is apartially observable Markov
decision process(POMDP). A POMDP is a discrete time model of how actions inﬂuence external
and observable states. In a POMDP, 1) each external state depends only on the current action
and previous external stateP(st |st−1,at), and 2) each observation depends only on the current
external stateP(ot |st). One can additionally specify 3) a distribution of preferences over external
trajectories P(s). Together, 1) & 2) forms the agent’s (POMDP) prediction model, and 2) & 3)
forms the agent’s (hidden Markov) preference model, which deﬁnes an active inference agent. A
simple simulation of active inference on a POMDP is provided in Figure 4; implementation details
on generic POMDPs are available in [81,91,261,262]. For more complex simulations of sequential
decision-making (e.g., involving hierarchical POMDPs), please see [88,91,223,224,263–265].
5.2.3 World model learning as inference
Due to a lack of domain knowledge, it may be challenging to specify an agent’s prediction and
preference model. For example, how do external states map to observations? Should external states
be represented in a discrete or continuous state space?
In active inference, generative models are learned by inferring their parameters [81,91,268] and
structure [81,263,269–271]. Suppose there is an unknown parameter (or structure variable)min the
prediction model, the preference model or both. By deﬁnition, each alternative parameterisation
m entails diﬀerent predictions P(o,s | a,m) and preferences P(o,s | m). Since unknowns are
simply external states, we treat the parameter as an additional external state. We equip the space
of parameters with a prior distributionP(m), and deﬁne the agent with an augmented prediction
(resp. preference) model that combines the diﬀerent alternativesP(o,s,m |a) ≡P(o,s |a,m)P(m)
(resp. P(o,s,m ) ≡P(o,s |m)P(m)). The parameter can then be inferred along with other external
states during preferential (43) or perceptual (44) inference [81,91,268]. Better yet, having speciﬁed
priors over parameters that are independent of actions, we can infer them separately, for example,
after ﬁxed-length sequences of decisions to reduce computational cost [81,268].
All this says that a priorP(m) and some dataπ≤t leads to approximate posterior beliefsQ(m) ≈
P(m|π≤t) aboutmodelparameters. Butwhataretherightpriors? Onewaytoanswerthisquestion
lies in optimising a free energy functionalF (a.k.a. an evidence lower bound [232]):
F ≡EQ(m)
[
−log P(m,π≤t)
]
  
energy
−S
[
Q(m)
]
  
entropy
= KL
[
Q(m) |P(m)
]
  
complexity
−EQ(m)
[
log P(π≤t |m)
]
  
accuracy
.
Choosing priors that minimise free energy leads to parsimonious models that explain the data
at hand [272]. This follows since maximising accuracy increases the likelihood of the data under
30
Figure 4: Sequential decision-making in a T-Maze environment. Left:The agent’s prediction model is a
partially observed Markov decision process (see text) represented here as a Bayesian network [266]. The
colour scheme illustrates the problem att= 2: the agent must make a decision (in red) based on previous
actions and observations (in grey), which are informative about external states and future observations (in
white). Right: st: The T-Maze has four possible spatiallocations: middle, top-left, top-right, bottom. One of
the top locations contains a reward (in red), while the other contains a punishment (in black). The reward’s
location determines thecontext. The bottom arm contains a cue whose colour (blue or green) discloses the
context. Together, location and context determine the external state.ot: The agent observes its spatial
location. In addition, when it is at the top of the Maze, it observes the reward or the punishment; when it
is at the bottom, it observes the colour of the cue.at: Each action corresponds to visiting one of the four
spatial locations. P(st): The agent prefers being at the reward’s location (−log P(st) = +3) and avoid the
punishment’s location (−log P(st) = −3). All other states have a neutral preference (−log P(st) = 0). o0:
The agent is in the middle of the Maze and is unaware of the context.a1: Visiting the bottom or top arms
have a lower ambiguity than staying, as they yield observations that disclose the context. However, staying
or visiting the bottom arm are safer options, as visiting a top arm risks receiving the punishment. By acting
to minimise both risk and ambiguity (42) the agent goes to the bottom.o1: The agent observes the colour
of the cue and hence determines the context.a2: All actions have equal ambiguity as the context is known.
Collecting the reward has a lower risk than staying or visiting the middle, which themselves have a lower risk
than collecting the punishment. Thus, the agent visits the arm with the reward. See [267] for more details.
the posterior model, while minimising complexity decreases the movement from prior to posterior,
which can be seen as a proxy for computational cost. Maximising accuracy usually results in gener-
ative models involving universal function approximators [88,224,264], while minimising complexity
results in organising representations in sparse, compartmentalised and hierarchical generative mod-
els [263,270,271], where higher levels of the hierarchy encode more abstract representations and
vice-versa [265]. A computationally eﬃcient method to compare priors by their free energy is
Bayesian model reduction [81,273]. In conclusion, free energy uniﬁes inference and model selection
under a single objective function.
31
5.2.4 Scaling active inference
We conclude by identifying promising scaling methods for active inference that enable computa-
tionally tractable implementations in a variety of applications.
Planning for all possible courses of action is computationally expensive as the number of action
sequences is exponential in the length of the sequence. One way to ﬁnesse this is by planning only
for intelligently chosen subsets of action sequences, using sampling algorithms such as Monte-Carlo
tree search [87,224,274–276]. Monte-Carlo sampling can be also be used to ﬁnesse the expectations
inherent in assessing action sequences (45) [224]. A complementary approach is to assess actions,
instead of action sequences, by conditioning future actions to be optimal in the sense that they
minimise the expected free energy [228,277]. This idea leads to a backward form of planning,
where the agent plans for the best action at the last time-step, followed by the best action at the
penultimate time-step, and so on, until the present. Crucially, it leads to smarter agents [228,277]
whose computational complexity scales linearly (as opposed to exponentially) in the length of action
sequences [278].
Scalable inference methods [279] can be used to make active inference more eﬃcient [280]. For
example, we can train neural networks to predict the various posterior distributions, including
the posterior over actions [88,224,281]. While training, the output of the neural network can be
used as an initial conditions for variational inference [282], resulting in accurate inferences whose
computational cost decrease as the network learns. Additionally, optimising free energy reduces to
eﬃcient message passing schemes, when one imposes certain simplifying restrictions to the family
of candidate distributions [283–287].
A much cheaper implementation of active inference exists for continuous states evolving in
continuous time. The method frames perception and decision-making as variational inference, by
simulating a gradient ﬂow on free energy in an extended state space [82,226]. Furthermore, it can be
combined with discrete active inference to operate eﬃciently in generative models combining discrete
and continuous states [288]. As an example, high-dimensional observations in the continuous domain
(e.g., speech) processed through continuous active inference are converted into discrete, abstract
representations (e.g., semantics) [281]. Based on these representations, the agent makes high-level,
categorical decisions (e.g., “I want to move over there”), which contextualise low-level, continuous
actions (e.g., the continuous motion of a limb towards the goal location) [289].
Acknowledgements
The authors thank Noor Sajid, Samuel Tenka, Zafeiros Fountas and Panagiotis Tigas for helpful
discussions on adaptive agents. LD is supported by the Fonds National de la Recherche, Lux-
embourg (Project code: 13568875). KF is supported by funding for the Wellcome Centre for
Human Neuroimaging (Ref: 205103/Z/16/Z) and a Canada-UK Artiﬁcial Intelligence Initiative
(Ref: ES/T01279X/1). GAP was partially supported by JPMorgan Chase & Co under J.P. Morgan
A.I. Research Awards in 2019 and 2021 and by the EPSRC, grant number EP/P031587/1. This
publication is based on work partially supported by the EPSRC Centre for Doctoral Training in
Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1). AB, GF,
MG and MIJ thank the support of the Army Research Oﬃce (ARO) under contract W911NF-17-
1-0304 as part of the collaboration between US DOD, UK MOD and UK Engineering and Physical
Research Council (EPSRC) under the Multidisciplinary University Research Initiative (MURI).
References
[1] R. I. McLachlan and G. R. W. Quispel. Splitting methods.Acta Numer., 11:341, 2002.
32
[2] E. Hairer, C. Lubich, and G. Wanner.Geometric Numerical Integration: Structure-Preserving Algorithms for
Ordinary Diﬀerential Equations. Springer, 2010.
[3] B. Leimkuhler and S. Reich.Simulating Hamiltonian Dynamics. Cambridge University Press, 2004.
[4] E. Celledoni, H. Marthinsen, and B. Owren. An introduction to Lie group integrators: basics, new developments
and applications. J. Comput. Phys., 257:1040–1061, 2014.
[5] J. E. Marsden and M. West. Discrete mechanics and variational integrators.Acta Numer., 10:357–514, 2001.
[6] M. Betancourt, M. I. Jordan, and A. Wilson. On symplectic optimization. 2018. arXiv:1802.03653 [stat.CO].
[7] A. Bravetti, M. L. Daza-Torres, H. Flores-Arguedas, and M. Betancourt. Optimization algorithms inspired by
the geometry of dissipative systems. 2019. arXiv:1912.02928 [math.OC].
[8] G. França, J. Sulam, D. P. Robinson, and R. Vidal. Conformal symplectic and relativistic optimization.J.
Stat. Mech., 2020(12):124008, 2020.
[9] G. França, M. I. Jordan, and R. Vidal. On dissipative symplectic integration with applications to gradient-based
optimization. J. Stat. Mech., 2021(4):043402, 2021.
[10] G. França, A. Barp, M. Girolami, and M. I. Jordan. Optimization on manifolds: A symplectic approach. 2021.
arXiv:2107.11231 [cond-mat.stat-mech].
[11] F. Alimisis, A. Orvieto, G. Becigneul, and A. Lucchi. Momentum improves optimization on Riemannian
manifolds. Int. Conf. Artiﬁcial Intelligence and Stats., 130:1351–1359, 2021.
[12] M. Rousset, G. Stoltz, and T. Lelievre. Free Energy Computations: A Mathematical Perspective. World
Scientiﬁc, 2010.
[13] S. Duane, A.D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo.Phys. Lett. B, 195(2):216–222,
1987.
[14] M. Betancourt, S. Byrne, S. Livingstone, and M. Girolami. The geometric foundations of Hamiltonian Monte
Carlo. Bernoulli, 23(4A):2257–2298, 2017.
[15] S. Livingstone, M. Betancourt, S. Byrne, and M. Girolami. On the geometric ergodicity of Hamiltonian Monte
Carlo. Bernoulli, 25(4A):3109–3138, 2019.
[16] A. Barp, A. Kennedy, and M. Girolami. Hamiltonian Monte Carlo on symmetric and homogeneous spaces via
symplectic reduction. 2019.
[17] E. Celledoni, M. J. Ehrhardt, C. Etmann, R.I. McLachlan, B. Owren, C. B.Schonlieb, and F. Sherry. Structure-
preserving deep learning.Eur. J. Applied Math., 32(5):888–936, 2021.
[18] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic. Geometric deep learning: Grids, groups, graphs,
geodesics, and gauges, 2021. arXiv:2104.13478 [cs.LG].
[19] M. Vaillant and J. Glaunes. Surface matching via currents. InBiennial Int. Conf. Information Processing in
Medical Imaging, pages 381–392. Springer, 2005.
[20] S. Durrleman, X. Pennec, A. Trouvé, and N. Ayache. Statistical models of sets of curves and surfaces based on
currents. Medical Image Analysis, 13(5):793–808, 2009.
[21] A. Barp, Chris J. Oates, E. Porcu, and M. Girolami. A Riemann-Stein Kernel Method. 2020. arXiv:1810.04946
[math, stat].
[22] P. Harms, P. W. Michor, X. Pennec, and S. Sommer. Geometry of sample spaces. 2020. arXiv:2010.08039.
[23] C. R. Rao. Information and the accuracy attainable in the estimation of statistical parameters. InBreakthroughs
in Statistics, pages 235–247. Springer, 1992.
[24] H. Jeﬀreys. An invariant form for the prior probability in estimation problems.Proc. Royal Soc. London. Series
A. Math. and Phys. Sci., 186(1007):453–461, 1946.
[25] S. Amari. Information geometry and its applications, volume 194. Springer, 2016.
[26] N. Ay, J. Jost, H. Vân Lê, and L. Schwachhöfer.Information geometry, volume 64. Springer, 2017.
[27] F. Nielsen. An elementary introduction to information geometry.Entropy, 22(10):1100, 2020.
[28] S. Amari. Diﬀerential-geometrical methods in statistics, volume 28. Springer Science & Business Media, 2012.
[29] N. N. Chentsov. Categories of mathematical statistics.Uspekhi Matematicheskikh Nauk, 20(4):194–195, 1965.
[30] J. Jost, H. V. Lê, and T. D. Tran. Probabilistic morphisms and bayesian nonparametrics.Eur. Phys. J. Plus,
136(4):1–29, 2021.
[31] B. T. Polyak. Some methods of speeding up the convergence of iteration methods.USSR Comp. Math. and
Math. Phys., 4(5):1–17, 1964.
[32] Y. Nesterov. A method of solving a convex programming problem with convergence rateO(1/k2). Soviet Math.
Doklady, 27(2):372–376, 1983.
[33] W. Su, S. Boyd, and E. J. Candès. A diﬀerential equation for modeling Nesterov’s accelerated gradient method:
Theory and insights.J. Mach. Learn. Res., 17(153):1–43, 2016.
[34] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization.
Proc. Nat. Acad. Sci., 113(47):E7351–E7358, 2016.
[35] A. Wilson, B. Recht, and M. I. Jordan. A Lyapunov analysis of accelerated methods in optimization.J. Mach.
Learn. Res., 22:1–34, 2021.
33
[36] G. Franca, D. Robinson, and R. Vidal. ADMM and accelerated ADMM as continuous dynamical systems.
80:1559–1567, 2018.
[37] G.França, D.P.Robinson, andR.Vidal. Anonsmoothdynamicalsystemsperspectiveonacceleratedextensions
of ADMM. 2018. arXiv:1808.04048 [math.OC].
[38] G. França, D. P. Robinson, and R. Vidal. Gradient ﬂows and proximal splitting methods: A uniﬁed view on
accelerated and stochastic optimization.Phys. Rev. E, 103:053304, 2021.
[39] M. Muehlebach and M. I. Jordan. Optimization with momentum: Dynamical, control-theoretic, and symplectic
perspectives. J. Mach. Learn. Res., 22(73):1–50, 2021.
[40] M.MuehlebachandM.I.Jordan. Onconstraintsinﬁrst-orderoptimization: Aviewfromnon-smoothdynamical
systems. 2021. arXiv:2107.08225, [math.OC].
[41] M. Takahashi and M. Imada. Monte Carlo calculation of quantum systems. II. Higher order correction.J.
Phys. Soc. Jpn., 53:3765—-3769, 1984.
[42] M. Suzuki. Fractal decomposition of exponential operators with applications to many-body theories and Monte
Carlo simulations.Phys. Lett. A, 146:319–323, 1990.
[43] H. Yoshida. Construction of higher order symplectic integrators.Phys. Lett. A, 150(5):262–268, 1990.
[44] J. M. Sanz-Serna. Symplectic integrators for Hamiltonian problems: An overview.Acta Numerica, 1:243–286,
1992.
[45] G. Benettin and A. Giorgilli. On the Hamiltonian interpolation of near-to-the-identity symplectic mappings
with application to symplectic integration algorithms.J. Stat. Phys., 74:1117–1143, 1994.
[46] R. I. McLachlan and G. R. W. Quispel. Geometric integrators for ODEs.J. Phys. A: Math. Gen., 39:5251–5285,
2006.
[47] E. Forest. Geometric integration for particle accelerators.J. Phys. A: Math. Gen., 39:5321––5377, 2006.
[48] A. D. Kennedy, P. J. Silva, and M. A. Clark. Shadow Hamiltonians, Poisson brackets, and gauge theories.
Phys. Rev. D, 87:034511, 2013.
[49] R. M. Neal. MCMC using Hamiltonian dynamics. InHandbook of Markov Chain Monte Carlo. Chapman and
Hall/CRC, 2011.
[50] F. Otto. The geometry of dissipative evolution equations: the porous medium equation. Comm. Partial
Diﬀerential Equations, 26:101–174, 2001.
[51] R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker-Planck equation.SIAM
J. Math. Anal., 29(1):1–17, 1998.
[52] R. M. Neal. Slice sampling.The annals of statistics, 31(3):705–767, 2003.
[53] I. Murray, R. Adams, and D. MacKay. Elliptical slice sampling. InInt. Conf. Artiﬁcial Intelligence and Stats.,
pages 541–548. JMLR Workshop and Conference Proceedings, 2010.
[54] Mark HA Davis. Piecewise-deterministic markov processes: A general class of non-diﬀusion stochastic models.
Journal of the Royal Statistical Society: Series B (Methodological), 46(3):353–376, 1984.
[55] A. Bouchard-Côté, S. J. Vollmer, and A. Doucet. The bouncy particle sampler: A nonreversible rejection-free
markov chain monte carlo method.J. Amer. Stats. Assoc., 113(522):855–867, 2018.
[56] P. Vanetti, A. Bouchard-Côté, G. Deligiannidis, and A. Doucet. Piecewise-deterministic Markov Chain Monte
Carlo. 2017. arXiv:1707.05296.
[57] J. Bierkens, P. Fearnhead, and G. Roberts. The zig-zag process and super-eﬃcient sampling for Bayesian
analysis of big data.Ann. Stats., 47(3):1288–1320, 2019.
[58] J. Bierkens and G. Roberts. A piecewise deterministic scaling limit of lifted metropolis–hastings in the curie–
weiss model. Ann. App. Prob., 27(2):846–882, 2017.
[59] E. A. J. F. Peters and G. de With. Rejection-free monte carlo sampling for general potentials.Phys. Rev. E,
85(2):026703, 2012.
[60] G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their discrete ap-
proximations. Bernoulli, pages 341–363, 1996.
[61] A. Durmus and E. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin algorithm.Ann.
App. Prob., 27(3):1551–1587, 2017.
[62] A. Durmus, E. Moulines, and M. Pereyra. Eﬃcient bayesian computation by proximal markov chain monte
carlo: when langevin meets moreau.SIAM J. on Imaging Sci., 11(1):473–506, 2018.
[63] A. Garbuno-Inigo, F. Hoﬀmann, W. Li, and A. M. Stuart. Interacting Langevin diﬀusions: gradient structure
And ensemble Kalman sampler. 2019. arXiv:1903.08866 [math].
[64] M. Betancourt. A conceptual introduction to hamiltonian monte carlo. 2017. arXiv:1701.02434.
[65] A. Barp, F-X. Briol, A. D. Kennedy, and M. Girolami. Geometry and dynamics for Markov Chain Monte
Carlo. Annual Rev. Stats. and its App., 5:451–471, 2018.
[66] N. R. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calcula-
tions by fast computing machines.J. Chem. Phys., 21(6):1087–1092, 1953.
34
[67] W. K. Hastings.Monte Carlo Sampling Methods Using Markov Chains and their Applications. Oxford Univer-
sity Press, 1970.
[68] B. J. Alder and T. E. Wainwright. Studies in molecular dynamics. I. general method. J. Chem. Phys.,
31(2):459–466, 1959.
[69] A. W. Van der Vaart.Asymptotic Statistics, volume 3. Cambridge university press, 2000.
[70] V. Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media, 1999.
[71] A. Hyvärinen and P. Dayan. Estimation of non-normalized statistical models by score matching.J. Mach.
Learn. Res., 6(4), 2005.
[72] M. Parry, A. P. Dawid, and S. Lauritzen. Proper local scoring rules.Ann. Stats., 40(1):561–592, 2012.
[73] C. Villani. Optimal Transport: Old and New. Springer, 2009.
[74] F. Bassetti, A. Bodini, and E. Regazzini. On minimum Kantorovich distance estimators.Stats. & Prob. Lett.,
76(12):1298–1302, 2006.
[75] G. Peyré, M. Cuturi, et al. Computational optimal transport: With applications to data science.Found. Trends
Mach. Learn., 11(5-6):355–607, 2019.
[76] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport.NeurIPS, 26:2292–2300, 2013.
[77] J. O. Berger.Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics. Springer-Verlag,
New York, second edition, 1985.
[78] J. Von Neumann and O. Morgenstern.Theory of Games and Economic Behavior. Princeton University Press,
1944.
[79] R. E. Bellman and S. E. Dreyfus.Applied Dynamic Programming. Princeton University Press, 2015.
[80] A. Barto and R. Sutton.Reinforcement Learning: An Introduction. A Bradford Book, 1992.
[81] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston. Active inference on discrete state-spaces:
A synthesis.J. Math. Psychology, 99:102447, 2020.
[82] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel. Action and behavior: A free-energy formulation.
Biological Cybernetics, 102(3):227–260, 2010.
[83] K. Friston. The free-energy principle: A uniﬁed brain theory?Nature Reviews Neuroscience, 11(2):127–138,
2010.
[84] KarlFriston, FrancescoRigoli, DimitriOgnibene, ChristophMathys, ThomasFitzgerald, andGiovanniPezzulo.
Active inference and epistemic value.Cognitive Neuroscience, 6(4):187–214, October 2015.
[85] Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata, Alexan-
der Tschantz, Beren Millidge, Martijn Wisse, Christopher L. Buckley, and Jun Tani. Active Inference in
Robotics and Artiﬁcial Agents: Survey and Challenges.arXiv:2112.01871 [cs], December 2021.
[86] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active Inference Could
Help Revolutionise Robotics.Entropy, 24(3):361, March 2022.
[87] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,
John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.Nature,
529(7587):484–489, 2016.
[88] B. Millidge. Deep active inference as variational policy gradients.J. Math. Psychology, 96:102348, 2020.
[89] O. van der Himst and P. Lanillos. Deep Active Inference for Partially Observable MDPs. 2020. arXiv:2009.03622
[cs, stat].
[90] Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active Inference in OpenAI Gym: A Paradigm
for Computational Investigations Into Psychiatric Illness.Biological Psychiatry: Cognitive Neuroscience and
Neuroimaging, 3(9):809–818, September 2018.
[91] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystiﬁed and Compared.
Neural Computation, 33(3):674–712, January 2021.
[92] Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. An empirical evaluation of active
inference in multi-armed bandits.Neural Networks, 144:229–246, December 2021.
[93] Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi. Active Inference for Stochastic Control.
arXiv:2108.12245 [cs], August 2021.
[94] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive Active Inference. In Advances in Neural
Information Processing Systems, May 2021.
[95] A. C. Hansen. A theoretical framework for backward error analysis on manifolds.J. Geom. Mech., 3(1):81–111,
2011.
[96] R. I. McLachlan, G R. W. Quispel, and N. Robidoux. Geometric integration using discrete gradients.Philosoph-
ical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences,
357(1754):1021–1045, 1999.
[97] R. McLachlan and M. Perlmutter. Conformal Hamiltonian systems.J. Geom. and Phys., 39:276–300, 2001.
35
[98] H. Marthinsen and B. Owren. Geometric integration of non-autonomous Hamiltonian problems.Adv. Comput.
Math., 42:313–332, 2016.
[99] M. Asorey, J. F. Carinena, and L. A. Ibort. Generalized canonical transformations for time-dependent systems.
J. Math. Phys., 24(12):2745–2750, 1983.
[100] H. C. Andersen. Rattle: A “velocity” version of the shake algorithm for molecular dynamics calculations.J.
Comput. Phys., 52(1):24–34, 1983.
[101] B. J. Leimkuhler and R. D. Skeel. Symplectic numerical integrators in constrained Hamiltonian systems.J.
Comput. Phys., 112:117–125, 1994.
[102] R. I. McLachlan, K. Modin, O. Verdier, and M. Wilkins. Geometric generalizations of SHAKE and RATTLE.
Found. Comput. Math., (14):339–370, 2014.
[103] B. Leimkuhler and C. Matthews. Eﬃcient molecular dynamics using geodesic integration and solvent-solute
splitting. Proc. Royal Soc. A: Math., Phys. and Eng. Sci., 472(2189):20160138, 2016.
[104] H. Zhang and S. Sra. First-order methods for geodesically convex optimization.Conf. Learning Theory, pages
1617–1638, 2016.
[105] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. Gradient Flows: In Metric Spaces and in the Space of
Probability Measures. Springer Science & Business Media, January 2005.
[106] Michela Ottobre. Markov Chain Monte Carlo and Irreversibility.Reports on Mathematical Physics, 77:267–292,
June 2016.
[107] Grigorios A. Pavliotis. Stochastic Processes and Applications: Diﬀusion Processes, the Fokker-Planck and
Langevin Equations. Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014.
[108] Cédric Villani. Hypocoercivity, volume 202 of Memoirs of the American Mathematical Society. American
Mathematical Society, November 2009.
[109] Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett, and Michael I. Jordan. Is
there an analog of nesterov acceleration for mcmc?, 2019.
[110] D. J. C. MacKay.Information theory, inference and learning algorithms. Cambridge university press, 2003.
[111] Martin Hairer. Ergodic Properties of Markov Processes. 2018.
[112] Alessandro Barp, So Takao, Michael Betancourt, Alexis Arnaudon, and Mark Girolami. A Unifying and
Canonical Description of Measure-Preserving Diﬀusions.arXiv:2105.02845 [math, stat], May 2021.
[113] Lars Hörmander. Hypoelliptic second order diﬀerential equations.Acta Mathematica, 119:147–171, 1967.
[114] J.M. Bismut. Martingales, the malliavin calculus and hypoellipticity under general hörmander’s conditions.
Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 56(4):469–505, 1981.
[115] Yi-An Ma, Tianqi Chen, and Emily B. Fox. A Complete Recipe for Stochastic Gradient MCMC.
arXiv:1506.04696 [math, stat], October 2015.
[116] Chii-Ruey Hwang, Shu-Yin Hwang-Ma, and Shuenn-Jyi Sheu. Accelerating diﬀusions.The Annals of Applied
Probability, 15(2):1433–1444, May 2005.
[117] Djalil Chafaï. Entropies, convexity, and functional inequalities, On $\Phi $-entropies and $\Phi $-Sobolev
inequalities. Journal of Mathematics of Kyoto University, 44(2):325–363, January 2004.
[118] Aldéric Joulin and Yann Ollivier. Curvature, concentration and error estimates for Markov chain Monte Carlo.
The Annals of Probability, 38(6):2418–2442, November 2010.
[119] Luc Rey-Bellet and Kostantinos Spiliopoulos. Irreversible Langevin samplers and variance reduction: A large
deviation approach.Nonlinearity, 28(7):2081–2103, July 2015.
[120] A. B. Duncan, T. Lelièvre, and G. A. Pavliotis. Variance Reduction Using Nonreversible Langevin Samplers.
Journal of Statistical Physics, 163(3):457–491, May 2016.
[121] U. G. Haussmann and E. Pardoux. Time Reversal of Diﬀusions.Annals of Probability, 14(4):1188–1205, October
1986.
[122] Radford M. Neal. Improving Asymptotic Variance of MCMC Estimators: Non-reversible Chains are Better.
arXiv:math/0407281, July 2004.
[123] Tony Lelièvre, Francis Nier, and Grigorios A. Pavliotis. Optimal non-reversible linear drift for the convergence
to equilibrium of a diﬀusion.Journal of Statistical Physics, 152(2):237–274, July 2013.
[124] Sheng-Jhih Wu, Chii-Ruey Hwang, and Moody T. Chu. Attaining the Optimal Gaussian Diﬀusion Acceleration.
Journal of Statistical Physics, 155:571–590, May 2014.
[125] Benjamin J. Zhang, Youssef M. Marzouk, and Konstantinos Spiliopoulos. Geometry-informed irreversible
perturbations for accelerated convergence of Langevin dynamics.arXiv:2108.08247 [math, stat], August 2021.
[126] Antonietta Mira. Ordering and Improving the Performance of Monte Carlo Markov Chains.Statistical Science,
16(4):340–350, November 2001.
[127] M. Girolami and B. Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods.Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123–214, 2011.
36
[128] Assyr Abdulle, Grigorios A. Pavliotis, and Gilles Vilmart. Accelerated convergence to equilibrium and reduced
asymptotic variance for Langevin dynamics using Stratonovich perturbations.Comptes Rendus Mathematique,
357(4):349–354, April 2019.
[129] Bernard Helﬀer. Remarks on Decay of Correlations and Witten Laplacians Brascamp–Lieb Inequalities and
Semiclassical Limit. Journal of Functional Analysis, 155(2):571–586, June 1998.
[130] Adrien Saumard and Jon A. Wellner. Log-concavity and strong log-concavity: A review.Statistics Surveys,
8(none):45–114, January 2014.
[131] Arnaud Guillin and Pierre Monmarché. Optimal linear drift for the speed of convergence of an hypoelliptic
diﬀusion. arXiv:1604.07295 [math], October 2021.
[132] Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett, and Michael I. Jordan. Is
There an Analog of Nesterov Acceleration for MCMC?arXiv:1902.00996 [cs, math, stat], October 2019.
[133] Martin Chak, Nikolas Kantas, Tony Lelièvre, and Grigorios A Pavliotis. Optimal friction matrix for under-
damped Langevin sampling. November 2021.
[134] A.B.Duncan, N.Nüsken, andG.A.Pavliotis. UsingPerturbedUnderdampedLangevinDynamicstoEﬃciently
Sample from Probability Distributions.Journal of Statistical Physics, 169(6):1098–1131, December 2017.
[135] J. C. Mattingly, A. M. Stuart, and D. J. Higham. Ergodicity for SDEs and approximations: Locally Lipschitz
vector ﬁelds and degenerate noise.Stochastic Processes and their Applications, 101(2):185–232, October 2002.
[136] Markos Katsoulakis, Yannis Pantazis, and Luc Rey-Bellet. Measuring the Irreversibility of Numerical Schemes
for Reversible Stochastic Diﬀerential Equations.ESAIM: Mathematical Modelling and Numerical Analysis -
Modélisation Mathématique et Analyse Numérique, 48(5):1351–1379, 2014.
[137] Jonathan C. Mattingly, Andrew M. Stuart, and M. V. Tretyakov. Convergence of Numerical Time-Averaging
and Stationary Measures via Poisson Equations.SIAM Journal on Numerical Analysis, 48(2):552–577, January
2010.
[138] T. Radivojević, M. Fernández-Pendás, J. M. Sanz-Serna, and E. Akhmatskaya. Multi-stage splitting integrators
for sampling with modiﬁed hamiltonian monte carlo methods.Journal of Computational Physics, 373:900–916,
2018.
[139] R. M. Neal. Bayesian training of backpropagation networks by the hybrid monte carlo method. Technical
report, Citeseer, 1992.
[140] Eric Cances, Frédéric Legoll, and Gabriel Stoltz. Theoretical and numerical comparison of some sampling
methods for molecular dynamics. ESAIM: Mathematical Modelling and Numerical Analysis, 41(2):351–389,
2007.
[141] Youhan Fang, Jesus-Maria Sanz-Serna, and Robert D Skeel. Compressible generalized hybrid monte carlo.The
Journal of chemical physics, 140(17):174108, 2014.
[142] A. Barp. The bracket geometry of statistics. 2020.
[143] A. Weinstein. The modular automorphism group of a poisson manifold.Journal of Geometry and Physics,
23(3-4):379–394, 1997.
[144] D. D. Holm, J. E. Marsden, and T. S. Ratiu. The euler–poincaré equations and semidirect products with
applications to continuum theories.Advances in Mathematics, 137(1):1–81, 1998.
[145] K. Modin, M. Perlmutter, S. Marsland, and R. McLachlan. Geodesics on lie groups: Euler equations and
totally geodesic subgroup. 2010.
[146] A. Barp. Hamiltonian monte carlo on lie groups and constrained mechanics on homogeneous manifolds. In
International Conference on Geometric Science of Information, pages 665–675. Springer, 2019.
[147] A. Holbrook, S. Lan, A. Vandenberg-Rodes, and B. Shahbaba. Geodesic lagrangian monte carlo over the space
of positive deﬁnite matrices: with application to bayesian spectral density estimation.Journal of statistical
computation and simulation, 88(5):982–1002, 2018.
[148] A. Holbrook, A. Vandenberg-Rodes, and B. Shahbaba. Bayesian inference on matrix manifolds for linear
dimensionality reduction.arXiv preprint arXiv:1606.04478, 2016.
[149] T. Lelièvre, M. Rousset, and G. Stoltz. Hybrid Monte Carlo methods for sampling probability measures on
submanifolds. Numerische Mathematik, 143(2):379–421, 2019.
[150] T. Lelièvre, G. Stoltz, and W. Zhang. Multiple projection mcmc algorithms on submanifolds.arXiv preprint
arXiv:2003.09402, 2020.
[151] M. M. Graham, A. H. Thiery, and A. Beskos. Manifold markov chain monte carlo methods for bayesian inference
in a wide class of diﬀusion models.arXiv preprint arXiv:1912.02982, 2019.
[152] K. X. Au, M. M. Graham, and A. H. Thiery. Manifold lifting: scaling mcmc to the vanishing noise regime.
arXiv preprint arXiv:2003.03950, 2020.
[153] S. Livingstone and M. Girolami. Information-geometric markov chain monte carlo methods using diﬀusions.
Entropy, 16(6):3074–3102, 2014.
[154] M. Tao. Explicit symplectic approximation of nonseparable hamiltonians: Algorithm and long time perfor-
mance. Physical Review E, 94(4):043303, 2016.
37
[155] . D. Cobb, A. G. Baydin, A. Markham, and S. J. Roberts. Introducing an explicit symplectic integration scheme
for riemannian manifold hamiltonian monte carlo.arXiv preprint arXiv:1910.06243, 2019.
[156] C. Predescu, R. A. Lippert, M. P. Eastwood, D. Ierardi, H. Xu, M. Jensen, K. J. Bowers, J. Gullingsrud,
C. A. Rendleman, R. O. Dror, et al. Computationally eﬃcient molecular dynamics integrators with improved
sampling accuracy.Molecular Physics, 110(9-10):967–983, 2012.
[157] S. Blanes, F. Casas, and J. M. Sanz-Serna. Numerical integrators for the hybrid monte carlo method.SIAM
Journal on Scientiﬁc Computing, 36(4):A1556–A1580, 2014.
[158] M. Fernández-Pendás, E. Akhmatskaya, and J. M. Sanz-Serna. Adaptive multi-stage integrators for optimal
energy conservation in molecular simulations.Journal of Computational Physics, 327:434–449, 2016.
[159] C. M. Campos and J. M. Sanz-Serna. Palindromic 3-stage splitting integrators, a roadmap.Journal of Com-
putational Physics, 346:340–355, 2017.
[160] N. Bou-Rabee and J. M. Sanz-Serna. Geometric integrators and the hamiltonian monte carlo method.Acta
Numerica, 27:113–206, 2018.
[161] M. A. Clark, B. Joó, A. D. Kennedy, and P. J. Silva. Improving dynamical lattice qcd simulations through
integrator tuning using poisson brackets and a force-gradient integrator.Physical Review D, 84(7):071502, 2011.
[162] M.B.B.J.M. Tuckerman, B. J. Berne, and G. J. Martyna. Reversible multiple time scale molecular dynamics.
The Journal of chemical physics, 97(3):1990–2001, 1992.
[163] J.C. Sexton and D.H. Weingarten. Hamiltonian evolution for the hybrid monte carlo algorithm.Nuclear Physics
B, 380(3):665–677, 1992.
[164] B. Shahbaba, S. Lan, W. O. Johnson, and R. M. Neal. Split hamiltonian monte carlo.Statistics and Computing,
24(3):339–349, 2014.
[165] P. B. Mackenze. An improved hybrid monte carlo method.Physics Letters B, 226(3-4):369–371, 1989.
[166] M. Betancourt. Identifying the optimal integration time in hamiltonian monte carlo. arXiv preprint
arXiv:1601.00225, 2016.
[167] Z. Wang, S. Mohamed, and N. Freitas. Adaptive hamiltonian and riemann manifold monte carlo. InInterna-
tional conference on machine learning, pages 1462–1470. PMLR, 2013.
[168] A. Durmus, E. Moulines, and E. Saksman. On the convergence of hamiltonian monte carlo.arXiv preprint
arXiv:1705.00166, 2017.
[169] C. M. Campos and J. M. Sanz-Serna. Extra chance generalized hybrid monte carlo.Journal of Computational
Physics, 281:365–374, 2015.
[170] J. Sohl-Dickstein, M. Mudigonda, and M. DeWeese. Hamiltonian monte carlo without detailed balance.Inter-
national Conference on Machine Learning, pages 719–726, 2014.
[171] M. D. Hoﬀman, A. Gelman, et al. The no-u-turn sampler: adaptively setting path lengths in hamiltonian
monte carlo.J. Mach. Learn. Res., 15(1):1593–1623, 2014.
[172] M. Ottobre, N. S. Pillai, F. J. Pinski, and A. M. Stuart. A function space hmc algorithm with second order
langevin diﬀusion limit.Bernoulli, 22(1):60–106, 2016.
[173] F. Heber, Z. Trst’anová, and B. Leimkuhler. Posterior sampling strategies based on discretized stochastic
diﬀerential equations for machine learning applications.Journal of Machine Learning Research, 21(228):1–33,
2020.
[174] A. M. Horowitz. A generalized guided monte carlo algorithm.Physics Letters B, 268(2):247–252, 1991.
[175] J. A. Izaguirre and S. S. Hampton. Shadow hybrid monte carlo: an eﬃcient propagator in phase space of
macromolecules. Journal of Computational Physics, 200(2):581–604, 2004.
[176] T. Radivojević and E. Akhmatskaya. Modiﬁed hamiltonian monte carlo for bayesian inference.Statistics and
Computing, 30(2):377–404, 2020.
[177] H. Strathmann, D. Sejdinovic, S. Livingstone, Z. Szabo, and A. Gretton. Gradient-free hamiltonian monte
carlo with eﬃcient kernel exponential families.arXiv preprint arXiv:1506.02564, 2015.
[178] C. Zhang, B. Shahbaba, and H. Zhao. Hamiltonian monte carlo acceleration using surrogate functions with
random bases. Statistics and computing, 27(6):1473–1490, 2017.
[179] T. Chen, E. Fox, and C. Guestrin. Stochastic gradient hamiltonian monte carlo. InInternational conference
on machine learning, pages 1683–1691. PMLR, 2014.
[180] M. Betancourt. The fundamental incompatibility of scalable hamiltonian monte carlo and naive data subsam-
pling. In International Conference on Machine Learning, pages 533–540. PMLR, 2015.
[181] A.Müller. Integralprobabilitymetricsandtheirgeneratingclassesoffunctions. Advances in Applied Probability,
29(2):429–443, 1997.
[182] N. Aronszajn. Theory of reproducing kernels.Transactions of the American mathematical society, 68(3):337–
404, 1950.
[183] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer
Science & Business Media, 2011.
[184] I. Steinwart and A. Christmann.Support vector machines. Springer Science & Business Media, 2008.
38
[185] A. Barp, C.J. Simon-Gabriel, and L. Mackey. Targeted convergence characteristics of maximum mean discrep-
ancies and kernel Stein discrepancies.In preparation.
[186] B.K.Sriperumbudur, A.Gretton, K.Fukumizu, B.Schölkopf, andG.R.G.Lanckriet. Hilbertspaceembeddings
and metrics on probability measures.The Journal of Machine Learning Research, 11:1517–1561, 2010.
[187] K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Schölkopf. Kernel mean embedding of distributions: A
review and beyond.arXiv preprint arXiv:1605.09522, 2016.
[188] L. Schwartz. Sous-espaces hilbertiens d’espaces vectoriels topologiques et noyaux associés (noyaux repro-
duisants). Journal d’analyse mathématique, 13(1):115–256, 1964.
[189] Carl-Johann Simon-Gabriel and Bernhard Schölkopf. Kernel distribution embeddings: Universal kernels, char-
acteristic kernels and kernel metrics on distributions.The Journal of Machine Learning Research, 19(1):1708–
1736, 2018.
[190] B. K. Sriperumbudur, K. Fukumizu, and G. R.G. Lanckriet. Universality, characteristic kernels and rkhs
embedding of measures.Journal of Machine Learning Research, 12(7), 2011.
[191] C. Carmeli, E. De Vito, A. Toigo, and V. Umanitá. Vector valued reproducing kernel hilbert spaces and
universality. Analysis and Applications, 8(01):19–61, 2010.
[192] C.J. Simon-Gabriel, A. Barp, B. Schölkopf, and L. Mackey. Metrizing weak convergence with maximum mean
discrepancies. arXiv preprint arXiv:2006.09268, 2020.
[193] Stewart N Ethier and Thomas G Kurtz.Markov processes: characterization and convergence, volume 282. John
Wiley & Sons, 2009.
[194] Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. InInternational Conference on
Machine Learning, pages 1292–1301. PMLR, 2017.
[195] C. Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random
variables. InProceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 2:
Probability theory, volume 6, pages 583–603. University of California Press, 1972.
[196] A. Anastasiou, A. Barp, F. Briol, B. Ebner, R. E. Gaunt, F. Ghaderinezhad, J. Gorham, A. Gretton, C. Ley,
Q. Liu, et al. Stein’s method meets statistics: A review of some recent developments. arXiv preprint
arXiv:2105.03481, 2021.
[197] J. M. Lee. Smooth manifolds. InIntroduction to Smooth Manifolds, pages 1–31. Springer, 2013.
[198] C. Liu and J. Zhu. Riemannian stein variational gradient descent for bayesian inference. InProceedings of the
AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[199] L. Hodgkinson, R. Salomone, and F. Roosta. The reproducing stein kernel approach for post-hoc corrected
sampling. arXiv preprint arXiv:2001.09266, 2020.
[200] A. D. Barbour. Stein’s method and poisson process convergence.Journal of Applied Probability, 25(A):175–184,
1988.
[201] J. Gorham, A. B. Duncan, S. J Vollmer, and L. Mackey. Measuring sample quality with diﬀusions.The Annals
of Applied Probability, 29(5):2884–2928, 2019.
[202] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm.
Advances in neural information processing systems, 29, 2016.
[203] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for monte carlo integration.Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 79(3):695–718, 2017.
[204] A.Barp, F.Briol, .Duncan, M.Girolami, andL.Mackey. Minimumsteindiscrepancyestimators. InH.Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019.
[205] W. Y. Chen, A. Barp, F. Briol, J. Gorham, M. Girolami, L Mackey, and C. Oates. Stein point markov chain
monte carlo. InInternational Conference on Machine Learning, pages 1011–1021. PMLR, 2019.
[206] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness of ﬁt. InInternational conference
on machine learning, pages 2606–2615. PMLR, 2016.
[207] Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for goodness-of-ﬁt tests. In International
conference on machine learning, pages 276–284. PMLR, 2016.
[208] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A kernel two-sample test.The Journal
of Machine Learning Research, 13(1):723–773, 2012.
[209] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean
discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
[210] H. Park, S. Amari, and K. Fukumizu. Adaptive natural gradient learning algorithms for various stochastic
models. Neural Networks, 13(7):755–764, 2000.
[211] Y. Chen and W. Li. Natural gradient in wasserstein statistical manifold.arXiv preprint arXiv:1805.08380,
2018.
[212] R. Karakida, M. Okada, and S. Amari. Adaptive natural gradient learning algorithms for unnormalized statis-
tical models. InInternational Conference on Artiﬁcial Neural Networks, pages 427–434. Springer, 2016.
39
[213] S. M. Kakade. A natural policy gradient.Advances in neural information processing systems, 14, 2001.
[214] S. Bonnabel. Stochastic gradient descent on riemannian manifolds.IEEE Transactions on Automatic Control,
58(9):2217–2229, 2013.
[215] M. Leok and J. Zhang. Connecting information geometry and geometric mechanics.Entropy, 19(10):518, 2017.
[216] F. Briol, A. Barp, A. B. Duncan, and M. Girolami. Statistical inference for generative models with maximum
mean discrepancy. 2019. arXiv:1906.05944.
[217] A. Gretton, K. Fukumizu, Z. Harchaoui, and B. K. Sriperumbudur. A fast, consistent kernel two-sample test.
In NIPS, volume 23, pages 673–681, 2009.
[218] Damien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the median heuris-
tic. arXiv preprint arXiv:1707.07269, 2017.
[219] Danica J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and
Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy.arXiv
preprint arXiv:1611.04488, 2016.
[220] Aaditya Ramdas, Sashank J Reddi, Barnabas Poczos, Aarti Singh, and Larry Wasserman. Adaptivity and
computation-statistics tradeoﬀs for kernel and distance based high dimensional two sample testing. arXiv
preprint arXiv:1508.00655, 2015.
[221] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. Mmd gan: Towards deeper
understanding of moment matching network.arXiv preprint arXiv:1705.08584, 2017.
[222] K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain.J. Physiology-Paris, 100(1-3):70–87,
2006.
[223] Thomas Parr.The Computational Neurology of Active Vision. PhD thesis, University College London, London,
2019.
[224] Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlo methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
[225] Karl Friston, Conor Heins, Kai Ueltzhöﬀer, Lancelot Da Costa, and Thomas Parr. Stochastic Chaos and
Markov Blankets.Entropy, 23(9):1220, September 2021.
[226] Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöﬀer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simple.arXiv:2201.06387 [cond-mat, physics:nlin,
physics:physics, q-bio], January 2022.
[227] Lancelot Da Costa, Karl Friston, Conor Heins, and Grigorios A. Pavliotis. Bayesian mechanics for sta-
tionary processes. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
477(2256):20210518, December 2021.
[228] Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated Inference.
Neural Computation, 33(3):713–763, February 2021.
[229] Thomas Parr, Lancelot Da Costa, Conor Heins, Maxwell James D. Ramstead, and Karl J. Friston. Memory
and Markov Blankets.Entropy, 23(9):1105, September 2021.
[230] Samuel Tenka. personal communication.
[231] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin Kronbichler,
and Karl J Friston. Computational mechanisms of curiosity and goal-directed exploration.eLife, page 45, 2019.
[232] David M. Blei, Alp Kucukelbir, and Jon D. McAuliﬀe. Variational Inference: A Review for Statisticians.
Journal of the American Statistical Association, 112(518):859–877, April 2017.
[233] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk.Econometrica,
47(2):263–291, 1979.
[234] Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.
arXiv:1805.00909 [cs, stat], May 2018.
[235] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On Stochastic Optimal Control and Reinforcement
Learning by Approximate Inference. InTwenty-Third International Joint Conference on Artiﬁcial Intelligence,
June 2013.
[236] Marc Toussaint. Robot trajectory optimization using approximate inference. InProceedings of the 26th Annual
International Conference on Machine Learning, ICML ’09, pages 1049–1056, Montreal, Quebec, Canada, June
2009. Association for Computing Machinery.
[237] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems.Journal of Basic Engineering,
82(1):35–45, March 1960.
[238] Emanuel Todorov. General duality between optimal control and estimation. In2008 47th IEEE Conference on
Decision and Control, pages 4286–4292, December 2008.
[239] HilbertJ. Kappen, Vicenç Gómez, andManfred Opper. Optimalcontrolas agraphical modelinference problem.
Machine Learning, 87(2):159–182, May 2012.
[240] B. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.PhD
thesis, Carnegie Mellon University, Pittsburgh, 2010.
40
[241] E. T. Jaynes. Information Theory and Statistical Mechanics.Physical Review, 106(4):620–630, May 1957.
[242] Andrzej Lasota and Michael C. MacKey.Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics. Springer-
Verlag, 1994.
[243] David J. C. MacKay.Information Theory, Inference and Learning Algorithms. Cambridge University Press,
Cambridge, UK ; New York, sixth printing 2007 edition edition, September 2003.
[244] D. V. Lindley. On a Measure of the Information Provided by an Experiment.The Annals of Mathematical
Statistics, 27(4):986–1005, 1956.
[245] David J. C. MacKay. Information-Based Objective Functions for Active Data Selection.Neural Computation,
4(4):590–604, July 1992.
[246] Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Computational
Approaches. Frontiers in Neurorobotics, 1:6, November 2007.
[247] Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010).IEEE Trans-
actions on Autonomous Mental Development, 2(3):230–247, September 2010.
[248] A. Barto, M. Mirolli, and G. Baldassarre. Novelty or Surprise?Frontiers in Psychology, 4, 2013.
[249] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration
in Dynamic Environments.arXiv:1103.5708 [cs, stat], March 2011.
[250] Edward Deci and Richard M. Ryan.Intrinsic Motivation and Self-Determination in Human Behavior. Per-
spectives in Social Psychology. Springer US, New York, 1985.
[251] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention.Vision research, 49(10):1295–1306,
May 2009.
[252] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative Models for
Active Vision.Frontiers in Neurorobotics, 15, 2021.
[253] H. B. Barlow.Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961.
[254] R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information
Theory. Annual Review of Neuroscience, 13(1):257–281, 1990.
[255] L. M. Optican and B. J. Richmond. Temporal encoding of two-dimensional patterns by single units in primate
inferiortemporalcortex.III.Informationtheoreticanalysis. Journal of Neurophysiology, 57(1):162–178, January
1987.
[256] Richard E. Bellman.Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957.
[257] K. J Åström. Optimal control of Markov processes with incomplete state information.Journal of Mathematical
Analysis and Applications, 10(1):174–205, February 1965.
[258] Abraham Kaplan. The Conduct of Inquiry. Transaction Publishers, 1973.
[259] Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, Bayesian optimal design,
and expected utility.arXiv:2110.04074 [cs, math, stat], September 2021.
[260] Oded Berger-Tal, Jonathan Nathan, Ehud Meron, and David Saltz. The Exploration-Exploitation Dilemma:
A Multidisciplinary Framework.PLOS ONE, 9(4):e95693, April 2014.
[261] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl Friston, Iain Couzin, and Alexander
Tschantz. Pymdp: A Python library for active inference in discrete state spaces.arXiv:2201.03904 [cs, q-bio],
January 2022.
[262] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its
application to empirical data.Journal of Mathematical Psychology, 107:102632, April 2022.
[263] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka.
Active Inference, Curiosity and Insight.Neural Computation, 29(10):2633–2683, October 2017.
[264] Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierar-
chical active inference.Neural Networks, 142:192–204, October 2021.
[265] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and
active inference.Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018.
[266] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, New York, 2006.
[267] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active
Inference: A Process Theory.Neural Computation, 29(1):1–49, January 2017.
[268] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O’Doherty, and Giovanni
Pezzulo. Active inference and learning.Neuroscience & Biobehavioral Reviews, 68:862–879, September 2016.
[269] Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An Active Inference Approach to
ModelingStructureLearning: ConceptLearningasanExampleCase. Frontiers in Computational Neuroscience,
14, May 2020.
[270] Karl Friston, Rosalyn J. Moran, Yukie Nagai, Tadahiro Taniguchi, Hiroaki Gomi, and Josh Tenenbaum. World
model learning and inference.Neural Networks, 144:573–590, December 2021.
41
[271] Samuel T Wauthier, Ozan Çatal, Tim Verbelen, and Bart Dhoedt. Sleep: Model Reduction in Deep Active
Inference. page 13, 2020.
[272] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models through active
inference. PLOS Computational Biology, 16(4):e1007805, April 2020.
[273] Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian model reduction.arXiv:1805.07092 [stat], October
2019.
[274] Théophile Champion, Howard Bowman, and Marek Grześ. Branching Time Active Inference: Empirical study
and complexity class analysis.arXiv:2111.11276 [cs], November 2021.
[275] Théophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grześ. Branching Time Active Infer-
ence: The theory and its generality.arXiv:2111.11107 [cs], November 2021.
[276] Domenico Maisto, Francesco Gregoretti, Karl Friston, and Giovanni Pezzulo. Active Tree Search in Large
POMDPs. arXiv:2103.13860 [cs, math, q-bio], March 2021.
[277] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. The relationship between
dynamic programming and active inference: The discrete, ﬁnite-horizon case. arXiv:2009.08111 [cs, math,
q-bio], September 2020.
[278] Aswin Paul, Lancelot Da Costa, Manoj Gopalkrishnan, and Adeel Razi. Active Inference for Stochastic and
Adaptive Control in a Partially Observable Environment.
[279] Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in Variational Inference.
arXiv:1711.05597 [cs, stat], November 2017.
[280] Thijs W. van de Laar and Bert de Vries. Simulating Active Inference Processes by Message Passing.Frontiers
in Robotics and AI, 6, 2019.
[281] Noor Sajid, Emma Holmes, Lancelot Da Costa, Cathy Price, and Karl Friston. A mixed generative model of
auditory word repetition, January 2022.
[282] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Control as Hybrid Inference.
arXiv:2007.05838 [cs, stat], July 2020.
[283] John Winn and Christopher M Bishop. Variational Message Passing.Journal of Machine Learning Research,
page 34, 2005.
[284] M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational Inference.Found.
Trends in Mach. Learn., 1(1–2):1–305, 2007.
[285] Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. Neuronal message passing using
Mean-ﬁeld, Bethe, and Marginal approximations.Scientiﬁc Reports, 9(1):1889, December 2019.
[286] Sarah Schwöbel, Stefan Kiebel, and Dimitrije Marković. Active Inference, Belief Propagation, and the Bethe
Approximation. Neural Computation, 30(9):2530–2567, September 2018.
[287] Théophile Champion, Marek Grześ, and Howard Bowman. Realizing Active Inference in Variational Message
Passing: The Outcome-Blind Certainty Seeker.Neural Computation, 33(10):2762–2826, September 2021.
[288] Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4):381–414, December 2017.
[289] Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology of movement
under active inference.Brain, March 2021.
42