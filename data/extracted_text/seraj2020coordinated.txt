Coordinated Control of UA Vs for Human-Centered Active Sensing of
Wildﬁres
Esmaeil Seraj1 and Matthew Gombolay 1
Abstract— Fighting wildﬁres is a precarious task, imperiling
the lives of engaging ﬁreﬁghters and those who reside in the
ﬁre’s path. Fireﬁghters need online and dynamic observation of
the ﬁrefront to anticipate a wildﬁre’s unknown characteristics,
such as size, scale, and propagation velocity, and to plan
accordingly. In this paper, we propose a distributed control
framework to coordinate a team of unmanned aerial vehicles
(UA Vs) for a human-centered active sensing of wildﬁres. We
develop a dual-criterion objective function based on Kalman
uncertainty residual propagation and weighted multi-agent
consensus protocol, which enables the UA Vs to actively infer the
wildﬁre dynamics and parameters, track and monitor the ﬁre
transition, and safely manage human ﬁreﬁghters on the ground
using acquired information. We evaluate our approach relative
to prior work, showing signiﬁcant improvements by reducing
the environments cumulative uncertainty residual by more than
102 and 105 times in ﬁrefront coverage performance to support
human-robot teaming for ﬁreﬁghting. We also demonstrate our
method on physical robots in a mock ﬁreﬁghting exercise.
I. I NTRODUCTION
Fighting wildﬁres is a dangerous task and requires accurate
online information regarding ﬁrefront location, size and
scale, shape, and propagation velocity [1]. Fireﬁghters may
lose their lives as a consequence of inaccurately anticipating
information either due to inherent stochasticity in ﬁre behav-
ior or low-quality and unusable information provided, such
as low-resolution satellite images [2], [3]. Fireﬁghters need
frequent, high-quality images to monitor the ﬁre propagation
and plan accordingly (Fig. 1). Due to recent advances in
aerial robotic technology, UA Vs have been proposed as a
solution to overcoming the challenges of needing real-time
information in ﬁghting ﬁres [4].
In [1], a cooperative approach is proposed to detect local
ﬁre areas in a wildﬁre using two groups of detector and ser-
vice UA V agents. In [5], utility of visual and infrared cameras
on heterogeneous UA Vs with hovering capabilities is investi-
gated to monitor the evolution of the ﬁrefront shape. In [6], a
leader-follower-based distributed framework is proposed for
a team of UA Vs to evenly distribute and track an elliptical ﬁre
perimeter. In [7], a heat-intensity-based distributed control
framework is designed for a team of UA Vs to be capable of
closely monitoring a wildﬁre in open space. More recently,
both model-based (i.e., Kalman estimation) and learning-
based (deep convolutional neural network) methods have
been used for cooperative prediction and tracking of the
ﬁrefront shape [8], [9], [10]. Additionally, other learning-
based approaches, such as reinforcement learning (RL), have
1Institute for Robotics and Intelligent Machines, Georgia Institute
of Technology, Atlanta, GA 30332, USA eseraj3@gatech.edu,
matthew.gombolay@cc.gatech.edu
Fig. 1: Fireﬁghters trying to control a back burn as the
Carr ﬁre spreads toward Douglas City. Courtesy of the Los
Angeles Times (May 2019).
also been applied to this problem to enable collaborative
monitoring of wildﬁres [11], [12], which can be enabled for
online data processing through NN pruning approaches [13].
Many of the aforementioned studies require an accurate
function for ﬁre-shape [10] to work, assuming an enlarging
elliptical perimeter to be monitored by UA Vs [1], [2], [6].
Supposing a shape model for large-scale wildﬁres is not
realistic and thus not accurate [14]. While vision-based
approaches are still struggling with ﬁre smoke elimination
and image stabilization problems [15], other approaches,
such as [7], require a heat-intensity model and are dependent
on an accurate estimation of maximum heat-intensity within
the entire ﬁre-map. Additionally, [16] notes that RL and
learning-based methods are prone to major drawbacks, such
as scalability and domain shift problems as well as lack of
formal guarantees on boundedness of errors, which are sig-
niﬁcant for safety-critical applications, such as ﬁreﬁghting.
There is a clear absence of human-centric approaches in
the literature for UA V teams for active sensing of wildﬁre and
ﬁre monitoring. This is mainly because a majority of previous
studies are solely focused on autonomous ﬁre detection
and surveilling a large burning area by drones rather than
focusing on local human-deﬁned areas of priority (areas of
ﬁreﬁghter activity) and serving ﬁreﬁghters. In this study, we
seek a better control strategy, toward a human-centered robot
coordination, through better perception and accurate local
situational awareness. We overcome key limitations in prior
work by developing an algorithmic framework to provide a
model-predictive mechanism that enables ﬁreﬁghters on the
ground to receive online, high-quality information regarding
their time-varying proximity to a ﬁre.
arXiv:2006.07969v1  [eess.SY]  14 Jun 2020
In our approach, we explicitly estimate the latent ﬁre
propagation dynamics and parameters via an adaptive ex-
tended Kalman ﬁlter (AEKF) predictor and the simpliﬁed
FARSITE wildﬁre propagation model [17] to account for
ﬁreﬁghter’s safety and provide them with online information
regarding propagating ﬁrefronts. This model allows us to
develop straightforward distributed control adapted from
vehicle routing literature [18] to enable track-based ﬁre cov-
erage. Moreover, a mathematical observation model through
which UA V sensors observe ﬁre is derived to map from
state space to observation space. The calculated models are
then used in combination to derive a dual-criteria objective
function in order to control a ﬂeet of UA Vs. The proposed
dual-criteria objective is an ad hoc, well-suited function to
the wildﬁre monitoring task, which minimizes environment’s
uncertainty on local, human-centered areas (ﬁrst criteria)
and maximizes coverage through ensemble-level formation
control of the robot network (second criteria).
We empirically evaluate our approach against simulated
wildﬁres alongside contemporary approaches for UA V cov-
erage [7] as well as against a reinforcement learning baseline,
demonstrating a promising utility of our approach. Our pro-
posed coordinated controller is capable of reducing the cu-
mulative uncertainty residual of the ﬁre environment by more
than 10 2 and 10 5 times in ﬁrefront coverage performance
to support human-robot teaming for ﬁreﬁghting. We also
assess the feasibility of our method through implementation
on physical robots in a mock ﬁreﬁghting scenario.
II. P RELIMINARIES
In this section, we ﬁrst introduce the simpliﬁed FARSITE
wildﬁre propagation mathematical model and calculate the
ﬁre dynamics. We then review the fundamentals of AEKF.
A. Fire Area Simulator (FARSITE): Fire Propagation Model
The Fire Area Simulator (FARSITE) wildﬁre propagation
model was ﬁrst introduced by Finney et al [17], which is
now widely used by the USDI National Park Service, USDA
Forest Service, and other land management agencies at the
federal and state levels. The model has been utilized to
simulate the spread of a wildﬁre, factoring in heterogeneous
conditions of terrain, fuels, and weather and their inﬂuence
on ﬁre dynamics. The full FARSITE model includes complex
equations, and precise model implementation requires a
signiﬁcant amount of geographical and physical information
on terrain, fuels, and weather. As such, researchers tend
to modify the model by considering few simplifying as-
sumptions [7]. The wildﬁre propagation dynamics using a
simpliﬁed FARSITE model are shown in Eq. 1 and 2.
qi
t = qi
t−1 + ˙qi
t−1δt (1)
˙qi
t = d
dt
(
qi
t
)
(2)
In the above equations, qi
t indicates the location of ﬁrefront i
at time t, and ˙qi
t is its growth rate (i.e., propagation velocity).
˙qt is a function of ﬁre spread rate ( Rt ), wind speed (Ut ), and
wind azimuth (θt ), which are available to our system through
weather forecasting equipment. By ignoring the superscript
i in Eq. 1 and 2 and without losing generality, ˙ qt can be
estimated for each propagating ﬁrefront by Eq. 3-4, where ˙qx
t
and ˙qx
t are ﬁrst-order ﬁrefront dynamics for X and Y axes [17]
˙qx
t = C(Rt ,Ut )sin(θt ) (3)
˙qy
t = C(Rt ,Ut )cos(θt ) (4)
where C(Rt ,Ut ) =Rt
(
1 − LB(Ut )
LB(Ut )+
√
GB(Ut )
)
in which LB(Ut ) =
0.936e0.256Ut +0.461e−0.154Ut −0.397 and GB(Ut ) =LB(Ut )2 −1.
B. Adaptive Extended Kalman Filter (AEKF)
We utilize an adaptive extended Kalman ﬁlter (AEKF) to
leverage the mathematical ﬁre propagation model and the ob-
servation model of a ﬂying drone with respect to a dynamic
object on the ground to actively sense the ﬁre-spots, infer
wildﬁre dynamics and parameters, and propagate all sources
of measurement uncertainty. In a conventional extended
Kalman ﬁlter (EKF), process and observation noise covari-
ances Qt and Γt are often chosen as constant matrices based
on the state-transition model and sensor accuracy and do not
receive updates. However, this selection process is highly
sensitive to user experience and can be extremely inaccurate.
We leverage AEKF [19], which introduces innovation and
residual-based updates for process and observation noise
covariances, as shown in Eq. 5-6, where α is a forgetting
factor and dt is the measurement innovation and is deﬁned
as the difference between the actual measurement and its
predicted value. Moreover, Kt is the Kalman gain and Ht is
the observation Jacobian matrix.
Qt = αQt−1 +(1 −α)
(
Kt dt dT
t KT
t
)
(5)
Γt = αΓt−1 +(1 −α)
(
˜yt ˜yT
t +Ht Pt|t−1HT
t
)
(6)
These adaptive updates remove the assumption of constant
covariances Qt and Γt and enable even more accurate predic-
tions over time as the Kalman ﬁlter leverages its observations
to improve the predicted covariance matrix Pt|t−1 [19].
III. P ROBLEM STATEMENT AND ALGORITHMIC
OVERVIEW
The focus of our study includes two important aspects of
wildﬁre ﬁghting: (1) providing high-quality information on
ﬁrefront status while accounting for physical and method-
ological errors and (2) human-centered coverage and tracking
of wildﬁre to account for ﬁreﬁghter safety. Accordingly, we
deﬁne high-quality information as high-resolution and online
images of areas prioritized by humans. We take advantage of
the estimated uncertainty of the environment to achieve these
objectives. Through a uniﬁed error propagation system, not
only can the physical and methodological uncertainties be
leveraged to manage the human teams and account for their
safety, they can also be used to manage the UA V team, both
in node-level and ensemble-level dynamics. An AEKF is a
proper candidate for the uncertainty propagation system here
since it can accumulate physical and methodological errors
and generate a cumulative error map through the calculated
probability distribution and predicted covariance matrix.
Accordingly, UA Vs initially calculate two uncertainty
maps: (1) a ﬁrefront uncertainty map (Section IV-A.1) and
(2) a human uncertainty map (Section IV-A.2). The latter is
generated through a bimodal distribution of human locations
as received by GPS devices while the ﬁrst map is created
by the AEKF’s online inference of ﬁrefront locations and
error propagation (Section V). Through the combination
of these two error-maps, we obtain our ﬁrst node-level
controller (uncertainty-based controller, Section IV-A). We
also incorporate an ensemble-level (formation) controller to
encourage the UA V team to maintain a formation consensus
for maximizing the coverage (Section VI-B). The two con-
trollers coordinate to generate a virtual position for each UA V
which is then fed to a path planning controller to generate the
force required to move the UA V to the determined position
based on UA Vs ﬂight dynamics (Section VI).
IV. M ETHOD
Algorithm 1 depicts an overview of the proposed human-
centered coordinated control procedure for monitoring wild-
ﬁres. Upon receiving a request, UA Vs travel to the human-
deﬁned areas of interest (i.e., rendezvous point, line 1 in
Algorithm 1). On arrival, UA Vs sense the ﬁrefront by ex-
trapolating ﬁre-spots qt and generate a general uncertainty
map U tot
t by fusing AEKF error propagation and areas of
human activity using GPS data (line 3-5 in Algorithm 1).
Afterwards, a combination of an uncertainty-based optimiza-
tion and a graph-based weighted consensus protocol forms
our new dual-criteria objective function H for a set of N
UA Vs (line 6 in Algorithm 1). This objective function is
then embedded as our coordinated control system to move
UA Vs to highly uncertain areas on the generated error map
to minimize the associated error while encouraging drones
to maintain a distributed formation to increase the team
efﬁciency in ﬁeld coverage (lines 7-8 in Algorithm 1). Mean-
while, AEKF is also used to infer the ﬁrefront characteristics,
such as spatial distribution ˆqt , propagation velocity ˙qt , and
direction , in order to calculate an individualized temporal
safety index (SI) (Eq. 40) for ﬁreﬁghters.
minH = argmax
qit ,pdt
(∫
i∈Q
U tot
t
(
qi
t
)
dq −
∫
d∈N
Ed j
(pd
t −pj/d
t

)
d p
)
(7)
where pd
t represents UA V positions and Q is the entire ﬁre-
map. To generate the required control inputs for UA Vs to
move, we calculate the negative derivative of the objective
function with respect to the location of drones at time t.
The following sections are dedicated to discussing and
formulating the two modules of the dual-criteria objective
function in Eq. 7, as well as elaborating on the uncertainty
map generation process.
A. Criteria 1: Uncertainty-based Controller
We design our coverage and tracking controller to minimize
the uncertainty of the ﬁrefront locations over time, while
focusing on the areas of human operation. To this end,
we generate two uncertainty maps for (1) the propagating
ﬁre locations Uf and (2) the areas of human activity Uh.
Eventually, we fuse these error maps together by linearly
Algorithm 1: Stages of the proposed human-centered
coordinated control for a team of UA Vs.
input : Obtain the rendezvous area pr, ﬁre-map Qt , human GPS data
ph
t , UA V positions pd
t , and ﬁre propagation model M
1 Move to rendezvous area: pd
t ←Move(pr, pd
t−1)
2 while MissionDuration do
3 Generate ﬁrefront uncertainty map: U f
t ,qt ←Sense(Qt )
4 Generate human uncertainty map: U h
t ←GPS(qh)
5 Combine uncertainty maps: U tot
t = U f
t +U h
t
6 Minimize the dual-criteria objective function:
minH = argmax
qit ,pdt
(∫
i∈Q
U tot
t
(
qi
t
)
dq −
∫
d∈N
Ed j
(pd
t −pj/d
t

)
d p
)
7 Calculate the overall control inputs and determine new virtual positions
for UA Vs:
pn
t+δt = pv
t −
(
uucc
d −uf cc
d
)
δt
8 Move to the new desired position: pd
t+δt ←Move(pn
t+δt , pd
t )
9 end
10 def Move(pd
g , pd
t ): // pd
g is the goal position for drone d
11 ud,t = ∑i uatti
d,t (pd
g , pd
t )+ urepi
d,t (pd
g , pd
t ), ∀i ∈Ft
12 pd
t+δt = pd
t +ud,t δt
13 def Sense(Qt ): // Ot is the UA V observation model
14 ˆqt = argmaxqt ρ
(
qt|t−1, pt−1,Mt−1,Ot−1
)
summing up the respective estimated uncertainty values of
each point to obtain the general uncertainty map U tot
t at time
t. The uncertainty-based controller’s objective is to minimize
the overall uncertainty residual in U tot
t . We present the
details of calculating Uf and Uh in the following sections.
Fig. 2 demonstrates the formation of the uncertainty map and
the foundation of our human-centered controller.
1) Firefront Uncertainty Map: We leverage AEKF to es-
timate a probability distribution of the ﬁre-spot locations and
compute a measurement covariance for each point through
linear error propagation techniques. Considering qt−1 as the
location of ﬁrefronts at current time and pd
t−1 as the UA V
coordinates, a ﬁrefront location ˆqt one step forward in time
is desired, given the current ﬁrefront distribution ( qt−1), ﬁre
propagation model with current parameters (Mt−1), and UA V
observation model of the ﬁeld ( Ot−1) as in Eq. 8
ˆqt = argmax
qt
ρ (qt−1, pt−1,Mt−1,Ot−1,qt ) (8)
In AEKF, the uncertainty of the ﬁrefront locations over
time is measured as the state covariance Pt|t at time t. It
has been shown previously (see [20]) that minimizing the
state covariance corresponds to maximizing the covariance
residual St in Eq. 9
St = Ht Pt|t−1HT
t +Γt (9)
where Pt|t−1 is the predicted covariance, Ft and Ht are process
and observation model Jacobians, and Qt and Γt are the
corresponding noise covariances and can be calculated as
Pt|t−1 = Ft Pt−1|t−1FT
t + Qt . According to Eq. 9, by setting
pt|t−1 to identity, we see that a maximally informative
position for drones is the one that minimizes the Ht HT
t , or
in other words, the closest possible position where dynamic
observations change rapidly [20]. As such, we generate an
uncertainty map which is reﬂective of the wildﬁre dynamics
where a measurement residual can be calculated for each
Fig. 2: Fusing AEKF measurement residual and human GPS
data to generate an uncertainty map. UA Vs focus on areas of
human activity while closely monitoring the ﬁre propagation.
point qt by summing up the estimated covariance residual
matrix St and set our objective to maximize St . Accordingly,
we derive our new objective function H as in Eq. 10, where
Tr(.) represents the trace operation, and f ovd
t is the ﬁeld of
view (FOV) of drone d at time t.
minU1 = argmax
qit ∈f ovdt
(Tr(St )) =argmax
qit ∈f ovdt
(
Tr
(
Ht Pt|t−1HT
t +Γt
))
= argmax
qit ∈f ovdt
(
Tr
(
Ht
(
Ft Pt−1|t−1FT
t +Qt
)
HT
t +Γt
))
(10)
Details of online inference of the parameters in the above
equation are presented in Section V.
2) Human Uncertainty Map: While hovering around the
highly uncertain areas to provide ﬁreﬁghters with online
information regarding the ﬁrefront, UA Vs are required to
focus on their human collaborators on the ground and take
their safety into account by putting additional concentration
on the areas of human operation. Accordingly, UA Vs receive
human positions ph
t (i.e., through GPS devices) at time t
as planar coordinates µx
t,h and µy
t,h and generate a bimodal
Gaussian distribution for each human h to account for both
error in GPS information as well as the mobility of the
humans. By assuming independence, a joint PDF can be
calculated as our human safety objective as in Eq. 11
U2 = argmax
qt ∈vh
∏
i∈vh
Pih
{
qi
t −ph
t ≥rs
}
≥Ps (11)
where vj represents the points in a safe circular vicinity
of human h with radius rs and Ps is a predeﬁned safety
threshold for probability. Pih is a cumulative distribution
function (CDF) with respect to each human location ph
t and
all approaching ﬁrefronts qi
t . To calculate the Pih, we leverage
the estimated ﬁre-spot locations qt alongside inferred ﬁre
parameters (i.e., ˆRt , ˆUt , and ˆθt ) by AEKF to calculate a CDF
for each human at location ph
t and all approaching ﬁrefronts
qi
t . We then integrate the resulting CDF to be greater than the
safe-distance. A Probability Pih is then calculated for each
individual as in Eq. 12
Pih =
∫ ∞
τ=ls
N
(
µqit
−ph
t ,σ2
t,i
)
dτ = 1 −CDF
(
rs|µqit
−ph
t ,σ2
t,i
)
(12)
where µqit
and σ2
t,i are calculated by AEKF (see Section V).
Eq. 11 is leveraged later in Section VII-A to compute an
individualized safety index for each ﬁreﬁghter.
B. Criteria 2: Weighted Multi-agent Consensus Protocol
To ensure that the combination of the above objective
functions results in local actions leading up to appropriate
global performance, we enforce an extra control term in such
a way that the UA Vs also act on other easily measurable
information, such as the relative displacements to neighbor-
ing drones. This is speciﬁcally important to disperse UA Vs,
while preserving the connectedness of the network, from
converging to an extreme minima (i.e., a highly uncertain
point). Accordingly, we leverage the weighted consensus
protocol [21] as in Eq. 13 for a set of N UA Vs with the
objective of minimizing the total displacement error Ei j while
preserving a distance of at least δ between all UA Vs
minE = argmin
pt
Ei j
(pi
t −pj
t

)
(13)
= argmin
pt
N
∑
i=1
∑
j∈Vi
1
2(∆−δ)


pit −pj
t
−δ
∆−
pit −pj
t



2
(14)
where j ∈Vi represents j−th UA V within the communication
range ∆ of UA V i. In this way, a negative force will be
generated to move UA Vs apart from or closer to each other if
they are getting closer than δ or farther than ∆ (i.e., the UA V
network will become disconnected). Note that δ should be
set high enough so that the UA V team can spread effectively.
V. O NLINE INFERENCE OF WILDFIRE DYNAMICS
The joint probability density function in Eq. 8 is calculated
through AEKF estimator. Using the aforementioned nota-
tions, the AEKF state transition and observation equations
can now be stated as in Eq. 15 and 16
qt = ft−1 (qt−1, pt−1,Rt−1,Ut−1,θt−1)+ ωt (15)
ˆqt =ht (qt , pt−1)+ νt (16)
where pt−1 is the physical location of UA V . We reform the
state transition Eq. in 15 to account for all state variables in
Θt =
[
qx
t ,qy
t , px
t , py
t , pz
t ,Rt ,Ut ,θt
]T as in Eq. 17[
Θt
]
8×1
=
[
∂ f
∂Θi
⏐⏐⏐⏐ˆΘt−1|t−1
]
8×8
[
Θt−1
]
8×1
+ωt , ∀i ∈Θ (17)
where the process noise ωt . Therefore, we form the state
transition Jacobian matrices Ft as in Eq. 18, including partial
derivatives of wildﬁre propagation dynamics in Eq. 1 and 2
with respect to all variables in state vector Θt .
∂ f
∂Θi
⏐⏐⏐⏐ˆΦt′
=
qx
t′ qy
t′ p(3)
t′ Rt′ Ut′ θt′




qxt 1 0 0 (3) ∂qx
t
∂Rt′
∂qx
t
∂Ut′
∂qx
t
∂θt′
qy
t 0 1 0 (3) ∂qy
t
∂Rt′
∂qy
t
∂Ut′
∂qy
t
∂θt′
p(3)
t 0 0 0 (3) 0 0 0
Rt 0 0 0 (3) 1 0 0
Ut 0 0 0 (3) 0 1 0
θt 0 0 0 (3) 0 0 1
(18)
where t′= t −1 and superscript (3) represent number of
column and row repetitions for all
[
px
t , py
t , pz
t
]
. We note that
the parameters Rt , Ut , and Ut are not necessarily dynamic
with time, and it is fairly reasonable to consider these
Fig. 3: The UA V observation model.
physical parameters as constants for short periods of time.
However, in the case of analyzing the system for longer
durations, temporal dynamics may apply [22], speciﬁcally
due to changes in wind speed and velocity. Exact estimation
of temporal dynamics related to these parameters are out
of the scope of the current study, since we assume locality
in time and space according to FARSITE [17]. The partial
derivatives of qx
t and qy
t with respect to parameters Rt−1,
Ut−1, and θt−1 are computed by applying the chain-rule and
using Eq. 3-4 as shown in Eq. 19-21, where L(θ) equals
sinθ and cos θ for X and Y axis, respectively.
∂qt
∂θt−1
= C(Rt ,Ut )∂L(θ)
∂θ δt (19)
∂qt
∂Rt−1
=
(
1 − LB(Ut )
LB(Ut )+
√
GB(Ut )
)
L(θ)δt (20)
∂qt
∂Ut−1
=
Rt′
(
LB(Ut′)∂GB(Ut′)
∂Ut′ −GB(Ut′)∂LB(Ut′)
∂Ut′
)
(
LB(Ut′)+
√
GB(Ut′)
)2 L(θ)δt (21)
Next, we derive the observation model through which
UA Vs perceive dynamic ﬁre-spots, according to Fig. 3. The
observation mapping Eq. in 16 is reformed into Eq. 22
[
ˆΦt
]
5×1
=
[
∂h
∂Θi
⏐⏐⏐⏐
Φt|t
]
5×8
[
Φt
]
8×1
+νt , ∀i ∈Θ (22)
where Φt =
[
ϕx
t ,ϕy
t , ˆRt , ˆUt , ˆθt
]T
is a mapping vector through
which the estimated parameters are translated into a uniﬁed,
observed angle-parameter vector ˆΦt . The angle parameters
(i.e., ϕx
t and ϕy
t ) contain information regarding both ﬁrefront
location [qx
t ,qy
t ] and UA V coordinates [px
t , py
t ]. According
to Fig. 3, by projecting the looking vector of UA V to
planar coordinates, the angle parameters are calculated as
ϕxt = tan−1
(qx
t −px
t
pz
t
)
and ϕy
t = tan−1
(
qy
t −py
t
pz
t
)
for X and Y axes,
respectively. Then, the observation Jacobian matrix Ht is
calculated as in Eq. 23
∂h
∂Θi
⏐⏐⏐⏐ˆΘt|t′
=
qx
t qy
t px
t py
t pz
t Rt Ut θt




ϕx
t
∂ϕ xt
∂qxt
0 ∂ϕ xt
∂ pxt
0 ∂ϕ xt
∂ pzt
0 0 0
ϕx
t 0
∂ϕ y
t
∂qy
t
0
∂ϕ y
t
∂ py
t
∂ϕ y
t
∂ py
t
0 0 0
ˆRt 0 0 0 0 0 1 0 0
ˆUt 0 0 0 0 0 0 1 0
ˆθt 0 0 0 0 0 0 0 1
(23)
where the partial derivatives are derived as in Eq. 24-26,
using the aforementioned angle parameter equations
∂ϕ xt
∂qxt
= 1
1 +
(
qxt −pxt
pz
t
)2
(1
pz
t
)
(24)
∂ϕ xt
∂ pxt
= 1
1 +
(
qxt −pxt
pz
t
)2
(−1
pz
t
)
(25)
∂ϕ xt
∂ pz
t
= 1
1 +
(
qxt −pxt
pz
t
)2 (qx
t −pz
t )
( −1
(pz
t )2
)
(26)
similar equations as above hold for Y-axis with qy
t and py
t .
The process noise ωt in Eq. 17 accounts for both stochas-
ticity in ﬁre behavior and wildﬁre propagation model in-
accuracy. Moreover, the observation noise νt is responsible
to account for the estimation errors associated with both qt
and pd
t which affect UA Vs ability to extrapolate where a
ﬁre is on the ground. Taking this into consideration is very
important. Both ωt and νt are modeled as a zero-mean white
Gaussian noise with covariances Qt and Γt , respectively.
Note that errors in X, Y , and Z axes coordinates of a
drone are loosely correlated, and thus, we also incorporate
non-diagonal elements in noise covariance matrices when
initializing them. Qt and Γt then receive adaptive updates
according to AEKF framework, as in Eq. 5 and 6.
VI. C ONTROLLER DESIGN
Fig. 4 represents our node-level controller architecture for
each UA V d with neighboring UA Vs pi/d
t . Our controller
consists of three components: (1) an uncertainty-based con-
trol component (UCC), (2) a formation control component
(FCC), and (3) a path planning component (PPC). The
ﬁrst controller performs exploitation to minimize the overall
uncertainty in the map as produced (i.e., ﬁrefront locations
or human areas of activity) while the second controller
is designed to manage the general formation of the UA V
swarm in order to maximize exploration as well as coverage.
The third controller component moves UA Vs to any desired
next position (i.e., to rendezvous point, to human location
for close monitoring, or to next monitoring positions as
determined by dual-criteria controller). Assuming ud = ˙pd
to be the quadcopter UA V dynamics, we develop each of
our control components in the following sections.
A. Uncertainty Controller Component (UCC)
The ﬁrst controller works based on the theory of artiﬁcial po-
tential ﬁeld [23] where each UA V is distributedly controlled
by a negative gradient of the generated total uncertainty map
Fig. 4: Node-level controller architecture of each drone d.
U tot
t from objective functions in Eq. 10 and 13, with respect
to its position pd
t =
[
px
t , py
t , pz
t
]T as follows in Eq. 27
uucc
d = −κ1
∂U tott
∂ pd
(27)
where κ1 is the proportional gain parameter for the ﬁrst
controller. To derive the gradients with respect to the UA V
coordinates, we ﬁrst need to analytically derive the uncer-
tainty objective function (Eq. 10). To do so, we insert the
values of process and observation Jacobian matrices (i.e., Ft
and Ht ) and the process and observation noise covariances
(i.e., Qt and Γt ) and calculate the trace of the ﬁnal matrix (see
Section V). Eventually, after the simpliﬁcations, the CTC
objective function equation can be derived as in 28
minU = argmax
qit ∈f ovdt
(
β1
(∂ϕ xt
∂qxt
)2
+β2
(∂ϕ y
t
∂qy
t
)2
+β3
(∂ϕ xt
∂ pxt
)2
+ β4
(∂ϕ y
t
∂ py
t
)2
+β5
((∂ϕ xt
∂ pz
t
)2
+
(∂ϕ y
t
∂ pz
t
)2))
(28)
where the gradient terms can be calculated using introduced
angle-parameters. βi are covariance constants and are equal
to β1 =
(
P11 +σ2
qx
)
, β2 =
(
P22 +σ2
qy
)
, β3 = σ2
px , β4 = σ2
py and
β5 = σ2
pz . Accordingly, the ﬁnal gradients in Eq. 27 with
respect to UA V pose can be calculated as in Eq. 29- 31
∂U
∂ px
d
= β1
∂
(∂ϕ x
t
∂qxt
)2
∂ px
d
+β3
∂
(∂ϕ x
t
∂ px
d
)2
∂ px
d
+β5
∂
(∂ϕ x
t
∂ pz
d
)2
∂ px
d
(29)
∂U
∂ py
d
= β2
∂
(
∂ϕ y
t
∂qy
t
)2
∂ py
d
+β4
∂
(
∂ϕ y
t
∂ py
d
)2
∂ px
d
+β5
∂
(
∂ϕ y
t
∂ pz
d
)2
∂ py
d
(30)
∂U
∂ pz
d
= β1
∂
(∂ϕ x
t
∂qxt
)2
∂ pz
d
+β2
∂
(
∂ϕ y
t
∂qy
t
)2
∂ pz
d
+β3
∂
(∂ϕ x
t
∂ px
d
)2
∂ pz
d
+β4
∂
(
∂ϕ y
t
∂ py
d
)2
∂ pz
d
+β5


∂
(∂ϕ x
t
∂ pz
d
)2
∂ pz
d
+
∂
(
∂ϕ y
t
∂ pz
d
)2
∂ pz
d

 (31)
and eventually, the control input to UA Vd from UCC module
at time t is noted as in 32
uucc
d,t =
[
κx
∂U tott
∂ px
d
,κy
∂U tott
∂ py
d
,κz
∂U tott
∂ pz
d
]
(32)
We note that there is no need to explicitly calculate the
gradients of human uncertainty map with respect to UA V
positions separately, since we linearly sum up the values
(non-negative) of the two maps (see Fig. 2).
B. Formation Controller Component (FCC)
Similar to the UCC, our formation controller component
(FCC) attempts to minimize the consensus displacement
error by using a gradient descent ﬂow of the weighted
consensus protocol in Eq. 13, with respect to UA V pose,
as represented in Eq. 33.
uf cc
d = −κ2
∂E
∂ pd
= − ∑
( j,d)∈E
(
1 − δpdt −pd/ j
t

)(
pdt −pd/ j
t
)
(
∆−
pdt −pd/ j
t

)3 (33)
Similar logistics as in Eq. 32 can be derived here for the three
axes of coordinate. Accordingly, the combination of control
inputs generated by UCC and FCC modules are leveraged
according to our dual-criteria objective function, introduced
in Eq. 7, in order to produce a new desired location pv
t for
each UA V to move to. As such, a UA V’s new virtual position
will be updated and fed to path planning controller (PPC) as
shown in Eq. 34.
pv
t+δt = pv
t −
(
uucc
d −uf cc
d
)
δt (34)
C. Path Planning Controller (PPC)
The purpose of this controller module is to help a UA V move
from its current position to a new position. The PPC module
generates either an attractive force toward a desired pose
or a repulsive force avoiding an undesirable one. Desired
poses include the initial rendezvous point where coverage
and tracking wildﬁre begins and the new virtual position
pv
t+δt generated through our dual-criteria objective function
as in Eq. 34. Undesirable poses include ones that are too
close to another UA V or too high/low of an altitude such
that the drone might capture low-quality pictures/catch ﬁre.
Leveraging an artiﬁcial potential ﬁeld, we address these
problems by generating attractive and repulsive forces using
a quadratic function of distances from desired or to undesired
points. The attractive control force applied to each UA V pd
t
to any goal points pg
t at time t can be calculated as noted in
Eq. 35
Datt
d = 1
2 κg
pd
g −pd
t

2
and uatt
d = −∇Datt
d = κg
(
pd
g −pd
t
)
(35)
where κg is the proportional gain. Using the same notation,
the repulsive control force generated to avoid any point pg
t
can be deﬁned as in Eq. 36
Drep
d =



1
2 κg
(
1pdg −pdt
−1
γ
)2
i f
pd
g −pd
t
< γ
0 otherwise.
(36)
urep
d =−ζ∇Drep
d (37)
=−ζκg
(
1pdg −pdt
−1
γ
)
1
pdg −pdt
3
(
pd
g −pd
t
)
(38)
where γ is the distance between current position and the
undesirable position pg
t and ζ = 1 only if
pg
t −pd
t
< γ.
Eventually, the general control law in order to generate the
required force to move UA Vs to their new locations can be
formed as in Eq. 39
ud,t = ∑
i
uatti
d,t +urepi
d,t , ∀i ∈Ft (39)
Fig. 5: Simulation results of eight sample time-steps between t = 20 (top-left) and t = 300 (bottom-right) for distributed
coverage, representing drone FOVs projected on the ground. Dot rays show the FOV centroids.
where Ft is the set of all generated attractive and repulsive
forces at time t. Thus, the ﬁnal position of UA V d gets
updated through pd
t+δt = pd
t +udδt.
VII. R ESULTS AND SIMULATION
We evaluate the efﬁciency of our controller in simulation and
against two benchmarks: (1) a state-of-the-art, model-based,
distributed control algorithm [7] and (2) a deep reinforcement
learning (RL) baseline. The ﬁrst benchmark [7] is a ﬁre heat-
intensity-based distributed control framework for wildﬁre
coverage which incorporates FARSITE (as in Section II-A)
and a model for ﬁre heat-intensity measure in order to max-
imize the area-pixel density of the UA Vs ﬁre observations.
Furthermore, we train an RL policy network to control UA Vs
to reduce the uncertainty residual as measured by AEKF.
The network consists of four convolutional layers followed
by three fully connected layers with ReLU activations. The
image of the ﬁre area is an input while a direction for UA Vs
is an output of the network. We deﬁne the reward at each
step as the negative sum of uncertainty residual across the
entire map, encouraging the agent to minimize uncertainty
over time.
In our simulations, we initialize the ﬁre-map with 20
randomly placed ignition points in [50 100] range and within
a 500-by-500 terrain where the ﬁre model parameters Rt , Ut ,
and θt were chosen similar to [7], for comparison. A total
of ﬁve drones were initialized around [50 300] coordinates
with initial altitude set to zero. UA V camera half-angles
were set to [π
4 , π
6 ]. The inter-distance δ and communication
range ∆ in our weighted consensus protocol were set to 50
and 500, respectively. The maximum and minimum altitudes
were chosen to be 15 and 45, respectively. Fig. 5 depicts
the simulation results of eight sample time-steps between
t = 20 (top-left) and t = 300 (bottom-right) as detailed above,
representing drone FOVs projected on the ground.
The left-side ﬁgure in Fig. 6 shows a comparison for a
team of UA Vs controlled by our method, the distributed
control proposed by [7], and the RL baseline. We ran
the simulations for 100 time-steps for all three methods
for a total of 10 trials where for each trial, a cumulative
uncertainty was calculated by the AEKF for ﬁre points not
covered by any drones at each step. While the RL baseline
Fig. 6: This ﬁgure depicts a quantitative comparison of our
coordinated controller with prior work (left-side) and human
safety index (SI) variations as a temporal quantity, with
respect to distance between an approaching ﬁrefront and a
human ﬁreﬁghter (right-side).
failed to learn during 800 episodes of training, our approach
shows signiﬁcant improvements by reducing the cumulative
uncertainty residual by more than 10 2x and 10 5x times.
We also evaluate the feasibility of our controller on
physical robots. The physical experiments with actual robots
were performed in the Robotarium, a remotely accessible
swarm robotics research platform [24]. We tested the cov-
erage performance of our controller using ﬁve robots and
similar ﬁre environment as above. Fig. 7 represents example
demonstrations of our experiment. Results of the experiment
is demonstrated in the supplementary video, which can also
be found at https://youtu.be/j3YdIO5u fE.
A. Safety Index to Secure Human Fireﬁghters
As a corollary of our algorithm, we calculate an individu-
alized safety index (SI) as a temporal quantity for human
ﬁreﬁghters on the ground by leveraging the estimated wild-
ﬁre dynamics and parameters and report this quantity to
ﬁreﬁghters for situational awareness. Now, an individualized
safety index (SI) as a measure of time is deﬁned as in Eq. 40
for human ﬁreﬁghters by taking into account the velocity of
the approaching ﬁrefront.
SIh
t = ∏
i∈vh
Pih
(
˙qi
t
pht −qitpht −qit

)−1
(40)
Fig. 7: Feasibility of the proposed distributed control algo-
rithm for wildﬁre coverage evaluated on physical robots in
Robotarium platform [24]. The experiment footage can be
found at https://youtu.be/j3YdIO5u fE.
In this equation, Pih is the CDF (from Eq. 12), vh is the
vicinity of human h, and ˙qi
t is the estimated ﬁre spread
velocity of ﬁre-spot i toward this vicinity. The ratio is to
account for the direction of the ﬁrefront and equals to 1
if the ﬁrefront is directly approaching the coordinates where
the human is located. Accordingly, we assume three different
ranges for SI to be announced at each time, namely (1) safe
if SIh
t ≥Ts, (2) warning if Tw ≤SIh
t < Ts, and (3) danger
if SIh
t < Tw. Parameters Ts and Tw are predeﬁned temporal-
bounds for safety and warning situations, respectively. We
leave the safety and warning thresholds Ts and Tw to be pre-
deﬁned by humans, as these variables are subjective to the
ﬁreﬁghting scenario (e.g., a burning hospital versus forest
ﬁre) and are dependent on situational severity. The right-
side ﬁgure in Fig. 6 depicts the variations (i.e. mean ±std) of
SI with respect to distance between an approaching ﬁrefront
with 10 points and a human ﬁreﬁghter over 100 trials of
simulation. For this case, a single UA V was placed over the
ﬁre area, inferring the ﬁre-spot locations and parameters.
VIII. C ONCLUSION
We combined a node-level and an ensemble-level control
criteria to introduce a novel coordinated control algorithm
for human-centered active sensing of wildﬁres, providing
high-quality, online information to human ﬁreﬁghters on the
ground. In our approach, we take advantage of AEKF’s
error propagation capability to generate a general uncertainty
map, incorporating uncertainties about ﬁrefront dynamics
and areas of human activity. Our approach outperformed
prior work for distributed control of UA Vs for wildﬁre
tracking as well as a reinforcement learning baseline.
ACKNOWLEDGMENT
We thank A. Silva for his role in implementing the RL
baseline. This work was funded by the Ofﬁce of Naval
Research under grant N00014-19-1-2076.
REFERENCES
[1] P. Sujit, D. Kingston, and R. Beard, “Cooperative forest ﬁre monitoring
using multiple uavs,” in Decision and Control, 2007 46th IEEE
Conference on. IEEE, 2007, pp. 4875–4880.
[2] D. W. Casbeer, D. B. Kingston, R. W. Beard, and T. W. McLain,
“Cooperative forest ﬁre surveillance using a team of small unmanned
air vehicles,” International Journal of Systems Science , vol. 37, no. 6,
pp. 351–360, 2006.
[3] J.-I. Kudoh and K. Hosoi, “Two dimensional forest ﬁre detection
method by using noaa avhrr images,” in Geoscience and Remote
Sensing Symposium, 2003. IGARSS’03. Proceedings. 2003 IEEE In-
ternational, vol. 4. IEEE, 2003, pp. 2494–2495.
[4] A. Ollero and L. Merino, “Unmanned aerial vehicles as tools for
forest-ﬁre ﬁghting,” Forest Ecology and Management, vol. 234, no. 1,
p. S263, 2006.
[5] L. Merino, F. Caballero, J. R. M. de Dios, I. Maza, and A. Ollero,
“Automatic forest ﬁre monitoring and measurement using unmanned
aerial vehicles,” in Proceedings of the 6th International Congress
on Forest Fire Research. Edited by DX Viegas. Coimbra, Portugal .
Citeseer, 2010.
[6] K. A. Ghamry and Y . Zhang, “Cooperative control of multiple uavs for
forest ﬁre monitoring and detection,” in Mechatronic and Embedded
Systems and Applications (MESA), 2016 12th IEEE/ASME Interna-
tional Conference on . IEEE, 2016, pp. 1–6.
[7] H. X. Pham, H. M. La, D. Feil-Seifer, and M. Deans, “A distributed
control framework for a team of unmanned aerial vehicles for dynamic
wildﬁre tracking,” in Intelligent Robots and Systems (IROS), 2017
IEEE/RSJ International Conference on. IEEE, 2017, pp. 6648–6653.
[8] T. Zhou, L. Ding, J. Ji, L. Li, and W. Huang, “Ensemble transform
kalman ﬁlter (etkf) for large-scale wildland ﬁre spread simulation
using farsite tool and state estimation method,” Fire Safety Journal ,
vol. 105, pp. 95–106, 2019.
[9] Z. Lin, H. H. Liu, and M. Wotton, “Kalman ﬁlter-based large-scale
wildﬁre monitoring with a system of uavs,” IEEE Transactions on
Industrial Electronics, vol. 66, no. 1, pp. 606–615, 2018.
[10] M. Kumar, K. Cohen, and B. Homchaudhuri, “Cooperative control
of multiple uninhabited aerial vehicles for monitoring and ﬁghting
wildﬁres,” Journal of Aerospace Computing, Information, and Com-
munication, vol. 8, no. 1, pp. 1–16, 2011.
[11] R. N. Haksar and M. Schwager, “Distributed deep reinforcement
learning for ﬁghting forest ﬁres with a network of aerial robots,” in
2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). IEEE, 2018, pp. 1067–1074.
[12] K. D. Julian and M. J. Kochenderfer, “Distributed wildﬁre surveillance
with autonomous aircraft using deep reinforcement learning,” Journal
of Guidance, Control, and Dynamics , pp. 1–11, 2019.
[13] F. Karimzadeh, N. Cao, B. Crafton, J. Romberg, and A. Raychowd-
hury, “Hardware-aware pruning of dnns using lfsr-generated pseudo-
random indices,” arXiv preprint arXiv:1911.04468 , 2019.
[14] D. Morvan, “Physical phenomena and length scales governing the
behaviour of wildﬁres: a case for physical modelling,” Fire technology,
vol. 47, no. 2, pp. 437–460, 2011.
[15] C. Yuan, Y . Zhang, and Z. Liu, “A survey on technologies for auto-
matic forest ﬁre monitoring, detection, and ﬁghting using unmanned
aerial vehicles and remote sensing techniques,” Canadian journal of
forest research, vol. 45, no. 7, pp. 783–792, 2015.
[16] E. Seraj, A. Silva, and M. Gombolay, “Safe coordination of human-
robot ﬁreﬁghting teams,” arXiv preprint arXiv:1903.06847 , 2019.
[17] M. A. Finney, “Farsite: Fire area simulator-model development and
evaluation,” Res. Pap. RMRS-RP-4, Revised 2004. Ogden, UT: US
Department of Agriculture, Forest Service, Rocky Mountain Research
Station. 47 p. , vol. 4, 1998.
[18] P. Toth and D. Vigo, The vehicle routing problem . SIAM, 2002.
[19] S. Akhlaghi, N. Zhou, and Z. Huang, “Adaptive adjustment of noise
covariance in kalman ﬁlter for dynamic state estimation,” in2017 IEEE
Power & Energy Society General Meeting . IEEE, 2017, pp. 1–5.
[20] R. Sim, “Stable exploration for bearings-only slam,” in Proceedings of
the 2005 IEEE International Conference on Robotics and Automation .
IEEE, 2005, pp. 2411–2416.
[21] J. Cort ´es and M. Egerstedt, “Coordinated control of multi-robot
systems: A survey,” SICE Journal of Control, Measurement, and
System Integration, vol. 10, no. 6, pp. 495–503, 2017.
[22] P. Delamatar, A. Finley, and C. Babcock, “Downloading and process-
ing noaa hourly weather station data,” dim (st) , vol. 1, no. 30538,
p. 12, 2013.
[23] S. S. Ge and Y . J. Cui, “New potential functions for mobile robot path
planning,” IEEE Transactions on robotics and automation , vol. 16,
no. 5, pp. 615–620, 2000.
[24] D. Pickem, P. Glotfelter, L. Wang, M. Mote, A. Ames, E. Feron, and
M. Egerstedt, “The robotarium: A remotely accessible swarm robotics
research testbed,” in Robotics and Automation (ICRA), 2017 IEEE
International Conference on . IEEE, 2017, pp. 1699–1706.