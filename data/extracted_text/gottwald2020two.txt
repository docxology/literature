The Two Kinds of Free Energy and the Bayesian Revolution
Sebastian Gottwald1,2 and Daniel A. Braun1
1Institute of Neural Information Processing, Ulm University, 89081 Ulm, Germany
2sebastian.gottwald@uni-ulm.de
Abstract
The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behav-
ioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental
principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions
of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to
the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maxi-
mum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies
deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action
selection in the face of model uncertainty or when information processing capabilities are limited. The second approach
directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories,
also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual
issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the
high-level deliberation of reasoning down to the low-level information processing of perception.
Keywords: free energy, intelligent agency, bayesian inference, maximum entropy, utility theory, active inference
This is a preprint of the article The two kinds of free energy
and the Bayesian revolution , PLOS Computational Biology 16(12),
2020.
1. Introduction
There is a surprising line of thought connecting some of the
greatest scientists of the last centuries, including Immanuel Kant,
Hermann von Helmholtz, Ludwig E. Boltzmann, and Claude E.
Shannon, whereby model-based processes of action, perception,
and communication are explained with concepts borrowed from
statistical physics. Inspired by Kant’s Copernican revolution and
motivated from his own studies of the physiology of the sen-
sory system, Helmholtz was one of the first proponents of the
analysis-by-synthesis approach to perception (Yuille and Kersten,
2006), whereby a perceiver is not simply conceptualized as some
kind of tabula rasa recording raw external stimuli, but rather
relies on internal models of the world to match and anticipate
sensory inputs. The internal model paradigm is now ubiquitous
in the cognitive and neural sciences and has even led some re-
searchers to propose a Bayesian brain hypothesis, whereby the
brain would essentially be a prediction and inference engine
based on internal models (Kawato, 1999; Flanagan et al., 2003;
Doya, 2007). Coincidentally, Helmholtz also invented the no-
tion of the Helmholtz free energy that plays an important role in
thermodynamics and statistical mechanics, even though he never
made a connection between the two concepts in his lifetime.
This connection was first made by Dayan, Hinton, Neal, and
Zemel in their computational model of perceptual processing
as a statistical inference engine known as the Helmholtz machine
(Dayan et al., 1995). In this neural network architecture, there
are feed-forward and feedback pathways, where the bottom-
up pathway translates inputs from the bottom layer into hid-
den causes at the upper layer (the recognition model), and top-
down activation translates simulated hidden causes into sim-
ulated inputs (the generative model). When considering log-
likelihood in this setup as energy in analogy to statistical me-
chanics, learning becomes a relaxation process that can be de-
scribed by the minimization of variational free energy. While it
should be emphasized that variational free energy is not the same
as Helmholtz free energy, the two free energy concepts can be
1
arXiv:2004.11763v4  [q-bio.NC]  7 Dec 2020
2 The Two Kinds of Free Energy and the Bayesian Revolution
formally related. Importantly, variational free energy minimiza-
tion is not only a hallmark of the Helmholtz machine, but of a
more general family of inference algorithms, such as the popu-
lar expectation-maximization (EM) algorithm (Neal and Hinton,
1998; Beal, 2003). In fact, over the last two decades, variational
Bayesian methods have become one of the foremost approxima-
tion schemes for tractable inference in the machine learning lit-
erature. Moreover, a plethora of machine learning approaches
use loss functions that have the shape of a free energy when
optimizing performance under entropy regularization in order
to boost generalization of learning models (Williams and Peng,
1991; Mnih et al., 2016).
In the meanwhile, free energy concepts have also made their
way into the behavioral sciences. In the economic literature,
for example, trade-offs between utility and entropic uncertainty
measures that take the form of free energies have been proposed
to describe decision-makers with stochastic choice behavior due
to limited resources (McKelvey and Palfrey, 1995; Sims, 2003;
Mattsson and Weibull, 2002; McFadden, 2005; Wolpert, 2006)
or robust decision-makers with limited precision in their mod-
els (Maccheroni et al., 2006; Hansen and Sargent, 2008). The
free energy trade-off between entropy and reward can also be
found in information-theoretic models of biological perception-
action systems (Still, 2009; Tishby and Polani, 2011; Ortega and
Braun, 2013), some of which have been subjected to experimen-
tal testing (Ortega and Stocker, 2016; Sims, 2016; Schach et al.,
2018; Lindig-León et al., 2019; Bhui and Gershman, 2018; Ho
et al., 2020). Finally, in the neuroscience literature the notion of
free energy has risen to recent fame as the central puzzle piece
in the Free Energy Principle (Friston, 2010) that has been used
to explain a cornucopia of experimental findings including neu-
ral prediction error signals (Sales et al., 2019), synaptic plastic-
ity rules(Bogacz, 2017), neural effects of biased competition and
attention (Friston et al., 2012; Parr and Friston, 2017), visual ex-
ploration in humans (Mirza et al., 2018), and more—see the ref-
erences in (Parr and Friston, 2019). Over time, the Free En-
ergy Principle has grown out of an application of the free en-
ergy concept used in the Helmholtz machine, to interpret corti-
cal responses in the context of predictive coding (Friston, 2005),
and has gradually developed into a general principle for intelli-
gent agency, also known as Active Inference (Friston et al., 2013,
2015; Parr and Friston, 2019). Consequences and implications
of the Free Energy Principle are discussed in neighbouring fields
like psychiatry (Schwartenbeck and Friston, 2016; Linson et al.,
2020) and the philosophy of mind (Clark, 2013; Colombo and
Wright, 2018).
Given that the notion of free energy has become such a per-
vasive concept that cuts through multiple disciplines, the main
rationale for this discussion paper is to trace back and to clar-
ify different notions of free energy, to see how they are related
and what role they play in explaining behavior and neural activ-
X
past 
crop yields
S
A
past 
soil quality
X′ 
S′ 
future 
soil quality
fertilization
future 
crop yields
p0(S)
p0(S′ | S, A)
p0(X′ | S′ )
p(S| X = x) = ?
Bayesian inference
p0(X| S) p0(A)
Figure 1. Graphical representation of an exemplary probabilistic model.
The arrows (edges) indicate causal relationships between the random vari-
ables (nodes). The full joint distribution p0 over all random variables is
sometimes also referred to as a generative model, because it contains the
complete knowledge about the random variables and their dependencies
and therefore allows to generate simulated data. Such a model could for
example be used by a farmer to infer the soil quality Sbased on the crop
yields X through Bayesian inference, which allows to determine a priori
unknown distributions such as p(S|X) from the generative model p0 via
marginalization and conditionalization.
ity. As the notion of free energy mainly appears in the context
of statistical models of cognition, the language of probabilistic
models constitutes a common framework in the following dis-
cussion. Section 2 therefore starts with preliminary remarks on
probabilistic modelling. Section 3 introduces two notions of free
energy that are subsequently expounded in Section 4 and Sec-
tion 5, where they are applied to models of intelligent agency.
Section 6 concludes the paper.
2. Probabilistic models and perception-action systems
Systems that show stochastic behavior, for example due to
randomly behaving components or because the observer ignores
certain degrees of freedom, are modelled using probability dis-
tributions. This way, any behavioral, environmental, and hidden
variables can be related by their statistics, and dynamical changes
can be modelled by changes in their distributions.
Consider, for example, the simple probabilistic model illus-
trated in Fig 1, consisting of the (for simplicity, discrete) variables
past and future soil quality S := (S,S′), past and future crop yields
X := (X,X′), and fertilization A. The graphical model shown
in the figure corresponds to the joint probability p0(X,S,A)
given by the factorization
p0(X′|S′) p0(X|S) p0(S′|S,A) p0(S) p0(A) , (1)
where p0(S) is the base probability of the past soil quality S,
The Two Kinds of Free Energy and the Bayesian Revolution 3
p0(X|S) is the probability of crop yields X depending on the
past soil quality S, and so forth. Given the joint distribution we
can now ask questions about each of the variables. For example,
we could ask about the probability distribution p(S|X= x) of
soil quality Sif we are told that the crop yields Xare equal to a
value x. We can obtain the answer from the probabilistic model
p0 by doing Bayesian inference, yielding the Bayes' posterior
p(S|X) = p(S,X)∑
sp(s,X) = p0(X|S)p0(S)∑
sp0(X|s)p0(s) , (2)
where the dependencies on X′, S′, and Ahave been summed out
to calculate the marginal p(S,X). In general, Bayesian inference
in a probabilistic model means to determine the probability of
some queried unobserved variables given the knowledge of some
observed variables. This can be viewed as transforming the prior
probabilistic model p0 to a posterior model p, under which the
observed values have probability one and unobserved variables
have probabilities given by the corresponding Bayes’ posteriors.
In principle, Bayesian inference requires only two different
kinds of operations, namely marginalization, i.e. summing out
unobserved variables that have not been queried, such as X′,S′
and Aabove, and conditionalization, i.e. renormalizing the joint
distribution over observed and queried variables—that may itself
be the result from a previous marginalization such as p(S,X)
above—to obtain the required conditional distribution over the
queried variables. In practice, however, inference is a hard com-
putational problem and many more efficient inference methods
are available that may provide approximate solutions to the exact
Bayes’ posteriors, including belief propagation (Pearl, 1988), ex-
pectation propagation (Minka, 2001), variational Bayesian infer-
ence (Hinton and van Camp, 1993), and Monte Carlo algorithms
(MacKay, 2002). Also note that inference is trivial if the sought-
after conditional distribution of the queried variable is already
given by one of the conditional distributions that jointly specify
the probabilistic model, e.g., p(X|S) = p0(X|S).
Probabilistic models can be used not only as external (observer)
models, but also as internal models that are employed by the agent
itself, or by a designer of the agent, in order to determine a de-
sired course of action. In this latter case, actions could either be
thought of as deterministic parameters of the probabilistic model
that influence the future ( influence diagrams ) or as random vari-
ables that are part of the probabilistic model themselves ( prior
models) (Boutilier et al., 1999). Either way, internal models al-
low making predictions over future consequences in order to
find actions or distributions over actions that lead to desirable
outcomes, for example actions that produce high rewards in the
future. In mechanistic or process model interpretations, some of
the specification procedures to find such actions are themselves
meant to represent what the agent is actually doing while rea-
soning, whereas as if interpretations simply use these methods
as tools to arrive at distributions that describe the agent’s behav-
ior. Free energy is one of the concepts that appears in both types
of methods.
3. The two notions of free energy
Vaguely speaking, free energy can refer to any quantity that is
of the form
free energy = energy ±const. ×entropy, (3)
where energy is an expected value of some quantitity of inter-
est, entropy refers to a quantity measuring disorder, uncertainty,
or complexity, that must be specified in the given context, and
const. is a constant term that translates between units of entropy
and energy, and is related to the temperature in physically moti-
vated free energy expressions. From relation (3), it is not surpris-
ing that free energy sometimes appears enshrouded by mystery,
as it relies on an understanding of entropy, and “nobody really
knows what entropy is anyway”, as John Von Neumann famously
quipped (Feynman et al., 1996).
Historically, the concept of free energy goes back to the roots
of thermodynamics, where it was introduced to measure the
maximum amount of work that can be extracted from a thermo-
dynamic system at a constant temperature and volume. If, for ex-
ample, all the molecules in a box move to the left, we can use this
kinetic energy to drive a turbine. If, however, the same kinetic
energy is distributed as random molecular motion, it cannot be
fully transformed into work. Therefore, only part of the total en-
ergy Eis usable, because the exact positions and momenta of the
molecules, the so-called microstates, are unknown. In this case,
the maximum usable part of the energy E is the Helmholtz free
energy, defined as
F = E−TS, (4)
where Sis the thermodynamic entropy. In general, the transfor-
mation between two macrostates with free energies F1 and F2
allows the extraction of work W ≤F2 −F1.
While the two notions of free energy that we discuss in the fol-
lowing are vaguely inspired by the physical original, their moti-
vations are rather distinct and the main reason they share the
nomenclature is due to their general form (3) resembling the
Helmholtz free energy (4).
3.1. Free energy from constraints
The first notion of free energy is closely tied to the princi-
ple of maximum entropy (Jaynes, 1957), which virtually appears
in all branches of science. From this vantage point, the phys-
ical free energy is merely a special instance of a more general
inference problem where we hold probabilistic beliefs about un-
known quantities (e.g., the exact energy values of the molecules
4 The Two Kinds of Free Energy and the Bayesian Revolution
in a gas) and we can only make coarse measurements or observa-
tions (e.g., the temperature of the gas) that we can use to update
our beliefs about these hidden variables. The principle of maxi-
mum entropy suggests that, among the beliefs that are compati-
ble with the observation, we should choose the most “unbiased”
belief, in the sense that it corresponds to a maximum number of
possible assignments of the hidden variables.
3.1.1. Wallis’ motivation of the maximum entropy principle
Consider the random experiment of distributing N elements
randomly in nequally probable buckets with N ≫ n, where
the resulting number of elements Ni in bucket i ∈{1,...,N}
determines the probability p(zi) := Ni
N . In principle, this way
we could generate any distribution p over a finite set Ω =
{z1,...,zn}that we like, however, a uniform distribution that
reflects the equiprobable assignment clearly is much more likely
than a Dirac distribution where all the probability mass is con-
centrated in one bucket. Here, the reason is that there are many
possible assignments of elements among the buckets that gen-
erate the uniform distribution, whereas there is only one for a
Dirac distribution. In fact, the number of possibilities of how to
distribute Nelements among nbuckets with Nielements in the
ith bucket is
ω := N!
N1! ···Nn! , (5)
because N! is the number of possible permutations of all N el-
ements, which overcounts by the number of permutations of el-
ements inside the same bucket and thus has to be divided by the
number of permutations Ni! for all i= 1,...,n. In the absence
of any further measurement constraints, the number of possibil-
ities (5) is maximized by Ni = N/nfor all i, and thus the typical
distribution p∗over Ω in this case is the uniform distribution,
i.e., p∗(zi) = 1
n for all i.
Consider now the problem of having to determine a typical
distribution p∗over Ω such that the expected value Ep∗[E] =:
⟨E⟩p∗of some quantity Eequals a measured value ε. A simple
example would be the experiment of throwing N dice and tak-
ing Eto be the number of dots, i.e., E(z1) = 1 ,...,E(z6) =
6, and trying to find the typical distribution p∗over outcomes
z1,...,z6 under the constraint that the average number of dots
is, say ε= 2. The solution to this problem is analogous to the
case of no constraints, but this time we only consider realiza-
tions that are compatible with the measurement constraint, that
is we let (N1,...,Nn) belong to the set of permissible occupa-
tion vectors
Γε:=
{
(N1,...,Nn)
⏐⏐⟨E⟩p = ε, p(zi) = Ni
N ∀i
}
.
A typical distribution p∗for a constraint εcan then be deter-
mined by a candidate in Γεwith the maximum number ωof pos-
sibilities (5). By assumption, Nis much larger than n, so that we
can get rid of the faculties by making use of Stirling’s approxi-
mation ln N! = Nln N −N + O(ln N). In particular, when
letting N,Ni →∞ such that p(zi) = Ni
N remains finite, we
obtain
1
N log ω = −
n∑
i=1
Ni
N log Ni
N
  
=H(p)
+ O
(log N
N
{
N→∞
−→ H(p) .
where H(p) := −∑
z∈Ω p(z) logp(z) denotes the ( Gibbs or
Shannon) entropy of p. Thus, instead of assessing typicality by
maximizing (5) in Γεfor large but fixed N, we can get rid of the
N-dependency by simply maximizing H,
p∗= argmax
p,⟨E⟩p=ε
H(p) . (6)
This constrained optimization problem is known as the principle
of maximum entropy. The motivation given here is essentially
the Wallis derivation presented by Jaynes (Jaynes, 2003).
3.1.2. Free energy from constraints and the Boltzmann distribution
The constrained optimization problem (6) can be translated
into an unconstrained problem by introducing a Lagrange mul-
tiplier β, known as the inverse temperature due to the analogy to
thermodynamics and the Helmholtz Free Energy (4), which has
to be chosen post hoc such that the constraint is satisfied. This
results in the minimization of the Lagrangian
F(p) := ⟨E⟩p−1
βH(p), (7)
which takes the form of a free energy (3). As we shall see later,
F takes its minimum at the Boltzmann distribution known from
statistical mechanics, given by
p∗(z) := 1
Ze−βE(z), (8)
where Z= ∑
z∈Ω e−βE(z) denotes the normalization constant.
Note that, the argument in the previous section implicitly as-
sumes a uniform reference distribution, because the buckets are
assumed to be equiprobable. When replacing this assumption by
the assumption of a general distribution p0 over Ω , we obtain the
principle of minimum relative entropy (Rosenkrantz, 1983), where
the so-called Kullback-Leibler (KL) divergence DKL(p∥p0) =
⟨log(p/p0)⟩p is minimized with respect to psubject to a con-
straint ⟨E⟩p = ε. Analogous to the maximum entropy princi-
ple, this translates to the unconstrained minimization of the La-
grangian
F(p,p0) := ⟨E⟩p+ 1
βDKL(p∥p0), (9)
with solution given by p∗(z) = 1
Zp0(z) e−βE(z).
The Two Kinds of Free Energy and the Bayesian Revolution 5
⟨ ⟩p H(p)
minimizeminimize
− 1
β
maximize
F(p) =
concentrated 
(on the option
                         )
maximal uncertainty 
(uniform distribution)
trade-oﬀ 
(controlled by    )β
p* p = δz* p = unif
E
<latexit sha1_base64="Ws0O9RjId2iTxYQlauihLfXfke4=">AAAC03ichVFLS8NAEJ7GV1tfVY9egkXwVBIpqLeiVrwIFUxbbIts0m0amhfJtlCLF/Hqzav+L/0tHvyypoIW6YbNzH4z883LDF0nFpr2nlEWFpeWV7K5/Ora+sZmYWu7HgfDyOKGFbhB1DRZzF3H54ZwhMubYcSZZ7q8YQ7OEntjxKPYCfwbMQ55x2O27/QciwlAt22Pib7FXLV6VyhqJU0edVbRU6VI6akFhQ9qU5cCsmhIHnHySUB3iVGMr0U6aRQC69AEWATNkXZOD5RH7BBeHB4M6AB/G69Wivp4J5yxjLaQxcWNEKnSPu6FZDThnWTl0GPIT9x7idn/ZphI5qTCMaQJxpxkvAIuqA+PeZFe6jmtZX5k0pWgHh3LbhzUF0ok6dP64TmHJQI2kBaVqtLTBocp3yNMwIc0UEEy5SmDKjvuQjIpuWTxU0YGvggymT7qwZr1v0udVeqHJb1cOrkuFyun6cKztEt7dICtHlGFLqmGOixkeaFXelMMZaI8Kk/frkomjdmhX0d5/gL9PZKS</latexit>
z* z*
z ⇤ = argmin E
Figure 2.Minimizing the free energy from constraints (7) requires to trade
oﬀ the competing terms of energy ⟨E⟩pand entropy H(p), here shown ex-
emplarily for the case of three elements. Assuming there exists a unique
minimal element z∗= argminzE(z), then minimizing only ⟨E⟩pover all
probability distributions presults in the ( Dirac delta) distribution δz∗that
assigns zero probability to all zi ̸= z∗and probability one to zi = z∗, and
therefore has zero entropy. In contrast, minimizing only the term−H(p)/β
is equivalent to maximizingH(p) and therefore would result in the uniform
distribution that gives equal probability to all elements. The resulting Boltz-
mann distribution p∗interpolates between these two extreme solutions of
minimal energy (β→∞ ) and maximum entropy (β→ 0).
3.1.3. The trade-oﬀ between energy and uncertainty
An important feature of the minimization of the free energies
(7) and (9) consists in the balancing of the two competing terms
of energy and entropy (cf. Fig 2). This trade-off between max-
imal uncertainty (uniform distribution, or p0) on the one hand
and minimal energy (e.g. a delta distribution) on the other hand
is the core of the maximum entropy principle. The inverse tem-
perature βplays the role of a trade-off parameter that controls
how these two counteracting forces are weighted.
The maximum entropy principle goes back to the principle of
insufficient reason (Bernoulli, 1713; de Laplace, 1812; Poincaré,
1912), which states that two events should be assigned the same
probability if there is no reason to think otherwise. It has been
hailed as a principled method to determine prior distributions
and to incorporate novel information into existing probabilis-
tic knowledge. In fact, Bayesian inference can be cast in terms
of relative entropy minimization with constraints given by the
available information (Williams, 1980). Applications of this idea
can also be found in the machine learning literature, where sub-
tracting (or adding) an entropy term from an expected value of a
function that must be optimized is known as entropy regulariza-
tion and plays an important role in modern reinforcement learn-
ing algorithms (Williams and Peng, 1991; Mnih et al., 2016) to
encourage exploration (Haarnoja et al., 2017) as well as to penal-
ize overly deterministic policies resulting in biased reward esti-
mates (Fox et al., 2016).
From now on, we refer to a free energy expression that is mo-
tivated from a trade-off between an energy and an entropy term,
such as (7) and (9), as free energy from constraints , in order to dis-
criminate it from the notion of free energy introduced in the fol-
lowing section, which—despite of its resemblance—has a differ-
ent motivation.
3.2. Variational free energy
There is another, distinct appearance of the term “free energy”
outside of physics, which is a priori not motivated from a trade-
off between an energy and entropy term, but from possible ef-
ficiency gains when representing Bayes’ rule in terms of an op-
timization problem. This technique is mainly used in variational
Bayesian inference (Koller, 2009, Ch. 11), originally introduced by
Hinton and van Camp (Hinton and van Camp, 1993). As before,
for simplicity all random variables are discrete, but most expres-
sions can directly be translated to the continuous case by replac-
ing probability distributions by probability densities and sums
by the corresponding integrals.
3.2.1. Variational Bayesian inference
As we have seen in Section 2, Bayesian inference consists in
the calculation of a conditional probability distribution over un-
known variables given the values of known variables. In the most
simple case of two variables, say X and Z, and a probabilistic
model of the form p0(X,Z) = p0(X|Z)p0(Z), Bayesian infer-
ence applies if Xis observed and Zis queried. Analogous to (2),
the exact Bayes’ posteriorp(Z|X= x) is defined by the renor-
malization of p0(x,Z) in order to obtain a distribution over Z
that respects the new information X= x,
p(Z|X= x) = p0(x,Z)
Z(x) = p0(x|Z) p0(Z)
Z(x) , (10)
with the normalization constant Z(x) = ∑
zp0(x,z).
In variational Bayesian inference, however, this Bayes’ poste-
rior is not calculated directly by renormalizing p0(x,Z) with re-
spect to Z, but indirectly by approximating it by a distribution
q(Z) that is adjusted through the minimization of an error mea-
sure that quantifies the deviation from the exact Bayes’ posterior.
Importantly, the value of this error measure can be determined
without having to know the exact Bayes’ posterior. To see this,
note that the KL divergence between q(Z) and p(Z|X= x) can
be written as
⟨
log q(Z)
p(Z|X= x)
⟩
q(Z)
  
= DKL(q(Z)∥p(Z|X= x))
= log Z(x) 
indep. of q
+
⟨
log q(Z)
p0(x,Z)
⟩
q(Z)
  
=: F(q(Z)∥p0(x,Z))
,
(11)
6 The Two Kinds of Free Energy and the Bayesian Revolution
q auxiliary trial distributions
ϕ unnormalized reference function
pϕ normalized version of ϕ
A
space of probability 
distributions
ϕ = (ϕ1, ϕ2)
pϕ
normalization
1.00.5
0.5
1.0
q
variation of q to 
minimize free 
energy
qpϕ
 free energy
ϕF( ∥ )q
1st vector component
2nd vector component
free energy minimization
B z
q
area < 1
ϕ
area = 1
normalization
pϕ
probability density
free energy 
minimization
Figure 3.The normalization of a functonφto obtain a probability distributionpφis equivalent to ﬁtting trial distributionsqto the shape of φby minimizing
free energy. In two dimensions, the normalization of a point φ = ( φ1,φ2) corresponds to a (non-orthogonal) projection onto the plane of probability
vectors (A). For continuous domains, where probability distributions are represented by densities, normalization corresponds to a rescaling of φsuch that
the area below the graph equals 1 (B). Instead, when minimizing variational free energy (red colour), the trial distributions qare varied until they ﬁt to the
shape of the unnormalized function φ(perfectly at q= pφ).
i.e., it can be decomposed into the sum of a constant term and a
term that does not depend on the normalization Z(x). In par-
ticular, a good approximation q(Z) of the exact Bayes’ posterior
(10) will effectively minimize this KL divergence, which—due to
(11)—can be done by minimizing F(q(Z)∥p0(x,Z)). In partic-
ular, the optimium of this minimization is exactly achieved at the
Bayes’ posterior (10),
argmin
q(Z)
⟨
log q(Z)
p0(x,Z)
⟩
q(Z)
= p(Z|X= x) , (12)
which is known as the variational characterization of Bayes’ rule.
This result is a special case of (14) in the following section.
3.2.2. Variational free energy, an extension of relative entropy
Any non-negative function φon a finite space Ω , can be nor-
malized to obtain a probability distribution pφ = φ/∑
zφ(z)
on Ω that differs from φonly by a scaling constant. In cases when
it is not beneficial to carry out the sum ∑
zφ(z) explicitly, such
a normalization might be replaced by the minimization of the
variational free energy
F(q∥φ) :=
⟨
log q(Z)
φ(Z)
⟩
q(Z)
, (13)
with respect to the so-called trial distributions q, because we have
argmin
q
F(q∥φ) = φ(Z)∑
zφ(z) = pφ(Z) . (14)
Thus, instead of normalizing φdirectly, one fits auxiliary distri-
butions qto approximate the shape of φin the space of proba-
bility distributions (cf. Fig 3). If this optimization process has no
constraints, then the trial distributions are adjusted until pφ is
achieved. In the case of constraints, for instance if the trial distri-
butions are parametrized by a non-exhaustive parametrization
(e.g., Gaussians), then the optimized trial distributions approx-
imate pφ as close as possible within this parametrization. The
minimal value of F(q∥φ) is
F(pφ∥φ) = min
q
F(q∥φ) = −log
∑
z
φ(z) . (15)
In particular, this implies that −F(q∥φ) ≤log ∑
zφ(z) for all
q, so that varying −F(q∥φ) with arbitrary trial distributions q
always provides a lower bound to the unknown normalization
constant ∑
zφ(z). In Bayesian inference this is the normaliza-
tion constant in Bayes’ rule and called the model evidence , which
is why the negative variational free energy is also called evidence
lower bound (ELBO).
The proof of (14) and (15) directly follows from Jensen’s in-
equality and only relies on the concavity of the logarithm. As we
have seen in the previous section, in variational Bayesian infer-
ence, the reference φusually takes the form of a joint distribution
evaluated at the observed variables, e.g., φ(Z) = p0(x,Z) in
which case (14) recovers (12). The variational free energy (13)
is a free energy in the sense of (3) since by the additivity of the
logarithm under multiplication ( log ab= log a+ logb),
F(q∥φ) = ⟨−log φ⟩q −H(q) (16)
with energy term ⟨−log φ⟩q and entropy term H(q). Note that,
for the choice φ= e−βE, Equation (14) becomes the Boltzmann
The Two Kinds of Free Energy and the Bayesian Revolution 7
observation
Z
p0(Z )
p0(X | Z )
p(Z | X = x) = p0(x, Z )
∑z p0(x, z)
naive 
Bayesian
inference
hidden 
variable
prior
likelihood
= argminq ⟨log q(Z )
p0(x, Z ) ⟩q
variational 
Bayesian
inference
p0(X, Z )
probabilistic
model
X
| {z }
<latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit>
variational free energy
|{z}
<latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit>
approximate inference
minθ F(qθ(Z )∥p0(x, Z ))
F(q(Z )∥p0(x, Z ))
iterative inference algorithms
minq(Z1)
F(q(Z1)q(Z2 | Z1)∥p0(x, Z1, Z2))
minq(Z2|Z1)
F(q(Z1)q(Z2 | Z1)∥p0(x, Z1, Z2))
1
2
z1
z2
1
1
2
p0(Z1, Z2 | x)
(e.g. by gradient descent)
p0(Z | x) qθ(Z )
z
qθ′ (Z )
probability (density)
qθ′ ′ (Z )
stepwise optimization
search space reduction
(e.g. EM, belief propagation, …) q′ (Z1, Z2)
q(Z1, Z2)
q′ ′ (Z1, Z2)
Figure 4.In variational Bayesian inference, the operation of renormalizing the probabilistic model p0 evaluated at an observation X= x(Bayes’ rule), is
replaced by an optimization problem. In practice, this variational representation is o/f_ten exploited to simplify a given inference problem, either by reducing
the seach space of distributions, for example through a restrictive parametrization resulting in approximate inference, or by splitting up the optimization
into multiple partial optimization steps that are potentially easier to solve than the original problem but might still converge to the exact solution. These
two simpliﬁcations can also be combined, for example in the case of mean-ﬁeld assumptions where the space of distributions is reduced and an eﬀicient
iterative inference algorithm is obtained at the same time.
distribution (8) and the variational free energy (16) formally cor-
responds to the free energy from constraints (7).
Variational free energy can be regarded as an extension of rel-
ative entropy with the reference distribution being replaced by
a non-normalized reference function, since in the case when φ
is already normalized, that is if ∑
zφ(z) = 1, then the free en-
ergy (13) coincides with the KL divergence DKL(q∥φ). In par-
ticular, while relative entropy is a measure for the dissimilarity
of two probability distributions, where the minimum is achieved
if both distributions are equal, variational free energy is a mea-
sure for the dissimilarity between a probability distribution q
and a (generally non-normalized) function φ, where the mini-
mum with respect to q is achieved at pφ. Accordingly, we can
think of the variational free energy as a specific error measure
between probability distributions and reference functions. In
principle, one could design many other error measures that have
the same minimum. This means that, a statement in a proba-
bilistic setting that a distribution q∗minimizes a variational free
energy F(q∥φ) with respect to a given reference φ, is analogous
to a statement in a non-probabilistic setting that some number
x = x∗minimizes the value of an error measure ϵ(x,y) (e.g.,
the squared error ϵ(x,y) = ( x−y)2) with respect to a given
reference value y.
3.2.3. Approximate and iterative inference
Representing Bayes’ rule as an optimization problem over
auxiliary distributions q has two main applications that both
can simplify the inference process (cf. Fig 4). First, it allows to
approximate exact Bayes’ posteriors by restricting the optimiza-
tion space, for example using a non-exhaustive parametrization,
e.g. an exponential family. Second, it enables iterative inference
algorithms consisting of multiple simpler optimization steps, for
example by optimizing with respect to each term in a factorized
representation of qseparately. A popular choice is the mean-field
approximation, which combines both of these simplifications, as
it assumes independence between hidden states, effectively re-
ducing the search space from joint distributions to factorized
ones, and moreover it allows to optimize with respect to each
factor alternatingly. Note, however, that mean-field approxi-
mations have limited use in sequential environments, where in-
dependence of subsequent states cannot be assumed and there-
fore less restrictive assumptions must be used instead (Opper and
Saad, 2001).
Many efficient iterative algorithms for exact and approxi-
mate inference can be viewed as examples of variational free
energy minimization, for example the EM algorithm (Demp-
ster et al., 1977; Neal and Hinton, 1998), belief propagation
(Pearl, 1988; Yedidia et al., 2001), and other message passing algo-
rithms(Minka, 2001; Wainwright et al., 2005; Winn and Bishop,
2005; Minka, 2005; Yedidia et al., 2005). While the (Bayesian) EM
algorithm (Beal, 2003) and Pearl’s belief propagation (Yedidia
et al., 2001) both can be seen as minimizing the same varia-
tional free energy, just with different assumptions on the approx-
imate posteriors, in (Minka, 2005), it is shown that also many
8 The Two Kinds of Free Energy and the Bayesian Revolution
other message passing algorithms such as (Minka, 2001; Wain-
wright et al., 2005; Winn and Bishop, 2005) can be cast as min-
imizing some type of free energy, the only difference being the
choice of the divergence measure as the entropy term. Simple
versions of these algorithms have often existed before their free
energy formulations were available, but the variational repre-
sentations usually allowed for extensions and refinements—see
(Csiszár and Tusnády, 1984; Hathaway, 1986; Neal and Hinton,
1998; Beal, 2003) in case of EM and (Yedidia et al., 2001; Heskes,
2003; Yuille, 2002; Yedidia et al., 2005) in case of message passing.
We are now turning to the question of how the two notions of
free energy introduced in this section are related to recent theo-
ries of intelligent agency.
4. Free energy from constraints in information process-
ing
4.1. The basic idea
The concept of free energy from constraints as a trade-
off between energy and uncertainty can be used in models of
perception-action systems, where entropy quantifies informa-
tion processing complexity required for decision-making (e.g.,
planning a path for fleeing a predator) and energy corresponds
to performance (e.g., distinguishing better and worse flight direc-
tions). The notion of decision in this context is very broad and
can be applied to any internal variable in the perception-action
pipeline (Kahneman, 2002), that is not given directly by the envi-
ronment. In particular, it also subsumes perception itself, where
the decision variables are given by the hidden causes that are be-
ing inferred from observations.
In rational choice theory (von Neumann and Morgenstern,
1944), a decision-maker selects decisions x∗from a set of options
Ω such that a utility function Udefined on Ω is maximized,
x∗= argmax
x∈Ω
U(x) . (17)
The utility values U(x) could either be objective, for example
a monetary gain, or subjective in which case they represent the
decision-maker’s preferences. In general, the utility does not
have to be defined directly on Ω , but could be derived from utility
values that are attached to certain states, for example to the con-
figurations of the playboard in a board game. In the case of per-
ception, utility values are usually given by (log-)likelihood func-
tions, in which case utility maximization without constraints
corresponds to greedy inference such as maximum likelihood es-
timation. Note that, for simplicity, in this section we consider
one-step decision problems. Sequential tasks can either be seen
as multiple one-step problems where the utility of a given step
might depend on the policy over future steps, or as path planning
problems where an action represents a full action path or policy
C( p, p0)uncertainty cost
non-admissible
non-optimal
bounded-optimal
⟨U⟩pexpected utility
x*
no cost 
maximal uncertainty
costly 
less uncertainty
more costly 
little uncertainty
very costly 
no uncertainty
A
B
Ω
Figure 5. A: Decision-making can be considered as a search process in the
space of options Ω , where options are progressively ruled out. Delibera-
tion costs are deﬁned to be monotone functions under such uncertainty re-
duction. B: Exemplary eﬀiciency curve resulting from the trade-oﬀ between
utility and costs, that separates non-optimal from non-admissible behavior.
The points on the curve correspond to bounded-optimal agents that opti-
mally trade oﬀ utility against uncertainty, analogous to the rate-distortion
curve in information theory.
(Whittle, 1990; Tishby and Polani, 2011; Grau-Moya et al., 2016;
Gottwald and Braun, 2019b).
While ideal rational decision-makers are assumed to per-
fectly optimize a given utility function U, real behavior is often
stochastic, meaning that multiple exposures to the same prob-
lem lead to different decisions. Such non-deterministic behav-
ior could be a consequence of model uncertainty, as in Bayesian
inference or various stochastic gambling schemes, or a conse-
quence of satisficing (Simon, 1955), where decision-makers do
not choose the single best option, but simply one option that
is good enough. Abstractly, this means that, the choice of a
single decision is replaced by the choice of a distribution over
decisions. More generally, also considering prior information
that the decision-maker might have from previous experience,
the process of deliberation during decision-making might be ex-
pressed as the transformation of a prior p0 to a posterior distri-
bution p.
When assuming that deliberation has a cost C(p,p0), then
arriving at narrow posterior distributions should intuitively be
more costly than choosing distributions that contain more un-
certainty (cf. Fig 5A). In other words, deliberation costs must be
increasing with the amount of uncertainty that is reduced by the
transformation from p0 to p. Uncertainty reduction can be un-
derstood as making the probabilities of options less equal to each
other, rigorously expressed by the mathematical concept of ma-
jorization (Marshall et al., 2011). This notion of uncertainty can
also be generalized to include prior information, so that the de-
gree of uncertainty reduction corresponds to more or less devi-
The Two Kinds of Free Energy and the Bayesian Revolution 9
ations from the prior (Gottwald and Braun, 2019a).
Maximizing expected utility ⟨U⟩pwith respect to punder re-
strictions on processing costs C(p,p0) is a constrained opti-
mization problem that can be interpreted as a particular model
of bounded rationality (Simon, 1955), explaining non-rational be-
havior of decision-makers that may be unable to select the single
best option by their limited information processing capability.
Similarly to the free energy trade-off between energy and en-
tropy (cf. Fig 2), this results in a trade-off between utility ⟨U⟩p
and processing costs C(p,p0),
Fβ(p) := ⟨U⟩p −1
βC(p,p0). (18)
Here, the trade-off parameter β is analogous to the in-
verse temperature in statistical mechanics (cf. Equation (7)) and
parametrizes the optimal trade-offs p∗
β = argmaxpFβ(p) be-
tween utility and cost, that define an efficiency frontier sep-
arating the space of perception-action systems into bounded-
optimal, non-optimal, and non-admissible systems (cf. Fig 5).
When assuming that the total transformation cost is the same
independent of whether a decision problem is solved in one step
or multiple sub-steps ( additivity under coarse-graining ) the trade-
off in (18) takes the general form (3) of a free energy in the sense
of energy (utility) minus entropy (cost), because then the cost
function is uniquely given by the relative entropy
C(p,p0) = DKL(p∥p0). (19)
Note that the additivity of (19) also implies a coarse-graining
property of the free energy (18) in the case when the decision
is split into multiple steps, such that the utility of preceding de-
cisions is effectively given by the free energy of following de-
cisions. Therefore, in this case, free energy can be seen as a
certainty-equivalent value of the subordinate decision problems,
i.e. the amount of utility the agent would have to receive to be
indifferent between this guaranteed utility and the potential ex-
pected utility of the subsequent decision steps taking account
the associated information processing costs. The special case
(19) has been studied extensively in multiple contexts, includ-
ing quantal response equilibria in the game-theoretic literature
(McKelvey and Palfrey, 1995; Wolpert, 2006), rational inatten-
tion and costly contemplation (Sims, 2003; Ergin and Sarver,
2010), bounded rationality with KL costs (Mattsson and Weibull,
2002; Ortega and Braun, 2013), KL control (Todorov, 2009; Kap-
pen et al., 2012), entropy regularization (Williams and Peng,
1991; Mnih et al., 2016), robustness (Maccheroni et al., 2006;
Hansen and Sargent, 2008), the emergence of heuristics (Binz
et al., 2020), thermodynamic models of computation (Wolpert,
2019), and the analysis of information flow in perception-action
systems (Tishby and Polani, 2011; Still, 2009). While (19) is of-
ten regarded as an abstract measure of uncertainty reduction or
a generic proxy for information processing costs, it can also be
p(X, S, A)
A
X′  
S′  S
X
p0(X, S, A)
prior model utility functionposterior model
U(X′  )
ingredients
maxp {⟨U⟩p− 1
β DKL(p∥p0)}
recipe
Trading oﬀ utility and uncertainty by maximizing free energy
A
X′  
S′  S
X
X′  
Figure 6.Overview of how to apply utility maximization with information
processing costs to the example from Section 2.
viewed as a physical capacity constraint, where the information
that is required to achieve a certain expected utility is considered
to be sent over a channel to the actuator (Miller, 1956; Garner,
1962; MacRae, 1970; Tatikonda and Mitter, 2004; Bhui and Ger-
shman, 2018). This view is also consistent with the maximum
entropy principle, as (18) and (19) favor distributions pthat can
be generated from p0 most easily in terms of statistics, and there-
fore with minimum communication complexity between p0 and
p(Harsha et al., 2010).
4.2. A Simple Example
Ingredients. Consider the probabilistic model shown in Fig 1
with the joint distribution p0(X,S,A) that is specified by the
factors in the decomposition (1). Here, S and X denote the
current environmental state and the corresponding observation,
and Adenotes the action that must be determined in order to
drive the system into a new state S′with observation X′. The
decision-making problem is specified by assuming that we have
given a utility function U over future observations X′which
the decision-maker seeks to maximize by selecting an action A,
while only having access to the current observation X. This
means that the decision-maker has control over the distribution
p(A|X), which replaces the prior p0(A) in the factorization (1)
of the prior model p0(X,S,A) to determine the factorization
of the posterior model p(X,S,A) in terms of the fixed compo-
nents in p0 (cf. Fig 6) as
p(X,S,A) = p0(X′|S′) p0(X|S) p0(S′|S,A) p0(S)  
p0(X,S|A)
p(A|X) .
(20)
Free energy from constraints. Further assuming that the decision-
maker is subject to an information processing constraint
DKL(p∥p0) ≤C0, for some non-negative bound C0, results in
the unconstrained optimization problem maxpF(p) with free
10 The Two Kinds of Free Energy and the Bayesian Revolution
energy given by (18), where the trade-off parameter βis tuned
to comply with the bound C0. Since the action distribution
p(A|X) is the only distribution in the posterior model (20) that
changes during decision-making, i.e., during the transformation
from prior to posterior, the total free energy simplifies to
F(p) = ⟨U⟩p(X,S,A) −1
βDKL(p(X,S,A)∥p0(X,S,A))
= ⟨V(X,A)⟩p(A|X)p(X) −1
β
⟨
DKL(p(A|X)∥p0(A))
⟩
= ⟨FA(p(A|X))⟩p(X) ,
where we have written p0(x|s)p0(s) = p(s|x)p(x) using Bayes
rule (2), and
V(X,A) :=
∑
s,s′,x′p(s|X) p0(s′|s,A) p0(x′|s′) U(x′) ,
FA(p(A|X)) := ⟨V(X,A)⟩p(A|X) −1
βDKL(p(A|X)∥p0(A)) .
Note that, here the expectation with respect to p(X) does not
affect the optimization with respect to p(A|X) since it can be
performed pointwise for each particular realization xof X. In
fact, we would have obtained the same result when conditioning
on an arbitrary value X= xfrom the outset. However, in gen-
eral, optimal information processing strategies may depend on
the entire distribution p(X) and can therefore not be obtained
from only considering single observations x, for example when
also optimizing with respect to the prior p0(A), see e.g., (Ge-
newein et al., 2015).
Free energy maximization.The optimal action distribution
p∗(A|X) maximizing FA is a Boltzmann distribution (8) with
“energy”V(X,A) and prior p0(A),
p∗(A|X) = 1
Z(X) p0(A) eβV(X,A) , (21)
where Z(X) := ∑
ap0(a)eβV(X,a). Note that in order to eval-
uate the utility V, it is required to determine the Bayes’ posterior
p(S|X). This shows how in a utility-based approach, the need to
perform Bayesian inference results directly from the assumption
about which variables are observed and which are not.
4.3. Critical points
The main idea of free energy in the context of information
processing with limited resources is that any computation can
be thought of abstractly as a transformation from a distribution
p0 of prior knowledge to a posterior distribution pthat encap-
sulates an advanced state of knowledge resulting from delibera-
tion. The progress that is made through such a transformation
is quantitatively captured by two measures: the expected utility
⟨U⟩pthat quantifies the quality of pand C(p,p0) that measures
the cost of uncertainty reduction from p0 to p. Clearly, the crit-
ical point of this framework is the choice of the cost function C.
In particular, we could ask whether there is some kind of univer-
sal cost function that is applicable to any perception-action pro-
cess or whether there are only problem-specific instantiations.
Of course, having a universal measure that allows applying the
same concepts to extremely diverse systems is both a boon and a
bane, because the practical insights it may provide for any con-
crete instance could be very limited. This is the root of a number
of critical issues:
(i) What is the cost C? An important restriction of all delibera-
tion costs of the form C(p,p0) is that they only depend on the
initial and final distributions and ignore the process of how to
get from p0 to p. When varying a single resource (e.g., processing
time) we can use C(p,p0) as a process-independent proxy for the
resource. However, if there are multiple resources involved (e.g.,
processing time, memory, and power consumption), a single cost
cannot tell us how these resources are weighted optimally with-
out making further process-dependent assumptions. In general,
the theory makes no suggestions whatsoever about mechanical
processes that could implement resource-optimal strategies, it
only serves as a baseline for comparison. Finally, simply requir-
ing the measure to be monotonic in the uncertainty reduction,
does not uniquely determine the form of C, as there have been
multiple proposals of uncertainty measures in the literature (see
e.g. (Csiszár, 2008)), where relative entropy is just one possibility.
However, relative entropy is distinguished from all other uncer-
tainty measures in its additivity property, that for example allows
to express optimal probabilistic updates from p0 to pin terms of
additions or subtractions of utilities, such as log-likelihoods for
evidence accumulation in Bayesian inference.
(ii) What is the utility? When systems are engineered, utilities
are usually assumed to be given such that desired behavior is
specified by utility maximization. However, when we observe
perception-action systems, it is often not so clear what the util-
ity should be, or in fact, whether there even exists a utility that
captures the observed behavior in terms of utility maximization.
This question of the identifiability of a utility function is stud-
ied extensively in the economic sciences, where the basic idea is
that systems reveal their preferences through their actual choices
and that these preferences have to satisfy certain consistency ax-
ioms in order to guarantee the existence of a utility function.
In practice, to guarantee unique identifiability these axioms are
usually rather strong, for example ignoring the effects of history
and context when choosing between different items, or ignor-
ing the possibility that there might be multiple objectives. When
not making these strong assumptions, utility becomes a rather
generic concept, like the concept of probability, and additional
assumptions like soft-maximization are necessary to translate
from utilities to choice probabilities.
(iii) The problem of infinite regress. One of the main concep-
The Two Kinds of Free Energy and the Bayesian Revolution 11
tual issues with the interpretation of Cas a deliberation cost is
that the original utility optimization problem is simply replaced
by another optimization problem that may even be more diffi-
cult to solve. This novel optimization problem might again re-
quire resources to be solved and could therefore be described by a
higher-level deliberation cost, thus leading to an infinite regress.
In fact, any decision-making model that assumes that decision-
makers reason about processing resources are affected by this
problem (Russell and Subramanian, 1995; Gigerenzer and Selten,
2001). A possible way out is to consider the utility-information
trade-off simply an as if description, since perception-action
systems that are subject to a utility-information trade-off do
not necessarily have to reason or know about their deliberation
costs. It is straightforward, for example, to design processes that
probabilistically optimize a given utility with no explicit notion
of free energy, but for an outside observer the resulting choice
distribution looks like an optimal free energy trade-off (Ortega
and Braun, 2014).
In summary, the free energy trade-off between utility and in-
formation primarily serves as a normative model for optimal
probability assignments in information-processing nodes or net-
works. Like other Bayesian approaches, it can also serve as a
guide for constructing and interpreting systems, although it is
in general not a mechanistic model of behavior. In that respect
it shares the fate of its cousins in thermodynamics and coding
theory (Shannon, 1948) in that they provide theoretical bounds
on optimality but devise no mechanism for processes to achieve
these bounds.
5. Variational free energy in Active Inference
5.1. The basic idea
Variational free energy is the main ingredient used in the Free
Energy Principle for biological systems in the neuroscience lit-
erature (Friston, 2005, 2010; Friston et al., 2015, 2006), which
has been considered as “arguably the most ambitious theory of
the brain available today” (Gershman, 2019). Since variational
free energy in itself is just a mathematical construct to measure
the dissimilarity between distributions and functions—see Sec-
tion 3—, the biological content of the Free Energy Principle must
come from somewhere else. The basic biological phenomenon
that the Free Energy Principle purports to explain is homeosta-
sis, the ability to actively maintain certain relevant variables (e.g.,
blood sugar) within a preferred range. Usually, homeostasis is
applied as an explanatory principle in physiology whereby the
actual value of a variable is compared to a target value and cor-
rections to deviation errors are made through a feedback loop.
However, homeostasis has also been proposed as an explana-
tory principle for complex behavior in the cybernetic literature
(Wiener, 1948; Ashby, 1960; Powers, 1973; Cisek, 1999)—for ex-
ample, maintaining blood sugar may entail complex feedback
loops of learning to hunt, to trade and to buy food. Crucially,
being able to exploit the environment in order to attain favor-
able sensory states, requires implicit or explicit knowledge of the
environment that could either be pre-programmed (e.g., insect
locomotion) or learnt (e.g., playing the piano).
The Free Energy Principle was originally suggested as a the-
ory of cortical responses (Friston, 2005) by promoting the free
energy formulation of predictive coding that was introduced by
Dayan and Hinton with the Helmholtz machine (Dayan et al.,
1995). It found its most recent incarnation in what is known
as Active Inference that attempts to extend variational Bayesian
inference to the problem of action selection. Here, the target
value of homeostasis is expressed through a probability distribu-
tion pdes under which desired sensory states have a high prob-
ability. The required knowledge about the environment is ex-
pressed through a generative model p0 that relates observations,
hidden causes, and actions. As the generative model allows to
make predictions about future states and observations, it en-
ables to choose actions in such a way that the predicted con-
sequences conform to the desired distribution. In Active Infer-
ence, this is achieved by merging the generative and the desired
distributions, p0 and pdes, into a single reference function φto
which trial distributions q over the unknown variables are fit-
ted by minimizing the variational free energy F(q∥φ). This free
energy minimization is analogous to variational Bayesian infer-
ence, where the reference is always given by a joint distribution
evaluated at observed quantities (cf. Section 3.2.1). In the result-
ing homeostatic process, the trial distributions q play the role
of internal variables that are manipulated in order to achieve
desired sensory consequences that are not directly controllable.
Minimizing variational free energy by the alternating variation
of trial distributions over actions qActions and trial distributions
over hidden states qStates,
min
qActions
F(q∥φ)
  
Action
and min
qStates
F(q∥φ)
  
Perception
, (22)
is then equated with processes of action and perception.
In a nutshell, the central tenet of the Free Energy Principle
states that organisms maintain homeostasis through minimiza-
tion of variational free energy between a trial distribution qand
a reference function φby acting and perceiving. Sometimes the
even stronger statement is made that minimizing variational free
energy is mandatory for homeostatic systems (Friston, 2013; Cor-
coran and Hohwy, 2018).
5.2. A Simple Example
Ingredients. Applying the Active Inference recipe (cf. Fig 7) to
our running example from Fig 1 with current and future states
S,S′, current and future observations X,X′, and action A, we
12 The Two Kinds of Free Energy and the Bayesian Revolution
q(X′ , S, A)
X′ A
X′ 
S′ S
X
p0(X, S, A)
generative model desired future trial distributions
pdes(X′ )
ingredients
ϕ(X, S, A)combine model and desired 
distributions into reference
ϕ ∝ p0 eQ(A) ϕ ∝ p0 pdes
A X′ Q(A)
X′ X′ 
min F(q∥ϕ)q
recipe
ﬁt trial distributions to reference
by minimizing free energy1 2
Schwöbel et al. 2018Friston et al. 2013-2017
A
X′ 
S′ S
A
X′ 
S′ S
or or A
X′ 
S′ S
full mean-ﬁeld 
Friston et al. 2013/15
partial mean-ﬁeld 
Friston et al. 2016/17
exact/Bethe approx.
Schwöbel et al. 2018 
Parr et al. 2019
or
minq(S)
F(q(S)q(S′ | A)q(A)∥ϕref)
minq(S′ |A)
F(q(S)q(S′ | A)q(A)∥ϕref)
minq(A)
F(q(S)q(S′ | A)q(A))∥ϕref)
through alternating optimization:
(exemplary for                                    , i.e. partial mean-ﬁeld)q = q(S)q(S′ | A)q(A)
Figure 7.Overview of the Active Inference recipe, applied to our example from Fig 1.
need a generative model p0, a desired distribution pdes, and trial
distributions q. The generative model p0(X,S,A) is specified
by the factors in the decomposition (1), the desired distribution
pdes(X′) is a given fixed probability distribution over future
sensory states X′, and the trial distributions q are probability
distributions over all unknown variables, S,S′,X′, and A.
In most treatments of Active Inference in the literature, the
trial distributions qare simplified, either by a full mean-field ap-
proximation over states and actions (Friston et al., 2013, 2015),
by a partial mean-field approximation where the dependency on
actions is kept but the states are treated independently of each
other (Friston et al., 2016, 2017a), or more recently (Schwöbel
et al., 2018; Parr et al., 2019) by the so-called Bethe approxima-
tion (Yedidia et al., 2001; Heskes, 2003), where subsequent states
are allowed to interact. In the partial mean-field assumption of
(Friston et al., 2016), the trial distribution over X′is fixed and
given by p0(X′|S′), while for A, Sand S′the trial distributions
are variable but restricted to be of the mean-field form for Sand
S′,
q(S,A) = q(S) q(S′|A) q(A), (23)
i.e., the hidden states S and S′are assumed to be independent
given A. While mean-field approximations can be good enough
for simple perceptual inference, where a single hidden cause
might be responsible for a set of observations, they can be too
strong simplifications for sequential decision-making problems
where the next state S′depends on the previous state S. In fact,
as can be seen for example in S.2, mean-field assumptions may
fail to show goal-directed behavior even for very simple tasks
such as the navigation in a grid world. A less restrictive assump-
tion would be a Bethe approximation, a special case of Kikuchi’s
cluster variation method (Kikuchi, 1951), which allows Sand S′
as well as S′and X′to be stochastically dependent—cf. Section
C in Appendix A.1, where we derive the update equations un-
der the Bethe assumption for the simple example of this section.
In general, the Bethe approximation achieves exact marginals
in tree-like models, such as the models that are considered in
the Active Inference literature, because it results in update equa-
tions that are equivalent to Pearl’s belief propagation algorithm
(Yedidia et al., 2001; Pearl, 1988).
Reference function. The reference φ is constructed by combin-
ing the two distributions pdes and p0. To do so, there have been
several proposals in the Active Inference literature, which fall
into one of two categories: either a specific value function Q
is defined (containing pdes), which is multiplied to the genera-
tive model using a soft-max function (Friston et al., 2015, 2016,
2017a),
φ(X′,S,A) := p0(X= x,X′,S|A) 1
Zp0(A)eQ(A) , (24)
or the desired distribution is multiplied directly to the generative
model (Schwöbel et al., 2018),
φ(X′,S,A) := pdes(X′) p0(X= x,X′,S,A). (25)
The Two Kinds of Free Energy and the Bayesian Revolution 13
While the reference function in (25) is already completely
specified, we still need to know how to determine the value func-
tion Qin the case of (24). For the partial mean-field assumption
(23) it is defined in the literature (Friston et al., 2016, 2017a) as
Q(a) := ⟨U(X′,S′)⟩q(X′,S′|A= a) + H
(
q(X′|A= a)
{
, (26)
where U(x′,s′) := log pdes(x′) + log p0(x′|s′) favors both
desirable and plausible future observations x′. While here de-
sirability and plausibility is built into the value function Qid-
iosyncratically, in utility-based approaches (cf. Section 4.2) only
desirability has to be put into the design of the utility function,
because there the likelihood p0(X′|S′) of future observations is
automatically taken into account by the expected utility V that
is (soft-)maximized by (21). Moreover, since Qcan be rewritten
as
Q(a) = −DKL
(
q(X′|A)∥pdes(X′)
{
−
⟨
H
(
p0(X′|S′)
{⟩
q(S′|A) ,
the extra entropy term in (26) has the effect of actions leading
to consequences that more or less match the desired distribution,
while also explicitly punishing actions that lead to a high vari-
ability of observations (by requiring a low average entropy of
p0(X′|S′)), rather than trying to produce the single most desired
outcome—see the discussion at the end of Section 5.3. Note also
that the value function Qdepends (non-linearly) on the trial dis-
tribution q(S′|A), because q(X′|A) = ∑
s′p0(X′|s′)q(s′|A)
is itself a function of q(S′|A), which is problematic during free
energy minimization (see (ii) in Section 5.3).
Free energy minimization. Once the form of the trial distributions
q—e.g. by a partial mean-field assumption (23) or a Bethe approx-
imation (see Derivation of exemplary update equations)—and
the reference φare defined, the variational free energy is simply
determined by F(q∥φ). In the case of a mean-field assumption,
the resulting free energy minimization problem is solved ap-
proximately by performing an alternating optimization scheme,
in which the variational free energy is minimized separately with
respect to each of the variable factors in a factorization of q, for
example by alternating between minq(S) F, minq(S′|A) F, and
minq(A) F in the case of the partial mean-field assumption (23),
where in each step the factors that are not optimized are kept
fixed (cf. Fig 7). In Derivation of exemplary update equations
we derive the update equations for the cases (24) and (25) under
mean-field and Bethe approximations for the one-step example
discussed in this section. Mean-field solutions for the general
case of arbitrarily many timesteps together with their exact so-
lutions can be found in Notebook: Comparison of different for-
mulations of Active Inference, where we also highlight the the-
oretical differences between various proposed formulations of
Active Inference. The effect of some of these differences can be
seen in the grid world simulations in Notebook: Grid world sim-
ulations.
5.3. Critical points
The main idea behind Active Inference is to express the prob-
lem of action selection in a similar manner to the perceptual
problem of Bayesian inference over hidden causes. In Bayesian
inference, agents are equipped with likelihood models p0(X|Z)
that determine the desirability of different hypotheses Z un-
der known data X. In Active Inference, agents are equipped
with a given desired distribution pdes(X′) over future outcomes
that ultimately determines the desirability of actions A. An im-
portant difference that arises is that perceptual inference has to
condition on past observations X= x, whereas naive inference
over actions would have to condition on desired future outcomes
X′= x′.
For a single desired future observation x′, Bayesian in-
ference could be applied in a straightforward way by sim-
ply conditioning the generative model p0 on X′= x′. Simi-
larly, one could condition on a desired distribution pdes(X′)
using Jeffrey’s conditioning rule (Jeffrey, 1965), resulting in
p(A|pdes) = ∑
x′p(A|x′) pdes(x′), which could be imple-
mented by first sampling a goal x′∼pdes(X′) and then infer-
ring p(A|x′) given the single desired observation x′. However,
one of the problems with such a naive approach is that the choice
of a goal is solely determined by its desirability, whereas its real-
izability for the decision-maker is not taken into account. This is
because by conditioning on pdes, the decision-maker effectively
seeks to choose actions in order to reproduce or match the desired
distribution.
To overcome this problem, Control as Inference or Planning
as Inference approaches in the machine learning literature (Tou-
ssaint and Storkey, 2006; Todorov, 2008; Kappen et al., 2012;
Levine, 2018; O’Donoghue et al., 2020) do not directly condition
on desired future observations but on future success by introduc-
ing an auxiliary binary random variable Rsuch that R = 1 en-
codes the occurence of desired outcomes. The auxiliary variable
Rcomes with a probability distribution p0(R|X′,...) that deter-
mines how well the outcomes satisfy desirability criteria of the
decision-maker, usually defined in terms of the reward or utility
attached to certain outcomes—see the discussion in (iii) below.
The extra variable gives the necessary flexibility to infer success-
ful actions by simply conditioning on R= 1. The advantage of
such an approach over direct Jeffrey conditionalization given a
desired distribution over future observations can be seen in the
grid world simulations in Notebook: Grid world simulations, es-
pecially the ability of choosing a desired outcome that is not only
desirable but also achievable—see also Fig 8.
Active Inference tries to overcome the same problem of recon-
ciling realizability and desirability, but without explicitly intro-
ducing extra random variables and without explicitly condition-
ing on the future. Instead, the desired distribution is combined
with the generative model to form a new reference function φ
14 The Two Kinds of Free Energy and the Bayesian Revolution
such that the posteriors q∗resulting from the minimization of
the free energy F(q∥φ) contain a baked-in tendency to reach
the desired future encoded by φ. This approach is the root of
a number of critical issues with current formulations of Active
Inference:
(i) How to incorporate the desired distribution into the reference?
Instead of using Bayesian conditioning directly in order to con-
dition the generative model p0 on the desired future, in Active
Inference it is required that the reference φcontains the desired
distribution in a way such that actions sampled from the result-
ing posterior model are more likely if they lead to the desired fu-
ture. As can be seen already for the one-step case in (24) and (25),
the method of how to incorporate the desired distribution into
the reference function is not unique and does not follow from
first principles. There have been essentially two different pro-
posals in the literature on Active Inference of how to combine
the two distributions pdes and p0 into φ(cf. Fig 7): Either a hand-
crafted value function Qis designed that specifically modifies
the action probability of the generative model, or the probability
over futures X′under the generative model p0 is modified by di-
rectly multiplying pdes to the likelihood p0(X′|S′). We discuss
both of these proposals in (ii) and (iii) below.
(ii) Proposal 1: Q-value Active Inference (Friston et al., 2013,
2015, 2016, 2017a) In the most popular formulation of Active
Inference, the probability over actions in the reference φis de-
fined by 1
Zp0(A)eQ(A), where the value function Q(also called
the “expected free energy”) depends non-linearly on the trial dis-
tributions q, as can be seen exemplarily in (26) for the one-step
case under the partial mean-field assumption of Friston et al.
(2016, 2017a), where q(S′|A) enters Q through q(X′|A) =∑
s′p0(X′|s′)q(s′|A). Note that, because of this non-linearity
the alternating free energy minimization would have no closed-
form solutions (cf. Derivation of exemplary update equations).
This means that both the trial distributions qand the reference
φ = φ(q) will change when q is varied during the minimiza-
tion of the total variational free energy F(q∥φ(q)), as would be
required when stipulating a single free energy functional for op-
timization. This highlights an important conceptual difference
to variational Bayesian inference, where one assumes a fixed ref-
erence φ—resulting from the evaluation of a fixed probabilistic
model p0 at known variables (see Section 3.2.1)—to which dis-
tributions qare fitted by minimizing F(q∥φ). In contrast, when
changing the reference φ(q) during the optimization process, it
is no longer clear what is actually achieved by this minimization.
As demonstrated by Notebook: Grid world simulations, this is-
sue has immediate practical implications, as respecting or ignor-
ing the extra qdependency can result in very different behavior
even in simple grid world simulations.
In the Active Inference literature, however, the extra q-
dependency of Qis largely ignored. Instead of optimizing the full
free energy F(q∥φ(q)) with respect to state and action distri-
butions, one alternatingly optimizes the free energy over states
FA for each action Aand then the full free energy with respect
to action distributions only, so that action and perception effec-
tively optimize two different free energies. It is crucial to note,
however, that unlike in variational Bayesian inference with fixed
reference, this separation does not follow from the formalism of
variational free energy, but is a design choice of the Active In-
ference framework that imposes this separation by force (see the
Appendix Separation of model and state variables for more de-
tails). This way, both separate optimizations can be considered as
variational inference in each single update, even though when al-
ternating them the reference φstill changes across the combined
optimization process. This is in contrast to alternating optimiza-
tion schemes in variational inference (e.g., in the Bayesian EM
algorithm) where the reference φdoes not change between opti-
mization steps. Thus, there are two choices: Either Q-value Ac-
tive Inference is regarded as some kind of approximation to vari-
ational inference under a single total free energy, or one has to
give up the idea of a single free energy function that is optimized.
Either way, the combined process of action and perception does
not correspond to a single variational inference process.
Finally, another important practical issue with Q-value Active
Inference models is that the definition of Qrelies on a mean-
field approximation of the trial distributions q, under which hid-
den states are assumed to be stochastically independent. This
simplification is too strong for sequential decision-making tasks,
which renders the approach unfit for environments where the
current state depends stochastically on previous states (see Note-
book: Grid world simulations for a demonstration).
(iii) Proposal 2: direct Active Inference (Schwöbel et al., 2018)
When multiplying pdes to the generative model directly, as in
(25), then the resulting reference φis no longer given by a joint
distribution of observations, states, and actions (since in general∑
x′pdes(x′)p0(x′|S′) ̸= 1 ). Instead, this formulation of Ac-
tive Inference turns out to be a special case of previous Control
as Inference approaches in the machine learning literature (Tou-
ssaint and Storkey, 2006; Levine, 2018), where one conditions
on an auxiliary success variable R. In particular, for our running
example from Fig 1 with a probabilistic model of the form (1),
Control as Inference defines
p0(R= 1|X′,S′,A) := er(X′,S′,A) = 1−p0(R= 0|X′,S′,A) ,
where r = r(X′,S′,A) denotes a general (negative) reward
function determining desirability. The full joint of the new set
of variables is then given by
p0(R,X,S,A) = p0(R|X′,S′,A) p0(X,S,A). (27)
Control as Inference then conditions actions on both, the his-
tory and future success ( R= 1). For our one-step example, this
The Two Kinds of Free Energy and the Bayesian Revolution 15
results in the Bayes’ posterior
p(A|X= x,R= 1) = 1
Z
∑
x′,s,s′
p0(R= 1|x′,s′,A) p0(x,s,A) .
(28)
It is straightforward to identify pdes(X′) of Active Inference
as a particular choice of a success probability p0(R= 1|X′), or
equivalently, log pdes(X′) as a reward function r = r(X′), so
that the joint distribution (27) reduces to the reference function
φin (25). Thus, the version of Active Inference in (Schwöbel et al.,
2018) is simply a variational formulation of Control as Inference
that approximates exact posteriors of the form (28), like other
previous variational Bayes’ approaches (Toussaint, 2009; Ziebart,
2010; Levine, 2018).
In summary, the assumption of a desired distribution pdes
over future outcomes has led to various attempts in the Active
Inference literature of using probabilistic inference to determine
profitable actions. Either an action distribution 1
Zp0(A)eQ(A)
is built into the reference function, which presupposes optimal
behavior by designing a value function Qthat leads to desired
consequences, or the outcome probability under the generative
model p0 is modified directly by multiplying pdes to p0. The lat-
ter case is the variational version of Control as Inference, well-
known in the machine learning literature (Toussaint and Storkey,
2006; Todorov, 2008; Toussaint, 2009; Ziebart, 2010; Kappen
et al., 2012; Levine, 2018; O’Donoghue et al., 2020). Considering
the issues of Q-value Active Inference discussed above, and the
fact that Control as Inference does not rely on a desired distri-
bution over outcomes, we could ask whether formulating pref-
erences by assuming a desired distribution is well-advised. As
can be seen from Fig 8, the difference between purely inference-
based methods, expected utility approaches, and Active Infer-
ence is mainly in how they treat the desired distribution. Should
pdes be matched or is it good enough if actions are chosen that
lead to a high desired outcome probability? While Control as
Inference and utility-based models essentially take the latter ap-
proach, Q-value Active Inference answers this question by re-
quiring that the desired distribution should be matched as long
as the average entropy of p0(X′|S′) is small.
6. So What Does Free Energy Bring To the Table?
6.1. A Practical Tool
It is unquestionable that the concept of free energy has seen
many fruitful practical applications outside of physics in the sta-
tistical and machine learning literature. As has been discussed in
Section 3, these applications generally fall into one of two cate-
gories, the principle of maximum entropy, and a variational for-
mulation of Bayesian inference. Here, the principle of maximum
entropy is interpreted in a wider sense of optimizing a trade-off
between uncertainty (entropy) and the expected value of some
quantity of interest (energy), which in practice often appears in
the form of regularized optimization problems (e.g., to prevent
overfitting) or as a general inference method allowing to deter-
mine unbiased priors and posteriors (cf. Section 3.1). In the vari-
ational formulation of Bayes’ rule, free energy plays the role of an
error measure that allows to do approximate inference by con-
straining the space of distributions over which free energy is op-
timized, but can also inform the design of efficient iterative in-
ference algorithms that result from an alternating optimization
scheme where in each step the full variational free energy is op-
timized only partially, such as the Bayesian EM algorithm, belief
propagation, and other message passing algorithms (cf. Section
3.2).
It is important to realize that, while the mathematical ex-
pressions of a free energy from constraints with “energy” Eand
trade-off parameter β and a variational free energy with ref-
erence φcan formally be transformed into each other by φ =
e−βE, the two kinds of free energy are inherently distinct, both
methodically and by their motivation. In the case of the free en-
ergy from constraints, we are given a constraint on some quan-
tity Eand we are trying to fulfil this constraint with minimum
bias by selecting a distribution that trades off the two compet-
ing terms Eand entropy. This trade-off also gives the reason for
the existence of the Lagrange multiplier βthat has to be deter-
mined according to the constraint. In this sense the free energy
from constraints is just a special case of the far more general La-
grangian method when applied to the optimization of expected
values ⟨E⟩punder entropy constraints (or the other way around).
In contrast, variational free energy is simply a tool to represent
the normalization of a reference function φin terms of an op-
timization problem, and therefore does a priori not assume the
existence of some quantity Ethat we may have observed in an
experiment or that has any other constraints attached, nor does
one explicitly consider entropy to be constrained or optimized.
Therefore, even though starting from a (positive) reference func-
tion φwe can always invent the existence of some quantity Eand
some multiplier βsuch that φ= e−βE, this does not explain why
these quantities should exist or why they should be mapped into
each other in that particular way. The Lagrangian method, on
the other hand, explains why for a given constraint on Ewe have
a Lagrange multiplier β, how it is determined, and why the equi-
librium distribution has the form p∗= 1
Ze−βE.
6.2. Theories of Intelligent Agency
These practical use-cases of free energy formulations have
also influenced models of intelligent behavior. In the cognitive
and behavioral sciences, intelligent agency has been modelled in
a number of different frameworks, including logic-based sym-
bolic models, connectionist models, statistical decision-making
models, and dynamical systems approaches. Even though statis-
tical thinking in a broader sense can in principle be applied to any
16 The Two Kinds of Free Energy and the Bayesian Revolution
pdes(X′ )
p(X′ | A = 1)
x1 x2 x3
p(X′ | A = 2)
x1 x2 x3
desired distribution
predictive distribution for A = 2
predictive distribution for A = 1
A = 1 A = 2
Conditioning on       
        (Jeﬀrey conditionalization)
Bayesian conditioning on  
auxiliary success variable  
(Control as Inference/ 
direct Active Inference)
A = 1 A = 2
early Q-value Active Inference 
(match desired and  
predictive distributions)
A = 1 A = 2
recent Q-value Active Inference 
(additionally subtracting entropy of emission probability)
A = 1 A = 2
p(A) = 1
Z ∑i
p(xi | A) pdes(xi)
−DKL(p(X′ | A)∥pdes(X′ ))
Expected Utility 
(soft-maximization,    = 1)
A = 1 A = 2
β
x1 x2 x3
A = 1 A = 2
p0(X′ | S′ )p0(X′ | S′ )
| {z }
<latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit><latexit sha1_base64="Jyn9l+1vPNzSc6seaFzWXG4b/p0=">AAAC+XichVHLSsNAFD2N7/qqunRTLIKrkoigS/GFG0HBtoKVMknHGpom6SQRNPQv/AF34tadW/0J/RYXnoxRUBEnTO6dc889c+9cO/TcKDbNl4IxNDwyOjY+UZycmp6ZLc3N16MgUY6sOYEXqBNbRNJzfVmL3diTJ6GSomd7smF3t7N441KqyA384/gqlGc90fHdc9cRMaFWqdpM/LZUthKOTJv9fiLaf/0HrXTQKlXMqqlX+bdj5U4F+ToMSq9ooo0ADhL0IOEjpu9BIOJ3CgsmQmJnSIkpeq6OSwxQZG5CliRDEO3y3+HpNEd9njPNSGc7vMXjVswsY5l7TyvaZGe3SvoR7Rv3tcY6f96QauWswitam4oTWvGAeIwLMv7L7OXMz1r+z8y6inGODd2Ny/pCjWR9Ol86O4woYl0dKWNXMzvUsPX5ki/g09ZYQfbKnwpl3XGbVmgrtYqfKwrqKdrs9VkPx2z9HOpvp75atcyqdbRW2dzKBz6ORSxhhVNdxyb2ccg6HNzgEU94NlLj1rgz7j+oRiHPWcC3ZTy8A7bNo6I=</latexit>
pdes(X′ )
p ( A )=
1
Z
X
i
p ( x i | A ) p des ( x i )
<latexit sha1_base64="rtGFmdQ+3YchVWeKr6TYAjpVOyI=">AAADDXichVHLThRBFL20Dx6+Rli6qTgxGRIz6SYkwoIEUAkbE0wcINKkU11TM1SmX6mqIWI738Bv+APuDFt2bFG/xYWnysZEiaE61ffWueeeurduWmXK2DD8PhXcun3n7vTM7Ny9+w8ePmo9nt815VgL2RNlVur9lBuZqUL2rLKZ3K+05Hmayb109NLF946lNqos3tmTSh7mfFiogRLcAkpam1VnY5GtsXiguaijSR3n3B4JnrH3ExabcZ4oVnU+JOoTaPFzViWeoPO6L83EBRaTVjvshn6x607UOG1q1k7Z+kEx9akkQWPKSVJBFn5GnAy+A4oopArYIdXANDzl45ImNIfcMVgSDA50hP8Qp4MGLXB2msZnC9ySYWtkMnqGveUVU7DdrRK+gf2J/dFjw//eUHtlV+EJbArFWa/4BrilIzBuyswb5lUtN2e6riwNaMV3o1Bf5RHXp/ij8woRDWzkI4xee+YQGqk/H+MFCtgeKnCvfKXAfMd9WO6t9CpFo8ihp2Hd66MejDn6d6jXnd2lbrTcXX273F7fbAY+Q0/oKXUw1Re0Ttu0gzoEfaYLuqRvwWnwJfganP2mBlNNzgL9tYLzXzW1qFg=</latexit>
p ( A )=
X
i
1
Z i
p ( x i | A ) p des ( x i )
<latexit sha1_base64="/E+CyBg93o2gvFsjYqGGPHNNFD0=">AAADEnichVHLThRBFL00KA9fAy7dVJyYDImZdBMSYUECvmBjgokDRJp0qmtqhsr0K1U1RGznL/wNf8CdceuOFUb+xIWnysZEiaE61ffWueeeurduWmXK2DA8nwqmZ27cnJ2bX7h1+87de63FpT1TjrWQPVFmpT5IuZGZKmTPKpvJg0pLnqeZ3E9Hz1x8/0Rqo8rijT2t5FHOh4UaKMEtoKS1XXW2ltkGi804TxSLB5qLOprUcc7tseAZe5uoCYsfs6rzLlEfwHV+4sM6r/vSTFxgOWm1w27oF7vqRI3Tpmbtlq3vFFOfShI0ppwkFWThZ8TJ4DukiEKqgB1RDUzDUz4uaUILyB2DJcHgQEf4D3E6bNACZ6dpfLbALRm2RiajR9gvvWIKtrtVwjewP7Hfe2z43xtqr+wqPIVNoTjvFV8Bt3QMxnWZecO8rOX6TNeVpQGt+W4U6qs84voUf3SeI6KBjXyE0QvPHEIj9ecTvEAB20MF7pUvFZjvuA/LvZVepWgUOfQ0rHt91IMxR/8O9aqzt9KNVrvrr1fbm0+bgc/RA3pIHUz1CW3SDu2iDkGf6Ix+0EXwMfgcfAm+/qYGU03OffprBd9+AdRcqfo=</latexit>
p ( A )=
1
Z
exp
h X
i
p ( x i | A ) log p des ( x i )
i
<latexit sha1_base64="0cI7I7+SBHqBwInjaQ8ptNCfEeE=">AAADJHichVFNb9QwEJ2Gr7Z8LXDkYrFC2kpolVSVgANSKR/iglQkti1sVpHj9abWOonlZKuWkL/D3+APcEMc4MCNK/wBDjybFAkqVEfOjN/Me57xpEarqg7Dz0vBmbPnzl9YXlm9eOnylau9a9d3qnJhhRyJUpd2L+WV1KqQo1rVWu4ZK3mearmbzh+5+O6BtJUqi5f1kZGTnGeFminBa0BJ75UZPFxjD1g8s1w0UdvEOa/3BdfN67ZlsTw0LN5S2ZjF1SJPFDODw0S9BSW+w2JdZswknmHzZiqr1kXXPGOS9PrhMPSLnXSizulTt7bL3heKaUolCVpQTpIKquFr4lThG1NEIRlgE2qAWXjKxyW1tAruAlkSGRzoHP8Mp3GHFjg7zcqzBW7R2BZMRrexn3rFFNnuVgm/gv2J/cZj2X9vaLyyq/AINoXiild8DrymfWScxsy7zONaTme6rmqa0T3fjUJ9xiOuT/FH5zEiFtjcRxg98ZkZNFJ/PsALFLAjVOBe+ViB+Y6nsNxb6VWKTpFDz8K610c9GHP071BPOjvrw2hjeP/FRn9zqxv4Mt2kWzTAVO/SJj2jbdQh6D19o+/0I3gXfAg+Bp9+pwZLHecG/bWCr78A0DqxcQ==</latexit>
p ( A )=
1
Z
exp
h X
i
p ( x i | A ) log
p des ( x i )
p ( x i | A )
i
<latexit sha1_base64="KVzvDAJ3oJsbX1w+unwS4OmzS+U=">AAADNHichVFdaxQxFL0dv9r6teqjL8FF2IIsM1JQH4RaP/BFqOC2xZ1lyMxmp2EzHyTZ0jrO3/Jv+Kz4Jr765puoJ3FW0CLNkLk3J+ec3OSmtZLGhuGHleDM2XPnL6yurV+8dPnK1d6167umWuhMjLJKVXo/5UYoWYqRlVaJ/VoLXqRK7KXzx25/71BoI6vylT2uxaTgeSlnMuMWUNKT9eDRBnvI4pnmWRO1TVxwe5Bx1bxuWxaLozrelvmYxWZRJJLVg6NEvoUivsNiVeWdrk68TBfNVJjWcTbaZsmFDSwmSa8fDkM/2Mkk6pI+dWOn6n2kmKZUUUYLKkhQSRa5Ik4G35giCqkGNqEGmEYm/b6gltahXYAlwOBA5/jnWI07tMTaeRqvznCKwtRQMrqN+cw7pmC7UwVyg/gd843H8v+e0HhnV+ExYgrHNe/4ArilAzBOUxYdc1nL6Up3K0szuu9vI1Ff7RF3z+yPzxPsaGBzv8PoqWfm8Ej9+hAvUCKOUIF75aUD8zeeInIfhXcpO0cOP43oXh/1oM3Rv009mezeHUabwwcvN/tb213DV+km3aIBunqPtug57aCOjN7TN/pBP4N3wafgc/DlNzVY6TQ36K8RfP0FHpq4lw==</latexit>
p ( A )=
1
Z
exp
h
  D KL
 
p ( X 0
| A ) k p des ( X 0
)
 
 h H ( p 0 ( X 0
| S 0
) i p 0 ( S 0 | A )
i
<latexit sha1_base64="dNY/OL36Njh2HpG62DwS4t5VZPw=">AAADVXichVFda9RAFL3ZrbWtH90q+OKDg4uYfeiSlIL1QWhrlYIKlbrt4mYJk+xsGnbywSRbrGn+gH9Q9Ff4AxQ8M6YFLdIJk3vnnHvPvTM3yGVclI7zzWq1F24s3lxaXrl1+87d1c7avaMim6tQDMJMZmoY8ELIOBWDMi6lGOZK8CSQ4jiYvdT88alQRZylH8qzXIwTHqXxNA55CcjvfMntnR57wbyp4mHl1pWX8PIk5LL6WNfME59ybzeORmx9zzeMSqo3b2sviCM7t4dPz3d63nl+SU1EUQPtab7H1pkneRpJwfbt3Hd0+CE4ZTC/0tChVkAd1Bj7na7Td8xiVx23cbrUrIOs8508mlBGIc0pIUEplfAlcSrwjcglh3JgY6qAKXix4QXVtILcOaIEIjjQGf4RTqMGTXHWmoXJDlFFYitkMnqC/dooBojWVQX8AvYn9meDRf+tUBll3eEZbADFZaP4DnhJJ4i4LjNpIi96uT5T36qkKW2Z28ToLzeIvmd4qbMHRgGbGYbRKxMZQSMw51O8QAo7QAf6lS8UmLnxBJYbK4xK2ihy6ClY/froB2N2/x3qVedoo+9u9p+/3+xu7zYDX6KH9JhsTPUZbdM+HaCPkH5YD6xHFmt9bf1qL7QX/4S2rCbnPv212qu/AY29vW0=</latexit>
Figure 8.Consequences of assuming a desired distribution pdes for action planning under purely inference-based methods, expected utility, and Active
Inference, in the case of a simple example with two actions, one with a deterministic outcome and one with random outcomes. As can be seen from the
displayed equations, conditioning on pdes (Jeﬀrey conditionalization) and conditioning on success (Control as Inference/direct Active Inference) only diﬀer
in the order of normalizing and taking the expectation over X′. While conditioning on pdes requires to ﬁrst sample a target outcome from pdes before an
action from p(A|x′) can be planned, conditioning on success directly weighs the desirability of an outcomepdes(x′) by its realizability p(x′|A). From this
point of view, the expected utility approach is very similar to Control as Inference (which can also be seen in the grid world environment Notebook: Grid
world simulations), since it also weighs the utility of an outcome with its realizability before so/f_t-maximizing. It only diﬀers in how it treats the desired
distribution as an exponentiated utility, moving the utility values closer together so that optionA= 1 is slightly preferred. The early version (Friston et al.,
2013) of Active Inference is similar to Jeﬀrey conditioning, because decision-makers are also assumed to match the desired distribution, by deﬁning the
value function Qas a KL divergence between the predicted and desired distributions. In later versions of Q-value Active Inference (Friston et al., 2015,
2016, 2017a), the value function Qis modiﬁed by an additional entropy term that explicitly punishes observations with high variability. Consequently,
even when the eﬀect of the action on future observations is kept the same, i.e., the predictive distributionp(X′|A) =
∑
s′p0(X′|s′)p0(s′|A) remains as
depicted in the le/f_t-hand column, the preference over actions now changes completely depending onp0(X′|S′)—whereas in the other approaches, only
the predictive distribution p(X′|A) and pdes(X′) inﬂuence planning. While there might be circumstances where this extra punishment of high outcome
variability could be beneﬁcial, it is questionable from a normative point of view why anything else other than the predicted outcome probability p(X′|A)
should be considered for planning. See Details on the example in Fig 8 for details about the choices made in the example.
The Two Kinds of Free Energy and the Bayesian Revolution 17
of the other frameworks as well, statistical models of cognition
in a more narrow sense have often focused on Bayesian mod-
els, where agents are equipped with probabilistic models of their
environment allowing them to infer unknown variables in or-
der to select actions that lead to desirable consequences (Tenen-
baum and Griffiths, 2001; Wolpert, 2006; Todorov, 2009). Natu-
rally, the inference of unknown variables in such models can be
achieved by a plethora of methods including the two types of free
energy approaches of maximum entropy and variational Bayes.
However, both free energy formulations go one step further in
that they attempt to extend both principles from the case of in-
ference to the case of action selection: utility optimization with
information constraints based on free energy from constraints
and Active Inference based on variational free energy.
While sharing similar mathematical concepts, both ap-
proaches differ in syntax and semantics. An apparent apple of
discord is the concept of utility (Gershman and Daw, 2012). Util-
ity optimization with information constraints requires the de-
termination of a utility function, whereas Active Inference re-
quires the determination of a reference function. In the eco-
nomic literature, subjective utility functions that quantify the
preferences of decision-makers are typically restrictive in order
to ensure identifiability when certain consistency axioms are sat-
isfied. In contrast, in Active Inference the reference function in-
volves determining a desired distribution given by the preferred
frequency of outcomes. However, these differences start to van-
ish when weakening the utility concept to something like log-
probabilities, such that the utility framework becomes more sim-
ilar to the concept of probability that is able to explain arbitrary
behavior. Moreover, Active Inference has to solve the additional
problem of marrying up the agent’s probabilistic model with its
desired distribution into a single reference function (cf. Section
5.3). The solution to this problem is not unique, in particular it
lies outside the scope of variational Bayesian inference, but it is
critical for the resulting behavior because it determines the exact
solutions that are approximated by free energy minimization. In
fact, as can be seen in simple simulations such as S.2, the vari-
ous proposals for this merging that can be found in the Active
Inference literature behave very differently.
Also, both approaches differ fundamentally in their motiva-
tion. The motivation of utility optimization with information
constraints is to capture the trade-off between precision and un-
certainty that underlies information processing. This trade-off
takes the form of a free energy once an informational cost func-
tion has been chosen (cf. Section 4.3). Note that Bayes’ rule can
be seen as the minimum of a free energy from constraints with
log-likelihoods as utilities, even though this equivalence is not
the primary motivation of this trade-off. In contrast, Active In-
ference is motivated from casting the problem of action selection
itself as an inference process (Friston et al., 2013), as this allows
to express both action and perception as the result of minimizing
the same function, the variational free energy. However, there is
no mystery in having such a single optimization function, be-
cause the underlying probabilistic model already contains both
action and perception variables in a single functional format and
the variational free energy is just a function of that model. More-
over, while approximate inference can be formulated on the ba-
sis of variational free energy, inference in general does not rely
on this concept, in particular inference over actions can easily be
done without free energy (Dayan and Hinton, 1997; Toussaint
and Storkey, 2006; Todorov, 2008; Kappen et al., 2012; Levine,
2018).
However, there are also plenty of similarities between the two
free energy approaches. For example, the assumption of a soft-
max action distribution in Active Inference is similar to the pos-
terior solutions resulting from utility optimization with infor-
mation constraints. Moreover, the assumption of a desired fu-
ture distribution relates to constrained computational resources,
because the uncertainty constraint in a desired distribution over
future states may not only be a consequence of environmental
uncertainty, but could also originate from stochastic preferences
of a satisficing decision-maker that accepts a wide range of out-
comes. In fact, as we have seen in the discussion around Fig 8,
various methods for inference over actions differ in how they
treat preferences given by a distribution over desired outcomes:
Some of them try to match the predictive and desired distribu-
tions, while others simply seek to reach states whose outcomes
have a high desired probability. In S.2, we provide a compari-
son of the discussed methods using grid world simulations, in
order to see their resulting behavior also in a sequential decision-
making task.
A remarkable resemblance among both approaches is the ex-
clusive appearance of relative entropy to measure dissimilarity.
In the Active Inference literature it is often claimed that every
homeostatic system must minimize variational free energy (Fris-
ton, 2013), which is simply an extension of relative entropy for
non-normalized reference functions (cf. Section 3.2.2). In utility-
based approaches, the relative entropy (19) is typically used to
measure the amount of information processing, even though the-
oretically other cost functions would be conceivable (Gottwald
and Braun, 2019a). For a given homeostatic process, the KL
divergence measures the dissimilarity between the current dis-
tribution and the limiting distribution and therefore is reduced
while approximating the equilibrium. Similarly, in utility-based
decision-making models, relative entropy measures the dissimi-
larity between the current posterior and the prior. In the Active
Inference literature the stepwise minimization of variational free
energy that goes along with KL minimization is often equated
with the minimization of sensory surprise (see Surprise mini-
mization for a more detailed explanation), an idea that stems
from maximum likelihood algorithms, but that has been chal-
lenged as a general principle (see (Biehl et al., 2020) and the re-
18 The Two Kinds of Free Energy and the Bayesian Revolution
sponse (Friston et al., 2020)). Similarly, one could in principle
rewrite free energy from constraints in terms of informational
surprise, which would however simply be a rewording of the
probabilistic concepts in log-space. The same kind of rewording
is well-known between probabilistic inference and the minimum
description length principle (Grünwald, 2007) that also operates
in log-space, and thus reformulates the inference problem as a
surprise minimization problem without adding any new features
or properties.
6.3. Biological Relevance
So far we have seen how free energy is used as a technical
instrument to solve inference problems and its corresponding
appearance in different models of intelligent agency. Crucially,
these kinds of models can be applied to any input-output sys-
tem, be it a human that reacts to sensory stimuli, a cell that tries
to maintain homeostasis, or a particle trapped by a physical po-
tential. Given the existing literature that has widely applied the
concept of free energy to biological systems, we may ask whether
there are any specific biological implications of these models.
Considering free energy from constraints, the trade-off be-
tween utility and information processing costs provides a nor-
mative model of decision-making under resource constraints,
that extends previous optimality models based on expected util-
ity maximization and Bayesian inference. Analogous to rate-
distortion curves in information theory, optimal solutions to
decision-making problems are obtained that separate achievable
from non-achievable regions in the information-utility plane
(cf. Fig 5). The behavior of real decision-making systems under
varying information constraints can be analyzed experimentally
by comparing their performance with respect to the correspond-
ing optimality curve. One can experimentally relate abstract in-
formation processing costs measured in bits to task-dependent
resource costs like reaction or planning times (Schach et al., 2018;
Ortega and Stocker, 2016). Moreover, the free energy trade-off
can also be used to describe networks of agents, where each agent
is limited in its ability, but the system as a whole has a higher in-
formation processing capacity—for example, neurons in a brain
or humans in a group. In such systems different levels of ab-
straction arise depending on the different positions of decision-
makers in the network (Lindig-León et al., 2019; Genewein et al.,
2015; Gottwald and Braun, 2019b). As we have discussed in Sec-
tion 4.3, just like coding and rate-distortion theory, utility theory
with information costs can only provide optimality bounds but
does not specify any particular mechanism of how to achieve op-
timality. However, by including more and more constraints one
can make a model more and more mechanistic and thereby grad-
ually move from a normative to a more descriptive model, such
as models that consider the communication channel capacity of
neurons with a finite energy budget (Bhui and Gershman, 2018).
Considering variational free energy, there is a vast literature
on biological applications mostly focusing on neural processing
(e.g., predictive coding, dopamine) (Schwartenbeck et al., 2015;
Friston et al., 2017b; Parr et al., 2019), but there are also a number
of applications aiming to explain behavior (e.g., human decision-
making, hallucinations) (Parr et al., 2018). Similarly to utility-
based models, Active Inference models can be studied in terms
of as if models, so that actual behavior can be compared to pre-
dicted behavior as long as suitable prior and likelihood models
can be identified from the experiment. When applied to brain dy-
namics, the as if models are sometimes also given a mechanistic
interpretation by relating iterative update equations that appear
when minimizing variational free energy with dynamics in neu-
ronal circuits. As discussed in Section 3.2.3, the update equations
resulting for example from mean-field or Bethe approximations,
can often be written in message passing form in the sense that
the update for a given variable only has contributions that re-
quires the current approximate posterior of neighbouring nodes
in the probabilistic model. These contributions are interpreted
as local messages passed between the nodes and might be related
to brain signals (Parr et al., 2019). Other interpretations (Friston
et al., 2006, 2017a; Bogacz, 2017) obtain similar update equations
by minimizing variational free energy directly through gradient
descent, which can again be related to neural coding schemes
like predictive coding. As these coding schemes have existed ir-
respective of free energy (Rao and Ballard, 1999; Aitchison and
Lengyel, 2017), especially since minimization of prediction er-
rors is already seen in maximum likelihood estimation (Rao and
Ballard, 1999), the question remains whether there are any spe-
cific predictions of the Active Inference framework that cannot
be explained with previous models (see (Colombo and Wright,
2018; Hohwy, 2020) for recent discussions of this question).
6.4. Conclusion
Any theory about intelligent behavior has to answer three
questions: Where am I? , where do I want to go? , and how do I get
there?, corresponding to the three problems of inference and per-
ception, goals and preferences, and planning and execution. All
three problems can be addressed either in the language of prob-
abilities or utilities. Perceptual inference can either be consid-
ered as finding parameters that maximize probabilities or like-
lihood utilities. Goals and preferences can either be expressed
by utilities over outcomes or by desired distributions. The third
question can be answered by the two free energy approaches
that either determine future utilities based on model predictions,
or infer actions that lead to outcomes predicted to have high
desired probability or match the desired distribution. In stan-
dard decision-making models actions are usually determined by
a utility function that ranks different options, whereas percep-
tual inference is determined by a likelihood model that quanti-
fies how probable certain observations are. In contrast, both free
The Two Kinds of Free Energy and the Bayesian Revolution 19
energy approaches have in common that they treat all types of
information processing, from action planning to perception, as
the same formal process of minimizing some form of free en-
ergy. But the crucial difference is not whether they use utilities
or probabilities, but how predictions and goals are interwoven
into action.
This article started out by tracing back the seemingly mys-
terious connection between Helmholtz free energy from ther-
modynamics and Helmholtz’ view of model-based information
processing that led to the analysis-by-synthesis approach of per-
ception, as exemplified in predictive coding schemes, and in par-
ticular to discuss the role of free energy in current models of
intelligent behavior. The mystery starts to dissolve when we
consider the two kinds of free energies discussed in this arti-
cle, one based on the maximum entropy principle and the other
based on variational free energy—a dissimilarity measure be-
tween distributions and (generally unnormalized) functions that
extends the well-known KL divergence from information the-
ory. The Helmholtz free energy is a particular example of an en-
ergy information trade-off that results from the maximum en-
tropy principle (Jaynes, 1957). Analysis-by-synthesis is a partic-
ular application of inference to perception, where determining
model parameters and hidden states can either be seen as a re-
sult of maximum entropy under observational constraints or of
fitting parameter distributions to the model through variational
free energy minimization. Thus, both notions of free energy can
be formally related as entropy-regularized maximization of log-
probabilities.
Conceptually, however, utility-based models with informa-
tion constraints serve primarily as ultimate explanations of be-
havior, this means they do not focus on mechanism, but on
the goals of behavior and their realizability under ideal circum-
stances. They have the appeal of being relatively straightfor-
ward generalization of standard utility theory, but they rely on
abstract concepts like utility and relative entropy that may not
be so straightforwardly related to experimental settings. While
these normative models have no immediate mechanistic inter-
pretation, their relevance for mechanistic models may be anal-
ogous to the relevance of optimality bounds in Shannon’s in-
formation theory for practical codes (Shannon, 1948). In con-
trast, Active Inference models of behavior often mix ultimate
and proximate arguments of explaining behavior (Alcock, 1993;
Tinbergen, 1963), because they combine the normative aspect
of optimizing variational free energy with the mechanistic in-
terpretation of the particular form of approximate solutions to
this optimization. While mean-field approaches of Active Infer-
ence may be particularly amenable to such mechanistic interpre-
tations, they are often too simple to capture complex behavior. In
contrast, the solutions of direct Active Inference resulting from
a Bethe assumption are equivalent to previous Control as Infer-
ence approaches (Toussaint and Storkey, 2006; Todorov, 2008;
Toussaint, 2009; Ziebart, 2010; Kappen et al., 2012; Levine, 2018;
O’Donoghue et al., 2020) that allow for Bayesian message passing
formulations whose biological implementability can be debated
irrespective of the existence of a free energy functional.
Finally, both kinds of free energy formulations of intelligent
agency are so general and flexible in their ingredients that it
might be more appropriate to consider them languages or tools
to phrase and describe behavior rather than theories that explain
behavior, in a sense similar to how statistics and probability the-
ory are not biological or physical theories but simply provide a
language in which we can phrase our biological and physical as-
sumptions.
Funding
This study was funded by the European Research Coun-
cil (ERC-StG-2015-ERC Starting Grant, Project ID: 678082,
“BRISC: Bounded Rationality in Sensorimotor Coordination”).
References
Aitchison, L. and Lengyel, M. (2017). With or without you: predictive coding and
bayesian inference in the brain. Current Opinion in Neurobiology , 46:219–227.
Computational Neuroscience.
Alcock, J. (1993). Animal behavior: an evolutionary approach . Sinauer Associates.
Ashby, W. (1960). Design for a Brain: The Origin of Adaptive Behavior . Springer
Netherlands.
Beal, M. J. (2003). Variational Algorithms for Approximate Bayesian Inference . PhD
thesis, University of Cambridge, UK.
Bernoulli, J. (1713). Ars conjectandi. Basel, Thurneysen Brothers.
Bhui, R. and Gershman, S. J. (2018). Decision by sampling implements efficient
coding of psychoeconomic functions. Psychological Review, 125(6):985–1001.
Biehl, M., Pollock, F. A., and Kanai, R. (2020). A technical critique of the
free energy principle as presented in "life as we know it" and related works.
arXiv:2001.06408.
Binz, M., Gershman, S. J., Schulz, E., and Endres, D. (2020). Heuristics from
bounded meta-learned inference.
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling per-
ception and learning. Journal of Mathematical Psychology , 76:198–211. Model-
based Cognitive Neuroscience.
Boutilier, C., Dean, T., and Hanks, S. (1999). Decision-theoretic planning: Struc-
tural assumptions and computational leverage. J. Artif. Int. Res. , 11(1):1–94.
Cisek, P. (1999). Beyond the computer metaphor: behaviour as interaction. Jour-
nal of Consciousness Studies , 6(11-12):125–142.
Clark, A. (2013). Whatever next? predictive brains, situated agents, and the future
of cognitive science. Behavioral and Brain Sciences , 36(3):181–204.
Colombo, M. and Wright, C. (2018). First principles in the life sciences: the free-
energy principle, organicism, and mechanism. Synthese.
20 The Two Kinds of Free Energy and the Bayesian Revolution
Corcoran, A. W. and Hohwy, J. (2018). Allostasis, interoception, and the free energy
principle: Feeling our way forward . Oxford University Press.
Csiszár, I. (2008). Axiomatic characterizations of information measures. Entropy,
10(3):261–273.
Csiszár, I. and Tusnády, G. (1984). Information geometry and alternating mini-
mization procedures. Statistics and Decisions, Supplement Issue , 1:205–237.
Dayan, P. and Hinton, G. E. (1997). Using expectation-maximization for rein-
forcement learning. Neural Computation, 9(2):271–278.
Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The helmholtz
machine. Neural Comput., 7(5):889–904.
de Laplace, P. S. (1812). Théorie analytique des probabilités . Ve. Courcier, Paris.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the em algorithm. Journal of the Royal Statistical Society.
Series B (Methodological) , 39(1):1–38.
Doya, K. (2007). Bayesian Brain: Probabilistic Approaches to Neural Coding . MIT
Press, Cambridge, Mass.
Ergin, H. and Sarver, T. (2010). A unique costly contemplation representation.
Econometrica, 78(4):1285–1339.
Feynman, R., Hey, A., and Allen, R. (1996). Feynman Lectures on Computation .
Advanced book program. Addison-Wesley.
Flanagan, J. R., Vetter, P., Johansson, R. S., and Wolpert, D. M. (2003). Prediction
precedes control in motor learning. Current Biology , 13(2):146–150.
Fox, R., Pakman, A., and Tishby, N. (2016). Taming the noise in reinforcement
learning via soft updates. In Proceedings of the Thirty-Second Conference on Un-
certainty in Artificial Intelligence , UAI’16, pages 202–211, Arlington, Virginia,
United States. AUAI Press.
Friston, K. (2013). Life as we know it. Journal of The Royal Society Interface ,
10(86):20130475.
Friston, K. (2018). Does predictive coding have a future? Nature Neuroscience,
21(8):1019–1021.
Friston, K., Costa, L. D., and Parr, T. (2020). Some interesting observations on
the free energy principle. arXiv:2002.04501.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, J., and Pez-
zulo, G. (2016). Active inference and learning. Neuroscience & Biobehavioral
Reviews, 68:862–879.
Friston, K., Schwartenbeck, P., Fitzgerald, T., Moutoussis, M., Behrens, T., and
Dolan, R. (2013). The anatomy of choice: active inference and agency. Fron-
tiers in Human Neuroscience , 7:598.
Friston, K. J. (2005). A theory of cortical responses. Philosophical Transactions of
the Royal Society B: Biological Sciences , 360(1456):815–836.
Friston, K. J. (2010). The free-energy principle: a unified brain theory? Nature
Reviews Neuroscience, 11:127–138.
Friston, K. J., FitzGerald, T. H. B., Rigoli, F., Schwartenbeck, P., and Pezzulo, G.
(2017a). Active inference: A process theory. Neural Computation, 29:1–49.
Friston, K. J., Kilner, J., and Harrison, L. M. (2006). A free energy principle for
the brain. Journal of Physiology-Paris , 100:70–87.
Friston, K. J., Parr, T., and de Vries, B. (2017b). The graphical brain: Belief prop-
agation and active inference. Network Neuroscience, 1(4):381–414.
Friston, K. J., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., and Pezzulo, G.
(2015). Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–
214.
Friston, K. J., Shiner, T., FitzGerald, T., Galea, J. M., Adams, R., Brown, H., Dolan,
R. J., Moran, R., Stephan, K. E., and Bestmann, S. (2012). Dopamine, affor-
dance and active inference. PLoS Computational Biology , 8(1):e1002327.
Garner, W. R. (1962). Uncertainty and structure as psychological concepts . Wiley.
Genewein, T., Leibfried, F., Grau-Moya, J., and Braun, D. A. (2015). Bounded
rationality, abstraction, and hierarchical decision-making: An information-
theoretic optimality principle. Frontiers in Robotics and AI , 2.
Gershman, S. J. (2019). What does the free energy principle tell us about the brain.
Neurons, Behavior, Data Analysis, and Theory .
Gershman, S. J. and Daw, N. D. (2012). Perception, action and utility: The tangled
skein. In Principles of Brain Dynamics . MIT Press.
Gigerenzer, G. and Selten, R. (2001). Bounded Rationality: The Adaptive Toolbox .
MIT Press: Cambridge, MA, USA.
Gottwald, S. and Braun, D. A. (2019a). Bounded rational decision-making from
elementary computations that reduce uncertainty. Entropy, 21(4).
Gottwald, S. and Braun, D. A. (2019b). Systems of bounded rational agents with
information-theoretic constraints. Neural Computation, 31(2):440–476.
Grau-Moya, J., Leibfried, F., Genewein, T., and Braun, D. A. (2016). Planning
with information-processing constraints and model uncertainty in markov
decision processes. In Machine Learning and Knowledge Discovery in Databases ,
pages 475–491. Springer International Publishing.
Grünwald, P. (2007). The Minimum Description Length Principle . MIT Press, Cam-
bridge, Mass.
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning
with deep energy-based policies. In ICML.
Hansen, L. P. and Sargent, T. J. (2008). Robustness. Princeton University Press.
Harsha, P., Jain, R., McAllester, D., and Radhakrishnan, J. (2010). The commu-
nication complexity of correlation. IEEE Transactions on Information Theory ,
56(1):438–449.
Hathaway, R. J. (1986). Another interpretation of the em algorithm for mixture
distributions. Statistics & Probability Letters , 4(2):53–56.
Heskes, T. (2003). Stable fixed points of loopy belief propagation are local min-
ima of the bethe free energy. In Becker, S., Thrun, S., and Obermayer, K.,
editors, Advances in Neural Information Processing Systems 15 , pages 359–366.
MIT Press.
Hinton, G. E. and van Camp, D. (1993). Keeping the neural networks simple by
minimizing the description length of the weights. In Proceedings of the Sixth
Annual Conference on Computational Learning Theory , COLT ’93, pages 5–13,
New York, NY, USA. ACM.
Ho, M. K., Abel, D., Cohen, J. D., Littman, M. L., and Griffiths, T. L. (2020). The
efficiency of human cognition reflects planned information processing. Pro-
ceedings of the 34th AAAI Conference on Artificial Intelligence .
Hohwy, J. (2020). Self-supervision, normativity and the free energy principle.
Synthese.
Jaynes, E. T. (1957). Information theory and statistical mechanics. Phys. Rev. ,
106:620–630.
The Two Kinds of Free Energy and the Bayesian Revolution 21
Jaynes, E. T. (2003). Probability Theory. Cambridge University Press.
Jeffrey, R. C. (1965). The Logic of Decision . University of Chicago Press, 1 edition.
Kahneman, D. (2002). Maps of bounded rationality: A perspective on intuitive
judgement. In Frangsmyr, T., editor, Nobel prizes, presentations, biographies, &
lectures, pages 416–499. Almqvist & Wiksell, Stockholm, Sweden.
Kappen, H. J., Gómez, V., and Opper, M. (2012). Optimal control as a graphical
model inference problem. Machine Learning, 87(2):159–182.
Kawato, M. (1999). Internal models for motor control and trajectory planning.
Current Opinion in Neurobiology , 9(6):718–727.
Kikuchi, R. (1951). A theory of cooperative phenomena. Physical Review ,
81(6):988–1003.
Koller, D. (2009). Probabilistic graphical models : principles and techniques . The MIT
Press, Cambridge, Massachusetts.
Levine, S. (2018). Reinforcement learning and control as probabilistic inference:
Tutorial and review. arXiv:1805.00909.
Lindig-León, C., Gottwald, S., and Braun, D. A. (2019). Analyzing abstraction
and hierarchical decision-making in absolute identification by information-
theoretic bounded rationality. Frontiers in Neuroscience , 13:1230.
Linson, A., Parr, T., and Friston, K. J. (2020). Active inference, stres-
sors, and psychological trauma: A neuroethological model of (mal)adaptive
explore-exploit dynamics in ecological context. Behavioural Brain Research ,
380:112421.
Maccheroni, F., Marinacci, M., and Rustichini, A. (2006). Ambiguity aversion,
robustness, and the variational representation of preferences. Econometrica,
74(6):1447–1498.
MacKay, D. J. C. (2002). Information Theory, Inference & Learning Algorithms . Cam-
bridge University Press, USA.
MacRae, A. W. (1970). Channel capacity in absolute judgment tasks: An artifact
of information bias? Psychological Bulletin, 73(2):112–121.
Marshall, A. W., Olkin, I., and Arnold, B. C. (2011). Inequalities: Theory of Ma-
jorization and Its Applications . Springer New York, 2nd edition.
Mattsson, L.-G. and Weibull, J. W. (2002). Probabilistic choice and procedurally
bounded rationality. Games and Economic Behavior , 41(1):61–78.
McFadden, D. L. (2005). Revealed stochastic preference: a synthesis. Economic
Theory, 26(2):245–264.
McKelvey, R. D. and Palfrey, T. R. (1995). Quantal response equilibria for normal
form games. Games and Economic Behavior , 10(1):6–38.
Miller, G. A. (1956). The magical number seven, plus or minus two: some limits
on our capacity for processing information. Psychological Review, 63(2):81–97.
Minka, T. (2005). Divergence measures and message passing. Technical Report
MSR-TR-2005-173, Microsoft.
Minka, T. P. (2001). Expectation propagation for approximate bayesian inference.
In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence , UAI
’01, pages 362–369, San Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mirza, M. B., Adams, R. A., Mathys, C., and Friston, K. J. (2018). Human vi-
sual exploration reduces uncertainty about the sensed world. PLOS ONE ,
13(1):e0190429.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D.,
and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement
learning. In Balcan, M. F. and Weinberger, K. Q., editors, Proceedings of The
33rd International Conference on Machine Learning , volume 48 of Proceedings
of Machine Learning Research , pages 1928–1937, New York, New York, USA.
PMLR.
Neal, R. M. and Hinton, G. E. (1998). A view of the em algorithm that justifies
incremental, sparse, and other variants. In Jordan, M. I., editor, Learning in
Graphical Models, pages 355–368. Springer Netherlands, Dordrecht.
O’Donoghue, B., Osband, I., and Ionescu, C. (2020). Making sense of rein-
forcement learning and probabilistic inference. In International Conference
on Learning Representations , ICLR ’20.
Opper, M. and Saad, D. (2001). Comparing the Mean Field Method and Belief Prop-
agation for Approximate Inference in MRFs , pages 229–239.
Ortega, P. A. and Braun, D. A. (2013). Thermodynamics as a theory of decision-
making with information-processing costs. Proceedings of the Royal Society A:
Mathematical, Physical and Engineering Sciences , 469(2153):20120683.
Ortega, P. A. and Braun, D. A. (2014). Generalized thompson sampling for se-
quential decision-making and causal inference. Complex Adaptive Systems
Modeling, 2(1):2.
Ortega, P. A. and Stocker, A. (2016). Human decision-making under limited time.
In 30th Conference on Neural Information Processing Systems .
Parr, T., Benrimoh, D. A., Vincent, P., and Friston, K. J. (2018). Precision and false
perceptual inference. Frontiers in Integrative Neuroscience , 12:39.
Parr, T. and Friston, K. J. (2017). Working memory, attention, and salience in
active inference. Scientific reports, 7(1):14678–14678.
Parr, T. and Friston, K. J. (2019). Generalised free energy and active inference.
Biological Cybernetics.
Parr, T., Markovic, D., Kiebel, S. J., and Friston, K. J. (2019). Neuronal message
passing using mean-field, bethe, and marginal approximations. Scientific Re-
ports, 9(1):1889.
Pearl, J. (1988). Belief updating by network propagation. In Pearl, J., editor, Prob-
abilistic Reasoning in Intelligent Systems , pages 143–237. Morgan Kaufmann,
San Francisco (CA).
Poincaré, H. (1912). Calcul des probabilités . Gauthier-Villars, Paris.
Powers, W. T. (1973). Behavior: The Control of Perception . Aldine, Chicago, IL.
Rao, R. P. N. and Ballard, D. H. (1999). Predictive coding in the visual cortex: a
functional interpretation of some extra-classical receptive-field effects. Na-
ture Neuroscience, 2(1):79–87.
Rosenkrantz, R. D. (1983). E.T. Jaynes: Papers on Probability, Statistics and Statistical
Physics. Springer Netherlands, Dordrecht.
Russell, S. J. and Subramanian, D. (1995). Provably bounded-optimal agents. Jour-
nal of Artificial Intelligence Research , 2(1):575–609.
Sales, A. C., Friston, K. J., Jones, M. W., Pickering, A. E., and Moran, R. J. (2019).
Locus coeruleus tracking of prediction errors optimises cognitive flexibility:
An active inference model. PLOS Computational Biology , 15(1):e1006267.
Saul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in in-
tractable networks. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E.,
editors, Advances in Neural Information Processing Systems 8 , pages 486–492.
MIT Press.
22 The Two Kinds of Free Energy and the Bayesian Revolution
Schach, S., Gottwald, S., and Braun, D. A. (2018). Quantifying motor task perfor-
mance by bounded rational decision theory. Frontiers in Neuroscience , 12:932.
Schwartenbeck, P., FitzGerald, T. H. B., Mathys, C., Dolan, R., and Friston, K.
(2015). The dopaminergic midbrain encodes the expected certainty about
desired outcomes. Cerebral cortex (New York, N.Y. : 1991) , 25(10):3434–3445.
Schwartenbeck, P. and Friston, K. (2016). Computational phenotyping in psychi-
atry: A worked example. eNeuro, 3(4):ENEURO.0049–16.2016.
Schwöbel, S., Kiebel, S., and Marković, D. (2018). Active inference, belief propa-
gation, and the bethe approximation. Neural Computation, 30(9):2530–2567.
Shannon, C. E. (1948). A mathematical theory of communication. The Bell System
Technical Journal, 27:379–656.
Simon, H. A. (1955). A behavioral model of rational choice. The Quarterly Journal
of Economics, 69(1):99–118.
Sims, C. A. (2003). Implications of rational inattention. Journal of Monetary Eco-
nomics, 50(3):665–690. Swiss National Bank/Study Center Gerzensee Con-
ference on Monetary Policy under Incomplete Information.
Sims, C. R. (2016). Rate–distortion theory and human perception. Cognition,
152:181–198.
Still, S. (2009). Information-theoretic approach to interactive learning. Euro-
physics Letters, 85(2):28005.
Tatikonda, S. and Mitter, S. (2004). Control under communication constraints.
IEEE Transactions on Automatic Control , 49(7):1056–1068.
Tenenbaum, J. B. and Griffiths, T. L. (2001). Generalization, similarity, and
bayesian inference. Behavioral and Brain Sciences , 24(4):629–640.
Tinbergen, N. (1963). On aims and methods of ethology. Zeitschrift für Tierpsy-
chologie, 20:410–433.
Tishby, N. and Polani, D. (2011). Information theory of decisions and actions. In
Cutsuridis, V., Hussain, A., and Taylor, J. G., editors, Perception-Action Cycle:
Models, Architectures, and Hardware , pages 601–636. Springer New York.
Todorov, E. (2008). General duality between optimal control and estimation. In
2008 47th IEEE Conference on Decision and Control . IEEE.
Todorov, E. (2009). Efficient computation of optimal actions. Proceedings of the
National Academy of Sciences , 106(28):11478–11483.
Toussaint, M. (2009). Robot trajectory optimization using approximate infer-
ence. In Proceedings of the 26th Annual International Conference on Machine
Learning - ICML '09. ACM Press.
Toussaint, M. and Storkey, A. (2006). Probabilistic inference for solving discrete
and continuous state markov decision processes. In Proceedings of the 23rd
International Conference on Machine Learning , ICML ’06, pages 945–952, New
York, NY, USA. Association for Computing Machinery.
von Neumann, J. and Morgenstern, O. (1944). Theory of Games and Economic
Behavior. Princeton University Press, Princeton, NJ, USA.
Wainwright, M., Jaakkola, T., and Willsky, A. (2005). Map estimation via agree-
ment on (hyper)trees: Message-passing and linear-programming approaches.
IEEE Transactions on Information Theory , 51(11):3697–3717.
Whittle, P. (1990). Risk-sensitive optimal control . Wiley, Chichester New York.
Wiener, N. (1948). Cybernetics: Or Control and Communication in the Animal and
the Machine. John Wiley.
Williams, P. M. (1980). Bayesian conditionalisation and the principle of minimum
information. The British Journal for the Philosophy of Science , 31(2):131–144.
Williams, R. J. and Peng, J. (1991). Function optimization using connectionist
reinforcement learning algorithms. Connection Science, 3(3):241–268.
Winn, J. and Bishop, C. M. (2005). Variational message passing. J. Mach. Learn.
Res., 6:661–694.
Wolpert, D. H. (2006). Information Theory – The Bridge Connecting Bounded Ratio-
nal Game Theory and Statistical Physics , pages 262–290. Springer Berlin Hei-
delberg.
Wolpert, D. H. (2019). The stochastic thermodynamics of computation. Journal
of Physics A: Mathematical and Theoretical , 52(19):193001.
Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2001). Generalized belief propaga-
tion. In Leen, T. K., Dietterich, T. G., and Tresp, V., editors, Advances in Neural
Information Processing Systems 13 , pages 689–695. MIT Press.
Yedidia, J. S., Freeman, W. T., and Weiss, Y. (2005). Constructing free-energy
approximations and generalized belief propagation algorithms. IEEE Trans-
actions on Information Theory , 51(7):2282–2312.
Yuille, A. and Kersten, D. (2006). Vision as bayesian inference: analysis by syn-
thesis? Trends in Cognitive Sciences, 10(7):301–308. Special issue: Probabilistic
models of cognition.
Yuille, A. L. (2002). Cccp algorithms to minimize the bethe and kikuchi free
energies: Convergent alternatives to belief propagation. Neural Computation,
14(7):1691–1722.
Ziebart, B. D. (2010). Modeling Purposeful Adaptive Behavior with the Principle of
Maximum Causal Entropy . PhD thesis, Carnegie Mellon Unversity.
A. Appendices
A.1. Derivation of exemplary update equations
A.1.1. Q-value Active Inference
In the simple example of Section 5.2 under the partial mean-
field assumption (23), and in the case when the desired dis-
tribution pdes is combined with the generative model p0 via
the value function Q as shown in Equation (24), i.e. if φ ∝
p0(x,X′,S,A) eQ(A), then the full free energy F(q∥φ) can be
written as
F(q∥φ) = F(q(S|A)q(A)∥p0(x|S)p0(S|A)p0(A)eQ(A))
=
⟨
FS(A) −Q(A)
⟩
q(A) + DKL(q(A)∥p0(A)) (29)
where, FS(A) −Q(A) is given by
⟨
log q(S) q(S′|A) ∑
s′p0(X′|s′)q(s′|A)
p0(x|S)p0(S) p0(S′|S,A) pdes(X′)p0(X′|S′)
⟩
(30)
where the expectation is with respect to q(X′,S|A). Thus, op-
timizing (29) over q(A), while keeping q(S|A) fixed, results in a
Boltzmann distribution with prior p0(A) and energy FS(A) −
The Two Kinds of Free Energy and the Bayesian Revolution 23
Q(A). When optimizing F(q∥φ) with respect to q(S) while
keeping q(S′|A) and q(A) fixed, we have
q∗(S) = argmax
q(S)
F(q∥φ) = argmax
q(S)
⟨FS(A)⟩q(A)
= argmax
q(S)
⟨
log q(S)
p0(x|S)p0(S)e⟨T⟩q(S′|A)q(A)
⟩
q(S)
  
F
(
q(S)
p0(x|S)p0(S)e⟨T⟩
{
, (31)
where T:= log p0(S′|S,A) is shorthand for the log-transition
probability. Hence, from (31) we can read off the solution q∗(S)
in virtue of the general optimum (14) of variational free energy.
While here it was enough to optimize ⟨FS⟩q, because in contains
the only dependencies of F(q∥φ) on q(S), this is not the case
for q(S′|A), since also Qdepends on q(S′|A). Thus, when op-
timizing (29) over q(S′|A) while keeping q(A) and q(S) fixed,
one has to optimize ⟨FS −Q⟩which does not take the form of
a free energy in q(S′|A) due to the functional dependency of
q(X′|A) = ∑
s′p0(X′|s′)q(s′|A) on q(S′|A) that appears in
(30). However, this type of dependency is largely ignored in the
Active Inference literature (as for example noted in the appendix
of (Friston et al., 2015)), since the optimization with respect to
q(S′|A) would not have a closed-form solution otherwise.
Once this term is ignored, then the objective for q(S′|A) takes
a very simple form,
q∗(S′|A) = argmax
q(S′|A)
F(q∥φ)
≈argmax
q(S′|A)
⟨
log q(S′|A)
e⟨T⟩q(S)
⟩
q(S′|A)
, (32)
from which we can again read off the resulting update equation.
In total, from (29),(31), and (32) we obtain the set of equations
q∗(S) = 1
Zp0(x|S)p0(S)e⟨T⟩q(S′|A)q(A) (33a)
q∗(S′|A) ≈ 1
Z(A) e⟨T⟩q(S) (33b)
q∗(A) = 1
Zp0(A)e−FS(A)+Q(A), (33c)
where Zdenotes the respective normalization constants and
T= log p0(S′|S,A).
It is important to note, however, that update equations in Ac-
tive Inference resulting from a mean-field assumption (even if it
is a partial mean-field assumption such as (23)) should be taken
with care, since—as is demonstrated in the grid world simula-
tions in S2 Notebook—even in very simple situations the result-
ing agents fail to correctly plan actions that lead to desired states.
A.1.2. Direct Active Inference (variational Control as Inference)—
mean-ﬁeld assumption
Here, we derive the update equations resulting from the mini-
mization of the variational free energy for the reference defined
in Equation (25a), i.e. a variational formulation of Control as in-
ference (Toussaint and Storkey, 2006), under the mean-field as-
sumption (23). We start by writing the variational free energy
F(q∥φ) in a form analogous to (29), where now φis given by
p0(X′|S′)p0(x,S|A)p0(A),
F(q∥φ) =
⣨
F(q(S|A)∥p0(x,S|A))  
=FS(A)
−G(A)
⟩
q(A)
+ DKL(q(A)∥p0(A)) ,
where
G(A) :=
⣨
⟨log pdes(X′)⟩p0(X′|S′)
  
=:g(S′)
⟩
q(S′|A)
.
Note that, compared to Q-value Active Inference, here we do not
have to make any additional approximations, because Gonly de-
pends linearly on q(S′|A).
Similarly to above, when optimizing with respect to q(A)
while keeping q(S) and q(S′|A) fixed, we obtain that q∗(A) is
a Boltzmann distribution with energy FS −Gand prior p0(A).
Optimizing q(S) while keeping q(A) and q(S′|A) constant has
the same result as shown in (33a) because as before the only de-
pendencies on q(S) are in FS. Finally, in order to read off the so-
lution of the optimization with respect to q(S′|A) while keeping
q(S) and q(A) constant, we can rewrite FS −Gas follows
q∗(S′|A) = argmax
q(S′|A)
F(q∥φ) = argmax
q(S′|A)
(
FS(A) −G(A)
{
= argmax
q(S′|A)
⟨
log q(S′|A)
e⟨T⟩q(S)+g(S′)
⟩
q(S′|A)
so that in total we obtain the set of equations
q∗(S) = 1
Zp0(x|S)p0(S)e⟨T⟩q(S′|A)q(A) (34a)
q∗(S′|A) = 1
Z(A) e⟨T⟩q(S)+g(S′) (34b)
q∗(A) = 1
Zp0(A)e−FS(A)+G(A), (34c)
where Zdenotes the respective normalization constants, and
again T = log p0(S′|S,A).
It is noteworthy that recently another free energy approach
similar to Active Inference has been introduced that does not
make use of variational free energy, but of a different functional
termed generalized free energy (Parr and Friston, 2019). Despite of
the different functional form, this version uses a reference func-
tion that is similar to the direct Active Inference approach, where
24 The Two Kinds of Free Energy and the Bayesian Revolution
the desired distribution is also multiplied directy to the gener-
ative model but with a renormalization that results in a modi-
fied generative model over observations, states, and actions. Us-
ing this renormalized reference in a variational free energy ap-
proach would result in trivial inference reproducing the fixed
prior p0(A), corresponding to Bayes’ conditioning the modified
generative model on the past analogous to perceptual Bayesian
inference, e.g., p(A|X) = p0(A) in the case of the one-step ex-
ample. In contrast, the minimization of the free energy func-
tional used in (Parr and Friston, 2019) does not correspond to a
Bayesian inference process, which is why we do not further dis-
cuss it here.
A.1.3. Direct Active Inference (variational Control as Inference)—
Bethe assumption
Here, we derive the update equations resulting from the min-
imization of the variational free energy for the reference (25a)
under a Bethe approximation, which therefore is a more pre-
cise variational formulation of Control as Inference as the mean-
field approximation of the previous section. In fact, it turns out
that such equations are equivalent to Belief propagation (Yedidia
et al., 2001), a well-known inference method that produces exact
marginals in tree-like graphs (Pearl, 1988), such as the probabilis-
tic models considered in the article and in the Active Inference
literature.
Analogous to the previous section, without any specific re-
strictions on qwe can write the total free energy for the one-step
example from Section 5.2 with the reference (25a) as
F(q∥φ) =
⟨
log q(X′,S,S ′|A)
p0(R= 1,X = x,X′,S,S ′|A)
⟩
q  
=: ⟨F(A)⟩q(A)
+ DKL(q(A)∥p0(A))
from which it immediately follows that minimizing with respect
to q(A), while considering q(X′,S,S ′|A) constant, results in
a Boltzmann distribution with energy F(A) and prior p0(A).
F(A) is the variational free energy of q(X′,S,S ′|A) with re-
spect to the reference p0(R= 1,X = x,X′,S,S ′|A) given by
p0(x|S)p0(S)  
=:f1(S)
p0(S′|S,A)  
=:f2(S,S′)
p0(X′|S′)  
=:f3(S′,X′)
pdes(X′) 
=:f4(X′)
.
Thus, minimizing F(A) with respect to q(X′,S,S ′|A) without
any restrictions or simplifications results in the exact Bayes’ pos-
terior p(X′,S,S ′|A,R = 1,X = x)
1
Z(A) f1(S) f2(S,S′) f3(S′,X′) f4(X′) ,
where Z(A) denotes the corresponding normalization< con-
stant. The problem that we want to solve is to find an approxima-
tion to this Bayes’ posterior that is more precise than the mean-
field approximation of the previous section but requires less in-
volved computations than the determination of Z(A). While
one attempt is to partition the full graph into smaller graphs
and apply a naive mean-field approximation inside of each sub-
graph, known as a structured mean-field approximation (Saul and
Jordan, 1996), the Bethe approximation follows a slightly dif-
ferent approach. It is the simplest version of the cluster varia-
tion methods often attributed to Kikuchi (Kikuchi, 1951), a fam-
ily of region-based free energy approximations (Yedidia et al.,
2005), where one keeps beliefs over different sections of the fac-
tor graph. Specifically, in the Bethe assumption, the regions con-
sist of each factor and its neighbouring nodes, which can also be
seen as allowing pair-wise interactions. Following the system-
atic treatment in (Yedidia et al., 2005), the Bethe approximation
for our example consists of seven belief functions, one for each
factor, b1,...,b4, and one for each variable, bS, bS′, and bX′,
q(S,S′,X′|A) = b1(S)b2(S,S′)b3(S′,X′)b4(X′)
bS(S)bS′(S′)bX′(X′) (35)
where the marginals of the factor beliefs are required to be con-
sistent with the single-variable beliefs. Thus, the variational free
energy F(A) can be written as
F(A) =
4∑
k=1
⟨
log bk
fk
⟩
bk
−
∑
Y∈{S,S′,X′}
⟨log bY⟩bY
which has to be minimized under the consistency and normal-
ization contraints, leading to the Lagrangian
F(A) +
∑
s
λ1(s)
(
bS(s) −b1(s)
{
+
∑
s
λ2S(s)
(
bS(s) −
∑
s′
b2(s,s′)
(
+
∑
s′
λ2S′(s′)
(
bS′(s′) −
∑
s
b2(s,s′)
(
+
∑
s′
λ3S′(s′)
(
bS′(s′) −
∑
x′
b3(s′,x′)
(
+
∑
x′
λ3X′(x′)
(
bX′(x′) −
∑
s′
b3(s′,x′)
(
+
∑
x′
λ4(x′)
(
bX′(x′) −b4(x′)
{
+
4∑
k=1
γk
(∑
bk −1
{
+
∑
Y∈{S,S′,X′}
γY
(∑
bY −1
{
The Two Kinds of Free Energy and the Bayesian Revolution 25
where the Lagrange multipliers for the consistency constraints
are denoted by λand the Lagrange multipliers for the normal-
ization constraints by γ. The equations for the beliefs at the sta-
tionary points (zeroes of the derivatives of the Lagrangian) are
b1(s) ∝ f1(s) eλ1(s) ,
b2(s,s′) ∝ f2(s,s′) eλ2S(s) eλ2S′(s′) ,
b3(s′,x′) ∝ f3(s′,x′) eλ3S′(s′) eλ3X′(x′) ,
b4(x′) ∝ f4(x′) eλ4(x′) ,
bS(s) ∝ eλ1(s) eλ2S(s) ,
bS′(s′) ∝ eλ2S′(s′) eλ3S′(s′) ,
bX′(x′) ∝ eλ3X′(x′) eλ4(x′) ,
where the proportionality sign ∝ means that the left-hand
side results from normalizing the right hand-side to obtain a
probability distribution. By writing ml := eλl for all l ∈
{1,2S,2S′,3S′,3X′,4}, we obtain from the stationarity con-
ditions and the consistency constraints
m2S(s) ∝ f1(s) (36a)
m1(s) ∝
∑
s′f2(s,s′) m2S′(s′) (36b)
m3S′(s′) ∝
∑
s
f2(s,s′) m2S(s) (36c)
m2S′(s′) ∝
∑
x′f3(s′,x′) m3X′(x′) (36d)
m4(x′) ∝
∑
s′f3(s′,x′) m3S′(s′) (36e)
m3X′(x′) ∝ f4(x′) . (36f)
The update equations for the beliefs in (35) can be obtained by
iterating the equations in (36) and using the stationarity condi-
tions that express the beliefs in terms of the ml. Note that the
quantities denoted by ml are usually interpreted as local mes-
sages that are sent between the nodes and factors of the underly-
ing graphical model (Yedidia et al., 2005), e.g., m3S′is considered
a message sent from node S′to factor 3, which can be used to de-
termine the message m4 from factor 3 to node X′by weighing
with f3 and summing over S′, etc. By this identification, vari-
ational inference under the Bethe approximation is equivalent
to belief propagation. While in (36) there is at most one message
that is multiplied to the factor fkbefore the sum is taken, in more
complex factor graphs, where more than 2 nodes are connected
to a factor, the messages coming in to a factor from the neigh-
boring nodes are multiplied before they are summed to calculate
the outgoing message, which is why this type of message-passing
is also known as the sum-product algorithm.
A.2. Details on the example in Fig 8
Here, we are giving additional details on Figure 8 in the
article. We consider the simple example of three possible
observations, x1,x2,x3, a desired distribution pdes(X′) =
(1/3,1/6,1/2), two actions a with predictive distributions
p(X′|A= 1) = (1,0,0) and p(X′|A= 2) = (0,1/2,1/2), and
a constant prior p0(A) = (1 /2,1/2). We can consider
p(X′|A) as a result of marginalizing the generative model
p0(X′,S′,A) = p0(X′|S′)p0(S′|A)p0(A) with state distribu-
tions p(S′|A= 1) = (1,0,0) and p(S′|A= 2) = (0,1/2,1/2)
and an emission probability p0(X′|S′) that is chosen such that
the given p(X′|A) equals p(X′|A) = ∑
s′p0(X′|s′)p0(s′|A).
Suitable emission probabilities have for example the form
p0(X′= xi|S′= sj) = Mij(t), where
M(t) =


1 0 0
0 t 1−t
0 1 −t t
(
(
for all t ∈ [0,1]. Note that the resulting average entropies
⟨H(p0(S′|A= 2))⟩p0(S′|A=2) are in the range [0,1] bit for A=
2 (always zero for A= 1), where the extreme values are assumed
at t∈{0,1}(0 bit) and t= 1/2 (1 bit).
Furthermore, for the application of Active Inference in Fig-
ure 8, we have considered an exact version of the value func-
tion, Q= Qexact, where the trial distribution q(S′|A) is re-
placed by the exact predictive distribution p0(S′|A). In this
“exact” interpretation, the corresponding action distributions
p(A) ∝p0(A)eQexact(A) could then be viewed as defining the
ideal behaviour that is approximated by the variational free en-
ergy minimization. In the Active Inference literature, p(A) ∝
p0(A)eQ(A) is considered a “prior”, because it is viewed as part of
the generative model and thus is part the input to the variational
inference process. However, by considering Qan approximation
of Qexact these distributions can be viewed as defining the ideal
behavior that is approximated by the trial distributions during
free energy minimization and are therefore more in line with the
“posteriors” in other decision-making models (even though the
value function Q(A)—and therefore p(A)—is presupposed, in
constrast to being the result of some principle).
A.3. Surprise minimization
The ( informational) surprise or surprisal of a given element x
with respect to a probability distribution p0(X) is defined as
S0 := −log p0(x), i.e. it is simply a strictly decreasing function
of probability such that outcomes xwith low probability have
high surprise and outcomes x with high probability have low
surprise. A common statement found in the literature (Parr and
Friston, 2017) is that variational free energy is an upper bound
on surprise and thus minimizing free energy also minimizes sur-
prise. This idea originates from the special case of greedy infer-
26 The Two Kinds of Free Energy and the Bayesian Revolution
ence with latent variables, where, for fixed data x, the goal is to
maximize the likelihood pθ(x) = ∑
zpθ(x,z) with respect to a
parameter θ. If the marginalization over the latent variable Zis
too hard to carry out directly, then one might take advantage of
the bound
F(q(Z)∥pθ(x,Z)) ≥− log pθ(x) =: Sθ, (37)
i.e. that the variational free energy of q(Z) is an upper bound
on the surprise Sθ, which might therefore be reduced by mini-
mizing its upper bound with respect to θas a proxy. In the varia-
tional Bayes’ approach to the above inference problem, where θ
is treated as a random variable Θ , minimization with respect to
θis replaced by the minimization with respect to q(Θ) . In this
case, the analogous bound to (37) is
F(q(Z|Θ) q(Θ) ∥p0(x,Z, Θ)) ≥− log
∑
z
e⟨log p0(x,z,Θ) ⟩q(Θ) ,
where the right-hand side is the minimum of the left-hand side
with respect to q(Z|Θ) . In this sense, variational free energy is
generally not a bound on the surprise SΘ anymore, but on a log-
sum-exp version of it instead. Nonetheless, also in this Bayesian
approach, variational free energy is an upper bound on the sur-
prise S0,
F(q(Z|θ)q(θ)∥p0(x,Z, Θ)) ≥− log p0(x) = S0, (38)
where the right-hand side is the minimum of the left-hand side
with respect to both q(Z|θ) and q(θ). However, in contrast to
(37), there is no variable left in S0 over which one could mini-
mize. Therefore, saying that minimizing free energy also mini-
mizes surprise (Parr and Friston, 2017), is generally only true in
the sense that minimizing free energy minimizes an upper bound
on surprise, however surprise itself is not minimized. Instead,
the important fact about (38) is that equality is achieved by the
Bayes’ posteriorsq(Z|Θ) = p0(Z|Θ ,x) and q(Θ) = p0(Θ |x)
as discussed in Section 3.2.1.
A.4. Separation of model and state variables
In Q-value Active Inference, action and perception do not op-
timize the same variational free energy but two different free en-
ergy expressions. This is motivated from the separation of model
variables Mand state variables in standard variational Bayesian
inference, where the full free energy can be split up into a sum
of a state free energy FM averaged over models M and a KL
term that is independent of state distributions. Optimizing the
full free energy can then be done separately by alternatingly do-
ing perceptual inference by optimizing FM for each model M
and optimizing the full free energy to find the model distribution
q(M). In Active Inference, where actions Amight be thought
of analogous to models M in Bayesian inference, the full free
energy is analogously split up into a sum of a state free energy
FA averaged over actions Aand a KL term which—in contrast
to standard Bayesian inference— does depend on state distribu-
tions. However, Active Inference essentially ignores this extra q-
dependency by following the analogous optimization scheme to
Bayesian inference: one alternatingly optimizes FAwith respect
to state distributions and then the full free energy with respect to
the action distributions q(A). In particular, this separation into
state and action free energies is not a consequence of optimizing
the full variational free energy, but a deliberate choice made by
Active Inference.
In the following, we discuss in more detail how this separation
follows from optimizing the full free energy in standard Bayesian
inference and highlight how Q-value Active Inference adopts the
same optimization scheme but by giving up the optimization of
a single variational free energy.
A.4.1. Bayesian inference
Consider the case of multiple probabilistic models pm(X,Z)
that are indexed by a label m, where each pm describes a differ-
ent probabilistic relationship between data Xand hidden states
Z. Given data X = x, one could find the best mby select-
ing the model with the largest marginal likelihood pm(x) =∑
zpm(x,z). A popular method to accomplish this is the ba-
sic EM algorithm Dempster et al. (1977), where mis optimized
greedily while Zis inferred using Bayesian inference for a given
m(either exact or approximate). In a purely Bayesian treatment,
one also assumes a prior distribution over models p0(M), so that
the full joint over data X, hidden states Z, and models M be-
comes
p0(X,Z,M ) := pM(X,Z)p0(M) =: p0(X,Z|M) p0(M) .
The Bayes’ posterior p(M|X) can then simply be determined
from the Bayes’ posterior p(Z,M|X) through marginalization
over Z. As discussed in the article (Section 3.2), if direct Bayesian
inference is infeasable then a variational formulation might be
useful, where trial distributions q(Z,M) over the unknown
variables M and Z are fitted to the reference φ(Z,M) :=
p0(x,Z,M ) by minimizing the variational free energy
F(q∥φ) =
⟨
log q(Z,M)
p0(x,Z,M )
⟩
q(Z,M)
.
By writing qand p0 in their factorized forms
q(Z,M) = q(Z|M)q(M),
p0(X,Z,M ) = p0(X,Z|M)p0(M) ,
The Two Kinds of Free Energy and the Bayesian Revolution 27
the variational free energy can be decomposed as
F(q∥φ) =
⟨
log q(Z|M)q(M)
p0(x,Z|M)p0(M)
⟩
q
=
⣨⣨
log q(Z|M)
p0(x,Z|M)
⟩
q(Z|M)
  
=:FM
⟩
q(M)
+
⣨
log q(M)
p0(M)
⟩
q(M)
= ⟨FM⟩q(M) + DKL(q(M)∥p0(M)) . (39)
Notably, the minimization of F with respect to qsplits up into
the minimization of the free energy over states
FM = F
(
q(Z|M)
p0(x,Z|M)
{
(40)
with respect to q(Z|M), and the minimization of (39) with re-
spect to q(M). In particular, the inference over models and
states, (M,Z), separates into inference over hidden states for
each model, which determines FM for each M, and inference
over M.
A.4.2. Active Inference
In Q-value Active Inference, action selection is treated sim-
ilarly to model selection in Bayesian inference discussed in the
previous section. However, the KL term in (39) also depends on
trial distributions over states, which means that a separation into
action and state variables analogous to the separation in model
selection is not possible when considering the problem of action
and perception as the minimization of a single free energy func-
tional, which is usually the conceptual starting point in the Active
Inference literature Friston (2010, 2018).
More precisely, as discussed in Section 5, the reference func-
tion φthat enters the variational free energy in Q-value Active
Inference is constructed from a given probabilistic model p0 and
a value function Qby replacing the fixed prior p0(A) over ac-
tions with the modified distribution ˜p0(A) := 1
Zp0(A)eQ(A).
As can be seen exemplarily in the one-step case in Equation (26),
the value function Qdepends on trial distributions qover hid-
den states and therefore ˜p0(A) depends on qas well. Despite this
dependency, the total free energy F(q∥φ) can still be written as
F(q∥φ) =
⟨
log q(X′,S|A)q(A)
p0(x,X′,S|A)˜p0(A)
⟩
q
= ⟨FA⟩q(A) + DKL
(
q(A)∥˜p0(A)
{
(41)
with
FA := F
(
q(X′,S|A)
p0(x,X′,S|A)
{
. (42)
in the case of the one-step example of Section 5.2. Equations (41)
and (42) are analogous to Equations (39) and (40), respectively.
However, when optimizing the full free energy F(q∥φ) with re-
spect to the factor q(X′,S|A), one would have to consider both
terms in the decomposition (41), since, unlike p0(M) in the pre-
vious section, here ˜p0(A) does depend on trial distributions over
states (the factor q(S′|A)). It should be noted that this depen-
dency is non-linear and non-local, and therefore a closed-form
solution cannot be derived (cf. (ii) in Section 5.3).
In Active Inference, this complication is avoided by simply ig-
noring the q-dependency of Qwhen deriving the update equa-
tions, or, put differently, one optimizes two different free ener-
gies for perception and action: one first optimizes FA with re-
spect to state distributions for each action Aand then one op-
timizes the full free energy (41) with respect to q(A). This is
in analogy to Bayesian model selection of the previous section,
where this separation was a consequence of the minimization of
the full free energy. However, here, due to the extra dependency
of ˜p0(A) on q, it is not a consequence but a choice made by Ac-
tive Inference. This means that one no longer does variational
inference over the combined set of states and actions, but varia-
tional inference over states with free energy FA and variational
inference over actions with free energy (41). In particular, there
is not a single free energy that is optimized by both perception
and action, but two different ones.
S. Supplementary Material
The following ancillary files are provided as supplementary
material:
S.1. Notebook: Comparison of diﬀerent formulations of Active
Inference
A detailed comparison of the different formulations of Active
Inference found in the literature (2013-2018), including their
mean-field and exact solutions in the general case of arbitrary
many time steps.
S.2. Notebook: Grid world simulations
We provide implementations of the models discussed in this
article in a grid world environment, both as a rendered html file
as well as a jupyter notebook that is available on github.