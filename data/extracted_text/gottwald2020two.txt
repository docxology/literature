The Two Kinds of Free Energy and the Bayesian Revolution
SebastianGottwald1,2andDanielA.Braun1
1InstituteofNeuralInformationProcessing,UlmUniversity,89081Ulm,Germany
2sebastian.gottwald@uni-ulm.de
Abstract
Theconceptoffreeenergyhasitsoriginsin19thcenturythermodynamics,buthasrecentlyfounditswayintothebehav-
ioralandneuralsciences,whereithasbeenpromotedforitswideapplicabilityandhasevenbeensuggestedasafundamental
principleofunderstandingintelligentbehaviorandbrainfunction.Wearguethatthereareessentiallytwodifferentnotions
offreeenergyincurrentmodelsofintelligentagency,thatcanbothbeconsideredasapplicationsofBayesianinferenceto
theproblemofactionselection: onethatappearswhentradingoffaccuracyanduncertaintybasedonageneralmaxi-
mumentropyprinciple,andonethatformulatesactionselectionintermsofminimizinganerrormeasurethatquantifies
deviationsofbeliefsandpoliciesfromgivenreferencemodels. Thefirstapproachprovidesanormativeruleforaction
selectioninthefaceofmodeluncertaintyorwheninformationprocessingcapabilitiesarelimited. Thesecondapproach
directlyaimstoformulatetheactionselectionproblemasaninferenceprobleminthecontextofBayesianbraintheories,
alsoknownasActiveInferenceintheliterature.Weelucidatethemainideasanddiscusscriticaltechnicalandconceptual
issuesrevolvingaroundthesetwonotionsoffreeenergythatbothclaimtoapplyatalllevelsofdecision-making,fromthe
high-leveldeliberationofreasoningdowntothelow-levelinformationprocessingofperception.
Keywords:freeenergy,intelligentagency,bayesianinference,maximumentropy,utilitytheory,activeinference
This is a preprint of the article The two kinds of free energy brain would essentially be a prediction and inference engine
andtheBayesianrevolution,PLOSComputationalBiology16(12), basedoninternalmodels(Kawato,1999;Flanaganetal.,2003;
2020. Doya, 2007). Coincidentally, Helmholtz also invented the no-
tionoftheHelmholtzfreeenergythatplaysanimportantrolein
thermodynamicsandstatisticalmechanics,eventhoughhenever
1. Introduction
madeaconnectionbetweenthetwoconceptsinhislifetime.
Thereisasurprisinglineofthoughtconnectingsomeofthe
ThisconnectionwasfirstmadebyDayan,Hinton,Neal,and
greatestscientistsofthelastcenturies,includingImmanuelKant,
Zemel in their computational model of perceptual processing
HermannvonHelmholtz,LudwigE.Boltzmann,andClaudeE. asastatisticalinferenceengineknownastheHelmholtzmachine
Shannon,wherebymodel-basedprocessesofaction,perception,
(Dayanetal.,1995). Inthisneuralnetworkarchitecture, there
andcommunicationareexplainedwithconceptsborrowedfrom
are feed-forward and feedback pathways, where the bottom-
statisticalphysics.InspiredbyKant’sCopernicanrevolutionand
up pathway translates inputs from the bottom layer into hid-
motivated from his own studies of the physiology of the sen-
dencausesattheupperlayer(therecognitionmodel),andtop-
sorysystem, Helmholtzwasoneofthefirstproponentsofthe
down activation translates simulated hidden causes into sim-
analysis-by-synthesisapproachtoperception(YuilleandKersten,
ulated inputs (the generative model). When considering log-
2006),wherebyaperceiverisnotsimplyconceptualizedassome
likelihood in this setup as energy in analogy to statistical me-
kind of tabula rasa recording raw external stimuli, but rather
chanics, learningbecomesarelaxationprocessthatcanbede-
reliesoninternalmodelsoftheworldtomatchandanticipate
scribedbytheminimizationofvariationalfreeenergy. Whileit
sensoryinputs.Theinternalmodelparadigmisnowubiquitous
shouldbeemphasizedthatvariationalfreeenergyisnotthesame
inthecognitiveandneuralsciencesandhasevenledsomere-
asHelmholtzfreeenergy, thetwofreeenergyconceptscanbe
searcherstoproposeaBayesianbrainhypothesis,wherebythe
1
0202
ceD
7
]CN.oib-q[
4v36711.4002:viXra
2 TheTwoKindsofFreeEnergyandtheBayesianRevolution
formallyrelated.Importantly,variationalfreeenergyminimiza-
tionisnotonlyahallmarkoftheHelmholtzmachine,butofa past future
soil quality soil quality
moregeneralfamilyofinferencealgorithms,suchasthepopu-
larexpectation-maximization(EM)algorithm(NealandHinton, S
p
0
(S′ |S,A)
S ′
1998;Beal,2003). Infact,overthelasttwodecades,variational
Bayesianmethodshavebecomeoneoftheforemostapproxima- p (S)
0 fertilization
tionschemesfortractableinferenceinthemachinelearninglit-
erature. Moreover, aplethoraofmachinelearningapproaches p(S|X=x)=? A
use loss functions that have the shape of a free energy when Bayesian inference
optimizingperformanceunderentropyregularizationinorder p (X|S) p (A) p (X′ |S′)
toboostgeneralizationoflearningmodels(WilliamsandPeng, 0 0 0
1991;Mnihetal.,2016). X X
′
Inthemeanwhile,freeenergyconceptshavealsomadetheir
way into the behavioral sciences. In the economic literature, past future
crop yields crop yields
forexample,trade-offsbetweenutilityandentropicuncertainty
measuresthattaketheformoffreeenergieshavebeenproposed Figure1. Graphicalrepresentationofanexemplaryprobabilisticmodel.
Thearrows(edges)indicatecausalrelationshipsbetweentherandomvari-
todescribedecision-makerswithstochasticchoicebehaviordue
ables(nodes). Thefulljointdistributionp0 overallrandomvariablesis
to limited resources (McKelvey and Palfrey, 1995; Sims, 2003;
sometimesalsoreferredtoasagenerativemodel,becauseitcontainsthe
Mattsson and Weibull, 2002; McFadden, 2005; Wolpert, 2006) completeknowledgeabouttherandomvariablesandtheirdependencies
orrobustdecision-makerswithlimitedprecisionintheirmod- andthereforeallowstogeneratesimulateddata. Suchamodelcouldfor
examplebeusedbyafarmertoinferthesoilqualitySbasedonthecrop
els (Maccheroni et al., 2006; Hansen and Sargent, 2008). The
yieldsX throughBayesianinference,whichallowstodetermineapriori
free energy trade-off between entropy and reward can also be unknowndistributionssuchasp(S|X)fromthegenerativemodelp0via
foundininformation-theoreticmodelsofbiologicalperception- marginalizationandconditionalization.
actionsystems(Still,2009;TishbyandPolani,2011;Ortegaand
Braun,2013),someofwhichhavebeensubjectedtoexperimen-
ity. Asthenotionoffreeenergymainlyappearsinthecontext
taltesting(OrtegaandStocker,2016;Sims,2016;Schachetal.,
ofstatisticalmodelsofcognition, thelanguageofprobabilistic
2018; Lindig-León et al., 2019; Bhui and Gershman, 2018; Ho
modelsconstitutesacommonframeworkinthefollowingdis-
etal.,2020).Finally,intheneuroscienceliteraturethenotionof
cussion.Section2thereforestartswithpreliminaryremarkson
freeenergyhasrisentorecentfameasthecentralpuzzlepiece
probabilisticmodelling.Section3introducestwonotionsoffree
intheFreeEnergyPrinciple(Friston,2010)thathasbeenused
energythataresubsequentlyexpoundedinSection4andSec-
toexplainacornucopiaofexperimentalfindingsincludingneu-
tion 5, where they are applied to models of intelligent agency.
ralpredictionerrorsignals(Salesetal.,2019),synapticplastic-
Section6concludesthepaper.
ityrules(Bogacz,2017),neuraleffectsofbiasedcompetitionand
attention(Fristonetal.,2012;ParrandFriston,2017),visualex-
plorationinhumans(Mirzaetal.,2018),andmore—seetheref- 2. Probabilisticmodelsandperception-actionsystems
erences in (Parr and Friston, 2019). Over time, the Free En-
Systems that show stochastic behavior, for example due to
ergy Principle has grown out of an application of the free en-
randomlybehavingcomponentsorbecausetheobserverignores
ergyconceptusedintheHelmholtzmachine,tointerpretcorti-
certaindegreesoffreedom,aremodelledusingprobabilitydis-
calresponsesinthecontextofpredictivecoding(Friston,2005),
tributions.Thisway,anybehavioral,environmental,andhidden
andhasgraduallydevelopedintoageneralprincipleforintelli-
gentagency,alsoknownasActiveInference(Fristonetal.,2013, variablescanberelatedbytheirstatistics,anddynamicalchanges
canbemodelledbychangesintheirdistributions.
2015;ParrandFriston,2019). Consequencesandimplications
oftheFreeEnergyPrinciplearediscussedinneighbouringfields Consider, for example, the simple probabilistic model illus-
likepsychiatry(SchwartenbeckandFriston,2016;Linsonetal., tratedinFig1,consistingofthe(forsimplicity,discrete)variables
2020) and the philosophy of mind (Clark, 2013; Colombo and pastandfuturesoilqualityS:=(S,S0),pastandfuturecropyields
Wright,2018). X := (X,X0),andfertilizationA. Thegraphicalmodelshown
in the figure corresponds to the joint probability p (X,S,A)
Giventhatthenotionoffreeenergyhasbecomesuchaper- 0
givenbythefactorization
vasiveconceptthatcutsthroughmultipledisciplines,themain
rationaleforthisdiscussionpaperistotracebackandtoclar-
p (X0|S0)p (X|S)p (S0|S,A)p (S)p (A), (1)
ifydifferentnotionsoffreeenergy,toseehowtheyarerelated 0 0 0 0 0
andwhatroletheyplayinexplainingbehaviorandneuralactiv-
where p (S) is the base probability of the past soil quality S,
0
TheTwoKindsofFreeEnergyandtheBayesianRevolution 3
p (X|S)istheprobabilityofcropyieldsX dependingonthe astoolstoarriveatdistributionsthatdescribetheagent’sbehav-
0
pastsoilqualityS,andsoforth.Giventhejointdistributionwe ior.Freeenergyisoneoftheconceptsthatappearsinbothtypes
cannowaskquestionsabouteachofthevariables.Forexample, ofmethods.
wecouldaskabouttheprobabilitydistributionp(S|X=x)of
soilqualitySifwearetoldthatthecropyieldsXareequaltoa
3. Thetwonotionsoffreeenergy
valuex.Wecanobtaintheanswerfromtheprobabilisticmodel
p 0 bydoingBayesianinference,yieldingtheBayes’posterior Vaguelyspeaking,freeenergycanrefertoanyquantitythatis
oftheform
p(S,X) p (X|S)p (S)
p(S|X)= = 0 0 , (2)
P P
s p(s,X) s p 0 (X|s)p 0 (s) freeenergy = energy ± const.×entropy, (3)
wherethedependenciesonX0,S0,andAhavebeensummedout where energy is an expected value of some quantitity of inter-
tocalculatethemarginalp(S,X).Ingeneral,Bayesianinference est,entropyreferstoaquantitymeasuringdisorder,uncertainty,
inaprobabilisticmodelmeanstodeterminetheprobabilityof orcomplexity,thatmustbespecifiedinthegivencontext,and
somequeriedunobservedvariablesgiventheknowledgeofsome const.isaconstanttermthattranslatesbetweenunitsofentropy
observedvariables.Thiscanbeviewedastransformingtheprior andenergy,andisrelatedtothetemperatureinphysicallymoti-
probabilisticmodelp 0 toaposterior modelp, underwhichthe vatedfreeenergyexpressions.Fromrelation(3),itisnotsurpris-
observedvalueshaveprobabilityoneandunobservedvariables ingthatfreeenergysometimesappearsenshroudedbymystery,
haveprobabilitiesgivenbythecorrespondingBayes’posteriors. asitreliesonanunderstandingofentropy,and“nobodyreally
knowswhatentropyisanyway”,asJohnVonNeumannfamously
In principle, Bayesian inference requires only two different
kinds of operations, namely marginalization, i.e. summing out quipped(Feynmanetal.,1996).
unobservedvariablesthathavenotbeenqueried,suchasX0,S0
Historically,theconceptoffreeenergygoesbacktotheroots
andAabove,andconditionalization,i.e.renormalizingthejoint
of thermodynamics, where it was introduced to measure the
distributionoverobservedandqueriedvariables—thatmayitself maximumamountofworkthatcanbeextractedfromathermo-
betheresultfromapreviousmarginalizationsuchasp(S,X) dynamicsystemataconstanttemperatureandvolume.If,forex-
above—toobtaintherequiredconditionaldistributionoverthe ample,allthemoleculesinaboxmovetotheleft,wecanusethis
queriedvariables.Inpractice,however,inferenceisahardcom- kineticenergytodriveaturbine. If,however,thesamekinetic
putationalproblemandmanymoreefficientinferencemethods energyisdistributedasrandommolecularmotion,itcannotbe
areavailablethatmayprovideapproximatesolutionstotheexact fullytransformedintowork.Therefore,onlypartofthetotalen-
Bayes’posteriors,includingbeliefpropagation(Pearl,1988),ex- ergyEisusable,becausetheexactpositionsandmomentaofthe
pectationpropagation(Minka,2001),variationalBayesianinfer- molecules,theso-calledmicrostates,areunknown. Inthiscase,
ence(HintonandvanCamp,1993),andMonteCarloalgorithms themaximumusablepartoftheenergyE istheHelmholtzfree
(MacKay,2002).Alsonotethatinferenceistrivialifthesought- energy,definedas
afterconditionaldistributionofthequeriedvariableisalready F =E−TS, (4)
givenbyoneoftheconditionaldistributionsthatjointlyspecify
whereSisthethermodynamicentropy.Ingeneral,thetransfor-
theprobabilisticmodel,e.g.,p(X|S)=p (X|S).
0
mationbetweentwomacrostateswithfreeenergiesF andF
1 2
Probabilisticmodelscanbeusednotonlyasexternal(observer)
allowstheextractionofworkW ≤F −F .
2 1
models,butalsoasinternalmodelsthatareemployedbytheagent
Whilethetwonotionsoffreeenergythatwediscussinthefol-
itself,orbyadesigneroftheagent,inordertodetermineade-
lowingarevaguelyinspiredbythephysicaloriginal,theirmoti-
siredcourseofaction.Inthislattercase,actionscouldeitherbe
vations are rather distinct and the main reason they share the
thoughtofasdeterministicparametersoftheprobabilisticmodel
thatinfluencethefuture(influencediagrams)orasrandomvari- nomenclature is due to their general form (3) resembling the
ables that are part of the probabilistic model themselves (prior Helmholtzfreeenergy(4).
models) (Boutilier et al., 1999). Either way, internal models al-
low making predictions over future consequences in order to 3.1. Freeenergyfromconstraints
findactionsordistributionsoveractionsthatleadtodesirable
The first notion of free energy is closely tied to the princi-
outcomes,forexampleactionsthatproducehighrewardsinthe
future. Inmechanisticorprocessmodelinterpretations,someof ple of maximum entropy (Jaynes, 1957), which virtually appears
in all branches of science. From this vantage point, the phys-
thespecificationprocedurestofindsuchactionsarethemselves
ical free energy is merely a special instance of a more general
meanttorepresentwhattheagentisactuallydoingwhilerea-
soning,whereasasif interpretationssimplyusethesemethods inferenceproblemwhereweholdprobabilisticbeliefsaboutun-
knownquantities(e.g.,theexactenergyvaluesofthemolecules
4 TheTwoKindsofFreeEnergyandtheBayesianRevolution
inagas)andwecanonlymakecoarsemeasurementsorobserva- cangetridofthefacultiesbymakinguseofStirling’sapproxi-
tions(e.g.,thetemperatureofthegas)thatwecanusetoupdate mationlnN! = NlnN −N +O(lnN). Inparticular,when
ourbeliefsaboutthesehiddenvariables.Theprincipleofmaxi- lettingN,N i → ∞suchthatp(z i ) = N N i remainsfinite, we
mumentropysuggeststhat,amongthebeliefsthatarecompati- obtain
blewiththeobservation,weshouldchoosethemost“unbiased”
n (cid:18) (cid:19)
belief,inthesensethatitcorrespondstoamaximumnumberof
1
logω =−
XN
i log
N
i +O
logN
N − → → ∞ H(p).
possibleassignmentsofthehiddenvariables. N N N N
i=1
| {z }
=H(p)
3.1.1. Wallis’motivationofthemaximumentropyprinciple
where H(p) := − P p(z)logp(z) denotes the (Gibbs or
z∈Ω
ConsidertherandomexperimentofdistributingN elements Shannon) entropy of p. Thus, instead of assessing typicality by
randomly in n equally probable buckets with N (cid:29) n, where maximizing(5)inΓ forlargebutfixedN,wecangetridofthe
ε
theresultingnumberofelementsN i inbucketi ∈ {1,...,N} N-dependencybysimplymaximizingH,
determinestheprobabilityp(z
i
) := N
N
i. Inprinciple,thisway
we could generate any distribution p over a finite set Ω = p∗ =argmax H(p). (6)
{z
1
,...,z
n
}thatwelike,however,auniformdistributionthat p,hEip=ε
reflectstheequiprobableassignmentclearlyismuchmorelikely
Thisconstrainedoptimizationproblemisknownastheprinciple
thanaDiracdistributionwherealltheprobabilitymassiscon-
ofmaximumentropy. Themotivationgivenhereisessentially
centratedinonebucket.Here,thereasonisthattherearemany
theWallisderivationpresentedbyJaynes(Jaynes,2003).
possibleassignmentsof elementsamongthebuckets thatgen-
eratetheuniformdistribution, whereasthereisonlyonefora
Diracdistribution.Infact,thenumberofpossibilitiesofhowto
3.1.2. FreeenergyfromconstraintsandtheBoltzmanndistribution
distributeN elementsamongnbucketswithN elementsinthe
i
ithbucketis Theconstrainedoptimizationproblem(6)canbetranslated
N!
ω := , (5) intoanunconstrainedproblembyintroducingaLagrangemul-
N
1
!···N
n
! tiplierβ,knownastheinversetemperatureduetotheanalogyto
becauseN!isthenumberofpossiblepermutationsofallN el- thermodynamicsandtheHelmholtzFreeEnergy(4),whichhas
ements,whichovercountsbythenumberofpermutationsofel- tobechosenposthocsuchthattheconstraintissatisfied. This
ementsinsidethesamebucketandthushastobedividedbythe resultsintheminimizationoftheLagrangian
numberofpermutationsN !foralli=1,...,n.Intheabsence
i F(p):=hEi − 1H(p), (7)
ofanyfurthermeasurementconstraints,thenumberofpossibil- p β
ities(5)ismaximizedbyN =N/nforalli,andthusthetypical
i whichtakestheformofafreeenergy(3). Asweshallseelater,
distributionp∗ overΩinthiscaseistheuniformdistribution,
F takesitsminimumattheBoltzmanndistributionknownfrom
i.e.,p∗(z )= 1 foralli.
i n statisticalmechanics,givenby
Considernowtheproblemofhavingtodetermineatypical
distributionp∗ overΩsuchthattheexpectedvalueE p∗ [E] =: p∗(z):= 1 e−βE(z), (8)
hEi ofsomequantityE equalsameasuredvalueε. Asimple Z
p∗
examplewouldbetheexperimentofthrowingN diceandtak- whereZ = P e−βE(z)denotesthenormalizationconstant.
ingE tobethenumberofdots, i.e., E(z ) = 1,...,E(z ) = z∈Ω
1 6
6, andtryingtofindthetypicaldistributionp∗ overoutcomes Notethat,theargumentintheprevioussectionimplicitlyas-
z ,...,z undertheconstraintthattheaveragenumberofdots sumesauniformreferencedistribution,becausethebucketsare
1 6
is,sayε = 2. Thesolutiontothisproblemisanalogoustothe assumedtobeequiprobable.Whenreplacingthisassumptionby
case of no constraints, but this time we only consider realiza- theassumptionofageneraldistributionp 0 overΩ,weobtainthe
tionsthatarecompatiblewiththemeasurementconstraint,that principleofminimumrelativeentropy(Rosenkrantz,1983),where
iswelet(N
1
,...,N
n
)belongtothesetofpermissibleoccupa- the so-called Kullback-Leibler (KL) divergence D
KL
(pkp
0
) =
tionvectors hlog(p/p 0 )i p isminimizedwithrespecttopsubjecttoacon-
strainthEi = ε. Analogoustothemaximumentropyprinci-
Γ ε := (cid:8) (N 1 ,...,N n ) (cid:12) (cid:12)hEi p =ε, p(z i )= N N i ∀i (cid:9) . ple,thistra p nslatestotheunconstrainedminimizationoftheLa-
grangian
A typical distribution p∗ for a constraint ε can then be deter- F(p,p ):=hEi + 1D (pkp ), (9)
minedbyacandidateinΓ withthemaximumnumberωofpos- 0 p β KL 0
ε
sibilities(5).Byassumption,N ismuchlargerthann,sothatwe withsolutiongivenbyp∗(z)= 1 p (z)e−βE(z).
Z 0
<latexit sha1_base64="Ws0O9RjId2iTxYQlauihLfXfke4=">AAAC03ichVFLS8NAEJ7GV1tfVY9egkXwVBIpqLeiVrwIFUxbbIts0m0amhfJtlCLF/Hqzav+L/0tHvyypoIW6YbNzH4z883LDF0nFpr2nlEWFpeWV7K5/Ora+sZmYWu7HgfDyOKGFbhB1DRZzF3H54ZwhMubYcSZZ7q8YQ7OEntjxKPYCfwbMQ55x2O27/QciwlAt22Pib7FXLV6VyhqJU0edVbRU6VI6akFhQ9qU5cCsmhIHnHySUB3iVGMr0U6aRQC69AEWATNkXZOD5RH7BBeHB4M6AB/G69Wivp4J5yxjLaQxcWNEKnSPu6FZDThnWTl0GPIT9x7idn/ZphI5qTCMaQJxpxkvAIuqA+PeZFe6jmtZX5k0pWgHh3LbhzUF0ok6dP64TmHJQI2kBaVqtLTBocp3yNMwIc0UEEy5SmDKjvuQjIpuWTxU0YGvggymT7qwZr1v0udVeqHJb1cOrkuFyun6cKztEt7dICtHlGFLqmGOixkeaFXelMMZaI8Kk/frkomjdmhX0d5/gL9PZKS</latexit>
TheTwoKindsofFreeEnergyandtheBayesianRevolution 5
Fromnowon,werefertoafreeenergyexpressionthatismo-
F(p) = ⟨
E
⟩p −
β
1 H(p) t
s
i
u
v
c
a
h
te
a
d
s
f
(
r
7
o
)
m
an
a
d
t
(
r
9
a
)
d
,
e
a
-
s
o
f
f
r
f
e
b
e
e
e
t
n
w
er
e
g
e
y
n
f
a
r
n
om
en
c
e
o
r
n
g
s
y
tra
a
i
n
n
d
ts
a
,
n
in
e
o
n
r
tr
d
o
e
p
r
y
to
te
d
r
i
m
s-
,
minimize minimize maximize
criminateitfromthenotionoffreeenergyintroducedinthefol-
lowingsection,which—despiteofitsresemblance—hasadiffer-
entmotivation.
p*
p =δ z* p =unif
3.2. Variationalfreeenergy
Thereisanother,distinctappearanceoftheterm“freeenergy”
z* z* outsideofphysics,whichisapriorinotmotivatedfromatrade-
trade-off concentrated maximal uncertainty
offbetweenanenergyandentropyterm,butfrompossibleef-
(controlled by β ) (on the option (uniform distribution)
z ⇤ = a r g m i n E ) ficiencygainswhenrepresentingBayes’ruleintermsofanop-
Figure2.Minimizingthefreeenergyfromconstraints(7)requirestotrade
timizationproblem.Thistechniqueismainlyusedinvariational
offthecompetingtermsofenergyhEipandentropyH(p),hereshownex- Bayesianinference(Koller,2009,Ch.11),originallyintroducedby
emplarilyforthecaseofthreeelements. Assumingthereexistsaunique HintonandvanCamp(HintonandvanCamp,1993).Asbefore,
minimalelementz∗ =argmin
z
E(z),thenminimizingonlyhEipoverall
forsimplicityallrandomvariablesarediscrete,butmostexpres-
probabilitydistributionspresultsinthe(Diracdelta)distributionδz∗that
assignszeroprobabilitytoallzi 6=z∗andprobabilityonetozi=z∗,and sionscandirectlybetranslatedtothecontinuouscasebyreplac-
thereforehaszeroentropy.Incontrast,minimizingonlytheterm−H(p)/β ingprobabilitydistributionsbyprobabilitydensitiesandsums
isequivalenttomaximizingH(p)andthereforewouldresultintheuniform
bythecorrespondingintegrals.
distributionthatgivesequalprobabilitytoallelements.TheresultingBoltz-
manndistributionp∗interpolatesbetweenthesetwoextremesolutionsof
minimalenergy(β→∞)andmaximumentropy(β→0).
3.2.1. VariationalBayesianinference
AswehaveseeninSection2,Bayesianinferenceconsistsin
3.1.3. Thetrade-offbetweenenergyanduncertainty thecalculationofaconditionalprobabilitydistributionoverun-
knownvariablesgiventhevaluesofknownvariables.Inthemost
Animportantfeatureoftheminimizationofthefreeenergies
simplecaseoftwovariables, sayX andZ, andaprobabilistic
(7)and(9)consistsinthebalancingofthetwocompetingterms
modeloftheformp (X,Z)=p (X|Z)p (Z),Bayesianinfer-
ofenergyandentropy(cf. Fig2). Thistrade-off betweenmax- 0 0 0
enceappliesifXisobservedandZisqueried.Analogousto(2),
imaluncertainty(uniformdistribution,orp 0 )ontheonehand theexactBayes’posteriorp(Z|X=x)isdefinedbytherenor-
andminimalenergy(e.g.adeltadistribution)ontheotherhand
malizationofp (x,Z)inordertoobtainadistributionoverZ
0
isthecoreofthemaximumentropyprinciple.Theinversetem-
thatrespectsthenewinformationX=x,
peratureβ playstheroleofatrade-offparameterthatcontrols
howthesetwocounteractingforcesareweighted. p (x,Z) p (x|Z)p (Z)
p(Z|X=x) = 0 = 0 0 , (10)
Themaximumentropyprinciplegoesbacktotheprincipleof Z(x) Z(x)
insufficient reason (Bernoulli, 1713; de Laplace, 1812; Poincaré, P
withthenormalizationconstantZ(x)= p (x,z).
1912),whichstatesthattwoeventsshouldbeassignedthesame z 0
probabilityifthereisnoreasontothinkotherwise. Ithasbeen
InvariationalBayesianinference,however,thisBayes’poste-
hailedasaprincipledmethodtodeterminepriordistributions riorisnotcalculateddirectlybyrenormalizingp 0 (x,Z)withre-
and to incorporate novel information into existing probabilis- specttoZ,butindirectlybyapproximatingitbyadistribution
ticknowledge. Infact,Bayesianinferencecanbecastinterms q(Z)thatisadjustedthroughtheminimizationofanerrormea-
ofrelativeentropyminimizationwithconstraintsgivenbythe surethatquantifiesthedeviationfromtheexactBayes’posterior.
availableinformation(Williams,1980).Applicationsofthisidea Importantly,thevalueofthiserrormeasurecanbedetermined
canalsobefoundinthemachinelearningliterature,wheresub- withouthavingtoknowtheexactBayes’posterior. Toseethis,
tracting(oradding)anentropytermfromanexpectedvalueofa notethattheKLdivergencebetweenq(Z)andp(Z|X=x)can
functionthatmustbeoptimizedisknownasentropyregulariza- bewrittenas
tionandplaysanimportantroleinmodernreinforcementlearn-
(cid:28) (cid:29) (cid:28) (cid:29)
q(Z) q(Z)
ingalgorithms(WilliamsandPeng,1991;Mnihetal.,2016)to log = logZ(x)+ log ,
p(Z|X=x) p (x,Z)
encourageexploration(Haarnojaetal.,2017)aswellastopenal- q(Z) | {z } 0 q(Z)
| {z } indep.ofq | {z }
izeoverlydeterministicpoliciesresultinginbiasedrewardesti-
=DKL(q(Z)kp(Z|X=x)) =:F(q(Z)kp0(x,Z))
mates(Foxetal.,2016). (11)
6 TheTwoKindsofFreeEnergyandtheBayesianRevolution
1.0
sp
d a is c t e ri b o u f t p io ro
n
b
s
ability
ygrene
eerf
F
ϕ
(q∥
u
ϕ
n
)
no
q
rm
p
a
ϕ
a
u
l
x
iz
il
e
ia
d
r
q
y
r e
tr
f
i
e
a
v
r
l
a
m
e
d
r
n
i
i
i
a
n
s
c
t
t
i
i
e
m
o
r i
n
f
b
i
u e
z
u
o
n
e
n
t
f
c
i
e
f
o
q
t
r
r i
n
o
e
g
t
s
n
o
e
y
q ϕ=(ϕ,ϕ ) p ϕ normalized version ofϕ
1 2
0.5 normalization
p ϕ
0.5 1.0
A 1st vector component
tnenopmoc
rotcev dn2
p
ϕ
m in fr im ee i z e a n t e io r
n
gy
normalization
q
area = 1
ϕ
area < 1
z
B
ytisned
ytilibaborp free energy minimization
Figure3.Thenormalizationofafunctonφtoobtainaprobabilitydistributionp φisequivalenttofittingtrialdistributionsqtotheshapeofφbyminimizing
freeenergy. Intwodimensions,thenormalizationofapointφ = (φ1,φ2)correspondstoa(non-orthogonal)projectionontotheplaneofprobability
vectors(A).Forcontinuousdomains,whereprobabilitydistributionsarerepresentedbydensities,normalizationcorrespondstoarescalingofφsuchthat
theareabelowthegraphequals1(B).Instead,whenminimizingvariationalfreeenergy(redcolour),thetrialdistributionsqarevarieduntiltheyfittothe
shapeoftheunnormalizedfunctionφ(perfectlyatq=p φ).
i.e.,itcanbedecomposedintothesumofaconstanttermanda bilitydistributions(cf.Fig3).Ifthisoptimizationprocesshasno
termthatdoesnotdependonthenormalizationZ(x). Inpar- constraints, thenthetrialdistributionsareadjusteduntilp is
φ
ticular,agoodapproximationq(Z)oftheexactBayes’posterior achieved.Inthecaseofconstraints,forinstanceifthetrialdistri-
(10)willeffectivelyminimizethisKLdivergence,which—dueto butionsareparametrizedbyanon-exhaustiveparametrization
(11)—canbedonebyminimizingF(q(Z)kp (x,Z)).Inpartic- (e.g., Gaussians), thentheoptimizedtrialdistributionsapprox-
0
ular,theoptimiumofthisminimizationisexactlyachievedatthe imatep ascloseaspossiblewithinthisparametrization. The
φ
Bayes’posterior(10), minimalvalueofF(qkφ)is
(cid:28) q(Z) (cid:29) X
F(p kφ) = minF(qkφ) = −log φ(z). (15)
argmin log = p(Z|X=x) , (12) φ
p (x,Z) q
q(Z) 0 q(Z) z
P
whichisknownasthevariationalcharacterizationofBayes’rule. Inparticular,thisimpliesthat−F(qkφ) ≤ log z φ(z)forall
Thisresultisaspecialcaseof(14)inthefollowingsection. q,sothatvarying−F(qkφ)witharbitrarytrialdistributionsq
alwaysprovidesalowerboundtotheunknownnormalization
P
constant φ(z). InBayesianinferencethisisthenormaliza-
3.2.2. Variationalfreeenergy,anextensionofrelativeentropy z
tionconstantinBayes’ruleandcalledthemodelevidence,which
Anynon-negativefunctionφonafinitespaceΩ,canbenor- iswhythenegativevariationalfreeenergyisalsocalledevidence
P
malizedtoobtainaprobabilitydistributionp φ = φ/ z φ(z) lowerbound(ELBO).
onΩthatdiffersfromφonlybyascalingconstant.Incaseswhen
itisnotbeneficialtocarryoutthesum P φ(z)explicitly,such Theproofof(14)and(15)directlyfollowsfromJensen’sin-
z
a normalization might be replaced by the minimization of the equalityandonlyreliesontheconcavityofthelogarithm.Aswe
variationalfreeenergy haveseenintheprevioussection,invariationalBayesianinfer-
ence,thereferenceφusuallytakestheformofajointdistribution
(cid:28) (cid:29)
q(Z) evaluated at the observed variables, e.g., φ(Z) = p (x,Z) in
F(qkφ):= log , (13) 0
φ(Z) whichcase(14)recovers(12). Thevariationalfreeenergy(13)
q(Z)
isafreeenergyinthesenseof(3)sincebytheadditivityofthe
withrespecttotheso-calledtrialdistributionsq,becausewehave logarithmundermultiplication(logab=loga+logb),
φ(Z) F(qkφ)=h−logφi −H(q) (16)
argminF(qkφ) = = p (Z). (14) q
P φ(z) φ
q z
withenergytermh−logφi andentropytermH(q).Notethat,
q
Thus,insteadofnormalizingφdirectly,onefitsauxiliarydistri- forthechoiceφ=e−βE,Equation(14)becomestheBoltzmann
butionsq toapproximatetheshapeofφinthespaceofproba-
TheTwoKindsofFreeEnergyandtheBayesianRevolution 7
hidden
variable
Z
p(Z) p 0 rior p(x,Z) naive p(Z|X=x)= 0 Bayesian ∑ z p 0(x,z) inference p 0 (X,Z) q(Z) variational
pro m ba o b d i e lis l tic =argminq⟨log p 0(x,Z) ⟩ q B in a fe y r e e s n ia c n e p(X|Z) <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJyyyynnnn9999llll++++1111vvvvPPPPNNNNzzzzSSSScccc6666sssseeeeaaaaFFFFzzzzWWWWXXXXGGGG4444bbbb////pppp0000===="""">>>>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==<<<<////llllaaaatttteeeexxxxiiiitttt>>>> 0 variational free energy likelihood | {z }
X
observation
| {z
}
<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJyyyynnnn9999llll++++1111vvvvPPPPNNNNzzzzSSSScccc6666sssseeeeaaaaFFFFzzzzWWWWXXXXGGGG4444bbbb////pppp0000===="""">>>>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==<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
approximate inference
q θ′( Z)
m (e i . n g. θ b F y ( g q r θ a ( d Z ie ) n ∥ t p d 0 e ( s x c , e Z n ) t) ) p 0(Z|x) q θ(Z) z
z 2 iterative inference algorithms p 0(Z 1,Z 2 |x) F(q(Z)∥p 0(x,Z)) 1 m q(Z i 1 n ) F(q(Z 1)q(Z 2 |Z 1)∥p 0(x,Z 1,Z 2)) 1
2 q( m Z 2 i | n Z 1) F(q(Z 1)q(Z 2 |Z 1)∥p 0(x,Z 1,Z 2)) 1 2
z
1
)ytisned(
ytilibaborp
q θ′′ ( Z)
searc r h e d s u p c a t c io e n
optim st iz e a p t w io i n se q(Z 1,Z 2) q′′ ( Z 1,Z 2)
(e.g. EM, belief propagation, …)
q′( Z 1,Z 2)
Figure4.InvariationalBayesianinference,theoperationofrenormalizingtheprobabilisticmodelp0evaluatedatanobservationX=x(Bayes’rule),is
replacedbyanoptimizationproblem.Inpractice,thisvariationalrepresentationisoftenexploitedtosimplifyagiveninferenceproblem,eitherbyreducing
theseachspaceofdistributions,forexamplethrougharestrictiveparametrizationresultinginapproximateinference,orbysplittinguptheoptimization
intomultiplepartialoptimizationstepsthatarepotentiallyeasiertosolvethantheoriginalproblembutmightstillconvergetotheexactsolution.These
twosimplificationscanalsobecombined,forexampleinthecaseofmean-fieldassumptionswherethespaceofdistributionsisreducedandanefficient
iterativeinferencealgorithmisobtainedatthesametime.
distribution(8)andthevariationalfreeenergy(16)formallycor- cansimplifytheinferenceprocess(cf. Fig4). First,itallowsto
respondstothefreeenergyfromconstraints(7). approximateexactBayes’posteriorsbyrestrictingtheoptimiza-
tionspace,forexampleusinganon-exhaustiveparametrization,
Variationalfreeenergycanberegardedasanextensionofrel-
e.g.anexponentialfamily. Second, itenablesiterative inference
ativeentropywiththereferencedistributionbeingreplacedby
algorithmsconsistingofmultiplesimpleroptimizationsteps,for
anon-normalizedreferencefunction,sinceinthecasewhenφ
P examplebyoptimizingwithrespecttoeachterminafactorized
isalreadynormalized,thatisif φ(z) = 1,thenthefreeen-
z representationofqseparately.Apopularchoiceisthemean-field
ergy(13)coincideswiththeKLdivergenceD (qkφ). Inpar-
KL approximation,whichcombinesbothofthesesimplifications,as
ticular,whilerelativeentropyisameasureforthedissimilarity
itassumesindependencebetweenhiddenstates, effectivelyre-
oftwoprobabilitydistributions,wheretheminimumisachieved
ducing the search space from joint distributions to factorized
ifbothdistributionsareequal,variationalfreeenergyisamea-
ones, and moreover it allows to optimize with respect to each
sure for the dissimilarity between a probability distribution q
factor alternatingly. Note, however, that mean-field approxi-
and a (generally non-normalized) function φ, where the mini-
mationshavelimiteduseinsequentialenvironments,wherein-
mumwithrespecttoq isachievedatp . Accordingly, wecan
φ
dependenceofsubsequentstatescannotbeassumedandthere-
thinkofthevariationalfreeenergyasaspecificerrormeasure
forelessrestrictiveassumptionsmustbeusedinstead(Opperand
between probability distributions and reference functions. In
Saad,2001).
principle,onecoulddesignmanyothererrormeasuresthathave
the same minimum. This means that, a statement in a proba- Many efficient iterative algorithms for exact and approxi-
bilisticsettingthatadistributionq∗minimizesavariationalfree mate inference can be viewed as examples of variational free
energyF(qkφ)withrespecttoagivenreferenceφ,isanalogous energy minimization, for example the EM algorithm (Demp-
toastatementinanon-probabilisticsettingthatsomenumber ster et al., 1977; Neal and Hinton, 1998), belief propagation
x = x∗ minimizesthevalueofanerrormeasure(cid:15)(x,y)(e.g., (Pearl,1988;Yedidiaetal.,2001),andothermessagepassingalgo-
thesquarederror(cid:15)(x,y) = (x−y)2)withrespecttoagiven rithms(Minka,2001;Wainwrightetal.,2005;WinnandBishop,
referencevaluey. 2005;Minka,2005;Yedidiaetal.,2005).Whilethe(Bayesian)EM
algorithm (Beal, 2003) and Pearl’s belief propagation (Yedidia
3.2.3. Approximateanditerativeinference et al., 2001) both can be seen as minimizing the same varia-
tionalfreeenergy,justwithdifferentassumptionsontheapprox-
Representing Bayes’ rule as an optimization problem over
imate posteriors, in (Minka, 2005), it is shown that also many
auxiliary distributions q has two main applications that both
8 TheTwoKindsofFreeEnergyandtheBayesianRevolution
othermessagepassingalgorithmssuchas(Minka,2001;Wain-
wrightetal.,2005;WinnandBishop,2005)canbecastasmin-
imizingsometypeoffreeenergy,theonlydifferencebeingthe
choiceofthedivergencemeasureastheentropyterm. Simple
versionsofthesealgorithmshaveoftenexistedbeforetheirfree
energy formulations were available, but the variational repre-
sentationsusuallyallowedforextensionsandrefinements—see
(CsiszárandTusnády,1984;Hathaway,1986;NealandHinton,
1998;Beal,2003)incaseofEMand(Yedidiaetal.,2001;Heskes, non-admissible
2003;Yuille,2002;Yedidiaetal.,2005)incaseofmessagepassing.
non-optimal
Wearenowturningtothequestionofhowthetwonotionsof
bounded-optimal
freeenergyintroducedinthissectionarerelatedtorecenttheo-
riesofintelligentagency. uncertainty costC(p,p 0)
4. Freeenergyfromconstraintsininformationprocess-
ing
4.1. Thebasicidea
The concept of free energy from constraints as a trade-
off between energy and uncertainty can be used in models of
perception-action systems, where entropy quantifies informa-
tion processing complexity required for decision-making (e.g.,
planningapathforfleeingapredator)andenergycorresponds
toperformance(e.g.,distinguishingbetterandworseflightdirec-
tions). Thenotionofdecisioninthiscontextisverybroadand
canbeappliedtoanyinternalvariableintheperception-action
pipeline(Kahneman,2002),thatisnotgivendirectlybytheenvi-
ronment.Inparticular,italsosubsumesperceptionitself,where
thedecisionvariablesaregivenbythehiddencausesthatarebe-
inginferredfromobservations.
In rational choice theory (von Neumann and Morgenstern,
1944),adecision-makerselectsdecisionsx∗fromasetofoptions
ΩsuchthatautilityfunctionU definedonΩismaximized,
x∗ =argmax U(x). (17)
x∈Ω
The utility values U(x) could either be objective, for example
amonetarygain,orsubjectiveinwhichcasetheyrepresentthe
decision-maker’s preferences. In general, the utility does not
havetobedefineddirectlyonΩ,butcouldbederivedfromutility
valuesthatareattachedtocertainstates,forexampletothecon-
figurationsoftheplayboardinaboardgame.Inthecaseofper-
ception,utilityvaluesareusuallygivenby(log-)likelihoodfunc-
tions, in which case utility maximization without constraints
correspondstogreedyinferencesuchasmaximumlikelihoodes-
timation. Notethat, forsimplicity, inthissectionweconsider
one-stepdecisionproblems. Sequentialtaskscaneitherbeseen
asmultipleone-stepproblemswheretheutilityofagivenstep
mightdependonthepolicyoverfuturesteps,oraspathplanning
problemswhereanactionrepresentsafullactionpathorpolicy
p⟩U⟨ytilitu
detcepxe
no cost costly more costly very costly
maximal uncertainty less uncertainty little uncertainty no uncertainty
x*
Ω
A
B
Figure5.A:Decision-makingcanbeconsideredasasearchprocessinthe
spaceofoptionsΩ,whereoptionsareprogressivelyruledout. Delibera-
tioncostsaredefinedtobemonotonefunctionsundersuchuncertaintyre-
duction.B:Exemplaryefficiencycurveresultingfromthetrade-offbetween
utilityandcosts,thatseparatesnon-optimalfromnon-admissiblebehavior.
Thepointsonthecurvecorrespondtobounded-optimalagentsthatopti-
mallytradeoffutilityagainstuncertainty,analogoustotherate-distortion
curveininformationtheory.
(Whittle,1990;TishbyandPolani,2011;Grau-Moyaetal.,2016;
GottwaldandBraun,2019b).
While ideal rational decision-makers are assumed to per-
fectlyoptimizeagivenutilityfunctionU,realbehaviorisoften
stochastic, meaningthatmultipleexposurestothesameprob-
lemleadtodifferentdecisions. Suchnon-deterministicbehav-
iorcouldbeaconsequenceofmodeluncertainty,asinBayesian
inference or various stochastic gambling schemes, or a conse-
quence of satisficing (Simon, 1955), where decision-makers do
not choose the single best option, but simply one option that
is good enough. Abstractly, this means that, the choice of a
singledecisionisreplacedbythechoiceof adistributionover
decisions. More generally, also considering prior information
thatthedecision-makermighthavefrompreviousexperience,
theprocessofdeliberationduringdecision-makingmightbeex-
pressedasthetransformationofapriorp toaposteriordistri-
0
butionp.
When assuming that deliberation has a cost C(p,p ), then
0
arrivingatnarrowposteriordistributionsshouldintuitivelybe
morecostlythanchoosingdistributionsthatcontainmoreun-
certainty(cf.Fig5A).Inotherwords,deliberationcostsmustbe
increasingwiththeamountofuncertaintythatisreducedbythe
transformationfromp top. Uncertaintyreductioncanbeun-
0
derstoodasmakingtheprobabilitiesofoptionslessequaltoeach
other,rigorouslyexpressedbythemathematicalconceptofma-
jorization(Marshalletal.,2011).Thisnotionofuncertaintycan
alsobegeneralizedtoincludepriorinformation,sothatthede-
greeofuncertaintyreductioncorrespondstomoreorlessdevi-
TheTwoKindsofFreeEnergyandtheBayesianRevolution 9
ationsfromtheprior(GottwaldandBraun,2019a).
MaximizingexpectedutilityhUi withrespecttopunderre-
S S′ 
p
strictions on processing costs C(p,p ) is a constrained opti- A
0
mizationproblemthatcanbeinterpretedasaparticularmodel
X X′ 
ofboundedrationality(Simon,1955),explainingnon-rationalbe-
haviorofdecision-makersthatmaybeunabletoselectthesingle prior model posterior model utility function
best option by their limited information processing capability. p (X,S,A) p(X,S,A) U(X′) 
0
Similarly to the free energy trade-off between energy and en-
tropy(cf. Fig2),thisresultsinatrade-offbetweenutilityhUi
p
andprocessingcostsC(p,p ), 0
F (p):=hUi − 1C(p,p ). (18)
β p β 0
Here, the trade-off parameter β is analogous to the in-
verse temperature in statistical mechanics (cf. Equation (7)) and
parametrizes the optimal trade-offs p∗ = argmax F (p) be-
β p β
tween utility and cost, that define an efficiency frontier sep-
arating the space of perception-action systems into bounded-
optimal,non-optimal,andnon-admissiblesystems(cf.Fig5).
Whenassumingthatthetotaltransformationcostisthesame
independentofwhetheradecisionproblemissolvedinonestep
ormultiplesub-steps(additivityundercoarse-graining)thetrade-
offin(18)takesthegeneralform(3)ofafreeenergyinthesense
of energy (utility) minus entropy (cost), because then the cost
functionisuniquelygivenbytherelativeentropy
C(p,p )=D (pkp ). (19)
0 KL 0
Note that the additivity of (19) also implies a coarse-graining
property of the free energy (18) in the case when the decision
issplitintomultiplesteps,suchthattheutilityofprecedingde-
cisions is effectively given by the free energy of following de-
cisions. Therefore, in this case, free energy can be seen as a
certainty-equivalentvalueofthesubordinatedecisionproblems,
i.e.theamountofutilitytheagentwouldhavetoreceivetobe
indifferentbetweenthisguaranteedutilityandthepotentialex-
pected utility of the subsequent decision steps taking account
the associated information processing costs. The special case
(19) has been studied extensively in multiple contexts, includ-
ingquantalresponseequilibriainthegame-theoreticliterature
(McKelveyandPalfrey,1995;Wolpert,2006), rationalinatten-
tion and costly contemplation (Sims, 2003; Ergin and Sarver,
2010),boundedrationalitywithKLcosts(MattssonandWeibull,
2002;OrtegaandBraun,2013),KLcontrol(Todorov,2009;Kap-
pen et al., 2012), entropy regularization (Williams and Peng,
1991; Mnih et al., 2016), robustness (Maccheroni et al., 2006;
Hansen and Sargent, 2008), the emergence of heuristics (Binz
etal.,2020), thermodynamicmodelsofcomputation(Wolpert,
2019),andtheanalysisofinformationflowinperception-action
systems(TishbyandPolani,2011;Still,2009). While(19)isof-
tenregardedasanabstractmeasureofuncertaintyreductionor
agenericproxyforinformationprocessingcosts,itcanalsobe
stneidergni
max
{
⟨U⟩p−
β
1D
KL
(p∥p
0
)
}
p
epicer
S S′ 
A
X′ 
X X′ 
Trading off utility and uncertainty by maximizing free energy
Figure6.Overviewofhowtoapplyutilitymaximizationwithinformation
processingcoststotheexamplefromSection2.
viewedasaphysicalcapacityconstraint,wheretheinformation
thatisrequiredtoachieveacertainexpectedutilityisconsidered
tobesentoverachanneltotheactuator(Miller,1956;Garner,
1962;MacRae,1970;TatikondaandMitter,2004;BhuiandGer-
shman,2018). Thisviewisalsoconsistentwiththemaximum
entropyprinciple,as(18)and(19)favordistributionspthatcan
begeneratedfromp mosteasilyintermsofstatistics,andthere-
0
forewithminimumcommunicationcomplexitybetweenp and
0
p(Harshaetal.,2010).
4.2. ASimpleExample
Ingredients. ConsidertheprobabilisticmodelshowninFig1
withthejointdistributionp (X,S,A)thatisspecifiedbythe
0
factors in the decomposition (1). Here, S and X denote the
currentenvironmentalstateandthecorrespondingobservation,
andAdenotestheactionthatmustbedeterminedinorderto
drivethesystemintoanewstateS0 withobservationX0. The
decision-makingproblemisspecifiedbyassumingthatwehave
given a utility function U over future observations X0 which
thedecision-makerseekstomaximizebyselectinganactionA,
while only having access to the current observation X. This
meansthatthedecision-makerhascontroloverthedistribution
p(A|X),whichreplacesthepriorp (A)inthefactorization(1)
0
ofthepriormodelp (X,S,A)todeterminethefactorization
0
oftheposteriormodelp(X,S,A)intermsofthefixedcompo-
nentsinp (cf.Fig6)as
0
p(X,S,A)=p (X0|S0)p (X|S)p (S0|S,A)p (S) p(A|X).
0 0 0 0
| {z }
p0(X,S|A)
(20)
Freeenergyfromconstraints.Furtherassumingthatthedecision-
maker is subject to an information processing constraint
D (pkp )≤C ,forsomenon-negativeboundC ,resultsin
KL 0 0 0
theunconstrainedoptimizationproblemmax F(p)withfree
p
10 TheTwoKindsofFreeEnergyandtheBayesianRevolution
energygivenby(18),wherethetrade-offparameterβ istuned icalpointofthisframeworkisthechoiceofthecostfunctionC.
to comply with the bound C . Since the action distribution Inparticular,wecouldaskwhetherthereissomekindofuniver-
0
p(A|X)istheonlydistributionintheposteriormodel(20)that salcostfunctionthatisapplicabletoanyperception-actionpro-
changesduringdecision-making,i.e.,duringthetransformation cessorwhetherthereareonlyproblem-specificinstantiations.
frompriortoposterior,thetotalfreeenergysimplifiesto Ofcourse,havingauniversalmeasurethatallowsapplyingthe
sameconceptstoextremelydiversesystemsisbothaboonanda
bane,becausethepracticalinsightsitmayprovideforanycon-
F(p)=hUi − 1D (p(X,S,A)kp (X,S,A))
p(X,S,A) β KL 0 creteinstancecouldbeverylimited.Thisistherootofanumber
=hV(X,A)i p(A|X)p(X) − β
1(cid:10)
D KL (p(A|X)kp 0 (A))
(cid:11)
ofcriticalissues:
=hF (p(A|X))i ,
A p(X) (i) WhatisthecostC? Animportantrestrictionofalldelibera-
wherewehavewrittenp 0 (x|s)p 0 (s)=p(s|x)p(x)usingBayes tioncostsoftheformC(p,p 0 )isthattheyonlydependonthe
rule(2),and initialandfinaldistributionsandignoretheprocessofhowto
getfromp top.Whenvaryingasingleresource(e.g.,processing
0
X time)wecanuseC(p,p )asaprocess-independentproxyforthe
V(X,A):= p(s|X)p (s0|s,A)p (x0|s0)U(x0), 0
s,s0,x0 0 0 resource.However,iftherearemultipleresourcesinvolved(e.g.,
F (p(A|X)):=hV(X,A)i − 1D (p(A|X)kp (A)). processingtime,memory,andpowerconsumption),asinglecost
A p(A|X) β KL 0
cannottellushowtheseresourcesareweightedoptimallywith-
Notethat, heretheexpectationwithrespecttop(X)doesnot outmakingfurtherprocess-dependentassumptions.Ingeneral,
affecttheoptimizationwithrespecttop(A|X)sinceitcanbe thetheorymakesnosuggestionswhatsoeveraboutmechanical
performedpointwiseforeachparticularrealizationxofX. In processes that could implement resource-optimal strategies, it
fact,wewouldhaveobtainedthesameresultwhenconditioning onlyservesasabaselineforcomparison.Finally,simplyrequir-
onanarbitraryvalueX=xfromtheoutset. However,ingen- ingthemeasuretobemonotonicintheuncertaintyreduction,
eral,optimalinformationprocessingstrategiesmaydependon doesnotuniquelydeterminetheformofC,astherehavebeen
theentiredistributionp(X)andcanthereforenotbeobtained multipleproposalsofuncertaintymeasuresintheliterature(see
fromonlyconsideringsingleobservationsx,forexamplewhen e.g.(Csiszár,2008)),whererelativeentropyisjustonepossibility.
also optimizing with respect to the prior p (A), see e.g., (Ge- However,relativeentropyisdistinguishedfromallotheruncer-
0
neweinetal.,2015). taintymeasuresinitsadditivityproperty,thatforexampleallows
toexpressoptimalprobabilisticupdatesfromp topintermsof
0
Free energy maximization. The optimal action distribution additionsorsubtractionsofutilities,suchaslog-likelihoodsfor
p∗(A|X)maximizingF A isaBoltzmanndistribution(8)with evidenceaccumulationinBayesianinference.
“energy”V(X,A)andpriorp (A),
0
(ii) Whatistheutility? Whensystemsareengineered, utilities
p∗(A|X)= 1 p (A)eβV(X,A), (21) are usually assumed to be given such that desired behavior is
Z(X) 0 specified by utility maximization. However, when we observe
perception-actionsystems,itisoftennotsoclearwhattheutil-
whereZ(X):= P a p 0 (a)eβV(X,a).Notethatinordertoeval- ityshouldbe,orinfact,whetherthereevenexistsautilitythat
uatetheutilityV,itisrequiredtodeterminetheBayes’posterior capturestheobservedbehaviorintermsofutilitymaximization.
p(S|X).Thisshowshowinautility-basedapproach,theneedto Thisquestionoftheidentifiabilityofautilityfunctionisstud-
performBayesianinferenceresultsdirectlyfromtheassumption iedextensivelyintheeconomicsciences,wherethebasicideais
aboutwhichvariablesareobservedandwhicharenot. thatsystemsrevealtheirpreferencesthroughtheiractualchoices
andthatthesepreferenceshavetosatisfycertainconsistencyax-
4.3. Criticalpoints ioms in order to guarantee the existence of a utility function.
Inpractice,toguaranteeuniqueidentifiabilitytheseaxiomsare
The main idea of free energy in the context of information
usuallyratherstrong,forexampleignoringtheeffectsofhistory
processingwithlimitedresourcesisthatanycomputationcan
andcontextwhenchoosingbetweendifferentitems, orignor-
bethoughtofabstractlyasatransformationfromadistribution
ingthepossibilitythattheremightbemultipleobjectives.When
p ofpriorknowledgetoaposteriordistributionpthatencap-
0 notmakingthesestrongassumptions, utilitybecomesarather
sulatesanadvancedstateofknowledgeresultingfromdelibera-
genericconcept,liketheconceptofprobability,andadditional
tion. Theprogressthatismadethroughsuchatransformation
assumptions like soft-maximization are necessary to translate
isquantitativelycapturedbytwomeasures: theexpectedutility
fromutilitiestochoiceprobabilities.
hUi thatquantifiesthequalityofpandC(p,p )thatmeasures
p 0
thecostofuncertaintyreductionfromp top.Clearly,thecrit- (iii) The problem of infinite regress. One of the main concep-
0
TheTwoKindsofFreeEnergyandtheBayesianRevolution 11
tualissueswiththeinterpretationofC asadeliberationcostis loopsoflearningtohunt, totradeandtobuyfood. Crucially,
thattheoriginalutilityoptimizationproblemissimplyreplaced beingabletoexploittheenvironmentinordertoattainfavor-
byanotheroptimizationproblemthatmayevenbemorediffi- ablesensorystates,requiresimplicitorexplicitknowledgeofthe
culttosolve. Thisnoveloptimizationproblemmightagainre- environmentthatcouldeitherbepre-programmed(e.g., insect
quireresourcestobesolvedandcouldthereforebedescribedbya locomotion)orlearnt(e.g.,playingthepiano).
higher-leveldeliberationcost,thusleadingtoaninfiniteregress.
TheFreeEnergyPrinciplewasoriginallysuggestedasathe-
Infact,anydecision-makingmodelthatassumesthatdecision-
oryofcorticalresponses(Friston,2005)bypromotingthefree
makers reason about processing resources are affected by this
energyformulationofpredictivecodingthatwasintroducedby
problem(RussellandSubramanian,1995;GigerenzerandSelten,
Dayan and Hinton with the Helmholtz machine (Dayan et al.,
2001). Apossiblewayoutistoconsidertheutility-information
1995). It found its most recent incarnation in what is known
trade-off simply an as if description, since perception-action asActive InferencethatattemptstoextendvariationalBayesian
systems that are subject to a utility-information trade-off do
inference to the problem of action selection. Here, the target
notnecessarilyhavetoreasonorknowabouttheirdeliberation
valueofhomeostasisisexpressedthroughaprobabilitydistribu-
costs.Itisstraightforward,forexample,todesignprocessesthat
tionp underwhichdesiredsensorystateshaveahighprob-
des
probabilisticallyoptimizeagivenutilitywithnoexplicitnotion
ability. Therequiredknowledgeabouttheenvironmentisex-
offreeenergy,butforanoutsideobservertheresultingchoice
pressedthroughagenerativemodelp thatrelatesobservations,
0
distributionlookslikeanoptimalfreeenergytrade-off(Ortega
hiddencauses, andactions. Asthegenerativemodelallowsto
andBraun,2014).
make predictions about future states and observations, it en-
ables to choose actions in such a way that the predicted con-
Insummary,thefreeenergytrade-offbetweenutilityandin-
sequencesconformtothedesireddistribution. InActiveInfer-
formation primarily serves as a normative model for optimal
ence,thisisachievedbymergingthegenerativeandthedesired
probabilityassignmentsininformation-processingnodesornet-
distributions,p andp ,intoasinglereferencefunctionφto
works. Like other Bayesian approaches, it can also serve as a 0 des
whichtrialdistributionsq overtheunknownvariablesarefit-
guide for constructing and interpreting systems, although it is
tedbyminimizingthevariationalfreeenergyF(qkφ).Thisfree
ingeneralnotamechanisticmodelofbehavior. Inthatrespect
energyminimizationisanalogoustovariationalBayesianinfer-
itsharesthefateofitscousinsinthermodynamicsandcoding
ence,wherethereferenceisalwaysgivenbyajointdistribution
theory(Shannon,1948)inthattheyprovidetheoreticalbounds
evaluatedatobservedquantities(cf.Section3.2.1).Intheresult-
onoptimalitybutdevisenomechanismforprocessestoachieve
ing homeostatic process, the trial distributions q play the role
thesebounds.
of internal variables that are manipulated in order to achieve
desiredsensoryconsequencesthatarenotdirectlycontrollable.
5. VariationalfreeenergyinActiveInference Minimizingvariationalfreeenergybythealternatingvariation
oftrialdistributionsoveractionsq andtrialdistributions
5.1. Thebasicidea Actions
overhiddenstatesq ,
States
VariationalfreeenergyisthemainingredientusedintheFree
Energy Principle for biological systems in the neuroscience lit-
minF(qkφ) and minF(qkφ), (22)
erature (Friston, 2005, 2010; Friston et al., 2015, 2006), which
qActions qStates
hasbeenconsideredas“arguablythemostambitioustheoryof | {z } | {z }
Action Perception
thebrainavailabletoday”(Gershman,2019). Sincevariational
freeenergyinitselfisjustamathematicalconstructtomeasure isthenequatedwithprocessesofactionandperception.
thedissimilaritybetweendistributionsandfunctions—seeSec-
In a nutshell, the central tenet of the Free Energy Principle
tion3—,thebiologicalcontentoftheFreeEnergyPrinciplemust
statesthatorganismsmaintainhomeostasisthroughminimiza-
comefromsomewhereelse. Thebasicbiologicalphenomenon
tionofvariationalfreeenergybetweenatrialdistributionqand
thattheFreeEnergyPrinciplepurportstoexplainishomeosta-
areferencefunctionφbyactingandperceiving. Sometimesthe
sis,theabilitytoactivelymaintaincertainrelevantvariables(e.g.,
evenstrongerstatementismadethatminimizingvariationalfree
bloodsugar)withinapreferredrange. Usually, homeostasisis
energyismandatoryforhomeostaticsystems(Friston,2013;Cor-
applied as an explanatory principle in physiology whereby the
coranandHohwy,2018).
actualvalueofavariableiscomparedtoatargetvalueandcor-
rectionstodeviationerrorsaremadethroughafeedbackloop.
However, homeostasis has also been proposed as an explana- 5.2. ASimpleExample
toryprincipleforcomplexbehaviorinthecyberneticliterature Ingredients. ApplyingtheActiveInferencerecipe(cf.Fig7)to
(Wiener,1948;Ashby,1960;Powers,1973;Cisek,1999)—forex- ourrunningexamplefromFig1withcurrentandfuturestates
ample, maintaining blood sugar may entail complex feedback S,S0,currentandfutureobservationsX,X0,andactionA,we
12 TheTwoKindsofFreeEnergyandtheBayesianRevolution
S S′
A X′
X X′
generative model desired future trial distributions
p
0
(X,S,A) p
des
(X′) q(X′, S,A)
stneidergni
combine model and desired ϕ(X,S,A) minF(q∥ϕ)
distributions into reference q
ϕ∝p
0
eQ(A) ϕ∝p
0
p
des
A Q(A) X′
X′ X′
epicer
S S′ S S′ S S′
A or A or A
X′ X′ X′
full mean-field partial mean-field exact/Bethe approx.
Friston et al. 2013/15 Friston et al. 2016/17 Schwöbel et al. 2018
Parr et al. 2019
fit trial distributions to reference 1 2
by minimizing free energy
through alternating optimization:
minF(q(S)q(S′ |A)q(A)∥ϕ )
q(S) ref
minF(q(S)q(S′ |A)q(A)∥ϕ )
or q(S′| A) ref
minF(q(S)q(S′ |A)q(A))∥ϕ )
q(A) ref
(exemplary for q = q ( S ) q ( S ′ | A ) q ( A ) , i.e. partial mean-field)
Friston et al. 2013-2017 Schwöbel et al. 2018
Figure7.OverviewoftheActiveInferencerecipe,appliedtoourexamplefromFig1.
needagenerativemodelp ,adesireddistributionp ,andtrial ascanbeseenforexampleinS.2,mean-fieldassumptionsmay
0 des
distributionsq. Thegenerativemodelp (X,S,A)isspecified fail to show goal-directed behavior even for very simple tasks
0
bythefactorsinthedecomposition(1),thedesireddistribution suchasthenavigationinagridworld.Alessrestrictiveassump-
p (X0) is a given fixed probability distribution over future tionwouldbeaBetheapproximation,aspecialcaseofKikuchi’s
des
sensory states X0, and the trial distributions q are probability clustervariationmethod(Kikuchi,1951),whichallowsSandS0
distributionsoverallunknownvariables,S,S0,X0,andA. aswellasS0andX0tobestochasticallydependent—cf.Section
CinAppendixA.1, wherewederivetheupdateequationsun-
Inmost treatmentsofActiveInferenceinthe literature, the
dertheBetheassumptionforthesimpleexampleofthissection.
trialdistributionsqaresimplified,eitherbyafullmean-fieldap-
In general, the Bethe approximation achieves exact marginals
proximationoverstatesandactions(Fristonetal.,2013,2015),
in tree-like models, such as the models that are considered in
byapartialmean-fieldapproximationwherethedependencyon
theActiveInferenceliterature,becauseitresultsinupdateequa-
actionsiskeptbutthestatesaretreatedindependentlyofeach
tionsthatareequivalenttoPearl’sbeliefpropagationalgorithm
other(Fristonetal.,2016,2017a), ormorerecently(Schwöbel
(Yedidiaetal.,2001;Pearl,1988).
etal.,2018;Parretal.,2019)bytheso-calledBetheapproxima-
tion(Yedidiaetal.,2001;Heskes,2003),wheresubsequentstates Reference function. The reference φ is constructed by combin-
areallowedtointeract. Inthepartialmean-fieldassumptionof ingthetwodistributionsp andp .Todoso,therehavebeen
des 0
(Fristonetal.,2016),thetrialdistributionoverX0 isfixedand several proposals in the Active Inference literature, which fall
givenbyp (X0|S0),whileforA,SandS0thetrialdistributions into one of two categories: either a specific value function Q
0
arevariablebutrestrictedtobeofthemean-fieldformforSand isdefined(containingp ), whichismultipliedtothegenera-
des
S0, tivemodelusingasoft-maxfunction(Fristonetal.,2015,2016,
q(S,A)=q(S)q(S0|A)q(A), (23) 2017a),
i.e.,thehiddenstatesS andS0 areassumedtobeindependent φ(X0,S,A):=p (X=x,X0,S|A) 1 p (A)eQ(A), (24)
givenA.Whilemean-fieldapproximationscanbegoodenough 0 Z 0
for simple perceptual inference, where a single hidden cause orthedesireddistributionismultiplieddirectlytothegenerative
mightberesponsibleforasetofobservations, theycanbetoo model(Schwöbeletal.,2018),
strongsimplificationsforsequentialdecision-makingproblems
wherethenextstateS0dependsonthepreviousstateS.Infact, φ(X0,S,A):=p (X0)p (X=x,X0,S,A). (25)
des 0
TheTwoKindsofFreeEnergyandtheBayesianRevolution 13
While the reference function in (25) is already completely 5.3. Criticalpoints
specified,westillneedtoknowhowtodeterminethevaluefunc-
tionQinthecaseof(24).Forthepartialmean-fieldassumption ThemainideabehindActiveInferenceistoexpresstheprob-
(23)itisdefinedintheliterature(Fristonetal.,2016,2017a)as lem of action selection in a similar manner to the perceptual
problemofBayesianinferenceoverhiddencauses. InBayesian
Q(a):=hU(X0,S0)i q(X0,S0|A=a) +H (cid:0) q(X0|A=a) (cid:1) , (26) inference,agentsareequippedwithlikelihoodmodelsp 0 (X|Z)
that determine the desirability of different hypotheses Z un-
where U(x0,s0) := logp des (x0) + logp 0 (x0|s0) favors both der known data X. In Active Inference, agents are equipped
desirableandplausiblefutureobservationsx0. Whileherede- withagivendesireddistributionp (X0)overfutureoutcomes
des
sirabilityandplausibilityisbuiltintothevaluefunctionQid-
thatultimatelydeterminesthedesirabilityofactionsA. Anim-
iosyncratically,inutility-basedapproaches(cf.Section4.2)only
portantdifferencethatarisesisthatperceptualinferencehasto
desirabilityhastobeputintothedesignoftheutilityfunction, conditiononpastobservationsX=x,whereasnaiveinference
becausetherethelikelihoodp (X0|S0)offutureobservationsis
0 overactionswouldhavetoconditionondesiredfutureoutcomes
automaticallytakenintoaccountbytheexpectedutilityV that X0=x0.
is(soft-)maximizedby(21).Moreover,sinceQcanberewritten
For a single desired future observation x0, Bayesian in-
as
ference could be applied in a straightforward way by sim-
Q(a)=−D KL (cid:0) q(X0|A)kp des (X0) (cid:1) − (cid:10) H (cid:0) p 0 (X0|S0) (cid:1)(cid:11) q(S0|A) ,
l
p
a
l
r
y
ly
c
,
o
o
n
n
d
e
iti
c
o
o
n
u
i
l
n
d
g
c
t
o
h
n
e
di
g
t
e
io
n
n
er
o
at
n
iv
a
e
d
m
e
o
si
d
r
e
e
l
d
p
d
0
is
o
tr
n
ib
X
ut
0
io
=
n
x
p
0. S
(
i
X
m
0
i
)
-
des
theextraentropytermin(26)hastheeffectofactionsleading using Jeffrey’s conditioning rule (Jeffrey, 1965), resulting in
toconsequencesthatmoreorlessmatchthedesireddistribution, p(A|p
des
)= P
x0
p(A|x0)p
des
(x0), which could be imple-
whilealsoexplicitlypunishingactionsthatleadtoahighvari- mentedbyfirstsamplingagoalx0∼p des (X0)andtheninfer-
ability of observations (by requiring a low average entropy of ringp(A|x0)giventhesingledesiredobservationx0. However,
p (X0|S0)),ratherthantryingtoproducethesinglemostdesired oneoftheproblemswithsuchanaiveapproachisthatthechoice
0
outcome—seethediscussionattheendofSection5.3.Notealso ofagoalissolelydeterminedbyitsdesirability,whereasitsreal-
thatthevaluefunctionQdepends(non-linearly)onthetrialdis- izabilityforthedecision-makerisnottakenintoaccount.Thisis
tributionq(S0|A),becauseq(X0|A) = P s0 p 0 (X0|s0)q(s0|A) becausebyconditioningonp des ,thedecision-makereffectively
isitselfafunctionofq(S0|A),whichisproblematicduringfree seekstochooseactionsinordertoreproduceormatchthedesired
energyminimization(see(ii)inSection5.3). distribution.
Freeenergyminimization.Oncetheformofthetrialdistributions To overcome this problem, Control as Inference or Planning
asInferenceapproachesinthemachinelearningliterature(Tou-
q—e.g.byapartialmean-fieldassumption(23)oraBetheapprox-
ssaint and Storkey, 2006; Todorov, 2008; Kappen et al., 2012;
imation (see Derivation of exemplary update equations)—and
Levine,2018;O’Donoghueetal.,2020)donotdirectlycondition
thereferenceφaredefined,thevariationalfreeenergyissimply
ondesiredfutureobservationsbutonfuturesuccessbyintroduc-
determinedbyF(qkφ).Inthecaseofamean-fieldassumption,
inganauxiliarybinaryrandomvariableRsuchthatR = 1en-
the resulting free energy minimization problem is solved ap-
codestheoccurenceofdesiredoutcomes.Theauxiliaryvariable
proximatelybyperforminganalternatingoptimizationscheme,
Rcomeswithaprobabilitydistributionp (R|X0,...)thatdeter-
inwhichthevariationalfreeenergyisminimizedseparatelywith 0
mineshowwelltheoutcomessatisfydesirabilitycriteriaofthe
respecttoeachofthevariablefactorsinafactorizationofq,for
decision-maker,usuallydefinedintermsoftherewardorutility
examplebyalternatingbetweenmin F,min F,and
q(S) q(S0|A) attachedtocertainoutcomes—seethediscussionin(iii)below.
min F inthecaseofthepartialmean-fieldassumption(23),
q(A)
Theextravariablegivesthenecessaryflexibilitytoinfersuccess-
where in each step the factors that are not optimized are kept
fulactionsbysimplyconditioningonR=1. Theadvantageof
fixed(cf. Fig7). InDerivationofexemplaryupdateequations
suchanapproachoverdirectJeffreyconditionalizationgivena
wederivetheupdateequationsforthecases(24)and(25)under
desireddistributionoverfutureobservationscanbeseeninthe
mean-fieldandBetheapproximationsfortheone-stepexample
gridworldsimulationsinNotebook:Gridworldsimulations,es-
discussed in this section. Mean-field solutions for the general
peciallytheabilityofchoosingadesiredoutcomethatisnotonly
caseofarbitrarilymanytimestepstogetherwiththeirexactso-
desirablebutalsoachievable—seealsoFig8.
lutionscanbefoundinNotebook:Comparisonofdifferentfor-
mulationsofActiveInference,wherewealsohighlightthethe- ActiveInferencetriestoovercomethesameproblemofrecon-
oretical differences between various proposed formulations of cilingrealizabilityanddesirability,butwithoutexplicitlyintro-
ActiveInference. Theeffectofsomeofthesedifferencescanbe ducingextrarandomvariablesandwithoutexplicitlycondition-
seeninthegridworldsimulationsinNotebook:Gridworldsim- ingonthefuture. Instead,thedesireddistributioniscombined
ulations. withthegenerativemodeltoformanewreferencefunctionφ
14 TheTwoKindsofFreeEnergyandtheBayesianRevolution
suchthattheposteriorsq∗ resultingfromtheminimizationof freeenergyF(qkφ(q))withrespecttostateandactiondistri-
the free energy F(qkφ) contain a baked-in tendency to reach butions,onealternatinglyoptimizesthefreeenergyoverstates
the desired future encoded by φ. This approach is the root of F foreachactionAandthenthefullfreeenergywithrespect
A
anumberofcriticalissueswithcurrentformulationsofActive toactiondistributionsonly,sothatactionandperceptioneffec-
Inference: tivelyoptimizetwodifferentfreeenergies. Itiscrucialtonote,
however,thatunlikeinvariationalBayesianinferencewithfixed
(i) Howtoincorporatethedesireddistributionintothereference? reference,thisseparationdoesnotfollowfromtheformalismof
variationalfreeenergy, butisadesignchoiceoftheActiveIn-
InsteadofusingBayesianconditioningdirectlyinordertocon-
ferenceframeworkthatimposesthisseparationbyforce(seethe
ditionthegenerativemodelp onthedesiredfuture,inActive
0 AppendixSeparationofmodelandstatevariablesformorede-
Inferenceitisrequiredthatthereferenceφcontainsthedesired
tails).Thisway,bothseparateoptimizationscanbeconsideredas
distributioninawaysuchthatactionssampledfromtheresult-
variationalinferenceineachsingleupdate,eventhoughwhenal-
ingposteriormodelaremorelikelyiftheyleadtothedesiredfu-
ternatingthemthereferenceφstillchangesacrossthecombined
ture.Ascanbeseenalreadyfortheone-stepcasein(24)and(25),
optimizationprocess.Thisisincontrasttoalternatingoptimiza-
themethodofhowtoincorporatethedesireddistributioninto
tionschemesinvariationalinference(e.g.,intheBayesianEM
thereferencefunctionisnotuniqueanddoesnotfollowfrom
algorithm)wherethereferenceφdoesnotchangebetweenopti-
firstprinciples. Therehavebeenessentiallytwodifferentpro-
mizationsteps.Thus,therearetwochoices:EitherQ-valueAc-
posalsintheliteratureonActiveInferenceofhowtocombine
tiveInferenceisregardedassomekindofapproximationtovari-
thetwodistributionsp andp intoφ(cf.Fig7):Eitherahand-
des 0 ationalinferenceunderasingletotalfreeenergy,oronehasto
crafted value function Q is designed that specifically modifies
giveuptheideaofasinglefreeenergyfunctionthatisoptimized.
theactionprobabilityofthegenerativemodel,ortheprobability
Eitherway,thecombinedprocessofactionandperceptiondoes
overfuturesX0underthegenerativemodelp ismodifiedbydi-
0 notcorrespondtoasinglevariationalinferenceprocess.
rectlymultiplyingp tothelikelihoodp (X0|S0). Wediscuss
des 0 Finally,anotherimportantpracticalissuewithQ-valueActive
bothoftheseproposalsin(ii)and(iii)below.
Inference models is that the definition of Q relies on a mean-
(ii) Proposal 1: Q-value Active Inference (Friston et al., 2013, fieldapproximationofthetrialdistributionsq,underwhichhid-
2015, 2016, 2017a) In the most popular formulation of Active den states are assumed to be stochastically independent. This
Inference,theprobabilityoveractionsinthereferenceφisde- simplificationistoostrongforsequentialdecision-makingtasks,
finedby 1 p (A)eQ(A),wherethevaluefunctionQ(alsocalled whichrenderstheapproachunfitforenvironmentswherethe
Z 0
the“expectedfreeenergy”)dependsnon-linearlyonthetrialdis- currentstatedependsstochasticallyonpreviousstates(seeNote-
tributionsq,ascanbeseenexemplarilyin(26)fortheone-step book:Gridworldsimulationsforademonstration).
case under the partial mean-field assumption of Friston et al. (iii) Proposal2:directActiveInference(Schwöbeletal.,2018)
(2016, 2017a), where q(S0|A) enters Q through q(X0|A) =
P p (X0|s0)q(s0|A). Notethat,becauseofthisnon-linearity When multiplying p des to the generative model directly, as in
s0 0 (25),thentheresultingreferenceφisnolongergivenbyajoint
thealternatingfreeenergyminimizationwouldhavenoclosed-
distributionofobservations,states,andactions(sinceingeneral
form solutions (cf. Derivation of exemplary update equations).
P p (x0)p (x0|S0) 6= 1). Instead,thisformulationofAc-
Thismeansthatboththetrialdistributionsqandthereference x0 des 0
tiveInferenceturnsouttobeaspecialcaseofpreviousControl
φ = φ(q)willchangewhenq isvariedduringtheminimiza-
tionofthetotalvariationalfreeenergyF(qkφ(q)),aswouldbe asInferenceapproachesinthemachinelearningliterature(Tou-
ssaint and Storkey, 2006; Levine, 2018), where one conditions
requiredwhenstipulatingasinglefreeenergyfunctionalforop-
onanauxiliarysuccessvariableR.Inparticular,forourrunning
timization. Thishighlightsanimportantconceptualdifference
tovariationalBayesianinference,whereoneassumesafixedref- examplefromFig1withaprobabilisticmodeloftheform(1),
erenceφ—resultingfromtheevaluationofafixedprobabilistic ControlasInferencedefines
modelp 0 atknownvariables(seeSection3.2.1)—towhichdis- p (R=1|X0,S0,A):=er(X0,S0,A) =1−p (R=0|X0,S0,A),
tributionsqarefittedbyminimizingF(qkφ).Incontrast,when 0 0
changingthereferenceφ(q)duringtheoptimizationprocess,it where r = r(X0,S0,A) denotes a general (negative) reward
isnolongerclearwhatisactuallyachievedbythisminimization.
functiondeterminingdesirability. Thefulljointofthenewset
AsdemonstratedbyNotebook:Gridworldsimulations,thisis-
ofvariablesisthengivenby
suehasimmediatepracticalimplications,asrespectingorignor-
ingtheextraqdependencycanresultinverydifferentbehavior p (R,X,S,A)=p (R|X0,S0,A)p (X,S,A). (27)
0 0 0
eveninsimplegridworldsimulations.
In the Active Inference literature, however, the extra q- Control as Inference then conditions actionson both, thehis-
dependencyofQislargelyignored.Insteadofoptimizingthefull toryandfuturesuccess(R =1).Forourone-stepexample,this
TheTwoKindsofFreeEnergyandtheBayesianRevolution 15
resultsintheBayes’posterior quantityofinterest(energy),whichinpracticeoftenappearsin
theformofregularizedoptimizationproblems(e.g.,toprevent
1 X
p(A|X=x,R=1)= p (R=1|x0,s0,A)p (x,s,A). overfitting)orasageneralinferencemethodallowingtodeter-
Z 0 0
mineunbiasedpriorsandposteriors(cf.Section3.1).Inthevari-
x0,s,s0
(28) ationalformulationofBayes’rule,freeenergyplaystheroleofan
It is straightforward to identify p (X0) of Active Inference errormeasurethatallowstodoapproximateinferencebycon-
des
asaparticularchoiceofasuccessprobabilityp (R=1|X0),or strainingthespaceofdistributionsoverwhichfreeenergyisop-
0
equivalently,logp (X0)asarewardfunctionr = r(X0),so timized,butcanalsoinformthedesignofefficientiterativein-
des
thatthejointdistribution(27)reducestothereferencefunction ferencealgorithmsthatresultfromanalternatingoptimization
φin(25).Thus,theversionofActiveInferencein(Schwöbeletal., schemewhereineachstepthefullvariationalfreeenergyisop-
2018)issimplyavariationalformulationofControlasInference timizedonlypartially,suchastheBayesianEMalgorithm,belief
that approximates exact posteriors of the form (28), like other propagation,andothermessagepassingalgorithms(cf.Section
previousvariationalBayes’approaches(Toussaint,2009;Ziebart, 3.2).
2010;Levine,2018).
It is important to realize that, while the mathematical ex-
pressionsofafreeenergyfromconstraintswith“energy”E and
In summary, the assumption of a desired distribution p
des trade-off parameter β and a variational free energy with ref-
overfutureoutcomeshasledtovariousattemptsintheActive
erenceφcanformallybetransformedintoeachotherbyφ =
Inferenceliteratureofusingprobabilisticinferencetodetermine e−βE,thetwokindsoffreeenergyareinherentlydistinct,both
profitableactions. Eitheranactiondistribution 1 p (A)eQ(A)
Z 0 methodicallyandbytheirmotivation.Inthecaseofthefreeen-
isbuiltintothereferencefunction,whichpresupposesoptimal
ergyfromconstraints,wearegivenaconstraintonsomequan-
behaviorbydesigningavaluefunctionQthatleadstodesired
tityE andwearetryingtofulfilthisconstraintwithminimum
consequences,ortheoutcomeprobabilityunderthegenerative
biasbyselectingadistributionthattradesoffthetwocompet-
modelp ismodifieddirectlybymultiplyingp top .Thelat-
0 des 0 ingtermsEandentropy.Thistrade-offalsogivesthereasonfor
tercaseisthevariationalversionofControlasInference,well-
theexistenceoftheLagrangemultiplierβ thathastobedeter-
knowninthemachinelearningliterature(ToussaintandStorkey,
minedaccordingtotheconstraint. Inthissensethefreeenergy
2006; Todorov, 2008; Toussaint, 2009; Ziebart, 2010; Kappen
fromconstraintsisjustaspecialcaseofthefarmoregeneralLa-
etal.,2012;Levine,2018;O’Donoghueetal.,2020).Considering
grangianmethodwhenappliedtotheoptimizationofexpected
theissuesofQ-valueActiveInferencediscussedabove,andthe
valueshEi underentropyconstraints(ortheotherwayaround).
p
factthatControlasInferencedoesnotrelyonadesireddistri-
Incontrast,variationalfreeenergyissimplyatooltorepresent
butionoveroutcomes,wecouldaskwhetherformulatingpref-
thenormalizationofareferencefunctionφintermsofanop-
erencesbyassumingadesireddistributioniswell-advised. As
timizationproblem,andthereforedoesapriorinotassumethe
canbeseenfromFig8,thedifferencebetweenpurelyinference-
existenceofsomequantityE thatwemayhaveobservedinan
based methods, expected utility approaches, and Active Infer-
experimentorthathasanyotherconstraintsattached,nordoes
enceismainlyinhowtheytreatthedesireddistribution.Should
oneexplicitlyconsiderentropytobeconstrainedoroptimized.
p bematchedorisitgoodenoughifactionsarechosenthat
des Therefore,eventhoughstartingfroma(positive)referencefunc-
lead to a high desired outcome probability? While Control as
tionφwecanalwaysinventtheexistenceofsomequantityEand
Inferenceandutility-basedmodelsessentiallytakethelatterap- somemultiplierβsuchthatφ=e−βE,thisdoesnotexplainwhy
proach, Q-value Active Inference answers this question by re-
thesequantitiesshouldexistorwhytheyshouldbemappedinto
quiringthatthedesireddistributionshouldbematchedaslong
eachotherinthatparticularway. TheLagrangianmethod, on
astheaverageentropyofp (X0|S0)issmall.
0 theotherhand,explainswhyforagivenconstraintonEwehave
aLagrangemultiplierβ,howitisdetermined,andwhytheequi-
6. SoWhatDoesFreeEnergyBringTotheTable? libriumdistributionhastheformp∗ = 1e−βE.
Z
6.1. APracticalTool
6.2. TheoriesofIntelligentAgency
Itisunquestionablethattheconceptoffreeenergyhasseen
manyfruitfulpracticalapplicationsoutsideofphysicsinthesta- These practical use-cases of free energy formulations have
tisticalandmachinelearningliterature.Ashasbeendiscussedin alsoinfluencedmodelsofintelligentbehavior. Inthecognitive
Section3,theseapplicationsgenerallyfallintooneoftwocate- andbehavioralsciences,intelligentagencyhasbeenmodelledin
gories,theprincipleofmaximumentropy,andavariationalfor- anumberofdifferentframeworks, includinglogic-basedsym-
mulationofBayesianinference.Here,theprincipleofmaximum bolicmodels,connectionistmodels,statisticaldecision-making
entropyisinterpretedinawidersenseofoptimizingatrade-off models,anddynamicalsystemsapproaches.Eventhoughstatis-
between uncertainty (entropy) and the expected value of some ticalthinkinginabroadersensecaninprinciplebeappliedtoany
<latexit sha1_base64="/E+CyBg93o2gvFsjYqGGPHNNFD0=">AAADEnichVHLThRBFL00KA9fAy7dVJyYDImZdBMSYUECvmBjgokDRJp0qmtqhsr0K1U1RGznL/wNf8CdceuOFUb+xIWnysZEiaE61ffWueeeurduWmXK2DA8nwqmZ27cnJ2bX7h1+87de63FpT1TjrWQPVFmpT5IuZGZKmTPKpvJg0pLnqeZ3E9Hz1x8/0Rqo8rijT2t5FHOh4UaKMEtoKS1XXW2ltkGi804TxSLB5qLOprUcc7tseAZe5uoCYsfs6rzLlEfwHV+4sM6r/vSTFxgOWm1w27oF7vqRI3Tpmbtlq3vFFOfShI0ppwkFWThZ8TJ4DukiEKqgB1RDUzDUz4uaUILyB2DJcHgQEf4D3E6bNACZ6dpfLbALRm2RiajR9gvvWIKtrtVwjewP7Hfe2z43xtqr+wqPIVNoTjvFV8Bt3QMxnWZecO8rOX6TNeVpQGt+W4U6qs84voUf3SeI6KBjXyE0QvPHEIj9ecTvEAB20MF7pUvFZjvuA/LvZVepWgUOfQ0rHt91IMxR/8O9aqzt9KNVrvrr1fbm0+bgc/RA3pIHUz1CW3SDu2iDkGf6Ix+0EXwMfgcfAm+/qYGU03OffprBd9+AdRcqfo=</latexit>
<latexit sha1_base64="rtGFmdQ+3YchVWeKr6TYAjpVOyI=">AAADDXichVHLThRBFL20Dx6+Rli6qTgxGRIz6SYkwoIEUAkbE0wcINKkU11TM1SmX6mqIWI738Bv+APuDFt2bFG/xYWnysZEiaE61ffWueeeurduWmXK2DD8PhXcun3n7vTM7Ny9+w8ePmo9nt815VgL2RNlVur9lBuZqUL2rLKZ3K+05Hmayb109NLF946lNqos3tmTSh7mfFiogRLcAkpam1VnY5GtsXiguaijSR3n3B4JnrH3ExabcZ4oVnU+JOoTaPFzViWeoPO6L83EBRaTVjvshn6x607UOG1q1k7Z+kEx9akkQWPKSVJBFn5GnAy+A4oopArYIdXANDzl45ImNIfcMVgSDA50hP8Qp4MGLXB2msZnC9ySYWtkMnqGveUVU7DdrRK+gf2J/dFjw//eUHtlV+EJbArFWa/4BrilIzBuyswb5lUtN2e6riwNaMV3o1Bf5RHXp/ij8woRDWzkI4xee+YQGqk/H+MFCtgeKnCvfKXAfMd9WO6t9CpFo8ihp2Hd66MejDn6d6jXnd2lbrTcXX273F7fbAY+Q0/oKXUw1Re0Ttu0gzoEfaYLuqRvwWnwJfganP2mBlNNzgL9tYLzXzW1qFg=</latexit>
<latexit sha1_base64="0cI7I7+SBHqBwInjaQ8ptNCfEeE=">AAADJHichVFNb9QwEJ2Gr7Z8LXDkYrFC2kpolVSVgANSKR/iglQkti1sVpHj9abWOonlZKuWkL/D3+APcEMc4MCNK/wBDjybFAkqVEfOjN/Me57xpEarqg7Dz0vBmbPnzl9YXlm9eOnylau9a9d3qnJhhRyJUpd2L+WV1KqQo1rVWu4ZK3mearmbzh+5+O6BtJUqi5f1kZGTnGeFminBa0BJ75UZPFxjD1g8s1w0UdvEOa/3BdfN67ZlsTw0LN5S2ZjF1SJPFDODw0S9BSW+w2JdZswknmHzZiqr1kXXPGOS9PrhMPSLnXSizulTt7bL3heKaUolCVpQTpIKquFr4lThG1NEIRlgE2qAWXjKxyW1tAruAlkSGRzoHP8Mp3GHFjg7zcqzBW7R2BZMRrexn3rFFNnuVgm/gv2J/cZj2X9vaLyyq/AINoXiild8DrymfWScxsy7zONaTme6rmqa0T3fjUJ9xiOuT/FH5zEiFtjcRxg98ZkZNFJ/PsALFLAjVOBe+ViB+Y6nsNxb6VWKTpFDz8K610c9GHP071BPOjvrw2hjeP/FRn9zqxv4Mt2kWzTAVO/SJj2jbdQh6D19o+/0I3gXfAg+Bp9+pwZLHecG/bWCr78A0DqxcQ==</latexit>
<latexit sha1_base64="dNY/OL36Njh2HpG62DwS4t5VZPw=">AAADVXichVFda9RAFL3ZrbWtH90q+OKDg4uYfeiSlIL1QWhrlYIKlbrt4mYJk+xsGnbywSRbrGn+gH9Q9Ff4AxQ8M6YFLdIJk3vnnHvPvTM3yGVclI7zzWq1F24s3lxaXrl1+87d1c7avaMim6tQDMJMZmoY8ELIOBWDMi6lGOZK8CSQ4jiYvdT88alQRZylH8qzXIwTHqXxNA55CcjvfMntnR57wbyp4mHl1pWX8PIk5LL6WNfME59ybzeORmx9zzeMSqo3b2sviCM7t4dPz3d63nl+SU1EUQPtab7H1pkneRpJwfbt3Hd0+CE4ZTC/0tChVkAd1Bj7na7Td8xiVx23cbrUrIOs8508mlBGIc0pIUEplfAlcSrwjcglh3JgY6qAKXix4QXVtILcOaIEIjjQGf4RTqMGTXHWmoXJDlFFYitkMnqC/dooBojWVQX8AvYn9meDRf+tUBll3eEZbADFZaP4DnhJJ4i4LjNpIi96uT5T36qkKW2Z28ToLzeIvmd4qbMHRgGbGYbRKxMZQSMw51O8QAo7QAf6lS8UmLnxBJYbK4xK2ihy6ClY/froB2N2/x3qVedoo+9u9p+/3+xu7zYDX6KH9JhsTPUZbdM+HaCPkH5YD6xHFmt9bf1qL7QX/4S2rCbnPv212qu/AY29vW0=</latexit>
<latexit sha1_base64="KVzvDAJ3oJsbX1w+unwS4OmzS+U=">AAADNHichVFdaxQxFL0dv9r6teqjL8FF2IIsM1JQH4RaP/BFqOC2xZ1lyMxmp2EzHyTZ0jrO3/Jv+Kz4Jr765puoJ3FW0CLNkLk3J+ec3OSmtZLGhuGHleDM2XPnL6yurV+8dPnK1d6167umWuhMjLJKVXo/5UYoWYqRlVaJ/VoLXqRK7KXzx25/71BoI6vylT2uxaTgeSlnMuMWUNKT9eDRBnvI4pnmWRO1TVxwe5Bx1bxuWxaLozrelvmYxWZRJJLVg6NEvoUivsNiVeWdrk68TBfNVJjWcTbaZsmFDSwmSa8fDkM/2Mkk6pI+dWOn6n2kmKZUUUYLKkhQSRa5Ik4G35giCqkGNqEGmEYm/b6gltahXYAlwOBA5/jnWI07tMTaeRqvznCKwtRQMrqN+cw7pmC7UwVyg/gd843H8v+e0HhnV+ExYgrHNe/4ArilAzBOUxYdc1nL6Up3K0szuu9vI1Ff7RF3z+yPzxPsaGBzv8PoqWfm8Ej9+hAvUCKOUIF75aUD8zeeInIfhXcpO0cOP43oXh/1oM3Rv009mezeHUabwwcvN/tb213DV+km3aIBunqPtug57aCOjN7TN/pBP4N3wafgc/DlNzVY6TQ36K8RfP0FHpq4lw==</latexit>
16 TheTwoKindsofFreeEnergyandtheBayesianRevolution
1
p(A)=
Z∑
p(xi|A)p des(xi)
i
(Je C ff o r n ey d i c ti o o n n d in iti g o n o a n li z p a d t e i o s( n X ) ′) a B u a x y i e li s a i r a y n s c u o c n c d es it s io v n a in ri g a b o l n e (soft E -m xp a e xi c m te iz d a t U io t n il , i t β y = 1)
(Control as Inference/
1 direct Active Inference) 1
x
1 p
de
x
s
2 (X′)
x
3
p(A)=
Xi Zi
p(xi| A)pdes(xi)
p(A)= 1 p(xi| A)pdes(xi)
p(A)=
Z
exp
hXi
p(xi| A)logpdes(xi)
i
desired distribution
ZXi
A=1 A=2 A=1 A=2 A=1 A=2
x 1 x 2 x 3 early Q-value Active Inference recent Q-value Active Inference
(match desired and (additionally subtracting entropy of emission probability)
p(X′ |A=1)
predictive distributions)
predictive distribution for A = 1 p(A)= Z 1 exp hXi p(xi| A)log p p d ( e x s i ( | x A i ) ) i p(A)= Z 1 exp h   DKL   p(X0| A) k pdes(X0)    h H(p0(X0| S0) ip0(S0|A) i
<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJyyyynnnn9999llll++++1111vvvvPPPPNNNNzzzzSSSScccc6666sssseeeeaaaaFFFFzzzzWWWWXXXXGGGG4444bbbb////pppp0000===="""">>>>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==<<<<////llllaaaatttteeeexxxxiiiitttt>>>>−D KL(p(X′ |A)∥p des(X′) )
| {z } p 0(X′ |S′) p 0(X′ |S′)
x x x
1 2 3
p(X′ |A=2) A=1 A=2 A=1 A=2 A=1 A=2
predictive distribution for A = 2
Figure8. Consequencesofassumingadesireddistributionpdesforactionplanningunderpurelyinference-basedmethods,expectedutility,andActive
Inference,inthecaseofasimpleexamplewithtwoactions,onewithadeterministicoutcomeandonewithrandomoutcomes.Ascanbeseenfromthe
displayedequations,conditioningonpdes(Jeffreyconditionalization)andconditioningonsuccess(ControlasInference/directActiveInference)onlydiffer
intheorderofnormalizingandtakingtheexpectationoverX0.Whileconditioningonpdesrequirestofirstsampleatargetoutcomefrompdesbeforean
actionfromp(A|x0)canbeplanned,conditioningonsuccessdirectlyweighsthedesirabilityofanoutcomepdes(x0)byitsrealizabilityp(x0|A).Fromthis
pointofview,theexpectedutilityapproachisverysimilartoControlasInference(whichcanalsobeseeninthegridworldenvironmentNotebook:Grid
worldsimulations),sinceitalsoweighstheutilityofanoutcomewithitsrealizabilitybeforesoft-maximizing. Itonlydiffersinhowittreatsthedesired
distributionasanexponentiatedutility,movingtheutilityvaluesclosertogethersothatoptionA=1isslightlypreferred.Theearlyversion(Fristonetal.,
2013)ofActiveInferenceissimilartoJeffreyconditioning,becausedecision-makersarealsoassumedtomatchthedesireddistribution,bydefiningthe
valuefunctionQasaKLdivergencebetweenthepredictedanddesireddistributions. InlaterversionsofQ-valueActiveInference(Fristonetal.,2015,
2016,2017a),thevaluefunctionQismodifiedbyanadditionalentropytermthatexplicitlypunishesobservationswithhighvariability. Consequently,
P
evenwhentheeffectoftheactiononfutureobservationsiskeptthesame,i.e.,thepredictivedistributionp(X0|A)=
s0
p0(X0|s0)p0(s0|A)remainsas
depictedintheleft-handcolumn,thepreferenceoveractionsnowchangescompletelydependingonp0(X0|S0)—whereasintheotherapproaches,only
thepredictivedistributionp(X0|A)andpdes(X0)influenceplanning.Whiletheremightbecircumstanceswherethisextrapunishmentofhighoutcome
variabilitycouldbebeneficial,itisquestionablefromanormativepointofviewwhyanythingelseotherthanthepredictedoutcomeprobabilityp(X0|A)
shouldbeconsideredforplanning.SeeDetailsontheexampleinFig8fordetailsaboutthechoicesmadeintheexample.
TheTwoKindsofFreeEnergyandtheBayesianRevolution 17
oftheotherframeworksaswell,statisticalmodelsofcognition thesamefunction,thevariationalfreeenergy.However,thereis
in a more narrow sense have often focused on Bayesian mod- no mystery in having such a single optimization function, be-
els,whereagentsareequippedwithprobabilisticmodelsoftheir causetheunderlyingprobabilisticmodelalreadycontainsboth
environment allowing them to infer unknown variables in or- actionandperceptionvariablesinasinglefunctionalformatand
dertoselectactionsthatleadtodesirableconsequences(Tenen- thevariationalfreeenergyisjustafunctionofthatmodel.More-
baumandGriffiths,2001;Wolpert,2006;Todorov,2009).Natu- over,whileapproximateinferencecanbeformulatedontheba-
rally,theinferenceofunknownvariablesinsuchmodelscanbe sisofvariationalfreeenergy,inferenceingeneraldoesnotrely
achievedbyaplethoraofmethodsincludingthetwotypesoffree onthisconcept,inparticularinferenceoveractionscaneasilybe
energyapproachesofmaximumentropyandvariationalBayes. donewithoutfreeenergy(DayanandHinton,1997;Toussaint
However,bothfreeenergyformulationsgoonestepfurtherin andStorkey,2006;Todorov,2008;Kappenetal.,2012;Levine,
thattheyattempttoextendbothprinciplesfromthecaseofin- 2018).
ferencetothecaseofactionselection:utilityoptimizationwith
However,therearealsoplentyofsimilaritiesbetweenthetwo
informationconstraintsbasedonfreeenergyfromconstraints
freeenergyapproaches. Forexample,theassumptionofasoft-
andActiveInferencebasedonvariationalfreeenergy.
maxactiondistributioninActiveInferenceissimilartothepos-
While sharing similar mathematical concepts, both ap- teriorsolutionsresultingfromutilityoptimizationwithinfor-
proachesdifferinsyntaxandsemantics. Anapparentappleof mationconstraints. Moreover,theassumptionofadesiredfu-
discordistheconceptofutility(GershmanandDaw,2012).Util- turedistributionrelatestoconstrainedcomputationalresources,
ityoptimizationwithinformationconstraintsrequiresthede- becausetheuncertaintyconstraintinadesireddistributionover
termination of a utility function, whereas Active Inference re- futurestatesmaynotonlybeaconsequenceofenvironmental
quires the determination of a reference function. In the eco- uncertainty,butcouldalsooriginatefromstochasticpreferences
nomic literature, subjective utility functions that quantify the ofasatisficingdecision-makerthatacceptsawiderangeofout-
preferencesofdecision-makersaretypicallyrestrictiveinorder comes. Infact,aswehaveseeninthediscussionaroundFig8,
toensureidentifiabilitywhencertainconsistencyaxiomsaresat- various methods for inference over actions differ in how they
isfied.Incontrast,inActiveInferencethereferencefunctionin- treatpreferencesgivenbyadistributionoverdesiredoutcomes:
volvesdeterminingadesireddistributiongivenbythepreferred Someofthemtrytomatchthepredictiveanddesireddistribu-
frequencyofoutcomes.However,thesedifferencesstarttovan- tions,whileotherssimplyseektoreachstateswhoseoutcomes
ish when weakening the utility concept to something like log- haveahighdesiredprobability. InS.2, weprovideacompari-
probabilities,suchthattheutilityframeworkbecomesmoresim- son of the discussed methods using grid world simulations, in
ilartotheconceptofprobabilitythatisabletoexplainarbitrary ordertoseetheirresultingbehavioralsoinasequentialdecision-
behavior.Moreover,ActiveInferencehastosolvetheadditional makingtask.
problemofmarryinguptheagent’sprobabilisticmodelwithits
Aremarkableresemblanceamongbothapproachesistheex-
desireddistributionintoasinglereferencefunction(cf.Section
clusiveappearanceofrelativeentropytomeasuredissimilarity.
5.3). Thesolutiontothisproblemisnotunique,inparticularit
IntheActiveInferenceliteratureitisoftenclaimedthatevery
liesoutsidethescopeofvariationalBayesianinference,butitis
homeostaticsystemmustminimizevariationalfreeenergy(Fris-
criticalfortheresultingbehaviorbecauseitdeterminestheexact
ton,2013),whichissimplyanextensionofrelativeentropyfor
solutionsthatareapproximatedbyfreeenergyminimization.In
non-normalizedreferencefunctions(cf.Section3.2.2).Inutility-
fact,ascanbeseeninsimplesimulationssuchasS.2,thevari-
basedapproaches, therelativeentropy(19)istypicallyusedto
ousproposalsforthismergingthatcanbefoundintheActive
measuretheamountofinformationprocessing,eventhoughthe-
Inferenceliteraturebehaveverydifferently.
oreticallyothercostfunctionswouldbeconceivable(Gottwald
Also, bothapproachesdifferfundamentallyintheirmotiva- and Braun, 2019a). For a given homeostatic process, the KL
tion. The motivation of utility optimization with information divergencemeasuresthedissimilaritybetweenthecurrentdis-
constraintsistocapturethetrade-offbetweenprecisionandun- tributionandthelimitingdistributionandthereforeisreduced
certaintythatunderliesinformationprocessing. Thistrade-off whileapproximatingtheequilibrium. Similarly,inutility-based
takestheformofafreeenergyonceaninformationalcostfunc- decision-makingmodels,relativeentropymeasuresthedissimi-
tionhasbeenchosen(cf.Section4.3). NotethatBayes’rulecan laritybetweenthecurrentposteriorandtheprior.IntheActive
beseenastheminimumofafreeenergyfromconstraintswith Inferenceliteraturethestepwiseminimizationofvariationalfree
log-likelihoods as utilities, even though this equivalence is not energythatgoesalongwithKLminimizationisoftenequated
theprimarymotivationofthistrade-off. Incontrast,ActiveIn- with the minimization of sensory surprise (see Surprise mini-
ferenceismotivatedfromcastingtheproblemofactionselection mization for a more detailed explanation), an idea that stems
itselfasaninferenceprocess(Fristonetal.,2013),asthisallows from maximum likelihood algorithms, but that has been chal-
toexpressbothactionandperceptionastheresultofminimizing lengedasageneralprinciple(see(Biehletal.,2020)andthere-
18 TheTwoKindsofFreeEnergyandtheBayesianRevolution
sponse(Friston etal.,2020)). Similarly, onecouldin principle Consideringvariationalfreeenergy,thereisavastliterature
rewritefreeenergyfromconstraintsintermsofinformational onbiologicalapplicationsmostlyfocusingonneuralprocessing
surprise, which would however simply be a rewording of the (e.g.,predictivecoding,dopamine)(Schwartenbecketal.,2015;
probabilisticconceptsinlog-space.Thesamekindofrewording Fristonetal.,2017b;Parretal.,2019),buttherearealsoanumber
iswell-knownbetweenprobabilisticinferenceandtheminimum ofapplicationsaimingtoexplainbehavior(e.g.,humandecision-
descriptionlengthprinciple(Grünwald,2007)thatalsooperates making, hallucinations) (Parr et al., 2018). Similarly to utility-
inlog-space, andthusreformulatestheinferenceproblemasa basedmodels,ActiveInferencemodelscanbestudiedinterms
surpriseminimizationproblemwithoutaddinganynewfeatures ofasif models,sothatactualbehaviorcanbecomparedtopre-
orproperties. dictedbehavioraslongassuitablepriorandlikelihoodmodels
canbeidentifiedfromtheexperiment.Whenappliedtobraindy-
namics,theasifmodelsaresometimesalsogivenamechanistic
6.3. BiologicalRelevance interpretationbyrelatingiterativeupdateequationsthatappear
whenminimizingvariationalfreeenergywithdynamicsinneu-
So far we have seen how free energy is used as a technical ronalcircuits.AsdiscussedinSection3.2.3,theupdateequations
instrument to solve inference problems and its corresponding resultingforexamplefrommean-fieldorBetheapproximations,
appearanceindifferentmodelsofintelligentagency. Crucially, canoftenbewritteninmessagepassingforminthesensethat
these kinds of models can be applied to any input-output sys- the update for a given variable only has contributions that re-
tem,beitahumanthatreactstosensorystimuli,acellthattries quiresthecurrentapproximateposteriorofneighbouringnodes
tomaintainhomeostasis,oraparticletrappedbyaphysicalpo- intheprobabilisticmodel. Thesecontributionsareinterpreted
tential. Giventheexistingliteraturethathaswidelyappliedthe aslocalmessagespassedbetweenthenodesandmightberelated
conceptoffreeenergytobiologicalsystems,wemayaskwhether tobrainsignals(Parretal.,2019).Otherinterpretations(Friston
thereareanyspecificbiologicalimplicationsofthesemodels. etal.,2006,2017a;Bogacz,2017)obtainsimilarupdateequations
Considering free energy from constraints, the trade-off be- byminimizingvariationalfreeenergydirectlythroughgradient
tweenutilityandinformationprocessingcostsprovidesanor- descent, which can again be related to neural coding schemes
mative model of decision-making under resource constraints, likepredictivecoding. Asthesecodingschemeshaveexistedir-
thatextendspreviousoptimalitymodelsbasedonexpectedutil- respectiveoffreeenergy(RaoandBallard,1999;Aitchisonand
ity maximization and Bayesian inference. Analogous to rate- Lengyel,2017), especiallysinceminimizationofpredictioner-
distortion curves in information theory, optimal solutions to rorsisalreadyseeninmaximumlikelihoodestimation(Raoand
decision-makingproblemsareobtainedthatseparateachievable Ballard,1999),thequestionremainswhetherthereareanyspe-
from non-achievable regions in the information-utility plane cificpredictionsoftheActiveInferenceframeworkthatcannot
(cf.Fig5). Thebehaviorofrealdecision-makingsystemsunder beexplainedwithpreviousmodels(see(ColomboandWright,
varyinginformationconstraintscanbeanalyzedexperimentally 2018;Hohwy,2020)forrecentdiscussionsofthisquestion).
bycomparingtheirperformancewithrespecttothecorrespond-
ingoptimalitycurve.Onecanexperimentallyrelateabstractin-
6.4. Conclusion
formationprocessingcostsmeasuredinbitstotask-dependent
resourcecostslikereactionorplanningtimes(Schachetal.,2018; Any theory about intelligent behavior has to answer three
OrtegaandStocker,2016). Moreover,thefreeenergytrade-off questions: WhereamI?, wheredoIwanttogo?, andhowdoIget
canalsobeusedtodescribenetworksofagents,whereeachagent there?,correspondingtothethreeproblemsofinferenceandper-
islimitedinitsability,butthesystemasawholehasahigherin- ception,goalsandpreferences,andplanningandexecution. All
formationprocessingcapacity—forexample,neuronsinabrain threeproblemscanbeaddressedeitherinthelanguageofprob-
or humans in a group. In such systems different levels of ab- abilitiesorutilities. Perceptualinferencecaneitherbeconsid-
stractionarisedependingonthedifferentpositionsofdecision- eredasfindingparametersthatmaximizeprobabilitiesorlike-
makersinthenetwork(Lindig-Leónetal.,2019;Geneweinetal., lihoodutilities. Goalsandpreferencescaneitherbeexpressed
2015;GottwaldandBraun,2019b).AswehavediscussedinSec- byutilitiesoveroutcomesorbydesireddistributions.Thethird
tion4.3,justlikecodingandrate-distortiontheory,utilitytheory question can be answered by the two free energy approaches
withinformationcostscanonlyprovideoptimalityboundsbut thateitherdeterminefutureutilitiesbasedonmodelpredictions,
doesnotspecifyanyparticularmechanismofhowtoachieveop- or infer actions that lead to outcomes predicted to have high
timality.However,byincludingmoreandmoreconstraintsone desiredprobabilityormatchthedesireddistribution. Instan-
canmakeamodelmoreandmoremechanisticandtherebygrad- darddecision-makingmodelsactionsareusuallydeterminedby
uallymovefromanormativetoamoredescriptivemodel,such autilityfunctionthatranksdifferentoptions,whereaspercep-
asmodelsthatconsiderthecommunicationchannelcapacityof tualinferenceisdeterminedbyalikelihoodmodelthatquanti-
neuronswithafiniteenergybudget(BhuiandGershman,2018). fieshowprobablecertainobservationsare.Incontrast,bothfree
TheTwoKindsofFreeEnergyandtheBayesianRevolution 19
energyapproacheshaveincommonthattheytreatalltypesof Toussaint,2009;Ziebart,2010;Kappenetal.,2012;Levine,2018;
informationprocessing,fromactionplanningtoperception,as O’Donoghueetal.,2020)thatallowforBayesianmessagepassing
the same formal process of minimizing some form of free en- formulationswhosebiologicalimplementabilitycanbedebated
ergy. Butthecrucialdifferenceisnotwhethertheyuseutilities irrespectiveoftheexistenceofafreeenergyfunctional.
orprobabilities,buthowpredictionsandgoalsareinterwoven
Finally,bothkindsoffreeenergyformulationsofintelligent
intoaction.
agency are so general and flexible in their ingredients that it
This article started out by tracing back the seemingly mys- mightbemoreappropriatetoconsiderthemlanguagesortools
terious connection between Helmholtz free energy from ther- tophraseanddescribebehaviorratherthantheoriesthatexplain
modynamicsandHelmholtz’viewofmodel-basedinformation behavior,inasensesimilartohowstatisticsandprobabilitythe-
processingthatledtotheanalysis-by-synthesisapproachofper- oryarenotbiologicalorphysicaltheoriesbutsimplyprovidea
ception,asexemplifiedinpredictivecodingschemes,andinpar- languageinwhichwecanphraseourbiologicalandphysicalas-
ticular to discuss the role of free energy in current models of sumptions.
intelligent behavior. The mystery starts to dissolve when we
consider the two kinds of free energies discussed in this arti-
Funding
cle,onebasedonthemaximumentropyprincipleandtheother
based on variational free energy—a dissimilarity measure be- This study was funded by the European Research Coun-
tweendistributionsand(generallyunnormalized)functionsthat cil (ERC-StG-2015-ERC Starting Grant, Project ID: 678082,
extendsthewell-knownKLdivergencefrominformationthe- “BRISC:BoundedRationalityinSensorimotorCoordination”).
ory.TheHelmholtzfreeenergyisaparticularexampleofanen-
ergyinformationtrade-offthatresultsfromthemaximumen-
tropyprinciple(Jaynes,1957).Analysis-by-synthesisisapartic-
References
ularapplicationofinferencetoperception,wheredetermining
modelparametersandhiddenstatescaneitherbeseenasare- Aitchison,L.andLengyel,M.(2017).Withorwithoutyou:predictivecodingand
bayesianinferenceinthebrain.CurrentOpinioninNeurobiology,46:219–227.
sultofmaximumentropyunderobservationalconstraintsorof
ComputationalNeuroscience.
fittingparameterdistributionstothemodelthroughvariational
freeenergyminimization.Thus,bothnotionsoffreeenergycan
Alcock,J.(1993).Animalbehavior:anevolutionaryapproach.SinauerAssociates.
beformallyrelatedasentropy-regularizedmaximizationoflog- Ashby,W.(1960). DesignforaBrain:TheOriginofAdaptiveBehavior. Springer
probabilities. Netherlands.
Conceptually, however, utility-based models with informa- Beal,M.J.(2003).VariationalAlgorithmsforApproximateBayesianInference.PhD
tionconstraintsserveprimarilyasultimateexplanationsofbe- thesis,UniversityofCambridge,UK.
havior, this means they do not focus on mechanism, but on
Bernoulli,J.(1713).Arsconjectandi.Basel,ThurneysenBrothers.
thegoalsofbehaviorandtheirrealizabilityunderidealcircum-
stances. They have the appeal of being relatively straightfor- Bhui,R.andGershman,S.J.(2018). Decisionbysamplingimplementsefficient
wardgeneralizationofstandardutilitytheory,buttheyrelyon
codingofpsychoeconomicfunctions.PsychologicalReview,125(6):985–1001.
abstractconceptslikeutilityandrelativeentropythatmaynot
Biehl, M., Pollock, F. A., and Kanai, R. (2020). A technical critique of the
besostraightforwardlyrelatedtoexperimentalsettings. While freeenergyprincipleaspresentedin"lifeasweknowit"andrelatedworks.
thesenormativemodelshavenoimmediatemechanisticinter- arXiv:2001.06408.
pretation,theirrelevanceformechanisticmodelsmaybeanal-
Binz,M.,Gershman,S.J.,Schulz,E.,andEndres,D.(2020). Heuristicsfrom
ogous to the relevance of optimality bounds in Shannon’s in- boundedmeta-learnedinference.
formationtheoryforpracticalcodes(Shannon,1948). Incon-
Bogacz,R.(2017). Atutorialonthefree-energyframeworkformodellingper-
trast, Active Inference models of behavior often mix ultimate
ceptionandlearning.JournalofMathematicalPsychology,76:198–211.Model-
andproximateargumentsofexplainingbehavior(Alcock,1993;
basedCognitiveNeuroscience.
Tinbergen, 1963), because they combine the normative aspect
of optimizing variational free energy with the mechanistic in- Boutilier,C.,Dean,T.,andHanks,S.(1999).Decision-theoreticplanning:Struc-
turalassumptionsandcomputationalleverage.J.Artif.Int.Res.,11(1):1–94.
terpretationoftheparticularformofapproximatesolutionsto
thisoptimization.Whilemean-fieldapproachesofActiveInfer- Cisek,P.(1999).Beyondthecomputermetaphor:behaviourasinteraction.Jour-
encemaybeparticularlyamenabletosuchmechanisticinterpre-
nalofConsciousnessStudies,6(11-12):125–142.
tations,theyareoftentoosimpletocapturecomplexbehavior.In
Clark,A.(2013).Whatevernext?predictivebrains,situatedagents,andthefuture
contrast,thesolutionsofdirectActiveInferenceresultingfrom ofcognitivescience.BehavioralandBrainSciences,36(3):181–204.
aBetheassumptionareequivalenttopreviousControlasInfer-
Colombo,M.andWright,C.(2018).Firstprinciplesinthelifesciences:thefree-
enceapproaches(ToussaintandStorkey,2006;Todorov,2008;
energyprinciple,organicism,andmechanism.Synthese.
20 TheTwoKindsofFreeEnergyandtheBayesianRevolution
Corcoran,A.W.andHohwy,J.(2018).Allostasis,interoception,andthefreeenergy Friston,K.J.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.,andPezzulo,G.
principle:Feelingourwayforward.OxfordUniversityPress. (2015).Activeinferenceandepistemicvalue.CognitiveNeuroscience,6(4):187–
214.
Csiszár,I.(2008).Axiomaticcharacterizationsofinformationmeasures.Entropy,
10(3):261–273. Friston,K.J.,Shiner,T.,FitzGerald,T.,Galea,J.M.,Adams,R.,Brown,H.,Dolan,
R.J.,Moran,R.,Stephan,K.E.,andBestmann,S.(2012). Dopamine,affor-
Csiszár,I.andTusnády,G.(1984). Informationgeometryandalternatingmini- danceandactiveinference.PLoSComputationalBiology,8(1):e1002327.
mizationprocedures.StatisticsandDecisions,SupplementIssue,1:205–237.
Garner,W.R.(1962).Uncertaintyandstructureaspsychologicalconcepts.Wiley.
Dayan,P.andHinton,G.E.(1997). Usingexpectation-maximizationforrein-
forcementlearning.NeuralComputation,9(2):271–278. Genewein,T.,Leibfried,F.,Grau-Moya,J.,andBraun,D.A.(2015). Bounded
rationality,abstraction,andhierarchicaldecision-making: Aninformation-
Dayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995). Thehelmholtz theoreticoptimalityprinciple.FrontiersinRoboticsandAI,2.
machine.NeuralComput.,7(5):889–904.
Gershman,S.J.(2019).Whatdoesthefreeenergyprincipletellusaboutthebrain.
deLaplace,P.S.(1812).Théorieanalytiquedesprobabilités.Ve.Courcier,Paris. Neurons,Behavior,DataAnalysis,andTheory.
Dempster,A.P.,Laird,N.M.,andRubin,D.B.(1977).Maximumlikelihoodfrom
Gershman,S.J.andDaw,N.D.(2012).Perception,actionandutility:Thetangled
incompletedataviatheemalgorithm. JournaloftheRoyalStatisticalSociety. skein.InPrinciplesofBrainDynamics.MITPress.
SeriesB(Methodological),39(1):1–38.
Gigerenzer,G.andSelten,R.(2001). BoundedRationality:TheAdaptiveToolbox.
Doya,K.(2007). BayesianBrain:ProbabilisticApproachestoNeuralCoding. MIT
MITPress:Cambridge,MA,USA.
Press,Cambridge,Mass.
Gottwald,S.andBraun,D.A.(2019a). Boundedrationaldecision-makingfrom
Ergin,H.andSarver,T.(2010). Auniquecostlycontemplationrepresentation. elementarycomputationsthatreduceuncertainty.Entropy,21(4).
Econometrica,78(4):1285–1339.
Gottwald,S.andBraun,D.A.(2019b).Systemsofboundedrationalagentswith
Feynman,R.,Hey,A.,andAllen,R.(1996). FeynmanLecturesonComputation. information-theoreticconstraints.NeuralComputation,31(2):440–476.
Advancedbookprogram.Addison-Wesley.
Grau-Moya,J.,Leibfried,F.,Genewein,T.,andBraun,D.A.(2016). Planning
Flanagan,J.R.,Vetter,P.,Johansson,R.S.,andWolpert,D.M.(2003).Prediction
withinformation-processingconstraintsandmodeluncertaintyinmarkov
precedescontrolinmotorlearning.CurrentBiology,13(2):146–150.
decisionprocesses.InMachineLearningandKnowledgeDiscoveryinDatabases,
pages475–491.SpringerInternationalPublishing.
Fox,R.,Pakman,A.,andTishby,N.(2016). Tamingthenoiseinreinforcement
learningviasoftupdates.InProceedingsoftheThirty-SecondConferenceonUn-
certaintyinArtificialIntelligence,UAI’16,pages202–211,Arlington,Virginia,
Grünwald,P.(2007).TheMinimumDescriptionLengthPrinciple.MITPress,Cam-
bridge,Mass.
UnitedStates.AUAIPress.
Friston,K.(2013). Lifeasweknowit. JournalofTheRoyalSocietyInterface, Haarnoja,T.,Tang,H.,Abbeel,P.,andLevine,S.(2017).Reinforcementlearning
withdeepenergy-basedpolicies.InICML.
10(86):20130475.
Friston,K.(2018). Doespredictivecodinghaveafuture? NatureNeuroscience,
Hansen,L.P.andSargent,T.J.(2008).Robustness.PrincetonUniversityPress.
21(8):1019–1021.
Harsha,P.,Jain,R.,McAllester,D.,andRadhakrishnan,J.(2010). Thecommu-
nicationcomplexityofcorrelation. IEEETransactionsonInformationTheory,
Friston,K.,Costa,L.D.,andParr,T.(2020). Someinterestingobservationson
thefreeenergyprinciple.arXiv:2002.04501. 56(1):438–449.
Hathaway,R.J.(1986). Anotherinterpretationoftheemalgorithmformixture
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,O’Doherty,J.,andPez-
zulo,G.(2016). Activeinferenceandlearning. Neuroscience&Biobehavioral
distributions.Statistics&ProbabilityLetters,4(2):53–56.
Reviews,68:862–879.
Heskes,T.(2003). Stablefixedpointsofloopybeliefpropagationarelocalmin-
Friston,K.,Schwartenbeck,P.,Fitzgerald,T.,Moutoussis,M.,Behrens,T.,and imaofthebethefreeenergy. InBecker,S.,Thrun,S.,andObermayer,K.,
Dolan,R.(2013). Theanatomyofchoice:activeinferenceandagency. Fron- editors,AdvancesinNeuralInformationProcessingSystems15,pages359–366.
tiersinHumanNeuroscience,7:598. MITPress.
Friston,K.J.(2005). Atheoryofcorticalresponses. PhilosophicalTransactionsof Hinton,G.E.andvanCamp,D.(1993). Keepingtheneuralnetworkssimpleby
theRoyalSocietyB:BiologicalSciences,360(1456):815–836. minimizingthedescriptionlengthoftheweights. InProceedingsoftheSixth
AnnualConferenceonComputationalLearningTheory,COLT’93,pages5–13,
Friston,K.J.(2010). Thefree-energyprinciple:aunifiedbraintheory? Nature NewYork,NY,USA.ACM.
ReviewsNeuroscience,11:127–138.
Ho,M.K.,Abel,D.,Cohen,J.D.,Littman,M.L.,andGriffiths,T.L.(2020). The
Friston,K.J.,FitzGerald,T.H.B.,Rigoli,F.,Schwartenbeck,P.,andPezzulo,G. efficiencyofhumancognitionreflectsplannedinformationprocessing.Pro-
(2017a).Activeinference:Aprocesstheory.NeuralComputation,29:1–49. ceedingsofthe34thAAAIConferenceonArtificialIntelligence.
Friston,K.J.,Kilner,J.,andHarrison,L.M.(2006). Afreeenergyprinciplefor Hohwy,J.(2020). Self-supervision,normativityandthefreeenergyprinciple.
thebrain.JournalofPhysiology-Paris,100:70–87. Synthese.
Friston,K.J.,Parr,T.,anddeVries,B.(2017b).Thegraphicalbrain:Beliefprop- Jaynes,E.T.(1957). Informationtheoryandstatisticalmechanics. Phys.Rev.,
agationandactiveinference.NetworkNeuroscience,1(4):381–414. 106:620–630.
TheTwoKindsofFreeEnergyandtheBayesianRevolution 21
Jaynes,E.T.(2003).ProbabilityTheory.CambridgeUniversityPress. Mnih,V.,Badia,A.P.,Mirza,M.,Graves,A.,Lillicrap,T.,Harley,T.,Silver,D.,
andKavukcuoglu,K.(2016).Asynchronousmethodsfordeepreinforcement
Jeffrey,R.C.(1965).TheLogicofDecision.UniversityofChicagoPress,1edition. learning. InBalcan,M.F.andWeinberger,K.Q.,editors,ProceedingsofThe
33rdInternationalConferenceonMachineLearning,volume48ofProceedings
Kahneman,D.(2002). Mapsofboundedrationality:Aperspectiveonintuitive ofMachineLearningResearch,pages1928–1937,NewYork,NewYork,USA.
judgement.InFrangsmyr,T.,editor,Nobelprizes,presentations,biographies,&
PMLR.
lectures,pages416–499.Almqvist&Wiksell,Stockholm,Sweden.
Neal,R.M.andHinton,G.E.(1998). Aviewoftheemalgorithmthatjustifies
Kappen,H.J.,Gómez,V.,andOpper,M.(2012). Optimalcontrolasagraphical incremental,sparse,andothervariants. InJordan,M.I.,editor,Learningin
modelinferenceproblem.MachineLearning,87(2):159–182. GraphicalModels,pages355–368.SpringerNetherlands,Dordrecht.
Kawato,M.(1999). Internalmodelsformotorcontrolandtrajectoryplanning. O’Donoghue, B., Osband, I., andIonescu, C.(2020). Makingsenseofrein-
CurrentOpinioninNeurobiology,9(6):718–727. forcementlearningandprobabilisticinference. InInternationalConference
onLearningRepresentations,ICLR’20.
Kikuchi, R. (1951). A theory of cooperative phenomena. Physical Review,
81(6):988–1003. Opper,M.andSaad,D.(2001).ComparingtheMeanFieldMethodandBeliefProp-
agationforApproximateInferenceinMRFs,pages229–239.
Koller,D.(2009).Probabilisticgraphicalmodels:principlesandtechniques.TheMIT
Press,Cambridge,Massachusetts. Ortega,P.A.andBraun,D.A.(2013).Thermodynamicsasatheoryofdecision-
makingwithinformation-processingcosts.ProceedingsoftheRoyalSocietyA:
Levine,S.(2018).Reinforcementlearningandcontrolasprobabilisticinference: Mathematical,PhysicalandEngineeringSciences,469(2153):20120683.
Tutorialandreview.arXiv:1805.00909.
Ortega,P.A.andBraun,D.A.(2014). Generalizedthompsonsamplingforse-
Lindig-León,C.,Gottwald,S.,andBraun,D.A.(2019). Analyzingabstraction quentialdecision-makingandcausalinference. ComplexAdaptiveSystems
andhierarchicaldecision-makinginabsoluteidentificationbyinformation- Modeling,2(1):2.
theoreticboundedrationality.FrontiersinNeuroscience,13:1230.
Ortega,P.A.andStocker,A.(2016).Humandecision-makingunderlimitedtime.
Linson, A., Parr, T., and Friston, K. J. (2020). Active inference, stres- In30thConferenceonNeuralInformationProcessingSystems.
sors,andpsychologicaltrauma: Aneuroethologicalmodelof(mal)adaptive
explore-exploitdynamicsinecologicalcontext. BehaviouralBrainResearch, Parr,T.,Benrimoh,D.A.,Vincent,P.,andFriston,K.J.(2018).Precisionandfalse
380:112421. perceptualinference.FrontiersinIntegrativeNeuroscience,12:39.
Maccheroni,F.,Marinacci,M.,andRustichini,A.(2006). Ambiguityaversion, Parr,T.andFriston,K.J.(2017). Workingmemory,attention,andsaliencein
robustness,andthevariationalrepresentationofpreferences. Econometrica, activeinference.Scientificreports,7(1):14678–14678.
74(6):1447–1498.
Parr,T.andFriston,K.J.(2019). Generalisedfreeenergyandactiveinference.
MacKay,D.J.C.(2002).InformationTheory,Inference&LearningAlgorithms.Cam- BiologicalCybernetics.
bridgeUniversityPress,USA.
Parr,T.,Markovic,D.,Kiebel,S.J.,andFriston,K.J.(2019). Neuronalmessage
MacRae,A.W.(1970). Channelcapacityinabsolutejudgmenttasks:Anartifact passingusingmean-field,bethe,andmarginalapproximations. ScientificRe-
ofinformationbias?PsychologicalBulletin,73(2):112–121. ports,9(1):1889.
Marshall,A.W.,Olkin,I.,andArnold,B.C.(2011). Inequalities: TheoryofMa- Pearl,J.(1988).Beliefupdatingbynetworkpropagation.InPearl,J.,editor,Prob-
jorizationandItsApplications.SpringerNewYork,2ndedition. abilisticReasoninginIntelligentSystems,pages143–237.MorganKaufmann,
SanFrancisco(CA).
Mattsson,L.-G.andWeibull,J.W.(2002).Probabilisticchoiceandprocedurally
boundedrationality.GamesandEconomicBehavior,41(1):61–78. Poincaré,H.(1912).Calculdesprobabilités.Gauthier-Villars,Paris.
McFadden,D.L.(2005). Revealedstochasticpreference:asynthesis. Economic Powers,W.T.(1973).Behavior:TheControlofPerception.Aldine,Chicago,IL.
Theory,26(2):245–264.
Rao,R.P.N.andBallard,D.H.(1999). Predictivecodinginthevisualcortex:a
McKelvey,R.D.andPalfrey,T.R.(1995).Quantalresponseequilibriafornormal functionalinterpretationofsomeextra-classicalreceptive-fieldeffects. Na-
formgames.GamesandEconomicBehavior,10(1):6–38. tureNeuroscience,2(1):79–87.
Miller,G.A.(1956).Themagicalnumberseven,plusorminustwo:somelimits Rosenkrantz,R.D.(1983).E.T.Jaynes:PapersonProbability,StatisticsandStatistical
onourcapacityforprocessinginformation.PsychologicalReview,63(2):81–97. Physics.SpringerNetherlands,Dordrecht.
Minka,T.(2005). Divergencemeasuresandmessagepassing. TechnicalReport Russell,S.J.andSubramanian,D.(1995).Provablybounded-optimalagents.Jour-
MSR-TR-2005-173,Microsoft. nalofArtificialIntelligenceResearch,2(1):575–609.
Minka,T.P.(2001).Expectationpropagationforapproximatebayesianinference. Sales,A.C.,Friston,K.J.,Jones,M.W.,Pickering,A.E.,andMoran,R.J.(2019).
InProceedingsofthe17thConferenceinUncertaintyinArtificialIntelligence,UAI Locuscoeruleustrackingofpredictionerrorsoptimisescognitiveflexibility:
’01,pages362–369,SanFrancisco,CA,USA.MorganKaufmannPublishers Anactiveinferencemodel.PLOSComputationalBiology,15(1):e1006267.
Inc.
Saul,L.K.andJordan,M.I.(1996). Exploitingtractablesubstructuresinin-
Mirza,M.B.,Adams,R.A.,Mathys,C.,andFriston,K.J.(2018). Humanvi- tractablenetworks. InTouretzky,D.S.,Mozer,M.C.,andHasselmo,M.E.,
sualexplorationreducesuncertaintyaboutthesensedworld. PLOSONE, editors,AdvancesinNeuralInformationProcessingSystems8,pages486–492.
13(1):e0190429. MITPress.
22 TheTwoKindsofFreeEnergyandtheBayesianRevolution
Schach,S.,Gottwald,S.,andBraun,D.A.(2018).Quantifyingmotortaskperfor- Williams,P.M.(1980).Bayesianconditionalisationandtheprincipleofminimum
mancebyboundedrationaldecisiontheory.FrontiersinNeuroscience,12:932. information.TheBritishJournalforthePhilosophyofScience,31(2):131–144.
Schwartenbeck,P.,FitzGerald,T.H.B.,Mathys,C.,Dolan,R.,andFriston,K. Williams,R.J.andPeng,J.(1991). Functionoptimizationusingconnectionist
(2015). Thedopaminergicmidbrainencodestheexpectedcertaintyabout reinforcementlearningalgorithms.ConnectionScience,3(3):241–268.
desiredoutcomes.Cerebralcortex(NewYork,N.Y.:1991),25(10):3434–3445.
Winn,J.andBishop,C.M.(2005). Variationalmessagepassing. J.Mach.Learn.
Schwartenbeck,P.andFriston,K.(2016).Computationalphenotypinginpsychi- Res.,6:661–694.
atry:Aworkedexample.eNeuro,3(4):ENEURO.0049–16.2016.
Wolpert,D.H.(2006).InformationTheory–TheBridgeConnectingBoundedRatio-
Schwöbel,S.,Kiebel,S.,andMarković,D.(2018).Activeinference,beliefpropa-
gation,andthebetheapproximation.NeuralComputation,30(9):2530–2567. nalGameTheoryandStatisticalPhysics,pages262–290. SpringerBerlinHei-
delberg.
Shannon,C.E.(1948).Amathematicaltheoryofcommunication.TheBellSystem
TechnicalJournal,27:379–656. Wolpert,D.H.(2019). Thestochasticthermodynamicsofcomputation. Journal
ofPhysicsA:MathematicalandTheoretical,52(19):193001.
Simon,H.A.(1955).Abehavioralmodelofrationalchoice.TheQuarterlyJournal
ofEconomics,69(1):99–118. Yedidia,J.S.,Freeman,W.T.,andWeiss,Y.(2001). Generalizedbeliefpropaga-
tion.InLeen,T.K.,Dietterich,T.G.,andTresp,V.,editors,AdvancesinNeural
Sims,C.A.(2003).Implicationsofrationalinattention.JournalofMonetaryEco- InformationProcessingSystems13,pages689–695.MITPress.
nomics,50(3):665–690. SwissNationalBank/StudyCenterGerzenseeCon-
ferenceonMonetaryPolicyunderIncompleteInformation. Yedidia,J.S.,Freeman,W.T.,andWeiss,Y.(2005). Constructingfree-energy
approximationsandgeneralizedbeliefpropagationalgorithms. IEEETrans-
Sims,C.R.(2016). Rate–distortiontheoryandhumanperception. Cognition, actionsonInformationTheory,51(7):2282–2312.
152:181–198.
Still,S.(2009). Information-theoreticapproachtointeractivelearning. Euro- Yuille,A.andKersten,D.(2006). Visionasbayesianinference:analysisbysyn-
physicsLetters,85(2):28005.
thesis?TrendsinCognitiveSciences,10(7):301–308.Specialissue:Probabilistic
modelsofcognition.
Tatikonda,S.andMitter,S.(2004). Controlundercommunicationconstraints.
IEEETransactionsonAutomaticControl,49(7):1056–1068. Yuille,A.L.(2002). Cccpalgorithmstominimizethebetheandkikuchifree
energies:Convergentalternativestobeliefpropagation.NeuralComputation,
Tenenbaum, J.B.andGriffiths, T.L.(2001). Generalization, similarity, and 14(7):1691–1722.
bayesianinference.BehavioralandBrainSciences,24(4):629–640.
Ziebart,B.D.(2010). ModelingPurposefulAdaptiveBehaviorwiththePrincipleof
Tinbergen,N.(1963). Onaimsandmethodsofethology. ZeitschriftfürTierpsy- MaximumCausalEntropy.PhDthesis,CarnegieMellonUnversity.
chologie,20:410–433.
Tishby,N.andPolani,D.(2011).Informationtheoryofdecisionsandactions.In
Cutsuridis,V.,Hussain,A.,andTaylor,J.G.,editors,Perception-ActionCycle: A. Appendices
Models,Architectures,andHardware,pages601–636.SpringerNewYork.
A.1. Derivationofexemplaryupdateequations
Todorov,E.(2008).Generaldualitybetweenoptimalcontrolandestimation.In
200847thIEEEConferenceonDecisionandControl.IEEE. A.1.1. Q-valueActiveInference
Todorov,E.(2009). Efficientcomputationofoptimalactions. Proceedingsofthe InthesimpleexampleofSection5.2underthepartialmean-
NationalAcademyofSciences,106(28):11478–11483. field assumption (23), and in the case when the desired dis-
tribution p is combined with the generative model p via
Toussaint,M.(2009). Robottrajectoryoptimizationusingapproximateinfer- des 0
ence. InProceedingsofthe26thAnnualInternationalConferenceonMachine the value function Q as shown in Equation (24), i.e. if φ ∝
Learning-ICML'09.ACMPress. p (x,X0,S,A)eQ(A),thenthefullfreeenergyF(qkφ)canbe
0
writtenas
Toussaint,M.andStorkey,A.(2006).Probabilisticinferenceforsolvingdiscrete
andcontinuousstatemarkovdecisionprocesses. InProceedingsofthe23rd
InternationalConferenceonMachineLearning,ICML’06,pages945–952,New F(qkφ)=F(q(S|A)q(A)kp 0 (x|S)p 0 (S|A)p 0 (A)eQ(A))
York,NY,USA.AssociationforComputingMachinery. (cid:10) (cid:11)
= F (A)−Q(A) +D (q(A)kp (A)) (29)
S q(A) KL 0
vonNeumann,J.andMorgenstern,O.(1944). TheoryofGamesandEconomic
Behavior.PrincetonUniversityPress,Princeton,NJ,USA. where,F (A)−Q(A)isgivenby
S
Wainwright,M.,Jaakkola,T.,andWillsky,A.(2005). Mapestimationviaagree- (cid:28) q(S) q(S0|A) P p (X0|s0)q(s0|A) (cid:29)
menton(hyper)trees:Message-passingandlinear-programmingapproaches. log s0 0 (30)
IEEETransactionsonInformationTheory,51(11):3697–3717. p
0
(x|S)p
0
(S)p
0
(S0|S,A)p
des
(X0)p
0
(X0|S0)
Whittle,P.(1990).Risk-sensitiveoptimalcontrol.Wiley,ChichesterNewYork. wheretheexpectationiswithrespecttoq(X0,S|A). Thus,op-
Wiener,N.(1948). Cybernetics:OrControlandCommunicationintheAnimaland timizing(29)overq(A),whilekeepingq(S|A)fixed,resultsina
theMachine.JohnWiley. Boltzmanndistributionwithpriorp
0
(A)andenergyF
S
(A)−
TheTwoKindsofFreeEnergyandtheBayesianRevolution 23
Q(A). When optimizing F(qkφ) with respect to q(S) while A.1.2. Direct Active Inference (variational Control as Inference)—
keepingq(S0|A)andq(A)fixed,wehave mean-fieldassumption
Here,wederivetheupdateequationsresultingfromthemini-
q∗(S)=argmax F(qkφ)=argmaxhF (A)i
S q(A) mizationofthevariationalfreeenergyforthereferencedefined
q(S) q(S)
(cid:28) (cid:29) inEquation(25a),i.e.avariationalformulationofControlasin-
q(S)
=argmax log , (31) ference(ToussaintandStorkey,2006),underthemean-fieldas-
q(S) p 0 (x|S)p 0 (S)ehTi q(S0|A)q(A) q(S) sumption(23). Westartbywritingthevariationalfreeenergy
| {z }
(cid:0) (cid:13) (cid:1) F(qkφ)inaformanalogousto(29), wherenowφisgivenby
F q(S)(cid:13)p0(x|S)p0(S)ehTi
p (X0|S0)p (x,S|A)p (A),
0 0 0
whereT := logp 0 (S0|S,A)isshorthandforthelog-transition D E
probability.Hence,from(31)wecanreadoffthesolutionq∗(S) F(qkφ)= F(q(S|A)kp 0 (x,S|A))−G(A)
| {z } q(A)
invirtueofthegeneraloptimum(14)ofvariationalfreeenergy.
=FS(A)
WhilehereitwasenoughtooptimizehF i ,becauseincontains
S q +D (q(A)kp (A)),
KL 0
theonlydependenciesofF(qkφ)onq(S), thisisnotthecase
forq(S0|A),sincealsoQdependsonq(S0|A). Thus,whenop- where
timizing(29)overq(S0|A)whilekeepingq(A)andq(S)fixed,
D E
onehastooptimizehF −Qiwhichdoesnottaketheformof G(A):= hlogp (X0)i .
a free energy in q(S0|A
S
) due to the functional dependency of |
des
{z
p0(X0|S0
}
)
q(S0|A)
q(X0|A) = P p (X0|s0)q(s0|A)onq(S0|A)thatappearsin =:g(S0)
s0 0
(30). However,thistypeofdependencyislargelyignoredinthe Notethat,comparedtoQ-valueActiveInference,herewedonot
ActiveInferenceliterature(asforexamplenotedintheappendix havetomakeanyadditionalapproximations,becauseGonlyde-
of(Fristonetal.,2015)),sincetheoptimizationwithrespectto pendslinearlyonq(S0|A).
q(S0|A)wouldnothaveaclosed-formsolutionotherwise.
Similarly to above, when optimizing with respect to q(A)
Oncethistermisignored,thentheobjectiveforq(S0|A)takes whilekeepingq(S)andq(S0|A)fixed,weobtainthatq∗(A)is
averysimpleform, aBoltzmanndistributionwithenergyF −Gandpriorp (A).
S 0
Optimizingq(S)whilekeepingq(A)andq(S0|A)constanthas
q∗(S0|A)=argmax F(qkφ)
thesameresultasshownin(33a)becauseasbeforetheonlyde-
q(S0|A)
pendenciesonq(S)areinF .Finally,inordertoreadofftheso-
(cid:28) q(S0|A) (cid:29) S
≈argmax log , (32)
lutionoftheoptimizationwithrespecttoq(S0|A)whilekeeping
q(S0|A) ehTiq(S) q(S0|A) q(S)andq(A)constant,wecanrewriteF S −Gasfollows
fromwhichwecanagainreadofftheresultingupdateequation. q∗(S0|A)=argmax F(qkφ)=argmax (cid:0) F (A)−G(A) (cid:1)
S
Intotal,from(29),(31),and(32)weobtainthesetofequations q(S0|A) q(S0|A)
(cid:28) q(S0|A) (cid:29)
q∗(S)=
Z
1 p
0
(x|S)p
0
(S)ehTi q(S0|A)q(A) (33a) =a
q
r
(
g
S
m
0|A
a
)
x log
ehTiq(S)+g(S0) q(S0|A)
q∗(S0|A)≈ 1 ehTiq(S) (33b)
Z(A)
sothatintotalweobtainthesetofequations
q∗(A)=
Z
1 p
0
(A)e−FS(A)+Q(A), (33c)
where Z denotes the respective normalization constants and
q∗(S)=
Z
1 p
0
(x|S)p
0
(S)ehTi q(S0|A)q(A) (34a)
T= logp (S0|S,A). q∗(S0|A)= 1 ehTiq(S)+g(S0) (34b)
0 Z(A)
Itisimportanttonote,however,thatupdateequationsinAc- q∗(A)= Z 1 p 0 (A)e−FS(A)+G(A), (34c)
tiveInferenceresultingfromamean-fieldassumption(evenifit
isapartialmean-fieldassumptionsuchas(23))shouldbetaken where Z denotes the respective normalization constants, and
withcare, since—asisdemonstratedinthegridworldsimula-
againT =logp
0
(S0|S,A).
tionsinS2Notebook—eveninverysimplesituationstheresult- It is noteworthy that recently another free energy approach
ingagentsfailtocorrectlyplanactionsthatleadtodesiredstates. similar to Active Inference has been introduced that does not
makeuseofvariationalfreeenergy,butofadifferentfunctional
termedgeneralizedfreeenergy(ParrandFriston,2019).Despiteof
thedifferentfunctionalform,thisversionusesareferencefunc-
tionthatissimilartothedirectActiveInferenceapproach,where
24 TheTwoKindsofFreeEnergyandtheBayesianRevolution
thedesireddistributionisalsomultiplieddirectytothegener- stant.Theproblemthatwewanttosolveistofindanapproxima-
ativemodelbutwitharenormalizationthatresultsinamodi- tiontothisBayes’posteriorthatismoreprecisethanthemean-
fiedgenerativemodeloverobservations,states,andactions.Us- fieldapproximationoftheprevioussectionbutrequireslessin-
ingthisrenormalizedreferenceinavariationalfreeenergyap- volved computations than the determination of Z(A). While
proach would result in trivial inference reproducing the fixed one attempt is to partition the full graph into smaller graphs
priorp (A),correspondingtoBayes’conditioningthemodified andapplyanaivemean-fieldapproximationinsideofeachsub-
0
generativemodelonthepastanalogoustoperceptualBayesian graph,knownasastructuredmean-fieldapproximation(Sauland
inference,e.g.,p(A|X) = p (A)inthecaseoftheone-stepex- Jordan, 1996), the Bethe approximation follows a slightly dif-
0
ample. In contrast, the minimization of the free energy func- ferent approach. It is the simplest version of the cluster varia-
tionalusedin(ParrandFriston,2019)doesnotcorrespondtoa tionmethodsoftenattributedtoKikuchi(Kikuchi,1951),afam-
Bayesianinferenceprocess,whichiswhywedonotfurtherdis- ily of region-based free energy approximations (Yedidia et al.,
cussithere. 2005),whereonekeepsbeliefsoverdifferentsectionsofthefac-
torgraph.Specifically,intheBetheassumption,theregionscon-
sistofeachfactoranditsneighbouringnodes,whichcanalsobe
A.1.3. Direct Active Inference (variational Control as Inference)—
seenasallowingpair-wiseinteractions. Followingthesystem-
Betheassumption
atictreatmentin(Yedidiaetal.,2005),theBetheapproximation
Here,wederivetheupdateequationsresultingfromthemin- forourexampleconsistsofsevenbelieffunctions,oneforeach
imizationofthevariationalfreeenergyforthereference(25a) factor,b ,...,b ,andoneforeachvariable,b ,b ,andb ,
1 4 S S0 X0
under a Bethe approximation, which therefore is a more pre-
cisevariationalformulationofControlasInferenceasthemean- q(S,S0,X0|A)= b 1 (S)b 2 (S,S0)b 3 (S0,X0)b 4 (X0) (35)
fieldapproximationoftheprevioussection. Infact,itturnsout b (S)b (S0)b (X0)
S S0 X0
thatsuchequationsareequivalenttoBeliefpropagation(Yedidia
etal.,2001),awell-knowninferencemethodthatproducesexact wherethemarginalsofthefactorbeliefsarerequiredtobecon-
marginalsintree-likegraphs(Pearl,1988),suchastheprobabilis- sistentwiththesingle-variablebeliefs.Thus,thevariationalfree
ticmodelsconsideredinthearticleandintheActiveInference
energyF(A)canbewrittenas
literature.
4 (cid:28) (cid:29)
Analogous to the previous section, without any specific re- F(A)= X log b k − X hlogb i
f Y bY
strictionsonqwecanwritethetotalfreeenergyfortheone-step k=1 k bk Y∈{S,S0,X0}
examplefromSection5.2withthereference(25a)as
whichhastobeminimizedundertheconsistencyandnormal-
(cid:28) q(X0,S,S0|A) (cid:29) izationcontraints,leadingtotheLagrangian
F(qkφ)= log
p (R=1,X=x,X0,S,S0|A)
0 q X (cid:0) (cid:1)
| {z } F(A)+ λ 1 (s) b S (s)−b 1 (s)
=:hF(A)iq(A)
s
+ D (q(A)kp (A)) !
KL 0 X X
+ λ (s) b (s)− b (s,s0)
2S S 2
fromwhichitimmediatelyfollowsthatminimizingwithrespect s s0
toq(A), whileconsidering q(X0,S,S0|A)constant, resultsin !
X X
a Boltzmann distribution with energy F(A) and prior p 0 (A). + λ 2S0 (s0) b S0 (s0)− b 2 (s,s0)
F(A)isthevariationalfreeenergyofq(X0,S,S0|A)withre- s0 s
specttothereferencep (R=1,X=x,X0,S,S0|A)givenby !
0 X X
+ λ (s0) b (s0)− b (s0,x0)
3S0 S0 3
p 0 (x|S)p 0 (S)p 0 (S0|S,A)p 0 (X0|S0)p des (X0) . s0 x0
| {z }| {z }| {z }| {z } !
=:f1(S) =:f2(S,S0) =:f3(S0,X0) =:f4(X0) + X λ (x0) b (x0)− X b (s0,x0)
3X0 X0 3
Thus,minimizingF(A)withrespecttoq(X0,S,S0|A)without x0 s0
anyrestrictionsorsimplificationsresultsintheexactBayes’pos- + X λ (x0) (cid:0) b (x0)−b (x0) (cid:1)
4 X0 4
teriorp(X0,S,S0|A,R=1,X =x) x0
4
1 f (S)f (S,S0)f (S0,X0)f (X0), + X γ k (cid:16)X b k −1 (cid:17) + X γ Y (cid:16)X b Y −1 (cid:17)
Z(A) 1 2 3 4
k=1 Y∈{S,S0,X0}
where Z(A) denotes the corresponding normalization< con-
TheTwoKindsofFreeEnergyandtheBayesianRevolution 25
wheretheLagrangemultipliersfortheconsistencyconstraints A.2. DetailsontheexampleinFig8
aredenotedbyλandtheLagrangemultipliersforthenormal-
Here, we are giving additional details on Figure 8 in the
izationconstraintsbyγ.Theequationsforthebeliefsatthesta-
article. We consider the simple example of three possible
tionarypoints(zeroesofthederivativesoftheLagrangian)are
observations, x ,x ,x , a desired distribution p (X0) =
1 2 3 des
(1/3,1/6,1/2), two actions a with predictive distributions
b (s) ∝ f (s)eλ1(s),
1 1 p(X0|A=1)=(1,0,0)andp(X0|A=2)=(0,1/2,1/2),and
b 2 (s,s0) ∝ f 2 (s,s0)eλ2S(s)eλ 2S0(s0), a constant prior p 0 (A) = (1/2,1/2). We can consider
b
3
(s0,x0) ∝ f
3
(s0,x0)eλ 3S0(s0)eλ 3X0(x0),
p
p(X
(X
0|
0
A
,S
)
0,
a
A
s
)
a
=
re
p
su
(
lt
X
o
0
f
|S
m
0)
a
p
rg
(
i
S
na
0|
l
A
izi
)
n
p
g
(
t
A
h
)
e
w
g
i
e
t
n
h
e
s
r
t
a
a
t
t
i
e
v
d
e
is
m
tr
o
ib
d
u
e
-
l
b 4 (x0) ∝ f 4 (x0)eλ4(x0), ti 0 ons p(S0|A=1)= 0 (1,0,0) 0 and p(S0 0 |A=2)=(0,1/2,1/2)
b S (s) ∝ eλ1(s)eλ2S(s), a th n e d g a i n ve e n m p i ( s X sio 0 n |A p ) ro eq b u ab a i ls lit p y (X p 0 0 ( | X A) 0|S = 0) P that p is ( c X ho 0| s s e 0 n )p su ( c s h 0| t A ha ) t .
b
S0
(s0) ∝ eλ 2S0(s0)eλ 3S0(s0),
Suitable emission probabilities have fo
s
r
0
e
0
xample th
0
e form
b X0 (x0) ∝ eλ 3X0(x0)eλ4(x0), p 0 (X0=x i |S0=s j )=M ij (t),where
 
where the proportionality sign ∝ means that the left-hand 1 0 0
side results from normalizing the right hand-side to obtain a M(t)= 0 t 1−t 
probability distribution. By writing m
l
:= eλl for all l ∈ 0 1−t t
{1,2S,2S0,3S0,3X0,4},weobtainfromthestationaritycon-
for all t ∈ [0,1]. Note that the resulting average entropies
ditionsandtheconsistencyconstraints
hH(p (S0|A=2))i areintherange[0,1]bitforA=
0 p0(S0|A=2)
m (s) ∝ f (s) (36a) 2(alwayszeroforA=1),wheretheextremevaluesareassumed
2S 1
X att∈{0,1}(0bit)andt=1/2(1bit).
m (s) ∝ f (s,s0)m (s0) (36b)
1 2 2S0
s0 Furthermore, fortheapplicationofActiveInferenceinFig-
X
m 3S0 (s0) ∝ f 2 (s,s0)m 2S (s) (36c) ure 8, we have considered an exact version of the value func-
s
m 2S0 (s0) ∝ X f 3 (s0,x0)m 3X0 (x0) (36d) t p i l o a n ce , d Q b = y Q th e e xa e c x t a , c w t h p e re re dic th ti e ve tr d ia is l t d ri i b st u r t i i b o u n ti p on (S q( 0| S A 0| ) A . ) In is t r h e i - s
x0 0
m (x0) ∝ X f (s0,x0)m (s0) (36e) “exact” interpretation, the corresponding action distributions
4 s0 3 3S0 p(A) ∝ p
0
(A)eQexact(A) couldthenbeviewedasdefiningthe
m 3X0 (x0) ∝ f 4 (x0). (36f) idealbehaviourthatisapproximatedbythevariationalfreeen-
ergyminimization. IntheActiveInferenceliterature, p(A) ∝
Theupdateequationsforthebeliefsin(35)canbeobtainedby p (A)eQ(A)isconsidereda“prior”,becauseitisviewedaspartof
0
iteratingtheequationsin(36)andusingthestationaritycondi-
thegenerativemodelandthusisparttheinputtothevariational
tionsthatexpressthebeliefsintermsofthem . Notethatthe
l inferenceprocess.However,byconsideringQanapproximation
quantitiesdenotedbym areusuallyinterpretedaslocalmes-
l ofQ thesedistributionscanbeviewedasdefiningtheideal
exact
sagesthataresentbetweenthenodesandfactorsoftheunderly-
behaviorthatisapproximatedbythetrialdistributionsduring
inggraphicalmodel(Yedidiaetal.,2005),e.g.,m isconsidered
3S0 freeenergyminimizationandarethereforemoreinlinewiththe
amessagesentfromnodeS0tofactor3,whichcanbeusedtode-
“posteriors”inotherdecision-makingmodels(eventhoughthe
terminethemessagem fromfactor3tonodeX0byweighing
4 valuefunctionQ(A)—andthereforep(A)—ispresupposed, in
withf andsummingoverS0,etc. Bythisidentification,vari-
3 constrasttobeingtheresultofsomeprinciple).
ational inference under the Bethe approximation is equivalent
tobeliefpropagation.Whilein(36)thereisatmostonemessage
A.3. Surpriseminimization
thatismultipliedtothefactorf beforethesumistaken,inmore
k
complexfactorgraphs,wheremorethan2nodesareconnected The (informational) surprise or surprisal of a given element x
toafactor,themessagescomingintoafactorfromtheneigh- with respect to a probability distribution p (X) is defined as
0
boringnodesaremultipliedbeforetheyaresummedtocalculate S :=−logp (x),i.e.itissimplyastrictlydecreasingfunction
0 0
theoutgoingmessage,whichiswhythistypeofmessage-passing ofprobabilitysuchthatoutcomesxwithlowprobabilityhave
isalsoknownasthesum-productalgorithm. high surprise and outcomes x with high probability have low
surprise.Acommonstatementfoundintheliterature(Parrand
Friston,2017)isthatvariationalfreeenergyisanupperbound
onsurpriseandthusminimizingfreeenergyalsominimizessur-
prise.Thisideaoriginatesfromthespecialcaseofgreedyinfer-
26 TheTwoKindsofFreeEnergyandtheBayesianRevolution
encewithlatentvariables,where,forfixeddatax,thegoalisto energyisanalogouslysplitupintoasumofastatefreeenergy
P
maximizethelikelihoodp (x)= p (x,z)withrespecttoa F averagedoveractionsAandaKLtermwhich—incontrast
θ z θ A
parameterθ.IfthemarginalizationoverthelatentvariableZis tostandardBayesianinference—doesdependonstatedistribu-
toohardtocarryoutdirectly,thenonemighttakeadvantageof tions.However,ActiveInferenceessentiallyignoresthisextraq-
thebound dependencybyfollowingtheanalogousoptimizationschemeto
Bayesianinference:onealternatinglyoptimizesF withrespect
A
F(q(Z)kp θ (x,Z))≥−logp θ (x)=:S θ , (37) tostatedistributionsandthenthefullfreeenergywithrespectto
theactiondistributionsq(A).Inparticular,thisseparationinto
i.e. that the variational free energy of q(Z) is an upper bound
stateandactionfreeenergiesisnotaconsequenceofoptimizing
onthesurpriseS ,whichmightthereforebereducedbymini-
θ thefullvariationalfreeenergy,butadeliberatechoicemadeby
mizingitsupperboundwithrespecttoθasaproxy.Inthevaria-
ActiveInference.
tionalBayes’approachtotheaboveinferenceproblem,whereθ
istreatedasarandomvariableΘ,minimizationwithrespectto Inthefollowing,wediscussinmoredetailhowthisseparation
θisreplacedbytheminimizationwithrespecttoq(Θ). Inthis followsfromoptimizingthefullfreeenergyinstandardBayesian
case,theanalogousboundto(37)is
inferenceandhighlighthowQ-valueActiveInferenceadoptsthe
sameoptimizationschemebutbygivinguptheoptimizationof
F(q(Z|Θ)q(Θ)kp
0
(x,Z,Θ))≥−log X ehlogp0(x,z,Θ)iq(Θ), asinglevariationalfreeenergy.
z
wheretheright-handsideistheminimumoftheleft-handside
withrespecttoq(Z|Θ). Inthissense,variationalfreeenergyis
generallynotaboundonthesurpriseS anymore,butonalog- A.4.1. Bayesianinference
Θ
sum-expversionofitinstead.Nonetheless,alsointhisBayesian
approach,variationalfreeenergyisanupperboundonthesur- Considerthecaseofmultipleprobabilisticmodelsp (X,Z)
m
priseS 0 , thatareindexedbyalabelm,whereeachp m describesadiffer-
entprobabilisticrelationshipbetweendataXandhiddenstates
F(q(Z|θ)q(θ)kp (x,Z,Θ))≥−logp (x)=S , (38)
0 0 0 Z. Given data X = x, one could find the best m by select-
ing the model with the largest marginal likelihood p (x) =
wheretheright-handsideistheminimumoftheleft-handside P m
p (x,z). Apopularmethodtoaccomplishthisistheba-
withrespecttobothq(Z|θ)andq(θ). However,incontrastto z m
sicEMalgorithmDempsteretal.(1977),wheremisoptimized
(37),thereisnovariableleftinS overwhichonecouldmini-
0 greedilywhileZisinferredusingBayesianinferenceforagiven
mize. Therefore,sayingthatminimizingfreeenergyalsomini-
m(eitherexactorapproximate).InapurelyBayesiantreatment,
mizessurprise(ParrandFriston,2017),isgenerallyonlytruein
onealsoassumesapriordistributionovermodelsp (M),sothat
thesensethatminimizingfreeenergyminimizesanupperbound 0
thefulljointoverdataX, hiddenstatesZ, andmodelsM be-
onsurprise, howeversurpriseitselfisnotminimized. Instead,
comes
theimportantfactabout(38)isthatequalityisachievedbythe
Bayes’posteriorsq(Z|Θ) = p (Z|Θ,x)andq(Θ) = p (Θ|x)
0 0 p (X,Z,M):=p (X,Z)p (M)=:p (X,Z|M)p (M).
0 M 0 0 0
asdiscussedinSection3.2.1.
The Bayes’ posterior p(M|X) can then simply be determined
fromtheBayes’posteriorp(Z,M|X)throughmarginalization
A.4. Separationofmodelandstatevariables
overZ.Asdiscussedinthearticle(Section3.2),ifdirectBayesian
InQ-valueActiveInference,actionandperceptiondonotop- inferenceisinfeasablethenavariationalformulationmightbe
timizethesamevariationalfreeenergybuttwodifferentfreeen- useful, where trial distributions q(Z,M) over the unknown
ergyexpressions.Thisismotivatedfromtheseparationofmodel variables M and Z are fitted to the reference φ(Z,M) :=
variablesM andstatevariablesinstandardvariationalBayesian p 0 (x,Z,M)byminimizingthevariationalfreeenergy
inference,wherethefullfreeenergycanbesplitupintoasum
(cid:28) (cid:29)
q(Z,M)
of a state free energy F M averaged over models M and a KL F(qkφ)= log .
p (x,Z,M)
termthatisindependentofstatedistributions. Optimizingthe 0 q(Z,M)
fullfreeenergycanthenbedoneseparatelybyalternatinglydo-
Bywritingqandp intheirfactorizedforms
ingperceptualinferencebyoptimizingF foreachmodelM 0
M
andoptimizingthefullfreeenergytofindthemodeldistribution
q(Z,M)=q(Z|M)q(M),
q(M). InActiveInference, whereactionsAmightbethought
p (X,Z,M)=p (X,Z|M)p (M),
of analogous to models M in Bayesian inference, the full free 0 0 0
TheTwoKindsofFreeEnergyandtheBayesianRevolution 27
thevariationalfreeenergycanbedecomposedas and(42)areanalogoustoEquations(39)and(40),respectively.
However,whenoptimizingthefullfreeenergyF(qkφ)withre-
(cid:28) (cid:29)
q(Z|M)q(M) specttothefactorq(X0,S|A),onewouldhavetoconsiderboth
F(qkφ)= log
p 0 (x,Z|M)p 0 (M) q termsinthedecomposition(41),since,unlikep 0 (M)inthepre-
DD q(Z|M) E E vioussection,herep˜ 0 (A)doesdependontrialdistributionsover
= log states(thefactorq(S0|A)). Itshouldbenotedthatthisdepen-
p 0 (x,Z|M) q(Z|M) q(M)
| {z } dencyisnon-linearandnon-local,andthereforeaclosed-form
=:FM solutioncannotbederived(cf.(ii)inSection5.3).
D q(M) E
+ log InActiveInference,thiscomplicationisavoidedbysimplyig-
p 0 (M) q(M) noringtheq-dependencyofQwhenderivingtheupdateequa-
=hF i +D (q(M)kp (M)). (39) tions,or,putdifferently,oneoptimizestwodifferentfreeener-
M q(M) KL 0
giesforperceptionandaction: onefirstoptimizesF withre-
A
Notably,theminimizationofF withrespecttoqsplitsupinto specttostatedistributionsforeachactionAandthenoneop-
theminimizationofthefreeenergyoverstates timizes the full free energy (41) with respect to q(A). This is
inanalogytoBayesianmodelselectionoftheprevioussection,
(cid:0) (cid:13) (cid:1)
F M =F q(Z|M)(cid:13)p 0 (x,Z|M) (40) wherethisseparationwasaconsequenceoftheminimizationof
thefullfreeenergy.However,here,duetotheextradependency
withrespecttoq(Z|M),andtheminimizationof(39)withre-
ofp˜ (A)onq,itisnotaconsequencebutachoicemadebyAc-
0
spect to q(M). In particular, the inference over models and
tiveInference. Thismeansthatonenolongerdoesvariational
states, (M,Z), separates into inference over hidden states for
inferenceoverthecombinedsetofstatesandactions,butvaria-
each model, which determines F for each M, and inference
M tionalinferenceoverstateswithfreeenergyF andvariational
A
overM.
inferenceoveractionswithfreeenergy(41).Inparticular,there
isnotasinglefreeenergythatisoptimizedbybothperception
andaction,buttwodifferentones.
A.4.2. ActiveInference
InQ-valueActiveInference, actionselectionistreatedsim-
S. SupplementaryMaterial
ilarlytomodelselectioninBayesianinferencediscussedinthe
previoussection.However,theKLtermin(39)alsodependson The following ancillary files are provided as supplementary
trialdistributionsoverstates,whichmeansthataseparationinto material:
actionandstatevariablesanalogoustotheseparationinmodel
selectionisnotpossiblewhenconsideringtheproblemofaction
S.1. Notebook: ComparisonofdifferentformulationsofActive
andperceptionastheminimizationofasinglefreeenergyfunc-
Inference
tional,whichisusuallytheconceptualstartingpointintheActive
InferenceliteratureFriston(2010,2018). AdetailedcomparisonofthedifferentformulationsofActive
Inference found in the literature (2013-2018), including their
Moreprecisely,asdiscussedinSection5,thereferencefunc-
mean-fieldandexactsolutionsinthegeneralcaseofarbitrary
tionφthatentersthevariationalfreeenergyinQ-valueActive
manytimesteps.
Inferenceisconstructedfromagivenprobabilisticmodelp and
0
avaluefunctionQbyreplacingthefixedpriorp (A)overac-
0
tionswiththemodifieddistributionp˜ (A) := 1 p (A)eQ(A). S.2. Notebook:Gridworldsimulations
0 Z 0
Ascanbeseenexemplarilyintheone-stepcaseinEquation(26), Weprovideimplementationsofthemodelsdiscussedinthis
thevaluefunctionQdependsontrialdistributionsqoverhid- articleinagridworldenvironment,bothasarenderedhtmlfile
denstatesandthereforep˜ 0 (A)dependsonqaswell.Despitethis aswellasajupyternotebookthatisavailableongithub.
dependency,thetotalfreeenergyF(qkφ)canstillbewrittenas
(cid:28) q(X0,S|A)q(A) (cid:29)
F(qkφ)= log
p (x,X0,S|A)p˜ (A)
0 0 q
(cid:0) (cid:1)
=hF i +D q(A)kp˜ (A) (41)
A q(A) KL 0
with
F
A
:=F (cid:0) q(X0,S|A) (cid:13) (cid:13)p
0
(x,X0,S|A) (cid:1) . (42)
inthecaseoftheone-stepexampleofSection5.2.Equations(41)