arXiv:2108.12245v1  [cs.LG]  27 Aug 2021
Active Inference for Stochastic Control
Aswin Paul 1,2,3, Noor Sajid 4, Manoj Gopalkrishnan 2, and Adeel Razi 3,4,5,6
1 IITB-Monash Research Academy , Mumbai, India
2 Department of Electrical Engineering, IIT Bombay , Mumbai, India
3 Turner Institute for Brain and Mental Health, Monash Univer sity , Australia
4 W ellcome Trust Centre for Human Neuroimaging, UCL, United K ingdom
5 Monash Biomedical Imaging, Monash University , Australia
6 CIF AR Azrieli Global Scholars Program, CIF AR, T oronto, Can ada
Abstract. Active inference has emerged as an alternative approach to c ontrol
problems given its intuitive (probabilistic) formalism. H owever, despite its the-
oretical utility , computational implementations have lar gely been restricted to
low-dimensional, deterministic settings. This paper high lights that this is a conse-
quence of the inability to adequately model stochastic tran sition dynamics, partic-
ularly when an extensive policy (i.e., action trajectory) s pace must be evaluated
during planning. Fortunately , recent advancements propos e a modiﬁed planning
algorithm for ﬁnite temporal horizons. W e build upon this wo rk to assess the util-
ity of active inference for a stochastic control setting. Fo r this, we simulate the
classic windy grid-world task with additional complexitie s, namely: 1) environ-
ment stochasticity; 2) learning of transition dynamics; and 3) partial observabil-
ity . Our results demonstrate the advantage of using active i nference, compared to
reinforcement learning, in both deterministic and stochas tic settings.
Keywords: Active inference · Optimal control · Stochastic control · So phisti-
cated inference
1 Introduction
Active inference, a corollary of the free energy principle, is a formal way of describ-
ing the behaviour of self-organising systems that interfac e with the external world and
maintain a consistent form over time [1,2,3]. Despite its ro ots in neuroscience, active in-
ference has snowballed to many ﬁelds owing to its ambitious s cope as a general theory
of behaviour [4,5,6]. Optimal control is one such ﬁeld, and s everal recent results place
active inference as a promising optimal control algorithm [ 7,8,9]. However, research
in the area has largely been restricted to low-dimensional a nd deterministic settings
where deﬁning, and evaluating, policies (i.e., action traj ectories) is feasible [9]. This
follows from the active inference process theory that neces sitates equipping agents a
priori with sequences of actions in time. For example, with 8 available actions and a
time-horizon of 15, the total number of (deﬁnable) policies that would need to b e con-
sidered → 3.5 × 1013.
This becomes more of a challenge in stochastic environments with inherently un-
certain transition dynamics, and no clear way to constrain t he large policy space to a
2 A. Paul et al.
smaller subspace. Happily, recent advancements like sophi sticated inference [10] pro-
pose a modiﬁed planning approach for ﬁnite-temporal horizo ns [11]. Brieﬂy, sophisti-
cated inference [10], compared to the earlier formulation [ 12,9], provides a recursive
form of the expected free energy that implements a deep tree s earch over actions (and
outcomes) in the future. W e reserve further details for Sect ion 3.2.
In this paper, we evaluate the utility of active inference fo r stochastic control us-
ing the sophisticated planning objective. For this, we util ise the windy grid-world task
[13], and assess our agent’s performance when varying level s of complexity are intro-
duced e.g., stochastic wind, partial observability, and le arning the transition dynamics.
Through these numerical simulations, we demonstrate that a ctive inference, compared
to a Q-learning agent [13], provides a promising approach fo r stochastic control.
2 Stochastic control in a windy grid-world
In this section, we describe the windy grid-world task, with additional complexity, used
for evaluating our active inference agent (Section 3). This is a classic grid-world task
from reinforcement learning [13], with a predeﬁned start ( S) and goal ( G) states (Fig. 1).
The aim is to navigate as optimally (i.e., within a minimum ti me horizon) as possible,
taking into account the effect of the wind along the way. The w ind runs upward through
the middle of the grid, and the goal state is located in one suc h column. The strength
of the wind is noted under each column in Fig. 1, and its amplit ude is quantiﬁed by
the number of columns shifted upwards that were unintended b y the agent. Here, the
agent controls its movement through 8 available actions (i.e., the King’s moves): North
(N), South ( S), East ( E), W est ( W ), North-W est ( NW ), South-W est ( SW ), South-
East ( SE), and North-East ( NE ). Every episode terminates either at the allowed time
horizon, or when the agent reaches the goal state.
2.1 Grid-world complexity
T o test the performance of our active inference agent in a com plex stochastic environ-
ment, we introduced different complexity levels to the wind y grid-world setting (T a-
ble 1).
T able 1: Five complexity levels for the windy grid-world tas k
Transition
Level Wind Observability Dynamics
1 Deterministic Full (MDP) Known
2 Stochastic Full (MDP) Known
3 Deterministic Full (MDP) Learned
4 Stochastic Full (MDP) Learned
5 Stochastic Partial (POMDP) Known
Active Inference for Stochastic Control 3
Fig. 1: Windy grid-world task. Here, S and G denote starting and goal locations.
On the x-axis, the wind amplitude is shown. This is quantiﬁed as the number of
unintended additional columns the agent moves during each a ction e.g., any ac-
tion in column four results in one unintended shift upwards. There are 8 actions:
N, S, E, W, NW, SW, SE, NE . W e plot sample paths from the start to the goal state
in light and dark blue. Notice, the indirect journey to the go al is a consequence of the
wind.
Wind properties In a deterministic setting, the amplitude of the wind remain s constant.
Conversely, in stochastic setting, for windy columns the ef fect varies by one from the
mean values. W e consider two settings: medium and high stoch asticity. For medium
stochasticity, the mean value is observed 70% of the time and similarly 40% of the
time in the high stochastic case (T able 2). The adjacent wind values are observed with
remaining probabilities. Here, stochasticity is not exter nally introduced to the system,
but it is inbuilt in the transition dynamics B (Section 3) of the environment.
T able 2: Stochastic nature of wind
Level Wind amplitude static Wind amplitude ± 1
Medium 70% of the time 15% each for ± 1
High 40% of the time 30% each for ± 1
Observability In the fully observable setting, the agent is aware of the cur rent state
i.e., there is no ambiguity about the states of affair. W e for malise this as a Markov
decision processes (MDP). Whereas in the partially observa ble environment, the agent
measures an indirect function of the associated state i.e., current observation. This is
used to infer the current state of the agent. W e formalise thi s as a partially observable
MDP (POMDP). Speciﬁc details of outcome modalities used in t he task are discussed
in Appendix B.
4 A. Paul et al.
T ransition dynamics known to agent In the known set-up, the agent is equipped
with the transition probabilities beforehand. However, if these are not known, the agent
begins the trials with a uninformative (uniform) priors and updates its beliefs (Eq.9)
using random transitions. Brieﬂy, random actions are sampl ed and transition dynamics
updated to reﬂect the best explanation for the observations at hand. Here, the learned
dynamics are used for planning.
3 Active inference on ﬁnite temporal horizons
3.1 Generative model
The generative model is formally deﬁned as a tuple of ﬁnite se ts (S, O, T, U, B, C, A ):
◦ s ∈ S : states where S = {1, 2, 3, ..., 70} and s1 is a predeﬁned (ﬁxed) start state.
◦ o ∈ O : where o = s, in the fully observable setting, and in partial observabil ity
o = f(s)7 .
◦ T ∈ N+, and is a ﬁnite time horizon available per episode.
◦ a ∈ U : actions, where U = {N, S, E, W, NW, SW, SE, NE }.
◦ B : encodes the transition dynamics, P (st|st− 1, at− 1, B) i.e., the probability that
action at− 1 taken at state st− 1 at time t − 1 results in st at time t.
◦ C : prior preferences over outcomes, P (o|C). Here, C preference for the predeﬁned
goal-state.
◦ A : encodes the likelihood distribution, P (oτ |sτ , A) for the partially observable
setting.
Accordingly, the agents generative model is deﬁned as the fo llowing probability
distribution:
P (o1:T , s1:T , a1:T − 1, A, B, C) = (1)
P (A)P (B)P (C)P (s1)
T∏
τ =2
P (sτ |sτ − 1, aτ − 1, B)
T∏
τ =1
P (oτ |sτ , A) (2)
3.2 Full observability
Perception: During full observability, states can be directly accessed by agent with
known or learned transition dynamics. Then the posterior es timates, Q(sτ +1|aτ , sτ ),
can be directly calculated from B [11].
Q(sτ +1|aτ , sτ ) =P (sτ +1|aτ , sτ , B). (3)
7 Here, outcomes introduce ambiguity for the agent as similar outcomes map to different (hid-
den) states. See Appendix B, T able B.1 for implementation de tails.
Active Inference for Stochastic Control 5
Planning: In active inference, expected free-energy ( G) [9] is used for planning. For
ﬁnite temporal horizons, the agent acts to minimise G [11]. Here, to calculate G we
using the recursive formulation introduced in [10]. This is deﬁned recursively as the
immediate expected free energy plus the expected free energ y for future actions:
G(aτ |sτ ) =G(aT − 1|sT − 1) =DKL [Q(sT |aT − 1, sT − 1)||C(sT )] (4)
for τ = T − 1 and,
G(aτ |sτ ) =DKL [Q(sτ +1|aτ , sT − 1)||C(sτ +1)] + EQ
[
G(nextstep)
]
(5)
for τ = 1, ..., T − 2. In Eq.5, the second term is calculated as,
EQ
[
G(nextstep)
]
= EQ(aτ +1,sτ +1|sτ ,aτ )[G(aτ +1|sτ +1)]. (6)
Prior preference over states are encoded such that the agent prefers to observe itself
in the goal state at every time-step. C(o = goal) = 1, and 0 otherwise. In the matrix
form, the ith element of C, corresponds to ith state in S.
Action selection: A distribution for action selection Q(aτ |sτ ) > 0 is deﬁned using
expected free energy such that,
Q(aτ |sτ ) =σ (−G (U|sτ )) . (7)
Here, σ is the softmax function ensuring that components sum to one. At each time-step,
actions are samples from:
at ∼ Q(at|st). (8)
Learning transition dynamics: W e learn the transition dynamics, B, across time using
conjugacy update rules [14,12,9]:
ba = ba +
t∑
τ =2
∑
aǫU
δa,aτ Q(a) (sa,τ ⊗ sa,τ − 1) . (9)
Here, ba ∼ Dir(b; α) is the learned transition dynamics updated over time, Q(a)
is the probability of taking action a, sa,τ is the state at time τ as a consequence of
action a, sa,τ − 1 is the state-vector at time τ − 1 taking action a, and ⊗ is the Kronecker-
product of the corresponding state-vectors. Furthermore, we also assessed the model
accuracy obtained after a given number of trials to update B, when random actions
were employed to explore transition dynamics. These learne d transitions were used for
control in Level-3 and Level-4 of the problem.
3.3 Partial observability
W e formalise partial observability as a partially observed MDP (POMDP). Here, the
agents have access to indirect observations about the envir onment. Speciﬁc details of
6 A. Paul et al.
outcome modalities used in this work are discussed in Append ix B. These outcome
modalities are same for many states for e.g., the states 2 and 11 have the same outcome
modalities (see Appendix B, T able B.1). Here, we evaluate th e ability of active infer-
ence agent to perform optimal inference and planning in the f ace of ambiguity. The
critical advancement with sophisticated inference [10] co mpared to the classical for-
mulation [9] allows us to perform deep-tree search for actio ns in the future. The agent
infers the hidden-states by minimising a functional of its p redictive distribution (gen-
erative model) of the environment called the variational fr ee-energy. This predictive
distribution can be deﬁned as,
Q(⃗ s|⃗ a,˜o) :=
T∏
τ =1
Q(sτ |aτ − 1, sτ − 1, ˜o). (10)
T o infer hidden-states from partial observations, thr agen t engages in minimising
variational free energy ( F) functional of Q using variational (Bayesian) inference. For
a rigorous treatment of it, please refer to [10,11]. In this s cheme, actions are considered
as random variables at each time-step, assuming successive actions are conditionally in-
dependent. This comes with a cost of having to consider many a ction sequences in time.
The search for policies in time is optimised both by restrict ing the search over future
outcomes which has a non-trivial posterior probability (Eg : > 1/16) as well as only
evaluating policies with signiﬁcant prior probabilities ( Eg: > 1/16) calculated from
the expected free energy (i.e., Occam’s window). In the part ially observable setting, the
expected free energy accommodates ambiguity in future obse rvations prioritising both
preference seeking as well as ambiguity reduction in observ ations [10].
4 Results
W e compare the performance of our active inference agent wit h a popular reinforcement
learning algorithm, Q-learning [13], in Level 1. Q-Learning is a model-free RL algo-
rithm that operates by learning the ’value’ of actions at a pa rticular state. It is well suited
for problems with stochastic transitions and reward dynami cs due to its model-free pa-
rameterisation. Q-Learning agents are extensively used in similar problem settings and
exhibit state-of-the-art (SOT A) performances [13]. T o tra in the Q-learning agents, we
used an exploration rate of 0.1, learning rate of 0.5 and discount factor of 1. Training
was conducted using 10 different random seeds to ensure unbi ased results. The training
depth for Q-Learning agents were increased with complexity of the environment.
W e instantiate two Q-learning agents, one trained for 500 time-steps (QLearning500)
and another for 5000 time-steps (QLearning5K) in Level-1. Both the active infer ence
agent and the QLearning5K agent demonstrate optimal succes s rate for the time-horizon
T = 8+(see Appendix A, Fig.A.1).
Using these baselines from the deterministic environment w ith known transition dy-
namics, we compared the performance of the agent in a complex setting with medium
and highly stochastic wind (Level 2; T able. 2).Here, the active inference agent is clearly
superior against the Q-Learning agents (Fig. 2 top row). Mor eover, they demonstrate
Active Inference for Stochastic Control 7
better success rates for shorter time-horizons, and ’optim al’ action selection. Note, suc-
cess rate is the percentage of trials for which the agent succ essfully reached the goal
within the allowed time-horizon.
Next, we considered how learning the transition dynamics im pacted agent behaviour
(Level 3 and 4). Here, we used Eq. 9 for learning the transition dynamics, B. First, the
algorithm learnt the dynamics by taking random actions over X steps (for example,
X is 5000 time steps in ’SophAgent (5K B-updates)’, see Fig. 2 middle r ow). These
learned transition dynamics B were used (see Fig. 3) by the active inference agent to es-
timate the action distribution in Eq. 8. Results for level 3 are presented in Appendix A,
Fig. A.2. Here, the Q-Learning algorithm with 5, 000 learning steps shows superior
performance to the active inference agents. However with lo nger time horizons, the ac-
tive inference agent shows competitive performance. Impor tantly, the active inference
agent used self-learned, and imprecise transition dynamic s B in these levels. Level 4
results for medium and highly stochastic setting are presen ted in Fig. 2 (middle row).
For medium stochasticity, the QLearning10K exhibited sati sfactory performance, how-
ever it failed with zero success rate in the highly stochasti c case. This shows the need
for extensive training for algorithms like Q-Learning in hi ghly stochastic environments.
However, the active inference agent demonstrated at-par pe rformance. Remarkably, the
performance was achieved using imprecise (compared to true -model), self-learned tran-
sition dynamics ( B) (see Fig. 3).
The active inference agent shows superior performance in th e highly stochastic en-
vironment even with partial observability (Fig. 2, last row ). Conversely, excessive train-
ing was required for the Q-Learning agent to achieve a high su ccess rate in a medium
stochastic environment, but even this training depth led to a zero success rate with high
stochasticity. These results present active inference, wi th a recursively calculated free-
energy, as a promising algorithm for stochastic control.
5 Discussion
W e explored the utility of the active inference with plannin g in ﬁnite temporal-horizons
for ﬁve complexity levels of the windy grid-world task. Acti ve inference agents per-
formed at-par, or superior, when compared with well-traine d Q-Learning agents. Im-
portantly, in the highly stochastic environments the activ e inference agent showed clear
superiority over the Q-Learning agents. The higher success rates at lower time horizons
demonstrated the ’optimality’ of actions in stochastic env ironments presented to the
agent. Additionally, this performance is obtained with no s peciﬁcations of acceptable
policies. The total number of acceptable policies scale exp onentially with the number of
available actions and time-horizon. Moreover, the Level 4 & 5results demonstrate the
need for extensive training for the Q-Learning agents when o perating in stochastic en-
vironments. W e also demonstrated the ability of the active i nference agents to achieve
high success rate even with self-learned, but sub-optimal, transition dynamics. Meth-
ods to equip the agent to learn both transition-dynamics B and outcome-dynamics A
for a partially observable setting have been previously exp lored [14,9]. For a stochastic
setting, we leave their implementation for future work.
8 A. Paul et al.
5 10 15 20 25
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 104 trials
Medium stochastic case (Level 2)
SophAgent
QLearning5K
QLearning20K
5 10 15 20 25 30
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 104 trials
High stochastic case (Level 2)
SophAgent
QLearning5K
QLearning20K
5 10 15 20 25
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 103 trials
Medium stochastic case (Level 4)
SophAgent (5K B-updates)
SophAgent (10K B-updates)
QLearning10K
5 10 15 20 25 30
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 104 trials
High stochastic case (Level 4)
SophAgent (5K B-updates)
SophAgent (10K B-updates)
QLearning10K
QLearning20K
2 3 4 5 6 7 8 9 10
Time-horizon
0.0
0.1
0.2
0.3
0.4
0.5Success rate over 103 trials
Medium stochastic case (Level 5)
SophAgent
QLearning5K
QLearning10K
2 3 4 5 6 7 8 9 10
Time-horizon
0.00
0.05
0.10
0.15
0.20
0.25Success rate over 103 trials
High stochastic case (Level 5)
SophAgent
QLearning10K
QLearning20K
Fig. 2: Stochastic environments: Performance comparison o f agents in Level-2 (top
row), Level-4 (middle row), and Level-5 (last row) of windy g rid-world task for
medium-stochastic (left column) and high-stochastic (rig ht column) environments, re-
spectively. Here, x-axis denotes time horizon and y-axis th e success rate over multiple-
trials. ’SophAgent’ represents the active inference agent , ’QLearning5K’ represents Q-
learning agent trained for 5, 000 time-steps, ’QLearning10K’ for the ’Q-learning agent
trained for 10, 000 time-steps, and ’QLearning20K’ for the Q-learning agent tr ained for
20, 000 time-steps. Each agent was trained using 10 different random seeds. ’SophA-
gent (5K B-updates)’ and SophAgent (10K B-updates) refers t o active inference agent
using self-learned transition dynamics B with 5000 and 10000 updates respectively.
Active Inference for Stochastic Control 9
5 10 15 20 25
Time-horizon
10
12
14
16
18
20Model deviation  (||BLearned − Btrue||)
Model deviation (Medium Stochastic case)
Model deviation 5K B -updates
Model deviation 10K B-updates
5 10 15 20 25 30
Time-horizon
12
14
16
18
20Model deviation  (||BLearned − Btrue||)
Model devia ion (High S ochas ic case)
Model devia ion 5K B-updates
Model  deviation 10K B-updates
Fig. 3: Accuracy of learned dynamics in terms of deviation fr om true transition dynam-
ics in Level-4 A: Medium stochastic case B: High stochastic c ase
The limitation yet to be addressed is the time consumed for tr ials in active infer-
ence. Large run-time restricted analysis for longer time ho rizons in Level 5. Deep learn-
ing approaches using tree searches, for representing polic ies were proposed recently
[15,16,17], may be useful in this setting. W e leave run-time analysis and optimisation
for more ambitious environments for future work. Also, comp aring active inference to
model based RL algorithms like Dyna-Q [13] and control as inf erence approaches [18]
is a promising direction to pursue.
W e conclude that the above results place active inference as a promising algorithm for
stochastic-control.
Software note The environments and agents were custom written in Python fo r fully
observable settings. The script ’SPM_MDP_VB_XX.m’ availa ble in SPM12 package
was used in the partially observable setting. All scripts ar e available in the following
link: https://github.com/aswinpaul/iwai2021_aisc.
Acknowledgments AP acknowledges research sponsorship from IITB-Monash Res earch Academy , Mumbai and
Department of Biotechnology , Government of India. AR is fun ded by the Australian Research Council (Refs: DE170100128
& DP200100757) and Australian National Health and Medical R esearch Council Investigator Grant (Ref: 1194910). AR is a
CIF AR Azrieli Global Scholar in the Brain, Mind & Consciousn ess Program. AR and NS are afﬁliated with The W ellcome
Centre for Human Neuroimaging supported by core funding fro m W ellcome [203147/Z/16/Z].
References
1. Friston, K.: The free-energy principle: a uniﬁed brain th eory?. Nat Rev Neuroscience 11,
127–138 (2010).
2. Kaplan, Raphael and Friston, Karl J: Planning and navigat ion as active inference. Biological
cybernetics 112(4), 323–343 (2018).
3. Kuchling, Franz and Friston, Karl and Georgiev , Georgi an d Levin: Morphogenesis as
Bayesian inference: A variational approach to pattern form ation and control in complex bi-
ological systems. Physics of life reviews, (2019)
4. Oliver, Guillermo and Lanillos, Pablo and Cheng, Gordon: Active inference body perception
and action for humanoid robots. arXiv preprint arXiv:1906. 03022, (2019)
10 A. Paul et al.
5. Rubin, Sergio and Parr, Thomas and Da Costa, Lancelot and F riston, Karl: Future climates:
Markov blankets and active inference in the biosphere. Jour nal of the Royal Society Interface
17(172), (2020)
6. Deane, George and Miller, Mark and Wilkinson, Sam: Losing Ourselves: Active Inference,
Depersonalization, and Meditation. Frontiers in Psycholo gy , (2020)
7. Friston KJ, Daunizeau J, Kiebel SJ.: Reinforcement Learn ing or Active Inference? PLoS ONE
4(7): e6421, (2009) https://doi.org/10.1371/journal.pone.0006421
8. Friston, Karl and Samothrakis, Spyridon and Montague, Re ad: Active inference and agency:
optimal control without cost functions. Biological cybern etics 106(8), 523-541 (2012)
9. Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston.: Active Inference: Demystiﬁed and
Compared. Neural Computation 33 (3), 674–712 (2021)
10. Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, Thomas Parr: Sophisticated
Inference. Neural Comput 2021; 33 (3), 713–763 (2021).
11. Lancelot Da Costa and Noor Sajid and Thomas Parr and Karl F riston and Ryan Smith:, The
relationship between dynamic programming and active infer ence: the discrete, ﬁnite-horizon
case.:, arXiv .2009.08111, (2020).
12. Da Costa, L., Parr, T ., Sajid, N., V eselic, S., Neacsu, V . , and Friston, K.: Active inference on
discrete state-spaces: a synthesis”, arXiv e-prints, (202 0).
13. Sutton, R., Barto, A.: Reinforcement Learning: An Intro duction. MIT Press (2018).
14. Friston, Karl and FitzGerald, Thomas and Rigoli, France sco and Schwartenbeck, Philipp and
Pezzulo, Giovanni: Active inference: a process theory . Neu ral computation 29(1), 1–49 (2017)
15. Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro AM and Friston, Karl: Deep active
inference agents using Monte-Carlo methods. arXiv preprin t arXiv:2006.04176, (2020)
16. Çatal, Ozan and Nauta, Johannes and V erbelen, Tim and Sim oens, Pieter and Dhoedt, Bart:
Bayesian policy selection using active inference. arXiv pr eprint arXiv:1904.08149, (2019)
17. van der Himst, Otto Lanillos, P .: Deep Active Inference f or Partially Observable MDPs.
In: V erbelen, Tim and Lanillos, Pablo and Buckley , Christop her L. and De Boom,
Cedric (eds.), Active Inference, pp. 61–71, Springer Inter national Publishing (2020).
https://doi.org/10.1007/978-3-030-64919-7
18. Millidge, Berenand Tschantz, Alexanderand Seth, Anil K . and Buckley , Christopher L.: On
the Relationship Between Active Inference and Control as In ference. In: V erbelen, Tim and
Lanillos, Pablo and Buckley , Christopher L. and De Boom, Ced ric (eds.), Active Inference, pp.
3–11, Springer International Publishing (2020). https:// doi.org/10.1007/978-3-030-64919-7
Active Inference for Stochastic Control 11
Supplementary information
A Results Level-1 and Level-3 (Non-stochastic settings)
2 4 6 8 10 12 14
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 105 trials
Results Level-1
QLearning500
SophAgent
2 4 6 8 10 12 14
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0
Results Level-1
RandomAgent
QLearning5K
Fig. A.1: Performance comparison of agents in Level-1 of win dy grid-world task. ’Ran-
domAgent’ refers to a naive-agent that takes all actions wit h equal probability at every
time step.
2 4 6 8 10 12 14
Time-horizon
0.0
0.2
0.4
0.6
0.8
1.0Success rate over 103 trials
Determinstic case (Level 3)
SophAgent (5K B-updates)
SophAgent (10K B-Updates)
QLearning5K
2 4 6 8 10 12 14
Time-horizon
12
14
16
18
20
22Model deviation  (||BLearned − Btrue||)
Model de iation (Non-Stochastic case)
Model deviation 5K B-updates
Model deviation 10K B-updates
Fig. A.2: A: Performance comparison of active inference age nts with learned B using
5000 and 10000 updates respectively to Q-Learning agent in L evel-3. ’Q-Learning5K’
stands for Q-Learning agent trained for 5000 time steps using 10 different random seeds.
B: Accuracy of learned dynamics in terms of deviation from tr ue dynamics.
12 A. Paul et al.
B Outcome modalities for POMDPs
In the partially observable setting, we considered two outc ome modalities and both of
them were the function of ’side’ and ’down’ coordinates deﬁn ed for every state in Fig.
1. Examples of the coordinates and modalities are given belo w . First outcome modality
is the sum of co-ordinates and second modality is the product of coordinates.
T able B.1: Outcome modalities speciﬁcations
State Down
coordinate (C1)
Side
coordinate (C2)
Outcome-1
(C1+C2)
Outcome-2
(C1*C2)
1 1 1 2 1
2 1 2 3 2
. . . . .
11 2 1 3 2
. . . . .
31 4 1 5 4
38 4 8 12 32
. . . . .
These outcome modalities are similar for many states (for e. g., states 2 and 11 have
the same outcome modalities (see T ab. B.1)). The results dem onstrates the ability of ac-
tive inference agent to perform optimal inference and plann ing in the face of ambiguity.
One of the output from ’SPM_MDP_VB_XX.m’ is ’MDP .P’. ’MDP .P ’ returns the ac-
tion probabilities an agent will use for a given POMDP as inpu t at each time-step. This
distribution was used to conduct multiple trails to evaluat e success rate of the active
inference agent.