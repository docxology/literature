arXiv:2312.00088v1  [cs.LG]  30 Nov 2023
IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024 1
Anomaly Detection via Learning-Based
Sequential Controlled Sensing
Geethu Joseph, Chen Zhong, M. Cenk Gursoy , Senem V elipasalar, and
Pramod K. Varshney Life Fellow, IEEE
Process 1
Process 2
.
.
.
Process N
Joint distribution
Environment
Our learning
algorithm
Anomalous
processes’ indices
Noisy sensor
measurements
Dynamic
sensor selection
Output
Abstract — In this paper, we address the problem of
detecting anomalies among a given set of binary pro-
cesses via learning-based controlled sensing. Each pro-
cess is parameterized by a binary random variable in-
dicating whether the process is anomalous. T o identify
the anomalies, the decision-making agent is allowed to
observe a subset of the processes at each time instant.
Also, probing each process has an associated cost. Our
objective is to design a sequential selection policy that
dynamically determines which processes to observe at
each time with the goal to minimize the delay in mak-
ing the decision and the total sensing cost. We cast
this problem as a sequential hypothesis testing problem
within the framework of Markov decision processes. This
formulation utilizes both a Bayesian log-likelihood ratio -based reward and an entropy-based reward. The problem is th en
solved using two approaches: 1) a deep reinforcement learni ng-based approach where we design both deep Q-learning
and policy gradient actor-critic algorithms; and 2) a deep a ctive inference-based approach. Using numerical experime nts,
we demonstrate the efﬁcacy of our algorithms and show that ou r algorithms adapt to any unknown statistical dependence
pattern of the processes.
Index Terms — Active hypothesis testing, anomaly detection, active infe rence, quickest state estimation, sequential
decision-making, sequential sensing.
I. I NT RODUCT ION
Sequential controlled sensing refers to a stochastic frame-
work in which an agent sequentially controls the process
of acquiring observations. The goal here is to minimize the
cost of making observations while satisfying the inference
objectives. W e consider a sequential controlled sensing pr ob-
lem in the context of anomaly detection wherein there are
N processes, each of which can be in either a normal or
an anomalous condition. Our goal is to identify the anoma-
lies among the given processes. T o this end, the decision-
making agent sequentially chooses a subset of processes (or
equivalently sensors monitoring these processes) at every time
instant, probes them, and obtain estimates of their conditi ons.
The agent generally obtains noisy observations, i.e., the o b-
This work was supported in part by the National Science Found ation
under grants ENG 60064237. The material in this paper was presented
in part at the IEEE International Workshop on Signal Process ing Ad-
vances in Wireless Communications, May 2020, Atlanta, GA, USA, and
the IEEE Global Communications Conference, December 2020, T aipei,
T aiwan.
G. Joseph is with the faculty of Electrical Engineering, Mat hematics,
and Computer Science, Delft T echnical University , 2628 XE, Nether-
lands (email: g.joseph@tudelft.nl).
C. Zhong, M. C. Gursoy , S. V elipasalar, and P . K.V arshney
are with the Department of Electrical and Computer
Engineering, Syracuse University , New Y ork, 13244, USA
(emails:{czhong03,mcgursoy ,svelipas,varshney}@syr.edu.)
served condition may get ﬂipped from the actual condition
with a certain probability. This paradigm is encountered in
many practical applications such as remote health monitori ng,
assembly lines, structural health monitoring and Internet of
Things (IoT). In such applications, the objective is to iden tify
the anomalies among a given set of different (not necessaril y
independent) functionalities of a system [1], [2]. Each sen sor
monitors a different functionality and sends observations to the
agent over a communication link. The received observation
may be distorted due to the unreliable nature of the sensor
hardware and/or the noisy link (e.g., a wireless channel)
between the sensor and the agent. Hence, the agent needs
to probe each process multiple times before declaring one or
more of the processes anomalous with the desired conﬁdence.
Repeatedly probing all the processes allows the agent to ﬁnd
any potential system malfunction or anomaly quickly, but th is
incurs a higher energy consumption that reduces the life spa n
of the network. Therefore, we address the question of how
the agent must sequentially choose the subset of processes t o
accurately detect the anomalies while minimizing the delay
and cost of making observations.
W e start with a brief literature review . A classical approac h
to solve the sequential process selection problem for anoma ly
detection is based on the active hypothesis testing frame-
work [3], [4]. Here, the decision-making agent constructs a
2 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
hypothesis corresponding to each of the possible condition s
of the processes and determines which one of these hypothese s
is true. The goal of active hypothesis testing is to infer
the true hypothesis by collecting relevant data sequential ly
until sufﬁciently strong evidence is gathered. In [5], Cher noff
proposed a randomized strategy and established its asympto tic
optimality. This seminal work in [5] was followed by several
other studies that investigated active hypothesis testing un-
der different settings [6]–[10]. These studies investigat ed the
theoretical aspects of the problem and presented a few model -
based algorithms to solve the problem. However, these model -
based algorithms are designed under simpliﬁed modeling
assumptions. This has motivated the researchers to design
data-driven deep learning algorithms for active hypothesi s test-
ing [3], [4], [11]–[15]. These algorithms are not only model
free and thus, more ﬂexible than the traditional algorithms ,
but they also possess reduced computational complexity. It
should be noted that the classical sequential hypothesis te sting
framework does not incorporate the sensing cost in the detec -
tion problem and assumes that the decision-maker chooses
the same ﬁxed number of processes at every time instant. Our
problem setting is different from these models. Speciﬁcall y,
the decision-maker can choose any number of processes at
each time instant, and this choice is determined by the cost
associated with the observations. W e also account for the
potential statistical dependence among the processes.
Our anomaly detection problem is different from the se-
quential parameter estimation. Sequential parameter esti ma-
tion refers to estimating the random parameter of a pro-
cess [16]–[18]. Although this goal is similar to ours, the co n-
trolled sequential selection of processes makes our proble m
fundamentally different from sequential parameter estima tion.
In particular, only one Bernoulli process is considered in [ 16]–
[18], and at every time instant, the decision-maker only
decides whether or not to continue collecting observations .
Hence, the results in these studies apply to our setting only
if we consider the set of all N processes as a single random
process with 2N states and choose the sub-optimal strategy of
observing all the processes all the time.
A. Our Contributions
T o the best of our knowledge, ours is the ﬁrst work
that formulated the anomaly detection problem as an active
hypothesis testing problem and developed speciﬁc solution s
for the problem. Our speciﬁc contributions are as follows:
• F ormulation of anomaly detection as a Markov decision
process: In Section III, we formulate the anomaly de-
tection problem as a Markov decision process (MDP).
W e deﬁne the posterior belief vector on the conditions
of the processes as the state of the MDP , the subset of
processes chosen by the decision-making agent as the
action, and two different types of reward functions: an av-
erage Bayesian log-likelihood ratio (LLR) based reward
and an entropy-based reward. The rewards are designed
such that the optimal policy of the MDP minimize the
sensing cost and the delay in decision-making. Finally,
the process/sensor selection is formulated as the problem
of ﬁnding an optimal policy that maximizes the long-
term reward of the MDP subject to the condition that the
conﬁdence level on the estimate exceeds a speciﬁc value.
• RL algorithms: In Section IV, we develop the deep
RL framework through which an optimal policy that
maximizes the discounted cumulative reward is learned.
W e develop two deep RL algorithms, based on the Q-
learning and actor-critic frameworks.
• Active inference: In Section V, we present an alternative
solution strategy called active inference. Here, we deﬁne
the notion of free-energy based on the entropy associated
with the estimate of the states of the processes and
the sensing cost and reformulate the anomaly detection
problem as an active inference problem to minimize the
free energy. The resulting algorithm is implemented using
deep neural networks which are relatively less explored
for active inference.
• Empirical validation: V ia our numerical results presented
in Section VI, we investigate the performance of different
frameworks and algorithms in terms of detection accu-
racy, delay, and sensing cost. W e show that the active
inference algorithm is more robust to the variations in
the system parameters and adapts better to statistical
dependence among the processes. Further, we observe
that as the statistical dependence between the states of
the processes increases, the delay in state estimation gets
diminished. This result implies that unlike the traditiona l
Chernoff test, our algorithms are able to learn and
exploit any underlying statistical dependence among the
processes to reduce the number of observations.
In summary, we use the model-based posterior updates to
tackle the uncertainties in the observations and the data-
driven neural networks to handle the underlying statistica l
dependence between the processes, balancing the model-bas ed
and the data-driven approaches.
Furthermore, in this paper, compared to the conference ver-
sions, we conduct a more comprehensive and uniﬁed analysis
of deep learning-based anomaly detection and make several
new contributions: 1) W e design a new RL algorithm based on
the deep Q-learning algorithm which we implement using the
dueling architecture; 2) in addition to the LLR-based rewar d,
we introduce an entropy-based reward (newly applied in deep
RL algorithms), and we mathematically show that the two
reward functions encourage the agent to achieve the desired
conﬁdence level as quickly as possible (see Proposition 1);
3) we derive the Chernoff test for the anomaly detection
problem and compare its performance with our algorithms;
4) we present a detailed numerical study that compares the
different algorithms when the cost and ﬂipping probabiliti es
are different across the processes.
The remainder of the paper is organized as follows. W e
present the system model in Section II and describe the MDP
problem in Section III. In Sections IV and V, we present our
RL and active inference algorithms, respectively. W e provi de
the simulation results in Section VI and offer our concludin g
remarks in Section VII.
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 3
II. A NOMAL Y DET ECT ION PROBLEM
W e consider a set of N processes wherein each process
is in one of the two conditions: normal (denoted by 0) or
anomalous (denoted by 1). The condition of the ith process is
denoted by the ith entry xi of a random vector x ∈ { 0, 1}N .
The vector x can take M ≜ 2N possible values denoted by{
h(i), i = 1 , 2, . . . , M
}
. The conditions of these processes
(entries of x) can be potentially statistically dependent. This
dependence is captured by the prior distribution of x that is
denoted using π(0) ∈ [0, 1]M whose ith entry πi(0) is
πi(0) = P
{
x = h(i)
}
. (1)
Our goal is to identify the anomalous processes out of the
N processes, which is equivalent to estimating the random
vector x. T o estimate x, the decision-making agent probes one
or more processes at every time instant and obtains potentia lly
erroneous observations of the corresponding entries of x. Let
the set of processes probed at time t be A(t) ∈ P (N) where
P(N) denotes the power set of {1, 2, . . . , N } without the
null set (i.e., |P(N)| = M − 1). Also, let the observation
corresponding to the ith process at time t be denoted as
yi(t). Depending on the condition, yi(t) obeys the following
probabilistic model:
yi(t) =
{
xi with probability 1 − pi
1 − xi with probability pi, (2)
where pi ∈ [0, 0.5] is called the cross-over (or ﬂipping)
probability of the ith process. W e also assume that given x,
the observations are jointly (conditionally) independent :
P
[
{y(τ)}t
τ =1
⏐
⏐
⏐x
]
=
t∏
τ =1
N∏
k=1
P [yk(τ)|x] , (3)
for any t > 0 where y(τ) ∈ { 0, 1}N and its kth entry is
yk(τ). Further, probing the ith process incurs a cost of ci > 0.
In short, at every time instant t, the decision-maker probes
the processes indexed by A(k), obtains the corresponding
observations denoted by yA(t)(t) ∈ { 0, 1}|A(t)|, and incurs
a sensing cost of ∑
k∈A(t) ck.
In this setting, the three performance metrics associated
with the decision making are the following:
1) stopping time denoted by T which is the time instant
when the decision-maker ends the observation acquisition
phase and yields its estimate ˆx of x;
2) detection accuracy given by the conditional probability
P
[
ˆx = x
⏐
⏐
⏐
⏐
{
yA(t)(t)
}T
t=1
]
;
3) sensing cost given by ∑ T
t=1
∑
k∈A(t) ck which represents
the total cost incurred during the observation acquisition .
The strategy of probing all the processes at all times may
lead to the most accurate and fastest decision, but at the
expense of a higher sensing cost. Therefore, the decision-
maker sequentially chooses a subset of processes A(t) ∈
P(N) to balance the trade-offs between the three performance
metrics. Our decision-making algorithm has two components :
1) sequential process selection: a mechanism to choose
A(t) ∈ P (N) at every time t,
yA(1)(1)
A(1)
π(1)π(0)
yA(2)(2)
A(2)
π(2)
yA(T )(T )
A(T )
π(T )
. . .
. . .
. . .
x
Fig. 1: Graphical model with posteriors (MDP states), actions
and observations
2) stopping rule: a mechanism to determine when to stop
taking observations and declare ˆx.
W e next present a novel anomaly detection algorithm that
we derive by casting the detection as a learning problem usin g
an MDP framework.
III. MDP- BASED LEARNING PROBLEM FORMULAT ION
This section describes the MDP framework that models
the anomaly detection problem. An MDP represents a se-
quential decision making problem in stochastic environmen ts
wherein the state of the environment depends on the action
of a decision-maker. The goal of the decision-maker is to
sequentially decide which action to choose while in a given
state to maximize the reward which is the same as ﬁnding
a mapping from the states to the actions. This mapping is
referred to as a policy and it can be either deterministic (i. e.,
a one-to-one mapping) or stochastic (described by conditio nal
probability distributions over actions given the states).
A. State and Action
In the context of our anomaly detection problem, we deﬁne
the state of the MDP at time t as the posterior belief vector
π(t) on the random vector x ∈ { 0, 1}N . The ith entry of the
posterior belief vector π(t) ∈ [0, 1]M is deﬁned as
πi(t) = P
[
x = h(i)
⏐
⏐
⏐
⏐
{
yA(τ )(τ)
}t
τ =1
]
. (4)
Further, the action at time t refers to the set of processes to
be observed, A(t) ∈ P (N).
W e next establish the connections between the states, ac-
tions, and corresponding observations which can be repre-
sented using a probabilistic graphical model depicted in Fi g. 1.
W e ﬁrst note that at time t, the data available to the agent is{
yA(τ )(τ), τ = 1 , 2, . . . , k
}
using which the posterior belief
vector π(t) ∈ [0, 1]M can be computed in closed form. From
4 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
(4), πi(t) is computed recursively from (3) and (1) as
πi(t) =
πi(t − 1)
∏
k∈A(t)
P
[
yk(t)
⏐
⏐
⏐x = h(i)
]
M∑
j=1
πj (t − 1)
∏
k∈A(t)
P
[
yk(t)
⏐
⏐
⏐x = h(j)
] , (5)
where we obtain from (2) that
P
[
yk(t)
⏐
⏐
⏐x = h(i)
]
= (1 − pk)
/BD
{
yk(t)=h(i)
k
} + pk
/BD
{
yk(t)̸=h(i)
k
} , (6)
/BD is the indicator function, and h(i)
k ∈ { 0, 1} is the kth entry
of h(i). As a result, given the previous posterior π(t − 1),
the action A(t) and the observation yA(t), we can exactly
compute the updated posterior belief vector π(t) using (5).
Before we deﬁne the notion of reward, we recall from
Section II that our goal is to achieve a balance between the
detection accuracy, stopping time, and sensing cost. W e can
use the posteriors or the MDP states to control the detection
accuracy via the stopping rule using a parameter πupper ∈
(0, 1). The parameter πupper represents the threshold on the
largest value among the beliefs on the different values of x,
max
i=1, 2,...,m
πi(T ) ≥ πupper. (7)
Having deﬁned the stopping rule (one of the two components
of the algorithm), which controls the detection accuracy, w e
next focus on the trade-off between the stopping time and
sensing cost. This trade-off is determined by the sequentia l
process selection (the other algorithm component), which w e
derive using the notion of reward and policy.
B. Reward
The reward r(t) at time t is a function of the posterior
beliefs π(t) and π(t − 1), and the selected processes A(t).
The reward indicates the intrinsic desirability of choosin g the
subset of processes as a function of the posterior belief. Fo r
a given reward function, the policy µ : [0 , 1]M → P (N)
is a mapping from the posterior belief vector π(t − 1) to
the processes to be probed A(t). The policy represents the
sequential process selection part of our algorithm, and it i s
designed to maximize the long-term reward given as follows:
R(T ) =
T∑
t=1
E {r(t)} . (8)
Here, the expectation is over the uncertainty in the value of x
and the observations yA(t)(t) when A(t) follows the policy µ.
The reward function balances the trade-off between the
stopping time and sensing cost. Here, the sensing cost can be
quantiﬁed as a function of A(t) using the term ∑
k∈A(t) ck.
However, we also need a term in the reward which forces
the policy to build the posterior belief on the true value of
x as quickly as possible, and thus minimize the stopping
time T . W e represent this term using ξ : [0 , 1]M → R
which is a function of the posterior beliefs. W e seek ξ that
encourages the decision-maker to move away from the non-
informative posterior π(t) = 1 /M1 and move towards the
posterior π(t) ∈ { ei}M
i=1. Here, 1 denotes the all-ones vector
and ei ∈ { 0, 1}M denotes the ith column of the M × M
identity matrix. T wo functions that achieve this goal are gi ven
by the following proposition [19].
Proposition 1: Let L, H : [0 , 1]M → R be two functions
deﬁned, respectively, as
L(π) =
M∑
i=1
πi log πi
1 − πi
(9)
H(π) = −
M∑
i=1
πi log πi. (10)
These functions satisfy
arg min
π ∈[0, 1]M
∑ M
i=1 π i=1
L(π) = arg max
π ∈[0, 1]M
∑ M
i=1 π i=1
H(π) = 1
M 1
arg max
π ∈[0, 1]M
∑ M
i=1 π i=1
L(π) = arg min
π ∈[0, 1]M
∑ M
i=1 π i=1
H(π) = {ei}M
i=1 ,
where 1 denotes the all-ones vector and ei ∈ [0, 1]M denotes
the ith column of the M × M identity matrix.
Proof: From the log-sum inequality, we have
M∑
i=1
ai log
(ai
bi
)
≥
(M∑
i=1
ai
)
log
(∑ M
i=1 ai
∑ M
i=1 bi
)
,
for any set {ai ≥ 0, bi ≥ 0}M
i=1, and equality holds only if
ai = αbi, for some constant α > 0. Thus, for any π ∈ [0, 1]M ,
L(π) − L
(1
M 1
)
=
M∑
i=1
πi log πi(M − 1)
1 − πi
≥
(M∑
i=1
πi
)
log
( ∑ M
i=1 πi
∑ M
i=1
1−π i
M−1
)
= 0 .
Hence, L(π) ≥ L
(1
M 1
)
and equality holds only if πi = 1 /M.
W e next look at the maximum of L(π) and we see that if
πi ∈ [0, 1) for all values of i, L(π) < ∞. Therefore, L(π)
attains the maximum value if and only if at least one entry of
π is 1. Hence, the desired maxima is achieved at {ei}M
i=1.
W e can compute the maxima and minima of H(π) using
similar arguments and thus, the proof is complete.
The above proposition implies that the functions L and H
are two good choices for ξ. W e note that in (9), the term
log π i
1−π i
is the likelihood ratio (LLR) of the two hypotheses
namely, x = h(i) and x ̸= h(i). Consequently, L(π) is the
Bayesian LLR obtained by applying the logit function on
the posterior belief. Further, maximizing the Bayesian LLR
increases the posterior belief on the true value of x. Also, H
is the entropy of the distribution π, and thus, minimizing H
reduces the uncertainty in estimation and builds the poster ior
belief on the true value of x.
Having deﬁned the function 1 ξ, we formulate the instanta-
1 The algorithmic development is independent of the choice of the reward
function (LLR and entropy-based). Therefore, in the remain der of the paper,
we use ξ(·) in the reward of the MDP which can either be L(·) deﬁned in
(9) or − H(·) deﬁned in (10).
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 5
neous reward of the MDP as a weighted sum of ξ and the
sensing cost:
r(t) = ξ(π(t)) − ξ(π(t − 1)) + λ
∑
k∈A(t)
ck, (11)
where λ > 0 is the weighing parameter that dictates the
balance between the stopping time and the total sensing cost .
Thus, from (8) and (11), the long-term expected reward of the
MDP up to time t is given by
R(t) = E


ξ(π(t)) − ξ(π(0)) − λ
t∑
τ =1
∑
k∈A(τ )
ck


,
where the expectation is over the distribution of x and the
observations yA(t)(t) given A(t). The MDP objective is to
ﬁnd a policy or sequence of actions {A(t) ∈ P (N)}T
t=1 that
maximizes the long-term average sum of the rewards. Hence,
a policy that maximizes the long-term reward improves the
accuracy of the estimate (quantiﬁed by ξ(π(t))) as soon as
possible while minimizing the overall sensing cost (the las t
term in R(t)). Further, the agent continues to take observa-
tions until it declares an estimate with the desired level of
conﬁdence given by πupper (i.e., t = T ). Therefore, if λ
is small, the reward ensures that the agent chooses actions
with a signiﬁcant change ξ(π(t)) − ξ(π(0)), leading to a
shorter stopping time. On the other hand, with a large λ, the
agent tries to minimize the sensing cost by probing a few
processes at every time instant which increases the stoppin g
time. Therefore, λ controls the stopping time and total sensing
cost.
Further, the Bayesian LLR L is unbounded unlike the
entropy satisfying H(π) ≤ log M for any π ∈ [0, 1]M . Also,
L(π) = −H(π) −
M∑
i=1
πi log(1 − πi) ≥ − H(π). (12)
Therefore, for the same value of λ, the Bayesian LLR reward
function gives a higher weight to the accuracy than the
cost. As a result, the sensitivity of the trade-off between t he
accuracy and sensing cost differs for the two reward functio ns.
W e discuss this point in detail in Section VI.
This completes our discussion on the reward function.
Using this formulation, we next present the deep learning
algorithms to obtain policies that maximize the long-term
reward of the MDP . W e use two approaches: the deep RL-
based approach presented in Section IV and deep active
inference-based approach presented in Section V.
IV . ANOMAL Y DET ECT ION USING DEEP RL
ALGORIT HMS
Our RL algorithms are designed to maximize the expected
discounted return ¯R(t) deﬁned as
¯R(t) = lim
T →∞
T∑
j=0
γj r(t + j), (13)
where 0 < γ < 1 (which is generally close to 1) is the
discount factor. This parameter γ weighs the rewards in the
distant future relative to those in the immediate future, i. e., a
reward received j time steps in the future is worth only γj−1
times what it would be worth if it were received immediately.
Hence, this approach encourages the agent to minimize the
stopping time.
T o maximize ¯R(t), the RL algorithms make process selec-
tion based on the value functions of the posterior-action pa ir
and the posterior. For a given policy µ, these value functions
are
Qµ (π, A) = E
{¯Rk
⏐
⏐π(t − 1) = π, A(t) = A
}
(14)
Vµ (π) = EA∼µ (π )
{¯Rk
⏐
⏐π(t − 1) = π
}
, (15)
where the expectations are evaluated given that the agent fo l-
lows the policy µ for all future actions. Intuitively, the action-
value function (referred to as the Q-function), Qµ (π, A), in
(14) indicates the long term desirability of choosing a part icu-
lar action when the posterior belief vector is π. Also, the state-
value function (referred to as the value function), Vµ (π), in
(15) speciﬁes the expected reward when starting with poster ior
belief vector π and following the policy µ thereafter. An RL
agent makes the action choices by evaluating the optimal val ue
estimates, Q-function or the value function, or both. If we
have the optimal values of the functions, then the actions th at
appear best after a one-step search are the optimal actions [ 20].
In the following, we present two different RL approaches, th e
Q-learning and actor-critic algorithms, and describe how t hey
estimate these functions to arrive at the optimal policy.
A. Dueling Deep Q-learning
The Q-learning approach is a popular RL algorithm where
the agent estimates the Q-function and chooses the action A(t)
that maximizes the Q-function given the posterior belief vector
π(t − 1) [21]. Further, in the deep Q-learning framework, the
unknown Q-function is modeled using a neural network [22],
and the dueling deep Q-learning framework refers to the
implementation of this neural network using a model called
the dueling architecture [23].
This architecture consists of a single Q-network and relies
on a quantity called the advantage function: Aµ (π, A) =
Qµ (π, A)−Vµ (π), which is a measure of how much Qµ (π, A)
deviates from the expected value over all the actions, Vµ (π),
and therefore, speciﬁes the relative preference of each act ion
for a given posterior belief vector. The dueling architectu re
estimates both Vµ (π) and Aµ (π, A) separately and combines
them to obtain Qµ (π, A). The input to the Q-network is the
posterior belief vector π ∈ RM and the output is an (M − 1)-
length vector whose ith entry corresponds to Qµ (π, ·) of the
ith possible action. The network parameter θDQN is obtained
by optimizing the loss function [23]:
LDQN(θDQN)= Eπ, A,π ′
{
r(t) + γ max
A′∈P(N)
Q(π′, A′; θ−
DQN)
− Q(π, A; θDQN)
}
, (16)
where θ−
DQN is the current network parameter estimate ob-
tained in the previous time. W e update the Q-function using
6 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
bootstrapping by basing its update in part on an current
estimate Q(π′, A′; θ−
DQN). Also, to ensure that the learned
value of the Q-function converges to the optimal Q-function,
we use the greedy policy to choose the successor action A′.
Using the learned Q-function, we next describe how to
obtain the optimal policy. W e derive the policy using a
combination of the decaying-epsilon greedy algorithm [23] ,
[24] and the Gibbs softmax method [25]. At every time step,
the agent takes action using the Gibbs softmax method with
a probability of 1 − ǫ and a random action with a probability
of ǫ. Here, ǫ is a parameter that decays with time. Also, the
Gibbs softmax method refers to choosing the action A(t) ∼
σ (Q(π(t), ·; θDQN)) ∈ [0, 1]M−1 where σ(·) is the softmax
function. The approach ensures that the entire action space is
explored while exploiting the best action with high probabi lity.
The algorithm chooses actions in the above fashion until the
posterior belief distribution satisﬁes the stopping rule i n (7).
W e present the pseudo-code for our dueling deep Q-learning-
based detection algorithm in Algorithm 1.
Algorithm 1 Dueling Q-learning algorithm for anomaly de-
tection
Parameters: Prior distribution π(0), discount rate γ ∈ (0, 1),
and conﬁdence level πupper ∈ (0, 1]
Initialization: Q-network parameter θDQN arbitrarily
1: repeat
2: Time index t = 1
3: while max
i
πi(t − 1) < π upper do
4: Choose action A(t) using the policy derived from
Q(π(t), ·; θDQN)
5: Generate observations yA(t)(t)
6: Compute π(t) using (5) and r(t) using (11)
7: Update θDQN by minimizing LDQN(θDQN) in (16)
8: Increase time index t = t + 1
9: end while
10: Declare ˆx = h(i∗)
k where i∗ = arg max
i
πi(t − 1)
11: until
B. Deep Actor-critic
The deep actor-critic algorithm is another RL algorithm tha t
directly learns the policy. This principle differs from tha t of
the dueling deep Q-learning algorithm, which learns the Q-
function and derives a policy based on the learned Q-function.
The actor-critic architecture consists of two separate neu ral
networks, actor and critic, with no shared features. The act or
learns the policy, which chooses the action based on the
posterior probabilities. Thus, its input is π ∈ RM and the
output is µAC(π, ·; θactor) ∈ [0, 1]M−1 where θactor represents
the neural network parameters. The policy returned by the
actor network is a stochastic policy which chooses an action
according to A ∼ µAC(π, ·; θactor). The critic refers to the
learned value function, which is an estimate of how good the
policy learned by the actor is and hence, essentially provid es
an evaluation of that policy. The evaluation of the action A(t)
taken corresponding to the posterior π(t − 1) takes the form
of the temporal-difference (TD) error as given by
δ(t) = r(t) + γVµ (π(t)) − Vµ (π(t − 1)),
for a given policy µ(·) with
Vµ (π) = Eπ ′, A∼µ (π ) {r(t) + γVµ (π′)|π(t − 1) = π} .
If the TD error is positive, the probability of choosing A(t)
in the future is increased, and vice versa. Therefore, the
input to the critic network is the posterior belief vectors
π ∈ [0, 1]M and the output is the corresponding value function
V (π; θcritic) ∈ R where θcritic represents the neural network
parameters.
W e next describe how to learn the two sets of network
parameters: θactor of the actor and θcritic of the critic. Since
the goal of the critic network is to ﬁt a model to estimate the
optimal value function, its parameter update is equivalent to
minimizing the model mismatch between the reward obtained
at the current time step and the learned value function. Thus ,
the critic network updates its parameter θcritic by minimizing
the square of the TD error given by
δ(t) = r(t) + γV (π(t); θcritic) − V (π(t − 1); θ−
critic), (17)
where θ−
critic is the current critic network parameter estimate
obtained in the previous time instant. On the other hand, the
goal of the actor network is to ﬁnd a policy that maximizes the
value function. Thus, its parameter update is via maximizat ion
of the value function. The actor updates its parameter [20] a s
θactor =θ−
actor + δ(t)∇θactor [log µAC(π(t−1), A(t); θactor)] ,
(18)
where θ−
actor is the current actor network parameter estimate
obtained in the previous time instant and δ(t) given by (17)
is obtained from the critic network.
The learned policy is straightforward in the case of the
actor-critic framework, as the actor network directly lear ns
the policy. Hence, at every time step, the agent chooses an
action based on the output of the actor network and receives
the reward for updating both actor and critic networks. The
agent stops collecting observations and returns an estimat e of
x when the conﬁdence level exceeds the desired level, i.e.,
when (7) holds. The pseudo-code of our algorithm is given in
Algorithm 2.
V . ANOMAL Y DET ECT ION USING DEEP ACT IVE
INF ERENCE
The active inference framework is an alternate approach to
solving the MDP problem described in Section III. It is in-
spired by a normative theory of brain function based on its pe r-
ception of the MDP , i.e., the active inference agent maintai ns a
generative model that represents its perception [26]–[28] . This
generative model φ(·) comprises a joint probability distribu-
tion on the posterior, the actions, and the corresponding ob ser-
vations:
{
π(t − 1), A(t), yA(t)(t), t > 0
}
. The model assigns
higher probabilities to the posteriors and actions favorab le to
the agent. Given a generative model, the agent inverts the
model to ﬁnd the conditional distribution of the action A(t)
corresponding to the posterior π(t − 1). However, since di-
rectly computing the marginals is difﬁcult, we use the metho d
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 7
Algorithm 2 Actor-critic RL for anomaly detection
Parameters: Prior distribution π(0), discount rate γ ∈ (0, 1),
and conﬁdence level πupper ∈ (0, 1]
Initialization: Actor and critic neural network parameters
θactor and θcritic arbitrarily
1: repeat
2: Time index t = 1
3: while max
i
πi(t − 1) < π upper do
4: Choose action A(t) using the policy derived from
µAC(π(t − 1), ·; θactor)
5: Generate observations yA(t)(t)
6: Compute π(t) using (5) and r(t) using (11)
7: Update θcritic by minimizing the squared temporal
error δ2(t) in (17)
8: Update θactor using (18)
9: Increase time index t = t + 1
10: end while
11: Declare ˆx = h(i∗)
k where i∗ = arg max
i
πi(t − 1)
12: until
of approximate Bayesian inference. T o this end, it deﬁnes
a variational distribution µAI(π, A) that is controlled by the
agent. The distribution µAI(·) is optimized by minimizing the
Kullback-Leibler (KL) divergence between the distributio ns
µAI(·) and φ(·). Therefore, a stochastic policy that chooses
actions according to the distribution µAI(·) maximizes the
obtained reward. The KL divergence between the variational
distribution and the generative model is called the variati onal
free energy. In other words, the goal of the active inference
agent is to ﬁnd the stochastic policy µAI(·) which minimizes
its expected free energy (EFE).
From (5), we know that the posterior belief vector π(t) can
be exactly inferred using the knowledge of the action A(t),
observation yA(t) and posterior belief vector π(t − 1). This
relationship, along with the Markov property, enables us to
completely deﬁne the generative model using the distributi on
φ(A(t), yA(t)(t)|π(t − 1)). This distribution is
φ(yA(t)(t), A(t)|π(t − 1))
= φ(yA(t)|A(t), π(t − 1))φ(A(t)|π(t − 1)).
The generative model is biased towards high rewards, encode d
into the generative model as the prior probability of the bel ief,
φ(yA(t)(t)|A(t), π(t − 1)) = σ (r(t)) , (19)
where we recall that σ(·) is the softmax function and r(t)
denotes the instantaneous reward of the MDP at time t.
W e next complete the construction of the generative model
by specifying the distribution φ(A(t)|π(t)). Since the agent
tries to minimize the total free energy of the expected traje c-
tories into the future, it is encoded into the generative mod el
as
φ(A(t)|π(t)) = σ (−G(A(t), π(t − 1))) , (20)
where G(·) is the total free energy of the expected trajectories
into the future, and the variational free energy is the KL
divergence between the variational distribution µAI(·) and the
generative model φ(·):
F (t) =
∑
A(t)∈P(N)
µAI(π(t − 1), A(t))
× log µAI(π(t − 1), A(t))
φ(A(t), yA(t)(t)|π(t − 1)) . (21)
Thus, the agent constructs the generative model using the EF E,
obtains the optimum policy by minimizing the EFE of all the
paths into the future, and chooses an action that minimizes t he
EFE. In other words, determining the optimal policy reduces
to computing and optimizing the EFE.
Next, we present the neural network architecture and learn-
ing of the neural network parameters. The deep active infer-
ence algorithm consists of two neural networks: the policy a nd
EFE. The policy network directly learns the process selecti on,
and therefore, it takes the posterior belief vector π(t − 1)
as the input. Its output is the stochastic selection policy
µAI(π(t − 1), ·; θpolicy ) ∈ [0, 1]M−1 which is a probability
distribution on P(N). Here, θpolicy denotes the neural net-
work parameters. The EFE network represents EFE’s learned
value, which estimates how close the learned policy is to
the generative model. Thus, the input of the EFE network
is the posterior π ∈ [0, 1]M , and the output is the EFE value
G(π, ·; θEFE) ∈ RM−1, representing the EFE values of each
action A ∈ P (N) and the posterior π. Here, θEFE denotes
the parameters of the EFE network.
The EFE can be approximated as follows [29]:
G(A(t), π(t − 1)) ≈ E {−r(t) + G(A(t + 1), π(t))} ,
where we use (19). Therefore, we learn the parameters of the
EFE network by optimizing the model mismatch between the
learned EFE value and the reward obtained:
LEFE(θEFE) = E
{(
G (A(t), π(t − 1); θEFE) + r(t)
− G
(
A(t + 1), π(t); θ−
EFE
))2}
, (22)
where θ−
EFE denotes the current estimate of the network pa-
rameter obtained in the previous time step and the expectati on
is over the action distribution A(t + 1) ∼ µAI(π(t), ·; θpolicy ).
T o update the policy network, we minimize the variational
free energy deﬁned in (21):
F (θpolicy ) =
∑
A∈P(N)
µAI(π(t − 1), A; θpolicy))
× log µAI(π(t − 1), A; θpolicy)
σ(r(t))σ(−G(π(t − 1), A; θEFE)) . (23)
Since r(t) is independent of θpolicy , the loss function is
LAI(θpolicy) = −H(µAI(π(t − 1), ·; θpolicy))
−
∑
A∈P(N)
µAI(π(t − 1), A) log σ(G(π(t − 1), A; θEFE)),
(24)
where H(·) is given by (10).
As in the case of the actor-critic algorithm, the policy to be
followed by the agent is directly obtained from the (policy)
8 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
neural network output. The agent follows this policy to choo se
an action at every time instant, collects the corresponding re-
ward, and updates the two neural networks using the obtained
reward. The algorithm is summarized in Algorithm 3.
Algorithm 3 Active inference algorithm for anomaly detection
Parameters: Prior distribution π(0) and conﬁdence level
πupper ∈ (0, 1]
Initialization: Policy and EFE network parameters θpolicy
and θEFE arbitrarily
1: repeat
2: Time index t = 1
3: while max
i
πi(t − 1) < π upper do
4: Choose action A(t) using the policy derived from
µAI(π(t − 1), ·; θpolicy)
5: Generate observations yA(t)(t)
6: Compute π(t) using (5) and r(t) using (11)
7: Update θEFE by minimizing LEFE(θEFE) in (22)
8: Update θpolicy by minimizing the variational free
energy LAI(θpolicy ) in (24)
9: Increase time index t = t + 1
10: end while
11: Declare ˆx = h(i∗)
k where i∗ = arg max
i
πi(t − 1)
12: until
A. Comparison With RL Methods
The active inference approach has many similarities to
RL-based algorithms, such as learning probabilistic model s,
exploring and exploiting various actions, and efﬁcient pla n-
ning. In particular, the active inference algorithm closel y
resembles the policy gradient methods (for example, the act or-
critic algorithm) since both approaches try to learn the pol icy
directly. W e recall that the actor-critic and the active inf erence
methods have two separate neural networks. The actor networ k
of the actor-critic algorithm and the policy network of the
active inference algorithm learn the policy to be followed. In
contrast, the other network estimates a function (TD error o r
EFE) used to evaluate and optimize the policy. However, the
two algorithms are derived based on different principles, a nd
the main differences between the RL framework and the active
inference framework are as follows:
1) Model-free and model-based : The traditional RL uses
a model-free approach where the algorithm aims at reward
maximization based on the Q function or the value function,
or both. The algorithm does not try to explicitly learn the
probabilistic model that governs the state transition or th e
generation of the observations of the MDP . However, the ac-
tive inference model relies on a hierarchical generative mo del,
which is based on variational free energy. It explicitly lea rns
the model consisting of states, actions, and observations o f
the MDP . T o be speciﬁc, in the most general setting, the deep
active inference algorithm consists of four neural network s:
policy network; EFE network; observation network to learn
the distribution of the observations given the state and act ion;
and state transition network to learn the distribution of th e next
state given the previous state, action, and observation [29 ].
However, in our case, (19) and (5) deﬁne the distributions
learned by the observation network and the state transition
network. Hence, the active inference algorithm comprises o nly
two neural networks. In other words, the active inference
algorithm naturally allows the algorithm to incorporate an y
knowledge of the environment’s statistics into the model.
2) Policy optimization : The actor-critic algorithm uses the
value functions directly to learn the policy. In contrast, t he
active inference algorithm relies on the generative probab ility
distribution derived from softmax over the variational fre e
energy as deﬁned in (20). Therefore, the actor-critic algor ithm
maximizes the expected reward function in the future, where as
the active inference algorithm reduces the surprise in the
future by learning the probabilistic model. Moreover, the
objective function of the actor-critic algorithm depends o nly
on the samples generated using the actions that the agent
took within the episode as opposed to the active inference
algorithm, which averages the objective function over all
possible actions in the next step (see the summation in (24)) .
Since the active inference algorithm computes the expected
value, it may lead to reduced variance and better performanc e.
B. Comparison With Chernoff T est
The Chernoff test, a standard algorithm for active hypoth-
esis testing [5], sequentially chooses actions that build t he
posterior belief on the true value of x as quickly as possible.
However, it does not take the sensing cost into account. It
follows the stochastic policy µChernoﬀ ,
µChernoﬀ (π(t − 1), ·) = arg max
q∈[0, 1]M−1
∑
i qi=1
min
ˆx∈{0, 1}M
ˆx̸=¯x(t−1)
qTd(¯x(t − 1), ˆx),
(25)
where we deﬁne the ith entry of d(·) ∈ RM−1 as
di(¯x(t), ˆx) ≜ KL (p (yAi (t)|x = ¯x(t)) ∥p (yAi (t)|x = ˆx))
¯x(t) = h(˜i∗); ˜i∗ = arg max
i
πi(t).
Here, KL denotes the KL divergence between the distribu-
tions, and Ai denotes that ith element of set P(N). However,
for any ˆx, the KL divergence term is maximized when all
the processes are selected. Thus, we have di(¯x(t), ˆx) ≤
d1(¯x(t), ˆx) with A1 = {1, 2, . . . , N } denoting the action of
selecting all processes. Therefore, we arrive at
max
q∈[0, 1]M−1
∑
i qi=1
min
ˆx∈{0, 1}M
ˆx̸=¯x(t−1)
qTd(¯x(t − 1), ˆx)
≤ min
ˆx∈{0, 1}M
ˆx̸=¯x(t−1)
d1(¯x(t), ˆx),
and equality holds when q =
[ 1 0 . . . 0]
∈ [0, 1]M−1.
Therefore, the Chernoff test always chooses the action A1
with probability one. This policy is expected as the Chernof f
test does not optimize the sensing cost, and thus, it achieve s
a small stopping time while incurring a high sensing cost. On
the contrary, our formulation balances the trade-off betwe en
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 9
0 1 2 3 4 5 6
Time step t
0
0.2
0.4
0.6
0.8
1
Posterior probability i(t)
x=000
x=001
x=010
x=011
x=100
x=101
x=110
x=111
(1/0,2/0)(1/0,2/1,3/1)(1/0,3/1)(3/1) (1/0) (1/1,2/0,3/1)
Processes chosen/observations
(a) Actor-critic’s policy when the process state is
[0 0 1]
0 1 2 3 4 5 6
Time step t
0
0.2
0.4
0.6
0.8
1
Posterior probability i(t)
x=000
x=001
x=010
x=011
x=100
x=101
x=110
x=111
(1/1,3/0)(1/0,2/1)(2/0) (1/0) (2/0) (1/0,2/0,3/0)
Processes chosen/observations
(b) Active inference’s policy when the process state
is [0 0 0]
Fig. 2 : A single realization of the variation of the belief
vector π(t), sensor selection A(t), and the corresponding
observations yA(t)(t) over time t. W e choose πupper = 0 .94,
ρ = 0 .8, and λ = 0 .2. The curves represent the evolution
of the posterior probabilities of different hypotheses. Th e se-
lected processes (sensors) and the corresponding observat ions
at different times are depicted at the top of the ﬁgure.
the stopping time and sensing cost via λ and exploits the sta-
tistical dependence in x modeled using π(0). W e corroborate
this point using results in Section VI (e.g., see Fig. 7).
VI. N UMERICAL RESULT S
In this section, we present numerical results comparing
the performances of deep RL and deep active inference
algorithms. W e choose the number of processes as N = 3
and, thus, M = 2 N = 8 . The prior probability of a process
being normal is taken as q = 0 .8. Here, the ﬁrst and second
processes are assumed to be statistically dependent, and th e
third is independent of the other two. The correlation betwe en
the dependent processes is captured by the parameter ρ ∈
[0, 1]:
P
{
x =
[ 0 0 ]}
= q2 + ρq(1 − q)
P
{
x =
[
0 1
]}
= P
{
x =
[
1 0
]}
= q(1 − q)(1 − ρ).
Also, we assume that the maximum number of time slots for
each episode (trial or run) is Tmax = 5000 .
W e implement all the neural networks (the Q-network of
deep Q-learning, the actor and critic networks, and the poli cy
and the bootstrapped EFE networks of active inference) with
three layers and the ReLU activation function between each
consecutive layer. T o update the network parameters, we app ly
the Adam Optimizer. Also, we set γ = 0 .9 for the RL
algorithms, and ǫ values linearly decrease from 0.4 to 0.05.
W e train the neural networks over multiple episodes (real-
izations) where, for each episode, we choose the process sta tes
from the prior distribution mentioned above, and the number
of time slots for each episode is ﬁxed as 50. The actor-critic
and active inference algorithms converge after 1000 episod es,
whereas the deep Q-learning algorithm requires 2000 episod es
to achieve a stable policy. So, the deep Q-learning algorith m
requires more extended training than the other two algorith ms.
After the training phase, we test the algorithms. W e start
with two illustrations in Fig. 2. They show the realizations of
the variation of the belief vector π(k), sensor selection A(k),
and the corresponding observations yA(k)(k) over time k until
the stopping time. Fig. 2a shows the sensor selection of the
actor-critic algorithm when the true hypothesis (process s tate)
is [0 0 1] . Here, the posterior probability corresponding to
the wrong process state [0 0 0] was high initially due to the
prior distribution. Since the true process state [0 0 1] and the
state [0 0 0] differ only in the state of the third process, the
posterior probability corresponding to the true process st ate
[0 0 1] is not high until the third process is observed at
k = 2 . Note that at k = 2 , the selected processes and the
corresponding observations are described by (1/0, 2/1, 3/1),
indicating that all three processes have been chosen and the
noisy observations are [0 1 1] . As the probability of the
true process state [0 0 1] increases (at time k = 2 ), the
algorithm observes the third process more often. Finally, a t
time k = 6 , the posterior probability of the true process state
[0 0 1] exceeds πupper = 0 .94 and the algorithm stops. W e
can make similar observations from Fig. 2b where the true
process state is [0 0 0] . Due to the error in observations from
the ﬁrst process at k = 1 and the second process at k = 2 ,
the posterior probability of the state [1 1 0] increases initially.
Then, the algorithm observes these two processes more often .
This policy allows the algorithm to observe that the probabi lity
of the state [1 1 0] decreases, and the probability of the true
process state [0 0 0] exceeds πupper at k = 6 .
Next, we show the performance of the algorithms. Like
the training phase, for each episode of the testing phase,
we choose the process states from the prior distribution. Th e
three performance metrics we use for comparison are detecti on
accuracy, stopping time, and total cost, as deﬁned in Sectio n II.
If the estimated hypothesis is the same as the true hypothesi s,
the (instantaneous) detection accuracy is one, and otherwi se,
it is zero. Also, stopping time is the shortest time at which t he
stopping criteria in (7) is met. The average detection accur acy,
stopping time, and total cost obtained using the 104 episodes
are shown in Figs. 3 through 7. Like the training phase, durin g
testing for each episode, we choose the process states from
the prior distribution mentioned above. In Figs. 4 to 6, we
also show bar plots where the heights are proportional to the
fraction of times each process is chosen. In the ﬁgures, we
10 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
0.8 0.85 0.9 0.95 1
Confidence level upper
0.5
0.6
0.7
0.8
0.9
1
Detection accuracy
AC: LLR
AI: LLR
DQN: LLR
0.8 0.85 0.9 0.95 1
Confidence level upper
4
6
8
10
12
14
16
18
20Stopping time  K
AC: LLR
AI: LLR
DQN: LLR
0.8 0.85 0.9 0.95 1
Confidence level upper
1
1.5
2
2.5
3
3.5
4
Total cost
AC: LLR
AI: LLR
DQN: LLR
0 0.2 0.4 0.6 0.8 1
Correlation parameter 
0.5
0.6
0.7
0.8
0.9
1
Detection accuracy
AC: Entropy
AI: Entropy
DQN: Entropy
0 0.2 0.4 0.6 0.8 1
Correlation parameter 
4
5
6
7
8
9
10
11
12
13Stopping time  K
AC: Entropy
AI: Entropy
DQN: Entropy
0 0.2 0.4 0.6 0.8 1
Correlation parameter 
1
1.5
2
2.5
3
Total cost
AC: Entropy
AI: Entropy
DQN: Entropy
0 0.5 1 1.5 2 2.5
Tradeoff parameter 
0.5
0.6
0.7
0.8
0.9
1
Detection accuracy
AC: LLR
AC: Entropy
AI: LLR
AI: Entropy
DQN: LLR
DQN: Entropy
0 0.5 1 1.5 2 2.5
Tradeoff parameter 
0
2
4
6
8
10Stopping time  K AC: LLR
AC: Entropy
AI: LLR
AI: Entropy
DQN: LLR
DQN: Entropy
0 0.5 1 1.5 2 2.5
Tradeoff parameter 
0.5
1
1.5
2
2.5Total cost AC: LLR
AC: Entropy
AI: LLR
AI: Entropy
DQN: LLR
DQN: Entropy
Fig. 3: Performance of the actor-critic, active inference, and de ep Q-learning algorithms for two different reward function s.
Unless otherwise mentioned in the plot, we choose πupper = 0 .8, ρ = 0 .8, and λ = 1 .
compare the three algorithms (label names in brackets), dee p
Q-learning ( DQN), actor-critic ( AC), and active inference ( AI)
algorithms considering both Bayesian LLR-based ( LLR) and
entropy-based ( Entropy) reward functions. Our observations
from the numerical results are presented next.
1) Conﬁdence level πupper: The variations in the perfor-
mance of different algorithms with πupper are shown in the
ﬁrst row of Fig. 3, and Figs. 4 to 6. All three performance
metrics increase with πupper in all cases. This observation is
intuitive as a higher value of πupper implies higher accuracy
and requires the algorithms to collect more observations
before they decide on anomalous processes. Also, the accura cy
levels achieved by all the algorithms are comparable in all
the settings because the common πupper sets the desired
conﬁdence level of detection.
2) Correlation parameter ρ: The second row of Fig. 3
illustrates the performances with varying ρ. The accuracy
is insensitive to ρ as it is decided by the conﬁdence level
πupper. On the other hand, the stopping time and total cost
decrease with ρ. This decrease is expected because when the
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 11
AC: LLR AI: LLR DQN: LLR
0
0.1
0.2
0.3
0.4
0.5Fraction of sensor selection
Sensor 1
Sensor 2
Sensor 3
0.8 0.85 0.9 0.95 1
Confidence level upper
4
4.5
5
5.5
6
6.5
7
7.5
8
Stopping time  K
AC: LLR
AI: LLR
DQN: LLR
0.8 0.85 0.9 0.95 1
Confidence level upper
2
3
4
5
6
7
8
9
10
11Total cost
AC: LLR
AI: LLR
DQN: LLR
Fig. 4: Performance of the actor-critic, active inference, and de ep Q-learning algorithms when the sensing costs differ: c1 = 2
and c2 = c3 = 0 .2. W e choose ρ = λ = 1 , pi = 0 .2 for i = 1 , 2, 3, and for the bar plot, we set πupper = 0 .94.
AC: LLR AI: LLR DQN: LLR
0
0.1
0.2
0.3
0.4
0.5Fraction of sensor selection
Sensor 1
Sensor 2
Sensor 3
0.8 0.85 0.9 0.95 1
Confidence level upper
4
5
6
7
8
9
10
11
12
13Stopping time  K
AC: LLR
AI: LLR
DQN: LLR
0.8 0.85 0.9 0.95 1
Confidence level upper
1.5
2
2.5
3
3.5
4
Total cost
AC: LLR
AI: LLR
DQN: LLR
Fig. 5: Performance of the actor-critic, active inference, and de ep Q-learning algorithms when the ﬂipping probabilities di ffer:
p1 = 0 .45 and p2 = p3 = 0 .2. W e choose ρ = λ = 1 , ci = 0 .2 for i = 1 , 2, 3, and for the bar plot, we set πupper = 0 .94.
AC: LLR AI: LLR DQN: LLR
0
0.1
0.2
0.3
0.4
0.5Fraction of sensor selection
Sensor 1
Sensor 2
Sensor 3
0.8 0.85 0.9 0.95 1
Confidence level upper
3
4
5
6
7
8Stopping time  K
AC: LLR
AI: LLR
DQN: LLR
0.8 0.85 0.9 0.95 1
Confidence level upper
2
3
4
5
6
7
8
9
10Total cost
AC: LLR
AI: LLR
DQN: LLR
Fig. 6: Performance of the actor-critic, active inference and dee p Q-learning algorithms when both sensing costs are differe nt:
c1 = 2 and c2 = c3 = 0 .2; and p1 = 0 .02, and p2 = p3 = 0 .2. W e choose ρ = λ = 1 , and we set πupper = 0 .94 for the bar
plot.
12 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2024
0 0.2 0.4 0.6 0.8
Correlation parameter 
0.5
0.6
0.7
0.8
0.9
1
Detection accuracy
AC: LLR
AI: LLR
DQN: LLR
Chernoff
0 0.2 0.4 0.6 0.8
Correlation parameter 
0
2
4
6
8
10Stopping time  K
AC: LLR
AI: LLR
DQN: LLR
Chernoff
0 0.2 0.4 0.6 0.8
Correlation parameter 
1
1.5
2
2.5
3
Total cost
AC: LLR
AI: LLR
DQN: LLR
Chernoff
Fig. 7: Comparison of our algorithms with the Chernoff test when πupper = 0 .82, λ = 0 and ci = pi = 0 .2 for i = 1 , 2, 3.
correlation increases, an observation corresponding to on e of
the dependent processes gives more information about the
other. Consequently, the algorithms require fewer observa tions
and a shorter stopping time to reach the same conﬁdence level .
3) T radeoff parameter λ: The last row of Fig. 3 depicts the
changes in the algorithm performances with λ. As in the case
of ρ, the accuracy and total cost do not vary signiﬁcantly with
λ for a ﬁxed value of πupper and ρ. This behavior is because
when ρ is ﬁxed, we need the same number of observations to
achieve the same conﬁdence level. However, as λ increases,
each observation becomes costlier, and the stopping time
increases. W e notice that the stopping time of the actor-cri tic
algorithm is more sensitive to λ compared to the deep Q-
learning and active inference algorithms. One reason for th is
could be that the temporal error, which is a function of only
the posterior, is more sensitive to the parameter λ than the Q-
function learned by the deep Q-learning algorithm and EFE
learned by the active inference algorithm, which are both
functions of the posterior belief and action.
4) Reward functions : From Fig. 3, we infer that all the al-
gorithms provide similar performance levels with both choi ces
of the reward function. However, the actor-critic and deep Q -
learning algorithms slightly underperform with the entrop y-
based reward function. Since the two reward functions have
λ ∥A(t)∥ in common, as λ increases, the performance differ-
ence also grows, as observed from the last row of Fig. 3. In
other words, the performance gap is the largest when λ = 0 ,
and the two reward functions become identical as λ goes to
∞. Further, we recall from Section III-B that for the same
value of λ, the Bayesian LLR reward functions give more
weight to the accuracy than the cost (see (12)). As a result, t he
performance with the entropy-based function for a particul ar
value of λ is similar to that with the Bayesian LLR reward
for a larger value of λ. For example, the sudden change in
the stopping time of the actor-critic algorithm with λ occurs
at λ = 0 .05 for the entropy-based function, whereas it occurs
at λ = 0 .2 for the Bayesian LLR-based reward. W e observe
similar behavior for the deep Q-learning algorithm as well.
5) Sensing cost ci and ﬂipping probability pi: W e analyze the
dependence of the algorithms’ performance on the sensing
cost and ﬂipping probability under three settings: 1) nonun i-
form sensing costs and uniform ﬂipping probabilities (see
Fig. 4); 2) uniform sensing costs and nonuniform ﬂipping
probabilities (see Fig. 5); and 3) nonuniform sensing costs
and ﬂipping probabilities (see Fig. 6) across the processes .
From Fig. 4, the deep Q-learning algorithm is more sensitive
to different cost values ci. In all settings considered in Fig. 4,
the deep Q-learning agent chooses the ﬁrst process less ofte n,
leading to the lowest total cost and best performance. The
actor-critic algorithm also adapts to the varying cost, whi le
the active inference algorithm is relatively less insensit ive to
the different costs. Similarly, when we increase the ﬂippin g
probability of the ﬁrst process in Fig. 5 (with uniform sensi ng
costs), we see that all algorithms adapt their policies. How ever,
the policy offered by the active inference algorithm has sho rter
stopping times than the other algorithms for comparable
values of the total cost. The differences in the policies of
the three algorithms are more evident in Fig. 6 when we vary
both sensing cost and ﬂipping probabilities. In this settin g, the
deep Q-learning algorithm chooses the ﬁrst process less oft en
despite its smaller ﬂipping probability. As in the case of Fi g. 5,
active inference is more sensitive to the ﬂipping probabili ty
than the cost, and as a result, it gives the shortest stopping
times at the price of a higher total cost. The performance
of the actor-critic algorithm is between those of the other t wo
algorithms. The actor-critic algorithm provides stopping times
comparable to those of the active inference algorithm while
incurring a smaller total sensing cost.
6) Competing algorithms : W e ﬁrst note that all the algo-
rithms have similar detection accuracy due to the common
stopping criteria in (7), i.e., they stop only when the detec tion
accuracy of the algorithm always exceeds πupper. So, the
choice of the best learning algorithm depends on the stoppin g
time and total cost. W e ﬁrst look at the algorithm perfor-
mances for the uniform cost and ﬁlliping probability case fr om
Figs. 3 and 7. For small values of λ, the actor-critic algorithm
offers the best stopping time but has a slightly higher cost
than the other algorithms. As λ increases, its stopping time
also increases, and the active inference algorithm provide s
the best stopping time for a comparable total cost. Also, the
G. JOSEPH et al. : ANOMAL Y DETECTION VIA LEARNING-BASED SEQUENTIAL CONTROLLED SENSING 13
active inference algorithm offers slightly better perform ance
than deep Q-learning. However, our experiments show that th e
Q-learning algorithm requires more episodes in the trainin g
phase than the other algorithms to achieve a stable policy.
The memory replay in the Q-learning algorithm also makes its
training phase further longer than the other algorithms. Th ere-
fore, the actor-critic algorithm is more suitable for sensi ng
cost-critical applications, and for time-sensitive appli cations,
we recommend the active inference algorithm over Q-learnin g.
Next, we look at the nonuniform setting in Figs. 4 to 6. W e
notice that the deep Q-learning algorithm is more sensitive
to the nonuniform sensing cost, whereas the active inferenc e
algorithm is more sensitive to the nonuniform ﬂipping prob-
ability. So, in the nonuniform setting, we prefer Q-learnin g
for cost-critical applications and active inference for st opping
time-sensitive applications. These observations further justify
our joint analysis of different learning-based methods.
7) Comparison with Chernoff test : Fig. 7 compares our
algorithms with the classical Chernoff test. The stopping t ime
and the total sensing cost of the Chernoff test are relativel y in-
sensitive to the variation in ρ. In contrast, our algorithms, par-
ticularly the active inference algorithm, adapt their stop ping
time and total sensing cost to ρ. This observation is intuitive
as the policy followed by the Chernoff test does not depend
on ρ or λ, and it assumes that the processes are independent.
Therefore, (25) leads to the optimum performance when ρ = 0
but deteriorates as ρ increases.
VII. C ONCLUSION
This paper considered the anomaly detection problem,
where the goal is to identify the anomalies among a given
set of processes. W e modeled the problem of anomaly de-
tection as an MDP problem aiming at the detection accuracy
exceeding a desired value while minimizing the delay and tot al
sensing cost. T o this end, we designed two objective functio ns
based on Bayesian LLR and entropy and presented two deep
RL-based algorithms and a deep active inference algorithm.
Through simulation results, we compared our algorithms and
showed that all algorithms perform similarly in the detecti on
accuracy for the same conﬁdence level. However, the dueling
deep Q-learning algorithm required a more prolonged traini ng
phase, and the active inference algorithm is more robust to
the trade-off parameter and adapts better to the correlatio n
parameter. W e also inferred that the policy of the dueling de ep
Q-learning algorithm always led to more negligible sensing
costs. In contrast, the active inference algorithm is more
sensitive to the ﬂipping probabilities. Extending our algo rithm
to track any changes in the behavior of the processes over a
more extended time period is an exciting future direction.
REF ERENCES
[1] W .-Y . Chung and S.-J. Oh, “Remote monitoring system with wireless
sensors module for room environment, ” Sensors Actuators B: Chemical,
vol. 113, no. 1, pp. 64–70, Jan. 2006.
[2] A. Bujnowski, J. Ruminski, A. Palinski, and J. Wtrorek, “ Enhanced
remote control providing medical functionalities, ” in Proc. Inter . Conf.
P ervasive Comput. T ech Healthc. W orkshops, May 2013, pp. 290–293.
[3] C. Zhong, M. C. Gursoy , and S. V elipasalar, “Deep actor-c ritic rein-
forcement learning for anomaly detection, ” in Proc. Globecom , Dec.
2019.
[4] G. Joseph, M. C. Gursoy , and P . K. V arshney , “ Anomaly dete ction under
controlled sensing using actor-critic reinforcement lear ning, ” in Proc.
IEEE Inter . W orkshop SP A WC, May 2020.
[5] H. Chernoff, “Sequential design of experiments, ” Ann. Math. Stat. ,
vol. 30, no. 3, pp. 755–770, Sep. 1959.
[6] S. A. Bessler, “Theory and applications of the sequentia l design of
experiments, k-actions and inﬁnitely many experiments. pa rt I. theory , ”
Stanford Univ CA Applied Mathematics and Statistics Labs, T ech. Rep.,
1960.
[7] M. Naghshvar and T . Javidi, “Extrinsic jensen-shannon d ivergence with
application in active hypothesis testing, ” in Proc. ISIT , Jul. 2012, pp.
2191–2195.
[8] M. Naghshvar, T . Javidi et al. , “ Active sequential hypothesis testing, ”
Ann. Stat. , vol. 41, no. 6, pp. 2703–2738, 2013.
[9] M. Franceschetti, S. Marano, and V . Matta, “Chernoff tes t for strong-
or-weak radar models, ” IEEE Trans. Signal Process., vol. 65, no. 2, pp.
289–302, Oct. 2016.
[10] B. Huang, K. Cohen, and Q. Zhao, “ Active anomaly detecti on in
heterogeneous processes, ” IEEE Trans. Inf. Theory , vol. 65, no. 4, pp.
2284–2301, Aug. 2018.
[11] D. Kartik, E. Sabir, U. Mitra, and P . Natarajan, “Policy design for active
sequential hypothesis testing using deep learning, ” in Proc. Allerton ,
Oct. 2018, pp. 741–748.
[12] G. Joseph, C. Zhong, M. C. Gursoy , S. V elipasalar, and P . K. V arshney ,
“ Anomaly detection via controlled sensing and deep active i nference, ”
in Proc. IEEE Globecom , Dec. 2020.
[13] G. Joseph, M. C. Gursoy , and P . K. V arshney , “T emporal de tection
of anomalies via actor-critic based controlled sensing, ” i n Proc. IEEE
Globecom, Dec. 2021, pp. 1–6.
[14] ——, “ A scalable algorithm for anomaly detection via lea rning-based
controlled sensing, ” in Proc. ICC, Jun. 2021, pp. 1–6.
[15] G. Joseph, C. Zhong, M. C. Gursoy , S. V elipasalar, and P . K. V arsh-
ney , “Scalable and decentralized algorithms for anomaly de tection via
learning-based controlled sensing, ” IEEE Trans. Signal Inf. Process.
Netw ., to appear .
[16] T . Y aacoub, G. V . Moustakides, and Y . Mei, “Optimal stop ping for
interval estimation in Bernoulli trials, ” IEEE Trans. Inf. Theory, vol. 65,
no. 5, pp. 3022–3033, 2018.
[17] P . Grambsch, “Sequential sampling based on the observe d Fisher infor-
mation to guarantee the accuracy of the maximum likelihood e stimator, ”
Ann. Stat. , pp. 68–77, 1983.
[18] P . J. Bickel and J. A. Y ahav , “ Asymptotically pointwise optimal proce-
dures in sequential analysis, ” in Proc. Fifth Berk. Symp. Math. Statist.
Probab, vol. 1, 1967, pp. 401–413.
[19] T . M. Cover, Elements of information theory . John Wiley & Sons,
1999.
[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
MIT press, 2018.
[21] C. J. C. H. W atkins, “Learning from delayed rewards, ” Ph .D. disserta-
tion, Psychology Department, King’ s College, Cambridge, U K, 1989.
[22] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. V eness , M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ost rovski
et al. , “Human-level control through deep reinforcement learnin g, ”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.
[23] Z. W ang, T . Schaul, M. Hessel, H. Hasselt, M. Lanctot, an d N. Freitas,
“Dueling network architectures for deep reinforcement lea rning, ” in
Proc. ICML, 2016, pp. 1995–2003.
[24] A. Ostovar, O. Ringdahl, and T . Hellstr ¨ om, “ Adaptive i mage threshold-
ing of yellow peppers for a harvesting robot, ” Robotics, vol. 7, no. 1,
p. 11, 2018.
[25] W . Kong, W . Krichene, N. Mayoraz, S. Rendle, and L. Zhang ,
“Rankmax: An adaptive projection alternative to the softma x function, ”
Adv . Neural Inf. Process. Syst., vol. 33, pp. 633–643, 2020.
[26] K. Friston, T . FitzGerald, F . Rigoli, P . Schwartenbeck , and G. Pezzulo,
“ Active inference: A process theory , ” Neural Comput. , vol. 29, no. 1,
pp. 1–49, Jan. 2017.
[27] K. J. Friston, M. Lin, C. D. Frith, G. Pezzulo, J. A. Hobso n, and
S. Ondobaka, “ Active inference, curiosity and insight, ” Neural Comput.,
vol. 29, no. 10, pp. 2633–2683, Oct. 2017.
[28] K. Friston, F . Rigoli, D. Ognibene, C. Mathys, T . Fitzge rald, and
G. Pezzulo, “ Active inference and epistemic value, ” J. Cogn. Neurosci.,
vol. 6, no. 4, pp. 187–214, Oct. 2015.
[29] B. Millidge, “Deep active inference as variational pol icy gradients, ” J.
Math. Psychol. , vol. 96, p. 102348, Jan. 2020.
[30] A. Dargazany , “Model-based actor-critic: GAN+ DRL (ac tor-critic) =>
AGI, ” arXiv preprint arXiv:2004.04574 , 2020.
Process 1
Process 2
.
.
.
Process N
Joint distribution
Environment
Our learning
algorithm
Anomalous
processes’ indices
Noisy sensor
measurements
Dynamic
sensor selection
Output
This figure "jsenga.png" is available in "png"
 format from:
http://arxiv.org/ps/2312.00088v1