0202
tcO
1
]GL.sc[
1v26200.0102:viXra
Active Inference or Control as Inference?
A Unifying View
Abraham Imohiosen*† Joe Watson*§ Jan Peters§
†RWTH Aachen University, Germany
§IAS, Technical University Darmstadt, Germany
abraham.imohiosen@rwth-aachen.de
{watson,peters}@ias.informatik.tu-darmstadt.de
Abstract
Activeinference(AI)isapersuasivetheoretical framework from com-
putational neuroscience that seeks to describe action and perception as
inference-based computation. However, this framework has yet to pro-
vide practical sensorimotor control algorithms that are competitive with
alternative approaches. In this work, we frame active inference through
the lens of control as inference (CaI), a body of work that presents tra-
jectory optimization as inference. From the wider view of ‘probabilistic
numerics’,CaIoffersprincipled,numericallyrobustoptimalcontrolsolvers
that provide uncertainty quantification, and can scale to nonlinear prob-
lems with approximate inference. We show that AI may be framed as
partially-observedCaIwhenthecostfunctionisdefinedspecificallyinthe
observation states.
1 Introduction
Active inference (AI) [2, 4, 5] is a probabilistic framework for sensorimotor
behavior that enjoyed sustained interest from computational neuroscientists.
However, its formulation has been criticized for its opacity and similarity to
optimal control [7, 8, 9], but is seemingly difficult to translate into an equally
effectivealgorithmicform. Inthiswork,weofferacriticalanalysisofAIfromthe
viewofcontrolasinference(CaI)[1,11,14,21,24,28], thesynthesisofoptimal
control and approximate inference. The goal is to appreciate the insights from
the AI literature, but in a form with computational and theoretical clarity.
2 Background
Here we outline the foundational theory and assumptions in this work.
1
2.1 Problem Formulation
Wespecificallyconsideraknownstochastic,continuous,discrete-time,partially-
observed,nonlinear,dynamicalsystemwithstatex∈Rdx,observationsy ∈Rdy
and control inputs u ∈ Rdu, operating over a time horizon T. We define
the states in upper case to denote the variables over the time horizon, i.e.
U = {u ,...,u }. The joint distribution (generative model) p(Y,X,U)
0 T−1
over these variables factorizes into several interpretable distributions: The dy-
namics p(x |x ,u ), observation model p(y | x ,u ), and behavior policy
t+1 t t t t t
p(u |x ).
t t
2.2 Variational Inference for Latent Variable Models
Inference may be described by minimizing the distance between the ‘true’ data
distributionp(·)andaparameterizedfamilyq (·)[17]. Apopularapproachisto
θ
minimize the Kullback-Liebler (KL) divergence, e.g. minD [q || p] w.r.t. θ.
KL θ
More complex inference tasks can be described by observationsy influenced by
unseen latent variables x. Given an observation y∗, maximizing the likelihood
involves integrating over the hidden states, and so is termed the marginal like-
lihoodp(y∗)= p(y=y∗,x)dx. Unfortunately this marginalizationis typically
intractable in clRosed-form. A more useful objective may be obtained by apply-
ing a variational approximation of latent state q (x | y∗) = q (x | y=y∗) to
θ θ
the logmarginallikelihoodandobtaininga lowerbound viaJensen’s inequality
[17]
log p(y∗,x)dx=log p(y∗,x)q q θ θ ( ( x x | | y y ∗ ∗ ) ) dx=logE x∼qθ(·|y∗) h q p θ ( ( y x ∗ | , y x ∗ ) )i , (1)
R R
≥E x∼qθ(·|y∗) h log q p θ ( ( y x ∗ | , y x ∗ ) )i =-D KL [q θ (x|y∗)||p(x,y∗))], (2)
=E x∼qθ(·|y∗) [logp(y∗ |x)]−D KL [q θ (x|y∗)||p(x)], (3)
where equations 2, 3 are variations of the ‘evidence lower bound objective’
(ELBO).Theexpectationmaximizationalgorithm(EM)[17],canbeunderstood
via Equation 3 as iteratively estimating the latent states (minimizing the KL
term via q) in the E step and maximizing the likelihood term in the M step.
3 Active Inference
Active Inference frames sensorimotor behaviour as the goal of equilibrium be-
tweenitscurrentanddesiredobservations,whichinpracticecanbeexpressedas
theminimizationofadistancebetweenthesetwoquantities. Thisdistanceisex-
pressedusing the KLdivergence,resultinginavariationalfreeenergyobjective
as described in Section 2.2. Curiously, AI is motivated directly by the ELBO,
whose negative is referred to in the AI literature as the ‘free energy’ F(·). The
minimizationofthisquantity, F(y∗,x,u)=D [q (x,u|y∗)||p(x,u,y∗)], as
KL θ
a model of behavior (i.e. state estimation and control), has been coined the
‘free energy principle’.
2
3.1 Free Energy of the Future
Despite the ELBO not being temporally restricted, AI delineates a ‘future’
free energy. This free energy is used to describe the distance between future
predicted and desired observations, where u is directly represented as a policy
u = π(x), so F(y∗,x |π) over the future trajectory is minimized. In active
t t
inference, π is commonly restricted to discrete actions or an ensemble of fixed
policies, so inferring p(π) can be approximated through a softmax σ(·) applied
tothe expected‘future’freeenergiesforeachpolicyovert=[τ,...,T−1],with
temperature γ and prior p(π)
p(π |Y∗)≈σ(logp(π)+γ T−1F(y∗,x ,|π)). (4)
t=τ t t
P
Moreover, for the ‘past’ where t = [0,...,τ −1], minimizing F(·) amounts for
state estimation of x given y. Another consideration is whether the dynamic
and observation models are known or unknown. In this work we assume they
are given, but AI can also include estimating these models from data.
3.2 Active Inference in Practice
InitialAIworkwasrestrictedtodiscretedomainsandevaluatedonsimplegrid-
world environments [5, 6]. Later work on continuous state spaces use various
black-box approaches such as cross-entropy [25], evolutionary strategies [26],
and policy gradient [16] to infer π. A model-based method was achieved by
using stochastic VI on expert data [3]. Connections between AI and CaI, per-
forming inference via message passing, have been previously discussed [13, 27].
AI has been applied to real robots for kinematic planning, performing gradient
descentonthefreeenergyusingthe Laplaceapproximationeverytimestep[18].
Despite these various approaches, AI has yet to demonstrate the sophisticated
control achieved by advanced optimal methods, such as differential dynamic
programming [20].
4 Control as Inference
From its origins in probabilistic control design [12], defining a state z ∈Rdz to
describe the desired system trajectory1 p(Z), optimal control can be expressed
as finding the state-action distribution that minimizes the distance for a gener-
ative model parameterized by θ, which can be framed as a likelihood objective
[17]
minD [p(Z)||q (Z)]≡max E [log q (Z,X,U)dXdU]. (5)
KL θ Z∼p(·) θ
R
Whenp(Z)simplydescribesadesiredstatez∗,sop(z )=δ(z −z∗),andthela-
t t t t
tent state-actiontrajectoryis approximatedby q (X,U), the objective (Equa-
φ
1whilezcouldbedefinedfrom[x,u]⊺,itcouldalsoincludeatransformation,e.g. applying
kinematicstojointspace-basedcontrolforacartesianspaceobjective.
3
tion 5) can be expressed as an ELBO where the ‘data’ is Z∗
maxE X,U∼qφ(·|Z∗) [logq θ (Z∗ |X,U)]−D KL [q φ (X,U |Z∗)|q θ (X,U)], (6)
whereφcapturesthe latentstateparameterizationandθ definesthe remaining
terms,i.e. thepriorsonthesystemparametersandlatentstates. Thisobjective
can be optimized using EM, estimating the latent state-action trajectory φ
in the E step and optimizing the remaining unknowns θ in the M step. By
exploiting the temporal structure, q (X,U | Z∗) can be inferred efficiently in
φ
theEstepbyfactorizingthejointdistribution(Equation7)andapplyingBayes
rule recursively
q (Z∗,X,U)=q (x ) T−1q (x |x ,u ) T q (z∗|x ,u )q (u |x ), (7)
φ φ 0 t=0 φ t+1 t t t=0 φ t t t φ t t
q (x ,u | Q z∗ )∝q (z∗ |x ,u ) Q q (x ,u |z∗ ), (8)
φ t t 0:t φ t t t φ t t 0:t−1
q (x ,u |z∗ )∝q (x ,u |x )q (x |z∗ ). (9)
φ t t 0:T φ t t t+1 φ t+1 0:T
Equations 8, 9 are commonly known as Bayesian filtering and smoothing [19].
The key distinction of this framework from state estimation is the handling of
u during the forward pass, as q (x ,u )=q (u | x )q (x ), control is incor-
φ t t φ t t φ t
porated into the inference. We can demonstrate this in closed-form with linear
Gaussian inference and linear quadratic optimal control.
4.1 Linear Gaussian Inference & Linear Quadratic Con-
trol
Whiletheformulationaboveisintentionallyabstract,itcanbegroundedclearly
by unifying linear Gaussian dynamical system inference (LGDS, i.e. Kalman
filtering and smoothing) and linear quadratic Gaussian (LQG) optimal con-
trol [22]. While both cases have linear dynamical systems, here LQG is fully-
observed2 and has a quadratic control cost, while the LGDS is partially ob-
served and has a quadratic log-likelihood due to the Gaussian additive un-
certainties. These two domains can be unified by viewing the quadratic con-
trol cost function as an Gaussian observation likelihood. For example, given
z =x +ξ,ξ ∼N(0,Σ) and z∗ =0∀t,
t t t
logq (z∗|x ,u )=-1(d log2π+log|Σ|+x⊺Σ-1x )=αx⊺Qx +β (10)
θ t t t 2 z t t t t
where(α,β)representstheaffinetransformationmappingthequadraticcontrol
cost x⊺Qx to the Gaussian likelihood. As convex objectives are invariant to
affine transforms, this mapping preservesthe controlproblem while translating
it into an inference one. The key unknown here is α, which incorporates Q
into the additive uncertainty ξ, Σ = αQ-1. Moreover, inference is performed
2Confusingly,LQGcanrefertobothGaussiandisturbanceand/orobservationnoise. While
all varieties share the same optimal solution as LQR, the observation noise case results in a
partiallyobservedsystemandthereforerequiresstateestimation. i2cismotivatedbytheLQR
solutionandthereforedoesnotconsiderobservationnoise,butitwouldbestraightforwardto
integrate.
4
by using message passing [15] in the E step to estimate X and U, while α is
optimizedintheMstep. ThisviewscalesnaturallytonotjustthetypicalLQG
cost x⊺Qx+u⊺Ru, but also nonlinear mappings to z by using approximate
inference. WhiletheclassicLQGresultincludesthebackwardRicattiequations
and an optimal linear control law, the inference setting derives direct parallels
tothe backwardpassduring smoothing[22]andthe linearconditionaldistribu-
tion of the Gaussian, q (u | x )=N(K x +k ,Σ ) [10] respectively. As the
θ t t t t t kt
conditional distribution is linear, updating the prior joint density p(x ,u ) in
t t
the forwardpasswith updated state estimate x′ correspondsto linear feedback
t
control w.r.t. the prior
p(u′)= p(u |x =x′)p(x′)dx′, (11)
t Z t t t t t
µ u′
t
=µ ut +K t (µ xt −µ x′
t
), (12)
Σ uu′ t =Σ uut −Σ uxt Σ- x 1 xt Σ⊺ xut +K t Σ xx′ t K t ⊺, (13)
K =Σ Σ-1 . (14)
t uxt xxt
FromEquation14,itisevidentthatthestrengthofthefeedbackcontroldepends
onboththe certaintyinthestateandthe correlationbetweentheoptimalstate
and action.
The generalEMalgorithmforobtainingq (x,u)fromp(Z)isreferredtoas
θ
inputinferenceforcontrol(i2c)[28]duetoitsequivalencewithinputestimation.
Note that for linear Gaussian EM, the ELBO is tight as the variational distri-
bution is the exact posterior. For nonlinear filtering and smoothing, mature
approximateinference methods suchas Taylorapproximations,quadratureand
sequentialMonteCarlomaybeusedforefficientandaccuratecomputation[19].
Another aspect to draw attention to is the inclusion of z compared to alter-
native CaI formulations, which frame optimality as the probability for some
discrete variable o, p(o=1 | x,u) [14]. Previous discussion on CaI vs AI have
framed this discrete variable as an important distinction. However, it is merely
a generalizationto allow for a general cost function C(·) to be framed as a log-
likelihood, i.e. p(o=1 | x,u) ∝ exp(−αC(x,u)). For the typical state-action
costfunctionsthatareadistancemetricinsometransformedspace,thekeycon-
sideration is the choice of observation space z and corresponding exponential
density.
5 The Unifying View: Control of the Observa-
tions
Akey distinction to the AI andCaI formulationsdescribedaboveis that, while
AI combines state estimation and control with a unified objective, CaI focuses
on trajectory optimization. However, this need not be the case. In a similar
fashion to the partially-observed case of LQG, CaI also naturally incorporates
5
observations [23]. As Section 4 describes i2c through a general Bayesian dy-
namical system, the formulation can be readily adapted to include inference
using past measurements. Moreover, as i2c frames the control objective as an
observation likelihood, when z and y are the same transform of x and u, the
objective can also be unified and directly compared to active inference. For
‘measurements’ Y∗ = {y∗,...,y∗ ,z∗,...,z∗ }, following Equation 5 using
0 τ-1 τ T−1
the F(·) notation
τ-1 T-1
minD [p(Y)||q (Y)]=min F (y∗,x ,u )+ F (z∗,x ,u ), (15)
KL θ ψ t t t ψ t t t
X X
t=0 t=τ
stateestimation optimalcontrol
| {z } | {z }
where ψ = {θ,φ}. Here, p(y )=δ(y −y∗) now also describes the empirical
t t t
density of pastmeasurements y∗ . The crucialdetailfor this representationis
<τ
that the observation model q (y | x ,u ,t) is now time dependent, switching
θ t t t
from estimation to control at t = τ. For the Gaussian example in Section 4.1,
Σ is the measurement noise and Σ-1 =αQ. A benefit of this view is that
<τ ≥τ
the computation of active inference can now be easily compared to the classic
resultsofKalmanfilteringandLQG(Fig. 1),andalsoscaledtononlineartasks
Prior Posterior y∗ z∗ lqr
x
x
1
2
0 20 40 60 80 100
t
u
Figure 1: Linear Gaussiani2c performing state estimationand controlfollow-
ing Section 5, with state x=[x ,x ]⊺, action u and [x,u]⊺ as the observation
1 2
space. With τ = 50, for t < τ i2c performs state estimation under random
controls. For t≥τ, i2c switches to optimalcontrol. This example is in the low
noise setting, with a large prior on u, to illustrate that i2c returns the LQR
solution for the same initial state and planning horizon.
6
through approximate inference. Moreover, obtaining the policy π(·) using the
jointdistributionq (x ,u ) is arguablya more informedapproachcomparedto
θ t t
direct policy search on an arbitrary policy class.
6 Conclusion
We have derived an equivalent formulation to active inference by consider-
ing partially-observed, inference-based optimal control, which has a principled
derivation and is well-suited for approximate inference. While we have delin-
eated state estimation as operating on past measurement and control as plan-
ning future actions (Equation 15), both AI and i2c demonstrate the duality
between estimation and control due to the mathematical similarity when both
aretreatedprobabilistically. WehopetheinclusionoftheCaIliteratureenables
a greater theoretical understanding of AI and more effective implementations
through approximate inference.
References
[1] Attias,H.: Planningbyprobabilisticinference.In: International WorkshoponArtificial
IntelligenceandStatistics(2003)
[2] Biehl, M., Guckelsberger, C., Salge, C., Smith, S.C., Polani, D.: Expanding the active
inference landscape: Moreintrinsicmotivations inthe perception-action loop. Frontiers
inNeurorobotics (2018)
[3] Catal, O., Nauta, J., Verbelen, T., Simoens, P., Dhoedt, B.: Bayesian policy selection
using active inference. In: ICLR Workshop on Structure & Priors in Reinforcement
Learning(2019)
[4] Friston, K.: The free-energy principle: a unified brain theory? Nature reviews neuro-
science(2010)
[5] Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck, P.,Pezzulo,G.: Activeinference:
aprocesstheory.Neuralcomputation (2017)
[6] Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforcement learning or active inference?
PloSone(2009)
[7] Gershman, S.J.: What does the freeenergy principletell us about the brain? Neurons,
Behavior,Dataanalysis,andTheory(2019)
[8] Guzm´an, N.: twitter.com/NoahGuzman14/status/1259953086241492992, Accessed:
2020-06-17
[9] Herreros, I., Verschure, P.F.: About the goal of a goals’ goal theory. Cognitive Neuro-
science(2015)
[10] Hoffmann,C.,Rostalski,P.: Linearoptimalcontrolonfactorgraphs-amessagepassing
perspective-.International FederationofAutomaticControl(2017)
[11] Kappen,H.J.: Pathintegralsandsymmetrybreakingforoptimalcontroltheory.Journal
ofStatistical Mechanics: TheoryandExperiment(2005)
[12] K´arny`,M.: Towardsfullyprobabilisticcontroldesign.Automatica(1996)
[13] vandeLaar,T.,O¨zc¸elikkale,A.,Wymeersch,H.: Applicationofthefreeenergyprinciple
toestimationandcontrol.arXivpreprintarXiv:1910.09823(2019)
[14] Levine, S.: Reinforcement learning and control as probabilistic inference: Tutorial and
review.arXivpreprintarXiv:1805.00909(2018)
7
[15] Loeliger, H.A., Dauwels, J., Hu, J., Korl, S., Ping, L., Kschischang, F.R.: The factor
graphapproachtomodel-basedsignalprocessing.Proc.oftheIEEE(2007)
[16] Millidge, B.: Deep active inference as variational policy gradients. arXiv preprint
arXiv:1907.03876(2019)
[17] Murphy,K.P.: Machinelearning: aprobabilisticperspective.MITpress(2012)
[18] Oliver, G., Lanillos, P., Cheng, G.: Active inference body perception and action for
humanoidrobots.arXivpreprintarXiv:1906.03022(2019)
[19] Srkk,S.: BayesianFilteringandSmoothing.CambridgeUniversityPress(2013)
[20] Tassa, Y., Erez, T., Todorov, E.: Synthesis and stabilization of complex behaviors
through online trajectory optimization. In: International Conference on Intelligent
RobotsandSystems.IEEE(2012)
[21] Todorov, E.: Linearly-solvablemarkovdecision problems. In: Advances inneural infor-
mationprocessingsystems(2007)
[22] Toussaint, M.: Robot trajectoryoptimization usingapproximate inference. In: Interna-
tionalconferenceonmachinelearning(2009)
[23] Toussaint, M., Charlin, L., Poupart, P.: Hierarchical pomdp controller optimization by
likelihoodmaximization.In: UncertaintyinArtificialIntelligence (2008)
[24] Toussaint, M., Storkey, A.: Probabilistic inference for solving discrete and continuous
state Markov Decision Processes. In: International Conference on Machine Learning
(2006)
[25] Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Reinforcement learning through
activeinference.arXivpreprintarXiv:2002.12636(2020)
[26] Ueltzho¨ffer,K.: Deepactiveinference.BiologicalCybernetics (2018)
[27] deVries,B.,Friston,K.J.: Afactorgraphdescriptionofdeeptemporalactiveinference.
FrontiersinComputational Neuroscience(2017)
[28] Watson,J.,Abdulsamad,H.,Peters,J.: Stochasticoptimalcontrolasapproximateinput
inference.In: ConferenceonRobotLearning(2019)
8