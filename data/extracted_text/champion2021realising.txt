1202
rpA
32
]GL.sc[
1v89711.4012:viXra
Realising Active Inference in Variational Message Passing:
the Outcome-blind Certainty Seeker
Th´eophile Champion TMAC3@KENT.AC.UK
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´s M.GRZES@KENT.AC.UK
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.BOWMAN@KENT.AC.UK
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a state-of-the-artframeworkinneurosciencethat offersa unified theory
ofbrainfunction. ItisalsoproposedasaframeworkforplanninginAI.Unfortunately,the
complex mathematics required to create new models — can impede application of active
inference in neuroscience andAI research. This paper addressesthis problemby providing
a complete mathematical treatment of the active inference framework — in discrete time
and state spaces — and the derivation of the update equations for any new model. We
leveragethetheoreticalconnectionbetweenactiveinferenceandvariationalmessagepassing
as describe by John Winn and Christopher M. Bishop in 2005. Since, variational message
passing is a well-defined methodology for deriving Bayesian belief update equations, this
paper opens the door to advanced generative models for active inference. We show that
using a fully factorized variational distribution simplifies the expected free energy — that
furnishespriorsoverpolicies—sothatagentsseekunambiguousstates. Finally,weconsider
futureextensionsthatsupportdeeptreesearchesforsequentialpolicyoptimisation—based
upon structure learning and belief propagation.
Keywords: Active Inference, Variational Message Passing,Free Energy Principle, Rein-
forcement Learning, Kullback Leibler Control
1. Introduction
The free energy principle aims to provide a unified theory of the brain based on Bayesian
probability theory (Friston, 2010; Buckley et al., 2017). It takes root in Helmholtz’s ar-
gument that observations are produced by hidden causes that must be inferred — and
the predictive coding formulation which argues that inference and learning emerges from
the reduction of the error between predicted and actual observations. Active inference
extends predictive coding to consider generative models of actions (Friston et al., 2016;
Da Costa et al., 2020a).
1
Champion et al.
In brief, active inference is a probabilistic framework that describes how agents should
act in their environment. It starts with the definition of a generative (probabilistic) model
that encodes the agent’s beliefs about its environment. However, active inference does not
rely on one particular generative model, instead it refers to a class of generative models
that consider the impact of their actions in their environment. Active inference also relies
on learning and inference to estimate the most likely states of the world and values of the
model parameters. However, the concept behind active inference does not dependent on a
particularinferencemethod,whichmeansthatbothvariationalinference(Fox and Roberts,
2012) and Monte Carlo Markov chains (Fountas et al., 2020) can, in principle, be used.
Active inference has been successfully applied in neuroscience to explain a widerange of
brainphenomenasuchashabitformation(Friston et al.,2016),Bayesiansurprise(Itti and Baldi,
2009),curiosity(Schwartenbeck et al.,2018),anddopaminergicdischarges(FitzGerald et al.,
2015). Active inference is also a form of planning as inference (Botvinick and Toussaint,
2012) consistent with Occam’s Razor (Blumer et al., 1987) and can be seen as a generali-
sation of reinforcement learning (van Hasselt et al., 2015; Lample and Chaplot, 2016) and
KullbackLeiblercontrol(Rawlik et al.,2013). Thisframeworkhasalsobeenusedtoground
active vision(Ognibene and Baldassare,2015;Heins et al.,2020;Van de Maele et al.,2021;
Mirza et al., 2016, 2018) within a strong theoretical framework.
This paper focuses on active inference using variational (a.k.a approximate Bayesian)
inference and highlights its connection to variational message passing (Winn and Bishop,
2005). This ubiquitous message passing algorithm builds on the variational inference lit-
erature by leveraging the structure of the generative model to split the update equations
into messages. Those messages transmit information about the new observations and —
by summing those messages — it is possible to compute the posterior distribution over the
parameters. The decomposition of the updates into messages formalises the modularity of
the method, while remaining biologically plausible (Friston et al., 2017b). Indeed, a key
question in machine learning and computational neuroscience is how to identify composi-
tional models — an issue that was identified early in the development of connectionism
(Bowman and Li, 2011; Fodor and Pylyshyn, 1988). The central requirement being that
higher-order representations (whether syntactic, semantic, perceptual, etc) can be con-
structed by “plugging together” lower order representations, in such a way that the mean-
ings of lower-order representations do not change (e.g. the “Jane” in “Jane loves John” is
the same “Jane” as in “John loves Jane”). It may be that the structural modularity pro-
vided by message passing implementations of Bayesian networks enable compositionality of
representations. According to modern trends, we use the formalism of Forney factor graphs
(Forney, 2001) to represent the updates as messages sent along the graph edges.
Forney factor graphs are graphical representations used to realise generative models.
They comprise of two kinds of round nodes that represent the observations and the latent
variables of themodel. Ifthenotion of observations can beunderstoodas thedata available
to the model, the notion of latent variables is a bit more abstract. As an example, let us
considertheMNISTdataset(LeCun and Cortes,2010)composedof images of handwritten
digits. In this example, the pixels are observations made by the model and latent variables
could be any variables encoding the digit being represented, such as its orientation or size.
The last type of nodes — square nodes — represent the dependency between observed and
latent variables. In other words, how does the digit being represented generate the pixels?
2
Active Inference and Variational Message Passing
The first goal of this paper is to provide the reader with a full intuition of the mathe-
matics underlyingactive inference and variational message passing. Then, this paper shows
how to derive the update equations for any new generative models. The hope is to facil-
itate the development of new models that could, for example, play Atari games or model
new brain mechanisms. Finally, we use our new generative model to prove that the up-
date equations of active inference can be understood as variational message passing. This
formal proof complements previous work that frames active inference as belief propagation
(Friston et al., 2017b) and enables us to create an automatic and modular implementation
of active inference (van de Laar and de Vries, 2019a; Cox et al., 2019). This message pass-
ingformulation hasparticularconsequences fortheexpectedfreeenergy, whichiseffectively
reduced by the change, resulting in an agent that seeks certainty, without any concern for
outcomes, whether preferred or not. We argue that the resulting behaviour may have simi-
larities to repetitive actions (sometimes called stimming) that are common, for example, in
autism (Gabriels, 2005).
Section 2 describes the problem used to present the (classic) model widely used in the
active inference literature. Sections 3 and 4 introduce variational inference and Forney
factor graphs, respectively. Next, Section 5 presents active inference as a decision theory
based on the Bayesian view of probability, followed by Section 6 that introduces the notion
of variational message passing. Then, Section 7 formulates active inference as variational
message passing under a fully factorised approximate posterior (i.e. variational distribu-
tion), and explains the implications of this approximation for the expected free energy that
underwrites policy selection. Before starting the next section, readers new to the active
inference literature might want to read Appendix D, which uses Bayes theorem to present
the simplest generative model sufficient for active inference.
2. Problem statement
Active inference crops up in many areas that require an agent to interact with its envi-
ronment. Throughout this paper, the explanations will be based on an agent named Bob,
whose goal is to solve the food problem presented in section 2.2. But before we investigate
this problem, let us have a look at how to simulate the interaction between Bob and his
environment.
2.1 Simulating active inference
Most living beings are able to sense their environment through sensory inputs, and process
this sensory information to act in the world. For example, carnivorous flowers use tiny
trigger hairs on their leaves to detect flies (sensing). When those hairs are stimulated,
the ion concentrations in the leaves increase (processing) resulting in an electrical current
that closes the leaf trapping the fly (acting). Similarly, humans gather sensory information
throughtheirfivesenses(sensing),processthisinformationtounderstandtheirenvironment
(processing), and finally, make use of this understanding to act with intelligence (acting).
Sensing,processingandactingcorrespondtothethreestepsoftheAction-Perceptioncy-
cle. Thiscycleconvenientlycastsactiveinferenceasaninfiniteloop(van de Laar and de Vries,
2019b). Each iteration begins by samplingthe environmentto obtain an observation, which
is provided to the agent. Then, the observation is used to perform inference (and learning)
3
Champion et al.
that produce a higher level of understanding, for example, an image might be mapped to a
representation of the objects that it contains. And finally, this representation is exploited
when acting to prepare your diner, drive your kids to school or solve your favourite maths
problem.
2.2 The food problem
Speaking of which, this section is concerned with the food problem initially proposed by
Oleg Solopchuk (2018). This problem concerns an agent, named Bob, striving to survive.
To producethe energy needed by his body, Bob needs to ingest nutriments. During periods
of starvation, Bob’s stomach produces an hormone called ghrelin. This hormone travels to
the brain through the blood and reaches a part of the brain, named the hippocampus. This
area has been shown to monitor the level of ghrelin in the blood (Kojima and Kangawa,
2005). At the moment ghrelin reaches the hippocampus, Bob’s brain can estimate the
content of his stomach. This information can then be exploited to choose between eating
and sleeping. However, the best action dependson the outcomes that Bob wants to witness
in the future. This paper assumes that mother nature has kindly set Bob’s preferences to
bebiased towards thesensation of feeling fed(i.e. Bob enjoys observinglow levels of ghrelin
in his blood), which is arguably a favourable traits under a Darwinism view of evolution.
Figure 1 summarises the food problem.
Observation: fed vs hungry
Bob
Hidden state: full vs empty
Action: eat vs sleep
Figure 1: This figure illustrates the food problem, where the goal of our agent — Bob — is
tokeep hisstomach full. ThefirstthingBobneedstoachieve hisgoal istoguessthestate of
his stomach, which can either be full or empty. This guess is informed by the observations
he makes, when feeling hungry (high level of ghrelin) or fed (low level of ghrelin). Finally,
onceBob hasreducedhisuncertainty abouthisstomach state, hecan engage inexploitative
behaviour by taking action in his environment, such as sleeping or eating.
4
Active Inference and Variational Message Passing
3. Variational Inference
In Bayesian statistics, one assumes a prior distribution over latent (a.k.a hidden) variables
thatrepresenttheprocessgeneratingthedata. Whencollectingmoredata,newobservations
bring information, allowing us to update our prior knowledge. The process of computing
the most likely values of the hidden variables is called inference. A simple inference method
is to use Bayes theorem to obtain the posterior probability distribution over the latent
variable(s) of the model:
likelihood prior
P(O|S)P(S) P(O|S)P(S)
P(S|O) = = .
P(O) P(O|S)P(S)
z }| {z}|{ S
posterior
evidence P
| {z }
Since Bayes theorem is a corollary|o{fzth}e product rule of probability and no approxima-
tion is needed, it belongs to the field of exact inference. However, the computation of the
evidence requires the marginalisation over all hidden variables, which makes it intractable
for all but the simplest models.
To address this intractability, one can turn to approximate or sampling based methods.
Variational inferencebelongstotheformerandreliesonanassumptionofindependence. As
willbeexplained in Section 6.1, theideabehindvariational inferenceis to usea distribution
Q(S) to approximate the true posterior P(S|O). This can be accomplished by minimising
the Kullback-Leibler (KL) divergence between some approximate and the true posterior:
D [Q(S)||P(S|O)].
KL
Minimising this KL divergence is impossible because the true posterior P(S|O) is un-
known. Fortunately however, it is equivalent to minimising the variational free energy F,
known in machine learning as the negative evidence lower bound (ELBO). The variational
freeenergyisdefinedastheKullback-Leiblerdivergencebetweenthevariationaldistribution
Q(S) and the generative model P(O,S):
F = D [Q(S)||P(O,S)] = −ELBO
KL
= D [Q(S)||P(S|O)]+lnP(O).
KL
The variational distribution Q(S) is used to approximate the true posterior P(S|O). In
addition to the introduction of this approximate posterior, the mean-field approximation
makes the computation tractable by assuming that all latent variables are independent:
Q(S) = Q (S ),
i i
i
Y
where Q (S ) is the distribution over the i-th hidden state of the model and Q(S) is the
i i
joint distribution over all latent variables. This assumption of independence constrains the
expressiveness of thevariational distribution, butallows thederivation of updateequations,
which can be evaluated efficiently.
At this point, an analogy might be useful to furnish an intuitive understandingof varia-
tional inference. Imagine you drop some coffee on a table, producinga stain with acomplex
5
Champion et al.
shape. To compute the area of the stain, it might be useful to first assume an elliptic shape
for the stain. However, since the stain is not actually elliptic, the solution will only be
an approximation. In this analogy, the stain is the true posterior, and the ellipse is the
approximate posterior.
This analogy should help with the understanding of Figure 2 that illustrates the kind
of results obtained by variational methods. As will be demonstrated in Section 6.2, it
is possible to prove (Fox and Roberts, 2012) that minimising the variational free energy F
withrespecttoQ (S )can beperformedbyiterating oneof thefollowing updateequations:
k k
lnQ (S )← lnQ∗(S )= hlnP(O,S)i (1)
k k k k ∼Qk
1
⇔ Q (S )← Q∗(S )= exphlnP(O,S)i ,
k k k k Z ∼Qk
where Q∗
k
(S
k
) is the optimal posterior, Z is a normalisation constant and h•i
∼Qk
is the
expectation over all factors but Q . Importantly, it is the coupling of the above update
k
equations (i.e. one updateper hidden variable S ) that justifies the iteration of the updates
k
until convergence to the free energy minimum.
P(S|O) Q(S)
S
Figure 2: This figure illustrates the kind of result obtained using variational inference. The
true posterior drawn in red has a complex shape and is approximated by the variational
distributiondrawninblue. Thegrey areadepicts theerrormadewhenusingthevariational
distribution to approximate the true posterior.
4. Forney Factor Graphs
Typically,generativemodelsarerepresentedgraphicallyusingagraphicalmodel(Koller and Friedman,
2009) or Forney factor graph (Forney, 2001). This section focuses on the latter represen-
tation introduced by David Forney in 2001, which uses three kinds of nodes. The nodes
representing hidden and observed variables are depicted by white and grey circles, respec-
tively. And factors are represented using white squares, which are linked to variable nodes
by arrows or lines. Arrows are used to connect factors to their target variable, while lines
link factors to their predictors. Figure 3 shows an example of a Forney factor graph corre-
sponding to the following generative model:
P(O,S) = P (O|S)P (S). (2)
O S
Generally, factor graphs only describe the model’s structure — in terms of the variables
and their dependencies — but not the individual factors. For example, the definitions of
6
Active Inference and Variational Message Passing
P and P are not given by Figure 3, and additional information is required, e.g. P (S) =
O S S
N(S;µ,σ) specifies P as a Gaussian distribution.
S
Initially, variables could only connect to a limited number of factors. However, a special
kind of factor, called an equality node, dissolves this limitation. Purists tend to represent
all equality nodes, while others make them implicit by allowing the variables to connect to
an arbitrary number of factors. For sake of clarity, this paper keeps equality nodes implicit.
Finally, factors — along with hidden and observed variables — are sometimes called
constraint, state and symbol, respectively. As explained by Yedidia (2011), those two
terminologies refer to two views on Forney factor graphs, where factors encode probabilities
and constraints encode costs. Infinite costs represent hard constraints, while finite costs
encode soft constraints. Here, hard constraints define which configurations of the state
space are forbidden (i.e. has a probability of zero) and soft constraints encode preferences
overthestateconfigurations(i.e. thehigherthecostthesmallerthestateprobability). This
reveals an interesting link between Bayesian statistics and symbolic artificial intelligence,
and promptsthe question of whetherBayesian statistics can beregarded as a generalisation
of symbolic artificial intelligence. For example, one could start by framing the problem of
constraint satisfaction, as an inference process on a Forney factor graph that encodes the
problem constraints.
P
S
Hidden variable
Line
S
Factor
Arrow
P
O
Observed variable
O
Figure 3: This figure illustrates the Forney factor graph corresponding to the following
generative model: P(O,S) = P (O|S)P (S). The hidden state is represented by a white
O S
circle with the variable’s name at the centre, and the observed variable is depicted similarly
butwithagrey background. Thefactors of thegenerative modelarerepresented bysquares
with a white background and the factor’s name at the centre. Finally, arrows connect the
factors to their target variable and lines link each factor to its predictor variables.
5. Active Inference
So far, we have discussed variational inference and Forney factor graphs. We now present
the intuition behind the various equations that comprise the active inference framework.
We will be working with the food problem that was introduced in Section 2.
7
Champion et al.
5.1 Generative model
We begin by presenting the generative model introduced by Friston et al. (2013). Instead
of presenting the full generative model at once, the next subsections build this model pro-
gressively. This should help the reader to understand both the model and its corresponding
Forney factor graph.
5.1.1 The D vector
As we shall see shortly, the full generative model represents the world as a sequence of
hidden states, and those states generate the observations made by the agent. For the sake
of organisation, those states are arranged chronologically using the index τ that runs from
the initial state (S ) to the state of the last time step (S ). This section focuses on the
0 T
initial state, whose distribution is a categorical, defined as follows:
P (S |D) = Cat(S ;D), (3)
S0 0 0
where D is a vector containing the parameters of the categorical distribution. In addition
to the categorical distribution, the model assumes a Dirichlet prior over the parameters D,
leading to:
P (D) = Dir(D;d). (4)
D
Inthiscontext,theparametersdoftheDirichletdistributionarecalledhyperparameters,
because they control the distribution of the parameters D. Figure 4 summarises this part
of the model by presenting an example of the vector D, and the Forney factor graph
corresponding to the two distributions constituting Bob’s generative model.
PD
D
D
=
PS0
S
0
states
P S
( 0 = empty)
0.2
0.8
P S
( 0 = full)
Figure 4: This figure illustrates the vector D that defines Bob’s beliefs about the initial
hiddenstate,andtheForneyfactorgraphcorrespondingto(3)and(4). Sincetheprobability
of S being full is higher than the probability of it being empty, Bob thinks that at the
0
beginning of each trial, his stomach is more likely to be full than empty.
8
Active Inference and Variational Message Passing
5.1.2 The A matrix
We have already mentioned that the probability of an observation (a.k.a outcome), such
as feeling hungry, depends on the value of the hidden state, i.e. whether Bob’ stomach is
full or empty. This dependency is represented by a conditional distribution, such that the
likelihood of an observation — given a particular value of the hidden states — is defined
by a categorical distribution, as follows:
P (O |S = j,A) = Cat(O ;A ),
Oτ τ τ τ •j
where the j-th column of A, denoted A , contains the parameters of the categorical distri-
•j
bution encoding the probability of the outcomes given that S = j. Additionally, we can
τ
re-write the above equation more concisely by letting S be a one hot vector, whose j-th
τ
element is equal to one, such that:
P (O |S ,A) = Cat(O ;AS ),
Oτ τ τ τ τ
wherebecauseS is a onehot vector, the multiplication of A and S selects the j-th column
τ τ
of A. Similarly to the treatment of the vector D, a prior over the columns of A is used. To
ensure the conjugacy between the distributions of the model, a Dirichlet prior is used for
each column. The probability of the overall matrix is then given by the following product
of Dirichlet:
P (A) = Dir(A ;a ),
A •i •i
i
Y
whereaisamatrixcontainingtheparametersoftheDirichletdistributions,i.e.,eachcolumn
of a contains the parameters of one Dirichlet distribution. Note that because each column
of the matrix A is a categorical distribution, then the conjugate prior of each column is
a Dirichlet distribution. Assuming independence of the columns of A, the conjugate prior
of the entire matrix A is a product of Dirichlet distributions. Importantly, the prior over
A is not a Dirichlet distribution whose parameters are obtained by concatenation of the
columns of A. Indeed, if we sample from such a (concatenated) prior, then the elements
of the entire matrix will sum up to one but the columns would not. This is problematic
because each column of A is supposed to be a categorical distribution that sum up to one.
We conclude this section with Figure 5 that illustrates the likely matrix A, along with the
resulting version of the generative model for Bob’s problem.
9
Champion et al.
PD
P(O τ =hungry|S τ =empty) P(O τ =hungry|S τ =full)
D
0.9 0.2
PS0
A = S
0
0.1 0.8
PA A PO0
O
P(O τ =fed|S τ =empty) P(O τ =fed|S τ =full) 0
outcomes
states
Figure 5: This figure illustrates the matrix A that defines how the hidden states generate
the observations. In our example with Bob, this matrix defines the probability of Bob
feeling hungry or fed while his stomach is full or empty. Furthermore, the new version of
the generative model is shown on the right.
5.1.3 The B matrices
NowthatthereaderisfamiliarwiththedefinitionofthelikelihoodmatrixA,wefocusonthe
temporal transitions between any pair of successive states. Those transitions are modelled
similarly to the matrix A that concerns the generation of observations from hidden states.
However here, we are concerned with the transition matrices that maps from states at one
point on time to the next. Crucially, there are as many of these matrices as the number
of allowable actions on the state in question. This follows from the idea that each action
has the potential to modify Bob’s stomach differently: for example, eating is more likely
to change Bob’s stomach from empty to full than sleeping. Accordingly, the transition
between two consecutive hidden states is defined by a set of matrices, called the transition
or B matrices, such that:
P (S |S = i,π = j,B) = Cat(S ;B[Uj] )
Sτ+1 τ+1 τ τ+1 τ •i
=∆ Cat(S ;B[U] ), (5)
τ+1 •i
where =∆ means equal by definition, U =∆ Uj is the action predicted at time step τ by the
τ
j-th policy, and B[U] is the matrix corresponding to the action U. Furthermore, active
inference defines policies as action sequences (cf. next section). By replacing the index i by
a one hot vector as in the previous section, Equation 5 can be re-written as:
P (S |S ,π,B) = Cat(S ;B[U]S ).
Sτ+1 τ+1 τ τ+1 τ
A Dirichlet prior is assumed for each column of the transition matrices B, leading to
the following prior:
P (B) = Dir(B[i] ;b[i] ),
B •j •j
i,j
Y
where b are the parameters of the Dirichlet distributions, i and j iterate over all possi-
ble actions and states, respectively. Finally, Figures 6 and 7 conclude this subsection by
illustrating the matrices B, and the updated version of the generative model.
10
Active Inference and Variational Message Passing
P(S τ+1=empty|S τ =empty)
0.1 0.2
B[eating] =
0.9 0.8
P(S τ+1=full|S τ =empty)
states
states P(S τ+1=empty|S τ =full)
0.9 0.8
B[sleeping] =
0.1 0.2
P(S τ+1=full|S τ =full)
states
states
Figure 6: This figure illustrates the matrices B that define the transition between any
two consecutive hidden states. In the context of the food problem, those matrices encode
the probability of transitioning from a full or empty stomach at time τ to a full or empty
stomach at time τ +1.
PD PB Pπ
D B π
PS0
S ... S ... S
0 PSt t PST T
PA
PO0 A POt
O O
0 t
Figure 7: This figure shows the next version of the generative model, where the transition
between hidden states is specified by a set of B matrices and the policies π. At this point,
it should be emphasized that the generation of outcomes through the matrix A stops after
the current time step t. This follows naturally from the idea that we cannot observe future
outcomes. Finally, the factor P has not been defined yet: it will be the subject of the next
π
section.
5.1.4 The prior over policies
We now consider the prior over the policy that was left undefined in Figure 7. But what do
we exactly mean by policies? In active inference, a policy is a sequence of actions over time,
i.e. {U ,...,U }. As a consequence, even if the agent expects the environment to be in
t T−1
the same state at two different time steps, picking two different actions at those time steps
is still possible. Therefore, an active inference agent can perform an epistemic action as
long as there is some uncertainty to be reduced and then switch to exploitative behaviours.
Note that this definition of policy is in opposition to most of the model-free reinforcement
learning literature, where a policy is a mapping from states to actions. In particular, states
11
Champion et al.
in the context of model-free reinforcement learning are observed and therefore are closer to
the notion of observations in active inference. Technically, active inference takes us out of
the world of fixed state-action policies (wherethe same action is taken from each state) into
the world of sequential policy optimisation, where different actions can be taken from the
same state — crucially, in a way that depends upon (Bayesian) beliefs about hidden states.
The last ingredient required to obtain the prior over the policies is a notion of policy
quality. In active inference, good policies are the ones that minimise the expected free
energy; that is, the free energy expected in the future, which is defined as follows:
expected outcomes priorpreferences
T
G(π) ≈ D [ Q(O |π) || P(O ) ] + E [H[P(O |S )]] , (6)
KL τ τ Q(Sτ|π) τ τ
" #
τ=t+1
X z }| {risk z }| { ambiguity
| {z } | {z }
whereH[·]istheShannonentropy, Gisavector containing asmanyelements asthenumber
of policies, and the i-th element of G represents the quality of the i-th policy. The reader
interested inthederivation oftheexpectedfreeenergyisreferredtoAppendixC.Weshould
mention here that Q(O |π) and Q(S |π) are computed based on the result of the inference
τ τ
process of the previous action-perception cycle. Therefore, G can be regarded as a model
parameter and is not represented as a random variable in the Forney factor graph. The
definition and justification of the expected free energy are provided in Appendix C and a
recent paper by Millidge et al. (2020). Also, the expected free energy arises naturally in
mathematical treatments of the free energy principle, when considering self-organisation at
non-equilibrium steady-state (Friston, 2019; Parr et al., 2020). At this point, we should
take a moment to understand the intuition behind the expected free energy.
Let us begin with the second term of Equation 6. For each value of the hidden state,
P(O |S = i) is a categorical distribution whose parameters correspond to the i-th column
τ τ
of A. This distribution defines the probability of future outcomes. Thus, the closer this
distribution is to a uniform distribution, the more uncertain we are about future outcomes.
This uncertainty is measured by the Shannon entropy, and the average of this quantity over
all possible values of S is called the ambiguity. Therefore, the ambiguity quantifies the
τ
degree to which a particular observation disambiguates among its hidden or latent causes.
Next, we need to encode Bob’s preferences over future outcomes, which are called prior
preferences. Formally, those preferences are defined as a categorical distribution whose
parameters are stored in the vector C. Figure 8 illustrates this vector. It should be noted
that those preferences define the goodness of future outcomes, and we shall come back
to this when discussing the link between active inference and reinforcement learning, cf.
Appendix A.
12
Active Inference and Variational Message Passing
P(O τ =hungry)
0.2
C =
0.8
P(O τ =fed)
Figure8: Thisfigureillustrates thevector C thatdefinesBob’spriorpreferencesover future
outcomes. This vector corresponds to the case where Bob prefers to feel fed rather than
hungry, and the intensity of those preferences can be changed by tweaking the probabilities
of the vector C. For example, C = (0,1) corresponds to an extreme preference towards
feeling fed.
To conclude, we need to consider the predicted or expected outcomes. One way to
predict future outcomes would be to compute the marginal distribution over O using for
τ
example the sum product algorithm (Kschischang et al., 2001). However, this might be
computationally expensive, so we will proceed with the following formula:
Q(O |π) = P(O |S = i,A)Q(S = i|π) = Asπ,
τ τ τ τ τ
i
X
where as will be discussed in Section 5.2, Q(S |π) =∆ Cat(S ;sπ). This equation can be
τ τ τ
understood as a form of marginalization, where the approximate posterior Q(S |π) is our
τ
most informed belief about the hidden states. Finally, the KL divergence between the
expected outcomes and the prior preference is called risk (cf. Appendix A for additional
details). Theriskpartofexpectedfreeenergyissimplythedivergencebetweentheexpected
outcomesandthepreferredoutcomes. Itisthispartofexpectedfreeenergythatunderwrites
policies thatlead topreferredoutcomes underuncertainty. Minimisingexpected freeenergy
therefore minimises risk (i.e., the divergence between anticipated and preferred outcomes)
and ambiguity (i.e., the conditional uncertainty about outcomes, given the causes). The
resulting prior over the policies is defined as:
P (π|γ) = σ(−γG),
π
whereσ(·)isthesoftmaxfunction,Gistheexpectedfreeenergy,γ determinesthesensitivity
of policy selection to the expected free energy of each policy, and the negative sign gives
high probability to policies minimising expected free energy. Importantly, the prior over
policies is an empirical prior because the expected free energy depends on the observations,
which means that it must be re-evaluated each time a new observation is made by the
agent. In other words, the prior over the policies is a Boltzmann distribution with γ being
the inverse temperature. Taking this view, small values for γ means a high temperature
and less precise prior beliefs about which policy should — or is — being pursued. Figure 10
shows an example of this distribution and Figure 9 illustrates the current generative model.
13
Champion et al.
Pγ
γ
PD PB Pπ
D B π
PS0
S0 ... PSt S t ... PST S T
PA
A
PO0 POt
O0 O
t
Figure 9: This figure illustrates the Forney factor graph of the entire generative model
of the sort presented by Friston et al. (2016). Section 5.1.1 described how the probability
of the initial states is defined by the vector D, and as discussed in Section 5.1.2, the
matrix A defines the probability of the observations given the hidden states. Section 5.1.3
explained that the B matrices define the transition between any successive pair of hidden
states. This transition depends on the action performed by the agent, i.e. on the policy π.
Furthermore, the prior over the policies has been chosen in Section 5.1.4, such that policies
minimising the expected free energy are more probable. Finally, we see in section 5.1.5 that
the precision parameter γ (which modulates the stochasticity of the agent behaviour) is
distributed according to a gamma distribution.
P(π|γ)
π1={Ut 1:eat, Ut 1
+1
:eat }
0.5
π2={Ut 2:eat, Ut 2
+1
:sleep}
0.375
π3={Ut 3:sleep, Ut 3
+1
:eat }
0.25
π4={Ut 4:sleep, Ut 4
+1
:sleep}
0.125
0 policies
π1 π2 π3 π4
Figure 10: A distribution over the policies that gives high probability to policies fulfilling
Bob’s preferences in the future. For example, the first policy where Bob is constantly
eating has high probability, while the fourth policy where Bob is constantly sleeping has
low probability. This is congruent with the notion that eating is more likely to make Bob
feel fed than hungry, and similarly, sleeping is more likely to make Bob feel hungry than
fed.
14
Active Inference and Variational Message Passing
5.1.5 The prior over the precision parameter
Wenowturntothelastpartofthegenerativemodel,i.e. thepriorovertheprecisionparam-
eter γ. Importantly, this precision parameter has been associated with the neuromodulator
dopamine through what is called the “precision hypothesis” (FitzGerald et al., 2015). This
association of dopamine and the precision parameter claims to unify two perspectives on
the role of dopamine. The first frames dopamine as an error signal on predicted reward
(Schultz et al., 1997) and uses the framework of TD-learning. The second, called the in-
centive salience hypothesis, frames dopamine as “associating salience and attractiveness to
visual, auditory, tactile, or olfactory stimuli” (Berridge, 2007).
But, let us come back to the prior over the precision parameters γ. In neurobiological
treatments, this prior usually takes the form of a gamma distribution with a rate parameter
β and a shape parameter fixed to one:
P (γ) = Γ(γ;1,β).
γ
The graph on the right of Figure 11 illustrates two variations of this prior for β = 1 and
β = 2. Also, we should mention that a more flexible prior can be obtained by removing the
constraint on the shape parameter (Friston et al., 2015), and the left hand side of Figure 11
illustrates this extension. However, in most artificial intelligence applications (that are not
concerned with biological implementation or dopamine), γ is usually assumed to be one.
Mainly, this design choice is made for the sake of simplicity, even if in practice forcing γ to
be one reduces the model flexibility, i.e. γ can no longer be learnt.
Γ(γ;2,0.5)
Γ(γ;1,1)
Γ(γ;2,1)
Γ(γ;1,2)
γ γ
0 2 4 6 0 2 4 6
Figure 11: This figure illustrates four gamma distributions where the values of the parame-
ters have been changed. The graph on the right shows the kind of prior the model believes
in by forcing the shape parameter to equal one.
15
Champion et al.
Notation Meaning
T The time horizon
t The current time steps
τ An iterator over time step
O The sequence of observations between time step 0 and t
0:t
S The sequence of hidden states between time step 0 and T
0:T
π The policies
Um =∆ U The action or control state predicted by the m-th policy at time step τ
τ
The matrix defining the likelihood mapping from the hidden states to the
A
observations
A The i-th column of the matrix A
•i
The set of transition matrices defining the mappings between any two consecutive
B
hidden states
B[U] The i-th column of the transition matrix B[U] corresponding to action U
•i
D The prior over the initial hidden states
a, b, d The parameters of the prior over A, B and D
a The i-th column of the matrix a
•i
b[U] The i-th column of the matrix b[U] corresponding to action U
•i
γ The precision parameter related to neuromodulators such as dopamine
σ(x) The softmax function
G The expected free energy
Γ(γ;α,β) Gamma distribution with shape and inverse scale parameters α and β
Cat(S ;D) Categorical distribution over S with parameter D
0 0
Dir(D;d) Dirichlet distribution
Table 1: Generative Model notation
16
Active Inference and Variational Message Passing
5.1.6 The entire generative model
Throughout this section, we have assembled incrementally the generative model usually
used in active inference, whoseForney factor graph is represented in Figure9. Thelast step
is to write down the equations that constitute its formal definition:
P(O ,S ,π,A,B,D,γ) = P(π|γ)P(γ)P(A)P(B)P(S |D)P(D)
0:t 0:T 0
t T
P(O |S ,A) P(S |S ,B,π), (7)
τ τ τ τ−1
τ=0 τ=1
Y Y
where:
P(π|γ) = σ(−γG) P(γ) = Γ(γ;1,β)
i
Y
P(A) = Dir(A ;a ) P(B) = Dir(B[i] ;b[i] )
•i •i •j •j
i i,j
Y Y
P(S |D) = Cat(S ;D) P(D) = Dir(D;d)
0 0
i
Y
P(O |S ,A) = Cat(O ;AS ) P(S |S ,B,π) = Cat(S ;B[U]S )
τ τ τ τ τ τ−1 τ τ−1
i
Y
Note that to keep the notation uncluttered, we have dropped the subscripts such that
P (S |D) becomes P(S |D), P (A) becomes P(A) and so forth. Table 1 provides a
S0 0 0 A
complete description of the notation used to define the generative model.
5.2 Variational Distribution
We now turn to the definition of the variational distribution, which is used to approximate
the true posterior during variational inference (a.k.a approximate Bayesian inference), i.e.
Q(x) ≈ P(x|o) where x and o denote the hidden variables and the observations, respec-
tively. Let us first recall that variational inference leverages independence between latent
variables in what is known as a mean-field approximation. A structured approximation,
often made in the active inference literature1 to simplify computations is that all latent
variables are independent except for the hidden states and the policy. This leads to the
following variational distribution:
T
Q(S ,π,A,B,D,γ) = Q (π)Q (A)Q (B)Q (D)Q (γ) Q (S |π), (8)
0:T π A B D γ Sτ τ
τ=0
Y
where:
Q (S |π) = Cat(S ;sπ) Q (π) = Cat(π;π)
Sτ τ τ τ π
Q (γ) = Γ(γ;1,β) Q (D) = Dir(D;d)
γ D
Q (A) = Dir(A ;a ) Q (B) = Dir(B[i] ;b[i] )
A •i •i B •j •j
i i,j
Y Y
1. Aninstance where thisgeneral assumption is not made can befound in (Parr et al., 2019).
17
Champion et al.
Once again, for the sake of compactness, the subscript will be dropped, e.g. Q (S |π)
Sτ τ
willbereplaced byQ(S |π). Table2summarisesthenotation usedtodefinethisvariational
τ
distribution. It is much easier to understand this distribution by comparing it to the
definition of the generative model in Equation 7. Indeed, the distributions over A, B
and D remain Dirichlet distributions, and the distributions over γ and S remain gamma
τ
and categorical distributions, respectively. Only the distribution over π changes from a
Boltzmann to a categorical distribution. However, both the Boltzmann and the categorical
are discrete distributions.
Notation Meaning
sπ The parameters of the posterior over S for each policy, i.e. a vector
τ τ
s• The parameters of the posterior over S for all policies, i.e. a matrix
τ τ
π The parameters of the posterior over π, i.e. a vector
The parameters of the posterior over A, B and D, i.e. a matrix,
a, b, d
a set of matrices and a vector, respectively
β The (inverse temperature) parameter of the posterior over γ
Table 2: Variational distribution notation
5.3 Variational Free Energy
Above, we have unpacked the generative model and variational distribution used in active
inference. This section combines those two concepts to form the second cornerstone of the
active inference framework, i.e. the variational free energy. Section 6.1 will explain how
the following equation can be derived from the Kullback-Leibler divergence between the
variational distribution and the true posterior. However, this section explains the intuition
behind the variational free energy, which is defined as follows:
F = E [lnQ(S ,π,A,B,D,γ)−lnP(O ,S ,π,A,B,D,γ)]
Q 0:T 0:t 0:T
= D [Q(x)||P(x|o)] − lnP(o) , (9)
KL
relativeentropy logevidence
where x = {S
0:T
,|π,A,B,{Dz ,γ} refe}rs to|th{ez m}odel’s hidden variables, and o = {O
0:t
}
refers to the sequence of observations made by the agent. Equation 9 highlights some
important properties of the variational free energy. Indeed, the relative entropy (a.k.a KL
divergence) ensures that the variational distribution Q(x) tends to get closer to the true
posterior P(x|o), as the free energy is reduced. Furthermore, it shows that the variational
free energy is an upper bound on the negative log evidence, because the relative entropy
cannot be negative. Also, if the variational distribution is equal to the true posterior, then
the variational free energy is equal to the (-ve) log evidence. The variational free energy
can also be re-arranged as:
F = D [Q(x)||P(x)]−E [lnP(o|x)], (10)
KL Q(x)
complexity accuracy
| {z } | {z }
18
Active Inference and Variational Message Passing
showing the trade-off between complexity and accuracy. The complexity penalises the
divergence of the posterior Q(x) from the prior P(x). The accuracy scores how likely the
observations are given the generative model and current belief of the hidden states. Inter-
estingly, in opposition to the Akaike information criterion (AIC) and Bayesian information
criterion (BIC), the complexity does not depend on the number of parameters. Conse-
quently, a model with a lot of parameters, but that does not vary from the prior will have
zero complexity, and a model with a small number of parameters that moves away a lot
from the prior will have a large complexity. Taking this view, a model is complex whenever
the knowledge encoded by the prior fails to explain the observed data accurately. In other
words, complexity scores the degree of belief updating that moves posterior beliefs away
from prior beliefs to provide an accurate account of any observations.
Comparison of the expression for expected free energy and variational free energy re-
veals an intimate relationship. One can see that the risk is the expected complexity, while
ambiguity is expected inaccuracy. These expectations are under the posterior predictive
beliefs about outcomes in the future under the policy in question. This is why G is called
expected free energy.
5.4 Update equations
All the update equations presented below come from the minimisation of the variational
free energy. This section presents the intuition behind those updates using the notations
summarized in Table 3. Let us start with the optimal updates of A, B and D that are
given by:
Q∗(D) = Dir(D;d) where d= d+s (11)
0
t
Q∗(A) = Dir(A ,a ) where a= a+ o ⊗s (12)
•i •i τ τ
i τ=0
Y X
Q∗(B) = Dir(B[u] ,b[u] ) where b[u] = b[u]+ sk ⊗sk π (13)
•i •i τ τ−1 k
Y
u,i (k,
X
τ)∈Ωu
Lookingattheaboveequations,theseupdatescanbeunderstoodascountingthenumber
of times an event appears. For example, the update of A counts the number of times a
pair of states-observations have been observed. Taking this view, a is the pseudo count
of previously occurring states-observations pairs, and o ⊗s takes into account the new
τ τ
observations. Similarly, the update of the B and D matrices, respectively count how many
times the state transitions and initial states have been observed. Additionally, the updates
of the hidden states are:
T
Q∗(S |π) = σ D¯ +I(0 ≤ t)o · A¯+ sπ ·B¯[Uπ] (14)
0 0 1 0
(cid:16) (cid:17)X i
Q∗(S |π) = σ B¯[Uπ ]sπ +I(τ ≤ t)o · A¯+ sπ ·B¯[Uπ] (15)
τ τ−1 τ−1 τ τ+1 τ
(cid:16) (cid:17) T
Q∗(S |π) = σ B¯[Uπ ]sπ +I(T ≤ t)o ·A¯+ B¯[Uπ ] (16)
T T−1 T−1 T T−1
(cid:16) pastorprior likelihood future (cid:17)X
| {z } | {z } | {z }
19
Champion et al.
Notation Meaning
A⊗B = ABT, A·B = ATB outer and inner products
Ja,bK all the natural numbers between a and b
all (k,τ) such that the k-th policy predicts
Ω = (k,τ) : Uk = u,τ ∈ J1,TK
u τ−1 action u at time τ −1
n o
s = s• ·π the expected state at time τ
τ τ
hf(X)i =∆ E [f(X)] is the expectation of f(X) over P
PX PX X
the digamma function used to compute
ψ(x)
analytical solutions, e.g. for hlnD i .
i QD
D¯ =hlnD i = ψ(d )−ψ( d ) the expected logarithm of D
i i QD i i i
A¯ = hlnA i = ψ(a )−ψ( a ) the expected logarithm of A
ij ij QA ij Pk kj
B¯[u] = hlnB[u] i = ψ(b[u] )−ψ( b[u] ) the expected logarithm of B
ij ij QB ij P k kj
P
Table 3: Update equations notation
where t can be thought of as a global variable referring to the present time point, and I(•)
is an indicator function that equals one if the condition is true and zero otherwise. A closer
look at these updates reveals that the hidden states are updated by gathering information
from the past, the future, and the likelihood mapping. In Equation 14, the information
from the past is replaced by some information from the prior over the initial state, and in
Equation 16, the information from thefuturedisappearsbecausewehave reached thelimits
of the time horizon (i.e. τ == T). Similarly, in Equations 15 and 16, the indicator function
ensures that there is no information from the likelihood mapping after the current time
step t because no observations are available. For additional information about the above
updates, the reader is referred to Sections 7.7 and 7.8 as well as Appendix G. Interestingly,
Parr and Friston (2018) proposed a model in which futureobservations are latent variables,
andinthiscase, informationwillbesentalongtheedgesconnectingfuturestates andfuture
observations. Finally, the update of γ and π takes the following form:
Q∗(γ) = Γ γ;1,β +G·(π−π )
0
(cid:16) 1 (cid:17)
Q∗(π) = σ − G−F
β
(cid:16) (cid:17)
where π = σ(−γ ·G), σ(·) is the softmax function, and F is a vector whose π-th element
0
is defined as:
T t
F = sπ ·(lnsπ −D¯)+ sπ ·(lnsπ −B¯[U]sπ )− o ·A¯sπ.
π 0 0 τ τ τ−1 τ τ
τ=1 τ=0
X X
Section 7 will derive update equations similar to those above that can be decomposed as a
sum of messages coming from the parent, children and co-parents of each node.
20
Active Inference and Variational Message Passing
5.5 Action selection
This section focuses on the various strategies available to pick the next action(s) that the
agent will then perform. In active inference, the action selection process is performed
after iteration of the update equations. Indeed, according to the Action-Perception cycle
presented in Section 2, the agent first minimises the variational free energy and then acts in
its environment. The first strategy entails summing the posterior evidence for the policies
predictingeach action, andtoexecutetheaction withthehighestsumofposteriorevidence:
|π|
u∗
t
= argmax δ
u,U t
mQ(π = m),
u
m=1
X
where |π| is the number of policies, Um is the action predicted at the current time step
t
by the policy π, and δ u,U t m is an indicator function that equals one if u = U t m and zero
otherwise. Since the model knows the posterior over the policies (i.e. sequences of actions)
another strategy is to simply sample an entire policy (e.g. a sequence of actions) without
re-computing the posterior at each timestep, i.e. Bob selects a policy, closes his eyes and
performs the sequence of actions entailed by that multi-step policy. In the case of single-
step policies, this is equivalent to the first strategy. This leads to a trade-off between
computational time and quality of the actions selected. Indeed, the more actions selected
at once, the less computational time required, but the less informed those actions will be.
Another strategy used in planning is called a Monte Carlo tree search (Browne et al.,
2012). The most well-known example of Monte Carlo tree search is probably the victory of
AlphaGo against Lee Sedol— the go world champion — in 2016 (Silver et al., 2016). Inter-
estingly, this method has been used recently with an active inference agent (Fountas et al.,
2020). The simplest version of this algorithm starts with an empty tree, i.e. a single node
representing the current state. Then, the root node is expanded such that the states that
are reachable from the current state become its children. Those children are linked to the
rootnodeby edges representingthe actions leadingto those states. Afterwards, simulations
of the environment are run to evaluate how good those new child states are. In the context
of reinforcement learning, the goodness of the states corresponds to whether or not reward-
ing terminal states are reached during the simulations. Similarly, in the context of active
inference, the expected free energy scores the goodness of outcomes. Finally, the reward or
EFE is back-propagated upward in the tree. Iterating this four-steps process (i.e. selection,
expansion, simulation and backpropagation) furnishes a posterior over the best action to
perform next.
6. Variational Message Passing
In the previous sections, our focus was on explaining the intuition behind active infer-
ence. The current section is more technical. We begin with the KL divergence between
the variational distribution Q(x) and the true posterior P(x|o), which underwrites the min-
imisation of the variational free energy. Then, we derive two update equations well known
from the Bayesian statistics community. The first explains how the approximate poste-
rior can be computed using variational inference. And the second reveals that the optimal
posterior can be thought of as a sum of messages. Finally, the message based equation is
21
Champion et al.
specialised for theclass of exponential conjugate models thatwe useto describethemethod
of Winn and Bishop (2005) as a five-step process. During this section, we will be using a
few properties that are summarised in Appendix B.
6.1 Justification of the Variational Free Energy
As mentioned in Section 3, the computation of the true posterior — using Bayes theorem
quickly becomes intractable as the number of hidden states increases. The variational free
energy (VFE), or equivalently, the negative evidence lower bound (-ELBO), aims to solve
this intractability problem by approximating the true posterior with another distribution:
the variational distribution. To justify theuseof the variational freeenergy, let us firstnote
that the following expression can be obtained from the product rule:
P(o,x)
P(x|o) = . (17)
P(o)
Since the KL divergence measures the distance between two distributions, we can min-
imise the KL divergence between the variational distribution and the true posterior. And
this will keep the variational distribution close to the true posterior. Starting with this KL
divergence, and substituting Equation 17 within it, we obtain:
D [Q(x)||P(x|o)] = D [Q(x)||P(x,o)]+E [lnP(o)]
KL KL Q(x)
= D [Q(x)||P(x,o)]+ lnP(o) ,
KL
VFE=-ELBO logevidence
| {z } | {z }
where the expectation over the log evidence can be dropped due to the lack of a depen-
dence of lnP(o) on Q(x). Because the log evidence does not dependon the latent variables,
it can be safely ignored during the minimisation process. In other words, minimising the
variational free energy is equivalent to minimising the KL divergence between the varia-
tional distribution and the true posterior, and ensuring that the variational distribution is
a good approximation of the true posterior.
6.2 Variational Inference Updates
As we have just noted, variational methods rely on the minimisation of the variational free
energy, or equivalently, the maximisation of an evidence lower bound. So, let us start with
the former:
D [Q(x)||P(o,x)] = E [lnQ(x)−lnP(x,o)].
KL Q(x)
Using the mean-field assumption Q(x) = Q (x ), the log property, and the linearity
i i i
of expectation. The above equation can be rewritten as:
Q
D [Q(x)||P(o,x)] = E [lnQ (x )]+E [ln Q (x )]−E [lnP(x,o)].
KL Q(x) k k Q(x) j j Q(x)
j6=k
Y
Note that lnQ (x ) is a constant w.r.t all factors but Q (x ), and ln Q (x ) is
k k k k j6=k j j
a constant w.r.t Q (x ). Using the expectation of a constant, the above equation can be
k k
Q
22
Active Inference and Variational Message Passing
rewritten as:
D [Q(x)||P(o,x)] = E [lnQ (x )]+E [ln Q (x )]−E [lnP(x,o)],
KL Qk(xk) k k ∼Qk(xk) j j Q(x)
j6=k
Y
where E [·] is the expectation over all factors but Q (x ). If the goal is to minimise
∼Qk(xk) k k
the free energy w.r.t Q (x ), the second term can be safely considered as a constant C.
k k
Also, using the factorisation of the variational distribution, the third term can be rewritten
as E [E [lnP(x,o)]], leading to:
Qk(xk) ∼Qk(xk)
D [Q(x)||P(o,x)] = E [lnQ (x )]−E [E [lnP(x,o)]]+C
KL Qk(xk) k k Qk(xk) ∼Qk(xk)
= E lnQ (x )−E [lnP(x,o)] +C
Qk(xk) k k ∼Qk(xk)
h i
=∆ E lnQ (x )−lnQ∗(x ) +C
Qk(xk) k k k k
= D [Qh (x )||Q∗(x )]+C, i
KL k k k k
where =∆ means equal by definition, and lnQ∗(x ) =∆ E [lnP(x,o)]. The KL diver-
k k ∼Qk(xk)
gence can not be negative which means that Q (x ) = Q∗(x ) minimises the free energy,
k k k k
and for this reason Q∗(x ) is called the optimal posterior.
k k
6.3 Variational Message Passing Updates
RestartingwiththedefinitionofQ∗(x )andusingthefactorisation ofthegenerative model,
k k
we get:
lnQ∗(x ) =∆ E [lnP(x,o)]
k k ∼Qk(xk)
= E [ln P(N |pa )],
∼Qk(xk) i i
i
Y
where N iterates over all nodes, i.e. all latent and observed variables, and pa are the
i i
parents of N . The term in the above product can be classified into three groups: the terms
i
that do not depend on x , the terms whose target variable (N ) is x and the terms whose
k i k
predictors (pa ) contains x . Building on this observation, one can use the log property and
i k
the linearity of expectation to isolate the terms that depend on x :
k
lnQ∗(x )= hln P(N |pa )i
k k i i ∼Qk
i
Y
= hlnP(x |pa )i + hlnP(c |x ,cp )i +C, (18)
k k ∼Qk j k kj ∼Qk
cjX ∈chk
where h·i is just another notation for E [·], and the constant C comes from the
∼Qk ∼Qk(xk)
terms of the product that do not depend on x . Equation 18 is the variational message
k
passing equation that tells us how to compute the optimal posterior of any hidden state x
k
based on its Markov blanket, i.e. x ’s parents pa , children ch and co-parents cp . For
k k k kj
readersunfamiliarwiththenotionofMarkov blankets, Figure12providesavisualdepiction
of the underlying notion.
23
Champion et al.
Markov blanket of A
Parent of A
Co-parent of A F G
E A B
D C
Child of A
Figure 12: This figure illustrates the Markov blanket of node A, which is drawn in grey
surrounded by a dashed line. The nodes F and G are the parents of A and the nodes C
and D are the children of A. The node E is the co-parent of A with respect to D and the
node B is the co-parent of A with respect to C.
6.4 Conjugate exponential model
The variational message passing algorithm can be derived for the class of conjugate expo-
nential models (Winn and Bishop, 2005). Those models have a likelihood function and a
prior in the exponential family. Furthermore, the prior and the likelihood are conjugate,
meaning that the posterior will have the same form as the prior. We follows the steps
in Winn and Bishop, while referring the interested reader to (Winn and Bishop, 2005) for
more details. The derivations in equations 19-23 are clarified in the example in Figure 13.
Returning to our goal of computing the posterior over x (cf. Equation 18), we assume
k
that P(x |pa ) and P(c |x ,cp ) are in the exponential family, i.e.
k k j k kj
lnP(x |pa ) =µ (pa )·u (x )+h (x )+z (pa ) (19)
k k k k k k k k k k
lnP(c |x ,cp ) = µ (x ,cp )·u (c )+h (c )+z (x ,cp ) (20)
j k kj j k kj j j j j j k kj
where µ (pa ), u (x ), h (x ) and z (pa ) are the parameters, the sufficient statistics, the
k k k k k k k k
underlying measure and the log partition, respectively. For a specific example, Equation 25
shows theDirichlet distributionwritten in theformof theexponential family. Thefirststep
of the Winn and Bishop method takes advantage of the conjugacy constraint to re-arrange
Equation 20 as a function of u (x ) that appears in Equation 19:
k k
lnP(c |x ,cp )= µ (c ,cp )·u (x )+λ(c ,cp ), (21)
j k kj j→k j kj k k j kj
whereµ (c ,cp )andλ(c ,cp )emergefromthere-arrangement. Foraspecificexample
j→k j kj j kj
of this first step, the reader is referred to the derivation from (26) to (27), Figure 13 also
provides an example of µ (c ,cp ). The second step substitutes Equations 21 and 19
j→k j kj
24
Active Inference and Variational Message Passing
within the variational message passing equation leading to:
lnQ∗(x )= hµ (pa )·u (x )+h (x )+z (pa )i
k k k k k k k k k k ∼Qk
+ hµ (c ,cp )·u (x )+λ(c ,cp )i +Const.
j→k j kj k k j kj ∼Qk
cjX ∈chk
The third step relies on taking the exponential of both sides, using the linearity of
expectation and factorising by u (x ) to obtain:
k k
Q∗(x ) = exp hµ (pa )i + hµ (c ,cp )i ·u (x )+h (x )+Const ,
k k k k ∼Qk j→k j kj ∼Qk k k k k
( )
h cjX ∈chk i
(22)
where the above constant just absorbed z (pa ) and λ(c ,cp ), which does not depend
k k j kj
on x . At this point, we already see that the prior (19) and the approximate posterior
k
(22) have the same functional form, i.e., only their parameters differ. The fourth step
re-parameterizes µ (pa ) and µ (c ,cp ) in terms of the expectation of the sufficient
k k j→k j kj
statistics of the children, parents and the co-parents:
Q∗(x ) = exp µ∗ ·u (x )+h (x )+Const
k k k k k k k
( )
µ∗ = µ˜ ({hu (i)i } )+ µ˜ (hu (c )i ,{hu (l)i } ), (23)
k k i Qi i∈pa k j→k j j Qj l Ql l∈cp kj
cjX ∈chk
where µ˜ is a re-parameterization of µ (pa ) in terms of the expectation of the sufficient
k k k
statistic of the parents of x , and similarly µ˜ is a re-parameterization of µ . The
k j→k j→k
exact form of µ˜ and µ vary from distribution to distribution. An example of those
k j→k
re-parameterizations is visible from Equation 28 to 29.
To understand the intuition behind (23), let us consider the following example: given
the Forney factor graph illustrated in Figure 13, we wish to compute the posterior of Y.
Then, the only parent of Y is Z, the only child of Y is X and the only co-parent of Y
with respect to X is W. Therefore, applying equation 23 to our example leads to the
equation presented in Figure 13 whose components can be interpreted as messages. Indeed,
each variable (i.e. X, Z and W) sends the expectation of their sufficient statistic (i.e. a
message) to the square node in the direction of Y (i.e. either P or P ). Those messages
X Y
are then combined using a function (i.e. either µ˜ or µ˜ ) whose output (i.e. another
Y X→Y
set of messages) are summed to obtain the optimal parameters µ∗ . The computation of
Y
the optimal parameters (23) can then be understood as a message passing procedure.
25
Champion et al.
m1 m2 m3 m4
PZ Z PY Y PX W PW
m5
X
m1 m5 m4
∗
µ Y = µ˜ Y (hu Z (Z)i QZ )+µ˜ X→Y (hu X (X)i QX ,hu W (W)i QW )
m2 m3
Figure 13: This figure illustrates the computation of the optimal posterior parameters for
the variable Y as a message passing procedure, which requires the transmission of messages
from the parent (m ) and child (m ) factors. Additionally, the message from the child
2 3
factor (m ) requires the computation of messages from the co-parent (m ) and child (m )
3 4 5
variables. Also, the message from the parent (m ) factor requires the computation of a
2
message (m ) from the parent variable. Set notation and associated brackets {} have been
1
dropped, since there is only ever one parent or co-parent.
Returning to the Winn and Bishop (2005) method, the last step computes the (set of)
expectations associated with {hu (j)i } , hu (X)i , and {hu (j)i } . Be-
j Qj j∈pa
Y
X QX j Qj j∈cp
YX
cause all nodes of the model are in the exponential family, the moment generating function
can be used to prove the following:
∂z˜ (θ )
N N
hu (N)i = − , (24)
N QN
∂θ
N
where N is any node of the graphical model, θ are the natural parameters of the distri-
N
bution over N, and z˜ (θ ) is a re-parameterisation of the log partition w.r.t the natural
Z N
parametersofthedistributionoverZ. Notethatanotherwaytocomputethoseexpectations
will be presented in Section 7.3.
7. The link between Active Inference and Variational Message Passing
The previous sections have presented the theory behind active inference and variational
message passing. This section focuses on the link between those two frameworks. First, we
slightly modify the generative model and the variational distribution. These modifications
concern a small part of the generative model and to ensure conjugacy between the random
variables of the model. Then, we derive new update equations based on the Winn and
Bishop method (Winn and Bishop, 2005). As we will see, those updates can be interpreted
as a passing of messages that highlight the connection between variational message passing
and belief updating in (planning as) active inference.
26
Active Inference and Variational Message Passing
7.1 Generative model modifications
In order to perform variational message passing, we have made three modifications to the
generative model described by Equation 7. First, the prior over the precision parameter γ
is removed. Second, the softmax function forming the prior over the policies is transformed
into a categorical distribution with parameters α. This is a mild modification because
the softmax function is frequently used to represent a categorical distribution, e.g. neural
classifiers using a softmax function as output layer or similarly to the updates of Q(s )
τ
and Q(π) presented in Section 5.4. Finally, we assume a Dirichlet distribution over the
parameters α. Figure 14 illustrates this new generative model where:
P(π|α) = Cat(π;α)
P(α) = Dir(α;θ).
The conjugacy between the Dirichlet and categorical distributions enables us to derive
updateequationsthatcanbeinterpretedasmessages. Recallthattheprioroverpolicieswas
used to bias the policy selection towards the policies that minimise expected free energy.
This can be implemented in a straightfoward way — while preserving conjugacy — by
setting the parameters of the Dirichlet as follows:
−→
θ = c −G,
−→
where G is the expected free energy and c is a vector of constants whose elements satisfy
the following properties:
−→ −→
1. ∀i,j : c = c , i.e. all elements are equal;
i j
−→
2. ∀j : c > max G , i.e. all θ are strictly positive.
j i i j
To better understand the influence of P(α) on the selection of policies, we imagine a
Dirichlet with K parameters as a distribution over a (K−1)-simplex. Assuming that all θ
i
are greater than one, the point of this simplex with the highest probability, i.e. the mode
m , has the following coordinates:
α
m =
θ1−1 ... θK−1
.
α PK
k=1
θk −K PK
k=1
θk −K
h i
Studying a few special case(cid:0)s of the(cid:1)above equ(cid:0)ation sh(cid:1)eds some light on how policy
selection is influenced by P(α). If the i-th numerator of the coordinates, i.e. θ −1, equal
i
oneandallothersequalzero,thenthemodem isatthecornerofthesimplexcorresponding
α
to the i-th axis. If all numerators are equal to one, then the mode is at the centre of the
simplex. Intuitively, this means that the bigger θ is relative to the other θ ∀j 6= i, the
i j
closer m is to the i-th corner of the simplex. Additionally, the closer m is to the i-th
α α
corner of the simplex, the more likely the i-th policy will be. Therefore, the bigger θ the
i
more likely the i-th policy. Finally, the only part of the numerators that is not a constant
is G and the smaller G the bigger the i-th numerator. Thus, in accord with the active
i i
inference literature, P(α) favours policies that minimise the expected free energy.
−→
Another perspective on this parameterisation of priors over policies is to think of c as
pseudo-counts that ‘promote’ each policy according to how often it was previously pursued,
27
Champion et al.
before adding (-ve) expected free energy. If these pseudo-counts are suitably small, adding
expected free energy will have a greater effect in the sense that expected free energy scores
the number of times each policy would be pursued. Quantitatively, this means that a
differenceintheexpectedfreeenergybetweenonepolicyandanothercannowbeinterpreted
in terms of Dirichlet parameters or pseudo-counts.
It could be argued that the Dirichlet parameterisation of the prior over policies is a
more natural parameterisation than the gamma distribution used to explain dopamine.
Furthermore, as noted above, in most applications, gamma is set to one. More importantly,
the precision parameter is only relevant for generative models where policies entail past
transitions. In look-ahead policies or tree search implementations of planning, policies only
concern future states. This means the precision of prior beliefs about policies relative to
posterior beliefs (based upon the evidence a particular policy is being pursued) becomes
irrelevant. In this case, the Dirichlet parameterisation above may be preferred.
Pα
α
PD PB Pπ
D B π
PS0
S ... S ... S
0 PSt t PST T
PA
A
PO0 POt
O O
0 t
Figure 14: The new generative model obtained after replacing the gamma distribution by
a Dirichlet distribution.
7.2 Variational distribution modifications
The variational distribution presented in Section 5.2 is an example of a structured varia-
tional distribution, because factors such as Q(S ,π) = Q(S |π)Q(π) model the (posterior)
τ τ
dependency between S and π. Performing inference with such a joint distribution falls
τ
under the category of structured variational inference (Wiegerinck, 2000; Xing et al., 2012)
and will not be covered in this paper. Instead, we assume a fully factorised distribution
28
Active Inference and Variational Message Passing
such that:
T
Q(S ,π,A,B,D,γ) = Q(π)Q(A)Q(B)Q(D)Q(γ) Q(S ),
0:T τ
τ=0
Y
where Q(π) = Cat(π;α˜), Q(S ) = Cat(S ;D˜ ) and all the other factors remain unchanged.
τ τ τ
This is a rather severe mean-field approximation: although it allows for straightforward
application ofvariational message passing,removingtheconditionaldependenciesofhidden
statesinthefutureonactionmeanstheagentcannotindividuatetheconsequencesofaction.
Under this functional form the expected free energy reduces to:
T
G(π)= E H[P(S |S ,B,π)] .
Q(Sτ−1,B) τ τ−1
X τ=1 h i
Namely, the expected conditional entropy of the hiddenstates. Also, we refer the interested
reader to Appendix H for a derivation of the above equation. Intuitively, this means that
goodpoliciesselect actions thatleadtounambiguoushiddenstates. Thishighlights amajor
limitation of the mean-field approximation required by the variational message passing
proposed by (Winn and Bishop, 2005) in the context of active inference. In other words,
when removing key structure from the variational distribution, the factor over the hidden
states Q(S |π) no longer depends on the policy π and most of the terms in the expected
τ
free energy become constants w.r.t π. Figure 15 illustrates an alternative generative model,
implementing tree search as a form of structure learning, which is not impacted by this
issuebecause thefuturestates in this model still dependupontheaction undertaken by the
agent. We refer the reader to our companion paper (Champion et al., 2021) for details. A
related treatment that performs exact Bayesian inference by considering a slightly different
generative model can be found in (Friston et al., 2020).
Before we turn to the derivation of the messages, we highlight the differences between
active inference as presented in Section 5 and the current treatment. The former is an
example of structured variational inference (∗). In contrast, the work presented in this
section assumes a fully factorised variational distribution and will be strictly framed as a
message passing algorithm, i.e. variational message passing (∗). Figure 16 illustrates those
differences. Finally, in the remaining sections, we present the derivation of the messages for
D, A, π and α, and we refer the reader to Appendices F and G for the derivations of the
messages for B and S , respectively.
τ
29
Champion et al.
PS0
S O
0 PO0 0
U
PU0 0 PS...
S O
... PO... ...
U
PU... ... PSt
S O
t POt t
PS{1} PS{2}
O S S O
{1} PO{1} {1} {2} PO{2} {2}
PS{11} PS{12} PS{22}
O {11} PO{11} S {11} S {12} S {22}
Figure15: Thisfigureillustrates analternative new(expandable)generative modelallowing
planningunderactiveinference. Inthismodel,thefutureisnowatreelikegenerativemodel
whose branches correspond to the policies considered by the agent. Each edge connecting
two states in the future correspond to an action and the nodes in light grey represent
possible expansions of the current generative model.
directmethods E SVI(∗) VI
messagebasedmethods BPT SVMP VMP(×)
exactinference approximateinference
Figure16: ThisfigureillustratesthedifferencesbetweentheframeworkpresentedinSection
5 that belongs to the field of structured variational inference (Bishop and Winn, 2003) de-
noted by (∗), and the work presented below that belongs to the field of variational message
passing (Winn and Bishop, 2005) denoted by (×). The other abbreviations BPT, E, VI
and SVMP correspond to belief propagation on tree graphical models (Kschischang et al.,
2001), the elimination algorithm (Cozman, 2000), variational inference (Blei et al., 2017)
and structured (or cluster) variational message passing (Lin et al., 2018), respectively. Im-
portantly, note that BPT is a specific kind of belief propagation which does not involve
generalized BP (Yedidia et al., 2000) or loopy belief propagation (Murphy et al., 2013).
30
Active Inference and Variational Message Passing
7.3 Messages for D
Thissection appliesthemethodofWinnandBishopdiscussedinSection6.4tocomputethe
messages ofD. Let usstartwiththedefinitionof theDirichlet andcategorical distributions
written in the form of the exponential family:
d −1 lnD
1 1
lnP(D;d) = ... · ... −lnB(d) (25)
   
d −1 lnD
|S| |S| zD(d)
   
µD(d) uD(D) | {z }
| {z } | {z }
lnD [S = 1]
1 0
lnP(S ;D) = ... · ... (26)
0
   
lnD [S = |S|]
|S| 0
   
µS0(D) uS0(S0)
| {z } | {z }
whereB(d)istheBetafunctionand|S|isthenumberofvaluesahiddenstatecantake. The
first step requires us to re-write Equation 26 as a function of u (D), this is straightforward
D
because µ (D) is just another name for u (D). Using the fact that the inner product is
S0 D
commutative:
[S = 1] lnD
0 1
lnP(S ;D) = ... · ... . (27)
0
   
[S = |S|] lnD
0 |S|
   
µS0→D(S0) uD(D)
| {z } | {z }
The second step aims to substitute Equations 25 and 27 within the variational message
passing equation (18), i.e.
d −1 lnD [S = 1] lnD
1 1 0 1
lnQ∗(D) = ... · ... −lnB(d) + ... · ... +Const,
       
d −1 lnD [S = |S|] lnD
D |S| |S| zD(d) E D 0 |S| E
       
µD(d) uD(D) | {z } µS0→D(S0) uD(D)
| {z } | {z } | {z } | {z }
whereh•ireferstoh•i
∼QD
. Notethatintheaboveequation,d
i
arefixedparameters,therefore
thereisnotanyposteriorover dandthefirstexpectation h·i canberemoved. Thethird
∼QD
step rests on taking the exponential of both sides, using the linearity of expectation and
factorising by u (D) to obtain:
D
d −1+h[S = 1]i
1 0
Q∗(D) = exp ... ·u (D)+Const , (28)
D
 
( d −1+h[S = |S|]i )
|S| 0
 
where z (d) have been absorbed into the constant term because it does not depend on
D
D. The fourth step is a re-parameterisation done by observing that h[S = i]i is the i-th
0
element of the expectation of the vector u (S ), i.e. hu (S )i = h[S = i]i:
S0 0 S0 0 i 0
31
Champion et al.
d −1+hu (S )i
1 S0 0 1
Q∗(D) = exp ... ·u (D)+Const . (29)
D
 
( d −1+hu (S )i )
|S| S0 0 |S|
 
µ˜D(...)+µ˜S0→D(...)
| {z }
The last step consists of computing the expectation of hu (S )i for all i. This can
S0 0 i
be achieved by realising that the probability of an indicator function for an event is the
probability of this event, i.e hu (S )i = h[S = i]i = Q(S = i) = D˜ . Substituting this
S0 0 i 0 0 0i
result in Equation 29, leads to the final result:
d −1+D˜
1 01
Q∗(D) = exp ... ·u (D)+Const .
D
 
( d −1+D˜ )
|S| 0|S|
 
Indeed, the above equation is in fact a Dirichlet distribution in exponential family form,
and can be re-written into its usual form to obtain the final update equation:
Q∗(D) = Dir(D;d+D˜ ).
0
In the following sections, we provide derivations for the messages of A, B, π, α, and S .
τ
Those derivations are similar to the one presented above. We encourage technical readers
togo throughthosederivations becausethey constitute themaincontribution ofthis paper.
However, a reader uninterested in the algebraic details of the proofs may want to jump to
Section 7.7.
7.4 Messages for A
Intheprevioussection, wehaveshownhowtocomputethemessagesforD,whicharebased
on the conjugacy between a categorical P(S |D) and a Dirichlet P(D;d) distributions. In
0
this section, we dive into the derivation of the messages of A, which relies on the same
kind of conjugacy. We start with the definition of P(A;a), which is a product of Dirichlet
distributions. This product can be turned into a sum by taking the logarithm of both sides
and using the log property to obtain:
lnP(A;a) = ln P(A ;a )= lnDir(A ;a )
•i •i •i •i
i i
Y X
a −1 lnA
1i 1i
= ... · ... −lnB(a )
   
•i
X i a |O|i −1 lnA |O|i
   
LogarithmofDirichlet
a −1 lnA
|11 {z11 }
= ... · ... − lnB(a ), (30)
   
•i
a |O||S| −1 lnA |O||S| X i
   
µA(a) uA(A)
zA(a)
| {z }
| {z } | {z }
32
Active Inference and Variational Message Passing
where |O| is the number of possible outcomes. Note that the vectors u (A) and µ (a) step
A A
through all the elements of the matrices A and a, respectively. Also, for each time step τ
up to the present time t, the random matrix A has one child O (see Figure 14), and its
τ
probability mass function P(O |A,S ) is a product of categorical distributions that can be
τ τ
written as:
lnP(O = k|A,S = l)= lnA
τ τ kl
= [O = i][S = j]lnA
τ τ ij
i,j
X
[S = 1]lnA [O = 1]
τ 11 τ
= ... · ... . (31)
   
[S = |S|]lnA [O =|O|]
τ |O||S| τ
   
µOτ (A,Sτ) uOτ (Oτ)
Finally, the re-parameterisation in|the fourth{zstep will}re|quire{tzhe pr}obability mass
function of S (see Figure 14), i.e. the co-parent of A with respect to O , to be written in
τ τ
the form of the exponential family as follows:
lnP(S = k|B,S = l,π = m)= lnB[Um ]
τ τ−1 τ−1 kl
= [S = i][S = j][π = k][Uk = u]lnB[u]
τ τ−1 τ−1 ij
i,j,k,u
X
= µ (B,S ,π)·u (S ), (32)
Sτ τ−1 Sτ τ
where:
[S = j][π = k][Uk = u]lnB[u]
j,k,u τ−1 τ−1 1j
µ (B,S ,π) = ... ,
Sτ τ−1
 P 
[S = j][π = k][Uk = u]lnB[u]
j,k,u τ−1 τ−1 |S|j
 
and: P
[S = 1]
τ
u (S ) = ... .
Sτ τ
 
[S = |S|]
τ
 
The first step requires us to re-write Equation 31 as a function of u (A), this is done
A
by expanding the inner product and re-arranging:
[O = 1][S = 1] lnA
τ τ 11
lnP(O |A,S )= ... · ... . (33)
τ τ
   
[O = |O|][S = |S|] lnA
τ τ |O||S|
   
µOτ→A(Oτ,Sτ) uA(A)
The second step aims to substitu|te Equatio{nzs 30 and 3}3|withi{nzthe}variational message
passing equation (18), i.e.
a 11 −1 t [O τ = 1][S τ = 1]
lnQ∗(A) = ... ·u (A) + ... ·u (A) +Const,
A A
   
D a |O||S| −1 E X τ=0D [O τ = |O|][S τ = |S|] E
   
33
Champion et al.
where h•i refers to h•i
∼QA
. The third step builds on this equation by pulling the sum over
all time steps τ inside the vector, using the linearity of expectation, factorising u (A), and
A
taking the exponential of both sides:
a −1+ t h[O = 1]ih[S = 1]i
11 τ=0 τ τ
Q∗(A) = exp ... ·u (A)+Const ,
A
 P 
( a −1+ t h[O = |O|]ih[S = |S|]i )
|O||S| τ=0 τ τ
 
where we used that a
ji
are hyperparPameters that are constant w.r.t the expectation h•i
∼QA
.
Thefourthstepconsists oftwo re-parameterisations performedby observingthath[O = j]i
τ
and h[S = i]i are the expectations of the j-th and i-th elements of the vectors u (O ) and
τ Oτ τ
u (S ), respectively (cf. Equation 31 and 32). Substituting those re-parameterisations in
Sτ τ
the above equation leads to:
a −1+ t hu (O )i hu (S )i
11 τ=0 Oτ τ 1 Sτ τ 1
Q∗(A) = exp ... ·u (A)+Const . (34)
A
 P 
( a −1+ t hu (O )i hu (S )i )
KN τ=0 Oτ τ |O| Sτ τ |S|
 
µP˜A(...)+P
τ
µ˜Oτ→A(...)
| {z }
The last step consists of computing the expectation of hu (O )i and hu (S )i for
Oτ τ i Sτ τ j
all i and j. Since, the probability of an indicator function for an event is the probability
of this event, we are searching for the probabilities of O = j and S = i. The probability
τ τ
of O = j is the j-th element of the vector o , which is a one hot vector containing the
τ τ
observation from theenvironment at timeτ. Theposterior probability of S is by definition
τ
Q(S ) = D˜ . Substituting the probabilities of O = j and S =i in Equation 34, leads to:
τ τ τ τ
a −1+ t o D˜
11 τ=0 τ1 τ1
Q∗(A) = exp ... ·u (A)+Const (35)
A
 P 
( a −1+ t o D˜ )
|O||S| τ=0 τ|O| τ|S|
 a −1+ t o D˜  lnA
1i P τ=0 τ1 τi 1i
= exp ... · ... +Const . (36)
 P   
Y i ( a |O|i −1+ t τ=0 o τ|O| D˜ τi lnA |O|i )
   
Finally, one can recognise in EquationP36 the product of Dirichlet distributions written
into their exponential form, i.e.
Q∗(A) = Dir(A ,a ) where a = a+ o ⊗D˜ .
•i •i τ τ
i τ
Y X
Theorigin oftheouter productinthecomputation oftheparameterscan beunderstood
by considering Pτ the outer product between o and s such that Pτ = o s . Then,
τ τ ij τi τj
Equation 35 shows that: a = a + Pτ ⇔ a = a+ o ⊗s .
ij ij τ ij τ τ τ
P P
7.5 Messages for π
We now turn to the messages for π. Note, that the definition of the P(S |B,S ,π) and
τ τ−1
P(π|α) are given by Equations 32 and 44, respectively. Thefirststep requires us to re-write
34
Active Inference and Variational Message Passing
Equation 32 as a function of u (π). Using the inner product definition and re-arranging we
π
obtain:
[S = i][U1 = u][S = j]lnB[u]
i,j,u τ τ−1 τ−1 ij
lnP(S = k|B,S = l,π = m)= ... ·u (π).
τ τ−1 P  π
[S = i][U
|π|
= u][S = j]lnB[u]
i,j,u τ τ−1 τ−1 ij
 
  (37)
P
The second step aims to substitute Equations 44 and 37 within the variational message
passing equation, i.e.
lnα
1
lnQ∗(π) = ... ·u (π)
π
 
lnα
D |π| E
  [S = i][U1 = u][S = j]lnB[u]
T i,j,u τ τ−1 τ−1 ij
+ ... ·u (π) +Const,
P  π
X τ=1D i,j,u [S τ = i][U τ |π − | 1 = u][S τ−1 = j]lnB[u] ij E
 
 
P
whereh•irefers toh•i
∼Qπ
. Thethirdstep relies on pullingthesummation over all time steps
inside the vector, taking the exponential of both sides, using the linearity of expectation
and factorising by u (π) to obtain:
π
hlnα i+ [U1 = u]h[S = i]ih[S = j]ihlnB[u] i
1 τ,i,j,u τ−1 τ τ−1 ij
Q∗(π) ∝ exp ... ·u (π) .
 P  π
(
hlnα i+ [U
|π|
= u]h[S = i]ih[S = j]ihlnB[u] i
)
|π| τ,i,j,u τ−1 τ τ−1 ij
 
 
P µ∗
π
| {z }
The fourth step is a re-parameterisation implemented by observing that hlnα i, h[S =
k τ
i]i,h[S = j]iandhlnB[u] iareelementsofthevectorshu (α)i,hu (S )i,hu (S )i
τ−1 ij α Sτ τ Sτ−1 τ−1
and hu (B)i, respectively:
B
hu (α)i + [U1 = u]hu (S )i hu (S )i hu (B)i
α 1 τ,i,j,u τ−1 Sτ τ i Sτ−1 τ−1 j B u,i,j
µ∗ = ... . (38)
π  P 
hu (α)i + [U
|π|
= u]hu (S )i hu (S )i hu (B)i

α |π| τ,i,j,u τ−1 Sτ τ i Sτ−1 τ−1 j B u,i,j

 
P
Thelaststepconsistsofcomputingtheexpectationofhu (α)i ,hu (S )i ,hu (S )i
α k Sτ τ i Sτ−1 τ−1 j
and hu (B)i for all i, j, k and u:
B u,i,j
• hu (α)i = hlnα i= ψ(α˜ )−ψ( α˜ ) =∆ α¯
α k k k l l k
• hu (S )i = h[S = i]i = D˜ P
Sτ τ i τ τi
• hu (S )i = h[S = j]i = D˜
Sτ−1 τ−1 j τ−1 (τ−1)j
• hu (B)i = hlnB[u] i= ψ(b[u] )−ψ( b[u] ) =∆ B¯[u]
B u,i,j ij ij l lj ij
P
35
Champion et al.
Furthermore,theindicatorfunctioninthek-throwofEquation38filtersoutallelements
where u6= Uk . Substituting those results in Equation 38, leads to the final result:
τ−1
α¯ + D˜ D˜ B¯[U1 ]
1 τ,i,j τi (τ−1)j τ−1 ij
Q∗(π) ∝ exp ... ·u (π) .
 P  π
( α¯ + D˜ D˜ B¯[U |π| ] )
|π| τ,i,j τi (τ−1)j τ−1 ij
 
 
Indeed, the above equation is a CaPtegorical distribution in the exponential family form,
and can be re-written into its usual form as follows:
T hD˜ τ ⊗D˜ τ−1 ,B¯[U τ 1 −1 ]i F
Q∗(π) = Cat(π;α∗) where α∗ = σ α¯+ F and F = ... ,
τ τ
 
τ=1 ! hD˜ ⊗D˜ ,B¯[U |π| ]i
X τ τ−1 τ−1 F
 
where it should be stressed that h•,•i
F
is not an expectation but the Frobenius product, i.e.
a generalisation of the inner product to matrices.
7.6 Messages for α
Inthis section, wefocus on themessages for α, whosederivation is identical to themessages
ofD. Toseethis,notethatP(D)wasaDirichletwithparametersd. Furthermore,theonly
child of D was S whose prior and posterior were categorical distributions with parameters
0
D and D˜. Similarly, note that P(α) is a Dirichlet with parameters θ. Furthermore, the
only child of α is π whose prior and posterior are categorical distributions with parameters
α and α˜. From this observation, we directly obtain the following result:
Q∗(α) = Dir(α;θ+α˜).
7.7 Summary of messages
Next, we focus on explaining the intuition behind the resulting equations. The first point
is the coloration of the equations in orange and purple. The orange colour corresponds
to messages from the parent factors, which correspond to messages of type m in Figure
2
13. This means that each orange message is a function of the expectation of the sufficient
statistic of the parent variables, i.e. a function of messages of type m . Similarly, the
1
purplecolour correspondsto messages from the child factors, which correspondto messages
of type m in Figure 13. Once again, this means that each purple message is a function of
3
the sufficient statistics of the co-parent and child variables, i.e. a function of messages of
type m and m , respectively. Let’s see how these play out in our newly derived equations.
4 5
Messages for α:
Q∗(α) = Dir(α;θ+α˜)
Recall that µ = θ is an m message (orange colour). However, α does not have any
α 2
parent variables thus µ is a constant, i.e. a function of zero m messages. Furthermore,
α 1
we know that α has only one child variable (π) and no co-parent variables. Therefore,
µ (α˜) = α˜ is the only m message (purple colour) for α, where α˜ = hu (π)i is an m
π→α 3 π Qπ 5
message.
36
Active Inference and Variational Message Passing
Messages for D:
Q∗(D) = Dir(D;d+D˜ )
0
Similarly for the messages of α, µ = d and µ (D˜ ) = D˜ , where D˜ should be
D S0→D 0 0 0
thought of as a message from a child variable (m message).
5
Messages for A:
Q∗(A) = Dir(A ,a ) where a = a+ o ⊗D˜
•i •i τ τ
i τ
Y X
Following the same reasoning, µ = a is an m message and because A does not have
A 2
any parent variables then µ is a constant. Also, A has one child variable (O ) for each
A τ
time step τ ∈ J0,tK and one co-parent variable (S ) for each of them, which implies that
τ
there are t +1 m messages for A, i.e. µ (o ,D˜ ) = o ⊗D˜ ∀τ ∈ J0,tK. Because
3 Oτ→A τ τ τ τ
the O are observed, we know that the m messages transmitted by this node will be the
τ 5
observation made at time τ (o ). Additionally, the m message from the hidden variables
τ 4
S are the expectation of their sufficient statistics, i.e. hu (S )i = D˜ . This confirms
τ Sτ τ QSτ τ
the idea that µ is a function of the sufficient statistics of the child and co-parent
Oτ→A
variables. Figure 17 concludes this paragraph with a visual representation of the messages
for A.
Pα
α
PD PB Pπ
D B π
PS0
S ... S ... S
0 PSt t PST T
PA
m m
5 4
A
PO0 POt
a = a+P t
τ=0
o
τ
⊗D˜
τ
O O m m
0 t 2 3
Figure 17: This figure illustrates the passing of messages required to update the posterior
over A. The messages of type m , m , m and m come from the parent factors, child
2 3 4 5
factors, co-parent variables and child variables, respectively.
37
Champion et al.
Messages for B:
Q∗(B) = Dir(B[u] ,b[u] ) where b[u] = b[u]+ α˜ D˜ ⊗D˜
•i •i k τ τ−1
Y
u,i (k,
X
τ)∈Ωu
Sticking with this reasoning, µ = b is an m message and becauseB does not have any
B 2
parent variables then µ is a constant equal to b. Also, B has one child variable (S ) for
B τ
each time step τ ∈ J1,TK and all policies ∀π ∈ J1,|π|K, along with two co-parent variables
(S and π) for each of those child variables. This implies that there are T × |π| m
τ−1 3
messages for B, i.e. µ (α˜ ,D˜ ,D˜ ) = α˜ D˜ ⊗ D˜ , ∀τ ∈ J1,TK, ∀π ∈ J1,|π|K
Sτ→SB k τ τ−1 k τ τ−1
where D˜ is an m message and α˜ along with D˜ are m messages.
τ 5 k τ−1 4
Messages for π:
T hD˜ τ ⊗D˜ τ−1 ,B¯[U τ 1 −1 ]i F
Q∗(π) = Cat(π;α∗) where α∗ = σ α¯+ F and F = ...
τ τ
 
τ=1 ! hD˜ ⊗D˜ ,B¯[U |π| ]i
X τ τ−1 τ−1 F
 
If we keep applying the same reasoning, we see that µ (α¯)= α¯ is an m message, which
π 2
is a function of the sufficient statistics of the parent variable α (m message). Moreover,
1
π has one child variable (S ) for each time step τ ∈ J1,TK, and for each of those child
τ
variables, π has two co-parent variables (S and B). Therefore, µ = F ∀τ ∈ J1,TK
τ−1 Sτ→π τ
correspond to T m messages. Those messages are function of two m messages (D˜ and
3 4 τ−1
B¯) and one m message (D˜ ).
5 τ
Messages for S :
τ
Q∗(S ) =Cat(S ;σ(µ∗ ))
τ τ Sτ
µ∗ = [τ =0]D¯ +[τ 6= 0] α˜ B¯[Uk ]D˜ +[τ ≤ t]o ·A¯+[τ 6= T] α˜ D˜ ·B¯[Uk]
Sτ k τ−1 τ−1 τ k τ+1 τ
k k
X X
To understand the above equation, we can consider two cases: τ = 0 and τ 6= 0. In the
first case, S only has one parent variable (D), and µ (D¯) = D¯ where D¯ = hu (D)i
0 S0 D QD
is a message from a parent variable (m message). In the second case, S has three parent
1 τ
variables (S , B and π), and µ (D˜ ,B¯,α˜) = α˜ B¯[Uk]D˜ where D˜ , B¯ and
τ−1 Sτ τ−1 k k τ τ−1 τ−1
α˜ are also m messages. Let us now think about the child variable(s) of S . If τ ≤ t, then
1 τ
S has a child variable from the likelihood mapping P and µ (o ,A¯) = o ·A¯, where o
τ Oτ→Sτ τ τ τ
is a message from the child variable (m message) and A¯ is a message from the co-parent
5
variable (m message). Additionally, if τ 6= T, then S receives a message from the future
4 τ
µ (α˜ ,D˜ ,B¯) = α˜ D˜ ·B¯[Uk], where α˜ and B¯ are m messages and D˜
Sτ+1→Sτ k τ+1 k k τ+1 τ k 4 τ+1
is a m message. Figure 18 concludes this section with an illustration the message passing
5
P
procedure for S .
0
38
Active Inference and Variational Message Passing
Pα
α
PD
PB Pπ
D
B π
PS0
S0 ... PSt St ... PST ST
PA
m1 m5 m4 m4 m5 m4
A
PO0 POt Q∗ (S0|π)=σ (cid:16) D¯ +o 0 · A¯ + k α˜k D˜ 1 · B¯ (U 0 k ) (cid:17)
P
O0 Ot m2 m3 m3
Figure 18: This figure illustrates the passing of messages required to update the posterior
over S . The messages of type m , m , m , m and m come from the parent variables,
0 1 2 3 4 5
parent factors, child factors, co-parent variables and child variables, respectively.
7.8 Messages vs update equations
In this section, we present a side by side comparison of the messages obtained using vari-
ational message passing and the update equations that underwrite belief updating in the
active inference literature. Throughout this section, the messages will always be presented
first, followed by the equivalent update equations. Let us start with the random variable
D:
Q∗(D) = Dir(D;d+D˜ )
0
Q∗(D) = Dir(D;d+s )
0
These two equations only differ in terms of labels, i.e. s and D˜ conceptually represent
0 0
the same quantity. Similarly, the updates of A are recovered up to a change of label:
t
Q∗(A) = Dir(A ,a ) where a = a+ o ⊗D˜
•i •i τ τ
i τ=0
Y X
t
Q∗(A) = Dir(A ,a ) where a = a+ o ⊗s
•i •i τ τ
i τ=0
Y X
39
Champion et al.
The update of B slightly differs from the messages obtained from variational message pass-
ing, which follows from the fact that we modified the variational distribution:
Q∗(B) = Dir(B[u] ,b[u] ) where b[u] = b[u]+ α˜ D˜ ⊗D˜
•i •i k τ τ−1
Y
u,i (k,
X
τ)∈Ωu
Q∗(B) = Dir(B[u] ,b[u] ) where b[u] = b[u]+ π sk ⊗sk
•i •i k τ τ−1
Y
u,i (k,
X
τ)∈Ωu
The only conceptual difference here is that sk depended upon the policy, while D˜ does not.
τ
Concerning S , we have re-arranged the update equation to highlight the similarity with
τ
the messages:
Q∗(S ) =Cat(S ;σ(µ∗ ))
τ τ Sτ
µ∗ = [τ =0]D¯ +[τ 6= 0] α˜ B¯[Uk ]D˜ +[τ ≤ t]o ·A¯+[τ 6= T] α˜ D˜ ·B¯[Uk]
Sτ k τ−1 τ−1 τ k τ+1 τ
k k
X X
µ∗ = [τ =0]D¯ +[τ 6= 0] B¯[Uπ ]sπ +[τ ≤ t]o ·A¯+[τ 6= T] sπ ·B¯[Uπ]
Sτ τ−1 τ−1 τ τ+1 τ
There are two main differences here. First, as for B, sk is replaced by D˜, which does not
τ
depend on the policies. Second, the past and future messages have an average over the
policies, while the updates do not. Unsurprisingly, since we replaced γ by α and changed
the type of distributions, the updates are quite different:
Q∗(α) = Dir α;θ+α˜
(cid:16) (cid:17)
Q∗(γ) = Γ γ;1,β +G·(π−π )
0
(cid:16) (cid:17)
We conclude this section with the messages and updates of π, which are formally distinct.
These differences come from the fact that we moved G from P(π|γ) to P(α) and turned
P(π|γ) into a categorical distribution P(π|α):
Q∗(π) = Cat(π;α∗)
T hD˜ τ ⊗D˜ τ−1 ,B¯[U τ 1 −1 ]i F
α∗ = σ α¯ + F and F = ...
τ τ
 
τ=1 ! hD˜ ⊗D˜ ,B¯[U |π| ]i
X τ τ−1 τ−1 F
 
T
1
α∗ = σ − G+ F and F = sπ ·B¯[U]sπ
β τ τ τ τ−1
!
τ=1
X
However, the general form of the updates remains unchanged with information coming
from the parent through α¯ and −1G, and from each child through the summation over
β
time steps.
40
Active Inference and Variational Message Passing
8. Conclusion
The increasing use of active inference in neuroscience has cast many brain processes as
Bayesian inference, the update equations of which can be thought of as a message passing
procedure. The first goal of this paper was to present a complete overview of the active
inference framework in discrete time and state space (Section 5) as well as a formal intro-
duction to the variational message passing literature (Section 6). Then, we simplified the
generative model and the variational distribution usually adopted in the active inference to
derive a new set of update equations using the method of Winn and Bishop (2005) — and
highlight the connection between active inference and variational message passing (Section
7).
We hope that the first few sections of this paper could be useful as an introduction
to variational inference, Forney factor graphs, active inference or/and variational message
passing. Section 7 might also be of interest to researchers searching for a clear link be-
tween active inference and variational message passing or researchers seeking to derive the
update equations of new generative models. Section 7 explains why a fully factorised varia-
tional distribution simplifies the expected free energy in a way that precludes risk sensitive
behaviour but preserves ambiguity avoidance. Finally, we note that this issue does not
confound generative models implementing tree search.
Onemightask whypreviousformulations ofbeliefupdatingormessagepassinginactive
inference have not exploited the simplifications considered in the current paper. For exam-
ple, usinga Dirichlet distribution to parameterise Bayesian beliefs over policies — or a fully
factorised variational distribution that would simplify message passing. One answer is that
much of the legacy literature in active inference is concerned with neuronal process theories
and biological implementation. For example, the only reason a Gibbs form was used for
the distribution over policies was to link the implicit temperature or sensitivity parameter
to dopaminergic discharges. Similarly, the minimisation of variational free energy — using
a gradient descent to implement structured variational message passing — was motivated
by the need to cast belief updating in terms of differential equations that could be plausi-
blyassociated withneuronaldynamics(andaccompanying electrophysiological responsesto
observations). However, if one frees oneself from the constraints of biological implementa-
tion, the repertoire of established schemes in machine learning and Bayesian statistics can,
in principle, be leveraged to reproduce kinds of choice behaviour active inference is trying
to explain and emulate. This paper has highlighted the putative usefulness of variational
message passing under a rationalisation of generative models.
It is interesting to consider whether the simplified expected free-energy — resulting
from our message passing formulation of active inference — can be linked in any sense to
humanbehaviour,whethernormativeorpathological. Inparticular,thefree-energywehave
obtained reflects a very specific functional impoverishment. The full factorisation that is
necessary for vanilla message passing precludes the ability to conditionalize the variational
posterioronpolicies. Thissuggestsaparticulardeficitintheability toplan,andablindness
to futurepossibilities, the uncertainty associated with those possibilities and their potential
to satisfy preferences. As a result, the agent’s objective becomes to seek out unambiguous
cues, with no concern for outcome.
41
Champion et al.
In fact, humans do exhibit patterns of behaviour that — due to their repetitiveness —
seem to reflect a desire for high predictability. Additionally, some of these patterns do not
seem obviously connected to rewarding or punishing outcomes. For example, those with
autism can exhibit very stereotyped repetitive behaviour: hand flapping, hand clapping,
rocking,etc(Gabriels,2005),whichisoftendescribedasstimming(Sundar Rajagopalan et al.,
2013). Theserepetitive andritualistic behaviours(Lam,2007)suggestan objective toavoid
exploration and the associated uncertainty.
This work naturally leads to future directions of research. For example, one could im-
plementthenewgenerative modelproposedinthispaperandcompareitsperformancewith
the model presented in Section 5. Furthermore, additional research needs to be done to
connect the original update equations of active inference to the cluster variational message
passing literature. Much work has already been done on structured variational message
passing; particularly relation to marginal message passing — and its advantages over re-
lated approaches based upon Bethe free energy (Yedidia, 2005; Parr et al., 2019). Another
interesting direction of research would be to design new generative models that can tackle
more complex tasks, such as playing Atari games, human-machine interaction using nat-
ural language and automatic structure learning. Partial answers to these directions of
research have already been provided with the use of deep active inference (Fountas et al.,
2020; Ueltzh¨offer, 2018; Tschantz et al., 2020), deep temporal models (Friston et al., 2018;
Heins et al.,2020)andBayesian modelreduction (Friston et al.,2018;Friston et al.,2017a;
Wauthier et al., 2020). Nevertheless, we anticipate that additional work will pursue these
avenues of research. Finally, one could also compare the update schemes under VMP to
belief propagation (Yedidia, 2011) or marginal message passing (Parr et al., 2019).
Acknowledgments
We would like to thank Karl Friston as well as the reviewers for their valuable feedback,
which greatly improved the quality of the present paper.
Appendix A: Active Inference, KL Control and Reinforcement Learning.
This appendix focuses on the relationship between Active Inference, KL Control and Rein-
forcement Learning (cf. Da Costa et al. (2020b) and Levine (2018) for more details). Let
us restart with the expected free energy given by Equation 6:
expected outcomes priorpreferences
T
G(π) ≈ D [ Q(O |π) || P(O ) ] + E [H[P(O |S )]].
KL τ τ Q(Sτ|π) τ τ
τ=t+1
X z }|expe{cted risk z }| { expected ambiguity
| {z } | {z }
If the expected ambiguity is equal to zero, then the expected free energy reduces to
the expected risk, which is the cost function minimised in the KL control literature. This
highlights that active inference generalises KL control (Rawlik et al., 2013) by taking into
account the ambiguity of the mapping between the hidden states and the observations.
Active inference therefore selects policies leading to unambiguous states. Furthermore, the
42
Active Inference and Variational Message Passing
expected risk can be re-written as follows:
expected risk = D [Q(O |π)||P(O )] = E [lnQ(O |π)]− E [P(O )].
KL τ τ Q(Oτ|π) τ Q(Oτ|π) τ
negativeentropy expected rewards
| {z } | {z }
If the negative entropy is zero, then the expected free energy reduces to the negative
expected prior preference. Thosepreferences encode the notion of good outcomes, or equiv-
alently, the notion of rewarding observations. This highlights why active inference can be
thought of as a generalisation of reinforcement learning (Mnih et al., 2013). Another view
on the expected free energy is:
(-ve)epistemicvalue extrinsicvalue
G(τ,π) = E [lnQ(S |π)−lnP(S |O ,π)]−E [lnP(O |π)], (39)
Q˜ τ τ τ Q˜ τ
z }| { z }| {
where Q˜ = P(O |S )Q(S ). The extrinsic value is another term for expected prior prefer-
τ τ τ
ences, whichis equivalent toexpected rewardsin reinforcementlearning. Itis worthlooking
in more detail at the negative epistemic value (-EV), which differentiates the learning ob-
jectives of reinforcement learning and active inference:
epistemicvalue
−EV = −−E [lnQ(S |π)−lnP(S |O ,π)]
Q˜ τ τ τ
⇔ EV = E z[lnP(S |O ,π)}−| lnQ(S |π)]. {
Q˜ τ τ τ
mutualinformationbetween Sτ andOτ
Thus, the epistemic value is app|roximately equ{azl to the mutua}l information between S
τ
and O . The mutual information encodes the expected information gain over one variable
τ
by knowingthevalue of another. Therefore,theepistemic valuetells ushow knowingfuture
observations reduces our uncertainty over future hidden states. The following should help
to see that the epistemic value is approximately equal to the mutual information between
S and O :
τ τ
I(S;O) = D [P(S ,O )||P(S )P(O )]
KL τ τ τ τ
= E [lnP(S |O )+lnP(O )−lnP(S )−lnP(O )]
P(Sτ,Oτ) τ τ τ τ τ
= E [lnP(S |O )−lnP(S )].
P(Sτ,Oτ) τ τ τ
Intuitively, the more an observation tells us about future states, the more valuable this
observationis. Thenegative epistemicvaluefromequation39directlyreflectsthisintuition,
andfavoursthepolicieswithhighmutualinformation. Moreimportantly,equation39allows
the agent to compare the information gain and the reward on the same scale, i.e. using
nats from information theory. This creates a sense in which an active inference agent deals
optimally with the trade-off between exploration and exploitation.
Appendix B: Useful Properties.
This appendix quickly reviews the properties used throughout this paper.
43
Champion et al.
Product rule: P(X,Y) = P(X|Y)P(Y),
where X and Y are random variables.
Linearity of expectation: E [aY +b]= aE [Y]+b,
P(Y) P(Y)
where a and b are constants, and Y is a random variable.
Expectation of a constant: E [a] = a,
P(Y)
where a is a constant, and Y is a random variable
Log property: ln(ab) = ln(a)+ln(b),
where a and b are real numbers
Exponential product property: exp(a+b)= exp(a)exp(b),
where a and b are real numbers
Exponential power property: exp(ab) = exp(a)b,
where a and b are real numbers
Appendix C: Definition and Justification of the Expected Free Energy.
In this appendix,we focus on the definition of the expected free energy and the justification
ofEquation6. Anothergoodresourceonthesubjectisthe“expected freeenergy”appendix
of Smith et al. (2021). For the sake of simplicity, we assume the following generative model
and variational distribution:
T T
P(O ,S ,B|π) = P(B)P(S ) P(S |S ,B,π) P(O |S )
0:T 0:T 0 τ τ−1 τ τ
τ=1 τ=0
Y Y
T
Q(S ,B|π) = Q(B) Q(S |π).
0:T τ
τ=0
Y
Furthermore, we let X = {B,S } denote the set of hidden variables of the model.
0:T
Note that in this appendix, we restrict ourself to the hidden variables X but new variables
such as A and D can be added without changing the idea of the following derivation.
Initially, the expected free energy was defined as the variational free energy conditioned on
the policy, i.e.
G(π) = D [Q(X|π)||P(O ,X|π)].
KL 0:t
However, the above definition does not take into account that observations will bemade
in the future. To make up for this, the expected free energy can be extended as follows:
G(π) = E D [Q(X|π)||P(O ,X|π)] where Q˜ =∆ Q˜(O |π). (40)
Q˜ KL 0:T t+1:T
h i
Since the future observations (O ) have not been made yet, we need to predict what
t+1:T
they could look like. This prediction relies on a predictive distribution Q˜(O |π) that
t+1:T
encodes our best guess about future outcomes, and is generally defined as follows:
T
Q˜(O |π) =∆ Q˜(O |π),
t+1:T τ
τ=t+1
Y
44
Active Inference and Variational Message Passing
where Q˜(O |π) =∆ Q˜(O ,S |π) and Q˜(O ,S |π) =∆ P(O |S )Q(S |π).
τ τ τ τ τ τ τ τ
X
Sτ
Note that the definition of Q˜(O |π) assumes independence between time steps and
t+1:T
Q˜(O |π) is obtained by marginalisation of Q˜(O ,S |π). By recalling the definition of the
τ τ τ
generative model as well as the definition of the variational distribution, we obtain the
following from Equation 40:
G(π) = E D [Q(S ,B|π)||P(O ,S ,B|π)]
Q˜ KL 0:T 0:T 0:T
h i
= D [Q(B)||P(B)]+D [Q(S |π)||P(S )]
KL KL 0 0
t X
+ E D [Q(S |π)||P(S |S ,B,π)]
Q(Sτ−1,B|π) KL τ τ τ−1
X τ=1 h i
t
+ E H[P(O |S )]
Q(Sτ|π) τ τ
X τ=0 h i
T
+ E D [Q(S |π)||P(S |S ,B,π)] +E H[P(O |S )] .
Q(Sτ−1,B|π) KL τ τ τ−1 Q(Sτ|π) τ τ
τ X =t+1 h i h i
It must now be mentioned that the policy does not have much of an impact on the
past and current hidden states (S ). The terms relying on those states are then removed
0:t
from the expected free energy to avoid unnecessary computational costs. Additionally,
the divergence between Q(B) and P(B) does not depend on the policy and can be safely
ignored, leading to:
T
G(π) = G(π,τ) (41)
τ=t+1
X
where:
G(π,τ) =∆ E D [Q(S |π)||P(S |S ,B,π)] +E H[P(O |S )] .
Q(Sτ−1,B|π) KL τ τ τ−1 Q(Sτ|π) τ τ
h i h i
WenowfocusonG(π,τ)tobridgethegap betweenEquations6and41. First, wemerge
the two terms of the above equation together:
G(π,τ) =∆ E lnQ(S |π)−lnP(O ,S |S ,B,π) .
P(Oτ|Sτ)Q(Sτ,Sτ−1,B|π) τ τ τ τ−1
h i
Then, we break the second term within the expectation using the product rule. Addi-
tionally, we realise that the following equation can be obtained from the product rule:
P(O ,S ,B,π) P(S ,B,π|O )
P(O |S ,B,π) =
τ τ−1
=
τ−1 τ
P(O ) ≈ P(O ),
τ τ−1
P(S ,B,π) P(S ,B,π)
τ τ
τ−1 τ−1
where we assumed that the fraction is equal to one. Doing this assumption means that
the observation O brings us very little information, i.e. the posterior is close to the prior.
τ
45
Champion et al.
Using the above result we get:
G(π,τ) = E lnQ(S |π)−lnP(S |O ,S ,B,π)−lnP(O |S ,B,π)
τ τ τ τ−1 τ τ−1
h i
≈ E lnQ(S |π)−lnP(S |O ,S ,B,π)−lnP(O ) ,
τ τ τ τ−1 τ
h i
wheretheexpectation is stillover P(O |S )Q(S ,S ,B|π). Then,weusesBayes theorem
τ τ τ τ−1
on the second term, the fact that (O ⊥⊥S ,B,π)|S and the log properties to get:
τ τ−1 τ
G(π,τ) = E lnQ(S |π)−lnP(S |O ,S ,B,π)−lnP(O )
τ τ τ τ−1 τ
h P(O |S ,S ,B,π)P(S |S ,iB,π)
= E lnQ(S |π)−ln τ τ τ−1 τ τ−1 −lnP(O )
τ τ
P(O |S ,B,π)
τ τ−1
h i
P(O |S )Q(S |π)
≈ E lnQ(S |π)−ln τ τ τ −lnP(O )
τ τ
Q(O |π)
τ
h i
= E lnQ(O |π)−lnP(O )−lnP(O |S ) ,
τ τ τ τ
h i
where we assumed that P(S |S ,B,π) ≈ Q(S |π) and P(O |S ,B,π) ≈ Q(O |π).
τ τ−1 τ τ τ−1 τ
The first assumption can be supported by the variational free energy (VFE) decomposition
in term of accuracy and complexity. Indeed, the VFE penalises the divergence between
Q(S |π) and P(S |S ,B,π). The second assumption can be supported as follows:
τ τ τ−1
P(O |S ,B,π) = P(O ,S |S ,B,π)
τ τ−1 τ τ τ−1
X
Sτ
≈ Q(O ,S |π)
τ τ
X
Sτ
= Q(O |π).
τ
Assuming that the posterior P(O ,S |S ,B,π) can be approximated by Q(O ,S |π).
τ τ τ−1 τ τ
Thelaststepreliesonthelinearity ofexpectation andtheexpectation ofaconstant, leading
to the final result:
G(π,τ) = D [Q(O |π)||P(O )]+E H[P(O |S )] .
KL τ τ Q(Sτ|π) τ τ
h i
Appendix D: The simplest generative model.
Thisappendixprovidesthereaderwiththesmallestgenerativemodelthatcanbeconsidered
as an active inference agent and aims to solve the k-armed bandit problem. As shown in
Figure 19, this problem is composed of k slot machines or equivalently k actions that the
agent can perform. Each machine has a different probability of producing a reward and
the agent must chose the action to perform to maximize the rewards obtained. The agent
only observes either a reward or a punishment after the execution of an action. Additional
information related to the usage of active inference in the context of the multi-arms bandit
(MAB) task can befoundin(Markovic et al.,2021)whereactive inferencewas comparedto
other major algorithms for solving MABs such as UCB sampling and Thompson sampling.
46
Active Inference and Variational Message Passing
U =1 U =2 U =3
Actions
U
Outcomes
O =1 O =2
O
Figure 19: This figure illustrates the 3-armed bandit problem and the generative model
used by the agent. Three slot machines are available to the agent and each machine has a
different probability of producing a reward. Additionally, there are two possible outcomes
when pulling a lever, the agent either wins plenty of money or gets nothing. The generative
model is composed of two nodes representing the possible outcomes and actions. Finally,
the agent’s goal is to maximize the rewards obtained, by picking the best strategy.
To solve the bandit problem using active inference, the first step is to create the gen-
erative model that encodes the agent’s beliefs of the environment. Two random variables
are used for this purpose, O represents the possible outcomes and U the available actions.
Furthermore, P(O|U) determines how the observation depends on the action performed by
theagent, andP(U)encodes anypriorpreferenceover theavailable actions. Moreprecisely,
P(O|U) and P(U) are categorical distributions defined as follows:
P(O = i|U = j) =A and P(U = j) = a ,
ij j
whereA definestheprobability ofthei-thoutcomegiven thatthej-thaction isperformed,
ij
and a encodes the prior over the j-th action. Note that even if the active inference frame-
j
work provides a way to learn the matrix A, this section assumes that it is given to the
agent. The next step is to pick an inference method to compute the posterior over the
hidden state U. This section keeps things simple and uses Bayes theorem:
P(O = 1|U = j)P(U = j) P(O = 1|U = j)P(U = j) A a
1j j
P(U = j|O = 1) = = = ,
P(O = 1) P(O = 1|U = k)P(U = k) A a
k k 1k k
where the definition of the generative model hasPbeen used in the last step and wPe condi-
tioned on O = 1 to infer the action that is more likely to be rewarding. At this point, it
is possible to act in our environment either by sampling the next action to perform from
the posterior P(U|O = 1) or by picking the action with the highest posterior probability.
Additionally, the posterior can be reused as an empirical prior for the next time step as
follows:
A a
1j j
P(U = j) ← P(U = j|O = 1) = .
A a
k 1k k
ThissimpleexampledoesnotcapturetheentiretheorPetical powerof theactive inference
framework. Nevertheless, it illustrates four important concepts related to the design and
47
Champion et al.
use of an active inference agent, namely, the design of a generative model, the inference
of the latent variable(s), the action selection process, and the use of the posterior as an
empirical prior.
Appendix E: Possible future research.
In this appendix, we propose future research directions aiming to understand the relation-
ship between P(π|γ) and P(π|α). The first direction relies on the following link between
Dirichlet and gamma distributions. If we let X ,...,X be mutually independent random
1 k
variables, each having a gamma distribution with parameters θ for i = 1,...,k and if we
i
define Y = Xi for i = 1,...,k, then (Y ,...,Y ) ∼ Dir(θ ,...,θ ). This naturally
i X1+...+Xk 1 k 1 k
leads to the hypothesis that the new generative model might be a generalisation of the old
generative model when all θ are equal.
i
Another interesting fact that could be studied in more detail comes from studying the
variance of the Dirichlet distribution. Recall that the variance of the random variable Y is
i
given by:
θ˜(1−θ˜)
i i
Var[Y ]= ,
i
θ +1
0
where θ˜ = θi and θ = k θ . If we stick to our definition of θ, i.e. θ = c−G with
−→
i θ0 0 j=1 j j j
c = c ∀j, then we can study how the variance of Y behaves as c goes to infinity. Let us
j j
P
begin with:
θ c−G c−G c 1
lim θ˜ = lim i = lim i = lim i = lim = ,
c→+∞ i c→+∞ k θ c→+∞ k c−G c→+∞kc− k G c→+∞kc k
j=1 j j=1 j j=1 j
where we note thaPt G and k GPbecome negligible as c→ +P∞. Returning to the limit
i j=1 j
of the variance:
P
θ˜(1−θ˜) θ˜(1−θ˜)
lim Var[Y ]= lim = lim = 0,
i
c→+∞ c→+∞ θ 0 +1 c→+∞ k c−G +1
j=1 j
(cid:16) (cid:17)
where we used the fact that θ˜ tends towards 1 (i.e. P a constant w.r.t c) and therefore
i k
the variance is only influenced by the c in the denominator, which tends towards +∞.
Additionally, from the definition of the mode of the Dirichlet, we see that as c → +∞
then the mode of the distribution tends towards the centre of the simplex because the G
i
becomes negligible, i.e.
lim m = 1 ... 1 .
c→+∞ α k k
Combining the behaviour of the variance an(cid:2)d the mod(cid:3)e as c → +∞, we see that as c
increases the prior becomes more and more compact around the centre of the simplex. In
other words, the policy selection becomes more and more stochastic as c increases. This is
not without recalling the role of γ as highlighted previously in the caption of Figure 9.
Appendix F: Messages for B.
In this appendix, we provide the derivation of the messages for B, which relies on the
conjugacybetweenacategorical andaDirichletdistribution. Letusstartwiththedefinition
48
Active Inference and Variational Message Passing
of P(B;b), which is a productof Dirichlet distributions that can bewritten in the following
form:
lnP(B;b) = ln P(B[u] ;b[u] )= lnDir(B[u] ;b[u] )
•i •i •i •i
i,u i,u
Y X
b[u] −1 lnB[u]
1i 1i
= ... · ... −lnB(b[u] )
   
•i
X i,u b[u] |S|i −1 lnB[u] |S|i
   
LogartithmofDirichlet
b[1] −1 lnB[1]
| 11 {z 11 }
= ... · ... − lnB(b[u] ), (42)
   
•i
b[|U|] |S||S| −1 lnB[|U|] |S||S| X i,u
   
µB(b) uB(B) zB(b)
| {z } | {z } | {z }
where |U| is the number of possible actions. Let Ja,bK denotes all the natural numbers
between a and b (inclusive). The random matrix B[u] has one child S for each time
τ
step τ ∈ J1,TK where action u has been predicted by the m-th policy, and its probability
mass function is given by Equation 32. Similarly, the probability mass function of S is
τ−1
obtained from Equation 32 by decreasing all indexes τ by one. The first step requires us to
re-write Equation 32 as a function of u (B). This can be done by using the definition of
B
the dot product and re-arranging to obtain:
[π = k][Uk = 1][S = 1][S = 1]
k τ−1 τ−1 τ
lnP(S = k|B,S = l,π = m) = ... ·u (B).
τ τ−1 B
 P 
[π = k][Uk = |U|][S = |S|][S = |S|]
k τ−1 τ−1 τ
 
P µSτ→B(Sτ,Sτ−1,π)
(43)
| {z }
The second step aims to substitute Equations 42 and 43 within the variational message
passing equation (18), i.e.
b[1] −1
11
lnQ∗(B) = ... ·u (B)
B
 
b[|U|] −1
D |S||S| E
T  k [π =  k][U τ k −1 = 1][S τ−1 = 1][S τ = 1]
+ ... ·u (B) +Const,
B
 P 
X τ=1D k [π = k][U τ k −1 = |U|][S τ−1 = |S|][S τ = |S|] E
 
where h•i refers to h•i
∼
P
QB
. Note that in the above Equation, b[u]
ij
are hyper parameters
that can therefore be considered as constants with respect to the expectation h•i
∼QB
. The
thirdstep buildsonthis insight, by pullingthesummation over time stepsinsidethevector,
factorising by u (B), using the linearity of expectation and by taking the exponential of
B
both sides to obtain:
Q∗(B) ∝exp{µ∗ ·u (B)}
B B
49
Champion et al.
b[1] −1+ h[π = k][Uk = 1][S = 1][S = 1]i
11 k,τ τ−1 τ−1 τ
µ∗ = ... .
B  P 
b[|U|] −1+ h[π = k][Uk = |U|][S = |S|][S = |S|]i
|S||S| k,τ τ−1 τ−1 τ
 
P
By looking at Equations 32, one can see that h[S = i]i and h[S = j]i are the i-th
τ τ−1
and j-th elements of the vector hu (S )i and hu (S −1)i, respectively. Furthermore,
Sτ τ Sτ−1 τ
because P(π) is a categorical distribution it can be expressed as:
lnα [π = 1]
1
P(π|α) = ... · ... , (44)
   
lnα [π = |π|]
|π|
   
µπ(α) uπ(π)
| {z } | {z }
where |π| is the number of policies. The above equation highlights that h[π = k]i is the
k-th element of hu (π)i. Using those three insights, we proceed with the following re-
π
parameterization (i.e. the fourth step):
b[1] −1+ [Uk =1]hu (π)i hu (S )i hu (S −1)i
11 k,τ τ−1 π k Sτ τ 1 Sτ−1 τ 1
µ∗ = ... , (45)
B  P 
b[|U|] −1+ [Uk =|U|]hu (π)i hu (S )i hu (S −1)i
|S||S| k,τ τ−1 π k Sτ τ |S| Sτ−1 τ |S|
 
P
where we focused on the optimal parameters because the rest remains unchanged. The last
step consists of computing the expectation of hu (S −1)i , hu (S )i , and hu (π)i
Sτ−1 τ i Sτ τ j π k
for all i, j and k:
• hu (S −1)i = h[S −1 = i]i = D˜
Sτ−1 τ i τ (τ−1)i
• hu (S )i = h[S = j]i =D˜
Sτ τ j τ τj
• hu (π)i = h[π = k]i = α˜
π k k
One last thing we need to look at is the interaction between the summation and the
indicator function in the i-th line of Equation 45. Indeed, the sum iterates over all time
steps τ and all policies k, but the indicator function filters out all elements where the k-th
policy does not predict the i-th action at time τ −1. Building on this insight, we can now
substitute the above results in Equation 45:
b[1] −1+ α˜ D˜ D˜
11 (k,τ)∈Ω1 k τ1 (τ−1)1
Q∗(B) ∝ exp ... ·u (B) .
 P  B
( b[|U|] −1+ α˜ D˜ D˜ )

|S||S| (k,τ)∈Ω
|U|
k τ|S| (τ−1)|S|

 
P
Finally, one can recognise in the above equation the logarithm of a product of Dirichlet
distributions written into their exponential form, i.e.
Q∗(B) = Dir(B[u] ,b[u] ) where b[u] = b[u]+ α˜ D˜ ⊗D˜ .
•i •i k τ τ−1
Y
u,i (k,
X
τ)∈Ωu
50
Active Inference and Variational Message Passing
Appendix G: Messages for S .
τ
This appendix shows how to derive the messages for S for all time steps. We will use
τ
Equations 26 and 32 that describe P(S |D) and P(S |S ,B,π) as a function of u (S ).
0 τ τ−1 Sτ τ
The first step requires us to re-arrange Equation 31 and P(S |S ,B,π) as a functions of
τ+1 τ
u (S ), whereP(S |S ,B,π) is obtained by addingone to all instances of τ in Equation
Sτ τ τ+1 τ
32. Those two re-arrangements lead to the following results:
[O = i]lnA [S = 1]
i τ i1 τ
lnP(O = k|A,S = l)= ... · ... (46)
τ τ
 P   
[O = i]lnA [S = |S|]
i τ i|S| τ
   
P uSτ (Sτ)
| {z }
[π = k][Uk = u][S = j]lnB[u]
j,k,u τ τ+1 1j
lnP(S = k|B,S = l,π = m) = ... ·u (S ).
τ+1 τ
P 
Sτ τ
[π = k][Uk = u][S = j]lnB[u]
j,k,u τ τ+1 |S|j
  (47)
P
For the second step, we need to substitute Equations 26, 32, 46 and 47 into the vari-
ational message passing equation. If τ = 0, the parent message will come from the prior
(i.e. Equation 26) otherwise from the past (i.e. Equation 32). Also, for all time steps such
that τ ≤ t there is a message from the likelihood mapping (i.e. Equation 46) and for all
time steps except τ = T there is a message from the future (i.e. Equation 47). Putting
everything together we obtain:
lnD
1
lnQ∗(S ) = [τ = 0] ... ·u (S )
τ
 
Sτ τ
lnD
D |S| E
  [π = k][Uk = u][S = j]lnB[u]
j,k,u τ−1 τ−1 1j
+ [τ 6= 0] ... ·u (S )
 P 
Sτ τ
[π = k][Uk = u][S = j]lnB[u]
D j,k,u τ−1 τ−1 |S|j E
 
[O = i]lnA
Pi τ i1
+ [τ ≤ t] ... ·u (S )
 P 
Sτ τ
[O = i]lnA
D i τ i|S| E

[π =
k][Uk
= u][S = j]lnB[u]
P j,k,u τ τ+1 1j
+ [τ 6= T] ... ·u (S )
 P 
Sτ τ
[π = k][Uk = u][S = j]lnB[u]
D j,k,u τ τ+1 |S|j E
 
+Const.
P
The third step requires us to factorise by u (S ), use the linearity of expectation and
Sτ τ
take the exponential of both sides:
Q∗(S ) ∝exp [τ = 0]µ∗+[τ 6= 0]µ∗+[τ ≤ t]µ∗+[τ 6= T]µ∗ ·u (S ) , (48)
τ 1 2 3 4 Sτ τ
( )
h i
51
Champion et al.
where:
hlnD i
1
µ∗ = ...
1  
hlnD i
|S|
 
[Uk = u]h[π = k]ih[S = j]ihlnB[u] i
j,k,u τ−1 τ−1 1j
µ∗ = ...
2  P 
[Uk = u]h[π = k]ih[S = j]ihlnB[u] i
j,k,u τ−1 τ−1 |S|j
 
P
[O =i]hlnA i
i τ i1
µ∗ = ...
3  P 
[O = i]hlnA i
i τ i|S|
 
P
[Uk = u]h[π = k]ih[S = j]ihlnB[u] i
j,k,u τ τ+1 1j
µ∗ = ... .
4  P 
[Uk = u]h[π = k]ih[S = j]ihlnB[u] i
j,k,u τ τ+1 |S|j
 
The fourth step is t P he re-parameterization relying on the fact that hlnD i, h[π = j]i,
i
h[S = k]i, hlnB[l] i, hlnA i and h[S = q]i are elements of hu (D)i, hu (π)i,
τ−1 mn op τ+1 D π
hu (S )i, hu (B)i, hu (A)i and hu (S )i, respectively. Focusing on the µ∗ be-
Sτ−1 τ−1 B A Sτ+1 τ+1 i
cause the rest remains unchanged, the result of the the re-parameterisation is:
hu (D)i
D 1
µ∗ = ...
1  
hu (D)i
D |S|
 
[Uk = u]hu (π)i hu (S )i hu (B)i
j,k,u τ−1 π k Sτ−1 τ−1 j B u1j
µ∗ = ...
2  P 
[Uk = u]hu (π)i hu (S )i hu (B)i i
j,k,u τ−1 π k Sτ−1 τ−1 j B u|S|j
 
P
[O = i]hu (A)i
i τ A i1
µ∗ = ...
3  P 
[O = i]hu (A)i
i τ A i|S|
 
P
[Uk = u]hu (π)i hu (S )i hu (B)i
j,k,u τ π k Sτ+1 τ+1 j B u1j
µ∗ = ... .
4  P 
[Uk = u]hu (π)i hu (S )i hu (B)i
j,k,u τ π k Sτ+1 τ+1 j B u|S|j
 
Finally, the last stePp consists of computing the expectations of all sufficient statistics as
follows:
52
Active Inference and Variational Message Passing
• hu (D)i = hlnD i = ψ(d )−ψ( d )=∆ D¯
D i i i r r i
• hu (π)i = h[π =j]i = α˜ P
π j j
• hu (S )i = h[S = k]i = D˜
Sτ−1 τ−1 k τ−1 (τ−1)k
• hu (B)i = hlnB[l] i = ψ(b[l] )−ψ( b[l] )=∆ B¯[l]
B lmn mn mn r rn mn
• hu (A)i = hlnA i= ψ(a )−ψ( a ) P =∆ A¯
A op op op r rp op
• hu (S )i = h[S = q]i = D˜ P
Sτ+1 τ+1 q τ+1 (τ+1)q
Substituting those expectations into the equations for the µ∗ leads to the following
i
results: µ∗ = D¯, µ∗ = α˜ B¯[Uk]D˜ , µ∗ = o · A¯ and µ∗ = α˜ D˜ · B¯[Uk].
1 2 k k τ τ−1 3 τ 4 k k τ+1 τ
Where o is a one hot vector containing the observation made by the agent and we used the
τ
P P
fact that the indicator function [Uk = u] filters out elements from the sum where u 6= Uk.
τ τ
The final result is obtained by substituting the values of the µ∗’s in Equation 48 to obtain
i
the following categorical distribution:
Q∗(S )∝ exp µ∗ ·u (S )
τ Sτ Sτ τ
n o
µ∗ = [τ = 0]D¯ +[τ 6= 0] α˜ B¯[Uk ]D˜ +[τ ≤ t]o ·A¯+[τ 6= T] α˜ B¯[Uk]D˜ .
Sτ k τ−1 τ−1 τ k τ τ+1
k k
X X
Appendix H: Derivation of the new expected free energy.
In this appendix, we derive the expected free energy of our new model. First, we restate
the factorisation of the generative model and the variational distribution:
P(O ,S ,π,A,B,D,α) = P(π|α)P(α)P(A)P(B)P(S |D)P(D)
0:t 0:T 0
t T
P(O |S ,A) P(S |S ,B,π) (49)
τ τ τ τ−1
τ=0 τ=1
Y Y
T
Q(S ,π,A,B,D,α) = Q(π)Q(A)Q(B)Q(D)Q(α) Q(S ). (50)
0:T τ
τ=0
Y
Remembering from Appendix C that the expected free energy is defined as:
G(π) = E D [Q(X|π)||P(O ,X|π)] , (51)
Q˜ KL 0:T
h i
where the latent variables are X = {S ,A,B,D,α}, Q˜ = Q˜(O ) =∆ T Q˜(O )
0:T t+1:T τ=t+1 τ
and Q˜(O )=∆ Q˜(O ,S ). Now we substitute Equation 49 and 50 into Equation 51 and
τ Sτ τ τ Q
P
53
Champion et al.
simplify by removing the terms that are constant w.r.t the policy π:
G(π)= E D [Q(S ,A,B,D,α|π)||P(O ,S ,A,B,D,α|π)]
Q˜ KL 0:T 0:T 0:T
= D
h
[Q(A)||P(A)]+D [Q(B)||P(B)]+D [Q(D)||P(D
iX
)]
KL KL KL
+D [Q(α)||P(α)]+E D [Q(S )||P(S |D)] X
KL Q(D) KL 0 0
T h iX
+ E D [Q(S )||P(S |S ,B,π)]
Q(Sτ−1,B) KL τ τ τ−1
X τ=1 h i
T
− E lnP(O |S ,A)
Q(Sτ,A)Q˜(Ot+1:T) τ τ
X τ=0 h i
T T
= E D [Q(S )||P(S |S ,B,π)] +C
Q(Sτ−1,B) KL τ τ τ−1
X τ=1 h i X τ=0
T T
= E lnQ(S )−lnP(S |S ,B,π) +C
Q(Sτ,Sτ−1,B) τ τ τ−1
X τ=1 h i X τ=0
T T
= E −E lnP(S |S ,B,π) +C
Q(Sτ−1,B) Q(Sτ) τ τ−1
X τ=1 h (cid:2) H[•] (cid:3) i X τ=0
T T
| {z }
= E H P(S |S ,B,π) +C ,
Q(Sτ−1,B) τ τ−1
X τ=1 h (cid:2) (cid:3) i X τ=0
where H[•] refer to −E
Q(Sτ)
lnP(S
τ
|S
τ−1
,B,π) in the last equation.
(cid:2) (cid:3)
References
Kent C. Berridge. The debate over dopamine’s role in reward: the case for incentive
salience. Psychopharmacology, 191(3):391–431, Apr 2007. ISSN 1432-2072. doi: 10.
1007/s00213-006-0578-x. URL https://doi.org/10.1007/s00213-006-0578-x.
Christopher Bishop and John Winn. Structured variational distributions in vibes. In
Proceedings Artificial Intelligence and Statistics. Society for Artificial Intelligence
and Statistics, Society for Artificial Intelligence and Statistics, January 2003. URL
https://www.microsoft.com/en-us/research/publication/structured-variational-distributions-in-vibes/.
ISBN 0-9727358-0-1.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational infer-
ence: A review for statisticians. Journal of the American Statistical Asso-
ciation, 112(518):859–877, 2017. doi: 10.1080/01621459.2017.1285773. URL
https://doi.org/10.1080/01621459.2017.1285773.
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. War-
muth. Occam’s razor. Information Processing Letters, 24(6):377 – 380,
54
Active Inference and Variational Message Passing
1987. ISSN 0020-0190. doi: https://doi.org/10.1016/0020-0190(87)90114-1. URL
http://www.sciencedirect.com/science/article/pii/0020019087901141.
Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sci-
ences, 16(10):485 – 488, 2012. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2012.
08.006.
Howard Bowman and Su Li. Cognition, concurrency theory and reverberations in the
brain: in search of a calculus of communicating (recurrent) neural systems. In An-
drei Voronkov and Margarita Korovina, editors, Higher-Order Workshop on Auto-
mated Runtime Verification and Debugging, EasyChair Proceedings, Festschrift celebrat-
ing Howard Barringer’s 60th Birthday, volume 1. EasyChair, December 2011. URL
https://kar.kent.ac.uk/30708/.
C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,
S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43,
2012.
Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free
energy principle for action and perception: A mathematical review. Journal of Mathe-
matical Psychology, 81:55 – 79, 2017. ISSN 0022-2496. doi: https://doi.org/10.1016/j.
jmp.2017.09.004.
Th´eophileChampion,HowardBowman, andMarek Grze´s. Active inferenceandtreesearch,
2021.
MarcoCox,ThijsvandeLaar,andBertdeVries. Afactorgraphapproachtoautomatedde-
signofBayesian signalprocessingalgorithms. Int. J. Approx. Reason., 104:185–204, 2019.
doi: 10.1016/j.ijar.2018.11.002. URL https://doi.org/10.1016/j.ijar.2018.11.002.
F. G. Cozman. Generalizing variable elimination in Bayesian networks. Proc.
IBERAMIA/SBIA-2000 Workshops (Workshop on Probabilistic Reasoning in
Artificial Intelligence), 2000. doi: 10.1016/S0004-3702(00)00029-1. URL
https://ci.nii.ac.jp/naid/30008396546/en/.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and
Karl Friston. Active inference on discrete state-spaces: a synthesis, 2020a.
Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. The rela-
tionship between dynamic programming and active inference: the discrete, finite-horizon
case, 2020b.
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine,
reward learning, and active inference. Frontiers in Computational Neuro-
science, 9:136, 2015. ISSN 1662-5188. doi: 10.3389/fncom.2015.00136. URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
55
Champion et al.
Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive ar-
chitecture: A critical analysis. Cognition, 28(1):3 – 71, 1988. ISSN
0010-0277. doi: https://doi.org/10.1016/0010-0277(88)90031-5. URL
http://www.sciencedirect.com/science/article/pii/0010027788900315.
G. D. Forney. Codes on graphs: normal realizations. IEEE Transactions on Information
Theory, 47(2):520–548, 2001.
ZafeiriosFountas,NoorSajid,PedroA.M.Mediano,andKarlFriston. Deepactiveinference
agents using Monte-Carlo methods, 2020.
Charles W. Fox and Stephen J. Roberts. A tutorial on variational bayesian inference.
Artificial Intelligence Review, 38(2):85–95, Aug 2012. ISSN 1573-7462. doi: 10.1007/
s10462-011-9236-8. URL https://doi.org/10.1007/s10462-011-9236-8.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neu-
roscience, 11(2):127–138, Feb 2010. ISSN 1471-0048. doi: 10.1038/nrn2787. URL
https://doi.org/10.1038/nrn2787.
Karl Friston. A free energy principle for a particular physics, 2019.
Karl Friston, Philipp Schwartenbeck, Thomas Fitzgerald, Michael Moutoussis, Tim
Behrens,andRaymondDolan. Theanatomy ofchoice: active inferenceandagency. Fron-
tiers in Human Neuroscience, 7:598, 2013. ISSN 1662-5161. doi: 10.3389/fnhum.2013.
00598. URL https://www.frontiersin.org/article/10.3389/fnhum.2013.00598.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzger-
ald, and Giovanni Pezzulo. Active inference and epistemic value. Cognitive
Neuroscience, 6(4):187–214, 2015. doi: 10.1080/17588928.2015.1020053. URL
https://doi.org/10.1080/17588928.2015.1020053. PMID: 25689102.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O Do-
herty,andGiovanniPezzulo. Activeinferenceandlearning. Neuroscience & Biobehavioral
Reviews, 68:862 – 879, 2016. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev.
2016.06.022.
Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian model reduction. arXiv e-prints,
art. arXiv:1805.07092, May 2018.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophis-
ticated inference, 2020.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson,
and Sasha Ondobaka. Active Inference, Curiosity and Insight. Neural Computa-
tion, 29(10):2633–2683, 10 2017a. ISSN 0899-7667. doi: 10.1162/neco a 00999. URL
https://doi.org/10.1162/neco_a_00999.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation
and active inference. Network Neuroscience, 1(4):381–414, 2017b. doi: 10.1162/NETN\
a\ 00018. URL https://doi.org/10.1162/NETN_a_00018.
56
Active Inference and Variational Message Passing
Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep
temporal models and active inference. Neuroscience & Biobehavioral Reviews, 90:486 –
501, 2018. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev.2018.04.004. URL
http://www.sciencedirect.com/science/article/pii/S0149763418302525.
Michael L; Hill Dina E; Ivers Bonnie J; Goldson Edward Gabriels, Robin L; Cuccaro.
Repetitive behaviors in autism: relationships with associated clinical features. Research
in developmental disabilities, 2005. ISSN 0891-4222.
R. Conor Heins, M. Berk Mirza, Thomas Parr, Karl Friston, Igor Kagan, and Are-
zoo Pooresmaeili. Deep active inference and scene construction. Frontiers in Arti-
ficial Intelligence, 3:81, 2020. ISSN 2624-8212. doi: 10.3389/frai.2020.509354. URL
https://www.frontiersin.org/article/10.3389/frai.2020.509354.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human at-
tention. Vision Research, 49(10):1295 – 1306, 2009. ISSN 0042-
6989. doi: https://doi.org/10.1016/j.visres.2008.09.007. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Vi-
sual Attention: Psychophysics, electrophysiology and neuroimaging.
Masayasu Kojima and Kenji Kangawa. Ghrelin: Structure and function. Phys-
iological Reviews, 85(2):495–522, 2005. doi: 10.1152/physrev.00012.2004. URL
https://doi.org/10.1152/physrev.00012.2004. PMID: 15788704.
D Koller and N Friedman. Probabilistic graphical models, massachusetts, 2009.
F. R. Kschischang, B. J. Frey, and H. . Loeliger. Factor graphs and the sum-product
algorithm. IEEE Transactions on Information Theory, 47(2):498–519, 2001. doi: 10.
1109/18.910572.
K. S. Lam. The repetitive behavior scale-revised : Independent validation in individuals
with autism spectrum disorders. Journal of Autism and Developmental Disorders, 37:
855–866, 2007. URL https://ci.nii.ac.jp/naid/20001501751/en/.
GuillaumeLampleandDevendraSinghChaplot. Playingfpsgameswithdeepreinforcement
learning, 2016.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL
http://yann.lecun.com/exdb/mnist/.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and
review, 2018.
Wu Lin, Nicolas Hubacher, and Mohammad Emtiyaz Khan. Variational message passing
with structured inference networks, 2018.
Dimitrije Markovic, Hrvoje Stojic, Sarah Schwoebel, and Stefan J. Kiebel. An empirical
evaluation of active inference in multi-armed bandits, 2021.
57
Champion et al.
Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Whence the expected free
energy?, 2020.
M. Berk Mirza, Rick A. Adams, Christoph D. Mathys, and Karl J. Friston. Scene
construction, visual foraging, and active inference. Frontiers in Computational Neu-
roscience, 10:56, 2016. ISSN 1662-5188. doi: 10.3389/fncom.2016.00056. URL
https://www.frontiersin.org/article/10.3389/fncom.2016.00056.
M. Berk Mirza, Rick A. Adams, Christoph Mathys, and Karl J. Friston. Hu-
man visual exploration reduces uncertainty about the sensed world. PLOS
ONE, 13(1):1–20, 01 2018. doi: 10.1371/journal.pone.0190429. URL
https://doi.org/10.1371/journal.pone.0190429.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,
Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning,
2013.
KevinMurphy,YairWeiss,andMichaelI.Jordan.Loopybeliefpropagationforapproximate
inference: An empirical study, 2013.
D. Ognibene and G. Baldassare. Ecological active vision: Four bioinspired principles to
integrate bottom–up and adaptive top–down attention tested with a simple camera-arm
robot. IEEE Transactions on Autonomous Mental Development, 7(1):3–25, 2015.
Thomas Parr and Karl J Friston. Generalised free energy and active inference:
can the future cause the past? bioRxiv, 2018. doi: 10.1101/304782. URL
https://www.biorxiv.org/content/early/2018/04/23/304782.
Thomas Parr, Markovic Dimitrije, Stefan J. Kiebel, and Karl J. Friston. Neu-
ronal message passing using mean-field, Bethe, and marginal approxima-
tions. Scientific Reports (Nature Publisher Group), 9(1), Dec 2019. URL
http://library.kent.ac.uk/cgi-bin/resources.cgi?url=https://www.proquest.com/scholarly-journals/neuronal-message-passing-using-mean-field-bethe/docview/2179737260/se-2?accountid=7408.
Copyright - This work is published under http://creativecommons.org/licenses/by/4.0/
(the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this
content in accordance with the terms of the License.
Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, in-
formation geometry and stochastic thermodynamics. Philosophical Trans-
actions of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 378(2164):20190159, 2020. doi: 10.1098/rsta.2019.0159. URL
https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2019.0159.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control
and reinforcement learning by approximate inference (extended abstract). In Proceedings
of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI ’13,
page 3052–3056. AAAI Press, 2013. ISBN 9781577356332.
Wolfram Schultz, Peter Dayan, and P. Read Montague. A neural substrate of prediction
and reward. Science, 275(5306):1593–1599, 1997. ISSN 0036-8075. doi: 10.1126/science.
275.5306.1593. URL https://science.sciencemag.org/content/275/5306/1593.
58
Active Inference and Variational Message Passing
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas H B FitzGer-
ald, Martin Kronbichler, and Karl Friston. Computational mechanisms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbren-
ner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks
and tree search. Nature, 529(7587):484–489, 2016. doi: 10.1038/nature16961. URL
https://doi.org/10.1038/nature16961.
RyanSmith,KarlJ.Friston,andChristopherJ.Whyte. Astep-by-steptutorialonactivein-
ferenceanditsapplicationtoempiricaldata,2021. URLhttps://psyarxiv.com/b4jm6/.
Oleg Solopchuk. Tutorial on active inference, 2018. URL
https://medium.com/@solopchuk/tutorial-on-active-inference-30edcf50f5dc.
Shyam Sundar Rajagopalan, Abhinav Dhall, and Roland Goecke. Self-stimulatory be-
haviours in the wild for autism diagnosis. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV) Workshops, June 2013.
A. Tschantz, M. Baltieri, A. K. Seth, and C. L. Buckley. Scaling active inference. In
2020 International Joint Conference on Neural Networks (IJCNN), pages 1–8, 2020. doi:
10.1109/IJCNN48605.2020.9207382.
Kai Ueltzh¨offer. Deep active inference. Biological Cybernetics, 112(6):547–
573, Dec 2018. ISSN 1432-0770. doi: 10.1007/s00422-018-0785-7. URL
https://doi.org/10.1007/s00422-018-0785-7.
Thijs van de Laar and Bert de Vries. Simulating active inference processes by message
passing. Front. Robotics and AI, 2019, 2019a. doi: 10.3389/frobt.2019.00020. URL
https://doi.org/10.3389/frobt.2019.00020.
Thijs W. van de Laar and Bert de Vries. Simulating active infer-
ence processes by message passing. Frontiers in Robotics and AI, 6:
20, 2019b. ISSN 2296-9144. doi: 10.3389/frobt.2019.00020. URL
https://www.frontiersin.org/article/10.3389/frobt.2019.00020.
Toon Van de Maele, Tim Verbelen, Ozan C¸atal, Cedric De Boom, and Bart Dhoedt.
Active vision for robot manipulators using the free energy principle. Frontiers in
Neurorobotics, 15:14, 2021. ISSN 1662-5218. doi: 10.3389/fnbot.2021.642780. URL
https://www.frontiersin.org/article/10.3389/fnbot.2021.642780.
Hado van Hasselt, ArthurGuez, and David Silver. Deep reinforcement learning withdouble
Q-learning, 2015.
59
Champion et al.
SamuelT.Wauthier, Ozan C¸atal, CedricDe Boom, Tim Verbelen, and Bart Dhoedt. Sleep:
Model reduction in deep active inference. In Tim Verbelen, Pablo Lanillos, Christo-
pher L. Buckley, and Cedric De Boom, editors, Active Inference, pages 72–83, Cham,
2020. Springer International Publishing. ISBN 978-3-030-64919-7.
Wim Wiegerinck. Variational approximations between mean field theory
and the junction tree algorithm. In Craig Boutilier and Mois´es Gold-
szmidt, editors, UAI ’00: Proceedings of the 16th Conference in Uncer-
tainty in Artificial Intelligence, Stanford University, Stanford, California, USA,
June 30 - July 3, 2000, pages 626–633. Morgan Kaufmann, 2000. URL
https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=73&proceeding_id=16.
John Winn and Christopher Bishop. Variational message passing. Journal of Machine
Learning Research, 6:661–694, 2005.
Eric P. Xing, Michael I. Jordan, and Stuart J. Russell. A generalized mean field algo-
rithm for variational inference in exponential families. CoRR, abs/1212.2512, 2012. URL
http://arxiv.org/abs/1212.2512.
J. S. Yedidia. Constructing free-energy approximations and generalized belief propagation
algorithms. IEEE Trans. Information Theory, 51(7):2282–2312, 2005. doi: 10.1109/TIT.
2005.850085. URL https://ci.nii.ac.jp/naid/30019661350/en/.
Jonathan S. Yedidia. Message-passing algorithms for inference and optimization. Jour-
nal of Statistical Physics, 145(4):860–890, Nov 2011. ISSN 1572-9613. doi: 10.1007/
s10955-011-0384-7. URL https://doi.org/10.1007/s10955-011-0384-7.
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation.
In Proceedings of the 13th International Conference on Neural Information Processing
Systems, NIPS’00, page 668–674, Cambridge, MA, USA, 2000. MIT Press.
60