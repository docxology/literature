pymdp: A Python library for active inference
in discrete state spaces
Conor Heins∗1,2,3,4, Beren Millidge4,5, Daphne Demekas6, Brennan Klein4,7,8,
Karl Friston9, Iain D. Couzin1,2,3, and Alexander Tschantz∗4,10,11
1Department of Collective Behaviour, Max Planck Institute of Animal Behavior,
78457 Konstanz, Germany
2Centre for the Advanced Study of Collective Behaviour, 78457 Konstanz, Germany
3Department of Biology, University of Konstanz, 78457 Konstanz, Germany
4VERSES Research Lab, Los Angeles, California, USA
5MRC Brain Networks Dynamics Unit, University of Oxford, Oxford, UK
6Department of Computing, Imperial College London, London, UK
7Network Science Institute, Northeastern University, Boston, MA, USA
8Laboratory for the Modeling of Biological and Socio-Technical Systems,
Northeastern University, Boston, USA
9Wellcome Centre for Human Neuroimaging, Queen Square Institute of Neurology,
University College London, London WC1N 3AR, UK
10Sussex AI Group, Department of Informatics, University of Sussex, Brighton, UK
11Sackler Centre for Consciousness Science, University of Sussex, Brighton, UK
May 6, 2022
Statement of need
Active inference is an account of cognition and behavior in complex systems which brings
together action, perception, and learning under the theoretical mantle of Bayesian inference
[1, 2, 3, 4]. Active inference has seen growing applications in academic research, especially
in ﬁelds that seek to model human or animal behavior [5, 6, 7]. The majority of applications
have focused on cognitive neuroscience, with a particular focus on modelling decision-making
under uncertainty. Nonetheless, the framework has broad applicability and has recently been
applied to diverse disciplines, ranging from computational models of psychopathology [8, 9,
10, 11], control theory [12, 13, 14] and reinforcement learning [15, 16, 17, 18, 19], through
to social cognition [7, 20, 21, 22] and even real-world engineering problems [23, 24, 25].
While in recent years, some of the code arising from the active inference literature has been
∗Correspondence: conor.heins@gmail.com; tschantz.alec@gmail.com
1
arXiv:2201.03904v2  [cs.AI]  4 May 2022
written in open source languages like Python and Julia [26, 27, 28, 29, 19], to-date, the most
popular software for simulating active inference agents is theDEM toolbox of SPM [30, 31].
SPM is a MATLAB library originally developed for the statistical analysis and modelling of
neuroimaging data, such as data collected by functional magnetic resonance imaging (fMRI)
or magneto- and electro-encephalographic (MEG/EEG) [32] methods. TheDEM toolbox, a
sub-library of SPM, was originally developed to simulate and perform Bayesian estimation
of dynamical systems [30], but in the last decade it has been augmented with a series of
demonstrative scripts and simulation routines related to active inference and the Free Energy
Principle more broadly [33, 34, 35].
Simulations of active inference are commonly performed in discrete time and space [36,
3]. This is partially motivated by the mathematical tractability of performing inference with
discrete probability distributions, but also by the intuition of modelling choice behavior as
a sequence of discrete, mutually-exclusive choices, in e.g. psychophysics or decision-making
experiments. The most popular generative models – used to realize active inference in this
context–arepartially-observableMarkovDecisionProcessesor POMDPs [37]. POMDPsare
state-space models that model the environment in terms of hidden states that stochastically
change over time, as a function of both the current state of the environment as well as
the behavioral output of an agent (control states or actions). Crucially, the environment is
partially-observable, i.e. the hidden states are not directly observed by the agent, but can
only be inferred through observations that relate to hidden states in a probabilistic manner,
suchthatobservationsaremodelledasbeinggeneratedstochasticallyfromthecurrenthidden
state.
In most POMDP problems, an agent is tasked with both inferring the hidden states
and selecting a sequence of control states or actions to change the hidden states in a way
that leads to desired outcomes (maximizing reward, or occupancy within some preferred
set of states). DEM contains a reliable, reproducible set of functions for simulating ac-
tive inference agents equipped with such generative models: they includespm_MDP_VB_X.m,
spm_MDP_game.m, and – recently introduced for simulating ‘sophisticated’ active inference –
spm_MDP_VB_XX.m [38]. Despite its robustness and widespread use among active inference
researchers, spm_MDP_VB_X.m is a single function, meaning that any active inference simu-
lation using the function has to comply with the constraints implied by its structure and
control ﬂow. Although options can be speciﬁed that initiate particular sub-routines or vari-
ants of active inference, it is still not straightforward to construct a custom active inference
process from scratch. In practice, this means that novel, bespoke applications require re-
searchers to manually adapt parts ofspm_MDP_VB_X.m for their own purposes, which limits
the general reproducibility and adaptability of academic active inference research, especially
for new practitioners. In addition, since all ofDEM is written in MATLAB, using the tool-
box can be prohibitive due to the cost of a MATLAB license, especially for researchers who
are unaﬃliated with institutions. Increasing interest in active inference, manifested both in
terms of sheer number as well as diversifying applications across scientiﬁc disciplines, has
thus created a need for generic, widely-available, and user-friendly code for simulating ac-
tive inference in open-source scientiﬁc computing languages like Python. The software we
2
present here,pymdp, represents a signiﬁcant step in this direction: namely, we provide the
ﬁrst open-source package for simulating active inference with discrete state-space generative
models. The namepymdp derives from the fact that the package is written in thePython pro-
gramming language and concerns discrete, Markovian generative models of decision-making,
which take the form of Markov Decision Processes orMDPs.
We developed pymdp to increase the accessibility and exposure of the active inference
framework to researchers, engineers, and developers with diverse disciplinary backgrounds.
In the spirit of open-source software, we also hope that it spurs new innovation, development,
and collaboration in the growing active inference community.
Summary
pymdp oﬀers a suite of robust, tested, and modular routines for simulating active inference
agents equipped with partially observable Markov Decision Process(POMDP) generative
models. Mathematically, a POMDP comprises a joint distribution over observationso, hid-
den states s, control statesu and hyperparameters φ: P(o,s,u,φ ). This joint distribution
further factorizes into a set of categorical and Dirichlet distributions: the likelihoods and
priors of the generative model. Withpymdp, one can build a generative model using a set
of prior and likelihood distributions, initialize an agent, and then link it to an external
environment to run active inference processes - all in a few lines of code. The agent and en-
vironment API is built according to the standardized framework of OpenAIGym commonly
used in reinforcement learning, where anAgent and Environment class recursively exchange
observations and actions over time [39].
In order to enhance the user-friendliness ofpymdp without sacriﬁcing ﬂexibility, we have
built the library to be highly modular and customizable, such that agents inpymdp can be
speciﬁed at a variety of levels of abstraction with desired parameterizations. In the next
section, we provide an overview of the structure of the package.
Package structure
The Agent Class
The high-level API oﬀered bypymdp is theAgent class. Instantiating an Agent allows the
user to abstract away the various optimization routines and sub-operations that make up an
active inference process, e.g. state estimation, action selection, and learning. The various
sub-routines of active inference are themselves abstracted as user-friendly methods ofAgent
(such asself.infer_states(obs)), calls to which will run the corresponding function.
Modules
The methods ofAgent themselves call functions from diﬀerent sub-modules ofpymdp. These
submodules can be roughly divided into three sorts of operations:perception, action and
learning. Anattractivefeatureofactiveinferenceisthatvariouscognitiveprocessesnaturally
3
emerge as diﬀerent variants of Bayesian inference. For instance, instantaneous inference
about dynamically-changing hidden states is often analogized toperception (c.f. perception
as inference [40, 41, 42, 43]), whereas inference about slower-changing variables (statistical
regularities in the environment) is analogized to learning [44]. Moreover,action is treated
as a process of inference, where agents select actions by inferring a distribution over control
states or sequence of control states [4]. Each module ofpymdp thus performs inference with
respect to diﬀerent components of a POMDP generative model – we summarize them brieﬂy
below.
The inference library of pymdp contains a set of functions for performing hidden state
inference or state estimation. These are the core functions that allow agents to update their
beliefs about the discrete hidden state of the environment, given observations. Functions
from this library are called by theself.infer_states() method of Agent. Speciﬁc argu-
ments can be passed into theAgent constructor to specify the type and parameterization of
the algorithm used to perform hidden state inference.
The control library of pymdp contains functions for inferring policies and sampling
actions from the posterior beliefs about control states.1 These functions are called internally
by theself.infer_policies() method of Agent.
Finally, thelearning library ofpymdp contains the functions necessary for the agent to
updatehyperparametersofitsgenerativemodel, i.e. Dirichletparametersoverthecategorical
prior and likelihood distributions. These functions are called internally by methods like
self.update_A(), self.update_B(), andself.update_D() of Agent.
For a more detailed overview of the functionality oﬀered by each ofpymdp’s modules,
please see Appendix B: Modules and Theory.
1 from pymdp . agent import Agent
2
3 # here you would set up your generative model
4 my A = ...
5 my_B = ...
6
7 # instantiate your agent with a call to the Agent () constructor
8 my_agent = Agent (A=my_A , B=my_B , C=my_C , D= my_D )
9
10 # define an environment
11 my_env = Env ()
12
13 # set an initial action
14 action = initial_action
15
16 for t in range (T):
17 o_t = my_env . step ( action )
18 my_agent . infer_states ( o_t )
19 my_agent . infer_policies ()
1In active inference ‘control states’ refer to the random variables in the generative model and approxi-
mate posterior, and can be thought of as the agent’s representation of its actions. Actions themselves are
realizations of these random variables, sampled from the posterior over control states.
4
20 action = my_agent . sample_action ()
Example 1: Minimal example of running active inference withpymdp.
Usage
Specifying a generative model is central to active inference, and to Bayesian modelling in
general. Intuitively, a generative model is a probabilistic speciﬁcation of how data or sensory
observations are generated. In the context of Bayesian agent-based models, the generative
model represents an agent’s probabilistic internal model of its environment, comprising a
set of structural assumptions about how the world generates observations and how action
changes the world. In discrete-state and -time active inference models, we typically assume
the generative model is apartially observable Markov Decision Processor POMDP, compris-
ing observations the agent receives,hidden states of the world, andactions the agent can
take to inﬂuence hidden states. Hidden states are called ‘hidden’ precisely because the agent
can never directly access them, but can onlyinfer them via observations. The POMDP
structure assumes that at each timestep, the observation is generated by the current hidden
state, while the hidden state itself changes over time as a function of its current setting and
some control state (i.e. action). Mathematically, a generative model is usually expressed
as a joint probability distributionP(o,s,u,φ ) over observationso, hidden statess, control
states u, and parametersφ. This joint distribution is called agenerative model because it
can be used to sample (or generate) sequences of potential observations according to the
probabilistic structure encoded in the model.
Specifying the generative model in terms of discrete probability distributions is the ﬁrst
step to building an active inference agent inpymdp. Below, we overview the steps involved
in building a generative model to provide intuition and illustrate its simplicity in the special
case of POMDPs.
The POMDP generative model
The POMDP generative model assumed bypymdp is a discrete-time and space generative
model that, like any probability distribution, can be factorized into a product of conditional
distributions (likelihoods) and marginals (priors). The most important of these distributions
–whenwritingdownagenerativemodelin pymdp –are1)theobservationlikelihood P(oτ|sτ),
which represents the agent’s beliefs about how hidden statessgenerate observationsoand 2)
the transition modelP(sτ|sτ,uτ−1), which represents the agent’s beliefs about how hidden
states at some time τ −1 cause hidden states at the next timeτ, conditioned on some
control states (actions)uτ−1. The agent also has a prior over initial hidden statesP(s1),
which represents the agent’s baseline belief, before gathering any observations, about the
probability of the diﬀerent hidden states at the ﬁrst timestep. For the sake of mathematical
convenience, we describe POMDP generative models with a ﬁnite time horizonT, but note
that in generalpymdp does not require a ﬁnite time horizon, so active inference agents can be
5
theoretically run indeﬁnitely (e.g. in streaming applications). Finally, there is an additional
prior distribution over observations,P(o1:T), which speciﬁes an agent’s goals as a desired
distribution over observations. In active inference, goals and desires are encoded as a prior in
the generative model, such that the probability that the model assigns to some conﬁguration
of observations, hidden states, control states and prior parameters is only maximized when
sampling preferred observations (e.g. if my model assigns high probability to observing body
temperature to be 37 degrees, prior preferences are realized when these observations are
sampled). As we will see in the following sections, active inference suggests that perception,
action and learning all work to maximize the marginal likelihood of observations. Prior
preferences only come into play during policy selection because only unobserved outcomes in
the future are random variables (see the section oncontrol.py in Appendix B for details).
The local dependence in time, captured by one-step conditional dependence between
hidden states in the transition likelihood, is what renders POMDPsMarkovian generative
models. As such, a general expression for the joint distribution overo, s, π and φ is as
follows:
P(o1:T,s1:T,π,φ ) = P(φ)P(s1)P(π)
T∏
τ=2
P(sτ|sτ−1,π; φ)
T∏
τ=1
P(oτ|sτ; φ) (1)
We have replaceduhere withπ to represent policies, or sequences of control statesu, i.e.
π = {ui,uj,uk,...}. Control states can be formally related to policies by writing down an
additional likelihood, a ‘policy-to-control’ mappingP(uτ|π), that links a given policy to the
control state it entails at timeτ. Under active inference, agents also performinference about
policies, which naturally entails goal-directed and uncertainty-resolving behavior (see the
section The Expected Free Energy in Appendix B for details on policy inference). Finally,
we capture any additional parameters as a single vector of hyperparametersφ, which might
correspond to the parameters of Dirichlet priors over the likelihood distributionsP(oτ|sτ)
and P(sτ|sτ−1,π), for instance. In active inference, inference about these hyperparameters is
often assumed to occur on a slower timescale than inference about hidden states and policies:
therefore, this process is referred to in the active inference literature as learning [44, 45] (see
learning.py for more details on hyperparameter inference).
Equipped with a generative model, active inference (and Bayesian inference more gener-
ally) entails inference over latent variabless, π and φ, given some observationsot gathered
over time. For a description of the mathematical basis of this inference – and how it is im-
plemented algorithmically inpymdp – please refer to Appendix B: Modules and Theory. Now
that we have expressed the POMDP generative model formally, highlighting its important
components for active inference, we can move on to the representation of these distributions
in pymdp.
Building blocks
pymdp considers generative models of discrete states that evolve in discrete time. This means
that there are an integer number of discrete levels of both the states of the environment and
6
of the observations. Because of this fundamental discreteness, a natural way to represent
the distributions of the generative model is by usingcategorical distributions, which assign
a probability value between 0 and 1 to each discrete outcome level of the distribution’s
sample space, with the usual constraint that the sum of the probabilities over levels is1.
Mathematically, we refer to categorical distributions with the notationP(x) = Cat(φ),φ ∈
{z ∈Rn |zi > 0,∑
izi = 1}. This means that the distribution over the random variable
x is described by a categorical distribution with an n-dimensional vector of parameters
φ, where n is the cardinality of the sample space of X. Numerically, these categorical
distributions can be represented as multidimensional arrays (also known as NDarrays or
tensors) that contain their parameters. These categorical distributions come in two ﬂavors:
vector-valued marginal distributions (e.g. P(x)) usually playing the role of priors, and
conditional categorical distributions (e.g. P(y|x)) in the form of NDarrays (matrices and
tensors), playing the role of likelihoods in the generative model.
Marginal categorical distributions are encoded inpymdp as simple 1-D vectors, which
technically are instances ofnumpy.ndarrays, the core data structure for representing multi-
dimensional arrays in the Python array programming library,numpy [46]. One can easily
instantiate categorical distributions innumpy using calls to the array constructor, e.g.
prior_over_states = np.array([0.5, 0.5]). Conditional categorical distributions are
just collections of 1-D categorical vectors, with as many 1-D vectors as there are levels of the
conditioning variable. Inpymdp, we encode these collections as matrices (2-D NDarrays) and
higher-order NDarrays. For instance, we would encode some discrete conditional distribution
relating two categorical variablesP(y|x) as a matrix of sizeN×M, whereN is the number
of levels of the support random variabley and M is the number of levels of the conditioning
random variablex. We represent such conditional categorical distributions mathematically
with the notationP(y |x) = Cat(φ),φ ∈Rn×m×p×..., where nowφ is a matrix or tensor
of parameters, whose columnsφ•jkl... have the properties of a single categorical distribution,
i.e. φ•jkl... ∈{z ∈Rm |zi >0,∑
izi = 1}.
The core distributions of the generative model – that are encoded in this way – are
the observation likelihood P(oτ|sτ) (also known as the observation model or the sensory
likelihood) and the transition likelihoodP(sτ|sτ−1,uτ−1) (also known as the transition model
or the dynamics model). For all the following descriptions, we borrow notation from theSPM
and DEM standards, which are summarized in papers like [3, 4, 31, 47]. Inpymdp notation,
the observation likelihood is constructed as theA array. In simple generative models2, A will
be aO×S matrix, whereO is the number of outcomes or levels of the observationso, and
S is the number of levels of hidden statess. The entry A[i,j] encodes the probability of
seeing observationigiven statej. In other words, each column of this matrixA[:,j] stores
a vector of categorical parameters that encodes the distributionP(oτ|sτ = sj). Similarly,
the transition likelihood is represented by theB array which is aS ×S ×U NDarray or
tensor, where U is the number of levels of the control states, and entryB[i,j,k] encodes
the probability transitioning to statei at time t from state j at time t−1, when control
state or actionkis taken by the agent. The general structure of numerical representations of
2See the section on Factorized representations for the more general form.
7
conditional distributions inpymdp can be expressed as follows: the ﬁrst dimensions (or rows)
of the matrix or NDarray represent the support of the conditional distribution, while the
lagging dimensions (columns, slices, etc.) represent the random variables being conditioned
on. Thus, for the observation likelihood P(oτ|sτ), the ﬁrst dimension ofA represents the
support ofo (and will have lengthO) while the second dimension represents the support of
s (and will have lengthS).
Beyond the A and B arrays, one can also specify an initial prior over statesP(s1) – in
pymdp this is called theD vector (of lengthS) and represents the agent’s beliefs about the
distribution over hidden states at the ﬁrst timestep of the time horizon (whenτ = 1).
Finally, in order to achieve goal-directed behavior under active inference, it is necessary
to build a representation of some desired state or goal into the generative model. In rein-
forcement learning this is handled using reward functions but in active inference we instead
specify a prior distribution over observations, also known as the ‘prior preferences’ or ‘goal
distribution’ [1]. Inference over control states is then biased by this preference distribution,
leading agents to choose actions that bring them to states that (they expect) will lead to
preferred observations. Inpymdp, this is represented by theC array of lengthO. The default
C array is a vector that is time-independent (the sameC is used for all timesteps), but it is
also possible to specify a time-dependentC array. This can be used to represent goals that
change over time or the desire to reach a speciﬁc goal in a time-dependent manner .
After the generative model has been speciﬁed in terms of a set of likelihood and prior
distributions, one can build an active inference agent in a single line using theAgent()
constructor: e.g. my_agent = Agent(A=A, B=B, ...). The Agent() constructor requires
A and B arrays as mandatory input, whileC and D vectors can be optionally included (the
defaults are uniform distributions for each).
The various methods of the resultingAgent instance can then be used to perform active
inference.
1 import numpy as np
2
3 import pymdp
4 from pymdp import utils , maths
5 from pymdp . agent import Agent
6
7 # create a simple model with one hidden state factor , and one
observation modality
8
9 n_obs = 3
10 n_states = 3
11
12 A = utils . obj_array (1)
13 A [0] = np. array ([[1.0 , 0.0 , 0.0] ,
14 [0.0 , 1.0 , 0.0] ,
15 [0.0 , 0.0 , 1.0]])
16
17 # introduce uncertainty into one of the hidden states
18 inv_temperature = 0.5
19 A [0][: ,2] = maths . softmax ( inv_temperature * A [0][: ,2])
8
20
21 # create a simple transition model with two possible actions
22
23 B = utils . obj_array (1)
24 B [0] = np. zeros ((3 , 3, 2))
25
26 # first action leads to first two states with uncertainty
27 B [0][: ,: ,0] = np. array ([[0.5 , 0.5 , 0.5] ,
28 [0.5 , 0.5 , 0.5] ,
29 [0.0 , 0.0 , 0.0]])
30
31 # second action leads to last state with certainty
32 B [0][: ,: ,1] = np. array ([[0.0 , 0.0 , 0.0] ,
33 [0.0 , 0.0 , 0.0] ,
34 [1.0 , 1.0 , 1.0]])
35
36 # specify prior preferences (C vector )
37 C = utils . obj_array_uniform ([ n_obs ])
38
39 # specify prior over hidden states (D vector )
40 D = utils . obj_array (1)
41 D [0] = utils . onehot (1 , n_states )
42
43 # instantiate your agent with a call to the ‘Agent () ‘ constructor
44 my_agent = Agent (A=A, B=B, C=C, D=D)
45
46 # write a simple environment class , where state depends on the action
probabilistically , and observation is deterministic function of the
state except for state 2, where it ’s randomly sampled
47
48 from pymdp . envs import Env
49
50 # sub - class it from the base Env class
51 class custom_env ( Env ):
52
53 def __init__ ( self ):
54 self . state = 0
55
56 def step (self , action ):
57
58 if action == 0:
59 self . state = 0 if np. random . rand () > 0.5 else 1
60 if action == 1:
61 self . state = 2
62
63 if self . state == 0:
64 obs = 0
65 elif self . state == 1:
66 obs = 1
67 elif self . state == 2:
68 obs = np. random . randint (3)
9
69
70 return obs
71
72 env = custom_env ()
73
74 action = 0
75
76 T = 10 # length of active inference loop in time
77 for t in range (T):
78
79 # sample an observation from the environment
80 o_t = env . step ( action )
81
82 # do active inference
83 qs = my_agent . infer_states ([ o_t ]) # get posterior over hidden
states
84 my_agent . infer_policies ()
85 action = my_agent . sample_action ()
86
87 # convert action into int , for use with environment
88 action = int ( action . squeeze ())
Example 2:Detailed example of building and running an active inference process inpymdp.
Specifying an environment
For most use-cases of active inference, the agent will need to interface with some kind of
environment or external world. The minimal deﬁnition of an environment is just a class
or function that takes actions of the agent as input, updates the true hidden state of the
environment (but does not convey this information to the agent) and returns observations
generated by the updated hidden state. In the Bayesian modelling literature, this environ-
ment is also abstractly referred to as the ‘generative process’ or ‘data-generating process’.
What is important to note is that this generative process does not have to be identical to
the generative model – i.e. there is no requirement that an active inference agent with a
POMDP generative model is operating in a world with discrete, POMDP-like dynamics [48,
28]. All that matters is that the environment accepts the agent’s actions and returns obser-
vations that are discrete and are compatible with the support of the likelihoodP(oτ|sτ) of
the agent’s generative model.
pymdp contains a library of pre-built environments which can be imported usingfrom
pymdp import envs. Following the convention of OpenAI Gym [39], users can also write
their own environment class. This class is traditionally written to have astep() method
which takes an action from the agent as input and returns observations that will be processed
by the agent at the next timestep. In many reinforcement learning and control problem
contexts, the environment has its own internal state that is updated by the agent’s action,
and which determines (either stochastically or deterministically) the next observation.
10
Closing the action-perception loop
The typical ‘active inference loop’ consists of three main steps: 1) sampling an observation
from the environment; 2) updating the agent’s beliefs about states and policies using the
observation; and 3) choosing an action, based on the agent’s posterior over policies (see
Appendix B: Modules and Theory for more details on state and policy inference). Inpymdp,
1) is implemented by calling the environment classenv.step(); 2) is implemented using
the pymdp functions agent.infer_states() and agent.infer_policies() and 3) is im-
plemented using thepymdp function agent.sample_action(). Wrapping these three steps
into a loop over time entails the entire active inference process; see the full example using
the Agent class in Example 2.
Factorized representations
Although many simple POMDPs can be constructed with simple 2-DA matrices and 3-DB
arrays, most of the interesting applications of active inference require what are referred to as
‘factorized representations’. This requires building additional structure into the generative
model, such that observationso are divided into separatemodalities and hidden statess into
separate factors. A multi-modality observationot and multi-factor hidden statest can be
expressed as follows:
ot =
{
o1
t,o2
t,...,o M
t
}
st =
{
s1
t,s2
t,...,s F
t
}
where here the superscript refers to the index of themth observation modality orfth hidden
state factor, respectively. This means that at any given time the agent receives a collection
of discrete observations, where each observation within the collection belongs to a distinct
‘modality’. Thename modalityisused toemphasize theanalogyto diﬀerent sensorychannels
(e.g. vision, audition, somatosensation) in biology that relay diﬀerent sorts of information.
Likewise, in a factorized hidden state representation, the environment’s structure is repre-
sented through several hidden state factors, that may encode distinct features of the world,
each of which may have its own dimensionality, dynamics, and relationship to observations.
Importantly, with such factorized representations, the likelihood arraysA and B become
more complex. In bothpymdp and SPM, we encode a multi-modalityA array as a collection
of sub-arraysA[m], with one for each observation modality. Each modality-speciﬁcA array
then represents the conditional probability of observations for modalitym, given the diﬀerent
conﬁguration of hidden states, i.e.,P(om|s). Note that the ‘ﬁrst’ index of the largerA ar-
ray selects a particular modality from the collection, e.g.A_modality = A[m], whereas the
subsequent multi-indexinto the modality-speciﬁcA array selects conditional probabilities or
arrays of such probabilities, e.g.,A_modality[0, 2, 3, ...]. Each A[m] thus encodes all
probabilistic dependencies between the diﬀerent hidden state factors hidden states and ob-
servations for themth modality: P(om|s) = P(om|s1,s2,...,s F). These complex conditional
relationships are encoded by accordingly higher-dimensional NDarrays inNumPy, with the
number of lagging dimensions encoding the number of hidden state factors that the obser-
vations depend on. Such factorized generative models require more involved belief updating
11
algorithms to achieve posterior inference, usually invoking a factorized approximate poste-
rior (e.g. a mean-ﬁeld factorization), where the full posterior over all hidden state factors is
factorized into a product of marginalsQ(s) = ∏F
i=1 Q(si), where eachQ(si) is the posterior
for hidden state factori. Fortunately,pymdp easily accommodates such higher-dimensional,
factorized generative models and will automatically perform message passing with respect to
such generative models with an arbitrary number of observation modalities and hidden state
factors. See Appendix A: Factorized Generative Models for more details on multi-factor
generative models.
Pedagogical materials & code
For more example code detailing how to usepymdp to simulate active inference in discrete
state-space environments, we refer the reader to the tutorials found in the oﬃcial documen-
tation for the repository: https://pymdp-rtd.readthedocs.io/.
Customizability
pymdp oﬀers a high degree of customizability in designing bespoke active inference processes,
such that the methods of theAgent class can be called in any particular order, depending
on the application, and furthermore they can be speciﬁed with various keyword arguments
that entail choices of implementation details at lower levels.
For instance, if one wanted to model a purely ‘perceptual’ task, i.e., where the agent
has no ability to act, but is only concerned with hidden state estimation, then one could
write an active inference loop where theAgent class only uses theinfer_states() func-
tion. This oﬀers an advantage over the main function used to perform active inference in
SPM, spm_MDP_VB_X.m where customization applications are limited and in practice are im-
plemented by modifying parts of the function by hand to suit one’s needs (e.g. commenting
out certain sections or adding in bespoke computations).
Moreover, by retaining a modular structure throughout the package’s dependency hier-
archy,pymdp also aﬀords the ability to ﬂexibly compose diﬀerent low level functions. This
allows users to customize and integrate their active inference loops with desired inference
algorithms and policy selection routines. For instance, one could sub-class theAgent class
and write a customized step() function, that combines whichever components of active
inference one is interested in.
Related software packages
The DEM toolbox within SPM in MATLAB is the current gold-standard in active inference
modelling. In particular, simulating an active inference process inDEM consists of deﬁning
the generative model in terms of a ﬁxed set of matrices and vectors, and then calling the
spm_MDP_VB_X.m function to simulate a sequence of trials.pymdp, by contrast, provides a
12
user-friendly and modular development experience, with core functionality split up into dif-
ferent libraries that separately perform the computations of active inference in a standalone
fashion. Moreover, pymdp provides the user the ability to write an active inference process
at diﬀerent levels of abstraction depending on the user’s level of expertise or skill with the
package – ranging from the high levelAgent functionality, which allows the user to deﬁne
and simulate an active inference agent in just a few lines of code, all the way to specifying
a particular variational inference algorithm (e.g. marginal-message passing) for the agent to
use during state estimation. InSPM, this would require setting undocumented ﬂags or else
manually editing the routines inspm_MDP_VB_X.m to enable or disable bespoke functionality.
pymdp has extensive, organized documentation and illustrative examples. While theDEM
toolbox is also replete with interesting examples that result in beautiful visualizations of
simulated behavior and synthetic neural responses, the available usage information for each
function remains limited to doc-strings in the source code. The closest to documentation
or an instruction manual forspm_MDP_VB_X.m is the comprehensive tutorial by Smith et
al. 2021 [31], which features a series of MATLAB tutorial scripts that walk through the
diﬀerent aspects of active inference, with a focus on applications to modelling (behavioral
and neurophysiological) empirical data.
A recent related, but largely non-overlapping project is ForneyLab, which provides a
set of Julia libraries for performing approximate Bayesian inference via message passing
on Forney Factor Graphs [49]. Notably, this package has also seen several applications
in simulating active inference processes, using ForneyLab as the backend for the inference
algorithms employed by an active inference agent [27, 50, 51, 52]. While ForneyLab focuses
on including a rigorous set of message passing routines that can be used to simulate active
inference agents,pymdp is speciﬁcally designed to help users quickly build agents (regardless
of their underlying inference routines) and plug them into arbitrary environments to run
active inference in a few easy steps.
Funding Statement CH and IDC acknowledge support from the Oﬃce of Naval Research
grant(ONR,N00014-64019-1-2556), withIDCfurtheracknowledgingsupportfromtheEuro-
pean Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-
Curie grant agreement (ID: 860949), the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) under Germany’s Excellence Strategy-EXC 2117- 422037984, and the
Max Planck Society. KF is supported by funding for the Wellcome Centre for Human Neu-
roimaging (Ref: 205103/Z/16/Z) and the Canada-UK Artiﬁcial Intelligence Initiative (Ref:
ES/T01279X/1). CH, DD, and BK acknowledge the support of a grant from the John
Templeton Foundation (61780). The opinions expressed in this publication are those of the
author(s) and do not necessarily reﬂect the views of the John Templeton Foundation.
Acknowledgements The authors would like to thank Dimitrije Markovic, Arun Niranjan,
Sivan Altinakar, Mahault Albarracin, Alex Kiefer, Magnus Koudahl, Ryan Smith, Casper
Hesp, and Maxwell Ramstead for discussions and feedback that contributed to development
of pymdp. We would also like to thank Thomas Parr for pointing out a technical error
13
in an earlier version of the paper. Finally, we are grateful to the many users of pymdp
whose feedback and usage of the package have contributed to its continued improvement
and development.
References
[1] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement learning or
active inference?” In:PLoS ONE 4.7 (2009), e6421.doi: 10.1371/journal.pone.
0006421.
[2] Karl J. Friston, Spyridon Samothrakis, and Read Montague. “Active inference and
agency: Optimal control without cost functions”. In:Biological Cybernetics 106.8-9
(2012), pp. 523–541.doi: 10.1007/s00422-012-0512-8.
[3] KarlJ.Friston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzger-
ald, and Giovanni Pezzulo. “Active inference and epistemic value”. In:Cognitive Neu-
roscience 6.4 (2015), pp. 187–214.doi: 10.1080/17588928.2015.1020053.
[4] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and
Giovanni Pezzulo. “Active inference: A process theory”. In:Neural Computation 29.1
(2017), pp. 1–49.doi: 10.1162/NECO_a_00912.
[5] Thomas Parr, Rajeev Vijay Rikhye, Michael M Halassa, and Karl J Friston. “Prefrontal
computation as active inference”. In:Cerebral Cortex30.2 (2020), pp. 682–695.
[6] Emma Holmes, Thomas Parr, Timothy D Griﬃths, and Karl J Friston. “Active infer-
ence,selectiveattention,andthecocktailpartyproblem”.In: Neuroscience & Biobehav-
ioral Reviews131 (2021), pp. 1288–1304.doi: 10.1016/j.neubiorev.2021.09.038.
[7] Rick A Adams, Peter Vincent, David Benrimoh, Karl J Friston, and Thomas Parr.
“Everything is connected: Inference and attractors in delusions”. In:Schizophrenia
research (2021). doi: 10.1016/j.schres.2021.07.032.
[8] P. Read Montague, Raymond J. Dolan, Karl J. Friston, and Peter Dayan. “Compu-
tational psychiatry”. In:Trends in Cognitive Sciences16.1 (2012), pp. 72–80. doi:
10.1016/j.tics.2011.11.018.
[9] Philipp Schwartenbeck, Thomas FitzGerald, Christoph Mathys, Ray Dolan, and Karl
J. Friston. “The dopaminergic midbrain encodes the expected certainty about desired
outcomes”. In:Cerebral Cortex 25.10 (2015), pp. 3434–3445.doi: 10.1093/cercor/
bhu159.
[10] Ryan Smith, Philipp Schwartenbeck, Jennifer L. Stewart, Rayus Kuplicki, Hamed
Ekhtiari, Martin P. Paulus, and Tulsa 1000 Investigators. “Imprecise action selection
in substance use disorder: Evidence for active learning impairments when solving the
explore-exploit dilemma”. In:Drug and Alcohol Dependence215 (2020), p. 108208.doi:
10.1016/j.drugalcdep.2020.108208.
14
[11] Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki,
Sahib S. Khalsa, Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. “Greater
decision uncertainty characterizes a transdiagnostic patient sample during approach-
avoidance conﬂict: A computational modelling approach”. In:Journal of Psychiatry &
Neuroscience 46.1 (2021), E74.doi: 10.1503/jpn.200032.
[12] Manuel Baltieri and Christopher L. Buckley. “PID control as a process of active infer-
ence with linear generative models”. In:Entropy 21.3 (2019), p. 257.doi: 10.3390/
e21030257.
[13] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “On
the relationship between active inference and control as inference”. In:International
Workshop on Active Inference. Springer. 2020, pp. 3–11.doi: 10.1007/978-3-030-
64919-7_1.
[14] Mohamed Baioumy, Corrado Pezzato, Carlos Hernandez Corbato, Nick Hawes, and
Riccardo Ferrari. “Towards stochastic fault-tolerant control using precision learning
and active inference”. In:arXiv preprint arXiv:2109.05870(2021). doi: 10.1007/978-
3-030-93736-2_48. url: https://arxiv.org/abs/2109.05870.
[15] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. “Re-
inforcement learning through active inference”. In:Bridging AI and Cognitive Sci-
ence at the International Conference on Learning Representations. 2020.url: https:
//baicsworkshop.github.io/pdf/BAICS_37.pdf.
[16] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. “Scal-
ing active inference”. In: 2020 International Joint Conference on Neural Networks
(IJCNN). IEEE. 2020, pp. 1–8.doi: 10.1109/IJCNN48605.2020.9207382.
[17] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. “Active inference: De-
mystiﬁed and compared”. In:Neural Computation33.3 (2021), pp. 674–712.doi: 10.
1162/neco_a_01357.
[18] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl J. Friston. “Deep ac-
tive inference agents using Monte-Carlo methods”. In:Advances in Neural Information
Processing Systems. 2020. url: https://proceedings.neurips.cc/paper/2020/
hash/865dfbde8a344b44095495f3591f7407-Abstract.html.
[19] Beren Millidge. “Deep active inference as variational policy gradients”. In:Journal of
Mathematical Psychology96 (2020), p. 102348.doi: 10.1016/j.jmp.2020.102348.
[20] Inês Hipólito and Thomas van Es.Enactive-Dynamic Social Cognition and Active In-
ference. 2021.url: http://philsci-archive.pitt.edu/19653/.
[21] Nadine Wirkuttis and Jun Tani. “Leading or following? Dyadic robot imitative interac-
tion using the active inference framework”. In:IEEE Robotics and Automation Letters
6.3 (2021), pp. 6024–6031.doi: 10.1109/LRA.2021.3090015.
15
[22] Remi Tison and Pierre Poirier. “Communication as socially extended active infer-
ence: An ecological approach to communicative behavior”. In:Ecological Psychology
33 (2021), pp. 197–235.doi: 10.1080/10407413.2021.1965480.
[23] Ernesto C. Martínez, Jong Woo Kim, Tilman Barz, and Mariano N. Cruz Bournazou.
“Probabilistic modeling for optimization of bioreactors using reinforcement learning
with active inference”. In:Computer Aided Chemical Engineering50 (2021), pp. 419–
424. doi: 10.1016/B978-0-323-88506-5.50066-8 .
[24] Adrián Rocandio Moreno. PID control as a process of active inference applied to a
refrigeration system. 2021. url: https://projekter.aau.dk/projekter/files/
415131289/1034_PID_Control_as_Active_Inference.pdf.
[25] Stephen Fox. “Active inference: Applicability to diﬀerent types of social organization
explained through reference to industrial engineering and quality management”. In:
Entropy 23.2 (2021), p. 198.doi: 10.3390/e23020198.
[26] KaiUeltzhöﬀer.“Deepactiveinference”.In: Biological Cybernetics112.6(2018),pp.547–
573. doi: 10.1007/s00422-018-0785-7.
[27] Thijs W. van de Laar and Bert de Vries. “Simulating active inference processes by
message passing”. In:Frontiers in Robotics and AI6 (2019), p. 20.doi: 10.3389/
frobt.2019.00020.
[28] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning action-
orientedmodelsthroughactiveinference”.In: PLoS Computational Biology16.4(2020),
e1007805. doi: 10.1371/journal.pcbi.1007805.
[29] Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt.
“Learning perception and planning with deep active inference”. In:IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2020,
pp. 3952–3956.doi: 10.1109/ICASSP40776.2020.9054364.
[30] Karl J. Friston, N. Trujillo-Barreto, and Jean Daunizeau. “DEM: A variational treat-
ment of dynamic systems”. In:NeuroImage 41.3 (2008), pp. 849–885.doi: 10.1016/
j.neuroimage.2008.02.054.
[31] Ryan Smith, Karl J Friston, and Christopher J Whyte. “A step-by-step tutorial on
active inference and its application to empirical data”. In:Journal of Mathematical
Psychology 107 (2022), p. 102632.doi: 10.1016/j.jmp.2021.102632.
[32] William D. Penny, Karl J. Friston, John T. Ashburner, Stefan J. Kiebel, and Thomas
E. Nichols. Statistical parametric mapping: The analysis of functional brain images.
2007. isbn: 978-0-12-372560-8.doi: 10.1016/B978-0-12-372560-8.X5000-1 .
[33] Karl J. Friston. “The free-energy principle: A uniﬁed brain theory?” In:Nature Reviews
Neuroscience 11.2 (2010), pp. 127–138.doi: 10.1038/nrn2787.
[34] Karl J. Friston. “Life as we know it”. In:Journal of the Royal Society Interface10.86
(2013), p. 20130475.doi: 10.1098/rsif.2013.0475.
16
[35] Karl J. Friston. “A free energy principle for a particular physics”. In:arXiv (2019).
url: https://arxiv.org/abs/1906.10184.
[36] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and
Karl J. Friston. “Active inference on discrete state-spaces: A synthesis”. In:Journal of
Mathematical Psychology99 (2020), p. 102447.doi: 10.1016/j.jmp.2020.102447.
[37] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. “Planning and
acting in partially observable stochastic domains”. In:Artiﬁcial Intelligence 101.1-2
(1998), pp. 99–134.doi: 10.1016/S0004-3702(98)00023-X.
[38] Karl J. Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr.
“Sophisticated inference”. In:Neural Computation33.3 (2021), pp. 713–763.doi: 10.
1162/neco_a_01351.
[39] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
Jie Tang, and Wojciech Zaremba. “Openai gym”. In:arXiv preprint arXiv:1606.01540
(2016).
[40] Herman Von Helmholtz and JPC Southall. Treatise on physiological optics (Vol. 3).
1910. doi: 10.1037/13536-000.
[41] Richard Langton Gregory. “Perceptions as hypotheses”. In:Philosophical Transactions
of the Royal Society of London. B, Biological Sciences290.1038 (1980), pp. 181–197.
doi: 10.1098/rstb.1980.0090.
[42] Geoﬀrey E. Hinton and Terrence J. Sejnowski. “Optimal perceptual inference”. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Vol. 448. Citeseer. 1983. url: http : / / www . cs . toronto . edu / ~hinton / absps /
optimal.pdf.
[43] Peter Dayan, Geoﬀrey E. Hinton, Radford M. Neal, and Richard S. Zemel. “The
Helmholtz Machine”. In:Neural Computation7.5 (1995), pp. 889–904.doi: 10.1162/
neco.1995.7.5.889.
[44] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John
O’Doherty, and Giovanni Pezzulo. “Active inference and learning”. In:Neuroscience &
Biobehavioral Reviews68 (2016), pp. 862–879.doi: 10.1016/j.neubiorev.2016.06.
022.
[45] Philipp Schwartenbeck, Johannes Passecker, Tobias U. Hauser, Thomas FitzGerald,
Martin Kronbichler, and Karl J. Friston. “Computational mechanisms of curiosity and
goal-directed exploration”. In:Elife 8 (2019), e41703.doi: 10.7554/eLife.41703.
[46] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli
Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J.
Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew
Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre
Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi,
Christoph Gohlke, and Travis E. Oliphant. “Array programming with NumPy”. In:
17
Nature 585.7825 (Sept. 2020), pp. 357–362.doi: 10.1038/s41586-020-2649-2 . url:
https://doi.org/10.1038/s41586-020-2649-2.
[47] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson,
and Sasha Ondobaka. “Active inference, curiosity and insight”. In:Neural Computation
29.10 (2017), pp. 2633–2683.doi: 10.1162/neco_a_00999.
[48] Manuel Baltieri and Christopher L Buckley. “Generative models as parsimonious de-
scriptions of sensorimotor loops”. In:arXiv preprint arXiv:1904.12937(2019).
[49] Marco Cox, Thijs van de Laar, and Bert de Vries. “A factor graph approach to auto-
mated design of Bayesian signal processing algorithms”. In:International Journal of
Approximate Reasoning104 (Jan. 2019), pp. 185–204.issn: 0888-613X.doi: 10.1016/
j.ijar.2018.11.002 . url: http://www.sciencedirect.com/science/article/
pii/S0888613X18304298 (visited on 11/16/2018).
[50] Mees Vanderbroeck, Mohamed Baioumy, Daan van der Lans, Rens de Rooij, and Tiis
van der Werf. “Active inference for robot control: A factor graph approach”. In:Student
Undergraduate Research E-journal!5 (2019), pp. 1–5.
[51] Burak Ergul, Thijs van de Laar, Magnus Koudahl, Martin Roa-Villescas, and Bert
de Vries. “Learning Where to Park”. In:International Workshop on Active Inference.
Springer. 2020, pp. 125–132.
[52] Thijs van de Laar, Ismail Senoz, Ayça Özçelikkale, and Henk Wymeersch. “Chance-
Constrained Active Inference”. In:arXiv preprint arXiv:2102.08792(2021). doi: 10.
1162/neco_a_01427.
[53] Mortimer Mishkin, Leslie G Ungerleider, and Kathleen A. Macko. “Object vision and
spatial vision: Two cortical pathways”. In:Trends in Neurosciences6 (1983), pp. 414–
417. doi: 10.1016/0166-2236(83)90190-X.
[54] Semir Zeki, J.D. Watson, C.J. Lueck, Karl J. Friston, C. Kennard, and R.S. Frack-
owiak. “A direct demonstration of functional specialization in human visual cortex”.
In: Journal of Neuroscience11.3 (1991), pp. 641–649.doi: 10.1523/JNEUROSCI.11-
03-00641.1991.
[55] Renaud Jardri, Sandrine Duverne, Alexandra S. Litvinova, and Sophie Denève. “Exper-
imental evidence for circular inference in schizophrenia”. In:Nature Communications
8.14218 (2017), pp. 1–13.doi: 10.1038/ncomms14218.
[56] Pantelis Leptourgos, Charles-Edouard Notredame, Marion Eck, Renaud Jardri, and
Sophie Denève. “Circular inference in bistable perception”. In:Journal of Vision20.4
(2020), p. 12.doi: 10.1167/jov.20.4.12.
[57] Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. “Neuronal
message passing using Mean-ﬁeld, Bethe, and Marginal approximations”. In:Scientiﬁc
Reports 9.1 (2019), pp. 1–18.doi: 10.1038/s41598-018-38246-3.
[58] Thomas Parr, Noor Sajid, and Karl J. Friston. “Modules or mean-ﬁelds?” In:Entropy
22.5 (2020), p. 552.doi: 10.3390/e22050552.
18
[59] Matthew James Beal. Variational Algorithms for Approximate Bayesian Inference.
University of London, University College London, 2003.url: https://cse.buffalo.
edu/faculty/mbeal/thesis/.
[60] Martin J. Wainwright and Michael Irwin Jordan.Graphical models, exponential fam-
ilies, and variational inference. Vol. 1. Now Publishers Inc, 2008. doi: 10 . 1561 /
2200000001.
[61] David J.C. MacKay. “Bayesian interpolation”. In: Neural Computation 4.3 (1992),
pp. 415–447.doi: 10.1162/neco.1992.4.3.415.
[62] Will D. Penny, Klaas E. Stephan, Andrea Mechelli, and Karl J. Friston. “Modelling
functional integration: A comparison of structural equation and dynamic causal mod-
els”. In:NeuroImage 23 (2004), S264–S274.doi: 10.1016/j.neuroimage.2004.07.
041.
[63] JonathanS. Yedidia, WilliamT. Freeman, andYair Weiss. “Generalizedbeliefpropaga-
tion”. In:Advances in Neural Information Processing Systems. Vol. 13. 2000, pp. 689–
695. url: https://dl.acm.org/doi/10.5555/3008751.3008848.
[64] John Winn and Christopher M. Bishop. “Variational message passing”. In:Journal
of Machine Learning Research6.23 (2005), pp. 661–694.url: http://jmlr.org/
papers/v6/winn05a.html.
[65] Thomas Parr and Karl J. Friston. “Generalised free energy and active inference”. In:
Biological Cybernetics113.5(2019), pp. 495–513.doi:10.1007/s00422-019-00805-w.
[66] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. “Whence the ex-
pected free energy?” In:Neural Computation33.2 (2021), pp. 447–482.doi: 10.1162/
neco_a_01354.
[67] Beren Millidge, Alexander Tschantz, AnilSeth, and Christopher Buckley. “Understand-
ing the origin of information-seeking exploration in probabilistic objectives for control”.
In: arXiv (2021). url: https://arxiv.org/abs/2103.06859.
[68] Lancelot Da Costa. Personal communication. 2021.
19
Appendix A: Factorized Generative Models
In this appendix, we explain observation and state factorization by using an in-depth exam-
ple. Let’s imagine a scenario where you have to infer two simultaneous states of the world,
given some sensory data. The two facts you need to estimate are 1) what time of day it is
(morning, midday, or evening), and 2) whether it rained recently (yes or no). We can rep-
resent this in a generative model as an environment characterized by two discrete random
variables or hidden state factors. The ﬁrst variable or factor we can call thetime-of-day
state, which has three levels: Morning, Midday, or Evening; the second factor we can call
the did-it-rain state, which has two levels: Rained and Did-not-rain. We use the following
notation to denote these hidden state factors and their respective levels:
s = {stime-of-day,sdid-it-rain}
stime-of-day ∈{Mo,Mid,Eve } sdid-it-rain ∈{Rain,NoRain}
where we assign each state level a cardinal index, e.g. Mo = 0 , Mid = 1 , Eve = 2 ,
and Rain= 0,NoRain = 1. Let’s now augment our simple hidden state representation with
observations, which a hypothetical agent would use to infer both the time of day and whether
it rained recently, i.e. to obtain a posterior distribution overs: P(s |o). Our observations
will also be factorized, into two diﬀerentmodalities or information channels. Each of these
modalities is also a discrete-valued random variable. Let’s imagine our two modalities are:
1) the ambient light level (dark, cloudy, or sunny) and 2) humidity (dry or humid). We
denote the observations as follows:
o = {olight,ohum}
olight ∈{Drk,Cld,Sun } ohumid ∈{Dry,Hmd }
Having speciﬁed a factorized representation of both states and observations, we can
now consider how observations lend evidence for or against diﬀerent states-of-aﬀairs in the
environment. For example, if you notice it’s dark outside (i.e.,olight = Drk), that provides
evidence to suggest that it’s night time, rather than being morning or midday. At the
same time, you might also notice that the air is humid through your humidity modality, i.e.
ohum = Hmd. We can imagine that the humidity observation provides no evidence for the
time of day it is, but it may suggest that it rained recently.
These probabilistic relationships between the observation modalities and the hidden state
factors, which are used to perform inference, are encoded in the observation likelihood
P(om|s), represented inpymdp as a modality-speciﬁc sub-arrayA[m]. Recall that each like-
lihood array encodes the conditional dependencies between each setting of the hidden states
s and the observations within modalitym, i.e. A[m] = P(om|s1,s2,...,s F). Therefore the
dimensionality of a givenA[m] array isOm×S1 ×...×SF, whereOm is the dimensionality of
modality mand Sf is the dimensionality of factorf. Following our simple example of infer-
ring the time of day and whether it just rained, the likelihood for the ﬁrst modality would be
a 3D array that represents the likelihood distributionP(olight|stime-of-day,sdid-it-rain). Speciﬁ-
cally, entryA[i,j,k] encodes the probability of observingolight = level i, givenstime-of-day =
20
level j and sdid-it-rain = level k. The second observation modality accordingly has its own
likelihood NDarray, encoding the likelihood distributionP(ohum|stime-of-day,sdid-it-rain).
These higher-dimensional likelihood arrays enable complex, conjunctive relationships to
be encoded in the generative model. For instance, we might imagine that theolight obser-
vation depends on both the time of dayand whether it just rained. For instance, all else
being equal we might expect the ambient lighting to be sunny, if the time of day is midday.
However, if it just rained then the probability of the ambient lighting being dark or cloudy
might be higher, even if the time of day is midday. This statement already requires a nonlin-
ear, conjunctive relationship betweenstime-of-day and sdid-it-rain. For example the probability
distribution over the 3 levels ofolight, givenstime-of-day is Mo and sdid-it-rain is Rain would be
encoded by the corresponding likelihood:P(olight|stime-of-day = Mo,sdid-it-rain = Rain). This
would then be easily encoded in the corresponding ‘slice’ of the high-dimensionalA[light]
array: A[light][:,Mo,Rain], wherelight is the index of theA array that encodes theolight
likelihood, andMo and Rain are the cardinal indices for those factor-speciﬁc state levels.
In the same way that the observation likelihood for each modalitymis represented using
a single likelihood NDarrayA[m], the transition likelihood for each hidden state factorf
is represented using a single likelihood NDarrayB[f], where the size of thefth transition
array is of sizeSf ×Sf ×Uf. So analogously to having a collection ofA[m] arrays, one for
each observation modality, we also have a collection ofB[f] arrays, one for each state factor.
Two important things to note are that: 1) constructing B matrices with thisSf ×Sf ×Uf
shape assumes that hidden state factors cannot inﬂuence each other dynamically, i.e. the
next state within a factorsf
t only depends on the past statefor that factorsf
t−1 and control
state uf
t−1, and 2) that control states are factorized just like hidden states, such that for each
hidden state factor there is a correspondingcontrol factorcf whose dimensionality is equal
to the number of control state levels or actions that can be taken upon hidden state factor
f. It is of course allowable to haveuncontrollable hidden state factors - in which case we
simply set the dimensionality of the corresponding control factor to 1, i.e.Uf = 1. We often
refer to these as ‘trivial’ control factors, since they don’t actually encode any kind of control.
The factorized representation described above oﬀers several advantages. First of all, if a
largehiddenstatespacecanbefactorizedintoacollectionofone-dimensionalrepresentations,
then the memory cost of storing the relevant probability distributions over hidden states can
be greatly ﬁnessed (e.g.P(st|st−1), P(s), etc.). For example, if you can represent the identity
ofanobjectanditslocationindependently, withouthavingtoenumerateallthecombinations
of both its identity and its location together, then the amount of memory used to store the
factorized representation will be linear in the dimensionality of the two hidden state factors,
whereas the ‘enumerated’ representation will be polynomial. For example, if location is a
1000-dimensional vector, and identity is a 1000-dimensional vector, then storing two 1000-
dimensional vectors is considerably cheaper than storing a single1000 ×1000-dimensional
vector.
Another advantage is the degree of interpretability and model transparency that fac-
torized representations aﬀord; a particular factorization is ideally explicitly designed, such
that hidden state factors are directly mapped to intuitive features of the environment whose
21
relationships are easy to reason about. If a multi-factor model of, for example, 3 hidden
state factors (e.g. the location, identity, and time of some event) were fully enumerated
into a single, 1-dimensional hidden state, then each level of the single hidden state would
correspond to a unique combination of “what”, “where” and “when”. When it comes to
encoding probabilistic relationships in the generative model (e.g. the observation and tran-
sition models), it becomes harder to visualize and reason about the relationships between
such high-dimensional state combinations. Thus factorization also proves a useful tool when
designing generative models based on prior domain or task knowledge.
Interestingly, when one optimizes the factorial structure of a generative model, using
marginal likelihood or variational free energy, the best factorisation maximizes marginal
likelihood (a.k.a., model evidence) by minimizing complexity: namely, the degrees of freedom
used to provide an accurate account of observations. This is an important aspect of active
inference; namely, thattoprovidethebestaccountofobservations–thatprecludesoverﬁtting
and ensures generalization – the (mean-ﬁeld) factorisation should be as simple as possible
but no simpler.
Finally, inference also may take advantage of the factorized structure of the generative
model. In doing so, inference is not only more memory-eﬃcient, but the belief-updating
algorithms have features like functional specialization [53, 54] and local message-passing
that have been linked to features of computation in the brain [55, 56]. It has been argued
that this aﬀords factorized generative models a higher degree of biological plausibility [57,
58].
Appendix B: Modules and Theory
inference.py
In this section we provide an overview of theinference module of pymdp and then brieﬂy
rehearse the mathematics of variational inference, both generally and as it is used inpymdp.
The inference.py ﬁle contains functions for performing variational inference about hid-
den states in discrete categorical generative models. Functions within this module are called
by theinfer_states() method of Agent. The core functions of this module are:
• update_posterior_states(obs, A, prior=None, **kwargs): This function com-
putes the variational (categorical) posterior over hidden states at the current timestep
Q(st). This function by default calls the standard or ‘vanilla’ inference algorithm of-
fered bypymdp, which estimates the marginal posteriors for each hidden state factor at
the current timestepQ(si
t) using ﬁxed-point iteration. This function can be generically
applied as the inference step in any discrete POMDP model, as all it requires are some
observationsobs, a likelihood arrayA and optionally a prior over hidden statesprior,
which will have the same structure as the resulting posterior. The additional argu-
ments **kwargs contain parameters that will be passed to therun_vanilla_fpi()
function inalgos/fpi.py (see the documentation for more details).
22
• update_posterior_states_full(A,B,prev_obs,policies, prev_actions=None,
prior=None, policy_sep_prior=True, **kwargs): Thisfunctioncomputesthevari-
ational (categorical) posterior over hidden states under all policies:Q(˜s|π). The nota-
tion ˜s represents a trajectory of hidden states over time. This is inspired by the ‘full
construct’ active inference process as implemented inspm_MDP_VB_X.m in DEM, where
the full posterior over both hidden states and policies is computed:Q(s,π). This func-
tion itself calls therun_mmp.py function within thealgos library, which estimates the
marginal posteriors for hidden state factori, at time pointτ under policyj: Q(si
τ |πj),
using marginal message passing [57] (for more details, see thealgos.py summary be-
low). Thisfunctioncalls run_mmp.py onceperpolicy, estimatingthepolicy-conditioned
posterior over all timepoints of the horizon and for all hidden state factors. The num-
ber of timepoints over which inference occurs is not necessarily identical to the total
time horizon of the simulation: rather this time horizon is a function of the number
of previous observations (len(prev_obs)) and the temporal depth of the policy under
consideration (len(policies[j])). Thus the hidden state beliefs indexed byτ refer
to a ﬁnite time horizon that is relative to the current timestept, where this horizon
[t−H0,t + H1] has a ‘lookback’ lengthH0 and a ‘planning horizon’H1. As argu-
ments, this function requires the observation (A) and transition (B) likelihoods of a
generative model, a list of previous (including current) observations (prev_obs), a list
of policies (policies), an optional list of actions taken up until the current timepoint
(prev_actions) an optional prior over hidden states at the start of the time horizon
(prior), and a keyword argumentpolicy_sep_prior, which determines whether the
prior is itself conditioned on policiesP(s0|π) vs. unconditioned on policiesP(s0). The
additional **kwargs contain parameters that will be passed to therun_mmp() function
in algos/mmp.py. For more details onrun_mmp(), see the documentation.
Speciﬁc usage examples–in addition to descriptions of other specialized functions in
inference.py–are more extensively covered in the oﬃcial documentation.
Bayesian and Variational Inference
A central task in statistics is to perform inference, which can be mathematically represented
as computing posterior distributions of one variable given another from a joint distribution.
For example, suppose we are given some observationo, and we then want to infer the likely
state sunderlying that observation. Critical to achieving this task is the possession of a gen-
erative model, or joint distribution,P(o,s) that tells us how observations and hidden states
are related. We can formulate the problem of inferrings as ﬁnding a posterior distribution
over the states, given the observation:P(s|o). We can compute this posterior distribution
using our generative model and Bayes Rule:
P(s|o) = P(o,s)
P(o)
The generative modelP(o,s) is often factorized into a likelihoodP(o|s) and a priorP(s),
while P(o) is known as themarginal likelihoodor model evidenceand can be computed by
23
solving the integralP(o) =
∫
P(o,s)ds. This expresses the idea that the marginal probability
of observationsoin P(o) is the sum (or integral) over all the ways that thatodepends ons,
for all possible settings ofs.
While Bayes rule provides a simple formula for relating the posterior distribution to the
generative model, explicitly computing this distribution can often be diﬃcult in practice due
to the computational expense involved in performing the integral over all states necessary
to compute the marginal likelihood P(o). However, a number of approximate Bayesian
inference methods have been developed which circumvent this computational diﬃculty at
the expense of only returning approximately correct posterior distributions.
Variational inference [59, 60] is a widely used and well understood approach for per-
forming approximate Bayesian inference. The central idea in variational inference is that
instead of directly computing the posterior, we instead optimize the parametersθ of an ar-
bitrary distributionQ(s; θ) so as to minimize the divergence between this distribution and
the true posterior. This arbitrary distribution is often named theapproximate posterior,
because in the course of minimizing the divergence, the arbitrary distribution becomes an
approximation to the true posterior, i.e.Q(s; θ) ≈P(s|o). In this way, variational inference
converts a challenging inference problem (involving computing intractable integrals) into a
relatively straightforward optimization problem, for which many powerful algorithms exist
in the optimization literature.
Ideally, variational inference would directly minimize the Kullback-Leibler divergence
between the approximate and true posteriors:
θ∗= argmin
θ
DKL[Q(s; θ)||P(s|o)]
In the present form„ this objective is also intractable since it depends on the true posterior
P(s|o), whose approximation is our goal. However, by supplementing the KL divergence with
the log marginal likelihood, which does not depend uponQ(s; θ), we can convert the above
KL divergence into anupper bound on the log marginal likelihood, calledvariational free
energy (VFE) F. Crucially, this can be rearranged into a computable form:
θ∗= argmin
θ
F
F= DKL[Q(s; θ) ∥P(s|o)] −ln P(o)
= EQ[ln Q(s; θ) −ln P(s,o)]
Thus, by minimizingF, we minimize the divergence between the approximate and true
posterior, thus forcing the approximate posterior to more closely resemble the true one.
Moreover, if this optimization ﬁnds the exact solution, such thatKL[Q(s; θ)||P(s|o)] = 0
then value ofF= −ln P(o) provides the marginal likelihood (P(o) ∝e−F) which can then be
used for model selection and structure learning. More generally,Fis known as an evidence
bound, because the KL divergence can never be less zero [61, 62].
24
Variational Inference inpymdp
Active inference agents inpymdp perform inference over both hidden statess and policies
π in a POMDP generative model. In this appendix, we only consider inference over states,
while inference over policies is treated in the following sectioncontrol.py.
Recall the POMDP generative model is a joint distribution over observations, states,
policies, and parameters. For the purposes of hidden state inference, we will condition the
whole generative model on some ﬁxed policyπ, so that we can re-write it as follows:
P(o1:T,s1:T,φ |π) = P(φ)P(s1)
T∏
τ=2
P(sτ|sτ−1,π,φ )
T∏
τ=1
P(oτ|sτ,φ) (2)
Foranactiveinferenceagentequippedwiththisgenerativemodel, instantaneousinference
consists in optimizing an approximation to the posterior over the current hidden state:
P(sτ|s\τ,o[1:τ]) given the past and future statess\τ = {s[1:τ−1],s[τ+1:T]}, and the observations
collectedupuntilthecurrenttimepoint o[1:τ]. Mathematically, thisinferencecanbedescribed
as minimizing the following free energy over trajectories with respect to the variational
parameters θ:
θ∗= argmin
θ
F1:T
F1:T = EQ[ln Q(s1:T; θ) −ln P(o1:T,s1:T; φ|π)]
Thus the goal of hidden state inference is the optimization of variational parametersθ
which parameterise the approximate posterior over hidden states:Q(s1:T; θ). In the case of
our discrete POMDP generative model, the variational parametersθare the suﬃcient statis-
tics of categorical distributions. Fortunately, these parameters are easy to interpret, since
they are identical to the probabilities of sampling each outcome level in the distribution’s
support: i.e. Q(s; θ) = Cat(θ). For example, the variational parameters of some categorical
distribution Q(s) =
[
0.1 0 .4 0 .5
]
would simply beθ=
[
0.1 0 .4 0 .5
]
. In the equations to
follow, we therefore exclude the variational parametersθ when writing the variational pos-
terior, referring to it hereafter as simplyQ(s). Below we describe the variational inference
methods currently oﬀered bypymdp as of this document’s writing (November 2021).
The run_vanilla_FPI function (within algos/fpi.py) implements an inference algo-
rithm known asﬁxed-point iterationto optimize the posterior over hidden statesQ(sτ) at a
given timestepτ. Central to this algorithm is the assumption of a factorized structure to the
variational posterior, such that the posterior at timeτ = i is independent of the posterior
at any other timestepτ = j, where j ̸= i. In addition to this temporal factorization, we
further assume the posterior at a given timestepQ(sτ) is factorized across diﬀerent hidden
state factors: i.e.Q(sf
τ) is independent ofQ(sf′
τ ) (see the section Factorized representations
for more on multi-factor hidden states). This factorization is also known as amean-ﬁeld
25
approximation in the statistics and physics literatures and can be expressed as follows:
Q(s[1:T]) =
T∏
τ=1
Q(sτ) Q(sτ) =
F∏
f=1
Q(sf
τ)
Given this factorization, the full free energy over trajectories now also factorizes into a
sum of free energies across time, which can be minimized independently of each other. Thus,
for a given timeτ, we can write the time-dependent free energy3:
Fτ = EQ[ln Q(sτ) −ln P(oτ|sτ)P(sτ|sτ−1,uτ−1)] (3)
where we now use the bold notations and o to express potentially multi-factor (or -modal)
hidden states (or observations) in the generative model, in the same way that the variational
posterior is factorized. Inference proceeds by optimizing Q(sτ) in order to minimize the
timestep-speciﬁc free energyFτ.
Importantly, we can solve for the variational posterior analytically for a given timestepτ
and factorf by setting the derivative of the free energyFτ to 0 and solving forQ(sf
τ). We
express this partial derivative as∂Fτ
∂qf , and can express it as follows:
∂Fτ
∂qf = ∂
∂qf
[∑
Q(sτ) (lnQ(sτ) −ln P(oτ,sτ))
]
= 0
= ln Q(sf
τ) + 1 −Eqi\f
[
ln P(oτ|sτ) −ln
(
EP(sτ−1,uτ−1)[P(sτ|sτ−1,uτ−1)]
)]
= 0
=⇒ ln Q(sf
τ) = Eqi\f [ln P(oτ|sτ)] + ln
(
EP(sf
τ−1,uf
τ−1)[P(sf
τ|sf
τ−1,uf
τ−1)]
)
−1
=⇒ Q∗(sf
τ) = σ
(
Eqi\f [ln P(oτ|sτ)] + ln
(
EP(sf
τ−1,uf
τ−1)[P(sf
τ|sf
τ−1,uf
τ−1)]
))
(4)
where the expectationEqi\f denotes an expectation with respect to all posterior marginals
Q(si
τ) besidesthemarginal Q(sf
τ) currentlybeingoptimized, and σ(x) = ex
∑
xex isanormalized
exponential or softmax function. The update equation for each marginal posterior oﬀers an
intuitive Bayesian interpretation, where the belief about the current state is the product of
an observation likelihood termP(oτ|sτ) and a ‘prior’ term,EP(sτ−1,uτ−1)[P(sτ|sτ−1,uτ−1)],
where the prior is dynamically determined by the previous state, previous action, and the
transition likelihood. Note that in practice we set the prior at any given timestep equal
to the posterior optimized at the previous timestep, i.e.P(sτ−1,uτ−1) ≡Q∗(sτ−1,uτ−1) =
Q∗(sτ−1)Q∗(uτ−1), whereQ∗(uτ−1) is a Dirac delta function over the action actually taken
(the agent has perfect knowledge of the action it just took). This means that the prior
term in the last line of Equation (4) can be rewritten asEP(sf
τ−1,uf
τ−1)[P(sf
τ|sf
τ−1,uf
τ−1)] =
EQ(sf
τ−1,uf
τ−1)[P(sf
τ|sf
τ−1,uf
τ−1)]. In the run_vanilla_fpi.py function of pymdp, the ﬁxed
pointequationissolvediterativelyforeachmarginalposterior Q(sf
τ), usingthelatestsolution
for the other marginalsQ(si\f
τ ) to compute the expected log-likelihood term for the marginal
3For the remainder of the section we remove the hyperparametersφ from the generative model and
approximate posterior, but inference over these are treated in the section onlearning.py
26
f currently being updated: Eqi\f [ln P(oτ|sτ)]. Note that the priorP(sf
τ|sf
τ−1,uf
τ−1)P(sf
τ−1)
only depends on the marginalf currently being updated, because inpymdp the transition
likelihoods are assumed to be independent across hidden states, i.e. hidden states from
factor i do not determine the dynamics of hidden states of another factorj (see Appendix
A: Factorized Generative Models for details). Given enough iterations,4 the ﬁxed point
equations converge to a unique solution of the variational posteriorQ∗(sτ).
In pymdp, the approximate posterior Q(s) is represented as a collection of 1-DNumPy
arrays (e.g.qs), where individual elements of the collection (e.g.qs[f]) store the marginal
posterior for a particular hidden state factor. The likelihood distributions are represented by
A and B arrays. If we take the limiting case of a generative model and variational posterior
with a single hidden state factor, the update equation for the variational posterior at a given
timestep using ﬁxed point iteration reduces to a single line ofNumPy code:
qs_current = softmax(np.log(A[o,:])+ np.log(B[:,:,u_last].dot(qs_last)))
where utility functions likesoftmax are available from theutils.py and maths.py modules
of pymdp. In the default initialization of theAgent class, theinfer_states() method will
call theupdate_posterior_states() function of theinference module; this in turns calls
upon ﬁxed point iteration (viarun_vanilla_fpi()) to update the variational posterior over
hidden states. Therefore all that is required for inference at a given timestep is to provide
some observation obs to infer_states(). The prior over hidden states is automatically
updated within the Agent class, where prior will either A) equal the initial belief about
hidden states (theD vector) in the case thatτ = 1 ; or B) the dot product of the transi-
tion likelihood conditioned on the last actionB[:,:,u_last], and the posterior at the last
timestep qs_last in the case thatτ >1: prior=B[:,:,u_last].dot(qs_last).
Another, morecomplexinferencealgorithmknownas marginal message passing(MMP)is
also implemented in therun_mmp() function, also found within thealgos module. Marginal
message passing makes weaker assumptions about the factorization of the variational pos-
terior, and incorporates the computational advantages of two well-known message passing
algorithms: belief propagation [63] and variational message passing [64]. In practice, using
marginal-message passing instead of standard ﬁxed-point iteration enables more accurate
inference due to its less restrictive assumptions as to the form of the variational posterior,
at the expense of additional computational cost. For the purpose of brevity and since this
algorithm has been discussed in detail elsewhere (speciﬁcally, see Appendix C of [4] as well
a comprehensive treatment in [57]), we will not describe the mathematics behind marginal
message passing here. It is worth noting that for beginning users, a standard active infer-
ence simulation will not require marginal-message passing to achieve the desired behavior;
state inference achieved with instantaneous ﬁxed-point iteration often suﬃces for practition-
ers interested in simulating a target behavior. However, cognitive neuroscientists are often
4The default number of iterations forrun_vanilla_fpi() function isnum_iter=10 but in many genera-
tive models with precise likelihood arrays (i.e. low entropy rows/columns), convergence is often achieved in
many fewer iterations. This is further controlled by a tolerance parameter that tracks the change in the free
energy across iterations.
27
interested in modelling neuronal responses based on estimated inferential dynamics. In this
case, more sophisticated schemes likerun_mmp() may be required, where actual dynamics of
belief updating might be used as a forward model of hypothesized electrophysiological pro-
cesses (e.g. local ﬁeld potentials or spiking activity). Finally, we also mention that in order
to achieve identical behavior to active inference agents simulated usingspm_MDP_VB_X.m, it
is necessary to userun_mmp().
In practice, a desired inference algorithm can be speciﬁed by passing the name of the algo-
rithm into theAgent() constructor, e.g.my_agent = Agent(...,inference_algo=‘MMP’).
For more detailed instructions on how to initialize anAgent with diﬀerent customization op-
tions, please see the documentation.
control.py
The core functions implementing policy inference and action selection (i.e. control) inpymdp
can be found in the control.py ﬁle. As with inference, these functions are called by
methods of Agent like infer_policies() and sample_action(), but can also be directly
imported from the control library and used for custom applications. Below we brieﬂy
summarize the core functions of thecontrol module:
• update_posterior_policies(qs, A, B, C, policies, use_utility=True,
use_states_info_gain=True, use_param_info_gain=False, pA=None, pB=None,
E=None, gamma=16.0): This function computes the posterior over policiesQ(π) using
an initial posterior belief about hidden states at the current timestepqs. Speciﬁcally, it
computes the expected free energy of each policy (discussed further in the next section)
by summing the expected free energies over a future path in the case of multi-timestep
or ‘temporally-deep’ policies. This function ﬁrst loops over all policies, computes the
expected states and observations under each policy, and then sums the expected free
energies calculated from those predicted future states and observations. The expected
free energy for each policy is combined with its prior probability under the generative
model P(π) (in pymdp represented by theE vector) and softmaxed to determine the
posterior over policies Q(π) (in pymdp represented by q_pi). Optional Boolean pa-
rameters likeuse_utility and use_states_info_gain can be turned on and oﬀ to
selectively enable (disable) computation of components of the expected free energy (see
the section on The Expected Free Energy for information on these diﬀerent expected
free energy terms).
• update_posterior_policies_full(qs_seq_pi, A,B,C, policies,use_utility=
True, use_states_info_gain=True, use_param_info_gain=False, prior=None,
pA=None, pB=None, F=None, E=None, gamma=16.0) : This function computes the
posterior over policies Q(π) using a posterior belief over hidden states over multi-
ple timesteps under all policies. This version diﬀers from the standard function,
update_posterior_policies(), in that the expected hidden states over future time-
points, under diﬀerent policies, have already been computed in the input, the posterior
28
beliefs qs_seq_pi. This function for policy inference should thus be used in tandem
with the ‘advanced’ inference schemes (like marginal message passing) where posterior
beliefs over multiple timesteps, under all policies, are computed during the inference
step. As a consequence, this function only computes the expectedobservations under
each policy for all future timesteps, and then uses the expected states (already part
of the inputs) and expected observations under all policies to calculate the expected
free energy for each policy. This is integrated with prior belief about policiesE and
the variational free energy of policiesF (see the section on Policy Inference for more
information on the variational free energy of policies) to ﬁnally determine the poste-
rior over policiesQ(π), often represented inpymdp as q_pi. This function’s remaining
arguments (e.g. use_utility) are identical to how they are used in the standard
update_posterior_policies() function.
• get_expected_states(qs, B, policy): This function computes a posterior distri-
bution over future states given a current state distribution (qs), a transition model
(B) and a policy (policy). Speciﬁcally, this function projects the current beliefs about
hidden states forward in time by iteratively taking the inner product ofqs with the
action-conditioned B matrix, where the actions are those entailed by thepolicy.
• get_expected_obs(qs_pi, A): This function computes the observations expected un-
der a (policy-conditioned) hidden state distributionqs_pi. In the case of a sequence
of hidden states over time,qs_pi will be a list of hidden states distributions with one
element per timestep e.g. qs_pi[t]. This function only requires an expected state
distribution qs_pi and an observation modelA.
• calc_expected_utility(qo_pi, C): This function computes the extrinsic value or
utility part of the expected free energy using the prior preferences or ‘goal distribution’
encoded by theC vector. The C is encoded in terms of relative log probabilities and
thus need not be a proper probability distribution.
• calc_states_info_gain(A, qs_pi): This function computes intrinsic value or infor-
mation gain part of the expected free energy, in particular the information gain or
epistemic value about hidden statess.
• calc_pA_info_gain(pA, qo_pi, qs_pi): This function computes the information
gain about the Dirichlet prior parameters over the observation model (A) , also known
as the ‘novelty’ term of the expected free energy [47]. It requires a Dirichlet prior
over the observation model pA, an expected observation distribution qo_pi and an
expected state distributionqs_pi. It is recommended to include this information gain
term in the expected free energy calculation, when also simultaneously performingA
array learning (i.e. inference over Dirichlet hyperparameters), since it leads to the
agent exploring regions which lead to the largest updates of the parameters ofA .
• calc_pB_info_gain(pB, qs_pi, qs_prev, policy): This function computes the in-
formation gain about the Dirichlet prior parameters over the transition model (B), also
29
known as the ‘novelty’ term of the expected free energy. It requires a Dirichlet prior
over the transition modelpB, an expected state distribution under a policyqs_pi, an
initial state distributionqs_prev, and a policypolicy. It is recommended to include
this information gain term in the expected free energy calculation, when also simulta-
neously performingB array learning (i.e. inference over Dirichlet hyperparameters).
• construct_policies(num_states, num_controls=None, policy_len=1,
control_fac_idx=None): This is a utility function which builds an array of policies
by combinatorially enumerating them from a set of actions and a time horizon. It can
be used to construct a full set of policies based on the time horizon and action space
of the environment, if the policy set is not explicitly stated by the user.
• sample_action(q_pi, policies, num_controls,
action_selection="deterministic", alpha=16.0): This function samples an ac-
tion, given the posterior distribution over policies and a desired sampling scheme. In
particular, this function computes the posterior over control statesu by marginal-
ising the posterior over policies with respect to each control state, i.e. Q(ut) =∑
π P(ut |π)Q(π), whereP(ut |π) is the mapping between policies and control states.
To obtain an action, the most probable action is either A) selected deterministically
(action_selection="deterministic") as the most probable control state or B) an
action is sampled from the control posterior (action_selection="stochastic"), us-
ing a Boltzmann distribution with inverse temperature given byalpha.
Control in Active Inference
Policy inference consists in computing the ‘goodness’ or ‘quality’ of each policy, given the
ability to compute the expected consequences of each policy and the agent’s goals. In active
inference this is done by using a quasi-utility function known in the literature as theExpected
Free Energy(EFE) (often denoted G). Under active inference, agents are equipped with
a particular prior over policiesP(π) that assumes policies are inversely proportional to the
free energy expected under their pursuit, i.e.:
P(π) = σ(−G) (5)
Equipped with this policy prior in the generative model, active inference agents perform
policy inference by optimizingQ(π), the variational posterior over policies. As we shall see in
the section Policy Inference, computingQ(π) entails computing the expected free energy of
each policy (the contribution from the prior) as well as the variational free energy of policies
(analogous to the ‘evidence’ for each policy).
The Expected Free Energy The expected free energy is the crucial component that
determines the behavior of active inference agents. The EFE is designed to be similar to the
VFE of standard variational inference but with two major modiﬁcations to enable its use as
30
an objective which, when minimized, will perform goal seeking behavior rather than simply
inference. Firstly, since the EFE ranks future performance, where future observations are
not known, it contains anexpectation over future observations. Secondly, as there needs to
be a way to integrate the notion of goals or rewards into the inference procedure, the EFE
alters the generative model of the agent to be ‘biased’ in such a way that it predicts the agent
reaches rewarding ora priori preferred states [65]. Thus, performing inference to maximize
the likelihood of visiting these rewarding states naturally leads to policies that help the agent
achieve its goals. Moreover, an additional beneﬁt is that minimizing the EFE also entails an
exploratory, inherently uncertainty-reducing component to behavior. This endows behavior
with an additional ‘epistemic drive’ which aids in computing the optimal long-term policies
[3]. For in-depth discussion of the nature of the EFE and the exploratory drive it induces
please see [3, 4, 66, 67].
The expected free energy is a function of observations, states, and policies, and is deﬁned
mathematically as:
G(o1:T,s1:T,π) = EQ[ln Q(s1:T,π) −ln ˜P(o1:T,s1:T,π)] (6)
where ˜P represents a generative model ‘biased’ towards the preferences of the agent. We
can write this predictive generative model at a single timestep, under a given policy, as
˜P(oτ,sτ|π) = P(sτ|oτ,π) ˜P(oτ), where ˜P(oτ) represents a ‘predictive prior’ over observations,
represented inpymdp with theC array. Given the factorization of the approximate posterior
Q(s,π) over time, the EFE for a single policy and timestep can also be deﬁned as follows:
Gτ(π) = EQ(oτ,sτ|π)[ln Q(sτ|π) −ln ˜P(oτ,sτ|π)]
= EQ(oτ,sτ|π)

ln Q(sτ|π) −ln ˜P(oτ,sτ|π) + lnQ(sτ|oτ,π) −ln Q(sτ|oτ,π)  
=0


= EQ(oτ,sτ|π)
[
Q(sτ|π) −Q(sτ|oτ,π) −ln ˜P(oτ)
]
+ EQ(oτ|π) [DKL[Q(sτ|oτ) ∥P(sτ|oτ,π)]]  
Expected approximation error≥0
≥−EQ(oτ|π) [DKL[Q(sτ|oτ,π) ∥Q(sτ|π)]]  
Epistemic Value
−EQ(oτ|π)[ln ˜P(oτ)]  
Utility
(7)
where the ﬁrst term, the epistemic value [3], encourages the pursuit of policies expected to
yield high information gain about hidden states, expressed here as the divergence between
the states predicted under a policy, with and without conditioning on observations. The
second term represents the degree to which expected outcomes under a policy will align with
prior preferences over observations. Since the prior over policies isinversely proportional
to the expected free energy, policies will thus be more likely if they visit states that resolve
uncertainty (maximize epistemic value) and satisfy prior preferences (maximize utility). The
epistemic value terms give active inference agents a degree of superior exploration capacity
compared to standard reinforcement learning agents. Inpymdp, the EFE is computed using
exactly this decomposition into epistemic value and utility, where the expected approxima-
tion error (penultimate line of Equation (7)) is implicitly assumed to be 0, so the bound
31
becomes equality. The utility term is computed by the functioncalc_expected_utility()
while the epistemic value term (also known as the information gain) is computed by the
function calc_states_info_gain(). Both of these functions are found withincontrol.py.
The computation of the utility term is particularly straightforward for categorical distribu-
tions, since it reduces to the dot product of the expected observations under a policyQ(oτ|π)
with the log of the prior preferences or ‘goal vector’˜P(oτ), i.e. theC array.
Parameter Information Gain In the case where the agent also maintains a variational
posterior over parametersQ(φ), the timestep- and policy-dependent EFE has an augmented
form, since it needs to account for the expected information gain over both hidden states
and parameters [47]:
Gτ(π) = EQ(oτ,sτ|π)[ln Q(sτ,φ|π) −ln ˜P(oτ,sτ,φ|π)]
≈EQ(oτ,sτ|π)[Q(sτ|π) −Q(sτ,oτ|π) + Q(φ|π) −Q(φ,oτ|π) −ln ˜P(oτ)]
= −EQ(oτ|π) [DKL[Q(sτ|oτ,π) ∥Q(sτ|π)]]  
(State) Epistemic Value
−EQ(oτ|π) [DKL[Q(φ|oτ,π) ∥Q(φ|π)]]  
(Parameter) Epistemic Value
−EQ(oτ|π)[ln ˜P(oτ)]  
Utility
(8)
So now the EFE is supplemented with an additional epistemic value, the so called
‘parameter’ epistemic value or ‘parameter information gain’. This additional term arises
when the approximate posterior includes variational beliefs about model hyperparameters:
Q(sτ,φ) = Q(sτ)Q(φ). The optimization of the posterior over model parametersQ(φ) is
handled in the next section on thelearning.py module. This presence of this term in the
expected free energy mediates what’s also been referred to as ‘active learning’ or ‘model
exploration’, i.e. the drive to resolve uncertainty about the parameters of one’s generative
model [45].
In the discrete state space case implemented inpymdp, this parameter epistemic value
is computed with respect to the Dirichlet parameters (conjugate priors over categorical dis-
tributions) that parameterise the prior and approximate posterior over the likelihoods and
priors over the generative model, i.e. theA, B, C and D arrays. This is implemented as of the
time of writing (December 2021) for information gain about the parameters of theA array
and B array, parameterised respectively by the Dirichlet conjugate priors pA andpB. The
relevant functions for computing these information gains arecalc_pA_info_gain(). and
calc_pB_info_gain().
Policy Inference Given the deﬁnition of the expected free energy in Equations (6) and
(7), we now are equipped to describe posterior inference over policies, i.e., how to obtain
Q(π).
We begin by expanding the variational free energyFas deﬁned in Equation (2), dropping
parameters φ for simplicity:
32
F1:T = EQ(s1:T,π) [lnQ(s1:T,π) −ln P(o1:T,s1:T,π)]
= EQ(s1:T,π)[ln Q(π) +
T∑
τ=1
ln Q(sτ|π) −ln P(π) −ln P(o1:T,s1:T|π)]
= DKL[Q(π) ∥P(π)] + EQ(π) [F(π)] (9)
where the variational free energy of a particular policyF(π) is deﬁned as follows:
F(π) = −EQ(s1:T|π)[ln P(o1:T,s1:T|π) −H[Q(s1:T|π)] (10)
The optimal posterior that minimizes the full variational free energyFis found by taking
the derivative ofFwith respect toQ(π) and setting this gradient to0, yielding the following
free-energy-minimizing solution forQ(π):
Q∗(π) = argmin
Q(π)
F= σ(ln P(π) −F(π)) (11)
where the prior over policiesP(π) is the softmax of the negative expected free energy
σ(−G(π)). Note that in the case of "temporally deep" or multi-timestep policies, the ex-
pected free energy of a given policyG(π) is the sum of the timestep-speciﬁc expected free
energies:
G(π) =
∑
τ
Gτ(π) (12)
In pymdp and theDEM toolbox of MATLAB, one has the option of augmenting the prior
over policies with a ‘baseline policy’ or ‘habit vector’P(π0), also referred to as theE vector.
This means the full expression for the optimal posterior can be written as (expandingln P(π)
as ln P(π0) −G(π)):
Q∗(π) = σ(−G(π) + lnP(π0) −F(π)) (13)
This means the inferred policy distribution combines inﬂuences from the expected free
energy of each policy (Gπ), a baseline prior probability assigned to each policy (ln P(π0)) and
the variational free energy of each policy (F(π)). Numerically, policy inference is achieved
by computing the expected and variational free energies of each policy and then combining
them with the policy prior (theE vector) before softmaxing them. The expected free energy
is computed per policy as the integral of the timestep-speciﬁc expected free energies, as
shown in Equation (7). This is achieved by computing the ‘posterior predictive densities’
expected under each policy: Q(ot:T,st:T|π) and using those densities to compute and add
33
together the epistemic value and utility for each policy. These posterior predictive densities
are simply the posterior beliefs at the current timestept ‘multiplied through’ the transition
and observation models (in code: theA and B arrays) over the temporal horizon of the policy.
By doing this iteratively across policies, theG vector ends up storing a ‘cost’ for each policy,
which is then integrated with the policy prior and variational free energy of each policy to
determine the posterior probability of each policy, stored inQ(π). This boils down to the
following single line ofNumPy code:
q_pi = softmax(-G + np.log(E) - F)
Inpymdp, thefunctionsupdate_posterior_policies() andupdate_posterior_policies
_full() of the control module perform the calculations needed for policy inference, and
themselves are called by theinfer_policies method of Agent.
learning.py
In this section we will summarize the functions in thelearning module and then derive the
update equations for updating model parameters of the likelihood and prior distributions
that comprise POMDP generative models.
The functions used to implement inference over model parameters can be found in
the learning.py ﬁle. These functions are called by methods of Agent like update_A(),
update_B(), and update_D(). We survey the most important functions of thelearning
module below:
• update_obs_likelihood_dirichlet(pA, A, obs, qs, lr=1.0, modalities="all"):
This function computes the posterior Dirichlet parametersQ(A) over theA array or
observation modelP(oτ|sτ,A). As input arguments this function requires the current
Dirichlet priorpA over the parameters of theA array, the current value of the categor-
ical A array (which is also the expected value of the Dirichlet priorpA), an observation
obs, the current posterior beliefs about hidden statesqs, a learning ratelr and a list of
which observation modalities to update,modalities. The default setting is to update
theA arraysassociatedwithallobservationmodalities( modalities = "all"), butthis
extra argument allows one to only update speciﬁc sub-arrays of a larger multi-modality
A array. For example,modalities = [0, 1] would only update sub-arraysA[0] and
A[1]. The learning rate parameter scales the size of the update to the posterior over
the A array.
• update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0,
factors="all"): This function computes the posterior Dirichlet parameters Q(B)
over the B array or transition model P(sτ|sτ−1,uτ−1,B). As input arguments this
function requires the current Dirichlet priorpB over the parameters of theB array, the
current value of the categoricalB array, the posterior beliefs about hidden states at the
current timestepqs, the posterior beliefs about hidden states at the previous timestep
qs_prev, a learning ratelr and a list of which hidden state factors to update,factors.
34
The default setting is to update theB arrays associated with all hidden state factors
(factors = "all"), but this extra argument allows you to only update speciﬁc sub-
arrays of a larger multi-factorB array. For example, factors = [0, 1] would only
update sub-arraysB[0] and B[1]. The learning rate parameter scales the size of the
update to the posterior over theB array.
• update_state_prior_dirichlet(pD, qs, lr=1.0, factors="all"): This function
computes the posterior Dirichlet parametersQ(D) over theD array or prior over initial
hidden statesP(s0|D). As input this function requires the current Dirichlet priorpD
over the parameters of theD array, the posterior beliefs about hidden states at the
current timestep qs, a learning rate lr and a list of which hidden state factors to
update, factors. The default setting is to update theD vectors associated with all
hidden state factors (factors = "all"), but this extra argument allows you to only
update speciﬁc sub-vectors of a larger multi-factorD array. For example,factors =
[0, 1] would only update sub-arraysD[0] and D[1]. The learning rate parameter
scales the size of the update to the posterior over theD array.
Inference of POMDP model parameters Under active inference, learning is cast as
inference about model parameters, and in the context of neuroscience is often analogized to
slower-scale changes to inter-neuronal synaptic weights (e.g. Hebbian learning) [36]. Param-
eter inference is referred to as ‘learning’ because it is often assumed to occur on a funda-
mentally slower timescale than hidden state and policy inference [44]. However, the update
equations for model parameters follow the exact same principles as hidden state inference -
namely, we optimize a variational posterior over model parametersQ(φ) by minimizing the
variational free energyF.
For the POMDP generative models used inpymdp, learning manifests as posterior infer-
ence over hyperparameters of the (categorical) likelihood and priors of the generative model.
We use Dirichlet distributions as conjugate priors for the categorical distributions5, meaning
that the hyperparametersφbecome the parameters of Dirichlet distributions. This choice of
parameterization results in remarkably simple and biologically-plausible updates for the pos-
teriors over these parameters, wherein ‘ﬁre-together-wire-together’-like Hebbian increments
are used to learn the parameters as a function of observations. Below we derive the update
rule for Dirichlet hyperparameters over theA, B, andD arrays.
To begin, we augment the POMDP generative model in (1) with the parameters of the
likelihood and prior categorical distributions and Dirichlet priors over each of them. In
order to do this, we divide the hyperparametersφin the into subsets that correspond to the
categorical and Dirichlet parameters over theA, B, andD arrays:
5a prior is calledconjugate to a likelihood when the resulting posterior is the same distribution family as
the prior
35
φ= {A,a,B,b,D,d }
P(oτ|sτ,A) = Cat(A)
P(A) =
∏
j
P(A•j), P (A•j) = Dir(a•j)
P(sτ|sτ−1,uτ−1,B) = Cat(B)
P(B) =
∏
j
∏
u
P(B•ju), P (B•ju) = Dir(b•ju)
P(s1|D) = Cat(D)
P(D) = Dir(d) (14)
where the notationX•j denotes the jth column of a matrixX. Under this parameteri-
sation, A, B, and D are arrays of categorical parameters (i.e. probabilities) that ‘ﬁll out’
the entries of theA, B, andD arrays respectively. The Dirichlet parametersa, b, andd are
similarly the parameters of Dirichlet priors over these categorical distributions, and have
identical dimensionality to the distributions they parameterise. The Dirichlet parameters
are constrained to be positive real numbers (a,b,d ∈R>0) that score the prior probability
of each entry of the categorical distribution they parameterize. Dirichlet values, like the
parameters of other common conjugate prior distributions, can be interpreted as ‘pseudo-
counts’ measuring how often a particular outcome level is expecteda priori (e.g. the prior
probability assigned to a particular state-observation coincidence in the case of theA dis-
tribution). Note that for notational convenience we assume the generative model is not
factorized into multiple hidden state factors and observation modalities, but for generality
one could add in additional indices to capture multiple hidden state factors and observa-
tion modalities. For instance, the most general form of a (potentially multi-modality and
multi-factor) observation model would be:
P(oτ|sτ,A) = {Cat(A1),Cat(A2),..., Cat(AM)}
P(Am) =
∏
j,k,...
P(Am
•jk...), P (Am
•jk...) = Dir(am
•jk...)
Given the introduction of the new Dirichlet priors in Equation (14), we can now write
down the augmented generative model, where the hyperparametersφ have been split into
individual priors overA, B, andD:
P(o[1:T],s[1:T],π,A,B,D ) = P(A)P(B)P(D)P(s1|D)
T∏
τ=2
P(sτ|sτ−1,B)
T∏
τ=1
P(oτ|sτ,A) (15)
Given the new generative model with Dirichlet priors, we can now formulate learning as
approximate inference about these parameters, i.e. optimizing variational posteriors over the
36
likelihood and prior parameters. We begin by expanding our expression of the variational
posterior to include beliefs over the values of theA, B, andD distributions:
Q(s[1:T],π,A,B,D ) = Q(A)Q(B)Q(D)Q(π)
T∏
τ=1
Q(sτ|π)
where Q(A) =
∏
j
Q(A•j), Q(A•j) = Dir(a•j)
Q(B) =
∏
j
∏
u
Q(B•ju), Q(B•ju) = Dir(b•ju)
Q(D) = Dir(d)
where now the variational parametersa, b, andd are Dirichlet parameters of the approx-
imate posteriorsQ(A), Q(B), andQ(D), respectively. Performing inference with respect to
A, B, and D thus amounts to optimizing the variational Dirichlet parameters in order to
minimize free energy. This is what is meant by ‘learning’ in active inference.
We will now step through the update rules for each of the Dirichlet posteriors over the
A, B, andD distributions. We begin by writing down the full variational free energy:
F1:T = EQ(s1:T,A,B,D,π) [ln Q(s1:T,A,B,D,π ) −ln P(o1:T,s1:T,π,A,B,D )]
= EQ(s1:T,A,B,D,π)[ln Q(A) −ln P(A) + lnQ(B) −ln P(B)
+ lnQ(π) −ln P(π) + lnQ(D) −ln P(s1)
+
T∑
τ=1
ln Q(sτ|π) −
T∑
τ=2
ln P(sτ|sτ−1,π,B )
−
T∑
τ=1
ln P(oτ|sτ,A)] (16)
Learning the observation model We begin with the update rule for the Dirichlet
parameters over Q(A), i.e. updating the parameters of the observation model orA array.
We can ﬁrst isolate the components of the free energy that depend onQ(A) since we’re
interested in the gradients of the free energy with respect toa, the parameters ofQ(A):
F1:T = DKL[Q(A) ∥P(A)] −
T∑
τ=1
EQ(sτ,π)Q(A)[ln P(oτ|sτ,A)] + ... (17)
We can expand the KL divergence betweenQ(A) andP(A) as follows, using the deﬁnition
of the KL divergence between Dirichlet distributions and the independence of the Dirichlet
matrices across diﬀerent columns (i.e. hidden state levels):
37
DKL[Q(A) ∥P(A)] =
∑
j
DKL[Dir(a•j) ∥Dir(a•j)]
=
∑
j
(
ln Γ(a0j) −
∑
i
ln Γ(aij) −ln Γ(a0j) +
∑
i
ln Γ(aij)
)
+ (a −a)EQ(A)[ln P(oτ|sτ,A)] (18)
where the termsa0 and a0 are matrices whose entries store the column-wise sums of the
Dirichlet parameters ofa and a, respectively, i.e.a0j = ∑
iaij. We can then combine the KL
divergence with the remaining term in the free energy that depends onQ(A) and take the
gradients of the free energy with respect toEQ(A)[ln P(oτ|sτ,A)], which we hereafter refer to
as lnA:
F1:T =
∑
j
(
ln Γ(a0j) −
∑
i
ln Γ(aij) −ln Γ(a0j) +
∑
i
ln Γ(aij)
)
+ (a −a)EQ(A)[ln P(oτ|sτ,A)] −
T∑
τ=1
EQ(sτ,π)Q(A)[ln P(oτ|sτ,A)] + ...
=⇒ ∂F
∂lnA = a −a−
T∑
τ=1
oτ ⊗sτ
where sτ =
∑
π
Q(sτ|π)Q(π) (19)
where ⊗denotes the outer product andsτ is also known as the Bayesian model average
of hidden states, where the average is taken with respect to the posterior over policies,Q(π).
This move is what allows one to convert from the full posterior over hidden states and policies
Q(s1:T,π) to a hidden state representation that is not conditioned on policies.6 If we set the
gradient ∂F
∂lnA equal to 0 and solve fora, then we obtain the ﬁxed-point solution for the
variational posterior:
a∗= a+
T∑
τ=1
oτ ⊗sτ (20)
Note that the use of the gradient with respect tolnA instead ofa directly is suﬃcient
for deriving the update rule [68]. This can be seen by rewritinglnA as ψ(a) −ψ(a0), where
ψ is the component-wise digamma function [44]. The digamma function is monotonically
increasing in a, meaning ∂F
∂lnA and ∂F
∂a have the same minima. This learning rule is imple-
mented by the functionupdate_obs_likelihood_dirichlet() in the learning module,
which itself is wrapped by theupdate_A() method of Agent.
6The Bayesian model average can be computed using the functionaverage_states_over_policies()
in theinference module.
38
Learning the transition model The updates for the Dirichlet posteriorQ(B) are de-
rived similarly to those forQ(A), starting from the full expression for the variational free
energy, isolating those terms that only depend onb:
F1:T = DKL[Q(B) ∥P(B)] −
T∑
τ=2
EQ(sτ,sτ−1,π)Q(B)[ln P(sτ|sτ−1,π,B )] + ...
= DKL[Q(B) ∥P(B)] −
T∑
τ=2
EQ(π)
[
Q(sτ|π)TEQ(B)[ln P(sτ|sτ−1,uτ,B)]Q(sτ−1|π)
]
(21)
where uτ is the action expected at timeτ under Q(π). If we take the gradients ofF1:T
with respect tolnB = EQ(B)[ln P(sτ|sτ−1,uτ,B)] and solve for ∂F
∂lnB = 0, then we recover
the variational solution for the posterior parametersb:
bu∗= bu +
T∑
τ=2
∑
π
Q(uτ|π)Q(π) (Q(sτ|π) ⊗Q(sτ−1|π)) (22)
This update can be expressed intuitively as follows: a given timestepτ the B matrix is
updated using the outer product of the beliefs about states atτ and the beliefs about states
at τ −1. These updates are done in an action-conditioned sense, such that the update only
applies to theuth ‘slice’ of theb Dirichlet parameters, depending on the action(s) expected at
time τ under Q(π). In pymdp, we assume that the actions at past timesteps are known with
certainty, meaning that the termQ(uτ|π)Q(π) reduces to a delta function over the action
actually taken, and the update forb only happens to one slicebu at a time. The relevant
function for implementing learningQ(B) is update_transition_likelihood_dirichlet()
in learning and theupdate_B() method of Agent.
Learning the state prior Finally, we can write down the updates for the Dirichlet
posterior over initial hidden statesQ(D) using the same formalism as used forQ(A) and
Q(B). First, we ﬁnd the terms of the free energy that depend ond:
F1:T = DKL[Q(D) ∥P(D)] −EQ(π)[Q(s1|π)]TEQ(D)[ln P(s1|D)] + ... (23)
Taking the gradients of F with respect to lnD = EQ(D)[ln P(s1|D)] and setting the
gradient to0 yields the following ﬁxed-form solution ford:
d∗= d+ EQ(π)[Q(s1|π)] (24)
Therelevantfunctionforimplementinglearning Q(D) isupdate_state_prior_dirichlet()
in learning and theupdate_D() method of Agent.
For more complete versions of each of these derivations, we refer the reader to Section 8
and Appendix A.1 of Da Costa et al. 2021 [36].
39