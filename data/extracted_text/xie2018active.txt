arXiv:1809.08406v1  [cond-mat.stat-mech]  22 Sep 2018
Active image restoration
Rongrong Xie 1, ∗, Shengfeng Deng 1, ∗, Weibing Deng 1, † and Armen E. Allahverdyan 2, †
1Key Laboratory of Quark and Lepton Physics (MOE) and Institu te of Particle Physics,
Central China Normal University, Wuhan 430079, China
2Yerevan Physics Institute, Alikhanian Brothers Street 2, Y erevan 375036, Armenia
We study active restoration of noise-corrupted images gene rated via the Gibbs probability of an
Ising ferromagnet in external magnetic ﬁeld. Ferromagneti sm accounts for the prior expectation
of data smoothness, i.e. a positive correlation between nei ghbouring pixels (Ising spins), while the
magnetic ﬁeld refers to the bias. The restoration is activel y supervised by requesting the true values
of certain pixels after a noisy observation. This additiona l information improves restoration of other
pixels. The optimal strategy of active inference is not know n for realistic (two-dimensional) images.
We determine this strategy for the mean-ﬁeld version of the m odel and show that it amounts to
supervising the values of spins (pixels) that do not agree wi th the sign of the average magnetization.
The strategy leads to a transparent analytical expression f or the minimal Bayesian risk, and shows
that there is a maximal number of pixels beyond of which the su pervision is useless. We show
numerically that this strategy applies for two-dimensiona l images away from the critical regime.
Within this regime the strategy is outperformed by its local (adaptive) version, which supervises
pixels that do not agree with their Bayesian estimate. We sho w on transparent examples how active
supervising can be essential in recovering noise-corrupte d images and advocate for a wider usage of
active methods in image restoration.
I. INTRODUCTION
Many inference problems in machine learning amount to restoration o f a hidden structure based on noisy observa-
tions. They are similar (sometimes isomorphic) to models of equilibrium s tatistical physics, where the role of noise
is played by frozen disorder. In particular, the problem of noise-co rrupted image restoration can be mapped to the
two-dimensional Ising ferromagnet [1–7]. The Gibbs probability of th is model serves as a prior probability for images.
Spins (two-value variables equal to ±1) of the Ising model refer to the black-white pixels of a digital image . The
ferromagnetic feature of the model means that neighbouring pixe ls are correlated, a legitimate minimal assumption
on the prior probability.
Methods of solving inference problems cannot perform without prio r information. Since its amount is limited, it is
natural to study informed strategies for requesting prior inform ation. In particular, prior information can be requested
on the ground of the previous functioning of the inference method . This notion of active inference [8] emerged in
statistics [9] and has been applied to various inference problems inclu ding Hidden Markov Models [10–12], network
inference [13–16], economics, experiment optimization etc [8]. In the context of noisy image restoration, the concept
of active inference assumes that there is a costly noiseless channe l by which the correct values for some of pixels can
be communicated directly to the image restorator. In practice, th is noiseless channel may mean an additional precise
(but costly) measurement of pixels. Since the channels is costly, on ly a small fraction (say 5%) of original pixels
is communicated with a hope that—if employed properly—they can lead to a sizable improvement in restoration of
other (noisy) pixels. The subject of active inference has two relat ed issues: which prior information is to be requested
and to what extent this information improves the method performa nce [8]. The ﬁrst question refers to the meaning
of (prior) information, the second one to its value.
Most existing theoretical results on active inference in graphical m odels are concerned with sub-modular cost
functions that allow to establish certain optimality guarantees [17, 1 8]. The sub-modularity assumption does not
hold in several interesting situations, e.g. for systems with long-ra nge correlations and scale-invariance, a range of
phenomena that is collectively designated as critical behavior. The c learest example of such a behavior is the second-
order phase transitions in statistical mechanics [7]. It was found th at natural images (e.g. nature scenes) are organized
in a scale-invariant way [19] and do have long-range correlations [20]. Statistical mechanics (e.g. the Ising model)
is the most appropriate way of describing such images [20]. Generally, the optimal strategy of active inference—i.e.
which variables are to be supervised given a noisy observation—is not known beyond a direct enumeration [18].
Here we shall study an active recognition (estimation or ﬁltering) of a noise-corrupt image, which was generated
via the two-dimensional Ising ferromagnet. The model does allow fo r a critical regime depending on the value of the
∗ These authors contributed equally to this work.
† Authors to whom any correspondence should be addressed: wde ng@mail.ccnu.edu.cn and armen.allahverdyan@gmail.com
2
ferromagnetic coupling constant, which is taken as the known hype rparameter of the model. For a large number of
pixels, this critical regime is realized via a second-order phase-tran sition, with magnetization being an order parameter.
We analytically determined the optimal active supervising strategy o f the Ising model in the mean-ﬁeld limit.
This mean-ﬁeld limit of the Ising model was already studied for a Gauss ian noise image-restoration problem [6,
7]. More generally, mean-ﬁeld methods are widely applied in statistical inference problems [21–25]; see [26] for a
book presentation. The inferred image is determined by minimizing the Bayesian risk [27]. The optimal mean-ﬁeld
strategy amounts to supervising only those pixels whose observed (noisy) value does not agree with the sign of the
magnetization. It also shows that there is a maximal number of supe rvised variables, beyond of which the supervising
is not meaningful. Without active supervising, the optimal inference of the mean-ﬁeld model shows only two (in a
sense trivial) regimes, viz. observation-dominated (where the prior information is not needed ) and prior-dominated
(where observations are redundant). With active supervising, th ere is a non-trivial coupling between observations
and the prior knowledge, i.e. no separation into two diﬀerent regimes is possible.
The mean-ﬁeld solution of the actively supervised model is transpar ent and guides the understanding of a more
complex, two-dimensional situation which is studied numerically. We sa w that the mean-ﬁeld optimal active super-
vising strategy performs well in the two-dimensional case, if the mo del is away from the critical regime. Otherwise,
we found a strategy that is superior to the mean-ﬁeld optimal one. This strategy amounts to comparing the observed
(noisy) value of pixel with its Bayesian estimate: only those pixels are supervised for which these values disagree. We
show on concrete examples how usual (not active) methods fail to restore noise-corrupted images, while the active
supervising does lead to a satisfactory restoration.
This text is organized as follows. Section II recalls the general theo ry of optimal Bayesian inference with and
without supervising. Here we also recall how this theory can be repr esented via Ising models. Next section ﬁnds out
the optimal active supervising strategy in the mean-ﬁeld version of the Ising model. Section IV studies numerically
the two-dimensional Ising model. It shows that several results de duced in the mean-ﬁeld situation also apply in this
more realistic case. Section IV also works out a local version of that optimal supervising strategy and shows that
it yields better results in the critical regime of the model, where the m ean-ﬁeld method does not apply. Section V
studies the Ising model, where the prior probability has a small bias (in the value of spins) given by a weak external
magnetic ﬁeld. Note that sections IV and V (numerical results on a s quare Ising lattice) can be read independently
from III (analytic results in the mean-ﬁeld limit).
We summarize in the last section. In preparation for future resear ch, Appendix studies hyperparameter estimation
of the mean-ﬁeld Ising model. The analysis here is deeper than norma lly done in literature, because we study the
inﬂuence of non-identiﬁability on the optimal restoration.
II. GENERAL FORMULATION OF IMAGE RESTORATION AND REPRESENT ATION VIA ISING
MODELS
A. Without supervising
Here we recall general ideas of the Bayesian inference for two-va lued random variables with and without supervising.
Next sections will specify them for the image restoration problem.
Let there are two dependent vector random variables X = ( X1, ..., XN ) and Y = ( Y1, ..., YN ). Now X is called
the hidden variable, since it cannot be observed directly. Instead, we assume that a concrete value y = ( y1, ..., yN )
of Y is observed. On the ground of y we want to ﬁnd a representative value (an estimate) ξ(y) of x = ( x1, ..., xN )
that is likely to be the source of y. We shall assume that yk = ±1, xk = ±1 and call them spins (pixels). The
joint probability is P (X, Y); e.g. the conditional probability P (Y|X) describes a noisy channel, where Y is the
noise-corrupted observation of X.
The quality of the estimate ξ(y) is given by the average Hamming distance (or overlap):
O(y; ξ) = 1
N
N∑
i=1
∑
x
ξi(y)xiP (x|y), (1)
so that a larger O( y; ξ) means a better restoration. We stress that the averaging over P (x|y) in (1) is done “by hands”:
since the hidden variables are not known, one generates them via P (x|y) (for ﬁxed observations) and then takes the
average. Eq. (1) coincides with the inverse Bayesian risk [27]. The lat ter is minimized, while (1) is maximized over
ξi(y).
In applications one also frequently studies the average of O( y) over P (y). This averaging can also be done by
hands, i.e. over suﬃciently many applications, but it is important to st ress that if N is suﬃciently large (and Y is an
3
ergodic process), O( y; ξ) does self-average, i.e. according to the law of large numbers the s um (1) over a large number
N ≫ 1 is replaced by its average O( y; ξ) ≃ ∑
y P (y)O(y; ξ).
Maximizing (1) over ξ(y) leads to the optimal method ( i = 1 , ..., N ) with the largest overlap ˆO(y) for given
observations y:
ˆξi(y) = sign
[ ∑
x
xiP (x|y)
]
= sign
[ ∑
xi=±1
xiP (xi|y)
]
, (2)
ˆO(y) = 1
N
N∑
i=1
⏐
⏐
⏐
⏐
⏐
∑
x
xiP (x|y)
⏐
⏐
⏐
⏐
⏐. (3)
Eqs. (3) can serve, e.g. for evaluating the maximum-likelihood metho d (ML). Recall that the ML does not employ
the prior probability P (x), and is based on maximizing P (y|x) for given observations y:
ξML(y) = arg max x[ P (y|x) ]. (4)
If the prior probability P (x) is available, ξML(y) is suboptimal from the viewpoint of (2), and one can determine how
far its prediction for the overlap is from the optimal value O( y) given by (3).
For completeness we mention an equivalent approach to introducing the overlap (1). Let us temporarily assume
that we know the original realization x of the random variable X. Once it passes through the noisy channel, we obtain
y = ǫ x, where ǫ is the noise variable and ǫ x means elemetwise multiplication of two vectors (Hadamard product) .
Now the overlap can be deﬁned as
Q = 1
N
N∑
i=1
xi ˆξi(ǫ x). (5)
The joint distribution of x and ǫ is PXY(x, ǫx) = P (x, ǫx).
To make Q independent from the concrete choice of x and ǫ we look at the average
Q = 1
N
N∑
i=1
∑
x, ǫ
P (x, ǫx)xi ˆξi(ǫ x) =
∑
y
P (y) ˆO(y), (6)
which brings us back to the averaged (3). However, there is no dire ct relation between O( y) and Q.
B. With supervising
Given observations y one requests an additional information on certain xi (supervising) so as to improve the quality
of restoration for the remaining variables. This is described by the v ector
n = ( n1, ..., nN ), n i = 0 , 1, (7)
where ni = 1 ( ni = 0) means that the true value of the corresponding xi is requested (not requested). Naturally, the
number of supervised spins is ﬁxed (i.e. not all spins are supervised)
N∑
i=1
ni = ρN. (8)
The supervising strategy is described by conditional probability P (n|y), i.e. the supervising is generally probabilistic.
Let us divide x into supervised ( x′′) and not supervised ( x′) parts. x′ and x′′ depend on n, but this dependence is
not indicated explicitly to avoid excessive notations. E.g. if x = ( x1, x2, x3) and n = (1 , 0, 0), then x′ = ( x2, x3) and
x′′ = ( x1). Let P (x′, y) and P (x′′, y) be (respectively) the joint probability of non-supervised and sup ervised spins
and observations. These probabilities are found from P (x, y).
Let the values of x′′ were requested and send via a noiseless channel (we explain below th e meaning of this
assumption). They appeared to be x′′ = η. Together with y and n, also η is by now given. Hence the distribution
P (x′|η, y) of the non-supervised spins is conditioned both by observations y and by supervised variables η.
4
We estimate non-supervised spins as ξ′(η, y, n). The corresponding overlap reads:
O(η, y, n; ξ′) = 1
N(1 − ρ)
N∑
k=1
(1 − nk)
∑
x′
ξ′
k(η, y, n) x′
k P (x′|η, y), (9)
where the summation is taken over non-supervised spins only. For a given n, the optimal overlap is calculated as in
(2, 3):
ˆξ′
k(η, y, n) = sign
[ ∑
x′
x′
kP (x′|η, y)
]
= sign

 ∑
x′
k=±1
x′
kP (x′
k|η, y)

 , (10)
ˆO(η, y, n) = 1
N(1 − ρ)
N∑
k=1
(1 − nk)
⏐
⏐
⏐
⏐
⏐
⏐
∑
x′
k=±1
x′
kP (x′
k|η, y)
⏐
⏐
⏐
⏐
⏐
⏐
. (11)
Now if y, n and η are suﬃciently long, ˆO(η, y, n) will self-average over P (η, y, n) = P (y)P (η|y)P (n|y).
The major problem of supervising is to ﬁnd the best P (n|y), which holds (8) and provides the largest average
overlap (11). For understanding this problem one should rely on mod els, because there is no general solution for it
under N ≫ 1 (for a small value of N one can proceed with straightforward calculations).
Let us now explain the practical meaning of the above assumption on the existence of a noiseless channel. We note
that frequently such channels do exist (e.g. because they relate t o a suﬃciently precise equipment), but they are
costly, i.e. the cost per pixel (or spin) for using such a channel is hig h. In that case it is useful to employ the noisy
channel for N pixels, but still to use the noiseless channel for ρN actively supervised pixels, where ρ ≪ 1.
C. Ising model
Let us now assume that the prior probability of hidden variables x = ( x1, ..., xN ) is given as [1–5, 7]
P (x) ∝ eJ′ ∑
{i,k} xixk , x i = ±1, (12)
where J′ > 0 is a constant hyperparameter (coupling constant), and −J′ ∑
{i,k } xixk is the Hamiltonian of the Ising
ferromagnet, where {i, k} means summation over neighbours of a lattice. In the context of th e image recognition
problem this is a square lattice. Here xi = ±1 refer to black-white pixels of the original image, and the ferromag netic
coupling J′ > 0 means that there is a prior information on positive pixel-pixel corre lations. This is just the standard
smoothness assumption.
We assume that the observed variables y = ( y1, ..., yN ) (noise-corrupted image) relate to x via a symmetric and
independent noise
P (y|x) =
N∏
i=1
p(yi|xi) = (2 cosh[ h])−N
N∏
i=1
eh ∑
i xiyi , (13)
h ≡ 1
2 ln 1 − ǫ
ǫ ≥ 0, (14)
where h relates to the error probability 0 < ǫ < 1/2, and where we recall that yi = ±1.
Combining (12) and (13) we get for the joint probability P (x, y) of hidden variables x and observations y
P (x, y) ∝ e−H(x, y), H (x, y) = −J′ ∑
{i,k }
xixk − h
∑
i
xiyi, (15)
Now P (x, y) refers to the Gibbs distribution of a statistical system at temper ature T = 1 and with Hamiltonian
H(x, y). We shall assume that J and h are known hyperparameters. More generally, they are not known , but should
be inferred from data; see [32] for a recent review. Standard met hods for doing that are discussed in Appendix
together with their limitations.
5
III. MEAN-FIELD ANALYSIS
A. The fully coupled (mean-ﬁeld) Ising model
To get from (15) a solvable model, we assume in (12, 15) that all xi couple with each other:
H(x, y) = − J
2N
∑
i̸=k
xixk − h
∑
i
xiyi, (16)
where we also assumed J′ = J/(2N) to show that we have to have H(x, y) = O(N) for N → ∞ . The ﬁrst sum in (16)
goes through all i, j = 1 , ..., N under i ̸= j. Eq. (16) refers to the mean-ﬁeld-interaction version of the ran dom-ﬁeld
Ising model. It is known to be solvable [6, 7].
Eqs. (15, 16) lead to
P (x, y) ∝ e
J
2N
∑
i̸=k xixk+h ∑
i xiyi = N3/ 2
√
2πJ
∫
dµ exp
[
− NJµ 2
2 +
∑
i
xi(Jµ + hyi)
]
, (17)
P (y) ∝
∑
x
e
J
2N
∑
i̸=k xixk+h ∑
i xiyi = N3/ 2
√
2πJ
∫
dµ exp
[
− NJµ 2
2 +
∑
i
ln cosh(Jµ + hyi)
]
. (18)
For N ≫ 1 the latter integral is taken by the saddle-point method:
P (y) ≃ exp
[
− NJm 2
2 +
∑
i
ln cosh(Jm + hyi)
]
, (19)
where the magnetization m is determined from the saddle-point equation as
m = 1
N
∑
i
tanh(Jm + hyi). (20)
Now formally m is a function of y, but for this model (and in the limit N ≫ 1) it self-averages and becomes (almost)
independent from y. This known fact can be conﬁrmed via calculating correlation functio ns between ( xi, yi) and
(xj , yj). Thus we return to (17) employ there the saddle-point method, u se (20) and end up with
P (x, y) ≃
∏
i
π(xi, yi), π (x, y) = eJmx+hxy
2 cosh[Jm + h] + 2 cosh[ Jm − h], (21)
P (y) ≃
∏
i
π(yi), π (y) = cosh(Jm + hy)
cosh[Jm + h] + cosh[ Jm − h] = 1
2[1 + y tanh(Jm) tanh(h)], (22)
where m is to be determined from (20) via self-averaging:
m =
∑
y=±1
π(y) tanh(Jm + hy) = tanh( Jm). (23)
One can verify from P (x) obtained via (17) that m coincides with the average collective spin of the original image:
m = 1
N
∑
x P (x) ∑
i xi.
For J < 1, (23) predicts m = 0. For J > 1, (23) predicts two solutions with m > 0 and m < 0. They refer to two
diﬀerent ergodic components. Thus we have a second-order phas e transition at J = 1. It belongs to the mean-ﬁeld
universality class.
Thus (2, 21, 22) imply for the optimal situation
ˆξi(y) = sign[ Jm + hyi], i = 1 , ..., N, (24)
and the self-averaged optimal overlap reads from (3)
ˆO =
∑
y=±1
π(y) tanh(|Jm + hy|) = max[ tanh( J|m|), tanh(h) ]. (25)
6
Note that tanh( h) is the overlap of the maximum-likelihood method; see (4, 13). This is c onﬁrmed by taking
ξML
i = sign[ yi] and using it together with (22) in
OML =
∑
y=±1
π(y)sign[y] tanh(Jm + hy) = tanh( h). (26)
Hence the message of (24, 25) is that there are only two extreme s ituations: for h > J |m|—which means a
weak noise according to (14)—the prior information is irrelevant, sin ce observations are reliable. Hence the optimal
estimation method coincides with the maximum-likelihood; see (4, 13). For h < J |m| (strong noise, as seen from
(14)) observations are irrelevant, since the estimate (24) depen ds only on the parameter J of the prior P (x). Put
diﬀerently, either the prior information (given by J in P (x)) is irrelevant, or observations are irrelevant.
B. Supervising
1. The optimal overlap after supervising
We recall from section II B that supervising is described by the cond itional probability P (x′|η, y) of non-supervised
spins given the values of supervised spins η and observations y, as well as by the conditional probability P (n|y) of
the coordinates n of supervised spins. Now (21, 22) show that for the present mode l the probabilities P (x, y) and
P (y) factorize. This implies that P (x′|η, y) does not depend on η. Using again (21, 22) we get from (11):
ˆO(η, y, n) = 1
N(1 − ρ)
N∑
k=1
(1 − nk) tanh[|Jm + hyk|], (27)
where m is determined from (23). Hence the average of (27) over P (n|y)P (y) reads
ˆO = 1
1 − ρ
∑
y=±1
π(y)p(0|y) tanh[|Jm + hy|], (28)
where π(y) is found from (22), and where we assumed that P (n|y) (the probability of n, given the observations y) is
symmetric in the sense that all ( yi, ni) do have the same marginal-conditional probability: p(ni|yi). The constraint
(8) will be implemented in average. Hence p(n|y) holds two constraints:
1 = p(1|y) + p(0|y), (29)
1 − ρ =
∑
y=±1
π(y)p(0|y). (30)
The meaning of (28) is intuitively clear: the optimal supervising amoun ts to changing—from π(y) to π(y)p(0|y)—the
(eﬀective) distribution of observations, where p(0|y) is the probability of not supervising a given spin. The same idea
can be implemented as an anzatz for more general models, but ther e it does not have to be optimal.
2. Random supervising provides no advantage
Let us ﬁrst consider the random supervising, where p(n|y) does not depend on y. This implies from (30):
p(0) = 1 − ρ, p (1) = ρ. (31)
It should be clear that we get from (28) the same expression (25) a s without any supervising.
3. Active supervising
We turn to the active supervising and maximize ˆO given by (28) over p(0|y) under constraints (29, 30). To
understand the idea of maximization, assume m > 0. Then |Jm + h| = Jm + h > |Jm − h|, and (28) maximizes
7
upon taking p(0|1) such that π(1)p(0|1) is maximally large and compatible with (29), and after that taking p(0| − 1)
as large as constraints (29, 30) still allow. The general solution is wr itten via introducing
ζ = 1
2[1 + tanh( J|m|) tanh(h)], (32)
which is equal to π(1) if m > 0. Then the probabilities maximizing (28) read
for m > 0 p(0|1) = min
[ 1 − ρ
ζ , 1
]
, p (0| − 1) = max
[
0, 1 − ρ
1 − ζ
]
, (33)
for m < 0 p(0| − 1) = min
[ 1 − ρ
ζ , 1
]
, p (0|1) = max
[
0, 1 − ρ
1 − ζ
]
. (34)
Note that in (33) the ﬁrst (second) argument of min is selected tog ether with the ﬁrst (second) argument of max.
The same holds in (34). The optimal ˆO reads from (33, 34):
ˆO = min
[ ζ tanh[J|m| + h] + (1 − ρ − ζ) tanh[ | J|m| − h| ]
1 − ρ , tanh[ J|m| + h]
]
. (35)
When ρ starts to increase from ρ = 0, (35) monotonously increases from its ρ = 0 value given by (25) till its maximal
value tanh[ J|m| + h]. According to the non-supervised maximal overlap (25), for J > 1, but Jm < h (i.e. the noise is
weak, as seen from (14)) the prior is not relevant. This is improved a fter supervising, now the prior is always relevant
provided that J > 1. It is not meaningful to supervise for 1 − ρ < ζ , since having reached tanh[ J|m| + h], the overlap
there does not anymore increase with increasing ρ; see (35). Hence the values of p(0|1) = 1−ρ
ζ and p(0| − 1) = 0 in
(33)—as well as p(0| − 1) = 1−ρ
ζ and p(0|1) = 0 in (34)—are redundant. Then (33, 34) show that observatio ns that
agree with the sign of the average magnetization m should not be supervised: p(1|sign[m]) = 0.
So far we assumed that the hyperparameters J and h in (respectively) P (x) and P (y|x) are known precisely.
Generally, this is not the case, and hyperparameters themselves a re to be found from data; see [32] for a recent
review. Appendix discusses this problem for the considered model.
IV. NUMERICAL RESULTS FOR A SQUARE LATTICE
A. Two methods for active supervising
The above results concerned the mean-ﬁeld model, i.e. an unrealistic situation if we take into account that real
images are two-dimensional. Hence we turn to studying numerically th e Ising model (15) on a square lattice with
periodic boundary conditions. We start from (9) and (15), and stu dy two diﬀerent strategies of active supervising.
In the ﬁrst (global) strategy we supervise Nρ spins selecting them via the following criterion: given the observation
vector y, the Nρ supervised spins ηi are chosen randomly among those that hold
yi sign
[
1
N
N∑
i=1
yi
]
= −1, (36)
i.e. those spins are supervised which do not agree with the sign of the collective value 1
N
∑ N
i=1 yi. Thus variables
n in (7) are determined via (36). This strategy is clearly inspired by the above mean-ﬁeld solution (33, 34), where
1
N
∑ N
i=1 yi is the observed magnetization; see Fig. 1 for the value of 1
N
∑ N
i=1 yi averaged over many samples, as well
as a single-sample form of it. For ǫ < 1/2 (which we assume to be the case from (13)) and a suﬃciently large N, we
get that 1
N
∑ N
i=1 xi and 1
N
∑ N
i=1 yi have the same sign. Note that strategy (36) is easy to implement.
Within the second (local) strategy we randomly select Nρ supervised spins among those that hold
yi ˆξi(y) = −1, (37)
i.e. now one ﬁrst calculates the optimal estimate ˆξi(y) according to (2, 15) and then supervises those spins that do
not agree with observations. This strategy did not show up within th e optimal solution for the mean-ﬁeld situation.
We choose it with an expectation that can work, where the mean-ﬁe ld method does not apply, i.e. ﬂuctuations are
essential.
8
●●●●●●●
●
●
●
●●●
■■■■■■■
■
■
■■■■
○○○○○○○
○
○
○
○
○○
□□□□□□□
□
□
□□□□
● 1
N ∑i xi
■ 1
N ∑i yi
○ 1
N ∑i xi
□ 1
N ∑i yi
0.40 0.45 0.50 0.55 0.60 0.65
0.0
0.2
0.4
0.6
0.8
1.0
J ′
FIG. 1: Magnetization 1
N
∑ N
i=1 xi and its observed value 1
N
∑ N
i=1 yi versus J′ under a ﬁxed ǫ = 0 .1 in (13) (hence h = 1 .09861).
For each J′ single realizations of x = {xi}N
i=1 and y = {yi}N
i=1 were generated according to (12, 13) (i.e. the external magn etic
ﬁeld is zero). We also plotted the averaged values 1
N
∑ N
i=1
xi and 1
N
∑ N
i=1
yi under the same ǫ = 0 .1. For each ﬁxed J′
the averaging was taken over 1000 realizations of random var iables x generated according to (12, 13) and selected such that
1
N
∑ N
i=1 xi > 0 for all samples included in the averaging. It is seen that 1
N
∑ N
i=1 xi and 1
N
∑ N
i=1 yi increase sharply in the
vicinity of J′ = 0 .45 indicating that in the limit N ≫ 1 the system has a second order phase-transition; cf. the dis cussion in
the beginning of section IV B.
Naturally, (36, 37) are to be compared with the random supervising , where Nρ supervised spins ηi are chosen
completely randomly, i.e. without any dependence on y. Note that besides (36, 37) we also studied several other
supervising strategies, e.g. when −1 in the right-hand-side of (36) or (37) is changed to +1. We shall no t discuss
such strategies, since they proved to be sub-optimal; frequently they are worse than the random supervising, and
sometimes they are worse than having no supervising at all.
B. Results
1. Generation of images
With a given coupling J′, we generated images x by Monte Carlo simulation (Metropolis algorithm) from the
two-dimensional Ising model (12) on a square lattice with periodic bo undary conditions. The overall number of spins
is N = 6400 spins. Given the noise probability ǫ (and hence h) from (13, 14), we ﬂip each spin of the original image x
generating the noisy image y. The conditional averages in (3, 10, 11) are calculated by averagin g over 2 × 103 samples
(we did check that this number suﬃces and the averages saturate ).
2. Three regimes of the square Ising ferromagnet
Recall that the square (two-dimensional) Ising model (12) has thr ee regimes depending on the ferromagnetic coupling
constant J′ [37]; see Fig. 1. For a small values of J′ the system is in the paramagnetic state, where each spin xi is equally
likely to assume values +1 or −1. In the vicinity of a certain critical value J′
c we enter into the critical regime, where
the paramagnetic state gets unstable and there are long-range ﬂ uctuations (hence correlations) [37]. It is well-known
that the inﬁnite square lattice has a second-order phase-transit ion point at J′
c = 1
2 ln(1 +
√
2) ≈ 0.440687 [37]. This
is close to the value J′
c = 0 .45 observed in our numerics done on a ﬁnite lattice; see Fig. 1. Anoth er indication of the
critical regime is that the averaged magnetization 1
N
∑ N
i=1
xi does not coincide with the single-sample magnetization
1
N
∑ N
i=1 xi, as shown by Fig. 1. This is a sign of strong-ﬂuctuations.
The third regime is set for a suﬃciently large J′. Here ﬂuctuations are relatively small. Indeed, Fig. 1 shows that
for J′ > 0.46 we get that a single-sample behavior coincides with the averaged o ne. The symmetry xi → − xi of
the Hamiltonian (12) is spontaneously broken (this process starts from the critical regime). Hence within each given
sample 1
N
∑ N
i=1 xi has a deﬁnite sign [37].
Images generated in the third regime show pixels of one color on the b ackground of another color; see Fig. 2. Such
ﬁgures are interesting also because they are very susceptible to n oise, and cannot be recovered by standard (i.e. not
9
TABLE I: ˆO is the optimal overlap calculated via (3) without supervis ing for a square lattice with N = 6400 spins and Nρ
supervised spins. ˆOR refers to the random supervising. ˆOG and ˆOL refer respectively to global and local strategies accordin g
to (36) and (37); cf. (9). ˆOMF and ˆOGF are the overlaps obtained for (respectively) median ﬁlter a nd Gaussian ﬁlter. abBoth
ﬁlters were applied with the (minimal) radius 1, i.e. via emb edding each pixel into a 3 × 3 box [36]. For each set of parameters
we underline the maximal overlap.
Parameters ˆO ˆOR ˆOG ˆOL ˆOMF ˆOGF
J′ = 0 .500, ǫ = 0 .3, ρ = 0 .1 0.90679 0.90727 0.92012 0.91850 0.90984 0.91563
J′ = 0 .555, ǫ = 0 .1, ρ = 0 .1 0.96341 0.96393 0.99213 0.99166 0.96313 0.96313
TABLE II: The same quantities as in Table I, but in the regime, where ˆOL > ˆOG and ˆOL > ˆOR. Note as well that ˆOMF < ˆOGF.
Parameters ˆO ˆOR ˆOG ˆOL ˆOMF ˆOGF
J′ = 0 .455, ǫ = 0 .1, ρ = 0 .09 0.88363 0.88616 0.92278 0.93314 0.83484 0.87094
J′ = 0 .417, ǫ = 0 .3, ρ = 0 .1 0.61359 0.64128 0.64830 0.66058 0.51936 0.53594
J′ = 0 .417, ǫ = 0 .3, ρ = 0 .2 0.61265 0.687619 0.682026 0.72048 0.51938 0.53594
active) methods; see Fig. 2. In contrast, images generated in the critical regime do show an interesting ﬁne-grained
structure. To some extent they can be recovered via standard m ethods, though the active supervising still leads to a
serious improvement; see Figs. 3 and 4.
3. Images and overlaps
The performance measure of strategies (36, 37) is checked via ov erlaps (3, 11). Overlap close to one is a necessary
condition for a good restoration. For deﬁniteness, we compared t he performance (i.e. the overlap) of the present
non-supervised method with those of several standard ﬁlters—e .g. the median ﬁlter or the Gaussian ﬁlter—that are
employed in the image-recognition; see e.g. [4]. Numerical packages f or implementing these ﬁlters were taken from
[28]. In agreement with the fact that the considered method optimiz es the overlap (3), we found that these ﬁltering
methods produce a smaller overlap than (3).
We note that in the present situation we have a possibility to look at ad ditional performance measures: since we
generate the data ourselves—i.e. we have the original image x—we can employ directly an analogue of (5) that checks
the restored image (after supervising) with the original image x:
1
N(1 − ρ)
N∑
i=1
(1 − ni) ˆξ′
i(η, y) xi, (38)
where ξ′
i(η, y) is deﬁned in (10). Note that since we do not average over η and y, the overlaps (11) and (38) are
generally not equal for a considered ﬁnite value of N = 6400. We conﬁrmed that (11) and (38) are not precisely equal
to each other, though they are normally quite close, since N = 6400 is still suﬃciently large, and the self-averaging
applies approximately. In all relevant situations, if a strategy has a superior performance according to (11), then it is
also superior according to (38). We also stress that all the results on diﬀerences between the strategies were checked
against varying the initial image x.
We identiﬁed two regimes in the behaviour of ˆOL and ˆOG, i.e. the overlaps (11) within the local and global
strategies, respectively. In both regimes the maximal (better) o f them is larger than the overlap ˆOR obtained via the
random supervising at the same number Nρ of supervised spins [see Tables I and II]:
max
[
ˆOL, ˆOG
]
> ˆOR. (39)
Table I shows the ﬁrst regime, where J′ is suﬃciently large, so that the system is away of the critical regime, which
realized for J′ ≈ 0.45; see Fig. 1. In these regime the strategy (36) is dominant over ( 37), though their predictions
10
are close to each other:
ˆOG > ˆOL, ˆOG ≈ ˆOL. (40)
Altogether, for J′ ≥ 0.45 the situation is close to the mean-ﬁeld regime. This is additionally con ﬁrmed by the fact
that the random supervising does not provide any substantial impr ovement: ˆOR ≈ ˆO; see Table I and Fig. 2, and
compare these with the discussion around (31). Fig. 2 describes re gime (40) and shows that without supervising
the real image restoration is absent, despite of the fact that the overlap for this restoration regime is sizable; see
Table I. The reason for this is that the non-supervised restoratio n is over-dominated by the prior information; cf. the
discussion after (26). Fig. 2 also shows that the image restoration greatly improves after applying active methods.
Both local and global methods lead to similar results here.
original image noisy image
 without supervising
 random supervising
local supervising
 global supervising
 median filter
 Gaussian filter
FIG. 2: An example of active supervising for two scenarios (g lobal and local, as given by (36) and (37), respectively) und er
J′ = 0 .555, ǫ = 0 .1 and ρ = 0 .1; see Table I. We display the original image, its noise-corr upted version, the non-supervised
restoration and the active supervision via completely rand omly selected spins (pixels). On decoded images, we denote b y red
(white) those white pixels of the original image that were de coded wrongly (correctly). Likewise, blue (black) shows bl ack
pixels of the original image that were decoded wrongly (corr ectly).
We also compare with the (non-supervised) results obtained via two standard ﬁlters: median and Gaussian. These ﬁlters w ere
applied with the (minimal) radius 1, i.e. via embedding each pixel into a 3 × 3 box [36]. Both active supervising scenarios
recover the image approximately, while other restoration m ethods are useless for this example. In this context, we emph asize
again that a large overlap is necessary but not suﬃcient for i mage recovery.
For smaller values of J′, the local strategy is better than the global one: ˆOG < ˆOL; see Table II. Now the random
supervising can improve over no supervising, and there are cases ( for a small J′), where the global strategy is worse
than the random: ˆOG < ˆOR. The reason why the global strategy does not apply is clear, since f or J′ < 0.45 the
mean-ﬁeld method does not apply, in particular because the model is within the crtical regime; see Fig. 1, where the
critical regime can be identiﬁed by the region, where ∑ N
i=1
xi and ∑ N
i=1
yi sharply increase from zero to values larger
than 0 .5. It is encouraging that even for J′ ≤ 0.45 the local strategy performs well, e.g. it is visibly better than the
random supervising; see Table II. Fig. 3 illustrates this situation for J′ = 0 .5. It is seen that the performance of the
non-supervised restoration is still poor, although better than wh at was seen for Fig. 3. Now the local active scenario
is clearly better than the global one. Finally, Fig. 4 presents a repre sentative example of J′ = 0 .455. Here all methods
perform more or less reasonably, but it is clearly seen that ﬁne deta ils of the original image are captured only by the
local supervising method.
Thus we suggest that the local strategy (37) is to be applied for th is and similar models, because even when it is
sub-optimal it is close to the optimal strategy.
11
original image noisy image
 without supervising
 random supervising
local supervising
 global supervising
 median filter
 Gaussian filter
FIG. 3: Various scenarios including active supervising (gl obal and local, as given by (36) and (37), respectively) unde r J′ = 0 .5,
ǫ = 0 .1 and ρ = 0 .1; cf. Fig. 2. Both active supervising scenarios recover the image, while the no-supervising and random
supervising cases are useless. Notations coincide with tho se in Fig. 2.
original image noisy image
 without supervising
 random supervising
local supervising
 global supervising
 median filter
 Gaussian filter
FIG. 4: Various scenarios including active supervising (gl obal and local, as given by (36) and (37), respectively) unde r J′ =
0.455, ǫ = 0 .1 and ρ = 0 .09; cf. Figs. 2 and 3. Now no-supervising and random supervis ing cases are not useless, but ﬁne
details of the original image are recovered only after the lo cal scenario.
V. NUMERICAL RESULTS WITH EXTERNAL MAGNETIC FIELD
So far we worked with the Hamiltonian (15) on a square lattice. The co rresponding prior density P (x) ∝ e−H(x)
is generated by the Ising Hamiltonian H(x) = −J′ ∑
{i,k } xixk, which is symmetric with respect to the inversion
xi → − xi. This symmetry can be broken with an external ﬁeld hf that introduces a bias in the distribution of x.
Now instead of (15) we look at
˜H(x, y) = −J′ ∑
{i,k }
xixk − hf
∑
i
xi − h
∑
i
xiyi, (41)
12
TABLE III: We present values for overlaps for external magne tic ﬁeld hf = 0 .01 a square lattice with N = 6400 spins and
Nρ supervised spins: ˆO (the optimal overlap calculated via (3) without supervisi ng), ˆOR (random supervising), ˆOG and ˆOL
refer respectively to global and local strategies accordin g to (36) and (37); cf. (9). ˆOMF and ˆOGF are the overlaps obtained for
(respectively) median ﬁlter and Gaussian ﬁlter.
For the global strategy the fraction of supervised spins is ρG = 0 .1. For the local strategy this value ρL is generally lower, as
shown below. Despite of this fact the local strategy produce d better overlaps: ˆOL > ˆOG.
Parameters ˆO ˆOR ˆOG ˆOL ˆOMF ˆOGF
J′ = 0 .38, ǫ = 0 .1, ρL = 0 .05 0.82766 0.83174 0.82865 0.85987 0.67563 0.82250
J′ = 0 .39, ǫ = 0 .1, ρL = 0 .05 0.82578 0.830263 0.82622 0.85691 0.69625 0.82375
J′ = 0 .40, ǫ = 0 .1, ρL = 0 .07 0.84375 0.84543 0.84688 0.88542 0.75516 0.83188
J′ = 0 .41, ǫ = 0 .1, ρL = 0 .07 0.84891 0.85719 0.85208 0.89147 0.76359 0.84375
J′ = 0 .42, ǫ = 0 .1, ρL = 0 .08 0.85406 0.85564 0.86701 0.90353 0.78313 0.84094
J′ = 0 .43, ǫ = 0 .1, ρL = 0 .09 0.86688 0.86058 0.86597 0.91896 0.80328 0.85593
J′ = 0 .44, ǫ = 0 .1, ρL = 0 .1 0.87813 0.89063 0.87951 0.93611 0.83375 0.87344
J′ = 0 .45, ǫ = 0 .1, ρL = 0 .1 0.90047 0.90104 0.90381 0.95243 0.86953 0.89188
which leads to the joint probability ˜P (x, y) ∝ e− ˜H(x, y), and to the prior probability ˜P (x) ∝ eJ′ ∑
{i,k} xixk+hf
∑
i xi .
If hf assumes a small but generic value, the magnetization 1
N
∑ N
i=1 xi and its observed value 1
N
∑ N
i=1 yi become
smoother functions of J′ that are closer to their average values; see Fig. 5 and compare it wit h Fig. 1. This is expected,
because the second-order phase-transition regime is now replace d by a smooth crossover from lower to higher values
of 1
N
∑ N
i=1 xi and 1
N
∑ N
i=1 yi; see Fig. 5. Since the situation is more stable (than for hf = 0), we can apply a smaller
amount of supervising (i.e. smaller values of ρ).
Fig. 6 shows the original and recovered images for a small but gener ic value of the external ﬁeld hf = 0 .01 and for
J′ = 0 .39. It is seen that the original image has a ﬁne-grained structure. At hf = 0 .01 such a structure is seen for J′
being roughly between 0 .37 and 0 .47. As compared to other presented examples, here we applied a sm aller amount of
supervising ρ = 0 .05 (i.e. the 5% of spins is supervised). Still this supervising is clearly us eful, especially in its active
scenario. In Fig. 6 we are still far from the mean-ﬁeld, since ˆOL > ˆOG; cf. Table III for further data.
●●●●●●●
●
●
●
●
●
●
●●●●●●●
■■■■■■■■
■
■
■
■
■■■■■■■■
○○○○○○○
○
○
○
○
○○○
○
○○
○
○○
□□□□□□
□
□
□
□
□
□□□□
□□
□□□
● 1
N ∑i xi
■ 1
N ∑i yi
○ 1
N ∑i xi
□ 1
N ∑i yi
0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65
0.0
0.2
0.4
0.6
0.8
1.0
J ′
(a)
● ● ●
●
●
●
●
●
●
● ● ● ● ●
■ ■ ■ ■
■
■
■
■
■
■ ■ ■ ■ ■
○ ○
○ ○
○
○
○
○
○
○ ○ ○ ○ ○
□ □ □ □
□
□
□
□
□
□ □ □ □ □
● 1
N ∑i xi
■ 1
N ∑i yi
○ 1
N ∑i xi
□ 1
N ∑i yi
0.1 0.2 0.3 0.4 0.5 0.6
0.0
0.2
0.4
0.6
0.8
1.0
J ′
(b)
FIG. 5: (a) The same as in Fig. 1, but with an external magnetic ﬁeld. Magnetization 1
N
∑ N
i=1 xi and its observed value
1
N
∑ N
i=1 yi versus J′ under a ﬁxed ǫ = 0 .1 in (13) (hence h = 1 .09861), and an external magnetic ﬁeld hf = 0 .01. Single
realizations of x = {xi}N
i=1 and y = {yi}N
i=1 (denotted by dotted lines) are closer to the average behavio r than in Fig. 1.
(b) The same as in (a), but for hf = 0 .1. It is seen that single realizations of x = {xi}N
i=1 and y = {yi}N
i=1 converge to the
their averages upon increasing hf .
13
original image noisy image
 without supervising
 random supervising
local supervising
 global supervising
 median filter
 Gaussian filter
FIG. 6: An example of active supervising for two scenarios (g lobal and local, as given by (36) and (37), respectively) und er
J′ = 0 .39, hf = 0 .01 (non-zero magnetic ﬁeld), ǫ = 0 .1 and ρ = 0 .05, i.e. everywhere we supervise 5% of spins. Notations and
other parameters coincide with those in Fig. 2.
VI. SUMMARY
Restoration of noise-corrupted images is a fundamental problem o f statistics, and it is also of obvious practical
importance [1–7]. It joins together mathematical and physical sta tistics, since the simplest non-trivial approach to
this problem is based on the two-dimensional, square-lattice Ising mo del with a ferromagnetic interaction between
neighbouring binary pixels (spins) [1, 3]. The prior probability of images within this model is determined via the
Gibbs distribution, where the ferromagnetic coupling constants ac count for natural smoothness (correlation) between
neighbouring spins. Hence the Ising model was extensively studied in the context of image restoration [1–7]. The
optimal approach to restoration within the Ising model is based on m aximizing the average overlap (the inverse
Bayesian risk). This approach demands calculating Gibbsian (or equilib rium) averages of all Ising spins. In the sense
of the average overlap, this approach outperforms those based on various types of ﬁltering (e.g. median ﬁltering) [4].
However, ﬁltering approaches are easier to implement in practice.
Here we studied a ferromagnetic Ising model for active restoratio n of images, where prior information on certain
(supervised) spins is requested using the initial observation of the noise-corrupted image. For a given number of
supervised spins, the optimal supervising strategy maximizes the a verage overlap of non-supervised spins with their
true (requested) values. Generally, this optimal strategy is not k nown. We found the optimal supervising strategy
within the mean-ﬁeld approximation, which is introduced via letting all s pins couple with each other. It applies
to realistic (two-dimensional) images, whenever the inter-spin coup ling is large (the image is smooth) and hence
ﬂuctuations are small. The optimal strategy in this situation amount s to supervising those pixels (spins) that do not
agree with the sign of the overall magnetization. The strategy sho ws that there is an upper limit on the number of
supervised spins, beyond of which it is meaningless to supervise, sinc e the performance will not change.
The mean-ﬁeld approximation allows to study the active supervision in detail. It also leads to strategies that can
apply more generally, i.e. in the critical regime of the two-dimensional model, where correlations are long-range and
probability distributions are self-similar [20]. The critical regime applies to natural images [19]. We explored one such
strategy, where we supervise only those spins (pixels) that disagr ee with their Bayesian estimates. The performance
of this strategy is comparable with the optimal one where the latter is known, i.e. the mean-ﬁeld applies. Otherwise
(e.g. in the critical regime), it outperforms over all other methods we tried.
In this study we assumed that hyperparamaters of the model—the smoothness parameter J of the prior probability
(ferromagnetic coupling), the bias hf (external magnetic ﬁeld) and the noise parameter h—are known. Generally,
this is not the case: hyperparameters are to be determined from d ata. This is one instance of the hyperparameter
learning for the Ising model that recently attracted much attent ion [29, 30]; see [31, 32] for reviews. In Appendix
we studied the hyperparameter learning within the mean-ﬁeld appro ximation, and saw that this problem is non-
trivial due to non-identiﬁability: if (for hf = 0 taken to be zero for simplicity) both hyperparameters J and h are
unknown, then neither of them can be found via the standard maxim um likelihood approach. The non-identiﬁability
14
problem is crucial for image restoration, since imprecisely known hyp erparameters may lead to a larger value of
the average overlap (over-conﬁdence), thereby creating a fals e impression about the performance of the restoration.
Appendix also shows that within the mean-ﬁeld approximation the non -identiﬁability problem can be resolved via the
active supervising. A pertinent open problem is how the active super vising alters hyperparameter learning for the
two-dimensional situation, especially in its critical regime.
Acknowledgement
A.E.A. acknowledges discussions with A. Galstyan. A.E.A. was support ed by SCS of Armenia (Grant No. 18RF-
015). This work is supported by National Natural Science Foundat ion of China (Grant No. 11505071), the Programme
of Introducing Talents of Discipline to Universities under Grant No. B 08033 and the Fundamental Research Funds
for the Central Universities.
Appendix: Unknown hyperparameters
1. The maximum-likelihood method for estimation of hyperpa rameters
The standard maximum-likelihood method of estimating unknown hype r-parameters is to start with the observations
y and probabilities P ◦(y) at trial values of the hyper-parameters [33]. For the mean-ﬁeld m odel studied in section
III A these trial values are J◦ and h◦, while the correct values of hyperparameters are still denoted by J and h.
Now if N ≫ 1, then ln P ◦(y) self-averages:
ln P ◦(y) ≃
∑
y
P (y) ln P ◦(y), (42)
with the true (i.e. containing the correct hyper-parameters J and h) probability P (y). The global maximum of (42)
is reached (among other possibilities) at J◦ = J and h◦ = h [33]. This fact follows from the positivity of the relative
entropy: ∑
y P (y) ln P (y)
P ◦(y) ≥ 0. However, the maximum is generally not unique, so the maximization o ver J◦ and h◦
does not need to return true values J and h. In practice the global optimization is generally intractable; hence it is
implemented via an expectation–maximization (EM) method (Baum-We lch algorithm) [33].
Now we should use (21, 22) with J, h and m replaced by respectively J◦, h◦ and m◦, where instead of (23) we get
m◦ = 1
N
∑
i
tanh(J◦m◦ + h◦yi) =
∑
y=±1
π(y) tanh(J◦m◦ + yh◦). (43)
Note that m◦ self-averages with probability π(y) containing true values of J and m.
Due to factorization (22) the maximization of (42) amounts to maxim izing
∑
y=±1
π(y) ln π◦(y), (44)
where π(y) is given by (22), and where [cf. (20)]
π◦(y) = 1
2 [1 + y tanh(J◦m◦) tanh(h◦)], (45)
Now it is clear from (22, 45) that for J > 1 1 the maximization of (44) will lead to π(1) = π◦(1), and this global
maximum is achieved for
tanh(Jm) tanh(h) = tanh( J◦m◦) tanh(h◦). (46)
1 For J < 1 we get π (1) = π (− 1) = 1 / 2. Then (43) leads to m◦ = 0. This satisﬁes (46), but there is no constraint on J◦ and on h◦, i.e.
no determination of hyperparameters is possible whatsoeve r. This is expected, because for J < 1 the initial distribution of the spins is
completely unbiased.
15
Using in (43) tanh[ a + b] = tanh[a]+tanh[b]
1+tanh[a] tanh[b] and (46) we simplify (43) as follows
m◦ = tanh[ J◦m◦]. (47)
Eq. (46) cannot determine two unknowns J◦ and h◦. If one of them is known precisely, e.g. h = h◦, the other one
will be found via (46). But if both J◦ and h◦ are unknown, neither of them will be found. This is an example of the
non-identiﬁability problem in inference [34, 35].
2. Overlap and over-conﬁdence
The overlap and magnetization still self-average under the correc t values J and h [cf. (25)]:
O◦ =
∑
y=±1
π(y) tanh[ | J◦m◦ + yh◦| ], (48)
where π(y) is given by (22), and we employed ˆξ◦
i (y) = sign[ J◦m◦ + h◦yi]. With (46) and (47) one deduces from (48):
O◦ = max ( tanh[ J◦|m◦| ], tanh[ h◦ ] ) . (49)
Recall that J◦ and h◦ hold (46). Under this constraint O ◦ in (49) can be either larger or smaller than the optimal
overlap ˆO given by (25); e.g. one can take J◦ suﬃciently large and h◦ small so as to hold (46) and get O ◦ > ˆO from
(49). Hence the overlap is a good criterion for distinguishing the opt imal solution only if the hyperparameters are
known precisely.
Once hyperparameters cannot be found precisely, there can be t wo possibilities for the overlap under trial values
O◦ [see (46, 49)] and the optimal overlap ˆO given by (25): O ◦ < ˆO and O ◦ > ˆO. The former is a fair situation, where
imprecise knowledge of hyperparameters ( J◦ ̸= J and h◦ ̸= h) brings in a reduction in the overlap. In contrast, the
latter is a dangerous situation, where imprecise knowledge leads to o ver-conﬁdence (over-ﬁtting). We feel that so far
not enough attention was devoted to the over-conﬁdence pheno menon both computationally and theoretically. One
of the advantages of the present model is that it makes the pheno menon obvious.
To avoid over-conﬁdence, one can minimize O ◦ in (49) over J◦ and over h◦ under constraint (46). Clearly, this
procedure will lead to O ◦ < ˆO. For our situation this produces:
O◦ =
√
tanh(Jm) tanh(h) ≤ ˆO, (50)
tanh(J◦m◦) = tanh( h◦) =
√
tanh(Jm) tanh(h). (51)
Eq. (51) is a reasonable setting for unknown hyperparameters. T his method works theoretically, but its algorithmic
implementation in hyperparameter learning algorithms is not yet clear .
3. Active supervising and hyper-parameter learning
We shall now study how the active supervising—which was designed so as to increase the overlap—inﬂuences on the
determination of the hyper-parameters. Instead of (42) and (4 4) we should now maximize the following (self-averaged)
expression over the trial hyper-parameters h◦ and m◦:
∑
η, y, n
P (η, y)P (n|y) ln[P ◦(η, y)P (n|y)], (52)
where we note that P (n|y) does not depend on trial hyperparameters, since it is designed on the ground of observations
y. When the supervising is absent, i.e. P (n|y) = 1 for n = (0 , ..., 0), (52) reverts to (42).
We now additionally assume that
P (n|y) =
N∏
k=1
p(nk|yk). (53)
Using (21, 22, 53) we write:
P ◦(η, y)P (n|y) =
N∏
k=1
χ(ηk, yk, nk), (54)
χ(η, y, n ) = π◦(η, y)p(n|y)δn1 + π◦(y)p(n|y)δn0, (55)
16
where δn0 is the Kronecker’s delta. Analogous formula can be written for P (η, y)P (n|y). Thus maximizing (52) over
trial hyperparameters J◦ and h◦ amounts to maximizing
∑
η,y
p(1|y) π(η, y) ln[ p(1|y)π◦(η, y) ] +
∑
y
p(0|y) π(y) ln[ p(0|y)π◦(y) ] (56)
=
∑
η,y
p(1|y) π(η, y) ln[ p(1|y) ] +
∑
η,y
p(1|y) π(η, y) ln[ π◦(η, y)/ π◦(y) ]
+
∑
η,y
p(1|y) π(η, y) ln[ π◦(y) ] +
∑
y
p(0|y) π(y) ln[ π◦(y) ] +
∑
y
p(0|y) π(y) ln[ p(0|y) ]
=
∑
η,y
p(1|y) π(η, y) ln[ π◦(η, y)/π◦(y) ] +
∑
y
π(y) ln[ π◦(y) ] (57)
+
∑
y
p(1|y) π(y) ln[ p(1|y) ] +
∑
y
p(0|y) π(y) ln[ p(0|y) ]. (58)
Thus we can maximize only (57), since (58) does not depend on trial h yperparameters. Note that the last term in
(57) is the expression (44) without supervising; it is recovered fro m (57) for p(1|y) = 0. The ﬁrst term in (57) is the
self-averaged expression for ln[ π◦(η|y)].
Inspecting directly (56) or (57) we conclude that now the maximizat ion over J◦ and h◦ will recover the true values
of the hyper-parameters: J◦ = J and h◦ = h, even in the regimes (33, 34), where the overlap is optimized. Note
that this conclusion holds even for the random supervising (31), wh ere its origin is especially clear: together with∑
y π(y) ln π◦(y) we maximize ∑
η,y π(η, y) ln[π◦(η, y)], and the later maximization leads to J◦ = J and h◦ = h, which
also maximizes the former; cf. (46).
[1] S. Geman and D. Geman, Stochastic relaxation, Gibbs distribution and the Bayesia n restoration of images , IEEE Trans.
Pattern Analysis Machine Intell. 6, 721 (1984).
[2] J. Besag, On the statistical analysis of dirty pictures , J. Roy. Sfat. Soc. B, 48, 259-302 (1986).
[3] J.M. Pryce and A.D. Bruce, Statistical mechanics of image restoration , J. Phys. A 28, 511 (1995).
[4] K. Tanaka, Statistical-mechanical approach to image processing , J. Phys. A 35, R81 (2002).
[5] E. Cohen, R. Heiman, O. Hadar, Image and video restoration via Ising-like models , in Proc. IS& T/SPIE Electronic Imaging
Conference, San-Francisco 2012.
[6] H. Nishimori and K.Y. Wong, Statistical mechanics of image restoration and error-corr ecting codes , Physical Review E,
60, 132 (1999).
[7] H. Nishimori, Statistical Physics of Spin Glasses and Information Proces sing: An Introduction (Oxford University Press,
Oxford, 2001).
[8] B. Settles. Active Learning Literature survey. Technical Report 1648, University of Wisconsin–Madison, 2 009.
[9] A. Wald, Sequential Tests of Statistical Hypotheses , The Annals of Mathematical Statistics, 16, 117-186 (1945).
[10] T. Scheﬀer, C. Decomain, and S. Wrobel. Active Hidden Markov Models for Information Extraction. In F. Hoﬀmann,
D. Hand, N. Adams, D. Fisher, and G. Guimaraes, editors, Advances in Intelligent Data Analysis , volume 2189 of Lecture
Notes in Computer Science , pages 309–318. Springer Berlin Heidelberg, 2001.
[11] B. Anderson and A. Moore, Active Learning for Hidden Mar kov Models, 22 nd International Conference on Machine
Learning, Bonn, Germany, 2005.
[12] A. E. Allahverdyan and A. Galstyan, Journal of Statisti cal Physics 161, 452-466 (2015).
[13] X. Zhu, J. Laﬀerty, and Z. Ghahramani. Combining Active Learning and Semi-Supervised Learning Us ing Gaussian Fields
and Harmonic Functions . In ICML 2003 workshop on The Continuum from Labeled to Unlabele d Data in Machine Learning
and Data Mining , pages 58–65, 2003.
[14] M. Bilgic and L. Getoor. Reﬂect and correct: A misclassiﬁcation prediction approac h to active inference. ACM Trans.
Knowl. Discov. Data , 3(4):20:1–20:32, Dec. 2009.
[15] M. Bilgic, L. Mihalkova, and L. Getoor. Active Learning for Networked Data. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10) , 2010.
[16] C. Moore, X. Yan, Y. Zhu, J.-B. Rouquier, and T. Lane. Active Learning for Node Classiﬁcation in Assortative and
Disassortative Networks. In Proceedings of the 17th ACM SIGKDD International Conferenc e on Knowledge Discovery and
Data Mining , KDD ’11, pages 841–849, New York, NY, USA, 2011. ACM.
[17] A. Krause and C. Guestrin. Near-optimal Nonmyopic Value of Information in Graphical M odels. In 21st International
Conference on Uncertainty in Artiﬁcial Intelligence (UAI) , page 5, 2005.
[18] A. Krause and C. Guestrin. Optimal Value of Information in Graphical Models . Journal of Artiﬁcial Intelligence Research ,
35:557–591, 2009.
[19] D.L. Ruderman, The statistics of natural images , Network: computation in neural systems, 5, 517 (1994).
17
[20] G. J. Stephens, T. Mora, G. Tkacik, and W. Bialek, Statistical Thermodynamics of Natural Images , Phys. Rev. Lett. 110,
018701 (2013).
[21] J. Zhang, The mean ﬁeld theory in EM procedures for Markov random ﬁelds , IEEE Transactions on Signal Processing, 40,
2570-2583 (1992).
[22] G. Celeux, F. Forbes, N. Peyrard, EM procedures using mean ﬁeld-like approximations for Mark ov model-based image
segmentation, Pattern Recognition, 36, 131-144 (2003).
[23] J. Inoue and K. Tanaka, Mean ﬁeld theory of EM algorithm for Bayesian grey scale imag e restoration, Journal of Physics
A: Mathematical and General, 36, 10997-11010 (2003).
[24] D. Geiger and F. Girosi, Mean ﬁeld theory for surface reconstruction , IEEE Transactions on Pattern Analysis and Machine
Intelligence, 13, 401412 (1991).
[25] J. Marroquin, S. Mitter, and T. Poggio, Probabilistic solution of ill posed problem in computer vis ion, Journal of the
American Statistical Association, 82, 7689 (1987).
[26] M. Opper, D. Saad (eds), Advanced Mean Field Methods: Theory and Practice , (MIT Press, 2001).
[27] C.P. Robert, The Bayesian choice (Springer Science + Business, NY, 2007).
[28] Library of image-processing packages: http://www.sc ipy-lectures.org/packages/scikit-image/
[29] H. J. Kappen and F. Rodriguez, Eﬃcient Learning in Boltzmann Machines Using Linear Respon se Theory , Neural Com-
putation 10, 1137 (1998).
[30] S. Cocco and R. Monasson, Adaptive cluster expansion for inferring Boltzmann machin es with noisy data Physical Review
Letters, 106, 090601 (2011).
E. Aurell and M. Ekeberg, Inverse Ising inference using all the data , Physical Review Letters, 108, 090201(2012).
H.C. Nguyen and J. Berg, Mean-ﬁeld theory for the inverse Ising problem at low temper atures, Physical Review Letters,
109, 050602 (2012).
A. Decelle and F. Ricci-Tersenghi, Pseudolikelihood decimation algorithm improving the infe rence of the interaction network
in a general class of ising models , Physical Review Letters, 112, 070603 (2014).
M. Castellana and W. Bialek, Inverse spin glass and related maximum entropy problems , Physical Review Letters, 113,
117204 (2014).
[31] Y. Roudi, E. Aurell, and J. A. Hertz, Statistical physics of pairwise probability models , Frontiers in computational neuro-
science 3 (2009).
[32] H. C. Nguyen, R. Zecchina, and J. Berg, Inverse statistical problems: from the inverse Ising probl em to data science ,
Advances in Physics, 66, 197-261 (2017).
[33] Y. Ephraim and N. Merhav, Hidden Markov processes , IEEE Trans. Inf. Th., 48, 1518-1569, (2002).
[34] D. Blackwell and L. Koopmans, On the identiﬁability problem for functions of ﬁnite Markov chains, Ann. Math. Statist.
28, 1011 (1957).
[35] H. Ito, S. Amari, and K. Kobayashi, Identiﬁability of Hidden Markov Information Sources , IEEE Trans. Inf. Th., 38, 324
(1992).
[36] G. R. Arce, Nonlinear Signal Processing: A Statistical Approach (Wiley, New Jersey, 2005).
[37] L.D. Landau and E.M. Lifshitz, Statistical Physics, I , (Pergamon Press Oxford, 1978).