8102
peS
22
]hcem-tats.tam-dnoc[
1v60480.9081:viXra
Active image restoration
Rongrong Xie1,∗, Shengfeng Deng1,∗, Weibing Deng1,† and Armen E. Allahverdyan2,†
1Key Laboratory of Quark and Lepton Physics (MOE) and Institute of Particle Physics,
Central China Normal University, Wuhan 430079, China
2Yerevan Physics Institute, Alikhanian Brothers Street 2, Yerevan 375036, Armenia
We study active restoration of noise-corrupted images generated via the Gibbs probability of an
Ising ferromagnet in external magnetic field. Ferromagnetism accounts for the prior expectation
of data smoothness, i.e. a positive correlation between neighbouring pixels (Ising spins), while the
magneticfieldreferstothebias. Therestorationisactivelysupervisedbyrequestingthetruevalues
ofcertainpixelsafteranoisyobservation. Thisadditionalinformationimprovesrestorationofother
pixels. Theoptimalstrategyofactiveinferenceisnotknownforrealistic(two-dimensional) images.
We determine this strategy for the mean-field version of the model and show that it amounts to
supervisingthevaluesofspins(pixels)thatdonotagreewiththesignoftheaveragemagnetization.
Thestrategy leads toa transparent analytical expression for the minimal Bayesian risk, and shows
that there is a maximal number of pixels beyond of which the supervision is useless. We show
numerically that this strategy applies for two-dimensional images away from the critical regime.
Within this regime the strategy is outperformed by its local (adaptive) version, which supervises
pixelsthatdonotagreewiththeirBayesianestimate. Weshowontransparentexampleshowactive
supervisingcanbeessential inrecoveringnoise-corrupted images andadvocateforawiderusage of
active methodsin image restoration.
I. INTRODUCTION
Many inference problems in machine learning amount to restoration of a hidden structure based on noisy observa-
tions. They are similar (sometimes isomorphic) to models of equilibrium statistical physics, where the role of noise
is played by frozen disorder. In particular, the problem of noise-corrupted image restoration can be mapped to the
two-dimensionalIsingferromagnet[1–7]. TheGibbs probabilityofthis modelservesasapriorprobabilityforimages.
Spins (two-value variables equal to 1) of the Ising model refer to the black-white pixels of a digital image. The
±
ferromagnetic feature of the model means that neighbouring pixels are correlated, a legitimate minimal assumption
on the prior probability.
Methods of solving inference problems cannot perform without prior information. Since its amount is limited, it is
naturaltostudyinformedstrategiesforrequestingpriorinformation. Inparticular,priorinformationcanberequested
on the ground of the previous functioning of the inference method. This notion of active inference [8] emerged in
statistics [9] and has been applied to various inference problems including Hidden Markov Models [10–12], network
inference [13–16], economics, experiment optimization etc [8]. In the context of noisy image restoration, the concept
of active inference assumes that there is a costly noiseless channel by which the correct values for some of pixels can
be communicated directly to the image restorator. In practice,this noiselesschannel may mean an additionalprecise
(but costly) measurement of pixels. Since the channels is costly, only a small fraction (say 5%) of original pixels
is communicated with a hope that—if employed properly—they can lead to a sizable improvement in restoration of
other(noisy)pixels. The subjectofactiveinference hastworelatedissues: whichpriorinformationisto be requested
and to what extent this information improves the method performance [8]. The first question refers to the meaning
of (prior) information, the second one to its value.
Most existing theoretical results on active inference in graphical models are concerned with sub-modular cost
functions that allow to establish certain optimality guarantees [17, 18]. The sub-modularity assumption does not
hold in several interesting situations, e.g. for systems with long-range correlations and scale-invariance, a range of
phenomena that is collectivelydesignatedas criticalbehavior. The clearestexample ofsucha behavioris the second-
orderphasetransitionsinstatisticalmechanics[7]. Itwasfoundthatnaturalimages(e.g. naturescenes)areorganized
in a scale-invariant way [19] and do have long-range correlations [20]. Statistical mechanics (e.g. the Ising model)
is the most appropriate way of describing such images [20]. Generally, the optimal strategy of active inference—i.e.
which variables are to be supervised given a noisy observation—is not known beyond a direct enumeration [18].
Here we shall study an active recognition (estimation or filtering) of a noise-corrupt image, which was generated
via the two-dimensional Ising ferromagnet. The model does allow for a critical regime depending on the value of the
∗ Theseauthorscontributed equallytothiswork.
† Authorstowhomanycorrespondenceshouldbeaddressed: wdeng@mail.ccnu.edu.cnandarmen.allahverdyan@gmail.com
2
ferromagnetic coupling constant, which is taken as the known hyperparameter of the model. For a large number of
pixels,thiscriticalregimeisrealizedviaasecond-orderphase-transition,withmagnetizationbeinganorderparameter.
We analytically determined the optimal active supervising strategy of the Ising model in the mean-field limit.
This mean-field limit of the Ising model was already studied for a Gaussian noise image-restoration problem [6,
7]. More generally, mean-field methods are widely applied in statistical inference problems [21–25]; see [26] for a
book presentation. The inferred image is determined by minimizing the Bayesian risk [27]. The optimal mean-field
strategy amounts to supervising only those pixels whose observed (noisy) value does not agree with the sign of the
magnetization. Italsoshowsthatthereisamaximalnumberofsupervisedvariables,beyondofwhichthesupervising
is not meaningful. Without active supervising, the optimal inference of the mean-field model shows only two (in a
sense trivial) regimes, viz. observation-dominated (where the prior information is not needed) and prior-dominated
(where observations are redundant). With active supervising, there is a non-trivial coupling between observations
and the prior knowledge, i.e. no separation into two different regimes is possible.
The mean-field solution of the actively supervised model is transparent and guides the understanding of a more
complex, two-dimensional situation which is studied numerically. We saw that the mean-field optimal active super-
vising strategy performs well in the two-dimensional case, if the model is away from the critical regime. Otherwise,
we founda strategythatis superiorto the mean-fieldoptimalone. This strategyamountsto comparingthe observed
(noisy)value ofpixelwith its Bayesianestimate: only thosepixels aresupervisedforwhich these valuesdisagree. We
show on concrete examples how usual (not active) methods fail to restore noise-corrupted images, while the active
supervising does lead to a satisfactory restoration.
This text is organized as follows. Section II recalls the general theory of optimal Bayesian inference with and
without supervising. Here we also recall how this theory can be represented via Ising models. Next section finds out
the optimal active supervising strategy in the mean-field version of the Ising model. Section IV studies numerically
the two-dimensional Ising model. It shows that several results deduced in the mean-field situation also apply in this
more realistic case. Section IV also works out a local version of that optimal supervising strategy and shows that
it yields better results in the critical regime of the model, where the mean-field method does not apply. Section V
studies the Ising model, where the prior probability has a small bias (in the value of spins) given by a weak external
magnetic field. Note that sections IV and V (numerical results on a square Ising lattice) can be read independently
from III (analytic results in the mean-field limit).
We summarize in the last section. In preparationfor future research,Appendix studies hyperparameterestimation
of the mean-field Ising model. The analysis here is deeper than normally done in literature, because we study the
influence of non-identifiability on the optimal restoration.
II. GENERAL FORMULATION OF IMAGE RESTORATION AND REPRESENTATION VIA ISING
MODELS
A. Without supervising
HerewerecallgeneralideasoftheBayesianinferencefortwo-valuedrandomvariableswithandwithoutsupervising.
Next sections will specify them for the image restorationproblem.
Let there are two dependent vector random variables X = (X ,...,X ) and Y = (Y ,...,Y ). Now X is called
1 N 1 N
the hidden variable, since it cannot be observed directly. Instead, we assume that a concrete value y = (y ,...,y )
1 N
of Y is observed. On the ground of y we want to find a representative value (an estimate) ξ(y) of x = (x ,...,x )
1 N
that is likely to be the source of y. We shall assume that y = 1, x = 1 and call them spins (pixels). The
k k
± ±
joint probability is P(X,Y); e.g. the conditional probability P(YX) describes a noisy channel, where Y is the
|
noise-corrupted observation of X.
The quality of the estimate ξ(y) is given by the average Hamming distance (or overlap):
N
1
O(y;ξ)= ξ (y)x P(xy), (1)
i i
N |
i=1 x
XX
sothatalargerO(y;ξ)meansabetterrestoration. WestressthattheaveragingoverP(xy)in(1)isdone“byhands”:
|
since the hidden variables are not known, one generates them via P(xy) (for fixed observations) and then takes the
|
average. Eq. (1) coincides with the inverse Bayesian risk [27]. The latter is minimized, while (1) is maximized over
ξ (y).
i
In applications one also frequently studies the average of O(y) over P(y). This averaging can also be done by
hands, i.e. oversufficiently manyapplications,but it is importantto stress thatif N is sufficiently large(andY is an
3
ergodicprocess),O(y;ξ)doesself-average,i.e. accordingtothelawoflargenumbersthe sum(1)overalargenumber
N 1 is replaced by its average O(y;ξ) P(y)O(y;ξ).
≫ ≃ y
Maximizing (1) over ξ(y) leads to the optimal method (i = 1,...,N) with the largest overlap Oˆ(y) for given
P
observations y:
ξˆ(y)=sign x P(xy) =sign x P(x y) , (2)
i i i i
" | # " | #
X
x xiX =±1
N
1
Oˆ(y)= x P(xy) . (3)
i
N (cid:12) | (cid:12)
X i=1(cid:12)X x (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Eqs. (3) can serve, e.g. for evaluating the max(cid:12)imum-likeliho(cid:12)od method (ML). Recall that the ML does not employ
the prior probability P(x), and is based on maximizing P(yx) for given observations y:
|
ξML(y)=arg max [P(yx)]. (4)
x |
Ifthe priorprobabilityP(x) is available,ξML(y) is suboptimalfromthe viewpointof(2), andone candetermine how
far its prediction for the overlap is from the optimal value O(y) given by (3).
For completeness we mention an equivalent approach to introducing the overlap (1). Let us temporarily assume
thatweknowtheoriginalrealizationxoftherandomvariableX. Onceitpassesthroughthenoisychannel,weobtain
y = ǫx, where ǫ is the noise variable and ǫx means elemetwise multiplication of two vectors (Hadamard product).
Now the overlap can be defined as
N
1
Q= x ξˆ(ǫx). (5)
i i
N
i=1
X
The joint distribution of x and ǫ is P (x,ǫx)=P(x,ǫx).
XY
To make Q independent from the concrete choice of x and ǫ we look at the average
N
1
Q= P(x,ǫx)x ξˆ(ǫx)= P(y)Oˆ(y), (6)
i i
N
i=1 x,ǫ y
XX X
which brings us back to the averaged(3). However, there is no direct relation between O(y) and Q.
B. With supervising
Givenobservationsyonerequestsanadditionalinformationoncertainx (supervising)soastoimprovethequality
i
of restorationfor the remaining variables. This is described by the vector
n=(n ,...,n ), n =0,1, (7)
1 N i
where n =1 (n =0) means that the true value of the corresponding x is requested (not requested). Naturally, the
i i i
number of supervised spins is fixed (i.e. not all spins are supervised)
N
n =ρN. (8)
i
i=1
X
ThesupervisingstrategyisdescribedbyconditionalprobabilityP(ny), i.e. the supervisingisgenerallyprobabilistic.
Let us divide x into supervised (x′′) and not supervised (x′) parts. | x′ and x′′ depend on n, but this dependence is
not indicated explicitly to avoid excessive notations. E.g. if x=(x ,x ,x ) and n=(1,0,0), then x′ =(x ,x ) and
1 2 3 2 3
x′′ = (x ). Let P(x′,y) and P(x′′,y) be (respectively) the joint probability of non-supervised and supervised spins
1
and observations. These probabilities are found from P(x,y).
Let the values of x′′ were requested and send via a noiseless channel (we explain below the meaning of this
assumption). They appeared to be x′′ = η. Together with y and n, also η is by now given. Hence the distribution
P(x′ η,y) of the non-supervised spins is conditioned both by observations y and by supervised variables η.
|
4
We estimate non-supervised spins as ξ′(η,y,n). The corresponding overlapreads:
N
1
O(η,y,n;ξ′)= (1 n ) ξ′(η,y,n)x′ P(x′ η,y), (9)
N(1 ρ) − k k k |
− k=1 x′
X X
where the summation is taken over non-supervised spins only. For a given n, the optimal overlap is calculated as in
(2, 3):
ξˆ′(η,y,n) = sign x′P(x′ η,y) =sign x′P(x′ η,y) , (10)
k " k | #  k k| 
x′ x′=±1
X kX
 
N
1
Oˆ(η,y,n) = (1 n ) x′P(x′ η,y) . (11)
N(1 ρ) − k (cid:12) k k| (cid:12)
− X k=1 (cid:12) (cid:12) x′ kX=±1 (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Now if y, n and η are sufficiently long, Oˆ(η,y,n) will self-av(cid:12)erageover P(η,y,n(cid:12))=P(y)P(η y)P(ny).
| |
The major problem of supervising is to find the best P(ny), which holds (8) and provides the largest average
|
overlap (11). For understanding this problem one should rely on models, because there is no general solution for it
under N 1 (for a small value of N one can proceed with straightforwardcalculations).
≫
Let us now explainthe practicalmeaning of the aboveassumptiononthe existence of a noiseless channel. We note
that frequently such channels do exist (e.g. because they relate to a sufficiently precise equipment), but they are
costly, i.e. the cost per pixel (or spin) for using such a channel is high. In that case it is useful to employ the noisy
channel for N pixels, but still to use the noiseless channel for ρN actively supervised pixels, where ρ 1.
≪
C. Ising model
Let us now assume that the prior probability of hidden variables x=(x ,...,x ) is given as [1–5, 7]
1 N
P(x) eJ′P {i,k} xixk, x i = 1, (12)
∝ ±
where J′ >0 is a constant hyperparameter (coupling constant), and J′ x x is the Hamiltonian of the Ising
− {i,k} i k
ferromagnet, where i,k means summation over neighbours of a lattice. In the context of the image recognition
{ } P
problemthis is a squarelattice. Here x = 1 referto black-whitepixels ofthe originalimage,andthe ferromagnetic
i
±
coupling J′ >0 means that there is a prior information on positive pixel-pixel correlations. This is just the standard
smoothness assumption.
We assume that the observed variables y = (y ,...,y ) (noise-corrupted image) relate to x via a symmetric and
1 N
independent noise
N N
P(yx)= p(y i x i )=(2cosh[h])−N ehP i xiyi, (13)
| |
i=1 i=1
Y Y
1 1 ǫ
h ln − 0, (14)
≡ 2 ǫ ≥
where h relates to the error probability 0<ǫ<1/2, and where we recall that y = 1.
i
±
Combining (12) and (13) we get for the joint probability P(x,y) of hidden variables x and observations y
P(x,y) e−H(x,y), H(x,y)= J′ x x h x y , (15)
i k i i
∝ − −
{Xi,k} X i
Now P(x,y) refers to the Gibbs distribution of a statistical system at temperature T = 1 and with Hamiltonian
H(x,y). We shallassume that J and h are knownhyperparameters. More generally,they arenot known, but should
be inferred from data; see [32] for a recent review. Standard methods for doing that are discussed in Appendix
together with their limitations.
5
III. MEAN-FIELD ANALYSIS
A. The fully coupled (mean-field) Ising model
To get from (15) a solvable model, we assume in (12, 15) that all x couple with each other:
i
J
H(x,y)= x x h x y , (16)
i k i i
−2N −
i6=k i
X X
wherewealsoassumedJ′ =J/(2N)toshowthatwehavetohaveH(x,y)= (N)forN . Thefirstsumin(16)
O →∞
goes through all i,j = 1,...,N under i = j. Eq. (16) refers to the mean-field-interaction version of the random-field
6
Ising model. It is known to be solvable [6, 7].
Eqs. (15, 16) lead to
N3/2 NJµ2
P(x,y) e2 J N P i6=k xixk+hP i xiyi = dµexp + x i (Jµ+hy i ) , (17)
∝ √2πJ "− 2 #
Z i
X
N3/2 NJµ2
P(y) e2 J N P i6=k xixk+hP i xiyi = dµexp + lncosh(Jµ+hy i ) . (18)
∝ √2πJ "− 2 #
x Z i
X X
For N 1 the latter integral is taken by the saddle-point method:
≫
NJm2
P(y) exp + lncosh(Jm+hy ) , (19)
i
≃ "− 2 #
i
X
where the magnetization m is determined from the saddle-point equation as
1
m= tanh(Jm+hy ). (20)
i
N
i
X
Now formally m is a function ofy, but for this model(and in the limit N 1) it self-averagesand becomes (almost)
≫
independent from y. This known fact can be confirmed via calculating correlation functions between (x ,y ) and
i i
(x ,y ). Thus we return to (17) employ there the saddle-point method, use (20) and end up with
j j
eJmx+hxy
P(x,y) π(x ,y ), π(x,y)= , (21)
i i
≃ 2cosh[Jm+h]+2cosh[Jm h]
i −
Y
cosh(Jm+hy) 1
P(y) π(y ), π(y)= = [1+ytanh(Jm)tanh(h)], (22)
i
≃ cosh[Jm+h]+cosh[Jm h] 2
i −
Y
where m is to be determined from (20) via self-averaging:
m= π(y)tanh(Jm+hy)=tanh(Jm). (23)
y=±1
X
One can verify from P(x) obtained via (17) that m coincides with the average collective spin of the original image:
m= 1 P(x) x .
N x i i
For J < 1, (23) predicts m = 0. For J > 1, (23) predicts two solutions with m >0 and m < 0. They refer to two
P P
different ergodic components. Thus we have a second-order phase transition at J = 1. It belongs to the mean-field
universality class.
Thus (2, 21, 22) imply for the optimal situation
ξˆ(y)=sign[Jm+hy ], i=1,...,N, (24)
i i
and the self-averagedoptimal overlap reads from (3)
Oˆ = π(y)tanh(Jm+hy )=max[tanh(J m),tanh(h)]. (25)
| | | |
y=±1
X
6
Note that tanh(h) is the overlap of the maximum-likelihood method; see (4, 13). This is confirmed by taking
ξML =sign[y ] and using it together with (22) in
i i
OML = π(y)sign[y]tanh(Jm+hy)=tanh(h). (26)
y=±1
X
Hence the message of (24, 25) is that there are only two extreme situations: for h > J m—which means a
| |
weak noise according to (14)—the prior information is irrelevant, since observations are reliable. Hence the optimal
estimation method coincides with the maximum-likelihood; see (4, 13). For h < J m (strong noise, as seen from
| |
(14)) observations are irrelevant, since the estimate (24) depends only on the parameter J of the prior P(x). Put
differently, either the prior information (given by J in P(x)) is irrelevant, or observations are irrelevant.
B. Supervising
1. The optimal overlap after supervising
WerecallfromsectionIIBthatsupervisingisdescribedbytheconditionalprobabilityP(x′ η,y)ofnon-supervised
|
spins given the values of supervised spins η and observations y, as well as by the conditional probability P(ny) of
|
the coordinates n of supervised spins. Now (21, 22) show that for the present model the probabilities P(x,y) and
P(y) factorize. This implies that P(x′ η,y) does not depend on η. Using again (21, 22) we get from (11):
|
N
1
Oˆ(η,y,n)= (1 n )tanh[Jm+hy ], (27)
k k
N(1 ρ) − | |
− k=1
X
where m is determined from (23). Hence the average of (27) over P(ny)P(y) reads
|
1
Oˆ = π(y)p(0y)tanh[Jm+hy ], (28)
1 ρ | | |
− y=±1
X
where π(y) is found from (22), and where we assumed that P(ny) (the probability of n, giventhe observations y) is
|
symmetric in the sense that all (y ,n ) do have the same marginal-conditional probability: p(n y ). The constraint
i i i i
|
(8) will be implemented in average. Hence p(ny) holds two constraints:
|
1=p(1y)+p(0y), (29)
| |
1 ρ= π(y)p(0y). (30)
− |
y=±1
X
The meaning of (28)is intuitively clear: the optimalsupervising amounts to changing—fromπ(y) to π(y)p(0y)—the
|
(effective) distribution of observations,where p(0y)is the probability of not supervising a given spin. The same idea
|
can be implemented as an anzatz for more general models, but there it does not have to be optimal.
2. Random supervising provides no advantage
Let us first consider the random supervising, where p(ny) does not depend on y. This implies from (30):
|
p(0)=1 ρ, p(1)=ρ. (31)
−
It should be clear that we get from (28) the same expression (25) as without any supervising.
3. Active supervising
We turn to the active supervising and maximize Oˆ given by (28) over p(0y) under constraints (29, 30). To
|
understand the idea of maximization, assume m > 0. Then Jm+h = Jm+h > Jm h, and (28) maximizes
| | | − |
7
upon taking p(01) such that π(1)p(01) is maximally large and compatible with (29), and after that taking p(0 1)
| | |−
as large as constraints (29, 30) still allow. The general solution is written via introducing
1
ζ = [1+tanh(J m)tanh(h)], (32)
2 | |
which is equal to π(1) if m>0. Then the probabilities maximizing (28) read
1 ρ ρ
for m>0 p(01)=min − ,1 , p(0 1)=max 0,1 , (33)
| ζ |− − 1 ζ
(cid:20) (cid:21) (cid:20) − (cid:21)
1 ρ ρ
for m<0 p(0 1)=min − ,1 , p(01)=max 0,1 . (34)
|− ζ | − 1 ζ
(cid:20) (cid:21) (cid:20) − (cid:21)
Note that in (33) the first (second) argument of min is selected together with the first (second) argument of max.
The same holds in (34). The optimal Oˆ reads from (33, 34):
ζtanh[J m +h]+(1 ρ ζ)tanh[ J m h ]
Oˆ =min | | − − | | |− | , tanh[J m +h] . (35)
1 ρ | |
(cid:20) − (cid:21)
When ρ starts to increase fromρ=0, (35)monotonously increasesfromits ρ=0 value givenby (25) till its maximal
value tanh[J m +h]. Accordingtothe non-supervisedmaximaloverlap(25),forJ >1,but Jm<h (i.e. the noiseis
| |
weak,as seenfrom(14))the prioris notrelevant. This is improvedafter supervising,nowthe prioris alwaysrelevant
providedthatJ >1. Itis notmeaningful to supervisefor 1 ρ<ζ, since havingreachedtanh[J m +h],the overlap
there does not anymore increase with increasing ρ; see (35) − . Hence the values of p(01) = 1−ρ a | nd | p(0 1) = 0 in
| ζ |−
(33)—as well as p(0 1) = 1−ρ and p(01) = 0 in (34)—are redundant. Then (33, 34) show that observations that
|− ζ |
agree with the sign of the averagemagnetization m should not be supervised: p(1sign[m])=0.
|
So far we assumed that the hyperparameters J and h in (respectively) P(x) and P(yx) are known precisely.
|
Generally, this is not the case, and hyperparameters themselves are to be found from data; see [32] for a recent
review. Appendix discusses this problem for the considered model.
IV. NUMERICAL RESULTS FOR A SQUARE LATTICE
A. Two methods for active supervising
The above results concerned the mean-field model, i.e. an unrealistic situation if we take into account that real
images are two-dimensional. Hence we turn to studying numerically the Ising model (15) on a square lattice with
periodic boundary conditions. We start from (9) and (15), and study two different strategies of active supervising.
In the first (global) strategy we supervise Nρ spins selecting them via the following criterion: given the observation
vector y, the Nρ supervised spins η are chosen randomly among those that hold
i
N
1
y sign y = 1, (36)
i i
" N # −
i=1
X
i.e. those spins are supervised which do not agree with the sign of the collective value 1 N y . Thus variables
N i=1 i
n in (7) are determined via (36). This strategy is clearly inspired by the above mean-field solution (33, 34), where
1 N y is the observed magnetization; see Fig. 1 for the value of 1 N y averaged ove P r many samples, as well
N i=1 i N i=1 i
as a single-sample form of it. For ǫ<1/2 (which we assume to be the case from (13)) and a sufficiently large N, we
get P that 1 N x and 1 N y have the same sign. Note that strate P gy (36) is easy to implement.
N i=1 i N i=1 i
Within the second (local) strategy we randomly select Nρ supervised spins among those that hold
P P
y ξˆ(y)= 1, (37)
i i
−
i.e. now one first calculates the optimal estimate ξˆ(y) according to (2, 15) and then supervises those spins that do
i
not agree with observations. This strategy did not show up within the optimal solution for the mean-field situation.
We choose it with an expectation that can work, where the mean-field method does not apply, i.e. fluctuations are
essential.
8
1.0
●○ ●○ ●○ ●○ ●○
●○
●○
0.8 ●○ ■□ ■□ ■□ ■ □ ■□ ■□
■□
0.6 ■□
0.4
●
■ ● N 1∑ i x i ○ N 1∑ i x i
0.2
● ■ ■ N 1∑ i y i □ N 1∑ i y i
0.0
●○ ■□ ●○■□ ●
○
■
□
○□
○□
0.40 0.45 0.50 0.55 0.60 0.65
′
J
FIG.1: Magnetization N 1 PN i=1 x ianditsobservedvalue N 1 PN i=1 y i versusJ′ underafixedǫ=0.1in(13)(henceh=1.09861).
Foreach J′ single realizations ofx={x i}N i=1 andy={y i}N i=1 weregenerated accordingto(12,13) (i.e. theexternalmagnetic
field is zero). We also plotted the averaged values N 1 PN i=1 x i and N 1 PN i=1 y i under the same ǫ = 0.1. For each fixed J′
the averaging was taken over 1000 realizations of random variables x generated according to (12, 13) and selected such that
N 1 PN i=1 x i > 0 for all samples included in the averaging. It is seen that N 1 PN i=1 x i and N 1 PN i=1 y i increase sharply in the
vicinity of J′ =0.45 indicating that in the limit N ≫ 1 the system has a second order phase-transition; cf. the discussion in
thebeginning of section IVB.
Naturally, (36, 37) are to be compared with the random supervising, where Nρ supervised spins η are chosen
i
completely randomly, i.e. without any dependence on y. Note that besides (36, 37) we also studied several other
supervising strategies, e.g. when 1 in the right-hand-side of (36) or (37) is changed to +1. We shall not discuss
−
such strategies, since they proved to be sub-optimal; frequently they are worse than the random supervising, and
sometimes they are worse than having no supervising at all.
B. Results
1. Generation of images
With a given coupling J′, we generated images x by Monte Carlo simulation (Metropolis algorithm) from the
two-dimensionalIsing model (12) on a squarelattice with periodic boundary conditions. The overallnumber ofspins
isN =6400spins. Giventhe noiseprobabilityǫ(andhence h)from(13,14),weflipeachspinofthe originalimagex
generatingthe noisyimagey. Theconditionalaveragesin(3,10,11)arecalculatedbyaveragingover2 103 samples
×
(we did check that this number suffices and the averagessaturate).
2. Three regimes of the square Ising ferromagnet
Recallthatthesquare(two-dimensional)Isingmodel(12)hasthreeregimesdependingontheferromagneticcoupling
constantJ′[37];seeFig.1. ForasmallvaluesofJ′thesystemisintheparamagneticstate,whereeachspinx isequally
i
likely to assume values +1 or 1. In the vicinity of a certain criticalvalue J′ we enter into the critical regime, where
− c
the paramagnetic state gets unstable and there are long-rangefluctuations (hence correlations)[37]. It is well-known
that the infinite square lattice has a second-order phase-transition point at J′ = 1ln(1+√2) 0.440687 [37]. This
is close to the value J′ =0.45 observedin our numerics done on a finite lattic c e; se 2 e Fig. 1. Ano ≈ ther indication of the
c
critical regime is that the averagedmagnetization 1 N x does not coincide with the single-sample magnetization
N i=1 i
1 N x , as shown by Fig. 1. This is a sign of strong-fluctuations.
N i=1 i P
The third regime is set for a sufficiently large J′. Here fluctuations are relatively small. Indeed, Fig. 1 shows that
for P J′ > 0.46 we get that a single-sample behavior coincides with the averaged one. The symmetry x x of
i i
→ −
the Hamiltonian (12) is spontaneously broken (this process starts from the critical regime). Hence within each given
sample 1 N x has a definite sign [37].
N i=1 i
Images generated in the third regime show pixels of one color on the backgroundof another color; see Fig. 2. Such
P
figures are interesting also because they are very susceptible to noise, and cannot be recovered by standard (i.e. not
9
TABLE I: Oˆ is the optimal overlap calculated via (3) without supervising for a square lattice with N = 6400 spins and Nρ
supervised spins. Oˆ R refers to the random supervising. Oˆ G and Oˆ L refer respectively to global and local strategies according
to (36) and (37); cf. (9). Oˆ MF and Oˆ GF are the overlaps obtained for (respectively) median filter and Gaussian filter. abBoth
filterswereappliedwiththe(minimal) radius1,i.e. viaembeddingeachpixelintoa3×3box[36]. Foreachsetofparameters
we underlinethe maximal overlap.
Parameters Oˆ Oˆ R Oˆ G Oˆ L Oˆ MF Oˆ GF
J′ =0.500, ǫ=0.3, ρ=0.1 0.90679 0.90727 0.92012 0.91850 0.90984 0.91563
J′ =0.555, ǫ=0.1, ρ=0.1 0.96341 0.96393 0.99213 0.99166 0.96313 0.96313
TABLEII:ThesamequantitiesasinTableI,butintheregime,whereOˆ L >Oˆ G andOˆ L >Oˆ R. NoteaswellthatOˆ MF <Oˆ GF.
Parameters Oˆ Oˆ R Oˆ G Oˆ L Oˆ MF Oˆ GF
J′ =0.455, ǫ=0.1, ρ=0.09 0.88363 0.88616 0.92278 0.93314 0.83484 0.87094
J′ =0.417, ǫ=0.3, ρ=0.1 0.61359 0.64128 0.64830 0.66058 0.51936 0.53594
J′ =0.417, ǫ=0.3, ρ=0.2 0.61265 0.687619 0.682026 0.72048 0.51938 0.53594
active) methods; see Fig. 2. In contrast, images generated in the critical regime do show an interesting fine-grained
structure. To some extent they can be recoveredvia standard methods, though the active supervising still leads to a
serious improvement; see Figs. 3 and 4.
3. Images and overlaps
The performance measure of strategies (36, 37) is checked via overlaps (3, 11). Overlap close to one is a necessary
condition for a good restoration. For definiteness, we compared the performance (i.e. the overlap) of the present
non-supervised method with those of several standard filters—e.g. the median filter or the Gaussian filter—that are
employed in the image-recognition; see e.g. [4]. Numerical packages for implementing these filters were taken from
[28]. In agreement with the fact that the considered method optimizes the overlap (3), we found that these filtering
methods produce a smaller overlapthan (3).
We note that in the present situation we have a possibility to look at additional performance measures: since we
generatethedataourselves—i.e. wehavetheoriginalimagex—wecanemploydirectlyananalogueof(5)thatchecks
the restored image (after supervising) with the original image x:
N
1
(1 n )ξˆ′(η,y)x , (38)
N(1 ρ) − i i i
− i=1
X
where ξ′(η,y) is defined in (10). Note that since we do not average over η and y, the overlaps (11) and (38) are
i
generallynotequalforaconsideredfinite valueofN =6400. We confirmedthat(11)and(38)arenotpreciselyequal
to each other, though they are normally quite close, since N = 6400 is still sufficiently large, and the self-averaging
applies approximately. In allrelevantsituations, if a strategyhas a superiorperformance accordingto (11), then it is
also superior according to (38). We also stress that all the results on differences between the strategies were checked
against varying the initial image x.
We identified two regimes in the behaviour of Oˆ and Oˆ , i.e. the overlaps (11) within the local and global
L G
strategies,respectively. In both regimes the maximal (better) of them is largerthan the overlapOˆ obtained via the
R
random supervising at the same number Nρ of supervised spins [see Tables I and II]:
max Oˆ , Oˆ >Oˆ . (39)
L G R
h i
Table I showsthe firstregime,whereJ′ issufficiently large,sothat the systemisawayofthe criticalregime,which
realized for J′ 0.45; see Fig. 1. In these regime the strategy (36) is dominant over (37), though their predictions
≈
10
are close to each other:
Oˆ >Oˆ , Oˆ Oˆ . (40)
G L G L
≈
Altogether, for J′ 0.45 the situation is close to the mean-field regime. This is additionally confirmed by the fact
that the random su ≥ pervising does not provide any substantial improvement: Oˆ Oˆ; see Table I and Fig. 2, and
R
≈
compare these with the discussion around (31). Fig. 2 describes regime (40) and shows that without supervising
the real image restoration is absent, despite of the fact that the overlap for this restoration regime is sizable; see
Table I. The reasonfor this is thatthe non-supervisedrestorationis over-dominatedby the priorinformation;cf. the
discussion after (26). Fig. 2 also shows that the image restoration greatly improves after applying active methods.
Both local and global methods lead to similar results here.
originalimage noisyimage withoutsupervising randomsupervising
localsupervising globalsupervising medianfilter Gaussianfilter
FIG. 2: An example of active supervising for two scenarios (global and local, as given by (36) and (37), respectively) under
J′ = 0.555, ǫ = 0.1 and ρ = 0.1; see Table I. We display the original image, its noise-corrupted version, the non-supervised
restoration and the active supervision via completely randomly selected spins (pixels). On decoded images, we denote by red
(white) those white pixels of the original image that were decoded wrongly (correctly). Likewise, blue (black) shows black
pixels of the original image that were decoded wrongly (correctly).
Wealso comparewith the(non-supervised)results obtained viatwostandard filters: median andGaussian. Thesefilterswere
applied with the (minimal) radius 1, i.e. via embedding each pixel into a 3×3 box [36]. Both active supervising scenarios
recover the image approximately, while other restoration methods are useless for this example. In this context, we emphasize
again that a large overlap is necessary but not sufficient for image recovery.
For smaller values of J′, the local strategy is better than the global one: Oˆ <Oˆ ; see Table II. Now the random
G L
supervising can improve over no supervising, and there are cases (for a small J′), where the global strategy is worse
than the random: Oˆ < Oˆ . The reason why the global strategy does not apply is clear, since for J′ < 0.45 the
G R
mean-field method does not apply, in particular because the model is within the crtical regime; see Fig. 1, where the
criticalregime canbe identified by the region,where N x and N y sharply increasefrom zeroto values larger
i=1 i i=1 i
than 0.5. It is encouraging that even for J′ 0.45 the local strategy performs well, e.g. it is visibly better than the
random supervising; see Table II. Fig. 3 illus ≤ trates th P is situation f P or J′ =0.5. It is seen that the performance of the
non-supervised restorationis still poor, although better than what was seen for Fig. 3. Now the local active scenario
isclearlybetterthantheglobalone. Finally,Fig.4presentsarepresentativeexampleofJ′ =0.455. Hereallmethods
perform more or less reasonably, but it is clearly seen that fine details of the original image are captured only by the
local supervising method.
Thus we suggest that the local strategy (37) is to be applied for this and similar models, because even when it is
sub-optimal it is close to the optimal strategy.
11
originalimage noisyimage withoutsupervising randomsupervising
localsupervising globalsupervising medianfilter Gaussianfilter
FIG.3: Variousscenariosincludingactivesupervising(globalandlocal,asgivenby(36)and(37),respectively)underJ′ =0.5,
ǫ = 0.1 and ρ = 0.1; cf. Fig. 2. Both active supervising scenarios recover the image, while the no-supervising and random
supervising cases are useless. Notations coincide with those in Fig. 2.
originalimage noisyimage withoutsupervising randomsupervising
localsupervising globalsupervising medianfilter Gaussianfilter
FIG. 4: Various scenarios including active supervising (global and local, as given by (36) and (37), respectively) under J′ =
0.455, ǫ = 0.1 and ρ = 0.09; cf. Figs. 2 and 3. Now no-supervising and random supervising cases are not useless, but fine
details of theoriginal image are recovered only after thelocal scenario.
V. NUMERICAL RESULTS WITH EXTERNAL MAGNETIC FIELD
So far we worked with the Hamiltonian (15) on a square lattice. The corresponding prior density P(x) e−H(x)
is generated by the Ising Hamiltonian H(x) = J′ x x , which is symmetric with respect to the ∝ inversion
− {i,k} i k
x x . This symmetry can be broken with an external field h that introduces a bias in the distribution of x.
i i f
→ − P
Now instead of (15) we look at
H(x,y)= J′ x x h x h x y , (41)
i k f i i i
− − −
{Xi,k} X i X i
e
12
TABLE III: We present values for overlaps for external magnetic field h = 0.01 a square lattice with N = 6400 spins and
f
Nρ supervised spins: Oˆ (the optimal overlap calculated via (3) without supervising), Oˆ R (random supervising), Oˆ G and Oˆ L
referrespectively toglobal andlocal strategies according to(36) and(37); cf.(9). Oˆ MF and Oˆ GF aretheoverlaps obtainedfor
(respectively) median filter and Gaussian filter.
For the global strategy the fraction of supervised spins is ρG =0.1. For the local strategy this value ρL is generally lower, as
shown below. Despite of thisfact thelocal strategy produced betteroverlaps: Oˆ L >Oˆ G.
Parameters Oˆ Oˆ R Oˆ G Oˆ L Oˆ MF Oˆ GF
J′ =0.38, ǫ=0.1, ρL =0.05 0.82766 0.83174 0.82865 0.85987 0.67563 0.82250
J′ =0.39, ǫ=0.1, ρL =0.05 0.82578 0.830263 0.82622 0.85691 0.69625 0.82375
J′ =0.40, ǫ=0.1, ρL =0.07 0.84375 0.84543 0.84688 0.88542 0.75516 0.83188
J′ =0.41, ǫ=0.1, ρL =0.07 0.84891 0.85719 0.85208 0.89147 0.76359 0.84375
J′ =0.42, ǫ=0.1, ρL =0.08 0.85406 0.85564 0.86701 0.90353 0.78313 0.84094
J′ =0.43, ǫ=0.1, ρL =0.09 0.86688 0.86058 0.86597 0.91896 0.80328 0.85593
J′ =0.44, ǫ=0.1, ρL =0.1 0.87813 0.89063 0.87951 0.93611 0.83375 0.87344
J′ =0.45, ǫ=0.1, ρL =0.1 0.90047 0.90104 0.90381 0.95243 0.86953 0.89188
which leads to the joint probability P(x,y) e−H e (x,y), and to the prior probability P(x) eJ′P {i,k} xixk+hfP i xi.
∝ ∝
If h assumes a small but generic value, the magnetization 1 N x and its observed value 1 N y become
f N i=1 i N i=1 i
smootherfunctionsofJ′ thatarecloseertotheiraveragevalues;seeFig.5andcompareeitwithFig.1. Thisisexpected,
P P
because the second-order phase-transition regime is now replaced by a smooth crossover from lower to higher values
of 1 N x and 1 N y ; see Fig. 5. Since the situation is more stable (than for h =0), we can apply a smaller
N i=1 i N i=1 i f
amount of supervising (i.e. smaller values of ρ).
P P
Fig. 6 shows the originaland recoveredimages for a small but generic value of the external field h =0.01 and for
f
J′ =0.39. It is seen that the original image has a fine-grainedstructure. At h =0.01 such a structure is seen for J′
f
being roughlybetween0.37and0.47. As comparedto otherpresentedexamples,herewe appliedasmalleramountof
supervising ρ=0.05 (i.e. the 5% of spins is supervised). Still this supervising is clearly useful, especially in its active
scenario. In Fig. 6 we are still far from the mean-field, since Oˆ >Oˆ ; cf. Table III for further data.
L G
1.0 1.0
●○ ●○ ●○ ●○ ●○ ●○ ●○ ●○ ●○ ●○ ●○
●○
0.8 ● ○ ●○ ■□ ■□ ■□ ■□ ■□ ■□ ■□ 0.8 ●○ ●○ ■ □ ■□ ■□ ■□ ■□
0.6 ● ○ ■ □ ■ □ 0.6 ■□ ■ □
■□
●○
0.4 ● 0.4 ■□
0 0 . . 0 2 ● ○■ □ ●○■□ ●○ ■□● ○ ■ □ ● ○ ■ □ ● ○ ■ □ ● ○ ■ □ ●○ ■□ ● ○■ □
○■
□ ● ■ N N 1 1 ∑ ∑ i i x y i i ○ □ N N 1 1 ∑ ∑ i i x y i i 0 0 . . 0 2 ●○■□ ●○ ■□ ●○ ■□ ● ○■□ ● ○ ■ □
●○
■□ ● ■ N N 1 1 ∑ ∑ i i x y i i ○ □ N N 1 1 ∑ ∑ i i x y i i
0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.1 0.2 0.3 0.4 0.5 0.6
′ ′
J J
(a) (b)
FIG. 5: (a) The same as in Fig. 1, but with an external magnetic field. Magnetization N 1 PN i=1 x i and its observed value
N 1 PN i=1 y i versus J′ under a fixed ǫ = 0.1 in (13) (hence h = 1.09861), and an external magnetic field h f = 0.01. Single
realizations of x={x i}N i=1 and y={y i}N i=1 (denotted bydotted lines) are closer to theaverage behavior than in Fig. 1.
(b) The same as in (a), but for h f = 0.1. It is seen that single realizations of x = {x i}N i=1 and y = {y i}N i=1 converge to the
their averages upon increasing h .
f
13
originalimage noisyimage withoutsupervising randomsupervising
localsupervising globalsupervising medianfilter Gaussianfilter
FIG. 6: An example of active supervising for two scenarios (global and local, as given by (36) and (37), respectively) under
J′ =0.39, h =0.01 (non-zero magnetic field), ǫ=0.1 and ρ=0.05, i.e. everywhere we supervise 5% of spins. Notations and
f
other parameters coincide with those in Fig. 2.
VI. SUMMARY
Restoration of noise-corrupted images is a fundamental problem of statistics, and it is also of obvious practical
importance [1–7]. It joins together mathematical and physical statistics, since the simplest non-trivial approach to
this problem is based on the two-dimensional, square-lattice Ising model with a ferromagnetic interaction between
neighbouring binary pixels (spins) [1, 3]. The prior probability of images within this model is determined via the
Gibbs distribution, where the ferromagneticcoupling constantsaccountfor naturalsmoothness (correlation)between
neighbouring spins. Hence the Ising model was extensively studied in the context of image restoration [1–7]. The
optimal approach to restoration within the Ising model is based on maximizing the average overlap (the inverse
Bayesianrisk). This approachdemands calculating Gibbsian(or equilibrium) averagesof all Ising spins. In the sense
of the averageoverlap,this approachoutperforms those based on various types of filtering (e.g. median filtering) [4].
However, filtering approaches are easier to implement in practice.
Here we studied a ferromagnetic Ising model for active restoration of images, where prior information on certain
(supervised) spins is requested using the initial observation of the noise-corrupted image. For a given number of
supervised spins, the optimal supervising strategy maximizes the average overlap of non-supervised spins with their
true (requested) values. Generally, this optimal strategy is not known. We found the optimal supervising strategy
within the mean-field approximation, which is introduced via letting all spins couple with each other. It applies
to realistic (two-dimensional) images, whenever the inter-spin coupling is large (the image is smooth) and hence
fluctuations are small. The optimal strategy in this situation amounts to supervising those pixels (spins) that do not
agree with the sign of the overall magnetization. The strategy shows that there is an upper limit on the number of
supervised spins, beyond of which it is meaningless to supervise, since the performance will not change.
The mean-field approximation allows to study the active supervision in detail. It also leads to strategies that can
apply more generally, i.e. in the critical regime of the two-dimensional model, where correlations are long-range and
probabilitydistributionsareself-similar[20]. Thecriticalregimeappliestonaturalimages[19]. Weexploredonesuch
strategy, where we supervise only those spins (pixels) that disagree with their Bayesian estimates. The performance
of this strategy is comparable with the optimal one where the latter is known, i.e. the mean-field applies. Otherwise
(e.g. in the critical regime), it outperforms over all other methods we tried.
Inthisstudyweassumedthathyperparamatersofthe model—thesmoothnessparameterJ ofthe priorprobability
(ferromagnetic coupling), the bias h (external magnetic field) and the noise parameter h—are known. Generally,
f
this is not the case: hyperparameters are to be determined from data. This is one instance of the hyperparameter
learning for the Ising model that recently attracted much attention [29, 30]; see [31, 32] for reviews. In Appendix
we studied the hyperparameter learning within the mean-field approximation, and saw that this problem is non-
trivial due to non-identifiability: if (for h = 0 taken to be zero for simplicity) both hyperparameters J and h are
f
unknown, then neither of them can be found via the standard maximum likelihood approach. The non-identifiability
14
problem is crucial for image restoration, since imprecisely known hyperparameters may lead to a larger value of
the average overlap (over-confidence), thereby creating a false impression about the performance of the restoration.
Appendix alsoshowsthatwithinthemean-fieldapproximationthenon-identifiabilityproblemcanberesolvedviathe
active supervising. A pertinent open problem is how the active supervising alters hyperparameter learning for the
two-dimensional situation, especially in its critical regime.
Acknowledgement
A.E.A. acknowledges discussions with A. Galstyan. A.E.A. was supported by SCS of Armenia (Grant No. 18RF-
015). ThisworkissupportedbyNationalNaturalScienceFoundationofChina(GrantNo.11505071),theProgramme
of Introducing Talents of Discipline to Universities under Grant No. B08033 and the Fundamental Research Funds
for the Central Universities.
Appendix: Unknown hyperparameters
1. The maximum-likelihood method for estimation of hyperparameters
Thestandardmaximum-likelihoodmethodofestimatingunknownhyper-parametersistostartwiththeobservations
y and probabilities P◦(y) at trial values of the hyper-parameters [33]. For the mean-field model studied in section
IIIA these trial values are J◦ and h◦, while the correct values of hyperparameters are still denoted by J and h.
Now if N 1, then lnP◦(y) self-averages:
≫
lnP◦(y) P(y)lnP◦(y), (42)
≃ y
X
with the true (i.e. containing the correct hyper-parameters J and h) probability P(y). The global maximum of (42)
is reached (among other possibilities) at J◦ =J and h◦ =h [33]. This fact follows from the positivity of the relative
entropy: P(y)ln P(y) 0. However,the maximum is generally not unique, so the maximization over J◦ and h◦
y P◦(y) ≥
does not need to return true values J and h. In practice the global optimization is generally intractable; hence it is
P
implemented via an expectation–maximization (EM) method (Baum-Welch algorithm) [33].
Now we should use (21, 22) with J, h and m replaced by respectively J◦, h◦ and m◦, where instead of (23) we get
1
m◦ = tanh(J◦m◦+h◦y )= π(y)tanh(J◦m◦+yh◦). (43)
i
N
i y=±1
X X
Note that m◦ self-averageswith probability π(y) containing true values of J and m.
Due to factorization (22) the maximization of (42) amounts to maximizing
π(y)lnπ◦(y), (44)
y=±1
X
where π(y) is given by (22), and where [cf. (20)]
1
π◦(y)= [1+ytanh(J◦m◦)tanh(h◦)], (45)
2
Now it is clear from (22, 45) that for J > 1 1 the maximization of (44) will lead to π(1) = π◦(1), and this global
maximum is achieved for
tanh(Jm)tanh(h)=tanh(J◦m◦)tanh(h◦). (46)
1 ForJ <1wegetπ(1)=π(−1)=1/2. Then(43) leads tom◦ =0. Thissatisfies(46), butthereisnoconstraint onJ◦ andonh◦,i.e.
nodeterminationofhyperparameters ispossiblewhatsoever. Thisisexpected, because forJ <1theinitialdistributionofthespinsis
completelyunbiased.
15
Using in (43) tanh[a+b]= tanh[a]+tanh[b] and (46) we simplify (43) as follows
1+tanh[a]tanh[b]
m◦ =tanh[J◦m◦]. (47)
Eq. (46) cannot determine two unknowns J◦ and h◦. If one of them is known precisely, e.g. h = h◦, the other one
will be found via (46). But if both J◦ and h◦ are unknown, neither of them will be found. This is an example of the
non-identifiability problem in inference [34, 35].
2. Overlap and over-confidence
The overlapand magnetization still self-average under the correct values J and h [cf. (25)]:
O◦ = π(y)tanh[ J◦m◦+yh◦ ], (48)
| |
y=±1
X
where π(y) is given by (22), and we employed ξˆ◦(y)=sign[J◦m◦+h◦y ]. With (46) and (47) one deduces from (48):
i i
O◦ =max(tanh[J◦ m◦ ], tanh[h◦]). (49)
| |
Recall that J◦ and h◦ hold (46). Under this constraint O◦ in (49) can be either larger or smaller than the optimal
overlap Oˆ given by (25); e.g. one can take J◦ sufficiently large and h◦ small so as to hold (46) and get O◦ >Oˆ from
(49). Hence the overlap is a good criterion for distinguishing the optimal solution only if the hyperparameters are
known precisely.
Once hyperparameters cannot be found precisely, there can be two possibilities for the overlap under trial values
O◦ [see (46, 49)]andthe optimaloverlapOˆ givenby (25): O◦ <Oˆ andO◦ >Oˆ. The formeris a fair situation, where
imprecise knowledge of hyperparameters (J◦ = J and h◦ = h) brings in a reduction in the overlap. In contrast, the
6 6
latter is a dangerous situation, where imprecise knowledge leads to over-confidence (over-fitting). We feel that so far
not enough attention was devoted to the over-confidence phenomenon both computationally and theoretically. One
of the advantages of the present model is that it makes the phenomenon obvious.
To avoid over-confidence, one can minimize O◦ in (49) over J◦ and over h◦ under constraint (46). Clearly, this
procedure will lead to O◦ <Oˆ. For our situation this produces:
O◦ = tanh(Jm)tanh(h) Oˆ, (50)
≤
tanh(J◦m◦)=tanh(hp ◦)= tanh(Jm)tanh(h). (51)
Eq. (51) is a reasonable setting for unknown hyperparameteprs. This method works theoretically, but its algorithmic
implementation in hyperparameter learning algorithms is not yet clear.
3. Active supervising and hyper-parameter learning
Weshallnowstudyhowtheactivesupervising—whichwasdesignedsoastoincreasetheoverlap—influencesonthe
determinationofthehyper-parameters. Insteadof(42)and(44)weshouldnowmaximizethefollowing(self-averaged)
expression over the trial hyper-parameters h◦ and m◦:
P(η,y)P(ny)ln[P◦(η,y)P(ny)], (52)
| |
η,y,n
X
wherewenotethatP(ny)doesnotdependontrialhyperparameters,sinceitisdesignedonthegroundofobservations
|
y. When the supervising is absent, i.e. P(ny)=1 for n=(0,...,0), (52) reverts to (42).
|
We now additionally assume that
N
P(ny)= p(n y ). (53)
k k
| |
k=1
Y
Using (21, 22, 53) we write:
N
P◦(η,y)P(ny)= χ(η ,y ,n ), (54)
k k k
|
k=1
Y
χ(η,y,n)=π◦(η,y)p(ny)δ +π◦(y)p(ny)δ , (55)
n1 n0
| |
16
where δ is the Kronecker’sdelta. Analogousformula can be written for P(η,y)P(ny). Thus maximizing (52)over
n0
trial hyperparameters J◦ and h◦ amounts to maximizing |
p(1y)π(η,y)ln[p(1y)π◦(η,y)]+ p(0y)π(y)ln[p(0y)π◦(y)] (56)
| | | |
η,y y
X X
= p(1y)π(η,y)ln[p(1y)]+ p(1y)π(η,y)ln[π◦(η,y)/π◦(y)]
| | |
η,y η,y
X X
+ p(1y)π(η,y)ln[π◦(y)]+ p(0y)π(y)ln[π◦(y)]+ p(0y)π(y)ln[p(0y)]
| | | |
η,y y y
X X X
= p(1y)π(η,y)ln[π◦(η,y)/π◦(y)]+ π(y)ln[π◦(y)] (57)
|
η,y y
X X
+ p(1y)π(y)ln[p(1y)]+ p(0y)π(y)ln[p(0y)]. (58)
| | | |
y y
X X
Thus we can maximize only (57), since (58) does not depend on trial hyperparameters. Note that the last term in
(57) is the expression (44) without supervising; it is recovered from (57) for p(1y)=0. The first term in (57) is the
|
self-averagedexpression for ln[π◦(η y)].
Inspecting directly (56) or (57)w | e conclude that now the maximizationoverJ◦ and h◦ will recoverthe true values
of the hyper-parameters: J◦ = J and h◦ = h, even in the regimes (33, 34), where the overlap is optimized. Note
that this conclusion holds even for the random supervising (31), where its origin is especially clear: together with
π(y)lnπ◦(y)wemaximize π(η,y)ln[π◦(η,y)],andthelatermaximizationleadstoJ◦ =J andh◦ =h,which
y η,y
also maximizes the former; cf. (46).
P P
[1] S. Geman and D. Geman, Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images, IEEE Trans.
Pattern Analysis Machine Intell.6, 721 (1984).
[2] J. Besag, On the statistical analysis of dirty pictures, J. Roy.Sfat. Soc. B, 48, 259-302 (1986).
[3] J.M. Pryceand A.D.Bruce, Statistical mechanics of image restoration, J. Phys.A 28, 511 (1995).
[4] K.Tanaka, Statistical-mechanical approach to image processing, J. Phys.A 35, R81 (2002).
[5] E.Cohen,R.Heiman,O.Hadar,ImageandvideorestorationviaIsing-likemodels,inProc.IS&T/SPIEElectronicImaging
Conference, San-Francisco 2012.
[6] H. Nishimori and K.Y. Wong, Statistical mechanics of image restoration and error-correcting codes, Physical Review E,
60, 132 (1999).
[7] H. Nishimori, Statistical Physics of Spin Glasses and Information Processing: An Introduction (Oxford University Press,
Oxford,2001).
[8] B. Settles. Active Learning Literature survey. Technical Report 1648, University of Wisconsin–Madison, 2009.
[9] A.Wald, Sequential Tests of Statistical Hypotheses, The Annals of Mathematical Statistics, 16, 117-186 (1945).
[10] T. Scheffer, C. Decomain, and S. Wrobel. Active Hidden Markov Models for Information Extraction. In F. Hoffmann,
D.Hand,N.Adams,D.Fisher, andG.Guimaraes, editors, Advances in Intelligent Data Analysis,volume2189 ofLecture
Notes in Computer Science, pages 309–318. Springer Berlin Heidelberg, 2001.
[11] B. Anderson and A. Moore, Active Learning for Hidden Markov Models, 22nd International Conference on Machine
Learning, Bonn, Germany, 2005.
[12] A.E. Allahverdyan and A. Galstyan, Journal of Statistical Physics 161, 452-466 (2015).
[13] X.Zhu,J.Lafferty,andZ.Ghahramani. CombiningActive Learning and Semi-Supervised Learning UsingGaussian Fields
andHarmonicFunctions. InICML2003workshoponTheContinuumfromLabeledtoUnlabeledDatainMachineLearning
and Data Mining,pages 58–65, 2003.
[14] M. Bilgic and L. Getoor. Reflect and correct: A misclassification prediction approach to active inference. ACM Trans.
Knowl. Discov. Data, 3(4):20:1–20:32, Dec. 2009.
[15] M. Bilgic, L. Mihalkova, and L. Getoor. Active Learning for Networked Data. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), 2010.
[16] C. Moore, X. Yan, Y. Zhu, J.-B. Rouquier, and T. Lane. Active Learning for Node Classification in Assortative and
Disassortative Networks. InProceedings ofthe 17thACM SIGKDDInternational ConferenceonKnowledge Discovery and
Data Mining,KDD ’11, pages 841–849, New York,NY,USA,2011. ACM.
[17] A. Krause and C. Guestrin. Near-optimal Nonmyopic Value of Information in Graphical Models. In 21st International
Conference on Uncertainty in Artificial Intelligence (UAI), page 5, 2005.
[18] A.KrauseandC.Guestrin. OptimalValueofInformationinGraphical Models. Journal ofArtificial IntelligenceResearch,
35:557–591, 2009.
[19] D.L. Ruderman,The statistics of natural images, Network: computation in neural systems, 5, 517 (1994).
17
[20] G.J.Stephens,T.Mora,G.Tkacik,andW.Bialek, Statistical Thermodynamics of Natural Images,Phys.Rev.Lett.110,
018701 (2013).
[21] J.Zhang,The mean field theory inEM procedures for Markov random fields,IEEETransactionsonSignalProcessing, 40,
2570-2583 (1992).
[22] G. Celeux, F. Forbes, N. Peyrard, EM procedures using mean field-like approximations for Markov model-based image
segmentation, Pattern Recognition, 36, 131-144 (2003).
[23] J. Inoue and K. Tanaka, Mean field theory of EM algorithm for Bayesian grey scale image restoration, Journal of Physics
A:Mathematical and General, 36, 10997-11010 (2003).
[24] D.GeigerandF.Girosi,Meanfieldtheoryforsurfacereconstruction, IEEETransactionsonPatternAnalysisandMachine
Intelligence, 13, 401412 (1991).
[25] J. Marroquin, S. Mitter, and T. Poggio, Probabilistic solution of ill posed problem in computer vision, Journal of the
American Statistical Association, 82, 7689 (1987).
[26] M. Opper,D. Saad (eds),Advanced Mean Field Methods: Theory and Practice, (MIT Press, 2001).
[27] C.P. Robert, The Bayesian choice (SpringerScience + Business, NY,2007).
[28] Library of image-processing packages: http://www.scipy-lectures.org/packages/scikit-image/
[29] H. J. Kappen and F. Rodriguez, Efficient Learning in Boltzmann Machines Using Linear Response Theory, Neural Com-
putation 10, 1137 (1998).
[30] S.CoccoandR.Monasson, Adaptive cluster expansion forinferring Boltzmann machines with noisydataPhysicalReview
Letters, 106, 090601 (2011).
E. Aurell and M. Ekeberg, Inverse Ising inference using all the data, Physical Review Letters, 108, 090201(2012).
H.C. Nguyen and J. Berg, Mean-field theory for the inverse Ising problem at low temperatures, Physical Review Letters,
109, 050602 (2012).
A.DecelleandF.Ricci-Tersenghi,Pseudolikelihooddecimationalgorithmimprovingtheinferenceoftheinteractionnetwork
in a general class of ising models, Physical ReviewLetters, 112, 070603 (2014).
M. Castellana and W. Bialek, Inverse spin glass and related maximum entropy problems, Physical Review Letters, 113,
117204 (2014).
[31] Y. Roudi, E. Aurell, and J. A. Hertz, Statistical physics of pairwise probability models, Frontiers in computational neuro-
science 3 (2009).
[32] H. C. Nguyen, R. Zecchina, and J. Berg, Inverse statistical problems: from the inverse Ising problem to data science,
Advancesin Physics, 66, 197-261 (2017).
[33] Y.Ephraim and N. Merhav, Hidden Markov processes, IEEE Trans. Inf.Th., 48, 1518-1569, (2002).
[34] D. Blackwell and L. Koopmans, On the identifiability problem for functions of finite Markov chains, Ann. Math. Statist.
28, 1011 (1957).
[35] H.Ito, S. Amari, and K.Kobayashi, Identifiability of Hidden Markov Information Sources, IEEE Trans. Inf. Th., 38, 324
(1992).
[36] G. R.Arce, Nonlinear Signal Processing: A Statistical Approach (Wiley, NewJersey, 2005).
[37] L.D. Landau and E.M. Lifshitz, Statistical Physics, I,(Pergamon Press Oxford,1978).