Workinprogress
GOAL-DIRECTED PLANNING BY REINFORCEMENT
LEARNING AND ACTIVE INFERENCE
DongqiHan KenjiDoya
CognitiveNeuroroboticsResearchUnit NeuralComputationUnit
OkinawaInstituteofScienceandTechnology OkinawaInstituteofScienceandTechnology
Okinawa,Japan Okinawa,Japan
JunTani∗
CognitiveNeuroroboticsResearchUnit
OkinawaInstituteofScienceandTechnology
Okinawa,Japan
ABSTRACT
Whatisthedifferencebetweengoal-directedandhabitualbehavior? Wepropose
a novel computational framework of decision making with Bayesian inference,
in which everything is integrated as an entire neural network model. The model
learnstopredictenvironmentalstatetransitionsbyself-explorationandgenerating
motoractionsbysamplingstochasticinternalstatesz. Habitualbehavior,which
is obtained from the prior distribution of z, is acquired by reinforcement learn-
ing. Goal-directedbehaviorisdeterminedfromtheposteriordistributionofz by
planning, using active inference which optimizes the past, current and future z
byminimizingthevariationalfreeenergyforthedesiredfutureobservationcon-
strained by the observed sensory sequence. We demonstrate the effectiveness of
the proposed framework by experiments in a sensorimotor navigation task with
cameraobservationsandcontinuousmotoractions.
1 INTRODUCTION
Themechanismofintelligentdecisionmakingisacentral,frequentlydiscussedproblemincognitive
science, neuroscience, andartificial intelligence. Inparticular, goal-directedplanning, i.e., how to
adaptively generate plans to achieve a non-fixed goal, has been of great interest since long ago
(Duncanetal.,1996;Tani,1996;Desmurgetetal.,1998).
The free energy principle (FEP) (Friston, 2010) and active inference (AIf) theory (Friston et al.,
2010; 2011) provide a Bayesian computational framework of the brain. AIf explains decision-
makingaschangingtheagent’sbeliefofperceptionandproprioceptiontominimizethedifference
(orsurpriseinFEP)betweenpredictedobservationandgoalobservation(Tani,1996;Fristonetal.,
2016;Matsumoto&Tani,2020). However, existingAIfstudiesareusuallyrestrictedtorelatively
simpleenvironmentswithlow-dimensionalobservationspaceand/oraknownstatetransitionmodel
(Fristonetal.,2009;2017;Ueltzho¨ffer,2018;Millidge,2020). Onemajorchallengeistothetrade-
offbetweenexplorationandexploitationbysimplyminimizingthefreeenergy(Ueltzho¨ffer,2018;
Millidge,2020;Fountasetal.,2020).
Ontheotherhand,deepreinforcementlearning(RL)hasbeendevelopedinrecentyearsasapower-
fulframeworkforlearningreward-orienteddecisionmaking. Goal-directedtaskscanbeconverted
to RL problems by providing a reward when the goal is achieved. Even though, modern deep RL
hassurpassedmanysingle-objectivetasks(Silveretal.,2017;Vinyalsetal.,2019;Lietal.,2020),
itisstillchallengingwithmultipleand/ornon-fixedgoals(rewardfunction)(Plappertetal.,2018).
In this paper, we suggest that RL and AIf may be both necessary to achieve efficient and flexible
goal-directedplanning.Wehereproposeanovelcomputationalframeworkofgoal-directedplanning
∗Correspondence:jun.tani@oist.jp
1
1202
nuJ
22
]GL.sc[
2v83990.6012:viXra
Workinprogress
Figure 1: Simplified diagrams of the the proposed framework. In the learning phase, the pre-
dictionmodelandRLnetworksaretrained. Intheactiveinferencephase,theinternalstates(latent
variable)zAIf arecomputedbyerrorbackpropagationtominimizethefreeenergyforagivengoal
observation(Eq.6). Trainablevariablesineachphaseareshowninblack.
with Bayesian (variational) inference, in which RL and AIf work in a complementary manner. It
performs deep RL for exploring the environment and acquiring habitual behavior for exploitation.
Meanwhile,itlearnsapredictivemodelforsensoryobservation. Then,goal-directedplanningcan
be performed under the framework of AIf by simply minimizing the free energy with respect to a
givengoalobservation.
We demonstrate the effectiveness of the proposed framework in a simulated navigation task with
high-dimensionalobservationspaceandcontinuousactionspace. Weshowthataflexiblehabitual
behavior was acquired via RL for reward-seeking and collision avoidance, and that near-optimal
goal-directedplanningwasobtainedsimplybyoptimizingtheAIfobjective.
2 METHODS
2.1 OVERVIEW
The proposed framework employs an integrated neural network model that can perform RL and
observation prediction simultaneously (Fig. 1). The core idea is based on a variational Bayesian
variable z, referred to as internal states in FEP (Friston, 2010). The internal states z entail the
probabilisticdistributionoftheagent’sbeliefaboutactionandperception.
Byexploringtheenvironment,networkconnectionsaretrainedtopredictobservationtransitionsand
tofulfillRLobjectives,asshowninlearningphaseofFig.1. Attimestept,thepriorzpissampled
t
bytheRNNmodelandthemotoractionisgeneratedviathepolicynetwork. Themodelalsouses
actualobservationstoinfertheposteriorzq sothattheRNNstatesreflectrealenvironmentalstate
t−1
(postdictionupdate).
Ifthemodeliswelllearned,theagentcanperformgoal-directedplanningusingAIf,i.e.,updating
itsinternalstateszAIf tominimizetheerrorbetweenthepredictedfutureobservationandthegoal
t
observation(Fig.1,AIfphase). Themotoractioncanthenbeobtainedusingthepolicynetworkas
intheRLcase,withoutrequiringaninversemodel.
Thefollowingsectionsdetailcomputationprocessesoftheproposedframework.
2.2 OBSERVATIONPREDICTIONMODEL
Toestimateaworldmodelingeneralcasesthatareprobablypartiallyobservable(A˚stro¨m,1965),
thereexistvariouskindsofRNN-basedmodels(Tani,1996;Ha&Schmidhuber,2018;Kaiseretal.,
2020; Han et al., 2020b). Here we employ a Bayesian RNN model, as a natural choice under the
FEPframework,forpredictingstatetransitions(Fig.2(a)). ItisanextensionofthevariationalRNN
(VRNN)modelinChungetal.(2015),andisexplainedasfollows.
2
Workinprogress
a Learning Phase b Active Inference Phase
c d e
Interacting with Environment (Habitual) RL
Figure2: Detaileddiagramsofhowtheframeworkfunctions. (a)Learningphase. (b)AIfphase,
whencurrenttimestepist. (c)Interactingwiththeenvironment. (d)ComputingRLfunctions. (e)
Explanationsofdiagramnodesandedges. In(b),(c)and(d). Actiona issampledfromthepolicy
t
functionπ .
t
First,leth denotetheRNNstates,whicharerecurrentlyupdatedby
t
h =f RNN (h ;zq), (1)
t t−1 t
whereweusedthelongshort-termmemoryrecurrency(Hochreiter&Schmidhuber,1997),andzq
t
areposteriorinternalstateswhichcanbeinferredbyh andcurrentrawobservationx :
t−1 t
zq ∼N (cid:0) µ ,diag(σ2 ) (cid:1) , (cid:2) µ ,σ2 (cid:3) =f posterior (x ,h ). (2)
t q,t q,t q,t q,t t t−1
Ontheotherhand,priorinternalstateszpcanbeobtainedfromh solely:
t t−1
zp ∼N (cid:0) µ ,diag(σ2 ) (cid:1) , (cid:2) µ ,σ2 (cid:3) =f prior (h ). (3)
t p,t p,t p,t p,t t−1
Themodelpredictsthenextobservation(forpixelobservation)as
decoder
x =sigmoid(µ ), µ =f (h ). (4)
t+1 x,t+1 x,t+1 t
Thefreeenergy(ornegativevariationallowerbound)objectivecanbewrittenas
T
(cid:88)
F = [D (q(z )||p(z )]−E [log(p(x =x¯ ))], (5)
KL t t q t t
t
wherepandqareparameterizedPDFsofpriorandposteriorz ,respectively,usingthereparameter-
t
izationtrick(Kingma&Welling,2013).Weestimatelog(p(x =x¯ ))bycrossentropy,assuminga
t t
Bernoullidistributionofx . ThemodelistrainedbyminimizingEq.6bybackpropagationthrough
t
time. SeeAppendixA.2formoredetails.
2.3 REINFORCEMENTLEARNING
Fig.2(c)demonstrateshowactionisobtainedinhabitualbehavior. Itisworthmentioningthatthe
connectionfromzptohpsharesthesamesynapticweightwiththeconnectionfromzqtohq.Thisis
t t t t
consistentwiththefactthatzp,hpandzq,hqrepresenttwosidesofthesamecoin,buttheposterior
t t t t
hasadditionalinformationabouttheactualobservationx¯ .
t
We used soft actor-critic (SAC) as the RL algorithm (Haarnoja et al., 2018a;b). As shown in
Fig. 2(d), the state value function, policy function and state-action value function are estimated
3
Workinprogress
habituated behavior performance
60
50
40
30
20
10
0 50 100 150
thousand steps
edosipe
1 hsinif
ot
spets
(a) N (c) goal
W E
habitual
S
init position
goal-directed
(b) (d)
Actual Goal observation
observation
Planned observation
Figure 3: Results of the maze navigation task. (a) The environment in the PyBullet simulator,
wheretheblackballistherobotandotherobjectsarewallsandobstacles. Ineachepisode,therobot
isinitializedatarandompositionintheorange,shadedareainthemiddle. (b)Thelearningcurveof
habitualbehavior(Mean±MSE.,40trials). Theverticallineindicatesthestartoflearning,before
which random exploration is conducted. (c) Comparison of moving trajectories between habitual
andgoal-directedbehavior. (d)ActualobservationandpredictedobservationinAIf,inanexample
episode (the northern camera is shown). Numbers in the parentheses indicate c in Eq. 6, i.e. the
τ
possibilityofachievingthegoalatthatthecorrespondingtimestep.
byf (h),f (h)andf (h,a),respectively,wheref ,f andf arefully-connectedfeedforward
V π Q V π Q
networks. Note that during learning, gradients backpropagate (through time) to the whole RNN
model. This is beneficial to shape representations of RNN states that are useful for motor control
(moredetailscanbefoundinAppendixA.2.3).
2.4 ACTIVEINFERENCEBYERRORREGRESSION
AfterlearningapredictivemodelandcontrolskillsbyRL,theproposedframeworkisabletoper-
formgoal-directedplanning. Asshownin(Fig.2(b)),thereisagoalobservationprovidedandAIf
is conducted by minimizing the corresponding free energy by updating internal states zAIf. More
specifically,wedoerrorregression(Ahmadi&Tani,2017;2019;Matsumoto&Tani,2020),which
istominimizetheerrorbetweenpredictedobservationandgoalobservationregularizedbyKLdi-
vergencebetweenpriorandposterior,thatis,thefreeenergywithrespecttothegoalobservation:
FAIf =D (qAIf(z )||p(z ))−log(p(x =x¯ ))+
KL t−1 t−1 t t
N−1
(cid:88) (cid:2) (cid:3)
D (qAIf(z )||p(z ))−c log(p(x =x )) . (6)
KL t+τ t+τ τ t+τ+1 goal
τ=0
The posterior internal states by AIf zAIf is computed using gradient descent and backpropagation
t
throughtimetominimizeEq.6ateachtimestept(seeAppendixA.2.4).Theprevious-stepposterior
zAIf isalsotrainedsoastorepresentthecurrenttrueenvironmentalstate(thefirstlineofEq.6).
t−1
Note that the prediction loss at future steps is multiplied by trainable scalar variables c where
τ
τ =0,1,··· ,N −1(weusedN =8). Thisisbecausethenumberofstepsbywhichtheagentcan
reachthegoalisunknown; therefore,weintroducec ,whichcanbeunderstoodastheprobability
τ
ofreachingthegoalatt+τ. Softmaxisusedsothatwehavec +c +···+c =1.
0 1 N−1
3 RESULTS
We tested the proposed framework in a simulated maze environment (Fig. 3(a)). In this task, the
goalistoreachtherewardareaatthesoutheastornorthwestcornerwhilebypasstheobstacles, at
4
Workinprogress
which the agent can receive a one-step reward and finish the episode. There is also a punishment
(negativereward)giveniftherobotcollideswithanywallorobstacle.
TherobotmakesobservationsusingRGBDcameraimages. Forsimplicity,wemounted4cameras
on the robot, each of which has a fixed orientation (east, south, west, north) so the observation
dependsonlyontherobotlocation. Therobotcanmoveonaplanebyexecutingacontinuous-value
motoraction(∆X,∆Y)withalimitedmaximumdistanceateachstep.
Ineachepisode,therewardareaisrandomlysetateitherthesoutheastornorthwestcorner,andthe
robot cannot see the reward area. Therefore, if no clue about the goal is available to the agent, it
should try to reach both reward areas. This is habituated behavior. In contrast, if a goal position
tobereachedisspecifiedasthesensoryobservationexpectedatthegoalposition,theagentshould
directly go to the corresponding position, which is goal-directed behavior. Using the proposed
framework,weperformedsimulationsonthistask.
First, the agent explored the environment with stochastic actions and collected data (observation,
actionandrewardsequences)initsreplaybuffer. ThemodelwastrainedbyminimizingEq.5and
RLobjectiveswithexperiencereplay. Itacquiredhabitualbehaviorbyfirstgoingtoonegoalarea
andthentotheother,ifthetaskisnotcompleted(Fig.3(b)andtheupperrowof(c)).
If a goal is assigned and the observation at the goal position is known to the agent, goal-directed
planning can be conducted using the method described in Sect. 2.4. To demonstrate goal-directed
behavior,wevisualizedthemovingtrajectoriesofgoal-directedbehaviorofanexampleagent(after
learning)inthelowerrowofFig.3(c),comparedtotrajectoriesofhabitualbehaviorintheupperrow.
Theagentcouldplanactionsdirectlytowardthedesiredgoal,incontrasttothefrequentredundant
movinginhabitualbehavior,duetolackofgoalclue.Wecanalsoseethatthegoal-directedbehavior
sharedgoodmotor-controlskillsofhabitualbehavior,suchasavoidingcollisionsorzigzagging. We
alsodemonstratehowtheagentpredictedfutureobservationsthatleadtothegivengoalinFig.3(d).
4 SUMMARY
ThisworkisaproofofconceptillustratinghowRLandAIfcollaborateinasingle,integratednet-
workmodeltoperformhabitualbehaviorlearningandgoal-directedplanningwithhigh-dimensional
observation space and continuous action space. Our key idea is to utilize the variational inter-
nal states z. The prior distribution zp corresponds to unconditional, habitual behavior for explo-
ration/exploitation. Goal-directed behavior is obtained by computing the posterior zAIf (including
decidingcurrentzAIf andrefreshingpreviouszAIf )usingAIftominimizetheexpectedfreeenergy
t t−1
for a given goal observation. The effectiveness of our framework is demonstrated using a maze
navigationtaskwithpixelimagesasobservationsandcontinuousmotoractions.
Future work will scale up the model to more realistic and challenging environments. Also, it is
interestingtoconsiderhowhabitualandgoal-directedbehaviorswitchwithoutartificialassignment,
whichmayunderlieanimportantmechanismofdecision-makinginthebrain.
ACKNOWLEDGEMENT
TheauthorsarefundedbyOISTgraduateschoolandthismaterialisbasedonworkthatispartially
fundedbyanunrestrictedgiftfromGoogle.
REFERENCES
AhmadrezaAhmadiandJunTani.Bridgingthegapbetweenprobabilisticanddeterministicmodels:
asimulationstudyonavariationalBayespredictivecodingrecurrentneuralnetworkmodel. In
InternationalConferenceonNeuralInformationProcessing,pp.760–769.Springer,2017.
Ahmadreza Ahmadi and Jun Tani. A novel predictive-coding-inspired variational rnn model for
onlinepredictionandrecognition. Neuralcomputation,pp.1–50,2019.
KarlJA˚stro¨m. OptimalcontrolofMarkovprocesseswithincompletestateinformation. Journalof
MathematicalAnalysisandApplications,10(1):174–205,1965.
5
Workinprogress
JunyoungChung,KyleKastner,LaurentDinh,KratarthGoel,AaronCCourville,andYoshuaBen-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processingsystems,pp.2980–2988,2015.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
roboticsandmachinelearning. http://pybullet.org,2016–2019.
MichelDesmurget,DenisPe´lisson,YvesRossetti,andClaudePrablanc.Fromeyetohand:planning
goal-directedmovements. Neuroscience&BiobehavioralReviews,22(6):761–788,1998.
JohnDuncan,HazelEmslie,PhyllisWilliams,RogerJohnson,andCharlesFreer. Intelligenceand
the frontal lobe: The organization of goal-directed behavior. Cognitive psychology, 30(3):257–
303,1996.
ZafeiriosFountas,NoorSajid,PedroAMMediano,andKarlFriston. Deepactiveinferenceagents
usingMonte-Carlomethods. InAdvancesinneuralinformationprocessingsystems,2020.
Karl Friston. The free-energy principle: a unified brain theory? Nature reviews neuroscience, 11
(2):127–138,2010.
KarlFriston,Je´re´mieMattout,andJamesKilner. Actionunderstandingandactiveinference. Bio-
logicalcybernetics,104(1):137–160,2011.
KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,GiovanniPezzulo,etal.
Activeinferenceandlearning. Neuroscience&BiobehavioralReviews,68:862–879,2016.
KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanniPezzulo.
Activeinference: aprocesstheory. Neuralcomputation,29(1):1–49,2017.
Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference?
PloSone,4(7):e6421,2009.
Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a free-
energyformulation. Biologicalcybernetics,102(3):227–260,2010.
DavidHaandJu¨rgenSchmidhuber.Recurrentworldmodelsfacilitatepolicyevolution.InS.Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
NeuralInformationProcessingSystems31,pp.2450–2462.CurranAssociates,Inc.,2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximumentropydeepreinforcementlearningwithastochasticactor. InInternationalConfer-
enceonMachineLearning,pp.1856–1865,2018a.
TuomasHaarnoja,AurickZhou,KristianHartikainen,GeorgeTucker,SehoonHa,JieTan,Vikash
Kumar,HenryZhu,AbhishekGupta,PieterAbbeel,etal. Softactor-criticalgorithmsandappli-
cations. arXivpreprintarXiv:1812.05905,2018b.
DongqiHan,KenjiDoya,andJunTani. Self-organizationofactionhierarchyandcompositionality
byreinforcementlearningwithrecurrentneuralnetworks.NeuralNetworks,129:149–162,2020a.
DongqiHan, KenjiDoya, andJunTani. Variationalrecurrentmodelsforsolvingpartiallyobserv-
ablecontroltasks. InProceedingsoftheInternationalConferenceonLearningRepresentations,
2020b.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780,1997.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski,DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,etal. Model-based
reinforcement learning for atari. In Proceedings of the International Conference on Learning
Representations(ICLR),2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
6
Workinprogress
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114,2013.
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao
Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement
learning. arXivpreprintarXiv:2003.13590,2020.
TakazumiMatsumotoandJunTani.Goal-directedplanningforhabituatedagentsbyactiveinference
usingavariationalrecurrentneuralnetwork. Entropy,22(5):564,2020.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology,96:102348,2020.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464,2018.
DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
withouthumanknowledge. Nature,550(7676):354,2017.
JunTani. Model-basedlearningformobilerobotnavigationfromthedynamicalsystemsperspec-
tive.IEEETransactionsonSystems,Man,andCybernetics,PartB(Cybernetics),26(3):421–436,
1996. doi: 10.1109/3477.499793.
KaiUeltzho¨ffer. Deepactiveinference. Biologicalcybernetics,112(6):547–573,2018.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
review,36(5):823,1930.
OriolVinyals, IgorBabuschkin, WojciechMCzarnecki, Michae¨lMathieu, AndrewDudzik, Juny-
oungChung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal. Grandmaster
levelinstarcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
APPENDIX A
A.1 TASKDETAILS
FigureA.1:Configurationoftherobotnavigationtask,wherethenumbersindicate(x,y)coordinates
ofthecorrespondingpoints.
We use PyBullet physics simulator for the experiments (Coumans & Bai, 2016–2019). Detailed
configurationofthetaskweusedcanbeseeninFig.A.1.Therobotisaspherewithradius=0.5.And
itsactionisitsspeedonthehorizontalplane: ∆xand∆y,where∆x ∈ [−2,2]and∆y ∈ [−2,2]
areboundedcontinuousvariables.
7
Workinprogress
Figure A.2: Architecture of our variational RNN model, where (cid:15)p and (cid:15)q are unit Gaussian white
t t
noisevector,whichhasthesamedimensionastheinternalstatesz .
t
Observationoftherobotis4RGBDcameras,eachwithresolution16×16.Allthecamerasarefixed
onthetopoftherobot,andtheirdirectionsaretowardeast,south,westandnorth,respectively(all
perpendiculartothez axis). Thedepthchannelisnormalizedsothatthedepthvalueisboundedin
[0,1]asRGBchannels. Wetreattheentireobservationasa12-channels16×16image.
Ateachepisode,onerandomgoalarea(outoftwo)willbeeffective. Oncethecenteroftherobot
reaches the goal area, a reward of 100 is given. If the robot collides with any wall or obstacle, it
receivesanegativerewardof-2.5.
A.2 IMPLEMENTATIONDETAILS
A.2.1 MODELARCHITECTURE
ThedetailedarchitectureoftheVRNNmodel(notincludingRLnetworks)isshowninFig.A.2.The
RLnetworksarethesameasinHaarnojaetal.(2018b)exceptthattheinputisnotrawobservation,
butRNNstatesh . DetaileddiagramsofthemodelindifferentphasesareshowninFig.2.
t
A.2.2 MODELHYPERPARAMETERS
Totrainthemodelforpredictingnextobservation,Eq.5wasminimizedusingobservationtransition
data from the replay buffer. The replay buffer recorded observation transition data (x ) of recent
t
2,500 episodes (it also records a ,r ,done for the sake of RL). The data used for each training
t t t
step is 8 length-16 sequences of x , randomly sampled from the replay buffer. We used learning
t
rate0.0003andAdamoptimizer(Kingma&Ba,2014). Atrainingstepwasperformedevery3time
steps.
A.2.3 RLALGORITHM
Weuseddiscountfactorγ = 0.8forRL.Replybufferandbatchsizesettingswerethesameasin
modeltraining. Agradientstepwasperformedevery3timesteps. Notethatforbetterexploration,
we used Ornstein–Uhlenbeck process (Uhlenbeck & Ornstein, 1930) for generating motor noise,
wheretheinverse-timescaleconstantθ =0.3,similartothatinHanetal.(2020a).
OU
8
Workinprogress
OtherparametersfollowedtheoriginalSAC(withadaptiveentropyregularization)implementation
Haarnojaetal.(2018b).
A.2.4 ERRORREGRESSION
Inactiveinferencephase,thegoalobservationwasgivenbytheenvironment,whichistheexpected
observationatthecenterofarandomlyselectedgoalarea(itcanalsobesampledfromtheagent’s
replay buffer). When computing the posterior zAIf that minimizes the free energy for the goal ob-
t
servation(Ahmadi&Tani,2019;Matsumoto&Tani,2020), allthemodelweightswerefixed. At
eachtimestep,trainablevariablesAµ andAσ (meanandstandarddeviationoftheGaussiandistri-
τ τ
butionofzAIf )wererandomlyinitialized,whereτ = −1,0,1,··· ,7(whichmeanssearchingfor
t+τ
1previousstepand8futuresteps,seeEq.6andFig.2(b)). TherewerealsotrainablevariablesAc
τ
initialized as zeros, where τ = 0,1,··· ,7 and c = softmax (Ac) (see Sect. 2.4). Then,
τ τ=0,···,7 τ
wehadzAIf = µAIf = tanh(Aµ)andσAIf = softplus(Aσ)sothatthefreeenergyEq.6couldbe
t+τ τ τ τ τ
computed.
Weusedbatchsize=16(whichmeansabatchofplanswithdifferentrandominitialization)sothat
therewasahigherchancethatatleastoneinthebatchconvergingtothedesiredplan.Wetrainedthe
batchofAµ,Aσ andAc usingaRMSPropoptimizerwithdecay=0.9andlearningrate=0.005,for
τ τ τ
1,000trainingstepsateveryactualtimestep.Tochooseabetterplanfromthebatch,wefirstfiltered
out the half with higher free energies for goal observation. Then we estimated the expected time
stepstoreachthegoalofeachplannedpathbyl=
(cid:80)7
c (τ+1). Finally,weselectedtheplanin
τ=0 τ
thebatchwithsmallestl(smallestexpectedtimestepstoreachthegoal)toobtainzAIf =tanh(Aµ)
t 0
andthegoal-directedpolicycouldbeobtainedfromzAIfviathepolicynetwork.
t
9