Work in progress
GOAL -DIRECTED PLANNING BY REINFORCEMENT
LEARNING AND ACTIVE INFERENCE
Dongqi Han
Cognitive Neurorobotics Research Unit
Okinawa Institute of Science and Technology
Okinawa, Japan
Kenji Doya
Neural Computation Unit
Okinawa Institute of Science and Technology
Okinawa, Japan
Jun Tani∗
Cognitive Neurorobotics Research Unit
Okinawa Institute of Science and Technology
Okinawa, Japan
ABSTRACT
What is the difference between goal-directed and habitual behavior? We propose
a novel computational framework of decision making with Bayesian inference,
in which everything is integrated as an entire neural network model. The model
learns to predict environmental state transitions by self-exploration and generating
motor actions by sampling stochastic internal states z. Habitual behavior, which
is obtained from the prior distribution of z, is acquired by reinforcement learn-
ing. Goal-directed behavior is determined from the posterior distribution of zby
planning, using active inference which optimizes the past, current and future z
by minimizing the variational free energy for the desired future observation con-
strained by the observed sensory sequence. We demonstrate the effectiveness of
the proposed framework by experiments in a sensorimotor navigation task with
camera observations and continuous motor actions.
1 I NTRODUCTION
The mechanism of intelligent decision making is a central, frequently discussed problem in cognitive
science, neuroscience, and artiﬁcial intelligence. In particular, goal-directed planning, i.e., how to
adaptively generate plans to achieve a non-ﬁxed goal, has been of great interest since long ago
(Duncan et al., 1996; Tani, 1996; Desmurget et al., 1998).
The free energy principle (FEP) (Friston, 2010) and active inference (AIf) theory (Friston et al.,
2010; 2011) provide a Bayesian computational framework of the brain. AIf explains decision-
making as changing the agent’s belief of perception and proprioception to minimize the difference
(or surprise in FEP) between predicted observation and goal observation (Tani, 1996; Friston et al.,
2016; Matsumoto & Tani, 2020). However, existing AIf studies are usually restricted to relatively
simple environments with low-dimensional observation space and/or a known state transition model
(Friston et al., 2009; 2017; Ueltzh¨offer, 2018; Millidge, 2020). One major challenge is to the trade-
off between exploration and exploitation by simply minimizing the free energy (Ueltzh ¨offer, 2018;
Millidge, 2020; Fountas et al., 2020).
On the other hand, deep reinforcement learning (RL) has been developed in recent years as a power-
ful framework for learning reward-oriented decision making. Goal-directed tasks can be converted
to RL problems by providing a reward when the goal is achieved. Even though, modern deep RL
has surpassed many single-objective tasks (Silver et al., 2017; Vinyals et al., 2019; Li et al., 2020),
it is still challenging with multiple and/or non-ﬁxed goals (reward function) (Plappert et al., 2018).
In this paper, we suggest that RL and AIf may be both necessary to achieve efﬁcient and ﬂexible
goal-directed planning. We here propose a novel computational framework of goal-directed planning
∗Correspondence: jun.tani@oist.jp
1
arXiv:2106.09938v2  [cs.LG]  22 Jun 2021
Work in progress
Figure 1: Simpliﬁed diagrams of the the proposed framework. In the learning phase, the pre-
diction model and RL networks are trained. In the active inference phase, the internal states (latent
variable) zAIf are computed by error backpropagation to minimize the free energy for a given goal
observation (Eq. 6). Trainable variables in each phase are shown in black.
with Bayesian (variational) inference, in which RL and AIf work in a complementary manner. It
performs deep RL for exploring the environment and acquiring habitual behavior for exploitation.
Meanwhile, it learns a predictive model for sensory observation. Then, goal-directed planning can
be performed under the framework of AIf by simply minimizing the free energy with respect to a
given goal observation.
We demonstrate the effectiveness of the proposed framework in a simulated navigation task with
high-dimensional observation space and continuous action space. We show that a ﬂexible habitual
behavior was acquired via RL for reward-seeking and collision avoidance, and that near-optimal
goal-directed planning was obtained simply by optimizing the AIf objective.
2 M ETHODS
2.1 O VERVIEW
The proposed framework employs an integrated neural network model that can perform RL and
observation prediction simultaneously (Fig. 1). The core idea is based on a variational Bayesian
variable z, referred to as internal states in FEP (Friston, 2010). The internal states z entail the
probabilistic distribution of the agent’s belief about action and perception.
By exploring the environment, network connections are trained to predict observation transitions and
to fulﬁll RL objectives, as shown in learning phaseof Fig. 1. At time step t, the prior zp
t is sampled
by the RNN model and the motor action is generated via the policy network. The model also uses
actual observations to infer the posteriorzq
t−1 so that the RNN states reﬂect real environmental state
(postdiction update).
If the model is well learned, the agent can perform goal-directed planning using AIf, i.e., updating
its internal states zAIf
t to minimize the error between the predicted future observation and the goal
observation (Fig. 1, AIf phase). The motor action can then be obtained using the policy network as
in the RL case, without requiring an inverse model.
The following sections detail computation processes of the proposed framework.
2.2 O BSERVATION PREDICTION MODEL
To estimate a world model in general cases that are probably partially observable ( ˚Astr¨om, 1965),
there exist various kinds of RNN-based models (Tani, 1996; Ha & Schmidhuber, 2018; Kaiser et al.,
2020; Han et al., 2020b). Here we employ a Bayesian RNN model, as a natural choice under the
FEP framework, for predicting state transitions (Fig. 2(a)). It is an extension of the variational RNN
(VRNN) model in Chung et al. (2015), and is explained as follows.
2
Work in progress
Learning Phase
Interacting with Environment (Habitual) 
Active Inference Phase
RL
a b
c d
 e
Figure 2: Detailed diagrams of how the framework functions. (a) Learning phase. (b) AIf phase,
when current time step is t. (c) Interacting with the environment. (d) Computing RL functions. (e)
Explanations of diagram nodes and edges. In (b),(c) and (d). Action at is sampled from the policy
function πt.
First, let ht denote the RNN states, which are recurrently updated by
ht = fRNN(ht−1; zq
t), (1)
where we used the long short-term memory recurrency (Hochreiter & Schmidhuber, 1997), and zq
t
are posterior internal states which can be inferred by ht−1 and current raw observation xt:
zq
t ∼N
(
µq,t,diag(σ2
q,t)
)
,
[
µq,t,σ2
q,t
]
= fposterior(xt,ht−1). (2)
On the other hand, prior internal states zp
t can be obtained from ht−1 solely:
zp
t ∼N
(
µp,t,diag(σ2
p,t)
)
,
[
µp,t,σ2
p,t
]
= fprior(ht−1). (3)
The model predicts the next observation (for pixel observation) as
xt+1 = sigmoid (µx,t+1) , µx,t+1 = fdecoder(ht). (4)
The free energy (or negative variational lower bound) objective can be written as
F =
T∑
t
[DKL(q(zt)||p(zt)] −Eq[log (p(xt = ¯xt))] , (5)
where pand qare parameterized PDFs of prior and posteriorzt, respectively, using the reparameter-
ization trick (Kingma & Welling, 2013). We estimatelog (p(xt = ¯xt)) by cross entropy, assuming a
Bernoulli distribution of xt. The model is trained by minimizing Eq. 6 by backpropagation through
time. See Appendix A.2 for more details.
2.3 R EINFORCEMENT LEARNING
Fig. 2(c) demonstrates how action is obtained in habitual behavior. It is worth mentioning that the
connection from zp
t to hp
t shares the same synaptic weight with the connection fromzq
t to hq
t. This is
consistent with the fact thatzp
t,hp
t and zq
t,hq
t represent two sides of the same coin, but the posterior
has additional information about the actual observation ¯xt.
We used soft actor-critic (SAC) as the RL algorithm (Haarnoja et al., 2018a;b). As shown in
Fig. 2(d), the state value function, policy function and state-action value function are estimated
3
Work in progress
habituated behavior performance
0 50 100 150
thousand steps
10
20
30
40
50
60steps to finish 1 episode
habitual
goal-directed
Actual 
observation
Planned
 observation
(b)
(c)
(d)
(a) N
E
S
W
init position
goal
Goal observation
Figure 3: Results of the maze navigation task. (a) The environment in the PyBullet simulator,
where the black ball is the robot and other objects are walls and obstacles. In each episode, the robot
is initialized at a random position in the orange, shaded area in the middle. (b) The learning curve of
habitual behavior (Mean ±MSE., 40 trials). The vertical line indicates the start of learning, before
which random exploration is conducted. (c) Comparison of moving trajectories between habitual
and goal-directed behavior. (d) Actual observation and predicted observation in AIf, in an example
episode (the northern camera is shown). Numbers in the parentheses indicate cτ in Eq. 6, i.e. the
possibility of achieving the goal at that the corresponding time step.
by fV(h), fπ(h) and fQ(h,a), respectively, wherefV, fπ and fQ are fully-connected feedforward
networks. Note that during learning, gradients backpropagate (through time) to the whole RNN
model. This is beneﬁcial to shape representations of RNN states that are useful for motor control
(more details can be found in Appendix A.2.3).
2.4 A CTIVE INFERENCE BY ERROR REGRESSION
After learning a predictive model and control skills by RL, the proposed framework is able to per-
form goal-directed planning. As shown in (Fig. 2(b)), there is a goal observation provided and AIf
is conducted by minimizing the corresponding free energy by updating internal states zAIf. More
speciﬁcally, we do error regression(Ahmadi & Tani, 2017; 2019; Matsumoto & Tani, 2020), which
is to minimize the error between predicted observation and goal observation regularized by KL di-
vergence between prior and posterior, that is, the free energy with respect to the goal observation:
FAIf =DKL(qAIf(zt−1)||p(zt−1)) −log (p(xt = ¯xt)) +
N−1∑
τ=0
[
DKL(qAIf(zt+τ)||p(zt+τ)) −cτ log (p(xt+τ+1 = xgoal))
]
. (6)
The posterior internal states by AIf zAIf
t is computed using gradient descent and backpropagation
through time to minimize Eq. 6 at each time stept(see Appendix A.2.4). The previous-step posterior
zAIf
t−1 is also trained so as to represent the current true environmental state (the ﬁrst line of Eq. 6).
Note that the prediction loss at future steps is multiplied by trainable scalar variables cτ where
τ = 0,1,··· ,N −1 (we used N = 8). This is because the number of steps by which the agent can
reach the goal is unknown; therefore, we introduce cτ, which can be understood as the probability
of reaching the goal at t+ τ. Softmax is used so that we have c0 + c1 + ··· + cN−1 = 1.
3 R ESULTS
We tested the proposed framework in a simulated maze environment (Fig. 3(a)). In this task, the
goal is to reach the reward area at the southeast or northwest corner while bypass the obstacles, at
4
Work in progress
which the agent can receive a one-step reward and ﬁnish the episode. There is also a punishment
(negative reward) given if the robot collides with any wall or obstacle.
The robot makes observations using RGBD camera images. For simplicity, we mounted 4 cameras
on the robot, each of which has a ﬁxed orientation (east, south, west, north) so the observation
depends only on the robot location. The robot can move on a plane by executing a continuous-value
motor action (∆X,∆Y) with a limited maximum distance at each step.
In each episode, the reward area is randomly set at either the southeast or northwest corner, and the
robot cannot see the reward area. Therefore, if no clue about the goal is available to the agent, it
should try to reach both reward areas. This is habituated behavior. In contrast, if a goal position
to be reached is speciﬁed as the sensory observation expected at the goal position, the agent should
directly go to the corresponding position, which is goal-directed behavior. Using the proposed
framework, we performed simulations on this task.
First, the agent explored the environment with stochastic actions and collected data (observation,
action and reward sequences) in its replay buffer. The model was trained by minimizing Eq. 5 and
RL objectives with experience replay. It acquired habitual behavior by ﬁrst going to one goal area
and then to the other, if the task is not completed (Fig. 3(b) and the upper row of (c)).
If a goal is assigned and the observation at the goal position is known to the agent, goal-directed
planning can be conducted using the method described in Sect. 2.4. To demonstrate goal-directed
behavior, we visualized the moving trajectories of goal-directed behavior of an example agent (after
learning) in the lower row of Fig. 3(c), compared to trajectories of habitual behavior in the upper row.
The agent could plan actions directly toward the desired goal, in contrast to the frequent redundant
moving in habitual behavior, due to lack of goal clue. We can also see that the goal-directed behavior
shared good motor-control skills of habitual behavior, such as avoiding collisions or zigzagging. We
also demonstrate how the agent predicted future observations that lead to the given goal in Fig. 3(d).
4 S UMMARY
This work is a proof of concept illustrating how RL and AIf collaborate in a single, integrated net-
work model to perform habitual behavior learning and goal-directed planning with high-dimensional
observation space and continuous action space. Our key idea is to utilize the variational inter-
nal states z. The prior distribution zp corresponds to unconditional, habitual behavior for explo-
ration/exploitation. Goal-directed behavior is obtained by computing the posterior zAIf (including
deciding current zAIf
t and refreshing previous zAIf
t−1) using AIf to minimize the expected free energy
for a given goal observation. The effectiveness of our framework is demonstrated using a maze
navigation task with pixel images as observations and continuous motor actions.
Future work will scale up the model to more realistic and challenging environments. Also, it is
interesting to consider how habitual and goal-directed behavior switch without artiﬁcial assignment,
which may underlie an important mechanism of decision-making in the brain.
ACKNOWLEDGEMENT
The authors are funded by OIST graduate school and this material is based on work that is partially
funded by an unrestricted gift from Google.
REFERENCES
Ahmadreza Ahmadi and Jun Tani. Bridging the gap between probabilistic and deterministic models:
a simulation study on a variational Bayes predictive coding recurrent neural network model. In
International Conference on Neural Information Processing, pp. 760–769. Springer, 2017.
Ahmadreza Ahmadi and Jun Tani. A novel predictive-coding-inspired variational rnn model for
online prediction and recognition. Neural computation, pp. 1–50, 2019.
Karl J ˚Astr¨om. Optimal control of Markov processes with incomplete state information. Journal of
Mathematical Analysis and Applications, 10(1):174–205, 1965.
5
Work in progress
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems, pp. 2980–2988, 2015.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016–2019.
Michel Desmurget, Denis P´elisson, Yves Rossetti, and Claude Prablanc. From eye to hand: planning
goal-directed movements. Neuroscience & Biobehavioral Reviews, 22(6):761–788, 1998.
John Duncan, Hazel Emslie, Phyllis Williams, Roger Johnson, and Charles Freer. Intelligence and
the frontal lobe: The organization of goal-directed behavior. Cognitive psychology, 30(3):257–
303, 1996.
Zafeirios Fountas, Noor Sajid, Pedro AM Mediano, and Karl Friston. Deep active inference agents
using Monte-Carlo methods. In Advances in neural information processing systems, 2020.
Karl Friston. The free-energy principle: a uniﬁed brain theory? Nature reviews neuroscience, 11
(2):127–138, 2010.
Karl Friston, J ´er´emie Mattout, and James Kilner. Action understanding and active inference. Bio-
logical cybernetics, 104(1):137–160, 2011.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo, et al.
Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, 2016.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo.
Active inference: a process theory. Neural computation, 29(1):1–49, 2017.
Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference?
PloS one, 4(7):e6421, 2009.
Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a free-
energy formulation. Biological cybernetics, 102(3):227–260, 2010.
David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 2450–2462. Curran Associates, Inc., 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1856–1865, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018b.
Dongqi Han, Kenji Doya, and Jun Tani. Self-organization of action hierarchy and compositionality
by reinforcement learning with recurrent neural networks.Neural Networks, 129:149–162, 2020a.
Dongqi Han, Kenji Doya, and Jun Tani. Variational recurrent models for solving partially observ-
able control tasks. In Proceedings of the International Conference on Learning Representations,
2020b.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. In Proceedings of the International Conference on Learning
Representations (ICLR), 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
6
Work in progress
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao
Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement
learning. arXiv preprint arXiv:2003.13590, 2020.
Takazumi Matsumoto and Jun Tani. Goal-directed planning for habituated agents by active inference
using a variational recurrent neural network. Entropy, 22(5):564, 2020.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology, 96:102348, 2020.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464, 2018.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Jun Tani. Model-based learning for mobile robot navigation from the dynamical systems perspec-
tive. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 26(3):421–436,
1996. doi: 10.1109/3477.499793.
Kai Ueltzh¨offer. Deep active inference. Biological cybernetics, 112(6):547–573, 2018.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
review, 36(5):823, 1930.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha ¨el Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
APPENDIX A
A.1 T ASK DETAILS
Figure A.1: Conﬁguration of the robot navigation task, where the numbers indicate (x,y) coordinates
of the corresponding points.
We use PyBullet physics simulator for the experiments (Coumans & Bai, 2016–2019). Detailed
conﬁguration of the task we used can be seen in Fig. A.1. The robot is a sphere with radius=0.5. And
its action is its speed on the horizontal plane: ∆xand ∆y, where ∆x ∈[−2,2] and ∆y ∈[−2,2]
are bounded continuous variables.
7
Work in progress
Figure A.2: Architecture of our variational RNN model, where ϵp
t and ϵq
t are unit Gaussian white
noise vector, which has the same dimension as the internal states zt.
Observation of the robot is 4 RGBD cameras, each with resolution 16×16. All the cameras are ﬁxed
on the top of the robot, and their directions are toward east, south, west and north, respectively (all
perpendicular to the zaxis). The depth channel is normalized so that the depth value is bounded in
[0, 1] as RGB channels. We treat the entire observation as a 12-channels 16×16 image.
At each episode, one random goal area (out of two) will be effective. Once the center of the robot
reaches the goal area, a reward of 100 is given. If the robot collides with any wall or obstacle, it
receives a negative reward of -2.5.
A.2 I MPLEMENTATION DETAILS
A.2.1 M ODEL ARCHITECTURE
The detailed architecture of the VRNN model (not including RL networks) is shown in Fig. A.2. The
RL networks are the same as in Haarnoja et al. (2018b) except that the input is not raw observation,
but RNN states ht. Detailed diagrams of the model in different phases are shown in Fig. 2.
A.2.2 M ODEL HYPERPARAMETERS
To train the model for predicting next observation, Eq. 5 was minimized using observation transition
data from the replay buffer. The replay buffer recorded observation transition data ( xt) of recent
2,500 episodes (it also records at,rt,donet for the sake of RL). The data used for each training
step is 8 length-16 sequences of xt, randomly sampled from the replay buffer. We used learning
rate 0.0003 and Adam optimizer (Kingma & Ba, 2014). A training step was performed every 3 time
steps.
A.2.3 RL ALGORITHM
We used discount factor γ = 0.8 for RL. Reply buffer and batch size settings were the same as in
model training. A gradient step was performed every 3 time steps. Note that for better exploration,
we used Ornstein–Uhlenbeck process (Uhlenbeck & Ornstein, 1930) for generating motor noise,
where the inverse-timescale constant θOU = 0.3, similar to that in Han et al. (2020a).
8
Work in progress
Other parameters followed the original SAC (with adaptive entropy regularization) implementation
Haarnoja et al. (2018b).
A.2.4 E RROR REGRESSION
In active inference phase, the goal observation was given by the environment, which is the expected
observation at the center of a randomly selected goal area (it can also be sampled from the agent’s
replay buffer). When computing the posterior zAIf
t that minimizes the free energy for the goal ob-
servation (Ahmadi & Tani, 2019; Matsumoto & Tani, 2020), all the model weights were ﬁxed. At
each time step, trainable variables Aµ
τ and Aσ
τ (mean and standard deviation of the Gaussian distri-
bution of zAIf
t+τ) were randomly initialized, where τ = −1,0,1,··· ,7 (which means searching for
1 previous step and 8 future steps, see Eq. 6 and Fig. 2(b)). There were also trainable variables Ac
τ
initialized as zeros, where τ = 0,1,··· ,7 and cτ = softmaxτ=0,··· ,7(Ac
τ) (see Sect. 2.4). Then,
we had zAIf
t+τ = µAIf
τ = tanh(Aµ
τ) and σAIf
τ = softplus(Aσ
τ) so that the free energy Eq. 6 could be
computed.
We used batch size=16 (which means a batch of plans with different random initialization) so that
there was a higher chance that at least one in the batch converging to the desired plan. We trained the
batch of Aµ
τ, Aσ
τ and Ac
τ using a RMSProp optimizer with decay=0.9 and learning rate=0.005, for
1,000 training steps at every actual time step. To choose a better plan from the batch, we ﬁrst ﬁltered
out the half with higher free energies for goal observation. Then we estimated the expected time
steps to reach the goal of each planned path byl= ∑7
τ=0 cτ(τ+ 1). Finally, we selected the plan in
the batch with smallest l(smallest expected time steps to reach the goal) to obtain zAIf
t = tanh(Aµ
0 )
and the goal-directed policy could be obtained from zAIf
t via the policy network.
9