META-REPRESENTATIONAL PREDICTIVE CODING :
BIOMIMETIC SELF -SUPERVISED LEARNING
Alexander G. Ororbia
Rochester Institute of Technology
Rochester, New York, USA
ago@cs.rit.edu
Karl Friston
VERSES AI Research Lab
Los Angeles, California, USA
karl.friston@verses.ai
Rajesh P. N. Rao
University of Washington
Seattle, Washington, USA
rao@cs.washington.edu
ABSTRACT
Self-supervised learning has become an increasingly important paradigm in the domain of
machine intelligence. Furthermore, evidence for self-supervised adaptation, such as contrastive
formulations, has emerged in recent computational neuroscience and brain-inspired research.
Nevertheless, current work on self-supervised learning relies on biologically implausible credit
assignment – in the form of backpropagation of errors – and feedforward inference, typically a
forward-locked pass. Predictive coding, in its mechanistic form, offers a biologically plausible
means to sidestep these backprop-specific limitations. However, unsupervised predictive coding
rests on learning a generative model of raw pixel input (akin to “generative AI” approaches),
which entails predicting a potentially high dimensional input; on the other hand, supervised
predictive coding, which learns a mapping between inputs to target labels, requires human
annotation, and thus incurs the drawbacks of supervised learning. In this work, we present a
scheme for self-supervised learning within a neurobiologically plausible framework that appeals
to the free energy principle, constructing a new form of predictive coding that we call meta-
representational predictive coding (MPC). MPC sidesteps the need for learning a generative
model of sensory input (e.g., pixel-level features) by learning to predict representations of
sensory input across parallel streams, resulting in anencoder-only learning and inference scheme.
This formulation rests on active inference (in the form of sensory glimpsing) to drive the learning
of representations, i.e., the representational dynamics are driven by sequences of decisions made
by the model to sample informative portions of its sensorium.
Keywords Self-supervised learning · Predictive coding · Free energy optimization ·
Brain-inspired computing · Biological credit assignment · Biomimetic intelligence
1 Introduction
Self-supervised learning has become an increasingly important paradigm in the domain of machine intelligence
[21, 35]. Furthermore, some forms of self-supervised adaptation, such as contrastive formulations — which
learn how to invert the process generating data samples [ 120]— have emerged in computational neuroscience
and brain-inspired computing [81, 90]. Nevertheless, current work on self-supervised learning (SSL) relies on
biologically-implausible credit assignment – in the form of backpropagation of errors (backprop) – and inference
– typically a forward-locked feedforward pass [46, 80]. A scheme that could conduct this kind of learning in a
neurobiologically-plausible manner, i.e., in a backprop-free manner, would be valuable. However, current important
computational and mechanistic frameworks, including most predictive coding schemes [92, 94, 75, 100], — which
provide viable accounts of neurobiological inference schemes (via message passing) and credit assignment (via
local plasticity rules) — are primarily formulated for learning complex generative models of raw sensory input or
mapping functions between input and supervisory signals. These unsupervised and supervised forms of predictive
coding, however, do not speak to a reverse perspective of neuronal inference and learning:what might predictive
coding look if it only learned a generator of latent states (an encoder only), as opposed to a generator of sensory
states (a generative decoder)? This could open the door to a form of self-supervised predictive coding focused on
learning distributed representations of sensory stimuli without explicitly modeling high dimensional inputs.
arXiv:2503.21796v1  [cs.NE]  22 Mar 2025
Preprint
In this work, we invert the premise of predictive processing [ 20, 107, 26, 15, 105] from top-down generative
learning to bottom-up representation acquisition, casting the goal of free energy [32, 27] minimization as prediction
in latent distributed representation spaces. To do so, we draw inspiration from how the visual system [23, 55, 69]
processes stimuli through central and peripheral streams and eye movements such as saccades (i.e., active vision).
Concretely, we frame inference and learning in the context of a predictive coding (PC) scheme that comprises
an architecture of neuronal streams, some of which process central (high-resolution) views of the input while
others process peripheral (low-resolution) ones. These central and peripheral streams interact by predicting the
dynamics/behavior of one another. As a result, we propose a generalization of predictive coding that conducts a
form of encoder-only self-supervised learning that we call meta-representational predictive coding (MPC). Our
work makes the following contributions to biomimetic intelligence and self-supervised learning:
• We present and formulate a framework for biologically-plausible inference and credit assignment that
rests on learning distributed representations of sensory input in a self-supervised manner.
• Casting self-supervised neural computation and credit assignment within and between streams results
in synaptic plasticity based on local neural statistics and inference conducted in a layer-wise parallel
fashion. This shows that a generative model can be learned without predicting raw sensory input (as in
machine learning implementations of predictive processing) by instead predicitng latent activity across
visual streams; this further obviates the need for common SSL mechanisms such as the production of
positive/negative examples as in contrastive learning.
• We demonstrate that MPC produces a global encoding of a sensory stimulus through an iterative sampling
and processing of portions (subsets of variables) of raw input, inspired by the saccades that biological
eyes enact, yielding a scalable scheme that is agnostic to the dimensionality of sensory data – the model’s
representations are shown to be highly effective in several downstream supervised tasks.
• This kind of (biomimetic) PC incorporates an enactive perspective on prediction: the coordinates of where
an MPC circuit is looking inform the dynamics of its constituent neuronal units, offering a stepping stone
towards models of active vision and inference such as active predictive coding [93, 91].
2 Representational Predictive Coding: Cross-Circuit Latent Dynamics and Learning
Notation. In this work, we use ⊙ to indicate the Hadamard product (or element-wise multiplication) and · to
denote matrix/vector multiplication. (v)T is the transpose of v. Matrices/vectors are depicted in bold font, e.g.,
matrix M or vector v (scalars shown in italics). zj will refer to the jth element of vectorz. < a, b > denotes vector
concatenation along the column dimension; i.e., the dot or inner product. Finally, ||v||2 denotes the Euclidean
norm of vector v. Sensory input has shape x ∈ RJ0×1 (J0 is the number of input features), and a neural layer has
shape zℓ ∈ RJℓ×1 (Jℓ is the number of neurons for layer ℓ). Matrix flattening (to a column vector of length equal
to the product of the matrix’s number of column and rows) is denoted as Flat().
2.1 Sensory Stimuli Processing: Eye Structure and Movements
As a starting point for how we structure the requisite neuronal model, we formulate its ability to selectively sample
its sensorium. In terms of the model structure, we draw inspiration from the functional anatomy [118] (of animals
and humans) in terms of central and peripheral vision [108, 112].1 In visual processing [96], central and peripheral
vision both play an important role. Central vision, further decomposed into foveal (extending to one degree of
eccentricity from the visual field’s center containing the highest density of cone receptors with highest resolution
[89, 16, 56]) and parafoveal (extending 4-5 degrees of eccentricity yet containing a high density of slightly
lower-resolution rod receptors [ 95, 111]) vision reports higher-resolution, detailed sensory information while
peripheral vision focuses on encoding coarser-grained, lower-resolution data features. Due to its higher density (and
smaller receptive field size) of rods and cones, central vision is thought to be important for high spatial frequency
recognition tasks [104, 68] (such as recognizing an object or face). On the other hand, peripheral vision, with the
highest proportion of rods at the lowest spatial resolution, is generally viewed as important for low spatial frequency
tasks that require obtaining a global gist of a scene [103, 60, 57] (or the “bigger picture” view). While it is the case
that vision tends to call on central and peripheral vision differently, depending on the task [112], it is clear that
these two visual streams are complementary in constructing a complete representation of the stimuli being observed
1We regard this morphological separation underlying central/peripheral cortical anatomy as a useful biological mean-field
approximation (MFA). There are many MFAs that emerge in biological self-organization and the general idea behind biological
manifestations of inference-and-learning that leverage them is that the biological system will work to “repair” the falsehood(s)
induced by such MFAs. Thus, a neural system, like the one we propose in this work, must work to compensate for the statistical
independence assumptions between central and peripheral structures thus requiring mechanisms for passing signals between
these specialized regions. This motivated our (later-described) message-passing scheme that links these structures.
2
Preprint
at any instant. Neuroscientific evidence, in the form of brain-imaging studies, further demonstrates that orderly
peripheral and central representations form/emerge in both low-level retinotopic visual areas, i.e., V1 to V4, as well
in higher areas/regions that characterize the ventral temporal cortex [53, 58, 40, 38, 2] as a result of the inference
and learning, evinced as visual perception or recognition (of faces, objects, or scenes). We draw inspiration from
the structural organization underwriting visual perception – the visual processing afforded by foveal, parafoveal,
and peripheral viewing – in constructing a neuronal circuit that (loosely) emulates this computational architecture.
Furthermore, we present a predictive coding perspective [27, 100] on the accompanying neuronal circuit’s message
passing and plasticity – our neuronal model’s inference and synaptic adjustments are driven by the predictions
induced across foveal, parafoveal, and peripheral streams/systems (in service of optimizing free energy [27]).
Figure 1: Illustration of two consecutive observations
of an image. Shown is one of the digits processed by the
MPC scheme over the course of two consecutive saccade-
produced glimpses. On the left is the full source image with
a red dashed box showing the approximate subspace sam-
pled by the glimpse. The right panels, within the expanded
dot-dashed rectangle, show how the sampled data within the
dashed box on the left is converted into six input represen-
tations, i.e., four overlapping “fovea patches”, a “parafovea
patch”, and a “peripheral patch”.
In addition to structure, we draw inspiration from
the oculomotor system that underwrites active vi-
sion [117, 115, 87, 72]. The ensuing visual palpa-
tion (and implicit epistemic foraging) [ 41] can be
roughly broken down into four key movements: sac-
cades, smooth pursuit movements, and vergence and
vestibulo-ocular movements. Saccades [54, 50, 24]
are rapid, ballistic (jumpy/jerky) movements which
change the eye’s fixation point, the movement/shift
of which ranges from smaller (in tasks such as read-
ing) to larger saccades (examining a scene). Sac-
cades are generally more involuntary (unconscious)
than voluntarily and can, from a modeling perspec-
tive, appear to be more itinerant in nature. Smooth
pursuit movement [98] entails slower movements
that focus on keeping a moving stimulus on the fovea
and are more voluntary in nature. Vergence move-
ment [88] works toward aligning the fovea of each
eye with targets at different distances (from the ob-
server) whereas vestibulo-ocular movements [113]
serve to stabilize the eyes relative to the observer’s
niche (working to compensate for head movement,
preventing the slippage of visual stimuli on the retina’s surface as the head moves).
For the purposes of this study, we focus on saccades as the driver of our model’s information foraging, since the
saccade is among the most common eye movements in humans (during waking hours) [19, 54]. In future research,
we will extend our model to incorporate contextual control [99] to drive voluntary saccades and smooth pursuit.
Using involuntary saccades means that our self-supervised models use an approximate of the quick, jumpy saccadic
movements of biological eyes to extract partial information (“glimpses”) from the visual scene, e.g., a pixel image.
Formally, we treat an observation at time tg, i.e., o(tg) (a single static image or a frame sampled from a video; tg
marks global time in milliseconds), as a small temporary environment from which our neural models extract a fixed-
length trajectory of glimpses produced as the result of randomly generated saccades (or, in future work, a motor-
control policy [93]), yielding the sequence S =
n 
g(0), a(0)

,
 
g(1), a(1)

, ...,
 
g(k), a(k)

, ...,
 
g(K), a(K)
o
where g(k) is the k-th saccade-driven “glimpse” of the sensory input, a(k) ∈ [−1, 1]2×1 is a saccadic action or
x-y coordinate vector recording the chosen center of the fixation-point of glimpse g(k), and K is the maximum
number of steps taken.
Each glimpse vector g(k) is made up of several groups (pixel patches) sampled from the observation o(tg).
Specifically: a combination of C “foveal” views (8 × 8 pixel patches), F “parafoveal” views (16 × 16 patches),
and P “peripheral” views (24 × 24 patches). We choose C = 4 (four overlapping foveal views, arranged in a
2 × 2 grid), F = 1 (one parafoveal view), and P = 1 (one peripheral view).2 Figure 1 illustrates the result of a
two-step (K = 2) saccade sequence over an image (of the digit zero, extracted from the MNIST database): the red
dot-dashed box shows the four foveal and the single parafoveal and peripheral patches used to create g(k).
To obtain the final glimpse vector, all foveal, parafoveal, and peripheral views (centered around the same center-
point) are first average pooled to always be the shape of S × S pixels, vectorized (i.e., flattened), and concatenated
2In the appendix, we present the results of preliminary experimentation justifying this particular arrangement of streams.
3
Preprint
to obtain g(k) ∈ R((C+F+P)∗(S∗S))×1. This means that the vector g(k) produced by the k-th saccade is:
g(k) = (< g1(k), g2(k), ...,gv(k), ...,gV (k) >)T (1)
where V = C + F + P, brackets < · > denote vector concatenation, and, specifically, indices v = 1, 2, 3, 4 would
correspond to flattened foveal views index v = 5 would correspond to a flattened parafoveal view and index v = 6
would correspond to a flattened peripheral view . See supplement for technical details on constructing g(k).
The above process means that any g(k) is a collection of sampled sub-spaces of the observation o(tg), represented
in terms of several higher-resolution (smaller/close-up) views and several lower-resolution (larger/zoomed out)
features. Although this scheme was designed for visual perception, a similar patch extraction process could
be considered for other sensory domains, e.g., raw audio waveform representations, where relevant anatomical
knowledge, e.g., the ear, spiral ganglion neurons in the cochlea, could be used to motivate the sampling scheme.
2.2 Dynamic Prediction of Latent Space Characteristics
To describe the MPC circuit model, we start with the objective that it seeks to optimize. In effect, we require
predictions to be made with respect to only “parts” of its internal representations, i.e., predictions are made in
portions of latent representation space. To decide what makes predictions (and what the targets of these will be), we
treat the circuit as an architecture of foveal, parafoveal, and peripheral streams where each stream is specialized to
receive and encode only one foveal, parafoveal, or peripheral input. Crucially, this architecture of streams, further
inspired by the multi/dual-streams hypothesis [65, 36]3, must learn to predict the activity dynamics of one another;
this means that each stream is continually guessing what the other streams are encoding (and each must adjust its
own activities based on how wrong its guesses are). This guessing game results in a lateral, cross-stream prediction
scheme, of which there are many variations that could be studied (only four are considered in this work). This type
of prediction scheme enables neuronal circuits to predict the statistical properties of a latent space without making
“downward” (decoder-oriented) predictions of raw sensory input. From this perspective, a successful architecture of
streams would be one that seeks consistency or coherence among distributed representations.
Within an MPC architecture, each individual stream, arranged in a heterarchical or hierarchical structure, fol-
lows a variational free energy (VFE) [ 25, 26, 27] gradient flow that is driven by at least one other stream.
In this work, we take this to mean that one stream, driven by a particular view of the sensory stimulus (at
one scale/resolution, e.g., a foveal view), seeks to predict the activity values of another stream that is driven
by a different yet complementary view of the same stimulus (e.g., another view but possibly at a different
scale/resolution, e.g., a peripheral view). Based on the sensory views produced by the saccades in Section 2.1,
the MPC architecture will consist of several “foveal” neuronal streams, “parafoveal” streams, and “peripheral”
streams that seek to predict one another, resulting in a message passing scheme combining intra-stream message
passing (internally-communicated mismatch signals) and inter-stream message passing (mismatches between
streams). For a general architecture made of V = C + F + P streams (all are assumed to have the same num-
ber of L layers and ℓ = 0 indexes the sensory input layer), with v-th stream composed of synaptic parameters
Θv = {Wℓ,v, Σℓ,v, Aℓ,v,1, Rℓ,v,1, Σℓ,v,1
C , ...,Aℓ,v,V , Rℓ,v,V , Σℓ,v,V
C }L
ℓ=1, the resulting free energy functional for
the v-th stream—which tries to predict the latent representations of any other stream (q ̸= v) as well as possibly
itself (v = q)—is given by:
Fv(Θv) =
LX
ℓ=1
X
q
N

zℓ,q(t); µℓ,v,q
C , Σℓ,v,q
C

| {z }
Cross-Representation Term
+
LX
ℓ=1
N
 
zℓ,v(t); µℓ,v, Σℓ,v
| {z }
Residual Energy
+ Ω

Θv

| {z }
Synaptic prior
(2)
=
LX
ℓ=1
X
q
N

zℓ,q(t); µC
 
zℓ,v(t), Rℓ,v,q, Aℓ,v,q
, Σℓ,v,q
C

+
LX
ℓ=1
N

zℓ,v(t); µ
 
zℓ−1,v(t); Wℓ,v
, Σℓ,v

+
X
p,i,j
N

Θv[p]ij; 0, λw

3Note that the two-streams hypothesis specifically refers to specialized circuitry related to “what” (ventral) and “where” (dor-
sal) pathways. Although our architecture does not specifically implement the dorsal and ventral pathways in a neuroanatomically
faithful manner, it does embody the spirit of the what-where distinction and implicit factorisation or mean-field approxi-
mation. Specifically, our model’s foveal streams acquire fine-grained information (high-resolution stroke/arc components)
whereas the parafoveal and peripheral streams acquire coarse-grained information (low-resolution, object/part chunks) within
which the finer-grained information is situated, i.e., the foveal visual primitives indicate what is being detected whereas the
parafoveal/peripheral streams indicate where the primitives can be found in the context of the “bigger picture”.
4
Preprint
(a) Flow of predictions (solid blue arrows).
 (b) Message passing (dashed black arcs) flow.
Figure 2: Illustration of the message passing in MPC. For generative predictive coding (GPC) and meta-
representational predictive coding (MPC), depicted is: (a) the flow/directional pattern of predictions made (solid
blue arrows, which indicate neuronal populations that produce a prediction) in GPC versus MPC, and ( b) the
flow/direction of message passing (dashed black arcs, which indicate feedback pathways that carry prediction
errors) that result from GPC versus MPC prediction patterns (in sub-figure a). Solid gray boxes indicate neuronal
populations encoding latent states, while green diamonds indicate populations of error neurons. Both types of PC
represent the same number of latent states; the MPC shown is an architecture of two streams where stream “A” is
shown processing foveal sensory information and stream ”B” processes peripheral sensory information.
where the three terms can be understood as follows. The cross-representation term dictates that, at time t, any
non-sensory layer ℓ >0 with activity zℓ,v(t) in the v-th stream predicts the activity valueszℓ,q of the corresponding
q-th target stream – the prediction connections from stream v’s layer ℓ convey the mean µℓ,v,q
C and covariance
Σℓ,v,q
C of that layer’s prediction of zℓ,q. The residual energy term captures the fact that each individual stream is
hierarchically structured, where every layer ℓ of neuronal units attempt to minimize the prediction error between its
activity and the prediction of this activity by layer ℓ + 1. The synaptic prior term is synaptic decay or, in other
words, a zero-mean Gaussian prior (with standard deviation λw) placed over the v-th stream’s plastic synapses
represented by the parameters Θv.4 Finally, for the entire MPC architecture, the “ensemble free energy” would be
the combination of all of the individual stream’s VFEs: F(Θ) = PV
v=1 Fv(Θv).
Neuronal architecture and dynamics. We next provide a mechanistic description (see Figure 2 for a depiction)
of the V streams (v ∈ {1, 2, ..., v, ..., V}) that compose an MPC architecture, which optimize the VFE F. Each
layer of the v-th stream encodes expectations (mean values) that are parameterized as neuronal (population) activity:
µℓ,v = µ
 
zℓ−1,v(t); Wℓ,v
= Wℓ,v · ϕℓ−1 
zℓ−1,v(t)

(3)
where ϕℓ−1() denotes the element-wise nonlinearity applied to the ℓ − 11-th layer’s state values zℓ−1,v(t).5 Wℓ,v
is a matrix that contains the (intra-stream) predictive synaptic efficacies for the v-th stream. Note that the bottom
(sensory) layer ℓ = 0 of an MPC stream has no nonlinearity, i.e., ϕ0(z0,v) = z0,v (the identity), and is clamped to
the relevant portion of the sensory input glimpse, i.e., this means that z0,v(t) = gv(k), where the v-th stream is
provided with the v-th view of the glimpse vector g(k). We furthermore set the intra-stream covariance parameters
to be scaled identity matrices Σℓ,v = σIℓ,v (where σ >0), which simplifies VFE optimization.6
4Θv is a tuple containing all of synaptic efficacy matrices for the v-th MPC stream; p retrieves the p-th synaptic parameter
matrix from Θv, whereas i and j index a particular synapse within the p-th matrix, i.e., Θv[p]ij returns a scalar value.
5Neurobiologically, in this work, we refer to values as pre- and post-synaptic depending on their relationship to the following
linear algebraic transformation: a = W · b; a contains the post-synaptic values, b contains the pre-synaptic values, and W
contains the synaptic efficacies themselves.
6We remark that incorporating dynamics inherent to neuronal implementations of precision-weighting, e.g., such as the
precision implementation in [109], would be useful for more complex (modeling) tasks.
5
Preprint
In order for the v-th stream to make predictions of the q-th stream, each layer ℓ >0 of neurons is further equipped
with lateral synaptic connections. This means that the cross-representation mean µℓ,v,q
C emitted by the v-th stream
is parameterized as follows:
µℓ,v,q
C = µC
 
zℓ,v(t); Rℓ,v,q, Aℓ,v,q
= Rℓ,v,q · ϕℓ

zℓ,v(t)

+ Aℓ,v,q · a(t) (4)
where Rℓ,v,q contains the cross-stream prediction synapses (emitting from stream v to stream q) and Aℓ,v,q
contains the action conditional, afferent synapses. a(t) ∈ [−1, 1]2 is the action taken by the MPC model at time
t, specifically a two-dimensional vector encoding the chosen coordinates (positional coordinates of the sensory
input relative to the MPC scheme, as described in Section 2.1 and the supplement). Furthermore, for additional
simplicity, we set inter-stream covariances to be scaled identity matrices, i.e., Σℓ,v
C = σIℓ,v where σ >0. Inspired
by [75]—which demonstrated that lateral competition is useful for learning generative models—the activation
function ϕ() that we chose induces a fast form of lateral competition (without requiring physical lateral synapses)
in the internal layers. Specifically, we chose ϕ() to promote high levels of sparsity within each stream, similar in
spirit to the part-whole spiking model proposed in [34]; given neuronal activities zℓ,v as its argument, the activation
ϕ(zℓ,v) can be written out as follows:
NWTA(zℓ,v) =
(
zℓ,v
j zℓ,v
j ∈ {Nw largest elements of zℓ,v}
0 otherwise (5)
which is the N-winners-take-all (NWTA) function [1] where only the Nw neurons with highest values within
the layer/group zℓ,v emit a non-zero firing rate (the rest that lose this cross-neuron competition will emit a zero).
Although this winner-take-all function worked well for the experiments carried out in this study, future work would
benefit by considering extensions, such as incorporating a duty-cycle [1] to promote cooperation and the sharing of
firing responsibilities among neurons within a group.
Crucially, embedded within each layerℓ >0 of an MPC stream is a set of prediction error neurons that compute the
mismatch signals reporting how far off each layer’s predictions are from their corresponding targets. There are two
kinds of error neurons, which result from the free energy functional of Equation 2, for every layer – intra-stream
error units eℓ,v and inter-stream error units eℓ,v,q
C . These two kinds of error neurons can be written down as:
eℓ,v = zℓ,v(t) − µℓ,v, // Intra-stream mismatch signals (6)
eℓ,v,q
C = zℓ,q(t) − µℓ,v,q
C // Inter-stream mismatch signals (7)
where we notice that mismatch signals will be produced as a result of either local predictions of intra-stream activity
(within v), i.e., µℓ,v attempting to guess zℓ,v(t), or inter-stream activity between v and q, i.e., µℓ,v,q
C attempting to
guess zℓ,q(t). Driven by the error neurons of Equations 6 and 7, the dynamics of the neuronal cells within each
layer of an MPC stream follow the gradient flow of free energy; this flow is presented by the following (vectorized)
ordinary differential equation (ODE):
∂F(Θ)
∂zℓ(t) = τz
∂zℓ,v(t)
∂t = −eℓ,v +

Eℓ,v
W · eℓ+1,v + Eℓ,v,q
R · eℓ,v,q
C

⊙ ∂ϕ
 
zℓ,v(t)
∂zℓ,v(t)

(8)
where τz is the neural cell membrane time constant (in milliseconds; ms) and ∂ϕ(zℓ,v)
∂zℓ,v is the partial derivative
of the activation function with respect to the neural state activities at t. Eℓ,v
W contains the v-th stream’s intra-
stream message-passing synapses whereas Eℓ,v,q
R contain its inter-stream message-passing synapses. One more
simplification—that could be applied to any MPC stream—is to set its feedback connection matrices to Eℓ,v
W =
(Wℓ,v)T and Eℓ,v,q
R = (Rℓ,v,q)T ; note that these can, alternatively, be learned with Hebbian rules, as in [94, 75].
Synaptic plasticity. Learning in an MPC stream follows the gradient flow of Equation 2 and synaptic connection
strengths are updated according to Hebbian plasticity rules. The intra-stream synapses Wℓ,v, the inter-stream
synapses Rℓ,v,q, and the action-conditional afferent synapses Aℓ,v,q of any MPC stream are updated as follows:
τw
∂Wℓ,v
∂t = −λwWℓ,v + eℓ,v ·
 
ϕ(zℓ−1,v)
T
, (9)
τw
∂Rℓ,v,q
∂t = −λwRℓ,v,q + eℓ,v,q
C ·
 
ϕ(zℓ,v)
T
, (10)
τw
∂Aℓ,v,q
∂t = −λwAℓ,v,q + eℓ,v,q
C ·
 
aℓ,vT
(11)
where τw is a synaptic plasticity time constant (in ms) and λw is a synaptic decay modulation coefficient. The
inference and learning steps in an MPC scheme are scheduled according to an expectation-maximization [17] (EM)
like process: 1) inference (E-step) is carried out in an MPC stream by applying Equation 8 using Euler integration,
6
Preprint
at
at+1
zt zt+1
zt+2
zt+2
zt
1 z
z   (t)3,1 z   (t)3,2
Peripheral 
Representation Stream
Foveal/Parafoveal
Representation Stream
Cross-stream 
predictive synapses
Neural states driven by 
glimpse decisions (and 
possibly prior states)
Figure 3: Graphical depiction of a simple dual stream, MPC architecture. A meta-representational predictive
coding (MPC) architecture works by processing inputs via two or more features of the sensory input, generally at
different resolutions which mimic the coarseness (acuity) of the spatial features extracted by the foveal/parafoveal
and the peripheral streams of the human eye. In this image, we depict a two stream architecture, where one
MPC stream produces a foveal/central representation z3,1(t) (in its third layer) of its input at time t while another
circuit produces a peripheral representation z3,2(t) of the input (also at t). The foveal MPC stream attempts to
predict the activities of the peripheral MPC circuit and vice versa (for the k-th glimpse at an image). Notice that
all MPC streams are conditioned by the actions, i.e., normalized x-y coordinates of the fixation point of all of
the foveal/parafoveal/peripheral views, taken by a saccade over the sensory input as well as possibly their prior
expectation (at time t − 1). In this work, a fixed K-length saccade sequence is produced by randomly jumping
across the sensory space, resulting in a perceptual input sampling policy. Green diamonds indicate error neuron
populations, light-gray or orange circles with slightly darker colors within denote state cell populations, light gray
arrows represent synaptic connections, dashed black circular arcs depict recurrent synapses, and blue dash-dotted
arcs denote lateral cross-circuit prediction synapses (not shown, to improve visual clarity, are feedback synapses).
for all layers ℓ >0, over a stimulus window length E = T/∆t 7; 2) then synaptic learning (M-step) is performed
by applying, via Euler integration, Equations 9, 10, and 11 for all layers ℓ >0 once. After the M-step is performed,
updated synaptic matrices are constrained to have unit column Euclidean norms.
In Figure 3, we illustrate what a simple MPC architecture with V = 2 two streams (each with L = 4 layers) would
look like; in this example, a single foveal stream predicts the activities (at each layer) of a single parafoveal stream
and vice versa; hence the ’meta’ aspect of the ensuing representations. Notice that, our free energy framework
complements self-supervised representation learning [21] schemes, such as those that work to avoid (dimensional)
collapse8 including information-maximization [22, 116, 5] and regularization approaches [3, 18]. Specifically,
MPC espouses a meta-representational narrative for self-supervised learning; representations of data features
should be able to predict one another in an internally consistent fashion. In some sense, the MPC architecture
speaks to the notion that complementary views of input patterns should yield embeddings that are ’close’ to one
another, as in some forms of masked prediction [12, 85]. However, instead of focusing on input data, MPC operates
7T is the length of stimulus presentation time for examining an input view z0,v(t) = gv(k) and ∆t is the integration time
constant; both are in milliseconds (ms).
8Dimensional collapse refers to the case where the two branches of a dual-embedding architecture (such as a Siamese neural
network [8]) degenerate to producing identical and constant output vectors. This is a degenerate outcome indicating that the
model ends up learning to simply ignore the input data.
7
Preprint
(a) GPC-fov.
 (b) MPC.
Figure 4: Structures of generative and meta-representational predictive coding schemes. Depicted are two
proposed variants of predictive coding – a “field-of-view” form of generative predictive coding (GPC-fov) and
meta-representational predictive coding (MPC) – that process the same visual scene. In this graphical example, two
portions of visual input at a particular point in time are extracted (yielding a dual view, possibly containing foveal,
parafoveal, or peripheral patch pixel information) by an eye movement process (represented by the pale green
fat arrows), such as the involuntary saccades described in Section 2.1. Specifically, we show: ( a) the proposed
GPC-fov (with two neural columns) processing a dual view of the sensory input, i.e., a variant of GPC that uses the
same information as our MPC models; and (b) the proposed MPC (with two neural columns or streams) processing
a dual view of the sensory input. Note that solid (black) arrows with open circles denote inhibitory (predictive)
synapses, dashed (black) arrows with solid squares denote a population of excitatory (message-passing) synapses,
purple boxes indicate a population of neurons encoding latent states and green diamonds denote a group of error
neurons for a specific layer. In the zoomed-in inset for sub-Figure 4b, we show the incoming and outgoing wired
connections to a single neuron within a population.
exclusively in latent space, where parallel streams encoding input effectively learn to resonate with one another as
a result of continuous prediction and message passing.
Generative predictive coding. To contrast the proposed MPC framework with standard PC [92, 75, 100] (which
we will refer to as generative PC9; GPC), we provide a brief explication of GPC’s requisite free energy functional
and its resultant dynamics. Specifically, when processing a clamped sensory input z0(t) = o(tg), a GPC circuit
works—under a dynamic expectation-maximization scheme—to optimize the following VFE:
F(Θ) =
L−1X
ℓ=0
N

zℓ(t); µℓ, Σℓ

+
X
p,i,j
N(Θ[p]ij; 0, λw) (12)
where, depending on the distribution that one assumes over sensory inputs, one can modify the above functional to
use other likelihoods at specific layers, e.g., a multivariate Bernoulli distribution for ℓ = 0 as was done in [75].
Note that the above VFE has been expressed such that it also includes the same synaptic prior used in the MPC
circuit in Equation 2. Given its goal to learn how to synthesize sensory inputs, a GPC scheme usually focuses on
processing and predicting the entire sensory input o(tg). Much as in the MPC circuit, the expectation µℓ at each
layer is produced via a linear transformation, i.e., µℓ(t) = Wℓ · ϕ(zℓ+1(t)). The covariance parameters, as in the
MPC model, are simplified to the scaled identity matrix Σℓ = σIℓ (σ >0).
In the above VFE (Equation 12) and structure for GPC, error neurons can be represented as a subtractive difference,
i.e., eℓ = zℓ(t) − µℓ(t), and message passing emerges as a consequence of using feedback synaptic connections in
tandem with these error units. Specifically, the free energy gradient flow—that the neuronal units adhere to—is the
following ODE:
∂F(Θ)
∂zℓ(t) = τz
∂zℓ(t)
∂t = −eℓ +

Eℓ · eℓ−1

⊙ ∂ϕ
 
zℓ(t)

∂zℓ(t) (13)
where Eℓ = (Wℓ)T . Note that Equation 13 (and the VFE of Equation 12) can be further modified to incorporate
additional constraints such as kurtotic priors that encourage sparsity in the latent states (we use a Laplacian prior in
9Even though all free energy-centric models learn probabilistic generative models, we emphasize the word “generative” to
emphasize the decoder-centric nature of most standard PC models.
8
Preprint
(a) MPC prediction pattern ‘-st2’.
 (b) MPC prediction pattern ‘-st3’.
 (c) MPC prediction pattern ‘-st4’.
Figure 5: Visualization of different MPC cross-circuit prediction patterns experimented with. Above are
shown three possible prediction schemes for how the individual streams interact with one another; dashed blue
arrows indicate a prediction direction (blue arrow head ends on prediction target) from which error messages flow
backwards. Note that each “Fovea”, “Parafovea”, and “Peripheral” box corresponds to a particular MPC stream
(from a top-down view).
the GPC models studied in Section 3, to recover the modeling setups of [92] and [73]). After solving Equation 13
for T/∆t steps (examining an observation z0 = o(tg)), Hebbian adjustments may be made to the synaptic weight
matrices as follows:
τw
∂Wℓ
∂t = −λwWℓ + eℓ ·
 
zℓ−1T
(14)
where the first term (right-hand side of the equality) constitutes the controllable weight decay that emerges from
the synaptic prior introduced in Equation 12; this recovers the Gaussian prior over synapses in [92]).
Note that, in this work, we further modified the above GPC model—which we named the “GPC-fov” (field-of-view
GPC) model—to (iteratively) process glimpses as in MPC. In order to do so, we changed the input that the GPC
model predicts to be z0 = g(k) and converted the GPC model’s bottom matrix W1 (or matrices W1, W2, ...
up to but not including WL) closest to the input, to a block matrix with a number of blocks set equal to the
number of foveal/parafoveal/peripheral streams. This means that, after every synaptic update (Equation 14), we
would constrain this block structure through application of a binary mask. This variation of GPC, the “GPC-fov”
model, is similar to the full patch-level model of [ 92] but, in this work, is a new variant that learns to generate
dynamically-extracted patches of different resolutions. In Figure 4, we depict the structure of various kinds of
predictive coding models we study in this work, including the classical GPC as well as the GPC-fov and MPC (both
shown, for simplicity, just processing two streams of the input).
3 Experiments
Simulation setup. To demonstrate the efficacy of our MPC framework, we simulate predictive coding of two
datasets of increasing complexity: 1) the MNIST digit recognition database [52], and 2) the Kuzushiji-MNIST
(K-MNIST) character recognition database [14]. MNIST and K-MNIST contain gray-scale 28 × 28 images from
10 categories. MNIST contains images of handwritten digits while Kuzushiji-MNIST is a challenging drop-in
replacement for MNIST, containing images depicting hand-drawn Japanese Kanji characters; in K-MNIST, each
class (out of 10 classes) corresponds to the character’s modern hiragana counterpart. The only pre-processing
applied to the images in these datasets was to normalize the pixel intensities to lie in the range of [0, 1]; note that,
whenever an image patch is extracted for a patch-level models (GPC-fov and MPC circuits), we only center it (i.e.,
subtract the mean value of patch from the patch group of pixels). For the relevant models (GPC-fov and any MPC
circuit), we process each sample pattern according to the saccade process scheme described in sub-Section 2.1.
Simulated models and baselines. We compare several circuit models (with L = 3 layers) in our experimental
simulations: 1) generative predictive coding (GPC) circuits that process the entire input image (the more traditional
PC model), including a full-image version of the classical model in [ 92], a variant of this model using ReLU
activations GPC-relu), and a variant of this GPC model using an NWTA activation with the number of winners
scaled to match the total number of winners across all neural columns in the GPC-fov and MPC models (GPC-nwta);
2) the GPC-fov described earlier, which processes the same sensory information as our MPC architecture, and; 3)
variants of our proposed MPC circuit. Specifically, we study four variants of our MPC model (V = 6 views), each
with a different topological structure that dictates how the predictions are made and how the message passing is
9
Preprint
MNIST K-MNIST
ACC (%) Dec-MSE (nats) ACC (%) Dec-MSE (nats)
BP-FNN 98.04 ± 0.03 – 90.57 ± 0.11 –
JEPA [33] 95.40 ± 0.23 – 79.65 ± 0.87 –
GPC [92, 75] 91.97 ± 0.03 1.230 ± 1.088 71.09 ± 0.98 4.8005 ± 1.099
GPC-relu 93.78 ± 0.05 3.409 ± 0.982 78.83 ± 0.32 12.003 ± 1.332
GPC-nwta 95.60 ± 0.08 5.908 ± 1.002 81.99 ± 0.03 17.288 ± 1.411
GPC-fov 96.83 ± 0.03 7.105 ± 0.113 79.95 ± 0.09 22.679 ± 0.617
MPC-st1 97.50 ± 0.15 6.200 ± 0.987 82.22 ± 0.16 19.988 ± 0.050
MPC-st2 97.81 ± 0.02 5.761 ± 1.222 85.68 ± 0.18 20.867 ± 0.154
MPC-st3 97.74 ± 0.04 3.937 ± 0.189 85.05 ± 0.13 20.618 ± 0.021
MPC-st4 97.80 ± 0.05 4.093 ± 0.059 85.48 ± 0.02 20.706 ± 0.053
Table 1: Generalization of different predictive coding circuits. Measurements of generalization accuracy (ACC,
in terms of %) and reconstruction decoder mean squared error (Dec-MSE, in terms of nats) of different types
of predictive coding (PC) schemes (mean ± standard deviation reported for 10 trials). BP-FFN is a supervised
reference model, i.e., a backprop-trained sparse feedforward neural network (with hidden ReLU activations) that
directly learned a mapping between inputs and labels. GPC is a generative PC network, GPC-fov is generative PC
reformulated to work with our saccade sensory processing scheme, and MPC is our proposed representational PC
model (MPC). A dashed suffix tag refers to a particular style of cross-circuit prediction: ‘-st1’ refers to all units
predict each other while ‘-st2’, ‘-st3’, and ‘-st4’ refer to variants of local, cross-stream prediction schemes (see
Figure 5 for visualization of the structure of the last three prediction schemes). JEPA is the encoder-only backprop
model proposed in [33] adapted to our study (to follow the same training process as well as have the same number
of parameters to ensure fair comparison).
driven across streams. While we remark that many others are possible, the four cross-circuit prediction patterns
that we studied (three of which are shown in 5) included:
• ‘-s1’ : this is the simplest—an all-to-all structure (every column predicts all other columns and themselves);
• ‘-s2’ : a single chain of local one-to-one foveal column predictions (and all parafoveal/peripheral streams
predict all foveal streams as well as themselves), as in Figure 5a;
• ‘-s3’ : a local two-neighbor (neighbor above or below and neighbor to the right or left) foveal column
prediction scheme (and all parafoveal/peripheral streams predict all foveal streams as well as themselves),
as in Figure 5b;
• ‘-s4’ : all foveal streams predict all foveal streams (whereas all parafoveal/peripheral streams predict all
foveal streams and themselves), as in Figure 5c.
Even though the GPC models work in an unsupervised fashion, they are all “decoder-centric” whereas all MPC
circuits are “encoder-centric”. Our interest is to see if our MPC scheme offers representations as useful as these
generative PC models, demonstrating that we can effectively construct encoder-centric biological models that
acquire useful distributed representations of sensory input data without having to predict raw, low-level data
features (e.g., pixel values).
We train all models on each database for 5 epochs with gradient ascent (using mini-batches of length 100). The
learning rate of the gradient ascent optimization of parameters was tuned for each model using the validation subset
of each database (we generally found the saccade-driven models preferred higher rates, while whole-image models
worked better with lower rates). For any model that used the NWTA activation—the GPC-nwta, the GPC-fov, and
all MPC circuits-we tuned—for each database for each model (using development data)—the number of winners
in the range of Nw = [10, 20] (often finding that the value of Nw = 15 yielded good results in general). Unless
stated otherwise, all saccade-driven models, i.e., GPC-fov and all MPC models, used K = 10 saccades.
To assess each model’s efficacy, we train each under the same experimental conditions and data. Since every
model is unsupervised or self-supervised, we allow each to process the data for a maximum number of epochs and
adapt parameters according to their specific dynamics and plasticity mechanisms. Once a model has completed
its unsupervised/self-supervised phase, we fix its synaptic connection strengths (disable its plasticity) and then
allow it to process the training data, validation data, and test data once, extracting its latent representation for each
data sample. If a model iteratively processes one input, we concatenated the sub-representations it produces across
the K-length saccade trajectory. The resulting representations of data samples are then used in the two following
down-stream analyses:
10
Preprint
(a) t-SNE of MPC MNIST latents.
 (b) t-SNE of MPC K-MNIST latents.
Figure 6: Visualization of MPC-acquired latent activity codes. t-SNE plots of the latent space induced by
meta-representational predictive coding (MPC). Rate codes are shown for: (A) MPC on MNIST, and (B) MPC
K-MNIST. Note: t-SNE coordinate units are dimensionless and are thus denoted as “tSNE units”).
• A log-linear classifier is fit to the latent codes of the training set (with validation latent codes used
for hyperparameter-tuning) and then evaluated on the test-set latent codes. We report the test-set error
measurements in Table 1 and compare performance against a reference backprop-trained MLP (BF-FNN).
• A single hidden-layer MLP decoder (with 1024 linear rectifier neurons in the hidden layer, trained with
gradient descent and Tikhonov regularization) was retro-fit to the training-set latent codes of our MPC
scheme (the MLP was also tuned using validation-set latent codes). This decoder’s reconstruction efficacy
was evaluated using test-set latent codes. We report mean squared error and compare the decoder’s
down-stream reconstruction against the natural reconstruction ability of the full-image GPC circuits.
3.1 Results and Analysis
Downstream usage of MPC latent codes As described before, we examined the utility of the distributed repre-
sentations acquired by our MPC models in the context of downstream classification and decoding/reconstruction.
The empirical results gathered from these experiments are summarized in Table 1. Specifically, we report the
mean and standard deviation (over 10 uniquely-seeded experimental trials) of the test-set accuracy (ACC; higher
is better) for the downstream classification probe and the test-set mean squared error (Dec-MSE; lower is bet-
ter) for the downstream decoder/reconstruction probe for all models on both MNIST and K-MNIST. Observe
Figure 7: Sample Efficiency on the MNIST Database.
Here, we plot test-set classification accuracy ( 10-trial
mean values) of our best MPC against the reference BP-
FNN (MLP) as a function of the number of samples used
to train each model. Note: x-axis was plotted on a loga-
rithmic scale to help visualise the generalization curve.
that, although all of the self-supervised models (gen-
erative and encoder-centric) do not exceed the perfor-
mance of the purely discriminative, fully-supervised
BP-FNN, our proposed encoder-only MPC scheme gets
quite close, with the MPC-st4 and MPC-st2 models pro-
ducing generalization accuracies that are only lower by
0.21-0.23 percentage points. In terms of reconstruction,
we observe that our MPC schemes facilitate the effec-
tive learning of a separate decoder, yielding decoder
MSE (Dec-MSE) scores that are comparable to the
powerful whole-image generative models (the GPC cir-
cuits), which themselves are trained to predict all of the
pixels of the images (which are expected, in most cases,
to yield the best reconstruction errors due to being spe-
cialized for reconstruction). The fact that the MPC cir-
cuits produce representations that facilitate downstream
decoders that are within a few nats of the specialized
GPC models is promising. In essence, Table 1 shows
that MPC schemes learn, in a self-supervised fashion,
distributed representations that can prove useful for
both downstream classification or reconstruction. For
reference, a decoder model trained with purely random
11
Preprint
(a) MPC MNIST receptive fields.
(b) MPC K-MNIST receptive fields.
Figure 8: Bottom-layer receptive fields acquired by an MPC scheme. The foveal, parafoveal, and peripheral
receptive fields of the bottom-most neuronal units closest to the sensory input (i.e., layer ℓ = 1 ) of a meta-
representational predictive coding (MPC) model, trained on MNIST patterns. Shown are the receptive fields for
each of the six neural streams that make up an MPC encoder circuit.
encodings (we assigned unique random vectors to data points that were the same dimension as the concatenated set
of MPC/GPC-fov representations) yields an MSE of 55.433 nats.
In Figure 7, we show the results of a small test we conducted to examine MPC’s sample efficiency on MNIST.
Specifically, we re-fit MPC-st4 and the reference BP-FNN on differently-sized training datasets—dataset sizes10
included {100, 200, 500, 1000, 5000, 10000, 25000, 50000}—and re-evaluated their generalization accuracy on
the full test-set (mean values over 10 trials are plotted). Notice that as the number of available samples declines
towards 100, the supervised BP-FNN’s generalization degrades significantly (as expected for a learning scheme
that requires labels) whereas MPC’s downstream performance remains relatively consistent (only declining by
about at most 2 percentage points). We hypothesize that MPC’s sample efficiency likely comes from the fact that it
treats each data point as a sort of “mini sensorium” via its saccade-driven sampling process. This means MPC
models work to extract reusable simpler features (such as strokes/arcs and object chunks) that appear to more
readily/easily generalize (in MPC, the lower layers capture these atomic features whereas the upper layers produce
weighted combinations of lower-level atomic features).
Qualitatively, we examined the receptive fields acquired by the neuronal units of one of our best-performing MPC
circuits, the MPC-st4, by visualizing them in terms of 2D images; the results of this examination/visualization
are shown in Figure 8. Note how the foveal receptive fields (the first four squares of 12 × 12 fields) look very
similar to those acquired by classical GPC models; such as the one of [ 92], representing a variety of different
possible strokes. These strokes are aggregated by higher-level layers of the scheme as it assembles higher-level
representations of sensory input. The parafoveal and peripheral receptive fields, i.e., the last two 12 × 12 squares
of receptive fields to the right, appear to encode either larger, more zoomed-out versions of strokes or possibly
low-resolution objects/chunks of objects (digits or Kanji characters).
Beyond examining receptive fields, we visualized the relationship between the latent codes that a trained MPC
circuit (MPC-st4) produced on both MNIST and K-MNIST’s test-sets. The results of this qualitative analysis used
t-SNE [110] to visualize the latent codes formed with respect to the test-set samples and is presented in Figure 6.
Notice that, despite never having access to the labels nor using any kind of supervisory signal, the MPC codes for
MNIST tend to cluster rather well (albeit a bit noisily in some spots), yielding ten distinct major representational
groupings with each corresponding to a different digit (labels were only used to color the t-SNE mapped latent
code points in the t-SNE figure). For K-MNIST, we see that MPC latent codes form groupings as well but there
many more groups/clusters or “sub-groupings” than the ten identified Kanji character categories; this behavior
makes sense, given that the Kanji characters are more complex and exhibit a higher degree of variety than the
handwritten digits in the MNIST database.
10Samples selected from the originally training database were randomly sampled without replacement.
12
Preprint
Assembling representations through MPC An interesting feature of the MPC model is that it represents stimuli
by encoding portions of input across saccades. This means at least several glimpses of the input are needed to obtain
a decent “bigger picture” encoding. To investigate how the number of glimpses affects an MPC scheme’s ability to
form useful representations, we measure the performance of the model—in terms of classification accuracy—in
response to the number of saccades allowed. In Figure 9, we examine the generalization of MPC (as measured
in terms of downstream, test-set classification accuracy) as a function of the number of glimpses it is allowed to
take (up to a maximum of 12 glimpses) when processing sensory input. Empirically, we notice that generalization
improves as more glimpses are taken, up to about a little above 97.8% on average (with 10 glimpses). There is,
however, a law of diminishing returns effect beyond8-10 glimpses. This saturation might be due to the fact that
enough of the sensory input was examined by the MPC circuit in order to craft a useful global representation
and further glimpses added no further information. It is likely that, for more complex sensory input (natural
images), more glimpses might be required to form effective global encodings. This effect motivates future work
to develop a complementary motor circuit to drive the selection of the MPC sampling of the input (with a bias
towards policies that entail the minimum number of saccades and that consider a better balance of representational
efficiency-efficacy). This would bring our model from an action-conditioned one to a self-driven model, much in
the spirit of active perception [66, 106] and selection through the framework of planning-as-inference [7, 31, 30].
Figure 9: Analysis of Number of Saccades and Down-
stream MNIST Accuracy. Here, we plot our best MPC
and our GPC-fov models’ performance (in terms of down-
stream test-set classification accuracy) as a function of
the number of involuntary saccades used to construct their
global representations of sensory inputs.
Given MPC’s iterative nature, when sampling sensory
input over the course of several saccades, in Figure
10, we asked what an MPC scheme is doing as it
processes a sensory stimulus throughout the course
of nine glimpses. Specifically, we examined two dif-
ferent digit patterns, i.e., a seven (in the top of Fig-
ure 10) and a zero (in the bottom of Figure 10). In
addition to depicting the raw sensory glances pro-
duced by our sensory glimpsing scheme, we show
the top four most activated receptive fields that each
neuronal stream yields in response to the observa-
tion of a glimpse (at each time step). Notice that,
for the sensory glimpse produced by each saccade,
the most highly-activated foveal receptive fields cap-
ture particular, essential characteristics of the exam-
ined input, e.g., the rotation/orientation of stroke/edge
of the overall digit pattern, while the most highly-
activated parafoveal/peripheral fields correspond to
either: 1) capturing broader feature shapes/profiles
(lower-resolution strokes and their orientations), or,
2) engaging in a form of template matching to the most relevant low-resolution object “chunk”. In some instances,
such as for the foveal streams, seemingly non-related features can appear among the more highly-activated fields,
such as a stroke or edge piece that just happens to fit “within” the stimulus area of the general feature.
Limitations: As promising as our proposed MPC framework for SSL is, there are several limitations which
afford avenues for future research and development. From a computational neuroscience perspective, while our
MPC model designs were biomimetic, leading us to a computational architecture with foveal, parafoveal, and
peripheral streams, MPC is still a loose inspiration and does not faithfully model the bio-circuitry that underlie visual
hierarchies and the oculomotor system. Future effort that modifies the MPC architecture to better emulate biological
details (e.g., crafting constrained layered structures that directly adhere to known neuroanatomy, operating with
spikes as opposed to rate-codes, etc.) may further benefit the generalization ability of models constructed within
our framing. Furthermore, it could prove fruitful to carry out neurobiological studies that ask if a cross-circuit-like
prediction/message passing scheme like that of MPC affords a useful explanation of empirical neuronal responses.
From a machine intelligence point-of-view, while this work demonstrates that MPC can extract representations that
facilitate promising downstream performance—demonstrating the viability of our biomimetic scheme—further
experimental studies will be needed, e.g., those carried out on more complex data; including natural images and
video databases. These efforts may help determine what extensions/mechanisms might be required to ensure
generalization across a greater variety of (visual) sensoria/niches. It is noteworthy that our MPC framework does
not rely on large batches11 or batch statistics/normalization [37, 13] or negative samples as many of the modern deep
learning [74, 47, 12] and neuro-mimetic schemes [44, 43, 78, 81] do. This means MPC may offer a regularization-
11We used a batch size greater than one solely to speed up simulation; MPC is inherently an online learning framework.
13
Preprint
1
 2
 3
4
 5
 6
7
 8
 9
1
 2
 3
4
 5
 6
7
 8
 9
Figure 10: A trained MPC scheme processing sensory inputs through a saccade sequence. Shown is a trained
MPC scheme iteratively processing a sensory stimulus, e.g., an image of a digit seven (top group of four rows) or a
zero (bottom group of four rows), through a series of (randomly selected) saccades. Within each group of rows,
which pertain to a particular sensory input digit, the first row shows a global view of the original input for reference,
while the other three rows show the saccade sequence taken by the MPC scheme (the bold number indicates the
saccade step k, out of nine total taken, corresponding to a particular saccade-sampled view). Within each saccade
view, the first half of the image shows the sensory-level glimpses (at step k) while the bottom half contains the top
four most highly activated receptive fields extracted from the MPC circuit in response to the input stimulus.
based [33, 4] SSL method that performs biologically-plausible inference and credit assignment. However, the
failure cases of MPC—as well as the missing heuristics needed to scale it to be competitive with modern-day
generative AI—will need to be developed. It will further be crucial to adapt and apply the analytical apparatus of
self-supervised and representational learning [35], such as characterizing problems such as dimensionality collapse
[48]. It is our hope that our MPC framework encourages exploration of brain-inspired and neuroscience-motivated
intelligence [80, 79] in the context of SSL, a space we label biomimetic self-supervised learning.
14
Preprint
Another key aspect that is missing from our framework is a mechanism for context-driven motor control12 further
inspired by machine intelligence work such as [66, 106], as it well-known that contextual task knowledge plays
an important role in determining where a biological vision system looks [99]. This next generalization is further
related to an implementation of the free energy principle known as active inference [31] and, since our MPC model
is already action-conditional (i.e., its representations are “aware” of actions taken), our framework could yield
to generalizations under active inference. In other words, it could deploy saccadic eye movements to maximize
expected information gain—or epistemic affordance—of visual samples. See [28, 84, 82, 83] for worked examples.
4 Related Work
Self-supervised learning and representations. Self-supervised learning (SSL), specifically self-supervised
representation learning [21] (SSRL), which can be viewed as a special case of unsupervised learning, strives to learn
features or (abstract) representations of data without using supervisory annotation, e.g., labels of semantic categories.
As opposed to unsupervised learning which centers around density estimation or (input data) reconstruction, SSRL
relies on what are known as “pretext tasks” or artificial tasks that exploit knowledge related to a particular input
modality (or modalities). Pretext SSL tasks can take a wide variety of forms, ranging from counting visual
primitives in a scene [71] to in-painting [119] one part of (masked out) input using other (non-masked) portions
of the input. SSRL methodology in machine intelligence research seeks to develop approaches that acquire
representations (or those that learn general features) of data that facilitate strong downstream supervised learning
performance without requiring time-consuming effort from human annotators (to produce the labels required by
supervised learning). The chosen pretext task(s) tend to be be less complex than full, raw data generative modeling,
possibly yielding representations of input that are less “distracted” by noise or irrelevant data details, with many
motivating cases coming from reinforcement learning research [9, 51, 59, 70].
In learning useful representations [6] or latent embeddings of input data, a wide variety of SSL/SSRL methods have
been studied and developed [35]. Recent approaches, particularly those that can be labeled as “joint-embedding
architectures” [5], can be broken down roughly into a few general categories: contrastive approaches, information-
maximizing / regularization approaches, or those which are driven by particular heuristics [37, 97, 13]. Contrastive
methods focus on constructing objectives that pull/attract embeddings of similar inputs (e.g., images) closer to one
another and push embeddings of dissimilar inputs away from each other. These schemes strongly rely on either:
• 1) an effective mining process for uncovering dissimilar images within a batch [12] or memory bank [42];
• 2) the design of a useful synthetic process that creates out-of-distribution or negative data examples
[43, 78]; or,
• 3) a quantization/clustering scheme [10] that assigns embeddings of dissimilar data patterns to different
clusters within a unit sphere.
Information-maximization SSL methods use objective functions that decorrelate and orthogonalise vari-
able/dimension pairs within an joint-embedding of latent vectors; this is argued to (indirectly) maximize the
information content of embedding vectors [ 5, 35]. Generally, these methods focus on either: 1) driving the
normalized cross-correlation matrix of the two embeddings (of two different, yet complementary inputs, e.g.,
two transformations of an image) produced by the architecture towards the identity [116, 5], or 2) whitening and
spreading out embedding vectors (of input) across the unit sphere [22]. Although MPC does not encode explicit
objectives that directly maximize information, its focus on encouraging “resonant”, predictive sub-representations
of different, dynamically-selected (temporally and spatially adjacent) subsets of input via a cross-stream message
passing passing scheme brings it closest to regularization-based SSL approaches, e.g., the JEPA family [33, 4].
Biomimetic self-supervised (representation) learning. With respect to biomimetic intelligence, there have been
several approaches to construct schemes that are (essentially) encoder-centric. Some approaches focus on Hebbian
plasticity [39, 63, 67]; while these models have the benefit of operating with only local pre- and post-synaptic
statistics—to drive various forms of associative learning—it is difficult to write down the cost/energy functionals
that they are optimizing, resulting in an obscure optimization-success tracking experience.13 See [45] for a worked
example with recurrent neural networks. More recent efforts include approaches that fall under the banner of
forward-only learning [49, 79], with particular approaches such as (the unsupervised forms of) forward-forward
[43] and predictive forward-forward [78] learning offering ways of conducting SSRL. Scientific inquiry along this
direction has led to insights into how systems of spiking neuronal cells could engage in self-supervised forward-
only learning [81, 62], building strong connections between machine learning, computational neuroscience, and
12Such mechanisms would prove useful for incorporating voluntary ocularmotor control as well as facilitate other types of
eye movement, such as smooth pursuit or vergence.
13Work is ongoing for uncovering the implicit objectives that these forms of plasticity might be approximating [86, 61].
15
Preprint
neuromorphic engineering. Nevertheless, these forms of neuro-mimetic learning inherit the same limitations as
related machine learning SSL contrastive methodology, i.e., they require the generation or mining of negative
samples to facilitate the proper organization of latent embeddings.
Predictive coding (PC), which is a promising biomimetic learning-and-inference scheme that has emerged from
theoretical/computational neuroscience [20, 73, 92] and been continuously developed in neuroscience-inspired
machine learning research [100, 80], primarily takes on the form of an auto-associative memory [102] structure
(thus decoder-focused) in modeling sensoria. The goal of any unsupervised PC model is to optimize its (variational)
free energy (VFE) [32, 27], leading to neuronal dynamics and message passing that follow the (gradient) flow
of this VFE, where synaptic connection strengths minimize the same VFE objective, resulting in error-guided
Hebbian plasticity. Most PC models are formulated as unsupervised associative memory engines (such as
this work’s GPC baseline models), which predict raw sensory inputs. These models acquire their distributed
representations, which constitute their underlying models-of-the-world, as a consequence of optimizing VFE
[92, 27, 11, 102, 75, 101, 100]. Beyond unsupervised reconstruction, PC has been formulated for supervised
learning, e.g., classification [114, 101, 76, 64], and for reinforcement learning/active inference [77]; although these
PC formats often do not take on a decoder/associative memory format, they generally require annotation/human
supervision to provide desired target signals or priors. This work directly recasts PC in terms of a non-auto-
associative, self-supervised learning framing—meta-representational PC (MPC)—showing that it is possible, from
a free energy principle perspective, to learn distributed representations of a sensorium without reconstructing
raw sensory inputs. In essence, MPC entails a kind of generative learning of latent causes, guided by afferent
synaptic connections that supply motor-action information to the circuit’s neuronal units (actions, in our case, being
the coordinates of saccades), further elaborated by leveraging conditional independencies [29] to reproduce the
computational architecture found in the visual system, e.g., central (foveal and parafoveal) and peripheral sensing,
and associated parvocellular and magnocellular streams in the visual cortical hierarchy.
Active predictive coding: A recent variant of PC called active predictive coding (APC) [91, 93], which is inspired
by the primacy of actions in the neocortex, utilizes a hierarchy of coupled sensory prediction and action policy
modules for solving a variety of sensory-motor tasks such as active perception, learning part-whole hierarchies
and spatial navigation. In APC, higher level latent sensory states and abstract actions change the lower-level
state prediction function and policy function respectively according to the current task using hypernetworks or
top-down modulation of lower-level networks. MPC shares with APC the emphasis on using actions (saccadic
eye movements) to generate a sequence of glimpses, which are then used for self-supervised learning. While
APC learns an eye movement policy to intelligently sample the scene according to the task at hand, the current
implementation of MPC uses randomly generated movements. On the other hand, APC relies on prediction errors
from predicting raw inputs for learning while MPC avoids input prediction and relies on prediction errors from
predicting latent states across multiple visual streams. An obvious direction for future research is to combine the
strengths of APC and MPC by learning hierarchical policy networks for intelligently sampling the sensorium while
learning task-specific and hierarchical latent state representations via predictions across visual or more broadly,
multimodal sensory streams.
5 Conclusions
In this study, we proposed a new formulation of predictive coding, meta-representational predictive coding (MPC),
which is a biologically-motivated form of self-supervised inference and learning in service of acquiring distributed
representations of sensory input. Our framework, grounded in the free energy principle, inverts the standard
premise of predictive processing from one of adapting a generative model to explain raw sensory inputs to one
of an encoder-centric scheme where representations of distinct data features predict each other. Empirically, our
results demonstrate that casting neuronal dynamics and synaptic plasticity as message passing—induced by within
and between stream prediction—offers a mechanistic explanation of how representations of sensory stimuli might
emerge. In the context of visual perception, MPC is instantiated by an architecture of visual streams that are
each concerned with processing foveal/parafoveal (high-resolution) or peripheral (low-resolution) sensory data
features. The resulting neuronal dynamics are driven by intra- and inter-stream message passing of predictions and
prediction errors.
Note the resulting framework engages with the relatively challenging problem of self-supervised learning (SSL)
of representations, side-stepping the need for positive and negative data samples (as in contrastive SSL schemes)
through cross visual stream prediction, showing how abstract encodings might emerge using only predictions
and prediction error alone (as opposed to schemes that use similarity metrics and complementary views of data
through random transformations). Our experimental simulations show that self-supervised MPC was capable of
learning representations that were useful for not only downstream discriminative learning but also for downstream
16
Preprint
reconstruction. Simulation results demonstrated that MPC was competitive with standard state-of-the-art predictive
coding and backprop-trained supervised learning, even though the framework never uses label information nor
does it need to predict high dimensional sensory data (e.g., pixels).
Acknowledgements
We would like to thank Viet Nguyen for writing the custom JEPA baseline used for the experiments in this paper.
This research was funded in whole, or in part, by the Cisco Research Gift Award #26224 (AO). This research
was funded in whole, or in part, by the Wellcome Trust [203147/Z/16/Z] (KF), the National Science Foundation
(NSF) (EFRI grant no. 2223495) (RPNR), and a Frameworks grant from the Templeton World Charity Foundation
(RPNR). For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author
Accepted Manuscript version arising from this submission.
References
[1] AHMAD , S., AND SCHEINKMAN , L. How can we be so dense? the benefits of using highly sparse
representations. arXiv preprint arXiv:1903.11257 (2019).
[2] ARCARO , M. J., M CMAINS , S. A., S INGER , B. D., AND KASTNER , S. Retinotopic organization of
human ventral visual cortex. Journal of neuroscience 29, 34 (2009), 10638–10652.
[3] ASSRAN , M., D UVAL, Q., M ISRA , I., B OJANOWSKI , P., V INCENT , P., R ABBAT, M., L ECUN, Y., AND
BALLAS , N. Self-supervised learning from images with a joint-embedding predictive architecture. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(2023), pp. 15619–
15629.
[4] BARDES , A., G ARRIDO , Q., P ONCE , J., C HEN , X., R ABBAT, M., L ECUN, Y., A SSRAN , M., AND
BALLAS , N. Revisiting feature prediction for learning visual representations from video. arXiv preprint
arXiv:2404.08471 (2024).
[5] BARDES , A., P ONCE , J., AND LECUN, Y. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. arXiv preprint arXiv:2105.04906 (2021).
[6] BENGIO , Y., C OURVILLE , A., AND VINCENT , P. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1798–1828.
[7] BOTVINICK , M., AND TOUSSAINT , M. Planning as inference. Trends in cognitive sciences 16, 10 (2012),
485–488.
[8] BROMLEY , J., G UYON , I., L ECUN, Y., SÄCKINGER , E., AND SHAH , R. Signature verification using a"
siamese" time delay neural network. Advances in neural information processing systems 6 (1993).
[9] BURDA , Y., E DWARDS , H., S TORKEY , A., AND KLIMOV, O. Exploration by random network distillation.
arXiv preprint arXiv:1810.12894 (2018).
[10] CARON , M., B OJANOWSKI , P., JOULIN , A., AND DOUZE , M. Deep clustering for unsupervised learning of
visual features. In Proceedings of the European conference on computer vision (ECCV)(2018), pp. 132–149.
[11] CHALASANI , R., AND PRINCIPE , J. C. Deep predictive coding networks. arXiv preprint arXiv:1301.3541
(2013).
[12] CHEN , T., K ORNBLITH , S., N OROUZI , M., AND HINTON , G. A simple framework for contrastive learning
of visual representations. In International conference on machine learning (2020), PMLR, pp. 1597–1607.
[13] CHEN , X., AND HE, K. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition (2021), pp. 15750–15758.
[14] CLANUWAT, T., B OBER -IRIZAR , M., K ITAMOTO , A., L AMB , A., Y AMAMOTO , K., AND HA, D. Deep
learning for classical japanese literature, 2018.
[15] CLARK , A. Surfing uncertainty: Prediction, action, and the embodied mind. Oxford University Press, 2015.
[16] CURCIO , C. A., S LOAN , K. R., K ALINA , R. E., AND HENDRICKSON , A. E. Human photoreceptor
topography. Journal of comparative neurology 292, 4 (1990), 497–523.
[17] DEMPSTER , A. P., L AIRD , N. M., AND RUBIN , D. B. Maximum likelihood from incomplete data via the
em algorithm. Journal of the royal statistical society: series B (methodological) 39, 1 (1977), 1–22.
[18] DROZDOV , K., S HWARTZ -ZIV, R., AND LECUN, Y. Video representation learning with joint-embedding
predictive architectures. arXiv preprint arXiv:2412.10925 (2024).
17
Preprint
[19] EFRON , N. 5 - lid wiper epitheliopathy. In Contact Lens Complications (Fourth Edition), N. Efron, Ed.,
fourth edition ed. Elsevier, Philadelphia, 2019, pp. 53–68.
[20] E LIAS , P. Predictive coding–i. IRE transactions on information theory 1, 1 (1955), 16–24.
[21] ERICSSON , L., G OUK , H., L OY, C. C., AND HOSPEDALES , T. M. Self-supervised representation learning:
Introduction, advances, and challenges. IEEE Signal Processing Magazine 39, 3 (2022), 42–62.
[22] ERMOLOV , A., S IAROHIN , A., S ANGINETO , E., AND SEBE , N. Whitening for self-supervised representa-
tion learning. In International conference on machine learning (2021), PMLR, pp. 3015–3024.
[23] FELLEMAN , D. J., AND VAN ESSEN , D. C. Distributed hierarchical processing in the primate cerebral
cortex. Cerebral cortex (New York, NY: 1991) 1, 1 (1991), 1–47.
[24] FINDLAY , J. M. Saccadic eye movement programming: Sensory and attentional factors. Psychological
research 73, 2 (2009), 127–135.
[25] FRISTON , K. A theory of cortical responses. Philosophical Transactions of the Royal Society B: Biological
Sciences 360, 1456 (2005).
[26] F RISTON , K. Hierarchical models in the brain. PLoS computational biology 4, 11 (2008), e1000211.
[27] FRISTON , K. The free-energy principle: a unified brain theory? Nature reviews neuroscience 11, 2 (2010),
127–138.
[28] FRISTON , K., A DAMS , R. A., P ERRINET , L., AND BREAKSPEAR , M. Perceptions as hypotheses: saccades
as experiments. Frontiers in psychology 3(2012), 151.
[29] FRISTON , K., AND BUZSÁKI , G. The functional anatomy of time: what and when in the brain. Trends in
cognitive sciences 20, 7 (2016), 500–511.
[30] FRISTON , K., F ITZ GERALD , T., R IGOLI , F., S CHWARTENBECK , P., AND PEZZULO , G. Active inference:
a process theory. Neural computation 29, 1 (2017), 1–49.
[31] FRISTON , K., F ITZ GERALD , T., R IGOLI , F., S CHWARTENBECK , P., P EZZULO , G., ET AL . Active
inference and learning. Neuroscience & Biobehavioral Reviews 68 (2016), 862–879.
[32] FRISTON , K., AND KIEBEL , S. Predictive coding under the free-energy principle. Philosophical transac-
tions of the Royal Society B: Biological sciences 364, 1521 (2009), 1211–1221.
[33] GARRIDO , Q., A SSRAN , M., B ALLAS , N., B ARDES , A., N AJMAN , L., AND LECUN, Y. Learning and
leveraging world models in visual representation learning. arXiv preprint arXiv:2403.00504 (2024).
[34] GEBHARDT , W., AND ORORBIA , A. G. Time-integrated spike-timing-dependent-plasticity. arXiv preprint
arXiv:2407.10028 (2024).
[35] GEIPING , J., G ARRIDO , Q., F ERNANDEZ , P., BAR, A., P IRSIAVASH , H., L ECUN, Y., AND GOLDBLUM ,
M. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210 (2023).
[36] GOODALE , M. A., AND MILNER , A. D. Separate visual pathways for perception and action. Trends in
neurosciences 15, 1 (1992), 20–25.
[37] GRILL , J.-B., S TRUB , F., A LTCHÉ , F., TALLEC , C., R ICHEMOND , P., BUCHATSKAYA , E., D OERSCH , C.,
AVILA PIRES , B., G UO, Z., G HESHLAGHI AZAR , M., ET AL . Bootstrap your own latent-a new approach
to self-supervised learning. Advances in neural information processing systems 33 (2020), 21271–21284.
[38] GRILL -SPECTOR , K., AND MALACH , R. The human visual cortex. Annu. Rev. Neurosci. 27, 1 (2004),
649–677.
[39] GRINBERG , L., H OPFIELD , J., AND KROTOV, D. Local unsupervised learning for image analysis. arXiv
preprint arXiv:1908.08993 (2019).
[40] HASSON , U., H AREL , M., L EVY, I., AND MALACH , R. Large-scale mirror-symmetry organization of
human occipito-temporal object areas. Neuron 37, 6 (2003), 1027–1041.
[41] HAYHOE , M., AND BALLARD , D. Eye movements in natural behavior. Trends in cognitive sciences 9, 4
(2005), 188–194.
[42] HE, K., F AN, H., W U, Y., X IE, S., AND GIRSHICK , R. Momentum contrast for unsupervised visual
representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition (2020), pp. 9729–9738.
[43] HINTON , G. The forward-forward algorithm: Some preliminary investigations. arXiv preprint
arXiv:2212.13345 (2022).
18
Preprint
[44] HINTON , G. E. Training products of experts by minimizing contrastive divergence. Neural computation 14,
8 (2002), 1771–1800.
[45] ISOMURA , T., AND FRISTON , K. Reverse-engineering neural networks to characterize their cost functions.
Neural computation 32, 11 (2020), 2085–2121.
[46] JADERBERG , M., C ZARNECKI , W. M., O SINDERO , S., V INYALS , O., G RAVES , A., S ILVER , D., AND
KAVUKCUOGLU , K. Decoupled neural interfaces using synthetic gradients. In International conference on
machine learning (2017), PMLR, pp. 1627–1635.
[47] JAISWAL , A., B ABU , A. R., Z ADEH , M. Z., B ANERJEE , D., AND MAKEDON , F. A survey on contrastive
self-supervised learning. Technologies 9, 1 (2020), 2.
[48] JING , L., V INCENT , P., L ECUN, Y., AND TIAN , Y. Understanding dimensional collapse in contrastive
self-supervised learning. arXiv preprint arXiv:2110.09348 (2021).
[49] KOHAN , A., R IETMAN , E. A., AND SIEGELMANN , H. T. Signal propagation: The framework for learning
and inference in a forward pass. IEEE Transactions on Neural Networks and Learning Systems (2023).
[50] KRAUZLIS , R. J. The control of voluntary eye movements: new perspectives. The Neuroscientist 11, 2
(2005), 124–137.
[51] LASKIN , M., S RINIVAS , A., AND ABBEEL , P. Curl: Contrastive unsupervised representations for
reinforcement learning. In International conference on machine learning (2020), PMLR, pp. 5639–5650.
[52] L ECUN, Y. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ (1998).
[53] LEVY, I., H ASSON , U., A VIDAN , G., H ENDLER , T., AND MALACH , R. Center–periphery organization of
human object areas. Nature neuroscience 4, 5 (2001), 533–539.
[54] LIVERSEDGE , S. P., AND FINDLAY , J. M. Saccadic eye movements and cognition. Trends in cognitive
sciences 4, 1 (2000), 6–14.
[55] LIVINGSTONE , M., AND HUBEL , D. Segregation of form, color, movement, and depth: anatomy, physiol-
ogy, and perception. Science 240, 4853 (1988), 740–749.
[56] LOSCHKY , L., M CCONKIE , G., Y ANG , J., AND MILLER , M. The limits of visual resolution in natural
scene viewing. Visual Cognition 12, 6 (2005), 1057–1092.
[57] LOSCHKY , L. C., S ETHI , A., S IMONS , D. J., P YDIMARRI , T. N., O CHS , D., AND CORBEILLE , J. L.
The importance of information localization in scene gist recognition. Journal of Experimental Psychology:
Human Perception and Performance 33, 6 (2007), 1431.
[58] MALACH , R., L EVY, I., AND HASSON , U. The topography of high-order human object areas. Trends in
cognitive sciences 6, 4 (2002), 176–184.
[59] MAZZAGLIA , P., V ERBELEN , T., AND DHOEDT , B. Contrastive active inference. Advances in neural
information processing systems 34 (2021), 13870–13882.
[60] MCCOTTER , M., G OSSELIN , F., S OWDEN , P., AND SCHYNS , P. The use of visual information in natural
scenes. Visual Cognition 12, 6 (2005), 938–953.
[61] M ELCHIOR , J., AND WISKOTT , L. Hebbian-descent. arXiv preprint arXiv:1905.10585 (2019).
[62] MERKEL , C., AND ORORBIA , A. G. Contrastive learning in memristor-based neuromorphic systems. In
2024 IEEE Workshop on Signal Processing Systems (SiPS)(2024), IEEE, pp. 171–176.
[63] MICONI , T. Hebbian learning with gradients: Hebbian convolutional neural networks with modern deep
learning frameworks. arXiv preprint arXiv:2107.01729 (2021).
[64] MILLIDGE , B., T SCHANTZ , A., AND BUCKLEY , C. L. Predictive coding approximates backprop along
arbitrary computation graphs. Neural Computation 34, 6 (2022), 1329–1368.
[65] MISHKIN , M., AND UNGERLEIDER , L. G. Contribution of striate inputs to the visuospatial functions of
parieto-preoccipital cortex in monkeys. Behavioural brain research 6, 1 (1982), 57–77.
[66] MNIH , V., H EESS , N., G RAVES , A., ET AL . Recurrent models of visual attention. Advances in neural
information processing systems 27 (2014).
[67] MORAITIS , T., T OICHKIN , D., J OURNÉ , A., C HUA , Y., AND GUO, Q. Softhebb: Bayesian inference
in unsupervised hebbian soft winner-take-all networks. Neuromorphic Computing and Engineering 2, 4
(2022), 044017.
19
Preprint
[68] MUSEL , B., B ORDIER , C., D OJAT, M., P ICHAT, C., C HOKRON , S., L E BAS, J.-F., AND PEYRIN ,
C. Retinotopic and lateralized processing of spatial frequencies in human visual cortex during scene
categorization. Journal of Cognitive Neuroscience 25, 8 (2013), 1315–1331.
[69] NEALEY , T. A., AND MAUNSELL , J. Magnocellular and parvocellular contributions to the responses of
neurons in macaque striate cortex. Journal of Neuroscience 14, 4 (1994), 2069–2079.
[70] NGUYEN , V. D., YANG , Z., B UCKLEY , C. L., AND ORORBIA , A. R-aif: Solving sparse-reward robotic
tasks from pixels with active inference and world models. arXiv preprint arXiv:2409.14216 (2024).
[71] NOROOZI , M., P IRSIAVASH , H., AND FAVARO, P. Representation learning by learning to count. In
Proceedings of the IEEE international conference on computer vision (2017), pp. 5898–5906.
[72] OGNIBENE , D., AND BALDASSARE , G. Ecological active vision: four bioinspired principles to integrate
bottom–up and adaptive top–down attention tested with a simple camera-arm robot. IEEE transactions on
autonomous mental development 7, 1 (2014), 3–25.
[73] OLSHAUSEN , B. A., AND FIELD , D. J. Sparse coding with an overcomplete basis set: A strategy employed
by v1? Vision research 37, 23 (1997), 3311–3325.
[74] OORD , A. V. D., L I, Y., AND VINYALS , O. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748 (2018).
[75] ORORBIA , A., AND KIFER , D. The neural coding framework for learning generative models. Nature
communications 13, 1 (2022), 2064.
[76] ORORBIA , A., AND MALI , A. Convolutional neural generative coding: Scaling predictive coding to natural
images. arXiv preprint arXiv:2211.12047 (2022).
[77] O RORBIA , A., AND MALI , A. Active predictive coding: Brain-inspired reinforcement learning for sparse
reward robotic control problems. In 2023 IEEE International Conference on Robotics and Automation
(ICRA) (2023), IEEE, pp. 3015–3021.
[78] ORORBIA , A., AND MALI , A. The predictive forward-forward algorithm. arXiv preprint arXiv:2301.01452
(2023).
[79] ORORBIA , A., M ALI , A., K OHAN , A., M ILLIDGE , B., AND SALVATORI , T. A review of neuroscience-
inspired machine learning. arXiv preprint arXiv:2403.18929 (2024).
[80] ORORBIA , A. G. Brain-inspired machine intelligence: A survey of neurobiologically-plausible credit
assignment. arXiv preprint arXiv:2312.09257 (2023).
[81] ORORBIA , A. G. Contrastive signal–dependent plasticity: Self-supervised learning in spiking neural circuits.
Science Advances 10, 43 (2024), eadn6076.
[82] PARR , T., AND FRISTON , K. J. The active construction of the visual world. Neuropsychologia 104 (2017),
92–101.
[83] PARR , T., AND FRISTON , K. J. Uncertainty, epistemics and active inference. Journal of the Royal Society
Interface 14, 136 (2017), 20170376.
[84] PARR , T., AND FRISTON , K. J. Working memory, attention, and salience in active inference. Scientific
reports 7, 1 (2017), 14678.
[85] PATHAK , D., K RAHENBUHL , P., D ONAHUE , J., D ARRELL , T., AND EFROS , A. A. Context encoders:
Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern
recognition (2016), pp. 2536–2544.
[86] PEHLEVAN , C., S ENGUPTA , A. M., AND CHKLOVSKII , D. B. Why do similarity matching objectives lead
to hebbian/anti-hebbian networks? Neural computation 30, 1 (2017), 84–124.
[87] PERRINET , L. U., A DAMS , R. A., AND FRISTON , K. J. Active inference, eye movements and oculomotor
delays. Biological cybernetics 108 (2014), 777–801.
[88] PIERROT -DESEILLIGNY , C., M ILEA , D., AND MÜRI , R. M. Eye movement control by the cerebral cortex.
Current opinion in neurology 17, 1 (2004), 17–25.
[89] P OLYAK, S. L. The retina. Univ. Chicago Press, 1941.
[90] PRINCE , J. S., A LVAREZ , G. A., AND KONKLE , T. Contrastive learning explains the emergence and
function of visual category-selective regions. Science Advances 10, 39 (2024), eadl1776.
[91] R AO, R. P. A sensory–motor theory of the neocortex. Nature neuroscience 27, 7 (2024), 1221–1235.
20
Preprint
[92] RAO, R. P., AND BALLARD , D. H. Predictive coding in the visual cortex: a functional interpretation of
some extra-classical receptive-field effects. Nature Neuroscience (1999).
[93] RAO, R. P., G KLEZAKOS , D. C., AND SATHISH , V. Active predictive coding: A unifying neural model
for active perception, compositional learning, and hierarchical planning. Neural Computation 36, 1 (2023),
1–32.
[94] RAO, R. P. N. An optimal estimation approach to visual perception and learning. Vision Research 39, 11
(1999), 1963–1989.
[95] RAYNER , K., I NHOFF , A. W., M ORRISON , R. E., S LOWIACZEK , M. L., AND BERTERA , J. H. Masking
of foveal and parafoveal vision during eye fixations in reading.Journal of Experimental Psychology: Human
perception and performance 7, 1 (1981), 167.
[96] R ENSINK , R. A. The dynamic representation of scenes. Visual cognition 7, 1-3 (2000), 17–42.
[97] RICHEMOND , P. H., G RILL , J.-B., A LTCHÉ , F., TALLEC , C., S TRUB , F., B ROCK , A., S MITH , S., D E, S.,
PASCANU , R., P IOT, B., ET AL . Byol works even without batch statistics. arXiv preprint arXiv:2010.10241
(2020).
[98] ROBINSON , D. A. The mechanics of human smooth pursuit eye movement. The Journal of Physiology 180,
3 (1965), 569.
[99] ROTHKOPF , C. A., B ALLARD , D. H., AND HAYHOE , M. M. Task and context determine where you look.
Journal of vision 7, 14 (2007), 16–16.
[100] SALVATORI , T., M ALI , A., B UCKLEY , C. L., L UKASIEWICZ , T., R AO, R. P., F RISTON , K., AND OROR -
BIA , A. Brain-inspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870
(2023).
[101] SALVATORI , T., P INCHETTI , L., M ILLIDGE , B., S ONG , Y., BAO, T., B OGACZ , R., AND LUKASIEWICZ ,
T. Learning on arbitrary graph topologies via predictive coding. Advances in neural information processing
systems 35 (2022), 38232–38244.
[102] SALVATORI, T., S ONG , Y., HONG , Y., SHA, L., F RIEDER , S., X U, Z., B OGACZ , R., AND LUKASIEWICZ ,
T. Associative memories via predictive coding. Advances in Neural Information Processing Systems 34
(2021), 3874–3886.
[103] S ANOCKI , T. Representation and perception of scenic layout. Cognitive Psychology 47, 1 (2003), 43–86.
[104] SASAKI , Y., H ADJIKHANI , N., F ISCHL , B., L IU, A. K., M ARRET , S., D ALE , A. M., AND TOOTELL ,
R. B. Local and global attention are mapped retinotopically in human occipital cortex. Proceedings of the
National Academy of Sciences 98, 4 (2001), 2077–2082.
[105] SETH , A. K., AND HOHWY, J. Predictive processing as an empirical theory for consciousness science.
Cognitive Neuroscience 12, 2 (2021), 89–90.
[106] SHARAFELDIN , A., I MAM , N., AND CHOI , H. Active sensing with predictive coding and uncertainty
minimization. Patterns (2024).
[107] SRINIVASAN , M. V., L AUGHLIN , S. B., AND DUBS , A. Predictive coding: a fresh view of inhibition in
the retina. Proceedings of the Royal Society of London. Series B. Biological Sciences 216 , 1205 (1982),
427–459.
[108] STRASBURGER , H., R ENTSCHLER , I., AND JÜTTNER , M. Peripheral vision and pattern recognition: A
review. Journal of vision 11, 5 (2011), 13–13.
[109] TANG , M., S ALVATORI , T., M ILLIDGE , B., S ONG , Y., LUKASIEWICZ , T., AND BOGACZ , R. Recurrent
predictive coding models for associative memory employing covariance learning. PLoS computational
biology 19, 4 (2023), e1010719.
[110] VAN DER MAATEN , L., AND HINTON , G. Visualizing data using t-sne. Journal of machine learning
research 9, 11 (2008).
[111] VAN DIEPEN , P. M., W AMPERS , M., AND D ’YDEWALLE , G. Functional division of the visual field:
Moving masks and moving windows. In Eye guidance in reading and scene perception. Elsevier, 1998,
pp. 337–355.
[112] WANG , P., AND COTTRELL , G. W. Central and peripheral vision for scene recognition: A neurocomputa-
tional modeling exploration. Journal of vision 17, 4 (2017), 9–9.
[113] WESTHEIMER , G., AND MCKEE, S. P. Visual acuity in the presence of retinal-image motion. JOSA 65, 7
(1975), 847–850.
21
Preprint
[114] WHITTINGTON , J. C., AND BOGACZ , R. An approximation of the error backpropagation algorithm
in a predictive coding network with local hebbian synaptic plasticity. Neural computation 29, 5 (2017),
1229–1262.
[115] WURTZ , R. H., M CALONAN , K., C AVANAUGH , J., AND BERMAN , R. A. Thalamic pathways for active
vision. Trends in cognitive sciences 15, 4 (2011), 177–184.
[116] ZBONTAR , J., J ING , L., M ISRA , I., L ECUN, Y., AND DENY, S. Barlow twins: Self-supervised learning via
redundancy reduction. In International conference on machine learning (2021), PMLR, pp. 12310–12320.
[117] ZEKI , S. The ferrier lecture 1995 behind the seen: the functional specialization of the brain in space and
time. Philosophical Transactions of the Royal Society B: Biological Sciences 360, 1458 (2005), 1145–1183.
[118] ZEKI , S., AND SHIPP, S. The functional logic of cortical connections. Nature 335, 6188 (1988), 311–317.
[119] ZHANG , L., D U, W., Z HOU , S., W ANG , J., AND SHI, J. Inpaint2learn: A self-supervised framework
for affordance learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision (2022), pp. 2665–2674.
[120] ZIMMERMANN , R. S., S HARMA , Y., S CHNEIDER , S., B ETHGE , M., AND BRENDEL , W. Contrastive
learning inverts the data generating process. In International Conference on Machine Learning (2021),
PMLR, pp. 12979–12990.
22
Preprint
Appendix / Supplementary Material
Glimpse Vector Creation: Additional Details
Each glimpse-action pair within a K-length (random/involuntary) saccade trajectory or sequence
{(g(0), a(0)), (g(1), a(1)), ...,(g(k), a(k)), ...(g(K), a(K))} is created by first generating a 2D vector contain
Cartesian x-y coordinates (in this work, a random policy is used to select each set of coordinates at each step within
the trajectory), extracting the relevant glimpse vector g(k) containing the foveal/parafoveal/peripheral patches at
the chosen coordinates, and then finally normalizing the coordinate vector to create the requisite action vector
a(k). Specifically, the 2D x-y coordinate action vector is created by normalizing the original raw x-y Cartesian
coordinates to the range of [−1, 1], i.e., a(k) = 2

[x, y]T

/D − 1 for a D × D pixel image where x and y are the
original Cartesian coordinates of the glimpse center-point.
Each glimpse vector g(k) itself is a concatenation of several views (pixel patches) sampled from the observation
o(tg). Specifically, it is a combination of several types of views, each of which can be expressed in terms of the
following piecewise function:
pv =



pv
c ∈ RSc×Sc v ∈ {Set of foveal patch indices}
pv
f ∈ RSf ×Sf v ∈ {Set of parafoveal patch indices}
pv
p ∈ RSp×Sp v ∈ {Set of peripheral patch indices}
∅ otherwise.
(15)
To produce the final glimpse vector, all C foveal, F parafoveal, and P peripheral views (centered around the
glimpse/gaze’s center-point) are first average pooled to always be the same final shape of S × S pixels, then
flattened to vectors, and finally concatenated to obtain g(k) ∈ R((C+F+P)∗(S∗S))×1. All of the above means that
the final glimpse vector produced as a result of the k-th saccade is:
g(k) = (< g1(k), g2(k), ...,gv(k), ...,gV (k) >)T (16)
where V = C + F + P, brackets < · > denote vector concatenation, and gv(k) = Flat
 
Pool(pv)

. For the
main model used in the paper, the set of foveal patch indices was{1, 2, 3, 4} while the set of parafoveal indices
was {5} and the peripheral was {6}. This choice was made based on preliminary experimentation (the stream
configuration that resulted in the best performing MPC model was chosen), the results of which are presented in
the next appendix section.
Upon creation, the foveal, parafoveal, and peripheral views are all aligned around a glimpse center-point (the x-y
coordinates of the center-point of our model’s gaze) with all views arranged around the center-point in a particular
topology (such as a grid). Foveal views are generally shaped such that SC = S whereas parafoveal views are
shaped such that SC > Sand peripheral views are shaped such that SP > SC > S; in this work, we specifically
choose for foveal views S = SC = 8 pixels, for parafoveal views SF = 16 pixels, and for peripheral views
SP = 24 pixels. As mentioned above, we use C = 4 foveal views which are arranged in a 2 × 2 grid (such that the
foveal views overlap with one another by 1-2 pixels) centered around the whole glimpse/gaze center-point; we only
use one (F = 1) parafoveal view and one (P = 1) peripheral views, which are both directly centered around the
glimpse/gaze center-point.
Analysis of View Extraction Schemes
In this supplementary section, we provide some experimental results for a small set of configurations of the input
stream patch extraction scheme that we utilize for the MPC and GPC- fov models. Table 2 presents the results
of these particular experimental results. Concretely, under each particular configuration of the input stream, we
fit an MPC to the training data as in the main paper, then probe the quality of the embeddings with: 1) a linear
probe/classifier, 2) a nonlinear attentive probe/classifier (set up in the same way as in [ 18]), and 3) a nonlinear
MLP decoder (for input reconstruction). The training details for the linear probe and MLP decoder are the same
as described in the main paper; the nonlinear attention probe was trained under similar conditions to the linear
probe (except it was trained using the Adam optimizer, used drop-out for regularization, and employed a decaying
adaptive learning rate).
Specifically, we analyze the performance of an MPC model on MNIST under a small set of several possible
stream configurations; concretely, we investigate the value of having only one coarser grained view (i.e., just the
peripheral) or two (i.e., the parafoveal patch and the peripheral patch) as well as having only a single foveal or
multiple (two or four foveal patches). As a result, the input stream configurations that we preliminarily investigated
included the following:
23
Preprint
F-PF-P Lin-ACC (%) Attn-ACC (%) Dec-MSE (nats)
C = 1 96.20 ± 0.07 98.00 ± 0.03 9.283 ± 0.137
C = 2 96.40 ± 0.08 98.30 ± 0.08 10.869 ± 0.933
C = 4 97.81 ± 0.05 98.80 ± 0.05 5.761 ± 1.222
F-P Lin-ACC (%) Attn-ACC (%) Dec-MSE (nats)
C = 1 95.08 ± 0.06 97.71 ± 0.03 9.942 ± 0.763
C = 2 96.66 ± 0.04 98.25 ± 0.02 7.481 ± 0.889
C = 4 97.56 ± 0.05 98.68 ± 0.04 4.971 ± 0.616
Table 2: Generalization on MNIST of meta-representational predictive coding under different glimpsing
structures. We examine MPC generalization ability under different formulations of its sensory input stream
structure – ‘F-PF-P‘ denotes foveal-parafoveal-peripheral whereas ‘F-P‘ denotes foveal-peripheral; C = {1, 2, 4}
controls the amount of foveal streams arranged for the structure (while only one parafoveal or peripheral view
are used). We specifically measure generalization in terms of downstream classification ability (in terms of %),
as measured by a linear probe (Lin-ACC) and a nonlinear attentive probe (Attn-ACC), as well as downstream
decoding ability in terms of mean squared error (Dec-MSE, in terms of nats).
• Foveal, parafoveal, peripheral (F-PF-P):
– One foveal (C = 1) + parafoveal + peripheral
– Two foveal (C = 2) + parafoveal + peripheral
– Four foveal (C = 4) + parafoveal + periperhal
• Foveal and only peripheral (F-P):
– One foveal (C = 1) + peripheral
– Two foveal (C = 2) + peripheral
– Four foveal (C = 4) + periperhal
In Table 2, we observe that the best classification performance (in terms of both the linear and nonlinear attentive
probes) is obtained with C = 4, F = 1, and P = 1; however, slightly better reconstruction comes from just
C = 4 and P = 1 (no parafoveal; F = 0). It is important to note that more foveal streams resulted in improved
performance in either of these overall settings (with C = 4 giving the best in the set of configurations explored). It
would prove beneficial if future work were to explore other configurations of the input streams (including both the
quantity as well as the spatial arrangement).
24