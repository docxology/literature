1202
voN
02
]CN.oib-q[
1v03501.1112:viXra
Kalman filters as the steady-state solution of gradient descent on
variational free energy
Manuel Baltieri1,∗, Takuya Isomura2
1 Araya Inc., Tokyo, Japan
2 Brain Intelligence Theory Unit,
RIKEN Centre for Brain Science, Saitama, Japan
∗ Corresponding author: manuel baltieri@araya.org
Abstract
The Kalman filter is an algorithm for the estimation of hidden variables in dynamical systems under
linearGauss-Markovassumptionswithwidespreadapplicationsacrossdifferentfields. Recently,itsBayesian
interpretation has received a growing amount of attention especially in neuroscience, robotics and machine
learning. In neuroscience, in particular, models of perception and control under the banners of predictive
coding,optimalfeedbackcontrol,activeinferenceandmoregenerallytheso-calledBayesianbrainhypothesis,
have all heavily relied on ideas behind the Kalman filter. Active inference, an algorithmic theory based
on the free energy principle, specifically builds on approximate Bayesian inference methods proposing a
variational account of neural computation and behaviour in terms of gradients of variational free energy.
Usingthisambitiousframework,severalworkshavediscusseddifferentpossiblerelationsbetweenfreeenergy
minimisation and standard Kalman filters. With a few exceptions, however, such relations point at a mere
qualitative resemblance or are built on a set of very diverse comparisons based on purported differences
betweenfreeenergyminimisationandKalmanfiltering. Inthiswork,wepresentastraightforwardderivation
of Kalman filters consistent with active inference via a variational treatment of free energy minimisation in
termsofgradient descent. Theapproach considered hereoffersamoredirect link betweenmodels ofneural
dynamicsasgradientdescentandstandardaccountsofperceptionanddecisionmakingbasedonprobabilistic
inference, further bridging the gap between hypotheses about neural implementation and computational
principles in brain and behavioural sciences.
1 Introduction
The Kalman filter is a method for the solution of estimation problems in dynamical models and arguably one
of the most significantadvances in estimation and filtering theory of the last century. Estimation, or inference,
refersto the task ofevaluatingsomelatent, orhidden, variablesgiventhe availabilityofsomeother measurable
quantities, usually named observations. Estimation problems on dynamical (i.e., time-dependent) quantities in
time series models are usually classified as either filtering, smoothing or prediction (forecasting). In filtering,
estimation is defined for some hidden variables attime t, s , givena (time) series ofobservationsy atprevious
t
and present time steps 0:t, i.e., y . Smoothing problems correspondto the estimation of a hidden variable s
0:t t
givenpast,presentandfutureobservationsy uptosome(future)timeT suchthat0<t<T. Forecasting,or
0:T
prediction, onthe other handentailsinference ona future hiddenvariables givenonly previousmeasurements
t
y 0:t−k , with k >0.
The Kalman filter provides a standard tool to approach filtering problems under a well-defined set of as-
sumptions including: Gaussian white noise, known inputs and parameters, linear dynamics and observation
laws and quadratic cost functions. Over the years, the Kalman filter has been (re)derived using a number of
different methods, e.g., orthogonal projections, innovations approach, partial differential equations, maximum
likelihood and a-posterior estimates, recursive Bayesian inference, variational methods/Lagrange multipliers,
hiddenMarkovmodels (forsomereviewsseeforinstance[54,17]). Recently,followingthe useofBayesianprin-
ciples for models in brain and behavioural studies, and the very emergence of the so-called “Bayesian brain”
hypothesis [83, 61, 25, 18], Kalman filters and a more generalclass of methods, i.e., Bayesianfilters, have been
linkedtodifferentemergingframeworksinneuroscience. Theseinclude,amongothers,predictivecoding[82,83],
optimal feedback control [88, 87] and active inference [45, 41]. Often, connections are proposed at a functional
level, but in some cases we can also find more direct, concrete hypotheses about their neural implementations
[24, 94], see in particular [27] for a recent review (and a new proposal).
1
Active inference is an ambitious framework originally developed in the natural sciences that is claimed to
provide a mathematical treatment of biological,cognitive and even socio-culturalprocesses under the umbrella
of Bayesian mechanics, i.e., “laws of motion” describing spatio-temporal dynamics of parametrised Bayesian
beliefs modelledinbrains,biologicalorganismsandotherself-sustainingnon-livingsystems athigherandlower
scales [45, 32, 34, 77, 91, 9, 59, 20]. In analogy with the principle of stationary action in physics, from which
Newtonian, Lagrangianand Hamiltonian dynamics can be obtained under different sets of assumptions, active
inference proponents claim that this framework is an attempt to derive Bayesianmechanics from a principle of
(variational) free energy minimisation. Bayesian mechanics can thus essentially be seen as a set of dynamical
Bayesian inference updates corresponding to a gradient descent/flow on parametrised statistical manifolds (cf.
similar work on Wesserstein spaces [16, 89] building on the JKO equation [56]).
Inthislight,wewillthenlookathowsuchaproposalrelatestoestablishedmethodsinBayesianestimation,
inparticularwhethersuchmethods canbe derivedassolutionstoasetofvariationalproblemsformulatedwith
different sets of assumptions under the same Bayesian principle. This follows works such as [7], approximating
active inference on trajectories for multiple embedding orders to obtain PID control, and builds on established
results that more generally link variationaltreatments of Bayesianinference to classicalestimation and control
(via the inference/controlduality) [26, 70, 22].
WhiledifferentconnectionsbetweenKalman,andmoreingeneralBayesian,filtersandactiveinferencehave
been proposed in the literature, see section 2, only a few have managed to show a clear and formal relation
[23, 90], specifically using factor graphs. The goal of this work is to further elucidate this connection under a
different light, with a variational(Gaussian) treatment of Bayesianfilters, presented in section 3, followedby a
generativemodelandrelatedfreeenergyfunction for aKalmanfilter insection4. In section5 we thenderive a
standard gradient descent/flow of free energy minimisation as specified by the Bayesianmechanics formulation
of active inference [45, 32, 34, 77, 91, 59, 20]. As we shall see, Kalman filters will emerge as the steady state
solution of a gradient descent on variational free energy under the right generative model. This derivation
will allow us to draw more direct connections to gradient-based frameworks in different fields, see concluding
remarks in section 6, highlighting then their role in neuroscience as a possible connection between models of
neuraldynamicsrelyingongradientdescentschemesandcomputationalprinciplesdescribingcognitivefunctions
as Bayesian belief updates.
2 Related work
Several works in the literature propose different — and at times contradicting — relations between active
inferenceandKalmanorKalman-Bucyfilters(thediscreteobservationscontinuousdynamicsversionofKalman
filters). For instance,some workssuggestthat Kalmanfilters areequivalentto predictive coding (itself claimed
to be a special case of active inference), at least in some cases and under some implicit assumptions [11, 71,
19, 10]. It is then often claimed that Kalman or Kalman-Bucy filters are superseded, generalised or entailed
by the free energy principle and/or active inference [45, 31, 2, 39, 3, 1, 60], but a formal derivation is usually
missing. At times, Kalman(-Bucy) and the more general Bayesian filters are said to “resemble”, be “formally
similar to” or “consistent with”, “have the same form as” or “take the form of” equations in active inference
[58,43,36,33, 37,28,29,35, 2,15, 80,30,81, 34,77,67]. Other workssuggestthat activeinference proposesa
formofBayesianfilteringandinsomecasesgeneralisesit[44,42,40,41,78,52,75,20]1. Relatedargumentsare
sometimes used to claim that generative models can be defined to support both active inference and Kalman
filtering [79], although the exact definition of such generative models is not provided. In some cases, it is also
stated that using variationalfree energy minimisationone canderive Kalmanfilters [62], but to the best of our
knowledge, no explicit proof of a gradient-based approach consistent with active inference and the free energy
principlehasbeenprovided. Ontheotherhand,Kalmanfiltershavebeenformallydescribedviaafactorgraphs
description of active inference algorithms [23, 90] relating to exact treatments of message passing [84].
Other works then argue that Kalman filters and variational free energy derivations in active inference are
distinct and in some cases can be compared to each other [45, 31, 6, 64, 60, 63, 67, 8, 5, 14, 4], often claiming
thatactiveinferenceformulationsoutperformKalmanfilters[45,31,6,64,60,67,14,4]. In[69,68]wethenalso
find claims that approximations of Kalman filters derived from active inference are supposedly more biological
plausible than their counterpart in standard filters. Finally, due to the connections between active inference
and controlas inference, Kalmanfilters have also been contrastedto active inference in [51], even if their exact
relation remains unstated.
In this literature we find that only but a few works attempted to provide explicit formal connections via
gradientdescentonvariationalfreeenergy[8,67,69,68],orviamessagepassingonfactorgraphs[23,90]. Even
fewer then have managed to correctly capture the actual equivalence [23, 90], none of which using gradient-
based algorithms. To address this series of inconsistencies, and to expand on previous results, in what follows
1Notice that in these accounts, Kalman filters are not mentioned directly as in other references, but their connection can be
inferredfromtheverydefinitionofBayesianfilters.
2
we derive state estimation for continuous-state distributions under active inference, with a focus on filtering
algorithms. For a closely related variational treatment, relying on algebraic manipulations over gradients and
with applications also to smoothing, regression and other inference problems, see also [74].
3 A variational (Gaussian) treatment of Bayesian filters
The standard formulation of Bayesian filtering problems for linear Gauss-Markov models can be described in
terms of recursive Bayesian estimates [17, 48, 85] over hidden states s with observations y for t=0...T:
t 0:t
p(y ,s )
p(s |y )= 0:t t
t 0:t p(y )
0:t
p(y t ,y 0:t−1 ,s t )
=
p(y t ,y 0:t−1 )
p(y t |s t ,y 0:t−1 )p(s t |y 0:t−1 )p(y 0:t−1 )
=
p(y t |y 0:t−1 )p(y 0:t−1 )
p(y t |s t )p(s t |y 0:t−1 )
= (1)
p(y t |y 0:t−1 )
where in the last line we used the fact that observations at time t, y , are conditionally independent of their
t
past givens . Fromthis, we canthen derive a variationaltreatmentestablishing a bound for the surprisal(i.e.,
t
the negative log-model evidence), −lnp(y t |y 0:t−1 ), introducing a variational density q(s t )
−lnp(y t |y 0:t−1 )=lnp(s t |y t ,y 0:t−1 )−lnp(y t ,s t |y 0:t−1 )
=lnp(s t |y t ,y 0:t−1 )−lnp(y t ,s t |y 0:t−1 )+lnq(s t )−lnq(s t )
q(s ) q(s )
t t
=ln −ln
p(y t ,s t |y 0:t−1 ) p(s t |y t ,y 0:t−1 )
q(s ) q(s )
= q(s )ln t ds − q(s )ln t ds (2)
Z t p(y t ,s t |y 0:t−1 ) t Z t p(s t |y t ,y 0:t−1 ) t
and defining the variational free energy as
q(s )
F ≡ q(s )ln t ds
Z t p(y t ,s t |y 0:t−1 ) t
=E q(st) [−lnp(y t ,s t |y 0:t−1 )+lnq(s t )] (3)
where E [•]denotes the expectation of• overq(s ). This givesan upper bound ofthe surprisalowing to the
q(st) t
non-negativity of the Kullback-Leibler divergence in the second term of the last line of equation (2). Under a
variational Gaussian assumption [73], the variational density is defined as
q(s t )=N (s t ;µ+ st ,Σ+ st )=(2π) −N/2|Σ+ st | −1/2e −1 2 (st −µ+ st )T(Σ+ st )−1(st −µ+ st ) (4)
The “+-notation” is introduced here to compare this derivation with standard Kalman filter treatments differ-
entiating between parameters that take into account information about observations y at time t (corrections,
t
or a-posteriori estimates of s ) and parameters that don’t (predictions, or a-priori estimates of s for which we
t t
−
will later adopt a “ -notation”). Using textbook results for the differential entropy of a multivariate Gaussian
distribution, the variational free energy reduces to
N 1
F =E q(st) [−lnp(y t ,s t |y 0:t−1 )]− 2 ln(2πe)− 2 ln|Σ+ st | (5)
Following [45], we then apply a Laplace approximation to the joint p(y t ,y 0:t−1 ,s t )2 [66]. In particular, this
reduces to a Taylor expansion up to second order of the log-joint distribution at the maximum a posteriori
(mode) of the posterior, sˆ
t
lnp(y t ,s t |y 0:t−1 )=lnp(y t ,s t |y 0:t−1 ) (cid:12)st=sˆt − 2 1 (s t −sˆ t )TS t −1(s t −sˆ t )+... (6)
(cid:12)
2Notice that with the variational treatment applied in [45], (cid:12) the posterior is effectively of the form: e E q(st)[−lnp(yt,y0:t−1,st)],
which entails the equivalence of the variational Gaussian approximation applied to q(st) and the Laplace method applied to
p(yt,y0:t−1,st)[73].
3
with the first order term not appearing (i.e., =0) at the expansion point, the mode, and with the matrix S
−1
t
equal to the Hessian of lnp(y t ,y 0:t−1 ,s t )) evaluated at the mode s t =sˆ t
∇ st lnp(y t ,s t |y 0:t−1 ) =0
(cid:12)st=sˆt
(cid:12)
∇ st ∇ st lnp(y t ,s t |y 0:t (cid:12) −1 ) =−S t −1
(cid:12)st=sˆt
(cid:12)
⇒S t −1 =−∇ st ∇ st lnp((cid:12)y t ,s t |y 0:t−1 ) (7)
(cid:12)st=sˆt
(cid:12)
(cid:12)
At this stage, the free energy can be rewritten, omitting higher order terms, as
F ≈−lnp(y t ,s t |y 0:t−1 ) (cid:12)
(cid:12)
st=sˆt + 1 2 E q(st)
(cid:2)
(s t −sˆ t )TS t −1(s t −sˆ t )
(cid:3)
− N 2 ln(2πe)− 2 1 ln|Σ+ st | (8)
(cid:12)
Following standard treatments, [45, 73] we will simplify the above free energy F by considering the case where
the variational density q(s ) is centred at the mode of the posterior sˆ, i.e., µ+ = sˆ, and where Σ+ = S ,
t t st t st t
i.e., the covariance of the variational density Σ+ is equal to the covarianceof the generative density S at each
st t
expansion point (i.e., the mode) of the Laplace method. This then implies that, using the trace trick, 3
N 1
F ≈−lnp(y t ,s t |y 0:t−1 ) (cid:12)st=sˆt − 2 ln(2π)− 2 ln|Σ+ st | (9)
(cid:12)
(cid:12)
4 Variational free energy for a Kalman filter
After writing down the variational free energy (under Gaussian assumptions) for a generic Bayesian filtering
problem, we now specify the components of a linear generative model, p(y t ,s t |y 0:t−1 ) = p(y t |s t )p(s t |y 0:t−1 ) as
follows:
p(y |s )=N (Cµ+,Σ )
t t st z
p(s t |y 0:t−1 )=N (µ − st ,Σ − st ) (10)
using the +− and − − notations described previously. Here, the a-priori estimates are given as functions of the
a-posteriori estimates at the last time step
µ − ≡Aµ+
st st−1
Σ − ≡AΣ+ AT +Σ (11)
st st−1 w
Notice that this can be seen, equivalently, as a linear state-space model under Gauss-Markov assumptions
s =w
0 0
s t =As t−1 +w t , for t>0
y =Cs +z (12)
t t t
with Gaussian noise w ∼N (0,Σ ),z ∼N (0,Σ ). In probabilistic form this is then
t w t z
p(s |s )=N (0,Σ )
1 0 w
p(s t |s t−1 )=N (As t−1 ,Σ w )
p(y |s )=N (Cs ,Σ ) (13)
t t t z
with equations that can be combined to form, using the Chapman-Kolmogorov equation [85] for standard
Gaussian distributions (i.e., a convolution of Gaussians), the following
p(s t |y 0:t−1 )= p(s t |s t−1 )p(s t−1 |y 0:t−1 )ds t−1
Z
= Z N (As t−1 ,Σ w )N (µ+ st−1 ,Σ+ st−1 )ds t−1
=N (Aµ+ ,AΣ+ AT +Σ )
st−1 st−1 w
=N (µ − ,Σ − ) (14)
st st
as seen in equation (10).
3Noticethattheequalityholdsstrictlyonlyforthecasewherebothq(st)andp(yt,st|y0:t−1)areGaussian.
4
The generative model described in equation (10) can be plugged into equation (9), with sˆ = µ+, thus
t st
obtaining the variational free energy4
F = 1 y −Cµ+ T Σ −1 y −Cµ+ + µ+ −µ − T Σ − −1 µ+ −µ − + 1 ln|Σ |+ 1 ln|Σ |
2 t st z t st st st st st st 2 zt 2 wt
h(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)i
N 1
− ln(2π)− ln|Σ+ | (15)
2 2 st
5 Free energy minimisation with respect to (µ+,Σ+)
s s
t t
AKalmanfiltercanbederivedfromtheminimisationofequation(15)withrespectto(µ+,Σ+)afterproviding
st st
the following gradients of free energy:
∇ F =−CTΣ −1 y −Cµ+ + Σ − −1 µ+ −µ −
µ+
st
z t st st st st
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
∇ Σ+ st F =∇ Σ+ st (cid:20) −lnp(y t ,s t |y 0:t−1 ) (cid:12) (cid:12) st=µ+ st (cid:21) − 2
1
(cid:0) Σ+ st (cid:1)
−1
= 2
1
∇ µ+ st ∇ µ+ st (cid:20) −lnp(y t ,s t |y
(cid:12)
0:t−1 ) (cid:12) (cid:12) st=µ+ st (cid:21) − 2
1
(cid:0) Σ+ st (cid:1)
−1
= 1 CTΣ −1C+ Σ − −1 − 1 Σ (cid:12) + −1 (16)
2 z st 2 st
(cid:16) (cid:0) (cid:1) (cid:17) (cid:0) (cid:1)
Notice that in the second equation, we applied Price’s and Bonnet’s theorems [73, 57] to compute (first order)
derivatives with respect to Σ+ using (second order) derivatives with respect to µ+.
st st
5.1 Covariance estimation
The equation for the correction updates of the covariance parameters Σ+ can be obtained starting from the
st
steady-state solution for ∇ F =0
Σ+
st
Σ+ −1 =CTΣ −1C+ Σ − −1
st z st
(cid:0) ⇒ (cid:1) Σ+ = CTΣ −1C+ (cid:0) Σ − (cid:1) −1 −1
st z st
=Σ (cid:16) − −Σ − CT (cid:0) Σ + (cid:1) C (cid:17) Σ − CT −1 CΣ −
st st z st st
=(I −K C)Σ −(cid:0) (cid:1) (17)
st st
where in the second line we applied the standard matrix inversion(Sherman–Morrison–Woodbury)lemma and
in the last line we used the familiar definition of the Kalman gain [86]
K =Σ − CT Σ +CΣ − CT −1 (18)
st st z st
(cid:0) (cid:1)
−
The definition of Σ in equation (11) reflects then the prediction step
st
Σ − =AΣ+ AT +Σ (19)
st st−1 w
and can be substituted in equation (17) to obtain a standard one-step update equation [86].
5.2 Mean estimation
For expectation parameters µ+, once again at steady-state (∇ F = 0), we obtain after a few manipulations
st µ+
st
the following variational updates
0=−CTΣ −1 y −Cµ+ + Σ − −1 µ+ −µ −
z t st st st st
CTΣ −1C+ Σ − −1 µ+ =CTΣ −1y + (cid:0) Σ − −1 µ (cid:1)− (cid:0) (cid:1) (cid:0) (cid:1)
z st st z t st st
(cid:16) (I −K C (cid:0) )Σ − (cid:1) − (cid:17) 1 µ+ =CTΣ −1y + (cid:0) Σ − (cid:1)−1 µ −
st st st z t st st
(cid:0) (cid:1) µ+ =µ − +(I −K (cid:0) C) (cid:1) Σ − CTΣ −1 y −Cµ −
st st st st z t st
=µ − +K (y −Cµ − ) (cid:0) (cid:1) (20)
st st t st
4Noticethat,unlikeinequation(8)andequation(9),hereweuseanequalitysign. Havingbynowassumedthatbothq(st)and
p(yt,st|y0:t−1)areGaussian,thereareinfactnohigherordertermstobeconsideredintheLaplacemethod.
5
where in the last line we used the equivalent form of the Kalman gain in equation (18)
K Σ +CΣ − CT =Σ − CT
st z st st
K I (cid:0) +CΣ − CTΣ −1(cid:1) =Σ − CTΣ −1
st st z st z
(cid:0) K (cid:1) =Σ − CTΣ −1−K CΣ − CTΣ −1
st st z st st z
=(I −K C)Σ − CTΣ −1
st st z
=Σ+CTΣ −1 (21)
st z
The definition of
µ−
in equation (11) then implements the prediction step:
st
µ − =Aµ+ (22)
st st−1
and, similarly to the covariance updates, can be plugged in equation (20) for a one-step update rule that
concludes our derivation.
6 Concluding remarks
Kalman filters are a cornerstone method for state estimation of dynamical systems under linear Gauss-Markov
assumptions and are often nowadays formulated as a process of recursive Bayesian inference. Active inference
is a frameworkoriginallydevelopedinneurosciencewith the goalofdescribing processesofneuralcomputation
and behaviouralfeatures of cognitive agents as approximateBayesianinference [32, 42, 38]. More recently, this
framework has been explicitly formalised in terms of Bayesian mechanics, equations of motion for parameter
updatesinastatisticalmanifold. Inpractice,theseequationscorrespondtoagradientdescent/flowinvariational
free energy schemes, usually under Gaussian assumptions [34, 59, 20].
In this note we shed light on a connection between Kalman filters and active inference, specifically its
formulationintermsofgradientdescent/flow. Inparticular,weshowedthatKalmanfiltersarethesteady-state
solution of a gradient descent scheme on the parameters (means and covariance matrix) of a linear generative
modelunderGaussianassumptions(Laplaceapproximation/variationalGauss[73]). Thiscomplementsexisting
algebraic formulations [74], including the ones based on message passing algorithms [23, 90], and clarifies the
role of Kalman filters as gradients of variational free energy, providing a precise connection between standard
filtering methods and gradients in Bayesian belief space.
Importantly, this derivation should not be confused with the idea of “steady-state (Kalman) filtering” [86]:
our derivation shows a generic Kalman filter as the steady-state solution of free energy minimisation, while
steady-state filters refer to methods where the dynamics of the filters themselves reach steady-state, i.e., the
covariance Σ+ becomes stationary.
st
With the assumption of linear generative models, it would then be natural also to question the necessity
of a variational treatment in the first place, given the usual interpretation of variational Bayes as a method
to provide an approximate bound for the model evidence of an analytically or numerically intractable problem
[55,12,13,45]. Herehowever,wetakeadifferentperspectiveandseethecurrentworkasinspiredbytreatments
such as [56], highlighting the tight connections between geometric (gradient descent on statistical manifolds),
variational (bounds in terms of free energy or ELBO) and Bayesian inference interpretations of filtering. This
approachcanbeseenforexampleincomplementaryworksforKalmanfiltersincontinuous-time(bothdynamics
and observations) such as [46, 47].
Inthislight,thepresentvariationalBayesianderivationoftheKalmanfiltershowsasawayto1)preserveits
classical connections to Bayesianinference for the exact (i.e., linear) case [49], 2) understand the Kalman filter
as the optimal linear estimator for non-linear systems [17, 85] in terms of a fixed-form (variational) Gaussian
approximation of arbitrary distributions [73, 50] and 3) link the filter to more recent (information) geometric
treatments of probabilistic inference [65, 72].
In addition, our gradient-based approach preserves, more directly, (potential) connections that have been
proposedbetweenactiveinferenceandframeworksin,e.g.,physics[34,9,59],chemistry[76],andbrainscience,
specifically for neural dynamics [38, 92, 93, 53, 21]. Finally, this clarifies a series of claims about the relation
between Kalman filters and active inference [45, 31, 44, 6, 64, 58, 11, 33, 29, 35, 28, 43, 36, 37, 2, 3, 39, 71, 15,
1, 60, 80, 30, 42, 81, 40, 23, 41, 63, 78, 52, 34, 79, 77, 67, 19, 10, 75, 62, 8, 51, 69, 68, 5, 90, 20, 14, 4], resolving
puzzles derived from ambiguous and inconsistent treatments found in the literature.
7 Acknowledgments
The authors would like to thank Lancelot Da Costa for constructive feedback on a previous draft of the
manuscript.
6
References
[1] RickAAdams,HarrietRBrown,andKarlJFriston. Bayesianinference,predictivecodinganddelusions. AVANT.J.Philos.
Int. Vanguard, 5:51–88,2014.
[2] RickAAdams,LaurentUPerrinet,andKarlFriston. Smoothpursuitandvisualocclusion: activeinferenceandoculomotor
controlinschizophrenia. PloS one,7(10):e47502, 2012.
[3] RickAAdams,KlaasStephan,HarrietBrown,ChristopherFrith,andKarlJFriston.Thecomputationalanatomyofpsychosis.
Frontiers inPsychiatry,4:47,2013.
[4] Ajith Anil Meera and Martijn Wisse. Dynamic expectation maximization algorithm for estimation of linear systems with
colorednoise. Entropy,23(10):1306, 2021.
[5] MohamedBaioumy,CorradoPezzato,RiccardoFerrari,CarlosHerna´ndezCorbato,andNickHawes. Fault-tolerantcontrolof
robotmanipulatorswithsensoryfaultsusingunbiasedactiveinference. arXiv preprint arXiv:2104.01817, 2021.
[6] Bhashyam Balaji and Karl Friston. Bayesian state estimation using generalized coordinates. In Signal Processing, Sensor
Fusion, and Target Recognition XX,volume8050, page80501Y. International SocietyforOpticsandPhotonics,2011.
[7] Manuel Baltieri and Christopher L Buckley. A probabilistic interpretation of PID controllers using active inference. In
International Conference on Simulation of Adaptive Behavior,pages 15–26.Springer,2018.
[8] Manuel Baltieri and Christopher L. Buckley. On Kalman-Bucy filters, linear quadratic control and active inference. arXiv
preprint arXiv:2005.06269, 2020.
[9] Manuel Baltieri, Christopher L Buckley, and Jelle Bruineberg. Predictions in the eye of the beholder: an active inference
accountofwattgovernors. arXiv preprint arXiv:2006.11495, 2020.
[10] HelenCBarron,RyszardAuksztulewicz,andKarlFriston. Predictionandmemory: Apredictivecodingaccount. Progressin
neurobiology,192:101821, 2020.
[11] AndreMBastos,WMartinUsrey,RickAAdams,GeorgeRMangun,PascalFries,andKarlJFriston.Canonicalmicrocircuits
forpredictivecoding. Neuron,76(4):695–711, 2012.
[12] Matthew J.Beal. Variational algorithms for approximate Bayesian inference. UniversityofLondonLondon, 2003.
[13] ChristopherMBishop. Pattern Recognition and Machine Learning. Springer-VerlagNewYork,2006.
[14] Fred Bos, Ajith Anil Meera, Dennis Benders, and Martijn Wisse. Free energy principle for state and input estimation of a
quadcopter flyinginwind. arXiv preprint arXiv:2109.12052, 2021.
[15] HarrietBrown, Rick A Adams, Isabel Parees, MarkEdwards, andKarl JFriston. Active inference, sensory attenuation and
illusions. Cognitive processing,14(4):411–427, 2013.
[16] PratikChaudhariandStefanoSoatto. Stochasticgradientdescentperformsvariationalinference,convergestolimitcyclesfor
deepnetworks. InInternational Conference on Learning Representations,2018.
[17] ZheChen. Bayesianfiltering: FromKalmanfilterstoparticlefilters,andbeyond. Statistics,182(1):1–69, 2003.
[18] Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain
Sciences,36(03):181–204, 2013.
[19] AndrewWCorcoran,GiovanniPezzulo,andJakobHohwy.Fromallostaticagentstocounterfactualcognisers: activeinference,
biologicalregulation,andtheoriginsofcognition. Biology & Philosophy, 35(3):1–45, 2020.
[20] LancelotDaCosta,KarlFriston,ConorHeins,andGrigoriosAPavliotis. Bayesianmechanicsforstationaryprocesses. arXiv
preprint arXiv:2106.13830, 2021.
[21] Lancelot Da Costa, Thomas Parr, Biswa Sengupta, and Karl Friston. Neural dynamics under active inference: Plausibility
andefficiencyofinformationprocessing. Entropy,23(4):454, 2021.
[22] JeanDaunizeau,KarlJFriston,andStefanJKiebel. Variationalbayesianidentificationandpredictionofstochasticnonlinear
dynamiccausalmodels. Physica D: nonlinear phenomena,238(21):2089–2118, 2009.
[23] Bert De Vries and Karl J Friston. A factor graph description of deep temporal active inference. Frontiers in computational
neuroscience,11:95,2017.
[24] SophieDeneve,Jean-Ren´eDuhamel,andAlexandrePouget. Optimalsensorimotorintegrationinrecurrentcorticalnetworks:
aneuralimplementationofkalmanfilters. Journal of neuroscience,27(21):5744–5756, 2007.
[25] KenjiDoya. Bayesian brain: Probabilistic approaches toneural coding. MITpress,2007.
[26] GregoryLEyink. Avariationalformulationofoptimalnonlinearestimation. arXiv preprint physics/0011049, 2000.
[27] Johannes Friedrich, Siavash Golkar, Shiva Farashahi, Alexander Genkin, Anirvan M. Sengupta, and Dmitri B. Chklovskii.
Neuraloptimalfeedbackcontrolwithlocallearningrules,2021.
[28] KarlFriston. Embodiedinferenceandspatialcognition. Cognitive Processing, 13(1):171–177, 2012.
[29] Karl Friston, Michael Breakspear, and Gustavo Deco. Perception and self-organized instability. Frontiers in computational
neuroscience,6:44,2012.
[30] KarlFriston,BiswaSengupta,andGennaroAuletta. Cognitivedynamics: Fromattractorstoactiveinference. Proceedings of
the IEEE, 102(4):427–445, 2014.
[31] KarlJFriston. Hierarchicalmodelsinthebrain. PLoS Computational Biology,4(11),2008.
[32] KarlJFriston. Thefree-energyprinciple: aunifiedbraintheory? Nature reviews. Neuroscience,11(2):127–138, 2010.
[33] KarlJFriston. Afreeenergyprincipleforbiologicalsystems. Entropy,14(11):2100–2121, 2012.
[34] KarlJFriston. Afreeenergyprincipleforaparticularphysics. arXiv preprint arXiv:1906.10184, 2019.
[35] KarlJFriston,RickAdams,LaurentPerrinet,andMichaelBreakspear. Perceptions ashypotheses: saccades asexperiments.
Frontiers inpsychology, 3:151,2012.
7
[36] Karl J Friston, Rick A Adams, and Read Montague. What is value? accumulated reward or evidence? Frontiers in
Neurorobotics, 6,2012.
[37] KarlJFristonandPingAo. Freeenergy,value,andattractors. Computational andmathematical methods inmedicine,2012,
2012.
[38] Karl J Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active inference: a
processtheory. Neural Computation, 29(1):1–49, 2017.
[39] KarlJFristonandDominicAFriston. Afreeenergyformulationofmusicgenerationandperception: Helmholtzrevisited. In
Sound-Perception-Performance, pages 43–69.Springer,2013.
[40] KarlJFriston,MichaelLevin,BiswaSengupta,andGiovanniPezzulo. Knowingone’splace: afree-energyapproachtopattern
regulation. Journal of The Royal Society Interface,12(105):20141383, 2015.
[41] Karl J Friston, Thomas Parr, and Bert de Vries. The graphical brain: belief propagation and active inference. Network
Neuroscience,1(4):381–414, 2017.
[42] Karl J Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pezzulo. Active
inferenceandepistemicvalue. Cognitive neuroscience,pages1–28,2015.
[43] KarlJFriston,TamaraShiner,ThomasFitzGerald,JosephMGalea,RickAdams,HarrietBrown,RaymondJDolan,Rosalyn
Moran, Klaas Enno Stephan, and Sven Bestmann. Dopamine, affordance and active inference. PLoS computational biology,
8(1):e1002327, 2012.
[44] KarlJFriston,KlaasStephan,BaojuanLi,andJeanDaunizeau.Generalisedfiltering.MathematicalProblemsinEngineering,
2010,2010.
[45] Karl J Friston, N. Trujillo-Barreto, and J. Daunizeau. DEM: A variational treatment of dynamic systems. NeuroImage,
41(3):849–885, 2008.
[46] AbhishekHalderandTryphonTGeorgiou. Gradientflowsinuncertaintypropagationandfilteringoflineargaussiansystems.
In2017 IEEE 56th Annual Conference on Decision and Control (CDC),pages 3081–3088. IEEE,2017.
[47] Abhishek Halder and Tryphon T Georgiou. Gradient flows in filtering and fisher-rao geometry. In 2018 Annual American
Control Conference (ACC),pages4281–4286. IEEE,2018.
[48] SimonHaykin. Neural networks and learning machines. NewYork: PrenticeHall,,2009.
[49] Y. C. Ho and R. C. K. Lee. A Bayesian approach to problems in stochastic estimation and control. IEEE transactions on
automatic control, 9(4):333–339, 1964.
[50] AnttiHonkela,TapaniRaiko,MikaelKuusela,MattiTornio,andJuhaKarhunen.ApproximateRiemannianconjugategradient
learningforfixed-formvariationalBayes. The Journal of Machine Learning Research,11:3235–3268, 2010.
[51] AbrahamImohiosen,JoeWatson, andJanPeters. Activeinferenceorcontrolasinference? aunifyingview. InInternational
Workshop on Active Inference,pages 12–19.Springer,2020.
[52] Takuya Isomura,Thomas Parr,andKarlFriston. Bayesianfilteringwithmultipleinternal models: towardatheory ofsocial
intelligence. Neural computation, 31(12):2390–2431, 2019.
[53] TakuyaIsomura,HideakiShimazaki,andKarlFriston. Canonical neuralnetworks performactiveinference. bioRxiv,2020.
[54] AndrewHJazwinski. Stochastic Processes and Filtering Theory,volume64. AcademicPress,1970.
[55] MichaelIJordan,ZoubinGhahramani,TommiSJaakkola,andLawrenceKSaul. Anintroductiontovariationalmethodsfor
graphicalmodels. Machine learning,37(2):183–233, 1999.
[56] RichardJordan,DavidKinderlehrer,andFelixOtto.Thevariationalformulationofthefokker–planckequation. SIAMjournal
on mathematical analysis,29(1):1–17, 1998.
[57] MohammadEmtiyazKhanandH˚avardRue. Thebayesianlearningrule. arXiv preprint arXiv:2107.04562, 2021.
[58] StefanJKiebelandKarlJFriston. Freeenergyanddendriticself-organization. Frontiersinsystemsneuroscience,5:80,2011.
[59] ChangSubKim.Bayesianmechanicsofperceptualinferenceandmotorcontrolinthebrain.BiologicalCybernetics,115(1):87–
102,2021.
[60] JanKneissler,Jan Drugowitsch, KarlFriston, and MartinV Butz. Simultaneous learningandfilteringwithout delusions: A
bayes-optimal derivation of combining predictive inference and adaptive filtering. Frontiers in computational neuroscience,
9:47,2015.
[61] DavidCKnillandAlexandrePouget. TheBayesianbrain: theroleofuncertaintyinneuralcodingandcomputation. Trends
inNeurosciences,27(12):712–719, 2004.
[62] Franz Kuchling, Karl J Friston, Georgi Georgiev, and Michael Levin. Morphogenesis as Bayesian inference: A variational
approachtopatternformationandcontrolincomplexbiologicalsystems. Physics of Life Reviews,33:88–108, 2020.
[63] PabloLanillosandGordonCheng.Adaptiverobotbodylearningandestimationthroughpredictivecoding.In2018IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),pages 4083–4090. IEEE,2018.
[64] BaojuanLi,JeanDaunizeau,KlaasEStephan,WillPenny,DewenHu,andKarlJFriston.Generalisedfilteringandstochastic
dcmforfmri. Neuroimage,58(2):442–457, 2011.
[65] Yubo Li, Yongqiang Cheng, Xiang Li, Hongqiang Wang, Xiaoqiang Hua, and Yuliang Qin. Bayesian nonlinear filtering via
informationgeometricoptimization. Entropy,19(12):655, 2017.
[66] DavidJCMacKay. Information theory, inference and learning algorithms. Cambridgeuniversitypress,2003.
[67] Ajith Anil Meera and Martijn Wisse. Free energy principle based state and input observer design for linear systems with
colorednoise. In2020 American Control Conference (ACC),pages5052–5058. IEEE,2020.
[68] Beren Millidge, Anil Seth, and Christopher L Buckley. Predictive coding: a theoretical and experimental review. arXiv
preprint arXiv:2107.12979, 2021.
[69] Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the relationship between active inference
andcontrolasinference. InInternational Workshop on ActiveInference,pages3–11. Springer,2020.
8
[70] SanjoyKMitterandNigelJNewton. Avariationalapproachtononlinearestimation. SIAMjournaloncontrolandoptimiza-
tion,42(5):1813–1833, 2003.
[71] Rosalyn J Moran, Pablo Campo, Mkael Symmonds, Klaas E Stephan, Raymond J Dolan, and Karl J Friston. Free energy,
precisionandlearning: theroleofcholinergicneuromodulation. Journal of Neuroscience,33(19):8227–8236, 2013.
[72] YannOllivier. TheextendedKalmanfilterisanaturalgradientdescentintrajectoryspace. arXivpreprintarXiv:1901.00696,
2019.
[73] ManfredOpper andC´edricArchambeau. ThevariationalGaussianapproximationrevisited. Neural computation, 21(3):786–
792,2009.
[74] Dirk Ostwald, Evgeniya Kirilina, Ludger Starke, and Felix Blankenburg. A tutorial on variational Bayes for latent linear
stochastictime-seriesmodels. Journal of Mathematical Psychology,60:1–19,2014.
[75] EnsorRafael Palacios, Adeel Razi, Thomas Parr, MichaelKirchhoff, and KarlFriston. Onmarkov blankets andhierarchical
self-organisation. Journal of Theoretical Biology,486:110089, 2020.
[76] ThomasParr. Messagepassingandmetabolism. Entropy,23(5):606, 2021.
[77] ThomasParr,LancelotDaCosta,andKarlFriston. Markovblankets,informationgeometryandstochasticthermodynamics.
Philosophical Transactions of the Royal Society A,378(2164):20190159, 2020.
[78] ThomasParrandKarlJFriston. Activeinferenceandtheanatomyofoculomotion. Neuropsychologia, 111:334–343, 2018.
[79] Thomas Parr, DimitrijeMarkovic, Stefan J Kiebel, and Karl J Friston. Neuronal message passing using mean-field, Bethe,
andmarginalapproximations. Scientificreports, 9(1):1–18, 2019.
[80] Laurent U Perrinet, RickA Adams, and Karl JFriston. Active inference, eye movements and oculomotor delays. Biological
cybernetics,108(6):777–801, 2014.
[81] L´eoPio-Lopez, Ange Nizard, KarlFriston, and Giovanni Pezzulo. Active inference androbot control: acase study. Journal
of The Royal Society Interface,13(122):20160616, 2016.
[82] RajeshPNRao. Anoptimalestimationapproachtovisualperceptionandlearning. Visionresearch,39(11):1963–1989, 1999.
[83] RajeshPNRaoandDanaHBallard. Predictivecodinginthevisualcortex: afunctionalinterpretationofsomeextra-classical
receptive-fieldeffects. Nature neuroscience,2(1):79–87, 1999.
[84] SamRoweisandZoubinGhahramani. Aunifyingreviewoflineargaussianmodels. Neuralcomputation,11(2):305–345, 1999.
[85] SimoS¨arkka¨. Bayesian filtering and smoothing. CambridgeUniversityPress,2013.
[86] DanSimon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. JohnWiley&Sons,2006.
[87] EmanuelTodorov. Stochasticoptimalcontrolandestimationmethodsadaptedtothenoisecharacteristicsofthesensorimotor
system. Neural computation, 17(5):1084–1108, 2005.
[88] Emanuel Todorov and Michael I Jordan. Optimal feedback control as atheory of motor coordination. Nature neuroscience,
5(11):1226, 2002.
[89] Nicolas GarciaTrillosand Daniel Sanz-Alonso. The bayesian update: variational formulations and gradient flows. Bayesian
Analysis,15(1):29–56, 2020.
[90] ThijsvandeLaar,Ay¸caO¨zc¸elikkale,andHenkWymeersch. Applicationofthefreeenergyprincipletoestimationandcontrol.
IEEE Transactions on Signal Processing, 69:4234–4244, 2021.
[91] Samuel PL Veissi`ere, Axel Constant, Maxwell JD Ramstead, Karl J Friston, and Laurence J Kirmayer. Thinking through
otherminds: Avariationalapproachtocognitionandculture. Behavioral and Brain Sciences,43,2020.
[92] James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm ina predictive coding
networkwithlocalhebbiansynapticplasticity. Neural computation, 29(5):1229–1262, 2017.
[93] James CR Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends in cognitive sciences,
23(3):235–250, 2019.
[94] Robert Wilson and Leif Finkel. A neural implementation of the Kalman filter. Advances in neural information processing
systems,22:2062–2070, 2009.
9