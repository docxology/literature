A Free-Energy Principle for Representation Learning
YansongGao1 andPratikChaudhari2
1AppliedMathematicsandComputationalScience,UniversityofPennsylvania.
2DepartmentofElectricalandSystemsEngineering,UniversityofPennsylvania.
Email: gaoyans@sas.upenn.edu,pratikac@seas.upenn.edu
Abstract
Thispaperemploysaformalconnectionofmachinelearningwiththermodynamicstochar-
acterizethequalityoflearntrepresentationsfortransferlearning. Wediscusshowinformation-
theoreticfunctionalssuchasrate,distortionandclassificationlossofamodellieonaconvex,
so-calledequilibriumsurface. Weprescribedynamicalprocessestotraversethissurfaceunder
constraints, e.g., an iso-classification process that trades off rate and distortion to keep the
classificationlossunchanged. Wedemonstratehowthisprocesscanbeusedfortransferringrep-
resentationsfromasourcedatasettoatargetdatasetwhilekeepingtheclassificationlossconstant.
Experimentalvalidationofthetheoreticalresultsisprovidedonstandardimage-classification
datasets.
Keywords: information theory; thermodynamics; rate-distortion theory; transfer learning;
informationbottleneck;optimaltransportation
1 Introduction
A representation is a statistic of the data that is “useful”. Classical Information Theory creates a
compressedrepresentationandmakesiteasiertostoreortransmitdata;thegoalisalwaystodecode
therepresentationtogettheoriginaldataback. Ifwearegivenimagesandtheirlabels,wecould
learnarepresentationthatisusefultopredictthecorrectlabels. Thisrepresentationisthusastatistic
of the data sufficient for the task of classification. If it is also minimal—sayin its size—it would
discardinformationinthedatathatisnotcorrelatedwiththelabels. Sucharepresentationisunique
tothechosentask,itwouldperformpoorlytopredictsomeotherlabelscorrelatedwiththediscarded
information. Ifinsteadtherepresentationweretohavelotsofredundantinformationaboutthedata,
itcouldpotentiallypredictotherlabelscorrelatedwiththisextrainformation.
Thepremiseofthispaperisourdesiretocharacterizetheinformationdiscardedintherepresenta-
tionwhenitisfitonatask. Wewanttodosoinordertolearnrepresentationsthatcanbetransferred
easilytoothertasks.
Ourmainideaistochooseacanonicaltask—inthispaper,wepickreconstructionoftheoriginal
data—as a way to measure the discarded information. Although one can use any canonical task,
reconstructionisspecial. Itisa“captureall”taskinthesensethatachievingperfectreconstruction
entails that the representation is lossless; information discarded by the original task is therefore
readily measured as the one that helps solve the canonical task. This leads to the study of the
1
0202
beF
72
]GL.sc[
1v60421.2002:viXra
followingLagrangianwhichissimilartotheInformationBottlenckofTishbyetal.[2000]
F(λ,γ) = min R+λD+γC
θ∈Θ,e (z|x),m (z),
θ θ
d (x|z),c (y|z)
θ θ
where the rate R is an upper bound on the mutual information of the representation learnt by the
encoder e (z|x) with the input data x, distortion D measures the quality of reconstruction of the
θ
decoder d (x|z) and C measures the classification loss of the classifier c (y|z). As Alemi and
θ θ
Fischer[2018]show,thisLagrangiancanbeformallyconnectedtoideasinthermodynamics. We
heavilyexploitandspecializethispointofview,assummarizednext.
1.1 Summaryofcontributions
OurmaintechnicalobservationisthatF(λ,γ)canbeintepretedasafree-energyandastochastic
learningprocessthatminimizesitscorrespondingHamiltonianconvergestotheoptimalfree-energy.
Thiscorrespondstoan“equilibriumsurface”ofinformation-theoreticfunctionalsR,D andC anda
surfaceΘ ofthemodelparametersatconvergence. Weprovethattheequilibriumsurfaceisconvex
λ,γ
and itsdual, the free-energy F(λ,γ), is concave. The free-energy is onlya function of Lagrange
multipliers(λ,γ),thefamilyofmodelparametersΘ,andthetask,andisthereforeinvariantofthe
learningdynamics.
Second,wedesignaquasi-staticstochasticprocess,akintoanequilibriumprocessinthermody-
namics,tokeepthemodelparametersθ ontheequilibriumsurface. Suchaprocessallowustotravel
to any feasible values of (R,D,C) while ensuring that the parameters θ of the model are on the
equilibriumsurface. Wefocusononeprocess,the“iso-classificationprocess”whichautomatically
tradesofftherateanddistortiontokeeptheclassificationlossconstant.
Weprescribeaquasi-staticprocessthatallowsforacontrolledtransferoflearntrepresentations.
Itadaptsthemodelparametersasthetaskischangedfromsomesourcedatasettoatargetdataset
whilekeepingtheclassificationlossconstant. Suchaprocessisinstarkcontrasttocurrenttechniques
intransferlearningwhichdonotprovideanyguaranteesonthequalityofthemodelonthetarget
dataset.
Weprovideextensiveexperimentalresultswhichrealizethetheorydevelopedinthispaper.
2 Theroetical setup
Thissectionintroducesnotationandpreliminariesthatformthebuildingblocksofourapproach.
2.1 Auto-Encoders
Consideranencodere(z|x)thatencodesdataxintoalatentcodezandadecoderd(x|z)thatdecodes
z backintotheoriginaldatax. Ifthetruedistributionofthedataisp(x)wemaydefinethefollowing
functionals.
H = E (cid:2) −logp(x) (cid:3)
x∼p(x)
(cid:20) (cid:90) (cid:21)
D = E − dz e(z|x)logd(x|z)
(1)
x∼p(x)
(cid:20)(cid:90) (cid:21)
e(z|x)
R = E dz e(z|x)log
x∼p(x) m(z)
2
(cid:82)
Wedenoteexpectationoverdatausingthenotation(cid:104)ϕ(cid:105) = dxp(x)ϕ. ThefirstfunctionalH
p(x)
is the Shanon entropy of the true data distribution; it quantifies the complexity of the data. The
distortionD measuresthequalityofthereconstructionthroughitslog-likelihood. TherateRisa
Kullback-Leibler(KL)divergence;itmeasurestheaverageexcessbitsusedtoencodesamplesfrom
e(z|x)usingacodethatwasbuiltforourapproximationofthetruemarginalonthelatentfactors
m(z).
2.2 Rate-Distortioncurve
Thefunctionalsin(1)cometogethertogivetheinequality
H −D ≤ I (x;z) ≤ R (2)
e
where I = KL(e(z|x) || p(z|x)) is the KL-divergence between the learnt encoder and the true
e
(unknown)conditionalofthelatentfactors. TheouterinequalityH ≤ D+Rformsthebasisfor
a large body of literature on Evidence Lower Bounds (ELBO, see Kingma and Welling [2013]).
ConsiderFig.1a,ifthecapacityofourcandidatedistributionse(z|x),m(z)andd(x|z)isinfinite,
wecanobtaintheequalityH = R+D. ThisisthethickblacklineinFig.1a.
For finite capacity variational families, say parameterized by θ, which we denote by e (z|x),
θ
d (x|z)andm (z)respectively,asAlemietal.[2017]argue,oneobtainsaconvexRDcurve(shown
θ θ
inredinFig.1a)correspondingtotheLagrangian
F(λ) = min R+λD. (3)
e (z|x),m (z),d (x|z)
θ θ θ
(a) (b)
Figure1: Schematicoftheequilibriumsurface. Fig.1ashowsthatrate(R)anddistortion(D)tradeoff
against each other on the equilibrium surface. Similarly in Fig. 1b, the equilibrium surface is a convex
constraintthatjoinsrate,distortionandtheclassificationloss. Trainingobjectiveswithdifferent(λ,γ)(shown
inredandblue)reachdifferentpartsoftheequilibriumsurface.
This Lagrangian is the relaxation of the idea that given a fixed variational family and data
distributionp(x),thereexistsanoptimalvalueof,say,rateR = f(D)thatbestsandwiches(2). The
optimalLagrangemultiplierisλ = ∂R evaluatedatthedesiredvalueofD.
∂D
3
2.3 Incorporatingtheclassificationloss
Letuscreateaclassifierthatusesthelearntrepresentationz astheinputandsettheclassification
lossasthenegativelog-likelihoodoftheprediction
(cid:20) (cid:90) (cid:21)
C = E − dz e(z|x)logc(y|z) . (4)
x∼p(x)
If the parameters of the model—which now consists of the encoder e(z|x), decoder d(x|z) and
the classifier c(y|z)—are denoted by θ, the training process for the model induces a distribution
p(θ|{(x,y)})where{(x,y)}denotesafinitedataset. InadditiontoR,DandC,theauthorsinAlemi
andFischer[2018]define
(cid:20) (cid:21)
p(θ|{x,y})
S = E log (5)
x∼p(x),y∼p(y|x) m(θ)
whichistherelativeentropyofthedistributiononparametersθ aftertrainingcomparedtoaprior
distributionm(θ)ofourchoosing. UsingaverysimilarargumentasSection2.2thefourfunctionals
R,D,C andS formaconvexthree-dimensionalsurfaceintheRDCSphasespace. Aschematicis
showninFig.1bforσ = 0. WecanagainconsideraLagrangerelaxationofthissurfacegivenby
F(λ,γ,σ) = min R+λD+γC +σS. (6)
e(z|x),m(z),d(x|z),c(y|z)
Remark1(‘The‘FirstLaw”oflearning). AlemiandFischer[2018]drawformalconnectionsof
theLagrangianin(6)withthetheoryofthermodynamics. Justlikethefirstlawofthermodynamicsis
astatementabouttheconservationofenergyinphysicalprocesses,thefactthatthefourfunctionals
aretiedtogetherinasmoothconstraintf(R,D,C,S) = 0leadstoanequationoftheform
dR = −λdD−γ dC −σ dS (7)
whichindicatesthatinformationinlearningprocessesisconserved. Theinformationinthelatent
representationziskepteithertoreconstructbacktheoriginaldataortopredictthelabels. Theformer
iscapturedbytheencoder-decoderpair,thelatteriscapturedbytheclassifier.
Remark2(Settingσ = 0). Thedistributionp(θ|{(x,y)})isaposteriorontheparametersofthe
modelgiventhedataset. Whilethisdistributioniswell-definedunderminortechnicalconditions,e.g.,
ergodicity,performingcomputationswiththisdistributionisdifficult. Wethereforeonlyconsider
thecasewhenσ = 0inthesequelandleavethegeneralcaseforfuturework.
Thefollowinglemma(provedinAppendixB)showsthattheconstraintsurfaceconnectingthe
information-theoretic functionals R,D and C is convex and its dual, the Lagrangian F(λ,γ) is
concave.
Lemma 3 (The RDC constraint surface is convex). The constraint surface f(R,D,C) = 0 is
convexandtheLagrangianF(λ,γ)isconcave.
WecanshowusingasimilarproofthattheentiresurfacejoiningR,D,C andS isconvexby
consideringthecasesλ = 0andγ = 0separately. NotethattheconstraintisconvexinR,D andC;
itneednotbeconvexinthemodelparametersθ thatparameterizee (z|x),m (z),etc.
θ θ
4
2.4 Equilibriumsurfaceofoptimalfree-energy
We next elaborate upon the objective in (6). Consider the functionals R,D and C parameterized
usingparametersθ ∈ Θ ⊆ RN. First,considertheproblem
F(λ,γ) = min R+λD+γC. (8)
e(z|x),θ∈Θ
Wecansolvethisusingcalculusofvariationstoget
(cid:18) (cid:90) (cid:19)
e(z|x) ∝ m (z)d (x|z)λexp γ dy p(y|x) logc (y|z) .
θ θ θ
Weassumeinthispaperthatthelabelsareadeterministicfunctionofthedata,i.e.,p(y|x) = δ(y−y )
x
wherey isthetruelabelofthedatumx. Wethereforehave
x
m (z)d (x|z)λc (y |z)γ
θ θ θ x
e(z|x) =
Z
θ,x
wherethenormalizationconstantis
(cid:90)
Z = dz m (z)d (x|z)λc (y |z)γ. (9)
θ,x θ θ θ x
TheobjectiveF(λ,γ)cannowberewrittenasmaximizingthelog-partitionfunction,alsoknownas
thefree-energyinstatisticalphysics[MezardandMontanari,2009],
F(λ,γ) = min −(cid:104)logZ (cid:105) . (10)
θ,x p(x)
θ∈Θ
Remark4(Whyisitcalledthe“equilibrium”surface?). Givenafinitedataset{(x,y)},onemay
minimizetheobjectivein(8)usingstochasticgradientdescent(SGD,RobbinsandMonro[1951])
onaHamiltonian
H(z;x,θ,λ,γ) ≡ −logm (z)−λlogd (x|z)−γlogc (y|z) (11)
θ θ θ
withupdatesgivenby
(cid:20)(cid:90) (cid:21)
θk+1 = θk −σ ∇ E dz e (z|x)H(z;x,θk,λ,γ) (12)
θ θk
x∼p(x)
where σ > 0 is the step-size; the gradient ∇ is evaluated over samples from p(x) and e (z|x).
θ θ
UsingthesametechniqueasthatofChaudhariandSoatto[2017],onecanshowthattheobjective
E (cid:2) (cid:104)−logZ (cid:105) (cid:3) −σH(p(θ | {x,y})).
θ,x p(x)
θ∼p(θ|{x,y})
decreasesmonotonically. Observethatourobjectivein(8)correspondstothelimitσ → 0ofthis
objectivealongwithauniformnon-informativepriorm(θ)in(5). Infact,thisresultisanalogous
to the classical result that an ergodic Markov chain makes monotonic improvements in the KL-
divergenceasitconvergestothesteady-state,alsoknownas,equilibrium,distribution[Levinand
5
Peres,2017]. Theposteriordistributionofthemodelparametersinducedbythestochasticupdates
in(12)istheGibbsdistributionp∗(θ | {(x,y)}) ∝ exp(−2(R+λD+γC)/σ).
ItisfortheabovereasonthatwecallthesurfaceinFig.1bparameterizedby
(cid:110) (cid:111)
Θ = θ ∈ Θ : −(cid:104)logZ (cid:105) = F(λ,γ) (13)
λ,γ θ,x p(x)
asthe“equilibriumsurface”. Learning,inthiscaseminimizing(8),isinitializedoutsidethissurface
andconvergestospecificpartsoftheequilibriumsurfacedependingupon(λ,γ);thisisdenotedby
theredandbluecurvesinFig.1b. Theconstraintthattiesresultsinthisequilibriumsurfaceisthat
variationalinequalitiessuchas(2)(morearegiveninAlemiandFischer[2018])aretightuptothe
capacityofthemodel. Thisisanalogoustotheconceptofequilibriuminthermodynamics[Sethna,
2006]
3 Dynamical processes on the equilibrium surface
Thissectionprescribesdynamicalprocessesthatexploretheequilibriumsurface. Foranyparameters
θ ∈ Θ,notnecessarilyontheequilibriumsurface,letusdefine
J(θ,λ,γ) = −(cid:104)logZ (cid:105) . (14)
θ,x p(x)
Ifθ ∈ Θ wehaveJ(θ,λ,γ) = F(λ,γ)whichimplies
λ,γ
∇ J(θ,λ,γ) = 0forallθ ∈ Θ . (15)
θ λ,γ
Quasi-static process. A quasi-static process in thermodynamics happens slowly enough for a
system to remain in equilibrium with its surroundings. In our case, we are interested in evolving
Lagrange multipliers (λ,γ) slowly and simultaneously keep the model parameters θ on the equi-
librium surface; the constraint (15) thus holds at each time instant. The equilibrium surface is
parameterizedbyR,D andC sochanging(λ,γ)adaptsthethreefunctionalstotracktheiroptimal
valuescorrespondingtoF(λ,γ).
Letuschoosesomevalues(λ˙,γ˙)andthetrivialdynamics dλ = λ˙ and dγ = γ˙. Thequasi-static
dt dt
constraintleadstothefollowingpartialdifferentialequation(PDE)
d ∂ ∂
0 ≡ ∇ J(θ,λ,γ) = ∇2J θ˙+λ˙ ∇ J +γ˙ ∇ J (16)
dt θ θ ∂λ θ ∂γ θ
validallθ ∈ Θ . Ateachlocationθ ∈ Θ theabovePDEindicateshowtheparametersshould
λ,γ λ,γ
evolveuponchangingtheLagrangemultipliers(λ,γ). WecanrewritethePDEusingtheHamiltonian
H in(11)asshownnext.
Lemma 5 (Equilibrium dynamics for parameters θ). Given (λ˙,γ˙), the parameters θ ∈ Θ
λ,γ
evolveas
θ˙ = A−1b λ˙ +A−1b γ˙
λ γ
(17)
= θ λ˙ +θ γ˙
λ γ
6
whereH istheHamiltonianin(11)and
(cid:104) (cid:68) (cid:69)(cid:105)
A = ∇2J = E (cid:10) ∇2H (cid:11) +(cid:104)∇ H(cid:105)(cid:104)∇ H(cid:105)(cid:62)− ∇ H ∇(cid:62)H ;
θ θ θ θ θ θ
x∼p(x)
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
∂ ∂∇ H ∂H ∂H
b = − ∇ J = − E θ − ∇ H + (cid:104)∇ H(cid:105) ;
λ θ θ θ
∂λ x∼p(x) ∂λ ∂λ ∂λ
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
∂ ∂∇ H ∂H ∂H
b = − ∇ J = − E θ − ∇ H + (cid:104)∇ H(cid:105) .
γ θ θ θ
∂γ x∼p(x) ∂γ ∂γ ∂γ
Alltheinnerexpectations(cid:104)·(cid:105)abovearetakenwithrespecttotheGibbsmeasureoftheHamiltonian,
(cid:82)
ϕexp(−H(z))dz
i.e.,(cid:104)ϕ(cid:105) = . Thedynamicsfortheparametersθ isthereforeafunctionofthetwo
(cid:82)
exp(−H(z))dz
directionalderivatives
θ = A−1 b , and θ = A−1 b (18)
λ λ γ γ
withrespecttoλandγ. NotethatAin(17)istheHessianofastrictlyconvexfunctional.
This lemma allows us to implement dynamical processes for the model parameters θ on the
equilibriumsurface. Asexpected,thisisanordinarydifferentialequation(17)thatdependsonour
chosenevolutionfor(λ˙,γ˙)throughthedirectionalderivativesθ ,θ . Theutilityoftheabovelemma
λ γ
thereforeliesintheexpressionsforthesedirectionalderivatives. AppendixCgivestheproofofthe
abovelemma.
Remark 6 (Implementing the equilibrium dynamics). The equations in Lemma 5 may seem
complicatedtocomputebutobservethattheycanbereadilyestimatedusingsamplesfromthedataset
x ∼ p(x)andthosefromtheencoderz ∼ e (z|x). Thekeydifferencebetween(17)and,say,the
θ
ELBO objective is that the gradient in the former depends upon the Hessian of the Hamiltonian
H. TheseequationscanbeimplementedusingHessian-vectorproducts[Pearlmutter,1994]. Ifthe
dynamicsinvolvescertainconstrainsamongthefunctionals,asRemark7shows,wesimplifythe
implementationofsuchequations.
3.1 Iso-classificationprocess
Aniso-thermalprocessinthermodynamicsisaquasi-staticprocesswhereasystemexchangesenergy
withitssurroundingsandremainsinthermalequilibriumwiththesurroundings. Wenowanalogously
defineaniso-classificationprocessthatadaptsparametersofthemodelθ whilethefree-energyis
subjecttoslowchangesin(λ,γ). Thisadaptationissuchthattheclassificationlossiskeptconstant
whiletherateanddistortionchangeautomatically.
Following the development in Lemma 5, it is easy to create an iso-classification process. We
simplyaddaconstraintoftheform
d
∇ J = 0 (Quasi-StaticCondition)
θ
dt
(19)
d
C = 0 (Iso-classificationCondition).
dt
Usingaverysimilarcomputation(giveninAppendixD)asthatintheproofofLemma5,thisleads
totheconstraineddynamics
0 = C λ˙ +C γ˙
λ γ
(20)
θ˙ = θ λ˙ +θ γ˙.
λ γ
7
ThequantitiesC andC aregivenby
λ γ
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
∂H ∂H (cid:68) (cid:69) (cid:68) (cid:69) (cid:68) (cid:69)
C = − E (cid:104)(cid:96)(cid:105)− (cid:96) + θ(cid:62)∇ H (cid:104)(cid:96)(cid:105)− (cid:96)θ(cid:62)∇ H + θ(cid:62)∇ (cid:96)
λ x∼p(x) ∂λ ∂λ λ θ λ θ λ θ
(21)
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
C = − E ∂H (cid:104)(cid:96)(cid:105)− ∂H (cid:96) + (cid:68) θ(cid:62)∇ H (cid:69) (cid:104)(cid:96)(cid:105)− (cid:10) (cid:96)θT∇ H (cid:11) + (cid:68) θ(cid:62)∇ (cid:96) (cid:69)
γ x∼p(x) ∂γ ∂γ γ θ γ θ γ θ
where (cid:96) = logc (y |z) is the logarithm of the classification loss. Observe that we are not free to
θ x
pickanyvaluesfor(λ˙,γ˙)fortheiso-classificationprocessanymore,theconstraint dC = 0tiesthe
dt
tworatestogether.
Remark7(Implementinganiso-classificationprocess). Thefirstconstraintin(33)allowsusto
choose
∂C ∂2F
λ˙ = −α = −α
∂γ ∂γ2
(22)
∂C ∂2F
γ˙ = α = α
∂λ ∂λ∂γ
whereαisaparametertoscaletime. ThesecondequalitiesinbothrowsfollowbecauseF(λ,γ)is
theoptimalfree-energywhichimpliesrelationslikeD = ∂F andC = ∂F. Wecannowcompute
∂λ ∂γ
thetwoderiativesin(22)usingfinitedifferencestoimplementaniso-classificationprocess. This
is equivalent to running the dynamics in (33) using finite-difference approximation for the terms
∂H, ∂H, ∂∇ θ H, ∂∇ θ H. Whileapproximatingalltheselistedquantitiesateachupdateofθ would
∂λ ∂γ ∂λ ∂γ
becumbersome,exploitingtherelationsin(33)isefficientevenforlargeneuralnetworks,asour
experimentsshow.
Remark8(Otherdynamicalprocessesofinterest). Inthispaper,wefocusoniso-classification
processes. However,followingthesameprogramasthatofthissection,wecanalsodefineother
processes of interest, e.g., one that keeps C +β−1R constant while fine-tuning a model. This is
similartothealternativeInformationBottleneckofAchilleandSoatto[2017]whereintherateis
definedusingtheweightsofanetworkastherandomvariableinsteadofthelatentfactorsz. Thisis
alsoeasilyseentobetheright-handsideofthePAC-Bayesgeneralizationbound[McAllester,2013].
Adynamicalprocessthatpreservesthisfunctionalwouldbeabletocontrolthegeneralizationerror
whichisaninterestingprospectforfuturework.
4 Transferring representations to new tasks
Section3demonstrateddynamicalprocesseswheretheLagrangemultipliersλ,γ changewithtime
andtheprocessadaptsthemodelparametersθ toremainontheequilibriumsurface. Thissection
demonstrates the same concept under a different kind of perturbation, namely the one where the
underlyingtaskchanges. Theprototypicalexampleoneshouldkeepinmindinthissectionisthatof
transferlearningwhereaclassifiertrainedonadatasetps(x,y)isfurthertrainedonanewdataset,
saypt(x,y). Wewillassumethattheinputdomainofthetwodistributionsisthesame.
8
4.1 Changingthedatadistribution
If i.i.d samples from the source task are denoted by Xs = (cid:8) xs,...,xs (cid:9) and those of the target
distributionareXt = (cid:8) xt,...,xt (cid:9) theempiricalsourceandta 1 rgetdist n r s ibutionscanbewrittenas
1 nt
1 (cid:88)
ns
1 (cid:88)
nt
ps(x) =
n s
δ x−xs
i
,andpt(x) =
n t
δ x−xt
i
i=1 i=1
respectively; here δ is a Dirac delta distribution at x(cid:48). We will consider a transport problem
x−x(cid:48)
that transports the source distribution ps(x) to the target distribution pt(x). For any t ∈ [0,1] we
interpolatebetweenthetwodistributionsusingamixture
p(x,t) = (1−t)ps(x)+tpt(x). (23)
Observe that the interpolated data distribution equals the source and target distribution at t = 0
and t = 1 respectively and it is the mixture of the two distributions for other times. We keep the
labelsofthedatathesameanddonotinterpolatethem. AsdiscussedinAppendixFwecanalso
use techniques from optimal transportation [Villani, 2008] to obtain a better transport; the same
dynamicalequationsgivenbelowremainvalidinthatcase.
4.2 Iso-classificationprocesswithachangingdatadistribution
TheequilibriumsurfaceΘ inFig.1bisafunctionofthetaskandalsoevolveswiththetask. We
λ,γ
nowgiveadynamicalprocessthatkeepsthemodelparametersinequilibriumasthetaskevolves
quasi-statically. Weagainhavethesameconditionsforthedynamicsasthosein(19). Thefollowing
lemmaisanalogoustoLemma5.
Lemma 9 (Dynamical process for changing data distribution). Given (λ˙,γ˙), the evolution of
modelparametersθ forachangingdatadistributiongivenby(23)is
θ˙ = θ λ˙ +θ γ˙ +θ (24)
λ γ t
where
(cid:90)
∂p(x,t)
θ = A−1 b =: −A−1 (cid:104)∇ H(cid:105) dx (25)
t t θ
∂t
andtheotherquantitiesareasdefinedinLemma5withtheonlychangethatexpectationsondata
xaretakenwithrespecttop(x,t)insteadofp(x). Theadditionaltermθ arisesbecausethedata
t
distributionchangeswithtime.
AsimilarcomputationasthatofSection3.1givesaquasi-staticiso-classificationprocessasthe
taskevolves
θ˙ = θ λ˙ +θ γ˙ +θ
λ γ t
(26)
0 = C λ˙ +C γ˙ +C
λ γ t
whereC andC areasgivenin(21)withtheonlychangebeingthattheouterexpectationistaken
λ γ
withrespecttox ∼ p(x,t). Thenewtermthatdependsontimetis
(cid:90) ∂p(x,t) (cid:104)(cid:68) (cid:69) (cid:68) (cid:69) (cid:68) (cid:69)(cid:105)
C = − (cid:104)(cid:96)(cid:105)dx− E θ(cid:62)∇ H (cid:104)(cid:96)(cid:105)− θ(cid:62)∇ H (cid:96) + θ(cid:62)∇ (cid:96) (27)
t ∂t x∼p(x,t) t θ t θ t θ
9
with(cid:96) = logc (y |z). Finallyget
θ xt
(cid:18) (cid:19) (cid:18) (cid:19)
C C
θ˙ = θ − λ θ λ˙ + θ − t θ
λ γ t γ
C γ C γ . (28)
=: θˆ λ˙ +θˆ
λ t
Thisindicatesthatθ = θ(λ,t)isasurfaceparameterizedbyλandt,equippedwithabasisoftangent
plane(θˆ ,θˆ).
λ t
4.3 Geodesictransferofrepresentations
ThedynamicsofLemma9isvalidforany(λ˙,γ˙). Weprovidealocallyoptimalwaytochange(λ,γ)
inthissection.
Remark10(Rate-distortiontrade-off). Notethat
C˙ = 0,
(cid:32) (cid:33)
∂D ∂D ∂2F ∂2F (cid:18) ∂2F (cid:19)2
D˙ = λ˙ + γ˙ = −α − = −αdet(Hess(F)),
∂λ ∂γ ∂λ2 ∂γ2 ∂λ∂γ (29)
∂R ∂R
R˙ = D˙ + C˙ = −λD˙.
∂D ∂C
Thefirstequalityissimplyouriso-classificationconstraint. Forα > 0,thesecondoneindicatesthat
D˙ < 0 using Lemma 3 which shows that 0 (cid:31) Hess(F). This also gives λ˙ > 0 in (22). The third
equalityisapowerfulobservation: itindicatesatrade-offbetweenrateanddistortion,ifD˙ < 0we
haveR˙ > 0. ItalsoshowsthegeometricstructureoftheequilibriumsurfacebyconnectingR˙ andD˙
together,whichwewillexploitnext.
ComputingthefunctionalsR,D andC duringtheiso-classificationtransferpresentsuswitha
curveinRDC space. GeodesictransferimpliesthatthefunctionalsR,D followtheshortestpath
inthisspace. Butnoticethatifweassumethatthemodelcapacityisinfinite,theRDC spaceis
Euclideanandthereforethegeodesicissimplyastraightline. Sincewekeeptheclassificationloss
constantduringthetransfer,C˙ = 0,straightlineimpliesthatslopedD/dRisaconstant,sayk. Thus
D˙ = kR˙. ObservethatR˙ = ∂RD˙ + ∂RC˙ + ∂R = −λD˙ + ∂R. Combiningtheiso-classification
∂D ∂C ∂t ∂t
constraintandthefactthatD˙ = kR˙ = −kλD˙ +k∂R,givesusalinearsystem:
∂t
∂D ∂D ∂D k∂R
+ λ˙ + γ˙ = ∂t ;
∂t ∂λ ∂γ 1+kλ
(30)
∂C ∂C ∂C
λ˙ + γ˙ + = 0
∂λ ∂γ ∂t
Wesolvethissystemtoupdate(λ,γ)duringthetransfer.
5 Experimental validation
This section presents experimental validation for the ideas in this paper. We first implement the
dynamicsinSection3thattraversestheequilibriumsurfaceandthendemonstratethedynamical
processfortransferlearningdevisedinSection4.
10
30
25
20
15
10 20
Rate
noitrotsiD
(0.25, 4) 25
(0.25, 6)
(0.25, 8)
20 (0.25, 10)
(0.25, 15)
15
10
5
1 2 3
Lambda
(a)
ammaG
0.8
(0.25, 4)
(0.25, 6)
(0.25, 8) 0.6
(0.25, 10)
(0.25, 15)
0.4
0.2
0.0
0 20 40 60
Number of Steps
(b)
ssoL
noitadilaV
(0.25, 4)
(0.25, 6)
(0.25, 8)
(0.25, 10)
(0.25, 15)
(c)
Figure2: Iso-classificationprocessforMNIST.Werun5differentexperimentsforinitialLagrangemul-
tipliersgivenbyλ = 0.25andγ ∈ {4,6,8,10,15}. Duringeachexperiment, wemodifytheseLagrange
multipliers(Fig.2b)tokeeptheclassificationlossconstantandplottherate-distortioncurve(Fig.2a)along
withthevalidationloss(Fig.2c).Thevalidationaccuracyisconstantforeachexperiment;itisbetween92–98%
fortheseinitialvaluesof(λ,γ). Similarlythetraininglossisalmostunchangedduringeachexperimentand
takesvaluesbetween0.06–0.2fordifferentvaluesof(λ,γ).
70
65
60
55
50
20 25 30
Rate
noitrotsiD
50
(0.5, 5)
(0.5, 10)
40
(0.5, 15)
(0.5, 20)
30
20
10
0
0.50 0.75 1.00 1.25
Lambda
(a)
ammaG
100
(0.5, 5)
(0.5, 10)
(0.5, 15) 95
(0.5, 20)
90
85
80
0 10 20 30
Number of Steps
(b)
ycaruccA
noitadilaV
(0.5, 5)
(0.5, 10)
(0.5, 15)
(0.5, 20)
(c)
Figure 3: Iso-classification process for CIFAR-10. We run 4 different experiments for initial Lagrange
multipliersλ = 0.5andγ ∈ {5,10,15,20}. Duringeachexperiment,wemodifytheLagrangemultipliers
(Fig.3b)tokeeptheclassificationlossconstantandplottherate-distortioncurve(Fig.3a)alongwiththe
validationaccuracy(Fig.3c). Thevalidationlossisconstantduringeachexperiment;ittakesvaluesbetween
0.5–0.8fortheseinitialvaluesof(λ,γ). Similarly, thetraininglossisconstantandtakesvaluesbetween
0.02–0.09fortheseinitialvaluesof(λ,γ). Observethattherate-distortioncurveinFig.3aismuchflatter
thantheoneinFig.2awhichindicatesthatthemodelfamilyΘforCIFAR-10ismuchmorepowerful;this
correspondstothestraightlineintheRDcurveforaninfinitemodelcapacityisasshowninFig.1a.
Setup. WeusetheMNIST[LeCunetal.,1998]andCIFAR-10[Krizhevsky,2009]datasetsforour
experiments. Weusea2-layerfully-connectednetwork(sameasthatofKingmaandWelling[2013])
astheencoderanddecoderforMNIST;theencoderforCIFAR-10isaResNet-18[Heetal.,2016]
architecturewhilethedecoderisa4-layerdeconvolutionalnetwork[Nohetal.,2015]. Fulldetailsof
thepre-processing,networkarchitectureandtrainingareprovidedinAppendixA.
11
46
44
42
40
0 50 100
Epoch
ygrenE
eerF
100
80
60
40
20
0
0 20 40 60
Number of Steps
(a)
ygrenE
eerF
(0.25, 4)
(b)
Figure 4: Variation of the free-energy F(λ,γ) across the equilibration and the iso-classification pro-
cesses. Fig.4ashowsthefree-energyduringequilibrationbetweensmallchangesof(λ,γ). Theinitialand
finalvaluesoftheLagrangemultipliersare(0.5,1)and(0.51,1.04)respectivelyandthefree-energyisabout
thesameforthesevalues. Fig.4bshowsthefree-energyas(λ,γ)undergoalargechangefromtheirinitial
valueof(0.25,4)to(3.5,26)duringtheiso-classificationprocessinFig.2. Sincetherate-distortionchangea
lot(Fig.2a),thefree-energyalsochangesalotevenifC isconstant(Fig.2c). NumberofstepsinFig.4b
referstothenumberofstepsofrunning(31).
5.1 Iso-classificationprocessontheequilibriumsurface
Thisexperimentdemonstratestheiso-classificationprocessinRemark7. AsdiscussedinRemark4,
trainingamodeltominimizethefunctionalR+λD+γC decreasesthefree-energymonotonically.
Details. GivenavalueoftheLagrangemultipliers(λ,γ)wefirstfindamodelontheequilibrium
surfacebytrainingfromscratchfor120epochswiththeAdamoptimizer[KingmaandBa,2014];
the learning rate is set to 10−3 and drops by a factor of 10 every 50 epochs. We then run the
iso-classificationprocessforthesemodelsinRemark7asfollows. Wemodify(λ,γ)accordingto
theequations
∂C ∂C
λ˙ = −α and γ˙ = α . (31)
∂γ ∂λ
Changesin(λ,γ)causetheequilibriumsurfacetochange,soitisnecessarytoadaptthemodel
parameters θ so as to keep them on the dynamically changing surface; let us call this process of
adaptation“equilibriation”. Weachievethisbytakinggradient-basedupdatestominimizeJ(λ,γ)
with a learning rate schedule that looks like a sharp quick increase from zero and then a slow
annealingbacktozero. Thelearningratescheduleisgivenbyη(t) = (t/T)2(1−t/T)5 wheret
isthenumberofmini-batchupdatestakensincethelastchangein(λ,γ)andT istotalnumberof
mini-batchupdatesofequilibration. Themaximumvalueofthelearningrateissetto1.5×10−3.
Thefree-energyshouldbeunchangedifthemodelparametersareontheequilibriumsurfaceafter
equilibration;thisisshowninFig.4a. Partialderivativesin(31)arecomputedusingfinite-differences.
Fig.2showstheresultfortheiso-classificationprocessforMNISTandFig.3showsasimilar
resultforCIFAR-10. Wecanseethattheclassificationlossremainsconstantthroughtheprocess.
Thisexperimentshowsthatwecanimplementaniso-classificationprocesswhilekeepingthemodel
parametersontheequilibriumsurfaceduringit.
12
5.2 Transferringrepresentationstonewdata
We next present experimental results of an iso-classification process for transferring the learnt
representation. Wepickthesourcedatasettobeallimagescorrespondingtodigits0–4inMNIST
andthetargetdatasetisitscomplement,imagesofdigits5–9. Ourgoalistoadaptamodeltrainedon
thesourcetasktothetargettaskwhilekeepingitsclassificationlossconstant. Werunthegeodesic
transferdynamicsfromSection4.3andtheresultsareshowninFig.5.
25
20
15
10
10 15
Rate
noitrotsiD
100
Geodesic Quasi-Static Process
95
90
85
0 20 40
number of steps
(a)
ycaruccA
noitadilaV
Non-Equilibrium Process
Geodesic Quasi-Static Process
Training on Target Domain
(b)
Figure5:TransferringfromsourcedatasetofMNISTdigits0–4tothetargetdatasetconsistingofdigits
5–9. Fig.5ashowsthevariationofrateanddistortionduringthetransfer; asdiscussedinSection4.3we
maintain a constant dR/dD during the transfer; the rate decreases and the distortion increases. Fig. 5b
showsthevalidationaccuracyduringthetransfer. Theorangecurvecorrespondstogeodesiciso-classification
transfer;thebluecurveistheresultofdirectlyfine-tuningthesourcemodelonthetargetdata(notethevery
lowaccuracyatthestart);thegreenpointistheaccuracyoftrainingonthetargettaskfromscratch.
Itisevidentthattheclassificationaccuracyisconstantthroughoutthetransferandisalsothe
sameasthatoftrainingfromscratchonthetarget. MNISTisansimpledatasetandtheaccuracygap
betweeniso-classificationtransfer,fine-tuningfromthesourceandtrainingfromscratchisminor.
The benefit of running the iso-classification transfer however is that we can be guaranteed about
thefinalaccuracyofthemodel. Weexpectthegapbetweenthesethreetobesignificantformore
complex datasets. Results for a similar experiment for transferring between a source dataset that
consists of all vehicles in CIFAR-10 to a target dataset that consists of all animals are provided
inAppendixG.
6 Related work
WearemotivatedbytheInformationBottleneck(IB)principleofShwartz-ZivandTishby[2017];
Tishby et al. [2000], which has been further explored by Achille and Soatto [2017]; Alemi et al.
[2016]; Higgins et al. [2017]. The key difference in our work is that while these papers seek to
understandtherepresentationforagiventask,wefocusonhowtherepresentationcanbeadapted
toanewtask. Further,theLagrangianin(8)hasconnectionstoPAC-Bayesbounds[Dziugaiteand
Roy, 2017; McAllester, 2013] and training algorithms that use the free-energy [Chaudhari et al.,
2019]. Ouruseofrate-distortionfortransferlearningisclosetotheworkonunsupervisedlearning
ofBrekelmansetal.[2019];VerSteegandGalstyan[2015].
ThispaperbuildsupontheworkofAlemiandFischer[2018];Alemietal.[2017]. Werefine
13
someresultstherein,viz.,weprovideaproofoftheconvexityoftheequilibriumsurfaceandidentify
itwiththeequilibriumdistributionofSGD(Remark4). Weintroducenewideassuchasdynamical
processesontheequilibriumsurface. Ouruseofthermodynamicsispurelyasaninspiration;thework
presentedhereismathematicallyrigorousandalsoprovidesanimmediatealgorithmicrealizationof
theideas.
Thispaperhasstrongconnectionstoworksthatstudystochasticprocessesinspiredfromstatistical
physicsformachinelearning,e.g.,approximateBayesianinferenceandimplicitregularizationof
SGD[ChaudhariandSoatto,2017;Mandtetal.,2017],variationalinference[Jordanetal.,1998;
KingmaandWelling,2013]. Theiso-classificationprocessinstantiatesan“automatic”regularization
via the trade-off between rate and distortion; this point-of-view is an exciting prospect for future
work. Thetechnicalcontentofthepaperalsodrawsfromoptimaltransportation[Villani,2008].
Alargenumberofapplicationsbeginwithpre-trainedmodels[Girshicketal.,2014;SharifRaza-
vian et al., 2014] or models trained on tasks different [Doersch and Zisserman, 2017]. Current
methodsintransferlearninghoweverdonotcomewithguaranteesovertheperformanceonthetarget
dataset,althoughthereisarichbodyofolderwork[Baxter,2000]andongoingworkthatstudies
this[Zamiretal.,2018]. Theinformation-theoreticunderstandingoftransferandtheconstrained
dynamicalprocessesdevelopedinourpaperisafirststeptowardsbuildingsuchguarantees. Inthis
context, our theory can also be used to tackle catastrophic forgetting Kirkpatrick et al. [2017] to
“detune”themodelpost-trainingandbuildupredundantfeatures.
7 Discussion
Wepresenteddynamicalprocessesthatmaintaintheparametersofmodelonanequilibriumsurface
thatarisesoutofacertainfree-energyfunctionalfortheencoder-decoder-classifierarchitecture. The
decoderactsasameasureoftheinformationdiscardedbytheencoder-classifierpairwhilefitting
onagiventask. Weshowedhowonecandevelopaniso-classificationprocessthattravelsonthe
equilibriumsurfacewhilekeepingtheclassificationlossconstant. Weshowedaniso-classification
transfer learning process which keeps the classification loss constant while adapting the learnt
representationfromasourcetasktoatargettask.
The information-theoretic point-of-view in this paper is rather abstract but its benefit lies in
its exploitation of the equilibrium surface. Relationships between the three functionals, namely
rate,distortionandclassification,thatdefinethissurface,asalsootherfunctionalsthatconnectto
thecapacityofthehypothesisclasssuchastheentropyS mayallowustodefineinvariantsofthe
learningprocess. Forcomplexmodelssuchasdeepneuralnetworks,suchaprogrammayleadan
understandingoftheprinciplesthatgoverntheirworking.
References
Achille, A. and Soatto, S. (2017). On the emergence of invariance and disentangling in deep representations.
arXiv:1706.01350.
Alemi,A.A.andFischer,I.(2018). Therml:Thermodynamicsofmachinelearning. arXivpreprintarXiv:1807.04162.
Alemi,A.A.,Fischer,I.,Dillon,J.V.,andMurphy,K.(2016).Deepvariationalinformationbottleneck.arXiv:1612.00410.
Alemi,A.A.,Poole,B.,Fischer,I.,Dillon,J.V.,Saurous,R.A.,andMurphy,K.(2017). Fixingabrokenelbo. arXiv
preprintarXiv:1711.00464.
14
Baxter,J.(2000). Amodelofinductivebiaslearning. Journalofartificialintelligenceresearch,12:149–198.
Brekelmans,R.,Moyer,D.,Galstyan,A.,andVerSteeg,G.(2019). Exactrate-distortioninautoencodersviaechonoise.
InAdvancesinNeuralInformationProcessingSystems,pages3884–3895.
Chaudhari,P.,Choromanska,A.,Soatto,S.,LeCun,Y.,Baldassi,C.,Borgs,C.,Chayes,J.,Sagun,L.,andZecchina,
R.(2019). Entropy-sgd: Biasinggradientdescentintowidevalleys. JournalofStatisticalMechanics: Theoryand
Experiment,2019(12):124018.
Chaudhari,P.andSoatto,S.(2017). Stochasticgradientdescentperformsvariationalinference,convergestolimitcycles
fordeepnetworks. arXivpreprintarXiv:1710.11029.
Cuturi,M.(2013). Sinkhorndistances:Lightspeedcomputationofoptimaltransport. InAdvancesinneuralinformation
processingsystems,pages2292–2300.
Doersch,C.andZisserman,A.(2017).Multi-taskself-supervisedvisuallearning.InProceedingsoftheIEEEInternational
ConferenceonComputerVision,pages2051–2060.
Dziugaite,G.K.andRoy,D.M.(2017). Computingnonvacuousgeneralizationboundsfordeep(stochastic)neural
networkswithmanymoreparametersthantrainingdata. arXivpreprintarXiv:1703.11008.
Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2014). Richfeaturehierarchiesforaccurateobjectdetectionand
semanticsegmentation. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
580–587.
He,K.,Zhang,X.,Ren,S.,andSun,J.(2016). Identitymappingsindeepresidualnetworks. arXiv:1603.05027.
Higgins,I.,Matthey,L.,Pal,A.,Burgess,C.,Glorot,X.,Botvinick,M.,Mohamed,S.,andA,L.(2017). beta-VAE:
LearningBasicVisualConceptswithaConstrainedVariationalFramework. InICLR.
Ioffe,S.andSzegedy,C.(2015). Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariate
shift. arXiv:1502.03167.
Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,andSaul,L.K.(1998). Anintroductiontovariationalmethodsforgraphical
models. InLearningingraphicalmodels,pages105–161.Springer.
Kingma,D.andBa,J.(2014). Adam:Amethodforstochasticoptimization. arXiv:1412.6980.
Kingma,D.P.andWelling,M.(2013). Auto-encodingvariationalBayes. arXiv:1312.6114.
Kirkpatrick,J.,Pascanu,R.,Rabinowitz,N.,Veness,J.,Desjardins,G.,Rusu,A.A.,Milan,K.,Quan,J.,Ramalho,T.,
Grabska-Barwinska,A.,etal.(2017). Overcomingcatastrophicforgettinginneuralnetworks. Proceedingsofthe
nationalacademyofsciences,114(13):3521–3526.
Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Master’s thesis, Computer Science,
UniversityofToronto.
LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P.(1998). Gradient-basedlearningappliedtodocumentrecognition.
ProceedingsoftheIEEE,86(11):2278–2324.
Levin,D.A.andPeres,Y.(2017). Markovchainsandmixingtimes,volume107. AmericanMathematicalSoc.
Mandt,S.,Hoffman,M.D.,andBlei,D.M.(2017). StochasticGradientDescentasApproximateBayesianInference.
arXiv:1704.04289.
McAllester,D.(2013). Apac-bayesiantutorialwithadropoutbound. arXiv:1307.2118.
Mezard,M.andMontanari,A.(2009). Information,physics,andcomputation. OxfordUniversityPress.
15
Noh,H.,Hong,S.,andHan,B.(2015). Learningdeconvolutionnetworkforsemanticsegmentation. InProceedingsofthe
IEEEinternationalconferenceoncomputervision,pages1520–1528.
Pearlmutter,B.A.(1994). Fastexactmultiplicationbythehessian. Neuralcomputation,6(1):147–160.
Robbins,H.andMonro,S.(1951). Astochasticapproximationmethod. Theannalsofmathematicalstatistics,pages
400–407.
Sethna,J.(2006). Statisticalmechanics:entropy,orderparameters,andcomplexity,volume14. OxfordUniversityPress.
SharifRazavian,A.,Azizpour,H.,Sullivan,J.,andCarlsson,S.(2014). Cnnfeaturesoff-the-shelf:anastoundingbaseline
forrecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognitionworkshops,pages
806–813.
Shwartz-Ziv,R.andTishby,N.(2017).Openingtheblackboxofdeepneuralnetworksviainformation.arXiv:1703.00810.
Tishby,N.,Pereira,F.C.,andBialek,W.(2000). Theinformationbottleneckmethod. arXivpreprintphysics/0004057.
VerSteeg,G.andGalstyan,A.(2015). Maximallyinformativehierarchicalrepresentationsofhigh-dimensionaldata. In
ArtificialIntelligenceandStatistics,pages1004–1012.
Villani,C.(2008). Optimaltransport:oldandnew,volume338. SpringerScience&BusinessMedia.
Zamir,A.R.,Sax,A.,Shen,W.,Guibas,L.J.,Malik,J.,andSavarese,S.(2018). Taskonomy:Disentanglingtasktransfer
learning. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages3712–3722.
16
Appendix
A Details of the experimental setup
Datasets. WeusetheMNIST[LeCunetal.,1998]andCIFAR-10[Krizhevsky,2009]datasetsfor
these experiments. The former consists of 28 ×28-sized gray-scale images of handwritten digits
(60,000trainingand10,000validation). Thelatterconsistsof32×32-sizedRGBimages(50,000
trainingand10,000forvalidation)spreadacross10classes;4oftheseclasses(airplane,automobile,
ship,truck)aretransportation-basedwhiletheothersareimagesofanimalsandbirds.
Architectureandtraining. Allmodelsinourexperimentsconsistofanencoder-decoderpair
alongwithaclassifierthattakesinthelatentrepresentationasinput. ForexperimentsonMNIST,both
encoderanddecoderaremulti-layerperceptronswith2fully-connectedlayers,thedecoderusesa
mean-squareerrorloss,i.e.,aGaussianreconstructionlikelihoodandtheclassifierconsistsofasingle
fully-connectedlayer. ForexperimentsonCIFAR-10,weusearesidualnetwork[Heetal.,2016]
with18layersasanencoderandadecoderwithonefully-connectedlayerand4deconvolutional
layers[Nohetal.,2015]. TheclassifiernetworkforCIFAR-10isasinglefully-connectedlayer. All
modelsuseReLUnon-linearitiesandbatch-normalization[IoffeandSzegedy,2015]. Furtherdetails
ofthearchitecturearegiveninAppendixA.WeuseAdam[KingmaandBa,2014]totrainallmodels
withcosinelearningrateannealing.
TheencoderanddecoderforMNISThas784–256–16neuronsoneachlayer;theencodingz is
thus16-dimensionalwhichistheinputtothedecoder. Theclassifierhasonehiddenlayerwith12
neuronsand10outputs. TheencoderforCIFAR-10isa18-layerresidualneuralnetwork(ResNet-18)
andthedecoderhas4deconvolutionallayers. Weusedaslightlylargernetworkforthegeodesic
transferlearningexperimentonMNIST.Theencoderanddecoderhave784–400–64neuronsineach
layerwithadropoutofprobability0.1afterthehiddenlayer. Theclassifierhasasinglelayerthat
takesthe64-dimensionalencodingandpredicts10classes.
B Proof of Lemma 3
ThesecondstatementdirectlyfollowsbyobservingthatF isaminimumofaffinefunctionsin(λ,γ).
Toseethefirst,evaluatetheHessianofRandF
(cid:32) ∂2R ∂2R (cid:33) (cid:32) ∂2F ∂2F (cid:33)
Hess(R)Hess(F) = ∂D2 ∂D∂C ∂λ2 ∂λ∂γ
∂2R ∂2R ∂2F ∂2F
∂C∂D ∂C2 ∂γ∂λ ∂γ2
SincewehaveF = min R+λD+γC,weobtain
e (z|x),d (x|z),m (z)
θ θ θ
∂R ∂R ∂F ∂F
λ = − , γ = − , D = , C = .
∂D ∂C ∂λ ∂γ
17
Wethenhave
(cid:18) ∂R (cid:19) ∂2R ∂2R
dλ = −d = − dD− dC
∂D ∂D2 ∂D∂C
∂2R (cid:18) ∂D ∂D (cid:19) ∂2R (cid:18) ∂C ∂C (cid:19)
= − dλ+ dγ − dλ+ dγ
∂D2 ∂λ ∂γ ∂D∂C ∂λ ∂γ
(cid:18) ∂2R∂2F ∂2R ∂2F (cid:19) (cid:18) ∂2R ∂2F ∂2R ∂2F (cid:19)
= − + dλ− + dγ;
∂D2 ∂λ2 ∂D∂C ∂γ∂λ ∂D2∂λ∂γ ∂D∂C ∂γ2
(cid:18) ∂R (cid:19) ∂2R ∂2R
dγ = −d = − dD− dC
∂C ∂C∂D ∂C2
∂2R (cid:18) ∂D ∂D (cid:19) ∂2R (cid:18) ∂C ∂C (cid:19)
= − dλ+ dγ − dλ+ dγ
∂C∂D ∂λ ∂γ ∂C2 ∂λ ∂γ
(cid:18) ∂2R ∂2F ∂2R ∂2F (cid:19) (cid:18) ∂2R ∂2F ∂2R∂2F (cid:19)
= − + dλ− + dγ.
∂C∂D ∂λ2 ∂C2∂γ∂λ ∂C∂D∂λ∂γ ∂C2 ∂γ2
Comparethecoefficientsonbothsidestoget
∂2R∂2F ∂2R ∂2F ∂2R ∂2F ∂2R∂2F
+ = + = −1;
∂D2 ∂λ2 ∂D∂C ∂γ∂λ ∂C∂D∂λ∂γ ∂C2 ∂γ2
∂2R ∂2F ∂2R ∂2F ∂2R ∂2F ∂2R ∂2F
+ = + = 0,
∂D2∂λ∂γ ∂D∂C ∂γ2 ∂C∂D ∂λ2 ∂C2∂γ∂λ
therefore
Hess(R)Hess(F) = −I.
Since 0 (cid:31) Hess(F), we have that Hess(R) (cid:31) 0, then the constraint surface f(R,D,C) = 0 is
convex.
C Proof of Lemma 5
Recallthedefinitionoftheobjectivefunction(14),firstwecomputethegradientoftheobjective
functionasfollowing:
∇ J(θ,λ,γ) = − E ∇ logZ
θ θ θ,x
x∼p(x)
1
= − E ∇ Z
θ θ,x
x∼p(x) Z θ,x
(cid:90)
1
= − E (−∇ H) exp(−H)dz
θ
x∼p(x) Z θ,x
= E (cid:104)∇ H(cid:105)
θ
x∼p(x)
18
Thenwithsomeeffortofcomputation,weget
(cid:90)
A = ∇2J(θ,λ,γ) = ∇ E (cid:2) 1 ∇ H exp(−H)dz (cid:3)
θ θ x∼p(x) Z θ,x θ
=
x∼
E
p(x)
(cid:34) −
Z
1
θ 2 ,x
(cid:18)(cid:90) (−∇θH)exp(−H)dz (cid:19)(cid:18)(cid:90) ∇T
θ
H exp(−H)dz (cid:19) +
Z
1
θ,x
(cid:90) ∇2
θ
H exp(−H)dz−
Z
1
θ,x
(cid:90) ∇θH∇(cid:62)
θ
H exp(−H)dz (cid:35)
(cid:104) (cid:68) (cid:69)(cid:105)
= E (cid:10) ∇2H (cid:11) +(cid:104)∇ H(cid:105)(cid:104)∇ H(cid:105)(cid:62)− ∇ H ∇(cid:62)H ;
θ θ θ θ θ
x∼p(x)
(cid:20) (cid:90) (cid:21)
∂ ∂ 1
b = − ∇ J = − E ∇ H exp(−H)dz
λ θ θ
∂λ ∂λ x∼p(x) Z θ,x
= −
x∼
E
p(x)
(cid:34) −
Z
1
θ 2 ,x
(cid:18)(cid:90) − ∂
∂
H
λ
exp(−H)dz (cid:19)(cid:18)(cid:90) ∇θH exp(−H)dz (cid:19) +
Z
1
θ,x
(cid:90)
∂
∂
λ
∇θH exp(−H)dz−
Z
1
θ,x
(cid:90) ∂
∂
H
λ
∇θH exp(−H)dz (cid:35)
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
∂∇ H ∂H ∂H
= − E θ − ∇ H + (cid:104)∇ H(cid:105) ;
θ θ
x∼p(x) ∂λ ∂λ ∂λ
(cid:90)
b = − ∂ ∇ J = − ∂ E (cid:2) 1 ∇ H exp(−H)dz (cid:3)
γ θ θ
∂γ ∂γ x∼p(x) Z θ,x
= −
x∼
E
p(x)
(cid:34) −
Z
1
θ 2 ,x
(cid:18)(cid:90) − ∂
∂
H
γ
exp(−H)dz (cid:19)(cid:18)(cid:90) ∇θH exp(−H)dz (cid:19) +
Z
1
θ,x
(cid:90)
∂
∂
γ
∇θH exp(−H)dz−
Z
1
θ,x
(cid:90) ∂
∂
H
γ
∇θH exp(−H)dz (cid:35)
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29) (cid:28) (cid:29) (cid:21)
∂∇ H ∂H ∂H
= − E θ − ∇ H + (cid:104)∇ H(cid:105) .
θ θ
x∼p(x) ∂γ ∂γ ∂γ
Accordingtothequasi-staticconstraints(16),wehave
Aθ˙−λ˙b −γ˙b = 0,
λ γ
thatimplies
θ˙ = A−1b λ˙ +A−1b γ˙ = θ λ˙ +θ γ˙. (32)
λ γ λ γ
D Computation of Iso-classification constraint
Westartwithcomputingthegradientofclassificationloss,clearthatC = E (cid:2) − (cid:82) dz e(z|x)logc(y|z) (cid:3) =
x∼p(x)
−E (cid:104)(cid:96)(cid:105),where(cid:96) = logc (y |z)isthelogarithmoftheclassificationloss,then
x∼p(x) θ x
(cid:20) (cid:90) (cid:21)
1
∇ C = −∇ E (cid:96) exp(−H)dz
θ θ
x∼p(x) Z θ,x
(cid:34) 1 (cid:18)(cid:90) (cid:19)(cid:18)(cid:90) (cid:19) 1 (cid:90) 1 (cid:90) (cid:35)
= − E − (−∇ H) exp(−H)dz (cid:96) exp(−H)dz + ∇ (cid:96) exp(−H)dz− (cid:96)∇ H exp(−H)dz
x∼p(x) Z θ 2 ,x θ Z θ,x θ Z θ,x θ
= − E [(cid:104)∇ (cid:96)(cid:105)+(cid:104)∇ H(cid:105)(cid:104)(cid:96)(cid:105)−(cid:104)(cid:96)∇ H(cid:105)];
θ θ θ
x∼p(x)
(cid:20) (cid:90) (cid:21)
∂ ∂ 1
C = − E (cid:96) exp(−H)dz
∂λ ∂λ x∼p(x) Z θ,x
(cid:34) 1 (cid:18)(cid:90) ∂H (cid:19)(cid:18)(cid:90) (cid:19) 1 (cid:90) ∂H (cid:35)
= − E − − exp(−H)dz (cid:96) exp(−H)dz − (cid:96) exp(−H)dz
x∼p(x) Z θ 2 ,x ∂λ Z θ,x ∂λ
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29)(cid:21)
∂H ∂H
= − E (cid:104)(cid:96)(cid:105)− (cid:96) ;
x∼p(x) ∂λ ∂λ
19
(cid:20) (cid:90) (cid:21)
∂ ∂ 1
C = − E (cid:96) exp(−H)dz
∂γ ∂γ x∼p(x) Z θ,x
(cid:34) (cid:35)
(cid:18)(cid:90) (cid:19)(cid:18)(cid:90) (cid:19) (cid:90)
1 ∂H 1 ∂H
= − E − − exp(−H)dz (cid:96) exp(−H)dz − (cid:96) exp(−H)dz
x∼p(x) Z θ 2 ,x ∂λ Z θ,x ∂γ
(cid:20)(cid:28) (cid:29) (cid:28) (cid:29)(cid:21)
∂H ∂H
= − E (cid:104)(cid:96)(cid:105)− (cid:96) .
x∼p(x) ∂γ ∂γ
Theiso-classificationlossconstrainstogetherwithquasi-staticconstrainsimplythat:
d
0 ≡ C
dt
∂C ∂C
= θ˙(cid:62) ∇ C +λ˙ +γ˙
θ
∂λ ∂γ
(cid:18) (cid:19) (cid:18) (cid:19)
∂C ∂C
= λ˙ θ(cid:62) ∇ C + +γ˙ θ(cid:62) ∇ C +
λ θ ∂λ γ θ ∂γ
= −λ˙
x∼
E
p(x)
(cid:20)(cid:28)∂
∂
H
λ
(cid:29) (cid:104)(cid:96)(cid:105)− (cid:28) (cid:96) ∂
∂
H
λ
(cid:29) + (cid:68) θ
λ
(cid:62)∇θH (cid:69) (cid:104)(cid:96)(cid:105)− (cid:68) (cid:96)θ
λ
(cid:62)∇θH (cid:69) + (cid:68) θ
λ
(cid:62)∇θ(cid:96) (cid:69)(cid:21) −γ˙
x∼
E
p(x)
(cid:20)(cid:28)∂
∂
H
γ
(cid:29) (cid:104)(cid:96)(cid:105)− (cid:28) (cid:96) ∂
∂
H
γ
(cid:29) + (cid:68) θ
γ
(cid:62)∇θH (cid:69) (cid:104)(cid:96)(cid:105)− (cid:68) (cid:96)θ
γ
(cid:62)∇θH (cid:69) + (cid:68) θ
γ
(cid:62)∇θ(cid:96) (cid:69)(cid:21)
= C λ˙ +C γ˙,
λ γ
wherethethirdequationisfollowedbytheequilibriumdynamics(17)forparametersθ. Sofarwe
developedtheconstraineddynamicsforiso-classificationprocess:
0 = C λ˙ +C γ˙
λ γ
(33)
θ˙ = θ λ˙ +θ γ˙.
λ γ
E Iso-classification equations for changing data distribution
Inthissectionweanalyzethedynamicsforiso-classificationlossprocesswhenthedatadistribution
∂p(x)
evolves with time. will lead to additional terms that represent the partial derivatives with
∂t
respecttotonboththequasi-staticandiso-classificationconstrains. Moreprecisely,thenewterms
are
(cid:90)
∂ ∂p(x)
b = − ∇ J = − (cid:104)∇ H(cid:105)dx;
t θ θ
∂t ∂t
(cid:90)
∂ ∂p(x)
C = − (cid:104)(cid:96)(cid:105)dx,
∂t ∂t
20
thenthequasi-staticandiso-classificationconstraintsarereadytobemodifiedas
d ∂∇ F ∂∇ F ∂∇ F
0 ≡ ∇ J(θ,λ,γ) ⇐⇒ 0 = ∇2F θ˙+λ˙ θ +γ˙ θ + θ
dt θ θ ∂λ ∂γ ∂t
⇐⇒ θ˙ = λ˙ A−1 b +γ˙ A−1 b +A−1 b
λ γ t
⇐⇒ θ˙ = λ˙ θ +γ˙ θ +θ ;
λ γ t
d ∂C ∂C ∂C
0 ≡ C ⇐⇒ 0 = θ˙(cid:62) ∇ C +λ˙ +γ˙ +
θ
dt ∂λ ∂γ ∂t
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
∂C ∂C ∂C
⇐⇒ 0 = λ˙ θ(cid:62) ∇ C + +γ˙ θ(cid:62) ∇ C + + θ(cid:62) ∇ C +
λ θ ∂λ γ θ ∂γ t θ ∂t
⇐⇒ 0 = λ˙ C +γ˙ C +C ,
λ γ t
whereA,b ,b ,C andC whereC andC areasgiveninlemma5and (21)withtheonlychange
λ γ λ γ λ γ
beingthattheouterexpectationistakenwithrespecttox ∼ p(x,t). Thenewtermsthatdependson
timetare
(cid:90) ∂p(x,t) (cid:104)(cid:68) (cid:69) (cid:68) (cid:69) (cid:68) (cid:69)(cid:105)
C = − (cid:104)(cid:96)(cid:105)dx− E θ(cid:62)∇ H (cid:104)(cid:96)(cid:105)− θ(cid:62)∇ H (cid:96) + θ(cid:62)∇ (cid:96) (34)
t ∂t x∼p(x,t) t θ t θ t θ
with(cid:96) = logc (y |z). Wecancombinemodifiedquasi-staticandiso-classificationconstraintsto
θ xt
get
(cid:18) (cid:19) (cid:18) (cid:19)
C C
θ˙ = θ − λ θ λ˙ + θ − t θ
λ γ t γ
C γ C γ . (35)
=: θˆ λ˙ +θˆ
λ t
Thisindicatesthatθ = θ(λ,t)isasurfaceparameterizedbyλandt,equippedwithabasisoftangent
plane(θˆ ,θˆ).
λ t
F Optimally transporting the data distribution
Wefirstgiveabriefdescriptionofthetheoryofoptimaltransportation. Theoptimaltransportmap
betweenthesourcetaskandthetargettaskwillbeusedtodefineadynamicalprocessforthetask.
Weonlycomputethetransportfortheinputsxbetweenthesourceandtargetdistributionsandusea
heuristictoobtainthetransportforthelabelsy. Thischoiceismadeonlytosimplifytheexposition;
itisstraightforwardtohandlethecaseoftransportonthejointdistributionp(x,y).
If i.i.d samples from the source task are denoted by (cid:8) xs,...,xs (cid:9) and those of the target
distributionare (cid:8) xt,...,xt (cid:9) theempiricalsourceandtargetd 1 istributi n o s nscanbewrittenas
1 nt
1 (cid:88)
ns
1 (cid:88)
nt
ps(x) =
n s
δ x−xs
i
,andpt(x) =
n t
δ x−xt
i
i=1 i=1
respectively; hereδ isaDiracdeltadistributionatx(cid:48). Sincetheempiricaldatadistributionis
x−x(cid:48)
asumofafinitenumberofDiracmeasures,thisisadiscreteoptimaltransportproblemandeasy
tosolve. WecanusetheKantorovichrelaxationtodenotebyB thesetofprobabilisticcouplings
betweenthetwodistributions:
(cid:110) (cid:111)
B = Γ ∈ Rns×nt : Γ1 = p,Γ(cid:62)1 = q
+ ns ns
21
where1 isann-dimensionalvectorofones. TheKantorovichformulationsolvesfor
n
(cid:88)
ns
(cid:88)
nt
Γ∗ = argmin Γ κ (36)
ij ij
Γ∈B
i=1 t=1
whereκ ∈ Rns×nt isacostfunctionthatmodelstransportingthedatumxs toxt. Thisisthemetric
+ i j
oftheunderlyingdatadomainandonemaychooseanyreasonablemetricforκ = (cid:107)xs−xt(cid:107)2. The
i j 2
problemin(36)isaconvexoptimizationproblemandcanbesolvedeasily;inpracticeweusethe
(cid:80)
Sinkhorn’salgorithm[Cuturi,2013]whichaddsanentropicregularizer−h(Γ) = Γ logΓ to
ij ij ij
theobjectivein(36).
F.1 Changingthedatadistribution
GiventheoptimalprobabilisticcouplingΓ∗ betweenthesourceandthetargetdatadistributions,we
caninterpolatebetweenthematanyt ∈ [0,1]byfollowingthegeodesicsoftheWassersteinmetric
p(x,t) = argmin(1−t)W2(ps,p)+tW2(p,pt).
2 2
p
Fordiscreteoptimaltransportproblems,asshowninVillani[2008],theinterpolateddistributionp
t
forthemetricκ = (cid:107)x2−xt(cid:107)2 isgivenby
ij i j 2
(cid:88)
ns
(cid:88)
nt
p(x,t) = Γ∗ δ . (37)
ij x−(1−t)xs−txt
i j
i=1 j=1
Observethattheinterpolateddatadistributionequalsthesourceandtargetdistributionatt = 0and
t = 1respectivelyanditconsistsoflinearinterpolationsofthedatainbetween.
Remark 11 (Interpolating the labels). The interpolation in (37) gives the marginal on the input
spaceinterpolatedbetweenthesourceandtargettasks. ToevaluatethefunctionalsinSection3for
theclassificationsetting,wewouldalsoliketointerpolatethelabels. Wedosobysettingthetrue
labeloftheinterpolateddatumx = (1−t)xs +txt tobelinearinterpolationbetweenthesource
i j
labelandthetargetlabel.
y(x,t) = (1−t)δ +tδ
y−y xs
i
y−y xt
j
for all i,j. Notice that the interpolated distribution p(x,t) is a sum of Dirac delta distributions
weightedbytheoptimalcoupling. Wethereforeonlyneedtoevaluatethelabelsatalltheinterpolated
data.
Remark12(Linearinterpolationofdata). Ourformulationofoptimaltransportationleadstoa
linearinterpolationofthedatain(23). Thismaynotworkwellforimage-baseddatawherethesquare
metricκ = (cid:107)xs−x−kt(cid:107)2 maynotbetheappropriatemetric. Wenotethatthisinterpolationof
ij i 2
dataisanartifactofourchoiceofκ ,otherchoicesforthemetricalsofitintotheformulationand
ij
shouldbeviablealternativesiftheyresultinefficientcomputation.
22
G Transfer learning between two subsets of CIFAR-10
The iso-classification process is a quasi-static process, i.e., the model parameters θ are lie on the
equilibriumsurfaceatalltimest ∈ [0,1]duringthetransfer. Notethatboththeequilibriumsurface
andthefree-energyF(λ,γ)arefunctionsofthedataandchangewithtime. Letuswritethisexplicitly
as
F(t) := R(t,λ(t),γ(t))+λD(t,λ(t),γ(t))+γC
0
where C is the classification loss. We prescribed a geodesic transfer above where the Lagrange
0
multipliersλ,γ wereadaptedsimultaneouslytoconfirmtotheconstraintsoftheequilibriumsurface
locally. Wecanforgotthisandinsteadadaptthemusingthefollowingheuristic. Weletλ˙ = k for
someconstantk anduse
∂C ∂C ∂C
λ˙ + γ˙ + = 0, (38)
∂λ ∂γ ∂t
togettheevolutioncurveofγ(t).
Herewepresentexperimentalresultsofaniso-classificationprocessfortransferringthelearnt
representation. Wepickthesourcedatasettobeallvehicles(airplane,automobile,shipandtruck)in
CIFAR-10andthetargetdatasetconsistsoffouranimals(bird,cat,deeranddog). Wesettheoutput
sizeofclassifiertobefour. Ourgoalistoadaptamodeltrainedonthesourcetasktothetargettask
whilekeepingitsclassificationlossconstant. Weruntheiso-ctransferdynamics(38)andtheresults
areshowninFig.6.
2.0
1.5
1.0
0.5
0.0
0 5 10
number of steps
ssoL
noitadilaV
Non-Equilibrium Process 100
Iso-Classification Process
Training on Target Domain
80
60
40
0 5 10
number of steps
(a)
ycaruccA
noitadilaV
Non-Equilibrium Process
Iso-Classification Process
Training on Target Domain
(b)
Figure6:TransferringfromsourcedatasetofCIFAR-10vehiclestothetargetdatasetconsistingoffour
animals. Fig. 6a shows the variation of validation loss during the transfer. Fig. 6b shows the validation
accuracyduringthetransfer. Theorangecurvecorrespondstoiso-classificationtransfer;thebluecurveisthe
resultofdirectlyfine-tuningthesourcemodelonthetargetdata(notetheverylowaccuracyatthestart);the
greenpointistheaccuracyoftrainingonthetargettaskfromscratch.
Itisevidentthatboththeclassificationaccuracyandlossareconstantthroughoutthetransfer.
CIFAR-10isamorecomplexdatasetascomparingwithMNISTandtheaccuracygapbetweeniso-
classificationtransfer,fine-tuningfromthesourceandtrainingfromscratchissignificant. Observe
thattheclassificationlossgapbetweeniso-classificationtransferandtrainingfromscratchonthe
target is also significant. The benefit of running the iso-classification transfer is that we can be
guaranteedaboutthefinalaccuracyandvalidationlossofthemodel.
23
G.1 DetailsoftheexperimentalsetupforCIFARtransferring
Atmomentt,parametersλ,γ determineourobjectivefunctions. Wecomputeiso-classificationloss
transferprocessbyfirstsettinginitialstates: (λ = 4,γ = 100). Wetrainonsourcedatasetfor300
epochswithAdamandalearningrateof1E-3thatdropsbyafactorof10afterevery120epochs
to obtain the initial state. We change λ, γ with respect to time t and then apply the equilibration
learningratescheduleofFig.4atoachievethetransitionbetweenequilibriumstates. Wecompute
thepartialderivatives ∂C, ∂C and ∂C byusingfinitedifference. Ateachtimet,solving(38)with
∂t ∂λ ∂γ
thepartialderivativesleadstothesolutionforγ˙,whereλ˙
isaconstant. Inourexperimentweset
λ˙ = −1.5.
24