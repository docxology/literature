K. Kishida (Ed.): Fourth International Conference
on Applied Category Theory (ACT 2021).
EPTCS 372, 2022, pp. 133–148, doi:10.4204/EPTCS.372.10
© T . St. Clere Smithe
This work is licensed under the Creative Commons
Attribution-Share Alike License.
Polynomial Life: the Structure of Adaptive Systems
T oby St. Clere Smithe
T opos Institute
toby@topos.institute
W e extend our earlier work on the compositional structure of cybernetic systems in order to account
for the embodiment of such systems. All their interactions p roceed through their bodies’ boundaries:
sensations impinge on their surfaces, and actions correspo nd to changes in their conﬁgurations. W e
formalize this morphological perspective using polynomia l functors. The ‘internal universes’ of
systems are shown to constitute an indexed category of stati stical games over polynomials; their
dynamics form an indexed category of behaviours. W e charact erize active inference doctrines as
indexed functors between such categories, resolving a numb er of open problems in our earlier work,
and pointing to a formalization of the free energy principle as adjoint to such doctrines. W e illustrate
our framework through fundamental examples from biology, i ncluding homeostasis, morphogenesis,
and autopoiesis, and suggest a formal connection between sp atial navigation and the process of proof.
1 Introduction
In a submission to ACT 2020 [15], we presented some ﬁrst steps towards a theory of categorical cyber-
netics, motivated by concerns about what gives physical sys tems life. W e explained that perception and
action could both be described as processes of Bayesian infe rence: on the one hand, adjusting beliefs
about the world on the basis of observational evidence; on th e other, adjusting the world itself in order
better to match beliefs. In each case, the system must instan tiate a number of structures: a choice of
‘prior’ belief about the state of the world; a mechanism to ge nerate predictions about sense-data on the
basis of that belief, called a ‘stochastic channel’; and a (t ypically approximate) Bayesian inversion of
that channel, by which to update those beliefs, in light of se nsory observations.
The pairing of a prior with a stochastic channel corresponds to what is called in the informal litera-
ture a generative model , and it is common to suppose that these models are hierarchic al: that is, that the
stochastic channel factors as some composite; one imagines that each factor corresponds to predictions
at some level of detail, cascading for example down from high -level abstractions to individual photore-
ceptors. In our earlier submission, we formalized this comp ositional structure using the bidirectional
‘lens’ pattern 1 — since predictions and inversions are oppositely directed — and characterized a number
of approximate inference processes using a novel category o f statistical games , whose best responses
correspond to optimal inferences. A cybernetic system was t hen deﬁned as a ‘dynamical realisation’ of
such a game.
This formalism left some things to be desired: our notion of d ynamical realisation was ill-deﬁned,
and the notion of ‘action’ was overly abstract. In this submi ssion, we resolve these issues, substantially
simplifying our presentation along the way . W e explain that various recipes for performing approximate
inference, corresponding to our earlier informal notion of dynamical realisation, form functorial approx-
imate inference doctrines , between appropriate categories of statistical games and d ynamical systems.
Then, to formalize a satisfactory notion of action, we note t hat any active system has a boundary deﬁning
1 See [14] for a pedagogical presentation of the fundamental s tructures.
134 Polynomial Life
its morphology , and that it acts by changing the shape of this boundary; in order to act on another system,
it couples part of this boundary to that other system, so as to change the composite shape.
T o formalize the shapes of systems and their interactions, w e adopt polynomial functors: each poly-
nomial will encode the ‘phenotype’ (possible shapes or conﬁ gurations) of a system, and the sensorium
possible in each conﬁguration. T o give such systems life, we construct categories of statistical games
and dynamical behaviours indexed by polynomials. An active inference doctrine is then an indexed
functor between such categories. This framework enables a n umber of possibilities: we can construct
a generative model for a corporation on the basis of models fo r its employees; we give compositional
descriptions of fundamental processes of life such as homeo stasis and morphogenesis, and point towards
an account of autopoiesis; and we sketch the process by which living systems internalize the structures of
their environments, and navigate accordingly , noting that such navigation in abstract spaces corresponds
precisely to the process of proof.
The work presented here is work in progress, and owing to cons traints of space and time, it has not
been possible to elaborate everything that we would have lik ed; this means that we defer proofs of the
main results to subsequent elaborations of this extended ab stract. T wo such elaborations are our series of
papers on compositional active inference, beginning with [ 16], and our work on open dynamical systems
with polynomial interfaces [18]. Notwithstanding these li mitations, we believe the results here go some
of the way to answering the open questions, of elegance and in teraction, sketched at the end of our earlier
submission. W e see this work as making baby steps towards a th eory of embodied cybernetics.
Acknowledgements W e thank the members of the T opos Institute for stimulating d iscussions, and the
Foundational Questions Institute and T opos Institute for ﬁ nancial support.
2 Simpler Statistical Games
W e begin by sketching a reﬁnement of the statistical games fo rmalism developed in [15]. For much more
detail, we refer the reader to [16]. First, we recall the bidi rectional structure of Bayesian inversion.
2.1 Bayesian Lenses
Deﬁnition 2.1 ([11, Def. 3.3]) . The category GrLensF of Grothendieck lenses for a pseudofunctor F :
C op →Cat is the total category of the Grothendieck construction for t he pointwise opposite of F .
Proposition 2.2 (GrLensF is a category) . The objects (GrLensF )0 of GrLensF are (dependent) pairs
(C,X ) with C : C and X : F (C), and its hom-sets GrLensF
(
(C,X ),(C′,X ′)
)
are dependent sums
GrLensF
(
(C,X ),(C′,X ′)
)
= ∑
f : C (C,C′)
F (C)
(
F ( f )(X ′),X
)
so that a morphism (C,X ) ↦→(C′,X ′) is a pair ( f , f † ) of f : C (C,C′) and f † : F (C)
(
F ( f )(X ′),X
)
. W e
call such pairs Grothendieck lenses for F or F -lenses.
Proof sketch. The identity Grothendieck lens on (C,X ) is id(C,X ) = (idC ,idX ). Sequential composition
is as follows. Given ( f , f † ) : (C,X ) ↦→(C′,X ′) and (g,g† ) : (C′,X ′) ↦→(D,Y ), their composite (g,g† ) ◦
|
( f , f † ) is deﬁned to be the lens
(
g ◦f , f † ◦F ( f )(g† )
)
: (C,X ) ↦→(D,Y ). Associativity and unitality of
composition follow from the functoriality of F .
T . St. Clere Smithe 135
Deﬁnition 2.3. Suppose F (C)0 = C0, with F : C op →Cat a pseudofunctor. Deﬁne SimpGrLensF to
be the full subcategory of GrLensF whose objects are duplicate pairs (C,C) of objects C in C . W e call
SimpGrLensF the category of simple F -lenses. More generally , any lens between such duplicate pa irs
will be called a simple lens . Since duplicating the objects in the pairs (C,C) is redundant, we will write
the objects simply as C.
Deﬁnition 2.4. Let (C ,⊗,I) be a monoidal category enriched in a Cartesian closed catego ry V. Deﬁne
the C -state-indexed category Stat : C op →V-Cat as follows.
Stat : C op →V-Cat
X ↦→Stat(X ) :=




Stat(X )0 := C0
Stat(X )(A,B) := V(C (I,X ),C (A,B))
idA : Stat(X )(A,A) :=
{
idA : C (I,X ) →C (A,A)
ρ ↦→ idA



 (1)
f : C (Y,X ) ↦→






Stat( f ) : Stat(X ) → Stat(Y )
Stat(X )0 = Stat(Y )0
V(C (I,X ),C (A,B)) → V(C (I,Y ),C (A,B))
α ↦→ f ∗α :
(
σ : C (I,Y )
)
↦→
(
α ( f •σ ) : C (A,B)
)






Composition in each ﬁbre Stat(X ) is as in C . Explicitly , indicating morphisms C (I,X ) →C (A,B)
in Stat(X ) by A X− →• B, and given
α : A X− →• B and β : B X− →• C, their composite is β ◦α : A X− →• C := ρ ↦→
β (ρ ) •α (ρ ), where here we indicate composition in C by •and composition in the ﬁbres Stat(X ) by ◦.
Given f : Y →• X in C , the induced functor Stat( f ) : Stat(X ) →Stat(Y ) acts by pullback.
Example 2.5. T ypically , a choice of C above will be the Kleisli category K ℓ(P) of a probability monad,
such as the Giry monad G : Meas →Meas on measurable spaces; and V will either be Set or (better)
some ‘nice’ category of measurable spaces. However, the cat egory Meas of general measurable spaces
is not Cartesian closed, as there is no general way to make the evaluation maps Meas(X ,Y ) ×X →
Y measurable, meaning that if we take C = K ℓ(G ) above, then we are forced to take V = Set. In
turn, this makes the inversion maps c† : K ℓ(G )(1,X ) →K ℓ(G )(Y,X ) into mere functions. W e can
salvage measurability by working instead with K ℓ(Q), where Q : QBS →QBS is the analogue of the
Giry monad for quasi-Borel spaces [10]. The category QBS is indeed Cartesian closed, and K ℓ(Q)
is enriched in QBS, so that we can instantiate Stat there, and the corresponding inversion maps are
accordingly measurable.
W e deﬁne the category of Bayesian lenses in C to be the category of Stat-lenses.
Deﬁnition 2.6. The category BayesLensC of Bayesian lenses in C is the category GrLensStat of
Grothendieck lenses for the functor Stat. A Bayesian lens is a morphism in BayesLensC . Where the
category C is evident from the context, we will just write BayesLens.
Unpacking this deﬁnition, we ﬁnd that the objects of BayesLensC are pairs (X ,A) of objects of
C . Morphisms (that is, Bayesian lenses) (X ,A) ↦→(Y,B) are pairs (c,c† ) of a channel c : X →• Y and a
“generalized Bayesian inversion” c† : B X− →• A; that is, elements of the hom objects
BayesLensC
(
(X ,A),(Y,B)
)
: = GrLensStat
(
(X ,A),(Y,B)
)
∼
= C (X ,Y ) ×V
(
C (I,X ),C (B,A)
)
.
136 Polynomial Life
The identity Bayesian lens on (X ,A) is (idX ,idA), where by abuse of notation idA : C (I,Y ) →C (A,A) is
the constant map idA deﬁned in equation (1) that takes any state on Y to the identity on A.
The sequential composite (d,d† ) ◦|(c,c† ) of (c,c† ) : (X ,A) ↦→(Y,B) and (d,d† ) : (Y,B) ↦→(Z,C) is
the Bayesian lens
(
(d •c),(c† ◦c∗d† )
)
: (X ,A) ↦→(Z,C) where (c† ◦c∗d† ) : C X− →• A takes a state π : I →• X
to the channel c†
π •d†
c•
π : C →• A.
Deﬁnition 2.7. Given a Bayesian lens (c,c′) : (X ,A) ↦→(Y,B), we will call c its forwards or prediction
channel and c′its backwards or update channel (even though c′is really a family of channels).
Deﬁnition 2.8 (Bayesian inversion) . W e say that a channel c : X →• Y admits Bayesian inversion with
respect to π : I →• X if there exists a channel c†
π : Y →• X , called the Bayesian inversion of c with respect
to π , satisfying the following equation [6, eq. 5] in the graphic al calculus of C :
c
π
X Y
=
c†
π
π
c
X Y
(2)
W e say that c admits Bayesian inversion tout court if c admits Bayesian inversion with respect to all states
π : I →• X such that c •π has non-empty support. W e say that a category C admits Bayesian inversion if
all its morphisms admit Bayesian inversion tout court .
Deﬁnition 2.9. W e call the pairing (π ,c) of a state π : I →• X with a channel c : X →• Y a generative model
X →• Y . It induces a joint distribution ω (π ,c) := (idX ⊗c) • X •π : I →• X ⊗Y .
Deﬁnition 2.10. Let (c,c† ) : (X ,X ) ↦→(Y,Y ) be a Bayesian lens. W e say that (c,c† ) is exact if c admits
Bayesian inversion and, for each π : I →• X such that c •π has non-empty support, c and c†
π together
satisfy equation (2). Bayesian lenses that are not exact are said to be approximate.
Deﬁnition 2.11 (Almost-equality). Given a state π : I →• X , we say that two parallel channels c,d : X →• Y
are π -almost-equal, denoted c π∼d, if the joint distributions of the two generative models (π ,c) and (π ,d)
are equal; that is, if (id⊗c) • •π = (id⊗d) • •π .
Theorem 2.12 ([14]). Let (c,c† ) and (d,d† ) be sequentially composable exact Bayesian lenses. Then
the contravariant component of the composite lens (d,d† ) ◦|(c,c† ) = (d •c,c† ◦c∗d† ) is, up to d •c •π -
almost-equality , the Bayesian inversion of d •c with respect to any state π on the domain of c such that
c •π has non-empty support.
2.2 Statistical Games
The performance of a statistical or cybernetic system depen ds upon its interaction with its environment,
and the prior beliefs that it started with. W e will therefore deﬁne a statistical game to be a Bayesian lens
paired with a ﬁtness function measuring performance in cont ext; and for this we need a notion of context.
For this notion, we take inspiration from compositional gam e theory [2].
T . St. Clere Smithe 137
Deﬁnition 2.13. A context for a Bayesian lens is an element of the profunctor BayesLensC deﬁned by
BayesLensC × BayesLensC
op → V
− × = ↦→BayesLensC ((I,I),−) ×BayesLensC (=,(I,I))
where I is the monoidal unit in C , BayesLensC ((I,I),−) : BayesLensC →V is the representable
copresheaf on (I,I), and BayesLensC (=,(I,I)) : BayesLensC
op →V is the representable presheaf.
Hence a context for a lens (X ,A) ↦→(Y,B) is an element of
BayesLensC
(
(X ,A),(Y,B)
)
:= BayesLensC
(
(I,I),(X ,A)) ×BayesLensC ((Y,B),(I,I)
)
.
Proposition 2.14. When I is terminal in C and the base V of enrichment of C is well pointed (as when
V = Set), we have BayesLensC
(
(X ,A),(Y,B)
) ∼
= C (I,X ) ×V
(
C (I,B),C (I,Y )
)
.
Proof. A straightforward calculation which we omit: use that X c− →Y !− →I = X !− →I for any c : X →Y .
Proposition 2.15. Given a context (π ,k) : BayesLensC
(
(X ,A),(Z,C)
)
for a composite lens (X ,A)
f
↦→
(Y,B)
g
↦→(Z,C), we obtain contexts for the factors:
BayesLensC
(
(X ,A),g
)
(π ,k) = (π ,k ◦|g) : BayesLensC
(
(X ,A),(Y,B)
)
,
BayesLensC
(
f ,(Z,C)
)
(π ,k) = (f ◦|π ,k) : BayesLensC
(
(Y,B),(Z,C)
)
.
W e are now in a position to deﬁne the category of statistical g ames over C .
Proposition 2.16. Let C be a V-category admitting Bayesian inversion and let (R,+,0) be a monoid
in V. Then there is a category SGame[R]C whose objects are the objects of BayesLensC and whose
morphisms (X ,A) →(Y,B) are statistical games : pairs ( f ,φ ) of a lens f : BayesLensC
(
(X ,A),(Y,B)
)
and a ﬁtness function φ : BayesLensC
(
(X ,A),(Y,B)
)
→R. When R is the monoid of reals R, then we
just denote the category by SGameC .
Proof. Suppose given statistical games ( f ,φ ) : (X ,A) →(Y,B) and (g,ψ ) : (Y,B) →(Z,C). W e seek a
composite game (g,ψ ) ◦( f ,φ ) := (g f ,ψ φ ) : (X ,A) →(Z,C). W e have g f = g ◦|f by lens composition.
Propositon 2.15 gives us a family of functions localCtx with signature
BayesLensC
(
(X ,A),(Z,C)
)
×BayesLensC
(
(X ,A),(Y,B)
)
×BayesLensC
(
(Y,B),(Z,C)
)
→BayesLensC
(
(X ,A),(Y,B)
)
×BayesLensC
(
(Y,B),(Z,C)
)
.
W e therefore identify the composite ﬁtness function ψ φ as
ψ φ := +◦(φ ,ψ ) ◦localCtx(−, f ,g)
where + : R ×R →R is the monoid operation. The identity game (X ,A) →(X ,A) is given by (id,0),
the pairing of the identity lens on (X ,A) with the unit 0 of the monoid R. Associativity and unitality are
immediate from lens composition and the monoid laws.
Deﬁnition 2.17. W e will write SimpSGameC֒→SGameC for the full subcategory of SGameC deﬁned
on simple Bayesian lenses (X ,X ) ↦→(Y,Y ). As in the case of simple lenses (Deﬁnition 2.3), we will
eschew redundancy by writing the objects (X ,X ) simply as X .
W e now present some key examples of statistical games.
138 Polynomial Life
Example 2.18 (Bayesian inference) . Let D : C (I,X )×C (I,X ) →R be a measure of divergence between
states on X . Then a (simple) D-Bayesian inference game is a statistical game (X ,X ) →(Y,Y ) with ﬁtness
function φ : BayesLensC
(
(X ,X ),(Y,Y )
)
→R given by φ (π ,k) =Ey∼k•c•π
[
D
(
c′
π (y),c†
π (y)
)]
, where
(c,c′) constitutes the lens part of the game and c†
π is the exact inversion of c with respect to π .
Note that we say that D is a “measure of divergence between states on X ”. By this we mean any
function of the given type with the semantical interpretati on that it acts like a distance measure between
states. But this is not to say that D is a metric or even pseudometric. One usually requires that D(π ,π ′) =
0 ⇐⇒π = π ′, but typical choices do not also satisfy symmetry nor subadd itivity . An important such
typical choice is the relative entropy or Kullback-Leibler divergence, denoted DKL .
Deﬁnition 2.19. The Kullback-Leibler divergence D KL : C (I,X ) ×C (I,X ) →R is deﬁned by
DKL (α ,β ) := E
x∼α
[log p(x)] − E
x∼α
[log q(x)]
where p and q are density functions corresponding to the states α and β .
T ypically , computing DKL (c′
π (x),c†
π (x)) is computationally difﬁcult, so one resorts to optimizing a n
upper bound; a prominent choice is the free energy [8].
Deﬁnition 2.20 (D-free energy) . Let (π ,c) be a generative model with c : X →• Y . Let pc : Y ×X →R+
and pπ : X →R+ be density functions corresponding to c and π . Let pc•π : Y →R+ be a density
function for the composite c •π . Let c′
π be a channel Y →• X that we take to be an approximation of the
Bayesian inversion of c with respect to π and that admits a density function q : X ×Y →R+. Finally , let
D : C (I,X ) ×C (I,X ) →R be a measure of divergence between states on X . Then the D-free energy of
c′
π with respect to the generative model given an observation y : Y is the quantity
F D(c′
π ,c,π ,y) := E
x∼c′π (y)
[−log pc (y|x)] +D
(
c′
π (y),π
)
. (3)
W e will elide the dependence on the model when it is clear from the context, writing only F D(y).
The D-free energy is an upper bound on D when D is the relative entropy DKL .
Proposition 2.21 (Evidence upper bound) . The DKL -free energy satisﬁes the following equality:
F DKL (y) =DKL
[
c′
π (y),c†
π (y)
]
−log pc•π (y) = E
x∼c′π (y)
[
log q(x|y)
pc (y|x) ·pπ (x)
]
Since log pc•π (y) is always negative, the free energy is an upper bound on DKL
[
c′
π (y),c†
π (y)
]
, where c†
π
is the exact Bayesian inversion of the channel c with respect to the prior π . Similarly , the free energy is
an upper bound on the negative log-likelihood −log pc•π (y). Thinking of this latter quantity as a measure
of the “model evidence” gives us the alternative name evidence upper bound for the DKL -free energy .
Any system that performs (approximate) Bayesian inversion can thus be seen as minimizing some
free energy . The free energy principle says that all it means to be an adaptive system is to embody a
process of approximate inference in this way . W e therefore d eﬁne free energy games:
Example 2.22 (Free energy game) . Let D : C (I,X ) ×C (I,X ) →R be a measure of divergence between
states on X . Then a simple D-free energy game is a simple statistical game (X ,X ) →(Y,Y ) with ﬁt-
ness function φ : BayesLensC
(
(X ,X ),(Y,Y )
)
→R given by φ (π ,k) =Ey∼k•c•π [F D (c′
π ,c,π ,y)] where
(c,c′) : (X ,X ) ↦→(Y,Y ) constitutes the lens part of the game.
T . St. Clere Smithe 139
Remark 2.23. It is also often of interest to consider parameterized channels, for which we can use the
Para construction [13]. This acts by adjoining an object of param eters to the domain of the category
at hand (such as the category of Bayesian lenses), and tensor ing parameters of composite morphisms.
Formally , this corresponds to a generalization of the index ed category of state-dependent morphisms,
and forms the subject of another paper in these proceedings [ 4]. The parameters might represent the
‘weights’ of a neural network, or encode some structure abou t the possible predictions. Lacking the
space to do justice to this structure here, we nonetheless le ave it at that, and refer the reader to our paper
[16] for more details.
3 Systems Within Interfaces; W orlds Within W orlds
In this section, we develop the structures required for exte nding the formalism of statistical games to
embodied systems.
3.1 Polynomials for Embodiment and Interaction
Each system in our universe inhabits some interface or bound ary . It receives signals from its environ-
ments through this boundary , and can act by changing its shap e (and, as we will see later, its position).
As a system changes its shape, the set of possible immanent si gnals might change accordingly: consider
a hedgehog rolling itself into a ball, thereby protecting it s soft underbelly from harm (amongst other
immanent signals). A system may also change its shape by coup ling itself to some other system, such as
when we pick up chalk to work through a problem. And shapes can be abstract: we change our ‘shapes’
when we enter an online video conference, or move within a vir tual reality . W e describe all of these
interactions formally using polynomial functors, drawing on the work of [12].
Deﬁnition 3.1. Let E be a locally Cartesian closed category , and denote by yA the representable co-
presheaf yA := E (A,−) : E →E . A polynomial functor p is a coproduct of representable functors, writ-
ten p := ∑ i: p(1) ypi , where p(1) : E is the indexing object. The category of polynomial functors in E is the
full subcategory PolyE֒→[E ,E ] of the E -copresheaf category spanned by coproducts of representab les.
A morphism of polynomials is therefore a natural transforma tion.
Remark 3.2. Every polynomial functor P : E →E corresponds to a bundle p : E →B in E , for which
B = P(1) and for each i : P(1), the ﬁbre pi is P(i). W e will henceforth elide the distinction between a
copresheaf P and its corresponding bundle p, writing p(1) := B and p[i] := pi , where E = ∑ i p[i]. A
natural transformation f : p →q between copresheaves therefore corresponds to a map of bund les. In the
case of polynomials, by the Y oneda lemma, this map is given by a ‘forwards’ map f1 : p(1) →q(1) and
a family of ‘backwards’ maps f # : q[ f1 (-)] →p[-] indexed by p(1), as in the left diagram below . Given
f : p →q and g : q →r, their composite g ◦f : p →r is as in the right diagram below .
E f ∗F F
B B C
f #
qp
f1
⌟
E f ∗g∗G G
B B D
(g f )#
rp
g1 ◦f1
⌟
Here, (g f )# is given by the p(1)-indexed family of composite maps r[g1 ( f1 (-))]
f ∗g#
−−→q[ f1 (-)]
f #
−→p[-].
In our morphological semantics, we will call a polynomial p a phenotype, its base type p(1) its
morphology and the total space ∑ i p[i] its sensorium. W e will call elements of the morphology shapes or
conﬁgurations , and elements of the sensorium immanent signals .
140 Polynomial Life
Proposition 3.3 ([12]). There is a monoidal structure (PolyE ,⊗,y) that we interpret as “putting systems
in parallel”. Given p : ∑ i p[i] →p(1) and q : ∑ j q[ j] →q(1), we have p ⊗q = ∑ i ∑ j p[i] ×q[ j] →p(1) ×
q(1). y : 1 →1 is then clearly unital.
Proposition 3.4 ([12]). The monoidal structure (PolyE ,⊗,y) is closed, with corresponding internal hom
denoted [−,−].
W e interpret morphisms ( f1 , f # ) of polynomials as encoding interaction patterns; in partic ular, such
morphisms encode how composite systems act as unities. For e xample, a morphism f : p ⊗q →r speci-
ﬁes how the systems p and q come together to form a system r: the map f1 encodes how r-conﬁgurations
are constructed from conﬁgurations of p and q; and the map f # encodes how immanent signals on p and
q result from signals on r or from the interaction of p and q. For intuition, consider two people engaging
in a handshake, or an enzyme acting on a protein to form a compl ex. The internal hom [o, p] encodes all
the possible ways that an o-phenotype system can “plug into” a p-phenotype system.
Remark 3.5. In the literature on active inference and the free energy pri nciple, there is much debate
about the concept and role of ‘Markov blankets’, which are so metimes conceived to represent the bound-
ary of an adaptive system. W e believe that the algebra of poly nomials will help to make such thinking
precise, where it departs from the established usage of Mark ov blankets in Bayesian networks.
3.2 Dynamical Systems on Polynomial Interfaces
Although some dynamical systems can be modelled within PolyE itself, we prefer to take a ﬁbrational
perspective, following the idea that polynomials represen t the boundaries of systems, separating ‘inter-
nal’ states from ‘external’. W e will therefore adopt a patte rn of indexing categories by polynomials: in
the case of dynamics, the ﬁbre over a polynomial will be a cate gory of possible internal systems, whose
projection forgets the internal structure. W e can only sket ch the relevant structures here, and so refer
the reader to our subsequent paper [18] where we give a genera l account of open dynamical systems as
coalgebras for polynomial functors.
Deﬁnition 3.6. Let P : E →E be a probability monad on the category E , and let p : PolyE be a
polynomial in E . Let (T,+,0) be a monoid in E , representing time. Then an open Markov process on
the interface p with time T consists in a triple
ϑ := (S,ϑ o ,ϑ u ) of a state space S : E and two morphisms
ϑ o : T ×S →p(1) and ϑ u : ∑ t :T ∑ s:S p[ϑ o (t ,s)] →P S, such that for any section σ : p(1) →∑ i: p(1) p[i]
of p, the maps ϑ σ : T ×S →P S given by
∑
t :T
S
ϑ o (−)∗σ
−−−−−→∑
t :T
∑
s:S
p[ϑ o (−,s)] ϑ u
−→P S
constitute an object in the functor category Cat
(
BT,K ℓ(P)
)
, where BT is the delooping of T and
K ℓ(P ) is the Kleisli category of P. W e call ϑ σ the closure of ϑ by σ .
Proposition 3.7. Open Markov processes over p with time T form a category , denoted MrkProcT
P (p).
Its morphisms are deﬁned as follows. Let
ϑ := (X ,ϑ o ,ϑ u ) and ψ := (Y,ψ o ,ψ u) be two Markov pro-
cesses over p. A morphism f : ϑ →ψ consists in a morphism f : X →Y such that, for any time t : T
and global section σ : p(1) → ∑
i: p(1)
p[i] of p, we have a natural transformation ϑ σ →ψ σ between the
closures. The identity morphism idϑ on the open Markov process ϑ is given by the identity morphism
idX on its state space X . Composition of morphisms of open Markov processes is given by composition
of the morphisms of the state spaces.
T . St. Clere Smithe 141
Proposition 3.8. MrkProc T
P extends to an indexed category , MrkProcT
P : PolyE →Cat. Suppose
ϕ : p →q is a morphism of polynomials. W e deﬁne the functor MrkProcT
P (
ϕ ) : MrkProcT
P (p) →
MrkProcT
P (q) as follows. Suppose (X ,
ϑ o,ϑ u ) is an object (open Markov process) in MrkProcT
P (p).
Then MrkProcT
P (
ϕ )(X ,ϑ o ,ϑ u ) is deﬁned as the triple (X ,ϕ 1 ◦ϑ o ,ϑ u ◦ϑ o ∗ϕ #) : MrkProcT
P (q). On
morphisms, MrkProcT
P (
ϕ )( f ) : MrkProcT
P (
ϕ )(X ,ϑ o,ϑ u ) →MrkProcT
P (
ϕ )(Y,ψ o ,ψ u ) is given by
the same underlying map f : X →Y of state spaces.
Remark 3.9 (Closed Markov chains and Markov processes) . A closed Markov chain is given by a
map X →P X , where P : E →E is a probability monad on E ; this is equivalently an object in
MrkProcT
P (y), and (again equivalently) an object in Cat
(
BN,K ℓ(P)
)
. With more general time T,
one obtains closed Markov processes : objects in Cat
(
BT,K ℓ(P)
)
. More explicitly , a closed Markov
process is a time-indexed family of Markov kernels; that is, a morphism
ϑ : T ×X →P X such that, for
all times s,t : T, ϑ s+t = ϑ s •ϑ t as a morphism in K ℓ(P ). Note that composition •in K ℓ(P) is given
by the Chapman-Kolmogorov equation, so this means that
ϑ s+t (y|x) =
∫
x′:X
ϑ s (y|x′)ϑ t (dx′|x).
W e will need to convert our ﬁbration of Markov processes into an ordinary category , in order to
supply ‘dynamical semantics’ for statistical games. W e can do so, with objects being appropriately
typed pairs and morphisms representing appropriately ‘bid irectional’ dynamical systems, by using the
following structure.
Proposition 3.10. Let D : PolyE →Cat be an indexed category over polynomials. Then there is a
category of hierarchical bidirectional D-systems, denoted HiBi(D), and deﬁned as follows. The objects
of HiBi(D) are pairs (X ,A) of objects in E . Morphisms (X ,A) →(Y,B) are functors D(X yA) →D(Y y B ).
The composition rule is just composition of the correspondi ng functors, and identity morphisms id(X ,A) :
(X ,A) →(X ,A) are given by the identity functor idD(X yA ) : D(X yA) →D(X yA).
Proof. Immediate from the associativity and unitality of composit ion of functors.
In particular, we can take D = MrkProcT
P to obtain “hierarchical bidirectional Markov systems”.
3.3 Nested Systems and Dependent Polynomials
The foregoing formalism sufﬁces to describe systems’ shape s, and behaviours of those shapes that de-
pend on their sensoria. But in our world, a system has a position as well as a shape! Indeed, one might
want to consider systems nested within systems, such that th e outer systems constitute the ‘universes’
of the inner systems; in this way , inner shapes may depend on o uter shapes, and inner sensoria on outer
sensoria.2 W e can model this situation polynomially .
Recall that an object in PolyE corresponds to a bundle E →B, equivalently a diagram 1 ←E
p
− →
B →1, and note that the unit polynomial y corresponds to a bundle 1 →1. W e can then think of PolyE
as the category of “polynomials in one variable”, or “polyno mials over y”. This presents a natural
generalization, to polynomials in many variables, corresp onding to diagrams J ←E →B →I; these
diagrams form the objects of a category PolyE (J,I). When J is a (polynomial) bundle
β over I, then we
can take the subcategory of PolyE (J,I) whose objects are commuting squares and whose morphisms are
prisms as follows; the commutativity ensures that inner and outer sensoria are compatible.
2 W e might even consider the outer shapes explicitly as positi ons in some world-space, and the outer sensorium as determin ed
by possible paths between positions, in agreement with the p erspective of [12] on polynomials.
142 Polynomial Life
Proposition 3.11. There is an indexed category of nested polynomials which by abuse of notation we
will call PolyE (−) : PolyE →Cat. Given β : J →I, the category PolyE (β ) has commuting squares as on
the left below as objects and prisms as on the right as morphis ms. Its action on polynomial morphisms
β →γ is given by composition.
E J
B I
E J
B f ∗F
B F
I C
Remark 3.12. This construction can be repeatedly iterated, modelling sy stems within systems within
systems. W e leave the consideration of the structure of this iteration to future work, though we expect
it to have an opetopic shape equivalent to that obtained by it erating the Para construction (cf. Remark
2.23).
Observation 3.13. Our polynomially indexed categories of dynamical behaviou rs and statistical games
(Prop. 4.9) generalize to the case of nested polynomials, gi ving a doubly-indexed structure. For more
information on the former case, we refer the reader to our pre print [17].
4 Theories of Approximate and Active Inference
W e now start to bring together the structures of the previous sections, in order to breathe life into poly-
nomials. W e begin by sketching approximate inference doctrines , which characterize dynamical systems
that optimize their performance at statistical games, with out reference to morphology . In this paper, we
do not concentrate on the detailed structure of these doctri nes, leaving their exposition and comparison
to future work, in which we will also be interested in morphis ms between doctrines.
4.1 Approximate Inference Doctrines
An approximate inference doctrine will be a monoidal functo r from a category of statistical games into
an appropriate category of dynamical systems, taking games to systems that ‘play’ those games, typically
by implementing an optimization process. In the free-energ y literature ( e.g., [3]), these systems have a
hierarchical structure in which the realization of a game ha s access to the dynamics realizing the prior,
mirroring the context-dependence of the games themselves, and it is for this reason that we use the
category of hierarchical bidirectional systems deﬁned abo ve.
Proposition 4.1. Let P : E →E be a probability monad on E , and let C be a category that admits
Bayesian inversion, equipped with a functor F : C →K ℓ(P). Since SGameC is a Grothendieck ﬁbra-
tion, it is equipped with a canonical projection functor SGameC →C . Suppose that G is a subcategory of
SGameC . Then there is a functor FG : G֒→SGameC →C F− →K ℓ(P) whose image imFG in K ℓ(P)
consists of the forwards maps of G realized as stochastic channels in K ℓ(P).
T . St. Clere Smithe 143
Deﬁnition 4.2. Suppose p : PolyE and that C is a subcategory of K ℓ(P). Denote by MrkProcT
P (p)|C
the subcategory of MrkProcT
P (p) whose objects (Θ ,
θ o ,θ u ) satisfy the condition that, for all t : T, the
following composite belongs to C :
∑
t :T
∑
x:Θ
p[θ o (t ,x)]
θ u(t )
−−− →P Θ
P θ o(t )
−−−−→P p(1).
Note that MrkProcT
P (−)|C does not in general deﬁne an indexed category .
Deﬁnition 4.3. Suppose C is a subcategory of K ℓ(P). Denote by HiBi
(
MrkProcT
P
)
|C the re-
striction of HiBi
(
MrkProcT
P
)
deﬁned as follows. The objects of HiBi
(
MrkProcT
P
)
|C are the ob-
jects of HiBi
(
MrkProcT
P
)
, but morphisms (X ,A) →(Y,B) are now functors MrkProcT
P |C (X yA) →
MrkProcT
P |C (Y y B ). Composition and identities are deﬁned as for HiBi
(
MrkProcT
P
)
.
W e are now in a position to deﬁne approximate inference doctr ines.
Deﬁnition 4.4. Let P : E →E be a probability monad on E and let G be a subcategory of SGameK ℓ(P).
An approximate inference doctrine in G with time T is a functor from G to HiBi
(
MrkProcT
P
)
|imFG ,
where FG is deﬁned as in Proposition 4.1.
Many informal approximate inference schemes—including Ma rkov chain Monte Carlo, variational
Bayes, expectation-maximization, particle ﬁltering—giv e rise to approximate inference doctrines; func-
toriality typically follows from Theorem 2 .12. Here we note one explicitly , for later reference; a detai led
presentation will appear in an upcoming paper.
Deﬁnition 4.5. W e say that f : X →• Y in K ℓ(P) is Gaussian if, for any x : X , the state f (x) : P Y is
Gaussian. Note that Gaussian morphisms are not themselves c losed under composition, though we can
consider the subcategory of morphisms generated by Gaussia n morphisms.
Lemma 4.6 (Laplace approximation) . Suppose:
(a) Gauss is the subcategory of channels generated by Gaussian morphi sms between ﬁnite-dimensional
Euclidean spaces in K ℓ(P );
(b) (
γ ,ρ ,φ ) : X →Y is a simple DKL -free energy game with Gaussian channels;
(c) for all priors π : P X , the statistical parameters of ρ π : Y →P X are denoted by (µ ρ π ,Σ ρ π ) : Y →
R|X |×R|X |×|X |, where |X |is the dimension of X ; and
(d) for all y : Y , the eigenvalues of Σ ρ π (y) are small.
Then the loss function φ : BayesLensGauss ((X ,X ),(Y,Y )) →R can be approximated by
φ (π ,k) = E
y∼k•γ •π
[
F (y)
]
≈ E
y∼k•γ •π
[
F L(y)
]
where
F L(y) =E(π ,γ )
(
µ ρ π (y),y
)
−SX [ρ π (y)] (4)
= −log pγ (y|µ ρ π (y)) −log pπ (µ ρ π (y)) −SX [ρ π (y)]
and where Sx [ρ π (y)] =Ex∼ρ π (y)[−log pρ π (x|y)] is the Shannon entropy of ρ π (y), and pγ : Y ×X →[0,1],
pπ : X →[0,1], and pρ π : X ×Y →[0,1] are density functions for γ , π , and ρ π respectively . The approx-
imation is valid when Σ ρ π satisﬁes
Σ ρ π (y) =
(
∂ 2
x E(π ,γ )
)(
µ ρ π (y),y
) −1 . (5)
144 Polynomial Life
Theorem 4.7. Suppose:
(a) Gauss is the subcategory of channels generated by Gaussian morphi sms between ﬁnite-dimensional
Euclidean spaces in K ℓ(P );
(b) SimpSGameGauss֒→SGameK ℓ(P) is the subcategory of simple statistical games over K ℓ(P)
with channels in Gauss; and
(c) G is the subcategory of DKL -Bayesian inference games in SimpSGameGauss.
Then the discrete-time free-energy principle under the Lap lace approximation induces an approximate
inference doctrine in G with time N, Laplace : G →HiBi
(
MrkProcT
P
)
|imFG .
4.2 Statistical Games over Polynomials
Approximate inference doctrines of the foregoing type do no t supply a satisfactory model of active
systems. One piece of structure is still missing, with which we can describe action and interaction
faithfully: an indexed category of statistical games over p olynomials. In order to construct this, we ﬁrst
deﬁne categories of “games on interfaces”: this is simpler t han slicing the category of statistical games,
as we do not require games between games.
Deﬁnition 4.8. Let P : E →E be a probability monad on E . Let X : E be an object in E . Deﬁne a
category of simple statistical games on the interface X , denoted by IntGameP (X ), as follows. Its
objects are simple statistical games with codomain X ; that is, points of ∑ A:E SimpSGameK ℓ(P)(A,X ).
Let (
γ ,ρ ,φ ) : A →X and (δ ,σ , χ ) : B →X be two such simple statistical games. Then a morphism
(γ ,ρ ,φ ) →(δ ,σ , χ ) is a deterministic function f : A →B—that is, a point of E (A,B)—such that γ =
δ ◦f . Unitality and associativity follow immediately from thos e properties in E /P X .
W e then use this to construct games over polynomials. The int uition here is that ‘inside’ a system
with a polynomial phenotype is a statistical model of the sys tem’s sensorium. This involves an object
representing the space of possible causes of observations, and a simple statistical game from this object
onto the sensorium; by its nature, this model induces predic tions about the system’s conﬁgurations, as
well as about the immanent signals. By sampling from these pr edicted conﬁgurations, the system can act;
by observing its actual conﬁguration and the corresponding immanent signals, it can update its internal
beliefs, and any parameters of the model. Later, we will equi p this process with (random) dynamics,
thereby giving the systems life.
Proposition 4.9. Let P : E →E be a probability monad on a locally Cartesian closed categor y E . There
is a polynomially indexed category of statistical games PSGame P : PolyE →Cat, deﬁned on objects
p as IntGameP
(
∑ i: p(1) p[i]
)
.
Example 4.10. T o understand the action of PSGameP (
ϕ ) on statistical games, it may help to consider
the example of a corporation. Such a system is composed of a nu mber of active systems, which instantiate
statistical games and interact according to some pattern fo rmalized by the polynomial map ϕ . Given such
a collection of games, PSGameP (ϕ ) tells us how to construct a game for the corporation as a whole :
in particular, we obtain a stochastic channel generating pr edictions for the (exposed) sensorium of the
corporation, and an inversion updating the constituent sys tems’ beliefs accordingly .
4.3 Active Inference
W e are now ready to deﬁne active inference doctrines; given a ll the foregoing structure, this proves
relatively simple.
T . St. Clere Smithe 145
Deﬁnition 4.11. Let P : E →E be a probablity monad, and let T be a time monoid. An active inference
doctrine is a monoidal indexed functor from PSGameP to MrkProcT
P .
Proposition 4.12. The Laplace doctrine lifts from approximate to active infer ence. The functors on each
ﬁbre are as before on games (here, objects), and on morphisms between games they are given merely
by lifting the corresponding maps to maps between state spac es. One then checks that morphisms of
polynomials correspond to natural transformations betwee n these functors.
Remark 4.13. It would be desirable to incorporate the compositional stru cture of the games themselves,
rather than treat them opaquely as objects. This suggests a d ouble-categorical structure the investigation
of which we leave to future work. Similarly , we do not here ela borate the extension of these indexed
categories to the dependent-polynomial case.
5 Polynomial Life and Embodied Cognition
Finally , we sketch how a number of classic biological proces ses can be modelled as processes of active
inference over polynomials. The key insight is that, by ﬁxin g the prior of an ‘active’ free-energy game to
encode high-precision (low-variance) beliefs about the ex ternal state, we can induce the system to prefer
acting ( i.e., reifying those beliefs) over perceiving ( i.e., updating the beliefs to match perceptions). In
doing so, one can induce volition or goal-directedness in the system. A key feature of these examples is
that they demonstrate ‘embodied’ cognition, in which a syst em’s form and interactions become part of
its cognitive apparatus.
Remark 5.1. Of course, one must be careful not to choose a prior with exces sively high precision (such
as a Dirac delta distribution), as this would cause the syste m to forego any belief-updating, thereby
rendering its actions independent of the ‘actual’ external state.
Example 5.2. Suppose that the system’s sensorium includes a key paramete r such as ambient tempera-
ture or blood pH. Suppose that by adjusting its conﬁguration , the system can move around in order to
sample this parameter. And suppose that the prior encodes a h igh-precision distribution centred on the
acceptable range of this parameter. Then it is straightforw ard to show that the system, by minimizing the
free energy , will attempt to conﬁgure itself so as to remain w ithin the acceptable parameter range. W e
can consider this as a simple model of homeostasis.
Example 5.3. W e can extend the previous example to a system with multiple ( polynomial) components,
each equipped with a “homeostasis game”, in order to model morphogenesis. Suppose the environmen-
tal parameter in the sensorium is the local concentration of some signalling molecule, and suppose the
polynomial morphism forming the composite system encodes t he pattern of signalling molecule concen-
trations in the neighbourhood of each system, as a result of t heir mutual conﬁgurations. Suppose then
that the target state encoded in the prior of each system corr esponds to the system being positioned in a
particular way relative to the systems around it, as represe nted by the signal concentrations. Free-energy
minimization then induces the systems to arrange themselve s in order to obtain the target pattern.
Remark 5.4. The foregoing examples begin to point towards a composition al theory of autopoiesis:
here, one might expect the target state to encode the proposi tion “maintain my morphology”, which ap-
pears self-referential. The most elegant way of encoding th is proposition in the prior is not immediately
clear, although a number of possibilities present themselv es (such as avoiding some undesirable con-
ﬁguration representing dissolution). W e expect a satisfac tory answer to this to be related to “Bayesian
mechanics” (see §6).
146 Polynomial Life
Remark 5.5. It has been shown informally that, given a ﬁnite time horizon Markov decision problem,
active inference can recover the Bellman-optimal policy tr aditionally obtained by backward induction
[7]. Ongoing work by the present author is directed at formal izing the structure of this relationship. In
particular, the result rests on encoding directly into the p rior the expectation of the loss function given
a policy and a goal, which strikes us as a large amount of infor mation to push into an unstructured
distribution over numbers.
Example 5.6. The examples need not be restricted to simple biological cas es. For instance, we can model
spatial navigation quite generally: we can use parameteriz ed statistical games to encode uncertainty
about the structure of the ‘external space’ (for instance: w hich points or neighbourhoods are connected to
which, and by which paths). By setting a high-precision prio r at some location, the system will attempt to
reach that location, learning the spatial structure along t he way; reducing the precision of the prior causes
the system to prefer “mere exploration”. One can attach sens e-data to each location using the natural
polynomial bundle structure. Moreover, the ‘external spac e’ need not be a simple topological space:
it may be something more structured. For instance, categori es and sites can themselves be modelled
polynomially . One can think of “taking an action” as precise ly analogous to “following a morphism”:
thus, in a topos-theoretic setting, one can consider the str ucture of the ‘external space’ to be a type-
theoretic context, and positions in the world to be objects i n the corresponding topos. One could then
encode in the prior a target proposition, and free-energy mi nimization would cause the system then to
explore the ‘space’ (learning its structure), and seek a pat h to the target. But such a path is precisely a
proof! There is increasing evidence that the neural mechani sms underlying spatial and abstract navigation
are the same [1], and a line of thinking as sketched here may su pply a mathematical justiﬁcation.
6 Future Directions
Besides expanding the examples above in detail, there are ma ny future directions to pursue. Our last
example points towards a ‘well-typed’ theory of cognition, ﬁnding type-theoretic analogues of cognitive
processes (such as action, planning, or navigation). By for malizing the connection between polynomial
statistical games and Markov decision processes, we hope ﬁn ally to relate our ‘statistical’ account of
cybernetics with the account emerging from research in comp ositional game theory . In particular, we
believe that the hierarchical/nested structure of our poly nomial systems is structurally similar to that of
(parameterized) players in open games. Along similar lines , we expect a connection between statistical
games and ‘learners’ [13] implementing backprop.
In a more physical direction, there is a controversy in the in formal literature about whether one
should expect any system with a boundary (and hence any system on a polynomial) to admit a canon-
ical statistical-game description; the typical suggestio n is that such a description should obtain at non-
equilibrium steady state, through a manipulation of the cor responding Fokker-Planck equation; this is
the notion of “Bayesian mechanics” [9]. Our results suggest that such a canonical description should
form a left adjoint to some active inference doctrine; this i s a matter of ongoing research by the author.
W orking topos-theoretically points further in a metaphysi cal direction: a Bayesian perspective lends
itself to subjectivism, but considering the “internal univ erse” of a navigating system to be a topos in
some context also points to a subjective realism. It seems li kely then that composite systems need not in
general agree about their observations. W e should therefor e expect to ﬁnd evidence of contextuality and
disagreement in multi-agent systems, and to investigate th is using cohomological tools ( e.g., [5]).
T . St. Clere Smithe 147
References
[1] Timothy EJ Behrens, Timothy H Muller, James CR Whittingt on, Shirley Mark, Alon B Baram,
Kimberly L Stachenfeld & Zeb Kurth-Nelson (2018): What is a cognitive map? Organizing knowl-
edge for ﬂexible behavior . Neuron 100(2), pp. 490–509, doi: 10.1016/j.neuron.2018.10.002.
[2] Joe Bolt, Jules Hedges & Philipp Zahn (2019): Bayesian open games . arXiv preprint . A vailable at
http://arxiv.org/abs/1910.03656.
[3] Christopher L Buckley , Chang Sub Kim, Simon McGregor & An il K Seth (2017): The free energy
principle for action and perception: A mathematical review . Journal of Mathematical Psychology
81, pp. 55–79, doi: 10.1016/j.jmp.2017.09.004.
[4] Matteo Capucci, Bruno Gavranovi´ c, Jules Hedges & Eigil Fjeldgren Rischel (2021): T owards foun-
dations of categorical cybernetics . this volume of EPTCS, Open Publishing Association. A vaila ble
at https://arxiv.org/abs/2105.06332.
[5] Giovanni Car ` u (2017): On the Cohomology of Contextuality . In: Proceedings of the 13th Inter-
national Conference on Quantum Physics and Logic , 236, pp. 21–39, doi: 10.4204/EPTCS.236.
2.
[6] Kenta Cho & Bart Jacobs (2017): Disintegration and Bayesian Inversion via String Di-
agrams. Mathematical Structures in Computer Science 29, pp. 938–971, doi: 10.1017/
S0960129518000488.
[7] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston & Ryan Smith (2020): The Relationship
between Dynamic Programming and Active Inference: The Disc rete, Finite-horizon Case . arXiv
preprint. A vailable at https://arxiv.org/abs/2009.08111.
[8] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburn er & W . Penny (2007): V ariational free energy
and the Laplace approximation . Neuroimage 34(1), pp. 220–234, doi: 10.1016/j.neuroimage.
2006.08.035.
[9] Karl Friston (2019): A free energy principle for a particular physics . arXiv preprint . A vailable at
http://arxiv.org/abs/1906.10184.
[10] Chris Heunen, Ohad Kammar, Sam Staton & Hongseok Y ang (2 017): A Convenient Category for
Higher-Order Probability Theory . In: 32nd Annual ACM/IEEE Symposium on Logic in Computer
Science (LICS) , IEEE, doi: 10.1109/lics.2017.8005137.
[11] David I. Spivak (2019): Generalized Lens Categories via functors C op →Cat. arXiv preprint .
A vailable at http://arxiv.org/abs/1908.02202.
[12] David I. Spivak (2020): P oly: An abundant categorical setting for mode-dependent d ynamics.
arXiv preprint (presented at the 3rd Annual International A pplied Category Theory Conference) .
A vailable at https://arxiv.org/abs/2005.01894.
[13] David I. Spivak (2021): Learners’ languages . this volume of EPTCS, Open Publishing Association.
A vailable at https://arxiv.org/abs/2103.01189.
[14] T oby St. Clere Smithe (2020): Bayesian Updates Compose Optically . arXiv preprint . A vailable at
https://arxiv.org/abs/2006.01631.
[15] T oby St. Clere Smithe (2020): Cyber Kittens, or Some First Steps T owards Categorical Cybe rnetics.
In: Proceedings 3rd Annual International Applied Category The ory Conference 2020 (ACT 2020) ,
333, pp. 108–124, doi: 10.4204/EPTCS.333.8.
148 Polynomial Life
[16] T oby St. Clere Smithe (2021): Compositional Active Inference I: Bayesian Lenses. Statis tical
Games. arXiv preprint . A vailable at https://arxiv.org/abs/2109.04461.
[17] T oby St. Clere Smithe (2021): Some Notions of (Open) Dynamical System on P olynomial Inter -
faces. arXiv preprint . A vailable at https://arxiv.org/abs/2108.11137.
[18] T oby St. Clere Smithe (2022): Open dynamical systems as coalgebras for polynomial functo rs, with
application to predictive processing . In: Proceedings 5th Annual International Applied Category
Theory Conference 2021 (forthcoming) . A vailable at https://arxiv.org/abs/2206.03868.