Online reinforcement learning with sparse rewards
through an active inference capsule
Alejandro Daniel Noel
Department of Cognitive Robotics
Delft University of Technology
adanielnoel@gmail.com
Charel van Hoof
Department of Cognitive Robotics
Delft University of Technology
charel.van.hoof@gmail.com
Beren Millidge
MRC Brain Network Dynamics Unit
University of Oxford
beren@millidge.name
Abstract
Intelligent agents must pursue their goals in complex environments with partial
information and often limited computational capacity. Reinforcement learning
methods have achieved great success by creating agents that optimize engineered
reward functions, but which often struggle to learn in sparse-reward environments,
generally require many environmental interactions to perform well, and are typi-
cally computationally very expensive. Active inference is a model-based approach
that directs agents to explore uncertain states while adhering to a prior model of
their goal behaviour. This paper introduces an active inference agent which mini-
mizes the novel free energy of the expected future. Our model is capable of solving
sparse-reward problems with a very high sample efﬁciency due to its objective
function, which encourages directed exploration of uncertain states. Moreover,
our model is computationally very light and can operate in a fully online manner
while achieving comparable performance to ofﬂine RL methods. We showcase
the capabilities of our model by solving the mountain car problem, where we
demonstrate its superior exploration properties and its robustness to observation
noise, which in fact improves performance. We also introduce a novel method
for approximating the prior model from the reward function, which simpliﬁes the
expression of complex objectives and improves performance over previous active
inference approaches.
1 Introduction
The ﬁeld of Reinforcement Learning (RL) has achieved great success in designing artiﬁcial agents
that can learn to navigate and solve unknown environments, and has had signiﬁcant applications in
robotics [Kober et al., 2013, Polydoros and Nalpantidis, 2017], game playing [Mnih et al., 2015, Silver
et al., 2017, Shao et al., 2019], and many other dynamically varying environments with nontrivial
solutions [Padakandla, 2020]. However, environments with sparse reward signals are still an open
challenge in RL because optimizing policies over Heaviside or deceptive reward functions such as
that in the mountain car problem requires substantial exploration to experience enough reward to
learn.
Recently, Bayesian RL approaches [Ghavamzadeh et al., 2015] and the inclusion of novelty in
objective functions [Stadie et al., 2015, Burda et al., 2018, Shyam et al., 2019] have begun to
explicitly address the inherent exploration-exploitation trade-off in such sparse reward problems. In
Preprint. Under review.
arXiv:2106.02390v1  [cs.LG]  4 Jun 2021
parallel to these developments, active inference (AIF) has emerged from the cognitive sciences as a
principled framework for intelligent and self-organising behaviour which naturally often converges
with state of the art paradigms in RL (e.g., Friston et al. [2009, 2015], Kaplan and Friston [2018],
Tschantz et al. [2020]). AIF agents minimize the divergence between an unbiased generative model
of the world and a biased generative model of their preferences (shortly, theprior). This objective
assigns an epistemic value to uncertain states, which enables directed exploration. Because of its
principled foundations and because reward functions can be seen as an indirect way of deﬁning prior
models (cf. reward shaping [Ng et al., 1999]), active inference is often presented as a generalization of
RL, with KL-control and control-as-inference as close ontological relatives [Millidge et al., 2020b].
1.1 Related work
Until recently, active inference implementations have been constrained to toy problems in theoretical
expositions [Friston et al., 2015, 2017a,b]. Based on the work by Kingma and Welling [2014] on
amortized variational inference, Ueltzhöffer [2018] proposed the ﬁrst scalable implementation of
AIF using deep neural networks to encode the unbiased generative model and evolution strategies to
estimate policy gradients from multiple parallel simulations on a GPU. Later publications proposed
more efﬁcient policy optimization schemes, such as amortized policy inference [Millidge, 2019]
and applying the cross-entropy method [Tschantz et al., 2019, 2020]. This latter work also uses an
improved extension of the model free energy to future states, namely, the free energy of the expected
future [Millidge et al., 2020a] (cf. divergence minimization [Hafner et al., 2020]). In these papers,
active inference is shown to deliver better performance than current state of the art RL algorithms
on sparse-reward environments, although they use the goal states as hard-coded priors. We improve
upon the model of Tschantz et al. [2020] by demonstrating fully online learning on a single CPU core,
by modeling the transition model with gated recurrent units that can capture environment dynamics
over longer periods, and by learning the prior model from the (sparse) reward function through a
novel reward shaping algorithm.
2 Active inference
The objective of an active inference (AIF1) agent is to minimize surprise, deﬁned as the negative
log-likelihood of an observation, −ln p(y). However, it is often intractable to compute this quantity
directly. Instead, we apply variational inference and minimize a tractable upper bound for surprisal,
namely, the variational free energy (VFE) of a latent model of the world. The agent possesses an
approximate posterior distribution q(x|y), where xis the latent state that is optimized to minimize
the variational free energy. The parameters of this approximate posterior can be thought of as the
agent’s ‘beliefs’ about its environment. The variational free energy can be written as:
VFE = Eq(xt|yt) [−ln p(yt |xt)] + DKL [q(xt |yt)∥p(xt)] (1)
which is equivalent to the negative of the expectation lower bound (ELBO) used in variational
inference (e.g., [Attias, 1999, Kingma and Welling, 2014]).
Active inference agents select actions that are expected to minimize the path integral of the VFE for
future states [Friston, 2012]. There are two common extensions of the VFE to account for future
states, the expected free energy[Friston et al., 2015] and thefree energy of the expected future(FEEF)
[Tschantz et al., 2020], which we use in this work. Millidge et al. [2020a] argues that the FEEF is
the only one consistent with Equation 1 when evaluated at the current time and additionally considers
the expected entropy in the likelihood p(yt |xt) when selecting a policy.
2.1 Free energy of the expected future
The FEEF is a scalar quantity that measures the KL-divergence between unbiased beliefs about
future states and observations and an agent’s preferences over those states. The preferences are
expressed as a biased generative model of the agent ˜p(y,x), also known as the prior. As in RL, the
agent’s world is modelled as a partially observed Markov decision process (POMDP) [Kaelbling
et al., 1998, Sutton and Barto, 1998, Murphy, 2000], where tis the current time, τ is some timestep
in the future and T is the prediction or planning horizon so that t≤τ ≤T. A policy is a sequence
1It is common to abbreviate active inference as AIF to avoid confusion with artiﬁcial intelligence.
2
of actions [at,...a τ,...a T] sampled from a Gaussian policy π that is conditionally independent
across timesteps (i.e., has diagonal covariance matrix). We use the notation π ∼π for a random
policy sample and its corresponding Gaussian policy, respectively. The FEEF can be separated into
an extrinsic (objective-seeking) term and an intrinsic (information-seeking) term when assuming the
factorization ˜p(yτ,xτ) ≈q(xτ |yτ) ˜p(yτ):
FEEF(π)τ = Eq(yτ,xτ|π)DKL [q(yτ,xτ |π) ∥˜p(yτ,xτ)]
≈Eq(xτ|π)DKL [q(yτ |xτ) ∥˜p(yτ)]  
Extrinsic value
−Eq(yτ|xτ,π)DKL [q(xτ |yτ) ∥q(xτ |π)]  
Intrinsic value
(2)
where the difference between the likelihoods q(yτ |xτ) and p(yτ |xτ) in Equation 1 is simply
notational.
Minimizing the extrinsic term biases agents towards policies that yield predictions close to their prior
(i.e. their desired future). Maximizing the intrinsic term gives agents a preference for states which
will lead to a large information gain – i.e., the agent tries to visit the states where it will learn the most.
The combination of extrinsic and intrinsic value together in a single objective leads to goal-directed
exploration, where the agent is driven to explore, but only in regions which are fruitful in terms of
achieving its goals. There is an additional exploratory factor implicit in the use of a KL-divergence in
the extrinsic term, which pushes the agent towards observations where the generative model is not as
certain about the likely outcome [Millidge et al., 2020a] due to the observation entropy term in the
KL-divergence.
The optimal Gaussian policy π∗is found through the optimization
π∗= arg min
π
T∑
τ=t
FEEF(π)τ (3)
by means of a maximum likelihood approach. Although the FEEF is not a likelihood, Whittle [1991]
shows that treating a path integral of a cost as a negative log-likelihood to minimize is formally
equivalent to least squares optimization methods, but more direct to compute.
3 The Active inference capsule
The active inference capsule consists of a variational autoencoder (V AE) which maps the agent’s noisy
observations to a latent representation, a gated recurrent unit (GRU) [Cho et al., 2014] which predicts
future latent states from the current latent state, and a policy optimization scheme that minimizes the
FEEF over a trajectory. The V AE and GRU learn an unbiased latent dynamical model in a similar
fashion as world models by Ha and Schmidhuber [2018]. Additionally, we propose an extension
where the prior model is also learned by the agent from the reward signal. A block-diagram of the
capsule is shown in Figure 1.
The active inference capsule
15
q(xt:T /uni2223x0:t, /uni03C0)y0:t x0:t
˜p(yt:T)
+−DKL in
DKL ex
FEEFt:T
/uni03C0*t:T = arg min/uni03C0t:T
/uni2211FEEFt:T
/uni03C0*t:T
Veridical model (VAE + latent GRU)
Policy optimization (CEM)
r0:t
1 − U(yt:T)
AIF Capsule
If prior is  
given
If prior model is  
learned from rewards
q(x0:t /uni2223y0:t) q(yt:T /uni2223xt:T)
q(xt,T /uni2223yt:T)
Figure 1: Block-diagram of the active inference capsule. The inputs are time-series of the observations
plus the biased priors on future states if given or else the rewards to learn from. The differences
between either source of priors are indicated with distinctive colors. The output is a time-series of
optimized beliefs over actions (i.e., a Gaussian policy) up to the planning horizon. Continuous lines
carry probability distributions whereas discontinuous lines carry real numbers (random samples).
3
Perception and planning Both perception and planning are treated as inference processes (see
Figure 2). During perception, the capsule performs inference on the observations y through the
unbiased variational posterior and transition models. This updates the belief on the latent states x,
the recurrent states hof the GRU, which integrate temporal relations between latent states, and the
parameters of these models through a learning step. During planning, on the other hand, the current
recurrent states are used as initial conditions for the transition model to project future trajectories and
evaluate the FEEF for policy optimization.
Perception (inference from  to )y0 yt
yt
xt
ht
yt−1
xt−1
ht−1
xt−2
ht−2
yt−2
at−1at−2
Planning (inference from  to )yt+1 yT
yt+2
xt+2
ht+2
yt+1
xt+1
ht+1ht
at+1at
Figure 2: Graphical models of the inference during perception and planning. The future latents are
inferred again from the predicted observations, which captures the uncertainty from the variational
posterior into the FEEF.
Gaussian variational autoencoder The variational autoencoder (V AE) consists of the variational
posterior q(xt |yt) (encoder) and the likelihood q(yt |xt) (decoder), both Gaussian and approxi-
mated through amortized inference with neural networks [Kingma and Welling, 2014]. As pointed
out by Mattei and Frellsen [2018], the optimization of V AEs for continuous outputs is ill-posed. To
circumvent the issue, we ﬁx the variance of the decoder to a default value or to the noise level of the
input if given (see the hyperparameters in Appendix A). The results are only mildly sensitive to this
parameter because the minima of the extrinsic term in Equation 2 with respect to the policy only de-
pends on the mode of the likelihood, which is ultimately unaffected by the noise. It does, nonetheless,
affect the spread of the likelihood and therefore the gradient of the optimization landscape.
Recurrent transition model The transition model is factorized into two terms:
q(xt+1 |xt,π) ≡q(xt+1 |ht) q(ht |ht−1,xt,π) (4)
The right term is implemented by a GRU which processes the temporal information of the input
through a recurrent hidden state h. The left term is implemented by a fully-connected (FC) network
which predicts both the update on the latent state and the expected variance. Because the recurrent
states are deterministic, the variance of the predicted latent distributions does not include prediction
errors, which could be an improvement for future work. Our diagram in Figure 3 differs from Ha and
Schmidhuber [2018] in that it predicts an update on the latent state rather than the latent state itself,
as done by Tschantz et al. [2020]. Learning is achieved via stochastic gradient descent where the loss
is the KL-divergence between the predicted latent distribution and the variational posterior after the
observation, so that q(xt+1 |π) ≈q(xt+1 |yt+1).
GRU
ht
ht−1
π FC
⟲ delay
q(xt+1 ∣ π)E[Δxt+1]
++
FC Var[q(xt+1 ∣ yt+1)]
E [q(xt ∣ yt)] E [q(xt+1 ∣ yt+1)]
Figure 3: Block-diagram of the transition model.
Model free energy In contrast to variational autoencoders, which only consider present observa-
tions, in an AIF capsule the latent states are also inferred from past sensory data. We propose the
following modiﬁcation of Equation 1 that accounts for the transition model by using it in place of the
variational posterior and instead using the variational posterior of the current observation as prior for
the predicted latent states:
VFEcapsule = Eq(xt|xt−1,π) [−ln p(yt |xt)] + DKL [q(xt |xt−1,π)∥q(xt |yt)] (5)
This expression is used for evaluating the learning progress of the true generative model.
4
3.1 Deﬁning the prior
The AIF capsule supports using both a pre-set prior or else learning one from the reward function. A
prior in this context is simply a distribution over goal states. Under active inference, the agent uses
its own unbiased world model to generate trajectories that maximize the likelihood of the prior (i.e.,
reaching the goal). Importantly, when learning the prior the agent can also store information about
the optimal solution in a trajectory-independent way by modelling intermediate goal states. Moreover,
learning the prior enables the use of active inference agents in situations where manually deﬁning a
prior is unfeasible. We propose a novel approach for learning the prior directly based on rewards using
Bellman’s optimality principle, therefore making a link with model-based reinforcement learning.
3.1.1 Method for learning the prior model from rewards
This section presents a novel method for optimal reward shaping [Ng et al., 1999] through a continuous
potential function that preserves the optimal policy and is compatible with the extrinsic value in
Equation 2. Let U(y) be a model of the utility of a state yto a trajectory passing through it. The
utility is a scaled sum of potential future rewards. We deﬁne a reward rt as a real number in the
range [−1,1] associated with an observation yt, where −1 is maximally undesirable and 1 maximally
desirable. We use the same range and deﬁnitions for the utility. We suggest a similarity between the
utility model and the extrinsic value of the FEEF, which instead is a real-valued in the range [0,∞)
where 0 corresponds to preferences being perfectly fulﬁlled and ∞to absolutely unpreferred states.
Assuming the agent is not too far from its generative model we approximate the relation as
1 −U(yt) ∝
∼
Eq(xt|π)DKL [q(yt |xt) ∥˜p(yt)] (6)
In other words, both terms have approximately the same landscape. Because the optimal policy does
not depend on the absolute values of the FEEF but only on its minima, we can use 1 −U(yt) during
policy optimization as a surrogate for the extrinsic term which implicitly contains the prior.
We model U as a multi-layered neural network with tanh activation on the outputs. At every
observation, information from the reward signal is infused into the utility model through a stochastic
gradient descend (SGD) step. The loss is the mean squared error between the predicted utility and
the utility by applying Bellman’s equation over a trajectory. In the latter, the discount factor is
exponentially decreased for past observations, which pushes the agent to more quickly reach the
rewarding states. Moreover, the learning rate in the SGD step is scaled with the absolute value of the
reward, which regularizes the magnitude of the model update with the intensity of the stimulus. We
also iterate multiple times the process to allow information to propagate back in time more effectively.
See algorithm 1 for pseudo-code of our dynamic programming approach of learning the utility model.
Algorithm 1: Learning the utility model from rewards
Input: Observations yt0:t — rewards rt0:t — utility model U — discount factor β— learning
rate α— iterations L
for Iteration i= 1 ...L do
Initialize empty list of expected utilities ˆu
for τ = t0 ...t do
if τ = t then
ˆut ←rτ (in an online setting, future observations are unavailable)
else
ˆuτ ←rτ + βt−τU(yτ+1) (Bellman’s equation)
end
end
L← MSE(U(yt0:t),ˆut0:t) (Compute loss)
∂WU
∂L ←Backpropagate(U,L) (Compute weight gradients)
WU ←WU −α|rt|∂WU
∂L (Update weights)
end
5
3.1.2 Policy optimization
Policies are optimized using the cross-entropy method (CEM) [Rubinstein, 1997]. Since the algorithm
is constrained to output Gaussian policies, the exact shape of theFEEF is not captured but the resulting
policies do track its minima [Tschantz et al., 2020]. The pseudocode for the optimization is provided
in algorithm 2.
Algorithm 2: Cross-entropy method for policy optimization
Input: Planning horizon T — Optimization iterations I — # policy samples N — # candidate
policies K— Transition model q(xt+1 |ht),q(ht |ht−1,xt,π) — encoder p(xt |yt) —
decoder p(yt |xt) — current states {xt,ht−1}— prior ˜p(yt)
Initialize a Gaussian policy π ←N(0,IH×H)
for iteration i= 1 ...I do
for sample policy j = 1 ...N do
π(j) ∼π
FEEF(j) = 0
for τ = t...T −1 do
hτ ←E[q(hτ |hτ−1,π(j)
τ ,xτ)]
q(xτ+1 |π(j)) ←q(xτ+1 |hτ)
q(yτ+1 |xτ+1) ←Eq(xτ+1|π(j))[q(yτ+1 |xτ+1)]
q(xτ+1 |yτ+1) ←Eq(yτ+1|π(j))[q(xτ+1 |yτ+1)]
FEEF(j)
τ+1 ← Eq(xτ+1|π(j))DKL[q(yτ+1 |xτ+1)∥˜p(yτ+1)]
−Eq(yτ+1|π(j))DKL[q(xτ+1 |yτ+1)∥q(xτ+1 |π(j))]
FEEF(j) ←FEEF(j) + FEEF(j)
τ+1
xτ+1 ←E[q(xτ+1 |π(j))]
end
end
Select best Kpolicies Reﬁt Gaussian policy π ←reﬁt(ˆπ)
end
return π
4 Experiments on the mountain car problem
In this section, we study the performance of the active inference capsule using the continuous
mountain car problem from the open-source code library OpenAI Gym [Brockman et al., 2016].
This is a challenging problem for reinforcement learning algorithms because it requires a substantial
amount of exploration to overcome the sparse reward function (negative for every additional action,
positive only at the goal). Moreover, the task requires the agent to move away from the goal at ﬁrst
in order to succeed. The objective is to reach the goal in less than 200 simulation steps. In our
experiments, the agent time-step size is 6 simulation steps and the planning window H = T −tis
deﬁned in the agent’s time-scale (see subsection 4.1 for details).
Online learning For all tasks, we initialize all the agents with random weights and learn online
only. Training an agent for 150 epochs takes about 3 minutes on a single CPU core (Intel I7-4870HQ).
In contrast, previous approaches using active inference [Ueltzhöffer, 2018, Tschantz et al., 2019,
2020] and policy gradient methods (e.g., [Liu et al., 2017]) use (ofﬂine) policy replay and typically
need hours of GPU-accelerated compute while achieving similar convergence. To our knowledge,
this is the ﬁrst model-based RL method to learn online using neural network representations. This is
afforded by the high sample efﬁciency of the FEEF, which directs exploration towards states that are
uncertain for both the encoder and transition models.
Given priors versus learned priors Figure 4 shows that agents with a given prior (a Gaussian
distribution around the goal state) depend on their planning window to ﬁnd more optimal policies,
Code and results at https://github.com/adanielnoel/Active-Inference-Capsule
6
whereas agents that learn the prior converge to optimal policies with much shorter planning windows
and without such dependency. Figure 5 shows that the given prior misleads agents with short foresight
to swing forward ﬁrst, whereas the learned prior integrates information about the better strategy
and can be followed without a full preview of the trajectory to the goal. These results highlight the
importance of the prior for model exploitation. The unbiased predictor is an egocentric model of the
world, whereas the prior model is an allocentric representation of the agent’s intended behaviour. The
active inference capsule effectively combines both during policy optimization, therefore deﬁning a
prior based only on the ﬁnal goal blurs the objective for shorter planning windows. This experiment
shows that the reward function is a simple means of indirectly modelling a complex prior.
0 25 50 75 100 125 150
Episodes
0
70
200
400
600
800
1000Steps until goal
Given vs. learned priors (episode length)
Random agent
Given prior, H=15
Given prior, H=10
Learned prior, H=10
Learned prior, H=5
Figure 4: Training curves for different types of agents. When given the prior, agents with a planning
window of 90 simulation steps (H=15) can reach the goal within the 200-step limit, whereas agents
with only a 60-step (H=10) foresight fail. The shortest possible time to the goal is about 70 simulation
steps. Agents that learn the prior converge to the optimal solution even if the planning horizon is
signiﬁcantly earlier than 70 steps ahead, showing that the learned prior also captures information
about the optimal trajectories and not just the goal. Despite starting from a randomly initialized
model, AIF agents can direct exploration already from the ﬁrst episode, evidenced by the better initial
performance compared to agents with purely random actions.
−1 0 1
Horizontal position
−1.0
−0.5
0.0
0.5
1.0
Velocity
tgoal = 149
Given prior, H=10
0.0
0.5
1.0
1.5
2.0
−1 0 1
Horizontal position
−1.0
−0.5
0.0
0.5
1.0
tgoal = 89
Given prior, H=15
0.0
0.5
1.0
1.5
2.0
−1 0 1
Horizontal position
−1.0
−0.5
0.0
0.5
1.0
tgoal = 74
Learned prior, H=5
0.0
0.5
1.0
1.5
2.0
KL extr.
Figure 5: Phase portraits of a trained agent of each type. The contour maps are the extrinsic values
of the FEEF, revealing the Gaussian priors and the learned prior, which also captures information
about the optimal trajectory (higher cost in being closer to the goal but having to swing back). The
red dots are the initial positions (randomized across trials), the thick continuous lines are the true
observations, the thick discontinuous lines are reconstructions by the V AE, and the thin lines are the
predictions projected onto the observation space through the decoder model.
Exploration properties Figure 4 also reveals that, despite starting without an objective, agents
that learn the prior on average ﬁnd the solution in the ﬁrst episode faster than agents that take random
actions and even agents with a given prior. This is an example of the information-seeking objective
of the FEEF. It results in a rapid and directed exploration of the state-space, which accelerates the
solution to this sparse reward RL problem.
7
Effect of observation noise We explore the effect of adding Gaussian noise to the observations.
Figure 6 shows that, despite a brief initial disadvantage, the agents with noisy observations match the
performance of those with clean sensory data and even converge towards the optimal solution. We
think that this robustness to observation noise is supported by the KL-divergence in the extrinsic term,
as pointed out by Hafner et al. [2020] in the divergence minimization framework. In fact, rather than
impairing the capsule, observation noise actually improves learning of the unbiased model, evidenced
by the much faster convergence of the model free energy ( VFEcapsule). [An, 1996] showed that
additional input noise induces a regularizing effect on the backpropagated errors that can improve
parameter exploration and prevent overﬁtting.
0 25 50 75 100 125 150
Episodes
070
200
400
600
800
1000Steps until goal
Effect of noise on episode length
Learned prior, H=5
Learned prior, H=5, with noise
0 25 50 75 100 125 150
Episodes
0
1000
2000
3000Cumulative free energy
Effect of noise on model free energy
Learned prior, H=5
Learned prior, H=5, with noise
Figure 6: (Left) Training curves for agents that learn their own prior, with and without observation
noise. Both have very similar convergence, showing that the model is robust to noise. (Right)
Cumulative free energy of each episode. The model free energy for agents with observation noise
converges much faster, possibly due to the additional regularizing effect against local optima.
Ablation study We explore the contributions of the intrinsic and the extrinsic terms in the behaviour
of the agents. Figure 7 shows that the intrinsic term alone drives convergent behaviour in the mountain
car problem. This is because the goal states are also the rarest (at most once per trial) and therefore
directed exploration is both necessary and sufﬁcient to solve the task. Instead, the extrinsic term
alone almost never ﬁnds the goal state. The extrinsic term promotes exploration of the observation
space but not of the latent space (see subsection 2.1), which results in a lower sample efﬁciency for
model exploration. However, if we hot-start the agent for a few steps before disabling the intrinsic
term the behaviour becomes bimodal: the prior model can sometimes gather enough experience for
the extrinsic term to maintain a convergent behaviour. These results show that, while the extrinsic
term is responsible for the convergence to optimal solutions, the intrinsic term is key for making this
behaviour robust because it promotes policies with a high entropy, which prevents convergence to
local minima, as well as generates sufﬁcient exploration of the state-space to obtain the sparse reward
necessary for learning.
0 25 50 75 100 125 150
Episodes
0
70
200
400
600
800
1000Steps until goal
Ablation study, learned prior, H=5
Full model
Only intrinsic term
Only extrinsic term
Only extrinsic term, 25-trial hot start
Figure 7: Training curves by selectively disabling the terms of the FEEF. The intrinsic term alone
(directed exploration) is enough to solve the mountain car problem. The extrinsic term alone is not
enough unless hot-started with some initial episodes with the full model. Then it either overcomes
the sparse rewards and converges or it fails to learn a good prior model due to insufﬁcient exploration
(the plot is bimodal). The full model combines the convergent but unstable behaviour of the extrinsic
term with the robustness furnished by the high sample efﬁciency of the intrinsic term.
8
4.1 Implementation details
We update the agent every 6 simulation steps and apply the same action during that period. This
simple action-repeat method reduces computation cost, can increase the prediction performance due
to higher feature gradients and lower variance of future choices, and is observed in human subjects
too [Mnih et al., 2015, Sharma et al., 2017]. Following the same idea, the agent also commits to
executing the ﬁrst two actions of the chosen policy. Effectively, the agent revises its policy every 12
simulation steps. For each experiment, we train 30 agents of each type and plot their mean and the
region of one standard deviation, clipped to the minimum or maximum per episode if exceeded. See
Appendix A for speciﬁc details of the neural networks and hyperparameters.
5 Discussion
We introduced an active inference capsule that can solve RL problems with sparse rewards online
and with a much smaller computational footprint than other approaches. This is achieved through
minimizing the novel free energy of the expected futureobjective with neural networks, which enables
a natural exploration-exploitation balance, a very high sample efﬁciency and robustness to input
noise.
Moreover, the capsule can either directly follow a given prior (i.e., the goal states) or learn one from
the reward signal using a novel algorithm based on Bellman’s equation. We compare both approaches
and show that agents with learned priors converge to optimal trajectories with much shorter planning
horizons. This is because learned priors approximate a density map of the rewarding trajectories,
whereas given priors typically only provide information of the ﬁnal goals.
Our results show that the FEEF induces entropy regularization on the policies through uncertainty
sampling, which prevents local convergence and accelerates learning. Moreover, our algorithm learns
priors that push the agent to achieve its goals as early as possible within the constraints of the problem.
The combination of these two characteristics results in a fast and consistent convergence towards the
optimal solution in the mountain-car problem, which is a challenge for state of the art RL methods.
Despite the success of the method in the mountain car problem, it remains unclear if the goal-directed
exploration properties will scale to high-dimensional inputs or much more complex dynamics. It is
also unclear whereas the method we introduced for learning the prior model from the reward signal is
generally applicable to any problem. When working with images as inputs, it may be necessary to
pre-train the V AE ofﬂine on a large dataset and to use a GPU for accelerating computations.
Finally, we believe that the AIF capsule could become a building block for hierarchical RL, where
lower layers abstract action and perception into increasingly expressive spatiotemporal commands
and higher layers output priors for the lower layers. In this set up, scalability and generality would be
achieved by designing wider and deeper networks of AIF capsules, rather than using a large single
capsule.
Broader Impact
Active inference and the underlying free energy principle describe the self-organising behaviour
of biological systems at different spatiotemporal scales, ranging from microscales (e.g., cells), to
intermediate scales (e.g., learning processes), to macroscales (e.g., societal organization and the
emergence of new species) [Hesp et al., 2019]. It is a relatively new science with broad-ranging
applications in any technology that has to interact with the real world. But because of its complexity
and lack of efﬁcient implementations, active inference has mostly remained an explanatory device
with limited applicability outside of the scientiﬁc scope. Developments like the active inference
capsule presented here may soon unlock the beneﬁts of this new technology for nanobiology, robotics,
artiﬁcial intelligence, ﬁnancial technologies, and other high-tech markets.
We acknowledge that the development of this technology may raise safety and ethical concerns in
the future, although the scope of the present work is still only methodological. Nonetheless, the
model-based nature of active inference renders its decisions partially explainable inasmuch as we
understand its priors, which are typically easy to interpret since they are expressed in the observation
space (e.g., Figure 5). This can be a great advantage over model-free RL methods, which instead are
very hard to interpret and to validate for safety-critical applications.
9
References
Guozhong An. The Effects of Adding Noise during Backpropagation Training on a Generalization
Performance. Neural Computation, 8(3):643–674, 1996. ISSN 08997667. doi: 10.1162/neco.1996.
8.3.643.
Hagai Attias. Inferring Parameters and Structure of Latent Variable Models by Variational Bayes.
In Fifteenth conference on Uncertainty in artiﬁcial intelligence , pages 21–30, jan 1999. URL
http://arxiv.org/abs/1301.6676.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym, 2016. URL http://arxiv.org/abs/1606.01540.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint, pages 1–17, 2018. ISSN 23318422.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder for
Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1724–1734, Stroudsburg, PA, USA, jun 2014.
Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL http://arxiv.
org/abs/1406.1078http://aclweb.org/anthology/D14-1179.
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. Neural Networks, 107(2015):3–11, 2018. ISSN
18792782. doi: 10.1016/j.neunet.2017.12.012.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and
Giovanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214,
oct 2015. ISSN 1758-8928. doi: 10.1080/17588928.2015.1020053.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo.
Active Inference: A Process Theory. Neural Computation, 29(1):1–49, jan 2017a. ISSN 0899-
7667. doi: 10.1162/NECO_a_00912. URL http://arxiv.org/abs/1803.01446https://
www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00912.
Karl J. Friston. A free energy principle for biological systems. Entropy, 14(11):2100–2121, 2012.
ISSN 10994300. doi: 10.3390/e14112100.
Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference?
PLoS ONE, 4(7):e6421, jul 2009. ISSN 1932-6203. doi: 10.1371/journal.pone.0006421.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha
Ondobaka. Active Inference, Curiosity and Insight. Neural Computation, 29(10):2633–2683, oct
2017b. ISSN 0899-7667. doi: 10.1162/neco_a_00999. URL http://arxiv.org/abs/1803.
01446https://direct.mit.edu/neco/article/29/10/2633-2683/8300.
Mohammed Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian Reinforcement
Learning: A Survey. Foundations and Trends® in Machine Learning, 8(5-6):359–483, 2015. ISSN
1935-8237. doi: 10.1561/2200000049.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint, 2018. ISSN 23318422. doi:
10.1016/b978-0-12-295180-0.50030-6.
Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl J. Friston, and Nicolas Heess. Action
and Perception as Divergence Minimization, sep 2020. URL http://arxiv.org/abs/2009.
01791.
Casper Hesp, Maxwell J.D. Ramstead, Axel Constant, Paul Badcock, Michael Kirchhoff, and Karl J.
Friston. A Multi-scale View of the Emergent Complexity of Life: A Free-Energy Proposal. In
Evolution, Develompent and Complexity, pages 195–227. Springer International Publishing, 2019.
doi: 10.1007/978-3-030-00075-2_7.
10
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artiﬁcial Intelligence, 101(1-2):99–134, may 1998. doi:
10.1016/S0004-3702(98)00023-X. URL https://linkinghub.elsevier.com/retrieve/
pii/S000437029800023X.
Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological
Cybernetics, 112(4):323–343, 2018. ISSN 14320770. doi: 10.1007/s00422-018-0753-2. URL
https://doi.org/10.1007/s00422-018-0753-2 .
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. InInternational Conference
on Learning Representations (ICLR), 2014. URL http://arxiv.org/abs/1312.6114.
Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.
International Journal of Robotics Research , 32(11):1238–1274, 2013. ISSN 02783649. doi:
10.1177/0278364913495721.
Yang Liu, Prajit Ramachandran, and Jian Peng. Stein variational policy gradient. arXiv preprint,
2017. ISSN 23318422.
Pierre Alexandre Mattei and Jes Frellsen. Leveraging the exact likelihood of deep latent variable
models. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pages 3859–3870, 2018.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology, 96:102348, 2019. ISSN 10960880. doi: 10.1016/j.jmp.2020.102348.
Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the Expected Free Energy?
arXiv preprint, 2020a. ISSN 23318422. doi: 10.1162/neco_a_01354.
Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the Relationship
Between Active Inference and Control as Inference. In International Workshop on Active Inference,
pages 3–11, 2020b. doi: 10.1007/978-3-030-64919-7_1. URL http://link.springer.com/
10.1007/978-3-030-64919-7_1 .
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learn-
ing. Nature, 518(7540):529–533, 2015. ISSN 14764687. doi: 10.1038/nature14236. URL
http://dx.doi.org/10.1038/nature14236.
K P Murphy. A survey of POMDP solution techniques. Technical report, 2000.
Andrew Y . Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations
: Theory and application to reward shaping. Sixteenth International Conference on Machine
Learning, 3:278–287, 1999. ISSN 1098-6596.
Sindhu Padakandla. A Survey of Reinforcement Learning Algorithms for Dynamically Varying
Environments. arXiv preprint, pages 1–15, 2020. ISSN 23318422.
Athanasios S. Polydoros and Lazaros Nalpantidis. Survey of Model-Based Reinforcement Learning:
Applications on Robotics. Journal of Intelligent and Robotic Systems: Theory and Applications,
86(2):153–173, 2017. ISSN 15730409. doi: 10.1007/s10846-017-0468-y.
Reuven Y . Rubinstein. Optimization of computer simulation models with rare events.European Jour-
nal of Operational Research, 99(1):89–112, 1997. ISSN 03772217. doi: 10.1016/S0377-2217(96)
00385-2.
Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, and Dongbin Zhao. A Survey of Deep
Reinforcement Learning in Video Games. arXiv preprint, 2019.
Sahil Sharma, Aravind Srinivas, and Balaraman Ravindran. Learning to Repeat: Fine Grained
Action Repetition for Deep Reinforcement Learning. Iclr, pages 34–47, feb 2017. URL http:
//arxiv.org/abs/1702.06054.
11
Pranav Shyam, Wojciech Jaskowski, and Faustino Gomez. Model-based active exploration. 36th
International Conference on Machine Learning, ICML 2019, 2019-June:10136–10152, 2019.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George Van Den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of Go without human knowledge. Nature, 550(7676):354–359, 2017. ISSN 14764687.
doi: 10.1038/nature24270. URL http://dx.doi.org/10.1038/nature24270.
Bradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing Exploration In Reinforcement
Learning With Deep Predictive Models. ArXiv preprint, pages 1–11, 2015. URL http://arxiv.
org/abs/1507.00814.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: an Introduction . MIT Press,
Cambridge, MA, 1 edition, 1998.
Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active
inference. arXiv preprint, pages 1–13, nov 2019.
Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement
Learning through Active Inference. arXiv preprint, feb 2020.
Kai Ueltzhöffer. Deep active inference. Biological Cybernetics, 112(6):547–573, 2018. ISSN
14320770. doi: 10.1007/s00422-018-0785-7.
P. Whittle. Likelihood and Cost as Path Integrals. Journal of the Royal Statistical Society: Series
B (Methodological), 53(3):505–529, jul 1991. ISSN 00359246. doi: 10.1111/j.2517-6161.1991.
tb01842.x. URL http://doi.wiley.com/10.1111/j.2517-6161.1991.tb01842.x.
12
A Implementation details and hyperparameters
The variational posterior model has a hidden layer with SiLU activations, which are typically better
than ReLU activations in RL settings [Elfwing et al., 2018], and two output layers for mean and
standard deviation. The likelihood model has the same structure but outputs a ﬁxed standard deviation
(0.05 by default, 0.1 in the case of noisy inputs). The GRU of the transition model has input size
dim(x) + dim(a) and hidden size dim(z) parametrized by 2H·dim(x), where H is the planning
window in the agent’s time-scale. The FC layers map fromdim(z) to dim(x). The observations are
the position and velocity (dim(y) = 2) and the actions are the horizontal force (dim(a) = 1). The
learned prior model consists of a single hidden layer with SiLU activations and Tanh activation on
the outputs. The full list of hyperparameters is shown in Table 1.
Table 1: Agent hyperparameters
General hyperparameters
Latent dimensions dim(x) 2
V AE hidden layer size 20
Observation noise std. 0 or 0.1
Time ratio simulation / agent 6
V AE learning rate (ADAM) 0.001
Transition model learning rate (ADAM) 0.001
Policy hyperparameters
Planning window H 6, 10 or 15
Actions before replanning 2
Policy samples N (CEM) 700 for H ∈{6,10}
1500 for H = 15
Candidate policies K(CEM) 70
Optimization iterations I (CEM) 2
Hyperparameters for learned priors
Hidden layer size (learned priors) 40
Learning rate (SGD) 0.1
SGD steps per reward 15
Discount factor β 0.995
13