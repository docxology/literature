arXiv:1902.11233v1  [q-bio.NC]  28 Feb 2019
The principles of adaptation in organisms and machines I:
machine learning, information theory, and thermodynamics
Hideaki Shimazaki
Graduate School of Informatics, Kyoto University
Abstract
How do organisms recognize their environment by acquiring k nowledge about the world, and what ac-
tions do they take based on this knowledge? This article exam ines hypotheses about organisms’ adaptation
to the environment from machine learning, information-the oretic, and thermodynamic perspectives. We
start with constructing a hierarchical model of the world as an internal model in the brain, and review
standard machine learning methods to infer causes by approx imately learning the model under the maxi-
mum likelihood principle. This in turn provides an overview of the free energy principle for an organism,
a hypothesis to explain perception and action from the princ iple of least surprise. Treating this statistical
learning as communication between the world and brain, lear ning is interpreted as a process to maximize
information about the world. We investigate how the classic al theories of perception such as the infomax
principle relates to learning the hierarchical model. We th en present an approach to the recognition and
learning based on thermodynamics, showing that adaptation by causal learning results in the second law
of thermodynamics whereas inference dynamics that fuses ob servation with prior knowledge forms a ther-
modynamic process. These provide a uniﬁed view on the adapta tion of organisms to the environment.
1 Introduction
How do organisms recognize their environment by ac-
quiring knowledge about the outside world for their
survival, and what actions do they take based on this
knowledge? The purpose of this article is to gain a
deeper understanding of this issue by explaining the
perception and behavior of organisms from multiple
perspectives in diﬀerent disciplines, namely machine
learning, information theory, statistical mechanics,
and thermodynamics.
Natural stimuli have characteristic statistical reg-
ularity that follows the rules of materials or life. As
a result of evolution and development, it is thought
that the nervous systems of living things adapt to
the statistical regularity of natural stimuli, and eﬃ-
ciently encode them in their activity [1, 2, 3, 4, 5, 6].
From an information theoretic point of view, Horace
Barlow proposed the eﬃcient coding hypothesis, an
adaptive principle of brains, which states that redun-
dancy of information in the environment is eliminated
and represented as independent activity in the brain
[2, 3]. According to the eﬃcient coding hypothesis, an
understanding of the statistical structure of natural
∗Address: Yoshida-honmachi, Sakyo-ku, Kyoto 606-
8501, Japan; Email: h.shimazaki@i.kyoto-u.ac.jp; URL:
http://www.neuralengine.org
stimuli such as the sounds of forests, coastal land-
scapes or ﬂowing streams deepens our understanding
of coding mechanisms of the brain [7, 8]. Indeed, it is
known that this hypothesis explains response charac-
teristics of early sensory neurons such as nonlinearity
of responses to stimuli, or sensitivity to the temporal
and spatial arrangement of stimuli, called receptive
ﬁelds [7, 8, 9, 10, 11].
Natural stimuli must originate from certain causes.
If we further consider the adaptive mechanisms of the
brain, we arrive at the concept of the brain as an or-
gan that infers the causes of input stimuli. According
to this view, the brain is an inference organ equipped
with an empirical model of what sorts of things in
the outside world cause the sensory data, and infers
the causes of the current data based on this model.
This view was proposed in the ﬁrst half of the 20th
century by a German physicist and physician, Her-
mann von Helmholtz [12]. 1 He used the term un-
bewusster Schluss (unconscious conclusions) to refer
to conclusions about the causes of sensory input, ar-
guing that these are dominated by inductive conclu-
sions originating from experiences and analogy rather
than deductive logic. 2 This concept is now known as
1Building on earlier work by Francis Bacon, Thomas
Hobbes and others.
2Helmholtz himself concludes the chapter on recognition in
1
1 INTRODUCTION
Helmholtz’s unconscious inference. Statistically opti-
mal reasoning is achieved by constructing a posterior
distribution of causes from a Bayesian formula to-
gether with a hierarchical model (generative model)
that describes the statistical structure of data gen-
eration. The hypothesis that organisms achieve op-
timal or approximate Bayesian inference using their
nervous systems is generally called the Bayesian brain
hypothesis [14]. In fact, in tasks where decisions have
to be made based on uncertain input stimuli, many
studies have reported that humans and other animals
arrive at conclusions that are close to those from
the Bayesian statistical inference [15]. Many ideas
have been proposed by theoretical groups regarding
mechanisms for calculating Bayesian posterior distri-
butions with neural systems [15, 16, 17, 18].
Hinton and Dayan et al. proposed an optimiza-
tion algorithm for a hierarchical model called the
Helmholtz machine, and introduced variational free
energy as its objective function [19, 20]. Friston et
al. argued that various related theories in recogni-
tion and learning can be handled in a uniﬁed way
by the principle of minimizing variational free en-
ergy (free energy principle) [21, 22, 23, 24, 25, 26].
This principle is also called surprise minimization be-
cause data is no longer surprising after learning. In
the literature of machine learning, the same frame-
work for stochastic latent variable models is called
variational inference [27, 28]. This framework origi-
nates from Dempster et al.’s Expectation Maximiza-
tion algorithm, and is established as an optimization
framework of applied models including various hid-
den Markov models, latent dirichlet allocation used
in natural language processing, and a variational au-
toencoder used in deep learning.
These models construct a generative model of the
external world, which is a hypothesis about data gen-
eration. Among them, predictive coding theory of the
brain [29, 30, 31, 32, 33] hypothesizes that recursive
hierarchical modules learn the statistical regularity.
These modules receive prediction from higher mod-
ules and send prediction error to the higher modules,
which successfully explained contextual modulation
of early visual neurons by the feedback prediction
signals [31]. An extended framework called message
passing algorithms (belief propagation) on graphical
models is discussed as computational architecture of
the cortex [32, 34], in particular in the context of the
free energy principle [35, 23, 36]. In this article, we
will see how information about the data is absorbed
his book [13] as follows: “it is the characteristic function of
the intellect to form general conceptions, that is, to searc h for
causes; and hence it can conceive (begreifen) of the world on ly
as being causal connection.”
into the model as prior knowledge through learning,
and how this knowledge can act as a top-down mod-
ulation signal, from perspectives of machine learning,
information-theory, and thermodynamics.
These modeling approaches, however, take into ac-
count only passive aspects of the brain responding
to presented stimuli. In real world, animals actively
explore the environment to acquire data of the out-
side world. Indeed, as has been pointed out by many
scientists and philosophers including Merleau-Ponty
and, more recently, Andy Clark, Rodney Brooks and
Francisco Valera, the brain is ﬁrst and foremost a de-
vice for moving the body, and it is not possible to
conclude the theory of recognition and learning with-
out considering organisms’ active interaction with the
environment [37, 38].
Friston’s theory builds on the static recognition
model to provide a uniﬁed view that also includes
actions [22, 39, 40, 36]. This hypothesis called active
inference assumes that actions are selected based on
the principle of least surprise. For example, consider
the task of recognizing the book you are holding. To
recognize a book using the static hierarchical mod-
els (including deep neural networks), it is necessary
to perform learning by using a large amount of data
obtained by tilting the book, rotating it, examining
the cover and so on, so that it is possible to recognize
an item as a “book” no matter what state it is in.
But how do humans do this? If you do not under-
stand what something is by looking only at the spine
of the book, you might try turning this object over.
When you see the front, you will recognize that it is
a book. In this example, the use of actions makes
it easier to recognize that the object is a book, even
when using an internal model that only points to the
cover of the book. When data does not match our
generative model of the world, it is more likely to
surprise us (because the goodness-of-ﬁt of the data to
the model is small). We can therefore make changes
to the outside world in order to adapt the object to
our recognition capabilities and reduce the surprise.
This is a way of interpreting behavior based on the
generative model of the world in the brain, and at-
tempts are being made to create a uniﬁed theory of
recognition and behavior by rewriting actions in this
framework, including the selection of actions by rein-
forcement learning [41, 40, 42].
In this article, we ﬁrst introduce the hierarchical
model of the environment, and explain how the model
is interpreted as a model of the brain. We review
an approximate inference method in machine learn-
ing, and explain how it is related to the free energy
principle. Next, we present an overview of learn-
ing hierarchical models from an information theoretic
2
2 LEARNING HIERARCHICAL MODELS
 Y
hidden state, neural activity
parameters, brain structuredata
environment organism 
action
measurement
recognition model
 W
 X
   q(x| Y) p(x| Y,W)
Figure 1: Interaction between organisms and the environment
viewpoint, and explain how it relates to the informa-
tion maximization principle (infomax principle) and
the eﬃcient coding hypothesis, which are the clas-
sical theories of sensory perception. Finally, from
the framework of statistical mechanics and thermo-
dynamics for entropy maximization, we redeﬁne free
energy by revisiting the model of recognition, and dis-
cuss the formal relationship among statistical learn-
ing, minimization of free energy, and the second law
of thermodynamics.
2 Learning hierarchical models
In this section, we introduce the hierarchical models
used in the ﬁelds of machine learning and statistics,
and consider the meaning of introducing hierarchi-
cal models as a model of the brain. Next, we deﬁne
statistical learning and clarify diﬃculties of learning
the hierarchical models to motivate the need of ap-
proximate inference methods that will be described
in detail in the next section.
2.1 Hierarchical models
Organisms and machines can infer underlying causes
from the observed data, and can use this knowledge in
performing their next actions (see Fig. 1). It was hy-
pothesized that organisms achieve these recognition
and behaviors by constructing a model on causal re-
lationships in the outside world. 3 Consider a model
that assumes the following hierarchical causal rela-
tionship. Let Y be the data obtained from the out-
side world, and assume that this data is sampled
3Here the ‘cause’ or ‘causative factor’ is the one assumed
in the model or brain. To verify if the assumed causes in-
deed causally inﬂuence data, we need to intervene the world
to control the causes. It is an act to test the causal model of
the world. We will touch upon action selection based on the
hierarchical model in the next section.
from a distribution ¯ p(y).4 The symbol X repre-
sents the causative factors behind the generation of
data. Furthermore, W represents the environmen-
tal factors or contexts that underlie these causative
factors and data. Assuming the distribution for the
causes and environmental factors, the joint distribu-
tion p(y, x, w) that represents the dependency be-
tween the data and these factors is called a genera-
tive model. The following hierarchical model is now
constructed as a generative model for data:
p(y, x, w) = p(y|x, w)p(x|w)p(w). (1)
In this article, the environmental factors w are
treated as a parameter without assuming a distribu-
tion.5 In this case, p(y|x, w) is called the observation
model, and p(x|w) is called the prior distribution. A
model of the data distribution can be derived by the
following equation,
p(y|w) =
∫
p(y, x|w)dx. (2)
This operation is called marginalization of latent vari-
ables.
A generative model is constructed based on an as-
sumed hierarchical structure for the data generation,
and this generative model can be used to estimate un-
derlying causes from data according to the following
Bayes’ theorem:
p(x|Y, w) = p(Y|x, w)p(x|w)
p(Y|w) . (3)
This is the distribution of causative factors for a given
set of data, and is called the posterior distribution. In
4In this article, we will use uppercase letters to denote ran-
dom variables, and lowercase letters to denote variables.
5x may be called the parameter of the observation model.
In this case, w is called the hyper-parameter.
3
2 LEARNING HIERARCHICAL MODELS
the case of machine learning, estimation based on the
posterior distribution may be performed by using its
analytical formula, or by using sampling techniques.
Organisms are also able to act based on inference
and prediction, whereby sensory data is encoded in
the form of neural activity and the network struc-
ture that supports it; therefore it is likely that the
brain implements functions similar to the Bayesian
inference [14]. Below we discuss how we consider the
hierarchical model as a model of the brain in this
article.
As a model of the brain, the posterior distribution
of the latent variable X is interpreted as neural activ-
ity caused by the stimulus Y. The prior distribution
of X is neural activity exposed to the various stimuli
in natural environment. To explain this in detail, we
introduce the following simpliﬁed hierarchical struc-
ture as an internal model of the world in the brain:
p(y, x|w) = p(y|x, φ )p(x|λ ), (4)
where φ is a parameter of the observation model, λ
is a parameter of the prior distribution, and w =
(φ , λ ).6 The observation model p(y|x, φ ) represents
how data is expressed by combinations of neuronal
activity X. The parameter φ can be the basis func-
tions for representing the data. The prior distribution
p(x|λ ) represents constraints on the neural activity.
Next, using this generative model, the posterior
distribution of the latent variable is obtained as
p(x|Y, w) = p(Y|x, φ )p(x|λ )
p(Y|w) . (5)
The posterior distribution of X represents the neu-
ral activity in response to stimulus Y.7 It is formed
by combination of the observation model and prior
distribution. Thus we can interpret that the poste-
rior is constructed by modulating the internal neural
activity represented as the prior distribution by ob-
serving data. In this case, we identify the prior as
spontaneous activity of neurons [43]. Alternatively,
we may consider the prior distribution constrains the
neural activity; therefore act as a bias signal on the
6From the original hierarchical model, this hierarchical
model is obtained as follows. First, the observation model i s
obtained as p(y|x, φ ) under the assumption that y is condition-
ally independent of λ given x. Second, the prior distribution
is obtained as p(x|λ ) given that x does not depend on φ . Note
that x is independent from φ because the node y that connects
these two is marginalized.
7Alternatively, often in literature on predictive coding an d
the free energy principle, activity rates (ﬁring rates) of n eu-
rons refers to maximum a posteriori (MAP) estimate of the
posterior distribution as a proxy of the distribution. Here we
take a view that neural activity represents a sample from the
posterior distribution.
Symbols Descriptions
Y, y data
sensory stimulus
X, x cause, latent variable
neural activity
W, w parameter, context
brain structure
φ parameter, basis function
brain structure
λ parameter in the prior
ω basis function, regularization
structure of spontaneous activity
β regularization
feedback modulation
p(Y|w) marginal likelihood, evidence
p(Y, X|w) complete data likelihood
p(y, x|w) generative model
p(y|x, φ ) observation model
p(x|λ ) prior distribution
spontaneous or modulation activity
p(x|Y, w) posterior distribution
evoked activity
q(x|Y) approximate posterior distribution
recognition model, evoked activity
L[q, p] lower bound, variational lower bound,
evidence lower bound (ELBO)
Q[q, p] expected complete data
log-likelihood, Q-function
H[q] entropy
neural activity. For example, if X is assumed to be
the activity of an area in the early visual cortex, the
inﬂuence of the activity from other areas (such as
feedback input from lateral neurons or upper areas)
can be expressed by the prior distribution.
The above provides a view to interpret the neu-
ral activity as dynamics to achieve inference of the
causative factors in the outside world. However, it
should be noted that the ﬂow of information in our
physiological systems is opposite: external stimuli or
causes make peripheral sensory receptors respond,
which successively activate peripheral and central
nervous systems. How this forward direction of the
information processing can be framed into the inverse
inference framework will be discussed in the section
that relates the inference framework with information
theory.
4
2 LEARNING HIERARCHICAL MODELS
2.2 Learning
Learning refers to the process of adjusting the model
parameters so that the true data distribution and
the model distribution become as close as possible.
Here it is necessary to deﬁne the concept of close-
ness for distributions. We use the Kullback-Leibler
divergence, according to which the closeness of a dis-
tribution to the true data distribution is deﬁned as
follows:
KL[¯p(y)||p(y|w)] ≡
∫
¯p(y) log ¯p(y)
p(y|w) dy
= EY log ¯p(Y) − EY log p(Y|w). (6)
Throughout this article, EY represents the expec-
tation by the true data distribution ¯ p(y).8 We
want to reduce this KL divergence by modifying
the model through learning, and this can be done
by maximizing EY log p(Y|w) in the second term.
The true data distribution EY is unknown, but this
term can be estimated by replacing it with the dis-
tribution of the observed data (empirical distribu-
tion). When an independent sample has been ob-
tained, the empirical distribution can be written as
p(y) = n−1 ∑ n
i=1 δ(y − Yi). For the sake of simplic-
ity, we use Y to represent either a set of all samples
{Y1, Y2, . . . , Yn} or one sample Yi (i = 1 , . . . , n )
(see discussion below).
Thus log p(Y|w) is the estimated value of the sec-
ond term in Eq. 6, and is a function of w since the
data point Y is given. p(Y|w) is called a marginal
likelihood function, and its logarithm is called the log
marginal likelihood function. The process of deter-
mining parameters that maximize the marginal like-
lihood function is called the Type II maximum likeli-
hood estimation. The resulting maximum likelihood
estimate value is as follows:
W∗ = arg max
w
log p(Y|w). (7)
Here the marginal likelihood function is given by
Eq. 2. The maximum likelihood estimation method is
the process of choosing the model distribution clos-
est to sample data in terms of the KL divergence
(Fig. 2). It is a projection of a sample from a higher-
dimensional data distribution to a restricted model
space.
The method for constructing a posterior distribu-
tion (Eq. 3) using the generative model with the pa-
rameter W∗ that is empirically obtained by learning
(maximum likelihood estimation) is called the empir-
8Note that this is not the expected value of the data gener-
ated by the model.
p(Y|w)
Y
Figure 2: Maximum likelihood estimation
ical Bayes method:
p(x|Y, W∗) = p(Y|x, W∗)p(x|W∗)
p(Y|W∗) . (8)
This estimation method is a hybrid of the Bayesian
inference and maximum likelihood estimation. In this
way, the empirical Bayes method that incorporates
the “experience” of data-based learning into prior
knowledge is an algorithm that realizes Helmholtz’s
unconscious inference.
But unfortunately, this direct empirical Bayes
method often runs into diﬃculties. To obtain the
marginal likelihood of Eq. 2, it is necessary to inte-
grate over the causative factors x, but it is diﬃcult to
perform this integration analytically except when the
observation model and prior distribution are normal
distributions or special distributions. If the marginal
likelihood and its gradient cannot be obtained, then
it will not be possible to perform parameter optimiza-
tion. Also, since the marginal likelihood appears as
a normalization term in the Bayes’ theorem of Eq. 8,
it will also be impossible to calculate the posterior
distribution.
In statistics and machine learning, methods were
therefore devised for approximately optimizing pa-
rameters and constructing the posterior distribution.
The algorithm used for this approximate inference is
described below. Since the inference processes in the
brain’s neural network should be performed approx-
imately, algorithms for learning approximate infer-
ence model in the statistics and machine learning is
instructive when we consider models of the brain [21].
Finally, it is important to note that learning the
parameters can be performed using diﬀerent sets of
5
3 APPROXIMATE INFERENCE
X
Y



N
Figure 3: Graphical representation of the hierarchical
model.
data, namely either an entire set of data or each
sample. Although expressions in the above and fol-
lowing descriptions do not distinguish these diﬀer-
ences for the sake of simplicity, it should be noted
that these diﬀerent optimization strategies result in
diﬀerent learning dynamics that operate in distinct
time-scales, and have signiﬁcant implications of the
parameters as a model of the brain. In this article, we
assume that the parameters in the observation model
φ are learned from an entire set of the observed data,
{Y1, Y2, . . . , Yn}. The graphical representation of
the model is shown in Fig. 3. For example, the ba-
sis functions to represent images are learned from a
set of many visual scenes exposed to an animal. In
organisms, it is expected that learning with multi-
ple samples are performed in an online manner that
marginally updates parameters according to the con-
tribution by each sample in turn. Thus this learning
is gradual. The same approach, called the stochastic
gradient descent (SGD) method, is taken in machine
learning to deal with a large number of samples.
In contrast, we can consider two types of param-
eters in the prior distribution λ = {β , ω }: one set
of parameters ω are optimized by using all samples,
and the others β are optimized for each sample of
the data. For example, the parameters that represent
spontaneous activity of neurons may be learned from
many samples [43], whereas the parameters that mod-
ulates representation by the neural activity according
to the current context needs to be learned from each
sample. This latter learning is faster than the others.
In this case, learning the prior plays an important role
in constructing a posterior distribution that is tai-
lored for an individual sample. Early sensory neurons
receive feedback/recurrent modulation via top-down
or lateral connections, which plays an essential role
in perceptual experiences [44, 45], attention [46, 47],
and reward modulation [48] (see [49] for a review).
This feedback/recurrent modulation is interpreted as
a prior signal that biases the observation model, and
construct the posterior (evoked activity of neurons).
In machine learning, the same approach that optimize
prior parameters for each sample is known as the au-
tomatic relevance determination (ARD) method [50],
and is often used to represent the data by imposing
sparsity to basis functions.
3 Approximate inference
In this section, we will explain the approximate infer-
ence methods that play an important role in machine
learning, known as the Expectation-Maximization
(EM) algorithm. This classical algorithm is a basis
of variational methods which are standard tools to
learn hierarchical models under the principle of min-
imizing variational free energy. Understanding the
logic of ﬁtting the model in the EM algorithm thus
provides the basis of constructing advanced genera-
tive models, and is also instructive when we consider
recognition and learning happening in the brain [21].
A detailed description of the EM algorithm can be
found in standard machine learning textbooks as well
[51, 50].
3.1 Recognition model and lower
bound
Instead of using an exact posterior distribution, the
approximate inference method considers another dis-
tribution that is easier to handle, and uses it to ap-
proximate the posterior distribution:
q(x|Y) ≈ p(x|Y, w). (9)
This approximate posterior distribution is also re-
ferred to as a recognition model. Using this recogni-
tion model, the logarithmic marginal likelihood can
be decomposed as follows [52]:
log p(Y|w)
=
∫
q(x|Y) log p(Y|w)dx
=
∫
q(x|Y) log p(Y, x|w)
p(x|Y, w) dx
=
∫
q(x|Y) log q(x|Y)
q(x|Y)
p(Y, x|w)
p(x|Y, w) dx
6
3 APPROXIMATE INFERENCE
=
∫
q(x|Y) log p(Y, x|w)
q(x|Y) dx
  
Lower bound
+
∫
q(x|Y) log q(x|Y)
p(x|Y, w) dx
  
KL divergence
. (10)
In the ﬁrst equation, the term
∫
q(x|Y)dx = 1
is inserted. The second equation uses the relation
p(Y|w) = p(Y, x|w)/p(x|Y, w) based on the formula
of a posterior distribution (see Eq. 3). In the third
equation, a recognition model is inserted into the log-
arithm.
Here, the ﬁrst term obtained by the last equation,
L[q, p] ≡
∫
q(x|Y) log p(Y, x|w)
q(x|Y) dx, (11)
is called the evidence lower bound (ELBO) or vari-
ational lower bound, and plays a central role in this
article. We will simply refer to it as the lower bound
in this article. The negative value of the lower bound
is called the variational free energy, which is the ob-
jective function in the free energy principle. In gen-
eral, the lower bound L[q, p] is a functional of the
recognition model q(x|Y) and the generative model
p(Y, x|w). The second term is the KL divergence
between the recognition model (an approximate pos-
terior distribution) and the exact posterior distribu-
tion.9 In summary, the log marginal likelihood is de-
composed as follows:
log p(Y|w) = L[q, p]+KL[ q(x|Y)||p(x|Y, w)]. (12)
Note that L[q, p] is called the lower bound because
KL divergence takes a non-negative value, so the log
marginal likelihood function is bound by this func-
tion.10
log p(Y|w) ≥ L [q, p]. (13)
9The expected value of this term according to ¯ p(y) is called
the conditional KL divergence. Thus the second term is an
estimate of the conditional KL divergence.
10The lower bound L[q, p ] can also be obtained using
Jensen’s inequality as:
log p(Y|w) = log
∫
p(Y, x|w)dx
= log
∫
q(x|Y) p(Y, x|w)
q(x|Y) dx
≥
∫
q(x|Y) log p(Y, x|w)
q(x|Y) dx
≡ L [q, p ].
The lower bound can be decomposed as follows:
L[q, p] =
∫
q(x|Y) log p(Y, x|w)dx
  
Q-function
−
∫
q(x|Y) log q(x|Y)dx
  
Entropy
. (14)
Here, the ﬁrst term is called the expected complete
data log-likelihood function or Q-function:
Q[q, p] ≡
∫
q(x|Y) log p(Y, x|w)dθ. (15)
The second term is the entropy of the recognition
model.
H[q] ≡ −
∫
q(x|Y) log q(x|Y)dx (16)
Using these equations, the EM algorithm alternates
between optimizing the approximate posterior distri-
bution and optimizing the parameters [53]. The ac-
tual steps of the EM algorithm are described below.
3.2 EM algorithm
The EM algorithm alternates between two steps
called the E-step and the M-step. Figure 4 shows
a schematic representation of these steps.
E-step: In the E-step, the recognition model is
optimized to provide good approximation of the ex-
act posterior distribution while the generative model
is ﬁxed. The purpose of this step is to ensure that the
lower bound provides a tight bound on the marginal
likelihood. First, note that the parameter w of the
generative model is ﬁxed, so the marginal likelihood
log p(Y|w) is constant. In this case, according to
Eq. 12, when KL divergence is reduced by approxi-
mating the recognition model to the exact posterior
distribution, the lower bound becomes correspond-
ingly larger and approaches the log marginal likeli-
hood function.
Many methods have been proposed for approxi-
mating the posterior distributions, although this ar-
ticle will not go into the details of these methods.
For example, stochastic methods include Monte Carlo
methods, and deterministic methods include Laplace
approximation (approximation by a Gaussian distri-
bution), variational approximation (mean ﬁeld ap-
proximation), and expectation propagation methods.
M-step: The aim of the M-step is to increase
the marginal likelihood by optimizing the parameter
w. In this step, the recognition model is ﬁxed. It
is presumed that optimization in the E-step results
7
3 APPROXIMATE INFERENCE
    L[q, p] = log p(Y | w) KL [q(x| Y) | p(x| Y,w)]
    L[q, p] = q(x| Y)log p(Y | x,w)dz KL [q(x| Y) | p(x| w)]
    L[q, p] = q(x| Y)log p(Y,x| w)dz + H[q(x| Y)]    
L[q, p] q(x| Y)log p(Y,x| w)
q(x| Y) dx
accuracy complexity
entropyexpected complete data log-likelihood
marginal log-likelihood KL between approx. and exact posterior 
Inference
(E-step)
Learning 
(M-step)
fixed optimized 
fixed optimized 
   L[q, p]
0 
 KL 
   l(w)
 KL 
   L[q, p]
M-step 
fixed 
   L[q, p]
 KL 
0 
 KL 
   L[q, p]
E-step 
Lower bound 
log p(Y | w) log p(Y | w)
optimized optimized 
Figure 4: The EM algorithm
in the best approximation of the exact posterior dis-
tribution p(x|Y, w) within the range of the model
assumed for the recognition model q(x|Y), therefore
the KL divergence of these two distributions (second
term of Eq. 12) is minimized. In this case, the KL di-
vergence is expected to increase due to the change in
the exact posterior distribution caused by change of
the parameter w, which then contributes to increas-
ing the marginal likelihood. Therefore, the change
of parameters can be performed so as to increase the
ﬁrst term of Eq. 12, i.e., the lower bound. Inciden-
tally, we have already seen that the lower bound can
be decomposed as follows:
L[q, p] = Q[q, p] + H[q]. (17)
Since the recognition model is now ﬁxed, only the Q-
function of the ﬁrst term is dependent on the param-
eters. Therefore, in the M-step, we use parameters
that maximize the Q-function.
In summary, when using E-step to optimize the
approximate posterior distribution, the lower bound
increases because the KL-divergence decreases with
constant marginal likelihood. In M-step, the lower
bound is increased by selecting parameters that max-
imize the Q-function. Performing E-step and M-step
alternately therefore causes the lower bound to in-
crease monotonically, which is expected to increase
the marginal likelihood. This justiﬁes changing the
objective function from the marginal likelihood to the
lower bound L[q, p]. As in the studies by Friston et
al., we may construct a theory that starts from min-
imization of the variational free energy that is the
negative lower bound.
3.3 Adaptation of the generative
model and behavior
The above description shows how approximate infer-
ence is achieved by the EM algorithm. In the fol-
lowing, we will look into details of the learning step,
considering cases where actions are included in this
reasoning. By rearranging the expressions discussed
above in terms of the lower bound L[q, p], we ob-
tain the following equations for E-step and M-step,
8
4 INFORMATION-THEORETIC APPROACHES TO ADAPTATION
respectively:
L[q, p] = log p(Y|w) − KL[q(x|Y)|p(x|Y, w)],
L[q, p] =
∫
q(x|Y) log p(Y, x|w)dx + H[q].
(18)
Furthermore, the lower bound can be decomposed as
follows:
L[q, p] =
∫
q(x|Y) log p(Y, x|w)
q(x|Y) dx
=
∫
q(x|Y) log p(Y|x, w)p(x|w)
q(x|Y) dx
=
∫
q(x|Y) log p(Y|x, w)dx
  
Accuracy
−
∫
q(x|Y) log q(x|Y)
p(x|w) dx
  
Complexity
(19)
Here, the ﬁrst term on the right side of the last equa-
tion is an expectation of the log observation model
by the recognition model, which represents the aver-
age goodness-of-ﬁt of the observation model. This is
sometimes referred to as the accuracy of the model.
The second term is the KL divergence of the approx-
imate posterior distribution and prior distribution,
which is sometimes referred to as the complexity of
the model. We discuss these two terms below.
First, let us consider the accuracy (the ﬁrst term).
In M-step, the parameters of the observation model
are optimized by maximizing the accuracy. For ex-
ample, parameters such as those of the basis func-
tions are optimized, thereby improving the explana-
tory accuracy of the data. Not only that, Friston et
al. hypothesized that actions are also chosen as to
maximize the accuracy [22, 39, 40, 36]. The inference
problem in which action is involved is studied as an
active inference problem, and it often refers to the
selection of action according to maximizing the lower
bound (or minimizing the variational free energy). In
this case, the behavior of an agent is generated ac-
cording to its generative model [41, 40, 42]. Actions
selected by this principle are expected to alter the
outside world so that it is more predictable for the
agent. Note that when we introduce actions as a way
of changing the data generation mechanism, the data
distribution ¯p(x) – which has so far remained ﬁxed –
is forced to change. However, organisms or agents are
not able to access the true data distribution. There-
fore, the only way to respond is by updating the gen-
erative model, and it is necessary for the generative
model to include statements of how the generation of
data is aﬀected by actions.
Now let’s consider the complexity (the second
term). Complexity is minimized by bringing the prior
distribution closer to the posterior distribution. In
particular, when the approximate posterior distribu-
tion is an exact posterior distribution, we have the
relation
∫
p(x|Y, w) log p(x|Y, w)
p(x|w) dx
= KL[ p(x|Y, w)||p(x|w)], (20)
which is called a Bayesian surprise [54]. This value
quantiﬁes how much the recognition model changes
when the data Y is provided, given the parameter w.
The EM algorithm therefore performs approximate
minimization of the Bayesian surprise. This means
that surprises in the data are eliminated by adapting
a prior distribution as a result of learning.
This process can represent the adaptability of or-
ganisms whereby the brain adapts to environmen-
tal factors or contexts, and incorporates them into
its internal model. In particular, it was shown that
the distance between the spontaneous ﬁring activity
and stimulus response activity of the visual cortex
of ferrets decreases during the ﬁrst 5 months of life
[43]. The sampling hypothesis suggests that this ob-
servation manifests that the prior distribution and
posterior distribution become closer due to learning
by identifying spontaneous activity as samples from
the prior distribution and stimulus-evoked activity as
samples from the posterior distribution. The next
section explains the adaptation process more gener-
ally from the viewpoint of information theory.
4 Information-theoretic ap-
proaches to adaptation
In this section, we treat the learning process as
communication through an information channel, and
look at the relationship between the maximization of
marginal likelihood in the previous section and max-
imization of information-theoretic quantities. Based
on this, we will examine the relationship between the
classical theory of perception [2, 5, 55] that is based
on the information maximization principle (Infomax
principle) and the approach with a generative model
in the previous section. This can deepen the under-
standing of the relationship between the hypothesis
of the brain as a nonlinear computing machine and
that of the brain as an inference machine. See also
[30, 20, 56, 26] for the relationship between the gen-
erative model and neural networks, and [23, 24, 57]
for the relationship between the free energy principle
and information theory.
9
4 INFORMATION-THEORETIC APPROACHES TO ADAPTATION
4.1 Maximization of mutual informa-
tion and optimization of the gen-
erative model
In this section, data Y is regarded as the input,
and the parameter W of the generative model is re-
garded as the output. Here, we want to maximize
the amount of mutual information between the ran-
dom variable Y (data) and the model parameter W.
The mutual information can be expressed in terms of
entropy and conditional entropy as follows:
I(Y; W) = H(Y) − H(Y|W)
= H(Y) + EYEW|Y log p(Y|W). (21)
The ﬁrst term on the right side in Eq. 21 is the en-
tropy of the data distribution H(Y) ≡ − EY log ¯p(Y),
and is constant when the data distribution is ﬁxed.
In this section, we will not consider changes to the
data distribution caused by actions. Let us consider
the second term on the right side. In the previous
section, we considered the reverse process from the
output W to the input Y as the data generation pro-
cess in the internal model. Using latent variables, this
path was obtained as the marginal distribution:
p(y|w) =
∫
p(y|x, w)p(x|w)dx. (22)
The second term in Eq. 21 is the expectation of log
of this function with respect to the joint distribution,
p(y, w) = ¯p(y)p(w|y). The hierarchical model of the
previous section possesses the parameter w, which
was learned from the data. This is a point estimate,
which means that the distribution of w for a given
set of data is given by p(w|Y) = δ(w − W⋆), where
δ(·) is the Dirac delta function. Note that W⋆ is a
function of Y. In this way, the second term on the
right side of Eq. 21 becomes EY log p(Y|W⋆). Fur-
ther, by replacing the expected value of Y with the
empirical distribution, we can use the data Y to ob-
tain an estimate of the second term, which is the log
marginal likelihood function. This means that we can
expect the mutual information between Y and W to
be maximized by adopting the maximum likelihood
estimate W∗ as a point estimation.
We note that, using a hidden state X, the mutual
information between Y and W is generally decom-
posed as
I(Y; W) = I(Y; X, W) − I(X; Y|W). (23)
See the Venn diagram (Fig. 5) for the visualization
of this decomposition. From this decomposition, it
is found that the mutual information I(Y; W) in the
= -
Y
W
I(Y;W) 
I(X;Y|W)I(Y;X,W) 
X
I(Y;W) 
Figure 5: Venn diagram and decomposition of mutual
information
sensory data Y and the parameter W can be decom-
posed into two factors: the mutual information rep-
resenting the encoding of sensory data Y by latent
variables X and parameters W (ﬁrst term), and the
conditional mutual information between the latent
variables X and data Y, given prerequisite knowl-
edge acquired by the parameter W (second term). 11
Below, we revisit the inference algorithm of the pre-
vious section, and clarify that they aim at optimizing
these information-theoretic quantities.
Based on Eq. 19, the marginal likelihood can be
expressed as
log p(Y|W⋆) = EX|Y,W⋆ log p(Y|X, W⋆)
− KL[p(x|Y, W⋆)∥p(x|W⋆)],
(24)
where it is assumed that an exact posterior distribu-
tion has been obtained. The ﬁrst term represents
the accuracy, and the second term represents the
11In this case, only the cost of encoding is considered. Tishby
et al.’s information bottleneck theory, which builds on Sha n-
non’s rate distortion theory, plays a complementary role in the
theory of information maximization under the constraint th at
the cost of decoding is given for a particular generation mod el.
10
4 INFORMATION-THEORETIC APPROACHES TO ADAPTATION
Bayesian surprise. We will conﬁrm that maximiza-
tion of the accuracy and minimization of the Bayesian
surprise correspond to maximization of the ﬁrst term
and minimization of the second term in Eq. 23, re-
spectively.
First we note that EW|Y represents expecta-
tion by the posterior distribution of W, namely
p(w|Y) = δ(w − W⋆). Therefore, the log marginal
likelihood can be expressed as log p(Y|W⋆) =
EW|Y log p(Y|W). By taking the expectation by
EY, we can see that the left side of Eq. 24 corre-
sponds to
EYEW|Y log p(Y|W) = −H(Y|W). (25)
Similarly, expectation of the ﬁrst term on the right
side by data distribution is
EYEW|YEX|Y,W log p(Y|X, W) = −H(Y|X, W).
(26)
Second, the expectation of the Bayesian surprise (the
second term on the right side) by the data distribu-
tion becomes the conditional mutual information:
EYEW|YKL[p(x|Y, W)∥p(x|W)] = I(X; Y|W).
(27)
Note that the mutual information X and Y available
under Z is computed from the KL divergence: 12
I(X; Y |Z)
=
∫ ∫∫
p(x, y, z ) log p(x, y|z)
p(x|z)p(y|z)dxdydz
= EX EZ|X KL[p(y|X, Z )||p(y|Z)] (29)
The mutual information for discrete random variables
can also be deﬁned in the same way by replacing the
integral with a summation. Eq. 27 is an extension
of this formula to multivariate variables. In other
words, Bayesian surprise is an estimate of conditional
mutual information. Taken together, expectation of
Eq. 24 by EY is simpliﬁed to
−H(Y|W) = −H(Y|X, W) − I(X; Y|W). (30)
12In general, the mutual information of continuous random
variables X and Y is given by
I(X; Y ) =
∫∫
p(x, y ) log p(x, y )
p(x)p(y) dxdy
=
∫∫
p(x)p(y|x) log p(y|x)
p(y) dxdy
= EX KL[p(y|X)||p(y)] (28)
By adding the entropy of the data H(Y) on both
sides of Eq. 30, we obtain Eq. 23. 13
By comparing Eq. 24 with Eq. 23, we can clearly
understand objectives that optimization of each term
in Eq. 24 aims at during the learning. First, while
keeping in mind the existence of an unknown constant
H(Y), maximizing the ﬁrst term on the right side of
Eq. 24 (accuracy) maximizes the (estimated) mutual
information related to the encoding from Y to X and
W. Second, minimization of complexity/Bayesian
surprise in the second term of Eq. 24 minimizes the
(estimated) mutual information between the neural
activity X and the data Y, conditional on the param-
eter W of the generative model. This means that the
information of the input data Y is absorbed during
the process of learning the parameter W, and as a
result, the neural activity X no longer has any infor-
mation about the input data Y other than what is
held by W.
4.2 Infomax principle for optimiza-
tion of nonlinear networks
In this subsection, we review the classical theory of
sensory perception. Based on this, we will consider
its relation to the approach based on the generative
model. Here we assume noiseless communication be-
tween the input Y and the output X, and that they
have the same dimension. For the noiseless channel,
we can represent X as a nonlinear function of Y, us-
ing x = f(y; ϕ ), for which a neural network can be
used. Here ϕ are the parameters that deﬁne the non-
linear function (e.g., weights of a neural network),
and comprise receptive ﬁelds of the neurons.
The adaptation of the neural activity X to the ex-
ternal input Y by maximizing the mutual information
I(Y; X) is known as the infomax principle [5]. This
mutual information is expressed in terms of entropy
as
I(Y; X) = H(X) − H(X|Y). (32)
In a noiseless channel, the second term of Eq. 32 is not
dependent on the nonlinear function f(y; ϕ ), so it can
13 Given the hierarchical model of the brain (Eq. 4), Eq. 23
can be further written as
I(Y; φ , λ ) = I(Y; X, φ ) − I(X; Y, φ |λ ). (31)
This is obtained as follows. Under the assumption of the hi-
erarchical model shown in Fig. 3, we obtain I(Y; X, φ , λ ) =
I(Y; X, φ ) since X is conditionally independent of λ . Using
the general decomposition rule of Eq. 23 on the distribution s
conditional on λ , we have I(Y; X|φ , λ ) = I(X; Y, φ |λ ) −
I(X; φ |λ ) = I(X; Y, φ |λ ). Here we used I(X; φ |λ ) = 0 be-
cause X and φ are independent if Y is marginalized.
11
4 INFORMATION-THEORETIC APPROACHES TO ADAPTATION
be ignored in optimization. 14 Therefore, maximizing
the mutual information is equivalent to maximizing
the entropy of the output X.
The method of optimizing parameter ϕ based on
this principle is known as independent component
analysis (ICA) used for blind signal separation[55].
That is, the parameter ϕ is updated using the fol-
lowing gradient,
∂I (Y; X)
∂ϕ = ∂H (X)
∂ϕ . (33)
Here, the entropy of X due to the stochastic data Y
is given by
H(X) = H(Y) + EY log det ∂f(y; ϕ )
∂y
⏐
⏐
⏐
⏐
y=Y
, (34)
because p(y) = p(x)
⏐
⏐
⏐∂x
∂y
⏐
⏐
⏐, where ∂x
∂y = ∂f(y;ϕ )
∂y is a
Jacobi matrix used for the change of variables, and
|·| = det · denotes a determinant. Using Eqs. 33 and
34, and replacing the expectation of data with a sam-
ple Y, the learning rule is obtained as
dϕ
dt ∝ ∂
∂ϕ log det ∂f(y; ϕ )
∂y
⏐
⏐
⏐
⏐
y=Y
. (35)
The mutual information maximization based on
maximizing the output entropy follows the framework
of the eﬃcient coding hypothesis proposed by Ho-
race Barlow [2, 3]. On the assumption of a noiseless
channel, Barlow proposed the principle of maximiz-
ing entropy of X (i.e., activity of neurons) as a goal
of encoding sensory data such as tactile and vision.
The eﬃcient coding hypothesis is also called the re-
dundancy reduction hypothesis since it involves elim-
inating redundancy from data as shown below.
To explain this, let’s see that the multivariate en-
tropy can be decomposed as follows.
H(X) = −I(X1; X2; . . . ; Xd) +
d∑
i=1
H(Xi). (36)
Here, the ﬁrst term on the right side is the multivari-
ate mutual information, deﬁned as I(X1; . . . ; Xd) =
KL[p(X||Π d
i=1p(Xi)]. Two conclusions can be de-
rived from this equation. From the ﬁrst term, we
can see that in order to increase the mutual informa-
tion at the inputs and outputs, it is better to have a
smaller quantity of mutual information between the
outputs. That is, the parameter ϕ should be ad-
justed so that the output variables become indepen-
dent random variables. This sort of encoding, which
14The conditional entropy is 0 for discrete distributions, or
negative inﬁnity for continuous distributions if the chann el is
noiseless.
A nonlinear function 
Stimulus distribution
Adaptation
Activity rate Density 
Stimulus value
Figure 6: Nonlinear functions and adaptation
aggregates external signals and converts them into
independent representations, is sometimes called fac-
torial coding.
Next, from the second term, it can be seen that in
order to increase the mutual information between the
input and output, the entropy of each random vari-
able of the output should be increased. Since the en-
tropy is maximized when the random variables have a
uniform distribution, the parameter ϕ should be ad-
justed so as to obtain a uniform output from the non-
linear function X = f(Y; ϕ ). For example, it is the
most appropriate if the activity of individual neurons
is covered uniformly over its dynamic range. Consider
a one-dimensional sensory input ¯p(y) for simplicity. If
we wish to construct a uniform distribution that has
an upper limit as an output X = f(Y ; ϕ), we should
use the nonlinear function f(y; ϕ) =
∫ y
−∞ ¯p(y′)dy′ or
a constant multiple thereof. This is because substi-
tuting a random variable Y whose distribution is ¯p(y)
to its own cumulative distribution function results
in a uniform distribution of values over the range
[0, 1].15 This nonlinear transformation is sometimes
called histogram equalization.
According to the eﬃcient coding hypothesis, or-
ganisms are thought to adapt its nonlinear activation
15If X is a random variable following the probability distri-
bution function FX (x) = ∫ x
−∞ fX (s)ds, then U = FX (X) has
a uniform distribution. It is also used as a way of creating
arbitrary sample distributions from uniform random number s
(inverse function method): X = F −1
X (U)
12
4 INFORMATION-THEORETIC APPROACHES TO ADAPTATION
functions of neurons to the distribution function of
the input data so they can use the output indepen-
dently and uniformly over its dynamic range. In par-
ticular, horizontal movements or multiplications are
added to basic nonlinear input/output functions to
rapidly adapt to the environment. This process is
called gain control (see Fig. 6). In insect retinal and
olfactory nerve cells [7, 58] and mammalian visual
neurons [59, 60, 61], the gain control is performed
to adapt to changes in the environment. Further-
more, the gain control explains higher-level cognitive
functions of a brain such as attention and coordinate
transformation of viewpoint [47, 62, 63, 64].
While the gain control allows fast adaptation to
changing environment, nonlinear functions them-
selves must be acquired from the data during learn-
ing processes in a longer time-scale. To capture the
non-Gaussian nature of sensory inputs, it is neces-
sary to adaptively generate a nonlinear function that
matches up the higher order statistics of the distribu-
tion. For the distribution whose dimension is higher
than two, the higher-order correlations (higher-order
statistics among the variables) in the distribution
must be captured. In this way, it is hypothesized
that nonlinear functions that are expressed through
the nonlinearity of neurons (nonlinear summation
of synaptic inputs at dendrites, ﬁring characteristics
based on threshold mechanisms, and network dynam-
ics) are adapted to the characteristics of the distribu-
tion of input data [65, 11].
Finally, we examine how the above arguments on
the infomax principle can be generalized to the ap-
proach with the generative model. We investigated
optimization of the nonlinear function x = f(y; ϕ ),
assuming a noiseless channel and equal dimensions
for y and x. For simplicity, here we assume that
the nonlinear function is bijective functions (one-to-
one functions), and deﬁne the inverse function as
y = g(x; φ ). The parameter φ represents how the
data is constructed from the neural activity, and is
called the basis function or projection ﬁeld [56]. This
is in contrast to the parameter ϕ that comprises the
receptive ﬁeld of neurons, which explains neural ac-
tivity in terms of the sensory inputs. 16
For the noiseless channel, the observation model
is written as p(Y|x, φ ) = δ(Y − g(x; φ )). Then the
16If the functions and parameters are linear ( Y = Xϕ and
X = Yφ ), we obtain the relation, ϕφ = I.
marginal likelihood function is written as 17
p(Y|w) =
∫
δ(Y − g(x; φ ))π(x|λ )dx
= π(X) · det ∂f(y; ϕ )
∂y
⏐
⏐
⏐
⏐
y=Y
. (38)
Here the prior distribution for X is denoted by π(·)
to avoid confusion. In the right hand side of this
equation, the sensory data Y is projected to the neu-
ral activity X by the nonlinear function x = f(y; ϕ ),
and the likelihood of the sensory data Y is now eval-
uated using the neural activity X with respect to its
prior distribution. Under the maximum likelihood
principle, learning of the nonlinear function is per-
formed so that the neural activity X ﬁts to the prior
distribution. At the same time, the parameter λ of
the prior distribution is optimized to ﬁt the neural
activity. These learning processes correspond to opti-
mization of the observation model and the prior dis-
tribution discussed in the previous section, respec-
tively. Note that if we assume a ﬂat prior, we obtain
the same gradient in Eq. 35 for the infomax princi-
ple from the principle of maximizing the log marginal
likelihood. This indicates that, in the approach with
the generative model, the optimal nonlinear function
x = f(y; ϕ ) is modulated by the prior distribution,
which can be interpreted as the gain modulation.
One can arrive at the generative model investigated
in the previous section by generalizing the above de-
terministic observation model to noisy observation
models togather with non-bijective nonlinear func-
tions. Often the mean µ of the observation model
is modeled as µ = g(Xφ ), where φ plays the role
of linear basis functions. Such a nonlinear function
g(·) is called a link function in statistics. Olshausen
& Filed introduced a sparse prior distribution to the
latent variables of the Gaussian observation model
with a linear link function, and make the basis func-
tions of the observation model learned from natural
images. In this model, the dimension of latent vari-
ables Y and corresponding basis functions are much
larger than the number of pixels in Y (overcomplete
model). They then found that the model learns high-
17Here, with the change of a variable y = g(x; φ ), we have
p(Y|w) =
∫
δ(Y − g(x; φ ))π (x|λ )dx
=
∫
δ(Y − y)π (g−1(y; φ )|λ )
⏐
⏐
⏐
⏐
∂x
∂y
⏐
⏐
⏐
⏐dy
= π (g−1(Y; φ )|λ ) det ∂x
∂y
⏐
⏐
⏐
⏐
y=Y
= π (f(Y; ϕ )|λ ) ·det ∂f(y; ϕ )
∂y
⏐
⏐
⏐
⏐
y=Y
. (37)
13
5 THERMODYNAMICS OF ADAPTATION
dimensional correlation structures such as lines and
edges that often appear in natural images [8, 9].
An alternative approach to augment the noiseless
model to a noisy model is to directly construct the
approximate posterior distribution q(x|Y) by adding
noise to the nonlinear function x = f(y; ϕ ). In
the original variational auto-encoder model, not only
the mean but variance of the approximated Gaus-
sian posterior distribution is learned by neural net-
works [52]. Contrary to the overcomplete representa-
tion, the dimension of the latent variable X is smaller
than that of the observation Y; therefore the poste-
rior called encoder maps the data into latent vari-
ables in a smaller subspace whereas the observation
model called decoder maps the latent variables to the
data, which has a larger dimension. A scheme diﬀer-
ent from the EM algorithm was developed to learn
the posterior and the generative model simultane-
ously using the neural networks. Common to all ap-
proaches, it is known that neither of these approaches
can fully eliminate redundancy in simple nonlinear
functions or non-Gaussian distributions [11, 10, 66],
so eﬀorts are being made to grasp the input correla-
tion structure with deeply hierarchical models.
5 Thermodynamics of adapta-
tion
In Section 3, we introduced the EM algorithm as
a learning method for hierarchical models based on
Helmholtz’s epistemology. This approach can be ex-
tended to variational inference using variational ap-
proximation methods. The variational inference is
a technique derived from mean ﬁeld approximation
in statistical physics that provides approximate solu-
tions to physical models of materials involving com-
plex interactions among their elements (e.g., Ising
model). Therefore its mathematical framework is
closely related to statistical physics and thermody-
namics.18 However, it is not always clear how the
laws of thermodynamics relate to recognition and
learning.
In this section, we will look at the learning process
from a thermodynamic viewpoint based on the prin-
ciple of entropy maximization, and we will discuss dy-
namics of learning by deﬁning energy and free energy
of a recognition model using an exponential family
distribution. Based on this, we explain the relation
18As a physiologist, Helmholtz himself developed a theory of
visual perception (the Young–Helmholtz theory of trichrom atic
color vision), and as a physicist, he contributed to establi shing
the ﬁrst law of thermodynamics and the theory of free energy
in chemical reactions.
between causal statistical learning and the second law
of thermodynamics. A similar treatment to the neu-
ral dynamics presented in this section can be found in
the thermodynamic analysis of neural population ac-
tivity in Shimazaki 2015, 2018 [67, 68] to which the
reader is referred. Methods for calculating thermo-
dynamic quantities of neural populations from actual
recordings of time-series neural spike data, together
with the results of these methods, can be found in
Tkaˇ cik et al. 2015 [69], Donner et al. 2017 and Gau-
dreault et al. 2018 [70, 71].
5.1 Maximum entropy model
Before entering the discussion, we start with a de-
scription of the terminology. Based on Eq. 14, the
lower bound L[q, p] with a recognition model q(x|Y)
can be expressed as follows:
L[q, p] = S −
∫
q(x|Y)[− log p(Y, x|w)]dx. (39)
In this section, the Shannon entropy of the recogni-
tion model is represented by S (S ≡ H[q(x|Y)]). The
second term is the negative of the Q-function ( −Q),
which is as follows:
∫
q(x|Y)[− log p(Y, x|w)]dx = E. (40)
As shown later, E is a quantity that can be referred to
as the energy of the recognition model. These terms
are used to bound the negative marginal likelihood
function as
− log p(Y|w) ≤ E − S ≡ F, (41)
where, in statistical physics, the left side is called
free energy and the right side F = E − S is called
the variational free energy. The variational free en-
ergy is the negative lower bound ( F = −L[q, p]), and
the problem of maximizing the lower bound that was
discussed in the section of approximate inference can
be substituted with the problem of minimizing the
variational free energy. In the following, a thermody-
namic approach to a recognition model will be intro-
duced to better understand dynamics of the entropy,
energy, and free energy (not variational free energy)
for a recognition model, induced by learning.
For this goal, we will construct a recognition model
from the maximum entropy principle. Consider the
problem of maximizing the entropy of a recognition
model q(x|Y) given an expected value of a genera-
tive model. This is an entropy maximization prob-
lem with constraints, which can be solved by the
method of Lagrange multipliers. In this method, the
14
5 THERMODYNAMICS OF ADAPTATION
constrained maximization problem is replaced with
maximization of a new function that includes the con-
straints. More speciﬁcally, we maximize the following
Lagrange function (Lagrangian):
˜Lβ [q] = −
∫
q(x|Y) log q(x|Y)dx
− β
{
−
∫
q(x|Y) log p(Y, x|w)dx − E
}
+ a
{ ∫
q(x|Y)dx − 1
}
. (42)
where β, and a are Lagrange multipliers. The last
term is a constraint due to the fact that the recogni-
tion model is a density or probability function. Ac-
cording to the variational principle, we need to obtain
the distribution by maximizing Eq. 42. The variation
with respect to the distribution q(x|Y) is given by
δ ˜Lβ [q]
δq =
∫
δq [−1 − log q(x|Y)
+ βlog p(Y, x|w) + a]dx. (43)
Thus it is found that the distribution obeys the fol-
lowing exponential family distribution:
q(x|Y) = 1
Zβ (Y) e−β{− log p(Y,x|w)}. (44)
The term Zβ (Y)(= ea−1) is called a normalization
term or partition function, and is given by
Zβ (Y) =
∫
e−β{− log p(Y,x|w)}dx. (45)
The Lagrange multiplier β is obtained at the maxima
of the Lagrangian ( ∂ ˜Lβ [q]
∂β = 0), which is given by
Eq. 40. That is, β is chosen to satisfy the constraints
of Eq. 40. Note that in this section, we consider the
set of parameters as w = {ϕ , ω } by excluding β , and
separately introduce β as a Lagrange multiplier that
is a parameter of the posterior distribution.
The maximum entropy method is a method that
maximizes the entropy of a distribution while apply-
ing speciﬁc constraints. It can be used to obtain dis-
tributions by eliminating statistical structures other
than constraints. As seen above, its distribution is
expressed by an exponential family distribution from
the deﬁnition of entropy. In statistical physics, this
distribution is called the Gibbs or Boltzmann dis-
tribution, the exponent of the exponential function
H(x) ≡ − log p(Y, x|w) is called the Hamiltonian,
and the expected value of the Hamiltonian is called
the energy as given by Eq. 40. Also, β is called the in-
verse temperature, and T ≡ 1/β is called the temper-
ature. When β = 1, the distribution that maximizes
the entropy according to Eq. 44 becomes
qβ=1(x|Y) = p(Y, x|w)
Zβ=1(Y) = p(x|Y, w), (46)
which is an exact posterior distribution.
5.2 Law of conservation of entropy for
a recognition model
The generative model can be divided into an observa-
tion model and a prior distribution. To manipulate
the entropy of the recognition model more precisely
than in the above formula, we now consider a recog-
nition model, for which the constraints are stated in
more details. More speciﬁcally, we search for the
recognition model whose entropy is maximized under
the following constraints:
⟨−log p(x|ω )⟩ = U, (47)
⟨−log p(Y|x, φ )⟩ = V. (48)
where ⟨·⟩ is the expected value of the recognition
model. In this case, the Lagrangian can be written
as follows:
˜Lβ,α[q] = −
∫
q(x|Y) log q(x|Y)dx
− β
{
−
∫
q(x|Y) log p(x|ω )dx − U
}
− α
{
−
∫
q(x|Y) log p(Y|x, φ )dx − V
}
+ a
{ ∫
q(x|Y)dx − 1
}
, (49)
where β, α, and a are Lagrange multipliers. 19 By
examining variations in the recognition model in the
same way as before, we obtain
δ ˜Lβ,α[q]
δq =
∫
δq [−1 − log q(x|Y)
+ βlog p(x|ω ) + αlog p(Y|x, φ ) + a]dx. (50)
Consequently, the recognition model obeys the fol-
lowing exponential family distribution:
q(x|Y) = 1
Zβ,α(Y) eβ log p(x|ω )+α log p(Y|x,φ ). (51)
The partition function is given by
Zβ,α(Y) =
∫
eβ log p(x|ω )+α log p(Y|x,φ )dx. (52)
19In the earlier equations, β was a Lagrange multiplier for
the generative model. However, it is now a Lagrange multipli er
for the prior distribution. Therefore, β is no longer a parameter
that controls energy. If we introduce α = βf , β controls the
energy.
15
5 THERMODYNAMICS OF ADAPTATION
The Lagrange multipliers β and α are chosen to sat-
isfy the constraints. When β = 1 and α = 1, the
recognition model becomes an exact posterior distri-
bution.
The recognition model of Eq. 51 can be regarded
as an exponential family distribution where −β and
−α are canonical parameters, and − log p(x|ω ) and
− log p(Y|x, φ ) are features. We can therefore derive
a number of important relations. First, the logarithm
of the partition function forms a cumulant generating
function. As a result, the ﬁrst derivative of the log
partition function by the canonical parameter gives
the expected value of the feature by the recognition
model. That is,
∂ log Zβ,α(Y)
∂(−β) = ⟨−log p(x|ω )⟩
∂ log Zβ,α(Y)
∂(−α) = ⟨−log p(Y|x, φ )⟩. (53)
Here, by deﬁning a function
G(β, α) ≡ − log Zβ,α(Y), (54)
they can be simpliﬁed to
∂G(β, α)
∂β = U, ∂G(β, α)
∂α = V. (55)
Next, the entropy of the recognition model can be
calculated as
S(U, V ) = ⟨−log q(x|Y)⟩
= β⟨−log p(x|ω )⟩+ α⟨−log p(Y|x, φ )⟩
+ log Zβ,α (Y)
= βU + αV − G (β, α). (56)
The combination of Eq. 55 and Eq. 56 forms a Legen-
dre transformation that transforms G(β, α) (a func-
tion of β and α) into the entropy S(U, V ) which is a
function of U and V .20
Since there is no loss of information by the Leg-
endre transformation deﬁned by Eqs. 55 and 56, it
is always possible to return to the original function
by using the inverse Legendre transformation. The
inverse Legendre transformation is given by
G(β, α) = βU + αV − S(U, V ). (57)
20In general, when a smooth convex function f(x) is ex-
pressed in terms of a new function f∗(p) = max x{px − f(x)},
this is called a Legendre transformation. However, since th e
derivative is zero at the maximum value, p = f′(x), and thus p
represents the slope of the function f(x). Hence, strictly speak-
ing, Eq. 56 multipled by − 1 represents the Legendre transfor-
mation from −G (β, α ) to − S(U, V )
and
∂S
∂U = β, ∂S
∂V = α. (58)
This transformation converts the entropy S(U, V ) (a
function of U and V ) into G(β, α) (a function of β
and α). When β = 1 and α = 1, an exact posterior
distribution is obtained, in which case the marginal
likelihood and negative log partition function coin-
cide: log p(Y|w) = −G(1, 1).21
According to Eq. 58, the total derivative of the
entropy of the recognition model,
dS =
( ∂S
∂U
)
V
dU +
( ∂S
∂V
)
U
dV, (60)
can be expressed as follows:
dS = βdU + αdV. (61)
This formula represents the contribution of the prior
distribution and the observation model (input stim-
uli) to the change of entropy in the recognition model,
and dictates the law of conservation of entropy. In
physics, Eq. 61 is called the ﬁrst law of thermodynam-
ics (law of conservation of energy). 22 Since entropy
is a state variable that is determined for a particu-
lar recognition model, the entropy change on the left
side of Eq. 61 is expressed as a diﬀerence in state
variables at two close recognition models. However,
the terms βdU and αdV on the right side depend on
the integration path since β and α are functions of U
and V .
The parameters β and α of the approximate poste-
rior distribution represent contributions of the prior
distribution and the likelihood function when con-
structing the recognition model. This recognition
model becomes an exact posterior distribution when
β = 1 and α = 1. It is expected that there ex-
ists neural dynamics that progressively approaches
the optimal state for the Bayesian inference. Thus
it is important to consider the dynamics of β and α
when we hypothesize that the Bayesian inference is
implemented by neural dynamics in the brain.
21In general, the relationship with the lower bound is ex-
pressed as
L[q, p ] = −G (β, α ) + ( β − 1)U + (α − 1)V. (59)
22The ﬁrst law of thermodynamics T dS = dU + fdV is ob-
tained from Eq. 61, using the temperature T = 1 /β and force
f = α/β . In this case dU is called the internal energy, which
is a state variable. On the other hand, the terms d¯Q = T dS
and d¯W = fdV , which are called heat and work, are repre-
sented using a path-dependent incomplete derivative d¯ . Using
these terms, the ﬁrst law of thermodynamics is also written a s
d¯Q = dU + d¯W .
16
5 THERMODYNAMICS OF ADAPTATION
For example, a likely scenario for the formation of
a posterior distribution by a neural network is as fol-
lows: after nerve cells have ﬁred in response to the
presentation of a stimulus, the neural activity is mod-
ulated by feedback input (including information cor-
responding to the prior knowledge), whereby the ob-
servation and prior knowledge are fused. In [67, 68],
the spontaneous ﬁring of neurons corresponding to
the prior distribution and the ﬁring activity of neu-
rons induced by a stimulus are represented by expo-
nential family distributions, to which we apply the
thermodynamic formulation introduced in this sec-
tion. In particular, it was shown that, when stimu-
lus response is modulated with a time delay due to
feedback/recurrent input, neural dynamics forms an
information-theoretic cycle (an analogue of heat en-
gine, termed neural engine). 23 Many studies have
shown that the modulation of late components of
stimulus response is related to attention, perceptual
experience, short term memory, and subjective re-
ward value [72, 73, 47, 44, 74, 45, 48]. This approach
allows us to quantify higher-order brain functions by
measuring active portion of the computation as en-
tropy emission related to the modulation of stimulus
response.
5.3 Learning and the principle of in-
creasing entropy
Several thermodynamic equations and the law of con-
servation of entropy have been concisely obtained by
adopting the maximum entropy model as a recogni-
tion model. Let’s see what happens in the learning
process of this model. In the discussion so far, the
parameter w = {φ , ω } has been ﬁxed without con-
sidering learning, although we discussed dynamics to
construct the optimal recognition model by chang-
ing β and α. Let us admit that parameter w is also
optimized by learning, and assume that the learning
dynamics also follows the maximum entropy princi-
ple. That is, the learning plays a role of another
factor besides β and α (or U and V ) that change the
entropy of the recognition model, and this factor al-
ways increase entropy. More speciﬁcally, the law of
increasing entropy can be derived as a statistical law
23In these articles, thermodynamic analysis of a neural pop-
ulation was proposed by expressing spontaneous/backgroun d
activity and stimulus-related activity of neurons by an exp o-
nential family distribution. In particular, if the level of the
background activity is changed with a time delay due to feed-
back inputs, the response of neurons to a stimulus undergo ga in
control. The dynamics of such a delayed gain-control of the
stimulus response turns out to be analogous to a heat engine
of thermodynamics. This response cycle preserve informati on
about stimuli with the presence of the delayed feedback sign al,
that would otherwise be lost.
for causal dynamics in which the forward and back-
ward processes are diﬀerent, from the ﬂuctuation the-
orem [75, 76, 77]. Assuming such causal dynamics for
learning, it is expected that
dS ≥ βdU + αdV. (62)
This is equivalent to the second law of thermodynam-
ics in physics, and it applies whenever an irreversible
process takes place. Here we examine how such a
causal learning rule relates to the optimization prin-
ciples introduced in previous sections.
Consider optimizing the parameters while keeping
β and α ﬁxed. The free energy is a convenient quan-
tity that can be used instead of entropy under such
conditions. In fact, we will see that the quantity
G(β, α) is the free energy. The total derivative of
G(β, α) that is a function of β and α becomes
dG(β, α) = d(βU + αV ) − dS
= ( Udβ + βdU ) + ( V dα + αdV ) − dS
= Udβ + V dα, (63)
where the ﬁrst law (Eq. 61) is used at the last equality.
It can also be obtained directly from the deﬁnition of
total derivative
dG(β, α) =
( ∂G
∂β
)
α
dβ +
( ∂G
∂α
)
β
dα, (64)
and from Eq. 55. G(β, α) is a thermodynamic quan-
tity that takes β and α as natural independent vari-
ables. Therefore it is particularly useful when these
independent variables are ﬁxed. In this article we
call G(β, α) the Gibbs free energy. 24 The reason why
we derive the total derivative using Eq. 63 in stead of
Eq. 64 is that it becomes clear that the following rela-
tionship holds when entropy is increased by learning:
24In thermodynamics, the dual function based on the Leg-
endre transformation of internal energy U is called thermody-
namic potential (free energy). The internal energy is given by
the formula dU = T dS + fdV , with S and V as natural inde-
pendent variables. For example, the Helmholtz free energy i s
F = U − T S, and from the relationship dF = d(U − T S) =
dU − (dT S + T dS) = fdV − SdT . Here we have V and S as
natural independent variables. This is a Legendre transfor ma-
tion. At constant temperature, dF = fdV . Hence, the work
done in an isothermal process can be expressed as a diﬀerence
of Helmholtz free energy. Similarly, the Gibbs free energy i n
thermodynamics is deﬁned as G = F + fV . This is convenient
expression to use in isothermal and isobaric processes beca use
dG = dF − (d fV + fdV ) = − SdT + V d f. However, the Gibbs
free energy G in this article is given by the relation G = βG .
When considering the recognition models of the brain and ma-
chine, the concepts of heat and work may aid understanding,
but it is not clear whether they will actually bring direct be ne-
ﬁts. Since the main focus of this article is on entropy, we hav e
regarded the Legendre transformation of entropy (rather th an
internal energy) as free energy.
17
6 SUMMARY AND PROSPECTS
dG(β, α) ≤ Udβ + V dα, (65)
by applying Eq. 62 to Eq. 63. In particular, if causal
learning occurs while β and α are ﬁxed, the Gibbs
free energy decreases,
dG(β, α) ≤ 0. (66)
That is, the following learning rule can be derived:
dw
dt = −ǫ ∂G
∂w, (67)
where w = {φ , ω }, and ǫ is a learning coeﬃcient of
the parameters. 25 Alternatively to Eq. 67, an unique
value of the learning coeﬃcient may be considered for
each parameter to account for adaptation to environ-
mental dynamics with diﬀerent time-scales.
From the above, when β and α are ﬁxed, the learn-
ing process that decreases the Gibbs free energy of
the recognition model is equivalent to the learning
that maximizes the entropy of the recognition model
(the second law of thermodynamics). In particular,
when β = 1 and α = 1, the Gibbs free energy be-
comes a negative marginal likelihood, and learning
according to the law of increasing entropy becomes
equivalent to learning according to the marginal like-
lihood maximization. Therefore, if there is a mech-
anism for forming an optimal posterior distribution
(β = 1, α = 1) as a recognition model after a stimulus
is received, and if learning is performed at this time,
we can expect the results obtained by minimizing the
Gibbs free energy are the same as those obtained by
maximizing the marginal likelihood. One may also
consider the hypothesis that actions are also selected
so as to reduce the Gibbs free energy, following the
assertions of Friston et al.
In this section, we clariﬁed the relationship be-
tween learning according to the law of increasing
entropy, learning according to the minimization of
Gibbs free energy, and the maximization of marginal
likelihood, by using a maximum entropy model to
form a recognition model. In thermodynamics, free
energy is introduced by ﬁnding a new thermodynamic
quantity having the same meaning as the law of in-
creasing entropy under certain conditions when we
need to know in which direction a phenomenon will
occur as prescribed by the second law of thermody-
namics. Changes in gases and liquids at constant
25 Here we derived learning rules from the second law of
thermodynamics or the minimization principle of free energ y,
but it is possible to decide on a speciﬁc causal learning rule
and derive the law of increasing entropy (the second law of
thermodynamics) [78, 79]. This approach to the learning pro -
cess started to be discussed with the recent development of
stochastic thermodynamics [80, 81].
temperature and constant pressure take place only
when some internal change such as a chemical re-
action takes place, and such reactions proceed in a
direction such that the Gibbs free energy decreases.
We explained how learning can be treated in the same
way in this section.
6 Summary and prospects
In this article, we examined hypotheses on adaptive
processes of organisms to their environments from
multiple viewpoints: maximizing the marginal like-
lihood (maximizing the lower bound and minimizing
the variational free energy), maximizing the mutual
information, the law of increasing entropy (the sec-
ond law of thermodynamics), and minimizing the free
energy.
One important topic which was not systematically
covered in this article is the adaptation of organisms
at diﬀerent time scales. Data from the environment
has a temporal hierarchy ranging from the formation
of context over long timescales to short-term ﬂuctu-
ations. Organisms are equipped with multiple adap-
tive mechanisms for such environmental changes. For
example, the intensity of light experienced by an or-
ganism changes with the cycle of day and night, but
also undergoes sharp changes as the organism moves
between light and shade. Animals perform multi-
ple adaptation processes to cope with the intensity
changes, starting with constriction of pupils known
as the pupillary light reﬂex, and including relatively
fast gain control performed in cells in retina [82, 59]
and primary visual cortex [61]. Furthermroe, it was
shown that visual attention of monkeys whereby the
animals change sensitivity to light contrast according
to the context of tasks is also explained by the gain
control mechanism [47]. The attentional mechanisms
in which organisms select data according to the con-
text may also be explained by the canonical adapta-
tion principle to the environment [47, 62, 63, 64, 83].
This implies that slower temporal dynamics is re-
quired for the neural dynamics to retain contextual
information occuring in a long time-scale. Indeed,
it has been reported that the intrinsic time scale of
neural activity slows down along the way from the
primary visual cortex to the prefrontal cortex [84].
Finally, the adaptation of visual stimuli to spatial
structures is a process of adaptation to data distri-
butions more slowly on a time scale corresponding to
the organism’s own development. All of these adap-
tations are thought to be performed using biophysi-
cal phenomena operating on diﬀerent time scales, in-
cluding electrical responses and intracellular signal
18
REFERENCES
propagation (from tens of milliseconds to several sec-
onds), and synaptic plasticity resulting from protein
synthesis (from tens of minutes to several hours). It
is necessary to clarify these hierarchical dynamics in
order to understand the adaptation of organisms to
the environment. 26
We touched upon these topics on the temporal hier-
archy in adaptation and learning at each section, but
not in a systematic manner. In Section 2 that intro-
duced learning, we mentioned diﬀerent sample size
for learning parameters of the models. In addition
to learning the parameters in the generative model
from multiple samples for adaptation in a long time-
scale, we also introduced a process to learn the prior
distribution for each sample to account for the short-
term adaptation. In the section of information the-
ory (Section 4), using a simple example of the noise-
less channel, we conﬁrmed that the nonlinear function
(neural network) should be adapted to the data dis-
tribution under the informax principle in both long
and short time-scales. Further, we discussed how this
nonlinear function can be mapped into the generative
model, and saw that the optimal nonlinear function
is modulated in the presence of the prior distribu-
tion. Finally, thermodynamic analysis in Section 5
formulated relative contributions of the observation
and prior to construct the recognition model, using
the weight paramters β and α. In this framework,
learning parameters of the observation model and
prior distribution in a longer time-scale was achieved
by minimization of the Gibbs free energy. Short-
term dynamics of the adaptation was discussed as
changes of the parameters β and α, which is described
analogously to a thermodynamic process. Further,
it was shown that this process works similarly to a
heat engine when the stimulus response (observation)
is modulated by feedback inputs (prior information)
via top-down or lateral connections [67, 68]. While
the issues of adapting to diﬀerent temporal scales
have been discussed in other articles, a uniﬁed the-
ory that can be brought into practice remains to be
constructed.
It can be stated that organisms are equipped with
a number of adaptive mechanisms to environments
composed of spatial and temporal hierarchies by uti-
lizing their biophysical phenomena with various time-
scales, in order to maintain information-theoretic bal-
ance with the environment. With active inference
that includes behavior, this balance is stabilized be-
cause the organisms build more predictable environ-
ments by use of the actions. By investigating these
26The method of extracting features based on diﬀerences in
time scales is called slow feature analysis, and has been pro -
posed as one of the brain’s guiding principles [85, 86, 87].
adaptive processes from multiple view points, we will
keep gaining deeper understanding about their prin-
ciples.
acknowledgement
I would like to thank Manuel Baltieri, Seyed-Amin
Moosavi, Sousuke Ito, Ryota Kobayashi, Shashwat
Shukla, Masanori Murayama, and Takuma Tanaka
for critical reading of the article and helpful discus-
sions.
References
[1] F. Attneave, “Some informational aspects of vi-
sual perception.,” Psychological review, vol. 61,
no. 3, pp. 183–193, 1954.
[2] H. B. Barlow, “Possible principles underlying the
transformations of sensory messages,” in Sen-
sory Communication (W. A. Rosenblith, ed.),
ch. 13, pp. 217–234, MIT press, 1961.
[3] H. B. Barlow, “Single units and sensation: a neu-
ron doctrine for perceptual psychology?,” Per-
ception, vol. 1, no. 4, pp. 371–394, 1972.
[4] D. J. Field, “Relations between the statistics of
natural images and the response properties of
cortical cells,” Journal of the Optical Society of
America A , vol. 4, no. 12, pp. 2379–2394, 1987.
[5] R. Linsker, “Self-organization in a perceptual
network,” Computer, vol. 21, no. 3, pp. 105–117,
1988.
[6] J. J. Atick and A. N. Redlich, “What does the
retina know about natural scenes?,” Neural com-
putation, vol. 4, no. 2, pp. 196–210, 1992.
[7] S. Laughlin, “A simple coding procedure
enhances a neuron’s information capacity,”
Zeitschrift f¨ ur Naturforschung c , vol. 36, no. 9-
10, pp. 910–912, 1981.
[8] B. A. Olshausen and D. J. Field, “Emergence
of simple-cell receptive ﬁeld properties by learn-
ing a sparse code for natural images,” Nature,
vol. 381, no. 6583, pp. 607–609, 1996.
[9] B. A. Olshausen and D. J. Field, “Sparse coding
with an overcomplete basis set: A strategy em-
ployed by v1?,” Vision research, vol. 37, no. 23,
pp. 3311–3325, 1997.
[10] E. P. Simoncelli and B. A. Olshausen, “Natu-
ral image statistics and neural representation,”
Annual review of neuroscience , vol. 24, no. 1,
pp. 1193–1216, 2001.
[11] O. Schwartz and E. P. Simoncelli, “Natural sig-
nal statistics and sensory gain control,” Nature
neuroscience, vol. 4, no. 8, p. 819, 2001.
19
REFERENCES
[12] H. von Helmholtz, Treatise on physiological op-
tics, vol. 3. The Optical Society of America,
1925.
[13] H. von Helmholtz, Helmholtz’s treatise on phys-
iological optics , vol. 3. Dover Publication, Inc.,
1962.
[14] K. Doya, Bayesian brain: Probabilistic ap-
proaches to neural coding . MIT press, 2007.
[15] W. J. Ma, J. M. Beck, P. E. Latham, and
A. Pouget, “Bayesian inference with proba-
bilistic population codes,” Nature neuroscience,
vol. 9, no. 11, pp. 1432–1438, 2006.
[16] A. Pouget, J. M. Beck, W. J. Ma, and P. E.
Latham, “Probabilistic brains: knowns and un-
knowns,” Nature neuroscience , vol. 16, no. 9,
pp. 1170–1178, 2013.
[17] J. M. Beck, P. E. Latham, and A. Pouget,
“Marginalization in neural circuits with divi-
sive normalization,” Journal of Neuroscience ,
vol. 31, no. 43, pp. 15310–15319, 2011.
[18] A. Funamizu, B. Kuhn, and K. Doya, “Neural
substrate of dynamic bayesian inference in the
cerebral cortex,” Nature neuroscience , vol. 19,
no. 12, pp. 1682–1689, 2016.
[19] G. E. Hinton and R. S. Zemel, “Autoencoders,
minimum description length and helmholtz free
energy,” in Advances in neural information pro-
cessing systems , pp. 3–10, 1994.
[20] P. Dayan, G. E. Hinton, R. M. Neal, and R. S.
Zemel, “The helmholtz machine,” Neural com-
putation, vol. 7, no. 5, pp. 889–904, 1995.
[21] K. Friston, “Learning and inference in the
brain,” Neural Networks , vol. 16, no. 9,
pp. 1325–1352, 2003.
[22] K. Friston, J. Kilner, and L. Harrison, “A
free energy principle for the brain,” Journal of
Physiology-Paris, vol. 100, no. 1-3, pp. 70–87,
2006.
[23] K. Friston, “The free-energy principle: a uniﬁed
brain theory?,” Nature reviews neuroscience ,
vol. 11, no. 2, p. 127, 2010.
[24] K. Friston, “A free energy principle for biologi-
cal systems,” Entropy, vol. 14, no. 11, pp. 2100–
2121, 2012.
[25] C. L. Buckley, C. S. Kim, S. McGregor, and
A. K. Seth, “The free energy principle for action
and perception: A mathematical review,” Jour-
nal of Mathematical Psychology , vol. 81, pp. 55–
79, 2017.
[26] R. Bogacz, “A tutorial on the free-energy frame-
work for modelling perception and learning,”
Journal of mathematical psychology , vol. 76,
pp. 198–211, 2017.
[27] D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe,
“Variational inference: A review for statisti-
cians,” Journal of the American Statistical As-
sociation, vol. 112, no. 518, pp. 859–877, 2017.
[28] C. Zhang, J. Butepage, H. Kjellstrom, and
S. Mandt, “Advances in variational inference,”
arXiv:1711.05597, 2017.
[29] D. Mumford, “On the computational architec-
ture of the neocortex,” Biological cybernetics ,
vol. 66, no. 3, pp. 241–251, 1992.
[30] M. Kawato, H. Hayakawa, and T. Inui, “A
forward-inverse optics model of reciprocal con-
nections between visual cortical areas,” Network:
Computation in Neural Systems , vol. 4, no. 4,
pp. 415–422, 1993.
[31] R. P. Rao and D. H. Ballard, “Predictive cod-
ing in the visual cortex: a functional interpre-
tation of some extra-classical receptive-ﬁeld ef-
fects,” Nature neuroscience, vol. 2, no. 1, p. 79,
1999.
[32] T. S. Lee and D. Mumford, “Hierarchical
bayesian inference in the visual cortex,” Jour-
nal of the Optical Society of America A , vol. 20,
no. 7, pp. 1434–1448, 2003.
[33] A. Clark, “Whatever next? predictive brains,
situated agents, and the future of cognitive sci-
ence,” Behavioral and brain sciences , vol. 36,
no. 3, pp. 181–204, 2013.
[34] X. Pitkow and D. E. Angelaki, “Inference in the
brain: statistics ﬂowing in redundant population
codes,” Neuron, vol. 94, no. 5, pp. 943–953, 2017.
[35] K. Friston, “Hierarchical models in the brain,”
PLoS computational biology , vol. 4, no. 11,
p. e1000211, 2008.
[36] K. Friston, R. Adams, L. Perrinet, and
M. Breakspear, “Perceptions as hypotheses: sac-
cades as experiments,” Frontiers in psychology ,
vol. 3, p. 151, 2012.
[37] A. Clark, Being there: Putting brain, body, and
world together again . MIT press, 1998.
[38] F. J. Varela, E. Thompson, and E. Rosch, The
embodied mind: Cognitive science and human
experience. MIT press, 1991.
[39] K. J. Friston, J. Daunizeau, J. Kilner, and S. J.
Kiebel, “Action and behavior: a free-energy for-
mulation,” Biological cybernetics, vol. 102, no. 3,
pp. 227–260, 2010.
[40] K. Friston, S. Samothrakis, and R. Montague,
“Active inference and agency: optimal control
without cost functions,” Biological cybernetics ,
vol. 106, no. 8-9, pp. 523–541, 2012.
[41] K. J. Friston, J. Daunizeau, and S. J. Kiebel,
“Reinforcement learning or active inference?,”
PloS one , vol. 4, no. 7, p. e6421, 2009.
20
REFERENCES
[42] P. Schwartenbeck, T. FitzGerald, R. Dolan, and
K. Friston, “Exploration, novelty, surprise, and
free energy minimization,” Frontiers in psychol-
ogy, vol. 4, p. 710, 2013.
[43] P. Berkes, G. Orb´ an, M. Lengyel, and J. Fiser,
“Spontaneous cortical activity reveals hallmarks
of an optimal internal model of the environ-
ment,” Science, vol. 331, no. 6013, pp. 83–87,
2011.
[44] H. Super, H. Spekreijse, and V. A. Lamme, “A
neural correlate of working memory in the mon-
key primary visual cortex,” Science, vol. 293,
no. 5527, pp. 120–124, 2001.
[45] S. Manita, T. Suzuki, C. Homma, T. Mat-
sumoto, M. Odagawa, K. Yamada, K. Ota,
C. Matsubara, A. Inutsuka, M. Sato,
M. Ohkura, A. Yamanaka, Y. Yanagawa,
J. Nakai, Y. Hayashi, M. E. Larkum, and
M. Murayama, “A top-down cortical circuit for
accurate sensory perception.,” Neuron, vol. 86,
no. 5, pp. 1304–1316, 2015.
[46] P. R. Roelfsema, V. A. Lamme, and H. Spekrei-
jse, “Object-based attention in the primary vi-
sual cortex of the macaque monkey,” Nature,
vol. 395, no. 6700, p. 376, 1998.
[47] J. H. Reynolds, T. Pasternak, and R. Desimone,
“Attention increases sensitivity of v4 neurons,”
Neuron, vol. 26, no. 3, pp. 703–714, 2000.
[48] W. Schultz, “Dopamine reward prediction-error
signalling: a two-component response.,” Nat Rev
Neurosci, vol. 17, no. 3, pp. 183–195, 2016.
[49] V. A. Lamme and P. R. Roelfsema, “The dis-
tinct modes of vision oﬀered by feedforward and
recurrent processing,” Trends in neurosciences ,
vol. 23, no. 11, pp. 571–579, 2000.
[50] C. M. Bishop, Pattern Recognition and Machine
Learning. Springer, 2006.
[51] D. J. MacKay, Information theory, inference and
learning algorithms. Cambridge university press,
2003.
[52] D. P. Kingma, Variational inference & deep
learning: A new synthesis . PhD thesis, the Uni-
versity of Amsterdam, 2017.
[53] A. P. Dempster, N. M. Laird, and D. B. Rubin,
“Maximum likelihood from incomplete data via
the em algorithm,” Journal of the royal statis-
tical society. Series B (methodological) , vol. 39,
no. 1, pp. 1–38, 1977.
[54] L. Itti and P. Baldi, “Bayesian surprise attracts
human attention,” Vision research , vol. 49,
no. 10, pp. 1295–1306, 2009.
[55] A. J. Bell and T. J. Sejnowski, “An information-
maximization approach to blind separation
and blind deconvolution,” Neural computation ,
vol. 7, no. 6, pp. 1129–1159, 1995.
[56] P. Dayan and L. F. Abbott, Theoretical neuro-
science. Cambridge, MA: MIT Press, 2001.
[57] T. Isomura, “A measure of information available
for inference,” Entropy, vol. 20, no. 7, p. 512,
2018.
[58] S. R. Olsen, V. Bhandawat, and R. I. Wilson,
“Divisive normalization in olfactory population
codes,” Neuron, vol. 66, no. 2, pp. 287–99, 2010.
[59] R. M. Shapley and J. D. Victor, “The eﬀect of
contrast on the transfer properties of cat reti-
nal ganglion cells.,” The Journal of physiology ,
vol. 285, no. 1, pp. 275–298, 1978.
[60] S. B. Laughlin, “The role of sensory adaptation
in the retina,” Journal of Experimental Biology ,
vol. 146, no. 1, pp. 39–62, 1989.
[61] I. Ohzawa, G. Sclar, and R. Freeman, “Contrast
gain control in the cat visual cortex,” Nature,
vol. 298, no. 5871, p. 266, 1982.
[62] E. Salinas and T. J. Sejnowski, “Gain modula-
tion in the central nervous system: where behav-
ior, neurophysiology, and computation meet.,”
The Neuroscientist , vol. 7, no. 5, pp. 430–440,
2001.
[63] J. H. Reynolds and D. J. Heeger, “The nor-
malization model of attention,” Neuron, vol. 61,
no. 2, pp. 168–185, 2009.
[64] M. Carandini and D. J. Heeger, “Normalization
as a canonical neural computation,” Nature re-
views neuroscience , vol. 13, no. 1, pp. 51–62,
2012.
[65] J.-P. Nadal and N. Parga, “Nonlinear neurons in
the low-noise limit: a factorial code maximizes
information transfer,” Network: Computation in
neural systems , vol. 5, no. 4, pp. 565–581, 1994.
[66] H. MaBouDi, H. Shimazaki, S.-i. Amari, and
H. Soltanian-Zadeh, “Representation of higher-
order statistical structures in natural scenes via
spatial phase distributions,” Vision research ,
vol. 120, pp. 61–73, 2016.
[67] H. Shimazaki, “Neurons as an information-
theoretic engine,” arXiv:1512.07855, 2015.
[68] H. Shimazaki, Neural Engine Hypothesis ,
pp. 267–291. Cham: Springer International Pub-
lishing, 2018.
[69] G. Tkaˇ cik, T. Mora, O. Marre, D. Amodei, S. E.
Palmer, M. J. Berry, and W. Bialek, “Thermo-
dynamics and signatures of criticality in a net-
work of neurons,” Proceedings of the National
Academy of Sciences , vol. 112, no. 37, pp. 11508–
11513, 2015.
[70] C. Donner, K. Obermayer, and H. Shimazaki,
“Approximate inference for time-varying inter-
21
REFERENCES
actions and macroscopic dynamics of neural pop-
ulations,” PLoS computational biology , vol. 13,
no. 1, p. e1005309, 2017.
[71] J. Gaudreault and H. Shimazaki, “State-space
analysis of an ising model reveals contributions
of pairwise interactions to sparseness, ﬂuctua-
tion, and stimulus coding of monkey v1 neu-
rons,” in International Conference on Artiﬁcial
Neural Networks , pp. 641–651, Springer, 2018.
[72] B. Libet, W. Alberts, E. Wright, and B. Fein-
stein, “Responses of human somatosensory cor-
tex to stimuli below threshold for conscious sen-
sation,” Science, vol. 158, no. 3808, pp. 1597–
1600, 1967.
[73] L. J. Cauller and A. T. Kulics, “The neural ba-
sis of the behaviorally relevant n1 component
of the somatosensory-evoked potential in si cor-
tex of awake monkeys: evidence that backward
cortical projections signal conscious touch sensa-
tion,” Experimental brain research, vol. 84, no. 3,
pp. 607–619, 1991.
[74] S. Sachidhanandam, V. Sreenivasan, A. Kyri-
akatos, Y. Kremer, and C. C. Petersen, “Mem-
brane potential correlates of sensory perception
in mouse barrel cortex,” Nature neuroscience ,
vol. 16, no. 11, p. 1671, 2013.
[75] G. E. Crooks, “Entropy production ﬂuctuation
theorem and the nonequilibrium work relation
for free energy diﬀerences,” Physical Review E ,
vol. 60, no. 3, p. 2721, 1999.
[76] U. Seifert, “Stochastic thermodynamics, ﬂuctua-
tion theorems and molecular machines,” Reports
on progress in physics , vol. 75, no. 12, p. 126001,
2012.
[77] S. Ito, “Uniﬁed framework for the second law
of thermodynamics and information thermody-
namics based on information geometry,” arXiv
preprint arXiv:1810.09545, 2018.
[78] S. Goldt and U. Seifert, “Stochastic thermo-
dynamics of learning,” Physical review letters ,
vol. 118, no. 1, p. 010601, 2017.
[79] D. S. Salazar, “Nonequilibrium thermodynam-
ics of restricted boltzmann machines,” Physical
Review E , vol. 96, no. 2, p. 022131, 2017.
[80] S. Ito and T. Sagawa, “Information thermody-
namics on causal networks,” Physical review let-
ters, vol. 111, no. 18, p. 180603, 2013.
[81] D. Hartich, A. C. Barato, and U. Seifert,
“Stochastic thermodynamics of bipartite sys-
tems: transfer entropy inequalities and a
maxwell’s demon interpretation,” Journal of
Statistical Mechanics: Theory and Experiment ,
vol. 2014, no. 2, p. P02016, 2014.
[82] B. Sakmann and O. D. Creutzfeldt, “Sco-
topic and mesopic light adaptation in the cat’s
retina.,” Pﬂugers Arch , vol. 313, no. 2, pp. 168–
185, 1969.
[83] E. Eldar, J. D. Cohen, and Y. Niv, “The eﬀects
of neural gain on attention and learning,” Nature
neuroscience, vol. 16, no. 8, p. 1146, 2013.
[84] J. D. Murray, A. Bernacchia, D. J. Freedman,
R. Romo, J. D. Wallis, X. Cai, C. Padoa-
Schioppa, T. Pasternak, H. Seo, D. Lee, et al. ,
“A hierarchy of intrinsic timescales across pri-
mate cortex,” Nature neuroscience , vol. 17,
no. 12, p. 1661, 2014.
[85] P. F¨ oldi´ ak, “Learning invariance from transfor-
mation sequences,” Neural computation , vol. 3,
no. 2, pp. 194–200, 1991.
[86] L. Wiskott and T. J. Sejnowski, “Slow feature
analysis: Unsupervised learning of invariances,”
Neural computation, vol. 14, no. 4, pp. 715–770,
2002.
[87] P. Berkes and L. Wiskott, “Slow feature analysis
yields a rich repertoire of complex cell proper-
ties,” Journal of vision , vol. 5, no. 6, pp. 9–9,
2005.
22