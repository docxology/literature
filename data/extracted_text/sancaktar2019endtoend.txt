End-to-End Pixel-Based Deep Active Inference for
Body Perception and Action
CansuSancaktar MarcelA.J.vanGerven
TechnicalUniversityofMunich DondersInstituteforBrain,CognitionandBehaviour
Germany RadboudUniversity
cansu.sancaktar@tum.de Netherlands
m.vangerven@donders.ru.nl
PabloLanillos∗
DondersInstituteforBrain,CognitionandBehaviour
RadboudUniversity
Netherlands
p.lanillos@donders.ru.nl
Abstract
Wepresentapixel-baseddeepactiveinferencealgorithm(PixelAI)inspiredby
humanbodyperceptionandaction. Ouralgorithmcombinesthefreeenergyprin-
ciplefromneuroscience,rootedinvariationalinference,withdeepconvolutional
decoderstoscalethealgorithmtodirectlydealwithrawvisualinputandprovide
onlineadaptiveinference. Ourapproachisvalidatedbystudyingbodyperception
andactioninasimulatedandarealNaorobot. Resultsshowthatourapproach
allowstherobottoperform1)dynamicalbodyestimationofitsarmusingonly
monocularcameraimagesand2)autonomousreachingto“imagined"armposes
invisualspace. Thissuggeststhatrobotandhumanbodyperceptionandaction
canbeefficientlysolvedbyviewingbothasanactiveinferenceproblemguidedby
ongoingsensoryinput.
1 Introduction
Learningandadaptationaretwocorecharacteristicsthatallowhumanstoperformflexiblewhole-
bodydynamicestimationandrobustactionsinthepresenceofuncertainty[30]. Wehypothesizethat
thehumanbrainacquiresarepresentation(model)ofthebody,alreadystartingattheearlieststages
oflife,bylearningamappingbetweentactile,proprioceptiveandvisualcues[31]. Furthermore,this
mappingisflexible,asexperimentshavedemonstratedthatbodyperceptionandactioncanbealtered
inlessthanoneminutejustbysynchronousvisuotactilestimulation[2,15]. Thissupportstheview
thatunsupervisedlearningmechanismsmustbeenhancedbyonlinesupervisedadaptation[8]. On
thecontrary,robotsusuallyuseafixed-rigidbodymodelwherethearmend-effectorisdefinedasa
pose,i.e.,a3Dpointinthespaceandorientation. Hence,anyerrorinthemodelorchangeinthe
conditionswillresultinfailure.
Several solutions have been proposed to overcome this problem, usually separated in perception
and control approaches. For instance, by working in visual space (e.g. visual servoing [16]) we
canexploitasetofinvariantvisualkeypointstoprovidecontrolthatincorporatesreal-worlderrors.
Bayesiansensoryfusionincombinationwithmodel-basedfittingallowsadaptationtosensorynoise
andmodelerrors[4]andmodel-basedactiveinferenceprovidesonlineadaptationinbothaction
∗ThisworkhasbeensupportedbySELFCEPTIONprojectEUHorizon2020Programme,grantnr.741941.
Preprint.Underreview.
0202
yaM
92
]VC.sc[
3v74850.1002:viXra
Forward Pass
Internal
Action
Belief Predicted
Sensation
Bottom
Camera Free-Energy
Backward Pass
Optimization
Visual Prediction Error
Observed
Visual
Sensation
Figure1: Pixel-baseddeepactiveinference(PixelAI).Therobotinfersitsbody(e.g.,jointangles)by
minimizingthevisualpredictionerror,i.e. discrepancybetweenthecamerasensorvaluex andthe
v
expectedsensationg(µ)computedusingaconvolutionaldecoder. Theerrorsignalisusedtoupdate
theinternalbelieftomatchtheobservedsensation(perceptualinference)andtogenerateanaction
atoreducethediscrepancybetweentheobservedandpredictedsensations. Botharecomputedby
minimizingthevariationalfree-energybound.
andperception[24]. Finally,learningapproacheshaveshownthatthedifferencebetweenthemodel
andrealitycanbeovercomebyoptimizingthebodyparametersorbyexplicitlylearningthepolicy
for a task, e.g., through imitation learning or reinforcement learning (RL). Recently, model-free
approaches,particularlydeepRL,havedemonstratedthepotentialfordirectlyusingrawimagesas
aninputforlearningvisualcontrolpolicies[19].
Inthiswork,weintroducePixelAI,apixel-baseddeepactiveinferencealgorithm,depictedinFig.1,
whichcombinesallofthesecharacteristics. Thatis,itdirectlyoperatesonvisualinput,provides
adaptation,model-freelearningand,moreover,itunifiesperceptionandactionintoasinglevariational
inferenceformulation.
We motivate our proposed approach using body perception and action (though it can easily be
generalizedtootheractiveperceptiontasks). Ourapproachisgroundedinhumanembodiedresearch
throughthemaximfirstthebodythenthepolicy[18]. Thatis,anagentmustfirstlearntoperceiveits
body(thecross-modalsensorimotorrelations[6,20])andafterwardsshouldusethisinformationto
adapttoonlinechangeswheninteractingintheworld.
Technically,ourmethodcombinesthefreeenergyprinciple[11],whichupdatesaninternalmodelof
thebodythroughperceptionandaction,withdeepconvolutionaldecoders,thatmapinternalbeliefs
toexpectedsensations. Moreconcretely,theagentlearnsagenerativemodelofthebodytoaidinthe
constructionofexpectedsensationsfromincomingpartialmultimodalinformation. Thisinformation,
insteadofbeingdirectlyencoded,isprocessedbymeansoftheerrorbetweenthepredictedoutcome
andthecurrentinput. Inthissense,weformalizebodyperceptionandactionasaconsequenceof
surpriseminimizationviapredictionerrors[25,11].
Following this approach, the robot should learn a latent representation (“state”) of the body and
therelationbetweenitsstateandtheexpectedsensations(“predictions”). Thesepredictionswill
becomparedwiththeobservedsensorydata,generatinganerrorsignalthatcanbepropagatedto
refinethebeliefthattherobothasaboutitsbodystate. Compensatoryactionswouldfollowasimilar
principleandwillbeexertedtobettercorrespondtothepredictionmadebythemodelslearnt,giving
therobotthecapacitytoactivelyadjusttoonlinechangesinthebodyandtheenvironment(seeFig.1).
Thisalsooffersanaturalwaytorealizeaplanbysettinganimaginedgoalinsensoryspace[22]. For
example,whenworkinginvisualspacewecanprovideanimageasagoal. Bymeansofoptimizing
thefree-energyboundtheagentthenexecutesthoseactionsthatminimizethediscrepancybetween
expectedsensations(ourgoal)andobservedsensations.Ourapproachwasvalidatedbystudyingbody
2
perceptionandactioninasimulatedandarealNaorobot. Resultsshowthatourapproachallowsthe
robottoperform1)dynamicalbodyestimationofitsarmusingonlyrawmonocularcameraimages
and2)autonomousreachingtoarmposesprovidedbyagoalimage.
2 Background
2.1 Thefreeenergyprinciple
Wemodelbodyperceptionasinferringthebodystatezbasedontheavailablesensorydatax. Given
a sensation x, the goal is to find x such that the posterior p(zx) = p(xz)p(z)/p(x). However,
| |
computingthemarginallikelihoodp(x)requiresanintegrationoverallpossiblebodystates. That
(cid:82)
is, p(x) = p(xz)p(z)dz, which becomes intractable for large state spaces. The free-energy
z |
[14],largelyexploitedinmachinelearning[29]andneuroscience[10],circumventsthisproblemby
introducingareferencedistribution(alsocalledrecognitiondensity)q(z)withknowntractablefrom.
Thegoaloftheminimizationproblemhencebecomesfindingthereferencedistributionq(z)thatbest
approximatestheposteriorp(zx).
|
For tractability purposes, this approximation is calculated by optimizing the negative variational
free energy F, also referred to as the evidence lower bound (ELBO). F can be defined as the
Kullback-LeiblerdivergenceD minusthenegativelog-evidenceorsensorysurprise lnp(x):
KL
−
(cid:90)
q(z)
F =D (q(z) p(zx)) lnp(x)= q(z)ln dz lnp(x), (1)
KL
(cid:107) | − p(zx) −
z |
which,duetothenon-negativitypropertiesofD ,isanupperboundonsurprise. Alternatively,
(cid:82) KL
wecanusetheidentitylnp(x)= q(z)lnp(x)dztoincludethesecondtermintotheintegraland
z
writeEquation(1)as
(cid:90) (cid:90) (cid:90)
q(z)
F = q(z)ln dz= q(z)lnp(x,z)dz+ q(z)lnq(z)dz. (2)
p(x,z) −
z z z
Accordingtothefreeenergyprinciple[11]bothperceptionandactionoptimizethefreeenergyand
henceminimizesurprise:
1. Perceptualinference: Theagentupdatesitsinternalbeliefbyapproximatingtheconditional
density(inference),maximizingthelikelihoodoftheobservedsensation:
z=argminF(z,x). (3)
z
2. Activeinference: Theagentgeneratesanactionathatresultsinanewsensorystatex(a)
thatisconsistentwiththecurrentinternalrepresentation:
a=argminF(z,x(a)). (4)
a
Under the Laplace approximation, the variational density can take the form of a Gaussian
q(z) = (µ,Σ), where µ is the conditional mode and Σ is the covariance of the parameters.
Byincorp N oratingthisreferencedistributioninEquation(2),thefree-energycanbeapproximatedas2,
1
F = lnp(x,µ) (ln Σ +nln2π), (5)
− − 2 | |
wherethefirsttermisthejointdensityoftheobservedandthelatentvariableswithµann-dimensional
statevector.
3 Pixel-basedDeepActiveInference
OurproposedPixelAIapproachcombinesfreeenergyoptimizationwithdeeplearningtodirectly
work with images as visual input. The optimization provides adaptation and the neural network
incorporateslearningofhigh-dimensionalinput. Weframeandexperimentallyvalidatetheproposed
2See[10]forfullderivation.
3
algorithminbodyperceptionandactioninrobots. Figure1visuallydescribesPixelAI.Theagentfirst
learnstheapproximategenerativeforwardmodelsofthebody,implementedhereasconvolutional
decoders. Whileinteracting,theexpectedsensation(predictedbythedecoder)iscomparedwiththe
realvisualinputandthepredictionerrorisusedto1)updatethebeliefand2)generateactions. This
isperformedbymeansofoptimizingthevariationalfree-energybound.
3.1 Activeinferencemodel
Weformalizebodyperceptionasinferringtheunobservedbodystateµ,e.g.,theestimationofthe
robotjointangleslikeshoulderpitchandroll. Wedefinetherobotinternalbeliefasann-dimensional
vector: µ[d] Rn for each temporal order d. For instance, for first-order (velocity) generalized
∈
coordinatesthebeliefis: µ = µ[0],µ[1] . Theobservedvariablesxarethevisualsensoryinput
{ }
x and the external causal variables ρ: x = x ,ρ For instance, the robot has access to visual
v v
{ }
informationx (animageofsizew h)andproprioceptioninformation(thejointencodervalues
v
×
q). Thecausalvariablesρareindependentvariablesthatproduceeffectsintheworld. Finally,letus
definetwogenerativemodelsthatdescribethesystem. Thesensoryforwardmodelg,whichisthe
predictorthatcomputesthesensoryoutcomex giventheinternalstateµandthebodyinternalstate
v
dynamicsf. Bothfunctionscanbeconsideredastheapproximationsthattheagenthasaboutthe
reality:
x =g(µ)+w (6)
v v
µ[1] =f(µ,ρ)+w (7)
µ
wherew ,w arebothprocessesnoiseandareassumedtobedrawnfromamultivariatenormal
v µ
distributionwithzeromeanandcovarianceΣ andΣ respectively.
v µ
Inordertocomputethevariationalfree-energyundertheLaplaceapproximationfromEquation(5)
weneedthejointdensity. Assumingindependenceoftheobservedvariables:
lnp(x,µ)=lnp(x ,ρ,µ)=lnp(x µ)+lnp(µ[1] µ[0],ρ) (8)
v v
| |
wherep(x µ)isthelikelihoodofhavingavisualsensationgiventheinternalstateandp(µ[1] µ[0],ρ)
v
| |
isthetransitiondynamicsofthelatentvariables(bodystate).
Body perception is then instantiated as computing the body state that minimizes the variational
free-energy. This can be performed through gradient optimization ∂F/∂µ. Since the temporal
differenceµ[0] µ[0] isequaltothefirst-orderdynamicsµ[1] atequilibrium,thistermhastobe
t+1− t
includedinthecomputationofµ˙[0]tofindastationarysolutionduringthegradientdescentprocedure
wherethegradient∂F/∂µvanishesatoptimum[3]. Hence,
∂F ∂lnp(x ,ρ,µ) lnp(x µ) lnp(µ[1] µ[0],ρ)
µ˙[0] µ[1] = = v = v | + | (9)
− −∂µ ∂µ ∂µ ∂µ
Inordertocomputethelikelihoods,weassumethattheobservedimagex isnoisyandfollowsa
v
normaldistributionwithameanatthevalueofg(µ)andwithvarianceΣ . Consideringthatevery
v
pixelcontributionisindependent,suchasΣ = diag(Σ ,...,Σ ),thelikelihoodp(x µ)is
v v1 vh·w v
|
obtainedasthecollectionofindependentGaussians:
h·w (cid:20) (cid:21)
(cid:89) 1 1
p(x µ)= exp (x g (µ))2 (10)
v
| k=1
(cid:112)
2πΣ vk −2Σ vk
vk
−
k
Analogously,thedensitythatdefinesthelatentvariabledynamicsisalsoassumedtobenoisyand
followsanormaldistributionwithmeanatthevalueofthefunctionf(µ,ρ)andwithvarianceΣ :
µ
n (cid:20) (cid:21)
(cid:89) 1 1
p(µ[1] µ[0],ρ)= exp (µ[1] f (µ,ρ))2 (11)
| i=1 (cid:112) 2πΣ µi −2Σ µi i − i
SubstitutingthelikelihoodsandcomputingthepartialderivativesofEquation9,thebodystateis
thengivenbythefollowingdifferentialequation:
∂g(µ) T ∂f(µ,ρ)T
µ˙[0] =µ[1]+ Σ−1 (x g(µ))+ Σ−1(µ[1] f(µ,ρ)) (12)
∂µ v v − ∂µ µ −
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)precision predictionerror
mapping
4
Fornotationsimplicity,wenamethesecondandthethirdtermasF andF respectively.
g f
Theactionisanalogouslycomputed. However, thesensoryinformationisonlyafunctionofthe
actionx(a)andthereforeitonlydependsonthefree-energytermswithsensoryinformationF 3:
g
∂F “∂F ∂x” ∂xT ∂g(µ)T
a˙= g = g = v Σ−1(x g(µ))= ∆ Σ−1(x g(µ)) (13)
− ∂a − ∂x ∂a − ∂a v v − − ∂µ t v v −
Toderivethelastequality,wehaveemployedthesameapproximationasin[24]assumingthatthe
actionsarejointvelocities. Inavelocitycontrollerschemewecanapproximatetheanglechange
between two time steps of each joint j as ∂q /∂a = ∆ , because the target values of the joint
j j t
encodersqarecomputedasqt+1 =qt+∆ atand∆ isafixedvaluethatdefinesthedurationof
t t
eachiteration. Then,assumingconvergenceforthesensationvaluesattheequilibriumpointwith
µ → qandg(µ) → x v ,theterm ∂ ∂ x a v canbecomputedusingthefollowingequation:
∂x ∂x ∂q ∂g(µ)∂µ ∂g(µ)
v v j j
= = = ∆ (14)
t
∂a ∂q ∂a ∂µ ∂a ∂µ
j j j j j j
Theupdateruleforbothµandaisfinallycalculatedwiththefirst-orderEulerintegration:
µ =µ +∆ µ˙ a =a +∆ a˙ (15)
t+1 t t t+1 t t
3.2 Scalingupwithdeepautoencoders
In order to perform pixel-based free-energy optimization we compute Equations (12) and (13)
exploitingtheforwardandbackwardpasspropertiesofthedeepneuralnetwork. Weapproximatethe
visualgenerativemodelg(µ)anditspartialderivative∂ g(µ)withrespecttotheinternalstateby
µ
meansofaconvolutionaldecoder.
3.2.1 Predictionoftheexpectedsensation
7 x 10 x 16 14 x 20 x 64 28 x 40 x 16 28 x 40 x 16 28 x 40 x 16
snorueN
215
snorueN
0211
FC1 FC2 R UpConv1 Conv1 UpConv2 Conv2 Dropout UpConv3
14x 20 x 64 56 x 80
Figure2:NetworkArchitectureoftheConvolutionalDecoder(FC:Fully-connectedlayer,R:Reshape
operator,UpConv: TransposedConvolution,Conv: Convolution)
Weapproximatetheforwardmodelg(µ)bymeansofagenerativenetwork,basedonthearchitecture
proposedin[7],describedinFig.2. Itoutputsthepredictedimagegiventherobot’sn-dimensional
internalbeliefofthebodystateµ,e.g.,thejointanglesoftherobot.
Theinputgoesthrough2fully-connectedlayers(FC1andFC2). Afterwards,thetransposedcon-
volution(UpConv)isperformedtoupsampletheimage. Thisdeconvolutionusestheinputasthe
weightsforthefiltersandcanberegardedasabackwardpassofthestandardconvolutionoperator[9].
Following[7],eachtransposedconvolutionlayerwasfollowedbyastandardconvolutionallayer,
whichhelpstosmoothoutthepotentialartifactsfromthetransposedconvolutionstep. Thereisan
additional1D-Dropoutlayerbeforethelasttransposedconvolutionlayertoavoidoverfittingand
achieve better generalization performance. All layers use the rectified linear unit (ReLU) as the
activationfunction,exceptforthelastlayer,whereasigmoidfunctionwasusedtogetpixelintensity
valuesintherange[0,1]. ThroughouttheconsecutiveUpConv-Convoperationsinthenetwork,the
numberofchannelsisincreasedanddecreasedagaintogettherequiredoutputimagesize.
3ThechainruleinquotationmarksofEq.13indicatestheabuseofnotation.Whenxisanimagethegradient
∂x wouldbeathree-dimensionaltensor.
∂a
5
3.2.2 Backwardpassandmappingtothelatentvariable
Anessentialterm,forcomputingbothperceptionandaction,isthemappingbetweentheerrorinthe
sensoryspaceandtheinferredvariables: ∂g(µ). Thistermiscalculatedbyperformingabackward
∂µ
passovertheconvolutionaldecoder. Infact,wecancomputethewholepartialderivativeofthevisual
inputterm∂F /∂µjustinoneforwardandbackwardpass. Thereasonisthatthepredictionerror
g
betweentheexpectedandobservedsensation(x g(µ))multipliedbytheinversevarianceandthe
v
−
partialderivativeisequivalenttoapplyingthebackpropagationalgorithm. Itisimportanttonotethat
whenthefunctiong(µ)outputsimagesofsizew h, ∂g(µ) isathreedimensionaltensor. Westack
× ∂µ
theoutputintoavector Rw·h(rowmajor). Thefollowingequationisobtained:
∈
∂g1,1 ∂g1,2
...
∂gw,h  ∂Fg 
∂µ1 ∂µ1 ∂µ1 ∂g1,1
∂F g =    ∂ ∂ g µ . 1 2 ,1 ∂ ∂ g µ . 1 2 ,2 .. . . ∂ ∂ g µ w . 2 ,h      ∂ ∂ g F . 1 g ,2    (16)
∂µ  . . . .  . 
 . . . .  . 
∂g1,1 ∂g1,2
...
∂gw,h ∂Fg
∂µ4 ∂µ4 ∂µ4 ∂gw,h
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
(∂g)T ∂Fg
∂µ ∂g
where ∂Fg isgivenby 1 (x g (µ)). Theactioniscomputedbyreusingthistermand
−∂gi,l Σvi,l vi,l − i,l
multiplyingitby∆ .
t
3.3 Formalizingthetaskwithattractordynamics
InActiveInferenceweincludethegoalaspriorinthebodystatedynamicsfunctionf(µ,ρ). For
example,toperformareachingtask,weencodethedesiredgoaloftherobotasaninstanceinthe
sensoryspace(image),whichactsasanattractorgeneratedbythecausalvariableρ. Thisproducesan
errorintheinferredstatethatwillpromoteanactiontowardsthegoal. Notethattheerrorρ g(µ)is
−
zerowhenthepredictionmatchesthedesiredgoal. Wedefinethebodystatedynamicswithacausal
variableattractoras:
f(µ,ρ)=T(µ)β(ρ g(µ)), (17)
−
whereβ isagainparameterthatdefinestheintensityoftheattractorandT(µ)=∂g(µ)T/∂µisthe
mappingfromthesensoryspace(e.g. pixel-domain)totheinternalbeliefµ(e.g. jointspace). Note
thatthistermisobtainedthroughthebackwardpassofthedecoder. Finally,substitutinginEq.(12)
thenewdynamicsgenerativemodelwewritethelastterm∂F /∂µas:
f
∂F ∂f(µ,ρ)T (cid:18) ∂g(µ)T (cid:19)
f = Σ−1 µ[1] β(ρ g(µ)) (18)
− ∂µ ∂µ µ − ∂µ −
Inthefinalmodelusedintheexperiments,wehavefurthersimplifiedthisequationbynotinclud-
ing the first-order internal dynamics into the optimization process µ[1] = 0 and noting that the
correctmappinganddirectionfromthesensoryspacetothelatentvariableisalreadyprovidedby
∂g(µ)T/∂µ. Thus,wegreedilyapproximate∂f(µ,ρ)/∂µto 1,avoidingtheHessiancomputation
−
ofTbutintroducinganoptimizationdetriment. Withtheseassumptions,thepartialderivativeofthe
dynamicstermbecomes:
∂F (cid:18) ∂g(µ)T (cid:19)
f =Σ−1 β(ρ g(µ)) =Σ−1f(µ,ρ) (19)
− ∂µ µ ∂µ − µ
3.4 PixelAIalgorithm
Algorithm1summarizestheproposedmethod. Intherobotbodyperceptionandactionapplication,
x issettotheimageprovidedbytherobotmonocularcameraandthedecoderinputbecomesthe
v
robot proprioception (e.g. joint angles). The convolutional decoder is trained with this mapping
obtainingapredictorofthevisualforwardmodel. Thepredictionerrore isthedifferencebetween
v
theexpectedvisualsensationandtheobservation(line6). Thevariationalfree-energyoptimization,
for perception (line 7) and action (line 8), updates the differential equations that drive the state
6
estimationandcontrol. Finally,weaddedinthedynamicstermthepossibilityofinputtingdesired
goalsinthevisualspace(line13). Althoughthisimplementationassumesthatµ[1] =0,itisstraight
forwardtoaddthe1storderdynamicswhenthereisvelocityimageinformationorjointencoders[24].
Algorithm1PixelAI:DeepActiveInferenceAlgorithm
Require: Σ ,Σ ,β,∆
v µ t
1: µ Initialjointsangleestimation
←
2: while(true)do
3: x v Resize(cameraimage) (cid:46)VisualSensation
←
4: g(µ) ConvDecoder.forward(µ)
←
5: ∂g ConvDecoder.backward(µ)
←
6: e v =(x v g(µ)) (cid:46)Predictionerror
7: µ˙ =K Σv ∂ − gTe v /Σ v 4
8: a˙ = (∂gTe v /Σ v )∆ t
−
9: if ρthen (cid:46)Goalattractorρdynamics
∃
10: e f =β(ρ g(µ))
−
11: µ˙ =µ˙ +∂gTe f /Σ µ
12: µ=µ+∆ t µ˙;a=a+∆ t a˙ (cid:46)1storderEulerintegration
13: SetVelocityController(a)
4 Experiments
WetestedthePixelAIinbothsimulatedandrealAldebaranNAOhumanoidrobot(Fig.4). Weused
theleftarmtotestbothperceptionandactionschemes. Thedatasetandthecodetoreplicatethe
experimentscanbefoundintobereleased.
(a) SimulatedNao (b) RealNao
Figure3: Experimentalsetupinsimulation(Gazebo)andwiththerealrobot.
4.1 Visualforwardmodel
Dataacquisition&preprocessing. Thedatasetusedtotrainthemodelconsistedof3200data
samplesoftheleftarmelbowandshoulderjointreadings(q = [q ,q ,q ,q ]T)andtheobserved
1 2 3 4
images x obtained through NAO’s bottom camera5. Data samples were generated using three
v
differentmethodsandlaterconcatenated( 25%, 20%and 55%foreachmethod).
∼ ∼ ∼
Inthefirstmethod,thejointangleswererandomlydrawnfromauniformdistributionintherange
ofthejointlimits. Afterwards,itrequiredthemanualeliminationofsampleswheretherobot’sarm
wasoutofthecameraframe. Theratiooftheacquiredimageswiththerobothandcenteredinthe
cameraimagewassignificantlylowerthantheimageswiththehandlocatedatthecornersofthe
4ThegainparameterK isaddedtoallowthemodeltogeneratelargeactionvalueswithoutincreasingthe
Σv
internalbeliefincrements.
5In simulation, the color of the right arm was changed to dark grey to achieve contrast with the grey
backgroundinthecameraimages.
7
frame. Toreducethisdrawback,inthesecondmethod,therobot’sarmwasmanuallymovedbyan
operatorandthejointanglereadingswererecordedduringthesetrajectories. Thisway,asubsetof
datawasobtained,wheretherobothandwascenteredintheimage. Finally,inthethirdmethod,a
multivariateGaussianwasfittothesecondsubsetusingtheexpectation-maximizationalgorithmand
randomsamplesweredrawnfromthisGaussianforthethirdandfinalpartofthedataset. Thegoal
wastointroducerandomnesstothecentered-imagesandnotbelimitedtotheoperator’schoiceof
trajectories.
FortheimagescollectedintheGazeboNAOSimulator,theonlypreprocessingstepperformedwas
re-sizingtheimageofsize640 480to80 56. FortherealNAO,theimageswereobtainedon
× ×
agreenbackground(Fig.3(b))andthefollowingpreprocessingstepswereperformed: 1)median
filtering with kernel size 11 on the original image, 2) masking the monochrome background,e.g.
green,intheHSVcolorspaceandreplacingitwithdarkgraytoensurecontrast3)convertingthe
imagetograyscale,and4)resizingimagetodimensions80 56.
×
Training. TheconvolutionaldecoderwastrainedusingtheADAMoptimizerusingamini-batchof
size200samplesandaninitiallearningrateofα=10−4withexponentialdecayof0.95every5000
steps. Thetrainingwasstoppedafterca. 7000iterationsforthesimulatedNAOdatasetand12000
iterationsfortherealNAOdatasettoavoidoverfitting,asthetestseterrorstartedtoincreaseforthe
correspondingmodel. Theoutputofthesecondfullyconnectedlayer(FC2)wasan1120-dimensional
vectorthatwasreshapedintoa7 10 16tensor. TheUpConvlayersallusestrideequalto2anda
× ×
paddingof1. Moreover,akernelwithasizeof4 4waschosentoavoidcheckerboardartifactsdue
×
tounevenoverlap[23]. Convolutionallayersusedakernelsize3,stride1andpadding1. Theratioof
drop-outwassetto0.15. Thefinallayeroutputsa1 56 80imagecorrespondingtoagrayscale
× ×
image.
4.2 BenchmarkforPixelAI
AbenchmarkwiththreelevelsofdifficultywascreatedtoevaluatetheperformanceofPixelAIon
randomizedsamplesforbothperceptualandactiveinference. Asetof50differentcores(i.e. images
of the arm) were generated by sampling the multivariate Gaussian distribution (see method 2 in
section4.1). AsubsetofthegeneratedcoresisshowninFig.5(a). Foreachofthecores,10different
randomtestswereperformed. Intotal, therewere2500trialscomposedof5runsof500testing
imagearmposesperbenchmarklevel6. Thetestsamplesforeachcoreweregenerateddifferently
dependingonthebenchmarklevel:
• Level 1 (close similar poses): One of the 4 joints was chosen randomly and a random
perturbation [5◦,10◦]sampledfromauniformdistributionwasaddedtothejointangle
±
valuetogeneratethenewtestsample.
• Level2(farsimilarposes): Forallofthe4joints,arandomperturbation [5◦,10◦]was
±
sampledfromauniformdistributionandaddedtothecorejointangles.
• Level3(random): Foreachcore,10differentcoreswerechosenrandomlyandusedasthe
testsamples.
PerceptualInference. Inordertoevaluatethebodyperceptionperformance,therobothastoinfer
itsrealarmposejustusingvisualinformation. Therobot’sarmwasinitializedtoeachcorepose
andthen10separatetestrunswereperformed,wheretheinternalbeliefwassettoaperturbedvalue
ofthecorrespondingpose. Thesetestsarestaticinnature,i.e. thechangesolelytakesplaceinthe
internalpredictionsoftherobot. Thegoalisthattherobotinternalbeliefµconvergestothetruearm
position,whichisequaltothejointanglesofthechosencore.
ActiveInference. Inordertoevaluatetheperceptionandactionperformance,thecoreposeswere
treatedasthedesiredgoal(image)encodedasanattractorinthemodel. Againforeachcore,10
separatetestrunswereperformed. Inthiscase,therobotarmwasinitializedtoacoreposeandalso
theinitialinternalbeliefwassettothecurrentjointmeasurements: µ = q. Ineachtest,thegoal
wasthattherobotreachedthedesiredimaginedarmpose. Theupdateoftheinternalbeliefshould
generate an action to compensate for the mismatch between the current and the predicted visual
6Fortherealrobotbenchmarktests(perceptualinference)onlyasubsetof20coreswereused.
8
sensations. Inasuccessfultestrun,therobotarmshouldmovetotheimaginedarmpositionandthe
internalbeliefshouldalsoconvergetotheimaginedjointangles,sothat: x =g(µ)=ρ.
v
Algorithm Parameters. The parameters of the Pixel AI algorithm are Σ ,K ,β,∆ , which
v Σv t
weredeterminedempirically7. Theintuitionbehindthevariancetermsisasfollows: theprediction
errors get multiplied by the inverse of the variances so these actually weigh the relevance of the
correspondingsensoryinformationerror[3].Theβterm,thatispartoftheattractordynamics(seeEq.
17),essentiallyhasthesameeffectanditcontrolshowmuchwewanttopushtheinternalbeliefin
thedirectionoftheattractor. Finally,the∆ dependsontheinternaltimeoftherobotloopexecution.
t
Theparametersusedforbenchmarklevels1and2werethesame. Forlevel3inperceptualinference,
weusedasmallerΣ untilthevisualpredictionerrorwasbelowacertainthreshold(0.01). This
v
introducesanewmodelparameterγ whichisusedtoscaleΣ ,oncetheerrorthresholdisreached.
Σv v
Thisheuristicmethodofadaptationhelpedspeeduptheconvergenceforthemorecomplexlevel3
trajectories. Finally,thegeneratedactions(velocityvalues)wereclippedsothateachjointcouldnot
movemorethan[ 2◦;2◦]eachtimestep.
−
Table1: PixelAIParametersusedfortheperceptualandactiveinferencebenchmarksinsimulation.
Σ β K γ
v Σv Σv
ActiveInference
Level1 6 102 2 10−5 10−3 1
Level2 2 × 102 5 × 10−5 10−3 1
Level3 × 20 5 × 10−4 10−3 1
×
PerceptualInference
Level1 2 104 - - 1
Level2 2 × 104 - - 1
Level3 2 × 103 - - 10
×
5 Results
5.1 Statisticalanalysisofperceptionandactioninsimulation
First, perceptualinferencetestswererunfor5000-timestepsforall3levels. Anexampleofthe
perceptual inference for each level is depicted in Fig. 5.1. For level 1 and 2, with less than 150
iterations, the algorithm converged to the ground truth, while inferring the body location from a
totallyrandominitialization(level3)raisedconsiderablythecomplexity. Table2showstheresulting
averageofalltrialsofthemeanabsolutejointerrors(q µ). Level1and2resultsconverged
| true− |
tointernalbeliefvaluessuccessfully. Figure5(c)showstheerrorduringtheoptimizationprocess.
Shoulderpitchandshoulderrollangleswereestimatedwithbetteraccuracycomparedtotheelbow
angles. This is due to the fact that a small change in the shoulder pitch angle yields to a greater
differenceinthevisualfieldincomparisonwiththesameamountofchangeintheelbowrollangle.
SincePixelAIachievesperceptionbyminimizingthevisualpredictionerror,theaccuracyincreases
whenthepixel-baseddifferenceisstronger. Therefore,themeanerrorandstandarddeviationincrease
fortheelbowjointangleestimations.
Theerrorsinlevel3,wheretherobothadtoconvergetorandomarmlocations,werelargercompared
tolevels1and2,asshowninFig.5(c). Thisisduetotworeasons. Thefirstoneisthelocalminima
probleminherentofourgradientdescentapproach. Thesecondoneaffectsthedesiredjointposition,
asdepictedinFig.6: severaljointsolutionshavesmallvisualpredictionerrorincreasingtheriskof
gettingintoalocalminimum.
Secondly,activeinferencetestswithgoalimageswereperformedusingthesimulatedNAOfor1000
timestepsinthebenchmarklevels. Theresultsforlevels1and2areshowninFig.8. Thejoint
encoderreadingsfollowedtheinternalbeliefvaluesthroughtheactionsgeneratedbyfree-energy
7SinceβandΣ achievethesameeffect,Σ isalwayssetto1andonlyβiskeptasaparameter,eliminating
µ µ
redundancy.
9
Figure4: Exampleoftheinternaltrajectoriesofthelatentspaceduringtheperceptualinferencetests
forthreedifferentlevelsofdifficultyforcore3.
Table2: Perceptualinferencejointanglesmeanabsoluteerror(indegrees).
Level ShoulderPitch ShoulderRoll ElbowYaw ElbowRoll
1 0.259 0.343 0.333 0.408 0.848 0.956 0.940 1.061
± ± ± ±
2 0.387 0.711 0.617 0.978 1.185 1.475 1.567 2.184
± ± ± ±
3 5.316 11.057 5.609 7.679 17.64 26.08 12.25 15.41
± ± ± ±
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000 1200 1400
Timestep(t)
(a) Benchmarkcoressubset
))µ(g−vs(ESM
Level1 1.2
Level2 Level3
1.0
0.8
0.6
0.4
0.2
0.0
0 200 400 600 800 1000 1200 1400
Timestep(t)
(b) VisualPredictionError
)snaidar(2kµ−psk
Level1
Level2 Level3
(c) L2Normofs −µ.
p
Figure 5: (a): A subset of the cores used for the benchmarking tests. (b)-(c): Simulated Nao
perceptualinferenceresultsforalllevels(1-3)ofthebenchmarkareshown. TheL2-Normofthe
errorbetweeninternalbeliefµands isplotted,aswellasthevisualpredictionerror.
p
optimization. Level3performancedetrimentshowsthatinteractingismorecomplexthanperceiving
asitincludesthebodyandtheworldconstraints.
5.2 Activeinferenceintherealrobot
Table3: Perceptualinferencejointanglesmeanabsoluteerrorintherealrobot. (indegrees).
Level ShoulderPitch ShoulderRoll ElbowYaw ElbowRoll
1 1.326 0.823 0.679 0.773 1.569 1.441 1.9690 2.032
± ± ± ±
2 1.862 1.846 2.221 3.038 2.935 3.311 3.807 3.323
± ± ± ±
3 9.799 12.63 12.77 10.91 29.35 35.48 21.82 17.23
± ± ± ±
Wetestedtheproposedalgorithmintherealrobot. Converselytosimulation,therobot’smovements
areimpreciseduetothemechanicalbacklashintheactuators( 5◦)[12]. Furthermore,wedeployed
±
the velocity controller over the built-in NAO position control yielding to a bad synchronization
between algorithm and the real movement. For instance, the robot had to wait for the generated
actions to be large enough, in order to send the commands to the controller. This caused the
movementstobeunsmooth. Moreover,aswedidnothavedirectaccesstothemotordriver,thetime
10
(a) Truearmposition (b) Initialinternalbelief (c) Deepconvolutional
(cameraimage). deepconvolutionaldecoder decoderoutputg(µ)forend
outputg(µ)(overlaidonthe internalbelief(overlaidon
cameraimage). thecameraimage).
0.06
0.05
0.04
0.03
0.02
0.01
0.00
0 200 400 600 800 1000
Timestep(t)
])µ(g−vs[ESM
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0 200 400 600 800 1000
Timestep(t)
(d) Visualpredictionerror.
)snaidar(kps−µk
(e) L2Normofµ−s .
p
Figure6: Exampleofthelocalminimaprobleminlevel3perceptualinferencetests.
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000 1200 1400
Timestep(t)
))µ(g−vs(ESM
Level1 1.2
Level2
Level3
1.0
0.8
0.6
0.4
0.2
0.0
0 200 400 600 800 1000 1200 1400
Timestep(t)
(a) VisualPredictionError
)snaidar(2kµ−psk
Level1
Level2
Level3
(b) L2Normofµ−s
p
Figure7: RealNAOperceptualinferenceresultsforall3benchmarklevels
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000 1200 1400
Timestep(t)
)vs−ρ(ESM
Level1 1.2
Level2
Level3
1.0
0.8
0.6
0.4
0.2
0.0
0 200 400 600 800 1000 1200 1400
Timestep(t)
(a) Visualerror
)snaidar(2krttaq−psk
Level1
Level2
Level3
(b) L2-NormofJointAngleErrors
Figure8: SimulatedNAOactiveinferencetestresultsforallthreelevels: (a)Visualerrorbetween
thevisualattractorρandtheobservedcameraimages . (b)L2-Normoftheerrorbetweenthejoint
v
anglesoftheattractorpositionq andtheproprioceptivesensorreadingss
attr p
.
theactionwasexecutedhadalargemismatchbetweentheinternalerrorandtheactualarmposition,
11
0.06
0.05
0.04
0.03
0.02
0.01
0.00
0 200 400 600 800 1000 1200 1400
Timestep(t)
ESM
ρ−sv sv−g(µ)
ρ−g(µ) 0.25
0.20
0.15
0.10
0.05
0.00
0 200 400 600 800 1000 1200 1400
Timestep(t)
(a) Level2:Visualerror
)snaidar(2krreqk
s µ p − −q q a a t t t t r r
(b) Level2:L2-NormofJointAngleErrors
Figure9: CloserlookatsimulatedNAOactiveinferenceresultsforlevel2
resultinginadesynchronizationbetweentheinternalmodelandtherealworld,whichcouldcause
thesystemtodiverge.
Furthermore,thevisualforwardmodelwasexpectedtomodelthemorecomplexstructureofthereal
robothand,thatissubjecttolightingdifferencesandhasareflectivesurface. Unlikeinthesimulator,
thesameconditionscannotberestoredperfectlyintherealworld,sothemodeltrainingisalways
subjecttoadditionalnoiseinthedataset. Weusedthesamedeepconvolutionaldecoderarchitecture
forourtestsontherealrobotaswell. Lowtrainingerrorwasachievedonthetrainingdataset(MSEin
pixel-intensity: ca. 0.0015). TheresultsoftheperceptualinferenceforrealNAOonall3benchmark
levelsareshowninFig.7. Similarbehavioursofperceptualconvergencewerefoundinlevel1and
2,whilelevel3hadalargererrorduetothelocalminima. Figure10showsthePixelAIalgorithm
runningontherobot.
Figure10: PixelAItestontherealNao. µistheinferredstate,qistherealjointanglesreadings,goal
isthegroundtruthgoalanglesand0.05steps=1s. (bottomrow)Armsequence: goalimageand
Naovisualinputareoverimposed.
6 Discussion
Wehaveshownthatvariationalfree-energyoptimizationcanworkasageneralinnermechanismfor
bothperception(estimation)andaction(control). Ouralgorithmextendspreviousactiveinference
workstacklinghigh-dimensionalvisualinputsandprovidingsensorygenerativemodelslearning.
Thispredictionerrorvariantofcontrolasinference[27]exploitstherepresentationlearnttoindirectly
generatetheactionswithoutapolicy. Therobotisproducingtheactionsreachthedesiredgoalinthe
visualspacewithoutlearningtheexplicitpolicy.
Thereareotherinterestingadvantagesofourproposedapproach. First,itisneuroscience-inspired,
specificallyitgroundsonpredictivecodingapproachandthefreeenergyprinciple[25,11]. This
means that we can directly make comparisons with human body perception [15]. Secondly, the
12
features learnt instead of being bound to the task, they are grounded to the body. Hence, learnt
representationshaveaphysicalmeaningandcanbeusedtosolveothertasks. Furthermore,itdoes
notneedrewardsengineeringandworksdirectlyinsensoryspace.
Adeeperanalysisshouldbeperformedtoevaluatethelimitsofthealgorithmtoscaletosequential
complextasks. Forinstance,wecandesignasystemthatgeneratesdesiredinternalbeliefsthatare
transformedintoexpectedsensationsthatwilldrivetheagenttowardsthegoal,inlinewiththeview
ofperceptionasahierarchicaldynamicalsystem[26].
Thefreeenergyformulation[11]andvariationalautoencoders[17]sharethesametheoryandsolve
similar problems, as shown in [32], where the action is directly outputted from the variational
autoencoder. However, autoencoders architecture do not account for the original ideas from the
Helmholtzmachine[5]. Perceptionshouldbeanactivecontinuousprocess[1]. Herewehaveshown
howwecanusevariationalinferencetoprovidetheactiveadaptationandinterpolationtoonlineinput
exploitingtheforwardandbackwardpassesoftheneuralnetwork.Thisallowsustoincorporatepriors
(top-downmodulation)whilemaintaininginferencedynamicsovertheobservations(bottom-up).
Besides,ourPixelAIalgorithmcomplementstheactiveinferencecommunityefforttoprovidescalable
modelsforrealapplications[28,21].
7 Conclusions
We have described a Pixel-based deep Active Inference algorithm and applied it for robot body
perceptionandaction. Ouralgorithmenabledestimationoftherobotarmjointstatesjustusinga
monocularcamerainputandperformgoaldrivenbehavioursusingimaginarygoalsinthevisual
space. Statisticalresultsshowedconvergenceinbothperceptionandactionindifferentlevelsof
difficultywithalargererrorwhendealingwithtotallyrandomarmposes. Thisneuroscience-inspired
approach is thought to make deeper interpretations than conventional engineering solutions [13],
givingsomegroundingfornovelmachinelearningdevelopments,especiallyforbodyperceptionand
action.
References
[1] RuzenaBajcsy, YiannisAloimonos, andJohnKTsotsos. Revisitingactiveperception. Au-
tonomousRobots,42(2):177–196,2018.
[2] Matthew Botvinick and Jonathan Cohen. Rubber hands ‘feel’touch that eyes see. Nature,
391(6669):756,1998.
[3] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free en-
ergy principle for action and perception: A mathematical review. Journal of Mathematical
Psychology,2017.
[4] CristinaGarciaCifuentes,JanIssac,ManuelWüthrich,StefanSchaal,andJeannetteBohg.Prob-
abilisticarticulatedreal-timetrackingforrobotmanipulation. IEEERoboticsandAutomation
Letters,2(2):577–584,2016.
[5] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz
machine. Neuralcomputation,7(5):889–904,1995.
[6] German Diez-Valencia, Takuya Ohashi, Pablo Lanillos, and Gordon Cheng. Sensorimotor
learningforartificialbodyperception. arXivpreprintarXiv:1901.09792,2019.
[7] AlexeyDosovitskiy,JostTobiasSpringenberg,MaximTatarchenko,andThomasBrox.Learning
togeneratechairs,tablesandcarswithconvolutionalnetworks. IEEEtransactionsonpattern
analysisandmachineintelligence,39(4):692–705,2016.
[8] KenjiDoya. Whatarethecomputationsofthecerebellum,thebasalgangliaandthecerebral
cortex? Neuralnetworks,12(7-8):961–974,1999.
[9] VincentDumoulinandFrancescoVisin. Aguidetoconvolutionarithmeticfordeeplearning.
arXivpreprintarXiv:1603.07285,2016.
[10] Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny.
Variationalfreeenergyandthelaplaceapproximation. Neuroimage,34(1):220–234,2007.
13
[11] KarlJ.Friston.Thefree-energyprinciple:aunifiedbraintheory? NatureReviews.Neuroscience,
11:127–138,022010.
[12] DavidGouaillier,CyrilleCollette,andChrisKilner. Omni-directionalclosed-loopwalkfornao.
In201010thIEEE-RASInternationalConferenceonHumanoidRobots,pages448–454.IEEE,
2010.
[13] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick.
Neuroscience-inspiredartificialintelligence. Neuron,95(2):245–258,2017.
[14] Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and
helmholtz free energy. In Advances in neural information processing systems, pages 3–10,
1994.
[15] Nina-AlisaHinz,PabloLanillos,HermannMueller,andGordonCheng. Driftingperceptual
patternssuggestpredictionerrorsfusionratherthanhypothesisselection: replicatingtherubber-
handillusiononarobot. arXivpreprintarXiv:1806.06809,2018.
[16] SethHutchinson,GregoryDHager,andPeterICorke. Atutorialonvisualservocontrol. IEEE
transactionsonroboticsandautomation,12(5):651–670,1996.
[17] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
[18] PabloLanillos,EmmanuelDean-Leon,andGordonCheng. Yieldingself-perceptioninrobots
throughsensorimotorcontingencies. IEEETrans.onCognitiveandDevelopmentalSystems,
(99):1–1,2016.
[19] SergeyLevine,ChelseaFinn,TrevorDarrell,andPieterAbbeel. End-to-endtrainingofdeep
visuomotorpolicies. TheJournalofMachineLearningResearch,17(1):1334–1373,2016.
[20] KingsonManandAntonioDamasio. Homeostasisandsoftroboticsinthedesignoffeeling
machines. NatureMachineIntelligence,1(10):446–452,2019.
[21] Beren Millidge. Deep active inference as variational policy gradients. arXiv preprint
arXiv:1907.03876,2019.
[22] AshvinVNair,VitchyrPong,MurtazaDalal,ShikharBahl,StevenLin,andSergeyLevine. Vi-
sualreinforcementlearningwithimaginedgoals. InAdvancesinNeuralInformationProcessing
Systems,pages9191–9200,2018.
[23] AugustusOdena,VincentDumoulin,andChrisOlah. Deconvolutionandcheckerboardartifacts.
Distill,1(10):e3,2016.
[24] GuillermoOliver,PabloLanillos,andGordonCheng. Activeinferencebodyperceptionand
actionforhumanoidrobots. arXivpreprintarXiv:1906.03022,2019.
[25] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional
interpretationofsomeextra-classicalreceptive-fieldeffects. Natureneuroscience,2(1):79–87,
1999.
[26] JunTani. Exploringroboticminds: actions, symbols, andconsciousnessasself-organizing
dynamicphenomena. OxfordUniversityPress,2016.
[27] MarcToussaint. Robottrajectoryoptimizationusingapproximateinference. InProceedingsof
the26thannualinternationalconferenceonmachinelearning,pages1049–1056,2009.
[28] AlexanderTschantz,ManuelBaltieri,AnilSeth,ChristopherLBuckley,etal. Scalingactive
inference. arXivpreprintarXiv:1911.10601,2019.
[29] Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and
variationalinference. FoundationsandTrendsinMachineLearning,1(1):1–305,2007.
[30] Daniel M Wolpert, Jörn Diedrichsen, and J Randall Flanagan. Principles of sensorimotor
learning. NatureReviewsNeuroscience,12(12):739,2011.
[31] YasunoriYamada,HoshinoriKanazawa,ShoIwasaki,YukiTsukahara,OsukeIwata,Shigehito
Yamada, and Yasuo Kuniyoshi. An embodied brain model of the human foetus. Scientific
Reports,6,2016.
[32] MartinaZambelli,AntoineCully,andYiannisDemiris. Multimodalrepresentationmodelsfor
predictionandcontrolfrompartialinformation.RoboticsandAutonomousSystems,123:103312,
2020.
14