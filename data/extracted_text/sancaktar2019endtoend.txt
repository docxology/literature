End-to-End Pixel-Based Deep Active Inference for
Body Perception and Action
Cansu Sancaktar
Technical University of Munich
Germany
cansu.sancaktar@tum.de
Marcel A. J. van Gerven
Donders Institute for Brain, Cognition and Behaviour
Radboud University
Netherlands
m.vangerven@donders.ru.nl
Pablo Lanillos∗
Donders Institute for Brain, Cognition and Behaviour
Radboud University
Netherlands
p.lanillos@donders.ru.nl
Abstract
We present a pixel-based deep active inference algorithm (PixelAI) inspired by
human body perception and action. Our algorithm combines the free energy prin-
ciple from neuroscience, rooted in variational inference, with deep convolutional
decoders to scale the algorithm to directly deal with raw visual input and provide
online adaptive inference. Our approach is validated by studying body perception
and action in a simulated and a real Nao robot. Results show that our approach
allows the robot to perform 1) dynamical body estimation of its arm using only
monocular camera images and 2) autonomous reaching to “imagined" arm poses
in visual space. This suggests that robot and human body perception and action
can be efﬁciently solved by viewing both as an active inference problem guided by
ongoing sensory input.
1 Introduction
Learning and adaptation are two core characteristics that allow humans to perform ﬂexible whole-
body dynamic estimation and robust actions in the presence of uncertainty [30]. We hypothesize that
the human brain acquires a representation (model) of the body, already starting at the earliest stages
of life, by learning a mapping between tactile, proprioceptive and visual cues [31]. Furthermore, this
mapping is ﬂexible, as experiments have demonstrated that body perception and action can be altered
in less than one minute just by synchronous visuotactile stimulation [2, 15]. This supports the view
that unsupervised learning mechanisms must be enhanced by online supervised adaptation [8]. On
the contrary, robots usually use a ﬁxed-rigid body model where the arm end-effector is deﬁned as a
pose, i.e., a 3D point in the space and orientation. Hence, any error in the model or change in the
conditions will result in failure.
Several solutions have been proposed to overcome this problem, usually separated in perception
and control approaches. For instance, by working in visual space (e.g. visual servoing [ 16]) we
can exploit a set of invariant visual keypoints to provide control that incorporates real-world errors.
Bayesian sensory fusion in combination with model-based ﬁtting allows adaptation to sensory noise
and model errors [ 4] and model-based active inference provides online adaptation in both action
∗This work has been supported by SELFCEPTION project EU Horizon 2020 Programme, grant nr. 741941.
Preprint. Under review.
arXiv:2001.05847v3  [cs.CV]  29 May 2020
Predicted	
Sensation
		
Visual	Prediction	Error
		
Free-Energy
Optimization Backward	Pass
Internal
	Belief	
Observed
Visual
Sensation	
Bottom
Camera
Action
Forward	Pass
Figure 1: Pixel-based deep active inference (PixelAI). The robot infers its body (e.g., joint angles) by
minimizing the visual prediction error, i.e. discrepancy between the camera sensor value xv and the
expected sensation g(µ) computed using a convolutional decoder. The error signal is used to update
the internal belief to match the observed sensation (perceptual inference) and to generate an action
a to reduce the discrepancy between the observed and predicted sensations. Both are computed by
minimizing the variational free-energy bound.
and perception [24]. Finally, learning approaches have shown that the difference between the model
and reality can be overcome by optimizing the body parameters or by explicitly learning the policy
for a task, e.g., through imitation learning or reinforcement learning (RL). Recently, model-free
approaches, particularly deep RL, have demonstrated the potential for directly using raw images as
an input for learning visual control policies [19].
In this work, we introduce PixelAI, a pixel-based deep active inference algorithm, depicted in Fig. 1,
which combines all of these characteristics. That is, it directly operates on visual input, provides
adaptation, model-free learning and, moreover, it uniﬁes perception and action into a single variational
inference formulation.
We motivate our proposed approach using body perception and action (though it can easily be
generalized to other active perception tasks). Our approach is grounded in human embodied research
through the maxim ﬁrst the body then the policy [18]. That is, an agent must ﬁrst learn to perceive its
body (the cross-modal sensorimotor relations [6, 20]) and afterwards should use this information to
adapt to online changes when interacting in the world.
Technically, our method combines the free energy principle [11], which updates an internal model of
the body through perception and action, with deep convolutional decoders, that map internal beliefs
to expected sensations. More concretely, the agent learns a generative model of the body to aid in the
construction of expected sensations from incoming partial multimodal information. This information,
instead of being directly encoded, is processed by means of the error between the predicted outcome
and the current input. In this sense, we formalize body perception and action as a consequence of
surprise minimization via prediction errors [25, 11].
Following this approach, the robot should learn a latent representation (“state”) of the body and
the relation between its state and the expected sensations (“predictions”). These predictions will
be compared with the observed sensory data, generating an error signal that can be propagated to
reﬁne the belief that the robot has about its body state. Compensatory actions would follow a similar
principle and will be exerted to better correspond to the prediction made by the models learnt, giving
the robot the capacity to actively adjust to online changes in the body and the environment (see Fig. 1).
This also offers a natural way to realize a plan by setting an imagined goal in sensory space [22]. For
example, when working in visual space we can provide an image as a goal. By means of optimizing
the free-energy bound the agent then executes those actions that minimize the discrepancy between
expected sensations (our goal) and observed sensations. Our approach was validated by studying body
2
perception and action in a simulated and a real Nao robot. Results show that our approach allows the
robot to perform 1) dynamical body estimation of its arm using only raw monocular camera images
and 2) autonomous reaching to arm poses provided by a goal image.
2 Background
2.1 The free energy principle
We model body perception as inferring the body state z based on the available sensory data x. Given
a sensation x, the goal is to ﬁnd x such that the posterior p(z|x) = p(x|z)p(z)/p(x). However,
computing the marginal likelihood p(x) requires an integration over all possible body states. That
is, p(x) =
∫
z p(x|z)p(z)dz, which becomes intractable for large state spaces. The free-energy
[14], largely exploited in machine learning [29] and neuroscience [10], circumvents this problem by
introducing a reference distribution (also called recognition density) q(z) with known tractable from.
The goal of the minimization problem hence becomes ﬁnding the reference distribution q(z) that best
approximates the posterior p(z|x).
For tractability purposes, this approximation is calculated by optimizing the negative variational
free energy F, also referred to as the evidence lower bound (ELBO). F can be deﬁned as the
Kullback-Leibler divergence DKL minus the negative log-evidence or sensory surprise −ln p(x):
F = DKL(q(z)∥p(z|x)) −ln p(x) =
∫
z
q(z) ln q(z)
p(z|x)dz −ln p(x) , (1)
which, due to the non-negativity properties of DKL, is an upper bound on surprise. Alternatively,
we can use the identity ln p(x) =
∫
z q(z) lnp(x)dz to include the second term into the integral and
write Equation (1) as
F =
∫
z
q(z) ln q(z)
p(x,z)dz = −
∫
z
q(z) lnp(x,z)dz +
∫
z
q(z) lnq(z)dz . (2)
According to the free energy principle [11] both perception and action optimize the free energy and
hence minimize surprise:
1. Perceptual inference: The agent updates its internal belief by approximating the conditional
density (inference), maximizing the likelihood of the observed sensation:
z = arg min
z
F(z,x) . (3)
2. Active inference: The agent generates an action a that results in a new sensory state x(a)
that is consistent with the current internal representation:
a = arg min
a
F(z,x(a)) . (4)
Under the Laplace approximation, the variational density can take the form of a Gaussian
q(z) = N(µ,Σ), where µis the conditional mode and Σ is the covariance of the parameters.
By incorporating this reference distribution in Equation (2), the free-energy can be approximated as2,
F = −ln p(x,µ) −1
2 (ln |Σ|+ nln 2π) , (5)
where the ﬁrst term is the joint density of the observed and the latent variables withµan n-dimensional
state vector.
3 Pixel-based Deep Active Inference
Our proposed PixelAI approach combines free energy optimization with deep learning to directly
work with images as visual input. The optimization provides adaptation and the neural network
incorporates learning of high-dimensional input. We frame and experimentally validate the proposed
2See [10] for full derivation.
3
algorithm in body perception and action in robots. Figure 1 visually describes PixelAI. The agent ﬁrst
learns the approximate generative forward models of the body, implemented here as convolutional
decoders. While interacting, the expected sensation (predicted by the decoder) is compared with the
real visual input and the prediction error is used to 1) update the belief and 2) generate actions. This
is performed by means of optimizing the variational free-energy bound.
3.1 Active inference model
We formalize body perception as inferring the unobserved body stateµ, e.g., the estimation of the
robot joint angles like shoulder pitch and roll. We deﬁne the robot internal belief as an n-dimensional
vector: µ[d] ∈Rn for each temporal order d. For instance, for ﬁrst-order (velocity) generalized
coordinates the belief is: µ= {µ[0],µ[1]}. The observed variables x are the visual sensory input
xv and the external causal variables ρ: x = {xv,ρ}For instance, the robot has access to visual
information xv (an image of size w×h) and proprioception information (the joint encoder values
q). The causal variables ρare independent variables that produce effects in the world. Finally, let us
deﬁne two generative models that describe the system. The sensory forward model g, which is the
predictor that computes the sensory outcome xv given the internal state µand the body internal state
dynamics f. Both functions can be considered as the approximations that the agent has about the
reality:
xv = g(µ) + wv (6)
µ[1] = f(µ,ρ) + wµ (7)
where wv,wµ are both processes noise and are assumed to be drawn from a multivariate normal
distribution with zero mean and covariance Σv and Σµ respectively.
In order to compute the variational free-energy under the Laplace approximation from Equation (5)
we need the joint density. Assuming independence of the observed variables:
ln p(x,µ) = ln p(xv,ρ,µ) = ln p(xv|µ) + lnp(µ[1]|µ[0],ρ) (8)
where p(xv|µ) is the likelihood of having a visual sensation given the internal state andp(µ[1]|µ[0],ρ)
is the transition dynamics of the latent variables (body state).
Body perception is then instantiated as computing the body state that minimizes the variational
free-energy. This can be performed through gradient optimization ∂F/∂µ. Since the temporal
difference µ[0]
t+1 −µ[0]
t is equal to the ﬁrst-order dynamics µ[1] at equilibrium, this term has to be
included in the computation of ˙µ[0] to ﬁnd a stationary solution during the gradient descent procedure
where the gradient ∂F/∂µvanishes at optimum [3]. Hence,
˙µ[0] −µ[1] = −∂F
∂µ = ∂ln p(xv,ρ,µ)
∂µ = ln p(xv|µ)
∂µ + ln p(µ[1]|µ[0],ρ)
∂µ (9)
In order to compute the likelihoods, we assume that the observed image xv is noisy and follows a
normal distribution with a mean at the value of g(µ) and with variance Σv. Considering that every
pixel contribution is independent, such as Σv = diag(Σv1 ,..., Σvh·w ), the likelihood p(xv|µ) is
obtained as the collection of independent Gaussians:
p(xv|µ) =
h·w∏
k=1
1√
2πΣvk
exp
[
− 1
2Σvk
(xvk −gk(µ))2
]
(10)
Analogously, the density that deﬁnes the latent variable dynamics is also assumed to be noisy and
follows a normal distribution with mean at the value of the function f(µ,ρ) and with variance Σµ:
p(µ[1]|µ[0],ρ) =
n∏
i=1
1√
2πΣµi
exp
[
− 1
2Σµi
(µ[1]
i −fi(µ,ρ))2
]
(11)
Substituting the likelihoods and computing the partial derivatives of Equation 9, the body state is
then given by the following differential equation:
˙µ[0] = µ[1] + ∂g(µ)
∂µ
T
  
mapping
Σ−1
v
precision
(xv −g(µ))  
prediction error
+∂f(µ,ρ)T
∂µ Σ−1
µ (µ[1] −f(µ,ρ)) (12)
4
For notation simplicity, we name the second and the third term as Fg and Ff respectively.
The action is analogously computed. However, the sensory information is only a function of the
action x(a) and therefore it only depends on the free-energy terms with sensory information Fg3:
˙ a=−∂Fg
∂a = −“∂Fg
∂x
∂x
∂a
”=−∂xT
v
∂a Σ−1
v (xv−g(µ))= −∂g(µ)T
∂µ ∆tΣ−1
v (xv−g(µ)) (13)
To derive the last equality, we have employed the same approximation as in [24] assuming that the
actions are joint velocities. In a velocity controller scheme we can approximate the angle change
between two time steps of each joint j as ∂qj/∂aj = ∆t, because the target values of the joint
encoders q are computed as qt+1 = qt + ∆tat and ∆t is a ﬁxed value that deﬁnes the duration of
each iteration. Then, assuming convergence for the sensation values at the equilibrium point with
µ→q and g(µ) →xv, the term ∂xv
∂a can be computed using the following equation:
∂xv
∂aj
= ∂xv
∂qj
∂qj
∂aj
= ∂g(µ)
∂µj
∂µj
∂aj
= ∂g(µ)
∂µj
∆t (14)
The update rule for both µand ais ﬁnally calculated with the ﬁrst-order Euler integration:
µt+1 = µt + ∆t ˙µ at+1 = at + ∆t˙a (15)
3.2 Scaling up with deep autoencoders
In order to perform pixel-based free-energy optimization we compute Equations (12) and (13)
exploiting the forward and backward pass properties of the deep neural network. We approximate the
visual generative model g(µ) and its partial derivative ∂µg(µ) with respect to the internal state by
means of a convolutional decoder.
3.2.1 Prediction of the expected sensation
7 x 10 x 16 14 x 20 x 64 28 x 40 x 16 28 x 40 x 16 28 x 40 x 16
512 Neurons
1120 Neurons
14 x 20 x 64 56 x 80 
FC1 FC2 R UpConv1 Conv1 UpConv2 Conv2 Dropout UpConv3
Figure 2: Network Architecture of the Convolutional Decoder (FC: Fully-connected layer, R: Reshape
operator, UpConv: Transposed Convolution, Conv: Convolution)
We approximate the forward modelg(µ) by means of a generative network, based on the architecture
proposed in [7], described in Fig. 2. It outputs the predicted image given the robot’s n-dimensional
internal belief of the body state µ, e.g., the joint angles of the robot.
The input goes through 2 fully-connected layers (FC1 and FC2). Afterwards, the transposed con-
volution (UpConv) is performed to upsample the image. This deconvolution uses the input as the
weights for the ﬁlters and can be regarded as a backward pass of the standard convolution operator [9].
Following [7], each transposed convolution layer was followed by a standard convolutional layer,
which helps to smooth out the potential artifacts from the transposed convolution step. There is an
additional 1D-Dropout layer before the last transposed convolution layer to avoid overﬁtting and
achieve better generalization performance. All layers use the rectiﬁed linear unit (ReLU) as the
activation function, except for the last layer, where a sigmoid function was used to get pixel intensity
values in the range [0,1]. Throughout the consecutive UpConv-Conv operations in the network, the
number of channels is increased and decreased again to get the required output image size.
3The chain rule in quotation marks of Eq. 13 indicates the abuse of notation. Whenx is an image the gradient
∂x
∂a would be a three-dimensional tensor.
5
3.2.2 Backward pass and mapping to the latent variable
An essential term, for computing both perception and action, is the mapping between the error in the
sensory space and the inferred variables: ∂g(µ)
∂µ . This term is calculated by performing a backward
pass over the convolutional decoder. In fact, we can compute the whole partial derivative of the visual
input term ∂Fg/∂µjust in one forward and backward pass. The reason is that the prediction error
between the expected and observed sensation (xv −g(µ)) multiplied by the inverse variance and the
partial derivative is equivalent to applying the backpropagation algorithm. It is important to note that
when the function g(µ) outputs images of size w×h, ∂g(µ)
∂µ is a three dimensional tensor. We stack
the output into a vector ∈Rw·h (row major). The following equation is obtained:
∂Fg
∂µ =


∂g1,1
∂µ1
∂g1,2
∂µ1
... ∂gw,h
∂µ1
∂g1,1
∂µ2
∂g1,2
∂µ2
... ∂gw,h
∂µ2
... ... ... ...
∂g1,1
∂µ4
∂g1,2
∂µ4
... ∂gw,h
∂µ4


  
( ∂g
∂µ )
T


∂Fg
∂g1,1
∂Fg
∂g1,2
...
∂Fg
∂gw,h


  
∂Fg
∂g
(16)
where −∂Fg
∂gi,l
is given by 1
Σvi,l
(xvi,l −gi,l(µ)). The action is computed by reusing this term and
multiplying it by ∆t.
3.3 Formalizing the task with attractor dynamics
In Active Inference we include the goal as prior in the body state dynamics function f(µ,ρ). For
example, to perform a reaching task, we encode the desired goal of the robot as an instance in the
sensory space (image), which acts as an attractor generated by the causal variableρ. This produces an
error in the inferred state that will promote an action towards the goal. Note that the errorρ−g(µ) is
zero when the prediction matches the desired goal. We deﬁne the body state dynamics with a causal
variable attractor as:
f(µ,ρ) = T(µ)β(ρ−g(µ)) , (17)
where βis a gain parameter that deﬁnes the intensity of the attractor and T(µ) = ∂g(µ)T/∂µis the
mapping from the sensory space (e.g. pixel-domain) to the internal belief µ(e.g. joint space). Note
that this term is obtained through the backward pass of the decoder. Finally, substituting in Eq. (12)
the new dynamics generative model we write the last term ∂Ff/∂µas:
−∂Ff
∂µ = ∂f(µ,ρ)T
∂µ Σ−1
µ
(
µ[1] −∂g(µ)T
∂µ β(ρ−g(µ))
)
(18)
In the ﬁnal model used in the experiments, we have further simpliﬁed this equation by not includ-
ing the ﬁrst-order internal dynamics into the optimization process µ[1] = 0 and noting that the
correct mapping and direction from the sensory space to the latent variable is already provided by
∂g(µ)T/∂µ. Thus, we greedily approximate ∂f(µ,ρ)/∂µto −1, avoiding the Hessian computation
of T but introducing an optimization detriment. With these assumptions, the partial derivative of the
dynamics term becomes:
−∂Ff
∂µ = Σ−1
µ
(∂g(µ)T
∂µ β(ρ−g(µ))
)
= Σ−1
µ f(µ,ρ) (19)
3.4 PixelAI algorithm
Algorithm 1 summarizes the proposed method. In the robot body perception and action application,
xv is set to the image provided by the robot monocular camera and the decoder input becomes the
robot proprioception (e.g. joint angles). The convolutional decoder is trained with this mapping
obtaining a predictor of the visual forward model. The prediction error ev is the difference between
the expected visual sensation and the observation (line 6). The variational free-energy optimization,
for perception (line 7) and action (line 8), updates the differential equations that drive the state
6
estimation and control. Finally, we added in the dynamics term the possibility of inputting desired
goals in the visual space (line 13). Although this implementation assumes that µ[1] = 0, it is straight
forward to add the 1st order dynamics when there is velocity image information or joint encoders [24].
Algorithm 1PixelAI: Deep Active Inference Algorithm
Require: Σv,Σµ,β, ∆t
1: µ←Initial joints angle estimation
2: while (true) do
3: xv ←Resize(camera image) ⊿Visual Sensation
4: g(µ) ←ConvDecoder.forward(µ)
5: ∂g ←ConvDecoder.backward(µ)
6: ev = (xv −g(µ)) ⊿Prediction error
7: ˙µ= KΣv ∂gTev/Σv 4
8: ˙a = −(∂gTev/Σv)∆t
9: if ∃ρthen ⊿Goal attractor ρdynamics
10: ef = β(ρ−g(µ))
11: ˙µ= ˙µ+ ∂gTef/Σµ
12: µ= µ+ ∆t ˙µ; a = a + ∆t˙a ⊿1st order Euler integration
13: SetVelocityController(a)
4 Experiments
We tested the PixelAI in both simulated and real Aldebaran NAO humanoid robot (Fig. 4). We used
the left arm to test both perception and action schemes. The dataset and the code to replicate the
experiments can be found in tobereleased.
(a) Simulated Nao
 (b) Real Nao
Figure 3: Experimental setup in simulation (Gazebo) and with the real robot.
4.1 Visual forward model
Data acquisition & preprocessing. The dataset used to train the model consisted of 3200 data
samples of the left arm elbow and shoulder joint readings (q = [q1,q2,q3,q4]T) and the observed
images xv obtained through NAO’s bottom camera5. Data samples were generated using three
different methods and later concatenated (∼25%, ∼20% and ∼55% for each method).
In the ﬁrst method, the joint angles were randomly drawn from a uniform distribution in the range
of the joint limits. Afterwards, it required the manual elimination of samples where the robot’s arm
was out of the camera frame. The ratio of the acquired images with the robot hand centered in the
camera image was signiﬁcantly lower than the images with the hand located at the corners of the
4The gain parameter KΣv is added to allow the model to generate large action values without increasing the
internal belief increments.
5In simulation, the color of the right arm was changed to dark grey to achieve contrast with the grey
background in the camera images.
7
frame. To reduce this drawback, in the second method, the robot’s arm was manually moved by an
operator and the joint angle readings were recorded during these trajectories. This way, a subset of
data was obtained, where the robot hand was centered in the image. Finally, in the third method, a
multivariate Gaussian was ﬁt to the second subset using the expectation-maximization algorithm and
random samples were drawn from this Gaussian for the third and ﬁnal part of the dataset. The goal
was to introduce randomness to the centered-images and not be limited to the operator’s choice of
trajectories.
For the images collected in the Gazebo NAO Simulator, the only preprocessing step performed was
re-sizing the image of size 640 ×480 to 80 ×56. For the real NAO, the images were obtained on
a green background (Fig. 3(b)) and the following preprocessing steps were performed: 1) median
ﬁltering with kernel size 11 on the original image, 2) masking the monochrome background,e.g.
green, in the HSV color space and replacing it with dark gray to ensure contrast 3) converting the
image to grayscale, and 4) resizing image to dimensions 80 ×56.
Training. The convolutional decoder was trained using the ADAM optimizer using a mini-batch of
size 200 samples and an initial learning rate of α= 10−4 with exponential decay of 0.95 every 5000
steps. The training was stopped after ca. 7000 iterations for the simulated NAO dataset and 12000
iterations for the real NAO dataset to avoid overﬁtting, as the test set error started to increase for the
corresponding model. The output of the second fully connected layer (FC2) was an 1120-dimensional
vector that was reshaped into a 7 ×10 ×16 tensor. The UpConv layers all use stride equal to 2 and a
padding of 1. Moreover, a kernel with a size of 4 ×4 was chosen to avoid checkerboard artifacts due
to uneven overlap [23]. Convolutional layers used a kernel size 3, stride 1 and padding 1. The ratio of
drop-out was set to 0.15. The ﬁnal layer outputs a 1 ×56 ×80 image corresponding to a grayscale
image.
4.2 Benchmark for PixelAI
A benchmark with three levels of difﬁculty was created to evaluate the performance of PixelAI on
randomized samples for both perceptual and active inference. A set of 50 different cores (i.e. images
of the arm) were generated by sampling the multivariate Gaussian distribution (see method 2 in
section 4.1). A subset of the generated cores is shown in Fig. 5(a). For each of the cores, 10 different
random tests were performed. In total, there were 2500 trials composed of 5 runs of 500 testing
image arm poses per benchmark level 6. The test samples for each core were generated differently
depending on the benchmark level:
• Level 1 (close similar poses): One of the 4 joints was chosen randomly and a random
perturbation ±[5◦,10◦] sampled from a uniform distribution was added to the joint angle
value to generate the new test sample.
• Level 2 (far similar poses): For all of the 4 joints, a random perturbation ±[5◦,10◦] was
sampled from a uniform distribution and added to the core joint angles.
• Level 3 (random): For each core, 10 different cores were chosen randomly and used as the
test samples.
Perceptual Inference. In order to evaluate the body perception performance, the robot has to infer
its real arm pose just using visual information. The robot’s arm was initialized to each core pose
and then 10 separate test runs were performed, where the internal belief was set to a perturbed value
of the corresponding pose. These tests are static in nature, i.e. the change solely takes place in the
internal predictions of the robot. The goal is that the robot internal belief µconverges to the true arm
position, which is equal to the joint angles of the chosen core.
Active Inference. In order to evaluate the perception and action performance, the core poses were
treated as the desired goal (image) encoded as an attractor in the model. Again for each core, 10
separate test runs were performed. In this case, the robot arm was initialized to a core pose and also
the initial internal belief was set to the current joint measurements: µ= q. In each test, the goal
was that the robot reached the desired imagined arm pose. The update of the internal belief should
generate an action to compensate for the mismatch between the current and the predicted visual
6For the real robot benchmark tests (perceptual inference) only a subset of 20 cores were used.
8
sensations. In a successful test run, the robot arm should move to the imagined arm position and the
internal belief should also converge to the imagined joint angles, so that: xv = g(µ) = ρ.
Algorithm Parameters. The parameters of the Pixel AI algorithm are Σv,KΣv ,β, ∆t, which
were determined empirically 7. The intuition behind the variance terms is as follows: the prediction
errors get multiplied by the inverse of the variances so these actually weigh the relevance of the
corresponding sensory information error [3]. The βterm, that is part of the attractor dynamics (see Eq.
17), essentially has the same effect and it controls how much we want to push the internal belief in
the direction of the attractor. Finally, the ∆t depends on the internal time of the robot loop execution.
The parameters used for benchmark levels 1 and 2 were the same. For level 3 in perceptual inference,
we used a smaller Σv until the visual prediction error was below a certain threshold ( 0.01). This
introduces a new model parameter γΣv which is used to scale Σv, once the error threshold is reached.
This heuristic method of adaptation helped speed up the convergence for the more complex level 3
trajectories. Finally, the generated actions (velocity values) were clipped so that each joint could not
move more than [−2◦; 2◦] each time step.
Table 1: PixelAI Parameters used for the perceptual and active inference benchmarks in simulation.
Σv β K Σv γΣv
Active Inference
Level 1 6 ×102 2 ×10−5 10−3 1
Level 2 2 ×102 5 ×10−5 10−3 1
Level 3 20 5 ×10−4 10−3 1
Perceptual Inference
Level 1 2 ×104 - - 1
Level 2 2 ×104 - - 1
Level 3 2 ×103 - - 10
5 Results
5.1 Statistical analysis of perception and action in simulation
First, perceptual inference tests were run for 5000-time steps for all 3 levels. An example of the
perceptual inference for each level is depicted in Fig. 5.1. For level 1 and 2, with less than 150
iterations, the algorithm converged to the ground truth, while inferring the body location from a
totally random initialization (level 3) raised considerably the complexity. Table 2 shows the resulting
average of all trials of the mean absolute joint errors (|qtrue −µ|). Level 1 and 2 results converged
to internal belief values successfully. Figure 5(c) shows the error during the optimization process.
Shoulder pitch and shoulder roll angles were estimated with better accuracy compared to the elbow
angles. This is due to the fact that a small change in the shoulder pitch angle yields to a greater
difference in the visual ﬁeld in comparison with the same amount of change in the elbow roll angle.
Since PixelAI achieves perception by minimizing the visual prediction error, the accuracy increases
when the pixel-based difference is stronger. Therefore, the mean error and standard deviation increase
for the elbow joint angle estimations.
The errors in level 3, where the robot had to converge to random arm locations, were larger compared
to levels 1 and 2, as shown in Fig. 5(c). This is due to two reasons. The ﬁrst one is the local minima
problem inherent of our gradient descent approach. The second one affects the desired joint position,
as depicted in Fig. 6: several joint solutions have small visual prediction error increasing the risk of
getting into a local minimum.
Secondly, active inference tests with goal images were performed using the simulated NAO for 1000
time steps in the benchmark levels. The results for levels 1 and 2 are shown in Fig. 8. The joint
encoder readings followed the internal belief values through the actions generated by free-energy
7Since βand Σµ achieve the same effect,Σµ is always set to 1 and onlyβis kept as a parameter, eliminating
redundancy.
9
Figure 4: Example of the internal trajectories of the latent space during the perceptual inference tests
for three different levels of difﬁculty for core 3.
Table 2: Perceptual inference joint angles mean absolute error (in degrees).
Level Shoulder Pitch Shoulder Roll Elbow Yaw Elbow Roll
1 0.259 ±0.343 0.333 ±0.408 0.848 ±0.956 0.940 ±1.061
2 0.387 ±0.711 0.617 ±0.978 1.185 ±1.475 1.567 ±2.184
3 5.316 ±11.057 5.609 ±7.679 17.64 ±26.08 12.25 ±15.41
(a) Benchmark cores subset
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.00
0.02
0.04
0.06
0.08
MSE(sv − g(µ))
Level 1
Level 2
Level 3 (b) Visual Prediction Error
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
∥sp − µ∥2 (radians)
Level 1
Level 2
Level 3 (c) L2 Norm of sp − µ.
Figure 5: (a): A subset of the cores used for the benchmarking tests. (b)-(c): Simulated Nao
perceptual inference results for all levels (1-3) of the benchmark are shown. The L2-Norm of the
error between internal belief µand sp is plotted, as well as the visual prediction error.
optimization. Level 3 performance detriment shows that interacting is more complex than perceiving
as it includes the body and the world constraints.
5.2 Active inferencein the real robot
Table 3: Perceptual inference joint angles mean absolute error in the real robot. (in degrees).
Level Shoulder Pitch Shoulder Roll Elbow Yaw Elbow Roll
1 1.326 ±0.823 0.679 ±0.773 1.569 ±1.441 1.9690 ±2.032
2 1.862 ±1.846 2.221 ±3.038 2.935 ±3.311 3.807 ±3.323
3 9.799 ±12.63 12.77 ±10.91 29.35 ±35.48 21.82 ±17.23
We tested the proposed algorithm in the real robot. Conversely to simulation, the robot’s movements
are imprecise due to the mechanical backlash in the actuators (±5◦) [12]. Furthermore, we deployed
the velocity controller over the built-in NAO position control yielding to a bad synchronization
between algorithm and the real movement. For instance, the robot had to wait for the generated
actions to be large enough, in order to send the commands to the controller. This caused the
movements to be unsmooth. Moreover, as we did not have direct access to the motor driver, the time
10
(a) True arm position
(camera image).
(b) Initial internal belief
deep convolutional decoder
output g(µ) (overlaid on the
camera image).
(c) Deep convolutional
decoder output g(µ) for end
internal belief (overlaid on
the camera image).
0 200 400 600 800 1000
Timestep (t)
0.00
0.01
0.02
0.03
0.04
0.05
0.06
MSE[sv − g(µ)]
(d) Visual prediction error.
0 200 400 600 800 1000
Timestep (t)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
∥µ − sp∥ (radians) (e) L2 Norm of µ − sp.
Figure 6: Example of the local minima problem in level 3 perceptual inference tests.
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.00
0.02
0.04
0.06
0.08
MSE(sv − g(µ))
Level 1
Level 2
Level 3
(a) Visual Prediction Error
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
∥sp − µ∥2 (radians)
Level 1
Level 2
Level 3 (b) L2 Norm of µ − sp
Figure 7: Real NAO perceptual inference results for all 3 benchmark levels
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.00
0.02
0.04
0.06
0.08
MSE(ρ − sv)
Level 1
Level 2
Level 3
(a) Visual error
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
∥sp − qattr∥2 (radians)
Level 1
Level 2
Level 3 (b) L2-Norm of Joint Angle Errors
Figure 8: Simulated NAO active inference test results for all three levels: (a) Visual error between
the visual attractor ρand the observed camera image sv. (b) L2-Norm of the error between the joint
angles of the attractor position qattr and the proprioceptive sensor readings sp
.
the action was executed had a large mismatch between the internal error and the actual arm position,
11
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.00
0.01
0.02
0.03
0.04
0.05
0.06
MSE
ρ − sv
sv − g(µ)
ρ − g(µ)
(a) Level 2: Visual error
0 200 400 600 800 1000 1200 1400
Timestep (t)
0.00
0.05
0.10
0.15
0.20
0.25
∥qerr∥2 (radians)
sp − qattr
µ − qattr (b) Level 2: L2-Norm of Joint Angle Errors
Figure 9: Closer look at simulated NAO active inference results for level 2
resulting in a desynchronization between the internal model and the real world, which could cause
the system to diverge.
Furthermore, the visual forward model was expected to model the more complex structure of the real
robot hand, that is subject to lighting differences and has a reﬂective surface. Unlike in the simulator,
the same conditions cannot be restored perfectly in the real world, so the model training is always
subject to additional noise in the dataset. We used the same deep convolutional decoder architecture
for our tests on the real robot as well. Low training error was achieved on the training dataset (MSE in
pixel-intensity: ca. 0.0015). The results of the perceptual inference for real NAO on all 3 benchmark
levels are shown in Fig. 7. Similar behaviours of perceptual convergence were found in level 1 and
2, while level 3 had a larger error due to the local minima. Figure 10 shows the PixelAI algorithm
running on the robot.
Figure 10: PixelAI test on the real Nao. µis the inferred state, qis the real joint angles readings, goal
is the ground truth goal angles and 0.05steps= 1s. (bottom row) Arm sequence: goal image and
Nao visual input are overimposed.
6 Discussion
We have shown that variational free-energy optimization can work as a general inner mechanism for
both perception (estimation) and action (control). Our algorithm extends previous active inference
works tackling high-dimensional visual inputs and providing sensory generative models learning.
This prediction error variant of control as inference [27] exploits the representation learnt to indirectly
generate the actions without a policy. The robot is producing the actions reach the desired goal in the
visual space without learning the explicit policy.
There are other interesting advantages of our proposed approach. First, it is neuroscience-inspired,
speciﬁcally it grounds on predictive coding approach and the free energy principle [ 25, 11]. This
means that we can directly make comparisons with human body perception [ 15]. Secondly, the
12
features learnt instead of being bound to the task, they are grounded to the body. Hence, learnt
representations have a physical meaning and can be used to solve other tasks. Furthermore, it does
not need rewards engineering and works directly in sensory space.
A deeper analysis should be performed to evaluate the limits of the algorithm to scale to sequential
complex tasks. For instance, we can design a system that generates desired internal beliefs that are
transformed into expected sensations that will drive the agent towards the goal, in line with the view
of perception as a hierarchical dynamical system [26].
The free energy formulation [11] and variational autoencoders [17] share the same theory and solve
similar problems, as shown in [ 32], where the action is directly outputted from the variational
autoencoder. However, autoencoders architecture do not account for the original ideas from the
Helmholtz machine [5]. Perception should be an active continuous process [1]. Here we have shown
how we can use variational inference to provide the active adaptation and interpolation to online input
exploiting the forward and backward passes of the neural network. This allows us to incorporate priors
(top-down modulation) while maintaining inference dynamics over the observations (bottom-up).
Besides, our PixelAI algorithm complements the active inference community effort to provide scalable
models for real applications [28, 21].
7 Conclusions
We have described a Pixel-based deep Active Inference algorithm and applied it for robot body
perception and action. Our algorithm enabled estimation of the robot arm joint states just using a
monocular camera input and perform goal driven behaviours using imaginary goals in the visual
space. Statistical results showed convergence in both perception and action in different levels of
difﬁculty with a larger error when dealing with totally random arm poses. This neuroscience-inspired
approach is thought to make deeper interpretations than conventional engineering solutions [ 13],
giving some grounding for novel machine learning developments, especially for body perception and
action.
References
[1] Ruzena Bajcsy, Yiannis Aloimonos, and John K Tsotsos. Revisiting active perception. Au-
tonomous Robots, 42(2):177–196, 2018.
[2] Matthew Botvinick and Jonathan Cohen. Rubber hands ‘feel’touch that eyes see. Nature,
391(6669):756, 1998.
[3] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free en-
ergy principle for action and perception: A mathematical review. Journal of Mathematical
Psychology, 2017.
[4] Cristina Garcia Cifuentes, Jan Issac, Manuel Wüthrich, Stefan Schaal, and Jeannette Bohg. Prob-
abilistic articulated real-time tracking for robot manipulation. IEEE Robotics and Automation
Letters, 2(2):577–584, 2016.
[5] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz
machine. Neural computation, 7(5):889–904, 1995.
[6] German Diez-Valencia, Takuya Ohashi, Pablo Lanillos, and Gordon Cheng. Sensorimotor
learning for artiﬁcial body perception. arXiv preprint arXiv:1901.09792, 2019.
[7] Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, and Thomas Brox. Learning
to generate chairs, tables and cars with convolutional networks. IEEE transactions on pattern
analysis and machine intelligence, 39(4):692–705, 2016.
[8] Kenji Doya. What are the computations of the cerebellum, the basal ganglia and the cerebral
cortex? Neural networks, 12(7-8):961–974, 1999.
[9] Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning.
arXiv preprint arXiv:1603.07285, 2016.
[10] Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny.
Variational free energy and the laplace approximation. Neuroimage, 34(1):220–234, 2007.
13
[11] Karl J. Friston. The free-energy principle: a uniﬁed brain theory? Nature Reviews. Neuroscience,
11:127–138, 02 2010.
[12] David Gouaillier, Cyrille Collette, and Chris Kilner. Omni-directional closed-loop walk for nao.
In 2010 10th IEEE-RAS International Conference on Humanoid Robots, pages 448–454. IEEE,
2010.
[13] Demis Hassabis, Dharshan Kumaran, Christopher Summerﬁeld, and Matthew Botvinick.
Neuroscience-inspired artiﬁcial intelligence. Neuron, 95(2):245–258, 2017.
[14] Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and
helmholtz free energy. In Advances in neural information processing systems , pages 3–10,
1994.
[15] Nina-Alisa Hinz, Pablo Lanillos, Hermann Mueller, and Gordon Cheng. Drifting perceptual
patterns suggest prediction errors fusion rather than hypothesis selection: replicating the rubber-
hand illusion on a robot. arXiv preprint arXiv:1806.06809, 2018.
[16] Seth Hutchinson, Gregory D Hager, and Peter I Corke. A tutorial on visual servo control. IEEE
transactions on robotics and automation, 12(5):651–670, 1996.
[17] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
[18] Pablo Lanillos, Emmanuel Dean-Leon, and Gordon Cheng. Yielding self-perception in robots
through sensorimotor contingencies. IEEE Trans. on Cognitive and Developmental Systems,
(99):1–1, 2016.
[19] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.
[20] Kingson Man and Antonio Damasio. Homeostasis and soft robotics in the design of feeling
machines. Nature Machine Intelligence, 1(10):446–452, 2019.
[21] Beren Millidge. Deep active inference as variational policy gradients. arXiv preprint
arXiv:1907.03876, 2019.
[22] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Vi-
sual reinforcement learning with imagined goals. In Advances in Neural Information Processing
Systems, pages 9191–9200, 2018.
[23] Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts.
Distill, 1(10):e3, 2016.
[24] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. Active inference body perception and
action for humanoid robots. arXiv preprint arXiv:1906.03022, 2019.
[25] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-ﬁeld effects. Nature neuroscience, 2(1):79–87,
1999.
[26] Jun Tani. Exploring robotic minds: actions, symbols, and consciousness as self-organizing
dynamic phenomena. Oxford University Press, 2016.
[27] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of
the 26th annual international conference on machine learning, pages 1049–1056, 2009.
[28] Alexander Tschantz, Manuel Baltieri, Anil Seth, Christopher L Buckley, et al. Scaling active
inference. arXiv preprint arXiv:1911.10601, 2019.
[29] Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and
variational inference. Foundations and Trends in Machine Learning, 1(1):1–305, 2007.
[30] Daniel M Wolpert, Jörn Diedrichsen, and J Randall Flanagan. Principles of sensorimotor
learning. Nature Reviews Neuroscience, 12(12):739, 2011.
[31] Yasunori Yamada, Hoshinori Kanazawa, Sho Iwasaki, Yuki Tsukahara, Osuke Iwata, Shigehito
Yamada, and Yasuo Kuniyoshi. An embodied brain model of the human foetus. Scientiﬁc
Reports, 6, 2016.
[32] Martina Zambelli, Antoine Cully, and Yiannis Demiris. Multimodal representation models for
prediction and control from partial information.Robotics and Autonomous Systems, 123:103312,
2020.
14