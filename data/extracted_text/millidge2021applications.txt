Applications of the Free Energy Principle to
Machine Learning and Neuroscience
Beren Millidge
T
H
E
U N I V E R S IT
Y
O
F
E D I N B U
R
G
H
Doctor of Philosophy
Institute for Adaptive and Neural Computation
School of Informatics
University of Edinburgh
2021
arXiv:2107.00140v1  [cs.AI]  30 Jun 2021
Abstract
In this thesis, we explore and apply methods inspired by the free energy principle to
two important areas in machine learning and neuroscience. The free energy principle
is a general mathematical theory of the necessary information-theoretic behaviours
of systems which maintain a separation from their environment. A core postulate of
the theory is that complex systems can be seen as performing variational Bayesian
inference and minimizing an information-theoretic quantity called the variational free
energy. The free energy principle originated in, and has been extremely inﬂuential in
theoretical neuroscience, having spawned a number of neurophysiologically realistic
process theories, and maintaining close links with Bayesian Brain viewpoints.
The thesis is split into three main parts where we apply methods and insights from the
free energy principle to understand questions ﬁrst in perception, then action, and ﬁnally
learning. Speciﬁcally, in the ﬁrst section, we focus on the theory of predictive coding,
a neurobiologically plausible process theory derived from the free energy principle
under certain assumptions, which argues that the primary function of the brain is to
minimize prediction errors. We focus on scaling up predictive coding architectures and
simulate large-scale predictive coding networks for perception on machine learning
benchmarks; we investigate predictive coding’s relationship to other classical ﬁltering
algorithms, and we demonstrate that many biologically implausible aspects of current
models of predictive coding can be relaxed without unduly harming the performance
of predictive coding models which allows for a potentially more literal translation of
predictive coding theory into cortical microcircuits.
In the second part of the thesis, we focus on the application of methods deriving from the
free energy principle to action. We study the extension of methods of ‘active inference’,
a neurobiologically grounded account of action through variational message passing,
to utilize deep artiﬁcial neural networks, allowing these methods to ‘scale up’ to be
competitive with state of the art deep reinforcement learning methods. Additionally, we
i
show that these active inference inspired methods can bring conceptual clarity and novel
perspectives to deep reinforcement learning. We show how active inference reveals the
importance of deep generative models and model-based planning for adaptive action,
as well as information-seeking exploration which arises under a uniﬁed mathematical
framework from active inference. Finally, we provide a uniﬁed mathematically princi-
pled framework for understanding and deriving many information-seeking exploration
objectives through the lens of a dichotomy between ‘evidence’ and ‘divergence’ ob-
jectives. We show that this distinction is crucial for understanding and relating the
many exploratory objectives in both the reinforcement learning, active inference, and
cognitive science communities and that this provides a general mathematical framework
for specifying the objectives underlying intelligent, adaptive behaviour.
Finally, we focus on applications of the free energy principle to questions of learning
where we attempt to understand how credit assignment can take place in the brain.
First, we demonstrate that, under certain conditions, the predictive coding algorithm can
closely approximate the backpropagation of error algorithm along arbitrary computation
graphs, which underlies the training of essentially all contemporary machine learning
architectures, thus indicating a potential path to the direct implementation of machine
learning algorithms in neural circuitry. Finally, we explore other algorithms for biologi-
cally plausible credit assignment in the brain, and present Activation Relaxation, a novel
algorithm which can approximate backprop using only local learning rules which are
substantially simpler than those necessary for predictive coding. We additionally show
that the some relaxations that apply to predictive coding, also work for the activation
relaxation algorithm, thus producing an extremely elegant and effective algorithm for
local approximations to backprop in the brain.
In sum, we believe we have demonstrated the theoretical utility of the free energy
principle, by demonstrating how methods inspired by it can interface productively with
other ﬁelds, speciﬁcally neuroscience and machine learning, to develop and improve
ii
existing methods, as well as inspire novel advances, in all three areas of perception,
action, and learning. Moreover, throughout this thesis, we demonstrate implicitly, the
theoretical beneﬁt brought about by the FEPs uniﬁed treatment of these seemingly
disparate processes, under the rubric of free energy minimization.
iii
Acknowledgements
I would like to thank my supervisor, Richard Shillcock, for all his help and advice over
the years; for his giving me freedom to work on the topics in this thesis even though they
did not align with his planned research trajectory, and for his perseverance in handling
my endless drafts. Secondly, a huge thanks to Christopher L Buckley, Alec Tschantz,
and Anil Seth for hosting me for a year at the Sackler Centre and the Evolutionary and
Adaptive Systems Group (EASY) at the University of Sussex. You have all taught me
so much about the craft of research, and without your mentorship and guidance, I would
not be half the researcher I am today. I would also like to thank Conor Heins for many
stimulating conversations around the free energy principle and related topics. Finally,
and above all, I would like to thank my wife, Mycah Banks, for her unending love and
support throughout this entire process; her aid with proofreading and ﬁgure preparation
for many papers, and her sacriﬁce in putting up with a husband who always has some
more research to do. Without you, none of this would be possible. Thank you, Mycah.
iv
Declaration
I declare that this thesis was composed by myself, that the work contained herein is my
own except where explicitly stated otherwise in the text, and that this work has not been
submitted for any other degree or professional qualiﬁcation except as speciﬁed.
(Beren Millidge)
v
Table of Contents
1 Introduction 1
1.1 Thesis Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Statement of Contributions . . . . . . . . . . . . . . . . . . . . . . . 7
1.2.1 Included in Thesis . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.2 Not Included in the Thesis . . . . . . . . . . . . . . . . . . . 13
2 The Free Energy Principle 16
2.1 History and Logical Structure . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.3 Markov Blankets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4 Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5 Intrinsic and Extrinsic information geometries . . . . . . . . . . . . . 37
2.6 Self-Organization and Variational Inference . . . . . . . . . . . . . . 40
2.7 The Expected Free Energy and Active Inference . . . . . . . . . . . . 47
2.8 Philosophical Status of the FEP . . . . . . . . . . . . . . . . . . . . . 50
2.9 Discussion of Assumptions required for the FEP . . . . . . . . . . . . 57
2.9.1 Assumptions on the Form of the Langevin Dynamics . . . . . 60
2.9.2 The Markov Blanket Condition . . . . . . . . . . . . . . . . 61
2.9.3 Assumptions of the free energy Lemma . . . . . . . . . . . . 63
2.10 Active Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.10.1 Discrete State-space models and Perception . . . . . . . . . . 71
vi
2.10.2 Action Selection and the Expected Free Energy . . . . . . . . 72
2.11 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3 Predictive Coding 76
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
3.2 Predictive Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.3 Hierarchical predictive coding . . . . . . . . . . . . . . . . . . . . . 83
3.3.1 Dynamical Predictive coding . . . . . . . . . . . . . . . . . . 90
3.4 Predictive Coding and Kalman Filtering . . . . . . . . . . . . . . . . 101
3.4.1 The Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . 105
3.4.2 Predictive Coding as Kalman Filtering . . . . . . . . . . . . . 107
3.4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.5 Relaxed Predictive Coding . . . . . . . . . . . . . . . . . . . . . . . 122
3.5.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4 Scaling Active Inference 144
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . 145
4.1.2 Deep Reinforcement Learning . . . . . . . . . . . . . . . . . 154
4.1.3 Model-free vs Model-based . . . . . . . . . . . . . . . . . . 156
4.1.4 Exploration and Exploitation . . . . . . . . . . . . . . . . . . 159
4.1.5 Control as Inference . . . . . . . . . . . . . . . . . . . . . . 163
4.2 Deep Active Inference . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.2.1 Model-Free: Active Inference as Variational Policy Gradients 171
4.2.2 Model-based: Reinforcement Learning through Active Inference189
vii
4.2.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.2.4 Iterative and Amortised Inference . . . . . . . . . . . . . . . 201
4.2.5 Control as Hybrid Inference . . . . . . . . . . . . . . . . . . 206
4.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5 The Mathematical Origins of Exploration 218
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.2 Origins of the Expected Free Energy . . . . . . . . . . . . . . . . . . 221
5.2.1 Control as Inference and Active Inference . . . . . . . . . . . 228
5.3 Evidence and Divergence Objectives . . . . . . . . . . . . . . . . . . 234
5.3.1 Control as Inference . . . . . . . . . . . . . . . . . . . . . . 241
5.3.2 KL Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.3.3 Active Inference . . . . . . . . . . . . . . . . . . . . . . . . 243
5.3.4 Action and Perception as Divergence Minimization . . . . . . 245
5.4 Towards a General Theory of Mean-Field Variational Objectives for
Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
5.4.1 Encoding Value . . . . . . . . . . . . . . . . . . . . . . . . . 250
5.4.2 General Graphical Models . . . . . . . . . . . . . . . . . . . 261
5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
6 Credit Assignment in the Brain 268
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
6.1.1 Backpropagation in the Brain . . . . . . . . . . . . . . . . . 270
6.2 Predictive Coding Approximates Backprop Along Arbitrary Computa-
tion Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
6.2.1 Methods and Results . . . . . . . . . . . . . . . . . . . . . . 286
6.2.2 RNN and LSTM . . . . . . . . . . . . . . . . . . . . . . . . 292
6.3 Interim Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
6.4 Activation Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 305
viii
6.4.1 Method and Results . . . . . . . . . . . . . . . . . . . . . . 309
6.4.2 Loosening Constraints . . . . . . . . . . . . . . . . . . . . . 311
6.4.3 Interim Discussion . . . . . . . . . . . . . . . . . . . . . . . 318
6.5 Three-Factor Learning Rules and a Direct Implementation . . . . . . 322
6.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
7 Discussion 333
7.1 Question 1: Scaling Active Inference . . . . . . . . . . . . . . . . . . 337
7.2 Question 2: The Mathematical Origins of Exploration . . . . . . . . . 345
7.3 Question 3: Credit Assignment in the Brain . . . . . . . . . . . . . . 352
7.4 Closing Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
A Derivation of Kalman Filtering Equations from Bayes’ Rule 367
B Appendix B: Equations of the LSTM cell 370
C Appendix C: Predictive Coding Under the Laplace Approximation 372
References 375
ix
List of Figures
2.1 The logical ﬂow of the argument of the FEP from the initial formulation
to the crucial approximate Bayesian inference lemma. We begin with a
setting of random Langevin stochastic dynamical systems, which pos-
sess a non-equilibrium-steady state. By applying the Ao decomposition,
we can understand the dynamics in terms of a gradient descent upon
the surprisal. Upon the addition of a Markov Blanket partition, we can
express subsets in terms of their own marginal ﬂows via the marginal
ﬂow lemma. If we then identify the internal states as parametrizing
a variational distribution over the external states, we can interpret the
marginal ﬂow on the surprisal as a ﬂow on the variational free energy,
under the Laplace approximation. . . . . . . . . . . . . . . . . . . . 20
x
2.2 The intuition behind the Markov Blanket partition. The brain (or bacil-
lus) consists of internal states µ which are separated from the outside
world (external states η by the blanket states b, which can themselves
be partitioned into sensory states s, representing the sensory epithelia,
and which are directly inﬂuenced by external states, and active states a
representing the organisms effectors and which are directly inﬂuenced
by internal states, and act on external states. We see that perception
concerns the minimization of free energy of the internal states, while
action concerns the minimization of the expected free energy of the
active states. Figure originally appeared in Friston (2019a) . . . . . . 32
3.1 MNIST digits in the training set recreated by the network. Top row the
actual digits, bottom row, the predictive reconstructions. . . . . . . . 86
3.2 Unseen MNIST digits in the test set recreated by the network. Top row
the actual digits, bottom row, the predictive reconstructions. . . . . . . 87
3.3 Images of hallucinated digits "dreamt" by the network. These were
generated by sampling the latent space around the representations of
some exemplar digits in the latent space, and then letting the predictive
coding network generate its prediction from the chosen latent state. . . 87
3.4 A PCA clustering plot of the values of test MNIST digits in the latent
space. Even though the 20 dimensional latent space has been reduced
down to two, clusters are still visible. For instance, all the 1s are
clustered in the top left corner. We thus see that predictive coding
appears to be a powerful and fully unsupervised learning algorithm,
capable of separating out distinct digits in the latent space, despite
not being trained with any label information at all – and purely on
reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
xi
3.5 Test set CIFAR digits reconstructed by the network. The ﬁrst of the two
lines is the image and the second is the reconstruction. The network is
extremely good at reconstructing CIFAR images. . . . . . . . . . . . 89
3.6 The CIFAR model interpolating between a horse and a cat. Read the
images left to right top to bottom - like text. The interpolation is done
by stepping in the latent space from the representation of the ﬁrst image
in the direction of the second until it is reached. . . . . . . . . . . . . 90
3.7 Prediction errors and prediction for simple toy dynamical models. The
task of the dynamical predictive coding model is to learn to predict
a sinewave using only the ﬁrst two dynamical orders – so including
position, velocity, and acceleration. The model starts from randomly
initialized parameters. We see that the model very quickly learns to
match the incoming sine wave observations with only minimal error at
the beginning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
3.8 Dynamical models tested on more challenging sawtooth and square
wave inputs. The model was randomly initialized and only modelled the
ﬁrst two dynamical orders (so position, velocity, acceleration). Apart
from a brief initial period of uncertainty, the model rapidly learned to
predict these more challenging wave shapes. . . . . . . . . . . . . . . 99
3.9 The training graphs of the full construct model. It can successfully pre-
dict the ﬁrst three temporal derivatives of a sine wave, and also minimise
prediction error up to multiple hierarchical layers. The full construct
model was randomly initialized, and learnt both parameters and inferred
states purely online – thus achieving a ‘double deconvolution’ (Friston,
Trujillo-Barreto, & Daunizeau, 2008). . . . . . . . . . . . . . . . . . 100
xii
3.10 The true dynamics, control input, and observations generated by a
random C matrix. These are the source of truth that the predictive
coding Kalman ﬁlter tries to approximate. The observations differ
substantially from the true dynamics due to the random C matrix, which
makes the inference problem faced by the predictive coding ﬁlter much
more challenging, since it must de-scramble the observations to infer
the true dynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.11 Tracking performance of our gradient ﬁlter compared to the true val-
ues and the analytical Kalman Filter.We show the tracking over 2000
timesteps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.12 Here, we zoom in on 100 timestep period to demonstrate tracking
performance in miniature and the effect of few gradient updates. In this
case, we plotted the estimates after only 5 steps. Even two steps often
sufﬁce (with a large learning rate) to provide very accurate estimates . 115
3.14 Filtering performance for adaptively learning both the A and B matrices
in concert (second row). The ﬁltering behaviour of the of the randomly
initialized ﬁlters without adaptive learning is also shown. Importantly,
with learning the estimated position, velocity, and accelerations track
their true values precisely while the estimates without learning and just
randomly initialized A or A and B matrices rapidly diverge from the truth.118
3.15 Very poor tracking behaviour with a learnt C matrix. This is despite
the fact that the Bayesian loss function rapidly decreases to a minimum.
This shows that the ﬁlter can ﬁnd a prediction-error minimizing "so-
lution" which almost arbitrarily departs from reality if the C-matrix is
randomized. Panel D shows the loss computed by the network which
rapidly declines, even while the predictions rapidly diverge from the
truth (Panels a,b,c) . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
xiii
3.16 The weight transport problem and our solution. On the left is the stan-
dard predictive coding network architecture. Our diagram represents
the prediction errors ε of one layer receiving predictions and transmitted
prediction errors to the value neurons of the layer above. Prediction
errors are transmitted upwards using the same weight matrix θT as
the predictions are transmitted downwards. On the right, our solution
eschews this biological implausibility by proposing a separate set of
backwards weight ˜θ (in red), which are learned separately using an
additional Hebbian learning rule. . . . . . . . . . . . . . . . . . . . . 129
3.17 Test accuracy of predictive coding networks with both learnt backwards
weights, and the ideal weight transposes with both relu and tanh ac-
tivation functions on the MNIST and FashionMNIST datasets. Both
networks obtain almost identical learning curves, thus suggesting that
learnt backwards weights allow for a solution to the weight-transport
problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.18 Test accuracy of predictive coding networks with and without the non-
linear derivative term, using relu and tanh activation functions on the
MNIST and FashionMNIST datasets. We ﬁnd that on the MNIST
dataset performance is similar, while on the FashionMNIST dataset and
the tanh activation function, the lack of the nonlinear derivative appears
to slightly hurt performance. . . . . . . . . . . . . . . . . . . . . . . 132
3.19 The error-connectivity problem and our solution. On the left, the
biologically implausible one-to-one connectivity between value and
error nodes required by the standard predictive coding theory. On
the right, our solution to replace these one to one connections by a
fully connected connectivity matrix ψ. By learning ψ with a Hebbian
learning rule we are able to achieve comparable performance to the
one-to-one connections with a fully dispersed connectivity matrix. . . 133
xiv
3.20 Test accuracy of predictive coding networks with and without learnable
error connections for both relu and tanh activation functions on the
MNIST and FashionMNIST datasets. We see that, interestingly, using
learnt error weights decreased performance only with the tanh but not
the relu nonlinearity, and then only slightly in the FashionMNIST case. 135
3.21 Schematic representations of the architecture across two layers of a.)
the standard predictive coding architecture and b.) The fully relaxed
architecture. Importantly, this architecture has full connectivity between
all nodes and also non-symmetric forward and backwards connectivity
in all cases. In effect, this architecture only maintains a bipartite graph
between error and value neurons, but no other clear structure . . . . . 136
3.22 Test accuracy standard and fully relaxed predictive coding networks
(the combined algorithm), for both relu and tanh activation functions
on the MNIST and FashionMNIST datasets. We see that, interest-
ingly,performance is degraded in all cases and that the relu networks
are especially affected – with catastrophic declines in performance to
become almost random. The reasons for this are currently unknown
and will be investigated in future work. . . . . . . . . . . . . . . . . . 137
4.1 Graphical model for control as inference, with optimality variables Ω.
Other than the optimality variables, the graphical model takes the form
of a Markov Decision Process with actions a and states s. The state of
a speciﬁc timestep depends on the action and state of the last time-step.
By writing out an explicit graphical model like this, we can apply a
whole ﬁeld’s worth of inference algorithms on graphical models like
this to solve control problems. . . . . . . . . . . . . . . . . . . . . . 163
xv
4.2 Comparison of the mean reward obtained by the Active Inference agent
compared to two reinforcement learning baseline algorithms – Actor-
Critic and Q learning on the CartPole environment. We demonstrate
the learning curves over 2000 episodes, averaged over 5 different seeds.
500 is the maximum possible reward. We see that while the vanilla
actor critic agent initially learns faster, over a long time horizon, the
active inference agent outperforms it – and both perform better than the
vanilla Q learning agent. . . . . . . . . . . . . . . . . . . . . . . . . 181
4.3 Comparison of Active Inference with standard reinforcement learning
algorithms on the Acrobot environment. Here we see the learning
curves plotted over ﬁve seeds over 20000 episodes. The maximum
possible reward in this environment was 0, so no agents are optimal.
We see again that active inference outperforms the other two methods
consistently. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.4 Comparison of Active Inference with reinforcement learning algorithms
on the Lunar-Lander environment. Learning curves presented over
15000 episodes, averaged over 5 seeds. Here the vanilla policy gra-
dient algorithm strongly outperforms the others, for unclear reasons,
although active inference is still comparable with the other standard
reinforcement learning algorithms. A score of 200 is optimal. . . . . . 183
4.5 We compare the full Active Inference agent (entropy regularization +
transition model) with an Active Inference agent without the transition
model, and without both the entropy term and the transition model).
We see that while removing the transition model appears to have little
effect, removing the entropy regularisation term substantially impairs
performance. This may be due to the entropy term aiding in staving off
policy collapse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
xvi
4.6 Comparison of the rewards obtained by the fully ablated Active Infer-
ence agent with standard reinforcement-learning baselines of Q-learning
and Actor-Critic on the CartPole environment. Learning curves are
averaged over 5 seeds. We see that despite being fully ablated, the
active inference agent continous to perform comparably with standard
reinforcement learning agents. . . . . . . . . . . . . . . . . . . . . . 185
4.7 (A) Mountain Car:Average return after each episode on the sparse-
reward Mountain Car task. Our algorithm achieves optimal performance
in a single trial. (B) Cup Catch:Average return after each episode on
the sparse-reward Cup Catch task. Here, results amongst algorithms are
similar, with all agents reaching asymptotic performance in around 20
episodes. (C & D) Half Cheetah:Average return after each episode on
the well-shaped Half Cheetah environment, for the running and ﬂipping
tasks, respectively. We compare our results to the average performance
of SAC after 100 episodes learning, demonstrating our algorithm can
perform successfully in environments which do not require directed
exploration. Each line is the mean of 5 seeds and ﬁlled regions show
+/- standard deviation. . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.8 (A & B) Mountain Car state space coverage: We plot the points in
state-space visited by two agents - one that minimizes the free energy of
the expected future (FEEF) and one that maximises reward. The plots
are from 20 episodes and show that the FEEF agent searches almost the
entirety of state space, while the reward agent is conﬁned to a region
that can be reached with random actions. (C) Ant Maze Coverage: We
plot the percentage of the maze covered after 35 episodes, comparing
the FEEF agent to an agent acting randomly. These results are the
average of 4 seeds. . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
xvii
4.9 Overview of classic RL and control algorithms in our scheme. Standard
model-free RL corresponds to amortised policies, planning algorithms
are iterative planning, and control theory infers iterative policies. The
amortised plans quadrant is empty, perhaps suggesting room for novel
algorithms. The position of the algorithms within the quadrant is not
meaningful. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
4.10 (a - c): Amortised predictions of qφ(a|s;θ) are shown in red, where •
denote the expected states, shaded areas denote the predicted actions
variance at each step, and the expected trajectory recovered by iterative
inference is shown in blue. At the onset of learning (a), the amortised
predictions are highly uncertain, and thus have little inﬂuence on the
ﬁnal approximate posterior. As the amortised model fφ(·) learns (b),
the certainty of the amortised predictions increase, such that the ﬁnal
posterior remains closer to the initial amortised guess. At convergence,
(c), the iterative phase of inference has negligible inﬂuence on the
ﬁnal distribution, suggesting convergence to a model-free algorithm.
(d) Here, we compare our algorithm to its constituent components –
the soft-actor critic (SAC) and an MPC algorithm based on the cross-
entropy method (CEM). These results demonstrate that the hybrid model
signiﬁcantly outperforms both of these methods. . . . . . . . . . . . . 212
xviii
5.1 Numerical illustration of optimizing a multimodal desired distribu-
tion with an Evidence objective (Panel A) vs a Divergence Objective
(panel B). The desire distribution consisted of the sum of two univari-
ate Gaussian distributions, with means of 1 and 4 and variances of
1 and 0.4 respectively. We then optimized an expected future distri-
bution, which also consisted of two Gaussians with free means and
variances using both an Evidence and a Divergence objective. As can
be seen, optimizing the Evidence Objective results in the agent ﬁt-
ting the predicted future density entirely to an extremely sharp peak
around the mode of the desired distribution. Conversely, optimizing
a divergence objective leads to a precise match of the predicted and
desired distributions (panel B shows the two distributions almost pre-
cisely on top of one another). As a technical note, to be able to see
both the evidence and deisre distributions on the same scale, for the
evidence objective the predicted distribution is normalized but the de-
sired distribution is not. Code for these simulations can be found at:
https://github.com/BerenMillidge/origins_information_seeking_exploration.237
6.1 Top: Backpropagation on a chain. Backprop proceeds backwards
sequentially and explicitly computes the gradient at each step on the
chain. Bottom: Predictive coding on a chain. Predictions, and prediction
errors are updated in parallel using only local information. Importantly,
while the original computation graph (black lines) must be a DAG, the
augmented predictive coding graph is cyclic, due to the backwards (red)
prediction error connections. . . . . . . . . . . . . . . . . . . . . . . 283
6.2 Top: The computation graph of the nonlinear test functionvL = tan(√θv0)+
sin(v2
0). Bottom: graphs of the log mean divergence from the true gra-
dient and the divergence for different learning rates. Convergence to
the exact gradients is exponential and robust to high learning rates. . . 289
xix
6.3 Mean divergence between the true numerical and predictive coding backprops
over the course of training. In general, the divergence appeared to follow a
largely random walk pattern, and was generally neglible. Importantly, the
divergence did not grow over time throughout training, implying that errors
from slightly incorrect gradients did not appear to compound. . . . . . . . 293
6.4 Training and test accuracies of the CNN network on the SVHN and CIFAR
datasets using the cross-entropy loss. As can be seen performance remains very
close to backprop, thus demonstrating that our predictive coding algorithm
can be used with different loss functions, not just mean-squared-error. . . . 294
6.5 Test accuracy plots for the Predictive Coding and Backprop RNN and
LSTM on their respective tasks, averaged over 5 seeds. Performance is
again indistinguishable from backprop. . . . . . . . . . . . . . . . . . 297
6.6 Training losses for the predictive coding and backprop RNN. As ex-
pected, they are effectively identical. . . . . . . . . . . . . . . . . . . 298
6.7 Computation graph and backprop learning rules for a single LSTM
cell. Inputs to the LSTM cell are the current input xt and the previous
embedding ht. These are then passed through three gates – an input,
forget, and output gate, before the output of the whole LSTM cell can
be computed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.8 The LSTM cell computation graph augmented with error units, evincing
the connectivity scheme of the predictive coding algorithm. The key
move is to associate each intermediate node in the computation graph
with its own prediction error unit . . . . . . . . . . . . . . . . . . . . 300
6.9 Training losses for the predictive coding and backprop LSTMs averaged
over 5 seeds. The performance of the two training methods is effectively
equivalent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
xx
6.10 Divergence between predictive coding and the correct backprop gradi-
ents as a function of sequence length. Crucially, this divergence only
increases linearly in the sequence length, allowing for very accurate
gradient computation even with long sequences. . . . . . . . . . . . . 302
6.11 Number of iterations to reach convergence threshold as a function
of sequence length. Importantly, the number of iterations required
to converge appears to grow sublinearly with sequence length, again
implying that convergence is not computationally unattainable even
with very long sequences. . . . . . . . . . . . . . . . . . . . . . . . . 303
6.12 Training losses for the predictive coding and backprop LSTMs averaged
over 5 seeds. The performance of the two training methods is effectively
equivalent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
6.13 Train and test accuracy and gradient angle (cosine similarity) for AR
vs backprop for MNIST and Fashion-MNIST datasets. Importantly
the training and test accuracies are virtually identical between the AR-
trained and backprop-trained networks. Additionally, the gradient angle
between the AR update and backprop is always substantially less than
the 90 degrees required to allow for learning. . . . . . . . . . . . . . 310
6.14 Train and test accuracy and gradient angle (cosine similarity) for AR vs
backprop for MNIST and Fashion-MNIST datasets. Importantly, we
see that even with the additional relaxations, the AR trained algorithm
performs comparably to backprop . . . . . . . . . . . . . . . . . . . 313
xxi
6.15 Angle between the AR and backprop updates in the learnable backwards
weights, no nonlinear derivatives, and the combined conditions. At
all times this angle remains under 90 degrees and is steady for all
cases apart from the no nonlinear derivatives cases, where it appears
to increase over time. Interestingly, this does not appear to hinder
learning performance noticeably, and may simply reﬂect the angle
getting increasingly worse as the network converges. . . . . . . . . . 315
6.16 Assessing whether the frozen feedforward pass assumption can be
relaxed. We show the resulting performance (test accuracy) against
baseline of relaxing this assumption on the MNIST dataset. All results
averaged over 10 seeds. These results show clearly that the frozen
feedforward pass assumption can be relaxed for the nonlinear derivative
and in the weight update nonlinear derivative, but not both nonlinear
derivatives simultaneously, and deﬁnitely not using thexT term in the
weight update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
6.18 Performance (test accuracy), averaged over 10 seeds, on CIFAR10
demonstrating the scalability of the learnable backwards weights and
dropping the nonlinear derivatives in a CNN architecture, compared
to baseline AR without simpliﬁcations. Performance is equivalent
throughout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
6.20 Potential schematic for a direct implementation of backprop in the brain.
All that is necessary for this to be plausible is three-factor learning rules.325
xxii
Chapter 1
Introduction
The Free Energy Principle (FEP) (Friston, 2019a; Friston & Ao, 2012a; Friston, Kilner,
& Harrison, 2006; Parr, Da Costa, & Friston, 2020) is an emerging theory in theoretical
neuroscience which aims to tackle an extremely deep and fundamental question – can
one characterise necessary behaviour of any system that maintains a statistical separation
from its environment (Bruineberg, Dolega, Dewhurst, & Baltieri, 2020; Friston, 2019a;
Parr, Da Costa, & Friston, 2020)? Speciﬁcally, it argues that any such system can
be seen as performing an elemental kind of Bayesian inference where the dynamics
of the internal states of such a system can be interpreted as minimizing a variational
free energy functional (Beal, 2003) 1, and thus performing approximate (variational)
Bayesian inference (Friston, 2019a). The FEP is thus effectively a formalization and
generalization of the Ashbyan good regulator principle (Conant & Ross Ashby, 1970),
where an intrinsic property of these kinds of systems is that they in some sense come to
embody a Bayesian model of their surroundings, and the perform a inference using this
model (Baltieri, Buckley, & Bruineberg, 2020).
The free energy principle therefore provides a close link between the notions of self-
organisation and dissipative structures in thermodynamics (Prigogine & Lefever, 1973;
1Hence the name the ‘ free energy principle’
1
Chapter 1. Introduction 2
Seifert, 2008), with cybernetic notions of feedback, regulation, and control (Johnson
& Moradi, 2005; Kalman et al., 1960; Wiener, 2019), to more ‘cognitive’ ideas of
inference and learning (Dayan & Daw, 2008; Rao & Ballard, 1999; Schmidhuber,
1991). Speciﬁcally, we see that, in some sense, all of these notions can be construed as
necessary properties and consequences of systems that self-organize to, and maintain
themselves at, a non-equilibrium steady state. While having developed over time into a
very general theory of self-organising systems, the free energy principle has emerged
from theoretical neuroscience as a way to understand the properties of biological and
cognitive systems, especially the brain (Friston, 2003; Friston et al., 2006). As such,
the most developed process theories, which are explicitly inspired by the free energy
principle – predictive coding (Friston, 2005; Mumford, 1992; Rao & Ballard, 1999),
and active inference (Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017a;
Friston, Rigoli, et al., 2015a; Friston, Samothrakis, & Montague, 2012), each purport
to be theories of inference, action, and learning in the brain. A core tenet of the free
energy principle is that perception, action, and learning can all be uniﬁed under a single
inference objective which minimizes a single objective – the variational free energy. As
we shall see, these different ‘process theories’ arise simply from the choice of a speciﬁc
parametrization of the generative model and the variational density, where certain
choices have been found to be useful and also give potentially biologically plausible
inference and learning rules. In this thesis, we focus on the high level applications of
the free energy principle to neuroscience and to machine learning, and make multiple
contributions to the literature through the application of the free energy principle to
all of perception, action, and learning. Speciﬁcally, in this thesis, we are primarily
concerned with scaling up methods which have emerged in the theoretical neuroscience
literature, to the extremely challenging and complex tasks which can be solved with
modern machine learning methods. The value of scaling up such methods is twofold.
Firstly, FEP inspired process theories often possess signiﬁcant biological plausibility,
in that they provide a potential account of what the brain is doing – thus, if they can
Chapter 1. Introduction 3
scale them up to handle the sort of tasks that the brain must solve, then we can be
more conﬁdent that these theories could, in theory, be actually implemented in the
brain. Secondly, the free energy principle and its process theories contain many insights
which can potentially be used to improve and extend current state of the art methods in
machine learning. In this thesis, we aim to present both kinds of contributions – ﬁrstly
by demonstrating that FEP inspired models and process theories can scale, and secondly,
by showcasing how ideas from the FEP can be used to advance the ﬁeld of machine
learning or neuroscience on its own terms.
1.1 Thesis Overview
This thesis is organized into three main parts. We begin with a detailed introduction
and mathematical walkthrough of the Free Energy Principle (FEP) as presented in its
most recent incarnation (Friston, 2019b; Parr, Da Costa, & Friston, 2020) (Chapter 2)
and then, in the ﬁrst section of original work (Chapter 3), we consider applications of
the free energy principle to perception – and make contributions to the FEP process
theory of predictive coding. In the second section (Chapters 4 and 5), we consider
applications of the free energy principle to action, and work with the process theory
of active inference. Finally, in the third section (Chapter 6), we consider applications
of the free energy principle to learning, and especially focus on what FEP-inspired
models and process theories can tell us about the nature of credit assignment in the brain.
Below is a chapter by chapter breakdown of the work in the thesis and what I see are the
main contributions to both machine learning and neuroscience in each. Throughout the
thesis, since each chapter covers a fairly distinct topic, I have tried to make each chapter
modular and mostly independent of the others. Each chapter contains an introduction
and mini literature review on the ﬁeld it discusses, as well as presenting my original
work.
In Chapter 2, I will give a detailed overview of the free energy principle, starting from
Chapter 1. Introduction 4
ﬁrst principles, and include a discussion of the mathematical assumptions and provide
some of my opinions on the philosophical nature of the theory and its potential utility. I
will then give a brief walk-through of discrete state space active inference as presented
in (Da Costa, Parr, et al., 2020; Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo,
2017b; Friston, Rigoli, et al., 2015a) which will be the focus of the ‘scaling up’ work in
Chapter 4.
In Chapter 3, we deal principally with models of perception and predictive coding. We
begin by giving a brief overview and mathematical walkthrough of predictive coding
theory as it is presented in (Buckley, Kim, McGregor, & Seth, 2017; Friston, 2005,
2008a). We then cover in depth two contributions to the theory of predictive coding.
First, we present work where we scale up and empirically test the performance of
large scale predictive coding networks on machine learning datasets, which had not
been tested before in the literature. We also clarify the relationship between predictive
coding and other known algorithms such as Kalman ﬁltering. Secondly, we discuss
relaxing various relatively un-biologically plausible aspects of the predictive coding
equations, such as the need for symmetric forward and backwards weights, the necessity
of using nonlinear derivatives in the update rules, and the one-to-one error to value
neuron connectivity required by the standard algorithms. All of these conditions put
serious constraints on the biological plausibility of the algorithm, and here we show
that to some extent they can each be relaxed without harming performance.
Then, in Chapters 4 and 5, I present my work on the applications of the free energy
principle to questions of action selection and control. Chapter 4 focuses predominantly
on scaling up active inference methods to achieve results comparable to those achieved
in the deep reinforcement learning literature, while Chapter 5 takes a more abstract and
mathematical approach and investigates in depths the mathematical origin of objective
functionals which combine both exploitatory and exploratory behaviour – an approach
which has the potential to ﬁnesse the exploration-exploitation dilemna (Friston, Rigoli,
Chapter 1. Introduction 5
et al., 2015a).
Speciﬁcally, in Chapter 4, I will ﬁrst review the rudiments of reinforcement learning
(RL), and its current incarnation in deep reinforcement learning, including the two
paradigms of model-free and model-based approaches. I will then present two of my
contributions of merging active inference and deep reinforcement learning under the new
paradigm of deep active inference. I will ﬁrst discuss deep active inference in the model-
free paradigm, and show how in this case the active inference equations can naturally be
understood as specifying an actor-critic architecture with a bootstrapped value function,
except one where the value-function becomes the expected-free energy functional,
which provides an intrinsic source of exploratory drive to the algorithm which can
improve performance. I then discuss the similarities and differences to standard deep
reinforcement learning algorithms and empirically compare the performance of the
algorithms on a number of challenging continuous control tasks from OpenAI gym
(Brockman et al., 2016).
Then I will present a second piece of work which applies active inference instead to
the model-based reinforcement learning paradigm. We will show that in this case, we
can use the powerful generative ‘world models’ (Ha & Schmidhuber, 2018) of active
inference to work as transition models of the learnt dynamics, and then the use of action
selection in planning. The use of the expected free energy functional again furnishes
an intrinsic exploratory for active inference agents, which we again show is crucial to
effective, goal-directed exploration and that it empirically improves performance on a
suite of continuous control tasks. We then conceptualize reinforcement learning through
the lens of inference, and understand the distinction between model-free and model-
based reinforcement learning through the lens of iterative and amortised inference.
We then demonstrate how these two types of inference can becombined, leading to a
novel hybrid inference algorithm which we show attains both the sample efﬁciency of
model-based reinforcement learning with the higher asymptotic performance and fast
Chapter 1. Introduction 6
computation time of model-free RL.
Then, in Chapter 5, we move into a more abstract, mathematical domain. Here we
grapple with deep questions underlying the objective functions of reinforcement learning.
Speciﬁcally, we wish to understand the mathematical origin and nature of the expected
free energy term which grants deep active inference agents their superior exploratory
capacities. Having tackled this, we turn to the deeper question of the mathematical
origin of information-seeking exploratory terms within the inference objective optimized
in reinforcement learning methods, and thus the mathematical origin of exploratory
drives. We present a new dichotomy between evidence and divergence objectives,
and demonstrate how only divergence objectives, which intuitively can be seen as
minimizing the divergence between the predicted and desired futures, rather than simply
maximizing the likelihood of the desired future, are required to obtain such terms. We
then relate this fundamental dichotomy to a number of objectives prominent in both the
cognitive science, neuroscience, and machine learning literatures. Finally, we further
seek to explore the general possible space of variational objective functionals for control,
and provide a wide-ranging categorisation of the potential of such functionals within
our framework.
Finally, in Chapter 6, we turn to the application of insights and ideas from the free
energy principle to learning. Speciﬁcally, we focus on the vexing question of how
to achieve credit assignment in the brain. This is necessary since, we assume, that
most of the statistical ‘parameters’ in the brain – such as synaptic weights – start out
initialized fairly randomly during development and thus need to be trained or learned
through interactions with the environment 2. Understanding how this learning can
take place is a fundamental question within neuroscience. One approach, which has
recently been immensely successful in machine learning with large artiﬁcial neural
networks, is the idea of learning through gradient descent using the backpropagation
2It is possible that some pathways, especially low-level subcortical pathways may be, to some extent,
hardwired by evolution. However, it is generally considered infeasible for the immense number and
complexity of the neocortical circuitry to be hardwired in this way
Chapter 1. Introduction 7
of error algorithm. Since backpropagation of error is such a successful algorithm in
artiﬁcial neural network – which, although simpliﬁed are nevertheless generally quite
a close substrate to biological neural networks – it is very likely that it would also
work to successfully train biological neural networks, if it could be implemented in a
biologically plausible manner in such networks. The question, then, becomes whether
and how backprop can be implemented in biologically plausible neural networks. While
this is an extremely broad question which cannot likely be answered in a single thesis,
we present two novel contributions to this question here.
Speciﬁcally, in Chapter 6, we ﬁrst provide a brief review of the credit assignment prob-
lem in the brain, as well as the backpropagation algorithm (and automatic differentiation
in general), for context, and then present our two contributions to this ﬁeld. First, we
demonstrate how under certain conditions, predictive coding itself can be utilized as a bi-
ologically plausible method of credit assignment in the brain, can apply to any arbitrary
computation graph, and can be used to train modern machine learning architectures
such as CNNs and LSTMs with performance comparable to backprop. Secondly, we
introduce a novel, simpler algorithm for credit assignment in the brain, which we call
Activation Relaxation and then discuss their similarities and differences. We end with a
discussion of the current state of credit assignment algorithms and backpropagation in
the brain, and the importance of this ﬁeld of research.
Finally, in Chapter 7, we provide a discussion and overview of the work in our thesis.
We will brieﬂy survey what has been achieved, and where the limitations and directions
for future work lie, as well as the implications of the work of this thesis.
1.2 Statement of Contributions
This statement provides a detailed overview of the work undertaken in this PhD which
has resulted in research papers, both the papers included in this thesis and also those
not included. I provide a brief summary of the key results and narrative of each paper,
Chapter 1. Introduction 8
as well as a detailed breakdown my contributions. * denotes equal contribution.
1.2.1 Included in Thesis
1.2.1.1 Chapter 3
• Predictive Coding – a Theoretical and Experimental review (2021). Beren
Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley. In Preparation
This paper provides a full review of recent advances in predictive coding, as
well as the mathematical basis of the theory. It covers all of the mathematical,
implementational, and neuronal aspects of predictive coding theory. As a ﬁrst
author paper, I conceptualised the idea, collated the necessary materials for the
review, and wrote the paper. Alexander Tschantz, Christopher L Buckley, and
Anil Seth, contributed edits and other editorial suggestions.
• Neural Kalman Filtering(2021). Beren Millidge, Alexander Tscahtnz, Anil Seth,
Christopher L Buckley. Arxiv
This paper reviews the close connection between Kalman ﬁltering and linear
predictive coding, demonstrates that predictive coding can closely approximate
the performance of Kalman ﬁltering on ﬁltering tasks, and proposes a low-level
neural implementation of predictive coding which could be implemented in
the brain. As a ﬁrst author paper, I conceptualised the idea, worked out the
mathematical derivations, implemented the model and experiments, and wrote
a ﬁrst draft of the paper. Alexander Tschantz, Christopher L Buckley, and Anil
Seth contributed editorial and narrative suggestions.
• Relaxed Predictive Coding (2020). Beren Millidge, Alexander Tschantz, Anil
Seth, Christopher L Buckley. Arxiv
This paper shows how several biologically implausible aspects of the predictive
coding algorithm – backwards weight symmetry, nonlinear derivatives, and one-to-
Chapter 1. Introduction 9
one error unit connectivity – can be relaxed without unduly harming performance
on challenging object recognition tasks. As a ﬁrst author paper, I conceptualised
the idea, implemented the code and experiments, and wrote up a ﬁrst draft of the
paper. Alexander Tschantz, Anil Seth, and Christopher L Buckley contributed
editorial suggestions.
• Implementing Predictive Processing and Active Inference: Preliminary Steps and
Results (2019). Beren Millidge, Richard Shillcock.
This paper provides reference implementations of multi-layer predictive coding
networks trained for object recognition within a machine learning paradigm. As
a ﬁrst author paper, I conceptualised the idea, wrote the code and experiments,
and wrote up the initial draft of the paper. Richard Shillcock contributed edits.
1.2.1.2 Chapter 4
• Deep Active Inference as Variational Policy Gradients(2019). Beren Millidge.
Published in the Journal of Mathematical Psychology.
This paper merges active inference and model-free deep reinforcement learning
to create a deep active inference agent, very similar to actor-critic methods in
deep reinforcement learning. The performance of deep active inference and deep
reinforcement learning is compared on a suite of OpenAI Gym continuous control
tasks. As a sole author paper, I conceptualised the idea, executed it mathematically
and in code, designed and implemented the experiments and wrote up the paper.
• Reinforcement Learning through Active Inference (2020). Alexander Tschantz*,
Beren Millidge* , Anil Seth, Christopher L Buckley. Published in ICLR work-
shop on Bridging AI and Cognitive Science.
Chapter 1. Introduction 10
This paper applies active inference to model-based reinforcement learning meth-
ods for continuous control. We use an ensemble transition model parametrised
by deep neural networks for model-based planning. The expected free energy
and free energy-of-the-expected future objective functionals provide additional
exploratory bonuses which allow considerably greater and faster performance of
the method compared to standard deep reinforcement learning baselines. I was
joint ﬁrst author on this paper. While the initial idea was primarily conceptualized
by Alexander Tschantz and Christopher L Buckley, I contributed equally to the
design and implementation of the algorithm, the experiments, and the writing of
the paper.
• Reinforcement Learning as Iterative and Amortised inference (2020). Beren
Millidge*, Alexander Tschantz*, Anil Seth, Christopher L Buckley. Arxiv.
This short workshop paper derives the key mathematical result of understanding
model-free and model-based reinforcement learning in terms of iterative and
amortised inference. It then partitions known reinforcement learning algorithms
into a quadrant based on two orthogonal axes – ﬁrstly whether it uses iterative or
amortised reinforcement learning, and secondly whether we optimize over plans
or over policies. As a joint ﬁrst author, I contributed equally (with Alexander
Tschantz) in the idea, formulation of the dichotomy, and the mathematical deriva-
tions. I also was the primary author of the text of the paper. Alexander Tschantz,
Christopher L Buckley, and Anil Seth also contributed edits to the paper draft.
• Control as Hybrid Inference (2020). Alexander Tschantz, Beren Millidge, Anil
Seth, Christopher L Buckley. Published in ICML workshop on the Theoretical
Foundations of Reinforcement Learning.
This paper combines both iterative (model-based) and amortised (model-free)
Chapter 1. Introduction 11
reinforcement learning methods to obtain a hybrid method which combines both
the sample-efﬁciency of model-based RL, with the asymptotic performance and
fast computation of model-free methods. As second author, I contributed equally
to the idea, the mathematical derivation, and architectural formulation of the
hybrid agent. Alexander Tschantz took the lead with implementing the agent in
code, designing and running, the experiments, and writing up the initial draft of
the paper. I then contributed paper edits along with Anil Seth and Christopher L
Buckley.
1.2.1.3 Chapter 5
• Whence the Expected Free Energy (2020). Beren Millidge, Alexander Tschantz,
Anil Seth, Christopher L Buckley. Published in Neural Computation.
This paper investigates the mathematical origin of the expected free energy
functional in active inference, demonstrates its relationship to other algorithm,
and proposes a novel, more principled objective, the free energy of the expected
future (FEEF). As a ﬁrst author, I primarily conceptualised the idea and worked
out the mathematical results. I also wrote up the initial draft and was instrumental
in handling later edits. Christopher L Buckley also contributed signiﬁcantly to
some of the mathematical results. Alexander Tschantz, Christopher L Buckley,
and Anil Seth, also contributed through edits to the main text of the paper.
• On the relationship of Active Inference and Control as Inference(2020). Beren
Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley. Published in
the IEEE International Workshop on Active Inference.
This paper derives the relationship between active inference methods, and the
control as inference paradigm which is popular within the reinforcement learning
community. As ﬁrst author, I conceptualised the idea, derived the primary mathe-
matical results, and wrote the initial draft of the paper. Alexander Tschantz, Anil
Chapter 1. Introduction 12
Seth, and Christopher L Buckley contributed paper edits.
• Understanding the Origin of Information-Seeking Exploration in Probabilistic
Objectives for Control (2021).Beren Millidge, Alexander Tschantz, Anil Seth,
Christopher L Buckley. Submitted to Arxiv.
This paper introduces the dichotomy between evidence and divergence objectives,
demonstrates how divergence objectives are necessary for the emergence of infor-
mation maximizing exploration, and uniﬁes many disparate objectives proposed
in the machine learning and cognitive science communities under this formalism.
As a ﬁrst author paper, I conceptualised and derived the mathematical results,
and wrote up a ﬁrst draft of the paper. Alexander Tschantz, Christopher L Buck-
ley, and Anil Seth contributed paper edits and narrative suggestions. Alexander
Tschantz contributed heavily to the cognitive science and psychophysics sections.
1.2.1.4 Chapter 6
• Predictive Coding Approximates Backprop along Arbitrary Computation Graphs
(2020). Beren Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley.
Arxiv
This paper demonstrates that predictive coding can approximate the backpropaga-
tion of error algorithm along arbitrary computation graphs. Predictive coding is
used to train state of the art machine learning architectures, and obtains identical
performance to backprop even for deep and complex architectures. As a ﬁrst au-
thor paper, I conceptualised the idea, implemented the models and experiments in
code, and wrote up the initial draft of the paper. Alexander Tschantz, Christopher
L Buckley, and Anil Seth contributed paper edits and narrative suggestions.
• Activation Relaxation: A Local, Dynamical Approximation to Backpropagation
in the Brain (2020). Beren Millidge, Alexander Tschantz, Anil Seth, Christopher
L Buckley. Arxiv
Chapter 1. Introduction 13
This paper introduces a novel algorithm for approximating backpropagation in a
local, biologically plausible way, which we call the activation relaxation algorithm.
Crucially, this approach is signiﬁcantly simpler than predictive coding, in that it
does not require a special population of error neurons. Additionally, we show
that several of the remaining biologically implausible aspects of the algorithm –
speciﬁcally the symmetric backwards weights – can also be relaxed, leading to
an extremely simple and biologically plausible algorithm for credit assignment.
As a ﬁrst author paper, I invented the algorithm and performend the mathematical
derivations. I implemented the model in code, and ran the experiments. I wrote
up the initial draft of the paper. Alexander Tschantz, Christopher L Buckley, and
Anil Seth contributed editorial suggestions.
• Investigating the Scalability and Biological Plausibility of the Activation Re-
laxation Algorithm (2020). Beren Millidge, Alexander Tschantz, Anil Seth,
Christopher L Buckley. Published at NeurIPS 2020 Workshop: Beyond Backprop.
This paper extends and empirically tests the Activation Relaxation algorithm
on more challenging tasks including large-scale CNN models. Moreover, it
investigates the degree to which the loosening the assumptions of the activation
relaxation algorithm hinder performance. As a ﬁrst author paper, I conceptualized
the core ideas to test, implemented the experiment and analyzed the results. I
wrote up the initial draft of the paper. Alexander Tschantz, Christopher L Buckley,
and Anil Seth contributed editorial suggestions.
1.2.2 Not Included in the Thesis
• Combining active inference and hierarchical predictive coding – a tutorial review
and case study (2019). Beren Millidge, Richard Shillcock.
This paper uses hierarchical predictive coding networks as a dynamics model
for a simple active inference approach which is then applied to discrete action
Chapter 1. Introduction 14
reinforcement learning tasks such as the cart-pole. As a ﬁrst author paper, I
conceptualised the idea, implemented the code and experiments, and wrote up
the initial draft. Richard Shillcock contributed paper edits and suggestions.
• A predictive processing account of visual saliency using cross-predicting autoen-
coders (2018). Beren Millidge, Richard Shillcock. Psyarxiv.
This paper demonstrates how applying a cross-modal prediction objective in
predictive coding, allows for the development of error representations which
provide a good empirical match to estimates of visual saliency in natural image
scenes. As a ﬁrst author paper, I contributed equally in conceptualising the
idea with my supervisor, Richard Shillcock. I implemented the models and
experiments, and wrote up the initial draft of the paper. Richard Shillcock then
contributed with editorial suggestions.
• Exploring infant vocal imitation in Tadarida brasiliensis mexicana(2019). Richard
Shillcock, Beren Millidge, Andrea Ravignani. Published in Neurobiology of
Speech and Language.
This paper introduces a multi-agent model of vocal imitation in bat infants, which
provides a gradient soundscape which can guide mothers to pups in a crowded bat
colony. As a second author paper, I contributed substantially to the development
of the model. I implemented the model in code and ran the experiments. I
contributed substantially to the writing of the paper.
• Curious inferences: reply to Sun and Firestone on the Dark Room Problem
(2020). Anil Seth, Beren Millidge, Christopher L Buckley, Alexander Tschantz.
Published in Trends in Cognitive Science.
This short response argues against the Dark Room Problem in predictive coding
by suggesting that the intrinsic exploratory drives of the expected free energy and
other objectives such as the free energy of the expected future sufﬁce to drive the
Chapter 1. Introduction 15
agent away from dark-room environments. I was involved in the conceptualisation
and writing of the piece, although the main impetus behind this response lay with
Anil Seth.
• The Acquisition of Culturally Patterned Attention Styles under Active Inference
(2020). Axel Constant, Alexander Tschantz, Beren Millidge, Filipo Criado-
Boado, Andy Clark. Arxiv.
This paper presents an active inference model of culturally patterned saccade
behaviour trained on archaeological vase patterns. It demonstrates that more
complex patterns result in more vertically oriented saccade behaviour, thus cor-
roborating experimental studies. I jointly designed and implemented the active
inference model with Alexander Tschantz and ran the experiments. I also wrote
the ﬁrst draft of the methods section of the paper. Andy Clark, Felipe Criado-
Boado, and Axel Constant conceptualised the idea and experiments.
Chapter 2
The Free Energy Principle
The free energy principle is a grand theory, arising out of theoretical neuroscience, with
deep ambitions to provide a uniﬁed understanding of the nature of self organisation
under the rubric of Bayesian inference (Friston, 2010, 2019c; Friston & Ao, 2012a;
Friston et al., 2006). Perhaps the central postulate of this theory is the ‘Free Energy
Lemma’ which states that one can interpret any self organizing system, of any type
and on any scale, as performing a kind of elemental Bayesian inference upon the
external environment that surrounds it (Friston, 2013, 2019a; Friston & Ao, 2012b).
More generally than this, it claims to provide a recipe, in terms of a set of statistical
independencies – which we call the ‘Markov Blanket’, following (Pearl, 2011) – which
deﬁne precisely and mathematically what it means to be a system at all (Friston, 2019a).
Understanding self-organization through the lens of inference provides an exceptionally
powerful perspective for understanding the nature of self-organizing systems, as it
allows one to immediately grasp the nature of the dynamics which undergird self-
organization, as well as apply the extremely large and powerful literature on Bayesian
inference methods and algorithms to the dynamics of self-organizing systems (Parr,
Da Costa, & Friston, 2020; Parr, Sajid, & Friston, 2020; Yedidia, 2011). Moreover,
by framing everything in statistical terms – in terms of conditional independencies,
16
Chapter 2. The Free Energy Principle 17
generative models, and approximate variational distributions – the free energy principle
provides a novel and powerful vocabulary to talk about such systems, as well as to ask
questions such as ‘what kind of generative model does this system embody?’ (Baltieri
et al., 2020; Maturana & Varela, 2012) which would be impossible to ask and answer
without it. Ultimately, this new statistical and inferential perspective upon dynamics
may lead to important advances or novel insights.
This perspective also has exceptionally close relationships with early cybernetic views
of control and regulation (Conant & Ross Ashby, 1970; Kalman, 1960; Wiener, 2019),
and philosophically the FEP can be seen as a mathematical generalization of Ashby’s
notion that every good regulator of a system must become, in effect, a model of the
system (Conant & Ross Ashby, 1970). The FEP nuances this notion slightly by instead
stating that every system that regulates itself against the external environment, must in
some sense embody a generative model of the environment, and also that the ﬂow of
the internal states of the system necessarily perform approximate variational inference
upon an approximate posterior distribution over the external states of the environment,
such that, broadly, they track the ﬂuctuations in the external environment.
The free energy principle originated in theoretical neuroscience, as an attempt to
understand the mathematical properties that a self-organising living, biotic system,must
possess in order to sustain itself against thermodynamic equilibrium. It was ﬁrst and
especially applied to understanding the function of the brain (Friston, 2012; Friston,
Daunizeau, Kilner, & Kiebel, 2010; Friston et al., 2006), and has been developed into
two main process theories – predictive coding (Friston, 2003, 2005, 2008a; Rao &
Ballard, 1999) and active inference (Da Costa, Parr, et al., 2020; Friston, Daunizeau, &
Kiebel, 2009; Friston, FitzGerald, et al., 2017b; Friston, Rigoli, et al., 2015a; Friston,
Rosch, Parr, Price, & Bowman, 2018a; Friston et al., 2012) which have been investigated
in a wide variety of paradigms, where it has been used to investigate a wide variety
of phenomena from (Friston, Levin, Sengupta, & Pezzulo, 2015; Friston, Rigoli, et
Chapter 2. The Free Energy Principle 18
al., 2015a; Friston et al., 2014), information foraging and saccades (Parr, 2019; Parr
& Friston, 2017b, 2018a) exploratory behaviour (Friston, Da Costa, Hafner, Hesp, &
Parr, 2020; Friston, Lin, et al., 2017; Friston, Rigoli, et al., 2015a; Schwartenbeck,
FitzGerald, Dolan, & Friston, 2013), concept learning (Schwartenbeck et al., 2019),
and a variety of neuropsychiatric disorders (Adams, Perrinet, & Friston, 2012; Cullen,
Davey, Friston, & Moran, 2018; Lawson, Rees, & Friston, 2014; Mirza, Adams, Parr, &
Friston, 2019). These process theories translate the abstract formulation of the FEP into
concrete and practical algorithms by specifying certain generative models, variational
distributions, and inference procedures, and have been shown to be extremely useful
both in providing powerful and biologically plausible theories of learning and inference
in the brain, and also in developing highly effective inference algorithms which have
advanced the state of the art in machine learning (Millidge, 2020; Millidge, Tschantz,
Seth, & Buckley, 2020b; Parr, Markovic, Kiebel, & Friston, 2019; Tschantz, Millidge,
Seth, & Buckley, 2020b).
In this chapter, we will provide a relatively self-contained step through of the key
mathematical results of the most recent incarnation of the free energy principle as
presented in (Friston, 2019a; Parr, Da Costa, & Friston, 2020), as well as the details
of the discrete-state-space active inference process theory (Da Costa, Parr, et al., 2020;
Friston, Rigoli, et al., 2015a). While none of the material in this chapter is original,
it is necessary (especially the material on active inference), to understand what is
to come in later chapters. Since this thesis covers a fairly wide range of topics, each
individual thesis chapter also comes with its own literature review covering the necessary
background for the original material in that chapter.
It is important to note that the material in the ﬁrst section of this chapter (walkthrough of
the Free Energy principle as described in Friston (2019a)) is not original to me and draws
heavily from an unpublished monograph (Friston, 2019a), albeit a monograph which has
widely been viewed as the canonical reference point for the theory. Additionally, many
Chapter 2. The Free Energy Principle 19
core elements of the theory presented here have been published elsewhere (Friston, 2013;
Friston, Da Costa, & Parr, 2020; Friston, Wiese, & Hobson, 2020; Parr, Da Costa, &
Friston, 2020). Notably, a fair amount of the material covered here is also controversial
within the community and the validity of many required assumptions still remains
to be assessed. Where relevant, in this section, we provide additional disclaimers
highlighting core assumptions and potentially problematic elements of the mathematical
exposition – and additionally in the discussion section we include an itemized list of
all assumptions as well as critical discussions on each. While some of this material is
somewhat extraneous to the original work covered in later chapters, we believe that
this presentation of the free energy principle gives the reader valuable context into the
broader paradigm of the FEP which has inspired much of the original work in this
thesis. Additionally, by condensing the logical ﬂow of the FEP, and providing a detailed
critical discussion of the logic and assumptions required, we aim to provide a broader
service to the community by helping to make clear the current state as well as current
controversies and debates at the cutting-edge of the free energy community.
Finally, it is important to note that the process theories derived from the FEP – which
we will primarily focus on in the rest of the thesis – do not strictly require the full
mathematical structure of the FEP to hold for their validity. As scientiﬁc theories
about the real world, they rise or fall on empirical considerations independently of the
overall mathematical construct of the FEP, and thus while the material in this chapter
is useful contextually, it is not necessary to understand the work in the chapters that
follow. Throughout we have aimed to make sure that each chapter, by and by large,
is ‘modular’, so that they can be read and understood in isolation. As such, we have
striven to ensure that each chapter contains sufﬁcient background information within it
to let it be understood and evaluated independently of the others.
Chapter 2. The Free Energy Principle 20
Langevin Dynamics Non-Equilibrium Steady State
p*(x)
·x = f(x, t) + ω
Ao Decomposition
·x = (Γ − Q) ∇xln p*(x)
fμ(π) = 𝔼p( ˜π|π)[ fμ(x)] = (Γ − Q) ∇μln p*(π)
Marginal Flow Lemma Markov Blanket Condition
p(x) = p(η| b)p(μ| b)p(b)
fμ(π) = (Γ − Q) ∇μℱparticular(μ, s, a)
Particular Free Energy Parametrisation by argmax
μ(b) = argmax p(μ| b)
η(b) = argmax p(η| b)
η(b) = σ(μ(b))
Identical true and variational posterior
q(η; η) = p(η| b)
·μ = (Γ − Q) ∇μℱ(μ, s, a)
Free Energy Lemma
(Approximate Bayesian Inference) Laplace Approximation
q(η; μ) = 𝒩(σ(μ), Σ(η))
Figure 2.1: The logical ﬂow of the argument of the FEP from the initial formulation to
the crucial approximate Bayesian inference lemma. We begin with a setting of random
Langevin stochastic dynamical systems, which possess a non-equilibrium-steady state.
By applying the Ao decomposition, we can understand the dynamics in terms of a
gradient descent upon the surprisal. Upon the addition of a Markov Blanket partition, we
can express subsets in terms of their own marginal ﬂows via the marginal ﬂow lemma. If
we then identify the internal states as parametrizing a variational distribution over the
external states, we can interpret the marginal ﬂow on the surprisal as a ﬂow on the
variational free energy, under the Laplace approximation.
Chapter 2. The Free Energy Principle 21
2.1 History and Logical Structure
Historically, the free energy principle has evolved over the course of about ﬁfteen
years. Its intellectual development can best be seen in two phases. In the ﬁrst phase, an
intuitive and heuristic treatment emerged with Friston et al. (2006) which stated that the
imperative to minimize variational free energy emerged from a necessary imperative
of minimizing the system’s entropy, or log model evidence, which is upper bounded
by variational free energy. This imperative emerges due to the self-sustaining nature
of biological systems such as brains, in that they maintain a set distribution against
the inexorably increasing entropic nature of thermodynamic reality (Friston, 2009). In
order to do so, systems must constantly seek to reduce and maintain their entropy across
their state space. Since the VFE is computationally tractable while the entropy itself is
not, it was postulated that neural systems maintain themselves by implicitly minimizing
this proxy rather than the actual entropy itself (Friston, 2010).
Later, in the second phase (Friston, 2013), this heuristic argument and intuition was
related more formally to concepts in stochastic thermodynamics (Friston & Ao, 2012a,
2012b). Speciﬁcally, the framework developed mathematically into a description
of stochastic dynamics (as stochastic differential equations) separated into ‘external,
internal, and blanket’ states by a statistical construct called a Markov Blanket. This
blanket makes precise the statistical independence conditions required to make sense of
talking about a ‘system’ as distinct from its ‘environment’. Moreover, by separating the
‘blanket’ into ‘sensory’ and ‘active’ states, one can obtain a statistical description of
the core elements of a perception-action-loop, a central concept in cybernetics, control
theory, and reinforcement learning. Secondly, the theory developed a precise notion
of what it means to maintain a stable ‘phenotype’ which is interpreted mathematically
as a non-equilibrium steady-state density (NESS) over the state-space. This steady
state is non-equilibrium due to the presence of ‘solenoidal ﬂows’ which are ﬂows
orthogonal to the gradient of the NESS density. Mathematically, such ﬂows do not
Chapter 2. The Free Energy Principle 22
increase or decrease the entropy of the steady-state-density, but do, however, in contrast
to an equilibrium steady state (ESS), provide a clear arrow of time. Given this, it is
claimed, that under certain conditions, one can draw a relationship between the ﬂow
dynamics and the process of variational Bayesian inference through the minimization
of the variational free energy (VFE)– which measures the discrepancy between an
approximate posterior and generative model – and that the dynamics that result from
this speciﬁc kind of ﬂow under a Markov blanket at the NESS density can be seen as
approximating a gradient descent upon the VFE, thus licensing the interpretation of the
system as performing a basic kind of Bayesian inference or, ‘self-evidencing’ T (Clark,
2015; Hohwy, Roepstorff, & Friston, 2008)
While the intuitions and basic logical structure of the theory has remained roughly
constant since Friston (2013); Friston and Ao (2012a), the mathematical formulation and
some of the arguments have been reﬁned in the most recent Friston (2019a) monograph
and related papers (Friston, Da Costa, & Parr, 2020; Parr, Da Costa, & Friston, 2020).
These papers have drawn close connections between the formulation of free energy
principle, and many aspects of physics including the principle of least action in classical
mechanics, and notions of information length and the arrow of time in stochastic
thermodynamics. Additionally, the Particular Physics monograph (Friston, 2019a)
contains a novel information-geometric gloss on the nature of the Bayesian inference
occurring in the system. Speciﬁcally, it argues that the internal states of the system
can be seen as points on an statistical manifold that parametrize distributions over the
external states, and that thus the internal states can be described using a ‘dual-aspect
information geometry.’ According to this perspective, internal states evolve in both
the ‘intrinsic’ state space of the system’s physical dynamics, while simultaneously
parameterising a manifold of statistical beliefs about external states - the so-called
‘extrinsic’ information geometry.
While the mathematical depths of the FEP often appears formidably complex to the
Chapter 2. The Free Energy Principle 23
uninitiated, the actual logical structure of the theory is relatively straightforward. First,
we want to deﬁne what it means to be ‘a system’ that keeps itself apart from the
outside ‘environment’ over a period of time. The FEP answers this question precisely
its own way. We deﬁne a ‘system’ as a dynamical system which has a non-equilibrium
steady state (NESS) which it maintains over an appreciable length of time, and that the
dynamics are structured in such a way that they obey the ‘Markov Blanket Condition’.
Speciﬁcally, having a NESS can be intuitively thought of as deﬁning dynamics which
produce something like a system – i.e. a recognizable pattern of states which persists
relatively unchanged for some period of time. For instance, we can think of the
biological systems in such a manner. Biological organisms maintain relatively steady
states, against constant entropic dissipation, for relatively long (by thermodynamic
standards) periods of time. Of course, from a purely thermodynamical perspective,
in resisting entropy themselves, biological organisms are not countering the law of
thermodynamics. To achieve their steady state requires a constant inﬂux of energy
– hence it is a non-equilibrium steady state (NESS). From this perspective, we can
understand biological organisation to be the process of creating ‘dissipative structures’
(Kondepudi & Prigogine, 2014; Prigogine & Lefever, 1973) which only manage to
maintain themselves at steady state and reduce their own entropy at the expense of
consuming energy and increasing the entropy production rate of their environment
(Prigogine, 2017). Illustrative physical examples of similar NESS states are Benard
convection cells , and the Belousov-Zhabotinsky reaction (Zwanzig, 2001). In practical
terms, we can consider the NESS density to be the ‘phenotype’ of the system. From the
perspective of the FEP, we are not usually concerned with whether a set of dynamics
possesses a NESS density, or how convergence to the NESS density works, instead
we take it as an axiom that we possess a system with a NESS density, and are instead
concerned with the dynamical behaviour of the system at the NESS density. While this
is clearly a special case, nevertheless dynamical systems at NESS already exhibit rich
behaviours to effectively maintain themselves there, and it is these properties which
Chapter 2. The Free Energy Principle 24
necessarily any system which maintains itself at NESS, which are the fundamental
object of study of the FEP.
Secondly, now that we have a system which has a NESS density, and thus exhibits some
stability through time, we also require a statistical way to separate the ‘system’ from the
‘environment’. The FEP handles this by stipulating that any system it considers must
fulﬁl a set of criteria which we call the Markov Blanket conditions. These conditions,
deriving from the idea of Markov blankets in Bayesian networks (Pearl, 2011, 2014),
set forth a set of conditional independence requirements that allow a system to be
statistically separated from its environment 1. Speciﬁcally, we require that the dynamics
of the system can be partitioned into three sets of states – ‘internal’ states which
belong to the system of study, ‘external’ states which correspond to the environment,
and ‘blanket states’ which correspond to the boundary between the system and its
environment. Speciﬁcally, we require the internal states to be conditionally independent
of the external states given the blanket states, and vice versa. Thus all ‘inﬂuence’ of
the environment must travel through the blanket, and cannot directly interact with the
internal states of the system which are ‘shielded’ behind the blanket2
Now that we have a system with a NESS density which obeys the Markov Blanket
conditions, so that we can partition it into external, internal, and blanket states, we
then wish to understand the dynamics of the system at the NESS density, so we can
understand the necessary behaviours of the system to allow the NESS to be maintained.
The derivation of the FEP then uses the Helmholtz (Ao) decomposition (Yuan & Ao,
2012; Yuan, Ma, Yuan, & Ao, 2011; Yuan, Tang, & Ao, 2017) to represent the dynamics
as a gradient ﬂow on the log of the NESS density (which is called the surprisal) with
1Whenever we say Markov Blanket, following standard use in the literature, we mean theminimal
Markov blanket – i.e. the Markov Blanket which requires the fewest number of blanket states to achieve
the required conditional independencies.
2Interestingly, mathematically, the MB condition and all of the FEP is completely symmetrical
between ‘internal’ and ‘external’ states. Thus from the perspective of the system, the ‘external states’ are
its environment, but from the perspective of the environment, the ‘external states’ are the system. This
means that the environment models and performs inference about the system just as the system models
and performs inference on the environment. We can thus think of the environment-system interaction as
a duality of inference, where each tries to model and infer the other in a loop.
Chapter 2. The Free Energy Principle 25
both dissipative (in the direction of the gradient) and solenoidal (orthogonal to the
gradient) components. Now that we can express the ﬂows of the system in terms of
gradients of the log NESS density, we then invoke theMarginal Flow Lemma to write
out the dynamics of each component of the partitioned dynamics (i.e. external, internal,
and blanket states) solely in terms of a gradient ﬂow on its own marginal NESS density.
This means that we can express, for instance, the dynamics of the internal states solely
in terms of gradient ﬂows on the marginal NESS density over the internal and blanket
states.
Given this marginal partition, we can analyze and understand each of the ﬂows in
each partition of the system independently. Speciﬁcally, to understand the Ashbyan
notion that ‘every good regulator of a system is a model of the system’ , we wish to
understand the relationship between the ﬂows of the internal and external states, which
are statistically separated from the blanket. Despite this separation, it is possible to
deﬁne a mapping between the most likely internal state, given a speciﬁc conﬁgura-
tion of the blanket states, and the distribution over the most likely external state of
the system. We can use this mapping to interpret internal states as parametrizing a
variational or approximate distribution over the external states. This interpretation sets
up the ‘dual-aspect’ information geometry of the internal states, since internal state
changes simultaneously represent changes in parameters of the distribution over internal
states (which can potentially be non-parametric), and changes to the parameters of the
variational distribution over external states. This latter interpretation means that the
internal states parameterise a statistical manifold equipped with a Fisher information
metric (if the variational distribution is in the exponential family), and in general be-
comes amenable to the techniques of information geometry (Amari, 1995; Ollivier,
Arnold, Auger, & Hansen, 2017) Finally, given that we can interpret the internal states
as paramterising a distribution over external states, we can reconsider the gradient
ﬂow upon the log NESS density with a new light. Speciﬁcally, we can understand the
marginal NESS density to represent the implicitgenerative model of the system, and the
Chapter 2. The Free Energy Principle 26
gradient ﬂow dynamics as a descent upon the free energy, with a perfect Bayes-optimal
posterior. Alternatively, if we invoke an approximate posterior distribution over external
states which is parametrized by the internal states, we can represent the gradient ﬂow of
the internal states as performing an approximate minimization of the variational free
energy (VFE), and thus the internal states of the system can be interpreted as performing
approximate variational Bayes. This is the key result of the FEP. It states, simply, that
the necessary dynamics of any system that maintains itself at a non-equilibrium steady
state, and possesses a Markov Blanket, can be interpreted as modelling, and performing
approximate variational inference upon the external states beyond its own Markov
Blanket. It thus generalizes and makes precise Ashby’s notion that every good regulator
must in some sense be a model of the system (Conant & Ross Ashby, 1970). Here we
see that in order to maintain a non-equilibrium steady state, to counteract the dissipative
forces inherent in thermodynamics, it is necessary to perform some kind of inference
about the environment beyond the system itself.
2.2 Formulation
Here we begin the precise mathematical description of the FEP. We aim to provide a
consistent notation, and more detailed derivations of key results than are often presented.
The presentation in this chapter mostly follows the order of presentation in Friston
(2019a), although many circumstantial topics are omitted to focus on the main ﬂow
of the argument. We begin with the basic mathematical setting and formulation of the
theory. We assume that the dynamics we wish to describe can be expressed in terms of
a Langevin stochastic differential equation (Jaswinski, 1970),
dx
dt = f (x)+ ω (2.1)
where x = [x0 . . .xN] is a vector of states of some dimensionality, and f(x) is an arbitrary
nonlinear but differentiable function of the states. Expressing the dynamics in terms
of a Langevin stochastic differential equation is a very ﬂexible parametrization of
Chapter 2. The Free Energy Principle 27
the dynamics, and is the standard form studied in the ﬁeld of stochastic differential
equations, thus allowing the immediate use of results from that ﬁeld. Speciﬁcally, here
we assume already that this process is not history dependent. The dynamics only depend
on the instantaneous values of the states. In practice, history dependent systems can be
represented in this fashion, albeit somewhat unintuitively by adding sufﬁcient statistics
of the history to the state itself.ω is assumed to be white (zero autocorrelation) Gaussian
noise with zero mean such that ω = N (x;0,2Γ) where Γ is the half the variance of the
noise. Zero autocorrelation means that the covariance between the noise at any two
time instants, even inﬁntesimally close together, is 0 – E[ωtωT
t+δ] =0. We assume that
this noise is added additively to the dynamics.
This stochastic differential equation can also be represented not in terms of dynamically
changing states, but in terms of a dynamically changing probability distribution over
states. This transformation is achieved through the Fokker-Planck equation, by which
we can derive that the change in the distribution over states can be written as,
d p(x,t)
dt = −∇x f (x,t)p(x,t)+ ∇xΓ∇x p(x,t) (2.2)
Where p(x,t) is the instantaneous distribution over the states at a given time t. Here
∇x f (x,t) is the gradient function and simply denotes the vector of partial deriva-
tives of the function f with respect to each element of the vector x. ∇x f (x) =
[∂f (x,t)
∂x0
, ∂f (x,t)
∂xN
, . . . ,∂f (x,t)
∂xN
]. ∇2
x f (x) represents the matrix of second partial derivatives of
the function.
Next, we presuppose that the dynamics expressed in Equation 2.1 tend towards a non-
equilibrium steady state limt→∞ p(x,t) →p∗(x) where we represent the steady state
distribution as p∗(x). Note that this distribution no longer depends on time, since it
is by deﬁnition at a steady state. We use p∗to make clear that this distribution is at
steady state. By deﬁnition a steady state distribution does not change with time, so that
d p∗(x)
dt = 0.
Chapter 2. The Free Energy Principle 28
The distinction between an equilibrium steady state and a non-equilibrium steady state
(NESS) distribution is subtle and important. An equilibrium steady state, mathemati-
cally, is one where the property of detailed balance holds. This means that any transition
between states at equilibrium is just as likely to go in the ‘forwards’ direction as it is
to go in the ‘backwards’ direction. In effect, the dynamics are completely symmetric
to time, and thus there is no notion of an arrow of time in such systems. Conversely,
a non-equilibrium steady state is one where detailed balance does not hold, so there
is a directionality to the dynamics, and thus an arrow of time, even though the actual
distribution over states remains constant. From a thermodynamic perspective, the
equilibrium-steady-state is the inexorable endpoint of the second law of thermodynam-
ics, since it is the maximum entropy state. Conversely, a NESS is not a maximum
entropy solution, since the directionality of the dynamics means that there is a degree of
predictability in the system which could in theory be exploited to produce work. Non-
equilibrium steady states can arise in thermodynamic systems but require an external
source of driving energy as a constant input to the system, which is then dissipated to
the external surroundings and gives the NESS a positive entropy production rate. To
take an intuitive example, we can think about the thermodynamic equilibrium of a cup
of coffee with cream added. The equilibrium steady state (ESS) is when the coffee and
cream have completely diffused into one another, so that the cream maintains a constant
proportion throughout the entire coffee cup. This will be the inevitable result (by the
second law of thermodynamics) of adding an initially low entropy highly concentrated
cream scoop into the coffee. On the other hand, we can think of the non-equilibrium
steady state (NESS) as to be when the cream and coffee are equally diffused throughout,
but somebody 3 is constantly stirring the coffee in a speciﬁc direction. Here, we are at
steady state because the concentrations of cream and coffee don’t change over time,
but nevertheless there is a directionality to the dynamics in the direction of the stirring.
3Of course the analogy fails here since this represents a system with external driving (the person
stirring) whereas the true NESS has no external driving and as such is just ‘intrinsically being stirred’
with no stirrer.
Chapter 2. The Free Energy Principle 29
This directionality is only maintained due to a constant input of energy 4 to the system
(the stirring) 5. The ﬂow caused by the stirring is referred to as the ‘solenoidal ﬂow’ and
mathematically is necessarily orthogonal to the gradient of the steady state distribution.
This is necessary so that the solenoidal ﬂow does not ascend or descent the gradient
of the density, and thus change the steady state distribution which, as a steady state,
by deﬁnition cannot change 6. Biological self organizing systems are often considered
to be ‘dissipative structures’, or non-equilibrium steady states from the perspective of
thermodynamics (Kondepudi & Prigogine, 2014; Prigogine & Lefever, 1973), since
they maintain a relatively steady state over time which requires a constant inﬂux of
energy to maintain.
Given that we presuppose a system with a NESS density, we wish to understand the
dynamics at the NESS density – speciﬁcally, how does the solenoidal ﬂow help prevent
the system from relaxing into an equilibrium-steady-state (ESS)? To understand this,
we utilize the Helmholtz decomposition (Friston & Ao, 2012b; Yuan & Ao, 2012;
Yuan et al., 2017) to rewrite the dynamics at the NESS into a form of a dissipative and
solenoidal ascent upon the gradient of the log NESS density,
dx
dt = (Γ(x)−Q(x))∇x ln p∗(x) (2.3)
Where Γ(x) is a dissipative component of the ﬂow which tries to ascend the log density.
It is the amplitude of the random ﬂuctuations in the original SDE formulation (R. Jordan,
4It’s important to note that here we are using physical intuition and concepts like ‘energy’ in a purely
metaphorical sense. All results here apply to arbitrary SDEs which do not necessarily follow the same
constraints as physical systems – i.e. respect conservation of energy
5Interestingly, physical experience with this analogy would suggest that the solenoidal dynamics
leading to NESS would lead to faster convergence to the NESS density compared to the strictly dissipative
dynamics leading to ESS – effectively, stirring helps the cream diffuse faster. This insight has been
applied to the design of highly efﬁcient Markov-Chain-Monte-Carlo samplers in machine learning
(M. J. Betancourt, 2013; Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953; Neal et al., 2011)
6Importantly, in this coffee-cream example, we are not claiming that if the stirring is removed then
it will settle into a different steady state distribution, merely that the steady state with the stirring is
non-equilibrium steady state (NESS) while the steady-state without stirring is an equilibrium steady
state (ESS) due to the lack of solenoidal ﬂow. Adding solenoidal ﬂow to an ESS always can generate a
NESS while the converse is not true. There are NESSs which can exist solely in virtue of their solenoidal
dynamics without a corresponding ESS. An example of this would be a spinning top, which remains
spinning solely due to its solenoidal motion.
Chapter 2. The Free Energy Principle 30
Kinderlehrer, & Otto, 1998; Yuan, Ma, Yuan, & Ao, 2010; Yuan et al., 2011), which in
effect are constantly trying to ‘smooth out’ the NESS density and increase its entropy.
Conversely, the Q(x) represents the solenoidal portion of the ﬂow which, although
orthogonal to the gradient of the log potential, successfully counteracts the dissipative
effects of the Γ(x) terms to maintain the dynamics at a steady state. While Γ(x) and
Q(x) can in theory be state-dependent, from here on out we typically assume that they
are not – Γ(x) =Γ; Q(x) =Q, and additionally assume that Γ is a diagonal matrix 7, so
there is no cross-correlation between states in the noise added to the system.
It is straightforward to verify that the Helmholtz decomposition of the dynamics satisﬁes
the steady state condition d p∗(X)
dt = 0 by plugging this form into the Fokker-Planck
equation (Equation 2.2),
d p∗(x)
dt = −∇x
[
(Γ−Q)∇x ln p∗(x)
]
p∗(x)+ Γ∇2
x p∗(x)
= −∇x
[
(Γ−Q)∇x p∗(x)
p∗(x)
]
p∗(x)+ Γ∇2
x p∗(x)
= −∇x
[
(Γ−Q)∇x p∗(x)
]
+Γ∇2
x p∗(x)
= −Γ∇2
x p∗(x)+ ∇xQ∇x p∗(x)+ Γ∇2
x p∗(x)
= ∇xQ∇x p∗(x) =0 (2.4)
Where the last line follows because, by deﬁnition, the gradient of the solenoidal ﬂow
with respect to the gradient of the log density is 0, since the solenoidal ﬂow must be
orthogonal to the gradient of the density, which is represented by the solenoidal Q
matrix being antisymmetric Q = −QT .
2.3 Markov Blankets
From these preliminaries, we have a set of dynamics of states x, which possess a NESS
density, and we can express the dynamics at the NESS density in terms of dissipative Γ
7Technically, we only need to assume a block-diagonal matrix, but we also typically also assume that
the noise in each state dimension is independent
Chapter 2. The Free Energy Principle 31
and a solenoidal Q ﬂows on the gradient of the log density. Now, we begin to explore
the statistical structure of these dynamics in terms of a Markov Blanket. Speciﬁcally,
we next require that we can partition the states x of the dynamics into three separate
units. External states η, internal states µ, and blanket states b such that x = [η,µ,b].
Intuitively, the external states represent the ‘environment’; the internal states represent
the ‘system’ we wish to describe, and the blanket states represent the statistical barrier
between the system and its environment. For instance, we might wish to describe
the dynamical evolution of a simple biological system such as a bacterium in such a
manner. Here, the internal states would describe the internal cellular environment of the
bacterium – the cytoplasm, the nucleus, the ribosomes etc. The external states would
be the environment outside the bacterium, while the blanket states would represent
the cell membrane, sensory epithelia, and potentially active instruments such as the
ﬂagella which interact physically with the external environment. The key intuition
behind the FEP is that although all inﬂuence between external and internal states is
mediated by the blanket states, simply maintaining the non-equilibrium steady state
against environmental perturbations requires that the internal states in some sense model
and perform (variational) Bayesian inference on the external states. The Markov Blanket
condition is straightforward. It simply states that the internal and external states must
be independent given the blanket states,
p∗(x) =p∗(η,µ,b) =p∗(η|b)p∗(µ|b)p∗(b) (2.5)
While in probabilistic terms this factorisation is straightforward, it has more complex
consequences for the dynamical ﬂow of the system. Firstly, we additionally decompose
the blanket states into sensorys and activea states such thatb = [s,a] and thus, ultimately
x = [η,µ,s,a]. Sensory states are blanket states that are causal children of the external
states – i.e. the states that the environment acts on directly. Active states are those
blanket states that are not causal children of the external states. Essentially, external
states inﬂuence sensory states, which inﬂuence internal states, which inﬂuence active
Chapter 2. The Free Energy Principle 32
Figure 2.2: The intuition behind the Markov Blanket partition. The brain (or bacillus)
consists of internal states µ which are separated from the outside world (external states
η by the blanket states b, which can themselves be partitioned into sensory states s,
representing the sensory epithelia, and which are directly inﬂuenced by external states,
and active statesa representing the organisms effectors and which are directly inﬂuenced
by internal states, and act on external states. We see that perception concerns the
minimization of free energy of the internal states, while action concerns the minimization
of the expected free energy of the active states. Figure originally appeared in Friston
(2019a)
Chapter 2. The Free Energy Principle 33
states, which inﬂuence external states. The circular causality implicit in this loop is
what allows the Markov Blanket condition to represent the perception-action loop. For
notational purposes, we also deﬁne autonomous states α = [a,µ] which consist of active
and internal states, and particular states π = [µ,s,a] which consist of sensory, active,
and internal states.
The next step is to understand what the conditional independence requirements put
forth in Equation 2.5 imply for the dynamics of the ﬂow. Speciﬁcally, we obtain the
marginal ﬂow lemma (see Friston (2019d) for a full derivation), which states that the
marginal ﬂow of a partition, averaged under its complement, can be expressed as an
Ao-decomposed ﬂow on the gradients of the log of the marginal distribution. For
instance, the ﬂow of the internal states µ, averaged under the complement ˜π of the
particular states π can be expressed as,
fµ(π) =: Ep(˜π|π)[dµ(x)
dt ] = (Γµµ −Qµµ)∇µ ln p∗(π)+ Qµ˜µ∇˜µ ln p∗(π) (2.6)
Importantly, we see that the marginal ﬂow lemma allows us to express the ﬂow of a
subset of states, averaged under their complements, in terms of independent Helmholtz
decompositions on their marginal NESS densities, if we ignore solenoidal coupling
terms (such as Qµ˜µ). This allows us to investigate in detail the information-theoretic
interactions of one set of states with another, and allows us to gain intuition and
understanding of the core information-theoretic properties of the perception-action loop.
For instance, using the marginal ﬂow lemma we can express the ﬂow of autonomous
(active and internal) α = (a,i) as,
fα(x) = (Γαα −Qαα)∇α ln p∗(π) (2.7)
where we see that autonomous states follow a gradient descent on the marginal NESS
density of the internal, sensory, and active states, and attempt to suppress their surprisal
or, on average, their entropy. We can use a series of mathematical ‘inﬂationary devices’
Chapter 2. The Free Energy Principle 34
to express this surprisal in terms of its interaction with the external states beyond the
blanket.
−ln p∗(π) =Ep∗(η|π)
[
−ln p∗(π)
]
= Ep∗(η|π)
[
ln p∗(η|π)−ln p∗(η,π)
]
= Ep∗(η|π)
[
ln p∗(η|π)−ln p∗(π|η)−ln p∗(η)
]
= Ep∗(η|π)
[
−ln p∗(π|η)
]
  
Inaccuracy
+DKL
[
p∗(η|π)||p(η)
]
  
Complexity
(2.8)
Thus we can see that the ﬂow of autonomous states acts to minimize the inaccuracy
(maximize accuracy) and minimize the complexity of the external states with respect
to the particular states of the system in question. Parsed into more intuitive terms, we
can thus see that the ﬂow of ‘system’ states (π) aim to maximize the likelihood of the
internal states given the external states – i.e. perform maximum likelihood inference on
themselves (c.f. ‘self evidencing’ (Hohwy, 2016)) – while simultaneously minimizing
the complexity – or the divergence between the external states given the internal states,
and the ‘prior’ distribution over the external states. In short, by re-expressing the ﬂow
in information-theoretic terms, we can obtain a decomposition of the entropy term into
intuitive and interpretable sub-components which can help us reason about the kinds of
behaviours these systems must exhibit.
2.4 Variational Inference
Variational inference is a technique and method for approximating intractable integrals
in Bayesian statistics (Feynman, 1998; Fox & Roberts, 2012; Ghahramani & Beal, 2001;
M. Jordan, Ghahramani, Jaakkola, & Saul, 1998; M. I. Jordan, Ghahramani, Jaakkola,
& Saul, 1999; Neal & Hinton, 1998). Typically, a direct application of Bayes-rule to
compute posteriors in complicated systems fails due to the intractability of the log model
evidence, which appears in the denominator of Bayes’ rule. While there exist numerical
or sampling-based methods to precisely compute this integral, they typically scale
Chapter 2. The Free Energy Principle 35
poorly with the dimension of the problem – a phenomenon which is known as the curse
of dimensionality (Goodfellow, Bengio, & Courville, 2016). Variational techniques
originated from methods in statistical physics in the 1970s and 1980s (Feynman, 1998),
and were then taken up in mainstream statistics and machine learning in the 1990s (Beal,
2003; Ghahramani & Beal, 2001; M. Jordan et al., 1998) where they have become an
inﬂuential, often dominant approach for approximating posteriors and ﬁtting complex
high-dimensional Bayesian models to data (Beal, 2003; Blei, Kucukelbir, & McAuliffe,
2017; Dayan, Hinton, Neal, & Zemel, 1995; Feynman, 1998; Ghahramani, Beal, et al.,
2000; M. I. Jordan et al., 1999; Kingma & Welling, 2013).
The core idea of variational inference is to approximate an intractable inference problem
with a tractable optimization problem. Thus, instead of directly computing a posterior
distribution p(H|D) where H is some set of hypotheses and D is the data, we instead
postulate an approximate or variational distribution q(H|D;θ) which is often, although
not always, parametrized with some ﬁxed number of parameters θ. We then seek to
optimize the parameters θ to minimize the divergence between the approximate and
true posterior,
θ∗= argmin
θ
DKL[q(H|D;θ)||p(H|D)] (2.9)
Unfortunately, this optimization problem is itself intractable since it contains the in-
tractable posterior as an element. Instead, we minimize a tractable bound on this
quantity called the variational free energy (VFE) F (D,θ),
F (D,θ) =DKL[q(H|D;θ)||p(H,D)]
= DKL[q(H|D;θ)||p(H|D)]−ln p(D)
≥DKL[q(H|D;θ)||p(H|D)] (2.10)
Since the VFE is based on a divergence between the variational distribution and the
generative model p(D,H), it is tractable as we assume we know the generative model
that gave rise to the data. By minimizing the VFE, therefore, we reduce the divergence
Chapter 2. The Free Energy Principle 36
between the true and approximate posteriors, and thus improve our estimate of the
posterior.
Secondly, the variational free energy is simultaneously a bound upon the log model
evidence ln p(D), a quantity of great important for model-selection (Friston, Parr, & Zei-
dman, 2018; Geweke, 2007), and which is usually intractable to compute due to the im-
plicit integration over all possible hypotheses (or parameters)p(D) =
∫
dH p(D|H)p(H).
ln p(D) =DKL[q(H|D;θ)||p(H,D)]−F (D,θ)
≥−F (D,θ) (2.11)
The second line follows due to the non-negativity of the KL divergence. The VFE is
the foundation of the free energy principle as, we shall show, we can interpret self-
organizing systems which maintain themselves at a non-equilibrium-steady state to be
implicitly minimizing the VFE, and thus performing variational Bayesian inference.
It is important to note here that while the variational free energy F is not technically a
KL divergence, since the two distributions it involves do not share the same support
(one being a posterior and the other a joint), for notational convenience in this thesis
we slightly abuse the KL notation to represent free energies of one form or another.
Formally, we will use F [q(x), p(x,y)] =Eq(x)[ln p(y|x)]+ DKL[q(x)||p(x)] =ln p(y)+
DKL[q(x)||p(x|y)] := DKL[q(x)||[(x,y)].
We can gain some intuition for the effects of minimizing the VFE by decomposing into
various constituent terms. Here we showcase two different decompositions which each
give light to certain facets of the objective function,
F (D,θ) =Eq(H|D;θ)[ln p(H,D)]  
Energy
−H[q(H|D;θ)]  
Entropy
(2.12)
= −Eq(H|D;θ)[ln p(D|H)]  
Accuracy
+DKL[q(H|D;θ)||p(H)]  
Complexity
(2.13)
Here we see that we can decompose the variational free energy into two separate decom-
positions, each consisting of two terms. The ﬁrst decomposition splits the VFE into an
Chapter 2. The Free Energy Principle 37
‘energy’ term, which effectively scores the likelihood of the generative model averaged
under the variational distribution, whilst the entropy term encourages the variational
distribution to become maximally entropic. Essentially, this decomposition can be inter-
preted as requiring that the variational distribution maximize the joint probability of
the generative model (energy), while simultaneously remaining as uncertain as possible
(entropy) 8. The second decomposition – into an ‘accuracy’ and a ‘complexity’ term –
speaks more to the role of the VFE in inference. Here the accuracy term can be inter-
preted as driving the variational density to produce a maximum likelihood ﬁt of the data,
by maximizing their likelihood under the variational density. The complexity term can
be seen as a regularizer, which tries to keep the variational distribution close to the prior
distribution, and thus restrains variational inference from pure maximum-likelihood
ﬁtting.
2.5 Intrinsic and Extrinsic information geometries
Now, we wish to understand the relationship between the internal states and the external
states, which are separated by the blanket states. Importantly, the existence of the
blanket means that we can deﬁne a mapping between the most likely internal state,
given a speciﬁc blanket state, and a distribution over external states 9. We deﬁne the
most likely internal and external states given a blanket state as,
ηηη(b) =argmax
η
p(η|b)
µµµ(b) =argmax
µ
p(i|b) (2.14)
Next, we assume that there is a smooth and differentiable function σ which maps
8Interestingly, this energy, entropy decomposition is precisely why this information-theoretic quantity
is named the variational free energy. The thermodynamic free energy, a central quantity in statistical
physics, has an identical decomposition into the energy and the entropy.
9This function is deﬁned if we assume injectivity between the most likely internal and blanket states
(Parr, Da Costa, & Friston, 2020).
Chapter 2. The Free Energy Principle 38
between the most likely internal and external states given a blanket state,
ηηη(b) =σ(µµµ(b)) (2.15)
Importantly, we interpret the output of this function – the most likely external states
given the blanket states – as parametrizing the mean over a full distribution over the
external states, as a function of the internal states q(η;ηηη(b)) =q(η;σ(µµµ(b))). This
allows us to interpret the ﬂow of internal states as parametrising distributions over the
external states.
Crucially, we can say that if any given set of internal states parametrizes a distribution
over external states, then the space of internal states effectively represent a space of
distributions over external states, parametrized by internal states. This space of dis-
tributions may be, and usually is, curved and non-euclidean in nature. The ﬁeld of
information geometry has emerged to allow us to describe and mathematically charac-
terise such spaces correctly (Amari, 1995; Caticha, 2015). A key result in information
geometry is that the space of parameters of families of exponential distributions is a non-
euclidean space with the Fisher Information as its metric. A metric is simply a notion of
distance for a given space. For instance, in Euclidean space, the metric is
√
∑N
µ x2µ where
N is the dimensionality of the space. We can represent general coordinate transformers
on spaces with any metric through the use of a metric tensorGGG. Essentially, we measure
differences between distributions in terms of the KL divergence, and thus if we want to
see how an inﬁnitesimal change in the parameters of a distribution results in changes to
the distribution itself, we can measure an inﬁnitesimal change in their KL divergence as
a function of the inﬁnitesimal change in the parameters. i.e.
∂p(x;θ)
∂θ = lim
δθ→0
DKL[p(x;θ)||p(x;θ+δθ)] (2.16)
In the case of the space of parameters of exponential distributions, the metric tensor is
the Fisher information, which arises as from the Taylor expansion of the inﬁnitesimal
KL divergence between the two distributions. We deﬁne θ′= θ+δθ. Speciﬁcally, since
Chapter 2. The Free Energy Principle 39
there is only an inﬁntesimal change, we can Taylor-expand around θ′= θ to obtain,
DKL[p(x;θ)||p(x;θ′)] ≈DKL[p(x;θ)||p(x;θ)]  
=0
+ ∂DKL[p(x;θ)||p(x;θ′)]
∂θ |θ=θ′(θ−θ′)
  
=0
+ ∂2DKL[p(x;θ)||p(x;θ′)]
∂θ2 |θ=θ′(θ−θ′)2 (2.17)
‘ Where the ﬁrst two terms vanish, so we need only handle the second term,
DKL[p(x;θ)||p(x;θ′)] ≈∂2DKL[p(x;θ)||p(x;θ′)]
∂θ2 |θ=θ′(θ−θ′)2
=
∫
p(x;θ)∂ln p(x;θ)
∂θ
∂ln p(x;θ)
∂θ
= F (2.18)
where µ is the Fisher information. Since the internal states can be interpreted as
parametrizing distributions over external states, as parameters, they lie on an information-
geometric manifold with a Fisher information metric. This is the extrinsic information
geometry. Simultaneously, the internal states also parametrize (implicitly) a second (em-
pirical) distribution over the internal states. This parametrization gives rise to a second
information geometry – the intrinsic geometry, since it represents the relationship the
internal states have to the distribution over themselves. Speciﬁcally, suppose µµµ deﬁne
the sufﬁcient statistics of a variational density over internal states q(µ;µµµ), and ηηη = σ(µµµ)
deﬁne the sufﬁcient statistics of the variational density over external states q(η;ηηη), then
we can see that the internal states in fact parametrize two densities and thus partake in
two simultaneous information geometries. First, there is a metric deﬁned over the space
of internal densities,
µ(µµµ) =∂2DKL[q(µ;µµµ)||q(µ;µµµ +δµµµ)]
∂µµµ2 |µµµ+δµµµ=µµµ (2.19)
which is called the intrinsic information geometry. And secondly, a metric deﬁned over
the space of external densities, parametrized by internal states,
µ(ηηη) =∂2DKL[q(η;ηηη)||q(η;ηηη+δηηη)]
∂ηηη2 |ηηη+δηηη=ηηη (2.20)
Chapter 2. The Free Energy Principle 40
which is called the extrinsic information geometry. These well-deﬁned intrinsic and
extrinsic information geometries, allow us to interpret the motion of the internal as
also movement on the intrinsic and extrinsic statistical manifolds. Crucially, enabling
us to make mathematically precise the link between two conceptually distinct ideas –
dynamical motion in space, and variational inference (i.e. Bayesian belief updating) on
parameters of distributions. Using this underlying information-geometric framework,
in the next section we shall go on to see how we can interpret the dynamics of a non-
equilibrium system at NESS as performing approximate variational Bayesian inference
on its external environment.
2.6 Self-Organization and Variational Inference
Here we present the key results of the free energy principle via the free energy lemma.
This says, ﬁrstly, that the dynamics of the autonomous states can be interpreted as
minimizing a free energy functional over the external states, and thus can be construed
as performing a kind of elemental Bayesian (variational) inference. Speciﬁcally, we will
ﬁrst consider the general case in terms of the ‘particular’ free energy, which stipulatively
assumes that the system obtains the correct posterior at every time-point, rendering the
traditional variational bound superﬂuous, and thus demonstrating that in a way self-
organizing systems maintaining themselves at NESS can be construed as performing
exact Bayesian inference on the generative model they embody through their NESS
density. We thus reach the key statement of the FEP – that the dynamics of self-
organizing systems that maintain themselves at NESS can be interpreted as performing
exact Bayesian inference on the external states beyond their blanket or, alternatively,
they can be interpreted as approximating approximate (variational) Bayesian inference.
We then introduce the general case of the variational free energy, which is in general a
bound upon the log of the NESS density, and we show in the special case of assuming
that the variational distribution over external states which is parametrized by the internal
Chapter 2. The Free Energy Principle 41
states can be approximated by the Laplace approximation, that we can interpret the
ﬂow of autonomous states as directly performing a descent upon the variational free
energy and thus directly performing variational Bayesian inference. Since we, as
the modeller, can specify the variational distribution in any desired way, then this
means that this interpretation is tenable for an extremely wide range of systems. The
Laplace approximation approximates the variational distribution as a Gaussian where
the variance is a function of the curvature at the mean. Intuitively, this assumption
is that the Gaussian is tightly peaked around the mean value. This approximation is
theoretically well-justiﬁed, due to the underlying Gaussianity of the stochastic noise
in the system, and the likely concentration of the probability mass around the mean.
Moreover, the Gaussian distribution arises regularly in nature whenever averages over
large numbers of independent events are taken (c.f. the Central Limit Theorem (CLT)),
and can thus be considered a natural modelling choice for distribution of the mode of
the external states given the blanket, which likely is composed of contributions from a
large number of speciﬁc external states.
To recall, we can write the ﬂow of autonomous states α = (i,a) in terms of a gradient
descent on the log NESS density of the particular states ln p(π) with both dissipative
and solenoidal components via the Helmholtz decomposition.
fα(x) =−(Γ−Q)∇α ln p∗(s,α) (2.21)
Then we can deﬁne the particular free energy as the variational free energy, where
the variational distribution over external states, is stipulatively deﬁned to be equal to
the ‘true’ posterior distribution over external states given the particular statesq(η|π) =
p∗(η|π) 10. With this assumption, we can deﬁne the particular free energy using the
10We implicitly assume here that the variational distribution can be stipulated to be of the same family
of the true posterior, so that they can match one another
Chapter 2. The Free Energy Principle 42
standard form for the variational free energy
Fparticular = DKL[q(η|π)||p∗(η,π)]
= −Eq(η|π[ln p∗(η|π)]  
Accuracy
+DKL[(qη|π)||p∗(π)]  
Complexity
= ln p∗(π)  
Evidence
+DKL[q(η|π)||p∗(η|π)]  
Bound
= ln p∗(π) (2.22)
where the last line follows because the bound is always 0 since we have deﬁned the
variational and true posteriors to be the same. Importantly, we see that the particular
free energy is then equal to the log of the NESS density over the sensory, internal, and
active states. As such, we can rewrite the dynamics of the autonomous states directly in
terms of the particular free energy,
fα(x) =−(Q −Γ)∇αFparticular (s,α) (2.23)
While this may seem like just a mathematical sleight of hand, it demonstrates how
systems which maintain the statistical structure of a Markov Blanket at equilibrium
can in fact be interpreted as performing variational Bayesian inference with a correct
posterior distribution. If, conversely, we relax this assumption somewhat, so that, as is
typical for variational inference when the class of distributions represented under the
variational density does not include the true posterior, then we retain an approximate
relationship. That is, when q(η|π;θ) ≈p(η|π), we obtain,
F = DKL[q(η|π)||p∗(η,π)]
= ln p∗(π)+ DKL[q(η|π)||p∗(η|π)]
≈ln p∗(π)
=⇒ fα(x) ≈−(Q −Γ)∇αF (s,α)
So we can see that in this case, we can interpret the dynamics of the autonomous states
as approximating approximate Bayesian inference. This is perhaps the most general
Chapter 2. The Free Energy Principle 43
statement of the FEP – that the dynamics of a system which maintains the statistical
structure of a Markov Blanket at NESS against external dissipative perturbations, can
be interpreted as performing approximate variational Bayesian inference to optimize a
distribution over the external states of the environment, parametrized by its own internal
states. The distinction between variational and particular free energy, with the particular
free energy always using the stipulatively correct posterior, while being somewhat a
mathematical trick, is also a useful philosophical distinction to draw. In effect, we can
think of the system as always performing correct Bayesian inference, simply because
the inference is over the system itself, where the generative model of the system is
simply its NESS density. Conversely, we can see the approximation arising from the
approximate variational distribution as being related to the imperfection of our own
understanding of the system as an exogenous modeller. The system is perfectly happy
using its Bayes-optimal posterior at all times. A variational distribution distinct from
this posterior must be, in some sense, the creature and creation of the modeller, not
of the system, and as such the approximations to the dynamics that arise from this
approximation is due to the approximations implicit in modelling rather than in the
dynamics of the system per-se. It is also important to note that while we have used an
approximation sign, in reality the variational free energy is anupper bound upon the log
model evidence or the particular free energy – i.e. F ≥Fparticular and the approximate
dynamics can be interpreted as driving the system towards the minimization of this
bound, and thus increasing the accuracy of the approximation in a manner analogous to
the similar process inherent in variational inference.
While in the general case above, the relationship between the dynamics of the system
and variational inference is only approximate, if we are only interested in themode of
the external states – i.e. the most likely external state conﬁguration – instead of the full
distribution, then the approximation becomes exact and we can directly see that the
dynamics of the system do perform variational inference upon the mode of the external
states. Here we can see that, in a sense, the maximum-a-posteriori (MAP) modes for
Chapter 2. The Free Energy Principle 44
the internal states precisely track the MAP modes for the external states and thus, under
the Laplace approximation, can be seen as directly performing a minimization of the
variational free energy.
Firstly, recall from previously that we had deﬁned the smooth mapping between the
modes of the external and internal states given the blanket state, ηηη(b) =σ(µµµ(b)). By
applying the chain rule to this function, it is straightforward to derive the ﬂow of the
external mode with respect to the internal mode,
fηηη(b) =∂σ(µµµ(b))
∂µµµ(b) fµµµ(b) (2.24)
Then, assuming that the mapping is invertible (requiring that the internal states and
external states have the same dimensionality), or rather in the general case that it has a
Moore-Penrose pseudoinverse, we can express the dynamics of the internal mode in
terms of the dynamics of the external mode,
fµµµ(b) =∂σ(µµµ(b))
∂µµµ(b)
−1
fηηη(b) (2.25)
Similarly, we can derive the expression of the NESS density over the external mode
in terms of the mode of the internal states, which provides a precise mapping, called
the synchronization manifold, between the two densities, even though they are in fact
separated by the Markov Blanket,
∂ln p(ηηη(b)|b)
∂µ = ∂ln p(ηηη(b)|b)
∂ηηη(b)
∂σ(µµµ(b))
∂µ (2.26)
Combining Equation 2.24 and Equation 2.26 and using the fact that the ﬂow of the
external mode, by the marginal ﬂow lemma is, fηηη(b) = (Γη −Qη)∇η ln p(ηηη(b)|b), we
can express the ﬂow of the internal mode in terms of the marginal NESS density over
the external states, thus understanding how the internal states probabilistically track
Chapter 2. The Free Energy Principle 45
changes in their environment,
fµµµ(b) =∂σ(µµµ(b))
∂µµµ(b)
−1 dηηη(b)
dt
= ∂σ(µµµ(b))
∂µµµ(b)
−1
(Γη −Qη)∇η ln p(ηηη(b)|b)
= ∂σ(µµµ(b))
∂µµµ(b)
−1
(Γη −Qη)∂σ(µµµ(b))
∂µµµ(b)
−1 ∂σ(µµµ(b))
∂µµµ(b) ∇η ln p(ηηη(b)|b)
= (Γσ −Qσ)∇µ ln p(σ(µµµ(b))) (2.27)
where (Γσ −Qσ) =∂σ(µµµ(b))
∂µµµ(b)
−1
(Γη −Qη)∂σ(µµµ(b))
∂µµµ(b)
−1
. Crucially, this expression allows us
to write the ﬂow of the internal mode as a gradient descent on the NESS density of the
external mode as a function of the internal mode, given the blanket, with respect to
the internal states. Fascinatingly, this relationship takes the same general form of the
Helmholtz decomposition with separate dissipative Γσ and solenoidal Qσ components
which are simply the original dissipative and solenoidal components with respect to
the internal states modulated by the inverse of the mapping function σ. In effect, this
implements a coordinate transform between the coordinates of the ﬂow of the external
states to the coordinates of the ﬂow of the mode of the external states, as a function of
internal states.
Now we demonstrate how we can interpret this gradient descent on the NESS density
of the mode over external states in terms of a direct descent on the variational free
energy, and thus as directly and exactly performing variational inference. First, we must
deﬁne our variational distribution q(ηηη|b;µµµ) which is a distribution over the modes of
external states, given the blanket states, parametrized by the mode of the internal states.
Since we are only interested now in distributions over the mode of the external states,
a reasonable assumption is that it is approximately Gaussian distributed due to the
central limit theorem. This means that a Laplace approximation, which is a Gaussian
approximation where the covariance is simply a function of the mean, derived via a
second order Taylor-expansion of the density at the mode, is a good approximation to
Chapter 2. The Free Energy Principle 46
use here. We thus deﬁne the variational density as,
q(ηηη|b;µµµ) =N (ηηη;µµµ,Σ(µµµ))
where Σ(µµµ) =∂2σ(µµµ)
∂σ2
−1
(2.28)
Importantly, if we substitute this deﬁnition of q into the variational free energy and drop
constants unrelated to the variational parameters µµµ, we obtain,
F = ln p(µµµ,b)+ 1
2tr(Σ(µµµ))∂2σ(µµµ)
∂σ2
−1
+ln|Σ(µµµ)|
=⇒ ∂F
∂µ = ∂ln p(µµµ,b)
∂µ
The second line follows since this is the only term where i is directly utilized. Then,
from this deﬁnition, we can see that the variational free energy is actually precisely
the gradient term we see in the expression for the ﬂow of the internal state mode, thus
allowing us to rewrite it as,
fµµµ(b) = (Γσ −Qσ)∇µF (2.29)
After this thicket of mathematics, we thus see a crucial result for the FEP. That, with
a Laplace-encoded variational density, we can see that the mode of the internal states
precisely tracks the mode of the external states, and the dynamics that allows it to
do so are precisely those of a gradient descent on the variational free energy, thus
enabling an exact interpretation of the ﬂow of the internal states as performing Bayesian
inference on the external states. This proof demonstrates the fundamentally Ashbyan
nature of self-organization at non-equilibrium steady state, where systems, in order to
maintain their steady state, and thus existence as distinct systems, are necessarily forced
to engage in some degree of modelling or tracking external states of the environment,
in order to counter their dissipative perturbations. Interestingly, this exact relationship
to variational inference only emerges when considering the modes of the system, not
the full distribution over environmental and internal states as was done previously,
where we only obtained an approximation to variational inference. Perhaps this is
Chapter 2. The Free Energy Principle 47
because, in some sense, the system need not perform inference on full distributions,
but only on modes. This perhaps makes more intuitive sense within the cybernetic
Ashbyan paradigm where, in general, the system is seen as signiﬁcantly smaller than
the environment, and thus simply cannot be expected to encode a fully accurate model
of the entire environment which, in the extreme case, includes the entire rest of the
universe. Instead, the system simply models and tracks coarse-grained environmental
variables such as the mode.
2.7 The Expected Free Energy and Active Inference
So far, we have only considered the relationship between internal and external states,
and observed that the ﬂow of the internal state can be considered to be performing a
variational gradient descent on the parameters of the variational density over external
states. The internal state dynamics exactly follow a variational gradient descent if we
assume that the internal states parametrize a Laplacian approximate posterior, or they
approximately follow a variational gradient descent if we assume a broader class of
variational posteriors. From this, we can interpret the ﬂow of the internal states as
performing some kind of ‘perceptual’ inference about the causes of ﬂuctuations in the
blanket states – namely, the external states. But what about the active states? How do
they ﬁt into this picture?
First, we recall from the approximate Bayesian inference lemma that we can express the
ﬂow of the autonomous states (active and internal) in terms of an approximate gradient
descent on the variational free energy (Equation 2.29). By the marginal ﬂow lemma, if
we ignore solenoidal coupling between internal and active states, we can partition this
descent into separate (marginal) descents on the internal and the active states, allowing
us to write the ﬂow of the active states as
fa(x) ≈(Γaa −Qaa)∇aF (s,α) (2.30)
Chapter 2. The Free Energy Principle 48
where Γaa and Qaa are the block matrices corresponding solely to the interactions
between active states in the largerΓ and Q matrices. Crucially, if we recall the deﬁnition
of the variational free energy,
F (π) =DKL[q(η|π;µµµ)||p∗(η,π)]
= −Eq(π|µµµ)[−ln p∗(π|η)]  
Inaccuracy
+DKL[q(η|π;µµµ)||p∗(η)]  
Complexity
(2.31)
Crucially, the only term in this decomposition that depends on the active states a is the
ﬁrst inaccuracy term. Thus, we can straightforwardly write down the ﬂow of the active
states as,
fa(x) ≈(Γaa −Qaa)∇aEq(η|µµµ)[−ln p∗(π|η)] (2.32)
Where we can intuitively see that the ﬂow of the active states effectively minimize
inaccuracy (or maximize accuracy). In effect, we can interpret the ﬂow of the active
states at the NESS density to try to ensure that the variational ‘beliefs’ encoded by the
blanket and internal states of the system are as accurate as possible. Since active states
can only inﬂuence external states and not internal states, the way this is achieved is by
acting upon the external states to bring them into alignment with the beliefs represented
by the internal states – hence active inference.
While this provides a good characterisation of the ﬂow of the system at equilibrium,
we are often also interested in the properties of dynamical systems as the self-organize
towards equilibrium. Speciﬁcally, we wish to characterise the nature of the active states
during this process of self-organization, so that we can understand the necessary kinds
of active behaviour any self-organizing system must evince. To begin to understand the
nature of this self-organization we ﬁrst deﬁne another information theoretic quantity, the
Expected Free Energy(EFE) which serves as an upper-bound on surprisal throughout
the entire process of self-organization, with equality only at the equilibrium itself.
Since we have this upper-bound, we can interpret self-organizing systems away from
equilibrium, by following their surprisal dynamics as approximating expected free
Chapter 2. The Free Energy Principle 49
energy minimization, using logic directly analogous to the approximate Bayesian
inference lemma. Conversely, turning this logic around lets us construct self-organizing
systems by deﬁning some desired NESS density, and then prescribing dynamics which
simply minimize the EFE.
To handle systems away from equilibrium, we deﬁne some new terminology. We deﬁne
p(ηt,µt,st,at|η0,µ0,s0,a0) to be the probability density over the variables of the system
at some time t, which depends on some set of initial conditionse0,µ0,s0,a0. To simplify,
we average over the external initial condition and only represent the particular initial
condition π0 = (µ0,s0,a0). Next we deﬁne the expected free energy G(π) similarly to
the variational free energy, but with the current-time predictive density taking the place
of the approximate variational posterior, and the NESS density taking the place of the
generative model.
G(π) =Ep(ηt ,πt )|πt )[ln p(ηt|πt,π0)−ln p∗(η,π)]
= Ep(ηt ,πt )|πt )[−ln p∗(π|η)]  
Ambiguity
+DKL[p(ηt|πt,π0)||p∗(η)]  
Risk
(2.33)
We see that the EFE mandates the minimization of both ambiguity (i.e. avoiding
situations which are heavily uncertain) and risk (avoiding large divergences between
the current state density and the equilibrium state. It is straightforward to see that the
EFE is an upper bound on the expected predictive surprisal at any time-point, by using
the fact that the KL-divergence is always greater than or equal to 0,
DKL[p(ηt,πt|π0)||p∗(η,π)] ≥0
=⇒G(πt)+ Ep(ηt ,πt )|πt ) ln p(πt|π0)] ≥0
=⇒G(πt) ≥−Ep(ηt ,πt )|πt )[ln p(πt|π0)]
Similarly, it is straightforward to see that at equilibrium, the EFE simply becomes the
surprisal.
DKL[p(ηt,πt|π0)||p∗(η,π)] =G(πt)+ Ep(ηt ,πt )|πt )[ln p(πt|π0)] =0
=⇒G(πt) =−Ep(ηt ,πt )|πt )[ln p(πt|π0)] (2.34)
Chapter 2. The Free Energy Principle 50
Since this is the case, we can understand the EFE as effectively quantifying the discrep-
ancy between the current predictive density and the equilibrium. Because of this, we
can see that the EFE is necessarily a Lyapunov function of self-organizing dynamics,
and it makes sense to interpret self-organizing dynamics under a Markov blanket as
minimizing the EFE. Conversely, if one wants to deﬁne a set of dynamics that self-
organize to some given attractor p∗(η,π) then one simply needs to deﬁne dynamics that
minimize the EFE to achieve convergence to the equilibrium (in the case where there
are no local minima).
Taking this converse approach allows us to move from simply providing an interpretative
characterisation of given dynamics in terms of inference, and move instead to construct-
ing or deﬁning systems, or agents, which can achieve speciﬁc goals. This approach is
taken in the literature on active inference process theories (Da Costa, Parr, et al., 2020;
Friston, FitzGerald, et al., 2017a; Friston, Rigoli, et al., 2015a; Friston et al., 2012)
where instead of simply describing a given stochastic differential equation, we instead
consider the NESS density to be thepreferences or desires of the agent often represented
as a Boltzmann distribution over environmental rewards p∗(η,π) =exp(−r(η)) and
the active states (the agent’s actions) being computed through a minimization of the
EFE, with this minimization either taking place directly as a gradient descent in contin-
uous time and space (Friston et al., 2009) or else as an explicit model-based planning
algorithm as in the discrete-time and discrete-space formulation (Friston, FitzGerald,
et al., 2017b; Millidge, 2020; Millidge, Tschantz, Seth, & Buckley, 2020b; Tschantz,
Millidge, et al., 2020b).
2.8 Philosophical Status of the FEP
It is worth stepping back from the mathematical morass, at this point, to try to deﬁne at
a high level what kind of theory, philosophically speaking the FEP is, and what kind
of claims it makes. There have been numerous debates in the literature about whether
Chapter 2. The Free Energy Principle 51
the FEP is ‘falsiﬁable’, or whether it is ‘correct’, and whether or not it makes any
speciﬁc, empirical claims (Andrews, 2020; D. Williams, 2020). However often debates
on this matter are obscured or confused by the challenging and deep mathematical
background required for a full understanding of the speciﬁcs of the FEP. It is clear
from the mathematics that the FEP offers only an ‘interpretation’ of already extant
dynamics. In short, FEP presupposes the existence of the kinds of dynamics it wishes to
make sense of – dynamical systems which organize themselves into a non-equilibrium
steady state, and which maintain the requisite statistical independency structure of
the Markov Blanket condition. Once these conditions are satisﬁed, the FEP gives an
interpretation of the dynamical evolution of such a system as performing a kind of
variational Bayesian inference whereby the internal states of the system (deﬁned by the
Markov Blanket partition) can be seen as inferring or representing external states which
are otherwise statistically isolated behind the Markov Blanket. Crucially, the FEP, in its
most general formulation does not make any speciﬁc predictions about the ﬂow of the
system. It offers an interpretation only. While systems that implement the FEP can be
derived, and several process theories have been explicitly derived from within the FEP
framework (Friston, 2005; Friston, Rigoli, et al., 2015a), all such theories necessitate
making speciﬁc and ultimately arbitrary modelling choices, such as of the generative
model and variational density. Such choices sit below the level of abstraction that the
mathematical theory of the FEP exists at. The FEP offers a mathematical interpretation
only of certain dynamical structures.
The FEP is often compared and analogised to the principle of least action in physics
(Lanczos, 2012) which allows one to describe many physical processes (although not
all) as minimizing the path integral of a functional called the ‘action’ over a trajectory
of motion (Sussman & Wisdom, 2015). This argument is often used to claim, in
my opinion correctly, that the FEP is a mathematical ‘principle’ or interpretation and
therefore cannot be falsiﬁed or empirically tested. In my opinion, however, the principle
of least action is, in its philosophical status, not directly analogous to the FEP. While
Chapter 2. The Free Energy Principle 52
the relationship between the path integral of the action and the dynamics prescribed
by the Euler-Lagrange equations is simply a mathematical truth, the principle of least
action itself, as applied to physics contains a fundamentally empirical and falsiﬁable
claim – that physical systems in the real world can be well described through its own
mathematical apparatus – that is of dynamics derived from minimizing an action. This
claim is in principle falsiﬁable. Not all dynamical systems can be derived from least
action principles. If physical systems predominantly came from the class that cannot be
so derived, the principle of least action in physics would be effectively falsiﬁed, and the
mathematical apparatus underlying it would have become nothing more than an arcane
mathematical curiosity. So far as we know, there is no a-priori reason why much of
physics can be so well understood through action principles, and indeed there are areas
of physics – such as statistical mechanics and thermodynamics, and dissipative non-
conservative systems in general – which cannot (so far) be described straightforwardly
in these terms.
It appears a closer physics analogy to the FEP might be one direction of Noether’s
theorem. Noether’s theorem proves a direct correspondences between symmetries
or invariances in a given system, and conservation laws. For instance, in physical
systems, time-translation symmetry implies the conservation of energy, and rotational
symmetry (of the underlying euclidean space, not any given object within it) implies the
conservation of angular momentum. The FEP, similarly, can be thought to show a corre-
spondence between the dynamics of a certain kind of system (NESS density, Markov
Blanket conditions) and the dynamics of variational Bayesian inference. Interestingly,
while the ‘forward’ direction from the NESS density and Markov Blanket conditions
is treated in the FEP, any reverse conditions – i.e. whether the presence of Bayesian
inference dynamics implies any kind of statistical structure upon the dynamics of the
system remains unclear, and this is likely a fruitful direction for further theoretical work.
Noether’s theorem, unlike the principle of least action, matches more closely than the
principle of least action since it only speciﬁes correspondences between certain kinds of
Chapter 2. The Free Energy Principle 53
mathematical objects (symmetries and conservation laws) just as the FEP only speciﬁes
a correspondence between dynamical ﬂows at NESS of a system with a Markov Blanket,
and the gradient ﬂows on the variational free energy.
While its status as a mathematical principle and interpretation only can shield the FEP
from the possibility of an empirical ‘falsiﬁcation’, this does not mean that the theory is
not subject to some kind of implicit intellectual review. Much of the core motivation
behind the FEP has been to try to derive universal properties of the kind of biological
self-organizing systems which give rise to structured behaviour including relatively
‘high level’ processes such as the perception-action-loop, explicit perception and infer-
ence about the causes of the external world and, ultimately, prospective inference and
planning. For instance, much of the FEP literature has been focused on and applied
to understanding brain function (Friston, 2008a; Friston, FitzGerald, et al., 2017b;
Friston, Rigoli, et al., 2015a). This ambition renders the FEP open to questions about its
‘applicability’, if not its falsiﬁability. The FEP imposes relatively stringent conditions
that dynamical systems must satisfy for the logical steps in the FEP to hold. In the next
section, we present a detailed itemized list and critical discussion of all the assumptions
required. Some especially key assumptions, which substantially restrict the potential
class of systems the FEP can apply to are:
• That the system in question can be adequately represented as a Langevin equation
(i.e. the system is Markov and does not depend on history) with additive white
Gaussian noise.
• that the dynamical system as a whole have a well-deﬁned NESS density (including
over the external states).
• that the system obey the Markov Blanket conditions, which are, in general,
relatively restrictive about the kinds of ﬂows that are possible, and appear to have
become more restrictive in Friston, Da Costa, and Parr (2020), which precludes
any solenoidal coupling between active and sensory states (indeed the didactic
Chapter 2. The Free Energy Principle 54
treatments of the free energy lemma typically require a block-diagonal Q matrix,
meaning no solenoidal coupling between subsets of states). If this assumption
is relaxed, then there are additional solenoidal coupling terms in the ﬂow of the
internal states, so at best one can say that gradient ascent upon the surprisal is a
component of the ﬂow.
• That there be an injective mapping between the most-likely internal state given the
blanket and the mode of the distribution of external states given the blanket states,
which is additionally smooth and differentiable (this is required for the dual-aspect
information geometry, and thus the identiﬁcation with Bayesian inference).
These conditions are quite strict about the class of systems that the FEP can apply
to, and it is unclear if ‘real systems’ of the kind that FEP desires to explain – such
as biological self-organization, and especially brains, can fulﬁl them. If it turns out
that such systems ﬂagrantly violate the conditions for the FEP, then the FEP cannot
be said to apply to them and thus cannot be of use in understanding them, even as an
interpretatory device. In this case, the FEP would fail the applicability criterion, and
would cease to be particularly useful for its original goals of neuroscience, even if it
remains not technically falsiﬁed and does, in fact, apply to some obscure mathematical
class of dynamical systems. Importantly, many of the assumptions of the FEP, when
interpreted strictly, do not appear to hold in general for complex biological systems
such as brains. For instance, to take extreme but illustrative examples, it is clear that no
biological system is ever in a true non-equilibrium steady state, since eventually all such
organisms will age and die, and indeed eventually the entire universe will likely decay
to a thermodynamic equilibrium state. Additionally, the Markov Blanket assumption
is directly violated by things such as x-rays (and indeed gravity) which can directly
interact with ‘internal states’ of the brain, such as neurons, without ﬁrst passing through
the Markov Blanket of the physical boundaries of the brain and the sensory epithelium.
As such, for a real physical system, we must take the assumptions of the FEP to be
Chapter 2. The Free Energy Principle 55
only approximations, which hold locally, or approximately, but not for all time and with
complete perfection. It remains to be seen, and empirically investigated if possible,
the extent to which the mathematical interpretations and logical statements of the FEP
remain robust to such slight relaxations of its core assumptions.
While the FEP provides a mathematical interpretation of certain kinds of dynamics in
terms of inference, it also, largely, remains to be seen whether such an interpretation
is useful for spurring new ideas, questions, and developments within the ﬁelds the
FEP hopes to inﬂuence – such as neuroscience, cognitive science, and dynamical
systems theory. Returning to our anaologies of the least action principle and Noether’s
theorem, while both of these mathematical results only provide interpretations of known
dynamics, by operating at a high level of abstraction they provide powerful capabilities
for generalization. For instance the principle of least action allows for dynamics to be
derived, via the Euler-Lagrange equations, directly from the high level speciﬁcation
of the action. For instance, the potentially new or counterfactual laws of physics
can be derived simply by postulating a given Lagrangian or Hamiltonian and then
working through the mathematical machinery of the principle of least action to derive
the ensuing dynamics. Additionally, by investigating invariances in the action, one
can often understand the kinds of invariances and degrees of freedom that exist in the
actually realized dynamics. Similarly, Noether’s theorem allows one to play with setting
up certain conserved quantities or symmetries a-priori, and then work out precisely the
consequences that these entail for the dynamics.
It is currently unclear to what extent the FEP offers such powerful advantages of
abstraction and generalization. This is largely due to the FEP being immature as a ﬁeld
compared to the cornerstones of classical physics, and the majority of the research effort
so far has gone into making the theory precise rather than deriving consequences and
generalizations from it, but there are some promising initial signs which have just begun
to emerge in the literature of the power the FEP perspective offers. From a practical
Chapter 2. The Free Energy Principle 56
perspective, the FEP appears to offer a number of novel techniques. Firstly, given a
desired NESS density, the free energy lemma provides a straightforward way of deriving
dynamics which will necessarily reach that density, due to the fact that the variational
free energy becomes a Lyapunov function of the system as a whole. This approach has
strong potential links to Markov-Chain-Monte-Carlo methods in machine learning and
statistics, which aim to approximate an intractable posterior distribution by the time
evolution of a Markov process (M. Betancourt, 2017; Brooks, Gelman, Jones, & Meng,
2011; Chen, Fox, & Guestrin, 2014; Metropolis et al., 1953; Neal et al., 2011). The FEP
provides a new perspective on such systems as fundamentally performing variational
Bayesian inference, and may in future be used to develop improved algorithms in
this domain, akin to the developments of Hamiltonian (M. J. Betancourt, 2013) and
Riemannian MCMC (Girolami & Calderhead, 2011) methods. For instance, there is
much potential in the idea of solenoidal ﬂow speeding up convergence to the desired
equilibrium density (Ma, Chen, & Fox, 2015). Conversely, the FEP, through the
Helmholtz decomposition, may additionally provide tools for inferring the eventual
NESS density given a speciﬁc set of dynamics (Friston, 2019a; Ma et al., 2015). This
would allow, again, for an analytical or empirical characterisation of the ultimate fate
of a system, and allow for characterising different kinds of systems purely by their
dynamics far from equilibrium.
A second strand of potentially directly useful research which has begun to arise from
the FEP is empirical and statistical methodologies for deﬁning, computing, and approxi-
mating Markov Blankets. This implies the ability to infer the statistical independency
structure of the dynamics purely either from analytical knowledge of the dynamics or,
alternatively, from purely observed trajectories. There are already two approaches to
achieve this in the literature. One which utilizes graph theory in the form of the graph
Laplacian to infer nodes of the Markov blanket based on the parents, and children of
parents of the largest eigenstates of the Jacobian (Friston, 2013; Friston, Fagerholm, et
al., 2020; Palacios, Razi, Parr, Kirchhoff, & Friston, 2017). A second approach directly
Chapter 2. The Free Energy Principle 57
uses the Hessian of the dynamics to attempt to read off the conditional independency
requirements it implies (Friston, Fagerholm, et al., 2020). These approaches may have
substantial merit and utility in understanding the effective statistical independency
structure of complex dynamical processes, especially questions regarding functional
independence in the brain. This strand of research heavily relates to the question of
abstraction in dynamical systems – namely, whether complex systems can or cannot be
straightforwardly partitioned into independent subsystems which can then be abstracted
over. For instance, the ideal would be the ability to, given a complex high-dimensional
dynamical system, parse this system into individual ‘entities’ (separated by Markov
blankets) which interact with each other according to another set of (hopefully simpler)
dynamical rules. This would allow for an automatic procedure to transform a high
dimensional complex system into a simpler, low-dimensional approximate system more
amenable for analysis and, ultimately understanding (Friston, 2013; Friston et al., 2007;
Parr, Sajid, & Friston, 2020).
2.9 Discussion of Assumptions required for the FEP
Here we provide a general overview and short discussion of every assumption required
at each stage of the FEP. Ultimately, the overall picture that emerges is that the FEP
requires many assumptions to work, and it is unlikely that all of them can be fulﬁlled
by the kinds of complex self-organizing systems that the FEP ultimately ‘wants’ to
be about – such as biological self organization and, ultimately, brains. However, this
is not necessarily overly problematic for the FEP as many of its assumptions may be
approximately, or locally true over small enough time periods. This is not necessarily
a bad thing – almost all of the sciences ultimately use simpliﬁed models to try to
understand their ultimate objects of study in a more tractable way. The FEP is simply
continuing that tradition, but if we do this, we need to make explicit the key distinction
between the model and the reality or, more memorably, the map and the territory.
Chapter 2. The Free Energy Principle 58
The ﬁrst set of key assumptions that the FEP makes comes through the deﬁnition of
the kinds of stochastic dynamical systems that it works with. Speciﬁcally, we make the
following assumptions about the form of the dynamics we deal with,
• The system as a whole can be modelled as a Langevin SDE of the form dx
dt =
f (x)+ ω
• The noise ω is Gaussian with 0 mean and a covariance matrix 2Γ.
• The noise is additive to the dynamics
• Γ does not change with time
• Γ has no state dependence (no heteroscedastic noise)
• Γ is a diagonal matrix (each state dimension has independent noise)
• The dynamics f (x) do not themselves change with time.
We also must make the following assumptions about the system as a whole,
• The system is ergodic, which means that state and time averages coincide or,
alternatively, that there must be some probability of ultimately reaching every
part of the system from every other part.
• The system possesses a well characterized nonequilibrium-steady-state density
(NESS), which does not change over time
• Once the system reaches this NESS density it cannot escape it – there is no
metastability or multiple competing attractors.
These assumptions setup the basic formalism we wish to consider. From here, we
then apply the Ao decomposition to rewrite the dynamics in the form of a gradient
descent on the log of the potential function with dissipative and solenoidal components
f (x) = (Q −Γ)∇x ln p∗(x). To be able to implement this decomposition requires,
• The dynamics function f be smooth and differentiable
Chapter 2. The Free Energy Principle 59
Now, we apply the Markov Blanket conditions at the NESS density,
• The state space x can be partitioned into a set of four states – internal i, external e,
active a and sensory s which, at the NESS density fulﬁll the following conditional
independence relationships: p∗(x) =p∗(η|s,a)p∗(µ|s,a)p∗(s,a).
• We thus require all partitions to be at NESS, including the external states. This
means that the environment also has to be at steady state, not just the system.
• We often assume no solenoidal coupling between internal and sensory states
(internal states do not directly act on sensory states – only the external states do),
nor between active and external states (active states drive the external state but
are not driven by it). Mathematically this corresponds to Qs,µ, = 0,Qη,a = 0.
• We may even require that Q be block diagonal – thus allowing for no solenoidal
coupling between subsets of the Markov Blanket at all.
Given the Markov Blanket conditions hold, we can then begin to move towards the free
energy lemma. To begin with, we must ﬁrst assume,
• There is a unique argmax ηηη,µµµ exists for both internal and external states for every
blanket state b.
• That there exists a function σ which maps from µµµ to ηηη
• That σ is invertible
• That σ is differentiable
• For the particular free energy, we assume that the variational posterior q(η;µµµ) is
equal to the true posterior, and thus that the true posterior can be represented by a
vector of sufﬁcient statistics (µµµ).
These assumptions on σ are quite restrictive. A more detailed discussion of what
these assumptions require can be found in the next section of this chapter, where every
restriction is listed and discussed in some depth.
Chapter 2. The Free Energy Principle 60
Finally, to reach the free energy lemma, we must make the following assumptions,
• The ﬂow of the sufﬁcient statistic of external states ηηη follows the same (Ao-
decomposition) dynamics as the external states themselves
• The variational distribution q(η;µµµ) is a Laplace distribution (Gaussian) with a
ﬁxed covariance Σ as a function of µµµ.
This ﬁrst assumption has come under heavy controversy and is discussed in more detail
later. These additional assumptions pertain to the Laplace approximation, but the ﬁnal
assumption here appears to go beyond what is typically required by variational Laplace
where, since the conditional distribution is a function of the blanket, one would expect
the conditional covariance to be one too.
2.9.1 Assumptions on the Form of the Langevin Dynamics
The FEP formulation makes reasonably strong assumptions about the nature of the
dynamics that it models – restricting them to the form of stochastic dynamics which can
be written as a Langevin equation with additive Gaussian noise. While the assumptions
on the dynamics function are not that strong, only requiring differentiability and time-
independence, the restrictions on the noise in the system are quite severe.
Firstly, it is important to note that using additive white noise, while a common modelling
assumption due to its mathematical simplicity, nevertheless imposes some restrictions
on the kind of systems that can be modelled – especially as complex self organizing
systems typically evince some kind of colored smooth noise, as well as often power-law
noise distributions which are associated with self-organized criticality (Ovchinnikov,
2016).
However, the further assumptions on the Γ covariance matrix – that it is diagonal, state-
independent, and time-independent – are also strong additional restrictions. Speciﬁcally,
this means that the noise to every dimension in the system is completely independent of
Chapter 2. The Free Energy Principle 61
any other dimension, and that the noise is constant at every point throughout the state
space and throughout time.
2.9.1.1 Ergodicity and the Ao Decomposition
The Ao decomposition requires both that the dynamics possess a consistent non-
equilibrium steady state density (which forms the potential function) and also that
the dynamics are ergodic. Additionally, this ergodicity assumption is implicitly used
in the Bayesian mechanics, which allows expectations of the surprisal to be taken and
interpreted as entropies, and thus to ultimately derive an interpretation of the dynamics
in terms of accuracy and complexity. In general, for many biological and self-organizing
systems, ergodicity does not hold and such systems typically exhibit substantial amounts
of path dependence and irreversibility. This means that on a strict reading, for most
systems the FEP desires to model, the ergodicity assumption does not hold. However,
it may still be possible to describe ergodicity as holding locally in the small region of
the state space around the NESS density and this may be sufﬁcient for an approximate
version of the FEP to hold, although the resistance of the FEP to slight perturbations of
its assumptions remains unclear.
2.9.2 The Markov Blanket Condition
2.9.2.1 Is Information Retained Behind the Blanket? – The Time Synchronous
Markov Blanket Condition
A potentially substantial problem, which has been raised by Martin Biehl and Nathaniel
Virgo, for the FEP is that the Markov Blanket conditions would appear to very strongly
imply that the internal states cannot store any more information about the external states
than the blanket states. This fact can be derived from a straightforward application of the
data processing inequality. Translated into the terminology of biological systems like
brains, this would mean that the state of the brain could contain no more information
Chapter 2. The Free Energy Principle 62
about the environment than the state of the sensory epithelia and actuators at the current
time. In effect, this would rule out systems obeying the FEP from exhibiting any sort of
long term memory or learning – clearly a very undesirable side-effect.
In discussions within the community, there have been many attempts to ﬁnesse this
apparent difﬁculty with appeals to notion of nested temporal scales and the local
applicability of the FEP. The intuitive argument is that if the Markov Blanket conditions
rule out information storage on the macroscale where they apply locally, they may
nevertheless allow for the slow accumulation of information over a longer timescale.
Effectively, if we can imagine that there are two kinds of variables – ‘fast’ variables
which can change over the a given timescale and ‘slow’ variables which do not. Then,
if we can consider the slow variables ﬁxed over some timescale, then we can consider
the fast variables to reach a NESS density over that timescale, however over longer
timescales, the values of the fast variables can inﬂuence the slow variables leading to
them changing over time, and thus inducing a different NESS density over among the
fast variables. The change in the slow variables can be considered to be learning, and
could allow for the accumulation of information over time. This process of timescale
separation is directly analogous to the classical distinction between inference (fast)
and learning (slow) in machine learning and control theory, and can also be expressed
physically in terms of an adiabatic reduction (Friston, 2019a) which explicitly separates
out the dynamics of the system into fast and slow eigenmodes. This construction,
however, does require a notion of ‘approximate’ NESS for a timescale which is long
enough for the ‘fast’ variables but also short enough for the ‘slow’ variables to appear
ﬁxed.
2.9.2.2 The Real Constraints on Solenoidal Coupling?
While the Markov blanket conditions only explicitly disallow solenoidal coupling
directly between the internal and external states – Qµ,η = 0, the free energy lemma
appears to require a signiﬁcantly greater reduction of solenoidal coupling. Speciﬁcally,
Chapter 2. The Free Energy Principle 63
the free energy lemma requires that, for a straightforward identiﬁcation of the surprisal
with the free energy, that the form of the dynamics for each marginal subset of states in
the partition take the same form as the dynamics of the full set of states x. Speciﬁcally,
this means that all solenoidal coupling between the subsets must be suppressed, since if
they were not then, by the marginal ﬂow lemma, there would be additional solenoidal
coupling terms in Equation 2.29, which would complicate the relation to free energy
minimization with additional solenoidal terms. As such, for the free energy lemma, as
currently presented, we appear to have the extremely strong condition of the diagonality
of Q, where each subset in the Markov Blanket is only allowed solenoidal interactions
with itself.
It is important to note that this restriction is signiﬁcantly stronger than those required just
by the Markov Blanket condition, and indeed is stronger even than the ﬂow constraints
proposed in Friston, Da Costa, and Parr (2020). While this does not entirely rule out
any interactions between different subsets of the Markov blanket, it does mean that
all interactions have to be mediated through the gradient term, since both the Γ and Q
matrices are assumed to be diagonal. However, it may be that the additional solenoidal
terms in the free energy lemma as a result of non-diagonal Q are not that deleterious to
the theory since as these are purely solenoidal terms, they are orthogonal to the ﬂow
and do not affect the ultimate minima of the system.
2.9.3 Assumptions of the free energy Lemma
2.9.3.1 The σ function
The existence and general properties of the σ function have also recently elicited much
discussion and debate within the community. Speciﬁcally, it is not at all clear that this
function exists in the general case, for arbitrary dynamics functions f and conditional
NESS distributions p∗(η|b) and p∗(µ|b). In later papers it is assumed to exist under
the condition of injectivity between ηηη and µµµ. In effect, this means that there must be
Chapter 2. The Free Energy Principle 64
a unique mapping between ηηη and µµµ for all blanket states – i.e. that for every blanket
state, if the argmax of the internal states is µµµ, then the argmax of the external states
must be ηηη. Additionally, there must be a corresponding (and separate) external argmax
for every internal argmax. There may, however, be some external argmaxes with no
corresponding internal argmaxes (although the converse condition does not hold). This
requires that the dimensionality of the external states be greater than or equal to the
dimensionality of the internal states – which should generally hold for most reasonable
systems where we can safely assume that the environment is larger than the system
itself. This injectivity condition also guarantees invertibility in the case that the internal
and external state spaces are of the same dimension. It is also possible to use the
Moore-Penrose pseudoinverse for the case where the external state space is larger, at
the cost of the free energy lemma becoming approximate instead of exact.
The differentiability of the σ function is a more stringent condition. In many cases this
is unlikely to be met, since the argmax functions which theσ function maps between are
generally nondifferentiable. It remains unclear to what extent differentiable σ functions
can exist in systems of interest.
2.9.3.2 The ﬂow of the Sufﬁcient Statistics η
An additional important assumption necessary for the free energy lemma, is that the
ﬂow of the sufﬁcient statistics of the external mode follow the same ﬂow as the external
states generally. This assumption turns out to be crucial to the free energy lemma which
relies heavily in the fact that the ﬂow of the sufﬁcient statistic ηηη can be written as a
gradient descent on the log surprisal – which can then be expressed in terms of a free
energy under the Laplace approximation.
This assumption is also problematic and has been the source of much discussion within
the community. The extent to which this assumption is justiﬁed remains unclear.
Speciﬁcally, it appears to rule out the use of arbitrary functions ξ (to be discussed in the
Chapter 2. The Free Energy Principle 65
next section) to parametrize the external sufﬁcient statistic (although not the internal
sufﬁcient statistic). The assumption effectively holds to the extent to which one can
describe the sufﬁcient statistic as equal to some external state ηηη(b) ≈η, which may
occur often for the argmax but not necessarily always. It remains to be seen whether
the argmax is in fact the optimal such function – which is dependent on the blanket,
but which can identify a consistent η to identify with and thus partake in the same
dynamics.
2.9.3.3 Potential and Optimal ξ Functions
While the didactic treatment of the FEP in (Friston, 2019a; Parr, Da Costa, & Friston,
2020), it is assumed that the σ function relates the argmax of η and of µ, this is a simple
assumption and is not particularly required by the theory. It only requires that there
be some function not that it necessarily be an argmax. This means that we could, in
theory use an arbitrary function µµµ(b) =ξ(b) instead of the argmax. Indeed, we might
desire to make this function contain as much information as possible about the true
conditional distribution of the internal states given the external states, so that when
the σ function maps this to the sufﬁcient statistic of the external density it can be seen
as performing inference with the most information possible between the external and
internal states. An additional beneﬁt of deﬁning an arbitrary function for ξ instead of
using ξ(b) =argmax p(µ|b) is that we can make ξ differentiable, which alleviates much
of the difﬁculty of making σ differentiable as well.
While this approach brings many beneﬁts, it also has the drawback of the necessity
to choose a suitable function ξ which introduces another degree of freedom into the
modelling process. One possible condition is that we could chose the optimal ξ to
be the one that contains the most information about the internal state or, alternatively
minimizes the KL between the approximate conditional distribution over the internal
states parametrized vy ξ and the true conditional over the blanket states. That is, we
Chapter 2. The Free Energy Principle 66
could deﬁne,
ξ∗= argmin
ξ
DKL[q(µ;ξ(b))||p(µ|b)]
This would reduce the number of degrees of freedom ofξ and provide a valid modelling
target, although the actual computability of this minimization process is potentially a
problem, as is whether this objective is actually optimal. Nevertheless, the use of an
arbitrary ξ function for the sufﬁcient statistics of the internal states may yet resolve or
ameliorate some of the difﬁculties with the free energy lemma, and is an interesting
inroad to begin understanding various relaxations or extensions to the current incarnation
of the free energy principle.
2.10 Active Inference
In the previous section, we have covered the very general and abstract form of the
FEP, here we elucidate the central process theory that has emerged from the FEP
literature in theoretical neuroscience – Active Inference (Friston et al., 2009, 2012).
Active inference is a normative theory of perception, decision-making, and learning
which ties these three core cognitive processes together under the general paradigm
of variational inference via the minimization of the variational free energy (Friston,
FitzGerald, et al., 2017b; Friston, Rigoli, et al., 2015a). Speciﬁcally, it views all of these
processes as emerging out of a central imperative of the system to minimize its free
energy over time, and thus perform inference. Perception can be quite clearly stated as
an inference problem of inferring the hidden states and causes of the world from sensory
observations. Learning too can be interpreted as inference over the parameters of the
generative model, which takes place on a slower timescale than perceptual inference.
Finally, action selection, decision-making, and planning can be described as inference
on policies over trajectories into the future. While there are numerous methods to
perform this inference, active inference chooses to minimize an expected free energy
Chapter 2. The Free Energy Principle 67
functional which encodes goals in terms of a prior over future states (Da Costa, Parr, et
al., 2020).
Active inference has been applied widely and productively in theoretical neuroscience,
and active inference models have been proposed for planning and navigation (R. Kaplan
& Friston, 2018), saccadic eye movements (Parr & Friston, 2017b, 2018a, 2018b)
and visual foraging (Heins et al., 2020; Parr, 2019), and general planning problems
in reinforcement learning and machine learning (Millidge, 2020; Tschantz, Millidge,
Seth, & Buckley, 2020a; Tschantz, Millidge, et al., 2020b; Ueltzhöffer, 2018). Addi-
tionally, by simulating various lesions or incorrect update rules in the active inference
scheme, one may obtain interesting behavioural anomalies or shortfalls which one can
then analogize to known psychiatric disorders – an approach known as computational
psychiatry (Cullen et al., 2018; Parr, 2019). By drawing correspondences between
disorders of behaviour in tractable artiﬁcial systems and the psychiatric symptoms of
disorders in humans or other animals, one may be able to shed new light upon the
actual mechanistic underpinnings of such disorders, which may lead to novel hypothe-
ses, experimental protocols and, ultimately, treatments. Computational psychiatric
approaches using active inference have pioneered statistical, mechanistic models of
impulsivity (Mirza et al., 2019), visual neglect (Parr & Friston, 2018c), autism (Lawson
et al., 2014), schizophrenia (Adams et al., 2012) substance use disorder and addiction
(Schwartenbeck et al., 2015), and rumination (Hesp et al., 2020). Finally, the epistemic
imperatives that arise from the minimization of the expected free energy functional have
given rise to a number of simulation studies applying the approach to exploration tasks
(Friston, Lin, et al., 2017; Friston, Rigoli, et al., 2015a; Schwartenbeck et al., 2013),
visual foraging and other information-seeking saccade behaviour (Heins et al., 2020;
Parr & Friston, 2017a), and exploration in complex sparse-reward environments from
reinforcement learning (Tschantz, Millidge, et al., 2020b), which will be signiﬁcantly
expanded upon in later chapters of this thesis.
Chapter 2. The Free Energy Principle 68
Broadly, there are two main classes of active inference models in the literature –
continuous-time, continuous-state models, which are an extension of predictive coding
models of brain function (Baltieri & Buckley, 2017, 2019; Friston et al., 2009; Friston,
Daunizeau, et al., 2010; Millidge, 2019a; Pio-Lopez, Nizard, Friston, & Pezzulo, 2016),
and discrete-time discrete-state-space active inference models which have been heavily
developed in the literature in the past decade, and perhaps now form the main theory of
active inference applied to the brain (Da Costa, Parr, et al., 2020; Friston, FitzGerald, et
al., 2017b). All of these models, however, ultimately are derived from the same mathe-
matical apparatus. As such, an advantage of active inference for modelling behaviour
is that due to its developed and shared mathematical apparatus, different models are
speciﬁed simply through the generative model and variational distribution, and can be
directly compared through Bayesian model comparison techniques. This can be used to
ﬁt active inference models to empirical behavioural data in a straightforward fashion.
Continuous time, continuous state based models are explained in depth in Chapter 3,
where I discuss my work with these models in the context of predictive coding. Here,
we present an introduction to discrete-state-space active inference which shall form the
basis of my work in Chapters 4 and 5 of the thesis.
Here we introduce a more standard notation which shall be used for the rest of the
thesis. We consider our agent to be situated in a Partially Observed Markov Decision
Process (POMDP) (Kaelbling, Littman, & Moore, 1996; Sutton, 1990). The agent is
given observations o, and must infer the hidden states of the world x that gave rise
to the observations. The agent may additionally possess models with parameters θ,
which can also be optimized. Finally, action selection consists of inferring actions a, or
policies π = [a0,a1, . . .aN] which are simply sequences of actions in order to achieve
some desired goal. Active inference is based around the fundamental imperative of
minimizing the variational free energy (VFE). We may recall from Equation 2.12 that
the VFE consists of the KL divergence between a variational distribution q(x|o) and a
generative model p(o,x).
Chapter 2. The Free Energy Principle 69
F (o) =DKL[q(x|o)||p(o,x)] (2.35)
If we additionally want to infer the parameters θ, we can extend the generative model
and variational density to include a distribution over the parameters – this provides a
fully Bayesian treatment of parameters in contrast to many machine learning schemes
which treat them effectively as point distributions,
F (o) =DKL[q(x,θ|o)||p(o,x,θ)] (2.36)
In order to implement a speciﬁc active inference scheme, the key thing to specify is
the nature of the generative model, and the nature of the variational distribution. These
two distributions sufﬁce to completely specify the model, and with these distributions
set, the processes of learning, inference, and action selection can be handled by the
standard mathematical apparatus of the theory. Speciﬁcally, given a speciﬁc variational
distribution and a generative model, we can implement perception as a minimization of
the VFE with respect to the variational distribution with respect to the hidden states,
and we can implement learning as the minimization of the VFE with respect to the
parameters
Perception: argmin
q(x|o)
F (o)
Learning: argmin
q(θ|x,o)
F (o) (2.37)
There are then two separate ways to implement action. The most straightforward
approach, which is utilized in the continuous time version of active inference is to
similarly implement action as a gradient descent on the VFE with respect to action,
Action: argmin
a
F (o(a)) (2.38)
Where we have made the implicit dependence of the observations, and hence the VFE
on action explicit, which makes such a minimization non-trivial. A second approach,
Chapter 2. The Free Energy Principle 70
which is typically used in the discrete-state-space paradigm is to assume a speciﬁc
functional form for the variational posterior over policies – that of a softmax distribution
over the Expected Free Energy (EFE) G(o,x) (Friston, Rigoli, et al., 2015a),
Action (discrete-state-space): Q(π) =σ(−G(o,x)) (2.39)
where σ is a softmax function. Effectively, what this states is that the optimal policy is a
softmax distributions over the path-integrals of the EFE into the future. Effectively, the
optimal distribution over policies is simply one that selects policies in proportion with
the exponentiated EFE resulting from executing that policy in the future. This means
that the policy with the greatest EFE is most likely, while policies with lesser EFE are
exponentially less likely to be selected based on the difference between their EFE and
that of the best policy.
Discrete state-space active inference therefore optimizes two complementary objective
functions. Optimizing the variational free energy, which is used for perception and
learning, ensures that the agent learns an accurate world model, and is able to accurately
infer the hidden states of the world from current observations. The second objective,
the expected free energy, is used to score potential plans or action policies, to allow the
agent to make decisions which are adaptive relative to its goals. To successfully predict
and infer with trajectories in the future requires a highly developed and accurate world-
model, able to make accurate multi-step predictions of the consequences of action. Such
a world model is provided by the minimization of the VFE in the inference and learning
steps. This separation of inference and action selection into two separate objectives
– the VFE and the EFE – introduces a measure of complexity into the theory which
may or may not be unavoidable. In Chapter 5, we focus especially on this question
and investigate the nature of the EFE, and whether all facets of inference, learning, and
action selection can be subsumed under a single uniﬁed objective.
Chapter 2. The Free Energy Principle 71
2.10.1 Discrete State-space models and Perception
The core component of the discrete-state-space model is the discrete generative models
and variational densities it is based upon. Speciﬁcally, we split the generative model
into a likelihood and prior distribution p(o,x) =p(o|x)p(x), and then represent each of
these distributions as a categorical distribution,
p(o,x) =p(o|x)p(x)
p(o|x) =Cat(o; ˆo) =AAA
p(x) =Cat(x; ˆx) =BBB (2.40)
A categorical distribution is one that simply directly assigns some probability value to
every possible discrete contingency. The parameters ˆo and ˆx of these distributions are
simply these probability values, which can be represented straightforwardly in terms
of matrices. AAA ∈R O ×R X is simply a normalized matrix of probabilities representing
the likelihood contingencies – that is, for every hidden state x, what is the probability
of each potential outcome. Similarly, the transition matrix BBB ∈R X ×R X is a matrix
of probabilities representing the probability of transitioning from any one hidden state
to any other hidden state. Similarly, we deﬁne our variational distribution q(x|o =
oµ; ˆxq) ∈R X to be a categorical distribution of the probability of each discrete hidden
state as a function of the observed observation oµ. With our variational and generative
model set, we can explicitly write out and evaluate the VFE,
F = DKL[q(x|o)||p(o,x)]
= Eq(x|o; ˆxq)[lnq(x|o; ˆxq)]−Eq(x|o; ˆxq)[ln p(o|x; ˆo)]−Eq(x|o; ˆxq)[ln p(x; ˆx)]
= ˆxq ln ˆxq −ˆxq lnAAA −ˆxq lnBBB (2.41)
Where we have simply explicitly written out the variational free energy in terms of
the parameters of the categorical distributions. The expectation operator E[] can be
computed as a simple dot product instead of an integral due to the discrete state space.
Importantly, because both our variational density and generative models are categorical
Chapter 2. The Free Energy Principle 72
distributions, we can derive an analytical expression for the minimum ofF with respect
to the variational parameters ˆxq, allowing for an exact Bayes-optimal single-step update
for perception,
∂F
∂ˆxq
= ∂
∂ˆxq
[ˆxq ln ˆxq −ˆxq lnAAA −ˆxq lnBBB]
= ln ˆxq +111 −lnAAA −lnBBB
∂F
∂ˆxq
= 0 =⇒ ˆx∗
q = σ(−lnAAA −lnBBB) (2.42)
Learning can be approached similarly, by placing suitable hyperpriors (typically dirichlet
(Schwartenbeck et al., 2019)) upon the parameters of the AAA and BBB matrices and then
minimizing the VFE with respect to these parameters. For more information on how
learning is implemented see Da Costa, Parr, et al. (2020); Friston, FitzGerald, et al.
(2017b).
2.10.2 Action Selection and the Expected Free Energy
Action selection is then handled via the variational posterior being equal to the soft-
maxed path integral of the EFE through time (Equation 2.39). Typically, in small
discrete state spaces, this path integral can be computed exactly, by simply comput-
ing the EFE for every single policy and every single possible trajectory through the
state-space. Unfortunately, this approach scales exponentially in the time horizon, and
is thus not suitable for long, open-ended tasks, although it remains a highly effective
method for simulating short tasks, such as single trials in a psychophysical, or simple
decision-making, paradigm (Friston, Da Costa, Hafner, et al., 2020; Friston, Rigoli,
et al., 2015b; Schwartenbeck et al., 2015). Various methods have been proposed to
handle this exponential complexity. One commonly proposed method is to simply prune
potential trajectories which become too unlikely (i.e. have too low an EFE) to be worth
considering further. Typically, such methods, however, do not reduce the algorithm
to a smaller (polynomial) complexity class, but instead simply reduce the exponential
Chapter 2. The Free Energy Principle 73
coefﬁcient which allows the method to scale to slightly larger tasks but does not remove
the fundamental exponential complexity of the algorithm. Other approaches involve
approximating the path integral with either bootstrapping value-function methods (Mil-
lidge, 2020), which take advantage of the recursive temporal decomposition of the EFE,
or alternatively Monte-Carlo techniques which approximate the EFE through a random
sampling of trajectories, which corresponds to classical model-predictive control algo-
rithms (Kappen, Gómez, & Opper, 2012). An additional consideration in the EFE is the
need to specify a desired or goal state for the action selection mechanism to achieve.
This can be considered to be a probabilistic description of rewards in reinforcement
learning and psychology, or of utility in economics. Mathematically, this speciﬁcation
is achieved by deﬁning a biased generative model ˜p(o,x) which contains a desired
distribution ˜p which encodes the rewards or utility as effectively priors in the inference
procedure.
Since action selection is dependent entirely on the path-integral of the EFE, the prop-
erties of the EFE functional essentially determines the kind of behaviour that ﬁnding
trajectories that minimize the EFE will induce. Here, we showcase two decompositions
of the EFE, and discuss its intrinsic exploratory drive.
G(o,x) =Eq(o,x)[lnq(x)−ln ˜p(o,x)]
= Eq(o,x)[ln p(o|x)]  
Ambiguity
+DKL[q(x)||˜p(x)]  
Risk
= Eq(o,x)[ln ˜p(o)]  
Extrinsic Value
−Eq(o)DKL[q(x|o)||q(x)]  
Information Gain
+Eq(o)DKL[q(x|o)||p(x|o)]  
Posterior Divergence
(2.43)
The ﬁrst decomposition into risk and ambiguity obtains when the goal distribution is
speciﬁed in terms of the hidden states of the world ˜p(x). In this case, EFE minimization
can be thought of as directly trying to match the expected states of the world to the
desired states, and thus achieving one’s goals, while simultaneously trying to minimize
Chapter 2. The Free Energy Principle 74
the ambiguity of the observations one receives. The second decomposition intoextrinsic
and intrinsic value (information gain) occurs when the goal distribution is speciﬁed
in terms of the observations ˜p(o). The extrinsic value term can be thought of as the
expected reward or expected utility, since it is the average amount of reward expected
under the predicted observation distribution. Most interestingly in this decomposition,
however, is the information gain term which encourages the agent to maximize the
divergence between the variational prior and the variational posterior. Effectively
this term encourages the agent to seek out information in the environment which will
maximally update its beliefs about the world. In effect, this term encourages a speciﬁc
kind of information-seeking exploration, where agents that minimize the EFE are
effectively driven to seek out and integrate resolvable uncertainty about the world into
their world model.
This intriguing property of active inference agents which minimize the EFE has been
extensively investigated in the literature – from simple tasks such as the T-maze which
require deciding whether to gain information by seeking out a cue or not (Friston, Rigoli,
et al., 2015a), to planning visual saccades in a way which maximizes the information
about the scene gained (Heins et al., 2020; Parr & Friston, 2017b) and to using directed
exploration to develop highly sample efﬁcient and powerful general reinforcement
learning algorithms for sparse-reward environments (Millidge, 2019b). Moreover, the
fact that the EFE naturally gives rise to an information-seeking exploration term offers
a promising and fascinating avenue for resolving the exploration-exploitation tradeoff
and deriving optimal exploration strategies directly from variational Bayesian inference
algorithms.
2.11 Discussion
In this chapter, we have already covered a substantial amount of ground. We have
reviewed the core tenets of the free energy principle, provided a mathematically detailed
Chapter 2. The Free Energy Principle 75
walk-through of the core results, and discussed their philosophical implications and
meaning. We have additionally provided a short review of a core process theory –
discrete state space active inference – which we will fundamentally build upon in
various ways in the rest of this thesis. Chapter 3 will focus on applying the free
energy principle to perception – and will focus on the implementation and extension
of the process theory of predictive coding. Chapter 4 will focus on merging active
inference as presented here with modern deep reinforcement learning methods to allow
active inference approaches, which are currently bottlenecked due to their discrete
explicit tabular representations and the exponential complexity of the action selection
algorithm, to be extended to challenging machine learning problems. Chapter 5 will
focus especially on the Expected Free Energy term and will seek to unravel the origin
of its information seeking properties and in the process will reveal deep connections
between active inference and other variational Bayesian approaches to action such as
control as inference, as well as revealing a substantially richer landscape of potential
variational functionals for control than has been previously realized.
Chapter 3
Predictive Coding
3.1 Introduction
In this chapter, we consider the application of the free energy principle to perception.
Here, we focus entirely on visual perception and the process theory of predictive
coding (Bastos et al., 2012; Buckley et al., 2017; Friston, 2003, 2005; Spratling, 2017).
This chapter is organized into four relatively independent sections which each present
a separate piece of work, which extends or contributes to the theory or practice of
predictive coding. The general theme of our work aims to scale up predictive coding to
reach the levels of performance achieved by machine learning, as well as to understand
the potential biological plausibility of the theory. Both of these are important for
understanding the potential predictive coding has as a general theory of cortical function
since, if it is actually implemented in the brain, it must meet both bars of extremely high
scalability (since the brain effortlessly handles perception and inference with extremely
detailed and complex inputs, as well as constructing extremely powerful and general
representations), as well as the biological plausibility necessary to allow the dynamics
prescribed by predictive coding to be implemented by neural circuitry.
We begin with a mathematical introduction to predictive coding and its dynamics. This
76
Chapter 3. Predictive Coding 77
is followed with the presentation of our work where we experiment with implementing
larger scale predictive coding networks than previously in the literature, and validate
their performance and capabilities on benchmark machine learning datasets – thus
demonstrating that predictive coding as a theory can be scaled up to the standards of
modern deep learning. We also experiment with dynamical predictive coding networks
using generalized coordinates (as introduced (Friston et al., 2008)) as well as combining
both hierarchical and dynamical predictive coding networks, although these implemen-
tations are only tested on relatively small toy tasks and scaling these up to larger and
much more challenging dynamical tasks, such as video prediction, remains an important
avenue for future work.
In the second section, we focus on understanding how predictive coding can be written as
a ﬁltering algorithm – and thus can be applied productively to fundamentally dynamical
instead of static stimuli. We demonstrate precisely how predictive coding is related to
Kalman ﬁltering – a ubiquitous and extremely successful Bayesian ﬁltering algorithm
(Kalman & Bucy, 1961; Kalman et al., 1960) – and also show how predictive coding can
extend this algorithm to allow for the online learning of the parameters of the generative
model (as well as inference of the states) – a capability which is not usually achieved
with Kalman ﬁltering alone. We validate the performance of this algorithm on simple
ﬁltering tasks.
Fourthly, we investigate the biological plausibility of predictive coding, show how the
standard model possesses three key implausibilities – weight transport, nonlinear deriva-
tives, and one-to-one error unit connectivity, and show how each can be overcome with
biologically plausible additions to the algorithm without causing much of a degradation
in the classiﬁcation performance of the algorithm.
Chapter 3. Predictive Coding 78
3.2 Predictive Coding
Predictive coding is an inﬂuential theory in computational and cognitive neuroscience,
which proposes a potential unifying theory of cortical function (Clark, 2013a; Friston,
2003, 2005, 2010; Rao & Ballard, 1999; Seth, 2014) – namely that the core function
of the brain is simply to minimize prediction error, where the prediction errors denote
mismatches between predicted input and the input actually received. This minimization
can be achieved in multiple ways: through immediate inference about the hidden states
of the world, which can explain perception (Beal, 2003), through updating a global
world-model to make better predictions, which could explain learning (Friston, 2003;
Neal & Hinton, 1998), and ﬁnally through action to sample sensory data from the world
that conforms to the predictions (Friston et al., 2009), which potentially provides an
account of adaptive behaviour and control. Prediction error minimization can also be
inﬂuenced by modulating the precision (or inverse variance) of sensory signals, which
may shed light on the neural implementation of attention mechanisms (Feldman &
Friston, 2010; Kanai, Komura, Shipp, & Friston, 2015). Predictive coding boasts an
extremely developed and principled mathematical framework, which formulates it as a
variational inference algorithm (Blei et al., 2017; Ghahramani et al., 2000; M. Jordan et
al., 1998), alongside many empirically tested computational models with close links
to machine learning (Beal, 2003; Dayan et al., 1995; Hinton & Zemel, 1994), which
address how predictive coding can be used to solve challenging perceptual inference and
learning tasks similar to those faced by the brain. Moreover, predictive coding also has
been translated into neurobiologically plausible microcircuit process theories (Bastos
et al., 2012; Shipp, 2016; Shipp, Adams, & Friston, 2013) which are increasingly
supported by neurobiological evidence (Walsh, McGovern, Clark, & O’Connell, 2020).
Predictive coding as a theory is also supported by a large amount of empirical evidence
and offers a single mechanism that accounts for diverse perceptual and neurobiological
phenomena such as end-stopping (Rao & Ballard, 1999), bistable perception (Hohwy
et al., 2008; Weilnhammer, Stuke, Hesselmann, Sterzer, & Schmack, 2017), repetition
Chapter 3. Predictive Coding 79
suppression (Auksztulewicz & Friston, 2016), illusory motions (Lotter, Kreiman, &
Cox, 2016; Watanabe, Kitaoka, Sakamoto, Yasugi, & Tanaka, 2018), and attentional
modulation of neural activity (Feldman & Friston, 2010; Kanai et al., 2015). As such,
and perhaps uniquely among neuroscientiﬁc theories, predictive coding encompasses
all three layers of Marr’s hierarchy by providing a well-characterised and empirically
supported view of ‘what the brain is doing’ at the computational, algorithmic, and
implementational level (Marr, 1982).
The core intuition behind predictive coding is that the brain is composed of a hierarchy of
layers, which each make predictions about the activity of the layers below (Clark, 2015;
Friston, 2008a). These descending downward predictions at each level are compared
with the activity and inputs of each layer to form prediction errors – which is the
information in each layer which could not be successfully predicted. These prediction
errors are then fed upwards to serve as inputs to higher levels, which can can then be
utilized to reduce their own prediction error. The idea is that, over time, the hierarchy
of layers instantiates a range of predictions at multiple scales, from the ﬁne details in
local variations of sensory data at low levels, to global invariant properties of the causes
of sensory data (e.g., objects, scenes) at higher or deeper levels. 1. Predictive coding
theory claims that the goal of the brain as a whole, in some sense, is to minimize these
prediction errors, and in the process of doing so performs both perceptual inference
and learning. Both of these processes can be operationalized via the minimization
of prediction error, ﬁrst through the optimization of neuronal ﬁring rates on a fast
timescale, and then the optimization of synaptic weights on a slow timescale (Friston,
2008a). Predictive coding proposes that using a simple unsupervised loss function, such
as simply attempting to predict incoming sensory data, is sufﬁcient to develop complex,
general, and hierarchically rich representations of the world in the brain, an argument
1This pattern is widely seen in the brain (Grill-Spector & Malach, 2004; Hubel & Wiesel, 1962) and
also in deep (convolutional) neural networks (Olah, Mordvintsev, & Schubert, 2017), but it is unclear
whether this pattern also holds for deep predictive coding networks, primarily due to the relatively few
instances of deep convolutional predictive coding networks in the literature so far.
Chapter 3. Predictive Coding 80
which has found recent support in the impressive successes of modern machine learning
models trained on unsupervised predictive or autoregressive objectives (Brown et al.,
2020; J. Kaplan et al., 2020; Radford et al., 2019). Moreover, the fact that, in these
machine learning models, errors are computed at every layer means that each layer
only has to focus on minimizing local errors rather than a global loss. This property
potentially enables predictive coding to learn in a biologically plausible way using only
local and Hebbian learning rules (Friston, 2003; Millidge, Tschantz, & Buckley, 2020a;
Whittington & Bogacz, 2017).
While originating from many varied intellectual currents, including the speculations
of Helmholtz (Helmholtz, 1866), ideas in information theory (Shannon, 1948) and
Barlow’s minimum redundancy principle (Barlow et al., 1961), as well as ideas from
cybernetics (Seth, 2014; Wiener, 2019) and early work on machine learning (Hinton &
Zemel, 1994; M. Jordan et al., 1998), modern predictive coding can be best described
as a variational inference algorithm (Beal, 2003) on the hidden causes of sensory
sensations, under Gaussian and Laplace assumptions. Variational inference is a method
of approximate Bayesian inference, arising in statistical physics (Feynman, 1998),
which turns an intractable inference problem into a potentially tractable optimization
problem. In brief, we postulate a variational density q, under the control of the modeller,
and try to minimize the divergence between this variational density and the true posterior.
Since this divergence is not tractable either (since it contains the true posterior), instead
we optimize a tractable bound on this divergence known as the variational free energy
F which measures the expected difference between the logs of the variational posterior
and the generative model. To make this concrete, suppose we have observations (or
data) o; we wish to infer hidden, or latent, states of the world x, with a generative model
p(o,x) and a variational density q(x|o;φ) with parameters φ. Then, we can write the
Chapter 3. Predictive Coding 81
variational free energy F as,
DKL[q(x|o;φ)||p(x|o)] =DKL[q(x|o;φ)||p(o,x)
p(o) ]
= DKL[q(x|o;φ)||p(o,x)]+ Eq(x|o;φ)[ln p(o)]
= DKL[q(x|o;φ)||p(o,x)]+ ln p(o)
≤DKL[q(x|o;φ)||p(o,x)] =F (3.1)
To derive a speciﬁc variational inference algorithm – such as predictive coding – we
must explicitly specify the forms of the variational posterior and the generative model.
In the case of predictive coding, we assume a Gaussian form for the generative model
p(o,x;θ) =p(o|x;θ)p(x;θ) =N (o; f (x;θ1),Σ2)N (x;g(¯µ;θ2),Σ1) where we ﬁrst par-
tition the generative model into likelihood p(o|x;θ) and prior p(x;θ) terms. The mean
of the likelihood Gaussian distribution is assumed to be some function f of the hidden
states x, which can be parametrized with parameters θ, while the mean of the prior
Gaussian distribution is set to some arbitrary function g of the prior mean ¯µ. We
also assume that the variational posterior is a dirac-delta (or point mass) distribution
q(x|o;φ) =δ(x −µ) with a center φ = µ2.
Given these deﬁnitions of the variational posterior and the generative model, we can
write down the concrete form of the variational free energy to be optimized. We ﬁrst
decompose the variational free energy into an ‘Energy’ and an ‘Entropy’ term
F = DKL[q(x|o;φ)||p(o,x;θ)]
= Eq(x|o;φ)[lnq(x|o;φ)]  
Entropy
−Eq(x|o;φ)[ln p(o,x;θ)]  
Energy
(3.2)
where, since the entropy of the dirac-delta distribution is 0 (it is a point mass distribu-
2In previous works, predictive coding has typically been derived by assuming a Gaussian variational
posterior under the Laplace approximation. This approximation effectively allows you to ignore the
variance of the Gaussian and concentrate only on the mean. This procedure is effectively identical to
the dirac-delta deﬁnition made here, and results in the same update scheme. However, the derivation
using the Laplace approximation is much more involved so, for simplicity, here we use the Dirac delta
deﬁnition. See Appendix C, or Buckley et al. (2017) for a detailed walkthrough of the Laplace derivation
Chapter 3. Predictive Coding 82
tion), we can ignore the entropy term and focus solely on writing out the energy.
Eq(x|o;φ)[ln p(o,x;θ)]  
Energy
= Eδ(x−µ)[ln
(
N (o; f (θ1x),Σ1)N (x;g(θ2 ¯µ),Σ2)
)
]
= lnN (o; f (θ1µ),Σ2)+ lnN (µ;g(θ2 ¯µ),Σ1)
= (o −f (θ1µ)2
Σ2
−ln2πΣ2 + (µ −g(θ2 ¯µ))2
Σ1
−ln2πΣ1
= Σ−1
2 ε2
o +Σ−1
1 ε2
x −ln4πΣ1Σ2 (3.3)
where we deﬁne the prediction errors εo = o −f (θ1µ) and εx = µ −g(θ2 ¯µ). We thus
see that the energy term, and thus the variational free energy, is simply the sum of
two squared prediction error terms, weighted by their inverse variances, plus some
additional log variance terms.
Finally, to derive the predictive coding update rules, we must make one additional
assumption – that the variational free energy is optimized using the method of gradient
descent such that,
dµ
dt = −∂F
∂µ (3.4)
Given this, we can derive dynamics for all variables of interest ( µ,θ1,θ2) by taking
derivatives of the variational free energyF . The update rules are as follows
dµ
dt = −∂F
∂µ = Σ−1
2 εo
∂f
∂µ θT −Σ−1
1 εx
dθ1
dt = ∂F
∂θ1
= −Σ−1
2 εo
∂f
∂θ1
µT
dθ2
dt = ∂F
∂θ2
= −Σ−1
1 εx
∂g
∂θ2
¯µT (3.5)
Furthermore while it is possible to run the dynamics for theµ and the θ simultaneously, it
is often better to treat predictive coding as an EM algorithm (Dempster, Laird, & Rubin,
1977) and alternate the updates. Empirically, it is typically best to run the optimization
of the µs, with ﬁxed θ until close to convergence, and then run the dynamics on the θ
with ﬁxed µ for a short while (Friston, 2005). This implicitly enforces a separation of
Chapter 3. Predictive Coding 83
timescales upon the model where the µ are seen as dynamical variables which change
quickly while the θ are slowly-changing parameters. For instance, the µs are typically
interpreted as rapidly changing neural ﬁring rates, while the θs are the slowly changing
synaptic weight values (Friston, 2005; Rao & Ballard, 1999).
Finally, we can see how this derivation of predictive coding maps onto putative psycho-
logical processes of perception and learning. The updates of the µ can be interpreted as
a process of perception, since the µ is meant to correspond to the true latent state of the
environment generating the o observations. By contrast, the dynamics of the θ can be
thought of as corresponding to learning, since these θ effectively deﬁne the mapping
between the latent state µ and the observations o.
3.3 Hierarchical predictive coding
Thus far, we have only derived a predictive coding scheme with a single level of latent
variables µ1. However, the expressivity of such a scheme is limited. The success of
deep neural networks in machine learning have demonstrated that having hierarchical
sets of latent variables is key to allowing methods to learn abstractions and to handle
intrinsically hierarchical dependencies of the sort humans intuitively perceive (Hinton,
Srivastava, & Swersky, 2012; Krizhevsky, Sutskever, & Hinton, 2012). Predictive
coding can be straightforwardly extended to handle hierarchical dynamics of arbitrary
depth. This is done through postulating multiple layers of latent variables x1. . .xL and
then deﬁning the generative model as follows,
p(x0 . . .xL) =p(xL)
L−1
∏
l=0
p(xl|xl+1) (3.6)
where p(xl|xl+1) =N (xl; fl(θl+1xl+1,Σl) and the ﬁnal layer p(xL) =N (xL|¯xL,ΣL) has
an arbitrary prior ¯xL and the latent variable at the bottom of the hierarchy is set to
the observation actually received x0 = o. Similarly, we deﬁne a separate variational
posterior for each layer q(x1:L|o) =∏L
l=1 δ(xl −µl), then the variational free energy can
Chapter 3. Predictive Coding 84
be written as a sum of the prediction errors at each layer,
F =
L
∑
l=1
Σ−1
l ε2
l +ln2πΣl (3.7)
where εl = µl −fl(θl+1µl+1). Given that the free energy divides nicely into the sum of
layer-wise prediction errors, it comes as no surprise that the dynamics of the µ and the
θ are similarly separable across layers.
dµl
dt = −∂F
∂µl
= Σ−1
l−1εl−1
∂fl−1
∂µl
θT
l −Σ−1
l εl (3.8)
dθl
dt = −∂F
∂θl
= Σ−1
l εl−1
∂fl−1
∂θl
µl (3.9)
We see that the dynamics for the variational means µ depend only on the prediction
errors at their layer and the prediction errors on the level below. Intuitively, we can
think of the µs as trying to ﬁnd a compromise between causing error by deviating from
the prediction from the layer above, and adjusting their own prediction to resolve error
at the layer below. In a neurally-implemented hierarchical predictive coding network,
prediction errors would be the only information transmitted ‘upwards’ from sensory
data towards latent representations, while predictions would be transmitted ‘downwards’.
Crucially for conceptual readings of predictive coding, this means that sensory data is
not transmitted directly up through the hierarchy, as is assumed in much of perceptual
neuroscience. The dynamics for the µs are also fairly biologically plausible as they
are effectively just the sum of the precision-weighted prediction errors from the µs
own layer and the layer below, the prediction errors from below being transmitted
back upwards through the synaptic weights θT and weighted with the gradient of the
activation function fl.
Importantly, the dynamics for the synaptic weights is entirely local, needing only the
prediction error from the layer below and the current µ at the given layer. The dynamics
Chapter 3. Predictive Coding 85
thus becomes a Hebbian rule between the presynapticεl−1 and postsynaptic µl, weighted
by the gradient of the activation function.
Based upon our previous work (Millidge, 2019c), we present empirical evaluations and
demonstrations of the expressive power of hierarchical predictive coding networks on
standard machine learning benchmarks with learnable generative models. While some
previous work has implemented hierarchical predictive coding models and tested them
on ‘blind deconvolution’ of simulated ERP data (Friston, 2005, 2008a), we present a
key demonstration of predictive coding networks within a machine learning paradigm,
and with a completely learnt generative model.
First, we tested the potential of predictive coding networks as autoencoders (Hinton
& Zemel, 1994) from machine learning. Here, the goal of the network is simply to
reconstruct its input data. In theory, this can be done trivially be learning the identity
mapping, so to make it difﬁcult we create an information bottleneck (Tishby, Pereira, &
Bialek, 2000) in the hidden layers, such that the input is compressed to a much smaller
latent code, which must then be decompressed to successfully reconstruct the image.
Autoencoders of this type are widely used in machine learning and a probabilistic
variant – variational autoencoders (Kingma & Welling, 2013) are still state of the art
at many image generation tasks (Child, 2020). Here we demonstrate that predictive
coding networks can also function as powerful autoencoders. We ﬁrst test the potential
of predictive coding on the MNIST dataset – a standard machine learning benchmark
dataset of 60,000 28x28 grayscale handwritten digits. We utilized a three layer predictive
coding network, with an input and output dimensionality of 784 (the size of a ﬂattened
vector of the MNIST digit), and a latent dimensionality of 20, meaning that the network
had to learn to compress a 784 dimensional manifold into a 20 dimensional latent space.
We trained the predictive coding network according to Equations 3.5 with a batch size
of 64 and a learning rate of 0.01. A sigmoid nonlinearity was used on the input and
latent layers. We trained the networks for 100 epochs. Each epoch consisted of updating
Chapter 3. Predictive Coding 86
the µs using Equation 3.8 for 100 steps, and then updating the weights θ using Equation
3.9 once. The model was able to recreate MNIST digits successfully, as shown in the
example reconstructions below:
Figure 3.1: MNIST digits in the training set recreated by the network. Top row the actual
digits, bottom row, the predictive reconstructions.
Additionally, the model was also able to recognize and reconstruct previously unseen
MNIST digits, albeit with slightly lower ﬁdelity. Nevertheless it is impressive how
rapidly and well the network is able to generalize to completely unseen digits.
Chapter 3. Predictive Coding 87
Figure 3.2: Unseen MNIST digits in the test set recreated by the network. Top row the
actual digits, bottom row, the predictive reconstructions.
Since the model is a generative model, it is also able to generalize outside the training
set to generate, or ‘dream’, completely unseen digits by sampling from the latent space.
Examples are shown below:
Figure 3.3: Images of hallucinated digits "dreamt" by the network. These were generated
by sampling the latent space around the representations of some exemplar digits in the
latent space, and then letting the predictive coding network generate its prediction from
the chosen latent state.
Chapter 3. Predictive Coding 88
Figure 3.4: A PCA clustering plot of the values of test MNIST digits in the latent space.
Even though the 20 dimensional latent space has been reduced down to two, clusters
are still visible. For instance, all the 1s are clustered in the top left corner. We thus
see that predictive coding appears to be a powerful and fully unsupervised learning
algorithm, capable of separating out distinct digits in the latent space, despite not being
trained with any label information at all – and purely on reconstruction.
Finally, to visualize the latent space, PCA (principal components analysis) was applied
to the learned representations in the 20 dimensional latent space to shrink it down to
a two dimensional space. It is apparent upon inspection of Figure 3.4 that the latent
space, even when shrunk down to two dimensions, does a good job of clustering the
MNIST, putting all the 1s in the top left corner, or all the zeros in the middle right. This
strongly indicates that the predictive coding model is able to learn the categories of
digits despite being trained in an entirely unsupervised way without any knowledge of
the true identities of the digits.
We additionally tested the network’s capability to reconstruct CIFAR images, which are
32x32 colour images of natural scnes. Our network was the same in the MNIST case
except that we used a latent dimension of 50.
The hierarchical predictive coding CIFAR models are able to learn to reconstruct
CIFAR images with impressive ﬁdelity given that they were compressed from a 1024
Chapter 3. Predictive Coding 89
dimensional image into a 50 dimensional latent state.
Figure 3.5: Test set CIFAR digits reconstructed by the network. The ﬁrst of the two lines
is the image and the second is the reconstruction. The network is extremely good at
reconstructing CIFAR images.
Importantly, due to having learnt a latent space, we are able to interpolate between
images, and thus investigate the properties of the learnt latent space. To interpolate,
we begin with two images o1 and o2 and take the difference vector in the latent space
ε = f (o2)−f (o1) where f is the encoder function. Then, we step in the latent space
Chapter 3. Predictive Coding 90
by a constant α and decode such that ˆo = g( f (o2) +αε) where g is the decoding
function. We stepped in increments of α = 0.1. Below we can see the network smoothly
interpolating between an image of a horse and a cat. We see that the predictive coding
network appears to naturally learn a smooth latent space, allowing for generalization to
unseen images and smooth interpolation between classes in the latent space.
Figure 3.6: The CIFAR model interpolating between a horse and a cat. Read the images
left to right top to bottom - like text. The interpolation is done by stepping in the latent
space from the representation of the ﬁrst image in the direction of the second until it is
reached.
3.3.1 Dynamical Predictive coding
While so far, we have only considered modelling just a single static stimuluso. However,
the data the brain receives comes in temporal sequences ¯o = [o1,o2 . . .]. To model such
temporal sequences, it is often useful to split the latent variables into states, which can
Chapter 3. Predictive Coding 91
vary with time, and parameters which cannot. In the case of sequences, instead of
minimizing the variational free energy, we must instead minimize the free action ¯F ,
which is simply the path integral of the variational free energy through time:
µ∗= argmin
µ
¯F
¯F =
∫
dtFt
Ft = DKL [q(xt|ot;φ)||p(ot,xt|xt−1)] (3.10)
While there are numerous methods to handle sequence data, one inﬂuential and elegant
approach (Friston, 2008a; Friston, Stephan, Li, & Daunizeau, 2010; Friston et al., 2008)
is to represent temporal data in terms of generalized coordinates of motion . These
coordinates represent not just the immediate observation state, but all the temporal
derivatives of the observation. For instance, suppose that the brain represents beliefs
about the position of an object. Under a generalized coordinate model, it would also
represent beliefs about the velocity (ﬁrst time derivative), acceleration (second time
derivative), jerk (third time derivative) and so on. All these time derivative beliefs
are concatenated to form a generalized state. The key insight into this dynamical
formulation is, that when written in such a way, many of the mathematical difﬁculties in
handling sequences disappear, leaving relatively straightforward and simple variational
ﬁltering algorithms which natively handle smoothly changing sequences. For instance,
we maintain a coherent concept of a stationary state, since we can deﬁne it as one in
which none of the time derivatives are changing. This allows the variational inference
procedure to track a moving target by representing it as a steady state in a moving frame
of reference.
Because the generalised coordinates are notationally awkward, we will be very explicit
in the following. We denote the time derivatives of the generalized coordinate using
a ′, so µ′ is the belief about the velocity of the µ, just as µ is the belief about the
‘position’ about theµ. A key point of confusion is that there is also a ‘real’ velocity
Chapter 3. Predictive Coding 92
of µ, which we denote ˙µ, which represents how the belief in µ actually changes over
time. Importantly, this is not necessarily the same as the belief in the velocity: ˙µ ̸= µ′,
except at the equilibrium state, which can be understood as the path of least action.
Intuitively, this makes sense as at equilibrium (mimimum of the free action, and thus
perfect inference), our belief about the velocity of muµ′and the ‘real’ velocity perfectly
match. Away from equilibrium, our inference is not perfect so they do not necessarily
match. We denote the generalized coordinate representation of a state ˜µ as simply a
vector of each of the beliefs about the time derivatives ˜µ = [µ,µ′,µ′′,µ′′′. . .]. We also
deﬁne the operator D which maps each element of the generalised coordinate to its
time derivative i.e. Dµ = µ′,D˜µ = [µ′,µ′′,µ′′′,µ′′′′. . .]. With this notation, we can deﬁne
a dynamical generative model using generalized coordinates. Crucially, we assume
that the noise ω in the generative model is not white noise, but is coloured, so it has
non-zero autocorrelation and can be differentiated. Effectively, coloured noise allows
one to model relatively slowly (not inﬁnitely fast) exogenous forces on the system.
For more information on coloured noise vs white noise see (Friston et al., 2008; Yuan
& Ao, 2012). With this assumption we can obtain a generative model in generalized
coordinates of motion by simply differentiating the original model.
o = f (x)+ ωo x = g(¯x)+ ωx
o′= f ′(x)x′+ω′
o x′= g′(¯x)x′+ω′
x
o′′= f ′(x)x′′+ω′′
o x′′= g′(¯x)x′′+ω′′
x
. . . . . . (3.11)
Where we have applied a local linearisation assumption (Friston et al., 2008) which
drops the cross terms in the derivatives. We can write these generative models more
compactly in generalized coordinates.
˜o = ˜f (˜x)+ ˜ωo ˜x = ˜g(˜¯x)+ ˜ωx (3.12)
which, written probabilistically is p( ˜o, ˜x) =p( ˜o|˜x)p(˜x). It has been shown (Friston et
al., 2008) that the optimal (equilibrium) solution to this free action is the following
Chapter 3. Predictive Coding 93
stochastic differential equation,
˙˜µ = D˜µ + ∂Eq(˜x|˜o;˜µ)[ln p( ˜o, ˜x)]
∂˜µ + ˜ω (3.13)
Where ˜ω is the generalized noise at all orders of motion. Intuitively, this is because
when
∂Eq(x|o;µ)[ln p( ˜o, ˜x)]
∂µ = 0 then ˙˜µ = D˜µ, or that the ‘real’ change in the variable is
precisely equal to the expected change. This equilibrium is a dynamical equilibrium
which moves over time, but precisely in line with the beliefs µ′. This allows the
system to track a dynamically moving optimal solution precisely, and the generalized
coordinates let us capture this motion while retaining the static analytical approach of
an equilibrium solution, which would otherwise necessarily preclude motion. There are
multiple options to turn this result into a variational inference algorithm. Note, the above
equation makes no assumptions about the form of variational density or the generative
model, and thus allows multimodal or nonparametric distributions to be represented.
For instance, the above equation (Equation 3.13) could be integrated numerically by
a number of particles in parallel, thus leading to a generalization of particle ﬁltering
(Friston, 2008b). Alternatively, a ﬁxed Gaussian form for the variational density can
be assumed, using the Laplace approximation. In this case, we obtain a very similar
algorithm to predictive coding as before, but using generalized coordinates of motion.
In the latter case, we can write out the free energy as,
Ft = ln p( ˜o|˜x)p(˜x)
∝ ˜Σ−1
o ˜ε2
o + ˜Σ−1
x ˜ε2
x (3.14)
Where ˜εo = ˜o −˜f (˜x) and ˜εx = ˜o −˜g(˜¯x). Moreover, the generalized precisions ˜Σ−1
not only encode the covariance between individual elements of the data or latent
space at each order, but also the correlations between generalized orders themselves.
Since we are using a unimodal (Gaussian) approximation, instead of integrating the
stochastic differential equations of multiple particles, we instead only need to integrate
the deterministic differential equation of the mode of the free energy,
˙˜µ = D˜µ −˜Σ−1
o ˜εo −˜Σ−1
x ˜εx (3.15)
Chapter 3. Predictive Coding 94
which cashes out in a scheme very similar to standard predictive coding (compare to
Equation 3.8), but in generalized coordinates of motion. The only difference is the D˜µ
term which links the orders of motion together. This term can be intuitively understood
as providing the ‘prior motion’ while the prediction errors provide ‘the force’ terms.
To make this clearer, let’s take a concrete physical analogy whereµ is the position of
some object and µ′is the expected velocity. Moreover, the object is subject to forces
˜Σ−1
o ˜εo + ˜Σ−1
x ˜εx which instantaneously affect its position. Now, the total change in
position ˙˜µ can be thought of as ﬁrst taking the change in position due to the intrinsic
velocity of the object Dµ and adding that on to the extrinsic changes due to the various
exogenous forces.
Things get more complex when we consider a model which has both dynamical and
hierarchical components where there are interactions between them. This we call a
full-construct model following the lead of this tutorial (Buckley, Kim, McGregor, &
Seth, 2017). In a full construct model there is a dynamical hierarchy of levels where it
is assumed that each dynamical order is only able to affect the level below:
o = f (µ;θ)
µ = f ′(µ′;θ′)
µ′= f ′(µ′′;θ′′)
... (3.16)
Similarly, there is simultaneously a hierarchy of levels, where each level is assumed to
Chapter 3. Predictive Coding 95
be predicted by the level above it:
o = g(µ0;θ0)
µ0 = g′(µ1;θ1)
µ1 = g′(µ2;θ2)
µ2 = g′(µ3;θ3)
... (3.17)
Therefore, each node in the lattice of hierarchical and dynamic hierarchies is inﬂuenced
by two separate predictions - the dynamical prediction going from higher dynamical
orders to lower, and the hierarchical prediction propagating from higher levels of the
hierarchy to lower ones. Thus, a single state of a cause-unit µn
i , where i is the level of
the hierarchy, and n is the dynamical order, is deﬁned to be:
µn
i = f (µn+1
i ;θn+1
i )+ g(µn
i+1;θn
i+1) (3.18)
This means that the variational free energy must sum over both dynamical and hierar-
chical prediction errors, such that:
F = ∑
i
∑
n
Σn
i
−1(εn
i )2 (3.19)
And that additionally the updates for the representation-units and the weights must take
this into account. The revised update rules are presented below:
dµn
i
dt = Σ−1
n+1εn+1
i
d f
dµn
i
θn+1
i +Σ−1
n εn
i +Σ−1
i−1εn
i−1
dg
dµn
i
θn
i−1 +Σ−1
i εn
i (3.20)
And for the weights the update rule thus becomes:
dθn
i
dt = Σn
i εn
i ( d f
dθn
i
µn+1
i
T
+ dg
dθn
i
µn
i+1
T ) (3.21)
These rules appear somewhat more complicated than the corresponding rules in the
static case. Nevertheless they only incur a linear (in the order of generalized coordinates
considered) additional computational cost.
Chapter 3. Predictive Coding 96
Preliminary dynamical and full construct models were implemented and tested on
simple stimuli. The ﬁrst task the dynamical models were tested on was predicting a
sine wave. This is the perfect toy-task since sine waves have analytic derivatives to
any inﬁnite degree. We used a dynamical model which represented three orders of
generalized motion. The model was trained to predict a sine wave autoregressively
and its’ ﬁrst two temporal derivatives. The model rapidly learned to predict the sine
wave, as can be seen from the training graphs below. However, there was a consistent
phase-error in the predictions it made, which could have been caused by the rapid rate
of change of the sine-wave observations.
Chapter 3. Predictive Coding 97
(a) Incoming sense data - i.e. a
sine wave
(b) First derivative of the incom-
ing sense data
(c) Predicted incoming sense
data
(d) Prediction temporal deriva-
tive of the incoming sense data
(e) Prediction error at the ﬁrst
dynamical level
(f) Prediction error at the sec-
ond dynamical level
(g) Acitvation of representation
units at the ﬁrst dynamical level
(h) Activation of the representa-
tion units at the second dynam-
ical level
Figure 3.7: Prediction errors and prediction for simple toy dynamical models. The task of
the dynamical predictive coding model is to learn to predict a sinewave using only the
ﬁrst two dynamical orders – so including position, velocity, and acceleration. The model
starts from randomly initialized parameters. We see that the model very quickly learns
to match the incoming sine wave observations with only minimal error at the beginning.
Chapter 3. Predictive Coding 98
The dynamical model does not only work with sine waves. The model was also tested
on more jerky waveforms sawtooth waves. In this case a two layer linear dynamical
model was used which learned to predict the sawtooth wave and its ﬁrst temporal
derivative. The model predicts the wave very successfully, including the temporal
derivative, although there it is a little less successful. Once again there is a persistent
patterned prediction error, likely caused by the lag time between the models predictions
and the the observations it receives.
Chapter 3. Predictive Coding 99
(a) Incomding sense data - the
sawtooth wave
(b) First derivative of the incom-
ing sense data
(c) Predicted incoming sense
data
(d) Prediction temporal deriva-
tive of the incoming sense data
(e) Prediction error at the ﬁrst
dynamical level
(f) Prediction error at the sec-
ond dynamical level
(g) Activation of representation
units at the ﬁrst dynamical level
(h) Activation of the representa-
tion units at the second dynam-
ical level
Figure 3.8: Dynamical models tested on more challenging sawtooth and square wave
inputs. The model was randomly initialized and only modelled the ﬁrst two dynamical
orders (so position, velocity, acceleration). Apart from a brief initial period of uncertainty,
the model rapidly learned to predict these more challenging wave shapes.
We can also train ‘full-construct’ models on dynamical stimuli. Here we used a model
with two hierarchical layers and three dynamical layers and was trained autoregressively
to predict a sine-wave. Training was ‘online’ with a learning rate of0.01. The generative
model parameters θ were updated initialized randomly and updated each epoch. For
every ‘tick’ of the sine wave, the variational parametersµ were updated for 100 steps.
Chapter 3. Predictive Coding 100
Training graphs are shown below:
(a) Incomding sense data -
Sine wave
(b) First derivative of the
sense-data
(c) Predicted incoming sense
data
(d) The models’ prediction of
the incomign sense data
(e) The models’ prediction of
the ﬁrst derivative of the in-
coming sense-data
(f) the models’ prediction of
the second derivative of the
incoming sense-data
(g) The prediction error at the
ﬁrst hierarcical layer
(h) The prediction error at the
second hierarcical layer
(i) The prediction error at the
ﬁrst dynamical layer
Figure 3.9: The training graphs of the full construct model. It can successfully predict
the ﬁrst three temporal derivatives of a sine wave, and also minimise prediction error
up to multiple hierarchical layers. The full construct model was randomly initialized,
and learnt both parameters and inferred states purely online – thus achieving a ‘double
deconvolution’ (Friston et al., 2008).
The top two rows of graphs show the incoming sense data and the ﬁrst two temporal
Chapter 3. Predictive Coding 101
derivatives of the sense-data. The next two rows show the prediction errors over time
for various levels of the hierarchy. The full construct model appears a bit less stable and
successful than the simple dynamical model, likely because it is much more complex
and has many more moving parts. Nevertheless it manages to learn the sine wave shapes
relatively faithfully and does also manage to rapidly reduce the prediction error over
time. Moreover these sorts of tasks do not really play well to the strength of the full-
construct model since the input data (the sine wave) contains no suitable hierarchical
structure for the higher levels to model. It seems likely that as these models are scaled
up to more challenging tasks, the greater expressivity and power of the full-construct
models will become more apparent. So far, we have only experimented with ‘full-
construct’ models on simple toy tasks such as sine waves. An interesting avenue for
future would work be experimenting as to whether full construct models could be scaled
up to handle challenging machine learning tasks dealing with sequential data such
as video prediction. While predictive coding networks have been proposed for this
task (Lotter et al., 2016), none to our knowledge have explicitly utilized generalized
coordinates in any capacity. There is, however, a literature using the predictive coding
schemes with generalized coordinates in nonlinear (chaotic) systems with separation
of temporal scales. This enables the recognition and prediction of things like birdsong
and speech – and indeed their learning of particular songs. Crucially, these applications
rest upon generalised coordinates of motion, usually up to 4th order motion (Friston &
Frith, 2015; Friston & Kiebel, 2009; Isomura, Parr, & Friston, 2019)
3.4 Predictive Coding and Kalman Filtering
A key intuition behind the utility of predictive coding is that it naturally handlesﬁltering
tasks. Filtering tasks require constant updates of a moving state estimate given sequences
of new data. Effectively, ﬁltering is the task of learning and inferring movements in
the hidden state of the world from changing input sequences – as opposed to the usual
Chapter 3. Predictive Coding 102
machine learning task of inferring hidden states (such as labels) from single static inputs.
Importantly the core task faced by much of the brain is fundamentally one of ﬁltering,
since the inputs the brain receives are actually temporally extended sequences, rather
than static ﬂashes. For instance, in vision the task of the brain is not to categorize static
images but rather to infer the state of, and ultimately interact with, a smoothly changing
external world situated in continuous time. Moreover, it is known that the brain takes
substantial advantage of the additional information given by integrating sequences over
time such as optical ﬂow (Gibson, 2002) and active motion to explore different angles
on a given scene (Henderson, 2017).
If, as we generally assume throughout the thesis, that the brain is fundamentally a
(Bayesian) inference machine, then the core task of the brain must beBayesian Filtering
instead of static Bayesian inference (Särkkä, 2013). Bayesian ﬁltering is mathematically
somewhat more involved, due to the need to perform inference over sequences instead
of single data-points, but there are a wide variety of algorithms in the literature which
perform Bayesian ﬁltering, often highly effectively (Kutschireiter, 2018; Kutschireiter,
Surace, & Pﬁster, 2020). Mathematically, we can formalize the ﬁltering problem as
follows (Jaswinski, 1970; Stengel, 1994). We have an estimated state ˆxt, and some
model of how the world evolves (the dynamics model): ˆxt+1 = f (ˆxt). We also receive
observations o, and you have some model of how the observations depend on the
estimated state (the observation model): o = g(ˆxt). The task, then, is to compute
p(ˆxt+1|ˆxt,o1...t). In the general nonlinear case, this calculation is analytically intractable
and extremely expensive to compute exactly. Some form of approximate solution is
required. Two forms of approximation are generally used. The ﬁrst is to approximate
the model - such as by assuming linearity of the dynamics and observation models.
The second method is to approximate the posterior – usually with a set of samples (or
particles). This approach is taken by the class of particle ﬁltering algorithms which
track the changing posterior by propagating the particles through the dynamics and
Chapter 3. Predictive Coding 103
then resampling based upon updated measurement information (Arulampalam, Maskell,
Gordon, & Clapp, 2002; N. J. Gordon, Salmond, & Smith, 1993). This approach
can handle general nonlinear ﬁltering cases, but suffers strongly from the curse of
dimensionality. If the state-space is high-dimensional the number of particles required
for a good approximation grows rapidly (Doucet, Godsill, & Andrieu, 2000).Moreover,
there has been some fascinating work on implementing particle ﬁltering methods in
neural circuitry (Kutschireiter, Surace, Sprekeler, & Pﬁster, 2015), as well as speculation
about whether perhaps the brain may utilize particle or sampling methods for inference
instead of variational ones (Sanborn & Chater, 2016).
Nevertheless, here we focus primarily on approximate variational approaches to infer-
ence. Speciﬁcally, we ﬁrst demonstrate that predictive coding is naturally a ﬁltering
algorithm – perhaps more naturally than one applied to static datasets. The only change
to the algorithm is simply what is predicted. If predictive coding is set up so as to
predict the next input (Clark, 2013b; Mumford, 1992), which is highly plausible in the
brain, then it can perform variational Bayesian ﬁltering. In this section, we explore
this predictive coding ﬁltering algorithm and show, crucially, that in the linear case
it becomes a variant of Kalman ﬁltering – a fundamental and ubiquitous algorithm
in classical control (Kalman, 1960; Kalman et al., 1960). Moreover, in the nonlinear
case, predictive coding becomes a variant of extended Kalman ﬁltering (Ollivier, 2019).
The Kalman Filter solves the general ﬁltering problem by making two simplifying
assumptions. The ﬁrst is that both the dynamics model and the observation model are
linear. The second assumption is that noise entering the system is white and Gaussian.
This makes both the prior and likelihoods Gaussian. Since the Gaussian distribution
is a conjugate prior to itself, this induces a Gaussian posterior, which can then serve
as the prior in the next timestep. Since both prior and posterior are Gaussian, ﬁltering
can continue recursively for any number of time-steps without the posterior growing
in complexity and becoming intractable. The Kalman ﬁlter is the Bayes-optimal so-
lution provided that the assumptions of linear models and white Gaussian noise are
Chapter 3. Predictive Coding 104
met (Kalman, 1960). The Kalman Filter, due to its simplicity and utility is widely used
in engineering, time-series analysis, aeronautics, and economics (Grewal & Andrews,
2010; Harvey, 1990; Leondes, 1970; Schneider, 1988).
Since predictive coding possesses several neurophysiologically realistic process theories
(Bastos et al., 2012), this correspondence provides an avenue for a biologically plausible
implementation of Kalman ﬁltering in the brain. There is substantial evidence that the
brain is capable of Bayes-optimal integration of noisy measurements, and is apparently
in possession of robust forward models both in perception (Simoncelli, 2009; Zago,
McIntyre, Senot, & Lacquaniti, 2008) and motor control (Gold & Shadlen, 2003;
Munuera, Morel, Duhamel, & Deneve, 2009; Todorov, 2004). de Xivry, Coppe, Blohm,
and Lefevre (2013) have even shown that a Kalman ﬁlter successfully ﬁts psychomotor
data on visually guided saccades and smooth pursuit movement, although they remain
agnostic on how it may be implemented in the brain. We demonstrate, however, a clear
mathematical link of the relationship between Kalman ﬁltering and predictive coding,
allowing us ﬁrst to use results from Kalman ﬁltering to understand the performance of
predictive coding algorithms, and second enabling us to utilize the process theories of
predictive coding to understand how the brain may perform crucial ﬁltering tasks.
First, we reveal the precise relationship between Kalman ﬁltering and predictive coding
– namely that both optimize the same Bayesian objective, which is convex in the linear
case. However, the Kalman ﬁlter solves the optimization problem analytically, thus
giving rise to its algebraic complexities and especially the highly neurobiologically im-
plausible Kalman gain matrix. Predictive coding, on the other hand, solves the objective
through a process of gradient descent on the sufﬁcient statistics of the variational dis-
tribution, thereby obtaining biologically plausible Hebbian update rules. Additionally,
the fully Bayesian perspective granted by predictive coding also allows us to perform
learning of the generative model – i.e. learning the coefﬁcients of the dynamics and
likelihood matrices – which allows us to handle cases where the model of the world is
Chapter 3. Predictive Coding 105
unknown, in contrast to traditional Kalman ﬁltering which assumes accurate (and linear)
dynamics and observation models of the world. While the close relationship between
Kalman ﬁltering and (linear) predictive coding has been hinted at before (Friston, 2005,
2008a), there it is claimed that predictive coding is ‘equivalent’ to Kalman ﬁltering –
which is not the case except insofar as the two algorithms optimize the same objective.
Baltieri and Buckley (2020) provide side by side comparisons of the update rules for
predictive coding and Kalman ﬁltering, but do not go beyond this superﬁcial analysis to
uncover the precise relationship between them.
Secondly, we directly compare the performance of the Kalman ﬁlter and our predictive
coding algorithm on a simpliﬁed location tracking task – which the Kalman ﬁlter
excels at. We show that despite the predictive coding algorithm performing a gradient
descent instead of an analytical solution, it performs comparably with the Kalman ﬁlter
and, due to the convexity of the underlying optimization problem, requires very few
iterations to converge. This rapid convergence is important, since the brain is heavily
time-constrained in its inferences – choices often must be made fast. Secondly, we
demonstrate that the learning rules for the likelihood and dynamics matrices allow us to
perform online tracking even when the model is completely unknown. We only show
that this is the case for the dynamics, however, as learning does not perform well with
an unknown observation model. We hypothesize that this is due to the ill-posedness of
the resulting optimization problem.
3.4.1 The Kalman Filter
The Kalman Filter is deﬁned upon the following linear state-space 3
xt+1 = Axt +But +ω
ot+1 = Cxt+1 +zt (3.22)
3For simplicity, the model is presented in discrete time. The continuous time analogue of the Kalman
ﬁlter is the Kalman-Bucy ﬁlter (Kalman & Bucy, 1961). Generalization of this scheme to continuous
time is an avenue for future work.
Chapter 3. Predictive Coding 106
Where xt represents the hidden or internal state at time t. ut is the control - or known
inputs to the system - at time t. Matrices A,B, and C parametrize the linear dynamics or
observation models, and ω and z are both zero-mean white noise Gaussian processes
with covariance Σω and Σz, respectively. Since the posterior p(xt+1|o1...t,xt) is Gaussian,
it can be represented by its two sufﬁcient statistics – the mean µ and covariance matrix
Σx.
Kalman ﬁltering proceeds by ﬁrst ‘projecting’ forward the current estimates according
to the dynamics model. Then these estimates are ‘corrected’ by new sensory data. The
Kalman ﬁltering equations are as follows:
Projection
ˆµt+1 = Aµt +But
ˆΣx(t +1) =AΣx(t)AT +Σω (3.23)
Correction
µt+1 = ˆµt+1 +K(ot+1 −Cˆµt+1)
Σx(t +1) = (I −K)ˆΣx(t +1)
K = ˆΣx(t +1)CT [C ˆΣx(t +1)CT +Σz]−1 (3.24)
Where µt and Σx(t) are the mean and variance of the estimate of the statex at time t, and
K is the Kalman gain matrix. Although these update rules provide an analytically exact
solution to the ﬁltering problem, the complicated linear algebra expressions, especially
that for the Kalman gain matrix K, make it hard to see how such equations could be
implemented directly in the brain.
Importantly, these Kalman ﬁltering equations can be derived directly from Bayes’ rule.
The mean of the posterior distribution is also the MAP (maximum-a-posteriori) point,
since a Gaussian distribution is unimodal. Thus, to estimate the new mean, we simply
Chapter 3. Predictive Coding 107
have to estimate,
argmax
ˆxt+1
p(ˆxt+1|ot+1, ˆxt) ∝ argmax
ˆxt+1
p(ot+1|ˆxt+1)p(ˆxt+1|ˆxt)
= argmax
ˆxt+1
N(ot+1;C ˆxt+1,Σz)N(ˆxt+1;A ˆxt +But,Σω)
= argmax
µt+1
1
Z exp(−(y −Cµt+1)T ΣZ(y −Cµt+1)
+(µt+1 −Aµt −But)T ˆΣx(µt+1 −Aµt −But)
= argmin
µt+1
−(y −Cµt+1)T ΣZ(y −Cµt+1)+( µt+1 −Aµt −But)T ˆΣx(µt+1 −Aµt −But)
(3.25)
In the second line, the algebraic form of the Gaussian density is substituted and we
have switched the maximization variable to µt+1 due to the fact that the maximum of a
Gaussian is also its mean. We also minimize the log probability instead of maximizing
the probability, which gets rid of the exponential and the normalizing constant (which
can be computed analytically since the posterior is Gaussian) 4. From this objective,
one can simply solve analytically for the optimal µ and Σ. For a full derivation see
Appendix A.
3.4.2 Predictive Coding as Kalman Filtering
Here we demonstrate the relationship between predictive coding and Kalman ﬁltering.
First, we need to explicitly write out and adapt the mathematical apparatus of predictive
coding to ﬁltering problems. To do so, we need to perform variational inference
over full trajectories o1:T ,x1:T of observations and hidden states. If we then assume
trajectories are Markov, and are thus licensed to apply a Markov factorization of the
generative model p(o1:T ,x1:T ) =p(o1|x1)p(x1)∏T
t=2 p(ot|xt)p(xt|xt−1) 5 and a mean-
ﬁeld temporal factorization of the variational density, so that it is independent across
timesteps q(x1:T ;θ) =∏T
t=1 q(xt;θ), then the variational free energy of the trajectory
4The log transformation is valid under maximization/minimization since the log function is monotonic.
5Where, to make this expression not a function of xt−1, we implicitly average over our estimate of
xt−1 from the previous timestep: p(xt |xt−1) =Eq(xt−1)[p(xt |xt−1)]
Chapter 3. Predictive Coding 108
factorizes into independently optimizable free-energies of a particular timestep,
F (o1:T ) =
T
∑
t=1
Ft(ot)
Ft(ot) =DKL[q(xt;θ)||p(ot,xt|xt−1)] (3.26)
This temporal factorization of the free energy means that the minimization at each
timestep is independent of the others, and so we only need consider a single minimiza-
tion of a single timestep to understand the solution, since all time-steps will be identical
in terms of the solution method. Applying the linear Gaussian assumptions of the
Kalman ﬁlter, we can specify our generative model in terms of Gaussian distributions,
p(ot,xt|xt−1) =p(ot|xt)p(xt|xt−1)
= N (ot;Cxt,Σz)N (xt|Axt−1,Σx) (3.27)
Since we know the posterior is Gaussian, it makes sense to also use a Gaussian distribu-
tion for the variational approximate distribution. Importantly, for predictive coding we
make an additional assumption – the Laplace Approximation – which characterises the
variance of this Gaussian as an analytic function of the mean, thus deﬁning,
q(xt;θ) =N (xt;µt,σ(µ)) (3.28)
where θ = [µt,σ(µt)] are the parameters of the variational distribution – in this case a
mean and variance since we have assumed a Gaussian variational distribution. With the
variational distribution and generative model precisely speciﬁed, it is now possible to
explicitly evaluate the variational free energy for a speciﬁc time-step,
Ft(ot) =Ft(ot) =DKL[q(xt;θ)||p(ot,xt|xt−1)]
= −Eq(xt ;θ)[ln p(ot,xt|xt−1)]−H[q(xt;θ)] (3.29)
Where the second term is the entropy of the variational distribution. Since we are
only interested in minimizing with respect to the mean µt and the expression for the
Chapter 3. Predictive Coding 109
entropy of a Gaussian does not depend on the mean, we can ignore this entropy term in
subsequent steps. The key quantity is the ‘energy’ termEq(xt ;θ[ln p(ot,xt|xt−1]. Since
the Laplace approximation ensures that most of the probability distribution is near the
mode µt of the variational distribution, we can well approximate the expectation using a
Taylor expansion to second order around the mode,
Eq(xt ;θ[ln p(ot,xt|xt−1)] ≈ln p(ot,µt|µt−1)+ E[∂p(ot,xt|xt−1)
∂xt
|xt =µt [xt −µt]
+E[∂2 p(ot,xt|xt−1)
∂x2t
|xt =µt [xt −µt]2
= ln p(ot,µt|µt−1)+ ∂p(ot,xt|xt−1)
∂xt
|xt =µt [E[xt]−µt]  
=0
+ ∂2 p(ot,xt|xt−1)
∂x2t
|xt =µt E[(xt −µt)2]  
=σ
(3.30)
Since the ﬁrst term vanishes as E[xt]−µt = µt −µt = 0 and we can neglect the second
term since it only depends on σ and not µ, then the only term that matters for the
minimization is the ﬁrst term ln p(ot,µt|µt−1). This means that we can write the overall
optimization problem solved by predictive coding as,
argmin
µt
Ft(ot) =argmin
µt
ln p(ot,µt|µt−1) (3.31)
which is the same as the MAP optimization problem presented in Equation 3.25. This
means that ultimately the variational inference problem solved by predictive coding
and the MAP estimation problem solved by the Kalman ﬁlter are the same although the
interpretation of µt differs slightly – from being a parameter of a Gaussian variational
distribution versus simply a variable in the generative model – the actual update rules
involving µt are the same in both cases. Now we know that (linear) predictive coding
and Kalman ﬁltering share the same objective, we can precisely state their differences.
While Kalman ﬁltering analytically solves this objective directly, in predictive coding,
we instead set the dynamics of the parameters to be a gradient descent on the variational
free energy, which reduces to the MAP objective solved by the Kalman Filter.
Chapter 3. Predictive Coding 110
For instance, we can derive the dynamics with respect to the variational parametersµt+1
which, in neural process theories, are typically operationalized as the ‘activation’ units
as,
dL
dµt+1
= 2CT Σzy −(CT ΣzC +CT ΣT
z C)µt+1 +(Σx +ΣT
x )µt+1 −2ΣxAµt −2ΣxBut
= 2CT ΣzCµt+1 −2CT ΣzCµt+1 +2Σxµt+1 −2ΣxAµt −2ΣxBut
= −CT Σz[y −Cµt+1]+ Σx[µt+1 −Aµt −Bµt]
= −CT Σzεz +Σxεx (3.32)
Where εz = y −Cµt+1 and εx = µt+1 −Aµt −But.Thus, we can see that the gradient
perfectly recapitulates the standard predictive coding scheme with precision weighted
prediction errors. Similarly, by taking gradients with respect to the A, B, and C matrices
of the generative model, we obtain familiar looking update rules which consist of
Hebbian update rules between the prediction errors and the presynaptic activations
dL
dA = d
dA [−2µT
t+1ΣxAµt +µT
t AT ΣxAµt +µT
t AT ΣxBut +uT
t BT ΣxAµt]
= −2ΣT
x µt+1µT
t +ΣT
x AµtµT
t ΣxAµtµT
t +ΣxButµT
t +ΣT
x ButµT
t
= −Σx[µt+1 −Aµt −But]µT
t
= −ΣxεxµT
t (3.33)
And similarly for the B matrix.
dL
dB = dL
dB [2uT
t BT ΣxAµt +uT
t BT ΣxBut −2µT
t+1ΣxBut]
= (Σx +ΣT
x )ButuT
t +2ΣxAµtuT
t −2Σxµt+1uT
t
= −Σx[µt+1 −Aµt −But]uT
t
= −ΣxεxuT
t (3.34)
Chapter 3. Predictive Coding 111
And the C observation matrix.
dL
dC = dL
dC[−2µT
t+1CT Ry +µT
t+1CT RCµt+1]
= −2RyµT
t+1 +2RCµt+1µT
t+1
= −R[y −cµt+1]µT
t+1
= −RεyµT
t+1 (3.35)
Chapter 3. Predictive Coding 112
3.4.3 Results
0 200 400 600 800 1000
Timestep
0
20
40
60
80
100Value
Time course of true dynamical variables
Position
Velocity
Acceleration
(a) True Dynamics
0 200 400 600 800 1000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0Value
Time Course of Control Input
Control Input (b) Control Input
0 200 400 600 800 1000
Timestep
−250
−200
−150
−100
−50
0
Value
Time course of obser ations (random C matrix)
Obser ed Position
Obser ed Velocity
Obser ed Acceleration
(c) Observations
Figure 3.10: The true dynamics, control input, and observations generated by a random
C matrix. These are the source of truth that the predictive coding Kalman ﬁlter tries to
approximate. The observations differ substantially from the true dynamics due to the
random C matrix, which makes the inference problem faced by the predictive coding
ﬁlter much more challenging, since it must de-scramble the observations to infer the true
dynamics.
Chapter 3. Predictive Coding 113
We now compare the analytical Kalman ﬁlter with our predictive coding algorithm
on a simple ﬁltering application – that of tracking the motion of an accelerating body
given only noise sensor measurements. The body is accelerated with an initial high
acceleration that rapidly decays according to an exponential schedule. The ﬁltering
algorithm must infer the position, velocity, and true acceleration of the body from only
a kinematic dynamics model and noisy sensor measurements. The body is additionally
perturbed by white Gaussian noise in all of the position, velocity and displacement. The
control schedule and the true position, velocity and displacement of the body are shown
in Figure 3.10 below.
The analytical Kalman ﬁlter was set up as follows. It was provided with the true
kinematic dynamics matrix (A) and the true control matrix (B),
A =


1 dt 1
2 dt2
,0 1 dt
0 0 1


B =
[
0 0 1
]
(3.36)
The observation matrix C matrix was initialized randomly with coefﬁcients drawn from
a normal distribution with 0 mean and a variance of 1. This effectively random mapping
of sensory states meant that the ﬁlter could not simply obtain the correct estimate
directly but had to disentangle the measurements ﬁrst. The Q and R matrices of the
analytical Kalman ﬁlter were set to constant diagonal matrices, where the constant was
the standard variance of the noise added to the system.
Chapter 3. Predictive Coding 114
0 200 400 600 800 1000
Timestep
0
10
20
30
40Predicted Value
Estimated Position
True Value
Kalman Filter
Gradient Method
(a) Position
0 200 400 600 800 1000
Timestep
0
20
40
60
80Predicted Value
Estimated Velocity
True Value
Kalman Filter
Gradient Method (b) Velocity
0 200 400 600 800 1000
Timestep
0
20
40
60
80
100Predicted Value
Estimated Acceleration
True Value
Kalman Filter
Gradient Method
(c) Acceleration
Figure 3.11: Tracking performance of our gradient ﬁlter compared to the true values and
the analytical Kalman Filter.We show the tracking over 2000 timesteps.
The performance of the analytical Kalman ﬁlter which computed updates using equa-
tions 1-3 is compared with that of our neural Kalman ﬁlter using gradient descent
dynamics.6 In this comparison the A, B, and C matrices are ﬁxed to their correct values
and only the estimated mean is inferred according to Equation 3.32. Comparisons
are provided for a number of different gradient steps. As can be seen in Figure 3.12,
6The code used for these simulations is freely available and online at htt ps :
//github.com/Bmillidgework /NeuralKalmanFiltering
Chapter 3. Predictive Coding 115
0 10 20 30 40 50 60 70 80 90 100
Timestep
13
14
15
16
17
18
19Predicted Value
Estimated Position
True Value
Kalman Filter
5 Gradient Steps
2 Gradient Steps
(a) Position
0 10 20 30 40 50 60 70 80 90 100
Timestep
50
52
54
56
58
60Predicted Value
Estimated Velocity
True Value
Kalman Filter
5 Gradient Steps
2 Gradient Steps (b) Velocity
0 10 20 30 40 50 60 70 80 90 100
Timestep
99.50
99.75
100.00
100.25
100.50
100.75
101.00
101.25Predicted Value
Estimated Acceleration
True Value
Kalman Filter
5 Gradient Steps
2 Gradient Steps
(c) Acceleration
Figure 3.12: Here, we zoom in on 100 timestep period to demonstrate tracking perfor-
mance in miniature and the effect of few gradient updates. In this case, we plotted the
estimates after only 5 steps. Even two steps often sufﬁce (with a large learning rate) to
provide very accurate estimates
Chapter 3. Predictive Coding 116
only a small number (5) of gradient descent steps are required to obtain performance
very closely matching the analytical result. This is likely due to the convexity of the
underlying optimization problem, and means that using gradient descent for "online"
inference is not prohibitively slow. The simulation also shows the estimate for too
few (2) gradient steps for which results are similar, but the estimate may be slightly
smoother.
Next, we demonstrate the adaptive capabilities of our algorithm. In Figure 3.14, we show
the performance of our algorithm in predicting the position, velocity, and acceleration
of the body when provided with a faulty A matrix. Using Equation 3.33, our model
learns the A matrix online via gradient descent. To ensure numerical stability, a very
small learning rate of 10−5 must be used. The entries of the A matrix given to the
algorithm were initialized as random Gaussian noise with a mean of 0 and a standard
deviation of 1. The performance of the algorithm without learning the A matrix is
also shown, and estimation performance is completely degraded without the adaptive
learning. The learning process converges remarkably quickly. It is interesting, moreover,
to compare the matrix coefﬁcients learned through the Hebbian plasticity to the known
true coefﬁcients. Often they do not match the true values, and yet the network is able
to approximate Kalman ﬁltering almost exactly. Precisely how this works is an area
for future exploration. If a system similar to this is implemented in the brain, then this
could imply that the dynamics model inherent in the synaptic weight matrix should not
necessarily be interpretable.
We also show (second row of Figure 3.14) that, perhaps surprisingly, both the A and B
matrix can be learned simultaneously. In the simulations presented below, either only
the A matrix, or both the A and the B matrix were initialized with random Gaussian
coefﬁcients, and the network learned to obtain accurate estimates of the hidden state in
these cases. 7
7The results of only learning the B matrix were extremely similar for that of the A matrix. For concise-
ness, the results were not included. Interested readers are encouraged to look at the NKFABmatrix.ipynb
ﬁle in the online code where these experiments were run.
Chapter 3. Predictive Coding 117
0 300 600 900 1200 1500
Timestep
−20
0
20
40
60
80
100
Predicted Value
Estimated position
True Value
Kalman Filter
Gradient Method
No A learning
(a) Position for learnt A matrix
0 300 600 900 1200 1500
Timestep
−20
0
20
40
60
80
100
120
140
Predicted Value
Estimated velocity
True Value
Kalman Filter
Gradient Method
No A learning (b) Velocity for learnt A matrix
0 300 600 900 1200 1500
Timestep
0
20
40
60
80
100Predicted Value
Estimated acceleration
True Value
Kalman Filter
Gradient Method
No A learning
(c) Filtering performance for adaptively learning
just the A matrix.
We also tried adaptively learning theC matrix using Equation 3.35, but all attempts to
do so failed. Although the exact reason is unclear, we hypothesise that an incorrect C
matrix corrupts the observations which provides the only "source of truth" to the system.
If the dynamics are completely unknown but observations are known, then the true state
of the system must be at least approximately near that implied by the observations, and
the dynamics can be inferred from that. On the other hand, if the dynamics are known,
but the observation mapping is unknown, then the actual state of the system could be
Chapter 3. Predictive Coding 118
0 200 400 600 800 1000
Timestep
−100
−80
−60
−40
−20
0
20
40
Predicted Val e
Estimated Position
Tr e Val e
Kalman Filter
Gradient Method
No Learning
(a) Position: learnt A and B matrices
0 200 400 600 800 1000
Timestep
0
50
100
150
200
250
300
350Predicted Value
Estimated Velocity
True Value
Kalman Filter
Gradient Method
No Learning (b) Velocity: learnt A and B matrices
0 200 400 600 800 1000
Timestep
−200
−150
−100
−50
0
50
100
Predicted Value
Estimated Acceleration
True Value
Kalman Filter
Gradient Method
No Learning
(c) Acceleration: learnt A and B matrices
Figure 3.14: Filtering performance for adaptively learning both the A and B matrices in
concert (second row). The ﬁltering behaviour of the of the randomly initialized ﬁlters
without adaptive learning is also shown. Importantly, with learning the estimated position,
velocity, and accelerations track their true values precisely while the estimates without
learning and just randomly initialized A or A and B matrices rapidly diverge from the
truth.
Chapter 3. Predictive Coding 119
on any of a large number of possible dynamical trajectories, but the exact speciﬁcs of
which are underspeciﬁed. Thus the network learns a C matrix which corresponds to
some dynamical trajectory, which succeeds in minimizing the loss function, but which
is completely dissimilar to the actual trajectory the system undergoes. This can be seen
by plotting the loss obtained according to Equation 3.25 in Figure 3.15, which rapidly
decreases, although the estimate diverges from the true values.
3.4.4 Discussion
Here we have elucidated the precise relationship between Kalman ﬁltering – an optimal
linear Bayesian ﬁltering algorithm – and predictive coding – a neurophysiologically
realistic theory of cortical function. Speciﬁcally, that they both optimize the same
objective function – a Bayesian MAP ﬁltering objective – while the Kalman ﬁltering
solves the resulting optimization problem analytically, predictive coding approaches
derive their dynamics from a gradient descent on the same objective. "This result
provides a new perspective on predictive coding; especially if we make the simplifying
assumption that the precisions are not optimised with respect to variational free energy
– or, in the examples above, we assume the conditional covariance is zero. This
reduces variational inference to a MAP optimisation problem. This follows due to
the Laplace approximation, which effectively means the variational precision can be
derived analytically from the expectation (from the curvature of the log likelihood at
the expectation) – see (Friston & Stephan, 2007) for details. Alternatively, we can just
ignore the conditional uncertainty as in the MAP optimisation perspective.
Our work also demonstrates how straightforwardly predictive coding can be applied to
solve Bayesian ﬁltering problems rather than simply static Bayesian inference problems.
Since the brain is enmeshed in continuous sensory exchange with a constantly moving
world, ﬁltering is a much more realistic challenge to solve than pure inference on a
static dataset. It thus seems likely that the neural circuitry dedicated to perception
Chapter 3. Predictive Coding 120
0 200 400 600 800 1000
Timestep
0
10
20
30
40Predicted Value
Estimated Position
True Value
Kalman Filter
Gradient Method
(a) Position: learnt C matrix
0 200 400 600 800 1000
Timestep
−60
−40
−20
0
20
40
60
80
Predicted Value
Estimated Velocity
True Value
Kalman Filter
Gradient Method (b) Velocity: learnt C matrix
0 200 400 600 800 1000
Timestep
0
20
40
60
80
100Predicted Value
Estimated Acceleration
True Value
Kalman Filter
Gradient Method
(c) Acceleration: learnt C matrix
0 200 400 600 800 1000
Timestep
0
10
20
30
40Loss
Bayesian loss function (d) Loss function over timesteps
Figure 3.15: Very poor tracking behaviour with a learnt C matrix. This is despite the fact
that the Bayesian loss function rapidly decreases to a minimum. This shows that the
ﬁlter can ﬁnd a prediction-error minimizing "solution" which almost arbitrarily departs
from reality if the C-matrix is randomized. Panel D shows the loss computed by the
network which rapidly declines, even while the predictions rapidly diverge from the truth
(Panels a,b,c)
Chapter 3. Predictive Coding 121
is specialized for solving precisely these sorts of ﬁltering problems. Moreover, due
to predictive coding’s biologically realistic properties, our results provide a powerful
biologically plausible approach for how the brain might solve such ﬁltering problems.
Nevertheless, there remain several deﬁciencies of our algorithm (and predictive coding
more generally) in terms of biological plausibility which it is important to state. Our
model assumes full connectivity for the ‘diffuse’ connectivity required to implement
matrix multiplications. Additionally in other cases it requires one-to-one excitatory
connectivity, both constraints which are not fully upheld in neural circuitry. Additionally,
in one case (that of the "C matrix" between the populations of neurons representing the
estimate and the sensory prediction errors), we have assumed a complete symmetry of
backward and forward weights, such that the connections which embody the C matrix
downwards also implement the CT matrix when traversing upwards. This is also a
constraint not satisﬁed within the brain. Additionally, our model can represent negative
numbers in states or prediction errors, which rate-coded neurons cannot. Several of
these implausibilities will be directly addressed in the context of (static) predictive
coding later in this chapter.
We believe, however, that despite some lack of biological plausibility, our model is useful
in that it shows how a standard engineering algorithm can be derived in a way more
amenable to neural computation, and provides a sketch at how it could be implemented
in the brain. Moreover, we hope to draw attention to Bayesian ﬁltering algorithms and
how they can be implemented neurally, instead of just Bayesian inference on static
posteriors.
Finally, while our algorithm and experiments have only considered the linear case, it
can be straightforwardly extended to the nonlinear case, where it results in standard
nonlinear predictive coding as discussed previously. Explicitly and empirically compar-
ing the performance of our algorithm against nonlinear extensions to the Kalman ﬁlter
such as extended or unscented (Wan & Van Der Merwe, 2000) Kalman ﬁltering are an
Chapter 3. Predictive Coding 122
important and exciting avenue for future work.
3.5 Relaxed Predictive Coding
In the literature predictive coding has been proposed as a general theory of cortical
function (Friston, 2003, 2005, 2008a; Kanai et al., 2015; Spratling, 2008). There is
additionally a small literature of process theories which try to translate the mathematical
formalism into purported neural circuitry (Bastos et al., 2012; Kanai et al., 2015;
Keller & Mrsic-Flogel, 2018), and some of the predictions of predictive coding have
been extensively compared and evaluated against neurophysiological data (Aitchison
& Lengyel, 2017; Clark, 2015; Friston, 2008a; Huang & Rao, 2011; Walsh et al.,
2020). Despite the general acceptance of predictive coding as a biologically plausible
algorithm which could in theory be implemented in the brain, there nevertheless are
several highly implausible aspects of the core algorithm that have been largely glossed
over in the formulations of the process theories, which focused primarily on macro-scale
connectivity constraints instead of the precise mathematical form of the learning and
update rules in the algorithm (Bastos et al., 2012). Here, we introduce three potentially
severe biological implausibilities which emerge directly from the form of the predictive
coding algorithm and demonstrate empirically how, with some ingenuity and adaptation
of the algorithm, these implausible assumptions can be ‘relaxed’ without major damage
to the empirical performance of predictive coding networks on object recognition tasks.
This work is based on (Millidge, Tschantz, Seth, & Buckley, 2020d)
Recall, that the core of the predictive coding formalism is three key relationships. First,
the concept of prediction error as the difference between the activity of the neurons in a
layer and the top-down predictions from higher layers. Second, the update rule for the
activities of a layer, which minimizes both the prediction errors at its own layer, as well
as the layer below. And thirdly, the learning rule for the weights, as a local Hebbian
Chapter 3. Predictive Coding 123
function of the prediction errors at their own layer (Friston, 2005).
εl = µl −f (θl+1µl+1)
dµl
dt = −εl +θlT εl−1 f ′(θlµl)
dθl
dt = εl−1 f ′(θlµl)µlT (3.37)
where f ′(θlµl) represents the partial derivative of the post-activations with respect to
either the µ or the θ depending upon the update rule. Equation3.37 states that prediction
errors are computed as a simple subtraction of the value neurons at a layer and the
prediction from the layer above. The vector µl represents the activity of the value
neurons at a speciﬁc level l. The vector εl is a vector of the activity of the error neurons
at a level l. Predictions are mapped down from the higher layers through a set of weights,
denoted W which is an M ×N matrix where M is the number of neurons at level l and
N is the number of neurons at level l +1. f (x) is a nonlinear activation function applied
to the outputs of a neuron and f ′(x) =∂f (x)
∂x is the pointwise derivative of the activation
function 8.
Equation 3.8 speciﬁes the update rule for the µl at a speciﬁc layer. The update is equal
to the sum of the prediction errors projected up from the layer below, multiplied by the
top down predictions and projected back through the weight matrix and subtracted from
the prediction errors at the current layer. This is a biologically plausible learning rule
as it is a simple sum of multiplication of locally available information. Note: the the
update includes prediction error terms from both the current layer and the layer below.
This equation is why it is necessary to transmit prediction errors upwards.
Equation 3.9 is the update rule for the weights θ. This obeys Hebbian plasticity since
it is simply a multiplication of the two quantities available at each end of the synaptic
connection – the prediction error of the layer below and the value neurons at the current
8Here we have specialized the predictive coding update rules somewhat from any arbitrary function f
to an elementwise nonlinearity followed by a multiplication with a weight matrix θ. This is because in
this section we consider the application of predictive coding to train artiﬁcial neural networks with this
speciﬁc type of structure
Chapter 3. Predictive Coding 124
layer. The only slight difﬁculty is the derivative of the nonlinear activation function
of the prediction. While this information is locally available in principle, it requires a
somewhat more complex neural architecture and it is not certain that the derivatives of
activation functions can be computed straightforwardly by neurons. Luckily, we show
below that this term is not needed for successful operation of the learning rule.
Importantly, we additionally recall that predictive coding can be considered to be a
variational inference algorithm, as Equations 3.8 and 3.9 can be directly derived as a
gradient descent upon the variational free energy F = ∑L
i=0 ε2
i which (under Gaussian
assumptions) takes the form of a simple sum of squared prediction errors at each layer.
We additionally ignore the precision parameters Σl in this analysis since in general their
biological plausibility has not been strongly analyzed in the literature (although there
are some speculative suggestions linking them to either lateral connectivity (Friston,
2005) or else subcortical activity in the pulvinar (Kanai et al., 2015)).
Speciﬁcally, we focus on three important implausibilities. The ﬁrst is the problem of
weight symmetry, or the required equality of forward and backward weights. This
problem is often called the weight transport problem in the literature (Crick, 1989;
Lillicrap, Cownden, Tweed, & Akerman, 2016; Lillicrap, Santoro, Marris, Akerman,
& Hinton, 2020). The learning rules in these networks require information to be sent
‘backwards’ through the network. Since synaptic connections are generally assumed to
be uni-directional, in practice this means that these backward messages need to be sent
through a second set of backwards connections with the exact same synaptic weights
as the forward connections. Clearly, expecting the brain to have an identical copy
of forward and backward weights is infeasible. Mathematically, this problem arises
from the θT
l term in the dynamics equation for the µs (Equation 3.8), since this weight
transpose uses the forward weight matrix θ but instead maps the bottom-up prediction
error to the level above, thus requiring information to be sent ‘backwards’ or ‘upwards’
through ‘downwards’ connections. In the brain, this would require information to
Chapter 3. Predictive Coding 125
propagate backwards from the soma of the post-synaptic neuron, back through the
axon and to the soma of the pre-synaptic cell – a possibility which is considered to
be extremely implausible (Lillicrap, Cownden, Tweed, & Akerman, 2014). Here, we
address this problem in predictive coding networks by using a separate set of randomly
initialized backwards weights trained with a separate Hebbian learning rule, which
also only requires local information. This removes the necessity of beginning with
symmetrical or identical weights and proposes a biologically plausible method of
learning good backward weights from scratch in an unsupervised fashion. In the brain
this would be implemented as a reciprocal set of ‘backwards’ connections going from
the lower-layers to higher layers, which are deﬁnitely present in the brain (Grill-Spector
& Malach, 2004).
The weight transport problem is also present in neural implementations of the backprop-
agation of error algorithm from machine learning, and there exists a small literature
addressing it within this context. A key paper (Lillicrap et al., 2014, 2016) shows that
simply using random backwards weights is sufﬁcient for some degree of learning. This
method is called feedback alignment (FA) since during training the feedforward weights
learn to align themselves with the random feedback weights so as to transmit useful
gradient information. A variant of this – direct feedback alignment (DFA) (Nøkland,
2016) has been shown that direct forward-backward connectivity is not necessary for
successful learning performance. Instead, all layers can receive backwards feedback
directly from the output layer. It has also been shown (Liao, Leibo, & Poggio, 2016)
that performance with random weights is substantially improved if the forward and
backward connections share merely the same sign, which is less of a constraint than the
exact value. One further possibility is to learn the backwards weights with an indepen-
dent learning rule. This has been proposed independently in Amit (2019) and Akrout,
Wilson, Humphreys, Lillicrap, and Tweed (2019) who initialize the backwards weights
randomly, but train them with some learning rule. Our work here differs primarily in
that we show that this learning rule works for predictive coding networks while they
Chapter 3. Predictive Coding 126
only apply it to deep neural networks learnt with backprop. Moreover, our Hebbian
learning rule can be straightforwardly derived in a mathematically principled manner as
part of the overarching variational framework of predictive coding.
The second problem is one of backward nonlinear derivatives. In predictive coding
networks (along with backprop), the update and learning rules require the pointwise
derivatives of the activation function to be computed at each neuron. Mathematically,
this is the f ′(θlµl) term. For individual biological neurons, while a nonlinear forward
activation function is generally assumed, the ability to compute the derivative of the
activation function is not known to be straightforward. While in some cases this issue
can be ameliorated by a judicious choice of activation function – for instance the
pointwise derivative of a rectiﬁed linear unit is simply 0 or 1, and is a simple step
function of the ﬁring rate – the problem persists in the general case. Here, we show
that, somewhat surprisingly, it is possible to simply ignore these pointwise derivatives
with relatively little impact on learning performance, despite the update rules now being
mathematically incorrect. This may free the brain of the burden of having to compute
these quantities.
A third issue, speciﬁc to frameworks that explicitly represent prediction errors, is the
requirement of one-to-one connections between activation units and their corresponding
error units. While not impossible, this precise, one-to-one connectivity pattern is likely
difﬁcult for the brain to create and maintain throughout development and learning. One
possibility, as explored by Sacramento, Costa, Bengio, and Senn (2018) is that prediction
errors and predictions may be housed in separate dendritic compartments on a single
neuron, thus potentially obviating this issue (although their scheme relied on another
set of one-to-one connections between pyramidal cells and inhibitory interneurons).
However, the plausibility of this idea in terms of actual dendritic morphology and
neurophysiology is unclear. Instead, here we present a network-level solution and show
that learning can continue unaffected despite random connectivity patterns between
Chapter 3. Predictive Coding 127
value and error units as long as these connection weights can also be learned. We propose
a further Hebbian and biologically plausible learning rule to update these weights which
also only requires local information. Finally, we experiment with combining our
solutions to all of these problems together to produce a fully relaxed predictive coding
architecture. Importantly, this architecture possesses a simple bipartite but otherwise
fully connected connectivity pattern with separate learnable weight matrices covering
every connection, all of which are updated with local Hebbian learning rules. We show
that despite the simplicity of the resulting relaxed scheme, that it can still be trained to
high classiﬁcation accuracy comparable with standard predictive coding networks and
ANNs using backpropagation.
3.5.1 Methods
To test the performance of the predictive coding network under various relaxations,
we utilize the canonical MNIST and FashionMNIST (Xiao, Rasul, & V ollgraf, 2017a)
benchmark datasets. Since this is a supervised classiﬁcation task, we follow the ap-
proach of Whittington and Bogacz (2017) and Millidge, Tschantz, and Buckley (2020a),
who utilized a ‘reverse’ predictive coding architecture where the inputs were presented
to the top layer of the network and the labels were predicted at the bottom. This for-
mulation allows for the straightforward representation of supervised learning problems
in predictive coding. In effect, the network tries to generate the label from the image.
We utilized a 4-layer predictive coding network consisting of 784, 300, 100, and 10
neurons in each layer respectively. We tested both rectiﬁed-linear (relu) and hyperbolic
tangent (tanh) activation functions, which are the most common activation functions
used in machine learning. During training the µs were updated for 100 iterations using
Equation 3.8 with both the input and labels held ﬁxed. After the iterations of the µs, the
remaining prediction errors in the network were used to update the weights according to
Equation 3.9. At test time, a digit image was presented to the network, and the top-down
predictions of the network were propagated downwards to produce a prediction at the
Chapter 3. Predictive Coding 128
lowest layer, which was compared to the true label to obtain the test accuracy.
The network was trained and tested on the MNIST and FashionMNIST datasets. MNIST
is a dataset of 60,000 28 ×28 grayscale images of handwritten digits from 0 to 9. The
goal is to predict the digit from the image. The FashionMNIST dataset contains images
of different types of clothing, which must be sorted into ten classes. FashionMNIST is
a more challenging classiﬁcation dataset, while also serving as a drop-in replacement
for MNIST since its data is in exactly the same format. The input images were ﬂattened
into a 784x1 vector before being fed into the network. Labels were represented as
one-hot vectors and were smoothed using a value of 0.1 for the incorrect labels. The
dataset was split into a training set of 50000 and a test set of 10000 images. All weight
matrices were initialized as draws from a multivariate Gaussian with a mean of 0 and
a variance of 0.05. It is likely, given the large literature on how to best initialize deep
neural networks, that there exist much better initialization schemes for predictive coding
networks as well, however we did not investigate this here. All results presented were
averaged over 5 random seeds. We plot error bars around the means as the standard
deviation of the seeds.
3.5.2 Results
3.5.2.1 Weight Transport
Mathematically, the weight transport problem is caused by the θT term in Equation
3.9. In neural circuitry this weight transpose corresponds to transmitting the message
backwards through the same connections or, alternatively, an identical copy of the
backward weights. We wish to replace this copy of the forward weights with an
alternative, unrelated set of weights ˜θ. Unlike FA or DFA methods, which simply
use random backwards weights, we propose to learn the backwards weights through a
simple synaptic plasticity rule.
d ˜θl
dt = µl( f ′(θlµl)εl−1)T
Chapter 3. Predictive Coding 129
𝜇
𝜇
𝜀
𝜀
𝜇
𝜇
𝜃,𝜃
𝜇
𝜇
𝜀
𝜀
𝜇
𝜇
˜θ
𝜃
Figure 3.16: The weight transport problem and our solution. On the left is the standard
predictive coding network architecture. Our diagram represents the prediction errors ε of
one layer receiving predictions and transmitted prediction errors to the value neurons of
the layer above. Prediction errors are transmitted upwards using the same weight matrix
θT as the predictions are transmitted downwards. On the right, our solution eschews
this biological implausibility by proposing a separate set of backwards weight ˜θ (in red),
which are learned separately using an additional Hebbian learning rule.
This rule is Hebbian since it is just the multiplication of the activities of the units at each
end of the connection. The backwards pointwise derivative poses a slight problem in
that it is ﬁrst multiplied with the errors of the level below. However, as we show below,
pointwise nonlinear derivatives are not actually needed for good learning performance,
so the problem is surmounted. This rule is simply the transposed version of the original
weight update rule (Equation 3.9). And thus, if the forward and backwards weights are
initialized to the same value, barring numerical error, they will stay the same throughout
training. Importantly, we demonstrate here that this rule allows rapid and effective
learning of the weights even if the forward and backwards matrices are initialized in a
completely independent (and random) fashion. This means that in the brain the forwards
and backwards connections originate completely independently, and that, moreover, if
we accept the forward weight update rule as plausible, we should accept the backwards
weight update as well, thus leading to no greater demands of biological plausibility for
this backwards weight update.
This procedure allows us to begin with a randomly initialized set of backwards weights,
Chapter 3. Predictive Coding 130
and then applying the learning rule to these weights allows us to very quickly recover
performance equal to the identical backwards weights. As shown in Figure 3.17, perfor-
mance both with and without the learnt backwards weights is almost identical for both
the relu and tanh nonlinearities and the MNIST and FashionMNIST datasets, thus sug-
gesting that this approach of simply learning an independent set of backwards weights
is a highly effective and robust method for tackling the weight transport problem.
(a) MNIST dataset; tanh activation
 (b) MNIST dataset; relu
(c) Fashion dataset; tanh
 (d) Fashion dataset; relu
Figure 3.17: Test accuracy of predictive coding networks with both learnt backwards
weights, and the ideal weight transposes with both relu and tanh activation functions
on the MNIST and FashionMNIST datasets. Both networks obtain almost identical
learning curves, thus suggesting that learnt backwards weights allow for a solution to
the weight-transport problem.
Chapter 3. Predictive Coding 131
3.5.2.2 Backwards nonlinear derivatives
The second remaining biological implausibility is that of the backwards nonlinear
derivatives. Note that in Equations 3.37, an f ′term regularly appears denoting the
pointwise derivative of the nonlinear activation function. Since these are pointwise
derivatives, when the mathematics is translated to neural circuitry, these derivatives need
to be computed at each individual neuron. It is not clear whether neurons are capable
of easily computing with the derivative of their own activation function. We apply
a straightforward remedy to address this. We simply experiment with removing the
pointwise derivatives from the update rules. For instance Equation 3.8 would become
just dµl
dt = −εl +θlT εl−1. Perhaps surprisingly we found that this modiﬁcation, although
not mathematically correct, did little to impair performance of the model at classiﬁcation
tasks, except perhaps for the hyperbolic tangent nonlinearity in the FashionMNIST
case.
In effect, by removing the pointwise nonlinear derivatives, we have made the gradient
updates linear in the parameters. Since the real updates are nonlinear, our update rules
are simply the projection of the nonlinear update rules onto a linear subspace. However,
using a similar argument to that in feedback alignment, we hypothesize that it is likely
that the linear projection of the nonlinear updates are quite close in angle to the nonlinear
updates, so the direction of the linear gradient, averaged over many batches and update
steps, is sufﬁciently close to the true gradient as to allow for learning in this model.
An alternative option is to note that the derivatives of the nonlinearity depend closely
upon the value of the function. In the case of the relu nonlinearity, the derivative is
only different from 1 when the activity is 0 (the neuron does not ﬁre). When there is
no ﬁring, there can still be updates to the dynamics – which depend on the prediction
errors and not on the actual ﬁring rate – and so there is still error when dropping the
derivative term. However, if most activations are greater than 0, the error should be
minimal, which is what we appear to observe. Similarly, in the hyperbolic tangent
Chapter 3. Predictive Coding 132
(a) MNIST dataset; tanh activation
 (b) MNIST dataset; relu activation
(c) Fashion dataset; tanh activation
 (d) Fashion dataset; relu activation
Figure 3.18: Test accuracy of predictive coding networks with and without the nonlinear
derivative term, using relu and tanh activation functions on the MNIST and Fashion-
MNIST datasets. We ﬁnd that on the MNIST dataset performance is similar, while on
the FashionMNIST dataset and the tanh activation function, the lack of the nonlinear
derivative appears to slightly hurt performance.
nonlinearity, the region of activation between −1 and 1 is broadly linear, and thus
we should expect the dropping of the nonlinear derivative term in this region to have
relatively little effect. The robustness and relatively little impact on training therefore
suggest that the activations of the predictive coding network largely remain within this
stable regime over the course of training – an intriguing and important ﬁnding given
that we made no efforts (such as regularization) to speciﬁcally encourage this outcome.
If brains operated in a similar regime, it may mean that explicit computation of the
activity derivatives is unnecessary, which would make credit assignment substantially
easier (if approximate).
Chapter 3. Predictive Coding 133
𝜇
𝜀
𝜇
𝜀
𝜇
𝜀
𝜇
𝜀
𝜓
Figure 3.19: The error-connectivity problem and our solution. On the left, the biologically
implausible one-to-one connectivity between value and error nodes required by the
standard predictive coding theory. On the right, our solution to replace these one to
one connections by a fully connected connectivity matrix ψ. By learning ψ with a
Hebbian learning rule we are able to achieve comparable performance to the one-to-one
connections with a fully dispersed connectivity matrix.
3.5.2.3 Error connections
The third and ﬁnal biological implausibility that we address in this section is that of the
one-to-one connections between value and the error units at a given layer. This can be
seen directly for the prediction errors in Equation 3.37, but broken down into individual
components (or neurons).
εl
i = µl
i −f (∑
j
θl+1
i, j µl+1
j ) (3.38)
We see that the activity of the error unit vector (i.e. each error neuron εi) is driven by a
one-to-one connection from its matching value neuron µi. By contrast, the top-down
predictions have a diffuse connectivity pattern, where every value neuron µj in the
layer above affects each error neuron εi through the synaptic weight θi, j. A one-to-one
connectivity structure is a highly precise and sensitive pattern and it is difﬁcult to see
how it could ﬁrst develop and then be maintained in the brain throughout the course of
an organisms life. Additionally, while precise connectivity can exist in theory, there is
little evidence neurophysiologically (Bastos et al., 2012; Walsh et al., 2020) for the kind
Chapter 3. Predictive Coding 134
of regular and repeatable one-to-one connectivity patterns that predictive coding would
require in the brain. Moreover, if predictive coding were implemented throughout the
cortex, this one-to-one connectivity should be highly visible to neuroscientists. To
relax this one-to-one connectivity constraint, we postulate a diffuse connectivity pattern
between them, mediated by a set of connection weights ψ. The new equation for the
prediction errors becomes:
εl
i = ∑
k
ψl
i,kµl
k −f (∑
j
θl+1
i, j µl+1
j )
=⇒εl = ψlµl −f (θl+1µl+1) (3.39)
While using randomly initialized weights ψ completely destroys learning performance,
it is possible to learn these weights in an online unsupervised fashion using another
Hebbian learning rule. The learning rule for the error weights ψ can be derived as a
gradient descent on the variational free energy function, whereby now the prediction
errors include the error weights,
dψl
dt = −∂F
∂ψ = −εl
∂εl
∂ψ
= −εlµT
l (3.40)
This rule is completely linear and Hebbian since it is simply a multiplication of the
activations at the two endpoints of the connection. We show in Figure 3.20 that using
this rule allows equivalent learning performance to the one-to-one case as the required
weight values are rapidly learnt.
We see that, overall, training performance can be maintained even with learnt error
connections, a perhaps surprising result given how key the prediction errors are in
driving learning. Interestingly, we see a strong effect of activation function on perfor-
mance. Performance is indistinguishable from baseline with a relu activation function
but asymptotes at a slightly lower value than the baseline with tanh. Investigating the
reason for the better performance of the relu nonlinearity would be an interesting task
for future work.
Chapter 3. Predictive Coding 135
(a) MNIST dataset; tanh activation
 (b) MNIST dataset; relu activation
(c) Fashion dataset; tanh activation
 (d) Fashion dataset; relu activation
Figure 3.20: Test accuracy of predictive coding networks with and without learnable error
connections for both relu and tanh activation functions on the MNIST and FashionMNIST
datasets. We see that, interestingly, using learnt error weights decreased performance
only with the tanh but not the relu nonlinearity, and then only slightly in the FashionMNIST
case.
3.5.2.4 Combining Relaxations
It is also possible to combine all of the above relaxations together in parallel, to create a
network architecture which is avoids many of the major biological plausibility pitfalls of
predictive coding. A schematic representation of this combined architecture compared
to the standard predictive coding architecture is shown below (Figure 3.21).
Chapter 3. Predictive Coding 136
(a) Standard PC
 (b) Relaxed PC
Figure 3.21: Schematic representations of the architecture across two layers of a.) the
standard predictive coding architecture and b.) The fully relaxed architecture. Importantly,
this architecture has full connectivity between all nodes and also non-symmetric forward
and backwards connectivity in all cases. In effect, this architecture only maintains a
bipartite graph between error and value neurons, but no other clear structure
The relaxed architecture no longer has any one-to-one connectivity patterns, which
have been replaced with full connectivity matrices parametrised by the error connection
weights ψ. Moreover, the forwards and backwards weights are separated into two
separate and independent weight matrices θ and ˜θ while the standard predictive coding
model uses the true weight transpose θT , which requires copying the weights. In
essence, the fully relaxed architecture simply consists of two bipartite populations of
neurons, which only synapse onto the other population. Beyond this there is no special
connectivity structure required. Nevertheless, we show that even this very simple
architecture, with only Hebbian learning rules, can still be trained to perform well at
supervised classiﬁcation.
We tested the classiﬁcation ability of the fully relaxed architecture with both hyperbolic
tangent and rectiﬁed linear activation functions, and on the MNIST and FashionMNIST
datasets, and the results are shown in Figure 3.22. Overall we found that another strong
effect of activation function where this time training was unstable and diverged when
using rectiﬁed linear units but not when using tanh neurons. We hypothesize that this
could be due to the rectiﬁed linear units not having a saturation point unlike tanh and
Chapter 3. Predictive Coding 137
(a) MNIST dataset; tanh activation
 (b) MNIST dataset; relu activation
(c) Fashion dataset; tanh activation
 (d) Fashion dataset; relu activation
Figure 3.22: Test accuracy standard and fully relaxed predictive coding networks (the
combined algorithm), for both relu and tanh activation functions on the MNIST and
FashionMNIST datasets. We see that, interestingly,performance is degraded in all
cases and that the relu networks are especially affected – with catastrophic declines in
performance to become almost random. The reasons for this are currently unknown and
will be investigated in future work.
thus being more prone to exploding gradients. We found, on the other hand, that while
performance of the fully relaxed network asymptotically tended to be slightly worse
than the standard, it was still very high on both the MNIST and fashion MNIST datasets,
thus showing that even highly relaxed and extremely local networks with an extremely
generic, essentially fully connected, connectivity pattern can be trained to very high
accuracies using this predictive coding algorithm which only requires Hebbian updates.
Chapter 3. Predictive Coding 138
3.5.3 Discussion
We have shown that it is possible to surmount three key biological implausibilities
of the predictive coding learning rules, and thus strengthen the case that predictive
coding may be implemented in cortical circuitry. In the weight transport and error
connections case, the solution has been to propose a separate set of weights which
are themselves learnable by a Hebbian rule. In the backwards nonlinearities case, it
sufﬁces simply to ignore the biologically implausible terms in the rule. Moreover,
we have shown that performance, at least under the hyperbolic tangent nonlinearity,
is still stable and roughly comparable with the baseline when all the relaxations are
combined together, thus resulting in an extremely local,straightforward, and biologically
plausible architecture. Overall, we believe these results show that predictive coding
offers a surprisingly robust model of learning and inference in the brain and that it
can survive often severe perturbations to its basic equations. Through this work we
have substantially diminished the constraints a neurophysiologically realistic process
theory of predictive coding must satisfy. In so doing, it is possible that predictive
coding may now ﬁt a greater part of neurophysiological data, while opening the way
to potentially constructing novel microcircuit designs which implement relaxed forms
of predictive coding. There may also be gains from applying these heuristics to other
biologically plausible approximations to backprop. We show in the ﬁnal chapter on
biologically plausible credit assignment in the brain, that many of these techniques also
work for other algorithms, thus hinting at potentially general properties of perturbational
robustness of these neurally inspired learning algorithms which may be of signiﬁcant
theoretical interest.
An additional theoretical note is the power of the assumption of variational optimality.
Like a Lagrangian in physics, the variational free energy F enables potentially complex
‘laws of motion’ to be derived through a simple mathematical apparatus and which
can be extended to lead to otherwise-difﬁcult insights. For instance, the learning rules
Chapter 3. Predictive Coding 139
for the error-weights ψ can be derived straightforwardly in the variational framework
as simple gradient descents on F given an augmented generative model containing
the ψ term. Regardless of one’s theoretical or ontological commitments to variational
inference in the brain, the mathematical reformulation of neural activity as encoding
solutions to a variational inference problem allows considerable modelling ﬂexibility
and mathematical insight through which results can be easily derived which would be
much harder to achieve through other means.
Having removed the symmetric backwards weights and the one-to-one error connectivity
scheme, we are left with an essentially bipartite graph. There are connections between
the value and error units of the same level, and the error units of one level and the
value units of the level above, but crucially there are no direct connections between
the value or error units of one layer and those of the layer above. Stepping out of
the predictive coding framework, we have effectively shown that a simple bipartite
connectivity structure and Hebbian learning rules sufﬁces to learn complex input-output
mappings, and may mathematically approximate backpropagation. This is a surprising
result given that previously Hebbian learning has not generally been thought to be
sufﬁcient to learn complex representations in the brain (Baldi & Sadowski, 2016). This
shows that perhaps it is possible for the brain to go further with clever connectivity
patterns and Hebbian learning than previously thought.
It is also important to note here that while we have resolved several biological implausi-
bilities of predictive coding, there are still several other difﬁculties that must be faced
before a direct implementation of predictive coding is possible given what we currently
know about neural circuitry. A key challenge is the simple problem of negative predic-
tion errors and activations – in the mathematical formalism, prediction errors and values
are real numbers which can be both positive and negative, and to any degree of accuracy.
In the brain, however, we assume that these numbers are represented by average ﬁring
rates, which cannot go negative, and additionally have a degree of accuracy constrained
Chapter 3. Predictive Coding 140
by the intrinsic noise levels of the brain and the integration windows over which post-
synaptic neurons can listen. While it seems likely that a lack of numerical accuracy
is not that signiﬁcant for neural networks in general, given recent results in machine
learning demonstrating that only 16 bit ﬂoats are necessary at most (Gupta, Agrawal,
Gopalakrishnan, & Narayanan, 2015), the issue of negative numbers is substantial since
negative prediction errors are absolutely necessary for the functioning of the algorithm.
One possibility is that the brain could maintain a high default ﬁring rate, and treat
deviations below this default as negative. However, the maintenance of a sufﬁciently
high default ﬁring rate would be energy inefﬁcient, and there is much evidence that
neurons primarily maintain low tonic ﬁring rates (Walsh et al., 2020). Another option
could be to utilize separate populations of ‘positive’ and ‘negative’ neurons, perhaps
excitatory and inhibitory neurons, however this would require a precise connectivity
scheme to integrate these two contributions together, which has largely not yet been
worked out in the context of predictive coding.
An additional limitation of our work is that we have only tested the performance of
the relaxations on relatively small networks and using the relatively simple MNIST
and Fashion-MNIST datasets which are simple enough that even fairly non-scalable
methods can work on them. A prime example of this is the study by Bartunov et al.
(2018) who showed that many of the methods in the literature for alternative biologically
plausible methods for credit assignment, although performing well on MNIST, generally
performed poorly on more challenging datasets such as CIFAR10, CIFAR100, and
ImageNet. A key task must be to investigate the scaling properties of these relaxations
to more challenging tasks and datasets, as well as different network architectures such
as convolutional neural networks. Some preliminary, but supportive results come from
Millidge, Tschantz, Seth, and Buckley (2020a) (discussed in chapter 6) where we show
that the learnable backwards weights and dropping the nonlinear derivatives do in fact
scale to larger scale CNN networks.
Chapter 3. Predictive Coding 141
3.6 Conclusion
In this chapter, we have studied the application of the free energy principle to perception
– speciﬁcally by investigating and proposing substantial extensions to the predictive
coding process theory (Friston, 2003, 2005, 2008a) as well as testing the performance
of large-scale implementations of the theory. Overall, we believe that our work in this
chapter has make substantial improvements to the theory and practice of predictive
coding.
Speciﬁcally, for the ﬁrst time, we have implemented and tested predictive coding
models on a larger scale than previously – and have compared them against machine
learning approaches on standard machine learning datasets such as MNIST. We have
demonstrated that predictive coding networks are able to successfully reconstruct digits
successfully, interpolate between them, and separate out different digit representations
in the learnt latent space despite being trained with an entirely unsupervised objective.
Moreover, we have demonstrated that predictive coding can achieve this in hierarchical
and dynamical setups with randomized initial weights which are then learned, in contrast
to prior work (Friston, 2005, 2008a; Friston et al., 2008) which primarily focus on the
inference capabilities of predictive coding and provide a-priori the correct generative
model. We also implemented and demonstrated that dynamical, and both hierarchical
and dynamical predictive coding models can function well on simple toy tasks and can
very quickly learn various challenging wave-forms.
Secondly, we have investigated the use of predictive coding algorithms for ﬁltering tasks
(as opposed to static inference). In ﬁltering, the aim is to infer an entire trajectory of
states given a trajectory of observations, instead of simply inferring a single hidden state
given a single observation. We make precise, for the ﬁrst time, the precise relationship
between Kalman ﬁltering – a ubiquitous algorithm in classical control and ﬁltering
theory – and predictive coding which is that the predictive coding dynamics can be
derived as a gradient descent on the Gaussian maximum-a-posteriori objective, which
Chapter 3. Predictive Coding 142
the Kalman ﬁlter equations can be derived as an analytical solution to. We then demon-
strate the successful ﬁltering capabilities of our predictive coding ﬁltering algorithm,
and demonstrate how the broader variational approach allows us to successfully also
learn the parameters of the generative model online for ﬁltering tasks – thus performing
double deconvolution where we infer both states and parameters simultaneously.
Finally, we have investigated and improve the biological plausibility of the predictive
coding process theory which, after all, is often explicitly proposed as a neuroscientiﬁc
theory of cortical function (Bastos et al., 2012; Friston, 2003). Here we focus on
and present solutions to three outstanding issues of biological implausibility with the
standard predictive coding dynamics. First, we address the weight transport problem –
the need to transmit activities backwards – by proposing a set of independent backwards
weights which are initialized randomly, and then can also be learnt with an independent
and biologically plausible Hebbian update rule. Secondly, we address the problem of
nonlinear derivatives by showing that in many cases these derivatives can be dropped
from the update rules with relatively small performance penalties, and thirdly, we
address the issue of needing precise one-to-one error to value neuron connectivity by
proposing instead fully distributed connectivity between error and value neurons, but
with an additional learnable weight matrix which can be additionally optimized with
another Hebbian rule. We show that these relaxations can substantially improve the
biological plausibility of the predictive coding algorithm while only causing relatively
small degradations of performance on machine learning benchmark classiﬁcation tasks.
Overall, therefore, we believe that in this chapter we have made signiﬁcant contri-
butions to the theory and practice of predictive coding. On the theoretical level, we
have demonstrated its relationship to Kalman ﬁltering, and we have addressed several
outstanding challenges of biological implausibility that the standard theory faces. On an
implementational and practical level, we have empirically investigated for the ﬁrst time
the performance of predictive coding networks within the machine learning paradigm
Chapter 3. Predictive Coding 143
on machine learning benchmark tasks, and especially in cases where the true generative
model is not provided to the network a-priori. Additionally, through our work, we have
substantially scaled up predictive coding approaches to handle signiﬁcantly larger and
more challenging tasks than previously, and have made these implementations available
to the community through a number of open-source software projects 9
In the next two chapters, we advance from the problem of perception, to consider
the problem of action selection through the lens of the free energy principle, and
its concomitant process theory active inference. In some ways this problem is more
challenging than pure perception, since it requires the modelling of entire trajectories of
observations, states, and actions, in order to make the best long term decisions which
will, over time, outperform locally greedy options. In the next Chapter (Chapter 4),
we focus primarily on scaling up existing active inference models using deep neural
networks to match the performance of state of the art reinforcement learning algorithms.
In the chapter after that (Chapter 5), we aim to provide a deep mathematical investigation
and, ultimately, insight into the nature of objective functionals which combine both
reward-seeking and information-seeking imperatives.
9See: https://github.com/BerenMillidge/PredictiveCodingBackprop,
https://github.com/BerenMillidge/RelaxedPredictiveCoding, https://github.com/BerenMillidge/NeuralKalmanFiltering
Chapter 4
Scaling Active Inference
4.1 Introduction
In this chapter, we consider the application of the free energy principle to action
selection, or control, problems. While in the previous chapter on perception, we focused
on the process theory of predictive coding, here we focus on the process theory of active
inference, and are especially inspired by the discrete state-space active inference theory
introduced in Chapter 2. Here, we aim to solve a key limitation of those methods – their
scalability. We propose to do so by parametrizing the key densities of the generative
model and recognition distribution by deep neural networks, and then utilizing the
tools of deep reinforcement learning to allow active inference agents to scale to levels
comparably achieved by contemporary machine learning.
This chapter comprises multiple sections. At the beginning, we give a detailed in-
troduction to reinforcement learning (Sutton & Barto, 2018), and especially deep
reinforcement learning, as well as the control of inference framework (Levine, 2018;
K. C. Rawlik, 2013) from reinforcement learning which also frames the control problem
as one of inference. Then we present two studies where we demonstrate that active
inference approaches can scale up to be comparable with contemporary deep reinforce-
144
Chapter 4. Scaling Active Inference 145
ment methods. We achieve this scaling by ﬁrst parametrizing the key distributions in
the active inference model by deep neural networks trained through gradient descent,
and secondly by approximating the (exponential time) computation of the path integral
of the expected free energy either with an amortized neural network prediction, or else
through monte-carlo trajectory sampling using a continuous action planning algorithm.
In doing so, we create algorithms that are comparable in scalability and performance
to current methods in deep reinforcement learning. Additionally, we demonstrate that
oftentimes these algorithms can outperform their reinforcement learning counterparts
due to the unique properties and insights active inference brings to the table.
In the second section of this chapter, we focus a little more abstractly in trying to
understand the difference between model-free and model-based reinforcement learn-
ing approaches in terms of inference, and determine that this difference is primarily
due to the difference between what we call iterative variational inference – where
the parameters of the variational distribution are directly optimized – and amortized
inference – where instead the parameters of a function which outputs the parameters
of the variational distribution are optimized. Given this distinction, we ﬁrst use it to
present a taxonomy of a wide range of current reinforcement learning algorithms using
a simple two-dimensional quadrant, and secondly, we derive novel algorithms which
emerge by combining both iterative and amortized inference together – an approach we
call hybrid inference – which results in powerful algorithms which combine the beneﬁts
of each approach, while ameliorating their respective weaknesses.
4.1.1 Reinforcement Learning
The reinforcement learning, or control, problem is one of the most fundamental prob-
lems in artiﬁcial intelligence and in engineering adaptive systems (Dayan & Hinton,
1997; Kaelbling et al., 1996; Sutton, 1990; Sutton, Barto, et al., 1998), and concerns the
computation of optimal action (Todorov, 2008; Wolpert, 1997). The problem is simple.
Chapter 4. Scaling Active Inference 146
We assume that there is some kind of agent in some kind of environment, and that the
agent can take actions which affect the environment (Sutton et al., 1998). Suppose that
the agent has some kind of notion of goals or desires that it wants to achieve – whether
these are encoded as a desire distribution, as an objective function, or as rewards given
by the environment. The control problem is to compute the optimal action schedule to
fulﬁll the agent’s goals. As might be expected, this question has huge applications and
implications for an extremely wide range of ﬁelds, from machine learning and artiﬁcial
intelligence (understanding how to make artiﬁcial agents act to achieve their goals)
(V . Mnih et al., 2013; Schrittwieser et al., 2019; Schulman, Levine, Abbeel, Jordan, &
Moritz, 2015; Silver et al., 2016; Sutton et al., 1998) to cognitive science and economics
(understanding how humans implement action strategies to achieve their goals) (Daw,
O’doherty, Dayan, Seymour, & Dolan, 2006; Dayan & Daw, 2008; Todorov, 2008;
Wolpert, 1997) to biology (how do all sorts of biological systems act adaptively) (Dayan,
2009; Krebs, Kacelnik, & Taylor, 1978; Mehlhorn et al., 2015; Pyke, 1984) to control
theory (how to design and program systems which can adaptively regulate and control
their environments) (Johnson & Moradi, 2005; Kalman et al., 1960; Kappen, 2005;
Kirk, 2004; Kwakernaak & Sivan, 1972; Sethi & Thompson, 2000).
While this provides an intuitive speciﬁcation of the control problem, to make real
progress we must make it precise mathematically. First, we assume that the environment
has states which we denote x and that the agent can emit actions a. Secondly, we
assume that there exists some reward function which emits rewards r dependent on
environmental states r = f (x). The only thing the agent has control over are its actions
a which can affect the environment to give it more rewards. We assume that the agent
optimizes over trajectories of states and actions going into the future, which we denote
as ˜x, ˜r, and ˜a. For the moment, to retain full generality, we remain indifferent to
whether the agent considers time continuous or discrete, so that ˜x = ∑t x(t) =
∫
dtx(t).
Finally, we assume that the environmental dynamics and the rewards granted can
both be stochastic and can thus be mathematically formalized in terms of probability
Chapter 4. Scaling Active Inference 147
distributions p(˜x|˜a) and p(˜r|˜x). A key advantage of this probabilistic formalism is that
it allows us to represent (and remain agnostic between) intrinsic stochasticity in the
environment, and the agent’s uncertainty about the environment. If the environment or
rewards are in fact deterministic and known, we can simply set the distributions to be
dirac deltas to recover a deterministic framework. Under this formalism, the objective
of the control problem is to maximize 1,
Lcontrol = argmax
a
∫
d ˜x p(˜r|˜x, ˜a)p(˜x|˜a)p( ˜a)
= argmax
a
Ep(˜x|˜a)p( ˜a)[ln p(˜r|˜x, ˜a)] (4.1)
Where we can safely take the log of the reward function since log is a monotonic
function and does not impact the optimum of the optimization process, but tends to
make things nicer numerically. Essentially, what this states is that the control objective is
simply to maximize the probability or amount of reward expected under the trajectory of
environment states given the agent’s trajectory of actions. From this, we can see that to
ﬁrst get a handle on the control problem, we need to understand ﬁrstly the environmental
dynamics p(˜x|˜a) and the reward or utility function p(˜r|˜x, ˜a). We typically represent
these dynamics as stochastic differential (or difference) equations depending on whether
time is discrete or continuous, as follows,
dx
dt = f (x1:t,a1:t,ω) (4.2)
for continuous time, where ω is some kind of noise or,
xt+1 = f (x1:T ,a1:T ,ω) (4.3)
for discrete time. One simplifying assumptions we often make is that the dynamics are
Markovian, meaning that the state at time t +1 can be computed solely in terms of the
1Here, following the convention in reinforcement learning and economics, we are optimistic and
we talk about reward (or equivalently utility) maximization. Control theory, on the other hand, takes
a more depressive interpretation and works in terms of minimizing costs. Mathematically, these two
formulations are completely equivalent.
Chapter 4. Scaling Active Inference 148
state at time t and the action at time t, thus that the dynamics become,
xt+1 = f (xt,at,ω) (4.4)
This approach simpliﬁes the analysis considerably. An additional assumption, which
is often made, is that the rewards depend only on the state and actions at the current
time p(˜r|˜x, ˜a) =Πt p(r(t)|s(t),a(t)). Under these assumptions the environment of the
control problem can be considered to be a Markov Decision Process (MDP). It is also
sometimes the case that we assume we do not know the true state of the environment,
but are only given access to partial observations ˜o which may not be Markov, even
though the hidden states are Markov. The observations are related to the states through
a likelihood mapping p( ˜o|˜x). This type of environment is called a Partially-Observed
Markov Decision Process (POMDP) (Kaelbling et al., 1996) and is substantially harder
to solve optimally than an MDP due to the need to correctly infer the hidden states ˜x
from the observations ˜o. Nevertheless, the POMDP model has a great deal of generality
since, as the state is hidden, it can be whatever is necessary to preserve Markovian
dynamics, thus enabling any non-Markovian environment to be written in terms of a
Markovian POMDP.
Early approaches to the control problem tried to use methods in variational calculus
to directly ﬁnd analytical solutions to the control problem. Such approaches yielded
success in some simple but important, cases, such as Markov linear Gaussian dynamics
and quadratic costs. These conditions correspond to dynamics which can be speciﬁed
as (assuming discrete time),
xt+1 = Axt +Bat +ω
rt = xT
t Qxt +aT
t Rat (4.5)
Where A, B, Q, and R, are known matrices and ω ∼N (0,I) is white Gaussian Wiener
noise (Wiener, 2019). In this case, an analytical solution exists in both continuous and
discrete time which gives rise to the linear quadratic regulator (Kalman, 1960; Kalman
Chapter 4. Scaling Active Inference 149
et al., 1960; Kirk, 2004), a centerpiece of modern control theory which is remarkably
effective for controlling even complex systems, and for which control solutions can
be computed very relatively cheaply and in real time. While the linear dynamics and
quadratic costs conditions are quite restrictive (especially the linear dynamics), the
linear quadratic regulator approach can be extended somewhat to nonlinear dynamics
by simply using a local linearity approximation at every timestep and applying model-
predictive control. This iterative LQR (W. Li & Todorov, 2004) algorithm, is quite
robust and can achieve signiﬁcant feats of nonlinear control, including use in controlling
industrial robotics (Feng, Whitman, Xinjilefu, & Atkeson, 2014). Other variational
approaches have also been applied and can be quite effective in many cases. For
instance, Pontryagin’s maximimum principle (Kirk, 2004; Kopp, 1962) or ‘bang-bang’
control can be applied productively to ﬁnd optimal policies in many settings. Recently,
there has been advances using path integral methods and the Feynman-Kac lemma to
ﬁnd control solutions for certain classes of nonlinear dynamics (Kappen, 2005; Kappen
et al., 2012; G. Williams, Wagener, et al., 2017).
Another approach to the control problem, which can work for arbitrary dynamics, is
to simply optimize the control function by gradient descent with respect to the actions.
Such an approach goes by the name of policy gradients, since given a policy function
a = fφ(s) parametrised by parameters φ, we can simply compute gradients of the
control problem loss ∂Lcontrol
∂φ and optimize the parameters by stochastic gradient descent.
The chief difﬁculty is to propagate gradients through the potentially nondifferentiable
and unknown expectation under the environmental dynamics Ep(˜x|˜a). Luckily, this is
achievable through the policy gradient theorem (Sutton et al., 1998; R. J. Williams &
Chapter 4. Scaling Active Inference 150
Zipser, 1989b)
∂Lcontrol
∂φ = ∂
∂φ
∫
d ˜x p(˜x, ˜a)ln p(˜r|˜x, ˜a)
=
∫
d ˜x ∂
∂φ p(˜x, ˜a)ln p(˜r|˜x, ˜a)
=
∫
d ˜x p(˜x, ˜a)∂ln p(˜x, ˜a)
∂φ ln p(˜r|˜x, ˜a)
= Ep(˜x, ˜a)[ln p(˜r|˜x, ˜a)∂ln p(˜x, ˜a)
∂φ ] (4.6)
Which allows an estimate of the gradient to be computed simply through averages of
environmental dynamics. Importantly, this approach does not require knowledge of
the true environmental dynamics at all (unlike classical control theory), since we only
require samples from the environment which can be obtained simply through interacting
with it. Intuitively, we can think of this theorem as saying that the gradient of the control
objective is simply the average gradient of the policy, weighted by the rewards received.
While this method works, the downside is that since the expectation is effectively
computed through Monte-Carlo sampling (and generally relatively few samples at that),
the gradient estimates generally have a very high variance, which makes learning trou-
blesome and slow. A number of baseline approaches have been invented to try to deal
with this problem (Sutton & Barto, 2018) and to make it more tractable. Nevertheless,
policy gradient approaches can be scaled up and applied successfully in challenging
tasks, especially continuous control tasks, by parametrizing the policy fφ(x) with a deep
neural network and directly applying the policy gradient theorem with some additional
tricks (Schulman et al., 2015; Schulman, Wolski, Dhariwal, Radford, & Klimov, 2017).
Another approach to the control problem in Markov conditions (but arbitrary dynamics
as long as they are Markov) is to use a recursive solution method pioneered by Richard
Bellman (Bellman, 1952). He noticed that optimal solutions to the control problem
satisfy an interesting recursive relationship – that the optimal path to the goal at a
timestep t, must include the optimal path to the goal at a later timestep t + 1. This
property allows you to build up a backwards recursion where you start at the goal at
Chapter 4. Scaling Active Inference 151
the end and then work backwards, constructing the optimal path in a piecewise fashion
from the previous optimal path. The key mathematical quantity, here, is the cost-to-go,
which intuitively is the cost of the optimal trajectory from the current position to the
goal. In modern reinforcement learning parlance, this cost-to-go is called the optimal
value function of a state, and is conversely the expected reward which would be attained
from a given state assuming the optimal policy is followed. Written out mathematically,
this approach gives rise to the recursive Bellman equation,
V ∗(xt) =r(xt)+ Ep(xt+1|at )[V ∗(xt+1)] (4.7)
which simply states that the optimal value function of a current state, is the reward
of the current state plus the maximum average value function of the next state. In
effect, if iterated backwards from the end (where the optimal value function is simply
r(xT ) and assumed known) this recursion allows you to build up the optimal path by
working backwards. If all environmental states and actions are known and ﬁnite, then
this algorithm can be run explicitly to compute the optimal solution in polynomial
time (as opposed to the exponential time approach of just trying all possible paths and
picking the best). This is thus a dynamic programming algorithm which is equivalent to
other standard dynamic programming algorithms in computer science such as Dijkstra’s
algorithm for the shortest paths.
Importantly, the Bellman recursion holds not just for the optimal policy and value
function, but indeed for any policy and value function, this allows solution methods
using this recursion to apply even when the state and action space is too large to
represent explicitly. In this case, we can write the Bellman recursion as,
V (xt) =r(xt)+ Ep(xt+1|at ,xt s)[V (xt+1)] (4.8)
and, without working backwards from the end, we can simply estimate the value
functions Vπ(x) of a given policy π by moving around in our environment, computing
Chapter 4. Scaling Active Inference 152
rewards and storing the state and the next state, and applying Equation 4.8. If we do this
sufﬁciently for a given policy, we can then form an estimate of the global value function
Vπ(x) for all x. With this value function, we can then improve the policy, by simply
deﬁning a new policy that takes π = max(r(xt,at)+ Ep(xt+1|xt ,at )[V (xt+1)]. Somewhat
surprisingly, it has been proven that if the estimated value function is accurate, then
the new policy deﬁned in such a manner is necessarily the same or better (in terms
of average reward obtained from the MDP) than the previous policy, and that if we
iterate the process of sampling new states to estimate the value function, and then
improving the policy, then we will converge upon the optimal policyπ∗(Sutton & Barto,
2018). This approach, called policy iteration, is the cornerstone of classic reinforcement
learning algorithms such as temporal difference learning (Sutton, 1988), and SARSA
(Singh & Sutton, 1996; Sutton, 1996).
A closely related approach is called Q-learning (Watkins & Dayan, 1992), which instead
of using the value function, instead maintains an estimate of the state-action value
function Q(x,a), which is called the Q-function for historical reasons. The Q function
satisﬁes a similar recursive relationship to the value function,
Q (xt,at) =r(xt,at)+ argmax
at
Ep(xt+1,at+1|at ,xt )[Q (xt+1,at+1)]
Given an estimate of the Q function, the policy improvement step is simple. πt+1 =
argmax
a
Q(s,a). The Q-learning algorithm combines both policy evaluation (estimating
the value or Q function) and policy improvement into one continuous algorithm, which
can be simply deﬁned as,
a(xt) =maxat Q(xt,at)
Q(xt,at) =r(xt,at)+ maxat+1 Ep(xt+1|xt ,at )[Q (xt+1,at+1)] (4.9)
The Q-learning algorithm is extremely popular and effective and has been central to
many major successes of reinforcement learning, from playing backgammon (Tesauro,
1994) to Atari (V . Mnih et al., 2013; Schrittwieser et al., 2019). The Q function and
Chapter 4. Scaling Active Inference 153
value function can be related straightforwardly by V (x) =
∫
daQ (x,a) – i.e. the value
function is simply the Q function averaged over all actions. Another approach is to
write everything instead in terms of an advantage function A(x,a) =Q (x,a)−V (x)
which simply subtracts the action-independent value function baseline from the Q-
value, effectively normalizing it, since the only important thing from the perspective
of Q learning is the relative values of each action. This approach reduces the gradient
variance when trying to estimate the Q function and often makes the resulting algorithms
more stable (Hessel et al., 2018).
In classical reinforcement learning we typically represent the Q and value function
explicitly in discrete-state and discrete-action environments. For instance, with discrete
states, the value function V (x) would simply be a vector of length S where S is the
number of distinct states. The Q function Q (x,a) is simply an S×A matrix where A is
the action dimension.
Another useful representation is the successor representation (Dayan & Hinton, 1997).
This approach rewrites the value function in terms of the instantaneous reward r(x)
and a successor matrix M (x,x′) which is a S×S matrix which represents the average
transition probabilities for a given policy from state x to state x′. This matrix M can
be thought of as the stationary transition distribution of the Markov chain for a given
policy. The value function can be decomposed into,
V (x) =M (x,x′)r(x) (4.10)
The successor representation, crucially, by separating out the policy-dependent compo-
nent (M) from the reward r allows for computation of different value functions for a
given policy rapidly under different reward functions. Thus this representation allows
for very ﬂexible changes of behaviour given a change in reward. Of course the optimal
policy also changes under a change of reward function, and this change cannot be
straightforwardly determined solely by the successor representation.
Chapter 4. Scaling Active Inference 154
4.1.2 Deep Reinforcement Learning
While classical reinforcement learning approaches typically represent the value or Q
functions explicitly as vectors and matrices, such methods only work for relatively
small and discrete state and action spaces, and cannot easily scale to the extremely large
state and action spaces required for playing complex games as well as for complex con-
tinuously valued action spaces such as in robotics, where simply discretizing the space
with a sufﬁciently ﬁne grid will simply result in too many states to handle. Therefore,
to maintain scalability, instead of explicitly representing the state and value functions, it
becomes necessary to approximate them with powerful function approximators. While
other approaches using linear features (Baird, 1995; G. J. Gordon, 1995), or nonlinear
basis function kernels (Doya, 2000) are possible, recent systems have overwhelmingly
used deep neural networks to approximate the value or Q functions directly. To make
this explicit, instead of representing the value function V [x] as a vector, we instead
represent it as a function Vψ(x) which takes a state x and maps it to a scalar value. This
function is implemented by a deep neural network with parameters ψ. We can then
learn these parameters using stochastic gradient descent on a loss function which is a
modiﬁed version of the Bellman recursion,
LValuenet (x) = (Vψ(x)−r(x)−maxaEp(x′|x,a)[Vψ(x′)]))2 (4.11)
which is the squared residual between the value predicted for a state x by the value
network, and the value predicted by the Bellman recurrence relation. By using this
approach, therefore, we utilize the intrinsic generalization capabilities of deep neural
networks to allow us to estimate value or Q functions for an otherwise intractably
large space. This approach can be straightforwardly extended to Q-learning and other
Bellman based methods. Similarly, applying policy gradients with deep neural networks
is even simpler – we simply parametrise the policy qψ(a|s) with a deep neural network
with parameters ψ and train it directly using stochastic gradient descent on the policy
gradient objective (Schulman et al., 2015, 2017).
Chapter 4. Scaling Active Inference 155
To get this approach working well in practice, however, requires quite a number of
tricks. For instance, the neural networks cannot be trained simply through continuous
interaction with the environment, as this gives rise to correlated data which leads to
overﬁtting and catastrophic forgetting within the value network (V . Mnih et al., 2013).
Thus, instead, it is necessary to make the data fed into the network as i.i.d as possible
by using a memory replay buffer, which stores all experience the agent has encountered
over its lifetime and then replays it at random to be optimized according to Equation
4.8 (V . Mnih et al., 2013). Similarly, note that in the value network update equation
(Equation 4.11), the parameters of the value network appears twice – once computing
the value of x and again computing the values of x′. It has been found empirically, that
this leads to instabilities in the optimization process which often destroy learning, since
the optimization process is effectively chasing a set of moving targets. To resolve this
problem, the value estimates of x′are often computed using a ‘frozen’ value network
which is not optimized directly, but is a copy of the value network from some number of
iterations past. The frozen network is then updated to match the current value network
every given number of iterations (V . Mnih et al., 2015). Another issue is that the value
estimates are often skewed extremely positive due to the max operator in the objective
interacting inaccurate value function estimates. This can be ameliorated empirically by
simply training two (or many) value networks in parallel, and then choosing the smallest
value estimates out of all of them, a technique known as dueling value networks (Wang
et al., 2016). For a thorough review of tricks and tips for training deep reinforcement
learning agents, we suggest Fujimoto, van Hoof, and Meger (2018); Hessel et al. (2018).
Nevertheless, once all of these instabilities have been addressed, the result is an ex-
tremely powerful and general learning technique which has been demonstrated empiri-
cally to scale up to solve very challenging tasks such as Atari games (A. Mnih & Gregor,
2014; V . Mnih et al., 2015) and Go (Silver et al., 2016, 2017), Starcraft II (Vinyals et al.,
2019), and ultimately very challenging tasks in robotics (Chua, Calandra, McAllister,
& Levine, 2018; Nagabandi, Konoglie, Levine, & Kumar, 2019; Watter, Springenberg,
Chapter 4. Scaling Active Inference 156
Boedecker, & Riedmiller, 2015; G. Williams, Aldrich, & Theodorou, 2017).
4.1.3 Model-free vs Model-based
It is important to note that all the methods we have discussed so far require samples
from the true environmental dynamics to approximate the expectation Ep(˜x, ˜a) from
the ultimate loss function (Equation 4.1. While these samples are easy to acquire in
the case where interacting with the environment is cheap and easy, such as when the
environment is a simulation such as a game or an OpenAI gym environment (Brockman
et al., 2016), in many real world tasks this is not the case. For instance, in robotics,
interacting with the real environment is often slow (the real robot has to actually move or
do things in physical space) and costly (this movement requires power and also induces
wear and tear on the robot. In extreme situations, bad policies may actually result in
the robot damaging itself). In this case, it is often better if we can somehow eschew
interacting with the real environment in favour of a model of the real environment.
Having a ‘world model’ (Ha & Schmidhuber, 2018) of the environment allows the agent
to plan and test different potential courses of action without having to sustain costly and
slow interactions with the real world. The utility of models does not just extend to the
environmental dynamics. It is often the case that the actual reward function of the agent
is unknown. This is not generally true in reinforcement learning and control theory,
which typically have well speciﬁed rewards, but is often true in the case of biological
organisms encountering novel contingencies, where it is not necessarily known a-priori
if a situation is good or bad. Thus, we can also learn a model of the reward function as
well.
Mathematically, we can formalize this property of having a model of the transition
dynamics or reward functions by postulating additional probability densities which
the agent possesses qφ(˜x|˜a) and qθ(˜r|˜x, ˜a) which represent the agent’s model of the
true environmental dynamics p(˜x|˜a) and true reward function p(˜r|˜x, ˜a). Then, using
Chapter 4. Scaling Active Inference 157
importance sampling, we can introduce these models into the previous loss function,
Lcontrol = argmax
a
Ep(˜x|˜a)p( ˜a)[ln p(˜r|˜x, ˜a)]
= argmax
a,φ,θ
Ep(˜x|˜a)p( ˜a)
q(˜x|˜a)
q(˜x|˜a)[ln p(˜r|˜x, ˜a)q(˜r|˜x, ˜a)
q(˜r|˜x, ˜a)]
= argmax
a,φ,θ
Eq(˜x|˜a)p( ˜a)
p(˜x|˜a)
q(˜x|˜a)[lnq(˜r|˜x, ˜a) p(˜r|˜x, ˜a)
q(˜r|˜x, ˜a)]
= argmax
a,φ,θ
Eq(˜x|˜a)p( ˜a) ln p(˜x|˜a)
q(˜x|˜a)[lnq(˜r|˜x, ˜a) p(˜r|˜x, ˜a)
q(˜r|˜x, ˜a)]
= argmax
a,φ,θ
Eq(˜x|˜a)p( ˜a)[lnq(˜r|˜x, ˜a)]  
Reward Maximization
−KL[q(˜x|˜a)||p(˜x|˜a)]  
System Identiﬁcation
−Eq(˜x|˜a)p( ˜a)
[q(˜r|˜x, ˜a)
p(˜r|˜x, ˜a)  
]
Reward Model Identiﬁcation
(4.12)
Here we see that the objective can be partitioned into three terms. The ﬁrst, reward
maximization, is equivalent to the original control objective except it utilizes the
model environmental dynamics and reward function instead of the true environmental
dynamics and reward function. The second term, ‘system identiﬁcation’, encodes
the KL divergence between the true and modelled environmental dynamics, and is
minimized with respect to the parameters of the model of the dynamics. Optimizing
this term encourages the agent to learn an accurate dynamics model of the world. The
third term, ‘reward model identiﬁcation’ encourages the reward model to match the true
reward function and optimizing this term with respect to the parameters of the reward
model minimizes the difference between the model reward distribution and the true
reward distribution.
Interestingly, mathematically, the minimization of all of the three parameters is over
all three terms. This leads to, for instance, the parameters of the reward and dynamics
model to be optimized over the reward maximization term, which effectively encourages
the dynamics and reward models to try to learn positively biased dynamics and reward
models which are encouraged to give out larger rewards. Conversely, the divergence
Chapter 4. Scaling Active Inference 158
terms between true and modelled environmental dynamics and reward function are also
being optimized with respect to action, so the optimisation process also encourages
action to make the true dynamics and true reward function correspond more closely
to the modelled ones. These interactions between the optimizations of the different
terms often have strange and deleterious effects on agent behaviour, especially learning
the dynamics to maximize the reward, which can give agents a highly dysfunctional
‘optimism bias’ (Levine, 2018). As such, in practice, this optimization is often split into
three separate and independent optimizations for each set of parameters respectively,
argmax
a
Eq(˜x|˜a)p( ˜a)[lnq(˜r|˜x, ˜a) (4.13)
argmin
φ
KL[qφ(˜x|˜a)||p(˜x|˜a)]
argmin
θ
Eq(˜x|˜a)p( ˜a)
q(˜r|˜x, ˜a;θ)
p(˜r|˜x, ˜a) (4.14)
These three optimization processes correspond to maximizing the reward (standard
reinforcement learning), learning the environmental dynamics model, and learning the
reward function, respectively. In control theory, the process of learning a dynamics
model is called system identiﬁcation.
Given good reward and dynamics models, it is then possible to utilize standard model-
free reinforcement learning approaches to estimate Q and value functions, or estimate
policy gradients directly through simulated samples of the dynamics model. With these
it is then possible to learn policies in the usual way without ever having to interact with
the environment (except to learn the models in the ﬁrst place). This approach, ﬁrst
introduced in the Dyna architecture (Sutton, 1991) has been studied in the literature for
a long time, and is generally more sample efﬁcient than pure model-free reinforcement
learning which learns directly from environmental transitions, as learning dynamics
models of the world is typically faster than learning reward models, since the prediction
errors in the state of the world is a richer informational signal than the reward prediction
error since the reward is usually scalar while the world-state is typically very high
Chapter 4. Scaling Active Inference 159
dimensional.
Another approach, once you have a model is to skip the model-free methods and
move directly to planning using the model. In the simplest case, this can be done by
sampling different action trajectories, simulating their consequences and rewards using
the dynamics and reward models, and then simply choosing the action trajectory with
the best estimated rewards. More advanced methods include the cross-entropy method,
which tries to ﬁt a probabilistic (Gaussian) action distribution to maximize rewards, and
the path-integral method, which ﬁts a Boltzmann distribution of action trajectories over
rewards (Kappen et al., 2012; K. C. Rawlik, 2013; J. Theodorou Evangelosnd Buchli &
Schaal, 2010b; D. Williams, 2018; G. Williams, Aldrich, & Theodorou, 2017). This kind
of model-based planning is often combined with Model-Predictive-Control, which is
simply where you re-plan fully at every time-step, and has been used to reach state of the
art performance on a wide variety of reinforcement learning (Nagabandi et al., 2019) and
robotics tasks (G. Williams, Drews, Goldfain, Rehg, & Theodorou, 2016; G. Williams,
Wagener, et al., 2017), all while requiring substantially fewer environmental interactions
than model-free reinforcement learning approaches.
4.1.4 Exploration and Exploitation
An interesting question that arises in the control problem, wherever there are any
unknowns, either of the optimal value function and policy or the environmental dynam-
ics and reward function, is the exploration-exploitation tradeoff (Berger-Tal, Nathan,
Meron, & Saltz, 2014; Cohen, McClure, & Yu, 2007; Dayan & Daw, 2008; Kaelbling,
Littman, & Cassandra, 1998; Mobbs, Trimmer, Blumstein, & Dayan, 2018; Sutton et
al., 1998). This trade-off arises because in order to obtain a more accurate estimate, it
is usually necessary to explore new regions of the state-space away from the locally
optimal location. However, by ignoring the locally optimum course of action, you incur
an opportunity cost equal to the difference between the (usually worse) reward from
Chapter 4. Scaling Active Inference 160
the exploring compared to the local optimum. Conversely, by only ever exploiting the
local optimum and never exploring, if there are actually better optima out there, which
you have simply not found, you incur a constant opportunity cost of the distance to
the true optimum every single time you exploit your local optimum. Thus, to ﬁnd the
truly optimal behaviours, it is necessary to explore widely, and not just be sucked into
whatever the closest local optima you ﬁnd. However, exploration has an intrinsic cost
associated with it, since assuming you have a halfway decent local optimum, almost
everything you explore will be worse than that, and thus exploration must be kept to a
minimum to maximize return, even in the long run.
The exploration-exploitation tradeoff is also central to the behaviour and performance
of reinforcement learning agents, especially deep reinforcement learning agents which
cannot explicitly represent every contingency in the state-space. Empirically, it has
been found that except in extremely simple tasks, or where the reward function is
a smooth gradient to the global optimum, some forms of exploration are necessary
to achieve good performance with deep reinforcement learning techniques. A large
number of heuristic exploration techniques have been developed in the literature which
work well for many tasks. Perhaps the simplest of these is the ε-greedy approach
(Sutton et al., 1998), which simply takes the greedy action (1 −ε) amount of the
time, and takes a random action ε percent of the time, where ε is a small number
– typically about 0.02 −0.05. This method essentially bakes in a certain degree of
random exploration into the method so that it will (eventually) explore all contingencies
purely due to its random actions. In many control tasks this method generates sufﬁcient
exploration to enable good performance. More sophisticated variants anneal the value
of ε over time; working on the idea that at the beginning when little is known you
should explore more, and then later when you already have a pretty good policy you
should explore less. Another approach in algorithms like Q learning is that instead
of simply taking the maximum value, take actions with a probability equal to their
softmaxed Q-values – q(a|x) = βexp(−Q (a|x))
∑a exp(−Q (a|x)) where β is a parameter which controls
Chapter 4. Scaling Active Inference 161
the spread or entropy of this distribution. When β is large, then the distribution is highly
peaked and tends towards the max. When β is small, then the distribution tends towards
a uniform over all action possibilities. This method is called Boltzmann exploration
(Cesa-Bianchi, Gentile, Lugosi, & Neu, 2017), since the action distribution is equivalent
to the Boltzmann distribution of statistical mechanics with the β parameter functioning
as an inverse temperature.
Another approach, which we will explore in detail in the next section, is to add an
entropy maximization term to the objective (Levine, 2018). Thus, instead of simply
maximizing the reward, the goal is to maximize the reward while keeping the entropy
of the policy as great as possible, and thus making action as random as possible 2 While
still a random kind of exploration, this performs better than ε greedy approaches since it
explicitly trades off randomness and reward maximization in the objective, rather than
as an ε parameter that needs to be tuned by hand.
While all these exploration methods work well in practice in many benchmark en-
vironments for deep reinforcement learning, they all fundamentally utilize random
exploration – i.e. actions are selected randomly in order to drive exploration. It is
important to note, however, that this strategy is necessarily very inefﬁcient, since even
with purely random actions, a random walk will explore slowly. Indeed, these methods
tend to perform rather poorly in more challenging large environments where all useful
contingencies cannot realistically be explored with a random walk in a reasonable
amount of time, and tend to perform especially poorly in sparse reward environments,
where rewards are hard to obtain and often require a pretty good policy to even get any
reward at all (Tschantz, Millidge, et al., 2020b). A good example of such an environ-
ment is many games, where you are only rewarded a 1 if you win the game. However, to
2This idea is similar to, but distinct from, ideas such as upper-conﬁdence-bound sampling which
assign optimism bonuses to states in proportion to the inverse degree to which they have been sampled.
The key difference is that maximum entropy approaches provide bonuses to actions based on the overall
entropy of the action distribution while UCB algorithms provide bonuses to states based on their epistemic
uncertainty.
Chapter 4. Scaling Active Inference 162
even win any games at all requires some skill at playing 3. To address the shortcomings
of random exploration, a signiﬁcant amount of work has been done on directed, or
information-seeking, exploration. Here, exploratory actions are not completely random
but instead directed at some exploratory goal – usually to accumulate information or to
resolve uncertainty about the world. This makes sense as in a stationary environment
there is little point in repeatedly exploring bad options which are known to be bad. The
key is to explore where there is remaining resolvable uncertainty.
A number of different objectives, or ‘intrinsic measures’ (Oudeyer & Kaplan, 2009)
have been proposed to achieve this. These include prediction error minimization (Pathak,
Agrawal, Efros, & Darrell, 2017), ensemble divergence (Chua et al., 2018), explicit
information gain (Shyam, Ja´skowski, & Gomez, 2019; Sun, Gomez, & Schmidhuber,
2011; Tschantz, Millidge, et al., 2020b), and empowerment (Klyubin, Polani, & Ne-
haniv, 2005). Typically such approaches postulate a separate ‘exploration objective’
and then either operate in two phases whereby ﬁrst the model optimizes the exploration
objective, and then it switches to optimizing the greedy objective (Shyam et al., 2019),
or alternatively, the exploration and greedy objectives are added together to form a
uniﬁed objective function which is minimized throughout (Tschantz, Millidge, et al.,
2020b). Such approaches therefore encourage the agent to seek a balance between its
exploratory and reward-maximization imperatives. This has the theoretical advantage of
encouraging the agent to only explore regions of the state-space which combine both a
high expected reward and much resolvable uncertainty, as opposed to simply resolving
uncertainty for its own sake. In the literature, most of these exploratory objectives are
simply postulated and argued for on intuitive grounds and then empirically compared.
3This is often addressed in practice by using reward shaping, where typically either the agent designer
or the environment designer will create a ‘proxy’ reward function for the real one which is less sparse.
For instance, in a game like chess, instead of simply rewarding winning or losing the game, the agent
might get rewards for taking pieces, or gaining positional advantage. While this approach works very
well in practice, it requires human intervention for every task the agent tries to accomplish, and is often
tricky to design a proxy reward which successfully leads to the correct ultimate behaviour. Ultimately, we
want agents to be able to interact with the world fully autonomously, which means that they should not
need special human-designed reward functions to handle any new situation, and so we do not consider
reward shaping further
Chapter 4. Scaling Active Inference 163
Figure 4.1: Graphical model for control as inference, with optimality variables Ω. Other
than the optimality variables, the graphical model takes the form of a Markov Decision
Process with actions a and states s. The state of a speciﬁc timestep depends on the
action and state of the last time-step. By writing out an explicit graphical model like this,
we can apply a whole ﬁeld’s worth of inference algorithms on graphical models like this
to solve control problems.
However, with the exception of the entropy maximization discussed above, which we
shall see arises from explicitly considering control as a variational inference problem,
the mathematically principled origins of these additional exploratory terms, especially
information-gain and empowerment terms remains mysterious. Chapter 5 is dedicated
to deriving the mathematical basis for such objectives.
4.1.5 Control as Inference
Since we have been so interested in understanding brain function through the lens of
Bayesian (variational) inference in this thesis, a natural question arises as to whether
the control problem as discussed previously can be cast in such an inference frame-
work. It turns out that this is indeed the case, and is quite straightforward to achieve.
Starting from (Attias, 2003) and then developed by (Kappen et al., 2012; Levine, 2018;
K. C. Rawlik, 2013; J. Theodorou Evangelosnd Buchli & Schaal, 2010a; Todorov, 2008;
Toussaint & Storkey, 2006), a small line of the literature has worked on investigating and
developing the close connections between the control problem and Bayesian inference.
While the control objective (Equation 4.1 is a probabilistic objective, it is not yet an
Chapter 4. Scaling Active Inference 164
inference objective. There is nothing there to be inferred. The key step is to deﬁne
dummy variables Ω1:T which are binary random variables which simply whether a
given trajectory timestep is optimal or not. Ωt = 1 if the timestep is optimal and
Ωt = 0 if it is not optimal. Given this dummy variable, the task of inferring the
optimal policy can be written simply as ﬁnding the distribution p( ˜a, ˜x|˜Ω = 1). To begin
inferring this distribution, it is ﬁrst necessary to make one more assumption about the
dummy variables Ω, in order to operationalize the notion of optimality. We deﬁne
p(Ωt = 1|at,xt) ∝ exp(−r(xt,at)) such that the probability of optimality is proportional
to the exponentiated reward. Intuitively, this can be seen as a mathematical trick
allowing the ‘log-likelihood of optimality’ to be equal to the reward, thus allowing us to
cast reward maximization as a process of maximum likelihood estimation.
One way we can ﬁnd the crucial distribution is simply to directly compute it via Bayes
rule. First, we write out Bayes rule explicitly,
p(at|xt, ˜Ω) =p( ˜Ω,xt,at)
p( ˜Ω,xt)
= p( ˜Ω|xt,at)p(at|xt)p(xt)
p( ˜Ω|xt)p(xt)
= p( ˜Ω|xt,at)p(at|xt)
p( ˜Ω|xt)
≈p( ˜Ω|xt,at)
p( ˜Ω|xt) (4.15)
Where, in the ﬁnal line, we assume that the action prior p(at|xt) is uniform. Now, look-
ing at the two terms we have left, we can intuitively think of the numeratorp( ˜Ω|xt,at) as
representing the probability of optimality of all future states, given the current state and
action. However, we have another term for this – the cost-to-go, or the Q-function. In
effect, we obtain the Bellman recursive relationship directly from Bayes rule. Secondly,
the denominator p( ˜Ω|xt) =
∫
dat p( ˜Ω|xt,at) clearly corresponds to the value function.
Chapter 4. Scaling Active Inference 165
We can from this directly derive recursive relationships among these terms,
p(Ωt:T |xt,at) =
∫
dxt+1dat+1 p(Ωt+1:T |xt+1,at+1)p(xt+1,at+1|xt,at)p(Ωt|xt,at)
(4.16)
We can thus see, that due to our deﬁnition of optimality thatp(Ωt|xt,at) =exp(−r(at,xt),
that to obtain the correspondence to the value and Q function requires the log of the
optimality probability. We thus have,
QCAI = ln p(Ωt:T |xt,at)
VCAI = ln p(Ωt:T |xt) (4.17)
Where, by taking the log of the integral and exponential, we effectively have the log-
softmax function instead of the max in the Bellman equations. This corresponds to a
‘soft’ maximum instead of the hard maximum used in the traditional Bellman recursion.
However, other than that, our new deﬁnitions of the value and Q function satisfy the
standard Bellman recursive relationship, and as such can be used to derive all the
traditional reinforcement learning algorithms such as Q-learning, temporal difference
learning, and SARSA (Sutton, 1996).
Another approach is to use variational inference and attempt to approximate the true
posterior distribution given optimality p( ˜a, ˜x|˜Ω) with a variational distribution q(˜x, ˜a).
To make this approximation accurate, we thus wish to minimize the divergence between
the two distribution.
LCAI = DKL[q(˜x, ˜a)||p(˜x, ˜a|˜Ω)]
= DKL[q(˜x, ˜a)||p(˜x, ˜a, ˜Ω)
˜Ω ]
= DKL[q(˜x, ˜a)||p(˜x, ˜a, ˜Ω)]  
ELBO
+ln ˜Ω (4.18)
Where we only need to optimize the Evidence Lower Bound (ELBO) term since ln ˜Ω is
constant with respect to the variational density q(˜x, ˜a). Crucially, we can then split up
Chapter 4. Scaling Active Inference 166
the ELBO term as follows,
LCAI = DKL[q(˜x, ˜a)||p(˜x, ˜a, ˜Ω)]
= DKL[q( ˜a|˜x)q(˜x)||p( ˜Ω|˜x, ˜a)p( ˜a|˜x)p(˜x)]
= Eq(˜x, ˜a)[ln p( ˜Ω|˜x, ˜a)]  
Reward Maximization
+DKL[q( ˜a|˜x)||p( ˜a|˜x)]  
Action Divergence
+ DKL[q(˜x)||p(˜x)]  
Dynamics Divergence
(4.19)
If we then assume that the variational dynamics q(˜x) are equal to the true environmental
dynamics p(˜x) (the agent cannot change the dynamics except through action) then the
action divergence term disappears. Additionally, if we use the fact, deﬁned earlier, that
p( ˜Ω|˜x, ˜a) =∏t exp(−r(at,xt)), then we obtain an objective which looks substantially
more similar to traditional reinforcement learning objectives except for an additional
action divergence term between the variational action distribution (the policy) and a
prior action distribution.
LCAI = Eq( ˜a|˜a)p(˜x)[
T
∏
t
r(xt,at)]
  
Reward Maximization
+DKL[q( ˜a|˜x)||p( ˜a|˜x)]  
Action Divergence
(4.20)
If we further assume a uniform action prior, then the control as inference objective
reduces to,
LCAI = Eq( ˜a|˜a)p(˜x)[
T
∏
t
r(xt,at)]−H[q( ˜a|˜x)]
= Eq( ˜a|˜a)p(˜x)[
T
∏
t
r(xt,at)−lnq(at|xt)] (4.21)
Which is simply reward maximization while also simultaneously maximizing the en-
tropy of the policy q( ˜a|˜x). This objective has been utilized in a number of recent works
(Abdolmaleki et al., 2018; Haarnoja, Tang, Abbeel, & Levine, 2017; Haarnoja, Zhou,
Abbeel, & Levine, 2018; K. Rawlik, Toussaint, & Vijayakumar, 2013), and forms the
basis of the soft-actor critic architecture (Haarnoja, Zhou, Abbeel, & Levine, 2018),
which simply optimizes a relatively standard actor-critic architecture on this objective.
It has been found to reach state-of-the-art performance for model-free reinforcement
Chapter 4. Scaling Active Inference 167
learning on a wide range of challenging continuous control tasks (Haarnoja, Zhou,
Hartikainen, et al., 2018; Hessel et al., 2018). Moreover, the simplicity and robustness
of this algorithm allow it to serve as an inﬂuential benchmark for the ﬁeld. This CAI
objective can be straightforwardly optimized by taking gradients of LCAI against what-
ever parameters there are. For instance, optimizing the control-as-inference objective
is identical to standard policy gradients except that each reward the agent receives
has lnq(at|xt) subtracted from it. This differentiates control as inference from previ-
ous works (O’Reilly, Wyatte, & Rohrlich, 2017), which heuristically used an entropy
regulariser to prevent policy collapse, but computed the entropy directly outside of
the expression for the reward. The control-as-inference objective is both simpler to
implement and more robust than this method.
Importantly, the control as inference objective, as it is a variational bound, can be
derived directly from, and as a lower bound on the marginal likelihood of optimality
ln p( ˜Ω). The derivation is straightforward and goes as follows,
ln p( ˜Ω) =ln
∫
p( ˜Ω, ˜x, ˜a) q( ˜a|˜x)
q( ˜a, ˜x)
≤Eq( ˜a, ˜x)[ p( ˜Ω, ˜x, ˜a)
q( ˜a, ˜x) ]
≤Eq( ˜a|˜x)p(˜x)[ p( ˜Ω|˜x, ˜a)p( ˜a|˜x)
q( ˜a|˜x) ]
≤Eq( ˜a|˜x)p(˜x)[
T
∏
t
r(xt,at)]−DKL[q(˜x|˜a)||p( ˜a|˜x)]
≤LCAI (4.22)
Under the assumption that the variational and generative dynamics are the same. Over-
all, the control as inference approach demonstrates that it is possible, even relatively
straightforward, to derive a range of reinforcement learning algorithms from a vari-
ational approach on an MDP graphical model augmented with additional optimality
variables. Doing so results in the standard reward maximization objective plus a reg-
Chapter 4. Scaling Active Inference 168
ularisation term which tries to keep the learned policy q( ˜a|˜x) as close as possible to
some action prior p( ˜a|˜x). If the action prior is set to be uniform, then this regularising
KL divergence simply reduces to the maximization of the entropy of the action policy.
This action entropy maximization term functions as a powerful regulariser and implicit
exploratory drive, which aims to keep the policy as random as possible while still
maintaining performance. This term is especially powerful and important for preventing
policy collapse, a well-known phenomenon in policy gradient and actor-critic methods
(Fujimoto et al., 2018), in which the probabilistic policy of an agent typically collapses
to some deterministic policy which may not even be very good. Once it is in this state,
it is very difﬁcult for the agent to continue to explore to ﬁnd better policies, since it
has minimal probability of taking other actions. The theoretical beneﬁts of this action
entropy term have been demonstrated empirically in the literature, where ‘soft’ control
as inference approaches have generally shown to outperform classical reinforcement
approaches as well as being more stable and easy to train. However, it is important to
note that although the action entropy term is effective at maintaining exploration, it only
encourages random exploration, or random walk behaviour in action-space. To solve
sparse reward tasks in a reasonable amount of time, it is possible that a more intelligent,
directed exploration strategy is needed, which focuses explicitly on minimizing resolv-
able uncertainty about the world. Such information-seeking exploration objectives are
a major focus and beneﬁt of active inference, and understanding their mathematical
nature and origins is the major task of Chapter 5 of this thesis.
4.2 Deep Active Inference
Active inference and reinforcement learning both purport to solve the same fundamental
problem – that of adaptive action selection to maximize some notion of rewards or
desires, given uncertainty about the world and about the optimal policy to take. While
active inference arises from the paradigm of variational Bayesian inference with pos-
Chapter 4. Scaling Active Inference 169
terior policy distributions and complex generative world models (Friston, FitzGerald,
et al., 2017b; Friston, Rigoli, et al., 2015b), reinforcement learning arises primarily
from the Bellman equation and the recursive properties of optimality (Kaelbling et al.,
1996; Sutton & Barto, 2018), and utilizes constructs such as value functions and policy
networks to learn adaptive behaviour even on challenging and complex control tasks.
Despite the very different origins of the two ﬁelds, since they are fundamentally trying to
solve the same problem, it seems likely that there is much each ﬁeld can learn from the
other, since they both illuminate difference facets of the same reality. Speciﬁcally, the
discrete-state-space active inference models introduced previously, in Chapter 2, suffer
from many limitations of scale. Speciﬁcally, they represent the core distributions as
discrete categorical variables, which require a relatively small, discrete and known state-
space to function. Moreover, active inference models typically assume knowledge of the
true generative process (i.e. the likelihood and prior matrices (A and B)), which often
cannot simply be assumed in more realistic control tasks. 4. An additional, and serious
obstacle to the scalability of classical active inference methods is the computation of
the policy prior, which is often taken to be the softmax of the expected free energy
over all policies. This is typically computed explicitly and exactly in the literature
(Da Costa, Parr, et al., 2020; Friston, Rigoli, et al., 2015b), and requires an explicit
enumeration of every single policy and its associated trajectory for which the expected
free energy can then be computed. In computational complexity terms, this results in
exponential complexity in both the time horizon and the size of the discrete state-space,
which clearly poses a signiﬁcant computational scaling issue even for relatively small
state-spaces and time-horizons. It is largely this obstacle which has prevented the
application of truly large scale active inference models and limited most studies to
4There is some work empirically investigating learning the A matrix using dirichlet hyperpriors over
its values (Schwartenbeck et al., 2019), and the rules for learning the B matrix are also straightforward.
However, most of the active inference literature eschews these methods in favour of hand-designed
likelihood and transition matrices (Friston, Rigoli, et al., 2015b; Friston, Rosch, Parr, Price, & Bowman,
2018b; Friston et al., 2012; Parr & Friston, 2017b), and large scale studies of the effectiveness of these
learning algorithms has not been ascertained at scale.
Chapter 4. Scaling Active Inference 170
toy tasks. There have been several methods in the literature proposed to somewhat
ameliorate the computational expense of the expected free energy, notably by pruning
away policies which have an a-priori likelihood less than some threshold (Friston,
Da Costa, Hafner, et al., 2020). However, although such approaches enable scaling to
slightly larger tasks, they do not attack the fundamentally exponential complexity of the
algorithm, rather they simply reduce the exponential coefﬁcient.
Indeed, all the scaling limitations of active inference are almost identical to those
of tabular reinforcement learning with explicitly represented state and action value
functions. In reinforcement learning, this scaling barrier was removed through the
use of deep neural networks as ﬂexible function approximators, to learn, via gradient
descent, to approximate the required constructs by training on a dataset of environmental
interactions (Sutton & Barto, 2018). We propose a similar approach may prove equally
useful for scaling up active inference models. In the next two sections we present two
studies which attempt to scale up active inference by using deep neural networks to
ﬂexibly approximate key densities in the active inference equation, as well as utilize
methods from deep reinforcement learning to approximate the evaluation of the expected
free energy over policies, which is fundamental to action selection in active inference.
In the ﬁrst study, which is based on the paper (Millidge, 2020), we are heavily inspired
by model-free deep reinforcement learning algorithms. We represent the transition
dynamics, observation likelihood, and variational action distribution as neural net-
works trained to jointly minimize the variational free energy. Similarly, we utilize a
bootstrappped value network to approximate the expected-free energy value function.
We show that this model-free deep active inference approach can scale to perform
equivalently, if not sometimes superiorly to contemporary deep reinforcement learning
approaches.
In the second study, which is based on the paper (Tschantz, Millidge, et al., 2020b),
which was a joint collaboration with Alexander Tschantz at the University of Sussex,
Chapter 4. Scaling Active Inference 171
we utilize a scheme inspired by model-based active inference for action selection.
Speciﬁcally, we use a model-based iterative planner to estimate the variational action
distribution, and estimate the expected free energy value function based on simulated
rollouts within the planner. We focus more heavily on the exploratory nature of the
behaviour furnished by the expected free energy (and free energy of the expected future –
to be discussed in chapter 4) objectives and demonstrate that optimizing these objectives
leads to empirically better performance, especially on sparse-reward tasks which require
substantial amounts of exploration.
4.2.1 Model-Free: Active Inference as Variational Policy Gradients
4.2.1.1 Derivation
The fundamental idea of active inference is to reformulate the control problem as a
variational inference one, and then use variational methods to solve it. Speciﬁcally,
we wish to recast the problem of control into one of inferring the optimal state and
action distribution. The formal setup we use to describe this is a discrete-time Partially
Observed Markov Decision Process (POMDP) model. In this model, the agent receives
observations o1:T ∈O, which are generated by some hidden environmental statex1:T ∈X
which satisﬁes the Markov property. The observations themselves do not necessarily
have to be Markov. The agent can then emit actions a1:T ∈A which can alter the latent
state of the environment and thus generate new observations. We assume that the agent
maintains a desire distribution ˜p(o1:T ) =∏t:T exp(−r(ot)) over observations such that
it most desires to be experiencing high rewards. Observations, states, and actions are
optimized over full discrete-time trajectories from times t = 0 to a given time horizon
T . Given these assumptions, we can write down a factorization of the environmental
POMDP as follows,
penv(o1:T ,x1:T |a1:T ) =penv(x1)penv(o1|x1)
T
∏
t=2
penv(ot|xt)p(xt|xt−1,at−1) (4.23)
We then assume that the agent knows the basic POMDP structure and factorisation
Chapter 4. Scaling Active Inference 172
properties of the agent (although not necessarily any details about the precise distribu-
tions involved), and maintains an additional generative distribution over actions which
specify its ideal action generating process. We can thus write the agent’s generative
model as,
pagent(o1:T ,x1:T ,a1:T ) =pagent(o1,x1)pagent(a1)
T
∏
t=2
pagent(ot|xt)pagent(xt|xt−1,at−1)pagent(at|xt)
(4.24)
From now on, since the true environmental generative process is never known, we do
not refer to it, only the generative model of the agent. Thus, for notational convenience,
we denote pagent simply as p. The inference problem we wish to solve, is to infer the
optimal action and state distribution given observations. That is, the key idea in active
inference is to infer the distribution,
p(x1:T ,a1:T |o1:T ) (4.25)
Note that unlike in control as inference approaches which encode reward directly into
the inference process by performing inference on a graphical model augmented with
additional optimality nodes, here we encode rewards or goals into the model through
the action prior, as we shall see later. In most situations, a direct computation of
this posterior distribution is intractable, so we resort to a variational approximation.
We deﬁne the variational distribution q(a1:T ,x1:T |o1:T ) which is under the control of
the agent, and then try to minimize the divergence between the true and approximate
posteriors,
L = argmin
a1:T
DKL[q(a1:T ,x1:T |o1:T )||p(x1:T ,a1:T |o1:T )] (4.26)
This divergence is still intractable since it contains the intractable posterior, however we
can derive a computable bound on this divergence known as the variational free energy,
Chapter 4. Scaling Active Inference 173
which we can then optimize,
F (o1:T ) =DKL[q(a1:T ,x1:T |o1:T )||p(x1:T ,a1:T ,o1:T )]
= KL[q(a1:T ,x1:T |o1:T )||p(x1:T ,a1:T |o1:T )]+ ln p(o1:T )
≥KL[q(a1:T ,x1:T |o1:T )||p(x1:T ,a1:T |o1:T )] (4.27)
Now, if we study the expression for the variational free energy F in some detail, we
can see that it can be split up into three interpretable terms,
F (o1:T ) =DKL[q(a1:T ,x1:T |o1:T )||p(x1:T ,a1:T ,o1:T )]
= −Eq(a1:T ,x1:T |o1:T )[ln p(o1:T |x1:T )]  
Reconstruction Error
+Eq(a1:T |x1:T )DKL[q(x1:T |o1:T )||p(x1:T )]  
State Divergence
+DKL[q(a1:T |x1:T )||p(a1:T |x1:T )]  
Action Divergence
(4.28)
If we apply the Markov assumption in the generative model, and assume that the varia-
tional posterior factorises across time such thatq(a1:T ,x1:T |o1:T ) =∏T
t=1 q(at|xt)q(xt|ot),
then we ﬁnd that the previous derivation (Equation 4.28) in terms of full trajectories
simpliﬁes considerably into a sum of individual timesteps. Thus, we can write,
F (o1:T ) =
T
∑
t=0
Ft(ot) =
T
∑
t=0
DKL[q(at,xt|ot)||p(xt,at,ot)]
=
T
∑
t=0
−Eq(at ,xt |ot [ln p(ot|xt)]  
Reconstruction Error
+Eq(at |xt )DKL[q(xt|ot)||p(xt|xt−1,at−1)]  
State Divergence
+DKL[q(at|xt)||p(at|xt)]  
Action Divergence
(4.29)
Examining these terms, we can see several familiar objectives from the machine learn-
ing literature. For instance, the ﬁrst reconstruction error term is simply just the log-
likelihood of observations expected under the trajectory belief distribution. This term
is commonly optimized in all sorts of machine learning tasks, of especial interest here
is its use as part of the objective of the variational autoencoder (Kingma & Welling,
2013) If the likelihood term p(ot|xt) can be thought of as the decoder of a variational
Chapter 4. Scaling Active Inference 174
autoencoder, then conversely the q(xt|ot) term can be thought of as the encoder. Sim-
ilarly, the state divergence term is often used as a regulariser or a method of training
a transition model in model-based reinforcement learning. Indeed, we can see the
transition dynamics term p(xt|xt−1,at−1) as encoding a direct model of the transition
dynamics. Thus, it is straightforward to parameterise these distributions using deep
neural networks. The q(xt|ot) and p(ot|xt) distributions are parametrised by the encoder
and decoder of a variational autoencoder respectively, which can be trained through a
reconstruction loss on the environmental observations. The p(xt|xt−1,at−1) distribution
can be encoded as a deep neural network trained on the transition dynamics of the envi-
ronment. With this, we turn to the two terms in the action divergence. The ﬁrst, q(at|xt),
can be thought of as a parametrized policy network, of the kind used in policy gradients
or actor critic methods in reinforcement learning. Speciﬁcally, it can be thought of as a
simple mapping between a state and the correct action to output from this state. So far
the inference procedure we have written has no notion of rewards or goals. To achieve
adaptive reward-sensitive action inference, this must be added somewhere. Following
common practice in the active inference literature, we encode goals or rewards into
the the action prior by assuming that it is equal to the softmax of the expected free
energies of future trajectories p(at|xt) =σ(γG(xt:T ,at:T ,ot:T )) where G is the expected
free energy functional from active inference, γ is a precision parameter which controls
the entropy or ‘temperature’ of the softmax, andσ(x) = exp(−x)∫
dxexp(−x) denotes the softmax
function. Intuitively, we can consider this agent trying to optimize the sum over time of
its expected free energy (and thus the extrinsic and intrinsic value components of the
EFE), and then selecting actions with a probability proportional to the relative value of
each choice. Such an action prior effectively implements a Boltzmann action distribu-
tion, which has been empirically studied in human and animal choice behaviours (Daw
et al., 2006). The action divergence term them simply tries to minimize the divergence
between the variational action policy q(at|xt) parametrised by a deep neural network,
and the ‘ideal’ action distributionp(at|xt).
Chapter 4. Scaling Active Inference 175
The key difﬁculty, then, is the computation of the softmaxed expected free energy, as
this is a path integral of the expected free energy of a trajectory into the future. Unlike
in tabular active inference approaches, we cannot simply enumerate all possible future
trajectories and evaluate them. Instead, we make use of a trick from deep reinforcement
learning, called bootstrapping, which takes advantage of the recursive nature of the
Bellman equation. Here, we ﬁrst note that the expected free energy, since it can be
simply written as a path integral through time, obeys a similar recursive relationship,
G(ot:T ,xt:T ) =
T
∑
t
Gt(ot,xt)
=⇒G(ot:T ,xt:T ) =Gt(ot,xt)+ Ep(ot+1,xt+1|xt ,at )[G(ot+1:T ,xt+1:T )] (4.30)
From here, we can expand the expected free energy term using its standard deﬁnition
(Friston, Rigoli, et al., 2015b) to obtain,
Gt(ot,xt) =Eq(ot ,xt )[lnq(xt)−ln ˜p(ot,xt)]
≈Eq(ot ,xt )[lnq(xt)−lnq(xt|ot)−ln ˜p(ot)]
≈−Eq(ot ,xt )[ln ˜p(ot)]−Eq(ot )DKL[q(xt|ot)||q(xt)]
≈− Eq(ot ,xt )[r(ot)]  
Reward Maximization
−Eq(ot )DKL[q(xt|ot)||q(xt)]  
Information Gain
(4.31)
Here, we see that the expected free energy can be (approximately) decomposed into
a reward maximization term and also an information gain term to be maximized.
This information gain term, here between posterior and prior expectations over states,
can be seen as an exploration-inducing ‘intrinsic reward’ inherent to active inference
agents, which furnishes them with greaterdirected exploration capabilities than baseline
reinforcement learning agents which lack this additional term and only focus on greedy
reward maximization.
Putting this all together, we realize that we can express the path integral of the expected
Chapter 4. Scaling Active Inference 176
free energy recursively as,
G(ot:T xt:T ) =Eq(ot ,xt )[r(ot)]−Eq(ot )DKL[q(xt|ot)||q(xt)]+ Eq(xt+1,ot+1|xt ,at )q(at |xt )[G(ot+1:T ,xt+1:T )]
(4.32)
To efﬁciently approximate this, we can then utilize the bootstrapping method used in
deep Q learning. Here we explicitly train a neural network to predict the expected
free energy value function G(ot:T ,xt:T ). While it may seem like this requires explicit
computation of the path integral, to produce correct targets for the network, in fact it
does not due to the recursive nature of the expected free energy equation. Let’s denote
the expected free energy value function predicted by the network as Gφ(ot,xt) which
takes as inputs only the current state and observation and uses it to predict the full path
integral. The value network has parameters φ. We can then approximate the true EFE
recursive relationship,
ˆG(ot:T xt:T ) =Eq(ot ,xt )[r(ot)]−Eq(ot )DKL[q(xt|ot)||q(xt)]+ Eq(xt+1,ot+1|xt ,at )q(at |xt )[Gφ(ot,xt)]
(4.33)
where we have replaced the recursive computation of the future expected free energies
with the prediction from the value network, which thus deﬁnes the value ‘estimate’ˆG.
Then, to train the value network, we simply minimize the squared difference between
the estimated EFE and the predicted EFE,
Lvalue(ot,xt) =||ˆG(ot,xt)−Gφ(ot,xt)||2 (4.34)
This method is referred to as bootstrapping because the real expected free energy path
integral is never computed. Instead the targets are also constructed from the value
network which are then used to train the value network. While this circular relationship
may make it unintuitive that this method can work, we can understand why it does work
empirically through the fact that at each step of the optimization, some local information
about the EFE is fed into the network through the (true) computation of Gt(ot,xt). Thus
over time, this local information builds up and allows the network to converge to the
Chapter 4. Scaling Active Inference 177
correct value function. Importantly, in Q learning, there are also several tricks that have
been found to be necessary to ensure a stable convergence. Principally, using the same
value network to update both the targets and the predictions simultaneously does lead to
instabilities and possible divergence. To ameliorate this, we use a ‘frozen’ copy of the
value network to compute the targets Gφ(ot,xt) and hold it ﬁxed while the true value
network is updated for N steps. After the N steps we replace the frozen value network
with another frozen copy of the newly trained value network. In this way, the targets do
not change rapidly over the course of optimization but nevertheless will slowly converge
towards the correct targets. Empirically, as in Q-learning, we found the use of a target
network a necessity for stable learning of the value function. Given a trained value
network which can output a prediction for the expected free energy value function, we
can then compute the action prior p(at|xt) for any given state and observation. We
have thus made concrete every single one of the distributions in the variational free
energy functional F , so that it can be explicitly evaluated. Once it can be evaluated, its
gradients can be computed through automatic differentiation techniques, and we can
train all the parameters of the agent jointly through stochastic gradient descent.
4.2.1.2 Model
At this point, to avoid losing sight of the wider picture, it is worthwhile to take a step
back and look at the model as a whole. We propose to represent and train the variational
posterior q(xt|ot) and the observation likelihood p(ot|xt) as a variational autoencoder.
We additionally represent the dynamics distribution p(xt|xt−1,at−1) as a deep neural
network transition model. We represent the variational action posterior q(at|xt) as a
deep neural network ‘policy network’ and can estimate the action priorp(at|xt) using
the softmaxed predictions of an expected free energy value network Gφ(ot,xt). All of
these terms are combined according to Equation 4.29 to compute the total variational
free energy which forms the uniﬁed loss function of the model. Then, all the parameters
of each network, except for the value network, are optimized according to a gradient
Chapter 4. Scaling Active Inference 178
descent on this total loss. The value network, by contrast, is trained using its own
separate bootstrapping loss.
The full deep active inference algorithm is presented below,
Algorithm 1:Deep Active Inference
Initialize Observation Networks Qθ(s|o), pθ(o|s) with parameters θ.
Initialize State Transition Network pφ(s|xt−1,at−1) with parameters φ
Initialize policy network Qξ(a|s) with parameters ξ
Initialize bootstrapped EFE-network Gψ(s,a) with parameters ψ
Receive prior state s1
Take prior action a1
Receive initial observation o1
Receive initial reward r1
while t < T do
ˆxt ←Qθ(s|o)(ot) ˆxt+1 ←pφ(s|xt−1,at−1)(ˆs) at ∼Qξ(a|s) Receive observation
ot+1 Receive reward rt+1
ˆG(s,a) ←rt+1 +EQ(xt+1 [log ˆxt+1 −log ˆxt+1]+ EQ(xt+1,at+1)[Gψ(xt+2,at+2)]
F ←EQ(s)[logp(o|s)]+ KL[ ˆxt+1||ˆxt+1]+
EQ(s)[
∫
daQξ(a|s)σ(−γGψ(s,a)(xt+1))+ H(Qξ(a|s))] θ ←θ+αdF
dθ
φ ←φ+αdF
dφ L ←||ˆG(s,a)−Gψ(s,a)||2 ψ ←ψ+αdL
dψ
end
While all the derivations are straightforwardly presented for a full POMDP model, in
our experiments we only utilized simple MDP examples with observable state. This
was because the main difﬁculty and contribution of this work is the action selection
mechanism, and that the extension to POMDPs is straightforward by just training a
separate variational autoencoder to estimate the latent state given the observations.
Because of this the two terms q(xt|ot) and p(ot|xt) are superﬂuous and not computed.
This leaves the transition model p(xt|xt−1,at−1), the policy network q(at|xt) and the
Chapter 4. Scaling Active Inference 179
value network Gφ(xt,ot). The transition model is necessary to compute the exploratory
information gain term in the expected free energy.
We represented the policy network, transition model, and value network each as two-
layer fully connected neural networks with relu activation functions and a hidden
size of 200 neurons. All networks, except the value network, were optimized by
minimizing jointly the variational free energy, through stochastic gradient descent with
the ADAM optimizer and a learning rate of 1e-4. The value network was trained on the
bootstrapping objective (Equation 4.34) with the same learning rate and optimizer. A
memory buffer was used to store all the agent’s experience which was replayed to the
agent for training at random in minibatches of size 64. No preprocessing was done on
the input data for any of the experiments. The models were implemented and automatic
differentiation was performed in the Flux.jl machine learning framework 5. We used
a target network to stabilize the learning of the value network. The target network
was copied from the value network every 50 episodes. Each episode consisted of a
full iteration through the replay buffer. Additionally, as is common in reinforcement
learning tasks, we utilized a temporal discount on the reward. Our temporal discount
factor was 0.99 for all models.
4.2.1.3 Results
We compared the performance of the active inference agent to two strong model-free
reinforcement learning baselines. Deep-Q learning (V . Mnih et al., 2013, 2015), and
an actor-critic (V . Mnih et al., 2016) architecture. Deep Q learning parametrises the Q
function using a deep neural network with a similar bootstrapping objective to the EFE
value function that we computed earlier, except only using reward. For the Q-learning
agent we utilized the same hyperparameters and value network size that we used for
the active inference agent, to enable a fair comparison. Moreover, we implemented the
5The code to reproduce all experiments can be found at:
https://github.com/BerenMillidge/DeepActiveInference.
Chapter 4. Scaling Active Inference 180
Q-learning agent with Boltzmann exploration (Cesa-Bianchi et al., 2017), which is very
similar to the softmax function used to compute the action prior.
We also compared the deep active inference agent to an actor-critic architecture. Unlike
a Q-learning agent which computes actions directly by maxing over the Q-function, the
actor-critic architecture maintains a separate ‘actor’ or policy network which is trained
on a policy gradient objective based on value function estimates learnt by the ‘critic’ –
a value function estimator trained through bootstrapping. The policy network and value
network of the actor-critic architectures were trained using the same hyperparameters
and architecture as the active inference agent, to enable a fair comparison of the methods.
We compared the performance of the active inference agent on three continuous control
environments from the OpenAI Gym (Brockman et al., 2016). These were CartPole,
Acrobot, and LunarLander. The cartpole environment is simple and requires the agent
to balance a pole atop a cart by pushing the cart either to the left or to the right. The
state-space of the cartpole environment is a four dimensional vector, comprising the cart
position angle and velocity, as well as the angle and velocity of the pole. A reward of
+1 is given for each timestep the episode does not end up to 200 steps. The episode will
end early if the cart is more than 2.4 units from the centre (the cart has left the screen),
or else the pole angle is more than 15 degrees from vertical (the pole has fallen down).
The acrobot environment requires the agent to learn to swing up and balance a triple
jointed pendulum. It has a state-space of 6 dimensions which represent the angles and
velocities of the joints. The action space is a three dimensional vector corresponding
to the force the agent wishes to exert on each joint. The reward schedule is -1 for
every timestep the pendulum is above the horizontal, and 0 if it is above horizontal.
The acrobot is a challenging task for exploration, since purely random actions are very
unlikely to lead to any reward. The lunarlander environment requires the agent to learn
to land a simulated spacecraft on a surface within a target region in a Newtonian physics
environment. It has a 8-dimensional state-space and a four-dimensional action space,
Chapter 4. Scaling Active Inference 181
with actions corresponding to ﬁre left engine, ﬁre right engine, ﬁre upwards engine, and
extend docking legs. The agent receives a reward of 100 for landing on the launchpad
(located at (0,0)), and a -0.3 reward for every time step the rocket’s engines are ﬁring. It
does not receive a penalty for simply doing nothing.
We compare the performance of the active inference, Q-learning, and actor-critic agent,
in terms of pure reward obtained in each environment 6. We ran for 15000 episodes
over 20 random seeds for each agent and plotted mean rewards obtained below,
Figure 4.2: Comparison of the mean reward obtained by the Active Inference agent
compared to two reinforcement learning baseline algorithms – Actor-Critic and Q learning
on the CartPole environment. We demonstrate the learning curves over 2000 episodes,
averaged over 5 different seeds. 500 is the maximum possible reward. We see that while
the vanilla actor critic agent initially learns faster, over a long time horizon, the active
inference agent outperforms it – and both perform better than the vanilla Q learning
agent.
6Note this comparison based purely on reward actually penalizes the active inference agent to some
degree, since it does not simply optimize the reward, but also by satisfying the epistemic drives furnished
by the EFE objective function
Chapter 4. Scaling Active Inference 182
Figure 4.3: Comparison of Active Inference with standard reinforcement learning algo-
rithms on the Acrobot environment. Here we see the learning curves plotted over ﬁve
seeds over 20000 episodes. The maximum possible reward in this environment was 0,
so no agents are optimal. We see again that active inference outperforms the other two
methods consistently.
Chapter 4. Scaling Active Inference 183
Figure 4.4: Comparison of Active Inference with reinforcement learning algorithms on the
Lunar-Lander environment. Learning curves presented over 15000 episodes, averaged
over 5 seeds. Here the vanilla policy gradient algorithm strongly outperforms the others,
for unclear reasons, although active inference is still comparable with the other standard
reinforcement learning algorithms. A score of 200 is optimal.
Since the active inference agent possesses several distinct features beyond the standard
actor critic architectures, we performed an ablation study to understand whence its
boost in performance arose.
Chapter 4. Scaling Active Inference 184
Figure 4.5: We compare the full Active Inference agent (entropy regularization + transition
model) with an Active Inference agent without the transition model, and without both
the entropy term and the transition model). We see that while removing the transition
model appears to have little effect, removing the entropy regularisation term substantially
impairs performance. This may be due to the entropy term aiding in staving off policy
collapse.
Chapter 4. Scaling Active Inference 185
Figure 4.6: Comparison of the rewards obtained by the fully ablated Active Inference
agent with standard reinforcement-learning baselines of Q-learning and Actor-Critic on
the CartPole environment. Learning curves are averaged over 5 seeds. We see that
despite being fully ablated, the active inference agent continous to perform comparably
with standard reinforcement learning agents.
We see that the key factor in the superior performance of the active inference agent
is the additional action entropy term that is additionally optimized. This provides
additional empirical conﬁrmation to the success of control-as-inference approaches
in the reinforcement literature which similarly utilize such an entropy regularisation
term. Perhaps surprisingly, we found little effect of the epistemic terms in the EFE on
total performance. We hypothesise that this was for two reasons. Firstly, the tasks that
were tested possessed a relatively dense reward structure, sufﬁcient to be learned by
standard reinforcement learning agents utilizing only random exploration strategies,
and thus that more advanced and powerful exploration strategies are likely unnecessary
for such tasks. Secondly, the epistemic action term was the information gain between
the prior and posterior states, which is effectively a measure of the predictive success
of the transition model. Importantly, we found that the transition model very rapidly
converged during training, much faster than the policy or value network, and thus that
Chapter 4. Scaling Active Inference 186
throughout most of the training period, the exploration term was thus negligible.
4.2.1.4 Interim Discussion
In this section, we have demonstrated how active inference approaches can be straightfor-
wardly scaled up by parametrizing the likelihood, inference, and transition distributions
with deep neural networks, and then additionally approximating the path integral of
the expected free energy with an amortized value network. This is possible because
the expected free energy satisﬁes a similar Bellman-like recursion to the reward in
reinforcement learning, because it is factorizable across time into a sum of independent
time-steps. Importantly, we have demonstrated that by taking this approach, our active
inference agent can handle complex machine learning benchmark tasks just as well
as several core deep reinforcement learning approaches, thus rendering it signiﬁcantly
more scalable than previous efforts in the literature which have generally been restricted
to small, discrete, and straightforwardly enumerable state spaces.
Moreover, we have shown that our algorithm is competitive, and in some cases superior,
to standard baseline reinforcement learning agents on a suite of reinforcement learning
benchmark tasks from OpenAI Gym (Brockman et al., 2016). While our active inference
agent performed worse than direct policy gradients on the Lunar-Lander task, we believe
this is due to the inaccuracy of the expected-free energy-value-function estimation
network, since the policy gradient method used direct and unbiased monte-carlo samples
of the reward rather than a bootstrapping estimator. Since the performance of Active
Inference, at least in the current incarnation, is sensitive to the successful training of the
EFE-network, we believe that improvements here could substantially aid performance.
Moreover, it is also possible to forego or curtail the use of the bootstrapping estimator
and use the generative model to directly estimate future states and the expected-free
energy thereof, at the expense of greater computational cost. We take this approach of
using the transition model to generate sample rollouts and using these to compute a
Monte-Carlo estimate of the EFE path integral in the next section, where these estimates
Chapter 4. Scaling Active Inference 187
are used to inform model-predictive planning.
An additional advantage of our approach is that due to having the transition model,
it is possible to predict future trajectories and rewards N steps into the future instead
of just the next time-step. These trajectories can then be sampled from and used to
reduce the variance of the bootstrapping estimator, which should work as long as the
transition model is accurate. This N could perhaps even be adaptively updated given
the current accuracy of the transition model and the variance of the gradient updates.
This is a way of controlling the bias-variance trade-off in the estimator, since the future
samples should reduce bias while increasing the variance of the estimate, and also the
computational cost for each update.
Another important parameter in active inference is the precision (Feldman & Friston,
2010; Kanai, Komura, Shipp, & Friston, 2015), which in the discrete-state-space
paradigm corresponds to the inverse temperature parameter in the softmax and so
controls the stochasticity of action selection 7. In all simulations reported above we
used a ﬁxed precision of 1. However, in the discrete state-space case, the precision
is often explicitly optimized against the variational free energy, and the same can be
done in our deep active inference algorithm. In fact, the derivatives of the precision
parameter can be computed automatically using automatic differentiation. Determining
the impact of precision optimization on the performance of these algorithms is a
potentially worthwhile avenue for future work.
While we did not ﬁnd that using the epistemic reward helped improve performance
on our benchmarks, this could be due to the simplicity of the tasks we were trying to
solve, for which random exploration is sufﬁcient. In the next section, we demonstrate
that the epistemic affordances engendered by the use of the EFE value function prove
instrumental in attaining high performance in sparse-reward tasks.
7In the continuous predictive coding paradigm the precision modulates the ‘importance’ of the
prediction errors.
Chapter 4. Scaling Active Inference 188
The entropy regularization term which emerges directly from the mathematical formu-
lation of active inference proved to be extremely important, and was often the factor
causing the superior performance of our active inference agent to the reinforcement
learning baselines. This entropy term is interesting, since it parallels similar develop-
ments in reinforcement learning, which have also found that adding an entropy term to
the standard sum of discounted returns objective improves performance, policy stability
and generalizability (Haarnoja, 2018; Haarnoja, Tang, Abbeel, & Levine, 2017). This
is of even more interest given that these algorithms can be derived from a similar
variational framework which also casts control as inference (Levine, 2018). Later (in
Chapter 5), we discuss in signiﬁcant detail how such paradigms relate to active infer-
ence. Additionally, many of the differences between active inference and the standard
policy gradients algorithm – such as the expectation over the action, and the entropy
regularization term – have been independently proposed to improve policy gradient
and actor critic methods (Fujimoto et al., 2018). The fact that these improvements
fall naturally out of the active inference framework could suggest that there is deeper
signiﬁcance to the probabilistic inference formulation espoused by active inference.
The other key difference between policy gradients and active inference is the optimiza-
tion of the policy probabilities versus the log policy probabilities, and multiplying by
the log of the probabilities of the estimated values, rather than the estimated values
directly. It is currently unclear precisely how important these differences are to the
performance of the algorithm, and their effect on the numerical stability or conditioning
of the respective algorithms, and this is also an important avenue for future research.
However, the comparable performance of active inference to actor-critic and policy
gradient approaches in our results suggest that the effect of these differences may be
minor.
Chapter 4. Scaling Active Inference 189
4.2.2 Model-based: Reinforcement Learning through Active Infer-
ence
While the last section focused on scaling active inference using model-free reinforce-
ment learning methods, here we focus on scaling active inference in a way inspired
by model-based reinforcement learning methods. Model-based reinforcement learning
is perhaps a better ﬁt for the central ideas in the classical active inference. Tabular
active inference, after all, is a model-based algorithm which explicitly evaluates and
optimizes future plans using explicit models of the environmental dynamics. Moreover,
as tabular active inference explicitly replans at every time-step, it can be considered to
be a model-predictive control algorithm. In fact, the explicit enumeration and evaluation
of every possible policy can perhaps best be thought of as a truly exhaustive planning
algorithm.
Similar to our previous approach described earlier, we propose to develop deep active
inference methods which utilize deep neural networks to parametrize key distributions
from the active inference framework. Speciﬁcally, we maintain deep neural network
representations of the observation likelihood distribution p(ot|xt) as well as the transi-
tion model which parametrises the dynamics model of the environment p(xt|xt−1,at−1).
The major difference is how the action policy q(at|xt) is handled. In the previous
model-free approach, this distribution was represented as an independent policy neural
network which was trained against the action prior which represented the softmax of
the expected free energy value function in an actor-critic like fashion. Here, we treat
the action posterior as the output of a model-based planning algorithm.
Speciﬁcally, and quite elegantly, we can show that under certain conditions of the
generative model of the future, that we can derive the optimal plan as a softmax over
the expected free energy in the future, (which was merely assumed to be the action
prior in the model-free case). Moreover, we then show that this path integral can be
approximated by monte-carlo sampling in the form of a model-based planning algorithm
Chapter 4. Scaling Active Inference 190
which samples and evaluates given potential future trajectories using the transition and
reward models possessed by the agent.
˜Fπ = DKL
(
q(ot,xt,θ,π)∥˜p(ot,xt,θ)
)
= Eq(ot ,xt ,θ,π)[logq(ot,xt,θ|π)+ logq(π)−log ˜p(ot,xt,θ,π)]
= Eq(π)
[
Eq(ot ,xt ,θ|π)[logq(π)−[log ˜p(ot,xt,θ)−logq(ot,xt,θ|π)]
]
= Eq(π)
[
logq(π)−Eq(ot ,xt ,θ|π)[log ˜p(ot,xt,θ)−logq(ot,xt,θ|π)]
]
= Eq(π)
[
logq(π)−
[
−Eq(ot ,xt ,θ|π)[logq(ot,xt,θ|π)−log ˜p(ot,xt,θ)]
]]
= Eq(π)
[
logq(π)−loge−
[
−Eq(ot ,xt ,θ|π)[logq(ot ,xt ,θ|π)−log ˜p(ot ,xt ,θ)]
]]
= Eq(π)
[
logq(π)−loge−DKL
(
q(ot ,xt ,θ|π)∥˜p(ot ,xt ,θ)
)]
= DKL
(
q(π)∥e−DKL
(
q(ot ,xt ,θ|π)∥˜p(ot ,xt ,θ)
))
= DKL
(
q(π)∥e−˜Fπ
)
(4.35)
It is important to note that here we do not use the expected free energy as our objective,
unlike in standard active inference. Instead, we use the recently introduced objective:
the free energy of the expected future (FEEF). This objective maintains the exploratory
information gain terms of the traditional expected free energy while possessing a clear
mathematical origin with strong intuitive grounding. Speciﬁcally, the FEEF can be
deﬁned as,
FEEF = ˜Fπ = DKL[q(ot,xt,θ|π)||˜p(ot,xt,θ)
≈Eq(xt|π)DKL[q(ot|xt)||˜p(ot)]  
ExtrinsicValue
−Eq(ot ;θ)DKL[q(xt|ot,θ)||q(xt)]  
State Information Gain
− DKL[q(θ|xt)||q(θ)]  
Parameter Information Gain
(4.36)
Where we can see that the FEEF can be split into approximately three terms – the like-
lihood divergence term which measures how much the expected observations diverge
Chapter 4. Scaling Active Inference 191
from the desired observations, and which effectively encodes reward or utility seeking
behaviour, and an information gain term to be maximized which induces exploratory,
uncertainty reducing behaviour. Since the FEEF objective includes both latent states
x and parameters θ, we actually obtain two separate information gain terms, one for
the states and one for the parameters. The core ﬁnding and argument of this part of the
chapter, and an example of what the theory of active inference can bring to contem-
porary deep reinforcement learning, is that the exploratory information-seeking terms
furnished by active inference objectives such as the expected free energy, or free energy
of the expected future, by inducing purposeful and goal-directed exploratory behaviour,
they can outperform traditional random exploration on a number of challenging rein-
forcement learning tasks, and their advantages become especially apparent in the case of
sparse rewards where random exploration is often simply insufﬁcient to ﬁnd any good
solutions in a reasonable time. Moreover, recent work in the literature, which utilizes
exploratory objectives, but in a ﬁrst exploratory, and then exploitatory phase, we argue
that it is necessary to combine the two objectives to be jointly optimized. In this way the
agent is furnished with a desire for goal directed exploration. It is not rewarded simply
for reducing any uncertainty, but only uncertainty that also exists in rewarding regions
in the state-space. In this way, agents explore precisely only as much as needed, thus
providing a step towards a practical solution to the exploration-exploitation tradeoff.
4.2.2.1 Model
As in previous work, we extended active inference by using deep neural networks to
parametrize key densities. Our model utilized a neural network transition model to
model the distribution p(xt|xt−1,at−1)). Since the FEEF objective requires the evaluation
of an information gain term over the parameters (denoted θ) of the transition model,
we maintained an approximate distribution over the parameters θ using an ensemble of
transition models with independently initialized parameters, and trained on different
batches from the replay buffer. This ensemble approach has been found to be widely
Chapter 4. Scaling Active Inference 192
useful in model-based reinforcement learning and to offer a superior representation of
the true posterior over the parameters than competing methods such as Bayesian neural
networks. Importantly, utilizing an explicit ensemble of transition models allows the
estimated posterior over the parameters to be multimodal, as opposed to the unimodal
Gaussian assumption implicit in the Bayesian neural networks approach (Gal, McAllis-
ter, & Rasmussen, 2016; Tran, Dusenberry, van der Wilk, & Hafner, 2018). Moreover,
an ensemble of models has been found empirically to help avoid overﬁtting in low-data
regimes, which are also when the advantages of model-based reinforcement learning
are most apparent. Each element of the transition model ensemble was implemented
as an independent neural network with two hidden layers with 400 neurons each. The
networks used the swish activation function. The transition networks predicted the
difference in the next state (Shyam et al., 2019) instead of the next state, as this has
been found to help capture environmental dynamics more accurately in practice. Since
we are evaluating future simulated rollouts, we cannot simply rely on environmentally
provided ‘true rewards’. While many methods in the literature assume the existence
of a known reward function which can be queried even for counterfactual or simulated
trajectories (Chua et al., 2018; Hafner et al., 2018), we do not, as such a reward oracle is
unrealistic in many if not most situations. Instead, we learnt a reward model based upon
previous interactions with the real environment, and then used the reward model to score
proposed trajectories. The reward model was parametrised by a two layer multi-layer
perceptron network with 400 units in the hidden layer and a relu activation function.
The reward model was trained on a mean-square error loss between actually observed
rewards for a given state, and the reward predicted by the reward model. Importantly,
the FEEF objective deﬁnes the extrinsic reward to be the KL divergence between the
observation likelihood and the desired observation distribution. Since our model was
situated purely in an MDP setting with fully observed state, the only observation was the
reward, and thus the reward model doubled as the likelihood model. We set the desired
reward observations to be a Gaussian distribution with a variance of 1 centred at the
Chapter 4. Scaling Active Inference 193
maximum possible reward for the environment. Since we interpret the predictions of the
reward model as representing the mean of a Gaussian distribution, we can analytically
calculate the KL divergence term. This allowed us to straightforwardly compute and
optimize the reward maximization part of the FEEF objective. Similarly, to evaluate the
information gain terms of the FEEF objective, we can rewrite it in a more tractable way,
Eq(xt |θ)DKL
(
q(θ|xt)∥q(θ)
)
= Eq(xt |θ)q(θ|rs)
[
logq(θ|xt)−logq(θ)
]
= Eq(xt ,θ)
[
logq(xt|θ)+ logq(θ)−logq(xt)−logq(θ)
]
= Eq(xt ,θ)
[
logq(xt|θ)−logq(xt)
]
= Eq(θ)q(xt |θ)
[
logq(xt|θ)
]
−Eq(θ)q(xt |θ)
[
logEq(θ)q(xt|θ)
]
= −Eq(θ)H
[
q(xt|θ)
]
+H
[
Eq(θ)q(xt|θ)
]
(4.37)
In effect, the parameter information gain term decomposes into an entropy of the average
state minus the average of the entropy. The average of the entropy can be computed
semi-analytically since each transition model ensemble models a Gaussian distribution,
which has a known and analytically calculable entropy. Then, the average can simply be
performed by directly averaging the entropies of each member of the ensemble together.
The entropy of the average term is more difﬁcult, since the average of many different
Gaussian distributions is not necessarily Gaussian. As such we approximate it with a
nearest neighbour entropy approximation (Mirchev, Kayalibay, Soelch, van der Smagt,
& Bayer, 2018) which we found worked well in practice.
To train the model, we optimized the reward and transition models on data taken from a
replay buffer. They were trained with stochastic gradient descent on their respective
loss functions using a negative log likelihood loss. We cold-started the training of the
agent at the end of every episode, as we found that this led to more consistent behaviour
and performance. We initialized each episode with a dataset D taken from an agent
Chapter 4. Scaling Active Inference 194
with a random policy, to ensure some degree of transition and reward model accuracy
before beginning with the model-based planning.
To compute actions we used a model-based planner (CEM) (Rubinstein, 1997) within
the model-predictive control paradigm. The CEM planning algorithm generates and
evaluates the reward of a large number of action trajectories, then takes the mean and
variance of some elite set of actions (usually the top 10) of the actions and restarts
the evaluation using actions sampled from a Gaussian distribution with this mean and
variance. For every timestep, a 30 step planning horizon was used resulting in a 30-step
action plan, of which the ﬁrst action was executed. In accordance with model-predictive
control, we replan at each step. For the CEM algorithm, we used 700 candidate action
sequences to be evaluated in each iteration, for 7 iterations. To train the transition and
reward models we used the ADAM optimizer with a learning rate of 1e-4 and trained
Chapter 4. Scaling Active Inference 195
for 100 epochs.
Algorithm 2:Inference of q(π)
Input: Planning horizon H | Optimisation iterations I | Number of candidate
policies J | Current state st | Likelihood p(oτ|sτ) | Transition distribution
p(sτ|sτ−1,θ,π) | Parameter distribution P(θ) | Global prior ˜p(oτ)
Initialize factorized belief over action sequences q(π) ←N (0,I).
for optimisation iteration i = 1...I do
Sample J candidate policies from q(π)
for candidate policy j = 1...J do
π( j) ∼q(π)
−˜F j
π = 0
for τ = t...t +H do
q(sτ|sτ−1,θ,π( j)) =Eq(sτ−1|θ,π( j))
[
p(sτ|sτ−1,θ,π( j))
]
q(oτ|sτ,θ,π( j)) =Eq(sτ|θ,π( j))
[
p(oτ|sτ)
]
−˜F j
π ←−˜F j
π +Eq(sτ,θ|π( j))
[
DKL
(
q(oτ|sτ,θ,π( j))∥˜p(oτ)
)]
+
H[q(sτ|sτ−1,θ,π( j))]−Eq(θ)
[
H[q(sτ|sτ−1,π( j),θ)]
]
end
end
q(π) ←reﬁt(−˜F j
π )
end
return q(π)
4.2.2.2 Results
We tested the performance of our algorithm against strong model-free and model-based
baselines on a number of challenging control tasks 8. We utilized ﬁrst the mountain-car
task from OpenAI Gym, which requires the agent to steer a car on a 1D line to a goal.
This task is difﬁcult because the agent must ﬁrst move the car away from the goal up a
8Different tasks are utilized in this section compared to the previous one because here we are
dealing with continuous actions while in the previous section we dealt only with learning discrete-action
controllers.
Chapter 4. Scaling Active Inference 196
hill, to build up momentum to be able to get over the larger hill. This poses a difﬁcult
exploration problem which purely random exploration agents struggle to solve. By
contrast, our agent can solve this task instantaneously, within only a single episode, due
to its goal directed exploration.
0 20
Episodes
0.0
0.5
1.0Average returns
Mountain Car
0 20 40
Episodes
0
500
1000
Cup Catch
0 50 100
Episodes
250
0
250
500
Cheetah Flip
0 50 100
Episodes
200
0
200
400
A B C D Cheetah Run
FEEF Reward Variance SAC
Figure 4.7: (A) Mountain Car: Average return after each episode on the sparse-reward
Mountain Car task. Our algorithm achieves optimal performance in a single trial.(B) Cup
Catch: Average return after each episode on the sparse-reward Cup Catch task. Here,
results amongst algorithms are similar, with all agents reaching asymptotic performance
in around 20 episodes. (C & D) Half Cheetah: Average return after each episode on the
well-shaped Half Cheetah environment, for the running and ﬂipping tasks, respectively.
We compare our results to the average performance of SAC after 100 episodes learning,
demonstrating our algorithm can perform successfully in environments which do not
require directed exploration. Each line is the mean of 5 seeds and ﬁlled regions show +/-
standard deviation.
Since the mountain car environment possesses only a two dimensional state space, we
can explicitly plot and compare the degree of the space covered by the active inference
agent vs the greedy reward maximizing reinforcement learning agent.
We thus see that the exploratory drives inherent in the active inference agent propel it
to explore a signiﬁcantly larger fraction of the state-space than the reward maximizing
agent, and it is this exploration which allows it to stumble upon the goal and thus rapidly
learn to solve the task. By contrast, the random exploration reward-maximizing agent is
unable to escape the local minimum at its start location by a purely random walk, since
Chapter 4. Scaling Active Inference 197
1.0
 0.5
 0.0 0.5
Position
0.06
0.04
0.02
0.00
0.02
0.04
0.06
Velocity
State Coverage (FEEF)
1.0
 0.5
 0.0 0.5
Position
0.06
0.04
0.02
0.00
0.02
0.04
0.06
Velocity
State Coverage (Reward)
FEEF Random
0
20
40
60
80
100Maze coverage (percent)
A B C Ant Maze Coverage
Figure 4.8: (A & B) Mountain Car state space coverage: We plot the points in state-
space visited by two agents - one that minimizes the free energy of the expected future
(FEEF) and one that maximises reward. The plots are from 20 episodes and show that
the FEEF agent searches almost the entirety of state space, while the reward agent is
conﬁned to a region that can be reached with random actions. (C) Ant Maze Coverage:
We plot the percentage of the maze covered after 35 episodes, comparing the FEEF
agent to an agent acting randomly. These results are the average of 4 seeds.
to do so requires too many correct moves in a sequence than can be generated within a
reasonable time. We also tested our agent on sparse reward versions of two challenging
control tasks. In the ﬁrst task, Cup Catch, the agent must actuate a cup to catch a ball
thrown at it. The agent receives a reward of 1 if it catches the ball, and a reward of 0 if
it does not. Similarly, in the Half-Cheetah environment, the agent takes control of the
limbs of a running planar cheetah in a semi-realistic physics simulation. Its goal is to
maximize the velocity at which the cheetah moves forward, ideally by running. We also
experimented with a no-reward environment to test the pure exploration capabilities
of our agent. For this we utilized the ant-maze environment in which the agent must
actuate an ant-like robot in order to explore as much of a maze as possible. The agent
receives no extrinsic rewards at any point in this task.
Overall, we see that the active inference and reward maximization agent performs simi-
larly in the cup-catch environment. We hypothesise that this is because, even with the
sparse reward, the reward is easy enough to obtain even with purely random behaviour.
Similarly, on the half-cheetah benchmark, our model performs signiﬁcantly better than
Chapter 4. Scaling Active Inference 198
the model-free SAC agent, due to the superior sample efﬁciency of model-based over
model-free methods, however it also does not show much signiﬁcant improvement
compared to reward maximizing model-based baselines. However in the ant-maze
environment, our agent evinces signiﬁcantly more exploration capacity and explores a
substantially larger proportion of the total state space than any other agent, once again
demonstrating its superior exploratory capabilities.
4.2.2.3 Interim Discussion
So far, in this chapter, we have applied an active inference perspective to reinforcement
learning and recast the traditional RL objective into a more active-inference-inspired one;
reformulating the reward maximization objective as that as minimizing the divergence
between predicted and desired probabilistic futures. From this starting point, we can
derive a novel algorithm that exhibits high performance as well as robustness, ﬂexibility,
and sample efﬁciency in a number of environments that are known to be exceptionally
challenging for traditional reinforcement learning methods, while also performing
comparably in environments where standard RL methods do well.
We believe that through these two studies, we have convincingly demonstrated that
active inference approaches can be successfully scaled to levels equal to contemporary
reinforcement learning methods in both model-free and model-based paradigms. More-
over, we hope to have shown some ways in which the integration of active inference
and reinforcement learning can provide novel and useful perspectives to inform and
inspire work in the deep reinforcement learning community. We demonstrate that the
exploration-inducing properties of active inference objective functionals such as the
Expected Free Energy and the Free Energy of the Expected Future are highly beneﬁcial
especially in more challenging tasks with sparse or no rewards, while also performing
comparably to pure reward maximization approaches on dense reward tasks that can
be solved with purely random exploration. Moreover, combining both exploratory
and reward maximizing terms in a single objective function and jointly optimizing
Chapter 4. Scaling Active Inference 199
them is crucial to derive algorithms which can simply learn to solve a task without
separate exploratory and exploitatory phases as in much of the literature (Shyam et
al., 2019), although the idea of inducing exploration by optimizing an epistemic term
(Oudeyer & Kaplan, 2009; Pathak et al., 2017; Schmidhuber, 2007) has been applied
previously in reinforcement learning, and that can be competitive directly with both
purely exploratory and purely exploitatory tasks in the regions where each of these
methods excel.
An additional idea, inspired by active inference, which could inform reinforcement
learning perspective is the idea of representing preferences, instead of scalar reward
values, as a distribution over observations. We believe that this modelling choice could
enable greater ﬂexibility in learning non-scalar, non-monotonic reward functions, as
well as providing a natural, Bayesian framework for handling the case of unknown,
uncertain, or nonstationary reward distributions. We believe that in many naturalistic
settings, especially for biological organisms, rewards are not simply given a-priori by
some known oracle, but are task-dependent, contingent, often highly uncertain, and
nonstationary. Active inference provides a straightforward Bayesian account to handle
precisely such conditions. Although in this work, and in much of the literature, we
instead take extremely simplifying assumptions such as ˜p(o) =exp(−r(o)) to make
active inference as close as possible to reinforcement learning, future work should
instead head in the opposite direction and try to deliberately explore the regions where
active inference approaches offer greater ﬂexibility than the traditional reinforcement
learning paradigm, and thus demonstrate the advantages of active inference approaches
there.
4.2.3 Related Work
Before our work in deep active inference there was a small amount of prior work
which is important to review. The seminal paper which began this ﬁeld is Deep Active
Chapter 4. Scaling Active Inference 200
Inference (Ueltzhöffer, 2018), which initiated the idea of using deep neural networks
to approximate key densities within the active inference paradigm. This paper uses
small neural networks to parametrise the transition dynamics and likelihood in active
inference, and uses genetic algorithms to directly optimize a policy module of the
expected free energy in a black-box fashion. This is because, under their problem
setup, the expected free energy depends on the environmental dynamics and is thus
nondifferentiable, assuming the environment itself is unknown. They produced a simple
agent which can learn to solve the mountain car problem from OpenAI gym after many
iterations. This paper was my inspiration to dive deeper into trying to understand the
commonalities and differences between deep active inference and deep reinforcement
learning.
Another piece of work, arising contemporaneously with my own initial work (Millidge,
2019a, 2020), was that by Çatal, Nauta, Verbelen, Simoens, and Dhoedt (2019). They
also parametrised the likelihood and transition dynamics using deep neural networks,
and additionally explicitly utilized an expected free energy value function. However,
instead of directly solving the sparse-reward challenge implicit in the mountain-car
environment they tested their agent in, they instead constructed a hand-crafted ‘state-
prior’ generated from expert-rollouts which already directly solved the task, thus
providing an effectively dense reward signal for this sparse reward problem.
Some other related work is that of Cullen et al. (2018) who also applied active inference
to more complex non-toy environments. They trained an active inference agent to play
a subset of DOOM – the ‘take-cover’ environment in OpenAI Gym. However, they
still fundamentally utilised the discrete-state-space active inference formulation by
discretising the continuous DOOM environment into 8 discrete states using the Harris
Corner detection algorithm, and then applying discrete-state active inference onto the
discrete states.
Just after my initial work came similar work by (Tschantz, Baltieri, Seth, & Buckley,
Chapter 4. Scaling Active Inference 201
2020), who instead applied active inference in a model-based fashion. Their model
parametrises the transition dynamics using a deep neural network, and then uses model
based planning (using the CEM algorithm) to optimize the expected free energy over
time. We jointly extended their model in (Tschantz, Millidge, et al., 2020b) to investigate
explicitly the exploratory effects of the EFE or FEEF objective, and whether such
methods can be further scaled through learning the transition and reward model.
After our work, several recent approaches have scaled active inference further. Çatal,
Verbelen, Nauta, De Boom, and Dhoedt (2020), situate deep active inference within
a purely POMDP setting, using a V AE encoder and decoder to parametrize the likeli-
hood and state-posterior mappings, and then explicitly compute an action search tree
using their transition model to approximate the path integral over the expected free
energy through time. Similarly, Fountas, Sajid, Mediano, and Friston (2020), also
explicitly compute a model-based EFE search tree in their tasks, while simultaneously
approximating the output of the action planner with a model-free ‘habitual’ policy
network.
4.2.4 Iterative and Amortised Inference
Now that we ﬁrmly understand the notion of implementing control as an inference
procedure, it is worth recapping a fundamental distinction between two different types
of inference, which are important and implicit in the literature, but rarely well explained.
The crucial distinction is between what we call iterative and amortised inference.
Iterative inference is the kind that arises directly from a naive application of Bayes Rule,
and was the standard inference approach used until the rise of deep learning very recently
(Beal, 2003; M. Jordan et al., 1998; Wainwright & Jordan, 2008). Almost all ‘classical’
variational or Bayesian inference methods are iterative. Amortised inference only
became prominent with the advent of the variational autoencoder (Kingma & Welling,
2013), but has since become the dominant approach, especially within machine learning.
Chapter 4. Scaling Active Inference 202
The key distinction is that iterative inference directly optimizes the parameters of the
variational distribution. For instance, suppose we assume our variational distribution is
Gaussian, than iterative inference tries to optimize the means and variance φ = {µ,σ}
of this Gaussian to ﬁt some datapoint. This is what is implied by the standard reading
of the ELBO or variational free energy equation (Beal, 2003; Hinton & Zemel, 1994).
Iterative = argmax
φ
Eq(x;φ)[lnq(x;φ)−ln p(o,x)] (4.38)
Amortised inference, by contrast, does not directly optimize the parameters of the
variational distribution. Rather, it learns the parameters of a function that maps data
to parameters of the variational distribution. Effectively, the variational parameters
themselves are never optimized directly, they are simply spit out of the amortised
function f , which is then learned. φ = fψ(D). Rather, it is the parameters of this
amortisation function fψ that are learned. Importantly, these functions are optimized
not just against a single data-point but across the whole dataset DR. Once learned, the
amortisation function f can be quickly used to compute estimated variational parameters
ˆφ for any data-point, thus amortising the cost of inference across the whole dataset.
By contrast, iterative inference must start from scratch from each individual data-point
given and optimize the variational parameters afresh. While it is often written the
same way, to make the notation very explicit, we write the amortisation objective to be
optimized as,
Amortised = argmax
ψ
Ep(D)
[
Eq(x;ˆφ= fψ(D))[lnq(x; ˆφ = fψ(D))−ln p(o,x)]
]
(4.39)
The reason that amortised inference has risen to such popularity and ubiquity lately is
due to the fact that the amortisation function fψ is straightforward to implement as a
deep neural network, where ψ are the neural network weights which can be trained by a
gradient descent on the ELBO or variational free energy. For instance, in a variational
autoencoder, fψ is effectively implemented by the encoder, which maps the data directly
Chapter 4. Scaling Active Inference 203
to the variational parameters (the mean and variance of the Gaussian). The iterative
approach, by contrast, would forego the encoder and run gradient descent directly on
the mean and variance themselves for each data-points. We thus see why amortised
methods are preferred. The amortised method can infer the mean and variance quickly,
in one feedforward pass of the network, while gradient descent training is split over an
entire dataset. By contrast, the iterative approach would require a gradient descent for
every inference that the network wishes to make. Importantly, however, the variational
parameters found by the amortised methods are in general worse estimates than those
found by iterative inference. This is simply because the iterative inference method
optimizes the parameters afresh with each data-point, while amortised inference must
try to estimate them given a general function which must work for every datapoint.
Thus the amortisation function fψ must generalize in a way that iterative methods do not
have to, and thus any generalization error will cause the amortised method to perform
worse. This difference in performance is called the amortisation gap.
Interestingly, it has recently been shown (Marino, Yue, & Mandt, 2018), that you
can gain performance improvements by combining iterative and amortised inference
together. For instance, in a variational autoencoder, if you ﬁrst perform the amortised
mapping to obtain initial estimates of the variational parameters φ but then run several
iterative descent steps directly on your initial estimates of the parameters, this can
improve the inference accuracy and reduce the amortisation gap compared to the pure
amortisation approach with only a relatively small computational penalty for each
inference.
Given that we know we can understand control problems in terms of inference, it is
also interesting to consider whether the type of inference applied in control as inference
can be best understood as iterative or amortised inference. Indeed, we argue that this
distinction between iterative and amortised inference maps rather cleanly (although
not perfectly) to the distinction between model-based and model-free reinforcement
Chapter 4. Scaling Active Inference 204
learning. Where model-free RL can be thought of as amortised inference and model-
based as iterative inference. The reasoning here is straightforward but requires some
subtly about what exactly is being inferred.
The key quantity to be inferred in control as inference approaches is the variational
distribution over actionsq(a|x). Model-free approaches which try to maintain a constant
estimate of the value function, Q function or advantage function using the Bellman
equation can be understood as amortised inference. This is most explicit in the case of
actor-critic or policy gradient methods which explicitly maintain an amortised policy
qψ(a|s), which is implemented as a deep neural network with weights ψ where the
weights are not optimized separately for each data-point, but rather across all data-
points. Approaches based purely on value function learning, such as Q learning, can
also be expressed in such a manner, because here the optimal policy depends in a
straightforward way upon the amortised value function. For standard deterministic Q
learning we have that qψ(a|s) =δ(a −maxaQψ(s,a)), or that the action distribution is
a dirac-delta over the maximum value of the Q function, which is itself amortised and
implemented as a deep neural network. In soft methods, the delta-max is replaced by a
softmax over all action values, so that actions are selected with probability proportional
to their relative exponentiated magnitudes.
Model-based methods, by contrast, appear to correspond to iterative inference ap-
proaches. The key to understanding this is that it is the planner which matters and is
effectively doing the inference, not anything to do with the model – i.e. the transition
model – in model-based methods, which is often amortised. We can treat the varieties of
planning algorithms such as CEM and path integral control as optimizing the actions or
action sequences directly over the course of multiple iterations, and thus corresponds to
iterative inference. Indeed Okada and Taniguchi (2020) have shown how these standard
planning algorithms can be derived as variational inference algorithms themselves under
certain conditions.
Chapter 4. Scaling Active Inference 205
To support these identiﬁcations intuitively model-free methods share the same advan-
tages and disadvantages as amortised inference – that they are trained across a whole
dataset but fast to compute for any individual instance, and less sample efﬁcient, since
the amortisation function can only be learnt across a wide range of experience to enable
good generalization. Model-based methods are the opposite and share the properties of
iterative inference approaches. They are very sample efﬁcient and perform well with
very small amounts of data (since planning occurs for each datapoint independently,
the only need for data is in the amortised training of the transition model). However,
they are much more computationally expensive per datapoint, since they must under-
take an iterative planning process for each state, instead of directly mapping a state
to an action, as an amortised policy does. Interestingly, however, for model-free vs
model-based approaches, the amortisation gap is often the other way around. Currently,
model-free amortised policies generally achieve a higher asymptotic accuracy than do
model-based planners (Haarnoja, Zhou, Abbeel, & Levine, 2018; Hafner, Lillicrap,
Ba, & Norouzi, 2019; Shyam et al., 2019). This is for two reasons. Firstly, there is an
additional distinction which must be made between inferring a single action, as is done
by model free policies, and inferring a whole sequence of actions (an action plan) which
is what is typically done by model-based planners (although often this whole sequence
is discarded and recomputed every time, an approach which is called model predictive
control). Inferring a full plan is almost always harder than inferring a single action to
take immediately, and this may be the cause of some of the reverse amortisation gap.
An additional and potentially more serious issue is that current planning methods are
generally quite crude and cannot represent expressive distributions over action plans.
For instance the cross-entropy method can only represent single unimodal Gaussian
plans, and similarly path integral control, the other state of the art method (E. Theodorou
& Todorov, 2012; J. Theodorou Evangelosnd Buchli & Schaal, 2010b; D. Williams,
2018; G. Williams, Aldrich, & Theodorou, 2017; G. Williams, Wagener, et al., 2017)
suffers from similar constraints. While there has been some recent work in improving
Chapter 4. Scaling Active Inference 206
the expressivity of planning methods, such as multimodal CEM (Okada, Kosaka, &
Taniguchi, 2020), much work remains to be done here to be able to match the expressive
capabilities of deep neural network policies.
Finally, it is important to note that the above distinction has revealed an additional
orthogonal dimension of whether it is single actions that are inferred or whole action
plans. We thus see that we can plot reinforcement learning and control methods into a
quadrant with two orthogonal dimensions – whether iterative or amortised inference is
used, and whether full action plans or just single actions are inferred. We thus see that
the standard distinctions of ‘model-free’ vs ‘model-based’ themselves map onto the
diagonal of the quadrant. Model-free reinforcement learning is amortised inference of
single actions, while the standard model-based methods correspond to iterative inference
of full action plans. Importantly, there are several methods on the off diagonal, such
as iLQR (W. Li & Todorov, 2004) which infers single actions in an iterative fashion.
Understanding and plotting reinforcement learning methods in such a way reveals the
full space of methods and which areas are potentially underexplored. For instance, we
immediately see that there are very few, if any, methods which utilize amortised plans,
even though learning amortised plans could well be straightforward and may even be
beneﬁcial. This would then be a fertile area for future work.
4.2.5 Control as Hybrid Inference
4.2.5.1 Introduction
Building on the observation that iterative and amortised inference can be combined
to construct an iterative-amortised inference scheme which combines the advantages
and ameliorates the respective disadvantages of both iterative and amortised inference
– enabling rapid and ﬂexible inference with a high asymptotic performance, and an
adaptive scheme which can leverage additional computing power only where it is most
needed. In this section, we apply iterative-amortised combination to reinforcement
Chapter 4. Scaling Active Inference 207
Amortised
Iterative
Policies Planning
Policy Gradients 
(Williams 1992)
Q-learning 
(Watkins 1989)
Soft Actor-Critic 
(Haarnoja 2018)
PAETS 
(Okada 2019)
MPPI 
(Williams 2017)
CEM 
(Rubinstein 1997)
CMA-ES 
(Hansen 2005)
iLQR 
(Li and Todorov 2004)
PID 
(Baltieri 2019)
SMPC 
(Piche 2019)
PI2 
(Theodorou 2010)
Figure 4.9: Overview of classic RL and control algorithms in our scheme. Standard
model-free RL corresponds to amortised policies, planning algorithms are iterative
planning, and control theory infers iterative policies. The amortised plans quadrant is
empty, perhaps suggesting room for novel algorithms. The position of the algorithms
within the quadrant is not meaningful.
learning which can be seen as combining model-based and model-free RL, using the
identiﬁcation previously developed.
Speciﬁcally, variants of model-based planning algorithms can be derived as variational
inference algorithms using mirror descent to optimize the variational free energy in
an iterative fashion (Okada & Taniguchi, 2020). Additionally, as discussed previously,
model-free reinforcement learning methods such as Q-learning, policy gradients, and
actor-critic can be cast as optimizing another variational free energy bound, but rather
this time in an amortised fashion. Given this, there are multiple potential ways to
combine model-based and model-free reinforcement learning approaches. Perhaps
one of the simplest approaches, which we apply in this study, is to use the model-free
policy as an initialization of the model-based planner. Model-based planning algorithms
such as CEM or MPPI, require an initial action distribution p1(a1:T ) =∏T
t=0 p1(at) to
begin with, which they they proceed to optimize. Usually this initial action distribution
is set to some simple known distribution such as a zero-centred normal distribution
p1(at) =N (a;0,σa) with a variance parameter σa which becomes a hyperparameter of
Chapter 4. Scaling Active Inference 208
the planning algorithm.
Instead, we propose to initialize the planner with the results of the amortised model-free
policy network p1(at) =qφ(xt). To do this for a potential action trajectory requires
knowledge of future states to feed into the model-free policy network. However,
conveniently, the model-based planner also has access to a transition model which is
used to generate these simulated state trajectories, given the actions output by the policy
network.
4.2.5.2 Model and Hyperparameter Details
To make our model concrete, we need to instantiate many distributions such as the tran-
sition models pθ(xt+1|xt,at), the parameterised policy qφ(at:T |xt:T ) and the variational
iterative planning algorithm which instantiates q(at:T |xt;ψ).
The transition model was instantiated as an ensemble of three layer multi-layer percep-
tron networks with a hidden size dimension of size 250, which was trained to output a
Gaussian distribution (mean and variance) over the change in environment state. That
is, rather than explicitly model p(xt+1|xt,at), we instead modelled p(xt+1 −xt|xt,(xt −
xt−1),at), which we could then use to reconstruct the next predicted environmental
state as ˆst+1 = xt +(xt+1 −xt). Training the transition model to predict state differences
instead of the states directly is a common trick used in model-based reinforcement
learning which has been found to signiﬁcantly improve modelling performance by
incorporating explicit information about the derivatives of the states, which is hard
to derive solely from the states themselves. To obtain a measure of uncertainty over
the transition model parameters θ, which can be utilized to drive information-gain
maximizing exploration, we maintained an ensemble of 5 transition models which
were each trained on independently sampled batches of transition data. Each ensemble
possessed independent randomly and uniformly initialized weights.
For the amortised action policy, we utilized the soft-actor-critic architecture (SAC)
Chapter 4. Scaling Active Inference 209
(Haarnoja, Zhou, Abbeel, & Levine, 2018), with a policy-network which consisted of a
three-layer MLP model with a hidden dimension of 256. We did not use an adaptive α
parameter for the SAC agent but set it to 0.2 throughout.
For the iterative planner, we used the standard and powerful CEM algorithm (De Boer,
Kroese, Mannor, & Rubinstein, 2005), with a time-horizon of 7, a number of iterations of
10, and a trajectory sample size of 500. For each generated trajectory, to encourage more
exploration, we added additional action noise sampled independently from∼N (0,0.3).
We maintained a memory buffer of all environmental interactions seen by the agent,
and used various samples to train the transition model and amortised policy network.
We trained the model over 10 epochs which iterated over the full memory buffer, with a
batch size of 50. The full control as hybrid inference algorithm is deﬁned as follows,
Chapter 4. Scaling Active Inference 210
Algorithm 3:Inferring actions via CHI
Input: Planning horizon H | Optimisation iterations I | Number of samples K |
Current state st | Transition distribution pλ(st+1|st,at) | Amortisation function
fφ(·)
Amortised Inference:
pφ(τ) =δ(st)∏T
t′=t pλ(st′+1|st′,at′)qφ(at′|st′;θ)
Extract θ(1) = {µt:T ,σ2
t:T }from pφ(τ)
Initialise q(a;θ) with parameters θ(1)
Iterative Inference:
for optimisation iteration i = 1...I do
Sample K action sequences {(a)k ∼q(a;θ)}K
k=1
Initialise particle weights W(i) := {w(i)
k }K
k=1
for action sequence k = 1...K do
w(i+1)
k ← W
(
(a)k
)
·q(i)
(
(a)k;θ
)
∑K
j=1
[
W
(
(a)j
)
·q(i)
(
(a)j;θ
)]
θ(i+1) ←reﬁt
(
W(i+1)
end
end
Extract µt:T from q(a;θ)
return µt
4.2.5.3 Related Work
There has been a small amount of prior work aiming at combining model-free and
model-based (Che et al., 2018; S. Li, 2020). For instance, a strand of research has
focused on using a learned transition model to generate additional simulated data which
can then be used to train a model-free policy ‘ofﬂine’. This approach was pioneered
with the Dyna architecture (Sutton, 1991), but has also been extended and applied in
more modern deep reinforcement learning settings (Gu, Lillicrap, Sutskever, & Levine,
Chapter 4. Scaling Active Inference 211
2016). Conversely, in (Farshidian, Neunert, & Buchli, 2014) and (Nagabandi, Kahn,
Fearing, & Levine, 2018) a model-based planner was used to initialize a model-free
policy – the opposite direction to our model. Our approach does share similarities with
the approach used in AlphaGo (Silver et al., 2017) which used learned amortized policy
networks to generate proposals for the monte-carlo-tree-search (MCTS) used to select
moves in that approach. However, their approach was justiﬁed on heuristic grounds and
they did not consider how their approach corresponds to a mathematically principled
combination of iterative and amortised variational inference. Indeed, it is not yet clear
if the MCTS algorithm can be cast as performing some kind of variational inference or
not.
While we are the ﬁrst to consider the combination of amortised and iterative inference
in reinforcement learning, and to make the connection to model-based and model-free
methods, there is a line of work combining the two approaches to inference in the
context of unsupervised generative modelling, typically using variational autoencoders.
(H. Kim, Kim, Jeong, Levine, & Song, 2018), employ amortised inference in a V AE to
initialize the set of variational parameters which are then optimized directly against the
ELBO. A similar approach was taken by Marino (Marino et al., 2018), who showed that
by repeatedly encoding the gradients and optimizing the variational parameters against
the ELBO, which was found empirically to improve performance and help narrow the
amortisation gap.
4.2.5.4 Results
We tested our hybrid agent ﬁrst on a didactic toy continuous control task. The goal of
this task was to simply explore how the iterative and amortised control schemes interact.
The environment was a simple 2-D planar environment, where the agent began in the
bottom-left corner, and where the goal was to arrive in the top-right corner. The reward
signal was a smooth gradient ﬁeld leading to the top-right which was implemented as
r(x,y) =1 −(||(x,y)−(gx,gy)||2) where gx and gy represent the x and y coordinates of
Chapter 4. Scaling Active Inference 212
Figure 4.10: (a - c) : Amortised predictions of qφ(a|s;θ) are shown in red, where •
denote the expected states, shaded areas denote the predicted actions variance at each
step, and the expected trajectory recovered by iterative inference is shown in blue. At
the onset of learning (a), the amortised predictions are highly uncertain, and thus have
little inﬂuence on the ﬁnal approximate posterior. As the amortised model fφ(·) learns
(b), the certainty of the amortised predictions increase, such that the ﬁnal posterior
remains closer to the initial amortised guess. At convergence, (c), the iterative phase of
inference has negligible inﬂuence on the ﬁnal distribution, suggesting convergence to a
model-free algorithm. (d) Here, we compare our algorithm to its constituent components
– the soft-actor critic (SAC) and an MPC algorithm based on the cross-entropy method
(CEM). These results demonstrate that the hybrid model signiﬁcantly outperforms both
of these methods.
the goal state. In the centre of the environment there was an impassable wall except for
a small opening through which the agent could pass. The agent could control its x and y
velocity – a = (˙x, ˙y) with a maximum velocity of 0.05 and a minimum velocity of -0.05.
The graph shows the evolution of the agent’s iterative and amortised policies as it learns
to complete the task. As can be seen, the iterative policy starts out highly uncertain, with
a high variance. As the amortised policy is slowly learnt, the variance of the iterative
policy shrinks, and the resulting policy closely matches the amortised policies. This
immediately suggests an adaptive method of saving computation – when the variance of
the iterative policy is small, or the iterative policy is very close to the amortised policy,
rely solely on the computationally cheap amortised policy only. Conversely, when
Chapter 4. Scaling Active Inference 213
the amortised policy or the iterative policy is highly uncertain (as at the beginning of
training), then the computationally expensive model-predictive-control of the iterative
policy should be utilized. In this way, the agent can attain the impressive sample
efﬁciency and rapid performance of model-based planning at the beginning of training,
when the amortised policy is poor, but then once the amortised policy becomes good,
the agent can simply rely on that and thus achieve the high asymptotic performance and
relative computational cheapness of model-free RL.
We also compared the hybrid agent on a challenging continuous control task – HalfChee-
tah Run S17,A6. This environment requires the agent to take control of a bipedal
simulated cheetah in a planar environment with semi-realistic physics. The agent’s goal
is to move the cheetah’s limbs in such a way as to maximize the overall velocity of the
cheetah, while simultaneously minimizing the total action applied. The reward function
for the task was r = v −0.1a2 where v denotes the overall velocity of the cheetah.
The hybrid agent was evaluated against strong model-free (SAC) and model-based
(CEM) baselines. As can be seen from Figure 4.10 the hybrid agent signiﬁcantly outper-
forms both baselines and simultaneously achieves the sample efﬁciency of model-based
methods with superior performance to the model-free SAC agent.
4.2.5.5 Interim Discussion
Empirically, we ﬁnd that the hybrid agent performs well, and that the interaction of the
iterative and amortised inference components allow for a natural adaptive scheme to
switch between and apportion computation in a way that maximizes the computational
resources available to the agent. Moreover, the use of an amortised policy to initialize the
iterative planner cuts signiﬁcantly down on the computational expense of the planner and
tends to stabilize performance. Additionally, the use of the iterative planner at the start
means that the agent rapidly discovers highly rewarding trajectories which are then used
to train the SAC agent, and thus provides a powerful source of implicit exploration for
the model-free SAC agent. Interestingly, however, this highly rewarding data generated
Chapter 4. Scaling Active Inference 214
by the iterative planner comes with a cost – it is heavily biased towards positive
trajectories, and thus the SAC agent, as it is not exposed directly to negative trajectories
in the real world, simply does not learn them and thus learns a highly optimistic value
function, which performs poorly when interacting with the real environment. We call
this the data-bias issue and left unchecked it inhibited the performance and learning of
the algorithm.
To ameliorate the data-bias issue, we instead train the SAC agent from the simulated
rollouts of the iterative planner. These rollouts, especially in the early stages of iteration,
contain many examples of (predicted) negative trajectories, which thus helps render
the dataset fed to the model-free SAC agent less positively biased. These rollouts do
have their own difﬁculties – namely that they are fundamentally from a simulation and
thus may be a poor representation of the actual dynamics of the world (especially when
the transition model is poor). Secondly, the rollouts are still biased to some degree by
the operation of the iterative planner even in the early stages of iteration. Additionally,
this becomes more acute as the model-free policy becomes better, as it learns to avoid
negative contingencies and its action predictions are then used to initialize the planner,
thus creating a compounding positive bias to the data that is fed into the SAC agent.
Nevertheless, we ﬁnd empirically that this solution sufﬁces to train a high performance
model-free policy network.
On a more theoretical level, it is important to note that we chose a relatively straight-
forward scheme of combination – using the amortised policy to simply initialize the
model-based planner. A more involved, but slightly more principled method may be to
set the action prior of the iterative planner (which is currently assumed uniform, as is
standard in the control as inference framework) to the amortised policyp(a|s) =qφ(a|s).
Using this method, instead of a direct initialization, the amortised policy would serve as
a regularizer on the iterative model-based planner, ensuring that the resulting iterative
policy is penalized for its divergence from the model-free amortised policy. Such
Chapter 4. Scaling Active Inference 215
regularisation methods have been found to be beneﬁcial for the stability of learning in a
number of reinforcement learning algorithms and especially in policy gradient methods,
where methods such as PPO (Schulman et al., 2017) reach state of the art performance.
4.3 Conclusion
In this chapter, we have investigated the application of methods derived from the free
energy principle – speciﬁcally active inference – to the general problem of optimal
action selection and control. We have focused especially on a core limitation of current
active inference methods: their scalability. We have demonstrated how many of the
key distributions which arise out of the free energy objective can be parametrized using
deep neural networks, to derive schemes which can look very similar to contemporary
deep reinforcement methods – both model-free and model-based methods. We show
that these methods – which we call deep active inference approaches – can perform
comparably and often better to their deep reinforcement learning counterparts.
Speciﬁcally, in the ﬁrst study presented in this chapter, we showcase how active in-
ference can be interpreted through the lens of model-free reinforcement learning. In
this case, we use a learnt action policy q(a|s) and parametrize the action prior using
an amortized expected-free energy value network, to approximate the required path
integral over the expected free energy. The resulting algorithm looks very similar to
actor-critic methods in model-free reinforcement learning, but using the expected free
energy instead of the reward. Additionally, we ﬁnd that this approach also utilizes
additional entropy regularisation terms which can be shown to substantially improve the
stability and the performance of the resulting algorithm – thus demonstrating how in-
sights and the mathematical formalism of active inference can also lead to improvements
in reinforcement learning algorithms.
In the second study, we instead approximate the path integral of the expected free
energy, with monte-carlo sampling of trajectories and ultimately use a model-predictive
Chapter 4. Scaling Active Inference 216
control planning algorithm to compute optimal trajectories instead of an amortised
policy network. This simple change is sufﬁcient to move us into the realm of model-
based reinforcement learning. Here, we show that active inference can again attain
the performance of comparable model-based deep reinforcement learning algorithms,
and can be applied to solve challenging continuous control tasks. Additionally, here
the superior exploratory capabilities of the expected-free energy functional come into
play, and we see that they allow the construction of powerfully exploratory goal-
directed, information-seeking behaviours, which can solve very challenging sparse
reward tasks, such as the mountain car, with ease. This demonstrates another way in
which insights from active inference can aid the development of deep reinforcement
learning algorithms.
We then turn to a more abstract consideration of the difference between model-based and
model-free reinforcement learning in terms of inference – an insight which is heavily
enabled by the active inference formulation of action selection as fundamentally an
inference problem. We demonstrate that we can see the distinction between model-free
and model-based as simply that of iterative vs amortized inference, where iterative
variational inference directly optimizes the parameters of the variational distribution,
while amortized inference instead optimizes the parameters of a mapping function
which maps observations directly to variational parameters. We then show how there
is a separate dichotomy between whether policies or plans are inferred, and that this
provides us with a simple two dimensional quadrant scheme upon which to place all
major reinforcement learning algorithms. It also demonstrates that there are several
areas which are underexplored in the literature – especially the direct computation of
amortized plans.
Finally, we use this insight into the nature of model-based and model-free reinforcement
learning in terms of iterative and amortized inference to ask how these two approaches
can be combined. We show that this can yield powerful algorithms which can com-
Chapter 4. Scaling Active Inference 217
bine both the sample-efﬁciency and rapid learning of model-based planning with the
asymptotic performance and computational cheapness of model-free reinforcement
learning. Importantly, investigations into this ﬁeld of combined or hybrid reinforcement
learning algorithms are only just beginning, and there are many design choices left to
be extensively investigated in future work.
Overall, crucially, we have shown that the free energy principle and active inference can
be successfully applied and scaled up to handle large and challenging control tasks and
to create algorithms which perform comparably with state of the art methods in deep
reinforcement learning.
In the next chapter, we extend the intuitions provided here about the importance of
combining exploration and exploitation and turn to a more abstract and mathematical
analysis of what kind of mathematical procedure gives rise to the combination of
exploratory and epistemic action that characterise such objective functionals as the
Expected Free Energy and the Free Energy of the Expected Future.
Chapter 5
The Mathematical Origins of
Exploration
5.1 Introduction
In the previous chapter, we have seen the importance and beneﬁts ofinformation-seeking
as opposed to random exploration for reinforcement learning tasks. Information-seeking
exploration, which explicitly aims to reduce uncertainty about either the environment
or the agent’s model of the environment, provides a powerful exploration strategy that
allows the rapid and efﬁcient exploration of an environment, as opposed to the random
walk strategy employed by random exploration. Moreover, when combined in a single
loss function with a reward maximizing term, this combination results in goal-oriented
exploration where the agent is only driven to explore contingencies which are also
likely to lead to high reward. We have seen that this goal-directed, or goal-oriented,
exploration mechanism has performed well in model-based reinforcement learning tasks
including sparse-reward environments which are challenging for standard reinforcement
learning agents. Moreover, this kind of exploration is almost certainly necessary for
biological organisms in more ecologically valid tasks, where rewards are often very
218
Chapter 5. The Mathematical Origins of Exploration 219
sparse and environments are typically very large compared to those in mainstream
reinforcement learning benchmarks.
In this chapter, we take a more abstract perspective, and study in depth the question of the
mathematical origin and meaning of such goal-directed exploration objectives which
unite both reward seeking and information maximizing terms in a single objective.
Speciﬁcally, we seek to understand whence they arise, and what the mathematical
formulation which can give rise to them is. While for practical purposes and engineering
applications it is often sufﬁcient to glue different terms together in an ad-hoc way to
construct an objective which gives rise to some desired behaviour, we wish to probe the
deeper theory underlying such functionals which has so far remained mostly mysterious.
It is the hope that by mathematically understanding the origin and nature of such
objectives, as well as their properties, we can illuminate a swathe of current methods in
reinforcement learning, cognitive sciences, decision theory, and behavioural economics,
as well as deeply understanding how they interrelate to one another. Moreover, it seems
likely, given the generally productive dialectic between theory and practice in all of
these ﬁelds, that by contributing to the underlying theory of such objectives, we can
ultimately contribute to the design of more powerful objectives and methods than are
currently used in the literature.
To begin, we wish to deeply examine the origin and nature of the Expected Free Energy
(EFE) functional. The EFE is central to the theory of active inference, where it is
proposed that all agents under the free energy principle, which must seek to minimize
the long term path integral of their surprise must choose policies that minimize the EFE.
The EFE has been widely used in almost all models in discrete-time active inference
(Da Costa, Parr, et al., 2020; Friston, FitzGerald, et al., 2017a, 2017b; Friston, Rigoli,
et al., 2015b; Friston, Rosch, et al., 2018a) with the exception of the later development
of the generalized free energy (Friston, Rigoli, et al., 2015a; Parr & Friston, 2017a,
2017b).Despite this ubiquitous use within the active inference community, the precise
Chapter 5. The Mathematical Origins of Exploration 220
mathematical origin and nature of the EFE functional have remained unclear. In the
literature, the EFE is often justiﬁed through a reductio-ad-absurdum argument (Friston,
Rigoli, et al., 2015a) which runs as follows – since (we assume under the FEP) all
agents minimize free energy, then they must think they will minimize free energy in
the future. Since the future is uncertain, instead of the standard variational free energy
(VFE), they must instead minimize their expected free energy (EFE), else they are not a
free-energy minimizing agent (disproven conclusion)1. Central to this logic is the claim
that the EFE is the ‘natural’ extension of the VFE to account for uncertain futures.
In the ﬁrst section of this chapter, we investigate this claim in detail. Speciﬁcally, we
argue that the EFE is not necessarily the only way to extend the VFE into the future,
and that there are in fact other, more straightforward extensions, such as an objective
we call the free energy of the future (FEF). We then perform a direct side-by-side
comparison of the EFE and FEF functionals and comment on their similarities and
differences, and discuss their respective bounding behaviour on the expected free energy.
We then discuss how active inference approaches are related to the control as infrerence
framework, and decide upon two key differences – the objective functional utilized for
action selection, where active inference uses the EFE, and control as inference uses
the FEF, and secondly the encoding of value or goals into the inference process, where
active inference directly encodes values into the generative model through the use of a
biased desire distribution ˜p(o), control as inference instead uses independent optimality
variables p(Ω|o) 2. Finally, we then introduce a second objective functional, which we
call the free energy of the Expected Future (FEEF) which combines both an intuitively
grounded starting point with the same exploration seeking term as is present in the EFE,
and which was investigated previously in Chapter 4. We discuss the nature of different
possible objective functionals for control.
1Technically this is more of an induction argument than a reductio-ad-absurdum, but we still refer to
it as such due to its description as a reductio in the literature (Friston, Rigoli, et al., 2015a)
2We also denote any distribution involving a desire distribution with a ˜p and, for instance, refer to
˜p(o,x) as a biased generative model)
Chapter 5. The Mathematical Origins of Exploration 221
In the second half of this chapter, we retreat from the speciﬁcs of the EFE, active infer-
ence, and control as inference, to instead deﬁne a general framework for understanding
the origin of information seeking exploration terms in control functionals. We argue that
the key distinction, is that between evidence objectives, which maximize the likelihood
of achieving a desire distribution, with divergence functionals which try to minimize
the divergence between a predicted and desire functional. Speciﬁcally, divergence
objectives give rise to information gain terms while evidence objectives do not. We
trace this capacity to the fact that divergence objectives implicitly maximize the entropy
of the agent’s future, in a close connection to empowerment objectives, while evidence
objectives do not. Finally, we put all this together into a coherent framework which can
be used to understand the full landscape of variational objective functionals for control
tasks.
The material in this chapter is heavily based on three ﬁrst-author papers. Whence the
expected free energy (Millidge, Tschantz, & Buckley, 2020b) (published in neural
computation), On the relationship between active inference and control as inference
(published at the IEEE international workshop on active inference) (Millidge, Tschantz,
Seth, & Buckley, 2020b), and (Millidge, Tschantz, Seth, & Buckley, 2021) Under-
standing the Origin of Information-Seeking Exploration in Probabilistic Objectives for
Control, Arxiv (to be submitted to Royal Society Interface).
5.2 Origins of the Expected Free Energy
Here we investigate the origins of the expected free energy (EFE) term within active
inference. It is often claimed that the reason active inference agents minimize this
term is that free energy minimizing agents must minimize the variational free energy
(VFE) into the future which, since the future is uncertain, constitutes the expected free
energy. To make this claim precise, we need to understand exactly the variational free
energy ‘into the future’ should consist of. We argue that it must satisfy two conditions,
Chapter 5. The Mathematical Origins of Exploration 222
which are both satisﬁed by the variational free energy, and which are crucial for that
objective functions to operate. First, we argue that, like the VFE, the VFE extended into
the future should be a divergence between a variational approximate posterior and a
generative model of future states. Secondly, we argue that, again like the VFE, any free
energy of the future should additionally be a bound on the log model evidence of future
observations. These conditions are both important precisely because they deﬁne why
the variational free energy is useful. Minimizing the divergence between the posterior
and the generative model is useful since it implicitly makes the variational posterior a
good approximation. Conversely, bounding the log model evidence is useful since the
log-model evidence provides a very general measure of how ‘good’ a speciﬁc model
is, which can be used for Bayesian model-comparison or even just to understand the
amount of inherent information in the data. Moreover, the log model evidence has
especial import for methods under the aegis of the free energy principle since the log
model evidence is simply the surprisal −ln p(o) which is the basic quantity which is
minimized throughout the theory.
First, we need to deﬁne precisely the mathematical setup of the problem. We assume
that our agent exists in a POMDP environment with states x, observations o, policies
(sequences of actions) π = [a1,a2 . . .aT ]. The agent maintains a variational distribution
over the states and actions and a generative model over the states, observations, and
policies. Speciﬁcally, although we are technically interested in the functionals over a
full trajectory o1:T , in practice the functional decomposes into a sum of independent
functionals for each timestep. Thus, for understanding the behaviour of agents opti-
mizing the functional, it sufﬁces to consider only a single timestep of the functional at
ot.
We argue that the expected free energy does not fulﬁl these conditions, but rather another
objective functional does, which we call the free energy of the future (FEF). We deﬁne
Chapter 5. The Mathematical Origins of Exploration 223
the FEF to be,
FEFt(π) =Eq(ot ,xt |π)[lnq(xt|ot)−ln ˜p(ot,xt)]
= Eq(ot )DKL[q(xt|ot)||˜p(ot,xt)] (5.1)
which is simply the KL divergence between the approximate posterior generative model
over future states, averaged under the expected future observations q(ot). This trivially
satisifes the ﬁrst condition, since it is a KL divergence between the variational posterior,
and the generative model, as is the VFE. Next, we show that this functional is a bound
on the expected log model evidence in the future.
−Eq(ot |π)
[
ln ˜p(ot)
]
= −Eq(ot |π)
[
ln
∫
dxt ˜p(ot,xt)
]
(5.2)
= −Eq(ot |π)
[
ln
∫
dxt ˜p(ot,xt)q(xt|ot)
q(xt|ot)
]
≤−Eq(ot |π)
∫
dxtq(xt|ot)
[
ln ˜p(ot,xt)
q(xt|ot)
]
≤−Eq(ot ,xt |π)
[
ln ˜p(ot,xt)
q(xt|ot)
]
≤Eq(ot ,xt |π)
[
ln q(xt|ot)
˜p(ot,xt)
]
≤Eq(ot |π)DKL[q(x|ot)||˜p(ot,xt|π)] =FEF(π) (5.3)
Crucially, we can see that this is an upper bound on the log model evidence, and thus
minimizing the FEF will tend to decrease the gap between the FEF and the expected
log-model evidence. This functional thus exhibits identical behaviour to the VFE. To
gain a better understanding of the key differences between the EFE and the FEF, we
can exhibit them side by side.
FEF = Eq(ot ,xt |π)[lnq(xt|ot)−ln ˜p(ot,xt)]
EFE = Eq(ot ,xt |π)[lnq(xt|π)−ln ˜p(ot,xt)] (5.4)
Chapter 5. The Mathematical Origins of Exploration 224
While the two formulations may look very similar, the key distinction is that the FEF
optimizes the divergence between the variational posterior q(xt|ot) and the generative
model while the EFE minimizes the variational prior q(xt). While this difference
may seem small, we see that it has a signiﬁcant impact when it comes to the resulting
interpretable terms from the decomposition of the two functionals,
FEF = −Eq(ot ,xt |π)
[
ln ˜p(ot|xt)
]
  
Extrinsic Value
+Eq(ot |π)DKL[q(xt|ot)||q(xt|π)]  
Epistemic Value
(5.5)
EFE = −Eq(ot ,xt |π)
[
ln ˜p(ot)
]
  
Extrinsic Value
−Eq(ot |π)DKL[q(xt|ot)||q(xt|π)]  
Epistemic Value
(5.6)
Speciﬁcally, we see that while both the FEF and the EFE can be split into ‘extrinsic’ and
‘intrinsic’ value terms, the intrinsic value term in the FEF is positive while in the EFE it
is negative. Speciﬁcally this means that the FEF tries to minimize exploration and keep
the posterior and prior as close together as possible. This minimizing information gain
term is analogous to the complexity term in the VFE which functions as a regulariser
which attempts to keep the posterior as close to the prior as possible, while still ﬁtting
the data. Here, we see that the goal of the FEF is to, in effect, maximize reward, while
trying to learn as little about the environment as possible. While this may seem to be an
unfortunate objective, in some small cases it may be beneﬁcial, especially when in the
case of ofﬂine reinforcement learning, where there is no continual interaction with an
environment, only trying to learn an optimal policy from a given dataset of interactions
((Levine, 2018). In such cases, failures of generalization and extrapolation can often
result in poor results whenever the learned policy is moved even slightly off the data
manifold, and this kind of conservative regularisation of the learning process can prove
highly beneﬁcial (Levine, Kumar, Tucker, & Fu, 2020). By contrast, the EFEmaximizes
the information gain term, since it is negative, and tries to drive the posterior and prior
as far apart as possible. This results in information-seeking exploration.
However, while the EFE has an intuitively better exploratory grounding, it is not a
Chapter 5. The Mathematical Origins of Exploration 225
bound on the log model evidence, as the FEF is. We can show this straightforwardly by
noting that the ‘extrinsic value’ term of the EFE simplyis the log model evidence,
EFE = Eq(ot ,xt |π)[lnq(xt|π)−ln ˜p(ot,xt)]
≈Eq(ot ,xt |π)[lnq(xt|π)−lnq(xt|ot)−ln ˜p(ot)]
≈ − Eq(ot |π)[ln ˜p(ot)]  
Negative Expected Log Model Evidence
−Eq(ot |π)DKL[q(xt|ot)∥q(xt|π)]|  
Information Gain
(5.7)
and that thus by the non-negativity of KL divergences, the EFE is a lower bound on the
log model evidence. This bound is in the wrong direction, since to make it tight, the
EFE should be maximized instead of minimized.
Importantly, in the deﬁnition of the EFE there is an approximation step where we
have approximated p(xt|ot) with the approximate posterior q(xt|ot). If we make this
approximation explicit, we can write the EFE as,
EFE = Eq(ot ,xt |π)[lnq(xt|π)−ln ˜p(ot,xt)]
≈Eq(ot ,xt |π)[lnq(xt|π)−ln p(xt|ot)−ln ˜p(ot)]
≈Eq(ot ,xt |π)[lnq(xt|π)−ln p(xt|ot)−ln ˜p(ot)+ lnq(xt|ot)−lnq(xt|ot)]
≈ − Eq(ot |π)[ln ˜p(ot)]  
Negative Expected Log Model Evidence
+Eq(ot |π)DKL[q(xt|ot)∥p(xt|ot)]|  
Posterior Approximation Error
  
FEF
−Eq(ot |π)DKL[q(xt|ot)∥q(xt|π)]|  
Information Gain
(5.8)
Where we see that the EFE can be both an upper and lower bound on the log model
evidence depending on whether the information gain term or the posterior divergence
term is larger. We can thus see that the likely time-course of the EFE is to cycle
around the bound over the course of inference until, potentially, it reaches it. This is
because, at the start of training, when inference is poor, the posterior divergence is
likely greater than the information gain, so the EFE functions as an upper bound and
Chapter 5. The Mathematical Origins of Exploration 226
minimizing it gets us closer to the true log model evidence. This effect likely quickly
fades away as the information gain term becomes bigger, and here the EFE minimizing
agent will preferentially explore its environment in an information-seeking fashion,
driving the EFE away from the real log model evidence for the environment. Finally,
if there are no residual sources either of posterior divergence (so that the true and
approximate posteriors are in the same class), or information gain (so that the agent has
a perfect model of the environment, and the environment has no intrinsic stochasticity
which gives rise to aleatoric uncertainty), then both the posterior divergence and the
information gain terms will be 0, and the EFE will ﬁnally converge to exactly the log
model evidence. While this behaviour of the EFE functional may lead to adaptive
behaviour, it is not particularly mathematically principled as an extension to the VFE,
and thus it is not necessarily clear why the EFE should be considered to be a better
extension of the VFE than the FEF.
This derivation also reveals an interesting connection between the EFE and the FEF.
Speciﬁcally, it is revealed that the EFE is simply the FEF minus an additional informa-
tion gain term, thus effectively comprising the free energy into the future (FEF), with
an additional exploratory information gain term. This derivation can also be derived
straightforwardly from a direct comparison of the two functionals,
FEFt(π)−IGt = Eq(ot ,xt |π) ln( q(xt|ot)
˜p(ot,xt))−Eq(ot ,xt |π) ln(q(xt|ot)
q(xt|π) )
= Eq(ot ,xt |π) ln( q(xt|ot)q(xt|π)
˜p(ot,xt)q(xt|ot))
= Eq(ot ,xt |π) ln( q(xt|π)
˜p(ot,xt))
= EFE(π)t (5.9)
We can thus understand the origin of the information gain term in the EFE – it is simply
the FEF into the future minus the information gain exploration term. This means that,
in effect, the exploratory properties of the EFE are simply present by construction. Is
Chapter 5. The Mathematical Origins of Exploration 227
it possible, then, to derive mathematically or intuitively principled objectives which
maintain the information seeking properties of the EFE?
While this question is deﬁnitively answered later in this chapter, here we present a
hint of the solution. We propose a novel objective, which we call the free energy
of the expected future (FEEF), which can be characterised simply as the divergence
between the expected beliefs about future observations and states q(ot,xt) and the
desired distribution ˜p(ot,xt). The FEEF objective can be written as,
π∗= argmin
π
DKL[q(ot:T ,xt:T |π)||˜p(ot:T ,xt:T )] (5.10)
In effect, this objective can be understood as compelling an agent to bring a predicted
(variational) world and a desired (generative) distribution into alignment. This objective
has a strong intuitive basis for understanding adaptive action, since we are simply trying
to minimize the difference between our veridical beliefs about the future and our desires.
Since the desire distribution is assumed ﬁxed, the only way to maximize their alignment
is to take action to force the predicted belief distribution towards the desired distribution.
If the belief distribution is accurate, then this will result in trajectories that really do
take the agent towards its desired distribution. Crucially, we can then decompose this
objective into an extrinsic and intrinsic information seeking term, just like the EFE.
FEEF(π)t = Eq(ot ,xt |π) ln
[q(ot,xt|π)
˜p(ot,xt)
]
≈Eq(xt |π)DKL
[
q(ot|xt)∥˜p(ot)
]
  
Extrinsic Value
−Eq(ot |π)DKL
[
q(xt|ot)∥q(xt|π)
]
  
Intrinsic Value
(5.11)
Here, we see that the epistemic information seeking term is identical to that of the EFE,
and thus we would expect FEEF and EFE minimizing agents to show similar exploratory
behaviour. The key difference between these objectives lies in the extrinsic value term.
While the EFE simply aims to maximize the likelihood of the desire distribution
Chapter 5. The Mathematical Origins of Exploration 228
under the variational belief distribution, the FEEF explicitly tries to minimize the KL
divergence between them, and thus try to match the two distributions.
Another way of looking at the same thing is to consider this straightforward relationship
between the EFE and the FEEF,
FEEF(π)t = DKL
[
q(ot,xt)∥˜p(ot,xt)
]
= Eq(ot ,xt )
[
lnq(ot|xt)]  
Observation Likelihood
+Eq(ot ,xt )
[
ln ˜p(ot|xt)]−Eq(ot |π)DKL
[
q(xt|ot)∥q(xt|π)
]
  
EFE
(5.12)
We can thus see, that the FEEF is simply the EFE plus an observation likelihood entropy
term. This term is to be maximized and thus effectively provides an additional source
of random exploration to the FEEF agent rather than the EFE. In effect, the FEEF agent
optimizes the EFE while trying to keep its observation mapping as random as possible.
Another advantage of the FEEF, is that it is equivalent to the VFE at the present time.
This is because the observation entropy term is constant since it cannot be affected
by future observations, and thus the expression as a whole reduces to the VFE. This
means that the FEEF can be used as a uniﬁed objective for both perception and action,
while the EFE can only be used for control. Due to this, a FEEF agent can have all
distributions trained jointly on the FEEF objective while for an active inference agent,
typically, if the transition and likelihood matrices are learnt, they are optimized against
the VFE, while only action selection takes place using the EFE. This adds an additional
degree of simplicity and elegance to FEEF-minimzing agents while they retain the same
exploratory behaviours as active inference agents.
5.2.1 Control as Inference and Active Inference
This relationship between the FEF and the EFE sheds light upon the relationship
between active inference and control as inference approaches to control. While the
formulations at an abstract level are very similar – both attempt to solve the control
Chapter 5. The Mathematical Origins of Exploration 229
problem by deriving inference algorithms which operate on graphical models, and
usually utilize the machinery of variational inference to do so – at a lower detailed
level the theories appear quite different and are presented with substantially differing
motivations and notation. Using our newfound understanding of variational objective
functionals of the future, such as the EFE and the FEF, here we pin down what exactly
the relationship between control as inference and active inference is.
First, we note that there are many straightforward notational differences between the
theories which can be overcome. One obvious difference is that active inference is
primarily concerned with the inferring of policies (or action sequences) while control
as inference concerns itself with simply inferring policies, or single actions for a given
timestep. It is important to note, however, that it is possible to reformulate active
inference so that it infers policies, and, conversely, to reformulate control as inference
so that it infers full action plans. A second distinction is that active inference is typically
formulated for POMDPs while control as inference only for MDPs. It is straightforward,
however, to extend control as inference to the POMDP setup, which results in the
following objective,
L(φ)CAI = DKL
(
qφ(xt,at)∥p(xt,at,ot,Ωt)
)
= −Eqφ(xt ,at )
[
ln p(Ω|xt,at)
]
  
Extrinsic Value
+DKL
(
q(xt)∥p(xt|xt−1,at−1)
)
  
State divergence
+Eq(xt )
[
DKL
(
qφ(at|xt)∥p(at|xt)
)]
  
Action Divergence
−Eqφ(xt ,at )
[
ln p(ot|xt)
]
  
Observation Ambiguity
(5.13)
Here we have used notation standard in control as inference derivations, namelyqφ(at|xt)
is an amortized state-action policy and Ωt is the ‘optimality variable’. Importantly,
this novel extension of control-as-inference to a POMDP setting leads directly to a
straightforward implementation in terms of deep reinforcement learning, similar to the
approaches in Chapter 4. Speciﬁcally, this objective can either be expressed directly
as a sum over trajectories, and thus optimized by planning algorithms using model-
based deep reinforcement learning or, alternatively, it can be expressed recursively and
Chapter 5. The Mathematical Origins of Exploration 230
computed using a value or Q function approach which lends itself naturally to model-
free deep reinforcement learning approaches. The key extension would be learning
a probabilistic encoder-decoder model, most likely a V AE, to infer the distributions
q(xt|ot) and p(ot|xt) and then to optimize the entropy of the V AE decoder in the control
objective, in accordance with this objective function. Empirically investigating the
performance of this method, and the impact of the observation ambiguity term, has not,
to my knowledge, been explored in the literature, and would be an interesting avenue
for further work.
Secondly, we can similarly reformulate the control as inference approach to infer plans
instead of policies. This is done by extending the generative model to cover whole
trajectories instead of single observations, states, or actions. We then infer a constant
random variable π the policy for the whole trajectory. Written out explicitly, from
this generative model you can derive a variant of the active inference optimal plan
derivation to discover that the optimal plan under the control as inference is simply the
softmax path integral over the variational free energy (VFE), augmented with optimality
variables, and extended into the future.
LCAI = DKL
(
q(xt:T ,π)∥p(xt:T ,π,ot:T ,Ωt:T )
)
= DKL
(
q(π)
T
∏
t
q(xt|π)∥p(π)
T
∏
t
p(Ωt|xt,π)p(ot|xt)p(xt|xt−1,π)
)
= DKL
(
q(π)
T
∑
t
DKL
[
q(xt|π)∥p(Ωt|xt,π)p(ot|xt)p(xt|xt−1,π)
]
∥p(π)
)
= DKL
(
q(π)∥p(π)exp(−
T
∑
t
Lt(π))
)
=⇒q∗(π) =σ
(
p(π)−
T
∑
t
Lt(π)
)
(5.14)
We can decompose this VFE functional into the future as,
Chapter 5. The Mathematical Origins of Exploration 231
Lt(π)CAI = Eq(xt |π)
[
lnq(xt|π)−ln p(xt,π,ot,Ωt)]
= −Eq(xt |π)
[
ln p(Ωt|xt,π)
]
  
Extrinsic Value
+DKL
(
q(xt|π)∥p(xt|xt−1,π)
)
  
State divergence
−Eq(xt |π)
[
ln p(ot|xt)
]
  
Observation Ambiguity
(5.15)
Which we can see is equivalent to the standard control as inference POMDP objective,
except that it is missing the action divergence terms. The action divergence terms
are missing simply because full policies are inferred instead of individual actions.
Conversely, we can rederive active inference to infer individual actions rather than full
policies. To do so, we simply need to add individual actions into the generative model
and variational density and then crank through the derivation,
−Ft(φ) =Eq(ot ,xt ,at )
[
lnqφ(at,xt)−ln ˜p(xt,ot,at)
]
= −Eq(ot |at )
[
ln ˜p(ot|at)
]
  
Extrinsic Value
−Eq(ot ,at |xt )
[
DKL
(
q(xt|ot,at)∥q(xt|at)
)]
  
Intrinsic Value
+Eq(xt )
[
DKL
(
qφ(at|xt)∥p(at|xt)
)]
  
Action Divergence
(5.16)
Here we see that the expression to be optimized with respect to the policy parameters φ
is simply the standard active inference objective with an additional action-divergence
term. If we assume the action prior p(at|xt) is uniform, then we regain the well known
control as inference policy entropy term. Now that we have extended the theories to
allow for a direct side-by-side comparison, we can see that the two major differences
lie in the information gain term for active inference as opposed to the complexity ‘state-
divergence’ term for control as inference, and secondly that the control as inference
approach contains an additional ‘likelihood entropy’ term in its objective which active
inference lacks. We know now that the information gain term in active inference arises
directly from the deﬁnition of the EFE functional, which is not an intrinsic part of active
Chapter 5. The Mathematical Origins of Exploration 232
inference and may not be theoretically justiﬁed. Indeed, if we replace the EFE in the
active inference derivation with the FEF, we can obtain an objective identical to the
control as inference approach except that it has no likelihood entropy term.
−ˆFt(φ) =Eqφ(xt ,ot ,at )
[
lnqφ(xt,at)−ln ˜p(ot,xt,at)
]
= −Eqφ(xt ,at )
[
ln ˜p(ot|xt)
]
  
Extrinsic Value
+DKL
(
q(xt)∥p(xt|xt−1,at−1)
)
  
State divergence
+DKL
(
qφ(at|xt)∥p(at|xt)
)
  
Action Divergence
(5.17)
This means that, effectively, we can consider control as inference as active inference
with a FEF objective or, conversely, that active inference is simply control as inference
with a nonstandard EFE objective. The question then remains, why does the control as
inference objective possess an additional likelihood entropy term which active inference
does not, since it is not due to a difference in objective function. We demonstrate
that the difference actually arises due to the way goals or likelihoods are encoded
in the inference procedure. Speciﬁcally, active inference encodes goals or rewards
directly into the generative model, so that active inference agents function with a
biased generative model which blends reward-driven and veridical perception. By
contrast, control as inference, keeps veridical inference and reward computation entirely
separate, and instead includes rewards into the inference procedure through the use of
exogenous optimality variables. The two methods thus solve subtly different inference
problems due to these distinctions in how they encode rewards. Put simply, active
inference encodes rewards and goals through priors; control as inference encodes them
through conditioning. Phrased intuitively, we can think of control as inference as
solving the inference problem: Assuming that I have acted optimally in the future, what
actions do I infer I will have taken, while active inference solves the inference problem:
Infer my most likely actions, given that I strongly believe in the future I will observe
highly desirable states . In sum, control as inference maintains a strict distinction
Chapter 5. The Mathematical Origins of Exploration 233
between a veridical perceptual generative model, which generates objectively likely
future outcomes for a given action sequence, while active inference maintains a biased
generative model which preferentially predicts desired outcomes. Control as inference
conditions this accurate generative model on observing high reward contingencies, and
thus maintains an adaptive action plan. Active inference, on the other hand, simply
maximizes the likelihood of this biased generative model, thus also inferring adaptive
actions.
While this distinction may seem arcane, it actually speaks to deep philosophical differ-
ences in perspectives between the two theories. Control as inference arises from the
cognitivist and logical views of artiﬁcial intelligence, which maintains that in some
sense intelligence is pure thought, which can then be unbiasedly applied to inferring
action. Control as inference maintains the modularity thesis, which is widespread
throughout artiﬁcial intelligence and engineering ﬁelds which is the strict separation
of perception and control. Perception aims to build up an accurate world model, while
control uses this world model to compute the best actions. Active inference, by contrast,
has much more in common with enactivist, embodied, and cybernetical views of control,
which see the agent and environment inextricably enmeshed in a fundamental feedback
process – the perception action loop. Here, it is unnecessary to maintain a strict separa-
tion between perception and control. Indeed, veridical perception is unnecessary since
the ultimate aim of perception, in this view, is not the unbiased modelling of the world,
but rather to subserve adaptive action.
The mathematical effect of this distinction is that control as inference maintains two
separate likelihoods - a veridical perceptual likelihood p(ot|xt), and a reward-encoding
optimality likelihood p(Ω|xt,at), while active inference only maintains a biased ob-
servational likelihood ˜p(ot). Due to this, control as inference approaches possess an
additional likelihood entropy term which active inference ones lack.
Chapter 5. The Mathematical Origins of Exploration 234
5.3 Evidence and Divergence Objectives
Given that we now see that the control as inference framework, even when extended
to full action policies and partially observable environments, does not manifest an
information-seeking exploration term while the expected free energy does and, more-
over, that conversely the EFE does not function as a bound on a known quantity like
the expected model evidence which is used in variational inference, we are left with
the question of trying to understand whether and how information-seeking objectives
can be derived in a mathematically principled manner. Here, we argue that objective
functionals for control existing in the literature can be sensibly split into two separate
classes – evidence objectives, which typically arise from a direct variational inference
approach, and which do not manifest information gain terms, and divergence objectives
which arise out of directly minimizing a KL divergence between two models, and which
do give rise to information seeking terms. We then show how well known objectives in
a variety of literatures can be understood in our scheme.
First, we need to formalize our mathematical setup. We assume that we have an
environment which, from the agent’s perspective, is an unknown black box. The agent
inputs actions a1:T to the environment and it outputs observations o1:T in return. The
exact process that produces these observations in the environment is forever unknown.
The agent can assume that the observations in the environment are produced by some
form of POMDP model with hidden latent states x1:T , but this is merely the agent’s
model of the environment. The agent can also parametrize its model with learnable
parameters θ, which it assumes are either constant throughout its entire lifetime, or else
change on a much slower timescale than the latent states x1:T . Next, to formalize a
control, as opposed to just an inference problem, we need some notion of the reward or
goal that the agent wishes to achieve. We formalize this by specifying that the agent
possesses an additional desire distribution ˜p(o1:T ) over the observations it receives. This
distribution encodes the ‘ideal world’ of the agent in the sense that these are precisely
Chapter 5. The Mathematical Origins of Exploration 235
the observations it aims to achieve. To map this to a standard reinforcement learning
setting, with rewards, we can perform the now familiar trick of deﬁning, ˜p(o1:T ) =
exp(−r(o1:T )) where r(o1:T ) is the total reward achieved across a given observation
trajectory. Importantly, this equivalence to standard reinforcement methodology is only
a special case, and that this formulation in terms of a desire distribution is more ﬂexible
in its speciﬁcation of the rewards. Speciﬁcally, here we assume no speciﬁc form of
the desire distribution, nor any factorization of desires across timesteps, for instance,
so the desired observations in the future can depend on the desired observations at the
present in arbitrarily complex ways. It is also straightforward to extend the framework
to a desired distributions over actions as well, by deﬁning ˜p(o1:T ,a1:T ). This is useful
for instance if we want to penalize the costs (i.e. energetic for biological organisms
or robotic systems) of action. For instance, to deﬁne a quadratic cost in the action
magnitude, we can deﬁne ˜p(a1:T ) =∏T
t N (a;0,σ), or a Gaussian distribution in the
action centered around 0. The variance of the Gaussian ( σ) effectively deﬁnes the
weighting coefﬁcient which scales the size of the penalty. Here, to keep notation simple,
we do not discuss action penalties and only work with a desired observation distribution
˜p(o1:T ), but the extension to actions is entirely straightforward.
The general objective of the control problem is to compute an action trajectory a1:T
which results in the realization of your goals, which are deﬁned through the desire
distribution ˜p(o1:T ). We also assume, that if the agent actually executes a given action
trajectory, it will receive a real trajectory of observations from the environment according
to some distribution p(o1:T |a1:T ). This distribution can be approximated either by
sampling real trajectories from the environment, as is implicitly the case in model-free
reinforcement learning methods, or else the agent can explicitly model this distribution
as p(o1:T |a1:T ;θ), with parameters θ, which may be implemented as a deep neural
network, for example. We call this distribution the predicted distribution, since in some
sense it is what the agent predicts will occur if it executes a given action trajectory.
Chapter 5. The Mathematical Origins of Exploration 236
With the preliminaries settled, we can formally deﬁne the evidence and divergence
objectives. Evidence objectives try to maximize the likelihood of the desire distribution
averaged under the predicted distribution. Intuitively, we wish to ﬁnd the action trajec-
tory that maximizes the expected likelihood of the desire distribution. Mathematically,
we can represent this as,
Levidence = argmax
a1:T
Ep(o1:T |a1:T )[ln ˜p(o1:T )] (5.18)
Where here, as is usual, we optimize the log of the desire distribution instead of the
desire distribution itself. Since the log is a monotonic function, this does not affect
the actual optimum of the problem and since we often factorize the desire distribution
into products, the log will turn that into a sum, which is usually much better behaved
numerically.
Conversely, for a divergence objective, instead of maximizing the likelihood of the
desires under the predicted distribution, we instead with to directly minimize the diver-
gence of the desire and predicted distribution. In effect, we want to make the desire
distribution and the predicted distribution match. Mathematically, we can write this as,
Ldivergence = argmin
a1:T
DKL[p(o1:T )|a1:T )||˜p(o1:T )] (5.19)
Where here we use the KL divergence DKL[Q||P] =EQ ln Q
P . Intuitively, we can think
of the distinction between evidence and divergence objectives being that the evidence
objective seeks to match the predicted distribution to the mode of the desire distribution
– i.e. it focuses all effort onto the most desired observations, while neglecting the
lesser desired observations. This is why evidence objectives typically arise from
direct maximizing principles such as utility maximization, or variational inference.
Meanwhile, divergence objectives seek to precisely match the predicted distribution and
the desired distribution, so that if some observation is not the most preferred one, but
has some moderate level of preference, the divergence objective would seek to have that
Chapter 5. The Mathematical Origins of Exploration 237
observation manifest some amount of time in proportion to its relative preferredness.
This means that in general, if the desire distribution is spread out, then agents will seek
to realize a spread-out distribution of predicted observations. In contrast, under an
evidence objective, even if the desire distribution is broad, the agent will continue to
place all effort to keep the predicted distribution a peak around the mode of the desire
distribution.
(a) Optimizing with an Evidence Objective
 (b) Optimizing with a Divergence Objective
Figure 5.1: Numerical illustration of optimizing a multimodal desired distribution with
an Evidence objective (Panel A) vs a Divergence Objective (panel B). The desire distri-
bution consisted of the sum of two univariate Gaussian distributions, with means of 1
and 4 and variances of 1 and 0.4 respectively. We then optimized an expected future
distribution, which also consisted of two Gaussians with free means and variances
using both an Evidence and a Divergence objective. As can be seen, optimizing the
Evidence Objective results in the agent ﬁtting the predicted future density entirely to
an extremely sharp peak around the mode of the desired distribution. Conversely,
optimizing a divergence objective leads to a precise match of the predicted and de-
sired distributions (panel B shows the two distributions almost precisely on top of one
another). As a technical note, to be able to see both the evidence and deisre dis-
tributions on the same scale, for the evidence objective the predicted distribution is
normalized but the desired distribution is not. Code for these simulations can be found
at: https://github.com/BerenMillidge/origins_information_seeking_exploration.
Chapter 5. The Mathematical Origins of Exploration 238
Another way we can interpret the difference between the objectives is in terms of the
effect of the objective upon the predicted distribution p(o1:T |a1:T ). Speciﬁcally, we can
think of the divergence objective as a balance between trying to maximize the likelihood
of the desired distribution under the predicted distribution, and trying to maximize the
entropy of the predicted distribution. We can think of this as the divergence objective
as effectively saying ‘try to maximize your desires or utility while also keeping the
future as broad as possible’ – i.e. keeping your options open. This can be demonstrated
mathematically through the simple deﬁnition of the KL divergence,
Ldivergence = argmin
a1:T
DKL[p(o1:T )|a1:T )||˜p(o1:T )]
= argmin
a1:T
−H[p(o1:T |a1:T )]  
Predicted Entropy
−Ep(o1:T |a1:T )[ln ˜p(o1:T )]  
Evidence Objective
Here we see that we can express the divergence objective simply as the evidence
objective plus the maximization of the entropy of the predicted distribution. In effect,
the divergence objective simply includes an entropy regulariser to the standard evidence
objective. Conversely, we can also express the relationship between the objectives in
the other way. We can think of the evidence objective as simply trying to match the
predicted and desire distribution while simultaneously minimizing the entropy of the
predicted distribution. This is straightforward to show mathematically,
LEvidence = argmax
a1:T
Ep(o1:T |a1:T )[ln ˜p(o1:T )]
= argmax
a1:T
Ep(o1:T |a1:T )[ln ˜p(o1:T ) p(o1:T |a1:T )
p(o1:T |a1:T )]
= argmax
a1:T
−DKL[p(o1:T |a1:T )|| ˜p(o1:T )]  
Divergence
− H[p(o1:T |a1:T )]  
Expected Future Entropy
(5.20)
This formulation gives a straightforward intuition for the ‘mode-seeking’ behaviour of
the evidence objective. The evidence objective seeks to match the predicted and desire
distribution, while also being penalized for the breadth of the predicted distribution.
The best way to resolve this tension is by forming a highly peaked predicted distribution
Chapter 5. The Mathematical Origins of Exploration 239
around the mode of the desire distribution so that it can cover as much probability mass
as possible.
Interestingly, differences between the two formulations generally only emerge when
the desire distribution is broad and complex. Here, the divergence objective will tend
to force the predicted distribution and desire distribution to match in their complexity,
while the evidence distribution will seek out and focus around its mode. Another
intuitive way of thinking about the distinction is that the divergence objective implicitly
maximizes some sort of future empowerment, by implicitly trying to keep all future
options open, by seeking to make future observations as entropic as possible. Evidence
objectives, by contrast, seek the opposite. They aim for ‘precise futures’ where the
amount of future variability is as low as possible. It is straightforward to show that,
unlike evidence objectives, divergence objectives can be immediately decomposed into
an ‘extrinsic’ value divergence term, and an information-seeking exploratory term,
DKL[p(o1:T |a1:T )||˜p(o1:T )] =Ep(o1:T |a1:T )[ln p(o1:T |a1:T )
˜p(o1:T ) ]
= Ep(o1:T |a1:T )[ln p(o1:T |a1:T )
˜p(o1:T ) ]
= Ep(o1:T |a1:T )[ln p(o1:T ,x1:T |a1:T )
˜p(o1:T )p(x1:T |o1:T )]
= Ep(x1:T )DKL[p(o1:T |x1:T )||˜p(o1:T )]  
Desire Divergence
−Ep(o1:T |a1:T )DKL[p(x1:T |o1:T )||p(x1:T )]  
Information Gain
(5.21)
Importantly, the property that divergence objectives give rise to directed information-
seeking exploratory terms, arises directly from the previously discussed intuition that
they attempt to maximize the entropy of future observations. Such information gain
terms arise whenever the predicted distribution is extended to model additional latent
Chapter 5. The Mathematical Origins of Exploration 240
variables or parameters. The proof of this is straightforward,
H[p(o1:T |a1:T )] =Ep(o1:T ,x1:T |a1:T )[ln p(o1:T |a1:T )]
= Ep(o1:T ,x1:T |a1:T )[ln p(o1:T ,x1:T |a1:T )
p(x1:T |o1:T ) ]
= −Ep(x1:T )H[p(o1:T |x1:T )]  
Likelihood Entropy
−Ep(o1:T |a1:T )DKL[p(x1:T |o1:T )||p(x1:T )]  
Expected Information Gain
(5.22)
Put verbally, this relationship shows that to maximize the entropy of a distribution, if
the distribution can be understood in terms of a latent set of variables, requires both
maximizing the conditional entropy of the variable given the latent variables, while
simultaneously maximizing the mutual information of the latent variables between
the observed and latent variables. Intuitively, within our context, this means that to
maximize the entropy of future observations, it is necessary to successfully model the
relationship between these future observations and their latent states, which entails
maximizing the mutual information between the latents and the observations. It is the
maximization of this mutual information which undergirds the exploratory information-
seeking behaviour which is manifested by divergence objectives. Conversely, the fact
that evidence objectives seek to minimize the entropy of the predicted distribution
means that they implicitly seek to minimize the amount of mutual information between
observation and latent variables. We can think of this as divergence objectives aim to
reach a given set of goals while also learning as much as possible about the environment,
in order to precisely match the two distributions. Evidence objectives, on the other hand,
seek to reach their goals while learning as little as possible about the environment, and
keeping the environment as regular and predictable as possible.
Now that we have proposed and given considerable intuition for our dichotomy between
evidence and divergence objectives, we look to see where these objectives appear in
the literature, and how they can explain differences in exploratory behaviour between
differing paradigms.
Chapter 5. The Mathematical Origins of Exploration 241
5.3.1 Control as Inference
It is straightforward to show that the control as inference, and variational inference
objectives are bounds upon the evidence objective. Put simply, we have that control
as inference aims to solve the inference problem of inferring an action distribution
given a desired set of observations. Speciﬁcally, we seek to obtain the distribution
p(a1:T |˜o1:T ) where we use ˜o to denote a set of hypothetical ‘optimal’ actions which
have been conditioned upon. To ﬁnd this posterior, we use a variational approximation
by deﬁning the variational density q(a1:T ) and optimizing the following variational
lower bound,
DKL[q(a1:T ||p(a1:T |˜o1:T )] =DKL[q(a1:T ||˜p(o1:T ,a1:T )]−ln p(o1:T )
≥DKL[q(a1:T ||˜p(o1:T ,a1:T )]  
CAI Objective
Which serves as the control as inference objective. It is then straightforward to show
that this objective serves as a bound on an evidence objective,
argmax
a1:T
ln ˜p(o1:T ) =argmax
a1:T
ln
∫
dx ˜p(o1:T x1:T )
= argmax
a1:T
ln
∫
dx ˜p(o1:T x1:T )q(a1:T )
q(a1:T )
≥argmax
a1:T
Eq(a1:T )[ln ˜p(o1:T ,a1:T )
q(a1:T ) ]
≥argmin
a1:T )
−DKL[q(a1:T )||˜p(o1:T ,a1:T )]
=≥LCAI (5.23)
And thus we can see that the control as inference framework optimizes a variational
bound on the evidence objective. This straightforwardly explains why control as
inference approaches do not give rise to directed, information-seeking exploration,
but rather instead only induce random action entropy maximizing exploration terms.
This random exploration, while highly efﬁcient in many contemporary dense-reward
reinforcement learning benchmark tasks, where a random policy often sufﬁces to cover
Chapter 5. The Mathematical Origins of Exploration 242
enough of the state space, it is increasingly ineffective in extremely high dimensional,
and sparse reward environments.
5.3.2 KL Control
Another control method in the literature that has been applied, and studied fairly
extensively is KL control. Although not as widely used in reinforcement learning, it is
often implicit optimized in control tasks, and has deep relationships with the beginning
of the control as inference approach, as well as more esoteric path integral methods.
Moreover, it has recently seen renewed application in deep reinforcement learning
approaches such as state-marginal matching of (L. Lee et al., 2019). KL control, as the
name implies, chooses control to minimize the KL divergence between the current state
and a set of desired states, leading to the following objective function,
LKL = argmin
a1:T
DKL[p(x1:T )||˜p(x1:T )] (5.24)
Here we have used x instead of o to denote that KL control has typically only been
applied to fully-observed Markovian MDP environments as opposed to full POMDP dy-
namics. As such, while the KL control objective is clearly just the divergence objective,
its superior exploratory capabilities have not been signiﬁcantly explored in the literature
due to the only applications currently being in fully observable environments while the
information-seeking exploratory terms require the extension to hidden variable models.
Another interesting point is that the objective in continuous time active inference in
predictive coding models can also be as a KL divergence between a desired ‘set-point’
and a currently observed point, and is thus an instance of KL control. However, these
models also do not handle latent variable models, and thus also do not manifest the full
exploratory capabilities of divergence objectives.
Chapter 5. The Mathematical Origins of Exploration 243
5.3.3 Active Inference
Given that we know from previously, that the expected free energy contains an informa-
tion gain term, which gives rise to the information-seeking exploratory behaviour of
active inference agents, it is worth investigating the relationship of the EFE to evidence
and divergence functionals. Recall, from the previous section that the EFE formed
neither an upper nor a lower bound upon the log model evidence, but instead formed an
upper bound when the posterior divergence was greater than the information gain term,
and a lower bound otherwise, with the goal of eventually converging directly to the log
model evidence in the case that both of these terms become zero. Noting that the log
model evidence simply, as the name suggests, is the evidence, objective, we can rewrite
this in terms of our new understanding as,
Eq(o,x)[ln ˜p(o)]  
Evidence Objective
= Eq(o,x)[lnq(x)−ln ˜p(o,x)]  
EFE
+Eq(o)DKL[q(x|o)||q(x)]  
Information Gain
−Eq(o)DKL[q(x|o)||p(x|o)]  
Posterior Divergence
=⇒Eq(o,x)[ln ˜p(o)]  
Evidence Objective
≥Eq(o,x)[lnq(x)−ln ˜p(o,x)]  
EFE
If Eq(o)DKL[q(x|o)||q(x)]  
Information Gain
≥Eq(o)DKL[q(x|o)||p(x|o)]  
Posterior Divergence
(5.25)
so that the EFE does not stand in a straightforward relationship as bound in any speciﬁc
direction on the evidence objective. Nevertheless, since the EFE contains an information
gain term to be maximized, as do divergence objectives, we might expect to obtain a
straightforward relationship between the EFE and the divergence objective. We can
write out the relationship between the divergence objective and the EFE as follows,
Chapter 5. The Mathematical Origins of Exploration 244
DKL[p(o)||˜p(o)]  
Divergence Objective
= Ep(o)[ln
∫
dxp (o,x)
˜p(o) ]
= Ep(o)[ln
∫
dxp (o,x)q(x|o)q(o,x)
˜p(o)q(x|o)q(o,x) ]
≤Ep(o)[ln
∫
dxp (o,x)q(o,x)
˜p(o)q(x|o)q(o,x)]
≤Ep(o)[ln
∫
dxp (o,x)q(o|x)q(x)
˜p(o)q(x|o)q(x|o)q(o)]
≤Ep(o)q(x|o)[lnq(x)−lnq(x|o)−ln ˜p(o)]  
EFE
−Ep(o)DKL[q(x|o)||p(o,x)]  
VFE
+Eq(x|o)p(o)DKL[q(x|o)||q(x)]  
Information Gain
(5.26)
Here we see that the EFE can be expressed in terms of the divergence objective, an
information gain term, and, interestingly, the variational free energy. In effect, the
divergence objective consists of the EFE, the VFE, and an information gain term.
Speciﬁcally, the EFE becomes an upper bound on the divergence objective when the
information gain term is greater than the variational free energy. This is similar to
previously where we saw that the EFE became a bound on the evidence when the
information gain is less than the posterior divergence. It is thus clear that the EFE
objective does not serve as a valid and consistent bound on either of the divergence
or the evidence objectives. Similarly, we can express the EFE directly in terms of the
divergence objective as follows,
DKL[p(o)||˜p(o)]  
Divergence Objective
= Eq(x|o)p(o)DKL[p(o)q(o,x)||˜p(o)q(o,x)]
= Eq(x|o)p(o)DKL[p(o)q(o|x)q(x)||˜p(o)q(x|o)q(o)]
= Eq(x|o)p(o)[lnq(x)−ln ˜p(o)−lnq(x|o)]  
EFE
+Ep(o)DKL[q(x|o)||q(x)]  
Information Gain
− H[p(o)]  
Marginal Entropy
(5.27)
Chapter 5. The Mathematical Origins of Exploration 245
Where we see that the divergence objectives simply is the EFE plus an information
gain term, minus the marginal or predicted entropy term. As such, even within this
framework, the mathematical origin and the behaviour of the EFE remains unclear since
the EFE does not form consistent bounds on either objective, but instead oscillates
above and below both.
5.3.4 Action and Perception as Divergence Minimization
A recent framework, inspired by active inference and advances in deep reinforcement
learning, which aims to unify perception and action under a single framework is
Action and Perception as Divergence Minimization (Hafner et al., 2020) (APDM). This
framework proposes that both action and perception can be modelled as an agent trying
to mininimize a divergence functional between two distributions an ‘actual’ distribution
A(x,o), and a target distribution T (x,o).
LAPDM = DKL[A(x,o)||T (x,o)]
= EA(x)DKL[A(o|x)||T (o)]  
Realizing Latent Preferences
−EA(x,o)[lnT (x|o)−lnA(x)]  
Information Bound
(5.28)
−EA(x,o)[lnT (x|o)−lnA(x)]  
Information Bound
= −EA(x,o)[lnT (x|o)−lnA(x)+ lnA(x|o)−lnA(x|o)]
= −EA(o)DKL[A(x|o)||A(x)]  
Information Gain
+EA(o)DKL[A(x|o)||T (x|o)]  
Posterior Divergence
(5.29)
By expressing this bound explicitly, we can see how it is an upper bound on the
information gain, since the posterior divergence is always positive. The tightness of the
bound then depends on how closely the actual and target distributions match. In general,
we can use this approach to write out a full expression for the divergence objective
Chapter 5. The Mathematical Origins of Exploration 246
between two joint distributions over both observations and latent variables.
Ljoint = argmin
a
DKL[p(o,x)||˜p(o,x)]
= Ep(x)DKL[p(o|x)||˜p(o)]  
Likelihod Divergence
−Ep(o,x)[ln ˜p(x|o)−ln p(x)]  
Information Bound
= Ep(x)DKL[p(o|x)||˜p(o)]  
Likelihod Divergence
−Ep(o)DKL[p(x|o)||p(x)]  
Information Gain
+Ep(o)DKL[p(x|o)||˜p(x|o)]  
Posterior Divergence
(5.30)
In effect, we see that minimizing the divergence between two joint distributions requires
the minimizations of both the likelihood divergence and the posterior divergence, while
also requiring the maximization of the information between posterior and prior of the
ﬁrst term in the joint KL.
It is also straightforward to relate this joint divergence to the divergence objective,
which is the divergence between marginals instead of joints.
DKL[p(o,x)||˜p(o,x)]  
Joint Divergence
= DKL[p(o)p(x|o)||˜p(x|o) ˜p(x|o)]
= DKL[p(o)||˜p(o)]  
Divergence Objective
+Ep(o)DKL[p(x|o)||˜p(x|o)]  
Posterior Divergence
≥DKL[p(o)||˜p(o)]  
Divergence Objective
(5.31)
Since the posterior divergence is always positive (as a KL divergence), we observe that
the joint divergence is simply an upper bound on the divergence objective. Since the
divergence is minimized, this bound is in the correct direction, and thus minimizing
the joint divergence is a reasonable proxy for minimizing the marginal divergence
objective. By minimizing the joint, it implicitly encourages agents to minimize both
the marginal divergence as well as the divergence between the predicted and desired
posterior distributions.
While the generic APDM divergence, as just a divergence of joints, is straightforwardly
an upper bound on the divergence objective, we show that under the common deﬁnitions
Chapter 5. The Mathematical Origins of Exploration 247
of the actual and target distributions, the APDM divergence can also be understood as a
lower bound on the evidence objective, thus providing a bridge between the two objec-
tives. Although the actual and target distributions can be deﬁned differently depending
on the objective you desire to reproduce, one canonical form of the actual and target
distributions, which can reproduce control as inference as well as variational perceptual
inference is as follows. We deﬁne the actual distribution to be the combination of the
‘real’ data distributionp(o) and also a variational belief distribution q(x|o) such that
A(o,x) =q(x|o)p(o). Similarly, we deﬁne the target distribution to be the product of
the agent’s veridical generative model p(o,x) and the exogenous desire distribution
˜p(o) such that T (o,x) =p(o,x) ˜p(o). This target distribution is valid as long as the
ultimate objective is optimized via gradients of the divergence, which does not require
that the target distribution be normalized. Under this deﬁnition of the actual and target
distributions, the APDM objective becomes,
LAPDM = DKL[q(x|o)p(o)||p(o,x) ˜p(o)] (5.32)
In the case of known observations in the past, we assume that the data distribution
becomes points around the actually observed observations p(o) =δ(o = ˆo) while the
desire distribution becomes uniform – as there is little use for control in having desires
about the unalterable past. Under these assumptions, the APDM objective simply
becomes the ELBO or the negative free energy, thus replicating perceptual inference.
However, on inputs in the future, the data distribution becomes a function of action
(since actions can change future observations) and the desire distribution becomes
relevant, thus allowing the minimization of the APDM functional to underwrite control.
To gain a better intuition for the interplay of perception and control in the APDM
Chapter 5. The Mathematical Origins of Exploration 248
functional, we showcase the following decomposition,
LAPDM = DKL[q(x|o)p(o)||p(o,x) ˜p(o)]
= Ep(o)DKL[q(x|o)||p(o,x)]  
ELBO
+DKL[p(o)||˜p(o)]  
Divergence Objective
(5.33)
which demonstrates that the APDM objective effectively uniﬁes action and perception
by summing together a perceptual objective (VFE) with the divergence objective for
control. This conﬁrms the previous ﬁnding that the APDM objective forms an upper
bound on the divergence objective since the ELBO, as a KL divergence, is bounded
below by 0. We also observe that this form of the APMD objective is also approximately
a lower bound on the expected evidence objective, thus providing a link between the
two objectives.
Ep(o|a)[ln ˜p(o)] =Ep(o|a)[ln
∫
dx ˜p(o,x)]
= Ep(o|a)[ln
∫
dx ˜p(o,x)q(x|o)p(o,x)
q(x|o)p(o,x) ]
≥Ep(o)q(x|o)[ln ˜p(o,x)p(o,x)
q(x|o)p(o,x) ]
≥Ep(o)q(x|o)[ln ˜p(o) ˜p(x|o)p(o,x)
q(x|o)p(o)p(x|o) ]
≥−Ep(o)q(x|o)[DKL[q(x|o)p(o)||p(o,x) ˜p(o)]  
APDM Objective
+Ep(o)q(x|o)[ln ˜p(x|o)−ln p(x|o)]  
Posterior Divergence Bound
≈≥Ep(o)q(x|o)[DKL[q(x|o)p(o)||p(o,x) ˜p(o)]  
APDM Objective
(5.34)
Which is approximately equal to the APDM objective under the condition that the
posterior divergence bound between desire posterior ˜p(x|o) and true posterior p(x|o) is
small.
Chapter 5. The Mathematical Origins of Exploration 249
5.4 Towards a General Theory of Mean-Field Variational
Objectives for Control
Now that we understand the division of objectives for control into two classes of evi-
dence and divergence functionals, we can start to try to understand the full possibilities
of the space of potential objectives. Here, in this ﬁnal section of Chapter 5, we try to
present precisely such a taxonomy. We focus speciﬁcally on ‘mean-ﬁeld’ variational
objective functionals, meaning that we can split the objective into a number of indepen-
dent objectives for each time-step of a trajectory which can be minimized independently.
Crucially, such a mean-ﬁeld assumption is also made in the traditional reinforcement
learning paradigm, where it is a necessary precondition for the Bellman equation, and
also is standard in the control as inference framework as well.
While the division into divergence and evidence objectives is clearly important, it cannot
be the full story. Recall that one of the key differences between control as inference
and active inference discussed earlier, was not just the objective of the EFE vs the FEF,
but also the way value was encoded into the inference procedure. Control as inference
encoded value via an additional set of ‘optimality variables’ which were augmentations
to the graphical model, and did not interact with any previously existing variables.
Active inference, by contrast, encoded value directly through a biased generative model
of the observations 3. We call the method used by control as inference, which does
not affect any currently existing variable an exogenous encoding of value, while the
method used by active inference, since it encodes value directly into the model itself, we
call an endogenous encoding. Exogenous and Endogenous encodings of value provide
an orthogonal dimension of objective variablility on top of the evidence-divergence
dichotomy, since clearly one can have an evidence, or a divergence objective with both
an endogenous or an exogenous value encoding. Finally, the actual speciﬁcs of the
generative model used clearly affects the variational objective irrespective of whether
3Active inference can also be formulated with biased states, see (Da Costa, Parr, et al., 2020)
Chapter 5. The Mathematical Origins of Exploration 250
it is an evidence or divergence objective, or uses an endogenous or exogenous value
encoding. For instance, whether we consider latent states, or various different types of
model parameters in the generative model leads to a different objective functional.
We thus see that we can break down the landscape of potential mean ﬁeld objective
functionals for control into three orthogonal dimensions.
• Whether an evidence or divergence functional is used.
• Whether value is encoded exogenously, or endogenously.
• The generative model underlying the objective functional.
Under different values for each of these dimensions, the objective functional that is
speciﬁed changes in a straightforward and principled manner. Thus, our scheme allows
the direct derivation of any given functional, and an understanding of its decompositions,
and hence the behaviour it induces, for any choice of these variables. While we have
covered the effect of choosing an evidence or a divergence functional previously, we
have not yet been explicit about the effect of the other two dimensions. In this section,
we explore the effects of these additional dimensions of design choice in more detail.
5.4.1 Encoding Value
However, the need to encode goals or desires into the inference procedure immediately
introduces design choices of how exactly this is to be done. We argue that these design
choices can ﬁrst be split along two orthogonal axes – ﬁrstly, whether goals are encoded
exogenously as an additional input to the inference process, or endogenously through
fundamentally biasing one or more aspects of the inference model. The second axis
of variation is whether goals and desires enter the inference procedure through the
generative model or the variational distribution. Making different design choices here
produces a variety of different variational algorithms for control.
If goals are encoded through the generative model, then whether the goals are encoded
Chapter 5. The Mathematical Origins of Exploration 251
exogenously or endogenously is the primary distinction between the formalisms of
control-as-inference and active inference. On the other hand, encodings goals through
the variational model instead leads to novel algorithms which generally have not been
much explored in the literature. Encoding goals exogenously through the variational
distribution leads to a variational bound similar to control-as-inference but with an
extrinsic value term with a reversed-KL-divergence which thus exhibits mode-seeking
rather than mean-seeking behaviour, which is related to pseudolikelihood methods (Pe-
ters & Schaal, 2007) in variational reinforcement learning. Encoding endogenous goals
through the variational distribution leads to a novel class of reverse-active-inference
algorithms which minimize a variational divergence between a biased approximate
posterior and a veridical generative model.
5.4.1.1 Exogenous Value: Maximum-Entropy RL
To encode goals exogenously into the generative model, we must augment the POMDP
graphical model with additional optimality variables Ωt:T . The idea here is that the
optimality variables are binary bernoulli variables which mark whether a trajectory is
optimal from the current state where the probability of optimality is often set to the
exponentiated reward p(Ωt = 1) ∝ exp(rt) so that ln p(Ωt = 1) =rt. Adaptive actions
are then inferred by ﬁrst assuming that the agent has acted optimally into the future,
and then inferring the actions that would be consistent with that belief – i.e. we wish to
infer the posterior p(at:T |xt:T ,Ωt:T = 1). This posterior can then be approximated by
minimizing the augmented variational bound:
FΩ = DKL
(
q(xt,at|ot)∥p(ot,xt,at,Ωt)
)
≥DKL
(
q(xt,at|ot)∥p(ot,xt,at|Ωt:T )
)
(5.35)
By splitting apart this bound into its constituent parts, we can investigate the expected
Chapter 5. The Mathematical Origins of Exploration 252
behaviour of agents which act so as to minimize the bound.
FΩ = DKL
(
q(xt,at|ot)∥p(ot,xt,at,Ωt)
)
= DKL
(
q(at|xt)q(xt|ot)∥p(Ωt|xt,at)p(ot|xt)p(at|xt)p(xt|xt−1,at−1)
)
= −Eq(xt ,at |ot )
[
ln p(Ωt|xt,at)
]
  
Extrinsic Value
−Eq(xt ,at |ot )
[
ln p(ot|xt)
]
  
Observation Ambiguity
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
+DKL
(
q(xt|ot)∥p(xt|xt−1,at−1)
)
  
State Divergence
(5.36)
The bound thus splits into four separate and identiﬁable terms – extrinsic value, obser-
vation ambiguity, action divergence, and state divergence. The ﬁrst extrinsic value term
corresponds to the expected external rewards given by the environment. This is due to
the deﬁnition of optimality that ln p(Ωt|xt,at) =: r(xt,at) so that the extrinsic value is
simply the expected reward of a given state-action pair. By minimizing the negative
expected reward, we wish to maximize the expected reward on a given time-step. This
is identical to the standard reinforcement learning objective of reward maximization
so that if FΩ only contained the extrinsic value term, it would be exactly equivalent to
reinforcement learning except that the expectation is taken with respect to the agent’s
beliefs over states and actions rather than the true environmental transition dynamics.
The observation ambiguity term Eq(xt ,at |ot )
[
ln p(ot|xt)
]
grants a bonus to agents for
reaching areas of state-space with a high expected likelihood, that is areas where
the state-observation mapping is highly precise. In effect, if the agent must learn a
likelihood mapping, this discourages exploration by granting bonuses for staying in
regions already well characterised. This term only arises in the POMDP setting due
to the addition of a likelihood term in the generative model. The third term is the
action divergence which is to be minimized, and penalizes the agent for the divergence
between its variational policy q(at|xt) and its prior policy p(at|xt). If the prior policy
is assumed to be uniform such that p(at|xt) =: 1
|A|where |A|is the cardinality of the
action space in discrete action-spaces, and p(at|xt) =: Unif(amin,amax) in continuous
Chapter 5. The Mathematical Origins of Exploration 253
action spaces with a minimum and maximum action value, then this action divergence
term reduces to the negative expected action entropy −Eq(xt |ot )
[
H [q(at|xt)]
]
. The ﬁnal
term is the state divergence, so that the agent tries to minimize the divergence between
its variational posterior over the state and the prior state expected under the generative
model. If the transition model is learnt, this leads it to prioritising transitions with
known dynamics, again causing the agent to primarily conﬁne itself to regions of the
state-space it has already modelled well. In the MDP setting, without observations, this
term vanishes since the variational posterior q(xt|ot) becomes the variational prior q(xt)
which is often assumed equal to the generative prior p(xt|xt−1,at−1). Thus in the case
of an MDP with a uniform action prior we obtain the maximum-entropy RL objective:
Fmaxent = −Eq(xt ,at )
[
ln p(Ωt|xt,at)
]
−Eq(xt )
[
H [q(at|xt)]
]
(5.37)
Which induces agents both to maximize expected rewards while also maximizing the
policy entropy. Intuitively this means that the agent should try to maximize rewards
while acting as randomly as possible (maximizing entropy). This policy entropy term
thus provides a crude bonus for random exploration and often helps prevents the
commonly-observed phenomenon of ‘policy collapse’ (Fujimoto et al., 2018) whereby
reinforcement learning agents will often rapidly learn to put all probability mass onto a
single action, thus preventing other actions from being taken, hindering exploration and
long-term performance. Interestingly, when extended to the POMDP case with learnt
transition and likelihood models, this formalism gives rise to additional observation-
ambiguity and state-divergence terms which serve to further disincentivise exploration
by penalising moving too far from the prior predicted state and giving bonuses for
highly predictable likelihoods. Intuitively we can think of these extra terms as trying to
do away with the additional uncertainty induced by the POMDP setting, so the agent
conﬁnes itself to the region which is as close to an MDP as possible.
Chapter 5. The Mathematical Origins of Exploration 254
5.4.1.2 Endogenous Value: Active Inference
While maximum entropy reinforcement methods can be derived by encoding goals
exogenously to the generative model through the use of additional ‘optimality variables’,
it is also possible to encode goals endogenously by directly biasing some aspect of
the generative model towards preferred outcomes. Intuitively, we can think of the
difference as follows. With exogenous optimality variables, we possess a veridical
generative model outputting the likely and unbiased trajectories for a given series of
actions. We then ‘shift’ these trajectories to converge on the goal by conditioning on the
optimality variables, and then infer the actions consistent with the shifted trajectories.
By contrast, active inference endogenously encodes goals by biasing the model so
that instead of a veridical generative model which generates trajectories that are then
shifted, instead we have a biased generative model which directly outputs a biased
trajectory of observations ˜ot:T converging on the goal, which can then be used to infer
the actions consistent with this biased trajectory. In this manner, instead of proposing
additional optimality variables, we directly posit a biased generative model ˜p(ot,xt,at)
and optimise the biased variational bound:
Flikelihood −AIF = DKL
(
q(xt,at|ot)∥˜p(ot,xt,at)
)
= DKL
(
q(at|xt)q(xt|ot)∥˜p(ot|xt)p(xt|xt−1,at−1)p(at|xt)
)
= −Eq(xt |ot )
[
ln ˜p(ot|xt)
]
  
Extrinsic Value
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
+DKL
(
q(xt|ot)∥p(xt|xt−1,at−1)
)
  
State Divergence
(5.38)
This variational bound decomposes into three terms as can be seen above. The ﬁrst,
extrinsic value, is such because it is the biased probability of observations expected
under the variational distribution. If we assume that the biased generative model is
inﬂuenced by the external rewards in the environment, to allow for consistency with
reinforcement learning and the maximum-entropy RL framework such thatln ˜p(ot|xt) ∝
Chapter 5. The Mathematical Origins of Exploration 255
exp(r(xt) then this ﬁrst term reduces to minimizing the expected sum of negative
rewards, or maximising expected rewards. The other two action-divergence and state-
divergence terms are equivalent to the terms in the maximum entropy bound above,
except that the active inference bound is lacking the ‘observation ambiguity’ term. This
is because exogenously encoding goals adds an additional set of optimality variables
to the variational bound, thus maintaining a distinction between veridical observations
and biased optimality variables, while endogenously encoding goals needs to ‘hijack’ at
least one degree of freedom in the bound in order to encode the goals directly. Here the
observation ambiguity term has effectively been hijacked by being biased with reward
to instead encode the extrinsic value.
Interestingly, the choice of encoding goals endogenously also gives an additional set of
choices of how to bias the generative model. The key choice is between having a biased
likelihood, as done above, or a biased marginal and posterior. This decomposition is
shown below:
Fmarginal−AIF = DKL
(
q(xt,at|ot)∥˜p(ot,xt,at)
)
= DKL
(
q(at|xt)q(xt|ot)∥˜p(ot) ˜p(xt|ot)p(at|xt)
)
= −Eq(xt |ot )
[
ln ˜p(ot)
]
  
Extrinsic Value
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
+DKL
(
q(xt|ot)∥˜p(xt|ot)
)
  
Biased State Divergence
(5.39)
Here, we assume that ln ˜p(ot) ∝ exp(r(ot)) such that the extrinsic value term is again
directly equal to the rewards. An interesting difference is that the biased state-divergence
is now between the variational posterior and the biased state posterior, which represents
the posterior over states that would be observed given the biased observations. This
gives this second term also the ﬂavour of an extrinsic value as the goal is not only to
maximize rewards but match the veridicial variational distribution to the biased state
distribution induced by the desired observations
Alternatively, it is also possible to consider encoding the bias into the states ˜p(xt)
Chapter 5. The Mathematical Origins of Exploration 256
instead of the observations. In this case, we obtain the following objective functional:
Fstate−AIF = DKL
(
q(xt,at|ot)∥˜p(ot,xt,at)
)
= DKL
(
q(at|xt)q(xt|ot)∥p(ot|xt) ˜p(xt)p(at|xt)
)
= DKL
(
q(xt|ot)∥˜p(xt)
)
  
Extrinsic Value
−Eq(xt ,at |ot )
[
ln p(ot|xt)
]
  
Observation Ambiguity
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
(5.40)
In this case, we have regained the observation-ambiguity term and instead the state-
divergence term has been ‘hijacked’ by the rewards to become the extrinsic value term,
which has become the divergence between predicted and desired states.
Given that there exist these two design choices of whether to encode goals exogenously
or endogenously and which lead to subtly different objectives, a natural question to ask
is what are the relative advantages and disadvantages of the two methods? What trade-
offs exist and where might each be useful? In general, the primary practical difference
between the methods is that by endogenously encoding rewards, the agent loses a degree
of freedom, which manifests itself as the loss of an observation ambiguity term if the
goals are encoded through biased observations, or the loss of the state-divergence term
if the goals are encoded as a desired state distribution. These additional terms tend to
discourage exploration by causing the agent to remain in areas with known mappings,
so that by disabling them endogenous goals tend to encourage exploration. On the other
hand, by keeping to well-known regions where the POMDP behaves like an MDP, the
exogenous goals method enforces a greater conservative bias towards safety, which may
be especially useful in settings where exploration is costly, satisﬁcing policies are easy
to ﬁnd, or in policies learnt in an ofﬂine or imitation-learning setting where venturing
outside of the training distribution can have deleterious effects on the policy.
There are also important philosophical and representational differences between the
two. On a representational note, although in the derivations above both the optimality
variable and the biased generative model have been deﬁned in terms of exponentiated
Chapter 5. The Mathematical Origins of Exploration 257
reward, this is not necessarily the case. Since in the endogenous encoding case, the
goals are encoded directly as prior distributions into the generative model, it is possible
to model complex and potentially nonstationary goals distributions in this manner.
However, due to the optimality variables being binary, and conditioned upon, this may
constrain their representational power compared to the endogenous method, although
in practice this difference may be negligible as although the variables themselves are
binary, the probability of optimality being one, can be deﬁned as an arbitrary function of
the states, actions or observations. Thus in practice, there may be little representational
difference between the two methods, except that the endogenous case has a slightly
simpler intuitive justiﬁcation as directly specifying a desired distribution over states or
observations.
The difference between exogenous and endogenous encodings of value also has sig-
niﬁcant philosophical import. Exogenous encodings, by adding desires on top of an
unbiased generative model maintain a clean distinction between veridical perception
and action selection, and goals. This maintains the core modularity thesis of much work
in artiﬁcial intelligence that perception, and action selection should be kept separate
such that ﬁrst a veridical world-model is constructed which tries to accurately model
the world, then given a set of arbitrary goals, a general-purpose planner or policy
can be utilised or learnt to enable the agent to achieve these goals. This approach
corresponds to the classical perceive-(value)-plan-act cycle in cognitive science and
maintains separate modules for a goal-agnostic perceptual system, a goal-agnostic plan-
ner or action-selection mechanism, and then a set of goals which are not intrinsic to the
agent but which are constructed or handed-down from on high. Endogenous encoding
methods, by contrast, tend to blur the boundaries between these sytems, since goals
are encoded and adaptive actions are selected through a process of biased perception
and inference whereby an agent does not ﬁrst infer a true trajectory, compares it to its
goals, and then tries to match the too, instead it simply sees a biased trajectory leading
to its goals and then acts consonantly with what it sees. This view has close links to
Chapter 5. The Mathematical Origins of Exploration 258
embodied and enactivist views in philosophy and cognitive science which stress that
rather than distinct modular systems of perception, valuation, and action there is instead
a single combined system or sensorimotor loop which directly acts on sensorimotor
contingencies in an adaptive fashion (Baltieri & Buckley, 2018).
5.4.1.3 Encoding Value into the Variational Distribution
Previously, we have encoded values either exogenously or endogenously into the
generative model of the agent. In the case of endogenous encodings, this means that the
agent makes biased predictions, rather than forming biased inferences. However, it is
also possible to consider and investigate what happens if instead values were encoded
into the variational distribution so that the agent’s inference procedure rather than model
is biased. We ﬁrst consider the case of exogenously encoded goals. This requires that
the variational distribution is augmented with binary optimality variables, just like the
generative model was previously giving q(at,xt,Ωt|ot). From this we can write the
relevant free energy functional:
FqΩ = DKL
(
q(at,xt,Ωt|ot)∥p(ot,xt,at)
)
= DKL
(
q(Ωt|xt,at)q(at|xt)q(xt|ot)∥p(ot|xt)p(at|xt)p(xt|xt−1,at−1)
)
= Eq(xt ,at |ot )
[
lnq(Ωt|xt,at)
]
  
Extrinsic Value
−Eq(xt ,at |ot )
[
ln p(ot|xt)
]
  
Observation Ambiguity
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
+DKL
(
q(xt|ot)∥p(xt|xt−1,at−1)
)
  
State Divergence
(5.41)
We see that the resulting functional, under the assumption thatlnq(Ω|xt,at) ∝ exp(r(xt,at),
is exactly equivalent to the functional obtained for the exogenously encoded generative
model, up to a sign difference in the extrinsic term which can be ﬁnessed without loss
of generality by inverting the sign of the reward function. We thus see that, if values are
exogenously encoded, it does not matter which distribution they are primarily encoded
through. Intuitively this is because since the veridical distributions are maintained
Chapter 5. The Mathematical Origins of Exploration 259
through endogenous coding, they are unaffected by the encoding of value into them,
thus which one to choose has no effect overall up to a trivial sign difference which can
be easily ﬁnessed through negatively encoding reward.
When rewards are endogenously encoded, however, the resulting functionals are not
equivalent. We ﬁrst show this with a biased-state functional which should be compared
to Equation 5.41, where we directly bias the state-inference part of the variational
functional so as to preferentially infer being in desired states from a given observation
˜q(at,xt|ot) =: q(at|xt) ˜q(xt|ot). Through this decomposition we obtain the functional:
Fstate−q−AIF = DKL
(
˜q(xt,at|ot)∥p(ot,xt,at)
)
= DKL
(
q(at|xt) ˜q(xt|ot)∥p(ot|xt)p(xt|xt−1,at−1)p(at|xt)
)
= DKL
(
˜q(xt|ot)∥p(xt|xt−1,at−1)
)
  
Extrinsic Value
−Eq(xt ,at |ot )
[
ln p(ot|xt)
]
  
Observation Ambiguity
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
(5.42)
which is very similar to the corresponding state extrinsic value functional in Equation
5.40, except that in the extrinsic value term the divergence is between the biased
variational posterior and a veridical generative prior, rather than a veridical variational
posterior and a biased generative prior. By having the biases occur on the left side
of the KL, we are essentially minimizing the reverse-KL compared to when value is
encoded into the generative model. This gives the resulting agents a mode-seeking
rather than a mean-seeking behaviour, since agents optimizing under the reverse KL will
suffer a large penalty if the desires are in regions with a very low veridical probability.
Moreover, there is a further subtle difference in the POMDP case here since we are
contrasting the biased distribution with a prior rather than a posterior. However, in the
Chapter 5. The Mathematical Origins of Exploration 260
MDP case this difference vanishes, and we obtain the simpliﬁed functional:
FqMDP = DKL
(
˜q(at,xt)∥p(xt,at)
)
= DKL
(
˜q(at|xt) ˜q(xt)∥p(at|xt)p(xt|xt−1,at−1)
)
= DKL
(
˜q(xt)∥p(xt|xt−1,at−1)
)
  
Extrinsic Value
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
(5.43)
Which is lacking the observation ambiguity term due to being an MDP, and also
the extrinsic value has become the divergence between desired variational states and
predicted generative states. This functional is exactly equivalent to the alternate formula
with values encoded endogenously into the generative model except that it uses the
reverse KL divergence. Moreover these functionals are deeply related to KL control
with an additional action divergence term, and thus when value is instead encoded
into the variational distribution we have reverse-KL control which uses the reverse KL
divergence and is very closely related to pseudo-likelihood methods in reinforcement
learning (Abdolmaleki et al., 2018; Peters & Schaal, 2007).
Overall then we have explored two orthogonal axes of variation for the problem of how
to encode a notion of value, reward, or desires into an otherwise value-agnostic varia-
tional inference procedure in order to be able to infer adaptive actions. We have shown
that the ﬁrst question of whether value is to be encoded exogenously or endogenously
makes subtle but signiﬁcant differences in the resulting functionals. Speciﬁcally, by
requiring the utilisation and biasing of one of the variables in the model to encode value,
endogenous encoding tends to lose one degree of freedon in its functional compared to
exogenous encoding. In the POMDP setting this is typically the observation-ambiguity
term if goals are encoded into observations, or the state-divergence term if goals are
encoded into states. Beyond this, we see that these different means of encoding value
also has signiﬁcant philosophical importance as to the nature of perception, action
and value. Exogenously encoding goals supports a modular description of these three
functions as independent systems which are each agnostic with respect to the outputs of
Chapter 5. The Mathematical Origins of Exploration 261
the others, while endogenously encoding goals merges them all together into a mixed
system where action and perception are intrinsically biased by goals towards adaptive
outcomes. Moreover, this dichotomy of exogenous or endogenous encoding is the
primary difference between variational control frameworks arising from reinforcement
learning and from active inference, and this fact speaks to the difference in underly-
ing cognitive philosophy between these theories where reinforcement learning draws
heavily from cognitivist and representational traditions in artiﬁcial intelligence which
prize principled, independent, and modular systems, while active inference comes
from a heavily embodied and enactive viewpoint inﬂuenced by dynamical systems
theory whereby systems are seen primarily in terms of their situatedness within a action-
perception sensorimotor loop, and there is not necessarily any clean distinction between
phases or subsystems of this loop.
Finally, we have seen that the second axis of variation is whether the goals are encoded
into the generative model or the variational distribution. The effects of this difference
are subtler than for the exogenous vs endogenous encoding dichotomy. Variational and
generative encodings are equivalent up to a sign difference in the exogenous case since
the encoding does not affect the veridicality of the distribution, while in the endogenous
case this causes subtle differences such that the generative and variational encodings
are typically equivalent up to the extrinsic value terms which becomes the reverse-KL
for the variational encoding relative to the generative. This leads to a close connection
with pseudolikelihood methods in reinforcement learning (Abdolmaleki et al., 2018)
and in fact provides a generalisation of these methods to the POMDP setting.
5.4.2 General Graphical Models
In all previous work, we have primarily focused on how the algorithms and functionals
differ when placed into a POMDP setting with visible observations and unobserved
(Markovian) hidden states, and actions which affect the hidden states. However this
Chapter 5. The Mathematical Origins of Exploration 262
is fundamentally a modelling choice. For instance, if state information is perfectly
observed, as is often assumed in RL, then the problem reduces to a simple MDP
formulation. The MDP formulation with exogenous rewards was addressed in Equation
5.41, however when encoding rewards endogenously, this must be encoded into the
generative or variational distributions, so that with the FEEF objective functional the
extrinsic term becomes simply a state divergence.
FEEF MDP = DKL
(
q(xt,at)∥˜p(xt,at)
)
= DKL
(
q(xt)∥˜p(xt)
)
  
Extrinsic Value
+Eq(xt )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
(5.44)
Here no information gain is possible since states are directly observed, thus the only
exploration is through random entropy maximizing terms. Importantly, if the goals
were instead encoded into the variational distribution, the only effect this would have
would be to ﬂip the extrinsic value KL into a reverse-KL and thus induce mode-seeking
rather than mean-seeking distribution matching behaviour. In the MDP setting, then,
we obtain a maximum-entropy KL-control objective.
While it is possible to restrict the generative models only to MDPs, we can also
consider extending them to also explicitly model the prior and posterior distributions
of parameters underlying the variational and generative distributions. For instance,
we have implicitly been utilising a variational posterior q(xt|ot), a transition model
p(xt|xt−1,at−1), a likelihood model p(ot|xt), an action policy q(at|xt) and an action
prior p(at|xt). All of these distributions may have parameters, or be parameterised by a
ﬂexible function approximator such as a neural network, which itself has parameters.
These parameters can be be included in the variational inference procedure by adding
a generative model and variational distribution over the parameters. For instance,
supposing we have some set of parameters θ which parametrise the transition model
such that the transition model becomesp(xt|xt−1,at−1;θ), then we can pose an inference
problem and write down a variational free energy functional which also includes a model
Chapter 5. The Mathematical Origins of Exploration 263
over parameters:
Fθ = DKL
(
q(xt,at,θt|ot)∥˜p(ot,xt,at,θt)
)
= DKL
(
q(at|xt)q(θt|xt)q(xt|ot)∥˜p(ot|xt)p(at|xt)p(xt|xt−1,at−1,θt)p(θt)
)
= −Eq(xt |ot )
[
ln ˜p(ot|xt)
]
  
Extrinsic Value
+Eq(xt |ot )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
+Eq(θt )
[
DKL
(
q(xt|ot)∥p(xt|xt−1,at−1,θ)
)]
  
State Divergence
+DKL
(
q(θt|xt)∥p(θt)
)
  
Parameter Divergence
(5.45)
We thus see that minimizing the variational free energy directly, requires minimizing
the divergence between posterior and prior beliefs over the parameters, thus implicitly
penalising updates which cause large parameter updates, and thus disincentivising
exploration. In general, by adding additional variables to the variational free energy,
we simply obtain additional divergence terms to be minimized as above, essentially
penalising deviations between the posterior and prior beliefs for that variable. However,
and analogously with the states, when we utilize a FEEF objective functional, we
can obtain a parameter information gain term as well as a countervailing parameter
approximation error term as shown below:
FEEF θ = DKL
(
q(ot,xt,at,θt)∥˜p(ot,xt,at,θt)
)
= DKL
(
q(ot|xt)q(at|xt)q(xt|θ)q(θt)q(xt|ot)q(θt|xt)
∥˜p(ot|xt)p(at|xt)p(xt|xt−1,at−1,θt)p(θt)q(xt|ot)q(θt|xt)
)
= Eq(at ,xt )
[
DKL
(
q(ot|xt)∥˜p(ot|xt)
)]
  
Extrinsic Value
+Eq(ot ,xt )
[
DKL
(
q(at|xt)∥p(at|xt)
)]
  
Action Divergence
−Eq(ot )
[
DKL
(
q(xt|ot)∥q(xt)
)]
  
Expected Information Gain
+Eq(ot )q(θt )
[
DKL
(
q(xt|ot)∥p(xt|xt−1,at−1,θt)
)]
  
Expected Posterior Divergence
−DKL
(
q(θt|xt)∥q(θt)
)
  
Parameter Information Gain
+DKL
(
q(θt|xt)∥p(θt)
)
  
Parameter Divergence
(5.46)
Chapter 5. The Mathematical Origins of Exploration 264
In the general case adding additional variables to the FEEF objective will create an
information gain term in that variable as well as the counteracting posterior divergence
term. Analogously, adding additional variables to the class of divergence functionals will
also create information gain terms in those variables without the posterior divergence.
The reason for this behaviour can ultimately be derived directly from the variational
marginal entropy by considering augmenting it with parameters θ:
V ME = Eq(ot )
[
lnq(ot)
]
= Eq(ot ,xt ,θt )
[
lnq(ot)
]
= Eq(ot ,xt ,θt )
[
ln q(ot,xt,θt)
q(xt,θt|ot)
]
= Eq(ot ,xt ,θt )
[
ln q(ot|xt)q(xt|θt)q(θt)
q(xt|ot)q(θt|xt)
]
= − H
[
q(ot|xt)
]
  
Likelihood Entropy
−Eq(ot )q(θt )
[
DKL
(
q(xt|ot)∥q(xt|θt)
)]
  
Expected State Information Gain
−DKL
(
q(θt|xt)∥q(θt)
)
  
Parameter Information Gain
(5.47)
Thus we have seen that by implicitly maximizing the marginal entropy, we in fact
are implicitly optimising the information gain for any latent variables in the model.
Similarly, as discussed previously, the primary difference between exogenous and
endogenous encoding of value is that endogenous encodings lack a degree of freedom
that exogenous encodings can make use of. In general therefore, as we augment the
graphical models these functionals are derived from with additional variables, we see
that the exogenous encoding is roughly equivalent to the endogenous encoding with
an additional degree of freedom. This thus derives two ‘scaling laws’ for our families
of functionals as additional sets of variables are added to the functional – that the
exogenous encoding will approximate the endogenous encoding with an additional
degree of freedom, and that with VFE derived functionals we will obtain divergence
terms to be minimized between the posterior and prior for the variable, with a FEEF
functional we will obtain an information gain term and a posterior approximation error
term, while with a divergence based functional we will obtain an information gain term
only in the new variable.
Chapter 5. The Mathematical Origins of Exploration 265
5.5 Discussion
In this chapter, we have answered and discussed two key questions. Firstly, we now
understand the mathematical origin of information-seeking exploration terms in varia-
tional objectives for control. While this appears arcane, this is actually a question with
deep implications both philosophically, as well as for applications. On a philosophi-
cal and mathematical note, we have uncovered the principled mathematical origin of
information-maximizing exploration. We see that it arises from minimizing divergences
between a predicted and a desired distribution, and speciﬁcally from the predicted
entropy maximization half of the divergence objective. In effect, we see that it is by
trying to maximize the entropy of future observations, that induces information seeking
exploration in latent space, whenever any latent states or parameters are added to the
generative model. This makes intuitive sense – if the future is broad and entropic, it
contains much information, and to maximize that breadth entails learning about the
world in order to ensure a precise match everywhere between predicted distribution
and a complex desire distribution. Conversely, we have seen that standard evidence
objectives used in variational inference or control result in effectively trying to match the
predicted and desired distribution while trying to minimize the entropy of future states
or, alternatively, to make the future maximially predictable as well as in conformity to
the agent’s desires. As such, the agent’s goal is to effectively minimize the amount of
information it receives about the world, learning only as much as is sufﬁcient for control,
and thus not giving rise to any kind of information-seeking exploratory behaviour. We
can thus understand precisely why standard objectives such as control as inference are
insufﬁcient to obtain information seeking behavioural objectives. Similarly, our ap-
proach allows us to understand and rationalize a number of approaches in the literature
(Klyubin et al., 2005; Oudeyer & Kaplan, 2009; Sun et al., 2011) which add additional
exploratory terms to their reward-maximizing agents, with the heuristic justiﬁcation
that they should increase exploration. Indeed, many such approaches are implicitly
minimizing a divergence objective without realizing it. Our advances here make this
Chapter 5. The Mathematical Origins of Exploration 266
practice explicit and allows one to understand the precise mathematical nature of the
objective being optimized. Moreover, this distinction also sheds light on the possible
objective functions used by biological creatures such as humans in psychological or
behavioural economics tasks, for instance, one can understand the otherwise-puzzling
phenomenon of probability matching, as the inevitable outcome of optimizing a diver-
gence functional, and this elegantly explains both why it is present and additionally, why
it is beneﬁcial – since optimizing this objective in more complex environments naturally
leads to exploratory information-seeking behaviour which often will outperform pure
reward maximization even on its own terms.
This approach is also useful for applications, since we can begin by deriving a variety
of methods using divergence objectives and understanding their exploratory behaviour.
While little work has yet been done on explicitly divergence minimizing agents, we
believe that this will be an important area in the future, as successful protocols and
algorithms for exploration in reinforcement learning will become increasingly important
as the sparsity, dimensionality, and difﬁculty of RL benchmark tasks increases.
Secondly, we understand the relationship between control as inference and active
inference. We know that the key difference is simply a difference in objective function
between the two (the EFE vs the FEF), and secondly a distinction between them is
their encodings of value – that active inference uses an endogenous, while control
as inference uses an exogenous encoding. Moreover, understanding the distinctions
between the two theories, as well as the general and broad distinction between evidence
and divergence objectives, then allows us to raise our eyes and perceive a much larger
vista of the full landscape of potential objective functionals for control. In the latter
half of this chapter, we have seen that these functionals can vary along two orthogonal
dimensions – whether an evidence or divergence functional is used, as well as the
nature of their encoding of value (endogenous or exogenous) as well as whether value
is encoded into the variational or generative distribution. Moreover, we have derived a
Chapter 5. The Mathematical Origins of Exploration 267
good understanding of the impact of different deﬁnitions of the generative model upon
the objective functionals that result. This allows us a broad and unique understanding of
the possible space of objective functionals, as well as the design choices which inﬂuence
which one to choose. Future work in this area should investigate the actual impacts
of different choices on agent behaviour, both in simple toy environments where the
effects can be easily understood, as well as in more complex and difﬁcult benchmark
tasks where using different objectives may well give rise to algorithms which are more
effective than current agents. As an example, while only using random exploration,
control as inference inspired approaches such as the soft-actor-critic have given rise
to state of the art performance. We see no reason why more exploratory information-
seeking agents, powered by divergence objectives, should not lead to similar gains in
performance, especially on challenging sparse-reward tasks. While we present some
preliminary results to this effect in Chapter 4, much more work remains to be done to
pin down what gains, if any, are possible by this approach.
Finally, our approach directly gives us the objective functionals utilizing different
generative models. For instance, if you want to extend your reinforcement learning
algorithms to POMDPs, or POMDPs with hierarchical levels of latent states, or to
explicitly model distributions over different model parameters, or to explicitly model a
reward distribution or reward model, then our framework provides a recipe for precisely
and immediately deriving the necessary objective to optimize.
In the next chapter, we move on to consider applications of the free energy principle
to learning, where we focus on deriving novel algorithms which can perform credit
assignment in neural networks in a biologically plausible fashion.
Chapter 6
Credit Assignment in the Brain
6.1 Introduction
In this chapter we shift gears again and now consider applications of the free energy
principle to the problems of learning in the brain. Speciﬁcally, here we aim to under-
stand the nature of credit assignment in the brain, and focus on how and whether the
backpropagation of error algorithm which underpins all the recent successes of machine
learning in training deep artiﬁcial neural networks, could potentially be implemented in
the brain. In this chapter, we present the fruit of our work investigating this extremely
important question for the case of rate-coded integrate and ﬁre neurons engaged in a
static task (such as object recognition), where there is only a feedforward pass to be
concerned with and all backpropagation is through space, and not time. While this
setting is considerably simpliﬁed from the one the brain faces in reality, it is also much
more tractable and well-understood and solving the problem in this domain may provide
vital clues into the full solution.
This chapter is split into four relatively independent sections. In the ﬁrst section, we
provide a general introduction and mini literature review on backpropagation and previ-
ous attempts to derive biologically plausible algorithms to implement backpropagation
268
Chapter 6. Credit Assignment in the Brain 269
in the brain. In the next two sections, we then present our work deriving new algorithms
for biologically plausible approximations to the backpropagation of error algorithm.
In the ﬁrst section, we show that predictive coding – the free energy process theory
from chapter 3 – can, if set-up correctly, exactly approximate the backpropagation of
error algorithm along arbitrary computation graphs. This result is fascinating since
predictive coding has a long history and well-developed literature on its properties,
performance, and especially its biological plausibility, as well as possessing several
well-developed theoretical neural implementations (Bastos et al., 2012; Kanai et al.,
2015; Keller & Mrsic-Flogel, 2018). We empirically validate this approximation to
backprop and showcase that predictive coding can perform equally to backprop at
training complex machine learning architectures such as CNNs and LSTMs.
Secondly, we develop a novel algorithm – Activation Relaxation (AR) – which also
can asymptotically converge to the required backpropagation error gradients using only
local connectivity – and which does not require two separate population of value and
error neurons. We empirically show that this algorithm can train complex machine
learning architectures with performance equal to backprop and, additionally, demon-
strate that the same relaxations shown in Chapter 3 for predictive coding – such as
using learnable backwards weights to overcome the weight transport problem, and
dropping the nonlinear derivatives also work for the AR algorithm, thus importantly
both substantially improving the overall biological plausibility of the AR algorithm as
well as demonstrating the generalizability of the results in Chapter 3 to other algorithms
1.
Finally, in the third section, we have included a more speculative discussion on a
potential further algorithm for solving backprop in rate-coded neurons directly, instead
of in an iterative fashion. We discuss the possible limitations of this algorithm as well
as the required neural circuitry and, for the ﬁrst time, begin to precisely understand
1For this thesis, the AR algorithm is not directly related to the FEP, although it was initially inspired
by research into predictive coding which is a process theory of the FEP
Chapter 6. Credit Assignment in the Brain 270
what the key problems are for the rate-coded sense and also what a real solution would
look like.
6.1.1 Backpropagation in the Brain
Due to the immense success of machine learning approaches based upon connection-
ist deep neural networks trained upon the backpropagation of error algorithm, our
paradigms of how the brain functions is also shifting. Speciﬁcally, the paradigm that the
brain, or at least the neocortex, is fundamentally a blank-slate learning machine which
uses general purpose learning algorithms to handle inputs, akin to a deep neural network
is becoming increasingly inﬂuential in neuroscience, partially displacing older views
that the brain consists of a series of separate ‘modules’ (Fodor, 1983; Pinker, 2003),
each of which performs a single specialized function using what are effectively speciﬁc
and pre-set algorithms, hard-coded over the course of evolutionary history. While
functional specialisation is an extremely notable characteristic of the brain, it is more
widely believed that this specialisation, especially in the cortex, is due to differences in
input and small inductive biases shaping the nature and output of a very general learning
algorithm which is implemented throughout the cortex, rather than each functional
module possessing its own independent and isolated suite of algorithms. This view
is supported by evidence of a remarkable uniformity of cortical cytoarchitecture and
neuroanatomy, belied by the heterogeneity observed in subcortical areas (Bear, Connors,
& Paradiso, 2020).
While, for a long time, it was empirically unclear whether the fundamentals of intelli-
gence – such as robust and generalizable perception, natural language capabilities, and
adaptive action planning – could emerge solely from learning algorithms with relatively
few inductive biases, applied to vast amounts of data, the past decade and its immense
advances in machine learning are suggestive that this may be possible after all. Modern
machine learning, effectively, represents the culmination and empirical veriﬁcation of
Chapter 6. Credit Assignment in the Brain 271
earlier connectionist theory (Rumelhart, Hinton, & Williams, 1986).
This view places the central focus on learning. Since the backpropagation of error
algorithm has proven so immensely successful in machine learning, to train an extremely
wide variety of architectures to perform an impressive array of tasks (Goodfellow et al.,
2014; Krizhevsky et al., 2012; Radford et al., 2019; Schmidhuber, 1999; Schrittwieser
et al., 2019), and given that the brain itself faces an almost identical credit assignment
problem in having to adjust synaptic strengths to allow for learning to occur, it is a very
interesting and important question to ask whether the learning algorithm implemented in
the brain could simply be backprop. If this question were conclusively answered in the
afﬁrmative, it would represent an enormous conceptual breakthrough in neuroscience
since it would provide, for the ﬁrst time, a general and powerful organizing principle
for the brain (or at least the cortex) as a whole, it would allow the importation directly
into neuroscience of a large quantity of results from machine learning. Such an answer
would additionally have deep philosophical implications. It would imply that there
is very little effective difference between current machine learning methods and the
kinds of learning, inference, and planning algorithms that are implemented in the brain
to give rise to undeniably intelligent and apparently conscious behaviour, and as such
would imply that the current paradigm in machine learning sufﬁces, with more scale
and potentially more expressive architectures, to create fully general intelligences akin
to humans or beyond (Bostrom, 2017).
On the other hand, if it were shown that the brain were conclusively not doing backprop,
then this would also be an advance, although a lesser one, in neuroscience. Such a
conclusion would necessarily shed light upon the actual algorithms utilized by the brain
for credit assignment and learning, which would provide both a general principle for
understanding the function and operation of the brain over time, as well as undoubtedly
provide key insights for machine learning in developing, improving, and scaling up
current methods.
Chapter 6. Credit Assignment in the Brain 272
The theory and algorithm for backpropagation of error (Backprop) emerges in the 1970s
(Linnainmaa, 1970), and by the 1980s was widely used for training connectionist neural
networks (Griewank et al., 1989; Rumelhart et al., 1986; Rumelhart & Zipser, 1985)
Already in the 1980s, researchers had proposed that backprop could be implemented
in the brain, and tried to ﬁnd commonalities between then contemporary neuroscience
and progress in connectionist modelling using neural networks (Rumelhart et al., 1986).
However, a number of articles argued convincingly that a direct implementation of
backpropagation is biologically implausible (Crick, 1989), which dampened down
potential interest in this connection considerably until the question was re-raised by
the successes of machine learning in the 2010s. Before investigating the ways in
which backpropagation appears biologically implausible, and how these issues might
be addressed, we ﬁrst give a detailed introduction to the backprop algorithm.
First we must decide on some terminology. Neural networks are typically trained to min-
imize some loss function L. The credit assignment problem concerns the computation
of the derivatives of the loss with respect to every parameter of the network ∂L
∂W . Back-
propagation of error is an algorithm that solves the credit assignment problem exactly
using the technique of Automatic Differentiation (AD) (Baydin, Pearlmutter, Radul, &
Siskind, 2017; Griewank et al., 1989; Paszke et al., 2017; Van Merriënboer, Breuleux,
Bergeron, & Lamblin, 2018). 2 . Given these derivatives, the network can be trained
with the stochastic gradient descent algorithm Wt+1 = Wt +η ∂L
∂W . However, given the
gradients computed by backprop, other gradient algorithms are possible including a
variety of modiﬁed descent procedures such as Nesterov Momentum (Nesterov, 1983),
RMS-prop (Hinton et al., 2012) and Adam (Kingma & Ba, 2014) second order methods
such as natural gradients (Amari, 1995) and Gauss-Newton optimization, as well as
stochastic sampling methods such as stochastic langevin dynamics (Welling & Teh,
2011), and Hamiltonian MCMC (Neal et al., 2011).
2There are other methods for computing derivatives, such as ﬁnite differences, but they are less
accurate and more computationally costly than AD and are not generally used in machine learning
Chapter 6. Credit Assignment in the Brain 273
The fundamental mathematics underlying backprop is the chain rule of calculus. Es-
sentially, if we have a function – such as a neural network – which consists of the
composition of many differentiable functions, then we can express the derivative of
a complex composite function as a product of the derivatives of all the component
functions with respect to one another. Suppose we have the forward function and loss
function,
ˆy = f L(WL f L−1(Wl−1 f L−2(. . .f 0(W0x))))
L = g(ˆy,t) (6.1)
where ˆy is the prediction outputted by the neural network, L is the number of layers,
[ f L . . .f 0] is the activation functions for each layer and [WL . . .W0] is the weights or
parameters for each layer. L is the overall loss function, t is the desired target outputs
and g is the loss function. With this forward function, we can compute the derivative of
the loss with respect to any parameter set – for instance W0 using the chain rule,
∂L
∂W0 = ∂L
∂ˆy
∂ˆy
∂yL
[l=1
∏
l=L
∂yl
∂yl−1
]∂y1
∂W0 (6.2)
which allows us to express the derivative of a product of all the derivatives of the
intermediate component functions. In general, this is possible for any function as long
as every component function is differentiable. We can represent any function as a
computation graph which is a graph where each intermediate step in the computation is
a vertex and each component function is an edge. While this example, and most simple
neural network, is simply a chain graph, other more complex graphs are possible. If
there are multiple paths through the graph, the derivatives of the paths are summed
together. As long as every component function is differentiable, every one of the
derivatives in Equation 6.2 can be explicitly computed and evaluated, thus allowing
the full derivative ∂L
∂W0 to be computed explicitly. Importantly, the computational cost
of such an evaluation is generally linear in the number of component functions – and
thus is of approximately the same complexity as simply evaluating the function in the
ﬁrst place. AD approaches like this, then, allow for the evaluation of derivatives of
Chapter 6. Credit Assignment in the Brain 274
any differentiable computation for a low and constant additional computational cost –
allowing for their widespread use within machine learning.
There are two approaches in AD to computing the chain of derivatives as in Equation 6.2,
which are called ‘forward-mode’ and ‘reverse-mode’ AD. The difference between these
methods is effectively whether the product of derivatives is computed from right to left
(forward-mode) or left to right (reverse mode). Forward-mode effectively accumulates
the gradients starting with the initial jacobian ∂y1
∂W0 and then moving leftwards down the
chain, in the same direction as the original function evaluation. This allows derivative
evaluation to take place in parallel with original function evaluation through the use of
‘dual numbers’ (Griewank et al., 1989) which extend every number with an ‘derivative
part’ analogous to how complex numbers extend real numbers with a complex part.
Since dual-numbers can be evaluated in parallel with the original function evaluation,
the computational cost of forward-mode AD is of the order of the input dimension
and it has a constant memory cost, since no intermediate products need to be stored in
memory.
Reverse-mode AD, conversely, evaluates the chain of derivatives from left to right. That
is, it starts with and accumulates onto the vector ∂L
∂ˆy , which is also called the adjoint
or pullback 3. It then iterates recursively ‘backwards’ through the chain using the
following equation,
∂L
∂yl = ∑
j
∂L
∂yl+1
j
∂yl+1
j
∂yl (6.3)
where the sum simply states that if there are multiple potential paths in the graph,
they should be summed together. Since it starts at the ‘end’ and works backwards,
reverse-mode AD requires the function to be evaluated ﬁrst after which the derivatives
can begin to be computed in a backwards sweep, thus leading reverse-mode AD to
use characteristic forward (function evaluation) and backwards (derivative evaluation)
sweeps. Since all intermediate activities must be stored in memory, reverse-mode
3In continuous time, Equation 6.3 becomes the adjoint ODE
Chapter 6. Credit Assignment in the Brain 275
AD has a memory cost linear in the number of component functions, as well as a
computational cost which scales with the dimension of the output. Since neural networks
typically have a scalar output loss and very high dimensional inputs (such as image
pixels), reverse-mode AD is typically computationally cheaper and is the method used
in practice for training deep networks. Reverse mode AD also has the advantage that
it backpropagates the gradients back to where the weights are directly, while forward-
mode AD does compute the weights, but they are all bunched up at the end of the graph
by the loss, and therefore would need, in a physical system such as the brain, to be
transmitted back to where the weights were originally as well.
Here, we focus primarily on the implementation of reverse-mode AD in the brain due to
its generally superior computational capabilities (and avoidance of this weight locality
problem for forward mode AD). It is possible, however, that the brain may use some
combination of forward and reverse mode in practice. One especially appealing method
is to use reverse-mode AD to handle hierarchical networks – i.e. nonlocality in space,
while using forward mode AD to handle recurrent credit assignment through time. Here
forward mode AD has the clear advantage that the gradients move forward in time at the
same rate as the weights themselves, so that there is no locality problem here. In fact,
the locality problem now afﬂicts reverse-mode AD which, in this circumstance, needs to
backpropagate gradients backwards through time, which is problematic. Here we do not
address the temporal credit assignment problem and focus entirely on backpropagation
through space, where we assume reverse-mode AD is the best approach.
Although reverse-mode AD is a well characterised algorithm, it is not at all clear
whether it can be implementated in the brain. Speciﬁcally, backpropagation has three
principal problems which make its apparent biological plausibility dubious – ﬁrstly,
backpropagation appears to require non-local information transfer, since the gradient of
the synaptic weights depends on activity from the rest of the network which ultimately
leads to the ﬁnal loss outcome. Secondly, if we look at the speciﬁc case of a rate-coded
Chapter 6. Credit Assignment in the Brain 276
standard integrate and ﬁre model, whereby the output is a function of the input and the
synaptic weights,
yl+1 = f (Wlyl) (6.4)
then the derivative of the loss with respect to the pre-activations becomes,
∂L
∂yl
= ∂L
∂yl+1
∂f (Wlyl)
∂yl WlT
(6.5)
which requires both the derivative of the activation function, which may or may not be
easy to compute locally, and also the transpose of the feedforward weightsWlT . This
transpose is problematic since effectively it requires the backwards pass activations to
be sent ‘backwards’ through the forwards weights – a process which is biologically
implausible. This problem is called the ‘weight transport problem’. Finally, if we look
at the update rule for the weights themselves,
∂L
∂Wl = ∂L
∂yl+1
∂f (Wlyl)
∂Wl yL (6.6)
which while it does depend on the pre-synaptic activationsyl also depends on the adjoint
vector ∂L
∂yl+1 which is generally non-local. Interestingly, the update rule speciﬁcally
does not depend at all on the post-synaptic activity, in contrast to the widely accepted
view of Hebbian plasticity being implemented in the brain, although it does depend
on the derivative of the post-synaptic activity with respect to the weights ∂f (Wlyl)
∂Wl ,
which may or may not be difﬁcult to compute. The issue ﬁrst of computing the adjoint
vector ∂L
∂yl+1 in a biologically plausible manner, and then transmitting it to the required
synapses, since it is non-local, thus form the core issue standing in the way of any
biologically plausible implementation of reverse-mode AD. A ﬁnal issue relates to the
need, in reverse-mode AD, for separate forward and backwards phases, while the brain
presumably needs to operate continuously in time. It has been suggested that the brain’s
rhythmic oscillations (Buzsaki, 2006) may allow it to ‘multiplex’ forward and backward
passes together, although this intuition has not yet, to my knowledge, been made precise
in the literature.
Chapter 6. Credit Assignment in the Brain 277
Due to a general understanding that backpropagation is biologically implausible, much
research has focused on other potentially more biologically plausible methods by which
the brain might learn. A large amount of attention has focused on Hebbian update
rules (Gerstner & Kistler, 2002), which only utilize the post and pre synaptic activities,
based on the initial intuitions of Donald Hebb (Hebb, 1949). A number of variants of
Hebbian learning have been proposed, and the full class of potential algorithms has
been exhaustively analysed in (Baldi & Sadowski, 2016). However, a key issue with
Hebbian learning is that, since it can only use local information in the form of pre and
post synaptic activities, it cannot incorporate information about the distant loss function,
and thus allow for the precise goal-directed learning that backprop is capable of. In
effect, Hebbian learning can only capture and strengthen the local correlations of ﬁring
rates across a network and thus, while it can often be used to solve tasks in shallow
networks with just a single or a few hidden layers, it fails to scale successfully to deep
layers (Lillicrap & Santoro, 2019).
A second approach is to use global neuromodulators as a part of a ‘three-factor’ learning
rule which includes contributions from both the pre-synaptic and post-synaptic activity
as well as this ‘third-factor’ (Gershman, 2018) which is often conceptualised to be
dopamine, in light of the fact that dopaminergic connections from the mid-brain are
well-known to innervate large parts of the cortex (Daw et al., 2006). These dopaminergic
neurons could be signalling some kind of global reward signal, inducing all neurons
to increase their weight when a positive reward is encountered and decrease it when
a negative reward occurs (Lillicrap et al., 2020; Roelfsema & Ooyen, 2005; Seung,
2003), in a procedure which is effectively equivalent to policy gradient methods from
reinforcement learning (R. J. Williams & Zipser, 1989a). While such approaches can,
asymptotically, learn complex functions in deep neural networks, their key limitation is
that the gradient estimates they compute have extremely high variance, since the global
neuromodulator cannot distinguish whether a particular weight helped give rise to a
reward or not, and thus cannot provide precise feedback like backprop. Instead it takes
Chapter 6. Credit Assignment in the Brain 278
a substantial amount of trials for the random noise provided by the contributions of
all the other neurons in the brain to be averaged out to get at the contribution of just
a single synaptic weight, which leads to slow and unstable learning in complex tasks
with deep networks (Lillicrap & Santoro, 2019). Additionally, this approach implicitly
assumes that the entire brain is optimized end-to-end for reward, however this seems
potentially unlikely given that large parts of the cortices deal with aspects like sensory
stimuli which are distant from reward and most likely use auxiliary losses such as their
own immediate prediction errors. Nevertheless, if it turns out that backpropagation
is not, in fact, used in the brain, then global neuromodulatory rules like this may be
the second best option. Importantly, even if it is the case that the brain does backprop,
it likely also uses this global neuromodulatory approach in a modulatory function to
bias learning towards high reward contingencies and perhaps also to adaptively tune
learning rates throughout the cortex so that highly valenced experiences (either positive
or negative) have a strong effect on plasticity throughout the brain.
Finally, there is also a small but growing literature attempting to understand how and
whether backpropagation can be directly implemented in the brain – namely whether it
is possible to design neural circuits to work around the key limitations proposed earlier.
An important line of work tackles the weight transport problem. Lillicrap et al. (2016)
demonstrate that in reality precise copying of the forward and backwards weights is not
necessary, and in fact random ﬁxed backward weights sufﬁce due to the phenomenon
of ‘feedback alignment’ whereby the random feedback weights effectively force the
forward weights to align with the backward ones to be able to learn. This approach
can be improved by ensuring that the sign of the elements of the feedback weight
matrix matches the sign of the forward weight matrix (potentially a more biologically
plausible constraint than exact copying of values) (Liao et al., 2016), or else learning
the backwards weights using an additional plasticity rule (Akrout et al., 2019; Amit,
2019; Millidge, Tschantz, Seth, & Buckley, 2020d). While the feedback alignment
technique does not typically scale to deep architectures, as the feedback path becomes
Chapter 6. Credit Assignment in the Brain 279
increasingly corrupted (Bartunov et al., 2018), an approach called direct feedback
alignment (DFA) (Nøkland, 2016), whereby all layers are directly connected to the
output layer through random backwards weights, has been shown to be able to scale
to large deep architectures (Launay, Poli, & Krzakala, 2019), although not the usual
convolutional neural networks used in vision. While an impressive result, DFA itself
violates known neural connectivity constraints which feature reciprocal connectivity
between regions and not every layer receiving direct backwards connections from the
‘output’.
Secondly, another line of work has focused on the algorithm of target-propagation
(Bengio & Fischer, 2015; D.-H. Lee, Zhang, Fischer, & Bengio, 2015) which is similar
to backprop except that instead of providing gradients back to the weights at each layer
it provides targets which can then be optimized locally. These targets are produced by
mapping backwards the output of the network ‘nudged’ towards the true target through
an inverse mapping at each layer. The intuition, is that we want to ﬁnd the targets
which, if the lower layers had matched their activations, would have produced a ﬁnal
output closer to the target. The past year has seen substantial advances in the theoretical
analysis of target-prop, where it is now recognised to not approximate backprop, but
instead be performing a hybrid form of Gauss-Newton optimization (Bengio, 2020;
Meulemans, Carzaniga, Suykens, Sacramento, & Grewe, 2020). While promising, large-
scale studies on the stability and scalability of target-prop learning have not yet been
done, although initial results are promising (Bartunov et al., 2018). Additionally, target-
prop does not provide solutions to the weight transport problems (now complicated by
the additional necessity of learning the backwards inverse weights), and the necessity
of storing and comparing the forward and backward information across phases. The
intuition of using the ﬁnal loss function to compute layer-wise targets has also been
applied, with variations, in other works (Kaiser, Mostafa, & Neftci, 2020; Ororbia &
Mali, 2019; Ororbia II, Haffner, Reitter, & Giles, 2017).
Chapter 6. Credit Assignment in the Brain 280
Another approach is to use only local information at the synapses, but use a backwards
phase which instead of being purely sequential is treated as a dynamical systems which
undergoes multiple iterations. Using this approach, while it takes longer than a sequen-
tial backwards pass, also allows using only local information, since the information
about the loss can be ‘leaked’ slowly backwards using the dynamics over time instead
of having to be explicitly transmitted backwards. This allows the brain to operate using
the same dynamical rules at all times, in general, instead of sequential forwards and
backwards transmission of information. One key example of such a framework is the
algorithm of Equilibrium Propagation (Bengio, Mesnard, Fischer, Zhang, & Wu, 2017;
Scellier & Bengio, 2017; Scellier, Goyal, Binas, Mesnard, & Bengio, 2018a, 2018b),
which uses two dynamical phases – a free phase where the dynamics of the system
are allowed to evolve without any inﬂuence of the targets, and a clamped phase in
which the output units are held at a value nudged towards the targets, which destabilizes
the free-phase equilibrium and instead sends the network towards a different clamped
equilibrium state. It turns out that the difference between these two states corresponds
closely to the gradients which would otherwise have been backpropagated through the
network and can thus be used to adjust the synaptic weights (Scellier & Bengio, 2017).
Equilibrium propagation has been extensively tested on small datasets like MNIST and
CIFAR, although it is not yet known how well the method scales. Additional problems
with EP are its necessary use of two distinct backwards phases and, crucially, the storage
of information (the equilibrium in the free-phase, throughout the entirety of the clamped
phase before their subtraction to obtain the gradients. Another iterative algorithm which
has been shown to approximate backprop is predictive coding (Whittington & Bogacz,
2017).
In this thesis chapter, we make two contributions to the theory of iterative algorithms for
approximating backprop. Firstly, we extend work by Whittington and Bogacz (2017),
showing that predictive coding can approximate backprop by making this claim precise
and exact, and extending it to arbitrary computation graphs (Millidge, Tschantz, &
Chapter 6. Credit Assignment in the Brain 281
Buckley, 2020a). Speciﬁcally, we show that predictive coding provides a fully general
iterative approach to approximating reverse-mode automatic differentiation through
an identiﬁcation of the equilibrium prediction error with the adjoint term. This allows
us to deﬁne predictive coding networks which can train any contemporary machine
learning architecture with accuracy equivalent to backprop in a local and biologically
plausible (ish) manner. We demonstrate this capability on CNNs and LSTMs, thus
substantially extending the range and scale of architectures to which predictive coding
has been applied.
Secondly, we propose a novel iterative algorithm – Activation Relaxation (Millidge,
Tschantz, Buckley, & Seth, 2020)– which converges precisely to the exact backprop
gradients, while also considerably simplifying the predictive coding update rules and
obviating the need for separate populations of error and ‘value’ neurons which predictive
coding possesses. Additionally, we demonstrate that certain remaining implausibilities
in the algorithm such as the weight transport problem and the nonlinear derivatives
problem can be ‘relaxed’ while retaining learning performance almost equivalent to
backprop, even on challenging and large-scale computer vision tasks.
A ﬁnal issue which undermines many of the proposed learning rules for both sequential
methods like target-prop and iterative ones like EP or predictive coding, is the necessity
of three-factor learning rules with precise vector feedback, like backprop. It is still fairly
unclear whether such rules can actually be implemented in the brain, although there
has been some work showing that prediction error, or gradient like quantities could
in theory be transmitted backwards through the network using segregated dendrites
(Sacramento et al., 2018) which may help maintain separate error representations
independently of the ﬁring rates of the rest of the somatic neuron. While the actual
biological plausibility of this approach is unclear, in the last section of this chapter,
we will speculate that if it is plausible, and the brain can maintain and update separate
error and value representations on single neurons, or indeed on separate populations
Chapter 6. Credit Assignment in the Brain 282
but with precise three-factor learning rules, then that is all that is necessary for a direct
biologically plausible implementation of backprop, especially given recent research
showing that the weight transport problem can be largely overcome through learning the
backwards weights. We present a theoretical algorithm, extremely similar to target-prop,
and with three-factor learning rules, which precisely corresponds to backprop in the
brain, which is mathematically equivalent to backprop. Thus, if three-factor learning
rules are possible in the brain, either through segregated dendrites, or else precise
interneuron connectivty, then exact backpropagation can be performed as simply as
target-prop or any other algorithm.
6.2 Predictive Coding Approximates Backprop Along
Arbitrary Computation Graphs
Here we demonstrate that predictive coding can approximate backpropagation on
arbitrary computation graphs. As we recall from chapter 5, predictive coding arises
from a variational inference algorithm on the activations on each layer of the hierarchy,
whereby the predictive coding update rules can be derived as a gradient descent on
the variational free energy F . To showcase how this methodology extends to arbitrary
graphs, we must deﬁne the variational inference problem to be solved on an arbitrary
computation graph. First, we must make the notion of a computation graph explicit.
A computation graph G = {E,V}is a directed acyclic graph (DAG) which can repre-
sent the computational ﬂow of essentially any program or computable function as a
composition of elementary functions. Each edge ei ∈E of the graph corresponds to
an intermediate step – the application of an elementary function – while each vertex
vi ∈V is an intermediate variable computed by applying the functions of the edges to
the values of their originating vertices. vi denotes the vector of activations within a layer
and we denote the set of all vertices as {vi}. Effectively, computation ﬂows ‘forward’
Chapter 6. Credit Assignment in the Brain 283
zv0 v1 v2 v3 T
∂L
∂v2
= ∂L
∂v3
∂v3
∂v2
∂L
∂v1
= ∂L
∂v2
∂v2
∂v1
∂L
∂v0
= ∂L
∂v1
∂v1
∂v0
v0 v1 v2
ϵ0 ϵ1 ϵ2 ϵ3
̂v1
̂v3
·v0 = − ϵ0 + ϵ1
∂ ̂v1
v0
·v1 = − ϵ1 + ϵ2
∂ ̂v2
v1
·v2 = − ϵ2 + ϵ3
∂ ̂v3
v2
T
̂v2
Figure 6.1: Top: Backpropagation on a chain. Backprop proceeds backwards sequen-
tially and explicitly computes the gradient at each step on the chain. Bottom: Predictive
coding on a chain. Predictions, and prediction errors are updated in parallel using only
local information. Importantly, while the original computation graph (black lines) must
be a DAG, the augmented predictive coding graph is cyclic, due to the backwards (red)
prediction error connections.
from parent nodes to all their children through the edge functions until the leaf nodes
give the ﬁnal output of the program as a whole (see Figure 6.1 and 6.2 (top) for an
example). Given a target T and a loss function L = g(T,vout), the graph’s output can be
evaluated and, and if every edge function is differentiable, automatic differentiation can
be performed on the computation graph.
We can extend predictive coding to arbitrary computation graphs in a supervised setting
by deﬁning the inference problem to be solved as that of inferring the vertex value vi
of each node in the graph given ﬁxed start nodes v0 (the data), and end nodes vN (the
targets). We deﬁne a generative model which parametrises the value of each vertex given
the feedforward prediction of its parents,p({vi}) =p(v0 . . .vN) =∏N
i p(vi|P(vi)) 4, and
4This includes the prior p(v0), which simply has no parents.
Chapter 6. Credit Assignment in the Brain 284
a factorised, variational posteriorQ({vi}|v0,vN) =Q(v1 . . .vN−1|v0,vN) =∏N
i Q(vi|P(vi),C(vi)),
where P(vi) denotes the set of parents and C(vi) denotes the set of children of a given
node vi. From this, we can deﬁne a suitable objective functional, the variational free
energy F (VFE), which acts as an upper bound on the divergence between the true and
variational posteriors.
F = KL[(Q(v1 . . .vN−1|v0,vN)∥p(v0 . . .vN)] ≥KL[(Q(v1 . . .vN−1)|v0,vN)∥p(v1 . . .vN−1|v0,vN)]
≈
N
∑
i=0
εT
i εi
(6.7)
Under Gaussian assumptions for the generative model p({vi}) =∏N
i N (vi; ˆvi,Σi), and
the variational posterior Q({vi}) =∏N
i N (vi), where the ‘predictions’ˆvi = f (P(vi);θi)
are deﬁned as the feedforward value of the vertex produced by running the graph
forward, and all the precisions, or inverse variances, Σ−1
i are ﬁxed at the identity, we
can write F as simply a sum of prediction errors (see chapter 5), with the prediction
errors deﬁned as εi = vi −ˆvi. Since F is an upper bound on the divergence between
true and approximate posteriors, by minimizing F , we reduce this divergence, thus
improving the quality of the variational posterior and approximating exact Bayesian
inference. Predictive coding minimizes F by employing the Cauchy method of steepest
descent to set the dynamics of the vertex variablesvi as a gradient descent directly on F
dvi
dt = ∂F
∂vi
= εi − ∑
j∈C(vi)
εj
∂ˆvj
∂vi
(6.8)
The dynamics of the parameters of the edge functionsW such that ˆvi = f (P(vi);W), can
also be derived as a gradient descent on F . In a neural network model, the parameters
correspond to the synaptic weights of each layer of the neural network. Importantly
these dynamics require only information (the current vertex value, prediction error, and
prediction errors of child vertices) locally available at the vertex.
dWi
dt = ∂F
∂Wi
= εi
∂ˆvi
∂Wi
(6.9)
Chapter 6. Credit Assignment in the Brain 285
To run generalized predictive coding on a given computation graph G = {E,V}, we
augment the graph with error units ε ∈E to obtain an augumented computation graph
˜G = {E,V,E}. The predictive coding algorithm then operates in two phases – a
feedforward sweep and a backwards iteration phase. In the feedforward sweep, the
augmented computation graph is run forward to obtain the set of predictions {ˆvi}, and
prediction errors {εi}= {vi −ˆvi}for every vertex. To achieve exact equivalence with
the backprop gradients computed on the original computation graph, we initializevi = ˆvi
in the initial feedforward sweep so that the output error computed by the predictive
coding network and the original graph are identical – an assumption we call the ﬁxed
prediction assumption.
In the backwards iteration phase, the vertex activities{vi}and prediction errors {εi}are
updated with Equation 6.8 for all vertices in parallel until the vertex values converge to a
minimum of F . After convergence the parameters are updated according to Equation 6.9.
Note we also assume, following Whittington and Bogacz (2017), that the predictions at
each layer are ﬁxed at the values assigned during the feedforward pass throughout the
optimisation of the vs. This is the ﬁxed-prediction assumption. In effect, by removing
the coupling between the vertex activities of the parents and the prediction at the child,
this assumption separates the global optimisation problem into a local one for each
vertex. We implement these dynamics with a simple forward Euler integration scheme
so that the update rule for the vertices became vt+1
i ←vt
i −ηdF
dvt
i
where η is the step-size
parameter. Importantly, if the edge function linearly combines the activities and the
parameters followed by an elementwise nonlinearity, then both the update rule for the
vertices (Equation 6.8) and the parameters (Equation 6.9) become Hebbian. Speciﬁcally,
the update rules for the vertices and weights become dvi
dt = εi −∑j εj f ′(θj ˆvj)θT
j and
Chapter 6. Credit Assignment in the Brain 286
dθi
dt = εi f ′(θi ˆvi) ˆviT , respectively.
Algorithm 4:Generalized Predictive Coding
Dataset: D = {X,L}, Augmented Computation Graph ˜G = {E,V,E}, inference
learning rate ηv, weight learning rate ηθ
for (x,L) ∈D do
ˆv0 ←x
for ˆvi ∈V do
ˆvi ←f ({P(ˆvi);θ)
end
εL ←L −ˆvL
while not converged do
for (vi,εi) ∈˜G do
εi ←vi −ˆvi
vt+1
i ←vt
i +ηv dF
dvt
i
end
end
for θi ∈E do
θt+1
i ←θt
i +ηθ dF
dθt
i
end
end
6.2.1 Methods and Results
To demonstrate that this predictive coding scheme approximates the backpropagation
of error algorithm at convergence is fairly straightforward. The key step is to show
that the recursion relationship of the adjoint term for the equilibrium of the prediction
errors in predictive coding is identical to that in reverse-mode AD, even for arbitrary
computation graphs. To make this clear more intuitively, we ﬁrst demonstrate that,
at the equilibrium of the dynamics, the prediction errors ε∗
i converge to the correct
backpropagated gradients ∂L
∂vi
, and consequently the parameter updates (Equation 6.9)
Chapter 6. Credit Assignment in the Brain 287
become precisely those of a backprop trained network.
First, under the ﬁxed prediction assumption, we can directly solve for the equilibrium
of the dynamics by setting the time derivative to 0,
ε∗
i = ∑
j∈C(vi)
ε∗
j
∂ˆvi
∂vj
(6.10)
If we compare this to the recursive relationship inherent to reverse-mode AD (Equation
6.3), we can see that the prediction errors satisfy the same recursive relationship. Since
this relationship is recursive, all that is needed for the prediction errors throughout the
graph to converge to the backpropagated derivatives is for the prediction errors at the
ﬁnal layer to be equal to the output gradient: ε∗
L = ∂L
∂ˆvL
. To see this explicitly, consider
a mean-squared-error loss function 5. at the output layer L = 1
2 (T −ˆvL)2 with T as a
vector of targets, and deﬁning εL = T −ˆvL. We then consider the equilibrium value of
the prediction error unit at a penultimate vertex εL−1. By Equation 6.10, we can see
that at equilibrium,
ε∗
L−1 = ε∗
L
∂ˆvL
∂vL−1
= (T −ˆv∗
L) ∂ˆvL
∂vL−1
(6.11)
since, (T −ˆvL) =∂L
∂ˆvL
, we can then write,
ε∗
L−1 = ∂L
∂ˆvL
∂ˆvL
∂vL−1
= ∂L
∂vL−1
(6.12)
Thus the prediction errors of the penultimate nodes converge to the correct backprop-
agated gradient. Furthermore, recursing through the graph from children to parents
allows the correct gradients to be computed6. Thus, by induction, we have shown that
the ﬁxed points of the prediction errors of the global optimization correspond exactly
5While the mean-squared-error loss function ﬁts most nicely with the Gaussian generative model,
other loss functions can be used in practice. If the loss function can be represented as a log probability
distribution, then the generative model can be amended to simply set the output distribution to that
distribution. If not, then there is no fully consistent generative model (although all nodes except the
output remain Gaussian), but the algorithm will still work in practice. See Figure 6.4 for results for CNNs
trained with a crossentropy loss.
6Some subtlety is needed here since vL−1 may have many children which each contribute to the loss.
However, these different paths sum together at the node vL−1, thus propagating the correct gradient
backwards.
Chapter 6. Credit Assignment in the Brain 288
to the backpropagated gradients. Intuitively, if we imagine the computation-graph as
a chain and the error as ‘tension’ in the chain, backprop loads all the tension at the
end (the output) and then systematically propagates it backwards. Predictive coding,
however, spreads the tension throughout the entire chain until it reaches an equilibrium
where the amount of tension at each link is precisely the backpropagated gradient.
By a similar argument, it is apparent that the dynamics of the parametersθi as a gradient
descent on F also exactly match the backpropagated parameter gradients.
dWi
dt = ∂F
∂Wi
= ε∗
i
∂ε∗
i
∂Wi
= ∂L
∂ˆvi
∂ˆvi
∂Wi
= ∂L
∂Wi
(6.13)
Which follows from the fact that ε∗
i = dL
d ˆvi and that dε∗
i
dθ = d ˆvi
dθi .
To demonstrate empirically that this approach works, we present a numerical test in
the simple scalar case, where we use predictive coding to derive the gradients of an
arbitrary, highly nonlinear test functionvL = tan(√θv0)+ sin(v2
0) where θ is an arbitrary
parameter. For our tests, we set v0 to 5 and θ to 2. The computation graph for this
function is presented in Figure 6.2. Although simple, this is a good test of predictive
coding because the function is highly nonlinear, and its computation graph does not
follow a simple layer structure but includes some branching. An arbitrary target of
T = 3 was set at the output and the gradient of the lossL = (vL −T )2 with respect to the
input v0 was computed by predictive coding. We show (Figure 6.2) that the predictive
coding optimisation rapidly converges to the exact numerical gradients computed by
automatic differentiation, and that moreover this optimization is very robust and can
handle even exceptionally high learning rates (up to 0.5) without divergence.
Secondly, we wish to show empirically that this approach can be used to train deep
neural network architectures, of the kind used in machine learning, up to high levels
of performance equivalent to those trained with backprop. First, we demonstrate a
predictive coding CNN model. Convolutional neural networks have been a cornerstone
of machine learning since the pioneering demonstrations of their power on ImageNet by
Chapter 6. Credit Assignment in the Brain 289
θ
* ( ⋅ , ⋅ ) ⋅ tan( ⋅ )
+( ⋅ , ⋅ )
( ⋅ )2 sin( ⋅ )
μ1 = θ * v0 μ2 = ( ̂v1) μ3 = tan( ̂v2)
μ4 = v2
0 μ5 = sin( ̂v4) μL = ̂v3 + ̂v5
̂v1 ̂v2 ̂v3
̂v4
̂v5
ϵ1 ϵ2 ϵ3
ϵ5ϵ4
T
ϵL
ϵL
dμL
d ̂v3
ϵL
dμL
d ̂v5
ϵ5
dμ5
d ̂v4
ϵ3
dμ3
d ̂v2
ϵ2
dμ2
d ̂v1ϵ1
dμ1
dθ
ϵ1
dμ1
dv0
ϵ4
dμ4
dv0
v0
Figure 6.2: Top: The computation graph of the nonlinear test function vL = tan(√θv0)+
sin(v2
0). Bottom: graphs of the log mean divergence from the true gradient and the
divergence for different learning rates. Convergence to the exact gradients is exponential
and robust to high learning rates.
Chapter 6. Credit Assignment in the Brain 290
(Krizhevsky et al., 2012), and are still widely used as state of the art models for image
processing.
The key concept in a CNN is that of an image convolution, where a small weight matrix
is slid (or convolved) across an image to produce an output image. Each patch of the
output image only depends on a relatively small patch of the input image. Moreover,
the weights of the ﬁlter stay the same during the convolution, so each pixel of the
output image is generated using the same weights. The weight sharing implicit in the
convolution operation enforces translational invariance, since different image patches
are all processed with the same weights.
The forward equations of a convolutional layer for a speciﬁc output pixel
vi, j =
k=i+ f
∑
k=i−f
l= j+ f
∑
l= j−f
θk,lxi+k, j+l (6.14)
Where vi, j is the (i, j)th element of the output, xi, j is the element of the input image
and θk,l is an weight element of a feature map. To set-up a predictive coding CNN, we
augment each intermediate xi and vi with error units εi of the same dimension as the
output of the convolutional layer.
Predictions ˆv are projected forward using the forward equations. Prediction errors also
need to be transmitted backwards for the architecture to work. To achieve this we must
have that prediction errors are transmitted upwards by a ‘backwards convolution’. We
thus deﬁne the backwards prediction errors ˆεj as follows:
ˆεi, j =
i+ f
∑
k=i−f
j+ f
∑
l= j−f
θj,i ˜εi, j (6.15)
Where ˜ε is an error map zero-padded to ensure the correct convolutional output size.
Inference in the predictive coding network then proceeds by updating the intermediate
values of each layer as follows:
dvl
dt = εl −ˆεl+1 (6.16)
Chapter 6. Credit Assignment in the Brain 291
The CNN weights can be updated using the simple Hebbian rule of the multiplication
of the pre and post synaptic potentials.
dθl
dt = ∑
i, j
εli, j vl−1T
i, j (6.17)
In our experiments we used a relatively simple CNN architecture consisting of one
convolutional layer of kernel size 5, and a ﬁlter bank of 6 ﬁlters. This was followed by
a max-pooling layer with a (2,2) kernel and a further convolutional layer with a (5,5)
kernel and ﬁlter bank of 16 ﬁlters. This was then followed by three fully connected
layers of 200, 150, and 10 (or 100 for CIFAR100) output units. Each convolutional and
fully connected layer used the relu activation function, except the output layer which
was linear. Although this architecture is far smaller than state of the art for convolutional
networks, our primary purpose here is to demonstrate the equivalence of predictive
coding and backprop. Further work could investigate scaling up predictive coding to
more state-of-the-art architectures.
Our datasets consisted of 32x32 RGB images. We normalised the values of all pixels of
each image to lie between 0 and 1, but otherwise performed no other image preprocess-
ing. We did not use data augmentation of any kind. We set the weight learning rate for
the predictive coding and backprop networks 0.0001. A minibatch size of 64 was used.
These parameters were chosen without any detailed hyperparameter search and so are
likely suboptimal. The magnitude of the gradient updates was clamped to lie between
-50 and 50 in all of our models. This was done to prevent divergences, as occasionally
occurred in the LSTM networks, likely due to exploding gradients.
First, we compare the test and training accuracy, as well as training loss plots for
convolutional CNNs trained on CIFAR10. Figure 6.4 shows convincingly that the
accuracy and indeed the training dynamics of predictive coding and backprop are the
same, to all intents and purposes, thus demonstrating that predictive coding approaches
can approximate backprop to a very high accuracy, using only local learning rules.
To investigate this further, we explicitly plotted the divergence between the gradient
Chapter 6. Credit Assignment in the Brain 292
estimates produced by predictive coding and the analytically correct gradients produced
by backprop.
Importantly, we found that the divergence between the true and predictive coding gradi-
ents was extremely small, and remained approximately constant throughout training
suggesting that predictive coding networks do not suffer from accumulating errors in
their gradient approximation process. Importantly, to achieve this level of convergence
required 100 backwards iterations using an inference learning rate of 0.1. This means
that the predictive coding has an approximately 100x computational overhead com-
pared to backprop – largely rendering it uncompetitive for direct competition in serial
computers. Nevertheless, this choice of 100 iterations is on the high end of what is
necessary, since we are primarily concerned with showing the asymptotic equivalence,
and in reality the number of iterations required may be substantially lower. Addition-
ally, predictive coding is a fully parallel algorithm unlike backprop, which must be
implemented sequentially and is a better ﬁt for the highly parallel neural circuitry.
It is also important to note that while predictive coding ‘naturally’ uses the mean-
squared error loss – so that the output error is a standard prediction error, other loss
functions as possible, such as the widely used cross-entropy loss. Predictive coding
can be straightforwardly extended to cover other loss functions by simply replacing
the ﬁnal prediction error εL with the gradient of the loss function with respect to the
outputs. Here we demonstrate that predictive coding with a multi-class cross entropy
loss also performs equivalently to the network trained with backprop on the CIFAR and
SVHN datasets.
6.2.2 RNN and LSTM
6.2.2.1 RNN
We additionally tested a predictive coding RNN and LSTM. To train these recurrent
networks with predictive coding, we simply used the approach of using predictive coding
Chapter 6. Credit Assignment in the Brain 293
(a) Conv Layer 1
 (b) Conv Layer 2
(c) FC Layer 1
 (d) FC Layer 2
Figure 6.3: Mean divergence between the true numerical and predictive coding backprops
over the course of training. In general, the divergence appeared to follow a largely random walk
pattern, and was generally neglible. Importantly, the divergence did not grow over time throughout
training, implying that errors from slightly incorrect gradients did not appear to compound.
Chapter 6. Credit Assignment in the Brain 294
SVHN training accuracy
 (a) SVHN test accuracy
(b) CIFAR training accuracy
 (c) CIFAR test accuracy
Figure 6.4: Training and test accuracies of the CNN network on the SVHN and CIFAR datasets
using the cross-entropy loss. As can be seen performance remains very close to backprop, thus
demonstrating that our predictive coding algorithm can be used with different loss functions, not
just mean-squared-error.
Chapter 6. Credit Assignment in the Brain 295
to approximate backpropagation through time (BPTT) and applied predictive coding to
the unrolled computation graph. With long sequence lengths, this lead to extremely deep
graphs for predictive coding to train. Crucially, we demonstrate that predictive coding’s
ability to train such graphs is not impaired by their depth, meaning that predictive
coding as a training algorithm has an exceptional scalability for extremely deep models
of the kind increasingly used in contemporary machine learning (He, Zhang, Ren, &
Sun, 2016; Radford et al., 2019)
The computation graph on RNNs is relatively straightforward. We consider only a
single layer RNN here although the architecture can be straightforwardly extended to
hierarchically stacked RNNs. An RNN is similar to a feedforward network except that
it possesses an additional hidden state h which is maintained and updated over time as a
function of both the current input x and the previous hidden state. The output of the
network y is a function of h. By considering the RNN at a single timestep we obtain the
following equations.
ht = f (θhht−1 +θxxt) (6.18)
yt = g(θyht) (6.19)
Where f and g are elementwise nonlinear activation functions. And θh,θx,θy are weight
matrices for each speciﬁc input. To predict a sequence the RNN simply rolls forward
the above equations to generate new predictions and hidden states at each timestep.
It is important to note that this is an additional aspect of biological implausibility that
we do not address in here. BPTT requires updates to proceed backwards through time
from the end of the sequence to the beginning. Ignoring any biological implausibility
with the rules themselves, this updating sequence is clearly not biologically plausible as
naively it requires maintaining the entire sequence of predictions and prediction errors
perfectly in memory until the end of the sequence, and waiting until the sequence ends
before making any updates. There is a small literature on trying to produce biologically
plausible, or forward-looking approximations to BPTT which does not require updates
Chapter 6. Credit Assignment in the Brain 296
to be propagated back through time (Lillicrap & Santoro, 2019; Ollivier, Tallec, &
Charpiat, 2015; Steil, 2004; Tallec & Ollivier, 2017; R. J. Williams & Zipser, 1989b).
While this is a fascinating area, we do not address it here. We are solely concerned
with the fact that predictive coding approximates backpropagation on feedforward
computation graphs for which the unrolled RNN graph is a sufﬁcient substrate.
To learn a predictive coding RNN, we ﬁrst augment each of the variablesht and yt of
the original graph with additional error units εht and εyt . Predictions ˆyt, ˆht are generated
according to the feedforward rules (16). A sequence of true labels {T1...TT }is then
presented to the network, and then inference proceeds by recursively applying the
following rules backwards through time until convergence.
εyt = L −ˆyt
εht = ht −ˆht
dht
dt = εht −εyt θT
y −εht+1 θT
h (6.20)
Upon convergence the weights are updated according to the following rules.
dθy
dt =
T
∑
t=0
εyt
∂g(θyht)
∂θy
hT
t
dθx
dt =
T
∑
t=0
εht
∂f (θhht−1 +θxxt)
∂θx
xT
t
dθh
dt =
T
∑
t=0
εht
∂f (θhht−1 +θxxt)
∂θh
hT
t+1 (6.21)
Since the RNN feedforward updates are parameter-linear, these rules are Hebbian, only
requiring the multiplication of pre and post-synaptic potentials. This means that the
predictive coding updates proposed here are biologically plausible and could in theory
be implemented in the brain. The only biological implausibility remains the BPTT
learning scheme.
Our RNN was trained on a simple character-level name-origin dataset which can be
found here: https://download.pytorch.org/tutorial/data.zip. The RNN was presented
Chapter 6. Credit Assignment in the Brain 297
with sequences of characters representing names and had to predict the national origin
of the name – French, Spanish, Russian, etc. The characters were presented to the
network as one-hot-encoded vectors without any embedding. The output categories
were also presented as a one-hot vector. The RNN has a hidden size of 256 units. A
tanh nonlinearity was used between hidden states and the output layer was linear. The
network was trained on randomly selected name-category pairs from the dataset.
We ﬁrst present the training and test accuracy for the backprop RNNs, averaged over
ﬁve seeds. In general, performance between the backprop-trained and predictive coding
networks was indistinguishable on this task.
Figure 6.5: Test accuracy plots for the Predictive Coding and Backprop RNN and LSTM
on their respective tasks, averaged over 5 seeds. Performance is again indistinguishable
from backprop.
Additionally, The training loss for the predictive coding and backprop RNNs, averaged
over 5 seeds is presented below (Figure 6.6).
6.2.2.2 LSTM
Unlike the other two models, the LSTM possesses a complex and branching internal
computation graph, and is thus a good opportunity to make explicit the predictive
coding ‘recipe’ for approximating backprop on arbitrary computation graphs. The
computation graph for a single LSTM cell is shown (with backprop updates) in Figure
6.8. Prediction for the LSTM occurs by simply rolling forward a copy of the LSTM cell
Chapter 6. Credit Assignment in the Brain 298
Figure 6.6: Training losses for the predictive coding and backprop RNN. As expected,
they are effectively identical.
for each timestep. The LSTM cell receives its hidden state ht and cell state ct from the
previous timestep. During training we compute derivatives on the unrolled computation
graph and receive backwards derivatives (or prediction errors) from the LSTM cell at
time t +1. For a full and detailed set of equations specifying the complex LSTM cell,
see Appendix B.
The recipe to convert this computation graph into a predictive coding algorithm is
straightforward. We ﬁrst rewire the connectivity so that the predictions are set to the
forward functions of their parents. We then compute the errors between the vertices and
the predictions.
During inference, the inputs ht,xt and the output yt are ﬁxed. The vertices and then the
prediction errors are updated. This recipe is straightforward and can easily be extended
to other more complex machine learning architectures. The full augmented computation
graph, including the vertex update rules, is presented in Figure 6.7.
For the LSTM we also observed a close correspondence between the performance (in
terms of training and test accuracy) between the predictive coding and backpropagation
networks, thus demonstrating that predictive coding can converge to the exact backprop
Chapter 6. Credit Assignment in the Brain 299
⊕
θf
xt
ht
σ
×ct
θinp θc
σ tanh
×
+
σ
θo
×
tanh
ct+1
ht+1
σ
θy
yt − T
v1 = ht ⊕ xt
v2 = σ(θf v1)
v3 = ctv2
v4 = σ(θinpv1) v5 = tanh(θcv1)
v6 = v5v4
v7 = v3 + v6
v8 = σ(θ0v1)
v9 = tanh(v7)
v10 = v8v9
y = σ(θyv10)
dL
dy
dL
dy
dy
dv10
dL
dht+1
dht+1
dv10
dL
dv10
dv10
dv9
dL
dv9
dv9
dv7
dL
dct+1
dct+1
dv7
dL
dv10
dv10
dv8
dL
dv7
dv7
dv3
dL
dv7
dv7
dv6
dL
dv6
dv6
dv4
dL
dv6
dv6
dv5
dL
dv5
dv5
dv1
dL
dv8
dv8
dv1
dL
dv4
dv4
dv1
dL
dv2
dv2
dv1
dL
dv3
dv3
dv2
dL
dv3
dv3
dct
dL
dv1
dv1
dxt
dL
dv1
dv1
dht
Figure 6.7: Computation graph and backprop learning rules for a single LSTM cell.
Inputs to the LSTM cell are the current input xt and the previous embedding ht. These
are then passed through three gates – an input, forget, and output gate, before the
output of the whole LSTM cell can be computed
gradients even on exceptionally deep and complex computation graphs such as the
LSTM
Importantly, we observed rapid convergence to the exact backprop gradients even in
the case of very deep computation graphs (as is an unrolled LSTM with a sequence
length of 100). Although convergence was slower than was the case for CNNs or
lesser sequence lengths, it was still straightforward to achieve convergence to the exact
numerical gradients with sufﬁcient iterations.
Below we plot the mean divergence between the predictive coding and true numerical
gradients as a function of sequence length (and hence depth of graph) for a ﬁxed
computational budget of 200 iterations with an inference learning rate of 0.05. As can
be seen, the divergence increases roughly linearly with sequence length. Importantly,
even with long sequences, the divergence is not especially large, and can be decreased
further by increasing the computational budget. As the increase is linear, we believe
Chapter 6. Credit Assignment in the Brain 300
⊕
θf
xt
ht
σ
×ct
θinp θc
σ tanh
×
+
σ
θo
×
tanh ct+1
ht+1
σ
θy
v1 = ht ⊕ xt
v2 = σ(θf v1)
μ3 = ctv2
μ4 = σ(θinpv1)
μ6 = v5v4
μ7 = v3 + v6
μ8 = σ(θ0v1)
μ9 = tanh(v7)
μ10 = v8v9
μy = σ(θyv10)
ϵy T
v10
ϵ10
v9
ϵ9
v7v3
v6
v5
v8
v4v4
ϵ7
ϵ6
ϵ3
ϵ4
ϵ8
ϵ5ϵ2
ϵht+1
ϵct+1
ϵy
dμy
dv10
ϵ10
dμ10
dv9
ϵ9
dμ9
dv7
ϵ7
dμ7
dv3
ϵ7
dμ7
dv6
ϵ6
dμ6
dv5
ϵ10
dμ10
dv8
ϵ8
dμ8
dv1
ϵ5
dμ5
dv1
ϵ4
dμ4
dv1
ϵ2
dμ2
dv1
ϵ3
dμ3
dv2
ϵ3
ϵ6
dμ6
dv4
Figure 6.8: The LSTM cell computation graph augmented with error units, evincing the
connectivity scheme of the predictive coding algorithm. The key move is to associate
each intermediate node in the computation graph with its own prediction error unit
that predictive coding approaches should be scalable even for backpropagating through
very deep and complex graphs.
We also plot the number of iterations required to reach a given convergence threshold
(here taken to be 0.005) as a function of sequence length (Figure 6.11). We see that the
number of iterations required increases sublinearly with the sequence length, and likely
asymptotes at about 300 iterations. Although this is a lot of iterations, the sublinear
convergence nevertheless shows that the method can scale to even extremely deep
graphs.
Our architecture consisted of a single LSTM layer (more complex architectures would
consist of multiple stacked LSTM layers). The LSTM was trained on a next-character
character-level prediction task. The dataset was the full works of Shakespeare, down-
loadable from Tensorﬂow. The text was shufﬂed and split into sequences of 50 charac-
ters, which were fed to the LSTM one character at a time. The LSTM was trained then
to predict the next character, so as to ultimately be able to generate text. The characters
Chapter 6. Credit Assignment in the Brain 301
Figure 6.9: Training losses for the predictive coding and backprop LSTMs averaged over
5 seeds. The performance of the two training methods is effectively equivalent.
were presented as one-hot-encoded vectors. The LSTM had a hidden size and a cell-size
of 1056 units. A minibatch size of 64 was used and a weight learning rate of 0.0001
was used for both predictive coding and backprop networks. To achieve sufﬁcient
numerical convergence to the correct gradient, we used 200 variational iterations with
an inference learning rate of 0.1. This rendered the predictive LSTM approximately
200x as costly as the backprop LSTM to run. A graph of the LSTM training loss for
both predictive coding and backprop LSTMs, averaged over 5 random seeds, can be
found below (Figure 6.12).
6.3 Interim Discussion
Here we have shown that predictive coding can be applied directly to arbitrary compu-
tation graphs and can rapidly and effectively converge to the exact gradients required
for the backpropagation of error algorithm. We have demonstrated this on deep and
state of the art machine learning architectures, thus achieving signiﬁcantly greater
scale than previous works using predictive coding (Millidge, 2019c; Orchard & Sun,
2019; Whittington & Bogacz, 2017). Moreover, the predictive coding learning rule
Chapter 6. Credit Assignment in the Brain 302
Figure 6.10: Divergence between predictive coding and the correct backprop gradients
as a function of sequence length. Crucially, this divergence only increases linearly in
the sequence length, allowing for very accurate gradient computation even with long
sequences.
uses only local learning dynamics and Hebbian weight updates in the case of the usual
feedforward neural networks (although they differ somewhat for the LSTM). Weights
are updated using only local prediction errors.
This approach also is innovative in that it phrases backprop in terms of variational
inference on the values of the nodes in the computation graph. While it may seem just
like a mathematical convenience, it actually has deep implications. It draws another link
between the processes of optimization and variational inference, in a rather different
manner from that which has largely been explored before. Instead of conceptualising
inference as optimization, as is typically done in variational inference, we instead
conceptualize a core component of optimization – credit assignment – purely in terms of
inference. While this duality has been considered before for two layer networks (Amari,
1995), our approach is substantially more powerful and general, by showcasing that it
Chapter 6. Credit Assignment in the Brain 303
Figure 6.11: Number of iterations to reach convergence threshold as a function of
sequence length. Importantly, the number of iterations required to converge appears to
grow sublinearly with sequence length, again implying that convergence is not computa-
tionally unattainable even with very long sequences.
holds for arbitrary computation graphs. Additionally, our approach provides an avenue
for interesting generalizations of backprop through the use of precision parameters in
predictive coding. Note that in our analysis, we have implicitly assumed that all of the
precision parameters are set to the identity Σ = I, as they do not feature in the learning
and update rules, as they do in Chapter 3. If we reintroduce precision in this context, we
see that it has the role of modulating gradient magnitudes – effectively implementing an
adaptive learning rate. What this means, intuitively, is that we can think about precision
weighting in this case as enabling an uncertainty aware backprop, which speciﬁcally
weights gradients by how uncertain they are – or by their variance. Effectively, this
method, with learnable precisions, would down-weight highly variable and uncertain
gradients while upweighting those known to be certain. When applied to the input
this would mimic features of attention, by downweighting noisy or otherwise uncertain
Chapter 6. Credit Assignment in the Brain 304
Figure 6.12: Training losses for the predictive coding and backprop LSTMs averaged
over 5 seeds. The performance of the two training methods is effectively equivalent.
inputs and having them play little role in learning. While such an adaptive modulatory
role for precision may bring learning beneﬁts, this must be explored further, as the
author hopes to do in future work.
Finally, it is worth discussing several drawbacks of the method. The key one is its
computational cost. The networks presented here were trained with 100 dynamical
iterations to converge to the prediction error equilibrium before each weight update,
giving predictive coding an approximately 100x computational cost compared to back-
prop. This is obviously highly signiﬁcant and renders these approaches unusable for
large scale networks on serial V on-Neumann computers. While the brain utilizes highly
parallel circuitry, and may therefore be more suited to such an iterative algorithm,
there are still issues with requiring a dynamical iteration for convergence. Speciﬁcally,
such iterations still require time and discrete phases so that the system cannot likely
simply operate in continuous time. Moreover, if too many iterations are required, since
the brain must respond to a continually changing world instead of just single images
presented in isolation, it may become overwhelmed by events and fail computationally,
if the input changes faster than it can dynamically converge to a solution. While not
Chapter 6. Credit Assignment in the Brain 305
necessarily as severe as needing dynamical approaches for inference, which would
entirely hamstring any response, here the dynamics are only required for learning and
weight updates, this may nevertheless prove to be a substantial drawback of the method.
Our method, like most others, also requires two distinct phases which must either be
somehow coordinated explicitly, or multiplexed in the brain.
Additionally, the ﬁxed-prediction assumption embedded in the model requires main-
taining the stored memory of the feedforward pass values somewhere in the network
throughout the backwards dynamical phase which is potentially problematic in neural
circuitry. Finally, although the predictive coding learning rules are local, in the sense
that they only require information from the same layer, they still require information
from the prediction error units to be transmitted to the activity units, where we assume
the synaptic weights are located (although in a segregated dendrite model they could
just be in different dendrites (Sacramento et al., 2018)).
6.4 Activation Relaxation
Here we introduce a second iterative algorithm which approximates the exact backprop-
agation gradients asymptotically at the equilibrium of a dyanmical system. We call this
algorithm Activation Relaxation (AR) because of the nature of the update rules, which
iteratively update the activations of neurons in the iterative phase rather than prediction
errors. Crucially, the update rules proposed by AR are exceedingly simple and elegant,
and do not require additional populations of ‘error neurons’ as in predictive coding, or
multiple backwards phases as in Equilibirium-prop. AR arises quite straightforwardly
by trying to take a ﬁrst principles approach to the iterative backprop approximation
schemes.
To establish notation, we consider the simple case of a fully-connected deep multi-layer
perceptron (MLP) composed of L layers of rate-coded neurons trained in a supervised
setting. The ﬁring rates of these neurons are represented as a single scalar value xl
i,
Chapter 6. Credit Assignment in the Brain 306
referred to as the neurons activation, and a vector of all activations at given layer is
denoted as xl. The activations of the hierarchically superordinate layer are a function of
the hierarchically subordinate layers activations xl+1 = f (Wlxl), where Wl ∈Θ is the
set of synaptic weights, and the product of activation and weights is transformed through
a nonlinear activation function f . The ﬁnal output xL of the network is compared with
the desired targets T , according to some loss function L(xL,T ). In this work, we
take this loss function to be the mean-squared-error (MSE) L(xL,T ) =1
2 ∑i(xL
i −Ti)2,
although the algorithm applies to any other loss function without loss of generality (see
Appendix B). We denote the gradient of the loss with respect to the output layer as dL
dxL .
In the case of the MSE loss, the gradient of the output layer is just the prediction error
εL = (xL −T ).
Firstly, we know that the key quantity we wish to approximate is the adjoint term ∂L
∂xl
for a given layer l. If we know this adjoint, and have it present somewhere in the
local environment, then the gradient with respect to the weights can be computed using
only locally available information. In predictive coding, we compute this adjoint term
using the recursive relationship of the prediction errors. Here, we take a different
approach. Instead, we ask what is the simplest possible dynamical system which can
converge to the exact adjoint term at the equilibrium. After some thought, we emerge at
a straightforward leaky integrator model.
dxl
dt = −xl + ∂L
∂xl (6.22)
which, at equilibrium, converges to
dxl
dt = 0 =⇒x∗l = ∂L
∂xl (6.23)
This update rule includes the very adjoint term we are trying to compute, however, so
these dynamics are not immediately computable. To make them so, we ﬁrst split up the
adjoint. By the chain rule, we can write Equation 6.22 as,
dxl
dt = −xl + ∂L
∂xl+1
∂xl+1
∂xl
⏐⏐⏐
xl=¯xl
(6.24)
Chapter 6. Credit Assignment in the Brain 307
where ¯xl is the value ofxl computed in the forward pass. Next, we note that if we use the
activations of the neurons at each layer instead of prediction errors to accumulate the
adjoint, we can express this in terms of the equilibrium activation of the superordinate
layer,
dxl
dt = −xl +x∗l+1 ∂xl+1
∂xl
⏐⏐⏐
xl=¯xl
(6.25)
However, to achieve these dynamics exactly in a multilayered network would require
the sequential convergence of layers, as each layer must converge to equilibrium before
the dynamics of the layer below can operate. This sequential convergence would make
the algorithm no better than the sequential backwards sweep of backprop. However, if
we approximate the equilibrium activations of the layer with the current activation, this
allows us to run all layers in parallel, yielding,
dxl
dt = −xl +x∗l+1 ∂xl+1
∂xl
⏐⏐⏐
xl=¯xl
≈−xl +xl+1 ∂xl+1
∂xl
⏐⏐⏐
xl=¯xl
(6.26)
≈−xl +xl+1 f ′(Wl, ¯xl)WlT
(6.27)
where f ′= ∂f ′(Wl, ¯xl)
∂¯xl represents the partial derivative of the postsynaptic activation with
respect to the presynaptic activation. Despite this approximation, we argue that the
system nevertheless converges to the same optimum as Equation 6.23. Speciﬁcally,
because we evaluate ∂xl+1
∂xl at the feedforward pass value ¯xl, this term remains constant
throughout the relaxation phase 7. Keeping this term ﬁxed effectively decouples each
layer from any bottom-up inﬂuence. If the top-down input is also constant, because it
has already converged so that xl+1 ≈xl+1∗, then the dynamics become linear, and the
system is globally stable due to possessing a Jacobian which is everywhere negative-
deﬁnite. The top-layer is provided with the stipulatively correct gradient, so it must
7The need to keep this term ﬁxed throughout the relaxation phase does present a potential issue of
biological plausibility. In theory it could be maintained by short-term synaptic traces, and for some
activation functions such as rectiﬁed linear units it is trivial. Moreover, later we show that this term can
be dropped from the equations without apparent ill-effect
Chapter 6. Credit Assignment in the Brain 308
converge. Recursing backwards through each layer, we see that once the top-level has
converged, so too must the penultimate layer, and so through to all layers.
Crucially, Equation 6.26, which is core to the AR algorithm is extremely simple and
biologically plausible. It only requires that the activations of a given layer are sensitive
to the difference between their own activity and that of the layer above mapped through
the backwards weights, and modulated by the nonlinear derivative of the postsynaptic
potential. This update rule thus functions as a kind of prediction error, but one that
emerges between layers, rather than being represented by speciﬁc prediction error units
at a given layer.
Computationally, the AR algorithm proceeds as follows. First, a standard forward
pass computes the network output, which is compared with the target to calculate the
top-layer error derivative εL and thus update the activation of the penultimate layer. 8.
Then, the network enters into a relaxation phase where Equation 6.26 is iterated globally
for all layers until convergence for each layer. Upon convergence, the activations of
each layer are precisely equal the backpropagated derivatives, and are used to update
the weights (via Equation (6.28).
∂L
∂Wl = ∂L
∂xl+1
∂xl+1
∂Wl
= ∂L
∂xl+1 f ′(Wlxl)xLT
(6.28)
8This top-layer error is simply a prediction error for the MSE loss, but may be more complicated and
less biologically-plausible for arbitrary loss functions
Chapter 6. Credit Assignment in the Brain 309
Algorithm 5:Activation Relaxation
Dataset D = {X,T}, parameters Θ = {W0 . . .WL}, inference learning rate ηx,
weight learning rate ηθ.
for (x0,t ∈D) do
for (xl,Wl) for each layer do
xl+1 = f (Wl,xl)
end
while not converged do
εL = T −xL
dxL = −xL +εL ∂εL
∂xL
for xl,Wl,xl+1 for each layer do
dxl = −xl +xl+1 ∂xl+1
∂xl
xlt+1 ←xlt +ηxdxl
end
end
for Wl ∈{W0 . . .WL}do
Wlt+1 ←Wlt +ηθxl ∂xl
∂Wl
end
end
6.4.1 Method and Results
We ﬁrst demonstrate that our algorithm can train a deep neural network with equal
performance to backprop. For training, we utilised the MNIST and Fashion-MNIST
(Xiao, Rasul, & V ollgraf, 2017b) datasets. The MNIST dataset consists of 60000
training and 10000 test 28x28 images of handwritten digits, while the Fashion-MNIST
dataset consists of 60000 training and 10000 test 28x28 images of clothing items.
The Fashion-MNIST dataset is designed to be identical in shape and size to MNIST
while being harder to solve. We used a 4-layer fully-connected multi-layer perceptron
(MLP) with rectiﬁed-linear activation functions and a linear output layer. The layers
Chapter 6. Credit Assignment in the Brain 310
(a) MNIST train accuracy
 (b) MNIST test accuracy
 (c) MNIST gradient angle
(d) Fashion train accuracy
 (e) Fashion test accuracy
AR/
(f) Fashion gradient angle
Figure 6.13: Train and test accuracy and gradient angle (cosine similarity) for AR
vs backprop for MNIST and Fashion-MNIST datasets. Importantly the training and
test accuracies are virtually identical between the AR-trained and backprop-trained
networks. Additionally, the gradient angle between the AR update and backprop is
always substantially less than the 90 degrees required to allow for learning.
consisted of 300, 300, 100, and 10 neurons respectively. In the dynamical relaxation
phase, we integrate Equation 6.26 with a simple ﬁrst-order Euler integration scheme.
xlt+1 = xlt −ηx dxl
dt where ηx was a learning rate which was set to 0.1. The relaxation
phase lasted for 100 iterations, which we found sufﬁcient to closely approximate the
numerical backprop gradients. After the relaxation phase was complete, the weights
were updated using the standard stochastic gradient descent optimizer, with a learning
rate of 0.0005. The weights were initialized as draws from a Gaussian distribution
with a mean of 0 and a variance of 0.05. Hyperparameter values were chosen based
on initial intuition and were not found using a grid-search. The AR algorithm was
applied to each minibatch of 64 digits sequentially. The network was trained with the
mean-squared-error loss.
Chapter 6. Credit Assignment in the Brain 311
In Figure 6.13 we show that the training and test performance of the network trained
with activation-relaxation is nearly identical to that of the network trained with back-
propagation, thus demonstrating that our algorithm can correctly perform credit as-
signment in deep neural networks with only local learning rules. We also empirically
investigate the angle between the AR-computed gradient updates and the true backprop-
agated updates. The gradient angle A was computed using the cosine similarity metric
A(∇θ) =cos−1 ∇T
θ ∇∗
θ
||∇θ||||∇∗
θ
, where ∇θ was the AR-computed gradients and ∇∗
θ were the
backprop gradients. To handle the fact that we had gradient matrices while the cosine
similarity metric only applies to vectors, following (Lillicrap et al., 2016), we simply
ﬂattened the gradient matrices into vectors before performing the computation. We
see that the updates computed by AR are very close in angle to the backprop updates
(under 10 degrees), although the angle increases slightly over the course of training.
The convergence in training and test accuracies between the AR and backprop shows
that this slight difference in gradient angle is not enough to impede effective credit
assignment and learning in AR. In Appendix C, we take a step towards demonstrating
the scalability of this algorithm, by showing preliminary results that indicate that AR,
including with the biologically plausible simpliﬁcations introduced below, can scale to
deeper CNN architectures and more challenging classiﬁcation tasks.
6.4.2 Loosening Constraints
While the AR algorithm as above precisely approximates adjoint term ∂L
∂xl central
to backprop, using only local learning rules, it still retains a number of biological
implausibilities. The core implausibility is the weight transport problem, which is still
present due to the weight transpose present in Equation 6.26. Following our previous
work on relaxed predictive coding (Chapter 3), we demonstrate how the same remedies
can be directly applied to the AR algorithm without jeopardising learning performance.
To address the weight transport problem, we take inspirations from the approaches of
Chapter 6. Credit Assignment in the Brain 312
feedback alignment (Lillicrap et al., 2016), and (Millidge, Tschantz, Seth, & Buckley,
2020d). We postulate an independent separate set of backwards weights ψl, so that the
update rule for the activations becomes,
dxl
dt = xl −xl+1 f ′(Wlxl)ψl (6.29)
Then, following our work on relaxed preditive coding in Chapter 5, we learn these
backwards weights with the following Hebbian update,
dψl
dt = xl+1T
f ′(Wlxl)xl (6.30)
The backwards weights were initialized as draws from a 0 mean, 0.05 variance Gaussian.
In Figure 6.14 we show that strong performance is obtained with the learnt backwards
weights. We found that using random feedback weights without learning (i.e. feedback
alignment), typically converged to a lower accuracy and had a tendency to diverge,
which may be due to a simple Gaussian weight initialization used here. Nevertheless,
when the backwards weights are learnt, we ﬁnd that the algorithm is stable and can obtain
performance comparable with using the exact weight transposes (Figure 6.14). This is a
very strong and encouraging result. First that this learning rule enables performance
with exact weight transposes is impressive, since it implies that the Hebbian update
rule on the backwards weights works and is highly effective, even early on in training.
Secondly, the generalizability of this remedy for weight transport, from predictive
coding networks, deep neural networks (Akrout et al., 2019; Amit, 2019) with backprop,
and now AR suggests that the backwards weights may be able to be independent and
robustly learned from scratch in the brain, thus largely resolving the weight transport
problem altogether.
We additionally plot the angle between the AR with learnable backwards weights and
the true BP gradients (Figure 6.15). The angle starts out very large (about 70 degrees)
since the backwards weights are randomly initialized but then rapidly decreases to about
30 degrees as the backwards weights are learnt, which seems empirically to be sufﬁcient
to enable strong learning performance.
Chapter 6. Credit Assignment in the Brain 313
(a) MNIST backwards
weights
(b) MNIST nonlinear deriva-
tives
(c) MNIST combined
(d) Fashion backwards
weights
(e) Fashion nonlinear deriva-
tives
(f) Fashion combined
Figure 6.14: Train and test accuracy and gradient angle (cosine similarity) for AR vs
backprop for MNIST and Fashion-MNIST datasets. Importantly, we see that even with
the additional relaxations, the AR trained algorithm performs comparably to backprop
Chapter 6. Credit Assignment in the Brain 314
An additional potential biological implausibility to address is the nonlinear derivative
problem, which consists of the f ′(Wl, ¯xl)WlT term. The biological plausibility of this
term depends heavily upon the activation function used in the network. For instance, in
a relu network, this term is trivial, being 0 if the postsynaptic output is greater than 0,
and 1 if it is. However, other commonly used activation functions like tanh, sigmoid,
and especially softmax are more complex and may be challenging to compute locally
in the brain. Here, we experiment with simply dropping the nonlinear derivative term
from the update rule, which results in the following dynamics,
dxl
dt = xl −xl+1WlT
(6.31)
Although the gradients do no longer match backprop, we show in Figure 6.14 that
learning performance against the standard model is relatively unaffected, showing that
the inﬂuence of the nonlinear derivative is small. We hypothesise that by removing
the nonlinear derivative, we are effectively projecting the backprop update onto the
closest linear subspace, which is still sufﬁciently close in angle to the true gradient that
it can support learning. Alternatively, it could be that in the regime of standard activity
values prevailing throughout the network during training, the nonlinear derivatives
generally are close to 1, and thus have little effect on the update rules in any case. If
this is the case, then given that we made no particular effort to constrain the activities
of the network, it supposes that this property, if it exists, may be highly beneﬁcial for
simplifying the computations in the brain.
By explicitly plotting the angle (Figure 6.15), we see that it always remains under about
30 degrees, sufﬁcient for learning, although the angle appears to rise over the course
of training, potentially due to the gradients becoming smaller and more noisy as the
network gets closer to convergence.
Moreover, we can combine these two changes of the algorithm such that there is both no
nonlinear derivative and also learnable backwards weights. Perhaps surprisingly, when
Chapter 6. Credit Assignment in the Brain 315
(a) Backwards weight angle
 (b) Nonlinear derivatives an-
gle
(c) Combined angles
Figure 6.15: Angle between the AR and backprop updates in the learnable backwards
weights, no nonlinear derivatives, and the combined conditions. At all times this angle
remains under 90 degrees and is steady for all cases apart from the no nonlinear
derivatives cases, where it appears to increase over time. Interestingly, this does not
appear to hinder learning performance noticeably, and may simply reﬂect the angle
getting increasingly worse as the network converges.
we do this we retain equivalent performance to the full AR algorithm (see Figure 6.14),
and therefore a valid approximation to backprop in an extremely simple and biologically
plausible form. The activation update equation for the fully simpliﬁed algorithm is:
dxl
dt = xl −xl+1ψ (6.32)
which requires only locally available information and is mathematically very simple. In
effect, each layer is only updated using its own activations and the activations of the
layer above mapped backwards through the feedback connections, which are themselves
learned through a local and Hebbian learning rule. This rule maintains high training
performance and a close angle between its updates and the true backprop updates
(Figure 6.15), and is, at least in theory, relatively straightforward to implement in neural
or neuromorphic circuitry.
Finally, we note that the AR update rules require the nonlinear derivative f ′(Wlxl)
to be evaluated with the activity xl evaluated at its feedforward pass value xl = ¯xl.
Additionally, in the weight update, the value of the activity xl needs to be evaluated at
Chapter 6. Credit Assignment in the Brain 316
its feedforward pass value
∂L
∂Wl = ∂L
∂xl+1
∂xl+1
∂Wl
⏐⏐⏐
xl+1=¯xl+1
= xl+1∗
¯xT ∂f (Wl ¯xl)
∂¯x
T
(6.33)
We call this the frozen feedforward pass assumption, and it is very closely related to
the ﬁxed-prediction assumption in predictive coding or, similarly the requirement in
equilibrium-propagation to store all the intermediate equilibrium values of the free
phase. Here we investigate to what extent this assumption can also be relaxed.
We evaluate whether the nonlinear derivative term can be unfrozen so that it uses the
current value of the activity in a.) the function derivative f ′in Equation 6.26, b.) in the
weight update equation (Equation 6.28), and c.) we investigate whether the activation
value itself can be replaced in the weight update equation.
In Figure 6.16, we see that the frozen feedforward pass assumption can be relaxed in the
case of the nonlinear derivatives for both the AR update and the weight update equation.
However, relaxing it in the case of the weight update equation destroys performance.
This means that ultimately, a direct implementation of AR in biological circuitry would
require neurons to store the feedforward pass value of their own activations.
All the experiments so far have been done on the relatively simple and straightforward
MNIST dataset with small MLP models. However, it is also important to verify the
scalability of this method. Here we demonstrate that AR can be used to train large
CNNs on challenging image recognition datasets (SVHN, CIFAR10, and CIFAR100)
and, moreover, that the previous loosenings of the biologically implausible constraints
on the algorithm still do not appear to degrade performance unduly. This extension
to CNNs is especially important because other biologically plausible schemes such
as feedback alignment (Lillicrap et al., 2014, 2016), and directed feedback alignment
(Nøkland, 2016), have been shown to struggle with the CNN116architectures (Launay
et al., 2019). We tested the simpliﬁcations (dropping nonlinearity or learning backwards
weights) on just the convolutional layers of the network, just the fully-connected layers
of the network, or both together. We found that ultimately performance was largely
Chapter 6. Credit Assignment in the Brain 317
(a) MNIST nonlinear derivative relax-
ation update
(b) MNIST nonlinear derivative weight
update
(c) MNIST both nonlinear derivative
 (d) MNIST current x weight update
Figure 6.16: Assessing whether the frozen feedforward pass assumption can be relaxed.
We show the resulting performance (test accuracy) against baseline of relaxing this
assumption on the MNIST dataset. All results averaged over 10 seeds. These results
show clearly that the frozen feedforward pass assumption can be relaxed for the nonlinear
derivative and in the weight update nonlinear derivative, but not both nonlinear derivatives
simultaneously, and deﬁnitely not using the xT term in the weight update
maintained even when both convolutional and fully connected layers in the network used
learnable backwards weights or had their nonlinear derivatives dropped from both the
update and weight equations. These results speak to the scalability and generalisability
of these relaxations, and the general robustness of the AR algorithm. We implemented
the learnable backwards weights of the CNN by applying Equation 6.28 to the ﬂattened
form of the CNN ﬁlter kernel weights.
Our CNN consisted of a convolutional layer followed by a max-pooling layer, followed
by an additional convolutional layers, then two fully connected layers. The convolutional
layers had 32 and 64 ﬁlters respectively, while the FC layers had 64,120, and 10 neurons
Chapter 6. Credit Assignment in the Brain 318
respectively. For CIFAR100 there are 100 output classes so the ﬁnal layer had 100
neurons. The labels were one-hot-encoded and fed to the network. All input images were
normalized so that their pixel values lay in the range [0,1] but no other preprocessing
was undertaken. We used hyperbolic tangent activations functions at every layer except
the ﬁnal layer which was linear. The network was trained on a mean-square-error loss
function.
(a) Conv backwards
weights
(b) FC backwards
weights
(c) Both backwards
weights
(d) Conv no nonlinear
derivative
(e) FC no nonlinear
derivative
(f) Both no nonlinear
derivative
We see that the simpliﬁcations also scale to the CNN for the SVHN dataset, although,
interestingly, performance is degraded on this dataset when both convolutional and
FC nonlinearities are dropped. However, since this does not occur in the other, more
challenging, CIFAR datasets, we take this result to be an anomaly.
6.4.3 Interim Discussion
In sum the AR algorithm uses only simple learning rules to asymptotically approximate
the adjoint terms of backprop over the course of multiple dynamical iterations. We
have demonstrated that the AR algorithm can apply to arbitrary computation graphs,
as can predictive coding, and can be used to train deep CNN models on challenging
Chapter 6. Credit Assignment in the Brain 319
(a) Conv backwards weights
 (b) FC backwards weights
 (c) Both backwards weights
(d) Conv no nonlinear deriva-
tive
(e) FC no nonlinear deriva-
tive
(f) Both no nonlinear deriva-
tive
Figure 6.18: Performance (test accuracy), averaged over 10 seeds, on CIFAR10 demon-
strating the scalability of the learnable backwards weights and dropping the nonlinear
derivatives in a CNN architecture, compared to baseline AR without simpliﬁcations.
Performance is equivalent throughout.
object recognition tasks with a performance equivalent to backprop. Moreover, AR
eschews much of the complexity of competing schemes such as predictive coding, by
not requiring two separate populations of value and error neurons, and equilibrium
propagation by not needing two separate backwards phases – a free phase and a clamped
phase. Additionally, we have demonstrated that some of the remaining biological
implausibilities of AR, such as the weight transport and backwards nonlinearities
problems, can be successfully ameliorated through the right extensions to the algorithm
such as learnable backwards weights with minimal effect on overall performance. Other
constraints, such as the necessity to use the feedforward pass activities in the dynamics
instead of the current activities cannot be relaxed without catastrophically damaging
overall performance.
Chapter 6. Credit Assignment in the Brain 320
(a) Conv backwards
weights
(b) FC backwards
weights
(c) Both backwards
weights
(d) Conv no nonlinear
derivative
(e) FC no nonlinear
derivative
(f) Both no nonlinear
derivative
Like predictive coding, a limitation of this method is its intrinsically iterative nature.
This iteration scheme means that it is at least several times more costly than standard
backpropagation of error – for instance, in these experiments we used 100 iterations to
reach exact convergence to the backpropagated gradients, although this is not strictly
necessary for good performance. While some of this computational cost may be
ameliorated by the intrinsic parallelism of neural circuitry, there is nevertheless a timing
issue if convergence is required to be complete before the next sensory datum arrives,
and issues of gradient interference if it is not. A speculative solution to this could be
synchronization mediated by the brain’s alpha/beta or gamma band frequencies in the
cortex, where one dynamical phase iterating to convergence would correspond to one
wavelength of the band. Such an identiﬁcation is highly speculative however, and the
computational function of such oscillations in the brain are still largely mysterious and
highly controversial (Buzsaki, 2006).
One additional and important drawback of the AR algorithm is that it requires keeping
a memory of the feedforward pass activations throughout the backwards dynamical
phase. This is because the activations swap their purpose from representing feedforward
Chapter 6. Credit Assignment in the Brain 321
pass values in the forward phase, to representing gradients in the backwards phase.
Taken literally, this requires that the learning rules become non-local in time, although
in practice the feedforward pass activations are just stored. While this is the most
substantial drawback to the biological plausibility of the algorithm, it is important
to note that this drawback is also shared with every other iterative algorithm in the
literature. Predictive coding similarly requires the ﬁxed-prediction assumption, which
is effectively the same, and equilibrium-propagation requires that all the activations at
the equilibrium of the free phase are stored while the clamped phase converges. We
suggest that this necessity for storage in iterative algorithms to approximate backprop is
universal and it arises for a simple reason. Namely, that the backpropagated gradients
themselves fundamentally only depend upon the feedforward pass values, since we are
backpropagating only on the feedforward pass and not through the dynamics themselves.
Since the gradients ultimately depend only on the feedforward pass, any dynamical
scheme to approximate them must ‘remember’ what these values are, somehow. In
theory, it may be possible to design algorithms such that this memory is implicit and
thus not explicitly necessary, but no such algorithms have, to my knowledge, been found
so far.
This issue of memory is also closely related to the core computational properties of
reverse-mode AD. Speciﬁcally, that it requires storage in memory of all intermediate
activations in the forward pass. This means that the memory issue also closely applies to
sequential backward algorithms such as target-propagation and, indeed, backprop itself.
The fundamental fact is that backwards pass can only take place after the forward pass
is complete, and that it requires knowledge of forward pass activities. This fact cannot
be ignored or cleverly wished away by any algorithm but must simply be addressed. If
the brain is performing reverse-mode AD, then it simply must have some way to store,
implicitly or explicitly, the feedforward pass values. The question then becomes how
can this be done in the brain? Some possibilities are multiplexing using the brain’s own
intrinsic rhythms (Buzsaki, 2006), or some kind of special parallel class or neurons to
Chapter 6. Credit Assignment in the Brain 322
maintain activity explicitly (O’Reilly, Braver, Cohen, et al., 1999), or else storage at the
synaptic level through mechanisms like eligibility traces (Bellec et al., 2020). While
we here remain ambiguous on the means by which such storage is achieved, we have
reached a point of conceptual clarity in knowing that there must be storage. The only
question is how.
6.5 Three-Factor Learning Rules and a Direct Implemen-
tation
After having gone through several iterative algorithms for approximating the backprop-
agation of error algorithm, it is worth taking a step back to understand what has been
shown and what is truly necessary. Here, we argue that in fact, despite strong claims
that backprop is biologically implausible, in fact an actual implementation of backprop
in the brain could be surprisingly simple and biologically plausible. Moreover, that
many of the algorithms, including the iterative algorithms previously, may simply be
complicating matters. The material in this section is speculative and early stage, and
will be investigated further in future work.
First, we begin by stating, somewhat boldly, that in general the weight transport problem
is solved. That is, there is now fairly strong evidence in the literature (Akrout et al.,
2019; Amit, 2019; Lillicrap et al., 2016; Millidge, Tschantz, Seth, & Buckley, 2020d)
ﬁrstly that a precise equality of forward and backwards weights is not necessary for
good learning performance, and secondly, that competitive performance with backprop
can be maintained, even for deep networks, through learnable backwards weights which
update with a fairly straightforward Hebbian learning rule. If we assume that the weight
transport problem is solved, then there is only the locality issues remaining with a direct
implementation of backprop.
Firstly, we note that if we are implementing reverse-mode AD, and we ﬁnd a way to
Chapter 6. Credit Assignment in the Brain 323
compute the adjoint ∂L
∂xl locally at a layer then we actual weight update ∂L
∂Wl = ∂L
∂xl
∂xl
∂Wl
requires only local information. This means that almost the entire challenge of backprop
is computing the adjoint term locally.
Secondly, we notice that the standard forward pass of an artiﬁcial neural network
xl = f (Wlxl−1) does not actually correspond to how it would work in the brain since
here the activation function (which is typically assumed to be applied through the
threshold for making an action potential in the cell soma) is applied after the weights,
while in the brain the synaptic weights are on the dendrites of the post-synaptic neuron,
and thus occur after the action potential. This means that instead we should use the
more biologically plausible forward pass as,
xl = Wl f (xl−1) (6.34)
Where the order of the weights and the activation function are switched. Speciﬁcally,
this means that the activation function only applies to the presynaptic activity and not
the weights. This approach considerably simpliﬁes the requisite gradients, so we get
the following expressions for the adjoint and the weight update
∂L
∂xl = ∂L
∂xl+1Wl+1T
f ′(xl)
∂xl
∂Wl = ∂L
∂xl f (xl) (6.35)
Speciﬁcally, the adjoint recursion is simply the adjoint of the layer above, multiplied
through the backwards weights and the derivative of the activation function of the
pre-synaptic activity. The weight update is substantially simpler since, because the
forward pass is linear in the weights, it is simply a multiplication of the adjoint with
the presynaptic activity which is essentially Hebbian except with the adjoint replacing
the post-synaptic term. Interestingly, this applies that in this case of backprop, the
post-synaptic term should have no effect on the synaptic weights, which is a very strong
and counterintuitive empirical prediction of backprop.
Chapter 6. Credit Assignment in the Brain 324
While it may seem that the switch in the order of the weights and the activation function
might have some serious impact on the expressive power of the network, we argue that
it likely does not. In fact, the two formulations are equivalent in deep neural networks
except for the beginning and ending layers. To see this, we can simply explicitly write
out the expression for the function computed by a 4 layer neural network,
y = f (W4 f (W3 f (W2 f (W1x)))) ≈W4 f (W3 f (W2 f (W1 f (x)))) (6.36)
For the vast majority of layers, these expressions are the same up to a re-bracketing.
The only difference is the last layer has no activation function using the more neural
forward pass (but often in neural networks we use a linear last layer anyway), and that
the input is ﬁrst passed to an activation function activation function where this is not
common in standard ANNs – but this function could always be set to the identity to
make the equivalence exact. In general, the effect of these differences will be minor for
deep networks.
Now we note the crucial point, that the only biological implausibilities in these learning
rules is the necessity to have the adjoint value present at the synapse for the weight
update rule, since it is just the multiplication of the adjoint and the presynaptic activation.
The recursive computation of the adjoint itself (Equation 6.3 is relatively plausible,
since the only difﬁculty is the nonlinear derivative term f ′(x) which now is only the
derivative of the activation function applied to the pre-synaptic input. Importantly, for
spiking neurons, this derivative is trivial as is essentially consists of a spike when the
neuron ﬁres and not when it doesn’t. As such, this rule effectively says that the adjoint
should only be updated when there is presynaptic ﬁring. Putting this all together, we
can imagine implementing this in the brain in a fairly direct forward-backward scheme
as in Figure 6.20
Speciﬁcally, if we assume that the weight transport problem can be solved with inde-
pendently learnable backwards weights, then the recursion for the adjoint becomes
Chapter 6. Credit Assignment in the Brain 325
ϵL
x3
x2
x1
I3
I2
I1
ϵL = dL
d ̂y
̂y = W4 f(x3)dW4
dt = dL
dW4
= ϵL f(x3)
Ψ4
Ψ3
Ψ2
f′ (x3)
f′ (x1)
f(x2)
f(x1)
I3 ≈ dL
dx3
= ϵLΨ4 f′ (x3)
I3 ≈ dL
dx2
= I3Ψ3 f′ (x2)
I3 ≈ dL
dx1
= I2Ψ2 f′ (x1)
x3 = W3 f(x2)
x2 = W2 f(x1)
dW3
dt = dL
dW3
= I3 f(x2)
dW2
dt = dL
dW2
= I2 f(x1)
W4
W3
W2
dΨ4
dt = f(x3)ϵT
L
dΨ3
dt = f(x2)IT
3
dΨ2
dt = f(x1)IT
2
f(x3)
f′ (x2)
Figure 6.20: Potential schematic for a direct implementation of backprop in the brain. All
that is necessary for this to be plausible is three-factor learning rules.
simply,
∂L
∂xl = ∂L
∂xl+1 ψT f ′(xl)
≈ ∂L
∂xl+1 ψT (6.37)
where the second line is the recursion if we simply ignore the nonlinear derivative term,
as we have also found does not hinder learning in practice (Millidge, Tschantz, Buckley,
& Seth, 2020; Millidge, Tschantz, Seth, & Buckley, 2020a, 2020d; Ororbia & Mali,
2019). This recursion is extremely simple and is just the adjoint mapped through the
backwards weights to the layer below. Thus, we can imagine keeping separate forward
and backward passes whereby the adjoint is represented by interneurons Il. Here, the
update rules simply become,
Il = Il+1ψT
∂L
∂Wl = Il f (xl) (6.38)
The simplicity of these update rules implies that the only potential biological implausi-
bility is in the weight update 6.28 where we have transmitted the value of the adjoint
Chapter 6. Credit Assignment in the Brain 326
‘interneurons’ to the synapses of the post-synaptic neuron, and used them to update those
weights. Through a careful analysis, we have revealed this question to be the ultimate
crux of whether backprop in the brain is plausible or not. Importantly, this ability to use
the adjoints as part of a ‘local’ learning rule is crucial to every purported ‘biologically-
plausible’ method in the literature – from predictive coding, to target-propagation, to
equilibrium-prop and AR.
Whether or not the adjoint can be transmitted to the synaptic weights in the brain is
currently a controversial and unresolved question, and to my knowledge, has not been
studied directly. While it may seem fairly obscure, this analysis suggests that this
question is absolutely crucial to our understanding of whether the brain can directly
implement backprop or not. An important possibility is that of segregated dendrites
(Sacramento et al., 2018). Having separate dendritic compartments would, presumably,
straightforwardly enable the broadcast of the adjoint values to the synaptic weights,
since they would be located in the dendritic tree of the same neuron. The key question
would then become whether information transmitted to the dendrites could remain
segregated. That is, could both Equation 6.34 and Equation 6.38 be implemented using
the same neuron. If this is the case then it would suggest an incredibly simple biological
implementation for backpropagation, needing only a single type of neuron which would
both send forward connections and reciprocally receive backwards connections from
neurons in the layer above. Such a simple architecture would lend great support to the
idea that the brain can indeed implement backprop, and perhaps that backpropagation
is so straightforward that it may even function as a computational primitive in neural
circuitry.
6.6 Discussion
Overall, in this chapter, we have shown that predictive coding, as an approximation to
the backpropagation of error algorithm, can be extended to arbitrary computation graphs
Chapter 6. Credit Assignment in the Brain 327
and we have applied predictive coding to large-scale machine learning architectures
such as CNNs and LSTMs and demonstrated that they perform comparably to backprop
trained networks. Moreover, we have posited a novel iterative algorithm – Activation
Relaxation – that also converges to the exact backprop gradients, does not require two
separate populations of predictions and prediction error units, and uses extremely simple
and elegant learning and update rules. We have shown that AR also scales to large-scale
CNN neural network models and is competitive with backprop trained networks at scale.
Finally, taking experience from our previous work in this ﬁeld, we have re-analyzesd the
problem of backpropagation in the brain from ﬁrst principles and discovered, somewhat
surprisingly, that if we assume that the weight transport problem is solved, the only
major issue of biological implausibility is whether recursively computed adjoints can
be ‘transferred’ onto synapses to be able to form part of the synaptic weight updates. If
they can, and it seems likely that this is possible through a mechanism of segregated
dendrites or, alternatively, backpropagating action potentials (Stuart, Spruston, Sakmann,
& Häusser, 1997), then we can be fairly certain that propagation in the brain is at least
theoretically achievable. This is a remarkable turn-around from the consensus only
ﬁve years ago that it was completely biologically implausible, and speaks to the rapid
development and advances in this ﬁeld.
Additionally, while this thesis chapter has presented a substantial extension to an existing
algorithm (predictive coding), and an entirely novel algorithm for credit assignment in
the brain (activation relaxation), we also wish to highlight the conceptual contributions
we have made while thinking about these issues deeply. In my opinion, these are
perhaps the most important sections of this work. Namely, ﬁrstly, the issue of memory
and time in any implementation or approximation to reverse-mode AD. Speciﬁcally,
that any biologically plausible algorithm, whether sequential or iterative, due to the very
nature of reverse-mode AD must store the values of the feedforward pass throughout
the backwards sweep or phase, either implicitly or explicitly. While the rationale for
this seems obvious in retrospect, it was not clear beforehand, and is still not at all clear
Chapter 6. Credit Assignment in the Brain 328
in the literature. Indeed, the dependence of almost all of these biologically plausible
algorithms on the memory of forward pass values is generally obfuscated or presented
as a minor hindrance, when in fact it is an absolutely irrevocable fact of the comptuation
these algorithms are trying to render biologically plausible. Secondly, and perhaps
most importantly, we have reached the crux of the issue of whether backpropagation
in the brain is plausible – namely whether adjoints, which must remain separate from
the post-synaptic activation – can modulate synaptic weight updates. If they can, then
very biologically plausible and elegant schemes exist for a direct implementation of
backpropagation in the brain (see Equations 6.38, 6.28). If it is not possible, then it
seems likely, given that the adjoint equation is absolutely fundamental to reverse-mode
AD, that backpropagation in the brain is not plausible or, at least, is explicitly achievable
except via some roundabout method. Moreover, if the transport of the adjoint onto
the synaptic weight terminals is possible, then it must be supported by some kind of
dedicated neurophysiological mechanism, which can and must be studied in detail if we
are to understand the explicit details of this key aspect of credit assignment in the brain.
Nevertheless, I now believe that the key mathematical and conceptual issues in the
question of whether the brain can do backpropagation (in this rate-coded static model)
have been largely worked out and depend now solely on details of neurophysiology.
The crucial caveat to this response is that we have only worked out credit assignment in
an incredibly simpliﬁed model of what occurs in the brain – namely with rate-coded
integrate and ﬁre neurons – on static inputs. Both of these assumptions, however,
are false in the brain. Firstly, neurons are spiking networks which may communicate
using precise spike timings to convey information. Understanding how to perform
credit assignment in such spiking networks is still a young and open ﬁeld, although
there has been much recent progress (Kaiser et al., 2020; Neftci, Mostafa, & Zenke,
2019; Schiess, Urbanczik, & Senn, 2016; Zenke & Ganguli, 2018). Moreover, and
crucially, the key problem the brain faces is not just backpropagation through space
(i.e. layers), but backpropagation through time. The brain must be able to assign credit
Chapter 6. Credit Assignment in the Brain 329
correctly to temporally distant events from the synaptic weight values that, ultimately
caused them. While a mathematical formulation of reverse-mode AD can be directly
formulated by simply performing backprop on a computation graph ‘unrolled through
time’, in practice this means that in the backwards phase thattime must run backwards
or, alternatively that the network must store not only the feedforward pass, but its entire
history, which is deﬁnitely biologically implausible. The key question is thus how to
implement backpropagation through time in a biologically plausible manner. There
has been much recent progress in this ﬁeld, also combined with spiking networks such
as (Bellec et al., 2020; Zenke & Ganguli, 2018). However, the innovative approach
engendered by Eligibility Propagation (E-prop) only applies to single layer recurrent
networks, leaving open the question of how to marry backpropagation through space
and backpropagation through time.
An additional interesting consideration is that throughout, and generally in the litera-
ture, only reverse-mode AD is considered to be a contender for the credit assignment
algorithm implemented in the brain. This is due to the historical use and generally
better computational properties of reverse-mode for artiﬁcial neural networks (Bay-
din et al., 2017; Griewank et al., 1989) and has thus become the dominant paradigm
(Goodfellow et al., 2016; Rumelhart & Zipser, 1985; Silver et al., 2017). However,
this is not necessarily the case in highly parallel architectures like the brain, for which
additional forward computation cost may be effectively negligible due to the degree
of parallelization. Forward-mode AD can be implemented through dual numbers –
which directly pair activations with their derivatives, and which it is interesting to
think about how this could relate to neural activity and synapses. Moreover, a key
computational advantage of forward-mode AD is that it imposes no memory cost, since
it is entirely online and requires no storage of intermediate activations, thus entirely
obviating the memory issues inherent in implementations of reverse-mode AD. The
disadvantage, however, with forward-mode AD in the brain is that it dislocates the
derivative computation from the physical location of the synapses. The computations
Chapter 6. Credit Assignment in the Brain 330
and derivatives ‘move forwards’ up through higher layers and levels of processing while
the synapses remain ﬁrmly put. it is necessary, then, whenever the computations and
derivatives reach the end of the process to transmit the fully computed derivatives back
to where they originated. Precisely working out this process has, to my knowledge,
not yet been done, but it may result in a practicable algorithm. One important case
where forward-mode AD makes sense is in backpropagation through time since, as the
computation moves forward in time, so do the synapses themselves. Thus, the correct
derivatives are always locally available precisely when they are needed. Forward-mode
AD through time is known as real-time-recurrent learning (RTRL) (R. J. Williams &
Zipser, 1989a), and is potentially a good algorithm for the brain to solve recurrence,
although it is extremely computationally expensive, rendering it uncompetitive with
reverse-mode BPTT for training large neural networks. Moreover, by taking various
sparse approximations to RTRL, it is possible to reduce the computational cost at the
cost of somewhat reduced learning performance. Algorithms such as eligibility-prop
essentially try to make RTRL updates biologically plausible, with some success.
6.7 Conclusion
In this chapter, we have proposed two novel biologically plausible algorithms for credit
assignment in the brain. Firstly, we demonstrate that predictive coding, under the
ﬁxed prediction assumption, and set-up in a ‘reverse mode’ naturally computed the
gradients required for the backpropagation of error algorithm, as its dynamics satisfy
the same recursive structure of the adjoint equation and thus, the ﬁxed points of the
prediction errors, upon convergence, equal the backpropagated error gradients which
can then be used to perform backprop. We have extensively empirically validated
this correspondence and used it to train large-scale and complex machine learning
architectures such as CNNs and LSTMs with performance equal to those trained by
backprop.
Chapter 6. Credit Assignment in the Brain 331
Secondly, we have utilized the insights gained by our work with predictive coding to
derive a new, and much simpler algorithm which we callActivation Relaxation (AR).
Here, instead of using separate prediction error neurons, we simply update the activation
of the value neurons themselves to become equal to the backpropagated errors during the
backwards iteration phase. While this eschews the ﬁxed feedforward pass assumption
required for predictive coding, it introduces a similar requirement of storing the initial
feedforward pass value throughout the backwards iteration phase, so that they can then
be used during the weight updates. We also empirically validate this correspondence
and demonstrate that AR can be used to train machine learning architectures with the
same performance as backpropagation. Importantly, we also investigate the potential for
applying the same biologically plausible relaxations to the AR algorithm as we applied
to predictive coding in Chapter 3, and show that the relaxations perform just as well
in this new setting – speaking to robustness and generalizability of these relaxations.
Overall, we believe the AR algorithm is simpler, more elegant, and more biologically
plausible than competing iterative backprop schemes such as predictive coding and
equilibrium-propagation. However, it suffers from the limitations inherent in all iterative
approaches – the necessity to somehow store the feedforward pass values throughout
the backwards pass. A clear understanding of this limitation, then opens the way
for future work to try to remedy it or propose a different method entirely for solving
backpropagation in the brain.
Finally, we have included some current (and unpublished) speculations on the po-
tential solution to backpropagation in the brain for simple feedforward networks of
rate-coded integrate and ﬁre neurons, and have constructed a relatively direct method
of implementing backpropagation with only a few moving components. Importantly,
this construction relies heavily ﬁrst on its nonstandard deﬁnition of the forward pass,
using xl+1 = W f (xl) – or the weights after the activation function, rather than inside
of it – which is non-standard for artiﬁcial neural networks, but is actually more bio-
logically plausible, and secondly on the solution to the weight transport problem to
Chapter 6. Credit Assignment in the Brain 332
allow for learnable backward weights. With these issues circumvented, we believe that
biologically plausible backpropagation for rate-coded integrate and ﬁre neurons actually
turns out to be relatively straightforward. The key next move for future work, now that
this base of understanding is established, is to start to attack the substantially harder
problem of biologically plausible implementations of backpropagation through time, as
well as with spiking neuron models.
Overall, in this chapter, we believe that we have made several clear contributions towards
understanding the biological plausibility of backpropagation in the brain – ﬁrstly by
providing and empirically validating two new iterative algorithms (predictive coding
and activation relaxation) and secondly by coming to a much clearer understanding of
what exactly the remaining stumbling blocks to a biological implementation are.
Chapter 7
Discussion
The computer scientist and mathematician Richard Hamming in his insightful essay
You and Your Research describes how he would pose the following question to his
colleagues at Bell Labs – ‘What is the most important question in your ﬁeld, and why
aren’t you working on it?’. As might be expected, this made him unpopular with many
of his colleagues. However, it speaks an important truth – the absolute and overriding
importance of posing and working on the right questions. An important question that is
impossible is futile. A tractable but unimportant question is useless. Throughout my
PhD, I have endeavoured to orient by Hamming’s maxim; to ﬁnd the most important
yet solvable question within my ﬁeld and to try to answer it. This thesis, then, can be
seen as a concatenation of three questions of progressively (in my opinion) increasing
scope and importance.
The ﬁrst question is local to the active inference community, but is very important within
it. Namely, can active inference be combined with contemporary deep reinforcement
learning methods, and thus be scaled to the kind of tasks that can be handled by
contemporary deep reinforcement learning? Conversely, does the theory of active
inference itself contain any insights which can be useful for machine learning theorists
and practitioners? I believe that through my work (Millidge, 2019a, 2020; Millidge,
333
Chapter 7. Discussion 334
Tschantz, Seth, & Buckley, 2020b; Tschantz, Millidge, et al., 2020a, 2020b) and others
(Çatal et al., 2020; Fountas et al., 2020; Tschantz, Baltieri, et al., 2020; Ueltzhöffer,
2018), both sides of this question have been deﬁnitively answered in the afﬁrmative.
Active inference can be straightforwardly scaled up using artiﬁcial neural networks and
the techniques of deep reinforcement learning while, conversely, active inference has
many interesting properties and ideas which could be of use to the deep reinforcement
learning community. In this thesis, we have explored several of these ideas and primarily
focused on how deep active inference and deep reinforcement learning can be merged.
Now that this has been answered, future work should focus on the converse – how deep
active inference differs from deep reinforcement learning and the extent to which it can
inform and lead to novel and performant algorithms in deep reinforcement learning.
The second question is more broadly targeted to the reinforcement learning and cognitive
science communities, and concerns the mathematical origins of exploration . This
question is central to a number of related disciplines such as reinforcement learning
(Sutton & Barto, 2018), decision theory (Daw et al., 2006), control theory (Kalman et
al., 1960), and behavioural economics (Tversky & Kahneman, 1974), which all share
the same fundamental object of study – adaptive decision-making under uncertainty.
Where there is uncertainty so that the true dynamics of the environment and/or the
value of each possible contingency are not known, then the optimal policy cannot
straightforwardly be computed, and agents are necessarily faced with the exploration-
exploitation trade-off. This trade-off arises because new information can generally
only be obtained by trying new courses of action or venturing into new regions of the
state-space. However, to explore new regions necessarily has an opportunity cost of
not doing what you thought to be the current best option, which could instead have
been exploited. Given that to succeed at complex tasks, it is almost always necessary
to explore, it is important to ﬁgure out how to explore in the most efﬁcient manner.
Speciﬁcally, we wish to design algorithms that can acquire the information necessary
for success as rapidly as possible while incurring the minimum opportunity cost. In
Chapter 7. Discussion 335
the literature, it has been discovered that a very good heuristic for doing this is simply
to optimize a combination of the greedy reward maximization objective exploit with
an additional information gain exploration term explore (Schmidhuber, 2007; Shyam
et al., 2019; Tschantz, Millidge, et al., 2020b). Speciﬁcally, the reward maximization
part of the objective ensures that agents do not spend large amounts of time exploring
informative but barren regions, while the information gain terms help the agent not to
get stuck in locally greedy, but globally poor optima. While this objective works well
in practice, its mathematical origin and nature remains obscure. In the literature, this
approach is often described intuitively as simply adding an additional exploratory term
to the loss function. While random entropy-maximizing exploration can be derived
straightforwardly from variational inference approaches to action (Levine, 2018), the
mathematical origin and commitments of speciﬁcally optimizing information-seeking
exploration terms has remained mysterious. This is the second question we set out to
answer in this thesis – mathematically, from what sort of fundamental objectives do
information-gain exploration terms arise, and how can we characterise the possible
space of such objectives?
In Chapter 5, we answer this question. We show that information gain maximizing ex-
ploration arises from minimizing the divergence between two distributions – a predicted
or expected distribution over likely states, given actions, and a desired distribution over
states, which encodes the goals of the agent. This differs crucially from evidence objec-
tives, which are typically used in control as inference schemes (Levine, 2018; K. Rawlik
et al., 2013), which only seek to maximize the likelihood of the desired states, rather
than explicitly match the two distributions. This ﬁnding has important implications for
a wide range of ﬁelds. Speciﬁcally, we argue that any kind of information-maximizing
exploratory behaviour can be seen as implicitly aiming for a matching of two distri-
butions rather than a likelihood maximization. This, for instance, can explain several
phenomenon, such as the probability matching behaviour that is regularly observed in
human participants in cognitive science and behavioural economics tasks (Daw et al.,
Chapter 7. Discussion 336
2006; Shanks, Tunney, & McCarthy, 2002; Vulkan, 2000; West & Stanovich, 2003),
which are puzzling under the presumption of evidence maximization (Gaissmaier &
Schooler, 2008; Tversky & Kahneman, 1974). Moreover, by understanding the origin
of information-seeking behaviour as emerging directly from divergence objectives, it
provides us with a greater and deeper understanding of what agents which optimize
these information-seeking terms are actually doing, while the ensuing mathematical
understanding may allow us to manipulate these terms more conﬁdently into more
easily computable or tractable versions which could aid implementations directly.
The third question is one with the greatest scope and importance. This question is
how can credit assignment be implemented in the brain? And, speciﬁcally, whether
and how (if it does) the brain can implement the backpropagation of error algorithm.
The solution to such a question would be of great importance to neuroscience, since
it would provide a unifying view and mechanistic, algorithmic explanation of at least
part of cortical function. Moreover, it would explain at a detailed level one of the core
functionalities of the brain, and the one that underpins almost all adaptive behaviour.
Credit assignment is crucial to any kind of long-range learning of the kind that must
be occurring in the brain. It is crucial for everything from learning the best way to
form and interpret sensory representations, to action selection operations to, potentially
long term memory and complex cognitive processing. While the brain undoubtedly
performs a substantial amount of top-down contextual feedback processing as well as
various kinds of homeostatic plasticity, which both remain poorly understood, we also
know from the stunning success of machine learning in the last decade that simple
feedforward passes on large neural networks trained with the backpropagation of
error algorithm can accomplish tasks such as visual object recognition (Child, 2020;
Krizhevsky et al., 2012), generating realistic images from text inputs ((Radford et al.,
2021), human-passable natural language generation (Radford et al., 2019), and playing
at a superhuman level games such as Go (Silver et al., 2017), Atari (V . Mnih et al.,
2013, 2015; Schrittwieser et al., 2019), and Starcraft (Vinyals et al., 2019), which ten
Chapter 7. Discussion 337
years ago were thought to be extremely challenging, if not impossible for computers to
accomplish. Credit assignment, then, must be one of the core operations in the brain,
and if we can understand this then it is possible we may obtain a grasp on how known
computational algorithms are implemented in the brain which allows us to grapple
tractably with its immense complexity.
While I, and the ﬁeld as a whole, have taken steps towards addressing and answering this
question, we are still a long way from a viable solution for the brain. Nevertheless, I feel
that, in general, with the profusion of algorithms addressing issues such as the weight
transport problem (Akrout et al., 2019; Lillicrap et al., 2016; Nøkland, 2016), and
well as addressing issues of locality (Ororbia & Mali, 2019; Scellier & Bengio, 2016;
Whittington & Bogacz, 2017), the ﬁeld is close to a good solution for the case of rate-
coded neurons on a temporally static graph. However, a solution under these constraints
is fundamentally only an abstraction of a much messier reality, where neurons in the
brain are not rate-coded but spiking, and must also achieve credit assignment not just
across space (layers) but across time (Lillicrap et al., 2020). While there are some
approaches which grapple with these additional, and harder problems (Bellec et al.,
2020; Zenke & Ganguli, 2018), there are not many and we are far from a viable global
solution to this problem. I suspect it is into these new domains that future research
should primarily be directed, and where important advances will be made.
7.1 Question 1: Scaling Active Inference
It turns out the active inference approaches can be quite straightforwardly merged with
those used in deep reinforcement learning and can thus straightforwardly be ‘scaled up’
to achieve performance comparable with the state of the art. Moreover, different choices
lead directly to different schools of model-free or model-based reinforcement learning.
Speciﬁcally, active inference fundamentally operates on several core probabilistic
distributions and objectives. The key distributions are the likelihood distribution p(o|x),
Chapter 7. Discussion 338
and the transition distribution p(xt|xt−1,at). While standard discrete-state-space active
inference approaches parametrize these explicitly with categorical distributions, and
optimization of the variational free energy exactly using analytical solutions resulting
in a ﬁxed-point iteration algorithm, deep reinforcement learning algorithms instead
amortize these distributions with artiﬁcial neural networks, and instead optimize the
variational free energy with respect to the amortised parameters (the weights of the
artiﬁcial neural networks). In pure inference terms this can be seen as an E-M algorithm,
with a trivial E-step (amortised inference as a forward pass through the networks), and
then an iterative M-step which corresponds to a gradient step of stochastic gradient
descent on the weights of the neural networks.
The next translation is to identify the value function in deep reinforcement learning with
the path integral of the expected free energy over time in active inference. Therefore the
derivation of the optimal policy under active inferenceq(π) =σ(
∫
dtG(π)t) can be seen
as a design choice in active inference to perform what is effectively Thompson sampling
over the softmaxed value function in reinforcement learning, a choice which is often,
but not necessarily made in comparable reinforcement learning algorithms (Osband
& Van Roy, 2015). Finally, the sole remaining question remains how to compute
or approximate the path integral of the expected free energy, or the value function.
While the discrete state-space active inference literature typically only deals with short
time horizons and small state-spaces where this integral can be exhaustively computed
(Da Costa, Parr, et al., 2020), or else by simply enumerating and pruning unlikely paths
(Friston, Da Costa, Hafner, et al., 2020), the statespaces and time horizons in deep
reinforcement learning problems are typically large enough that this approach becomes
infeasible.
The sole remaining question remains how to approximate this path integral, and here we
can augment active inference with methods well used in the deep reinforcement learning
community. The approach taken by model-free reinforcement learning is to utilize the
Chapter 7. Discussion 339
iterative and recursive nature of the Bellman equation to maintain and update at all times
a bootstrapped estimate of the value function (Kaelbling et al., 1996; V . Mnih et al.,
2013). This approach was pioneered through the temporal-difference (Sutton, 1988), and
Q-learning algorithms (Watkins & Dayan, 1992), and gives rise to the model-free family
of reinforcement learning algorithms. Translating this into the terms of active inference
is quite straightforward. Since the expected free energy objective can be factorised
into separate independent contributions for each timestep, the path integral satisﬁes a
similar recursive Bellman-like equation. This equivalence has been recently used to
prove the similarities between active inference and reinforcement learning (Da Costa,
Sajid, Parr, Friston, & Smith, 2020). This approach allows us to straightforwardly
deﬁne Q-learning and actor-critic like active inference approaches, as was pioneered
in my paper (Millidge, 2019b). One minor distinction is that the computation of the
expected free energy contains an information gain term which necessitates a model of
the states or dynamics of the world, which would not be necessary when using standard
reinforcement learning approaches, but this information gain yields superior exploratory
capabilities and ultimately performance.
While, in Millidge (2020) we explicitly included action within the generative model in
the agent, so that the action prior p(π) becomes the path integral of the expected free
energy, and the variational policy posterior q(π) becomes the independently trained
policy, thus recapitulating and providing a variational inference gloss on standard
actor-critic algorithms, this is fundamentally a design choice. If we instead ignore the
policy prior p(π) and deﬁne the variational policy posterior q(π) directly in terms of
the value function, we obtain an algorithm very similar to soft-Q-learning from deep
reinforcement learning (Haarnoja, Zhou, Abbeel, & Levine, 2018), except it optimizes
a value functional of the expected free energy instead of the reward.
Conversely, we can take the approach used in deep reinforcement learning to approxi-
mate the value function at every timestep through samples of model rollouts. This is
Chapter 7. Discussion 340
straightforward because the value function is just fundamentally the expected value
of the reward across all possible trajectories under a given policy. Using model-based
rollouts to approximate this is simply taking a monte-carlo approximation of an expecta-
tion where the real environmental dynamics are approximated by the model’s transition
dynamics. By using importance sampling on this objective, we can unsurprisingly see
that the goodness of this approximation depends crucially on the match between the
true and modelled dynamics.
If we are equipped with a model of the transition dynamics of the worldp(xt|xt−1,at−1),
we can approximate the path integral of the expected free energy over time in a similar
way. By simulating rollouts through the transition model under a given policy, and then
averaging together the path integral of the expected free energy across rollouts, we form
a monte carlo estimate of the expected free energy value function. This can then be
used to directly compute the posterior distribution over policies, or else can be fed into
an iterative planning algorithm such as path integral control or CEM which can then be
used to obtain an action plan. Using model-predictive control (replanning at every step),
then allows for the creation of ﬂexible plans for any given situation. Due to utilizing
a transition model and simulated rollouts to estimate the local value function, instead
of bootstrapping from previous experience, this model-based approach is substantially
more sample efﬁcient than the model-free alternative. In the Tschantz, Millidge, et al.
(2020b) paper, we took this approach and demonstrated performance comparable to or
superior to standard model-based benchmarks. Additionally, as before, the exploratory
properties of the expected free energy functional lead to improved performance.
Given that we thus know that active inference can relatively straightforwardly be
mapped to existing algorithms in deep reinforcement learning, we now turn to the
other face of the question – whether deep reinforcement learning can learn anything
from active inference. We again argue in the afﬁrmative. Namely that active inference,
through the expected free energy functional, provides superior exploratory capabilities
Chapter 7. Discussion 341
of active inference agent which, in challenging sparse-reward tasks are necessary to
obtain intelligent behaviour where random exploration is simply not sufﬁcient. While
in reinforcement learning this is a small literature on training agents with additional
exploratory loss functions (Chua et al., 2018; Klyubin et al., 2005; Nagabandi et al.,
2019; Pathak et al., 2017; Shyam et al., 2019; Still & Precup, 2012), active inference
provides a mathematically principled and uniﬁed way of looking at this, rather than
simply postulating ad-hoc additional loss functions (Oudeyer & Kaplan, 2009). More-
over, active inference also provides a theory of reinforcement learning deeply grounded
in variational inference, and can thus, for instance, be straightforwardly extended to
POMDP models in a way that is nontrivial for standard reinforcement learning algo-
rithms. Finally, by proposing a uniﬁed objective of the expected free energy, active
inference allows, in principle, all hyperparameters of the algorithm to be optimized
directly by gradient descents against this objective, thus theoretically obviating the need
for expensive hyperparamter sweeps and tuning.
Although the close connection between deep active inference and deep reinforcement
learning is now understood, and we know it is possible to scale up active inference
to the level of deep reinforcement learning, there still remains much work to be done
actually realizing this connection and constructing deep active inference agents, using
the insights of active inference, which can compete head-to-head with the state of the
art in deep reinforcement learning and, potentially, exceed it. I believe that especially
as the ﬁeld moves towards facing more challenging environments with sparse rewards,
the exploratory drives implicitly embedded within active inference agents will become
increasingly important and impactful, since many current environments possess straight-
forward dense rewards which provide a continuous reward gradient from any initial
condition to a successful ﬁnal policy. In such environments purely random exploration
sufﬁces for learning effective policies. However, such environments are not in general
representative of the kind of environments that face biological organisms in the real
world, and thus to model their behaviour additional exploratory instincts appear to be
Chapter 7. Discussion 342
required.
Furthermore, on a general note, while current work has been focused on trying to
maximize the commonalities between deep active inference and deep reinforcement
learning, as the goal has been to establish the connection and derive proof of principle
scaled up active inference models, later work should go the other way, and try to retain
the scalability of deep reinforcement learning while maximizing on what is unique
about active inference.
One intriguing possibility in this direction is to experiment with more complex distribu-
tions of rewards. While active inference can be formulating in a reward maximizing
way, the real object active inference handles is the biased generative model ˜p(o,x), or
the desire distribution ˜p(o). While this can be deﬁned to be equal to reward maximiza-
tion by simply deﬁning the desire distribution to be a Boltzmann distribution over the
realized rewards ˜p(o) =exp(−r(o)) (Friston et al., 2012), this is not the only way to
do it. Indeed, more complex and potentially multimodal reward distributions could
be deﬁned and optimized directly in the algorithm. In theory this could lead to more
ﬂexible behaviour or, alternatively, being able to model more complex, context-sensitive
or contingent rewards naturally within the framework. There has been very little work
done in this direction, and it remains an exciting avenue for future work.
Another interesting direction is to explicitly model the generative processes producing
action within an inference framework. For instance, search algorithms such as Monte-
Carlo-Tree-Search have been vital in the success in key reinforcement learning tasks
such as playing Go and Chess (Silver et al., 2017), and can theoretically be written in
a probabilistic generative model. Doing this would then allow them to be combined
productively with all the standard tools of Bayesian and variational inference and could
potentially lead to substantially more ﬂexible algorithms for action selection which
would provide extremely powerful inductive biases over standard MLP policy modules
which would allow fast and very effective learning. In a similar vein, continuous-action
Chapter 7. Discussion 343
planning algorithms, which are currently rather primitive (such as CEM (Rubinstein,
1997) and path integral control (Kappen, 2007)) and can only explicitly model unimodal
policies, are generally quite ineffective. Okada and Taniguchi (2020) has shown how
many of these algorithms can be directly modelled as part of a generative model and
directly used in variational inference, and they use this result to derive more effective
algorithms such as multimodal CEM (Okada et al., 2020). Further extending this
work may lead to the derivation of highly effective and efﬁcient planning algorithms
for continuous control, which are currently sorely lacking and which would lead to a
substantial improvement in the abilities of model-based continuous control.
Another interesting avenue, which has begun to be explored in the literature (Friston,
Da Costa, Hafner, et al., 2020), which may lead to more nuanced and effective forms
of exploration, is to explicitly model, in model rollouts, the change in its own beliefs
the agent expects to encounter. If this is explicitly modelled, then the agent can design
exploration strategies speciﬁcally to test hypotheses and explore different strategies. In
short, this kind of meta self-knowledge of the likely changes of one’s own beliefs are
vital for the kind of scientiﬁc and experimental thinking that often characterises humans’
phenomenological experience of the planning process. Such agents would be able to
intelligently consider and compute the value of information both now and the expected
value of information in the future under their expected future beliefs. Basic models
of this have been explored using the discrete-state-space paradigm (Friston, Da Costa,
Hafner, et al., 2020; Hesp et al., 2020), however ﬁguring out how to implement this
within the deep reinforcement learning paradigm in a computationally tractable and
efﬁcient way, as well as to test its performance on tasks which require such nuanced
exploration strategies remains a serious challenge and a worthy research project.
Finally the perspective of active inference – that action and control are merely inference
problems over a graphical model which includes action variables – naturally lends
itself to an understanding of different kinds of inference – speciﬁcally amortised and
Chapter 7. Discussion 344
iterative inference (Y . Kim, Wiseman, Miller, Sontag, & Rush, 2018; Marino et al.,
2018; Millidge, Tschantz, Seth, & Buckley, 2020c). Understanding how these different
types of action can be combined and merged together, to inherit the strengths of both
and ameliorate the weaknesses of each other, is ultimately going to be very important
in designing algorithms which can initially learn rapidly from data and then slowly
converge to a high asymptotic performance. There is also strong, but circumstantial
evidence, that a system like this, which combines iterative and amortised inference,
takes place in the brain. For instance, whenever you start learning a new skill, it
takes a lot of thought and explicit mental planning, but you can learn quickly without
needing an extremely large number of interactions with the environment. However,
as you continue to practice, slowly your skills become habitual. They do not need
mental effort and can occur almost automatically, allowing you to focus on other things
while they are occurring. This distinction between conscious, effortful action and
unconscious, effortless habit is precisely the distinction between iterative and amortised
inference. The beneﬁts of having such a hybrid system are obvious. It is quick to
learn new skills since it can explicitly plan with them, while once a skill has been
practiced many times it can be ofﬂoaded onto a computationally cheap habit system.
This eliminates the necessity, in current model-based reinforcement learning systems,
to undertake expensive model-predictive control and planning on every single timestep,
even when the system has practiced a given contingency many many times, and also
where computing the best action is actually extremely straightforward.
While we have undertaken some preliminary work in this direction, as is reviewed in
this thesis, really the combination of the two to design systems with explicit planning
and habit systems is just beginning and there are very many questions which remain
unanswered. For instance, what is the best way to train the habitual system – should it
be trained to mimic the decisions of the explicit planner, or be completely independently
trained on the reward, or both (i.e. by using the output of the planner as a regulariser
of some kind)? Should the habit system and explicit planner optimize separate reward
Chapter 7. Discussion 345
functions (for instance, should the planner be more exploratory and the habit system
just optimize rewards?)? Should the habit system be used by the planner in any manner
– for instance habit value functions could be used to endstop the model rollouts of the
planner to provide better local value function estimates? Should the habit policy be
used to initialize the planner? How should these systems interact to produce actual
output actions – should the output be the sum of both systems? or should there be some
gating mechanism which selects one or the other to produce the output? If there is such
a gating mechanism, how does it work and how should it compute? How do we know
when to turn off the planner and just use the habit system, if ever? The answer to all
of these questions is a fascinating combination of engineering practice and machine
learning theory, and the end result of getting these questions right will be ﬂexible, robust,
adaptable and sample-efﬁcient systems which also have a high asymptotic performance
with low latency and computational cost when the habit is established. Such systems
could also be used to model similar computations that occur in biological brains and
may shed light into the design choices available for such system and, ultimately, their
neural implementations.
7.2 Question 2: The Mathematical Origins of Exploration
In Chapter 5, we delved deeply into the mathematical origins of exploratory behaviour.
We saw that to obtain information-seeking exploration as a core part of the objective
functional, in addition to reward maximization crucially entails minimizing adivergence
objective instead of an evidence objective. We then related this new dichotomy between
divergence and evidence objectives to a wide range of currently used objectives within
the reinforcement learning and theoretical neuroscience communities. The importance
of this result, really, lies not in the relationship to existing methods, but what it tells us
about the deep foundation of exploration. Put simply, we see that extrinsic exploratory
drives emerge from trying to match rather than maximize. Matching tries to maintain
Chapter 7. Discussion 346
the complexity of the inputs, so that given a complex desire distribution, agents are
driven to stabilize a similarly complex future. Conversely, maximizing implicitly tries
to simplify the inputs, ideally a maximizing agent would collapse all future inputs
to a dirac delta around the future reward. It is only the extent to which there is
uncertainty in the world, or in the reward function, or a lack of controllability in the
world which prevents this full maximization. This difference is what gives rise to
intrinsic exploratory behaviour in the divergence minimization case, and to effectively
anti-exploratory, information-minimizing behaviour in the evidence, reward, or utility
maximizing case. We additionally see that reward maximizing agents do not have, and
cannot have, any intrinsic exploratory drives, since the very nature of their objective
compels them to minimize information gain. Information and learning, to them, is a
cost which must be borne, and not a reward to be pursued for its own sake.
This additional information gain term is very important, because it drives agents which
optimize it to explore and seek out new contingencies, and to ﬁnd and update upon
resolvable uncertainty in their world. This means that agents which have an expressive
desire distribution, will tend to learn faster and better world models, as well as pursue
more exploratory policies, which in the long run lead to higher performance than
purely reward-maximizing agents, even when judged on rewards alone. This has been
investigated by ourselves in Chapter 4, as well as under many other approaches in the
literature. What is most interesting here is that our understanding of these objectives as
divergence objectives provides a precise mathematical characterisation of what these
objectives are implicitly doing.
It is important to note that it is possible to derive some form of information gain ex-
ploration directly from reward maximization – in the form of explicitly computing
and calculating with the value of information (Osband, Van Roy, Russo, & Wen, 2019;
Schmidhuber, 2007; Still & Precup, 2012; Tishby & Polani, 2011), where we can oper-
ationalize the value of information purely in terms of reward as the additional amount
Chapter 7. Discussion 347
of reward expected given better policies as the result of obtaining and integrating the
information into your world and policy models. While this value of information compu-
tation is theoretically optimal given a reward maximization objective, in practice it is
intractable to compute exactly, and there has been little investigation in the literature as
to direct approximations of this term. However, there are some heuristic approaches
which seek to approximate it, although it is not clear how well. For instance, Osband et
al. (2019) argue that using ‘optimistic value functions’ which automatically up-weight
unknown contingencies can lead to a good approximation of the value of information,
since in practice, the agent will automatically explore until it has diminished its opti-
mistic bias to the extent that the bias for all contingencies is lower than the current best
option. This approach, under the names of the upper-conﬁdence bound is widely used in
the multi-armed bandit literature (Garivier & Moulines, 2011) and additionally has been
used to great effect in Monte-Carlo Tree Search algorithms (Kocsis & Szepesvári, 2006),
and has been investigated to some degree within deep reinforcement learning (Silver
et al., 2017). Another approach, known as Thompson sampling (Russo & Van Roy,
2016), explicitly computes or approximates the posterior distribution over actions given
the current history of observations and rewards, and then achieves some degree of
exploration by sampling from this posterior – the idea being that in uncertain regions,
the posterior distribution should be fairly uniform, and thus provides effectively random
exploration, while when the posterior is sharp then it is likely that the true optimum has
been found and thus exploration is unnecessary and costly.
An important challenge is that inference based approaches such as control as inference
do not naturally compute any analogue of the value of information. This is because,
ultimately, these approaches take a mean-ﬁeld factorisation across time and split their
objective up across time-steps. This means that information can only ﬂow through time
through the transition model, and thus the agent cannot model any kind of learning in
the future, where information it may or may not discover in the future leads it to change
its model, leading it to perform better (or worse) in the future. Due to this limitation,
Chapter 7. Discussion 348
which is ultimately applied for reasons of computational tractability, approaches like
control as inference do not compute any kind of value of information across time-steps.
This limitation also applies to the information-seeking methods we discuss which arise
from divergence functionals. If these functionals are also mean-ﬁeld factorized, then
agents only seek to maximize the information-gain in the current time-step. Extending
these methods by relaxing the temporal mean-ﬁeld assumptions will likely yield more
effective and nuanced forms of exploration, which can induce consistent exploratory
behaviour across multiple timesteps and thus handle more complex contingencies.
However, designing effective and mathematically tractable algorithms which can do
this largely remains an avenue for future work.
However, this information-maximizing exploration with divergence functionals is not
done explicitly to gain any kind of future reward. Instead, agents optimizing divergence
objectives treat optimizing information gain as an intrinsic good. This is what allows an
information gain objective to arise even though the objective satisﬁes the same mean
ﬁeld assumptions as the control as inference objective. However, this means that to
some extent divergence minimizing agents will continue to explore beyond the point
which is strictly necessary for reward maximization. This means that, in effect, from
the perspective of a purely reward maximizing agent, divergence minimization is just an
additional exploratory heuristic by which to approximate the value of information terms
within a mean-ﬁeld formulation. However, in general, it has been empirically found that
the information seeking exploration in a mean-ﬁeld fashion, while not strictly the value
of information, gives a good approximation in general and will lead to good performance
especially in high dimensional, sparse environments. However, because it explicitly
trades off a reward maximizing and an information-seeking objective, it will tend to
over-explore relative to pure reward-maximizing value of information computation, by
exploring regions with much resolvable uncertainty but relatively little reward, and will
continue exploring even when it is likely (but not certain) that the optimal solution
has been found. However, as long as all uncertainty in the environment is resolvable,
Chapter 7. Discussion 349
then the divergence objective will eventually converge to the reward maximization
objective since the information gain term will eventually become negligible once the
agent possesses a very good and accurate world model.
An interesting direction for future work will lie in relaxing the mean ﬁeld assumptions
which currently underpin all of these functionals. While it is likely that a full relaxation
will be intractable, there are many intermediate relaxations which have been proposed
in the general variational inference literature which could prove highly fruitful within
the control task. For instance, the Bethe free energy and related objectives (Pearl, 2014;
Schwöbel, Kiebel, & Markovi´c, 2018; Yedidia, Freeman, & Weiss, 2001), allow for
temporal pairwise correlations to be explicitly considered. Moreover, there is a highly
general family of ‘region graph’ approximations (Yedidia, 2011; Yedidia, Freeman, &
Weiss, 2005) which have been developed within the variational inference literature, and
which allow for more complex interactions to be modelled within a relatively tractable
computational framework and which have been found to improve inference performance.
If there is a way to compute successively better Bayesian approximations to things
like the value of information, or to relax the temporal mean-ﬁeld approximations made
in contemporary divergence and evidence objectives for control, it will likely come
from a thorough mathematical and experimental investigation of these more advanced
and accurate approximation techniques. Understanding how the information gain
functionals from divergence objectives function and change as the temporal mean-ﬁeld
approximation is relaxed is especially interesting, since preliminary investigations, even
within the mean ﬁeld paradigm, show that when expressly writing out the objective in
terms of entire trajectories, terms similar to empowerment (Klyubin et al., 2005) and
ﬁltering information gain (backwards in time) result.
Related to the relaxation of the temporal mean-ﬁeld approximation, there also needs
to be much work allowing agents to explicitly model changes to their own beliefs in
the future. This is necessary to truly compute realistic information-seeking objectives
Chapter 7. Discussion 350
when utilizing multi-step planning algorithms, since currently all information gain is
computed with respect to the agent’s current beliefs, which will not necessarily hold in
the future if and when it actually eventually reaches this information. Such a process
would enable an agent to understand how various kinds of information would change its
own beliefs and policies, and thus be able to plan for multiple sequential ‘realizations’.
Human planning is certainly capable of such introspective capabilities, where we can,
for instance, decide to seek out and investigate certain phenomenon in order to be able
to understand and better seek out information about another task, and so on. Some
fascinating recent work has begun to explore these sorts of metacognitive abilities within
the active inference framework (Friston, Da Costa, Hafner, et al., 2020), but only within
a discrete state space and nonscalable task. The true issue with such approaches will
be their inherent computational difﬁculty, since on a naive approach they will require
the agent to simulate its own belief updates, requiring it to store and introspect upon a
copy of its own models and inference procedures. However, since such metacognitive
abilities are likely crucial to effective long range planning, an important strand of future
work will be designing approximations and objectives which can accomplish this in a
computationally tractable manner.
Additionally, the divergence vs evidence framework presents a new class of objectives
which are, in theory, distinct from the usual paradigm of reward or utility maximization.
Here, instead we simply seek to minimize divergence to a complex reward or desire
distribution. In theory, the idea of having a complex, multimodal and potentially non-
scalar desire distribution instead of a simply scalar reward function to maximize is that
in theory it allows for more complex notions of goals or desires to be implemented and
optimized by agents. Speciﬁcally, it allows for vector-valued, and multimodal goals
which are not well handled within the standard reward-maximization framework, but
which can be straightforwardly handled within our formulation of a desire distribution
by both evidence and divergence objectives. While current work has mostly focused
on demonstrating the equivalence of reward-maximization, and the desire distribution
Chapter 7. Discussion 351
under certain conditions (the desire distribution being a Boltzmann distribution of the
reward), and thus showing that the probabilistic case is a strict generalization of the
scalar deterministic case, the real interest of the probabilistic representation is precisely
how it can differ from simple reward-maximization. Much work remains to be done to
understand the possibilities for more ﬂexible and expressive goal or value representation
which is unlocked by this more general formalism, and how it can be leveraged to
design artiﬁcial systems which perform more capably in practice.
Finally, the idea of divergence minimization also has extremely close links with Markov-
Chain-Monte-Carlo inference procedures, where it has recently been realized that much
of this paradigm can be expressed within a simple framework of stochastic dynamical
systems theory, whereby all the various samplers can be interpreted as implementing a
certain stochastic differential equation which explicitly performs a gradient descent on a
divergence objective (Ma et al., 2015), with different algorithms in the literature simply
specifying different noise terms and solenoidal ﬂows (Yuan et al., 2017). Beyond this,
the idea of divergence minimization can also intriguingly be linked to recent advances in
stochastic non-equilibrium thermodynamics (Seifert, 2012), which has developed ways
to translate classic thermodynamic notions of entropy and entropy production from
properties of large ensembles to properties of individual statistical trajectories (Esposito
& Van den Broeck, 2010). A crucial result in this new formalism of stochastic dynamics
is that any system with positive entropy production can be construed as minimizing a
divergence between its current state and its ultimate steady state density (Esposito &
Van den Broeck, 2010) – and can thus be interpreted as performing a form of direct
divergence minimization – thus potentially implying that this objective may in some
sense be a more natural one for systems and agents to perform than pure evidence
maximization, and secondly that the laws of thermodynamics themselves may implicitly
require information-maximizing behaviour from disspative non-equilibrium systems.
While these links currently remain speculative, and further investigation will likely
discover greater nuance and require some qualiﬁcation of these claims, in my opinion
Chapter 7. Discussion 352
there is signiﬁcant potential here to link discoveries in stochastic thermodynamics to
help us build a fully general picture of the necessary nature of exploratory behaviours
in systems evincing the classic action-perception loop.
7.3 Question 3: Credit Assignment in the Brain
By showing that predictive coding can approximate backpropagation along arbitrary
computation graphs, instead of just MLPs, we have speciﬁcally turned predictive coding
from a direct model of brain function or perception, into a learning algorithm which
can be applied to arbitrary architectures. This dual perspective, where predictive coding
is both a generic learning algorithm, as well as speciﬁcally a model of learning and
perceptual inference in the brain is most interesting and, as far as I am aware, is unique
to predictive coding. Speciﬁcally, casting predictive coding as a learning algorithm
makes clear several interesting correspondences between inference procedures and
learning. Speciﬁcally, that we can derive a learning algorithm on a computational graph
by trying to infer the values of the nodes in the graph. Predictive coding additionally
provides for a straightforward extension to backprop in the form of precisions, which
allow for the learnable up or down weighting of certain gradient signals depending on
the intrinsic noise of their generating process. Such a system, while not particularly
useful in the standard machine learning paradigm of independent, identically distributed
datasets, may prove extremely important for learning with more ecologically valid
sensory streams which contain various amounts of noise and distractor information
which should not be learnt from. This perspective of learning and credit assignment as
a kind of inference also immediately lends itself to further applications and extensions
beyond just precision. For instance, the effect of different generative models other
than Gaussian remain to be determined, as well as potentially different optimization
procedures and variational functionals to be optimized. In general, this perspective
allows the highly developed machinery of inference in graphical models (Beal, 2003;
Chapter 7. Discussion 353
Ghahramani & Beal, 2001; Pearl, 2014; Yedidia, 2011) to be deployed to improve
credit assignment and optimization algorithms. This area, I believe, is an exciting and
potentially highly impactful one for future work since the impact of improving either
credit assignment or optimization processes, which are at the heart of all of modern
machine learning, will necessarily be substantial.
Furthermore, by demonstrating that many of the biological implausibilities in the pre-
dictive coding scheme can be relaxed (Millidge, Tschantz, Seth, & Buckley, 2020d), we,
for the ﬁrst time, demonstrate a biologically plausible local approximation to backprop
which does not suffer from either issues of locality or issues of weight transport. Fur-
thermore, we develop a novel and much simpliﬁed algorithm – Activation Relaxation –
which possesses relatively straightforward, local, and elegant update rules when com-
pared with predictive coding which succeeds in approximating the backpropagation of
error algorithm to arbitrary accuracy given enough iteration steps. Moreover, we have
shown that the same relaxations which work with predictive coding, also work with the
activation relaxation algorithm, thus demonstrating the robustness and efﬁcacy both of
the AR algorithm and of the methods of relaxation utilized. Crucially, if we look at the
ﬁnal, relaxed AR update rule (Equation 6.32, we see that the change in activities for a
layer only requires the current activation values of the layer above, mapped through the
backwards weights which are learnt independently of the forward weights, be subtracted
from the current activation of the layer. This is sufﬁcient, over a number of iterations,
to allow the activations of each layer of the network to converge to the gradients of
backprop. Then, once this is achieved, the weights can be updated.
While these algorithms have considerable advantages – they exactly approximate back-
prop given enough iterations, they require only biologically plausible local update rules,
and they are simple and potentially straightforward to implement in neural circuitry, they
also have substantial disadvantages. I believe these disadvantages are worth discussing
in some depth since they provide a precise speciﬁcation of the areas for improvement
Chapter 7. Discussion 354
in current algorithms. The fundamental disadvantage of iterative schemes like this is
their iterative nature. Speciﬁcally, they require separate phases of operation – a forward
phase which is equivalent to a feedforward pass through the network, and a backwards
iterative phase of multiple dynamical iterations. A signiﬁcant issue is that it is unclear
whether such phases can realistically exist within the brain. While there is evidence
for different oscillatory frequency bands in the brain (Buzsaki, 2006), and even in
superﬁcial vs deep cortical layers (Bastos, Lundqvist, Waite, Kopell, & Miller, 2020;
Bastos et al., 2015), it is unclear whether these rhythms do, or can, coordinate separate
feedforward and iterative phases. Such a scheme, if implemented in the brain, would
form a kind of clock, only allowing feedforward information to be processed in short
bursts in between the iterative phases. While not impossible, this seems at odds with our
current understanding of the brain where feedforward and feedback inputs are combined
together in real time. Another very straightforward problem is simply the number of
iterations these schemes require. The brain cannot wait for tens or hundreds of iterations
until convergence, even under generous assumptions about rhythmic activity implement-
ing the phases, while these schemes often require a substantial amount of iterations
to converge to nearly exactly the backprop gradients. While this could be ameliorated
somewhat with high learning rates, and settling for less than exact convergence to the
backprop gradients, it has not yet been extensively investigated whether the algorithms
are even stable under such conditions. Understanding and optimizing the number of
iterations and various parameters like the learning rate is still an open area of research.
Unlike backprop with deep neural networks, where all hyperparameters have been
effectively extensively tuned in the literature over a decade of experimentation, good
hyperparameter settings for these alternative algorithms have barely been explored at
all, and their empirical limits of performance at scale have largely yet to be determined.
A ﬁnal serious difﬁculty with such approaches is that to match backprop they require
some level of nonlocality in time, where information from the feedforward pass is
stored and then utilized throughout or at the end of the backwards pass. This storage of
Chapter 7. Discussion 355
information is fundamentally necessary because the backprop gradients depend only
on the state of the network in the feedforward pass. If the state changes due to the
iterative algorithm, then the old information from the forward pass must be stored
somehow to maintain convergence to backprop on the forward pass. This manifests
itself in predictive coding as the ﬁxed-prediction assumption, which explicitly assumes
that the values of the forward pass are stored. In the AR algorithm, it manifests in the
necessity to store the original value of the activity neurons to use to update the weights
after the backwards pass is complete. Due to this storage, the actual update equations
become nonlocal in time. This shortcoming could potentially be addressed in two ways.
Firstly, it might be possible to store the information from the forward pass, for instance
in local recurrent units which are insulated from the activity changes in the dynamical
iterations, or secondly, if the number of iterations is short enough, such information
could be persisted simply through multi-step recurrent connectivity. Nevertheless, even
if this could be done, the circuitry to align and ensure the correct time of arrival of all
the necessary signals could be quite complex.
In this ﬁeld, it is important to reﬂect deeply upon the role and utility of simpliﬁed
models of neural dynamics. Almost all work in this area operates with very simple
models of rate-coded integrate and ﬁre neurons, typically often in a temporally static
‘instantaneous’ computation graph. Biological plausibility within this model, while a
somewhat vague concept, is often deﬁned by conditions such as only local connectivity,
or that connectivity must be additionally Hebbian, and that neural connectivity cannot
be too precise, and that information cannot be directly transmitted backwards along
axons. However, this deﬁnition necessarily ignores some important aspects of reality.
Obviously the brain uses spiking neurons and must also achieve credit assignment
through time, but additionally there are even questions about the simpliﬁcation of
neural architecture. For instance, typically such models implicitly assume a multilayer
perceptron (MLP) style architecture as just a stack of fully connected layers, however
each region of the cortex has an intricate 6-layer structure, and it is not clear whether
Chapter 7. Discussion 356
each such layer in the cortex should be considered as a single layer of the MLP, or only
the cortical region. If the former, then the different properties and neurophysiology
of each cortical layer is not modelled. If the latter, then it is not at all clear to what
extent the entire cortical region can be modelled as a simple fully connected layer. An
additional interesting neurophysiological fact which is rarely explicitly modelled is the
predominance of cortical columns in the visual cortical regions. These columns do
not at all correspond to fully connected layers and it is not yet fully clear what their
computational role is. A straightforward hypothesis is that they are the brain’s way of
implementing a local receptive ﬁeld operation, like convolution in convolutional neural
networks, but without the advantage of shared weights across space which is core to the
generalization capabilities of the CNN (Hawkins & Blakeslee, 2007).
The question remains, however, how applicable are such simpliﬁed rate-coded models
to the full complexity of credit assignment in spiking networks through time. The hope,
ideally, is that by and by large important computational primitives and algorithms which
have been developed for biologically plausible credit assignment in rate-coded neural
networks remain functional, or only require minor adaptation, to work in a spiking
context. This is a strong possibility, but it could also be completely false, and that the
brain implements an algorithm which relies heavily on the unique properties of spiking
networks for its credit assignment capabilities. Ultimately, before we fully understand
the mechanisms of credit assignment in the brain, it will be hard to fully assess the
degree to which work on rate-coded models will generalize. There has been some
preliminary successes though. For instance, the surrogate gradient technique shows that
with only minor tweaking (deﬁning a surrogate gradient to avoid the nondifferentiable
threshold spiking function), backprop through time (BPTT) can be straightforwardly
used to train spiking neural networks for complex tasks (Neftci et al., 2019; Zenke &
Ganguli, 2018). However, these methods currently utilize the biologically implausible
BPTT algorithm and it is unclear to what extent biologically plausible alternatives to
BPTT can be straightforwardly applied in such a manner. To answer such a research
Chapter 7. Discussion 357
question would be an important and timely research agenda which would substantially
advance our understanding of the generalizability of such models.
Fundamentally, the core challenge is that of time. Most models (with some exceptions
(Bellec et al., 2020; Schiess et al., 2016)) focus only on backpropagation and credit
assignment through space (i.e. layers of a neural network architecture) and not through
time. However, time is an inextricable component of the computation in the brain.
Fundamentally, perception in the brain is not about handling static i.i.d datasets, but
rather ﬁltering on constantly changing sensory streams of information. Moreover, credit
assignment through time is in some sense a substantially harder problem, in that the key
information necessary at each step disappears, while when backproapgating through
space the information is always present somewhere in the graph. In the case of BPTT, all
the intermediate activations at each timestep are stored and then replayed in backwards
sequence once the sequence has ended. However, such an acausal solution is clearly
not suitable for computation in the brain. There are alternatives to BPTT which do not
require explicitly repeating computations backwards through time, but allow for online
integration of gradients. The key such algorithm is the RTRL algorithm which maintains
a Jacobian of values at each timestep and iteratively updates them at every timestep. In
algorithmic terms, RTRL corresponds to forward-model automatic differentiation while
BPTT corresponds to reverse-mode. Unfortunately, however, RTRL is substantially
more expensive to implement on digital computers, scaling with O(n4) where N is the
number of parameters, rather than O(n2T ) for BPTT, where T is the time horizon. One
key advantage of RTRL however is that unlike BPTT it is not bound by sequence length.
While BPTT must choose some point to stop and then backpropagate all the gradients
and then truncate gradients from after time T, RTRL can operate on sequences with
indeﬁnite length without issues, although credit is slowly diluted away through time.
However, RTRL is also likely not neurally plausible and it is not clear how to implement
RTRL in systems with multiple recurrent layers interacting without a blowup in the
number of learning rules required. My personal hunch is that extending current models
Chapter 7. Discussion 358
of local, biologically plausible credit assignment to spiking neural networks will not
be especially challenging, although they might not be the method that the brain uses.
However, I believe that credit assignment through time is a fundamentally different and
more challenging problem than credit assignment through space, and that ultimately, to
solve the problem of credit assignment in the brain we must grapple head on with the
problem of local credit assignment through time.
There are already some methods existing in the literature which accomplish this, specif-
ically eligibility-prop (Bellec et al., 2020) proposes eligibility traces to compute the
RTRL algorithm in a local fashion. However the method currently only works for a
single recurrent layer while the brain is deep both in time and in space, and that this
depth requires new algorithms, or the combination of existing algorithms for backprop
in space with those for backprop in time. It is also unclear to what extent the brain com-
putes the full credit assignment mandated by RTRL or else some sparse approximation
therein, perhaps aided by the natural sparsity properties of spiking neural networks.
An additional consideration when thinking about credit assignment in the brain is
the question of feedforward vs feedback processing (Kriegeskorte, 2015). While the
BPTT is only designed for backpropagating through immediately recurrent feedforward
neural networks, the brain actually contains a multitude of long-range recurrent loops
mediated by top-down feedback connectivity (Felleman & Van Essen, 1991; Grill-
Spector & Malach, 2004). Understanding the properties of these and whether they are
used for credit assignment, or whether the brain computes credit backwards through
these top-down connections is also crucial for understanding the full picture of credit
assignment in the brain. An additional, and almost entirely unanswered question is
the role of long term memory in credit assignment. Current methods, including BPTT,
typically use some temporal cut-off to stop considering gradient information beyond
some time-horizon, and a key ﬂaw in naive recurrent architectures is that information is
slowly lost over time (Hochreiter & Schmidhuber, 1997; Ollivier et al., 2015). The key
Chapter 7. Discussion 359
innovation in LSTM units is that they contain an explicit forget/remember gate which
allows for memories to be stored potentially indeﬁnitely (Hochreiter & Schmidhuber,
1997). There are additionally various modiﬁcations for recurrent RNNs which help
ameliorate this problem as well (Ollivier et al., 2015). Since (we assume) the brain is
equipped with a fairly standard recurrent architecture, the architectural or learning-rule
modiﬁcations that enable it to avoid this problem and to successfully store and utilize
even long term credit assignment remains to be explored and is a very important and
fundamental question in understanding credit assignment and learning in the brain. One
hypothesis is that synapses in the brain may store a temporal hierarchy of eligibility
traces which retain information over progressively longer timescales, thus allowing
them to act on information which has been accumulated even in the relatively distant
past. However, ﬁguring out the actual speciﬁc operation of such a system remains an
open research challenge.
A further interesting question, raised by the recent successes of instantaneous feedfor-
ward architectures such as transformers in processing sequential data such as natural
language text over recurrent architectures such as LSTMs is to what extent recurrence
is actually a useful computational primitive for handling sequence data as opposed
to one the brain may be forced into due to its inherent computational constraints and
need for online processing instead of batch processing at the end of a sequence, as in
transformers. A key advantage of attention, as implemented in transformers (Vaswani
et al., 2017), is that it enables arbitrary time-to-time modulation, rather than in recurrent
architectures where the immediate past inputs are combined with the present ones to
predict the future. In this way, transformers can handle data in a fundamentally acausal
manner, with accompanying computational advantages. It remains to be seen whether
attention like mechanisms can be implemented in a recurrent way, whether recurrent
architectures can be successfully scaled to the level that transformers have done, or
whether they remain on an inferior scaling curve (J. Kaplan et al., 2020), and the precise
mechanism by which similar sequence computations are implemented in the brain. The
Chapter 7. Discussion 360
key lesson of attention is that recurrence is not the only way to handle intrinsically
sequential inputs. It remains to be seen whether other non-recurrent mechanisms are
implemented in the brain.
Importantly, although this line of work has focused on determining whether the back-
propagation of error algorithm can be implemented in the brain, which is inspired by
the impressive success of modern machine learning, which is based on this algorithm, it
is not entirely certain that the brain achieves credit assignment through backpropaga-
tion at all. There are several alternative methods which are worth discussing in some
detail. For instance, it is also possible that the brain may be implementing some more
advanced kind of learning algorithm than stochastic gradient descent. Meulemans et al.
(2020) showed that target propagation implements an approximate version of a second
order gradient descent scheme – known as Gauss-Newton optimization. Similarly there
are other optimization methods, such as conjugate gradients, or coordinate ascent, or
ﬁxed-point iteration, as well as a variety of probabilistic message passing schemes
which do not require explicit gradients to be computed (Parr et al., 2019; Yedidia, 2011).
Moreover, it is possible that the brain, while needing to compute gradients, does not
compute by the backpropagation of error algorithm. For instance, it is possible to com-
pute gradients by ﬁnite differences, and although these ﬁnite differences perform strictly
worse than automatic differentiation techniques computationally, they are very simple to
implement in practice. For instance, the brain could easily contain circuits which could
compute time derivatives of a constantly varying temporal stream. Then, once we have
the time derivatives of two variables, it is possible to directly compute their gradient by
dx
dy = dx
dt /dy
dt . Such a circuit would only need local temporal ﬁnite differences as well
as a divisive feedback connection to combine the two time derivatives, well within the
possibilities afforded by known neurophysiological constraints. A further option would
be that the brain simply may not compute with derivatives at all, all the while optimizing
some objective function. There are many ‘black box’ optimization methods which do
not require gradients of the objective. For instance, genetic algorithms (Salimans, Ho,
Chapter 7. Discussion 361
Chen, Sidor, & Sutskever, 2017), or other brute-force-esque algorithms may instead
be implemented, and indeed it has been shown that in some cases genetic algorithms,
when sufﬁciently scaled are able to compete with backprop on some optimization tasks
(Salimans et al., 2017; Such et al., 2017). Another possibility, is that the brain could
learn solely by global rewards broadcast to all neurons, where learning effectively
takes place via the policy gradient theorem (Roelfsema & Ooyen, 2005). There is
some circumstantial neurophysiological evidence in favour of this – speciﬁcally the
well known role of dopamine as a spur to synaptic plasticity (Dayan, 2009; Dayan
& Daw, 2008), as well as the fact that many cortical pyramidal cells receive global
dopaminergic inputs from subcortical regions. There have also been a variety of models
(Pozzi, Bohté, & Roelfsema, 2018; Roelfsema & Ooyen, 2005) proposed of this kind
of global reward-driven learning. However, this type of learning suffers from a severe
intrinsic ﬂaw that the gradients it estimates for each parameter have extremely high
variance. This is because each neuron is only provided with a global reward signal,
which is ultimately caused by the interaction of an extremely large number of other
neurons. Thus, averaging out all the noise introduced by all the other neurons in the
brain requires a very large statistical sample, thus effectively leading to extremely high
variance gradients and slow learning, which scales increasingly poorly with network
size. This is precisely why backprop is such a useful algorithm, since it provides precise
feedback to each neuron about the loss, thus meaning that the only source of noise
is minibatch noise rather than intrinsic noise due to the activities of other neurons.
This means that backprop computed gradients have much lower variance and can lead
to much faster learning. Nevertheless, it remains inarguable that pyramidal cells are
generally innervated by dopaminergic inputs and that dopamine, which is released when
there is a reward prediction error in the basal ganglia (Dayan, 2009; Schultz, 1998;
Schultz, Tremblay, & Hollerman, 1998) can strongly modulate learning. This may
imply that there are effectively two learning systems on top of one another in the brain.
The ﬁrst, potentially older, is the global slow dopaminergic system, while the second,
Chapter 7. Discussion 362
entirely cortically based uses precise vector-feedback with some backpropagation like
algorithm. The global reward signals, then can potentially modulate various aspects of
learning so that contingencies with high reward prediction error are especially salient
and may induce larger changes than those without (Daw et al., 2006; Roelfsema &
Ooyen, 2005).
Finally, it is always possible that we have been misled by the contemporary successes of
machine learning, and that the core formulation where we perceive the brain implicitly
or explicitly optimizing some objective is simply wrong, and that the brain cannot be
described in such a way at all. While this is a very general formulation of perception
and learning, with support from both machine learning and statistics and control theory
in engineering, it nevertheless could not be a productive framework in which to think
about the function of the brain. Such a scenario would arise, for instance, if the brain
was merely a grab-bag of various heuristics and reﬂexes, implicitly tuned over the
course of evolution, without any serious potential for learning. While this may be true
of the brains of some simple animals, it is clearly not for humans and other creatures
with complex, learnt cognition. Nevertheless, if this is somehow the case, the question
will then become what is a productive mathematical framework in which to think about
what the brain is doing, if not in the language of probabilistic models and objective
functions. It may be that the answer to this question, if it must be posed, is more
interesting and productive than discovering that the brain was simply doing backprop
all along. Nevertheless, it is extremely unclear at present which of these possibilities
is true. My opinion, given the mathematical elegance and generality, as well as the
empirical successes of the objective function viewpoint, is that it is a valuable and most
likely correct framework for understanding the operation of the brain. However, it is
always worth noting that this is purely a speculative hunch, and there remains little
non-circumstantial evidence one way or another.
Chapter 7. Discussion 363
7.4 Closing Thoughts
This thesis is titled ‘Applications of the Free Energy Principle for Machine Learning and
Neuroscience’, and throughout the thesis we have endeavoured to demonstrate how to
adapt and extend methods from the free energy principle and its primary process theories
– active inference and predictive coding – to make advances in deep reinforcement
learning, and in neuroscientiﬁc theories of perception and credit assignment in the
brain. Interestingly, the pattern of progress in this thesis is that, in parallel, between
the neuroscience and the machine learning, is that ﬁrst we simply try to adapt and
extend the methods of the free energy principle to test their capabilities against other
existing methods from the core literatures of these subjects, and then strive to show how
process theories like active inference and predictive coding can extend and advance upon
the current state of the art. Then, this process inevitably reveals new and interesting
questions by itself, which are somewhat separate from the free energy principle and
its process theories. These questions are ﬁrstly the mathematical origin of exploration
(which emerges from considering the nature of the expected free energy objective in
active inference), and secondly credit assignment in the brain, which emerges from
considering how predictive coding relates to backpropgation of error. In the second
set of chapters (5 and 6) we have then tried to address these further questions, using
methods inspired by the free energy principle, but not necessarily directly deriving
from it. We believe that in many ways this reﬂects the theoretical fertility and utility
of the FEP as an abstract principle, that it allows the postulation and exploration of
deeper questions than would be possible without it. Indeed, this theoretical fertility may
be one of the primary means of judging the utility of the FEP since, as we discussed
in Chapter 2, the FEP is technically non-falsiﬁable and can be considered more of a
mathematical principle, or perspective, rather than a theory. If this is the case, then the
work in this thesis provides support to the contention that the FEP provides a useful and
fruitful perspective to understanding a variety of questions both machine learning and
neuroscience.
Chapter 7. Discussion 364
The phenomenological process of intellectual understanding is an interesting one. At
ﬁrst you appear to spend ages groping around in the dark, hitting various unknown
objects in your way, but slowly gathering a picture of the obstacles in your path. Then,
at long last, you eventually stumble your way to a light-switch and the whole vista is
revealed. What were once unknown lurking obstacles are transformed into precisely
speciﬁed, and tractable, objects, whose relation to one another can be seen at ﬁrst glance.
In the light, everything is at once clearer, but also smaller. Questions and dilemmas
which seems huge and irreducibly complex are revealed to be straightforward, even
trivial, in the light of understanding. So much so that it is often easy to forget, looking
back, how these questions appeared when the answer was unknown. From a personal
perspective, and one I have hoped to share in this thesis, I believe I have managed
to obtain and present a clear understanding of two topics which were not clear in the
literature before my PhD – whether active inference can be successfully scaled up to
compete with modern reinforcement learning methods (and the relationships between
the two theories), and the mathematical origins of the exploration term in the expected
free energy, and its relationship to more standard functionals such as the variational free
energy for perception. I have additionally contributed to the theory and implementation
of predictive coding models, as well as algorithms for biologically plausible backprop in
the brain. I feel, however, that while I have uncovered certain key aspects and facets of
the problem of credit assignment in the brain, which were previously shrouded, I have
not yet reached the point where all mystery falls away and the light pours in. Similarly,
I hope that the work in this thesis has helped you, the reader, understand some things at
least a little more clearly.
To conclude, we offer some ideas of for the future development of the ideas worked
out in this thesis. We believe it is clear, we have shown, that active inference can be
productively related and merged with the large and extremely powerful set of deep
reinforcement learning agents to enable general and ﬂexible learning based algorithms
which can succeed in challenging tasks, even those requiring a considerably degree
Chapter 7. Discussion 365
of exploration. Further progress in this ﬁeld, from the perspective of active inference,
should move beyond merely scaling up, and instead focus on the unique insights and
ideas that active inference brings to the table. Examples of this include its distinct
and exploratory objective functionals such as the Expected Free Energy, or the free
energy of the Expected Future. Another potentially interesting avenue lies in active
inference’s more ﬂexible consideration of reward as a speciﬁc priordistribution, rather
than a scalar value. This could enable more ﬂexible behaviour through better reward
speicﬁcation, and include the ability to learn ﬂexible reward distributions on the ﬂy,
effectively performing reward shaping in a semi-autonomous manner. A ﬁnal avenue for
exploration is the reﬁnement of the speciﬁc generative models used in control agents. A
key tenet of active inference is the idea of deriving powerful behaviours from inference
on detailed and ﬂexible generative models of the dynamics of the world. While in
this thesis, we have focused primarily on simply approximating and parametrizing the
distributions in the generative model with deep neural networks, a more reﬁned approach
might be to investigate whether the world can be factorized in a tractable manner, and
design generative models to explicitly exploit these factorizations to allow for more
tractable and efﬁcient inference – effectively giving the agent just the right inductive
biases about the world to subserve its control objectives. Secondly, for the question of
credit assignment in the brain, I believe that the key advances will come in understanding
the temporal component of credit assignment, and this requires deeply understanding
the fundamental computations of the brain as performing dynamical algorithms such
as ﬁltering and smoothing rather than merely static Bayesian inference. This line of
research would focus attention both on how the brain can achieve backpropagation
through time as well as through space. There are also extremely interesting links here
with predictive coding, where dynamical approaches using generalized coordinates
remain underexplored, and it may be that by further developing the explicitly Bayesian
approaches to ﬁltering provided by the dynamical formulations of predictive coding, we
can better characterise and understand the temporal nature of the brain’s computations.
Chapter 7. Discussion 366
The work in this thesis relating predictive coding and Kalman ﬁltering is only the
beginning, there needs to be much work done scaling and precisely characterising the
performance of dynamical predictive coding algorithms, understanding whether such
recurrent predictive coding networks can be utilized to perform some form of temporal
credit assignment, as static predictive coding networks can.
Appendix A
Derivation of Kalman Filtering
Equations from Bayes’ Rule
In this appendix we derive the Kalman ﬁltering equations directly from Bayes rule. The
ﬁrst step is to derive the projected covariance,
E[ˆxt+1 ˆxT
t+1] =E[(Ax +Bu +ω)(Ax +Bu +ω)T ] (A.1)
= E[AxxT AT ]+ E[AxuT BT ]+ E[AxωT ]+ E[BuxT AT ]+ E[BuωT ]+ E[ωT
x AT ]
(A.2)
+E[ωuT BT ]+ E[ωωT ] (A.3)
= AE[xxT ]AT +E[ωωT ] (A.4)
= AΣx(t)AT +Σω (A.5)
Step 11 uses the fact that matrices A,B are constant so come out of the expectation
operator, and that it is assumed that covariances between the state, the noise, and the
control – E[xuT ],E[xωT ], E[uωT ] – are 0. Step 13 uses the fact that E[xxT ] =Σx(t) and
that E[ωωT ] =Σω.
Next we optimize the following loss function, derived from Bayes’ rule above (equation
367
Appendix A. Derivation of Kalman Filtering Equations from Bayes’ Rule 368
5).
L = −(y −Cµt+1)T ΣZ(y −Cµt+1)+( µt+1 −Aµt −But)T ˆΣx(µt+1 −Aµt −But)
(A.6)
To obtain the Kalman estimate for µt+1 we simply take derivatives of the loss, set it to
zero and solve analytically.
0 = dL
dµt+1
[µT
t+1[CT RC+Σx]µt+1 −µT
t+1[CT Ry −ΣxAµt −ΣxBut] (A.7)
−[yT R −µT
t AT Σx −uT
t BT Σx]µt+1 (A.8)
= 2[CT RC+Σx]µt+1 −2[CT Ry +Σx(Aµt +But)] (A.9)
µt+1 = [CT RC+Σ−1
x [CT Ry +Σx(Aµt +But] (A.10)
= [Σ−1
x −Σ−1
x CT [CΣxCT +R]−1CΣ−1
x [CT Ry +Σx(Aµt +But)] (A.11)
= [Σ−1
x −KCΣ−1
x ][CT Ry +Σx(Aµt +But)] (A.12)
= Aµt +But +Σ−1
x CT Ry −KCΣ−1
x CT Ry −KC(Aµt +But) (A.13)
= ˆµt+1 −KC ˆµt+1 +[Σ−1
x CT R −KCΣ−1
x CT R]y (A.14)
= ˆµt+1 −KC ˆµt+1 +KK−1[Σ−1
x CT R −KCΣ−1
x CT R]y (A.15)
= ˆµt+1 −KC ˆµt+1 +K[(CΣxCT +R)C−T Σx[Σ−1
x CT R]−CΣxCT R]y (A.16)
= ˆµt+1 −KC ˆµt+1 +K[(CΣxCT +R−1)R −CΣxCT R]y (A.17)
= ˆµt+1 −KC ˆµt+1 +Ky (A.18)
= ˆµt+1 +K[y −C ˆµt+1] (A.19)
Where K = Σ−1
x CT [CΣxCT +R]−1 and is the Kalman gain and ˆµt+1 = Aµt +But is the
projected mean.
The ﬁrst few steps rearrange the loss function into a convenient form and then derive an
expression for µt+1 directly. Step 22 applies the Woodbury matrix inversion lemma to
the [CT RC+Σx]−1 term. The next step rewrites the formula in terms of the Kalman gain
matrix K and multiplies it through. The other major manipulation is the multiplication
of the last term of equation 23 by KK−1 which is valid since KK−1 = I.
Appendix A. Derivation of Kalman Filtering Equations from Bayes’ Rule 369
This derives the optimal posterior mean as the analytical solution to the optimization
problem. Deriving the optimal covariance is straightforward and done as follows,
E[µt+1µT
t+1] =E[( ˆmut+1 +Ky −KC ˆmut+1)( ˆµt+1 +Ky −KC ˆµt+1)T ] (A.20)
= E[ ˆµt+1 ˆµt+1T ]−E[ ˆµt+1 ˆµt+1T ]CT KT −KCE[ ˆµt+1 ˆµt+1T ] (A.21)
+KE[yyT ]KT +KCE[ ˆµt+1 ˆµt+1T ]CT KT (A.22)
= Σ ˆµt+1 −Σ ˆµt+1CT KT −KCΣ ˆµt+1 +K[R +CΣ ˆµt+1CT ]KT (A.23)
= Σ ˆµt+1 −Σ ˆµt+1CT KT −KCΣ ˆµt+1 +Σ ˆµt+1CT [CΣ ˆµt+1CT +R]−1[R +CΣ ˆµt+1CT ]KT
(A.24)
= Σ ˆµt+1 −Σ ˆµt+1CT KT −KCΣ ˆµt+1 +Σ ˆµt+1CT KT (A.25)
= Σ ˆµt+1 −KCΣ ˆµt+1 (A.26)
= [I −KC]Σ ˆµt+1 (A.27)
Which is the Kalman update equation for the optimal variance. The second line follows
on the assumption that E[xyT ] =0. On equation 32 the deﬁnition of the Kalman gain is
substituted back in and the two CΣxCT +R terms cancel.
Appendix B
Appendix B: Equations of the LSTM
cell
The equations that specify the computation graph of the LSTM cell are as follows.
v1 = ht ⊕xt
v2 = σ(θiv1)
v3 = ctv2
v4 = σ(θinp v1)
v5 = tanh(θcv1)
v6 = v4v5
v7 = v3 +v6
v8 = σ(θov1)
v9 = tanh(v7)
v10 = v8v9
y = σ(θyv10)
The recipe to convert this computation graph into a predictive coding algorithm is
370
Appendix B. Appendix B: Equations of the LSTM cell 371
straightforward. We ﬁrst rewire the connectivity so that the predictions are set to the
forward functions of their parents. We then compute the errors between the vertices and
the predictions.
ˆv1 = ht ⊕xt
ˆv2 = σ(θiv1)
ˆv3 = ctv2
ˆv4 = σ(θinp v1)
ˆv5 = tanh(θcv1)
ˆv6 = v4v5
ˆv7 = v3 +v6
ˆv = σ(θov1)
ˆv9 = tanh(v7)
ˆv10 = v8v9
ˆvy = σ(θyv10)
ε1 = v1 −ˆv1
ε2 = v2 −ˆv2
ε3 = v3 −ˆv3
ε4 = v4 −ˆv4
ε5 = v5 −ˆv5
ε6 = v6 −ˆv6
ε7 = v7 −ˆv7
ε8 = v8 −ˆv8
ε9 = v9 −ˆv9
ε10 = v10 −ˆv10
Appendix C
Appendix C: Predictive Coding Under
the Laplace Approximation
In the main derivation in Chapter 3 of the variational free energy F , we used the
assumption that the variational density is a dirac delta function: q(x|o;µ) =δ(x −µ).
However, the majority of derivations, including the original derivations in (Friston,
2005) instead applied the Laplace approximation to the variational distribution q. This
approximation deﬁnes q to be a Gaussian distribution with a variance which is a function
of the mean µ: q(x|o;µ) =N (x;µ,σ(µ)). Notationally, it is important to distinguish
between the generative model Σ, and the variational distribution σ. Here we use the
lower-case σ to denote the parameter of the variational distribution. The lower-case
is not meant to imply it is necessarily scalar. As we shall see, the optimal σ will be
become the inverse-Hessian of the free energy at the mode.
Intuitively, this is because the curvature at the mode of a Gaussian distribution gives a
good indication of the variance of the Gaussian, since a Gaussian with high curvature at
the mode (i.e. the mean) will be highly peaked and thus have a small variance, while a
Gaussian with low curvature will be broad, and thus have a large variance. While our
derivation using a dirac-delta approximation and the standard derivation using a Laplace
372
Appendix C. Appendix C: Predictive Coding Under the Laplace Approximation373
approximation obviously differ, they ultimately arrive at the same expression for the
variational free energy F . This is because both approximations effectively remove the
variational variances from consideration and only use the variational mean in practice.
Under the Laplace approximation, the variational coveriance has an analytical optimal
form and thus does not need to be optimized, and plays no real role in the optimization
process for the µs either. We chose to present our derivation using dirac-deltas in the
interests of simplicity, since as we shall see the Laplace approximation derivation is
somewhat more involved.
To begin, we return to the standard energy function in multilayer case, this time under
the assuption the Laplace approximation.
F =
L
∑
i=
Eq(x|o;µ)
[
ln p(xi|xi+1)
]
  
Energy
+Eq(x|o;µ)
[
lnq(x|o;µ)
]
  
Entropy
=
L
∑
i=
EN (x;µ,σ(µ))
[
lnN (xi; f (µi+1,Σ(µ)))
]
−ln2πσi (C.1)
Where we have used the analytical result that the entropy of a gaussian distribution
H[N ] =ln2πσ. Then, we apply a Taylor expansion around xi = µi to each element in
the sum,
F ∝
L
∑
i=
Eq
[
ln p(µi|µi+1)
]
+Eq
[∂ln p(xi|xi +1)
∂xi
(xi −µi)
]
+Eq
[∂2 ln p(xi|xi +1)
∂x2
i
(xi −µi)2]
−ln2πσi
=
L
∑
i=0
ln p(µi|µi+1)+ ∂2 ln p(xi|xi +1)
∂x2
i
σi −ln2πσi (C.2)
Where in the second line, we have used the fact thatEq
[
xi −µi
]
= Eq
[
xi]−µi = µi −µi =
0 and that Eq
[
(xi −µi)2] =Σi, which is that the expected squared residual simply is the
variance. We also drop the expectation around the ﬁrst term, since as a function only of
µ and µi+1, it is no longer a function of xi which is the variable the expectation is under.
We can then differentiate this expression with respect to σi and solve for 0 to obtain the
Appendix C. Appendix C: Predictive Coding Under the Laplace Approximation374
optimal variance.
∂F
∂σi
= ∂2 ln p(xi|xi +1)
∂x2
i
−σ−1
i
∂F
∂σi
= 0 =⇒σi = ∂2 ln p(xi|xi +1)
∂x2
i
−1
(C.3)
Given this analytical result, there is no point optimizingF with respect to the variational
variances σi, so our objective simply becomes,
F =
L
∑
i=0
ln p(µi|µi+1) (C.4)
which is exactly the same result as obtained through the dirac delta approximation.
References
Abdolmaleki, A., Springenberg, J. T., Tassa, Y ., Munos, R., Heess, N., & Ried-
miller, M. (2018). Maximum a posteriori policy optimisation. arXiv preprint
arXiv:1806.06920.
Adams, R. A., Perrinet, L. U., & Friston, K. (2012). Smooth pursuit and visual
occlusion: active inference and oculomotor control in schizophrenia. PloS one,
7(10), e47502.
Aitchison, L., & Lengyel, M. (2017). With or without you: predictive coding and
bayesian inference in the brain. Current Opinion in Neurobiology, 46, 219–227.
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., & Tweed, D. B. (2019). Deep
learning without weight transport. In Advances in neural information processing
systems (pp. 974–982).
Amari, S.-I. (1995). Information geometry of the em and em algorithms for neural
networks. Neural networks, 8(9), 1379–1408.
Amit, Y . (2019). Deep learning with asymmetric connections and hebbian updates.
Frontiers in computational neuroscience, 13, 18.
Andrews, M. (2020). The math is not the territory: Navigating the free energy principle.
Arulampalam, M. S., Maskell, S., Gordon, N., & Clapp, T. (2002). A tutorial on particle
ﬁlters for online nonlinear/non-gaussian bayesian tracking. IEEE Transactions
on Signal Processing, 50(2), 174–188.
Attias, H. (2003). Planning by probabilistic inference. In Aistats.
Auksztulewicz, R., & Friston, K. (2016). Repetition suppression and its contextual
375
References 376
determinants in predictive coding. cortex, 80, 125–140.
Baird, L. (1995). Residual algorithms: Reinforcement learning with function approxi-
mation. In Machine learning proceedings 1995 (pp. 30–37). Elsevier.
Baldi, P., & Sadowski, P. (2016). A theory of local learning, the learning channel, and
the optimality of backpropagation. Neural Networks, 83, 51–74.
Baltieri, M., & Buckley, C. L. (2017). An active inference implementation of phototaxis.
In Artiﬁcial life conference proceedings 14 (pp. 36–43).
Baltieri, M., & Buckley, C. L. (2018). The modularity of action and perception revisited
using control theory and active inference. In Artiﬁcial life conference proceedings
(pp. 121–128).
Baltieri, M., & Buckley, C. L. (2019). Pid control as a process of active inference
with linear generative models. Entropy, 21(3), 257. Retrieved from https://
www.mdpi.com/1099-4300/21/3/257
Baltieri, M., & Buckley, C. L. (2020). On kalman-bucy ﬁlters, linear quadratic
control and active inference. arXiv preprint arXiv:2005.06269. Retrieved from
https://arxiv.org/abs/2005.06269
Baltieri, M., Buckley, C. L., & Bruineberg, J. (2020). Predictions in the eye
of the beholder: an active inference account of watt governors. In Artiﬁ-
cial life conference proceedings (pp. 121–129). Retrieved from https://
www.mitpressjournals.org/doi/abs/10.1162/isal_a_00288
Barlow, H. B., et al. (1961). Possible principles underlying the transformation of
sensory messages. Sensory communication, 1, 217–234.
Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G., & Lillicrap, T. (2018).
Assessing the scalability of biologically-motivated deep learning algorithms
and architectures. In Advances in neural information processing systems (pp.
9368–9378).
Bastos, A. M., Lundqvist, M., Waite, A. S., Kopell, N., & Miller, E. K. (2020).
Layer and rhythm speciﬁcity for predictive routing. Proceedings of the National
References 377
Academy of Sciences, 117(49), 31459–31469.
Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K.
(2012). Canonical microcircuits for predictive coding. Neuron, 76(4), 695–711.
Bastos, A. M., Vezoli, J., Bosman, C. A., Schoffelen, J.-M., Oostenveld, R., Dowdall,
J. R., . . . Fries, P. (2015). Visual areas exert feedforward and feedback inﬂuences
through distinct frequency channels. Neuron, 85(2), 390–401.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic
differentiation in machine learning: a survey. The Journal of Machine Learning
Research, 18(1), 5595–5637.
Beal, M. J. (2003). Variational algorithms for approximate bayesian inference(Unpub-
lished doctoral dissertation). UCL (University College London).
Bear, M., Connors, B., & Paradiso, M. A. (2020). Neuroscience: Exploring the brain.
Jones & Bartlett Learning, LLC.
Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass,
W. (2020). A solution to the learning dilemma for recurrent networks of spiking
neurons. bioRxiv, 738385.
Bellman, R. (1952). On the theory of dynamic programming. Proceedings of the
National Academy of Sciences of the United States of America, 38(8), 716.
Bengio, Y . (2020). Deriving differential target propagation from iterating approximate
inverses. arXiv preprint arXiv:2007.15139.
Bengio, Y ., & Fischer, A. (2015). Early inference in energy-based models approximates
back-propagation. arXiv preprint arXiv:1510.02777.
Bengio, Y ., Mesnard, T., Fischer, A., Zhang, S., & Wu, Y . (2017). Stdp-compatible ap-
proximation of backpropagation in an energy-based model. Neural Computation,
29(3), 555–577.
Berger-Tal, O., Nathan, J., Meron, E., & Saltz, D. (2014). The exploration-exploitation
dilemma: a multidisciplinary framework. PloS One, 9(4), e95693.
Betancourt, M. (2017). A conceptual introduction to hamiltonian monte carlo. arXiv
References 378
preprint arXiv:1701.02434.
Betancourt, M. J. (2013). Generalizing the no-u-turn sampler to riemannian manifolds.
arXiv preprint arXiv:1304.1920.
Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A
review for statisticians. Journal of the American statistical Association, 112(518),
859–877.
Bostrom, N. (2017). Superintelligence. Dunod.
Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., &
Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.
Brooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of markov chain
monte carlo. CRC press.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . .
others (2020). Language models are few-shot learners. arXiv preprint
arXiv:2005.14165.
Bruineberg, J., Dolega, K., Dewhurst, J., & Baltieri, M. (2020). The emperor’s new
markov blankets.
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy princi-
ple for action and perception: A mathematical review. Journal of Mathematical
Psychology, 81, 55–79.
Buzsaki, G. (2006). Rhythms of the brain. Oxford University Press.
Çatal, O., Nauta, J., Verbelen, T., Simoens, P., & Dhoedt, B. (2019). Bayesian policy
selection using active inference. arXiv preprint arXiv:1904.08149.
Çatal, O., Verbelen, T., Nauta, J., De Boom, C., & Dhoedt, B. (2020). Learning percep-
tion and planning with deep active inference. arXiv preprint arXiv:2001.11841.
Caticha, A. (2015). The basics of information geometry. In Aip conference proceedings
(V ol. 1641, pp. 15–26).
Cesa-Bianchi, N., Gentile, C., Lugosi, G., & Neu, G. (2017). Boltzmann exploration
done right. In Advances in neural information processing systems (pp. 6284–
References 379
6293).
Che, T., Lu, Y ., Tucker, G., Bhupatiraju, S., Gu, S., Levine, S., & Bengio, Y . (2018).
Combining model-based and model-free rl via multi-step control variates.
Chen, T., Fox, E., & Guestrin, C. (2014). Stochastic gradient hamiltonian monte carlo.
In International conference on machine learning (pp. 1683–1691).
Child, R. (2020). Very deep vaes generalize autoregressive models and can outperform
them on images. arXiv preprint arXiv:2011.10650.
Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances
in neural information processing systems (pp. 4754–4765).
Clark, A. (2013a). Whatever next? predictive brains, situated agents, and
the future of cognitive science. Behavioral and brain sciences , 36(3),
181–204. Retrieved from https://www.cambridge.org/core/journals/
behavioral-and-brain-sciences/article/whatever-next-predictive
-brains-situated-agents-and-the-future-of-cognitive-science/
33542C736E17E3D1D44E8D03BE5F4CD9
Clark, A. (2013b). Whatever next? predictive brains, situated agents, and the future of
cognitive science. , 36(3), 181–204. doi: 10.1017/S0140525X12000477
Clark, A. (2015). Surﬁng uncertainty: Prediction, action, and the em-
bodied mind . Oxford University Press. Retrieved from https://
books.google.co.uk/books?hl=en&lr=&id=TnqECgAAQBAJ&oi=fnd&pg=
PP1&dq=andy+clark+surfing+uncertainty&ots=aurm4jE3NO&sig=
KxeHGJ6YJJdN9tKyr6snwDyBBKg&redir_esc=y#v=onepage&q=andy%
20clark%20surfing%20uncertainty&f=false
Cohen, J. D., McClure, S. M., & Yu, A. J. (2007). Should i stay or should i go? how the
human brain manages the trade-off between exploitation and exploration. Philo-
sophical Transactions of the Royal Society B: Biological Sciences, 362(1481),
933–942.
References 380
Conant, R. C., & Ross Ashby, W. (1970). Every good regulator of a system must be a
model of that system. International journal of systems science, 1(2), 89–97.
Crick, F. (1989). The recent excitement about neural networks. Nature, 337(6203),
129–132.
Cullen, M., Davey, B., Friston, K., & Moran, R. J. (2018). Active inference in
openai gym: A paradigm for computational investigations into psychiatric illness.
Biological psychiatry: Cognitive Neuroscience and neuroimaging , 3(9), 809–
818.
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V ., & Friston, K. (2020). Active
inference on discrete state-spaces: a synthesis. arXiv preprint arXiv:2001.07203.
Da Costa, L., Sajid, N., Parr, T., Friston, K., & Smith, R. (2020). The relationship
between dynamic programming and active inference: The discrete, ﬁnite-horizon
case. arXiv preprint arXiv:2009.08111. Retrieved from https://arxiv.org/
abs/2009.08111
Daw, N. D., O’doherty, J. P., Dayan, P., Seymour, B., & Dolan, R. J. (2006). Cortical
substrates for exploratory decisions in humans. Nature, 441(7095), 876–879.
Dayan, P. (2009). Goal-directed control and its antipodes. Neural Networks, 22(3),
213–219.
Dayan, P., & Daw, N. D. (2008). Decision theory, reinforcement learning, and the brain.
Cognitive, Affective, & Behavioral Neuroscience, 8(4), 429–453.
Dayan, P., & Hinton, G. (1997). Using expectation-maximization for reinforcement
learning. Neural Computation, 9(2), 271–278.
Dayan, P., Hinton, G., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine.
Neural Computation, 7(5), 889–904.
De Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y . (2005). A tutorial on the
cross-entropy method. Annals of Operations Research, 134(1), 19–67.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the em algorithm. Journal of the Royal Statistical Society:
References 381
Series B (Methodological), 39(1), 1–22.
de Xivry, J.-J. O., Coppe, S., Blohm, G., & Lefevre, P. (2013). Kalman ﬁltering
naturally accounts for visually guided and predictive smooth pursuit dynamics.
Journal of Neuroscience, 33(44), 17301–17313.
Doucet, A., Godsill, S., & Andrieu, C. (2000). On sequential monte carlo sampling
methods for bayesian ﬁltering. Statistics and Computing, 10(3), 197–208.
Doya, K. (2000). Reinforcement learning in continuous time and space. Neural
Computation, 12(1), 219–245.
Esposito, M., & Van den Broeck, C. (2010). Three faces of the second law. i. master
equation formulation. Physical Review E, 82(1), 011143.
Farshidian, F., Neunert, M., & Buchli, J. (2014). Learning of closed-loop motion control.
In 2014 ieee/rsj international conference on intelligent robots and systems(pp.
1441–1446).
Feldman, H., & Friston, K. (2010). Attention, uncertainty, and free-energy. Frontiers in
human neuroscience, 4, 215. Retrieved from https://www.frontiersin.org/
articles/10.3389/fnhum.2010.00215/full
Felleman, D. J., & Van Essen, D. C. (1991). Distributed hierarchical processing in the
primate cerebral cortex. In Cereb cortex.
Feng, S., Whitman, E., Xinjilefu, X., & Atkeson, C. G. (2014). Optimization based
full body control for the atlas robot. In 2014 ieee-ras international conference on
humanoid robots (pp. 120–127).
Feynman, R. (1998). Statistical mechanics: a set of lectures (advanced book classics).
Fodor, J. A. (1983). The modularity of mind. MIT press.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active inference
agents using monte-carlo methods. arXiv preprint arXiv:2006.04176.
Fox, C. W., & Roberts, S. J. (2012). A tutorial on variational bayesian inference.
Artiﬁcial intelligence review, 38(2), 85–95.
Friston, K. (2003). Learning and inference in the brain. Neural Networks, 16(9),
References 382
1325–1352.
Friston, K. (2005). A theory of cortical responses. Philosophical Transactions of the
Royal Society B: Biological sciences, 360(1456), 815–836.
Friston, K. (2008a). Hierarchical models in the brain. PLoS Computational Biology,
4(11).
Friston, K. (2008b). Variational ﬁltering. NeuroImage, 41(3), 747–766.
Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in
Cognitive Sciences, 13(7), 293–301.
Friston, K. (2010). The free-energy principle: a uniﬁed brain theory? Nature reviews
neuroscience, 11(2), 127–138.
Friston, K. (2012). The history of the future of the bayesian brain. NeuroIm-
age, 62(2), 1230–1233. Retrieved from https://www.sciencedirect.com/
science/article/pii/S1053811911011657
Friston, K. (2013). Life as we know it. Journal of the Royal Society Interface, 10(86),
20130475.
Friston, K. (2019a). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston, K. (2019b). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184. Retrieved from https://arxiv.org/pdf/1906.10184
.pdf
Friston, K. (2019c). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston, K. (2019d). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston, K., & Ao, P. (2012a). Free energy, value, and attractors. Computational and
mathematical methods in medicine, 2012.
Friston, K., & Ao, P. (2012b). Free energy, value, and attractors. Computational and
mathematical methods in medicine, 2012.
References 383
Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2020). Sophisticated
inference. arXiv preprint arXiv:2006.04120. Retrieved from https://arxiv
.org/abs/2006.04120
Friston, K., Da Costa, L., & Parr, T. (2020). Some interesting observations on the free
energy principle. arXiv preprint arXiv:2002.04501.
Friston, K., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active
inference? PloS one, 4(7).
Friston, K., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and behavior: a free-
energy formulation. Biological Cybernetics, 102(3), 227–260. Retrieved from
https://link.springer.com/article/10.1007/s00422-010-0364-z
Friston, K., Fagerholm, E. D., Zarghami, T. S., Parr, T., Hipólito, I., Magrou, L., &
Razi, A. (2007). Parcels and particles: Markov blankets in the brain. Network
Neuroscience(Just Accepted), 1–76.
Friston, K., Fagerholm, E. D., Zarghami, T. S., Parr, T., Hipólito, I., Magrou, L., & Razi,
A. (2020). Parcels and particles: Markov blankets in the brain. arXiv preprint
arXiv:2007.09704. Retrieved from https://arxiv.org/abs/2007.09704
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017a). Active
inference: a process theory. Neural Computation, 29(1), 1–49.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017b). Active
inference: a process theory. Neural Computation, 29(1), 1–49.
Friston, K., & Frith, C. (2015). A duet for one. Consciousness and Cognition, 36,
390–405.
Friston, K., & Kiebel, S. (2009). Predictive coding under the free-energy princi-
ple. Philosophical Transactions of the Royal Society B: Biological Sciences ,
364(1521), 1211–1221.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain.
Journal of Physiology-Paris, 100(1-3), 70–87.
Friston, K., Levin, M., Sengupta, B., & Pezzulo, G. (2015). Knowing one’s place: a
References 384
free-energy approach to pattern regulation. Journal of the Royal Society Interface,
12(105), 20141383. Retrieved from https://royalsocietypublishing.org/
doi/full/10.1098/rsif.2014.1383
Friston, K., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017).
Active inference, curiosity and insight. Neural Computation, 29(10), 2633–2683.
Friston, K., Parr, T., & Zeidman, P. (2018). Bayesian model reduction. arXiv preprint
arXiv:1805.07092.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G. (2015a).
Active inference and epistemic value. Cognitive Neuroscience, 6(4), 187–214.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G. (2015b).
Active inference and epistemic value. , 6(4), 187–214. doi: 10.1080/17588928
.2015.1020053
Friston, K., Rosch, R., Parr, T., Price, C., & Bowman, H. (2018a). Deep temporal models
and active inference. Neuroscience & Biobehavioral Reviews, 90, 486–501.
Friston, K., Rosch, R., Parr, T., Price, C., & Bowman, H. (2018b). Deep temporal
models and active inference. ,90, 486–501. Retrieved 2019-11-15, from http://
www.sciencedirect.com/science/article/pii/S0149763418302525 doi:
10.1016/j.neubiorev.2018.04.004
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and agency:
optimal control without cost functions. Biological Cybernetics, 106(8-9), 523–
541.
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., & Dolan,
R. J. (2014). The anatomy of choice: dopamine and decision-making. Philo-
sophical Transactions of the Royal Society B: Biological Sciences, 369(1655),
20130481.
Friston, K., Stephan, K., Li, B., & Daunizeau, J. (2010). Generalised ﬁlter-
ing. Mathematical Problems in Engineering, 2010. Retrieved from https://
www.hindawi.com/journals/mpe/2010/621670/
References 385
Friston, K., & Stephan, K. E. (2007). Free-energy and the brain. Synthese, 159(3),
417–458.
Friston, K., Trujillo-Barreto, N., & Daunizeau, J. (2008). Dem: a variational treatment
of dynamic systems. Neuroimage, 41(3), 849–885.
Friston, K., Wiese, W., & Hobson, J. A. (2020). Sentience and the origins of conscious-
ness: From cartesian duality to markovian monism. Entropy, 22(5), 516.
Fujimoto, S., van Hoof, H., & Meger, D. (2018). Addressing function approximation
error in actor-critic methods. arXiv preprint arXiv:1802.09477.
Gaissmaier, W., & Schooler, L. J. (2008). The smart potential behind probability
matching. Cognition, 109(3), 416–422.
Gal, Y ., McAllister, R., & Rasmussen, C. E. (2016). Improving pilco with bayesian
neural network dynamics models. In Data-efﬁcient machine learning workshop,
icml (V ol. 4, p. 25).
Garivier, A., & Moulines, E. (2011). On upper-conﬁdence bound policies for switching
bandit problems. In International conference on algorithmic learning theory (pp.
174–188).
Gershman, S. J. (2018). Uncertainty and exploration. bioRxiv, 265504.
Gerstner, W., & Kistler, W. M. (2002). Mathematical formulations of hebbian learning.
Biological Cybernetics, 87(5), 404–415.
Geweke, J. (2007). Bayesian model comparison and validation. American Economic
Review, 97(2), 60–64.
Ghahramani, Z., & Beal, M. J. (2001). Propagation algorithms for variational bayesian
learning. In Advances in neural information processing systems (pp. 507–513).
Ghahramani, Z., Beal, M. J., et al. (2000). Graphical models and variational methods.
Advanced mean ﬁeld methods-theory and practice. MIT Press.
Gibson, J. J. (2002). A theory of direct visual perception. Vision and Mind: selected
readings in the philosophy of perception, 77–90.
Girolami, M., & Calderhead, B. (2011). Riemann manifold langevin and hamiltonian
References 386
monte carlo methods. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(2), 123–214.
Gold, J. I., & Shadlen, M. N. (2003). The inﬂuence of behavioral context on the
representation of a perceptual decision in developing oculomotor commands.
Journal of Neuroscience, 23(2), 632–651.
Goodfellow, I., Bengio, Y ., & Courville, A. (2016).Deep learning. MIT press.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., . . .
Bengio, Y . (2014). Generative adversarial nets. Advances in neural Information
Processing Systems, 27, 2672–2680.
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In
Machine learning proceedings 1995 (pp. 261–268). Elsevier.
Gordon, N. J., Salmond, D. J., & Smith, A. F. (1993). Novel approach to nonlinear/non-
gaussian bayesian state estimation. In Ieee proceedings f (radar and signal
processing) (V ol. 140, pp. 107–113).
Grewal, M. S., & Andrews, A. P. (2010). Applications of kalman ﬁltering in aerospace
1960 to the present [historical perspectives]. IEEE Control Systems Magazine,
30(3), 69–78.
Griewank, A., et al. (1989). On automatic differentiation. Mathematical Programming:
recent developments and applications, 6(6), 83–107.
Grill-Spector, K., & Malach, R. (2004). The human visual cortex. Annu. Rev. Neurosci.,
27, 649–677.
Gu, S., Lillicrap, T., Sutskever, I., & Levine, S. (2016). Continuous deep q-learning
with model-based acceleration. In International conference on machine learning
(pp. 2829–2838).
Gupta, S., Agrawal, A., Gopalakrishnan, K., & Narayanan, P. (2015). Deep learning
with limited numerical precision. InInternational conference on machine learning
(pp. 1737–1746).
Ha, D., & Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122.
References 387
Haarnoja, T. (2018). Acquiring diverse robot skills via maximum entropy deep rein-
forcement learning (Unpublished doctoral dissertation). UC Berkeley.
Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th international conference
on machine learning-volume 70 (pp. 1352–1361).
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. arXiv
preprint arXiv:1801.01290.
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., . . . others (2018).
Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.
Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2019). Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J.
(2018). Learning latent dynamics for planning from pixels. arXiv preprint
arXiv:1811.04551.
Hafner, D., Ortega, P. A., Ba, J., Parr, T., Friston, K., & Heess, N. (2020). Action and
perception as divergence minimization. arXiv preprint arXiv:2009.01791.
Harvey, A. C. (1990). Forecasting, structural time series models and the kalman ﬁlter.
Cambridge university press.
Hawkins, J., & Blakeslee, S. (2007). On intelligence: How a new understanding of the
brain will lead to the creation of truly intelligent machines. Macmillan.
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image
recognition. In Proceedings of the ieee conference on computer vision and
pattern recognition (pp. 770–778).
Hebb, D. O. (1949). The ﬁrst stage of perception: growth of the assembly. The
Organization of Behavior, 4, 60–78.
Heins, R. C., Mirza, M. B., Parr, T., Friston, K., Kagan, I., & Pooresmaeili, A. (2020).
Deep active inference and scene construction. Frontiers in Artiﬁcial Intelligence,
References 388
3, 81.
Helmholtz, H. v. (1866). Concerning the perceptions in general. Treatise on physiologi-
cal optics,.
Henderson, J. M. (2017). Gaze control as prediction. Trends in Cognitive Sciences,
21(1), 15–23.
Hesp, C., Tschantz, A., Millidge, B., Ramstead, M., Friston, K., & Smith, R. (2020).
Sophisticated affective inference: Simulating anticipatory affective dynamics
of imagining future events. In International workshop on active inference (pp.
179–186).
Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., . . .
Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement
learning. In Thirty-second aaai conference on artiﬁcial intelligence.
Hinton, G., Srivastava, N., & Swersky, K. (2012). Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Powerpoint Presentation,
14(8).
Hinton, G., & Zemel, R. S. (1994). Autoencoders, minimum description length and
helmholtz free energy. In Advances in neural information processing systems (pp.
3–10).
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computa-
tion, 9(8), 1735–1780.
Hohwy, J. (2016). The self-evidencing brain. Noûs, 50(2), 259–285.
Hohwy, J., Roepstorff, A., & Friston, K. (2008). Predictive coding explains binocular
rivalry: An epistemological review. Cognition, 108(3), 687–701.
Huang, Y ., & Rao, R. P. (2011). Predictive coding. Wiley Interdisciplinary Reviews:
Cognitive Science, 2(5), 580–593. Retrieved from https://onlinelibrary
.wiley.com/doi/pdf/10.1002/wcs.142?casa_token=TJvdr2nDbr8AAAAA:
0T3LOAIXt6I7YYpJIqOs204qnwU0FFQiVC976sVifVv0XB4wFlrLZ7WvALY9x
_qdoIGciEZWd12hfNQ
References 389
Hubel, D. H., & Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and
functional architecture in the cat’s visual cortex. The Journal of Physiology ,
160(1), 106.
Isomura, T., Parr, T., & Friston, K. (2019). Bayesian ﬁltering with multiple internal
models: toward a theory of social intelligence. Neural Computation, 31(12),
2390–2431.
Jaswinski, A. (1970). Stochastic processes and ﬁltering theory, 1970. Academic Press.
Johnson, M. A., & Moradi, M. H. (2005). Pid control. Springer.
Jordan, M., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1998). An introduction to
variational methods for graphical models. In Learning in graphical models (pp.
105–161). Springer.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction
to variational methods for graphical models. Machine learning, 37(2), 183–233.
Jordan, R., Kinderlehrer, D., & Otto, F. (1998). The variational formulation of the
fokker–planck equation. SIAM journal on mathematical analysis, 29(1), 1–17.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artiﬁcial Intelligence, 101(1-2), 99–134.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A
survey. Journal of artiﬁcial intelligence research, 4, 237–285.
Kaiser, J., Mostafa, H., & Neftci, E. (2020). Synaptic plasticity dynamics for deep
continuous local learning (decolle). Frontiers in Neuroscience, 14, 424.
Kalman, R. E. (1960). A new approach to linear ﬁltering and prediction problems.
Kalman, R. E., & Bucy, R. S. (1961). New results in linear ﬁltering and prediction
theory. Journal of Basic Engineering, 83(1), 95–108.
Kalman, R. E., et al. (1960). Contributions to the theory of optimal control. Bol. soc.
mat. mexicana, 5(2), 102–119.
Kanai, R., Komura, Y ., Shipp, S., & Friston, K. (2015). Cerebral hierarchies: predictive
processing, precision and the pulvinar. Philosophical Transactions of the Royal
References 390
Society B: Biological Sciences, 370(1668), 20140169.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., . . .
Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361.
Kaplan, R., & Friston, K. (2018). Planning and navigation as active inference.Biological
Cybernetics, 112(4), 323–343.
Kappen, H. J. (2005). Path integrals and symmetry breaking for optimal control theory.
Journal of statistical mechanics: theory and experiment, 2005(11), P11011.
Kappen, H. J. (2007). An introduction to stochastic control theory, path integrals and
reinforcement learning. In Aip conference proceedings (V ol. 887, pp. 149–181).
Kappen, H. J., Gómez, V ., & Opper, M. (2012). Optimal control as a graphical model
inference problem. Machine learning, 87(2), 159–182.
Keller, G. B., & Mrsic-Flogel, T. D. (2018). Predictive processing: a canonical cortical
computation. Neuron, 100(2), 424–435.
Kim, H., Kim, J., Jeong, Y ., Levine, S., & Song, H. O. (2018). Emi: Exploration with
mutual information. arXiv preprint arXiv:1810.01176.
Kim, Y ., Wiseman, S., Miller, A. C., Sontag, D., & Rush, A. M. (2018). Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550.
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114.
Kirk, D. E. (2004). Optimal control theory: an introduction. Courier Corporation.
Klyubin, A. S., Polani, D., & Nehaniv, C. L. (2005). Empowerment: A universal agent-
centric measure of control. In 2005 ieee congress on evolutionary computation
(V ol. 1, pp. 128–135).
Kocsis, L., & Szepesvári, C. (2006). Bandit based monte-carlo planning. In European
conference on machine learning (pp. 282–293).
References 391
Kondepudi, D., & Prigogine, I. (2014). Modern thermodynamics: from heat engines to
dissipative structures. John Wiley & Sons.
Kopp, R. E. (1962). Pontryagin maximum principle. In Mathematics in science and
engineering (V ol. 5, pp. 255–279). Elsevier.
Krebs, J. R., Kacelnik, A., & Taylor, P. (1978). Test of optimal sampling by foraging
great tits. Nature, 275(5675), 27–31.
Kriegeskorte, N. (2015). Deep neural networks: a new framework for modeling
biological vision and brain information processing. Annual review of vision
science, 1, 417–446.
Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing
systems (pp. 1097–1105).
Kutschireiter, A. (2018). Nonlinear ﬁltering in neuroscience: theory and application
(Unpublished doctoral dissertation). University of Zurich.
Kutschireiter, A., Surace, S. C., & Pﬁster, J.-P. (2020). The hitchhiker’s guide to
nonlinear ﬁltering. Journal of Mathematical Psychology, 94, 102307.
Kutschireiter, A., Surace, S. C., Sprekeler, H., & Pﬁster, J.-P. (2015). The neural particle
ﬁlter. arXiv preprint arXiv:1508.06818.
Kwakernaak, H., & Sivan, R. (1972). Linear optimal control systems (V ol. 1). Wiley-
interscience New York.
Lanczos, C. (2012). The variational principles of mechanics. Courier Corporation.
Launay, J., Poli, I., & Krzakala, F. (2019). Principled training of neural networks with
direct feedback alignment. arXiv preprint arXiv:1906.04554.
Lawson, R. P., Rees, G., & Friston, K. (2014). An aberrant precision account of
autism. Frontiers in human neuroscience, 8, 302. Retrieved from https://
www.frontiersin.org/articles/10.3389/fnhum.2014.00302/full
Lee, D.-H., Zhang, S., Fischer, A., & Bengio, Y . (2015). Difference target propagation.
In Joint european conference on machine learning and knowledge discovery in
References 392
databases (pp. 498–515).
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., & Salakhutdinov, R.
(2019). Efﬁcient exploration via state marginal matching. arXiv preprint
arXiv:1906.05274.
Leondes, C. T. (1970). Theory and applications of kalman ﬁltering (Tech. Rep.).
Advisory Group for Aerospace Research and Development Neuilly-Sur-Seine
(France).
Levine, S. (2018). Reinforcement learning and control as probabilistic inference:
Tutorial and review. arXiv preprint arXiv:1805.00909.
Levine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Ofﬂine reinforcement learn-
ing: Tutorial, review, and perspectives on open problems. arXiv preprint
arXiv:2005.01643.
Li, S. (2020). Robot playing kendama with model-based and model-free reinforcement
learning. arXiv preprint arXiv:2003.06751.
Li, W., & Todorov, E. (2004). Iterative linear quadratic regulator design for nonlinear
biological movement systems. In Icinco (1) (pp. 222–229).
Liao, Q., Leibo, J. Z., & Poggio, T. (2016). How important is weight symmetry in
backpropagation? In Thirtieth aaai conference on artiﬁcial intelligence.
Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2014). Random
feedback weights support learning in deep neural networks. arXiv preprint
arXiv:1411.0247.
Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2016). Random synap-
tic feedback weights support error backpropagation for deep learning. Nature
communications, 7(1), 1–10.
Lillicrap, T. P., & Santoro, A. (2019). Backpropagation through time and the brain.
Current Opinion in Neurobiology, 55, 82–89.
Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., & Hinton, G. (2020). Back-
propagation and the brain. Nature Reviews Neuroscience, 1–12.
References 393
Linnainmaa, S. (1970). The representation of the cumulative rounding error of an
algorithm as a taylor expansion of the local rounding errors. Master’s Thesis (in
Finnish), Univ. Helsinki, 6–7.
Lotter, W., Kreiman, G., & Cox, D. (2016). Deep predictive coding networks for video
prediction and unsupervised learning. arXiv preprint arXiv:1605.08104.
Ma, Y.-A., Chen, T., & Fox, E. B. (2015). A complete recipe for stochastic gradient
mcmc. arXiv preprint arXiv:1506.04696.
Marino, J., Yue, Y ., & Mandt, S. (2018). Iterative amortized inference.arXiv preprint
arXiv:1807.09356.
Marr, D. (1982). Vision: A computational investigation into the human representation
and processing of visual information.
Maturana, H. R., & Varela, F. J. (2012). Autopoiesis and cognition: The realization of
the living (V ol. 42). Springer Science & Business Media.
Mehlhorn, K., Newell, B. R., Todd, P. M., Lee, M. D., Morgan, K., Braithwaite, V. A.,
. . . Gonzalez, C. (2015). Unpacking the exploration–exploitation tradeoff: A
synthesis of human and animal literatures. Decision, 2(3), 191.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E.
(1953). Equation of state calculations by fast computing machines. The Journal
of Chemical Physics, 21(6), 1087–1092.
Meulemans, A., Carzaniga, F. S., Suykens, J. A., Sacramento, J., & Grewe, B. F. (2020).
A theoretical framework for target propagation.arXiv preprint arXiv:2006.14331.
Millidge, B. (2019a). Combining active inference and hierarchical predictive coding: A
tutorial introduction and case study.
Millidge, B. (2019b). Deep active inference as variational policy gradients. arXiv
preprint arXiv:1907.03876.
Millidge, B. (2019c). Implementing predictive processing and active inference: Prelim-
inary steps and results.
Millidge, B. (2020). Deep active inference as variational policy gradients. Journal of
References 394
Mathematical Psychology, 96, 102348.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020a). Predictive coding approximates
backprop along arbitrary computation graphs. arXiv preprint arXiv:2006.04182.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020b). Whence the expected free
energy? arXiv preprint arXiv:2004.08128.
Millidge, B., Tschantz, A., Buckley, C. L., & Seth, A. (2020). Activation relaxation: A
local dynamical approximation to backpropagation in the brain. arXiv preprint
arXiv:2009.05359.
Millidge, B., Tschantz, A., Seth, A., & Buckley, C. (2021). Understanding the origin
of information-seeking exploration in probabilistic objectives for control. arXiv
preprint arXiv:2103.06859.
Millidge, B., Tschantz, A., Seth, A., & Buckley, C. L. (2020a). Investigating the
scalability and biological plausibility of the activation relaxation algorithm.arXiv
preprint arXiv:2010.06219.
Millidge, B., Tschantz, A., Seth, A., & Buckley, C. L. (2020d). Relaxing the constraints
on predictive coding models. arXiv preprint arXiv:2010.01047.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020b). On the re-
lationship between active inference and control as inference. arXiv preprint
arXiv:2006.12964.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020c). Reinforcement
learning as iterative and amortised inference. arXiv preprint arXiv:2006.10524.
Mirchev, A., Kayalibay, B., Soelch, M., van der Smagt, P., & Bayer, J. (2018).
Approximate bayesian inference in spatial environments. arXiv preprint
arXiv:1805.07206.
Mirza, M. B., Adams, R. A., Parr, T., & Friston, K. (2019). Impulsivity and active
inference. Journal of Cognitive Neuroscience, 31(2), 202–220.
Mnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief
networks. arXiv preprint arXiv:1402.0030.
References 395
Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., . . . Kavukcuoglu,
K. (2016). Asynchronous methods for deep reinforcement learning. In Interna-
tional conference on machine learning (pp. 1928–1937).
Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &
Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602.
Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., . . .
others (2015). Human-level control through deep reinforcement learning. Nature,
518(7540), 529–533.
Mobbs, D., Trimmer, P. C., Blumstein, D. T., & Dayan, P. (2018). Foraging for
foundations in decision neuroscience: insights from ethology. Nature Reviews
Neuroscience, 19(7), 419–427.
Mumford, D. (1992). On the computational architecture of the neocortex. Biological
Cybernetics, 66(3), 241–251.
Munuera, J., Morel, P., Duhamel, J.-R., & Deneve, S. (2009). Optimal sensorimotor
control in eye movement sequences. Journal of Neuroscience, 29(10), 3026–
3035.
Nagabandi, A., Kahn, G., Fearing, R. S., & Levine, S. (2018). Neural network dynamics
for model-based deep reinforcement learning with model-free ﬁne-tuning. In
2018 ieee international conference on robotics and automation (icra) (pp. 7559–
7566).
Nagabandi, A., Konoglie, K., Levine, S., & Kumar, V . (2019). Deep dynamics models
for learning dexterous manipulation. arXiv preprint arXiv:1909.11652.
Neal, R. M., & Hinton, G. (1998). A view of the em algorithm that justiﬁes incremental,
sparse, and other variants. In Learning in graphical models (pp. 355–368).
Springer.
Neal, R. M., et al. (2011). Mcmc using hamiltonian dynamics. Handbook of Markov
Chain Monte Carlo, 2(11), 2.
References 396
Neftci, E. O., Mostafa, H., & Zenke, F. (2019). Surrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking
neural networks. IEEE Signal Processing Magazine, 36(6), 51–63.
Nesterov, Y . (1983). A method of solving a convex programming problem with
convergence rate o (1/kˆ 2) o (1/k2). InSov. math. dokl (V ol. 27).
Nøkland, A. (2016). Direct feedback alignment provides learning in deep neural
networks. In Advances in neural information processing systems (pp. 1037–
1045).
Okada, M., Kosaka, N., & Taniguchi, T. (2020). Planet of the bayesians: Reconsidering
and improving deep planning network by incorporating bayesian inference. arXiv
preprint arXiv:2003.00370.
Okada, M., & Taniguchi, T. (2020). Variational inference mpc for bayesian model-based
reinforcement learning. In Conference on robot learning (pp. 258–272).
Olah, C., Mordvintsev, A., & Schubert, L. (2017). Feature visualization. Distill, 2(11),
e7.
Ollivier, Y . (2019). The extended kalman ﬁlter is a natural gradient descent in trajectory
space. arXiv preprint arXiv:1901.00696.
Ollivier, Y ., Arnold, L., Auger, A., & Hansen, N. (2017). Information-geometric
optimization algorithms: A unifying picture via invariance principles. Journal of
Machine Learning Research, 18(18), 1–65.
Ollivier, Y ., Tallec, C., & Charpiat, G. (2015). Training recurrent networks online
without backtracking. arXiv preprint arXiv:1507.07680.
Orchard, J., & Sun, W. (2019). Making predictive coding networks generative. arXiv
preprint arXiv:1910.12151. Retrieved from https://arxiv.org/abs/1910
.12151
O’Reilly, R. C., Wyatte, D. R., & Rohrlich, J. (2017). Deep predictive learning: a
comprehensive model of three visual streams. arXiv preprint arXiv:1709.04654.
Ororbia, A. G., & Mali, A. (2019). Biologically motivated algorithms for propagating
References 397
local target representations. In Proceedings of the aaai conference on artiﬁcial
intelligence (V ol. 33, pp. 4651–4658).
Ororbia II, A. G., Haffner, P., Reitter, D., & Giles, C. L. (2017). Learning to adapt by
minimizing discrepancy. arXiv preprint arXiv:1711.11542.
Osband, I., & Van Roy, B. (2015). Bootstrapped thompson sampling and deep explo-
ration. arXiv preprint arXiv:1507.00300.
Osband, I., Van Roy, B., Russo, D. J., & Wen, Z. (2019). Deep exploration via
randomized value functions. Journal of Machine Learning Research, 20(124),
1–62.
Oudeyer, P.-Y ., & Kaplan, F. (2009). What is intrinsic motivation? a typology of
computational approaches. Frontiers in neurorobotics, 1, 6.
Ovchinnikov, I. V . (2016). Introduction to supersymmetric theory of stochastics.
Entropy, 18(4), 108.
O’Reilly, R. C., Braver, T. S., Cohen, J. D., et al. (1999). A biologically based compu-
tational model of working memory. Models of working memory: Mechanisms of
active maintenance and executive control, 375–411.
Palacios, E. R., Razi, A., Parr, T., Kirchhoff, M., & Friston, K. (2017). Biological self-
organisation and markov blankets. BioRxiv, 227181. Retrieved from https://
www.biorxiv.org/content/10.1101/227181v1.abstract
Parr, T. (2019). The computational neurology of active vision (Unpublished doctoral
dissertation). UCL (University College London).
Parr, T., Da Costa, L., & Friston, K. (2020). Markov blankets, information geometry
and stochastic thermodynamics. Philosophical Transactions of the Royal Society
A, 378(2164), 20190159.
Parr, T., & Friston, K. (2017a). The active construction of the visual world. Neuropsy-
chologia, 104, 92–101.
Parr, T., & Friston, K. (2017b). Uncertainty, epistemics and active inference. Journal
of The Royal Society Interface, 14(136), 20170376.
References 398
Parr, T., & Friston, K. (2018a). Active inference and the anatomy of oculomotion.
Neuropsychologia, 111, 334–343.
Parr, T., & Friston, K. (2018b). The anatomy of inference: Generative models and brain
structure. Frontiers in computational neuroscience, 12.
Parr, T., & Friston, K. (2018c). The computational anatomy of visual neglect. Cerebral
Cortex, 28(2), 777–790.
Parr, T., Markovic, D., Kiebel, S. J., & Friston, K. (2019). Neuronal message passing
using mean-ﬁeld, bethe, and marginal approximations. Scientiﬁc reports, 9(1),
1–18.
Parr, T., Sajid, N., & Friston, K. (2020). Modules or mean-ﬁelds? Entropy, 22(5), 552.
Retrieved from https://www.mdpi.com/1099-4300/22/5/552
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., . . . Lerer, A.
(2017). Automatic differentiation in pytorch.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the ieee conference on computer
vision and pattern recognition workshops (pp. 16–17).
Pearl, J. (2011). Bayesian networks.
Pearl, J. (2014). Probabilistic reasoning in intelligent systems: networks of plausible
inference. Elsevier.
Peters, J., & Schaal, S. (2007). Reinforcement learning by reward-weighted regression
for operational space control. In Proceedings of the 24th international conference
on machine learning (pp. 745–750).
Pinker, S. (2003). The language instinct: How the mind creates language . Penguin
UK.
Pio-Lopez, L., Nizard, A., Friston, K., & Pezzulo, G. (2016). Active inference and
robot control: a case study. Journal of The Royal Society Interface , 13(122),
20160616.
Pozzi, I., Bohté, S., & Roelfsema, P. (2018). A biologically plausible learning rule for
References 399
deep learning in the brain. arXiv preprint arXiv:1811.01768.
Prigogine, I. (2017). Non-equilibrium statistical mechanics. Courier Dover Publica-
tions.
Prigogine, I., & Lefever, R. (1973). Theory of dissipative structures. In Synergetics (pp.
124–135). Springer.
Pyke, G. H. (1984). Optimal foraging theory: a critical review. Annual review of
ecology and systematics, 15(1), 523–575.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., . . . others
(2021). Learning transferable visual models from natural language supervision.
arXiv preprint arXiv:2103.00020.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language
models are unsupervised multitask learners. OpenAI blog, 1(8), 9.
Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-ﬁeld effects.Nature Neuroscience,
2(1), 79–87.
Rawlik, K., Toussaint, M., & Vijayakumar, S. (2013). On stochastic optimal control and
reinforcement learning by approximate inference. In Twenty-third international
joint conference on artiﬁcial intelligence.
Rawlik, K. C. (2013). On probabilistic inference approaches to stochastic optimal
control.
Roelfsema, P. R., & Ooyen, A. v. (2005). Attention-gated reinforcement learning of
internal representations for classiﬁcation. Neural Computation, 17(10), 2176–
2214.
Rubinstein, R. Y . (1997). Optimization of computer simulation models with rare events.
European Journal of Operational Research, 99(1), 89–112.
Rumelhart, D. E., Hinton, G., & Williams, R. J. (1986). Learning representations by
back-propagating errors. nature, 323(6088), 533–536.
Rumelhart, D. E., & Zipser, D. (1985). Feature discovery by competitive learning.
References 400
Cognitive science, 9(1), 75–112.
Russo, D., & Van Roy, B. (2016). An information-theoretic analysis of thompson
sampling. The Journal of Machine Learning Research, 17(1), 2442–2471.
Sacramento, J., Costa, R. P., Bengio, Y ., & Senn, W. (2018). Dendritic cortical
microcircuits approximate the backpropagation algorithm. In Advances in neural
information processing systems (pp. 8721–8732).
Salimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I. (2017). Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864.
Sanborn, A. N., & Chater, N. (2016). Bayesian brains without probabilities. Trends in
Cognitive Sciences, 20(12), 883–893.
Särkkä, S. (2013). Bayesian ﬁltering and smoothing (No. 3). Cambridge University
Press.
Scellier, B., & Bengio, Y . (2016). Towards a biologically plausible backprop. arXiv
preprint arXiv:1602.05179, 914.
Scellier, B., & Bengio, Y . (2017). Equilibrium propagation: Bridging the gap be-
tween energy-based models and backpropagation. Frontiers in computational
neuroscience, 11, 24.
Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y . (2018a). Extending the
framework of equilibrium propagation to general dynamics.
Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y . (2018b). General-
ization of equilibrium propagation to vector ﬁeld dynamics. arXiv preprint
arXiv:1808.04873.
Schiess, M., Urbanczik, R., & Senn, W. (2016). Somato-dendritic synaptic plasticity
and error-backpropagation in active dendrites. PLoS Computational Biology,
12(2).
Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in
model-building neural controllers. In Proc. of the international conference on
simulation of adaptive behavior: From animals to animats (pp. 222–227).
References 401
Schmidhuber, J. (1999). Artiﬁcial curiosity based on discovering novel algorithmic
predictability through coevolution. In Proceedings of the 1999 congress on
evolutionary computation-cec99 (cat. no. 99th8406) (V ol. 3, pp. 1612–1618).
Schmidhuber, J. (2007). Simple algorithmic principles of discovery, subjective beauty,
selective attention, curiosity & creativity. InInternational conference on discovery
science (pp. 26–38).
Schneider, W. (1988). Analytical uses of kalman ﬁltering in econometrics—a survey.
Statistical Papers, 29(1), 3–33.
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., . . .
others (2019). Mastering atari, go, chess and shogi by planning with a learned
model. arXiv preprint arXiv:1911.08265.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy
optimization. In International conference on machine learning (pp. 1889–1897).
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347.
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of
neurophysiology, 80(1), 1–27.
Schultz, W., Tremblay, L., & Hollerman, J. R. (1998). Reward prediction in primate
basal ganglia and frontal cortex. Neuropharmacology, 37(4-5), 421–429.
Schwartenbeck, P., FitzGerald, T., Dolan, R., & Friston, K. (2013). Exploration, novelty,
surprise, and free energy minimization. Frontiers in Psychology, 4, 710.
Schwartenbeck, P., FitzGerald, T. H., Mathys, C., Dolan, R., Wurst, F., Kronbichler, M.,
& Friston, K. (2015). Optimal inference with suboptimal models: addiction and
active bayesian inference. Medical Hypotheses, 84(2), 109–117.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., &
Friston, K. (2019). Computational mechanisms of curiosity and goal-directed
exploration. , 8, e41703. Retrieved 2019-11-15, from https://doi.org/10
.7554/eLife.41703 doi: 10.7554/eLife.41703
References 402
Schwöbel, S., Kiebel, S., & Markovi´c, D. (2018). Active inference, belief propagation,
and the bethe approximation. Neural Computation, 30(9), 2530–2567.
Seifert, U. (2008). Stochastic thermodynamics: principles and perspectives. The
European Physical Journal B, 64(3), 423–431.
Seifert, U. (2012). Stochastic thermodynamics, ﬂuctuation theorems and molecular
machines. Reports on Progress in Physics, 75(12), 126001.
Seth, A. K. (2014). The cybernetic bayesian brain . Open MIND. Frankfurt am
Main: MIND Group. Retrieved from https://open-mind.net/papers/the
-cybernetic-bayesian-brain
Sethi, S. P., & Thompson, G. L. (2000). What is optimal control theory? Springer.
Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic
synaptic transmission. Neuron, 40(6), 1063–1073.
Shanks, D. R., Tunney, R. J., & McCarthy, J. D. (2002). A re-examination of probability
matching and rational choice. Journal of Behavioral Decision Making, 15(3),
233–250.
Shannon, C. E. (1948). A mathematical theory of communication. The Bell system
technical journal, 27(3), 379–423.
Shipp, S. (2016). Neural elements for predictive coding. Frontiers in Psychology, 7,
1792.
Shipp, S., Adams, R. A., & Friston, K. (2013). Reﬂections on agranular architecture:
predictive coding in the motor cortex. Trends in neurosciences, 36(12), 706–716.
Shyam, P., Ja ´skowski, W., & Gomez, F. (2019). Model-based active exploration.
In International conference on machine learning (pp. 5779–5788). Retrieved
2019-10-11, from http://proceedings.mlr.press/v97/shyam19a.html
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., . . .
others (2016). Mastering the game of go with deep neural networks and tree
search. nature, 529(7587), 484.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., . . .
References 403
others (2017). Mastering the game of go without human knowledge. Nature,
550(7676), 354–359.
Simoncelli, E. P. (2009). Optimal estimation in sensory systems. The Cognitive
Neurosciences, IV, 525–535.
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility
traces. Machine Learning, 22(1), 123–158.
Spratling, M. W. (2008). Reconciling predictive coding and biased competition models
of cortical function. Frontiers in Computational Neuroscience, 2, 4.
Spratling, M. W. (2017). A review of predictive coding algorithms.Brain and Cognition,
112, 92–97. Retrieved from https://www.sciencedirect.com/science/
article/pii/S027826261530035X?casa_token=zzTchZsrFesAAAAA:
5bJNguAnRfn4BOjlCtmGvjiQT0Mkk3CE1By9JsrGrDIT0qY-CUKLUwVROkHB9S
_kUx6mtH-nc74
Steil, J. J. (2004). Backpropagation-decorrelation: online recurrent learning with o (n)
complexity. In 2004 ieee international joint conference on neural networks (ieee
cat. no. 04ch37541) (V ol. 2, pp. 843–848).
Stengel, R. F. (1994). Optimal control and estimation. Courier Corporation.
Still, S., & Precup, D. (2012). An information-theoretic approach to curiosity-driven
reinforcement learning. Theory in Biosciences, 131(3), 139–148.
Stuart, G., Spruston, N., Sakmann, B., & Häusser, M. (1997). Action potential initiation
and backpropagation in neurons of the mammalian cns. Trends in Neurosciences,
20(3), 125–131.
Such, F. P., Madhavan, V ., Conti, E., Lehman, J., Stanley, K. O., & Clune, J.
(2017). Deep neuroevolution: Genetic algorithms are a competitive alterna-
tive for training deep neural networks for reinforcement learning. arXiv preprint
arXiv:1712.06567.
Sun, Y ., Gomez, F., & Schmidhuber, J. (2011). Planning to be surprised: Optimal
bayesian exploration in dynamic environments. In International conference on
References 404
artiﬁcial general intelligence (pp. 41–51).
Sussman, G. J., & Wisdom, J. (2015). Structure and interpretation of classical
mechanics. The MIT Press.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences.
Machine Learning, 3(1), 9–44.
Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based
on approximating dynamic programming. In Machine learning proceedings 1990
(pp. 216–224). Elsevier.
Sutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and
reacting. ACM Sigart Bulletin, 2(4), 160–163.
Sutton, R. S. (1996). Generalization in reinforcement learning: Successful examples
using sparse coarse coding. Advances in Neural Information Processing Systems,
1038–1044.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT
press.
Sutton, R. S., Barto, A. G., et al. (1998). Introduction to reinforcement learning
(V ol. 135). MIT press Cambridge.
Tallec, C., & Ollivier, Y . (2017). Unbiased online recurrent optimization.arXiv preprint
arXiv:1702.05043.
Tesauro, G. (1994). Td-gammon, a self-teaching backgammon program, achieves
master-level play. Neural Computation, 6(2), 215–219.
Theodorou, E., & Todorov, E. (2012). Relative entropy and free energy dualities:
Connections to path integral and kl control. In 2012 ieee 51st ieee conference on
decision and control (cdc) (pp. 1466–1473).
Theodorou, J., Evangelosnd Buchli, & Schaal, S. (2010a). A generalized path integral
control approach to reinforcement learning. journal of machine learning research,
11(Nov), 3137–3181.
Theodorou, J., Evangelosnd Buchli, & Schaal, S. (2010b). Reinforcement learning
References 405
of motor skills in high dimensions: A path integral approach. In 2010 ieee
international conference on robotics and automation (pp. 2397–2403).
Tishby, N., Pereira, F. C., & Bialek, W. (2000). The information bottleneck method.
arXiv preprint physics/0004057.
Tishby, N., & Polani, D. (2011). Information theory of decisions and actions. In
Perception-action cycle (pp. 601–636). Springer.
Todorov, E. (2004). Optimality principles in sensorimotor control.Nature Neuroscience,
7(9), 907.
Todorov, E. (2008). General duality between optimal control and estimation. In 2008
47th ieee conference on decision and control (pp. 4286–4292).
Toussaint, M., & Storkey, A. (2006). Probabilistic inference for solving discrete
and continuous state markov decision processes. In Proceedings of the 23rd
international conference on machine learning (pp. 945–952).
Tran, D., Dusenberry, M. W., van der Wilk, M., & Hafner, D. (2018). Bayesian layers:
A module for neural network uncertainty. arXiv preprint arXiv:1812.03973.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling active
inference. In 2020 international joint conference on neural networks (ijcnn) (pp.
1–8).
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020a). Control as hybrid
inference. arXiv preprint arXiv:2007.05838.
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020b). Reinforcement
learning through active inference. arXiv preprint arXiv:2002.12636.
Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and
biases. science, 185(4157), 1124–1131.
Ueltzhöffer, K. (2018). Deep active inference. ,112(6), 547–573. Retrieved 2019-10-19,
from http://arxiv.org/abs/1709.02341 doi: 10.1007/s00422-018-0785-7
Van Merriënboer, B., Breuleux, O., Bergeron, A., & Lamblin, P. (2018). Automatic
differentiation in ml: Where we are and where we should be going. In Advances
References 406
in neural information processing systems (pp. 8757–8767).
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polo-
sukhin, I. (2017). Attention is all you need. In Advances in neural information
processing systems (pp. 5998–6008).
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., . . .
others (2019). Grandmaster level in starcraft ii using multi-agent reinforcement
learning. Nature, 575(7782), 350–354.
Vulkan, N. (2000). An economist’s perspective on probability matching. Journal of
economic surveys, 14(1), 101–118.
Wainwright, M. J., & Jordan, M. I. (2008). Graphical models, exponential families,
and variational inference. Now Publishers Inc.
Walsh, K. S., McGovern, D. P., Clark, A., & O’Connell, R. G. (2020). Evaluating the
neurophysiological evidence for predictive processing as a model of perception.
Annals of the New York Academy of Sciences, 1464(1), 242.
Wan, E. A., & Van Der Merwe, R. (2000). The unscented kalman ﬁlter for nonlinear
estimation. In Proceedings of the ieee 2000 adaptive systems for signal processing,
communications, and control symposium (cat. no. 00ex373) (pp. 153–158).
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N. (2016).
Dueling network architectures for deep reinforcement learning. In International
conference on machine learning (pp. 1995–2003).
Watanabe, E., Kitaoka, A., Sakamoto, K., Yasugi, M., & Tanaka, K. (2018). Illusory
motion reproduced by deep neural networks trained for prediction. Frontiers in
Psychology, 9, 345.
Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279–292.
Watter, M., Springenberg, J. T., Boedecker, J., & Riedmiller, M. (2015). Embed to
control: A locally linear latent dynamics model for control from raw images.
arXiv preprint arXiv:1506.07365.
Weilnhammer, V ., Stuke, H., Hesselmann, G., Sterzer, P., & Schmack, K. (2017). A
References 407
predictive coding account of bistable perception-a model-based fmri study. PLoS
Computational Biology, 13(5), e1005536.
Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin
dynamics. In Proceedings of the 28th international conference on machine
learning (icml-11) (pp. 681–688).
West, R. F., & Stanovich, K. E. (2003). Is probability matching smart? associations
between probabilistic choices and cognitive ability. Memory & Cognition, 31(2),
243–251.
Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation
algorithm in a predictive coding network with local hebbian synaptic plasticity.
Neural Computation, 29(5), 1229–1262.
Wiener, N. (2019). Cybernetics or control and communication in the animal and the
machine. MIT press.
Williams, D. (2018). Predictive processing and the representation wars. Minds and
Machines, 28(1), 141–172. Retrieved from https://link.springer.com/
article/10.1007/s11023-017-9441-6
Williams, D. (2020). Is the brain an organ for prediction error minimization?
Williams, G., Aldrich, A., & Theodorou, E. (2017). Model predictive path integral
control: From theory to parallel computation. Journal of Guidance, Control, and
Dynamics, 40(2), 344–357.
Williams, G., Drews, P., Goldfain, B., Rehg, J. M., & Theodorou, E. (2016). Aggressive
driving with model predictive path integral control. In 2016 ieee international
conference on robotics and automation (icra) (pp. 1433–1440).
Williams, G., Wagener, N., Goldfain, B., Drews, P., Rehg, J. M., Boots, B., & Theodorou,
E. (2017). Information theoretic mpc for model-based reinforcement learning.
In 2017 ieee international conference on robotics and automation (icra) (pp.
1714–1721).
Williams, R. J., & Zipser, D. (1989a). Experimental analysis of the real-time recurrent
References 408
learning algorithm. Connection Science, 1(1), 87–111.
Williams, R. J., & Zipser, D. (1989b). A learning algorithm for continually running
fully recurrent neural networks. Neural Computation, 1(2), 270–280.
Wolpert, D. M. (1997). Computational approaches to motor control.Trends in Cognitive
Sciences, 1(6), 209–216.
Xiao, H., Rasul, K., & V ollgraf, R. (2017a). Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xiao, H., Rasul, K., & V ollgraf, R. (2017b). Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms.
Yedidia, J. S. (2011). Message-passing algorithms for inference and optimization.
Journal of Statistical Physics, 145(4), 860–890.
Yedidia, J. S., Freeman, W. T., & Weiss, Y . (2001). Generalized belief propagation. In
Advances in neural information processing systems (pp. 689–695).
Yedidia, J. S., Freeman, W. T., & Weiss, Y . (2005). Constructing free-energy approxi-
mations and generalized belief propagation algorithms. IEEE Transactions on
information theory, 51(7), 2282–2312.
Yuan, R., & Ao, P. (2012). Beyond itô versus stratonovich. Journal of Statistical
Mechanics: Theory and Experiment, 2012(07), P07010.
Yuan, R., Ma, Y ., Yuan, B., & Ao, P. (2010). Constructive proof of global lyapunov
function as potential function. arXiv preprint arXiv:1012.2721.
Yuan, R., Ma, Y ., Yuan, B., & Ao, P. (2011). Potential function in dynamical systems
and the relation with lyapunov function. In Proceedings of the 30th chinese
control conference (pp. 6573–6580).
Yuan, R., Tang, Y ., & Ao, P. (2017). Sde decomposition and a-type stochastic interpre-
tation in nonequilibrium processes. Frontiers of Physics, 12(6), 1–9.
Zago, M., McIntyre, J., Senot, P., & Lacquaniti, F. (2008). Internal models and
prediction of visual gravitational motion. Vision Research, 48(14), 1532–1538.
Zenke, F., & Ganguli, S. (2018). Superspike: Supervised learning in multilayer spiking
References 409
neural networks. Neural Computation, 30(6), 1514–1541.
Zwanzig, R. (2001). Nonequilibrium statistical mechanics. Oxford University Press.