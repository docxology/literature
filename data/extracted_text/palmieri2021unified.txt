A Uniﬁed View of Algorithms for Path Planning
Using Probabilistic Inference on Factor Graphs
Francesco A. N. Palmieri1 Krishna R. Pattipati2 Giovanni Di Gennaro1
Giovanni Fioretti1 Francesco Verolla1 Amedeo Buonanno3
1Dipartimento di Ingegneria
Università degli Studi della Campania “Luigi Vanvitelli”
Aversa (CE), Italy
{francesco.palmieri, giovanni.digennaro}@unicampania.it
{giovanni.fioretti, francesco.verolla}@studenti.unicampania.it
2Department of Electrical and Computer Engineering
University of Connecticut
Storrs (CT), USA
krishna.pattipati@uconn.edu
3ENEA
Department of Energy Technologies and Renewable Energy Sources
Portici (NA), Italy
amedeo.buonanno@enea.it
Abstract
Even if path planning can be solved using standard techniques from dynamic
programming and control, the problem can also be approached using probabilistic
inference. The algorithms that emerge using the latter framework bear some
appealing characteristics that qualify the probabilistic approach as a powerful
alternative to the more traditional control formulations. The idea of using estimation
on stochastic models to solve control problems is not new and the inference
approach considered here falls under the rubric of Active Inference (AI) and
Control as Inference (CAI). In this work, we look at the speciﬁc recursions that
arise from various cost functions that, although they may appear similar in scope,
bear noticeable differences, at least when applied to typical path planning problems.
We start by posing the path planning problem on a probabilistic factor graph, and
show how the various algorithms translate into speciﬁc message composition rules.
We then show how this uniﬁed approach, presented both in probability space and
in log space, provides a very general framework that includes the Sum-product,
the Max-product, Dynamic programming and mixed Reward/Entropy criteria-
based algorithms. The framework also expands algorithmic design options for
smoother or sharper policy distributions, including generalized Sum/Max-product
algorithm, a Smooth Dynamic programming algorithm and modiﬁed versions of
the Reward/Entropy recursions. We provide a comprehensive table of recursions
Preprint. Under review.
arXiv:2106.10442v1  [cs.LG]  19 Jun 2021
and a comparison through simulations, ﬁrst on a synthetic small grid with a single
goal with obstacles, and then on a grid extrapolated from a real-world scene with
multiple goals and a semantic map.
1 Introduction
Probabilistic methods for modeling the dynamic behavior of agents navigating in complex environ-
ments are becoming increasingly popular in the recent literature. In many applications, the recognition
that a stochastic control problem can be solved through probabilistic inference on a generative model,
has sparked new growing interest. The idea that an agent can use its best inference about the future
based on its approximate knowledge of the environment, to condition its present actions, can be a
quite powerful paradigm. Dynamic Programming (DP) algorithms for Markov Decision Processes
(MDP) and Partially-Observable Markov Decision Processes (POMDP) (Thrun et al., 2006; Bertsekas,
2019) are based on this concept, suggesting that an agent can act optimally following a best behaviors
derived from a value function back-propagated from the hypothetical future. Analogies between
control and estimations can be traced back to the work of Kalman (Todorov, 2008) and to more recent
attempts to see probabilities and rewards under the same framework (Kappen et al., 2012; Levine,
2018).
In path planning, since the original paper by Attias (2003), there has been a growing body of literature
in trying to clarify the connections between the probabilistic methods and more traditional stochastic
control strategies (Toussaint and Storkey, 2006; Toussaint, 2009; Levine, 2018). Also the terms
Active Inference (AI), Control as Inference (CAI), have been recently coined (Verbelen et al., 2020)
with some of these models based on the so-called free-energy principle (Buckley et al., 2017; Parr and
Friston, 2018b), on KL-learning (Alexandre et al., 2018; Parr and Friston, 2018a; Kaplan and Friston,
2018), and on Max entropy (Ziebart et al., 2010). Furthermore, intriguing connections have been
drawn, for some of these methods, to neuroscience and brain theory (Baltieri and Buckley, 2017).
Causal reasoning (Nair et al., 2019) also seems to share some elements of this goal-directed behavior.
In our present work, we try to go beyond the now well-recognized analogies between probability
and control, because even if some of the resulting algorithms look similar, they may bear marked
differences when applied to real world scenarios. In Reinforcement Learning (RL) (Sutton and Barto,
2018), for example, often we have only partial knowledge of the environment via an approximate
stochastic model and we need to perform further exploration. But how do we choose among the
various algorithms, which are typically introduced for known system dynamics, when they are used
on temporary knowledge? It is often argued that stochastic policy methods may be more appropriate
during exploration, because they can easily encompass uncertainties and allow cautious behavior
(Levine, 2018). However, there are still many open questions on the exploration-exploitation issues
and even more fundamentally there is still a lack of general view on this powerful paradigm, even
with known dynamics.
In some of our previous works, we have proposed various techniques for modeling the motion
behaviors of pedestrians and ships (Castaldo et al., 2014; Coscia et al., 2016; Coscia et al., 2018;
Coscia et al., 2018a,b). More recently, however, in experimenting with probability propagation
methods for determining the best path (Palmieri et al., 2021), we came to realize that the probabilistic
algorithms may be the most promising approaches for agile modeling of intelligent agent motion in
complex scenes.
In this work, we assume that the system’s stochastic transition function is known and that both the
state and the action spaces are discrete ﬁnite sets that can be handled with tabular methods. Extensions
to continuous space can be considered with approximations to the value function, but they will not be
addressed here. Also, we do not address learning here, because we believe that a uniﬁed view on
the various cost functions and recursions with known stochastic system dynamics should be the ﬁrst
step in trying to understand the more challenging RL adaptation rules. More speciﬁcally, focusing on
the path planning problem on a discrete grid, and assuming that the system dynamics are stochastic
and known to the agent, we can systematically compare the value functions and the corresponding
optimal policies that result from various recursions. Standard probability message propagation, such
as the Sum-product and the Max-product algorithms (Barber, 2012), are compared to DP using a
uniﬁed view together with other methods based on joint Reward/Entropy maximization (Verbelen
et al., 2020; Levine, 2018; Ziebart et al., 2009; Millidge et al., 2020; Imohiosen et al., 2020). To
2
our knowledge, no comprehensive comparison exists in the literature, and our contribution aims at
providing the reader with a ready-to-use suite of algorithms derived withing a unifying framework.
The basic idea of this work is to show how we can map the stochastic problem to a factor graph
where different methods correspond to different propagation rules through some of the graph’s blocks.
Information can travel in the system both in the probability space and in the log-space; this allows
us to include and generalize the previously proposed methods to new suites of algorithms, thereby
increasing our options to condition the agent’s policy distribution.
In this paper, we use directed Factor Graphs (FG), that assign variables to edges and factors to
interconnected blocks. Generally speaking, message propagation in FG is more easily handled in
comparison to propagation in graphs in which the variables are in the nodes (Koller and Friedman,
2009). Further reduction of the burden of deﬁning message composition rules can be achieved
using Factor Graphs in normal form (FGn), proposed by Forney (2001) (Loeliger, 2004). A FGn
conveniently includes junction nodes (equality constraint nodes) that split incoming and outgoing
messages when variables are shared by multiple factors. We have proposed a small modiﬁcation to
the FGn in our Factor Graph in Reduced normal form (FGrn) (Palmieri, 2016) by including shaded
blocks that map single variables to joint spaces. In fact, in an oriented graph, when a variable has more
than one parent, proper forward and backward messages have to go through the parents’ joint space
(married parents). In a FGrn, the shaded blocks describe this passage and allow a unique deﬁnition
of message propagation rules through Single-Input/Single-Output (SISO) blocks. Computational
complexity issues in some FGrn architectures have been addressed in (Di Gennaro et al., 2021). In
the standard sum-product algorithm, backward propagation through shaded blocks corresponds to
marginalization. We will show in this paper how this operation can be generalized and how it plays a
crucial role in the different path planning algorithms. As mentioned above, we conﬁne ourselves here
to discrete variables. However, factor graphs that propagate continuous distributions are possible and
may be devised also for path planning. Gaussian messages have been introduced in Loeliger et al.
(2007) and have already been used for Kalman ﬁlter-based tracking in (Castaldo and Palmieri, 2015)
using FGrn. This issue will not be addressed here and will be the subject of future work.
The contributions of this paper can be summarized as follows:
1. The path planning problem is mapped to a Factor Graph in Reduced Normal Form.
2. Various algorithms, such as the Sum-product, the Max-product, DP and Reward/Entropy maxi-
mization (the latter related to structural variational inference), are included in the same framework,
both in probability and in log spaces. They are all derived using different cost functions, but they are
all reduced to speciﬁc propagation rules through some of the FGrn blocks.
3. The equivalent Q-functions and V-functions in the probability space, seen as alternatives to the
well-known DP formulation, allows us to write the policy distribution for all the algorithms with a
unique expression.
4. Using this general framework, we extend some of the known algorithms to a whole suite of
new parametric updates that can control the smoothness in the policy distributions. These proposed
parametric updates can be used to balance exploration and exploitation in reinforcement learning.
5. We provide simulations, ﬁrst on a small grid with one goal and obstacles, then on a larger
grid extracted from a real scene with multiple goals (exits) and a semantic map. The results show
some marked differences in : (a) the speed of converge to the steady-state value function, where
probabilistic methods are clearly favored; (b) how the Max-product algorithm may be preferred
for its faster convergence and for the shape and smoothness of its value functions; (c) how various
algorithms can be controlled with parametric updates to exhibit different smoothness in their policy
distributions.
We believe that our contribution in this paper may prove useful for further deployment of RL
algorithms, especially when the environment is not completely known and exploration and exploitation
have to be properly balanced on the basis of partial model knowledge.
Outline of the paper: In Section 2, we present the bayesian model and the corresponding factor
graph. In Section 3, the Sum-product algorithm is discussed in the framework of FGrn with the
message composition rules and the updates. The inferences are presented both in a parallel and in a
progressive version of the message composition. In Section 4, the maximum a posteriori solution of
the Max-product algorithm is analyzed with our proposed Sum/Max-product algorithm described in
3
Section 5. Dynamic programming is translated into this framework in Section 6 and our proposals for
a generalized SoftDP are in Section 7. The approaches to combined maximum reward and entropy are
discussed in Section 8, where a cost function that includes smooth generalizations is also discussed.
Simpliﬁcations of some of the recursions when the system equations are deterministic, are discussed
in Section 9. The extension to inﬁnite horizon models and considerations on the steady-state solutions
are included in Section 10. Simulations are conducted on two types of grids in Section 11 and
conclusions and suggestions for further research are in Section 12. The analysis of the soft-max
functions used in the paper are in Appendix A, and the proofs for the reward/entropy methods are in
Appendix B.
Figure 1: State-Action Model as a Bayesian graph
2 The Bayesian Model
Figure 1 shows the state-action model as a Bayesian graph where {St}is the state sequence, {At}is
the action sequence. We assume, without loss of generality, that both sequences belong to discrete
ﬁnite sets: At ∈A and St ∈S. The reward/outcome sequence {Ot}is binary with Ot ∈{0,1}.
The model evolves over a ﬁnite horizon T and the joint probability distribution of the state-action-
outcome sequence corresponds to the factorization1
p(s1a1o1 ...s TaToT) =p(oT|sTaT)p(s1)p(aT)
T−1∏
t=1
p(st+1|stat)p(at)p(ot|stat),
where the function p(st+1|stat) describes the system dynamics, p(at) are the action priors and
p(ot|stat) are the reward/priors on the state-action pairs. More speciﬁcally, we assume that
P(Ot = 1|stat) ∝c(stat) ≥0; P(Ot = 0|stat) ∝U(stat),
where the function c(stat) acts as a prior distribution on the pair (stat), only if Ot = 1. When
Ot = 0, no prior information is available on that state-action pair, and the factor becomes the uniform
distribution U(stat).2
This formulation allows the introduction of a reward function as
R(stat) = logc(stat) +K, (1)
where K is an arbitrary positive constant. The value K is really irrelevant because going back to
probabilities we have
c(stat) ∝eR(stat)−K,
with the constant that disappears after normalization. We can set Kto a large value if we do not like
to handle the negative rewards that we get from the log function for K = 0. In the following, without
loss of generality, we assume that all our rewards are negative (K = 0).
1Even if the notation should have capital letters for random variables as subscripts and lower case letters for
their values in the functions, as in
pS1A1O1...STATOT(s1a1o1 . . . sT aT oT ); pSt+1|StAt(st+1|stat); pOt|StAt(ot|stat),
we use a compact notation with no subscripts when there is no ambiguity. In some of the messages that follow
we include the subscripts only when necessary.
2In our deﬁnition, we assume that c(stat) is normalized to be a valid pdf, even if it is well-known that, in
performing inference in a probabilistic graph, normalization is irrelevant.
4
The introduction of the sequence{Ot}has often been proposed for connecting rewards to probabilities
(Kappen et al., 2013; Levine, 2018). We would like to emphasize that interpreting the factors c(stat)
as prior information in the probability factorization, may solve, at least for planning problems, the
well-known issue of deﬁning an appropriate reward function. In fact, in a practical problem, we may
have available statistics on how often a state is visited and how certain actions may be more likely
than others.
Note that when a state-action pair has zero probability, for example with forbidden states, or impos-
sible actions, obstacles, etc., the reward function takes value −∞. This is really not a problem in
practice, because we can easily approximate such a value with a large negative number.
Note that in the model we have included a separate factor p(at) for the priors on At, even if such
information could be included in c(stat). We have preferred, to be consistent with the Bayesian
graph of Figure 1, to keep the two factors separate, one for marginal action priors and one for joint
priors (rewards).
Omitting the sequence {Ot}in the notation, or equivalently assuming that prior information is always
available, the factorization is more compactly written as
p(s1a1...sTaT) =c(sTaT)p(s1)p(aT)
T−1∏
t=1
p(st+1|stat)p(at)c(stat). (2)
This is without loss of generality because, for some values of t, the distribution c(stat) can just be
taken to be uniform.
Figure 2: State-Action Model for T = 4as a Factor Graph in Reduced normal form. The one-edge
blocks are sources (priors); the two-edge white blocks represent the system dynamics; the shaded
blocks map single variables to their joint space; the diverters connect the variables constrained to be
equal.
2.1 The factor graph
Inference in a graphical model is more easily handled with reference to an equivalent factor graph,
where variables are on the branches and factors are in the blocks. We use here Factor Graphs in
Reduced normal form (FGrn) (Palmieri, 2016). Factor graphs in normal form (FGn) have been
introduced by Forney (2001) and discussed by Loeliger (2004), mostly for coding problems. They
allow the probability factorization to be written in terms of a graph in which variables are edges and
factors are nodes with diverters (equality constraints nodes), that multiplex messages to the other
factors. Since, in a FGn, a factor can still have multiple inputs, we impose a further simpliﬁcation
in the FGrn by transforming the graph into a conﬁguration that includes shaded blocks (marginaliz-
ers/expanders); this reduces the graph to a composition of only SISO (Single-Input-Single-Output)
blocks and diverters. FGrn provide a very detailed display of the message propagation ﬂow and have
a simple unique formulation in terms of message propagation rules.
Figure 2 shows the factor graph in reduced normal form for the Bayesian model of Figure 1 forT = 4.
The prior distributions p(at) and c(stat) are in the source nodes and the dynamics p(st+1|stat)
are in SISO blocks. The junctions describe equality constraints, and the shaded blocks describe
the mapping from single variables’ space to a joint space, i.e. p(stat|a′
t) = U(st)δ(at −a′
t) and
p(stat|s′
t) = δ(st −s′
t)U(at). Essentially, in a shaded block, the input variable is copied to the
output and joined to the other variable that, in that branch, carries no information in the forward
direction. Each branch has a direction and a forward and a backward message associated to it. Just as
in any belief propagation network, all messages are proportional to probability distributions and their
5
composition rules allow the agile derivation of inference algorithms. Note how the replicas (StAt)i,
i= 1 : 4, of the same variable (StAt) around the diverter block have different names and messages
associated with them.
We will see how the path planning problem has a unique formulation on the factor graph. By changing
the propagation rules for some of the blocks, we obtain the optimal solutions for the various problem
formulations.
2.2 Introducing constraints
One of the main advantages of studying inference problems on graphs using messages, is that problem
constraints are easily included in the ﬂow. Furthermore, we have assumed a ﬁnite time segment
t= 1 :T, but this model may as well represent a segment t= t0 + 1 :t0 + T of a longer process,
where we have freedom to introduce the initial and ﬁnal conditions in the forward messages at the
beginning, and in the backward messages at the end, respectively.
For example: (a) A known starting state S1 = s1 can be included as a forward message fS1 (s1) =
δ(s1 −s1), where δ(x) = 1if x = 0, and 0 else; (b) If we have no prior information on S1, we
set fS1 (s1) =U(s1); (c) Knowledge of the initial action A1 = a1 can be included as fA1 (a1) =
δ(a1 −a1); (d) Knowledge of the ﬁnal state (only) ST = sT is b(STAT)4 (sTaT) = δ(sT −
sT)U(aT); (e) Knowledge of the state at timet0 may be included as fSt0 (st0 ) =δ(st0 −st0 ); (f) In a
planning problem, a known map m(st) can be associated to the factor c(stat) with f(StAt)3 (stat) ∝
m(st)U(at); (g) In the same planning problem, joint map-action information can injected as the
message f(StAt)3 (stat) ∝c(stat); if action and map are independent, and the action prior is p(at),
f(StAt)3 (stat) ∝m(st)p(at), or equivalently f(StAt)3 (stat) ∝m(st)U(at) and fAt(at) =p(at);
etc.
We denote collectively all the constraints available on the joint model asK1:T, with the joint model
written in compact form as p(s1a1 ...s TaT|K1:T).
2.3 Inference objectives
Our inference aims at providing solutions to one or more of the following problems:
1. Find the best state sequence (S): s∗
1 →s∗
2 →···→ s∗
T
2. Find the best action sequence (A): a∗
1 →a∗
2 →···→ a∗
T
3. Find the best joint state-action sequence (SA) : (s1a1)∗→(s2a2)∗→...→(sTaT)∗
4. Find the best state-action sequence (SASA): s∗
1 →a∗
1 →s∗
2 →a∗
2 →···→ s∗
T →a∗
T
5. Find the best action-state sequence (ASAS): a∗
1 →s∗
1 →a∗
2 →s∗
2 →···→ a∗
T →s∗
T
6. Find the best policy distributions (P): π∗(a1|s1),π∗(a2|s2) ...π ∗(aT|sT)
We will see in the following how various cost functions determine the message composition rules
across the blocks to solve the above problems both in the probability space and in the log-space and
according to different optimization criteria. In the following discussion, we will concentrate mostly
on the S sequence, on the SASA sequence and on the policy distribution, since the extensions to A,
SA, and ASAS sequences are straightforward.
3 Marginalization and the Sum-product
Standard inference in Bayesian model consists in marginalizing the total joint probability distribution
to obtain distributions that are proportional to the posteriors on single variables (Koller and Friedman,
2009). More speciﬁcally, for states only, for actions only and for states and actions jointly, we want
6
to compute the posteriors as
p(st|K1:T) ∝
∑
sj,j̸=t,j=1:T
aj,j=1:T
p(s1a1 ...s TaT|K1:T),
p(at|K1:T) ∝
∑
sj,j=1:T
aj,j̸=t,j=1:T
p(s1a1 ...s TaT|K1:T),
p(stat|K1:T) ∝
∑
sj,aj,j̸=t,j=1:T
p(s1a1 ...s TaT|K1:T).
(3)
The policy distributions are obtained by ﬁxing the statest at time t, and accounting for the foreseeable
future until T
π∗(at|st) ≜ p(at|st,Kt:T) =p(stat|Kt:T)
p(st|Kt:T) , t = 1 :T. (4)
The policy distribution describes at time thow likely it is to take action at from state st, given all the
available information (constraints, priors, etc.) about the future (Kt:T).
All the above functions can be obtained using forward and backward message propagation using the
Sum-product rule (Koller and Friedman, 2009; Barber, 2012). This approach essentially averages
over the variables that are eliminated across each SISO block. With reference to Figure 2, just by
following the ﬂow, for some of the backward messages we have
b(StAt)4 (stat) ∝∑
st+1 p(st+1|stat)bSt+1 (st+1),
b(StAt)1 (stat) ∝p(at)c(stat)b(StAt)4 (stat),
b(StAt)2 (stat) ∝f(StAt)1 (stat)c(stat)b(StAt)4 (stat),
bAt(at) =∑
st b(StAt)2 (stat),
bSt(st) =∑
at b(StAt)1 (stat).
(5)
For some of the forward messages
fSt+1 (st+1) ∝∑
stat p(st+1|stat)f(StAt)4 (stat),
f(StAt)1 (stat) =fSt(st)U(at),
f(StAt)2 (stat) =U(st)fAt(at),
f(StAt)4 (stat) ∝f(StAt)1 (stat)f(StAt)2 (stat)c(stat).
(6)
Note that going backward through a block, the message may not be normalized. Around the
diverters, outgoing messages are the product of the incoming ones, and are not normalized. Message
composition rules are summarized in Tables 3, 4, 5, 6 and 7 for the Sum-product and for the other
approaches that will be discusses later.
Posterior distributions are obtained by taking the product of forward and backward messages
p(st|K1:T) ∝fSt(st)bSt(st),
p(at|K1:T) ∝fAt(at)bAt(at),
p(stat|K1:T) ∝f(StAt)i(stat)b(StAt)i(stat), for any i= 1,2,3,4.
(7)
For readers not too familiar with probability message propagation, we would like to emphasize that
this framework is a rigorous application of Bayes’ theorem and marginalization. Also all messages
can be normalized to be valid distributions, even if it is not strictly necessary (it is their shape that
matters). However, it is often advised to keep messages normalized for numerical stability.
The policy distribution at each tis derived as a consequence of the inference obtained from the
probability ﬂow and is
π∗(at|st) ∝f(StAt)1 (stat)b(StAt)1 (stat)
fSt(st)bSt(st) = fSt(st)U(at)b(StAt)1 (stat)
fSt(st)bSt(st) = b(StAt)1 (stat)
bSt(st) , (8)
where we have used the branch with i = 1. It is easy to verify that the solution would have an
equivalent expression for any other branch. Note also how the policy depends only on the backward
messages. The reason for this is that by conditioning on st, all the information coming from the left
side of the graph is blocked.
7
3.1 Max Posterior sequences
Optimal sequence values can be obtained in parallel using maximization on the posteriors
s∗
t = argmax
st
p(st|K1:T) = argmax
st
fSt(st)bSt(st),
a∗
t = argmax
at
p(at|K1:T) = argmax
at
fAt(at)bAt(at), t = 1 :T
(stat)∗= argmax
stat
p(stat|K1:T) = argmax
stat
f(StAt)1 (stat)b(StAt)1 (stat),
(9)
The max posterior solutions are taken separately on each variable and, even if they are often used in
the applications (for example in decoding convolutional codes - the algorithm, is named BCJR after
its authors Bahl et al. (1974)), they may provide unsatisfactory sequences for path planning. In fact,
the sequences that result from the maximizations are unconstrained in time and may correspond to
disconnected paths (Palmieri et al., 2021).
3.2 Progressive Max Posterior sequences
Better solutions for the max posterior approach are obtained progressively in time following a forward
procedure.3
For the states-only (S) sequence
s∗
1 = argmax
s1
p(s1|K1:T) = argmax
s1
fS1 (s1)bS1 (s1),
s∗
2 = argmax
s2
p(s∗
1s2|K2:T) = argmax
s2
fS2 (s2|s∗
1)bS2 (s2),
s∗
3 = argmax
s3
p(s∗
1s∗
2s3|K3:T) = argmax
s3
fS3 (s3|s∗
2)bS3 (s3),
···
s∗
t = argmax
st
p(s∗
1...s∗
t−1st|Kt:T) = argmax
st
fSt(st|s∗
t−1)bSt−1 (st−1),
···
(10)
where the conditioned forward messages come from a one-step propagation
fSt(st|s∗
t−1) =
∑
at−1
p(st|s∗
t−1at−1)f(St−1At−1)4 (s∗
t−1at−1)
=
∑
at−1
p(st|s∗
t−1at−1)p(at−1)c(s∗
t−1at−1). (11)
Note on the graph that knowledge of the state at time t−1 "breaks" the forward ﬂow. Only the
backward ﬂow drives the inference.
Similarly, for the best State-Action (SASA) sequence, the Progressive Max-posterior algorithm using
the messages on the graph is
s∗
1 = argmax
s1
p(s1|K1:T) = argmax
s1
fS1 (s1)bS1 (s1),
a∗
1 = argmax
a1
p(s∗
1a1|K1:T) = argmax
a1
fA1 (a1)bA1 (a1|s∗
1),
s∗
2 = argmax
s2
p(s∗
1a∗
1s2|K2:T) = argmax
s2
fS2 (s2|s∗
1a∗
1)bS2 (s2),
a∗
2 = argmax
a2
p(s∗
1a∗
1s∗
2a2|K2:T) = argmax
a2
fA2 (a2)bA2 (a2|s∗
2),
···
s∗
t = argmax
st
p(s∗
1a∗
1...s∗
t−1a∗
t−1st|Kt:T) = argmax
st
fSt(st|s∗
t−1a∗
t−1)bSt(st),
a∗
t = argmax
at
p(s∗
1a∗
1...s∗
t−1a∗
t−1s∗
tat|Kt:T) = argmax
at
fAt(at)bAt(at|s∗
t),
···
(12)
3On a ﬁxed time horizon, a similar procedure can be derived going backward in time. We prefer to maintain
the framework causal and leave it out for brevity.
8
where the conditioned forward and backward messages mean that we have considered their values
when the conditioning variables on the left side of the graph are ﬁxed. For the conditioned forward
we have
fSt(st|s∗
t−1a∗
t−1) =p(st|s∗
t−1a∗
t−1). (13)
For the conditioned backward we have
bAt(at|s∗
t) ∝b(StAt)2 (s∗
tat) ∝c(s∗
tat)b(StAt)4 (s∗
tat)f(StAt)1 (s∗
tat). (14)
that since
b(StAt)1 (stat) ∝b(StAt)4 (stat)f(StAt)2 (stat)c(stat) =b(StAt)4 (stat)p(at)U(st)c(stat), (15)
or
b(StAt)4 (stat) ∝b(StAt)1 (stat)
p(at)c(stat) , (16)
can be rewritten as
bAt(at|s∗
t) ∝b(StAt)1 (s∗
tat)
p(at) f(StAt)1 (s∗
tat) =b(StAt)1 (s∗
tat)
p(at) δ(st −s∗
t)U(at) =b(StAt)1 (s∗
tat)
p(at) .
(17)
Therefore, the SASA estimation simpliﬁes to
s∗
1 = argmax
s1
fS1 (s1)bS1 (s1),
a∗
1 = argmax
a1
b(S1A1)1 (s∗
1a1),
s∗
2 = argmax
s2
p(s2|s∗
1a∗
1)bS2 (s2),
a∗
2 = argmax
a2
b(S2A2)1 (s∗
2a2),
···
s∗
t = argmax
st
p(st|s∗
t−1a∗
t−1)bSt(st),
a∗
t = argmax
at
b(StAt)1 (s∗
tat),
···
(18)
which is obviously the same if we use explicitly the optimal policy distribution (8)
s∗
1 = argmax
s1
p(s1|K1:T) = argmax
s1
fS1 (s1)bS1 (s1),
a∗
1 = argmax
a1
π∗(a1|s∗
1),
s∗
2 = argmax
s2
p(s2|s∗
1a∗
1)bS2 (s2),
a∗
2 = argmax
a2
π∗(a2|s∗
2),
···
s∗
t = argmax
st
p(st|s∗
t−1a∗
t−1)bSt(st),
a∗
t = argmax
at
π∗(at|s∗
t),
···
(19)
Note in all cases the crucial role played by the backward ﬂow. We have successfully demonstrated this
approach for path planning in our previous work (Palmieri et al., 2021). In fact, in the progressive max
posterior algorithm, the forward ﬂow is not necessary. Action-only sequences and ASAS sequence
can be obtained in a similar fashion and are omitted here for brevity.
3.3 Sum-product in the log-space
We have seen above how in the factorized model (2), prior distributions are related to rewards via the
log transformation in (1). For the comparisons that follow, it is convenient to consider also some of
the Sum-product recursions in the log-space. We deﬁne the following functions
Q(StAt)i(stat) ≜ log b(StAt)i(stat), i = 1,2,3,4
VSt(st) ≜ log bSt(st), (20)
9
Table 1: Summarized backup rules in probability space with b(stat) ≜ b(StAt)1 (stat); b(st) ≜
bSt(st); c′(stat) ≜ p(at)c(stat).
b(stat) b(st)
Sum product c′(stat)
∑
st+1
p(st+1|stat)b(st+1)
∑
at
b(stat)
Max product c′(stat) max
st+1
p(st+1|stat)b(st+1) max
at
b(stat)
Sum/Max product
(α≥1) c′(stat) α
√∑
st+1
p(st+1|stat)αb(st+1)α α
√∑
at
b(stat)α
DP c′(stat)e
∑
st+1 p(st+1|stat) logb(st+1) max
at
b(stat)
Max-Rew/Ent
(α> 0) c′(stat)e
∑
st+1 p(st+1|stat) logb(st+1)
α
√∑
at
b(stat)α
SoftDP (β >0) c′(stat)e
∑
st+1 p(st+1|stat) logb(st+1)
e
∑
atb(stat)βlog b(stat)
∑
a′
t
b(sta′
t)β
Table 2: Summarized backup rules in log space with Q(stat) ≜ Q(StAt)1 (stat); V(st) ≜ VSt(st);
R′(stat) = logp(at) +R(stat).
Q(stat) V(st)
Sum product R′(stat) + log
∑
st+1
elog p(st+1|stat)+V(st+1) log
∑
at
eQ(stat)
Max product R′(stat) + max
st+1
(log p(st+1|stat) +V(st+1)) max
at
Q(stat)
Sum/Max product
(α≥1) R′(stat) +1
αlog
∑
st+1
eα(log p(st+1|stat)+V(st+1)) 1
αlog
∑
at
eαQ(stat)
DP R′(stat) +
∑
st+1
p(st+1|stat)V(st+1) max
at
Q(st,at)
Max-Rew/Ent
(α> 0) R′(stat) +
∑
st+1
p(st+1|stat)V(st+1) 1
αlog
∑
at
eαQ(stat)
SoftDP (β >0) R′(stat) +
∑
st+1
p(st+1|stat)V(st+1)
∑
at Q(stat)eβQ(stat)
∑
a′
t
eβQ(sta′
t)
Note that there is a Qfunction for each message around the diverter. The choice of the notations
Q(Q-function) and V (Value-function) is not casual, as it will be clear in the discussion that will
follow on dynamic programming. There is also a deﬁnition of V-function for the action variable At,
VAt(at) ≜ log bAt(at). From the deﬁnition, is obvious that both the Q- and V-functions are negative
(we have already pointed out above that this is not a limitation). We concentrate here mostly on the
state St for which the backward recursions in (5), are written in the log-space as
Q(StAt)4 (stat) ∝log ∑
st+1 p(st+1|stat)eVSt+1 (st+1),
Q(StAt)1 (stat) ∝log p(at) +R(stat) +Q(StAt)4 (stat),
VSt(st) ∝log ∑
at eQ(StAt)1 (stat).
(21)
All messages can be propagated in the log-space: the product rule around the diverter of Figure 2
becomes a sum and the backward propagation rules across the dynamics block and the shaded block
are simply translated. For better comparison with the formulations that follow, we re-write the ﬁrst
10
equation of (21) as
Q(StAt)4 (stat) ∝log
∑
st+1
elog p(st+1|stat)+VSt+1 (st+1). (22)
The recursions are summarized for comparison in the ﬁrst row of Tables 1 and 2.
The same recursions, and some of the deﬁnitions in the log-space, have been reported by Levine
(2018) that also notes how the transformation y= log∑N
j=1 exj is a soft-max (y∼max(x1,...,x N)
when the xis are large), in contrast to the the hard max that is used in dynamic programming.
Appendix A summarizes the properties of the soft-max functions that arise in our analyses.
The best SASA sequence of equations (19) is equivalently written in the log-space as
s∗
1 = argmax
s1
p(s1|K1:T) = argmax
s1
log p(s1) +VS1 (s1),
a∗
1 = argmax
a1
Q(S1A1)1 (a1,s∗
1),
s∗
2 = argmax
s2
log p(s2|s∗
1a∗
1) +VS2 (s2),
a∗
2 = argmax
a2
Q(S2A2)1 (a1,s∗
1),
···
s∗
t = argmax
st
log p(st|s∗
t−1a∗
t−1) +VSt(st),
a∗
t = argmax
at
Q(StAt)1 (a1,s∗
1),
···
(23)
The policy distribution (8) is rewritten as
π∗(at|st) ∝eQ(StAt)1 (stat)−VSt(st). (24)
Figure 3: Backward recursions for the Max-product algorithm for T = 4. Note the presence of the
backward message b(S4A4)4 (s4a4) at the end of the chain that may carry information from further
steps or may represent ﬁnal constraints.
4 Maximum a posteriori and the Max-product
The max posterior rules, described in Section 3.1 and 3.2, are used extensively for inference in
Bayesian networks, even if it is often ignored that they do not necessarily solve the global maximum
a posteriori problem
(s∗
1a∗
1 ...s ∗
Ta∗
T) = argmax
s1a1...sTaT
p(s1a1...sTaT|K1:T). (25)
The Sum-product propagation rules solvemarginal maximum a posteriori problems after averaging on
the eliminated variables, while the global optimization requires a different strategy for obtaining the
11
solution. The Max-product algorithm (Barber, 2012; Loeliger et al., 2007), in propagating messages
in the graph, instead of computing averages across the blocks, propagates maxima values, provides
the solution. This is often named bi-directional Viterbi algorithm (Barber, 2012). The detailed
recursions are derived explicitely in Figure 3 for a model with T = 4in the notation of the factor
graph of Figure 2. At a generic step t, the recursions for some of the backward messages are
b(StAt)4 (stat) = max
st+1
p(st+1|stat)bSt+1 (st+1),
b(StAt)1 (stat) =p(at)c(stat)b(StAt)4 (stat),
bSt(st) = max
at
b(StAt)1 (stat).
(26)
Again the crucial role is played by the backward ﬂow that, going through each SISO block, does
not undergo a summation, but a max (in Max-product bayesian networks also the forward ﬂow is
computed using max rather that sum (Barber, 2012); we focus here mostly on the backward ﬂow). In
the log-space, the backward recursions for the states are rewritten as
Q(StAt)4 (stat) = max
st+1
[
log p(st+1|stat) +VSt+1 (st+1)
]
,
Q(StAt)1 (stat) = logp(at) +R(stat) +Q(StAt)4 (stat),
VSt(st) = max
at
Q(StAt)1 (stat).
(27)
The best SASA sequence is computed in the forward direction in way similar to the Sum-product, in
both the probability space and in the log-space, as follows
At t= 1, we have
s∗
1 = argmax
s1
p(s1)bS1 (s1) = argmax
s1
p(s1)eVS1 (s1) = argmax
s1
log p(s1) +VS1 (s1),
a∗
1 = argmax
a1
b(S1A1)1 (s∗
1a1) = argmax
a1
Q(S1A1)1 (s∗
1a1). (28)
At t= 2
s∗
2 = argmax
s2
p(s2|s∗
1a∗
1)bS2 (s2) = argmax
s2
p(s2|s∗
1a∗
1)eVS2 (s2)
= argmax
s2
log p(s2|s∗
1a∗
1) +VS2 (s2),
a∗
2 = argmax
a2
b(S2A2)1 (s∗
2a2) = argmax
a2
Q(S2A2)1 (s∗
2a2),
(29)
... etc.
Note how the forward recursions are formally identical to the ones derived for the Sum-product
algorithm. Also, the policy has the same formal expression
π∗(at|st) ∝b(StAt)1 (stat)
bSt(st) = eQ(StAt)1 (stat)−VSt(st). (30)
Clearly, the Q- and the V-functions here have a different meaning. The recursions for the best SASA
sequence can be rewritten in terms of policy and they look formally identical to the ones derived for
the Sum-product algorithm. All the other sequences, S, A, SA, ASAS can be computed using the
probability ﬂow in the graph following the same formal approach, both in the Max-product and in the
Sum-product, simply by changing some of the propagation rules. For brevity, we concentrate here
only on some of the messages, but a detailed analysis of other parts of the ﬂow may reveal interesting
aspects of the inference.
Tables 3, 4, 5, 6 and 7 summarize the propagation rules across the factor graph for the Sum-product,
the Max-product and all the other approaches that will follow. The main backup recursions are also
summarized for comparison in Tables 1 and 2 in the log-space.
We would like to emphasize that propagating information via probability distributions includes all the
cases in which there may be deterministic values in the system, i.e., when the distributions are delta
functions. Furthermore, in the Max-product algorithm, when multiple equivalent maxima are present,
the distributions can carry implicitly multiple peaks. We will see, in some of the simulation examples
that will follow, that the Max-product messages provide a complete set of options in the policy
distributions, also when more than one best action is available. Obviously, in writing the algorithms,
some attention must be devoted to unnormalized distributions with values that are close zeros, to
avoid numerical problems. The problem is usually overcame by normalization and by replacing zeros
with very small values.
12
5 The Sum/Max-Product
The unifying view provided by the graphical method, both in the Sum-product and in the Max-product
approaches, is quite appealing and one wonders whether there may be a general rule that encompasses
both. To examine this, by looking at the recursions for the Sum-product algorithm (Tables 1 and 2,
ﬁrst two rows), we immediately observe that the Sum-product, both in the probability and in the log-
space, can be seen as a soft version of the Max-product because of the soft-max functions. Therefore,
we propose a general rule that interpolates between the two solutions using the parametrized soft-max
functions discussed in Appendix A. We name this generalization the Sum/Max-product algorithm,
that in the log-space gives
Q(StAt)4 (stat) =1
αlog
∑
st+1
eα[log p(st+1|stat)+VSt+1 (st+1)],
Q(StAt)1 (stat) = logp(at) +R(stat) +Q(StAt)4 (stat),
VSt(st) =1
αlog
∑
at
eαQ(StAt)1 (stat),
(31)
with α≥1. In probability space, the updates are immediately translated as
b(StAt)4 (stat) ∝

∑
st+1
p(st+1|stat)αbSt+1 (st+1)α


1
α
,
b(StAt)1 (stat) ∝p(at)c(stat)b(StAt)4 (stat),
bSt(st) ∝
[∑
at
b(StAt)1 (stat)α
]1
α
.
(32)
Note that the function that emerges in the probability space recursions, is also a soft-max. Therefor,
both in the log-space and in the probability space, for α→∞, the parametric soft-max functions
converges to the hard max (see Appendix A for details about the soft-max functions). For α= 1,
equations (31) and (32) become identical to those derived for the Sum-product algorithm. The Max-
product approach usually produces much more deﬁned value functions and policies, in comparison to
the Sum-product, as will be shown in some of the examples that follow. Interpolating between the
two solutions may provide the planning problem with a whole range of new solutions beyond the
traditional Sum-product and Max-product approaches. The Sum/Max-product updates are added as
the third row in Tables 1 and 2 and the detailed block propagation rules are included in Tables 3, 4, 5,
6 and 7. The policy is formally the same as in the Sum-product and the Max-product. Evidently, the
messages, both in the probability space and in the log-space, carry different information.
5.1 What function is being optimized?
The generalization of the Sum/Max-product has been derived as a straightforward interpolation
between the Sum-product and the Max-product and such a function can span the whole range of
solutions between the maximization of the marginals of the Sum-product algorithm to the maximiza-
tion of the global posterior of the Max-product. What is then, for each value of the parameter α, the
function that the algorithm optimizes ?
In the lower part of Figure 4, we have reported the recursions of the Sum/Max-product algorithm in
the probability space for T = 4. It is easily seen, by looking at the top of the same ﬁgure, that they
match the recursions of the Sum-product algorithm as applied to the factorization
p(s1a1 ...s TaT)α = c(sTaT)αp(s1)αp(aT)α
T−1∏
t=1
p(st+1|stat)αp(at)αc(stat)α, (33)
Obviously the power of a distribution is not a normalized distribution, but this is not a problem as we
mentioned before, because normalization is just a scale that is irrelevant for the inference.
13
Figure 4: Backward recursions for the Sum/Max-product algorithm for T = 4. Note how the
recursions can be seen as the Sum-product algorithm applied to the factorization where all the factors
are raised to a power α.
Therefore, in analogy to the Sum-product algorithm, the Sum/Max-product algorithm provides the
posteriors
p(st|K1:T) ∝
∑
sj,j̸=t,j=1:T
aj,j=1:T
p(s1a1 ...s TaT|K1:T)α,
p(at|K1:T) ∝
∑
sj,j=1:T
aj,j̸=t,j=1:T
p(s1a1 ...s TaT|K1:T)α,
p(stat|K1:T) ∝
∑
sj,aj,j̸=t,j=1:T
p(s1a1 ...s TaT|K1:T)α.
(34)
To better explain the generalization, we recall that raising a probability distribution to a power greater
than one, has the effect of sharpening the distribution around its maximum (or maxima, if multiple
maxima are present). Therefore, raising the whole joint density to a large power has the effect of
concentrating it on the global maximum a posteriori solution of the Max-product algorithm. Note
that the maxima on the posteriors can be computed in parallel, or progressively in sequence. The
speciﬁc discussion is omitted here for brevity, but follows the same strategy used for the Sum-product
algorithm.
Figure 5: Backward recursions for the Dynamic programming algorithm forT = 4. Note the presence
of the backward message Q(S4A4)4 (s4a4) that may carry information from time steps beyond T, or
may represent ﬁnal constraints.
6 Dynamic programming on the factor graph
The standard approach to dynamic programming is based on the maximization of the expected sum of
rewards (Bertsekas, 2019; Sutton and Barto, 2018). In previous sections, we have included rewards in
factorization (2), but we have formulated the optimization problem as the maximization of posterior
14
probabilities, or marginals, which only implicitly involve the rewards. Obviously, one wonders
whether the two approaches can be seen under a uniﬁed framework - after all Bellman backups
resemble backward message combinations. We show here that it is possible to map DP directly
into the factor graph formulation if we consider rewards and their expectations as contributing to
probability messages, but in the log-space. We can see how in the probability space, the DP messages
can travel on the factor graph, just as in the Sum/Max-product algorithm, but with different deﬁnitions
of the propagation rules through the building blocks.
The dynamic programming algorithm (Bertsekas, 2019) is derived as the solution to the following
problem
(a∗
1 ...a ∗
T) = argmax
a1...aT
E∼p(s1a1...sTaT)
[ T∑
t=1
(R(stat) + logp(at))
]
= argmax
a1...aT
∑
s1,...,sT
p(s1a1...sTaT)
[ T∑
t=1
(R(stat) + logp(at))
]
= argmax
a1...aT
∑
s1,...,sT
p(s1)
T−1∏
t=1
p(st+1|stat)
[ T∑
t=1
(R(stat) + logp(at))
]
,
(35)
where p(s1a1...sTaT) does not include the rewards and the priors on at appears in the log in the
summation. This is slightly different than the sum of pure rewards. The reason for this modiﬁcation
has been to obtain the same formal recursions that we have derived for the Sum-product and for
the Max-product algorithms. In any case, this is not a crucial problem because log p(at) could be
incorporated into R(stat) and p(at) can be assumed to be uniform.
We have reported in Figure 5, the DP recursions in the notations of the factor graph of Figure 2 for
T = 4. Note that, in comparison to analogous backups in the probability space for the Sum-product
(or Max-product) algorithm, the rewards appear as additive terms and there is a mix of max and sums.
Formally
Q(StAt)1 (stat) = logp(at) +R(stat) +
∑
st+1
p(st+1|stat)VSt+1 (st+1),
VSt(st) = max
at
Q(StAt)1 (stat).
(36)
Translating the recursions in the probability space, we have
b(StAt)1 (stat) =p(at)c(stat)e
∑
st+1 p(st+1|stat) logbSt+1 (st+1)
,
bSt(st) = max
at
b(StAt)1 (stat). (37)
The crucial difference between DP and the Sum-product algorithm is in the fact that averages and
maxima are taken in the log-space on the value function. Conversely in the Sum-product, they are
taken in the probability space on the backward distributions. Therefore, DP can be formulated in
terms of probability messages traveling on the same factor graph of the Sum-product algorithm, but
with different combination rules. The DP recursions are reported in Tables 1 and 2 for comparison,
and the speciﬁc rules for the messages through the blocks are in Tables 3, 4, 5, 6 and 7.
The best SASA sequence, written both in the log-space and in the probability space, is immediately
derived from the graph:
At t= 1we have
s∗
1 = argmax
s1
p(s1)VS1 (s1) = argmax
s1
p(s1) logbS1 (s1),
a∗
1 = argmax
a1
Q(S1A1)1 (s∗
1a1) = argmax
a1
b(S1A1)1 (s∗
1a1). (38)
At t= 2
s∗
2 = argmax
s2
p(s2|s∗
1a∗
1)VS2 (s2) = argmax
s2
p(s2|s∗
1a∗
1) logbS2 (s2),
a∗
2 = argmax
a2
Q(S2A2)1 (s∗
2a2) = argmax
a2
b(S2A2)1 (s∗
2a2), (39)
... etc.
The unique formulation on the factor graph allows the derivation of all other inferences, such as A,
S, AS -sequences, also for DP, simply using the speciﬁc propagation rules on the graph. The policy
distribution has the same formal expression as in (30).
15
7 SoftDP
The presence of the max operator in the DP algorithm, suggests that, similarly to the Sum/Max-
product approach, we could replace the max operator with a generic soft-max function to provide
a different interpolation between a more entropic solution and the optimal DP algorithm. Using a
soft-max function, we propose the following SoftDP updates
Q(StAt)1 (stat) = logp(at) +R(stat) +
∑
st+1
p(st+1|stat)VSt+1 (st+1),
VSt(st) =
∑
at eβQ(StAt)1 (stat)Q(StAt)1 (stat)
∑
a′
t
eβQ(StAt)1 (sta′
t) .
(40)
The parameter βcan be used to control the sharpness of the soft-max function. If βis a large potisive
number, the soft-max tends to the maximum. When β is a small positive number, the soft-max
function tends to return the mean. The soft-max function used here is popular in the neural networks
literature. Details about its behavior are in Appendix A. We have not investigated the existence of a
function that these recursions optimize for a ﬁnite value of β, as in the case of the Sum/Max-product
algorithm. We leave it to further analyses. However, we observe that lowering the value ofβshifts
the policy distribution towards a smoother, i.e., more entropic, conﬁguration. We show this effect in
the simulations in a later section.
In the probability space, the recursion for the backward message b(StAt)1 (stat) is the same as in DP,
while the update for bSt(st) becomes
bSt(st) ∝exp
[∑
at log b(StAt)1 (stat)bSt(st)β
∑
a′
t
b(StAt)1 (sta′
t)β
]
. (41)
All recursions are included in Tables 1 and 2. Also the propagation rule through the blocks are in
Tables 3, 4, 5, 6 and 7.
8 Maximum expected reward and entropy
In all the previous approaches to optimal control, we have derived the solutions as optimal inferences
on the factorized model of Figure 1, in the probability space for the Sum- and Max-product algorithms,
or as the best action sequence in the log-space for DP. The policy distribution is then written as a
consequence of the optimization algorithm on that graph.
A different formulation can be adopted if we formally add to the Bayesian graph "policy" branches
π(at|st) that go from each state St to each action At and pose the problem as the functional
optimization problem of ﬁnding the best π(at|st), given the evidence K1:T. The question is: how do
we formalized the total reward function?
Levine (2018), in his review, suggests that ”less conﬁdent” behaviors with respect to the standard
probabilistic inference (the Sum-product) could be obtained if we modify the function to optimize.
In fact, he maintains that the recursions for the Sum-product approach derived above, may be too
optimistic within the context of RL. The idea is to add an extra term to the rewards to account also
for policy entropy. Levine shows that the modiﬁcation can also be related to structural variational
inference (Levine, 2018). Entropy maximization is also a common criterion in practical uses of
RL (Ziebart et al., 2009) and stochastic control (Ziebart et al., 2010). Levine (2018) proposes the
following formulation
{π∗(a1|s1) ...π ∗(aT|sT)}=
argmax
π(a1|s1)...π(aT|sT)
E∼ˆp(s1a1...sTaT)
[ T∑
t=1
(R(stat) + logp(at) −log π(at|st))
]
(42)
where
ˆp(s1a1 ...s TaT) =p(s1)π(aT|sT)
T−1∏
t=1
p(st+1|stat)π(at|st).
16
Note that here the policy distributions are included in the factorization. The extra term log π(at|st)
will gives rise to entropy maximization This will be better explained in the generalization that follows
and in Appendix B. The backup recursions for the optimal policy distributions (Levine, 2018), in the
factor graph notations, are
Q(StAt)1 (stat) = logp(at) +R(stat) +
∑
st+1
p(st+1|stat)VSt+1 (st+1),
VSt(st) = log
∑
at
eQ(StAt)1 (stat).
(43)
The optimal policy distributions are also shown to have the usual formal expression
π∗(at|st) ∝e(Q(StAt)1 (stat)−V(st)). (44)
In our effort to provide more general approaches to the policy search, we have generalized the
soft-max function to include an extra parameter α, with the recursions
Q(StAt)1 (stat) = logp(at) +R(stat) +
∑
st+1
p(st+1|stat)VSt+1 (st+1),
VSt(st) =1
αlog
∑
at
eαQ(StAt)1 (stat).
(45)
The function used in the value function update is the same one used for the Sum/Max product
algorithm and is such that for α→∞ gives the maximum and therefore the DP solution.
We have worked the recursion backward and we show (see Appendix B for the proof) that the above
recursions solve the following optimization problem
{π∗(a1|s1) ...π ∗(aT|sT)}=
argmax
π(a1|s1)...π(aT|sT)
E∼ˆp(s1a1...sTaT)
[ T∑
t=1
(
R(stat) + logp(at) −1
αlog πα(at|st)
)]
(46)
where
ˆp(s1a1 ...s TaT) =p(s1)πα(aT|sT)
T−1∏
t=1
p(st+1|stat)πα(at|st),
πα(at|st) = π(at|st)α
∑
a′
t
π(a′
t|st)α, t = 1 :T.
(47)
The above expression generalizes (42) by including a parameter α >0. We show in Appendix B
that, when αis large, the extra term becomes progressively irrelevant, and the distributions πα(at|st)
become more concentrated on the max value of the Q-function resulting in a hard DP solution.
Furthermore, when α < 1, more weight is given to the extra term, the distributions πα(at|st)
becomes smoother and we have more entropic policy distributions. We demonstrate this effect in
some of the simulations that follow.
In Appendix B, we discuss explicitly this formulation for T = 4deriving the recursions (45) showing
how the extra term gives rise to (iterative) simultaneous reward and entropy maximization. It is
pointed out in our discussion that the criterion does not simply add an entropy term to the rewards
because the policy distribution affects also the reward as it appears in the factorization used in the
expectation.
The recursions (45) can be translated in the probability space as
b(StAt)1 (stat) =p(at)c(stat)e
∑
st+1 p(st+1|stat) logbSt+1 (st+1)
bSt(st) =
[∑
at
b(StAt)1 (stat)α
]1
α (48)
The recursions, both in the probability and the log space, are included in Tables 1 and 2. Also the
propagation rule through the blocks are reported in Tables 3, 4, 5, 6 and 7.
17
9 Deterministic systems
The approach to optimal control in this paper is based on the assumption that the system description
is stochastic. This is quite useful in the applications when we do not have exact knowledge of the
system dynamics and the the probability distribution p(st+1|stat) can be our best estimate. There
are cases however, in which the system response is deterministic, i.e., given stat, we have exact
knowledge of st+1 through a deterministic function st+1 = g(stat). In the stochastic framework,
this translates into a transition probability function that is delta function
p(st+1|stat) =δ(st+1 −g(stat)) . (49)
Also, if no prior on the actions is available, p(at) =U(at). The updates in these cases do not change,
but some of them in the various methods may coincide, because the summations (expectations) in the
updates disappear and the prior on At is irrelevant. More speciﬁcally, by looking at Tables 1 and 2,
the updates for the Q-functions, and their probability-space counterparts, have the same (Bellman’s)
recursions
Q(StAt)1 (stat) =R(stat) +VSt+1 (g(stat)),
b(StAt)1 (stat) =c(stat)bSt+1 (g(stat)). (50)
However, there are differences in theV-function updates. For the Sum-product and the Max-Rew/Ent
(α= 1) we have
VSt(st) = log
∑
at
eQ(StAt)1 (stat),
bSt(st) =
∑
at
b(StAt)1 (stat). (51)
For the Max-product and DP, we have
VSt(st) = max
at
Q(StAt)1 (stat),
bSt(st) = max
at
b(StAt)1 (stat). (52)
For the others we have the parametrized soft-max function with various values of βand α.
By direct comparison, we can conclude that, when the system is deterministic: DP and Max-product
coincide; Mean-product and Max-Rew/Ent (α= 1) coincide (also recognized in Levine (2018)). The
remaining cases are interpolations of the others. We have veriﬁed in our limited simulations that this
is indeed the case and that the solutions in the various groups, even in this deterministic case, are
different.
10 Inﬁnite horizon case and the steady-state
We have presented the model in Figure 1 and the various algorithm with reference to a ﬁnite horizon
scenario. However, all the analyses easily extends to an inﬁnite-horizon framework simply by adding
a discount factor 0 < γ≤1 to the optimized functions and then to the updates. For example, the
standard DP updates, in both spaces become
Q(StAt)1 (stat) = logp(at) +R(stat) +γ
∑
st+1
p(st+1|stat)VSt+1 (st+1),
b(StAt)1 (stat) =p(at)c(stat)e
γ∑
st+1 p(st+1|stat) logbSt+1 (st+1)
.
(53)
Also, for the Sum-product, we have immediately
Q(StAt)1 (stat) = logp(at) +R(stat) +γlog
∑
st+1
elog p(st+1|stat)+VSt+1 (st+1) (54)
In general, also if γ = 1, the backward recursions can be run to verify that a steady-state conﬁguration
for the Q, the V-function and the policy π∗can be found. This is clearly the solution to a Bellmann
equation applied to the different updates. The analysis of the mathematical conditions for convergence
are beyond the scope of this paper. However, generally speaking, if all the states are reachable, a stable
conﬁguration should exist. We have veriﬁed experimentally that all the methods do converge (also for
γ = 1), but they exhibit marked differences in the number of iterations necessary to reach the steady
state equilibrium. The Max-product algorithm shows the fastest convergence with the Sum-product
following in the list. DP and the other methods seem to show a much slower convergence speed. We
show this effect in the simulations that follow.
18
Value for Sum-Product (22 iterations) Policy for Sum-Product
Value for Max-Product (12 iterations) Policy for Max-Product
Value for Sum/Max-Product (15 iterations) Policy for Sum/Max-Product
Value for SoftDP (89 iterations) Policy for SoftDP
Value for SoftDP (69 iterations) Policy for SoftDP
Value for DP (81 iterations) Policy for DP
Value for Max Rew/Ent (125 iterations) Policy for Max Rew/Ent
Value for Max Rew/Ent (90 iterations) Policy for Max Rew/Ent
Value for Max Rew/Ent (88 iterations) Policy for Max Rew/Ent
Comparison
Figure 6: Visualization of the max policy direction for the various algorithms. At the top of each
ﬁgure are reported also the number of iterations necessary to reach a steady-state value function.
Reported on the left columns are also the numerical ﬁnal values for the value function. The lowest
right plot shows the value function increments as the iterations progress towards steady-state.
11 Simulations
We have simulated the various recursions for path planning on two discrete grids.
The ﬁrst set of simulations is performed on a small 6x6 square grid shown in Figure 6, where we
have one goal (bull’s eye and green) and obstacles (dark gray). The states are the positions on the
grid and the actions correspond to one of the nine possible one-pixel motions {up-left, up, up-right,
left, center (still), right, down-left, down, down-right}. The reward function has the values 0 on the
goal, −10 on the obstacles and −1 on other pixel positions. The motion is stochastic with a transition
19
Sum-Product (29 iterations)
Max-Product (11 iterations)
Sum/Max-Product (15 iterations)
SoftDP (127 iterations)
SoftDP (113 iterations)
DP (120 iterations)
Max Rew/Ent (187 iterations)
Max Rew/Ent (128 iterations)
Max Rew/Ent (120 iterations)
Figure 7: Visualization of the max policy direction for the various algorithms. At the top each ﬁgure
are reported also the number of iterations necessary for the value function to reach its steady-state
conﬁguration.
function p(st+1|stat) that has probability 1/2 for the intended direction and the rest spread equally
on the other eight directions. Built in the transition function are also re-normalizations when the
transition is close to the boundaries: when some of the new projected states are outside the grid,
their probabilities are set to zero, and the remaining probability is spread equally on the other pixels.
No initial or ﬁnal conditions are set on the model. The recursions are run until convergence to a
steady state value function. All the algorithms lead to policies that would allow an agent, starting
from any position on the grid, to reach the goal in a ﬁnite number of steps. The values reported
in the squares and the max policy arrows in Figure 6 reveal how the different solutions direct our
potential agent in slightly different paths to avoid the obstacles. In the lower right corner of the ﬁgure,
20
Sum-Product (29 iterations)
Max-Product (11 iterations)
Sum/Max-Product (15 iterations)
SoftDP (127 iterations)
SoftDP (113 iterations)
DP (120 iterations)
Max Rew/Ent (187 iterations)
Max Rew/Ent (128 iterations)
Max Rew/Ent (120 iterations)
Figure 8: Numerical visualization of the value function for the various algorithms.
we also report the increments in reaching the steady-state solution for the various algorithms in a
log-log graph (also the parameters are reported in the legend). The algorithm is stopped after all
the increments in the value function are below 10−5. It is noteworthy to see how the Max-product
algorithm reaches the steady-state solution in a very limited number of steps (fastest convergence)
and how the Sum-product and the Sum/Max-product algorithms converge at a much faster rate in
comparison to the others.
The results of another set of simulations are reported in Figures 7, 8, 9, 10 and 11. Here, we have a
grid extracted from a real dataset acquired at an intersection on the Stanford campus with pedestrians
and bikes. The scene, with no agents, is simpliﬁed to 17 ×23 pixels, with goals (exits) and rewards
assigned to various areas (semantic map) as shown in Figure 7. We assume that our agent is a
21
Sum-Product
200
150
100
50
0
5
10
15
20 5
10
15
Max-Product
200
150
100
50
0
5
10
15
20 5
10
15
200
150
100
50
0
5
10
15
20 5
10
15
Sum/Max-Product
200
150
100
50
0
5
10
15
20 5
10
15
SoftDP
250
300
350
200
150
100
50
0
5
10
15
20 5
10
15
SoftDP
250
300
350
200
150
100
50
0
5
10
15
20 5
10
15
DP
250
300
350
200
100
0
5
10
15
20 5
10
15
Max-Rew/Ent
300
400
500
200
150
100
50
0
5
10
15
20 5
10
15
Max-Rew/Ent
250
300
350
200
150
100
50
0
5
10
15
20 5
10
15
Max-Rew/Ent
250
300
350
Figure 9: 3D Plots of −VSt(st) for the various algorithms.
pedestrian and the actions are the same nine actions we have used above for the small grid. The
rewards are: R(st) =0 (goals: bull’s eye and green); -1 (pedestrian walkways: white); -10 (streets:
light gray); -20 (grass: dark gray); -30 (obstacles: dark). The convergence behavior to a steady-state
value function is similar to the one shown for the smaller grid. The number of iterations to reach
a precision of 10−5 on all the states are: [Sum-product: 29; Max-product: 11; Sum/Max-product
(α= 3): 15; Soft DP (β = 0.2): 127; Soft DP (β = 0.6): 113; DP: 120; Max Rew/Ent (α= 0.2):
187; Max Rew/Ent (α= 1): 128; Max Rew/Ent (α= 6): 120]. Note how quickly the Max-product
and the probabilistic methods converge when compared to the others. The graph of the actual
increments for the various algorithm is shown in log scale in Figure 11. Figure 7 shows, for each
state, the maximum policy directions for all the algorithms. The arrows point towards the preferences
imposed by the semantic information: pedestrians prefer walkways to streets; grass and obstacles
are avoided. We observe a marked effect on the results of the Max-product algorithm that maintains
the multiple maxima directions corresponding to the equivalent solutions. These multiple options
appear smoothed out in the other methods. The Max-product requires just the minimum number of
steps to stabilize its conﬁgurations. Figure 9 shows also some of the value functions −V(st) (they
22
Sum/Max-Product
Max-Product
Sum-Product SoftDP
DP
Max Rew/Ent
Max Rew/Ent
Max Rew/Ent
0
0,5
1
0
0,5
1
0
0,5
1
0
0,5
1
SoftDP
0
0,5
1
0
0,5
1
0
0,5
1
0
0,5
1
0
0,5
1
Figure 10: Policy distributions at a generic point (red on the map) for the various methods for different
parameter choices. The goals are depicted in green.
Comparison
Figure 11: Comparison of the value function increments for the various algorithm for the example of
Figure 7.
can be thought as potential functions) superimposed on the original scene for the various methods
and for some parameter choices. The comparison clearly shows that the various algorithm lead to
intriguing comparable smooth solutions, except for the Max-product that produces a very sharp value
function with very well deﬁned valleys. Just as in the simulations on the small grid, we have included
no paths on the map, because in all methods an agent that starts anywhere, will reach one of the goals
in all cases. This can be easily veriﬁed by following the arrows in Figure 7. A much more revealing
visualization of the differences among the various methods is displayed in Figure 10, where at a
generic point on the map, we plot the policy distributions. In the ﬁrst column, the policies for the
probabilistic methods are shown with the Max-product clearly producing a rather sharp behavior
with all the multiple equivalent options. Recall that the map has multiple goals and the agent in that
position has more than one option to achieve optimality (see also Figure 7 in that position). In the
second column, we report the results of the DP approach in its standard form (bottom graph) and in
its soft parametrized versions. Note how, for the two values β = 0.3 and β = 0.6, an agent may be
led to consider more options with respect to DP and if we look also at the maximum policy on the
map of Figure 7, it may even be taken on a different path. In the third column, we report the results
of the Max Rew/Ent algorithm for various values of the parameter α. We notice, as expected from
the theory, that when α< 1, the policy distribution is more entropic and that when αincreases, the
distribution tends to the DP policy.
23
12 Conclusions and future directions
In this paper, we have provided a uniﬁed view on the optimal solutions to path planning using a
probability formulation on a factor graph. The various methods correspond to different combination
rules through two of the blocks on a Factor Graph in Reduced Normal Form and correspond to
different cost functions that go from maximum a posteriori to maximum combination of reward
and entropy. We have generalized some of the algorithms previously presented in the literature by
using parametrized soft-max functions. The resulting set of choices, presented here in a compact
fashion, both in the probability and in the log space, may enhance the algorithmic design options for
reinforcement learning that needs to obtain the V-functions or the Q-functions, perhaps on the basis
of current model knowledge. We have included typical results on discrete grids that reveal marked
differences among some of the methods. Our computational results suggest that the Max-product
algorithm, optimal maximum a posteriori solution, together with the other probabilistic methods, such
as the Sum-product and its Sum/Max-product generalizations, shows fast converge to the steady-state
conﬁguration in comparison to the other reward-based methods, that are typically derived in the
log space. This general approach to the topic may allow agile use of the various methods during
exploration in RL. Further work on the topic will be devoted to extensions to continuous space and
relative approximations, and applications involving interacting agents.
A Soft-max functions
We review here some of the soft-max functions that are used in the recursions discussed in the paper.
For all the functions, we start from a set or real numbers x1,...,x N and consider the ranked set
x(1),x(2),...,x (N), with x(1) ≤x(2) ≤···≤ x(N).
A. Consider the following expression
s(x1,...,x N) = log
N∑
j=1
exj.
This function has the property that when x1,...,x N →+∞, s(x1,...,x N) →max(x1,...,x N).
Proof: The function can rewritten as
s(x1,...,x N) = log (ex(1) + ex(2) + ··· + ex(N) )
= logex(N)
(
ex(1)−x(N) + ex(2)−x(N) + ··· + 1
)
.
When the xis become large, also their difference to the max x(N) becomes a large negative number.
Therefore the ﬁrst N −1 terms inside the parenthesis tend to zero and s(x1,...,x n) →x(N).
B. A parametrized soft-max function can be deﬁned as the expression
g(x1,...,x N; α) = 1
αlog
N∑
j=1
eαxj
where α≥1. This function has the property that
lim
α→∞
g(x1,...,x N; α) = max(x1,...,x N). (55)
Proof: Using the ranked set, the function can be bounded as
1
αlog eαx(N) ≤g(x1,...,x N; α) ≤1
αlog Neαx(N) , (56)
x(N) ≤g(x1,...,x N; α) ≤log N
α + x(N),
that for α →∞ achieves the maximum x(N). Note that for α >1, from the bound, the soft-max
always exceeds the maximum value, i.e., tends to x(N) from the right.
It is useful to look at the expression when 0 <α< 1. When αis a very small positive number
g(x1,...,x N; α) ≃1
αlog N + µ,
24
where µ = (1/N) ∑N
i=1 xi is the arithmetic mean. The function diverges for α →0, but when
considered for small values of α, the function tends to become independent on any speciﬁc xi.
Proof: The function can be written as
g(x1,...,x N; α) = 1
αlog
(N∑
i=1
eα(xi−µ)eαµ
)
= 1
αlog
N∑
i=1
eα(xi−µ) + µ.
When αapproaches zero, the exponents become ≃1 and we have the result.
C. Another parametric soft-max function is
h(x1,...,x N; α) =


N∑
j=1
xα
j


1
α
,
where here xi ≥0,i = 1 :N. Here too, for α→∞, h(x1,...,x N; α) →x(N).
Proof: From the bounds
(xα
(N))
1
α ≤h(x1,...,x N; α) ≤
(
Nxα
(N)
)1
α
,
x(N) ≤h(x1,...,x N; α) ≤N
1
αx(N),
when αgrows N
1
α →1 and the function tends to x(N). In all soft-max functions, the larger values
in the set x1,...,x N, tend to dominate over the others when they become large.
The function h(x1,...,x N; α), just as g(x1,...,x N; α), diverges for α→0, but for small α
h(x1,...,x N; α) ≃N
1
α,
which, again as in g, deas not depend on any of the xi.
Proof: easily seen as xα
i ≃1 for small α.
D. Another soft-max functions can be deﬁned as
r(x1,...,x N; α) =
∑N
i=1 xieαxi
∑N
j=1 eαxj
.
This function is well-known in the neural network literature, where the vector functioneαxi/∑
jeαxj
tends to a distribution concentrated on the maximum. By taking the expectation with such a distribu-
tion we get the soft-max. Therefore, when α→∞, r(x1,...,x N; α) →x(N).
Proof: The function can re-written using the ranked set as
r(x1,...,x N; α) =
∑N−1
i=1 x(i)eαx(j) + x(N)eαx(N)
∑N−1
j=1 eαx(j) + eαx(N)
=
∑N−1
i=1 x(i)eα(x(j)−x(N)) + x(N)
∑N−1
j=1 eα(x(j)−x(N)) + 1
.
For α →∞ both summations tend to zero, because the exponents are negative, and we have the
result.
This soft-max function, when α → 0+ does not diverge, but tends to the arithmetic mean
r(x1,...,x N; α) →1/N∑N
i=1 xi.
Proof: Trivial, because for α= 0all the exponentials are equal to one.
25
B Optimizing Reward and Entropy
To better understand the nature of the function being optimized in (46), and how it gives rise to
an entropy term, let us write it explicitely for T = 4, using the compact notation R′(stat) =
R(stat) + logp(at). The function to optimize is
∑
s1...s4
∑
a1...a4
p(s1)πα(a1|s1)p(s2|s1a1)πα(a2|s2)p(s3|s2a2)πα(a3|s3)p(s4|s3a3)πα(a4|s4)
[
R′(s1a1) −1
αlog πα(a1|s1) +R′(s2a2) −1
αlog πα(a2|s2) +R′(s3a3)
−1
αlog πα(a3|s3) +R′(s4a4) −1
αlog πα(a4|s4)
]
,
(57)
Starting from the last term, we identify the backward recursions
1
α
∑
s4
p(s4|s3a3)


∑
a4
πα(a4|s4)αR′(s4a4) +
H(πα(a4|s4))
  ∑
a4
πα(a4|s4) log 1
πα(a4|s4)
  
V(s4)


  
Q(s3a3)
1
α
∑
s3
p(s3|s2a2)


∑
a3
πα(a3|s3)(αR′(s3a3) +αQ(s3a3)) +
H(πα(a3|s3))
  ∑
a3
πα(a3|s3) log 1
πα(a3|s3)
  
V(s3)


  
Q(s2a2)
1
α
∑
s2
p(s2|s1a1)


∑
a2
πα(a2|s2)(αR′(s2a2) +αQ(s2a2)) +
H(πα(a2|s2))
  ∑
a2
πα(a2|s2) log 1
πα(a2|s2)
  
V(s2)


  
Q(s1a1)
1
α
∑
s1
p(s1)


∑
a1
πα(a1|s1)(αR′(s1a1) +αQ(s1a1)) +
H(πα(a1|s1))
  ∑
a1
π(a1|s1) log 1
πα(a1|s1)
  
V(s1)


(58)
Note how the value function V(st) (not optimized here) is written as a recursive superposition of
reward and policy entropy. The parameter αcontrols the balance between the two terms and the
power of the distribution. Note that the policy function multiplies also the reward term. Therefore,
the optimized policy distribution will shape, in a non trivial way, the effects of the rewards with
respect to the entropy.
26
Following Levine’s approach (Levine, 2018), using our modiﬁed cost function, we search for the best
policy distribution starting from re-writing the last term using the KL-divergence
1
α
∑
s4
p(s4|s3a3)
[∑
a4
πα(a4|s4) (αR′(s4a4) −log πα(a4|s4))
]
=
1
α
∑
s4
p(s4|s3a3)
[∑
a4
πα(a4|s4)
(
log eαR′(s4a4)
πα(a4|s4)
∑
a′
4
eαR′(s4a′
4)
∑
a′
4
eαR′(s4a′
4)
)]
=
1
α
∑
s4
p(s4|s3a3)


−DKL
(
πα(a4|s4)

eαR′(s4a4)
∑
a′
4
eαR′(s4a′
4)
)
+ log
∑
a′
4
eαR′(s4a′
4)
  
eαV(s4)


=
∑
s4
p(s4|s3a3)
[
−DKL
(
πα(a4|s4)

eαR′(s4a4)
eαV(s4)
)
+ V(s4)
]
.
(59)
The optimum value is obtained when the DKL(.∥.) = 0, i.e., when πα(a4|s4) =eαR′(s4a4)
eαV(s4) , and the
optimal policy distribution is
π∗(a4|s4) ∝eR′(s4a4)
eV(s4) = eQ(s4a4)
eV(s4) , (60)
where we have deﬁned Q(s4a4) =R′(s4a4). Now the optimized expression ∑
s4 p(s4|s3a3)V(s4)
is carried over
R′(s3a3) +
∑
s4
p(s4|s3a3)V(s4)
  
Q(s3a3)
−1
αlog πα(a3|s3). (61)
Taking the expectation, we have
1
α
∑
s3
p(s3|s2a2)
[∑
a3
πα(a3|s3) (αQ(s3a3) −log πα(a3|s3))
]
=
1
α
∑
s3
p(s3|s2a2)
[∑
a3
πα(a3|s3)
(
log eαQ(s3a3)
πα(a3|s3)
∑
a′
3
eαQ(s3a′
3)
∑
a′
3
eαQ(s3a′
3)
)]
=
1
α
∑
s3
p(s3|s2a2)


−DKL
(
πα(a3|s3)

eαQ(s3a3)
∑
a′
3
eαQ(s3a′
3)
)
+ log
∑
a′
3
eαQ(s3a′
3)
  
eαV(s3)


=
1
α
∑
s3
p(s3|s2a2)
[
−DKL
(
πα(a3|s3)

eαQ(s3a3)
eαV(s3)
)
+ αV(s3)
]
.
(62)
Similarly to above, DKL = 0 when πα(a3|s3) = eαQ(s3a3)
eαV(s4) . The best policy distribution is then
π∗(a3|s3) ∝eQ(s3a3)
eV(s4) . Carrying over ∑
s3 p(s3|s2a2)V(s3), we have
R′(s2a2) +
∑
s3
p(s3|s2a2)V(s3)
  
Q(s2a2)
−1
αlog πα(a2|s3). (63)
Following similar steps, we have π∗(a2|s2) ∝eQ(s2a2)
eV(s2) and
R′(s1a1) +
∑
s2
p(s2|s1a1)V(s2)
  
Q(s1a1)
−1
αlog πα(a1|s1). (64)
27
The last step is
1
α
∑
s1
p(s1)
[∑
a1
πα(a1|s1) (αQ(s1a1) −log πα(a1|s1))
]
=
1
α
∑
s1
p(s1)
[∑
a1
πα(a1|s1)
(
log eαQ(s1a1)
πα(a1|s1)
∑
a′
1
eαQ(s1a′
1)
∑
a′
1
eαQ(s1a′
1)
)]
=
1
α
∑
s1
p(s1)


−DKL
(
πα(a1|s1)

eαQ(s1a1)
∑
a′
1
eαQ(s1a′
1)
)
+ log
∑
a′
1
eαQ(s1a′
1)
  
eαV(s1)


=
1
α
∑
s1
p(s1)
[
−DKL
(
πα(a1|s1)

eαQ(s1a1)
eαV(s1)
)
+ αV(s1)
]
.
(65)
This term is minimized when πα(a1|s1) = eαQ(s1a1)
eαV(s1) , with the optimal policy distribution
π∗(a1|s1) ∝eQ(s1a1)
eV(s1) . Therefore, the recursions at a generic time step tare
Q(StAt)1 (stat) = logp(at) +R(stat) +
∑
st+1
p(st+1|stat)VSt+1 (st+1),
VSt(st) = 1
αlog
∑
at
eαQ(StAt)1 (stat),
(66)
with the optimal policy distribution π∗(at|st) ∝eQ(stat)−V(st).
References
Alexandre, Z., Oleg, S., and Giovanni, P. (2018). An information-theoretic perspective on the costs
of cognition. Neuropsychologia, 123:5–18.
Attias, H. (2003). Planning by probabilistic inference. In Proc. of the 9th Int. Workshop on Artiﬁcial
Intelligence and Statistics, page .
Bahl, L., Cocke, J., Jelinek, F., and Raviv, J. (1974). Optimal decoding of linear codes for minimizing
symbol error rate. IEEE Transactions on Information Theory, IT-20(2):284–287.
Baltieri, M. and Buckley, C. L. (2017). An active inference implementation of phototaxis. ARXIV.
Barber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press.
Bertsekas, D. (2019). Reinforcement Learning and Optimal Control. Athena Scientiﬁc.
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). The free energy principle for
action and perception: a mathematica review. Journal of Mathematica Phychology, 81:55–79.
Castaldo, F. and Palmieri, F. (2015). Target tracking using factor graphs and multi-camera systems.
IEEE Transactions on Aerospace and Electronic Systems (TAES), 51(3):1950 – 1960.
Castaldo, F., Palmieri, F., and Regazzoni, C. (2014). Application of Bayesian Techniques to Behavior
Analysis in Maritime Environments, volume 37, chapter 5, pages 175–183. Springer, Cham.
Coscia, P., Braca, P., Milleﬁori, L. M., Palmieri, F. A. N., and Willett, P. K. (2018a). Multiple
ornstein-uhlenbeck processes for maritime trafﬁc graph representation. IEEE Transactions on
Aerospace and Electronic Systems, 54:2158–2170.
Coscia, P., Castaldo, F., Palmieri, F. A. N., Alahi, A., Savarese, S., and Ballan, L. (2018b). Long-term
path prediction in urban scenarios using circular distributions. Image and Vision Computing ,
69:81–91.
28
Coscia, P., Castaldo, F., Palmieri, F. A. N., Ballan, L., Alahi, A., and Savarese, S. (2016). Point-based
path prediction from polar histograms. In Proceedings of the 19th International Conference on
Information Fusion (FUSION 2016), pages 1961–1967.
Coscia, P., N. Palmieri, F. A., Braca, P., Milleﬁori, L. M., and Willett, P. (2018). Unsupervised
maritime trafﬁc graph learning with mean-reverting stochastic processes. In2018 21st International
Conference on Information Fusion (FUSION), pages 1822–1828.
Di Gennaro, G., Buonanno, A., and Palmieri, F. A. N. (2021). Optimized realization of bayesian
networks in reduced normal form using latent variable model. Soft Computing, Springer, pages
1–12.
Forney, G.D., J. (2001). Codes on graphs: normal realizations.Information Theory, IEEE Transactions
on, 47(2):520–548.
Imohiosen, A., Watson, J., and Peters, J. (2020). Active inference or control as inference?a unifying
view. In Proceedings of First International Workshop, IWAI 2020, Co-located with ECML/PKDD
2020, Ghent, Belgium, September 14.
Kaplan, R. and Friston, K. J. (2018). Planning and navigation as active inference. Biological
Cybernetics, 112:323–347.
Kappen, H., Gomez, V ., and Opper, M. (2013). Optimal control as a graphical model inference
problem. In Proceedings of the Twenty-Third International Conference on Automated Planning
and Scheduling.
Kappen, H. J., Gomez, V ., and Manfred, M. (2012). Optimal control as a graphical model inference
problem. Machine Learning, 87:159–182.
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.
MIT Press.
Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv:1805.00909v3 [cs.LG] 20 May 2018.
Loeliger, H. A. (2004). An introduction to factor graphs. IEEE Signal Processing Magazine, 21(1):28
– 41.
Loeliger, H.-A. ., Dauwels, J., Hu, J., Korl, S., Li, P., and Kschischang, F. (2007). The factor graph
approach to model-based signal processing. Proceedings of the IEEE, 95(6):1295 –1322.
Millidge, B., Tschantz, A., Seth, A. K., and Buckley, C. L. (2020). On the relationship between
activeinference and control as inference. In Proceedings of First International Workshop, IWAI
2020, Co-located with ECML/PKDD 2020, Ghent, Belgium, September 14.
Nair, S., Zhu, Y ., Savarese, S., and Li, F.-F. (2019). Causal induction from visual observations for
goal directed tasks. arXiv:1910.01751v1 [cs.LG] 3 Oct 2019.
Palmieri, F. A. N. (2016). A comparison of algorithms for learning hidden variables in bayesian factor
graphs in reduced normal form. IEEE Transactions on Neural Networks and Learning Systems,
27(11):2242–2255.
Palmieri, F. A. N., Pattipati, K. R., Fioretti, G., Di Gennaro, G., and Buonanno, A. (2021). Path
planning using probability tensor ﬂows. IEEE Aerospace and Electronic Systems Magazine, 36(1).
Preliminary version on arXiv:2003.02774.
Parr, T. and Friston, K. J. (2018a). The anatomy of inference: Generative models and brain structure.
Frontiers in Computational Neuroscience, 12.
Parr, T. and Friston, K. J. (2018b). Generalised free energy and active inference: can the future cause
the past? BioARXIV.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction (second edition).
MIT Press.
29
Thrun, S., Burgard, W., and Fox, D. (2006). Probabilistic Robotics. MIT Press.
Todorov, E. (2008). General duality between optimal control and estimation. In Proceedings of
the47th IEEE Conference on Decision and Control, Cancun, Mexico, Dec. 9-11.
Toussaint, M. (2009). Probabilistic inference as a model of planned behavior. Kunstliche Intelligenz,
3.
Toussaint, M. and Storkey, A. (2006). Probabilistic inference for solving discrete and continuous
state markov decision processes. Proceedings of the 23rd International Conference on Machine
Learning, 2006:945–952.
Verbelen, T., Lanillos, P., and Buckley, Christopher L. De Boom, C., editors (2020).Abtive Inference,
First International Workshop, IWAI 2020Co-located with ECML/PKDD 2020Ghent, Belgium,
September 14. Springer.
Ziebart, B., Ratliff, N., Gallagher, G., Mertz, C., Peterson, K., Bagnell, J., Hebert, M., Dey, A., and
Srinivasa, S. (2009). Planning-based prediction for pedestrians. pages 3931–3936.
Ziebart, B. D., Bagnell, A., and Dey, A. K. (2010). Modeling interaction via the principle of maximum
causal entropy. In Proceedings of the 27th International Conference on Machine Learning (ICML),
Haifa, Israel.
Table 3: Forward distributions for the source blocks.
fAt(at) fS0 (s0) f(StAt)(stat)
Sum product
Max product
Sum/Max product
DP
Max-Rew/Ent
SoftDP
p(at) p(s0) c(stat)
Table 4: Propagation rules for action shaded blocks.
bAt(at) f(StAt)(stat)
Sum product
Max-Rew/Ent (α= 1)
∑
st
b(StAt)(stat) fAt(at)U(st)
Max product
DP max
st
b(StAt)(stat) fAt(at)U(st)
Sum/Max product
Max-Rew/Ent (α̸= 1) α
√∑
st
b(StAt)(stat)α fAt(at)U(st)
SoftDP exp
[∑
st log b(StAt)(stat)b(StAt)(stat)β
∑
s′
t
b(StAt)(s′
tat)β
]
fAt(at)U(st)
30
Table 5: Propagation rules for state shaded blocks.
bSt(st) f(StAt)(stat)
Sum product
Max-Rew/Ent (α= 1)
∑
at
b(StAt)(stat) fSt(st)U(at)
Max product
DP max
at
b(StAt)(stat) fSt(st)U(at)
Sum/Max product
Max-Rew/Ent (α̸= 1) α
√∑
at
b(StAt)(stat)α fSt(st)U(at)
SoftDP exp
[∑
at log b(StAt)(stat)b(StAt)(stat)β
∑
a′
t
b(StAt)(sta′
t)β
]
fSt(st)U(at)
Table 6: Propagation rules for the dynamics block.
b(StAt)(stat) fSt+1 (st+1)
Sum product
∑
st+1
p(st+1|stat)bSt+1 (st+1)
∑
stat
p(st+1|stat)f(StAt)(stat)
Max product max
st+1
p(st+1|stat)bSt+1 (st+1) max
stat
p(st+1|stat)f(StAt)(stat)
Sum/Max product α
√∑
st+1
p(st+1|stat)αbSt+1 (st+1)α α
√∑
stat
p(st+1|stat)αf(StAt)(stat)α
DP
SoftDP
Max-Rew/Ent
e
∑
st+1 p(st+1|stat) logbSt+1 (st+1) e
∑
stat p(st+1|stat) logf(StAt)(stat)
31
Table 7: Propagation rules for the diverter.
Sum product
Max product
Sum/Max product
DP
Max-Rew/Ent
SoftDP
b(StAt)1 (stat) ∝f(StAt)2 (stat)f(StAt)3 (stat)b(StAt)4 (stat)
b(StAt)2 (stat) ∝f(StAt)1 (stat)f(StAt)3 (stat)b(StAt)4 (stat)
b(StAt)3 (stat) ∝f(StAt)1 (stat)f(StAt)2 (stat)b(StAt)4 (stat)
f(StAt)4 (stat) ∝f(StAt)1 (stat)f(StAt)2 (stat)b(StAt)3 (stat)
32