A Unified View of Algorithms for Path Planning
Using Probabilistic Inference on Factor Graphs
FrancescoA.N.Palmieri1 KrishnaR.Pattipati2 GiovanniDiGennaro1
GiovanniFioretti1 FrancescoVerolla1 AmedeoBuonanno3
1DipartimentodiIngegneria
UniversitàdegliStudidellaCampania“LuigiVanvitelli”
Aversa(CE),Italy
{francesco.palmieri, giovanni.digennaro}@unicampania.it
{giovanni.fioretti, francesco.verolla}@studenti.unicampania.it
2DepartmentofElectricalandComputerEngineering
UniversityofConnecticut
Storrs(CT),USA
krishna.pattipati@uconn.edu
3ENEA
DepartmentofEnergyTechnologiesandRenewableEnergySources
Portici(NA),Italy
amedeo.buonanno@enea.it
Abstract
Even if path planning can be solved using standard techniques from dynamic
programmingandcontrol,theproblemcanalsobeapproachedusingprobabilistic
inference. The algorithms that emerge using the latter framework bear some
appealing characteristics that qualify the probabilistic approach as a powerful
alternativetothemoretraditionalcontrolformulations.Theideaofusingestimation
on stochastic models to solve control problems is not new and the inference
approach considered here falls under the rubric of Active Inference (AI) and
ControlasInference(CAI).Inthiswork,welookatthespecificrecursionsthat
arisefromvariouscostfunctionsthat,althoughtheymayappearsimilarinscope,
bearnoticeabledifferences,atleastwhenappliedtotypicalpathplanningproblems.
Westartbyposingthepathplanningproblemonaprobabilisticfactorgraph,and
showhowthevariousalgorithmstranslateintospecificmessagecompositionrules.
Wethenshowhowthisunifiedapproach,presentedbothinprobabilityspaceand
inlogspace,providesaverygeneralframeworkthatincludestheSum-product,
the Max-product, Dynamic programming and mixed Reward/Entropy criteria-
based algorithms. The framework also expands algorithmic design options for
smootherorsharperpolicydistributions,includinggeneralizedSum/Max-product
algorithm,aSmoothDynamicprogrammingalgorithmandmodifiedversionsof
theReward/Entropyrecursions. Weprovideacomprehensivetableofrecursions
Preprint.Underreview.
1202
nuJ
91
]GL.sc[
1v24401.6012:viXra
andacomparisonthroughsimulations,firstonasyntheticsmallgridwithasingle
goalwithobstacles,andthenonagridextrapolatedfromareal-worldscenewith
multiplegoalsandasemanticmap.
1 Introduction
Probabilisticmethodsformodelingthedynamicbehaviorofagentsnavigatingincomplexenviron-
mentsarebecomingincreasinglypopularintherecentliterature.Inmanyapplications,therecognition
thatastochasticcontrolproblemcanbesolvedthroughprobabilisticinferenceonagenerativemodel,
hassparkednewgrowinginterest. Theideathatanagentcanuseitsbestinferenceaboutthefuture
basedonitsapproximateknowledgeoftheenvironment,toconditionitspresentactions,canbea
quitepowerfulparadigm. DynamicProgramming(DP)algorithmsforMarkovDecisionProcesses
(MDP)andPartially-ObservableMarkovDecisionProcesses(POMDP)(Thrunetal.,2006;Bertsekas,
2019)arebasedonthisconcept,suggestingthatanagentcanactoptimallyfollowingabestbehaviors
derivedfromavaluefunctionback-propagatedfromthehypotheticalfuture. Analogiesbetween
controlandestimationscanbetracedbacktotheworkofKalman(Todorov,2008)andtomorerecent
attemptstoseeprobabilitiesandrewardsunderthesameframework(Kappenetal.,2012;Levine,
2018).
Inpathplanning,sincetheoriginalpaperbyAttias(2003),therehasbeenagrowingbodyofliterature
intryingtoclarifytheconnectionsbetweentheprobabilisticmethodsandmoretraditionalstochastic
control strategies (Toussaint and Storkey, 2006; Toussaint, 2009; Levine, 2018). Also the terms
ActiveInference(AI),ControlasInference(CAI),havebeenrecentlycoined(Verbelenetal.,2020)
withsomeofthesemodelsbasedontheso-calledfree-energyprinciple(Buckleyetal.,2017;Parrand
Friston,2018b),onKL-learning(Alexandreetal.,2018;ParrandFriston,2018a;KaplanandFriston,
2018),andonMaxentropy(Ziebartetal.,2010). Furthermore,intriguingconnectionshavebeen
drawn,forsomeofthesemethods,toneuroscienceandbraintheory(BaltieriandBuckley,2017).
Causalreasoning(Nairetal.,2019)alsoseemstosharesomeelementsofthisgoal-directedbehavior.
Inourpresentwork,wetrytogobeyondthenowwell-recognizedanalogiesbetweenprobability
andcontrol,becauseevenifsomeoftheresultingalgorithmslooksimilar,theymaybearmarked
differenceswhenappliedtorealworldscenarios. InReinforcementLearning(RL)(SuttonandBarto,
2018),forexample,oftenwehaveonlypartialknowledgeoftheenvironmentviaanapproximate
stochasticmodelandweneedtoperformfurtherexploration. Buthowdowechooseamongthe
variousalgorithms,whicharetypicallyintroducedforknownsystemdynamics,whentheyareused
ontemporaryknowledge? Itisoftenarguedthatstochasticpolicymethodsmaybemoreappropriate
duringexploration,becausetheycaneasilyencompassuncertaintiesandallowcautiousbehavior
(Levine,2018). However,therearestillmanyopenquestionsontheexploration-exploitationissues
andevenmorefundamentallythereisstillalackofgeneralviewonthispowerfulparadigm,even
withknowndynamics.
In some of our previous works, we have proposed various techniques for modeling the motion
behaviorsofpedestriansandships(Castaldoetal.,2014;Cosciaetal.,2016;Cosciaetal.,2018;
Coscia et al., 2018a,b). More recently, however, in experimenting with probability propagation
methodsfordeterminingthebestpath(Palmierietal.,2021),wecametorealizethattheprobabilistic
algorithmsmaybethemostpromisingapproachesforagilemodelingofintelligentagentmotionin
complexscenes.
Inthiswork,weassumethatthesystem’sstochastictransitionfunctionisknownandthatboththe
stateandtheactionspacesarediscretefinitesetsthatcanbehandledwithtabularmethods.Extensions
tocontinuousspacecanbeconsideredwithapproximationstothevaluefunction,buttheywillnotbe
addressedhere. Also,wedonotaddresslearninghere,becausewebelievethataunifiedviewon
thevariouscostfunctionsandrecursionswithknownstochasticsystemdynamicsshouldbethefirst
stepintryingtounderstandthemorechallengingRLadaptationrules. Morespecifically,focusingon
thepathplanningproblemonadiscretegrid,andassumingthatthesystemdynamicsarestochastic
andknowntotheagent,wecansystematicallycomparethevaluefunctionsandthecorresponding
optimalpoliciesthatresultfromvariousrecursions. Standardprobabilitymessagepropagation,such
astheSum-productandtheMax-productalgorithms(Barber,2012),arecomparedtoDPusinga
unifiedviewtogetherwithothermethodsbasedonjointReward/Entropymaximization(Verbelen
etal.,2020;Levine,2018;Ziebartetal.,2009;Millidgeetal.,2020;Imohiosenetal.,2020). To
2
ourknowledge,nocomprehensivecomparisonexistsintheliterature,andourcontributionaimsat
providingthereaderwithaready-to-usesuiteofalgorithmsderivedwithingaunifyingframework.
Thebasicideaofthisworkistoshowhowwecanmapthestochasticproblemtoafactorgraph
wheredifferentmethodscorrespondtodifferentpropagationrulesthroughsomeofthegraph’sblocks.
Informationcantravelinthesystembothintheprobabilityspaceandinthelog-space;thisallows
ustoincludeandgeneralizethepreviouslyproposedmethodstonewsuitesofalgorithms,thereby
increasingouroptionstoconditiontheagent’spolicydistribution.
In this paper, we use directed Factor Graphs (FG), that assign variables to edges and factors to
interconnectedblocks. Generallyspeaking,messagepropagationinFGismoreeasilyhandledin
comparisontopropagationingraphsinwhichthevariablesareinthenodes(KollerandFriedman,
2009). Further reduction of the burden of defining message composition rules can be achieved
usingFactorGraphsinnormalform(FGn),proposedbyForney(2001)(Loeliger,2004). AFGn
convenientlyincludesjunctionnodes(equalityconstraintnodes)thatsplitincomingandoutgoing
messageswhenvariablesaresharedbymultiplefactors. Wehaveproposedasmallmodificationto
theFGninourFactorGraphinReducednormalform(FGrn)(Palmieri,2016)byincludingshaded
blocksthatmapsinglevariablestojointspaces.Infact,inanorientedgraph,whenavariablehasmore
thanoneparent,properforwardandbackwardmessageshavetogothroughtheparents’jointspace
(marriedparents). InaFGrn,theshadedblocksdescribethispassageandallowauniquedefinition
of message propagation rules through Single-Input/Single-Output (SISO) blocks. Computational
complexityissuesinsomeFGrnarchitectureshavebeenaddressedin(DiGennaroetal.,2021). In
thestandardsum-productalgorithm,backwardpropagationthroughshadedblockscorrespondsto
marginalization. Wewillshowinthispaperhowthisoperationcanbegeneralizedandhowitplaysa
crucialroleinthedifferentpathplanningalgorithms. Asmentionedabove,weconfineourselveshere
todiscretevariables. However,factorgraphsthatpropagatecontinuousdistributionsarepossibleand
maybedevisedalsoforpathplanning. GaussianmessageshavebeenintroducedinLoeligeretal.
(2007)andhavealreadybeenusedforKalmanfilter-basedtrackingin(CastaldoandPalmieri,2015)
usingFGrn. Thisissuewillnotbeaddressedhereandwillbethesubjectoffuturework.
Thecontributionsofthispapercanbesummarizedasfollows:
1. ThepathplanningproblemismappedtoaFactorGraphinReducedNormalForm.
2. Variousalgorithms,suchastheSum-product,theMax-product,DPandReward/Entropymaxi-
mization(thelatterrelatedtostructuralvariationalinference),areincludedinthesameframework,
bothinprobabilityandinlogspaces. Theyareallderivedusingdifferentcostfunctions,buttheyare
allreducedtospecificpropagationrulesthroughsomeoftheFGrnblocks.
3. TheequivalentQ-functionsandV-functionsintheprobabilityspace,seenasalternativestothe
well-knownDPformulation,allowsustowritethepolicydistributionforallthealgorithmswitha
uniqueexpression.
4. Using this general framework, we extend some of the known algorithms to a whole suite of
newparametricupdatesthatcancontrolthesmoothnessinthepolicydistributions. Theseproposed
parametricupdatescanbeusedtobalanceexplorationandexploitationinreinforcementlearning.
5. We provide simulations, first on a small grid with one goal and obstacles, then on a larger
gridextractedfromarealscenewithmultiplegoals(exits)andasemanticmap. Theresultsshow
somemarkeddifferencesin: (a)thespeedofconvergetothesteady-statevaluefunction, where
probabilistic methods are clearly favored; (b) how the Max-product algorithm may be preferred
foritsfasterconvergenceandfortheshapeandsmoothnessofitsvaluefunctions;(c)howvarious
algorithmscanbecontrolledwithparametricupdatestoexhibitdifferentsmoothnessintheirpolicy
distributions.
We believe that our contribution in this paper may prove useful for further deployment of RL
algorithms,especiallywhentheenvironmentisnotcompletelyknownandexplorationandexploitation
havetobeproperlybalancedonthebasisofpartialmodelknowledge.
Outline of the paper: In Section 2, we present the bayesian model and the corresponding factor
graph. In Section 3, the Sum-product algorithm is discussed in the framework of FGrn with the
messagecompositionrulesandtheupdates. Theinferencesarepresentedbothinaparallelandina
progressiveversionofthemessagecomposition. InSection4,themaximumaposteriorisolutionof
theMax-productalgorithmisanalyzedwithourproposedSum/Max-productalgorithmdescribedin
3
Section5. DynamicprogrammingistranslatedintothisframeworkinSection6andourproposalsfor
ageneralizedSoftDPareinSection7. Theapproachestocombinedmaximumrewardandentropyare
discussedinSection8,whereacostfunctionthatincludessmoothgeneralizationsisalsodiscussed.
Simplificationsofsomeoftherecursionswhenthesystemequationsaredeterministic,arediscussed
inSection9. Theextensiontoinfinitehorizonmodelsandconsiderationsonthesteady-statesolutions
are included in Section 10. Simulations are conducted on two types of grids in Section 11 and
conclusions and suggestions for further research are in Section 12. The analysis of the soft-max
functionsusedinthepaperareinAppendixA,andtheproofsforthereward/entropymethodsarein
AppendixB.
Figure1: State-ActionModelasaBayesiangraph
2 TheBayesianModel
Figure1showsthestate-actionmodelasaBayesiangraphwhere{S }isthestatesequence,{A }is
t t
theactionsequence. Weassume,withoutlossofgenerality,thatbothsequencesbelongtodiscrete
finitesets: A ∈AandS ∈S. Thereward/outcomesequence{O }isbinarywithO ∈{0,1}.
t t t t
ThemodelevolvesoverafinitehorizonT andthejointprobabilitydistributionofthestate-action-
outcomesequencecorrespondstothefactorization1
T−1
(cid:89)
p(s a o ...s a o )=p(o |s a )p(s )p(a ) p(s |s a )p(a )p(o |s a ),
1 1 1 T T T T T T 1 T t+1 t t t t t t
t=1
where the function p(s |s a ) describes the system dynamics, p(a ) are the action priors and
t+1 t t t
p(o |s a )arethereward/priorsonthestate-actionpairs. Morespecifically,weassumethat
t t t
P(O =1|s a )∝c(s a )≥0; P(O =0|s a )∝U(s a ),
t t t t t t t t t t
where the function c(s a ) acts as a prior distribution on the pair (s a ), only if O = 1. When
t t t t t
O =0,nopriorinformationisavailableonthatstate-actionpair,andthefactorbecomestheuniform
t
distributionU(s a ).2
t t
Thisformulationallowstheintroductionofarewardfunctionas
R(s a )=logc(s a )+K, (1)
t t t t
whereK isanarbitrarypositiveconstant. ThevalueK isreallyirrelevantbecausegoingbackto
probabilitieswehave
c(s a )∝eR(stat)−K,
t t
withtheconstantthatdisappearsafternormalization. WecansetK toalargevalueifwedonotlike
tohandlethenegativerewardsthatwegetfromthelogfunctionforK =0. Inthefollowing,without
lossofgenerality,weassumethatallourrewardsarenegative(K =0).
1Evenifthenotationshouldhavecapitallettersforrandomvariablesassubscriptsandlowercaselettersfor
theirvaluesinthefunctions,asin
p (s a o ...s a o ); p (s |s a ); p (o |s a ),
S1A1O1...STATOT 1 1 1 T T T St+1|StAt t+1 t t Ot|StAt t t t
weuseacompactnotationwithnosubscriptswhenthereisnoambiguity.Insomeofthemessagesthatfollow
weincludethesubscriptsonlywhennecessary.
2Inourdefinition,weassumethatc(s a )isnormalizedtobeavalidpdf,evenifitiswell-knownthat,in
t t
performinginferenceinaprobabilisticgraph,normalizationisirrelevant.
4
Theintroductionofthesequence{O }hasoftenbeenproposedforconnectingrewardstoprobabilities
t
(Kappenetal.,2013;Levine,2018). Wewouldliketoemphasizethatinterpretingthefactorsc(s a )
t t
aspriorinformationintheprobabilityfactorization,maysolve,atleastforplanningproblems,the
well-knownissueofdefininganappropriaterewardfunction. Infact,inapracticalproblem,wemay
haveavailablestatisticsonhowoftenastateisvisitedandhowcertainactionsmaybemorelikely
thanothers.
Notethatwhenastate-actionpairhaszeroprobability,forexamplewithforbiddenstates,orimpos-
sibleactions,obstacles,etc.,therewardfunctiontakesvalue−∞. Thisisreallynotaproblemin
practice,becausewecaneasilyapproximatesuchavaluewithalargenegativenumber.
Notethatinthemodelwehaveincludedaseparatefactorp(a )forthepriorsonA ,evenifsuch
t t
informationcouldbeincludedinc(s a ). Wehavepreferred, tobeconsistentwiththeBayesian
t t
graphofFigure1,tokeepthetwofactorsseparate,oneformarginalactionpriorsandoneforjoint
priors(rewards).
Omittingthesequence{O }inthenotation,orequivalentlyassumingthatpriorinformationisalways
t
available,thefactorizationismorecompactlywrittenas
T−1
(cid:89)
p(s a ...s a )=c(s a )p(s )p(a ) p(s |s a )p(a )c(s a ). (2)
1 1 T T T T 1 T t+1 t t t t t
t=1
Thisiswithoutlossofgeneralitybecause,forsomevaluesoft,thedistributionc(s a )canjustbe
t t
takentobeuniform.
Figure2: State-ActionModelforT =4asaFactorGraphinReducednormalform. Theone-edge
blocksaresources(priors);thetwo-edgewhiteblocksrepresentthesystemdynamics;theshaded
blocksmapsinglevariablestotheirjointspace;thedivertersconnectthevariablesconstrainedtobe
equal.
2.1 Thefactorgraph
Inferenceinagraphicalmodelismoreeasilyhandledwithreferencetoanequivalentfactorgraph,
wherevariablesareonthebranchesandfactorsareintheblocks. WeusehereFactorGraphsin
Reduced normal form (FGrn) (Palmieri, 2016). Factor graphs in normal form (FGn) have been
introducedbyForney(2001)anddiscussedbyLoeliger(2004),mostlyforcodingproblems. They
allowtheprobabilityfactorizationtobewrittenintermsofagraphinwhichvariablesareedgesand
factorsarenodeswithdiverters(equalityconstraintsnodes),thatmultiplexmessagestotheother
factors. Since,inaFGn,afactorcanstillhavemultipleinputs,weimposeafurthersimplification
intheFGrnbytransformingthegraphintoaconfigurationthatincludesshadedblocks(marginaliz-
ers/expanders);thisreducesthegraphtoacompositionofonlySISO(Single-Input-Single-Output)
blocksanddiverters. FGrnprovideaverydetaileddisplayofthemessagepropagationflowandhave
asimpleuniqueformulationintermsofmessagepropagationrules.
Figure2showsthefactorgraphinreducednormalformfortheBayesianmodelofFigure1forT =4.
The prior distributions p(a ) and c(s a ) are in the source nodes and the dynamics p(s |s a )
t t t t+1 t t
are in SISO blocks. The junctions describe equality constraints, and the shaded blocks describe
themappingfromsinglevariables’spacetoajointspace,i.e. p(s a |a(cid:48)) = U(s )δ(a −a(cid:48))and
t t t t t t
p(s a |s(cid:48)) = δ(s −s(cid:48))U(a ). Essentially, in a shaded block, the input variable is copied to the
t t t t t t
outputandjoinedtotheothervariablethat, inthatbranch, carriesnoinformationintheforward
direction. Eachbranchhasadirectionandaforwardandabackwardmessageassociatedtoit. Justas
inanybeliefpropagationnetwork,allmessagesareproportionaltoprobabilitydistributionsandtheir
5
compositionrulesallowtheagilederivationofinferencealgorithms. Notehowthereplicas(S A )i,
t t
i=1:4,ofthesamevariable(S A )aroundthediverterblockhavedifferentnamesandmessages
t t
associatedwiththem.
Wewillseehowthepathplanningproblemhasauniqueformulationonthefactorgraph.Bychanging
thepropagationrulesforsomeoftheblocks,weobtaintheoptimalsolutionsforthevariousproblem
formulations.
2.2 Introducingconstraints
Oneofthemainadvantagesofstudyinginferenceproblemsongraphsusingmessages,isthatproblem
constraints are easily included in the flow. Furthermore, we have assumed a finite time segment
t=1:T,butthismodelmayaswellrepresentasegmentt=t +1:t +T ofalongerprocess,
0 0
wherewehavefreedomtointroducetheinitialandfinalconditionsintheforwardmessagesatthe
beginning,andinthebackwardmessagesattheend,respectively.
Forexample: (a)AknownstartingstateS =s canbeincludedasaforwardmessagef (s )=
1 1 S1 1
δ(s −s ), whereδ(x) = 1ifx = 0, and0else; (b)IfwehavenopriorinformationonS , we
1 1 1
setf (s ) = U(s );(c)KnowledgeoftheinitialactionA = a canbeincludedasf (a ) =
S1 1 1 1 1 A1 1
δ(a − a ); (d) Knowledge of the final state (only) S = s is b (s a ) = δ(s −
1 1 T T (STAT)4 T T T
s )U(a );(e)Knowledgeofthestateattimet maybeincludedasf (s )=δ(s −s );(f)Ina
T T 0 St0 t0 t0 t0
planningproblem,aknownmapm(s )canbeassociatedtothefactorc(s a )withf (s a )∝
t t t (StAt)3 t t
m(s )U(a ); (g)Inthesameplanningproblem, jointmap-actioninformationcaninjectedasthe
t t
messagef (s a )∝c(s a );ifactionandmapareindependent,andtheactionpriorisp(a ),
(StAt)3 t t t t t
f (s a ) ∝ m(s )p(a ),orequivalentlyf (s a ) ∝ m(s )U(a )andf (a ) = p(a );
(StAt)3 t t t t (StAt)3 t t t t At t t
etc.
WedenotecollectivelyalltheconstraintsavailableonthejointmodelasK ,withthejointmodel
1:T
writtenincompactformasp(s a ...s a |K ).
1 1 T T 1:T
2.3 Inferenceobjectives
Ourinferenceaimsatprovidingsolutionstooneormoreofthefollowingproblems:
1. Findthebeststatesequence(S):s∗ →s∗ →···→s∗
1 2 T
2. Findthebestactionsequence(A):a∗ →a∗ →···→a∗
1 2 T
3. Findthebestjointstate-actionsequence(SA):(s a )∗ →(s a )∗ →...→(s a )∗
1 1 2 2 T T
4. Findthebeststate-actionsequence(SASA):s∗ →a∗ →s∗ →a∗ →···→s∗ →a∗
1 1 2 2 T T
5. Findthebestaction-statesequence(ASAS):a∗ →s∗ →a∗ →s∗ →···→a∗ →s∗
1 1 2 2 T T
6. Findthebestpolicydistributions(P):π∗(a |s ),π∗(a |s )...π∗(a |s )
1 1 2 2 T T
Wewillseeinthefollowinghowvariouscostfunctionsdeterminethemessagecompositionrules
acrosstheblockstosolvetheaboveproblemsbothintheprobabilityspaceandinthelog-spaceand
accordingtodifferentoptimizationcriteria. Inthefollowingdiscussion,wewillconcentratemostly
ontheSsequence,ontheSASAsequenceandonthepolicydistribution,sincetheextensionstoA,
SA,andASASsequencesarestraightforward.
3 MarginalizationandtheSum-product
StandardinferenceinBayesianmodelconsistsinmarginalizingthetotaljointprobabilitydistribution
toobtaindistributionsthatareproportionaltotheposteriorsonsinglevariables(KollerandFriedman,
2009). Morespecifically,forstatesonly,foractionsonlyandforstatesandactionsjointly,wewant
6
tocomputetheposteriorsas
(cid:88)
p(s |K )∝ p(s a ...s a |K ),
t 1:T 1 1 T T 1:T
sj,j(cid:54)=t,j=1:T
aj,
(cid:88)
j=1:T
p(a |K )∝ p(s a ...s a |K ),
t 1:T 1 1 T T 1:T (3)
sj,j=1:T
aj,j(cid:54)=t
(cid:88)
,j=1:T
p(s a |K )∝ p(s a ...s a |K ).
t t 1:T 1 1 T T 1:T
sj,aj,j(cid:54)=t,j=1:T
Thepolicydistributionsareobtainedbyfixingthestates attimet,andaccountingfortheforeseeable
t
futureuntilT
p(s a |K )
π∗(a |s )(cid:44)p(a |s ,K )= t t t:T , t=1:T. (4)
t t t t t:T p(s |K )
t t:T
Thepolicydistributiondescribesattimethowlikelyitistotakeactiona fromstates ,givenallthe
t t
availableinformation(constraints,priors,etc.) aboutthefuture(K ).
t:T
Alltheabovefunctionscanbeobtainedusingforwardandbackwardmessagepropagationusingthe
Sum-productrule(KollerandFriedman,2009;Barber,2012). Thisapproachessentiallyaverages
overthevariablesthatareeliminatedacrosseachSISOblock. WithreferencetoFigure2,justby
followingtheflow,forsomeofthebackwardmessageswehave
(cid:80)
b (s a )∝ p(s |s a )b (s ),
(StAt)4 t t st+1 t+1 t t St+1 t+1
b (s a )∝p(a )c(s a )b (s a ),
(StAt)1 t t t t t (StAt)4 t t
b (s a )∝f (s a )c(s a )b (s a ), (5)
(StAt)2 t t
(cid:80)
(StAt)1 t t t t (StAt)4 t t
b (a )= b (s a ),
At t
(cid:80)
st (StAt)2 t t
b (s )= b (s a ).
St t at (StAt)1 t t
Forsomeoftheforwardmessages
(cid:80)
f (s )∝ p(s |s a )f (s a ),
St+1 t+1 stat t+1 t t (StAt)4 t t
f (s a )=f (s )U(a ),
(StAt)1 t t St t t (6)
f (s a )=U(s )f (a ),
(StAt)2 t t t At t
f (s a )∝f (s a )f (s a )c(s a ).
(StAt)4 t t (StAt)1 t t (StAt)2 t t t t
Note that going backward through a block, the message may not be normalized. Around the
diverters,outgoingmessagesaretheproductoftheincomingones,andarenotnormalized. Message
compositionrulesaresummarizedinTables3,4,5,6and7fortheSum-productandfortheother
approachesthatwillbediscusseslater.
Posteriordistributionsareobtainedbytakingtheproductofforwardandbackwardmessages
p(s |K )∝f (s )b (s ),
t 1:T St t St t
p(a |K )∝f (a )b (a ), (7)
t 1:T At t At t
p(s a |K )∝f (s a )b (s a ), foranyi=1,2,3,4.
t t 1:T (StAt)i t t (StAt)i t t
Forreadersnottoofamiliarwithprobabilitymessagepropagation,wewouldliketoemphasizethat
thisframeworkisarigorousapplicationofBayes’theoremandmarginalization. Alsoallmessages
canbenormalizedtobevaliddistributions,evenifitisnotstrictlynecessary(itistheirshapethat
matters). However,itisoftenadvisedtokeepmessagesnormalizedfornumericalstability.
The policy distribution at each t is derived as a consequence of the inference obtained from the
probabilityflowandis
f (s a )b (s a ) f (s )U(a )b (s a ) b (s a )
π∗(a |s )∝ (StAt)1 t t (StAt)1 t t = St t t (StAt)1 t t = (StAt)1 t t , (8)
t t f (s )b (s ) f (s )b (s ) b (s )
St t St t St t St t St t
where we have used the branch with i = 1. It is easy to verify that the solution would have an
equivalentexpressionforanyotherbranch. Notealsohowthepolicydependsonlyonthebackward
messages. Thereasonforthisisthatbyconditioningons ,alltheinformationcomingfromtheleft
t
sideofthegraphisblocked.
7
3.1 MaxPosteriorsequences
Optimalsequencevaluescanbeobtainedinparallelusingmaximizationontheposteriors
s∗=argmaxp(s |K )=argmaxf (s )b (s ),
t t 1:T St t St t
st st
a∗=argmaxp(a |K )=argmaxf (a )b (a ), t=1:T
t t 1:T At t At t (9)
at at
(s a )∗=argmaxp(s a |K )=argmaxf (s a )b (s a ),
t t t t 1:T (StAt)1 t t (StAt)1 t t
stat stat
Themaxposteriorsolutionsaretakenseparatelyoneachvariableand,eveniftheyareoftenusedin
theapplications(forexampleindecodingconvolutionalcodes-thealgorithm,isnamedBCJRafter
itsauthorsBahletal.(1974)),theymayprovideunsatisfactorysequencesforpathplanning. Infact,
thesequencesthatresultfromthemaximizationsareunconstrainedintimeandmaycorrespondto
disconnectedpaths(Palmierietal.,2021).
3.2 ProgressiveMaxPosteriorsequences
Bettersolutionsforthemaxposteriorapproachareobtainedprogressivelyintimefollowingaforward
procedure.3
Forthestates-only(S)sequence
s∗=argmaxp(s |K )=argmaxf (s )b (s ),
1 1 1:T S1 1 S1 1
s1 s1
s∗=argmaxp(s∗s |K )=argmaxf (s |s∗)b (s ),
2 1 2 2:T S2 2 1 S2 2
s2 s2
s∗=argmaxp(s∗s∗s |K )=argmaxf (s |s∗)b (s ),
3 1 2 3 3:T S3 3 2 S3 3 (10)
s3 s3
···
s∗=argmaxp(s∗...s∗ s |K )=argmaxf (s |s∗ )b (s ),
t 1 t−1 t t:T St t t−1 St−1 t−1
st st
···
wheretheconditionedforwardmessagescomefromaone-steppropagation
(cid:88)
f (s |s∗ )= p(s |s∗ a )f (s∗ a )
St t t−1 t t−1 t−1 (St−1At−1)4 t−1 t−1
a (cid:88)t−1 (11)
= p(s |s∗ a )p(a )c(s∗ a ).
t t−1 t−1 t−1 t−1 t−1
at−1
Note on the graph that knowledge of the state at time t−1 "breaks" the forward flow. Only the
backwardflowdrivestheinference.
Similarly,forthebestState-Action(SASA)sequence,theProgressiveMax-posterioralgorithmusing
themessagesonthegraphis
s∗=argmaxp(s |K )=argmaxf (s )b (s ),
1 1 1:T S1 1 S1 1
s1 s1
a∗=argmaxp(s∗a |K )=argmaxf (a )b (a |s∗),
1 1 1 1:T A1 1 A1 1 1
a1 a1
s∗=argmaxp(s∗a∗s |K )=argmaxf (s |s∗a∗)b (s ),
2 1 1 2 2:T S2 2 1 1 S2 2
s2 s2
a∗=argmaxp(s∗a∗s∗a |K )=argmaxf (a )b (a |s∗),
2 1 1 2 2 2:T A2 2 A2 2 2 (12)
a2 a2
···
s∗=argmaxp(s∗a∗...s∗ a∗ s |K )=argmaxf (s |s∗ a∗ )b (s ),
t 1 1 t−1 t−1 t t:T St t t−1 t−1 St t
st st
a∗=argmaxp(s∗a∗...s∗ a∗ s∗a |K )=argmaxf (a )b (a |s∗),
t 1 1 t−1 t−1 t t t:T At t At t t
at at
···
3Onafixedtimehorizon,asimilarprocedurecanbederivedgoingbackwardintime.Weprefertomaintain
theframeworkcausalandleaveitoutforbrevity.
8
wheretheconditionedforwardandbackwardmessagesmeanthatwehaveconsideredtheirvalues
whentheconditioningvariablesontheleftsideofthegrapharefixed. Fortheconditionedforward
wehave
f (s |s∗ a∗ )=p(s |s∗ a∗ ). (13)
St t t−1 t−1 t t−1 t−1
Fortheconditionedbackwardwehave
b (a |s∗)∝b (s∗a )∝c(s∗a )b (s∗a )f (s∗a ). (14)
At t t (StAt)2 t t t t (StAt)4 t t (StAt)1 t t
thatsince
b (s a )∝b (s a )f (s a )c(s a )=b (s a )p(a )U(s )c(s a ), (15)
(StAt)1 t t (StAt)4 t t (StAt)2 t t t t (StAt)4 t t t t t t
or
b (s a )
b (s a )∝
(StAt)1 t t
, (16)
(StAt)4 t t p(a )c(s a )
t t t
canberewrittenas
b (s∗a ) b (s∗a ) b (s∗a )
b (a |s∗)∝ (StAt)1 t t f (s∗a )= (StAt)1 t t δ(s −s∗)U(a )= (StAt)1 t t .
At t t p(a ) (StAt)1 t t p(a ) t t t p(a )
t t t
(17)
Therefore,theSASAestimationsimplifiesto
s∗=argmaxf (s )b (s ),
1 S1 1 S1 1
s1
a∗=argmaxb (s∗a ),
1 (S1A1)1 1 1
a1
s∗=argmaxp(s |s∗a∗)b (s ),
2 2 1 1 S2 2
s2
a∗=argmaxb (s∗a ),
2 (S2A2)1 2 2 (18)
a2
···
s∗=argmaxp(s |s∗ a∗ )b (s ),
t t t−1 t−1 St t
st
a∗=argmaxb (s∗a ),
t (StAt)1 t t
at
···
whichisobviouslythesameifweuseexplicitlytheoptimalpolicydistribution(8)
s∗=argmaxp(s |K )=argmaxf (s )b (s ),
1 1 1:T S1 1 S1 1
s1 s1
a∗=argmaxπ∗(a |s∗),
1 1 1
a1
s∗=argmaxp(s |s∗a∗)b (s ),
2 2 1 1 S2 2
s2
a∗=argmaxπ∗(a |s∗),
2 2 2 (19)
a2
···
s∗=argmaxp(s |s∗ a∗ )b (s ),
t t t−1 t−1 St t
st
a∗=argmaxπ∗(a |s∗),
t t t
at
···
Noteinallcasesthecrucialroleplayedbythebackwardflow.Wehavesuccessfullydemonstratedthis
approachforpathplanninginourpreviouswork(Palmierietal.,2021).Infact,intheprogressivemax
posterioralgorithm,theforwardflowisnotnecessary. Action-onlysequencesandASASsequence
canbeobtainedinasimilarfashionandareomittedhereforbrevity.
3.3 Sum-productinthelog-space
Wehaveseenabovehowinthefactorizedmodel(2),priordistributionsarerelatedtorewardsviathe
logtransformationin(1). Forthecomparisonsthatfollow,itisconvenienttoconsideralsosomeof
theSum-productrecursionsinthelog-space. Wedefinethefollowingfunctions
Q (s a )(cid:44)logb (s a ), i=1,2,3,4
(StAt)i t t (StAt)i t t (20)
V (s )(cid:44)logb (s ),
St t St t
9
Table 1: Summarized backup rules in probability space with b(s a ) (cid:44) b (s a ); b(s ) (cid:44)
t t (StAt)1 t t t
b (s );c(cid:48)(s a )(cid:44)p(a )c(s a ).
St t t t t t t
b(s a ) b(s )
t t t
(cid:88) (cid:88)
Sumproduct c(cid:48)(s a ) p(s |s a )b(s ) b(s a )
t t t+1 t t t+1 t t
st+1 at
Maxproduct c(cid:48)(s a )maxp(s |s a )b(s ) maxb(s a )
t t t+1 t t t+1 t t
st+1 at
Sum/Maxproduct (cid:115) (cid:88) (cid:115) (cid:88)
(α≥1) c(cid:48)(s t a t ) α p(s t+1 |s t a t )αb(s t+1 )α α b(s t a t )α
st+1 at
DP c(cid:48)(s t a t )e (cid:80) st+1 p(st+1|stat)logb(st+1) maxb(s t a t )
at
( M α a > x-R 0) ew/Ent c(cid:48)(s t a t )e (cid:80) st+1 p(st+1|stat)logb(st+1) (cid:115) α (cid:88) b(s t a t )α
at
SoftDP(β >0) c(cid:48)(s t a t )e (cid:80) st+1 p(st+1|stat)logb(st+1) e (cid:80) at b (cid:80) (s a t (cid:48) t at b ) ( β st l a o (cid:48) t g ) b β (stat)
Table2: SummarizedbackuprulesinlogspacewithQ(s a )(cid:44)Q (s a );V(s )(cid:44)V (s );
t t (StAt)1 t t t St t
R(cid:48)(s a )=logp(a )+R(s a ).
t t t t t
Q(s a ) V(s )
t t t
(cid:88) (cid:88)
Sumproduct R(cid:48)(s a )+log elogp(st+1|stat)+V(st+1) log eQ(stat)
t t
st+1 at
Maxproduct R(cid:48)(s a )+max(logp(s |s a )+V(s )) maxQ(s a )
t t t+1 t t t+1 t t
st+1 at
Sum/Maxproduct 1 (cid:88) 1 (cid:88)
(α≥1)
R(cid:48)(s
t
a
t
)+
α
log eα(logp(st+1|stat)+V(st+1))
α
log eαQ(stat)
st+1 at
(cid:88)
DP R(cid:48)(s a )+ p(s |s a )V(s ) maxQ(s ,a )
t t t+1 t t t+1 t t
at
st+1
( M α a > x-R 0) ew/Ent R(cid:48)(s t a t )+ (cid:88) p(s t+1 |s t a t )V(s t+1 ) 1 log (cid:88) eαQ(stat)
α
st+1 at
SoftDP(β >0) R(cid:48)(s
t
a
t
)+
(cid:88)
p(s
t+1
|s
t
a
t
)V(s
t+1
)
(cid:80)
at
Q(s
t
a
t
)eβQ(stat)
st+1
(cid:80)
a(cid:48)
t
eβQ(sta(cid:48)
t
)
NotethatthereisaQfunctionforeachmessagearoundthediverter. Thechoiceofthenotations
Q(Q-function)andV (Value-function)isnotcasual,asitwillbeclearinthediscussionthatwill
followondynamicprogramming. ThereisalsoadefinitionofV-functionfortheactionvariableA ,
t
V (a )(cid:44)logb (a ). Fromthedefinition,isobviousthatboththeQ-andV-functionsarenegative
At t At t
(wehavealreadypointedoutabovethatthisisnotalimitation). Weconcentrateheremostlyonthe
stateS forwhichthebackwardrecursionsin(5),arewritteninthelog-spaceas
t
Q
(StAt)4
(s
t
a
t
)∝log (cid:80)
st+1
p(s
t+1
|s
t
a
t
)eVSt+1 (st+1),
Q (s a )∝logp(a )+R(s a )+Q (s a ), (21)
(StAt)1 t t t t t (StAt)4 t t
V
St
(s
t
)∝log (cid:80)
at
eQ (StAt)1(stat).
Allmessagescanbepropagatedinthelog-space: theproductrulearoundthediverterofFigure2
becomesasumandthebackwardpropagationrulesacrossthedynamicsblockandtheshadedblock
aresimplytranslated. Forbettercomparisonwiththeformulationsthatfollow,were-writethefirst
10
equationof(21)as
(cid:88)
Q
(StAt)4
(s
t
a
t
)∝log elogp(st+1|stat)+VSt+1 (st+1). (22)
st+1
TherecursionsaresummarizedforcomparisoninthefirstrowofTables1and2.
Thesamerecursions,andsomeofthedefinitionsinthelog-space,havebeenreportedbyLevine
(2018)thatalsonoteshowthetransformationy =log (cid:80)N
j=1
exj isasoft-max(y ∼max(x
1
,...,x
N
)
when the x s are large), in contrast to the the hard max that is used in dynamic programming.
i
AppendixAsummarizesthepropertiesofthesoft-maxfunctionsthatariseinouranalyses.
ThebestSASAsequenceofequations(19)isequivalentlywritteninthelog-spaceas
s∗=argmaxp(s |K )=argmaxlogp(s )+V (s ),
1 1 1:T 1 S1 1
s1 s1
a∗=argmaxQ (a ,s∗),
1 (S1A1)1 1 1
a1
s∗=argmaxlogp(s |s∗a∗)+V (s ),
2 2 1 1 S2 2
s2
a∗=argmaxQ (a ,s∗),
2 (S2A2)1 1 1 (23)
a2
···
s∗=argmaxlogp(s |s∗ a∗ )+V (s ),
t t t−1 t−1 St t
st
a∗=argmaxQ (a ,s∗),
t (StAt)1 1 1
at
···
Thepolicydistribution(8)isrewrittenas
π∗(a
t
|s
t
)∝eQ (StAt)1(stat)−VSt (st). (24)
Figure3: BackwardrecursionsfortheMax-productalgorithmforT =4. Notethepresenceofthe
backwardmessageb (s a )attheendofthechainthatmaycarryinformationfromfurther
(S4A4)4 4 4
stepsormayrepresentfinalconstraints.
4 MaximumaposterioriandtheMax-product
The max posterior rules, described in Section 3.1 and 3.2, are used extensively for inference in
Bayesiannetworks,evenifitisoftenignoredthattheydonotnecessarilysolvetheglobalmaximum
aposterioriproblem
(s∗a∗...s∗a∗)= argmax p(s a ...s a |K ). (25)
1 1 T T 1 1 T T 1:T
s1a1...sTaT
TheSum-productpropagationrulessolvemarginalmaximumaposterioriproblemsafteraveragingon
theeliminatedvariables,whiletheglobaloptimizationrequiresadifferentstrategyforobtainingthe
11
solution. TheMax-productalgorithm(Barber,2012;Loeligeretal.,2007),inpropagatingmessages
inthegraph,insteadofcomputingaveragesacrosstheblocks,propagatesmaximavalues,provides
the solution. This is often named bi-directional Viterbi algorithm (Barber, 2012). The detailed
recursionsarederivedexplicitelyinFigure3foramodelwithT = 4inthenotationofthefactor
graphofFigure2. Atagenericstept,therecursionsforsomeofthebackwardmessagesare
b (s a )=maxp(s |s a )b (s ),
(StAt)4 t t
st+1
t+1 t t St+1 t+1
b (s a )=p(a )c(s a )b (s a ), (26)
(StAt)1 t t t t t (StAt)4 t t
b (s )=maxb (s a ).
St t
at
(StAt)1 t t
Againthecrucialroleisplayedbythebackwardflowthat,goingthrougheachSISOblock,does
notundergoasummation,butamax(inMax-productbayesiannetworksalsotheforwardflowis
computedusingmaxratherthatsum(Barber,2012);wefocusheremostlyonthebackwardflow). In
thelog-space,thebackwardrecursionsforthestatesarerewrittenas
(cid:2) (cid:3)
Q (s a )=max logp(s |s a )+V (s ) ,
(StAt)4 t t
st+1
t+1 t t St+1 t+1
Q (s a )=logp(a )+R(s a )+Q (s a ), (27)
(StAt)1 t t t t t (StAt)4 t t
V (s )=maxQ (s a ).
St t
at
(StAt)1 t t
ThebestSASAsequenceiscomputedintheforwarddirectioninwaysimilartotheSum-product,in
boththeprobabilityspaceandinthelog-space,asfollows
Att=1,wehave
s∗
1
=argmaxp(s
1
)b
S1
(s
1
)=argmaxp(s
1
)eVS1 (s1) =argmaxlogp(s
1
)+V
S1
(s
1
),
s1 s1 s1 (28)
a∗=argmaxb (s∗a )=argmaxQ (s∗a ).
1 (S1A1)1 1 1 (S1A1)1 1 1
a1 a1
Att=2
s∗
2
=argmaxp(s
2
|s∗
1
a∗
1
)b
S2
(s
2
)=argmaxp(s
2
|s∗
1
a∗
1
)eVS2 (s2)
s2 s2
=argmaxlogp(s 2 |s∗ 1 a∗ 1 )+V S2 (s 2 ), (29)
s2
a∗=argmaxb (s∗a ) =argmaxQ (s∗a ),
2 (S2A2)1 2 2 (S2A2)1 2 2
a2 a2
... etc.
Note how the forward recursions are formally identical to the ones derived for the Sum-product
algorithm. Also,thepolicyhasthesameformalexpression
b (s a )
π∗(a
t
|s
t
)∝ (St
b
At)
(
1
s )
t t =eQ (StAt)1(stat)−VSt (st). (30)
St t
Clearly,theQ-andtheV-functionsherehaveadifferentmeaning. TherecursionsforthebestSASA
sequencecanberewrittenintermsofpolicyandtheylookformallyidenticaltotheonesderivedfor
theSum-productalgorithm. Alltheothersequences,S,A,SA,ASAScanbecomputedusingthe
probabilityflowinthegraphfollowingthesameformalapproach,bothintheMax-productandinthe
Sum-product,simplybychangingsomeofthepropagationrules. Forbrevity,weconcentratehere
onlyonsomeofthemessages,butadetailedanalysisofotherpartsoftheflowmayrevealinteresting
aspectsoftheinference.
Tables3,4,5,6and7summarizethepropagationrulesacrossthefactorgraphfortheSum-product,
theMax-productandalltheotherapproachesthatwillfollow. Themainbackuprecursionsarealso
summarizedforcomparisoninTables1and2inthelog-space.
Wewouldliketoemphasizethatpropagatinginformationviaprobabilitydistributionsincludesallthe
casesinwhichtheremaybedeterministicvaluesinthesystem,i.e.,whenthedistributionsaredelta
functions. Furthermore,intheMax-productalgorithm,whenmultipleequivalentmaximaarepresent,
thedistributionscancarryimplicitlymultiplepeaks. Wewillsee,insomeofthesimulationexamples
that will follow, that the Max-product messages provide a complete set of options in the policy
distributions,alsowhenmorethanonebestactionisavailable. Obviously,inwritingthealgorithms,
someattentionmustbedevotedtounnormalizeddistributionswithvaluesthatareclosezeros,to
avoidnumericalproblems. Theproblemisusuallyovercamebynormalizationandbyreplacingzeros
withverysmallvalues.
12
5 TheSum/Max-Product
Theunifyingviewprovidedbythegraphicalmethod,bothintheSum-productandintheMax-product
approaches,isquiteappealingandonewonderswhethertheremaybeageneralrulethatencompasses
both. Toexaminethis,bylookingattherecursionsfortheSum-productalgorithm(Tables1and2,
firsttworows),weimmediatelyobservethattheSum-product,bothintheprobabilityandinthelog-
space,canbeseenasasoftversionoftheMax-productbecauseofthesoft-maxfunctions. Therefore,
weproposeageneralrulethatinterpolatesbetweenthetwosolutionsusingtheparametrizedsoft-max
functionsdiscussedinAppendixA.WenamethisgeneralizationtheSum/Max-productalgorithm,
thatinthelog-spacegives
Q
(StAt)4
(s
t
a
t
)=
α
1
log
(cid:88) eα[logp(st+1|stat)+VSt+1 (st+1)]
,
st+1
Q (s a )=logp(a )+R(s a )+Q (s a ), (31)
(StAt)1 t t t t t (StAt)4 t t
1 (cid:88)
V
St
(s
t
)=
α
log eαQ (StAt)1(stat),
at
withα≥1. Inprobabilityspace,theupdatesareimmediatelytranslatedas
 1
α
(cid:88)
b (StAt)4 (s t a t )∝ p(s t+1 |s t a t )αb St+1 (s t+1 )α  ,
st+1
b (s a )∝p(a )c(s a )b (s a ), (32)
(StAt)1 t t t t t (StAt)4 t t
(cid:34) (cid:35)1
α
(cid:88)
b (s )∝ b (s a )α .
St t (StAt)1 t t
at
Notethatthefunctionthatemergesintheprobabilityspacerecursions,isalsoasoft-max. Therefor,
bothinthelog-spaceandintheprobabilityspace,forα → ∞,theparametricsoft-maxfunctions
convergestothehardmax(seeAppendixAfordetailsaboutthesoft-maxfunctions). Forα = 1,
equations(31)and(32)becomeidenticaltothosederivedfortheSum-productalgorithm. TheMax-
productapproachusuallyproducesmuchmoredefinedvaluefunctionsandpolicies,incomparisonto
theSum-product,aswillbeshowninsomeoftheexamplesthatfollow. Interpolatingbetweenthe
twosolutionsmayprovidetheplanningproblemwithawholerangeofnewsolutionsbeyondthe
traditionalSum-productandMax-productapproaches. TheSum/Max-productupdatesareaddedas
thethirdrowinTables1and2andthedetailedblockpropagationrulesareincludedinTables3,4,5,
6and7. ThepolicyisformallythesameasintheSum-productandtheMax-product. Evidently,the
messages,bothintheprobabilityspaceandinthelog-space,carrydifferentinformation.
5.1 Whatfunctionisbeingoptimized?
The generalization of the Sum/Max-product has been derived as a straightforward interpolation
betweentheSum-productandtheMax-productandsuchafunctioncanspanthewholerangeof
solutionsbetweenthemaximizationofthemarginalsoftheSum-productalgorithmtothemaximiza-
tionoftheglobalposterioroftheMax-product. Whatisthen,foreachvalueoftheparameterα,the
functionthatthealgorithmoptimizes?
InthelowerpartofFigure4,wehavereportedtherecursionsoftheSum/Max-productalgorithmin
theprobabilityspaceforT =4. Itiseasilyseen,bylookingatthetopofthesamefigure,thatthey
matchtherecursionsoftheSum-productalgorithmasappliedtothefactorization
T−1
(cid:89)
p(s a ...s a )α =c(s a )αp(s )αp(a )α p(s |s a )αp(a )αc(s a )α, (33)
1 1 T T T T 1 T t+1 t t t t t
t=1
Obviouslythepowerofadistributionisnotanormalizeddistribution,butthisisnotaproblemaswe
mentionedbefore,becausenormalizationisjustascalethatisirrelevantfortheinference.
13
Figure 4: Backward recursions for the Sum/Max-product algorithm for T = 4. Note how the
recursionscanbeseenastheSum-productalgorithmappliedtothefactorizationwhereallthefactors
areraisedtoapowerα.
Therefore,inanalogytotheSum-productalgorithm,theSum/Max-productalgorithmprovidesthe
posteriors
(cid:88)
p(s |K )∝ p(s a ...s a |K )α,
t 1:T 1 1 T T 1:T
sj,j(cid:54)=t,j=1:T
aj,
(cid:88)
j=1:T
p(a |K )∝ p(s a ...s a |K )α,
t 1:T 1 1 T T 1:T (34)
sj,j=1:T
aj,j(cid:54)=t
(cid:88)
,j=1:T
p(s a |K )∝ p(s a ...s a |K )α.
t t 1:T 1 1 T T 1:T
sj,aj,j(cid:54)=t,j=1:T
Tobetterexplainthegeneralization,werecallthatraisingaprobabilitydistributiontoapowergreater
thanone,hastheeffectofsharpeningthedistributionarounditsmaximum(ormaxima,ifmultiple
maximaarepresent). Therefore,raisingthewholejointdensitytoalargepowerhastheeffectof
concentratingitontheglobalmaximumaposteriorisolutionoftheMax-productalgorithm. Note
thatthemaximaontheposteriorscanbecomputedinparallel,orprogressivelyinsequence. The
specificdiscussionisomittedhereforbrevity,butfollowsthesamestrategyusedfortheSum-product
algorithm.
Figure5:BackwardrecursionsfortheDynamicprogrammingalgorithmforT =4.Notethepresence
ofthebackwardmessageQ (s a )thatmaycarryinformationfromtimestepsbeyondT,or
(S4A4)4 4 4
mayrepresentfinalconstraints.
6 Dynamicprogrammingonthefactorgraph
Thestandardapproachtodynamicprogrammingisbasedonthemaximizationoftheexpectedsumof
rewards(Bertsekas,2019;SuttonandBarto,2018). Inprevioussections,wehaveincludedrewardsin
factorization(2),butwehaveformulatedtheoptimizationproblemasthemaximizationofposterior
14
probabilities, or marginals, which only implicitly involve the rewards. Obviously, one wonders
whether the two approaches can be seen under a unified framework - after all Bellman backups
resemble backward message combinations. We show here that it is possible to map DP directly
intothefactorgraphformulationifweconsiderrewardsandtheirexpectationsascontributingto
probabilitymessages,butinthelog-space. Wecanseehowintheprobabilityspace,theDPmessages
cantravelonthefactorgraph,justasintheSum/Max-productalgorithm,butwithdifferentdefinitions
ofthepropagationrulesthroughthebuildingblocks.
Thedynamicprogrammingalgorithm(Bertsekas,2019)isderivedasthesolutiontothefollowing
problem
(cid:34) T (cid:35)
(cid:88)
(a∗...a∗)=argmaxE (R(s a )+logp(a ))
1 T ∼p(s1a1...sTaT) t t t
a1...aT
t=1
(cid:34) T (cid:35)
(cid:88) (cid:88)
=argmax p(s 1 a 1 ...s T a T ) (R(s t a t )+logp(a t )) (35)
a1...aT
s1,...,sT
T−1
t=1
(cid:34) T (cid:35)
(cid:88) (cid:89) (cid:88)
=argmax p(s ) p(s |s a ) (R(s a )+logp(a )) ,
1 t+1 t t t t t
a1...aT
s1,...,sT t=1 t=1
wherep(s a ...s a )doesnotincludetherewardsandthepriorsona appearsintheloginthe
1 1 T T t
summation. Thisisslightlydifferentthanthesumofpurerewards. Thereasonforthismodification
has been to obtain the same formal recursions that we have derived for the Sum-product and for
theMax-productalgorithms. Inanycase,thisisnotacrucialproblembecauselogp(a )couldbe
t
incorporatedintoR(s a )andp(a )canbeassumedtobeuniform.
t t t
WehavereportedinFigure5,theDPrecursionsinthenotationsofthefactorgraphofFigure2for
T =4. Notethat,incomparisontoanalogousbackupsintheprobabilityspacefortheSum-product
(orMax-product)algorithm,therewardsappearasadditivetermsandthereisamixofmaxandsums.
Formally
(cid:88)
Q (s a )=logp(a )+R(s a )+ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
st+1 (36)
V (s )=maxQ (s a ).
St t
at
(StAt)1 t t
Translatingtherecursionsintheprobabilityspace,wehave
b
(StAt)1
(s
t
a
t
)=p(a
t
)c(s
t
a
t
)e
(cid:80)
st+1
p(st+1|stat)logbSt+1 (st+1)
,
(37)
b (s )=maxb (s a ).
St t
at
(StAt)1 t t
ThecrucialdifferencebetweenDPandtheSum-productalgorithmisinthefactthataveragesand
maximaaretakeninthelog-spaceonthevaluefunction. ConverselyintheSum-product,theyare
takenintheprobabilityspaceonthebackwarddistributions. Therefore,DPcanbeformulatedin
termsofprobabilitymessagestravelingonthesamefactorgraphoftheSum-productalgorithm,but
withdifferentcombinationrules. TheDPrecursionsarereportedinTables1and2forcomparison,
andthespecificrulesforthemessagesthroughtheblocksareinTables3,4,5,6and7.
ThebestSASAsequence,writtenbothinthelog-spaceandintheprobabilityspace,isimmediately
derivedfromthegraph:
Att=1wehave
s∗=argmaxp(s )V (s )=argmaxp(s )logb (s ),
1 1 S1 1 1 S1 1
s1 s1 (38)
a∗=argmaxQ (s∗a )=argmaxb (s∗a ).
1 (S1A1)1 1 1 (S1A1)1 1 1
a1 a1
Att=2
s∗=argmaxp(s |s∗a∗)V (s )=argmaxp(s |s∗a∗)logb (s ),
2 2 1 1 S2 2 2 1 1 S2 2
s2 s2 (39)
a∗=argmaxQ (s∗a )=argmaxb (s∗a ),
2 (S2A2)1 2 2 (S2A2)1 2 2
a2 a2
... etc.
Theuniqueformulationonthefactorgraphallowsthederivationofallotherinferences,suchasA,
S,AS-sequences,alsoforDP,simplyusingthespecificpropagationrulesonthegraph. Thepolicy
distributionhasthesameformalexpressionasin(30).
15
7 SoftDP
The presence of the max operator in the DP algorithm, suggests that, similarly to the Sum/Max-
productapproach,wecouldreplacethemaxoperatorwithagenericsoft-maxfunctiontoprovide
adifferentinterpolationbetweenamoreentropicsolutionandtheoptimalDPalgorithm. Usinga
soft-maxfunction,weproposethefollowingSoftDPupdates
(cid:88)
Q (s a )=logp(a )+R(s a )+ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
st+1
V (s )=
(cid:80)
at
eβQ (StAt)1(stat)Q
(StAt)1
(s
t
a
t
)
.
(40)
St t (cid:80)
a(cid:48)
eβQ (StAt)1(sta(cid:48)
t
)
t
Theparameterβcanbeusedtocontrolthesharpnessofthesoft-maxfunction. Ifβisalargepotisive
number, the soft-max tends to the maximum. When β is a small positive number, the soft-max
functiontendstoreturnthemean. Thesoft-maxfunctionusedhereispopularintheneuralnetworks
literature. DetailsaboutitsbehaviorareinAppendixA.Wehavenotinvestigatedtheexistenceofa
functionthattheserecursionsoptimizeforafinitevalueofβ,asinthecaseoftheSum/Max-product
algorithm. Weleaveittofurtheranalyses. However,weobservethatloweringthevalueofβ shifts
thepolicydistributiontowardsasmoother,i.e.,moreentropic,configuration. Weshowthiseffectin
thesimulationsinalatersection.
Intheprobabilityspace,therecursionforthebackwardmessageb (s a )isthesameasinDP,
(StAt)1 t t
whiletheupdateforb (s )becomes
St t
(cid:34)(cid:80)
logb (s a )b (s )β
(cid:35)
b (s )∝exp at (StAt)1 t t St t . (41)
St t (cid:80) b (s a(cid:48))β
a(cid:48)
t
(StAt)1 t t
AllrecursionsareincludedinTables1and2. Alsothepropagationrulethroughtheblocksarein
Tables3,4,5,6and7.
8 Maximumexpectedrewardandentropy
Inallthepreviousapproachestooptimalcontrol,wehavederivedthesolutionsasoptimalinferences
onthefactorizedmodelofFigure1,intheprobabilityspacefortheSum-andMax-productalgorithms,
orasthebestactionsequenceinthelog-spaceforDP.Thepolicydistributionisthenwrittenasa
consequenceoftheoptimizationalgorithmonthatgraph.
AdifferentformulationcanbeadoptedifweformallyaddtotheBayesiangraph"policy"branches
π(a |s ) that go from each state S to each action A and pose the problem as the functional
t t t t
optimizationproblemoffindingthebestπ(a |s ),giventheevidenceK . Thequestionis: howdo
t t 1:T
weformalizedthetotalrewardfunction?
Levine(2018),inhisreview,suggeststhat”lessconfident”behaviorswithrespecttothestandard
probabilisticinference(theSum-product)couldbeobtainedifwemodifythefunctiontooptimize.
Infact,hemaintainsthattherecursionsfortheSum-productapproachderivedabove,maybetoo
optimisticwithinthecontextofRL.Theideaistoaddanextratermtotherewardstoaccountalso
forpolicyentropy. Levineshowsthatthemodificationcanalsoberelatedtostructuralvariational
inference (Levine, 2018). Entropy maximization is also a common criterion in practical uses of
RL(Ziebartetal.,2009)andstochasticcontrol(Ziebartetal.,2010). Levine(2018)proposesthe
followingformulation
{π∗(a |s )...π∗(a |s )}=
1 1 T T
(cid:34) T (cid:35)
(cid:88) (42)
argmax E (R(s a )+logp(a )−logπ(a |s ))
∼pˆ(s1a1...sTaT) t t t t t
π(a1|s1)...π(aT|sT) t=1
where
T−1
(cid:89)
pˆ(s a ...s a )=p(s )π(a |s ) p(s |s a )π(a |s ).
1 1 T T 1 T T t+1 t t t t
t=1
16
Notethatherethepolicydistributionsareincludedinthefactorization. Theextratermlogπ(a |s )
t t
willgivesrisetoentropymaximizationThiswillbebetterexplainedinthegeneralizationthatfollows
andinAppendixB.Thebackuprecursionsfortheoptimalpolicydistributions(Levine,2018),inthe
factorgraphnotations,are
(cid:88)
Q (s a )=logp(a )+R(s a )+ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
(cid:88) st+1 (43)
V
St
(s
t
)=log eQ (StAt)1(stat).
at
Theoptimalpolicydistributionsarealsoshowntohavetheusualformalexpression
π∗(a
t
|s
t
)∝e (Q (StAt)1(stat)−V(st)) . (44)
In our effort to provide more general approaches to the policy search, we have generalized the
soft-maxfunctiontoincludeanextraparameterα,withtherecursions
(cid:88)
Q (s a )=logp(a )+R(s a )+ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
1 (cid:88)
st+1
(45)
V
St
(s
t
)=
α
log eαQ (StAt)1(stat).
at
The function used in the value function update is the same one used for the Sum/Max product
algorithmandissuchthatforα→∞givesthemaximumandthereforetheDPsolution.
Wehaveworkedtherecursionbackwardandweshow(seeAppendixBfortheproof)thattheabove
recursionssolvethefollowingoptimizationproblem
{π∗(a |s )...π∗(a |s )}=
1 1 T T
(cid:34) T (cid:18) (cid:19)(cid:35)
(cid:88) 1 (46)
argmax E R(s a )+logp(a )− logπ (a |s )
∼pˆ(s1a1...sTaT) t t t α α t t
π(a1|s1)...π(aT|sT) t=1
where
T−1
(cid:89)
pˆ(s a ...s a )=p(s )π (a |s ) p(s |s a )π (a |s ),
1 1 T T 1 α T T t+1 t t α t t
t=1 (47)
π(a |s )α
π (a |s )= t t , t=1:T.
α t t (cid:80) π(a(cid:48)|s )α
a(cid:48) t t
t
Theaboveexpressiongeneralizes(42)byincludingaparameterα > 0. WeshowinAppendixB
that,whenαislarge,theextratermbecomesprogressivelyirrelevant,andthedistributionsπ (a |s )
α t t
become more concentrated on the max value of the Q-function resulting in a hard DP solution.
Furthermore, when α < 1, more weight is given to the extra term, the distributions π (a |s )
α t t
becomessmootherandwehavemoreentropicpolicydistributions. Wedemonstratethiseffectin
someofthesimulationsthatfollow.
InAppendixB,wediscussexplicitlythisformulationforT =4derivingtherecursions(45)showing
how the extra term gives rise to (iterative) simultaneous reward and entropy maximization. It is
pointedoutinourdiscussionthatthecriteriondoesnotsimplyaddanentropytermtotherewards
becausethepolicydistributionaffectsalsotherewardasitappearsinthefactorizationusedinthe
expectation.
Therecursions(45)canbetranslatedintheprobabilityspaceas
b
(StAt)1
(s
t
a
t
)=p(a
t
)c(s
t
a
t
)e
(cid:80)
st+1
p(st+1|stat)logbSt+1 (st+1)
(cid:34) (cid:35)1
α (48)
(cid:88)
b (s )= b (s a )α
St t (StAt)1 t t
at
Therecursions,bothintheprobabilityandthelogspace,areincludedinTables1and2. Alsothe
propagationrulethroughtheblocksarereportedinTables3,4,5,6and7.
17
9 Deterministicsystems
Theapproachtooptimalcontrolinthispaperisbasedontheassumptionthatthesystemdescription
isstochastic. Thisisquiteusefulintheapplicationswhenwedonothaveexactknowledgeofthe
systemdynamicsandthetheprobabilitydistributionp(s |s a )canbeourbestestimate. There
t+1 t t
are cases however, in which the system response is deterministic, i.e., given s a , we have exact
t t
knowledgeofs throughadeterministicfunctions = g(s a ). Inthestochasticframework,
t+1 t+1 t t
thistranslatesintoatransitionprobabilityfunctionthatisdeltafunction
p(s |s a )=δ(s −g(s a )). (49)
t+1 t t t+1 t t
Also,ifnopriorontheactionsisavailable,p(a )=U(a ). Theupdatesinthesecasesdonotchange,
t t
butsomeoftheminthevariousmethodsmaycoincide,becausethesummations(expectations)inthe
updatesdisappearandtheprioronA isirrelevant. Morespecifically,bylookingatTables1and2,
t
theupdatesfortheQ-functions,andtheirprobability-spacecounterparts,havethesame(Bellman’s)
recursions
Q (s a )=R(s a )+V (g(s a )),
(StAt)1 t t t t St+1 t t (50)
b (s a )=c(s a )b (g(s a )).
(StAt)1 t t t t St+1 t t
However,therearedifferencesintheV-functionupdates. FortheSum-productandtheMax-Rew/Ent
(α=1)wehave
(cid:88)
V
St
(s
t
)=log eQ (StAt)1(stat),
(cid:88) at (51)
b (s )= b (s a ).
St t (StAt)1 t t
at
FortheMax-productandDP,wehave
V (s )=maxQ (s a ),
St t
at
(StAt)1 t t
(52)
b (s )=maxb (s a ).
St t
at
(StAt)1 t t
Fortheotherswehavetheparametrizedsoft-maxfunctionwithvariousvaluesofβ andα.
Bydirectcomparison,wecanconcludethat,whenthesystemisdeterministic: DPandMax-product
coincide;Mean-productandMax-Rew/Ent(α=1)coincide(alsorecognizedinLevine(2018)). The
remainingcasesareinterpolationsoftheothers. Wehaveverifiedinourlimitedsimulationsthatthis
isindeedthecaseandthatthesolutionsinthevariousgroups,eveninthisdeterministiccase,are
different.
10 Infinitehorizoncaseandthesteady-state
WehavepresentedthemodelinFigure1andthevariousalgorithmwithreferencetoafinitehorizon
scenario. However,alltheanalyseseasilyextendstoaninfinite-horizonframeworksimplybyadding
adiscountfactor0 < γ ≤ 1totheoptimizedfunctionsandthentotheupdates. Forexample,the
standardDPupdates,inbothspacesbecome
(cid:88)
Q (s a )=logp(a )+R(s a )+γ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
st+1 (53)
b
(StAt)1
(s
t
a
t
)=p(a
t
)c(s
t
a
t
)e
γ(cid:80)
st+1
p(st+1|stat)logbSt+1 (st+1)
.
Also,fortheSum-product,wehaveimmediately
(cid:88)
Q
(StAt)1
(s
t
a
t
)=logp(a
t
)+R(s
t
a
t
)+γlog elogp(st+1|stat)+VSt+1 (st+1) (54)
st+1
Ingeneral,alsoifγ =1,thebackwardrecursionscanberuntoverifythatasteady-stateconfiguration
fortheQ,theV-functionandthepolicyπ∗canbefound. ThisisclearlythesolutiontoaBellmann
equationappliedtothedifferentupdates.Theanalysisofthemathematicalconditionsforconvergence
arebeyondthescopeofthispaper.However,generallyspeaking,ifallthestatesarereachable,astable
configurationshouldexist. Wehaveverifiedexperimentallythatallthemethodsdoconverge(alsofor
γ =1),buttheyexhibitmarkeddifferencesinthenumberofiterationsnecessarytoreachthesteady
stateequilibrium. TheMax-productalgorithmshowsthefastestconvergencewiththeSum-product
followinginthelist. DPandtheothermethodsseemtoshowamuchslowerconvergencespeed. We
showthiseffectinthesimulationsthatfollow.
18
Value for Sum-Product (22 iterations) Policy for Sum-Product Value for Max-Product (12 iterations) Policy for Max-Product
Value for Sum/Max-Product (15 iterations) Policy for Sum/Max-Product Value for SoftDP (89 iterations) Policy for SoftDP
Value for SoftDP (69 iterations) Policy for SoftDP Value for DP (81 iterations) Policy for DP
Value for Max Rew/Ent (125 iterations) Policy for Max Rew/Ent Value for Max Rew/Ent (90 iterations) Policy for Max Rew/Ent
Comparison
Value for Max Rew/Ent (88 iterations) Policy for Max Rew/Ent
Figure6: Visualizationofthemaxpolicydirectionforthevariousalgorithms. Atthetopofeach
figurearereportedalsothenumberofiterationsnecessarytoreachasteady-statevaluefunction.
Reportedontheleftcolumnsarealsothenumericalfinalvaluesforthevaluefunction. Thelowest
rightplotshowsthevaluefunctionincrementsastheiterationsprogresstowardssteady-state.
11 Simulations
Wehavesimulatedthevariousrecursionsforpathplanningontwodiscretegrids.
Thefirstsetofsimulationsisperformedonasmall6x6squaregridshowninFigure6,wherewe
haveonegoal(bull’seyeandgreen)andobstacles(darkgray). Thestatesarethepositionsonthe
gridandtheactionscorrespondtooneoftheninepossibleone-pixelmotions{up-left,up,up-right,
left,center(still),right,down-left,down,down-right}. Therewardfunctionhasthevalues0onthe
goal,−10ontheobstaclesand−1onotherpixelpositions. Themotionisstochasticwithatransition
19
Sum-Product (29 iterations) Max-Product (11 iterations) Sum/Max-Product (15 iterations)
SoftDP (127 iterations) SoftDP (113 iterations) DP (120 iterations)
Max Rew/Ent (187 iterations) Max Rew/Ent (128 iterations) Max Rew/Ent (120 iterations)
Figure7: Visualizationofthemaxpolicydirectionforthevariousalgorithms. Atthetopeachfigure
arereportedalsothenumberofiterationsnecessaryforthevaluefunctiontoreachitssteady-state
configuration.
functionp(s |s a )thathasprobability1/2fortheintendeddirectionandtherestspreadequally
t+1 t t
ontheothereightdirections. Builtinthetransitionfunctionarealsore-normalizationswhenthe
transitionisclosetotheboundaries: whensomeofthenewprojectedstatesareoutsidethegrid,
theirprobabilitiesaresettozero,andtheremainingprobabilityisspreadequallyontheotherpixels.
Noinitialorfinalconditionsaresetonthemodel. Therecursionsarerununtilconvergencetoa
steadystatevaluefunction. Allthealgorithmsleadtopoliciesthatwouldallowanagent,starting
from any position on the grid, to reach the goal in a finite number of steps. The values reported
inthesquaresandthemaxpolicyarrowsinFigure6revealhowthedifferentsolutionsdirectour
potentialagentinslightlydifferentpathstoavoidtheobstacles. Inthelowerrightcornerofthefigure,
20
Sum-Product (29 iterations) Max-Product (11 iterations) Sum/Max-Product (15 iterations)
SoftDP (127 iterations) SoftDP (113 iterations) DP (120 iterations)
Max Rew/Ent (187 iterations) Max Rew/Ent (128 iterations) Max Rew/Ent (120 iterations)
Figure8: Numericalvisualizationofthevaluefunctionforthevariousalgorithms.
wealsoreporttheincrementsinreachingthesteady-statesolutionforthevariousalgorithmsina
log-loggraph(alsotheparametersarereportedinthelegend). Thealgorithmisstoppedafterall
theincrementsinthevaluefunctionarebelow10−5. ItisnoteworthytoseehowtheMax-product
algorithmreachesthesteady-statesolutioninaverylimitednumberofsteps(fastestconvergence)
andhowtheSum-productandtheSum/Max-productalgorithmsconvergeatamuchfasterratein
comparisontotheothers.
TheresultsofanothersetofsimulationsarereportedinFigures7,8,9,10and11. Here,wehavea
gridextractedfromarealdatasetacquiredatanintersectionontheStanfordcampuswithpedestrians
andbikes. Thescene,withnoagents,issimplifiedto17×23pixels,withgoals(exits)andrewards
assigned to various areas (semantic map) as shown in Figure 7. We assume that our agent is a
21
Sum-Product Max-Product Sum/Max-Product
200 200 200
150 150 150
100 100 100
50 50 50
0 0 0
5 5 5
10 15 10 15 10 15
15 10 15 10 15 10
20 5 20 5 20 5
SoftDP SoftDP DP
350 350 350
300 300 300
250 250 250
200 200 200
150 150 150
100 100 100
50 50 50
0 0 0
5 5 5
10 15 10 15 10 15
15 10 15 10 15 10
20 5 20 5 20 5
Max-Rew/Ent Max-Rew/Ent Max-Rew/Ent
500 350 350
400 300 300
250 250
300 200 200
200 150 150
100 100
100
50 50
0 0 0
5 5 5
10 15 10 15 10 15
15 10 15 10 15 10
20 5 20 5 20 5
Figure9: 3DPlotsof−V (s )forthevariousalgorithms.
St t
pedestrianandtheactionsarethesamenineactionswehaveusedaboveforthesmallgrid. The
rewardsare: R(s )=0(goals: bull’seyeandgreen);-1(pedestrianwalkways: white);-10(streets:
t
lightgray);-20(grass: darkgray);-30(obstacles: dark). Theconvergencebehaviortoasteady-state
valuefunctionissimilartotheoneshownforthesmallergrid. Thenumberofiterationstoreach
aprecisionof10−5 onallthestatesare: [Sum-product: 29;Max-product: 11;Sum/Max-product
(α = 3): 15;SoftDP(β = 0.2): 127;SoftDP(β = 0.6): 113;DP:120;MaxRew/Ent(α = 0.2):
187;MaxRew/Ent(α=1): 128;MaxRew/Ent(α=6): 120]. NotehowquicklytheMax-product
and the probabilistic methods converge when compared to the others. The graph of the actual
incrementsforthevariousalgorithmisshowninlogscaleinFigure11. Figure7shows,foreach
state,themaximumpolicydirectionsforallthealgorithms. Thearrowspointtowardsthepreferences
imposedbythesemanticinformation: pedestrianspreferwalkwaystostreets;grassandobstacles
areavoided. WeobserveamarkedeffectontheresultsoftheMax-productalgorithmthatmaintains
themultiplemaximadirectionscorrespondingtotheequivalentsolutions. Thesemultipleoptions
appearsmoothedoutintheothermethods. TheMax-productrequiresjusttheminimumnumberof
stepstostabilizeitsconfigurations. Figure9showsalsosomeofthevaluefunctions−V(s )(they
t
22
Sum-Product SoftDP Max Rew/Ent
1 1 1
0,5 0,5 0,5
0 0 0
Max-Product SoftDP Max Rew/Ent
1 1 1
0,5 0,5 0,5
0 0 0
Sum/Max-Product DP Max Rew/Ent
1 1 1
0,5 0,5 0,5
0 0 0
Figure10:Policydistributionsatagenericpoint(redonthemap)forthevariousmethodsfordifferent
parameterchoices. Thegoalsaredepictedingreen.
Comparison
Figure11: Comparisonofthevaluefunctionincrementsforthevariousalgorithmfortheexampleof
Figure7.
canbethoughtaspotentialfunctions)superimposedontheoriginalsceneforthevariousmethods
andforsomeparameterchoices. Thecomparisonclearlyshowsthatthevariousalgorithmleadto
intriguingcomparablesmoothsolutions,exceptfortheMax-productthatproducesaverysharpvalue
functionwithverywelldefinedvalleys. Justasinthesimulationsonthesmallgrid,wehaveincluded
nopathsonthemap,becauseinallmethodsanagentthatstartsanywhere,willreachoneofthegoals
inallcases. ThiscanbeeasilyverifiedbyfollowingthearrowsinFigure7. Amuchmorerevealing
visualizationofthedifferencesamongthevariousmethodsisdisplayedinFigure10, whereata
genericpointonthemap,weplotthepolicydistributions. Inthefirstcolumn,thepoliciesforthe
probabilisticmethodsareshownwiththeMax-productclearlyproducingarathersharpbehavior
withallthemultipleequivalentoptions. Recallthatthemaphasmultiplegoalsandtheagentinthat
positionhasmorethanoneoptiontoachieveoptimality(seealsoFigure7inthatposition). Inthe
secondcolumn,wereporttheresultsoftheDPapproachinitsstandardform(bottomgraph)andin
itssoftparametrizedversions. Notehow,forthetwovaluesβ =0.3andβ =0.6,anagentmaybe
ledtoconsidermoreoptionswithrespecttoDPandifwelookalsoatthemaximumpolicyonthe
mapofFigure7,itmayevenbetakenonadifferentpath. Inthethirdcolumn,wereporttheresults
oftheMaxRew/Entalgorithmforvariousvaluesoftheparameterα. Wenotice,asexpectedfrom
thetheory,thatwhenα<1,thepolicydistributionismoreentropicandthatwhenαincreases,the
distributiontendstotheDPpolicy.
23
12 Conclusionsandfuturedirections
In this paper, we have provided a unified view on the optimal solutions to path planning using a
probabilityformulationonafactorgraph. Thevariousmethodscorrespondtodifferentcombination
rules through two of the blocks on a Factor Graph in Reduced Normal Form and correspond to
different cost functions that go from maximum a posteriori to maximum combination of reward
andentropy. Wehavegeneralizedsomeofthealgorithmspreviouslypresentedintheliteratureby
usingparametrizedsoft-maxfunctions. Theresultingsetofchoices,presentedhereinacompact
fashion,bothintheprobabilityandinthelogspace,mayenhancethealgorithmicdesignoptionsfor
reinforcementlearningthatneedstoobtaintheV-functionsortheQ-functions,perhapsonthebasis
ofcurrentmodelknowledge. Wehaveincludedtypicalresultsondiscretegridsthatrevealmarked
differencesamongsomeofthemethods. OurcomputationalresultssuggestthattheMax-product
algorithm,optimalmaximumaposteriorisolution,togetherwiththeotherprobabilisticmethods,such
astheSum-productanditsSum/Max-productgeneralizations,showsfastconvergetothesteady-state
configuration in comparison to the other reward-based methods, that are typically derived in the
logspace. Thisgeneralapproachtothetopicmayallowagileuseofthevariousmethodsduring
explorationinRL.Furtherworkonthetopicwillbedevotedtoextensionstocontinuousspaceand
relativeapproximations,andapplicationsinvolvinginteractingagents.
A Soft-maxfunctions
Wereviewheresomeofthesoft-maxfunctionsthatareusedintherecursionsdiscussedinthepaper.
Forallthefunctions, westartfromasetorrealnumbersx ,...,x andconsidertherankedset
1 N
x ,x ,...,x ,withx ≤x ≤···≤x .
(1) (2) (N) (1) (2) (N)
A.Considerthefollowingexpression
N
(cid:88)
s(x ,...,x )=log exj.
1 N
j=1
Thisfunctionhasthepropertythatwhenx ,...,x →+∞,s(x ,...,x )→max(x ,...,x ).
1 N 1 N 1 N
Proof: Thefunctioncanrewrittenas
s(x
1
,...,x
N
)=log(ex(1) +ex(2) +···+ex(N))
=logex(N) (cid:0) ex(1)−x(N) +ex(2)−x(N) +···+1 (cid:1) .
Whenthex sbecomelarge,alsotheirdifferencetothemaxx becomesalargenegativenumber.
i (N)
ThereforethefirstN −1termsinsidetheparenthesistendtozeroands(x ,...,x )→x .
1 n (N)
B.Aparametrizedsoft-maxfunctioncanbedefinedastheexpression
N
1 (cid:88)
g(x ,...,x ;α)= log eαxj
1 N α
j=1
whereα≥1. Thisfunctionhasthepropertythat
lim g(x ,...,x ;α)=max(x ,...,x ). (55)
1 N 1 N
α→∞
Proof: Usingtherankedset,thefunctioncanbeboundedas
1 1
α
logeαx(N) ≤g(x
1
,...,x
N
;α)≤
α
logNeαx(N), (56)
logN
x ≤g(x ,...,x ;α)≤ +x ,
(N) 1 N α (N)
thatforα → ∞achievesthemaximumx . Notethatforα > 1,fromthebound,thesoft-max
(N)
alwaysexceedsthemaximumvalue,i.e.,tendstox fromtheright.
(N)
Itisusefultolookattheexpressionwhen0<α<1. Whenαisaverysmallpositivenumber
1
g(x ,...,x ;α)(cid:39) logN +µ,
1 N α
24
where µ = (1/N)
(cid:80)N
x is the arithmetic mean. The function diverges for α → 0, but when
i=1 i
consideredforsmallvaluesofα,thefunctiontendstobecomeindependentonanyspecificx .
i
Proof: Thefunctioncanbewrittenas
(cid:32) N (cid:33) N
1 (cid:88) 1 (cid:88)
g(x ,...,x ;α)= log eα(xi−µ)eαµ = log eα(xi−µ)+µ.
1 N α α
i=1 i=1
Whenαapproacheszero,theexponentsbecome(cid:39)1andwehavetheresult.
C.Anotherparametricsoft-maxfunctionis
 1
N α
(cid:88)
h(x 1 ,...,x N ;α)= xα j ,
j=1
whereherex ≥0,i=1:N. Heretoo,forα→∞,h(x ,...,x ;α)→x .
i 1 N (N)
Proof: Fromthebounds
(cid:16) (cid:17)1
(xα
(N)
)α 1 ≤h(x
1
,...,x
N
;α)≤ Nxα
(N)
α ,
x
(N)
≤h(x
1
,...,x
N
;α)≤Nα 1x
(N)
,
whenαgrowsNα 1 →1andthefunctiontendstox
(N)
. Inallsoft-maxfunctions,thelargervalues
inthesetx ,...,x ,tendtodominateovertheotherswhentheybecomelarge.
1 N
Thefunctionh(x ,...,x ;α),justasg(x ,...,x ;α),divergesforα→0,butforsmallα
1 N 1 N
h(x
1
,...,x
N
;α)(cid:39)Nα 1,
which,againasing,deasnotdependonanyofthex .
i
Proof: easilyseenasxα (cid:39)1forsmallα.
i
D.Anothersoft-maxfunctionscanbedefinedas
r(x ,...,x ;α)=
(cid:80)N
i=1
x
i
eαxi
.
1 N (cid:80)N eαxj
j=1
Thisfunctioniswell-knownintheneuralnetworkliterature,wherethevectorfunctioneαxi/ (cid:80) eαxj
j
tendstoadistributionconcentratedonthemaximum. Bytakingtheexpectationwithsuchadistribu-
tionwegetthesoft-max. Therefore,whenα→∞,r(x ,...,x ;α)→x .
1 N (N)
Proof: Thefunctioncanre-writtenusingtherankedsetas
r(x ,...,x ;α)=
(cid:80)N
i=
−
1
1x
(i)
eαx(j) +x
(N)
eαx(N)
=
(cid:80)
i
N
=
−
1
1x
(i)
eα(x(j)−x(N))+x
(N) .
1 N (cid:80)N−1eαx(j) +eαx(N) (cid:80)N−1eα(x(j)−x(N))+1
j=1 j=1
Forα → ∞bothsummationstendtozero, becausetheexponentsarenegative, andwehavethe
result.
This soft-max function, when α → 0+ does not diverge, but tends to the arithmetic mean
r(x ,...,x ;α)→1/N
(cid:80)N
x .
1 N i=1 i
Proof: Trivial,becauseforα=0alltheexponentialsareequaltoone.
25
B OptimizingRewardandEntropy
To better understand the nature of the function being optimized in (46), and how it gives rise to
an entropy term, let us write it explicitely for T = 4, using the compact notation R(cid:48)(s a ) =
t t
R(s a )+logp(a ). Thefunctiontooptimizeis
t t t
(cid:80) (cid:88)
p(s )π (a |s )p(s |s a )π (a |s )p(s |s a )π (a |s )p(s |s a )π (a |s )
s1...s4 1 α 1 1 2 1 1 α 2 2 3 2 2 α 3 3 4 3 3 α 4 4
a (cid:20)1...a4
1 1
R(cid:48)(s a )− logπ (a |s )+R(cid:48)(s a )− logπ (a |s )+R(cid:48)(s a )
1 1 α α 1 1 2 2 α α 2 2 3 3
(cid:21)
1 1
− logπ (a |s )+R(cid:48)(s a )− logπ (a |s ) ,
α α 3 3 4 4 α α 4 4
(57)
Startingfromthelastterm,weidentifythebackwardrecursions
 
H(πα(a4|s4))
 (cid:122) (cid:125)(cid:124) (cid:123)
1 (cid:88) (cid:88) (cid:88) 1 
p(s |s a ) π (a |s )αR(cid:48)(s a )+ π (a |s )log 
α 4 3 3  α 4 4 4 4 α 4 4 π (a |s )
 α 4 4 
s4

a4 a4

(cid:124) (cid:123)(cid:122) (cid:125)
V(s4)
(cid:124) (cid:123)(cid:122) (cid:125)

Q(s3a3)

H(πα(a3|s3))
 (cid:122) (cid:125)(cid:124) (cid:123)
1 (cid:88) (cid:88) (cid:88) 1 
p(s |s a ) π (a |s )(αR(cid:48)(s a )+αQ(s a ))+ π (a |s )log 
α 3 2 2  α 3 3 3 3 3 3 α 3 3 π (a |s )
 α 3 3 
s3

a3 a3

(cid:124) (cid:123)(cid:122) (cid:125)
V(s3)
(cid:124) (cid:123)(cid:122) (cid:125)

Q(s2a2)

H(πα(a2|s2))
 (cid:122) (cid:125)(cid:124) (cid:123)
1 (cid:88) (cid:88) (cid:88) 1 
p(s |s a ) π (a |s )(αR(cid:48)(s a )+αQ(s a ))+ π (a |s )log 
α 2 1 1  α 2 2 2 2 2 2 α 2 2 π (a |s )
 α 2 2 
s2

a2 a2

(cid:124) (cid:123)(cid:122) (cid:125)
V(s2)
(cid:124) (cid:123)(cid:122) (cid:125)

Q(s1a1)

H(πα(a1|s1))
 (cid:122) (cid:125)(cid:124) (cid:123)
1 (cid:88) (cid:88) (cid:88) 1 
p(s ) π (a |s )(αR(cid:48)(s a )+αQ(s a ))+ π(a |s )log 
α 1  α 1 1 1 1 1 1 1 1 π (a |s )
 α 1 1 
s1

a1 a1

(cid:124) (cid:123)(cid:122) (cid:125)
V(s1)
(58)
NotehowthevaluefunctionV(s )(notoptimizedhere)iswrittenasarecursivesuperpositionof
t
rewardandpolicyentropy. Theparameterαcontrolsthebalancebetweenthetwotermsandthe
powerofthedistribution. Notethatthepolicyfunctionmultipliesalsotherewardterm. Therefore,
the optimized policy distribution will shape, in a non trivial way, the effects of the rewards with
respecttotheentropy.
26
FollowingLevine’sapproach(Levine,2018),usingourmodifiedcostfunction,wesearchforthebest
policydistributionstartingfromre-writingthelasttermusingtheKL-divergence
(cid:34) (cid:35)
1 (cid:88) (cid:88)
p(s |s a ) π (a |s )(αR(cid:48)(s a )−logπ (a |s )) =
α 4 3 3 α 4 4 4 4 α 4 4
1 (cid:88)
s4
(cid:34) (cid:88)
a4
(cid:32) eαR(cid:48)(s4a4) (cid:80) a(cid:48) eαR(cid:48)(s4a(cid:48) 4 )(cid:33)(cid:35)
p(s |s a ) π (a |s ) log 4 =
α
s4
4 3 3
 a4
α 4 4 π
α
(a
4
|s
4
)(cid:80)
a(cid:48) 4
eαR(cid:48)(s4a(cid:48)
4
)

 (cid:32) (cid:13) (cid:33)  (59)
α 1 (cid:88) s4 p(s 4 |s 3 a 3 )      −D KL π α (a 4 |s 4 ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:80) e a(cid:48) 4 αR eα (cid:48)( R s4 (cid:48)( a s 4 4 ) a(cid:48) 4 ) +log (cid:88) a(cid:48) 4 eαR(cid:48)(s4a(cid:48) 4 )      =
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:34) (cid:32) (cid:13) (cid:33) (cid:35)
eαV(s4)
(cid:88)
(cid:13)eαR(cid:48)(s4a4)
p(s |s a ) −D π (a |s )(cid:13) +V(s ) .
4 3 3 KL α 4 4 (cid:13)
(cid:13)
eαV(s4) 4
s4
TheoptimumvalueisobtainedwhentheD (.(cid:107).)=0,i.e.,whenπ (a |s )=
eαR(cid:48)(s4a4)
,andthe
KL α 4 4 eαV(s4)
optimalpolicydistributionis
eR(cid:48)(s4a4) eQ(s4a4)
π∗(a |s )∝ = , (60)
4 4 eV(s4) eV(s4)
wherewehavedefinedQ(s a )=R(cid:48)(s a ). Nowtheoptimizedexpression (cid:80) p(s |s a )V(s )
4 4 4 4 s4 4 3 3 4
iscarriedover
(cid:88) 1
R(cid:48)(s a )+ p(s |s a )V(s )− logπ (a |s ). (61)
3 3 4 3 3 4 α α 3 3
s4
(cid:124) (cid:123)(cid:122) (cid:125)
Q(s3a3)
Takingtheexpectation,wehave
(cid:34) (cid:35)
1 (cid:88) (cid:88)
p(s |s a ) π (a |s )(αQ(s a )−logπ (a |s )) =
α 3 2 2 α 3 3 3 3 α 3 3
1 (cid:88)
s3
(cid:34)
(cid:88)
a3
(cid:32)
eαQ(s3a3)
(cid:80)
a(cid:48)
eαQ(s3a(cid:48)
3
)(cid:33)(cid:35)
p(s |s a ) π (a |s ) log 3 =
α
s3
3 2 2
 a3
α 3 3 π
α
(a
3
|s
3
)(cid:80)
a(cid:48) 3
eαQ(s3a(cid:48)
3
)

 (cid:32) (cid:13) (cid:33)  (62)
α 1 (cid:88) s3 p(s 3 |s 2 a 2 )      −D KL π α (a 3 |s 3 ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:80) e a(cid:48) 3 αQ eα (s Q 3 ( a s 3 3 ) a(cid:48) 3 ) +log (cid:88) a(cid:48) 3 eαQ(s3a(cid:48) 3 )      =
(cid:124) (cid:123)(cid:122) (cid:125)
1 (cid:88)
(cid:20) (cid:18) (cid:13) (cid:13)eαQ(s3a3)(cid:19) (cid:21)
eαV(s3)
α
p(s
3
|s
2
a
2
) −D
KL
π
α
(a
3
|s
3
)(cid:13)
(cid:13) eαV(s3)
+αV(s
3
) .
s3
Similarly to above, D = 0 when π (a |s ) = eαQ(s3a3) . The best policy distribution is then
KL α 3 3 eαV(s4)
π∗(a |s )∝ eQ(s3a3) . Carryingover (cid:80) p(s |s a )V(s ),wehave
3 3 eV(s4) s3 3 2 2 3
(cid:88) 1
R(cid:48)(s a )+ p(s |s a )V(s )− logπ (a |s ). (63)
2 2 3 2 2 3 α α 2 3
s3
(cid:124) (cid:123)(cid:122) (cid:125)
Q(s2a2)
Followingsimilarsteps,wehaveπ∗(a |s )∝ eQ(s2a2) and
2 2 eV(s2)
(cid:88) 1
R(cid:48)(s a )+ p(s |s a )V(s )− logπ (a |s ). (64)
1 1 2 1 1 2 α α 1 1
s2
(cid:124) (cid:123)(cid:122) (cid:125)
Q(s1a1)
27
Thelaststepis
(cid:34) (cid:35)
1 (cid:88) (cid:88)
p(s ) π (a |s )(αQ(s a )−logπ (a |s )) =
α 1 α 1 1 1 1 α 1 1
1 (cid:88)
s1
(cid:34)
(cid:88)
a1
(cid:32)
eαQ(s1a1)
(cid:80)
a(cid:48)
eαQ(s1a(cid:48)
1
)(cid:33)(cid:35)
p(s ) π (a |s ) log 1 =
α
s1
1
 a1
α 1 1 π
α
(a
1
|s
1
)(cid:80)
a(cid:48) 1
eαQ(s1a(cid:48)
1
)

 (cid:32) (cid:13) (cid:33)  (65)
α 1 (cid:88) s1 p(s 1 )      −D KL π α (a 1 |s 1 ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:80) e a(cid:48) 1 αQ eα (s Q 1 ( a s 1 1 ) a(cid:48) 1 ) +log (cid:88) a(cid:48) 1 eαQ(s1a(cid:48) 1 )      =
(cid:124) (cid:123)(cid:122) (cid:125)
1 (cid:88)
(cid:20) (cid:18) (cid:13) (cid:13)eαQ(s1a1)(cid:19) (cid:21)
eαV(s1)
α
p(s
1
) −D
KL
π
α
(a
1
|s
1
)(cid:13)
(cid:13) eαV(s1)
+αV(s
1
) .
s1
This term is minimized when π (a |s ) = eαQ(s1a1) , with the optimal policy distribution
α 1 1 eαV(s1)
π∗(a |s )∝ eQ(s1a1) . Therefore,therecursionsatagenerictimesteptare
1 1 eV(s1)
(cid:88)
Q (s a )=logp(a )+R(s a )+ p(s |s a )V (s ),
(StAt)1 t t t t t t+1 t t St+1 t+1
1 (cid:88)
st+1
(66)
V
St
(s
t
)=
α
log eαQ (StAt)1(stat),
at
withtheoptimalpolicydistributionπ∗(a
t
|s
t
)∝eQ(stat)−V(st).
References
Alexandre,Z.,Oleg,S.,andGiovanni,P.(2018). Aninformation-theoreticperspectiveonthecosts
ofcognition. Neuropsychologia,123:5–18.
Attias,H.(2003). Planningbyprobabilisticinference. InProc.ofthe9thInt.WorkshoponArtificial
IntelligenceandStatistics,page.
Bahl,L.,Cocke,J.,Jelinek,F.,andRaviv,J.(1974). Optimaldecodingoflinearcodesforminimizing
symbolerrorrate. IEEETransactionsonInformationTheory,IT-20(2):284–287.
Baltieri,M.andBuckley,C.L.(2017). Anactiveinferenceimplementationofphototaxis. ARXIV.
Barber,D.(2012). BayesianReasoningandMachineLearning. CambridgeUniversityPress.
Bertsekas,D.(2019). ReinforcementLearningandOptimalControl. AthenaScientific.
Buckley, C.L., Kim, C.S., McGregor, S., andSeth, A.K.(2017). Thefreeenergyprinciplefor
actionandperception: amathematicareview. JournalofMathematicaPhychology,81:55–79.
Castaldo,F.andPalmieri,F.(2015). Targettrackingusingfactorgraphsandmulti-camerasystems.
IEEETransactionsonAerospaceandElectronicSystems(TAES),51(3):1950–1960.
Castaldo,F.,Palmieri,F.,andRegazzoni,C.(2014). ApplicationofBayesianTechniquestoBehavior
AnalysisinMaritimeEnvironments,volume37,chapter5,pages175–183. Springer,Cham.
Coscia, P., Braca, P., Millefiori, L. M., Palmieri, F. A. N., and Willett, P. K. (2018a). Multiple
ornstein-uhlenbeckprocessesformaritimetrafficgraphrepresentation. IEEETransactionson
AerospaceandElectronicSystems,54:2158–2170.
Coscia,P.,Castaldo,F.,Palmieri,F.A.N.,Alahi,A.,Savarese,S.,andBallan,L.(2018b). Long-term
path prediction in urban scenarios using circular distributions. Image and Vision Computing,
69:81–91.
28
Coscia,P.,Castaldo,F.,Palmieri,F.A.N.,Ballan,L.,Alahi,A.,andSavarese,S.(2016). Point-based
pathpredictionfrompolarhistograms. InProceedingsofthe19thInternationalConferenceon
InformationFusion(FUSION2016),pages1961–1967.
Coscia, P., N. Palmieri, F. A., Braca, P., Millefiori, L. M., and Willett, P. (2018). Unsupervised
maritimetrafficgraphlearningwithmean-revertingstochasticprocesses.In201821stInternational
ConferenceonInformationFusion(FUSION),pages1822–1828.
DiGennaro,G.,Buonanno,A.,andPalmieri,F.A.N.(2021). Optimizedrealizationofbayesian
networksinreducednormalformusinglatentvariablemodel. SoftComputing,Springer,pages
1–12.
Forney,G.D.,J.(2001).Codesongraphs:normalrealizations.InformationTheory,IEEETransactions
on,47(2):520–548.
Imohiosen,A.,Watson,J.,andPeters,J.(2020). Activeinferenceorcontrolasinference?aunifying
view. InProceedingsofFirstInternationalWorkshop,IWAI2020,Co-locatedwithECML/PKDD
2020,Ghent,Belgium,September14.
Kaplan, R. and Friston, K. J. (2018). Planning and navigation as active inference. Biological
Cybernetics,112:323–347.
Kappen, H., Gomez, V., and Opper, M. (2013). Optimal control as a graphical model inference
problem. InProceedingsoftheTwenty-ThirdInternationalConferenceonAutomatedPlanning
andScheduling.
Kappen,H.J.,Gomez,V.,andManfred,M.(2012). Optimalcontrolasagraphicalmodelinference
problem. MachineLearning,87:159–182.
Koller,D.andFriedman,N.(2009). ProbabilisticGraphicalModels: PrinciplesandTechniques.
MITPress.
Levine,S.(2018). Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview.
arXiv:1805.00909v3[cs.LG]20May2018.
Loeliger,H.A.(2004). Anintroductiontofactorgraphs. IEEESignalProcessingMagazine,21(1):28
–41.
Loeliger,H.-A..,Dauwels,J.,Hu,J.,Korl,S.,Li,P.,andKschischang,F.(2007). Thefactorgraph
approachtomodel-basedsignalprocessing. ProceedingsoftheIEEE,95(6):1295–1322.
Millidge, B., Tschantz, A., Seth, A.K., andBuckley, C.L.(2020). Ontherelationshipbetween
activeinferenceandcontrolasinference. InProceedingsofFirstInternationalWorkshop,IWAI
2020,Co-locatedwithECML/PKDD2020,Ghent,Belgium,September14.
Nair,S.,Zhu,Y.,Savarese,S.,andLi,F.-F.(2019). Causalinductionfromvisualobservationsfor
goaldirectedtasks. arXiv:1910.01751v1[cs.LG]3Oct2019.
Palmieri,F.A.N.(2016). Acomparisonofalgorithmsforlearninghiddenvariablesinbayesianfactor
graphsinreducednormalform. IEEETransactionsonNeuralNetworksandLearningSystems,
27(11):2242–2255.
Palmieri, F.A.N., Pattipati, K.R., Fioretti, G., DiGennaro, G., andBuonanno, A.(2021). Path
planningusingprobabilitytensorflows. IEEEAerospaceandElectronicSystemsMagazine,36(1).
PreliminaryversiononarXiv:2003.02774.
Parr,T.andFriston,K.J.(2018a). Theanatomyofinference: Generativemodelsandbrainstructure.
FrontiersinComputationalNeuroscience,12.
Parr,T.andFriston,K.J.(2018b). Generalisedfreeenergyandactiveinference: canthefuturecause
thepast? BioARXIV.
Sutton,R.S.andBarto,A.G.(2018). ReinforcementLearning: AnIntroduction(secondedition).
MITPress.
29
Thrun,S.,Burgard,W.,andFox,D.(2006). ProbabilisticRobotics. MITPress.
Todorov, E. (2008). General duality between optimal control and estimation. In Proceedings of
the47thIEEEConferenceonDecisionandControl,Cancun,Mexico,Dec.9-11.
Toussaint,M.(2009). Probabilisticinferenceasamodelofplannedbehavior. KunstlicheIntelligenz,
3.
Toussaint,M.andStorkey,A.(2006). Probabilisticinferenceforsolvingdiscreteandcontinuous
statemarkovdecisionprocesses. Proceedingsofthe23rdInternationalConferenceonMachine
Learning,2006:945–952.
Verbelen,T.,Lanillos,P.,andBuckley,ChristopherL.DeBoom,C.,editors(2020). AbtiveInference,
First International Workshop, IWAI 2020Co-located with ECML/PKDD 2020Ghent, Belgium,
September14.Springer.
Ziebart,B.,Ratliff,N.,Gallagher,G.,Mertz,C.,Peterson,K.,Bagnell,J.,Hebert,M.,Dey,A.,and
Srinivasa,S.(2009). Planning-basedpredictionforpedestrians. pages3931–3936.
Ziebart,B.D.,Bagnell,A.,andDey,A.K.(2010).Modelinginteractionviatheprincipleofmaximum
causalentropy. InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML),
Haifa,Israel.
Table3: Forwarddistributionsforthesourceblocks.
f (a ) f (s ) f (s a )
At t S0 0 (StAt) t t
Sumproduct
Maxproduct
Sum/Maxproduct
p(a ) p(s ) c(s a )
DP t 0 t t
Max-Rew/Ent
SoftDP
Table4: Propagationrulesforactionshadedblocks.
b (a ) f (s a )
At t (StAt) t t
Sumproduct (cid:88)
b (s a ) f (a )U(s )
Max-Rew/Ent(α=1) (StAt) t t At t t
st
Maxproduct
maxb (s a ) f (a )U(s )
DP st (StAt) t t At t t
S M u a m x- /M Re a w x / p E r n o t d ( u α ct (cid:54)=1) (cid:115) α (cid:88) b (StAt) (s t a t )α f At (a t )U(s t )
st
(cid:34)(cid:80)
logb (s a )b (s a )β
(cid:35)
SoftDP exp st
(cid:80)
(StA
b
t) t t
(s(cid:48)
(
a
St
)
A
β
t) t t f At (a t )U(s t )
s(cid:48)
t
(StAt) t t
30
Table5: Propagationrulesforstateshadedblocks.
b (s ) f (s a )
St t (StAt) t t
Sumproduct (cid:88)
b (s a ) f (s )U(a )
Max-Rew/Ent(α=1) (StAt) t t St t t
at
Maxproduct
maxb (s a ) f (s )U(a )
DP at (StAt) t t St t t
S M u a m x- /M Re a w x / p E r n o t d ( u α ct (cid:54)=1) (cid:115) α (cid:88) b (StAt) (s t a t )α f St (s t )U(a t )
at
(cid:34)(cid:80)
logb (s a )b (s a )β
(cid:35)
SoftDP exp at
(cid:80)
(StA
b
t) t t
(s
(
a
S
(cid:48)
t
)
A
β
t) t t f St (s t )U(a t )
a(cid:48)
t
(StAt) t t
Table6: Propagationrulesforthedynamicsblock.
b (s a ) f (s )
(StAt) t t St+1 t+1
(cid:88) (cid:88)
Sumproduct p(s |s a )b (s ) p(s |s a )f (s a )
t+1 t t St+1 t+1 t+1 t t (StAt) t t
st+1 stat
Maxproduct maxp(s |s a )b (s ) maxp(s |s a )f (s a )
st+1
t+1 t t St+1 t+1
stat
t+1 t t (StAt) t t
(cid:115) (cid:115)
Sum/Maxproduct (cid:88) p(s |s a )αb (s )α (cid:88) p(s |s a )αf (s a )α
α t+1 t t St+1 t+1 α t+1 t t (StAt) t t
st+1 stat
DP
SoftDP e (cid:80) st+1 p(st+1|stat)logbSt+1 (st+1) e (cid:80) stat p(st+1|stat)logf(StAt)(stat)
Max-Rew/Ent
31
Table7: Propagationrulesforthediverter.
Sumproduct
b (s a )∝f (s a )f (s a )b (s a )
Maxproduct (StAt)1 t t (StAt)2 t t (StAt)3 t t (StAt)4 t t
Sum/Maxproduct b (StAt)2 (s t a t )∝f (StAt)1 (s t a t )f (StAt)3 (s t a t )b (StAt)4 (s t a t )
DP b (s a )∝f (s a )f (s a )b (s a )
(StAt)3 t t (StAt)1 t t (StAt)2 t t (StAt)4 t t
Max-Rew/Ent
f (s a )∝f (s a )f (s a )b (s a )
SoftDP
(StAt)4 t t (StAt)1 t t (StAt)2 t t (StAt)3 t t
32