Compositional Active Inference II:
Polynomial Dynamics. Approximate Inference Doctrines.
Toby St. Clere Smithe
University of Oxford
&
Topos Institute
toby@topos.institute
August 26, 2022
We develop the compositional theory of active inference by introducing activity, functorially
relating statistical games to the dynamical systems which play them, using the new notion of
approximate inference doctrine. In order to exhibit such functors, we /f_irst develop the necessary
theory of dynamical systems, using a generalization of the language of polynomial functors to
supply compositional interfaces of the required types: with the resulting polynomially indexed
categories of coalgebras, we construct monoidal bicategories of diﬀerential and dynamical “hier-
archical inference systems”, in which approximate inference doctrines have semantics. We then
describe “externally parameterized” statistical games, and use them to construct two approxi-
mate inference doctrines found in the computational neuroscience literature, which we call the
‘Laplace’ and the ‘Hebb-Laplace’ doctrines: the former produces dynamical systems which op-
timize the posteriors of Gaussian models; and the latter produces systems which additionally
optimize the parameters (or ‘weights’) which determine their predictions.
1 Introduction
In the /f_irst paper in this series [1], we introduced a compositional framework in which to make sense of the
‘statistical games’ played by adaptive and cybernetic systems, with a view to generalizing and contextualizing
the free energy principle that lies at the heart of theories of active inference [2]. Yet, these statistical games are
but one aspect of an active adaptive system, and if a theory of active inference is to be a theory of anything,
then it must also acknowledge activity! As a starting point, the framework of statistical games accounts for
systems that are open to their environment, and whose predictive performance is accordingly contextual,
but the next step — and the step taken in this paper — is to animate these statistical games, constructing
dynamical systems that play these games, and that can be correspondingly embodied in a changing world.
The behaviours of these model systems can then be compared with observations of natural adaptive systems,
and the models can then be re/f_ined accordingly.
It is a remarkable fact that our most infamous natural adaptive system, the mammalian brain, seems in
part to exemplify the hierarchical bidirectional structure of statistical games: certain neural circuits in sen-
sory cortex exhibit forward-looking predictions alongside backward-looking corrections that together can be
1
arXiv:2208.12173v1  [nlin.AO]  25 Aug 2022
modelled as a kind of dynamical Bayesian inference process, and which appear to couple together to approxi-
mate hierarchically structured Bayesian networks [3]. Understanding this resemblance is one of the principal
motivations for this work.
Since the brain is best understood as an ‘open’ ( i.e., embodied and interacting) dynamical system, this
resemblance seems to imply a functorial relationship between a category of statistical models on the one
hand and a category of open dynamical systems on the other: the functor would take an appropriately de/f_ined
statistical model or statistical game, and return a dynamical system that could be understood as playing the
game (or inverting the model); the functoriality of this relationship would ensure that the compositional
(including hierarchical) structure of the model would be recapitulated in the compositional structure of the
resulting dynamical system.
Exhibiting functors of this type, which collectively we call approximate inference doctrines , is the task of
Section 4, and indeed we /f_ind that the aforementioned neural circuit models arise precisely in this way. Not
only does this explain the mathematical origin of the structure of these circuits, but it simpli/f_ies the job
of modelling, as one no longer needs to perform a complicated computation for each model: instead, it is
suﬃcient to obtain the dynamics for each factor of the model, and compose them according to the rules of
the category. (In this paper, we focus on functors from statistical models to dynamical systems. One claim of
the free energy framework is that it furnishes a universal way to understand adaptive dynamical systems in
terms of Bayesian inference [4], suggesting functors in the opposite direction which we might hypothesize
to be appropriately adjoint. Understanding this relationship is the subject of future work.)
Overview of this paper Before we can exhibit any such functors, we need to lay the appropriate mathe-
matical groundwork. For our purposes, there are two overlapping aspects: a mathematical language in which
to talk about stochastic interacting systems; and a de/f_inition of open dynamical system that can be expressed
in this language and that can be cast into the the relevant compositional form.
In §2 therefore, we introduce the category of polynomial functors as our choice of language for interaction.
We think of a polynomial as playing a formal role akin to that of the notion of Markov blanket in the informal
active inference literature, as it de/f_ines the shape or boundary or interface of a type of system; morphisms of
polynomials describe how information /f_lows between the boundaries of coupled systems. In §2.1, we general-
ize the usual category of polynomials in order to capture stochastic interactions and the /f_low of probabilistic
information.
Then, in §3, we turn our attention to dynamics. We begin the section by de/f_ining a general notion of
dynamical system on an interface using the language of polynomials. We then package these systems up
into categories indexed by polynomials: each category represents a collection of ways that an interface may
be animated. Subsequently, in §3.1, we bring these categories together with the category of polynomials
itself to construct a new collection of categories of hierarchical bidirectional dynamical systems which have
the necessary compositional structure to de/f_ine approximate inference doctrines; then, in §3.2, we present
corresponding categories of diﬀerential systems, which often form a useful intermediate step on the way to
dynamical systems, and show how to obtain dynamical systems from them.
Finally, in §4, we introduce approximate inference doctrines, concentrating on two that are neuroscienti/f_i-
cally relevant. We begin the section by introducing two pieces of auxiliary technology: categories of Gaussian
channels (§4.2, to capture the two neuroscienti/f_ic doctrines); and parameterized statistical games (§4.1, to cap-
ture parameter learning like synaptic plasticity). This puts us in the position at last to de/f_ine two doctrines:
the Laplace doctrine (§4.3) for Gaussian channels; and the Hebb-Laplace doctrine (§4.4) for parameterized
Gaussian channels, where not only is the model inverted but the parameters are learnt, too.
2
2 Polynomial functors: a language for interacting systems
In order to be considered adaptive, a system must have something to adapt to. This ‘something’ is often what
we call the system’s environment, and we say that the system is open to its environment. The interface or
boundary separating the system from its environment can be thought of as ‘inhabited’ by the system: the
system is embodied by its interface of interaction; the interface is animated by the system. In this way, the
system can aﬀect the environment, by changing the shape or con/f_iguration of its interface 1; through the
coupling, these changes are propagated to the environment. In turn, the environment may impinge on the
interface: its own changes, mediated by the coupling, arrive at the interface as immanent signals; and the
type of signals to which the system is alive may depend on the system’s con/f_iguration (as when an eye can
only perceive if its lid is open). Thus, information /f_lows across the interface.
The mathematical language capturing this kind of inhabited interaction is that ofpolynomial functors, which
we adopt following Spivak and Niu [5]. Informally, a polynomial functor is determined by a type or set of
possible con/f_igurations, along with, for each possible con/f_iguration, a corresponding type or set of possible
immanent signals (‘inputs’). We will often write pto denote a polynomial, pp1qits possible con/f_igurations,
and for each i: pp1q, prisfor the corresponding inputs.
In this section, we introduce the basic theory of polynomial functors; in the following subsection, we extend
the theory to allow for more general kinds of interaction, to allow for explicitly probabilistic information
/f_lows. Taking a broader view, in this paper we only make use of a fragment of the richness of polynomial
interaction: just enough to build open and hierarchical dynamical systems that can perform inference within
a single system. Later in this series, we will expand our use of the language to treat multiple interacting active
inference systems, to provide something like a theory of “polynomial life”, building on our earlier work [6].
Now, however, we begin by introducing the formal de/f_inition of the classical category of polynomial functors.
De/f_inition 2.1.Let E be a locally Cartesian closed category (such asSet), and denote byyAthe representable
copresheaf yA :“EpA,´q: E ÑE. A polynomial functor pis a coproduct of representable functors, written
p :“ ř
i:pp1qypi, where pp1q : E is the indexing object. The category of polynomial functors in E is the
full subcategory PolyE ãÑrE,Esof the E-copresheaf category spanned by coproducts of representables. A
morphism of polynomials is therefore a natural transformation.
Remark 2.2. Every polynomial functor P : E Ñ E corresponds to a bundle p : E Ñ B in E, for which
B “ Pp1qand for each i : Pp1q, the /f_ibrepi is Ppiq. We will henceforth elide the distinction between a
copresheaf P and its corresponding bundle p, writing pp1q :“ B and pris :“ pi, where E “ ř
ipris. A
natural transformation f : p Ñqbetween copresheaves therefore corresponds to a map of bundles. In the
case of polynomials, by the Yoneda lemma, this map is given by a ‘forwards’ map f1 : pp1q Ñqp1qand
a family of ‘backwards’ maps f# : qrf1p-qs Ñpr-sindexed by pp1q, as in the left diagram below. Given
f : pÑqand g: qÑr, their composite g˝f : pÑris as in the right diagram below.
E f˚F F
B B C
f#
qp
f1
{
E f˚g˚G G
B B D
pgfq#
rp
g1˝f1
{
where pgfq# is given by the pp1q-indexed family of composite maps rrg1pf1p-qqs
f˚g#
ÝÝÝÑqrf1p-qs
f#
Ý Ý Ñpr-s.
We now recall a handful of useful facts about polynomials and their morphisms, each of which is explained
in Spivak and Niu [5] and summarized in Spivak [7].
1Such changes can be very general: consider for instance the changes involved in producing sound ( e.g., rapid vibration of tissue)
or light (e.g., connecting a luminescent circuit, or the molecular interactions involved therein).
3
Proposition 2.3. Polynomial morphisms pÑycorrespond to sections pp1qÑ ř
iprisof the corresponding
bundle p.
Proposition 2.4. There is an embedding of E into PolyE given by taking objects X : E to the linear poly-
nomials Xy : PolyE and morphisms f : X ÑY to morphisms pf,idXq: Xy ÑYy.
Proposition 2.5. There is a symmetric monoidal structure pb,yqon PolyE that we call tensor, and which
is given on objects by pbq :“ ř
i:pp1q
ř
j:qp1qyprisˆqrjs and on morphisms f :“ pf1,f#q : p Ñ p1 and
g:“pg1,g#q: qÑq1by f bg:“pf1 ˆg1,f# ˆg#q.
Proposition 2.6. pPolyE,b,yqis symmetric monoidal closed, with internal hom denotedr´,“s. Explicitly,
we have rp,qs“ ř
f:pÑqy
ř
i:pp1qqrf1piqs. Given an object A: E, we have rAy,ys– yA.
Proposition 2.7. The composition of polynomial functors q˝p: E ÑE ÑE induces a monoidal structure
on PolyE, which we denote Ÿ, and call ‘composition’ or ‘substitution’. Its unit is again y. Famously, Ÿ-
comonoids correspond to categories and their comonoid homomorphisms are cofunctors [8]. IfT is a monoid,
then the comonoid structure onyT corresponds witnesses it as the categoryBT. Monomials of the form SyS
can be equipped with a canonical comonoid structure witnessing the codiscrete groupoid on S.
2.1 Generalized polynomials for stochastic feedback
The category of polynomial functors PolyE introduced above for a locally Cartesian closed category E can
be considered as a category of ‘deterministic’ polynomial interaction; notably, morphisms of polynomials,
which encode the coupling of systems’ interfaces, do not explicitly incorporate any kind of randomness or
uncertainty. Even if the universe is deterministic, however, the /f_initeness of systems and their general inability
to perceive the totality of their environments make it a convenient modelling choice to suppose that systems’
interactions may be uncertain; this will be useful not only in allowing for stochastic interactions between
systems, but also to de/f_ine stochastic dynamical systems ‘internally’ to a category of polynomials.
To reach the desired generalization, we begin by recalling that PolyE is equivalent to the category of
Grothendieck lenses for the self-indexing ofE [5, 9]: PolyE –
ş
E{´op, where the opposite is taken pointwise
on each E{B; this is the formal basis for Remark 2.2. We de/f_ine our categories of generalized polynomials
from this perspective, by considering categories indexed by their “deterministic subcategories”: this allows
us to de/f_ine categories of Grothendieck lenses which behave likePolyE (when restricted to the deterministic
case), but also admit uncertain inputs.
Notation 2.8. Suppose C is a symmetric monoidal category. We writeComonpCqto denote the subcategory
of commutative comonoids and comonoid homomophisms in C.
Example 2.9. Suppose P : E ÑE is a probability monad 2 on E. Then every object in KℓpPqis equipped
with a canonical comonoid structure (the copy-discard structure [11, §2]), and Comon
`
KℓpPq
˘
is the wide
subcategory of ‘deterministic’ channels. Intuitively, this follows almost by de/f_inition: a deterministic process
is one that has no informational side-eﬀects; that is to say, whether we copy a state before performing the
process on each copy, or perform the process and then copy the resulting state, or whether we perform the
process and then marginalize, or just marginalize, makes no diﬀerence to the resulting state. This is just what
it means for the process to be a comonoid homomorphism; in other words, deterministic processes introduce
no new correlations. In fact, Comon
`
KℓpPq
˘
–E.
2By ‘probability monad’, we mean a monadP on E taking each object Xto an object PXthat behaves like a ‘space of probability
distributions on X’. The monad multiplication performs a ‘weighted average’ of distributions, and the monad unit returns the
point or ‘Dirac delta’ distribution on each element. For more information on and a number of examples of probability monads,
we refer the reader to Jacobs [10]. We will often write P to denote a generic probability monad.
4
With these ideas in mind, we make the following de/f_initions.
De/f_inition 2.10.Suppose pC,b,Iqis a copy-delete category such that ComonpCqis /f_initely complete and
I is terminal in ComonpCq. De/f_ine an indexed categoryP : ComonpCqop Ñ Cat as follows. For each
object B : ComonpCq, the category PpBqhas as objects the homomorphisms E ÑBof ComonpCqsuch
that for any other homomorphism AÑB, the pullback AˆB Esatis/f_ies the universal property inC. Given
a morphism f : C Ñ B, the functor Ppfq : PpBq ÑPpCqis given by pullback: Ppfq :“ f˚; this is
well-de/f_ined by the universal property.
De/f_inition 2.11.Suppose each functor Ppfq: PpBqÑ PpCqhas a left adjoint, denoted Σf. We de/f_ine the
category PolyC of polynomials in C to be the category of P-lenses: PolyC :“
ş
Pop, where the opposite is
taken pointwise.
Example 2.12. When C is any locally Cartesian closed category such as Set, equipped with its Cartesian
monoidal structure, De/f_inition 2.10 recovers its self-indexing and hencePolyC is the usual category of poly-
nomials in C.
Example 2.13. Suppose E is a /f_initely complete category andM is a monoidal monad on E. Denote by ι
the identity-on-objects inclusion E ãÑKℓpMqgiven on morphisms by post-composing with the unitηof the
monad structure. Setting C “KℓpMq, we /f_ind that forB : E, PpBqis the full subcategory of KℓpMq{B on
those objects ιp : EÑ‚Bwhich correspond to maps E
p
Ý ÑB
ηB
Ý Ý ÑMB in the image of ι. Given a morphism
f : C ÑBin E, the functorPpfqtakes objects ιp: EÑ‚Bto ιpf˚pq: f˚EÑ‚Cwhere f˚pis the pullback ofp
along fin E, included intoKℓpMqby ι. Now suppose thatαis a morphismpE,ιp : EÑ‚BqÑp F,ιq : FÑ‚Bq
in PpBq, and note that since we must have ιq‚α“ιp, αmust correspond to a family of maps αx : prxsÑ
Mqrxsfor x: B. Therefore, Ppfqpαqcan be de/f_ined pointwise asPpfqpαqy :“αfpyq : prfpyqsÑ Mqrfpyqs
for y: C.
Notation 2.14. For any such monoidal monad M where E has dependent sums, we will write PolyM as
shorthand denoting the corresponding generalized category of polynomialsPolyKℓpMq. Since every category
C corresponds to a trivial monad which we can also denote by C, this notation subsumes that of De/f_inition
2.11.
Remark 2.15. We can think of PolyM as a dependent version of the category of M-monadic lenses, in the
sense of Clarke et al. [12, §3.1.3].
Unwinding Example 2.13 further, we /f_ind that the objects ofPolyM are the same polynomial functors as
constitute the objects of PolyE. The morphisms f : pÑqare pairs pf1,f#q, where f1 : B ÑCis a map in
E and f# is a family of morphisms qrf1pxqsÑ‚prxsin KℓpMq, making the following diagram commute:
ř
x:BMprxs ř
b:Bqrf1pxqs ř
y:C qrys
B B C
f#
qηB˚p
f1
{
Our principal example of interest is of this form, being PolyP for a probability monad P on E3. We we
consider each such category PolyP to be a category of polynomials with stochastic feedback .
3Ideally, E would also be locally Cartesian closed, so thatPolyP recapitulates much of the basic structure ofPolySet (see Remark
2.17): such examples include the category QBS of quasi-Borel spaces equipped with the quasi-Borel distribution monad [13], or
the category Set equipped with the /f_initely-supported distribution monad.
5
Remark 2.16. By assuming that the categoryC has a monoidal structurepb,Iq, its corresponding generalized
category of polynomials PolyC inherits a tensor akin to that de/f_ined in Proposition 2.5, and which we also
denote by pb,Iq: the de/f_inition only diﬀers by substituting the structurepb,Iqon C for the productpˆ,1qon
E. This follows from the monoidal Grothendieck construction:P is lax monoidal, with laxator takingp: PpBq
and q: PpCqto pbq: PpBbCq.
On the other hand, for PolyC also to have an internal hom rq,rsrequires each /f_ibre ofP to be closed with
respect to the monoidal structure. In cases of particular interest,ComonpCqwill be locally Cartesian closed,
and restricting P to its self-indexing produces /f_ibres which are thus Cartesian monoidal closed. In these cases,
we can think of the broader /f_ibres ofP, and thus PolyC itself, as being ‘deterministically’ closed. This means,
for the stochastic example PolyP, we get an internal hom satisfying the adjunction PolyPppbq,rq –
PolyPpp,rq,rsqonly when the backwards components of morphisms pbqÑrare ‘uncorrelated’ between
pand q.
Remark 2.17. For PolyC to behave faithfully like the usual category of polynomial functors, we should
want the substitution functorsPpfq: PpCqÑ PpBqto have right adjoints as well as left. As in the preceding
remark, these only obtain in restricted circumstances; we will consider the case of PolyM for a monad M,
writing f˚to denote the functor Ppfq.
Denote the putative right adjoint by Πf : PpBq ÑPpCq, and for ιp : EÑ‚B suppose that pΠfEqrysis
given by the set of ‘partial sections’σ: f´1tyuÑ ME of pover f´1tyuas in the commutative diagram:
f´1tyu tyu
ME B C
f
{
ηB˚p
σ
Then we would need to exhibit a natural isomorphism PpBqpf˚D,Eq– PpCqpD,ΠfEq. But this will only
obtain when the ‘backwards’ componentsh#
y : DrysÑ MpΠfEqrysare in the image of ι—otherwise, it is
not generally possible to pull f´1tyuout of M.
3 Open dynamical systems on polynomial interfaces
Having constructed PolyC, we are now in a position to construct, for each p : PolyC, a category of open
dynamical systems CoalgT
Cppqwith interface p, and we can even state the de/f_inition entirely in the language
of PolyC. Here, T is a monoid object pT,`,0qin ComonpCqthat represents time, which is necessary in
general to ensure that the dynamics can ‘/f_low’ appropriately; slightly more formally, we will need to ensure
that evolving the dynamics for time t: T and then s: T produces the same trajectory as evolving it for time
t`s, and that evolving it for no time 0 : T induces no change. If we choose C “KℓpPqfor P a probability
monad, we obtain categories of stochastic systems that we call open Markov processes , although we develop
the theory in a more general context (allowing for other types of transition, as as nondeterministic).
We /f_irst give a concise de/f_inition, internal toPolyC, before unpacking it into a more elementary form.
De/f_inition 3.1.An open dynamical system with interface p: PolyC, state space S : C and time pT,`,0qis
a polynomial morphism β : SyS ÑrTy,pssuch that, for any section σ: pÑy, the induced morphism
SyS β
Ý ÑrTy,ps
rTy,σs
Ý ÝÝÝ ÑrTy,ys „Ý ÑyT
is a Ÿ-comonoid homomorphism.
Unpacking this de/f_inition gives us the following characterization:
6
Proposition 3.2. An open dynamical system β : SyS ÑrTy,psin PolyC consists in a triple pS,βo,βuqof
a state space S : C and two morphisms βo : TˆS Ñpp1qin ComonpCqand βu : ř
t:T
ř
s:S prϑopt,sqsÑ S
in C, such that, for any section σ: pp1qÑ ř
i:pp1qprisof p, the morphisms βσ : T ˆS ÑSgiven by
ÿ
t:T
S
βoptq˚σ
Ý ÝÝÝÝ Ñ
ÿ
t:T
ÿ
s:S
prβopt,sqs
βu
ÝÑS
form an object in the functor category Cat
`
BT,C
˘
, where BT is the delooping of T. We call the closed
system βσ, induced by a section σof p, the closure of βby σ. Equivalently, we can say that βσ : T ˆS ÑS
forms an action of the monoid T on Sin C.
Open dynamical systems on p form a category, which we denote by CoalgT
Cppq. We can exhibit this
category abstractly, by noting that a morphism SyS Ñrof polynomials is equivalent to a morphism S Ñ
rpSqin C: that is, to an r-coalgebra; morphisms of open dynamical systems then correspond to coalgebra
homomorphisms, and this gives us a category. For our purposes here, however, it is more illuminating to
exhibit CoalgT
Cppqexplicitly.
Proposition 3.3. Open dynamical systems onpwith timeT form a category, denotedCoalgT
C. Its morphisms
are de/f_ined as follows. Letϑ:“pX,ϑo,ϑuqand ψ:“pY,ψo,ψuqbe two such systems. A morphism f : ϑÑ
ψconsists in a morphismf : X ÑY in C such that, for any timet: T and global sectionσ: pp1qÑ ř
i:pp1q
pris
of p, the following square commutes:
X ř
x:X
prϑopt,xqs X
Y ř
y:Y
prψopt,yqs Y
ϑoptq˚σ ϑuptq
f f
ψoptq˚σ ψuptq
The identity morphism idϑ on ϑis given by the identity morphism idX on its state space X. Composition of
morphisms is given by composition of the morphisms of the state spaces.
Since open dynamical systems onpare morphisms SyS ÑrTy,psof polynomials, there is a natural covari-
ant reindexing of systems along morphisms pÑq, given by postcomposing with the map rTy,psÑr Ty,qs
induced by the functorrTy,´s. This givesCoalgT
Cp´qthe structure of an opindexed categoryPolyC ÑCat,
which we spell out in the following proposition.
Proposition 3.4. CoalgT
Cppqextends to an opindexed category, CoalgT
Cp´q : PolyC Ñ Cat. Suppose
ϕ : p Ñq is a morphism of polynomials. We de/f_ine a corresponding functorCoalgT
Cpϕq: CoalgT
Cppq Ñ
CoalgT
Cpqqas follows. SupposepX,ϑo,ϑuq: CoalgT
Cppqis an object (system) inCoalgT
Cppq. Then CoalgT
CpϕqpX,ϑo,ϑuq
is de/f_ined as the triplepX,ϕ1 ˝ϑo,ϑu˝ϑo˚ϕ#q: CoalgT
Cpqq, where the two maps are explicitly the following
composites:
T ˆX ϑo
ÝÑpp1q
ϕ1
ÝÑqp1q,
ÿ
t:T
ÿ
x:X
qrϕ1 ˝ϑopt,xqs
ϑo˚ϕ#
ÝÝÝÝÑ
ÿ
t:T
ÿ
x:X
prϑopt,xqs ϑu
ÝÑX.
On morphisms, CoalgT
Cpϕqpfq : CoalgT
CpϕqpX,ϑo,ϑuq ÑCoalgT
CpϕqpY,ψo,ψuqis given by the same
underlying map f : X ÑY of state spaces.
It is sometimes useful to relate dynamical systems with diﬀerent time monoids—for instance, to discretize a
continuous-time system, or to adjust the timescale of evolution of a system—and for these purposes we have
the following proposition.
7
Proposition 3.5. Any map f : T1 ÑT of monoids induces an indexed functor CoalgT
C ÑCoalgT1
C .
Proof. We /f_irst consider the induced functorCoalgT
CppqÑ CoalgT1
C ppq, which we denote by ∆p
f. Note that
we have a morphism rfy,ps: rTy,psÑr T1y,psof polynomials by substitution (precomposition). A system
β in CoalgT
C is a morphism SyS Ñ rTy,psfor some S, and so we de/f_ine∆p
fpβqto be rf,ps˝ β : SyS Ñ
rTy,psÑr T1y,ps. To see that this satis/f_ies the monoid action axiom, consider that the closure∆p
fpβqσ for
any section σ: pÑyis given by
ÿ
t:T1
S
βopfptqq˚σ
Ý ÝÝÝÝÝÝ Ñ
ÿ
t:T1
ÿ
s:S
prβopfptq,sqs
βu
ÝÑS
which is an object in the functor category CatpBT1,Cqsince f is a monoid homomorphism. On morphisms
of systems, the functor ∆p
f acts trivially.
To see that ∆f collects into an indexed functor, consider that it is de/f_ined on each polynomial pby the
contravariant action rf,psof the internal hom r´,“s, and that the reindexingCoalgTpϕqfor any morphism
ϕof polynomials is similarly de/f_ined by the covariant actionrTy,ϕs. By the bifunctoriality of r´,“s, we
have rT1y,ϕs˝r fy,ps“r fy,ϕs“r fy,q s˝r Ty,ϕs, and so CoalgT1
C pϕq˝ ∆p
f “∆q
f ˝CoalgT
C.
Corollary 3.6. For each k : R, the canonical inclusion ιk : N ãÑ R : i ÞÑ ki induces a corresponding
‘discretization’ indexed functorDisck :“∆ι : CoalgR
C ÑCoalgN
C .
Using the tensor productbof polynomials, we can put systems’ interfaces “in parallel”, and it will be useful
to do the same for the systems themselves. We can do this using the corresponding lax monoidal structure of
CoalgT
Cp´q.
Proposition 3.7. CoalgT
Cp´qis lax monoidalpPolyC,b,yqÑp Cat,ˆ,1q. The componentsλp,q : CoalgT
Cppqˆ
CoalgT
Cpqq ÑCoalgT
Cppbqqof the laxator natural transformation λare the functors de/f_ined as follows.
On objects, givenβ : XyX ÑrTy,psover pand γ : YyY ÑrT,qsover q, the systemλp,qpβ,γqis the system
pXbYqypXbYq “Ý ÑXyX bYyY βbγ
Ý ÝÝ ÑrTy,psbr T,qs
υp,q
ÝÝÑrTy,p bqs
with state space XˆY. The forwards component
υ1 : ComonpCq
`
T,pp1q
˘
ˆComonpCq
`
T,qp1q
˘
ÑComonpCq
`
T,pp1qˆ qp1q
˘
of υp,q forms the product of two trajectories, taking f : T Ñpp1qand g: T Ñqp1qto
υ1pf,gq:“T ÝÑT bT
fbg
ÝÝÑpp1qb qp1q.
The backwards components witness simultaneous inputs; in elementwise form, we have
υ#
f,g :
ÿ
t:T
prfptqsb qrgptqsÑ
ÿ
t,t1:T
prfptqsb qrgpt1qs
pt,a,b qÞÑp t,t,a,b q.
On morphisms ϕ : β Ñβ1 and ψ : γ Ñγ1, λp,qpϕ,ψq: λp,qpβ,γqÑ λp,qpβ1,γ1qis de/f_ined by taking the
product of the underlying maps of state spaces ϕ: X ÑX1and ψ: Y ÑY1. We will overload the notation,
writing βbγin place of λp,qpβ,γq, and similarly ϕbψon morphisms.
Finally, the unitor ϵ : 1 ÑCoalgT
Cpyqis the functor taking the unique object ‹in the terminal category 1
to the (‘closed’) systemp1,!o,!uqover ywith trivial state space, trivial output map, and trivial update map. It
sends the unique morphism id‹in 1 to the identity map on 1.
8
Proof sketch. Firstly, it is straightforward to check that the functors λp,q and ϵreturn well-de/f_ined systems
and morphisms, and that they are themselves well-de/f_ined as functors. Next, we check that the functorsλp,q
collect into a natural transformation. This follows almost immediately from the functoriality ofrTy,´b“s :
PolyC ˆPolyC ÑPolyC. Finally, we check that the axioms of associativity and unitality are satis/f_ied. This
follows from the associativity and unitality of the monoidal structure pb,yqon PolyC.
Note that CoalgT
C really is lax monoidal—the laxators are not equivalences—since not all systems over the
parallel interface pbqfactor into a system over palongside a system over q.
3.1 Monoidal bicategories of hierarchical inference systems
Whereas it is the morphisms (1-cells) of categories of lenses and statistical games that represent open systems,
it is the objects (0-cells) of the opindexed categoriesCoalgT
C
4 that play this role; in fact, the objects ofCoalgT
C
each represent both an open system and its (polynomial) interface. In order to supply dynamical semantics
for statistical games—functors from categories of statistical games to categories of dynamical systems—we
need to cleave the dynamical systems from their interfaces, making the interfaces into 0-cells and systems
into 1-cells between them, thereby letting the systems’ types and composition match those of the games.
To do this, we will associate to each pair of objects pA,Sqand pB,T qof a category of Bayesian lenses 5
a polynomial vAyS,ByTwwhose con/f_igurations correspond to lenses and whose inputs correspond to the
lenses’ inputs. The categories CoalgT
P
`
vAyS,ByTw
˘
will then form the hom-categories of bicategories of
hierarchical inference systems, and it is in these bicategories that we will /f_ind our dynamical semantics.
De/f_inition 3.8.Let BayesLensC be the category of (non-dependent) Bayesian lenses in C, with C enriched
in ComonpCq. Then for any pair of objects pA,Sqand pB,T qin BayesLensC, we de/f_ine a polynomial
vAyS,ByTwin PolyC by
vAyS,ByTw:“
ÿ
l:BayesLensC
`
pA,Sq,pB,Tq
˘yCpI,AqˆT .
Remark 3.9. We can think ofvAyS,ByTwas an ‘external hom’ polynomial forBayesLensC, playing a role
analogous to the internal hom rp,qsin PolyC. Its ‘bipartite’ structure—with domain and codomain parts—is
what enables cleaving systems from their interfaces, which are given by these parts. The de/f_inition, and the
following construction of the monoidal bicategory, are inspired by the operadOrg introduced by Spivak [14]
and generalized by St Clere Smithe [15].
Remark 3.10. Note that vAyS,ByTwis strictly speaking a monomial, since it can be written in the formIyJ
for I “BayesLensC
`
pA,Sq,pB,T q
˘
and J “CpI,AqˆT. However, we have written it in polynomial form
with the view to extending it in future work to dependent lenses and dependent optics [16, 17] — where we
will call systems over such external hom polynomials cilia, as they “control optics” — and these generalized
external homs will in fact be true polynomials.
Proposition 3.11. De/f_inition 3.8 de/f_ines a functorBayesLensop
C ˆBayesLensC Ñ PolyC. Suppose
c :“ pc1,c#q : pZ,Rq ÞÑ pA,Sqand d :“ pd1,d#q : pB,T q ÞÑ pC,Uqare Bayesian lenses. We obtain
a morphism of polynomials vc,dw : vAyS,ByTw Ñ vZyR,CyUwas follows. Since the con/f_igurations of
vAyS,ByTware lenses pA,Sq ÞÑpB,T q, the forwards map acts by pre- and post-composition:
vc,dw1 :“dp´qc: BayesLensC
`
pA,Sq,pB,T q
˘
ÑBayesLensC
`
pZ,Rq,pC,Uq
˘
lÞÑdlc
4or, more precisely, their corresponding op/f_ibrations
ş
CoalgT
C
5We will assume that these lenses are non-dependent lenses, as in St. Clere Smithe [1].
9
For each such l, the backwards map vc,dw#
l has type CpI,Zqb U ÑCpI,Aqb T in C, and is obtained by
analogy with the backwards composition rule for Bayesian lenses. We de/f_ine
vc,dw#
l :“CpI,Zqb U
c1˚bU
ÝÝÝÝÑCpI,Aqb U
bU
ÝÝÝÑCpI,Aqb CpI,Aqb U¨¨¨
¨¨¨
CpI,Aqbl1˚bU
ÝÝÝÝÝÝÝÝÝÑCpI,Aqb CpI,Bqb U
CpI,Aqbd#bU
Ý ÝÝÝÝÝÝÝÝ ÑCpI,Aqb CpU,Tqb U¨¨¨
¨¨¨
CpI,AqbevU,T
ÝÝÝÝÝÝÝÝÑCpI,Aqb T
where l1 is the forwards part of the lens l: pA,Sq ÞÑpB,T q, and c1˚ :“CpI,c1qand l1˚ :“CpI,l1qare the
push-forwards alongc1 and l1, andevU,T is the evaluation map induced by the enrichment ofC in ComonpCq.
In the special case where C “KℓpPqand ComonpCq“ E, we can write vc,dw#
l as the following map in E,
depicted as a string diagram:
vc,dw#
l “
c1˚
l1˚
d5
PZ
U
PT
PA
str
Here, we have assumed that KℓpPqpI,Aq “PA, and de/f_ined5 : PB ˆU Ñ PT to be the image of
d# : PB ÑKℓpPqpU,Tqunder the Cartesian closure of E, and str : PAˆPT ÑP
`
PAˆTqthe (right)
strength of the strong monad P.
Proof. We need to check that the mappings de/f_ined above respect identities and composition. It is easy to see
that the de/f_inition preserves identities: in the forwards direction, this follows from the unitality of composition
in BayesLensC; in the backwards direction, because pushing forwards along the identity is again the identity,
and because the backwards component of the identity Bayesian lens is the constant state-dependent morphism
on the identity in C.
To check that the mapping preserves composition, we consider the contravariant and covariant parts sep-
arately. Suppose b:“pb1,b#q: pY,Qq ÞÑpZ,Rqand e:“pe1,e#q: pC,Uq ÞÑpD,V qare Bayesian lenses.
We consider the contravariant case /f_irst: we check thatvcb,ByTw “ vb,ByTw˝v c,ByTw. The forwards
direction holds by pre-composition of lenses. In the backwards direction, we note from the de/f_inition that
only the forwards channel c1 plays a role in vc,ByTw#
l , and that role is again pre-composition. We therefore
only need to check that pc1 ‚b1q˚ “c1˚˝b1˚, and this follows immediately from the functoriality ofCpI,´q.
We now consider the covariant case, that vAyS,e dw“v AyS,ew˝v AyS,dw. Once again, the forwards
direction holds by composition of lenses. For simplicity of exposition, we consider the backwards direction in
the case C “KℓpPqand reason graphically. In this case, the backwards map on the right-hand side is given,
10
for a lens l: pA,Sq ÞÑpB,T qby the following string diagram:
l1˚
e5
PA
V
d5˚d1˚
str
PU
PA
It is easy to verify that the composition of backwards channels here is precisely the backwards channel given
by ed—compare St. Clere Smithe [1, Theorem 3.14] or [18, Theorem 5.2]—which establishes the result. The
case for general C is directly analogous, on the other side of the tensor-hom adjunction.
Now that we have an ‘external hom’, we might expect also to have a corresponding ‘external composition’,
represented by a family of morphisms of polynomials; we establish such a family now, and it will be important
in our bicategorical construction.
De/f_inition 3.12.We de/f_ine an ‘external composition’ natural transformationc, with components
vAyS,ByTwbv ByT,CyUwÑv AyS,CyUw
given in the forwards direction by composition of Bayesian lenses. In the backwards direction, for each pair
of lenses c: pA,Sq ÞÑpB,T qand d: pB,T q ÞÑpC,Uq, we need a map
c#
c,d : CpI,Aqb U ÑCpI,Aqb T bCpI,Bqb U
˘
which we de/f_ine as follows:
c#
c,d :“CpI,Aqb U
b
Ý ÝÝÝ ÑCpI,Aqb CpI,Aqb U bU¨¨¨
¨¨¨
CpI,Aqbc1˚bUbU
ÝÝÝÝÝÝÝÝÝÝÝÑCpI,Aqb CpI,Bqb U bU¨¨¨
¨¨¨
CpI,Aqb bCpI,BqbUbU
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpI,Aqb CpI,Bqb CpI,Bqb U bU
¨¨¨
CpI,AqbCpI,Bqbd#bUbU
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑCpI,Aqb CpI,Bqb CpU,Tqb Y bU
¨¨¨
CpI,AqbCpI,BqevU,TbU
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpI,Aqb CpI,Bqb T bU
¨¨¨
CpI,AqbswapbU
Ý ÝÝÝÝÝÝÝÝÝ ÑCpI,Aqb T bCpI,Bqb U
where c1˚and evU,T are as in 3.11.
In the case where C “KℓpPq, we can equivalently (and more legibly) de/f_inec#
c,d by the following string
11
diagram:
c#
c,d :“
d5
c1˚
str
PA
PT
PB
U
PA
U
where d5and str are also as in Proposition 3.11.
We leave to the reader the detailed proof that this de/f_inition produces a well-de/f_ined natural transforma-
tion, noting only that the argument is analogous to that of Proposition 3.11: one observes that, in the forwards
direction, the de/f_inition is simply composition of Bayesian lenses (which is immediately natural); in the back-
wards direction, one observes that the de/f_inition again mirrors that of the backwards composition of Bayesian
lenses.
Next, we establish the structure needed to make our bicategory monoidal.
De/f_inition 3.13.We de/f_ine a distributive lawd of v´,“wover b, a natural transformation with components
vAyS,ByTwbv A1yS1
,B1yT1
wÑv AyS bA1yS1
,ByT bB1yT1
w,
noting that AySbA1yS1
“pAbA1qypSbS1qand ByT bB1yT1
“pBbB1qypTbT1q. The forwards component
is given simply by taking the tensor of the corresponding Bayesian lenses, using the monoidal product (also
denoted b) in BayesLensC. Backwards, for each pair of lenses c : pA,Sq ÞÑ pB,T qand c1 : pA1,S1q ÞÑ
pB1,T1q, we need a map
d#
c,c1 : CpI,A bA1qb T bT1 ÑCpI,Aqˆ T ˆCpI,A1qˆ T1
for which we choose
CpI,A bA1qb T bT1 bTbT1
Ý ÝÝÝÝÝ ÑCpI,A bA1qb CpI,A bA1qb T bT1¨¨¨
¨¨¨
CpI,projAqbCpI,projA1qbTbT1
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑCpI,Aqb CpI,A1qb T bT1¨¨¨
¨¨¨
CpI,AqbswapbT1
ÝÝÝÝÝÝÝÝÝÝÑCpI,Aqb T bCpI,A1qb T1
where swap is the symmetry of the tensor bin C. Note that d#
c,c1 so de/f_ined does not in fact depend on either
cor c1.
We now have everything we need to construct a monoidal bicategory HierT
C of dynamical hierarchical
inference systems in C, following the intuition outlined at the beginning of this section.
Remark 3.14. The notion of bicategory that we adopt is the standard one of ‘category weakly enriched in
Cat’, so that between any two 0-cells we have a category of 1-cells (and 2-cells between them), such that
composition of 1-cells is associative and unital up to natural isomorphism.
12
De/f_inition 3.15.Let HierT
C denote the monoidal bicategory whose 0-cells are objectspA,Sqin BayesLensC,
and whose hom-categories HierT
C
`
pA,Sq,pB,T q
˘
are given by CoalgT
C
`
vAyS,ByTw
˘
. The identity 1-cell
idpA,Sq : pA,SqÑp A,Sqon pA,Sqis given by the system with trivial state space 1, trivial update map, and
output map that constantly emits the identity Bayesian lens pA,Sq ÞÑpA,Sq. The composition of a system
pA,SqÑp B,T qthen a system pB,T qÑp C,Uqis de/f_ined by the functor
HierT
C
`
pA,Sq,pB,T q
˘
ˆHierT
C
`
pB,T q,pC,Uq
˘
“CoalgT
C
`
vAyS,ByTw
˘
ˆCoalgT
C
`
vByT,CyUw
˘
λÝ ÑCoalgT
C
`
vAyS,ByTwbv ByT,CyUw
˘
CoalgT
Cpcq
ÝÝÝÝÝÝÑCoalgT
C
`
vAyS,CyUw
˘
“HierT
C
`
pA,Sq,pC,Uq
˘
where λis the laxator and c is the external composition morphism of De/f_inition 3.12.
The monoidal structure pb,yqon HierT
C derives from the structures on PolyC and BayesLensC, justifying
our overloaded notation. On 0-cells, pA,Sqbp A1,S1q:“pAbA1,S bS1q. On 1-cells pA,SqÑp B,T qand
pA1,S1qÑp B1,T1q, the tensor is given by
HierT
C
`
pA,Sq,pB,T q
˘
ˆHierT
C
`
pA1,S1q,pB1,T1q
˘
“CoalgT
C
`
vAyS,ByTw
˘
ˆCoalgT
C
`
vA1yS1
,B1yT1
w
˘
λÝ ÑCoalgT
C
`
vAyS,ByTwbv A1yS1
,B1yT1
w
˘
CoalgT
Cpdq
ÝÝÝÝÝÝÑCoalgT
C
`
vAyS bA1yS1
,ByT bB1yT1
w
˘
“HierT
C
`
pA,Sqbp A1,S1q,pB,T qbp B1,T1q
˘
where d is the distributive law of De/f_inition 3.13. The same functors
HierT
C
`
pA,Sq,pB,T q
˘
ˆHierT
C
`
pA1,S1q,pB1,T1q
˘
ÑHierT
C
`
pA,Sqbp A1,S1q,pB,T qbp B1,T1q
˘
induce the tensor of 2-cells; concretely, this is given on morphisms of dynamical systems by taking the product
of the corresponding morphisms between state spaces.
We do not give here a proof that this makes HierT
C into a well-de/f_ined monoidal bicategory; brie/f_ly, the
result follows from the facts that the external compositionc and the tensorbare appropriately associative and
unital, that CoalgT
P is lax monoidal, that v´,“wis functorial in both positions, and that v´,“wdistributes
naturally over b.
Before we move on to considering doctrines of approximate inference, it will be useful to spell out concretely
the elements of a morphism pA,SqÑp B,T qin HierT
KℓpPq.
Proposition 3.16. Suppose P is a monad on a Cartesian closed categoryE. Then a 1-cellϑ: pA,SqÑp B,T q
in HierT
KℓpPqis given by a tuple ϑ:“pX,ϑo
1,ϑo
2,ϑuqof
• a choice of state space X,
• a forwards output map ϑo
1 : T ˆXˆAÑPBin E,
• a backwards output map ϑo
2 : T ˆXˆPAˆT ÑPSin E, and
• an update map ϑu : T ˆXˆPAˆT ÑPX in E,
satisfying the ‘/f_low’ condition of Proposition 3.2.
Proof. The result follows immediately upon unpacking the de/f_initions, using the Cartesian closure ofE.
13
3.2 Diﬀerential and ‘cybernetic’ systems
Approximate inference doctrines describe how systems play statistical games, and are particularly of interest
when one asks how systems’ performance may improve during such game-playing. One prominent method
of performance improvement involves descending the gradient of the statistical game’s loss function, and
we will see below that this method is adopted by both the Laplace and the Hebb-Laplace doctrines. The
appearance of gradient descent prompts questions about the connections between such statistical systems
and other ‘cybernetic’ systems such as deep learners or players of economic games, both of which may also
involve gradient descent [19, 20]; indeed, it has been proposed [21] that parameterized gradient descent should
form the basis of a compositional account of cybernetic systems in general6.
In order to incorporate gradient descent explicitly into our own compositional framework, we follow the
recipes above to de/f_ine here /f_irst a category of diﬀerential systems opindexed by polynomial interfaces and
then a monoidal bicategory of diﬀerential hierarchical inference systems. We then show how we can obtain
dynamical from diﬀerential systems by integration, and sketch how this induces a “change of base” from
dynamical to diﬀerential hierarchical inference systems.
Notation 3.17. Write DiﬀC for the subcategory of compact smooth manifold objects in ComonpCqand
diﬀerentiable morphisms between them. Write T : DiﬀC Ñ VectpDiﬀCqfor the corresponding tangent
bundle functor, where VectpDiﬀCqis (the total category of) the /f_ibration of vector bundles overDiﬀC and
their homomorphisms. Write U : VectpDiﬀCqÑ DiﬀC for the functor that forgets the bundle structure.
Write T :“UT : DiﬀC ÑDiﬀC for the induced endofunctor.
Recall that morphisms AyB Ñpin PolyC correspond to morphisms AÑpBin C.
De/f_inition 3.18.For each p : PolyC, de/f_ine the categoryDiﬀSysCppqas follows. Its objects are objects
M : DiﬀC, each equipped with a morphism m : MyTM Ñpof polynomials in PolyC, such that for any
section σ: pÑyof p, the composite morphismσ˝m: MyTM Ñycorresponds to a sectionmσ : M ÑTM
of the tangent bundleTM ÑM. A morphismα: pM,mqÑp M1,m1qin DiﬀSysCppqis a mapα: M ÑM1
in DiﬀC such that the following diagram commutes:
M pTM
M1 pTM1
m
α
m1
pTα
Proposition 3.19. DiﬀSysC de/f_ines an opindexed categoryPolyC ÑCat. Given a morphism ϕ: pÑqof
polynomials, DiﬀSysCpϕq: DiﬀSysCppqÑ DiﬀSysCpqqacts on objects by postcomposition and trivially
on morphisms.
Proposition 3.20. The functor DiﬀSysC is lax monoidal pPolyC,b,yqÑp Cat,ˆ,1q.
Proof sketch. Note that T is strong monoidal, with Tp1q– 1 and TpMqb TpNq– TpM bNq. The unitor
1 ÑDiﬀSysCpyqis given by the isomorphism 1yT1 –1y1 –yinduced by the strong monoidal structure of
T. The laxator λp,q : DiﬀSysCppqˆDiﬀSysCpqqÑ DiﬀSysCppbqqis similarly determined: given objects
6Our own view on cybernetics is somewhat more general, since not all systems that may be seen as cybernetic are explicitly struc-
tured as gradient-descenders, and nor even is explicit diﬀerential structure always apparent. In earlier work, we suggested that
statistical inference was perhaps more inherent to cybernetics [22], although today we believe that a better, though more informal,
de/f_inition of cybernetic system is perhaps “an intentionally-controlled open dynamical system”. Nonetheless, we acknowledge
that this notion of “intentional control” may generally be reducible to a stationary action principle, again indicating the impor-
tance of diﬀerential structure. We leave the statement and proof of this general principle to future work.
14
m: MyTM Ñpand n: NyTN Ñq, take their tensor mbn: pMbNqyTMbTN and precompose with the
induced morphism pMbNqyTpMbNq ÑpMbNqyTMbTN; proceed similarly on morphisms of diﬀerential
systems. The satisfaction of the unitality and associativity laws follows from the monoidality of T.
We now de/f_ine a monoidal bicategoryDiﬀHierC of diﬀerential hierarchical inference systems, following
the de/f_inition ofHierT
C above.
De/f_inition 3.21.Let DiﬀHierC denote the monoidal bicategory whose 0-cells are again the objectspA,Sqof
BayesLensC and whose hom-categories DiﬀHierC
`
pA,Sq,pB,T q
˘
are given by DiﬀSysC
`
vAyS,ByTw
˘
.
The identity 1-cell idpA,Sq : pA,SqÑp A,Sqon pA,Sqis given by the diﬀerential system y ÑvAyS,ByTw
with state space 1, trivial backwards component, and forwards component that picks the identity Bayesian
lens on pA,Sq. The composition of diﬀerential systems pA,SqÑp B,T qthen pB,T qÑp C,Uqis de/f_ined
by the functor
DiﬀHierC
`
pA,Sq,pB,T q
˘
ˆDiﬀHierC
`
pB,T q,pC,Uq
˘
“DiﬀSysC
`
vAyS,ByTw
˘
ˆDiﬀSysC
`
vByT,CyUw
˘
λÝ ÑDiﬀSysC
`
vAyS,ByTwbv ByT,CyUw
˘
DiﬀSysCpcq
ÝÝÝÝÝÝÝÑDiﬀSysC
`
vAyS,CyUw
˘
“DiﬀHierC
`
pA,Sq,pC,Uq
˘
where λis the laxator of Proposition 3.20 and c is the external composition morphism of De/f_inition 3.12.
The monoidal structurepb,yqon DiﬀHierC is similarly de/f_ined following that ofHierT
C. On 0-cells, pA,Sqb
pA1,S1q :“ pAbA1,S bS1q. On 1-cells pA,Sq Ñ pB,T qand pA1,S1q Ñ pB1,T1q(and their 2-cells), the
tensor is given by the functors
DiﬀHierC
`
pA,Sq,pB,T q
˘
ˆDiﬀHierC
`
pA1,S1q,pB1,T1q
˘
“DiﬀSysC
`
vAyS,ByTw
˘
ˆDiﬀSysC
`
vA1yS1
,B1yT1
w
˘
λÝ ÑDiﬀSysC
`
vAyS,ByTwbv A1yS1
,B1yT1
w
˘
CoalgT
Cpdq
ÝÝÝÝÝÝÑDiﬀSysC
`
vAyS bA1yS1
,ByT bB1yT1
w
˘
“DiﬀHierC
`
pA,Sqbp A1,S1q,pB,T qbp B1,T1q
˘
where d is the distributive law of De/f_inition 3.13.
Following Prop. 3.16, we have the following characterization of a diﬀerential hierarchical inference system
pA,SqÑp B,T qin KℓpPq, for P : E ÑE.
Proposition 3.22. A 1-cell δ: pA,SqÑp B,T qin DiﬀHierKℓpPqis given by a tuple δ:“pX,δo
1,δo
2,δ#qof
• a choice of ‘state space’X : DiﬀE;
• a forwards output map δo
1 : XˆAÑPBin E,
• a backwards output map δo
2 : XˆPAˆT ÑPSin E,
• a stochastic vector /f_ieldδ# : XˆPAˆT ÑPTX in E.
We can obtain continuous-time dynamical systems from diﬀerential systems by integration, and consider
how to discretize these /f_lows to give discrete-time dynamical systems.
Proposition 3.23. Integration induces an indexed functor Flow : DiﬀSysC ÑCoalgR
C .
15
Proof. Suppose pM,mqis an object in DiﬀSysCppq. The morphism m : MyTM Ñ p consists of a map
m1 : M Ñpp1qin ComonpCqalong with a morphism m# : ř
x:M prm1pxqsÑ TM in C. Since, for any
section σ : p Ñy, the induced map mσ : M ÑTM is a vector /f_ield on a compact manifold, it generates a
unique global /f_lowFlowppqpmqσ : R ˆM ÑM [23, Thm.s 12.9, 12.12], which factors as
ÿ
t:R
M
m˚
1 σ
Ý ÝÝ Ñ
ÿ
t:R
ÿ
x:M
prm1pxqs
Flowppqpmqu
ÝÝÝÝÝÝÝÑM.
We therefore de/f_ine the systemFlowppqpmqto have state space M, output map m1 (for all t: R), and update
map Flowppqpmqu. Since Flowppqpmqσ is a /f_low for any sectionσ, it immediately satis/f_ies the monoid action
condition. On morphisms α : m Ñm1, we de/f_ineFlowppqpαqby the same underlying map on state spaces;
this is again well-de/f_ined by the condition thatαis compatible with the tangent structure. Given a morphism
ϕ: pÑqof polynomials, both the reindexing DiﬀSysCpϕqand CoalgR
C pϕqact by postcomposition, and so
it is easy to see that CoalgR
C pϕq˝ Flowppq– Flowpqq˝ DiﬀSysCpϕqnaturally.
Remark 3.24. From Proposition 3.23 and the earlier Corollary 3.6, we obtain a family of composite indexed
functors DiﬀSysC
FlowÝ ÝÝ ÑCoalgR
C
Disck
ÝÝÝÑCoalgN
C taking each diﬀerential system to a discrete-time dynamical
system inC. Below, we will de/f_ine approximate inference doctrines in discrete time that arise from processes of
(stochastic) gradient descent, and which therefore factor through diﬀerential systems, but the form in which
these are given—and in which they are found in the informal literature ( e.g., Bogacz [24])—is not obtained
via the composite Disck ˝Flow for any k, even though there is a free parameter k that plays the same role
(intuitively, a ‘learning rate’). Instead, one typically adopts the following ‘naïve’ discretization scheme.
Let CartDiﬀSysC denote the sub-indexed category of DiﬀSysC spanned by those systems with Carte-
sian state spaces Rn. Naive discretization induces a family of indexed functors Naivek : CartDiﬀSysC Ñ
CoalgN
C , for k : R, which we illustrate for a single system pRn,mqover a /f_ixed polynomialp, with m :
RnyRnˆRn
Ñp(since TRn –RnˆRn). This system is determined by a pair of morphisms m1 : Rn Ñpp1q
and m# : ř
x:Rn prm1pxqsÑ Rn ˆRn, and we can write the action of m# as px,yqÞÑp x,vxpyqq.
Using these, we de/f_ine a discrete-time dynamical systemβover pwith state spaceRn. This βis given by an
output map βo, which we de/f_ine to be equal tom1, βo :“m1, and an update mapβu : ř
x:Rn prβopxqsÑ Rn,
which we de/f_ine bypx,yqÞÑ x`kvxpyq. Together, these de/f_ine a system inCoalgN
C ppq, and the collection
of these systems βproduces an indexed functor by the de/f_initionNaivekppqpmq:“β.
By contrast, the discrete-time system obtained via Disck ˝Flow involves integrating a continuous-time
one for kunits of real time for each unit of discrete time: although this in general produces a more accurate
simulation of the trajectories implied by the vector /f_ield, it is computationally more arduous; to trade oﬀ
simulation accuracy against computational feasibility, one may choose a more sophisticated discretization
scheme than that sketched above, or at least choose a “suﬃciently small” timescale k.
Finally, we can use the foregoing ideas to translate diﬀerential hierarchical inference systems to dynamical
hierarchical inference systems.
Corollary 3.25. Let CartDiﬀHierC denote the restriction ofDiﬀHierC to hom-categories inCartDiﬀSysC.
The indexed functorsDisck : CoalgR
C ÑCoalgN
C , Flow : DiﬀSysC ÑCoalgR
C , andNaivek : CartDiﬀSysC Ñ
CoalgN
C induce functors (respectively) HDisck : HierR
C Ñ HierN
C , HFlow : DiﬀHierC Ñ HierR
C and
HNaivek : CartDiﬀHierC ÑHierN
C by change of base of enrichment.
4 Approximate inference doctrines
We are now in a position to build the bridge between abstract statistical models and the dynamical systems that
play them, with the categories of hierarchical dynamical systems developed in the previous section supplying
16
the semantics. These bridges will be functors, which we call approximate inference doctrines . In general,
they will be functors from categories of parameterized statistical models, whose parameters form part of the
dynamical state spaces, and often we are particularly interested in only a particular class of statistical models,
which typically form a subcategory of a broader category of stochastic channels. We therefore make the
following de/f_inition.
De/f_inition 4.1.Let D be a subcategory of PC. An approximate inference doctrine for D in time T is a functor
D ÑHierT
C.
Here, PC denotes the external parameterization of C, to the de/f_inition of which we now turn.
4.1 External parameterization
In the previous instalment of this series, we considered parameterized Bayesian lenses [1, §3.4] and statistical
games [1, Cor. 4.14, Ex. 5.5], in order to treat systems with the ability to improve their statistical performance.
Approximate inference doctrines operationalize this improvement, but in this context it is preferable to con-
sider statistical systems that are ‘externally’ rather than ‘internally’ parameterized: the improvement of the
performance is typically a process that is ‘external’ to the solution of the statistical problem (e.g., inference)
itself; for instance, learning is often assumed [25] to take place on a slower timescale than inference.
Technically, we can see this distinction by considering the type of an internally parameterized Bayesian
lens, following St. Clere Smithe [1, §3.4]. If pγ,ρq : pA,Sq
pΘ,Ωq
ÝÝÝÑ| pB,T qis such a lens, then its forward
channel γhas the type Θ bAÑ‚B, and the backwards channel ρhas the type CpI,Θ bAqÑ CpT,Ω bSq.
Notice that this means that in general the inversion ρdepends on a joint prior over Θ bA, and produces
an updated state over Ω bS, even though one is often interested only in a family of inversions of the type
CpI,Aq ÑCpT,Sqparameterized by Ω, with the updating of the parameters taking place in an external
process that ‘observes’ the performance of the statistical game. We make this distinction formal using the
notion of external parameterization.
De/f_inition 4.2.Given a category C enriched in pE,ˆ,1q, we de/f_ine theexternal parameterization PC of C
in E as the following bicategory. 0-cells are the objects of C, and each hom-category PCpA,Bqis given by
the slice category E{CpA,Bq. The composition of 1-cells is by composing in C after taking the product of
parameters: given f : Θ ÑCpA,Bqand g: Ω ÑCpB,Cq, their composite g˝f is
g˝f :“Ω ˆΘ
gˆf
ÝÝÑCpB,Cqˆ CpA,Bq ‚Ý ÑCpA,Cq
where ‚is the composition map for C in E. The identity 1-cells are the points on the identity morphisms in
C. For instance, the identity 1-cell on Ais the corresponding point idA : 1 ÑCpA,Aq. We will denote 1-cells
using our earlier notation for parameterized morphisms: for instance, f : A ΘÝ ÑB and idA : A 1Ý ÑA. The
horizontal composition of 2-cells is given by taking their product.
As an example, let us consider externally parameterized statistical games.
Example 4.3. The category PSGameC of externally parameterized statistical games inC has as 0-cells pairs
of objects in C (as in the case of Bayesian lenses or plain statistical games). Its 1-cells pA,Sq ΘÝ ÑpB,T qare
parameterized games, consisting in a choice of parameter spaceΘ, an externally parameterized lens f : Θ Ñ
BayesLensCppA,Sq,pB,T qq, and an externally parameterized loss function φ : ř
ϑ:Θ Ctxpfϑq ÑR. The
identity onpA,Sqis given by the trivially parameterized elementidpA,Sq : 1 ÑBayesLensCppA,Sq,pA,Sqq,
equipped with the zero loss function, as in the case of unparameterized statistical games. Given parameterized
17
games pf,φq: pA,Sq ΘÝ Ñ pB,T qand pg,ψq: pB,T q Θ1
ÝÑ pC,Uq, we form their composite as follows. The
composite parameterized lens is given by taking the product of the parameter spaces:
ΘˆΘ1 fˆg
ÝÝÑBayesLensC
`
pA,Sq,pB,T q
˘
ˆBayesLensC
`
pB,T q,pC,Uq
˘ Ý ÑBayesLensC
`
pA,Sq,pC,Uq
˘
The composite /f_itness function is given accordingly:
ÿ
ϑ:Θ,ϑ1:Θ1
Ctxpgϑ1 fϑqÝÑ
ÿ
ϑ,ϑ1
Ctxpgϑ1 fϑq2 pgϑ1˚,fϑ˚q
ÝÝÝÝÝÝÑ
ÿ
ϑ,ϑ1
Ctxpfϑqˆ Ctxpgϑ1q
pφϑ,ψϑ1q
ÝÝÝÝÝÑR ˆR `Ý ÑR
For concision, when we say parameterized statistical game or parameterized lens in the absence of further
quali/f_ication, we will henceforth mean the externally (as opposed to internally) parameterized versions.
Remark 4.4. In prior work, this external parameterization construction has been called ‘proxying’ [26]. We
prefer the more explicit name ‘external parameterization’, reserving ‘proxying’ for a slightly diﬀerent double-
categorical construction to appear in future work.
Remark 4.5. Before moving on to examples of approximate inference doctrines, let us note the similarity
of the notions of external parameterization, diﬀerential system, and dynamical system: both of the latter can
be considered as externally parameterized systems with extra structure, where the extra structure is a mor-
phism or family of morphisms back into (an algebra of) the parameterizing object: in the case of diﬀerential
systems, this ‘algebra’ is the tangent bundle; for dynamical systems, it is trivial; and forgetting this extra struc-
ture returns a mere external parameterization. Approximate inference doctrines are thus functorial ways of
equipping morphisms with this extra structure, and in this respect they are close to the current understanding
of general compositional game theory [20, 21].
4.2 Channels with Gaussian noise
Our motivating examples from the computational neuroscience literature are de/f_ined over a subcategory
of channels between Cartesian spaces with additive Gaussian noise [24, 25, 27]; typically one writes x ÞÑ
fpxq` ω for a deterministic map f : X Ñ Y and ω sampled from a Gaussian distribution over Y. This
choice is made, as we will see, because it permits some simplifying assumptions which mean the resulting
dynamical systems resemble known neural circuits. In this section, we develop the categorical language in
which we can express such Gaussian channels. We begin by introducing the category of probability spaces
and measure-preserving maps, which we then use to de/f_ine channels of the general form x ÞÑ fpxq` ω,
before restricting to the /f_inite-dimensional Gaussian case.
De/f_inition 4.6.Let P-Spc be the category Comon
`
1{KℓpPq
˘
of probability spaces pM,µqwith µ: 1Ñ‚M
in KℓpPq(i.e., 1 Ñ PM in E), and whose morphisms f : pM,µq Ñ pN,νqare measure-preserving maps
f : M ÑN (i.e., such that f ‚µ“νin KℓpPq).
We can think ofxÞÑfpxq`ωas a map parameterized by a noise source, and so to construct a category of
such channels, we can use theParaconstruction in its actegorical form. We will use the monoidal-actegorical
de/f_inition ofPara given in St. Clere Smithe [1, §2.3], following Capucci et al. [21]; for a comprehensive
reference on actegory theory, see Capucci and Gavranović [28]. The /f_irst step is to spell out the actegory
structure.
Proposition 4.7. Let P : E ÑE be a probability monad on the symmetric monoidal categorypE,ˆ,1q. Then
there is a P-Spc-actegory structure ˚ : P-Spc Ñ CatpE,Eqon E as follows. For each pM,µq : P-Spc,
de/f_inepM,µq˚p´q : E ÑE by pM,µq˚X :“MˆX. For each morphism f : pM,µqÑp M1,µ1qin P-Spc,
de/f_inef ˚X :“f ˆidX.
18
Proof sketch. The action on morphisms is well-de/f_ined because each morphismf : MÑ‚Nin Comon
`
1{KℓpPq
˘
corresponds to a map f : M ÑN in E; it is clearly functorial. The unitor and associator are inherited from
the Cartesian monoidal structure pˆ,1qon E.
The resulting Para bicategory, Parap˚q, can be thought of as a bicategory of maps each of which is
equipped with an independent noise source; the composition of maps takes the product of the noise sources,
and 2-cells are noise-source reparameterizations. The actegory structure ˚is symmetric monoidal, and the
1-categorical truncation Parap˚q1 [1, Prop. 2.47] is a copy-delete category [11, Def. 2.2] (also [1, Def. 2.20])
as we now sketch.
Proposition 4.8. Consider the actegory structure ˚of Proposition 4.7. Then Parap˚q1 is a copy-delete
category.
Proof sketch. The monoidal structure is de/f_ined following Proposition 2.44 of St. Clere Smithe [1]. We need
to de/f_ine a right costrengthρwith components pN,νq˚p X ˆYq „Ý ÑX ˆppN,νq˚ Yq. Since ˚is de/f_ined
by forgetting the probability structure and taking the product, the costrength is given by the associator and
symmetry in E:
pN,νq˚pXˆYq“ NˆpXˆYq „Ý ÑNˆpY ˆXq „Ý ÑpNˆYqˆX „Ý ÑXˆpNˆYq“ XˆppN,νq˚Yq
It is clear that this de/f_inition gives a natural isomorphism; the rest of the monoidal structure follows from
that of the product on E.
We now need to de/f_ine a symmetry natural isomorphismβX,Y : X ˆY „Ý ÑY ˆX in Parap˚q. This is
given by the symmetry of the product inE, under the embedding of E in Parap˚qthat takes every map to its
parameterization by the terminal probability space.
The rest of the copy-delete structure is inherited similarly from E.
If we think of KℓpPqas a canonical category of stochastic channels, for Parap˚q1 to be considered as a
subcategory of Gaussian channels, we need the following result.
Proposition 4.9. There is an identity-on-objects strict monoidal embedding ofParap˚q1 into KℓpPq. Given
a morphism f : X
pΩ,µq
ÝÝÝÑY in Parap˚q1, form the composite f ‚pµ,idXq: XÑ‚Y in KℓpPq.
Proof sketch. First, the given mapping preserves identities: the identity inParap˚qis trivially parameterized,
and is therefore taken to the identity in KℓpPq. The mapping also preserves composites, by the naturality of
the unitors of the symmetric monoidal structure onKℓpPq. That is, given f : X
pΩ,µq
ÝÝÝÑY and g: Y
pΘ,νq
ÝÝÝÑZ,
their composite g˝f : X
pΘbΩ,νbµq
ÝÝÝÝÝÝÝÑZis taken to
X„Ý Ñ‚1 b1 bXνbνbidX
Ý ÝÝÝÝÝ Ñ‚ Θ bΩ bX
g˝f
ÝÝÑ‚ Z
where here g ˝f is treated as a morphism in KℓpPq. Composing the images of g and f under the given
mapping gives
X„Ý Ñ‚1 bX
µbidX
Ý ÝÝÝ Ñ‚ Ω bX
f
Ý Ñ‚Y „Ý Ñ‚1 bY νbYÝ ÝÝ Ñ‚ Θ bY
g
Ý Ñ‚Z
which is equal to
X„Ý Ñ‚1 b1 bX
νbµbidX
Ý ÝÝÝÝÝ Ñ‚ Θ bΩ bX
idΘ bf
ÝÝÝÝÑ‚ Θ bY
g
Ý Ñ‚Z
which in turn is equal to the image of the composite above.
19
The given mapping is therefore functorial. To show that it is an embedding is to show that it is faithful and
injective on objects. Since Parap˚qand KℓpPqhave the same objects, the embedding is trivially identity-on-
objects (and hence injective); it is similarly easy to see that it is faithful, as distinct morphisms in Parap˚q
are mapped to distinct morphisms in KℓpPq.
Finally, since the embedding is identity-on-objects and the monoidal structure on Parap˚qis inherited
from that on KℓpPq(producing identical objects), the embedding is strict monoidal.
We now restrict our attention to Gaussian maps.
De/f_inition 4.10.We say that f : XÑ‚Y in KℓpPqis Gaussian if, for any x : X, the state fpxq : PY is
Gaussian7. Similarly, we say that f : X
pΩ,µq
ÝÝÝÑY in Parap˚qis Gaussian if its image under the embedding
Parap˚q1 ãÑKℓpPqis Gaussian. Given a category of stochastic channels C, write GausspCqfor the subcat-
egory generated by Gaussian morphisms and their composites inC. Given a separable Banach spaceX, write
GausspXqfor the space of Gaussian states on X.
Example 4.11. A class of examples of Gaussian morphisms in Parap˚qthat will be of interest to us in
section 4.4 is of the form xÞÑfpxq` ωfor some map f : X ÑY and ωdistributed according to a Gaussian
distribution over Y. Writing Erωsfor the mean of this distribution, the resulting channel in KℓpPqemits for
each x: X a Gaussian distribution with mean fpxq` Erωsand variance the same as that of ω.
Remark 4.12. In general, Gaussian morphisms are not closed under composition: pushing a Gaussian distri-
bution forward along a nonlinear transformation will not generally result in another Gaussian. For instance,
consider the Gaussian morphisms x ÞÑ fpxq` ω and y ÞÑ gpyq` ω1. Their composite in Parap˚qis the
morphism xÞÑg
`
fpxq`ωq
˘
`ω1; even if g
`
fpxq`ωq
˘
is Gaussian-distributed, the sum of two Gaussians is
in general not Gaussian, and sog
`
fpxq`ωq
˘
`ω1will not be Gaussian. This non-closure underlies the power
of statistical models such as the variational autoencoder, which are often constructed by pushing a Gaussian
forward along a learnt nonlinear transformation [29], in order to approximate an unknown distribution; since
sampling from Gaussians is relatively straightforward, this method of approximation can be computationally
tractable. The Gauss construction here is an abstraction of the Gaussian-preserving transformations invoked
by Shiebler [30], and is to be distinguished from the category Gauss introduced by Fritz [31], whose mor-
phisms are aﬃne transformations (which do preserve Gaussianness) and which are therefore closed under
composition; there is nonetheless an embedding of Fritz’sGauss into our Gauss
`
KℓpPq
˘
.
Proposition 4.13. Let FdCartSpcpEq denote the full subcategory of E spanned by /f_inite-dimensional
Cartesian spaces Rn, where n : N. Let P-FdCartSpc denote the corresponding subcategory of P-Spc.
Let ‹: P-FdCartSpc ÑCat
`
FdCartSpcpEq,FdCartSpcpEq
˘
be the corresponding restriction of the
monoidal action ˚: P-Spc ÑCatpE,Eqfrom Proposition 4.7. Then Parap‹qis a monoidal subbicategory
of Parap˚q.
We will write PFd : FdCartSpcpEq ÑFdCartSpcpEqto denote the restriction of the probability
monad P : E ÑE to FdCartSpcpEq.
Finally, we give the density function representation of Gaussian channels in KℓpPFdq.
Proposition 4.14. Every Gaussian channel c: XÑ‚Y in KℓpPFdqadmits a density function pc : Y ˆX Ñ
r0,1swith respect to the Lebesgue measure on Y. Moreover, since Y “ Rn for some n : N, this density
function is determined by two maps: the mean µc : X ÑRn, and the covariance Σc : X ÑRnˆn in E. We
call the pair pµu,Σcq: X ÑRn ˆRnˆn the statistical parameters for c.
7We admit Dirac delta distributions, and therefore deterministic channels, as Gaussian, since delta distributions can be seen as
Gaussians with in/f_inite precision.
20
Proof. The density function pc : Y ˆX Ñr0,1ssatis/f_ies
log pcpy|xq“ 1
2
A
ϵc,Σcpxq´1ϵc
E
´log
a
p2πqndet Σcpxq
where ϵc : Y ˆX ÑY : py,xqÞÑ y´µcpxq.
4.3 The Laplace doctrine
Our /f_irst example of a doctrine arises in the computational neuroscience literature, which has sought to explain
the apparently ‘predictive’ nature of sensory cortical circuits using ideas from the theory of approximate
inference [3]; the general name for this neuroscienti/f_ic theory ispredictive coding, and the task of a predictive
coding model is to de/f_ine a dynamical system whose structures and behaviours mimic those observed in neural
circuits in vivo . One way to satisfy this constraint is to describe a procedure that turns a statistical problem
into a dynamical system of a form known to be simulable by a neural circuit: that is to say, there are certain
classes of dynamical systems which are known to reproduce the phenomenology of neural circuits and which
are built out of parts that correspond to known biological structures, and so a “biologically plausible” model
of predictive coding should produce an instance of such a class, given a statistical problem.
This procedure pushes the ‘plausibility’ constraint back to the level of the statistical problem (since there
are presently no known neural circuit models that can solve any inference problem in general), and one
restriction that is usefully made is that all noise sources in the model are Gaussian. This restriction allows us
to make an approximation, known as theLaplace approximation, to the loss function of an autoencoder game
which in turn entails that performing stochastic gradient descent on this loss function (with respect to the
mean of the posterior distribution) generates a dynamical system that is biologically plausible (up to some
level of biological plausibility) [3, 24].
In this section, we begin by de/f_ining the Laplace approximation and the resulting dynamical system, and
go on to show both how it arises and how the procedure is functorial: that is, we show that it constitutes an
approximate inference doctrine, and describe how this presentation clari/f_ies the role of what has been called
the “mean /f_ield” assumption in earlier literature [27]. (We leave the study of the biological plausibility of
compositional dynamical systems for future work.)
Lemma 4.15 (Laplace approximation). Suppose:
1. pγ,ρ,φ q : pX,Xq Ñ pY,Y qis a simple DKL-autoencoder game with Gaussian channels between
/f_inite-dimensional Cartesian spaces;
2. for all priors π: GausspXq, the statistical parameters ofρπ : Y ÑPXare denoted pµρπ,Σρπq: Y Ñ
R|X|ˆR|X|ˆ|X|, where |X|is the dimension of X; and
3. for all y: Y, the eigenvalues of Σρπpyqare small.
Then the loss function φ: Ctxpγ,ρqÑ R can be approximated by
φpπ,kq“ E
y„Lπ|γ|kM
“
Fpyq
‰
« E
y„Lπ|γ|kM
“
FLpyq
‰
where
FLpyq“ Epπ,γqpµρπpyq,yq´ SX rρπpyqs (1)
“´ log pγpy|µρπpyqq´ log pπpµρπpyqq´ SX rρπpyqs
where Sxrρπpyqs “Ex„ρπpyqr´log pρπpx|yqsis the Shannon entropy of ρπpyq, and pγ : Y ˆX Ñ r0,1s,
pπ : X Ñ r0,1s, and pρπ : X ˆY Ñ r0,1sare density functions for γ, π, and ρπ respectively. The
approximation is valid when Σρπ satis/f_ies
Σρπpyq“
`
B2
xEpπ,γq
˘
pµρπpyq,yq´1 . (2)
21
We call FL the Laplacian free energy and Epπ,γqthe corresponding Laplacian energy .
Proof. Following Proposition 4.14, we can write the density functions as:
log pγpy|xq“ 1
2
@
ϵγ,Σγ´1ϵγ
D
´log
b
p2πq|Y|det Σγ
log pρπpx|yq“ 1
2
@
ϵρπ,Σρπ
´1ϵρπ
D
´log
b
p2πq|X|det Σρπ (3)
log pπpxq“ 1
2
@
ϵπ,Σπ´1ϵπ
D
´log
b
p2πq|X|det Σπ
where for clarity we have omitted the dependence of Σγ on xand Σρπ on y, and where
ϵγ : Y ˆX ÑY : py,xqÞÑ y´µγpxq,
ϵρπ : XˆY ÑX : px,yqÞÑ x´µρπpyq, (4)
ϵπ : Xˆ1 ÑX : px,˚qÞÑ x´µπ.
Then, recall from [1, Remark 5.12] that we can write the free energyFpyqas the diﬀerence between expected
energy and entropy:
Fpyq“ E
x„ρπpyq
„
log pρπpx|yq
pγpy|xq¨ pπpxq

“ E
x„ρπpyq
r´log pγpy|xq´ log pπpxqs´ SX rρπpyqs
“ E
x„ρπpyq
“
Epπ,γqpx,yq
‰
´SX rρπpyqs
Next, since the eigenvalues of Σρπpyqare small for all y : Y, we can approximate the expected energy by its
second-order Taylor expansion around the mean µρπpyq:
Fpyq« Epπ,γqpµρπpyq,yq` 1
2
@
ϵρπ pµρπpyq,yq,
`
B2
xEpπ,γq
˘
pµρπpyq,yq¨ ϵρπ pµρπpyq,yq
D
´SX
“
ρπpyq
‰
.
where
`
B2
xEpπ,γq
˘
pµρπpyq,yqis the Hessian of Epπ,γqwith respect to xevaluated at pµρπpyq,yq.
Note that
@
ϵρπ pµρπpyq,yq,
`
B2
xEpπ,γq
˘
pµρπpyq,yq¨ ϵρπ pµρπpyq,yq
D
“tr
“`
B2
xEpπ,γq
˘
pµρπpyq,yqΣρπpyq
‰
, (5)
that the entropy of a Gaussian measure depends only on its covariance,
SX
“
ρπpyq
‰
“1
2 log detp2πe Σρπpyqq,
and that the energy Epπ,γqpµρπpyq,yqdoes not depend on Σρπpyq. We can therefore write down directly the
covariance Σ˚
ρπpyqminimizing Fpyqas a function of y. We have
BΣρπFpyq« 1
2
`
B2
xEpπ,γq
˘
pµρπpyq,yq` 1
2Σρπ
´1 .
Setting BΣρπFpyq“ 0, we /f_ind the optimum as expressed by equation (2)
Σ˚
ρπpyq“
`
B2
xEpπ,γq
˘
pµρπpyq,yq´1 .
Finally, on substituting Σ˚
ρπpyqin equation (5), we obtain the desired expression of equation (1)
Fpyq« Epπ,γqpµρπpyq,yq´ SX rρπpyqs“: FLpyq.
22
Remark 4.16. The terms ϵγ : Y ˆX ÑY (&c.) of eq. (4) are known as error functions, since they encode the
diﬀerence between y : Y and the expected element µγpxq: Y given x: X. In applications, one often thinks
of these errors as prediction errors, interpreting µγ as the system’s prediction of the expected state ofY.
In this context one then also de/f_ines theprecision-weighted errors
ηγpy,xq:“Σγpxq´1ϵγpy,xq: Y ˆX ÑY , (6)
noting that the inverse covariance matrix Σγpxq´1 can be interpreted as encoding the ‘precision’ of a belief:
roughly speaking, low variance (or ‘diﬀusivity’) means high precision 8. The log-densities of eq. (4.15) are
then understood as measuring the precision-weighted length of the error vectors.
De/f_inition 4.17.Suppose γ : XÑ‚Y is a Gaussian channel inKℓpPq. Then the discrete-time Laplace doctrine
de/f_ines a systemLpγq : pX,Xq Ñ pY,Y qin HierN
GausspKℓpPFdqq as follows (using the representation of
Proposition 3.16).
• The state space is X;
• the forwards output map Lpγqo
1 : XˆX ÑGausspYqis given by γ:
Lpγqo
1 :“XˆX
proj2
Ý ÝÝ ÑX
γ
Ý ÑGausspYq
• the backwards output map Lpγqo
2 : XˆGausspXqˆ Y ÑGausspXqis given by:
Lpγqo
2 : XˆGausspXqˆ Y ÑR|X|ˆR|X|ˆ|X| ãÑGausspXq
px,π,y qÞÑ
`
x,Σρpx,π,y q
˘ (7)
where the inclusion picks the Gaussian state with the given statistical parameters, whose covariance
Σρpx,π,y q:“
`
B2
xEpπ,γq
˘
px,yq´1 is de/f_ined following equation (2) (Lemma 4.15);
• the update mapLpγqu : XˆGausspXqˆY ÑGausspXqreturns a point distribution on the updated
mean
Lpγqu : XˆGausspXqˆ Y ÑGausspXq
px,π,y qÞÑ ηP
X
`
µρpx,π,y q
˘
where ηP
X : X ÑGausspXqdenotes the unit of the monad P and µρ is de/f_ined by
µρpx,π,y q:“x`λBxµγpxqTηγpy,xq´ ληπpxq.
Here, the precision-weighted error terms η are as in equation (6) (Remark 4.16), and λ : R` is some
choice of ‘learning rate’.
Remark 4.18. Note that the update mapLpgqu as de/f_ined here is actually deterministic, in the sense that it is
de/f_ined as a deterministic map followed by the unit of the probability monad. However, the general stochastic
setting is necessary, because the composition of system depends on the composition of Bayesian lenses, which
is necessarily stochastic.
De/f_inition 4.19.A Laplacian statistical game is a parameterized statistical gamepγ,ρ,φ q: pX,Xq XÝ ÑpY,Y q
satisfying the following conditions:
1. X and Y are /f_inite-dimensional Cartesian spaces;
8Consider the one-dimensional case: as the variance σof a normal distribution tends to 0, the distribution approaches a Dirac delta
distribution, which is “in/f_intely precise”.
23
2. the forward channel γis an unparameterized Gaussian channel;
3. the backward channel ρis parameterized by Xand de/f_ined as the backwards output map of the Laplace
doctrine (equation (7) of De/f_inition 4.17); that is,
ρ: XˆGausspXqˆ Y ÑR|X|ˆR|X|ˆ|X| ãÑGausspXq
px,π,y qÞÑ
`
x,Σρpx,π,y q
˘
where the inclusion picks the Gaussian with mean xand Σρpx,π,y q“
`
B2
xEpπ,γq
˘
px,yq´1;
4. the loss function φ: ř
x:X Ctx
`
γ,ρx
˘
ÑR is given for eachx: Xby φxpπ,kq“ Ey„Lπ|γ|kM
“
FLpyq
‰
,
where FL is the Laplacian free energy
FLpyq“ Epπ,γqpx,yq´ SX
“
ρpx,π,y q
‰
“´ log pγpy|xq´ log pπpxq´ SX
“
ρpx,π,y q
‰
as de/f_ined in equation (1) of Lemma 4.15.
(By “unparameterized channel”, we mean a channel parameterized by the trivial space1; the pair pγ,ρqcon-
stitutes a parameterized Bayesian lens with parameter space X, where the choice of γ simply forgets the
parameter, discarding it along the universal map X Ñ1.)
Proposition 4.20. Given a Laplacian statistical game pγ,ρ,φ q : pX,Xq Ñ pY,Y q, Lpγqis obtained by
stochastic gradient descent of the loss function φwith respect to the mean xof the posterior ρpx,π,y q.
Proof. We have φxpπ,kq“ Ey„Lπ|γ|kM
“
FLpyq
‰
, where
FLpyq“´ log pγpy|xq´ log pπpxq´ SX
“
ρpx,π,y q
‰
.
Since the entropy SX rρπpyqsdepends only on the variance Σρpx,π,y q, to optimize the mean xit suﬃces
to consider only the energy Epπ,γqpx,yq. We have
Epπ,γqpx,yq“´ log pγpy|xq´ log pπpxq
“´1
2
A
ϵγpy,xq,Σγpxq´1ϵγpy,xq
E
´1
2
@
ϵπpxq,Σπ´1ϵπpxq
D
`log
b
p2πq|Y|det Σγpxq` log
b
p2πq|X|det Σπ
and a straightforward computation shows that
BxEpπ,γqpx,yq“´B xµγpxqTΣγpxq´1ϵγpy,xq` Σπ´1ϵπpxq.
We can therefore rewrite the mean parameter µρpx,π,y qemitted by the update map Lpγqu as
µρpx,π,y q“ x`λBxµγpxqTηγpy,xq´ ληπpxq
“x´λBxEpπ,γqpx,yq
“x´λBxFLpyq
where the last equality holds because the entropy does not depend onx. This shows that Lpγqu descends the
gradient of the Laplacian energy with respect to x.
To see then thatLpγqu performs stochastic gradient descent ofφ, note that in the dynamical semantics, the
input y : Y is supplied by the context. In HierT
GausspKℓpPFdqq, the dynamics in the context are stochastic,
24
meaning that each y : Y is in general sampled from a random variable valued in Y. If we /f_ix the context to
sample yfrom Lπ|γ|kMthen, for a given x: X, the expected trajectory of µρ is given by
E
y„Lπ|γ|kM
“
µρpx,π,y q
‰
“ E
y„Lπ|γ|kM
“
x´λBxFLpyq
‰
“x´λBx E
y„Lπ|γ|kM
“
FLpyq
‰
by linearity of expectation
“x´λBxφxpπ,kq.
Since Lπ|γ|kMis just a placeholder for the random variable from which y is sampled, this establishes the
result.
Using the preceding proposition, we obtain the following theorem, expressing the Laplacian statistical
games in the image of an approximate inference doctrine.
Theorem 4.21. Let G denote the subcategory of PSGameKℓpPFdqgenerated by Laplacian statistical games
pγ,ρ,φ q: pX,Xq XÝ ÑpY,Y qand by the structure morphisms of a monoidal category.
Then L extends to a strict monoidal functor GausspKℓpPFdqq ãÑ G Ñ HierN
GausspKℓpPFdqq, where the
/f_irst factor is the embedding taking any suchγ to the corresponding Laplacian game, and the second factor
performs stochastic gradient descent of loss functions with respect to their external parameterization.
It helps to separate the proof of the theorem from the proof of the following lemma.
Lemma 4.22. There is an identity-on-objects strict monoidal embedding of GausspKℓpPFdqqinto G.
Proof. The structure morphisms of GausspKℓpPFdqqare mapped to the (trivially parameterized) structure
morphisms of G, and any Gaussian channel γ : XÑ‚Y is mapped to the unique Laplacian statistical game
with γas the (unparameterized) forward channel, and the (parameterized) backward channel and loss function
determined by the de/f_inition of Laplacian statistical game. It is clear that this de/f_inition gives a faithful functor,
and thus an embedding. Since it preserves explicitly the monoidal structure, it is also strict monoidal.
Proof of Theorem 4.21. Thanks to Lemma 4.22, we now turn to the functor G ÑHierN
GausspKℓpPFdqq, which
we will also denote by L; the composite functor is obtained by pulling this functor G ÑHierN
GausspKℓpPFdqq
back along the embedding GausspKℓpPFdqqãÑG.
Suppose then that g :“ pγ,ρ,φ q : pX,Xq XÝ Ñ pY,Y qis a Laplacian statistical game. Proposition 4.20
tells us that Lpgqis obtained by stochastic gradient descent of the loss function φwith respect to the mean
parameter of the backwards channel ρ. By de/f_inition ofρ, this mean parameter is given precisely by the
external parameterization, and so we have that Lpgqis obtained by stochastic gradient descent of φ with
respect to this parameterization.
To extend L to a functor accordingly, we need to check that performing stochastic gradient descent with
respect to the external parameterization preserves identities and composition. First we note that, following
De/f_inition 4.17, the dynamical systems in the image ofL emit lenses by /f_illing in the parameterization with
the dynamical state, and by the preceding remarks, update the state by stochastic gradient descent. Next,
note that identity parameterized lenses are trivially parameterized, so there is no parameter to ‘/f_ill in’, and no
state to update; similarly, the loss function of an identity game is the constant function on 0, and therefore
has zero gradient. On identity games pX,Xq 1Ý Ñ pX,Xq, therefore, L returns the system with trivial state
space 1 that constantly outputs the identity lens pX,Xq ÞÑpX,Xq: but this is just the identity on pX,Xqin
HierN
GausspKℓpPFdqq, so L preserves identities.
25
We now consider composites. Suppose h :“ pδ,σ,ψ q : pY,Y q YÝ Ñ pZ,Zqis another Laplacian game
satisfying the hypotheses of the theorem. Since HierN
GausspKℓpPFdqq is a bicategory, we need to show that
Lphq˝ Lpgq– Lph˝gq. In fact, we will show the stronger result that Lphq˝ Lpgq“ Lph˝gq, which means
demonstrating equalities between the state spaces, output maps, and update maps of the systems on the left-
and right-hand sides.
On state spaces, the equality obtains since the composition of externally parameterized games (Example
4.3) returns a game whose parameter space is the product of the parameter spaces of the factors. Similarly,
composition of systems in HierN
GausspKℓpPFdqq (after De/f_inition 3.15) returns a system whose state space is
the product of the state spaces of the factors. Finally, L acts by taking parameter spaces to state spaces, and
we have XˆY “XˆY.
Next, we note that the output of a composite system in HierN
GausspKℓpPFdqq is given by composing the
outputs of the factors. This is the same as the output returned byL on a composite game, since outputs in the
image of L just /f_ill in the external parameter using the dynamical state. Therefore
`
Lphq˝Lpgq
˘o “Lph˝gqo.
We now consider the update maps, beginning by computingLph˝gqu. The state space is XˆY and h˝g
has type pX,Xq XˆYÝÝÝÑpZ,Zq, so Lph˝gqu has type XˆY ˆGausspXqˆZ ÑGausspXˆYq. Following
Example 4.3, the composite loss function pψφq: ř
µρ:X,µσ:Y Ctxphµσ gµρqÑ R is given by:
pψφqpµρ,µσ,π,k q“ E
y„σpµσqγ‚πX‚Lπ|γ|δ˚kM
“
FL`
ρpµρqπX,γ; πX,y
˘‰
` E
z„LpMbγq‚π|δ|kM
“
FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘‰
Here, µρ and µσ are the parameters in Xand Y, respectively, and we writegµρ and hµσ to indicate the corre-
sponding lenses with those parameters. The context ispπ,kq, with π: 1Ñ‚MbXin GausspKℓpPqqand πX
denoting its Xmarginal, and with continuationk: GausspKℓpPqqp1,M bZqÑ GausspKℓpPqqp1,N bZq,
for some choices of residual objectsM and N. The backwards channels ρand σare externally parameterized
and state-dependent, so that ρpµρqπX : YÑ‚X is returned by ρpµρqat πX. Explicitly, ρhas the type X Ñ
E
`
GausspXq,GausspKℓpPqqpY,Xq
˘
, and σhas the type Y ÑE
`
GausspYq,GausspKℓpPqqpZ,Y q
˘
. Fi-
nally, δ˚kis the function
GausspKℓpPqqp1,M bYq
GausspKℓpPqqp1,Mbδq
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑGausspKℓpPqqp1,M bZq kÝ ÑGausspKℓpPqqp1,N bZq
obtained by pulling back kalong δ.
We therefore have Lπ|γ|δ˚kM“LpM bγq‚ π|δ|kM, meaning that we can rewrite the loss function as
E
z„Lπ|γ|δ˚kM
«
FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘
` E
y„σpµσqγ‚πXpzq
“
FL`
ρpµρqπX,γ; πX,y
˘‰
ﬀ
.
In the dynamical semantics for stochastic gradient descent, z and πX are supplied by the inputs to the dy-
namical system: the inputs replace the context for the game. Rewriting the loss accordingly gives a function
f : pz,πX,µρ,µσqÞÑ FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘
` E
y„σpµσqγ‚πXpzq
“
FL`
ρpµρqπX,γ; πX,y
˘‰
.
Next, we compute Bpµρ,µσqfpz,πXq. We obtain
Bpµρ,µσqfpz,πXq“
˜
Bµρ E
y„σpµσqγ‚πXpzq
“
FL`
ρpµρqπX,γ; πX,y
˘‰
, Bµσ FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘
¸
“
˜
E
y„σpµσqγ‚πXpzq
“
BµρFL`
ρpµρqπX,γ; πX,y
˘‰
, Bµσ FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘
¸
.
26
Now, Lph˝gqu is de/f_ined as returning the point distribution onpµρ,µσq´ λBpµρ,µσqfpz,πXq:
pµρ,µσq´ λBpµρ,µσqfpz,πXq
“
˜
E
y„σpµσqγ‚πXpzq
“
µρ ´λBµρFL`
ρpµρqπX,γ; πX,y
˘‰
, µσ ´λBµσ FL`
σpµσqγ‚πX,δ; γ‚πX,z
˘
¸
.
We can simplify this expression by making some auxiliary de/f_initions
ρupa,π,y q:“a´λBaFL`
ρpaqπ,γ; π,y
˘
σupb,π1,zq:“b´λBbFL`
σpbqπ1,δ; π1,z
˘
so that
pµρ,µσq´ λBpµρ,µσqfpz,πXq“
˜
E
y„σpµσqγ‚πXpzq
rρupµρ,πX,yqs, σupµσ,γ ‚πX,zq
¸
. (8)
By currying ρupµρ,πX,yqinto a function ρupµρ,πXq: Y ÑPX, we can simplify this still further, since
E
y„σpµσqγ‚πXpzq
rρupµρ,πX,yqs“ ρupµρ,πXq‚ σpµσqγ‚πXpzq.
Since equation (8) de/f_inesLph˝gqu, we have
Lph˝gqupµρ,µσ,π,z q“ ηP
XˆY
`
ρupµρ,πq‚ σpµσqγ‚πpzq, σupµσ,γ ‚π,zq
˘
(9)
where ηP
XˆY : X ˆY ÑGausspX ˆYqis the component of the unit of the monad P at X ˆY, which
takes values in Dirac delta distributions and is therefore Gaussian.
Next, we compute the update map of the system Lphq˝Lpgq, using De/f_initions 3.12 and 3.15 (which de/f_ine
composition in HierN
GausspKℓpPFdqq). This update map is given by composing the ‘double strength’ 9 dst :
GausspXqˆ GausspYqÑ GausspXˆYqafter the following string diagram:
γ˚
σ5
Lpgqu
Lphqu
GausspXq
Y
X
Z GausspYq
GausspXq
(10)
9The double strength is also known as the ‘commutativity’ of the monadP with the product ˆ. It says that a pair of distributions
πon X and χon Y can also be thought of as a joint distribution pπ,χqon XˆY. It is Gaussian on Gaussians, as the product
of two Gaussians is again Gaussian.
27
Here, σ5 denotes the uncurrying of the parameterized state-dependent channel σ : Y ÑStatpYqpZ,Y q: we
can equivalently write the type of σas Y ÑE
`
GausspYq,GausspKℓpPqqpZ,Y q
˘
, which we can uncurry
twice to give the type Y ˆGausspYqˆ Z ÑGausspYq.
Observe now that we can write Lpgqu and Lphqu as
Lpgqupa,π,y q“ ηP
X
`
ρupa,π,y q
˘
Lphqupb,π1,zq“ ηP
Y
`
σupb,π1,zq
˘
and that ηP
XˆY “dstpηP
X,ηP
Yq. Reading the string diagram and applying this equality, we /f_ind that it repre-
sents
`
Lphq˝ Lpgq
˘upµρ,µσ,π,z qas
ηP
XˆY
`
ρupµρ,πq‚ σpµσqγ‚πpzq, σupµσ,γ ‚π,zq
˘
which is precisely the same as the de/f_inition ofLph˝gqu in equation (9).
Therefore, as required,
`
Lphq˝ Lpgq
˘u “Lph˝gqu.
Finally, because the functorL is identity-on-objects, the unit and multiplication of its monoidal structure are
easily seen to be given by identity morphisms, and soL is strict monoidal: L maps the structure morphisms to
constant dynamical systems emitting the structure morphisms ofHierN
GausspKℓpPFdqq, and so the associativity
and unitality conditions are satis/f_ied.
Remark 4.23. From the diagram (10), we can re/f_ine our understanding of what is known in the literature as
the mean /f_ieldapproximation [27, around eq.39], in which the posterior overXbY is assumed at each instant
of time to have independent marginals. We note that, even though the backwards output maps emit posterior
distributions with means determined entirely by their local parameterization, and even though these parame-
ters are updated by the tensorLpgqubLphqu, the resulting dynamical states are correlated across time by the
composition rule: this is made very clear by the wiring of diagram (10), since both factors Lpgqu and Lphqu
have common inputs. We also note that, even if the means of the emitted posteriors are entirely parameter-
determined, this is not true of their covariances, which are functions of both the prior and the observation.
The operational result of these observations is that the functorial (and pictorial) approach advocated here (as
opposed to writing down a complete, and complex, joint distribution for each model of interest and proceed-
ing from there) helps us understand the structural properties of complex systems—where it is otherwise easy
to get lost in the weeds.
Remark 4.24. Above we exhibited the Laplace doctrine directly as a functor
GausspKℓpPFdqqãÑG ÑHierN
GausspKℓpPFdqq.
In fact, Proposition 4.20 implies that it factors further, as
GausspKℓpPFdqqãÑG ∇Ý ÑDiﬀHierGausspKℓpPFdqq
HNaivek
ÝÝÝÝÝÑHierN
GausspKℓpPFdqq
where ∇ : G Ñ DiﬀHierGausspKℓpPFdqq takes an externally parameterized statistical game and returns a
diﬀerential system that performs gradient descent on its loss function with respect to its parameterization.
We leave the precise exhibition of this factorisation for future work.
4.4 The Hebb-Laplace doctrine
The Laplace doctrine constructs dynamical systems that produce progressively better posterior approxima-
tions given a /f_ixed forwards channel, but natural adaptive systems do more than this: they also re/f_ine the
28
forwards channels themselves, in order to produce better predictions. In doing so, these systems better re-
alize the abstract nature of autoencoder games, for which improving performance means improving both
prediction as well as inversion. To be able to improve the forwards channel requires allowing some freedom
in its choice, which means giving it a nontrivial parameterization.
The Hebb-Laplace doctrine that we introduce in this section therefore modi/f_ies the Laplace doctrine by
/f_ixing a class of parameterized forwards channels and performing stochastic gradient descent with respect
to both these parameters as well as the posterior means; we call it the Hebb-Laplace doctrine as the particu-
lar choice of forwards channels results in their parameter-updates resembling the ‘local’ Hebbian plasticity
known from neuroscience, in which the strength of the connection between two neurons is adjusted accord-
ing to their correlation. (Here, we could think of the ‘neurons’ as encoding the level of activity along a basis
vector.)
We begin by de/f_ining the category of these parameterized forwards channels, after which we introduce
Hebbian-Laplacian games and the resulting Hebb-Laplace doctrine, which is derived similarly to the Laplace
doctrine above. Recall from De/f_inition 4.2 that we writePC to denote the external parameterization of C in
its base of enrichment E.
De/f_inition 4.25.Let H denote the subcategory ofPGausspParap‹qqgenerated by the structure morphisms
of the symmetric monoidal category GausspParap‹qq(trivially parameterized), and by morphisms X ÑY
of the form (written in E)
ΘX ÑGausspParap‹qqpX,Y q
θ ÞÑ
´
xÞÑθhpxq` ω
¯
where his a diﬀerentiable map X ÑY, ΘX is the vector space of square matrices on X, and ωis sampled
from a Gaussian distribution on Y.
Note that there is a canonical embedding of PGausspParap‹qqinto PKℓpPFdq, obtained in the image of
Proposition 4.9 under the external parameterization P.
De/f_inition 4.26.A Hebbian-Laplacian statistical game is a parameterized statistical gamepγ,ρ,φ q: pX,Xq ΘXˆXÝÝÝÝÑ
pY,Y qsatisfying the following conditions:
1. X and Y are /f_inite-dimensional Cartesian spaces;
2. the forward channel γis a morphism in H (i.e., of the form xÞÑθhpxq` ω);
3. the backward channel is as for a Laplacian statistical game (De/f_inition 4.19);
4. the loss function is as for a Laplacian statistical game, with the substitution γ ÞÑγpθqfor parameter
θ: ΘX.
We will write GH to denote the subcategory of PSGame generated by Hebbian-Laplacian statistical games
and by the structure morphisms of a monoidal category.
De/f_inition 4.27.Suppose γ : X ÑY is a morphism in H. Then the discrete-time Hebb-Laplace doctrine
de/f_ines a systemHpγq : pX,Xq Ñ pY,Y qin HierN
GausspKℓpPFdqq as follows (using the representation of
Proposition 3.16).
• The state space is ΘX ˆX (where ΘX is again the vector space of square matrices on X);
• the forwards output map Hpγqo
1 : ΘX ˆXˆX ÑGausspYqis given by γ:
Hpγqo
1 :“ΘX ˆXˆX
proj1,3
Ý ÝÝÝ ÑΘX ˆX
γ5
Ý ÑGausspYq
where γ5 is the uncurried form of the morphism γ : ΘX ÑGausspParap‹qqpX,Y qin the image of
the embedding of H in PKℓpPq;
29
• the backwards output map Hpγqo
2 : ΘX ˆXˆGausspXqˆ Y ÑGausspXqis given by:
Hpγqo
2 : ΘX ˆXˆGausspXqˆ Y ÑR|X|ˆR|X|ˆ|X| ãÑGausspXq
pθ,x,π,y qÞÑ
`
x,Σρpθ,x,π,y q
˘
where the inclusion picks the Gaussian state with the given statistical parameters, whose covariance
Σρpθ,x,π,y q:“
`
B2
xEpπ,γpθqq
˘
px,yq´1 is de/f_ined following equation (2) (Lemma 4.15);
• the update map Hpγqu : ΘX ˆXˆGausspXqˆY ÑGausspΘX ˆXqoptimizes the parameter for
γas well as the mean of the posterior (as in the Laplace doctrine):
Hpγqu : ΘX ˆXˆPXˆY ÑPpΘX ˆXq
pθ,x,π,y qÞÑ ηP
ΘXˆX
`
θupθ,x,y q,µρpθ,x,π,y q
˘
where ηP denotes the unit of the monad P, and θu and µρ are de/f_ined by
θupθ,x,y q:“θ´λθηγpθqpy,xqhpxqT
µρpθ,x,π,y q:“x`λρBxhpxqTθTηγpθqpy,xq´ λρηπpxq.
Here, λθ,λρ : R` are chosen learning rates, and the precision-weighted error terms ηare again as in
equation (6) (Remark 4.16).
Remark 4.28. The ‘Hebbian’ part of the Hebb-Laplace doctrine enters in the forwards-parameter update
map, θupθ,x,y q“ θ´λθηγpθqpy,xqhpxqT, since the change in parameters is proportional to something re-
sembling the correlation between ‘pre-synaptic’ and ‘post-synaptic’ activity. Here, the post-synaptic activity
is represented by the termhpxq: we may think of the components of the vectorxas each representing the “in-
ternal activity” of a single neuron, and the “activation function”has returning the corresponding /f_iring rates;
these are ‘post-synaptic’ as the /f_iring is emitted down a neuron’s axon, which occurs computationally ‘after’
the neuron’s synaptic inputs. The synaptic inputs (generating the pre-synaptic activity) are then thought to
be represented by the error termηγpθqpy,xq, so that expected trajectory of the outer productηγpθqpy,xqhpxqT
computes the correlation between pre- and post-synaptic acivity.
Note that this means that typically one assumes that λθ ă λρ, because the neural activity xitself must
change on a faster timescale than the synaptic weights θ, in order for θto learn these correlations.
Given the foregoing de/f_inition, we obtain the following theorem.
Theorem 4.29. The Hebb-Laplace doctrine H de/f_ines an identity-on-objects strict monoidal functorH ãÑ
GH ÑHierN
GausspKℓpPFdqq.
This theorem follows in the same way as the corresponding result for the Laplace doctrine; and so we begin
with a small lemma, and subsequently show that the doctrine arises by stochastic gradient descent, before
putting the pieces together to prove the theorem itself.
Lemma 4.30. There is an identity-on-objects strict monoidal embedding H ãÑGH.
Proof sketch. The proof proceeds much as the proof of Lemma 4.22, except that the forwards channels of
games in the image of the embedding are given by the parameterized morphisms of H.
Proposition 4.31. Given a Hebbian-Laplacian statistical game pγ,ρ,φ q : pX,Xq ΘXˆXÝÝÝÝÑ pY,Y q, Hpγqis
obtained by stochastic gradient descent of the loss functionφwith respect to the weight matrixθ: ΘX of the
channel γand the mean x: X of the posterior ρ.
30
Proof. The proof proceeds much as the proof of Proposition 4.20, except now the forwards channel γ is pa-
rameterized: this gives us another factor against which to perform gradient descent, and furthermore means
that γpθqmust be substituted for γin expressions in the derivation of µρ.
The /f_irst such expression is the de/f_inition of the loss functionφ: ř
pθ,xq:ΘXˆX Ctx
`
γpθq,ρpxq
˘
ÑR; we
will write φpθ,xqfor the component ofφat pθ,xqwith the corresponding typeCtx
`
γpθq,ρpxq
˘
ÑR. We have
φpθ,xqpπ,kq“ Ey„Lπ|γpθq|kM
“
FLpyq
‰
, where now
FLpyq“ ´log pγpθqpy|xq´ log pπpxq´ SX
“
ρpx,π,y q
‰
.
We /f_ind
BxFLpyq“B xEpπ,γpθqq
“ ´BxµγpθqpxqTΣγpθqpxq´1ϵγpθqpy,xq` Σπ´1ϵπpxq
“ ´BxhpxqTθTηγpθqpy,xq` ηπpxq
and
BθFLpyq“B θEpπ,γpθqq
“ ´Bθ
2
A
ϵγpθqpy,xq,Σγpθqpxq´1ϵγpθqpy,xq
E
“ ´Bθ
2
A
y´θhpxq,Σγpθqpxq´1`
y´θhpxq
˘E
“Σγpθqpxq´1`
y´θhpxq
˘
hpxqT
“Σγpθqpxq´1ϵγpθqpy,xqhpxqT
“ηγpθqpy,xqhpxqT .
Consequently, we have
µρpθ,x,π,y q“ x`λρBxhpxqTθTηγpθqpy,xq´ λρηπpxq
“x´λρBxFLpyq
and
θupθ,x,y q“ θ´λθηγpθqpy,xqhpxqT
“θ´λθBθFLpyq,
and this means that we can write
Hpγqupθ,x,π,y q“ ηP
ΘXˆX ˝
´
pθ,xq´p λθ,λρqBpθ,xqFLpyq
¯
“ηP
ΘXˆX ˝
´
p´λBpFLpyq
¯
where p :“pθ,xqand λ :“pλθ,λρq, which establishes that Hpγqu descends the gradient of the free energy
with respect to the parameterization p.
Finally, with ysampled from a /f_ixed context, we can see that the expected trajectory ofHpγqfollows
E
y„Lπ|γpθq|kM
´
p´λBpFLpyq
¯
“
´
p´λBp E
y„Lπ|γpθq|kM
“
FLpyq
‰¯
“
´
p´λBpφppπ,kq
¯
which demonstrates that Hpγqperforms stochastic gradient descent of the loss function.
31
Proof of Theorem 4.29. Lemma 4.30 gives us the /f_irst factorH ãÑGH, so we only need to establish that the
Hebb-Laplace doctrine obtains by pulling a functor GH Ñ HierN
GausspKℓpPFdqq back along this inclusion.
We now turn to establishing that stochastic gradient descent returns the desired identity-on-objects functor
GH ÑHierN
GausspKℓpPFdqq. Proposition 4.31 shows thatH is obtained by applying stochastic gradient descent
to morphisms in GH, so we need to show that the resulting mapping is functorial.
As in the case of Theorem 4.21, the structure morphisms are preserved trivially: they have trivial parameter-
ization, and so stochastic gradient descent returns the trivial systems constantly emitting the corresponding
lenses; in particular, this means that stochastic gradient descent preserves identities.
We now show that, for composable games hand g, Hphq˝ Hpgq“ Hph˝gq. This means demonstrating
equalities between state spaces, output maps, and update maps. As for Theorem 4.21, the state spaces are
given by the external parameterization, and the parameterization of the composite game h˝gand the state
space of the composite system Hphq˝ Hpgqare both given by taking the product of the factors, and so the
state spaces on the left- and right-hand sides of the desired equation are equal.
The proof that the equality holds for output maps is also as in the proof of Theorem 4.21: the output of
a composite system is given by composing the output lenses of the factors, which is the same as the output
returned by H on a composite game, since outputs in the image of H are obtained by /f_illing in the external
parameter.
We now turn to the update maps, for which we need to show that
`
Hphq˝ Hpgq
˘u “Hph˝gqu. Suppose
g :“ pγ,ρ,φ q : pX,Xq Ñ pY,Y qand h :“ pσ,δ,ψ q : pY,Y q Ñ pZ,Zqare Hebbian-Laplacian statistical
games; we will denote the corresponding parameters by pθγ,µρqand pθδ,µσqrespectively. Following the
proof of Theorem 4.21, we can write the loss function of the composite game pσ,δ,ψ q˝p γ,ρ,φ qas
E
z„Lπ|γpθγq|δpθδq˚kM
”
FL`
σpµσqγpθγq‚πX,δpθδq; γpθγq‚ πX,z
˘
` E
y„σpµσqγpθγq‚πXpzq
“
FL`
ρpµρqπX,γpθγq; πX,y
˘‰ı
.
(This expression is obtained by making the substitutions γ ÞÑ γpθγqand δ ÞÑ δpθδqin the corresponding
expression in the proof of Theorem 4.21.)
As before, zand πX are supplied by the inputs to the dynamical system, and so we obtain a function
f : pz,πX,θγ,µρ,θδ,µσqÞÑ FL`
σpµσqγpθγq‚πX,δpθδq; γpθγq‚ πX,z
˘
` E
y„σpµσqγpθγq‚πXpzq
“
FL`
ρpµρqπX,γpθγq; πX,y
˘‰
.
If we write p :“ pθγ,µρqand q :“ pθδ,µσq, then pp,qqdenotes the parameter for h˝g. Since H performs
stochastic gradient descent with respect to the parameterization, Hph˝gqu is therefore de/f_ined as returning
the point distribution onpp,qq´λBpp,qqfpz,πXq, where λ:“pλp,λqq, and λp “pλγ,λρqand λq “pλδ,λσq.
We have Bpp,qqf “
`
Bpf,Bqfqand so
pp,qq´ λBpp,qqfpz,πXq“
`
p´λpBpfpz,πXq,q ´λqBqfpz,πXq
˘
.
We make some auxiliary de/f_initions
gupθγ,µρ,π,y q:“pθγ,µρq´ λpBpθγ,µρqFL`
ρpµρqπ,γpθγq; π,y
˘
hupθδ,µσ,π1,zq:“pθδ,µσq´ λqBpθδ,µσqFL`
σpµσqπ1,δpθδq; π1,z
˘
32
and /f_ind that
pp,qq´ λBpp,qqfpz,πXq
“pθγ,µρ,θδ,µσq´ λBpθγ,µρ,θδ,µσqfpz,πXq
“
`
pθγ,µρq´ λpBpθγ,µρqfpz,πXq,pθδ,µσq´ λqBpθδ,µσqfpz,πXq
˘
“
˜
E
y„σpµσqγpθγq‚πXpzq
“
gupθγ,µρ,πX,yq
‰
, hu`
θδ,µσ,γpθγq‚ πX,z
˘
¸
“
´
gupθγ,µρ,πXq‚ σpµσqγpθγq‚πXpzq, hu`
θδ,µσ,γpθγq‚ πX,z
˘¯
.
Writing PQ to denote the composite parameter spaceΘXˆXˆΘY ˆY, the foregoing computation de/f_ines
Hph˝gqu : PQ ˆGausspXqˆ Z ÑGausspPQqas
Hph˝gqupθγ,µρ,θδ,µσ,π,z q“ ηP
PQ
´
gupθγ,µρ,πXq‚σpµσqγpθγq‚πXpzq, hu`
θδ,µσ,γpθγq‚πX,z
˘¯
. (11)
The update map of the composite system
`
Hphq˝ Hpgq
˘u is given by composing the double strength dst :
GausspPqˆ GausspQqÑ GausspP ˆQqafter the string diagram
γ5
˚
σ5
Hpgqu
Hphqu
GausspXq
Y
X
Z
GausspQq
GausspPq
ΘX
ΘY
where γ5
˚indicates the uncurrying of the pushforwards of the parameterized forwards channel γ:
γ : ΘX ÑGauss
`
Parap‹q
˘
pX,Y q
embedsÞÝ ÝÝÝ Ñ ΘX ÑGausspKℓpPqqpX,Y q
p´q˚
ÞÝ ÝÝ Ñ ΘX ÑE
`
GausspKℓpPqqp1,Xq,GausspKℓpPqqp1,Y q
˘
„ÞÝ Ñ ΘX ÑEpGausspXq,GausspYqq
p´q5
ÞÝ ÝÝ Ñγ5
˚ : ΘX ˆGausspXqÑ GausspYq.
33
Next, note that we can write Hpgqu and Hphqu as
Hpgqupθγ,µρ,π,y q“ ηP
P
`
gupθγ,µρ,π,y q
˘
Hphqupθδ,µσ,π1,zq“ ηP
Q
`
hu`
θδ,µσ,π1,z
˘˘
where P :“ ΘX ˆX and Q :“ ΘY ˆY, and that ηP
PQ “ dstpηP
P,ηP
Qq. Reading the string diagram and
comparing with equation (11), we therefore /f_ind that
`
Hphq˝ Hpgq
˘u “Hph˝gqu.
Finally, the proof that H is strict monoidal is precisely analogous to the proof that L is strict monoidal: H
is identity-on-objects and maps structure morphisms to structure morphisms, so that the associativity and
unitality conditions are immediately satis/f_ied.
5 References
[1] Toby St. Clere Smithe. “Compositional Active Inference I: Bayesian Lenses. Statistical Games”. In:
(09/09/2021). arXiv: 2109.04461 [math.ST].
[2] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference. The Free Energy Principle in Mind,
Brain, and Behavior . MIT Press, 2022, p. 288. /i.sc/s.sc/b.sc/n.sc: 9780262045353.
[3] A. M. Bastos et al. “Canonical microcircuits for predictive coding”. In: Neuron 76.4 (11/2012), pp. 695–
711. /d.sc/o.sc/i.sc: 10.1016/j.neuron.2012.10.038.
[4] Karl Friston. “A free energy principle for a particular physics”. In: (06/24/2019). arXiv:http://arxiv.org/abs/1906.10184v1 [q-bio.NC].
[5] David I Spivak and Nelson Niu. Polynomial Functors: A General Theory of Interaction . 2021./u.sc/r.sc/l.sc: https://raw.githubusercontent.com/ToposInstitute/poly/main/Book-Poly.pdf.
[6] Toby St. Clere Smithe. “Polynomial Life: the Structure of Adaptive Systems”. In: Fourth International
Conference on Applied Category Theory (ACT 2021) . Ed. by K. Kishida. Vol. EPTCS 370. 2021, pp. 133–147.
/d.sc/o.sc/i.sc: 10.4204/EPTCS.370.28.
[7] David I. Spivak. “A reference for categorical structures onPoly”. In: (02/01/2022). arXiv:2202.00534 [math.CT].
[8] Danel Ahman and Tarmo Uustalu. “Directed Containers as Categories”. In: EPTCS 207, 2016, pp. 89-98
(04/05/2016). /d.sc/o.sc/i.sc: 10.4204/EPTCS.207.5. arXiv: 1604.01187 [cs.LO].
[9] David I. Spivak. “Poly: An abundant categorical setting for mode-dependent dynamics”. In: (05/05/2020).
arXiv: 2005.01894 [math.CT].
[10] Bart Jacobs. “From probability monads to commutative eﬀectuses”. In: Journal of Logical and Algebraic
Methods in Programming 94 (01/2018), pp. 200–237. /d.sc/o.sc/i.sc: 10.1016/j.jlamp.2016.11.006.
[11] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Diagrams”. In: Math.
Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). /d.sc/o.sc/i.sc: 10.1017/S0960129518000488. arXiv:
http://arxiv.org/abs/1709.00322v3 [cs.AI].
[12] Bryce Clarke et al. “Profunctor optics, a categorical update”. In: (01/21/2020). arXiv:2001.07488v1 [cs.PL].
[13] Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In: (01/10/2017).
/d.sc/o.sc/i.sc: 10.1109/lics.2017.8005137. arXiv:http://arxiv.org/abs/1701.02547 [cs.PL].
34
[14] David I. Spivak. “Learners’ languages”. In: (03/01/2021). arXiv: 2103.01189 [math.CT].
[15] Toby St Clere Smithe. “Open dynamical systems as coalgebras for polynomial functors, with application
to predictive processing”. In: (06/08/2022). arXiv:2206.03868 [math.CT].
[16] Pietro Vertechi. “Dependent Optics”. In: (04/20/2022). arXiv: 2204.09547 [math.CT].
[17] Dylan Braithwaite et al. “Fibre optics”. In: (12/21/2021). arXiv: 2112.11145 [math.CT].
[18] Toby St. Clere Smithe. “Bayesian Updates Compose Optically”. In: (05/31/2020). arXiv:2006.01631v1 [math.CT].
[19] Geoﬀrey S. H. Cruttwell et al. “Categorical Foundations of Gradient-Based Learning”. In: Programming
Languages and Systems. Springer International Publishing, 2022, pp. 1–28./d.sc/o.sc/i.sc: 10.1007/978-3-030-99336-8_1 .
[20] Matteo Capucci. “Diegetic representation of feedback in open games”. In: (06/24/2022). arXiv:2206.12338 [cs.GT].
[21] Matteo Capucci et al. “Towards foundations of categorical cybernetics”. In: (05/13/2021). arXiv:2105.06332 [math.CT].
[22] Toby St. Clere Smithe. “Cyber Kittens, or Some First Steps Towards Categorical Cybernetics”. In: Pro-
ceedings 3rd Annual International Applied Category Theory Conference 2020 (ACT 2020) . 2020.
[23] John M. Lee. Smooth Manifolds . New York, NY: Springer New York, 2012, pp. 1–31. /i.sc/s.sc/b.sc/n.sc: 978-1-4419-
9982-5. /d.sc/o.sc/i.sc: 10.1007/978-1-4419-9982-5_1 . /u.sc/r.sc/l.sc: https://doi.org/10.1007/978-1-4419-9982-5_1 .
[24] Rafal Bogacz. “A tutorial on the free-energy framework for modelling perception and learning”. In:Jour-
nal of Mathematical Psychology 76 (02/2017), pp. 198–211. /d.sc/o.sc/i.sc: 10.1016/j.jmp.2015.11.003.
[25] Christopher L Buckley et al. “The free energy principle for action and perception: A mathematical re-
view”. In:Journal of Mathematical Psychology 81 (05/24/2017), pp. 55–79. arXiv:http://arxiv.org/abs/1705.09156v1 [q-bio.NC].
[26] Matteo Capucci, Bruno Gavranović, and Toby St. Clere Smithe. “Parameterized Categories and Cate-
gories by Proxy”. In:Category Theory 2021 . 2021.
[27] K. Friston et al. “Variational free energy and the Laplace approximation”. In:Neuroimage 34.1 (01/2007),
pp. 220–234. /d.sc/o.sc/i.sc: 10.1016/j.neuroimage.2006.08.035.
[28] Matteo Capucci and Bruno Gavranović. “Actegories for the Working Amthematician”. In: (03/30/2022).
arXiv: 2203.16351 [math.CT].
[29] Diederik P. Kingma. “Variational Inference & Deep Learning. A New Synthesis”. PhD thesis. University
of Amsterdam, 2017./u.sc/r.sc/l.sc: https://hdl.handle.net/11245.1/8e55e07f-e4be-458f-a929-2f9bc2d169e8 .
[30] Dan Shiebler. “Categorical Stochastic Processes and Likelihood”. In:Compositionality 3, 1 (2021) (05/10/2020).
/d.sc/o.sc/i.sc: 10.32408/compositionality-3-1. arXiv: 2005.04735 [cs.AI].
[31] Tobias Fritz. “A synthetic approach to Markov kernels, conditional independence and theorems on suﬃ-
cient statistics”. In: (08/19/2019). arXiv:http://arxiv.org/abs/1908.07021v3 [math.ST].
35