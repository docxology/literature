arXiv:1205.3997v1  [stat.ML]  17 May 2012
JMLR: Workshop and Conference Proceedings vol:1–10, 2012 EWRL 2012
Free Energy and the Generalized Optimality Equations for
Sequential Decision Making
Pedro A. Ortega pedro.ortega@tuebingen.mpg.de
Max Planck Institute for Biological Cybernetics
Max Planck Institute for Intelligent Systems
ORAND S.A.
Daniel A. Braun daniel.braun@tuebingen.mpg.de
Max Planck Institute for Biological Cybernetics
Max Planck Institute for Intelligent Systems
Editor: Marc Deisenroth, Csaba Szepesvari, Jan Peters
Abstract
The free energy functional has recently been proposed as a varia tional principle for bounded
rational decision-making, since it instantiates a natural trade-oﬀ between utility gains and
information processing costs that can be axiomatically derived. Her e we apply the free
energy principle to general decision trees that include both advers arial and stochastic en-
vironments. We derive generalized sequential optimality equations t hat not only include
the Bellman optimality equations as a limit case, but also lead to well-know n decision-rules
such as Expectimax, Minimax and Expectiminimax. We show how these d ecision-rules can
be derived from a single free energy principle that assigns a resourc e parameter to each
node in the decision tree. These resource parameters express a c oncrete computational
cost that can be measured as the amount of samples that are need ed from the distribu-
tion that belongs to each node. The free energy principle therefor e provides the normative
basis for generalized optimality equations that account for both ad versarial and stochastic
environments.
Keywords: Foundations of AI, free energy, Bellman optimality equations, boun ded ratio-
nality.
1. Introduction
Decision trees are a ubiquitous tool in decision theory and a rtiﬁcial intelligence research to
represent a wide range of decision-making problems that inc lude the classic reinforcement
learning paradigm as well as competitive games (
Osborne and Rubinstein , 1999; Russell and
Norvig, 2010). Depending on the kind of system one is interacting with, th ere are diﬀerent
decision rules one has to apply—the most famous ones being Expectimax, Minimax and
Expectiminimax—see Figure 1. When an agent interacts with a stochastic system, the
agent chooses its decisions based on Expectimax. Essentially, Expectimax is the dynamic
programming algorithm that solves the Bellman optimality e quations, thereby recursively
maximizing expected future reward in a sequential decision problem ( Bellman, 1957).
In two-player zero-sum games where strictly competitive pl ayers make alternate moves,
an agent should use the Minimax strategy. The motivation underlying minimax decisions is
c⃝ 2012 P.A. Ortega & D.A. Braun.
Ortega Braun
that the agent wants to optimize the worst-case gain as a mean s of protecting itself against
the potentially harmful decisions made by the adversary. Fi nally, there are games that mix
the two previous interaction types. For instance, in Backga mmon, the course of the game
depends on the skill of the players and chance elements. In th ese cases, the agent bases its
decisions on the Expectiminimax rule ( Michie, 1966).
PSfrag replacements
Expectimax Minimax Expectiminimax
max
E
max
E
max
min
max
min
E
max
E
min
Figure 1: Illustration of Expectimax, Minimax and Expectim inimax in decision trees rep-
resenting three diﬀerent interaction scenarios. The intern al nodes can be of three
possible types: maximum ( △), minimum ( ▽) and expectation ( ◦). The optimal
decision is calculated recursively using dynamic programm ing.
What is common to all of these decision-making schemes is tha t they presuppose a fully
rational decision-maker that is able to compute all of the re quired operations with absolute
precision. In contrast, a bounded rational decision-maker trades oﬀ expected utility gains
against the cost of the required computations ( Simon, 1984). Recently, the free energy
has been suggested as a normative variational principle for such bounded rational decision-
making that takes the computational eﬀort into account ( Ortega and Braun , 2011; Braun
and Ortega , 2011; Ortega, 2011). This builds on previous work on eﬃcient computation of
optimal actions that trades oﬀ the beneﬁts obtained from max imizing the utility function
against the cost of changing the uncontrolled dynamics give n by the environment ( Kappen,
2005; Todorov, 2006, 2009; Kappen et al. , 2012). The aim of this paper is to extend these
results to generalized decision trees such that Expectimax , Minimax, Expectiminimax, and
bounded rational acting can all be derived from a single opti mization principle. Moreover,
this framework leads to a natural measure of computational c osts spent at each node of the
decision tree. All the proofs are given in the appendix.
2. Free Energy
2.1. Equilibrium Distribution
In
Ortega and Braun (2011) and in Ortega (2011) it was shown that a bounded rational
decision-making problem can be formalized based on the negative free energy diﬀerence
between two information processing states represented by t wo probability distributions P
and Q. The decision process then transforms the initial choice pr obability Q into a ﬁnal
2
Generalized Optimality Equations
choice probability P by taking into account the utility gains (or losses) and the t ransfor-
mation costs. This transformation process can be formalize d as
P (x) = 1
Z Q(x)eαU (x), where Z =
∑
x
Q(x)eαU (x). (1)
Accordingly, the choice pattern of the decision-maker is pr edicted by the equilibrium dis-
tribution P . Crucially, the probability distribution P extremizes the following functional
(Callen, 1985; Keller, 1998):
Deﬁnition 1 (Negative Free Energy Diﬀerence) Let Q be a probability distribution
and let U be a real-valued utility function over the set X . For any α ∈ R, deﬁne the
negative free energy diﬀerence Fα [P ] as
Fα [P ] :=
∑
x
P (x)U(x) − 1
α
∑
x
P (x) log P (x)
Q(x). (2)
The parameter α is called the inverse temperature.
Although strictly speaking, the functional Fα [P ] corresponds to the negative free energy
diﬀerence, we will refer to it as the “free energy” in the follo wing for simplicity. When
inserting the equilibrium distribution ( 1) into ( 2), the extremum of Fα yields:
1
α log
( ∑
x
Q(x)eαU (x)
)
. (3)
For diﬀerent values of α , this extremum takes the following limits:
lim
α →∞
1
α log Z = max
x
U(x) (maximum node)
lim
α →0
1
α log Z =
∑
x
Q(x)U(x) (chance node)
lim
α →−∞
1
α log Z = min
x
U(x) (minimum node)
The case α → ∞ corresponds to the perfectly rational agent, the case α → 0 corresponds to
the expectation at a chance node and the case α → −∞ anticipates the perfectly rational
opponent. Therefore, the single expression 1
α log Z can represent the maximum, expectation
and minimum depending on the value of α .
The inspection of ( 2) reveals that the free energy encapsulates a fundamental de cision-
theoretic trade-oﬀ: it corresponds to the expected utility, penalized—or regularized—by the
information cost of transforming the base distribution Q into the ﬁnal distribution P . The
inverse temperature plays the role of the conversion factor between units of information and
units of utility.
If we want to change the temperature α to β while keeping the equilibrium and reference
distributions equal, then we need to change the correspondi ng utilities from U to V in a
manner given by the following theorem. Temperature changes will be important for the
application of the free energy principle to the general deci sion trees in Section 3.
3
Ortega Braun
Theorem 2 Let P be the equilibrium distribution for a given inverse tempera ture α , utility
function U and reference distribution Q. If the temperature changes to β while keeping P
and Q ﬁxed, then the utility function changes to
V (x) = U(x) −
(
1
α − 1
β
)
log P (x)
Q(x).
2.2. Resource Costs
Consider the problem of picking the largest number in a seque nce U0, U 1, U 2, . . . of i.i.d.
data, where each Ui ∈ U is drawn from a source with probability distribution M. After α
draws the largest number will be given by max {U1, U 2, . . . , U α }. Naturally, the larger the
number of draws, the higher the chances of observing a large n umber.
Theorem 3 Let X be a ﬁnite set. Let Q and M be strictly positive probability distributions
over X . Let α be a positive integer. Deﬁne Mα as the probability distribution over the max-
imum of α samples from M. Then, there are strictly positive constants δ and ξ depending
only on M such that for all α ,
⏐
⏐
⏐
⏐
⏐
Q(x)eαU (x)
∑
x′ Q(x′)eαU (x′) − Mα (x)
⏐
⏐
⏐
⏐
⏐ ≤ e−(α −ξ)δ.
Consequently, one can interpret the inverse temperature as a resource parameter that
determines how many samples are drawn to estimate the maximu m. Note that the distri-
bution M is arbitrary as long as it has the same support as Q. This interpretation can
be extended to a negative α , by noting that αU (x) = ( −α )(−U(x)), i.e. instead of the
maximum we take the minimum of −α samples.
3. General Decision Trees
A generalized decision tree is a tree where each node corresp onds to a possible interaction
history x≤t ∈ X t, where t is smaller or equal than some ﬁxed horizon T , and where edges
connect two consecutive interaction histories. Furthermo re, every node x≤t has an associ-
ated inverse temperature β(x≤t); and every transition has a base probability Q(xt|x<t) of
moving from state x<t to state x≤t = x<txt representing the stochastic law the interactions
follow when it is not controlled, and an immediate reward R(xt|x<t). The objective of the
agent is to make decisions such that the sum ∑ T
t=1 R(xt|x<t) is maximized subject to the
temperature constraints.
3.1. Free Energy for General Decision Trees
The free energy principle is stated above for one decision va riable x. If x represents a tuple
of (possibly dependent) random variables x1, . . . , x T , then the free energy principle can
be applied in a straightforward manner to the corresponding tree. However, all nodes of
the tree will have the same inverse temperature assigned to t hem and, therefore, the same
amount of computational resources will be spent at each node of the tree. This allows for
4
Generalized Optimality Equations
PSfrag replacements
α
α α
β (ε)
β (0) β (1)
S(0|ε) S(1|ε)
S(0|0) S(1|0) S(0|1) S(1|1)
R(0|ε) R(1|ε)
R(0|0) R(1|0) R(0|1) R(1|1)
U(ε)
U(0) U(1)
V (ε)
V (0) V (1)
Figure 2: The free energy formalism can only be applied in a st raightforward manner to
trees with uniform resource allocation (left). In order to a pply it to general trees
that have diﬀerent resource parameters at each node (right), we need to transform
the utilities as described in ( 4) to preserve the equilibrium distribution.
example deriving the formalisms of path integral control an d KL control ( Todorov, 2009;
Braun and Ortega , 2011; Kappen et al. , 2012).
In the case of general decision trees the assumption of unifo rm temperatures has to be
relaxed (Figure 2). In general, we can then dedicate diﬀerent amounts of comput ational
resources to each node of the tree. However, this requires a t ranslation between a tree
with a single temperature and to a tree with diﬀerent temperat ures. This translation can
be achieved using Theorem 2. Deﬁne a reward as the change in utility of two subsequent
nodes. Then, the rewards of the resulting decision tree are g iven by
R(xt|x<t) :=
[
V (x≤t) − V (x<t)
]
=
[
U(x≤t) − U(x<t)
]
−
(
1
α − 1
β (x<t)
)
log P (xt|x<t)
Q(xt|x<t). (4)
This allows introducing a collection of node-speciﬁc (not n ecessarily time-speciﬁc) inverse
temperatures β(x<t), allowing for a greater degree of ﬂexibility in the represe ntation of
information costs. The next theorem states the connection b etween the free energy and the
general decision tree formulation.
Theorem 4 The free energy of the whole trajectory can be rewritten in ter ms of rewards:
Fα [P ] =
∑
x≤T
P (x≤T )
{
U(x≤T ) − 1
α log P (x≤T )
Q(x≤T )
}
= U(ε) +
∑
x≤T
P (x≤T )
T∑
t=1
{
R(xt|x<t) − 1
β(x<t) log P (xt|x<t)
Q(xt|x<t)
}
. (5)
This translation allows applying the free energy principle to each node with a diﬀerent
resource parameter β(x<t). By writing out the sum in ( 5), one realizes that this free energy
has a nested structure where the latest time step forms the in nermost variational problem
and all other variational problems of the previous time step s can be solved recursively by
working backwards in time. This then leads to the following s olution:
5
Ortega Braun
Theorem 5 The solution to the free energy in terms of rewards is given by
P (xt|x<t) = 1
Z(x<t)Q(xt|x<t) exp
{
β(x<t)
[
R(xt|x<t) + 1
β(x≤t) log Z(x≤t)
] }
,
where Z(x≤T ) = 1 and where for all t < T
Z(x<t) =
∑
xt
Q(xt|x<t) exp
{
β(x<t)
[
R(xt|x<t) + 1
β(x≤t) log Z(x≤t)
] }
.
3.2. Generalized Optimality Equations
Theorem
5 together with the properties of the free energy extremum ( 3) suggest the following
deﬁnition.
Deﬁnition 6 (Generalized Optimality Equations)
V (x<t) = 1
β(x<t) log
{ ∑
xt
Q(xt|x<t) exp
{
β(x<t)
[
R(xt|x<t) + V (x≤t)
] } }
.
By virtue of our previous analysis, this equation tells us ho w to recursively calculate the
value function (i.e. the utility of each node) given the computational reso urces allocated in
each node.
It is immediately clear that the three kinds of decision tree s mentioned in the introduc-
tion are special cases of general decision trees. In particu lar, the three classical operators
are obtained as limit cases:
V (x<t) =









max
xt
{R(xt|x<t) + V (x≤t)} if β(x<t) = ∞,
E{R(xt|x<t) + V (x≤t)} if β(x<t) = 0,
min
xt
{R(xt|x<t) + V (x≤t)} if β(x<t) = −∞.
The familiar Bellman optimality equations for stochastic s ystems are obtained by consider-
ing an agent decision node followed by a random decision node :
V (x<t) = max
xt
{
R(xt|x<t) + V (x≤t)
}
= max
xt
{
R(xt|x<t) + E
[
R(xt+1|x≤t) + V (x≤t+1)
] }
.
4. Discussions & Conclusions
Bounded rational decision-making schemes based on the free energy generalize classic decision-
making schemes by taking into account information processi ng costs measured by the
Kullback-Leibler divergence (
Wolpert, 2004; Todorov, 2009; Peters et al. , 2010; Ortega
and Braun , 2011; Kappen et al. , 2012). Ultimately, these costs are determined by Lagrange
multiplier constraints given by the inverse temperature pl aying the role of a resource param-
eter. Here we generalize this approach to general decision t rees where each node can have
a diﬀerent resource allocation. Consequently, we obtain gen eralized optimality equations
6
Generalized Optimality Equations
for sequential decision-making that include the well-know n Bellman optimality equation as
well as Expectimax-, Minimax- and Expectiminimax-decisio n rules depending on the limit
values of the resource parameters. The resource parameters themselves are amenable to
interesting computational, statistical and economic inte rpretations. In the ﬁrst sense they
measure the number of samples needed from a distribution bef ore applying the max operator
and therefore correspond directly to computational eﬀort. I n the second sense they reﬂect
the conﬁdence of the estimate of the maximum and therefore th ey can also express risk
attitudes. Finally, the resource parameters reﬂect the con trol an agent has over a random
variable. These diﬀerent ramiﬁcations need to be explored fu rther in the future.
Appendix A. Proofs
A.1. Proof of Theorem
2
Proof ∑
x
P (x)U(x) − 1
α
∑
x
P (x) log P (x)
Q(x) =
∑
x
P (x)V (x) − 1
β
∑
x
P (x) log P (x)
Q(x)
Since the equilibrium and reference distributions P (x) and Q(x) are constant but arbitrarily chosen,
it must be that
U(x) − 1
α log P (x)
Q(x) = V (x) − 1
β log P (x)
Q(x).
Hence,
V (x) = U(x) −
(
1
α − 1
β
)
log P (x)
Q(x).
A.2. Proof of Theorem 3
Proof Let x1, x 2, . . . , x N be the ordering of X such that U(x1), U (x2), . . . , U (xN ). It is well
known that the distribution over the maximum of α samples is equal to Fα (x) = F (x)α , where
F is the cumulative distribution F (xn) = ∑
k≤ n M(xk). Deﬁning F (x0) := 0, one has Mα (xn) =
F (xn)α − F (xn− 1)α . Hence, the probability can be bounded as 0 ≤ Mα (xn) ≤ F (xn)α , or
0 ≤ Mα (xn) ≤ e− αγ (xn), (6)
if we use F (xn) = e− γ (xn) where γ(xn) ≥ 0. The Boltzmann distribution can be bounded as
0 ≤ Q(xn)eαU (xn)
∑
k Q(xk)eαU (xk ) ≤ Q(xn)eαU (xn)
Q(xN )eαU (xN ) .
The upper bound is obtained by dropping all the summands in the expe ctation but the largest. In
exponential form, the bounds are written as
0 ≤ Q(xn)eαU (xn)
∑
k Q(xk)eαU (xk ) ≤ e− αδ (xn)+c(xn), (7)
where δ(xn) := U(xN ) − U(xn), c(xn) := − log Q(xN ) + log Q(xn). Note that δ(xn) is positive.
Subtracting the inequalities ( 6) from ( 7) yields
−e− αγ (xn) ≤ Q(xn)eαU (xn)
∑
k Q(xk)eαU (xk) − Mα (xn) ≤ e− αδ (xn)+c(xn).
7
Ortega Braun
Choosing ξ(xn) = c(xn)/δ (xn) ≥ 0 allows rewriting the upper bound and changing the lower bound
to
−e− (α − ξ(xn))γ (xn) ≤ Q(xn)eαU (xn)
∑
k Q(xk)eαU (xk ) − Mα (xn) ≤ e− (α − ξ(xn))δ(xn).
Finally, choosing ξ := max n{ξ(xn)} and δ = max {maxn{δ(xn)}, maxn{γ(xn)}} yields the bounds
of the theorem
−e− (α − ξ)δ ≤ Q(xn)eαU (xn)
∑
k Q(xk)eαU (xk) − Mα (xn) ≤ e− (α − ξ)δ.
A.3. Proof of Theorem 4
Proof The free energy of the whole trajectory with inverse temperatur e α is given by
∑
x≤ T
P (x≤ T )
{
U(x≤ T ) − 1
α log P (x≤ T )
Q(x≤ T )
}
.
Using a telescopic sum ∑ T
t=1(at − at− 1) = aT − a0 for the utilities yields
U(ε) +
∑
x≤ T
P (x≤ T )
T∑
t=1
{ [
U(x≤ t) − U(x<t)
]
− 1
α log P (xt|x<t)
Q(xt|x<t)
}
.
Using the deﬁnition of rewards ( 4), one gets the result
U(ε) +
∑
x≤ T
P (x≤ T )
T∑
t=1
{
R(xt|x<t) − 1
β(x<t) log P (xt|x<t)
Q(xt|x<t)
}
.
A.4. Proof of Theorem 5
Proof The inner sum of the free energy
U(ε) +
∑
x≤ T
P (x≤ T )
T∑
t=1
{
R(xt|x<t) − 1
β(x<t) log P (xt|x<t)
Q(xt|x<t)
}
.
can be expanded as
U(ε)+
∑
x1
P (x1)
{
R(x1) − 1
β(ε) log P (x1)
Q(x1)
+
∑
x2
P (x2|x1)
{
R(x2|x1) − 1
β(x1) log P (x2|x1)
Q(x2|x1)
+ · · ·
+
∑
xT
P (xT |x<T )
{
R(xT |x<T ) − 1
β(x<T ) log P (xT |x<T )
Q(xT |x<T )
}
· · ·
}}
.
8
Generalized Optimality Equations
This can be solved by induction, starting with the innermost sums and then recursively solving the
outer sums. The innermost sums
∑
xT
P (xT |x<T )
{
R(xT |x<T ) − 1
β(x<T ) log P (xT |x<T )
Q(xT |x<T )
}
are maximized when
P (xT |x<T ) = 1
Z(x<T )Q(xT |x<T ) exp
{
β(x<T )R(xT |x<T )
}
.
This can be seen by noting that for probabilities pi and positive numbers ri > 0, the quantity∑
i pi log(pi/r i) is minimized by choosing pi = 1
Z ri, where Z = ∑
i ri is just a normalizing constant.
Substituting this solution yields the outer sums
∑
xt
P (xt|x<t)
{
R(xt|x<t) − 1
β(x<t) log P (xt|x<t)
Q(xt|x<t) + 1
β(x≤ t) log Z(x≤ t)
}
where
Z(x<t) =
∑
xt
Q(xt|x<t) exp
{
β(x<t)
[
R(xt|x<t) + 1
β(x≤ t) log Z(x≤ t)
] }
.
These sums are then maximized by choosing
P (xt|x<t) = 1
Z(x<t)Q(xt|x<t) exp
{
β(x<t)
[
R(xt|x<t) + 1
β(x≤ t) log Z(x≤ t)
] }
.
References
R.E. Bellman. Dynamic programming, 1957.
D. A. Braun and P. A. Ortega. Path integral control and bounde d rationality. In IEEE
Symposium on adaptive dynamic programming and reinforceme nt learning , pages 202–
209, 2011.
H.B. Callen. Thermodynamics and an introduction to thermostatistics . John Wiley & Sons,
New York, 1985.
H.J. Kappen. A linear theory for control of non-linear stoch astic systems. Physical Review
Letters, 95:200201, 2005.
H.J. Kappen, V. G´ omez, and M. Opper. Optimal control as a gra phical model inference
problem. Machine Learning, 1:1–11, 2012.
G. Keller. Equilibrium States in Ergodic Theory . London Mathematical Society Student
Texts. Cambridge Univeristy Press, 1998.
D. Michie. Game-playing and game-learning automata. Advances in Programming & Non-
Numerical Computation , pages 183–200, 1966.
9
Ortega Braun
P. Ortega. A uniﬁed framework for resource-bounded autonomous agents interacting with
unknown environments . PhD thesis, Department of Engineering, University of Cam-
bridge, UK, 2011.
P.A. Ortega and D.A. Braun. Information, utility and bounde d rationality. In Lecture notes
on artiﬁcial intelligence , volume 6830, pages 269–274, 2011.
M.J. Osborne and A. Rubinstein. A Course in Game Theory . MIT Press, 1999.
J. Peters, K. M¨ ulling, and Y. Altun. Relative entropy polic y search. In AAAI, 2010.
S.J. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach . Prentice-Hall,
Englewood Cliﬀs, NJ, 3rd edition edition, 2010.
H. Simon. Models of Bounded Rationality . MIT Press, Cambridge, MA, 1984.
E. Todorov. Linearly solvable markov decision problems. In Advances in Neural Information
Processing Systems, volume 19, pages 1369–1376, 2006.
E. Todorov. Eﬃcient computation of optimal actions. Proceedings of the National Academy
of Sciences U.S.A. , 106:11478–11483, 2009.
D.H. Wolpert. Complex Engineering Systems , chapter Information theory - the bridge
connecting bounded rational game theory and statistical ph ysics. Perseus Books, 2004.
10