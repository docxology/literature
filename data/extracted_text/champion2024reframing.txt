arXiv:2402.14460v1  [cs.AI]  22 Feb 2024
Reframing the Expected Free Energy:
Four Formulations and a Uniﬁcation.
Th´ eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology and School of Computer Science,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
University College London, Wellcome Centre for Human Neuroimaging (honorary)
London WC1N 3AR, United Kingdom
Dimitrije Markovi´ c dimitrije.markovic@tu-dresden.de
Technische Universit¨ at Dresden, Department of Psychology
Dresden 01069, Germany
Marek Grze´ s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a leading theory of perception, learnin g and decision making, which can be applied
to neuroscience, robotics, psychology, and machine learni ng. Active inference is based on the expected
free energy, which is mostly justiﬁed by the intuitive plaus ibility of its formulations, e.g., the risk
plus ambiguity and information gain / pragmatic value formu lations. This paper seek to formalize
the problem of deriving these formulations from a single roo t expected free energy deﬁnition, i.e., the
uniﬁcation problem. Then, we study two settings, each one ha ving its own root expected free energy
deﬁnition. In the ﬁrst setting, no justiﬁcation for the expe cted free energy has been proposed to date,
but all the formulations can be recovered from it. However, i n this setting, the agent cannot have
arbitrary prior preferences over observations. Indeed, on ly a limited class of prior preferences over
observations is compatible with the likelihood mapping of t he generative model. In the second setting,
a justiﬁcation of the root expected free energy deﬁnition is known, but this setting only accounts for two
formulations, i.e., the risk over states plus ambiguity and entropy plus expected energy formulations.
Keywords: Active Inference, Expected Free Energy, Uniﬁcation Proble m
1. Introduction
Active inference (Friston et al., 2016; Itti and Baldi, 2009; Schwa rtenbeck et al., 2018; FitzGerald et al.,
2015; Fountas et al., 2020; Sancaktar et al., 2020; C ¸ atal et al., 20 20; Cullen et al., 2018; Millidge, 2019)
is a framework for decision-making under uncertainty, in which the a gent is equipped with a (gener-
ative) model that encodes the environment dynamics, and a variat ional posterior approximating the
true posterior over latent variables. The variational posterior is c omputed by minimizing a function
called the variational free energy (VFE), also known as the negativ e evidence lower bound in machine
learning (Fox and Roberts, 2012; ?). While the variational posterior deﬁnes the most likely state of the
environment, it does not indicate which action should be selected. In stead, the agent aims to reach a
set of preferred states or observations, by minimizing the expect ed free energy (EFE).
1
While the variational free energy has a clear root deﬁnition that all o ther formulations are derived
from, the literature does not clearly identify such a root deﬁnition f or the expected free energy, leaving
the question of antecedence amongst its many formulations as moo t.
The EFE is a function deﬁning the cost of performing a particular polic y as a trade-oﬀ between
exploration and exploitation, e.g., the goal is to maximize pragmatic va lue (reward), while also maxi-
mizing information gain. The pragmatic value relies on the prior prefer ences of the agent, which specify
the preferred states or observations, and provides the agent w ith its goal-directed behaviour.
In the active inference literature, the prior preferences over ob servation are usually denoted P (o; C),
where C is a vector of parameters, i.e., there is no distribution over C. Thus, the prior preferences over
observation can also be written P (o). Importantly, the prior preferences are denoted using the lett er
P , suggesting that these preferences are part of the generative model. However, P (o) in the generative
model refers to the marginal likelihood. Therefore, the symbol P (o) has a dual meaning, i.e., it refers to
both the prior preferences and the marginal likelihood. This dual me aning may not seem problematic
at ﬁrst, but introduces hidden inconsistencies. For example, in mos t cases, the prior preferences will
be constant, i.e., not updating during a run. However, in general, th e prior over future state P (s) will
change as time passes and the agent gathers new observations, t hus the marginal likelihood will change,
suggesting an inconsistency resulting from the conﬂation of the tw o interpretations. More formally,
when using Bayes theorem:
P (s|o) = P (o|s)P (s)
P (o) ,
the denominator corresponds to the joint distribution P (o, s) marginalized over the states s. However,
since the prior preferences are also denoted P (o), the denominator could be interpreted as being the
prior preferences, but in most cases: P (o) ̸= P (o; C). Importantly, the dual meaning problem can also
emerge if the joint distribution P (o, s) is marginalized using the sum-rule of probability or split using
the product rule, i.e.,
P (o) =
∑
s
P (o, s) or P (o, s) = P (s|o)P (o).
To solve the dual meaning problem, the prior preferences are some time considered as being part of a
target distribution. However, in this paper, we show that this assu mption restricts the class of valid
prior preferences, and leads to a deﬁnition of the expected free e nergy that is not currently justiﬁed. In
the following sections, we explore two possible interpretations of Pa rr et al. (2022), and explain their
limitation. Appendix B and C provide a description of the properties us ed throughout this paper.
2. Generative model
In active inference, the agent is equipped with a (generative) mode l of its environment that spans all
time steps until the present time t. This model is composed of (a) hidden states s0:t representing
states of the environment that the agent does not directly obser ve, (b) observations o0:t, which represent
measurements made by the agent, and (c) actions a0:t− 1 that the agent performed in the environment.
For the sake of conciseness, s0:t, o0:t, and a0:t− 1 will be denoted s
, o, and a, repectively. Moreover, in this
paper, we assume that observations depend on states, and each state depends on the state and action at
the previous time step. Formally, this setting is called a Partially Obser vable Markov Decision Process
(POMDP), and the model deﬁnition is as follows:
P (o, s|a) =∆ P (s0)
t∏
τ=0
P (oτ |sτ )
t∏
τ=1
P (sτ |sτ− 1, aτ− 1).
3. Variational distribution
The generative model described in the previous section encodes pr ior beliefs about the environment
dynamics. However, when making measurements of key quantities, e.g., o
, the agent needs to compute
2
posterior beliefs about the states, e.g., P (s|o, a). These posterior beliefs encode the new beliefs of
the agent when taking into consideration the new observations. Un fortunately, computing the true
posterior can either be analytically intractable or simply too computa tionally expensive. Therefore, the
true posterior is generally approximated by a variational distributio n Q(s|a):
Q(s|a)  
variational posterior
≈ P (s|o, a)  
true posterior
∝ P (o, s|a)  
generative model
In active inference, the variational posterior 1) factorises over time steps, i.e., a temporal mean-ﬁeld
approximation, but 2) all states still depend on the policy a. These two assumptions lead to the
following deﬁnition of the variational distribution:
Q(s|a) =∆
t∏
τ=0
Q(sτ |a)
4. Variational inference and the variational free energy
To sum up, the agent is provided with a generative model P (o
, s|a) and a variational distribution
Q(s|a). Given some measurements o, the variational distribution needs to approximate the true poste rior
P (s|o, a). This can be formally expressed as minimising the Kullback-Leibler dive rgence between the
approximate and true posteriors:
Q∗(s|a) = arg min
Q(s|a)
DKL [ Q(s|a)||P (s|o, a)]
Minimising this KL-divergence and minimising the variational free energ y (VFE) is equivalent (see
proof below). Intuitively, the VFE trades-oﬀ accuracy, i.e., how we ll are the observations predicted, and
complexity, i.e., how far the posterior is from the prior. More formally , the VFE is deﬁned as follows:
F
[
Q(s|a), a, o
]
= DKL [ Q(s|a)||P (s|a)]  
complexity
−EQ(s|a)[ln P (o|s)]  
accuracy
The derivation of the variational free energy proceeds as follows:
Q∗(s|a) = arg min
Q(s|a)
DKL [ Q(s|a)||P (s|o, a)]
= arg min
Q(s|a)
EQ(s|a)
[
ln Q(s|a) −ln P (o|s)P (s|a)
P (o|a)
]
= arg min
Q(s|a)
EQ(s|a)[ln Q(s|a) −ln P (o|s)P (s|a)] + ln P (o|a)  
constant
= arg min
Q(s|a)
DKL [ Q(s|a)||P (s|a)]  
complexity
−EQ(s|a)[ln P (o|s)]  
accuracy
= arg min
Q(s|a)
F
[
Q(s|a), a, o
]
  
variational free energy
,
P
3
where the d-separation criterion (Koller and Friedman, 2009) (whic h simpliﬁes P (o|s, a) to P (o|s)
in the second line, given the independence of o and a given s) and Bayes theorem have been used,
along with the linearity of expectation and the log-property.
P
In this paper, we focus on planning, therefore we do not explain how the optimal variational
distribution Q∗(s|a) is computed. The interested reader is referred to (Champion et a l., 2022b;
Parr et al., 2019; Friston et al., 2017; Winn et al., 2005) for an appro ximate scheme based on vari-
ational message passing, and (Champion et al., 2022c; Kschischang et al., 2001) for an exact scheme
based on the sum-product algorithm.
C
5. Planning and the expected free energy
After performing inference, the agent has at its disposal poster ior beliefs encoded by the optimal varia-
tional distribution Q∗(s
|a). At this point, the agent needs to choose the next action to perf orm in the
environment. In active inference, the cost of a policy is given by the expected free energy (EFE), thus
the goal of planning is to identify the policy with the smallest EFE.
Unfortunately, given a time horizon of planning h, the number of policies is |A|h− t, where Ais the
set of all actions available to the agent, and |A|is the cardinality of this set. As the number of policies
grows exponentially with the time horizon, computing the EFE of all po licies requires exponential time.
Therefore, it is important to search the space of policies eﬃciently. For example, one can use Monte-
Carlo tree search (Champion et al., 2022a,d), which maintains a balanc e between exploiting policies with
low EFE and exploring rarely visited action sequences. Another solut ion would be to use sophisticated
inference (Friston et al., 2021), which implements a tree search ove r actions and outcomes in the future,
by using a recursive form of expected free energy.
In this paper, we postulate that the expected free energy is base d on two distributions: the fore-
cast F (
o, s|a) and target T (o, s|a) distributions, where the future observations, states, and act ions are
denoted o = ot+1:h, s = st+1:h and a = at:h− 1, respectively. The forecast distribution predicts the fu-
ture according to the agent’s best beliefs about the current stat es of the environment, and its generative
model. In contrast, the target distribution encodes the states a nd observations the agent wants to reach.
A general formulation of the target is given here, allowing it to chang e with policies. Although, in most
cases, it will not change.
Why is the forecast distribution introduced?Q
In the active inference literature, it is frequent to see Bayes theo rem being used between factors of
the generative model P (o, s|a) and the variational distribution Q(s|a), or even to see factors from
the generative model P (x|y) being replaced by their variational counterpart Q(x|y). However,
Bayes theorem is a corollary of the product rule of probability, i.e.,
P (x, y) = P (x|y)P (y) = P (y|x)P (x) ⇔P (y|x) = P (x|y)P (y)
P (x) .
Thus, technically, Bayes theorem cannot be used between factor s of two diﬀerent distributions. To
be really explicit, equality between P (y|x) and the right-hand-side would not hold if, for example,
P (y) were replaced by any distribution diﬀerent to P (y), and similarly for P (x|y)P (y) and P (x).
Importantly, it is straightforward to see that the generative mod el and the variational distribution
A
4
are two diﬀerent distributions. The easiest way to see this, is to rea lise that these two distributions
do not even share the same domain, i.e., the generative model is a dist ribution over states s and
observations o, while the variational distribution is a distribution over states s only. One might
consider observations to be implicitly present, but in this respect, t hey are a ground term, with a
particular value, rather than random variables with a distribution ov er possible values. Another
intuitive argument is that the generative model encodes the prior b eliefs of the agent, then upon
receiving new data, the agent computes its (approximate) poster ior beliefs. If the prior was equal to
the posterior, there will be no point in performing inference in the ﬁr st place. Since the generative
model and the variational distribution are two diﬀerent distribution s, one should be careful when
replacing factors from one distribution by factors from the other . It is for this reason that the
forecast distribution is introduced. Put simply, the forecast distr ibution provides a bridge between
factors of the variational posterior and those of the generative model.
A
5.1 The uniﬁcation problem
In this section, we formalise the problem of deriving the four EFE for mulations, which can be found
in the literature (see below), from the EFE deﬁnition, i.e., the uniﬁcation problem . Speciﬁcally, the
uniﬁcation problem is a 4-tuple P= ⟨F, T, Grt, C⟩, where F is the forecast distribution, T is the target
distribution, Grt is the deﬁnition of the expected free energy, and C= {CRSA , CROA, CIGP V , C3E }is a set
containing the four formulations of the expected free energy. So lving Pconsists of ﬁnding a deﬁnition
of F , T , and Grt such that it is possible to derive CX from Grt for all CX ∈C.
We now deﬁne the four formulations of the expected free energy, which are based upon the deﬁnitions
in Parr et al. (2022). The formulation for the risk over states and a mbiguity is as follows:
CRSA (
a) =∆ DKL [ F (s|a)||T (s|a)]  
risk over states
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
.
Importantly, the risk over states is the KL-divergence between t he predictive posterior over states F (s|a)
and the prior preferences over states T (s|a), and the ambiguity is the expected entropy of the likelihood
mapping according to the generative model. The risk over states pu shes the predictive posterior towards
the prior preferences, while the ambiguity encourages the agent t o visit states producing a low entropy
distribution over observations, i.e., if we arrive at a state, we know w hich observation(s) to expect. The
formulation for the risk over observations and ambiguity is as follows :
CROA(a) =∆ DKL [ F (o|a)||T (o|a)]  
risk over observations
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
.
The ambiguity term is identical, and the risk over observations is a KL- divergence, which pushes the
predictive posterior over observations F (o|a) to be as close as possible to the prior preferences over
observations T (o|a). The formulation for the information gain and pragmatic value is as f ollows:
CIGP V (a) =∆ −EF (o|a)
[
DKL
[
F (s|o, a)||F (s|a)
]]
  
information gain
−EF (o|a)[ln T (o|a)]  
pragmatic value
.
Importantly, the information gain is a KL-divergence that relies only on factors from the forecast dis-
tribution. This prevents degenerate behaviours were the agent s tops exploring its environment, i.e.,
information loss (Champion et al., 2023). In addition, the pragmatic v alue is based on the preferred
observations T (o|a), which provides the agent with its goal directed behaviour. Finally, the expected
energy and entropy formulation is as follows:
C3E (a) =∆ −H[F (s|a)]  
entropy
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
5
The entropy term ensures that a good policy is one which keeps our o ptions open by allowing us to reach a
wide variety of states, as implied by Jaynes’ theory of maximum entr opy (Jaynes, 1957a,b). Additionally,
as shown in the proof below, the expected energy encourages the agent to reach its preferred states,
while also pushing the agent to select states for which the associate d distribution over observations has
low entropy, i.e., given a state, we know which observation(s) to exp ect.
Starting with the negative expected energy, one can use the prod uct rule, the log-property and the
linearity of expectation to get:
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
= −EF (o, s|a)[ln T (o|s, a)] −EF (s|a)[ln T (s|a)].
Then, assuming that the forecast distribution is a partially observa ble Markov decision process,
and that the likelihood of the forecast and target distributions are the same, we obtain:
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
= −EF (o, s|a)[ln F (o|s, a)] −EF (s|a)[ln T (s|a)]
= −EF (o, s|a)[ln F (o|s)] −EF (s|a)[ln T (s|a)]
where, F (o|s, a) = F (o|s), due to d-separation, observing that o and a are conditionally indepen-
dent given s. Lastly, using this same property again after applying the product rule and then using
the deﬁnition of entropy gives the ﬁnal result:
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
= EF (s|a)[H[F (o|s)]]  
ambiguity
−EF (s|a)[ln T (s|a)]  
pragmatic value
,
This equation shows that maximising expected energy means selectin g states that minimise the
ambiguity of the likelihood while maximising the pragmatic value of states .
P
5.2 Forecast distribution
As previously mentioned, the forecast distribution predicts the fu ture according to the agent’s best
beliefs about the current states of the environment, and its gene rative model. More formally, the
forecast distribution factorizes as follows:
F (
o, s|a) =∆ F (st+1|at)
h∏
τ=t+1
F (oτ |sτ )
h∏
τ=t+2
F (sτ |sτ− 1, aτ− 1).
Additionally, we make three assumptions to deﬁne the factors of th e forecast distribution. These assump-
tions deﬁne the forecast distribution in terms of factors from the generative model and the variational
distribution. But importantly, these deﬁnitions are made explicit her e, leaving no uncertainty about
these relationships amongst probability distributions. First, we ass ume that the likelihood in the future
F (oτ |sτ ) is the same as in the past P (oτ |sτ ), i.e., the likelihood of the forecast distribution is the same
as the likelihood of the generative model. Second, the temporal tra nsition in the future F (sτ |sτ− 1, aτ− 1)
is the same as in the past P (sτ |sτ− 1, aτ− 1), i.e., the temporal transition of the forecast distribution is the
same as the temporal transition of the generative model. Third, th e agent’s best prior over the current
state F (st) is given by the optimal variational posterior Q∗(st|a). More formally:
F (oτ |sτ ) = P (oτ |sτ )
F (sτ |sτ− 1, aτ− 1) = P (sτ |sτ− 1, aτ− 1)
F (st) = Q∗(st|a)
6
Using the above assumptions, we obtain F (st+1|at) by taking the expectation of the temporal transition
w.r.t. the prior over states at time t. For generality, we deﬁne this as an integral, but this could be
speciﬁed to a summation in the discrete case:
F (st+1|at) =
∫
st
F (st+1|st, at)F (st) dst =
∫
st
P (st+1|st, at)Q∗(st|a) dst
5.3 Target distribution
The second distribution of interest is the target distribution, which encodes the states and observations
that the agent wants to reach. In the following section, we deﬁne t he target distribution as follows:
T (
o, s|a) =∆
h∏
τ=t+1
T (oτ |sτ )T (sτ |a),
where T (oτ |sτ ) = P (oτ |sτ ) and T (sτ |a) = Cat( sτ ; Cs). Note, Cs is the matrix known as the C matrix
in the active inference literature, where the prior preferences ar e deﬁned w.r.t. states. Also, the target’s
likelihood is equal to P (oτ |sτ ), which means that given any state, the agent wants to reach the obser-
vations that will arise naturally according to the likelihood of the gene rative model, i.e., P (oτ |sτ ). If
the target changes with policy a, this would have to be built into the Cs matrix, but in most cases, the
target distribution would be ﬁxed.
5.4 Solving the uniﬁcation problem
With the forecast and target distributions laid out, we now focus on the uniﬁcation problem. We
will explore whether any of the EFE formulations could serve as a roo t deﬁnition from which all other
formulations could be derived. First, we deﬁne the root expected f ree energy as the risk over observations
plus ambiguity:
Grt(
a) =∆ DKL [ F (o|a)||T (o|a)]  
risk over observations
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
= CROA(a). (1)
5.4.1 The information gain / pragmatic value formulation
In this section, we demonstrate that the information gain / pragma tic value formulation can be recovered
from the CROA(
a)-as-root expected free energy deﬁnition. The derivation relies o n the following equality:
F (s|a)
F (s|o, a) = F (o|a)
F (o|s), (2)
which holds because the forecast distribution is a partially observab le Markov decision process.
We start by re-arranging Bayes theorem as follows:
F (s|o, a) = F (o|s, a)F (s|a)
F (o|a) ⇔ F (s|a)
F (s|o, a) = F (o|a)
F (o|s, a).
Then, in a partially observable Markov decision process, o ⊥ ⊥a |s, (i.e., the Markov property
ensures that observation sequences are conditionally independen t of policies, if the sequence of
states is known), thus F (o|s, a) = F (o|s) and:
F (s|a)
F (s|o, a) = F (o|a)
F (o|s).
P
7
Importantly, by starting with the deﬁnition of CIGP V (a) and using (2), one can show that:
Grt(a) = −EF (o|a)[DKL [ F (s|o, a)||F (s|a)]]  
information gain
−EF (o|a)[ln T (o|a)]  
pragmatic value
= CIGP V (a). (3)
Starting with the deﬁnition of CIGP V (a):
CIGP V (a) = −EF (o|a)[DKL [ F (s|o, a)||F (s|a)]]  
information gain
−EF (o|a)[ln T (o|a)]  
pragmatic value
,
and using the KL-divergence deﬁnition, and that F (o, s|a) = F (s|o, a)F (o|a), by the product rule,
we obtain:
CIGP V (a) = −EF (o, s|a)[ln F (s|o, a) −ln F (s|a)] −EF (o|a)[ln T (o|a)]
Then, by using the log-properties and (2) to replace F (s|o, a)
F (s|a) by F (o|s)
F (o|a) , we get:
CIGP V (a) = −EF (o, s|a)[ln F (o|s) −ln F (o|a)] −EF (o|a)[ln T (o|a)]
Next, the linearity of expectation can be applied to re-arrange the expression as follows:
CIGP V (a) = −EF (o, s|a)[ln F (o|s)] + EF (o|a)[ln F (o|a) −ln T (o|a)]
Lastly, recognizing the entropy and KL-divergence deﬁnitions lead s to the ﬁnal results:
CIGP V (a) = DKL [ F (o|a)||T (o|a)]  
risk over observations
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
= Grt(a).
P
5.4.2 The risk over states vs ambiguity formulation
In this section, we demonstrate that the risk over states plus amb iguity is an upper bound of the expected
free energy. Restarting from the EFE deﬁnition, one can show tha t:
Grt(
a) ≤DKL [ F (o, s|a)||T (o, s|a)] + EF (s|a)
[
H[F (o|s)]
]
.
We follow the proof provided in Appendix B of Parr et al. (2022), but u sing our notation and going
from the end of the proof to the beginning. Restarting from the EF E deﬁnition:
Grt(a) = DKL [ F (o|a)||T (o|a)]  
risk over observations
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
,
we obtain an upper bound on the EFE by adding the following bound, wh ich is the expectation of
a KL-divergence and cannot be negative:
Grt(a) ≤DKL [ F (o|a)||T (o|a)]  
risk over observations
+ EF (s|a)
[
H[F (o|s)]
]
+ EF (o|a)
[
DKL [ F (s|o, a)||T (s|o, a)]
]
  
bound
.
P
8
Next, using the linearity of expectation and the log-property, the bound can be merged to the risk
over observations:
Grt(a) ≤DKL [ F (o, s|a)||T (o, s|a)] + EF (s|a)
[
H[F (o|s)]
]
. (4)
P
Additionally, if one assumes that T (o|s) = F (o|s), then restarting from Equation 4, one can show that
the risk over states plus ambiguity is an upper bound of the expecte d free energy, i.e.,
Grt(a) ≤DKL [ F (s|a)||T (s|a)] + EF (s|a)
[
H[F (o|s)]
]
= CRSA (a). (5)
Once again, we keep following the proof presented in Appendix B of Pa rr et al. (2022) backward.
Let us restart from Equation 4:
Grt(a) ≤DKL [ F (o, s|a)||T (o, s|a)] + EF (s|a)
[
H[F (o|s)]
]
.
Then, using the deﬁnition of the KL-divergence, the linearity of exp ectation and the log-property,
we can split the KL-divergence as follows:
Grt(a) ≤DKL [ F (s|a)||T (s|a)] + EF (s|a)
[
H[F (o|s)]
]
+ EF (o, s|a)[ln F (o|s)] −EF (o, s|a)[ln T (o|s)].
Next, using our assumption that T (o|s) = F (o|s), we can get:
Grt(a) ≤DKL [ F (s|a)||T (s|a)] + EF (s|a)
[
H[F (o|s)]
]
+ EF (o, s|a)[ln F (o|s)] −EF (o, s|a)[ln F (o|s)]  
=0
,
which simpliﬁes to:
Grt(a) ≤DKL [ F (s|a)||T (s|a)]  
risk over states
+ EF (s|a)
[
H[F (o|s)]
]
  
ambiguity
= CRSA(a).
P
Importantly, since the risk over states plus ambiguity is an upper bo und of the EFE, minimising the
upper bound will also minimise the EFE.
5.4.3 Expected energy vs entropy formulation
Finally, restarting from the risk over states plus ambiguity in Equatio n 5, one can demonstrate that:
Grt(
a) ≤CRSA(a) = −H[F (s|a)]  
entropy
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
= C3E (a)
Thus, the expected energy vs entropy formulation can be recove red in this setup.
Restarting from Equation 5:
Grt(a) ≤CRSA (a) = DKL [ F (s|a)||T (s|a)] + EF (s|a)
[
H[F (o|s)]
]
.
P
9
and using the KL-divergence and entropy deﬁnitions, we obtain:
Grt(a) ≤CRSA (a) = EF (s|a)
[
ln F (s|a) −ln T (s|a)
]
−EF (o, s|a)
[
ln F (o|s)
]
.
Then, using our assumption that F (o|s) = T (o|s), as well as the linearity of expectation and the
log-property, we get:
Grt(a) ≤CRSA(a) = EF (s|a)
[
ln F (s|a) −ln T (s|a)
]
−EF (o, s|a)
[
ln T (o|s)
]
= EF (s|a)[ln F (s|a)] −EF (o, s|a)[ln T (o, s|a)].
Finally, recognizing the entropy deﬁnition, we obtain the desired res ult:
Grt(a) ≤CRSA (a) = −H[F (s|a)]  
entropy
−EF (o, s|a)[ln T (o, s|a)]  
expected energy
= C3E (a).
P
6. Limitations
In the previous section, we deﬁned the expected free energy as t he risk over observations plus ambiguity,
i.e., CROA(
a), and showed that it is equal to the information gain and pragmatic v alue formulation,
i.e., CIGP V (a). Then, following the proof in Appendix B of Parr et al. (2022), the r isk over states plus
ambiguity CRSA (a) was shown to be an upper bound of CROA(a). Finally, the entropy plus expected
energy formulation C3E (a) was derived from CRSA (a). In summary:
Grt(a) =∆ CROA(a) = CIGP V (a) ≤CRSA(a) = C3E (a). (6)
Importantly, the proofs leading to equation (6) rely on the assump tion that the likelihood of the forecast
and target distributions are equal, i.e., F (o|s) = T (o|s). In the following subsections, we study the
limitations of the formalism presented in Section 5.
6.1 Prior preferences over observations
In this section, we study the assumptions made in Section 5.4 and the ir consequences. For simplicity,
we only consider the case where the time horizon h is equal to t + 1, and where ot+1, st+1, as well
as at are discrete random variables. In this case,
o = ot+1:h = ot+1, s = st+1:h = st+1, and similarly
a = at:h− 1 = at. Note, in Section 5.4, the EFE is deﬁned as the risk over observation s plus ambiguity,
i.e.,
Grt(at) =∆ DKL [ F (ot+1|at)||T (ot+1|at)]  
risk over observations
+ EF (st+1|at)
[
H[F (ot+1|st+1)]
]
  
ambiguity
= CROA(at).
6.1.1 The assumptions seemingly lead to an equation with no valid solution
In this section, we show that the assumptions made in Section 5, see mingly lead to an equation with no
valid solution. First, note that in the active inference literature, th e prior preferences over observations
are deﬁned as follows: T (ot+1|at) = Cat( ot+1; Co). Additionally, recall that the proof in Section 5.4.2
relies on the assumption that T (ot+1|st+1) = F (ot+1|st+1), where the likelihood of the forecast distri-
bution is also the likelihood of the generative model, i.e., T (ot+1|st+1) = F (ot+1|st+1) = P (ot+1|st+1).
In the active inference literature, the likelihood of the generative m odel is deﬁned as: P (ot+1|st+1) =
Cat(ot+1|st+1; A). To sum up, we have T (ot+1|at) = Cat( ot+1; Co) and T (ot+1|st+1) = Cat( ot+1|st+1; A).
Using the sum and product rules of probability, we have:
T (ot+1|at) =
∑
st+1
T (ot+1|st+1, at)T (st+1|at) ⇔Co = ACs, (7)
10
where without loss of generality, we let T (st+1|at) = Cat( st+1; Cs). Importantly, the above assumes
that:
T (ot+1|st+1, at) = T (ot+1|st+1),
which holds because in the target distribution, the observations ar e independent of the policy given the
states, i.e., using the d-separation criteria one can show that: ot+1 ⊥ ⊥at |st+1. Re-starting from (7),
one can solve for Cs and get:
Co = ACs ⇔Cs = A− 1Co.
However, the above equation may not have any valid solution. For ex ample, if:
A =
[
0.6 0 .4
0.4 0 .6
]
and Co =
[
0.8
0.2
]
, (8)
then one can show that:
A− 1 =
[ 3 −2
−2 3
]
and Cs = A− 1Co =
[ 2
−1
]
,
which is not a valid solution as Cs are the parameters of a categorical distribution. Thus, the eleme nts
of Cs should add up to one, and be between zero and one. Importantly, ﬁ nding a renormalization ¯Cs
of Cs is impossible. Indeed, if A is invertible, then the inverse is unique. Thus, Cs is the only matrix
that satisﬁes: Cs = A− 1Co.
6.1.2 The class of valid prior preferences over observations
The problem described in the previous section occurs because we ar e deﬁning two distributions over the
random variable ot+1, and these distributions are not compatible with each other. This ind icates that in
the setting of Section 5, one cannot deﬁned the prior preference s over observations arbitrarily. Instead,
given the likelihood mapping T (ot+1|st+1) = Cat( ot+1|st+1; A), one can only deﬁne prior preferences over
states, i.e., T (st+1|at) = Cat( st+1; Cs), Then, the prior preferences over observations can be comput ed
as follows:
T (ot+1|at) =
∑
st+1
T (ot+1|st+1, at)T (st+1|at) ⇔Co = ACs. (9)
While this tells us how the prior over observations can be computed, it does not characterise the class
of all valid prior preferences over observations. To characterise this class, we need to remember that
n ×n matrices can be understood as linear transformations of the n-dimensional Euclidean space. To
understand this, let’s take the following 2 ×2 matrix as an example:
B =


| |
⃗b1 ⃗b2
| |

 ,
and let:
⃗i =
[
1
0
]
, and: ⃗j =
[
0
1
]
,
be the two standard basis vectors. One can see that: ⃗b1 = B⃗i and ⃗b2 = B⃗j. In other words, the
matrix B transforms the vector ⃗i into the vector ⃗b1, and the vector ⃗j into the vector ⃗b2. More generally,
the matrix B transforms each vector ⃗ xinto a vector ⃗ y= B⃗ x. Geometrically, this can be understood
11
as mapping each point of the Euclidean space ⃗ x, to another point ⃗ yin the space. For example, the
transformation corresponding to the following matrix:
B =
[
0.5 0 .5
0.5 −0.5
]
,
is illustrated in Figure 1. Importantly, since the transformation is line ar, the lines of the grid in Figure
1 remain parallel and equally spaced. Thus, knowing where ⃗i and ⃗j land under the transformation B is
enough to know where all the other points on the grid land, i.e., ⃗i and ⃗j deﬁnes one parallelogram and all
the others (parallelograms) are obtained by copy and pasting this p arallelogram along the transformed
axes. Going back to the following equation: Co = ACs, we can now better understand which prior
preferences over observations Co are compatible with the likelihood mapping A.
⃗ a2
⃗ a1
⃗j
⃗i
Figure 1: This ﬁgure illustrates how matrices can be seen as linear tra nsformations of the Euclidean
space. The example taken is a matrix that rotates the Euclidean spa ce by 45 degree clockwise
and scales each axis by a factor of 0.5.
Note that Co is a linear transformation of Cs, where the transformation is deﬁned by the elements of the
matrix A. Moreover, Cs are the parameters of a categorical distribution, which means tha t its elements
are positive and sum up to one. Geometrically, this means that Cs is in the 1-dimensional simplex of
the Euclidean space. Additionally, since A deﬁnes the probability of each observation given each state,
all the elements of A are positive and the columns of A sum up to one. In other words, the columns of
A correspond to points on the 1-dimensional simplex of the Euclidean s pace.
Figure 2 illustrates the linear transformation corresponding to the A matrix of the previous section, c.f.,
Equation 8. Recall that the columns of A (i.e., ⃗ a1 and ⃗ a2) correspond to points on the 1-dimensional
simplex (represented in blue on the left of Figure 2). Importantly, t he standard basis vector ⃗i and ⃗j are
mapped by A to ⃗ a1 and ⃗ a2, respectively. While this is happening, the gray grid (Figure 2 left) will be
squeezed into the red grid (Figure 2 right), and the 1-dimensional s implex represented by a long blue
segment will be squeezed into a shorter blue segment. This short blu e segment is the class of valid prior
preferences over observations Co.
Indeed, the matrix A− 1 performs the inverse linear transformation, i.e., A− 1 transforms the red grid
into the gray grid. Therefore, if a point is on the short blue segment in the right-hand-side of Figure
2, it will be mapped back to the original 1-dimensional simplex (the long blue segment). However, if a
point starts on the blue dotted line (outside the short blue segment ), it will be mapped outside of the
original 1-dimensional simplex.
12
To conclude, the class of valid prior preferences over observation s is the class of all vectors Co that
can be obtained as a linear combination of the columns of A, where the weights of the linear combina-
tion are positive numbers between zero and one that sum up to one, i.e., all the vectors that satisﬁes the
equation Co = ACs. Geometrically, the class of valid prior preferences over observat ions corresponds
to the short blue segment obtained by applying A to the 1-dimensional simplex.
✗
✓ ✗✓
Figure 2: This ﬁgure illustrates the linear transformation correspo nding to the A matrix in Equation
8. The gray grid represents a set of points in the Euclidean space on which the linear trans-
formation corresponding to the A matrix will be applied. The red grid represents where the
gray grid lands when applying this linear transformation. The long (blu e) segment on the
left-hand-side of the ﬁgure corresponds to the 1-dimensional sim plex in which the prior pref-
erences over states Cs lives. The short (blue) segment on the right-hand-side of the ﬁgur e
corresponds to where the 1-dimensional simplex lands when applying the linear transforma-
tion, i.e., the class of valid prior preferences over observations Co. The green point ( ✓ on
the right-hand-side) corresponds to a point that lives on the proj ected simplex (i.e., the short
blue segment). This point will be projected back to the original 1-dim ensional simplex (i.e.,
the long blue segment) by A− 1. The brown point ( ✗ on the right-hand-side) corresponds to
a point that lives outside the short blue segment. This point will be pro jected outside of the
long blue segment by A− 1.
6.2 Justiﬁcation of the expected free energy
In this section, we discuss the expected free energy justiﬁcation presented in Appendix B of Parr et al.
(2022). The appendix derives CRSA(
a) from the assumption that an agent aims to reach its prior
preferences, i.e., F (sτ ) = T (sτ |aτ− 1) for some time step τ in the future. Importantly, this derivation
provides a justiﬁcation for CRSA(a), i.e., an upper bound of Grt(a) as deﬁned in Section 5, but not for
Grt(a) itself. Thus, the expected free energy of Section 5, i.e., CROA(a), remains justiﬁed only by its
intuitive deﬁnition. To resolve this issue, we could try to deﬁne the ex pected free energy as the risk over
states plus ambiguity:
CROA(a) = CIGP V (a) ≤CRSA(a) = C3E (a) =∆ Grt(a).
In this case, we have a justiﬁcation for the expected free energy , i.e., CRSA(a), but CROA(a) and CIGP V (a)
are now lower bounds of Grt(a). As minimizing a lower bound of the expected free energy does not im ply
minimizing the expected free energy, CROA(a) and CIGP V (a) cannot be recovered in this setting. The
results of the derivations presented in this paper are summarized in Table 1. Out of two potential
deﬁnitions for the expected free energy, one is justiﬁed but can o nly recover two EFE formulations, and
the other is not currently justiﬁed but can recover the four form ulations.
13
CIGP GCRSACROAC3E
justiﬁed
risk over states vs ambiguity as deﬁnition ✗ ✓ ✗ ✓ ✓
risk over observations vs ambiguity as deﬁnition ✓ ✓ ✓ ✓ ✗
Table 1: This table summarises the results of the derivations of the e xpected free energy formulations,
and whether a justiﬁcation of the EFE has been provided in the litera ture. The ﬁrst row
corresponds to the deﬁnition of Parr et al. (2022) where the expe cted free energy is deﬁned
as the risk over states vs ambiguity formulation, while the second ro w corresponds to another
interpretation of Parr et al. (2022) where the expected free ene rgy is deﬁned as the risk over
observations vs ambiguity formulation. Cells containing ✗ mean that the formulation cannot
be recovered (or no justiﬁcation is known), while cells containing ✓ correspond to the case
where the formulation can be recovered (or a justiﬁcation is known ).
7. Conclusion
This paper aimed to formalize the expected free energy deﬁnition, a s well as the problem of deriving
its four formulations, i.e., the uniﬁcation problem. When the expecte d free energy is deﬁned as the
risk over observations plus ambiguity, all formulations can be recov ered, and can therefore be used in
practice. However, an important contribution of this paper was to show that some prior preferences over
observations are incompatible with the likelihood mapping. Thus, we ar e left with a dilemma, either
the modeller has to carefully pick the prior preferences of the agen t to avoid any conﬂict, or we have to
let go of the theoretical connection between the four formulation s.
Another issue is the absence of a justiﬁcation for the risk over obs ervations plus ambiguity formula-
tion. While there exists a justiﬁcation for the risk over states plus a mbiguity formulation, justifying a
lower bound is not enough to justify the expected free energy. Th erefore, future research should focus
on ﬁnding a derivation of the risk over observations plus ambiguity fr om ﬁrst principles. Importantly,
while the risk over states plus ambiguity is justiﬁed, this deﬁnition of t he expected free energy does not
allow us to recover the four formulations. Thus, it does not constit ute a valid solution to the uniﬁcation
problem.
Note, we only studied two possible deﬁnitions of the expected free e nergy. An alternative set of
proofs and/or another factorization of the forecast and targe t deﬁnitions may allow us to recover all four
decompositions, while also removing the conﬂict between prior prefe rences and likelihood. However,
testing all possible factorizations and proofs is outside the scope o f this paper.
Finally, this paper provides a solid foundation for future research, especially in the context of deep
active inference. Indeed, this paper clariﬁes the expected free e nergy deﬁnition, but unfortunately, it
does explain how to compute it using deep neural networks. Thus, a dditional research is required to
implement and empirically evaluate the proposed expected free ener gy deﬁnition.
References
Ozan C ¸ atal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Ba rt Dhoedt. Learn-
ing perception and planning with deep active inference. In 2020 IEEE International Con-
ference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020 , pages 3952–3956. IEEE, 2020. doi: 10.1109/ICASSP40776.2020 .9054364. URL
https://doi.org/10.1109/ICASSP40776.2020.9054364.
Th´ eophile Champion, Howard Bowman, and Marek Grze´ s. Branchin g time active in-
ference: Empirical study and complexity class analysis. Neural Networks , 152:450–
14
466, 2022a. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neu net.2022.05.010. URL
https://www.sciencedirect.com/science/article/pii/S0893608022001824.
Th´ eophile Champion, Lancelot Da Costa, Howard Bowman, and Mare k Grze´ s. Branch-
ing time active inference: The theory and its generality. Neural Networks , 151:295–
316, 2022b. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neu net.2022.03.036. URL
https://www.sciencedirect.com/science/article/pii/S0893608022001149.
Th´ eophile Champion, Marek Grze´ s, and Howard Bowman. Multi-mod al and multi-factor branching
time active inference, 2022c. URL https://arxiv.org/abs/2206.12503.
Th´ eophile Champion, Marek Grze´ s, and Howard Bowman. Branchin g Time Active Inference with
Bayesian Filtering. Neural Computation , 34(10):2132–2144, 09 2022d. ISSN 0899-7667. doi: 10.1162/
neco a 01529. URL https://doi.org/10.1162/neco_a_01529.
Th´ eophile Champion, Marek Grze´ s, Lisa Bonheme, and Howard Bow man. Deconstructing deep active
inference, 2023. URL https://arxiv.org/abs/2303.01618.
Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Act ive inference
in openai gym: A paradigm for computational investigations into psy chiatric ill-
ness. Biological Psychiatry: Cognitive Neuroscience and Neuroi maging, 3(9):809 –
818, 2018. ISSN 2451-9022. doi: https://doi.org/10.1016/j.bpsc .2018.06.010. URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617. Computational
Methods and Modeling in Psychiatry.
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopa mine, reward learning, and active
inference. Frontiers in Computational Neuroscience , 9:136, 2015. ISSN 1662-5188. doi: 10.3389/
fncom.2015.00136. URL https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl J. Fr iston. Deep ac-
tive inference agents using Monte-Carlo methods. In Hugo Laroch elle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, edit ors, Advances
in Neural Information Processing Systems 33: Annual Confer ence on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL
https://proceedings.neurips.cc/paper/2020/hash/865dfbde8a344b44095495f3591f7407-Abstract.htm 
Charles W. Fox and Stephen J. Roberts. A tutorial on variational B ayesian inference. Artiﬁcial In-
telligence Review , 38(2):85–95, Aug 2012. ISSN 1573-7462. doi: 10.1007/s10462- 011-9236-8. URL
https://doi.org/10.1007/s10462-011-9236-8 .
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwarte nbeck, John O Doherty, and Gio-
vanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews , 68:862 – 879,
2016. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev .2016.06.022.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, a nd Thomas Parr. Sophisticated In-
ference. Neural Computation , 33(3):713–763, 03 2021. ISSN 0899-7667. doi: 10.1162/neco a 01351.
URL https://doi.org/10.1162/neco_a_01351.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical br ain: Belief propagation and active
inference. Network Neuroscience , 1(4):381–414, 12 2017. ISSN 2472-1751. doi: 10.1162/NETN a
00018. URL https://doi.org/10.1162/NETN_a_00018.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human a ttention. Vision Research , 49
(10):1295 – 1306, 2009. ISSN 0042-6989. doi: https://doi.org/10 .1016/j.visres.2008.09.007. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Visual Attention:
Psychophysics, electrophysiology and neuroimaging.
15
Edwin T Jaynes. Information theory and statistical mechanics. Physical review , 106(4):620, 1957a.
Edwin T Jaynes. Information theory and statistical mechanics. ii. Physical review , 108(2):171, 1957b.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and technique s. MIT press,
2009.
Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor gr aphs and the sum-product algorithm.
IEEE Transactions on information theory , 47(2):498–519, 2001.
Beren Millidge. Combining active inference and hierarchical predictive coding: A tutorial introduction
and case study., 2019. URL https://doi.org/10.31234/osf.io/kf6wc.
Thomas Parr, Dimitrije Markovic, Stefan J Kiebel, and Karl J Friston . Neuronal message passing using
mean-ﬁeld, bethe, and marginal approximations. Scientiﬁc reports , 9(1):1889, 2019.
Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind,
brain, and behavior . MIT Press, 2022.
Cansu Sancaktar, Marcel A. J. van Gerven, and Pablo Lanillos. End -to-end pixel-based deep ac-
tive inference for body perception and action. In Joint IEEE 10th International Conference on
Development and Learning and Epigenetic Robotics, ICDL-Ep iRob 2020, Valparaiso, Chile, Octo-
ber 26-30, 2020 , pages 1–8. IEEE, 2020. doi: 10.1109/ICDL-EpiRob48136.2020.92 78105. URL
https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105.
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Th omas H B FitzGer-
ald, Martin Kronbichler, and Karl Friston. Computational mechanis ms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
John Winn, Christopher M Bishop, and Tommi Jaakkola. Variational m essage passing. Journal of
Machine Learning Research , 6(4), 2005.
Appendix A: comments, questions, answers, and proofs
In this paper, we used the following blocks:
A comment.C
A question.Q
An answer.A
A proof.P
Appendix B: important properties
In this paper, we made extensive use of four basic properties. The ﬁrst is the sum-rule of probability.
Given two sets of random variables X and Y , the sum-rule of probability states that:
P (Y ) =
∫
X
P (X, Y ) dX. (10)
16
The sum-rule can then be used to sum out random variables from a jo int distribution. The second
property is called the product-rule of probability, and can be used t o split a joint distribution into
conditional distributions. Given two sets of random variables X and Y , the product-rule of probability
states that:
P (X, Y ) = P (X|Y )P (Y ). (11)
Next, if a and b are two real numbers, then a relevant property of logarithm is the following:
ln(a ×b) = ln( a) + ln( b). (12)
Put simply, this allows us to turn the logarithm of a product into a sum o f logarithms, and we will
refer to the above equation as the “log-property”. Finally, the las t property is called the linearity of
expectation. Given a random variable X, and two real numbers a and b, the linearity of expectation
states that:
E[aX + b] = aE[X] + b, (13)
where the expectation is w.r.t. the marginal distribution over X, i.e., P (X).
Appendix C: d-separation criterion
Given a Bayesian network, the d-separation criterion provides a br idge between the graph topology and
the independence assumptions holding within the Bayesian network. Let Grt = ( V, E) be a directed graph
corresponding to a Bayesian network, where Vand Eare the graph’s vertices and edges, respectively.
A trail is a sequence of vertices ( V1, V2, ..., Vk) such that there is an edge Vi →Vi+1 or Vi+1 →Vi for all
i ∈{1, ..., k −1}. Intuitively, trails connect two variables V1 and Vk; conceptually if a trail is blocked,
then V1 does not provide any new information about Vk through this trail. The notion of blocked trail
is based on colliders:
Deﬁnition 1 (Collider) Within a trail (V1, V2, ..., Vk), a collider is a vertex Vj s.t. Vj− 1 →Vj ←Vj+1.
Deﬁnition 2 (Blocked trail) Given a set of vertices S ⊆V, and two vertices V1, V2 ∈V, we say that
a trail between V1 and V2 is blocked by S if at least one node of the trail is a collider not in S and with
no descendants in S, or at least one vertex of the trail that is not a collider is in S.
Importantly, two vertices in the graph can be connected through multiple trails; if all trails between
these two vertices are blocked, then we say that these vertices a re d-separated. This can be generalized
to sets of vertices as shown below:
Deﬁnition 3 (D-separated) Given a set of vertices S ⊆V, and two vertices V1, V2 ∈V, we say that
V1 and V2 are d-separated by S if all trails between V1 and V2 are blocked.
Given three sets of vertices V1, V2, S ⊆V. We say that V1 and V2 are d-separated by S, if each
node in V1 is d-separated from all nodes in V2 given the nodes in S.C
Finally, the d-separation theorem states that: if two sets of vert ices are d-separated in the graph, then
the associated random variables are conditionally independent, i.e.,
Theorem 4 (d-separation and independence) Given three sets V1, V2, S ⊆V. If V1 and V2 are
d-separated by S, then V1 and V2 are conditionally independent given S, i.e., V1 ⊥ ⊥V2 |S.
In practice, the d-separation theorem is used on the generative m odel’s graph. The goal is to know
whether a conditional assumption holds, i.e., whether V1 ⊥ ⊥V2 |S holds. If it does, then: P (V1|S, V2) =
P (V1|S).
17