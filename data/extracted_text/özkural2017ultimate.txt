7102
peS
8
]IA.sc[
1v97830.9071:viXra
Ultimate Intelligence Part III: Measures of
Intelligence, Perception and Intelligent Agents
Eray O¨zkural
G¨ok Us Sibernetik Ar&GeLtd.S¸ti.
Abstract. We propose that operator induction serves as an adequate
model of perception. We explain how to reduce universal agent models
to operator induction. We propose a universal measure of operator in-
ductionfitness,andshowhowitcanbeusedinareinforcement learning
model and a homeostasis (self-preserving) agent based on the free en-
ergyprinciple.Weshowthattheaction ofthehomeostasis agentcan be
explained bythe operator induction model.
“Wir mu¨ssen wissen – wir werden wissen!”
— David Hilbert
1 Introduction
The ultimate intelligence research program is inspired by Seth Lloyd’s work on
the ultimate physical limits to computation [15]. We investigate the ultimate
physical limits and conditions of intelligence. This is the third installation of
thepaperseries,thefirsttwopartsproposednewphysicalcomplexitymeasures,
priors and limits of inductive inference [18,17].
We frame the question of ultimate limits of intelligence in a generalphysical
setting, for this we provide a general definition of an intelligent system and a
physical performance criterion, which as anticipated turns out to be a relation
of physical quantities and information, the latter of which we had conceptually
reduced to physics with minimum machine volume complexity in [18].
2 Notation and Background
2.1 Universal Induction
Let us recallSolomonoff’s universaldistribution [21]. Let U be a universalcom-
puter which runs programs with a prefix-free encoding like LISP; y = U(x)
denotes that the output of program x on U is y where x and y are bit strings.
1Anyunspecifiedvariableorfunctionisassumedtoberepresentedasabitstring.
1
A prefix-free code is a set of codes in which no code is a prefix of another. A com-
puterfileusesaprefix-freecode,endingwithanEOFsymbol,thus,mostreasonable
programming languages are prefix-free.
2 Eray O¨zkural
|x| denotes the length of a bit-string x. f(·) refers to function f rather than its
application.
The algorithmic probability that a bit string x ∈ {0,1}+ is generated by a
random programπ ∈{0,1}+ of U is:
P (x)= 2−|π| (1)
U X
U(π)∈x(0|1)∗∧π∈{0,1}+
which conforms to Kolmogorov’saxioms [13]. P (x) considers any continuation
U
of x, taking into account non-terminating programs.2 P is also called the uni-
U
versal prior for it may be used as the prior in Bayesian inference, for any data
can be encoded as a bit string.We also give the basic definitions of Algorithmic
InformationTheory (AIT) [14], where the algorithmicentropy,or complexity of
a bit string x∈{0,1}+ is
H (x)=min({|π| | U(π)=x}) H∗(x)=−log P (x) (2)
U U 2 U
We use some variables in overloadedfashion in the paper, e.g., π might be a
program, a policy, or a physical mechanism depending on the context.
2.2 Operator induction
Operator induction is a general form of supervised machine learning where we
learnastochasticmapfromn questionandanswerpairsD ={(q ,a )}sampled
i i
from a (computable) stochastic source µ. Operator induction can be solved by
findinginavailabletimeasetofoperatorsOj(·|·),eachaconditionalprobability
density function (cpdf), such that the following goodness of fit is maximized
Ψ = ψj (3)
X n
j
for a stochastic source µ where each term in the summation is the contribution
of a model:
n
ψj =2−|(Oj(·|·)| Oj(a |q ). (4)
n Y i i
i=1
q anda arequestion/answerpairsintheinputdatasetdrawnfromµ,andOj is
i i
a computable cpdf in Equation 4. We can use the found m operators to predict
unseen data with a mixture model [24]
m
P (a |q )= ψjOj(a |q ) (5)
U n+1 n+1 X n n+1 n+1
j=1
The goodness of fit in this case strikes a balance between high a priori proba-
bility andreproductionofdatalikeinminimummessagelength(MML)method
[27,26],yetusesauniversalmixturelikeinsequenceinduction.Theconvergence
theorem for operator induction was proven in [23] using Hutter’s extension to
arbitraryalphabet,anditboundstotalerrorbyH (µ)ln2similarlytosequence
U
induction.
2
Weused theregular expression notation in language theory.
Measuring Intelligence 3
2.3 Set induction
Set induction generalizesunsupervisedmachine learning wherewe learna prob-
ability density function (pdf) from a set of n bitstrings D = {d ,d ,...,d }
1 2 n
sampledfromastochasticsourceµ.We cantheninductivelyinfernewmembers
to be added to the set with:
P (D∪d )
P(d )= U n+1 (6)
n+1
P (D)
U
Setinduction is clearlyarestrictedcaseofoperatorinduction wherewe setQ ’s
i
to null string. Set induction is a universal form of clustering, and it perfectly
models perception. If we apply set induction over a large set of 2D pictures of
a room, it will give us a 3D representation of it necessarily. If we apply it to
physical sensor data, it will infer the physical theory – perfectly general, with
infinite domains – that explains the data, perception is merely a specific case
of scientific theory inference in this case, though set induction works both with
deterministic and non-deterministic problems.
2.4 Universal measures of intelligence
There is much literature on the subject of defining a measure of intelligence.
Hutter has defined an intelligence order relation in the context of his univer-
salreinforcementlearning(RL)modelAIXI [8],whichsuggeststhatintelligence
correspondstothesetofproblemsanagentcan solve.Alsonotableistheuniver-
sal intelligence measure [10,11], which is again based on the AIXI model. Their
universal intelligence measure is based on the following philosophical definition
compiled from their review of definitions of intelligence in the AI literature.
Definition 1 (Legg & Hutter). Intelligence measures an agent’s ability to
achieve goals in a wide range of environments.
It implies that intelligence requires an autonomous goal-following agent. The
intelligence measure of [10] is defined as
Υ(π)= 2−HU(µ)Vπ (7)
X µ
µ∈E
where µ is a computable reward bounded environment, And Vπ is the ex-
µ
pected sum of future rewards in the total interaction sequence of agent π.
Vπ = E [ ∞ γtr ], where r is the instantaneous reward at time t gener-
at µ edfrom µ,π th P e i t n = t 1 eract t ionbetwee t n the agentπ andthe environmentµ,and γt is
the time discount factor.
2.5 The free energy principle.
InAsimov’sstorytitled“TheLastQuestion”,thetaskoflifeisidentifiedasover-
coming the second law of thermodynamics, however futile. Variational free en-
ergyessentiallymeasurespredictiveerror,anditwasintroducedbyFeynmannto
4 Eray O¨zkural
address difficult path integral problems in quantum physics. In thermodynamic
free energy, energies are negative log probabilities like entropy. The free energy
principle states that any system must minimize its free energy to maintain its
order.Anadaptivesystemthattendstominimizeaveragesurprise(entropy)will
tend to survive longer. A biological organism can be modelled as an adaptive
systemthathasanimplicitprobabilisticmodeloftheenvironment,andthevari-
ational free energy puts an upper bound on the surprise, thus minimizing free
energy will improve the chances of survival. The divergence between the pdf of
environment and an arbitrary pdf encoded by its own mechanism is minimized
in Friston’s model [9]. It has been shown in detail that the free energy principle
adequatelymodelsaself-preservingagentinastochasticdynamicalsystem[6,9],
whichwecaninterpretasanenvironmentwithcomputablepdf.Anactiveagent
maybedefinedintheformalismofstochasticdynamicalsystems,bypartitioning
the physical states X of the environment into X =E×S×A×Λ where e∈E
is an external state, s ∈ S is a sensory state, a ∈ A an active state, and λ ∈ Λ
is an internal state. Self-preservation is defined by the Markov blanket S ×A,
the removal of which partitions X into external states E and internal states Λ
that influence each other only through sensory and action states. E influences
sensations S, which in turn influence internal states Λ, resulting in the choice
of action signals S, which impact E, forming the feedback loop of the adaptive
system. The system states x∈X evolve according to the stochastic equation:
f (e,s,a)
e
 
f (e,s,a)
x˙(t)=f(x)+ω (8) f(x)=  s  (10)
f
a
(s,a,λ)
x(0)=x 0 (9)  f (s,a,λ)  
λ
where f(x) is the flow of system states and it is decomposed into flows over
thesetsinthesystempartition,explicitlyshowingthedependenciesamongstate
sets;ωmodelsfluctuations.Fristonformalizestheself-preservation(homeostasis)
problem as finding an internal dynamics that minimizes the uncertainty (Shan-
non entropy)of the externalstates, and shows a solution based onthe principle
ofleastaction[9]whereinminimizingfreeenergyissynonymouswithminimizing
the entropyof the externalstates (principle ofleastaction),whichsubsequently
correspondstoactiveinference.Wehavespaceforonlysomekeyresultsfromthe
rather involved mathematical theory. p(s,f|m) is the generative pdf that gen-
erates sensorium s and fictive (hidden) states f ∈ F from probabilistic model
m, and q(f|λ) is the recognition pdf that predicts hidden states F in the world
given internal state. Generative pdf factorizes as p(s,f|m) = p(s|f,m)p(f|m).
Free energy is defined as energy minus entropy
F(s,λ)=E [−lnp(s,f|m)]−H(q(f|λ)) (11)
q
which can be subjectively computed by the system. Free energy is also equal to
surprise plus divergence between recognition and generative pdf’s.
F(s,λ)=E [−lnp(s,f|m)]+D (q(f|λ)||p(f|s,m)) (12)
q KL
Measuring Intelligence 5
Minimizingdivergenceminimizesfreeenergy,internalstatesλmaybeoptimized
to minimize predictive error using Equation 12, and surprise is invariant with
respect to λ. Free energy may be formulated as complexity plus accuracy of
recognition, as well.
F(s,λ)=E [−lnp(s,a|f,m)]+D (q(f|λ)||p(f,m)) (13)
q KL
Inthiscase,wemaychooseanactionthatchangessensationstoreducepredictive
error. Only the first term is a function of action signals. Minimization of free
energy turns out to be equivalent to the information bottleneck principle of
Tishby[9,25].Theinformationbottleneckmethodisequivalenttothepioneering
work of Ashby, which is simple enough to state here [3,2]:
S =I(λ;F)−I(S;λ) (14)
B
where the first term is the mutual information between internal and hidden
states, and the second term is the mutual information between sensory states
and internal states. Both terms are expanded using conditional entropy, and
then two terms in the middle are eliminated because they are not relevant to
the optimizationproblem–we donotknow the hiddenvariablesinH(λ|F) and
H(S) is constant.
S =H(λ)−H(λ|F)−H(S)+H(S|λ) (15)
B
S∗ =H(λ)+H(S|λ) (16)
B
Minimizing S∗ Equation 16 thus minimizes the sum of the entropy of internal
B
statesandtheentropyrequiredtoencodesensorystatesgiveninternalstates.In
other words,it strikesan optimalbalance between modelcomplexity H(λ), and
modelaccuracyH(S|λ).Fristonfurther showsthatEquation16directlyderives
from the free energy principle, closing potential loopholes in the theory. Please
see[5]foracomprehensiveapplicationofthefreeenergyprincipletoagentsand
learning. Note also that the bulk of the theory assumes the ergodic hypothesis.
3 Perception as General Intelligence
Since we are chiefly interested in stochastic problems in the physical world, we
propose a straightforwardinformal definition of intelligence:
Definition 2. Intelligence measures the capability of a mechanism to solve pre-
diction problems.
Mechanism is any physical machine as usual, see [4] which suggests likewise.
Therefore, a general formulation of Solomonoff induction, operator induction,
might serve as a model of general intelligence, as well [24]. Recall that operator
induction can infer any physically plausible cpdf, thus its approximation can
solve any classical supervised machine learning problem. The only slight issue
with Equation7 might be that it seems to exclude classicalAI systems that are
not agents, e.g., expert systems, machine learning tools, knowledge representa-
tionsystems,searchandplanningalgorithms,andsoforth,whicharesomewhat
more naturally encompassed by our informal definition.
6 Eray O¨zkural
3.1 Is operator induction adequate?
Aquestionnaturallyarisesastowhetheroperatorinductioncanadequatelysolve
every prediction problem we require in AI. There are two strong objections to
operatorinductionthatweknowof.Itisarguedthatinadynamicenvironment,
as in a physicalenvironment,we must use anactive agentmodel sothat we can
account for changes in the environment, as in the space-time embedded agent
[16]whichalsoprovidesanagent-basedintelligencemeasure.Thisobjectionmay
be answered by the simple solution that each decision of an active intelligent
system may be considered a separate induction problem. The second objection
is that the basic Solomonoff induction can only predict the next bit, but not
the expected cumulative reward, which its extensions can solve. We counter
this objection by stating that we can reduce an agent model to a perception
andaction-planningproblemas inOOPS-RL[20]. In OOPS-RL,the perception
module searches for the best world-model given the history of sensory input
and actions in allotted time using OOPS, and the planning module searches
for the best control program using the world-model of the perception module
to determine the action sequence that maximizes cumulative reward likewise.
OOPShasageneralizedLevinSearch[12]whichmaybe tweakedto solveeither
prediction or optimization problems. Hutter has also observed that standard
sequenceinductiondoesnotreadilyaddressoptimizationproblems[8].However,
Solomonoff induction is still complete in the sense of Turing, and can infer any
computable cpdf; and when the extension to Solomonoff induction is applied to
sequence prediction, it does not yield a better error bound, which seems like a
conundrum.Ontheotherhand,LevinSearchwithaproperuniversalprobability
density function (pdf) of programscanbe modified to solve induction problems
(sequence, set, operator, and sequence prediction with arbitrary loss), inversion
problems(computer science problemsinPandNP),andoptimizationproblems
[23]. The planning module of OOPS-RL likewise requires us to write such an
optimizationprogram.Inthatsense,AIXIimpliesyetanothervariationofLevin
Search for solving a particular universal optimization problem, however, it also
has the unique advantage that formal transformations between AIXI problem
and many important problems including function minimization and strategic
games have been shown [8]. Nevertheless, the discussion in [23] is rather brief.
Also see [1] for a discussion of universal optimization.
Proposition 1. A discrete-time universalRLmodel may be reducedtooperator
induction.
More formally, the perceptual task of an RL agent would be inferring from a
history the cumulative rewards in the future, without loss of generality.Let the
chronologyC beasequenceofsensory,reward,andactiondataC =[(s ,r ,a ),
1 1 1
(s ,r ,a ),...,(s ,r ,a )] where C accesses ith element, and C accesses the
2 2 2 n n n i i:j
subsequence [C ,C ,...,C ]. Let r be the cumulative reward function where
i i+1 j c
r (C,i,j) = k≤jr . After observing (s ,r ,a ), we construct dataset D as
c Pk=i k n n n c
follows. For every unique (i,j) pair such that 1 < i ≤ j ≤ n, we concatenate
historytuplesC ,andweformaquestionstringthatalsoincludesthenext
1:(i−1)
Measuring Intelligence 7
action,iandj,q =[(s ,r ,a ),(s ,r ,a ),...,(s ,r ,a )],a ,i,j,and
1 1 1 2 2 2 (i−1) (i−1) (i−1) i
an answer string which is the cumulative rewarda=r (C,i,j). Solving the op-
c
erator induction problem for this dataset D will yield a cpdf which predicts
C
cumulativerewardsinthefuture.Afterthat,choosingthenextactionisasimple
matter of maximizing r(C ,a ,n+1,λ) where λ is the planning horizon. The
1:n i
reductioncausesquadraticblow-upinthe number ofdata items.Oursomewhat
cumbersome reduction suggests that all of the intelligence here comes from op-
erator induction, surely an argmax function, or a summation of rewards does
not provide it, but rather it builds constraints into the task. In other words,
we interpret that the intelligence in an agent model is provided by inductive
inference, rather than an additional application of decision theory.
4 Physical Quantification of Intelligence
Definition 1 correspondsto any kind of reinforcement-learningor goal-following
agentinAIliteraturequitewell,andcanbeadaptedtosolveotherkindsofprob-
lems. The unsupervised, active inference agent approach is proposed instead of
reinforcement learning approachin [7], and the authors argue that they did not
need to invoke the notion of reward, value or utility. The authors in particu-
lar claim that they could solve the mountain-car problem by the free-energy
formulation of perception. We thus propose a perceptual intelligence measure.
4.1 Universal measure of perception fitness
Notethatoperatorinductionisconsideredtobeinsufficienttodescribeuniversal
agentssuchasAIXI,becausebasicsequenceinductionisinappropriateformod-
ellingoptimizationproblems[8].However,amodifiedLevinsearchprocedurecan
solve such optimization problems as in finding an optimal control program[20].
In OOPS-RL, the perception module searches for the best world-model given
the history of sensory input and actions in allotted time using OOPS, and the
planningmodule searchesforthebestcontrolprogramusingthe world-modelof
the perception module to determine the controlprogramthat maximizes cumu-
lative rewardlikewise. In this paper, we consider the perception module of such
a generic agent which must produce a world-model, given sensory input.
We can use the intelligence measure Equation 7 in a physical theory of in-
telligence, however it contains terms like utility that do not have physical units
(i.e., we would be preferring a more reductive definition). We therefore attempt
to obtain such a measure using the more benign goodness-of-fit (Equation 3).
Let the universal measure of the fitness of operator induction be defined as
Υ (π)= 2−HU(µ)Ψ(µ,π) (17)
O X
µ∈S
where S is the set of possible stochastic sources in the observable universe U
and π is a physical mechanism, and Ψ is relative to a stochastic source µ and a
8 Eray O¨zkural
physical mechanism (computer) π. This would be maximum if we assume that
operator induction were solved exactly by an oracle machine.
Note that H (µ) is finite; Ψ(µ,π) is likewise bounded by the amount of
U
computation π will spend on approximating operator induction.
4.2 Application to homeostasis agent
In a presentation to Friston’s group in January 2015, we noted that the mini-
mizationofS∗ is identicalto MinimumMessageLengthprinciple,whichcanbe
B
further refined as
S′ =H∗(Λ)+H∗(S|Λ) (18)
B
using Solomonoff’s entropy formulation that takes the negative logarithm of al-
gorithmic probability [22]. In the unsupervised agent context, solving this min-
imization problem corresponds to inferring an optimal behavioral policy as Λ
constitutes internal dynamics which may be modeled as a non-terminating pro-
gram. We could directly apply induction to minimize KL divergence, as well.
Note the correspondence to operator induction.
Theorem 1. Minimizing the free energy is equivalent to solving the operator
induction problem for (λ,s) pairs where q ∈Λ and a ∈S.
i i
Proof. Observe that minimizing Equation 16 corresponds to picking maximum
ψj since in entropy form,
n
n
−log (ψj)=−log (2−|Oj(·|·)|)−log ( Oj(s |λ ))
2 n 2 2 Y i i
i=1
n
=|Oj(·|·)|− log (Oj(a |q ))=|Oj(·|·)|+H(Oj(a |q )).
X 2 i i i i
i=1
We define a non-redundant selection of ψj’s, |Oj(·|·)| = H (Oj(·|·)), e.g., we
n U
pick only the shortest programs that produce the same cpdf, otherwise the en-
tropyformwoulddiverge.MinimizingEquation18isexactly operatorinduction,
even though the questions are programs, the ensemble here is of all programs
and all sensory state, program pairs in space-time. |Oj(·|·)| = H∗(Λ) and
H(Oj(a |q )) = H∗(S|Λ). Note that this merely e P stablishes model equiva-
i i
P
lence, we have not yet explained how it is to be computed in detail.
Proposition 2. By the above theorem, Equation 17 measures the goodness of
fit for a given homeostasis agent mechanism, for all possible environments.
The mechanism π that maximizes Ψ(µ,π) achieves less error with respect to
a source (which may be taken to correspond to the whole random dynamical
systemintheframeworkoffreeenergyprinciple),whileΥ (π)normalizesΨ(µ,π)
O
withrespecttoarandomdynamicalsystem.ItholdsforthesamereasonsLegg’s
Measuring Intelligence 9
measureholds,whicharenotdiscusseddue tospacelimits inthe presentpaper.
We prefer the unsupervised homeostasis agent among the two agent models we
discussedbecauseitprovidesanexceptionallyelegantandreductionistmodelof
autonomousbehavior,thathasbeenrigorouslyformulatedphysically.Notethat
thisagentisconceptuallyrelatedtothesurvivalpropertyofRLagentsdiscussed
in [19].
4.3 Discussion
Theunsupervisedmodelstillachievesexplorationandcuriosity,becauseitwould
stochastically sample and navigate the environment to reduce predictive errors.
Whileweeitheroptimizeperceptualmodelsorchooseanactionthatwouldbefit
expectations,itmightbepossibletoexpresstheoptimaladaptiveagentpolicyin
ageneraloptimizationframework.Amorein-depthanalysisoftheunsupervised
agent will be presented in a subsequent publication. A more general reductive
definition of intelligence should also be researched. These developments could
eventually help unify AGI theory.
References
1. Alpcan, T., Everitt, T., Hutter, M.: Can we measure the difficulty of an op-
timization problem? In: 2014 IEEE Information Theory Workshop, ITW 2014,
Hobart, Tasmania, Australia, November 2-5, 2014. pp. 356–360. IEEE (2014),
http://dx.doi.org/10.1109/ITW.2014.6970853
2. Ashby, W.R.: Principles of the self-organizing system. In: v. Foerster, H., Zopf,
G.W. (eds.) Principles of Self-Organization: Transactions of the University of Illi-
nois Symposium, pp.255–278. Pergamon, London (1962)
3. Ashby,W.:Principles of theself-organizing dynamicsystem. TheJournal ofGen-
eral Psychology 37(2), 125–128 (1947)
4. Dowe, D.L., Hern´andez-Orallo, J., Das, P.K.: Artificial General Intelligence: 4th
InternationalConference,AGI2011,MountainView,CA,USA,August3-6,2011.
Proceedings, chap. Compression and Intelligence: Social Environments and Com-
munication, pp. 204–211. Springer Berlin Heidelberg, Berlin, Heidelberg (2011),
http://dx.doi.org/10.1007/978-3-642-22887-2_21
5. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P.,
ODoherty, J., Pezzulo, G.: Active inference and learning. Neu-
roscience and Biobehavioral Reviews 68, 862 – 879 (2016),
http://www.sciencedirect.com/science/article/pii/S0149763416301336
6. Friston, K., Kilner, J., Harrison, L.: A free energy principle for
the brain. Journal of Physiology-Paris 100(13), 70 – 87 (2006),
http://www.sciencedirect.com/science/article/pii/S092842570600060X,
theoretical and Computational Neuroscience: UnderstandingBrain Functions
7. Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforcement learn-
ing or active inference? PLOS ONE 4(7), 1–13 (07 2009),
https://doi.org/10.1371/journal.pone.0006421
8. Hutter, M.: Universal algorithmic intelligence: A mathematical top→down ap-
proach. In: Goertzel, B., Pennachin, C. (eds.) Artificial General Intelligence, pp.
227–290. Cognitive Technologies, Springer, Berlin (2007)
10 Eray O¨zkural
9. Karl,F.:Afreeenergyprincipleforbiologicalsystems.Entropy14(11),2100–2121
(2012), http://www.mdpi.com/1099-4300/14/11/2100
10. Legg, S., Hutter, M.: Universal intelligence: A definition of machine intelligence.
Minds Mach. 17(4), 391–444 (Dec 2007)
11. Legg, S., Veness, J.: An approximation of the universal intelligence measure. In:
Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelli-
gence,LectureNotesinComputerScience,vol.7070,pp.236–249. SpringerBerlin
Heidelberg (2013)
12. Levin,L.:Universalproblemsoffullsearch.ProblemsofInformationTransmission
9(3), 256–266 (1973)
13. Levin,L.A.:Sometheoremsonthealgorithmicapproachtoprobabilitytheoryand
information theory.CoRR abs/1009.5894 (2010)
14. Li, M., Vitanyi, P.M.: An Introduction to Kolmogorov Complexity and Its Appli-
cations. SpringerPublishing Company, Incorporated, 3 edn. (2008)
15. Lloyd,S.: Ultimate physical limits to computation. Nature406 (Aug2000)
16. Orseau, L., Ring, M.: Space-time embedded intelligence. In: Bach, J., Go-
ertzel, B., Ikl, M. (eds.) Artificial General Intelligence, Lecture Notes in
Computer Science, vol. 7716, pp. 209–218. Springer Berlin Heidelberg (2012),
http://dx.doi.org/10.1007/978-3-642-35506-6_22
17. O¨zkural, E.: Ultimate Intelligence Part II: Physical Measure and Complexity of
Intelligence. ArXiv e-prints(Apr2015)
18. O¨zkural,E.: Ultimateintelligence part I:physicalcompleteness and objectivity of
induction. In: Artificial General Intelligence - 8th International Conference, AGI
2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedings. pp. 131–141
(2015), http://dx.doi.org/10.1007/978-3-319-21365-1_14
19. Ring,M., Orseau,L.:Delusion,survival,and intelligent agents.In:ArtificialGen-
eral Intelligence, pp.11–20. Springer Berlin Heidelberg (2011)
20. Schmidhuber,J.: Optimal ordered problem solver. Machine Learning 54, 211–256
(2004)
21. Solomonoff, R.J.: A formal theory of inductive inference, part i. Information and
Control 7(1), 1–22 (March 1964)
22. Solomonoff, R.J.: Complexity-based induction systems: Comparisons and conver-
gencetheorems.IEEETrans.onInformationTheoryIT-24(4),422–432(July1978)
23. Solomonoff,R.J.:Progressinincrementalmachinelearning.Tech.Rep.IDSIA-16-
03, IDSIA,Lugano, Switzerland (2003)
24. Solomonoff, R.J.: Three kinds of probabilistic induction: Universal distributions
and convergence theorems. The Computer Journal 51(5), 566–570 (2008)
25. Tishby,N., Pereira, F.C., Bialek, W.: The information bottleneck method. ArXiv
Physics e-prints(Apr2000)
26. Wallace, C.S., Dowe, D.L.: Minimum message length and kol-
mogorov complexity. The Computer Journal 42(4), 270–283 (1999),
http://comjnl.oxfordjournals.org/content/42/4/270.abstract
27. Wallace, C.S.,Boulton, D.M.: Ainformation measurefor classification. Computer
Journal 11(2), 185–194 (1968)