arXiv:1709.03879v1  [cs.AI]  8 Sep 2017
Ultimate Intelligence Part III: Measures of
Intelligence, Perception and Intelligent Agents
Eray ¨Ozkural
G¨ ok Us Sibernetik Ar&Ge Ltd. S ¸ti.
Abstract. We propose that operator induction serves as an adequate
model of perception. We explain how to reduce universal agen t models
to operator induction. We propose a universal measure of ope rator in-
duction ﬁtness, and show how it can be used in a reinforcement learning
model and a homeostasis (self-preserving) agent based on th e free en-
ergy principle. We show that the action of the homeostasis ag ent can be
explained by the operator induction model.
“Wir m¨ ussen wissen – wir werden wissen!”
— David Hilbert
1 Introduction
The ultimate intelligence research program is inspired by Seth Lloyd’s w ork on
the ultimate physical limits to computation [15]. We investigate the ultim ate
physical limits and conditions of intelligence. This is the third installation of
the paper series, the ﬁrst two parts proposed new physical comp lexity measures,
priors and limits of inductive inference [18,17].
We frame the question of ultimate limits of intelligence in a general phys ical
setting, for this we provide a general deﬁnition of an intelligent syst em and a
physical performance criterion, which as anticipated turns out to be a relation
of physical quantities and information, the latter of which we had co nceptually
reduced to physics with minimum machine volume complexity in [18].
2 Notation and Background
2.1 Universal Induction
Let us recall Solomonoﬀ’s universal distribution [21]. Let U be a universal com-
puter which runs programs with a preﬁx-free encoding like LISP; y = U(x)
denotes that the output of program x on U is y where x and y are bit strings.
1Any unspeciﬁed variable or function is assumed to be represented a s a bit string.
1 A preﬁx-free code is a set of codes in which no code is a preﬁx of another. A com-
puter ﬁle uses a preﬁx-free code, ending with an EOF symbol, t hus, most reasonable
programming languages are preﬁx-free.
2 Eray ¨Ozkural
|x| denotes the length of a bit-string x. f(·) refers to function f rather than its
application.
The algorithmic probability that a bit string x ∈ { 0, 1}+ is generated by a
random program π ∈ { 0, 1}+ of U is:
PU (x) =
∑
U(π )∈x(0|1)∗ ∧π ∈{0, 1}+
2−|π | (1)
which conforms to Kolmogorov’s axioms [13]. PU (x) considers any continuation
of x, taking into account non-terminating programs. 2 PU is also called the uni-
versal prior for it may be used as the prior in Bayesian inference, fo r any data
can be encoded as a bit string. We also give the basic deﬁnitions of Algo rithmic
Information Theory (AIT) [14], where the algorithmic entropy, or c omplexity of
a bit string x∈ { 0, 1}+ is
HU (x) = min( {|π| |U(π) = x}) H∗
U (x) = − log2 PU (x) (2)
We use some variables in overloaded fashion in the paper, e.g., π might be a
program, a policy, or a physical mechanism depending on the contex t.
2.2 Operator induction
Operator induction is a general form of supervised machine learning where we
learn a stochastic map from nquestion and answer pairs D= {(qi,a i)} sampled
from a (computable) stochastic source µ. Operator induction can be solved by
ﬁnding in available time a set of operators Oj (·|·), each a conditional probability
density function (cpdf), such that the following goodness of ﬁt is m aximized
Ψ =
∑
j
ψj
n (3)
for a stochastic source µ where each term in the summation is the contribution
of a model:
ψj
n = 2 −|(Oj (·|·)|
n∏
i=1
Oj (ai|qi). (4)
qi and ai are question/answer pairs in the input dataset drawn from µ, and Oj is
a computable cpdf in Equation 4. We can use the found m operators to predict
unseen data with a mixture model [24]
PU (an+1|qn+1) =
m∑
j=1
ψj
nOj (an+1|qn+1) (5)
The goodness of ﬁt in this case strikes a balance between high a prior i proba-
bility and reproduction of data like in minimum message length (MML) met hod
[27,26], yet uses a universal mixture like in sequence induction. The co nvergence
theorem for operator induction was proven in [23] using Hutter’s ex tension to
arbitrary alphabet, and it bounds total error by HU (µ) ln 2 similarly to sequence
induction.
2 We used the regular expression notation in language theory.
Measuring Intelligence 3
2.3 Set induction
Set induction generalizes unsupervised machine learning where we lea rn a prob-
ability density function (pdf) from a set of n bitstrings D = {d1,d 2,...,d n}
sampled from a stochastic source µ. We can then inductively infer new members
to be added to the set with:
P(dn+1) = PU (D∪ dn+1)
PU (D) (6)
Set induction is clearly a restricted case of operator induction wher e we set Qi’s
to null string. Set induction is a universal form of clustering, and it p erfectly
models perception. If we apply set induction over a large set of 2D pic tures of
a room, it will give us a 3D representation of it necessarily. If we apply it to
physical sensor data, it will infer the physical theory – perfectly g eneral, with
inﬁnite domains – that explains the data, perception is merely a speciﬁ c case
of scientiﬁc theory inference in this case, though set induction wor ks both with
deterministic and non-deterministic problems.
2.4 Universal measures of intelligence
There is much literature on the subject of deﬁning a measure of inte lligence.
Hutter has deﬁned an intelligence order relation in the context of his univer-
sal reinforcement learning (RL) model AIXI [8], which suggests tha t intelligence
corresponds to the set of problems an agent can solve. Also notable is the univer-
sal intelligence measure [10,11], which is again based on the AIXI model. Their
universal intelligence measure is based on the following philosophical d eﬁnition
compiled from their review of deﬁnitions of intelligence in the AI literatu re.
Deﬁnition 1 (Legg & Hutter). Intelligence measures an agent’s ability to
achieve goals in a wide range of environments.
It implies that intelligence requires an autonomous goal-following agen t. The
intelligence measure of [10] is deﬁned as
Υ(π) =
∑
µ ∈E
2−HU (µ )Vπ
µ (7)
where µ is a computable reward bounded environment, And Vπ
µ is the ex-
pected sum of future rewards in the total interaction sequence o f agent π.
Vπ
µ = Eµ,π [∑ ∞
t=1 γtrt], where rt is the instantaneous reward at time t gener-
ated from the interaction between the agent π and the environment µ, and γt is
the time discount factor.
2.5 The free energy principle.
In Asimov’s story titled “The Last Question”, the task of life is identiﬁ ed as over-
coming the second law of thermodynamics, however futile. Variation al free en-
ergy essentially measures predictive error, and it was introduced b y Feynmann to
4 Eray ¨Ozkural
address diﬃcult path integral problems in quantum physics. In ther modynamic
free energy, energies are negative log probabilities like entropy. Th e free energy
principle states that any system must minimize its free energy to main tain its
order. An adaptive system that tends to minimize average surprise (entropy) will
tend to survive longer. A biological organism can be modelled as an ada ptive
system that has an implicit probabilistic model of the environment, an d the vari-
ational free energy puts an upper bound on the surprise, thus min imizing free
energy will improve the chances of survival. The divergence betwee n the pdf of
environment and an arbitrary pdf encoded by its own mechanism is min imized
in Friston’s model [9]. It has been shown in detail that the free energ y principle
adequately models a self-preserving agent in a stochastic dynamica l system [6,9],
which we can interpret as an environment with computable pdf. An ac tive agent
may be deﬁned in the formalism of stochastic dynamical systems, by partitioning
the physical states X of the environment into X = E× S× A× Λ where e∈ E
is an external state, s ∈ S is a sensory state, a ∈ A an active state, and λ ∈ Λ
is an internal state. Self-preservation is deﬁned by the Markov bla nket S× A,
the removal of which partitions X into external states E and internal states Λ
that inﬂuence each other only through sensory and action states . E inﬂuences
sensations S, which in turn inﬂuence internal states Λ, resulting in the choice
of action signals S, which impact E, forming the feedback loop of the adaptive
system. The system states x ∈ X evolve according to the stochastic equation:
˙x(t) = f(x) + ω (8)
x(0) = x0 (9)
f(x) =





fe(e,s,a )
fs(e,s,a )
fa(s,a,λ )
fλ (s,a,λ )




 (10)
where f(x) is the ﬂow of system states and it is decomposed into ﬂows over
the sets in the system partition, explicitly showing the dependencies among state
sets; ω models ﬂuctuations. Friston formalizes the self-preservation (ho meostasis)
problem as ﬁnding an internal dynamics that minimizes the uncertaint y (Shan-
non entropy) of the external states, and shows a solution based on the principle
of least action [9] wherein minimizing free energy is synonymous with min imizing
the entropy of the external states (principle of least action), wh ich subsequently
corresponds to active inference. We have space for only some key results from the
rather involved mathematical theory. p(s,f |m) is the generative pdf that gen-
erates sensorium s and ﬁctive (hidden) states f ∈ F from probabilistic model
m, and q(f|λ) is the recognition pdf that predicts hidden states F in the world
given internal state. Generative pdf factorizes as p(s,f |m) = p(s|f,m )p(f|m).
Free energy is deﬁned as energy minus entropy
F(s,λ ) = Eq[− ln p(s,f |m)] − H(q(f|λ)) (11)
which can be subjectively computed by the system. Free energy is a lso equal to
surprise plus divergence between recognition and generative pdf’s .
F(s,λ ) = Eq[− ln p(s,f |m)] + DKL (q(f|λ)||p(f|s,m )) (12)
Measuring Intelligence 5
Minimizing divergence minimizes free energy, internal states λ may be optimized
to minimize predictive error using Equation 12, and surprise is invarian t with
respect to λ. Free energy may be formulated as complexity plus accuracy of
recognition, as well.
F(s,λ ) = Eq[− ln p(s,a |f,m )] + DKL (q(f|λ)||p(f,m )) (13)
In this case, we may choose an action that changes sensations to r educe predictive
error. Only the ﬁrst term is a function of action signals. Minimization o f free
energy turns out to be equivalent to the information bottleneck pr inciple of
Tishby [9,25]. The information bottleneck method is equivalent to the p ioneering
work of Ashby, which is simple enough to state here [3,2]:
SB = I(λ; F) − I(S; λ) (14)
where the ﬁrst term is the mutual information between internal an d hidden
states, and the second term is the mutual information between se nsory states
and internal states. Both terms are expanded using conditional e ntropy, and
then two terms in the middle are eliminated because they are not relev ant to
the optimization problem – we do not know the hidden variables in H(λ|F) and
H(S) is constant.
SB = H(λ) − H(λ|F) − H(S) + H(S|λ) (15)
S∗
B = H(λ) + H(S|λ) (16)
Minimizing S∗
B Equation 16 thus minimizes the sum of the entropy of internal
states and the entropy required to encode sensory states given internal states. In
other words, it strikes an optimal balance between model complexit y H(λ), and
model accuracy H(S|λ). Friston further shows that Equation 16 directly derives
from the free energy principle, closing potential loopholes in the the ory. Please
see [5] for a comprehensive application of the free energy principle t o agents and
learning. Note also that the bulk of the theory assumes the ergodic hypothesis.
3 Perception as General Intelligence
Since we are chieﬂy interested in stochastic problems in the physical world, we
propose a straightforward informal deﬁnition of intelligence:
Deﬁnition 2. Intelligence measures the capability of a mechanism to solv e pre-
diction problems.
Mechanism is any physical machine as usual, see [4] which suggests like wise.
Therefore, a general formulation of Solomonoﬀ induction, operat or induction,
might serve as a model of general intelligence, as well [24]. Recall th at operator
induction can infer any physically plausible cpdf, thus its approximatio n can
solve any classical supervised machine learning problem. The only sligh t issue
with Equation 7 might be that it seems to exclude classical AI systems that are
not agents, e.g., expert systems, machine learning tools, knowledg e representa-
tion systems, search and planning algorithms, and so forth, which a re somewhat
more naturally encompassed by our informal deﬁnition.
6 Eray ¨Ozkural
3.1 Is operator induction adequate?
A question naturally arises as to whether operator induction can ad equately solve
every prediction problem we require in AI. There are two strong obj ections to
operator induction that we know of. It is argued that in a dynamic en vironment,
as in a physical environment, we must use an active agent model so t hat we can
account for changes in the environment, as in the space-time embe dded agent
[16] which also provides an agent-based intelligence measure. This ob jection may
be answered by the simple solution that each decision of an active inte lligent
system may be considered a separate induction problem. The secon d objection
is that the basic Solomonoﬀ induction can only predict the next bit, bu t not
the expected cumulative reward, which its extensions can solve. We counter
this objection by stating that we can reduce an agent model to a pe rception
and action-planning problem as in OOPS-RL [20]. In OOPS-RL, the per ception
module searches for the best world-model given the history of sen sory input
and actions in allotted time using OOPS, and the planning module search es
for the best control program using the world-model of the perce ption module
to determine the action sequence that maximizes cumulative reward likewise.
OOPS has a generalized Levin Search [12] which may be tweaked to solv e either
prediction or optimization problems. Hutter has also observed that standard
sequence induction does not readily address optimization problems [8 ]. However,
Solomonoﬀ induction is still complete in the sense of Turing, and can inf er any
computable cpdf; and when the extension to Solomonoﬀ induction is a pplied to
sequence prediction, it does not yield a better error bound, which s eems like a
conundrum. On the other hand, Levin Search with a proper univers al probability
density function (pdf) of programs can be modiﬁed to solve inductio n problems
(sequence, set, operator, and sequence prediction with arbitra ry loss), inversion
problems (computer science problems in P and NP), and optimization p roblems
[23]. The planning module of OOPS-RL likewise requires us to write such a n
optimization program. In that sense, AIXI implies yet another varia tion of Levin
Search for solving a particular universal optimization problem, howe ver, it also
has the unique advantage that formal transformations between AIXI problem
and many important problems including function minimization and strat egic
games have been shown [8]. Nevertheless, the discussion in [23] is ra ther brief.
Also see [1] for a discussion of universal optimization.
Proposition 1. A discrete-time universal RL model may be reduced to operato r
induction.
More formally, the perceptual task of an RL agent would be inferrin g from a
history the cumulative rewards in the future, without loss of gener ality. Let the
chronology C be a sequence of sensory, reward, and action data C = [( s1,r 1,a 1),
(s2,r 2,a 2),..., (sn,r n,a n)] where Ci accesses ith element, and Ci:j accesses the
subsequence [ Ci,C i+1,...,C j ]. Let rc be the cumulative reward function where
rc(C,i,j ) = ∑ k≤j
k=i rk. After observing ( sn,r n,a n), we construct dataset Dc as
follows. For every unique ( i,j ) pair such that 1 < i≤ j ≤ n, we concatenate
history tuples C1:(i−1), and we form a question string that also includes the next
Measuring Intelligence 7
action, iand j, q= [( s1,r 1,a 1), (s2,r 2,a 2),..., (s(i−1),r (i−1),a (i−1))],a i,i,j , and
an answer string which is the cumulative reward a= rc(C,i,j ). Solving the op-
erator induction problem for this dataset DC will yield a cpdf which predicts
cumulative rewards in the future. After that, choosing the next a ction is a simple
matter of maximizing r(C1:n,a i,n + 1,λ ) where λ is the planning horizon. The
reduction causes quadratic blow-up in the number of data items. Ou r somewhat
cumbersome reduction suggests that all of the intelligence here co mes from op-
erator induction, surely an argmax function, or a summation of rew ards does
not provide it, but rather it builds constraints into the task. In oth er words,
we interpret that the intelligence in an agent model is provided by indu ctive
inference, rather than an additional application of decision theory .
4 Physical Quantiﬁcation of Intelligence
Deﬁnition 1 corresponds to any kind of reinforcement-learning or g oal-following
agent in AI literature quite well, and can be adapted to solve other kin ds of prob-
lems. The unsupervised, active inference agent approach is propo sed instead of
reinforcement learning approach in [7], and the authors argue that they did not
need to invoke the notion of reward, value or utility. The authors in p articu-
lar claim that they could solve the mountain-car problem by the free- energy
formulation of perception. We thus propose a perceptual intelligen ce measure.
4.1 Universal measure of perception ﬁtness
Note that operator induction is considered to be insuﬃcient to desc ribe universal
agents such as AIXI, because basic sequence induction is inapprop riate for mod-
elling optimization problems [8]. However, a modiﬁed Levin search proce dure can
solve such optimization problems as in ﬁnding an optimal control prog ram [20].
In OOPS-RL, the perception module searches for the best world-m odel given
the history of sensory input and actions in allotted time using OOPS, a nd the
planning module searches for the best control program using the w orld-model of
the perception module to determine the control program that max imizes cumu-
lative reward likewise. In this paper, we consider the perception mod ule of such
a generic agent which must produce a world-model, given sensory inp ut.
We can use the intelligence measure Equation 7 in a physical theory of in-
telligence, however it contains terms like utility that do not have phys ical units
(i.e., we would be preferring a more reductive deﬁnition). We therefo re attempt
to obtain such a measure using the more benign goodness-of-ﬁt (E quation 3).
Let the universal measure of the ﬁtness of operator induction be deﬁned as
ΥO(π) =
∑
µ ∈S
2−HU (µ )Ψ(µ,π ) (17)
where S is the set of possible stochastic sources in the observable universe U
and π is a physical mechanism, and Ψ is relative to a stochastic source µ and a
8 Eray ¨Ozkural
physical mechanism (computer) π. This would be maximum if we assume that
operator induction were solved exactly by an oracle machine.
Note that HU (µ) is ﬁnite; Ψ(µ,π ) is likewise bounded by the amount of
computation π will spend on approximating operator induction.
4.2 Application to homeostasis agent
In a presentation to Friston’s group in January 2015, we noted tha t the mini-
mization of S∗
B is identical to Minimum Message Length principle, which can be
further reﬁned as
S′
B = H∗(Λ) + H∗(S|Λ) (18)
using Solomonoﬀ’s entropy formulation that takes the negative loga rithm of al-
gorithmic probability [22]. In the unsupervised agent context, solvin g this min-
imization problem corresponds to inferring an optimal behavioral po licy as Λ
constitutes internal dynamics which may be modeled as a non-termin ating pro-
gram. We could directly apply induction to minimize KL divergence, as we ll.
Note the correspondence to operator induction.
Theorem 1. Minimizing the free energy is equivalent to solving the oper ator
induction problem for (λ,s ) pairs where qi ∈ Λ and ai ∈ S.
Proof. Observe that minimizing Equation 16 corresponds to picking maximum
ψj
n since in entropy form,
− log2(ψj
n) = − log2(2−|Oj (·|·)|) − log2(
n∏
i=1
Oj (si|λi))
= |Oj (·|·)| −
n∑
i=1
log2(Oj (ai|qi)) = |Oj (·|·)|+ H(Oj (ai|qi)).
We deﬁne a non-redundant selection of ψj
n’s, |Oj (·|·)| = HU (Oj (·|·)), e.g., we
pick only the shortest programs that produce the same cpdf, oth erwise the en-
tropy form would diverge. Minimizing Equation 18 is exactly operator induction,
even though the questions are programs, the ensemble here is of a ll programs
and all sensory state, program pairs in space-time. ∑ |Oj (·|·)| = H∗(Λ) and∑ H(Oj (ai|qi)) = H∗(S|Λ). Note that this merely establishes model equiva-
lence, we have not yet explained how it is to be computed in detail.
Proposition 2. By the above theorem, Equation 17 measures the goodness of
ﬁt for a given homeostasis agent mechanism, for all possible environments.
The mechanism π that maximizes Ψ(µ,π ) achieves less error with respect to
a source (which may be taken to correspond to the whole random dy namical
system in the framework of free energy principle), while ΥO(π) normalizes Ψ(µ,π )
with respect to a random dynamical system. It holds for the same r easons Legg’s
Measuring Intelligence 9
measure holds, which are not discussed due to space limits in the pres ent paper.
We prefer the unsupervised homeostasis agent among the two age nt models we
discussed because it provides an exceptionally elegant and reductio nist model of
autonomous behavior, that has been rigorously formulated physic ally. Note that
this agent is conceptually related to the survival property of RL ag ents discussed
in [19].
4.3 Discussion
The unsupervised model still achieves exploration and curiosity, be cause it would
stochastically sample and navigate the environment to reduce pred ictive errors.
While we either optimize perceptual models or choose an action that w ould beﬁt
expectations, it might be possible to express the optimal adaptive a gent policy in
a general optimization framework. A more in-depth analysis of the u nsupervised
agent will be presented in a subsequent publication. A more general reductive
deﬁnition of intelligence should also be researched. These developme nts could
eventually help unify AGI theory.
References
1. Alpcan, T., Everitt, T., Hutter, M.: Can we measure the diﬃ culty of an op-
timization problem? In: 2014 IEEE Information Theory Works hop, ITW 2014,
Hobart, Tasmania, Australia, November 2-5, 2014. pp. 356–3 60. IEEE (2014),
http://dx.doi.org/10.1109/ITW.2014.6970853
2. Ashby, W.R.: Principles of the self-organizing system. I n: v. Foerster, H., Zopf,
G.W. (eds.) Principles of Self-Organization: Transaction s of the University of Illi-
nois Symposium, pp. 255–278. Pergamon, London (1962)
3. Ashby, W.: Principles of the self-organizing dynamic sys tem. The Journal of Gen-
eral Psychology 37(2), 125–128 (1947)
4. Dowe, D.L., Hern´ andez-Orallo, J., Das, P.K.: Artiﬁcial General Intelligence: 4th
International Conference, AGI 2011, Mountain View, CA, USA , August 3-6, 2011.
Proceedings, chap. Compression and Intelligence: Social E nvironments and Com-
munication, pp. 204–211. Springer Berlin Heidelberg, Berl in, Heidelberg (2011),
http://dx.doi.org/10.1007/978-3-642-22887-2_21
5. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P.,
ODoherty, J., Pezzulo, G.: Active inference and learning. N eu-
roscience and Biobehavioral Reviews 68, 862 – 879 (2016),
http://www.sciencedirect.com/science/article/pii/S0149763416301336
6. Friston, K., Kilner, J., Harrison, L.: A free energy princ iple for
the brain. Journal of Physiology-Paris 100(13), 70 – 87 (200 6),
http://www.sciencedirect.com/science/article/pii/S092842570600060X,
theoretical and Computational Neuroscience: Understandi ng Brain Functions
7. Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforceme nt learn-
ing or active inference? PLOS ONE 4(7), 1–13 (07 2009),
https://doi.org/10.1371/journal.pone.0006421
8. Hutter, M.: Universal algorithmic intelligence: A mathe matical top → down ap-
proach. In: Goertzel, B., Pennachin, C. (eds.) Artiﬁcial Ge neral Intelligence, pp.
227–290. Cognitive Technologies, Springer, Berlin (2007)
10 Eray ¨Ozkural
9. Karl, F.: A free energy principle for biological systems. Entropy 14(11), 2100–2121
(2012), http://www.mdpi.com/1099-4300/14/11/2100
10. Legg, S., Hutter, M.: Universal intelligence: A deﬁniti on of machine intelligence.
Minds Mach. 17(4), 391–444 (Dec 2007)
11. Legg, S., Veness, J.: An approximation of the universal i ntelligence measure. In:
Algorithmic Probability and Friends. Bayesian Prediction and Artiﬁcial Intelli-
gence, Lecture Notes in Computer Science, vol. 7070, pp. 236 –249. Springer Berlin
Heidelberg (2013)
12. Levin, L.: Universal problems of full search. Problems o f Information Transmission
9(3), 256–266 (1973)
13. Levin, L.A.: Some theorems on the algorithmic approach t o probability theory and
information theory. CoRR abs/1009.5894 (2010)
14. Li, M., Vitanyi, P.M.: An Introduction to Kolmogorov Com plexity and Its Appli-
cations. Springer Publishing Company, Incorporated, 3 edn . (2008)
15. Lloyd, S.: Ultimate physical limits to computation. Nat ure406 (Aug 2000)
16. Orseau, L., Ring, M.: Space-time embedded intelligence . In: Bach, J., Go-
ertzel, B., Ikl, M. (eds.) Artiﬁcial General Intelligence, Lecture Notes in
Computer Science, vol. 7716, pp. 209–218. Springer Berlin H eidelberg (2012),
http://dx.doi.org/10.1007/978-3-642-35506-6_22
17. ¨Ozkural, E.: Ultimate Intelligence Part II: Physical Measu re and Complexity of
Intelligence. ArXiv e-prints (Apr 2015)
18. ¨Ozkural, E.: Ultimate intelligence part I: physical comple teness and objectivity of
induction. In: Artiﬁcial General Intelligence - 8th Intern ational Conference, AGI
2015, AGI 2015, Berlin, Germany, July 22-25, 2015, Proceedi ngs. pp. 131–141
(2015), http://dx.doi.org/10.1007/978-3-319-21365-1_14
19. Ring, M., Orseau, L.: Delusion, survival, and intellige nt agents. In: Artiﬁcial Gen-
eral Intelligence, pp. 11–20. Springer Berlin Heidelberg ( 2011)
20. Schmidhuber, J.: Optimal ordered problem solver. Machi ne Learning 54, 211–256
(2004)
21. Solomonoﬀ, R.J.: A formal theory of inductive inference , part i. Information and
Control 7(1), 1–22 (March 1964)
22. Solomonoﬀ, R.J.: Complexity-based induction systems: Comparisons and conver-
gence theorems. IEEE Trans. on Information Theory IT-24(4) , 422–432 (July 1978)
23. Solomonoﬀ, R.J.: Progress in incremental machine learn ing. Tech. Rep. IDSIA-16-
03, IDSIA, Lugano, Switzerland (2003)
24. Solomonoﬀ, R.J.: Three kinds of probabilistic inductio n: Universal distributions
and convergence theorems. The Computer Journal 51(5), 566– 570 (2008)
25. Tishby, N., Pereira, F.C., Bialek, W.: The information b ottleneck method. ArXiv
Physics e-prints (Apr 2000)
26. Wallace, C.S., Dowe, D.L.: Minimum message length and ko l-
mogorov complexity. The Computer Journal 42(4), 270–283 (1 999),
http://comjnl.oxfordjournals.org/content/42/4/270.abstract
27. Wallace, C.S., Boulton, D.M.: A information measure for classiﬁcation. Computer
Journal 11(2), 185–194 (1968)