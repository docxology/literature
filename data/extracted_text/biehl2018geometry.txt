8102
voN
02
]IA.sc[
1v14280.1181:viXra
Geometry of Friston’s active inference
Martin Biehl
martin@araya.org
Araya Inc.,Tokyo
November 21, 2018
Abstract
We reconstruct Karl Friston’s active inference and give a geometrical
interpretation of it.
1 Introduction
In Biehl et al. (2018) we have reconstructed the active inference approach as
used in Friston et al. (2015). Here we present a radically shortened and more
generalaccountofactive inference. We alsopresentforthe firsttime a geomet-
rical interpretation of active inference.
WeusethesamenotationandmodelasinBiehl et al.(2018). Therereaders
can also find a translation table to the notations used in Friston et al. (2015,
2016).
E E E
0 1 2
S S S
0 1 2
A A
1 2
M M
1 2
Figure 1: Bayesiannetworks of the PA-loop
The active inference agent we are describing in the following interacts with
anenvironmentaccordingtotheperception-actionloopdefinedbytheBayesian
network in Section 1. There, we write E for the environment state, S for the
t t
sensor value, A for the action, and M for the memory state of the agent at
t t
time step t. We assume that for all time steps t the transition dynamics of the
1
Θ2
Θ3 Eˆ Eˆ Eˆ
0 1 2
Sˆ Sˆ Sˆ
0 1 2
Aˆ Aˆ
1 2
Θ1
Figure 2: Bayesian network of the generative model.
environment p(e |a ,e ) and the dynamics of the sensors p(s |e ) are time-
t+1 t+1 t t t
homogenous and fixed. We further assume that the agent has perfect memory,
i.e. at all times m :=sa .
t ≺t
Thedifferencetothesetupofreinforcementlearningforpartiallyobservable
Markovdecisionproblems is thatthere is no explicitrewardsignal. Insteadthe
agentusesamotivationfunctionalMwhichevaluatestheagent’scurrentbeliefs
aboutthe consequencesof actions(see Section3). A standardrewardsignal R
t
can easily be added as an additional sensor value by letting S →(S ,R ).
t t t
With the environment, sensor, and memory dynamics fixed it remains to
specify the actiongenerationmechanismp(a |m ). We firstdescribe this intwo
t t
separate steps, inference and action selection, that can be performed one after
the other. Then we show how active inference combines the two steps in one
optimization procedure. On the way we also define motivation functionals.
Actiongenerationinmoredetail: Theagentemploysaparameterizedmodel
inorderto predictthe consequencesofits actions. Ateachtime stepit receives
a new sensor value in response to an action and updates its model by condi-
tioning on the new memory state. Additionally conditioning on the various
possible future actions (or policies) results in a conditional probability distri-
bution which we call the active posterior. The active posterior represents the
agent’s beliefs about the consequences (for future sensors, latent variables, and
internal parameters) of its actions. Obtaining the active posteriors is referred
to as the inference step. Subsequently, the agent constructs (using softmax)
a probability distribution over future actions by assigning high probability to
those actions whose entries in the active posterior achieve high values when
plugged into the motivation functional (e.g. expected future reward if there is
an explicit one). This results in the what we call the induced policy. Obtaining
the latter is referred to as the action selection step. Afterwards the agent can
simply sample from this induced policy to generate actions.
In active inference, both inference and action selection are turned into op-
timization problems and then combined in a multiobjective optimization. The
2
inference step can be turned into an optimization using variational inference
(see e.g. Bishop, 2011; Blei et al., 2017). Variational inference introduces a
variationalversionofthe activeposterior. Since the variationalactive posterior
generally differs from the original/true active posterior it leads to a, generally
different, variational induced policy. Action selection is then formulated as an
optimizationbyintroducing anadditional(third) policy whose divergencefrom
the variational induced policy is to be minimized. Active inference then op-
timizes a sum of the respective objective functions and afterwards the agent
can sample from the third policy. With a bit of notational trickery this can be
writtenintheformFristonuses,whichlookssimilartoavariationalfreeenergy
(or evidence lower bound).
2 Inference
The agent performs inference on the generative model given by the Bayesian
network in Section 1. The variables that model variables occurring outside of
p(a|m)intheperception-actionloop(Section1),aredenotedashattedversions
of their counterparts. To clearly distinguish the probabilities defined by the
generative model from the true dynamics, we use the symbol q instead of p.
Here, θ1,θ2,θ3 are the parameters of the model. To save space,write θ :=
(θ1,θ2,θ3).
The last modelled time step Tˆ can be chosen as Tˆ = T (T is the final step
of the PA-loop), but it is also possible to always set it to Tˆ = t+n, in which
case n specifies a future time horizon from current time step t.
Activeposterior Attimettheagentplugsitsexperiencesa intoitsgenera-
≺t
tivemodelbysettingAˆ =a ,for τ <tandSˆ =s ,for τ <tandconditioning
τ τ τ τ
on these. The consequences of future actions can be obtained by additionally
conditioningoneachpossiblefutureactionsequenceaˆ . Thisleadstothecon-
t:Tˆ
ditionalprobabilitydistributionthatwecalltheactiveposterior (theexperience
sa is considered fixed):
≺t
q(sˆ ,eˆ ,θ|aˆ ,sa ) (1)
t:Tˆ 0:Tˆ t:Tˆ ≺t
Variational active posterior In active inference the active posterior is ob-
tained via variational inference (see e.g. Blei et al., 2017).
We write r (instead of p or q) to indicate variational probability distri-
butions and φ for the entire set of variational parameters. Note, the pa-
rameter φ contains parameters for each of the future action sequences aˆ
t:Tˆ
i.e. φ = (φ ) . The variational active posterior is of the form
aˆt:Tˆ aˆt:Tˆ∈ATˆ−t+1
r(sˆ ,eˆ ,θ|aˆ ,φ). To construct the variational active posterior we cycle
t:Tˆ 0:Tˆ t:Tˆ
through the possible future action sequences aˆ and compute each entry. For
t:Tˆ
a fixed aˆ the variational free energy, also known as the (negative) evidence
t:Tˆ
3
lower bound (ELBO) in variational inference literature, is defined as:
F[aˆ ,φ,sa ]:=
t:Tˆ ≺t
r(sˆ ,eˆ ,θ|aˆ ,φ)log
r(sˆ t:Tˆ ,eˆ 0:Tˆ ,θ|aˆ t:Tˆ ,φ)
dθ
(2)
Z t:Tˆ 0:Tˆ t:Tˆ q(s ,sˆ ,eˆ ,θ|aˆ ,a )
sˆt:
X
Tˆ,eˆ 0:Tˆ
≺t t:Tˆ 0:Tˆ t:Tˆ ≺t
Then variational inference amounts to solving for each aˆ the optimisation
t:Tˆ
problem:
φ∗ :=argminF[aˆ ,φ,sa ]. (3)
aˆt:Tˆ,sa≺t
φ
t:Tˆ ≺t
3 Action selection and induced policy
Let ∆ be the space of active posteriors. Then a motivation functional is
AP
a map M : ∆ × ATˆ−t+1 → R taking active posteriors d(.,.,.|.) ∈ ∆
AP AP
and a sequences of future actions aˆ to a real value M(d(.,.,.,|.),aˆ ) ∈ R.
t:Tˆ t:Tˆ
An example of a motivation functional is the expected value of the sum over
future rewards (if one of the sensor values is defined as the reward). Other
possibilitiescanbefoundinBiehl et al.(2018). Nowdefineforsomeγ ∈R+and
0
somemotivationfunctionalMasoftmaxoperatorσM mappingactiveposteriors
γ
d(.,.,.|.) to probability distributions over future action sequences:
1
σ
γ
M
[d(.,.,.|.)](aˆ
t:Tˆ
):=
Z(γ)
eγM(d(.,.,.|.),aˆt:Tˆ).
(4)
Then we call
M
q(aˆ |sa ):=σ [q(.,.,.|.,sa )](aˆ ) (5)
t:Tˆ ≺t γ ≺t t:Tˆ
the induced policy of active posterior q(.,.,.|.,sa ) and M. And we call
≺t
M
r(aˆ |φ):=σ [r(.,.,.|.,φ)](aˆ ) (6)
t:Tˆ γ t:Tˆ
the induced variational policy of variational active posterior r(.,.,.|.,φ) and M.
Note that,if φ is not the optimized parameter φ∗ then the induced vari-
sa≺t
ational policy cannot be expected to lead to actions that actually reflect the
preferences encoded in M. On the other hand, if the true active posterior, or
the optimizedvariationalactiveposteriorareusedandγ →∞the inducedpol-
icy should correspond to the best guess for an agent with the given generative
model, variational distributions, and motivation M.
4 Active inference
Now introduce an additional variational policy s(aˆ |ρ) parameterized by ρ in
t:Tˆ
order to approximate the induced variational policy r(aˆ |φ) of the variational
t:Tˆ
4
q(aˆ |sa )
t:Tˆ ≺t D s(aˆ |ρ)
2 t:Tˆ
r(aˆ |φ)
t:Tˆ
σM
γ
σM
γ
D
1 r(.,.,.|.,φ)
q(.,.,.|.,sa )
≺t
Figure3: Ontopthespaceofprobilitydistributionsoverfutureactionsequences
∆ on the bottom the space of active posteriors ∆ .
AˆTˆ−t+1 AP
posterior(actionselection). TomakethishappenwecanminimizetheKullback-
Leibler divergence between the two.
Todoinferenceandactionselectionatoncewecanthenminimizethesumof
the variational free energy (Equation (2)) and the Kullback-Leibler divergence
w.r.t. ρ,φ (for an illustration of the situation see Figure 3):
s(aˆ |ρ)F[aˆ ,φ,sa ]+KL[s(Aˆ |ρ)||r(Aˆ |φ)] (7)
t:Tˆ t:Tˆ ≺t t:Tˆ t:Tˆ
X
aˆt:Tˆ
D2
| {z }
D1
| {z }
If φ and ρ are optimized the agent can then sample actions from s(aˆ |ρ).
t:Tˆ
Now if we change notation and let s(aˆ |ρ) → r(aˆ |ρ) and r(aˆ |φ) →
t:Tˆ t:Tˆ t:Tˆ
q(aˆ |φ) then the above can be rewritten as:
t:Tˆ
r(sˆ ,eˆ ,θ,aˆ |ρ,φ)
r(sˆ ,eˆ ,θ,aˆ |ρ,φ)log
t:Tˆ 0:Tˆ t:Tˆ
dθ.
Z t:Tˆ 0:Tˆ t:Tˆ q(s ,sˆ ,eˆ ,θ,aˆ |φ,a )
aˆt:Tˆ,
X
sˆt:Tˆ,eˆ 0:Tˆ
≺t t:Tˆ 0:Tˆ t:Tˆ ≺t
(8)
This looks similar to a variational free energy or evidence lower bound and is
the form of the free energy found in Friston et al. (2016). What distinguishes
it from a true variational free energy is the occurrence of the parameter φ in
numerator and denominator.
Acknowledgement
I want to thank Manuel Baltieri and Thomas Parr for helpful discussions as
well as Christian Guckelsberger, Daniel Polani, Christoph Salge, and Sim´on C.
5
Smith, for feedback on this article.
References
Biehl, M., Guckelsberger, C., Salge, C., Smith, S. C., and Polani, D. (2018).
Expanding the Active Inference Landscape: More Intrinsic Motivations in
the Perception-Action Loop. Frontiers in Neurorobotics, 12.
Bishop, C. M. (2011). Pattern Recognition and Machine Learning. Information
Science and Statistics. Springer, New York.
Blei, D. M., Kucukelbir,A., andMcAuliffe, J. D. (2017). VariationalInference:
A Review for Statisticians. Journal of the American Statistical Association,
112(518):859–877.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G.
(2016). Active Inference: A Process Theory. Neural Computation, 29(1):1–
49.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., and Pezzulo,
G. (2015). Active Inference and Epistemic Value. Cognitive Neuroscience,
6(4):187–214.
6