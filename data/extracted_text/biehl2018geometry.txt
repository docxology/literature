arXiv:1811.08241v1  [cs.AI]  20 Nov 2018
Geometry of Friston’s active inference
Martin Biehl
martin@araya.org
Araya Inc.,Tokyo
November 21, 2018
Abstract
We reconstruct Karl Friston’s active inference and give a geometrical
interpretation of it.
1 Introduction
In
Biehl et al. (2018) we have reconstructed the active inference approach as
used in Friston et al. (2015). Here we present a radically shortened and more
general account of active inference. We also present for the ﬁrs t time a geomet-
rical interpretation of active inference.
We use the same notation and model as in Biehl et al. (2018). There readers
can also ﬁnd a translation table to the notations used in Friston et al. (2015,
2016).
E1 E2
S1 S2
A1 A2
M1 M2
E0
S0
Figure 1: Bayesian networks of the PA-loop
The active inference agent we are describing in the following interact s with
an environment according to the perception-action loop deﬁned by the Bayesian
network in Section 1. There, we write Et for the environment state, St for the
sensor value, At for the action, and Mt for the memory state of the agent at
time step t. We assume that for all time steps t the transition dynamics of the
1
ˆE1 ˆE2
ˆS1 ˆS2
ˆA1 ˆA2
ˆE0
ˆS0
Θ 3
Θ 2
Θ 1
Figure 2: Bayesian network of the generative model.
environment p( et+1|at+1, e t) and the dynamics of the sensors p( st|et) are time-
homogenous and ﬁxed. We further assume that the agent has per fect memory,
i.e. at all times mt := sa≺t.
The diﬀerence to the setup of reinforcement learning for partially o bservable
Markov decision problems is that there is no explicit reward signal. Ins tead the
agent uses a motivation functional M which evaluates the agent’s current beliefs
about the consequences of actions (see Section 3). A standard reward signal Rt
can easily be added as an additional sensor value by letting St → (St, R t).
With the environment, sensor, and memory dynamics ﬁxed it remains to
specify the action generation mechanism p( at|mt). We ﬁrst describe this in two
separate steps, inference and action selection, that can be performed one after
the other. Then we show how active inference combines the two ste ps in one
optimization procedure. On the way we also deﬁne motivation functio nals.
Action generation in more detail: The agent employs a parameterized model
in order to predict the consequences of its actions. At each time st ep it receives
a new sensor value in response to an action and updates its model by condi-
tioning on the new memory state. Additionally conditioning on the vario us
possible future actions (or policies) results in a conditional probabilit y distri-
bution which we call the active posterior. The active posterior represents the
agent’s beliefs about the consequences (for future sensors, lat ent variables, and
internal parameters) of its actions. Obtaining the active posterio rs is referred
to as the inference step. Subsequently, the agent constructs (using softmax)
a probability distribution over future actions by assigning high proba bility to
those actions whose entries in the active posterior achieve high valu es when
plugged into the motivation functional (e.g. expected future rewa rd if there is
an explicit one). This results in the what we call the induced policy. Obtaining
the latter is referred to as the action selection step. Afterwards the agent can
simply sample from this induced policy to generate actions.
In active inference, both inference and action selection are turne d into op-
timization problems and then combined in a multiobjective optimization. The
2
inference step can be turned into an optimization using variational in ference
(see e.g. Bishop, 2011; Blei et al. , 2017). Variational inference introduces a
variational version of the active posterior. Since the variational a ctive posterior
generally diﬀers from the original/true active posterior it leads to a, generally
diﬀerent, variational induced policy. Action selection is then formulated as an
optimization by introducing an additional (third) policy whose diverge nce from
the variational induced policy is to be minimized. Active inference then op-
timizes a sum of the respective objective functions and afterward s the agent
can sample from the third policy. With a bit of notational trickery this can be
written in the form Friston uses, which looks similar to a variational fr ee energy
(or evidence lower bound).
2 Inference
The agent performs inference on the generative model given by th e Bayesian
network in Section
1. The variables that model variables occurring outside of
p(a|m) in the perception-action loop (Section 1), are denoted as hatted versions
of their counterparts. To clearly distinguish the probabilities deﬁne d by the
generative model from the true dynamics, we use the symbol q inst ead of p.
Here, θ1, θ 2, θ 3 are the parameters of the model. To save space,write θ :=
(θ1, θ 2, θ 3).
The last modelled time step ˆT can be chosen as ˆT = T (T is the ﬁnal step
of the PA-loop), but it is also possible to always set it to ˆT = t + n, in which
case n speciﬁes a future time horizon from current time step t.
Active posterior At time t the agent plugs its experience sa≺t into its genera-
tive model by setting ˆAτ = aτ , for τ < t and ˆSτ = sτ , for τ < t and conditioning
on these. The consequences of future actions can be obtained by additionally
conditioning on each possible future action sequence ˆ at: ˆT . This leads to the con-
ditional probability distribution that we call the active posterior(the experience
sa≺t is considered ﬁxed):
q(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , sa ≺t) (1)
Variational active posterior In active inference the active posterior is ob-
tained via variational inference (see e.g. Blei et al. , 2017).
We write r (instead of p or q) to indicate variational probability distri-
butions and φ for the entire set of variational parameters. Note, the pa-
rameter φ contains parameters for each of the future action sequences ˆ at: ˆT
i.e. φ = ( φˆat: ˆT )ˆat: ˆT ∈A ˆT −t+1 . The variational active posterior is of the form
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ). To construct the variational active posterior we cycle
through the possible future action sequences ˆ at: ˆT and compute each entry. For
a ﬁxed ˆat: ˆT the variational free energy, also known as the (negative) evidenc e
3
lower bound (ELBO) in variational inference literature, is deﬁned as :
F[ˆat: ˆT ,φ, sa ≺t] :=
∑
ˆst: ˆT , ˆe0: ˆT
∫
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ ) log r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ )
q(s≺t, ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , a ≺t) dθ (2)
Then variational inference amounts to solving for each ˆ at: ˆT the optimisation
problem:
φ∗
ˆat: ˆT ,sa ≺t := arg min
φ
F[ˆat: ˆT , φ, sa ≺t]. (3)
3 Action selection and induced policy
Let ∆ AP be the space of active posteriors. Then a motivation functional is
a map M : ∆ AP × A ˆT −t+1 → R taking active posteriors d( ., ., . |. ) ∈ ∆ AP
and a sequences of future actions ˆ at: ˆT to a real value M(d(., ., ., |. ), ˆat: ˆT ) ∈ R.
An example of a motivation functional is the expected value of the su m over
future rewards (if one of the sensor values is deﬁned as the rewar d). Other
possibilities can be found in
Biehl et al. (2018). Now deﬁne for some γ ∈ R+
0 and
some motivation functional M a softmax operator σM
γ mapping active posteriors
d(., ., . |. ) to probability distributions over future action sequences:
σM
γ [d(., ., . |. )](ˆat: ˆT ) := 1
Z(γ)eγ M(d(.,.,. |. ), ˆat: ˆT ). (4)
Then we call
q(ˆat: ˆT |sa≺t) := σM
γ [q(., ., . |., sa ≺t)](ˆat: ˆT ) (5)
the induced policy of active posterior q( ., ., . |., sa ≺t) and M. And we call
r(ˆat: ˆT |φ) := σM
γ [r(., ., . |., φ )](ˆat: ˆT ) (6)
the induced variational policyof variational active posterior r( ., ., . |., φ ) and M.
Note that,if φ is not the optimized parameter φ∗
sa≺t then the induced vari-
ational policy cannot be expected to lead to actions that actually re ﬂect the
preferences encoded in M. On the other hand, if the true active posterior, or
the optimized variational active posterior are used and γ → ∞ the induced pol-
icy should correspond to the best guess for an agent with the given generative
model, variational distributions, and motivation M.
4 Active inference
Now introduce an additional variational policy s(ˆ at: ˆT |ρ) parameterized by ρ in
order to approximate the induced variational policy r(ˆ at: ˆT |φ) of the variational
4
q(., ., . |., sa ≺t) r(., ., . |., φ )
s(ˆat: ˆT |ρ)
r(ˆat: ˆT |φ)
q(ˆat: ˆT |sa≺t)
D1
D2
σM
γ
σM
γ
Figure 3: On top the space of probility distributions over future act ion sequences
∆ ˆA ˆT −t+1 on the bottom the space of active posteriors ∆ AP .
posterior (action selection). To make this happen we can minimize the Kullback-
Leibler divergence between the two.
To do inference and action selection at once we can then minimize the s um of
the variational free energy (Equation ( 2)) and the Kullback-Leibler divergence
w.r.t. ρ, φ (for an illustration of the situation see Figure 3):
∑
ˆat: ˆT
s(ˆat: ˆT |ρ) F[ˆat: ˆT , φ, sa ≺t]
  
D1
+ KL[s( ˆAt: ˆT |ρ)||r( ˆAt: ˆT |φ)]  
D2
(7)
If φ and ρ are optimized the agent can then sample actions from s(ˆ at: ˆT |ρ).
Now if we change notation and let s(ˆ at: ˆT |ρ) → r(ˆat: ˆT |ρ) and r(ˆ at: ˆT |φ) →
q(ˆat: ˆT |φ) then the above can be rewritten as:
∑
ˆat: ˆT , ˆst: ˆT , ˆe0: ˆT
∫
r(ˆst: ˆT , ˆe0: ˆT , θ, ˆat: ˆT |ρ, φ ) log r(ˆst: ˆT , ˆe0: ˆT , θ, ˆat: ˆT |ρ, φ )
q(s≺t, ˆst: ˆT , ˆe0: ˆT , θ, ˆat: ˆT |φ, a ≺t) dθ.
(8)
This looks similar to a variational free energy or evidence lower bound and is
the form of the free energy found in Friston et al. (2016). What distinguishes
it from a true variational free energy is the occurrence of the par ameter φ in
numerator and denominator.
Acknowledgement
I want to thank Manuel Baltieri and Thomas Parr for helpful discus sions as
well as Christian Guckelsberger, Daniel Polani, Christoph Salge, and Sim´ on C.
5
Smith, for feedback on this article.
References
Biehl, M., Guckelsberger, C., Salge, C., Smith, S. C., and Polani, D. (201 8).
Expanding the Active Inference Landscape: More Intrinsic Motiva tions in
the Perception-Action Loop. Frontiers in Neurorobotics, 12.
Bishop, C. M. (2011). Pattern Recognition and Machine Learning. Information
Science and Statistics. Springer, New York.
Blei, D. M., Kucukelbir, A., and McAuliﬀe, J. D. (2017). Variational Inf erence:
A Review for Statisticians. Journal of the American Statistical Association,
112(518):859–877.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo , G.
(2016). Active Inference: A Process Theory. Neural Computation, 29(1):1–
49.
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., and Pez zulo,
G. (2015). Active Inference and Epistemic Value. Cognitive Neuroscience,
6(4):187–214.
6