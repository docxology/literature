M3ViT: Mixture-of-Experts Vision Transformer
for Efﬁcient Multi-task Learning
with Model-Accelerator Co-design
Hanxue Liang1∗, Zhiwen Fan1∗, Rishov Sarkar2, Ziyu Jiang3, Tianlong Chen1,
Kai Zou4, Yu Cheng5, Cong Hao2, Zhangyang Wang1
1University of Texas at Austin,2Georgia Institute of Technology
3Texas A&M University,4Protagolabs Inc, 5Microsoft Research
{haliang,zhiwenfan,tianlong.chen,atlaswang}@utexas.edu
{rishov.sarkar,callie.hao}@gatech.edu, jiangziyu@tamu.edu
kz@protagolabs.com, yu.cheng@microsoft.com
Abstract
Multi-task learning (MTL) encapsulates multiple learned tasks in a single model
and often lets those tasks learn better jointly. However, when deploying MTL onto
those real-world systems that are often resource-constrained or latency-sensitive,
two prominent challenges arise: (i) during training, simultaneously optimizing all
tasks is often difﬁcult due to gradient conﬂicts across tasks, and the challenge is
ampliﬁed when a growing number of tasks have to be squeezed into one compact
model; (ii) at inference, current MTL regimes have to activate nearly the entire
model even to just execute a single task. Yet most real systems demand only one or
two tasks at each moment, and switch between tasks as needed: therefore such “all
tasks activated” inference is also highly inefﬁcient and non-scalable.
In this paper, we present a model-accelerator co-design framework to enable ef-
ﬁcient on-device MTL, that tackles both training and inference bottlenecks. Our
framework, dubbed M3ViT, customizes mixture-of-experts (MoE) layers into a
vision transformer (ViT) backbone for MTL, and sparsely activates task-speciﬁc
experts during training, which effectively disentangles the parameter spaces to
avoid different tasks’ training conﬂicts. Then at inference with any task of interest,
the same design allows for activating only the task-corresponding sparse “expert”
pathway, instead of the full model. Our new model design is further enhanced by
hardware-level innovations, in particular, a novel computation reordering scheme
tailored for memory-constrained MTL that achieves zero-overhead switching be-
tween tasks and can scale to any number of experts. Extensive experiments on
PASCAL-Context [1] and NYUD-v2 [2] datasets at both software and hardware
levels are conducted to demonstrate the effectiveness of the proposed design. When
executing single-task inference, M3ViT achieves higher accuracies than encoder-
focused MTL methods, while signiﬁcantly reducing 88% inference FLOPs. When
implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design
framework reduces the memory requirement by 2.40×, while achieving energy
efﬁciency up to 9.23×higher than a comparable FPGA baseline.
Code is available at: https://github.com/VITA-Group/M3ViT.
1 Introduction
Vision Transformers (ViTs) [3, 4, 5, 6], as the latest performant deep models, have achieved impressive
performance on various computer vision tasks [ 7, 8, 9]. These models are specially trained or tested
∗Equal contribution
36th Conference on Neural Information Processing Systems (NeurIPS 2022).
arXiv:2210.14793v1  [cs.CV]  26 Oct 2022
for only one or a few tasks; however, many real-world applications require one compact system
that can handle many different tasks efﬁciently, and often need to swiftly switch between tasks per
demand. For example, an autonomous driving system [10] needs to perform and switch between many
tasks such as drivable area estimation, lane detection, pedestrian detection, and scene classiﬁcation:
apparently both single task inference and cross-task switching need to happen at ultra-low latency.
As another example, smart-home indoor robots [11] are expected to address semantic segmentation,
navigation, tracking, or other tasks in varying contexts, with very limited on-board resources. Multi-
task learning (MTL) [12, 13, 14] solves multiple tasks simultaneously within a single model and learns
improved feature representations [15] shared by related tasks [ 16, 17]. Therefore, accomplishing
realistic efﬁcient MTL is becoming a key knob for building real-time sophisticated AI systems.
Despite the promise, challenges persist to build an efﬁcient MTL model suitable for real-world
applications:  during training, prior works [18, 19, 20] indicate the competition of different tasks
in training may degrade MTL, since the same weights might receive and be confused by conﬂicting
update directions. Speciﬁcally, [ 19] reveals that negative cosine similarities between different tasks’
gradients are detrimental. [ 21, 22] conﬁrm that conﬂicting gradients not only slow down convergence
but also bias the learned representations against some tasks. That is only getting worse on compact
models owing to their limited modeling capacity. To tackle the cross-task conﬂicts, solutions have
been proposed by varying learning rate speeds of different tasks [20], using “cross-stitch” sharing [23],
or re-balancing task gradients [19, 24, 20, 25]. However, they either require task-speciﬁc design or
signiﬁcantly increase the model complexity which contradicts our efﬁciency goal.  at inference,
existing MTL regimes typically activate the entire backbone model unconditionally. However,many
real systems only need to call upon one or a few tasks at each moment, hence the “all activated”
inference is heavily inefﬁcient and non-scalable. For example, current regimes [14, 23, 26, 27] have
to activate the whole gigantic ResNet [28] encoder even just to execute a single monocular depth
estimation task or so. If the number of tasks scale up [29] and the backbone keeps growing bigger,
the “per task” inference efﬁciency of the resultant MTL model could become catastrophically poor.
To tackle these bottlenecks, we propose a model-accelerator co-design framework that enables
efﬁcient on-device MTL. Speciﬁcally, in the software level, we propose to adapt mixture of experts
(MoE) layers [30, 31] into the MTL backbone, as MoE can adaptively divide-and-conquer the entire
model capacity into smaller sub-models [30, 32]. Here, we replace the dense feed-forward network
in the ViT with sparsely activated MoE experts (MLPs). A task-dependent gating network will be
trained to select the subset of experts for each input token, conditioning on tasks. During training,
this task-dependent routing principle effectively disentangles the parameter spaces, balancing feature
reuse and automatically avoiding different tasks’ training conﬂicts. Meanwhile, at the inference
stage with any task of interest, this design naturally allows for sparse activation of only the experts
corresponding to the task instead of the full model, thus achieving highly sparse and efﬁcient inference
for the speciﬁc task. In the hardware level, we propose a novel computation reordering mechanism
tailored for memory-constrained MTL and MoE, which allows scaling up to any number of experts
and also achieves zero-overhead switching between tasks. Speciﬁcally, based on ViT, we push tokens
to per-expert queues to enable expert-by-expert computation rather than token-by-token. We then
implement a double-buffered computation strategy that hides the memory access latency required to
load each expert’s weights from off-chip memory, regardless of task-speciﬁc expert selection. This
design naturally incurs no overhead for switching between frames or tasks in FPGA.
To validate the effectiveness, we evaluate our performance gain using the ViT-small backbone on
the NYUD-v2 and PASCAL-Context datasets. On the NYUD-v2 dataset with two tasks, our model
achieves comparable results with encoder-focused MTL methods while reducing 71% FLOPs for
single-task execution. When we evaluate on the PASCAL-Context dataset with more tasks, our model
achieves even better performance (2.71 vs. 0.60) and reduces 88% inference FLOPs.
We found the MTL performance gain brought by MoE layers consistently increases as the task count
grows. When implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design
framework reduces the memory requirement by 2.40 ×while achieving energy efﬁciency (as the
product of latency and power) up to 9.23×higher than comparable FPGA baselines and up to 10.79×
higher than the GPU implementation. Our contributions are outlined below:
• We target the problem of efﬁcient MTL, and adopt the more realistic inference setting
(activating one task at a time, while switching between tasks). We introduce MoE as the
uniﬁed tool to attain two goals: both resolving cross-task training conﬂicts ( better MTL
performance), and sparsely activating paths for single-task inference ( better efﬁciency).
2
Speciﬁcally for MTL, the MoE layer is accompanied with a task-dependent gating network
to make expert selections conditioning on the current task.
• We implement the proposed MTL MoE ViT framework on a hardware platform of one Xilinx
ZCU104 FPGA, which enables us to exploit a memory-efﬁcient computation reordering
scheme that consolidates per-expert Multiply-and-ACcumulate (MAC) operations such that
only one expert’s weights are needed on-chip at a time. Our design is scalable to any number
of experts while requiring no frame-switching or task-switching overhead.
• We conduct extensive experiments to justify its inference effectiveness in both accuracy and
on-edge efﬁciency metrics. Our framework, dubbed M3ViT, achieves higher accuracies than
encoder-focused MTL methods, while signiﬁcantly reducing 88% inference FLOPs; on
hardware, it reduces the memory requirement by 2.40×and costs up to 9.23×and 10.79×
less energy compared to the FPGA and GPU baselines, respectively.
2 Related Works
Multi-task Learning The generic multi-task learning problem has been studied for a long history.
Some non-deep learning-based methods propose to use distance metric [ 33, 34, 35], probabilistic
prior [ 36, 37, 38, 39] to model the common information among tasks. With the emergence of the
deep learning technique, MTL [14, 40, 23, 41, 42, 43] is performed to learn shared representation
among tasks. The emergence of ViT further makes it possible to extend the task range from only
vision tasks to other modalities tasks (e.g., text, audio) [44, 45, 46, 47, 48]. Current MTL models can
be roughly categorized into two types based on where the task interactions take place in the network.
The encoder-focused architectures [23, 40, 26, 27] only share information in the encoder, before
decoding each task with an independent task-speciﬁc head. Cross-stitch networks [23] introduce linear
combination in each layer. NDDR-CNN [ 26] improves it by dimensional reduction. MTAN [ 27]
leverages an attention mechanism to learn between tasks. TAPS [ 49] adapts a base model to a
new task by modifying a small task-speciﬁc subset of layers. The second type, decoder-focused
models [42, 43, 50, 51], make initial task predictions in decoder and then leverage features from these
initial predictions to further improve output. Although they report higher performance, their models
consume a large number of FLOPs, according to [14]. This makes it difﬁcult to deploy them onto
those real-world systems that are often resource-constrained or latency-sensitive. And they need to
execute all the tasks for initial prediction, which is heavily inefﬁcient in the common scenario when
only one or few tasks are needed. Hence, we focus on encoder-focused architecture in this work.
Many methods [25, 20, 52, 27] are also proposed to handle the MTL training conﬂicts problem.
Mixture of Experts (MoE) MoE contains a series of sub-models (i.e., experts) and performs
conditional computation in an input-dependent fashion [ 53, 54, 55, 56, 57], based on learned or
deterministic routing policies [58, 57]. The traditional dense MoEs suffer from intensive compu-
tational costs since they select all experts [ 59]. Recent studies [ 30, 60, 61] in natural language
processing (NLP) propose sparse MoE that sparsely activates a few experts during both training
and inference, thus substantially reducing the cost and allowing gigantic language models even
with trillions of parameters [61]. Unfortunately, such a sparse-gated manner still has limitations of
unstable training and imbalanced selections among experts. Various solutions are invented from regu-
larization [62, 60, 61] and optimization [63, 64] perspectives. Moreover, MoE has drawn increasing
popularity in computer vision [59, 65, 66, 67, 68, 69, 70], where it mainly focuses on considerably
smaller network backbones compared to the ones in NLP. For instance, [67] and [68] formulate the
channel and kernel of convolutional layers as experts and establish the MoE framework. Several
pioneer investigations also explore MoE for multi-task learning, which are related to this work.
Particularly, [17, 71, 72] introduce task-speciﬁc gating networks to choose different parts of models
for processing information from each task. They present certain possibilities of using MoE to solve
MTL problems in some cases like classiﬁcation for medical signals [ 71], digital number images
(MNIST) [72], and recommendation systems [17]. We make a further attempt to adapt MoE into a
compact model for dense prediction multi-task learning, along with software-hardware co-design.
Vision Transformer There are growing interests in exploring the use of transformers [73, 3] for
computer vision tasks since its success in the natural language processing [73, 74, 75], including image
generation [76, 77], generative adversarial networks [78, 79], image classiﬁcation [76, 3, 80, 81, 82,
83, 81, 84], semantic segmentation [8, 85], object detection [6, 86], 3D data processing [87, 88, 89],
novel view synthesis [90, 91], and many others [92, 93, 94, 95].
3
Hardware FPGA acceleration of Transformer-based models has attracted increasing attention.
Pioneering works [96, 97, 98, 99] note that transformers are computation- and memory-intensive
and are too large to ﬁt on the FPGA. Therefore, various model compression methods have been
proposed, such as activation quantization, token pruning, block-circulant matrices (BCM) for weights,
block-balanced weight pruning, and column-balanced block weight pruning. Such compression
methods are lossy and require compression-aware training to regain accuracy. To our best knowledge,
there is no existing FPGA accelerator for MoE in a Transformer-based model. The MoE mechanism
exposes great challenges to FPGA since it requires swift expert switching between tokens and frames,
which may introduce signiﬁcant overhead of memory and parameter loading. In this work, however,
we propose a novel expert-by-expert computation-reordering approach that can reduce the overhead
to negligible despite the number of experts, and does not require model compression or re-training.
3 Method
Overview We ﬁrst describe the standard Vision Transformer and MoEs, and then show the proposed
MoE ViT design for MTL. To enable dynamically adapting between different tasks with minimum
overhead on FPGA, we detail the hardware implementation. Figure 1 shows the whole framework.
…
Task A
 Task B
 Both NotActivated
(a) MoE ViT Design (b) Hardware Design
Decoder A Decoder B
Self-Attention
Load
Parameters
Compute
Expert
Gating Function
Time
Final output
Intermediate
output
Intermediate input Initial input
Layer
Experts
Linear Projection
…
…
token
embedding
Figure 1: The overall structure of the proposed M 3ViT pipeline. The input image is split into
ﬁxed-size patches, embedded, and combined with position embeddings. In training, the MTL MoE
ViT adaptively activates the model by sparsely selecting relevant experts using its task-dependent
routers. During inference, only one task will be performed at a time. The hardware collects all
patches allocated for each expert and processes them expert-by-expert with the “load parameters” and
“compute expert” modules.
Layer
Norm Attention Layer
Norm
Router
Task B
Expert1
Expert2
ExpertN-1
ExpertN
…
Router
Task A
Layer 
Norm Attention Layer 
Norm
Expert1
Expert2
ExpertN-1
ExpertN
…
Router
(a) Multi-gate MoE layer design. (b) Task-conditioned MoE layer design.
Task-conditioned
embedding
Figure 2: The proposed two variants of MTL MoE layers . In the left ﬁgure, each task selects
its experts using its own router. In the right one, all tasks share one router, while a task-speciﬁc
embedding is concatenated with the token embedding to formulate the input of the shared router.
3.1 Task-dependent MoE ViT Design
Vision Transformer The representative Vision Transformer architecture [3] ﬁrst splits the input
image into non-overlapped patches and projects the patches to a higher hidden dimension using one
4
convolutional layer. The projected patches (a.k.a. tokens) are then passed through several consecutive
transformer layers. Each layer contains a self-attention module and a feed-forward network (MLPs).
The self-attention is computed using the scaled-dot product:
Attention(Q,K,V ) = softmax
(
QKT
√
C
)
V (1)
where Q,K,V ∈RN×C are the query, key and value matrices computed from input tokens; N and
Cindicate the token number and the hidden dimension. In our experiments, we adopt the DeiT [4] as
the backbone encoder, which is a data-efﬁcient ViT variant that distills tokens to ensure the student
learns from the teacher through attention.
Mixture of Experts Layer A Mixture of Experts (MoE) layer typically consists a group of N
experts f1,f2,··· ,fN along with a router R(or gating network) to select the corresponding experts.
The experts network stands for multi-layer perceptrons [61, 100] in ViTs. The router Rplays a key
role within our MoE ViT design as it determines task routings via only sparsely activating relevant
experts. We adopt a representative router called top-Kgating [30] based on ViT. With inputx, the
resultant output of MoE layers can be formulated as the summation of the selected top K experts
from N expert candidates using a router:
y=
K∑
k=1
R(x)k ·fk(x), (2)
R(x) = TopK(softmax(G(x),K)), (3)
TopK(v,K) =
{
v if v is in the top Kelements
0 otherwise (4)
where Grepresents the learnable network within the router, for which we employ a single-layer MLP
in practice. The softmax(·) together with TopK(·,K) sets all elements of the vector to zero except
the elements with the largest Kvalues. In practice, we choose K= 4 out of N = 16 expert candidates.
Each expert is computed with W2σgelu (W1x), where σgelu is the GELU activation [101]. W1 and W2
are two learnable weight matrices. Note that we scale down the expert size by four times compared
to that in standard ViT MLP layers to make the computation FLOPs equivalent. We also employ the
load and important balancing loss with the weight of 0.01 following [30] to avoid always picking the
same experts while ignoring others. This loss term is also employed for the two task-dependent MTL
MoE designs that we introduce next.
Multi-gate MoE ViT for MTL MoE brings training dynamics to balance between large capacity
and efﬁciency, by selecting only a subset of experts using the router. To adapt vanilla MoE into our
dense prediction MTL framework, we ﬁrst propose to assign each dense prediction task a router Ri
to specify its own experts, denoted as multi-gate MTL MoE ViT:
yi =
K∑
k=1
Ri(x)k ·fk(x) (5)
where idenotes task index. Expert candidates fk are shared across tasks. The ﬂow chart of the
multi-gate variant is shown in Figure 2(a); task-dependent routers take as input the shared token
embedding and do their expert selections.
Task-conditioned MoE ViT for MTL Conditional encoding has been widely applied to multi-
modal [102] and multi-task [103] models. To achieve task-dependent routing with one gating network,
we propose the task-conditioned MTL MoE ViT shown in Figure 2(b). Speciﬁcally, suppose we have
ntasks in training. We manually deﬁne a n-dimensional one-hot task-conditioned vector. The vector
is fed into a two-layer MLP to extract a 64-dimensional task embedding, which is then concatenated
with token embeddings to form the task-dependent input for the router in the MoE layer:
yi =
K∑
k=1
R(x,ti)k ·fk(x), (6)
ti = ReLU(T(x,ei)) (7)
where T indicates the two-layer MLPs to extract task-conditioned embeddings, ei ∈{0,1}n, and∑n
j=1 ej = 1 . We denote this conditional design as task-conditioned MTL MoE ViT, in which
backbone model parameters do not proportionally increase if we include more tasks in training.
5
Self-Attention
Compute QKV
Layer Norm
Q × K Soft 
Max × VHead 1
Compute QKV Q × K Soft 
Max × VHead 12
⋮ Linear
Projection
ViT Layer
MoE Layer
Gating 
Function
Load Expert
Compute Expert GELU
Combine 
Experts 
Per Token
Load Weights
Compute Fully-Connected Layer GELU
Layer
type?
1 2 3
4
3
4
5
5
6
Figure 3: Hardware implementation of a ViT block of M 3ViT. The hardware implementation
consists of a layer norm unit, a self-attention unit containing 12 independent heads followed by a
linear projection, a unit to compute the fully-connected layers of a standard ViT layer, and a unit to
select and compute the experts in an MoE layer. Numerical indicators within the ﬁgure indicate the
path through which data ﬂows during the computation of a single layer of M3ViT, either a ViT layer
or an MoE layer. This hardware is shared across all blocks.
3.2 Circuit-level Implementation
We co-design the hardware to support MTL MoE ViT. We design a layer-wise implementation of
M3ViT on a Xilinx ZCU104 FPGA, a diagram of which is shown in Figure 3. The design computes
layers sequentially but parallelizes computation steps within each layer. By proposing a novel
computation reordering scheme, our hardware design features memory-efﬁcient expert computation
that also achieves zero-overhead task switching and frame switching.
Challenges of Naive Method A straightforward (but naive) implementation would compute the
output for each token in the order it appears in the input sequence: all tokens choose any Kexperts
out of the N candidates, so ostensibly the only way to avoid data loading overhead would be to keep
weights of all N experts on-chip at all times. However, this requires extreme on-chip memory usage,
scaling with O(N) and typically exceeding FPGA on-chip memory capacity unless N is very small.
Challenges of Cache-based Method We can adopt a cache to store several experts on-chip at any
given time. However, this on-demand approach incurs long delays from off-chip DRAM accesses
whenever the cache needs to be repopulated with an expert’s weights. Further, we experimentally
found that all experts are likely to be activated at least once across all tokens, exhibiting a cache-
unfriendly access pattern. Therefore, although a cache-based design alleviates memory inefﬁciency,
it incurs severe delays by frequently loading the weights of experts.
Proposed Solution: Memory-efﬁcient Computation Reordering The crux of the problem lies in
the unpredictability of the set of experts that will be needed by tokens at any given time. We address
this problem at its root by designing a novel computation reordering scheme that ﬂips the compute
pattern on its head: rather than computing the MoE layer token-by-token, we instead compute it
expert-by-expert. The overall ﬂow chart of the reordering scheme is shown in Figure 4.
Expert selection Computation reordering
⋮
Token
1 3
2 4
Experts Expert
3
Tokens
⋯
⋯
⋯
⋮
Double-buffering and pre-fetching
2. Meanwhile, load 
the parameters for 
the next expert
1. Compute partial 
results for all tokens 
that use expert
3. Swap between iterations
Reorder
1 2
1
2
1
2
2 [              ]
([    ])1
Prefetch Buffer
⋮⋮
Figure 4: The computation reordering ﬂow used by M 3ViT for hardware memory efﬁciency.
The MoE gating function selects K experts for each token, which are used to route tokens to per-
expert queues. This is followed by a double-buffered computation ﬂow that computes one expert’s
results on its entire token queue while loading another expert’s parameters, swapping buffers between
iterations.
6
Table 1: Comparisons with encoder-focused MTL architectures on the PASCAL-Context dataset.
Model Backbone Seg.
(mIoU↑)
Norm.
(mErr)↓
H. Parts
(mIoU)↑
Sal.
(mIoU)↑
Edge
(odsF) ↑
∆m
(%) ↑
FLOPS
(G) ↓
Energy
(W·s)↓
STL-B ResNet-18 66.2 13.9 59.9 66.3 68.8 0.00 167 1.029
MTL-B ResNet-18 63.8 14.9 58.6 65.1 69.2 −2.86 167 1.029
Uncertainty [25] (MTL-B) ResNet-18 65.4 16.5 59.2 65.6 68.6 −4.60 167 1.029
DW A [52] (MTL-B) ResNet-18 63.4 14.9 58.9 65.1 69.1 −2.94 167 1.029
GradNorm [20] (MTL-B) ResNet-18 64.7 15.4 59.0 64.5 67.0 −3.97 167 1.029
MGDA [27] (MTL-B) ResNet-18 64.9 15.6 57.9 62.5 61.4 −6.81 167 1.029
MTAN [27] ResNet-18 63.7 14.8 58.9 65.4 69.6 −2.39 212 5.306
Cross-Stitch [23] ResNet-18 66.1 13.9 60.6 66.8 69.9 +0.60 647 6.001
NDDR-CNN [26] ResNet-18 65.4 13.9 60.5 66.8 69.8 +0.39 747 5.034
M-ViT (MTL-B) ViT-small 70.7 15.5 58.7 64.9 68.8 −1.77 83 3.062
M2ViT (+MoE) MoE ViT-small 72.8 14.5 62.1 66.3 71.7 +2.71 84 7.446
M3ViT (+MoE+Codesign) MoE ViT-small 72.8 14.5 62.1 66.3 71.7 +2.71 84 0.690
Speciﬁcally, we propose to add each token to a queue for its selected top- K experts, instead of
computing the token output immediately.
Our hardware then makes use of the per-expert queues via a double-buffered computation ﬂow, also
known as ping-pong buffering: one buffer is ﬁlled with an expert’s weights from off-chip memory
accesses, while another already-loaded buffer is used to compute another expert’s results for its entire
token queue. After both operations ﬁnish, the buffers are swapped, and the process repeats.
Scalability and Efﬁciency Our approach hides nearly all latency from off-chip memory accesses
to load expert weights, and it uses O(1) on-chip memory with respect to Kand N, making it scalable
to any number of experts. Additionally, our method’s efﬁciency does not rely on any speciﬁc usage
pattern of experts for a given frame or a given task, so we naturally achieve zero-overhead switches
between frames and between tasks. Task switches and frame switches in our hardware design do not
change our computation ﬂow at all, and there is no speciﬁc step taken to execute the switch.
4 Experiments
4.1 Experiment Setup
To evaluate the propose method, we conduct experiments on two popular dense labeling MTL
benchmarks, i.e. NYUD-v2 [2] and PASCAL-Context [1]. Both datasets are described below.
Datasets The PASCAL-Context [1] contains a total of 10,103 images, for the ﬁve tasks of edge
detection (Edge), semantic segmentation (Seg.), human parts segmentation (H.Parts), surface normals
(Norm.), and saliency detection (Sal.). The NYUD-v2 dataset [2] is an indoor dataset which consists
of RGB-D images of 464 indoor scenes. There are 795 images for training and 654 images for testing,
both with annotation for semantic segmentation (Seg.) and monocular depth estimation (Depth).
Evaluation Metrics For software level evaluation, we adopt the standard evaluation metrics follow-
ing [14, 104, 50]. Particularly, we use mean intersection over union (mIoU) for semantic segmentation,
human parts segmentation, and saliency; mean error (mErr) for surface normals estimation, root
mean square error (rmse) for depth estimation; and optimal dataset F-measure (odsF) [105] for edge
detection. Following [14], we use ∆m to evaluate a MTL model mas the average per task drop with
respect to the STL model bover all tasks: ∆m = 1
T
∑T
i (−1)li (Mm,i −Mb,i)/Mb,i, where Mm,i
and Mb,i are the metrics of task ifor the model mand brespectively, and li = 1 if a lower value
means better performance.
To evaluate our model-accelerator design, we consider latency, energy usage (as the product of latency
and power), and on-chip memory usage for single-task inference using a batch size of 1.
Network Conﬁguration and Implementation Details We evaluate our model based on several
versions of ViT backbone [4] including ViT-tiny, ViT-small, and ViT-base.
Our FPGA designs target the Xilinx ZCU104 FPGA at a 300 MHz clock frequency, consuming 10 W
of power. The GPU baselines are measured on the NVIDIA Quadro RTX 8000.
7
MTL-B
M-ViT
M3-ViT
MTL-BM-ViTM3-ViT
Figure 5: Qualitative result on PASCAL-Context. We compare between vanilla MTL-B, M-ViT
and M2ViT models, and our model outperforms baseline on edge detection, semantic segmentation
and human parts segmentation and saliency detection.
Table 2: Comparisons with encoder-focused MTL architectures on the NYUD-v2 dataset.
Model Backbone Seg.
(mIoU)↑
Depth
(rmse)↓
∆m
(%) ↑
FLOPS
(G) ↓
Energy
(W·s) ↓
STL-B ResNet-50 43.9 0.585 0.00 192 2.145
MTL-B ResNet-50 44.4 0.587 +0.41 192 2.145
Uncertainty [25] (MTL-B) ResNet-50 44.0 0.590 −0.23 192 2.145
DW A [52] (MTL-B) ResNet-50 44.1 0.591 −0.28 192 2.145
GradNorm [20] (MTL-B) ResNet-50 44.2 0.581 +1.45 192 2.145
MGDA [27] (MTL-B) ResNet-50 43.2 0.576 +0.02 192 2.145
MTAN [27] ResNet-50 45.0 0.584 +1.32 320 5.036
Cross-Stitch [23] ResNet-50 44.2 0.570 +1.61 310 4.221
NDDR-CNN [26] ResNet-50 44.2 0.573 +1.38 340 4.244
M-ViT (MTL-B) ViT-small 40.9 0.631 −6.27 100 2.097
M2ViT (+MoE) MoE ViT-small 45.6 0.589 +1.59 100 8.189
M3ViT (+MoE+Co-design) MoE ViT-small 45.6 0.589 +1.59 100 0.845
The results reported below are based on ViT-small. Please refer to the supplementary materials for
training setup, more details on network conﬁguration, results on ViT-tiny and ViT-base, and details of
our target hardware platforms.
4.2 Comparison with State-of-the-art Dense Prediction MTL
As we target an efﬁcient MTL system under the single-task inference setting, we conduct experiments
on encoder-focused architectures (more details in Section 2). MTL-B [ 14] is a vanilla multi-task
learning baseline model which is composed of a shared backbone in combination with task-speciﬁc
heads. Several state-of-the-art (SoTA) encoder-focused MTL models, including MTAN [27], Cross-
Stitch [23] and NDDR-CNN [ 26], improve MTL-B by proposing feature sharing methods in the
encoder. Our methods are all conducted on vanilla MTL-B, namely, applying MTL on ViT (M-ViT),
8
Table 3: Effect of task-dependent MoE design. M 3ViT-Single, M3ViT-Multi., and M3ViT-Task-
cond. refer to the MTL MoE model with single router, multi routers, and task-conditioned router,
respectively.
PASCAL-Context Seg.
(mIoU↑)
Norm.
(mErr)↓
H. Parts
(mIoU)↑
Sal.
(mIoU)↑
Edge
(odsF) ↑
∆m
(%) ↑
FLOPS
(G) ↓
STL-B 66.2 13.9 59.9 66.3 68.8 0.00 167
M-ViT (MTL-B) 70.7 15.5 58.7 64.9 68.8 −1.76 83
M3ViT-Single 71.5 14.8 61.2 65.9 71.5 +1.40 84
M3ViT-Multi. 72.8 14.5 62.1 66.3 71.7 +2.71 84
M3ViT-Task-cond. 72.0 14.4 61.3 65.8 71.8 +2.22 85
NYUD-v2 Seg.
(mIoU)↑
Depth
(rmse)↓ – – – ∆m
(%) ↑
FLOPS
(G) ↓
STL-B 43.9 0.585 – – – 0.00 192
M-ViT (MTL-B) 40.9 0.631 – – – −6.27 100
M3ViT-Single 45.3 0.600 – – – +0.31 100
M3ViT-Multi. 45.6 0.589 – – – +1.59 100
M3ViT-Task-cond. 45.3 0.595 – – – +0.74 101
adding task-dependent MoE design (M2ViT), and adding hardware co-design on FPGA (M3ViT).
We also compare with previous works that handle the multi-task training conﬂicts problem, including
uncertainty weighting [25], GradNorm [20], DWA [52], and MGDA [27], and they are evaluated
on MTL-B. Single task learning baseline (STL-B) is used for MTL performance evaluation ∆m.
As multi-gate MoE shows better performance than task-conditioned MoE, the reported M2ViT and
M3ViT results are based on the multi-gate design.
Results on PASCAL-Context Dataset As shown in Table 1, even using a vanilla MTL-B frame-
work, introducing MoE (M2ViT) can achieve the highest performance over all previous encoder-
focused works (+2.71% MTL performance); meanwhile, it signiﬁcantly reduces their single task
inference FLOPs (particularly, reducing Cross-Stitch by 88%). Comparing against Uncertainty [25],
DW A [52], GradNorm [20], and MGDA [27], the superior performance of M2ViT demonstrates its
strong capacity in handling training conﬂict. Moreover, leveraging the Model-Accelerator co-design
helps us to consume less than one-tenth the energy cost when deploying our model on FPGA. Some
qualitative results are shown in Figure 5.
Results on NYUD-v2 Dataset On this dataset, M 2ViT can reduce previous SoTA’s inference
FLOPs by 68% while achieving comparable MTL performance. Introducing MoE to M-ViT helps to
enlarge the model capacity without increasing inference FLOPs, which results in a MTL performance
boost from −6.27% to +1.59%. With our Model-Accelerator co-design, we also see a nearly tenfold
increase in energy efﬁciency. Results are shown in Table 2.
4.3 Effect of Task-dependent MoE Design
To evaluate the effectiveness of our task-dependent MoE design, we compare between several models
in Table 3 including STL-B, MTL ViT (M-ViT), M3ViT with one gating function for all the tasks,
M3ViT with multi gates, and M3ViT with task-conditioned token input. Results on both PASCAL-
Context and NYUD-v2 datasets show that adding MoE layers into ViT with only one gating function
for all tasks can already improve the M-ViT model. Making MoE selection task-dependent can
further improve the performance, where multi-gating performs better than task-conditioned gating
design. Particularly, comparing our model performance with STL-B as well as previous SoTAs in
Table 1 and 2, we ﬁnd that our MoE model better demonstrates its effectiveness when more tasks
need to be encapsulated in the system. More results about our model’s performance on different
numbers of tasks can be found in the supplement.
4.4 Hardware Performance Results
Results comparing hardware performance metrics on FPGA and GPU are shown in Table 4. We ﬁrst
discuss our memory efﬁciency. The naive approach described in Section 3.2 would require 11.610
MiB of on-chip memory (too much for our FPGA platform), but our compute-reordering design
9
Table 4: Quantitative comparisons of hardware metrics between FPGA and GPU implementations.
CR indicates usage of our memory-efﬁcient computation reordering method (Section 3.2).
Platform Backbone CR Memory
(MiB)
PASCAL-Context NYUD-v2
Latency (ms) Energy (W·s) Latency (ms) Energy (W·s)
GPU ResNet-18 – 21.336 3.489 1.029 – –
GPU ResNet-50 – 44.939 – – 7.270 2.145
GPU ViT-small – 42.058 10.381 3.062 7.110 2.097
GPU MoE ViT-small – 82.747 25.239 7.446 27.760 8.189
FPGA ViT-small – 4.828 68.931 0.689 84.418 0.844
FPGA MoE ViT-small  4.840 637.478 6.375 750.557 7.506
FPGA MoE ViT-small  4.840 69.033 0.690 84.538 0.845
achieves the same result using only 4.840 MiB, demonstrating a 2.40×reduction. Moreover, when
comparing against a memory-constrained MoE ViT on FPGA without our compute-reordering method
(using the cache-based method described in Section 3.2), we see that our method takes 9.23×less
latency and energy on the PASCAL-Context dataset and 8.88×less latency and energy on NYUD-v2.
Our compute-reordering M3ViT on FPGA also beats all GPU baselines in energy efﬁciency; e.g., it
beats MoE ViT on GPU by 10.79×on PASCAL-Context and by 9.69×on NYUD-v2. For discussion
on the latency breakdown of M3ViT, please refer to the supplement.
5 Conclusion, Discussion of Limitation and Broader Impact
In this paper, we propose a model-accelerator co-design for efﬁcient on-device MTL. By customizing
MTL mixture-of-experts layers into a ViT backbone, we sparsely activate task-speciﬁc experts in
training to mitigate MTL gradient conﬂicts. For inference, we can activate only the sparse “expert”
pathway relevant to the task of interest for efﬁciency, and can further achieve zero-overhead switching
between tasks with our hardware-level co-design. Extensive experiments that show M3ViT surpasses
the top-performing encoder-focused MTL methods, reduces 88% FLOPs, and saves more than 8×
energy over our baseline. The limitation of our work is that M 3ViT is so far mainly evaluated on
academic datasets; we will try real applications like autonomous driving in the future. For broader
impact, our work can reduce the resource and energy consumption needed for MTL regimes, while
still maintaining SOTA performance, which can effectively serve the goal of Green AI.
Acknowledgment
Zhiwen Fan, Rishov Sarkar, Cong Hao and Zhangyang Wang are in part supported by the DARPA
In-Pixel Intelligent Processing (IP2) program.
References
[1] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler,
Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic
segmentation in the wild. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 891–898, 2014.
[2] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation
and support inference from rgbd images. In European conference on computer vision, pages
746–760. Springer, 2012.
[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.
[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herve Jegou. Training data-efﬁcient image transformers & distillation through attention. In
International Conference on Machine Learning, volume 139, pages 10347–10357, July 2021.
10
[5] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. InProceedings
of the IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021.
[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on
computer vision, pages 213–229. Springer, 2020.
[7] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense predic-
tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
12179–12188, 2021.
[8] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.
Segformer: Simple and efﬁcient design for semantic segmentation with transformers.Advances
in Neural Information Processing Systems, 34, 2021.
[9] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 568–578, 2021.
[10] Dong-Gyu Lee. Fast drivable areas estimation with multi-task learning for real-time au-
tonomous driving assistant. Applied Sciences, 11(22):10713, 2021.
[11] Wonsuk Kim and Junhee Seok. Indoor semantic segmentation for robot navigating on mobile.
In 2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN), pages
22–25. IEEE, 2018.
[12] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-
task model: Growing a neural network for multiple nlp tasks.arXiv preprint arXiv:1611.01587,
2016.
[13] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017.
[14] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin
Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE
transactions on pattern analysis and machine intelligence, 2021.
[15] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization.Advances
in neural information processing systems, 26, 2013.
[16] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 3712–3722, 2018.
[17] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task
relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
1930–1939, 2018.
[18] Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.
[19] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea
Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing
Systems, 33:5824–5836, 2020.
[20] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-
dient normalization for adaptive loss balancing in deep multitask networks. In International
Conference on Machine Learning, pages 794–803. PMLR, 2018.
[21] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai,
and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient
sign dropout. Advances in Neural Information Processing Systems, 33:2039–2050, 2020.
11
[22] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. Gradient vaccine: Investigating
and improving multi-task optimization in massively multilingual models. arXiv preprint
arXiv:2010.05874, 2020.
[23] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks
for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 3994–4003, 2016.
[24] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task
prioritization for multitask learning. In Proceedings of the European conference on computer
vision (ECCV), pages 270–287, 2018.
[25] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh
losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7482–7491, 2018.
[26] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L Yuille. Nddr-cnn: Layerwise feature
fusing in multi-task cnns by neural discriminative dimensionality reduction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3205–3214,
2019.
[27] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with
attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 1871–1880, 2019.
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.
[29] Introducing Pathways: A next-generation AI architecture. https://blog.google/
technology/ai/introducing-pathways-next-generation-ai-architecture/ .
[30] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538, 2017.
[31] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech
Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers.
Advances in Neural Information Processing Systems, 34:9895–9907, 2021.
[32] Yoshua Bengio. Deep learning of representations: Looking forward. In International confer-
ence on statistical language and speech processing, pages 1–37. Springer, 2013.
[33] Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. Multi-task learning for
classiﬁcation with dirichlet process priors. Journal of Machine Learning Research, 8(1), 2007.
[34] Laurent Jacob, Jean-philippe Vert, and Francis Bach. Clustered multi-task learning: A convex
formulation. Advances in neural information processing systems, 21, 2008.
[35] Jiayu Zhou, Jianhui Chen, and Jieping Ye. Clustered multi-task learning via alternating
structure optimization. Advances in neural information processing systems, 24, 2011.
[36] Kai Yu, V olker Tresp, and Anton Schwaighofer. Learning gaussian processes from multiple
tasks. In Proceedings of the 22nd international conference on Machine learning , pages
1012–1019, 2005.
[37] Su-In Lee, Vassil Chatalbashev, David Vickrey, and Daphne Koller. Learning a meta-level
prior for feature relevance from multiple related tasks. In Proceedings of the 24th international
conference on Machine learning, pages 489–496, 2007.
[38] Hal Daumé III. Bayesian multitask learning with latent hierarchies. arXiv preprint
arXiv:0907.0783, 2009.
[39] Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning.
arXiv preprint arXiv:1206.6417, 2012.
12
[40] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Søgaard. Latent multi-
task architecture learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pages 4822–4829, 2019.
[41] Liyan Sun, Zhiwen Fan, Xinghao Ding, Yue Huang, and John Paisley. Joint cs-mri reconstruc-
tion and segmentation with a uniﬁed deep network. In International conference on information
processing in medical imaging, pages 492–504. Springer, 2019.
[42] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks guided
prediction-and-distillation network for simultaneous depth estimation and scene parsing. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages
675–684, 2018.
[43] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-afﬁnitive
propagation across depth, surface normal and semantic segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4106–4115, 2019.
[44] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones,
and Jakob Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.
[45] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural
networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.
[46] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-
task vision and language representation learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 10437–10446, 2020.
[47] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a uniﬁed
transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 1439–1449, 2021.
[48] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for
computer vision. arXiv preprint arXiv:2111.11432, 2021.
[49] Matthew Wallingford, Hao Li, Alessandro Achille, Avinash Ravichandran, Charless Fowlkes,
Rahul Bhotika, and Stefano Soatto. Task adaptive parameter sharing for multi-task learning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 7561–7570, 2022.
[50] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, and Jian Yang. Joint task-
recursive learning for semantic segmentation and depth estimation. In Proceedings of the
European Conference on Computer Vision (ECCV), pages 235–251, 2018.
[51] Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Mti-net: Multi-scale task
interaction networks for multi-task learning. In European Conference on Computer Vision,
pages 527–543. Springer, 2020.
[52] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization.Advances
in neural information processing systems, 31, 2018.
[53] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive
mixtures of local experts. Neural computation, 3(1):79–87, 1991.
[54] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.
Neural computation, 6(2):181–214, 1994.
[55] Ke Chen, Lei Xu, and Huisheng Chi. Improved learning algorithms for mixture of experts in
multiclass classiﬁcation. Neural networks, 12(9):1229–1252, 1999.
[56] Seniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts.
IEEE Transactions on Neural Networks and Learning Systems, 23(8):1177–1193, 2012.
13
[57] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. Hash layers for
large sparse models. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan,
editors, Advances in Neural Information Processing Systems, 2021.
[58] Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan.
Tricks for training sparse translation models. arXiv preprint arXiv:2110.08246, 2021.
[59] David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in
a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.
[60] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with
conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
[61] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.
[62] Jakob V ogdrup Hansen. Combining predictors: comparison of ﬁve meta machine learning
methods. Information Sciences, 119(1-2):91–105, 1999.
[63] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base
layers: Simplifying training of large, sparse models. In International Conference on Machine
Learning, pages 6265–6274. PMLR, 2021.
[64] Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan
Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed
scaling laws for routed language models. arXiv preprint arXiv:2202.01169, 2022.
[65] Karim Ahmed, Mohammad Haris Baig, and Lorenzo Torresani. Network of experts for large-
scale image categorization. In European Conference on Computer Vision, pages 516–532.
Springer, 2016.
[66] Sam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale
weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6865–6873, 2017.
[67] Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell,
and Joseph E Gonzalez. Deep mixture of experts via shallow embedding. In Uncertainty in
artiﬁcial intelligence, pages 552–562. PMLR, 2020.
[68] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally
parameterized convolutions for efﬁcient inference. Advances in Neural Information Processing
Systems, 32, 2019.
[69] Alhabib Abbas and Yiannis Andreopoulos. Biased mixtures of experts: Enabling computer
vision inference under data transfer limitations. IEEE Transactions on Image Processing,
29:7656–7667, 2020.
[70] Svetlana Pavlitskaya, Christian Hubschneider, Michael Weber, Ruby Moritz, Fabian Huger,
Peter Schlicht, and Marius Zollner. Using mixture of expert models to gain insights into
semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops, pages 342–343, 2020.
[71] Raquel Aoki, Frederick Tung, and Gabriel L Oliveira. Heterogeneous multi-task learning with
expert diversity. arXiv preprint arXiv:2106.10595, 2021.
[72] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua
Chen, Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the
mixture of experts with applications to multi-task learning. Advances in Neural Information
Processing Systems, 34, 2021.
[73] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation
extraction with selective attention over instances. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2124–2133,
2016.
14
[74] Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable
attention model for natural language inference. arXiv preprint arXiv:1606.01933, 2016.
[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017.
[76] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever. Generative pretraining from pixels. In International Conference on Machine
Learning, pages 1691–1703. PMLR, 2020.
[77] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages
4055–4064. PMLR, 2018.
[78] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make
one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34,
2021.
[79] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. Vitgan:
Training gans with vision transformers. arXiv preprint arXiv:2107.04589, 2021.
[80] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. In
International Conference on Machine Learning, pages 10347–10357. PMLR, 2021.
[81] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing
sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information
Processing Systems, 34:19974–19988, 2021.
[82] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit
position encodings for vision transformers? arXiv e-prints, pages arXiv–2102, 2021.
[83] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in
transformer. arXiv preprint arXiv:2103.00112, 2021.
[84] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing
in deep vision transformers via the fourier domain analysis: From theory to practice. In
International Conference on Learning Representations, 2022.
[85] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei
Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from
a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pages 6881–6890, 2021.
[86] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk. Toward
transformer-based object detection. arXiv preprint arXiv:2012.09958, 2020.
[87] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction
with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1954–1963, 2021.
[88] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16259–
16268, 2021.
[89] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min
Hu. Pct: Point cloud transformer. Computational Visual Media, 7(2):187–199, 2021.
[90] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ra-
mamoorthi. Vision transformer for nerf-based view synthesis from a single input image. arXiv
preprint arXiv:2207.05736, 2022.
[91] Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, and
Zhangyang Wang. Is attention all nerf needs? arXiv e-prints, pages arXiv–2207, 2022.
15
[92] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture
transformer network for image super-resolution. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 5791–5800, 2020.
[93] Wenqing Zheng, Qiangqiang Guo, Hao Yang, Peihao Wang, and Zhangyang Wang. Delayed
propagation transformer: A universal computation engine towards practical control in cyber-
physical systems. Advances in Neural Information Processing Systems, 34, 2021.
[94] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from
transformers. arXiv preprint arXiv:1908.07490, 2019.
[95] Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, and Vibhav Vineet. Dall-e for
detection: Language-driven context image synthesis for object detection. arXiv preprint
arXiv:2206.09592, 2022.
[96] Mengshu Sun, Haoyu Ma, Guoliang Kang, Yifan Jiang, Tianlong Chen, Xiaolong Ma,
Zhangyang Wang, and Yanzhi Wang. Vaqf: Fully automatic software-hardware co-design
framework for low-bit vision transformer. arXiv preprint arXiv:2201.06618, 2022.
[97] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen, Mimi Xie,
Lipeng Wan, Hang Liu, and Caiwen Ding. Ftrans: energy-efﬁcient acceleration of transform-
ers using fpga. In Proceedings of the ACM/IEEE International Symposium on Low Power
Electronics and Design, pages 175–180, 2020.
[98] Panjie Qi, Yuhong Song, Hongwu Peng, Shaoyi Huang, Qingfeng Zhuge, and Edwin Hsing-
Mean Sha. Accommodating transformer onto fpga: Coupling the balanced model compression
and fpga-implementation optimization. In Proceedings of the 2021 on Great Lakes Symposium
on VLSI, pages 163–168, 2021.
[99] Hongwu Peng, Shaoyi Huang, Tong Geng, Ang Li, Weiwen Jiang, Hang Liu, Shusen Wang,
and Caiwen Ding. Accelerating transformer-based deep learning models on fpgas using
column balanced block pruning. In 2021 22nd International Symposium on Quality Electronic
Design (ISQED), pages 142–148. IEEE, 2021.
[100] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André
Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.
Advances in Neural Information Processing Systems, 34, 2021.
[101] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
[102] Chao Xiong, Xiaowei Zhao, Danhang Tang, Karlekar Jayashree, Shuicheng Yan, and Tae-
Kyun Kim. Conditional convolutional neural network for modality-aware face recognition.
In Proceedings of the IEEE International Conference on Computer Vision, pages 3667–3675,
2015.
[103] Jonathan Pilault, Amine Elhattami, and Christopher Pal. Conditionally adaptive multi-task
learning: Improving transfer learning in nlp using fewer parameters & less data. arXiv preprint
arXiv:2009.09139, 2020.
[104] Guolei Sun, Thomas Probst, Danda Pani Paudel, Nikola Popovi´c, Menelaos Kanakis, Jagruti
Patel, Dengxin Dai, and Luc Van Gool. Task switching network for multi-task learning. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8291–8300,
2021.
[105] David R Martin, Charless C Fowlkes, and Jitendra Malik. Learning to detect natural image
boundaries using local brightness, color, and texture cues. IEEE transactions on pattern
analysis and machine intelligence, 26(5):530–549, 2004.
[106] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence ,
40(4):834–848, 2017.
16
[107] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio
Savarese. Which tasks should be learned together in multi-task learning? In International
Conference on Machine Learning, pages 9120–9132. PMLR, 2020.
17
A Implementation Details
A.1 Scale-up Our Model
For the MTL encoder, we evaluate our model based on several variants of ViT following DeiT [4],
including ViT-tiny, ViT-small, and ViT-base. The ﬁnal ViT block’s output feature will be fed into
decoders for multi-task predictions. We embed MoE expert layers once in every two ViT blocks.
The router is a single-layer MLP which maps token embedding to experts’ selection probability. In
task-conditioned MoE ViT, the task embedding network T is a two-layer MLP of dimensions 64 and
64. As for MLP decoder, the previous SoTA works [27, 23, 26] uses Deeplab [106] as the decoder
for a ResNet backbone. However, Deeplab is deﬁned for Conv backbone and not suitable for ViT
encoder output. Therefore, we follow the prior work [ 85] and use a PUP [ 85] as decoder, which
is a progressive upsampling strategy that alternates conv layers and upsampling operations. Each
decoder contains ﬁve conv layers (the ﬁrst four of dimension 256 and the ﬁnal one of dimension
corresponding to task prediction) and four upsampling layers. This decoder is of lighter weight and
consumes fewer FLOPs than Deeplab. The output feature of last and second last conv layers will also
be used in a multi-tasks feature distillation module. The distillation module will only be used during
train stage and deactivated during inference stage, thus adding no extra FLOPs to the whole network.
A.2 Training Setup
Pre-training on ImageNet During the MTL pre-train stage, all the encoder backbones will be
pre-trained on ImageNet and the decoder will be randomly initialized. In the M-ViT models, we use
the pre-trained weights provided by DeiT [ 4] to initialize all the transformer layers and the input
linear projection layer in the encoder. In the MoE ViT models, we pre-train our encoder on ImageNet
following the same strategy as its counterpart DeiT ViT encoder in [4].
MTL Training For both NYUD-v2 and PASCAL-Context datasets, we adopt a polynomial learning
rate decay schedule and employ SGD as the optimizer with initial learning rate 0.002. Momentum
and weight decay are set to 0.9 and 0.0001, respectively. The batch size is 16.
A.3 Hardware Details
Platform Speciﬁcations Our targeted FPGA, the Xilinx ZCU104 FPGA, has 1,728 DSPs, 504K
LUTs, 461K registers, 11 Mbit block RAM, and 27 Mbit UltraRAM. Our GPU used for baseline
measurements, the NVIDIA Quadro RTX 8000, has 4,608 CUDA cores and 48 GB of GDDR6
memory. It runs at a clock frequency of 1,395 MHz and consumes 295 W of power.
B More Experiment Results
B.1 Additional Experiments on ViT-tiny and ViT-base
We further evaluate M3ViT on different variants of ViT including ViT-tiny and ViT-base; results are
shown in Table 5. We compare against STL-B, MTL-B, and SoTA encoder-focused MTL model
TAPS[49], Cross-Stitch [ 23]. For TAPS, we adopt joint MTL strategy for comparable training
longitude. It can be observed that MoE ViT-base increases the SoTA performance by a large margin,
achieving +4.00% on PASCAL-Context and +8.32% on NYUD-v2. Meanwhile, it also consumes
lower FLOPs compared to previous ResNet-based methods. MoE ViT-tiny consumes much fewer
FLOPs than all previous methods (in particular, less than 1/10 FLOPs of the previous SoTA method
Cross-Stitch). Additionally, our hardware co-design of MoE ViT-tiny achieves energy consumption
an order of magnitude lower than Cross-Stitch.
B.2 Additional Experiments on Different Numbers of Tasks
To evaluate the performance of our model, we further conduct experiments on different levels of
MTL difﬁculties with different numbers of tasks. We compare between STL-B, MTL-B, SoTA work
Cross-Stitch [23], MTL-B with ViT-small (M-ViT), and MTL-B with MoE ViT-small (M 3ViT);
results are shown in Table 6. It can be observed that M3ViT consistently outperforms MTL-B with
18
Table 5: Performance of M3ViT on ViT-tiny and ViT-base
PASCAL-Context Backbone Seg.
(mIoU↑)
Norm.
(mErr)↓
H. Parts
(mIoU)↑
Sal.
(mIoU)↑
Edge
(odsF) ↑
∆m
(%) ↑
FLOPS
(G) ↓
Energy
(W·s)↓
STL-B ResNet-18 66.2 13.9 59.9 66.3 68.8 0.00 167 1.029
MTL-B ResNet-18 63.8 14.9 58.6 65.1 69.2 −2.86 167 1.029
Cross-Stitch [23] ResNet-18 66.1 13.9 60.6 66.8 69.9 +0.60 647 6.001
M3ViT MoE ViT-tiny 65.3 15.2 57.9 64.2 68.5 −3.53 62 0.265
M3ViT MoE ViT-base 75.2 14.8 64.5 66.1 72.6 +4.00 161 2.325
NYUD-v2 Backbone Seg.
(mIoU)↑
Depth
(rmse)↓ – – – ∆m
(%) ↑
FLOPS
(G) ↓
Energy
(W·s) ↓
STL-B ResNet-50 43.9 0.585 – – – 0.00 192 2.145
MTL-B ResNet-50 44.4 0.587 – – – +0.41 192 2.145
TAPS[49] ResNet-50 44.5 0.581 – – – +1.05 192 2.312
Cross-Stitch [23] ResNet-50 44.2 0.570 – – – +1.61 310 4.221
M3ViT MoE ViT-tiny 40.3 0.643 – – – −9.05 74 0.351
M3ViT MoE ViT-base 49.1 0.557 – – – +8.32 191 2.798
less computational FLOPs on different numbers of tasks on both NYUD-v2 and PASCAL-Context.
Compared to SoTA encoder-focused work Cross-Stitch, although M3ViT performs slightly lower
on NYUD-v2 with two tasks, it achieves better performance on all the other settings. In particular,
it surpasses Cross-Stitch on NYUD-v2 when the number of tasks increases to four ( −0.91% vs.
−3.26%), which demonstrates the strong capacity of our model on handling more tasks. On PASCAL-
Context dataset, introducing MoE (M3ViT) can achieve much better performance than Cross-Stitch.
Noticing that M3ViT performs slightly worse on normal estimation and saliency detection tasks, we
speculate that it is because these two tasks require a relatively small receptive ﬁeld to retain a detailed
estimation, and Cross-Stitch allows to use limited local information (i.e., small receptive ﬁeld) when
fusing the activations from the different single-task networks. But for other tasks that require larger
receptive ﬁelds, our model performs signiﬁcantly better than Cross-Stitch, since our task-dependent
MoE design helps effectively avoid different tasks’ training conﬂict. Meanwhile, M3ViT consumes
much less computational power than previous methods.
Furthermore, we conduct experiments by choosing tasks from the large-scale Taskonomy dataset
[16]. Like our main manuscript, we use MTL-ViT-small as the baseline model and MTL-MoE-
ViT-small for our model. We increase the number of tasks from three to nine and perform detailed
evaluations. Following the same data pre-processing and evaluation method [ 107], we report the
relative performance improvement from M³ViT over the baseline MTL-ViT. As shown in the Table 7,
M³ViT demonstrates even stronger superiority as the number of tasks increases.
B.3 Comparisons with Decoder-focused Methods
Decoder-focused architectures typically require initial predictions or intermediate features of all the
tasks, both in training and inference, to improve the predictions. However, activating all tasks in
inference violates our motivation: sparsely activating the network to achieve efﬁcient MTL inference.
Moreover, those models consume a large number of FLOPs [ 14], which makes them difﬁcult to
deploy onto real-world edge devices with resource and latency constraints. This is because they need
higher parallelism factors, more resources, or clever tricks to hit the desired latency requirement,
which is out of scope of the discussion of this paper.
Ignoring the previously mentioned efﬁciency and memory bottleneck, we conduct comparisons
between our M3ViT-base model and decoder-focused work PAD-Net [42], which have similar FLOPs
(PAD-Net: 212 GFLOPs vs. Ours: 191 GFLOPs). Our MoE ViT-base model achieves higher
performance than PAD-Net on both the PASCAL Context dataset (Ours: +4.0% vs. PAD-Net:
-4.41%) and the NYUD-V2 dataset (Ours: +8.32% vs. PAD-Net: +7.43%).
C Latency Breakdown of Our Model
Our FPGA implementation of M3ViT using ViT-small takes 84.538 ms for inference on the NYUD-
v2 dataset, which is split between patch embedding, ViT layers, and MoE layers as shown in the
19
Table 6: Performance on different numbers of tasks
PASCAL-Context Backbone Seg.
(mIoU↑)
Norm.
(mErr)↓
H. Parts
(mIoU)↑
Sal.
(mIoU)↑
Edge
(odsF) ↑
∆m
(%) ↑
FLOPS
(G) ↓
STL-B ResNet-18 66.2 13.9 59.9 66.3 68.8 0.00 167
MTL-B ResNet-18 60.8 14.5 – – – −6.23 167
Cross-Stitch [23] ResNet-18 65.4 14.2 – – – −1.68 647
M-ViT MoE ViT-small 65.3 15.6 – – – −6.79 83
M3ViT MoE ViT-small 72.7 14.4 – – – +3.11 84
MTL-B ResNet-18 63.8 14.9 58.6 65.1 69.2 −2.86 167
Cross-Stitch [23] ResNet-18 66.1 13.9 60.6 66.8 69.9 +0.60 647
M-ViT MoE ViT-small 70.7 15.5 58.7 64.9 68.8 −1.76 83
M3ViT MoE ViT-small 72.8 14.5 62.1 66.3 71.7 +2.71 84
NYUD-v2 Backbone Seg.
(mIoU)↑
Depth
(rmse)↓
Norm.
(mErr)↓
Edge
(odsF) ↑ – ∆m
(%) ↑
FLOPS
(G) ↓
STL-B ResNet-50 43.9 0.585 19.8 68.4 – 0.00 192
MTL-B ResNet-50 44.4 0.587 – – – +0.41 192
Cross-Stitch [23] ResNet-50 44.2 0.570 – – – +1.61 310
M-ViT MoE ViT-small 40.9 0.631 – – – −6.27 100
M3ViT MoE ViT-small 45.6 0.589 – – – +1.59 100
MTL-B ResNet-50 41.9 0.618 21.3 69.0 – −4.22 192
Cross-Stitch [23] ResNet-50 42.2 0.629 20.1 68.3 – −3.26 310
M-ViT MoE ViT-small 40.9 0.636 21.5 65.0 – −7.28 100
M3ViT MoE ViT-small 44.8 0.612 20.1 68.6 – −0.91 100
Table 7: Performance on different numbers of tasks on Taskonomy dataset
Tasks Depth Norm. Seg. Edge Occ. Reshad. Key2d. Curvature Autoenc. Average
3 tasks 3.33% 0.44% 7.74% – – – – – – 3.84%
6 tasks 4.68% 2.58% 10.36% 0.80% 3.28% 8.20% – – – 4.98%
9 tasks 5.41% 1.58% 7.67% 0.34% 4.34% 5.06% 7.83% 0.26% 15.01% 5.28%
breakdown in Figure 6. As shown in this ﬁgure, the time required to compute all experts in the MoE
layers (18.567 ms) is nearly equal to the time required to compute the fully-connected layers within
the ViT layers (18.447 ms). This afﬁrms that our hardware computation reordering mechanism is
able to maintain memory efﬁciency with near-zero impact on latency.
Total: 84.538 ms
Patch embedding:
0.768 ms
Standard ViT layers:
41.825 ms
Self-attention:
23.378 ms
Fully-connected layers:
18.447 ms
MoE layers:
41.945 ms
Self-attention:
23.378 ms
Experts computation:
18.567 ms
Figure 6: A breakdown of the FPGA inference latency on the NYUD-v2 dataset. The total
latency can be split into the patch embedding step, the six standard ViT layers, and the six MoE
layers in the backbone. The ViT and MoE layers can further be divided into self-attention, which
is identical for both types of layers, and either the ViT fully-connected MLPs or the MoE experts
computation.
20