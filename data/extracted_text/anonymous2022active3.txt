This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
10
Active Inference as a Unified Theory
of Sentient Be hav ior
In general we are least aware of what our minds do best.
— Marvin Minsky
10.1 Introduction
In this chapter, we wrap up Active Inference’s main theoretical points (from
the first part of the book) and its practical implementations (from the sec-
ond part). Then, we connect the dots: we abstract away from the specific
Active Inference models discussed in previous chapters to focus on integrative
aspects of the framework. One benefit of Active Inference is that it provides
a complete solution to the adaptive probl ems that sentient organisms have
to solve. It therefore offers a unified perspective on probl ems like perception,
action sel ection, attention, and emotion regulation, which are usually treated
in isolation in psyc holo gy and neuroscience—a nd addressed using distinct
computational approaches in artificial intelligence. We will discuss the Active
Inference perspective on each of t hese probl ems (and more) in the context
of established theories, such as cybernetics, ideomotor theory of action, rein-
forcement learning, and optimal control. Fin ally, we briefly discuss how the
scope of Active Inference can be extended to cover other biological, social,
and technological topics that are not discussed in depth in this book.
10.2 Wrapping Up
This book offers a systematic account of the theoretical underp innings and
practical implementations of Active Inference. Here, we briefly summarize
the discussion of the first nine chapters. This offers an opportunity to
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
192 Chapter 10
rehearse the key constructs of Active Inference that w ill be useful in the
remainder of this chapter.
In chapter 1, we introduced Active Inference as a normative approach
to understanding sentient creatures that form part of action-p erception
loops with their environment (Fuster 2004). We explained that normative
approaches start from first princip les to derive and test empirical predic-
tions about the phenomenon of interest—h ere, the ways living organisms
persist while engaging in adaptive exchanges (action- perception loops) with
their environment. We also considered that one could arrive at Active Infer-
ence by following a low road or a high road.
In chapter 2, we illustrated the low road to Active Inference. This road
starts from the idea that the brain is a prediction machine, endowed with a
generative model: a probabilistic repres ent at ion of how hidden c auses in the
world generate sensations (e.g., how light reflected off an apple stimulates
the reti na). By inverting this model, it infers the c auses of its sensations
(e.g., w hether I am seeing an apple, given that my reti na is stimulated in a
certain way). This view of perception (aka perception-a s-i nference) has its
historical roots in the Helmholtzian notion of unconscious inference and,
more recently, in the Bayesian brain hypothesis. Active Inference extends
this view by bringing action control and planning within the compass of
inference (aka control-a s-i nference, planning-a s-i nference). Most impor-
tantly, it shows that perception and action are not quintessentially separa-
ble proc esses but fulfill the same objective. We first described this objective
more informally, as the minimization of a discrepancy between one’s model
and the world (which generally reduces to surprise or prediction error mini-
mization). Put simply, one can minimize the discrepancy between a model
and the world in two ways: by changing one’s mind to fit the world (per-
ception) or by changing the world to fit the model (action). T hese can be
described in terms of Bayesian inference. However, exact inference is often
intractable, so Active Inference uses a (variational) approximation (noticing
that exact inference may be seen as a special case of approximate inference).
This leads to the second, more formal description of the common objective
of perception and action, as variational f ree energy minimization. This is the
core quantity used in Active Inference and may be unpacked in terms of
its constituent parts (e.g., energy and entropy, complexity and accuracy,
or surprise and divergence). Fin ally, we introduced a second kind of f ree
energy: expected f ree energy. This is particularly import ant during planning,
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 193
as it affords a way to score alternative policies by considering the future
outcome that they are expected to generate. This too may be unpacked in
terms of its constituent parts (e.g., information gain and pragmatic value,
expected ambiguity and risk).
In chapter 3, we illustrated the high road to Active Inference. This alter-
native road starts from the deflationary imperative for biological organisms
to preserve their integrity and avoid dissipation, which can be described
as avoiding surprising states. We then introduced the notion of a Markov
blanket: a formalization of the statistical separation between the organism’s
internal states and the world’s external states. Crucially, internal and exter-
nal states can only influence each other vicariously via intermediate (active
and sensory) variables, called blanket states. This statistical separation—
mediated by the Markov blanket—is crucial to endowing an organism with
some degree of autonomy from the external world. To understand why this
is a useful perspective, consider the following three consequences.
First, an organism with a Markov blanket appears to model the exter-
nal environment in a Bayesian sense: its internal states correspond—on
average—to an approximate posterior belief about external states of the
world. Second, the autonomy is guaranteed by the fact that the organism’s
model (its internal states) is not unbiased but prescribes some existential
preconditions (or prior preferences) that must be maintained—f or example,
for a fish, being in the w ater. Third, equipped with this formalism, it is pos-
sib le to describe optimal beh avi or (with res pect to prior preferences) as the
maximization of (Bayesian) model evidence by perception and action. By
maximizing model evidence (i.e., self- evidencing) an organism ensures that
it realizes its prior preferences (e.g., a fish stays in the water) and avoids
surprising states. In turn, the maximization of model evidence is (approxi-
mately) mathematically equivalent to the minimization of variational free
energy—h ence we arrive again (in another way) at the same central con-
struct of Active Inference discussed in chapter 2. Fin ally, we detailed the
relationship between minimizing surprise and Hamilton’s princip le of least
Action. This evinces the formal relationship between Active Inference and
first princip les in statistical physics.
In chapter 4, we outlined the formal aspects of Active Inference. We focused
on the passage from Bayesian inference to a tractable approximation—
variational inference— and the resulting objective for organisms to mini-
mize variational f ree energy via perception and action. The insight from this
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
194 Chapter 10
treatment is the importance of the generative model that creatures use to
make sense of their world. We introduced two kinds of generative models
that express our beliefs about how data are generated, using discrete or con-
tinuous variables. We explained that both afford the same Active Inference,
but they apply when states of affairs are formulated in discrete time (as par-
tially observed Markov decision probl ems) or continuous time (as stochastic
differential equations), respectively.
In chapter 5, we remarked on the difference between the normative
princip le of free energy minimization and a proc ess theory about how this
princip le may be implemented by the brain—a nd explained that the lat-
ter generates testable predictions. We then outlined aspects of the proc ess
theories accompanying Active Inference, which encompass domains such
as neuronal message passing, including neuroanatomical circuitry (e.g.,
cortico-s ubcortical loops) and neuromodulation. For example, at an ana-
tomical level, message passing maps nicely to a canonical cortical microcir-
cuit, with predictions that stem from deep cortical layers at one level and
target superficial cortical layers at the level below (Bastos et al. 2012). At a
more systemic level, we discussed how Bayesian inference, learning, and
precision weighting correspond to neuronal dynamics, synaptic plasticity,
and neuromodulation, respectively, and how the top-d own and bottom-up
neural message passing of predictive coding maps to slower (e.g., alpha or
beta) and faster (e.g., gamma) brain rhythms. T hese and other examples
illustrate that a fter designing a specific Active Inference model, one can
draw neurobiological implications from the form of its generative model.
In chapter 6, we provided a r ecipe to design Active Inference models.
We saw that while all creatures minimize their variational free energy, they
behave in diff ere nt, sometimes opposite ways because they are endowed
with diff ere nt generative models. Therefore, what distinguishes diff ere nt
(e.g., simpler from more complex) creatures is just their generative model.
There is a rich repertoire of poss ib le generative models, which correspond to
diff ere nt biological (e.g., neuronal) implementations and produce diff ere nt
adaptive—or maladaptive—b eh avi ors in diff ere nt contexts and ecological
niches. This renders Active Inference equally appropriate for characterizing
simple creatures like bacteria that sense and seek nutrient gradients, com-
plex creatures like us that pursue sophisticated goals and engage in rich
cultural practices, or even diff ere nt individuals—to the extent that ones
appropriately characterizes their respective generative models. Evolution
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 195
appears to have discovered increasingly sophisticated design structures for
brains and bodies that made organisms able to deal with (and shape) rich
ecological niches. Modelers can reverse-e ngineer this proc ess and specify
the designs for brains and bodies of creatures of interest, in terms of genera-
tive models, based on the kinds of niche they occupy. This corresponds to a
series of design choices (e.g., models using discrete or categorical variables,
shallow or hierarchical models)—w hich we unpacked in the chapter.
In chapters 7 and 8, we provided numerous examples of Active Infer-
ence models in discrete and continuous time, which address probl ems of
perceptual inference, goal-d irected navigation, model learning, action con-
trol, and more. T hese examples w ere designed to showcase the variety of
emergent beh avi ors under these models and to detail the princip les of how
they are specified practically.
In chapter 9, we discussed how to use Active Inference for model- based
data analys is and to recover the para meters of an individual’s generative
model, which better explain the subject’s beh avi or in a task. This computa-
tional phenotyping uses the same form of Bayesian inference discussed in the
rest of the book, but in a diff ere nt way: it helps design and evaluate (objec-
tive) models of o thers’ (subjective) models.
10.3 Connecting the Dots: The Integrative Perspective
of Active Inference
Some dec ades ago, the phil oso p her Dennett lamented that cognitive sci-
entists devoted too much effort to modeling isolated subsystems (e.g., per-
ception, language understanding) whose bounda ries are often arbitrary. He
suggested to try instead modeling “the whole iguana”: a complete cognitive
creature (perhaps a s imple one) and an environmental niche for it to cope
with (Dennett 1978).
One benefit of Active Inference is that it offers a first princip le account of
the ways in which organisms solve their adaptive probl ems. The normative
approach pursued in this book assumes that it is poss ib le to start from the
princip le of variational f ree energy minimization and derive implications
about specific cognitive proc esses, such as perception, action sel ection,
attention and emotion regulation, and their neuronal underp innings.
Imagine a s imple creature that must solve probl ems like finding food
or shelter. When cast as Active Inference, the creature’s probl ems can be
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
196 Chapter 10
described in enactive terms, as acting to solicit preferred sensations (e.g.,
food-r elated sensations). To the extent that t hese preferred sensations are
included (as prior beliefs) in its generative model, the organism is effectively
gathering evidence for its model—or, more allegorically, for its existence
(i.e., maximizing model evidence or self-e videncing). This simple princi-
ple has ramifications for psychological functions traditionally considered
in isolation, such as perception, action control, memory, attention, inten-
tion, emotion, and more. For example, perception and action are both self-
evidencing, in the sense that a creature can align what it expects, given its
generative model, with what it senses e ither by changing its beliefs (about
the presence of food) or by changing the world (soliciting food-r elated sen-
sations). Memory and attention can also be thought of as optimizing the
same objective. Long-t erm memory develops through learning the par-
ameters of a generative model. Working memory is belief updating when
beliefs are about external states in the past and f uture. Attention is the opti-
mization of beliefs about the precision of sensory input. Forms of planning
(and intentionality) can be conceptualized by appealing to the capacity of
(some) creatures to select among alternative futures, which in turn requires
temporally deep generative models. T hese predict the outcomes that would
result from a course of action and are optimistic about t hese outcomes. This
optimism manifests as the belief that f uture outcomes w ill lead to preferred
outcomes. Deep temporal models can also help us understand sophisticated
forms of prospection (where beliefs about the pres ent are used to derive
beliefs about the f uture) and retrospection (where beliefs about the pres ent
are used to update beliefs about the past). Forms of interoceptive regulation
and emotion can be conceptualized by appealing to generative models of
internal physiology that predict the allostatic consequences of f uture events.
As the above examples illustrate, t here is an import ant consequence
of studying cognition and beh avi or from the perspective of a normative
theory of sentient beh avi or. Such theory does not start by assembling sepa-
rate cognitive functions, such as perception, decision-m aking, and plan-
ning. Rather, it starts by providing a complete solution to the probl ems
that organisms have to solve and then analyzing the solution to derive
implications about cognitive functions. For example, which mechanisms
permit a living organism or artificial creature (e.g., a robot) to perceive the
world, remember it, or plan (Verschure et al. 2003, 2014; Verschure 2012;
Pezzulo, Barsalou et al. 2013; Krakauer et al. 2017)? This is an impor tant
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 197
move as the taxonomies of cognitive functions—u sed in psyc holo gy and
neuroscience textbooks—l argely inherit from early philosophical and psy-
chological theories (sometimes called Jamesian categories). Despite their
great heuristic value, they may be quite arbitrary—or they may not corre-
spond to separate cognitive and neural proc esses (Pezzulo and Cisek 2016,
Buzsaki 2019, Cisek 2019). Indeed, t hese Jamesian categories may be can-
didates for how our generative models explain our engagement with the
sensorium—as opposed to explaining that engagement. For example, the
solipsistic hypothesis that “I am perceiving” is just my explanation for cur-
rent states of affairs that include my belief updating.
Adopting a normative perspective may also help in identifying formal
analogies between cognitive phenomena studied in dif fer ent domains.
One example is the trade-o ff between exploration and exploitation, which
appears in vario us guises (Hills et al. 2015). This trade-o ff is often studied
during foraging, when creatures must choose between exploiting previous
successful plans and exploring novel (potentially better) ones. However, the
same trade-o ff occurs during memory search and deliberation with l imited
resources (e.g., time limitations or search effort), when creatures have the
choice between exploiting their current best plan versus investing more time
and cognitive effort to explore additional possibilities. Characterizing t hese
apparently disconnected phenomena in terms of f ree energy can potentially
reveal deep similarities (Friston, Rigoli et al. 2015; Pezzulo, Cartoni et al. 2016;
Gottwald and Braun 2020).
Fin ally, in addition to a unified perspective on psychological phenom-
ena, Active Inference offers a principled means of understanding the cor-
responding neural computations. In other words, it offers a proc ess theory
that connects cognitive proc essing to (expected) neuronal dynamics. Active
Inference assumes that everyt hing that m atters about brains, minds, and
beh avi or can be described in terms of the minimization of variational free
energy. In turn, this minimization has specific neural signatures (in terms of,
e.g., message passing or brain anatomy) that can be empirically validated.
In the rest of this chapter, we explore some implications of Active Infer-
ence for psychological functions—as if we w ere sketching a psyc holo gy
textbook. For each of these functions, we also highlight some points of
contact (or divergence) between Active Inference and other popul ar theo-
ries in the lit er a ture.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
198 Chapter 10
10.4 Predictive Brains, Predictive Minds, and Predictive Proc essing
I have this picture of pure joy
it’s of a child with a gun
he’s aiming straight in front of himself,
shooting at something that isn’t there.
— Afterhours, “Quello che non c’è” (Something that i sn’t there)
Traditional theories of brain and cognition emphasize feedforward trans-
ductions from external stimuli to internal repres ent at ions and then motor
actions. This has been called a “sandwich model,” as everyt hing that is
in between stimuli and responses is assigned the label “cognitive” (Hurley
2008). In this perspective, the main function of the brain is to transform
incoming stimuli into contextually appropriate responses.
Active Inference departs significantly from this view by emphasizing
predictive and goal-d irected aspects of brain and cognition. In psycho-
logical terms, Active Inference creatures (or their brains) are probabilistic
inference machines, which continuously generate predictions based on their
generative models.
Self-e videncing creatures use their predictions in two fundamental ways.
First, they compare predictions with incoming data to validate their hypoth-
eses (predictive coding) and—at a slower timescale—r evise their models
(learning). Second, they enact predictions to guide the ways they gather
data (Active Inference). By d oing so, Active Inference creatures fulfill two
imperatives: epistemic (e.g., visually exploring places where salient informa-
tion is pres ent that can resolve uncertainty about hypotheses or models)
and pragmatic (e.g., moving to locations where preferred observations such
as rewards can be secured). The epistemic imperative renders both percep-
tion and learning active proc esses, whereas the pragmatic imperative renders
be hav ior goal directed.
10.4.1 Predictive Pro cessing
This predictive- and goal- centric view of brain— and cognition—is closely
related to (and provided inspiration for) predictive pro cessing (PP): an emerg-
ing framework in philosophy of mind and epistemology, which sees pre-
diction as central to brain and cognition and appeals to concepts of
“predictive brains” or “predictive minds” (Clark 2013, 2015; Hohwy 2013).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 199
Sometimes PP theories appeal to the specific functioning of Active Infer-
ence and some of its constructs, such as generative models, predictive
coding, free energy, precision control, and Markov blankets, but they
sometimes appeal to other constructs, such as coupled inverse and for-
ward models, which are not part of Active Inference. Therefore, the term
predictive proc essing is used in a broader (and less constrained) sense com-
pared to Active Inference.
Predictive proc essing theories have attracted considerable attention in
philosophy, given their potential for unification in many senses: across mul-
tiple domains of cognition, including perception, action, learning, and psy-
chopathology; from lower (e.g., sensorimotor) to higher levels of cognitive
proc essing (e.g., psychological constructs); from s imple biological organisms
to brains, individuals, and social and cultural constructs. Another appeal
of PP theories is that they make use of conceptual terms, such as beliefs
and surprise, which speak to a psychological level of analys is familiar to phi-
loso p hers (with the caveat that sometimes t hese terms may have technical
meanings that differ from common usage).
Yet, as the interest in PP grows, it has become apparent that phil oso p hers
have diff ere nt opinions on its theoretical and epistemological implications.
For example, it has been interpreted in internalist (Hohwy 2013), embod-
ied or action-b ased (Clark 2015), and enactivist and nonrepres ent at ional
terms (Bruineberg et al. 2016, Ramstead et al. 2019). The debate around
these conceptual interpretations goes beyond the scope of this book.
10.5 Perception
You c an’t depend on your eyes when your imagination is out of focus.
— Mark Twain
Active Inference considers perception as an inferential proc ess based on a
generative model of how sensory observations are generated. Bayes’ rule
essentially inverts the model to compute a belief about the hidden state of
the environment, given the observations. This idea of perception-a s-i nference
dates back to Helmholtz (1866) and was often reproposed in psyc holo gy, com-
putational neuroscience, and machine learning (e.g., analysis-b y-s ynthesis)
(Gregory 1980, Dayan et al. 1995, Mesulam 1998, Yuille and Kersten 2006).
This generative modeling approach has been demonstrated to be effective in
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
200 Chapter 10
facing challenging perceptual probl ems, such as breaking text-b ased CAPT-
CHAs (George et al. 2017).
10.5.1 Bayesian Brain Hypothesis
The most prominent cont emporary expression of this idea is the Bayes-
ian brain hypothesis, which has been applied to several domains such as
decision-m aking, sensory proc essing, and learning (Doya 2007). Active Infer-
ence provides a normative foundation to these inferential ideas by deriv-
ing them from the imperative of minimizing variational f ree energy. As
the same imperative extends to action dynamics, Active Inference naturally
models active perception and the ways in which organisms actively sample
observations to test their hypotheses (Gregory 1980). U nder the Bayesian
brain agenda, instead, perception and action are modeled in terms of dif-
fere nt imperatives (where action requires Bayesian decision theory; see
section 10.7.1).
More broadly, the Bayesian brain hypothesis refers to a f amily of approa-
che s that are not necessarily integrated and often make diff ere nt empirical
predictions. T hese include, for example, the computational-l evel proposal
that the brain performs Bayes-o ptimal sensorimotor and multisensory inte-
gration (Kording and Wolpert 2006), the algorithmic-l evel proposal that
the brain implements specific approximations of Bayesian inference, such
as decision-b y-s ampling (Stewart et al. 2006), and the neural-l evel proposals
about the specific ways in which neural populations may perform proba-
bilistic computations or encode probability distributions—f or example, as
samples or probabilistic population codes (Fiser et al. 2010, Pouget et al.
2013). At each level of explanation, t here are competing theories on the
field. For example, it is common to appeal to approximations of exact
Bayesian inference to explain deviations from optimal beh avi or, but diff er-
ent works consider diff ere nt (and not always compatible) approximations,
such as diff ere nt sampling approaches. More broadly, the relations between
proposals at diff ere nt levels are not always straightforward. This is b ecause
Bayesian computations can be realized (or approximated) in multiple algo-
rithmic ways, even without explici tly representing probability distributions
(Aitchison and Lengyel 2017).
Active Inference provides a more integrated perspective that connects
normative princip les and proc ess theories. At the normative level, its central
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 201
assumption is that all proc esses minimize variational free energy. The corre-
sponding proc ess theory for inference uses a gradient descent on f ree energy,
which has clear neurophysiological implications, explored in chapter 5
(Friston, FitzGerald et al. 2016). More broadly, one can start from the
princip le of free energy minimization to derive implications about brain
architectures.
For example, the canonical proc ess model of perceptual inference (in
continuous time) is predictive coding. Predictive coding was initially pro-
posed as a theory of hierarchical perceptual proc essing by Rao and Ballard
(1999) to explain a range of documented top-d own effects, which were dif-
ficult to reconcile with feedforward architectures as well as known physio-
logical facts (e.g., the existence of forward, or bottom-up, and backward, or
top-d own, connections in sensory hierarchies). However, predictive coding
can be derived from the princip le of f ree energy minimization, u nder some
assumptions, such as the Laplace approximation (Friston 2005). Further-
more, Active Inference in continuous time can be constructed as a directed
extension of predictive coding into the domain of action—by endowing a
predictive coding agent with motor reflexes (Shipp et al. 2013). This leads
us to the next point.
10.6 Action Control
If you c an’t fly then run, if you c an’t run then walk, if you c an’t walk then crawl,
but whate ver you do you have to keep moving forward.
— Martin Luther King
In Active Inference, action proc essing is analogous to perceptual pro-
cessing, as both are guided by forward predictions—e xteroceptive and
proprioceptive, respectively. It is the (proprioceptive) prediction that “my
hand grasps the cup” that induces a grasping movement. The equivalence
between action and perception exists also at the neurobiological level: the
architecture of the motor cortex is org an ized in the same way as the sensory
cortex—as a predictive coding architecture, with the exceptions that it can
influence motor reflexes in the brain stem and spine (Shipp et al. 2013)
and that it receives relatively little ascending input. Motor reflexes per-
mit controlling movement by setting “equilibrium points” along a desired
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
202 Chapter 10
trajectory—an idea that corresponds to the equilibrium point hypothesis
(Feldman 2009).
Importantly, initiating an action—l ike grasping a cup—r equires regula-
tion of the precision (inverse variance) of prior beliefs and sensory streams
appropriately. This is b ecause the relative values of t hese precisions deter-
mine the way in which a creature manages the conflict between its prior
belief (that it holds the cup) and its sensory input (signaling that it does
not). An imprecise prior belief about grasping a cup can be easily revised in
the light of conflicting sensory evidence—p roducing a change of mind and
no action. Rather, when the prior belief dominates (i.e., has higher preci-
sion), it is maintained even in the face of conflicting sensory evidence—a nd
it induces a grasping action to resolve the conflict. To ensure that this is the
case, action initiation induces a transient sensory attenuation (or down-
weighting sensory prediction errors). Failure of this sensory attenuation can
have maladaptive consequences, such as the failure to initiate or control
movements (Brown et al. 2013).
10.6.1 Ideomotor Theory
In Active Inference, action stems from (proprioceptive) predictions and
not motor commands (Adams, Shipp, and Friston 2013). This idea connects
Active Inference to ideomotor theory of action: a framework to understand
action control that dates back to William James (1890) and the l ater theo-
ries of “event coding” and “anticipatory behavioural control” (Hommel
et al. 2001, Hoffmann 2003). Ideomotor theory suggests that action- effect
links (similar to forward models) are key mechanisms in the architecture
of cognition. Importantly, t hese links can be used bidirectionally. When
they are used in the action- effect direction, they permit generating sensory
predictions; when they are used in the effect- action direction, they permit
selecting actions that achieve desired perceptual consequences—i mplying
that actions are selected and controlled on the basis of their predicted con-
sequences (hence the term ideo + motor). This anticipatory view of action
control is supported by a body of lit era t ure that documents the effects
of (anticipated) action consequences on action sel ection and execution
(Kunde et al. 2004). Active Inference provides a mathematical character-
ization of this idea that also includes additional mechanisms, such as the
importance of precision control and sensory attenuation, which are not
fully investigated in (but are compatible with) ideomotor theory.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 203
10.6.2 Cybernetics
Active Inference is closely related to cybernetic ideas about the purposeful,
goal-d irected nature of beh avi or and the importance of (feedback-b ased)
agent-e nvironment interactions, as exemplified by the TOTE (Test, Oper-
ate, Test, Exit) and related models (Miller et al. 1960; Pezzulo, Baldassarre
et al. 2006). In both TOTE and Active Inference, the sel ection of actions
is determined by the discrepancy between a preferred (goal) state and the
current state. T hese approaches diverge from s imple stimulus-r esponse rela-
tionships, as more commonly assumed in behaviorist theory and compu-
tational frameworks like reinforcement learning (Sutton and Barto 1998).
The notion of action control in Active Inference is particularly akin to
perceptual control theory (Powers 1973). Central to perceptual control theory
was the notion that what is controlled is a perceptual state, not a motor
output or action. For example, while driving, what we control—a nd keep
stable over time in the face of disturbances—is our reference or desired
velocity (e.g., 90 mph), as signaled by the speedometer, whereas the actions
we select for this (e.g., accelerating or decelerating) are more variable and
context dependent. For example, depending on the disturbance (e.g., wind,
a steep road, or other cars), we would need to either accelerate or decelerate
to maintain the reference velocity. This view implements William James’s
(1890) suggestion that “h umans achieve stable goals via flexible means.”
While in both Active Inference and perceptual control theory it is a per-
ceptual (and specifically a proprioceptive) prediction that controls action,
the two theories differ in how control is operated. In Active Inference but
not perceptual control theory, action control has anticipatory or feedfor-
ward aspects, based on generative models. In contrast, perceptual control
theory assumes that feedback mechanisms are largely sufficient to control
beh avi or, whereas trying to predict a disturbance, or exerting feedforward
(or open-l oop) control, is worthless. However, this objection was mainly
intended to address the limitations of control theories that use inverse-
forward models (see next section). U nder Active Inference, generative or
forward models are not used to predict a disturbance but to predict future
(desired) states and trajectories to be fulfilled by acting—a nd to infer the
latent cause of perceptual events.
Fin ally, another import ant point of contact between Active Inference and
perceptual control theory is the way they conceptualize control hierarchies.
Perceptual control theory proposes that higher hierarchical levels control
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
204 Chapter 10
lower hierarchical levels by setting their reference points or set-p oints (i.e.,
what they have to achieve) by leaving them f ree to select the means to
achieve them rather than by setting or biasing the actions that the lower
levels have to perform (i.e., how to operate). This stands in contrast with
most theories of hierarchical and top-d own control, in which higher levels
either directly select plans (Botvinick 2008) or bias the sel ection of actions
or motor commands at lower hierarchical levels (Miller and Cohen 2001).
Similar to perceptual control theory, in Active Inference one can decompose
hierarchical control in terms of a (top-d own) cascade of goals and subgoals,
which can be autonomously achieved at the appropriate (lower) levels. Fur-
thermore, in Active Inference, the contribution of goals represented at dif-
fere nt levels of the control hierarchy can be modulated (precision weighted)
by motivational proc esses, in such a way that the more salient or urgent
goals are prioritized (Pezzulo, Rigoli, and Friston 2015, 2018).
10.6.3 Optimal Control Theory
The way Active Inference accounts for action control is significantly diff er-
ent from other models of control in neuroscience, such as optimal control
theory (Todorov 2004, Shadmehr et al. 2010). This framework assumes that
the brain’s motor cortex selects actions using a (reactive) control policy
that maps stimuli to responses. Active Inference, instead, assumes that the
motor cortex conveys predictions, not commands.
Furthermore, while both optimal control theory and Active Inference
appeal to internal models, they describe internal modeling in diff ere nt
ways (Friston 2011). In optimal control, t here is a distinction between two
kinds of internal models: inverse models encode stimulus-r esponse contin-
gencies and select motor commands (according to some cost function),
whereas forward models encode action-o utcome contingencies and provide
inverse models with simulated inputs to replace noisy or delayed feedback,
hence g oing beyond a pure feedback control scheme. Inverse and forward
models can also operate in a loop that is detached from external action-
perception (i.e., when inputs and outputs are suppressed) to support inter-
nal, “what if” simulations of action sequences. Such internal simulations of
action have been linked to vario us cognitive functions, such as planning,
action perception, and imitation in social domains ( Jeannerod 2001, Wolpert
et al. 2003) as well as vario us disorders of movement and psychopatholo-
gies (Frith et al. 2000).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 205
In contrast to the forward-i nverse modeling scheme, in Active Inference
forward (generative) models do the heavy lifting of action control, whereas
inverse models are minimalistic and often reduce to s imple reflexes resolved
at the peripheral level (i.e., in the brain stem or spinal cord). Action is ini-
tiated when t here is a difference between anticipated and observed states
(e.g., desired, current arm positions)—t hat is, a sensory prediction error.
This means a motor command is equivalent to a prediction made by the
forward model as opposed to something computed by an inverse model as
in optimal control. The sensory (more precisely, proprioceptive) prediction
error is resolved by an action (i.e., arm movement). The gap to be filled by
action is considered so small that it does not require a sophisticated inverse
model but a much simpler motor reflex (Adams, Shipp, and Friston 2013).1
What renders a motor reflex simpler than an inverse model is that it does
not encode a mapping from inferred states of the world to action but a
much simpler mapping between action and sensory consequences. See
Friston, Daunizeau et al. (2010) for further discussion.
Another crucial difference between optimal motor control and Active
Inference is that the former uses a notion of cost or value function to moti-
vate action, whereas the latter replaces it with the Bayesian notion of prior
(or prior preference, implicit in expected free energy)—as we discuss in the
next section.
10.7 Utility and Decision-M aking
Action expresses priorities.
— Mahatma Gandhi
The notion of a cost or value function of states is central in many fields, such
as optimal motor control, economic theories of utility maximization, and
reinforcement learning. For example, in optimal control theory, the optimal
control policy for a reaching task is often defined as the one that minimizes
a specific cost function (e.g., is smoother or has minimum jerk). In rein-
forcement learning probl ems, such as navigating in a maze that includes
one or more rewards, the optimal policy is the one that permits maximizing
(discounted) reward while also minimizing movement costs. T hese prob-
lems are often solved using the Bellman equation (or the Hamilton-J acobi-
Bellman equation in continuous time), whose general idea is that the value
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
206 Chapter 10
of a decision can be decomposed in two parts: the immediate reward and
the value of the remaining part of the decision probl em. This decomposition
affords the iterative procedure of dynamic programming, which is at the core
of control theory and reinforcement learning (RL) (Bellman 1954).
Active Inference differs from the above approach in two main ways. First,
Active Inference does not consider utility maximization alone but the broader
objective of (expected) f ree energy minimization, which also includes addi-
tional (epistemic) imperatives, such as the disambiguation of current state
and novelty seeking (see figure 2.5). T hese additional objectives are some-
times added on to classical rewards—f or example, as a “novelty bonus”
(Kakade and Dayan 2002) or “intrinsic reward” (Schmidhuber 1991, Oudeyer
et al. 2007, Baldassarre and Mirolli 2013, Gottlieb et al. 2013)—b ut they
arise automatically in Active Inference, enabling it to resolve exploration-
exploitation trade-o ffs implicit in many decisions. The reason for this is that
free energies are functionals of beliefs, which means we are in the realm of
belief optimization as opposed to external reward functions. This is essen-
tial in explorative probl ems, wherein success depends on resolving as much
uncertainty as pos si ble.
Second, in Active Inference, the notion of cost is absorbed into the prior.
The prior (or prior preference) specifies an objective for control—f or exam-
ple, a trajectory to follow or an endpoint to reach. Using priors to encode
preferred observations (or sequences) may be more expressive than using
utilities (Friston, Daunizeau, and Kiebel 2009). Using this method, find-
ing the optimal policy is recast as a probl em of inference (of a sequence of
control states that realize the preferred trajectory) and does not require a
value function or the Bellman equation— although can appeal to a simi-
lar recursive logic (Friston, Da Costa et al. 2020). There are at least two
fundamental differences between the ways priors and value functions are
normally used in Active Inference and RL, respectively. First, RL methods
use value functions of states or of state- action pairs—w hereas Active Infer-
ence uses priors over observations. Second, value functions are defined in
terms of the expected return of being in a state (or performing an action in
a state) following a specific policy—t hat is, the sum of f uture (discounted)
rewards obtained by starting in the state and then executing the policy.
In contrast, in Active Inference, priors do not usually sum future rewards,
nor do they discount them. Rather, something analogous to the expected
return only emerges in Active Inference when the expected f ree energy
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 207
of a policy is calculated. The implication is that expected f ree energy is
the closest analogue to the value function. However, even this differs in
the sense that expected f ree energy is a functional of beliefs about states,
not a function of states. Having said this, it is poss ib le to construct pri-
ors that resemble value functions of states in RL—f or example, by caching
expected f ree energy calculations in t hese states (Friston, FitzGerald et al.
2016; Maisto, Friston, and Pezzulo 2019).
Furthermore, absorbing the notion of utility into the prior has an impor-
tant theoretical consequence: priors play the role of goals and render the
generative model biased—or optimistic, in the sense that the creature
believes it w ill encounter preferred outcomes. It is this optimism that under-
writes inferred plans that achieve desired outcomes in Active Inference; a
failure of this sort of optimism may correspond to apathy (Hezemans et al.
2020). This stands in contrast with other formal approaches to decision-
making, such as Bayesian decision theory, which separate the probability
of events from their utility. Having said this, this distinction is somewhat
superficial, as a utility function can always be rewritten as encoding a prior
belief, consistent with the fact that be havi ors that maximize a utility func-
tion are a priori (and by design) more probable. From one (slightly tauto-
logical) deflationary perspective, this is the definition of utility.
10.7.1 Bayesian Decision Theory
Bayesian decision theory is a mathematical framework that extends the ideas
of the Bayesian brain (discussed above) to the domains of decision-m aking,
sensorimotor control, and learning (Kording and Wolpert 2006, Shadmehr
et al. 2010, Wolpert and Landy 2012). Bayesian decision theory describes
decision-m aking in terms of two distinct proc esses. The first proc ess uses
Bayesian computations to predict the probability of f uture (action- or policy-
dependent) outcomes, and the second proc ess defines the preference over
plans, using a (fixed or learned) utility or cost function. The final decision
(or action sel ection) proc ess integrates both streams, thus selecting (with
higher probability) the action plan that has the higher probability of yield-
ing the higher reward. This stands in contrast to Active Inference, in which
the prior distribution directly signals what is valuable for the organism (or
what has been valuable during evolutionary history). However, parallels
could be drawn between the two streams of Bayesian decision theory and
the optimization of variational and expected f ree energy, respectively. U nder
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
208 Chapter 10
Active Inference, the minimization of variational free energy affords accu-
rate (and s imple) beliefs about the state of the world and its likely evolution.
The prior belief that expected f ree energy w ill be minimized through policy
sel ection incorporates the notion of preferences.
In some circles, t here are concerns about the status of Bayesian decision
theory. This follows from the complete class theorems (Wald 1947, Brown
1981) that say for any given pair of decisions and cost functions, t here
exist some prior beliefs that render the decisions Bayes optimal. This means
that t here is an implicit duality or degeneracy when dealing separately with
prior beliefs and cost functions. In one sense, Active Inference resolves this
degeneracy by absorbing utility or cost functions into prior beliefs in the
form of preferences.
10.7.2 Reinforcement Learning
Reinforcement learning (RL) is an approach to solving Markov decision
probl ems that is popul ar in both artificial intelligence and the cognitive sci-
ences (Sutton and Barto 1998). It focuses on how agents learn a policy (e.g.,
pole balancing strategy) by trial and error: by trying out actions (e.g., move
to the left) and receiving positive or negative reinforcements, depending on
action success (e.g., pole balanced) or failure (e.g., pole fallen).
Active Inference and RL address overlapping sets of probl ems but differ
in many res pects mathematically and conceptually. As noted above, Active
Inference dispenses with the notions of reward, value functions, and Bell-
man optimality that are key to reinforcement learning approaches. Further-
more, the notion of policy is used differently in the two frameworks. In RL a
policy denotes a set of stimulus-r esponse mappings that need to be learned.
In Active Inference, a policy is part of the generative model: it denotes a
sequence of control states that need to be inferred.
Reinforcement learning approaches are plentiful, but they can be sub-
divided into three main families. The first two methods try to learn good
(state or state-a ction) value functions, albeit in two diff ere nt ways.
Model- free methods of RL learn value functions directly from experience:
they perform actions, collect rewards, update their value functions, and
use them to update their policies. The reason they are called model- free is
because they do not use a (transition) model that permits predicting f uture
states—of the sort used in Active Inference. Instead, they implicitly appeal
to simpler kinds of models (e.g., state-a ction mappings). Learning value
functions in model-f ree RL often involves computing reward prediction
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 209
errors, as in the popul ar temporal-d ifference rule. While Active Inference
often appeals to prediction errors, these are state prediction errors (as there
is no notion of reward in Active Inference).
Model- based methods of RL do not learn value functions or policies directly
from experience. Rather, they learn a model of the task from experience,
use the model to plan (simulate poss ib le experiences), and update value
functions and policies from t hese simulated experiences. While both Active
Inference and reinforcement learning appeal to model-b ased planning, they
use it differently. In Active Inference, planning is the computation of the
expected f ree energy for each policy, not a means to update value functions.
Arguably, if the expected f ree energy is seen as a value functional, it could
be said that inferences drawn using the generative model are used to update
this functional—o ffering a point of analogy between t hese approaches.
The third f amily of RL approaches, policy gradient methods, tries to optimize
policies directly, without intermediate value functions, which are central to
both model-b ased and model-f ree RL. T hese methods start from par ame -
terized policies, able to generate (for example) movement trajectories, and
then optimizes them by changing the para meters to increase (decrease) the
likelihood of a policy if the trajectory results in a high (low) positive reward.
This approach relates policy gradient methods to Active Inference, which
also dispenses with value functions (Millidge 2019). However, the general
objective of policy gradients (maximizing long-t erm cumulative reward) dif-
fers from Active Inference.
Besides the formal differences between Active Inference and RL, t here
are also several import ant conceptual differences. One difference regards
how the two approaches interpret goal-d irected and habitual beh avi or. In
the animal learning lite ra t ure, goal-d irected choices are mediated by the
(prospective) knowledge of the contingency between an action and its out-
come (Dickinson and Balleine 1990), whereas habitual choices are not pro-
spective and depend on simpler (e.g., stimulus-r esponse) mechanisms. A
popul ar idea in RL is that goal-d irected and habitual choices correspond to
model- based and model- free RL, respectively, and that t hese are acquired in
parallel and continuously compete to control beh avi or (Daw et al. 2005).
Active Inference instead maps goal-d irected and habitual choices to dif-
fere nt mechanisms. In Active Inference (in discrete time), policy sel ection is
quintessentially model-b ased and hence fits the definition of goal- directed,
deliberative choices. This is similar to what happens in model-b ased RL, but
with a difference. In model-b ased RL, actions are selected in a prospective
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
210 Chapter 10
manner (using a model) but are controlled in a reactive way (using stimulus-
response policies); in Active Inference, actions can be controlled in a proac-
tive way—t hrough fulfilling proprioceptive predictions (on action control,
see section 10.6).
In Active Inference, habits can be acquired by executing goal- directed
policies and then caching information about which policies are successful
in which contexts. The cached information can be incorporated as a prior
value of policies (Friston, FitzGerald et al. 2016; Maisto, Friston, and Pezzulo
2019). This mechanism permits executing policies that have a high prior
value (in a given context) without deliberation. This can be thought of sim-
ply as observing “what I do” and learning that “I am the sort of creature that
tends to do this” over multiple exposures to a task. In contrast to model-f ree
RL, where habits are acquired ind ep end ently of goal-d irected policy sel ection,
in Active Inference habits are acquired by repeatedly pursuing goal-d irected
policies (e.g., by caching their results).
In Active Inference, goal-d irected and habitual mechanisms can cooper-
ate rather than only compete. This is b ecause the prior belief over policies
depends on both a habitual term (a prior value of policies) and a delibera-
tive term (expected f ree energy). Hierarchical elaborations of Active Inference
suggest that reactive and goal-d irected mechanisms could be arranged in a
hierarchy rather than as parallel pathways (Pezzulo, Rigoli, and Friston 2015).
Fin ally, it is worth noting that Active Inference and RL differ subtly in
how they conceive beh avi or and its c auses. RL originates from behavior-
ist theory and the idea that beh avi or results from trial-a nd-e rror learning
mediated by reinforcement. Active Inference assumes instead that beh avi or
is the result of an inference. This leads us to the next point.
10.7.3 Planning as Inference
In the same way that it is poss ib le to cast perceptual probl ems as prob-
lems of inference, it is also poss ib le to cast control probl ems in terms of
(approximate) Bayesian inference (Todorov 2008). In keeping with this, in
Active Inference, planning is seen as an inferential proc ess: the inference of
a sequence of control states of the generative model.
This idea is closely related to other approaches, which include control- as-
inference (Rawlik et al. 2013, Levine 2018), planning- as- inference (Attias 2003,
Botvinick and Toussaint 2012), and risk- sensitive and KL control (Kappen et al.
2012). In t hese approaches, planning proceeds through inferring a posterior
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 211
distribution over actions, or sequences of actions, using a dynamic genera-
tive model that encodes probabilistic contingencies between states, actions,
and f uture (expected) states. The best action or plan can be inferred by
conditioning the generative model on observing f uture rewards (Pezzulo
and Rigoli 2011, Solway and Botvinick 2012) or optimal future trajectories
(Levine 2018). For example, it is poss ib le to clamp (i.e., fix the value of ) the
future desired state in the model and then infer the sequence of actions that
is more likely to fill the gap from the current state to the f uture desired state.
Active Inference, planning-a s-i nference, and other related schemes use
a prospective form of control, which starts from an explicit repres ent at ion
of f uture, to-b e-o bserved states rather than from a set of stimulus-r esponse
rules or policies, as is more typically done in optimal control theory and
RL. However, the specific implementations of control- and planning-a s-
inference vary along at least three dimensions— namely, what form of infer-
ence they use (e.g., sampling or variational inference), what they infer (e.g.,
a posterior distribution over actions or action sequences), and the goal of
inference (e.g., maximizing the marginal likelihood of an optimality condi-
tion or the probability of getting reward).
Active Inference takes a unique perspective on each of these dimensions.
First, it uses a scalable approximate scheme—v ariational inference—to solve
the challenging computational probl ems that arise during planning-a s-
inference. Second, it affords model-b ased planning, or the inference of a
posterior over control states—w hich correspond to action sequences or
policies, not single actions.2 Third, to infer action sequences, Active Infer-
ence considers the expected f ree energy functional, which mathematically
subsumes other widely used planning-a s-i nference schemes (e.g., KL con-
trol) and can h andle ambiguous situations (Friston, Rigoli et al. 2015).
10.8 Beh avi or and Bounded Rationality
The wise are instructed by reason, average minds by experience, the stupid by
necessity and the brute by instinct.
— Marcus Tullius Cicero
Beh avi or in Active Inference automatically combines multiple components:
deliberative, perseverative, and habitual (Parr 2020). Imagine a person who
is walking to a shop close to her h ouse. If she predicts the consequences
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
212 Chapter 10
of her actions (e.g., turning left or right), she can elaborate a good plan to
reach the shop. This deliberative aspect of beh avi or is provided by expected
free energy, which is minimized when one acts in a way to achieve preferred
observations (e.g., being in the shop). Note that expected f ree energy also
includes a drive to reduce uncertainty, which can manifest in deliberation.
For example, if the person is unsure about the best direction, she can move
to an appropriate vantage point, from which she can find the way to the
shop easily, even if this implies a longer route. In short, her plans acquire
epistemic affordance.
If the person is less able to engage in deliberation (e.g., b ecause she is dis-
tracted), she may continue walking a fter reaching the shop. This perseverative
aspect of beh avi or is provided by variational f ree energy, which is minimized
when one gathers observations that are compatible with current beliefs,
including beliefs about the current course of actions. The sensory and pro-
prioceptive observations that the person gathers provide evidence for “walk-
ing” and hence may determine perseveration in the absence of deliberation.
Fi nally, another thing the person could do— when she is less able to
deliberate—is select the usual plan to go home, without thinking about
it. This habitual component is provided by the prior value of policies.
This could allocate high probability to a plan to go home—a plan she has
observed herself enacting multiple times in the past—a nd can become
dominant if not superseded by deliberation.
Note that deliberative, perseverative, and habitual aspects of beh avi or
coexist and can be combined in Active Inference. In other words, one can
infer that, in this situation, a habit is the most likely course of action. This
is diff ere nt from “dual theories,” which assume that we are driven by two
separate systems, one rational and one intuitive (Kahneman 2017). The
mixture of deliberative, perseverative, and habitual aspects of beh avi or
plausibly depends on contextual conditions, such as the amount of experi-
ence and the amount of cognitive resources one can invest in deliberative
proc esses that may have a high complexity cost.3
The impact of cognitive resources on decision-m aking has been widely
studied under the rubric of bounded rationality (Simon 1990). The core idea
is that while an ideal rational agent should always fully consider the out-
comes of its actions, a bounded rational agent has to balance the costs, effort,
and timeliness of computation—f or example, the information-p rocessing
costs of deliberating the best plan (Todorov 2009, Gershman et al. 2015).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 213
10.8.1 F ree Energy Theory of Bounded Rationality
Bounded rationality has been cast in terms of Helmholtz f ree energy minimi-
zation: a thermodynamic construct that is strictly related to the notion of
variational f ree energy as used in Active Inference; see Gottwald and Braun
(2020) for details. The “f ree energy theory of bounded rationality” formu-
lates the trade-o ffs of action sel ection with l imited information-p rocessing
capabilities in terms of two components of f ree energy: energy and entropy
(see chapter 2). The former represents the expected value of a choice (an
accuracy term), and the latter represents the costs of deliberation (a complex-
ity term). What is costly during deliberation is decreasing the entropy (or
complexity) of one’s beliefs before a choice to render them more precise
(Ortega and Braun 2013, Zénon et al. 2019). Intuitively, the choice would
be more accurate (and potentially entail higher utility) with a more pre-
cise posterior belief, but b ecause increasing the precision of beliefs has a
cost, a bounded decision-m aker has to find a compromise—by minimizing
free energy. The same trade-o ffs emerge in Active Inference, thus produc-
ing forms of bounded rationality. The notion of bounded rationality also
resonates with the use of a variational bound on evidence (or marginal
likelihood) that is a definitive aspect of Active Inference. In sum, Active
Inference provides a model of (bounded) rationality and optimality, where
the best solution to a given probl em results from the compromise between
complementary objectives: accuracy and complexity. T hese objectives stem
from a normative (f ree energy minimization) imperative that is richer than
classical objectives (e.g., utility maximization) usually considered in eco-
nomic theory.
10.9 Valence, Emotion, and Motivation
Consider your origins: you were not made to live as brutes, but to follow virtue
and knowledge.
— Dante Alighieri
Active Inference focuses on (negative) free energy as a meas ure of fitness
and the capacity of an organism to realize its goals. While Active Infer-
ence proposes that creatures act to minimize their f ree energy, this does
not mean that they ever have to compute it. Generally, it is sufficient to
deal with the gradients of the f ree energy. By analogy, we do not need to
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
214 Chapter 10
know our altitude to find the top of a hill but can simply follow the slope
upward. However, some have suggested creatures may model how their f ree
energy changes over time. Proponents of this hypothesis suggest that it
might permit characterizations of phenomena like valence, emotion, and
motivation.
On this view, it has been proposed that emotional valence, or the positive
or negative character of emotions, can be conceived as the rate of change
(first time-d erivative) of f ree energy over time ( Joffily and Coricelli 2013).
Specifically, when a creature experiences an increase in its f ree energy over
time, it may assign a negative valence to the situation; whereas when it
experiences a decrease of its f ree energy over time, it may assign it a positive
valence. Extending this line of thought to long-t erm dynamics of free energy
(and second time-d erivatives), it may be poss ib le to characterize sophisti-
cated emotional states; for example, the relief of passing from a phase of low
valence to a phase of high valence, or the disappointment of passing from
a phase of high valence to a phase of low valence. Monitoring f ree energy
dynamics (and the emotional states they elicit) may permit adapting the
behavioral strategies or learning rates to long-t erm environmental statistics.
It may seem a bit of a leap to assume a second generative model whose
role is to monitor the f ree energy of the first. However, t here is another
way in which t hese ideas can be interpreted. An int ere sti ng formalization
of t hese perspectives rests on thinking about what c auses rapid changes
in free energy. As it is a functional of beliefs, a rapid change in free energy
must be due to fast belief updating. The key determinant of this speed is
precision, which acts as a time-c onstant in the dynamics of predictive cod-
ing. Interestingly, this ties in with the notion of higher derivatives of the
free energy, as precision is the negative of the second derivative (i.e., the
curvature of a f ree energy landscape). However, this begs the question as to
why we should associate precision with valence. The answer comes from
noticing that precision is inversely related to ambiguity. The more precise
something is, the less ambiguous its interpretation. Choosing a course of
action that minimizes expected f ree energy also means minimizing ambi-
guity and therefore maximizing precision. Here we see a direct association
between high order derivatives of the f ree energy, its rate of change, and
motivated be hav ior.
Expectations about (increases or decreases of ) f ree energy may play moti-
vational roles and incentivize beh avi or, too. In Active Inference, a surrogate
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 215
expectation about changes (increases or decreases) of free energy is the pre-
cision of beliefs about policies. This again highlights the importance of this
second order statistic. For example, a highly precise belief signals that one
has found a good policy—t hat is, a policy that can be confidently expected
to minimize f ree energy. Interestingly, the precision of (beliefs about) poli-
cies has been linked to dopamine signaling (FitzGerald, Dolan, and Friston
2015). From this perspective, stimuli that increase the precision of beliefs
about policies trigger dopamine bursts—w hich may indicate their incen-
tive salience (Berridge 2007). This perspective may help shed light on the
neurophysiological mechanisms linking expectations of goal or reward
achievement to increases in attention (Anderson et al. 2011) and motiva-
tion (Berridge and Kringelbach 2011).
10.10 Homeostasis, Allostasis, and Interoceptive Proc essing
There is more wisdom in your body than in your deepest philosophy.
— Friedrich Nietz sche
A creature’s generative model is not just about the external world but also—
and perhaps even more importantly—a bout the internal milieu. A genera-
tive model of a body’s inside (or interoceptive schema) has a dual role: to
explain how interoceptive (bodily) sensations are generated and to ensure
the correct regulation of physiological para meters (Iodice et al. 2019), like
body temperature or sugar levels in the blood. Cybernetic theories (touched
on in section 10.6.2) assume that a central objective of living organisms is
maintaining homeostasis (Cannon 1929)—e nsuring that physiological par-
ameters remain within v iable ranges (e.g., body temperature never becomes
too high)—a nd that homeostasis can only be achieved by exerting a suc-
cessful control over the environment (Ashby 1952).
This form of homeostatic regulation can be achieved in Active Infer-
ence by specifying the v iable ranges of physiological para meters as priors
over interoceptive observations. Interestingly, homeostatic regulation can
be achieved in multiple, nested ways. The simplest regulatory loop is the
engagement of autonomic reflexes (e.g., vasodilation), when certain par-
ameters are (expected to be) out of range—f or example, when body temper-
ature is too high. This autonomic control can be constructed as interoceptive
inference: an Active Inference proc ess that operates on interoceptive streams
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
216 Chapter 10
rather than proprioceptive streams, as in the case of externally directed
actions (Seth et al. 2012, Seth and Friston 2016, Allen et al. 2019). For this,
the brain may use a generative model that predicts interoceptive and physi-
ological streams and triggers autonomic reflexes to correct interoceptive
prediction errors (e.g., a surprisingly high body temperature). This is analo-
gous to the way motor reflexes are activated to correct proprioceptive pre-
diction errors and steer externally directed actions.
Active Inference extends beyond s imple autonomic loops: it can correct
the same interoceptive prediction error (high body temperature) in increas-
ingly sophisticated ways (Pezzulo, Rigoli, and Friston 2015). It can use
predictive, allostatic strategies (Sterling 2012, Barrett and Simmons 2015,
Corcoran et al. 2020) that go beyond homeostasis and preemptively control
physiology in an allostatic fashion before interoceptive prediction errors
are triggered— for example, finding shade before overheating. Another
predictive strategy entails mobilizing resources before expected excursions
from physiological setpoints—f or example, increasing cardiac output before
a long run in anticipation of increased oxygen demands. That requires
modifying the priors over interoceptive observations dynamically, g oing
beyond homeostasis (Tschantz et al. 2021). Eventually, predictive brains
can develop sophisticated goal-d irected strategies, such as ensuring that
one brings cold w ater to the beach, meeting the same imperative (control-
ling body temperature) in richer and more effective ways.
Biological and interoceptive regulation may be crucial for affect and emo-
tional proc essing (Barrett 2017). During situated interactions, the brain’s
generative model constantly predicts not just what w ill happen next but
also what the consequences for interoception and allostasis are. Interocep-
tive streams—e licited during the perception of external objects and events—
imbue them with an affective dimension, which signals how good or bad they
are for the creature’s allostasis and survival, hence making them “meaning-
ful.” If this view is correct, then disorders of this interoceptive and allostatic
proc essing may engender emotional dysregulation and vario us psychopatho-
logical conditions (Pezzulo 2013; Barrett et al. 2016; Maisto, Barca et al. 2019;
Pezzulo, Maisto et al. 2019).
There is an emerging bedfellow for interoceptive inference— namely, emo-
tional inference. In this application of Active Inference, emotions are con-
sidered part of the generative model: they are just another construct or
hypothesis that the brain employs to deploy precision in deep generative
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 217
models. From the perspective of belief updating, this means anxiety is just
a commitment to the Bayesian belief “I am anxious” that best explains the
prevailing sensory and interoceptive queues. From the perspective of acting,
the ensuing (interoceptive) predictions augment or attenuate vario us preci-
sions (i.e., covert action) or enslave autonomic responses (i.e., overt action).
This may look much like arousal, which confirm the hypothesis that “I am
anxious.” Usually, emotional inference entails belief updating that is domain
general, assimilating information from both interoceptive and exteroceptive
sensory streams—h ence the intimate relationship between emotion, intero-
ception, and attention in health (Seth and Friston 2016; Smith, Lane et al.
2019; Smith, Parr, and Friston 2019) and disease (Peters et al. 2017, J. E.
Clark et al. 2018).
10.11 Attention, Salience, and Epistemic Dynamics
True ignorance is not the absence of knowledge, but the refusal to acquire it.
— Karl Popper
Given the number of times we have referred to precision and expected free
energy in this chapter alone, it would be negligent not to devote a l ittle space
to attention and salience. T hese concepts recur throughout psyc holo gy, hav-
ing been subject to numerous redefinitions and classifications. Sometimes
these terms are used to refer to synaptic gain control mechanisms (Hillyard
et al. 1998), which preferentially select some sensory modality or subset of
channels within a modality. Sometimes they refer to how we orient our-
selves, through overt or covert action, to gain more information about the
world (Rizzolatti et al. 1987; Sheliga et al. 1994, 1995).
Although the uncertainty afforded by the many meanings of attention
underwrites some of the epistemic attractiveness of this field of study, t here
is also value in resolving the attendant ambiguity. One of the t hings offered
by a formal perspective on psyc holo gy is that we do not need to worry about
this ambiguity. We can operationally define attention as the precision asso-
ciated with some sensory input. This neatly maps to the concept of gain
control, as sensations we infer to be more precise w ill have greater influ-
ence over belief updating than those inferred to be imprecise. The construct
validity of this association has been demonstrated in relation to psychologi-
cal paradigms, including the famous Posner paradigm (Feldman and Friston
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
218 Chapter 10
2010). Specifically, responding to a stimulus at a location in visual space that
is afforded a higher precision is faster than responding to stimuli in other
locations.
This leaves the term salience in want of a similar formal definition. Typi-
cally, in Active Inference, we associate salience with expected information
gain (or epistemic value): a component of the expected f ree energy. Intui-
tively, something is more salient when we expect it to yield more informa-
tion. However, this defines salience in terms of an action or policy, while
attention is an attribute of beliefs about sensory input. This fits with the
notion of salience as overt or covert orienting. We saw in chapter 7 that we
could further subdivide expected information gain into salience and novelty.
The former is the potential to infer, while the latter is the potential to learn.
An analogy that expresses the difference between attention and salience (or
novelty) is the design and analys is of a scientific experiment. Attention is
the proc ess of selecting the highest quality data from what we have already
meas ured and using these to inform our hypothesis testing. Salience is the
design of the next experiment to ensure the highest quality data.
We do not dwell on this issue to simply add another reclassification
of attentional phenomena to the lit era t ure but to highlight an impor tant
advantage in committing to a formal psy cholo gy. U nder Active Inference,
it does not matter if others define attention (or any other construct) differ-
ently—as we can simply refer to the mathematical constructs in question
and preclude any confusion. A final point of consideration is that these
definitions offer a s imple explanation for why attention and salience are so
often conflated. Highly precise data are minimally ambiguous. This means
that they should be afforded attention and that actions to acquire t hese
data are highly salient (Parr and Friston 2019a).
10.12 Rule Learning, Causal Inference, and Fast Generalization
Yesterday I was clever, so I wanted to change the world. T oday I am wise, so I am
changing myself.
— Rumi
Humans and other animals excel at making sophisticated causal inferences,
learning abstract concepts and the causal relationships between objects,
and generalizing from l imited experience—in contrast to current machine
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 219
learning paradigms, which require a large number of examples to attain
similar perf orm ance. This difference suggests that current machine learning
approaches, which are largely based on sophisticated pattern recognition,
may not fully capture the ways h umans learn and think (Lake et al. 2017).
The learning paradigm of Active Inference is based on the development
of generative models that capture the causal relations between actions,
events, and observations. In this book, we have considered relatively s imple
tasks (e.g., the T-m aze example of chapter 7) that require unsophisticated
generative models. In contrast, understanding and reasoning about com-
plex situations require deep generative models that capture the latent
structure of the environment—s uch as hidden regularities that permit gen-
eralizing across a number of apparently dissimilar situations (Tervo et al.
2016; Friston, Lin et al. 2017).
One s imple example of a hidden rule that governs sophisticated social
interactions is a traffic intersection. Imagine a naïve person who observes
a busy crossroad and has to predict (or explain) on which occasions pedes-
trians or cars cross the road. The person can accumulate statistics about the
co-o ccurrence of events (e.g., a red car stopping and a tall man crossing; an
old w oman stopping and a big car passing), but most are ultimately use-
less. The person can eventually discover some recurrent statistical patterns,
such as that pedestrians cross the road soon a fter all cars stop at a certain
point on the road. This determination would be deemed sufficient in a
machine learning setting if the task w ere just to predict when pedestrians
are about to walk, but it would not entail any understanding of the situa-
tion. In fact, it may even lead to the erroneous conclusion that the stopping
of cars explains the movement of pedestrians. This sort of error is typical in
machine learning applications that do not appeal to (causal) models—a nd
cannot distinguish w hether the rain explains the wet grass or the wet grass
explains the rain (Pearl and Mackenzie 2018).
On the other hand, inferring the correct hidden (e.g., traffic light) rule
provides a deeper understanding of the causal structure of the situation
(e.g., it is the traffic light that c auses the cars to stop and the pedestrians
to walk). The hidden rule not only affords better predictive power but also
renders inference more parsimonious, as it can abstract away from most
sensory details (e.g., the color of cars). In turn, this permits generalizing to
other situations, such as diff ere nt crossroads or cities, where most sensory
details differ significantly—w ith the caveat that facing crossroads in some
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
220 Chapter 10
cities, like Rome, may require more than looking at traffic lights. Fin ally,
learning about traffic light rules may also enable more efficient learning in
novel situations—or to develop what is called a “learning set” in psyc hol-
ogy or a learning- to- learn ability in machine learning (Harlow 1949). When
facing a crossroad where the traffic light is off, one cannot use the learned
rule but may nevertheless have the expectation that there is another, simi-
lar hidden rule in play—a nd this could help understanding what the traffic
police officer is d oing.
As this s imple example illustrates, learning rich generative models—of
the latent structure of the environment (aka structure learning)—a ffords
sophisticated forms of causal reasoning and generalization. Scaling up
generative models to address t hese sophisticated situations is an ongoing
objective in computational modeling and cognitive science (Tenenbaum
et al. 2006, Kemp and Tenenbaum 2008). Interestingly, t here is a tension
between current machine learning trends—w herein the general idea is “the
bigger, the better”—a nd the statistical approach of Active Inference—w hich
suggests the importance of balancing the accuracy of a model with its com-
plexity and to f avor simpler models. Model reduction (and the pruning of
unnecessary para meters) is not simply a way to avoid wasting resources—it
is also an effective way to learn hidden rules, including during offline peri-
ods like sleep (Friston, Lin et al. 2017), perhaps manifesting in resting state
activity (Pezzulo, Zorzi, and Corbetta 2020).
10.13 Active Inference and Other Fields: Open Directions
It has to start somewhere, it has to start sometime,
what better place than here? What better time than now?
— Rage Against the Machine, “Guerrilla Radio”
In this book, we mainly focus on Active Inference models that address biolog-
ical probl ems of survival and adaptation. Yet Active Inference can be applied
in many other domains. In this last section, we briefly discuss two such
domains: social and cultural dynamics and machine learning and robotics.
Addressing the former requires thinking about the ways in which multiple
Active Inference agents interact and the emergent effects of such interaction.
Addressing the latter requires understanding how Active Inference can be
endowed with more effective learning (and inference) mechanisms to scale
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 221
up to more complex probl ems—b ut in a way that is compatible with the basic
assumptions of the theory. Both are int ere sti ng open directions for research.
10.13.1 Social and Cultural Dynamics
Many int ere sti ng aspects of our (h uman) cognition relate to social and
cultural dynamics rather than individualistic perceptions, decisions, and
actions (Veissière et al. 2020). By definition, social dynamics require mul-
tiple Active Inference creatures that engage in physical interactions (e.g.,
joint actions, such as playing team sports) or more abstract interactions
(e.g., elections or social networking). S imple demonstrations of inter-A ctive
Inference between identical organisms already produced int ere sti ng emer-
gent phenomena, such as the self-o rganization of s imple life forms that
resist dispersion, the possibility to engage in morphogen et ic proc esses to
acquire and restore a body form, and mutual coordinated prediction and
turn taking (Friston 2013; Friston and Frith 2015a; Friston, Levin et al.
2015). Other simulations have addressed the ways in which creatures can
extend their cognition to material artifacts and shape their cognitive niches
(Bruineberg et al. 2018).
These simulations capture only a fraction of the complexity of our social
and cultural dynamics, but they illustrate the potential of Active Inference
to expand from a science of individuals to a science of societ ies—a nd how
cognition extends beyond our skulls (Nave et al. 2020).
10.13.2 Machine Learning and Robotics
The generative modeling and variational inference methods discussed in
this book are widely used in machine learning and robotics. In these fields,
the emphasis is often on how to learn (connectionist) generative models—
as opposed to how to use them for Active Inference, the focus of this book.
This is int ere sti ng as machine learning approaches are potentially useful to
scale up the complexity of the generative models and of the probl ems con-
sidered in this book—w ith the caveat that they may call on very diff ere nt
proc ess theories of Active Inference.
While it is impossible to review h ere the vast lite ra t ure on generative
modeling in machine learning, we briefly mention some of the most popu-
lar models, from which many variants have been developed. Two early
connectionist generative models, the Helmholtz machine and the Boltzmann
machine (Ackley et al. 1985, Dayan et al. 1995), provided paradigmatic
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
222 Chapter 10
examples of how to learn the internal repres ent at ions of a neural network
in an unsupervised way. The Helmholtz machine is especially related to the
variational approach of Active Inference, as it uses separate recognition and
generative networks to infer a distribution over hidden variables and sample
from them to obtain fictive data. The early practical success of these meth-
ods was l imited. But afterward, the possibility to stack multiple (restricted)
Boltzmann machines enabled learning of multiple layers of internal repre-
sent at ions and was one of the early successes of unsupervised deep neural
networks (Hinton 2007).
Two recent examples of connectionist generative models, variational auto-
encoders or VAEs (Kingma and Welling 2014) and generative adversarial net-
works or GANs (Goodfellow et al. 2014), are widely used in machine learning
applications, such as recognizing or generating pictures and videos. VAEs
exemplify an elegant application of variational methods to learning in gen-
erative networks. Their learning objective, the evidence lower bound (ELBO),
is mathematically equivalent to variational free energy. This objective enables
learning of an accurate description of the data (i.e., maximizes accuracy) but
also f avors internal repres ent at ions that do not differ too much from their
priors (i.e., minimizes complexity). The latter objective acts as a so-c alled
regularizer, which helps to generalize and avoid overfitting.
GANs follow a diff ere nt approach: they combine two networks, a gen-
erative network and a discriminative network, which continuously compete
during learning. The discriminative network learns to distinguish which
example data produced by the generative network are real or fictive. The
generative network tries to generate fictive data that fool (i.e., are misclassi-
fied by) the discriminative network. The race between t hese two networks
forces the generative network to improve its generative capabilities and
produce high fidelity fictive data—an ability that has been widely exploited
to generate, for example, realistic images.
The above generative models (and o thers) can be used for control tasks.
For example, Ha and Eck (2017) have used a (sequence-t o-s equence) VAE
to learn to predict pencil strokes. By sampling from the internal repres en-
tat ion of the VAE, the model can construct novel stroke-b ased drawings.
Generative modeling approaches have been used to control robot move-
ments, too. Some of these approaches use Active Inference (Pio-L opez et al.
2016, Sancaktar et al. 2020, Ciria et al. 2021) or closely related ideas, but
in a connectionist setting (Ahmadi and Tani 2019, Tani and White 2020).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Active Inference as a Unified Theory of Sentient Be hav ior 223
One of the main challenges in this domain is that robot movements
are high dimensional and require (learning) sophisticated generative mod-
els. One int ere sti ng aspect of Active Inference and related approaches is
that the most import ant t hing to be learned is a forward mapping between
actions and sensory (e.g., visual and proprioceptive) feedback at the next
time step. This forward mapping can be learned in vario us ways: by autono-
mous exploration, by demonstration, or even by direct interaction with a
human—f or example, a teacher (the experimenter) who guides the hands of
the robot along a trajectory to the goal, hence scaffolding the acquisition of
effective goal-d irected actions (Yamashita and Tani 2008). The possibility to
learn generative models in vario us ways greatly expands the scope of robot
skills that can be eventually achieved. In turn, the possibility to develop
more advanced (neuro-) robots using Active Inference could be import ant
not just for technological but also for theoretical reasons. Indeed, some
key aspects of Active Inference, such as the adaptive agent-e nvironment
interactions, the integration of cognitive functions, and the importance of
embodiment, are naturally addressed in robotic settings.
10.14 Summary
Home is behind, the world ahead,
and there are many paths to tread
through shadows to the edge of night,
until the stars are all alight.
— J. R. R. Tolkien, The Lord of the Rings
We started this book by asking w hether it is poss ib le to understand brain
and beh avi or from first princip les. We then introduced Active Inference as
a candidate theory to meet this challenge. We hope that the reader has been
convinced that the answer to our original question is yes. In this chapter, we
considered the unified perspective that Active Inference offers on sentient
beh avi or and what implications this theory has for familiar psychological
constructs, such as perception, action sel ection, and emotion. This gave us
the opportunity to revisit the concepts introduced throughout the book
and to remind ourselves of the fascinating questions still open for f uture
research. We hope this book provides a useful complement to related works
on Active Inference, including on the one hand the philosophy (Hohwy
2013, Clark 2015) and on the other hand the physics (Friston 2019a).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
224 Chapter 10
We are now at the end of our journey. Our aim has been to offer an
introduction to t hose interested in using t hese methods—b oth at concep-
tual and formal levels. However, it is import ant to emphasize that Active
Inference is not something that can be learned purely in theory. We encour-
age anyone who has enjoyed this book to think about pursuing it in prac-
tice. Import ant rites of passage in theoretical neurobiology are trying to
write down a generative model, experiencing the frustration when simula-
tions misbehave, and learning from violations of your prior beliefs when
something unexpected happens. W hether or not you choose to pursue this
practice at a computational level, we hope that you will reflect on it as
you engage in Active Inference in day- to-d ay life. This may manifest in the
compulsion to direct your eyes to resolve uncertainty about something in
your peripheral vision. It may be in choosing to eat at a favorite restaurant
to fulfill prior (gustatory) preferences. It may be in reducing the heat when
the shower is too hot to ensure the temperature conforms to your model of
how the world should be. Ultimately, we are confident that you w ill con-
tinue to pursue Active Inference in some form.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:ThomasParr,GiovanniPezzulo,KarlJ.Friston
DOI:10.7551/mitpress/12441.001.0001
ISBN(electronic):9780262369978
Publisher:TheMITPress
Published:2022
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025
MIT Press Direct
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided
comments on drafts of this book. The generous work of academic experts is essential
for establishing the authority and quality of our publications. We acknowledge with
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services.
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J.
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior /
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. |
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246589/c008300_9780262369978.pdf by guest on 12 December 2025