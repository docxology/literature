Think How Your Teammates Think: Active Inference Can Benefit
Decentralized Execution
Hao Wu1, 2*, Shoucheng Song1, 2*, Chang Yao1, 2, Sheng Han1, 2,
Huaiyu Wan1, 2, Youfang Lin1, 2, Kai Lv1, 2â€ 
1School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China
2Beijing Key Laboratory of Traffic Data Mining and Embodied Intelligence, Beijing, China
{wuhao , insis songsc, yaochang, shhan, hywan, yflin, lvkai}@bjtu.edu.cn
Abstract
In multi-agent systems, explicit cognition of teammatesâ€™ de-
cision logic serves as a critical factor in facilitating coordi-
nation. Communication (i.e., â€œTellâ€) can assist in the cog-
nitive development process by information dissemination,
yet it is inevitably subject to real-world constraints such
as noise, latency, and attacks. Therefore, building the un-
derstanding of teammatesâ€™ decisions without communication
remains challenging. To address this, we propose a novel
non-communication MARL framework that realizes the con-
struction of cognition through local observation-based mod-
eling (i.e.,â€œThinkâ€). Our framework enables agents to model
teammatesâ€™active inferenceprocess. At first, the proposed
method produces three teammate portraits: perception-belief-
action. Specifically, we model the teammateâ€™s decision pro-
cess as follows: 1) Perception: observing environments; 2)
Belief: forming beliefs; 3) Action: making decisions. Then,
we selectively integrate the belief portrait into the decision
process based on the accuracy and relevance of the percep-
tion portrait. This enables the selection of cooperative team-
mates and facilitates effective collaboration. Extensive exper-
iments on the SMAC, SMACv2, MPE, and GRF benchmarks
demonstrate the superior performance of our method.
Introduction
Multi-agent reinforcement learning (MARL) has garnered
significant attention due to its wide applications in fields
such as autonomous driving (Kiran et al. 2021), smart grids
(Roesch et al. 2020), and transportation (Lee, Chung, and
Sohn 2019). In decentralized systems, the lack of cognition
regarding teammatesâ€™ decision logic may induce miscoordi-
nation among agents and result in suboptimal policies.
To address the mentioned issue, one intuitive approach
is â€œTell agent how its teammates actâ€, which can be im-
plemented through communication mechanisms. The com-
munication (i.e., â€œTellâ€) methods can facilitate the under-
standing of teammatesâ€™ behaviors by exchanging decision-
relevant messages (Wang et al. 2019; Yuan et al. 2022; Sun
et al. 2023a, 2024). However, communication effectiveness
may be limited under some conditions, including limited
*These authors contributed equally.
â€ Corresponding Author: Kai Lv (lvkai@bjtu.edu.cn).
Copyright Â© 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Perception
 Action
Belief
How Teammate 
Thinks
Agent i
Figure 1: Modeling the Active Inference Process of the
Teammate. In this scenario, agentimodels the perception-
belief-action involved in its teammateâ€™s active inference pro-
cess when facing the goalkeeper. This allowsito obtain its
teammateâ€™s decision-relevant information and achieve effec-
tive collaboration.
bandwidth, high latency, and significant noise (Tung et al.
2021; Hu et al. 2023; Song et al. 2025). Therefore, we ex-
plore the method for developing the cognition of teammatesâ€™
decisions in the absence of communication.
In contrast to directly telling an agent how its team-
mates act, we advocate that agents could engage in â€œThink
how your teammates thinkâ€. Specifically, the agent actively
builds comprehension of teammatesâ€™ decisions. To achieve
this, the direct method is to model teammatesâ€™ decision pro-
cess. However, existing agent modeling methods fail to ful-
fill this. On the one hand, some methods rely on access to
other agentsâ€™ trajectories during the modeling (Rabinowitz
et al. 2018; Zintgraf et al. 2021), which is unavailable dur-
ing decentralized execution. On the other hand, some meth-
ods typically only enable a single agent to model other
agents that possess fixed parameters (Xie et al. 2021; Pa-
poudakis, Christianos, and Albrecht 2021; Yu, Jiang, and Lu
2024). This configuration imposes an upper bound on the
systemâ€™s collaborative efficiency, preventing the team from
learning more optimal policies. Furthermore, these methods
only model incomplete decision components (e.g., behav-
iors or intentions), risking inaccuracies from discrepancies
arXiv:2511.18761v1  [cs.MA]  24 Nov 2025
between the model and the actual situation. To address this,
we rely solely on local observations to model teammatesâ€™
complete decision processes, reducing modeling inaccura-
cies during decentralized execution.
Inspired by human brain decision-making mechanisms
and active inference theory (Friston et al. 2016), we model
the teammateâ€™s decision process as an active inference pro-
cess comprising perception-belief-action. In this process,
the teammate perceives environments, forms beliefs, and
then takes actions by integrating perceptions and beliefs.
Consequently, we employ local observation-based model-
ing (i.e.,â€œThinkâ€) to acquire teammatesâ€™ perception-belief-
action (i.e.,â€œHow your teammates thinkâ€). Figure 1 pro-
vides an illustrative example of this modeling process.
In this paper, we propose a novel non-communication
framework for modeling theActiveInference of teammates
inMARL (AIM). The framework consists of two parts: At
first, we develop a modeling method to model teammatesâ€™
three portraits: perception-belief-action, solely based on lo-
cal observations. Meanwhile, the perception and belief por-
traits are optimized by minimizing the discrepancy between
predicted and actual actions. Then, we propose a dual-filter
mechanism to enhance teammatesâ€™ cognition utilization.
This mechanism features selective collaboration by choos-
ing teammates whose modeled portraits have high accu-
racy. Additionally, by considering the perception relevance
among agents, we adopt an attention module to dynamically
integrate teammatesâ€™ belief portraits, thereby optimizing the
decision process. Our proposed method demonstrates signif-
icant improvement in tasks within SMAC (Samvelyan et al.
2019), SMACv2 (Ellis et al. 2024), MPE (Mordatch and
Abbeel 2018) and GRF (Kurach et al. 2020).
Our contributions are outlined as follows:
â€¢ We replace â€œcommunication (i.e.,Tell)â€ with â€œmodel-
ing (i.e.,Think)â€, enabling agents to construct the cog-
nition of teammatesâ€™ decision logic without communica-
tion during decentralized execution.
â€¢ We propose an active inference framework to model
teammatesâ€™ three portraits: perception-belief-action, to
understand how they think.
â€¢ We introduce a dual filter that leverages the accuracy
and relevance of perception portraits to select coopera-
tive teammates.
â€¢ We conduct experiments on SMAC, SMACv2, MPE, and
GRF. The results show that our method achieves optimal
or near-optimal performance in most scenarios.
Related Works
Communication in MARL
Several communication methods, such as (Das et al. 2019;
Ding, Huang, and Lu 2020; Yuan et al. 2022; Sun et al.
2023b; Sun 2024; Li et al. 2025; Yao et al. 2025), design
communication networks that enable agents to exchange
decision-relevant messages during the decentralized exe-
cution. However, in some real-world scenarios, limitations
such as high noise, high latency, and low bandwidth often
prevent these communication algorithms from performing
well (Tung et al. 2021; Hu et al. 2023; Song et al. 2025).
Furthermore, the communication attack may introduce ma-
licious information, disrupting agentsâ€™ decision-making and
hindering collaboration (Xue et al. 2021; Zhu, Dastani, and
Wang 2024). In comparison, our method adopts an alter-
native perspective. We propose a novel communication-free
framework that models teammatesâ€™ active inference to com-
prehensively understand their decision logic.
Agent Modeling Methods
Methods based on agent modeling typically acquire agentsâ€™
behaviors, beliefs, or intentions. For example, (Rabinowitz
et al. 2018; Yang et al. 2018; Tian et al. 2019; Zintgraf et al.
2021; Zhai et al. 2023) model agentsâ€™ psychological state
and beliefs using the Theory of Mind and Bayesian rea-
soning. (He et al. 2016; Raileanu et al. 2018) infer agentsâ€™
policy based on the modeled agentsâ€™ observations and ac-
tions. (Papoudakis and Albrecht 2020) model agentsâ€™ policy
as a latent distribution. However, these methods rely on a
strong assumption that agents can directly access the mod-
eled agentâ€™ trajectories, which is infeasible during the de-
centralized execution.
Although (Papoudakis, Christianos, and Albrecht 2021;
Xie et al. 2021; Yu, Jiang, and Lu 2024) model other agents
solely based on local observations, they maintain team-
matesâ€™ policies as fixed parameters. Nevertheless, collabo-
ration performance becomes constrained by fixed teammate
parameters, preventing team policy optimization. More-
over, the above methods only partially model other agentsâ€™
decision-making processes, introducing inevitable inaccura-
cies and negatively impacting cooperative performance ow-
ing to mismatches between the model and reality. In con-
trast, our framework models teammatesâ€™ complete decision-
making process through three key aspects: perception-
belief-action to increase modeling accuracy. In this way, our
method enables agents to understand how they think.
Background
A fully cooperative multi-agent task can be modeled as a
Decentralized Partially Observable Markov Decision Pro-
cess(Dec-POMDP) (Oliehoek, Amato et al. 2016), repre-
sented as a tupleâŸ¨I, S,{A i}N
i=1,{â„¦ i}N
i=1, O,T, R, Î³âŸ©. In
this model,I={1, . . . , N}is the set of agents andNis the
number of agents.Sis the state space. For each agentiâˆˆI,
Ai is the individual action space, andA=A 1 Ã— Â·Â·Â· Ã—AN
is the joint action space.â„¦ i is the observation space for
agenti.O(o i |s, i)is the observation function over local
observationso i âˆˆâ„¦ i given statesâˆˆSand agenti. The
state transition functionT(s â€² |s,a)defines the probability
of transitioning to states â€² âˆˆSgiven current statesâˆˆS
and joint actionaâˆˆA. The reward functionR(s,a)gives
the immediate reward, andÎ³âˆˆ[0,1)is the discount fac-
tor, used to balance long-term and short-term rewards. At
each time stept, agentiâˆˆIreceives a local observation
oi âˆ¼O(Â· |s, i), selects an actionai âˆˆA i, and the joint ac-
tiona=âŸ¨a 1, . . . , aN âŸ©results in a state transition and a re-
wardR(s,a). The goal of multi-agent system algorithms is
to find a joint policyÏ€to maximize the expected cumulative
reward, formulated as:V Ï€(s) =E Ï€ [Pâˆ
t=0 Î³tR(st,at)].
Active 
Inference
Dual Filter
ğ‘œğ‘–
ğ‘¡
Mixing Network
ğ‘„ğ‘¡ğ‘œğ‘¡(ğ‘ ,ğ’‚)ğ“›ğ‘»ğ‘«
ğ‘„âˆ’ğ‘–
ğ‘™ğ‘œğ‘ğ‘ğ‘™
ğ‘ ğ‘¡
ğ‘œğ‘–
ğ‘¡
ğğ›ğ¬ğğ«ğ¯ğšğ­ğ¢ğ¨ğ§
ğ„ğ§ğœğ¨ğğğ«
Belief Portrait
ğ‘–ğ‘‘âˆ’ğ‘–
â„ğ‘–
ğ‘¡
ğ‘§âˆ’ğ‘–
ğ‘¡
ğ“›ğ’ğ’Š,ğ“›ğ’„ğ’
â„ğ‘–
ğ‘¡
Imagine
ğ‘–
ğ‘— ğ‘—
ğ‘–
Perception Portrait
ğğğ«ğœğğ©ğ­ğ¢ğ¨ğ§
ğ„ğ§ğœğ¨ğğğ«
à· â„âˆ’ğ‘–
ğ‘¡
à·œğ‘âˆ’ğ‘–ğ‘âˆ’ğ‘–
ğ‘¡ğ‘Ÿğ‘¢ğ‘’ Argmax
Action Portrait
ğ‘§âˆ’ğ‘–
ğ‘¡
ğ“›ğ’„ğ’†ğ‘§âˆ’ğ‘–
ğ‘¡â„ğ‘–
ğ‘¡
ğğğ¥ğ¢ğğŸ
ğ„ğ§ğœğ¨ğğğ«
â„ğ‘–
ğ‘¡
â„ğ‘–
ğ‘¡ ğ‘§ğ‘˜
ğ‘¡
Q K V
ğ›¼âˆ’ğ‘–
ğ‘’ğ‘–
ğ‘¡
ğ‘„ğ‘–
ğ‘™ğ‘œğ‘ğ‘ğ‘™(ğ‘’ğ‘–
ğ‘¡,â„ğ‘–
ğ‘¡,ğ‘ğ‘–
ğ‘¡)
Relevance Filter
Accuracy Filter
ğ‘§âˆ’ğ‘–
ğ‘¡
(c) Dual Filter(b) Active Inference
ğ“›ğ’”ğ’š,ğ“›ğ’”ğ’† ğ‘»ğ’ğ’‘_ğ’Œ
(a) Training Framework
à·œğ‘œğ‘–ğ‘—
ğ‘¡ğ‘œğ‘–
ğ‘¡ à·œğ‘œğ‘—
ğ‘¡
ğ‘„ğ‘–
ğ‘™ğ‘œğ‘ğ‘ğ‘™
ğ‘’ğ‘–
ğ‘¡
ğ‘§âˆ’ğ‘–
ğ‘¡
à· â„âˆ’ğ‘–
ğ‘¡
à· â„âˆ’ğ‘–
ğ‘¡
à· â„âˆ’ğ‘–
ğ‘¡
à· â„âˆ’ğ‘–
ğ‘¡
à· â„ğ‘˜
ğ‘¡
à· ğ‘„âˆ’ğ‘–
à·œğ‘œğ‘–ğ‘—
ğ‘¡
ğŒğ‹ğ
ğ’‡
Belief Embedding
Fusion Embedding
Gradient Backward
Concatenate Dot Product
Agent ğ‘–
Figure 2: The overall framework of AIM. (a) The training framework comprises the agent network and the mixing network; (b)
The active inference module, which includes perception portrait, belief portrait, and action portrait; (c) The dual filter module,
consisting of the accuracy filter and the relevance filter.
Method
To build the cognition of teammatesâ€™ decision logic dur-
ing decentralized execution, our core idea is to model their
complete decision-making process. On the one hand, we in-
troduce active inference to construct teammatesâ€™ three por-
traits: perception-belief-action, which represent â€œhow team-
mates thinkâ€. On the other hand, due to modeling errors and
the diversity of teammates, we devise a dual filter that dy-
namically integrates teammatesâ€™ belief portraits based on the
accuracy and relevance of their perception portraits.
Teammate Portrait via Active Inference
In active inference, the teammateâ€™s decision-making process
is divided into three stages: perception of the environment,
belief formation, and action execution. Perception serves as
the foundational step, capturing the change of entities, which
supports the following processes. Beliefs represent agentsâ€™
deeper understanding of the environment, serving as the ba-
sis for deriving action. The final decision, which is the ac-
tion output, is based on both perception and belief and can
be used as posterior information to optimize the perceptions
and beliefs. Here, we give the details of the triple model pro-
cess in AIM.
Perception PortraitTo understand teammatesâ€™ behavior,
agents should first understand what they have experienced.
AIM aims to construct the world in teammatesâ€™ eyes, gen-
erating their local observations from a teammate-centered
perspective. In detail, for agenti, the perceptionË†ot
j of team-
matejis constructed based oniâ€™s own observationso t
i. The
specific process involves setting each teammateâ€™s position as
the origin and recalculating the positions of the other agents
relative to this origin, while also adding other relevant infor-
mation. As shown in Figure 2, we only select the portion of
ot
i that intersects withË†ot
j as the perception portraitË†ot
ij. This
process is essentially a viewpoint transformation operation.
More details about the transformation can be found in Ap-
pendix B. We then useË†ot
ij as the input and apply a GRU to
obtain the historical trajectory information Ë†ht
ij of the team-
matej. Subsequently, we acquire Ë†ht
âˆ’i of all teammates.
Belief PortraitBelief serves as a higher-level abstraction
of actions, serving as the core basis for policies. However,
unlike the perception portrait, which is objective, the belief
portrait typically exhibits high subjectivity and variability.
In scenarios with limited observation, these characteristics
become more pronounced, making it challenging to model
beliefs from teammatesâ€™ perspectives accurately.
Therefore, we construct the belief portrait of teammates
from the agentâ€™s perspective. For agenti, we use its trajec-
toryh t
i and teammatesâ€™ indexidâˆ’i for modeling, as shown in
Figure 2. The input(h t
i, idâˆ’i)is fed into the belief encoder
to obtain the belief distributionN(Âµ t
i, Î´t
i). Through repa-
rameterization, we obtain the teammatesâ€™ belief portraits
zt
âˆ’i which should exhibit two characteristics: (1) Decision-
support ability; (2) Stability over the short term.
To enhance the decision-support ability, we get inspired
by (Yuan et al. 2022) and maximize the mutual information
between teammatesâ€™ actions and belief portraitsz t
âˆ’i, condi-
tioned onh t
i andid âˆ’i. Through variational inference, maxi-
mizing mutual information can be transformed into the fol-
lowing loss. The derivation can be found in Appendix A.
Lmi =E

DKL
 
p(zt
âˆ’i |h t
i, idâˆ’i)âˆ¥
qÎ¾(zt
âˆ’i |h t
i, at
âˆ’i, idâˆ’i)

, (1)
whereD KL(. . .||. . .)represents the Kullback-Leibler di-
vergence,q Î¾(zt
âˆ’i |h t
i, at
âˆ’i, idâˆ’i)is used as a variational dis-
tribution to approximate the conditional distributionp(z t
âˆ’i |
ht
i, idâˆ’i). MinimizingL mi is equivalent to maximizing the
relevance between the belief portraits and selected actions.
To improve the stability ofz t
âˆ’i, we calculate the cosine
similarity between thez t
âˆ’i at adjacent time step, formalized
as:
Lcn =E
"
âˆ’ ztâˆ’1
âˆ’i Â·z t
âˆ’i
âˆ¥ztâˆ’1
âˆ’i âˆ¥âˆ¥zt
âˆ’iâˆ¥
#
.(2)
Action PortraitAction is the most direct outcome of ac-
tive inference. The accuracy of the action serves as a critique
of the modeling process, especially the coherence of the be-
lief portrait. Moreover, joint actions induce environmental
state transitions, which in turn influence teammatesâ€™ percep-
tion portraits. Hence, we optimize the perception and belief
portrait by leveraging posterior action predictions. In AIM,
we concatenate belief portraitsz t
âˆ’i and historical perception
information Ë†ht
âˆ’i as the input of action prediction network,
and obtain the imagined action distributionË†a âˆ’i. By mini-
mizing the cross-entropy loss betweenË†aâˆ’i and the true ac-
tionsa true
âˆ’i , AIM optimizes the action modeling network and
the perception-belief portrait.
Lce =âˆ’
X
i
atrue
âˆ’i log Ë†aâˆ’i.(3)
The complete loss for the teammate portrait via active in-
ference is expressed as:
LMD =Î» miLmi +Î» cnLcn +Î» ceLce,(4)
whereÎ» mi,Î» cn, andÎ» ce represent the hyperparameters of
the loss function, which are used to balance their effects.
Dual Filter of Accuracy and Relevance
Due to local observation, errors in the active inference pro-
cess are unavoidable. Indiscriminately utilizing erroneous
portraits of teammates can directly distort the agentâ€™s com-
prehension of the current environment, resulting in non-
cooperative behavior. Furthermore, since collaboration in
multi-agent systems is often localized, it is redundant to in-
corporate the portraits of all teammates in decision-making.
Hence, we propose a dual filter mechanism for selecting co-
operative teammates, focusing on two aspects: the accuracy
and the relevance of portraits.
Accuracy FilterAmong triple portraits, the perception
portrait is derived through perspective transformation, ren-
dering it inherently partial. Therefore, an evaluation method
is required to assess the accuracy of perception portraits.
Specifically, we learn a mappingf:R h 7â†’Rthat maps
the perception portrait to an accuracy score. At timet, we
simultaneously process theNÃ—Nportraits, constructing the
evaluation matrixC t.Nrepresents the number of agents.c t
ij
refers to the accuracy score of agentiâ€™s perception portrait
Ë†ht
ij to agentj, formed as:
ct
ij =softmax(f( Ë†ht
ij)),(5)
wheref(Â·)represents a two-layer MLP network. A higher
value ofc ij signifies that the perception portrait of agentj
by agentiis more accurate. The evaluation matrixC t should
possess the following characteristics.
(1)Mutual evaluation is similar.For agentsiandj, per-
ception portraits of each other are essentially different per-
spectives on the same intersection of their observations.
(2)Self-evaluation is highest.The perception portrait of
an agentâ€™s own is guaranteed to be accurate, so the networkâ€™s
evaluation of this result should be the highest.
(3)High similarity gets a high score.If an agentâ€™s per-
ception portrait is similar to the real observation of any other
agent, it indicates high accuracy.
To satisfy theMutual evaluation is similar, we introduce
a symmetry lossL sy to optimize the evaluation matrixC,
which is formalized as:
Lsy =âˆ¥C âˆ’ CT âˆ¥F,(6)
whereC T is the transpose of the matrixC, andâˆ¥Â·âˆ¥ F denotes
the Frobenius-norm.
To enhanceSelf-evaluation is highest, we utilize a diag-
onal lossL se to maximize the accuracy score of trueh t
ii.
Lse =âˆ’
X
i
cii.(7)
RegardingHigh similarity gets a high score, deep neu-
ral networks exhibit the property that similar inputs produce
similar outputs. Combined with characteristic (2), percep-
tion portraits similar to the trueh t
ii can be mapped to higher
scores, thereby satisfying characteristic (3) without the ad-
ditional loss function.
Having the evaluation matrixC, we sample thetop kin-
dices for each agent based on the accuracy to select potential
teammates for the next filter.
Relevance FilterBuilding on the accuracy filter, we also
need to apply the relevance filter from a decision-making
perspective, selecting teammates more relevant to the agent
for effective collaboration. Given that collaboration in multi-
agent systems is often localized, we use the perception por-
trait as a proxy for relevance. As for how to utilize team-
matesâ€™ portraits to aid decision-making, the most intuitive
approach is to combine teammatesâ€™ action portraits to miti-
gate the lack of cognition about teammates. However, due to
local observation, accurately modeling actions is challeng-
ing. Therefore, we instead combine belief portraits, lever-
aging higher-level behavioral bases to dilute the impact of
single-step modeling errors.
0 1M 2M0
20
40
60
80
100T est Win Rate %
(a) 8m_vs_9m
0 1M 2M0
20
40
60
80
100
(b) 3M_vs_2z5m
0 1M 2M0
20
40
60
80
100
(c) 2s3z_vs_2s4z
0 1.5M 3M0
20
40
60
80
100
(d) 3s5z_vs_3s6z
0 1M 2M0
20
40
60
80
100
(e) corridor
0 2M 4M0
20
40
60
80
100T est Win Rate %
(f) 6h_vs_8z
0 1M 2M0
20
40
60
80
(g) protoss_5_vs_5
0 1M 2M0
20
40
60
80
(h) terran_5_vs_5
0 1M 2M0
20
40
60
80
(i) zerg_5_vs_5
0 1M 2M0
20
40
60
80
(j) protoss_10_vs_10
0 1M 2M0
20
40
60
80T est Win Rate %
(k) terran_10_vs_10
0 1M 2M0
20
40
60
80
(l) zerg_10_vs_10
0 1M 2M0
10
20
30
40
50T est Return Mean
(m) PD (2v1)
0 1M 2M0
300
600
900
1200
1500
(n) PP (6v2)
0 1M 2M0
700
1400
2100
2800
3500
(o) PP (9v3)
QMIX RODE QPLEX SIRD COLA OMG MAIC T2MAC AIM (Ours)
Figure 3: Performance comparison between AIM and baselines on SMAC, SMACv2, and MPE. (a)-(f) Six representative maps
on SMAC. (g)-(l) Six tasks on SMACv2. (m)-(o) Three tasks on MPE.
Here, we apply the attention mechanism (Vaswani 2017)
to achieve the relevance filter and fusion of belief portraits.
In AIM, for agenti, we adopt the true perception historyh t
i
as the query, the historical perception Ë†ht
k of the selectedk
teammates as the key, and adopt the belief portrait represen-
tationsz t
k of thekteammates as the value. We then fuse the
zt
k, with the attention score represented as:
Î±i,k =
exp

1âˆš
dkey
(ht
iWQ)Â·( Ë†ht
kWK)T

Pk
j=1 exp

1âˆš
dkey
(ht
iWQ)Â·( Ë†ht
jWK)T
,(8)
whereW Q andW K are learnable weight matrices applied to
the queryh t
i and the key Ë†ht
k, whiled key denotes the dimen-
sion of the key vector. The attention scoreÎ±i,k represents the
relevance between agentiand its collaborative teammates.
The combined result ise t
i = Pk
j=1 Î±i,j Â·z t
j, which is con-
catenated withh t
i and subsequently processed by a linear
network to compute the localQ-valueQ local
i .
The optimization objective of this section is:
LDF =Î» syLsy +Î» seLse,(9)
whereÎ» sy andÎ» se represent the hyperparameter of the loss
function.
Overall Training Objective
Following the fusion of teammatesâ€™ belief portraits, we uti-
lize the basic CTDE training framework QMIX (Rashid
et al. 2020) for training. Meanwhile, AIM remains compat-
ible with other value decomposition methods, such as VDN
(Sunehag et al. 2017) or QPLEX (Wang et al. 2020). All
model parameters are updated by minimizing theL TD :
LTD =E
h
(yâˆ’Q tot(Ï„,a)) 2
i
,(10)
wherey=r+Î³max aâ€² Ë†Qtot(Ï„â€²,a â€²)is the target network
of the joint action-value function. By integrating the triple
portrait and the dual filter, the complete training loss is as
follows:
Ltot =L TD +L MD +L DF .(11)
Experiments
We select several methods as our primary baselines, includ-
ing QMIX (Rashid et al. 2020), QPLEX (Wang et al. 2020),
RODE (Wang et al. 2021), COLA (Xu et al. 2023), and
SIRD (Zeng, Peng, and Li 2023). To evaluate the efficacy of
our communication-free method in constructing teammatesâ€™
decision logic, we include two communication-based meth-
ods: MAIC (Yuan et al. 2022) and T2MAC (Sun et al. 2024)
as supplementary baselines. Furthermore, we set OMG (Yu,
Jiang, and Lu 2024) with all teammates being trainable,
serving as a baseline for agent modeling.
0 1M 2M0
10
20
30
40
50T est Return Mean
(a) PD (2v1)
0 1.5M 3M
0
20
40
60
80
100T est Win Rate %
(b) 3s5z_vs_3s6z
0 1M 2M
0
20
40
60
80
100
(c) 3M_vs_2z5m
0 1M 2M
0
20
40
60
80
100
(d) 2s3z_vs_2s4z
AIM_w/o_Belief
AIM_w/o_Action
AIM_w/o_Filter
AIM_w/o_MI
AIM_w/o_CN
AIM_w/o_CE
AIM_w/o_SY
AIM_w/o_SE
AIM
Figure 4: Ablation studies. (a)-(b) illustrate module-wise ablation. (c)-(d) present loss-wise ablation. â€œAIM w/o Beliefâ€ re-
moves belief portrait, â€œAIM w/o Actionâ€ removes action portrait, while â€œAIM w/o Filterâ€ removes dual filter.
Performance Comparisons
We conduct experiments on four MARL benchmarks:
SMAC (Samvelyan et al. 2019), SMACv2 (Ellis et al. 2024),
the Multi-agent Particle-Environment (MPE) (Mordatch and
Abbeel 2018) and the Google Research Football (GRF) (Ku-
rach et al. 2020). Notably, although GRF provides a fully
observable environment, the soccer task demands frequent
collaboration to achieve scoring goals, posing challenges to
the agent modeling methods. For evaluation, we use five dif-
ferent random seeds and plot the average test results as bold
lines. Due to page limitations, we leave the GRF experi-
ments in Appendix D.1.
SMACAs the widely used benchmark in MARL, SMAC
requires agents to master micro-level policies such as â€œfo-
cus fire,â€ â€œkite,â€ and â€œdraw aggroâ€ to defeat enemies un-
der relatively concentrated initial positions. Figure 3 (a-f)
presents the performance of AIM on six tasks, demonstrat-
ing optimal results on most maps. Particularly on maps that
require a clear division of labor and the selection of the best
collaborators, such as3s5z vs 3s6z,corridor, and
6h vs 8z, AIM even outperforms communication-based
baselines. This demonstrates that when agents are nearby,
communication exerts negligible effects on decision-making
quality. In contrast, AIM enables agents to select the most
cooperative teammates by modeling teammatesâ€™ active in-
ference processes. This capability enhances task allocation
efficiency and contributes to achieving team objectives. In
our OMG setup, each allied agent independently models
teammates, failing to capture inter-agent coordination pat-
terns and thus impairing collaboration.
SMACv2As an extension of SMAC, SMACv2 introduces
additional randomness in the initial positions and unit types,
with dispersed agent distributions. It also employs true unit
attack and sight ranges. The performance of AIM is shown
in Figure 3 (g-l). Due to the difficulty in adapting to the
environmental randomness, the baseline methods perform
poorly. In contrast, AIM demonstrates superior adaptabil-
ity to this uncertain environment. The results show that the
proposed perception portrait module in AIM effectively ad-
justs to changes in the agentsâ€™ sight ranges, validating its ef-
ficacy. Furthermore, even with initially dispersed agent posi-
tions, AIM achieves performance comparable to or surpass-
ing communication-based baselines. This validates the effi-
cacy of AIM in assisting agents to understand teammatesâ€™
decisions and enhancing cooperation.
MPEThe MPE provides a 2D physics-based environ-
ment supporting both continuous and discrete action spaces
(Papoudakis et al. 2020). We evaluate the performance of
AIM in the following three tasks with discrete actions: (1)
Physical-Deception(PD)(2v1), (2) Predator-Prey(PP)(6v2),
and (3) Predator-Prey(PP)(9v3). Since the original MPE is
fully observable, we modify it to evaluate the effectiveness
of the perception portrait. Specifically, we set the agentsâ€™
view radius to 0.8. As demonstrated in Figure 3 (m-o), AIM
effectively infers teammatesâ€™ decision logic through model-
ing, enabling collaboration even under strict partial observ-
ability conditions.
Ablation Studies
In this section, we analyze the impact of active inference and
dual filter modules on overall performance using ablation
experiments. We progressively remove these key modules
and evaluate their effects.
Active Inference ModuleWe conduct ablation studies
to validate the active inference moduleâ€™s efficacy. Since
the different sight ranges among units of SMACv2 have
already validated the efficacy of the perception portrait,
we focus on analyzing the belief and action portrait.
We compare AIM with three ablation configurations: 1)
AIM w/o Belief, which only removes the belief portrait;
2)AIM w/o Action, which only removes the action por-
trait; 3)AIM w/o Filter, which removes the dual filter
of accuracy and relevance. The ablation results presented
in Figure 4 (a)-(b) demonstrate that the removal of any in-
dividual module leads to significant performance degrada-
tion. This highlights that: (1) the comprehensive modeling
of teammatesâ€™ decision processes is essential for maintain-
ing accurate models, and (2) the dual filters are crucial for
obtaining optimal teammatesâ€™ information.
To analyze the impact of different loss functions in AIM,
we conduct ablation studies on each loss function. As
demonstrated in Figures 4 (c)-(d), the elimination of any
0 1M 2M
0
20
40
60
80
100T est Win Rate %
3M_vs_2z5m
k=1
k=3
k=4
k=5
k=2 (Ours)
0 1M 2M
0
20
40
60
80
100
2s3z_vs_2s4z
k=1
k=3
k=4
k=5
k=2 (Ours)
0 1.5M 3M
0
20
40
60
80
100
3s5z_vs_3s6z
k=2
k=6
k=8
k=4 (Ours)
0 2M 4M
0
20
40
60
80
100
6h_vs_8z
k=4
k=6
k=2 (Ours)
Figure 5: Analysis of the parameterkin selective collaboration. Different values ofkhave varying impacts on performance.
0 1M 2M
0
20
40
60
80
100T est Win Rate %
2c_vs_64zg
MAPPO
AIM+MAPPO
0 1M 2M
0
20
40
60
80
100
8m_vs_9m
MAPPO
AIM+MAPPO
0 1M 2M0
50
100
150
200
250T est Return Mean
PP (3v1)
MADDPG
AIM+MADDPG
0 1M 2M0
50
100
150
200
250
PP (6v2)
MADDPG
AIM+MADDPG
Figure 6: Scalability Experiments. AIM + MAPPO denotes the extension of AIM to the MAPPO framework, while AIM +
MADDPG represents the adaptation of AIM to the MADDPG framework for continuous action spaces.
Maps QMIX AIM(Teammate) AIM(Ours)
8m vs 9m 87.2 89.194.0
3M vs 2z5m 32.3 84.493.3
3s5z vs 3s6z 00.0 50.458.7
6h vs 8z 00.0 53.667.6
Table 1: Analysis of belief portraits from different perspec-
tives. â€œAIM (Teammate)â€ models belief portraits from the
teammatesâ€™ perspective, while AIM adopts the agentâ€™s per-
spective for modeling. Bold indicates the best performance.
single loss function results in significant performance de-
terioration. Furthermore, the performance decline is more
pronounced whenL cn orL se is removed alone. This high-
lights the critical importance of maintaining continuous be-
lief modeling of teammates while preserving self-focused
decision cognition for enhancing complex task success rates.
Subsequently, we compare the construction of belief por-
traits from the agentâ€™s perspective with that from the team-
mateâ€™s perspective. Table 1 shows theAIM (Teammate)
models belief portraits based on inaccurate perception por-
traits constructed from teammatesâ€™ perspectives. This ap-
proach introduces modeling biases that hinder collaboration.
In contrast, AIM constructs belief portraits of teammates
from the agentâ€™s perspective, effectively reducing inaccura-
cies and improving cooperative performance.
Dual Filter ModuleWe analyze the impact of the pa-
rameterkselected from the perception evaluation matrix,
with a focus on its role in filtering redundant information.
We conduct experiments using four super-hard maps from
SMAC, and the results are shown in Figure 5. It is evident
that the choice ofksignificantly influences the performance
of AIM. Askincreases, collaboration with more teammates
enhances task success rates due to increased information,
but surpassing a certain threshold introduces redundancy,
hindering decision-making. Therefore, the dual filter mod-
ule optimizes the teammate selection, effectively preventing
information redundancy and misleading decisions.
Scalability Across Policy-Based Frameworks
To further investigate the scalability of AIM across different
policy-based frameworks, we extend AIM to both MAPPO
(Yu et al. 2022) and MADDPG (Lowe et al. 2017). We con-
duct evaluations in both SMAC and continuous-action MPE.
The results in Figure 6 demonstrate AIM + MAPPO surpass-
ing MAPPO and AIM + MADDPG exceeding MADDPG.
Experimental results validate the dual scalability capabil-
ities of AIM: (1) extensibility within policy-based frame-
works, and (2) adaptability to continuous action space en-
vironments.
Conclusion and Discussion
In this paper, we propose AIM, a novel framework that re-
places explicit communication (i.e., â€œTellâ€) with agent mod-
eling (i.e., â€œThinkâ€) to construct the cognition of teammatesâ€™
decision logic. AIM models teammatesâ€™ active inference
through perception, belief, and action portraits, and employs
a dual filter module to exclude inaccurate and irrelevant por-
traits. Experiments validate the effectiveness of AIM. How-
ever, in scenarios where selective collaboration is required,
AIM selects a fixed set oftop kteammates as collaborators.
The dynamic selection of collaborative teammates based on
portraits remains an open issue and is left for future work.
Acknowledgments
This work was supported by the National Natural Sci-
ence Foundation of China (Grant No. 62576029) and the
Aeronautical Science Foundation of China (Grant No.
202300010M5001).
References
Das, A.; Gervet, T.; Romoff, J.; Batra, D.; Parikh, D.; Rab-
bat, M.; and Pineau, J. 2019. Tarmac: Targeted multi-agent
communication. InInternational Conference on machine
learning, 1538â€“1546. PMLR.
Ding, Z.; Huang, T.; and Lu, Z. 2020. Learning individ-
ually inferred communication for multi-agent cooperation.
Advances in Neural Information Processing Systems, 33:
22069â€“22079.
Ellis, B.; Cook, J.; Moalla, S.; Samvelyan, M.; Sun, M.; Ma-
hajan, A.; Foerster, J.; and Whiteson, S. 2024. Smacv2: An
improved benchmark for cooperative multi-agent reinforce-
ment learning.Advances in Neural Information Processing
Systems, 36.
Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.;
Pezzulo, G.; et al. 2016. Active inference and learning.Neu-
roscience & Biobehavioral Reviews, 68: 862â€“879.
He, H.; Boyd-Graber, J.; Kwok, K.; and DaumÂ´e III, H. 2016.
Opponent modeling in deep reinforcement learning. InIn-
ternational conference on machine learning, 1804â€“1813.
PMLR.
Hu, G.; Zhu, Y .; Zhao, D.; Zhao, M.; and Hao, J.
2023. Event-Triggered Communication Network With
Limited-Bandwidth Constraint for Multi-Agent Reinforce-
ment Learning.IEEE Transactions on Neural Networks and
Learning Systems, 34(8): 3966â€“3978.
Kiran, B. R.; Sobh, I.; Talpaert, V .; Mannion, P.; Al Sal-
lab, A. A.; Yogamani, S.; and P Â´erez, P. 2021. Deep rein-
forcement learning for autonomous driving: A survey.IEEE
Transactions on Intelligent Transportation Systems, 23(6):
4909â€“4926.
Kurach, K.; Raichuk, A.; Sta Â´nczyk, P.; Zajac, M.; Bachem,
O.; Espeholt, L.; Riquelme, C.; Vincent, D.; Michalski, M.;
Bousquet, O.; et al. 2020. Google research football: A novel
reinforcement learning environment. InProceedings of the
AAAI conference on artificial intelligence, volume 34, 4501â€“
4510.
Lee, J.; Chung, J.; and Sohn, K. 2019. Reinforcement learn-
ing for joint control of traffic signals in a transportation net-
work.IEEE Transactions on Vehicular Technology, 69(2):
1375â€“1387.
Li, D.; Lou, N.; Xu, Z.; Zhang, B.; and Fan, G. 2025. Effi-
cient Communication in Multi-Agent Reinforcement Learn-
ing with Implicit Consensus Generation. InProceedings of
the AAAI Conference on Artificial Intelligence, volume 39,
23240â€“23248.
Lowe, R.; Wu, Y . I.; Tamar, A.; Harb, J.; Pieter Abbeel, O.;
and Mordatch, I. 2017. Multi-agent actor-critic for mixed
cooperative-competitive environments.Advances in neural
information processing systems, 30.
Mordatch, I.; and Abbeel, P. 2018. Emergence of grounded
compositional language in multi-agent populations. InPro-
ceedings of the AAAI conference on artificial intelligence,
volume 32.
Oliehoek, F. A.; Amato, C.; et al. 2016.A concise introduc-
tion to decentralized POMDPs, volume 1. Springer.
Papoudakis, G.; and Albrecht, S. V . 2020. Variational Au-
toencoders for Opponent Modeling in Multi-Agent Systems.
CoRR, abs/2001.10829.
Papoudakis, G.; Christianos, F.; and Albrecht, S. 2021.
Agent modelling under partial observability for deep rein-
forcement learning.Advances in Neural Information Pro-
cessing Systems, 34: 19210â€“19222.
Papoudakis, G.; Christianos, F.; Sch Â¨afer, L.; and Albrecht,
S. V . 2020. Benchmarking multi-agent deep reinforcement
learning algorithms in cooperative tasks.arXiv preprint
arXiv:2006.07869.
Rabinowitz, N.; Perbet, F.; Song, F.; Zhang, C.; Eslami,
S. A.; and Botvinick, M. 2018. Machine theory of mind. In
International conference on machine learning, 4218â€“4227.
PMLR.
Raileanu, R.; Denton, E.; Szlam, A.; and Fergus, R. 2018.
Modeling others using oneself in multi-agent reinforcement
learning. InInternational conference on machine learning,
4257â€“4266. PMLR.
Rashid, T.; Samvelyan, M.; De Witt, C. S.; Farquhar, G.;
Foerster, J.; and Whiteson, S. 2020. Monotonic value func-
tion factorisation for deep multi-agent reinforcement learn-
ing.Journal of Machine Learning Research, 21(178): 1â€“51.
Roesch, M.; Linder, C.; Zimmermann, R.; Rudolf, A.;
Hohmann, A.; and Reinhart, G. 2020. Smart grid for in-
dustry using multi-agent reinforcement learning.Applied
Sciences, 10(19): 6900.
Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-
erster, J.; and Whiteson, S. 2019. The starcraft multi-agent
challenge.arXiv preprint arXiv:1902.04043.
Song, S.; Lin, Y .; Han, S.; Yao, C.; Wu, H.; Wang, S.; and
Lv, K. 2025. CoDe: Communication Delay-Tolerant Multi-
Agent Collaboration via Dual Alignment of Intent and Time-
liness.In Proceedings of the AAAI Conference on Artificial
Intelligence, 39(22): 23304â€“23312.
Sun, C.; Zang, Z.; Li, J.; Li, J.; Xu, X.; Wang, R.; and Zheng,
C. 2024. T2MAC: Targeted and Trusted Multi-Agent Com-
munication through Selective Engagement and Evidence-
Driven Integration. InProceedings of the AAAI Conference
on Artificial Intelligence, volume 38, 15154â€“15163.
Sun, X. 2024.Assessing Model Robustness in Complex Vi-
sual Environments. Ph.D. thesis, The Australian National
University (Australia).
Sun, X.; Leng, X.; Wang, Z.; Yang, Y .; Huang, Z.; and
Zheng, L. 2023a. Cifar-10-warehouse: Broad and more
realistic testbeds in model generalization analysis.arXiv
preprint arXiv:2310.04414.
Sun, X.; Yao, Y .; Wang, S.; Li, H.; and Zheng, L. 2023b. Al-
ice Benchmarks: Connecting Real World Re-Identification
with the Synthetic.arXiv preprint arXiv:2310.04416.
Sunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W. M.; Zam-
baldi, V .; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,
J. Z.; Tuyls, K.; et al. 2017. Value-decomposition net-
works for cooperative multi-agent learning.arXiv preprint
arXiv:1706.05296.
Tian, Z.; Wen, Y .; Gong, Z.; Punakkath, F.; Zou, S.; and
Wang, J. 2019. A regularized opponent model with maxi-
mum entropy objective.arXiv preprint arXiv:1905.08087.
Tung, T.-Y .; Kobus, S.; Roig, J. P.; and G Â¨undÂ¨uz, D. 2021.
Effective Communications: A Joint Learning and Commu-
nication Framework for Multi-Agent Reinforcement Learn-
ing Over Noisy Channels.IEEE Journal on Selected Areas
in Communications, 39(8): 2590â€“2603.
Vaswani, A. 2017. Attention is all you need.Advances in
Neural Information Processing Systems.
Wang, J.; Ren, Z.; Liu, T.; Yu, Y .; and Zhang, C. 2020.
Qplex: Duplex dueling multi-agent q-learning.arXiv
preprint arXiv:2008.01062.
Wang, T.; Gupta, T.; Mahajan, A.; Peng, B.; Whiteson, S.;
and Zhang, C. 2021.{RODE}: Learning Roles to Decom-
pose Multi-Agent Tasks. InInternational Conference on
Learning Representations.
Wang, T.; Wang, J.; Zheng, C.; and Zhang, C. 2019. Learn-
ing nearly decomposable value functions via communica-
tion minimization.arXiv preprint arXiv:1910.05366.
Xie, A.; Losey, D.; Tolsma, R.; Finn, C.; and Sadigh, D.
2021. Learning latent representations to influence multi-
agent interaction. InConference on robot learning, 575â€“
588. PMLR.
Xu, Z.; Zhang, B.; Li, D.; Zhang, Z.; Zhou, G.; Chen, H.;
and Fan, G. 2023. Consensus learning for cooperative multi-
agent reinforcement learning. InProceedings of the AAAI
Conference on Artificial Intelligence, volume 37, 11726â€“
11734.
Xue, W.; Qiu, W.; An, B.; Rabinovich, Z.; Obraztsova, S.;
and Yeo, C. K. 2021. Mis-spoke or mis-lead: Achieving ro-
bustness in multi-agent communicative reinforcement learn-
ing.arXiv preprint arXiv:2108.03803.
Yang, T.; Meng, Z.; Hao, J.; Zhang, C.; Zheng, Y .; and
Zheng, Z. 2018. Towards efficient detection and optimal
response against sophisticated opponents.arXiv preprint
arXiv:1809.04240.
Yao, C.; Lin, Y .; Song, S.; Wu, H.; Ma, Y .; Han, S.; and Lv,
K. 2025. From General Relation Patterns to Task-Specific
Decision-Making in Continual Multi-Agent Coordination.
arXiv preprint arXiv:2507.06004.
Yu, C.; Velu, A.; Vinitsky, E.; Gao, J.; Wang, Y .; Bayen, A.;
and Wu, Y . 2022. The surprising effectiveness of ppo in
cooperative multi-agent games.Advances in Neural Infor-
mation Processing Systems, 35: 24611â€“24624.
Yu, X.; Jiang, J.; and Lu, Z. 2024. Opponent modeling based
on subgoal inference.Advances in Neural Information Pro-
cessing Systems, 37: 60531â€“60555.
Yuan, L.; Wang, J.; Zhang, F.; Wang, C.; Zhang, Z.; Yu, Y .;
and Zhang, C. 2022. Multi-agent incentive communication
via decentralized teammate modeling. InProceedings of
the AAAI Conference on Artificial Intelligence, volume 36,
9466â€“9474.
Zeng, X.; Peng, H.; and Li, A. 2023. Effective and stable
role-based multi-agent collaboration by structural informa-
tion principles. InProceedings of the AAAI conference on
artificial intelligence, volume 37, 11772â€“11780.
Zhai, Y .; Peng, P.; Su, C.; and Tian, Y . 2023. Dynamic Be-
lief for Decentralized Multi-Agent Cooperative Learning. In
Elkind, E., ed.,Proceedings of the Thirty-Second Interna-
tional Joint Conference on Artificial Intelligence, 344â€“352.
Zhu, C.; Dastani, M.; and Wang, S. 2024. A survey of multi-
agent deep reinforcement learning with communication.Au-
tonomous Agents and Multi-Agent Systems, 38(1): 4.
Zintgraf, L.; Devlin, S.; Ciosek, K.; Whiteson, S.; and
Hofmann, K. 2021. Deep interactive bayesian rein-
forcement learning via meta-learning.arXiv preprint
arXiv:2101.03864.