LEARNINGPERCEPTIONANDPLANNINGWITHDEEPACTIVEINFERENCE
OzanC¸atal TimVerbelen JohannesNauta CedricDeBoom BartDhoedt
IDLab
DepartmentofInformationTechnologyatGhentUniversity-imec
ABSTRACT tooptimizingfortwodistinctobjectives. Ontheonehandthe
agentactivelysamplestheworldtoupdateitsinternalmodel
Active inference is a process theory of the brain that states
of the world and better explain observations. On the other
that all living organisms infer actions in order to minimize
hand the agent is driven to visit preferred states that it be-
their (expected) free energy. However, current experiments
lieves a priori it will visit—a kind of global prior— which arelimitedtopredefined, oftendiscrete,statespaces. Inthis
carrylittleexpectedfreeenergy.
paper we use recent advances in deep learning to learn the
Formally, we assume an agent entertains a generative
statespaceandapproximatethenecessaryprobabilitydistri-
model P(o˜,s˜,a˜,π) of the environment, which specifies the
butionstoengageinactiveinference.
joint probability of observations, actions and their hidden
Index Terms— active inference, deep learning, percep- causes, where actions are determined by some policy π. If
tion,planning
the environment is modelled as a Markov Decision Process
(MDP)thisgenerativemodelfactorizesas:
1. INTRODUCTION
P(o˜,s˜,a˜,π)=P(π)P(s )
0
Activeinferencepostulatesthatactionselectioninbiological (cid:89) T (1)
P(o |s )P(s |s ,a )P(a |π)
systems, in particular the human brain, is actually an infer- t t t t−1 t−1 t−1
t=1
ence problem where agents are attracted to a preferred prior
state distribution in a hidden state space [1]. To do so, each Thefreeenergyisthendefinedas:
living organism builds an internal generative model of the
F =E [logQ(s˜)−logP(o˜,s˜,a˜)]
world, by minimizing the so-called free energy. The idea Q
of active inference stems from neuroscience [2, 3] and has =D KL (Q(s˜)(cid:107)P(s˜,a˜|o˜))−logP(o˜) (2)
already been adopted to solve different control and learning =D (Q(s˜)(cid:107)P(s˜,a˜))−E [logP(o˜|s˜)]
KL Q
tasks [4, 5, 1]. These experiments however typically make
use of manually engineered state transition models and pre-
whereQ(s˜)isanapproximateposteriordistribution.Thesec-
defined,oftendiscrete,statespaces. ond equality shows that the free energy is minimized when
In this paper we show that we can also learn the state the KL divergence term becomes zero, meaning that the ap-
space and state transition model, by using deep neural net- proximateposteriorbecomesthetrueposterior,inwhichcase
works as probability density estimators. By sampling from thefreeenergybecomesthenegativelogevidence. Thiscan
thelearntstatetransitionmodel,wecanplanaheadminimiz- also be rewritten as the third equality, which is the negative
ingtheexpectedfreeenergy,tradingoffgoal-directedbehav- evidencelowerbound(ELBO)thatalsoappearsinthevaria-
ioranduncertainty-resolvingbehavior. tionalautoencoders(VAE)framework[6,7].
Inactiveinference,agentsinfertheactionsthatwillresult
invisitingstatesoflowexpectedfreeenergy. Theydothisby
2. ACTIVEINFERENCE
samplingactionsfromapriorbeliefaboutpoliciesaccording
to how much expected free energy that policy will induce.
Activeinferencestatesthateveryorganismoragententertains
Formally, this means that the probability of picking a policy
an internal model of the world, and implicitly tries to mini-
isgivenby[8]:
mizethedifferencebetweenwhatitbelievesabouttheworld
and what it perceives, hence minimizing its own variational
P(π)=σ(−γG(π))
freeenergy[2]. Moreover,theagentbelievesthatitwillmin-
(cid:88) (3)
imizeitsexpectedfreeenergyinthefuture, ineffectturning G(π)= G(π,τ)
action selection into an inference problem. This boils down τ
whereσ isthesoftmaxfunctionwith precisionparameterγ,
OzanC¸atalisfundedbyaPh.D.grantoftheFlandersResearchFounda-
tion(FWO).ThisworkissupportedbyAIResearchFlanders which governs the agents goal-directedness and randomness
0202
beF
42
]GL.sc[
2v14811.1002:viXra
in its behavior. Here G is the expected free energy at future
timestepτ whenfollowingpolicyπ[8]:
G(π,τ)=E [logQ(s |π)−logP(o ,s |π)]
Q(oτ,sτ|π) τ τ τ
=E [logQ(s |π)−logP(o |s ,π)
Q(oτ,sτ|π) τ τ τ
−logP(s |π)]
τ
=D (Q(s |π)(cid:107)P(s ))+E [H(o |s )]
KL τ τ Q(sτ) τ τ
(4)
WeusedQ(o ,s |π)=P(o |s )Q(s |π)andthattheprior
τ τ τ τ τ
probabilityP(s |π)isgivenbyapreferredstatedistribution
τ
P(s ). Thisresultsintotwoterms: aKLdivergencetermbe-
τ
tween thepredicted statesand theprior preferredstates, and
anentropytermreflectingtheexpectedambiguityunderpre-
dictedstates.Actionselectioninactiveinferencethusentails:
1. EvaluateG(π)foreachpolicyπ
2. CalculatethebeliefP(π)overpolicies
3. InferthenextactionusingP(π)P(a |π)
t+1
3. DEEPACTIVEINFERENCE
For the agent’s model, we use deep neural networks to pa-
rameterize the various factors of equation (1): i.e. the tran-
sition model p (s |s ,a ) and the likelihood distribution
θ t t−1 t
p (o |s ). Also the approximate posterior is parameterized
ξ t t
by a neural network p (s |s ,a ,o ). All distributions
φ t t−1 t t
areparameterizedasi.i.dmultivariateGaussiandistributions,
i.e. the outputs of the neural networks are the means µ and
standard deviations σ of each Gaussian. Sampling is done
using the reparameterization trick, computing µ + (cid:15)σ with
(cid:15) ∼ N(0,1),whichallowsforbackpropagationofthegradi-
ents.Minimizingthefreeenergythenboilsdowntominimiz-
ingthefollowinglossfunction:
∀t:minimize:−logp (o |s )
ξ t t
φ,θ,ξ (5)
+D (p (s |s ,a ,o )(cid:107)p (s |s ,a ))
KL φ t t−1 t−1 t θ t t−1 t−1
Figure 1 shows an overview of the information flow be-
tween the transition model, approximate posterior and like-
lihood neural networks. To engage in active inference using
thesemodels,weneedtoestimateG(π,τ),whichinvolveses-
timatingQ(s |π).Asourmodeltakesastatesampleasinput,
τ
andonlyestimatesthestatedistributionofthenexttimestep,
the only way to get an estimate of the state distribution at a
futuretimestepτ > t+1isbyMonteCarlosampling. Con-
cretely, to infer P(π), we sample for each policy N trajec-
tories following π using p . This results in N state samples
θ
sˆ ,forwhichwecangetN observationestimatesoˆ viap .
τ τ ξ
To be able to calculate the KL divergence and entropy, we
use a Gaussian distribution fitted on the samples’ mean and
variance. Wethenestimatetheexpectedfreeenergyforeach
policyfromcurrenttimesteptonwardasfollows:
Model Transition
Posterior
Model Transition
Likelihood
Posterior Sample
Fig. 1: We train simultaneously a transition model
p (s |s ,a ),anapproximateposteriordistributionmodel
θ t t−1 t
p (s |s ,a ,o ), and a likelihood distribution p (o |s ) φ t t−1 t t ξ t t
model,byminimizingthevariationalfreeenergy.
t+K
Gˆ(π)= (cid:88) D (N(µ ,σ )(cid:107)P(s ))+ 1 H(N(µ ,σ ))
t KL sˆτ sˆτ τ ρ oˆτ oˆτ
τ=t+1
+ (cid:88) σ(−γGˆ (π(cid:48)))Gˆ (π(cid:48)) (6)
t+K t+K
π(cid:48)
The first summation term looks K timesteps ahead, cal-
culating the KL divergence between expected and preferred
statesandtheentropyontheexpectedobservations. Wealso
introduce an additional hyperparameter ρ, which allows for
atrade-offbetweenreachingpreferredstatesontheonehand
andresolvinguncertaintyontheotherhand.Thesecondsum-
mation term implies that after K timesteps, we continue to
selectpoliciesaccordingtotheirexpectedfreeenergy,hence
recursivelyre-evaluatingtheexpectedfreeenergyofeachpol-
icy at timestep t+K. In practice, we unroll this D times,
resultingintoasearchtreewithaneffectiveplanninghorizon
ofT =K×D.
4. EXPERIMENTS
We experiment with the Mountain Car problem, where an
agentneedstodrivethecarupthemountainin1Dbythrot-
tlingleftorright,asshownonFigure2. Thetopofthemoun-
tain can only be reached by first building up momentum be-
forethrottlingright. Theagentspawnsatarandomposition,
and only observes a noisy position sensor and has no access
toitscurrentvelocity. Ateachtimestep,theagentcanchoose
between two policies: π to throttle to the left, π to throttle
l r
totheright. Weexperimentwithtwoflavorsofthisenviron-
ment: onewheretheagentstartswithfixedzerovelocity,and
onewheretheagentstartswitharandominitialvelocity.
Forourgenerativemodel,weinstantiatep (s |s ,a ),
θ t t−1 t
p (s |s ,a ,o ) and p (o |s ) as fully connected neural
φ t t−1 t t ξ t t
againforcestheagenttoputmoreweightonresolvinguncer-
tainty,preferringthepolicyinFigure4a.
5. DISCUSSION
Using deep neural networks to instantiate the generative
model and to approximate both prior and posterior distribu-
tions,hastheadvantagethatthegenerativemodelisindepen-
dentofanystaterepresentation. Themodelcanlearnthebest
state representation for the observed data. Employing deep
Fig. 2: The mountain car environment. The shown position
neural networks also opens up the possibility of using high-
ofthecarat−0.5isthestartingpositioninourevaluations.
dimensionalsensorinputs,e.g.images. Thedownsideofour
model, however, istherequiredsamplingstep, whichmeans
thatadistributionisonlycalculatedforthenexttimestep,and
networks with 20 hidden neurons, and a state space with 4 distributionsfortimestepsτ furtherinthefuturecanonlybe
dimensions. Tobootstrapthemodel,wetrainonactionsand approximatedbysampling.
observation of a random agent minimizing the loss function Another point of discussion is the definition of the pre-
in Equation (5) using stochastic gradient descent. Next, we ferredstatedistribution. InourcaseweoptedforaGaussian
instantiateanactiveinferenceagentthatusesEquation(6)to statedistribution, centeredaroundthestatevisitedbyanex-
planaheadandselectthebestpolicy.Aspreferredstatedistri- pertdemonstration,similartoourearlierwork[9]. However,
butionP(s τ ),wemanuallydrivethecarupthemountainand the standard deviation of this distribution will determine the
evaluatethemodel’sposteriorstateattheendofthesequence absolutevalueoftheKLterminEquation(6). Asmallstan-
sˆ end , and set P(s τ ) = N(sˆ end ,1). To limit the computa- darddeviationwillblowuptheKLterm,completelyignoring
tions,theactiveinferenceagentplansaheadfor90timesteps, theentropyterm. Alargestandarddeviationwillassignprob-
allowingtoswitchpolicyevery30timesteps,effectivelyeval- abilitymasstoneighboringstates,possiblyintroducinglocal
uatingasearchtreewithdepth3,using100samplesforeach optimathatdon’treachtheactualgoalstate. Tomitigatethis,
policy(K =30,D =3,N =100). we introduced an additional ρ parameter that balances risk
Figure 3 shows the sampled trajectories for all branches andambiguity.
ofthesearchtree,inthecasethemodelisbootstrappedwith Finally,planningbygeneratingandevaluatingtrajectories
onlyasingleobservationatposition−0.5. Thisisachalleng- ofthecompletesearchtreeiscomputationallyexpensive. In
ing starting position as the car needs enough momentum in thispaper,weintentionallypursuedthisapproachinorderto
order to reach up the hill from there. In the case of a ran- directly investigate the effect of the KL term versus the en-
domstartingvelocity,thegenerativemodelisnotsureabout tropy term. To mitigate the computational load, one might
thevelocityafter onlythefirstobservation. Thisis reflected amortizetheresultingpolicybytrainingapolicyneuralnet-
by the entropy (i.e. the expected ambiguity) of the sampled work p (a |s ) based on the visited states and the planned
π t t
trajectories. Nowfollowingπ fromthestartwillsometimes actionsbytheagent,similarto[10]. Inotherapproaches,the
r
reachthepreferredstate,dependingontheinitialvelocity. In policyislearneddirectlythroughend-to-endtraining. Forex-
this case the active inference agent’s behavior is determined ample, K. Ueltzho¨ffer uses evolution strategies to learn both
bytheparameterρ.Forρ>1,theagentwillactgreedily,pre- a model and a policy, that requires part of the state space to
ferringthepolicythathasachanceofreachingthetopearly, be fixed, to contain information of the preferred state [11].
cf. Figure 3e. When setting ρ << 1, the entropy term will B.Millidgeontheotherhandamortizestheexpectedfreeen-
playabiggerrole,andtheagentwillselectthepolicythatis ergy G as function of the state, similar to value function es-
lessuncertainabouttheoutcomes,renderingamorecautious timationinreinforcementlearning[12]. Again,however,the
agent that prefers a more precise and careful policy, moving perceptionpartisomittedandthestatespaceisfixedupfront.
totheleftfirst seeFig.3a. Wefoundsettingρ=0.1yieldsa
goodtrade-offforthemountaincaragent.
6. CONCLUSION
Intheenvironmentwithnoinitialvelocity, thetransition
model learnt by the agent is quite accurate and the entropy Inthispaper,wehaveshownhowgenerativemodelsparame-
termsareanorderofmagnitudelower,asshowninFigure4. terizedbyneuralnetworksaretrainedbyminimizingthefree
However, in terms of preferred state the lowest KL is still energy,andhowthesecanbeexploitedbyanactiveinference
achievedbyfollowingπ . ThisisduetothefactthattheKL agenttoselecttheoptimalpolicy. Wewillfurtherextendour
r
termisevaluatedeachtimestep,andmovingtotheleft,away modelstoworkinmorecomplexenvironments,inparticular
fromthepreferredstateinthesequenceoutweighsthebenefit towards more complex sensory inputs such as camera, lidar
ofreachingthepreferredstateintheend. Choosingρ = 0.1 orradardata.
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
noitisop
KL: 7.02, H: -0.37, G: 3.28
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(a)left-right-right
noitisop
KL: 7.09, H: -0.16, G: 5.50
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(b)left-right-left
noitisop
KL: 7.89, H: -0.26, G: 5.32
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(c)left-left-right
noitisop
KL: 7.90, H: -0.28, G: 5.05
(d)left-left-left
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
noitisop
KL: 5.07, H: 0.09, G: 5.96
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(e)right-right-right
noitisop
KL: 5.38, H: 0.16, G: 6.93
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(f)right-right-left
noitisop
KL: 7.07, H: -0.04, G: 6.68
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(g)right-left-right
noitisop
KL: 7.35, H: -0.08, G: 6.58
(h)right-left-left
Fig.3: Dependingontherandominitialvelocity,thecarwillreachthehillfastusingtherightpolicyonlypartofthecases(e),
howeverstartingwiththeleftpolicyfirstalsoreachesthehilltopandwithlowerentropyonthetrajectories(a). Agreedyagent
(ρ>1)willpick(e)whereasacautiousagent(ρ<<1)willfavor(a). ForeachpolicywereportthevaluesofKL,HandGfor
ρ=0.1.
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
noitisop
KL: 10.55, H: -1.83, G: -7.70
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(a)left-right-right
noitisop
KL: 10.44, H: -1.14, G: -0.96
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(b)left-right-left
noitisop
KL: 10.26, H: -1.25, G: -2.19
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(c)left-left-right
noitisop
KL: 10.42, H: -1.31, G: -2.67
(d)left-left-left
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
noitisop
KL: 9.14, H: -1.33, G: -4.16
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(e)right-right-right
noitisop
KL: 9.88, H: -1.40, G: -4.16
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(f)right-right-left
noitisop
KL: 11.03, H: -1.49, G: -3.87
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25 0 20 40 60 80
time
(g)right-left-right
noitisop
KL: 11.11, H: -1.70, G: -5.91
(h)right-left-left
Fig. 4: When the environment starts the car with a fixed zero velocity, the model is on average much more certain on the
predictedtrajectories,resultinginlowerentropyterms. However,policy(e)stillachievesthelowestKLvalue,asthistermis
evaluatedeachtimestep,andmovingawayfromthepreferredstateyieldsahighKLpenalty.Whenchoosingρ=0.1,theagent
againfavors(a). ForeachpolicywereportthevaluesofKL,HandGforρ=0.1.
7. REFERENCES inferenceindeepgenerativemodels,” inProceedingsof
the31stInternationalConferenceonMachineLearning,
[1] Karl Friston, Thomas FitzGerald, Francesco Rigoli, 2014.
PhilippSchwartenbeck,andGiovanniPezzulo, “Active
inference:AProcessTheory,”NeuralComputation,vol. [8] Philipp Schwartenbeck, Johannes Passecker, Tobias
29,pp.1–49,2017. Hauser, Thomas H B FitzGerald, Martin Kronbichler,
and Karl J Friston, “Computational mechanisms of
[2] Karl Friston, “The free-energy principle: A unified
curiosity and goal-directed exploration,” bioRxiv, p.
brain theory?,” Nature Reviews Neuroscience, vol. 11,
411272,2018.
no.2,pp.127–138,2010.
[9] Cedric De Boom, Tim Verbelen, and Bart Dhoedt,
[3] Karl Friston, James Kilner, and Lee Harrison, “A free
“Deep active inference for state estimation by learning
energy principle for the brain,” Journal of Physiology
from demonstration,” in Conference of Complex Sys-
Paris,vol.100,no.1-3,pp.70–87,2006.
tems (CCS), Satellite Symposium on Complexity from
[4] Karl Friston, Spyridon Samothrakis, and Read Mon- Cells to Consciousness: Free Energy, Integrated Infor-
tague, “Active inference and agency: Optimal control mation,andEpsilonMachines,2018.
without cost functions,” Biological Cybernetics, vol.
[10] Ozan Catal, Johannes Nauta, Tim Verbelen, Pieter
106,no.8-9,pp.523–541,2012.
Simoens, and Bart Dhoedt, “Bayesian policy selection
[5] Karl Friston, Philipp Schwartenbeck, Thomas FitzGer- using active inference,” in Workshop on Structure &
ald, Michael Moutoussis, Timothy Behrens, and Ray- Priors in Reinforcement Learning at ICLR 2019 : pro-
mond J. Dolan, “The anatomy of choice: active infer- ceedings,2019,p.9.
ence and agency,” Frontiers in Human Neuroscience,
vol.7,no.September,pp.1–18,2013. [11] Kai Ueltzho¨ffer, “Deep active inference,” Biological
Cybernetics,vol.112,no.6,pp.547–573,Dec2018.
[6] DiederikP.KingmaandMaxWelling, “Auto-encoding
variationalbayes,” CoRR,vol.abs/1312.6114,2013. [12] Beren Millidge, “Deep active inference as variational
policygradients,” CoRR,vol.abs/1907.03876,2019.
[7] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra, “Stochasticbackpropagationandapproximate