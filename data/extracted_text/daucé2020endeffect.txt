End-Effect Exploration Drive for Effective
Motor Learning
Emmanuel Dauc¬¥e1,2[0000‚àí0001‚àí6596‚àí8168]
1 Institut de Neurosciences de la Timone, CNRS/Aix-Marseille Univ, France
2 Ecole Centrale de Marseille, France
emmanuel.dauce@univ-amu.fr
Abstract. Stemming on the idea that a key objective in reinforcement
learning is to invert a target distribution of effects, end-effect drives are
proposedasaneffectivewaytoimplementgoal-directedmotorlearning,
intheabsenceofanexplicitforwardmodel.Anend-effectmodelrelieson
asimplestatisticalrecordingoftheeffectofthecurrentpolicy,hereused
as a substitute for the more resource-demanding forward models. When
combined with a reward structure, it forms the core of a lightweight
variational free energy minimization setup. The main difficulty lies in
the maintenance of this simplified effect model together with the online
update of the policy. When the prior target distribution is uniform,
it provides a ways to learn an efficient exploration policy, consistently
withtheintrinsiccuriosityprinciples.Whencombinedwithanextrinsic
reward, our approach is finally shown to provide a faster training than
traditional off-policy techniques.
Keywords: Reinforcement Learning ¬∑ Intrinsic reward ¬∑ Model-based
Exploration ¬∑ Motor learning
1 Introduction
Recentdevelopmentsinartificialintelligencehaveproducedimportantqualitative
leaps in the field of pattern recognition, in video games and assisted driving.
However,anumberoftasksconsideredassimple,arestrugglingtofindaconvincing
artificialimplementation...Thisisthefieldofactionselectionandmotorcontrol.
For instance, the fine manipulation of objects, as well as movement in natural
environments, and their combination through real time motor control, remain
major scientific challenges at present. Compared to the case of video games,
reinforcementlearning,forinstance,remainsratherlimitedinthefieldofrobotic
and motor control. The huge improvements ‚Äúfrom-scratch‚Äù obtained in virtual
environmentsaredifficulttotransfertorealrobotics,wheremillionsofplayscan
not be engaged under risk of breakage, and simulators are expensive to develop.
The brain capability to develop motor skills in a very wide range of areas has
thus no equivalent in the field of artificial learning.
In short, learning a motor command, or a motor skill, happens to become
difficultwhenconsideringamoderatelycomplexsetofeffectors,likeamulti-joint
0202
tcO
5
]IA.sc[
2v06951.6002:viXra
2 E. Dauc¬¥e
arm for instance, operating in the physical world. One aspect of the problem is
that the set of circuits that process the sensory signals and produce a motor
response is composed of digital units, the neurons, operating at fast pace, and
adapting rapidly, while, on the other side, the operation space is made of many
joints,rigidelementsandmusclescoveringawide,continuousdomainofresponses
with many degrees of freedom and much longer response times.
The reinforcement learning framework [16] provides a very generic setup to
address the question of learning, both from a machine learning and the brain
modelling perspectives. It contains many of the interesting constraints that an
agentisfacinginordertolearnamotoract.Thetheory,however,isconstructed
around digital (discrete) control principles. The aim of a digital controller is to
establish a one to one correspondence between stimuli and actions, by matching
the input with a template action. Given a set of pre-defined actions, an agent is
expectedtopicktheonethatmatchesthemosttheinput.Inordertolearnhow
to act, the agent is guided by a reward signal, that is much cheaper to extract
than an exact set point. Then the choice of the action is monitored by a scalar
quantity, the utility, that is the long term sum of rewards [16]. A Reinforcement
Learning agent is agnostic about what the world is. It is just acting so as to
maximizetheutility.Supportedbythesolidprinciplesofdynamicprogramming,
this agent is expected to end-up in an optimal motor behavior with regards to
therewardconstraintsprovidedbytheenvironment.Ahiddendifficultyhowever
lies in finding a good training dataset for the agent. A classical problem in that
case is the lack of sampling efficacy due to the sparsity of the rewards, or to the
closed-loop structure of the control task, where the examples encountered are
statistically dependent on the controller parameters, providing a risk of a self
referential loop and local optimum.
The counterpart to digital control is analog control, corresponding to the
‚Äúclassic‚Äù way to design a controller in the majority of real-world scenarios. In
the analog control case, both the controller and the environment are dynamical
systems. The control of a dynamic system generally relies on a model [14]. The
controller is capable to mimic the behavior of the environment, and know the
effect of its actions on the environment. When a certain objective is given,
the controller can act so as to reach the objective through model inversion,
though, in most cases of interest (like the control of an articulated body), the
modelsarenotinvertibleandnosingledefinitecontrolcanbeestablishedfroma
given objective [7] without setting up additionaland task-specific regularization
constraints.Thiskindofcontrollerneedsaforwardmodel,thatisgenerallygiven,
containing a large engineering knowledge about the agent‚Äôs effector structure
anditsenvironment.Aproblemariseswhenthelearningofamotorcommandis
considered. Such controllers generally lack in adaptivity, and motor adaptation
is generally hard to train when the environmental conditions change.
There is thus an apparent trade-off between, on one side, maintaining a
model, that may include the many mechanical, environmental and sensory data
interactions,and,ontheotherside,beingguidedbyasimplisticreward,concentrating
all the complexity of the external world constraints in a single scalar. The main
End-Effect Exploration Drive for Effective Motor Learning 3
difference between both approaches appears to be the presence (or the absence)
of a definite model of the external world. Knowing the effect of our own actions
in the world provides ways to anticipate and do planning to reach an objective,
at the cost of maintaining and updating a model of the world [11]. At present
time, the trade-off between model-free and model-based control has provoked
manydebatesinthereinforcementlearningcommunity,withapreferencetoward
model-free approaches for they are cheaper to maintain and easier to control,
leaving unsolved the problem of the sampling efficacy and causing very long
training sessions in moderately complex environments.
Our argument here is that the sampling efficacy is bound to the problem of
trainingamodel,andonecannotexpecttohaveanefficientsamplingwithouta
minimal model of the effect of action. This model does not need to be perfectly
accurate,butitneedstobegoodenoughtoallowtheagenttoefficientlysample
itsenvironmentinordertograballthedisposableinformationinrelationtothe
task at hand. We assert here that a simple effect model is enough to provide all
the needed variability in probing the effect of an action or a policy.
2 Method
2.1 A probabilistic view to motor supervision
Theprobabilisticviewtolearningcause-effectrelationshipsisatthecoreofmany
recentdevelopmentsinmachinelearning,withabodyofoptimizationtechniques
known as ‚Äúvariational inference‚Äù implementing model training from data [10].
We assume for simplicity that the environment is not hidden to the agent, i.e.
theenvironmentisfullyobservable.Wealsoassumeadiscreteupdatingofstates
and actions, like in classic reinforcement learning. Then, if s is the state of the
environment (or a context), and a an action performed by the agent, consider e
as the effect of the action performed by the agent in that particular state.
Theeffectmayreflect,tosomepoint,theresult,ortheoutcome,oftheaction.
Modelling an effect thus supposes that an action should come to an end, a final
point, from which it is possible to evaluate or record a result. Consistently with
a bunch of neurological observations [5], a simple end-effector open-loop control
is here assumed to take place, with a compositional motor command [8] driving
a multi-joint effector toward a fixed point, without feedback during the motor
realization.
The effect can be a short-term effect, like reading a new state from the
environment.Itcanalsobealongtermeffect,likewinningorloosingagame,or
reaching an objective s‚àó in the future. Because there is a lot of uncertainty on
theeffectofanaction,itismodeledasaprobabilitydistributionp(E|s,a).When
the effect is not dependent on a context, it can be noted more simply p(E|a).
Given a certain policy œÄ(a|s), one can also consider the average distribution of
effects obtained when applying that specific policy, namely
p(E|s)=E p(E|s,a) (1)
a‚àºœÄ(A|s)
4 E. Dauc¬¥e
This marginal distribution is said the effect distribution. By construction, it is
dependent on a particular policy (possibly stochastic) over states and actions,
and may, for instance, represent the invariant measure of an MDP under that
given policy. In our case however, we mostly consider the open-loop control
case. The policy is defined over the elements of a compositional action, that is
choosing the components of an action. The end-effect of such a compositional
action is the terminal state attained at the end of the action, without reading
the intermediate states.
In goal-directed control, if e is an expected effect, an inverse control policy,
whose role is to maximize the chance to reach the effect, can be defined using
Bayes rule as:
p(e|s,a)œÄ(a|s)
œÄ(a|s,e)= (2)
p(e|s)
Thatistheinversionofthemodelinaprobabilisticsetup[1].Herethemarginal
effect distribution plays the role of a set point, that fixates the distribution of
states toward which the action should head for.
Assume now p‚àó(e|s) be a target distribution of effects. This distribution is
distinct from p(e|s) that is the distribution of effects under the current policy.
Itisassumedtoberealizablefroman(unknown)targetpolicyœÄ‚àó(a|s),thatcan
be decomposed into:
p‚àó(e|s)
œÄ‚àó(a|s)=E œÄ‚àó(a|s,e) (3)
e‚àºp(E|s,a) p(e|s,a)
The right side of the equation provides an estimation of the optimal policy
basedonasampleeoftheeffectoftheaction.Unfortunately,theoptimalinverse
control policy œÄ‚àó(a|s,e) is unknown. A shortcut is to approximate it with the
current inverse control policy œÄ(a|s,e). In that case, it happens from equation
(2) that the formula simplifies to :
p‚àó(e|s)
œÄ‚àó(a|s)(cid:39)E œÄ(a|s,e) (4)
e‚àºp(E|s,a) p(e|s,a)
p‚àó(e|s)
=œÄ(a|s)E (5)
e‚àºp(E|s,a) p(e|s)
This formula shows that a correction term can be applied to the current policy
withoutowninganexplicitforwardmodel,butratherthroughreadingtheaverage
effect of the policy. This forms the basis of a supervised approach to motor
learning,allowingtoupdateapolicysoastoreachatargetmarginaldistribution.
For instance, in a dicrete setup, assuming there exists a Z(s)‚ààR such that
logœÄ(a|s)=Œ≤Q(s,a)+Z(s)(softmaxpolicy)makesitpossibletoupdateQ(s,a)
with the last effect sample e like:
Œ±
Q(s,a)‚ÜêQ(s,a)‚àí (logp(e|s)‚àílogp‚àó(e|s)) (6)
Œ≤
End-Effect Exploration Drive for Effective Motor Learning 5
Thisupdaterendersthecurrentpolicyclosertotheoptimalone.Asideeffectof
thisupdateisthatitalsochangestheeffectmodelthatincludesthecontribution
of the policy (see eq. (1)). Repeating the operation with a small Œ± and different
samples of a and e should, on average, reduce the divergence between œÄ and œÄ‚àó.
Equation (6) also provides an interesting identity when considering the classical
(reward-based) TD-error, with 1(logp(e|s)‚àílogp‚àó(e|s)) taking the role of the
Œ≤
TD error, i.e. being identified with Q(s,a)‚àíRÀú(e) (with RÀú(e) a putative sum of
rewards up to e), making it possible, for instance, to set up an intrinsic reward
implementing a policy that realizes a known prior on the effects. This intrinsic
reward is called here the ‚ÄúEnd-Effect Drive‚Äù.
This supervised approach to policy relies on an effect model p(e|s) that is
lessdetailedthanaforwardmodel.Variouskindsofapproximateforwardmodels
canbefoundingoal-directedmotorcontrolliterature,likedynamicgoals[9]and
distal teacher [7], though generally learning to associate the current action with
a distal effect [15,11]. In our case, the model knows nothing about the actions
that are performed by the agent. Only the end-effects are recorded to build the
model.This‚Äúaction-agnostic‚Äùforwardmodelisclosetotheconceptofstate-visit
counter, as it is proposed in [2].
2.2 A uniform exploration drive
An important special case is when the objective is not to reach a certain effect,
but rather to explore uniformly the range of all possible effects. In that case,
the objective effect distribution p‚àó is uniform over the effect space. This kind of
supervisioncanbeseenasageneralizationofmultiple-goalsupervision[6]toward
defining each possible effect as a goal. The expected outcome of this uniform
drive is to provide a uniform sampling over the effect space, i.e. implement a
uniform exploration policy. This intrinsic reward is called here the ‚ÄúEnd-Effect
ExplorationDrive‚Äù(E3Dinshort).Itisconsistentwiththepseudo-countbonus
proposed in [2]. A similar drive was also proposed in a recent draft as the ‚Äústate
marginal matching‚Äù drive [12].
Byconstruction,theE3Dispositivewheneisrarelyvisitedunderthecurrent
policy (p(e|s) < p‚àó(e)), and negative the other way. It thus tries to promote
rare and ‚Äúsurprising‚Äù effects, and lower the occurrence of habitual ‚Äúboring‚Äù
effects. It must be noticed that the promotion of rare effects tends to make
them less rare, and the rejection of habitual effects tends to make them less
habitual,uptoanequilibriumwherethelog-probabilityratioshouldbecloseto
zero. Though the circular dependence between the policy update and the effect
model update can provoke some convergence issues, and the equilibrium may
not be reached in the case of too fast fluctuations of both distributions during
the training process. Some form of regularization is needed in most cases, and,
most importantly, should be counterbalanced with some form of utility drive, in
order to implement policy optimization through reward maximization. This is
the reason why a variational inference setup is particularly well suited in that
case, with the distal uniform drive taking the role of a prior under a variational
formulation.
6 E. Dauc¬¥e
2.3 Link with variational inference
A key intuition in Friston‚Äôs ‚Äúunified brain theory‚Äù paper [4] is interpreting the
utility, as it is defined in economy and reinforcement learning, as a measure of
the negative surprise (i.e the log probability over the sensory data distribution).
Combined with a prior distribution in the control space, the action takes the
roleofalatentvariablethatisupdatedsoastoreducethepredictionerrorwith
regards to the prior, much like in predictive coding.
The unified approach proposed by Friston and colleagues is more generally
consistentwiththevariationalauto-encoderprinciples,inwhichalatentdescription
ofthedataisconstructedsoastoimplementatrade-offbetweenthecomplexity
of the description and the accuracy of the prediction. Variational reinforcement
learning was recently proposed as a way to reconcile the discrete view to motor
controlwiththecontinuousprocessingofthelatentvariableinvariationalauto-
encoders [13,3,6], with the motor command playing the role of a latent code for
the reward data. In our simplified writing, the utility maximization (or surprise
minimization) rests on minimizing:
‚àíE Œ≤R(e)+KL(œÄ(a|s)||œÄ‚àó(a)) (7)
a‚àºœÄ(a|s);e‚àºp(e|s,a)
withR(e)hereameasureofthesumof(extrinsic)rewards,uptoe.Interestingly,
thecurrentpolicyœÄ(a|s)liesatthecrossroadofareference(maximumentropy)
policy œÄ‚àó and reward maximization, with the softmax policy representing a
compromise between both tendencies in the discrete case.
Extending toward a uniform prior over the space of effects can be written
when both considering the motor command and the effect as latent variables
that may both explain the current observation, that writes:
‚àíE Œ≤R(e)+KL(p(a,e|s))||p‚àó(a,e)) (8)
a,e‚àºp(a,e|s))
For the purpose of illustration, we propose here an additional simplification,
thatisassuminganindependence ofbothfactors(eanda)oncausingthecurrent
data (Na¬®ƒ±ve Bayes assumption), dividing the KL term in two parts:
‚àíE Œ≤R(e)+KL(œÄ(a|s))||œÄ‚àó(a))+KL(p(e|s))||p‚àó(e)) (9)
a‚àºœÄ(a|s);e‚àºp(e|s,a)
Thisformsthebaselineofourvariationalpolicyupdatesetup.Theoptimization,
thatisdoneonœÄ(a|s),obeysinthatcaseonatripleconstraint,thatismaximizing
the reward through minimizing both the distance to a baseline policy and the
distance of the effect to a reference (supposedly uniform) effect distribution.
Inadiscretesetup,theuniformpriorontheactionissupposedimplemented
withthesoftmaxdecisionrule.Itisthensufficienttoassumethefollowingupdate
for the action-value function. After reading e, the TD-error should be defined
as:
1
TD(s,a,e)=Œª(Q(s,a)‚àíR(e))+ (logp(e|s)‚àílogp‚àó(e))) (10)
Œ≤
End-Effect Exploration Drive for Effective Motor Learning 7
With Œª a precision hyperparameter accounting for the different magnitudes
of rewards, allowing to manipulate the balance between reward seeking and
exploration-seeking drives. Interestingly, the reward R(e) has here the role of a
regularizer with regards to the current Q-value. The sum of the future rewards
can be estimated using the classical Bellman recurrence equation, i.e. Q(s,a)‚àº
r(s,a)+Q(s(cid:48),a(cid:48)), in which case the training procedure needs to maintain an
estimate of a standard action-value function Q (s,a) to update the actual
ref
parameters of the policy Q(s,a).
3 Results
We present in simulation a pilot implementation of the principle presented in
the previous section. The principal idea is to illustrate an important feature of
biological motor control, that is the control of an effector showing many degrees
of freedom, like e.g. an articulated limb with many joints.
LetAacontrolspaceaccountingforasingledegreefofreedom(hereadiscrete
set of actions i.e. A = {E,S,W,N}), each motor command owning n degrees
of freedom, i.e. a = a ,...,a ‚àà An. The effect space is expected to be much
1:n 1 n
smaller, like it is the case in end-effector control, where only the final set point
of a movement in the peripheral space is considered as the result of the action.
Each degree of freedom is supposed to be independent, i.e. the choice of a does
i
not depend on the choice of a , so that œÄ(a |s ) = œÄ(a |s )√ó...√óœÄ(a |s ).
j 1:n 0 1 0 n 0
When a single context s is considered, the policy writes simply œÄ(a ). The
0 1:n
size of the action space is thus combinatorially high, and one can not expect to
enumerate every possible action in reasonable computational time. In contrast,
theeffectspaceisbounded,andthenumberofallfinalstatescanbeenumerated.
However, the environment is constructed in such a way that some final states
are very unlikely to be reached under a uniform sampling of the action space.
The environment we consider is a grid world with only 18 states and two
rooms, with a single transition allowing to pass from room A to room B (see
figure 1). Starting in the upper left corner of room A, the agent samples a
trajectory a ‚àà A from a policy œÄ, that trajectory being composed of 7
1:7
elementary displacements. The agent does not have the capability to read the
intermediatestatesitispassingthrough,itcanonlyreadthefinalstateafterthe
fulltrajectoryisrealized.Insuchanenvironment,abaselineuniformexploration
does not provide a uniform distribution of the final states. In particular, when
acting at random, the chance to end-up in the first room is significantly higher
than the chance to end up in the second room.
The agent starts from scratch, and has to build a policy œÄ(a) and an effect
model p(s ), with s the final state. There are two task at hand. A first task
n n
is a simple exploration task and the objective is to uniformly sample the effect
space,whichshouldimplyanon-uniformsamplingpolicy.Asecondtaskconsists
in reaching the lower-right corner, the state that shows the lowest probability
withauniformsampling.Forthat,arewardof1isgivenwhentheagentreaches
the lower corner, and 0 otherwise.
8 E. Dauc¬¥e
START
üòÄ
üåü
Fig.1:Asimpletwo-roomsenvironment.Startingfromtheupper-leftcorner,the
agentisaskedtoplanafullsequencemadeof7elementaryactionsa ,...,a ,each
1 7
elementary action being in (E,S,W,N). The only read-out from the environment
is the final state, and a reward, that is equal to 1 if the final state is the lower-
right corner, and 0 otherwise.
Theupdateprocedureismadeofasingleloop(seealgorithm1).Theupdate
is done online at each trial. Both the policy and the effect model are updated,
with different training parameters. The general idea is to train the effect model
a little more ‚Äúslowly‚Äù than the policy, for the policy improvement to firmly take
place before they are passed on the effect model.
Stemming from a uniform policy, the effect of the E3D drive is to render the
‚Äùrare‚Äùstatesmoreattractive,fortheyareboundwithapositiveintrinsicreward,
while the more commonly visited states are bound with a negative reward, that
reflects a form of ‚Äùboredom‚Äù. Marking a rare state as ‚Äúattractive‚Äù tends to
increasethenumberofvisits,andfinallylowertheinitialpositivereward.Inthe
case of a ‚Äúgradient‚Äù in the likelihood of the final states, with a number of final
visits inversely proportional to the distance to the initial state, the E3D drive
favors a progressive ‚Äúexpansion‚Äù of the visiting territory, for each peripheral
state attained will increase the probability to reach its further neighbors, up to
the final limit of the state space. In small environment like the one proposed
here, the limit is rapidly attained and a rapid alternation of visits is observed
over the full state space.
Thefinaldistributionofstatesiscomparedinfigure2inthecaseofauniform
policyandtheE3Ddrive.Inthatspecificsetup,astrongbiasinfavorofthefirst
room is observed, and a gradient of likelihood is observed from the initial state
towardthelowerrightcorner(figure2A).Incontrast,atimeconsistentuniform
pattern of visit is observed in the second case, that illustrates the capability of
the E3D drive to set up specific polices devoted to the wide exploration of the
environment.
Whenarewardrisprovidedbytheenvironment,thequestioncomeswhether
tobalancethepolicyupdateprocedureinfavorofseekingforrewardsorseeking
for novel effects. By construction, the exploration drive is insensitive to the
value of Œ≤, for the update is exactly proportional to 1. A high Œ≤ is associate
Œ≤
with a small update and vice versa. this is not the case for the leftmost part
of the update (equation 10). A high Œ≤ render the agent more sensitive to the
extrinsicrewards.Inpractice,whilenoreward(orauniformreward)isprovided
End-Effect Exploration Drive for Effective Motor Learning 9
Algorithm 1 End-Effect Exploration Drive (E3D)
Require: Œ±, Œ≤, Œª, Œ∑
Q‚Üê(cid:126)0
|A|√ón
p‚Üê Uniform
p‚àó ‚Üê Uniform
while number of trials not exceeded do
sample a ‚àºœÄ(A )
1:n 1:n
read s ,r
n
p‚Üê(1‚àíŒ∑)p+Œ∑1
S=sn
for i‚àà1..n do
Q(a )‚Üê(1‚àíŒ±Œª)Q(a )+Œ±Œªr‚àí Œ±(logp(s )‚àílogp‚àó(s ))
i i Œ≤ n n
end for
end while
(A)
(B)
(b) Fig. 3. Task 2 : a reward r =
1 is provided by the environment
when the agent reaches the lower-
right corner. Cumulative sum
(a) Fig. 2. Task 1 : no reward is provided of rewards over 5000 trials, on
bytheenvironment.Empiricaldistribution 10 training sessions. The E3D
of the final states, after 5000 trials. A. algorithm is compared with state-
Uniform policy. B. End-Effect Exploration of-the-art epsilon-greedy update.
Drive (E3D) algorithm. Œ± = 0.3, Œ≤ = 1, Œ± = 0.3, Œ≤ = 100, Œª = 0.03,
Œª=0.03, Œ∑=10‚àí2. Œ∑=10‚àí2, Œµ=0.1.
by the environment, the agent is only guided by the exploration drive. Once
a reward is encountered, it tends to overtake the initial uniform exploration,
providing a firm tendency toward a reward-effective selection of action. This is
in contrast with the standard epsilon-greedy strategy, imposing to balance the
exploration/exploitation trade-off by hand.
The E3D approach finally provides an Online/on-Policy training procedure
thatconformstothemainrequirementsofefficientreinforcementlearning,showing
bothanefficientexplorationpolicywhentherewardsaresparse,andthecapability
tomonitortheexploration/exploitationtradeoff withtheinversetemperature Œ≤
in function of the magnitude of the rewards.
The cumulative rewards obtained with the E3D update and a state-of-the
art off-policy/epsilon greedy update are compared in figure 3, with Œµ set to 0.1.
Ifbothtechniquesmanageto reacha reward-efficient policy inthelong run,the
exploration strategy developed in E3D makes it easier to reach the rewarding
10 E. Dauc¬¥e
state, providing the reward earlier in time and developing a fast-paced reward-
seeking strategy that overtakes the baseline approaches.
4 Conclusions
Despite its simplicity, our pilot training setup allows to illustrate the main
featuresexpectedfromtheinversionofatargetdistributionofeffects,thatisthe
capabilitytorapidlyexploretheenvironmentthroughareciprocalupdateofthe
policy and the effect model. We found in practice that the update of the model
needstobeabitslowerthanthatofthepolicytoallowforthepolicytoimprove
over time and increase the extent of the effect space in a step by step manner.
By balancing the effect of the rewards with the inverse temperature parameter,
it is possible to catch and exploit very sparse rewards in large environments.
Themodelisdevelopedhereasafirstdraftinan‚Äúend-effect‚Äùsetup,withvery
littleinfluenceofthecontextorthestatesvisitedinthemonitoringofthepolicy.
Extensions toward closed-loop state-action policies is not far from reach, for
equation (10) allows to exploitthe Bellman recurrence to guide the exploration-
driven policy with a reference action-value function, that should be updated
in parallel to the model and the current policy. Extensions toward continuous
actionspacesarealsoneededinordertoaddresseffectivemotorcontrollearning,
which resorts to a deeper interpretation of our approach toward the variational
inference setup.
References
1. Bays,P.M.,Wolpert,D.M.:Computationalprinciplesofsensorimotorcontrolthat
minimize uncertainty and variability. The Journal of physiology 578(2), 387‚Äì396
(2007)
2. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., Munos, R.:
Unifyingcount-basedexplorationandintrinsicmotivation.In:Advancesinneural
information processing systems. pp. 1471‚Äì1479 (2016)
3. Fox, R., Pakman, A., Tishby, N.: Taming the noise in reinforcement learning via
soft updates. arXiv preprint arXiv:1512.08562 (2015)
4. Friston, K.: The free-energy principle: a unified brain theory? Nature reviews
neuroscience 11(2), 127‚Äì138 (2010)
5. Graziano, M.S., Taylor, C.S., Moore, T.: Complex movements evoked by
microstimulation of precentral cortex. Neuron 34(5), 841‚Äì851 (2002)
6. Haarnoja, T., Tang, H., Abbeel, P., Levine, S.: Reinforcement learning with deep
energy-based policies. arXiv preprint arXiv:1702.08165 (2017)
7. Jordan,M.I.,Rumelhart,D.E.:Forwardmodels:Supervisedlearningwithadistal
teacher. Cognitive science 16(3), 307‚Äì354 (1992)
8. Kadmon Harpaz, N., Ungarish, D., Hatsopoulos, N.G., Flash, T.: Movement
decomposition in the primary motor cortex. Cerebral Cortex 29(4), 1619‚Äì1633
(2019)
9. Kaelbling, L.P.: Learning to achieve goals. In: IJCAI. pp. 1094‚Äì1099. Citeseer
(1993)
End-Effect Exploration Drive for Effective Motor Learning 11
10. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
11. Kurutach,T.,Clavera,I.,Duan,Y.,Tamar,A.,Abbeel,P.:Model-ensembletrust-
region policy optimization. arXiv preprint arXiv:1802.10592 (2018)
12. Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., Salakhutdinov, R.:
Efficientexplorationviastatemarginalmatching.arXivpreprintarXiv:1906.05274
(2019)
13. Levine, S., Koltun, V.: Guided policy search. In: International Conference on
Machine Learning. pp. 1‚Äì9 (2013)
14. Miall,R.C.,Wolpert,D.M.:Forwardmodelsforphysiologicalmotorcontrol.Neural
networks 9(8), 1265‚Äì1279 (1996)
15. Mishra,N.,Abbeel,P.,Mordatch,I.:Predictionandcontrolwithtemporalsegment
models. arXiv preprint arXiv:1703.04070 (2017)
16. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press
(2018)