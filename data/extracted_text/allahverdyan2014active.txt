4102
voN
3
]LM.tats[
1v0360.1141:viXra
Active Inference for Binary Symmetric HMMs
Armen Allahverdyan Aram Galstyan
Yerevan Physics Institute USC Information Sciences Institute
Yerevan, Armenia Marina del Rey, CA, USA
Abstract [20]. Since this quantity is not readily available in
practice,severalheuristicmethodsweredeveloped[1].
For instance, a class of methods seeks to implement
We consider active maximum a posteriori
active inference in a way of obtaining possibly unique
(MAP)inferenceproblemforHiddenMarkov
solution (or reducing the set of available solutions)
Models(HMM),where,givenaninitialMAP
[1, 18]. A related familiy of methods tries to learn
estimate of the hidden sequence, we select
thosevariableswhoseuncertaintyislarge[1,19]. Both
to label certain states in the sequence to
heuristics have intuitive rationale and can do well in
improve the estimation accuracy of the re-
practice,buttheoreticalunderstandingofhowexactly
mainingstates. Wedevelopananalyticalap-
thoseheuristicsrelatetoerrorreductionislargelylack-
proachto this problem for the case of binary
ing. Inparticular,mostexistingtheoreticalresultsare
symmetric HMMs, and obtain a closed form
concerned with sub-modular cost functions that allow
solution that relates the expected error re-
to establishcertainoptimality guarantees[8, 9] on ac-
ductiontomodelparametersunderthespeci-
tiveselectionstrategies. However,thesecostfunctions
fiedactiveinferencescheme. Wethenusethis
do not usually refer directly to the error reduction of
solutiontodeterminemostoptimalactivein-
some inference method.
ference scheme in terms of error reduction,
andexaminethe relationofthose schemesto Here we consider the active maximum a posteriori
heuristic principles of uncertainty reduction (MAP) inference problem for the binary symmetric
and solution unicity. HMMs, or BS-HMM (see [3] for a brief review of ac-
tive estimation methods in HMM). BS-HMM is suffi-
ciently simple so it allows to study active MAP infer-
1 Introduction ence analytically [15, 16], yet it still contains all the
essential features of HMMs that make them a flexible
In a typical statistical inference problem, we want to andwidelyusedtoolforprobabilisticmodelinginvar-
infer the true state of some hidden variables based on ious fields [12, 11]. We emphasize that even for the
the observations of some other related variables. The simpleBS-HMMmodelconsideredhere,obtainingan-
quality of statistical inference is generally improved alytical insights about active inference strategies is a
with adding more data or priorinformationabout the non-trivial problem, as we demonstrate below.
hidden variables. When obtaining such information
Wedevelopananalyticalapproachforstudyingtheac-
is costly, it is desirable to select this informationopti-
tive MAP inference problem for BS-HMM, and use it
mallysothatitismostbeneficialtotheinferencetask.
to determine the mostefficientschemes ofuncertainty
This notion of active inference, or active learning [1],
reduction, where labeling a small number of states re-
whereoneoptimizesnotonlyoverthemodels,butalso
sultsinthelargestexpectederrorreductionintheesti-
overthedata,hasbeenusedinvariousinferenceprob-
mationofthe remaining(unlabeled) states. Ouranal-
lems including Hidden MarkovModels (HMMs) [3,2],
ysis allows us examine how the active estimation in
network inference [4, 5, 6, 7], etc.
BS-HMM relates to the heuristic principles of uncer-
The efficiency of active inference is naturally deter- tainty reduction and solution unicity. Specifically, we
minedfromtheerrorreductionoftheinferencemethod obtain a closedform expressionfor the expected error
reduction as a function of model parameters, and use
it to assessthe optimality of active inference schemes.
Finally, we compare our analytical predictions with
numerical simulations and obtain excellent agreement
Copyright 2014 by theauthors.
Active Inference for Binary Symmetric HMMs
within the validity range of the proposed analysis. wherex˜ (x ,y)isobtainedundertheMAPestimation
i 1
with the fixed value of x . Let us define the overlap
The rest of the paper is organized as follows: Sec- 1
gain
tion 2 defines generalities of active MAP-inference.
Sections3and4introducethe BS-HMManditsMAP
N
analysis,respectively. Section5presentsour the main ∆O[xˆ(y),y;1]= [x˜ (x ,y)−xˆ (y)]x p(x|y), (5)
i 1 i i
analytical findings on active inference, and is followed Xx Xi=2
by experimental validation of those findings in Sec-
tion 6. We conclude by discussing our main findings, Then supervising x 1 is meaningful if this gain is posi-
their relationships to the existing heuristics for active tive, ∆O > 0. Note that the sum N is taken over
i=2
inference,andidentifying somefuture directionsofre- the non-supervised variables [cf. (2P)]. In ∆O[xˆ(y),y]
search in Section 7. the average is taken also over all possible values x =
(x ,...,x ). The fully averaged overlap gain ∆O¯ is
1 N
determined from ∆O[xˆ(y),y;1] as in (2).
2 Active inference: generalities
If x was already correctly recovered using the initial
1
2.1 MAP inference MAP estimation,thensupervisingx does notchange
1
anything: {x˜ (xˆ (y),y)}N = {xˆ (y)}N . Hence, we
i 1 i=2 i i=2
Let x= (x ,...,x ) and y =(y ,...,y ) be realiza-
1 N 1 N get from (5):
tions of discrete-time random processes X and Y, re-
spectively. Y isthenoisyobservationofX. Weassume N
a binary alphabet x = ±1, y = ±1 and write the ∆O[xˆ(y),y;1]= [x˜ (−xˆ (y),y)−xˆ (y)]
i i i 1 i
probabilitiesasp(x)andp(y). Theinfluenceofnoiseis x2 X,...,xN Xi=2
describedbytheconditionalprobabilityp(y|x). Given ×x p(−xˆ (y),x ,...,x |y). (6)
i 1 2 N
an observationsequence y, the MAP estimate xˆ(y)=
(xˆ (y),...,xˆ (y)) of x is found from maximizing over
1 N
3 Reducing HMMs to the Ising model
x the posterior probability p(x|y)=p(y|x)p(x)/p(y):
xˆ(y)=argmaxp(x|y). (1) We now consider a binary symmetric HMM. X is a
x
Markovprocessgivenbythefollowingstate-transition
In general, the minimization can produce N(y) out- probabilities
comes. Since they are equivalent, eachof them is cho-
sen (for a given y) with probability 1/N(y), which p(x)= N p(x |x )p(x ), x =±1, (7)
defines a realization xˆ of a random variable Xˆ. k=2 k k−1 1 k
Y
Theestimationaccuracyismeasuredusingtheoverlap where p(x x |x k−1 ) is the transition probability param-
(larger O means better estimate) eterized by a single number 0<q <1:
1
O¯ = p(y) O[xˆ(y),y], (2) p(+1|+1) = p(−1|−1)=1−q,
N(y)
Xy xˆX (y) p(+1|−1) = p(−1|+1)=q.
N
O[xˆ(y),y]= xˆ (y)x p(x|y), (3)
i i The noise is memory-less and unbiased:
Xx Xi=1
N
where three averages (over X, Y and Xˆ) are involved p(y|x)= π(y |x ), y =±1 (8)
k k k
inO¯,andwheretheerrorprobabilityreadsP = 1[1− Y k=1
e 2
O¯
]. Below we are interested by the conditional—over where
N
a fixed y and xˆ—overlap O[xˆ(y),y].
π(−1|+1) = π(+1|−1)=ǫ,
2.2 Active MAP inference π(+1|+1) = π(−1|−1)=1−ǫ,
Given y and xˆ(y) we request the true value of one of and ǫ is the probability of error. Here memory-less
the variables(sayx 1 ,generalizationtomanyvariables referstothefactorizationin(8),whileunbiasedmeans
is straightforward). Below we refer to this procedure that the noise acts symmetrically on both realizations
as supervising. After supervising x 1 , we re-estimate of the Markov process: π(1|−1)=π(−1|1).
other variables:
Recall that the composite process XY is Markovina,
(xˆ (y),...,xˆ (y))→(x˜ (x ,y),...,x˜ (x ,y)), (4) but Y is generally not a Markovianprocess [11, 12].
2 N 2 1 N 1
Armen Allahverdyan, Aram Galstyan
To proceed further, we use the following parametriza- or alternatively, at the critical noise intensities [15]
tion of the HMM [13]:
ǫm =1/[1+ǫ4J/m] , m=1,2,.. (17)
c
eJxkxk−1 1 1−q
p(x |x )= , J = ln , (9)
k k−1
2coshJ 2 (cid:20) q (cid:21)
Furthermore,for any noise intensity above the critical
ehyixi 1 1−ǫ value ǫ>ǫ1, the MAP solution is macroscopicallyde-
π(y |x )= , h= ln , (10) c
i i 2coshh 2 (cid:20) ǫ (cid:21) generate: For any observation sequence of length N,
there are exponentially many (in N) sequences that
MAP estimation is then equivalent to minimizing the
simultaneously maximize the MAP objective [15].
energy function (Hamiltonian) defined as H(y,x) ≡
−ln[p(y|x)p(x)] [13]: Let us focus on the structure of the MAP solution. A
straightforward analysis (first carried out in [14] for
H(y,x)=−J N x x −h N y x , (11) the one-dimensional Ising model) shows that the de-
X k=1 k k+1 X k=1 k k pendenceofzˆ(τ)onτ islocal: τ isfactorizedintonon-
where we have omitted irrelevant factors that are ei- overlapping domains τ = (τ 1 ,w 1 ,τ 2 ,w 2 ,...), where
ther additive or vanish for N ≫1. w k are the walls separating domains. Walls are se-
quences of two or more up spins joined by positive
We findit convenientto introduce the followinggauge
bonds; see below. The estimate sequence zˆ admits
transformation:
similar factorization zˆ=(ˆz ,w ,zˆ ,w ,...), such that
1 1 2 2
ˆz hasthesamelengthasτ ,andisdeterminedsolely
z =x y , τ =y y . (12) k k
i i i i i i+1 from τ (non-uniquely in general).
k
For a given y, the role of observations will be played
We now proceed to describe the domain structure for
by τ = {τ }, while the original hidden variables now
i thosedifferentregimesascharacterizedbyh/J. With-
correspond to z={z }. Hence, (11) reads
i out loss of generality we assume J >0, h>0.
N N
H(τ,z) = −J τ z z −h z ,(13)
k k k+1 k 4.1 The maximum likelihood regime
k=1 k=1
X X
p(τ,z) ∝ exp[−H(τ,z)]. (14)
For sufficiently weak noise [cf. (10)]
Weshallrefertoτ =±1asbond(plusorminus),and
k
to z = ± as spin (up and down). Eq. (13) describes h>2J, (18)
k
theHamiltonianofaone-dimensionalIsingmodelwith
the MAP estimate copies observations xˆ = y. Hence,
randombondsτ inanexternalfieldh[13]. TheMAP
k
zˆ =1 for all i; see (13). The prior probability p(x) is
inference reduces to minimizing this Hamiltonian for i
a given τ, i.e. to taking the zero-temperature limit irrelevant in this regime: on can take J = 0, i.e. the
MAP estimation reduces to the maximum-likelihood
T → 0 [22]. Note also that Eq. (14) is the Gibbs
estimation,wherethe noiseprobabilityp(y|x) is max-
distribution at temperature T =1 [22].
imized for a fixed y; see (8).
Inthe newvariables,theoverlapgaingivenbyEq.(6)
The overlapgain(15)nullifies for(18),because super-
reads:
vising a spin does not change other spins: zˆ =z˜.
i i
N
∆O[ˆz,τ;1]= [z˜(−zˆ (τ),τ)−zˆ(τ)]
i 1 i 4.2 First non-trivial regime
z2,X...,zN Xi=2
z i p(−zˆ 1 (τ),z 2 ,...,z N |τ). (15) We now focus on the following regime:
Below we suppress the arguments of ∆O, if they are
J <h<2J, (19)
clear from the context.
Here 2J > h ensures that there are zˆ-spins equal to
4 Domain structure of MAP −1[otherwisewearebackto(18)],whileh>J means
that two spins joined by a positive bound are both
The MAP estimation for BS-HMMs is characterized equal to 1. To verify it consider
by(countablyinfinite)numbersofoperationalregimes
↑ − ↑ + ↑ − ↑, (20)
that are related to each other via first-order phase
transitions [15]. Those transitions occur at the pa- ↑ − ↓ + ↓ − ↑, (21)
rameter values
where z =1 and z = −1 are represented as ↑ and ↓,
i i
hm =2J/m, (16) respectively, while ± refer to τ =±.
c i
Active Inference for Binary Symmetric HMMs
The energyof (20) is J−4hwhich is smaller than the three spins joined by two positive bonds always look
energy −3J of (21) precisely when h >J. Hence, the up, while h < J means that two spins joined by one
minimal size of the wall in the regime (19) is just two positive bonds can both look down.
(+1) spins joined by a positive bond.
• The olddomains stay intactif they areseparated
Intheregime(19)therearethefollowingdomains[14].
(from both ends) by (at least) three up-looking
spins joined by (at least) two positive bonds.
• A frustrated 2n+1-domains consists of an odd
(and larger than one) number 2n + 1 of conse- • Newdomainsareformedbypair(s)ofspinsjoined
quitive minus bonds (i.e. bonds with τ = −1) byapositivebond,whichareimmersedintoaset
k
thatareboundfrombothendsbypositivebonds. of negative bonds. In the new domains only the
pairsofspinsjoinedbypositivebondcanbe frus-
Frustrated means the correspondence between
trated. Thefrustrationrulefollowstheabovepat-
{τ } and {zˆ } is not unique, e.g. the following
k k tern, where now the pairs (super-spin) play the
two configurations have the same energy
role of single spins in the previous regime (19),
while the odd (even) number of negative spins
+ ↑ − ↑ − ↓ − ↑ +, (22)
built up into one negative (positive) superbond
+ ↑ − ↓ − ↑ − ↑ +. (23)
[14]. Here is an example of this situation
Both (22) and (23) have the same values of +↑+↑−↓−↑−↓+↓ − ↑+↑+, (25)
{τ k }5 k=1 . However,the sequences{zˆ k }4 k=1 aredif- ↑ − ↓ − ↑
ferent,thoughtheyhaveexactlythe sameenergy. | {z }| {z }|{z}|{z}| {z }
wherethesuper-spinsandsuper-bondsareshown
More generally, a frustrated 2n+1-domain sup-
in the second line. This domain is not frustrated.
ports n+1 equivalent sequences [14]; see also be-
Note that in the regime (19), (25) breaks down
low. This non-uniqueness is reflected via a zero-
into two separate domains, the first of which is
temperature entropy [14, 15].
frustrated.
• Non-frustrated domains consist of an even num-
ber of consecutive minus bonds bound from both The domain structure for h < 2J/3 (and smaller val-
ends by positive bonds. In this case, the corre- uesofh)isconstructedrecursivelyfollowingtheabove
spondence between {τ k } and {zˆ k } is unique, e.g. pattern [14].
(+ ↑ − ↓ − ↑ +).
5 Active estimation of basic domains
It is worthwhile to note that the notion of frustra-
tion described above is inherently related to the ex- Weimplementactiveestimationforseparatedomains,
ponential degeneracy (in the number of observations i.e. in each domain we supervise one spin z . Ad-
1
N) of the MAP solution above the first critical noise vantages of this implementation are as follows: (i)
intensity [15]. Indeed, consider, for instance, the frus- properly choosing already one spin (and finding out
trated 3-domain shown in 20 and 21. In any observa- that it is incorrect) allows to get rid of the non-
tion sequence y of length N, the expected number of uniquenessoftheMAPestimate;(ii)there-estimation
such frustrated 3-domains n d scales linearly with N, ˆz(τ)→˜z(x 1 ,τ) is restricted to one domain only; (iii)
n d =αN,whereαisa(possiblysmall)constant. And (15) can be calculated approximately.
sinceeachsuchdomainmultipliesthenumberofMAP
solutionbytwo(i.e.,simultaneouslyflippingbothfrus- 5.1 Frustrated 3-domain
trated spins is still a solution), there overall number
of solutions will scale as ∝2αN, thus indicating expo- In the domain (22) we supervise the second spin. If it
nential degeneracy. is oppositetothe MAP estimate,thenwe moveto the
configuration (23) that has the same energy:
4.3 Further regimes
ˆz(τ)=(+ ↑ − ⇑ − ↓ − ↑ +), (26)
Now lets us focus on the third regime defined by ˜z(τ)=(+ ↑ − ⇓ − ↑ − ↑ +), (27)
2J/3<h<J, (24) where ⇑ shows the supervised spin. Hence, the active
estimation removes the non-uniqueness of the MAP
In this regime, the walls are formed by two (or more) solution. The overlap gain is calculated from (15):
up-spins joined together by two (or more) positive
bonds. Thus 2J/3 < h is precisely the condition why ∆O=2 z 3 p(−1,z 3 |τ), (28)
X
z3
Armen Allahverdyan, Aram Galstyan
where z is the third spin in (26, 27); the one that 5.2 Frustrated 5-domain
3
changes after the re-estimation. The marginal proba-
bility p(−1,z |τ) in (28) cannnot be determined in a There are 3 MAP sequences for the 5-domain
3
closed analytic form [15, 16].
ˆz=(+ ↑ − ↑ − ↓ − ⇑ − ↓ − ↑ +), (35)
Wecalculatep(z 2 ,z 3 |τ)inaimpenetrablewallapproxi- ˜z=(+ ↑ − ↓ − ↑ − ⇓ − ↑ − ↑ +), (36)
mation: whenJ andharesufficientlylargeonecanne-
(+ ↑ − ↓ − ↑ − ↑ − ↓ − ↑ +). (37)
glectfluctuationsofthespinswithinthewallsseparat-
ing domains as compared to spins located within the Thefollowingscenariosarethemostefficientones,i.e.
domains. Hence, we write forn-domain(n=3,5,7,... they lead to the largest ∆O (under (19)): (i) going
and normalization is omitted) from (35) to (36) by supervising the fourth spin (de-
noted by ⇑ in (35)). (ii) going from (36) to (35) by
p(z 2 ,...,z n |τ)∝e−J(z2+zn)−JP n k= − 2 1zkzk+1+hPn k=2 zk. supervising the third spin. Both these cases have the
(29)
same overlap gain (38) due to the mirror symmetry
withrespecttothecenterofthedomainthatisshared
We checked numerically that even for thinnest two-
also by (29).
spin walls the approximation works well already for
J ≃ 1; see below. Using (29) with n = 3 in (28) we The optimal overlap gain is calculated from (29)
get
∆O =2 (z +z −z )p(z ,z ,−1,z |τ)=
2(1−e−2h) 3 5 2 2 3 5
∆O = . (30) z2 X,z3,z5
2+e−2h+e−4J+2h
2e4J e2h 3e2h+1 e4J −2e2h+e4h−2 −1
.
Likewise,iftheoriginallyestimatesequenceisgivenby (3e2h+(cid:0)2)e2h(cid:0)+(cid:0)8J +(2e(cid:1)2h+3e4h+4e6h+1)e(cid:1)4J +(cid:1)e8h
(31), thenupon supervisingthe secondspinweobtain
(38)
zˆ(τ)=(+ ↑ − ⇓ − ↑ − ↑ +), (31)
It can be shownthat (38) is largerthan (30), which is
z˜(τ)=(+ ↑ − ⇑ − ↓ − ↑ +). (32)
thebestoverlapgainfora3-domain. In(37)thereare
no spins whose supervision can lead to the maximal
Instead of (28), we need to use
overlap gain.
∆O=−2 z p(1,z |τ), (33)
3 3 For the 5-domainwe meeta situation thatwas absent
X
z3
in the previous 3-domain case. While (38) calculates
which yields
theaverageoverlapgainforsupervisingthefourthspin
2(1−e−2(2J−h)) in (35), it is possible that this spin was recovered by
∆O = . (34)
2+e−2h+e−4J+2h the MAP correctly, i.e. it does not flip after supervis-
ing. Then we are left with no overlap gain and also
Remarkably, (34) and (30) are different. This seems
with uncertainty between (35) and (37), since they
surprising,sincegiventheapparentsymmetrybetween
both have the correct fourth spin. We can now su-
(26, 27) and (31, 32), one would expect that the over-
pervise anadditional spin, so as to recoverthe unique
lap gain will be the same in both cases. This is not
original sequence. It is seen from (35, 37) that when
the case, because (33) with (28) are defined quite dif-
starting from (35) there are 2 possibilities: to super-
ferently, e.g. (33) involves p(1,z |τ), while (28) has
3 vise the second spin or the third one. Additional 2
p(−1,z |τ).
3 possibilities exist when supervising from (37). Out of
Both (30) and (34) are positive in the regime (19), these4possibilitiesthelargestoverlapgainisachieved
but (30)>(34): when the supervised (initially) agrees for supervising the second spin in (35):
with the observations, the gain of active estimation is
∆O =2 z p(−1,z ,1,z |τ)= (39)
larger. 3 3 5
zX 3,z5
For h < J we can employ the same two domains (26, 8e4h+6Jsinh(h)cosh(h−2J)
27) and (31, 32) provided that they are surrounded ,
(3e2h+2)e2h+8J +(2e2h+3e4h+4e6h+1)e4J +e8h
by sufficiently thick walls; see Section 4.3. Now the
opposite relation holds: (30)<(34). where p(−1,z ,1,z |τ) shows that the fourth spin is
3 5
fixed to its correct MAP value 1. The same ∆O as
Note finally that wheneverthe supervisingshows that
in (39) is reached when supervising the third spin in
the true value of the spin was already recognized cor-
(37). The remaining two possibilities are inferior.
rectly,theoverlapdoesnotchange,butitisstilluseful,
since the numberofsolutionsdecreases,i.e. insteadof Our analysis shows that Eq. (39) is smaller than both
two solutions in (22, 23) we have only one. (38) and (30). Thus, one can turn to the situation
Active Inference for Binary Symmetric HMMs
described by (39), only if some budget is left after su- where, as above, ⇑,⇓ indicate on the supervised spin.
pervising 3-domains; cf. the discussion after (50). We have [cf. (28)]
5.3 Frustrated 7-domains ∆O =−2 z p(−1,z |τ). (48)
4 4
X
z4
In this case there are four MAP sequences defined as
We employ [cf. (29)]
follows:
p(z ,z |τ)∝e−Jz3+Jz3z4−Jz4+h(z3+z4) (49)
zˆ=(+ ↑−↑−↓−⇑−↓−↑−↓−↑ +), (40) 3 4
z˜=(+ ↑−↓−↑−⇓−↑−↑−↓−↑ +), (41) and get for (48)
zˆ=(+ ↑−↓−↑−↓−⇑−↓−↑−↑ +), (42)
2(e2(2J−h)−1)
z˜=(+ ↑−↓−↑−↑−⇓−↑−↓−↑ +). (43) ∆O = . (50)
2+e2h+e4J−2h
Two most efficient active inference schemes are real-
ized by transitions from (40) to (41) and from (42) This is smaller than both (30) and (34): it is less effi-
to (43). They are again related to each other by the cient to supervise a spin inside a (two-spin) wall than
mirror symmetry, hence their overlap gains are equal inside a (two-spin) domain. In the regime (19), (50)
and is calculated from (29). Furthermore, tedious can be both smaller or larger than (39) depending on
but straightforwardcalculationsdemonstrate that the concrete values of h and J.
overlap gain for those scheme is larger than the one
given by (38), the best overlap gain for a 5-domain. 5.6 Summary
We also note that the most efficient supervisionis ab-
sent if one starts from (41) or from (43). To summarize this section, we converge to the follow-
ing picture of active estimation: given the unsuper-
Similarto5-domain,itispossiblethataspindoesnot
visedMAP-sequence,onefixesthewallsandfactorizes
flip after semi-supervising (e.g., it was recovered cor-
thesequenceoverfrustrateddomainsofodd-numbered
rectlyintheoriginalMAPinference). Inthiscase,one
bonds. Starting from the longest domain (as it corre-
shouldconsidersupervisingotherspinsinthedomain.
sponds to the largest overlap gain) one supervises the
The analysis of possible strategies can be performed
spin (or spins) in each domain that yield maximum
as explained in Section 5.2.
expected overlap gain, as described above. Thereby,
a unique supervised MAP-sequence is gained. Once
5.4 Non-frustrated domains all the domains are supervisedin this way, one should
spend the remaining “budget” for supervising spins
A simple analysis suggests that it is meaningless to
inside the walls.
supervise a spin inside a non-frustrated domain (even
number of negative bonds surrounded by two walls).
6 Comparison with numerical results
Indeed, consider
ˆz=(+ ↑−↓−⇑−↓−↑ +), (44)
Wenowreportonourexperimentsaimedatvalidating
˜z=(+ ↑−↑−⇓−↑−↑ +), (45) our analytical predictions. We will focus on the non-
trivialregime of sufficiently high noise intensities (ǫ>
where the third spin is supervised. Calculating the ǫ1), for which there are exponentially many (in the
c
overlapgainfollowingtotheabovemethod,weconfirm
numberofobservations)MAPsolutionscorresponding
that thesis gain is zero.
to a given observation sequence [15]. Recalling our
discussionofthefrustrateddomains,itisclearthatall
5.5 Supervising inside a wall
thosesolutionscorrespondtopermutedconfigurations
of the frustrated spins.
It is possible that even after supervising all the frus-
trateddomains,onestillhasbudgetforsupervisingad- In our experiments, we generate random HMM se-
ditional spins. Hence, we study supervising of a spin quences, then runthe Viterbi algorithmto find one of
inside a wall. Consider the thinnest wall (two spins the MAP sequences. We then apply the active infer-
joined by a positive bond) in the regime (19), and as- ence strategies described above, and compare the nu-
sumethatitissurroundedbylargerwalls(atleasttwo mericalfindingswiththeanalyticpredictions. Forsim-
positive bonds). plicity, here we focus on our prediction for 3-domains;
see Section 5.1. Recall that our analytical results are
ˆz=(+↑+ ↑ − ⇑ + ↑ − ↑ +↑+), (46)
obtained under the impenetrable wall approximation
˜z=(+↑+ ↑ − ⇓ + ↓ − ↑ +↑+), (47) (29), which requires that the constant J is not very
Armen Allahverdyan, Aram Galstyan
small; cf. (10). In the experiments below we set 7 Conclusion
J =1.5.
7.1 Discussion of the main results
First, we study the statistics of different domains as
a function of noise. Figure 1(a) shows the fraction
We have developed an analytical approach to study
f of spins that belong to frustrated domains of size
i active inference problem in binary symmetric HMMs.
i, for i = 3,5,7,9. We observe that the overall frac-
We obtained a closed-form expressionthat relates the
tion of spins inside those domains is rather small. For
expected accuracy gain to model parameters within
instance, for ǫ = 0.1, only 2% of spins belong to 3-
the impenetrabledomainapproximation,specifiedthe
domains, and less then 0.01% belong to 9-domains.
validityrangeoftheanalysis,andtestedouranalytical
Thus, for the parameter range considered here, any
predictions with numerical simulations. Based on the
gaininactiveinferencewillbe mostlydue to 3,5,and
analyticalinsights,wesuggestanoptimalactiveinfer-
7-domains.
ence strategies for BS-HMM, that can be summarized
Next,wecompareouranalyticalpredictionwithsimu- as follows: Given an observation sequence y and a
lationresultsfortwoactiveinferencestrategiesapplied corresponding(unsupervised)MAP-sequencexˆ(y),we
to 3-domains. For the simulations, we generatedsuffi- first find all the frustrated domains composed of odd-
ciently long sequences (105), identified all 3-domains, numbered bonds. Then, starting from the longer do-
andappliedtwoactiveinferencestrategiesasdescribed mains,wesuperviseoneofthespinsinsidethedomain,
in Section 5.1. For the first strategy, we always su- accordingtotheoptimalitycriteriaoutlinein(30,38).
pervise one of two spins that is looking up (i.e., the Onlyaftersupervisingallthefrustrateddomains(and
inferred hidden state is aligned with the observation). subject to budget constraints), one can consider su-
The expect gain in overlap for this strategy is given pervising schemes described in (39) and (50).
by Eq. 30. And for the second strategy, we supervise
Note that while our focus was on batch active infer-
the spinthatis lookingdown(i.e., the inferredhidden
ence, our results are also applicable to the online set-
stateis misalignedwiththe observation). Inthis case,
tings (e.g., active filtering), owing to the separation
the expected overlapgain is given by Eq. 34.
oftheestimatedhiddensequenceintoseparateweakly
The results are shown in Figure 1(b), where we plot interacting domains.
theexpectedoverlapgainasafunctionofnoise,inthe
range ǫ 1 ≤ ǫ ≤ ǫ3; cf. (17). The agreement between 7.2 Relationship to existing heuristics for
c c
thepredictionandthesimulationsisnear-perfectwhen active inference
the noise intensity is in the range ǫ 1 ≤ ǫ ≤ ǫ2. In
c c
particular, the switching of the optimal supervising Here we discuss to which extent solutions studied
strategy, as predicted from Eqs. (30) and (34), is re- above relate to heuristic methods for active inference;
produced in the experiments. Interestingly, for one see Section 1. Below we always assume the regime of
of the “branches” (given by Eq. 34), the agreement parameters, where (29) applies.
remains near-perfect even for larger noise intensities.
Within the uncertainty reduction heuristics we super-
However, the agreement with the other branch (given
vise x if some suitable measure of uncertainty, e.g.
l
by Eq. 34) deteriorates with increasing noise. We re-
1− p2(x |y), maximizes at k = l. For the 3-
callthatthe analyticalpredictions areobtainedunder xk=±1 k
domPain (26, 27) both spins of the domain are equally
the impenetrable wall approximation, which assumes
uncertain [see (29)], but we saw that their supervis-
thatdomainsaresurroundedbysufficientlythickwalls.
ing leads to different results. Thus, the uncertainty
Whenincreasingnoise,theassumptionsstartstograd-
reduction does not correlatewith the optimalscheme.
uallybreakdown,andtheapproximationbecomesless
accurate. For the 5-domain (35–37) the third spin (and due to
the mirror symmetry the fourth one) is the most un-
Finally, Figure 1(c) compares inference errors for the
certain one. This correlates with the optimal scheme
original MAP, MAP with random supervising, and
(35-36),butdoesnotallowtorecoverthisscheme,e.g.
MAP with active inference. We see that the active
supervising the third spin in (37) is not optimal. The
inference strategyyields lowerinference erroroverthe
same correlation is seen for the 7-domain, where the
wholerangeofnoiseintensities. Infact,thisdifference
fourth spin is the most uncertain one; see (40–43).
becomes more pronounced for large noise intensities.
We also note that the slight kinks in the curves cor- Consider now another (related) heuristic principle for
responding to the phase transition between different active inference that tries to supervise spins in such a
domains [15]. way that it leads to a unique solution. This principle
does not apply 3-domain, since here the choice of ei-
ther spin leads to a unique solution. It partially holds
Active Inference for Binary Symmetric HMMs
0.02
0.01
0
0.05 0.1
sniamod−ddo
ni
snips
fo
noitcarF
f3
1
f5
f7
f9
0.8
0.6
ǫ
0.4
0.05 ǫ 0.1
(a)
O¯∆
0.04
ǫ2 c
0.02
0.05 ǫ 0.1
(b)
rorrE
MAP
Random Supervising
Active Inference
(c)
Figure 1: (a) Fraction of spins belonging to different domains plotted against noise; (b) The average overlap
gainplottedagainstnoise obtainedfromsimulations(open symbols)andanalyticalpredictions givenby Eqs.30
and 34 (solid lines); (c) Inference error for different methods plotted against noise. In all three figures we use
J =1.5. For (b) and (c), we used sequences of size 105, and averagedthe results over 1000 random realizations.
for the optimal solution of the 5-domain: if in (35) datessolutionshavingapproximatelyequallikelihoods
the supervised spin was found to be wrong, then we orenergies[25,26]. Inthisscenario,wecanintuitively
are left with only one possible configuration (36), e.g. define frustrated variables as those that change from
supervising the third spin in (35) does lead to unique one solution to another.
sequence: it can be either (36) or (37). However, if
Also,usingtheanalogywithstatisticalphysics,webe-
the supervised spin in (35) was found to be correct,
lieve that the active MAP inference via domains and
we are not left with a unique configuration [an ad-
wallscanbegeneralizedbeyondthe(one-dimensional)
ditional supervising is necessary to achieve a unique
HMM models, and it possibly applies to active recog-
configuration, but it is beneficial to supervise instead
nition of two-dimensional patterns. Now instead of
another3-domainof5-domain;seethediscussionafter
one-dimensionalIsingmodel,wegetatwo-dimensional
(39)]. Likewise,forthe7-domain,nocorrelationexists
random Ising spin system. Its T = 0 situation corre-
betweentheoptimalsupervisingandtheuniquenessof
sponds to the MAP recognition of patterns. We note
the resulting configuration: even if the optimally su-
that two-dimensional Ising-type models haven been
pervisedisfoundtobewrong,wedonotautomatically
used extensively in various pattern recognition tasks
appear in a unique configuration; see (40, 41).
such as computer vision [23].
Finally,thereareseveralinterestinganalogiesbetween
7.3 Open Problems
the optimal inference strategies uncovered here and
human heuristics in information processing [17, 24].
There are several interesting directions for extending Note that the optimal scheme in the regime (19)
thepresentedresults. First,herewefocusedonthein- amounts to supervising the up-spins, e.g., the ones
ference task, assuming that the model parameters are thatagreewithobservations. Thispointcanberelated
known. In a more realistic scenario, those parameters to falsifiability [17], where onelooksatcaseswhen the
((q,ǫ),or(J,h))areunknownandneedtobeestimated existing (prior) knowledge can be challenged. In con-
fromthe data,e.g., using Baum-Welchalgorithm[12]. trast, people often show confirmation bias, i.e. they
Whiledevelopingafullanalyticalsolutionforthejoint tend to confirm the existing knowledge rather than
active inference/learning problem might be challeng- questionit; see[24]forareview. Further development
ing even for the simple BS-HMM considered here, it ofthoseideasmayprovideinterestingconnectionswith
should be possible to extend the methods presented cognitive aspects of active inference strategies.
here for studying the robustness of optimal inference
strategies with respect to misspecified parameters.
References
Anotherinterestingandimportantquestionistowhat
extent the optimal active inference schemes analyzed [1] B. Settles. Active Learning Literature survey.
hereapplytomoregeneralclassofHMMs. Webelieve Technical Report 1648, University of Wisconsin–
thattheanswertothisquestioncanbeexaminedviaa Madison, 2009.
generalized definition of frustration. In the context of
[2] T. Scheffer, C. Decomain, and S. Wrobel. Ac-
MAP estimation, this implies looking at generalized tive Hidden Markov Models for Information Ex-
(k-best) Viterbi algorithm that return several candi- traction. In F. Hoffmann, D. Hand, N. Adams,
Armen Allahverdyan, Aram Galstyan
D. Fisher, and G. Guimaraes, editors, Advances [16] A. Halder and A. Adhikary, Statistical Physics
in Intelligent Data Analysis, volume 2189 of Lec- AnalysisofMaximumaPosteriori Estimationfor
ture Notes in Computer Science, pages 309–318. Multi-channel Hidden Markov Models, J. Stat.
SpringerBerlin Heidelberg, 2001. Phys.150, 744-775 (2013).
[3] B. Anderson and A. Moore, Active Learning for [17] M. Oaksford and N.Chater, A Rational Analysis
nd of the Selection Task as Optimal Data Selection,
Hidden Markov Models, 22 International Con-
Psychological Review, 101, 608-631 (1994).
ference on Machine Learning, Bonn, Germany,
2005.
[18] H.S. Seung, M Opper and H. Sompolinsky,Query
by Committee, Computational Learning Theory
[4] X.Zhu,J.Lafferty,andZ.Ghahramani. Combin-
(pp.287294) (1992).
ing Active Learning and Semi-Supervised Learn-
ing Using Gaussian Fields and Harmonic Func-
[19] D. Lewis and W. Gale, A Sequential Algorithm
tions. In ICML 2003 workshop on The Contin-
for Training Text Classifiers, in Proceedings of
uum from Labeled to Unlabeled Data in Machine
the ACM SIGIR Conference on Research and
Learning and Data Mining,pages 58–65, 2003.
Development in Information Retrieval, (pp. 312)
ACM/Springer (1994).
[5] M. Bilgic and L. Getoor. Reflect and correct:
A misclassification prediction approach to active [20] N.RoyandA.McCallum,TowardOptimalActive
inference. ACM Trans. Knowl. Discov. Data, Learning Through Sampling Estimation of Error
3(4):20:1–20:32, Dec. 2009. Reduction,Proc.18thInternationalConf.onMa-
chineLearning(pp.441–448).MorganKaufmann,
[6] M. Bilgic, L. Mihalkova, and L. Getoor. Ac- San Francisco (2001).
tive Learning for Networked Data. InProceedings
of the 27th International Conference on Machine [21] T. Scheffer, C. Decomain, and S. Wrobel, Ac-
Learning (ICML-10), 2010. tive Hidden Markov Models for Information Ex-
traction, inProceedingsoftheInternationalCon-
[7] C. Moore, X. Yan, Y. Zhu, J.-B. Rouquier, and ference on Advances in Intelligent Data Analysis
T. Lane. Active Learning for Node Classifica- (CAIDA,pp. 309–318). Springer-Verlag, 2001.
tion in Assortative and Disassortative Networks.
In Proceedings of the 17th ACM SIGKDD Inter- [22] L.D. Landau and E.M. Lifshitz, Statistical
national Conference on Knowledge Discovery and Physics, I (Pergamon Press Oxford, 1978).
DataMining,KDD’11,pages841–849,NewYork,
[23] S. Geman and D. Geman, Stochastic Relaxation,
NY,USA,2011. ACM.
GibbsDistributions,andtheBayesianRestoration
ofImages,IEEETrans.PatternAnalysisandMa-
[8] A.KrauseandC.Guestrin.Near-optimalNonmy-
chineIntelligence, 6, 721 (1984).
opicValueofInformationinGraphicalModels. In
21st International Conference on Uncertainty in
[24] A.E.AllahverdyanandA.Galstyan,OpinionDy-
Artificial Intelligence (UAI), page 5, 2005.
namics with Confirmation Bias,PLoS ONE9(7),
e99557 (2014).
[9] A.KrauseandC.Guestrin. Optimal Value of In-
formation in Graphical Models. Journal of Artifi- [25] L.A. Foreman, Generalisation of the Viterbi Al-
cial Intelligence Research, 35:557–591, 2009. gorithm, IMA J. Management Math. 4, 351-367
(1992).
[10] T. M. Cover and J. A. Thomas, Elements of In-
formation Theory, (Wiley,New York,1991). [26] D. Epstein, Finding the k Shortest Paths. In:
Proc. 35th Symp. Foundations of Computer Sci-
[11] Y.Ephraim and N. Merhav, Hidden Markov pro- ence, pp.154165. IEEE, Los Alamitos (1994)
cesses, IEEE Trans. Inf. Th., 48, 1518-1569,
(2002).
[12] L.R.Rabiner,ATutorialonHiddenMarkovMod-
els and Selected Applications in Speech Recogni-
tion, Proc. IEEE, 77, 257-286, (1989).
[13] O.Zuk,I.KanterandE.Domany,TheEntropyof
a Binary Hidden Markov Process, J. Stat. Phys.
121, 343 (2005).
[14] K.J. Williams, Ground State Properties of Frus-
trated Ising Chains, J. Phys. C, 14, 4095-4107
(1981).
[15] A.E.AllahverdyanandA.Galstyan,OntheMax-
imum A Posteriori Estimation of Hidden Markov
Processes, Uncertainty in Artificial Intelligence,
2009.