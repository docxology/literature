arXiv:1411.0630v1  [stat.ML]  3 Nov 2014
Active Inference for Binary Symmetric HMMs
Armen Allahverdyan Aram Galstyan
Yerevan Physics Institute
Yerevan, Armenia
USC Information Sciences Institute
Marina del Rey, CA, USA
Abstract
We consider active maximum a posteriori
(MAP) inference problem for Hidden Markov
Models (HMM), where, given an initial MAP
estimate of the hidden sequence, we select
to label certain states in the sequence to
improve the estimation accuracy of the re-
maining states. We develop an analytical ap-
proach to this problem for the case of binary
symmetric HMMs, and obtain a closed form
solution that relates the expected error re-
duction to model parameters under the speci-
ﬁed active inference scheme. We then use this
solution to determine most optimal active in-
ference scheme in terms of error reduction,
and examine the relation of those schemes to
heuristic principles of uncertainty reduction
and solution unicity.
1 Introduction
In a typical statistical inference problem, we want to
infer the true state of some hidden variables based on
the observations of some other related variables. The
quality of statistical inference is generally improved
with adding more data or prior information about the
hidden variables. When obtaining such information
is costly, it is desirable to select this information opti-
mally so that it is most beneﬁcial to the inference task.
This notion of active inference, or active learning [1],
where one optimizes not only over the models, but also
over the data, has been used in various inference prob-
lems including Hidden Markov Models (HMMs) [3, 2],
network inference [4, 5, 6, 7], etc.
The eﬃciency of active inference is naturally deter-
mined from the error reduction of the inference method
Copyright 2014 by the authors.
[20]. Since this quantity is not readily available in
practice, several heuristic methods were developed [1].
For instance, a class of methods seeks to implement
active inference in a way of obtaining possibly unique
solution (or reducing the set of available solutions)
[1, 18]. A related familiy of methods tries to learn
those variables whose uncertainty is large [1, 19]. Both
heuristics have intuitive rationale and can do well in
practice, but theoretical understanding of how exactly
those heuristics relate to error reduction is largely lack-
ing. In particular, most existing theoretical results are
concerned with sub-modular cost functions that allow
to establish certain optimality guarantees [8, 9] on ac-
tive selection strategies. However, these cost functions
do not usually refer directly to the error reduction of
some inference method.
Here we consider the active maximum a posteriori
(MAP) inference problem for the binary symmetric
HMMs, or BS-HMM (see [3] for a brief review of ac-
tive estimation methods in HMM). BS-HMM is suﬃ-
ciently simple so it allows to study active MAP infer-
ence analytically [15, 16], yet it still contains all the
essential features of HMMs that make them a ﬂexible
and widely used tool for probabilistic modeling in var-
ious ﬁelds [12, 11]. We emphasize that even for the
simple BS-HMM model considered here, obtaining an-
alytical insights about active inference strategies is a
non-trivial problem, as we demonstrate below.
We develop an analytical approach for studying the ac-
tive MAP inference problem for BS-HMM, and use it
to determine the most eﬃcient schemes of uncertainty
reduction, where labeling a small number of states re-
sults in the largest expected error reduction in the esti-
mation of the remaining (unlabeled) states. Our anal-
ysis allows us examine how the active estimation in
BS-HMM relates to the heuristic principles of uncer-
tainty reduction and solution unicity. Speciﬁcally, we
obtain a closed form expression for the expected error
reduction as a function of model parameters, and use
it to assess the optimality of active inference schemes.
Finally, we compare our analytical predictions with
numerical simulations and obtain excellent agreement
Active Inference for Binary Symmetric HMMs
within the validity range of the proposed analysis.
The rest of the paper is organized as follows: Sec-
tion 2 deﬁnes generalities of active MAP-inference.
Sections 3 and 4 introduce the BS-HMM and its MAP
analysis, respectively. Section 5 presents our the main
analytical ﬁndings on active inference, and is followed
by experimental validation of those ﬁndings in Sec-
tion 6. We conclude by discussing our main ﬁndings,
their relationships to the existing heuristics for active
inference, and identifying some future directions of re-
search in Section 7.
2 Active inference: generalities
2.1 MAP inference
Let x = ( x1, . . . , x N ) and y = ( y1, . . . , y N ) be realiza-
tions of discrete-time random processes X and Y, re-
spectively. Y is the noisy observation of X . We assume
a binary alphabet xi = ±1, yi = ±1 and write the
probabilities as p(x) and p(y). The inﬂuence of noise is
described by the conditional probability p(y|x). Given
an observation sequence y, the MAP estimate ˆx(y) =
(ˆx1(y), ..., ˆxN (y)) of x is found from maximizing over
x the posterior probability p(x|y) = p(y|x)p(x)/p(y):
ˆx(y) = arg max
x
p(x|y). (1)
In general, the minimization can produce N (y) out-
comes. Since they are equivalent, each of them is cho-
sen (for a given y) with probability 1 /N (y), which
deﬁnes a realization ˆ xof a random variable ˆX .
The estimation accuracy is measured using the overlap
(larger O means better estimate)
¯O =
∑
y
p(y) 1
N (y)
∑
ˆ x(y)
O[ˆ x(y), y], (2)
O[ˆ x(y), y] =
∑
x
N∑
i=1
ˆxi(y)xi p(x|y), (3)
where three averages (over X , Y and ˆX ) are involved
in ¯O, and where the error probability reads Pe = 1
2 [1−
¯O
N ]. Below we are interested by the conditional—over
a ﬁxed y and ˆ x—overlap O[ˆ x(y), y].
2.2 Active MAP inference
Given y and ˆx(y) we request the true value of one of
the variables (say x1, generalization to many variables
is straightforward). Below we refer to this procedure
as supervising. After supervising x1, we re-estimate
other variables:
(ˆx2(y), ..., ˆxN (y)) → (˜x2(x1, y), ..., ˜xN (x1, y)), (4)
where ˜xi(x1, y) is obtained under the MAP estimation
with the ﬁxed value of x1. Let us deﬁne the overlap
gain
∆ O[ˆ x(y), y; 1] =
∑
x
N∑
i=2
[˜xi(x1, y) − ˆxi(y)]xi p(x|y), (5)
Then supervising x1 is meaningful if this gain is posi-
tive, ∆ O > 0. Note that the sum ∑ N
i=2 is taken over
the non-supervised variables [cf. (2)]. In ∆ O[ˆ x(y), y]
the average is taken also over all possible values x =
(x1, ..., xN ). The fully averaged overlap gain ∆ ¯O is
determined from ∆ O[ˆ x(y), y; 1] as in (2).
If x1 was already correctly recovered using the initial
MAP estimation, then supervising x1 does not change
anything: {˜xi(ˆx1(y), y)}N
i=2 = {ˆxi(y)}N
i=2. Hence, we
get from (5):
∆ O[ˆ x(y), y; 1] =
∑
x2,...,x N
N∑
i=2
[˜xi(−ˆx1(y), y) − ˆxi(y)]
×xi p(−ˆx1(y), x2, ..., xN |y). (6)
3 Reducing HMMs to the Ising model
We now consider a binary symmetric HMM. X is a
Markov process given by the following state-transition
probabilities
p(x) =
∏ N
k=2
p(xk|xk− 1)p(x1), x k = ±1, (7)
where p(xx|xk− 1) is the transition probability param-
eterized by a single number 0 < q < 1:
p(+1| + 1) = p(−1| − 1) = 1 − q,
p(+1| − 1) = p(−1| + 1) = q.
The noise is memory-less and unbiased:
p(y|x) =
∏ N
k=1
π(yk|xk), y k = ±1 (8)
where
π(−1| + 1) = π(+1| − 1) = ǫ,
π(+1| + 1) = π(−1| − 1) = 1 − ǫ,
and ǫ is the probability of error. Here memory-less
refers to the factorization in (8), while unbiased means
that the noise acts symmetrically on both realizations
of the Markov process: π(1| − 1) = π(−1|1).
Recall that the composite process X Y is Markovina,
but Y is generally not a Markovian process [11, 12].
Armen Allahverdyan, Aram Galstyan
To proceed further, we use the following parametriza-
tion of the HMM [13]:
p(xk|xk− 1) = eJxkxk− 1
2 cosh J , J = 1
2 ln
[ 1 − q
q
]
, (9)
π(yi|xi) = ehyixi
2 cosh h, h = 1
2 ln
[ 1 − ǫ
ǫ
]
, (10)
MAP estimation is then equivalent to minimizing the
energy function (Hamiltonian) deﬁned as H(y, x) ≡
− ln[ p(y|x)p(x) ] [13]:
H(y, x) = −J
∑ N
k=1
xkxk+1 − h
∑ N
k=1
ykxk, (11)
where we have omitted irrelevant factors that are ei-
ther additive or vanish for N ≫ 1.
We ﬁnd it convenient to introduce the following gauge
transformation:
zi = xiyi, τ i = yiyi+1. (12)
For a given y, the role of observations will be played
by τ = {τi}, while the original hidden variables now
correspond to z = {zi}. Hence, (11) reads
H(τ , z) = −J
∑ N
k=1
τkzkzk+1 − h
∑ N
k=1
zk,(13)
p(τ , z) ∝ exp[−H(τ , z)]. (14)
We shall refer to τk = ±1 as bond (plus or minus), and
to zk = ± as spin (up and down). Eq. (13) describes
the Hamiltonian of a one-dimensional Ising model with
random bonds τk in an external ﬁeld h [13]. The MAP
inference reduces to minimizing this Hamiltonian for
a given τ , i.e. to taking the zero-temperature limit
T → 0 [22]. Note also that Eq. (14) is the Gibbs
distribution at temperature T = 1 [22].
In the new variables, the overlap gain given by Eq. (6)
reads:
∆ O[ˆ z, τ ; 1] =
∑
z2,...,z N
N∑
i=2
[˜zi(−ˆz1(τ ), τ ) − ˆzi(τ )]
zi p(−ˆz1(τ ), z2, ..., zN |τ ). (15)
Below we suppress the arguments of ∆ O, if they are
clear from the context.
4 Domain structure of MAP
The MAP estimation for BS-HMMs is characterized
by (countably inﬁnite) numbers of operational regimes
that are related to each other via ﬁrst-order phase
transitions [15]. Those transitions occur at the pa-
rameter values
hm
c = 2 J/m, (16)
or alternatively, at the critical noise intensities [15]
ǫm
c = 1 /[1 + ǫ4J/m ] , m = 1 , 2, .. (17)
Furthermore, for any noise intensity above the critical
value ǫ > ǫ 1
c, the MAP solution is macroscopically de-
generate: For any observation sequence of length N,
there are exponentially many (in N) sequences that
simultaneously maximize the MAP objective [15].
Let us focus on the structure of the MAP solution. A
straightforward analysis (ﬁrst carried out in [14] for
the one-dimensional Ising model) shows that the de-
pendence of ˆz(τ ) on τ is local: τ is factorized into non-
overlapping domains τ = ( τ 1, w1, τ 2, w2, ...), where
wk are the walls separating domains. Walls are se-
quences of two or more up spins joined by positive
bonds; see below. The estimate sequence ˆz admits
similar factorization ˆz = ( ˆz1, w1, ˆz2, w2, ...), such that
ˆzk has the same length as τ k, and is determined solely
from τ k (non-uniquely in general).
We now proceed to describe the domain structure for
those diﬀerent regimes as characterized by h/J. With-
out loss of generality we assume J > 0, h > 0.
4.1 The maximum likelihood regime
For suﬃciently weak noise [cf. (10)]
h > 2J, (18)
the MAP estimate copies observations ˆx = y. Hence,
ˆzi = 1 for all i; see (13). The prior probability p(x) is
irrelevant in this regime: on can take J = 0, i.e. the
MAP estimation reduces to the maximum-likelihood
estimation, where the noise probability p(y|x) is max-
imized for a ﬁxed y; see (8).
The overlap gain (15) nulliﬁes for (18), because super-
vising a spin does not change other spins: ˆ zi = ˜zi.
4.2 First non-trivial regime
We now focus on the following regime:
J < h < 2J, (19)
Here 2 J > h ensures that there are ˆ z-spins equal to
−1 [otherwise we are back to (18)], while h > J means
that two spins joined by a positive bound are both
equal to 1. To verify it consider
↑ − ↑ + ↑ − ↑ , (20)
↑ − ↓ + ↓ − ↑ , (21)
where zi = 1 and zi = −1 are represented as ↑ and ↓,
respectively, while ± refer to τi = ±.
Active Inference for Binary Symmetric HMMs
The energy of (20) is J − 4h which is smaller than the
energy −3J of (21) precisely when h > J . Hence, the
minimal size of the wall in the regime (19) is just two
(+1) spins joined by a positive bond.
In the regime (19) there are the following domains [14].
• A frustrated 2 n + 1-domains consists of an odd
(and larger than one) number 2 n + 1 of conse-
quitive minus bonds (i.e. bonds with τk = −1)
that are bound from both ends by positive bonds.
Frustrated means the correspondence between
{τk} and {ˆzk} is not unique, e.g. the following
two conﬁgurations have the same energy
+ ↑ − ↑ − ↓ − ↑ +, (22)
+ ↑ − ↓ − ↑ − ↑ + . (23)
Both (22) and (23) have the same values of
{τk}5
k=1. However, the sequences {ˆzk}4
k=1 are dif-
ferent, though they have exactly the same energy.
More generally, a frustrated 2 n + 1-domain sup-
ports n + 1 equivalent sequences [14]; see also be-
low. This non-uniqueness is reﬂected via a zero-
temperature entropy [14, 15].
• Non-frustrated domains consist of an even num-
ber of consecutive minus bonds bound from both
ends by positive bonds. In this case, the corre-
spondence between {τk} and {ˆzk} is unique, e.g.
(+ ↑ − ↓ − ↑ +).
It is worthwhile to note that the notion of frustra-
tion described above is inherently related to the ex-
ponential degeneracy (in the number of observations
N) of the MAP solution above the ﬁrst critical noise
intensity [15]. Indeed, consider, for instance, the frus-
trated 3-domain shown in 20 and 21. In any observa-
tion sequence y of length N, the expected number of
such frustrated 3-domains nd scales linearly with N,
nd = αN, where α is a (possibly small) constant. And
since each such domain multiplies the number of MAP
solution by two (i.e., simultaneously ﬂipping both frus-
trated spins is still a solution), there overall number
of solutions will scale as ∝ 2αN , thus indicating expo-
nential degeneracy.
4.3 Further regimes
Now lets us focus on the third regime deﬁned by
2J/3 < h < J, (24)
In this regime, the walls are formed by two (or more)
up-spins joined together by two (or more) positive
bonds. Thus 2 J/3 < h is precisely the condition why
three spins joined by two positive bonds always look
up, while h < J means that two spins joined by one
positive bonds can both look down.
• The old domains stay intact if they are separated
(from both ends) by (at least) three up-looking
spins joined by (at least) two positive bonds.
• New domains are formed by pair(s) of spins joined
by a positive bond, which are immersed into a set
of negative bonds. In the new domains only the
pairs of spins joined by positive bond can be frus-
trated. The frustration rule follows the above pat-
tern, where now the pairs (super-spin) play the
role of single spins in the previous regime (19),
while the odd (even) number of negative spins
built up into one negative (positive) superbond
[14]. Here is an example of this situation
+ ↑ + ↑
 
↑
− ↓ − ↑ −

 
−
↓ + ↓  
↓
−
−
↑ + ↑ +  
↑
, (25)
where the super-spins and super-bonds are shown
in the second line. This domain is not frustrated.
Note that in the regime (19), (25) breaks down
into two separate domains, the ﬁrst of which is
frustrated.
The domain structure for h < 2J/3 (and smaller val-
ues of h) is constructed recursively following the above
pattern [14].
5 Active estimation of basic domains
We implement active estimation for separate domains,
i.e. in each domain we supervise one spin z1. Ad-
vantages of this implementation are as follows: (i)
properly choosing already one spin (and ﬁnding out
that it is incorrect) allows to get rid of the non-
uniqueness of the MAP estimate; (ii) the re-estimation
ˆz(τ ) → ˜z(x1, τ ) is restricted to one domain only; (iii)
(15) can be calculated approximately.
5.1 Frustrated 3-domain
In the domain (22) we supervise the second spin. If it
is opposite to the MAP estimate, then we move to the
conﬁguration (23) that has the same energy:
ˆ z(τ ) = (+ ↑ − ⇑ − ↓ − ↑ +), (26)
˜ z(τ ) = (+ ↑ − ⇓ − ↑ − ↑ +), (27)
where ⇑ shows the supervised spin. Hence, the active
estimation removes the non-uniqueness of the MAP
solution. The overlap gain is calculated from (15):
∆ O = 2
∑
z3
z3p(−1, z3|τ ), (28)
Armen Allahverdyan, Aram Galstyan
where z3 is the third spin in (26, 27); the one that
changes after the re-estimation. The marginal proba-
bility p(−1, z3|τ ) in (28) cannnot be determined in a
closed analytic form [15, 16].
We calculate p(z2, z3|τ ) in a impenetrable wall approxi-
mation: when J and h are suﬃciently large one can ne-
glect ﬂuctuations of the spins within the walls separat-
ing domains as compared to spins located within the
domains. Hence, we write for n-domain ( n = 3 , 5, 7, ...
and normalization is omitted)
p(z2, ..., zn|τ ) ∝ e− J(z2+zn)− J ∑ n− 1
k=2 zkzk+1+h ∑ n
k=2 zk .
(29)
We checked numerically that even for thinnest two-
spin walls the approximation works well already for
J ≃ 1; see below. Using (29) with n = 3 in (28) we
get
∆ O = 2(1 − e− 2h)
2 + e− 2h + e− 4J+2h . (30)
Likewise, if the originally estimate sequence is given by
(31), then upon supervising the second spin we obtain
ˆz(τ ) = (+ ↑ − ⇓ − ↑ − ↑ +), (31)
˜z(τ ) = (+ ↑ − ⇑ − ↓ − ↑ +). (32)
Instead of (28), we need to use
∆ O = −2
∑
z3
z3p(1, z3|τ ), (33)
which yields
∆ O = 2(1 − e− 2(2J− h))
2 + e− 2h + e− 4J+2h . (34)
Remarkably, (34) and (30) are diﬀerent. This seems
surprising, since given the apparent symmetry between
(26, 27) and (31, 32), one would expect that the over-
lap gain will be the same in both cases. This is not
the case, because (33) with (28) are deﬁned quite dif-
ferently, e.g. (33) involves p(1, z3|τ ), while (28) has
p(−1, z3|τ ).
Both (30) and (34) are positive in the regime (19),
but (30) >(34): when the supervised (initially) agrees
with the observations, the gain of active estimation is
larger.
For h < J we can employ the same two domains (26,
27) and (31, 32) provided that they are surrounded
by suﬃciently thick walls; see Section 4.3. Now the
opposite relation holds: (30) <(34).
Note ﬁnally that whenever the supervising shows that
the true value of the spin was already recognized cor-
rectly, the overlap does not change, but it is still useful,
since the number of solutions decreases, i.e. instead of
two solutions in (22, 23) we have only one.
5.2 Frustrated 5-domain
There are 3 MAP sequences for the 5-domain
ˆz = (+ ↑ − ↑ − ↓ − ⇑ − ↓ − ↑ +), (35)
˜z = (+ ↑ − ↓ − ↑ − ⇓ − ↑ − ↑ +), (36)
(+ ↑ − ↓ − ↑ − ↑ − ↓ − ↑ +). (37)
The following scenarios are the most eﬃcient ones, i.e.
they lead to the largest ∆ O (under (19)): (i) going
from (35) to (36) by supervising the fourth spin (de-
noted by ⇑ in (35)). (ii) going from (36) to (35) by
supervising the third spin. Both these cases have the
same overlap gain (38) due to the mirror symmetry
with respect to the center of the domain that is shared
also by (29).
The optimal overlap gain is calculated from (29)
∆ O = 2
∑
z2,z 3,z 5
(z3 + z5 − z2)p(z2, z3, −1, z5|τ ) =
2e4J (
e2h ((
3e2h + 1
)
e4J − 2e2h + e4h − 2
)
− 1
)
(3e2h + 2) e2h+8J + (2e2h + 3e4h + 4e6h + 1) e4J + e8h .
(38)
It can be shown that (38) is larger than (30), which is
the best overlap gain for a 3-domain. In (37) there are
no spins whose supervision can lead to the maximal
overlap gain.
For the 5-domain we meet a situation that was absent
in the previous 3-domain case. While (38) calculates
the average overlap gain for supervising the fourth spin
in (35), it is possible that this spin was recovered by
the MAP correctly, i.e. it does not ﬂip after supervis-
ing. Then we are left with no overlap gain and also
with uncertainty between (35) and (37), since they
both have the correct fourth spin. We can now su-
pervise an additional spin, so as to recover the unique
original sequence. It is seen from (35, 37) that when
starting from (35) there are 2 possibilities: to super-
vise the second spin or the third one. Additional 2
possibilities exist when supervising from (37). Out of
these 4 possibilities the largest overlap gain is achieved
for supervising the second spin in (35):
∆ O = 2
∑
z3,z 5
z3p(−1, z3, 1, z5|τ ) = (39)
8e4h+6J sinh(h) cosh(h − 2J)
(3e2h + 2) e2h+8J + (2e2h + 3e4h + 4e6h + 1) e4J + e8h ,
where p(−1, z3, 1, z5|τ ) shows that the fourth spin is
ﬁxed to its correct MAP value 1. The same ∆ O as
in (39) is reached when supervising the third spin in
(37). The remaining two possibilities are inferior.
Our analysis shows that Eq. (39) is smaller than both
(38) and (30). Thus, one can turn to the situation
Active Inference for Binary Symmetric HMMs
described by (39), only if some budget is left after su-
pervising 3-domains; cf. the discussion after (50).
5.3 Frustrated 7-domains
In this case there are four MAP sequences deﬁned as
follows:
ˆz = (+ ↑ − ↑ − ↓ − ⇑ − ↓ − ↑ − ↓ − ↑ +), (40)
˜z = (+ ↑ − ↓ − ↑ − ⇓ − ↑ − ↑ − ↓ − ↑ +), (41)
ˆz = (+ ↑ − ↓ − ↑ − ↓ − ⇑ − ↓ − ↑ − ↑ +), (42)
˜z = (+ ↑ − ↓ − ↑ − ↑ − ⇓ − ↑ − ↓ − ↑ +). (43)
Two most eﬃcient active inference schemes are real-
ized by transitions from (40) to (41) and from (42)
to (43). They are again related to each other by the
mirror symmetry, hence their overlap gains are equal
and is calculated from (29). Furthermore, tedious
but straightforward calculations demonstrate that the
overlap gain for those scheme is larger than the one
given by (38), the best overlap gain for a 5-domain.
We also note that the most eﬃcient supervision is ab-
sent if one starts from (41) or from (43).
Similar to 5-domain, it is possible that a spin does not
ﬂip after semi-supervising (e.g., it was recovered cor-
rectly in the original MAP inference). In this case, one
should consider supervising other spins in the domain.
The analysis of possible strategies can be performed
as explained in Section 5.2.
5.4 Non-frustrated domains
A simple analysis suggests that it is meaningless to
supervise a spin inside a non-frustrated domain (even
number of negative bonds surrounded by two walls).
Indeed, consider
ˆz = (+ ↑ − ↓ − ⇑ − ↓ − ↑ +), (44)
˜z = (+ ↑ − ↑ − ⇓ − ↑ − ↑ +), (45)
where the third spin is supervised. Calculating the
overlap gain following to the above method, we conﬁrm
that thesis gain is zero.
5.5 Supervising inside a wall
It is possible that even after supervising all the frus-
trated domains, one still has budget for supervising ad-
ditional spins. Hence, we study supervising of a spin
inside a wall. Consider the thinnest wall (two spins
joined by a positive bond) in the regime (19), and as-
sume that it is surrounded by larger walls (at least two
positive bonds).
ˆ z= (+ ↑ + ↑ − ⇑ + ↑ − ↑ + ↑ +), (46)
˜ z= (+ ↑ + ↑ − ⇓ + ↓ − ↑ + ↑ +), (47)
where, as above, ⇑, ⇓ indicate on the supervised spin.
We have [cf. (28)]
∆ O = −2
∑
z4
z4p(−1, z4|τ ). (48)
We employ [cf. (29)]
p(z3, z4|τ ) ∝ e− Jz3+Jz3z4− Jz4+h(z3+z4) (49)
and get for (48)
∆ O = 2(e2(2J− h) − 1)
2 + e2h + e4J− 2h . (50)
This is smaller than both (30) and (34): it is less eﬃ-
cient to supervise a spin inside a (two-spin) wall than
inside a (two-spin) domain. In the regime (19), (50)
can be both smaller or larger than (39) depending on
concrete values of h and J.
5.6 Summary
To summarize this section, we converge to the follow-
ing picture of active estimation: given the unsuper-
vised MAP-sequence, one ﬁxes the walls and factorizes
the sequence over frustrated domains of odd-numbered
bonds. Starting from the longest domain (as it corre-
sponds to the largest overlap gain) one supervises the
spin (or spins) in each domain that yield maximum
expected overlap gain, as described above. Thereby,
a unique supervised MAP-sequence is gained. Once
all the domains are supervised in this way, one should
spend the remaining “budget” for supervising spins
inside the walls.
6 Comparison with numerical results
We now report on our experiments aimed at validating
our analytical predictions. We will focus on the non-
trivial regime of suﬃciently high noise intensities ( ǫ >
ǫ1
c ), for which there are exponentially many (in the
number of observations) MAP solutions corresponding
to a given observation sequence [15]. Recalling our
discussion of the frustrated domains, it is clear that all
those solutions correspond to permuted conﬁgurations
of the frustrated spins.
In our experiments, we generate random HMM se-
quences, then run the Viterbi algorithm to ﬁnd one of
the MAP sequences. We then apply the active infer-
ence strategies described above, and compare the nu-
merical ﬁndings with the analytic predictions. For sim-
plicity, here we focus on our prediction for 3-domains;
see Section 5.1. Recall that our analytical results are
obtained under the impenetrable wall approximation
(29), which requires that the constant J is not very
Armen Allahverdyan, Aram Galstyan
small; cf. (10). In the experiments below we set
J = 1 .5.
First, we study the statistics of diﬀerent domains as
a function of noise. Figure 1(a) shows the fraction
fi of spins that belong to frustrated domains of size
i, for i = 3 , 5, 7, 9. We observe that the overall frac-
tion of spins inside those domains is rather small. For
instance, for ǫ = 0 .1, only 2% of spins belong to 3-
domains, and less then 0 .01% belong to 9-domains.
Thus, for the parameter range considered here, any
gain in active inference will be mostly due to 3, 5, and
7-domains.
Next, we compare our analytical prediction with simu-
lation results for two active inference strategies applied
to 3-domains. For the simulations, we generated suﬃ-
ciently long sequences (10 5), identiﬁed all 3-domains,
and applied two active inference strategies as described
in Section 5.1. For the ﬁrst strategy, we always su-
pervise one of two spins that is looking up (i.e., the
inferred hidden state is aligned with the observation).
The expect gain in overlap for this strategy is given
by Eq. 30. And for the second strategy, we supervise
the spin that is looking down (i.e., the inferred hidden
state is misaligned with the observation). In this case,
the expected overlap gain is given by Eq. 34.
The results are shown in Figure 1(b), where we plot
the expected overlap gain as a function of noise, in the
range ǫc1 ≤ ǫ ≤ ǫ3
c ; cf. (17). The agreement between
the prediction and the simulations is near-perfect when
the noise intensity is in the range ǫc1 ≤ ǫ ≤ ǫ2
c . In
particular, the switching of the optimal supervising
strategy, as predicted from Eqs. (30) and (34), is re-
produced in the experiments. Interestingly, for one
of the “branches” (given by Eq. 34), the agreement
remains near-perfect even for larger noise intensities.
However, the agreement with the other branch (given
by Eq. 34) deteriorates with increasing noise. We re-
call that the analytical predictions are obtained under
the impenetrable wall approximation, which assumes
that domains are surrounded by suﬃciently thick walls.
When increasing noise, the assumptions starts to grad-
ually break down, and the approximation becomes less
accurate.
Finally, Figure 1(c) compares inference errors for the
original MAP, MAP with random supervising, and
MAP with active inference. We see that the active
inference strategy yields lower inference error over the
whole range of noise intensities. In fact, this diﬀerence
becomes more pronounced for large noise intensities.
We also note that the slight kinks in the curves cor-
responding to the phase transition between diﬀerent
domains [15].
7 Conclusion
7.1 Discussion of the main results
We have developed an analytical approach to study
active inference problem in binary symmetric HMMs.
We obtained a closed-form expression that relates the
expected accuracy gain to model parameters within
the impenetrable domain approximation, speciﬁed the
validity range of the analysis, and tested our analytical
predictions with numerical simulations. Based on the
analytical insights, we suggest an optimal active infer-
ence strategies for BS-HMM, that can be summarized
as follows: Given an observation sequence y and a
corresponding (unsupervised) MAP-sequence ˆx(y), we
ﬁrst ﬁnd all the frustrated domains composed of odd-
numbered bonds. Then, starting from the longer do-
mains, we supervise one of the spins inside the domain,
according to the optimality criteria outline in (30, 38).
Only after supervising all the frustrated domains (and
subject to budget constraints), one can consider su-
pervising schemes described in (39) and (50).
Note that while our focus was on batch active infer-
ence, our results are also applicable to the online set-
tings (e.g., active ﬁltering), owing to the separation
of the estimated hidden sequence into separate weakly
interacting domains.
7.2 Relationship to existing heuristics for
active inference
Here we discuss to which extent solutions studied
above relate to heuristic methods for active inference;
see Section 1. Below we always assume the regime of
parameters, where (29) applies.
Within the uncertainty reduction heuristics we super-
vise xl if some suitable measure of uncertainty, e.g.
1 − ∑
xk=± 1 p2(xk|y), maximizes at k = l. For the 3-
domain (26, 27) both spins of the domain are equally
uncertain [see (29)], but we saw that their supervis-
ing leads to diﬀerent results. Thus, the uncertainty
reduction does not correlate with the optimal scheme.
For the 5-domain (35–37) the third spin (and due to
the mirror symmetry the fourth one) is the most un-
certain one. This correlates with the optimal scheme
(35-36), but does not allow to recover this scheme, e.g.
supervising the third spin in (37) is not optimal. The
same correlation is seen for the 7-domain, where the
fourth spin is the most uncertain one; see (40–43).
Consider now another (related) heuristic principle for
active inference that tries to supervise spins in such a
way that it leads to a unique solution. This principle
does not apply 3-domain, since here the choice of ei-
ther spin leads to a unique solution. It partially holds
Active Inference for Binary Symmetric HMMs
0.05 0.10
0.01
0.02Fraction of spins in odd−domains
ǫ
 
 
f3
f5
f7
f9
(a)
0.05 0.10.4
0.6
0.8
1
ǫ
∆ ¯O ǫ2
c
(b)
0.05 0.1
0.02
0.04
ǫ
Error
 
 
MAP
Random Supervising
Active Inference
(c)
Figure 1: (a) Fraction of spins belonging to diﬀerent domains plotted against noise; (b) The average overlap
gain plotted against noise obtained from simulations (open symbols) a nd analytical predictions given by Eqs. 30
and 34 (solid lines); (c) Inference error for diﬀerent methods plot ted against noise. In all three ﬁgures we use
J = 1 .5. For (b) and (c), we used sequences of size 10 5, and averaged the results over 1000 random realizations.
for the optimal solution of the 5-domain: if in (35)
the supervised spin was found to be wrong, then we
are left with only one possible conﬁguration (36), e.g.
supervising the third spin in (35) does lead to unique
sequence: it can be either (36) or (37). However, if
the supervised spin in (35) was found to be correct,
we are not left with a unique conﬁguration [an ad-
ditional supervising is necessary to achieve a unique
conﬁguration, but it is beneﬁcial to supervise instead
another 3-domain of 5-domain; see the discussion after
(39)]. Likewise, for the 7-domain, no correlation exists
between the optimal supervising and the uniqueness of
the resulting conﬁguration: even if the optimally su-
pervised is found to be wrong, we do not automatically
appear in a unique conﬁguration; see (40, 41).
7.3 Open Problems
There are several interesting directions for extending
the presented results. First, here we focused on the in-
ference task, assuming that the model parameters are
known. In a more realistic scenario, those parameters
((q, ǫ), or ( J, h)) are unknown and need to be estimated
from the data, e.g., using Baum-Welch algorithm [12].
While developing a full analytical solution for the joint
active inference/learning problem might be challeng-
ing even for the simple BS-HMM considered here, it
should be possible to extend the methods presented
here for studying the robustness of optimal inference
strategies with respect to misspeciﬁed parameters.
Another interesting and important question is to what
extent the optimal active inference schemes analyzed
here apply to more general class of HMMs. We believe
that the answer to this question can be examined via a
generalized deﬁnition of frustration. In the context of
MAP estimation, this implies looking at generalized
(k-best) Viterbi algorithm that return several candi-
dates solutions having approximately equal likelihoods
or energies [25, 26]. In this scenario , we can intuitively
deﬁne frustrated variables as those that change from
one solution to another.
Also, using the analogy with statistical physics, we be-
lieve that the active MAP inference via domains and
walls can be generalized beyond the (one-dimensional)
HMM models, and it possibly applies to active recog-
nition of two-dimensional patterns. Now instead of
one-dimensional Ising model, we get a two-dimensional
random Ising spin system. Its T = 0 situation corre-
sponds to the MAP recognition of patterns. We note
that two-dimensional Ising-type models haven been
used extensively in various pattern recognition tasks
such as computer vision [23].
Finally, there are several interesting analogies between
the optimal inference strategies uncovered here and
human heuristics in information processing [17, 24].
Note that the optimal scheme in the regime (19)
amounts to supervising the up-spins, e.g., the ones
that agree with observations. This point can be related
to falsiﬁability [17], where one looks at cases when the
existing (prior) knowledge can be challenged. In con-
trast, people often show conﬁrmation bias , i.e. they
tend to conﬁrm the existing knowledge rather than
question it; see [24] for a review. Further development
of those ideas may provide interesting connections with
cognitive aspects of active inference strategies.
References
[1] B. Settles. Active Learning Literature survey.
Technical Report 1648, University of Wisconsin–
Madison, 2009.
[2] T. Scheﬀer, C. Decomain, and S. Wrobel. Ac-
tive Hidden Markov Models for Information Ex-
traction. In F. Hoﬀmann, D. Hand, N. Adams,
Armen Allahverdyan, Aram Galstyan
D. Fisher, and G. Guimaraes, editors, Advances
in Intelligent Data Analysis , volume 2189 of Lec-
ture Notes in Computer Science , pages 309–318.
Springer Berlin Heidelberg, 2001.
[3] B. Anderson and A. Moore, Active Learning for
Hidden Markov Models, 22 nd International Con-
ference on Machine Learning, Bonn, Germany,
2005.
[4] X. Zhu, J. Laﬀerty, and Z. Ghahramani. Combin-
ing Active Learning and Semi-Supervised Learn-
ing Using Gaussian Fields and Harmonic Func-
tions. In ICML 2003 workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine
Learning and Data Mining , pages 58–65, 2003.
[5] M. Bilgic and L. Getoor. Reﬂect and correct:
A misclassiﬁcation prediction approach to active
inference. ACM Trans. Knowl. Discov. Data ,
3(4):20:1–20:32, Dec. 2009.
[6] M. Bilgic, L. Mihalkova, and L. Getoor. Ac-
tive Learning for Networked Data. In Proceedings
of the 27th International Conference on Machine
Learning (ICML-10) , 2010.
[7] C. Moore, X. Yan, Y. Zhu, J.-B. Rouquier, and
T. Lane. Active Learning for Node Classiﬁca-
tion in Assortative and Disassortative Networks.
In Proceedings of the 17th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining , KDD ’11, pages 841–849, New York,
NY, USA, 2011. ACM.
[8] A. Krause and C. Guestrin. Near-optimal Nonmy-
opic Value of Information in Graphical Models. In
21st International Conference on Uncertainty in
Artiﬁcial Intelligence (UAI) , page 5, 2005.
[9] A. Krause and C. Guestrin. Optimal Value of In-
formation in Graphical Models . Journal of Artiﬁ-
cial Intelligence Research , 35:557–591, 2009.
[10] T. M. Cover and J. A. Thomas, Elements of In-
formation Theory , (Wiley, New York, 1991).
[11] Y. Ephraim and N. Merhav, Hidden Markov pro-
cesses, IEEE Trans. Inf. Th., 48, 1518-1569,
(2002).
[12] L. R. Rabiner, A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recogni-
tion, Proc. IEEE, 77, 257-286, (1989).
[13] O. Zuk, I. Kanter and E. Domany, The Entropy of
a Binary Hidden Markov Process , J. Stat. Phys.
121, 343 (2005).
[14] K.J. Williams, Ground State Properties of Frus-
trated Ising Chains , J. Phys. C, 14, 4095-4107
(1981).
[15] A.E. Allahverdyan and A. Galstyan, On the Max-
imum A Posteriori Estimation of Hidden Markov
Processes, Uncertainty in Artiﬁcial Intelligence,
2009.
[16] A. Halder and A. Adhikary, Statistical Physics
Analysis of Maximum a Posteriori Estimation for
Multi-channel Hidden Markov Models , J. Stat.
Phys. 150, 744-775 (2013).
[17] M. Oaksford and N. Chater, A Rational Analysis
of the Selection Task as Optimal Data Selection ,
Psychological Review, 101, 608-631 (1994).
[18] H.S. Seung, M Opper and H. Sompolinsky, Query
by Committee , Computational Learning Theory
(pp. 287294) (1992).
[19] D. Lewis and W. Gale, A Sequential Algorithm
for Training Text Classiﬁers , in Proceedings of
the ACM SIGIR Conference on Research and
Development in Information Retrieval, (pp. 312)
ACM/Springer (1994).
[20] N. Roy and A. McCallum, Toward Optimal Active
Learning Through Sampling Estimation of Error
Reduction, Proc. 18th International Conf. on Ma-
chine Learning (pp. 441–448). Morgan Kaufmann,
San Francisco (2001).
[21] T. Scheﬀer, C. Decomain, and S. Wrobel, Ac-
tive Hidden Markov Models for Information Ex-
traction, in Proceedings of the International Con-
ference on Advances in Intelligent Data Analysis
(CAIDA, pp. 309–318). Springer-Verlag, 2001.
[22] L.D. Landau and E.M. Lifshitz, Statistical
Physics, I (Pergamon Press Oxford, 1978).
[23] S. Geman and D. Geman, Stochastic Relaxation,
Gibbs Distributions, and the Bayesian Restoration
of Images , IEEE Trans. Pattern Analysis and Ma-
chine Intelligence, 6, 721 (1984).
[24] A.E. Allahverdyan and A. Galstyan, Opinion Dy-
namics with Conﬁrmation Bias , PLoS ONE 9(7),
e99557 (2014).
[25] L.A. Foreman, Generalisation of the Viterbi Al-
gorithm, IMA J. Management Math. 4, 351-367
(1992).
[26] D. Epstein, Finding the k Shortest Paths . In:
Proc. 35th Symp. Foundations of Computer Sci-
ence, pp. 154165. IEEE, Los Alamitos (1994)