Scaling active inference
AlexanderTschantz1,2 ManuelBaltieri2,3 Anil. K.Seth1,4
tschantz.alec@gmail.com
ChristopherL.Buckley2
1SacklerCentreforConsciousnessScience,UniversityofSussex,Brighton,UK
2EvolutionaryandAdaptiveSystemsResearchGroup,UniversityofSussex,Brighton,UK
3RIKENCentreforBrainScience,Saitama,Japan
4CanadianInstituteforAdvancedResearch,UniversityofSussex,Brighton,UK
Abstract
In reinforcement learning (RL), agents often operate in partially observed and
uncertain environments. Model-based RL suggests that this is best achieved by
learningandexploitingaprobabilisticmodeloftheworld.‘Activeinference’isan
emergingnormativeframeworkincognitiveandcomputationalneurosciencethat
offers a unifying account of how biological agents achieve this. On this frame-
work,inference,learningandactionemergefromasingleimperativetomaximize
theBayesianevidenceforanichedmodeloftheworld.However,implementations
ofthisprocesshavethusfarbeenrestrictedtolow-dimensionalandidealizedsitu-
ations.Here,wepresentaworkingimplementationofactiveinferencethatapplies
tohigh-dimensionaltasks,withproof-of-principleresultsdemonstratingefficient
exploration and an order of magnitude increase in sample efficiency over strong
model-freebaselines.Ourresultsdemonstratethefeasibilityofapplyingactivein-
ferenceatscaleandhighlighttheoperationalhomologiesbetweenactiveinference
andcurrentmodel-basedapproachestoRL.
1 Introduction
In model-based reinforcement learning (RL), agents first learn a predictive model of the world,
beforeusingthismodeltodetermineactions[1]. Encodingamodeloftheworldplausiblyaffords
several advantages. For instance, such models can be used to perform perceptual inference [2],
implementprospectivecontrol[3,4], quantifyandresolveuncertainty[5], andgeneralizeexisting
knowledgetonewtasksandenvironments[6].Assuch,theuseofpredictivemodelshasbeentouted
asapotentialsolutiontothesampleinefficienciesofmodernRLalgorithms[7,8].
Atthesametime,thetheoreticalframeworkofactiveinferencehasemergedincognitiveandcom-
putational neuroscience as a unifying account of perception, action, and learning [9, 10]. Active
inferencesuggeststhatbiologicalsystemslearnaprobabilisticmodeloftheirhabitableenvironment
andthatthestatesofthesystemchangetomaximizetheevidenceforthismodel[11,12].Theresult-
ingschemecastsperception,actionandlearningasemergentprocessesof(approximate)Bayesian
inference, thereby offering a potentially unifying theory of adaptive biological systems. Despite
its strong theoretical foundations, existing computational implementations have been restricted to
low-dimensional tasks, often with discrete state spaces and actions [11, 10, 13, 14, 15]. Here, we
establishaformalconnectionbetweenactiveinferenceandmodel-basedRL.Indoingso,weextend
practicalimplementationsofactiveinferencesothattheyworkeffectivelyatscale, andwesituate
model-basedRLwithinthebroadtheoreticalcontextofferedbyactiveinference.
9102
voN
42
]GL.sc[
1v10601.1191:viXra
We present a model of active inference that is applicable in high-dimensional control tasks with
both continuous states and actions. Our model builds upon previous attempts to scale active in-
ference[16,17,18]byincludinganefficientplanningalgorithm, aswellasthequantificationand
activeresolutionofmodeluncertainty.Consistentwiththeactiveinferenceframework,learningand
inferenceareachievedbymaximizingsinglelowerboundonBayesianmodelevidence,andpolicies
areselectedtomaximizealowerboundonexpectedBayesianmodelevidence[11].Wedemonstrate
thatthisunifiednormativeschemeenablessampleefficientlearning,strongperformanceondifficult
controltasks, andaprincipledapproachtoactiveexploration. Moreover, weestablishhomologies
betweenouractiveinferencebasedmodelandstate-of-the-artapproachestomodel-basedRL.
In what follows, we specify the general mathematical formulation of active inference, before de-
scribingourimplementation,whichisapplicableinbothpartially-observedandfully-observeden-
vironments. We then present preliminary results in three challenging fully-observed continuous
control benchmarks, leaving the analysis of partially-observed environments (i.e. pixels) to future
work. Theseresultsdemonstratethatouralgorithmfacilitatesactiveexplorationoverlongtemporal
horizons and significantly outperforms a strong model-free RL baseline, in terms of both sample
efficiencyandperformance.
2 Activeinference
Followingpreviouswork[10,11],weconsideractiveinferenceinthecontextofapartiallyobserved
Markovdecisionprocess(POMPD).Ateachtimestept,thetruestateoftheenvironmentˆs
t
∈Rdsˆ
evolves according to the stochastic transition dynamics ˆs
t
∼ p(ˆs
t
|ˆs
t−1
,a
t−1
), where a ∈ Rda
denotesanagent’sactions. Agentsdonotalwayshaveaccesstothetruestateoftheenvironment,
butmightinsteadreceiveobservationso
t
∈ Rdo,whicharegeneratedaccordingtoo
t
∼ p(o
t
|ˆs
t
).
Assuch,agentsmustoperateonbeliefss
t
∈Rds aboutthetruestateoftheenvironmentˆs
t
. Inwhat
follows, wedenotethetruedynamicswithuprightlettersp(·)andamodelofthesedynamics(the
agent)withitalicsp(·).
Active inference proposes that agents implement and update a generative model of their world
p(o˜,˜s,π,θ),wherethetildenotationdenotesasequenceofvariablesthroughtimex˜ ={x ,...,x },
0 T
πdenotesapolicy,π ={a ,...,a },andθ ∈Θdenotesparametersofthegenerativemodel,which
0 T
arethemselvesrandomvariables. Additionally,agentsmaintainarecognitiondistributionq(˜s,π,θ),
representinganagent’s(approximatelyoptimal)beliefsoverstates˜s,policiesπandmodelparame-
tersθ.
Asnewobservationsaresampled,agentsupdatetheparametersoftheirrecognitiondistributionto
minimizevariationalfreeenergyF:
F(o˜)=E [lnq(˜s,π,θ)−lnp(o˜,˜s,π,θ)]
q(˜s,π,θ)
(1)
≥−lnp(o˜)
This makes the recognition distribution q(˜s,π,θ) converge towards an approximation of the (in-
tractable)posteriordistributionp(˜s,π,θ|o˜),therebyimplementingatractableformof(approximate)
Bayesianinference[19].
Crucially,activeinferencealsoproposesthatanagent’sgoalsanddesiresareencodedinthegenera-
tivemodelaspriorpreferencesforfavourableobservations[12,20],i.e. bloodtemperatureat37°C.
Freeenergythenprovidesaproxyforhowsurprising(i.e.,unlikely)someobservationsareunderthe
agent’smodel. WhileminimisingEq. 1providesanestimateforhowsurprisingsomeobservations
are, it cannot reduce this quantity directly. To achieve this, agents must change their observations
through action. Acting to minimise variational free energy ensures the minimisation of surprisal
−lnp(o˜),orthemaximisationofthe(Bayesian)modelevidencep(o˜),sincefreeenergyprovidesan
upperboundonsurprisal. Activeinference,therefore,proposesthatagent’sselectpoliciesinorder
to minimize expected free energy G [12], where the expected free energy for a given policy π at
somefuturetimeτ is:
G(π,τ)=E [lnq(s ,θ|π)−lnp(o ,s ,θ|π)]
q(oτ,sτ,θ|π) τ τ τ
(2)
≥−E (cid:2) lnp(o |π) (cid:3)
q(oτ|π) τ
2
Expectedfreeenergyprovidesaboundonexpectedsurprisal,andcanbedecomposedintoextrinsic
value, which quantifies the degree to which expected observations are congruent with an agent’s
prior beliefs, and intrinsic value, which quantifies the amount of information an agent expects to
gain from enacting some policy [13, 11, 10]. This decomposition affords a natural interpretation:
to avoid being surprised, one should sample unsurprising data, but also learn about the world to
makedatalesssurprisingperse. SelectingpoliciesthatminimizeEq. 2will,therefore,ensurethat
probable(i.e.favourable,givenanagent’snormativepriors)observationsarepreferentiallysampled,
whilealsoensuringthatagentsgatherinformationabouttheirenvironment.
3 Model
Incognitiveandcomputationalneuroscience,implementationsofactiveinferenceagentsgenerally
followoneoftwoapproaches. Thefirstconsidersthegenerativemodelandrecognitiondistribution
tobeGaussianundertheLaplaceapproximationandprescribesgradient-descentupdatesthatrecur-
rentlyminimizefreeenergywitheachnewobservation[21,22,20].Whilethisapproachispurported
asbiologicallyplausibleandenjoysempiricalsupportundertheguiseofpredictivecoding[21,23],
itisnotclearhow,oratleastnotstraightforward,toextendthisimplementationtotheprospective
freeenergyminimizationdiscussedinSec. 2. Thesecondapproachemploysdiscretedistributions
(e.g., Categorical, Dirichlet) that are updated via variational message-passing [11]. While this ap-
proachprovidesanelegantframeworkforevaluatingexpectedfreeenergy,itcanonlybeappliedin
discretestateandactionspaces,meaningitisnotdirectlyapplicabletothehigh-dimensionalstates
andcontinuousactionsconsideredinRLbenchmarks.
In the current paper, we take an alternative approach and employ amortized inference [24], which
utilizesfunctionapproximators(i.e.,neuralnetworks)toparameterizedistributions. Freeenergyis
thenminimizedwithrespecttotheparametersofthefunctionapproximators,andnotthevariational
parametersthemselves.WedetailourgenerativemodelandrecognitiondistributioninSec.3.1,how
learning and inference are implemented in Sec. 3.2, how policy selection and trajectory sampling
areimplementedinSec. 3.3&Sec. 3.4, andhowtoevaluateexpectedfreeenergyinsectionSec.
3.5. Finally,wedescribetheimplementationdetailsforthefully-observedcaseinSec. 3.6.
3.1 Generativemodel&recognitiondistribution
We consider a generative model p(o˜,˜s,π,θ) over sequences of observations o˜, hidden states ˜s,
policiesπandparametersθ:
T
(cid:89)
p(o˜,˜s,π,θ)=p(θ)p(π) p(o |s )p(s |s ,π ,θ)
t t t t−1 t−1
t=1
p(o |s )=N(o ;µ ,σ2)
t t t λ λ
where[µ ,σ2]=f (s )
λ λ λ t (3)
p(s |s ,π ,θ)=N(s ;µ ,σ2)
t t−1 t−1 t θ θ
where[µ ,σ2]=f (s ,π )
θ θ θ t−1 t−1
p(θ)=N(θ;0,I)
p(π)=σ(−G(π))
where we have assumed that s is fixed. In Eq. 3, we have parametrized both the likelihood dis-
0
tribution p(o |s ) and the transition distribution p(s |s ,π ,θ) with function approximators.
t t t t−1 t−1
Specifically, the likelihood distribution is described by a multivariate Gaussian distribution with a
meanandcovarianceparameterizedbysome(potentiallynon-linear)functionapproximatorf (s ),
λ t
while the prior distribution is described by a Gaussian with mean and variance parameterized by
somefunctionapproximatorf (s ,π ).
θ t−1 t−1
Amortizingtheinferenceprocedureoffersseveralbenefits. Forinstance,thenumberofparameters
remainsconstantwithrespecttothesizeofthedataandinferencecanbeachievedthroughasingle
forward pass of a network. Moreover, while the amount of information encoded about variables
is fixed, the conditional relationship between variables can be arbitrarily complex. In Eq. 3, the
3
parametersofthetransitiondistribution,θ,arethemselvesrandomvariables. Inthecurrentcontext,
these parameters are the weights of the neural network f (s ,π ). This approach allows the
θ t−1 t−1
uncertaintyabouttheseparameterstobequantifiedandcastslearningasaprocessof(variational)
inference [25]. The prior probability of θ is given by a standard Gaussian, which acts as a regu-
larizerduringlearning. Althoughwehaveonlyconsidereddistributionsovertheparametersofthe
transitiondistributionθ,thesameschemecouldbeappliedtotheparametersofthelikelihooddis-
tribution,λ. Finally,thepriorprobabilityofpoliciesisasoftmaxfunctionofthenegativeexpected
freeenergyofthosepolicies−G(π)[11]. Thisformalizesthenotionthatpoliciesarea-priorimore
likelyiftheyareexpectedtominimizefreeenergyinthefuture[12].
TomakeactiveinferenceapplicabletothekindsoftasksconsideredinRL,wetreatrewardsignals
or asobservationsinaseparatemodality. Therefore,weextendthegenerativemodeltoincludean
additional scalar Gaussian over reward observations p(or|s ) with unit variance and mean f (s ),
t t α t
wheref (s )isafully-connectedneuralnetworkwithparametersα.
α t
Weconsiderarecognitiondistributionq(˜s,π,θ)oversequencesofhiddenstatess ,policiesπ and
t
parametersθ:
T
(cid:89)
q(˜s,π,θ)=q(θ)q(π) q(s |o )
t t
t=0
q(θ)=N(θ;µ ,σ2)
ξ ξ (4)
q(π)=N(π;µ ,σ2)
ψ ψ
q(s |o )=N(s ;µ ,σ2)
t t t φ φ
where[µ ,σ2]=f (o )
φ φ φ t
The distribution q(s |o ) is a diagonal Gaussian with mean and variance parameterized by some
t t
functionapproximatorf (o ),whilethethevariationalposterioroverparametersθ andpoliciesπ
φ t
arebothdiagonalGaussians.
3.2 Learning&Inference
Inordertoimplementlearning,wederivetheupdatesforξ = {µ ,σ2},φ,λandαthatminimize
ξ ξ
free energy F. Given Eq. 3 and 4, the variational free energy F for a given time point t can be
definedas:
F (o ,ξ,φ,λ,α)=
t t
E (cid:104) E (cid:2) D [q(s |o )||p(s |s ,π ,θ)] (cid:3)(cid:105) (5)
θ∼q(θ) q(st−1|ot−1) KL t t t t−1 t−1
+D (cid:2) q(θ)||p(θ) (cid:3) −E [lnp(o |s )]
KL q(st|ot) t t
wherewehavefollowed[11]andomittedtheadditionaltermD [q(π)||p(π)]fromtheoptimisa-
KL
tionofξ,φ,λ,α, allowingustoignorethedependencybetweenhiddenstatesand(thepriorprob-
ability of) policies. We optimise q(π) with respect to F separately, as described in the following
section.
Eq. 5 can be minimized with respect to ξ,φ,λ,α using stochastic gradient descent. Given some
observationo , thenegativelog-likelihood(thirdterm)canbecalculatedbymappingtheobserva-
t
tiontothevariationalparametersofq(s |o ),e.g.,[µ ,σ2] = f (o ). Thereparameterizationtrick
t t φ φ φ t
[24]isthenutilizedtoobtainadifferentiablesamplefromq(s |o )1,whichisthenpassedthrough
t t
f (s ),givingtheparametersofthelikelihooddistribution[µ ,σ2]. Thenegative-loglikelihoodof
λ t λ λ
theobservationsisthencalculatedunderthisdistribution. Next, theparameterdivergence(second
term) is calculated analytically, as both distributions are fully factorized Gaussians. Finally, The
statedivergence(firstterm)iscalculatedbytakingK samplesfromq(θ),againusingthereparame-
terizationtrick. Foreachsampleθ(i)inK,areparameterizedsamplefromthepreviousbeliefsover
hiddenstatesq(s |o )ispropagatedthroughf (s ,π )(whereπ referstotheaction
t−1 t−1 θ(i) t−1 t−1 t−1
1ForaGaussianN(x;µ,σ2),areparameterizedsampleisobtainedviax=µ+σ(cid:12)(cid:15),where(cid:15)∼N(0,1)
4
that was taken at the previous time step), giving the parameters of the transition distribution. The
KL-divergencetermisthenanalyticallycalculatedforeachsampleinK andaveraged.
Thisprocedureiscarriedoutinbatchedfashionovertheavailabledataset. Attesttime,inference
canbeachievedbydirectlymappingobservationstothevariationalparametersusingf (o ). This
φ t
approach to inferring hidden states is similar to that of a variational autoencoder [24], but here
the global prior has been replaced with a prior based on the transition distribution. Moreover, the
inference of parameters θ is homologous to the Bayesian neural network approach to parameter
learning[25].
Deriving updates for all parameters through a single (variational) objective function offers several
potentialbenefits.First,thelearnedlatentspaceisforcedtobalancebetweenthecompressionofob-
servationsand(action-conditioned)temporaltransitions.Thisisincontrastto‘modular’approaches,
wherebyalatentspaceisfirstlearnedtocompressobservations,andsubsequently,atransitionmodel
islearnedinthisfixedlatentspace[2]. Moreover,thisapproachallowsthequantificationofuncer-
taintyinbothhiddenstatesandmodelparameters,therebyquantifyingbothaleatoricandepistemic
uncertainty[26,27].
3.3 Policyselection
Under active inference, policy selection is achieved by updating q(π) in order to minimize free
energyF.Giventhepriorbeliefthatpoliciesminimizeexpectedfreeenergy,i.e.,p(π)=σ(−G(π))
(asspecifiedinEq. 3),freeenergyisminimizedwhenq(π) = σ(−G(π))[11]. Fordiscreteaction
spaces with short temporal horizons, G(π) can be evaluated in full by considering each possible
policy[10]. However,incontinuousactionspaces,thereareinfinitepolicies,meaninganalternative
approachisrequired.
Inthecurrentwork,wetreatq(π)asadiagonalGaussianwithparametersψ = {µ ,σ2}. Ateach
ψ ψ
timestep,weoptimiseψsuchthatq(π)∝−G(π). Whilethissolutionwillfailtocapturetheexact
shapeof−G(π),agentsneedonlyidentifythepeakofthelandscapetoenacttheoptimalpolicy. To
optimise the parameters of q(π), we utilise the cross-entropy method (CEM) [6, 3]. At each time
step t, we consider policies of a fixed horizon H, using notation πt:t+H = {a ,...,a }. The
t t+H
distributionoverpoliciesisinitializedasq(πt:t+H)←N(πt:t+H;0,I)andoptimizedasfollows:
(i) SampleN policiesfromq(πt:t+H)
(ii) Evaluate−G(πt:t+H)foreachsampleπt:t+H (describedinthefollowingsection),return-
ingascalarvalue
(iii) Refitq(πt:t+H)tothetopM samples
This procedure is carried out I times, after which the mean of the belief for the current time step
a = E[q(πt:t+H)]isreturned. Moreover,thisprocedureiscarriedoutaftereachnewobservation.
t t
Forthecurrentexperiments,H =12,N =1000,M =100andI =10.
Thisprocessofmodelpredictivecontrol[28]wasselectedforconsistencywithpreviouscomputa-
tionalmodelsofactiveinference[10],whereadistributionoverpoliciesisupdatedaftereachnew
observation. Alternative approachesincludeoptimizingaparametrizedpolicywithrespect topast
evaluationsofexpectedfreeenergy[16].However,thisapproachisnotsuitedfornon-stationaryob-
jectivefunctionsoractiveexploration[5]. Alternatively,aparametrizedpolicycouldbeoptimized
withrespecttoimaginedrolloutsfromatransitionmodel[6],whichwouldenableactiveexploration
[5]. Theeffectivenessoftheseapproachesdependsonthecomplexityofthevaluefunctionrelative
tothetransitiondynamics[29],aswellasthestationarityofthevaluefunction.
3.4 Trajectorysampling
Toevaluatetheexpectedfreeenergyforagivenpolicy−G(π), itisfirstnecessarytoevaluatethe
expectedfuturebeliefsconditionedonthatpolicyq(˜st:t+H,o˜t:t+H|π). Thefactthatthetransition
model is probabilistic, and the parameters of the transition model are random variables, induces a
distributionoverfuturetrajectories[11].Severalapproachesexisttoapproximatethepropagationof
uncertaintrajectories[3]. Forinstance,onecanignoreuncertaintyentirelyandpropagatethemean
ofthedistributions,oronecanexplicitlypropagatethefullstatisticsofthedistribution[30]. Inthe
5
currentwork,weutiliseaparticleapproach[3,6],wherebyasetofMonteCarlosamplesareprop-
agated. Inparticular, weconsiderB samplesfromtheparameterdistributionθ(i) ∼ q(θ), andfor
eachsampleinB,propagateJ samplesthroughthetransitionmodels(j) ∼ p(s |s ,π ,θ(i)).
t t t−1 t−1
Toinferobservationsandrewards,wepassallsamplesthroughtherespectivemodelandaverage.
3.5 Expectedfreeenergy
Inthissectionwedescribehowtoevaluate−G(π),wherewehaveusedπ = πt:t+H fornotational
convenience.Thenegativeexpectedfreeenergyforapolicyisequaltothesumofnegativeexpected
freeenergiesovertime,−G(π)=
(cid:80)t+H−G(π,τ),where
τ=t
−G(π,τ)≈E [lnp(or)]
q(or τ |π) τ
(cid:124) (cid:123)(cid:122) (cid:125)
Extrinsicvalue
(cid:104) (cid:105)
+H[q(o |π)]−E H[q(o |s ,π)]
τ q(sτ|π) τ τ
(cid:124) (cid:123)(cid:122) (cid:125) (6)
Stateinformationgain
(cid:104) (cid:105)
+H[q(s |π)]−E H[q(s |π,θ)]
τ q(θ) τ
(cid:124) (cid:123)(cid:122) (cid:125)
Parameterinformationgain
We refer to [10] for a derivation of Eq. 6. The first term (extrinsic value) quantifies the degree
towhichtheexpectedobservationsq(or|π)arecongruentwiththeagent’spriorbeliefs(i.e., pref-
τ
erences) p(or). Note that in active inference, there is no intrinsic delineation of reward signals -
t
all observations are assigned some a-priori probability. However, as RL environments specify a
distinct reward signal, we have defined the agent’s prior preferences over reward observations or
only. Moreover,asRLenvironmentsareconstructedsuchthatagentswishtosimplymaximizethe
sum of rewards (rather than obtain any particular reward observation), we evaluate extrinsic value
as or ∼ q(o |π), such thatextrinsicvalue increasesaslarger rewardsare predicted. Wereferthe
τ τ
readerto[18]foranalternativeformulationwhereagent’slearnaspecificpriordistribution.
Thesecondterm(stateinformationgain)quantifiestheexpectedreductioninuncertaintyinbeliefs
over hidden states q(s ). In other words, it promotes agents to sample data in order to resolve
τ
uncertaintyaboutthehiddenstateoftheenvironment. Thistermisformallyequivalenttoanumber
ofestablishedquantities,suchas(expected)Bayesiansurprise,mutualinformation,andtheexpected
reduction in posterior entropy [11, 31], and has been used to describe various epistemic foraging
behaviors,suchassaccades[32,33,34,35]andsentencecomprehension[15]. Inthecurrentpaper,
we conduct experiments in fully observed environments, and as such, do not consider the state
informationgainterminouranalysis.
The final term (parameter epistemic value) quantifies the expected reduction in uncertainty in be-
liefs over parameters q(θ), and promotes agents to actively explore the environment in order to
resolveuncertaintyintheirmodel[36,14]. Inordertoevaluateparameterepistemicvalue,weuse
a nearest neighbor estimate of the entropies [27, 37]. In other words, we estimate the entropy via
spatial properties of samples from the relevant distributions. Specifically, we estimate the entropy
asH[p(x)]= 1 (cid:80)n ln(nρ )+ln2+C ,wherenisthenumberofsamplesfromthedistribu-
n i=1 n,i E
tion,ρ isthenearestneighbordistanceofasamplex fromothersamplesx andC istheEuler
n,i i j E
constant.Alternatively,parameterepistemicvaluecouldberewrittenasthe(expected)Bayesiansur-
priseofthedistributionoverparametersandthencalculatedanalytically[38,39,34,35]. However,
thisrequiresdoingfictiveupdatestotheparameterdistribution,whichiscomputationallyexpensive
whenconductedforeachcandidatepolicyateachtimestep.
3.6 Fullyobservedmodel
Themodelpresentedintheprecedingsectionsservesasthemostgeneralformulation,applicablein
both partially-observed and fully-observed environments. In what follows, we describe an imple-
mentationforthefully-observedcase, leavingananalysisofthepartially-observedcaseforfuture
work.
6
Toadaptthegenerativemodelforfully-observedenvironments,weutiliseafixedidentitycovariance
forthelikelihooddistributionp(o |s ),andparameterizethemeanasµ =f (s )=E[s ],thereby
t t λ λ t t
encodingthebeliefthatthereisadirectmappingbetweenstatesandobservations. Forthetransition
distributionp(s |s ,π ,θ),weparameterizethemeanasf (s ,π )andutilizeafixedunit
t t−1 t−1 θ t−1 t−1
variance. In all experiments, f (s ,π ) is a feed-forward network with two fully connected
θ t−1 t−1
layersofsize500withReLUactivations,whichdefinesthedimensionalityofp(θ)andq(θ).
Note that by treating the variance of the transition distribution as fixed, the evaluation of parame-
ter epistemic value is significantly simplified. Specifically, the second entropy term in parameter
epistemicvaluebecomesconstantunderpolicies,suchthatweneedonlyevaluatethefirstentropy
termH[q(s |π)] = H[E [q(s |π,θ)]]. Weuse5samplesfromq(θ)toevaluatetheexpectation
τ q(θ) τ
inthisentropytermthroughout. Finally,wetreatthevarianceofq(s |o )asafixedunitparameter
t t
andparameterizethemeanasµ = f (o ) = o ,therebyencodingthebeliefthatthereisadirect
φ φ t t
mappingbetweenobservationsandstates. Notethatthismeansthattheparametersofλandφare
fixedandarethusexcludedfromtheoptimisationscheme.
4 Experiments
In this section, we investigate (i) whether the proposed active inference model can successfully
promote exploration in the absence of reward observations (i.e. exploration), and (ii) whether the
modelcanachievegoodperformanceandhighsampleefficiencyonchallengingcontinuouscontrol
tasks(i.e. exploitation). Weevaluatethesetwoaspectsofthemodelseparately,leavinganalysisof
theirjointperformance(i.e. theexploration-exploitationdilemma)tofuturework.
Weutilisethefollowinglearningschemeforboththeexplorationandexploitationexperiments. We
initializethedatasetwith5seedepisodescollectedunderrandomactions. Foreachiterationofthe
experiment,wetraintheagent’smodelviaEq. 5with100batchesrandomlysampledfromthedata
set,usingabatchsizeof50. Agentsthencollectdatafromtheenvironmentuntiltheepisodeends
(whenthemaximumnumberofstepsisreached,orwhenagenttheagentreachesaterminalstate).
4.1 Exploration
Totestwhethertheactiveinferencemodelenablesefficientexploration,weexplorethestatespace
visitedbydifferentalgorithmsinthecontinuousMountainCarenvironment(O ∈R2,A∈R1). We
compare theactive inference model totwo algorithms, (i)a ‘reward’ agent whichoperates via the
samescheme,butonlyselectsactionsbasedonextrinsicvalue,and(ii)andan(cid:15)-greedyagentwhich
selectsactionbasedonextrinsicvalue,butadditionallyaddsGaussianexplorationnoise(σ2 =0.3)
toeachaction. Agentslearnandactintheenvironmentfor100epochs. Thecumulativecoverage
of state space is plotted in Fig. 1. These results demonstrate that the active inference agent can
effectivelyexploremoreofthestatespace,relativetotheotheralgorithms.
Figure 1: Comparison of exploration strategies. (A) The cumulative state-space coverage after
100epochsfortherewardagent. (B)Thecumulativestate-spacecoverageafter100epochsforthe
(cid:15)-greedyagent. (C)Thecumulativestate-spacecoverageafter100epochsfortheactiveinference
agent. These results demonstrate that the active inference agent explores more of the state space,
relativetotheotherexplorationstrategies.
7
4.2 Exploitation
Next,weinvestigatewhethertheactiveinferenceagentcanachievegoodperformanceoncontinuous
control tasks. We explore performance in the inverted pendulum task (O ∈ R3, A ∈ R1) and the
morechallenginghoppertask(S ∈R15,A∈R3). Theperformanceofourmodeliscomparedtoa
strongmodel-freebaseline,DDPG[40].
Asbothenvironmentshavewell-shapedrewards,weonlyconsidertheexploitationcomponent(ex-
trinsic value) of the expected free energy objective function, ignoring the exploration component
(epistemicvalue). Assuch,weutiliseapoint-estimateversionofthemodel,thusremovingthedis-
tributionsoverparameters. Toretainstochasticityinthetransitionmodel,weparameterizeboththe
meanandvarianceofthetransitiondistributionusingafunctionapproximator(asopposedtojustthe
mean),andfixthevarianceoftherecognitiondistributionto0.1. Moreover,following[6],weuse
anactionrepeatof3forallalgorithms,enablingshorterplanninghorizonsandamorepronounced
learningsignal.
In Fig. 2, we plot the performance of both algorithms as a function of the number of epochs. We
show the mean performance over a fixed set of 5 random seeds and the shaded lines shown the
95%interquartilerangeateachepoch. Theseresultsdemonstratethattheactiveinferenceagentcan
achievestrongperformanceinunder100epochsonbothtasks,demonstratinganorderofmagnitude
bettersampleefficiencycomparedtothemodel-freebaseline.
Figure 2: Comparison of Performance on two continuous control tasks. (A) Average returns
over1500epochsontheinvertedpendulumtaskfortheactiveinferenceagentandthemodel-free
DDPG agent. (B) Average returns over 1500 epochs on the hopper task for the active inference
agentandthemodel-freeDDPGagent. NotethatforA&B,westoppedtheactiveinferenceagent
after 100 epochs due to convergence. (C & D) A zoomed-in view of figures A & B, showing a
morefine-grainedviewoftheactiveinferenceagent’sprogressionover100epochs. Forallfigures,
the filled lines represent the mean of 5 random seeds, whereas the shaded areas denote the 95%
interquartile range. Together, these results demonstrate that the active inference agent can learn
difficult continuous control tasks, with a far greater sample efficiency, relative to a strong model-
freebaseline.
5 Previouswork
Deepactiveinference Previousworkhasexploredtheprospectofscalingactiveinferenceusing
amortizedinference. In[17],theauthorsparameterizedboththegenerativemodelandrecognition
distributionwithfunctionapproximatorsandusedevolutionarystrategiestooptimisethefreeenergy
functional when gradients were not available. Similarly, [16] utilized amortization to parametrize
distributionsandalsoamortizedactionbylearningaparameterizedapproximationoftheexpected
freeenergybound. Finally,[18]extendedpreviousworktoincludeaspecificplanningcomponent
8
based on CEM. The authors focused on the problem of learning the prior distribution over reward
observationsp(or)anddemonstratedthiscouldbeimplementedinalearning-from-exampleframe-
work.
Ourworkbuildsuponthesepreviousmodelsbyincorporatingmodeluncertaintyanditsactiveres-
olution. Inotherwords,weextendthepreviouspoint-estimatemodelstoincludefulldistributions
overparametersandupdatetheexpectedfreeenergyfunctionalsuchthattheuncertaintyinthesedis-
tributionsisactivelyminimized.Thisbringsourimplementationinlinewiththecanonicalmodelsof
activeinferencefromthecognitiveandcomputationalneuroscienceliterature[12]. Moreover,iten-
ablesustoevaluatethefeasibilityofactiveexplorationunderthescaledactiveinferenceframework,
applythemodeltomorecomplexcontroltasks,andobtainincreasedsampleefficiency,relativeto
previousmodels.
Model-basedRL Themodelpresentedinthecurrentworkbearsanumberofresemblanceswith
model-based approaches to RL. First, variational autoencoders have been used extensively to map
observationsintoacompressedlatentspace,therebysimplifyingtheproblemofactionselectionand
the process of learning a forward transition model [2, 6, 41, 42, 43, 39, 44]. Moreover, the CEM
algorithm is a popular method for implementing planning in model-based RL [6, 3, 45]. Recent
workhasadditionallyhighlightedtheimportanceofusingaprobabilisticdynamicsmodelinorderto
captureepistemicuncertainty[3,6,7,46,47,48].Thesuccessoftheseapproacheshasdemonstrated
thatdeterministicmodelsarepronetomodelbias,whichcanleadtooverfittinginlowdataregimes.
MostapproacheseitherutilizeBayesianneuralnetworks[27],ensemblesofdeterministicnetworks
[3],dropout[46]orGaussianprocesses[30]inordertocaptureuncertainty. Inthecurrentwork,we
opted for Bayesian neural networks to ensure consistency with the variational principles espoused
by the active inference framework, but note that ensembles can be made explicitly Bayesian with
minormodifications[49].
Informationgain Identifyingscalableandefficientexplorationstrategiesremainsoneofthekey
openquestionsinRL.Model-freemethods,suchas(cid:15)-greedyorBoltzmannchoicerules[50],utilise
noiseintheactionselectionprocessoruncertaintyintherewardstatistics[51,52].
Amorepowerfulapproach[53]istoconstructamodeloftheworld,allowingtheagenttoevaluate
whichpartsofstatespaceithas(andhasnot)visited. Forinstance, [54]constructapseudo-count
measure for estimating state visitation frequency in continuous state spaces. Alternatively, an ex-
plicitforwardmodelofthetransitiondynamicscanbelearned. Thisallowsformeasuressuchasthe
amountofpredictionerror[55,56,57,58]orpredictionerrorimprovement[59]tobeutilizedfor
exploration.
If the learned model (implicitly or explicitly) captures probabilistic features then information-
theoretic measures can be used to guide exploration (see [60] for a review). In [61], the authors
derived an information-theoretic measure to maximize the predictive power of the agent, while in
[62],theauthorsderivedanobjectivefunctiontomaximizethemutualinformationbetweenactions
andfuturestatesoftheenvironment(i.e.,empowerment).
Of particular relevance to the current work is the use of information gain to promote exploration,
whichhasbeendemonstratedtooutperformalternativemeasuressuchaspredictionerror[63].From
atheoreticalperspective,informationgainhelpsovercomewhatisknownasthe"TV-problem"[34],
where(unpredictable)noiseintheenvironmentismistakenlytreatedasepistemicallyvaluable.This
isbecauseinformationgainconsiderstheamountofinformationprovidedforbeliefs,asopposedto
theamountofinformationprovidedbythesignalperse.
Information gain can be traced back to [64], who used it to quantify the amount of information
to be gained from some experiment. [65] developed a Bayesian framework in order to maximize
informationgainviadynamicprogramming,however,theexperimentswerelimitedtodiscretestate
spacesusingtabularMDPs. In[38], theauthorsutilizedBayesianneuralnetworkstoquantifythe
amount of information gained from some (action-conditioned) transition. This work was further
extended in [39], where the amount of information gain was quantified with respect to a latent
dynamicsmodel.
Inparallelwiththecurrentwork,[5]lookedtomaximizeexpected informationgain,whichentails
anactiveapproachtoexploration. ThisisincontrasttothemajorityofexplorationstrategiesinRL,
whicharereactive,inthesensethattheymustfirstobserveaninformativestatebeforebeingableto
9
gatherinformation[5]. Thiscanleadtoproblemsofover-commitment,wherebyinformativeparts
of state space must be unlearned once the relevant information has been gathered. However, [5]
optimized expected information gain offline, whereas the current model uses an online approach.
Finally,Theuseofnearest-neighbourentropyestimatorsforinformationgainhasbeenexploredin
[66,27].
6 Discussion&Conclusion
Wehavepresentedamodelofactiveinferencethatcanscaletocontinuouscontroltasks,complex
dynamics and high-dimensional state spaces. The presented model can be trained via a single ob-
jective function, expected free energy, that captures both epistemic and aleatoric uncertainty, and
prescribesbothgoal-directedandinformation-gatheringbehaviourviaasinglenormativedrive.
Our model makes two primary contributions. First, we showed that the full active inference con-
struct can be scaled to the kinds of tasks considered in the RL literature. This involved extending
previous models of deep active inference to include model uncertainty and expected information
gain.Second,wehighlightedtheoverlapbetweenactiveinferenceandstate-of-the-artapproachesto
model-basedRL.Theseincludetheuseofvariationalinferenceforthecompressionofobservations,
the use of variational inference for learning distributions over parameters, the use of probabilistic
models of dynamics, the use of prospective planning in latent space, and the active resolution of
uncertainty.
Whileactiveinferencedefinedthepropertiesoflivingsystemsfromfirstprinciples[12],andmodel-
based RL has attempted to engineer adaptive agents through the most effective means available,
both perspectives have converged on similar solutions. Our work has exploited this convergence
to show that active inference provides a principled and unified theoretical framework in which to
contextualize the various developments in model-based RL. This perspective by itself offers little
practical benefit. However, active inference offers two potentially novel perspectives from which
model-based RL can benefit. The first is casting reward as (prior) probabilities. This provides
a principled framework for learning reward structure (i.e., reward shaping), for assigning rewards
(i.e.,probability)acrossmultipleobservationmodalities[67],andforlearning-from-demonstration
[18]. The second perspective is casting both exploration and exploitation as two components of a
singleimperativetomaximizeexpectedBayesianmodelevidence.Thisperspectivehasthepotential
to recast the exploration-exploitation dilemma as a problem of optimising parameters in order to
maximisemodelevidence. Weleaveapracticalinvestigationofthisperspectivetofuturework.
7 Acknowledgements
ATisfundedbyaPhDstudentshipfromtheDr. MortimerandTheresaSacklerFoundationandthe
School of Engineering and Informatics at the University of Sussex. CLB is supported by BBRSC
grant number BB/P022197/1. MB acknowledges support as an International Research Fellow of
theJapanSocietyforthePromotionofScience. ATandAKSaregratefultotheDr. Mortimerand
TheresaSacklerFoundation,whichsupportstheSacklerCentreforConsciousnessScience. AKSis
additionallygratefultotheCanadianInstituteforAdvancedResearch(AzrieliProgrammeonBrain,
Mind,andConsciousness).
References
[1] ChristopherG.AtkesonandJuanCarlosSantamaria. Acomparisonofdirectandmodel-based
reinforcement learning. In In International Conference on Robotics and Automation, pages
3557–3564.IEEEPress,1997.
[2] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution.
arXiv:1809.01999[cs,stat],2018.
[3] KurtlandChua,RobertoCalandra,RowanMcAllister,andSergeyLevine.Deepreinforcement
learning in a handful of trials using probabilistic dynamics models. arXiv:1805.12114 [cs,
stat],2018.
[4] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy
10
Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned
model. arXiv:1911.08265[cs,stat],2019.
[5] PranavShyam,WojciechJas´kowski,andFaustinoGomez. Model-basedactiveexploration. In
InternationalConferenceonMachineLearning,pages5779–5788,2019.
[6] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,and
JamesDavidson. Learninglatentdynamicsforplanningfrompixels. arXiv:1811.04551[cs,
stat],2018.
[7] M. P. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-efficient approach
topolicysearch. In28thInternationalConferenceonMachineLearning(ICML2011).IMLS,
2011.
[8] JürgenSchmidhuber.Makingtheworlddifferentiable:Onusingself-supervisedfullyrecurrent
neural networks for dynamic reinforcement learning and planning in non-stationary environ-
ments,1990.
[9] KarlFriston. Thefree-energyprinciple:aunifiedbraintheory? NatureReviewsNeuroscience,
11(2):127–138,2010.
[10] KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanniPez-
zulo. Activeinference: Aprocesstheory. NeuralComputation,29(1):1–49,2017.
[11] KarlFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzgerald,and
GiovanniPezzulo. Activeinferenceandepistemicvalue. CognitiveNeuroscience,6(4):187–
214,2015.
[12] KarlFriston. Afreeenergyprincipleforaparticularphysics. 2019.
[13] KarlJ.Friston,MarcoLin,ChristopherD.Frith,GiovanniPezzulo,J.AllanHobson,andSasha
Ondobaka. Activeinference, curiosityandinsight. NeuralComputation, 29(10):2633–2683,
2017.
[14] KarlJ.Friston,MarcoLin,ChristopherD.Frith,GiovanniPezzulo,J.AllanHobson,andSasha
Ondobaka. Activeinference, curiosityandinsight. NeuralComputation, 29(10):2633–2683,
2017.
[15] KarlJ.Friston,RichardRosch,ThomasParr,CathyPrice,andHowardBowman.Deeptempo-
ralmodelsandactiveinference. Neuroscience&BiobehavioralReviews,90:486–501,2018.
[16] BerenMillidge. Deepactiveinferenceasvariationalpolicygradients. arXiv:1907.03876[cs],
2019.
[17] KaiUeltzhöffer. Deepactiveinference. BiologicalCybernetics,112(6):547–573,2018.
[18] OzanÇatal,JohannesNauta,TimVerbelen,PieterSimoens,andBartDhoedt. Bayesianpolicy
selectionusingactiveinference. arXiv:1904.08149[cs],2019.
[19] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for
statisticians. JournaloftheAmericanStatisticalAssociation,112(518):859–877,2017.
[20] ManuelBaltieriandChristopherLBuckley. Pidcontrolasaprocessofactiveinferencewith
lineargenerativemodels. Entropy,21(3):257,2019.
[21] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle. Philo-
sophical Transactions of the Royal Society of London. Series B, Biological Sciences,
364(1521):1211–1221,2009.
[22] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free en-
ergy principle for action and perception: A mathematical review. Journal of Mathematical
Psychology,81:55–79,2017.
[23] Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive
science. TheBehavioralandBrainSciences,36(3):181–204,2013.
[24] DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes. arXiv:1312.6114[cs,
stat],2013.
[25] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncer-
taintyinneuralnetworks. arXiv:1505.05424[cs,stat],2015.
11
[26] StefanDepeweg,JoséMiguelHernández-Lobato,FinaleDoshi-Velez,andSteffenUdluft. De-
compositionofuncertaintyinbayesiandeeplearningforefficientandrisk-sensitivelearning.
arXiv:1710.07283[cs,stat],2017.
[27] Stefan Depeweg, José Hernández-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty for active learning and reliable reinforcement learning in stochastic
systems. 2017.
[28] Eduardo F. Camacho and Carlos Bordons Alba. Model Predictive Control. Advanced Text-
booksinControlandSignalProcessing.Springer-Verlag,2edition,2007.
[29] KefanDong,YupingLuo,andTengyuMa. Bootstrappingtheexpressivitywithmodel-based
planning. arXiv:1910.05927[cs,stat],2019.
[30] MarcPeterDeisenroth,DieterFox,andCarlEdwardRasmussen. Gaussianprocessesfordata-
efficientlearninginroboticsandcontrol. IEEETransactionsonPatternAnalysisandMachine
Intelligence,37(2):408–423,2015.
[31] AlexanderTschantz,AnilK.Seth,andChristopherL.Buckley.Learningaction-orientedmod-
elsthroughactiveinference. bioRxiv,page764969,2019.
[32] ThomasParrandKarlJ.Friston. Thediscreteandcontinuousbrain: Fromdecisionstomove-
ment—andbackagain. NeuralComputation,30(9):2319–2347,2018.
[33] ScottCheng-HsinYang,MátéLengyel,andDanielMWolpert. Activesensinginthecatego-
rizationofvisualpatterns. eLife,5,2019.
[34] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision Research,
49(10):1295–1306,2009.
[35] M.BerkMirza,RickA.Adams,KarlFriston,andThomasParr. Introducingabayesianmodel
ofselectiveattentionbasedonactiveinference. ScientificReports,9(1):1–22,2019.
[36] PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,Martin
Kronbichler, and Karl J Friston. Computational mechanisms of curiosity and goal-directed
exploration. eLife,8:e41703,2019.
[37] J.Beirlant, E.J.Dudewicz, andL.Györfi. Nonparametricentropyestimation: Anoverview.
InternationalJournalofMathematicalandStatisticalSciences,6,1997.
[38] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.
VIME:Variationalinformationmaximizingexploration. arXiv:1605.09674[cs,stat],2016.
[39] TrevorBarron,OliverObst,andHeniBenAmor. Informationmaximizingexplorationwitha
latentdynamicsmodel. arXiv:1804.01238[cs,stat],2018.
[40] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa,DavidSilver,andDaanWierstra. Continuouscontrolwithdeepreinforcementlearning.
arXiv:1509.02971[cs,stat],2019.
[41] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep
variationalreinforcementlearningforPOMDPs. arXiv:1806.02426[cs,stat],2018.
[42] Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep
variational bayes filters: Unsupervised learning of state space models from raw data.
arXiv:1605.06432[cs,stat],2016.
[43] LukaszKaiser,MohammadBabaeizadeh,PiotrMilos,BlazejOsinski,RoyH.Campbell,Kon-
radCzechowski,DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,AfrozMo-
hiuddin,RyanSepassi,GeorgeTucker,andHenrykMichalewski. Model-basedreinforcement
learningforatari. arXiv:1903.00374[cs,stat],2019.
[44] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Em-
bed to control: A locally linear latent dynamics model for control from raw images.
arXiv:1506.07365[cs,stat],2015.
[45] Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural net-
work dynamics for model-based deep reinforcement learning with model-free fine-tuning.
arXiv:1708.02596[cs],2017.
[46] YarinGal,RowanMcAllister,andCarlEdwardRasmussen. ImprovingPILCOwithbayesian
neuralnetworkdynamicsmodels. InData-EfficientMachineLearningworkshop,2016.
12
[47] GregoryKahn,AdamVillaflor,VitchyrPong,PieterAbbeel,andSergeyLevine. Uncertainty-
awarereinforcementlearningforcollisionavoidance. arXiv:1702.01182[cs],2017.
[48] Tung-Long Vuong and Kenneth Tran. Uncertainty-aware model-based policy optimization.
arXiv:1906.10717[cs,math,stat],2019.
[49] TimPearce,NicolasAnastassacos,MohamedZaki,andAndyNeely. Bayesianinferencewith
anchoredensemblesofneuralnetworks,andapplicationtoexplorationinreinforcementlearn-
ing. arXiv:1805.11324[cs,stat],2018.
[50] RichardS.SuttonandAndrewG.Barto. IntroductiontoReinforcementLearning. MITPress,
1stedition,1998.
[51] ShipraAgrawalandNavinGoyal. Analysisofthompsonsamplingforthemulti-armedbandit
problem. arXiv:1111.1797[cs],2012.
[52] MaartenSpeekenbrinkandEmmanouilKonstantinidis. Uncertaintyandexplorationinarest-
lessbanditproblem. TopicsinCognitiveScience,7(2):351–367,2015.
[53] IanOsband,BenjaminVanRoy,andZhengWen. Generalizationandexplorationviarandom-
izedvaluefunctions. arXiv:1402.0635[cs,stat],2016.
[54] MarcG.Bellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,andRemi
Munos. Unifying count-based exploration and intrinsic motivation. arXiv:1606.01868 [cs,
stat],2016.
[55] BradlyC.Stadie,SergeyLevine,andPieterAbbeel.Incentivizingexplorationinreinforcement
learningwithdeeppredictivemodels. arXiv:1507.00814[cs,stat],2015.
[56] SebastianB.Thrun. Efficientexplorationinreinforcementlearning,1992.
[57] Nuttapong Chentanez, Andrew G. Barto, and Satinder P. Singh. Intrinsically motivated re-
inforcement learning. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural
InformationProcessingSystems17,pages1281–1288.MITPress,2005.
[58] Jean-Arcady Meyer and Stewart W. Wilson. A possibility for implementing curiosity and
boredom in model-building neural controllers. In From Animals to Animats: Proceedings of
theFirstInternationalConferenceonSimulationofAdaptiveBehavior.MITP,1991.
[59] ManuelLopes,TobiasLang,MarcToussaint,andPierre-yvesOudeyer. Explorationinmodel-
basedreinforcementlearningbyempiricallyestimatinglearningprogress.InF.Pereira,C.J.C.
Burges,L.Bottou,andK.Q.Weinberger,editors,AdvancesinNeuralInformationProcessing
Systems25,pages206–214.CurranAssociates,Inc.,2012.
[60] Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in
reinforcementlearning. arXiv:1908.06976[cs],2019.
[61] SusanneStillandDoinaPrecup. Aninformation-theoreticapproachtocuriosity-drivenrein-
forcementlearning. TheoryinBiosciences=TheorieinDenBiowissenschaften,131(3):139–
148,2012.
[62] ShakirMohamedandDaniloJimenezRezende. Variationalinformationmaximisationforin-
trinsicallymotivatedreinforcementlearning. arXiv:1509.08731[cs,stat],2015.
[63] Todd Hester and Peter Stone. Intrinsically motivated model learning for developing curious
robots. ArtificialIntelligence,247:170–186,2017.
[64] D. V. Lindley. On a measure of the information provided by an experiment. The Annals of
MathematicalStatistics,27(4):986–1005,1956.
[65] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to be surprised: Optimal
bayesianexplorationindynamicenvironments. arXiv:1103.5708[cs,stat],2011.
[66] AtanasMirchev,BarisKayalibay,MaximilianSoelch,PatrickvanderSmagt,andJustinBayer.
Approximatebayesianinferenceinspatialenvironments. arXiv:1805.07206[cs,stat],2018.
[67] Keno Juechems and Christopher Summerfield. Where does value come from? Trends in
CognitiveSciences,23(10):836–850,2019.
13