arXiv:2208.05269v1  [cs.LG]  10 Aug 2022
IEEE COMMUNICA TIONS LETTERS 1
A Novel Resource Allocation for Anti-jamming in
Cognitive-UA Vs: an Active Inference Approach
Ali Krayani, Graduate Student Member , IEEE, Atm S. Alam, Member , IEEE, Lucio Marcenaro, Senior
Member , IEEE, Arumugam Nallanathan, F ellow , IEEE, and Carlo Regazzoni, Senior Member , IEEE
Abstract—This work proposes a novel resource allocation strat-
egy for anti-jamming in Cognitive Radio using Active Infere nce
(AIn), and a cognitive-UA V is employed as a case study . An
Active Generalized Dynamic Bayesian Network (Active-GDBN )
is proposed to represent the external environment that join tly
encodes the physical signal dynamics and the dynamic interaction
between UA V and jammer in the spectrum. W e cast the action
and planning as a Bayesian inference problem that can be solv ed
by avoiding surprising states (minimizing abnormality) du ring
online learning. Simulation results verify the effectiven ess of the
proposed AIn approach in minimizing abnormalities (maximizing
rewards) and has a high convergence speed by comparing it wit h
the conventional Frequency Hopping and Q-learning.
Index T erms—Active Inference, Resource Allocation, General-
ized Bayesian Filtering, Anti-jamming, Cognitive Radio.
I. I N T RO D U CT IO N
With the integration of Unmanned Aerial V ehicles (UA Vs),
Wireless Communications (WCs) are more prone to terrestria l
jammers due to the high heterogeneity and dominant Line-
of-Sight (LoS) links [1]. Jammers cause damage to commu-
nication and degrade the system’s performance. Therefore, it
is crucial to develop an anti-jamming strategy to reach robu st
connectivity and improve communication security.
Cognitive Radio is a key technology to accomplish in-
telligent resource management in jamming scenarios. In de-
tecting the existence of the jammers and avoiding jamming
attacks, conventional anti-jamming solutions that use ﬁxe d
transmission patterns can be used. However, they are unable
to deal with dynamic jamming patterns in complicated radio
environments with high uncertainty, and unpredictable jam -
ming behaviours [2]. Recently, Reinforcement Learning (RL )
has attracted much attention in WCs to design anti-jamming
solutions in complex environments. RL methods such as Q-
learning (QL) [3] are used to deal with different types of
jammers. However, they suffer from slow convergence if the
state and action spaces are large, which leads to anti-jammi ng
performance degradation. Deep-QL has been proposed in [4]
to overcome that issue and learn efﬁcient defence policy.
RL methods are based on a reward signal coming from the
environment as a feedback to evaluate the performed action.
However, deﬁning a proper reward function in complex and
dynamic environments is a big challenge [5]. Active Inference
Ali Krayani is with DITEN, University of Genoa, 16145 Genoa, Italy , and
also with EECS, Queen Mary University of London, London E1 4N S, U.K.
(e-mail: ali.krayani@edu.unige.it, a.krayani@qmul.ac. uk). Lucio Marcenaro
and Carlo Regazzoni are with DITEN, University of Genoa, 161 45 Genoa,
Italy , and the Italian National Consortium for T elecommuni cations (CNIT).
(e-mails: {lucio.marcenaro, carlo.regazzoni}@unige.it ). Atm S. Alam and
Arumugam Nallanathan are with EECS, Queen Mary University o f London,
London E1 4NS, U.K. (e-mails: {a.alam, a.nallanathan}@qmu l.ac.uk).
(AIn) [6] can overcome this challenging task by replacing
reward functions with prior beliefs about desired sensory
signals received from the environment. Thus, AIn agent can
learn to describe how it expects itself to behave without
getting a feedback from the environment. AIn is a promising
emerging theory from cognitive neuroscience; it provides a
theoretical Bayesian framework that supports how biologic al
agents perceive and act in the real world through the free-
energy principle and offers an alternative to RL.
This letter proposes an AIn framework as a novel resource
allocation strategy for anti-jamming and studies the Cogni tive-
UA V based scenario. Under the AIn framework, the Cognitive-
UA V is endowed with a joint internal representation (genera -
tive model) of the external environment, encoding the physi cal
signal and the available physical resources jointly. This e nables
encoding the dynamic interaction between the UA V and the
jammer in the spectrum. The objective is to learn the best
set of actions performed by the UA V as interaction with a
jammer that leads to the minimum surprise (positive reward) .
Such a representation goes over the necessity of mapping
actions to signals’ states directly (unlike the RL approach )
and modelling them over a continuous state-space, which can
be a complicated task in RL. There are four main rationals to
use AIn approach over RL ([3], [4]): i) AIn operates in a pure
belief-based setting allowing one to seek information abou t
the environment and resolve uncertainty in a Bayesian-opti mal
fashion. ii) AIn enables speeding up the learning process by
performing multiple updates simultaneously while adaptin g to
the dynamic changes in the spectrum. iii) There is a dynamic
balance between the exploration and exploitation due to the
pure belief-based mode, while RL is driven by a value functio n
that updates a single state action at each step. iv) In AIn
the reliance on an explicit reward signal coming from the
environment is not necessary; the reward is substituted by
Generalized Errors that can be treated as self-information to
avoid surprising states (i.e., states under attack) and rea ch the
equilibrium. T o our best knowledge, this is the ﬁrst work that
adopts AIn for anti-jamming in WCs.
II. S Y S T E M MO D E L A N D PRO BL E M FO RM U L AT IO N
Consider a cellular-connected UA V communicating with its
respective Ground Base Station (GBS) to receive the tele-
commands during a given mission of duration T over the
Command and Control (C2) link which does not exceed a
data rate of 100 Kbps [7], while a malicious terrestrial jammer
transmits jamming signals with the intention of disturbing
the legitimate UA V communications. The jammer may adopt
IEEE COMMUNICA TIONS LETTERS 2
constant, random or sweep jamming patterns during a certain
experience. The UA V , GBS and jammer are denoted as /u1D462, /u1D454
and /u1D457, respectively. The 3D coordinate of GBS and jammer are
ﬁxed at /u1D490/u1D488= [/u1D465/u1D454, /u1D466/u1D454, /u1D467/u1D454] and /u1D491/u1D48B= [/u1D465/u1D457, /u1D466/u1D457, /u1D467/u1D457], respectively,
while the time-varying coordinate of UA V at time instant /u1D461
is deﬁned as /u1D492/u1D496
/u1D495= [/u1D465/u1D462
/u1D461, /u1D466/u1D462
/u1D461, /u1D467/u1D462
/u1D461]. The path-loss model from
the ground equipment (i.e., GBS or jammer) to UA V follows
the cellular to UA V path-loss model, which can be expressed
according to [8] as: PLe,u
t (/u1D451/u1D461, /u1D703/u1D461) = PLter (/u1D451/u1D461) + /u1D702(/u1D703/u1D461) + /u1D712(/u1D703/u1D461),
where /u1D452∈ { /u1D454, /u1D457}, PLter
t (/u1D451/u1D461) = 10/u1D6FClog(/u1D451/u1D461) is the terrestrial
path-loss of the point beneath the UA V , /u1D6FCis the terrestrial path-
loss exponent that depends on the propagation environment
and /u1D451/u1D461=
√
(/u1D465/u1D462
/u1D461− /u1D465/u1D452)2 + ( /u1D466/u1D462
/u1D461− /u1D466/u1D452)2 is the 2D distance between
/u1D452and /u1D462. In addition, /u1D702(/u1D703/u1D461) = /u1D436(/u1D703/u1D461− /u1D7030 ) exp (− /u1D703/u1D461−/u1D7030
/u1D437
) + /u1D7020 is
the excess aerial path-loss and /u1D712(/u1D703/u1D461) is a zero-mean Gaussian
variable with an angle-dependent standard deviation descr ibing
the shadowing effect such that /u1D712(/u1D703/u1D461) ∼ N ( 0, /u1D70E(/u1D703/u1D461)=/u1D44E/u1D703/u1D461+ /u1D70E0 ),
where /u1D436is the excess path-loss scaler, /u1D437is the angle scaler,
/u1D7030 is the angle offset, /u1D7020 is the excess path-loss offset, /u1D44Eis the
UA V shadowing slope, /u1D703/u1D461= arctan (/u1D467/u1D462
/u1D461−/u1D467/u1D452
/u1D461
/u1D451/u1D461
) is the depression
angle and /u1D70E0 is the UA V shadowing offset. The GBS assigns
one Physical Resource Block (PRB) to the UA V each /u1D461where
C2 data are transmitted [9]. The set of available links is
denoted as RB ={ /u1D4531 , . . . , /u1D453/u1D45B, . . . , /u1D453/u1D441}, 1 ≤ n ≤ /u1D441, where
|RB| =/u1D441is the total number of available PRBs that depends
on the channel bandwidth /u1D435/u1D44A. T o cope with the malicious
jamming, the UA V aims to learn the best allocation strategy
online by selecting the proper PRBs that are not targeted by t he
jammer while interacting with the environment and sending
updated information to GBS to adapt to the environmental
dynamic changes. Denote H0 and H1 as the hypotheses of
the absence (i.e., UA V and jammer selected different PRBs)
and presence (i.e., UA V and jammer selected the same PRB)
of the jammer, respectively. The complex signal that is re-
ceived at the UA V at time instant /u1D461and over /u1D453/u1D45Bis given as
/u1D45F/u1D461,/u1D453/u1D45B= ℎ/u1D454,/u1D462
/u1D461,/u1D453/u1D45B
/u1D465/u1D462
/u1D461,/u1D453/u1D45B
+ /u1D463/u1D461and /u1D45F/u1D461,/u1D453/u1D45B= ℎ/u1D454,/u1D462
/u1D461,/u1D453/u1D45B
/u1D465/u1D462
/u1D461,/u1D453/u1D45B
+ ℎ /u1D457,/u1D462
/u1D461,/u1D453/u1D45B
/u1D465/u1D457
/u1D461,/u1D453/u1D45B
+ /u1D463/u1D461at
hypotheses H0 and H1 , respectively, where /u1D465/u1D462
/u1D461,/u1D453/u1D45B
denotes the
C2 signal, ℎ/u1D454,/u1D462
/u1D461,/u1D453/u1D45B
= 1/PLt /u1D454,/u1D462is the channel gain from GBS to
UA V ,/u1D465/u1D457
/u1D461,/u1D453/u1D45B
stands for the jammer’s signal, ℎ /u1D457,/u1D462
/u1D461,/u1D453/u1D45B
= 1/PLt /u1D457,/u1D462
is the channel gain from jammer to UA V and /u1D463/u1D461is the
random noise. The corresponding SINR at the UA V is given
by /u1D6FE/u1D461= /u1D443/u1D462
/u1D461ℎ/u1D454,/u1D462
/u1D461,/u1D453/u1D45B
/(/u1D6FC/u1D443/u1D457
/u1D461ℎ /u1D457,/u1D462
/u1D461,/u1D453/u1D45B
+ /u1D70E2 ), where /u1D443/u1D462
/u1D461is the transmitted
power, /u1D443/u1D457
/u1D461is the jammer power, whose presence is denoted by
/u1D6FCwhich is equal to 0 under H0 and equals to 1 under H1.
The anti-jamming defense problem can be formulated as
a partially observable Markov decision process (POMDP)
since the spectrum is only partially observable to the UA V .
A discrete-time POMDP that models the relationship between
the UA V and its environment can be described as 7-element
tuple ( /u1D47A, /u1D47F, A, P/u1D496
/u1D749, P/u1D48B
/u1D749, /u1D6B7/u1D482/u1D496
/u1D749, ˜/u1D481/u1D495,/u1D487/u1D48F), where /u1D47Aand /u1D47F are sets
of the environmental hidden states, A is a set of actions where
action is PRB selection ( /u1D44E/u1D461∈ RB ), P/u1D496
/u1D749and P/u1D48B
/u1D749are the time-
varying transition models for UA V and jammer, respectively .
/u1D6B7/u1D482/u1D496
/u1D749 is the AIn-table that encodes the state-action couple and
˜/u1D481/u1D495,/u1D487/u1D48Fare the observations received at each /u1D461over /u1D453/u1D45B. During
the ofﬂine training, UA V learns a dynamic model M encoding
the dynamic rules that generate desired sensory signals (i. e.,
without jamming interference). During the active inferenc e
process (i.e., online learning), UA V predicts the environm en-
tal hidden states characterized by the posterior distribut ions
P(/u1D460∗
/u1D461∈/u1D47A|/u1D467/u1D461∈ ˜/u1D481/u1D495,/u1D487/u1D48F, M) and P(/u1D465∗
/u1D461∈ /u1D47F|/u1D467/u1D461∈ ˜/u1D481/u1D495,/u1D487/u1D48F, M) based on
a prior belief (encoded in M) and infers the actions most
likely to generate preferred sensory signals (i.e., clean s ig-
nals without jamming interference). Then, UA V can evaluate
the situation after receiving the current observation /u1D467/u1D461and
calculate the similarity between predictions and observat ions
using a probabilistic distance D (i.e., abnormality indicator).
If the similarity is high (i.e., H0 ), UA V can understand that the
selected action has led to desired states and to the receptio n
of desired signals. If the similarity is low (i.e., H1), UA V
can understand that the selected action is a bad action and
updates /u1D6B7/u1D482/u1D496
/u1D749 accordingly to avoid selecting actions that lead
to surprising states (i.e., high abnormality). Therefore, while
acting and sensing the spectrum, the UA V aims to minimise
the cumulative abnormality:
min/u1D44E/u1D461
T/summationdisplay.1
/u1D461=1
D
(
P (/u1D460∗
/u1D461|/u1D467/u1D461,M) ,P (/u1D467/u1D461|/u1D460∗
/u1D461,M)
)
. (1)
It is to note that (1) is equivalent to maximize the SINR.
III. P RO P O S E D AN T I-JA M M IN G M E T H O D
A. Radio Environment Representation
W e assume that the environment is described by a
Generalized-state-space model, comprised of:
˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B= F ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B) + ˜/u1D464/u1D461,/u1D453/u1D45B, (2)
˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B= /u1D434˜/u1D44B/u1D462
/u1D461−1,/u1D453/u1D45B+ /u1D435/u1D448˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
+ ˜/u1D464/u1D461,/u1D453/u1D45B, (3)
˜/u1D44D/u1D461,/u1D453/u1D45B= /u1D43B˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B+ /u1D43B˜/u1D44B/u1D457
/u1D461,/u1D453/u1D45B+ ˜/u1D710/u1D461,/u1D453/u1D45B, (4)
In (2), ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
are discrete random variables (or Generalized
superstates GSS) describing the discrete clusters of the UA Vs’
C2 signals that evolve according to (2) where F(.) is a non-
linear function describing the signals’ dynamic transitio ns
among the discrete variables and its evolution over time
at a speciﬁc PRB ( /u1D453/u1D45B) and ˜/u1D464/u1D461,/u1D453/u1D45Bis a Generalized process
noise such that, ˜/u1D464/u1D461,/u1D453/u1D45B∼N (0, Σ ˜/u1D464/u1D461,/u1D453/u1D45B). The dynamic model in
(3) explains the dynamic evolution of the continuous random
variables ˜/u1D44B/u1D461,/u1D453/u1D45B(or Generalized states GS) where /u1D434∈R2/u1D451,2/u1D451,
/u1D435∈R2/u1D451,2/u1D451are the dynamic model and control model matrices,
respectively, and /u1D448˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
is the control vector. The observation
model is given in (4) where ˜/u1D44D/u1D461,/u1D453/u1D45B∈R2/u1D451is the generalized
observations including the signals’ features in terms of /u1D43C
and /u1D444components and the 1/u1D460/u1D461-order temporal derivatives ( /dotacc/u1D43C,
/dotacc/u1D444) where /u1D451is the space dimensionality. W e assume that
each sensory signal is a linear combination of one hidden
GS ( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
) affected by additive random noise in a normal
situation (i.e., under H0 ) and by additional interference ( ˜/u1D44B/u1D457
/u1D461,/u1D453/u1D45B
)
caused by the jammer in an abnormal situation (i.e., under
H1). ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
and ˜/u1D44B/u1D457
/u1D461,/u1D453/u1D45B
are the UA V’s GS and the jammer’s GS
(that is caused by ˜/u1D446/u1D457
/u1D461,/u1D453/u1D45B
), respectively. /u1D43B∈R2/u1D451,2/u1D451maps hidden
states to observations, /u1D453/u1D45Bis the /u1D45B-th PRB where /u1D453/u1D45B∈RB and
˜/u1D710/u1D461,/u1D453/u1D45B∼N (0, Σ ˜/u1D710/u1D461,/u1D453/u1D45B).
IEEE COMMUNICA TIONS LETTERS 3
Fig. 1. Graphical representation of the proposed Active-GD BN. The top-
level of the hierarchy stands for the active states ( /u1D44E/u1D462
/u1D461−1 ) representing the
actions that the UA V can perform. The UA V can predict the cons equences
of the performed actions that affect the hidden environment al states ( ˜St,fn ,
˜Xt,fn ) causing sensory signals ( ˜Z/u1D461,/u1D453/u1D45B). ˜St,fn are discrete variables representing
the clusters and ˜Xt,fn are continuous variables representing the dynamics of
the physical signal inside a certain cluster . Edges represe nt the conditional
dependencies among random variables at multiple levels. Ea ch level of the
hierarchy holds beliefs about the variables of the level bel ow . Beliefs are
signalled via predictive messages in a top-down manner and c ompared against
sensory signals, resulting in multi-level abnormality ind icators and generalized
errors that are fed back via diagnostic messages in a bottom- up manner .
B. Ofﬂine learning of desired observations
During training, we assume that the jammer is absent and
the UA V aims to learn the dynamics of the desired obser-
vations (i.e., C2 signals without jamming interference) wh ile
sensing the spectrum. UA V starts perceiving the surroundin gs
by partially sensing the spectrum, supposing that no signal s
are present and observations are subject to a stationary noi se
process that evolves according to static rules. UA V relays
on (
3) to predict the continuous signal’s state where the
force at sensing PRB ( /u1D453/u1D45B) is /u1D448˜/u1D446/u1D461,/u1D453/u1D45B
=0, as no rules have been
discovered yet. In case of active transmissions in /u1D453/u1D45B, UA V
detects abnormalities all the time and calculates the Gener -
alized Errors (GEs) projected on the GS space as follows:
˜E ˜Xu
t,fn
=
[ ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
, P( /dotaccE ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)
]
=
[ ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
, /u1D43B−1 ˜E ˜/u1D44D/u1D461,/u1D453/u1D45B
]
, where /dotaccE ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
is
the difference between predictions and observations that c ap-
ture the dynamics of the signals present inside the spectrum
and should be applied to ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
and ˜E ˜/u1D44D/u1D461,/u1D453/u1D45B
= ˜/u1D44D/u1D461,/u1D453/u1D45B− /u1D43B˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
. GEs
can be clustered in an unsupervised manner using the Growing
Neural Gas (GNG) to learn the top level of abstraction
(semantic level). GNG produces a set of GSS (or clusters) en-
coding the GEs into discrete regions described by the set ˜/u1D47A/u1D496
/u1D487/u1D48F
,
such that: ˜/u1D47A/u1D496
/u1D487/u1D48F
={ ˜/u1D446/u1D462
1,/u1D453/u1D45B
, ˜/u1D446/u1D462
2,/u1D453/u1D45B
, . . . , ˜/u1D446/u1D462
/u1D440,/u1D453/u1D45B
}, where /u1D440is the total
number of clusters associated with a speciﬁc PRB. Analysing
the signal’s dynamic transitions among the GSS and how
they vary with time allows estimating the time-varying tran -
sition probabilities /u1D70B/u1D462
/u1D456/u1D453/u1D45B| /u1D457/u1D453/u1D45B,/u1D70F=P( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
=/u1D456| ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
= /u1D457, /u1D70F) which is
encoded in the time-varying transition matrix Π/u1D462
/u1D453/u1D45B,/u1D70Fwhere
/u1D456, /u1D457∈ ˜/u1D47A/u1D496
/u1D487/u1D48F
. Moreover, each discrete variable ˜/u1D446/u1D462
/u1D45A,/u1D453/u1D45B
∈ ˜/u1D47A
/u1D462
/u1D453/u1D45Bis asso-
ciated with statistical proprieties as generalized mean ˜/u1D707˜/u1D446/u1D462
/u1D45A,/u1D453/u1D45B
and covariance Σ ˜/u1D446/u1D462
/u1D45A,/u1D453/u1D45B
. During ofﬂine learning, UA V has been
trained to learn and encode the dynamic rules that generate
desired sensory signals (i.e., without jamming attacks) us ing
multiple observations (over multiple RBs).
C. Active Inference stage (online learning)
The hierarchical dynamic models formulated in terms of
stochastic processes as deﬁned in (
2),(3),(4) are structured in
an Active Generalized Dynamic Bayesian Networks (Active-
GDBN) depicted in Fig.1. The Active-GDBN allows to solve
the POMDP to ﬁnd the best set of actions by predicting the
situation the UA V could encounter in the future, conditione d
on the actions it executes. Thus, AIn provides a way, through
planning as inference, to form beliefs about the future and
describe the causal relationship among actions, hidden sta tes
and outcomes at multiple levels.
1) Initialization: P/u1D496
/u1D749and P/u1D48B
/u1D749are the /u1D441×/u1D441time-varying
matrices encoding the possible transitions among the /u1D441avail-
able resources performed by the UA V and encoding the UA V’s
belief about the possible actions that the jammer can perfor m,
respectively. Since there is no a priori information concer ning
the jammer’s behaviour inside the spectrum, the probabilit y
entries in both P/u1D496
/u1D749and P/u1D48B
/u1D749are initially assigned equal values:
P/u1D496/u1D749=










P (Π/u1D462
/u1D4531 | /u1D4531 ,/u1D70F) ... P (Π/u1D462
/u1D4531 | /u1D453/u1D441,/u1D70F)
.
.
.
...
.
.
.
P (Π/u1D462
/u1D453/u1D441| /u1D4531 ,/u1D70F) ... P (Π/u1D462
/u1D453/u1D441| /u1D453/u1D441,/u1D70F)










,P /u1D48B
/u1D749=











P (Π /u1D457
/u1D4531 | /u1D4531 ,/u1D70F) ... P (Π /u1D457
/u1D4531 | /u1D453/u1D441,/u1D70F)
.
.
.
...
.
.
.
P (Π /u1D457
/u1D453/u1D441| /u1D4531 ,/u1D70F) ... P (Π /u1D457
/u1D453/u1D441| /u1D453/u1D441,/u1D70F)











,
(5)
where P(Π/u1D462
/u1D453/u1D45F| /u1D453/u1D45E,/u1D70F)= 1
/u1D441, P(Π/u1D457
/u1D453/u1D45F| /u1D453/u1D45E,/u1D70F)= 1
/u1D441∀/u1D45F, /u1D45E∈RB . /u1D6B7/u1D482/u1D496
/u1D749∈R/u1D441,/u1D441
is a time-varying matrix encoding the probabilistic de-
pendencies between states and actions representing the
link /u1D44E/u1D462
/u1D461−1→ ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
in the Active-GDBN that describes
P(/u1D44E/u1D462
/u1D461−1= /u1D453/u1D456| ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) and deﬁned as:
/u1D6B7/u1D482/u1D496
/u1D749 =










P (/u1D44E1 = /u1D4531 | ˜/u1D446/u1D462
/u1D461−1,/u1D4531
) ... P (/u1D44E/u1D441= /u1D453/u1D441| ˜/u1D446/u1D462
/u1D461−1,/u1D4531
)
.
.
.
...
.
.
.
P (/u1D44E1 = /u1D4531 | ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D441
) ... P (/u1D44E/u1D441= /u1D453/u1D441| ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D441
)










, (6)
where P(/u1D44E/u1D462
/u1D461−1= /u1D453/u1D456| ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
)= 1
/u1D441∀/u1D456, /u1D458∈RB . UA V’s action depends
on the state-action couple encoded in Π/u1D44E/u1D462
/u1D70Fand on its belief
about the presence of the jammer in the radio spectrum
encoded in P /u1D457
/u1D70F.
2) Action selection process : Initially, UA V performs ran-
dom sampling to select the actions during the 1/u1D460/u1D461iteration as
every possible action has the same probability ( 1
/u1D441) of being
chosen. The selected action /u1D44E/u1D462
/u1D461−1 indicates what will be the
next hidden state ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
according to P( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
, /u1D44E/u1D462
/u1D461−1). ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
encodes the predicted cluster of the model and the activated
PRB ( /u1D453/u1D45B).
In the successive iterations, ﬁrst, UA V predicts the future
activity of the jammer implicitly according to P/u1D462
/u1D70F. Then, it can
adjust the action selection step by skipping the risky resou rces
(i.e., resources expected with high probability to be targe ted by
the jammer in the near future). The action selection procedu re
depends on a certain policy adopted by the UA V according to:
/u1D44E/u1D462∗
/u1D461−1 = argmax ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
,P/u1D462/u1D70F( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) /u1D70B(/u1D44E/u1D462
/u1D461−1), (7)
where /u1D70B(/u1D44E/u1D462
/u1D461−1)=P(/u1D44E/u1D462
/u1D461−1 | ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) is a speciﬁc row in Π/u1D44E/u1D462
/u1D70Fand
P/u1D462
/u1D70F( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) is a speciﬁc row selected from ( P/u1D462
/u1D70F) represent-
ing the dynamic model associated with ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) where the
jammer’s transitions are implicitly encoded. The model has
prior belief about how a certain state ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
) will evolve
IEEE COMMUNICA TIONS LETTERS 4
into another ( ˜/u1D446/u1D462∗
/u1D461,/u1D453/u1D458
) depending on the chosen action ( /u1D44E/u1D462∗
/u1D461−1)
according to: P( ˜/u1D446/u1D462∗
/u1D461,/u1D453/u1D458
|/u1D44E/u1D462∗
/u1D461−1, ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
), where ˜/u1D446/u1D462∗
/u1D461,/u1D453/u1D458
is the expected
state associated with the selected action.
3) Perception and joint state-prediction : After selecting
the action that indicates the chosen PRB, UA V can rely
on the corresponding transition matrix ( Π/u1D462
/u1D453/u1D45F| /u1D453/u1D45E,/u1D70F) to per-
form the predictions by employing the Modiﬁed Markov
Jump Particle Filter (M-MJPF) [
9], that uses a combina-
tion of Particle Filter (PF) and a bank of Kalman Fil-
ters (KFs). PF starts by propagating /u1D43Fparticles equally
weighted based on the proposal density encoded in Π/u1D462
/u1D453/u1D45F| /u1D453/u1D45E,/u1D70F,
such that: < ˜/u1D446/u1D462,/u1D459
/u1D461,/u1D453/u1D45B
, /u1D44A/u1D459
/u1D461>∼</u1D70B/u1D462
/u1D456/u1D453/u1D45B| /u1D457/u1D453/u1D45B,/u1D70F, 1
/u1D43F>. For each particle
˜/u1D446/u1D462,/u1D459
/u1D461,/u1D453/u1D45B
, a KF is employed to predict ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
. The prediction
at this level is driven by the higher level as pointed out
in (
3) (where /u1D448˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
= ˜/u1D707˜/u1D446/u1D462,/u1D459
/u1D461,/u1D453/u1D45B
) which can be expressed as
P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D44B/u1D462
/u1D461−1,/u1D453/u1D45B
, ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
). The posterior probability associated
with ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
is given by: /u1D70B( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)=P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
, ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D44D/u1D461−1,/u1D453/u1D45B).
Once a new sensory signal is received, diagnostic messages
propagate in bottom-up to adjust the expectations and updat e
belief in hidden variables. Thus, the posterior can be updat ed
using: P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
, ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D44D/u1D461,/u1D453/u1D45B)=/u1D70B( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)/u1D706( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
). In addition,
the likelihood message /u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) can be used to update the
particles’ weights according to: /u1D44A/u1D459
/u1D461=/u1D44A/u1D459
/u1D461/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
), where:
/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)=/u1D706( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)=P( ˜/u1D44D/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
),
and P( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
| ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)∼N ( /u1D707˜/u1D446/u1D462
/u1D45A,/u1D453/u1D45B
, Σ ˜/u1D446/u1D462
/u1D45A,/u1D453/u1D45B
) denotes a multivariate
Gaussian distribution. Also, GE ( ˜E ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) at the superstate
level conditioned on transiting from ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
can be expressed
as: ˜E˜Su
t,fn
=
[ ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
, /dotaccE ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
]
, where /dotaccE ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
is an aleatory
variable whose probability density function is given by
P( /dotaccE ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)=/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) − /u1D70B( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) representing the new force that
can be used to update P/u1D462
/u1D70Fand thus improve future predictions.
4) Abnormality measurements : In order to evaluate to
what extent the current signal’s evolution at the discrete l evel
matches the predicted one based on the learned and encoded
dynamics in the model, we used an abnormality indicator
(/u1D6BC˜/u1D47A/u1D496
/u1D495, /u1D487/u1D48F
) based on the Symmetric Kullback-Leibler ( SKL )
Divergence ( /u1D437/u1D43E/u1D43F) [
9]. /u1D6BC˜/u1D47A/u1D496
/u1D495, /u1D487/u1D48F
calculates the similarity between
the two messages that represent discrete probability distr ibu-
tions entering to node ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
, namely, /u1D70B( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) and /u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
), it
is associated with ˜E˜Su
t,fn
and formulated as:
/u1D6BC˜/u1D47A/u1D496
/u1D495, /u1D487/u1D48F
=
/summationdisplay.1
/u1D456∈S
P/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B= /u1D456)/u1D437/u1D43E/u1D43F
(/u1D70B( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B) | |/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B)) +
/summationdisplay.1
/u1D456∈S
P/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B= /u1D456)/u1D437/u1D43E/u1D43F
(/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B) | | /u1D70B( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B)) ,
(8)
where P/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) is the probability of occurrence of each super-
state picked from the histogram at time instant /u1D461and calculated
as follows: P/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)=
/u1D453/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B=/u1D456)
/u1D441 , where /u1D453 /u1D45F(.) is the frequency
of occurrence of a speciﬁc superstate /u1D456, /u1D441is the total number
of particles propagated by PF , and S is the set consisting of all
winning particles, such that: S = { /u1D456|P/u1D45F( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
) > 0} , /u1D456∈ /u1D47A/u1D496
/u1D487/u1D48F
.
Likewise, it is possible to understand how much the obser-
vation supports the predictions at the GS level using:
/u1D6BC˜/u1D47F/u1D496
/u1D495, /u1D487/u1D48F
= − ln
(
BC (/u1D70B( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B), /u1D706( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B)) )
, (9)
where BC( .) =
∫ √
/u1D70B( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)/u1D706( ˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
)/u1D451˜/u1D44B/u1D462
/u1D461,/u1D453/u1D45B
is the Bhat-
tacharyya coefﬁcient and /u1D6BC˜/u1D47F/u1D496
/u1D495, /u1D487/u1D48F
is associated with ˜E ˜Xu
t,fn
.
5) Updating of action selection process: After acting in the
environment, UA V can save the consequence of the chosen
action (i.e., the transition from ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
to ˜/u1D446/u1D462∗
/u1D461,/u1D453/u1D458
) in P/u1D462
/u1D70Fand
evaluate how much the sensory outcomes support predictions
and thus evaluate if the performed action was good or bad by
using the abnormality measurements deﬁned in (
8) and ( 9). In
addition, it is possible to calculate the GE ( ˜E/u1D44E/u1D462
/u1D461−1 ) during ab-
normal situations to adapt UA V’s strategy in selecting acti ons
and understand how it should behave in the future to avoid the
jammer. ˜E/u1D44E/u1D462
/u1D461−1 is the difference between observation and expec-
tation which can be expressed as: ˜E/u1D44E/u1D462
/u1D461−1 =
[
/u1D44E/u1D462∗
/u1D461−1, /dotaccE/u1D44E/u1D462
/u1D461−1
]
, where
/dotaccE/u1D44E/u1D462
/u1D461−1 depicts an aleatory variable representing the new force
that should be applied to update /u1D745(/u1D44E/u1D462
/u1D461−1) and its probability
density function is given by P( /dotaccE/u1D44E/u1D462
/u1D461−1 )=/u1D706(/u1D44E/u1D462
/u1D461−1) − /u1D745(/u1D44E/u1D462
/u1D461−1) that
can be used as a metric alternative to the reward in RL. /u1D706(/u1D44E/u1D462
/u1D461−1)
is the diagnostic message travelling from ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
towards /u1D44E/u1D462
/u1D461−1
and deﬁned as: /u1D706(/u1D44E/u1D462
/u1D461−1 )=/u1D706( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
)P( ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
|/u1D44E/u1D462
/u1D461−1) representing a
discrete probability distribution that holds information about
the observed sensory signal and encoding the probabilities
about how the states ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
belonging to the available frequen-
cies change based on the evidence, it is given by:
/u1D706(/u1D44E/u1D462
/u1D461−1 ) =
{
P/u1D70F−1 ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
) − /u1D6FE∗, if /u1D44E/u1D462
/u1D461−1 = /u1D44E/u1D462∗
/u1D461−1,
P/u1D70F−1 ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D45B
) + /u1D6FE∗
/u1D441−1 , if /u1D44E/u1D462
/u1D461−1 ≠ /u1D44E/u1D462∗
/u1D461−1, (10)
where /u1D6FEdepends on the GE ( ˜E˜Su
t,fn
), that is: /u1D6FE=/u1D6FE∗ if ˜E ˜/u1D446/u1D461,/u1D453/u1D458
≥th,
and /u1D6FE=0 if ˜E ˜/u1D446/u1D461,/u1D453/u1D458
<th, where th is the threshold indicating
whether the radio situation is normal or abnormal and the
value of /u1D6FE∗ depends on the abnormality indicators deﬁned in
(
8) and ( 9). Hence, GE ( ˜E/u1D44E/u1D462
/u1D461−1 ) is proportional to ˜E˜Su
t,fn
due to
the messages propagated from lower level towards the higher
levels, such that ˜E/u1D44E/u1D462
/u1D461−1 = /u1D453( ˜E ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
). When the UA V get surprised
by the sensory outcomes after performing a certain action, i t
can use the prediction error signal to update its belief abou t
the jammer’s transition model to improve future actions. Th e
core idea is that the user occupying a piece of the spectrum
should minimize the abnormality (surprise) associated wit h
ﬁnding itself in unlikely states (states under attack). Jam mer’s
dynamic model ( P /u1D457
/u1D70F) can be updated following:
P /u1D457
/u1D70F(. , ˜/u1D446/u1D457
/u1D461,/u1D453/u1D45B
) = P /u1D457
/u1D70F−1 (. , ˜/u1D446/u1D457
/u1D461,/u1D453/u1D45B
) − P( /dotaccE/u1D462
/u1D44E/u1D461−1 ), (11)
In an abnormal situation, the user and jammer share the same
RB, which means they performed the same action. Thus,
the user should update Π/u1D44E/u1D462
/u1D70Fby decreasing the probability of
selecting that action as follows:
/u1D70B∗(/u1D44E/u1D462
/u1D461−1) = /u1D70B(/u1D44E/u1D462
/u1D461−1) + P( /dotaccE/u1D462
/u1D44E/u1D461−1 ), (12)
and update P/u1D462
/u1D70Fby decreasing the probability of transiting to
˜/u1D446/u1D462
/u1D461,/u1D453/u1D458
from ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458
after choosing action /u1D44E/u1D462∗
/u1D461−1 using the GE
( ˜E˜Su
t,fn
) following:
P/u1D462
/u1D70F( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458, ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B) = P/u1D462
/u1D70F−1 ( ˜/u1D446/u1D462
/u1D461−1,/u1D453/u1D458, ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B) + P( /dotaccE ˜/u1D446/u1D462
/u1D461,/u1D453/u1D45B
). (13)
IEEE COMMUNICA TIONS LETTERS 5
0 50 100 150 200 250 300 350 400 450 500
0
500
1000
1500
0
2000
4000
6000
8000
10000
(a) Constant Jammer
0 50 100 150 200 250 300 350 400 450 500
0
500
1000
1500
0
2000
4000
6000
8000
10000
(b) Random Jammer
0 50 100 150 200 250 300 350 400 450 500
400
600
800
1000
1200
0
2000
4000
6000
(c) Sweep Jammer
Fig. 2. Performance comparison of cumulative reward and abn ormality ( SK L ) with the proposed AIn, FH and QL under different jamming strategies.
0 50 100 150 200 250 300 350 400 450 500
0
500
1000
1500
2000
(a) Constant Jammer
0 50 100 150 200 250 300 350 400 450 500
-500
0
500
1000
1500
2000
(b) Random Jammer
0 50 100 150 200 250 300 350 400 450 500
500
1000
1500
2000
(c) Sweep Jammer
Fig. 3. Performance comparison of cumulative SINR with the p roposed AIn, FH and QL under different jamming strategies.
IV . R E S U LT S A N D DIS CU S S IO N
T o evaluate the performance of the proposed AIn approach
for anti-jamming, following three types of jammers are con-
sidered in the simulation: 1) Constant jammer that acts on
statistically pre-conﬁgured channels; 2) Sweep jammer that
attacks by sweeping among the available PRBs at each time
slot; and 3) Random jammer that selects uniformly random
actions to attack the available PRBs. The simulation settin gs
are as: BW= 10MHz; FDD; sub-carrier spacing of 15 KHz;
number of PRBs per BW is 50; sampling frequency of 1.92
MHz; /u1D441/u1D439/u1D439/u1D447of 128; 7 OFDM symbols per slot; normal CP;
SNR of 15/u1D451/u1D435; QPSK for C2 and jamming signal; jamming
to signal power ratio (JSR) of 6dB; and a total of 200 radio
frames. In addition, the propagation environment is a typic al
suburban, mean aerial speed is 4.8m/s, BS height is 30m,
UA V height is 60m and the channel model parameters [
8] are
/u1D6FC=3.04, /u1D70E0 =8.52, /u1D436= − 23.29, /u1D7020 =20.70, /u1D7030= − 3.61, /u1D437=4.14,
/u1D44E= − 0.41, /u1D70E0 =5.86, where a perfect CSI is assumed. Also,
we consider a jamming hit rate (JHR) of JHR= 40%. C2 data,
jamming signals and UA V trajectory are generated as in [9].
Let us compare the performance of AIn in terms of cu-
mulative abnormality (deﬁned in (8)) and cumulative reward
with that of random Frequency Hopping (FH-random) and Q-
Learning (QL), as illustrated in Fig. 2. Here, the objective
of AIn is to minimize abnormality while that of QL is to
maximize reward. Thus, the reward is considered in AIn
approach just for the sake of comparison with QL. W e consider
a binary reward which is equal to −1 under H1 and +1 under
H0 . Nevertheless, the relationship of these metrics is opposi tes
to one another. For a fair comparison with QL, we use
time-varying q-tables to deal with the dynamic environment al
changes. The exploration process in QL follows the /u1D716-greedy
policy with /u1D716= 1 decaying to 0. It can be seen from the
ﬁgure that AIn outperforms QL and FH-random under different
jamming strategies while AIn converges faster than QL due to
its capability in discovering jammer’s policy and performi ng
multiple updates. Fig. 3 depicts the cumulative SINR under
different jamming patterns achieved by the proposed AIn and
compared it with FH-random and QL. By observing Fig. 2
and Fig. 3, we can notice that minimizing the abnormality
(or maximizing the reward) leads to maximizing the SINR
where the time needed to reach the convergence is equivalent
to that in Fig. 2 and AIn beats both the FH-random and QL.
This means that avoiding surprising states minimizes the ab -
normality and maximises reward and SINR. AIn outperforms
FH and QL due to its ability to characterize the jammer
and discover its attacking strategy, explaining how the UA V
should act in the environment. Since AIn operates in a pure
belief-based setting. It can evaluate whether the action wa s
correct or wrong and also understand how to correct those
actions using the errors by performing multiple updates to
the AIn-table, which speeds up the learning process and reach
convergence faster. In contrast, QL performs single update s to
the q-table without being able to explain how to correct the
wrong actions, hindering the learning process. While FH can
not reach convergence as it is always selecting random actio ns.
V . C O N CL U S IO N
This letter has proposed a novel resource allocation strate gy
using Active Inference for anti-jamming in a Cognitive-UA V
scenario. Simulated results have indicated that the propos ed
method outperforms conventional Frequency Hopping and Q-
Learning in terms of learning speed (convergence). Further
research will explore performance improvements by facing
smart reactive jammers in fully-observable environments.
RE F E RE N CE S
[1] Q. Wu, W . Mei, and R. Zhang, “Safeguarding Wireless Netwo rk with
UA Vs: A Physical Layer Security Perspective, ” IEEE W ireless Commu-
nications, vol. 26, no. 5, pp. 12–18, 2019.
[2] Q. Qiu, H. Li, H. Zhang, and J. Luo, “Bandit based Dynamic S pectrum
Anti-jamming Strategy in Software Deﬁned UA V Swarm Network , ” in
2020 IEEE 11th International Conference on Software Engine ering and
Service Science (ICSESS) , 2020, pp. 184–188.
[3] S. Machuzak et al. , “Reinforcement learning based anti-jamming with
wideband autonomous cognitive radios, ” in 2016 IEEE/CIC International
Conference on Communications in China (ICCC) , 2016, pp. 1–5.
[4] G. Han, L. Xiao, and H. V . Poor, “T wo-dimensional anti-ja mming
communication based on deep reinforcement learning, ” in 2017 IEEE
International Conference on Acoustics, Speech and Signal P rocessing
(ICASSP), 2017, pp. 2087–2091.
[5] Z. Hu, K. W an, X. Gao, and Y . Zhai, “A Dynamic Adjusting Rew ard
Function Method for Deep Reinforcement Learning with Adjus table
Parameters, ” Mathematical Problems in Engineering , vol. 2019, 2019.
[6] K. Friston et al. , “Cognitive Dynamics: From Attractors to Active
Inference, ” Proceedings of the IEEE , vol. 102, no. 4, pp. 427–445, 2014.
[7] S. R. Sabuj, A. Ahmed, Y . Cho, K. Lee, and H. Jo, “Cognitive UA V -
Aided URLLC and mMTC Services: Analyzing Energy Efﬁciency a nd
Latency, ” IEEE Access , vol. 9, pp. 5011–5027, 2021.
[8] A. Al-Hourani and K. Gomez, “Modeling Cellular-to-UA V P ath-Loss for
Suburban Environments, ” IEEE W ireless Communications Letters , vol. 7,
no. 1, pp. 82–85, Feb 2018.
[9] A. Krayani et al. , “Self-Learning Bayesian Generative Models for Jammer
Detection in Cognitive-UA V -Radios, ” in GLOBECOM 2020 - 2020 IEEE
Global Communications Conference , 2020, pp. 1–7.