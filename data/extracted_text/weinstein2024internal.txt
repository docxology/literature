An Internal Model Principle For Robots
Vadim K. Weinstein, Tamara Alshammari, Kalle G. Timperi,
Mehdi Bennis, Steven M. LaValle
University of Oulu
Abstract. When designing a robot’s internal system, one often makes assumptions about the
structure of the intended environment of the robot. One may even assign meaning to various internal
components of the robot in terms of expected environmental correlates. In this paper we want to
make the distinction between robot’s internal and external worlds clear-cut. Can the robot learn
about its environment, relying only on internally available information, including the sensor data?
Are there mathematical conditions on the internal robot system which can be internally verified
and make the robot’s internal system mirror the structure of the environment? We prove that
sufficiency is such a mathematical principle, and mathematically describe the emergence of the
robot’s internal structure isomorphic or bisimulation equivalent to that of the environment. A
connection to the free-energy principle is established, when sufficiency is interpreted as a limit case
of surprise minimization. As such, we show that surprise minimization leads to having an internal
model isomorphic to the environment. This also parallels the Good Regulator Principle which states
that controlling a system sufficiently well means having a model of it. Unlike the mentioned theories,
ours is discrete, and non-probabilistic.
Keywords: Internal Model Principle· Free Energy Principle· Semantics · Good Regulator Prin-
ciple · Surprise Minimization· Sufficiency · Information Transition Systems
1 Introduction
From an algorithmic perspective, we are familiar with internal robot models used for mo-
tion planning, navigation, and so on, represented with coordinates inRn and geometric
primitives. We have also witnessed the rapid rise of deep learning approaches to robotics,
which encode internal models as parameters in a neural net, tuned from copious amounts
of input-output data. Such systems still require extensive training with vast amounts
of supervisory data from human experts and numerous reinforcement learning trials in
virtual environments [2]. Despite this, the derived robots remain unable to function effec-
tively in unfamiliar environments [15], unlike humans who can adapt to new situations
with ease. According to [18], this discrepancy may be due to the ability of humans to de-
velop internal models of how the world works. Currently, these internal models are often
pre-defined for training environments, making it difficult for robots to operate in different
settings. Therefore, it is crucial to understand how robots can independently construct
internal models that correlate with their external environments in a useful way without
relying on prior assumptions.
To address this challenge, we explore fundamental questions: What does it mean for a
robottoexploreitsenvironment?Howshouldweconceptualizeit?Whatdoesitmeanfora
robot to have a useful internal structure in relationship to its external environment? These
questions are fundamental to advancing robotics as they address the core challenges of
enabling robots to develop a deeper understanding and adaptability to their surroundings.
arXiv:2406.11237v1  [cs.RO]  17 Jun 2024
2 V. K. Weinstein et al.
1.1 Connection to other disciplines and related work
The relationship between the structure of an internal model and that of the external en-
vironment has been studied in the context of control theory. In [5] it was shown that an
effective regulator of a given system needs to contain an isomorphic copy of the system
itself. In [6] this statement is formulated in the context of linear, time-invariant, finite-
dimensional, multi-variable control systems. The incorporation of an internal model was
formalized in terms of the divisibility of the invariant factors of the matrix describing
the (linear) dynamics of the compensator by the minimal polynomial of a matrix repre-
senting the dynamics of the exogenous signals affecting the plant. It was concluded that
the synthesis of a plant and a compensator (controller) is structurally stable only if the
compensator incorporates an internal model of the plant dynamics.
Meanwhile, for biophysics and cognitive systems, the field ofactive inference[22] seeks
to interpret the notions of agent, mind, and brain through the general framework of the
free-energy principle [9,23]. Its stated aim is to explain behavior of agents through the
over-arching objective of surprise minimization via action. A recent work [11] illustrates
how collective behavior can emerge from the simultaneous updating of each agent’s inter-
nal variables, guided by the minimization of surprise (indirectly via the minimization of
free energy). However, the internal variables have pre-assigned relationships with the en-
vironment (local distance between the agent and its neighbors) governed by pre-assigned
dynamics.
In the context of discrete-time stochastic dynamical systems, the problem of discov-
ering economical, predictive representations based solely on action-observation dynamics
has been formalized in the theory ofpredictive state representations (PSR)[19,20]. Based
on the earlier, deterministic framework ofdiversity-based inference[24], PSR suggests a
way to model such systems through maintaining a vector of probabilities for the likelihood
of observing certain finite-length action-observation sequences, called core tests. Maintain-
ing a relatively low-dimensional vector of such probabilities turns out to be sufficient for
effectively modelling the statistical behaviour of the unknown dynamics.
From the perspective of classical logic, one typically deals with a languageL (such as
first-order logic) andL-model M with domain dom(M). A crucial aspect of the model
involves functions that map elements ofdom(M) to constant symbols inL, subsets of
dom(M)n to n-ary relation symbols inL, and functions fromdom(M)n to dom(M) to n-
ary functions inL. This mapping is the semantic function and assigns meaning to elements
of the languageL, relating them to elements of the world. This models, in particular, our
understanding of natural language. For example, the word “truck” represents an actual ob-
ject in the world, and as humans, we inherently understand this connection. Regardless of
whether we can explain the origin or the “reality” of this connection, we can still formalize
it using semantic functions. By formalizing these connections, we can better understand
and analyze the structure of languages and theories. Consequently, these semantic func-
tions allow us to introduce the following informal definition:A logical theory or a theory
of agency is representationalist if and only if it relies on semantic functions. In this sense,
all logical theories – from classical first-order and second-order logic to intuitionistic logic
and dependence logic – are representationalist.
An Internal Model Principle For Robots 3
When the internal structure (of a human or robot “brain”) is postulated to bear content
or represent external states, the theory doing so is calledrepresentational. For example,
in robotics, the internal state is often understood as serving as arepresentation1 of its
external environment. However, the mapping, thesemantic function, which assigns mean-
ings to this representation to relate it to elements of the external environment is often
overlooked. Theoreticians or designers frequently assume that the environment is struc-
tured in a certain way and that sensor data reflects that structure in some pre-defined
manner [14,27]. This assumption helps them to infuse the symbols in the robot’s internal
structure with meanings. Nonetheless, such an assumption limits the applicability of many
robotic solutions to more realistic scenarios, such as search and rescue tasks, where the
environment is previouslyunknown. Navigating anunknown environment presents signif-
icant challenges. A high degree of complexity and autonomy is required for the robot to
make decisions based on the data it observes [1]. To design a robot capable of autonomous
learning, we should separate the meaning assigned by the designers to the labels in the
robot’s brain from the meaning therobot itselfassigns to them. For the robot to be truly
flexible and able to generalize to unseen environments, it should be capable of learning
meanings based on its own histories of interaction.
Various semantic mappings were explicitly analysed in recent work in the context
of perception engineering and robotics by a subset of the authors [17]. Understanding
the nature of semantic functions is essential for developing robots capable of autonomous
learning and flexible adaptation. In this paper, we make the following philosophical propo-
sition: a semantic function is inherently arbitrary. By this, we mean that the semantic
function does not arise from structure in some non-arbitrary or systematic way. As an
example of structurally non-arbitrary assignment is the assignment of the fundamental
group to a topological space. Quoting the linguist Ferdinand de Saussure,“the bond be-
tween the signifier and the signified is arbitrary”[4,26]. In Saussure’s theory, the “signifier”
is the sound-image (or the form of a word), and the “signified” is the concept (or the mean-
ing associated with that word). The quote describes the relationship between the signifier
and the signified as an arbitrary relation, meaning that there is no inherent connection
between the form of a word and its meaning; rather, this relationship is established by
convention within a language community.
Enactivism, a branch of (philosophy of) cognitive science, argues that postulating
representations and semantic content is often philosophically questionable and practically
useless [10,12,13,28]. This argument could be extended to robotics in the light of the above
discussion. Enactivism argues that the complex structure of the brain is there not to
represent the world around us, but rather to make our engagement with it more efficient.
A counterargument is that building elaborate representations could actually help with
efficient engagement. There are two counter-counterarguments. First is that elaborate
represenations might be useful, but they might also be unnecessary. Is it possible to
circumvent elaborate representations and go directly to efficient engagement? The second
is that when some internal configuration has structural similarities to the environment, it
does not mean it is a representation. In our context specifically we see (Corollary 34) that
1 That is, a correspondence between the properties or elements of the internal model with those of the external
environment is assumed either implicitly or explicitly.
4 V. K. Weinstein et al.
such internal structures emerge even when it is not the intention or the goal of the system
to generate them. For more on the second counter-counterargument from a philosophical
perspective, see [12,13].
1.2 Contribution
We develop a mathematical framework which enables robots to learn an internal model of
the external environment by relying solely on quantities which can be internally evaluated.
This can be seen asemergent structural semantics. The robot only engages in resolving
internal “conflicts” such as minimization of surprise.
We mathematically prove that resolving these internal conflicts invariably results in
the emergence of an internal structure that is structurally similar to the environment.
The main statement capturing this intuition is Theorem 28.
One application even shows that sometimes only proprioceptive data is enough for the
emergence of an internal structure which is isomorphic to the external state space, even
though no other sensor data is available, see Remark 2 and Example 2. An example of
“embodied cognition”.
In this regard, our work can be viewed as a first step towards a discrete, combinatorial,
non-probabilistic formulation of the free-energy principle (FEP) [7,8,9,22]. The FEP has
gained significant attention as a model of the brain and “living systems” in general. It has
two principal components. One is the analysis of the independence of a “living system”
from the rest of the world, formalized through Markov blankets. Another component is
the idea of minimization of surprise by the system and the emergent internal structure
which results from it. This aspect of the FEP is the one we “replicate” in this paper.
Unlike most available accounts on the FEP, our framework is not probabilistic, but rather
combinatorial and in line with the previous work on history information spaces [25,30].
Our results are also limit-cases in which the surprise in not just minimized, but actually
brought to zero. The result can also be seen as a combinatorial and deterministic version
of the Good Regulator Theorem presented in [5].
2 Surpriseless couplings and bisimulation
The main tenet of the FEP that we focus on is surprise minimization. The notion of
sufficiency is the quintessential notion of surprise minimization in the framework of infor-
mation transition systems [16,25,30]. By definition, sufficiency is a condition posed on an
equivalence relation over a dynamical system which states that each equivalence “predicts”
the next equivalence class. This allows to transfer the notion of surprise minimization from
the probabilistic context (where it is measured by entropy) to the context of countable
discrete dynamical systems and transition systems. In this section we introduce basic
definitions and the first result stating a connection between surprise minimization and
equivalence (in this case bisimulation equivalence) of the internal state space with the
external state space.
We will fixU and Y to be the sets of motor commands and sensor data, respectively.
An Internal Model Principle For Robots 5
Definition 1 A deterministic transition system (DTS) is a tripleS = ( S, A, τ) where
τ : S × A → S is the transition function. Alabeled DTS is(S, A, τ, h) where (S, A, τ) is
DTS andh is some labeling function. A (labeled)sensorimotor transition systemis a DTS
with A = U × Y and a possible labeling functionh with range inY . A(labeled) internal
system is a (resp. labeled) sensorimotor transtion system withS = I the internal state
spaceand τ = φ the information transition function. Theexternal system, orenvironment
is a labeled DTS withS = X, A = U, τ = f and the range ofh being Y . Both external
and internal systems are special cases of sensorimotor transition systems. ⊣
One may ask why is the input to the internal systemU ×Y , and not justY . Typically
we expect the behaviour of a robot to depend on the history of sensor data, but not on its
own motor commands which are determined by the sensor data anyway. We do this for
our framework to be general enough. For example, in a situation where the robot’s policy
has not been determined yet, all possible sequences of motor commands are possible. Or
perhaps the DTS is a model of only a part of the robot, a part which does not (yet) decide
the motor commands and has to be prepared for all possibilities. An internal system is
exploratory, if it is prepared to “try” all possible motor commands indpendently of the
sensor data. Mathematically,(I, U× Y, φ) is exploratory, if the value ofφ(ι, u, y) is not
dependent on y, i.e. for all y, y′ ∈ Y we have φ(ι, u, y) = φ(ι, u, y′). For exploratory
internal systems we may drop the componentY from the definition. This leads to:
Definition 2 An exploratory internal systemis a triple(I, U, φ) in whichI is the internal
state space, andφ: I × U → I. ⊣
Since we are interested in constructing internal systems and not in finding policies, we
will deal mostly with exploratory internal systems. We justified the title “exploratory” by
the intuition that the agent is “curious” and tries all possible paths notwithstanding the
sensor data. Our theorems about emergence of bisimulation equivalence and isomorphism
will justify this title formally because they work precisely for such systems. Another reason
to introduce the class of exploratory systems is to simplify some of the mathematical
exposition.
Definition 3 Given a setA, the free DTS generated byA, denoted F(A), is the DTS
(A<N, A,⌢) where the state spaceA<N is the set of finite sequences of elements ofA, and
the state-transition function is the concatenation of sequences. WhenA = U, this DTS
is the collection of all possible actuator sequences. IfA = U × Y , then it is the collection
of all actuation-observation, or sensorimotor, sequences and is equivalent to the history
information spaceIhist of [25], see also [16]. ⊣
Definition 4 [Universal systems] The systemF(U × Y ), the free DTS generated by the
set U × Y is the universal information transition system (with respect to U and Y ).
Every connected internal transition system is a quotient ofF(U × Y ), see Corollary 23.
The universal exploratory systemis F(U). Every connected exploratory internal system
is its quotient. ⊣
6 V. K. Weinstein et al.
Definition 5 For a DTS(S, A, τ), s ∈ S, and ¯a ∈ A<N, define s ∗ ¯a by induction on
the length of ¯a. If ¯a = ∅, then s ∗ ¯a = s, and if s ∗ ¯a is defined and a ∈ A, then
s ∗ (¯a⌢a) = τ(s ∗ ¯a, a). ⊣
Definition 6 Given an internal systemI = (I, U× Y, φ) and an external systemX =
(X, U, f, h), define the coupled systemX ⋆ I to be the DTS(X × I, U, g) where g : X ×
I × U → X × I is defined by
g(x, ι, u) = (f(x, u), φ(ι, u, h(x))). (1)
For exploratory systems, if the componentY is not written, the equation (1) becomes
g(x, ι, u) = (f(x, u), φ(ι, u)). ⊣
In view of Definition 6,U and Y constitute the “interface” between the internal and
external.
Definition 7 Given a finite sequence¯u = (u1, . . . , un) ∈ Un and states x ∈ X, ι ∈ I,
there are unique sequences¯x = (x1, . . . , xn) and ¯ι = (ι1, . . . , ιn) such thatx1 = x, ι1 = ι
and for eachk < n, g(xk, ιk, uk) = (xk+1, ιk+1). Denote this¯x by x ∗ ¯u and ¯ι by ι ⋄ ¯u.
Note that the definition ofx ∗ ¯u here coincides with Definition 5. This is why we use
the same notation for it. But the operation⋄ on I requires the coupling to be defined.
If the external system and/or the initial states need to be specified, we denoteι ⋄ ¯u =
ι ⋄X ¯u = ι ⋄X,x,ι ¯u ⊣
Remark 1. If we viewU<N as the free monoid under concatenation, thenx 7→ x ∗ ¯u and
ι 7→ ι ⋄ ¯u are actions of this monoid onX and I, respectively. ⊣
The coupling restricts the internal system by removing the impossible sensorimotor
sequences. More precisely:
Definition 8 Suppose I = ( I, U× Y, φ) is an internal system and X = ( X, U, f, h)
external, and let (x0, ι0) ∈ X × I. Let I ↾ X,x0,ι0 be the DTS (I, U, ψ) where ψ(ι, u) =
ι⋄X,x0,ι0 u. If the initial states are clear from the context, we drop them from the subscript.
⊣
Definition 9 [Surpriseless] SupposeI = (I, U× Y, φ) and X = (X, U, f, h) are internal
and external systems, respectively. We say that(x0, ι0) ∈ X ×I is surpriseless, if there are
no sequences¯u = (u1, . . . , un) ∈ Un and ¯u′ = (u′
1, . . . , u′
m) ∈ Um such thatι0 ⋄ ¯u = ι0 ⋄ ¯u′,
but h(x0 ∗ ¯u) ̸= h(x0 ∗ ¯u′). ⊣
Asurpriseoccursifthesensorydatacannotbe“predicted” basedontherobot’sinternal
state. Note that the condition of being surpriseless can be evaluated within the internal
model because the definition speaks only about internal states and sensory data.
Theorem 10 The coupling of F(U × Y ) to any environment X = ( X, U, f, h) is sur-
priseless.
An Internal Model Principle For Robots 7
Proof. In the universal automaton, any two sequences¯u, baru′ yield a different internal
state ι0 ⋄ ¯u ̸= ι0⋄ if and only if¯u ̸= ¯u′. But if¯u = ¯u′, then alsox0 ∗ ¯u = x0 ∗ ¯u′, so we have
checked the definition. ⊓ ⊔
Definition 11 [Bisimulation] SupposeI = (I, U× Y, φ, h′) is a labeled internal system.
Suppose X = ( X, U, f, h) is an external system. We say thatx0 ∈ X is bisimulation
equivalent toι0 ∈ I in X ⋆ I, if there is a relationR ⊆ X ×I such that (B1.)(x0, ι0) ∈ R,
(B2.) for all(x, ι) ∈ R and all u ∈ U, (x ∗ u, ι⋄ u) ∈ R, and (B3.) for all(x, ι) ∈ R we
have h(x) = h′(ι). ⊣
At first glance, bisimulation equivalence cannot be evaluated within the internal sys-
tem because by definition it requires finding a binary relation between the internal and
the external. In Theorem 14 below we prove, however, that it is equivalent to being sur-
priseless.
Definition 12 A DTS (S, A, τ) is strongly connected, if for alls, s′ ∈ S there is ¯a ∈ A
with s ∗ ¯a = s′. ⊣
Lemma 13 Suppose the coupled systemX ⋆ I is strongly connected. If(x0, ι0) ∈ X × I
is surpriseless, then there is a well-definedˆh: I → Y such thatˆh(ι) = h(x0 ∗ ¯u) for some
(all) ¯u ∈ U<N with ι0 ∗ ¯u = ι.
Proof.Itisenoughtoshowthatforall ¯u, ¯u′ ∈ U<N,if ι0⋄¯u = ι0⋄¯u′,then h(x∗¯u) = h(x∗¯u′),
but this is exactly the definition of surpriseless. ⊓ ⊔
Theorem 14 Suppose the coupled systemX ⋆ I is strongly connected. The following are
equivalent
1. (x0, ι0) is surpriseless inX ⋆ I
2. ˆh is well-defined
3. ˆh is well-defined andx0 is bisimulation equivalent toι0 in X ⋆ I over ˆh.
Proof.We already proved1 → 2 (Lemma 13). Assume 2. Let
R = {(x, ι) ∈ X × I | ∃¯u ∈ U<N((x, ι) = (x0 ∗ ¯u, ι⋄ ¯u))}.
Conditions (B1) and (B2) are clearly satisfied. If(x, ι) ∈ R, thenh(x) = h(x0 ∗ ¯u) = ˆh(ι)
by definition ofˆh which proves also condition (B3). This proves2 → 3. Clearly 3 → 2.
Suppose that 2 and we will prove 1. Suppose¯u, ¯u′ are such thatι0 ⋄ ¯u = ι0 ⋄ ¯u′. Sinceˆh is
well-defined, we haveh(x0 ∗ ¯u) = ˆh(ι0 ⋄ ¯u) = ˆh(ι0 ⋄ ¯u′) = h(x0 ∗ ¯u′). ⊓ ⊔
Theorem 14 shows that if the agent had a method to reduce surprise, then once suc-
cessful, its internal system would be bisimulation equivalent to the environment. This
result is reminiscent of the Good Regulator [5]. Surpriselessness implies the theoretical
possibility to control the system completely. In a surpriseless system, the internal state
determines the results of all possible future actions (see also Lemma 27). On the other
hand we just proved that such potential to control the system implies bisimulation equiv-
alence. Bisimulation equivalence, however, is significantly weaker than isomorphism. Also,
Theorem 14 is non-constructive, it does not say whether or when surpriseless couplings
exist, or how to obtain them. These questions will be addressed in the next section.
8 V. K. Weinstein et al.
3 Theory for General Deterministic Transition Systems
In the previous section we showed that if an internal state space is adapted to the envi-
ronment with the goal of minimizing surprise, then it will become bisimulation equivalent
to it. What are the conditions under which surprise minimization leads to the internal
system being isomorphic, and not just bisimilar, to the external system? A bisimulation
equivalence between transition systems is an isomorphism, if both systems are minimal in
some sense. The robot cannot know whether the environment is in this sense minimal, but
it can “know” when its internal system is. It is cumbersome to speak about the function
ˆh of Lemma 13 because it is not always well-defined. Also, Theorem 14 does not shed
light on how to systematically obtain surpriseless internal systems when nothing is known
about the environment or when do they even exist. We will now present a framework
which enables a more systematic characterization of when the internal system is both sur-
priseless and isomorphic to the external system, and how to obtain them, theoretically,
but not algorithmically.2 This approach will pave the path to algorithm design in future
work, see Section 6.
Definition 15 Given a DTSS = (S, A, τ). We say that an equivalence relationE on S
is sufficient, orS-sufficient, if for alls, s′ ∈ S and a ∈ A we have that(s, s′) ∈ E implies
(τ(s, a), τ(x′, a)) ∈ E. An equivalence relationE′ is arefinement of an equivalence relation
E, ifE′ ⊆ E. We say that an equivalence relationE′ is aminimal sufficient refinementof
E, ifE′ is sufficient,E′ ⊆ E and for all sufficient refinementsE′′ ⊆ E we haveE′′ ⊆ E′.
⊣
The following is Theorem 4.19 of [30].
Theorem 16 Suppose S = ( S, A, τ) is a DTS. Let E be an equivalence relation on
S. Then the minimal S-sufficient refinement of E exists and is unique. We denote it
by MSR(E) = MSRS(E). ⊓ ⊔
Definition 17 Let Si = (Si, A, τi) be a DTS fori ∈ {0, 1}. A functionh: S0 → S1 is
called ahomomorphism fromS0 to S1, if for all(s, a) ∈ S0 × A we have
τ1(h(s), a) = h(τ0(s, a)).
It is called anepimorphism, if it is onto, andisomorphism if it is a bijection. ⊣
Definition 18 Let E be an equivalence relation onS. Ifs ∈ S, denote by[s], or by[s]E,
the equivalence class ofs which is the set{s′ ∈ S | (s, s′) ∈ E}. The set of all equivalence
classes is denoted byS/E. Suppose thath: S → Y is a function andE is an equivalence
relation onY . Define
h−1(E) = {(s, s′) ∈ S × S | (h(s), h(s′)) ∈ E}.
2 Our result shows how to find such systems by computing minimal sufficient refinements on infinite trees. As
such this is not computationally feasible, but gives a theoretical doorway towards designing algorithms.
An Internal Model Principle For Robots 9
It is easy to verify thath−1(E) is an equivalence relation onS. Denote byEh the equiv-
alence relation h−1(idY ). Equivalently, (s, s′) ∈ Eh ⇐⇒ h(s) = h(s′). A function
h: S → Y is E-invariant, if E is a refinement of Eh. We say that an equivalence re-
lation E is h-closed, ifEh is a refinement ofE. ⊣
We leave the following two observations for the reader to prove:
Lemma 19 Let S = ( S, A, τ) be DTS and h: S → Y . Then Eh is h-closed, and h is
Eh-invariant. ⊓ ⊔
Lemma 20 Suppose S = (S, A, τ) is a DTS,E is an S-sufficient equivalence relation,
and h: S → Y . If E is h-closed, then(h/E): S/E → Y defined by(h/E)([s]) = h(s) is
well-defined. ⊓ ⊔
Lemma 21 (Lemma 4.5 of [30]) SupposeS = (S, A, τ) is a DTS, andE is anS-sufficient
equivalence relation. LetS′ = S/E and defineτ′ : S′ ×A → S′ by τ′([s], a) = [τ(s, a)] (also
denoted τ′ = τ/E ). Thenτ′ is well-defined andS′ = (S′, A, τ′) is a DTS (also denoted by
S′ = S/E). ⊓ ⊔
Proposition 22 (Pullback)Suppose Si = ( Si, A, τi) is a DTS for i ∈ {0, 1}. Sup-
pose that E1 is an S1-sufficient equivalence relation onS1. Suppose thath: S0 → S1 is
an epimorphism. ThenE0 = h−1(E1) is an S0-sufficient equivalence relation onS0 and
S0/E0 ∼= S1/E1.
Proof.Suppose s, s′ ∈ S0 are E0-equivalent and supposea ∈ A. Then we haveh(s) E1 h(s′)
and so by the sufficiency ofE1 we haveτ1(h(s), a) E1 τ1(h(s′), a). On the other hand be-
cause h is a homomorphism,h(τ0(s, a)) = τ1(h(s), a) and h(τ0(s′, a)) = τ1(h(s′), a). Com-
bining these, we have thath(τ0(s, a)) and h(τ(s′, a)) are E1-equivalent. By the definition
of E0, this means that τ0(s, a) and τ0(s′, a) are E0-equivalent which proves thatE0 is
S0-sufficient.
Now we prove thatS0/E0 ∼= S1/E1. To simplify notation, for alli ∈ {0, 1}, denote[s]i
instead of[s]Ei , and alsoS′
i and τ′
i instead ofSi/Ei and τi/Ei. Defineˆh: S′
0 → S′
1 by
ˆh([s]0) = [h(s)]1 (2)
By the definition ofE0 it is easy to see thatˆh is well-defined. Suppose thats ∈ S0 and
a ∈ A are arbitrary. Then
ˆh(τ′
0([s]0, a))
(a)
= ˆh([τ0(s, a)]0)
(b)
= [h(τ0(s, a))]1
(c)
= [τ1(h(s), a)]1
(d)
= τ′
1([h(s)]1, a)
(e)
=τ′
1(ˆh([s]0), a)
Here (a) and (d) follow from the definitions ofτ′
0 and τ′
1, (b) and (e) from (2), and (c)
from thath is a homomorphism.
This shows thatˆh is a homomorphism fromS0/E0 to S1/E1. It remains to show that
ˆh is a bijection. Sinceh is onto (by the definition of an epimorphism), it is standard to
check thatˆh is also onto. Supposes, s′ ∈ S0 are notE0-equivalent. Then by the definition
10 V. K. Weinstein et al.
of E0 we have thath(s) and h(s′) are not E1-equivalent, so by (2) we haveˆh([s]0) ̸=
ˆh([s′]0) which proves that ˆh is one-to-one. Thus we have proved thatˆh is a bijective
homomorphism, i.e. an isomorphism, fromS0/E0 to S1/E1, so the proof is complete.⊓ ⊔
Corollary 23 (Universality)Let F(A) be as in Definition 3. The free DTS of isuni-
versal in the sense that ifS = (S, A, τ) is any strongly connected DTS, then there is an
equivalence relationE on A<N such thatF(A)/E ∼= S.
Proof.Fix s0 ∈ S. Define the maph: A<N → S by induction on the length of¯a as follows.
Let h(∅) = s0, and if h(¯a) is defined, let h(¯a⌢a) = τ(h(¯a), a). Let E1 = id S. Then it
is clearlyS-sufficient. LetE0 = h−1(E1). By strong connectedness,h is an epimorphism.
Now the result follows from Proposition 22. ⊓ ⊔
Corollary 23 means that finding a DTS is equivalent to finding an equivalence relation
on F(A). It follows that finding an appropriate internal system for a robot’s brain can
be reformulated as finding such an equivalence relation. The condition of sufficiency is
essentially the same as surprise minimization: by definition of sufficiency the equivalence
class of a state predicts the equivalence class of the following state. This condition can be
evaluated within the internal system, but we will show analogously to Theorem 14 that if
the robot can achieve sufficiency, then it achieves equivalence. In fact, minimal sufficiency
will imply isomorphism between the internal and external systems under some conditions.
Definition 24 Suppose H ⊆ S × S is any set. Then ⟨H⟩ is the equivalence relation
generated byH. It is defined to be the set of all pairs(s0, s1) ∈ S × S such that there
exists a finite sequence(x0, . . . , xk) of elements ofS such thats0 = x0, s1 = xk, and for
all i ∈ {0, . . . , k− 1} we have(xi, xi+1) ∈ H or (xi+1, xi) ∈ H. ⊣
Lemma 25 SupposeSi = (Si, A, τi) is DTS fori ∈ {0, 1}, E, E0, E1, . . . , Em−1 are equiv-
alence relations onS0, E1 an equivalence relation onS1, andh: S0 → S1 a function. Then
the following hold:
1. Each Ei is a refinement of⟨E0 ∪ ··· ∪Em−1⟩,
2. If Ei are S0-sufficient for alli < m, then so is⟨E0 ∪ ··· ∪Em−1⟩,
3. If Ei is a refinement ofE for all i < m, then⟨E0 ∪ ··· ∪Em−1⟩ is also a refinement
of E.
4. If E is a refinement ofEi for alli < m, thenE is a refinement of⟨E0 ∪ ··· ∪Em−1⟩.
5. If Ei is h-closed for alli < m, then so is⟨E0 ∪ ··· ∪Em−1⟩.
6. Eh is a refinement ofh−1(E1).
7. If h is onto, andE is h-closed, then
h(E) = {(h(s), h(s′)) ∈ S1 × S1 | (s, s′) ∈ E}
is an equivalence relation onS1.
8. If h is onto,E is h-closed, andE is a refinement ofh−1(E1), thenh(E) is a refinement
of E1.
9. If h is an epimorphism,E is S0-sufficient andh-closed, thenh(E) is S1-sufficient.
10. If h is a homomorphism andE1 is S1-sufficient, thenh−1(E1) is S0-sufficient,
An Internal Model Principle For Robots 11
11. If h is a homomorphism, thenEh is S0-sufficient.
Proof.From definitions it follows that
Ei ⊆ E0 ∪ ··· ∪Em−1 ⊆ ⟨E0 ∪ ··· ∪Em−1⟩
and by definition of a refinement we have 1. 2 is Theorem 4.15 of [30]. Suppose(s0, s1) ∈
⟨E0∪···∪ Em−1⟩ andx0, . . . , xk−1 witnessthis.Thenbytheassumption,eachpair (xi, xi+1)
isin E.Bytransitivityof E thenalso (s0, s1) ∈ E.Thisproves3.4followseasilyfrom1.By
definition of beingh-closed, 5 follows directly from 4. Suppose(s, s′) ∈ Eh. Thenh(s) =
h(s′), so by reflexivity ofE1, (h(s), h(s′)) ∈ E1 which proves 6. For 7, reflexivity follows
from the surjectivity ofh. Symmetry follows from the symmetry ofE. For transitivity,
assume (s1, s2) ∈ h(E) and (s2, s3) ∈ h(E). Lets′
1, s′
2, s′′
2, s′
3 be some elements ofS0 such
that h(s′
1) = s1, h(s′
2) = h(s′′
2) = s2, and h(s′
3) = s′′
3, and so that (s′
1, s′
2) ∈ E and
(s′′
2, s′
3) ∈ E. But(s′
2, s′′
2) ∈ Eh which is a refinement ofE, so also(s′
2, s′′
2) ∈ E. Now by
transitivity ofE, we have(s′
1, s′
3) ∈ E and so(s1, s3) = (h(s′
1), h(s′
3)) ∈ h(E) and we are
done. By 7h(E) is an equivalence relation and sinceE ⊆ h−1(E1) implies h(E) ⊆ E1,
this proves 8.
For 9, let(s1, s2) ∈ h(E) and a ∈ A. Let s′
1, s′
2 ∈ S0 witness that (s1, s2) ∈ h(E).
Then τ(s1, a) = τ(h(s′
1), a) and τ(s2, a) = τ(h(s′
2), a) and (s′
1, s′
2) ∈ E. By the assumed
sufficiency ofE, and the properties of a homomorphism, we haveτ(s1, a) = τ(h(s′
1), a) =
h(τ(s′
1, a)) = h(τ(s′
2, a)) = τ(h(s′
2), a) = τ(s2, a) which proves the required sufficiency
of h(E). The proof of 10 is analogous. 11 follows from 10 and the sufficiency ofidS1 . ⊓ ⊔
The following theorem is the key to understanding why internal processing is “enough”.
It shows that the operatorMSR commutes with any epimorphism. One can compute the
minimalsufficientrefinementofarelationandthentaketheinverseimageoftheresult.Or,
one can first take the inverse image, and then compute the minimal sufficient refinement
of that. According to Theorem 26 both give the same result. This is valuable for us,
because the inverse image of the relation (before applyingMSR) corresponds to what the
robot can sense. Then, the robot can internally applyMSR, and can be sure to get the
same as ifMSR was applied in the external system first. We explore the applications of
Theorem 26 to the robot system in Section 4.
Theorem 26 Let Si = (Si, A, τi) be DTS fori ∈ {0, 1}, and suppose thath: S0 → S1 is
an epimorphism. SupposeE1 is an equivalence relation onS1. Then
MSRS0 (h−1(E1)) = h−1(MSRS1 (E1)).
Proof. Denote E0 = h−1(MSRS1 (E1)). By Proposition 22 the relation E0 is sufficient.
It is also a refinement ofh−1(E1) because MSRS1 (E1) ⊆ E1 implies h−1(MSRS1 (E1)) ⊆
h−1(E1). By Theorem 16 it is now enough to show thatE0 is a minimal sufficient refine-
ment ofh−1(E1). Suppose for a contradiction that there exists anS0-sufficient refinement
E′
0 of h−1(E1) such that E′
0 ̸⊆ E0. Let E′
1 = h(⟨E0 ∪ E′
0 ∪ Eh⟩). By Lemma 25(2) and
25(11) the relation⟨E0 ∪ E′
0 ∪ Eh⟩ is S0-sufficient, and by Lemma 25(1) and 25(4) it is
h-invariant. So by 25(9)E′
1 is S1-sufficient. SinceE′
0 ̸⊆ E0, we have by 25(1) that
⟨E0 ∪ E′
0 ∪ Eh⟩ ̸⊆E0
12 V. K. Weinstein et al.
and so
E′
1 = h(⟨E0 ∪ E′
0 ∪ Eh⟩) ̸⊆ h(E0) = MSRS1 (E1). (3)
On the other handE0 and E′
0 are refinements ofh−1(E1), so by 25(3) and 25(6)⟨E0 ∪
E′
0 ∪ Eh⟩ is a refinement ofh−1(E1). By 25(8) we now have thatE′
1 is a refinement ofE1.
Combining this with (3) above, and the sufficiency ofE′
1, we have a contradiction with
the minimality ofMSRS1 (E1). ⊓ ⊔
Lemma 27 If E is a sufficient equivalence relation onS = (S, A, τ), then for all¯a ∈ A<N
and alls, s′ ∈ S we have
(s, s′) ∈ E =⇒ (s0 ∗ ¯a, s′ ∗ ¯a) ∈ E.
Proof.By induction on the length of¯a. ⊓ ⊔
4 Application to Internal and External Systems
Recall that we conventionally fixU and Y to be the sets of motor commands and sensor
data, respectively.
In this section we will work with the universal exploratory system (Definition 2)I =
F(U) in which the initial state is always the empty sequence. Given an external system
X = (X, U, f, h), and initial statex0 ∈ X, let ˆfx0 : U<N → X denote the functionˆfx0 (¯u) =
x0 ∗ ¯u. Note thatˆfx0 is a homomorphism fromF(U) to X.
Theorem 28 LetX = (X, U, f, h) be a strongly connected external system with the initial
statex0 ∈ X. Suppose thatE is any equivalence relation onX. LetEI = MSRF(U)( ˆf−1
x0 (E)),
and EX = MSR(E). Then
I/EI ∼= X/EX .
Proof.Since X is connected, ˆfx0 is an epimorphism. By Theorem 26 we have
EI) = MSRF(U)( ˆf−1
x0 (E)) = ˆf−1
x0 (MSRX (E)) = ˆf−1
x0 (EX ).
The result now follows from Proposition 22. ⊓ ⊔
It is useful to note thatMSR(Eh) can be a measure of symmetry of the environment
(X, U×Y, f, h). A bisimulation istrivial, if it is the identity relation. Anautobisimulation
is a bisimulation of a DTS with itself, a relation onX × X. The following is Theorem 4.4
of [30].
Theorem 29 An environmentX = (X, U, f, h), has a non-trivial autobisimulation if and
only ifMSR(Eh) ̸= idX. ⊓ ⊔
The notion of autobisimulation is a weak version of automorphism. There is also a
one-sided version of this theorem for automorphisms:
Theorem 30 If there is a non-trivial automorphism ofX, thenMSR(Eh) ̸= idX.
An Internal Model Principle For Robots 13
Proof. An automorphism is a special case of an autobisimulation, so the result follows
from Theorem 29. ⊓ ⊔
So we can call the environmentchiral, if MSR(Eh) = id X. The equivalence relation
ˆf−1
x0 (Eh) on F(U) equates those paths which lead to identical sensor data. So we may
call it the relation ofsensory indistinguishability. The following corollary shows that if
the environment is chiral, then taking the quotient of the history information space by
the minimal sufficient refinement of the sensory indistinguishability yields a structure
isomorphic to the environment.
Corollary 31 SupposeX = (X, U, f, h) is a strongly connected environment, andx0 ∈ X.
Suppose thatMSRX (Eh) = idX. Then
F(U)/ MSR( ˆf−1
x0 (Eh)) ∼= X.
Proof.Since X/ idX ∼= X, the result follows from Theorem 28. ⊓ ⊔
Taking the minimal sufficient refinement can be interpreted as minimizing surprise
while also minimizing computational resources. The minimization of surprise is due to
the definition of sufficiency which say that the current equivalence class predicts the next
one. Minimization of resources is captured by the minimality of the refinement, as it
yields the smallest possible quotient space. Isomorphism is a special case of bisimulation,
so applying Theorem 14 we get:
Corollary 32 Suppose X is as in Corollary 31, andI = F(U)/ MSR( ˆf−1
x0 (Eh)). Then
I ⋆ X is surpriseless. ⊓ ⊔
An interesting class of chiral environments consists of those environments in whichEh
has one equivalence class which is a singleton. Call such an equivalence relationpointed.
We will show that environments with pointedEh are chiral under some very minimal
assumptions onX. An automaton(S, A, τ) isminimally distinguishing, if for alls0, s1, s2 ∈
S and alla ∈ A we have that ifτ(s1, a) = τ(s2, a) = s0, then one of the following holds:
s0 = s1, s1 = s2 or s0 = s2.
Theorem 33 Let S = (S, A, τ) be a strongly connected and minimally distinguishing. If
E is a pointed equivalence relation onX, thenMSR(S) = idX.
Proof.Let s0 ∈ S be an element such that{s0} is anE-equivalence class. Lets1, s2 ∈ S.
We will show thats1 ̸= s2 implies (s1, s2) /∈ MSR(E). Let ¯ai be of minimal length such
that si∗¯ai = s0 for i ∈ {1, 2}. These exist by connectedness. Denoteki = |¯ai| for i ∈ {1, 2}.
Without loss of generality assume thatk1 ⩽ k2. There are two cases:k1 < k2 and k1 = k2.
Case 1:k1 < k2. By the minimality of|¯a2|, we haves2 ∗ ¯a1 ̸= s0, buts1 ∗ ¯a1 = s0. By
the choice ofs0 we have now(s2 ∗ ¯a1, s1 ∗ ¯a1) /∈ E, and so(s2 ∗ ¯a1, s1 ∗ a1) /∈ MSR(E). By
Lemma 27 we now conclude that(s1, s2) /∈ MSR(E).
Case 2:k1 = k2. By induction onk = k1 = k2. We will show again that
s1 ̸= s2 =⇒ s1 ∗ ¯a1 ̸= s2 ∗ a1 (4)
14 V. K. Weinstein et al.
which by the same argument as above implies that(s1, s2) /∈ MSR(E). We will proceed
by induction onk.
If k = 0, then ¯a1 = ¯a2 are both empty, sos1 = s2 = s0, and the premises1 ̸= s2 is
false. Suppose we have proved (4) for allk ⩽ n and thatk = n + 1. Assumes1 ̸= s2. Let
¯a1 = a0
1 ··· an
1 . Letj be the smallest one such that
s1 ∗ a0
1 ··· aj
1 = s2 ∗ a0
2 ··· aj
2. (5)
If such j does not exist, then we are done. Otherwise we will deduce a contradiction.
Define
s′
1 = s1 ∗ a0
1 ··· aj−1
1 ,
s′
2 = s2 ∗ a0
1 ··· aj−1
1 ,
s′
0 = s1 ∗ a0
1 ··· aj−1
1 aj
1 = s2 ∗ a0
1 ··· aj−1
1 aj
1 = τ(s′
1, aj
1) = τ(s′
2, aj
1).
Now by the assumption thatX is minimally distinguishing we must now have one of the
following: (a) s′
0 = s′
1, (b) s′
1 = s′
2, or(c) s′
0 = s′
2. By the minimality ofj we cannot have
(b). Let
¯a<j
1 = a0
1 ··· aj−1
1 and ¯a>j
1 = aj+1
1 ··· an
1 .
If (a) holds, then we have
s0 = s1 ∗ (¯a<j
1
⌢aj⌢ ¯a>j
1 ) = ((s1 ∗ (¯a<j
1
⌢aj)) ∗ ¯a>j
1
= s′
0 ∗ ¯a>j
1 s1 ∗ (¯a<j
1
⌢aj
1) = s0
= s′
1 ∗ ¯a>j
1 by (a)
= (s1 ∗ ¯a<j
1 ) ∗ ¯a>j
1 = s1 ∗ (¯a<j
1
⌢¯a>j
1 )
contradicting the minimality of|¯a1|. If(c) holds we get similarly a contradiction with the
minimality of|¯a2|:
s0 = s′
0 ∗ ¯a>j
1 = s′
2 ∗ ¯a>j
1 by (c)
= (s2 ∗ ¯a<j
1 ) ∗ ¯a>j
1 = s2 ∗ (¯a<j
1
⌢¯a>j
1 ).
This completes the proof of the theorem. ⊓ ⊔
We say that a sensor mappingh: X → Y is pointed, if Eh is a pointed equivalence
relation which is equivalent to saying that there is somey ∈ Y such that h−1(y) is a
singleton.
Corollary 34 Let (X, U, f, h) be a strongly connected minimally distinguishing external
system. Let x0 ∈ X, and suppose that h: X → Y is a pointed sensor mapping. Let
E = MSRF(U)( ˆf−1
x0 (Eh)). Then
F(U)/E ∼= X.
Proof.By Theorem 33,MSRX (S) = idX . Then apply Corollary 31. ⊓ ⊔
An Internal Model Principle For Robots 15
Remark 2. Supposethestatespaceconsistsofpositionsoftherobot’sbodyintheenviron-
ment. Then it follows that the robot does not need any “external” sensors to successfully
mirror the environment internally. It is enough to have proprioceptive feedback to single
one specific position of its own body. Striving for sufficiency (surprise minimization) takes
care of the rest by Corollary 34. ⊣
Finally, we give an application for a mathematically ideal situation where the motor
commands generate a group acting on the external state space:
Corollary 35 Suppose that (G, +) is a group generated byU ⊆ G, and suppose that
τ : X × G → X is a transitive action of G on X. Consider the environment X =
(X, U, f, h) where f = τ ↾(X × U) and h is pointed. LetE = MSRF(A)( ˆf−1
x0 (Eh)). Then
F(A)/E ∼= X.
Proof. By Corollary 34 it is enough to show thatX is connected and minimally distin-
guishing. Since the action is transitive,X is connected. Supposex1 ∗ u = x2 ∗ u = x0 for
some x0, x1, x2 ∈ X and u ∈ U. Multiplying by the inverse ofu on the right this implies
that x1 = x2. ⊓ ⊔
5 Examples
Example 1. Simple examples of finite external systems which can be “learned” by taking
minimal sufficient refinements over the exploratory internal state space are depicted in
Figures 1 and 2. It is easy to see that in both cases the external state space is minimally
distinguishing and the sensor mapping is pointed, so we can apply Corollary 34. ⊣
In the final example below we show that very limited proprioceptive feedback is enough
for the emergent of internal structure isomorphic to the environment.
Example 2. (Robot arm) In Figure 3 there is a robot arm with three joints. For the sake
of applicability of our theory, assume that the state space is discrete. Each joint can turn
by, say,±1◦. If the arm is pushing against an obstacle while applying a rotation of any of
the joints, then the external state stays the same as before. The configuration space is a
subset of the (discrete) 3-torus. Assume that there is one (and possibly only one!) position
of the arm where it receives a proprioceptive feedback. It could be the original position of
the arm where it receives a “click” and no sensor feedback in any other position. Then this
sensor mapping is pointed. It is also not hard to see that the robot arm satisfies minimal
distinguishability: Suppose the arm is in some positionsx1 or x2 and u ±1◦ rotation of
one of the joints. If this rotation does not result in hitting an obstacle for eitherx1 or x2,
then x1 ̸= x2 implies x1 ∗ u ̸= x2 ∗ u because the rotationu acts “homeomorphically” on
the torus. If, however, say,x1 faces an obstacle when rotating byu, thenx1 = x1 ∗ u, so
we are done. Applying Corollary 34 we see that if such robot arm succeeds in minimizing
the surprise of “when is the proprioceptive ‘click’ feedback received”, then it will end up
building an isomorphic copy of the external state space. ⊣
16 V. K. Weinstein et al.
(a)
 (b)
(c)
 (d)
Fig.1: The environment (a) has four states. Agent can move clockwise and counter-
clockwise, but in states 1 and 4 nothing happens, if it tries to go left or right respectively.
In (b) the binary tree of action observation sequences is depicted. In most nodes the sensor
data is “white” but when the agent is in the left-most state, the data is “green”. In (c) we
show the minimal sufficient refinement of (b), and in (d) we have taken the quotient of
(c) with respect to the refinement. It turns out to be isomorphic to (a), as predicted by
Corollary 34.
An Internal Model Principle For Robots 17
(a)
 (b)
(c)
 (d)
Fig.2: Same idea, as in Figure 1, but with a circular environment. In this case Corollary 35
could be applied, with the groupZ/4Z acting onX.
Fig.3: Robot arm with three joints and obstacles.
18 V. K. Weinstein et al.
6 Discussion
Our aim was to introduce a mathematical principle that the robot can internally evaluate,
and if satisfied, ensures that the internal system becomes isomorphic or bisimulation
equivalent to the environment. In doing so, we proposed a framework without pre-assigned
meanings or assumed correlations between the internal and the external systems. Yet, a
correlation emerges between them due to structural coupling. The proposed mathematical
principle is sufficiency, which can be interpreted as the lack of surprise. In this way, our
framework can be seen as an extension of the FEP framework to the combinatorial realm
of finite or countable automata. We propose to explore the ideas of FEP further within
this framework. One future direction is to explore the notion of Markov blankets in the
context of discrete non-probabilistic systems. After all, the notion of independence is more
ubiquitous than that dictated by probability theory [3, Ch.2], [21,29].
One drawback of our results is that both our assumptions and conclusions are very
strong. The assumption of sufficiency is unrealistic, and the resulting similarity between
the internal and external models is too strong for most applications. Thus, another di-
rection for future research is to explore significantly weaker notions. For example, in-
stead of minimizing surprise globally, the agent may focus on minimizing surprise rel-
ative to its goals. We envision replacing minimal sufficient refinements by equivalences
that retain only task-relevant information. This approach will guide the theory towards
game-theoretic aspects, incorporating von Neumann’s concepts of turn-taking and discrete
strategies, as well as Nash’s payoff functions and Aumann’s epistemology works.
We are also working on extending the framework to incorporate multiple sensing
modalities within one agent’s brain, as well as multiple agents, thereby uncovering new
pathways in communication theory.
Acknowledgments. Authors 1, 3, and 5 were supported by a European Research Council Advanced Grant (ERC
AdG, ILLUSIVE: Foundations of Perception Engineering, 101020977).
References
1. Antsaklis, P.: Autonomy and metrics of autonomy. Annual Reviews in Control49, 15–26 (2020)
2. Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A.: Deep reinforcement learning: A brief
survey. IEEE Signal Processing Magazine34(6), 26–38 (2017)
3. Baldwin, J.T.: Fundamentals of Stability Theory. Cambridge University Press (Mar 2017)
4. Berger, A.A.: Semiotics and society. Society51(1), 22–26 (Feb 2014 2014/02//)
5. Conant, R.C., Ashby, W.R.: Every good regulator of a system must be a model of that system. International
Journal of Systems Science1(2), 89–97 (1970)
6. Francis, B., Wonham, W.: The internal model principle of control theory. Automatica12(5), 457–465 (1976)
7. Friston, K.: A theory of cortical responses. Philosophical Transactions - Royal Society. Biological Sciences
360(1456), 815–836 (2005)
8. Friston, K.: The free-energy principle: a unified brain theory? Nat. Rev. Neurosci.11, 127–138 (2010)
9. Friston, K.: A free energy principle for biological systems. Entropy14(11), 2100–2121 (2012)
10. Gallagher, S.: Enactivist Interventions: Rethinking the Mind. Oxford University Press (2017)
11. Heins, C., Millidge, B., Costa, L.D., Mann, R.P., Friston, K.J., Couzin, I.D.: Collective behavior from surprise
minimization. Proceedings of the National Academy of Sciences of the United States of America121(17)
(2024)
12. Hutto, D.D., Myin, E.: Radicalizing enactivism: Basic minds without content. MIT Press (2012)
13. Hutto, D.D., Myin, E.: Evolving Enactivism. MIT Press (2017)
An Internal Model Principle For Robots 19
14. Kantaros, Y., Zavlanos, M.M.: Sampling-based optimal control synthesis for multirobot systems under global
temporal tasks. IEEE Transactions on Automatic Control64(5), 1916–1931 (2019)
15. Lanillos, P., Meo, C., Pezzato, C., Meera, A.A., Baioumy, M., Ohata, W., Tschantz, A., Millidge, B., Wisse,
M., Buckley, C.L., et al.: Active inference in robotics and artificial agents: Survey and challenges. arXiv
preprint arXiv:2112.01871 (2021)
16. LaValle, S.M.: Planning Algorithms. Cambridge University Press, Cambridge, U.K. (2006), also available at
http://lavalle.pl/planning/
17. LaValle, S.M., Center, E.G., Ojala, T., Pouke, M., Prencipe, N., Sakcak, B., Suomalainen, M., Timperi, K.G.,
Weinstein, V.: From virtual reality to the emerging discipline of perception engineering. Annual Review of
Control, Robotics, and Autonomous Systems7(1) (Nov 2023)
18. LeCun, Y., Courant: A path towards autonomous machine intelligence version 0.9.2, 2022-06-27 (2022)
19. Littman, M., Sutton, R.S.: Predictive representations of state. Advances in neural information processing
systems 14 (2001)
20. Ma, B., Tang, J., Chen, B., Pan, Y., Zeng, Y.: Tensor optimization with group lasso for multi-agent predictive
state representation. Knowledge-based Systems221, 106893 (2021)
21. Paolini, G.: Independence logic and abstract independence relations. Mathematical Logic Quarterly61(3),
202–216 (May 2015)
22. Parr, T., Pezzulo, G., Friston, K.J.: Active inference: The Free Energy Principle in Mind, Brain and Behaviour
(2022)
23. Ramstead, M.J.D., Friston, K.J., Hipólito, I.: Is the free-energy principle a formal theory of semantics? from
variational density dynamics to neural and phenotypic representations. Entropy22(8), 889 (2020)
24. Rivest, R.L., Schapire, R.E.: Diversity-based inference of finite automata. Journal of the Association for
Computing Machinery41(3), 555–589 (1994)
25. Sakcak, B., Weinstein, V., LaValle, S.M.: The limits of learning and planning: Minimal sufficient information
transition systems. In: 2022 International Workshop on the Algorithmic Foundations of Robotics (WAFR)
(2022)
26. de Saussure, F., Baskin, W., Meisel, P., Saussy, H.: Course in General Linguistics. Columbia University Press
(2011)
27. Tumova, J., Dimarogonas, D.V.: Multi-agent planning under local ltl specifications and event-based synchro-
nization. Automatica70, 239–248 (2016)
28. Varela, F., Rosch, E., Thompson, E.: The Embodied Mind: Cognitive Science and Human Experience. Cam-
bridge, Massachusetts: MIT Press (1992)
29. Väänänen, J.: Dependence Logic: A New Approach to Independence Friendly Logic. London Mathematical
Society Student Texts, Cambridge University Press (2007)
30. Weinstein, V., Sakcak, B., LaValle, S.M.: An enactivist-inspired mathematical model of cognition. Frontiers
in Neurorobotics16 (2022)