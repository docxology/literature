Distributionally Robust Free Energy Principle for Decision-Making
Allahkaram Shaﬁei 1,∗ Hozefa Jesawada 2,∗ Karl Friston 3 Giovanni Russo 4 /Letter
November 25, 2025
Abstract. Despite their groundbreaking performance, autonomous agents can misbeh ave when
training and environmental conditions become inconsistent, with mi nor mismatches leading to undesir-
able behaviors or even catastrophic failures. Robustness towards the se training-environment ambiguities
is a core requirement for intelligent agents and its fulﬁllment is a long-standing challenge towards their
real-world deployments. Here, we introduce a Distributionally R obust Free Energy model (DR-FREE)
that instills this core property by design. Combining a robust exte nsion of the free energy principle with
a resolution engine, DR-FREE wires robustness into the agent decis ion-making mechanisms. Across
benchmark experiments, DR-FREE enables the agents to complete th e task even when, in contrast,
state-of-the-art models fail. This milestone may inspire both dep loyments in multi-agent settings and,
at a perhaps deeper level, the quest for an explanation of how natural agent s – with little or no training
– survive in capricious environments.
Introduction
A popular approach to designing autonomous agents is to feed them with data, u sing Reinforcement
Learning (RL) and simulators to train a policy (Fig.
1a). Deep RL agents designed on this paradigm have
demonstrated remarkable abilities, including outracing human champ ions in Gran Turismo, 1 playing Atari
games,2 controlling plasmas 3 and achieving champion-level performance in drone races. 4 However, despite
their groundbreaking performance, state-of-the-art agents cannot yet c ompete with natural intelligence
in terms of policy robustness: natural agents have, perhaps through evol ution, acquired decision-making
abilities so that they can function in challenging environments des pite little or no training. 6–8 In contrast,
for artiﬁcial agents, even when they have access to a high ﬁdelity sim ulator, the learned policies can be
brittle to mismatches, or ambiguities, between the model availabl e during learning and the real environment
(Fig. 1b). For example, drone-champions and Atari-playing agents assume consi stent environmental
conditions from training, and if this assumption fails, because, e.g., t he environment illumination or color
of the objects changes, or the drone has a malfunctioning – so that its dyn amics becomes diﬀerent from
the one available during training – learned policies can fail. More gene rally, model ambiguities – even if
minor – can lead to non-robust behaviors and failures in open-world envi ronments.9 Achieving robustness
towards these training/environment ambiguities is a long-standing ch allenge10–12 for the design of intelligent
machines6, 13, 14 that can operate in the real world.
1 Czech Technical University, Prague, Czech Republic.2 New York University Abu Dhabi, Abu Dhabi Emirate.3 Wellcome
Centre for Human Neuroimaging, Institute of Neurology, University College London, United Kingdom. 4 Department of
Information and Electrical Engineering and Applied Mathematics, University of Salerno, Italy.
∗ These authors contributed equally./Lettere-mail: giovarusso@unisa.it
1
arXiv:2503.13223v3  [cs.AI]  24 Nov 2025
Here we present DR-FREE, a free energy 15, 16 computational model that addresses this challenge: DR-
FREE instills this core property of intelligence directly into t he agent decision-making mechanisms. This
is achieved by grounding DR-FREE in the minimization of the free ene rgy, a unifying account across
information theory, machine learning, 17–23 neuroscience, computational and cognitive sciences. 24–30 The
principle postulates that adaptive behaviors in natural and artiﬁcial age nts arise from the minimization
of variational free energy (Fig. 1c). DR-FREE consists of two components. The ﬁrst component is an
extension of the free energy principle: the distributionally robu st (DR) free energy (FREE) principle,
which fundamentally reformulates how free energy minimizing agent s handle ambiguity. While classic
free energy models (Fig. 1c) obtain a policy by minimizing the free energy based on a model of the
environment available to the agent, under our robust principle, fre e energy is instead minimized across
all possible environments within an ambiguity set around a trained mod el. The set is deﬁned in terms of
statistical complexity around the trained model. This means the acti ons of the agent are sampled from a
policy that minimizes the maximum free energy across ambiguities. Th e robust principle yields the problem
statement for policy computation. This is a distributionally robust p roblem having a free energy functional
as objective and ambiguity constraints formalized in terms of statistic al complexity. The problem has not
only a nonlinear cost functional with nonlinear constraints but also prob ability densities over decision
variables that equip agents with explicit estimates of uncertainty an d conﬁdence. The product of this
framework is a policy that minimizes free energy and is robust across mo del ambiguities. The second key
component of DR-FREE – its resolution engine – is the method to comput e this policy. In contrast to
conventional approaches for policy computation based on free energy models , our method shows that the
policy can be conveniently found by ﬁrst maximizing the free energy across model ambiguities – furnishing
a cost under ambiguity – and then minimizing the free energy in the p olicy space (Fig. 1d). Put simply,
policies are selected under the best worst-case scenario, where th e worst cases accommodate ambiguity.
Our robust free energy principle yields – when there is no ambigui ty – a problem statement for policy
computation naturally arising across learning 22, 31, 32 – in the context of maximum diﬀusion (MaxDiﬀ) and
maximum entropy (MaxEnt) – and control. 46 This means that DR-FREE can yield policies that not only
inherit all the desirable properties of these approaches but ensure s them across an ambiguity set, which is
explicitly deﬁned in the formulation. In MaxEnt – and MaxDiﬀ – robust ness depends on the entropy of
the optimal policy, with explicit bounds on the ambiguity set over wh ich the policy is robust available in
discrete settings. 32 To compute a policy that robustly maximizes a reward, MaxEnt needs to be used with
a diﬀerent, pessimistic, reward 32 – this is not required in DR-FREE. These desirable features of our f ree
energy computational model are enabled by its resolution engine. This i s – to the best of our knowledge
– the only available method tackling the full distributionally robu st, nonlinear and inﬁnite-dimensional
policy computation problem arising from our robust principle; see Res ults and Sec. S2 in Supplementary
Information for details. In Supplementary Information we also highlight a connection with the Markov
Decision Processes (MDPs) formalism. DR-FREE yields a policy wi th a well-deﬁned structure: this is a
soft-max with its exponent depending on ambiguity. This structure elucidates the crucial role of ambiguity
on optimal decisions, i.e., how it modulates the probability of selec ting a given action.
DR-FREE not only returns the policy arising from our free energy mode l, but also establishes its per-
formance limits. In doing so, DR-FREE harbors two implications. Firs t, DR-FREE policy is interpretable
and supports (Bayesian) belief updating. The second implication is t hat it is impossible for an agent faced
with ambiguity to outperform an ambiguity-free agent. As ambiguity vanish es, DR-FREE recovers the
policy of an agent that has perfect knowledge of its environment, and no agen t can obtain better per-
formance. Vice-versa, as ambiguity increases, DR-FREE shows that the policy down-weights the model
2
available to the agent over ambiguity.
We evaluate DR-FREE on an experimental testbed involving real rover s, which are given the task
of reaching a desired destination while avoiding obstacles. The trai ned model available to DR-FREE is
learned from biased experimental data, which does not adequately captu re the real environment and
introduces ambiguity. In the experiments – even despite the amb iguity arising from having learned a
model from biased data – DR-FREE successfully enables the rovers to complete their task, even in settings
where both a state-of-the-art free energy minimizing agent and other m ethods struggle, unable to complete
the task. The experiments results – conﬁrmed by evaluating DR-FR EE in a popular higher dimensional
simulated environment – suggest that, to operate in open environments , agents require built-in robustness
mechanisms and these are crucial to compensate for poor training. DR-FR EE, providing a mechanism
that deﬁnes robustness in the problem formulation, delivers this c apability.
Our free energy computational model, DR-FREE, reveals how free ene rgy minimizing agents can com-
pute optimal actions that are robust over an ambiguity set deﬁned in the p roblem formulation. It establishes
a normative framework to both empower the design of artiﬁcial agents buil t upon free energy models with
robust decision-making abilities, and to understand natural behavi ors beyond current free energy explana-
tions.35–39 Despite its success, there is no theory currently explaining i f and how these free energy agents
can compute actions in ambiguous settings. DR-FREE provides these ex planations.
Results
DR-FREE. DR-FREE comprises a distributionally robust free energy princip le and the accompanying
resolution engine – the principle (Fig.
2a) is the problem statement for policy computation; the resolution
engine is the method for policy computation. The principle establis hes a sequential policy optimization
framework, where randomized policies arise from the minimization of th e maximum free energy over ambi-
guity. The resolution engine ﬁnds the solution in the space of policies . This is done by computing – via the
maximum free energy over all possible environments in the ambiguity set – a cost associated to ambiguity.
Then, the ensuing maximum free energy is minimized in policy spac e (Fig. 1d).
In Fig. 1a, random variables Xk−1 and Uk are state at time k −1 and action at k (see Methods and Sec.
S3 of Supplementary Information). The agent infers the state of a stoch astic environment pk (xk ∣xk−1, uk)
and the action is sampled from πk (uk ∣xk−1). In the decision horizon (e.g., from 1 to N) the agent-
environment interactions are captured by p0∶N , deﬁned as p0(x0)∏N
k=1 pk (xk ∣xk−1, uk)πk (uk ∣xk−1), where
p0(x0) is an initial prior. The trained model available to the agent, ¯ pk (xk ∣xk−1, uk), does not (necessarily)
match its environment and the goal of DR-FREE is to compute a policy that, while optimal, is robust
against the training/environment ambiguities. For example, if – as in our e xperiments – the robot model
of Fig.
1a is learned from corrupted/biased data, then it diﬀers from the real rob ot and this gives rise to
ambiguities. These ambiguities are captured via the ambiguity set of F ig. 1b. For a given state/action
pair, this is the set of all possible environments with statistical c omplexity within ηk (xk−1, uk) from the
trained model. For this reason, the ambiguity set Bη (¯pk (xk ∣xk−1, uk)) is captured via the Kullback-
Leibler (KL) divergence: the ambiguity set is the set of all possible m odels, say pk (xk ∣xk−1, uk), such that
DKL (pk (xk ∣xk−1, uk) ∣∣ ¯pk (xk ∣xk−1, uk))≤ηk (xk−1, uk). The radius of ambiguity available to DR-FREE,
ηk (xk−1, uk), is positive and bounded. For a state/action pair, a small radius indicate s low ambiguity,
meaning that the agent is conﬁdent in its trained model. Vice-versa, h igh values indicate larger ambiguity
and hence low conﬁdence in the model. See Methods for the detailed deﬁnitions.
DR-FREE computes the optimal policy via a distributionally robust ge neralization of the free en-
3
Figure 1: Comparison between free energy and robust free energy for polic y computation. a.
A robotic agent navigating a stochastic environment to reach a destinat ion while avoiding obstacles. At a
given time-step, k −1, the agent determines an action Uk from a policy using a model of the environment
(e.g., available at training via a simulator possibly updated via real w orld data) and observations/beliefs
(grouped in the state Xk−1). The environment and model can change over time. Capital letters are
random variables, lower-case letters are realizations. b. The trained model and the agent environment
diﬀer. This mismatch is a training/environment (model) ambiguity: for a state/action pair, the ambiguity
set is the set of all possible environments that have statistical comp lexity from the trained model of at
most ηk (xk−1, uk). We use the wording trained model in a very broad sense. A trained mo del is any
model available to the agent oﬄine: for example, this could be a model ob tained from a simulator or, for
natural agents, this could be hardwired into evolutionary processes or even determined by prior beliefs.
c. A free energy minimizing agent in an environment matching its own mo del. The agent determines an
action by sampling from the policy π⋆
k (uk ∣xk−1). Given the model, the policy is obtained by minimizing
the variational free energy: the sum of a statistical complexity (with respect to a generative model, q0∶N )
and expected loss (state/action costs, c(x)
k (xk) and c(u)
k (uk)) terms. d. DR-FREE extends the free energy
principle to account for model ambiguities. According to DR-FREE, t he maximum free energy across all
environments – in an ambiguity set – is minimized to identify a robu st policy. This amounts to variational
policy optimization under the epistemic uncertainty engendered b y ambiguous environment.
ergy principle that accounts for ambiguities. This ﬁrst component of DR -FREE, providing the problem
statement for policy computation, generalizes the conventional free en ergy principle to ensure policies re-
main robust even when the environment deviates from the trained mod el. Our principle is formulated as
follows: over the decision horizon, the optimal policy sequence {π⋆
k (uk ∣xk−1)}1∶N is obtained by mini-
mizing over the policies the maximum free energy across all possible environments in the ambiguity set
expected under the policy in question. The expected free energy combines two terms: (i) the statistical
complexity, DKL (p0∶N ∣∣q0∶N ), of the agent-environment behavior from q0∶N ; (ii) the expected cumulative
loss Ep0∶N [∑N
k=1 (c(x)
k (Xk)+c(u)
k (Uk) )]. The principle is formalized in Fig.
2a (see Methods for details)
as a distributionally robust policy optimization problem in the probabi lity space. DR-FREE computes the
4
Figure 2: DR-FREE a. Summarizing the distributionally robust free energy principle – the problem
statement for policy computation. Our generalization of active inferenc e yields an optimization framework
where policies emerge by minimizing the maximum free energy over all possible environments in the
ambiguity set, which formalizes the constraints in the problem formu lation. b. The resolution engine to
ﬁnd the policy. Given the current state, the engine uses the gener ative model and the loss to ﬁnd the
maximum free energy DKL (pk (xk ∣xk−1, uk) ∣∣qk (xk ∣xk−1, uk)) + Epk(xk∣xk−1,uk) [¯ck (Xk)] across all the
environments in the ambiguity set. This yields the cost of ambiguity ηk (xk−1, uk)+˜c (xk−1, uk) that builds
up the expected loss for the subsequent minimization problem. In t his second problem, the variational free
energy is minimized in the space of polices providing: (i) π⋆
k (uk ∣xk−1), the DR-FREE policy from which
actions are sampled. Elements that guarantee robustness in green – thes e terms depend on ambiguity; (ii)
the smallest free energy that the agent can achieve, i.e., the cost-to- go ¯ck (xk) fed back to the maximization
problem at the next time-step. For reactive actions, where N =1, the cost-to-go equals the state cost given
by the agent loss. c. Using the generative model and the state-cost, DR-FREE ﬁrst compute s the cost of
ambiguity, which is non-negative. This, together with the action cost i s then used to obtain the exponential
kernel in the policy, i.e. exp (−c(u)
k (uk)−ηk (xk−1, uk)−˜c (xk−1, uk)). After multiplication of the kernel
with qk (uk ∣xk−1) and normalization, this returns π⋆
k (uk ∣xk−1).
5
policy via bi-level optimization, without requiring the environm ent pk (xk ∣xk−1, uk) and without perform-
ing stochastic sampling for the ambiguity set. The ambiguity set deﬁ nes the problem constraints – these
are nonlinear in the decision variable – and the min-max objective is th e free energy – also nonlinear in the
decision variables (see Sec. S2 in the Supplementary Information for connections with other frameworks).
As also shown in Fig. 2a, the complexity term in the min-max objective regularizes the opti mal policy
to prevent environment-agent interactions that are overly complex w ith respect to q0∶N . This is speciﬁed
as q0(x0)∏N
k=1 qk (xk ∣xk−1, uk)qk (uk ∣xk−1), with q0(x0) being a prior. Hence, the ﬁrst term in the min-
max objective biases the optimal solution of our robust principle – DR-F REE policy – towards q0∶N . The
second term in the objective minimizes the worst case expected los s across ambiguity. We refer to q0∶N as
generative model, although – for the application of our principle in Fig
2a – this is not necessarily required
to be a time-series model. For example, in some of our navigation exper iments, q0∶N only encodes the
agent goal destination – in this case the complexity term in Fig. 2a encodes an error from the goal. In
contrast, in a second set of experiments – where we relate DR-FREE t o a state-of-the-art approach from
the literature 22, 31 – q0∶N encodes a time-series model. Our formulation also allows for the gen erative model
qk (xk ∣xk−1, uk) – provided to DR-FREE – to be diﬀerent from the true environment pk (xk ∣xk−1, uk) –
not available to the agent – and the trained model ¯ pk (xk ∣xk−1, uk), see also Fig. 1a-b. This feature can
be useful in applications where, by construction, the model availab le to the agent and the generative model
diﬀer.25 Both reactive and planned behaviors can be seen through the lenses of our formulation: a width
of the decision horizon N greater than 1 means that the policy at time-step k is computed as part of a plan.
If N = 1, the agent action is reactive (or greedy). These actions frequently em erge as reﬂexive responses
in biologically inspired motor control. 40, 41 In brief, this generalization of active inference can be regarded
as robust Bayesian model averaging to accommodate epistemic uncertai nty about the environment, where
the prior over models is supplied by the KL divergence between e ach model and the trained model. When
there is no ambiguity, our robust principle in Fig. 2a connects with free energy minimization in active
inference based upon expected free energy 42 – which itself generalizes schemes such as KL control and
control as inference. 39, 43–45 See Methods and Sec. S2 of Supplementary Information.
The policy optimization problem in Fig. 2a is inﬁnite-dimensional as both minimization and maxi-
mization are in the space of probability densities. This enables a Bay es-optimal handling of uncertainty
and ambiguity that characterizes control and planning as (active) infere nce. DR-FREE resolution engine –
the method to compute the policy – not only ﬁnds the policy but also, pe rhaps counterintuitively, returns
a solution with a well-deﬁned and explicit functional form. The analy tical results behind the resolution
engine are in Sec. S3 and Sec. S6 of the Supplementary Information. In s ummary, the analytical results
show that, at each k, the optimal policy can be found via a bi-level optimization approach, ﬁr st maximizing
free energy across the ambiguity constraint and then minimizing over the policies. While the maximization
problem is still inﬁnite dimensional, its optimal value – yielding a cost of ambiguity – can be obtained
by solving a scalar optimization problem. This scalar optimization proble m is convex and has a global
minimum. Therefore, once the cost of ambiguity is obtained, the resul ting free energy can be minimized in
the policy space and the optimal policy is unique. These theoretical ﬁ ndings are summarized in Fig. 2b.
Speciﬁcally, the policy at time-step k is a soft-max (Fig. 2b) obtained by equipping the generative model
qk (uk ∣xk−1) with an exponential kernel. The kernel contains two costs: the action cost c(u)
k (uk) and the
cost of ambiguity, ηk (xk−1, uk)+˜c (xk−1, uk). Intuitively, given the cost-to-go ¯ ck (xk), the latter cost is the
maximum free energy across all possible environments in the ambiguity set. The inﬁnite dimensional free
energy maximization step, which can be reduced to scalar convex optimi zation, yields a cost of ambiguity
that is always bounded and non-negative (Fig. 2c and Methods). This implies that an agent always in-
6
curs a positive cost for ambiguity and the higher this cost is for a given s tate/action pair, the lower the
probability of sampling that action is. The result is that DR-FREE poli cy balances between the cost of
ambiguity and the agent beliefs encoded in the generative model (Fig. 2c).
DR-FREE succeeds when ambiguity-unaware free energy minimizin g agents fail. To evaluate
DR-FREE we speciﬁcally considered an experiment where simplic ity was a deliberate feature, ensuring that
the eﬀects of model ambiguity on decision-making could be identiﬁed , benchmarked against the literature, 46
and measured quantitatively. The experimentation platform (Fig. 3a) is the Robotarium, 47 providing both
hardware and a high-ﬁdelity simulator. The task is robot navigation: a rov er needs to reach a goal desti-
nation while avoiding obstacles (Fig. 3b). In this set-up we demonstrate that an ambiguity-unaware free
energy minimizing agent – even if it makes optimal actions – does not rel iably complete the task, while
DR-FREE succeeds. The ambiguity-unaware agent from the literature 46 computes the optimal policy by
solving a relaxation of the problem in Fig. 2a without ambiguity. This agent solves a policy computation
problem – relevant across learning and control 56 – having DR-FREE objective but without constraints.
We performed several experiments: in each experiment, DR-FREE , used to compute reactive actions, only
had access to a trained model ¯ pk (xk ∣xk−1, uk) and did not know pk (xk ∣xk−1, uk). We trained oﬀ-line a
Gaussian Process model, learned in stages. At each stage, data were obtai ned by applying randomly sam-
pled actions to the robot and a bias was purposely added to the robot position s (see Experiments settings
in Methods for training details and Sec. S5 in Supplementary Inform ation for the data) thus introducing
ambiguity. The corrupted data from each stage were then used to learn a t rained model via Gaussian
Processes. Fig. 3c shows the performance of DR-FREE at each stage of the training, compared t o the per-
formance of a free energy minimizing agent that makes optimal decisions b ut is ambiguity unaware. In the
ﬁrst set of experiments, when equipped with DR-FREE, the robot i s always able to successfully complete
the task (top panels in Fig. 3c): in all the experiments, the robot was able to reach the goal while avoi ding
the obstacles. In contrast, in the second set of experiments, when t he robot computes reactive actions by
minimizing the free energy – without using DR-FREE – it fails the task, crashing in the obstacles, except
in trivial cases where the shortest path is obstacle-free (see Fig. 3c, bottom; details in Methods). The
conclusion is conﬁrmed when this ambiguity-unaware agent is equippe d with planning capabilities. As
shown in Supplementary Fig. 3 for diﬀerent widths of the planning h orizon, the ambiguity-unaware agent
still only fulﬁlls the task when the shortest path is obstacle-free , conﬁrming the ﬁndings shown in Fig. 3c,
bottom. The experiments provide two key highlights. First, ambigu ity alone can have a catastrophic im-
pact on the agent and its surroundings. Second, DR-FREE enables agents to succeed in their task despite
the very same ambiguity. This conclusion is also supported by experi ments where DR-FREE is deployed
on the Robotarium hardware. As shown in Fig. 3d, DR-FREE in fact enabled the robot provided by the
Robotarium to navigate to the destination, eﬀectively completing th e task despite model ambiguity. The
computation time measured on the Robotarium hardware experiments was of ap proximately 0 .22 seconds
(Methods for details). See Data Availability for a recording; code also provided (see Code Availability).
Supplementary Fig. 4 presents results from a complementary set of e xperiments in the same domain but
featuring diﬀerent goal positions and obstacle conﬁgurations. The experi ments conﬁrm that, despite ambi-
guity, DR-FREE consistently enables the robot to complete the task ac ross all tested environments (code
also available).
DR-FREE elucidates the mechanistic role of ambiguity on optimal deci sion making. DR-
FREE policy (Fig.
2b) assigns lower probabilities to states and actions associated with hi gher ambiguity.
7
Figure 3: DR-FREE evaluation. a. Unicycle robots of 11cm ×8.5cm×7.5cm (width, length, height) that
need to achieve the goal destination, xd, avoiding obstacles. The work area is 3m ×2m, the robot position
is the state, and actions are vertical/horizontal speeds; qk (xk ∣xk−1, uk) is a Gaussian centered in xd and
qk (uk ∣xk−1) is uniform. See Methods for the settings. b. The non-convex state cost for the navigation
task. See Methods for the expression. c. Comparison between DR-FREE and a free-energy minimizing
agent that makes optimal decisions but is unaware of the ambiguity. DR-FR EE enables the robot to
successfully complete the task at each training stage. The ambiguity- unaware agent fails, except when
the shortest path is obstacle-free. Training details are in Methods . d. Screenshots from the Robotarium
platform recording of one experiment. DR-FREE allows the robot (start ing top-right) to complete the
task (trained model from stage 3 used). e. How DR-FREE policy changes as a function of ambiguity.
By increasing the radius of ambiguity by 50%, DR-FREE policy (left) be comes a policy dominated by
ambiguity (right). As a result, actions with low ambiguity are assigned h igher probability. Screenshot of
the robot policy when this is in position [0.2, 0.9], i.e., near the middle obstacle. The ambiguity increase
deterministically drives the robot bottom-left (note the higher pr obability) regardless of the presence of the
obstacle. f. Belief update. Speeds/positions from the top-right experiments in panel c) are used together
with F = 16 state/action features, φi(xk−1, uk) = E¯pk(xk∣xk−1,uk) [ϕi(Xk)] in Supplementary Fig. 1b. Once
the optimal weights, w⋆
i , are obtained, the reconstructed cost is −E¯pk(xk∣xk−1,uk) [∑16
i=1 w⋆
i ϕi(Xk)]. Since
this lives in a 4-dimensional space, we show −∑16
i=1 w⋆
i ϕi(xk), which can be conveniently plotted.
8
In simpler terms, an agent that follows DR-FREE policy is more likely to select actions and states associated
with lower ambiguity. DR-FREE yields a characterization of the agent b ehavior in regimes of small and
large ambiguity. Intuitively, as ambiguity increases, DR-FREE yiel ds a policy dominated by the agent’s
generative model and radius of ambiguity. In essence, as ambiguity incr eases, DR-FREE implies that the
agent grounds decisions on priors and ambiguity, reﬂecting its lack of conﬁ dence. Conversely, when the
agent is conﬁdent about its trained model, DR-FREE returns the polic y of a free energy minimizing agent
making optimal decisions in a well-understood, ambiguity-free, e nvironment.
Characterizing optimal decisions in the regime of large ambiguity amounts at studying DR-FREE pol-
icy (Fig. 2b) as ηk (xk−1, uk) increases. More precisely, this means studying what happens when ηmin =
min ηk (xk−1, uk) increases. Since ˜ c (xk−1, uk) and c(u)
k (uk) are non-negative, and since ˜ c (xk−1, uk) does
not depend on ηk (xk−1, uk) for suﬃciently large ηmin, this means that, when ηk (xk−1, uk) is large enough,
exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk)) ≈ exp (−ηk (xk−1, uk)) in DR-FREE policy (derivations in
Supplementary Information). Therefore, as ambiguity increases, π⋆
k (uk ∣xk−1) only depends on the gen-
erative model and the ambiguity radius. Essentially, DR-FREE show s that, when an agent is very unsure
about its environment, optimal decisions are dominated by the generativ e model and the radius of ambi-
guity. In our experiments, an implication of this is that with larger ηk (xk−1, uk) the robot may sacriﬁce
physical risk (obstacle avoidance) in favor of epistemic risk (avoid ing model mismatch). Hence, an inter-
esting but not usual outcome is that risk-averse is not equal to obstacl e-avoidance especially when model
uncertainty is not evenly distributed. This behavior is clearly evidenced in our experiments. As shown
in Fig. 3e, as ambiguity increases the agent’s policy, becoming dominated by ambi guity, deterministically
directs the robot towards the goal position – associated to the lowest amb iguity – disregarding the presence
of obstacles. At the time-step captured in the ﬁgure, the robot was to t he right of the middle obstacle.
Consequently, while DR-FREE policy with the original ambiguity radi us would assign higher probabilities
to speeds that drive the robot towards the bottom of the work-area, when am biguity increases the robot
is instead directed bottom-left and this engenders a behavior that m akes the robot crash in the obstacle.
Conversely, to characterize the DR-FREE policy in the regimes of lo w ambiguity, we need to study
how the policy changes as ηk (xk−1, uk) shrinks. As ηk (xk−1, uk) → 0, the ambiguity constraint is relaxed
and ˜c (xk−1, uk) simply becomes DKL (¯pk (xk ∣xk−1, uk) ∣∣qk (xk ∣xk−1, uk))+E¯pk(xk∣xk−1,uk) [¯ck (Xk)]. See
Supplementary Information for the derivations. This yields the opti mal policy from the literature. 46 The
minimum free energy attained by this policy when there is no ambigui ty is always smaller than the free
energy achieved by an agent aﬀected by ambiguity. Hence, DR-FREE shows that ambiguity cannot be
exploited by a free energy minimizing agent to obtain a better cost. S ee Methods for further discussion.
Supplementary Fig. 5 show experiments results for diﬀerent ambi guity radii. Experiments conﬁrm that,
due to the presence of ambiguity, when the radius is set to zero the agent – now ambiguity unaware – does
not always complete the task. Additionally, we conduct a second set of ex periments in which the trained
model ¯pk (xk ∣xk−1, uk) coincides with pk (xk ∣xk−1, uk). This scenario remains stochastic but no longer
features ambiguity. The results (Supplementary Fig. 6) show that – u nlike in the previous experiments –
DR-FREE enables the robot to complete the task even when the ambiguit y radius is set to 0. This outcome
is consistent with our analysis: when there is no ambiguity, the KL d ivergence between pk (xk ∣xk−1, uk)
and ¯pk (xk ∣xk−1, uk) is identically zero and DR-FREE therefore succeeds even for a zer o ambiguity radius.
We provide the code to replicate the results (see Code Availabili ty).
DR-FREE supports Bayesian belief updating. DR-FREE policy associates higher probabilities to
actions for which the combined action and ambiguity cost c(u)
k (uk)+ηk (xk−1, uk)+˜c (xk−1, uk) is higher.
9
This means that the reason why an action is observed can be understood by estimating this combined cost:
DR-FREE supports a systematic framework to achieve this. Given a s equence of observed states/actions,
(ˆxk−1, ˆuk) and the generative policy qk (uk ∣xk−1), the combined cost can be estimated by minimizing
the negative log-likelihood. The resulting optimization problem is convex if a widely adopted (see Meth-
ods) linear parametrization of the cost in terms of known (and arbitrary) features is available. Given F
state/action features and G action features, this parametrization is ∑F
i=1 wiφi(xk−1, uk) + ∑G
i=1 viγi(uk).
Reconstructing the cost then amounts at ﬁnding the optimal weights th at minimize the negative log-
likelihood, with likelihood function being ∏M
k=1 p⋆
k (ˆuk ∣ ˆxk−1; v, w). Here, M is the number of observed
state/inputs pairs and p⋆
k (ˆuk ∣ ˆxk−1; v, w) is, following the literature,
46 the DR-FREE policy itself but
with −c(u)
k (uk)−ηk (xk−1, uk)−˜c (xk−1, uk) replaced by the linear parametrization ( v and w are the stacks
of the parametrization weights). We unpack the resulting optimization problem in the Methods. This
convenient implication of DR-FREE policy allows one to reconstruct t he cost driving the actions of the
rovers using DR-FREE in our experiments and Fig. 3f shows the outcome of this process. The similarity
with Fig. 3b is striking and to further assess the eﬀectiveness of the recons tructed cost we carried out a
number of additional experiments. In these experiments, the robot s are equipped with DR-FREE policy
but, crucially, −c(u)
k (uk)−ηk (xk−1, uk)−˜c (xk−1, uk) is replaced with the reconstructed cost. The outcome
from these experiments conﬁrms the eﬀectiveness of the results : as shown in Supplementary Fig. 1d,
the robots are again able to fulﬁll their goal, despite ambiguity. Additional ly, we also benchmarked our
reconstruction result with other state-of-the-art approaches. Spec iﬁcally, we use an algorithm from the
literature that, building on maximum entropy, 48 is most related to our approach. 49 This algorithm makes
use of Monte Carlo sampling and soft-value iteration. When using this al gorithm – after benchmarking it
on simpler problems – we observed that it would not converge to a reasonabl e estimate of the robot cost
(see Supplementary Fig. 1e for the reconstructed cost using this app roach and the Methods for details).
Since our proposed approach leads to a convex optimization problem, our cost reconstruction results could
be implemented via oﬀ-the-shelf software tools. The code for the i mplementation is provided (see Code
Availability).
Relaxing ambiguity yields maximum diﬀusion. Maximum diﬀusion (MaxDiﬀ) is a policy compu-
tation framework that generalizes maximum entropy (MaxEnt) and inherit s its robustness properties. It
outperforms other state-of-the-art methods across popular benchmarks .
22, 31 We show that the distribu-
tionally robust free energy principle (Fig. 2a) can recover, with a proper choice of q0∶N , the MaxDiﬀ
objective when ambiguity is relaxed. This explicitly connects DR -FREE to MaxDiﬀ and – through it – to
a broader literature on robust decision-making (Sec. S2 of Supplemen tary Information). In MaxEnt – and
MaxDiﬀ – robustness guarantees stem from the entropy of the optimal polic y,32 with explicit a-posteriori
bounds on the ambiguity set over which the policy guarantees robustnes s available for discrete settings and
featuring a constant radius of ambiguity [ 32, Lemma 4.3]. To compute policies that robustly maximize a
reward, MaxEnt must be used with an auxiliary, pessimistic, reward .32 In contrast, by tackling the prob-
lem in Fig. 2a, DR-FREE deﬁnes robustness guarantees directly in the problem for mulation, explicitly via
the ambiguity set. As a result, DR-FREE policy is guaranteed to be robu st across this ambiguity set. As
detailed in Sec. S2 of Supplementary Information, DR-FREE is, to our knowledge, the full min-max prob-
lem in Fig. 2a – featuring at the same time a free energy objective and distributi onally robust constraints
– remains a challenge for many methods. 11, 22, 31, 32 This is not just a theoretical achievement uniquely
positioning DR-FREE in the literature – we explore its implication s by revisiting our robot navigation
task: we equip DR-FREE with a generative model that recovers MaxDi ﬀ objective and compare their
10
performance. The experiments reveal that DR-FREE succeeds in se ttings where MaxDiﬀ fails. This is
because DR-FREE not only retains the desirable properties of MaxDiﬀ, but also guarantees them in the
worst case over the ambiguity set.
In MaxDiﬀ, given some initial state x0, policy computation is framed as minimizing in the policy space
DKL (p0∶N ∣∣pmax(x0∶N , u1∶N )). This is the KL divergence between (using the time-indexing an d notation
in Fig. 2a for consistency) p0∶N and pmax(x0∶N , u1∶N ) = 1
Z ∏N
k=1 pmax(xk ∣ xk−1)exp (r(xk, uk)). In this last
expression, r(xk, uk) is the state/action reward when the agent transitions in state xk under action uk, Z
is the normalizer and pmax(xk ∣ xk−1) is the maximum entropy sample path probability.
22 On the other
hand, the distributionally robust free energy principle (Fig. 2a) is equivalent to
min
{πk(uk∣xk−1)}1∶N
max
pk(xk∣xk−1,uk)∈Bη(¯pk(xk∣xk−1,uk))
DKL (p0∶N ∣∣ ˜q0∶N ) (1)
in the sense that the optimal solution of ( 1) is the same as the optimal solution of the problem in Fig. 2a
(see Methods). In the above expression, ˜ q0∶N = 1
Z q0∶N exp(−∑N
k=1(c(x)
k (xk)+c(u)
k (uk))) and Z is again a
normalizer. When there is no ambiguity, the optimization problem in (
1) is relaxed and it becomes
min
{πk(uk∣xk−1)}1∶N
DKL (p0∶N ∣∣ ˜q0∶N ) (2)
Given an initial state x0, as we unpack in the Methods, this problem has the same optimal solution as
the MaxDiﬀ objective when the reward is −c(x)
k (xk)−c(u)
k (uk) and in the DR-FREE generative model –
which we recall is deﬁned as q0(x0)∏N
k=1 qk (xk ∣xk−1, uk)qk (uk ∣xk−1) – we set qk (xk ∣xk−1, uk) to be the
maximum entropy sample path probability and qk (uk ∣xk−1) to be uniform.
The above derivations show that relaxing ambiguity in DR-FREE yield s, with a properly deﬁned q0∶N ,
the MaxDiﬀ objective – provided that the rewards are the same. This m eans that, in this setting, DR-
FREE guarantees the desirable MaxDiﬀ properties in the worst case, ov er Bη (¯pk (xk ∣xk−1, uk)). To eval-
uate the implications of this ﬁnding, we revisit the robot navigation tas k. We equip DR-FREE with q0∶N
deﬁned as described above, in accordance with MaxDiﬀ, and compare DR-F REE with MaxDiﬀ itself. In
the experiments, the cost is again the one of Fig.
3b and again the agents have access to ¯ pk (xk ∣xk−1, uk).
Initial conditions are the same as in Fig. 3c. Given this setting, Fig. 4a summarizes the success rates of
MaxDiﬀ – i.e., the number of times the agent successfully reaches t he goal – for diﬀerent values of key
hyperparameters:22 samples and horizon, used to compute the maximum entropy sample path pr obability.
The ﬁgure shows a sweetspot where 100% success rate is achieved, yiel ding two key insights. First, increas-
ing samples increases the success rate. Second – and rather counter- intuitively – planning horizons that are
too long yield a decrease in the success rate and this might be an eﬀec t of planning under a wrong model.
Worst performance are obtained with horizon set to 2 and Fig. 4b conﬁrms that the success rates remain
consistent when an additional temperature-like 22 hyperparameter is changed. Given this analysis, to com-
pare DR-FREE with MaxDiﬀ, we equip DR-FREE with the generative mo del computed in accordance
with MaxDiﬀ (with horizon set to 2 and samples to 50). In this setting, M axDiﬀ successfully completes
the task when the shortest path between the robot initial position and t he goal is obstacle free (Fig. 4c).
In contrast, in this very same setting, the experiments show that D R-FREE – computing reactive actions
– allows the robot to consistently complete its task (Fig. 4d). This desirable behavior is conﬁrmed when
samples is decreased to 10 (Fig. 4e). In summary, the experiments conﬁrm that DR-FREE succeeds in
settings where MaxDiﬀ fails. Experiments details are reported in M ethods (Experiments settings) and
Supplementary Information (Sec. S6 and Tab. S-1). See also code availab ility.
11
Finally, we evaluate DR-FREE in the MuJoCo 33 Ant environment (Fig. 5a). The goal is for the
quadruped agent to move forward along the x-axis while maintaining an upright posture. Each episode lasts
1000 steps, unless the Ant becomes unhealthy – a terminal condition deﬁ ned in the standard environment
that indicates failure. We compare DR-FREE with all previously consi dered methods, as well as with
model-predictive path integral control 34 (NN-MPPI). Across all experiments, the agents have access to
the trained model ¯ pk (xk ∣xk−1, uk) and not to pk (xk ∣xk−1, uk). The trained model was obtained using
the same neural network architecture as in the original MaxDiﬀ paper, 22 which also included benchmarks
with NN-MPPI. The cost provided to the agents is the same across all the e xperiments and corresponds
to the negative reward deﬁned by the standard environment. Fig. 5b shows the experimental results for
this setting. The experiments yield two main observations. First , DR-FREE outperforms all comparison
methods on average, and even the highest error bars (standard deviations from the mean) of the other
methods do not surpass DR-FREE average return. Second, in some of the trials, the other methods
would terminate prematurely the episode due to the Ant becoming un healthy. In contrast, across all DR-
FREE experiments the Ant is always kept healthy and therefore epis odes do not terminate prematurely.
See Experiments Settings in Methods and Supplementary Informati on for details; code also provided.
Discussion
Robustness is a core requirement for intelligent agents that need to operate in the real world. Rather
than leaving its fulﬁllment to – quoting the literature
5 – an emergent and potentially brittle property
from training, DR-FREE ensures this core requirement by design, b uilding on the minimization of the free
energy and installing sequential policy optimization into a rigorous (v ariational or Bayesian) framework.
DR-FREE provides not only a free energy principle that accounts for e nvironmental ambiguity, but also
the resolution engine to address the resulting sequential policy op timization framework. This milestone
is important because addresses a challenge for intelligent machines op erating in open-worlds. In doing
so, DR-FREE elucidates the mechanistic role of ambiguity on optimal dec isions and its policy supports
(Bayesian) belief-based updates. DR-FREE establishes what are th e limits of performance in the face of
ambiguity, showing that, at a very fundamental level, it is impossib le for an agent aﬀected by ambiguity to
outperform an ambiguity-free free energy minimizing agent. These anal ytic results are conﬁrmed by our
experiments.
In the navigation experiments, we compared the behaviors of an ambiguit y-unaware free energy min-
imizing agent 46 with the behavior of an agent equipped with DR-FREE. All the experim ents show that
DR-FREE is essential for the robot to successfully complete the tas k amid ambiguity, and this is conﬁrmed
when we consider additional benchmarks and diﬀerent environments. DR-FREE enables to reconstruct
the cost functions that underwrote superior performance over relate d methods. Our experimental setting
is exemplary not only for intelligent machines, underscoring the se vere consequences of ambiguity, but also
for natural intelligence. For example, through evolutionary adaptation, bac teria can navigate unknown
environments and this crucial ability for survival is achieved with little or no training. DR-FREE suggests
that this may be possible if bacteria follow a decision-making strat egy that, while simple, foresees a robust-
ness promoting step. Run-and-tumble motions 50, 51 might be an astute way to achieve this: interpreted
through DR-FREE, tumbles might be driven by free energy maximizati on, needed to quantify across the
environment a cost of ambiguity, and runs would be sampled from a free-e nergy minimizing policy that
considers this cost.
DR-FREE oﬀers a model for robust decision making via free energy min imization, with robustness guar-
12
Figure 4: DR-FREE and MaxDiﬀ. a. MaxDiﬀ success rates for diﬀerent values of the sampling size
and planning horizon. Experiments highlight a sweetspot in the hype rparameters with 100% success rate.
Worst rates are obtained for low horizons, where the success rate is bet ween 25% and approximately 40%.
All experiments are performed with the temperature-like hyperp arameter α set to 0 .1. Data for each cells
obtained from 12 experiments corresponding to the initial conditions in Fig. 3c. b. Success rates for
diﬀerent values of α and samples when horizon is set to 2. Success rates are consistent with the previous
panel – for the best combination of parameters, MaxDiﬀ agent completes the task half of the times. See
Supplementary Fig. 7 for a complementary set of MaxDiﬀ experiments. c. Robot trajectories using the
MaxDiﬀ policy when the horizon is equal to 2 and samples is set to 50. MaxDi ﬀ fulﬁlls the task when the
shortest path is obstacle-free. d. DR-FREE allows the robot to complete the task when it is equipped
with a generative model from MaxDiﬀ computed using the same set of hyp erparameters from the previous
panel. e. This desirable behavior is conﬁrmed even when samples is decrease d to 10. See Methods and
Supplementary Information for details.
antees deﬁned in the problem formulation – it also opens a number of int erdisciplinary research questions.
First, our results suggest that a promising research direction origin ating from this work is to integrate DR-
13
Figure 5: Ant experiments. a. Screenshot from the MuJoCo environment (Ant v-3). The state space is
29-dimensional and the action space is 8-dimensional. b. Performance comparison. Charts show means and
bars standard deviations from the means across 30 experiments. In some ep isodes, the ambiguity unaware,
MaxDiﬀ and NN-MPPI agents terminate prematurely due to the Ant becoming unhealthy; rewards were
set to zero from that point to the end of the episode. The Ant becomes u nhealthy in 6% of the episodes
for the ambiguity unaware agent, 20% for MaxDiﬀ and 23% for NN-MPPI. In contrast, t he Ant remains
healthy in all DR-FREE experiments. As in previous experiments, DR-FREE is used to compute reactive
actions. The ambiguity-unaware policy 46 corresponds to DR-FREE with the ambiguity radius set to zero.
FREE with perception and learning, coupling training with policy com putation. This framework would
embed distributional constraints in the formulation of the policy comp utation problem, as in DR-FREE,
while retaining perception and learning mechanisms inspired by, for example, MaxDiﬀ and/or evidence
free energy minimization. The framework would motivate analytical stud ies to quantify the beneﬁts of
integrated learning over an oﬄine pipeline. Along these lines, analyt ical studies should be developed to
extend our framework so that it can explicitly account for ambiguities i n the agent cost/reward. Second,
DR-FREE takes as input the ambiguity radius and this motivates the der ivation of a radius estimation
mechanism within our model. Through our analytic results we know that reducing ambiguity improves
performance; hence, integrating in our framework a method to learn amb iguity would be a promising step
towards agents that are not only robust, but also antifragile. 52 Finally, our experiments prompt a broader
question: what makes for a good generative model/planning horizon in the presence of ambiguity? The
answer remains elusive – DR-FREE guarantees robustness against ambigui ty and experiments suggest
that it compensates for poor planning/models; however, with e.g., more task-oriented model/planning,
ambiguity-unaware agents could succeed. This yields a follow-up qu estion. In challenging environments,
is a specialized model better than a multi-purpose one for survival ?
If, quoting the popular aphorism, all models are wrong, but some are usefu l, then relaxing the require-
ments on training, DR-FREE makes more models useful. This is achiev ed by departing from views that
14
emphasize the role, and the importance, of training: in DR-FREE the em phasis is instead on rigorously
installing robustness into decision-making mechanisms. With it s robust free energy minimization principle
and resolution engine, DR-FREE suggests that, following this path, in telligent machines can recover ro-
bust policies from largely imperfect, or even poor, models. We hope t hat this work may inspire both the
deployment of our free energy model in multi-agent settings (with h eterogeneous agents such as drones, au-
tonomous vessels and humans) across a broad range of application domains and, comb ining DR-FREE with
Deep RL, lead to learning schemes that – learning ambiguity – succeed when classic methods fail. At a
perhaps deeper level – as ambiguity is a key theme (see also Sec. S7 i n Supplementary Information) across,
e.g., psychology, economics and neuroscience 53–55 – we hope that this work may provide the foundation for
a biologically plausible neural explanation of how natural agents – with lit tle or no training – can operate
robustly in challenging environments.
Methods
The agent has access to: (i) the generative model, q0∶N ; (ii) the loss, speciﬁed via state/action costs
c(x)
k ∶ X → R, c(u)
k ∶ U → R, with X and U being the state and action spaces (see Sec. S1 and Sec. S3 of the
Supplementary Information for notation and details); (iii) the trained model ¯pk (xk ∣ xk−1, uk).
Distributionally Robust Free Energy Principle. Model ambiguities are speciﬁed via the ambiguity set around
¯pk (xk ∣ xk−1, uk), i.e., Bη (¯pk (xk ∣ xk−1, uk)). This is the set of all models with statistical complexity of at most
ηk (xk−1, uk) from ¯pk (xk ∣ xk−1, uk). Formally, Bη (¯pk (xk ∣ xk−1, uk)) is deﬁned as the set
{pk (xk ∣ xk−1, uk) ∈ D ∶ DKL (pk (xk ∣ xk−1, uk) ∣∣ ¯pk (xk ∣ xk−1, uk)) ≤ ηk (xk−1, uk), supp pk (xk ∣ xk−1, uk) ⊆ supp qk (xk ∣ xk−1, uk)}.
The symbol D stands for the space of densities and supp for the support. The ambiguity set captures all
the models that have statistical complexity of at most ηk (xk−1, uk) from the trained model and that have a
support included in the generative model. This second property ex plicitly built in the ambiguity set makes
the optimization meaningful as violation of the property would make the opti mal free energy inﬁnite. The
radius ηk (xk−1, uk) is positive and bounded. As summarized in Fig.
2a, the principle in the main text yields
the following sequential policy optimization framework:
{π⋆
k (uk ∣ xk−1)}1∶N ∈ argmin
{πk(uk∣xk−1)}1∶N
max
{pk(xk∣xk−1,uk)}1∶N
Complexity
/ucurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/ucurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/ucurlyright
DKL (p0∶N ∣∣ q0∶N )+
Expected Loss
/ucurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/ucurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/ucurlyright
Ep0∶N
⎡
⎢
⎢
⎢
⎣
N
∑
k=1
c(x)
k (Xk)+c(u)
k (Uk)
⎤
⎥
⎥
⎥
⎦
s. t. pk (xk ∣ xk−1, uk) ∈ Bη (¯pk (xk ∣ xk−1, uk))
/dcurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
Ambiguity
, ∀k = 1, . . . , N.
This is an extension of the free energy principle
28 accounting for policy robustness against model ambigu-
ities. We are not aware of any other free energy account that considers thi s setting and the corresponding
inﬁnite-dimensional optimization framework cannot be solved with exc ellent methods. When the ambi-
guity constraint is removed and the loss is the negative log-likelihoo d, our formulation reduces to the
expected free energy minimization in active inference. In this s pecial case, the expected complexity (i.e.,
ambiguity cost) becomes risk; namely, the KL divergence between in ferred and preferred (i.e., trained)
outcomes. The expected free energy can be expressed as risk plus amb iguity; however, the ambiguity
in the expected free energy pertains to the ambiguity of likelihoo d mappings in the generative model
15
(i.e., conditional entropy), not ambiguity about the generative model c onsidered in our free energy model.
In both robust and conventional active inference, the complexity ter m establishes a close relationship be-
tween optimal control and Jaynes’ maximum caliber (a.k.a., path entropy ) or minimum entropy production
principle.57, 58 It is useful to note that, oﬀering a generalization to free energy mini mization in active in-
ference, our robust formulation yields as special cases other popular com putational models such as KL
control,59control as inference, 43 and the Linear Quadratic Gaussian Regulator. Additionally, when the
loss is the negative log-likelihood, the negative of the variational free energy in the cost functional is the
evidence lower bound 60 a key concept in machine learning and inverse reinforcement learni ng.48 With its
resolution engine, DR-FREE shows that in this very broad set-up the optimal policy can still be computed.
Drawing the connection between MaxDiﬀ and DR-FREE. We start with showing that the robust free energy
principle formulation in Fig. 2a has the same optimal solution as (1). We have the following identity:
DKL (p0∶N ∣∣ q0∶N )+Ep0∶N
⎡
⎢
⎢
⎢
⎣
N
∑
k=1
c(x)
k (Xk)+c(u)
k (Uk)
⎤
⎥
⎥
⎥
⎦
= DKL (p0∶N ∣∣ ˜q0∶N )−ln Eq0∶N
⎡
⎢
⎢
⎢
⎢
⎣
exp ⎛
⎝−
N
∑
k=1
(c(x)
k (Xk)+c(u)
k (Uk))⎞
⎠
⎤
⎥
⎥
⎥
⎥
⎦
The left hand-side is the objective of Fig.
2a. In the right-hand side, ˜q0∶N is given in the main text and
Eq0∶N [exp (−∑N
k=1 (c(x)
k (Xk)+c(u)
k (Uk)))] is the normalizing constant denoted by Z in the main text. The last
term in the right-hand side does not depend on the decision variables and this yields that the min-max
problem in Fig.
2a has the same optimal solution as (1). Next, we show why – with the choice of q0∶N
described in the main text – DR-FREE yields the MaxDiﬀ objective when there is no ambiguity. In this
case, the ambiguity constraint is relaxed and DR-FREE min-max problem becomes
min
{πk(uk∣xk−1)}1∶N
DKL
⎛
⎝p0∶N ∣∣ 1
Z q0∶N exp ⎛
⎝−
N
∑
k=1
(c(x)
k (xk)+c(u)
k (uk))⎞
⎠
⎞
⎠ (3)
This is the problem given in (
2) but with ˜q0∶N explicitly included in the objective functional. To establish t he
connection between MaxDiﬀ and DR-FREE we recall that the MaxDiﬀ objec tive consists in minimizing in
the policy space the KL divergence between p0∶N and pmax(x0∶N , u1∶N ), which can be conveniently written as
1
Z ∏N
k=1 (pmax(xk ∣ xk−1))exp (∑N
k=1 r(xk, uk)). Now, this minimization problem has the same optimal solution of
min
{πk(uk∣xk−1)}1∶N
DKL
⎛
⎝p0∶N ∣∣ 1
Z
N
∏
k=1
(pmax(xk ∣ xk−1)¯p(uk ∣ xk−1))exp ⎛
⎝
N
∑
k=1
r(xk, uk)⎞
⎠
⎞
⎠ (4)
when ¯p(uk ∣ xk−1) is uniform. The equivalence can be shown by noticing that this refor mulation has been
obtained by adding and subtracting the constant quantity ln ∏N
k=1 ¯p(uk ∣ xk−1) to the MaxDiﬀ cost functional.
The similarity between (
4) and (3) is striking. In particular, the two problems are the same when: (i) x0 is
given, (ii) in q0∶N – which we recall is deﬁned as p(x0)∏N
k=1 qk (xk ∣ xk−1, uk)qk (uk ∣ xk−1) – we have qk (xk ∣ xk−1, uk)
set to pmax(xk ∣ xk−1) and qk (uk ∣ xk−1) uniform; (iii) r(xk, uk) = −c(x)
k (xk)−c(u)
k (uk). This establishes the connection
between DR-FREE and MaxDiﬀ objective from the Results.
Resolution engine. Both the variational free energy and the ambiguity constraint are nonlinear in the inﬁnite-
dimensional decision variables and this poses a number of challenges th at are addressed with our resolution
engine. The resolution engine allows to tackle the sequential policy optimization framework arising from
our robust free energy principle. We detail here the resolution engi ne and refer to Supplementary Infor-
mation for the formal treatment. Our starting point is the robust free e nergy principle formulated via
the above sequential policy optimization framework. This can be solved via a backward recursion where,
16
starting from k = N, at each k the following optimization problem needs to be solved:
min
π(u)
k∣k−1
DKL (π(u)
k∣k−1 ∣∣ q(u)
k∣k−1)+Eπ(u)
k∣k−1
[c(u)
k (Uk)]+ max
p(x)
k∣k−1
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣ q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]]
s. t. p (x)
k∣k−1 ∈ Bη (¯pk (xk ∣ xk−1, uk)).
In the above expression, for compactness we used the shorthand notation s π(u)
k∣k−1, q(u)
k∣k−1, p(x)
k∣k−1 and q(x)
k∣k−1 for
πk (uk ∣ xk−1), qk (uk ∣ xk−1), pk (xk ∣ xk−1, uk) and qk (xk ∣ xk−1, uk), respectively. The term ¯ck (xk) is the cost-to-go.
This is given by ¯ck (xk) = c(x)
k (xk)+ ˆck+1 (xk), where ˆck+1 (xk) is the smallest free energy that can be achieved
by the agent at k +1. That is, ˆck+1 (xk) is the optimal solution of the above optimization problem evaluated
at k + 1. When k = N, ˆcN+1 (xN ) is initialized at 0. This means that, for reactive actions, e.g., reﬂexes,
¯ck (xk) = c(x)
k (xk). The above reformulation is convenient because it reveals that, at each k, π⋆
k (uk ∣ xk−1) can
be computed via a bi-level optimization approach, consisting in ﬁrst maximizing over pk (xk ∣ xk−1, uk), ob-
taining the maximum expected variational free energy across all possibl e environments in the ambiguity
set, to ﬁnally minimize over the policies. Crucially, this means that to make optimal decisions, the agent
does not need to know the environment that maximizes the free ener gy but rather it only needs to know
what the actual maximum free energy is. In turn, this can be found by ﬁr st tackling the problem in green
in Fig. 2b, i.e., ﬁnding the cost of ambiguity, and then taking the expectation Eπk(uk∣xk−1) [⋅]. While the
problem in green in Fig. 2b is inﬁnite-dimensional, DR-FREE ﬁnds the cost of uncertainty by solving
a convex and scalar optimization problem. This is possible because the op timal value of the problem in
green in Fig. 2b equals ηk (xk−1, uk)+minα≥0 ˜Vα (xk−1, uk). In this expression, α is a scalar decision variable and
˜Vα (xk−1, uk), detailed in the Supplementary Information, is a scalar function of α, convex for all α ≥ 0. The
global non-negative minimum of ˜Vα (xk−1, uk) is ˜c (xk−1, uk). In summary, the free energy maximization step
can be conveniently solved with oﬀ-the-shelf software tools. In DR- FREE, the cost of ambiguity promotes
robustness and contributes to the expected loss for the subsequen t minimization problem in Fig. 2b. The
optimal solution of this class of problems has an explicit expression ( π⋆
k (uk ∣ xk−1) in Fig. 2b) and the optimal
value is ˆck (xk−1) used at the next step in the recursion. Derivations in Supplement ary Information.
Why is it always better to be ambiguity-aware. As ambiguity vanishes, ˜c (xk−1, uk) becomes DKL (¯p(x)
k∣k−1 ∣∣ q(x)
k∣k−1)+
E¯p(x)
k∣k−1
[¯ck (Xk)]. Thus, the DR-FREE policy becomes
π⋆
k (uk ∣ xk−1) =
q(u)
k∣k−1 exp (−DKL (¯pk (xk ∣ xk−1, uk) ∣∣ qk (xk ∣ xk−1, uk))−Epk(xk∣xk−1,uk) [¯ck (Xk)]−c(u)
k (uk))
∫ qk (uk ∣ xk−1)exp (−DKL (¯pk (xk ∣ xk−1, uk) ∣∣ qk (xk ∣ xk−1, uk))−Epk(xk∣xk−1,uk) [¯ck (Xk)]−c(u)
k (uk))duk
.
This is the optimal policy of an ambiguity-free agent 46 (with pk (xk ∣ xk−1, uk) = ¯pk (xk ∣ xk−1, uk)). Given the
current state xk−1, the optimal cost is
−ln ∫U
qk (uk ∣ xk−1)exp (−DKL (¯pk (xk ∣ xk−1, uk) ∣∣ qk (xk ∣ xk−1, uk))−E¯pk(xk∣xk−1,uk)[¯ck (xk)]−c(u)
k (uk))duk.
This is smaller than the cost achieved by the agent aﬀected by ambiguity . In fact, when there is ambiguity,
the DR-FREE policy achieves the optimal cost −ln ∫ qk (uk ∣ xk−1)exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk))duk and
DKL (¯p(x)
k∣k−1 ∣∣ q(x)
k∣k−1)+E¯p(x)
k∣k−1
[c(x)
k (Xk)] < ηk (xk−1, uk)+˜c (xk−1, uk). See Sec. S4 in the Supplementary Information for
the formal details.
Why DR-FREE Supports Bayesian belief updating. The approach adopts a widely used parametrization
46, 48, 61, 62
17
of the cost in terms of F state-action features, φi(xk−1, uk), and G action features, γi(uk). No assumptions
are made on the features, which can be e.g., nonlinear. With this paramet rization, given M observed
actions/state pairs, the likelihood function – inspired by the lit erature46, 62 – is
M
∏
k=1
qk (uk = ˆuk ∣ xk−1 = ˆxk−1)exp (∑F
i=1 wiφi (ˆxk−1, ˆuk)+∑G
i=1 viγi (ˆuk))
∫ qk (uk ∣ xk−1 = ˆxk−1)exp (∑F
i=1 wiφi (ˆxk−1, uk)+∑G
i=1 viγi (uk))duk
,
where w and v are the stacks of the weights wi and vi, respectively. The negative log-likelihood
46, 62 is then
given by
−L(w, v) = −
M
∑
k=1
⎛
⎝ln qk (uk = ˆuk ∣ xk−1 = ˆxk−1)+
F
∑
i=1
wiφi (ˆxk−1, ˆuk)+
G
∑
i=1
viγi (ˆuk)
−ln ∫ qk (uk ∣ xk−1 = ˆxk−1)exp ⎛
⎝
F
∑
i=1
wiφi (ˆxk−1, uk)+
G
∑
i=1
viγi (uk)⎞
⎠duk
⎞
⎠.
The cost reconstruction in the main paper is then obtained by ﬁnding t he weights that are optimal for
the problem minw,v −L(w, v), after dropping the ﬁrst term from the cost because it does not depen d on the
weights. Convexity of the problem follows because
46, 62 the cost functional is a conical combination of con-
vex functions. See Supplementary Information.
Experiments settings. DR-FREE was turned into Algorithm 1 shown in the Supplementary Inf ormation and
engineered to be deployed on the agents (see Code Availability). In t he robot experiments pk (xk ∣ xk−1, uk) is
N (xk−1 +ukdt, Σ) with Σ =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.001 0 .0002
0.0002 0 .001
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, and where xk = [px,k, py,k ]T is the position of the robot at time-step k,
uk = [vx,k, vy,k ]T is the input velocity vector, and dt = 0.033s is the Robotarium time-step. In these experiments,
the state space is [−1.5, 1.5] × [−1, 1] m, matching the work area, and the action space is [−0.5, 0.5] × [−0.5, 0.5]
m/s. In accordance with the maximum allowed speed in the platform, the inputs to the robot were
automatically clipped by the Robotarium when the speed was higher than 0.2 m/s. DR-FREE does not
have access to pk (xk ∣ xk−1, uk). Its trained model, ¯pk (xk ∣ xk−1, uk), is learned via Gaussian Processes (GPs)
with covariance function being an exponential kernel. The trained m odel was learned in stages and the
model learned in a given stage would also use the data from the previous s tages. The data for the
training had a bias: the input uk was sampled from a uniform policy and the next observed robot position
was corrupted by adding a quantity proportional to the current position (0.1xk−1). See Supplementary
Information for the details and the data used for training. This means that the models learned at each
stage of the training data were necessarily wrong: the parameters of the t rained model at each stage
of the training are in Supplementary Fig. 1a. For the generative model, qk (xk ∣ xk−1, uk) = N (xd, Σx), with
Σx = 0.0001I2, and qk (uk ∣ xk−1) being the uniform distribution. Also, the ambiguity radius, ηk (xk−1, uk) =
DKL (qk (xk ∣ xk−1, uk) ∣∣ ¯pk (xk ∣ xk−1, uk)) and clipped at 100, is higher the farther the robot is from the goal
position (we recall that the agent has access to both qk (xk ∣ xk−1, uk) and ¯pk (xk ∣ xk−1, uk)). This captures
higher agent conﬁdence as it gets closer to its goal destination (see Suppl ementary Fig. 2 , also reporting
the number of steps recorded across the experiments). The state cos t in Fig.
3b, adapted from the
literature,46 is c(xk) = 50(xk − xd)2 + 20 ∑n
i=1 gi(xk) + 5b(xk), where: (i) xd is the goal destination and thus this
term promotes goal-reaching; (ii) n = 6, gi are Gaussians, N (oi, Σo), with Σo = 0.025I2 (I2 is the identity
matrix of dimension 2). The oi’s are in Supplementary Fig. 1c) and capture the presence of the obstacl es.
Hence, the second term penalizes proximity to obstacles; (iii) b(xk) is the boundary penalty term given
by b(xk) ∶= ∑2
j=1 (exp (−0.5 (
px,k−bxj
σ )
2
)+exp (−0.5 (
py,k −byj
σ )
2
))/σ
√
2π, with bxj , byj representing the jth component
of the boundary coordinates ( bx = [−1.5, 1.5], by = [−1, 1]) and σ = 0.02. In summary, the cost embeds obstacle
18
avoidance in the formulation. The optimal policy of the ambiguity-unawar e free energy minimizing agent
is available in the literature. 46 This is again exponential but, contrary to DR-FREE policy in Fig. 3b, the
exponent is now −DKL (¯pk (xk ∣ xk−1, uk) ∣∣ qk (xk ∣ xk−1, uk))−E¯pk(xk∣xk−1,uk) [c(x)
k (xk)]. Across experiments, the ﬁrst
term dominated the second, which accounted for obstacles (see Supple mentary Information for details).
As a result, the policy consistently directs the robot along the short est path to the goal, disregarding the
presence of the obstacles. This explains the behavior observed in F ig. 3c. The computation time reported
in the main text were obtained from DR-FREE deployment on the Robotari um hardware – measurements
obtained using the time() function in Python and averaging across the e xperiment.
The benchmark for our belief updating is with respect to the Inﬁni te Horizon Maximum Causal En-
tropy Inverse RL algorithm with Monte-Carlo policy evaluation and soft-v alue iteration (not required in
DR-FREE belief update) from the literature. 49 In order to use this algorithm, we discretized the state
space in a 50 ×50 grid (this step is not required within our results) and we needed t o redeﬁne the action
space as [−1.0, 1.0]×[−1.0, 1.0]. This was then discretized into a 5 ×5 grid. The corresponding reconstructed
cost in Supplementary Fig. 1e was obtained with the same dataset and featur es used to obtain Fig. 3f.
After multiple trials and trying diﬀerent settings, we were not abl e to obtain a reconstructed cost that
was better than the one in Supplementary Fig. 1e. The settings used to obtain Supplementary Fig. 1e
are: (i) initial learning rate of 1, with an exponential decay function to update the learning rate after
each iteration; (ii) discount factor for soft-value iteration, 0.9; (iii) initial feature weights randomly selected
from a uniform distribution with support [−100, 100]; (iv) gradient descent stopping threshold, 0.01. The
code to replicate all the results is provided (see Code Availabili ty). For the comparison with MaxDiﬀ 22
we used the code available on the paper repository. In the experimen ts reported in Fig. 4, consistently
with all the other experiments, MaxDiﬀ had access to ¯pk (xk ∣ xk−1, uk) and cost/reward from Fig. 3b. In the
experiments of the Supplementary Fig. 7b, MaxDiﬀ learns both the rew ard and the model. In this case,
the data used by MaxDiﬀ is corrupted by the same bias as the data used by DR-FREE ( 100000 data points
are used for learning, consistently with the original MaxDiﬀ repository ). Across all MaxDiﬀ experiments,
again consistently with the original code, the additional MaxDiﬀ hyperp arameters are γ = 0.95, λ = 0.5 and
learning rate 0.0005 (this last parameter is used only in the experiments where MaxDiﬀ d oes not have access
to the reward/model). No changes to the original MaxDiﬀ code were made to obtain the results. The
available MaxDiﬀ implementation natively allows users to conﬁgure if the framework uses reward/model
supplied by the environment. These details, together with a table summarizing all the parameter settings
used in the experiments, are given in the Supplementary Informati on (see at the end of Sec. S5 and Tab.
S-1). For the experiments of Fig. 5, the Ant becomes unhealthy when either the value of any of the states
is no longer ﬁnite or the height of the torso is too low/high (we refer to the environment documentation
at https://gymnasium.farama.org/environments/mujoco/ant/ for the standard deﬁnition of the unhealthy condition).
The cost is the negative reward provided by the environment and the t rained model ¯pk (xk ∣ xk−1, uk) is
learned from 10000 data points collected through random policy rollouts, using the same neu ral network
as in the Ant experiments in the MaxDiﬀ paper. 22 The same network – providing as output mean and
variance – is used across all the experiments. For the DR-FREE generat ive model, qk (uk ∣ xk−1) is uniform
and qk (xk ∣ xk−1, uk) is set as an anisotropic Gaussian having the same mean provided by the ne ural network
but with a diﬀerent variance. The ambiguity radius captures higher age nt conﬁdence as this is moving in
the right direction. See Supplementary Information for details. MaxD iﬀ and NN-MPPI implementations
are from the MaxDiﬀ repository.
Schematic ﬁgure generation. The schematic ﬁgures were generated in Apple Keynote (v. 13.2). Panels w ere
19
assembled using the same software.
Data Availability
All (other) data needed to evaluate the conclusions in the paper and to r eplicate the experiments are
available in the paper, Supplementary Information and accompanying cod e (see the Assets folder in Code
Availability). A recording from the robot experiments, together with the ﬁgures of this paper, is available
at the folder Assets of our repository.
63
Code Availability
Pseudocode for DR-FREE is provided in the Supplementary Inform ation. The full code for DR-FREE to
replicate all the experiments is provided at our repository.
63 The folder Experiments contains our DR-
FREE implementation for the Robotarium experiments. The folder also contains: (i) the code for the
ambiguity-unaware free energy minimizing agent; (ii) the data show n in Fig. SI-8, together with the GP
models and the code to train the models; (iii) the code to replic ate the results in Fig. 3f and Fig. 3e. The
folder also contains the code for the experiments in Supplementary Fig. 4 and provides the instructions to
replicate the experiments of Supplementary Fig. 5 and Supplement ary Fig. 6. The folder Belief Update
Benchmark contains the code to replicate our benchmarks for the beli ef updating results. The folder Assets
contains all the ﬁgures of this paper, the data from the experiments us ed to generate these ﬁgures, and the
movie from which the screen-shots of Fig. 3d were taken. The folder MaxDiﬀ Benchmark contains the code
to replicate our MaxDiﬀ benchmark experiments. We build upon the or iginal code-base from the MaxDiﬀ
paper,22 integrating it in the Robotarium Python environment. The sub-folder Ant Benchmark contains
the code for the Ant experiments. MaxDiﬀ and NN-MPPI implementations are from the literature. 22
As highlighted in the Discussion, extending our analytical results t o consider ambiguity inherently in
the reward is an open theoretical research direction, interesting p er se. Nevertheless, we now discuss how
DR-FREE can be adapted to this setting. To achieve this, quoting form the literature, 32 one could deﬁne
a modiﬁed problem formulation where the reward is appended to the obs ervations available to the agent.
In this setting, the reward becomes – quoting the literature 32 – the last coordinate of the observation, so
that it can be embedded into model ambiguity.
References
[1] Peter R. Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, K aushik Subramanian,
Thomas J. Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, F lorian Fuchs, Leilani Gilpin,
Piyush Khandelwal, Varun Kompella, HaoChih Lin, Patrick MacAlpine, Decl an Oller, Takuma Seno,
Craig Sherstan, Michael D. Thomure, Houmehr Aghabozorgi, Leon Barrett, Rory D ouglas, Dion White-
head, Peter D¨ urr, Peter Stone, Michael Spranger, and Hiroaki Kitano. Ou tracing champion Gran
Turismo drivers with deep reinforcement learning. Nature, 602(7896):223–228, February 2022.
[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu , Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostro vski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Sh ane Legg, and
20
Demis Hassabis. Human-level control through deep reinforcement learni ng. Nature, 518(7540):529–533,
February 2015.
[3] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, B rendan Tracey, Francesco Carpanese,
Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de las Casas, Craig Donne r, Leslie Fritz,
Cristian Galperti, Andrea Huber, James Keeling, Maria Tsimpoukelli , Jackie Kay, Antoine Merle,
Jean-Marc Moret, Seb Noury, Federico Pesamosca, David Pfau, Olivier S auter, Cristian Sommariva,
Stefano Coda, Basil Duval, Ambrogio Fasoli, Pushmeet Kohli, Koray Kavuk cuoglu, Demis Hassabis,
and Martin Riedmiller. Magnetic control of Tokamak plasmas through deep rei nforcement learning.
Nature, 602(7897):414–419, February 2022.
[4] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M¨ ull er, Vladlen Koltun, and Davide
Scaramuzza. Champion-level drone racing using deep reinforcement l earning. Nature, 620(7976):982–987,
August 2023.
[5] Katherine M. Collins, Ilia Sucholutsky, Umang Bhatt, Kartik Chand ra, Lionel Wong, Mina Lee, Cede-
gao E. Zhang, Tan Zhi-Xuan, Mark Ho, Vikash Mansinghka, Adrian Weller, Joshua B. T enenbaum,
and Thomas L. Griﬃths. Building machines that learn and think with peop le. Nature Human Behaviour ,
8(10):1851–1863, October 2024.
[6] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. G ershman. Building
machines that learn and think like people. Behavioral and Brain Sciences , 40:e253, November 2016.
[7] Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. K oonin. Toward a theory of
evolution as multilevel learning. Proceedings of the National Academy of Sciences , 119(6), February 2022.
[8] H´ ector Mar´ ın Manrique, Karl John Friston, and Michael John Walker. ‘snakes and ladders’ in paleoan-
thropology: From cognitive surprise to skillfulness a million years ago. Physics of Life Reviews , 49:40–70,
July 2024.
[9] Mayank Kejriwal, Eric Kildebeck, Robert Steininger, and Abhina v Shrivastava. Challenges, evaluation
and opportunities for open-world learning. Nature Machine Intelligence , 6(6):580–588, June 2024.
[10] Robert D. McAllister and Peyman M. Esfahani. Distributionally robu st model predictive control:
Closed-loop guarantees and scalable algorithms. IEEE Transactions on Automatic Control , 70(5):2963–2978,
May 2025.
[11] Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever , and Jan Peters. Robust
reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge
Extraction, 4(1):276–315, March 2022.
[12] Bahar Taskesen, Dan Iancu, C ¸ a˘ gı l Ko¸ cyi˘ git, and Daniel Kuhn. Distributionally robust linear quadratic
control. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in
Neural Information Processing Systems , 36, 18613–18632. Curran Associates, Inc., 2023.
[13] Luc Rocher, Arnaud J. Tournier, and Yves-Alexandre de Montjoye. Adve rsarial competition and
collusion in algorithmic markets. Nature Machine Intelligence , 5(5):497–504, May 2023.
21
[14] Maxwell T. West, Shu-Lok Tsang, Jia S. Low, Charles D. Hill, Christop her Leckie, Lloyd C. L. Hol-
lenberg, Sarah M. Erfani, and Muhammad Usman. Towards quantum enhanced adv ersarial robustness
in machine learning. Nature Machine Intelligence , 5(6):581–589, May 2023.
[15] Geoﬀrey E. Hinton and Richard S. Zemel. Autoencoders, minimum de scription length and Helmholtz
free energy. In J. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing
Systems, volume 6. Morgan-Kaufmann, 1993.
[16] Geoﬀrey E. Hinton, Peter Dayan, Radford M. Neal, and Richard S.Zemel. The Helmholtz machine.
Neural Computation , 7:889–904, September 1995.
[17] Sharu T . Jose and Osvaldo Simeone. Free energy minimization: A uniﬁ ed framework for modeling,
inference, learning, and optimization [lecture notes]. IEEE Signal Processing Magazine , 38(2):120–125, March
2021.
[18] Mohamed Hibat-Allah, Estelle M. Inack, Roeland Wiersema, Roger G. Mel ko, and Juan Carrasquilla.
Variational neural annealing. Nature Machine Intelligence , 3(11):952–961, October 2021.
[19] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle in Mind, Brain,
and Behavior . The MIT Press, March 2022.
[20] Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. On the statistical consi stency of risk-
sensitive Bayesian decision-making. In Proceedings of the 37th International Conference on Neural I nformation Processing
Systems, NIPS ’23, Red Hook, NY, USA, 2024. Curran Associates Inc.
[21] Terence D. Sanger. Risk-aware control. Neural Computation , 26(12):2669–2691, December 2014.
[22] Thomas A. Berrueta, Allison Pinosky, and Todd D. Murphey. Maximum di ﬀusion reinforcement
learning. Nature Machine Intelligence , 6(5):504–514, May 2024.
[23] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive activ e inference. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
Processing Systems, volume 34, pages 13870–13882. Curran Associates, Inc., 2021.
[24] Karl Friston. The free-energy principle: a rough guide to the brai n? Trends in Cognitive Sciences ,
13(7):293–301, July 2009.
[25] Conor Heins, Beren Millidge, Lancelot Da Costa, Richard P. Mann, Karl J. Friston, and Iain D. Couzin.
Collective behavior from surprise minimization. Proceedings of the National Academy of Sciences , 121(17), April
2024.
[26] Tony J. Prescott and Stuart P. Wilson. Understanding brain function al architecture through robotics.
Science Robotics, 8(78), May 2023.
[27] Jakob Hohwy. The Predictive Mind . Oxford University Press, November 2013.
[28] Karl Friston, Lancelot D. Costa, Noor Sajid, Conor Heins, Kai Ueltzh¨ oﬀer, Grigorios A. Pavliotis,
and Thomas Parr. The free energy principle made simpler but not too si mple. Physics Reports, 1024:1–29,
June 2023.
22
[29] Sebastian Gottwald and Daniel A. Braun. The two kinds of free energy and the Bayesian revolution.
PLOS Computational Biology , 16(12):e1008420, December 2020.
[30] Abraham Imohiosen, Joe Watson, and Jan Peters. Active Inference or Control as Inference? A Unifying View ,
page 12–19. Springer International Publishing, September 2020.
[31] Thomas Berrueta. Robot Thermodynamics . Ph.d. dissertation, Nortwestern University, December 2024.
Available online at https://www.proquest.com/openview/faffd739b9b7a1becbd5e99b0fbd83fe/.
[32] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provabl y) solves some robust RL
problems. In International Conference on Learning Representations , 2022.
[33] Emanuel Todorov, Tom Erez and Yuval Tassa. MuJoCo: A physics engine for model-based control.
In Proceedings of 2012 IEEE/RSJ International Conference on I ntelligent Robots and Systems , pp. 5026–5033, 2012
[34] Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James Rehg, B yron Boots, Evangelos
Theodorou. Information theoretic MPC for model-based reinforcement learning. In Proceedings of 2017
IEEE International Conference on Robotics and Automation , pp. 1714–1721, 2017
[35] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Math ys, Thomas Fitzgerald, and Gio-
vanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, March 2015.
[36] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active i nference. Journal of The Royal
Society Interface , 14(136):20170376, November 2017.
[37] Yuki Konaka and Honda Naoki. Decoding reward–curiosity conﬂict in deci sion-making from irrational
behaviors. Nature Computational Science , 3(5):418–432, May 2023.
[38] Koosha Khalvati, Seongmin A. Park, Saghar Mirbagheri, Remi Philippe, M ariateresa Sestito, Jean-
Claude Dreher, and Rajesh P. N. Rao. Modeling other minds: Bayesian i nference explains human
choices in group decision-making. Science Advances, 5(11), November 2019.
[39] Antonella Maselli, Pablo Lanillos, and Giovanni Pezzulo. Active infe rence uniﬁes intentional and
conﬂict-resolution imperatives of motor control. PLOS Computational Biology , 18(6):e1010095, June 2022.
[40] Julian F .V . Vincent, Olga A . Bogatyreva, Nikolaj R . Bogatyrev, Adrian Bow yer, and Anja K.
Pahl. Biomimetics: its practice and theory. Journal of the Royal Society Interface , 3(9):471–482, April 2006.
[41] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active infer ence, homeostatic regulation and
adaptive behavioural control. Progress in neurobiology , 134:17–35, November 2015.
[42] Thomas Parr and Karl J. Friston. Generalised free energy and active in ference. Biological Cybernetics ,
113(5–6):495–513, September 2019.
[43] Bart V.D. Broek, Wim Wiegerinck, and Hilbert J. Kappen. Risk sens itive path integral control. In
Conference on Uncertainty in Artiﬁcial Intelligence , 2010.
[44] Hagai Attias. Planning by probabilistic inference. In Christopher M. Bishop and Brendan J. Frey,
editors, Proceedings of the Ninth International Workshop on Artiﬁci al Intelligence and Statistics , volume R4 of Proceedings
of Machine Learning Research , 9–16. Reissued by PMLR on 01 April 2021.
23
[45] Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences ,
16(10):485–488, October 2012.
[46] Emiland Garrabe, Hozefa Jesawada, Carmen Del Vecchio, and Giovanni Ru sso. On convex data-driven
inverse optimal control for nonlinear, non-stationary and stochastic sys tems. Automatica, 173:112015,
March 2025.
[47] Sean Wilson, Paul Glotfelter, Li Wang, Siddharth Mayya, Gennaro Notomis ta, Mark Mote, and Mag-
nus Egerstedt. The Robotarium: Globally Impactful Opportunities, C hallenges, and Lessons Learned
in Remote-Access, Distributed Control of Multirobot Systems. IEEE Control Systems Magazine , 40(1):26–44,
February 2020.
[48] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maxi mum Entropy Inverse
Reinforcement Learning. In Proceedings of the 23rd National Conference on Artiﬁcial In telligence - Volume 3 , AAAI’08,
page 1433–1438. AAAI Press, 2008.
[49] Zhengyuan Zhou, Michael Bloem, and Nicholas Bambos. Inﬁnite Time Horiz on Maximum Causal En-
tropy Inverse Reinforcement Learning. IEEE Transactions on Automatic Control , 63(9):2787–2802, September
2018.
[50] Zahra Alirezaeizanjani, Robert Grossmann, Veronika Pfeifer, Mariu s Hintsche, and Carsten Beta.
Chemotaxis strategies of bacteria with multiple run modes. Science Advances, 6(22), May 2020.
[51] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learn ing action-oriented models
through active inference. PLOS Computational Biology , 16(4):e1007805, April 2020.
[52] Nassim N. Taleb. Antifragile: Things that gain from disorder. Random House, No vember 2012.
[53] Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or surp rise? Frontiers in Psychology ,
4:907, December 2013.
[54] Ming Hsu, Meghana Bhatt, Ralph Adolphs, Daniel Tranel, and Colin F. Came rer. Neural systems
responding to degrees of uncertainty in human decision-making. Science, 310:1680–1683, December 2005.
[55] Paul J. Zak. Neuroeconomics. Philosophical Transactions of the Royal Society B: Biologi cal Sciences, 359:1737–1748,
November 2004.
[56] ´Emiland Garrab´ e and Giovanni Russo. Probabilistic design of optimal seq uential decision-making
algorithms in learning and control. Annual Reviews in Control , 54:81–102, November 2022.
[57] Edwin T . Jaynes. The minimum entropy production principle. Annual Review of Physical Chemistry ,
31(1):579–601, October 1980.
[58] Hermann Haken and Juval Portugali. Relationships. Bayes, Friston, Jaynes and Synergetics 2nd Foundation, 85 –
104. Springer International Publishing, February 2021.
[59] Emanuel Todorov. Eﬃcient computation of optimal actions. Proceedings of the National Academy of Sciences ,
106(28):11478–11483, April 2009.
[60] Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics . MIT Press, 2023.
24
[61] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
[62] Krishnamurthy Dvijotham and Emanuel Todorov. Inverse optimal cont rol with Linearly-Solvable
MDPs. In 27th International Conference on Machine Learning , 335–342, 2010.
[63] Hozefa Jesawada, DR-FREE repository , Zenodo, https://doi.org/10.5281/zenodo.17638771, 2025
Acknowledgments. AH and HJ did this work while at University of Salerno. HJ and GR supported by
the European Union-Next Generation EU Mission 4 Component 1 CUP E53D23014640001. KF sup -
ported by funding from the Wellcome Trust (Ref: 226793/Z/22/Z). AS supported b y MOST - Sustainable
Mobility National Research Center and received funding from the Europ ean Union Next-GenerationEU
(PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR)–MISSIONE 4 COMPONENTE 2, IN-
VESTIMENTO 1.4–D.D. 1033 17/06/2022) under Grant CN00000023. This document reﬂects only the
authors’ views and opinions. We acknowledge the use of ChatGPT for assist ance in improving the wording
and grammar of this document. GR and HJ wish to thank Prof. Del Vecchio (S annio University) who
allowed HJ to perform early preliminary experiments before him join ing University of Salerno. GR thanks
Prof. Francesco Bullo (University College Santa Barbara, USA), Prof. Mic hael Richardson (Macquarie
University, Australia) and Prof. Mirco Musolesi (University College Lon don, UK) for the insightful dis-
cussions and comments on an early version of this paper.
Author contributions. KF and GR conceptualized, designed and formulated the research. AS and G R con-
ceptualized, designed and formulated resolution engine concepts. AS d eveloped all the proofs with inputs
from GR. AS, GR and HJ revised the proofs. GR and HJ designed the experim ents. HJ, with inputs from
GR and AS, implemented DR-FREE, performed the experiments and obt ained the corresponding ﬁgures
and data. DR-FREE code was revised by AS. All authors contributed to the interpretation of the results.
GR wrote the manuscript with inputs from all the authors. All authors con tributed to and edited the
manuscript.
Competing interests. The authors declare no competing interests.
25
Supplementary Information: Distributionally Robust Free
Energy Principle for Decision-Making
Allahkaram Shaﬁei 1,∗ Hozefa Jesawada 2,∗ Karl Friston 3 Giovanni Russo 4 /Letter
Supplementary Information
We provide supplementary ﬁgures and the formal details for the statem ents and results in the main text.
After providing some background (Sec.
S1) we relate DR-FREE with other frameworks (Sec. S2) and
give the formal statements behind the resolution engine in Fig. 2b (Sec . S3). In Sec. S4 we show why
DR-FREE shows that it is always better to be ambiguity aware. After r eporting the supplementary details
of the experiments (Sec. S5) we provide the proofs of all the statements (Sec. S6). Finally, in Sec. S7, we
unpack some possible psychological and translational implications of DR-FR EE.
1 Czech Technical University, Prague, Czech Republic.2 New York University Abu Dhabi, Abu Dhabi Emirate.3 Wellcome
Centre for Human Neuroimaging, Institute of Neurology, University College London, United Kingdom. 4 Department of
Information and Electrical Engineering and Applied Mathematics, University of Salerno, Italy.
∗ These authors contributed equally./Lettere-mail: giovarusso@unisa.it
1
Supplementary Figures
Supplementary Fig. 1. a. The kernel function for the Gaussian Process learning (from Method s) of
the trained model together with the parameters learned at each stage of le arning. b. The features for
cost reconstruction used to obtain Figure 3f. c. Centers of the Gaussians promoting obstacles avoidance
in Fig. 3 experiments (the parameter oi in the agent cost given in Methods). d. Recordings from the
Robotarium simulator of experiments where the robots are equipped wit h DR-FREE policy but using the
reconstructed cost. The initial positions are diﬀerent from the ones used in the experiments in the main
paper. e. Cost reconstructed with related method from the literature. 1
Supplementary Fig. 2. In all the experiments from Fig. 3c, pk (xk ∣xk−1, uk) always belongs
to the ambiguity set Bη (¯pk (xk ∣xk−1, uk)). The plots conﬁrm this by showing that ηk (xk−1, uk) –
see Experiments settings in Methods for the radius used in the ex periments – is always bigger than
DKL (pk (xk ∣xk−1, uk) ∣∣ ¯pk (xk ∣xk−1, uk)) across all the experiments. Column a. is from the exper-
iments in Fig. 3c top-left. Column b. is from the experiments in Fig. 3c top-middle. Col-
umn c. is from the experiments in Fig. 3c top-right. In all plots, ηk (xk−1, uk) is in red and
DKL (pk (xk ∣xk−1, uk) ∣∣ ¯pk (xk ∣xk−1, uk)) in green. Time series stop when the episode ends ( x-axes show
the number of steps for each experiment).
Supplementary Fig. 3. Behavior of the ambiguity-unaware agent when equipped with planning. Panel
a. shows the behavior of the agent when N =3. Panels b–d. show the behavior of the agent when N =5,
N =10 and N =50, respectively. In all panels the initial positions for the robot are th e same as in Fig. 3c.
Supplementary Fig. 4. DR-FREE experiments for the robot navigation task in diﬀerent envir onments.
a. Experimental results for diﬀerent obstacles and goal conﬁgurations. Acros s all the experiments, DR-
FREE enables the robot to successfully complete the task, consiste ntly with our main results. b. The
corresponding costs (scale on the right). The cost function is the same to the one described in Methods,
with updated coordinates for obstacles and goal positions to match the diﬀe rent experimental set-ups.
Supplementary Fig. 5. Experiments results for diﬀerent ambiguity levels. a. With respect to the
settings used in Fig. 3c, here the ambiguity radii are: (i) decreased b y 50% (left); (ii) decreased by 90%
(middle); set to 0 (right). In this last case, pk (xk ∣xk−1, uk) never belongs to the ambiguity constraint.
Consequently, consistently with our analytical results, the agent c an fail in the presence of ambiguity. b.
Following the format of Supplementary Fig. 2, each column shows – for each of the experiments – when
pk (xk ∣xk−1, uk)belongs to the ambiguity set Bη (¯pk (xk ∣xk−1, uk)). Color coding is as in Expected Data 2.
This panel (middle column) reveals that DR-FREE can still complete the task even when pk (xk ∣xk−1, uk)
does not always lie within the ambiguity set. c. DR-FREE policy for diﬀerent ambiguity levels. The
heatmaps – computed for the same ambiguity levels as in the top panels – sh ow how the robot policy
changes when this is in position [−0.5, −0.5]. When the radius is set to 0, the ambiguity constraint is
relaxed and DR-FREE policy coincides with the optimal policy for this relaxed problem. The process is
representative for all robot trajectory points. See Sec. S5 in Supplementary Information.
Supplementary Fig. 6. Experiments results when there is no ambiguity. a. The trained model
¯pk (xk ∣xk−1, uk) coincides with pk (xk ∣xk−1, uk). Experiments use the same ambiguity radii as in Supple-
mentary Fig. 5. This time, since the scenario does not feature ambigui ty, DR-FREE enables the robot to
fulﬁll the task in all cases, even when the ambiguity radius is set to 0 (right). b. As in Supplementary Fig.
5 and consistently with our analysis, when the radius is set to 0, the am biguity constraint is relaxed and
DR-FREE policy again coincides with the optimal policy for this relaxed problem. As in Supplementary
Fig. 5, the heatmap corresponds to the robot policy when this is in positi on [−0.5, −0.5]. See also Fig.
SI-9.
Supplementary Fig. 7. Additional MaxDiﬀ experiments. Panel a. shows the experiments with MaxDiﬀ
having access to both reward and model. The top row shows the experi ments with hyperparameters taken
from the sweetspot in Fig. 4a where MaxDiﬀ achieves 100% success rate – hor izon set to 20 and samples
to 50. The agent is in fact able to always complete the task. However, near t he goal position, in some of
the experiments, we observe an erratic behavior. Upon experimenti ng with the environment, we believe
that this behavior might be due to planning under a model aﬀected by ambiguity. This phenomenon is
more apparent in the bottom row. Here, the horizon set to 50 and samples to 100. In panel b. MaxDiﬀ
does not have access to the reward/model and the policy is learned – th ese experiments are developed to
check consistency of success rates. The learning rate is set to 0 .0005 and 100000 data points are used to
learn the policy, consistently with the MaxDiﬀ code for the point mas s example. Data are corrupted by the
same bias used in DR-FREE experiments. The top row shows the beha vior of the agent using the learned
policy when horizon is 2 and samples 50 (as in Fig. 4c). The robot behavior is consistent with the one in
Fig. 4c. The bottom row has horizon set to 20 and samples to 50. The learned poli cy achieves results that
are consistent with the top row (panel a). In all panels, initial positi ons are the same as the ones in the
main text. See Methods (at the end of Experiments settings) and Sup plementary Information (Sec. S5 –
MaxDiﬀ settings) for the details.
S1 Background
Sets and operators are in calligraphic characters and vectors in bold. A rand om variable is denoted by V
and its realization is v. The symbol ∶= denotes a deﬁnition. We consider continuous (discrete) random
variables and we denote the probability density function (pdf) of V by p(v) (for discrete variables, p(v)
is probability mass function , pmf). The convex subset of pdfs (pmfs) is denoted by D. The expectation
of a function h(⋅) of a continuous V is denoted Ep[h(V)] ∶= ∫v h(v)p(v)dv, where the integral is over
the (compact) support of p(v), which we denote by supp p; whenever it is clear from the context, we
omit the subscript in the sum (for discrete variables, the integral is replaced with the sum). The joint
pdf/pmf of V1 and V2 is denoted by p(v1, v2); the conditional pdf/pmf of V1 with respect to (w.r.t.) V2
is p (v1 ∣v2). Given p(v) and q(v), we say that p(v) is absolutely continuous with respect to (w.r.t.) q(v)
if supp p ⊆supp q. We denote this by writing p ≪q. Countable sets are denoted by {wk}k1∶kn , where wk is
the generic set element, k1 (kn) is the index of the ﬁrst (last) element and k1 ∶kn is the set of consecutive
integers between (including) k1 and kn. Finally, we denote by L1(V) the space integrable functions on V
and we use the shorthand notation a.s. for almost surely.
The Kullback-Leibler Divergence. We recall the deﬁnition for the Kullback-Leibler (KL) divergence
2
Deﬁnition S1.1. The KL divergence of p(v) w.r.t. q(v) with p ≪q is:
DKL (p ∣∣q)∶=∫v
p(v)ln (p(v)
q(v))dv.
Intuitively, the KL divergence is a measure of the proximity of th e pair of pdfs. This is bounded only
if p ≪ q [3, Chapter 8]. Also, (p, q) ↦ DKL (p ∣∣q) is a jointly convex function and hence p ↦ DKL (p ∣∣q),
q ↦DKL (p ∣∣q) are convex. We also recall the following chain rule for the KL divergen ce:
Lemma S1.1. Let V and Z be two random variables and let p(v, z) and q(v, z) be two joint pdfs. Then:
DKL (p(v, z) ∣∣q(v, z))=DKL (p(v) ∣∣q(v))+Ep(v) [DKL (p(z ∣v) ∣∣q(z ∣v))].
Finally, the following result is useful to characterize the feasi bility domain of our control problem. In
the statement, adapted from [ 4, Proposition 2.1], M(V) is the subset of probability measures on V ⊆Rk
Lemma S1.2. For any µ ∈M(V), V ⊆Rk and any M <∞, the set
{ν ∈M(V)∶ DKL (ν ∣∣µ)≤M},
is compact with respect to weak convergence of probability measures.
S2 Relating DR-FREE with other frameworks
We recall that DR-FREE computes the policy via the distributionall y robust generalization of the free
energy principle in Fig. 2a. This formulation – oﬀering the problem stat ement for policy computation
– explicitly embeds ambiguity constraints to account for (model) amb iguity. The formulation yields a
distributionally robust sequential policy optimization problem feat uring the free energy as objective and
ambiguity constraints formalized via the KL divergence. Both the obje ctive and constraints are nonlinear
SI-9
in the decision variables of the problem. The constraints explicitl y deﬁne an ambiguity set with radius that
can depend on both state and action. In the main text we showed that the pol icy optimization problem
in Fig. 2a tackled by DR-FREE has the same optimal solution as
min
{πk(uk∣xk−1)}1∶N
max
pk(xk∣xk−1,uk)∈Bη(¯pk(xk∣xk−1,uk))
DKL (p0∶N ∣∣ ˜q0∶N ) (SI-1)
We then showed that – with a proper choice of ˜ q0∶N – the MaxDiﬀ objective is a relaxation of the above
problem when there is no ambiguity. Starting from this ﬁnding – and b uilding on the links highlighted in
the main text and in the Methods – we now connect DR-FREE with other related robust decision-making
frameworks, placing it in the context of the broad literature on maximum entropy, variational inference,
control-as-inference, distributionally robust learning and optimi zation.
As highlighted in the main text, MaxDiﬀ can generalize 5, 6 maximum entropy 7, 8 (MaxEnt) inheriting
the desirable properties of MaxEnt policies. A remarkable property i s policy robustness to perturbations in
the dynamics. 9 Formally, MaxEnt policies maximize a lower bound of the distribution ally robust objective 9
max
{πk(uk∣xk−1)}1∶N
min
pk(xk∣xk−1,uk)∈˜P
Ep0∶N [
N
∑
k=1
˜r(Xk, Uk)] (SI-2)
In ( SI-2), ˜r is a pessimistic reward, 9 this means that to compute policies that robustly maximize a reward
MaxtEnt must be used with a diﬀerent (pessimistic) reward func tion. Also, in ( SI-2) the ambiguity set
˜P is not arbitrary; rather, its radius 9 is constant – it directly depends upon the entropy of the optimal
policy, which is the solution of MaxEnt itself and explicit bounds are available only in discrete settings.
The bounds come from the fact that, to obtain robustness estimates, the authors9 do not solve the full
problem in ( SI-2) but rather obtain a lower bound for the inner minimization. DR-FREE solves the
problem in Fig. 2a – and hence in ( SI-1). In doing so, DR-FREE deﬁnes robustness against ambiguity
in the problem formulation, via the ambiguity set. This means not only t hat DR-FREE policy is robust
across this ambiguity set, but also that DR-FREE does not need to be app lied on a pessimistic policy. The
key enabler to achieve these desirable properties is the ability t o solve the robust free energy formulation
in Fig. 2a. As we detail next, we are not aware of any other approach to solve this problem in the broad
literature on robust decision-making. Namely, our suvey highlights that , in this broader landscape, there
is no method that can solve the full min −max problem in Fig. 2a. We take as a starting point MaxEnt
– which we discussed in the main text. While distributionally rob ust versions of imitation RL based on
this principle have been proposed, results are obtained 10 by assuming that – besides ﬁnite state spaces –
the underlying optimization features a cost functional and constraint s that are both linear in the decision
variable of the inner optimization problem. This does not hold for the fu nctional and constraints of our
robust free energy principle. More broadly, as also surveyed in the l iterature11–13, 16–18, 20 the linearity
assumption on the objective function in one of the decision variables is crucial for other state-of-the-art
policy computation methods across learning and control. This includes highly inﬂuential distributionally
robust frameworks such as distributionally robust Q-learning. 21 In this work, while ambiguity constraints
are formalized via KL divergence – as in DR-FREE – the optimization obje ctive beneﬁts from being
linear in the decision variable of the inner problem. As a result, dis tributionally robust Q-learning cannot
compute a policy that solves the full problem in Fig. 2a. KL divergence i s also widely used as a cost
functional in the context of variational inference (and control-as-infe rence). However, also in this literature,
when distributionally robust frameworks are proposed, in line with t he distributionally robust Q-learning
setting, the KL divergence (and the entropy) is included in the pr oblem in a way that guarantees linearity
SI-10
of the cost with respect to the inner decision variable. 14 Finally, we also position DR-FREE – and its
contributions – in the context of the rich literature on distributi onally robust optimization (DRO). Since
Scarf’s pioneering work, 15 DRO (and Bayesian DRO) has emerged as a central topic in robust optimizat ion
– gaining increasing attention in the context of machine learning. 22–26, 29 DRO concepts have developed
signiﬁcantly over the years and we refer to surveys 27, 28 for an overview of this vast literature. In these
works – and surveys – the optimization problem beneﬁts from the cost be ing linear in at least one decision
variable – thus ruling out having free energy as objective. More rece ntly,24 nonlinear DRO problems are
considered and a Frank-Wolfe algorithm has been introduced to tackle a class of problems featuring cost
functions that have a continuous Lipschitz G-derivative. This assu mption does not hold for the free energy
objective of the distributionally robust free energy principle in Fig. 2a.
In summary, DR-FREE allows to solve distributionally robust, min −max, policy optimization problems
having free energy as cost functional and, at the same time, distribut ionally robust constraints deﬁned
via the KL divergence. This contribution – interesting per se – d oes not just bring broad theoretical
implications. As discussed in the main text, it is this key advancem ent that enables DR-FREE to retain
the desirable theoretical properties of state-of-the-art – provably robust – methods such as MaxEnt and
MaxDiﬀ, guaranteeing these properties over the ambiguity set deﬁne d in the problem formulation.
S2.1 DR-FREE and the MDP formalism
Following the literature on distributionally robust Markov Decisi on Processes,
21, 30 we complement the
above survey by relating the sequential policy computation problem t ackled in DR-FREE (Fig. 2a) and
the MDP formalism. MDPs can be typically deﬁned according to a 6-tupl e30 (T, γ, X, U, ˜p, r), where: (i)
T is the decision horizon; (ii) γ ∈ (0, 1] is the discount factor; (iii) X and U are the state/action spaces;
(iv) ˜p(xk ∣xk−1, uk) is the probability of reaching xk from xk−1 after taking action uk; (v) r is the expected
reward. These elements are directly mapped onto DR-FREE policy com putation problem. Namely, in
DR-FREE: (i) T is the decision horizon ( N in Fig. 2a); (ii) γ = 1; (iii) X and U are again the state and
action spaces (see also at the beginning of Sec. S3); (iv) ˜p(xk ∣xk−1, uk) is pk (xk ∣xk−1, uk); (v) the reward
is the negative of the cost. Following standard distributionally robus t MDP frameworks, 21 an adversarial
picks – within an ambiguity set – the worst-case transition model that minimizes the cumulative reward.
In DR-FREE it maximizes the cumulative cost in the ambiguity set. T hat is:
max
{pk(xk∣xk−1,uk)}1∶N
DKL (p0∶N ∣∣q0∶N )+Ep0∶N [
N
∑
k=1
c(x)
k (Xk)+c(u)
k (Uk)]
s. t. p k (xk ∣xk−1, uk)∈Bη (¯pk (xk ∣xk−1, uk)), ∀k =1, . . . , N.
This is the inner maximization problem in Fig. 2a with ambiguity set de ﬁned in Methods.
S3 Resolution Engine Details
We now describe the formal details of the engine in Fig. 2b (proofs in Se c.
S6). The state-space is X⊆Kn
and the action space is U⊆Kp. The spaces can be both continuous and discrete (we let K be either R or
Z). Also, we make the following two assumptions:
A1 pk (xk ∣xk−1, uk)and qk (xk ∣xk−1, uk)are bounded and supp ¯pk (xk ∣xk−1, uk)⊆supp qk (xk ∣xk−1, uk);
SI-11
A2 the state cost is non-negative upper bounded in X, the action cost is non-negative lower bounded in U.
Our starting point is the optimization problem from the Methods, also reported here for convenience:
min
π(u)
k∣k−1∈D
DKL (π(u)
k∣k−1 ∣∣q(u)
k∣k−1)+Eπ(u)
k∣k−1
[c(u)
k (Uk)]+ max
p(x)
k∣k−1
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]]
s. t. p (x)
k∣k−1 ∈Bη (¯pk (xk ∣xk−1, uk)).
(SI-3)
We ﬁrst show how ( SI-3) is tackled in DR-FREE. Then we show that this problem is the right one that
should be solved to ﬁnd the optimal policy in accordance with the robust free energy principle.
Tackling SI-3. At each k, according to ( SI-3) the optimal policy can be found by ﬁrst ﬁnding the optimal
value of the inner maximization problem and then minimizing over the policies. DR-FREE relies on ﬁnding
the optimal value of the inner problem via a convenient scalar reformul ation. To obtain the reformulation,
we ﬁrst show that for the inner problem in ( SI-3) the expectation and maximization can be swapped.
Intuitively, this is possible because the feasibility domain is w ell-behaved (convex and compact, see Sec.
S6.2) and the decision variable of the optimization problem does not depend on the pdf over which the
expectation is taken. This intuition is formalized with the followi ng result:
Lemma S3.1. The optimal solution, p(x),⋆
k∣k−1 ∶= p⋆
k (xk ∣xk−1, uk), for the inner maximization problem in
(
SI-3) exists and it holds that:
Eπ(u)
k∣k−1
[DKL (p(x),⋆
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x),⋆
k∣k−1
[¯ck (Xk)]]
= max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]].
Essentially, the above result establishes that the maximization st ep can be performed by ﬁrst solving
the problem in green in Fig. 2b and then taking the expectation. Still, this is an inﬁnite dimensional
optimization problem that we seek to conveniently recast as a scalar opti mization. To do so, we leverage
a change of variables technique 29 using the likelihood ratio (or Radon-Nikodym derivative)
rk∣k−1 ∶=
p(x)
k∣k−1
¯p(x)
k∣k−1
. (SI-4)
With this change of variables, and exploiting the fact that the frontier of the feasibility domain is the set
of all p(x)
k∣k−1 ∈D such that DKL (p(x)
k∣k−1 ∣∣ ¯p(x)
k∣k−1)=ηk (xk−1, uk), we obtain the following:
Lemma S3.2. Consider the problem in green in Fig. SI-3. Then:
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]=ηk (xk−1, uk)−max
α≥0
min
rk∣k−1∈R
L(rk∣k−1, α),
(SI-5)
SI-12
where L(rk∣k−1, α) is given by
L(rk∣k−1, α)=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)+α (rk∣k−1 ln rk∣k−1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
−αηk (xk−1, uk). (SI-6)
The result establishes that the cost of ambiguity is ηk (xk−1, uk)−maxα≥0 minrk∣k−1∈R L(rk∣k−1, α). Next,
we need to ﬁnd the optimal value of −maxα≥0 minrk∣k−1∈R L(rk∣k−1, α). This is ˜ c (xk−1, uk) in Fig. 2b. To
this aim, using (
SI-6) we have:
−max
α≥0
min
rk∣k−1∈R
L(rk∣k−1, α)=
−max
α≥0
−ηk (xk−1, uk)α + min
rk∣k−1∈R
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)+αrk∣k−1 ln rk∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
(SI-7)
Then, we deﬁne
Wα (xk−1, uk)∶= min
rk∣k−1∈R
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)+αrk∣k−1 ln rk∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎦
, (SI-8)
so that the problem in (
SI-7) can be written as
min
α≥0
ηk (xk−1, uk)α −Wα (xk−1, uk). (SI-9)
The ˜c (xk−1, uk) in Fig. 2b is the optimal value of the above problem. The following theore m states that
˜c (xk−1, uk) can be found by solving a scalar and convex optimization problem:
Theorem S3.1. For each xk−1 and uk, the optimal value of the problem in (SI-9) is ﬁnite and given by:
˜c (xk−1, uk)∶=min
α≥0
˜Vα (xk−1, uk), (SI-10)
where
˜Vα (xk−1, uk)=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
α ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
+αηk (xk−1, uk), α >0
M (xk−1, uk), α =0,
(SI-11)
with
M (xk−1, uk)∶= lim sup
xk∈supp ¯p(x)
k∣k−1
ln
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
≥0. (SI-12)
The above results shows that, in order to determine its optimal dec ision amid environmental ambiguity,
the agent does not need to know what is the worst case pk (xk ∣xk−1, uk). Rather, the agent only needs to
know what is the cost of ambiguity, i.e., ˜ c (xk−1, uk) and ηk (xk−1, uk). In our experiments (see Results)
we computed ˜c (xk−1, uk), and hence built the cost of ambiguity in Fig. 2b, by solving the optimiz ation
problem in (
SI-10). This problem is the one included in the algorithm deployed on the age nts (reported in
Section S5). Moreover, we did not have to build any specialized software to solv e this problem because it is
SI-13
convex and admits a global minimum. We can claim this because of the next r esult, which also establishes
that ˜c (xk−1, uk) is bounded. To introduce the result we deﬁne explicitly
Vα (xk−1, uk)=α ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
+αηk (xk−1, uk). (SI-13)
Theorem S3.2. For each xk−1 and uk, the following statements hold.
1. lim
α→∞
Vα (xk−1, uk)=+∞. Also, Vα (xk−1, uk)>0 for every α >0;
2. Vα (xk−1, uk) is: (i) linear and equal to ln ¯c+ηk (xk−1, uk)α, if there exists some constant, say ¯c, such
that
¯p(x)
k∣k−1 exp ¯ck(xk)
q(x)
k∣k−1
= ¯c; (ii) strictly convex on (0, +∞) otherwise;
3. lim
α→0
Vα (xk−1, uk)=M (xk−1, uk), where M (xk−1, uk) is deﬁned by (SI-12);
4. ˜Vα (xk−1, uk) has a global minimum on [0, +∞).
In summary, the above results show that the optimal value for the inne r maximization problem in ( SI-3)
is Eπ(u)
k∣k−1
[ηk (Xk−1, Uk)+˜c (Xk−1, Uk)] and ˜c (xk−1, uk) can be eﬀectively computed via convex optimiza-
tion. This yields the problem in Fig. 2b where the variational free ene rgy is minimized across the policies.
The next step is to show that ( SI-3) is the right problem to solve and, if so: (i) give the optimal solution
of the minimization problem; (ii) establish how ¯ ck (xk) is built. This is done next.
The optimal policy. The next result shows why, at each k, problem in ( SI-3) needs to be solved, gives
the optimal policy and determines what is the lowest free energy that the agent can achieve.
Corollary S3.1. The distributionally robust free energy principle yields the optim al policy {π(u),⋆
k∣k−1}, with
π(u),⋆
k∣k−1 =π⋆
k (uk ∣xk−1)=π(u),⋆
k∣k−1 =
q(u)
k∣k−1 exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk))
∫ q(u)
k∣k−1 exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk))duk
, (SI-14)
where
˜c (xk−1, uk)=min
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
lim sup
xk∈supp ¯p(x)
k∣k−1
ln
⎛
⎜
⎝
¯p(x)
k∣k−1 exp (¯ck (xk))
q(x)
k∣k−1
⎞
⎟
⎠
,
min
α>0
ηk (xk−1, uk)α +α ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
;
(SI-15)
and
¯ck (xk)=c(x)
k (xk)+ˆck+1 (xk), (SI-16)
with
ˆcN+1 (xN )=0,
ˆck (xk−1)=−ln ∫ q(u)
k∣k−1 exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk))duk, k ∈1 ∶N.
(SI-17)
SI-14
Moreover, the optimal cost, i.e., the smallest free energy that can b e achieved, is bounded and given by
∑
k∈1∶N
Ep(xk−1) [ˆck (Xk−1)]. (SI-18)
S4 Why It Is Better to Be Ambiguity-Free
We give the formal details explaining why an agent interacting with an amb iguous environment cannot
outperform an agent that has no environmental ambiguity. First, we charact erize what happens to the
problem in the left hand-side of (
SI-25) as the ambiguity set shrinks. Then, we discuss what this implies
for the optimal policy. We recall that the radius of ambiguity is bounded an d therefore there is some
η <+∞ such that ηk (xk−1, uk)≤η. Also, the optimal value of the problem in ( SI-25) is ˜c (xk−1, uk), which
is given in Theorem S3.1, and we use the notation ˜ cη (xk−1, uk) to stress the dependency of ˜ c (xk−1, uk) on
η. We also discuss why the optimal policy down-weights the model ove r the ambiguity radius (see main
text) when ambiguity increases.
Theorem S4.1. Let the set-up of item 2(ii) in Theorem
S3.2 hold. Then:
lim
η→0
˜cη (xk−1, uk)=DKL (¯p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+E¯p(x)
k∣k−1
[¯ck (Xk)]. (SI-19)
We characterize what happens to ˜ c (xk−1, uk) as ambiguity vanishes. Next, we establish that an
ambiguity-aware agent making optimal decisions necessarily outperform s an agent aﬀected by ambigu-
ity. In essence, DR-FREE forbids that ambiguity is exploited to ach ieve better performance.
Lemma S4.1. The cost of an agent aﬀected by ambiguity cannot be better than the opti mal cost of an
ambiguity-free agent. That is,
−ln ∫U
q(u)
k∣k−1 exp (−DKL (¯p(x)
k∣k−1 ∣∣q(x)
k∣k−1)−E¯p(x)
k∣k−1
[¯ck (xk)]−c(u)
k (uk))duk (SI-20)
<−ln ∫U
q(u)
k∣k−1 exp (−ηk (xk−1, uk)−˜c (xk−1, uk)−c(u)
k (uk))duk (SI-21)
We recall from the main text that characterizing the policy when ambigu ity increases amounts at
studying what happens when ηmin =min ηk (xk−1, uk) increases. Note that, from Theorem S3.1, ˜c (xk−1, uk)
is independent on ηk (xk−1, uk) when ηmin is suﬃciently large. In fact, from the proof of Theorem S3.2(i)
the left hand side in the ﬁrst line of ( SI-11) is greater than αηk (xk−1, uk). Therefore, for ηmin suﬃciently
large, the exponents in DR-FREE policy become −ηk (xk−1, uk), thus yielding the results observed in the
main text.
S5 Supplementary Details For The Experiments
The hardware used for the in-silico experiment results was a laptop with an 12th Gen Intel Core i5-12500H
2.50 GHz processor and 18 GB of RAM. A comparative table of the time taken, at each step, to compute
the control action for each of the policy computation methods from the main t ext is reported at the end
of the Supplementary Information (Tab.
S-3).
DR-FREE deployment. The pseudocode implementing the resolution engine deployed on t he agents
is given in Algorithm 1. We ﬁrst discuss the setting for the robot experiments. The Inpu ts line speciﬁes
SI-15
what information, given the current position/state, DR-FREE has availab le at each k. The algorithm out-
puts (line 4) the policy π⋆
k (uk ∣xk−1), which we recall being a soft-max. The exponential in the soft-max
contains the radius of ambiguity, ηk (xk−1, uk), and ˜ c (xk−1, uk). This latter term is computed in lines
1 − 3, which directly follow from the statement of Theorem S3.1. In the experiments, the input space
is discretized in a 5 × 5 grid. In our experimental deployment, ηk (xk−1, uk) is deﬁned as described in
the Methods (Experiments settings); as highlighted in the Discu ssion and in the Methods, in the current
implementation, the radius of ambiguity is provided to DR-FREE rath er than learned. As also outlined
in the Methods, the ηk (xk−1, uk) used in the experiments is set to capture higher agent conﬁdence (i .e.,
lower ambiguity) as the agent gets closer to its goal destination. Supplem entary Fig. 2 shows that, ef-
fectively, pk (xk ∣xk−1, uk) is always inside the ambiguity set. All other inputs to the algorithm d eﬁned
within the Methods. In Fig. 3, the generative model qk (xk ∣xk−1, uk) for the experiments – a Gaussian
centered in the goal position – is goal-encoding. Instead, for the DR-FRE E experiments reported in Fig. 4,
qk (xk ∣xk−1, uk) is set to pmax (xk ∣xk−1) from the MaxDiﬀ framework as described in Results. See Code
Availability. In the accompanying code, we used scipy solver to tackle the optimization and we veriﬁed
numerically that it would achieve the global minimum. In line 1 of Algorithm 1, the max was computed
by obtaining samples from ¯ pk (xk ∣xk−1, uk) and evaluating the expression on the right-hand side for each
sample. The largest value was then selected as maximum.
Algorithm 1: Pseudocode for DR-FREE resolution engine deployed on the robots
Result: Robot policy
Input: qk (xk ∣xk−1, uk), qk (uk ∣xk−1), ¯pk (xk ∣xk−1, uk), ηk (xk−1, uk), c(x)
k (xk)
Output: π⋆
k (uk ∣xk−1)
1 M (xk−1, uk)← max
xk∈supp ¯pk(xk∣xk−1,uk)
ln (
¯pk(xk∣xk−1,uk)exp c(x)
k (xk)
qk(xk∣xk−1,uk) );
2 ˜Vα (xk−1, uk)←
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
α ln E¯pk(xk∣xk−1,uk)
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯pk (xk ∣xk−1, uk)exp c(x)
k (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
+αηk (xk−1, uk), α >0
M (xk−1, uk), α =0;
3 ˜c (xk−1, uk)←min
α≥0
˜Vα (xk−1, uk);
4 π⋆
k (uk ∣xk−1)← qk(uk∣xk−1)exp(−ηk(xk−1,uk)−˜c(xk−1,uk))
∫ qk(uk∣xk−1)exp(−ηk(xk−1,uk)−˜c(xk−1,uk))duk
;
Robot trajectories used to obtain the trained model at each phase of train ing are shown in Fig. SI-8.
The trajectories were obtained by sampling actions from a uniform dis tribution and then injecting in the
position a bias equal to 0 .1xk−1. See Code Availability for the data used for training.
Ambiguity-unaware agent deployment. The optimal policy from the literature 19 is given by
exp (−DKL (¯pk (xk ∣xk−1, uk) ∣∣qk (xk ∣xk−1, uk))−E¯pk(xk∣xk−1,uk) [c(x)
k (xk)])
∑uk exp (−DKL (¯pk (xk ∣xk−1, uk) ∣∣qk (xk ∣xk−1, uk))−E¯pk(xk∣xk−1,uk) [c(x)
k (xk)])
.
In the experiments, the divergence was considerably higher than th e expected cost, which accounted
for the presence of obstacles. As a result, in the policy, the exponen t was approximately equal to
exp (−DKL (¯pk (xk ∣xk−1, uk) ∣∣qk (xk ∣xk−1, uk))). Since in the experiments qk (xk ∣xk−1, uk) is a Gaus-
sian centered in the goal destination, this policy would direct the rob ot along the shortest path towards
the goal without taking into account the presence of the obstacles and thu s explaining the behavior ob-
SI-16
Figure SI-8: Position data collected for training at each training stage. The correspon ding trained model
is detailed in the Methods.
served in the main text. In the accompanying code, the policy is comp uted with a numerical method that
prevents underﬂow due to large divergences.
Belief update. The optimization problem solved to obtain the belief update results i s
min
v,w
M
∑
k=1
⎛
⎝−
F
∑
i=1
wifi (ˆxk−1, ˆuk)+ln ∑
uk
qk (uk ∣xk−1 = ˆxk−1)exp (
F
∑
i=1
wifi (ˆxk−1, uk)+
G
∑
i=1
vigi (uk))⎞
⎠.
This problem – derived by dropping the ﬁrst term from the negative l og-likelihood in the Methods – is
convex. Data points used to reconstruct the cost and code available (s ee Code Availability). The code
builds on the one from.
19
Increasing ambiguity. Fig. 3e gives a screenshot of the policy when the ambiguity radius is inc reased,
without clipping, as detailed in the main text. When exp (−ηk (xk−1, uk)) leads to underﬂows, we replace
0 with 1e-10. The ﬁgure was obtained by discretizing the input space in a 50 ×50 grid. See Code Availability.
Decreasing ambiguity. As in Fig. 3e, Supplementary Fig. 5c and Supplementary Fig. 6b were obtaine d
by discretizing the input space in a 50 ×50 grid. As described in the main paper, when the ambiguity
radius is zero (the ambiguity constraint is relaxed) DR-FREE policy can be conveniently computed by
simply setting ˜c (xk−1, uk) as in ( SI-19). Fig. SI-9 conﬁrms that, as the radius decreases, DR-FREE policy
approaches the optimal policy for the relaxed problem also known in the l iterature.19
MaxDiﬀ settings. We use the code provided in the original MaxDiﬀ paper, 5 interfacing it with the
Robotarium. We created a wrapper around the Robotarium environment so th at this could be compatible
with the MaxDiﬀ available code structure. In the original code, the MaxDiﬀ authors provide a ﬂag to let
MaxDiﬀ use the real environment (reward and model) rather than learni ng it from data. In the MaxDiﬀ
experiments of the main text and Supplementary Fig. 7 (panel a) MaxDiﬀ has access to the reward and
model and this is achieved by properly setting the ﬂag. For the experiments in the Supplementary Fig. 7
(panel b) the ﬂag value is changed so that MaxDiﬀ learns reward and model. See Code Availabi lity for the
code to replicate the results. Tab. S-1 contains the MaxDiﬀ hyperparameters used to obtain the results
described in the paper.
SI-17
Figure SI-9: Mismatch between DR-FREE policy at diﬀerent ambiguity levels and t he optimal policy
from the literature. 19 The mismatch is quantiﬁed using the KL divergence between the tw o policies. Blue
curve corresponds to Supplementary Fig. 5 and red curve to Supplem entary Fig. 6, respectively. The
x-axis shows the scaling factor for the ambiguity radius and the y-axis t he KL divergence. Values are for
radii decreased by 50%, 70%, 90%, 99% and for a radius of 0. The dashed lines connec ting the points are
interpolations. When the radius is zero, the KL divergence is also ze ro – the two policies are the same.
Ant experiments settings. The task consists in moving the Ant forward, in the x-coordinate dire ction.
The covariance matrix for the generative model in DR-FREE is provid ed in our repository. For the MaxDiﬀ
and NN-MPPI implementations, we use the code from the literature. 5 In MaxDiﬀ and NN-MPPI – both
having access to the environment reward – the horizon is set to 2 in ac cordance with the other MaxDiﬀ
experiments from the main text. The other hyperparameters, summar ized in Tab. S-4, are taken as in
the Ant experiments from the literature. 5 The table also reports the architecture of the neural network
providing mean and variance for ¯ pk (xk ∣xk−1, uk). This is the same architecture used in the MaxDiﬀ
paper5 for the Ant experiments. The ambiguity radius, capturing higher conﬁ dence as the Ant moves
forward, is the KL divergence between a Gaussian centered at a torso x -velocity of 1 m/s and a Gaussian
whose mean and variance are obtained by the neural network for the torso x-v elocity. Fig. SI-10 shows
that pk (xk ∣xk−1, uk) is always inside the ambiguity set. See repository for details.
S6 Proving The Statements
We now give the detailed proofs for the formal statements in the Suppl ementary Information. Proofs are
presented for continuous variables; the discrete case follows analogous arguments. First, we introduce some
instrumental results.
S6.1 Instrumental Results
The following result, Bauer’s Maximum Principle, is adapted from [
31, Corollary A.4.3]. The result is
instrumental to tackle concave programs that arise within the proof of ou r results when maximizing free
energy across all possible environments in the ambiguity set
SI-18
Figure SI-10: pk (xk ∣xk−1, uk) belongs to the ambiguity set Bη (¯pk (xk ∣xk−1, uk)). As in Supple-
mentary Fig. 2, the plot conﬁrms this by showing that ηk (xk−1, uk) – in red – is always bigger than
DKL (pk (xk ∣xk−1, uk) ∣∣ ¯pk (xk ∣xk−1, uk)) – in green. The ﬁgure, obtained from one experiment, is repre-
sentative for all the Ant experiments.
Lemma S6.1. Let: (i) K be a nonempty compact set in a Hausdorﬀ locally convex topological ve ctor space
X; (ii) ∂K be the frontier of K; (iii) f ∶ X → R be an upper-semi-continuous convex function. Then,
max
K
f =max
∂K
f.
We also leverage the following Lagrange Duality result [
32, Section 8 .6, Theorem 1]
Theorem S6.1. Let: (i) X be a linear vector space; (ii) Ω be a convex subset of X; (iii) f be a real-valued
convex functional on Ω; (iv) G be a real convex mapping of X. Assume that:
1. there exists some ˜x ∈Ω such that G (˜x)<η;
2. µ0 ∶={min f(x), s.t G(x)≤η, x ∈Ω} is ﬁnite, i.e., µ0 <∞.
Then:
min
G(x)≤η
x∈Ω
f(x)=max
α≥0
min
x∈Ω
[f(x)+α(G(x)−η)].
S6.2 Finding The DR-FREE Policy
After characterizing the set Bη (¯pk (xk ∣xk−1, uk)), we give a proof for Lemma
S3.1, which shows that in
(SI-3) the expectation and maximization can be swapped.
Lemma S6.2. The set Bη (¯pk (xk ∣xk−1, uk)) in (SI-3) is convex and compact in L1(X) equipped with the
usual norm.
SI-19
Proof. We ﬁrst show convexity. Pick any ˜ p1, ˜p2 ∈ Bη (¯pk (xk ∣xk−1, uk)) and a constant, say t ∈ [0, 1].
Then, by convexity of the KL divergence we have:
DKL (t˜p1 +(1 −t)˜p2 ∣∣ ¯p(x)
k∣k−1)≤tDKL (˜p1 ∣∣ ¯p(x)
k∣k−1)+(1 −t)DKL (˜p2 ∣∣ ¯p(x)
k∣k−1)≤ηk (xk−1, uk).
This, together with the fact that supp (t˜p1 +(1 −t)˜p2)⊆q(x)
k∣k−1 shows that Bη (¯pk (xk ∣xk−1, uk)) is convex.
Now, we prove that the set is also compact and in what follows we let η < +∞ be a positive number
such that ηk (xk−1, uk) ≤ η, ∀xk−1, uk. To this aim, we leverage Lemma S1.2. By Radon-Nikodym The-
orem [ 33, Theorem 6.10, page 121] it follows that for each probability measure ν ∈ M(X), there exists
a unique pdf, say pν , such that ν(E) = ∫E pν dλ for all measurable set E (λ is the Lebesgue measure on
X). In turn, by means of Lemma S1.2, we have that the set ˜Bη = {ν ∈ M(X) ∶ DKL (ν ∣∣µ) ≤ η} is
compact. Let ¯p(x)
k∣k−1 be the pdf corresponding to µ and consider the functional Φ ∶ M(X) → L1(X), with
Φ(ν)=px,v
k∣k−1, ( L1(X) is equipped with the usual norm). We now prove that Φ is continuous. To this aim,
pick the measure ν ∈ M(X) given by νn → ν, i.e., for all E, one has that ∫E px,vn
k∣k−1dλ → ∫E px,v
k∣k−1dλ and
hence Φ (νn)→Φ(ν) in the usual L1 norm. This shows that Φ is in fact continuous. Moreover, continuous
functions map compact sets into compact sets and, additionally, Φ (˜Bη) = Bη (¯pk (xk ∣xk−1, uk)). This
yields the conclusion.
Next, we prove Lemma S3.1, where we establish that in ( SI-3) the expectation and maximization can
be swapped.
Proof of Lemma S3.1. First, note that (as shown in Lemma S6.2) the set Bη (¯pk (xk ∣xk−1, uk)) is
convex and compact. This, together with the continuity and convexity of the KL divergence in the decision
variable implies that p(x),⋆
k∣k−1 exists. By deﬁnition, we have
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]
=DKL (p(x),⋆
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x),⋆
k∣k−1
[¯ck (Xk)],
and hence
Eπ(u)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]
⎤
⎥
⎥
⎥
⎥
⎥
⎦
=Eπ(u)
k∣k−1
[DKL (p(x),⋆
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x),⋆
k∣k−1
[¯ck (Xk)]]
≤ max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]].
(SI-22)
Additionally, for each feasible p(x)
k∣k−1,
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]≤ max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)].
SI-20
That is, for each feasible p(x)
k∣k−1,
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]]
≤Eπ(u)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
Hence we may continue the chain of inequalities in (
SI-22) with
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
Eπ(u)
k∣k−1
[DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]]
≤Eπ(u)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]
⎤
⎥
⎥
⎥
⎥
⎥
⎦
,
(SI-23)
Combining (
SI-22) and ( SI-23) yields the desired conclusion.
Next, we obtain the scalar reformulation enabling the computation of the cos t of ambiguity in Fig. 2b.
Before proving this result, consider the likelihood ratio in ( SI-4). Note that, a.s., for every feasible p(x)
k∣k−1,
we have rk∣k−1 ≥0 and E¯p(x)
k∣k−1
[rk∣k−1]=1. This motivates the following:
Deﬁnition S6.1. The set of likelihood ratios generated by p(x)
k∣k−1 is R ∶= {rk∣k−1 ∶ E¯p(x)
k∣k−1
[rk∣k−1] =
1, r k∣k−1 ≥0, a.s.}.
We can now prove Lemma S3.2.
Proof of Lemma S3.2. The maximization problem in green in Fig. 2b can be recast as:
max
rk∣k−1∈R
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1
⎛
⎜
⎝
ln rk∣k−1 +ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+¯ck (Xk)
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
s. t.E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]≤ηk (xk−1, uk).
(SI-24)
Also, from Lemma
S6.1, by setting:
• f(rk∣k−1)∶=E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1 +rk∣k−1 ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+rk∣k−1¯ck (Xk)];
• K ∶= {rk∣k−1 ∶ rk∣k−1 ∈ R, E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1] ≤ ηk (xk−1, uk)}, which is compact and convex.
Compactness follows from Lemma S6.2, while convexity in the decision variable follows from the fact
that E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1] is convex w.r.t. rk∣k−1;
• X ∶=L1(X), which is a topological vector space;
SI-21
we have that the optimal value of the problem in ( SI-24) is given by:
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
max
rk∣k−1∈R
E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1 +rk∣k−1 ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+rk∣k−1¯ck (Xk)]
s.t E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]=ηk (xk−1, uk)
=ηk (xk−1, uk)+
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
max
rk∣k−1∈R
E¯p(x)
k∣k−1
[rk∣k−1 ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+rk∣k−1¯ck (Xk)]
s.t E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]=ηk (xk−1, uk).
,
which, by assumptions A1–A2, has a bounded optimal value. Also, we have:
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
max
rk∣k−1∈R
E¯p(x)
k∣k−1
[rk∣k−1 ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+rk∣k−1¯ck (Xk)]
s.t E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]=ηk (xk−1, uk)
=−
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
min
rk∣k−1∈R
E¯p(x)
k∣k−1
[rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)]
s.t E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]=ηk (xk−1, uk)
.
(SI-25)
The cost in the right-hand side of ( SI-25) is linear in the decision variable; thus, its optimal value is the
same as
min
rk∣k−1∈R
E¯p(x)
k∣k−1
[rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)]
s.t E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]≤ηk (xk−1, uk).
Thus, we can leverage Theorem S6.1 to obtain a reformulation of the functional convex optimization
problem in the right-hand side of ( SI-25) having rk∣k−1 as decision variable. To this aim, in Theorem S6.1
set: (i) X ∶=L1(X); (ii) Ω ∶=K, with K deﬁned above; (iii) G(rk∣k−1)=E¯p(x)
k∣k−1
[rk∣k−1 ln rk∣k−1]−ηk (xk−1, uk),
and f(rk∣k−1)=E¯p(x)
k∣k−1
[rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)]. Further, note that, with these deﬁnitions: (i) Ω is a
convex subset of the linear vector space X; (ii) f is real-valued and convex w.r.t. rk∣k−1 on Ω; (iii) G is a
real convex mapping of rk∣k−1. The hypotheses of Theorem S6.1 are all satisﬁed and this implies that
max
p(x)
k∣k−1∈Bη(¯pk(xk∣xk−1,uk))
DKL (p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+Ep(x)
k∣k−1
[¯ck (Xk)]=ηk (xk−1, uk)−max
α≥0
min
rk∣k−1∈R
L(rk∣k−1, α),
with L(rk∣k−1, α) given in ( SI-6). This yields the desired conclusion.
Next, we give the proof for Theorem S3.1, which yields the scalar, convex, optimization problem to
compute ˜c (xk−1, uk).
Proof of Theorem S3.1. We discuss separately two cases for α >0 and α =0.
Case (i): α > 0. The problem in ( SI-8) is a convex functional optimization problem with cost and
constraints that are diﬀerentiable in the decision variable. We ﬁnd t he optimal solution by studying the
SI-22
variations of the Lagrangian associated to the problem ( SI-8):
˜L(rk∣k−1, λ)=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)+α (rk∣k−1 ln rk∣k−1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
+λ (E¯p(x)
k∣k−1
[rk∣k−1]−1)
=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−rk∣k−1¯ck (Xk)+α (rk∣k−1 ln rk∣k−1)+λrk∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎦
−λ.
In particular, by studying the variations of the Lagrangian, we ﬁnd a station ary solution of the problem,
which, since this is convex, is a global minimum. First, we conside r the variation of the Lagrangian with
respect to rk∣k−1. This is given by:
δr ˜L(rk∣k−1, λ)=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−¯ck (Xk)+α (1 +ln rk∣k−1)+λ
⎤
⎥
⎥
⎥
⎥
⎥
⎦
. (SI-26)
Therefore, any candidate optimal solution, say ˜ rk∣k−1, must satisfy:
ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−¯ck (Xk)+α (1 +ln ˜rk∣k−1)+λ =0. (SI-27)
That is,
˜rk∣k−1 =exp (−λ −α
α )
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α ). (SI-28)
Now, the variation of the Lagrangian w.r.t. λ is given by: E¯p(x)
k∣k−1
[rk∣k−1]−1 =0. This condition must hold
for every feasible solution and, hence, in particular for ˜ rk∣k−1 given in ( SI-28). By imposing this stationarity
condition on ˜rk∣k−1 we obtain:
exp (−λ −α
α )= 1
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
(SI-29)
Hence, from (
SI-29) and ( SI-28) we get:
r∗
k∣k−1 =
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
, (SI-30)
which is well deﬁned ∀α >0. Note that, by assumptions A1–A2, r∗
k∣k−1 is bounded. Thus, one implication is
that supp p(x)
k∣k−1 ⊂supp ¯p(x)
k∣k−1. In turn, again from assumption A1 this implies that supp p(x)
k∣k−1 ⊂supp q(x)
k∣k−1
Next, we show that the optimal cost of the problem in (
SI-8) is ﬁnite. To this aim, we note that the
SI-23
optimal cost of the problem equals the value of the Lagrangian evaluated at r∗
k∣k−1. In turn, we have:
˜L(r∗
k∣k−1, λ)=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
r∗
k∣k−1 ln
q(x)
k∣k−1
¯p(x)
k∣k−1
−r∗
k∣k−1¯ck (Xk)+α (r∗
k∣k−1 ln r∗
k∣k−1)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
+λ (E¯p(x)
k∣k−1
[r∗
k∣k−1]−1)
=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ln
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
q(x)
k∣k−1
¯p(x)
k∣k−1
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)exp (¯ck (xk))
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
α
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
−¯ck (Xk)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ln
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
exp (¯ck (xk))
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1
q(x)
k∣k−1
)
1
α
exp (¯ck(xk)
α )
⎤
⎥
⎥
⎥
⎥
⎦
α
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
−¯ck (Xk)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
=−α ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
which is ﬁnite by assumptions A1–A2. By using this last expression into (
SI-9) yields the desired conclusion.
Case (ii): α = 0. We note that M (xk−1, uk) is bounded (assumptions A1–A2) and, when α = 0,
ηk (xk−1, uk)α −Wα (xk−1, uk) becomes
max
rk∣k−1∈R
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+rk∣k−1¯ck (Xk)
⎤
⎥
⎥
⎥
⎥
⎥
⎦
= max
rk∣k−1∈R
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1
⎛
⎜
⎝
ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+¯ck (Xk)
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
, (SI-31)
which we want to prove being equal to M (xk−1, uk). To this aim, note that, by deﬁnition of limit superior,
there exists a sequence x(n)
k ∈supp ¯p(x)
k∣k−1 such that (using the extended notation)
ln
¯pk (x(n)
k ∣xk−1, uk)
qk (x(n)
k ∣xk−1, uk)
exp (¯ck (x(n)
k ))→M (xk−1, uk), and pk (x(n)
k ∣xk−1, uk)→p, as n →∞, (SI-32)
Therefore, for the cost in ( SI-31), we have that, in the feasibility domain of the problem:
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1
⎛
⎜
⎝
ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+¯ck (xk)
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
≤E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1 lim sup ln
⎡
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp(¯ck (xk))
q(x)
k∣k−1
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎥
⎥
⎥
⎥
⎥
⎦
=M (xk−1, uk).
Next, we show that this is indeed the optimal value of the problem. Ind eed, by contradiction, assume that
there exists some ˜M such that
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
rk∣k−1
⎛
⎜
⎝
ln
¯p(x)
k∣k−1
q(x)
k∣k−1
+¯ck (xk)
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
< ˜M <M (xk−1, uk).
Then, by taking the limit superior of the above expression we would h ave M (xk−1, uk)≤ ˜M <M (xk−1, uk),
SI-24
which is a contradiction. Finally, we prove that M (xk−1, uk) is non-negative. Indeed, by contradiction,
assume that M (xk−1, uk)<0. In turn, this means that there exists some β <0 such that
lim sup ln
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
<β <0,
so that, in particular
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
<β.
In turn, this implies that ¯ p(x)
k∣k−1 exp ¯ck (xk)<q(x)
k∣k−1 exp β. By taking integration we get
1 =∫ ¯p(x)
k∣k−1dxk ≤∫ ¯p(x)
k∣k−1 exp ¯ck (xk)dxk ≤exp β,
where the ﬁrst inequality follows from the fact that the cost is non-n egative (following from assumption
A2). The above chain of inequalities yields a contraction as we assumed th at β <0.
Finally, we prove Corollary S3.1, which ultimately states that the policy in Fig. 2b is in fact optimal
and explains how ¯ ck (xk) is built. Since Corollary S3.1 tackles the minimization problem in the policy
space – a problem consistent with prior literature, 19 the proof leverages its key arguments.
Proof of Corollary S3.1. Consider the cost in the robust free energy principle (main text). By the
chain rule for the KL divergence (Lemma S1.1) we can recast the robust free energy principle as the sum
of the following sub-problems, see, e.g.,: 19
min
{π(u)
k∣k−1}
1∶N−1
max
{p(x)
k∣k−1}
1∶N−1
DKL (p0∶N−1 ∣∣q0∶N−1)+
N−1
∑
k=1
Ep(xk−1) [Epk∣k−1 [c(x)
k (Xk)+c(u)
k (Uk)]]
s. t. π (u)
k∣k−1 ∈D, p (x)
k∣k−1 ∈Bη (¯pk (xk ∣xk−1, uk)), ∀k ∈1 ∶N −1,
(SI-33)
and
min
π(u)
N∣N−1
max
p(x)
N∣N−1
Ep(xN−1) [DKL (pN (xN , uN ∣xN−1) ∣∣qN (xN , uN ∣xN−1))+EpN (xN ,uN ∣xN−1) [¯cN (XN )+c(u)
N (UN )]]
s. t. π (u)
N∣N−1 ∈D, p (x)
N∣N−1 ∈Bη (¯pN (xN ∣xN−1, uN )),
(SI-34)
where ¯cN (xN ) = c(x)
N (xN ). Now, following similar arguments to the ones used in Lemma S3.1, we have
that the problem in ( SI-34) can be solved by solving
min
π(u)
N∣N−1
max
pN (xN ∣xN−1,uN )
DKL (pN (xN , uN ∣xN−1) ∣∣qN (xN , uN ∣xN−1))+EpN (xN ,uN ∣xN−1) [¯cN (XN )+c(u)
N (UN )]
s. t. π (u)
N∣N−1 ∈D, p (x)
N∣N−1 ∈Bη (¯pN (xN ∣xN−1, uN )),
(SI-35)
and then by taking the expectation over p(xN−1). To tackle the problem in ( SI-35) we note that the ﬁrst
term in the cost can be written as
EπN (uN ∣xN−1) [DKL (pN (xN ∣xN−1, uN ) ∣∣qN (xN ∣xN−1, uN ))]+DKL (πN (uN ∣xN−1) ∣∣qN (uN ∣xN−1)),
SI-25
and also
EpN∣N−1 [¯cN (XN )+c(u)
N (UN )]=
EπN (uN ∣xN−1) [EpN (xN ∣xN−1,uN ) [¯cN (XN )]]+EπN (uN ∣xN−1) [c(u)
N (UN )].
That is, the cost of the problem in ( SI-35) can be written as:
Eπ(u)
k∣k−1
[DKL (p(x)
N∣N−1 ∣∣q(x)
N∣N−1)+Ep(x)
N∣N−1
[¯cN (XN )]]+DKL (π(u)
N∣N−1 ∣∣q(u)
N∣N−1)+Eπ(u)
N∣N−1
[c(u)
N (UN )].
(SI-36)
This yields a problem of the form of ( SI-3) and hence, by the above results:
• the optimal solution of the problem in ( SI-36) is
π(u),⋆
N∣N−1 =
q(u)
N∣N−1 exp (−ηN (xN−1, uN )−˜c (xN−1, uN )−c(u)
N (uN ))
∫ q(u)
N∣N−1 exp (−ηN (xN−1, uN )−˜c (xN−1, uN )−c(u)
N (uN ))duN
, (SI-37)
with
˜c (xN−1, uN )=min
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
lim sup
xN ∈supp ¯p(x)
N∣N−1
ln
⎛
⎜
⎝
¯p(x)
N∣N−1 exp (¯cN (xN ))
q(x)
N∣N−1
⎞
⎟
⎠
,
min
α>0
ηN (xN−1, uN )α +α ln E¯p(x)
N∣N−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯cN (xN )
q(x)
k∣k−1
⎞
⎟
⎠
1
α ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
;
• the corresponding optimal cost is
ˆcN (xN−1)=−ln ∫ q(u)
N∣N−1 exp (−ηk (xk−1, uk)−˜c (xN−1, uN )−c(u)
N (uN ))duN ,
and therefore the cost for the problem in (
SI-34) is
Ep(xN−1) [ˆcN (XN−1)], (SI-38)
which is also bounded.
The policy and cost in ( SI-37) and ( SI-38) are, respectively, the optimal solution and cost given in the
statement of the result at k = N. Therefore, by using ( SI-38) and ( SI-33) we have that the robust free
energy principle can be written as
min
{π(u)
k∣k−1}
1∶N−1
max
{p(x)
k∣k−1}
1∶N−1
DKL (p0∶N−1 ∣∣q0∶N−1)+
N−1
∑
k=1
Ep(xk−1) [Epk∣k−1 [c(x)
k (Xk)+c(u)
k (Uk)]]
+Ep(xN−1) [ˆcN (XN−1)]
s. t. π (u)
k∣k−1 ∈D, p (x)
k∣k−1 ∈Bη (¯pk (xk ∣xk−1, uk)), ∀k ∈1 ∶N −1.
(SI-39)
On the other hand, we have that
Ep(xN−1) [ˆcN (XN−1)]=Ep(xN−2) [EpN−1∣N−2 [ˆcN (XN−1)]],
SI-26
so that (using again the chain rule for the KL divergence) the cost in ( SI-39) can be written as:
DKL (p0∶N−2 ∣∣q0∶N−2)+
N−2
∑
k=1
Ep(xk−1) [Epk∣k−1 [c(x)
k (Xk)+c(u)
k (Uk)]]
+Ep(xN−2) [DKL (pN−1∣N−2 ∣∣qN−1∣N−2)+EpN−1∣N−2 [¯cN−1 (XN−1)+c(u)
N−1 (UN−1)]],
with
¯cN−1 (xN−1)∶=c(x)
N−1 (xN−1)+ˆcN (xN−1). (SI-40)
The problem at k = N −1 can again be solved independently on the others and ¯ cN−1 (xN−1) is bounded.
Hence, by iterating the above steps one obtains that:
• the optimal solution at k =N −1 is given by
π(u),⋆
N−1∣N−2 =
q(u)
N−1∣N−2 exp (−ηN−1 (xN−2, uN−1)−˜c (xN−2, uN−1)−c(u)
N−1 (uN−1))
∫ q(u)
N−1∣N−2 exp (−ηN−1 (xN−2, uN−1)−˜c (xN−2, uN−1)−c(u)
N−1 (uN−1))duN−1
, (SI-41)
with
˜c (xN−2, uN−1)=min
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
lim sup
xN−1∈supp ¯p(x)
N−1∣N−2
ln
⎛
⎜
⎝
¯p(x)
N−1∣N−2 exp (¯cN−1 (xN−1))
q(x)
N−1∣N−2
⎞
⎟
⎠
,
min
α>0
ηN−1 (xN−2, uN−1)α +α ln E¯p(x)
N−1∣N−2
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯cN−1 (xN−1)
q(x)
k∣k−1
⎞
⎟
⎠
1
α ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
.
This is the optimal policy given in the statement at k =N −1
• the optimal cost for the problem at k =N −1 is bounded and given by
Ep(xN−2) [ˆcN−1 (XN−2)]. (SI-42)
The desired conclusions are then drawn by induction after noticing that, at each k, the problem in the
robust free energy formulation can always be broken down into two sub- problems, with the problem at the
last time-step given by:
min
π(u)
k∣k−1
max
p(x)
k∣k−1
Ep(xk−1) [DKL (pk∣k−1 ∣∣qk∣k−1)+Epk∣k−1 [¯ck (Xk)+c(u)
k (Uk)]]
s. t. π (u)
k∣k−1 ∈D, p (x)
k∣k−1 ∈Bη (¯pk (xk ∣xk−1, uk)),
(SI-43)
with ¯ck (xk) bounded and given, at each k, by the recursion in (
SI-16) and ( SI-17).
S6.3 Computing The Cost Of Ambiguity
Theorem
S3.2 establishes several properties useful to compute ˜ c (xk−1, uk).
SI-27
Proof. To prove part (1) it suﬃces to show that
ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
≥0.
The proof is then by contradiction. Indeed, if we assumed that
ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
<0,
this would imply
∫X
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )¯p(x)
k∣k−1 − ¯p(x)
k∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
dxk <0.
In turn, this would mean that
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )¯p(x)
k∣k−1 − ¯p(x)
k∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
<0, a.e in X
and consequently,
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α
<1 a .e in X (SI-44)
which by taking logarithm implies that
ln
¯p(x)
k∣k−1
q(x)
k∣k−1
<0 a .e in X.
Finally, by multiplying both sides of the above inequality by ¯ p(x)
k∣k−1 and taking the integral we would have:
DKL (¯p(x)
k∣k−1 ∣∣q(x)
k∣k−1)≤0,
so that ¯p(x)
k∣k−1 =q(x)
k∣k−1. However, this contradicts ( SI-44) as by assumption A2 the state cost is positive.
Next we prove part (2) and we start with the case (ii). Note that, for a given function f, we have that
if f(α) is strictly convex for α > 0, then αf(1
α ) is also strictly convex in the same domain. In fact, pick
SI-28
some α, β >0 and t ∈[0, 1]. If the function is strictly convex, we have
(tα +(1 −t)β)f ( 1
tα +(1 −t)β )=(tα +(1 −t)β)[f ( tα
tα +(1 −t)β . 1
α + (1 −t)β
tα +(1 −t)β . 1
β )]
<(tα +(1 −t)β)[ tα
tα +(1 −t)β f (1
α)+ (1 −t)β
tα +(1 −t)β f (1
β )]
=tαf (1
α)+(1 −t)βf (1
β ),
which shows the desired statement. Let
y(α)∶=∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
¯p(x)
k∣k−1dxk
and f(α) ∶= ln ∫ (
¯p(x)
k∣k−1 exp ¯ck(xk)
q(x)
k∣k−1
)
α
¯p(x)
k∣k−1dxk +ηk (xk−1, uk) = ln y(α)+ηk (xk−1, uk). Then, Vα (xk−1, uk) =
αf(1
α ) and we prove the result by showing that f(α) is strictly convex. Now, the function f is diﬀerentiable
and its second derivative w.r.t. α is:
d2
dαf(α)= y
′′
(α)y(α)−(y
′
(α))2
y2(α) ,
where we the shorthand notation y
′
(α) and y
′′
(α) to denote the ﬁrst and second derivative of y w.r.t. α.
Now, a direct calculation yields:
y
′′
(α)y(α)−(y
′
(α))2 =∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
2
¯p(x)
k∣k−1dxk
⋅∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
¯p(x)
k∣k−1dxk
−
⎛
⎜
⎝∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
¯p(x)
k∣k−1dxk
⎞
⎟
⎠
2
.
(SI-45)
Moreover, note that:
⎛
⎜
⎝∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
¯p(x)
k∣k−1dxk
⎞
⎟
⎠
2
=
⎛
⎜
⎜
⎝
∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
2
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
(¯p(x)
k∣k−1)
1
2 .
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
2
(¯p(x)
k∣k−1)
1
2 dxk
⎞
⎟
⎟
⎠
2
<∫
⎛
⎜
⎜
⎝
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
2
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
(¯p(x)
k∣k−1)
1
2
⎞
⎟
⎟
⎠
2
dxk. ∫
⎛
⎜
⎜
⎝
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
2
(¯p(x)
k∣k−1)
1
2
⎞
⎟
⎟
⎠
2
dxk
=∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
⎛
⎜
⎝
ln
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
2
¯p(x)
k∣k−1dxk ∫
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
α
¯p(x)
k∣k−1dxk,
SI-29
where we used Cauchy inequality (which is strict in this case). B y combining the above expression with
(SI-45) we get that y
′′
(α)y(α) −(y
′
(α))2 > 0 and hence d2
dα2 f(α) > 0. That is, Vα (xk−1, uk) is strictly
convex. To prove case (i) it suﬃces to know that, when ln
¯p(x)
k∣k−1 exp ¯ck(xk)
q(x)
k∣k−1
= ¯c, Cauchy inequality holds with
equality.
We now prove part (3) and start with noticing that lim sup α→0 Vα (xk−1, uk) ≤ M (xk−1, uk). In fact,
by deﬁnition of M (xk−1, uk) we get that
Vα (xk−1, uk)≤ln (∫ exp (M (xk−1, uk)
α )¯p(x)
k∣k−1dxk)
α
+αηk (xk−1, uk),
which means that lim sup α→0 Vα (xk−1, uk) ≤ M (xk−1, uk). Next, we show that lim inf α→0 Vα (xk−1, uk) ≥
M (xk−1, uk), and from here we will draw the desired conclusions. To this aim, pi ck any ε > 0 and deﬁne
the set
Xε ∶=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
xk ∈X ∶
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
≥exp(M (xk−1, uk)−ϵ)
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
,
which is non-empty by deﬁnition of M (xk−1, uk). Then, note that
Vα (xk−1, uk)=α ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1
q(x)
k∣k−1
⎞
⎟
⎠
1
α
exp (¯ck (xk)
α )
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
+ηk (xk−1, uk)α
≥ln
⎛
⎜
⎜
⎝
∫Xε
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α
¯p(x)
k∣k−1dxk
⎞
⎟
⎟
⎠
α
+αηk (xk−1, uk)
≥(M (xk−1, uk)−ϵ)+ln (∫Xε
¯p(x)
k∣k−1dxk)
α
+αηk (xk−1, uk),
where to obtain the ﬁrst inequality we used the fact that
∫X
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α
¯p(x)
k∣k−1dxk ≥∫Xε
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α
¯p(x)
k∣k−1dxk.
The above chain of inequalities shows that
lim inf
α→0
Vα (xk−1, uk)≥M (xk−1, uk)−ϵ.
Hence, we have that:
M (xk−1, uk)−ϵ ≤lim inf
α→0
Vα (xk−1, uk)≤lim sup
α→0
Vα (xk−1, uk)≤M (xk−1, uk).
Moreover, since ϵ is arbitrary, by taking the limit ε →0 we get lim inf α→0 Vα (xk−1, uk)=lim supα→0 Vα (xk−1, uk)=
M (xk−1, uk). In turn, this means that lim α→0 Vα (xk−1, uk)=M (xk−1, uk).
The proof of part (4) is by contradiction. Deﬁne the following set
S ={α ≥0; ˜Vα (xk−1, uk)≤M (xk−1, uk)}.
SI-30
We note that S is: (i) non-empty (by deﬁnition of M (xk−1, uk)); (ii) closed, due to (right) continuity
of the function; (iii) bounded (indeed, if S was unbounded, then it would have been possible to ﬁnd
an unbounded sequence, αn → ∞, such that αn ∈ S and therefore ˜Vα (xk−1, uk) ≤ M (xk−1, uk) and this
contradicts part (1) of the statement). Now, since ˜Vα (xk−1, uk) is continuous at 0 (by part (3)), then it has
a minimum over the compact and bounded set S, i.e., there exists α⋆ ∈S such that min α∈S ˜Vα (xk−1, uk)=
˜Vα⋆(xk−1, uk). We want to show that this is indeed a global minimum over the set [0, ∞). To prove
this, assume by contradiction that there exists some β > 0 that does not belong to S and such that
˜Vβ(xk−1, uk) < ˜Vα⋆(xk−1, uk). In turn, this would imply that ˜Vβ(xk−1, uk) < ˜Vα⋆(xk−1, uk) ≤ M (xk−1, uk).
That is, β ∈S. However, this is a contradiction because β was assumed to not belong to S.
S6.4 Determining The Role Of Ambiguity
Theorem
S4.1 establishes how ˜c (xk−1, uk) changes when the radius of ambiguity shrinks. This result is
instrumental to determine if an agent aﬀected by ambiguity can outperf orm an ideal, ambiguity-free, agent.
Proof of Theorem S4.1. We use the formulation given in the problem in ( SI-25) and let
ϕ(θ)∶=ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (Xk)
q(x)
k∣k−1
⎞
⎟
⎠
θ⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
In what follows we use the shorthand notation ϕ(k)(θ) to denote the k-th order derivative of ϕ(θ). By
deﬁnition of the optimal solution of the problem in (
SI-25), we have the following chain of identities:
η =E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎦
ln
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎦
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
=
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆
ln (
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
⎤
⎥
⎥
⎥
⎥
⎦
α⋆E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎦
−ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (Xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Hence, by letting θ = 1
α⋆ , this yields
η =θϕ(1)(θ)−ϕ(θ), (SI-46)
SI-31
Also, for the optimal value of the problem in ( SI-25) we have
˜c (xk−1, uk)=E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎦
ln
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (Xk)
q(x)
k∣k−1
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
=
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆
ln (
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
⎤
⎥
⎥
⎥
⎥
⎦
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎣
(
¯p(x)
k∣k−1 exp ¯ck(Xk)
q(x)
k∣k−1
)
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎦
=ϕ(1)(θ).
Next, we prove the result by: (i) showing that the equation in (
SI-46) has a solution, say θ⋆; (ii) obtaining
an expression for ˜ c (xk−1, uk) by computing ϕ(1)(θ⋆) and expressing it as a function of η.
Finding the root of (SI-46). First, we show that a solution exists and is unique. To this sim, si nce ϕ is
continuous and diﬀerentiable, we Taylor expand the right-hand side i n ( SI-46) around θ =0. This yields:
θϕ(1)(θ)−ϕ(θ)=
∞
∑
m=0
1
m!ϕ(m+1)(0)θm+1 −
∞
∑
m=0
1
m!ϕ(m)(0)θm =
∞
∑
m=0
1
m!νm+1θm+1 −
∞
∑
m=0
1
m!νmθm
=
∞
∑
m=1
[ 1
(m −1)! − 1
m!]νmθm =
∞
∑
m=2
1
m(m −2)!νmθm = 1
2ν2θ2 + 1
3ν3θ3 +O(θ4),
where we used the shorthand notation νm to denote ϕ(m)(0). Moreover, following similar arguments as
those used to show ( SI-45), we have that ν2 = ϕ(2)(0) > 0. Thus, for small enough η, there exists a root
for ( SI-46) and, since ϕ is strictly convex by assumption, the root θ⋆ is unique. Moreover, the function
θϕ(1)(θ)−ϕ(θ) is strictly increasing ( d
dθ [θϕ(1)(θ)−ϕ(θ)]=θϕ(2)(θ) >0 for θ >0) and hence we can invert
(SI-46). In particular, we have:
η = 1
2ν2θ⋆2 + 1
3ν3θ⋆3 + 1
8ν4θ⋆4 +O(θ⋆5)= 1
2θ⋆2ν2 [1 + 2ν3
3ν2
θ⋆ + ν4
ν2
θ⋆2 +O(θ⋆3)],
so that
θ⋆ =
√
2η
ν2
[1 +(2ν3
3ν2
θ⋆ + ν4
ν2
θ⋆2 +O(θ⋆3))]
−1
2
=
√
2η
ν2
[1 − ν3
3ν2
θ⋆ +O(θ⋆2)], (SI-47)
where we used the binomial series expansion for a negative fractional po wer. Here, we used the identity
(1 +(a1x +a2x2 +O(x3)))
−1
2 =1 − 1
2a1x +O(x2).
Next, disregarding the higher order terms in θ⋆ we have
θ∗ ≈
√
2η
ν2
, (SI-48)
SI-32
which, together with ( SI-47), yields
θ⋆ =
√
2η
ν2
[1 − ν3
3ν2
θ⋆ +O(θ⋆2)]=
√
2
ν2
η
1
2 − 2ν3
3ν2
2
η +O(η
3
2 ). (SI-49)
We can now compute ˜ c (xk−1, uk).
Computing ˜c (xk−1, uk). By using the above expression, we have, by letting A(x)∶=ln
¯p(x)
k∣k−1 exp ¯ck(xk)
q(x)
k∣k−1
:
˜c (xk−1, uk)=ϕ(1)(θ∗)=
∞
∑
k=0
νk+1
k! θ∗k =ν1 +ν2θ∗ + ν3
2 θ∗2 +O(θ∗3)
=ν1 +ν2
⎛
⎝
√
2
ν2
η
1
2 − 2ν3
3ν2
2
η +O(η
3
2 )⎞
⎠+ ν3
2 ( 2
ν2
η +O(η
3
2 ))+O(η
3
2 )
=ν1 +
√
2ν2η
1
2 + ν3
3ν2
η +O(η
3
2 )
=E¯p(x)
k∣k−1
[A(x)]+O(η
1
2 ),
where we used the deﬁnition of ν1. The above expression gives the desired conclusion, as
lim
η→0
˜c (xk−1, uk)=E¯p(x)
k∣k−1
[A(x)]=DKL (¯p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+E¯p(x)
k∣k−1
[¯ck (Xk)].
We can now give the proof of Lemma S4.1, which establishes that, for a free energy minimizing agent,
ambiguity cannot be exploited to achieve better performance.
Proof of Lemma
S4.1. By breaking down ( SI-20), we need to show that
DKL (¯p(x)
k∣k−1 ∣∣q(x)
k∣k−1)+E¯p(x)
k∣k−1
[¯ck (Xk)]<ηk (xk−1, uk)+˜c (xk−1, uk).
That is, by exploiting the deﬁnition of the KL divergence
E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
ln
¯p(x)
k∣k−1 exp c(x)
k (xk)
q(x)
k∣k−1
⎤
⎥
⎥
⎥
⎥
⎥
⎦
<ηk (xk−1, uk)+˜c (xk−1, uk). (SI-50)
Assuming on the contrary that the above equation does not hold, we then ha ve ηk (xk−1, uk)+˜c (xk−1, uk)≤
∫X ln
¯p(x)
k∣k−1 exp c(x)
k (xk)
q(x)
k∣k−1
¯p(x)
k∣k−1dxk, this leads to
˜c (xk−1, uk)<∫X
ln
¯p(x)
k∣k−1 exp c(x)
k (xk)
q(x)
k∣k−1
¯p(x)
k∣k−1dxk. (SI-51)
Next, we will analyze two cases,
SI-33
Case (1): if ˜ c (xk−1, uk)=M (xk−1, uk), by deﬁnition of M (xk−1, uk), we get
˜c (xk−1, uk)<∫X
ln
¯p(x)
k∣k−1 exp c(x)
k (xk)
q(x)
k∣k−1
¯p(x)
k∣k−1dxk ≤∫X
M (xk−1, uk)¯p(x)
k∣k−1dxk =M (xk−1, uk)
which is a contradiction.
Case (2): in this case, we have that there exists some α⋆ >0 such that
˜c (xk−1, uk)=ηk (xk−1, uk)α⋆ +α⋆ ln E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
1
α⋆ ⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Then, employing Jensen inequality, it follows that:
˜c (xk−1, uk)≥ηk (xk−1, uk)α⋆ +E¯p(x)
k∣k−1
⎡
⎢
⎢
⎢
⎢
⎢
⎣
ln
⎛
⎜
⎝
¯p(x)
k∣k−1 exp ¯ck (xk)
q(x)
k∣k−1
⎞
⎟
⎠
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
This however contradicts (
SI-51) since α⋆ >0 and ηk (xk−1, uk)>0.
S7 Concluding Interdisciplinary Remarks And Potential Implications
across Psychology, Economics and Neuroscience
Ambiguity is a key theme in both psychology and economics. 45–47 The large literature in this area speaks to
a key dialectic: on the one hand, psychological studies suggest that peopl e prefer certainty over uncertainty.
On the other hand, the opportunity to resolve uncertainty underwri tes cognitive ﬂexibility information and
novelty-seeking behaviour.35, 37, 40, 41, 48, 52, 53 In active inference, this is often described in terms of intrins ic
motivation or epistemic aﬀordance. 37, 44, 50 The paradigms used to study ambiguity generally leverage the
exploration-exploitation dilemma: 36, 38, 39, 48, 50 e.g., using the two-step maze task or multiarmed bandit
problems. An empirical evaluation of active inference formulations is a vailable in the literature. 43 In
behavioural economics, ambiguity refers to uncertain outcomes with u ncertain probabilities in contrast
with risk that refers to uncertain outcomes with certain probabilit ies. Similarly, ambiguity in this work
refers to uncertainty about the environment (i.e., world) model – as opposed to the risk entailed by
uncertainty about outcomes under a given model.
The neural correlates of ambiguity and risk – as assessed using EEG and mu ltiarmed bandit tasks 54
– appear to segregate; with activity in the frontal, central and parietal r egions reﬂecting ambiguity; with
activity in frontal and central brain regions reﬂecting risk. In econom ic decision making using fMRI,
ambiguous cues elicit activity in the frontal and parietal cortex durin g outcome anticipation. 34 The authors
suggest that these regions subserve a general function of contextual analy sis that reﬂects situational or
ambiguity awareness.
In this broad context, DR-FREE can potentially oﬀer a formal model of ambi guity awareness that
can be leveraged using computational phenotyping. 42, 49 In principle, it is possible to estimate ambiguity
awareness – e.g., in terms of the ambiguity radius – by estimating the radius that best explains a given
subject’s responses. For example, computational phenotyping of this sort has been used to characterise
psychiatric cohorts in terms of greater decision uncertainty during approach-avoidance conﬂict. 51
SI-34
Hyperparameter Figure 4.c SF 4.a top SF 4.a bottom SF 4.b top SF 4.b bottom
Model layer N/A N/A N/A 128×128 128×128
Discount factor ( γ) 0.95 0.95 0.95 0.95 0.95
Learning rate N/A N/A N/A 0.0005 0.0005
Batch size N/A N/A N/A 128 128
α 0.1 0.1 0.1 0.1 0.1
λ 0.5 0.5 0.5 0.5 0.5
Horizon 2 20 50 2 20
Samples 50 50 100 50 50
Table S-1: MaxDiﬀ Hyperparameters across experiments. SF stands for Supplemen tary Figure. Model layer refers to the architecture of the neural
network model used to approximate dynamics and reward. The notation x ×y means that there are two layers – the width of the ﬁrst layer is of x
neurons and the width of the second layer is of y neurons. Discount (γ in the text) tunes the importance of future rewards in planning. Learning rate
is used in gradient-based optimization for updating the dynamics/rewar d model parameters. Batch size determines the number of training data points
used per update step in dynamics/reward model training. The paramete r α in the text is the temperature-like parameter in MaxDiﬀ’s objective.
The parameter λ is used to scale the softmax-like weighting of trajectory costs. It c ontrols how sharply or smoothly the algorithm diﬀerentiates between
high-cost and low-cost trajectories during action selection. Horizon sets the planning horizon for policy roll-outs. Samples indicates the number of
trajectory samples used for expectation estimation during policy rol l-outs. We refer to the original MaxDiﬀ code for a more detailed explanat ion of each
of the hyperparameters. The values used for the experiments of this paper are set in accordance with the code from the Maxdiﬀ repository.
Hyperparameter Value
Discount factor 0.99
Stopping criterion 10−5
Maximum iteration 2000
Gradient stopping criterion 0.001
Learning rate 1
1+k
Table S-2: Hyperparameter settings for the belief update benchmark. Discount factor determines the trade-oﬀ between immediate and long-term
returns in soft-value iteration. Stopping Criterion speciﬁes the convergence threshold for soft-value iteration. Maximum iteration is the maximum
number of iterations allowed in the soft-value iteration optimisation l oop. Gradient stopping criterion is the stopping criterion for gradient descent
optimisation. Learning rate of the gradient descent decays linearly with rate from 1 ( k is the learning step). All parameters are deﬁned in our
repository.
SI-35
Method Average Computation time (sec)
DR-FREE 0.22 (763 steps)
Ambiguity Unaware 0.04 (556 steps)
MaxDiﬀ 0.02 (637 steps)
Table S-3: average time required to output an action at each k for all the policy computation methods considered in the paper. The me asurements
were obtained as described in the Methods (Experiments settings section). The recordings were obtained from the Robotarium hardware exp eriments
– in parentheses the number of steps for each experiment (Ambiguity Unaware has a lower number of steps because the robot encountered an obst acle
before reaching the destination). In DR-FREE and ambiguity unaware age nt q0∶N is the one from the ﬁrst section in the Results – also, the number of
samples used in line 2 of Algorithm 1 was set to 50, consistently with the MaxDiﬀ settings. In fact, for the M axDiﬀ agent, the planning horizon was set
to 20, and the number of sampled trajectories was also set to 50. This was a c onﬁguration where MaxDiﬀ would consistently complete the task.
Hyperparameter MaxDiﬀ NN-MPPI
Model layer 512 ×512 ×512 512 ×512 ×512
Discount factor ( γ) 0.95 0.95
α 5 N/A
λ 0.5 0.5
Samples 1000 1000
Table S-4: Relevant MaxDiﬀ and NN-MPPI hyperparameters for the Ant experiments. Deﬁnitions are as in Tab. S-1 and parameters are as in the
literature.5 The network, used in all our Ant experiments, is from the original MaxDi ﬀ repository: it outputs mean and variance of ¯ pk (xk ∣xk−1, uk).
SI-36
Supplementary References
[1] Zhengyuan Zhou, Michael Bloem, and Nicholas Bambos. Inﬁnite Time Horiz on Maximum Causal
Entropy Inverse Reinforcement Learning. IEEE Transactions on Automatic Control , 63(9):2787–2802,
September 2018.
[2] S. Kullback and R. Leibler. On information and suﬃciency. Annals of Mathematical Statistics , 22:79–87,
1951.
[3] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommu-
nications and Signal Processing) . Wiley-Interscience, USA, 2006.
[4] Francis J . Pinski, Gideon Simpson, Andrew M . Stuart, and Hendrik W eber. Kullback–Leibler ap-
proximation for probability measures on inﬁnite dimensional spaces. SIAM Journal on Mathematical
Analysis, 47(6):4091–4122, November 2015.
[5] Thomas A. Berrueta, Allison Pinosky, and Todd D. Murphey. Maximum di ﬀusion reinforcement learn-
ing. Nature Machine Intelligence , 6(5):504–514, May 2024.
[6] Thomas Berrueta. Robot Thermodynamics. Ph.d. dissertation, Nortwestern University, December 2024.
Available online at
https://www.proquest.com/openview/faffd739b9b7a1becbd5e99b0fbd83fe/.
[7] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforceme nt learning with deep
energy-based policies. In Proceedings of the 34th International Conference on Machine Lear ning -
Volume 70 , ICML’17, 1352–1361, 2017.
[8] Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling int eraction via the principle
of maximum causal entropy. In Proceedings of the 27th International Conference on Internation al
Conference on Machine Learning , ICML’10, 1255–1262, Madison, WI, USA, 2010. Omnipress.
[9] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provabl y) solves some robust RL
problems. In International Conference on Learning Representations , 2022.
[10] Mohammad A. Bashiri, Brian Ziebart, and Xinhua Zhang. Distributionally robust imitation learning.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, e ditors, Advances
in Neural Information Processing Systems , 34, 24404–24417. Curran Associates, Inc., 2021.
[11] Hyuk Park, Duo Zhou, Grani A. Hanasusanto, and Takashi Tanaka. Distributional ly robust path
integral control. In 2024 American Control Conference (ACC) , pages 1164–1171, 2024.
[12] Charalambos D. Charalambous and Farzad Rezaei. Stochastic uncertain sys tems subject to relative
entropy constraints: Induced norms and monotonicity properties of min imax games. IEEE Transactions
on Automatic Control , 52(4):647–663, April 2007.
[13] Bart P. G. Van Parys, Daniel Kuhn, Paul J. Goulart, and Manfred Morari. D istributionally robust
control of constrained stochastic systems. IEEE Transactions on Automatic Control , 61(2):430–442,
February 2016.
[14] Bohan Wu, Bennett Zhu, and David M Blei. Distributionally robuss t posterior sampling – a variational
Bayes approach. In Frontiers of Probabilistic Inference workshop at ICLR 2025 , 2025.
SI-37
[15] Herbert E. Scarf. A Min-Max Solution of an Inventory Problem . RAND Corporation, Santa Monica,
CA, 1957.
[16] Shengbo Wang, Nian Si, Jose Blanchet, and Zhengyuan Zhou. On the foundati on of distributionally
robust reinforcement learning, 2024.
[17] Robert D. McAllister and Peyman M. Esfahani. Distributionally robu st model predictive control:
Closed-loop guarantees and scalable algorithms. IEEE Transactions on Automatic Control , 70(5):2963–
2978, May 2025.
[18] Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever , and Jan Peters. Robust
reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge
Extraction, 4(1):276–315, March 2022.
[19] Emiland Garrabe, Hozefa Jesawada, Carmen Del Vecchio, and Giovanni Ru sso. On convex data-driven
inverse optimal control for nonlinear, non-stationary and stochastic sys tems. Automatica, 173:112015,
March 2025.
[20] Bahar Taskesen, Dan Iancu, C ¸ a˘ gı l Ko¸ cyi˘ git, and Daniel Kuhn. Distributionally robust linear quadratic
control. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances
in Neural Information Processing Systems , 36, 18613–18632. Curran Associates, Inc., 2023.
[21] Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and Zhengyuan Zhou.
Distributionally robust Q-learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepe s-
vari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine
Learning, volume 162 of Proceedings of Machine Learning Research , 13623–13643, July 2022.
[22] Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause. Distributionally robust
bayesian optimization. In Silvia Chiappa and Roberto Calandra, editors , Proceedings of the Twenty
Third International Conference on Artiﬁcial Intelligence and Stat istics, volume 108 of Proceedings of
Machine Learning Research , 2174–2184, August 2020.
[23] Alexander Shapiro, Enlu Zhou, and Yifan Lin. Bayesian distributionall y robust optimization. SIAM
Journal on Optimization , 33(2):1279–1304, June 2023.
[24] Mohammed Rayyan Sheriﬀ and Peyman Mohajerin Esfahani. Nonlinear dist ributionally robust opti-
mization. Mathematical Programming, December 2024.
[25] Bart PG Van Parys, Peyman M. Esfahani, and Daniel Kuhn. From data to dec isions: Distributionally
robust optimization is optimal. Management Science , 67(6):3387–3402, November 2020.
[26] John C. Duchi, Peter W. Glynn, and Hongseok Namkoong. Statistics of robus t optimization: A
generalized empirical likelihood approach. Mathematics of Operations Research , 46(3):946–969, August
2021.
[27] Fengming Lin, Xiaolei Fang, and Zheming Gao. Distributionally robust opt imization: A review on
theory and applications. Numerical Algebra, Control & Optimization , 12(1):159, March 2022.
[28] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distrib utionally robust optimiza-
tion. Open Journal of Mathematical Optimization , 3:1–85, July 2022.
SI-38
[29] Zhaolin Hu and L J. Hong. Kullback-Leibler divergence constrained dist ributionally robust optimiza-
tion. Available at Optimization Online , 1(2):9, November 2013.
[30] Huan Xu and Shie Mannor. Distributionally Robust Markov Decision Proc esses. In J. Laﬀerty,
C. Williams, J. Shawe-Taylor, R. Zemel and A. Culotta, editors, Advances in Neural Information
Processing Systems, 23. Curran Associates, Inc., 2010.
[31] Constantin P. Niculescu and Lars-Erik Persson. Convex Functions and Their Applications . Springer
New York, 2006.
[32] David G. Luenberger. Optimization by Vector Space Methods . John Wiley & Sons, Inc., USA, 1st
edition, 1997.
[33] Walter Rudin. Real and complex analysis . McGraw-Hill International Editions, 1987
[34] Dominik R. Bach, Ben Seymour, and Raymond J. Dolan. Neural activity ass ociated with the passive
prediction of ambiguity and risk for aversive events. Journal of Neuroscience , 29:1648–1656, February
2009.
[35] Daniel E. Berlyne. Novelty and curiosity as determinants of explan atory behaviour. British Journal
of Psychology-General Section , 41:68–80, May 1950.
[36] Jonathan D. Cohen, Samuel M. McClure, and Angela J. Yu. Should I stay or should I go? How the
human brain manages the trade-oﬀ between exploitation and exploration. Philosophical Transactions
of the Royal Society B: Biological Sciences , 362:933–942, March 2007.
[37] Andrew W. Corcoran, Giovanni Pezzulo, and Jakob Hohwy. From allostatic agent s to counterfactual
cognisers: active inference, biological regulation, and the origins of cogni tion. Biology & Philosophy ,
35:32, April 2020.
[38] Nathaniel D. Daw, John P. O’Doherty, Peter Dayan, Ben Seymour, and R aymond J. Dolan. Cortical
substrates for exploratory decisions in humans. Nature, 441:876–879, June 2006.
[39] Shin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploit ation-exploration meta-
parameter in reinforcement learning. Neural Networks , 15:665–687, July 2002.
[40] Julian Kiverstein, Marius Miller, and Erik Rietveld. The fee ling of grip: novelty, error dynamics, and
the predictive brain. Synthese, 196, October 2017.
[41] Romy M. Krebs, Bj¨ orn H. Schott, Henrik Schutze, and Emrah Duzel. T he novelty exploration bonus
and its attentional modulation. Neuropsychologia, 47:2272–2281, September 2009.
[42] Dragutin Markovi´ c, Annalina M.F. Reiter, and Stefan J. Kiebel. R evealing human sensitivity to a
latent temporal structure of changes. Frontiers in Behavioral Neuroscience , 16:962494, October 2022.
[43] Dragutin Markovi´ c, Hrvoje Stoji´ c, Sven Schw¨ obel, and Stefan J . Kiebel. An empirical evaluation of
active inference in multi-armed bandits. Neural Networks , 144:229–246, December 2021.
[44] Pierre-Yves Oudeyer and Fr´ ed´ eric Kaplan. What is intrinsic m otivation? A typology of computational
approaches. Frontiers in Neurorobotics , 1:6, November 2007.
SI-39
[45] Joel M. Pearson, Kristina K. Watson, and Michael L. Platt. Decision m aking: the neuroethological
turn. Neuron, 82:950–965, June 2014.
[46] Ming Hsu, Meghana Bhatt, Ralph Adolphs, Daniel Tranel, and Colin F. Came rer. Neural systems
responding to degrees of uncertainty in human decision-making. Science, 310:1680–1683, December
2005.
[47] Paul J. Zak. Neuroeconomics. Philosophical Transactions of the Royal Society B: Biological S ciences,
359:1737–1748, November 2004.
[48] Philipp Schwartenbeck, Thomas Fitzgerald, Raymond J. Dolan, and Kar l Friston. Exploration, nov-
elty, surprise, and free energy minimization. Frontiers in Psychology , 4:710, October 2013.
[49] Philipp Schwartenbeck and Karl Friston. Computational phenotypin g in psychiatry: A worked exam-
ple. eNeuro, 3:0049-0016.2016, August 2016.
[50] Philipp Schwartenbeck, Johannes Passecker, Tobias U. Hauser, Thomas H. FitzGerald, Martin Kro-
nbichler, and Karl J. Friston. Computational mechanisms of curiosity an d goal-directed exploration.
eLife, 8:e41703, May 2019.
[51] Ryan Smith, Natasa Kirlic, Jennifer L. Stewart, Joni Touthang, Robert Kuplicki, Sahib S. Khalsa,
Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. Greater d ecision uncertainty character-
izes a transdiagnostic patient sample during approach-avoidance conﬂic t: a computational modelling
approach. Journal of Psychiatry & Neuroscience , 46:E74–E87, January 2021.
[52] Bianca C. Wittmann, Nathaniel D. Daw, Ben Seymour, and Raymond J. Dol an. Striatal activity
underlies novelty-based choice in humans. Neuron, 58:967–973, June 2008. Philosophical Transactions
of the Royal Society B: Biological Sciences , 359:1737–1748, 2004.
[53] Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or surp rise? Frontiers in Psychology,
4:907, December 2013.
[54] Shiyu Zhang, Yuchen Tian, Qiang Liu, and Hao Wu. The neural correlates of novelty and variability in
human decision-making under an active inference framework. eLife Sciences Publications, Ltd , February
2025.
SI-40