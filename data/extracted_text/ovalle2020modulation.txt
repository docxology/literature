Modulation of viability signals for self-regulatory
control
Alvaro Ovalle and Simon M. Lucas
Queen Mary University of London. London, UK
{a.ovalle,simon.lucas}@qmul.ac.uk
Abstract. Werevisittheroleofinstrumentalvalueasadriverofadap-
tivebehavior.Inactiveinference,instrumentalorextrinsicvalueisquan-
tifiedbytheinformation-theoreticsurprisal ofasetofobservationsmea-
suringtheextenttowhichthoseobservationsconformtopriorbeliefsor
preferences. That is, an agent is expected to seek the type of evidence
that is consistent with its own model of the world. For reinforcement
learning tasks, the distribution of preferences replaces the notion of re-
ward. We explore a scenario in which the agent learns this distribution
in a self-supervised manner. In particular, we highlight the distinction
between observations induced by the environment and those pertaining
more directly to the continuity of an agent in time. We evaluate our
methodology in a dynamic environment with discrete time and actions.
Firstwithasurprisalminimizingmodel-freeagent(intheRLsense)and
then expanding to the model-based case to minimize the expected free
energy.
Keywords: perception-action loop · active inference · reinforcement
learning · self-regulation · anticipatory systems · instrumental value.
1 Introduction
The continual interaction that exists between an organism and the environment
requiresanactiveformofregulationofthemechanismssafeguardingitsintegrity.
Thereareseveralaspectsanagentmustconsider,rangingfromassessingvarious
sources of information to anticipating changes in its surroundings. In order to
decide what to do, an agent must consider between different courses of action
andfactorinthepotentialcostsandbenefitsderivedfromitshypotheticalfuture
behavior. This process of selection among different value-based choices can be
formallydescribedasanoptimizationproblem.Dependingontheformalism,the
cost or utility functions optimized by the agent presuppose different normative
interpretations.
In reinforcement learning (RL) for instance, an agent has to maximize the
expectedrewardguidedbyasignalprovidedexternallybytheenvironmentinan
oracularfashion.Therewardinsomecasesisalsocomplementedwithanintrin-
sic contribution, generally corresponding to an epistemic deficiency within the
agent. For example prediction error [24], novelty [3,5,23] or ensemble disagree-
ment[25].Itisimportanttonotethatincorporatingthesesurrogaterewardsinto
0202
tcO
31
]CN.oib-q[
2v79290.7002:viXra
2 A. Ovalle and S. M. Lucas
the objectives of an agent is often regarded as one of many possible enhance-
mentstoincreaseitsperformance,ratherthanbeenmotivatedbyaconcernwith
explaining the roots of goal-directed behavior.
In active inference [14], the optimization is framed in terms of the minimiza-
tion of the variational free energy to try to reduce the difference between sensa-
tions and predictions. Instead of rewards, the agent holds a prior over preferred
future outcomes, thus an agent minimizing its free energy acts to maximize the
occurrence of these preferences and to minimize its own surprisal. Value arises
notasanexternalpropertyoftheenvironment,butinsteaditisconferredbythe
agent as a contextual consequence of the interplay of its current configuration
and the interpretation of stimuli.
There are recent studies that have successfully demonstrated how to refor-
mulate RL and control tasks under the active inference framework. While for
living processes it is reasonable to assume that the priors emerge and are re-
fined over evolutionary scales and during a lifetime, translating this view into
a detailed algorithmic characterization raises important considerations because
there is no evolutionary prior to draw from. Thus the approaches to specify a
distribution of preferences have included for instance, taking the reward an RL
agent would receive and encoding it as the prior [16,21,29,32,33,34], connecting
it to task objectives [29] or through expert demonstrations [6,7,30].
In principle this would suggest that much of the effort that goes into reward
engineering in RL is relocated to that of specifying preferred outcomes or to
the definition of a phase space. Nonetheless active inference provides important
conceptual adjustments that could potentially facilitate conceiving more princi-
pled schemes towards a theory of agents that could provide a richer account of
autonomous behavior and self-generation of goals, desires or preferences. These
include the formulation of objectives and utilities under a common language re-
sidinginbeliefspace,andappealingtoaworldviewinwhichutilityisnottreated
asindependentordetachedfromtheagent.Inparticularthelattercouldencour-
age a more organismic perspective of the agent in terms of the perturbations it
must endure and the behavioral policies it attains to maintain its integrity [11].
Hereweexplorethisdirectionbyconsideringhowasignalacquiresfunctional
significanceastheagentidentifiesitasaconditionnecessaryforitsviabilityand
future continuity in the environment. Mandated by an imperative to minimize
surprisal,theagentlearnstoassociatesensorimotoreventstospecificoutcomes.
First,westartbyintroducingthesurpriseminimizingRL(SMiRL)specification
[4] before we proceed with a brief overview of the expected free energy. Then
we motivate our approach from the perspective of a self-regulatory organism.
Finally,wepresentresultsfromourcasestudyandclosewithsomeobservations
and further potential directions.
Modulation of viability signals for self-regulatory control 3
2 Preliminaries
2.1 Model-free surprisal minimization
Consider an environment whose generative process produces a state s ∈ S at
t
each time step t resulting in an agent observing o ∈ O. The agent acts on the
t
environmentwitha ∈Aaccordingtoapolicyπ,obtainingthenextobservation
t
o .Supposetheagentperformsdensityestimationonthelastt−kobservations
t+1
to obtain a current set of parameter(s) θ summarizing p (o). As these sufficient
t θ
statistics contain information about the agent-environment coupling, they are
concatenatedwiththeobservationsintoanaugmentedstatex =(o ,θ ).Every
t t t
timestep,theagentcomputesthesurprisalgeneratedbyanewobservationgiven
its current estimate and then updates it accordingly. In order to minimize sur-
prisalunderthismodel-freeRLsetting,theagentshouldmaximizetheexpected
log of the model evidence E[ (cid:80) γtlnp (o )] [4]. Alternatively, we maintain con-
t θt t
sistency with active inference by expressing the optimal surprisal Q-function
as,
Q (x,a)=E [−lnp (o)+γminQ (x(cid:48),a(cid:48))] (1)
π∗ π θ π∗
a(cid:48)
estimated via DQN [22] or any function approximator with parameters φ
such that Q (x,a)≈Q(x,a;φ).
π∗
2.2 Expected free energy
The free energy principle (FEP) [15] has evolved from an account of message
passing in the brain to propose a probabilistic interpretation of self-organizing
phenomena[13,27,28].CentraltocurrentdiscoursearoundtheFEPisthenotion
oftheMarkovblankettodescribeacausalseparationbetweentheinternalstates
of a system from external states, as well as the interfacing blanket states (i.e.
sensory and active states). The FEP advances the view that a system remains
farfromequilibriumbymaintainingalowentropydistributionoverthestatesit
occupies during its lifetime. Accordingly, the system attempts to minimize the
surprisal of an event at a particular point in time.
Thiscanbemoreconcretelyspecifiedifweconsideradistributionp(o)encod-
ing the states, drives or desires the system should fulfil. Thus the system strives
to obtain an outcome o that minimizes the surprisal −lnp(o). Alternatively,
we can also state this as the agent maximizing its model evidence or marginal
likelihood p(o). For most cases estimating the actual marginal is intractable,
therefore a system instead minimizes the free energy [10,18] which provides an
upper bound on the log marginal [19],
F=E [lnq(s)−lnp(o,s)] (2)
q(s)
where p(o,s) is the generative model and q(s) the variational density ap-
proximating hidden causes. Equation 2 is used to compute a static form of free
energyandinferhiddencausesgivenasetofobservations.Howeverifweinstead
4 A. Ovalle and S. M. Lucas
consider an agent that acts over an extended temporal dimension, it must infer
and select policies that minimize the expected free energy (EFE) G [14] of a
policy π for a future step τ >t. This can be expressed as,
G(π,τ)=E [lnq(s |π)−lnp(o ,s |π)] (3)
q(oτ,sτ|π) τ τ τ
where p(o ,s |π) = q(s |o ,π)p(o ) is the generative model of the future.
τ τ τ τ τ
Rearranging G as,
G(π,τ)=−E [lnp(o )]−E (cid:2) D [lnq(s |o ,π)||lnq(s |π)] (cid:3) (4)
q(oτ|π) τ q(oτ|π) KL τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
instrumental value epistemic value
which illustrates how the EFE entails a pragmatic, instrumental or goal-
seeking term that realizes preferences and an epistemic or information seek-
ing term that resolves uncertainty. An agent selects a policy with probability
(cid:80)
q(π) = σ(−β G (π)) where σ is the softmax function and β is the inverse
τ τ
temperature.Insummary,anagentminimizesitsfreeenergyviaactiveinference
by changing its beliefs about the world or by sampling the regions of the space
that conforms to its beliefs.
3 Adaptive control via self-regulation
The concept of homeostasis has played a crucial role in our understanding of
physiological regulation. It describes the capacity of a system to maintain its
internal variables within certain bounds. Recent developments in the FEP de-
scribing the behavior of self-organizing systems under the framework, can be
interpreted as an attempt to provide a formalization of this concept [28]. From
this point of view, homeostatic control in an organism refers to the actions nec-
essarytominimizethesurprisalofthevaluesreportedbyinteroceptivechannels,
constraining them to those favored by a viable set of states. Something that is
less well understood is how these attracting states come into existence. That is,
how do they emerge from the particular conditions surrounding the system and
how are they discovered among the potential space of signals.
Recently, it has been shown that complex behavior may arise by minimizing
surprisal in observation space (i.e. sensory states) without pre-encoded fixed
priordistributionsinlargestatespaces[4].Hereweconsideranalternativeangle
intendedtoremainclosertothehomeostaticcharacterizationofasystem.Inour
scenario,weassumethatgiventheparticulardynamicsofanenvironment,ifan
agentisequippedonlywithabasicdensityestimationcapacity,thenstructuring
itsbehavioraroundthetypeofregularitiesinobservationspacethatcansustain
itintimewillbedifficult.Inthesesituationswithfastchangingdynamics,rather
thanminimizingfreeenergyoversensorysignals,theagentmayinsteadleverage
them to maintain a low future surprisal of another target variable. That implies
thatalthoughtheagentmayhaveinprincipleaccesstomultiplesignalsitmight
be interested in maintaining only some of them within certain expected range.
Modulation of viability signals for self-regulatory control 5
Defining what should constitute the artificial physiology in simulated agents
is not well established. Therefore we assume the introduction of an information
channel representing in abstract terms the interoceptive signals that inform the
agentaboutitscontinuityintheenvironment.Wecandrawarudimentarycom-
parison,andthinkofthisvalueinasimilarwayinwhichfeelingsagglutinateand
coarse-grain the changes of several internal physical responses [9]. In addition,
we are interested in the agent learning to determine whether it is conductive to
its self-preservation in the environment or not.
3.1 Case Study
We assess the behavior of an agent in the Flappy Bird environment (fig. 1 left).
This is a task where a bird must navigate between obstacles (pipes) at differ-
ent positions while stabilizing its flight. Despite the apparent simplicity, the
environment offers a fundamental aspect present in the physical world. Namely,
theinherentdynamicsleadsspontaneouslytothefunctionaldisintegrationofthe
agent.Iftheagentstopspropelling,itsuccumbstogravityandfalls.Atthesame
timetheenvironmenthasaconstantscrollingrate,whichimpliesthattheagent
cannotremainfloatingatasinglepointandcannotsurvivesimplybyflyingaim-
lessly.Originally,thetaskprovidesarewardeverytimethebirdtraversesinbe-
tweentwopipes.Howeverforourcasestudytheinformationabouttherewardsis
neverpropagatedandthereforedoesnothaveanyimpactonthebehaviorofthe
agent. The agent receives a feature vector of observations indicating its location
andthoseof theobstacles.Inaddition, theagentobtains ameasurement v indi-
catingitspresenceinthetask(i.e.1or0).Thismeasurementdoesnotrepresent
anythingpositiveornegativebyitself,itissimplyanothersignalthatweassume
the agent is able to calculate. Similarly to the outline in 2.1, the agent monitors
the last t−k values of this measurement and estimates the density to obtain θ .
t
These become the statistics describing the current approximated distribution of
preferences p(v|θ ) or p (v), which are also used to augment the observations
t θt
to x = (o ,θ ). When the agent takes a new measurement v , it evaluates the
t t t t
surprisal against p (v ). In this particular case it is evaluated via a Bernoulli
θt−1 t
density function such that −lnp (v ) = −(v lnθ +(1−v )ln(1−θ )).
θt−1 t t t−1 t t−1
First, we train a baseline model-free surprisal minimizing DQN as specified in
2.1parameterizedbyaneuralnetwork(NN).Thenweexaminethebehaviorofa
second agent that minimizes the expected free energy. Thus the agent learns an
augmented state transition model of the world, parameterized by an ensemble
of NNs, and an expected surprisal model, also parameterized by another NN.
In order to identify an optimal policy we apply rolling horizon evolution [26] to
generatecandidatepoliciesπ =(a ,...,a )andtoassociatethemtoanexpected
τ T
free energy given by (appendix A),
G(π,τ)≈−E D [q(s |,o ,v ,π)||q(s |π)]−E [lnp (v )]
q(oτ,vτ,θ|π) KL τ τ τ τ q(vτ,θ,sτ|π) θ τ
(5)
6 A. Ovalle and S. M. Lucas
If we explicitly consider the model parameters φ, equation 5 can be decom-
posed as (appendix B),
G(π,τ)≈−E D [q(s |o ,v ,π)||q(s |π)]
q(oτ,vτ,φ|π) KL τ τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
salience
−E D [q(φ|s ,o ,v ,π)||q(φ)]
q(oτ,vτ,sτ|π) KL τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
novelty
−E [lnp (v )]
q(oτ,vτ,sτ,φ|π) θ τ
(cid:124) (cid:123)(cid:122) (cid:125)
instrumental value
(6)
The expression unpacks further the epistemic contributions to the EFE in
terms of salience and novelty [17]. These terms refer to the expected reduction
in uncertainty about hidden causes and in the parameters respectively. For this
task o=s, thus only the first and third term are considered.
3.2 Evaluation
The plot on fig. 1 (center) tracks the performance of an EFE agent in the en-
vironment (averaged over 10 seeds). The dotted line represents the surprisal
minimizing DQN agent after 1000 episodes. The left axis corresponds to the
(unobserved) task reward while the right axis indicates the approximated num-
beroftimestepstheagentsurvives.Duringthefirsttrials,andbeforetheagent
exhibits any form of competence, it was observed that the natural coupling be-
tweenagentandenvironmentgrantstheagentalifeexpectancyofroughly19-62
time steps in the task. This is essential as it starts to populate the statistics of
v. Measuring a specific quantity v, although initially representing just another
signal, begins to acquire certain value due to the frequency that it occurs. In
turn, this starts to dictate the preferences of the agent as it hints that measur-
ingcertainsignalcorrelateswithhavingastableconfigurationforthisparticular
environment as implied by its low surprisal. Right fig. 1 shows the evolution of
parameter θ (averaged within an episode) corresponding to the distribution of
preferred measurements p (v) which determines the level of surprisal assigned
θ
when receiving the next v. As the agent reduces its uncertainty about the en-
vironment it also becomes more capable of associating sensorimotor events to
specific measurements. The behavior becomes more consistent with seeking less
surprising measurements, and as we observe, this reinforces its preferences, ex-
hibiting the circular self-evidencing dynamics that characterize an agent mini-
mizing its free energy.
4 Discussion
Learning preferences in active inference: The major thesis in active infer-
ence is the notion of an agent acting in order to minimize its expected surprise.
Modulation of viability signals for self-regulatory control 7
Fig.1. Left: The Flappy Bird environment. Center: Performance of an EFE agent.
The left axis indicates the unobserved rewards as reported by the task and the right
axisthenumberoftimestepsitsurvivesintheenvironment.Thedottedlineshowsthe
average performance of an SM-DQN after 1000 episodes. Right: Parameter θ in time,
summarizing the intra-episode sufficient statistics of p (v).
θ
This implies that the agent will exhibit a tendency to seek for the sort of out-
comesthathavehighpriorprobabilityaccordingtoabiasedmodeloftheworld,
giving rise to goal-directed behavior. Due to the difficulty of modeling an agent
to exhibit increasing levels of autonomy, the agent based simulations under this
framework, and similarly to how it has largely occurred in RL, have tended to
concentrate on the generation of a particular expected behavior in the agent.
That is, on how to make the agent perform a task by encoding predefined goals
[16,21,29,32,33,34]orprovidingguidance[6,7,30].Howevertherehasbeenrecent
progresstryingtomitigatethisissue.Forexample,insomeofthesimulationsin
[29]theauthorsincludedadistributionoverpriorpreferencestoaccountforeach
of the cells in Frozen Lake, a gridworld like environment. Over time the prior
preferences are tuned, leading to habit formation. Most related to our work, are
the studies on surprise minimizing RL (SMiRL) by [4], where model-free agents
performed density estimation on their observation space and acquired complex
behavior in various tasks by maximizing the model evidence of their observa-
tions. Here we have also opted for this approach, however we have grounded it
on organismic based considerations of viability as inspired by insights on the
nature of agency and adaptive behavior [1,12,11]. It has been suggested that
evenifsomeoftheseaspectsaredefinedexogenouslytheycouldcapturegeneral
components of all physical systems and could potentially be derived in a more
objective manner compared to task based utilities [20]. Moreover these views
suggest that the inherent conditions of precariousness and the perturbations an
agent must face are crucial ingredients for the emergence of purpose generating
mechanisms. In that sense, our main concern has been to explore an instance of
theconditionsinwhichastablesetofattractingstatesarises,conferringvalueto
observations and leading to what seemed as self-sustaining dynamics. Although
all measurements lacked any initial functional value, the model presupposes the
capacityoftheagenttomeasureitsoperationalintegrityasitwouldoccurinan
organism monitoring its bodily states. This raises the issue of establishing more
principled protocols to define what should constitute the internal milieu of an
agent.
8 A. Ovalle and S. M. Lucas
Agent-Environment coupling: A matter of further analysis, also motivated
by results in [4], is the role of the environment to provide structure to the be-
havior of the agent. For instance, in the environments in [4], a distribution of
preferences spontaneously built on the initial set of visual observations tends to
correlate with good performance on the task. In the work presented here the
initial set of internal measurements afforded by the environment contributes to
the formation of a steady state, with the visual features informing the actions
necessary to maintain it. Hence similarly to [4], the initial conditions of the
agent-environmentcouplingthatfurnishthedistributionp(v)provideastarting
solution for the problem of self-maintenance as long as the agent is able to pre-
serve the statistics. Thus if the agent lacks a sophisticated sensory apparatus,
thecapacitytoextractinvariancesortheinitialstatisticsofsensorydatadonot
favor the emergence of goal-seeking behavior, tracking its internal configuration
may suffice for some situations. However this requires further unpacking, not
only because as discussed earlier it remains uncertain how to define the inter-
nal aspects of an agent, but also because often simulations do not capture the
essential characteristics of real environments either [8].
Drive decomposition:Whileherewehaveaffordedourmodelcertainlevelsof
independencebetweenthesensorydataandtheinternalmeasurements,itmight
besensibletoimaginethatinternalstateswouldaffectperceptionandperceptual
misrepresentation would affect internal states. Moreover, as the agent moves
from normative conditions based entirely on viability to acquire other higher
level preferences, it learns to integrate and balance different drives and goals.
Fromequation8itisalsopossibletoconceiveasimplifiedscenarioandestablish
the following expression (appendix D),
G(π,τ)≈E [lnq(s |π)−lnp(s |o ,π)]
q(oτ,vτ,θ,sτ|π) τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
epistemic value
−E [lnp(o )]
q(oτ,vτ,θ,sτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125)
high level value
+E H[p(v |s ,o ,π)]
q(oτ,sτ|π) τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
regulatory value
(7)
Wherethegoal-seekingvalueisdecomposedintoacomponentthatconsiders
preferences encoded in a distribution p(o) and another element estimating the
expectedentropyofthedistributionofessentialvariables.Policieswouldbalance
thecontributionsresolvingforhypotheticalsituations,suchasahigherlevelgoal
being at odds with the viability of the system.
Acknowledgment
ThisresearchutilisedQueenMary’sApocritaHPCfacility,supportedbyQMUL
Research-IT. doi:10.5281/zenodo.438045
Modulation of viability signals for self-regulatory control 9
References
1. Barandiaran, X.E., Paolo, E.D., Rohde, M.: Defining Agency: Individuality, Nor-
mativity,Asymmetry,andSpatio-temporalityinAction:.AdaptiveBehavior(2009)
2. Beirlant, J., Dudewicz, E.J., Gyo¨rfi, L., D´enes, I.: Nonparametric entropy estima-
tion. An overview. International Journal of Mathematical and Statistical Sciences
6(1), 17–39 (1997)
3. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., Munos,
R.: Unifying count-based exploration and intrinsic motivation. In: Lee, D.D.,
Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neu-
ral Information Processing Systems 29, pp. 1471–1479. Curran Associates, Inc.
(2016)
4. Berseth,G.,Geng,D.,Devin,C.,Rhinehart,N.,Finn,C.,Jayaraman,D.,Levine,
S.: SMiRL: Surprise Minimizing RL in Dynamic Environments. arXiv:1912.05510
[cs, stat] (2020)
5. Burda,Y.,Edwards,H.,Storkey,A.,Klimov,O.:Explorationbyrandomnetwork
distillation. In: International Conference on Learning Representations (2018)
6. C¸atal,O.,Nauta,J.,Verbelen,T.,Simoens,P.,Dhoedt,B.:Bayesianpolicyselec-
tion using active inference. arXiv:1904.08149 [cs] (2019)
7. C¸atal, O., Wauthier, S., Verbelen, T., De Boom, C., Dhoedt, B.: Deep Active
Inference for Autonomous Robot Navigation (2020)
8. Co-Reyes, J.D., Sanjeev, S., Berseth, G., Gupta, A., Levine, S.: Ecological Rein-
forcement Learning. arXiv:2006.12478 [cs, stat] (2020)
9. Damasio,A.R.:EmotionsandFeelings:ANeurobiologicalPerspective.In:Feelings
and Emotions: The Amsterdam Symposium, pp. 49–57. Studies in Emotion and
Social Interaction, Cambridge University Press, New York, NY, US (2004)
10. Dayan,P.,Hinton,G.E.,Neal,R.M.,Zemel,R.S.:TheHelmholtzmachine.Neural
Comput 7(5), 889–904 (1995)
11. Di Paolo, E.A.: Organismically-inspired robotics : Homeostatic adaptation and
teleology beyond the closed sensorimotor loop (2003)
12. Di Paolo, E.A.: Robotics Inspired in the Organism. intel 53(1), 129–162 (2010)
13. Friston, K.: Life as we know it. Journal of The Royal Society Interface 10(86),
20130475 (2013)
14. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active In-
ference: A Process Theory. Neural Computation 29(1), 1–49 (2016)
15. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology-Paris 100(1), 70–87 (2006)
16. Friston,K.,Samothrakis,S.,Montague,R.:Activeinferenceandagency:Optimal
control without cost functions. Biol Cybern 106(8), 523–541 (2012)
17. Friston,K.J.,Lin,M.,Frith,C.D.,Pezzulo,G.,Hobson,J.A.,Ondobaka,S.:Active
Inference, Curiosity and Insight. Neural Comput 29(10), 2633–2683 (2017)
18. Hinton, G.E., Zemel, R.S.: Autoencoders, minimum description length and
Helmholtz free energy. In: Proceedings of the 6th International Conference on
Neural Information Processing Systems. pp. 3–10. NIPS’93, Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA (1993)
19. Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,Saul,L.K.:AnIntroductiontoVari-
ational Methods for Graphical Models. Machine Learning 37(2), 183–233 (1999)
20. Kolchinsky, A., Wolpert, D.H.: Semantic information, autonomous agency, and
nonequilibrium statistical physics. Interface Focus 8(6), 20180041 (2018)
10 A. Ovalle and S. M. Lucas
21. Millidge,B.:Deepactiveinferenceasvariationalpolicygradients.JournalofMath-
ematical Psychology 96, 102348 (2020)
22. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller,M.:PlayingAtariwithDeepReinforcementLearning.arXiv:1312.5602
[cs] (2013)
23. Ostrovski, G., Bellemare, M.G., Oord, A., Munos, R.: Count-Based Exploration
with Neural Density Models. In: International Conference on Machine Learning.
pp. 2721–2730. PMLR (2017)
24. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by
self-supervised prediction. In: Proceedings of the 34th International Conference
on Machine Learning - Volume 70. pp. 2778–2787. ICML’17, JMLR.org, Sydney,
NSW, Australia (2017)
25. Pathak,D.,Gandhi,D.,Gupta,A.:Self-SupervisedExplorationviaDisagreement.
In: International Conference on Machine Learning. pp. 5062–5071. PMLR (2019)
26. Perez, D., Samothrakis, S., Lucas, S., Rohlfshagen, P.: Rolling horizon evolution
versustreesearchfornavigationinsingle-playerreal-timegames.In:GECCO’13.
pp. 351–358 (2013)
27. Ramstead,M.J.D.,Constant,A.,Badcock,P.B.,Friston,K.J.:Variationalecology
and the physics of sentient systems. Physics of Life Reviews 31, 188–205 (2019)
28. Ramstead,M.J.D.,Badcock,P.B.,Friston,K.J.:AnsweringSchro¨dinger’squestion:
A free-energy formulation. Physics of Life Reviews 24, 1–16 (2018)
29. Sajid, N., Ball, P.J., Friston, K.J.: Active inference: Demystified and compared.
arXiv:1909.10863 [cs, q-bio] (2020)
30. Sancaktar, C.,van Gerven,M., Lanillos,P.: End-to-EndPixel-BasedDeep Active
Inference for Body Perception and Action. arXiv:2001.05847 [cs, q-bio] (2020)
31. Tasfi, N.: PyGame Learning Environment. Github repository (2016)
32. Tschantz, A., Baltieri, M., Seth, A.K., Buckley, C.L.: Scaling active inference.
arXiv:1911.10601 [cs, eess, math, stat] (2019)
33. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Reinforcement Learning
through Active Inference. arXiv:2002.12636 [cs, eess, math, stat] (2020)
34. Ueltzho¨ffer, K.: Deep active inference. Biol Cybern 112(6), 547–573 (2018)
A Expected Free Energy with measurements v
We consider a generative model p(s,o,v|π) for the EFE equation and obtain
a joint distribution of preferences p(o,v). If we are interested exclusively in v,
assuming and treating o and v as if they were independent modalities, and
ignoring o we obtain:
G(π,τ)=E [lnq(s |π)−lnp(s ,o ,v |π)] (8)
q(oτ,vτ,θ,sτ|π) τ τ τ τ
≈E [lnq(s |π)−lnq(s |,o ,v ,π)−lnp(o ,v )]
q(oτ,vτ,θ,sτ|π) τ τ τ τ τ τ
≈E [lnq(s |π)−lnq(s |,o ,v ,π)−lnp(o )−lnp (v )]
q(oτ,vτ,θ,sτ|π) τ τ τ τ τ θ τ
≈E [lnq(s |π)−lnq(s |,o ,v ,π)−lnp (v )]
q(oτ,vτ,θ,sτ|π) τ τ τ τ θ τ
≈−E D [q(s |,o ,v ,π)||q(s |π)]−E [lnp (v )]
q(oτ,vτ,θ|π) KL τ τ τ τ q(vτ,θ,sτ|π) θ τ
(9)
Modulation of viability signals for self-regulatory control 11
B Novelty and salience
The derivation is equivalent to those found in the classical tabular descriptions
of active inference where instead of learning transitions via a function approxi-
mator, a mapping from hidden states to observations is encoded by a likelihood
matrix A. In the tabular case the beliefs of the probability of an observation
given a state are contained in the parameters a , which are updated as the
ij
agent obtains a particular observation.
G(π,τ)=E [lnq(s ,φ|π)−lnp(o ,v ,s ,φ|π)]
q(oτ,sτ,vτ,φ|π) τ τ τ τ
=E [lnq(φ)+lnq(s |π)
q(oτ,sτ,vτ,φ|π) τ
−lnp(φ|s ,o ,v ,π)−lnp(s |o ,v ,π)−lnp(o ,v )]
τ τ τ τ τ τ τ τ
≈E [lnq(φ)+lnq(s |π)
q(oτ,sτ,vτ,φ|π) τ
−lnq(φ|s ,o ,v ,π)−lnq(s |o ,v ,π)−lnp (v )]
τ τ τ τ τ τ θ τ
≈E [lnq(s |π)−lnq(s |o ,v ,π)]
q(oτ,sτ,vτ,φ|π) τ τ τ τ
+E [lnq(φ)−lnq(φ|s ,o ,v ,π)]
q(oτ,sτ,vτφ|π) τ τ τ
−E [lnp(v )]
q(oτ,sτ,vτ,φ|π) τ
≈−E [lnq(s |o ,v ,π)−lnq(s |π)]
q(oτ,sτ,vτ,φ|π) τ τ τ τ
−E [lnq(φ|s ,o ,v ,π)−lnq(φ)]
q(oτ,sτ,vτ,φ|π) τ τ τ
−E [lnp(v )]
q(oτ,sτ,vτ,φ|π) τ
≈−E (cid:2) D [q(s |o ,v ,π)||q(s |π)] (cid:3)
q(oτ,vτ,φ|π) KL τ τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
salience
−E (cid:2) D [q(φ|s ,o ,v ,π)||q(φ)] (cid:3)
q(oτ,vτ,sτ|π) KL τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
novelty
−E [lnp(v )]
q(oτ,vτ,sτ,φ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125)
instrumental value
(10)
C Implementation
We tested on the Flappy Bird environment [31]. The environment sends a non-
visual vector of features containing:
– the bird y position
– the bird velocity.
– next pipe distance to the bird
– next pipe top y position
– next pipe bottom y position
– next next pipe distance to the bird
– next next pipe top y position
12 A. Ovalle and S. M. Lucas
– next next pipe bottom y position
The parameter θ of the Bernoulli distribution p(v) was estimated from a
measurement buffer (i.e. queue) containing the last N values of v gathered by
the agent. We tested the agents with large buffers (e.g. 206) as well as small
buffers(e.g.20)withoutsignificantchangeinperformance.Theresultsreported
infig.1wereobtainedwithsmallsizedbuffersasdisplayedinthehyperparameter
table below.
The DQN agent was trained to approximate with a neural network a Q-
function Q ({s,θ},.). For our case study s = o which contains the vector
φ
of features, while θ is the parameter corresponding to the current estimated
statistics of p(v). An action is sampled uniformly with probability (cid:15) otherwise
a =min Q ({s ,θ },a). (cid:15) decays during training.
t a φ t t
For the EFE agent, the transition model p(s |s ,φ,π) is implemented as
t t−1
a N({s ,θ };f (s ,θ ,a ),f (s ,θ ,a )). Where a is an action of
t t φ t−1 t−1 t−1 φ t−1 t−1 t−1
a current policy π with one-hot encoding and f is an ensemble of K neural
φ
networks which predicts the next values of s and θ. The surprisal model is also
implemented with a neural network and trained to predict directly the surprisal
in the future as f (s ,θ ,a )=−lnp (v ).
ξ t−1 t−1 t−1 θt−1 t
Inordertocalculatetheexpectedfreeenergyinequation6fromasimulated
sequenceoffuturesteps,wefollowtheapproachdescribedinappendixGin[33]
where they show that an information gain of the form E D [q(φ|s)||q(φ)]
q(s|φ) KL
can be decomposed as,
E D [q(φ|s)||q(φ)]=−E (φ)H[q(s|φ)]+H[E q(s|φ)] (11)
q(s|φ) KL q q(φ)
with the first term computed analytically from the ensemble output and the
second term approximated with a k-NN estimator [2].
Hyperparameters DQNEFE
Measurement v buffer size 20 20
Replay buffer size 106 106
Batch size 64 50
Learning rate 1−3 1−3
Discount rate 0.99 -
Final (cid:15) 0.01 -
Seed episodes 5 3
Ensemble size - 25
Planning horizon - 15
Number of candidates - 500
Mutation rate - 0.5
Shift buffer - True
Modulation of viability signals for self-regulatory control 13
D Drive decomposition
G(π,τ)=E [lnq(s |π)−lnp(s ,o ,v |π)]
q(oτ,vτ,θ,sτ|π) τ τ τ τ
=E [lnq(s |π)−lnp(v |s ,o ,π)−lnp(s ,o |π)]
q(oτ,vτ,θ,sτ|π) τ τ τ τ τ τ
=E [lnq(s |π)−lnp(s |o ,π)−lnp(o )−lnp(v |s ,o ,π)]
q(oτ,vτ,θ,sτ|π) τ τ τ τ τ τ τ
≈E [lnq(s |π)−lnp(s |o ,π)]−E [lnp(o )]
q(oτ,vτ,θ,sτ|π) τ τ τ q(oτ,vτ,θ,sτ|π) τ
+E H[p(v |s ,o ,π)]
q(oτ,sτ|π) τ τ τ
(12)