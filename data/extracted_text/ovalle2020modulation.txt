Modulation of viability signals for self-regulatory
control
Alvaro Ovalle and Simon M. Lucas
Queen Mary University of London. London, UK
{a.ovalle,simon.lucas}@qmul.ac.uk
Abstract. We revisit the role of instrumental value as a driver of adap-
tive behavior. In active inference, instrumental or extrinsic value is quan-
tiﬁed by the information-theoretic surprisal of a set of observations mea-
suring the extent to which those observations conform to prior beliefs or
preferences. That is, an agent is expected to seek the type of evidence
that is consistent with its own model of the world. For reinforcement
learning tasks, the distribution of preferences replaces the notion of re-
ward. We explore a scenario in which the agent learns this distribution
in a self-supervised manner. In particular, we highlight the distinction
between observations induced by the environment and those pertaining
more directly to the continuity of an agent in time. We evaluate our
methodology in a dynamic environment with discrete time and actions.
First with a surprisal minimizing model-free agent (in the RL sense) and
then expanding to the model-based case to minimize the expected free
energy.
Keywords: perception-action loop · active inference · reinforcement
learning · self-regulation · anticipatory systems · instrumental value.
1 Introduction
The continual interaction that exists between an organism and the environment
requires an active form of regulation of the mechanisms safeguarding its integrity.
There are several aspects an agent must consider, ranging from assessing various
sources of information to anticipating changes in its surroundings. In order to
decide what to do, an agent must consider between diﬀerent courses of action
and factor in the potential costs and beneﬁts derived from its hypothetical future
behavior. This process of selection among diﬀerent value-based choices can be
formally described as an optimization problem. Depending on the formalism, the
cost or utility functions optimized by the agent presuppose diﬀerent normative
interpretations.
In reinforcement learning (RL) for instance, an agent has to maximize the
expected reward guided by a signal provided externally by the environment in an
oracular fashion. The reward in some cases is also complemented with an intrin-
sic contribution, generally corresponding to an epistemic deﬁciency within the
agent. For example prediction error [24], novelty [3,5,23] or ensemble disagree-
ment [25]. It is important to note that incorporating these surrogate rewards into
arXiv:2007.09297v2  [q-bio.NC]  13 Oct 2020
2 A. Ovalle and S. M. Lucas
the objectives of an agent is often regarded as one of many possible enhance-
ments to increase its performance, rather than been motivated by a concern with
explaining the roots of goal-directed behavior.
In active inference [14], the optimization is framed in terms of the minimiza-
tion of the variational free energy to try to reduce the diﬀerence between sensa-
tions and predictions. Instead of rewards, the agent holds a prior over preferred
future outcomes, thus an agent minimizing its free energy acts to maximize the
occurrence of these preferences and to minimize its own surprisal. Value arises
not as an external property of the environment, but instead it is conferred by the
agent as a contextual consequence of the interplay of its current conﬁguration
and the interpretation of stimuli.
There are recent studies that have successfully demonstrated how to refor-
mulate RL and control tasks under the active inference framework. While for
living processes it is reasonable to assume that the priors emerge and are re-
ﬁned over evolutionary scales and during a lifetime, translating this view into
a detailed algorithmic characterization raises important considerations because
there is no evolutionary prior to draw from. Thus the approaches to specify a
distribution of preferences have included for instance, taking the reward an RL
agent would receive and encoding it as the prior [16,21,29,32,33,34], connecting
it to task objectives [29] or through expert demonstrations [6,7,30].
In principle this would suggest that much of the eﬀort that goes into reward
engineering in RL is relocated to that of specifying preferred outcomes or to
the deﬁnition of a phase space. Nonetheless active inference provides important
conceptual adjustments that could potentially facilitate conceiving more princi-
pled schemes towards a theory of agents that could provide a richer account of
autonomous behavior and self-generation of goals, desires or preferences. These
include the formulation of objectives and utilities under a common language re-
siding in belief space, and appealing to a worldview in which utility is not treated
as independent or detached from the agent. In particular the latter could encour-
age a more organismic perspective of the agent in terms of the perturbations it
must endure and the behavioral policies it attains to maintain its integrity [11].
Here we explore this direction by considering how a signal acquires functional
signiﬁcance as the agent identiﬁes it as a condition necessary for its viability and
future continuity in the environment. Mandated by an imperative to minimize
surprisal, the agent learns to associate sensorimotor events to speciﬁc outcomes.
First, we start by introducing the surprise minimizing RL (SMiRL) speciﬁcation
[4] before we proceed with a brief overview of the expected free energy. Then
we motivate our approach from the perspective of a self-regulatory organism.
Finally, we present results from our case study and close with some observations
and further potential directions.
Modulation of viability signals for self-regulatory control 3
2 Preliminaries
2.1 Model-free surprisal minimization
Consider an environment whose generative process produces a state st ∈S at
each time step t resulting in an agent observing ot ∈O. The agent acts on the
environment with at ∈A according to a policy π, obtaining the next observation
ot+1. Suppose the agent performs density estimation on the lastt−kobservations
to obtain a current set of parameter(s) θt summarizing pθ(o). As these suﬃcient
statistics contain information about the agent-environment coupling, they are
concatenated with the observations into an augmented state xt = (ot,θt). Every
time step, the agent computes the surprisal generated by a new observation given
its current estimate and then updates it accordingly. In order to minimize sur-
prisal under this model-free RL setting, the agent should maximize the expected
log of the model evidence E[∑
tγtln pθt(ot)] [4]. Alternatively, we maintain con-
sistency with active inference by expressing the optimal surprisal Q-function
as,
Qπ∗(x,a) = Eπ[−ln pθ(o) + γmin
a′
Qπ∗(x′,a′)] (1)
estimated via DQN [22] or any function approximator with parameters φ
such that Qπ∗(x,a) ≈Q(x,a; φ).
2.2 Expected free energy
The free energy principle (FEP) [15] has evolved from an account of message
passing in the brain to propose a probabilistic interpretation of self-organizing
phenomena [13,27,28]. Central to current discourse around the FEP is the notion
of the Markov blanket to describe a causal separation between the internal states
of a system from external states, as well as the interfacing blanket states (i.e.
sensory and active states). The FEP advances the view that a system remains
far from equilibrium by maintaining a low entropy distribution over the states it
occupies during its lifetime. Accordingly, the system attempts to minimize the
surprisal of an event at a particular point in time.
This can be more concretely speciﬁed if we consider a distributionp(o) encod-
ing the states, drives or desires the system should fulﬁl. Thus the system strives
to obtain an outcome o that minimizes the surprisal −ln p(o). Alternatively,
we can also state this as the agent maximizing its model evidence or marginal
likelihood p(o). For most cases estimating the actual marginal is intractable,
therefore a system instead minimizes the free energy [10,18] which provides an
upper bound on the log marginal [19],
F = Eq(s)[ln q(s) −ln p(o,s)] (2)
where p(o,s) is the generative model and q(s) the variational density ap-
proximating hidden causes. Equation 2 is used to compute a static form of free
energy and infer hidden causes given a set of observations. However if we instead
4 A. Ovalle and S. M. Lucas
consider an agent that acts over an extended temporal dimension, it must infer
and select policies that minimize the expected free energy (EFE) G [14] of a
policy π for a future step τ >t. This can be expressed as,
G(π,τ) = Eq(oτ,sτ|π)[ln q(sτ|π) −ln p(oτ,sτ|π)] (3)
where p(oτ,sτ|π) = q(sτ|oτ,π)p(oτ) is the generative model of the future.
Rearranging G as,
G(π,τ) = −Eq(oτ|π)[ln p(oτ)]  
instrumental value
−Eq(oτ|π)
[
DKL[ln q(sτ|oτ,π)||ln q(sτ|π)]
]
  
epistemic value
(4)
which illustrates how the EFE entails a pragmatic, instrumental or goal-
seeking term that realizes preferences and an epistemic or information seek-
ing term that resolves uncertainty. An agent selects a policy with probability
q(π) = σ(−β∑
τ Gτ(π)) where σ is the softmax function and β is the inverse
temperature. In summary, an agent minimizes its free energy via active inference
by changing its beliefs about the world or by sampling the regions of the space
that conforms to its beliefs.
3 Adaptive control via self-regulation
The concept of homeostasis has played a crucial role in our understanding of
physiological regulation. It describes the capacity of a system to maintain its
internal variables within certain bounds. Recent developments in the FEP de-
scribing the behavior of self-organizing systems under the framework, can be
interpreted as an attempt to provide a formalization of this concept [28]. From
this point of view, homeostatic control in an organism refers to the actions nec-
essary to minimize the surprisal of the values reported by interoceptive channels,
constraining them to those favored by a viable set of states. Something that is
less well understood is how these attracting states come into existence. That is,
how do they emerge from the particular conditions surrounding the system and
how are they discovered among the potential space of signals.
Recently, it has been shown that complex behavior may arise by minimizing
surprisal in observation space (i.e. sensory states) without pre-encoded ﬁxed
prior distributions in large state spaces [4]. Here we consider an alternative angle
intended to remain closer to the homeostatic characterization of a system. In our
scenario, we assume that given the particular dynamics of an environment, if an
agent is equipped only with a basic density estimation capacity, then structuring
its behavior around the type of regularities in observation space that can sustain
it in time will be diﬃcult. In these situations with fast changing dynamics, rather
than minimizing free energy over sensory signals, the agent may instead leverage
them to maintain a low future surprisal of another target variable. That implies
that although the agent may have in principle access to multiple signals it might
be interested in maintaining only some of them within certain expected range.
Modulation of viability signals for self-regulatory control 5
Deﬁning what should constitute the artiﬁcial physiology in simulated agents
is not well established. Therefore we assume the introduction of an information
channel representing in abstract terms the interoceptive signals that inform the
agent about its continuity in the environment. We can draw a rudimentary com-
parison, and think of this value in a similar way in which feelings agglutinate and
coarse-grain the changes of several internal physical responses [9]. In addition,
we are interested in the agent learning to determine whether it is conductive to
its self-preservation in the environment or not.
3.1 Case Study
We assess the behavior of an agent in the Flappy Bird environment (ﬁg. 1 left).
This is a task where a bird must navigate between obstacles (pipes) at diﬀer-
ent positions while stabilizing its ﬂight. Despite the apparent simplicity, the
environment oﬀers a fundamental aspect present in the physical world. Namely,
the inherent dynamics leads spontaneously to the functional disintegration of the
agent. If the agent stops propelling, it succumbs to gravity and falls. At the same
time the environment has a constant scrolling rate, which implies that the agent
cannot remain ﬂoating at a single point and cannot survive simply by ﬂying aim-
lessly. Originally, the task provides a reward every time the bird traverses in be-
tween two pipes. However for our case study the information about the rewards is
never propagated and therefore does not have any impact on the behavior of the
agent. The agent receives a feature vector of observations indicating its location
and those of the obstacles. In addition, the agent obtains a measurement v indi-
cating its presence in the task (i.e. 1 or 0). This measurement does not represent
anything positive or negative by itself, it is simply another signal that we assume
the agent is able to calculate. Similarly to the outline in 2.1, the agent monitors
the last t−k values of this measurement and estimates the density to obtain θt.
These become the statistics describing the current approximated distribution of
preferences p(v|θt) or pθt(v), which are also used to augment the observations
to xt = (ot,θt). When the agent takes a new measurement vt, it evaluates the
surprisal against pθt−1 (vt). In this particular case it is evaluated via a Bernoulli
density function such that −ln pθt−1 (vt) = −(vtln θt−1 + (1 −vt) ln(1−θt−1)).
First, we train a baseline model-free surprisal minimizing DQN as speciﬁed in
2.1 parameterized by a neural network (NN). Then we examine the behavior of a
second agent that minimizes the expected free energy. Thus the agent learns an
augmented state transition model of the world, parameterized by an ensemble
of NNs, and an expected surprisal model, also parameterized by another NN.
In order to identify an optimal policy we apply rolling horizon evolution [26] to
generate candidate policies π= (aτ,...,a T) and to associate them to an expected
free energy given by (appendix A),
G(π,τ) ≈−Eq(oτ,vτ,θ|π)DKL[q(sτ|,oτ,vτ,π)||q(sτ|π)] −Eq(vτ,θ,sτ|π)[ln pθ(vτ)]
(5)
6 A. Ovalle and S. M. Lucas
If we explicitly consider the model parameters φ, equation 5 can be decom-
posed as (appendix B),
G(π,τ) ≈−Eq(oτ,vτ,φ|π)DKL[q(sτ|oτ,vτ,π)||q(sτ|π)]  
salience
−Eq(oτ,vτ,sτ|π)DKL[q(φ|sτ,oτ,vτ,π)||q(φ)]  
novelty
−Eq(oτ,vτ,sτ,φ|π)[ln pθ(vτ)]  
instrumental value
(6)
The expression unpacks further the epistemic contributions to the EFE in
terms of salience and novelty [17]. These terms refer to the expected reduction
in uncertainty about hidden causes and in the parameters respectively. For this
task o= s, thus only the ﬁrst and third term are considered.
3.2 Evaluation
The plot on ﬁg. 1 (center) tracks the performance of an EFE agent in the en-
vironment (averaged over 10 seeds). The dotted line represents the surprisal
minimizing DQN agent after 1000 episodes. The left axis corresponds to the
(unobserved) task reward while the right axis indicates the approximated num-
ber of time steps the agent survives. During the ﬁrst trials, and before the agent
exhibits any form of competence, it was observed that the natural coupling be-
tween agent and environment grants the agent a life expectancy of roughly 19-62
time steps in the task. This is essential as it starts to populate the statistics of
v. Measuring a speciﬁc quantity v, although initially representing just another
signal, begins to acquire certain value due to the frequency that it occurs. In
turn, this starts to dictate the preferences of the agent as it hints that measur-
ing certain signal correlates with having a stable conﬁguration for this particular
environment as implied by its low surprisal. Right ﬁg. 1 shows the evolution of
parameter θ (averaged within an episode) corresponding to the distribution of
preferred measurements pθ(v) which determines the level of surprisal assigned
when receiving the next v. As the agent reduces its uncertainty about the en-
vironment it also becomes more capable of associating sensorimotor events to
speciﬁc measurements. The behavior becomes more consistent with seeking less
surprising measurements, and as we observe, this reinforces its preferences, ex-
hibiting the circular self-evidencing dynamics that characterize an agent mini-
mizing its free energy.
4 Discussion
Learning preferences in active inference:The major thesis in active infer-
ence is the notion of an agent acting in order to minimize its expected surprise.
Modulation of viability signals for self-regulatory control 7
Fig. 1.Left: The Flappy Bird environment. Center: Performance of an EFE agent.
The left axis indicates the unobserved rewards as reported by the task and the right
axis the number of time steps it survives in the environment. The dotted line shows the
average performance of an SM-DQN after 1000 episodes. Right: Parameter θ in time,
summarizing the intra-episode suﬃcient statistics of pθ(v).
This implies that the agent will exhibit a tendency to seek for the sort of out-
comes that have high prior probability according to a biased model of the world,
giving rise to goal-directed behavior. Due to the diﬃculty of modeling an agent
to exhibit increasing levels of autonomy, the agent based simulations under this
framework, and similarly to how it has largely occurred in RL, have tended to
concentrate on the generation of a particular expected behavior in the agent.
That is, on how to make the agent perform a task by encoding predeﬁned goals
[16,21,29,32,33,34] or providing guidance [6,7,30]. However there has been recent
progress trying to mitigate this issue. For example, in some of the simulations in
[29] the authors included a distribution over prior preferences to account for each
of the cells in Frozen Lake, a gridworld like environment. Over time the prior
preferences are tuned, leading to habit formation. Most related to our work, are
the studies on surprise minimizing RL (SMiRL) by [4], where model-free agents
performed density estimation on their observation space and acquired complex
behavior in various tasks by maximizing the model evidence of their observa-
tions. Here we have also opted for this approach, however we have grounded it
on organismic based considerations of viability as inspired by insights on the
nature of agency and adaptive behavior [1,12,11]. It has been suggested that
even if some of these aspects are deﬁned exogenously they could capture general
components of all physical systems and could potentially be derived in a more
objective manner compared to task based utilities [20]. Moreover these views
suggest that the inherent conditions of precariousness and the perturbations an
agent must face are crucial ingredients for the emergence of purpose generating
mechanisms. In that sense, our main concern has been to explore an instance of
the conditions in which a stable set of attracting states arises, conferring value to
observations and leading to what seemed as self-sustaining dynamics. Although
all measurements lacked any initial functional value, the model presupposes the
capacity of the agent to measure its operational integrity as it would occur in an
organism monitoring its bodily states. This raises the issue of establishing more
principled protocols to deﬁne what should constitute the internal milieu of an
agent.
8 A. Ovalle and S. M. Lucas
Agent-Environment coupling:A matter of further analysis, also motivated
by results in [4], is the role of the environment to provide structure to the be-
havior of the agent. For instance, in the environments in [4], a distribution of
preferences spontaneously built on the initial set of visual observations tends to
correlate with good performance on the task. In the work presented here the
initial set of internal measurements aﬀorded by the environment contributes to
the formation of a steady state, with the visual features informing the actions
necessary to maintain it. Hence similarly to [4], the initial conditions of the
agent-environment coupling that furnish the distribution p(v) provide a starting
solution for the problem of self-maintenance as long as the agent is able to pre-
serve the statistics. Thus if the agent lacks a sophisticated sensory apparatus,
the capacity to extract invariances or the initial statistics of sensory data do not
favor the emergence of goal-seeking behavior, tracking its internal conﬁguration
may suﬃce for some situations. However this requires further unpacking, not
only because as discussed earlier it remains uncertain how to deﬁne the inter-
nal aspects of an agent, but also because often simulations do not capture the
essential characteristics of real environments either [8].
Drive decomposition:While here we have aﬀorded our model certain levels of
independence between the sensory data and the internal measurements, it might
be sensible to imagine that internal states would aﬀect perception and perceptual
misrepresentation would aﬀect internal states. Moreover, as the agent moves
from normative conditions based entirely on viability to acquire other higher
level preferences, it learns to integrate and balance diﬀerent drives and goals.
From equation 8 it is also possible to conceive a simpliﬁed scenario and establish
the following expression (appendix D),
G(π,τ) ≈Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(sτ|oτ,π)]  
epistemic value
−Eq(oτ,vτ,θ,sτ|π)[ln p(oτ)]  
high level value
+ Eq(oτ,sτ|π)H[p(vτ|sτ,oτ,π)]  
regulatory value
(7)
Where the goal-seeking value is decomposed into a component that considers
preferences encoded in a distribution p(o) and another element estimating the
expected entropy of the distribution of essential variables. Policies would balance
the contributions resolving for hypothetical situations, such as a higher level goal
being at odds with the viability of the system.
Acknowledgment
This research utilised Queen Mary’s Apocrita HPC facility, supported by QMUL
Research-IT. doi:10.5281/zenodo.438045
Modulation of viability signals for self-regulatory control 9
References
1. Barandiaran, X.E., Paolo, E.D., Rohde, M.: Deﬁning Agency: Individuality, Nor-
mativity, Asymmetry, and Spatio-temporality in Action:. Adaptive Behavior (2009)
2. Beirlant, J., Dudewicz, E.J., Gy¨ orﬁ, L., D´ enes, I.: Nonparametric entropy estima-
tion. An overview. International Journal of Mathematical and Statistical Sciences
6(1), 17–39 (1997)
3. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., Munos,
R.: Unifying count-based exploration and intrinsic motivation. In: Lee, D.D.,
Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neu-
ral Information Processing Systems 29, pp. 1471–1479. Curran Associates, Inc.
(2016)
4. Berseth, G., Geng, D., Devin, C., Rhinehart, N., Finn, C., Jayaraman, D., Levine,
S.: SMiRL: Surprise Minimizing RL in Dynamic Environments. arXiv:1912.05510
[cs, stat] (2020)
5. Burda, Y., Edwards, H., Storkey, A., Klimov, O.: Exploration by random network
distillation. In: International Conference on Learning Representations (2018)
6. C ¸ atal, O., Nauta, J., Verbelen, T., Simoens, P., Dhoedt, B.: Bayesian policy selec-
tion using active inference. arXiv:1904.08149 [cs] (2019)
7. C ¸ atal, O., Wauthier, S., Verbelen, T., De Boom, C., Dhoedt, B.: Deep Active
Inference for Autonomous Robot Navigation (2020)
8. Co-Reyes, J.D., Sanjeev, S., Berseth, G., Gupta, A., Levine, S.: Ecological Rein-
forcement Learning. arXiv:2006.12478 [cs, stat] (2020)
9. Damasio, A.R.: Emotions and Feelings: A Neurobiological Perspective. In: Feelings
and Emotions: The Amsterdam Symposium, pp. 49–57. Studies in Emotion and
Social Interaction, Cambridge University Press, New York, NY, US (2004)
10. Dayan, P., Hinton, G.E., Neal, R.M., Zemel, R.S.: The Helmholtz machine. Neural
Comput 7(5), 889–904 (1995)
11. Di Paolo, E.A.: Organismically-inspired robotics : Homeostatic adaptation and
teleology beyond the closed sensorimotor loop (2003)
12. Di Paolo, E.A.: Robotics Inspired in the Organism. intel 53(1), 129–162 (2010)
13. Friston, K.: Life as we know it. Journal of The Royal Society Interface 10(86),
20130475 (2013)
14. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active In-
ference: A Process Theory. Neural Computation 29(1), 1–49 (2016)
15. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology-Paris 100(1), 70–87 (2006)
16. Friston, K., Samothrakis, S., Montague, R.: Active inference and agency: Optimal
control without cost functions. Biol Cybern 106(8), 523–541 (2012)
17. Friston, K.J., Lin, M., Frith, C.D., Pezzulo, G., Hobson, J.A., Ondobaka, S.: Active
Inference, Curiosity and Insight. Neural Comput 29(10), 2633–2683 (2017)
18. Hinton, G.E., Zemel, R.S.: Autoencoders, minimum description length and
Helmholtz free energy. In: Proceedings of the 6th International Conference on
Neural Information Processing Systems. pp. 3–10. NIPS’93, Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA (1993)
19. Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., Saul, L.K.: An Introduction to Vari-
ational Methods for Graphical Models. Machine Learning 37(2), 183–233 (1999)
20. Kolchinsky, A., Wolpert, D.H.: Semantic information, autonomous agency, and
nonequilibrium statistical physics. Interface Focus 8(6), 20180041 (2018)
10 A. Ovalle and S. M. Lucas
21. Millidge, B.: Deep active inference as variational policy gradients. Journal of Math-
ematical Psychology 96, 102348 (2020)
22. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602
[cs] (2013)
23. Ostrovski, G., Bellemare, M.G., Oord, A., Munos, R.: Count-Based Exploration
with Neural Density Models. In: International Conference on Machine Learning.
pp. 2721–2730. PMLR (2017)
24. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by
self-supervised prediction. In: Proceedings of the 34th International Conference
on Machine Learning - Volume 70. pp. 2778–2787. ICML’17, JMLR.org, Sydney,
NSW, Australia (2017)
25. Pathak, D., Gandhi, D., Gupta, A.: Self-Supervised Exploration via Disagreement.
In: International Conference on Machine Learning. pp. 5062–5071. PMLR (2019)
26. Perez, D., Samothrakis, S., Lucas, S., Rohlfshagen, P.: Rolling horizon evolution
versus tree search for navigation in single-player real-time games. In: GECCO ’13.
pp. 351–358 (2013)
27. Ramstead, M.J.D., Constant, A., Badcock, P.B., Friston, K.J.: Variational ecology
and the physics of sentient systems. Physics of Life Reviews 31, 188–205 (2019)
28. Ramstead, M.J.D., Badcock, P.B., Friston, K.J.: Answering Schr¨ odinger’s question:
A free-energy formulation. Physics of Life Reviews 24, 1–16 (2018)
29. Sajid, N., Ball, P.J., Friston, K.J.: Active inference: Demystiﬁed and compared.
arXiv:1909.10863 [cs, q-bio] (2020)
30. Sancaktar, C., van Gerven, M., Lanillos, P.: End-to-End Pixel-Based Deep Active
Inference for Body Perception and Action. arXiv:2001.05847 [cs, q-bio] (2020)
31. Tasﬁ, N.: PyGame Learning Environment. Github repository (2016)
32. Tschantz, A., Baltieri, M., Seth, A.K., Buckley, C.L.: Scaling active inference.
arXiv:1911.10601 [cs, eess, math, stat] (2019)
33. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Reinforcement Learning
through Active Inference. arXiv:2002.12636 [cs, eess, math, stat] (2020)
34. Ueltzh¨ oﬀer, K.: Deep active inference. Biol Cybern 112(6), 547–573 (2018)
A Expected Free Energy with measurementsv
We consider a generative model p(s,o,v |π) for the EFE equation and obtain
a joint distribution of preferences p(o,v). If we are interested exclusively in v,
assuming and treating o and v as if they were independent modalities, and
ignoring o we obtain:
G(π,τ) = Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(sτ,oτ,vτ|π)] (8)
≈Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln q(sτ|,oτ,vτ,π) −ln p(oτ,vτ)]
≈Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln q(sτ|,oτ,vτ,π) −ln p(oτ) −ln pθ(vτ)]
≈Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln q(sτ|,oτ,vτ,π) −ln pθ(vτ)]
≈−Eq(oτ,vτ,θ|π)DKL[q(sτ|,oτ,vτ,π)||q(sτ|π)] −Eq(vτ,θ,sτ|π)[ln pθ(vτ)]
(9)
Modulation of viability signals for self-regulatory control 11
B Novelty and salience
The derivation is equivalent to those found in the classical tabular descriptions
of active inference where instead of learning transitions via a function approxi-
mator, a mapping from hidden states to observations is encoded by a likelihood
matrix A. In the tabular case the beliefs of the probability of an observation
given a state are contained in the parameters aij, which are updated as the
agent obtains a particular observation.
G(π,τ) = Eq(oτ,sτ,vτ,φ|π)[ln q(sτ,φ|π) −ln p(oτ,vτ,sτ,φ|π)]
= Eq(oτ,sτ,vτ,φ|π)[ln q(φ) + lnq(sτ|π)
−ln p(φ|sτ,oτ,vτ,π) −ln p(sτ|oτ,vτ,π) −ln p(oτ,vτ)]
≈Eq(oτ,sτ,vτ,φ|π)[ln q(φ) + lnq(sτ|π)
−ln q(φ|sτ,oτ,vτ,π) −ln q(sτ|oτ,vτ,π) −ln pθ(vτ)]
≈Eq(oτ,sτ,vτ,φ|π)[ln q(sτ|π) −ln q(sτ|oτ,vτ,π)]
+ Eq(oτ,sτ,vτφ|π)[ln q(φ) −ln q(φ|sτ,oτ,vτ,π)]
−Eq(oτ,sτ,vτ,φ|π)[ln p(vτ)]
≈−Eq(oτ,sτ,vτ,φ|π)[ln q(sτ|oτ,vτ,π) −ln q(sτ|π)]
−Eq(oτ,sτ,vτ,φ|π)[ln q(φ|sτ,oτ,vτ,π) −ln q(φ)]
−Eq(oτ,sτ,vτ,φ|π)[ln p(vτ)]
≈−Eq(oτ,vτ,φ|π)
[
DKL[q(sτ|oτ,vτ,π)||q(sτ|π)]
]
  
salience
−Eq(oτ,vτ,sτ|π)
[
DKL[q(φ|sτ,oτ,vτ,π)||q(φ)]
]
  
novelty
−Eq(oτ,vτ,sτ,φ|π)[ln p(vτ)]  
instrumental value
(10)
C Implementation
We tested on the Flappy Bird environment [31]. The environment sends a non-
visual vector of features containing:
– the bird y position
– the bird velocity.
– next pipe distance to the bird
– next pipe top y position
– next pipe bottom y position
– next next pipe distance to the bird
– next next pipe top y position
12 A. Ovalle and S. M. Lucas
– next next pipe bottom y position
The parameter θ of the Bernoulli distribution p(v) was estimated from a
measurement buﬀer (i.e. queue) containing the last N values of v gathered by
the agent. We tested the agents with large buﬀers (e.g. 20 6) as well as small
buﬀers (e.g. 20) without signiﬁcant change in performance. The results reported
in ﬁg. 1 were obtained with small sized buﬀers as displayed in the hyperparameter
table below.
The DQN agent was trained to approximate with a neural network a Q-
function Qφ({s,θ},.). For our case study s = o which contains the vector
of features, while θ is the parameter corresponding to the current estimated
statistics of p(v). An action is sampled uniformly with probability ϵ otherwise
at = minaQφ({st,θt},a). ϵ decays during training.
For the EFE agent, the transition model p(st|st−1,φ,π ) is implemented as
a N({st,θt}; fφ(st−1,θt−1,at−1),fφ(st−1,θt−1,at−1)). Where a is an action of
a current policy π with one-hot encoding and fφ is an ensemble of K neural
networks which predicts the next values of s and θ. The surprisal model is also
implemented with a neural network and trained to predict directly the surprisal
in the future as fξ(st−1,θt−1,at−1) = −ln pθt−1 (vt).
In order to calculate the expected free energy in equation 6 from a simulated
sequence of future steps, we follow the approach described in appendix G in [33]
where they show that an information gain of the form Eq(s|φ)DKL[q(φ|s)||q(φ)]
can be decomposed as,
Eq(s|φ)DKL[q(φ|s)||q(φ)] = −Eq(φ)H[q(s|φ)] + H[Eq(φ)q(s|φ)] (11)
with the ﬁrst term computed analytically from the ensemble output and the
second term approximated with a k-NN estimator [2].
Hyperparameters DQN EFE
Measurement v buﬀer size 20 20
Replay buﬀer size 10 6 106
Batch size 64 50
Learning rate 1 −3 1−3
Discount rate 0.99 -
Final ϵ 0.01 -
Seed episodes 5 3
Ensemble size - 25
Planning horizon - 15
Number of candidates - 500
Mutation rate - 0.5
Shift buﬀer - True
Modulation of viability signals for self-regulatory control 13
D Drive decomposition
G(π,τ) = Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(sτ,oτ,vτ|π)]
= Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(vτ|sτ,oτ,π) −ln p(sτ,oτ|π)]
= Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(sτ|oτ,π) −ln p(oτ) −ln p(vτ|sτ,oτ,π)]
≈Eq(oτ,vτ,θ,sτ|π)[ln q(sτ|π) −ln p(sτ|oτ,π)] −Eq(oτ,vτ,θ,sτ|π)[ln p(oτ)]
+ Eq(oτ,sτ|π)H[p(vτ|sτ,oτ,π)]
(12)