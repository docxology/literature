5102
raM
31
]IA.sc[
1v78140.3051:viXra
A Minimal Active Inference Agent
Simon McGregor1, Manuel Baltieri1 and Christopher L. Buckley1
1University of Sussex, Brighton, UK
s.mcgregor@sussex.ac.uk, m.baltieri@sussex.ac.uk, c.l.buckley@sussex.ac.uk
March 16, 2015
Abstract
Research on the so-called “free-energy principle” (FEP) in cognitive
neuroscienceisbecomingincreasinglyhigh-profile. Todate,introductions
to this theory have proved difficult for many readers to follow, but it
depends mainly upon two relatively simple ideas: firstly that normative
or teleological values can be expressed as probability distributions (ac-
tiveinference),andsecondlythatapproximateBayesianreasoningcanbe
effectively performed by gradient descent on model parameters (thefree-
energy principle). The notion of active inference is of great interest for a
numberofdisciplinesincludingcognitivescienceandartificialintelligence,
aswell as cognitiveneuroscience, and deservestobemore widely known.
This paper attempts to provide an accessible introduction to active
inferenceandinformational free-energy,forreadersfrom arangeofscien-
tific backgrounds. In this work introduce an agent-based model with an
agent trying to make predictions about its position in a one-dimensional
discretized world using methods from theFEP.
1 Introduction
Active inference is the name given by Karl Friston to a process of an agent’s
simultaneously co-constrained inference regarding the world combined and di-
rected actions to change it, understood specifically as the minimisation of an
information-theoreticquantitydescribingtheinstantaneousrelationofasystem
to its “typical” sensory environment.
According to proponents of active inference, all mechanical implementation
of cognitivebehaviour(including both the active and inferentialaspects of cog-
nition) can be reduced to the minimisation of a single informational “free en-
ergy”quantity; moreover,this quantity can be assignedan interpretationusing
cognitive concepts resembling beliefs and intentions.
This theory has attracted a great deal of attention (both positive and neg-
ative), since it claims to unify a wide variety of principles in cognitive science,
machine learning and neuroscience. However, a clear and balanced discussions
of the framework’s strengths and weaknesses is still lacking.
We suspectthatone ofthe barrierspreventinga wider understandingofthe
theory is the language which been used to describe it. The language is tech-
nically dense, using terms like ’surprise’, ’generative’, ’ensemble’, ’variational’,
1
and can seem to beg philosophical questions in a way which is not strictly
mandated by the formalism of the active inference theory. For instance, the
framework is frequently characterised as claiming that cognitive agents act so
as to “confirm their predictions”, or “minimise their surprise”; we will see that
alternative interpretations exist for the same formal assertions.
Furthermore one of the central strengths of the formalism is its potential
to unite an understanding off action and perception within a single unified
framework[13].Howeverunderstandingthe exactimplicationsofthiswilllikely
requiretheutilisationofagentbasedapproacheswhichtodatehavelargelybeen
absent.
This paper aims to clarify the foundations of the active inference or “free-
energy” framework in terms which can be understood by a wide audience. We
startbygivingabriefconceptualoverviewoftheinformationFree-Energy(IFE)
principle and active inference in an intuitive language
We groundourdiscussionby developingaFree-Energyformalismforsimple
activeagentbehavinginone-dimensionaldiscrete-timeworld. Thisagentbased
approach allows us to examine the relationship between action and perception
that emerges from the interaction of the agents’ internal model and the world
around it. We finish by the outlining the main contributions and the known
currentlimitations ofthe IFE . Inparticularwe will addresssome ofthe claims
made regarding free-energy or active inference, and attempt to deflate them in
a way which may help readers make more sense of the underlying ideas.
The foundational premise of the IFE principle is that adaptive agents are
defined by their ability occupy only a limited repertoire of physical states [13].
Adaptive agents achievethis by exportingthermodynamic entropy (thus resist-
ing the third law) and preserving the defining traits that comprise their very
identity. However agents do not have direct access to states that constitute
there world and body and instead must work vicariously on sensory data do
this. Formally speaking, the active inference framework proposes that agents
do this by acting to minimise an information-theoretic quantity derived from
sensory data known as surprisal. The interpretation of this quantity is some-
what subtle, and it should absolutely not be conflated with the psychological
phenomenon we call surprise.
Friston has previously asserted that agents which export thermodynamic
entropy must do so by minimising the informational entropy of their sensory
inputs (considered as an empirical distribution sampled over time). He argues
thatthe onlytractablewayofdoingsoistominimise informationalfreeenergy.
Thus, the very fact that a living system maintains its organisation in the face
ofenvironmentalperturbationsupposedlyjustifiesadvertingtothe“free-energy
principle” [14].
Oneofthemostinterestingaspectsofthe activeinferenceframeworkisthat
twothingswhichwegenerallyregardasquitedistinctaregiventhesameformal
treatment: on the one hand, beliefs or predictions about sensory input and the
external world which underlies it, and on the other hand intentions regarding
the outcomesof ouractions. Inthis sense,active inference blurs the distinction
between a prediction and an intention; both are treated as conceptions-of-the-
future, with the main difference being in what physical variables change most
when the reality turns out to be different from the expectation [13].
Althoughthe free-energyliteratureuses the terms “prediction”and“expec-
tation”interchangeablyforthisconcept,itseemsthatonlytheverb“expect”in
2
English really captures the ambiguity between intention and prediction which
the active inference frameworkrests upon. It is unfortunate that the word“ex-
pectation” also has a technical meaning which is relevant to active inference,
i.e. the mean of a probability distribution. Hence, we propose the term “ex-
pectance” to refer to the superclass of probabilistic beliefs and desires implicit
in the active inference framework.
In this language, active inference asserts that agents constantly behave so
as to reduce the discrepancy between their expectances and reality; the resolu-
tion of discrepancy by changing internal variables is known as “updating one’s
predictions”, while the resolution of discrepancy by changing reality is known
as “acting on one’s environment”.
Crucially, under active inference both of those dynamics are always occur-
ring simultaneously. It is not necessary to pre-judge which of those dynamics
will predominate by calling the expectance a “prediction” or an “intention” in
advance of observing the outcome.
The typical presentation of active inference states that agents behave so
as to make their “predictions” come true. We believe this wording erases the
distinction between internal and external resolution of expectance violation, in
anunhelpful way. Intuitively, if anagent“predicts”its armto moveto the left,
and the arm instead stays in place, we would expect it to change internally so
as to make different predictions; no action on the world would be presupposed
by the discrepancy. On the other hand, if the agent “intends” its arm to move
to the left, and the arm instead stays in place, we would expect it to exert a
greater force on its arm by contracting the relevant muscles.
Of course, it is entirely possible that the agent could begin by attempting
to move its arm and, in doing so, learn that its range of movement is restricted
by external forces. The expectance violation can be resolved by coming to a
halfway point in which the agent “updates its predictions” to reflect the fact
that its movement is restricted, “modifies its intentions” to be consistent with
what is achievable, and “acts on the world” by moving within its available
range. Suchanequilibriumcanemergenaturallyfromacontinuous-timeprocess
where informationalfree energyis minimisedalongdistinct internaldimensions
simultaneously.
Recentdiscussionsofactiveinferencehavetendedtooccurwithinthecontext
of a neuroscientific theory known as the predictive coding hypothesis [9]. The
predictivecodingideaisessentiallythis: themainfunctionofsensoryneuronsis
to predicttheir sensoryinputs (usingefferentsignalsfromhigherbrainregions)
and report their prediction errors, with the consequence that afferent sensory
signals are nonzero only when they carry new information. The hierarchical
predictive coding hypothesis additionally posits that this mode of operation
extends to multiple layers of neurons, with each layer providing input for the
next.
Friston states that, if one makes a number of (non-trivial) mathematical
assumptions andapproximations,active inference via informationalfree-energy
minimisation can be implemented in a hierarchical predictive coding neural
model. Within this hierarchical predictive coding model for active inference
[16], efferent (motor) signals are treated similarly to other signals. The work-
ings of this process can be a little difficult to understand at first; the idea is
that the neural architectureimposes certainspecific patterns of expectances on
instantaneous kinesthetic sensations [13]. These expectances correspond to the
3
agent’s desired behaviour, such that the expectancy error directly becomes a
motor signal.
Unfortunately,itisnotatallclearthatadirectmappingbetweenexpectancy
error on kinesthetic sensation and motor signal makes sense within a general
frameworkof cognition. Firstly, the brain’s kinesthetic model is learned,rather
than genetically determined, and consequently its relationships with motor sig-
nals need to be learned as well. For active inference to work as a universal
explanatory principle, the formalism needs to be able to explain how this map-
ping arises in the first place; an isomorphism between the semantic meaning of
motorsignalsandthesemanticmeaningofkinestheticsensationscannotsimply
be assumed.
Secondly, general theories of cognition ought to be widely relevant; their
application cannot be restricted to humans, mammals or even animals. The
principleswhichunderlieadaptivebehaviourextendalsotoplants,single-celled
organisms, and arguably even to systems such as social insect colonies or non-
biological dissipative structures. How does one model the error in kinesthetic
sensoryexpectanceofabacterium? Oraswarmofbees? Thisproblembecomes
most evident when one attempts to construct a minimal simulation based on
active inference.
Fortunately,theissueiscausedbynotintrinsictotheactiveinferenceframe-
work itself, but only by the simplification which treats kinesthetic sensation as
a special mode of sensation. If this assumption is not made, the free-energy
equations give a generalsolution: filter the choice of actionthroughthe agent’s
(current provisional)model of action’s effect on the whole of its sensorium. We
show in this paper that such an approach works perfectly well for a simple
simulated artificial agent.
2 The Free Energy Formalism
State variables Meaning
Ψ ,ψ World state at time t
i i i
B ,b Internal (brain) state at time t
i i i
S ,s Sensory input at time t
i i i
A ,a Motor trajectory between t and t
i i i i+1
Table 1: State variables
FollowingtheworkbyDayanetal. [12]ontheHelmholtzmachine,wedefine
2 densities that will constitute the free energy term to be optimised:
• agenerativedensityp(ψ,s|m)representingthe jointprobabilityofworld
states ψ and sensory input s based on a probabilistic predictive model m
by the agent or brain
• a recognition density q(ψ | b) encoding the agent’s (or brain’s) beliefs
about the causes ψ with a set of brain states b that fully describe these
beliefs.
4
IFE is then defined as:
q(ψ |b)
F(s,b)= q(ψ |b)ln dψ (1)
p(ψ,s|m)
Zψ
Optimising this term requires then a brain (agent) to be able to
• changeits perception,usingthe internalstates bto alterits beliefs q(ψ |
b) thus enhancing its ability of explaining the world causes as they are
produced by the generative model, or
• directing its actions to result in different sensory input in the generative
density p(ψ,s|m) that match the agent’s beliefs.
Two alternative analytical forms for the IFE better show dependence on
perception and action respectively, helping us to understand the contributions
to the free energy minimisation of both:
2.1 Perception
F(s,b)=D(q(ψ |b)||p(ψ |s,m))−lnp(s|m) (2)
where D(q(ψ | b) || p(ψ | s,m)) is the Kullback-Leibler (KL) divergence
(defined in the appendix A.2), between the recognition density (q(ψ | b)) and
the true posterior of the world states (p(ψ | s); −lnp(s | m) is the surprise
about the sensory input the brain cannot directly evaluate. Changing the set
of brain states b allows to approximate the posterior of the world states with
the brain’s beliefs about the world. In the ideal case where the two coincide,
free energy would be equal to the surprise (the KL divergence would be zero,
appendix[A.2]) thus being able to interpret this term otherwise hidden to the
brain.
The optimal internal state bopt is defined as:
bopt =argminF(s,b)
b
2.2 Action
Analternativeformshowsinsteadhowthefreeenergydependsonsensoryinput,
that can be thought as dependent on a a which represent the set of actions a
system can perform in a certain environment, giving then s(a).
F(s,b)=D(q(ψ |b)||p(ψ |m))−hlnp(s(a)|ψ,m)i (3)
q
with D(q(ψ |b)||p(ψ |m)), KL divergence between the recognition density
q(ψ |b)(i.e. posteriorbeliefaboutthecauses/worldstates)andtheprior(belief)
p(ψ |m))ofthe worldstatesψ; hlnp(s(a)|ψ,m)i beingtheexpectationabout
q
sensations s(a) under the density q. In this formulation we can see how free
energy is minimised using an optimum action aopt, that will sample inputs as
predicted by the recognition density.
aopt =argminF(˜s(a),b)
a
5
3 A discretized approach to the FEP
In this work we will consider a discrete representation of our state variables,
definingspecificallyΨ′,ψ′ andB′,b′ respectivelyasworldandinternalstatesat
time t .
i+1
In its discrete version,InformationalFree Energy (IFE) becomes a quantity
relating an action a and a sensation s to two possible internal states b and b′
based on a generative and a recognition densities which are attributed to the
agent by the theorist.
Densities Name
q(Ψ′ |B′)=P(Ψ′ |B′) Recognition density
p(Ψ′,S |B,A)=P(Ψ′,S |B,A) Generative density
Table 2: Key probability densities
Therecognitiondensitydescribedtheagent’sinternal“encoding”ofexternal
world states, while the generative density represents the agent’s “predictive
model” of physical dynamics.
Most active inference papers use a continuous-space definition of the infor-
mationalfree energyF[17,14,13],butinthis paper wewillconsideradiscrete-
space version:
q(ψ′ |b′)
F(b′,b,s,a)= q(ψ′ |b′)ln (4)
p(ψ′,s|b,a)
ψ′
X
IFE has severalinteresting properties, corresponding to different rearrange-
ments of the formula, which give rise to different interpretations [13] as we
described in the previous section. We will focus in this section on its role in
approximate Bayesian filtering (i.e. ongoing inference about a changing world
state). Following our discretized definition, the equation can be rewritten as
F(b′,b,s,a)=D (q(ψ′ |b′)||p(ψ′ |s,b,a))−lnp(s|b,a) (5)
KL
where the first term is the KL divergence between q(ψ′ | b′) and p(ψ′ | s,b,a),
as defined previously. The second one is the informational surprisal about the
sensory inputs s that cannot be directly evaluated by the agent. Since the KL
divergence is always a positive quantity, free energy represent an upper bound
of surprisal, meaning that by minimising IFE we indirectly optimise surprisal
at the same time.
Assuming the agent does not expect sensation s at time t to depend on
i i
actiona thatoriginatesamovementfromtimet totimet ,i.e. p(s|b,a)=
i i t+1
p(s | b), the second term is independent of b′ and a. This has two interesting
consequences:
1. Approximate Inference Minimising F(b ,b ,s ,a ) with respect to b cor-
1 0 0 0 1
responds to approximate Bayesian inference regarding the state of the
world at time t .
1
6
2. Optimal Control Minimising F(b∗,b ,s ,a ) with respect to a corre-
0 0 0 0
spondstooptimalcontrol(subjecttotheagent’sassumptionsaboutworld
dynamics) if b∗ encodes a desired distribution over world states at time
t .
1
NotethatthesecondstatementdrawsadistinctionbetweenbeliefP(ψ |b )
1 1
and desire P(ψ∗ | b∗) which does not reflect the original maths of the active
inference framework [18]. This will be helpful for expository purposes. Mecha-
nismsbywhichgoal-directedactioncanbe encodedinthe pureactiveinference
formalism,without distinguishing predictions fromintentions, will be discussed
later.
Wehavenotmentionedtheactualdynamicsoftheenvironment;infact,they
are not necessary for this analysis. We assume that the only interface between
the agent and its environment is the agent’s sensorimotor dynamics; hence, we
are free to consider how the agent’s (attributed) cognitive dynamics proceed
when it is fed an arbitrary series of sensory inputs by an experimenter resem-
bling Descartes’ “malicious demon”. In either case, we expect that the agent’s
behaviourshouldbe approximatelyrational,giventhe beliefs andintentions we
have attributed it.
3.1 Practical IFE Minimisation
The value of IFE minimisation as a computational model of active inference,
either for machine learning purposes or in neuroscience modelling, depends on
howrealisticproposedmechanisms forminimising IFE are. Inthe generalcase,
IFE minimisation is not much more computationally tractable than the sum
required to perform exact Bayesian inference, because it involves a sum (or,
in the continuous case, integral) over all possible world states. It is of course
conceivablethatthebraincanimplicitlycomputethisintegralthroughphysical
means which are expensive to simulate computationally. However, IFE is sig-
nificantly easier to compute when certain certain conditions are met regarding
how the agent-environmentdynamics workand how the agentrepresentsworld
states. Theseconditions,alongwithadditionalapproximations,allowIFEmin-
imisation to form a tractable computational approach to active inference [15,
16] and are argued [13] to provide an elegant model of neural function.
3.2 Unifying Beliefs and Desires
We have indicated why minimising F(b ,b ,s ,a ) with respect to b consti-
1 0 0 0 1
tutes approximate Bayesian filtering, and why minimising F(b∗,b ,s ,a ) with
0 0 0
respect to a constitutes optimal control. In a machine learning context, it will
0
frequentlybe convenienttomaintainadistinctionbetweenbeliefandintention,
and the computationalcost of maintaining the distinction is small, because the
function F (and its derivatives) can be called with arbitrary arguments.
However, the free-energy formulation in neuroscience postulates a different
mechanism. The same IFE term is used to provide a gradient term which is
applied to the dynamics of both internal state and action [13]. Arguably, this
is more biologically plausible since an agent has only one brain state, but the
interpretation is less clear.
7
4 A Minimal Free-Energy Agent
This section outlines in detail how the active inference formalism works for a
“minimal” abstract model of active agency. Agents are modelled as organisms
thatinhabitasinglediscretespace(a“cell”)inaone-dimensionaldiscrete-time
world,andwhicharesensitive to achemicalwhichoccursintheir environment.
The world has periodic boundary conditions (i.e. it “wraps around” at the
edges), and each cell contains a concentration of the chemical. The concentra-
tion of the chemical follows a gradient across the environment, being highest
in a “source” cell and lowest in the cell furthest from the source; if the reader
likes, this can be imagined as the result of a diffusion process.
Theagent’sinteractionswithitsenvironmentareextremelylow-bandwidth:
ithasa1-bitsensor,whichfireswithaprobabilityproportionaltothechemical’s
concentration,anda1-bitmotor,whichattemptstomoveitonewayortheother
along the 1d world. We will arbitrarilymodel the agentas having a “desire”to
move to a particular spot in its world (relative to the “source”).
This system is simple enough that exact Bayesian inference can be per-
formed directly, making approximate schemes such as free-energyminimisation
unnecessary. Hence, free-energy minimisation for this system is done purely
for didactic purposes, with the advantage that the results can be compared to
exactBayesianposteriors. For more complex systems,exactBayesianinference
rapidly becomes intractable, and approximate methods are necessary.
Figure 1: Illustration of agent-environment system. The agent has a sensor
whichreadsHighorLowandissensitivetochemicalconcentration. Theagent’s
motor can attempt to move the agent clockwise or anticlockwise.
4.1 Definitions
We will begin by describing the simulation framework for our agents and the
environments they inhabit. The agent-environment dynamics are modelled in
discretetimesteps. Theagentisrepresentedasasystemwithaninternal‘brain’
8
state b. After emitting an action a, the agent receives a sensory input s and
updates its brain to a new state b′ based on b, a and s. It then emits a new
action a′ based on b′, and the cycle starts again.
It will sometimes be worth considering how the agent’s dynamics proceed
whenitisfedanarbitraryseriesofsensoryinputsbyanexperimenterresembling
Descartes’ “malicious demon”, but in general we are interested in the coupled
dynamics of an agent and its “natural” environment. The environment will be
represented as a system with a state ψ, which changes to state ψ′ depending
on its current state and the agent’s action, and the sensory input of the agent
depends on the instantaneous state of the environment.
This is a fairly standard approachto modelling discrete-time agency, but in
fact there is nothing “agentive” in the equations - they describe an arbitrary
coupled dynamical system. As cognitive scientists we will be assigning a cogni-
tive interpretation to the agent and its dynamics; this interpretation can (and
should)be distinguishedfromthebare‘physical’dynamicsofthe agentsystem.
Inthismodel,akeypartoftheinterpretationwillbetheassertionthateach
brain state ‘encodes’ a belief about the world1. Following Bayesian principles,
thebeliefwilltakeaprobabilisticform: insteadofholdingonlyclear-cutbeliefs,
theagent’sbrainwillencodebeliefsthatcanhavevaryingdegreesofuncertainty.
It is important to note that these probabilities are understood as degrees of
subjective uncertainty for the agent, not as the ‘objective’ probability of any
simulated outcome.
The agent-environment system’s ‘physical’ dynamics can be directly ob-
served over time, producing a series of brain states. The theorist can then
projectthesebrainstatesontoacorrespondingseriesofBayesianbeliefs: a“be-
lief dynamics” of the agent system under a particular cognitive interpretation.
4.2 Formulation
In our case, the environment is modelled as having a length of n=16 cells and
its state ψ consists only of the agent’s position x (ψ = x). The agent’s brain
state b will therefore encode a probability distribution P(ψ | b) over possible
cell locations.
Therecognitiondensity,whichencodesthe agent’sinternalbeliefs abouthis
position at time t+1 will be described as follows. The agent’s brain state b′ is
a vector of real numbers b′,··· ,b′ and we use a softmax encoding:
1 n
eb′
ψ
q(ψ′ |b′)=P(ψ′ |b′)= (6)
eb′
i
i
X
As described the sectionabove,for anactive inference agentwe define a sortof
targetbrainstate: the state the agentwould“prefer”its braintobe in,subject
to the constraint that its brain is updated rationally. Another interpretation
is that it encodes the location distribution which the agent would prefer to
occupyoveranensembleofpossibleUniverses. Consequentlywewriteanagents
1In principle, we should distinguish the “agent’s world”, about which it can have beliefs,
from the “theorist’s world” that corresponds to its ‘actual’ environment. For expository
purposeswewillignorethisdistinction.
9
intention as
eb∗
ψ
q(ψ∗ |b∗)=P(ψ∗ |b∗)= (7)
eb∗
i
i
X
Thegenerativedensityassociatedwiththeagent’spredictionsabouthisnext
position ψ′ given his actual state, can be decomposed as:
p(ψ′,s|b,a)=P(ψ′,s|b,a)= P(ψ′ |s,b,a,ψ)P(s|b,a,ψ)P(ψ |b,a)
ψ∈X
X
(8)
We make some more assumptions on the environment to simplify the last con-
structs, that will be named environmental dynamics, sensory dynamics
and brain encoding (this last one following a specific assumption which will
simplify the third term into the agent’s belief at the actual time step t):
• P(ψ′ | s,b,a,ψ) = P(ψ′ | a,ψ), the position at time t +1, ψ′, is only
influenced by the action a taken from the position ψ at time t
• P(s | b,a,ψ)= P(s | ψ), the sensation scoming from the chemical source
is anobjective parameter,depending onlyonthe positionattime t inthe
world
• P(ψ |b,a)=P(ψ |b), the previous position ψ at time t is not influenced
by the action a, that only affects the subsequent ψ′ at time t+1
Theagentsenvironmental dynamics specifieshowψ(positionx)changes
as a consequence of the agent’s action a ∈ {−1,1}. With fixed probability
ρ = 0.75, the agent moves one cell in the direction determined by his action.
Otherwise, the agent remains in its current cell.
1−ρ, if ψ′ =ψ mod n
P(ψ′ |ψ,a)= ρ, if ψ′ =(ψ+a) mod n (9)

0, otherwise
Sensory Dynamics are described by the probability of the agent’s sensor
registeringHighwillbesetequaltotheconcentrationofchemicalintheagent’s
current cell ψ, which is assumed to fall exponentially with increasing distance
from the source position ψ = n/2, according to a decay parameter ω = log4
0 16
−1
and a maximum value k=416:
ke−ω|ψ−ψ0|, if s=High
P(s|ψ)= (10)
(1−ke−ω|ψ−ψ0|, if s=Low
As explained previously, the brain encoding of brain state b is a vector of
real numbers encoded using the softmax function:
ebψ
P(ψ |b)= (11)
ebi
i
X
Active inference simulations such as [18] usually assume a differential equa-
tion model in which brain state variables follow a negative IFE gradient in
10
continuoustime. Inourdiscretemodel,weusethesamegradient-descentprinci-
ple,butperformanumberofgradientdescentiterationsbetweeneachsimulated
time step in the world.
Essentially,the simulation works as follows, after selecting a learning rate η
and number of iterations k for the gradient descent:
function optimise(b,s,a)
b′ =0 ⊲ n-dimensional vector 0 represents maximal ignorance
for i∈{1···k} do
b′ ←b′−η· ∂ F(b′,b,s,a) ⊲ gradient descent on b′
∂b′
end for
return b′
end function
procedure simulate(ψ,b,b∗) ⊲ initial values for ψ,b,b∗
loop
s← random value using P(s|ψ)
a←argmin F(b∗,b,s,a) ⊲ exhaustively computed
a
b′ ← optimise(b,s,a)
ψ′ ← random value using P(ψ′ |ψ,a)
b,ψ ←b′,ψ′
end loop
end procedure
5 Results
An example of the dynamics of a simulated agent is given in Figure 2. The
trajectory of the agent over time, and its subjective confidence regarding its
location, can be seen in the graph.
Theagent’sgoalistooccupythelocationstowardsthebottomofthegraph,
which are assigned a higher intentional probability than other locations (the
intentiondistributionisshowningreyattheright-handsideofthefigure). The
agent’sonlyenvironmentalcluetoitslocationisitssensorreading,whichprob-
abilistically detects the local concentration of a chemical. The spatial gradient
of chemical concentration is shown on the left grey bar.
It can be seen that the agent is effective in simultaneous online estimation
of its location, with a brief period of confusion compensated for fairly quickly.
Itisworthnotingthatalthoughthe“physics”oftheagent-environmentsys-
temareverysimple,theagent’staskisnotcompletelytrivial. Theagent’starget
location is an arbitrary position; since the environment is symmetric, there is
another location sharing the same concentration (and therefore instantaneous
sensory statistics) as the target, which means that the task cannot be achieved
reliablybyareactiveagent. Theagentthereforeneedstocombinesimultaneous
estimation and control of its position based on limited and noisy sensorimotor
channels.
The smallerthe IFEassociatedwith the agent’sinternalstate,the closerits
inference is to the exact Bayesian posterior. We show that the degree of ap-
proximation can be controlled by varying the effectiveness of the minimisation
procedure. Figure4showsthebeliefsofdifferentagentswithincreasingnumber
ofgradientdescentiterationsbetweentime steps,onthe samesetofsensorimo-
11
Figure 2: Inferred position (shading: darker = higher credence) and actual
location (triangles) of active inference agent over time. Triangle direction indi-
cates agent’s action and triangle colour indicates sensor reading (black=high;
white=low). Left grey bar indicates chemical concentration gradient at differ-
ent locations (darker = higher concentration). Right grey bar indicates agent’s
positional target (darker = more highly desired), in this case no preference is
given.
0
2
4
6
8
10
12
14
0 20 40 60 80 100
Time
noitisoP
Approximate Inference, k=100
Figure 3: Inferred position (shading: darker = higher credence) and actual
location (triangles) of active inference agent over time. Triangle direction indi-
cates agent’s action and triangle colour indicates sensor reading (black=high;
white=low). Left grey bar indicates chemical concentration gradient at differ-
ent locations (darker = higher concentration). Right grey bar indicates agent’s
positional target (darker = more highly desired).
tor data. The exact inference is shown at the bottom for comparison. Actions
were produced randomly for this figure, so the agent is effectively inferring its
position over a random walk.
Gradientdescent is begun froma “null brain” representing the uniform dis-
tribution, so with fewer gradient descent iterations the agent’s inferences will
tend to be biased towards higher uncertainty. We can plausibly expect in-
formation stored about past sensorimotor events to decay faster with poorer
approximationsto the exactposterior(which encapsulatesallrelevantinforma-
tion from arbitrary time points). Both these phenomena can be observed in
the figure shown, with the upper graphs tending to be more blurred and more
rapidly responsive to immediate sensory input.
5.1 Inferential Quality and Task Performance
Although the agent’s quality of inference falls noticeably with poorer IFE op-
timisation, it is not a priori obvious what effect this should have on its task
performance. Figure 5, shows the typical location profile (over 500 time steps)
for agents using weak, moderate and strong optimisation procedures (charac-
12
0
2
4
6
8
10
12
14
0 20 40 60 80
Time
noitisoP
Approximate Inference, k=25
0
2
4
6
8
10
12
14
0 20 40 60 80
Time
noitisoP
Approximate Inference, k=100
0
2
4
6
8
10
12
14
0 20 40 60 80
Time
noitisoP
Approximate Inference, k=200
0
2
4
6
8
10
12
14
0 20 40 60 80
Time
noitisoP
Exact Inference
Figure 4: Comparison of approximate with exact inference for the same senso-
rimotor data. Blue graphs, top to bottom: approximate inference with 25, 50
and 200 gradient descent iterations respectively. Green graph: exact Bayesian
inference.
terised by 20, 50 or 100 gradient descent iterations between time steps). The
agents were directed to maintain their location around a particular target and
initialised in uniformly random locations.
0.20
0.15
0.10
0.05
0.00
0 2 4 6 8 10 12 14
Position
ycneuqerF
Location Profile for Active Inference Agents
k=20
k=50
k=100
Target
Figure 5: Positionfrequencies over 500 time steps for agents updating their in-
ternalstateusing20,50and100gradientdescentiterationsrespectively. Curves
are horizontally offset slightly for visual clarity. Points represent means of 300
runs; vertical bars show plus or minus one standard deviation.
In fact, the statistical behaviour of a weakly-minimising agent appears re-
13
markably similar to that of a strongly-minimising agent. This is interesting,
because it suggests that accurate localisation is not particularly important for
optimal control of position on this simple task. According to embodied / situ-
ated theories of cognition, many tasks can be accomplished by exploiting sim-
ple sensorimotor correlations directly, without the necessity for a high level of
information processing; this simulation demonstrates the phenomenon clearly.
Remember, the agent (indirectly, but provably) always selected the action on
the basis of its (theorist-attributed) Bayesianbelief.
Noneoftheagents’statisticallocationprofilesovertimecloselyapproximate
the target profile. This is an artefact of the simulation model, in which an
agent’s motor action is chosen deterministically to minimise IFE with respect
to its target belief. Consequently, since it does not use any forward planning,
the agentinvariablyattempts to movein the directionofits highestpreference,
even when preferences are closely matched. This could be easily rectified by
including probabilistic motor control, where the agent’s motor commands are
real numbers determining the probabilities of particular actions; under such a
scheme, motor commands must be minimised using a numerical optimisation
scheme like other variables.
6 Discussion
The IFE principle promises a exciting new account of action and perception
within the same powerful framework. However the influence of these ideas
has been hindered by the complexity of current formalism. Furthermore while
central strength of the original formalism is it potential to unify both action
and perception within a single theoretical framework yet to date agent based
approaches have largely been absence except see [19, 17].
While this claim may havesome heuristic merit, it is worthnoting that itis
not without issues.
• Friston’smathematicalargumentsdependontheassumptionthatsensory
statistics are ergodic (i.e. that it makes sense to conflate the statistics of
a system over a long time interval with its “typical” behaviour at any
instant); this assumption seems problematic in a biological context.
• Exportingthermodynamicentropydoesnotlogicallynecessitateminimis-
ing sensory entropy or internal system state entropy (either thermody-
namic or information-theoretic). It is entirely possible for more complex
systems to be more stable (and better at self-regulation) than simpler
systems.
Anergodicsystemisonewhosetimeaverageisthesameasitsstateaverage;
ergodicity is a physicist’s “trick” for making the maths of statistical mechanics
more tractable. However, it is pretty clear that most biological systems are
anything but ergodic in their dynamics. A human being is unlikely to experi-
ence the identically same waking sensory state twice in their lifetime. Certain
primitive aspects of sensory state may be approximately ergodic (for instance,
sensed core body temperature), but even there it seems plausible that there is
scope for significant non-ergodicity.
14
Friston’sargumentaboutminimisingentropyisverysimilartotheargument
presentedin“EveryGoodRegulatorofaSystemMustBeaModelofThatSys-
tem”, [11]. However, the authors of that paper explicitly concede that entropy
is not a universally sound measure of successful regulation:
[Entropy and RMS error] tend to be similar... though the mathe-
matician can devise examples to show that they are essentially in-
dependent.
It is perhaps unfortunate that this ergodic-entropic reasoning has been pre-
sented as the primary motivation for the active inference framework, since the
framework is consequently put in tension with the concept of information ac-
quisition as an intrinsic motivation [dark room]. This tension comes from the
mathematical fact that minimising surprisal also minimises information gain.
There is no reasonin principle why an agentproceeding according to active
inference shouldnothaveexpectations regardingits informationgainaswellas
its sensory input, and recent research by Friston and colleagues [20] explicitly
models infotaxis by exactly such a method. However, these kinds of expecta-
tions regarding higher-order temporal features of internal dynamics do not fit
wellwiththeergodic-entropicargument,whichconsidersonly“detemporalised”
statistical features of external interactions with the environment.
Moreover,theactiveinferenceframeworkcanbemotivatedbymoreabstract
considerations relating to
6.1 Learning Without Reinforcement: Where Does The
Intelligence Come From?
One ofthe moredisappointingaspectsofthe activeinferenceframeworkis that
italmostentirelyfailstoprovideamechanisticexplanationforanyspecificpat-
tern of intelligent behaviour. Under active inference, the agent’s sensorimotor
behaviour emerges from the pre-existing structure of its sensory expectances.
Alloftheagent’sintelligenceisencodedinthemechanismwhichproducesthese
expectances,aboutwhichtheactiveinferenceframeworkitselfhaslittleornoth-
ing to say.
Formachinelearningorroboticsapplications,thisisnotaviablealternative
toconventionalparadigmssuchasreinforcementlearningorsupervisedlearning.
The challenges faced in robot controllie precisely in understanding how partic-
ular desired external behaviours translate into sensory and motor patterns; in
ordertobuildarobotusingactiveinferenceprinciples,itisnecessarytospecify
exactly what the robot should expect to experience.
It seems likely that the active inference framework is powerful enough that
a reinforcement learning task could be re-formulated as an inbuilt expectance
to receive reward signals, along with an internal model whose parameters can
be modified to learn the relation between reward, action, sensation and envi-
ronmental state. However, this merely transfers the problem to the question
of how to implement such a model. It also raises the question of whether the
active inference framework is too explanatorily powerful; in other words, is the
“free-energy principle” falsifiable?
15
References
[1] P. M. Binder. “Philosophy of science: Theories of almost everything”. In:
Nature455.7215(Oct.2008),pp.884–885.issn:0028-0836.doi:10.1038/455884a.
url: http://dx.doi.org/10.1038/455884a.
[2] NickBostrom.Anthropicbias: Observation selection effectsin scienceand
philosophy. Psychology Press, 2002.
[3] Selmer Bringsjord and Michael John Zenzen. Superminds: People Har-
ness Hypercomputation, and More. Norwell,MA, USA: KluwerAcademic
Publishers, 2003. isbn: 140201094X.
[4] RodneyABrooks.Cambrian intelligence: the early history of the new AI.
The MIT Press, 1999.
[5] B. Carter and W. H. McCrea. “The Anthropic Principle and its Implica-
tionsforBiologicalEvolution[andDiscussion]”.In:Philosophical Transac-
tionsoftheRoyalSocietyofLondon.SeriesA,Mathematical andPhysical
Sciences 310.1512(Dec.1983),pp.347–363.doi:10.1098/rsta.1983.0096.
url: http://dx.doi.org/10.1098/rsta.1983.0096.
[6] Nick Chater and Paul Vita´nyi. “Simplicity: a unifying principle in cogni-
tive science?” In: Trends in cognitive sciences 7.1 (2003), pp. 19–22.
[7] Ron Chrisley. “Natural Intensions”. In: Adaptation and Representation.
2007,pp.3–11.url:http://interdisciplines.org/medias/confs/archives/archive_4.pdf.
[8] Rudi Cilibrasi and Paul MB Vita´nyi. “Clustering by compression”. In:
Information Theory, IEEE Transactions on 51.4 (2005), pp. 1523–1545.
[9] A. Clark. “Whatever next? Predictive brains, situated agents, and the
futureofcognitivescience”.In:BEHAVIORALANDBRAIN SCIENCES
(2013), pp. 1–73.
[10] Andy Clark. Being there: Putting brain, body, and world together again.
MIT press, 1998.
[11] Roger C. Conant and Ross W. Ashby. “Everygood regulatorof a system
must be a model of that system”. In: International Journal of Systems
Science 1.2 (1970), pp. 89–97. doi: 10.1080/00207727008920220. url:
http://dx.doi.org/10.1080/00207727008920220.
[12] P. Dayan, G. E. Hinton, and R. M. Neal. “The Helmholtz machine”. In:
Neural Computation 7 (1995), pp. 889–904.
[13] Karl Friston. “The free-energy principle: a unified brain theory?” In: Na-
tureReviewsNeuroscience 11.2(2010),pp.127–138.issn:1471-003X.doi:
10.1038/nrn2787. url: http://dx.doi.org/10.1038/nrn2787.
[14] Karl Friston, James Kilner, and Lee Harrison. “A free energy principle
for the brain”. In: Journal of Physiology-Paris 100.1-3 (2006). Theoret-
ical and Computational Neuroscience: Understanding Brain Functions,
pp. 70–87. issn: 0928-4257. doi: 10.1016/j.jphysparis.2006.10.001.
url:http://www.sciencedirect.com/science/article/B6VMC-4MBC500-1/2/5a0b34eb07e1bee0fb38
[15] Karl Friston et al. “Variational free energy and the Laplace approxima-
tion”. In: Neuroimage 34.1 (Jan. 2007), pp. 220–234.
16
[16] Karl J. Friston. “Hierarchical Models in the Brain.” In: PLoS Computa-
tionalBiology 4.11(2008).url:http://dblp.uni-trier.de/db/journals/ploscb/ploscb4.html#Fr
[17] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. “Reinforcement
learning or active inference?” In: PloS one 4.7 (2009), e6421.
[18] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement
Learning or Active Inference?” In: 4.7 (2009). Ed. by Olaf Sporns. issn:
1932-6203.doi:10.1371/journal.pone.0006421.url:http://dx.plos.org/10.1371/journal.pone.
[19] KarlJ.Friston,SpyridonSamothrakis,andP.ReadMontague.“Activein-
ferenceandagency:optimalcontrolwithoutcostfunctions.”In:Biological
Cybernetics 106.8-9(2012),pp.523–541.url:http://dblp.uni-trier.de/db/journals/bc/bc106.htm
[20] Karl J. Friston et al. “Perceptions as Hypotheses: Saccades as Experi-
ments”. In: 3 (2012). issn: 1664-1078.doi: 10.3389/fpsyg.2012.00151.
url:http://www.frontiersin.org/Perception\_Science/10.3389/fpsyg.2012.00151/abstract.
[21] Kurt G¨odel. “U¨ber formal unentscheidbare Sa¨tze der Principia Mathe-
matica und verwandter Systeme I”. In: Monatshefte fu¨r mathematik und
physik 38.1 (1931), pp. 173–198.
[22] Marcus Hutter. “Open problems in universal induction & intelligence”.
In: Algorithms 2.3 (2009), pp. 879–906.
[23] MarcusHutter.“UniversalAlgorithmicIntelligence:AMathematicalTop→Down
Approach”. In: Artificial General Intelligence. Ed. by B. Goertzel and C.
Pennachin. Cognitive Technologies. Berlin: Springer, 2007, pp. 227–290.
isbn: 3-540-23733-X.
[24] Edwin T Jaynes. Probability theory: the logic of science. Cambridge uni-
versity press, 2003.
[25] Kevin T Kelly. “Justification as truth-finding efficiency: how Ockham’s
Razor works”. In: Minds and Machines 14.4 (2004), pp. 485–505.
[26] S.; Kullback andR.A. Leibler.“OnInformationandSufficiency”.In: The
Annals of Mathematical Statistics 22.1 (1951), pp. 79–86.
[27] MingLiandPaulM.B.Vitanyi.AnIntroductiontoKolmogorov Complex-
ity and Its Applications. 3rd ed. Springer Publishing Company, Incorpo-
rated, 2008. isbn: 0387339981,9780387339986.
[28] John R Lucas. “Minds, machines and G¨odel”. In: Philosophy (1961),
pp. 112–127.
[29] Simon McGregor.“AlgorithmicInformationTheory andNovelty Genera-
tion”. In: Proceedings of the 4th Internation Joint Workshop on Compu-
tational Creativity. 2007,pp. 109–112.
[30] MarkusMu¨ller.“Stationaryalgorithmicprobability”.In:TheoreticalCom-
puter Science 411.1 (2010), pp. 113–130.
[31] RogerPenrose.The Emperor’s New Mind.OxfordUniversityPress,1989.
[32] SamuelRathmannerandMarcusHutter.“Aphilosophicaltreatiseofuni-
versal induction”. In: Entropy 13.6 (2011), pp. 1076–1136.
[33] J. Schmidhuber. Algorithmic Theories of Everything. Tech. rep. IDSIA-
20-00, quant-ph/0011122.Manno (Lugano), Switzerland: IDSIA, 2000.
17
[34] Ju¨rgenSchmidhuber.“DiscoveringneuralnetswithlowKolmogorovcom-
plexityandhighgeneralizationcapability”.In:NeuralNetworks 10.5(1997),
pp. 857–873.
[35] Ju¨rgenSchmidhuber.“TheSpeedPrior:anewsimplicitymeasureyielding
near-optimal computable predictions”. In: Computational Learning The-
ory. Springer. 2002, pp. 216–228.
[36] John R Searle et al. “Minds, brains, and programs”. In: Behavioral and
brain sciences 3.3 (1980), pp. 417–457.
[37] RoyA.Sorensen.Blindspots.OxfordUniversityPress,1988.isbn:0198249810
9780198249818.
[38] Tom Florian Sterkenburg. “The Foundations of Solomonoff Prediction”.
MA thesis. University of Utrecht, 2013.
[39] Alan Mathison Turing. “On computable numbers, with an application to
the Entscheidungsproblem”.In: J. of Math 58 (1936), pp. 345–363.
[40] Aron Vallinder. “SolomonoffInduction: A Solution to the Problemof the
Priors?” MA thesis. Lund University, 2012.
[41] DavidHWolpert.“Physicallimitsofinference”.In:Physica D: Nonlinear
Phenomena 237.9 (2008), pp. 1257–1281.
Appendix
A Background
A.1 Kronecker delta
Named after Leopold Kronecker, the Kronecker delta is a function of two vari-
ables, usually represented as δ :
xy
1 if x=y
δ = (12)
xy
(0 otherwise
A.2 Kullback-Leibler (KL) divergence
Usedasameasureofthe differencebetweentwoprobabilitydensities(e.g. q(x)
andp(x)), the Kullback-Leiblerdivergenceis defined bySolomonKullbackand
Richard Leibler in [26] as:
q(x)
D(q(x)||p(x))= q(x)log dx (13)
p(x)
Z
Important properties:
• D(q(x)||p(x))6=D(p(x)||q(x)) (the divergence is not symmetric)
• D(q(x)||p(x))≥0
• D(q(x)||p(x)=0 ⇐⇒ q(x)=p(x)
18
KL divergence is measured in bits (if log x), bans (if log x) or nats (if
2 10
log x), with the latters easily convertible to bits, and it can be considered as
e
a quantity which gives the number of extra bits (nats or bans) required by the
density q(x) to represent the density p(x).
B Calculating IFE and its Gradient
B.1 IFE
We define the IFE F as
P(ψ′ |b′)
F(b′,b,s,a)= P(ψ′ |b′)log
P(ψ′,s|b,a)
ψ′
X
=−E(b′,b,s,a)−H(Ψ′ |b′)
E(b′,b,s,a)= P(ψ′ |b′)logP(ψ′,s|b,a)
ψ′
X
P(ψ′,s|b,a)= P(ψ′ |ψ,a)P(s|ψ)P(ψ |b)
ψ
X
We can define Pre (ψ′) as the set of states which ψ′ can be reached from by
a
performing action a, i.e. the set {ψ : P(ψ′ | ψ,a) > 0}, which will usually be
smaller than the entire support of Ψ. Hence, we can rewrite E as
E(b′,b,s,a)= P(ψ′ |b′)log P(ψ′ |ψ,a)P(s|ψ)P(ψ |b)
 
X
ψ′ ψ∈P
X
rea(ψ′)
 
B.2 Partial Derivatives
With the free energy F defined as
q(ψ′ |b′)
F(b′,b,s,a)= q(ψ′ |b′)ln
p(ψ′,s|b,a)
ψ′
X
the partial derivatives for each possible belief b′ are
j
∂F(b′,b,s,a) ∂q(ψ′ |b′) q(ψ′ |b′)
= 1+ln (14)
∂b′ ∂b′ p(ψ′,s|b,a)
j ψ′∈X j (cid:18) (cid:19)
X
where
∂q(ψ′ |b′)
∂b′ =q(ψ′ |b′)(δ ψ′j −q(j |b′ j ))
j
with δ ψ′j as the Kroneckerdelta described in A.1.
19