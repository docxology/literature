arXiv:1503.04187v1  [cs.AI]  13 Mar 2015
A Minimal Active Inference Agent
Simon McGregor1, Manuel Baltieri1 and Christopher L. Buckley1
1University of Sussex, Brighton, UK
s.mcgregor@sussex.ac.uk, m.baltieri@sussex.ac.uk, c.l.buckley@sussex.ac.uk
March 16, 2015
Abstract
Research on the so-called “free-energy principle” (FEP) in cognitive
neuroscience is becoming increasingly high-proﬁle. To dat e, introductions
to this theory have proved diﬃcult for many readers to follow , but it
depends mainly upon two relatively simple ideas: ﬁrstly tha t normative
or teleological values can be expressed as probability dist ributions (ac-
tive inference), and secondly that approximate Bayesian re asoning can be
eﬀectively performed by gradient descent on model paramete rs (the free-
energy principle). The notion of active inference is of grea t interest for a
number of disciplines including cognitive science and arti ﬁcial intelligence,
as well as cognitive neuroscience, and deserves to be more wi dely known.
This paper attempts to provide an accessible introduction t o active
inference and informational free-energy, for readers from a range of scien-
tiﬁc backgrounds. In this work introduce an agent-based mod el with an
agent trying to make predictions about its position in a one- dimensional
discretized world using methods from the FEP.
1 Introduction
Active inference is the name given by Karl Friston to a process of an agent’s
simultaneously co-constrained inference regarding the world comb ined and di-
rected actions to change it, understood speciﬁcally as the minimisat ion of an
information-theoretic quantity describing the instantaneous rela tion of a system
to its “typical” sensory environment.
According to proponents of active inference, all mechanical implem entation
of cognitive behaviour (including both the active and inferential asp ects of cog-
nition) can be reduced to the minimisation of a single informational “fr ee en-
ergy” quantity; moreover, this quantity can be assigned an interp retation using
cognitive concepts resembling beliefs and intentions.
This theory has attracted a great deal of attention (both positiv e and neg-
ative), since it claims to unify a wide variety of principles in cognitive scie nce,
machine learning and neuroscience. However, a clear and balanced d iscussions
of the framework’s strengths and weaknesses is still lacking.
We suspect that one of the barriers preventing a wider understan ding of the
theory is the language which been used to describe it. The language is tech-
nically dense, using terms like ’surprise’, ’generative’, ’ensemble’, ’varia tional’,
1
and can seem to beg philosophical questions in a way which is not strict ly
mandated by the formalism of the active inference theory. For inst ance, the
framework is frequently characterised as claiming that cognitive ag ents act so
as to “conﬁrm their predictions”, or “minimise their surprise”; we will see that
alternative interpretations exist for the same formal assertions .
Furthermore one of the central strengths of the formalism is its p otential
to unite an understanding oﬀ action and perception within a single uniﬁ ed
framework [13]. However understanding the exact implications of th is will likely
require the utilisation of agent based approaches which to date hav e largely been
absent.
This paper aims to clarify the foundations of the active inference or “free-
energy” framework in terms which can be understood by a wide audie nce. We
start by giving a brief conceptual overview of the information Free -Energy (IFE)
principle and active inference in an intuitive language
We ground our discussion by developing a Free-Energy formalism for simple
active agent behaving in one-dimensional discrete-time world. This a gent based
approach allows us to examine the relationship between action and pe rception
that emerges from the interaction of the agents’ internal model and the world
around it. We ﬁnish by the outlining the main contributions and the kno wn
current limitations of the IFE . In particular we will address some of t he claims
made regarding free-energy or active inference, and attempt to deﬂate them in
a way which may help readers make more sense of the underlying ideas .
The foundational premise of the IFE principle is that adaptive agent s are
deﬁned by their ability occupy only a limited repertoire of physical sta tes [13].
Adaptive agents achieve this by exporting thermodynamic entropy (thus resist-
ing the third law) and preserving the deﬁning traits that comprise th eir very
identity. However agents do not have direct access to states tha t constitute
there world and body and instead must work vicariously on sensory d ata do
this. Formally speaking, the active inference framework proposes that agents
do this by acting to minimise an information-theoretic quantity derive d from
sensory data known as surprisal. The interpretation of this quant ity is some-
what subtle, and it should absolutely not be conﬂated with the psych ological
phenomenon we call surprise.
Friston has previously asserted that agents which export thermo dynamic
entropy must do so by minimising the informational entropy of their s ensory
inputs (considered as an empirical distribution sampled over time). H e argues
that the only tractable way of doing so is to minimise informational fre e energy.
Thus, the very fact that a living system maintains its organisation in t he face
of environmental perturbation supposedly justiﬁes adverting to the “free-energy
principle” [14].
One of the most interesting aspects of the active inference frame work is that
two things which we generally regard as quite distinct are given the sa me formal
treatment: on the one hand, beliefs or predictions about sensory input and the
external world which underlies it, and on the other hand intentions r egarding
the outcomes of our actions. In this sense, active inference blurs the distinction
between a prediction and an intention; both are treated as concep tions-of-the-
future, with the main diﬀerence being in what physical variables chan ge most
when the reality turns out to be diﬀerent from the expectation [13 ].
Although the free-energy literature uses the terms “prediction” and “expec-
tation” interchangeably for this concept, it seems that only the ve rb “expect” in
2
English really captures the ambiguity between intention and predictio n which
the active inference framework rests upon. It is unfortunate th at the word “ex-
pectation” also has a technical meaning which is relevant to active inf erence,
i.e. the mean of a probability distribution. Hence, we propose the ter m “ex-
pectance” to refer to the superclass of probabilistic beliefs and desires implicit
in the active inference framework.
In this language, active inference asserts that agents constant ly behave so
as to reduce the discrepancy between their expectances and rea lity; the resolu-
tion of discrepancy by changing internal variables is known as “upda ting one’s
predictions”, while the resolution of discrepancy by changing reality is known
as “acting on one’s environment”.
Crucially, under active inference both of those dynamics are always occur-
ring simultaneously. It is not necessary to pre-judge which of thos e dynamics
will predominate by calling the expectance a “prediction” or an “inten tion” in
advance of observing the outcome.
The typical presentation of active inference states that agents behave so
as to make their “predictions” come true. We believe this wording era ses the
distinction between internal and external resolution of expectan ce violation, in
an unhelpful way. Intuitively, if an agent “predicts” its arm to move to the left,
and the arm instead stays in place, we would expect it to change inter nally so
as to make diﬀerent predictions; no action on the world would be pres upposed
by the discrepancy. On the other hand, if the agent “intends” its a rm to move
to the left, and the arm instead stays in place, we would expect it to e xert a
greater force on its arm by contracting the relevant muscles.
Of course, it is entirely possible that the agent could begin by attemp ting
to move its arm and, in doing so, learn that its range of movement is re stricted
by external forces. The expectance violation can be resolved by c oming to a
halfway point in which the agent “updates its predictions” to reﬂect the fact
that its movement is restricted, “modiﬁes its intentions” to be cons istent with
what is achievable, and “acts on the world” by moving within its available
range. Such an equilibrium can emerge naturally from a continuous-t ime process
where informational free energy is minimised along distinct internal d imensions
simultaneously.
Recent discussions of active inference have tended to occur within the context
of a neuroscientiﬁc theory known as the predictive coding hypothe sis [9]. The
predictive coding idea is essentially this: the main function of sensory neurons is
to predict their sensory inputs (using eﬀerent signals from higher b rain regions)
and report their prediction errors, with the consequence that aﬀ erent sensory
signals are nonzero only when they carry new information. The hiera rchical
predictive coding hypothesis additionally posits that this mode of ope ration
extends to multiple layers of neurons, with each layer providing input for the
next.
Friston states that, if one makes a number of (non-trivial) mathem atical
assumptions and approximations, active inference via informationa l free-energy
minimisation can be implemented in a hierarchical predictive coding neur al
model. Within this hierarchical predictive coding model for active infe rence
[16], eﬀerent (motor) signals are treated similarly to other signals. T he work-
ings of this process can be a little diﬃcult to understand at ﬁrst; the idea is
that the neural architecture imposes certain speciﬁc patterns o f expectances on
instantaneous kinesthetic sensations [13]. These expectances co rrespond to the
3
agent’s desired behaviour, such that the expectancy error direc tly becomes a
motor signal.
Unfortunately, it is not at all clear that a direct mapping between ex pectancy
error on kinesthetic sensation and motor signal makes sense within a general
framework of cognition. Firstly, the brain’s kinesthetic model is lear ned, rather
than genetically determined, and consequently its relationships with motor sig-
nals need to be learned as well. For active inference to work as a unive rsal
explanatory principle, the formalism needs to be able to explain how th is map-
ping arises in the ﬁrst place; an isomorphism between the semantic me aning of
motor signals and the semantic meaning of kinesthetic sensations ca nnot simply
be assumed.
Secondly, general theories of cognition ought to be widely relevant ; their
application cannot be restricted to humans, mammals or even animals . The
principles which underlie adaptive behaviour extend also to plants, sin gle-celled
organisms, and arguably even to systems such as social insect colo nies or non-
biological dissipative structures. How does one model the error in k inesthetic
sensory expectance of a bacterium? Or a swarm of bees? This prob lem becomes
most evident when one attempts to construct a minimal simulation ba sed on
active inference.
Fortunately, the issue is caused by not intrinsic to the active infere nce frame-
work itself, but only by the simpliﬁcation which treats kinesthetic sen sation as
a special mode of sensation. If this assumption is not made, the fre e-energy
equations give a general solution: ﬁlter the choice of action throug h the agent’s
(current provisional) model of action’s eﬀect on the whole of its sen sorium. We
show in this paper that such an approach works perfectly well for a simple
simulated artiﬁcial agent.
2 The Free Energy Formalism
State variables Meaning
Ψ i,ψi World state at time ti
Bi,bi Internal (brain) state at time ti
Si,si Sensory input at time ti
Ai,ai Motor trajectory between ti and ti+1
Table 1: State variables
Following the work by Dayan et al. [12] on the Helmholtz machine, we deﬁ ne
2 densities that will constitute the free energy term to be optimised :
•a generative density p(ψ,s |m) representing the joint probability of world
states ψ and sensory input s based on a probabilistic predictive model m
by the agent or brain
•a recognition density q(ψ | b) encoding the agent’s (or brain’s) beliefs
about the causes ψ with a set of brain states b that fully describe these
beliefs.
4
IFE is then deﬁned as:
F(s,b) =
∫
ψ
q(ψ|b) ln q(ψ|b)
p(ψ,s |m) dψ (1)
Optimising this term requires then a brain (agent) to be able to
•change its perception, using the internal states bto alter its beliefs q(ψ|
b) thus enhancing its ability of explaining the world causes as they are
produced by the generative model, or
•directing its actions to result in diﬀerent sensory input in the generative
density p(ψ,s |m) that match the agent’s beliefs.
Two alternative analytical forms for the IFE better show depende nce on
perception and action respectively, helping us to understand the c ontributions
to the free energy minimisation of both:
2.1 Perception
F(s,b) = D(q(ψ|b) ||p(ψ|s,m)) −ln p(s|m) (2)
where D(q(ψ |b) ||p(ψ |s,m)) is the Kullback-Leibler (KL) divergence
(deﬁned in the appendix A.2), between the recognition density ( q(ψ |b)) and
the true posterior of the world states ( p(ψ |s); −ln p(s |m) is the surprise
about the sensory input the brain cannot directly evaluate. Chang ing the set
of brain states b allows to approximate the posterior of the world states with
the brain’s beliefs about the world. In the ideal case where the two c oincide,
free energy would be equal to the surprise (the KL divergence wou ld be zero,
appendix[A.2]) thus being able to interpret this term otherwise hidden to the
brain.
The optimal internal state bopt is deﬁned as:
bopt = arg min
b
F(s,b)
2.2 Action
An alternative form shows instead how the free energy depends on sensory input,
that can be thought as dependent on a a which represent the set of actions a
system can perform in a certain environment, giving then s( a).
F(s,b) = D(q(ψ|b) ||p(ψ|m)) −⟨ln p(s(a) |ψ,m)⟩q (3)
with D(q(ψ|b) ||p(ψ|m)), KL divergence between the recognition density
q(ψ|b) (i.e. posterior belief about the causes/world states) and the prio r (belief)
p(ψ|m)) of the world states ψ; ⟨ln p(s(a) |ψ,m)⟩q being the expectation about
sensations s( a) under the density q. In this formulation we can see how free
energy is minimised using an optimum action aopt, that will sample inputs as
predicted by the recognition density.
aopt = arg min
a
F(˜ s(a),b)
5
3 A discretized approach to the FEP
In this work we will consider a discrete representation of our state variables,
deﬁning speciﬁcally Ψ ′,ψ′ and B′,b′ respectively as world and internal states at
time ti+1.
In its discrete version, Informational Free Energy (IFE) become s a quantity
relating an action a and a sensation s to two possible internal states b and b′
based on a generative and a recognition densities which are attribut ed to the
agent by the theorist.
Densities Name
q(Ψ ′ |B′) = P(Ψ ′ |B′) Recognition density
p(Ψ ′,S |B,A) = P(Ψ ′,S |B,A) Generative density
Table 2: Key probability densities
The recognition density described the agent’s internal “encoding” of external
world states, while the generative density represents the agent’s “predictive
model” of physical dynamics.
Most active inference papers use a continuous-space deﬁnition of the infor-
mational free energy F[17, 14, 13], but in this paper we will consider a discrete-
space version:
F(b′,b,s,a ) =
∑
ψ′
q(ψ′ |b′) ln q(ψ′ |b′)
p(ψ′,s |b,a) (4)
IFE has several interesting properties, corresponding to diﬀere nt rearrange-
ments of the formula, which give rise to diﬀerent interpretations [1 3] as we
described in the previous section. We will focus in this section on its ro le in
approximate Bayesian ﬁltering (i.e. ongoing inference about a chang ing world
state). Following our discretized deﬁnition, the equation can be rew ritten as
F(b′,b,s,a ) = DKL(q(ψ′ |b′) ||p(ψ′ |s,b,a )) −ln p(s|b,a) (5)
where the ﬁrst term is the KL divergence between q(ψ′ |b′) and p(ψ′ |s,b,a ),
as deﬁned previously. The second one is the informational surprisa l about the
sensory inputs s that cannot be directly evaluated by the agent. Since the KL
divergence is always a positive quantity, free energy represent an upper bound
of surprisal, meaning that by minimising IFE we indirectly optimise surpr isal
at the same time.
Assuming the agent does not expect sensation si at time ti to depend on
action ai that originates a movement from time ti to time tt+1 , i.e. p(s|b,a) =
p(s |b), the second term is independent of b′ and a. This has two interesting
consequences:
1. Approximate Inference Minimising F(b1,b0,s0,a0) with respect to b1 cor-
responds to approximate Bayesian inference regarding the state of the
world at time t1.
6
2. Optimal Control Minimising F(b∗,b0,s0,a0) with respect to a0 corre-
sponds to optimal control (subject to the agent’s assumptions a bout world
dynamics) if b∗ encodes a desired distribution over world states at time
t1.
Note that the second statement draws a distinction between belief P(ψ1 |b1)
and desire P(ψ∗ |b∗) which does not reﬂect the original maths of the active
inference framework [18]. This will be helpful for expository purpos es. Mecha-
nisms by which goal-directed action can be encoded in the pure active inference
formalism, without distinguishing predictions from intentions, will be d iscussed
later.
We have not mentioned the actual dynamics of the environment; in f act, they
are not necessary for this analysis. We assume that the only interf ace between
the agent and its environment is the agent’s sensorimotor dynamics ; hence, we
are free to consider how the agent’s (attributed) cognitive dynam ics proceed
when it is fed an arbitrary series of sensory inputs by an experiment er resem-
bling Descartes’ “malicious demon”. In either case, we expect that the agent’s
behaviour should be approximately rational, given the beliefs and inte ntions we
have attributed it.
3.1 Practical IFE Minimisation
The value of IFE minimisation as a computational model of active infer ence,
either for machine learning purposes or in neuroscience modelling, de pends on
how realistic proposed mechanisms for minimising IFE are. In the gene ral case,
IFE minimisation is not much more computationally tractable than the s um
required to perform exact Bayesian inference, because it involves a sum (or,
in the continuous case, integral) over all possible world states. It is of course
conceivable that the brain can implicitly compute this integral throug h physical
means which are expensive to simulate computationally. However, IF E is sig-
niﬁcantly easier to compute when certain certain conditions are met regarding
how the agent-environment dynamics work and how the agent repr esents world
states. These conditions, along with additional approximations, allo w IFE min-
imisation to form a tractable computational approach to active infe rence [15,
16] and are argued [13] to provide an elegant model of neural func tion.
3.2 Unifying Beliefs and Desires
We have indicated why minimising F(b1,b0,s0,a0) with respect to b1 consti-
tutes approximate Bayesian ﬁltering, and why minimising F(b∗,b0,s0,a0) with
respect to a0 constitutes optimal control. In a machine learning context, it will
frequently be convenient to maintain a distinction between belief and intention,
and the computational cost of maintaining the distinction is small, bec ause the
function F (and its derivatives) can be called with arbitrary arguments.
However, the free-energy formulation in neuroscience postulate s a diﬀerent
mechanism. The same IFE term is used to provide a gradient term which is
applied to the dynamics of both internal state and action [13]. Argua bly, this
is more biologically plausible since an agent has only one brain state, but the
interpretation is less clear.
7
4 A Minimal Free-Energy Agent
This section outlines in detail how the active inference formalism work s for a
“minimal” abstract model of active agency. Agents are modelled as o rganisms
that inhabit a single discrete space (a “cell”) in a one-dimensional disc rete-time
world, and which are sensitive to a chemical which occurs in their envir onment.
The world has periodic boundary conditions (i.e. it “wraps around” at the
edges), and each cell contains a concentration of the chemical. Th e concentra-
tion of the chemical follows a gradient across the environment, bein g highest
in a “source” cell and lowest in the cell furthest from the source; if the reader
likes, this can be imagined as the result of a diﬀusion process.
The agent’s interactions with its environment are extremely low-ban dwidth:
it has a 1-bit sensor, which ﬁres with a probability proportional to th e chemical’s
concentration, and a 1-bit motor, which attempts to move it one wa y or the other
along the 1d world. We will arbitrarily model the agent as having a “des ire” to
move to a particular spot in its world (relative to the “source”).
This system is simple enough that exact Bayesian inference can be pe r-
formed directly, making approximate schemes such as free-energ y minimisation
unnecessary. Hence, free-energy minimisation for this system is d one purely
for didactic purposes, with the advantage that the results can be compared to
exact Bayesian posteriors. For more complex systems, exact Bay esian inference
rapidly becomes intractable, and approximate methods are necess ary.
Figure 1: Illustration of agent-environment system. The agent ha s a sensor
which reads High or Lowand is sensitive to chemical concentration. The agent’s
motor can attempt to move the agent clockwise or anticlockwise.
4.1 Deﬁnitions
We will begin by describing the simulation framework for our agents an d the
environments they inhabit. The agent-environment dynamics are m odelled in
discrete time steps. The agent is represented as a system with an in ternal ‘brain’
8
state b. After emitting an action a, the agent receives a sensory input s and
updates its brain to a new state b′ based on b, a and s. It then emits a new
action a′ based on b′, and the cycle starts again.
It will sometimes be worth considering how the agent’s dynamics proc eed
when it is fed an arbitrary series of sensory inputs by an experiment er resembling
Descartes’ “malicious demon”, but in general we are interested in t he coupled
dynamics of an agent and its “natural” environment. The environme nt will be
represented as a system with a state ψ, which changes to state ψ′ depending
on its current state and the agent’s action, and the sensory input of the agent
depends on the instantaneous state of the environment.
This is a fairly standard approach to modelling discrete-time agency, but in
fact there is nothing “agentive” in the equations - they describe an arbitrary
coupled dynamical system. As cognitive scientists we will be assigning a cogni-
tive interpretation to the agent and its dynamics; this interpretation can (and
should) be distinguished from the bare ‘physical’ dynamics of the age nt system.
In this model, a key part of the interpretation will be the assertion t hat each
brain state ‘encodes’ a belief about the world 1. Following Bayesian principles,
the belief will take a probabilistic form: instead of holding only clear-cu t beliefs,
the agent’s brain will encode beliefs that can have varying degrees o f uncertainty.
It is important to note that these probabilities are understood as d egrees of
subjective uncertainty for the agent, not as the ‘objective’ probability of any
simulated outcome.
The agent-environment system’s ‘physical’ dynamics can be directly ob-
served over time, producing a series of brain states. The theorist can then
project these brain states onto a corresponding series of Bayes ian beliefs: a “be-
lief dynamics” of the agent system under a particular cognitive inter pretation.
4.2 Formulation
In our case, the environment is modelled as having a length of n= 16 cells and
its state ψ consists only of the agent’s position x (ψ = x). The agent’s brain
state b will therefore encode a probability distribution P(ψ |b) over possible
cell locations.
The recognition density, which encodes the agent’s internal beliefs about his
position at time t+ 1 will be described as follows. The agent’s brain state b′ is
a vector of real numbers b′
1,··· ,b′
n and we use a softmax encoding:
q(ψ′ |b′) = P(ψ′ |b′) = eb′
ψ
∑
i
eb′
i
(6)
As described the section above, for an active inference agent we d eﬁne a sort of
target brain state: the state the agent would “prefer” its brain t o be in, subject
to the constraint that its brain is updated rationally. Another inter pretation
is that it encodes the location distribution which the agent would pref er to
occupy over an ensemble of possible Universes. Consequently we wr ite an agents
1In principle, we should distinguish the “agent’s world”, about which it can have beliefs,
from the “theorist’s world” that corresponds to its ‘actual’ environment. For expository
purposes we will ignore this distinction.
9
intention as
q(ψ∗ |b∗) = P(ψ∗ |b∗) = eb∗
ψ
∑
i
eb∗
i
(7)
The generative density associated with the agent’s predictions abo ut his next
position ψ′ given his actual state, can be decomposed as:
p(ψ′,s |b,a) = P(ψ′,s |b,a) =
∑
ψ∈X
P(ψ′ |s,b,a,ψ )P(s|b,a,ψ )P(ψ|b,a)
(8)
We make some more assumptions on the environment to simplify the las t con-
structs, that will be named environmental dynamics, sensory dynamics
and brain encoding (this last one following a speciﬁc assumption which will
simplify the third term into the agent’s belief at the actual time step t):
•P(ψ′ |s,b,a,ψ ) = P(ψ′ |a,ψ), the position at time t+ 1, ψ′, is only
inﬂuenced by the action a taken from the position ψ at time t
•P(s|b,a,ψ ) = P(s|ψ), the sensation scoming from the chemical source
is an objective parameter, depending only on the position at time tin the
world
•P(ψ|b,a) = P(ψ|b), the previous position ψ at time t is not inﬂuenced
by the action a, that only aﬀects the subsequent ψ′ at time t+ 1
The agents environmental dynamicsspeciﬁes how ψ(position x) changes
as a consequence of the agent’s action a ∈ {−1,1}. With ﬁxed probability
ρ = 0 .75, the agent moves one cell in the direction determined by his action .
Otherwise, the agent remains in its current cell.
P(ψ′ |ψ,a) =





1 −ρ, if ψ′ = ψ mod n
ρ, if ψ′ = ( ψ+ a) mod n
0, otherwise
(9)
Sensory Dynamics are described by the probability of the agent’s sensor
registering High will be set equal to the concentration of chemical in the agent’s
current cell ψ, which is assumed to fall exponentially with increasing distance
from the source position ψ0 = n/2, according to a decay parameter ω = log 4
16
and a maximum value k= 4
−1
16 :
P(s|ψ) =
{
ke−ω|ψ−ψ0|, if s= High
1 −ke−ω|ψ−ψ0|, if s= Low (10)
As explained previously, the brain encoding of brain state bis a vector of
real numbers encoded using the softmax function:
P(ψ|b) = ebψ
∑
i
ebi
(11)
Active inference simulations such as [18] usually assume a diﬀerentia l equa-
tion model in which brain state variables follow a negative IFE gradient in
10
continuous time. In our discrete model, we use the same gradient-d escent princi-
ple, but perform a number of gradient descent iterations between each simulated
time step in the world.
Essentially, the simulation works as follows, after selecting a learning rate η
and number of iterations k for the gradient descent:
function optimise(b,s,a )
b′ = 0 ⊲ n-dimensional vector 0 represents maximal ignorance
for i∈{1 ···k}do
b′ ←b′ −η· ∂
∂b′ F(b′,b,s,a ) ⊲ gradient descent on b′
end for
return b′
end function
procedure simulate(ψ,b,b ∗) ⊲ initial values for ψ,b,b ∗
loop
s←random value using P(s|ψ)
a←arg minaF(b∗,b,s,a ) ⊲ exhaustively computed
b′ ←optimise(b,s,a )
ψ′ ←random value using P(ψ′ |ψ,a)
b,ψ ←b′,ψ′
end loop
end procedure
5 Results
An example of the dynamics of a simulated agent is given in Figure 2. The
trajectory of the agent over time, and its subjective conﬁdence regarding its
location, can be seen in the graph.
The agent’s goal is to occupy the locations towards the bottom of t he graph,
which are assigned a higher intentional probability than other locatio ns (the
intention distribution is shown in grey at the right-hand side of the ﬁg ure). The
agent’s only environmental clue to its location is its sensor reading, w hich prob-
abilistically detects the local concentration of a chemical. The spatia l gradient
of chemical concentration is shown on the left grey bar.
It can be seen that the agent is eﬀective in simultaneous online estima tion
of its location, with a brief period of confusion compensated for fair ly quickly.
It is worth noting that although the “physics” of the agent-enviro nment sys-
tem are very simple, the agent’s task is not completely trivial. The age nt’s target
location is an arbitrary position; since the environment is symmetric, there is
another location sharing the same concentration (and therefore instantaneous
sensory statistics) as the target, which means that the task can not be achieved
reliably by a reactive agent. The agent therefore needs to combine simultaneous
estimation and control of its position based on limited and noisy senso rimotor
channels.
The smaller the IFE associated with the agent’s internal state, the closer its
inference is to the exact Bayesian posterior. We show that the deg ree of ap-
proximation can be controlled by varying the eﬀectiveness of the min imisation
procedure. Figure 4 shows the beliefs of diﬀerent agents with incre asing number
of gradient descent iterations between time steps, on the same se t of sensorimo-
11
Figure 2: Inferred position (shading: darker = higher credence) a nd actual
location (triangles) of active inference agent over time. Triangle dir ection indi-
cates agent’s action and triangle colour indicates sensor reading (b lack=high;
white=low). Left grey bar indicates chemical concentration gradie nt at diﬀer-
ent locations (darker = higher concentration). Right grey bar indic ates agent’s
positional target (darker = more highly desired), in this case no pre ference is
given.
0 20 40 60 80 100
Time
0
2
4
6
8
10
12
14Position
Approximate Inference, k=100
Figure 3: Inferred position (shading: darker = higher credence) a nd actual
location (triangles) of active inference agent over time. Triangle dir ection indi-
cates agent’s action and triangle colour indicates sensor reading (b lack=high;
white=low). Left grey bar indicates chemical concentration gradie nt at diﬀer-
ent locations (darker = higher concentration). Right grey bar indic ates agent’s
positional target (darker = more highly desired).
tor data. The exact inference is shown at the bottom for comparis on. Actions
were produced randomly for this ﬁgure, so the agent is eﬀectively in ferring its
position over a random walk.
Gradient descent is begun from a “null brain” representing the unif orm dis-
tribution, so with fewer gradient descent iterations the agent’s inf erences will
tend to be biased towards higher uncertainty. We can plausibly expe ct in-
formation stored about past sensorimotor events to decay fast er with poorer
approximations to the exact posterior (which encapsulates all rele vant informa-
tion from arbitrary time points). Both these phenomena can be obs erved in
the ﬁgure shown, with the upper graphs tending to be more blurred and more
rapidly responsive to immediate sensory input.
5.1 Inferential Quality and Task Performance
Although the agent’s quality of inference falls noticeably with poorer IFE op-
timisation, it is not a priori obvious what eﬀect this should have on its t ask
performance. Figure 5, shows the typical location proﬁle (over 50 0 time steps)
for agents using weak, moderate and strong optimisation procedu res (charac-
12
0 20 40 60 80
Time
0
2
4
6
8
10
12
14 Position
Approximate Inference, k=25
0 20 40 60 80
Time
0
2
4
6
8
10
12
14 Position
Approximate Inference, k=100
0 20 40 60 80
Time
0
2
4
6
8
10
12
14 Position
Approximate Inference, k=200
0 20 40 60 80
Time
0
2
4
6
8
10
12
14 Position
Exact Inference
Figure 4: Comparison of approximate with exact inference for the s ame senso-
rimotor data. Blue graphs, top to bottom: approximate inference with 25, 50
and 200 gradient descent iterations respectively. Green graph: e xact Bayesian
inference.
terised by 20, 50 or 100 gradient descent iterations between time s teps). The
agents were directed to maintain their location around a particular t arget and
initialised in uniformly random locations.
0 2 4 6 8 10 12 14
Position
0.00
0.05
0.10
0.15
0.20Frequency
Location Profile for Active Inference Agents
k =20
k =50
k =100
T arget
Figure 5: Position frequencies over 500 time steps for agents upda ting their in-
ternal state using 20, 50 and 100 gradient descent iterations res pectively. Curves
are horizontally oﬀset slightly for visual clarity. Points represent m eans of 300
runs; vertical bars show plus or minus one standard deviation.
In fact, the statistical behaviour of a weakly-minimising agent appe ars re-
13
markably similar to that of a strongly-minimising agent. This is interest ing,
because it suggests that accurate localisation is not particularly imp ortant for
optimal control of position on this simple task. According to embodie d / situ-
ated theories of cognition, many tasks can be accomplished by explo iting sim-
ple sensorimotor correlations directly, without the necessity for a high level of
information processing; this simulation demonstrates the phenome non clearly.
Remember, the agent (indirectly, but provably) always selected th e action on
the basis of its (theorist-attributed) Bayesian belief.
None of the agents’ statistical location proﬁles over time closely ap proximate
the target proﬁle. This is an artefact of the simulation model, in which an
agent’s motor action is chosen deterministically to minimise IFE with res pect
to its target belief. Consequently, since it does not use any forwar d planning,
the agent invariably attempts to move in the direction of its highest p reference,
even when preferences are closely matched. This could be easily rec tiﬁed by
including probabilistic motor control, where the agent’s motor comma nds are
real numbers determining the probabilities of particular actions; un der such a
scheme, motor commands must be minimised using a numerical optimisa tion
scheme like other variables.
6 Discussion
The IFE principle promises a exciting new account of action and perce ption
within the same powerful framework. However the inﬂuence of the se ideas
has been hindered by the complexity of current formalism. Further more while
central strength of the original formalism is it potential to unify bo th action
and perception within a single theoretical framework yet to date ag ent based
approaches have largely been absence except see [19, 17].
While this claim may have some heuristic merit, it is worth noting that it is
not without issues.
•Friston’s mathematical arguments depend on the assumption that sensory
statistics are ergodic (i.e. that it makes sense to conﬂate the stat istics of
a system over a long time interval with its “typical” behaviour at any
instant); this assumption seems problematic in a biological context.
•Exporting thermodynamic entropy does not logically necessitate min imis-
ing sensory entropy or internal system state entropy (either th ermody-
namic or information-theoretic). It is entirely possible for more com plex
systems to be more stable (and better at self-regulation) than sim pler
systems.
An ergodic system is one whose time average is the same as its state a verage;
ergodicity is a physicist’s “trick” for making the maths of statistical mechanics
more tractable. However, it is pretty clear that most biological sys tems are
anything but ergodic in their dynamics. A human being is unlikely to expe ri-
ence the identically same waking sensory state twice in their lifetime. C ertain
primitive aspects of sensory state may be approximately ergodic (f or instance,
sensed core body temperature), but even there it seems plausible that there is
scope for signiﬁcant non-ergodicity.
14
Friston’s argument about minimising entropy is very similar to the argu ment
presented in “Every Good Regulator of a System Must Be a Model of That Sys-
tem”, [11]. However, the authors of that paper explicitly concede t hat entropy
is not a universally sound measure of successful regulation:
[Entropy and RMS error] tend to be similar... though the mathe-
matician can devise examples to show that they are essentially in-
dependent.
It is perhaps unfortunate that this ergodic-entropic reasoning h as been pre-
sented as the primary motivation for the active inference framewo rk, since the
framework is consequently put in tension with the concept of inform ation ac-
quisition as an intrinsic motivation [dark room]. This tension comes from the
mathematical fact that minimising surprisal also minimises information gain.
There is no reason in principle why an agent proceeding according to a ctive
inference should not have expectations regarding its information g ain as well as
its sensory input, and recent research by Friston and colleagues [2 0] explicitly
models infotaxis by exactly such a method. However, these kinds of expecta-
tions regarding higher-order temporal features of internal dyn amics do not ﬁt
well with the ergodic-entropic argument, which considers only “det emporalised”
statistical features of external interactions with the environme nt.
Moreover, the active inference framework can be motivated by mo re abstract
considerations relating to
6.1 Learning Without Reinforcement: Where Does The
Intelligence Come From?
One of the more disappointing aspects of the active inference fram ework is that
it almost entirely fails to provide a mechanistic explanation for any spe ciﬁc pat-
tern of intelligent behaviour. Under active inference, the agent’s s ensorimotor
behaviour emerges from the pre-existing structure of its sensor y expectances.
All of the agent’s intelligence is encoded in the mechanism which produc es these
expectances, about which the active inference framework itself h as little or noth-
ing to say.
For machine learning or robotics applications, this is not a viable altern ative
to conventional paradigms such as reinforcement learning or supe rvised learning.
The challenges faced in robot control lie precisely in understanding h ow partic-
ular desired external behaviours translate into sensory and moto r patterns; in
order to build a robot using active inference principles, it is necessar y to specify
exactly what the robot should expect to experience.
It seems likely that the active inference framework is powerful eno ugh that
a reinforcement learning task could be re-formulated as an inbuilt ex pectance
to receive reward signals, along with an internal model whose param eters can
be modiﬁed to learn the relation between reward, action, sensation and envi-
ronmental state. However, this merely transfers the problem to the question
of how to implement such a model. It also raises the question of wheth er the
active inference framework is too explanatorily powerful; in other w ords, is the
“free-energy principle” falsiﬁable?
15
References
[1] P. M. Binder. “Philosophy of science: Theories of almost everyth ing”. In:
Nature 455.7215 (Oct. 2008), pp. 884–885. issn: 0028-0836. doi: 10.1038/455884a.
url: http://dx.doi.org/10.1038/455884a.
[2] Nick Bostrom. Anthropic bias: Observation selection eﬀects in science an d
philosophy. Psychology Press, 2002.
[3] Selmer Bringsjord and Michael John Zenzen. Superminds: People Har-
ness Hypercomputation, and More . Norwell, MA, USA: Kluwer Academic
Publishers, 2003. isbn: 140201094X.
[4] Rodney A Brooks. Cambrian intelligence: the early history of the new AI .
The MIT Press, 1999.
[5] B. Carter and W. H. McCrea. “The Anthropic Principle and its Implic a-
tions for Biological Evolution [and Discussion]”. In: Philosophical Transac-
tions of the Royal Society of London. Series A, Mathematical and Physical
Sciences 310.1512 (Dec. 1983), pp. 347–363. doi: 10.1098/rsta.1983.0096.
url: http://dx.doi.org/10.1098/rsta.1983.0096.
[6] Nick Chater and Paul Vit´ anyi. “Simplicity: a unifying principle in cogn i-
tive science?” In: Trends in cognitive sciences 7.1 (2003), pp. 19–22.
[7] Ron Chrisley. “Natural Intensions”. In: Adaptation and Representation .
2007, pp. 3–11. url: http://interdisciplines.org/medias/confs/archives/archive_4.pdf.
[8] Rudi Cilibrasi and Paul MB Vit´ anyi. “Clustering by compression”. In:
Information Theory, IEEE Transactions on 51.4 (2005), pp. 1523–1545.
[9] A. Clark. “Whatever next? Predictive brains, situated agents, and the
future of cognitive science”. In: BEHAVIORAL AND BRAIN SCIENCES
(2013), pp. 1–73.
[10] Andy Clark. Being there: Putting brain, body, and world together again .
MIT press, 1998.
[11] Roger C. Conant and Ross W. Ashby. “Every good regulator of a system
must be a model of that system”. In: International Journal of Systems
Science 1.2 (1970), pp. 89–97. doi: 10.1080/00207727008920220. url:
http://dx.doi.org/10.1080/00207727008920220.
[12] P. Dayan, G. E. Hinton, and R. M. Neal. “The Helmholtz machine”. In:
Neural Computation 7 (1995), pp. 889–904.
[13] Karl Friston. “The free-energy principle: a uniﬁed brain theor y?” In: Na-
ture Reviews Neuroscience 11.2 (2010), pp. 127–138. issn: 1471-003X. doi:
10.1038/nrn2787. url: http://dx.doi.org/10.1038/nrn2787.
[14] Karl Friston, James Kilner, and Lee Harrison. “A free energy p rinciple
for the brain”. In: Journal of Physiology-Paris 100.1-3 (2006). Theoret-
ical and Computational Neuroscience: Understanding Brain Funct ions,
pp. 70–87. issn: 0928-4257. doi: 10.1016/j.jphysparis.2006.10.001.
url: http://www.sciencedirect.com/science/article/B6VMC-4MBC500-1/2/5a0b34eb07e1bee0fb38
[15] Karl Friston et al. “Variational free energy and the Laplace ap proxima-
tion”. In: Neuroimage 34.1 (Jan. 2007), pp. 220–234.
16
[16] Karl J. Friston. “Hierarchical Models in the Brain.” In: PLoS Computa-
tional Biology 4.11 (2008). url: http://dblp.uni-trier.de/db/journals/ploscb/ploscb4.html#Fr
[17] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. “Reinforc ement
learning or active inference?” In: PloS one 4.7 (2009), e6421.
[18] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinfo rcement
Learning or Active Inference?” In: 4.7 (2009). Ed. by Olaf Sporns . issn:
1932-6203. doi: 10.1371/journal.pone.0006421. url: http://dx.plos.org/10.1371/journal.pone.
[19] Karl J. Friston, Spyridon Samothrakis, and P. Read Montague . “Active in-
ference and agency: optimal control without cost functions.” In : Biological
Cybernetics 106.8-9 (2012), pp. 523–541. url: http://dblp.uni-trier.de/db/journals/bc/bc106.htm
[20] Karl J. Friston et al. “Perceptions as Hypotheses: Saccades as Experi-
ments”. In: 3 (2012). issn: 1664-1078. doi: 10.3389/fpsyg.2012.00151.
url: http://www.frontiersin.org/Perception\_Science/10.3389/fpsyg.2012.00151/abstract.
[21] Kurt G¨ odel. “ ¨Uber formal unentscheidbare S¨ atze der Principia Mathe-
matica und verwandter Systeme I”. In: Monatshefte f¨ ur mathematik und
physik 38.1 (1931), pp. 173–198.
[22] Marcus Hutter. “Open problems in universal induction & intelligen ce”.
In: Algorithms 2.3 (2009), pp. 879–906.
[23] Marcus Hutter. “Universal Algorithmic Intelligence: A Mathema tical Top→Down
Approach”. In: Artiﬁcial General Intelligence . Ed. by B. Goertzel and C.
Pennachin. Cognitive Technologies. Berlin: Springer, 2007, pp. 227 –290.
isbn: 3-540-23733-X.
[24] Edwin T Jaynes. Probability theory: the logic of science . Cambridge uni-
versity press, 2003.
[25] Kevin T Kelly. “Justiﬁcation as truth-ﬁnding eﬃciency: how Ockh am’s
Razor works”. In: Minds and Machines 14.4 (2004), pp. 485–505.
[26] S.; Kullback and R.A. Leibler. “On Information and Suﬃciency”. In : The
Annals of Mathematical Statistics 22.1 (1951), pp. 79–86.
[27] Ming Li and Paul M.B. Vitanyi. An Introduction to Kolmogorov Complex-
ity and Its Applications . 3rd ed. Springer Publishing Company, Incorpo-
rated, 2008. isbn: 0387339981, 9780387339986.
[28] John R Lucas. “Minds, machines and G¨ odel”. In: Philosophy (1961),
pp. 112–127.
[29] Simon McGregor. “Algorithmic Information Theory and Novelty G enera-
tion”. In: Proceedings of the 4th Internation Joint Workshop on Compu-
tational Creativity . 2007, pp. 109–112.
[30] Markus M¨ uller. “Stationary algorithmic probability”. In: Theoretical Com-
puter Science 411.1 (2010), pp. 113–130.
[31] Roger Penrose. The Emperor’s New Mind . Oxford University Press, 1989.
[32] Samuel Rathmanner and Marcus Hutter. “A philosophical trea tise of uni-
versal induction”. In: Entropy 13.6 (2011), pp. 1076–1136.
[33] J. Schmidhuber. Algorithmic Theories of Everything . Tech. rep. IDSIA-
20-00, quant-ph/0011122. Manno (Lugano), Switzerland: IDSI A, 2000.
17
[34] J¨ urgen Schmidhuber. “Discovering neural nets with low Kolmog orov com-
plexity and high generalization capability”. In: Neural Networks 10.5 (1997),
pp. 857–873.
[35] J¨ urgen Schmidhuber. “The Speed Prior: a new simplicity measur e yielding
near-optimal computable predictions”. In: Computational Learning The-
ory. Springer. 2002, pp. 216–228.
[36] John R Searle et al. “Minds, brains, and programs”. In: Behavioral and
brain sciences 3.3 (1980), pp. 417–457.
[37] Roy A. Sorensen. Blindspots. Oxford University Press, 1988. isbn: 0198249810
9780198249818.
[38] Tom Florian Sterkenburg. “The Foundations of Solomonoﬀ Pred iction”.
MA thesis. University of Utrecht, 2013.
[39] Alan Mathison Turing. “On computable numbers, with an applicatio n to
the Entscheidungsproblem”. In: J. of Math 58 (1936), pp. 345–363.
[40] Aron Vallinder. “Solomonoﬀ Induction: A Solution to the Problem o f the
Priors?” MA thesis. Lund University, 2012.
[41] David H Wolpert. “Physical limits of inference”. In: Physica D: Nonlinear
Phenomena 237.9 (2008), pp. 1257–1281.
Appendix
A Background
A.1 Kronecker delta
Named after Leopold Kronecker, the Kronecker delta is a function of two vari-
ables, usually represented as δxy:
δxy =
{
1 if x= y
0 otherwise (12)
A.2 Kullback-Leibler (KL) divergence
Used as a measure of the diﬀerence between two probability densitie s (e.g. q(x)
and p(x)), the Kullback-Leibler divergence is deﬁned by Solomon Kullback and
Richard Leibler in [26] as:
D(q(x) ||p(x)) =
∫
q(x) log q(x)
p(x) dx (13)
Important properties:
•D(q(x) ||p(x)) ̸= D(p(x) ||q(x)) (the divergence is not symmetric)
•D(q(x) ||p(x)) ≥0
•D(q(x) ||p(x) = 0 ⇐⇒q(x) = p(x)
18
KL divergence is measured in bits (if log 2 x), bans (if log 10 x) or nats (if
logex), with the latters easily convertible to bits, and it can be considered as
a quantity which gives the number of extra bits (nats or bans) requ ired by the
density q(x) to represent the density p(x).
B Calculating IFE and its Gradient
B.1 IFE
We deﬁne the IFE F as
F(b′,b,s,a ) =
∑
ψ′
P(ψ′ |b′) log P(ψ′ |b′)
P(ψ′,s |b,a)
= −E(b′,b,s,a ) −H(Ψ ′ |b′)
E(b′,b,s,a ) =
∑
ψ′
P(ψ′ |b′) log P(ψ′,s |b,a)
P(ψ′,s |b,a) =
∑
ψ
P(ψ′ |ψ,a)P(s|ψ)P(ψ|b)
We can deﬁne Pre a(ψ′) as the set of states which ψ′ can be reached from by
performing action a, i.e. the set {ψ : P(ψ′ |ψ,a) > 0}, which will usually be
smaller than the entire support of Ψ. Hence, we can rewrite Eas
E(b′,b,s,a ) =
∑
ψ′
P(ψ′ |b′) log

 ∑
ψ∈Prea(ψ′)
P(ψ′ |ψ,a)P(s|ψ)P(ψ|b)


B.2 Partial Derivatives
With the free energy F deﬁned as
F(b′,b,s,a ) =
∑
ψ′
q(ψ′ |b′) ln q(ψ′ |b′)
p(ψ′,s |b,a)
the partial derivatives for each possible belief b′
j are
∂F(b′,b,s,a )
∂b′
j
=
∑
ψ′∈X
∂q(ψ′ |b′)
∂b′
j
(
1 + ln q(ψ′ |b′)
p(ψ′,s |b,a)
)
(14)
where
∂q(ψ′ |b′)
∂b′
j
= q(ψ′ |b′)(δψ′j −q(j |b′
j))
with δψ′j as the Kronecker delta described in A.1.
19