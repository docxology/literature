On Kalman-Bucy filters, linear quadratic control and active inference
Manuel Baltieri1, Christopher L. Buckley2
1 Laboratory for Neural Computation and Adaptation,
RIKEN Centre for Brain Science, Saitama, Japan
2 Evolutionary and Adaptive Systems Research Group and Sussex Neuroscience,
Department of Informatics, University of Sussex, Brighton, UK
manuel.baltieri@riken.jp
May 14, 2020
Abstract
LinearQuadraticGaussian(LQG)controlisaframeworkfirstintroducedincontroltheorythatprovides
anoptimalsolutiontolinearproblemsofregulationinthepresenceofuncertainty. Thisframeworkcombines
Kalman-BucyfiltersfortheestimationofhiddenstateswithLinearQuadraticRegulatorsforthecontrolof
theirdynamics. Nowadays,LQGisalsoacommonparadigminneuroscience,whereitisusedtocharacterise
differentapproachestosensorimotorcontrolbasedonstateestimators,forwardandinversemodels. Accord-
ing to this paradigm, perception can be seen as a process of Bayesian inference and action as a process of
optimalcontrol. Recently,activeinferencehasbeenintroducedasaprocesstheoryderivedfromavariational
approximationofBayesianinferenceproblemsthatdescribes,amongothers,perceptionandactioninterms
of(variationalandexpected)freeenergyminimisation. Activeinferencereliesonamathematicalformalism
similar to LQG, but offers a rather different perspective on problems of sensorimotor control in biological
systems based on a process of biased perception. In this note we compare the mathematical treatments
of these two frameworks for linear systems, focusing on their respective assumptions and highlighting their
commonalities and technical differences.
1 Introduction
The Linear Quadratic Regulator (LQR) is is a method originally developed in the field of control theory
[57, 55, 3, 85] with the goal of describing a solution to linear control problems using “full-state” feedback
methods, i.e., methods relying on a state of the system to regulate (or plant) that is fully available. In more
realisticapplicationshowever,itiscommontoassumethatthestateofaplantanditsensuingdynamicscannot
be fully observed, i.e., they are only partially observable, or hidden. With incomplete information, an observer
(or estimator) is usually introduced to first infer the hidden dynamics of a system, combining this inference on
the (hidden) state of a plant with LQR methods to build an optimal feedback law [5, 3, 85]. In the case of
linear continuous time problems, the optimal observer has the form of a Kalman-Bucy filter [50, 4, 85]. The
combination of Kalman-Bucy filters and LQR often goes by the name of Linear Quadratic Gaussian (LQG)
controllers [105, 5, 3, 85]. Despite some of its well-known limitations (e.g., its robustness issues [25]), LQG
constitutesnonethelessastandardapproachforalargeclassoflinearproblemsinstochastic optimalcontrol[5],
due to its analytical tractability and guaranteed optimality under a given set of assumptions. In the last few
decades, LQG and some of its extensions have also become a standard paradigm outside of the field of control
theory, in particular for models of sensorimotor control in neuroscience [68, 95, 92, 31]. This reflects recent
developments in computational models of perception and behaviour, nowadays mostly described in terms of
state estimation, or inference [64, 80, 66, 63, 35], and optimal control [62, 103, 95, 91, 31] respectively.
In the last few years, active inference has also been introduced in computational and cognitive neuroscience
asatheorytodescribebrainfunctioningintermsof(approximateBayesian)inferenceandcontrol[38,35,40,39,
78]. Similarly to LQG, active inference has also been instrumental in defining models of perceptual and motor
processes,fromphysiologicalaccountsofmotorcontrol[1],allthewaytocomputationaldescriptionsofoptimal
behaviourclaimedtobecontrastingLQG[36]. Therapidlyevolvingnatureofthistheoryhassofarencouraged
different authors to cover and summarise some of its different technical aspects [19, 21, 17, 81, 47, 23]. At
themoment, however, therelationshipbetweenthisframeworkandpopularLQG-basedmodelsinneuroscience
remains still unclear, especially considering how the two share a common mathematical background based on
methods coming from (approximate) Bayesian inference and stochastic optimal control, but generate rather
1
0202
yaM
31
]CN.oib-q[
1v96260.5002:viXra
different predictions. The goal of this note is thus to produce a comparison of the mathematical treatments
introduced by active inference and LQG. The discussion will be based on 1) continuous time linear problems,
using 2) standard formulations of both frameworks (thus not explicitly addressing more advanced proposals,
such as the “optimal feedback control” extension of LQG [95, 92], leaving this for future work), to focus on 3)
their possible implications for the neural and the cognitive sciences.
2 Linear Quadratic Gaussian control for linear multivariate contin-
uous time systems
InthissectionweintroducethemathematicalunderpinningsofLQGcontrol,startingfromageneralformulation
oftheKalman-BucyfilterandLQRformultivariatesystemsincontinuoustime,andthencombiningtheirresults
under the separation principle [105] to form LQG controllers. A full derivation, including mathematical proofs
and other technical details not relevant to our comparison can be found in standard treatments of Kalman
filtering [50, 22] and LQR/LQG [3, 85].
2.1 Kalman-Bucy filter or Linear Quadratic Estimator (LQE)
The Kalman-Bucy filter, also known as Linear Quadratic Estimator (LQE), is an algorithm for the estimation
of hidden or latent variables evolving over time, i.e., hidden states with some (linear) dynamics [56, 84, 50, 22].
Its popularity is mainly due to the fact that
• it’s the optimal1 filter for estimation/inference of hidden states in linear systems,
• it provides a recursive algorithm well suited for computer simulations and online applications with new
data added over time, and
• it makes no assumptions on the stationarity of the hidden dynamics a system, particularly relevant for
real-world applications and especially control problems.
In this treatment we will focus on the continuous formulation of this algorithm, the Kalman-Bucy filter [58],
as opposed to its discrete formulation, often simply going by the name of Kalman filter [56]. To introduce the
Kalman-Bucyfilter,weinitiallydefinea(linear)continuousdynamicalsystemsinstate-spaceform,representing
the simplest plant whose hidden state the filter ought to estimate:
dx=Axdt+dw
y =Cx+dz (1)
Here the bold characters indicate vectors. The first equation describes linear dynamics for a vector of hidden
states x ∈ Rn. In the second equation, y ∈ Rm is a vector of noisy measurements, or observations, of
hidden states x. Vectors w ∈ Rr,z ∈ Rs correspond to Wiener processes for state and observation equations
respectively, with dw = N(0,Σ ),dz = N(0,Σ ) thus defined as zero-mean white Gaussian random variables
w z
with covariance matrices Σ ,Σ [50, 22]. Matrices are represented using capital letters, with C ∈ Rm×n as
w z
the observation matrix mapping dynamics x to observations y and A ∈ Rn×n as the state transition matrix
characterising the time-dependent evolution of x.
The Kalman-Bucy filter is known in the literature to be the optimal estimator for linear systems with
quadratic cost functions and Gaussian white random variables [56, 50, 22, 6]. Under these assumptions, it is
also a minimum variance estimator [50, 22], i.e., it minimises an objective function represented by the mean
square error (MSE), or variance of the error, given by:
J =E[(x−xˆ)T(x−xˆ)] (2)
where xˆ is a vector of the estimates of states x. Through the Kalman-Bucy algorithm, one can determine
estimates x via [50, 85, 22]:
xˆ˙ =Axˆ+K(y−Cxˆ)
K =PCT(Σ )−1
z
P˙ =Σ +AP +PAT −K(Σ )KT (3)
w z
The vector xˆ is thus updated using the sum of the current best estimate multiplied by the known transition
matrix, Axˆ, and prediction errors (or innovations), y −Cxˆ. These prediction errors are multiplied by the
1Inaleast-meansquare/maximumlikelihoodsenseand,fortheconditionsmetbyLQGcontrolandadoptedinthisnote,also
minimumvarianceandunbiased[50,22].
2
Kalman gain matrix, K ∈ Rn×m, expressed in the second equation, and representing the optimal trade-off
between previous estimates and information gathered from new observations. To calculate the Kalman gain
matrix one then needs to estimate P ∈Rn×n, the a-posteriori error covariance matrix, expressing the accuracy
of the state estimate in the first equation. The trace of P (i.e., the sum of the components on the main
diagonal) gives the sum of the independent components of the covariance matrix equivalent to the MSE in
equation (2), thus describing the accuracy of the state estimation process too. Kalman(-Bucy) filters can also
be described in terms of Bayesian inference, with xˆ and P representing mean and covariance matrix of an
estimated (multivariate) Gaussian distribution of hidden states x [50, 71, 22].
2.2 Linear Quadratic Regulator (LQR)
Linear Quadratic Regulator (LQR) is defined for deterministic linear systems with quadratic cost functions.
Undertheseassumptions,theoptimalcontrollawisequivalenttoanegative(proportional)feedbackmechanism2
[57, 55, 3]. The noiseless plant to regulate is represented by a linear differential equation:
dx=Axdt+Badt (4)
with x ∈ Rn as a vector of measured variables to be controlled. In this case we assume that the state of
the plant is directly observable (no measurement noise, dz in the LQE formulation) so while the differential
form dx is not strictly necessary (i.e., we could replace dx with x˙ and drop the dt notation), it is maintained
for consistency with the Kalman-Bucy filter definition provided previously. A ∈ Rn×n is the state transition
matrix, as in the case of the Kalman-Bucy filter, while B ∈Rn×p is the control matrix mapping actions a∈Rp
to outputs x. Actions are often represented by u in the control theory literature, however here we highlight a
difference that will become important later: in the present note, u ∈ Rl describes inputs from a more general
state-space perspective, i.e., all the variables that affect the (hidden) states of a system but are not states
themselves, such that
u∈Rl ={a∈Rp,d∈Rq} withl=p+q (5)
In this formulation, a is used to represent only a part of all possible inputs, i.e., the subset of inputs produced
by a regulator that have an effect on the hidden state of a system (in the sense on Kalman’s controllability of
a plant [55, 85]), such as the motor actions of an agent that affect its environment. Variables d ∈ Rq include,
on the other hand, disturbances that influence a system but cannot be governed by a controller. Often, the
definitions of u and a are often used interchangeably, assuming either that
• all the modeled inputs are controllable, i.e., there are no other unknown external forces affecting the
state of a system, only small and especially brief perturbations that can be considered as part of noise on
dynamics (dw) [68, 49], or that
• perturbations are control-dependent, i.e., they are only due to motor actions and can thus not account
for other exogenous sources [95, 92].
In LQR, the goal is to stabilise (control, compensate or regulate) variables x around target values defined
by v ∈ Rp3, controlling x through actions a. Such actions are determined via the optimisation of a function
that accumulates costs over time, called cost-to-go or value functional:
(cid:90) ∞ 1 1
J = (x−Bv)TQ(x−Bv)+ aTRadt (6)
2 2
0
whichrepresentstheinfinitehorizonsimplificationoftheproblem,i.e.,theupperlimitoftheintegralisinfinity.
The instantaneous version, simply called cost function or cost rate, is defined for LQR as:
1 1
c(x,a)= (x−Bv)TQ(x−Bv)+ aTRa (7)
2 2
with Q ∈ Rn×n ≥ 0 and R ∈ Rp×p > 0 as arbitrary matrices representing the relative balance between the
minimisation of the distance from the target and costs for control respectively. In LQR, the optimal action
vector a is computed using [3, 85, 93]:
a=−L(x−Bv)
L=R−1BTV
−V˙ =Q+ATV +VA−LTR−1L (8)
2Inthecaseofinfinite-horizoncontrolproblem.
3Herewewanttohighlightthatingeneralp(cid:54)=n,meaningthatthedegreeofcontrollabilityofasystem,orratherthedimension
ofactionvectoratoreachtargetvaluesv,neednotnecessarilycoincidewiththenumberofstatesinx.
3
The first equation implements a negative feedback mechanism on x using actions a. In the second equation, L
is the feedback gain matrix, determined using the matrix V, which is the Hessian of the cost-to-go functional
defined in equation (6) [94].
2.3 Linear Quadratic Gaussian (LQG) control
One of the limitations of standard LQR controllers lies in the fact that they do not explicitly deal with
state/observation uncertainty (or noise), i.e., the original formulation is for deterministic systems only [55].
On the other hand, in real-world engineering applications as well as in biological systems, it is more common
to think of systems with limited access to information from the environment and actions a thus applied to a
set of hidden states x when only noisy measurements y are available. In the control theory literature, Linear
QuadraticGaussian(LQG)control[5,8,3,85]isthusintroducedasacombinationofestimationandcontrolfor
linear systems, building on the previously defined Kalman-Bucy filter and LQR. Under a particular set of as-
sumptions,LQGcontrollerscanthenbeseenas“modular”,withestimationandcontrolprocessesindependently
designedtoformanoptimalsolutionfollowingtheseparation principle ofcontroltheory[54,105,46]. Theidea
behindtheseparationtheoremisverycloselyrelatedtothecertainty equivalence describedineconometricsand
decision making [83, 90], although some works, including [102, 14, 85], highlight their differences especially in
the context of “dual effects” (see Discussion) and risk-sensitive control. In information theory, Shannon [82]
alsointroducedadifferentnotionofaseparationprinciple,toexplaincodingviatwo(separate)phasesofsource
compression and channel coding [45]. The connections between Shannon’s work and the separation principle in
control theory have become more clear in recent years, thanks to a growing literature showing how Shannon’s
definitioncapturesandpotentiallygeneralisestheresultsfromcontroltheory, seeforinstance[89,88,30]. Here
however, the focus will be on the principle traditionally described in control theory for LQG in continuous
systems [54, 105], under the following standard assumptions [105, 5, 14, 3, 85]:
1. linear process dynamics and observation laws describing the environment and its latent variables,
2. Gaussian white additive (cf. [92]) noise in both process and measurement equations,
3. known covariance matrices for both process and measurement noise,
4. quadratic cost function used to measure the performance of a system, and
5. known inputs, u={a,d}.
The last condition is particularly relevant for a comparison with active inference and is due to the assumptions
behindtheKalman-Bucyfilter,definedasanoptimalestimatorinanunbiased senseonlyforknowninputs[32],
outputs (i.e., measurements) and parameters. To provide a more formal treatment of the separation principle,
we then define a general linear system to be regulated in the presence of noise or uncertainty:
dx=Axdt+Badt+dw
y =Cx+dz (9)
whereallthevariablesandparametersfollowthenotationpreviouslydefinedforKalman-BucyfiltersandLQR.
Inthiscase,thecostrateismodifiedtodealwithastochasticsystemwithwhiteadditivenoiseonbothdynamics
and observations. To do so, the standard formulation of LQR is extended, including stochastic variables so to
minimise the expected cost-to-go [85, 93]:
(cid:34) (cid:35)
(cid:90) ∞ 1 1
J =E (x−Bv)TQ(x−Bv)+ aTRa (10)
2 2
0
Importantly, one can show that under the assumptions introduced above
1 1
c(x,a)=c(xˆ,a)= (xˆ−Bv)TQ(xˆ−Bv)+ aTRa (11)
2 2
where we replaced states x with their estimates xˆ, thus implying that the optimal control can be computed
using only the state (point) estimates, i.e., means xˆ. Minimising the expected value of the cost-to-go is thus
equivalent to minimising the cost-to-go for the expected state.
Optimal estimation and control are then achieved by sequentially combining two separate parts, a Kalman-
BucyfilterandaLQR,toproducearegulatoroptimalinaminimum-variance,unbiasedsense[3,85]. Onethus
4
obtain the following scheme:
xˆ˙ =Axˆ+Ba+K(y−Cxˆ)
a=−L(xˆ−Bv)
K =PCT(Σ )−1
z
L=R−1BTV
P˙ =Σ +AP +PAT −K(Σ )KT
w z
−V˙ =Q+ATV +VA−LTRL (12)
where the Kalman-Bucy filter provides optimal (unbiased) state-estimates xˆ of observations y that are con-
ditionally dependent only on the vector of motor actions a that contributed to the generation of the current
observationsy (i.e., atthetimeoftheestimation), thankstoaMarkovassumptionduetothepresenceofwhite
noise [22]. The LQR then uses these point estimates to implement an optimal controller that essentially treats
the problem as a deterministic one by replacing the state of a plant with its best estimate, the mean, see also
Fig. 1. The ability to analytically evaluate the expected cost rate using only expectations xˆ, see equation (11),
is thus at the core of LQG architectures. In this way, estimation and control processes interact via the sharing
of some information in the form of estimates xˆ to the controller and a copy of actions a to the estimator, but
are otherwise seen as fundamentally separable modules to be optimised independently.
Figure1: Equations for Kalman-Bucy filter, LQR and LQG.HerewesummarisetheKalman-Bucyfilter,
LQR and LQG algorithms and their respective plants.
3 Active inference for linear multivariate continuous time systems
Active inference is a process theory describing brain functioning and behaviour in living systems using ideas
from Bayesian inference and stochastic optimal control [38, 35, 40, 21, 39]. In this section we begin to establish
its relations to the LQG architecture, building an active inference solution to the regulation problem of a linear
multivariate system. To do so, we first define a generative model [15, 18] for a system that prescribes 1) the
dynamicsoftheplanttocontrol(seeequation(9))and2)howtheserelatetoobservations. Thelineargenerative
modelispresentedinastate-spaceform,similartothetheoneexpressedinequation(9),butisaratherdifferent
mathematical object: while equation (9) describes the “real” plant (i.e., its generative process [38, 21, 39]), in
a generative model we write a set of equations embodying our assumptions regarding a plant that are used to
derive update equations that define a controller. Importantly, the mapping between a generative process and
5
a generative model is not necessarily one-to-one, and different generative models can ensure regulation while
representing different sets of assumptions [9, 96]. The generative model to regulate the same linear system
previously introduced in equation (9) is thus defined as4:
x(cid:48) =Aˆx(cid:48)+Bˆv+w
y =Cˆx+z (13)
where the hat over matrices Aˆ,Bˆ,Cˆ highlights the important fact that such matrices do not (have to) mirror
their counterparts describing the world dynamics in equation (9), i.e., as explained above one can describe
several generative models for a given plant. In the simplest case, this may simply imply that matrices A,B,C
are unknown and should also be estimated [79, 34]. More in general however, different matrices in a generative
model can implement desired dynamics that substantially differ from the observed ones in order to obtain
specific desired behaviours5 as previously shown in for instance [37, 38, 10, 13]. As we will discuss later, this
constitutes a point of departure from the traditional assumption behind architectures built on the separation
principle, where the variables described in LQG controllers reflect, by construction, the linear dynamics of the
observed system, such that
Aˆ=A, Bˆ =B, Cˆ =C (14)
Furthermore,whileinLQGonehastoassumefullknowledgeofmotoractionsafortheunbiasednessofKalman
filters [85], in active inference this vector is not explicitly modeled, leading to a case where no copy of motor
signals is available or even necessary, relying instead on a process of biased estimation [12] (cf. [32]). Active
inference proposes that a deeper duality of estimation and control exists whereby, in the simplest case, actions
arejustperipheralopen-loopresponsestothepresenceofpredictionerrorsonobservations,irrespectivelyofthe
causes of sensations: self-generated, a, or external, d [36, 41, 20, 1]. While our focus remains on the continuous
time formulation of active inference, it should be noted that in more recent discrete formulations of behaviour
under this framework, this account was extended with action cast as a problem of minimising expected free
energy, inferring (fictitious) control states c or rather, time-dependent sequences of control states, or policies
π 6 associated to a set of actions a [41, 40, 96]. Importantly however, both these proposals rely on theories in
c
neuroscience that suggest a lack of information on self-produced controls (i.e., efference copy [101]), proposing
analternativeforeffectivemotorcontrolinbiologicalsystems[27,36,1,28,65]. Thegenerativemodelthusdoes
not describe directly the role of actions a as seen in equation (9). In their place, the vector v encodes instead
priorsintheformofexternalorexogenousinputsinageneralstate-spacemodelssense, (cf. uinequation(5)),
or biases in probabilistic settings. In this light, priors can be used as targets for the regulation problem (see
equation (6) and equation (10)), effectively biasing the estimator to infer desired rather than observed states,
with a controller instantiating actions on the world to fulfill the target (state or trajectory) of a controller
[13]. Variables z,w model noise in or uncertainty about the environment in a Stratonovich sense, i.e., having
non-zero autocorrelations (see Discussion).
Following the general formulation of the variational free energy under Gaussian assumptions (Laplace and
variational Gaussian approximations) applied to the multivariate case, we define (dropping terms constant
during the minimisation process):
(cid:12)
F ≈−lnP(y,ϑ)(cid:12) (15)
(cid:12)
ϑ=ϑˆ
with a full derivation found in, for instance, [21, 24, 9]. Here ϑ denotes the set of all the hidden variables of a
generative model, such that ϑ={x,v,θ,γ}, with x,v defined above and parameters θ including the matrices
Aˆ,Bˆ,Cˆ previously introduced, while hyperparameters γ encode the stochastic properties of the generative
model, i.e., the covariance matrices Σ ,Σ and their possible reparameterizations.
z w
With Gaussian assumptions on z,w, the state-space model in equation (13) can then be written down in a
probabilistic form, mapping the measurements equation to a likelihood
P(y|x,v)=P(y|x) (16)
assuming that observations y are conditionally independent on inputs v. The system’s dynamics are also then
expressed as a prior [34]
P(x,v)=P(x|v)P(v)=P(x|v) (17)
4ItshouldbenotedthatWienerprocessesincontinuoustimeareoftenpresentedusinganotationintermsofdifferentialsdx,dt
anddW whereW isaWienerprocessorBrownianmotion,seeequation(9). However,itisalsopossibletosimplyuseaLangevin
form as we do here even in the case of white noise, at least until an appropriate calculus (e.g., Ito or Stratonovich) is introduced
andtheappropriateinterpretationapplied[98]. NoticealsothatweadopthereaLagrangenotationforderivatives,withNewton’s
notationintroducedlatertospecifytheensembledynamicsoftherecognitiondynamics,see[21].
5withinreachabilityandcontrollabilityconstraints[85]
6calleduandπu in[40].
6
where we assumed that inputs v are known, with their prior P(v) thus reducing to distribution with its mass
densely concentrated around its mean (in the limit, a delta function). This assumption simply expresses the
fact that in our current formulation of control problems, v will encode values assigned by us and representing
the target-state or trajectory of the system. Since the probabilities densities are both multivariate Gaussian,
they can be written as:
1 (cid:16) 1 (cid:17)
P(y|x)= exp − (y−Cˆx)TΣ−1(y−Cˆx)
(cid:112) (2π)m|Σ | 2 z
z
1 (cid:16) 1 (cid:17)
P(x,v)= exp − (x(cid:48)−Aˆx(cid:48)−Bˆv)TΣ−1(x(cid:48)−Aˆx(cid:48)−Bˆv) (18)
(cid:112) (2π)n|Σ | 2 w
w
where m,n represent the dimensions of vectors y and x and |Σ |,|Σ | are the determinants of the respective
z w
covariance matrices. By substituting equation (18) in equation (15), the variational free energy for a generic
linear multivariate system becomes:
F ≈ 1
2
(cid:20)(cid:16) y−Cˆxˆ (cid:17)T Π
z
(cid:16) y−Cˆxˆ (cid:17) + (cid:16) xˆ(cid:48)−Aˆµˆ
x
−Bˆvˆ (cid:17)T Π
w
(cid:16) xˆ(cid:48)−Aˆxˆ−Bˆvˆ (cid:17) −ln (cid:12) (cid:12)Π
z
Π
w
(cid:12) (cid:12)+(m+n)ln2π (cid:21)
(19)
where we replaced x,v with their expectations xˆ,vˆ, since under the Laplace and the variational Gaussian
assumptions,thefreeenergyinequation(15)mustbeevaluatedatthemodeofP(y,ϑ),equivalenttothemean
forGaussianvariables. Inthesameway,wethenintroducedprecisionmatricesΠˆ ,Πˆ astheinverseofthebest
z w
estimateofcovariancematricesΣˆ ,Σˆ ,derivedfromanapplicationoftheLaplaceapproximationtocovariances
z w
Σ ,Σ [34]. Itisimportanttohighlightthat, ingeneral, thecovariancematrices(ortheirinverse, theprecision
z w
matrices) used in the generative model can in fact be different from the ones used to describe the environment
or generative process [13]. The same approximations can in principle be applied to θ = {Aˆ,Bˆ,Cˆ} [34], but
to simplify the treatment (and in line with the idea of comparing active inference and LQG on equal ground),
we assumed that these parameters are known quantities, even if different from their respective matrices in the
generative process (see equation (14)).
Therecognitiondynamics,prescribingestimationandcontrolinasystemminimisingvariationalfreeenergy
[43, 42, 21] are implemented in standard active inference formulations as a gradient descent scheme on free
energy with respect to xˆ and xˆ(cid:48) for estimation:
∂F (cid:16) (cid:17) (cid:16) (cid:17)
xˆ˙ =Dxˆ− =xˆ(cid:48)+CˆTΠ y−Cˆxˆ +AˆTΠ xˆ(cid:48)−Aˆxˆ−Bˆvˆ
∂xˆ z w
∂F (cid:16) (cid:17)
xˆ˙(cid:48) =Dxˆ(cid:48)− =−Π xˆ(cid:48)−Aˆxˆ−Bˆvˆ (20)
∂xˆ(cid:48) w
This system for the update of the means of x and x(cid:48) is similar to the standard form of prediction and update
stepsinaKalmanfilterfordiscretesystems[22,33]. TheequationsincludeatermD,ensuringtheconvergence
of the scheme when higher embedding orders are also optimised, i.e., x(cid:48), see [43]. This differential operator is
definedsothatDxˆ=xˆ(cid:48) =0andthusgives0forallhigherordersofderivativesDxˆ(cid:48) =xˆ(cid:48)(cid:48),Dxˆ(cid:48)(cid:48) =xˆ(cid:48)(cid:48)(cid:48) =0,...
that are normally assumed to be white noise (see discussion in [43]). Its use is equivalent to the presence of
a-priori state estimates found in the update equations of traditional treatments of Kalman(-Bucy) filters, and
computed in the prediction step. In practice the system in equation (20) collapses to a single equation when
the update step (second line in equation (20)) is assumed to be faster:
xˆ˙(cid:48) =0 =⇒ xˆ(cid:48) =Aˆxˆ+Bˆvˆ
giving an equation resembling the Kalman-Bucy matrix equation for the estimated mean in equation (3):
xˆ˙ =Aˆxˆ+Bˆvˆ+CˆTΠ (y−Cˆxˆ)
z
On the other hand, control is defined via a gradient descent on actions a, assuming the perspective of a system
that can only infer that actions have an effect on observations y (i.e., no direct knowledge of how a affect the
latent states x or their estimates xˆ):
∂F ∂F ∂y ∂yT (cid:16) (cid:17)
a˙ =− =− =− Π y−Cˆxˆ (21)
∂a ∂y ∂a ∂a z
These equations are finally reported in Fig. 2, with the introduction of learning rates M and N to facilitate a
more direct comparison.
7
Figure 2: LQG and active inference equations. Different algorithms for the control of continuous time
linear partially observable dynamical systems, LQG on the left and active inference on the right.
4 A comparison of LQG and active inference formulations
4.1 Related work
In recent years, several reviews [19, 21, 17, 47, 23] have tackled different mathematical aspects of active infer-
ence, in order to analyse some of its claims and possibly fully uncover its potential as a general theory for the
neurosciences. This note however differs in crucial ways from said reviews, offering a technical perspective that
shouldbeseenascomplementarytopreviousworks. Inparticular,unlike[19]herewefocusonsensorimotorcon-
trol rather than on pure perceptual accounts of cognitive systems. The current mathematical treatment builds
on [21] where an explicit derivation of active inference in generalised coordinates of motion for non-Markovian
univariate continuous time stochastic processes was introduced, inspired by more general formulations found,
for instance, in [43, 34]. Here we extend this formulation to multivariate systems to introduce a comparison
with the standard LQG paradigm, inspired by work initially discussed in [11, 12, 9]. In [17, 81, 23] the focus is
ondiscretetimeformulationsofactiveinference[40,39]basedonpartiallyobservableMarkovdecisionprocesses
(POMDPs) and thus differ in fundamental ways from the work presented here which, on the other hand, relies
oncontinuoustimesystems[43,38,21]. Finally,theworkpresentedin[47]alsofocusesondiscreteformulations,
without including an explicit account of control and behaviour (similar to [19]).
4.2 Differences
As seen in previous sections, and summarised in Fig. 2, LQG and active inference share a common mathe-
matical background rooted in Bayesian inference and optimal control. In practice, however, there are a few
key differences between these two architectures for linear continuous time systems. Some of them are rather
substantial and affect even more general cases, while others end up playing a relatively minor role, especially
for the simplified linear systems presented in this note. Here we will discuss, in particular, differences due to:
• state-space model formulations
• action selection policies
• minimisation schemes
• cost functionals and general derivations
8
4.2.1 Different state-space model formulations
LQG and active inference both rely on state-space model formulations, however while LQG implements a
more traditional one, active inference is said to introduce a definition of models in generalised coordinates of
motion, i.e., higher embedding orders for continuous time models [43, 34]. These models are equivalent to a
continuous time version of standard treatments of coloured (Gaussian) noise, i.e., non-Markovian (Markovian
of order N, or semi-Markovian) processes, in terms of augmented state-space models seen for discrete systems
[50, 34]. When the noise is not white, the typical strategy involves increasing the dimensionality of the state-
space, treating coloured noise as an autoregressive model of independent white noise components that can be
explicitly separated by introducing higher embedding dimensions that maintain an overall Markov property
[50]. In generalised state-space models for continuous systems, random variables are treated as analytical
noise with non-zero autocorrelation, replacing the (Markovian) Wiener processes typical of Ito’s formulations
of stochastic variables7 and adopted in most standard formulations of Kalman-Bucy filters and LQG control.
Wiener processes are usually considered a good approximation of coloured noise only when the time scale of
an estimator is much slower than the true time scale of the dynamics of an environment/plant. In this light,
non-MarkovianprocessesshouldsimplybeseenasanaturalgeneralisationoftreatmentsbasedonWienernoise,
whose derivatives are mathematically not well defined in the continuous limit, thus making it less suited for
modelswhosecharacteristictimescalemorecloselymatchestherealdynamics. Thisisoftenthecaseforsystems
studied in various areas of engineering [86, 50], physics [29, 99, 69, 44], ecology [48, 100] and neuroscience
[34, 97, 67], among others. Generalised state-space models use a Stratonovich’ formulation that allows for
differentiable (analytical) noise [16, 86], often thought to be a better model for coloured noise [104, 86, 98, 34].
This generalisation is claimed to allow for the treatment of non-Markovian continuous random variables in
a more principled way where derivatives of stochastic processes are defined as in traditional (non-stochastic)
calculus and included as extra dimensions of the state-space in terms of higher embedding orders (cf. the role
of delayed coordinates in embedding theorems [77, 87]). These “extra” embedding orders represent a set of
coordinates expressed in terms of “position”, “velocity”, “acceleration”, etc., for each variable and correspond
to a linearised (for convenience [34, 21]) path generated by a Taylor expansion in time of each trajectory.
Thedifferencesbetweengeneralisedandstandardstate-spacemodelsarehoweverlessnoticeableforthelinear
casetreatedinthisnote,oftenusedasastartingpointformodelsofsensorimotorcontrolincomputationaland
cognitive neuroscience. This is due both to 1) the treatment in terms of white noise adopted in equation (9),
making higher embedding orders redundant when the plant’s model is known, and to 2) the white noise being
additive. Undertheseassumptions,whilethetheoreticalinterpretationsofItoandtheStratonovichformulations
still differ [98], they lead to the same results [50]. For nonlinear problems the discrepancies between these two
approaches become more obvious, at the cost however, of analytical tractability since stronger assumptions are
required for the treatment of more general cases. For instance, in LQG architectures, the separation principle
heavily relies on the linearity of observations and dynamics, and only weaker versions of this principle are
at the moment defined for some special classes of nonlinear systems, e.g., [7]. In active inference, the effects
of the local linearity assumption on higher embedding order remains largely unexplored especially for non-
smooth processes. More in general, however, while it is sometimes claimed that active inference has a clear
edge over traditional approaches Markovian noisem, such as LQG [43, 42], one can show that non-Markovian
systems can be formulated as Markovian ones using an augmented state-space [50, 34], in the same way higher
embeddingordersincreasethenumberofstateequationsinactiveinference[34]. Duetothis,somecomparisons
between active inference algorithms and existing methods in the literature may appear at the moment slightly
confusing. For instance, in [43] we see an analysis comparing (extended) Kalman-Bucy filters and Dynamic
ExpectationMaximisation(DEM,oneofthestandardalgorithmicimplementationsofestimationalgorithmsin
activeinference). Inpresenceofwhitenoise, thetwomethodsachieveverysimilarperformances, whilefornon-
white noise, DEM is claimed to display better results [43]. At the same time, while the number the state-space
dimension for DEM was effectively increased via the use of generalised coordinates of motion, the dimension of
the state-space of the (extended) Kalman-Bucy filter remained unchanged, thus suggesting that differences are
mainly due to the treatments (or lack of) of coloured noise, with the (extended) Kalman-Bucy filter capable of
replicating the same results with an adequate state augmentation.
4.2.2 Different update equations for action
As shown in Fig. 2, the update equations for action in LQG and active inference present a few crucial dif-
ferences. Importantly, actions in LQG are implemented using an algebraic equation equivalent to standard
negative proportional feedback mechanisms, while in active inference the same update relies, effectively, on an
implementationbasedonbiased perceptionandakintointegral control[13]. Thisismainlyduetothefactthat
while LQG requires full knowledge of inputs a to calculate the best estimates xˆ, active inference relies on an
implicit mechanism to handle the lack of such inputs: integral control. In particular, the target value for the
7InItocalculusautocorrelationsarestrictlyequaltozero[50].
9
control problem, encoded by the vector v, is introduced as a prior in the form of a constant “disturbance”, or
bias, in the generative model in equation (13). This unaccounted disturbance generates a biased estimate (cf.
[32])thatencodesthedesiredstate/trajectoryofasystem,reachedviatheuseofanintegralcontrollerthatacts
to“regulate”suchbias[51, 52]. Integralcontrolcaninfactbeseenasanimplicitmechanismtoestimatelinear
(i.e., constant) perturbations/inputs, inferred by accumulating evidence based on steady-state errors generated
by the presence of the bias vector term v8.
A second key point is that actions built in LQG schemes use prediction errors on hidden states, while
active inference updates are based on errors on observed states. In LQG, this is due to an assumption on the
invertibilityofmatrixC,mappingestimatedstatesxˆtoobservationsy,seeforinstanceeqn. 4.1-1to4.1-8in[3].
Thankstothisassumption,thedesired/targettrajectorycanbeexpresseddirectlyintotheframeofreferenceof
hiddenratherthanobservedstates, adapting(thankstotheseparationprinciple)aproblemofoutput-feedback
controlintooneofstate-feedback. WhilethisassumptionholdsforthelinearsystemsdefinedinLQGunderthe
separation principle, it is not trivially generalised to nonlinear cases where processes of perception and control
have strong co-dependences [14]. On the other hand, active inference implementations can in principle easily
provide approximate solutions to increasingly complicated problems of (sensorimotor) control for a reasonable
choice of the peripheral (open-loop) “inverse” model,
∂yT
[13].
∂a
Furthermore, it should also be noted that the update equation for action in active inference replaces the
arbitrary positive definite weight matrix R, standard in LQG control, with the expected precision matrix of
observation noise Πˆ (=Σˆ−1). This move can be seen in light of the established duality of deterministic control
z z
and stochastic filtering/estimation problems first formalised by Kalman [56, 57, 58]. Unlike standard uses of
this duality theorem, normally simply highlighting matching terms in the solutions of these two problems in
the linear case under a coordinate transformation (including time reversal), active inference assumes that the
matrixR infactjust–is –theexpectedprecisionmatrixofobservationnoiseΠˆ . Theimplicationsofthisidea
z
include, among others, an intrinsic “dual effect” of actions within the active inference framework due to the
lack the conditions for the separation principle, cf. LQG [14]. This dual effect can be seen as an instantiation
of the fundamental exploration-exploitation dilemma [26], portraying the time-constrained trade-off between
inferring/learning about the unknown structure of a system and regulating its state to a desired target. The
activeinferenceformulationforlinearsystemsconstitutesthusanintermediatestagebetweenLQGmodelswhere
dual effects are not present [14], and more general dualities of estimation and control for nonlinear problems
found for instance in [72, 94]. These generalised dualities are related to the formulation of the information
filter, a linear estimator propagating the error precision, rather than covariance, matrix. The information filter
provides a formulation that simplifies the duality relations by rearranging matching terms, creating new and
differentdualities,andinparticularbyreplacingadifficultgeneralisationofthematrixtransposeoperatorwith
a second time reversal (for details see [94]). The same method based on an exponential transformation was
likewise applied in path integral control [60, 59]. In this scheme, one can also show how R∝Σ−1 (rather than
w
R ∝Π as in the Kalman duality expressed by active inference in the linear case) for a class of tractable (i.e.,
z
linear in a) fully-observable nonlinear problems where the conditions for the standard separation principle are
not met (i.e., LQG emerges only as a special case [60]).
4.2.3 Different minimisation schemes
The updates proposed with active inference are apparently consistent with Kalman-Bucy filters, although an
important difference is clear: the Kalman gain, K, and feedback gain, L are not explicitly computed in active
inference. The Kalman gain, K, and feedback gain, L, matrices prescribe the optimal (i.e., minimum variance)
updatespeedofestimatesofhiddenstatesandcontrols,whilebalancingpriorinformationandnewobservations.
BothK andLrequiresolvingRiccatiequationsinvolvingknowledgeofthecovariancematricesofdynamicsand
observation noise in the first case, and weights representing costs for estimation and control in the cost-to-go
function for the latter. In current formulations of active inference, the matrices K and L are not as clearly
directly defined as they are in LQG architectures. If we consider only the Kalman gain K, the main reason is
thatactiveinferencereliesonaseriesofsimplifyingassumptionsperhapsnotbestsuitedtocomputation,online,
Kalman and feedback gains. In particular, while the Laplace [15] and variational Gaussian approximations [75]
greatlysimplifythevariationalBayesiantreatmentofinferenceandcontrolproblems(andare,together,exactfor
linear, Gaussian plants), they are usually accompanied by further approximations. These include, for example,
the post-hoc optimisation of the covariance matrix of an approximate density under the variational Gaussian
approximation,i.e.,performedafteracertainnumberofobservations[43](althoughsee[42]foranapproximate
onlineversion). Ingeneralhowever,noequationsoftheRiccatitypecurrentlyappearinstandardformulationsof
variationalfreeenergyminimisationinactiveinference. Underfurtherassumptions, forexamplebyconsidering
fading-memory Kalman filters, it is however possible to draw a more direct comparison that will be developed
8Anexplicitversionofthismethodcorrespondstointroducingextravariablesinastate-spacemodelfortheinferenceof(linear)
inputsasshown,forinstance,in[53].
10
infuturework. Kalmanfilterswithfadingmemorycan,infact,beshowntobeequivalenttoanaturalgradient
descent on statistical manifolds ([2]) in the univariate case, see for instance [73, 74]. Following then the known
correspondence ([70]) between natural gradient and Gauss-Newton type methods used in active inference [34]
for exponential family models, one can consider cases where active inference emerges as a special (i.e., fading-
memory)caseof(extended)Kalman-filters,andthendiscussasimilarapproachforthedualequationsinvolving
action and the feedback matrix L. It should be highlighted that, unlike repeatedly stated in the literature, we
believe it is all but clear how the minimisation scheme used by active inference ought to generalise algorithms
such as (extended) Kalman-filters beyond, perhaps, the use of higher embedding orders (however see above).
At the moment, most practical applications simply replace explicit K and L with approximations based on the
use of learning rates, e.g. [10], or on local linearisation methods [76] with varying integration time-steps [43].
4.2.4 Different cost functionals and derivations
A further point of departure between these models can be found in the cost functional minimised by the two
methods: a value, or cost-to-go, functional for LQG, equation (10), and a variational free energy functional
for active inference, equation (19). The most striking difference is that the former includes a time integral of
costs, while the second one doesn’t. This difference is crucial in nonlinear cases, or rather in cases where the
separation principle does not hold: the minimisation of variational free energy can in fact be seen as giving a
“time-independent”equilibriumstrategy,generatingafixedcontrolstrategyindependentoffutureobservations
[61] or equivalently, following [14], one can see the minimisation of variational free energy as implementing a
feedback rather than closed-loop policy that assumes fixed dynamics and costs in the limit of an infinite time
horizon [61]. At the same time, LQG methods based on the separation principle rely on the same (future)
time-independence assumption [14], thus implying that the two functionals are not fundamentally different at
this level. A more appropriate comparison may be drawn on generalisations of both approaches, path integral
control/KLcontrolononesideandtheminimisationofexpected freeenergyontheother,whereactiveinference
is claimed to generalise other control approaches [39, 23], however this comparison lies outside the scope of this
note, focusing instead on linear continuous time and continuous state-space models.
Finally, the free energy functional equation (15) appears to include extra terms when compared to the
value function of LQG equation (10). The presence of multiple prediction errors in the variational free energy
formulation is however just an expression of the probabilistic (i.e., Bayesian) derivation of inference methods
[50], nowgeneralisedtocontrolproblems. Thedifferentpredictionserrorsinthevariationalfreeenergydirectly
map to likelihood and prior distributions as obtained from the joint density of observed and hidden states used
to define free energy, see [21]. On the other hand, LQG derivations are often based on least-square methods
[50, 85] with no a-priori interpretation of uncertainty, normally included only post-hoc.
References
[1] RickAAdams,StewartShipp,andKarlJFriston. Predictionsnotcommands: activeinferenceinthemotorsystem. Brain
Structure and Function,218(3):611–643,2013.
[2] Shun-IchiAmari. Naturalgradientworksefficientlyinlearning. Neural computation,10(2):251–276,1998.
[3] BrianAndersonandJohnBMoore. Optimal control: linear quadratic methods. Prentice-Hall,Inc.,1990.
[4] PhilipWAnderson. Moreisdifferent. Science,177(4047):393–396,1972.
[5] KarlJ˚Astr¨om. Introduction to stochastic control theory. AcademicPress,1970.
[6] KarlJ˚Astr¨omandRichardMMurray. Feedbacksystems: anintroductionforscientistsandengineers. Princetonuniversity
press,2010.
[7] Ahmad N Atassi and Hassan K Khalil. A separation principle for the stabilization of a class of nonlinear systems. IEEE
Transactions on Automatic Control,44(9):1672–1687,1999.
[8] Michael Athans. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. IEEE
transactions on automatic control,16(6):529–552,1971.
[9] ManuelBaltieri. Activeinference: buildinganewbridgebetweencontroltheoryandembodiedcognitivescience. PhDthesis,
UniversityofSussex,2019.
[10] ManuelBaltieriandChristopherLBuckley. Anactiveinferenceimplementationofphototaxis. InArtificialLifeConference
Proceedings,pages36–43.MITPress,2017.
[11] Manuel Baltieri and Christopher L Buckley. The modularity of action and perception revisited using control theory and
activeinference. InArtificial Life Conference Proceedings,pages121–128.MITPress,2018.
[12] Manuel Baltieri and Christopher L Buckley. Nonmodular architectures of cognitive systems based on active inference. In
2019 International Joint Conference on Neural Networks (IJCNN),pages1–8.IEEE,2019.
[13] Manuel Baltieri and Christopher L Buckley. PID control as a process of active inference with linear generative models.
Entropy,21(3):257,2019.
[14] YaakovBar-ShalomandEdisonTse. Dualeffect,certaintyequivalence,andseparationinstochasticcontrol. IEEETransac-
tions on Automatic Control,19(5):494–500,1974.
[15] MatthewJ.Beal. Variational algorithms for approximate Bayesian inference. UniversityofLondonLondon,2003.
11
[16] YuKBelyaev. Analyticrandomprocesses. Theory of Probability & Its Applications,4(4):402–409,1959.
[17] MartinBiehl,ChristianGuckelsberger,ChristophSalge,SimnC.Smith,andDanielPolani. Expandingtheactiveinference
landscape: Moreintrinsicmotivationsintheperception-actionloop. Frontiers in Neurorobotics,12:45,2018.
[18] ChristopherMBishop. Pattern Recognition and Machine Learning. Springer-VerlagNewYork,2006.
[19] Rafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning. Journal of mathematical
psychology,76:198–211,2017.
[20] HarrietBrown,RickAAdams,IsabelParees,MarkEdwards,andKarlJFriston. Activeinference,sensoryattenuationand
illusions. Cognitive processing,14(4):411–427,2013.
[21] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free energy principle for action and
perception: Amathematicalreview. Journal of Mathematical Psychology,14:55–79,2017.
[22] ZheChen. Bayesianfiltering: Fromkalmanfilterstoparticlefilters,andbeyond. Statistics,182(1):1–69,2003.
[23] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,andKarlJFriston. Activeinferenceon
discretestate-spaces: asynthesis. arXiv preprint arXiv:2001.07203,2020.
[24] Jean Daunizeau. The variational laplace approach to approximate bayesian inference. arXiv preprint arXiv:1703.02089,
2017.
[25] JohnCDoyle. Guaranteedmarginsforlqgregulators. IEEE Transactions on automatic Control,23(4):756–757,1978.
[26] AlexanderAFeldbaum. Dual-controltheoryi-iv. Optimal Control Systems,1965.
[27] AnatolGFeldman. Newinsightsintoaction–perceptioncoupling. Experimental Brain Research,194(1):39–58,2009.
[28] Anatol G Feldman. Active sensing without efference copy: referent control of perception. Journal of neurophysiology,
116(3):960–976,2016.
[29] RonaldFFox. Stochasticcalculusinphysics. Journal of statistical physics,46(5-6):1145–1157,1987.
[30] RoyFoxandNaftaliTishby.Minimum-informationlqgcontrolparti: Memorylesscontrollers.In2016IEEE55thConference
on Decision and Control (CDC),pages5610–5616.IEEE,2016.
[31] DavidWFranklinandDanielMWolpert. Computationalmechanismsofsensorimotorcontrol. Neuron,72(3):425–442,2011.
[32] BernardFriedland. Treatmentofbiasinrecursivefiltering. IEEE Transactions on Automatic Control,14(4):359–367,1969.
[33] KarlFriston, BiswaSengupta, andGennaro Auletta. Cognitivedynamics: From attractors toactiveinference. Proceedings
of the IEEE,102(4):427–445,2014.
[34] KarlJFriston. Hierarchicalmodelsinthebrain. PLoS Computational Biology,4(11),2008.
[35] KarlJFriston. Thefree-energyprinciple: aunifiedbraintheory? Nature reviews. Neuroscience,11(2):127–138,2010.
[36] KarlJFriston. Whatisoptimalaboutmotorcontrol? Neuron,72(3):488–498,2011.
[37] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference? PLoS One, 4(7):e6421,
2009.
[38] Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-energy formulation.
Biological Cybernetics,102(3):227–260,2010.
[39] Karl J Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active inference: a
processtheory. Neural Computation,29(1):1–49,2017.
[40] Karl J Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pezzulo. Active
inferenceandepistemicvalue. Cognitive neuroscience,pages1–28,2015.
[41] Karl J Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal control without cost
functions. Biological Cybernetics,106(8-9):523–541,2012.
[42] KarlJFriston,KlaasStephan,BaojuanLi,andJeanDaunizeau.Generalisedfiltering.MathematicalProblemsinEngineering,
2010,2010.
[43] Karl J Friston, N. Trujillo-Barreto, and J. Daunizeau. DEM: A variational treatment of dynamic systems. NeuroImage,
41(3):849–885,2008.
[44] CrispinGardiner. Stochastic methods: a handbook for the natural and social sciences,volume4. springerBerlin,2009.
[45] MichaelGastpar,BixioRimoldi,andMartinVetterli.Tocode,ornottocode: Lossysource-channelcommunicationrevisited.
IEEE Transactions on Information Theory,49(5):1147–1158,2003.
[46] Tryphon T Georgiou and Anders Lindquist. The separation principle in stochastic control, redux. IEEE Transactions on
Automatic Control,58(10):2481–2494,2013.
[47] SamuelJGershman. Whatdoesthefreeenergyprincipletellusaboutthebrain? arXiv preprint arXiv:1901.07945,2019.
[48] JohnMHalley. Ecology,evolutionand1f-noise. Trends in ecology & evolution,11(1):33–37,1996.
[49] JipingHe,WilliamSLevine,andGeraldELoeb. Feedbackgainsforcorrectingsmallperturbationstostandingposture. In
Proceedings of the 28th IEEE Conference on Decision and Control,,pages518–526.IEEE,1991.
[50] AndrewHJazwinski. Stochastic Processes and Filtering Theory,volume64. AcademicPress,1970.
[51] Carroll D. Johnson. Optimal control of the linear regulator with constant disturbances. IEEE Transactions on Automatic
Control,13(4):416–421,1968.
[52] Carroll D. Johnson. Further study of the linear regulator with disturbances – The case of vector disturbances satisfying a
lineardifferentialequation. IEEE Transactions on Automatic Control,15(2):222–228,1970.
[53] Carroll D. Johnson. On observers for systems with unknown and inaccessible inputs. International journal of control,
21(5):825–831,1975.
[54] DPeterJosephandTJuliusTou. Onlinearcontroltheory. Transactionsof theAmericanInstitute ofElectricalEngineers,
Part II: Applications and Industry,80(4):193–196,1961.
12
[55] RudolfEKalman. Contributionstothetheoryofoptimalcontrol. Bol. Soc. Mat. Mexicana,5(2):102–119,1960.
[56] Rudolf E Kalman. A new approach to linear filtering and prediction problems. Journal of basic Engineering, 82(1):35–45,
1960.
[57] Rudolf E Kalman. On the general theory of control systems. In Proceedings First International Conference on Automatic
Control, Moscow, USSR,1960.
[58] Rudolf E Kalman and Richard S Bucy. New results in linear filtering and prediction theory. Journal of basic engineering,
83(1):95–108,1961.
[59] HilbertJKappen. Lineartheoryforcontrolofnonlinearstochasticsystems. Physical review letters,95(20):200201,2005.
[60] HilbertJKappen. Pathintegralsandsymmetrybreakingforoptimalcontroltheory. Journalofstatisticalmechanics: theory
and experiment,2005(11):P11011,2005.
[61] Hilbert J Kappen, Vicen¸c G´omez, and Manfred Opper. Optimal control as a graphical model inference problem. Machine
learning,87(2):159–182,2012.
[62] MitsuoKawato. Internalmodelsformotorcontrolandtrajectoryplanning. Current opinion in neurobiology,9(6):718–727,
1999.
[63] DavidCKnillandAlexandrePouget. TheBayesianbrain: theroleofuncertaintyinneuralcodingandcomputation. Trends
in Neurosciences,27(12):712–719,2004.
[64] DavidCKnillandWhitmanRichards. Perception as Bayesian inference. CambridgeUniversityPress,1996.
[65] MarkLLatash. Physics of Biological Action and Perception. AcademicPress,2019.
[66] TaiSingLeeandDavidMumford. Hierarchicalbayesianinferenceinthevisualcortex. JOSA A,20(7):1434–1448,2003.
[67] BaojuanLi,JeanDaunizeau,KlaasEStephan,WillPenny,DewenHu,andKarlJFriston.Generalisedfilteringandstochastic
dcmforfmri. neuroimage,58(2):442–457,2011.
[68] Gerald E Loeb, WS Levine, and Jiping He. Understanding sensorimotor feedback through optimal control. In Cold Spring
Harbor symposia on quantitative biology,volume55,pages791–803.ColdSpringHarborLaboratoryPress,1990.
[69] J L(cid:32)uczka. Non-markovian stochastic processes: Colored noise. Chaos: An Interdisciplinary Journal of Nonlinear Science,
15(2):026107,2005.
[70] JamesMartens. Newinsightsandperspectivesonthenaturalgradientmethod. arXiv preprint arXiv:1412.1193,2014.
[71] RichardJMeinholdandNozerDSingpurwalla. Understandingthekalmanfilter. TheAmericanStatistician,37(2):123–127,
1983.
[72] Sanjoy K Mitter and Nigel J Newton. A variational approach to nonlinear estimation. SIAM journal on control and
optimization,42(5):1813–1833,2003.
[73] YannOllivier. Onlinenaturalgradientasakalmanfilter. Electronic Journal of Statistics,12(2):2930–2961,2018.
[74] YannOllivier. Theextendedkalmanfilterisanaturalgradientdescentintrajectoryspace. arXivpreprintarXiv:1901.00696,
2019.
[75] ManfredOpperandC´edricArchambeau. Thevariationalgaussianapproximationrevisited. Neuralcomputation,21(3):786–
792,2009.
[76] TohruOzaki.Abridgebetweennonlineartimeseriesmodelsandnonlinearstochasticdynamicalsystems: alocallinearization
approach. Statistica Sinica,pages113–135,1992.
[77] NormanHPackard,JamesPCrutchfield,JDoyneFarmer,andRobertSShaw.Geometryfromatimeseries.Physicalreview
letters,45(9):712,1980.
[78] Thomas Parr and Karl J Friston. The discrete and continuous brain: from decisions to movementand back again. Neural
computation,30(9):2319–2347,2018.
[79] RajeshPNRao. Anoptimalestimationapproachtovisualperceptionandlearning. Visionresearch,39(11):1963–1989,1999.
[80] RajeshPNRaoandDanaHBallard.Predictivecodinginthevisualcortex: afunctionalinterpretationofsomeextra-classical
receptive-fieldeffects. Nature neuroscience,2(1):79–87,1999.
[81] NoorSajid,PhilipJBall,andKarlJFriston. Activeinference: demystifiedandcompared. arXivpreprintarXiv:1909.10863,
2019.
[82] ClaudeElwoodShannon. Amathematicaltheoryofcommunication. Bell system technical journal,27(3):379–423,1948.
[83] HerbertASimon. Dynamicprogrammingunderuncertaintywithaquadraticcriterionfunction. Econometrica, Journal of
the Econometric Society,pages74–81,1956.
[84] HaroldWSorenson. Least-squaresestimation: fromgausstokalman. IEEE spectrum,7(7):63–68,1970.
[85] RobertFStengel. Optimal control and estimation. CourierCorporation,1994.
[86] RouslanLStratonovich. Topics in the theory of random noise,volume2. CRCPress,1967.
[87] Floris Takens. Detecting strange attractors in turbulence. In Dynamical systems and turbulence, Warwick 1980, pages
366–381.Springer,1981.
[88] TakashiTanaka,PMohajerinEsfahani,andSanjoyKMitter. Lqgcontrolwithminimalinformation: Three-stageseparation
principleandsdp-basedsolutionsynthesis. arXiv preprint arXiv:1510.04214,2015.
[89] SekharCTatikonda. Control under communication constraints. PhDthesis,MassachusettsInstituteofTechnology,2000.
[90] HenriTheil. Anoteoncertaintyequivalenceindynamicplanning. Econometrica: JournaloftheEconometricSociety,pages
346–349,1957.
[91] EmanuelTodorov. Optimalityprinciplesinsensorimotorcontrol. Nature neuroscience,7(9):907–915,2004.
[92] EmanuelTodorov.Stochasticoptimalcontrolandestimationmethodsadaptedtothenoisecharacteristicsofthesensorimotor
system. Neural computation,17(5):1084–1108,2005.
13
[93] EmanuelTodorov. Optimalcontroltheory. Bayesian brain: probabilistic approaches to neural coding,pages269–298,2006.
[94] EmanuelTodorov. Generaldualitybetweenoptimalcontrolandestimation. InDecisionandControl,2008.CDC2008.47th
IEEE Conference on,pages4286–4292.IEEE,2008.
[95] EmanuelTodorovandMichaelIJordan. Optimalfeedbackcontrolasatheoryofmotorcoordination. Nature neuroscience,
5(11):1226,2002.
[96] Alexander Tschantz, Anil K Seth, and Christopher L Buckley. Learning action-oriented models through active inference.
PLOS Computational Biology,16(4):e1007805,2020.
[97] PedroAValdes-Sosa,AlardRoebroeck,JeanDaunizeau,andKarlJFriston. Effectiveconnectivity: influence,causalityand
biophysicalmodeling. Neuroimage,58(2):339–361,2011.
[98] NicolaasGVanKampen. Itoˆversusstratonovich. Journal of Statistical Physics,24(1):175–187,1981.
[99] NicolaasGVanKampen. Stochastic processes in physics and chemistry,volume1. Elsevier,1992.
[100] DavidAVasseurandPeterYodzis. Thecolorofenvironmentalnoise. Ecology,85(4):1146–1152,2004.
[101] ErichvonHolstandHorstMittelstaedt. Dasreafferenzprinzip. Naturwissenschaften,37(20):464–476,1950.
[102] PeterWhittle. Risk-sensitivelinear/quadratic/gaussiancontrol. Advances in Applied Probability,13(4):764–777,1981.
[103] Daniel M Wolpert and Zoubin Ghahramani. Computational principles of movement neuroscience. Nature neuroscience,
3(11s):1212,2000.
[104] EugeneWongandMosheZakai.Ontheconvergenceofordinaryintegralstostochasticintegrals.TheAnnalsofMathematical
Statistics,36(5):1560–1564,1965.
[105] WMurrayWonham. Ontheseparationtheoremofstochasticcontrol. SIAM Journal on Control,6(2):312–326,1968.
14