arXiv:1802.10546v2  [cs.AI]  18 Jun 2018
Computational Theories of Curiosity-Driven Learning
Pierre-Yves Oudeyer ∗
Inria and Ensta ParisT ech, France
Abstract
What are the functions of curiosity? What are the mechanisms of curiosity-driven learn-
ing? W e approach these questions about the living using conc epts and tools from machine
learning and developmental robotics. W e argue that curiosi ty-driven learning enables organ-
isms to make discoveries to solve complex problems with rare or deceptive rewards. By foster-
ing exploration and discovery of a diversity of behavioural skills, and ignoring these rewards,
curiosity can be efﬁcient to bootstrap learning when there i s no information, or deceptive in-
formation, about local improvement towards these problems . W e also explain the key role
of curiosity for efﬁcient learning of world models. W e revie w both normative and heuristic
computational frameworks used to understand the mechanism s of curiosity in humans, con-
ceptualizing the child as a sense-making organism. These fr ameworks enable us to discuss
the bi-directional causal links between curiosity and lear ning, and to provide new hypotheses
about the fundamental role of curiosity in self-organizing developmental structures through
curriculum learning. W e present various developmental rob otics experiments that study these
mechanisms in action, both supporting these hypotheses to u nderstand better curiosity in hu-
mans and opening new research avenues in machine learning an d artiﬁcial intelligence. Finally,
we discuss challenges for the design of experimental paradi gms for studying curiosity in psy-
chology and cognitive neuroscience.
Keywords: Curiosity, intrinsic motivation, world models, rewards, free-energy principle,
learning progress hypothesis, lifelong learning, predict ions, machine learning, AI, develop-
mental robotics, development, curriculum learning, self- organization.
1. Introduction
Humans and many other animals spontaneously explore their e nvironments. This often happens
without a pressure for ﬁnding extrinsic rewards like food, a nd without external incentives from
their social peers. Such spontaneous exploration seems to b e produced by an internal mechanism
pushing them to make sense of their world: they explore for th e intrinsic purpose of getting better
at predicting and controlling the world. This spontaneous i nvestigation of the environment, and of
the link between one’s own physical and cognitive capabilit ies and the environment, can take many
different forms. This ranges from babies trying various way s to locomote, or exploring grasping,
∗ http://www.pyoudeyer.com
2 Pierre-Yves Oudeyer
manipulating, banging or throwing of all sorts of objects, o r testing how social peers respond to
vocalizations, to children practicing tool building with w ooden sticks, or throwing wooden sticks
in rivers to see how they will ﬂow , to adults searching inform ation about a hobby , learning a new
sport, or a scientist turning his telescope towards faraway galaxies.
All these exploratory behaviours can be seen as questions po sed by the organism about its envi-
ronment or about the relation between its environment and it ’s own current state of knowledge and
skills. These questions can be formulated in various ways ra nging from actual physical/behavioural
experimentation to formulating a linguistic question.
These mechanisms have been discussed from various perspect ives in the scientiﬁc litera-
ture, and in particular using the related concepts of curios ity [Berlyne, 1960], intrinsic moti-
vation [Harlow , 1950], and free play [Bruner et al., 1976]. A cross many different ﬁelds, theo-
rists have suggested that interest is engaged by what is just beyond current knowledge, neither
too well known nor too far beyond what is understandable. Thi s idea has been offered many
times in psychology , through concepts like cognitive disso nance [Kagan, 1972], optimal incon-
gruity [Hunt, 1965], intermediate novelty [Berlyne, 1960, Kidd et al., 2012] and optimal challenge
[Csikszenthmihalyi, 1991], and recent research in neurosc ience is now investigating the associated
neural mechanisms [Gottlieb et al., 2013].
Several formal frameworks have recently enabled to improve our theoretical understanding
of these mechanisms. This includes frameworks considering the curiosity system as a machine
which goal is to build predictive and control models of the wo rld [Kaplan and Oudeyer, 2007a,
Oudeyer and Smith, 2016] - and sometimes the brain as a whole i s conceptualized like this as in
the free energy principle [Friston et al., 2017a, Friston et al., 2017b]. Related to this, reinforcement
learning and optimization frameworks consider curiosity a s a mechanism which allows to gener-
ate diversity in exploration, enabling to get out of local mi nima in searching for behaviours that
maximize extrinsic rewards or ﬁtness [Schmidhuber, 1991, B arto, 2013, Baldassarre and M., 2013,
Lehman and Stanley , 2008, Bellemare et al., 2016, Forestier et al., 2017, Colas et al., 2018].
2. Curiosity for Exploration and Discovery in an Open W orld
An apparent evolutionary mystery is that such spontaneous i nvestigations of the environment are
often very costly in time and energy , and do not appear at ﬁrst sight to provide a direct beneﬁt for
feeding, mating, survival and reproduction. So how could su ch mechanisms evolve? What is their
function?
In an uncertain world with rare resources, one could expect t hat organisms spare their energy
to explore only parts of the environment that are likely to pr ovide information about where to
get these resources. However, the real world is not only unce rtain, but from the point of view of
many basic physiological needs like ﬁnding food, it is full o f multimodal multidimensional stimuli
that are not obviously relevant to these needs. Animals have initially little ideas of what kinds
of actions are required to fulﬁl them. Thus, when extrinsic r ewards (resources) are hard to ﬁnd,
the main challenge is not to estimate the relative reward pro babilities associated to a few reward-
Computational Theories of Curiosity-Driven Learning 3
relevant options in order to maximize the efﬁciency of a know n solution. Rather, the challenge
is to discover the ﬁrst few bits of rewards and how to build coa rse strategies to get them. In this
context where discovery of new strategies and new outcomes b ecomes the main issue (as opposed
to reﬁning a known strategy for getting a known outcome), one can better see how to make sense
of curiosity-driven exploration in living organisms.
Let’s take the example of an 8-9 months old baby , sitting on th e ground and alone in a room.
He has seen a box of sweets on top of a kitchen’s furniture, and aims to get them. While the baby
might really be motivated to get the sweets for a moment, this task is for him a real conundrum.
The situation is full of multiple kinds of deep uncertaintie s, and most importantly deep unknowns.
First of all, the child has only a very approximate idea of the current state of the world: he sees the
sweet box from far away with his eyes, and estimating simple t hings such as distance and height is
already very difﬁcult, since he is mostly used to interact wi th objects that are already in his hands,
and has limited skills to estimate the state of far away objec ts. Second, the child has no clue about
how to get to the sweet box, and has no clue where to look to ﬁnd i nformation about a solution. He
does not even know how to stand up on its two feet, and his crawl ing strategy is very imprecise to
move around. He does not know yet what a tool is, and his brain c annot even imagine at this point
the possibility to push a chair next to the furniture, then tr y climb it to get to the box (at this point,
chairs are perceived as obstacles for him). Here, he is much b eyond uncertainty: it is meaningless
for him to compute probabilities or uncertainties associat ed to the success of this strategy (and its
associated sub-goals), as they involve events that are not a lready part of the space of hypotheses
he can consider. Just think of the intermediate skill of stan ding up on its two feet and climbing the
chair. Even if targeting these skills can be suggested by obs erving its social peers, they involve such
complex internal muscular synergies that initially the chi ld has also little cue about what patterns
of muscle activations, and what proprioceptive and externa l visual information to attend to control
these skills. Also very little can be inferred from observin g others, as high-dimensional muscular
activations and covert attention in these skills are not dir ectly observable.
So what could the child do? He might consider a proxy internal measure for guiding its search
for the sweet box, such as the current distance between his bo dy and the sweets. Y et, optimizing it
with its current skills would just make it crawl to the bottom of the furniture and extend its arm for
a hopeless far reach. In that case this proxy measure would be less sparse than the sweet reward
itself, but it would be highly deceptive, and equally inoper ant. Rather, a good strategy for the child
shall be to simply forget about this target sweet box for a whi le, go back to his playground and
explore spontaneously , driven by curiosity , various forms of skills ranging from body locomotion
to object manipulation and tool use. Playing around with its own body shall then lead him to
discover both the concept of and strategies for climbing up f urniture, as well as the concept and
strategies for using objects of all kinds as tools. Then, whe n coming back a few months later to the
task of getting to the sweet, and once he will have acquired a s kill repertoire including walking,
pushing chairs around, and using them as tools to get upwards , the solution might become much
more accessible. At this point only the problem will be looki ng like a classical reinforcement
learning problem in the lab with few obvious relevant option s to choose from.
Research in developmental robotics and artiﬁcial intellig ence has conﬁrmed this intuition in
4 Pierre-Yves Oudeyer
the last decade through computational and robotics experim ents. Several strands of works have
considered the problem of how a machine could solve difﬁcult problems with rare or deceptive
rewards [Barto et al., 2004, Schmidhuber, 1991, Lehman and S tanley , 2008], sometimes directly
aiming at modelling how human children explore and learn in t hese contexts [Oudeyer et al., 2007,
Oudeyer and Smith, 2016]. They found that various forms of cu riosity-driven exploration can in-
deed be the key to make discoveries and solve these complex re inforcement learning problems.
An example is the artiﬁcial intelligence and robotics exper iment presented in
[Forestier et al., 2017] (see Figure 1), where a high-dimens ional humanoid ’baby’ robot is
situated in an environment with many objects. Some of them ar e more or less difﬁcult to learn
to control, and some other are uncontrollable by the robot, s uch as another robot moving around.
However, the ’baby’ robot does not know initially which obje cts it can learn to control and which
ones it cannot. Among these objects, a ball is placed in an are a beyond the direct reach of the
robot. Y et, other objects can be used as tools to enable the ro bot to move the ball. The robot can
use its hand to push a joystick, which in turn moves a tele-ope rated crane toy , and this crane toy
can push the ball. However, the robot has no concept of tool in itially and does not know that these
objects can physically interact: it has no pre-coded way to r epresent such physical interactions. It
can send low-level motor commands into the motors of its arm. It can perceive object positions
and movements individually . Y et, it does not know initially how speciﬁc sequences of motor
commands relate to potential movements of each object (and h ow the movements of objects might
relate to each other). While this robotic situation is simpl iﬁed as compared to most real world
situations encountered by human infants, it is already extr emely complex. Indeed, the sequence of
motor commands for a simple arm movement needed to reach for a n object amounts to specifying
several dozen continuous numbers, while the perception of t he movement of each single object
during one second is also encoded by several dozens continuo us numbers. As a whole, the
sensorimotor space that the robot explores has several hund red dimensions.
Given such an environment, let us imagine an engineer imposi ng the following external ob-
jective to the robot: it should learn to move the ball forward at a maximal speed. T o deﬁne this
objective, the engineer can design an extrinsic reward sign al: each time the robot tries a sequence
of motor commands that produce a movement of the ball, this re ward is a scalar number propor-
tional to the speed and target direction of the ball. Each tim e the motor commands do not produce
any ball movement, the reward is zero.
The standard machine learning approach to enable the robot t o solve this problem is rein-
forcement learning (RL) [Sutton and Barto, 2018]. This can b e viewed as a family of optimization
algorithms that learn optimal controllers, i.e. learn to pr oduce sequences of motor commands
that maximize the reward. The way standard approaches to RL w ork is through a combination of
hill-climbing (gradient descent) and random exploration. For example, state-of-the-art deep rein-
forcement learning algorithms for learning continuous con trol, such as DDPG and related algo-
rithms [Lillicrap et al., 2015, Schulman et al., 2017, Sigau d and Stulp, 2018], work by alternating
between updating the current controller solution in order t o climb the hill of rewards (this requires
that rewards of different magnitudes are observed when slig htly changing the controller), and pro-
ducing random perturbations of the current best controller to obtain further information about the
Computational Theories of Curiosity-Driven Learning 5
Figure 1. Curiosity-driven exploration through autonomous goal set ting and self-organized
curriculum learning in the experimental setup presented in [Forestier et al., 20 17] (see video:
https://www.youtube.com/watch?v=NOLAwD4ZTW0). Here a humanoid robot is sur-
rounded by objects. The robot can learn to control some of the m, while others are unlearnable
distractors. Some objects can interact with each other, e.g . be used as tool to control other ob-
jects. The robot initially does not know which skills and obj ects are learnable, nor that objects
may interact with each others. If an engineer would like the r obot to learn how to move the ball
forward by providing rewards only when the ball moves, then u sing classical reinforcement learn-
ing approaches would fail. Indeed, RL approaches combine hi ll-climing mechanisms that require
observations of non-zero reward to improve the current solu tion, and rely on random exploration
otherwise. Here, the time required by random exploration to enable the robot to ﬁnd such a rare
reward would be prohibitively long. The robot initially has no idea where to look for cues to solve
this task. Indeed, moving the ball entails moving the white e lectric crane toy , which can only
be moved by pushing one of the two joysticks, which can itself only be moved by appropriate
movements of the hand, requiring speciﬁc sequences of motor commands in the joints. A more
efﬁcient approach is to use curiosity-driven exploration, where one lets the robot self-generate its
own goals for controlling various objects, and spend time on the ones which produce maximal
learning progress. The internal generation of goals, and th e focus on goals providing maximal
learning progress, model a form of curiosity . Doing so, the r obot will ﬁrst focus on playing with its
hand, which is initially providing maximal learning progre ss. Then, it will discover as a side effect
how to move the joysticks. In turn, due to the physical coupli ngs in the environment, focusing on
learning about the joysticks makes the robot discover how to move the crane toy and the ball in a
few hours.
6 Pierre-Yves Oudeyer
reward distribution.
However, such an RL approach will not work in the robotic envi ronment considered here.
Indeed, in this particular environment, the problem of movi ng the ball forward is said to be a ’rare
reward’ problem. Indeed, due to the structure of the environ ment, the vast majority of possible
sequences of motor commands will produce a reward of zero. Ac tually , most random sequences of
motor commands will make that the robot will not even touch th e joystick in front of him. Thus, if
the robot tries actions randomly , there is a very low probabi lity to get a non-zero reward, i.e. that its
arms touches the joystick in a peculiar way that makes the cra ne toy in a peculiar way that makes
the ball move. This is a problem as the hill-climbing mechani sms of RL algorithms need to observe
distributions of non-zero reward to update the controller, and doing random exploration of motor
commands would here take a prohibitively long time before th e ﬁrst non-zero rewards are found,
i.e. before ﬁnding the ﬁrst few action sequences that produc e ball movement. This problem of
RL approaches that focus on hill-climing of the extrinsic re ward is now well-known, and applies to
man environments with rare or deceptive rewards 1 [Bellemare et al., 2016, Sigaud and Stulp, 2018,
Colas et al., 2018].
An alternative computational approach studied in [Foresti er et al., 2017] has consisted in equip-
ping the robot with the capability to self-generate and expl ore many other goals than moving the
ball. For example, the system can self-generate and focus on goals such as moving the hand in
various directions, pushing any of the joysticks along part icular trajectories, or trying to move the
crane toy in diverse ways. The idea is that the system decides to spend time learning about one
of these goals based on an intrinsic measure of the learning p rogress towards solving them, and
ignoring completely how much they provide information abou t the goal of the engineer (move the
ball). Formally , self-generating a goal (e.g. move toy in di rection d) amounts to self-deﬁning an
internal reward function which quantiﬁes how well sequence of motor commands produce the tar-
geted outcome (e.g. the reward can be a scalar value proporti onal to the difference between the
reached toy direction and the target toy direction). Then, t he system can use RL algorithms to
learn controllers for these internally deﬁned goals and rew ards. This whole process models sev-
eral aspects of curiosity-driven exploration. First, it in cludes internal spontaneous generation of
goals. Second, it includes a meta-cognitive mechanism to as sess the relative ’interestingness’ of
self-generated goals, based on quantifying expected learn ing progress. This is used by the robot to
decide on which goal to focus on at any given moment in time, an d to self-organize a curriculum
of learning.
Intuitively this may appear to be an even more difﬁcult situa tion for the robot, as it consists in
providing it with many other potentially difﬁcult problems in addition to the initial problem of the
engineer. However, this turned out to facilitate the acquis ition of skills to move the ball forward.
Indeed, due to the structure and richness of the physical env ironment, the robot manages to ﬁnd
1 Deceptive rewards are local improvement of rewards that pus h the learner to update the controller in wrong direc-
tions in relation with the global optimum. For example, in th e infant example above, this would be the child observing
that elevating the hand towards the sweet box on the table dec reases the distance between him and the sweetbox. If the
child is considering the hand-sweet distance as a measure of reward, this would be reinforcing this strategy . However,
this would get the child away from more efﬁcient strategies o n the long term such as fetching a chair and climbing it.
Computational Theories of Curiosity-Driven Learning 7
continuously goals where learning can happen efﬁciently du e to dense reward feedback, leading
to an increase in behavioural diversity , and then to an incre ase of probability to ﬁnd sequences of
motor commands that solve other goals with rarer rewards. In this particular case, this curiosity-
driven exploration mechanism pushed the robot to ﬁrst focus on moving its hand around, leading
to a greater diversity of motor behaviours. As a side effect, this enabled the discovery of how to
move the joystick 2 . Then in turn, moving the joystick became interesting as a so urce of learning
progress. This increased the focus on practicing goals rela ted to the joysticks, which made the
robot discover that the crane toy can be moved around, leadin g quickly to the discovery of how to
move the ball. As the ﬁrst ball movements are discovered, cho osing the goal of moving the ball
becomes easier, as the hill-climbing mechanism of reinforc ement learning can efﬁciently compute
how to change the controller to improve ball movements. Foll owing this process, curiosity-driven
exploration enables the robot to learn in a few hours how to so lve the task of moving the ball
around, even without initially pointing this task as partic ularly relevant among many other tasks
that are also learnt. On the contrary , this would have been pr ohibitively long and difﬁcult to learn
by only considering and focusing on this task externally deﬁ ned by the engineer.
This example relies on computational models of curiosity-d riven exploration that were explic-
itly motivated by modeling mechanisms of human spontaneous exploration and their role in ex-
plaining the discovery of tool use [Forestier and Oudeyer, 2 016b, Forestier and Oudeyer, 2016a],
vocalization [Moulin-Frier et al., 2014] and language [For estier et al., 2017] by children. However,
several other strands of research in artiﬁcial intelligenc e have converged to the same conclusion
that curiosity-driven exploration could be a key to solve co mplex tasks with rare or deceptive re-
wards. For example, several works in the ﬁeld of evolutionar y computation have shown that novelty
search could be essential in solving difﬁcult optimization problems [Lehman and Stanley , 2008],
sometimes in combination with the task-speciﬁc reward [Mou ret and Doncieux, 2012], but also
sometimes by completely forgetting this task speciﬁc rewar d in the novelty search process
[Lehman and Stanley , 2008]. In the domain of reinforcement l earning, the idea of exploration
bonuses [Andreae and Andreae, 1978, Sutton, 1990, Dayan and Sejnowski, 1996] has also been
shown to increase the efﬁciency of reinforcement learning a lgorithms, with the addition of
a reward component measuring quantities such as the novelty of a state [Sutton, 1990], pre-
diction progress [Schmidhuber, 1991, Kaplan and Oudeyer, 2 003], density of state exploration
[Ostrovski et al., 2017], predictive information [Martius et al., 2013], predictive information gain
[Little and Sommer, 2013], or empowerment [Salge et al., 201 4]. Within an intrinsically mo-
tivated multi-goal reinforcement learning perspective, m easures of competence progress to-
wards self-generated goals were used to automatically gene rate learning curriculum for acquir-
ing complex skills [Baranes and Oudeyer, 2013, Oudeyer et al ., 2013, Nguyen and Oudeyer, 2014,
2 When the robot targets a goal, but observes effects relevant to another goal as a side effect, it is able to learn
retrospectively about this other goal using the collected o bservation. For example, if it targets to move the hand on the
right, but in practice moves it on the left and moves the joyst ick, it uses this observation to learn how to move the ball
left and how to move the joystick. This is a central property o f these intrinsically motivated goal exploration processe s
[Baranes and Oudeyer, 2013, Forestier et al., 2017], which i s shared with other related multi-goal learning algorithms in
RL such as Hindsight Experience Replay [Andrychowicz et al. , 2017].
8 Pierre-Yves Oudeyer
Forestier et al., 2017]. Within a hierarchical reinforceme nt learning perspective, intrinsically
motivated RL approaches have also been used as a framework en abling the discovery of hi-
erarchically reusable skills for boosting exploration [Ba rto et al., 2004, Kulkarni et al., 2016,
Machado et al., 2017]. Recent breakthrough in deep reinforc ement learning have leveraged these
various strands of works. For example, several deep reinfor cement learning algorithms were shown
to be able to solve difﬁcult problems with rare rewards (e.g. video games), sometimes by even
ignoring the score measure [Pathak et al., 2017], either by i ntroducing an intrinsic reward measur-
ing forms of novelty or learning progress [Ostrovski et al., 2017, Kulkarni et al., 2016], or by in-
troducing auxiliary tasks [Jaderberg et al., 2016, Andrych owicz et al., 2017, Florensa et al., 2017,
Cabi et al., 2017] in ways that are related to intrinsically m otivated goal exploration approaches in
developmental robotics [Baranes and Oudeyer, 2013, Forest ier et al., 2017, P ´ er´ e et al., 2018].
3. The Child as a Sense-Making Organism
This computational perspective provides a conceptual fram ework to understand how various forms
of curiosity-driven exploration can be instrumental for an organism to discover solutions to extrin-
sic tasks that are essential to its survival, such as ﬁnding e xtrinsic rewards like food. In this context,
it becomes possible to make sense of curiosity-driven explo ration in an evolutionary perspective
[Smith, 2013], and indeed further computational models hav e shown that intrinsic reward systems
could be the result of evolutionary processes optimizing fo r an individual ﬁtness to reproduce in
changing environments [Singh et al., 2010]. This perspecti ve converges with the hypothesis pro-
posed in psychology that humans might be equipped with dedic ated motivational neural circuitry
pushing them to explore the world for the pure sake of making s ense of it, and independently (yet
complementarily) from the search of extrinsic rewards [Ber lyne, 1960]. For example, Gopnick
[Gopnik, 2012] has suggested that the child could be viewed a s a curiosity-driven little scientist
equipped with an intrinsic urge to discover the underlying c ausal structure of the world (see also
[Chater and Loewenstein, 2016]).
Several theoretical models have considered mechanisms of c uriosity-driven exploration as or-
ganized exploration of the world with the purpose to build go od predictive and control models
of the world dynamics [Oudeyer et al., 2007, Friston et al., 2 017a]. For example, Friston and col-
leagues [Friston et al., 2017a] have developed a normative t heoretical framework, termed the free
energy principle, to characterize the theoretical propert ies of an ideal optimal Bayesian learner
trying to build a good predictive model of the world. More pre cisely , the free energy principle
framework views the brain as an active Bayesian inference sy stem that aims to minimize future ex-
pected surprise. A useful property of this framework is to ma thematically formalize various forms
of uncertainties which are necessarily encountered by the a ctive inference system, and which re-
duction can lead to corresponding various forms of explorat ion akin to curiosity (which are also
present in the learning progress framework detailed below) . For example, sources of uncertainty
that appear when mathematically decomposing the expected f ree energy include uncertainty about
hidden states given an action policy: minimizing it leads to active sensing and perceptual infer-
Computational Theories of Curiosity-Driven Learning 9
ence, i.e. a form of curiosity aiming to improve the subjecti ve estimation of the current world state.
Another dimension is uncertainty about policies in terms of expected future states or model param-
eters: this leads to forms of epistemic exploration and lear ning, i.e. a form of curiosity aiming to
improve the predictive model of what will be observed in the f uture depending on one’s own ac-
tions. Y et another dimension is uncertainty about the model structure itself: this leads to structure
learning and insight, a form of curiosity aiming to ﬁnd new ab stractions with structures that en-
able better predictions of what is happening in the environm ent. While the concept of goals is not
directly covered by the free-energy principle, other frame works like the autonomous goal setting
paradigm [Oudeyer and Kaplan, 2007, Forestier et al., 2017] , presented in the robotic experiments
above, point to other forms of uncertainty and curiosity cen tered around goals. Indeed, another
form of uncertainty is related to the self-evaluation of goa l competences: minimizing these forms
of uncertainty lead to curiosity-driven self-generation a nd practice of goals to learn about one’s
own competence to achieve them.
4. Normative or Heuristic Accounts? The Learning Progress H y-
pothesis
As the landscape of computational/mathematical theories o f curiosity-driven exploration quickly
develops [Baldassarre and M., 2013, Oudeyer and Smith, 2016 ], one becomes better equipped with
formal and experimental tools to conceptualize better what curiosity might be, and what it could
be used for in humans. However, this landscape of theories al so raises multiple open questions to
explain human curiosity . A ﬁrst fundamental question is how to articulate normative approaches
(e.g. the free-energy principle, empowerment) with heuris tics-based approaches (e.g. the learning
progress hypothesis [Kaplan and Oudeyer, 2007a, Oudeyer an d Smith, 2016]) to account for hu-
man behaviour and brain mechanisms. T aking the perspective of David Marr’s levels of analysis
for understanding cognition and the brain, shall normative approaches be standing at the computa-
tional level (describing which are the problems and the quan tities that the brain actually target) and
heuristic approaches at the algorithmic level (describing which algorithms can actually solve these
problems) on top of the implementation level investigating how neural circuitry could implement
these algorithms? The answer to this epistemological quest ion does not appear to be resolved yet.
Indeed, while normative approaches like the free-energy pr inciple have been used successfully
to account for several behavioural and neural observations ranging from saccadic eye movements
to habit learning to place cell activity [Friston et al., 201 7a, Friston et al., 2017b], it relies on as-
sumptions that are still speculative about what could be hap pening in the brain. First, it relies on
the assumption that the human brain is capable to achieve (ap proximate) hierarchical Bayesian in-
ference, but several strands of work have shown a number of si tuations where the brain uses heuris-
tics that are far away from a Bayesian behaviour (e.g; [Giger enzer and Goldstein, 1996]). Second,
such a normative Bayesian framework requires that modelled human subjects initially know the full
space of possible hypotheses about possible world states, p ossible policies, possible model param-
eters, and possible model structures. This deviates strong ly from the deep unknown encountered
10 Pierre-Yves Oudeyer
by children as described in the example above: new hypothese s can be formed out of interaction
with the world and unsupervised representation learning. F inally , active structured Bayesian in-
ference is computationally very costly [Buckley et al., 201 7], and quickly become computationally
intractable for problems of moderate size.
The challenges to address efﬁcient and tractable curiosity -driven exploration in real world
situations also underly the development of heuristic theor ies of curiosity-driven learning
[Oudeyer et al., 2013]. Within the perspective of the ’child as a sense making organism’, these
heuristic theories have considered what mechanisms could e nable efﬁcient exploration and learning
in high-dimensional physical bodies, and under limited res ources of time, energy and cognitive ca-
pabilities. One such heuristic-oriented theoretical fram ework is the ’learning progress (LP) hypoth-
esis’, proposed in [Oudeyer and Kaplan, 2006, Kaplan and Oud eyer, 2007a, Oudeyer et al., 2007,
Oudeyer and Kaplan, 2007]. Here, curiosity-driven explora tion in living organisms is viewed as
driven by the heuristic estimation and search of various forms of expected learning progress3 .
More precisely , this includes a heuristic meta-cognitive m echanism that estimates expected learn-
ing progress associated to competing activities (stimuli t o observe, situations or self-generated
goals to engage in). This estimation of LP is then used to choo se which activities to explore,
selecting in priority activities that maximize expected le arning progress.
The fundamental similarity between the free-energy princi ple and the LP hypothesis is that both
frameworks consider curiosity-driven exploration as a pro cess that aims to collect new information
to maximize the quality of an internal world model, and where this world model includes a model of
self-knowledge and self-competences. Another similarity is that both frameworks consider various
forms of learning progress, as the organism can learn variou s forms of knowledge and skills at
various scales of space and time. Some forms of learning prog ress can result from an attentional
action on a short time scale, providing information gain to b etter estimate the current world state.
But it can be also result from the choice to focus on an activit y for a longer time scale, producing
various forms of improvement of a predictive world model, ra nging from reduction in empirical
prediction errors to reduction of uncertainty about these p redictions (uncertainty could improve
without improving the average prediction error, and still t his would be a form of learning progress).
This latter form of learning progress (longer time scale of l earning, improvement of predictive
world model) has been the focused of most computational expe riment of the LP hypothesis so far
[Oudeyer and Smith, 2016].
There are also fundamental differences between the free-en ergy principle and the LP frame-
work. In the free-energy framework, the mechanisms used to r epresent and update world models
and their associated uncertainties are based on Bayesian in ference. On the contrary , the LP frame-
3 V arious works in artiﬁcial intelligence, machine learning and optimal experiment design have studied, in the last 50
years, how mechanisms of active learning can push a machine to explore parts of the state-action space that maximize
forms of information gain and learning progress. However, t hese lines of work did not propose and study the hypothesis
that related mechanisms could explain aspects of human curiosity-driven learning, and in other animals. Coming fr om
this modeling perspective, the LP hypothesis was developed independantly of these lines of work in machine learning
and AI. The convergence of these various strands of work towa rds related algorithms supports the strength and scope of
these mechanisms.
Computational Theories of Curiosity-Driven Learning 11
work has considered heuristic algorithms to learn world mod els from observations, and to esti-
mate uncertainty and learning progress, using mechanisms s uch as memory-based and lazy learn-
ing techniques [Forestier et al., 2017], as well as neural ne tworks [Kaplan and Oudeyer, 2003] and
evolution strategies [Benureau and Oudeyer, 2016]. One par ticular aspect of this difference is that
heuristic-based approaches do not require that the learner knows all possible events, event rep-
resentations, and model representations as it discovers th e world (on the contrary , the normative
framework requires priors about these events and their repr esentations, which is untractable for
real world situations). Unsupervised neural network repre sentation learning techniques can for ex-
ample be used to learn new spaces of representations in which to make predictions and set goals,
as the world is being discovered (e.g. [P ´ er´ e et al., 2018]) . Unsupervised learning techniques are
also used in the LP framework to learn incrementally abstrac tions of the low-level sensorimotor
ﬂow , enabling for example to form distinct concepts of the se lf, of inanimate objects and of others
based on their associated learnability properties [Oudeye r et al., 2007]. Finally , another difference
already mentionned earlier, is that the LP framework has bee n extended to cover mechanisms of
autonomous goal setting, which is a key dimension of curiosi ty-driven exploration in humans.
As the LP framework has studied what heuristics can drive efﬁ cient learning of world mod-
els in the real world, evaluation has leveraged robotics exp eriments under constraints of time
and processing, showing how these mechanisms enable learni ng multiple forms of locomotion
[Baranes and Oudeyer, 2013], manipulation of ﬂexible objec ts [Nguyen and Oudeyer, 2014], and
tool use discovery [Forestier et al., 2017] in high-dimensi onal continuous action and perceptual
spaces.
Beyond its theoretical origin, the learning progress hypot hesis makes behavioural predictions
that are compatible with a growing set of experimental evide nces in psychology . For example,
Gerken et al. [Gerken et al., 2011] showed that 17-months old children attend more to learnable ar-
tiﬁcial linguistic patterns than to unlearnable ones. Also , Kidd et al. [Kidd et al., 2012] showed that
infants prefer sequences of perceptual stimuli that have an intermediate level of complexity . Sim-
ilarly , Begus et al. [Begus et al., 2016] showed that infants selectively ask the help of informants
based on expected information they can provide. Baranes et a l. [Baranes et al., 2014] showed how
adult subjects, who were free to explore and select tasks of v arious difﬁculties, spontaneously or-
ganize their exploration in growing order of difﬁculty and s ettle on levels of intermediate difﬁculty
just beyond their current skill level.
5. Curiosity and Self-Organization in Development
These computational studies of the learning progress hypot hesis have also uncovered a crucial
emergent property of such a heuristic mechanism: it spontan eously leads the learner to avoid sit-
uations which are either trivial or too complicated 4 , and focus on situations that are just beyond
4 Other heuristics like novelty or surprise bonuses proposed in the RL litterature do not scale as well in open real
world environments as there are many tasks or situations whi ch produce novelty or surprise but yet should be avoided as
they are not learnable. An activity can be unlearnable due to unobservable causal factors, or to intrinsic unpredictabi lity .
12 Pierre-Yves Oudeyer
the current skills in prediction or control, exploring them as long as learning progress happens in
practice.
This has enabled to generate the new hypothesis that mechani sms of curiosity drive the
emergence of ordered developmental trajectories at long ti me scales [Kaplan and Oudeyer, 2007a,
Oudeyer and Smith, 2016]. Several studies have shown that su ch trajectories match sev-
eral fundamental properties of infant trajectories in doma ins such as vocal development
[Moulin-Frier et al., 2014], imitation [Kaplan and Oudeyer , 2007b] and tool use discovery
[Forestier and Oudeyer, 2016a, Forestier and Oudeyer, 2016 c]. Related models of curiosity-driven
learning, with intrinsic rewards based on information gain measured through empirical pre-
diction errors, have also shown how it could model the format ion of pro-social behaviors
[Baraglia et al., 2016], or model the development of aspects of binocular vision [Zhu et al., 2017]
or visual search in infants [T womey and W estermann, 2017], a s well as different forms of ex-
ploratory behaviours in other animals [Gordon et al., 2014] .
5.1. The Playground Experiment
An example of self-organization of structured development al trajectories driven by
curiosity-driven exploration is the Playground Experimen t 5 [Oudeyer and Kaplan, 2006,
Kaplan and Oudeyer, 2007a, Oudeyer et al., 2007]. It illustr ates how mechanisms of curiosity-
driven exploration, dynamically interacting with learnin g, physical and social constraints, can
self-organize developmental trajectories. In particular , this leads a learner to successively discover
two important functionalities: object affordances and voc al interaction with its peers.
In these Playground Experiments, a quadruped learning robo t (the learner) is placed on an
infant play mat with a set of nearby objects and is joined by an adult robot (the teacher), see
Figure 2 (A) [Oudeyer and Kaplan, 2006, Kaplan and Oudeyer, 2 007a, Oudeyer et al., 2007]. On
the mat and near the learner are objects for discovery: an ele phant (which can be bitten or grasped
by the mouth), a hanging toy (which can be bashed or pushed wit h the leg). The teacher is pre-
programmed to imitate (with a different pitch of voice) the s ounds made by the learner when the
learning robot looks to the teacher while vocalizing at the s ame time.
The learner is equipped with a repertoire of motor primitive s parameterized by several contin-
uous numbers that control movements of its legs, head and a si mulated vocal tract. Each motor
primitive is a dynamical system controlling various forms o f actions: (a) turning the head in dif-
ferent directions; (b) opening and closing the mouth while c rouching with varying strength and
timing; (c) rocking the leg with varying angle and speed; (d) vocalizing with varying pitch and
length. These primitives can be combined to form a large cont inuous space of possible actions.
Similarly , sensory primitives allow the robot to detect vis ual movements, salient visual proper-
ties, proprioceptive touch in the mouth, and pitch and lengt h of perceived sounds. For the robot,
these motor and sensory primitives are initially black boxe s and he has no knowledge about their
semantics, effects or relations.
5 The text describing the Playground Experiment in this secti on, and the interaction of curiosity with social guidance
below , is partly adapated from [Oudeyer and Smith, 2016].
Computational Theories of Curiosity-Driven Learning 13
The learning robot learns how to use and tune these primitive s to produce diverse effects on its
surrounding environment, and exploration is driven by the m aximization of learning progress: the
robot chooses to perform physical experiences (experiment s) that improve maximally the quality
of predictions of the consequences of its actions, i.e. that improve its world model.
Figure 2 (B) outlines the computational architecture of cur iosity-driven learning, called IAC,
used in the playground experiment [Oudeyer et al., 2007]. A p rediction machine (M) learns to
predict the consequences of actions taken by the robot in giv en sensory contexts. For example,
this module might learn to predict (with a neural network) wh ich visual movements or propri-
oceptive perceptions result from using a leg motor primitiv e with certain parameters. A meta-
cognitive module estimates the evolution of errors in predi ction of M in various regions of the
sensorimotor space. More precisely , this module estimates the decrease in prediction errors for
particular kinds of actions or particular contexts. An exam ple of such a context could be the
prediction of the consequences of a leg movement when this ac tion is applied towards a partic-
ular area of the environment. These estimates of error reduc tion, measuring a form of learning
progress, are used to compute an intrinsic reward. This rewa rd is an internal quantity (a num-
ber) that is proportional to the decrease of prediction erro rs, and the maximization of this quan-
tity is the objective of action selection within a computati onal reinforcement learning architec-
ture [Kaplan and Oudeyer, 2003, Oudeyer et al., 2007, Oudeye r and Kaplan, 2007]. Importantly ,
the action selection system chooses most often to explore ac tivities where the expected intrinsic
reward is high, i.e. where the expected learning progress is high. However, this choice is prob-
abilistic, which leaves the system open to learning in new ar eas and open to discovering other
activities that may also yield progress in learning 6 . Since the sensorimotor ﬂow does not come
pre-segmented into activities and tasks, a system that seek s to maximize differences in learnability
is also used to progressively categorize the sensorimotor s pace into differentiated contexts. This
categorization thereby models the incremental creation an d reﬁning of cognitive categories differ-
entiating activities/tasks.
T o illustrate how such an exploration mechanism can automat ically generate ordered learning
stages, let us ﬁrst imagine a learner confronted with four ca tegories of activities, as shown on
ﬁgure 2 (C). The practice of each of these four activities, wh ich can be of varying difﬁculty , leads
to different learning rates at different points in time (see the top curves, which show the evolution
of prediction errors in each activity if the learner were to f ocus full-time and exclusively on each).
If, however, the learner uses curiosity-driven exploratio n to decide what and when to practice by
focusing on progress niches, it will avoid activities alrea dy predictable (curve 4) or too difﬁcult to
learn to predict (curve 1), in order to focus ﬁrst on the activ ity with the highest learning rate (curve
3) and eventually , when the latter starts to reach a ’plateau ’, to switch to the second most promising
6 T echnically the decision on how much time to spend on a given a ctivity/context is achieved using Multi-Armed
Bandit algorithms for the so-called exploration/exploita tion dilemma [Audibert et al., 2009]. As the measure of learn ing
progress in each arm is used as the reward to maximize, this is a non-stationary bandit algorithm setting. However, a
speciﬁcity of learning architectures used in the robotic ex periments presented here is that instead of relying on a set o f
pre-deﬁned bandit arms [Audibert et al., 2009], an unsuperv ised learning algorithm dynamically builds new bandit arms
to select from [Baran` es and Oudeyer, 2009].
14 Pierre-Yves Oudeyer
Figure 2. The Playground Experiment [Oudeyer and Kaplan, 20 06, Oudeyer et al., 2007] (A) The
learning context; (B) The computational architecture for c uriosity-driven exploration: 1) the robot
learner probabilistically selects actions and contexts ac cording to their potential to provide infor-
mation that improves the world model (i.e. reduces predicti on errors); 2) an unsupervised learning
algorithms progressively differentiates actions and cont exts to be selected; (C) Illustration of a
self-organized developmental sequence where the robot aut omatically identiﬁes, categorizes and
shifts from simple to mode complex learning experiences. Fi gure adapted with permission from
[Gottlieb et al., 2013].
learning situation (curve 2). Thus, embodied exploration d riven by learning progress creates an
organized exploratory strategy , i.e. a developmental traj ecotory: the system systematically achieves
these learning experiences in an order and does so because th ey yield (given the propensities of the
learner and the physical world) different patterns of uncer tainty reduction.
In the Playground experiment, multiple experimental runs l ead to two general categories of
results: self-organization and a mixture of regularities a nd diversities in the developmental patterns
Computational Theories of Curiosity-Driven Learning 15
[Oudeyer and Kaplan, 2006, Oudeyer et al., 2007].
5.2. Self-Organization
In all of the runs, one observes the self-organization of str uctured developmental trajectories, where
the robot explores objects and actions in a progressively mo re complex stage-like manner. During
this exploration, the robot acquires autonomously diverse affordances and skills that can be reused
later on and that change the learning progress in more compli cated tasks, triggering a developmen-
tal cascade. The following developmental sequence was typi cally observed:
1. In a ﬁrst phase, the learner achieves unorganized body bab bling;
2. In a second phase, after learning a ﬁrst rough model and met a-model7 , the robot stops com-
bining motor primitives, exploring them one by one, but each primitive is explored itself in
a random manner;
3. In a third phase, the learner begins to experiment with act ions towards zones of its environ-
ment where the external observer knows there are objects (th e robot is not initially provided
with a representation of the concept of object), but in a non- affordant manner (e.g. it vocal-
izes at the non-responding elephant or tries to bash the teac her robot which is too far to be
touched);
4. In a fourth phase, the learner now explores the affordance s of different objects in the envi-
ronment: typically focussing ﬁrst on grasping movements wi th the elephant, then shifting to
bashing movements with the hanging toy , and ﬁnally shifting to explorations of vocalizing
towards the imitating teacher.
5. In the end, the learner has learnt sensorimotor affordanc es with several objects, as well
as social affordances, and has mastered multiple skills. No ne of these speciﬁc objectives
were pre-programmed. Instead, they self-organized throug h the dynamic interaction be-
tween curiosity-driven exploration, statistical inferen ce, the properties of the body , and the
properties of the environment.
These playground experiments do not simply simulate partic ular skills (such as batting at toys
to make them swing or vocalizations) but simulate an ordered and systematic developmental tra-
jectory , with a universality and stage-like structure that may be mistakenly taken to indicate an
internally-driven process of maturation. However, the tra jectory is created through activity and
through the general principle that sensorimotor experienc es that reduce uncertainty in predic-
tion are rewarding. In this way , developmental achievement s can build on themselves without
speciﬁc pre-programmed dependencies but nonetheless like evolution itself create structure (see
[Smith and Breazeal, 2007, Smith, 2013], for related ﬁnding s and arguments).
7 The ’model’ refers here to the predictive world model being l earnt, which enables to predict the consequences of
actions in a given context. The ’meta-model’ is another mode l built by the meta-cognite process, and continuously
estimates expected learning progress of the lower-level wo rld model.
16 Pierre-Yves Oudeyer
5.3. Regularities and Diversity
Because these are self-organizing developmental processe s, they generate not only strong regular-
ities but also diversity across individual developmental t rajectories. For example, in most runs one
observes successively unorganized body babbling, then foc used exploration of head movements,
then exploration of touching an object, then grasping an obj ect, and ﬁnally vocalizing towards a
peer robot (pre-programmed to imitate). This can be explain ed as a gradual exploration of new
progress niches, and those stages and their ordering can be v iewed as a form of attractor in the
space of developmental trajectories. Y et, with the same mec hanism and same initial parameters,
individual trajectories may invert stages, or even generat e qualitatively different behaviours. This
is due to stochasticity (the same motor commands do not produ ce always the same results), to
small variability in the physical realities and to the fact t hat this developmental dynamical system
has several attractors with more or less extended and strong domains of attraction (an attractor is a
part of the state-space in which the dynamical system conver ges, depending on what was his initial
state). W e see this diversity as a positive outcome since ind ividual development is not identical
across different individuals but is always, for each indivi dual, unique in its own ways. This kind
of approach, then, offers a way to understand individual dif ferences as emergent in developmental
processes itself and makes clear how developmental process es might vary across contexts, even
with an identical learning mechanism.
A further result to be highlighted is the early development o f vocal interaction. With a sin-
gle generic mechanism, the robot both explores and learns ho w to manipulate objects and how
to vocalize to trigger speciﬁc responses from a more mature p artner [Oudeyer and Kaplan, 2006,
Kaplan and Oudeyer, 2007a]. V ocal babbling and language gam es have been shown to be key
in infant language development; however, the motivation to engage in vocal play has often
been associated with hypothesized language speciﬁc motiva tion. The Playground Experiment
makes it possible to see how the exploration and learning of c ommunicative behaviour might
be at least partially explained by general curiosity-drive n exploration of the body affordances,
as also suggested by Oller [Oller, 2000]. Exploring this ide a further, Forestier and Oudeyer
[Forestier and Oudeyer, 2017] studied simulation showing h ow these mechanisms can drive the
joint development of speech and tool use, where speech is dis covered as a particular tool enabling
to get social peers achieve targeted actions.
5.4. Interaction with Social Guidance
Other robotic models have explored how social guidance can b e leveraged by an intrinsically mo-
tivated active learner and dynamically interact with curio sity to structure developmental trajecto-
ries [Thomaz and Breazeal, 2008, Nguyen and Oudeyer, 2014]. Focusing on vocal development,
Moulin-Frier et al. conducted experiments where a robot exp lored the control of a realistic model
of the vocal tract in interaction with vocal peers through a d rive to maximize learning progress
[Moulin-Frier et al., 2014]. This model relied on a physical model of the vocal tract, its motor
control and the auditory system. The experiments showed sel f-organization of vocal development
Computational Theories of Curiosity-Driven Learning 17
trajectories that share structural similarities with infa nts [Oller, 2000]. They showed how these
mechanism generate an adaptive transition from vocal self- exploration with little inﬂuence from
the speech environment, to a later stage where vocal explora tion becomes inﬂuenced by vocal-
izations of peers. Within the initial self-exploration pha se, a sequence of vocal production stages
self-organizes, and shares properties with infant data: th e vocal learner ﬁrst discovers how to con-
trol phonation, then vocal variations of unarticulated sou nds, and ﬁnally articulated proto-syllables.
As the vocal learner becomes more proﬁcient at producing com plex sounds, the vocalizations of
the teacher become vocal goals to imitate that provide high l earning progress, resulting in a shift
from self-exploration to vocal imitation.
6. Challenges and Perspectives
Computational theories have enabled to better understand t he potential structures and functions of
curiosity-driven learning in the last decade. These theori es have identiﬁed a wide diversity of algo-
rithmic mechanisms that could produce the kind of spontaneo us exploration displayed by humans
and other animals. This diversity concerns both the measure s of interests (e.g. novelty , surprise,
learning progress, knowledge gap, intermediate complexit y , ...) and the entities to which the brain
may apply them (e.g. actions, states, goals, objects, tools , places, games, activities, learning strate-
gies, social informants, ...), with time scales ranging fro m the moment-to-moment to days and
months. Furthermore, theoretical models of curiosity-dri ven learning, and their application in arti-
ﬁcial intelligence and machine learning, have shown the key role of these mechanisms for making
discoveries and solving real-world problems with rare or de ceptive rewards, in large and changing
environments. In brief, computational theories:
1. have shown that the term ’curiosity’ covers a wide diversi ty of complex mechanisms, gener-
ating different forms of exploration;
2. have proposed ways to model and study these mechanisms for mally , contributing to the
naturalization of the concept of ’curiosity’;
3. have shown that curiosity mechanism are essential to lear ning and development, and thus
should become a central topic in cognitive sciences and neur osciences.
Related work in psychology and neuroscience are beginning t o converge with computational
theories towards conceptualizing how mechanisms of curios ity can play a fundamental role in many
aspects of development, ranging from sensorimotor, cognit ive, emotional, to social development.
However, for multiple reasons, experimental work studying the underlying mechanisms of curiosity
have been very limited so far in psychology and neuroscience . The empirical testing of compu-
tational theories have been for a large part beyond the reach of existing experimental paradigms
in psychology and neuroscience. Several challenges need to be addressed to leverage further the
interaction between theory and experimentation.
18 Pierre-Yves Oudeyer
The need for novel experimental paradigms in psychology and cognitive neuroscience. A
ﬁrst general challenge is that curiosity denotes a set of mec hanisms that push individuals to explore
what is interesting for themselves, out of the consideratio n of external tasks or expectations of
social peers. Y et, the very act of participating to an experi ment in a lab brings up expectations in
the subject’s mind about what the experimenter wants to obse rve or analyze, or will think about
what they do. In the lab, curiosity can disappear quickly as s oon as one begins to observe it. This is
probably less the case with very young infants, but in their c ase the presence of social peers is also
bound to inﬂuence what they do, and their limited capabiliti es for physical exploration and verbal
reporting makes it difﬁcult to study advanced forms of curio sity . So, how to study curiosity when
setting up a controlled experiment introduces complex inte raction with other motivational forces
that are hard to control and evaluate? It is interesting to no te that the most clear observations of
curiosity in the lab do not come from studies targeting curio sity and information-seeking, but are
rather observations of child behaviour spontaneously doin g things that are wildly different from
the task the experimenter designed for them. For example, in recent experiments of Lauriane
Rat-Fisher and colleagues 8 about tool use development, children are asked to retrieve a salient
toy stuck in a tube (the toy was expected to be very attractive to the child). Y et, several children
showed spontaneous strong intrinsic interest in exploring how to push sticks and objects in the tube,
continuing to do it with a lot of fun after getting the toy out o f the tube, and completely ignoring the
toy . Unfortunately , these making off observations are typi cally removed from the lens of analysis
of traditional experimental studies, while they may displa y some of the most fundamental and
mysterious mechanisms of learning and cognition.
Another experimental challenge is how to disentangle the po tentially different mechanisms
identiﬁed by theory , which may be simultaneously at play in i ndividuals, and potentially on dif-
ferent time scales. For example, it could be possible that cu riosity-driven attention on very short
time scales may be driven by intrinsic rewards measuring dif ferent forms of novelty , surprise or
prediction error. However, on longer time-scales, the curi ous brain may value the intrinsic interest
of activities, games or goals with other measures of interes t like learning progress. The study of
curiosity over long time scales, focusing on how it may contr ibute to sculpt sensorimotor, cog-
nitive and social development, on how it develops itself wit h time, and on how it interacts with
other developmental forces such as social learning, is mayb e the most important and most difﬁcult
challenge in this scientiﬁc area.
7. Acnknowledgements
This manuscript has beneﬁted from very useful feeback and di scussions with members of the Flow-
ers team at Inria, as well as with Jacqueline Gottlieb, Linda Smith and Olivier Sigaud.
8 personal communication
Computational Theories of Curiosity-Driven Learning 19
References
[Andreae and Andreae, 1978] Andreae, P . M. and Andreae, J. H. (1978). A teachable machine in
the real world. International Journal of Man-Machine Studies , 10(3):301–312.
[Andrychowicz et al., 2017] Andrychowicz, M., Crow , D., Ray , A., Schneider, J., Fong, R., W elin-
der, P ., McGrew , B., T obin, J., Abbeel, P ., and Zaremba, W . (2 017). Hindsight experience replay .
In Advances in Neural Information Processing Systems .
[Audibert et al., 2009] Audibert, J. Y ., Munos, R., and Szepe svri, C. (2009). Explorationexploita-
tion tradeoff using variance estimates in multi-armed band its. Theoretical Computer Science ,
410(19):1876–1902.
[Baldassarre and M., 2013] Baldassarre, G. and M., M. (2013) . Intrinsically Motivated Learning
in Natural and Artiﬁcial Systems . Springer.
[Baraglia et al., 2016] Baraglia, J., Nagai, Y ., and Asada, M . (2016). Emergence of altruistic be-
havior through the minimization of prediction error. IEEE Transactions on Cognitive and De-
velopmental Systems , 8(3):141–151.
[Baranes et al., 2014] Baranes, A., Oudeyer, P ., and Gottlie b, J. (2014). The effects of task difﬁ-
culty , novelty and the size of the search space on intrinsica lly motivated exploration. Frontiers
in Neuroscience .
[Baran` es and Oudeyer, 2009] Baran` es, A. and Oudeyer, P .-Y . (2009). R-iac: Robust intrinsically
motivated exploration and active learning. IEEE Transactions on Autonomous Mental Develop-
ment, 1(3):155–169.
[Baranes and Oudeyer, 2013] Baranes, A. and Oudeyer, P .-Y . ( 2013). Active learning of inverse
models with intrinsically motivated goal exploration in ro bots. Robotics and Autonomous Sys-
tems, 61(1).
[Barto, 2013] Barto, A. (2013). Intrinsic motivation and re inforcement learning. In Intrinsically
motivated learning in natural and artiﬁcial systems , pages 17–47. Springer Berlin Heidelberg.
[Barto et al., 2004] Barto, A. G., Singh, S., and Chentanez, N . (2004). Intrinsically motivated
learning of hierarchical collections of skills. In Proceedings of the 3rd International Conference
on Development and Learning , pages 112–19.
[Begus et al., 2016] Begus, K., Gliga, T ., and Southgate (201 6). Infants’ preferences for na-
tive speakers are associated with an expectation of informa tion. Proc Natl Acad Sci U S A ,
113:12397–12402.
[Bellemare et al., 2016] Bellemare, M., Srinivasan, S., Ost rovski, G., Schaul, T ., Saxton, D., , and
Munos, R. (2016). Unifying count-based exploration and int rinsic motivation. In Advances in
Neural Information Processing Systems , page 14711479.
20 Pierre-Yves Oudeyer
[Benureau and Oudeyer, 2016] Benureau, F . C. and Oudeyer, P . -Y . (2016). Behavioral diversity
generation in autonomous exploration through reuse of past experience. Frontiers in Robotics
and AI , 3:8.
[Berlyne, 1960] Berlyne, D. (1960). Conﬂict, Arousal and Curiosity . McGraw-Hill, New Y ork.
[Bruner et al., 1976] Bruner, J. S., Jolly , A., and Sylva, K. ( 1976). Play: Its role in development
and evolution . Basic Books.
[Buckley et al., 2017] Buckley , C., Kim, C., McGregor, S., an d Seth, A. (2017). The free energy
principle for action and perception: A mathematical review . Journal of Mathematical Psychol-
ogy, 81:55–79.
[Cabi et al., 2017] Cabi, S., Colmenarejo, S. G., Hoffman, M. W ., Denil, M., W ang, Z., and
De Freitas, N. (2017). The intentional unintentional agent : Learning to solve many continu-
ous control tasks simultaneously . In 1st Conference on Robot Learning , CoRL.
[Chater and Loewenstein, 2016] Chater, N. and Loewenstein, G. (2016). The under-appreciated
drive for sense-making. Journal of Economic Behavior & Organization , 126:137–154.
[Colas et al., 2018] Colas, C., Sigaud, O., and Oudeyer, P .-Y . (2018). Gep-pg: Decoupling explo-
ration and exploitation in deep reinforcement learning alg orithms. In International Conference
on Machine Learning (ICML) .
[Csikszenthmihalyi, 1991] Csikszenthmihalyi, M. (1991). Flow-the Psychology of Optimal Expe-
rience. Perennial, Harper.
[Dayan and Sejnowski, 1996] Dayan, P . and Sejnowski, T . J. (1 996). Exploration bonuses and
dual control. Machine Learning , 25(1):522.
[Florensa et al., 2017] Florensa, C., Held, D., Wulfmeier, M ., Zhang, M., and Abbeel, P . (2017).
Reverse curriculum generation for reinforcement learning . In Proceedings of the 1st Annual
Conference on Robot Learning, in PMLR 78 , pages 482–495.
[Forestier et al., 2017] Forestier, S., Mollard, Y ., and Oud eyer, P .-Y . (2017). Intrinsically
motivated goal exploration processes with automatic curri culum learning. arXiv preprint
arXiv:1708.02190.
[Forestier and Oudeyer, 2016a] Forestier, S. and Oudeyer, P .-Y . (2016a). Curiosity-driven devel-
opment of tool use precursors: a computational model. In Proceedings of the 38th Annual
Conference of the Cognitive Science Society , pages 1859–1864.
[Forestier and Oudeyer, 2016b] Forestier, S. and Oudeyer, P .-Y . (2016b). Modular active curiosity-
driven discovery of tool use. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ Inter -
national Conference on , pages 3965–3972. IEEE.
Computational Theories of Curiosity-Driven Learning 21
[Forestier and Oudeyer, 2016c] Forestier, S. and Oudeyer, P .-Y . (2016c). Overlapping waves in
tool use development: a curiosity-driven computational mo del. In The Sixth Joint IEEE Inter-
national Conference Developmental Learning and Epigeneti c Robotics .
[Forestier and Oudeyer, 2017] Forestier, S. and Oudeyer, P . -Y . (2017). A uniﬁed model of speech
and tool use early development. In Proceedings of the 39th Annual Meeting of the Cognitive
Science Society .
[Friston et al., 2017a] Friston, K., FitzGerald, T ., Rigoli , F ., Schwartenbeck, P ., and Pezzulo, G.
(2017a). Active inference: A process theory . Neural computation , 29(1):149.
[Friston et al., 2017b] Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., and On-
dobaka, S. (2017b). Active inference, curiosity and insigh t. Neural computation , 29(10):2633–
2683.
[Gerken et al., 2011] Gerken, L., Balcomb, F . K., and Minton, J. L. (2011). Infants avoid labouring
in vain by attending more to learnable than unlearnable ling uistic patterns. Developmental
science, 14(5):972–979.
[Gigerenzer and Goldstein, 1996] Gigerenzer, G. and Goldst ein, G. (1996). Reasoning the fast and
frugal way: models of bounded rationality . Psychological review , 103(4):650.
[Gopnik, 2012] Gopnik, A. (2012). Scientiﬁc thinking in you ng children: Theoretical advances,
empirical research, and policy implications. Science, 337(6102):1623–1627.
[Gordon et al., 2014] Gordon, G., Fonio, E., and Ahissar, E. ( 2014). Emergent exploration via
novelty management. Journal of Neuroscience , 34(38):12646–12661.
[Gottlieb et al., 2013] Gottlieb, J., Oudeyer, P .-Y ., Lopes , M., and Baranes, A. (2013).
Information-seeking, curiosity , and attention: computat ional and neural mechanisms. Trends
in cognitive sciences , 17(11):585–593.
[Harlow , 1950] Harlow , H. (1950). Learning and satiation of response in intrinsically motivated
complex puzzle performances by monkeys. J. Comp. Physiol. Psychol , 43:289–294.
[Hunt, 1965] Hunt, J. (1965). Intrinsic motivation and its r ole in psychological development. In
Nebraska Symposium on Motivation , volume 13, pages 189–282.
[Jaderberg et al., 2016] Jaderberg, M., Mnih, V ., Czarnecki , W . M., Schaul, T ., Leibo, J. Z., Silver,
D., and Kavukcuoglu, K. (2016). Reinforcement learning wit h unsupervised auxiliary tasks.
arXiv preprint arXiv:1611.05397.
[Kagan, 1972] Kagan, J. (1972). Motives and development. J. P ers. Soc. Psychol , 22:5166.
22 Pierre-Yves Oudeyer
[Kaplan and Oudeyer, 2003] Kaplan, F . and Oudeyer, P .-Y . (20 03). Motivational principles for
visual know-how development. In Prince, C., Berthouze, L., Kozima, H., Bullock, D., Stojanov ,
G., and Balkenius, C., editors, Proceedings of the 3rd international workshop on Epigeneti c
Robotics : Modeling cognitive development in robotic syste ms, no. 101 , pages 73–80. Lund
University Cognitive Studies.
[Kaplan and Oudeyer, 2007a] Kaplan, F . and Oudeyer, P .-Y . (2 007a). In search of the neural cir-
cuits of intrinsic motivation. Frontiers in neuroscience , 1:17.
[Kaplan and Oudeyer, 2007b] Kaplan, F . and Oudeyer, P .-Y . (2 007b). The progress-drive hypoth-
esis: an interpretation of early imitation. In Dautenhahn, K. and Nehaniv , C., editors, Models
and mechanisms of imitation and social learning: Behaviour al, social and communication di-
mensions, pages 361–377. Cambridge University Press.
[Kidd et al., 2012] Kidd, C., Piantadosi, S., and Aslin, R. (2 012). The goldilocks effect: Human
infants allocate attention to visual sequences that are nei ther too simple nor too complex. PLOS
ONE, 7:e36399.
[Kulkarni et al., 2016] Kulkarni, T ., Narasimhan, K., Saeed i, A., , and T enenbaum, J. (2016). Hi-
erarchical deep reinforce- ment learning: Integrating tem poral abstraction and intrinsic motiva-
tion. In Advances in Neural Information Processing Systems , page 36753683.
[Lehman and Stanley , 2008] Lehman, J. and Stanley , K. O. (200 8). Exploiting open-endedness to
solve problems through the search for novelty . In ALIFE, pages 329–336.
[Lillicrap et al., 2015] Lillicrap, T . P ., Hunt, J. J., Pritz el, A., Heess, N., Erez, T ., T assa, Y ., Sil-
ver, D., and Wierstra, D. (2015). Continuous control with de ep reinforcement learning. arXiv
preprint arXiv:1509.02971 .
[Little and Sommer, 2013] Little, D. Y . and Sommer, F . T . (201 3). Learning and exploration in
action-perception loops. Frontiers in neural circuits, 7 .
[Machado et al., 2017] Machado, M. C., Bellemare, M. G., and B owling, M. (2017). A lapla-
cian framework for option discovery in reinforcement learn ing. In International Conference on
Machine Learning (ICML) .
[Martius et al., 2013] Martius, G., Der, R., and A y , N. (2013) . Information driven self-organization
of complex robotic behaviors. PloS one , 8(5):e63400.
[Moulin-Frier et al., 2014] Moulin-Frier, C., Nguyen, S. M. , and Oudeyer, P . Y . (2014). Self-
organization of early vocal development in infants and mach ines: the role of intrinsic motivation.
Frontiers in psychology , 4:1006.
[Mouret and Doncieux, 2012] Mouret, J. B. and Doncieux, S. (2 012). Encouraging behavioral
diversity in evolutionary robotics: An empirical study . Evolutionary computation , 20(1):91–
133.
Computational Theories of Curiosity-Driven Learning 23
[Nguyen and Oudeyer, 2014] Nguyen, M. and Oudeyer, P .-Y . (20 14). Socially guided intrinsic
motivation for robot learning of motor skills. Autonomous Robots , 36(3):273–294.
[Oller, 2000] Oller, D. (2000). The emergence of the speech capacity . Lawrence Erlbaum and
Associates Inc, Mahwah, NJ.
[Ostrovski et al., 2017] Ostrovski, G., Bellemare, M. G., Oo rd, A. V . D., and Munos, R. (2017).
Count-based exploration with neural density models. In Proceedings of the International Con-
ference on Machine Learning .
[Oudeyer and Kaplan, 2007] Oudeyer, P . and Kaplan, F . (2007) . What is intrinsic motivation? a
typology of computational approaches. Frontiers in neurorobotics , 1:6.
[Oudeyer et al., 2013] Oudeyer, P .-Y ., Baranes, A., and Kapl an, F . (2013). Intrinsically motivated
learning of real-world sensorimotor skills with developme ntal constraints. In Intrinsically moti-
vated learning in natural and artiﬁcial systems , page 303365. Springer, Berlin, Heidelberg.
[Oudeyer and Kaplan, 2006] Oudeyer, P .-Y . and Kaplan, F . (20 06). Discovering communication.
Connection Science , 18(2):189–206.
[Oudeyer et al., 2007] Oudeyer, P .-Y ., Kaplan, F ., and Hafne r, V . (2007). Intrinsic motivation sys-
tems for autonomous mental development. IEEE Trans. Evol. Comput , 11(2):265–286.
[Oudeyer and Smith, 2016] Oudeyer, P .-Y . and Smith, L. (2016 ). How evolution can work through
curiosity-driven developmental process. T op. Cogn. Sci , 8(2):492–502.
[Pathak et al., 2017] Pathak, D., Agrawal, P ., Efros, A. A., a nd Darrell, T . (2017). Curiosity-driven
exploration by self-supervised prediction. In International Conference on Machine Learning
(ICML).
[P ´ er´ e et al., 2018] P ´ er´ e, A., Forestier, S., Sigaud, O., and Oudeyer, P .-Y . (2018). Unsupervised
learning of goal spaces for intrinsically motivated goal ex ploration. In International Conference
on Learning Representations (ICLR) .
[Salge et al., 2014] Salge, C., Glackin, C., and Polani, D. (2 014). Changing the environment based
on empowerment as intrinsic motivation. Entropy, 16(5):2789–2819.
[Schmidhuber, 1991] Schmidhuber, J. (1991). Curious model -building control systems. In Pro-
ceedings of the International Joint Conference on Neural Ne twork, volume 2, pages 1458–1463.
[Schulman et al., 2017] Schulman, J., W olski, F ., Dhariwal, P ., Radford, A., and Klimov , O.
(2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
[Sigaud and Stulp, 2018] Sigaud, O. and Stulp, F . (2018). Pol icy search in continuous action do-
mains: an overview . arXiv preprint arXiv:1803.04706 .
24 Pierre-Yves Oudeyer
[Singh et al., 2010] Singh, S., Lewis, R. L., Barto, A. G., and Sorg, J. (2010). Intrinsically moti-
vated reinforcement learning: An evolutionary perspectiv e. IEEE Transactions on Autonomous
Mental Development , 2(2):70–82.
[Smith, 2013] Smith, L. (2013). Its all connected: Pathways in visual object recognition and early
noun learning. American Psychologist , 68(8):618.
[Smith and Breazeal, 2007] Smith, L. and Breazeal, C. (2007) . The dynamic lift of developmental
process. Developmental Science , 10(1):61–68.
[Sutton, 1990] Sutton, R. (1990). Integrated architecture s for learning, planning, and reacting
based on approximating dynamic programming. In Proceedings of the Seventh International
Conference on Machine Learning , pages 216–224.
[Sutton and Barto, 2018] Sutton, R. S. and Barto, A. G. (2018) . Reinforcement learning: An in-
troduction, volume 1. MIT press Cambridge. Second Edition.
[Thomaz and Breazeal, 2008] Thomaz, A. L. and Breazeal, C. (2 008). Experiments in socially
guided exploration: Lessons learned in building robots tha t learn with and without human teach-
ers. Connection Science , 20(23):91–110.
[T womey and W estermann, 2017] T womey , K. and W estermann, G. (2017). Curiosity-based learn-
ing in infants: A neurocomputational approach. Developmental Science .
[Zhu et al., 2017] Zhu, Q., Triesch, J., and Shi, B. (2017). Jo int learning of binocularly driven
saccades and vergence by active efﬁcient coding. Frontiers in neurorobotics , 11:58.