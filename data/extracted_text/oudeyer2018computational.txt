8102
nuJ
81
]IA.sc[
2v64501.2081:viXra
Computational Theories of Curiosity-Driven Learning
Pierre-Yves Oudeyer∗
InriaandEnstaParisTech,France
Abstract
Whatare the functionsof curiosity? Whatare the mechanismsof curiosity-drivenlearn-
ing? We approach these questions about the living using concepts and tools from machine
learninganddevelopmentalrobotics. We arguethatcuriosity-drivenlearningenablesorgan-
ismstomakediscoveriestosolvecomplexproblemswithrareordeceptiverewards.Byfoster-
ingexplorationanddiscoveryofadiversityofbehaviouralskills,andignoringtheserewards,
curiositycanbeefficienttobootstraplearningwhenthereisnoinformation,ordeceptivein-
formation, about local improvement towards these problems. We also explain the key role
of curiosity for efficient learning of world models. We review both normative and heuristic
computationalframeworksused to understand the mechanisms of curiosity in humans, con-
ceptualizing the child as a sense-making organism. These frameworks enable us to discuss
thebi-directionalcausallinksbetweencuriosityandlearning,andtoprovidenewhypotheses
about the fundamental role of curiosity in self-organizing developmental structures through
curriculumlearning. Wepresentvariousdevelopmentalroboticsexperimentsthatstudythese
mechanismsinaction,bothsupportingthesehypothesestounderstandbettercuriosityinhu-
mansandopeningnewresearchavenuesinmachinelearningandartificialintelligence.Finally,
wediscusschallengesforthedesignofexperimentalparadigmsforstudyingcuriosityinpsy-
chologyandcognitiveneuroscience.
Keywords: Curiosity, intrinsic motivation, world models, rewards, free-energy principle,
learning progress hypothesis, lifelong learning, predictions, machine learning, AI, develop-
mentalrobotics,development,curriculumlearning,self-organization.
1. Introduction
Humans and many other animals spontaneously explore their environments. This often happens
without a pressure for finding extrinsic rewards like food, and without external incentives from
their social peers. Such spontaneous exploration seems to be produced by an internal mechanism
pushing them tomakesense oftheir world: they explore fortheintrinsic purpose ofgetting better
atpredicting andcontrolling theworld. Thisspontaneous investigation oftheenvironment, andof
thelinkbetweenone’sownphysicalandcognitivecapabilitiesandtheenvironment, cantakemany
different forms. This ranges from babies trying various ways to locomote, or exploring grasping,
∗http://www.pyoudeyer.com
2 Pierre-YvesOudeyer
manipulating, banging or throwing of all sorts of objects, or testing how social peers respond to
vocalizations, to children practicing tool building with wooden sticks, or throwing wooden sticks
in rivers to see how they will flow, to adults searching information about a hobby, learning a new
sport,orascientist turninghistelescope towardsfarawaygalaxies.
Alltheseexploratorybehaviourscanbeseenasquestionsposedbytheorganismaboutitsenvi-
ronmentorabouttherelationbetweenitsenvironment andit’sowncurrentstateofknowledgeand
skills. Thesequestionscanbeformulatedinvariouswaysrangingfromactualphysical/behavioural
experimentation toformulating alinguistic question.
These mechanisms have been discussed from various perspectives in the scientific litera-
ture, and in particular using the related concepts of curiosity [Berlyne, 1960], intrinsic moti-
vation [Harlow,1950], and free play [Bruneretal.,1976]. Across many different fields, theo-
rists have suggested that interest is engaged by what is just beyond current knowledge, neither
too well known nor too far beyond what is understandable. This idea has been offered many
times in psychology, through concepts like cognitive dissonance [Kagan,1972], optimal incon-
gruity[Hunt,1965],intermediatenovelty[Berlyne,1960,Kiddetal.,2012]andoptimalchallenge
[Csikszenthmihalyi, 1991],andrecentresearchinneuroscience isnowinvestigating theassociated
neuralmechanisms[Gottliebetal.,2013].
Several formal frameworks have recently enabled to improve our theoretical understanding
of these mechanisms. This includes frameworks considering the curiosity system as a machine
which goal is to build predictive and control models of the world [KaplanandOudeyer,2007a,
OudeyerandSmith,2016] - and sometimes the brain as a whole is conceptualized like this as in
thefreeenergyprinciple[Fristonetal.,2017a,Fristonetal.,2017b]. Relatedtothis,reinforcement
learning and optimization frameworks consider curiosity as a mechanism which allows to gener-
ate diversity in exploration, enabling to get out of local minima in searching for behaviours that
maximizeextrinsicrewardsorfitness[Schmidhuber, 1991,Barto,2013,BaldassarreandM.,2013,
LehmanandStanley,2008,Bellemareetal.,2016,Forestieretal.,2017,Colasetal.,2018].
2. Curiosity for Exploration and Discovery in an Open World
An apparent evolutionary mystery is that such spontaneous investigations of the environment are
often very costly intimeand energy, and donot appear atfirstsight to provide adirect benefit for
feeding, mating, survival andreproduction. Sohowcould suchmechanisms evolve? Whatistheir
function?
In an uncertain world with rare resources, one could expect that organisms spare their energy
to explore only parts of the environment that are likely to provide information about where to
get these resources. However, the real world is not only uncertain, but from the point of view of
manybasicphysiological needslikefindingfood,itisfullofmultimodalmultidimensional stimuli
that are not obviously relevant to these needs. Animals have initially little ideas of what kinds
of actions are required to fulfil them. Thus, when extrinsic rewards (resources) are hard to find,
the main challenge is not to estimate the relative reward probabilities associated to a few reward-
Computational TheoriesofCuriosity-Driven Learning 3
relevant options in order to maximize the efficiency of a known solution. Rather, the challenge
is to discover the first few bits of rewards and how to build coarse strategies to get them. In this
context wherediscovery ofnewstrategies andnewoutcomes becomes themainissue(asopposed
to refining a known strategy for getting a known outcome), one can better see how to make sense
ofcuriosity-driven exploration inlivingorganisms.
Let’s take the example of an 8-9 months old baby, sitting on the ground and alone in a room.
Hehas seen abox ofsweets ontopofakitchen’s furniture, and aimstogetthem. While thebaby
might really be motivated to get the sweets for a moment, this task is for him a real conundrum.
Thesituation isfullofmultiplekindsofdeepuncertainties, andmostimportantly deepunknowns.
Firstofall,thechildhasonlyaveryapproximate ideaofthecurrentstateoftheworld: heseesthe
sweetboxfromfarawaywithhiseyes,andestimatingsimplethingssuchasdistanceandheightis
already verydifficult, since heismostlyusedtointeract withobjects thatarealready inhishands,
andhaslimitedskillstoestimatethestateoffarawayobjects. Second, thechildhasnoclueabout
howtogettothesweetbox,andhasnocluewheretolooktofindinformationaboutasolution. He
does notevenknowhowtostand uponitstwofeet, andhiscrawling strategy isveryimprecise to
movearound. Hedoesnotknowyetwhatatoolis,andhisbraincannotevenimagineatthispoint
thepossibility topushachairnexttothefurniture, thentryclimbittogettothebox(atthispoint,
chairs areperceived asobstacles forhim). Here,heismuchbeyond uncertainty: itismeaningless
for him to compute probabilities oruncertainties associated tothe success of this strategy (and its
associated sub-goals), as they involve events that are not already part of the space of hypotheses
hecanconsider. Justthinkoftheintermediate skillofstanding uponitstwofeetandclimbingthe
chair. Eveniftargetingtheseskillscanbesuggestedbyobservingitssocialpeers,theyinvolvesuch
complex internal muscular synergies that initially the child has also little cue about what patterns
ofmuscleactivations, andwhatproprioceptive andexternalvisualinformation toattend tocontrol
these skills. Also very little can be inferred from observing others, as high-dimensional muscular
activations andcovertattention intheseskillsarenotdirectlyobservable.
Sowhatcouldthechilddo? Hemightconsider aproxyinternalmeasureforguidingitssearch
forthesweetbox,suchasthecurrentdistance betweenhisbodyandthesweets. Yet,optimizingit
withitscurrentskillswouldjustmakeitcrawltothebottomofthefurnitureandextenditsarmfor
a hopeless far reach. In that case this proxy measure would be less sparse than the sweet reward
itself,butitwouldbehighlydeceptive,andequallyinoperant. Rather,agoodstrategyforthechild
shall be to simply forget about this target sweet box for a while, go back to his playground and
explore spontaneously, driven by curiosity, various forms of skills ranging from body locomotion
to object manipulation and tool use. Playing around with its own body shall then lead him to
discover both the concept of and strategies for climbing up furniture, as well as the concept and
strategiesforusingobjectsofallkindsastools. Then,whencomingbackafewmonthslatertothe
task of getting to the sweet, and once he will have acquired a skill repertoire including walking,
pushing chairs around, and using them as tools to get upwards, the solution might become much
more accessible. At this point only the problem will be looking like a classical reinforcement
learning probleminthelabwithfewobviousrelevantoptionstochoosefrom.
Research in developmental robotics and artificial intelligence has confirmed this intuition in
4 Pierre-YvesOudeyer
the last decade through computational and robotics experiments. Several strands of works have
considered the problem of how a machine could solve difficult problems with rare or deceptive
rewards [Bartoetal.,2004, Schmidhuber, 1991, LehmanandStanley,2008], sometimes directly
aimingatmodellinghowhumanchildrenexploreandlearninthesecontexts[Oudeyeretal.,2007,
OudeyerandSmith,2016]. They found that various forms of curiosity-driven exploration can in-
deedbethekeytomakediscoveries andsolvethesecomplexreinforcement learningproblems.
An example is the artificial intelligence and robotics experiment presented in
[Forestieretal.,2017] (see Figure 1), where a high-dimensional humanoid ’baby’ robot is
situated in an environment with many objects. Some of them are more or less difficult to learn
to control, and some other are uncontrollable by the robot, such as another robot moving around.
However, the ’baby’ robot does not know initially which objects itcan learn tocontrol and which
ones it cannot. Among these objects, a ball is placed in an area beyond the direct reach of the
robot. Yet, other objects can be used as tools to enable the robot to move the ball. The robot can
use its hand to push a joystick, which in turn moves a tele-operated crane toy, and this crane toy
canpushtheball. However,therobothasnoconcept oftoolinitially anddoesnotknowthatthese
objects can physically interact: ithas no pre-coded wayto represent such physical interactions. It
can send low-level motor commands into the motors of its arm. It can perceive object positions
and movements individually. Yet, it does not know initially how specific sequences of motor
commandsrelatetopotential movementsofeachobject(andhowthemovementsofobjectsmight
relate to each other). While this robotic situation is simplified as compared to most real world
situations encountered byhumaninfants, itisalready extremelycomplex. Indeed, thesequence of
motorcommandsforasimplearmmovementneeded toreachforanobject amountstospecifying
several dozen continuous numbers, while the perception of the movement of each single object
during one second is also encoded by several dozens continuous numbers. As a whole, the
sensorimotor spacethattherobotexploreshasseveralhundred dimensions.
Given such an environment, let us imagine an engineer imposing the following external ob-
jective to the robot: it should learn to move the ball forward at a maximal speed. To define this
objective, the engineer can design an extrinsic reward signal: each time the robot tries asequence
of motor commands that produce a movement of the ball, this reward is a scalar number propor-
tionaltothespeedandtargetdirection oftheball. Eachtimethemotorcommandsdonotproduce
anyballmovement,therewardiszero.
The standard machine learning approach to enable the robot to solve this problem is rein-
forcementlearning (RL)[SuttonandBarto,2018]. Thiscanbeviewedasafamilyofoptimization
algorithms that learn optimal controllers, i.e. learn to produce sequences of motor commands
that maximize the reward. The way standard approaches to RL work is through a combination of
hill-climbing (gradient descent) and random exploration. For example, state-of-the-art deep rein-
forcement learning algorithms for learning continuous control, such as DDPG and related algo-
rithms [Lillicrapetal.,2015, Schulmanetal.,2017, SigaudandStulp,2018], work by alternating
betweenupdating thecurrent controller solution inordertoclimbthehillofrewards(thisrequires
thatrewardsofdifferent magnitudes areobserved whenslightly changing thecontroller), andpro-
ducing random perturbations of the current best controller to obtain further information about the
Computational TheoriesofCuriosity-Driven Learning 5
Figure 1. Curiosity-driven exploration through autonomous goal setting and self-organized
curriculum learning in the experimental setup presented in [Forestieretal.,2017] (see video:
https://www.youtube.com/watch?v=NOLAwD4ZTW0). Here a humanoid robot is sur-
rounded by objects. The robot can learn to control some of them, while others are unlearnable
distractors. Some objects can interact with each other, e.g. be used as tool to control other ob-
jects. The robot initially does not know which skills and objects are learnable, nor that objects
may interact with each others. If an engineer would like the robot to learn how to move the ball
forward byproviding rewardsonly whentheballmoves, thenusing classical reinforcement learn-
ing approaches would fail. Indeed, RL approaches combine hill-climing mechanisms that require
observations of non-zero reward to improve the current solution, and rely on random exploration
otherwise. Here, the time required by random exploration to enable the robot to find such a rare
rewardwouldbeprohibitively long. Therobotinitially hasnoideawheretolookforcuestosolve
this task. Indeed, moving the ball entails moving the white electric crane toy, which can only
be moved by pushing one of the two joysticks, which can itself only be moved by appropriate
movements of the hand, requiring specific sequences of motor commands in the joints. A more
efficient approach is to use curiosity-driven exploration, where one lets the robot self-generate its
own goals for controlling various objects, and spend time on the ones which produce maximal
learning progress. The internal generation of goals, and the focus on goals providing maximal
learningprogress,modelaformofcuriosity. Doingso,therobotwillfirstfocusonplayingwithits
hand,whichisinitiallyprovidingmaximallearningprogress. Then,itwilldiscoverasasideeffect
how to movethe joysticks. In turn, due to the physical couplings in the environment, focusing on
learning about the joysticks makes the robot discover how to move the crane toy and the ball in a
fewhours.
6 Pierre-YvesOudeyer
rewarddistribution.
However, such an RL approach will not work in the robotic environment considered here.
Indeed, inthisparticular environment, theproblem ofmovingtheballforward issaidtobea’rare
reward’ problem. Indeed, due to the structure of the environment, the vast majority of possible
sequencesofmotorcommandswillproducearewardofzero. Actually,mostrandomsequencesof
motorcommandswillmakethattherobotwillnoteventouchthejoystick infrontofhim. Thus,if
therobottriesactionsrandomly,thereisaverylowprobabilitytogetanon-zeroreward,i.e. thatits
arms touches the joystick ina peculiar way that makes the crane toy in apeculiar waythat makes
theballmove. Thisisaproblemasthehill-climbingmechanismsofRLalgorithmsneedtoobserve
distributions of non-zero reward to update the controller, and doing random exploration of motor
commands would here take a prohibitively long time before the first non-zero rewards are found,
i.e. before finding the first few action sequences that produce ball movement. This problem of
RLapproachesthatfocusonhill-climingoftheextrinsicrewardisnowwell-known,andappliesto
manenvironmentswithrareordeceptiverewards1[Bellemareetal.,2016,SigaudandStulp,2018,
Colasetal.,2018].
Analternativecomputationalapproachstudiedin[Forestieretal.,2017]hasconsistedinequip-
ping the robot with the capability to self-generate and explore many other goals than moving the
ball. For example, the system can self-generate and focus on goals such as moving the hand in
various directions, pushing anyof thejoysticks along particular trajectories, ortrying tomovethe
crane toy in diverse ways. The idea is that the system decides to spend time learning about one
of these goals based on an intrinsic measure of the learning progress towards solving them, and
ignoring completely howmuchtheyprovide information aboutthegoaloftheengineer (movethe
ball). Formally, self-generating a goal (e.g. move toy in direction d) amounts to self-defining an
internal rewardfunction whichquantifies howwellsequence ofmotorcommands produce thetar-
geted outcome (e.g. the reward can be a scalar value proportional to the difference between the
reached toy direction and the target toy direction). Then, the system can use RL algorithms to
learn controllers for these internally defined goals and rewards. This whole process models sev-
eral aspects of curiosity-driven exploration. First, it includes internal spontaneous generation of
goals. Second, it includes a meta-cognitive mechanism to assess the relative ’interestingness’ of
self-generated goals,basedonquantifying expectedlearningprogress. Thisisusedbytherobotto
decide on which goal to focus on at any given moment in time, and to self-organize a curriculum
oflearning.
Intuitively this mayappear tobeanevenmoredifficult situation forthe robot, asitconsists in
providing itwithmanyotherpotentially difficultproblems inaddition totheinitial problem ofthe
engineer. However, this turned out to facilitate the acquisition of skills to move the ball forward.
Indeed, due to the structure and richness of the physical environment, the robot manages to find
1Deceptiverewardsarelocalimprovementofrewardsthatpushthelearnertoupdatethecontrollerinwrongdirec-
tionsinrelationwiththeglobaloptimum. Forexample,intheinfantexampleabove,thiswouldbethechildobserving
thatelevatingthehandtowardsthesweetboxonthetabledecreasesthedistancebetweenhimandthesweetbox. Ifthe
childisconsideringthehand-sweetdistanceasameasureofreward,thiswouldbereinforcingthisstrategy. However,
thiswouldgetthechildawayfrommoreefficientstrategiesonthelongtermsuchasfetchingachairandclimbingit.
Computational TheoriesofCuriosity-Driven Learning 7
continuously goals where learning can happen efficiently due to dense reward feedback, leading
to an increase in behavioural diversity, and then to an increase of probability to find sequences of
motor commands that solve other goals with rarer rewards. In this particular case, this curiosity-
driven exploration mechanism pushed the robot to first focus on moving its hand around, leading
to a greater diversity of motor behaviours. As a side effect, this enabled the discovery of how to
move the joystick 2. Then in turn, moving the joystick became interesting as a source of learning
progress. This increased the focus on practicing goals related to the joysticks, which made the
robot discover thatthe cranetoy canbemovedaround, leading quickly tothediscovery ofhowto
move the ball. As the first ball movements are discovered, choosing the goal of moving the ball
becomeseasier, asthehill-climbing mechanism ofreinforcement learning canefficiently compute
how tochange thecontroller toimprove ball movements. Following this process, curiosity-driven
exploration enables the robot to learn in a few hours how to solve the task of moving the ball
around, even without initially pointing this task as particularly relevant among many other tasks
that are also learnt. Onthe contrary, this would have been prohibitively long and difficult tolearn
byonlyconsidering andfocusing onthistaskexternally definedbytheengineer.
Thisexample reliesoncomputational modelsofcuriosity-driven exploration thatwereexplic-
itly motivated by modeling mechanisms of human spontaneous exploration and their role in ex-
plaining the discovery of tool use [ForestierandOudeyer,2016b, ForestierandOudeyer, 2016a],
vocalization[Moulin-Frier etal.,2014]andlanguage[Forestieretal.,2017]bychildren. However,
several other strands of research in artificial intelligence have converged to the same conclusion
that curiosity-driven exploration could be a key to solve complex tasks with rare or deceptive re-
wards. Forexample,severalworksinthefieldofevolutionarycomputationhaveshownthatnovelty
search could be essential in solving difficult optimization problems [LehmanandStanley,2008],
sometimes in combination with the task-specific reward [MouretandDoncieux, 2012], but also
sometimes by completely forgetting this task specific reward in the novelty search process
[LehmanandStanley,2008]. In the domain of reinforcement learning, the idea of exploration
bonuses [AndreaeandAndreae,1978, Sutton,1990, DayanandSejnowski,1996] has also been
shown to increase the efficiency of reinforcement learning algorithms, with the addition of
a reward component measuring quantities such as the novelty of a state [Sutton, 1990], pre-
diction progress [Schmidhuber, 1991, KaplanandOudeyer,2003], density of state exploration
[Ostrovskietal.,2017], predictive information [Martiusetal.,2013], predictive information gain
[LittleandSommer,2013], or empowerment [Salgeetal.,2014]. Within an intrinsically mo-
tivated multi-goal reinforcement learning perspective, measures of competence progress to-
wards self-generated goals were used to automatically generate learning curriculum for acquir-
ingcomplexskills[BaranesandOudeyer,2013,Oudeyeretal.,2013,NguyenandOudeyer,2014,
2When the robot targets a goal, but observes effects relevant to another goal as a side effect, it is able to learn
retrospectivelyaboutthisothergoalusingthecollectedobservation. Forexample,ifittargetstomovethehandonthe
right,butinpracticemovesitontheleftandmovesthejoystick,itusesthisobservationtolearnhowtomovetheball
leftandhowtomovethejoystick. Thisisacentralpropertyoftheseintrinsicallymotivatedgoalexplorationprocesses
[BaranesandOudeyer,2013,Forestieretal.,2017],whichissharedwithotherrelatedmulti-goallearningalgorithmsin
RLsuchasHindsightExperienceReplay[Andrychowiczetal.,2017].
8 Pierre-YvesOudeyer
Forestieretal.,2017]. Within a hierarchical reinforcement learning perspective, intrinsically
motivated RL approaches have also been used as a framework enabling the discovery of hi-
erarchically reusable skills for boosting exploration [Bartoetal.,2004, Kulkarnietal.,2016,
Machadoetal.,2017]. Recent breakthrough in deep reinforcement learning have leveraged these
variousstrandsofworks. Forexample,severaldeepreinforcementlearningalgorithmswereshown
to be able to solve difficult problems with rare rewards (e.g. video games), sometimes by even
ignoring thescore measure[Pathaketal.,2017],eitherbyintroducing anintrinsic rewardmeasur-
ing forms of novelty or learning progress [Ostrovskietal.,2017, Kulkarnietal.,2016], or by in-
troducing auxiliary tasks [Jaderberg etal.,2016, Andrychowicz etal.,2017, Florensaetal.,2017,
Cabietal.,2017]inwaysthatarerelated tointrinsically motivatedgoalexploration approaches in
developmental robotics [BaranesandOudeyer,2013,Forestieretal.,2017,Pe´re´ etal.,2018].
3. The Child as a Sense-Making Organism
Thiscomputational perspectiveprovidesaconceptualframeworktounderstand howvariousforms
ofcuriosity-driven exploration canbeinstrumental foranorganism todiscoversolutions toextrin-
sictasksthatareessentialtoitssurvival,suchasfindingextrinsicrewardslikefood. Inthiscontext,
it becomes possible to make sense of curiosity-driven exploration in an evolutionary perspective
[Smith,2013],andindeed further computational modelshave shownthatintrinsic rewardsystems
could be the result of evolutionary processes optimizing for an individual fitness to reproduce in
changing environments [Singhetal.,2010]. This perspective converges with the hypothesis pro-
posed in psychology that humans might be equipped with dedicated motivational neural circuitry
pushing them to explore the worldfor thepure sake ofmaking sense ofit, andindependently (yet
complementarily) from the search of extrinsic rewards [Berlyne,1960]. For example, Gopnick
[Gopnik,2012] has suggested that the child could be viewed as a curiosity-driven little scientist
equipped with an intrinsic urge to discover the underlying causal structure of the world (see also
[ChaterandLoewenstein, 2016]).
Several theoretical models have considered mechanisms of curiosity-driven exploration as or-
ganized exploration of the world with the purpose to build good predictive and control models
of the world dynamics [Oudeyeretal.,2007, Fristonetal.,2017a]. Forexample, Friston and col-
leagues [Fristonetal.,2017a] have developed a normative theoretical framework, termed the free
energy principle, to characterize the theoretical properties of an ideal optimal Bayesian learner
trying to build a good predictive model of the world. More precisely, the free energy principle
frameworkviewsthebrainasanactiveBayesianinferencesystemthataimstominimizefutureex-
pectedsurprise. Ausefulproperty ofthisframeworkistomathematically formalizevarious forms
of uncertainties which are necessarily encountered by the active inference system, and which re-
duction can lead to corresponding various forms of exploration akin to curiosity (which are also
present in the learning progress framework detailed below). For example, sources of uncertainty
thatappearwhenmathematically decomposing theexpectedfreeenergy includeuncertainty about
hidden states given an action policy: minimizing it leads to active sensing and perceptual infer-
Computational TheoriesofCuriosity-Driven Learning 9
ence,i.e. aformofcuriosityaimingtoimprovethesubjectiveestimationofthecurrentworldstate.
Anotherdimensionisuncertaintyaboutpoliciesintermsofexpectedfuturestatesormodelparam-
eters: this leads to forms of epistemic exploration and learning, i.e. a form of curiosity aiming to
improve the predictive model of what will be observed in the future depending on one’s own ac-
tions. Yetanother dimension isuncertainty about themodel structure itself: this leads tostructure
learning and insight, a form of curiosity aiming to find new abstractions with structures that en-
able better predictions ofwhatishappening intheenvironment. Whiletheconcept ofgoals isnot
directly covered by the free-energy principle, other frameworks like the autonomous goal setting
paradigm [OudeyerandKaplan,2007,Forestieretal.,2017],presented intherobotic experiments
above, point to other forms of uncertainty and curiosity centered around goals. Indeed, another
form of uncertainty is related to the self-evaluation of goal competences: minimizing these forms
of uncertainty lead to curiosity-driven self-generation and practice of goals to learn about one’s
owncompetence toachievethem.
4. Normative or Heuristic Accounts? The Learning Progress Hy-
pothesis
As the landscape of computational/mathematical theories of curiosity-driven exploration quickly
develops[Baldassarre andM.,2013,OudeyerandSmith,2016],onebecomesbetterequippedwith
formal and experimental tools to conceptualize better what curiosity might be, and what it could
be used for in humans. However, this landscape of theories also raises multiple open questions to
explain human curiosity. A first fundamental question is how to articulate normative approaches
(e.g. thefree-energy principle, empowerment) withheuristics-based approaches (e.g. thelearning
progress hypothesis [KaplanandOudeyer, 2007a, OudeyerandSmith,2016]) to account for hu-
man behaviour and brain mechanisms. Taking the perspective of David Marr’s levels of analysis
forunderstanding cognition andthebrain,shallnormativeapproaches bestanding atthecomputa-
tionallevel(describingwhicharetheproblemsandthequantitiesthatthebrainactuallytarget)and
heuristicapproaches atthealgorithmiclevel(describing whichalgorithmscanactuallysolvethese
problems) on top of the implementation level investigating how neural circuitry could implement
thesealgorithms? Theanswertothisepistemological question doesnotappeartoberesolved yet.
Indeed, whilenormativeapproaches likethefree-energy principlehavebeenusedsuccessfully
to account for several behavioural and neural observations ranging from saccadic eye movements
to habit learning to place cell activity [Fristonetal.,2017a, Fristonetal.,2017b], it relies on as-
sumptions that are still speculative about what could be happening in the brain. First, it relies on
theassumption thatthehumanbrainiscapable toachieve(approximate) hierarchical Bayesianin-
ference,butseveralstrandsofworkhaveshownanumberofsituationswherethebrainusesheuris-
ticsthatarefarawayfrom aBayesianbehaviour (e.g;[Gigerenzer andGoldstein, 1996]). Second,
suchanormativeBayesianframeworkrequiresthatmodelledhumansubjectsinitiallyknowthefull
spaceofpossible hypotheses aboutpossibleworldstates,possible policies, possiblemodelparam-
eters, and possible model structures. This deviates strongly from the deep unknown encountered
10 Pierre-YvesOudeyer
by children as described in the example above: new hypotheses can be formed out of interaction
with the world and unsupervised representation learning. Finally, active structured Bayesian in-
ferenceiscomputationallyverycostly[Buckleyetal.,2017],andquicklybecomecomputationally
intractable forproblemsofmoderatesize.
The challenges to address efficient and tractable curiosity-driven exploration in real world
situations also underly the development of heuristic theories of curiosity-driven learning
[Oudeyeretal.,2013]. Within the perspective of the ’child as a sense making organism’, these
heuristictheorieshaveconsideredwhatmechanismscouldenableefficientexplorationandlearning
inhigh-dimensional physicalbodies,andunderlimitedresourcesoftime,energyandcognitiveca-
pabilities. Onesuchheuristic-orientedtheoreticalframeworkisthe’learningprogress(LP)hypoth-
esis’, proposed in [OudeyerandKaplan,2006, KaplanandOudeyer,2007a, Oudeyeretal.,2007,
OudeyerandKaplan,2007]. Here, curiosity-driven exploration in living organisms is viewed as
driven by the heuristic estimation and search of various forms of expected learning progress3.
More precisely, this includes aheuristic meta-cognitive mechanism that estimates expected learn-
ing progress associated to competing activities (stimuli to observe, situations or self-generated
goals to engage in). This estimation of LP is then used to choose which activities to explore,
selecting inpriority activities thatmaximizeexpectedlearning progress.
Thefundamentalsimilaritybetweenthefree-energyprincipleandtheLPhypothesisisthatboth
frameworksconsidercuriosity-driven explorationasaprocessthataimstocollectnewinformation
tomaximizethequalityofaninternalworldmodel,andwherethisworldmodelincludesamodelof
self-knowledgeandself-competences. Anothersimilarityisthatbothframeworksconsidervarious
forms of learning progress, as the organism can learn various forms of knowledge and skills at
various scales of space and time. Some forms of learning progress can result from an attentional
action on a short time scale, providing information gain to better estimate the current world state.
Butit can be also result from the choice to focus on an activity for a longer timescale, producing
various forms of improvement of a predictive world model, ranging from reduction in empirical
prediction errors to reduction of uncertainty about these predictions (uncertainty could improve
withoutimprovingtheaveragepredictionerror,andstillthiswouldbeaformoflearningprogress).
This latter form of learning progress (longer time scale of learning, improvement of predictive
world model) has been the focused of mostcomputational experiment ofthe LPhypothesis so far
[OudeyerandSmith,2016].
There are also fundamental differences between the free-energy principle and the LP frame-
work. In the free-energy framework, the mechanisms used to represent and update world models
andtheirassociated uncertainties arebasedonBayesianinference. Onthecontrary, theLPframe-
3Variousworksinartificialintelligence,machinelearningandoptimalexperimentdesignhavestudied,inthelast50
years,howmechanismsofactivelearningcanpushamachinetoexplorepartsofthestate-actionspacethatmaximize
formsofinformationgainandlearningprogress.However,theselinesofworkdidnotproposeandstudythehypothesis
thatrelatedmechanismscouldexplainaspectsofhumancuriosity-drivenlearning,andinotheranimals. Comingfrom
thismodelingperspective,theLPhypothesiswasdevelopedindependantlyoftheselinesofworkinmachinelearning
andAI.Theconvergenceofthesevariousstrandsofworktowardsrelatedalgorithmssupportsthestrengthandscopeof
thesemechanisms.
Computational TheoriesofCuriosity-Driven Learning 11
work has considered heuristic algorithms to learn world models from observations, and to esti-
mate uncertainty and learning progress, using mechanisms such as memory-based and lazy learn-
ingtechniques [Forestieretal.,2017],aswellasneuralnetworks[KaplanandOudeyer,2003]and
evolution strategies [Benureau andOudeyer,2016]. Oneparticular aspect ofthisdifference isthat
heuristic-based approaches do not require that the learner knows all possible events, event rep-
resentations, and model representations as it discovers the world (on the contrary, the normative
framework requires priors about these events and their representations, which is untractable for
realworldsituations). Unsupervised neuralnetworkrepresentation learningtechniques canforex-
ample be used to learn new spaces of representations in which to make predictions and set goals,
as the world is being discovered (e.g. [Pe´re´ etal.,2018]). Unsupervised learning techniques are
also used in the LP framework to learn incrementally abstractions of the low-level sensorimotor
flow,enabling forexampletoformdistinct concepts oftheself, ofinanimateobjects andofothers
based ontheirassociated learnability properties [Oudeyeretal.,2007]. Finally, another difference
already mentionned earlier, is that the LP framework has been extended to cover mechanisms of
autonomous goalsetting, whichisakeydimension ofcuriosity-driven exploration inhumans.
As the LP framework has studied what heuristics can drive efficient learning of world mod-
els in the real world, evaluation has leveraged robotics experiments under constraints of time
and processing, showing how these mechanisms enable learning multiple forms of locomotion
[BaranesandOudeyer,2013], manipulation of flexible objects [NguyenandOudeyer,2014], and
tool use discovery [Forestieretal.,2017] in high-dimensional continuous action and perceptual
spaces.
Beyond its theoretical origin, the learning progress hypothesis makes behavioural predictions
that are compatible with a growing set of experimental evidences in psychology. For example,
Gerkenetal. [Gerkenetal.,2011]showedthat17-monthsoldchildrenattendmoretolearnablear-
tificiallinguisticpatternsthantounlearnableones. Also,Kiddetal. [Kiddetal.,2012]showedthat
infants prefer sequences of perceptual stimuli that have an intermediate level of complexity. Sim-
ilarly, Begus et al. [Begusetal.,2016] showed that infants selectively ask the help of informants
basedonexpected information theycanprovide. Baranesetal. [Baranesetal.,2014]showedhow
adult subjects, who were free to explore and select tasks of various difficulties, spontaneously or-
ganizetheirexplorationingrowingorderofdifficultyandsettleonlevelsofintermediatedifficulty
justbeyondtheircurrent skilllevel.
5. Curiosity and Self-Organization in Development
These computational studies of the learning progress hypothesis have also uncovered a crucial
emergent property of such a heuristic mechanism: it spontaneously leads the learner to avoid sit-
uations which are either trivial or too complicated4, and focus on situations that are just beyond
4Otherheuristics likenoveltyor surprise bonuses proposed intheRLlitteraturedonot scale aswellinopen real
worldenvironmentsastherearemanytasksorsituationswhichproducenoveltyorsurprisebutyetshouldbeavoidedas
theyarenotlearnable.Anactivitycanbeunlearnableduetounobservablecausalfactors,ortointrinsicunpredictability.
12 Pierre-YvesOudeyer
the current skills in prediction or control, exploring them as long as learning progress happens in
practice.
This has enabled to generate the new hypothesis that mechanisms of curiosity drive the
emergence ofordered developmental trajectories atlong timescales [KaplanandOudeyer,2007a,
OudeyerandSmith,2016]. Several studies have shown that such trajectories match sev-
eral fundamental properties of infant trajectories in domains such as vocal development
[Moulin-Frier etal.,2014], imitation [KaplanandOudeyer, 2007b] and tool use discovery
[ForestierandOudeyer,2016a,ForestierandOudeyer,2016c]. Relatedmodelsofcuriosity-driven
learning, with intrinsic rewards based on information gain measured through empirical pre-
diction errors, have also shown how it could model the formation of pro-social behaviors
[Baragliaetal.,2016], or model the development of aspects of binocular vision [Zhuetal.,2017]
or visual search in infants [TwomeyandWestermann, 2017], as well as different forms of ex-
ploratory behaviours inotheranimals[Gordonetal.,2014].
5.1. The PlaygroundExperiment
An example of self-organization of structured developmental trajectories driven by
curiosity-driven exploration is the Playground Experiment 5 [OudeyerandKaplan,2006,
KaplanandOudeyer, 2007a, Oudeyeretal.,2007]. It illustrates how mechanisms of curiosity-
driven exploration, dynamically interacting with learning, physical and social constraints, can
self-organize developmental trajectories. Inparticular, thisleadsalearnertosuccessively discover
twoimportantfunctionalities: objectaffordances andvocalinteraction withitspeers.
In these Playground Experiments, a quadruped learning robot (the learner) is placed on an
infant play mat with a set of nearby objects and is joined by an adult robot (the teacher), see
Figure 2 (A) [OudeyerandKaplan,2006, KaplanandOudeyer, 2007a, Oudeyeretal.,2007]. On
thematandnearthelearner areobjectsfordiscovery: anelephant (whichcanbebitten orgrasped
by the mouth), a hanging toy (which can be bashed or pushed with the leg). The teacher is pre-
programmed to imitate (with a different pitch of voice) the sounds made by the learner when the
learning robotlookstotheteacherwhilevocalizing atthesametime.
Thelearner isequipped witharepertoire ofmotorprimitives parameterized byseveralcontin-
uous numbers that control movements of its legs, head and a simulated vocal tract. Each motor
primitive is a dynamical system controlling various forms of actions: (a) turning the head in dif-
ferent directions; (b) opening and closing the mouth while crouching with varying strength and
timing; (c) rocking the leg with varying angle and speed; (d) vocalizing with varying pitch and
length. These primitives can be combined to form a large continuous space of possible actions.
Similarly, sensory primitives allow the robot to detect visual movements, salient visual proper-
ties, proprioceptive touch in the mouth, and pitch and length of perceived sounds. For the robot,
these motor and sensory primitives are initially black boxes and he has no knowledge about their
semantics, effectsorrelations.
5ThetextdescribingthePlaygroundExperimentinthissection,andtheinteractionofcuriositywithsocialguidance
below,ispartlyadapatedfrom[OudeyerandSmith,2016].
Computational TheoriesofCuriosity-Driven Learning 13
Thelearningrobotlearnshowtouseandtunetheseprimitivestoproducediverseeffectsonits
surrounding environment, andexploration isdriven by themaximization oflearning progress: the
robot chooses to perform physical experiences (experiments) that improve maximally the quality
ofpredictions oftheconsequences ofitsactions, i.e. thatimproveitsworldmodel.
Figure 2 (B) outlines the computational architecture of curiosity-driven learning, called IAC,
used in the playground experiment [Oudeyeretal.,2007]. A prediction machine (M) learns to
predict the consequences of actions taken by the robot in given sensory contexts. For example,
this module might learn to predict (with a neural network) which visual movements or propri-
oceptive perceptions result from using a leg motor primitive with certain parameters. A meta-
cognitive module estimates the evolution of errors in prediction of M in various regions of the
sensorimotor space. More precisely, this module estimates the decrease in prediction errors for
particular kinds of actions or particular contexts. An example of such a context could be the
prediction of the consequences of a leg movement when this action is applied towards a partic-
ular area of the environment. These estimates of error reduction, measuring a form of learning
progress, are used to compute an intrinsic reward. This reward is an internal quantity (a num-
ber) that is proportional to the decrease of prediction errors, and the maximization of this quan-
tity is the objective of action selection within a computational reinforcement learning architec-
ture [KaplanandOudeyer,2003, Oudeyeretal.,2007, OudeyerandKaplan,2007]. Importantly,
the action selection system chooses most often to explore activities where the expected intrinsic
reward is high, i.e. where the expected learning progress is high. However, this choice is prob-
abilistic, which leaves the system open to learning in new areas and open to discovering other
activities that may also yield progress in learning6. Since the sensorimotor flow does not come
pre-segmented intoactivitiesandtasks,asystemthatseekstomaximizedifferences inlearnability
is also used to progressively categorize the sensorimotor space into differentiated contexts. This
categorization thereby models the incremental creation and refining ofcognitive categories differ-
entiating activities/tasks.
Toillustrate how such an exploration mechanism can automatically generate ordered learning
stages, let us first imagine a learner confronted with four categories of activities, as shown on
figure2(C).Thepractice ofeach ofthese four activities, whichcan beofvarying difficulty, leads
todifferent learning rates atdifferent points intime(see the topcurves, which show the evolution
ofprediction errorsineachactivity ifthelearnerweretofocus full-timeandexclusively oneach).
If, however, the learner uses curiosity-driven exploration to decide what and when to practice by
focusing onprogress niches, itwillavoid activities already predictable (curve 4)ortoo difficultto
learntopredict(curve1),inordertofocusfirstontheactivitywiththehighestlearningrate(curve
3)andeventually,whenthelatterstartstoreacha’plateau’,toswitchtothesecondmostpromising
6Technically thedecision on how much time tospend on agiven activity/context is achieved using Multi-Armed
Banditalgorithmsfortheso-calledexploration/exploitationdilemma[Audibertetal.,2009].Asthemeasureoflearning
progressineacharmisusedastherewardtomaximize, thisisanon-stationarybanditalgorithmsetting. However, a
specificityoflearningarchitecturesusedintheroboticexperimentspresentedhereisthatinsteadofrelyingonasetof
pre-definedbanditarms[Audibertetal.,2009],anunsupervisedlearningalgorithmdynamicallybuildsnewbanditarms
toselectfrom[Barane`sandOudeyer,2009].
14 Pierre-YvesOudeyer
Figure 2. ThePlayground Experiment [OudeyerandKaplan,2006, Oudeyeretal.,2007](A) The
learning context; (B)Thecomputational architecture forcuriosity-driven exploration: 1)therobot
learner probabilistically selects actions and contexts according to their potential to provide infor-
mationthatimprovestheworldmodel(i.e. reduces prediction errors); 2)anunsupervised learning
algorithms progressively differentiates actions and contexts to be selected; (C) Illustration of a
self-organized developmental sequence where the robot automatically identifies, categorizes and
shifts from simple to mode complex learning experiences. Figure adapted with permission from
[Gottliebetal.,2013].
learning situation (curve 2). Thus, embodied exploration driven by learning progress creates an
organizedexploratorystrategy,i.e. adevelopmentaltrajecotory: thesystemsystematicallyachieves
theselearningexperiencesinanorderanddoessobecausetheyyield(giventhepropensities ofthe
learnerandthephysicalworld)different patternsofuncertainty reduction.
In the Playground experiment, multiple experimental runs lead to two general categories of
results: self-organization andamixtureofregularitiesanddiversitiesinthedevelopmentalpatterns
Computational TheoriesofCuriosity-Driven Learning 15
[OudeyerandKaplan,2006,Oudeyeretal.,2007].
5.2. Self-Organization
Inalloftheruns,oneobservestheself-organizationofstructureddevelopmentaltrajectories,where
therobot explores objects andactions inaprogressively morecomplexstage-like manner. During
thisexploration, therobotacquiresautonomously diverseaffordances andskillsthatcanbereused
lateronandthatchangethelearningprogressinmorecomplicatedtasks,triggeringadevelopmen-
talcascade. Thefollowingdevelopmental sequence wastypically observed:
1. Inafirstphase,thelearnerachievesunorganized bodybabbling;
2. Inasecond phase, afterlearning afirstrough modelandmeta-model7,therobot stopscom-
bining motor primitives, exploring them one by one, but each primitive is explored itself in
arandom manner;
3. In athird phase, the learner begins to experiment withactions towards zones ofits environ-
mentwheretheexternalobserver knowsthereareobjects(therobotisnotinitially provided
witharepresentation oftheconcept ofobject), butinanon-affordant manner (e.g. itvocal-
izes at the non-responding elephant or tries to bash the teacher robot which is too far to be
touched);
4. In a fourth phase, the learner now explores the affordances of different objects in the envi-
ronment: typically focussing firstongrasping movementswiththeelephant, thenshiftingto
bashing movements with the hanging toy, and finally shifting to explorations of vocalizing
towardstheimitating teacher.
5. In the end, the learner has learnt sensorimotor affordances with several objects, as well
as social affordances, and has mastered multiple skills. None of these specific objectives
were pre-programmed. Instead, they self-organized through the dynamic interaction be-
tween curiosity-driven exploration, statistical inference, the properties of the body, and the
properties oftheenvironment.
Theseplayground experiments donotsimplysimulate particular skills (such asbatting attoys
to make them swing or vocalizations) but simulate an ordered and systematic developmental tra-
jectory, with a universality and stage-like structure that may be mistakenly taken to indicate an
internally-driven process of maturation. However, the trajectory is created through activity and
through the general principle that sensorimotor experiences that reduce uncertainty in predic-
tion are rewarding. In this way, developmental achievements can build on themselves without
specific pre-programmed dependencies but nonetheless like evolution itself create structure (see
[SmithandBreazeal, 2007,Smith,2013],forrelatedfindingsandarguments).
7The’model’refersheretothepredictiveworldmodelbeinglearnt,whichenablestopredicttheconsequences of
actions in a given context. The ’meta-model’ is another model built by the meta-cognite process, and continuously
estimatesexpectedlearningprogressofthelower-levelworldmodel.
16 Pierre-YvesOudeyer
5.3. Regularities andDiversity
Because these areself-organizing developmental processes, theygenerate notonly strong regular-
itiesbutalsodiversityacrossindividual developmental trajectories. Forexample,inmostrunsone
observes successively unorganized body babbling, then focused exploration of head movements,
then exploration of touching an object, then grasping an object, and finally vocalizing towards a
peer robot (pre-programmed to imitate). This can be explained as a gradual exploration of new
progress niches, and those stages and their ordering can be viewed as a form of attractor in the
space of developmental trajectories. Yet, with the same mechanism and same initial parameters,
individual trajectories may invert stages, or even generate qualitatively different behaviours. This
is due to stochasticity (the same motor commands do not produce always the same results), to
small variability in the physical realities and to the fact that this developmental dynamical system
hasseveralattractors withmoreorlessextendedandstrongdomainsofattraction (anattractorisa
partofthestate-spaceinwhichthedynamicalsystemconverges, dependingonwhatwashisinitial
state). We see this diversity as a positive outcome since individual development is not identical
across different individuals but is always, for each individual, unique in its own ways. This kind
ofapproach, then, offersawaytounderstand individual differences asemergent indevelopmental
processes itself and makes clear how developmental processes might vary across contexts, even
withanidentical learningmechanism.
A further result to be highlighted is the early development of vocal interaction. With a sin-
gle generic mechanism, the robot both explores and learns how to manipulate objects and how
to vocalize to trigger specific responses from a more mature partner [OudeyerandKaplan,2006,
KaplanandOudeyer, 2007a]. Vocal babbling and language games have been shown to be key
in infant language development; however, the motivation to engage in vocal play has often
been associated with hypothesized language specific motivation. The Playground Experiment
makes it possible to see how the exploration and learning of communicative behaviour might
be at least partially explained by general curiosity-driven exploration of the body affordances,
as also suggested by Oller [Oller,2000]. Exploring this idea further, Forestier and Oudeyer
[ForestierandOudeyer,2017] studied simulation showing how these mechanisms can drive the
joint development ofspeech andtooluse, wherespeech isdiscovered asaparticular toolenabling
togetsocialpeersachievetargeted actions.
5.4. Interaction withSocial Guidance
Other robotic models have explored how social guidance can beleveraged byan intrinsically mo-
tivated active learner and dynamically interact with curiosity to structure developmental trajecto-
ries [ThomazandBreazeal,2008, NguyenandOudeyer, 2014]. Focusing on vocal development,
Moulin-Frier et al. conducted experiments wherearobot explored the control ofarealistic model
of the vocal tract in interaction with vocal peers through a drive to maximize learning progress
[Moulin-Frier etal.,2014]. This model relied on a physical model of the vocal tract, its motor
control and the auditory system. Theexperiments showed self-organization ofvocal development
Computational TheoriesofCuriosity-Driven Learning 17
trajectories that share structural similarities with infants [Oller,2000]. They showed how these
mechanism generate an adaptive transition from vocal self-exploration with little influence from
the speech environment, to a later stage where vocal exploration becomes influenced by vocal-
izations of peers. Within the initial self-exploration phase, a sequence of vocal production stages
self-organizes, andshares properties withinfant data: thevocal learner firstdiscovers howtocon-
trolphonation,thenvocalvariationsofunarticulatedsounds,andfinallyarticulatedproto-syllables.
As the vocal learner becomes more proficient at producing complex sounds, the vocalizations of
the teacher become vocal goals to imitate that provide high learning progress, resulting in a shift
fromself-exploration tovocalimitation.
6. Challenges and Perspectives
Computational theories haveenabled tobetter understand thepotential structures andfunctions of
curiosity-driven learninginthelastdecade. Thesetheorieshaveidentifiedawidediversityofalgo-
rithmic mechanisms that could produce thekind of spontaneous exploration displayed byhumans
and other animals. This diversity concerns both the measures of interests (e.g. novelty, surprise,
learning progress, knowledge gap, intermediate complexity, ...) andtheentities towhichthebrain
mayapplythem(e.g. actions,states,goals,objects, tools,places,games,activities, learningstrate-
gies, social informants, ...), with time scales ranging from the moment-to-moment to days and
months. Furthermore, theoretical modelsofcuriosity-driven learning, andtheirapplication inarti-
ficialintelligence andmachinelearning, haveshownthekeyroleofthesemechanisms formaking
discoveries andsolving real-world problems withrareordeceptive rewards, inlargeandchanging
environments. Inbrief,computational theories:
1. haveshownthattheterm’curiosity’ coversawidediversityofcomplexmechanisms, gener-
atingdifferentformsofexploration;
2. have proposed ways to model and study these mechanisms formally, contributing to the
naturalization oftheconcept of’curiosity’;
3. have shown that curiosity mechanism are essential to learning and development, and thus
shouldbecomeacentraltopicincognitive sciencesandneurosciences.
Related work in psychology and neuroscience are beginning to converge with computational
theoriestowardsconceptualizinghowmechanismsofcuriositycanplayafundamentalroleinmany
aspects of development, ranging from sensorimotor, cognitive, emotional, to social development.
However,formultiplereasons,experimentalworkstudyingtheunderlyingmechanismsofcuriosity
have been very limited so far in psychology and neuroscience. The empirical testing of compu-
tational theories have been for a large part beyond the reach of existing experimental paradigms
in psychology and neuroscience. Several challenges need to be addressed to leverage further the
interaction betweentheoryandexperimentation.
18 Pierre-YvesOudeyer
The need for novel experimental paradigms in psychology and cognitive neuroscience. A
firstgeneralchallengeisthatcuriositydenotesasetofmechanismsthatpushindividualstoexplore
what is interesting for themselves, out of the consideration of external tasks or expectations of
social peers. Yet, the very act of participating to an experiment in a lab brings up expectations in
the subject’s mind about what the experimenter wants to observe or analyze, or will think about
whattheydo. Inthelab,curiositycandisappearquicklyassoonasonebeginstoobserveit. Thisis
probablylessthecasewithveryyounginfants, butintheircasethepresenceofsocialpeersisalso
bound toinfluence whatthey do, and their limited capabilities for physical exploration and verbal
reporting makes it difficult tostudy advanced forms ofcuriosity. So, how tostudy curiosity when
setting up a controlled experiment introduces complex interaction with other motivational forces
that are hard to control and evaluate? It is interesting to note that the most clear observations of
curiosity in the lab do not come from studies targeting curiosity and information-seeking, but are
rather observations of child behaviour spontaneously doing things that are wildly different from
the task the experimenter designed for them. For example, in recent experiments of Lauriane
Rat-Fisher and colleagues8 about tool use development, children are asked to retrieve a salient
toy stuck in a tube (the toy was expected to be very attractive to the child). Yet, several children
showedspontaneousstrongintrinsicinterestinexploringhowtopushsticksandobjectsinthetube,
continuingtodoitwithalotoffunaftergettingthetoyoutofthetube,andcompletelyignoringthe
toy. Unfortunately, these making off observations are typically removed from the lens of analysis
of traditional experimental studies, while they may display some of the most fundamental and
mysterious mechanismsoflearning andcognition.
Another experimental challenge is how to disentangle the potentially different mechanisms
identified by theory, which may be simultaneously at play in individuals, and potentially on dif-
ferent time scales. For example, it could be possible that curiosity-driven attention on very short
time scales may be driven by intrinsic rewards measuring different forms of novelty, surprise or
prediction error. However,onlonger time-scales, thecurious brainmayvaluetheintrinsic interest
of activities, games or goals with other measures of interest like learning progress. The study of
curiosity over long time scales, focusing on how it may contribute to sculpt sensorimotor, cog-
nitive and social development, on how it develops itself with time, and on how it interacts with
otherdevelopmental forcessuchassociallearning, ismaybethemostimportantandmostdifficult
challenge inthisscientificarea.
7. Acnknowledgements
ThismanuscripthasbenefitedfromveryusefulfeebackanddiscussionswithmembersoftheFlow-
ersteamatInria,aswellaswithJacqueline Gottlieb,LindaSmithandOlivierSigaud.
8personalcommunication
Computational TheoriesofCuriosity-Driven Learning 19
References
[AndreaeandAndreae,1978] Andreae, P.M.and Andreae, J. H.(1978). Ateachable machine in
therealworld. International JournalofMan-MachineStudies, 10(3):301–312.
[Andrychowicz etal.,2017] Andrychowicz,M.,Crow,D.,Ray,A.,Schneider,J.,Fong,R.,Welin-
der,P.,McGrew,B.,Tobin,J.,Abbeel,P.,andZaremba,W.(2017). Hindsightexperiencereplay.
InAdvances inNeuralInformation Processing Systems.
[Audibertetal.,2009] Audibert, J.Y.,Munos,R.,andSzepesvri, C.(2009). Explorationexploita-
tion tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science,
410(19):1876–1902.
[Baldassarre andM.,2013] Baldassarre, G. and M., M. (2013). Intrinsically Motivated Learning
inNaturalandArtificialSystems. Springer.
[Baragliaetal.,2016] Baraglia, J., Nagai, Y., and Asada, M. (2016). Emergence of altruistic be-
havior through the minimization of prediction error. IEEE Transactions on Cognitive and De-
velopmental Systems,8(3):141–151.
[Baranesetal.,2014] Baranes, A., Oudeyer, P., and Gottlieb, J. (2014). The effects of task diffi-
culty, novelty and the size ofthe search space on intrinsically motivated exploration. Frontiers
inNeuroscience.
[Barane`sandOudeyer,2009] Barane`s, A. and Oudeyer, P.-Y. (2009). R-iac: Robust intrinsically
motivatedexploration andactivelearning. IEEETransactionsonAutonomousMentalDevelop-
ment,1(3):155–169.
[BaranesandOudeyer,2013] Baranes, A. and Oudeyer, P.-Y. (2013). Active learning of inverse
models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Sys-
tems,61(1).
[Barto,2013] Barto, A. (2013). Intrinsic motivation and reinforcement learning. In Intrinsically
motivatedlearning innaturalandartificial systems,pages17–47.SpringerBerlinHeidelberg.
[Bartoetal.,2004] Barto, A. G., Singh, S., and Chentanez, N. (2004). Intrinsically motivated
learningofhierarchicalcollectionsofskills. InProceedingsofthe3rdInternationalConference
onDevelopmentandLearning, pages112–19.
[Begusetal.,2016] Begus, K., Gliga, T., and Southgate (2016). Infants’ preferences for na-
tive speakers are associated with an expectation of information. Proc Natl Acad Sci U S A,
113:12397–12402.
[Bellemareetal.,2016] Bellemare, M.,Srinivasan, S.,Ostrovski, G.,Schaul,T.,Saxton,D.,,and
Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. In Advances in
NeuralInformation Processing Systems,page14711479.
20 Pierre-YvesOudeyer
[BenureauandOudeyer, 2016] Benureau, F. C. and Oudeyer, P.-Y. (2016). Behavioral diversity
generation in autonomous exploration through reuse of past experience. Frontiers in Robotics
andAI,3:8.
[Berlyne, 1960] Berlyne,D.(1960). Conflict, ArousalandCuriosity. McGraw-Hill,NewYork.
[Bruneretal.,1976] Bruner, J. S., Jolly, A., and Sylva, K. (1976). Play: Its role in development
andevolution. BasicBooks.
[Buckleyetal.,2017] Buckley, C., Kim, C., McGregor, S., and Seth, A. (2017). The free energy
principle foraction andperception: Amathematical review. Journal ofMathematical Psychol-
ogy,81:55–79.
[Cabietal.,2017] Cabi, S., Colmenarejo, S. G., Hoffman, M. W., Denil, M., Wang, Z., and
De Freitas, N. (2017). The intentional unintentional agent: Learning to solve many continu-
ouscontroltaskssimultaneously. In1stConference onRobotLearning, CoRL.
[ChaterandLoewenstein, 2016] Chater, N. and Loewenstein, G. (2016). The under-appreciated
driveforsense-making. JournalofEconomicBehavior&Organization, 126:137–154.
[Colasetal.,2018] Colas, C.,Sigaud, O.,andOudeyer, P.-Y.(2018). Gep-pg: Decoupling explo-
ration and exploitation indeepreinforcement learning algorithms. InInternational Conference
onMachineLearning(ICML).
[Csikszenthmihalyi, 1991] Csikszenthmihalyi, M.(1991). Flow-thePsychology ofOptimalExpe-
rience. Perennial,Harper.
[DayanandSejnowski,1996] Dayan, P. and Sejnowski, T. J. (1996). Exploration bonuses and
dualcontrol. MachineLearning, 25(1):522.
[Florensaetal.,2017] Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and Abbeel, P. (2017).
Reverse curriculum generation for reinforcement learning. In Proceedings of the 1st Annual
ConferenceonRobotLearning, inPMLR78,pages482–495.
[Forestieretal.,2017] Forestier, S., Mollard, Y., and Oudeyer, P.-Y. (2017). Intrinsically
motivated goal exploration processes with automatic curriculum learning. arXiv preprint
arXiv:1708.02190.
[ForestierandOudeyer,2016a] Forestier, S. and Oudeyer, P.-Y. (2016a). Curiosity-driven devel-
opment of tool use precursors: a computational model. In Proceedings of the 38th Annual
ConferenceoftheCognitiveScienceSociety, pages1859–1864.
[ForestierandOudeyer,2016b] Forestier,S.andOudeyer,P.-Y.(2016b). Modularactivecuriosity-
driven discovery of tool use. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ Inter-
national Conferenceon,pages3965–3972. IEEE.
Computational TheoriesofCuriosity-Driven Learning 21
[ForestierandOudeyer,2016c] Forestier, S. and Oudeyer, P.-Y. (2016c). Overlapping waves in
tool use development: a curiosity-driven computational model. In The Sixth Joint IEEE Inter-
national ConferenceDevelopmental LearningandEpigenetic Robotics.
[ForestierandOudeyer,2017] Forestier, S.andOudeyer, P.-Y.(2017). Aunifiedmodelofspeech
and tool use early development. In Proceedings of the 39th Annual Meeting of the Cognitive
ScienceSociety.
[Fristonetal.,2017a] Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G.
(2017a). Activeinference: Aprocesstheory. Neuralcomputation, 29(1):149.
[Fristonetal.,2017b] Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., and On-
dobaka, S.(2017b). Activeinference, curiosity andinsight. Neuralcomputation, 29(10):2633–
2683.
[Gerkenetal.,2011] Gerken,L.,Balcomb,F.K.,andMinton,J.L.(2011). Infantsavoidlabouring
in vain by attending more to learnable than unlearnable linguistic patterns. Developmental
science, 14(5):972–979.
[Gigerenzer andGoldstein, 1996] Gigerenzer,G.andGoldstein,G.(1996). Reasoningthefastand
frugalway: modelsofbounded rationality. Psychological review,103(4):650.
[Gopnik,2012] Gopnik, A. (2012). Scientific thinking in young children: Theoretical advances,
empiricalresearch, andpolicyimplications. Science, 337(6102):1623–1627.
[Gordonetal.,2014] Gordon, G., Fonio, E., and Ahissar, E. (2014). Emergent exploration via
noveltymanagement. JournalofNeuroscience, 34(38):12646–12661.
[Gottliebetal.,2013] Gottlieb, J., Oudeyer, P.-Y., Lopes, M., and Baranes, A. (2013).
Information-seeking, curiosity, and attention: computational and neural mechanisms. Trends
incognitive sciences, 17(11):585–593.
[Harlow,1950] Harlow, H. (1950). Learning and satiation of response in intrinsically motivated
complexpuzzleperformances bymonkeys. J.Comp.Physiol.Psychol,43:289–294.
[Hunt,1965] Hunt, J. (1965). Intrinsic motivation and its role in psychological development. In
NebraskaSymposium onMotivation, volume13,pages189–282.
[Jaderberg etal.,2016] Jaderberg, M.,Mnih,V.,Czarnecki,W.M.,Schaul,T.,Leibo,J.Z.,Silver,
D., and Kavukcuoglu, K. (2016). Reinforcement learning with unsupervised auxiliary tasks.
arXivpreprint arXiv:1611.05397.
[Kagan,1972] Kagan,J.(1972). Motivesanddevelopment. J.Pers.Soc.Psychol,22:5166.
22 Pierre-YvesOudeyer
[KaplanandOudeyer,2003] Kaplan, F. and Oudeyer, P.-Y. (2003). Motivational principles for
visualknow-howdevelopment. InPrince,C.,Berthouze,L.,Kozima,H.,Bullock,D.,Stojanov,
G., and Balkenius, C., editors, Proceedings of the 3rd international workshop on Epigenetic
Robotics : Modeling cognitive development in robotic systems, no. 101, pages 73–80. Lund
UniversityCognitiveStudies.
[KaplanandOudeyer,2007a] Kaplan, F. and Oudeyer, P.-Y. (2007a). In search of the neural cir-
cuitsofintrinsic motivation. Frontiersinneuroscience, 1:17.
[KaplanandOudeyer,2007b] Kaplan, F.and Oudeyer, P.-Y.(2007b). Theprogress-drive hypoth-
esis: an interpretation of early imitation. In Dautenhahn, K. and Nehaniv, C., editors, Models
and mechanisms of imitation and social learning: Behavioural, social and communication di-
mensions, pages361–377. CambridgeUniversityPress.
[Kiddetal.,2012] Kidd, C., Piantadosi, S., and Aslin, R. (2012). The goldilocks effect: Human
infantsallocateattentiontovisualsequencesthatareneithertoosimplenortoocomplex. PLOS
ONE,7:e36399.
[Kulkarnietal.,2016] Kulkarni, T.,Narasimhan, K.,Saeedi, A.,,andTenenbaum, J.(2016). Hi-
erarchical deepreinforce- mentlearning: Integrating temporalabstraction andintrinsic motiva-
tion. InAdvancesinNeuralInformation Processing Systems,page36753683.
[LehmanandStanley,2008] Lehman,J.andStanley, K.O.(2008). Exploiting open-endedness to
solveproblemsthrough thesearchfornovelty. InALIFE,pages329–336.
[Lillicrapetal.,2015] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Sil-
ver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv
preprintarXiv:1509.02971.
[LittleandSommer,2013] Little, D. Y. and Sommer, F. T. (2013). Learning and exploration in
action-perception loops. Frontiers inneuralcircuits, 7.
[Machadoetal.,2017] Machado, M. C., Bellemare, M. G., and Bowling, M. (2017). A lapla-
cianframeworkforoption discoveryinreinforcement learning. InInternational Conference on
MachineLearning (ICML).
[Martiusetal.,2013] Martius,G.,Der,R.,andAy,N.(2013). Informationdrivenself-organization
ofcomplexroboticbehaviors. PloSone,8(5):e63400.
[Moulin-Frier etal.,2014] Moulin-Frier, C., Nguyen, S. M., and Oudeyer, P. Y. (2014). Self-
organizationofearlyvocaldevelopmentininfantsandmachines: theroleofintrinsicmotivation.
Frontiersinpsychology, 4:1006.
[MouretandDoncieux, 2012] Mouret, J. B. and Doncieux, S. (2012). Encouraging behavioral
diversity in evolutionary robotics: An empirical study. Evolutionary computation, 20(1):91–
133.
Computational TheoriesofCuriosity-Driven Learning 23
[NguyenandOudeyer,2014] Nguyen, M. and Oudeyer, P.-Y. (2014). Socially guided intrinsic
motivationforrobotlearningofmotorskills. AutonomousRobots,36(3):273–294.
[Oller,2000] Oller, D. (2000). The emergence of the speech capacity. Lawrence Erlbaum and
Associates Inc,Mahwah,NJ.
[Ostrovskietal.,2017] Ostrovski, G., Bellemare, M. G., Oord, A. V. D., and Munos, R. (2017).
Count-based exploration withneural density models. In Proceedings ofthe International Con-
ferenceonMachineLearning.
[OudeyerandKaplan,2007] Oudeyer, P. and Kaplan, F. (2007). What is intrinsic motivation? a
typology ofcomputational approaches. Frontiersinneurorobotics, 1:6.
[Oudeyeretal.,2013] Oudeyer, P.-Y.,Baranes, A.,and Kaplan, F.(2013). Intrinsically motivated
learningofreal-worldsensorimotorskillswithdevelopmentalconstraints. InIntrinsically moti-
vatedlearninginnaturalandartificial systems,page303365. Springer, Berlin,Heidelberg.
[OudeyerandKaplan,2006] Oudeyer, P.-Y. and Kaplan, F. (2006). Discovering communication.
Connection Science, 18(2):189–206.
[Oudeyeretal.,2007] Oudeyer, P.-Y.,Kaplan,F.,andHafner, V.(2007). Intrinsic motivation sys-
temsforautonomous mentaldevelopment. IEEETrans.Evol.Comput,11(2):265–286.
[OudeyerandSmith,2016] Oudeyer,P.-Y.andSmith,L.(2016). Howevolutioncanworkthrough
curiosity-driven developmental process. Top.Cogn.Sci,8(2):492–502.
[Pathaketal.,2017] Pathak,D.,Agrawal,P.,Efros,A.A.,andDarrell,T.(2017). Curiosity-driven
exploration by self-supervised prediction. In International Conference on Machine Learning
(ICML).
[Pe´re´ etal.,2018] Pe´re´, A., Forestier, S., Sigaud, O., and Oudeyer, P.-Y. (2018). Unsupervised
learningofgoalspacesforintrinsicallymotivatedgoalexploration. InInternationalConference
onLearningRepresentations (ICLR).
[Salgeetal.,2014] Salge,C.,Glackin,C.,andPolani,D.(2014). Changingtheenvironmentbased
onempowermentasintrinsicmotivation. Entropy, 16(5):2789–2819.
[Schmidhuber, 1991] Schmidhuber, J. (1991). Curious model-building control systems. In Pro-
ceedingsoftheInternationalJointConferenceonNeuralNetwork,volume2,pages1458–1463.
[Schulmanetal.,2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
(2017). Proximalpolicyoptimization algorithms. arXivpreprintarXiv:1707.06347.
[SigaudandStulp,2018] Sigaud, O. and Stulp, F. (2018). Policy search in continuous action do-
mains: anoverview. arXivpreprintarXiv:1803.04706.
24 Pierre-YvesOudeyer
[Singhetal.,2010] Singh, S., Lewis, R. L., Barto, A. G., and Sorg, J. (2010). Intrinsically moti-
vatedreinforcement learning: Anevolutionary perspective. IEEETransactions onAutonomous
MentalDevelopment, 2(2):70–82.
[Smith,2013] Smith,L.(2013). Itsallconnected: Pathwaysinvisualobjectrecognition andearly
nounlearning. AmericanPsychologist, 68(8):618.
[SmithandBreazeal, 2007] Smith,L.andBreazeal,C.(2007). Thedynamicliftofdevelopmental
process. Developmental Science, 10(1):61–68.
[Sutton,1990] Sutton, R. (1990). Integrated architectures for learning, planning, and reacting
based on approximating dynamic programming. In Proceedings of the Seventh International
ConferenceonMachineLearning,pages216–224.
[SuttonandBarto,2018] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An in-
troduction, volume1. MITpressCambridge. SecondEdition.
[ThomazandBreazeal,2008] Thomaz, A. L. and Breazeal, C. (2008). Experiments in socially
guidedexploration: Lessonslearnedinbuildingrobotsthatlearnwithandwithouthumanteach-
ers. Connection Science, 20(23):91–110.
[TwomeyandWestermann,2017] Twomey,K.andWestermann,G.(2017).Curiosity-basedlearn-
ingininfants: Aneurocomputational approach. Developmental Science.
[Zhuetal.,2017] Zhu, Q., Triesch, J., and Shi, B. (2017). Joint learning of binocularly driven
saccades andvergence byactiveefficientcoding. Frontiersinneurorobotics, 11:58.