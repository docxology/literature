The Poincar´e-Boltzmann Machine: from Statistical Physics to
Machine Learning and back
Pierre Baudot
Median Technologies, Sophia Antipolis and Inserm UNIS UMR1072 - Universit Aix-Marseille
pierre.baudot@gmail.com
5th July 2019
Abstract
Thispaperpresentsthecomputationalmethodsofinformationcohomologyappliedtogeneticexpressionin
[125,16,17]andinthecompanionpaper[16]andproposesitsinterpretationsintermsofstatisticalphysicsand
machinelearning. InordertofurtherunderlinetheHochschildcohomologicalnatureafinformationfunctions
and chain rules, following [13, 14, 133], the computation of the cohomology in low degrees is detailed to
show more directly that the k multivariate mutual-informations (I ) are k-coboundaries. The k-cocycles
k
condition corresponds to I = 0, which generalize statistical independence to arbitrary dimension k [16]. k
Hence the cohomology can be interpreted as quantifying the statistical dependences and the obstruction to
factorization. The topological approach allows to investigate Shannon’s information in the multivariate case
withouttheassumptionsofindependentidenticallydistributedvariablesandofstatisticalinteractionswithout
mean field approximations. We develop the computationally tractable subcase of simplicial information
cohomology represented by entropy H and information I landscapes and their respective paths. The I
k k 1
component defines a self-internal energy functional U , and (−1)kI components define the contribution
k k,k≥2
to a free energy functional G (the total correlation) of the k-body interactions. The set of information
k
paths in simplicial structures is in bijection with the symmetric group and random processes, provides a
trivial topological expression of the 2nd law of thermodynamic. The local minima of free-energy, related
to conditional information negativity, and conditional independence, characterize a minimum free energy
complex. This complex formalizes the minimum free-energy principle in topology, provides a definition of
a complex system, and characterizes a multiplicity of local minima that quantifies the diversity observed in
biology. IgiveaninterpretationofthiscomplexintermsoffrustrationinglassandofVanDerWallsk-body
interactions for data points.
”Now what is science? ...it is before all a
classification, a manner of bringing together
facts which appearances separate, though they
are bound together by some natural and hidden
kinship. Science, in other words, is a system of
relations. ...it is in relations alone that
objectivity must be sought. ...it is relations
alone which can be regarded as objective.
External objects... are really objects and not
fleeting and fugitive appearances, because they
are not only groups of sensations, but groups
cemented by a constant bond. It is this bond,
and this bond alone, which is the object in itself,
and this bond is a relation.”
H. Poincar´e
Contents
1 Introduction 2
2 Information cohomology 3
2.1 A long march through information topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Information functions (definitions) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.3 Information structures and coboundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3.1 first degree (k=1). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1
9102
luJ
6
]CN.oib-q[
1v68460.7091:viXra
2.3.2 Second degree (k=2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.3.3 Third degree (k=3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.3.4 Higher degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3 Simplicial information cohomology 9
3.1 Simplicial substructures of information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 Topological Self and Free Energy of k-body interacting system - Poincar´e-Shannon Machine . . . 10
3.2.1 Topological Self and Free Energy of k-body interacting system . . . . . . . . . . . . . . . 10
3.3 k-Entropy and k-Information landscapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.4 Information Paths and Minimum Free Energy Complex . . . . . . . . . . . . . . . . . . . . . . . 13
3.4.1 Information Paths (definition). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.4.2 Derivatives, inequalities and conditional mutual information negativity . . . . . . . . . . . 14
3.4.3 Information paths are random processes: topological 2nd law of thermodynamic and en-
tropy rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.4.4 Local minima and critical dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4.5 Sum over paths and mean information path . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4.6 Minimum free energy complex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4 Discussion 19
4.1 Complexity through finite dimension non-extensivity and non iid . . . . . . . . . . . . . . . . . . 19
4.2 Epigenetic topological learning - biological diversity . . . . . . . . . . . . . . . . . . . . . . . . . 20
1 Introduction
The present paper hasno deep mathematical or novelty pretension and just aims topresent some computational
aspects and applications to data of the simplicial simplest special case of the cohomology theory developed in
[13, 16] and extended by Vigneaux [133, 135] to Tsallis entropies and differential entropy (etc.). The direct ap-
plication to genetic expression measures, performance in cell type or gene module detection and its relevance in
epigenetic co-regulation and differentiation terms can be find in [125]. In its application to empirically measured
data, information cohomology is at the cross-road of both data analysis and statistical physics, and I aim to give
somekeysofitsinterpretationwithinthosetwofields,whatcouldbequotedas”passingtheinformationbetween
disciplines” in reference to Mezard’s review [95].
Unsupervisedneuralnetworklearning: theoriginalworkbasedonspinnetworksbyHopfield[59]formalized
fully recurrent networks as n binary random variables (N =2). Ackley, Hinton and Sejnowski [1] followed up by
imposing the Markov Field condition, allowing the introduction of conditional independence to handle network
structures with hidden layers and hidden nodes. The result, the Boltzmann or Helmholtz machine [36], relies on
the maximum entropy or free-energy minimization principle, and originally on minimizing the relative entropy
between the network and environmental states [1]. The informational approach of topological data analysis pro-
vides a direct probabilistic and statistical analysis of the structure of a dataset which allows to bridge the gap
with neural networks analysis, and may be to go a step beyond their formalization in mathematic. Indeed, as
proposed in [12] and references therein (notably see the whole opus of Marcolli with application to linguistic),
therearesomereasonstobelievethatartificialornaturalcognitioncouldbehomologicalbynature. Considering
neuron as binary random variables provides in the present context an homologically constrained approach of
those neural networks where the first input layer is represented by the marginal (single variable, degree 1 com-
ponent) while hidden layers are associated to higher degrees. In a very naive sense higher cohomological degrees
distinguish higher order patterns (or higher dimensional patterns in the simplicial case), just as receptive fields
of neural networks recognize higher order features when going to higher depth-rank of neural layers as described
in David Marr’s original sketch [88], and now implemented efficiently in deep network structures. Here, in order
to allow a direct interpretation of the information topology in standard terms of machine learning; I provide an
energy functional interpretation of Mutual Information I and of Total Correlation G functions. Notably, the
k k
introductionofthismultiplicityof”energyfunctions”broadencommonunsupervisedmethodsinneuralnetwork
: instead of a single energy and associated gradient descent, Mutual informations provide a combinatorial family
ofanalyticallyindependentfunctionswithindependentgradients(see[16]forassociatedtheorems). Thescopeis
restricted to unsupervised learning here, but the supervised subcase with explicit homological information ver-
sion of the back propagation chain rule, can be readily inferred from this and will be detailed with data analysis
examples in future publication and illustrated here [15]. Notably, the mathematical depth of back-propagation
algorithm [68, 38, 114] comes from the fact that it implements the chain rule for derivation with respect to the
parameters, allowing to learn in the class of differentiable functions. In the information cohomological reformu-
lation,thechainrulebecomesthechainruleofmutual-informationthatarecoboundaries(commonlydifferential
operator), parameters are provided by the label-teaching variables.
2
Statistical physics: On the
Contents
sideofstatisticalphysics,thegeneralizationofbinaryneuronal-isingmodelstoarbitrarymultivaluedvariables
withN valuescorrespondstothegeneralizationofspinmodelstoPottsmodels[143]. Suchgeneralizationisper-
i
tinent for biological systems in general, since coding and meaningful variables in biology are usually non-digital
and only in quite exceptional cases binary. More interestingly, Mutual-Information negativity, a phenomenon
knowntoprovidethesignatureoffrustratedstatesinglassessincetheworkofMatsuda[89],isrelatedhere,inthe
context of the more general conditional mutual information, to a kind of first order transition, yet seen topolog-
ically as the critical points of a simplicial complex. We propose to analyse the statistical interactions quantified
byMutual-InformationsinfullanalogywiththeVanDerWallsinteractionmodelwhileleavinganyhomogeneous
or mean-field assumptions on the variable. This observation makes coincide two phenomena, condensation in
statistical physics and clustering in data science. We hence further suggest that the Mutual-Informations func-
tions and homology should be peculiarly useful in the study of frustrated states and other related problems (see
Mezard and Montanari [96]), and for data points clustering.
Topological Data Analysis: On the side of applied algebraic topology, the identification of the topological
structures of dataset has motivated important researches following the development of persistent homology [81,
44, 28]. Combining in a single framework statistical and topological structures remains an active challenge of
data analysis that already gave some interesting results [102, 27]. Some recent works have proposed information
theoretical approaches grounded on homology, defining persistent entropy [32, 93], graph’s topological entropy
[122],spectralentropy[83],ormultilevelIntegrationentropies[84]. Althoughthepresentworkisformallydifferent
and arose independently of persistence methods, it is possible to provide an intuitive non-rigorous interpretation
in persistence terms. Most of the persistent methods consist in approximating the birth and death of the Betti’s
numbers of the C˘ech complex obtained by considering balls around each data point while the radius of the ball
grows. The C˘ech complex is given by the intersection of the balls, and for combinatorial computational reasons,
most of the algorithm restrict to pairwise intersections giving the Vietoris-Ripps complex as an approximation
of the C˘ech complex. Our method focuses on intersection of random variables rather than balls around data
points: a theorem of Hu Kuo Ting [60] (recalled in [16]) shows the equivalence of mutual information functions
with set theoretic finite measurable functions endowed with the intersection operator, formalizing the naive
Venn diagram usual interpretation of mutual informations. Hence, leaving the mathematical rigor to allow an
interpretation of the current algorithm in the common language of persistence: I compute here an information
theoretic analog of the C˘ech complex, and it is not excluded that such analogy can be made formal, and notably
to establish some nerve theorem for information structures (see Oudot for review [104]). It turns out that ”zero
information intersections” is exactly equivalent to statistical independence (theorem 1.2 [16]). Hence, in regard
to current topological data analysis methods, the methods presented here provides an intrinsically probabilistic
cohomologicalframework: thedifferentialoperatorsarefundamentalmapsinprobability-informationtheory. As
a consequence, no metric assumption is required apriori: in practice it is possible to compute the homology for
example on position variables and-or on qualitative variable such as ”nice” and ”not nice”. The present method
is topological and avoids the introduction a priori of such a metric: rather, a family of Shannon’s pseudometric
emerges from the formalism as a first cohomological class (cf. section 2.3.1): considering a symmetric action of
conditioning, we obtain Shannon’s metric parametrized by a scalar multiplicative constant. Notably, the notion
of geodesic used in machine learning is replaced by the homotopical notion of path. On the data analysis side,
it provides new algorithm and tools for Topological Data Analysis allowing to rank, detect clusters, functional
modules and to make dimensionality reduction; all these classical tasks in data analysis have indeed a direct
homologicalmeaning. Iproposetocallthedataanalysismethodspresentedhere,thePoincar´e-Shannonmachine,
since it implements simplicial homology and information theory in a single framework, applied effectively to
empirical data.
2 Information cohomology
Thissectionprovidesashortbibliographicalnoteontheinscriptionofinformationandprobabilitytheorywithin
homologicaltheories2.1. Wealsorecallthedefinitionofinformationfunctions2.2andprovideashortdescription
of information cohomology computed in the low degrees 2.3, such that the interpretation of entropy and mutual-
informations within Hochschild cohomology appears straightforward and clear. There is no new result in this
section, but I hope a more simple and helpful presentation for some researchers outside the field of topology, of
what can be find in [13, 133, 135] that should be considered for more precise and detailed exposition.
3
2.1 A long march through information topology
From the mathematical point of view, a motivation of information topology is to capture the ambiguity theory
ofGalois, whichistheessenceofgrouptheoryordiscretesymmetries(seeAndr´e’sreviews[4,5]),andShannon’s
information uncertainty theory in a common framework, a path already paved by some results on information
inequalities (see Yeung’s results [146]) and in algebraic geometry. In the work of Cathelineau, [30], entropy first
appeared in the computation of the degree one homology of the group SL(2,C) with coefficients in the adjoint
action by choosing a pertinent definition of the derivative of the Bloch-Wigner dilogarithm. It could be shown
that the functional equation with 5-terms of the dilogarithm implies the functional equation of entropy with
4-terms. Kontsevitch [72] discovered that a finite truncated version of the logarithm appearing in cyclotomic
studiesalsosatisfiedthefunctionalequationofentropy, suggestingahigherdegreegeneralizationofinformation,
analog to polylogarithm, and hence showing that the functional equation of entropy holds in p and 0 field
characteristics. Elbaz-Vincent and Gangl used algebraic means to construct this information generalization
which holds over finite fields [42], and where information functions appear as derivations [43]. After entropy
appeared in tropical and idempotent semi-ring analysis in the study of the extension of Witt semiring to the
characteristic 1 limit [33], Marcolli and Thorngren developed thermodynamic semiring, and entropy operad that
could be constructed as deformation of the tropical semiring [86]. Introducing Rota-Baxter algebras, it allowed
to derive a renormalization procedure [85]. Baez, Fritz and Leinster defining the category of finite probability
and using Fadeev axiomatization, could show that the only family of functions that has the functorial property
is Shannon information loss [9, 11]. Boyom, basing his approach on information and Koszul geometry, developed
a more geometrical view of statistical models that notably considers foliations in place of the random variables
[25]. Introducing a deformation theoretic framework, and chain complex of random variables, Drumond-Cole,
Park and Terilla [39, 40, 105] could construct a homotopy probability theory for which the cumulants coincide
with the morphisms of the homotopy algebras. A probabilistic framework, used here, was introduced in [13],
and generalized to Tsallis entropies by Vigneaux [133, 135]. The diversity of the formalisms employed in these
independent but convergent approaches is astonishing. So, to the question what is information topology, it is
only possible to answer that it is under development at the moment. The results of Catelineau, Elbaz-Vincent
and Gangl inscribed information into the theory of motives, which according to Beilison’s program is a mixed
Hodge-Tate cohomology [18]. All along the development of the application to data, following the cohomology
developed by [13, 133] on an explicit probabilistic basis, we aimed to preserve such a structure and unravel its
expressionininformationtheoreticterms. Moreover,followingAomoto’sresults[7,53],theactualconjecture[13]
isthatthehigherclassesofinformationcohomologyshouldbesomekindofpolylogarithmick-form(k-differential
volume that are symmetric and additive, and that correspond to the cocycle conditions for the cohomology of
Lie groups [7]). The following developments suggest that these higher information groups should be the families
of functions satisfying the functional equations of k-independence I =0, a rather vague but intuitive view that
k
can be tested in special cases.
2.2 Information functions (definitions)
The information functions used in [13] and the present study were originally defined by Shannon [117] and
Kullback[74]andfurthergeneralizedanddevelopedbyHuKuoTing[60]andYeung[147](seealsoMcGill[92]).
These functions include entropy, noted H = H(X;P), joint entropy, noted H = H(X ,...,X ;P), mutual-
1 k 1 k
information noted I = I(X ;X ;P), multivariate k-mutual-information, noted I = I(X ;...;X ;P) and the
2 1 2 k 1 k
conditional entropy and mutual information, noted Y.H = H(X ,...,X |Y;P) and Y.I = I(X ;...;X |Y;P).
k 1 k k 1 k
The classical expression of these functions is the following (using k =−1/ln2, the usual bit unit):
• The Shannon-Gibbs entropy of a single variable X is defined by [117]:
j
Nj
(cid:88) (cid:88)
H =H(X ;P )=k p(x)lnp(x)=k p lnp (1)
1 j Xj i i
x∈[Nj] i=1
where [N ]={1,...,N } denotes the alphabet of X .
j j j
• TherelativeentropyorKullback-Lieblerdivergence,whichwasalsocalled”discriminationinformation”by
Kullback [74], is defined for two probability mass function p(x) and q(x) by:
(cid:88) q(x)
D(p(x)||q(x))=D(X;p(x)||q(x))=k p(x)ln
p(x)
x∈X (2)
=H(X;p(x),q(x))−H(X;p(x))
where H(X;p(x),q(x)) is the cross-entropy and H(X;p(x)) the Shannon entropy. It hence generates as a
special case minus entropy, taking the deterministic constant probability q(x) = 1. With the convention
k =−1/ln2, D(p(x)||q(x)) is always positive or null.
4
• The joint entropy is defined for any joint-product of k random variables (X ,...,X ) and for a probability
1 k
joint-distribution P by [117]:
(X1,...,Xk)
H =H(X ,...,X ;P )
k 1 k X1,...,Xk
N1×
(cid:88)
...×Nk
=k p(x .....x )lnp(x .....x )
1 k 1 k
x1,...,xk∈[N1×...×Nk] (3)
N1(cid:88) ,...,Nk
=k p lnp
ij...k ij...k
i,j,...,k (cid:124)(cid:123)(cid:122)(cid:125)
kindices
where [N ×...×N ]={1,...,N ×...×N } denotes the alphabet of (X ,...,X ).
1 k j k 1 k
• The mutual information of two variables X ,X is defined as [117]:
1 2
I(X ;X ;P )=k
N (cid:88)1×N2
p(x .x )ln
p(x
1
)p(x
2
)
(4)
1 2 X1,X2 1 2 p(x .x )
1 2
x1,x2∈[N1×N2]
And it can be generalized to k-mutual-information (also called co-information) using the alternated sums
given by equation 17, as originally defined by McGill [92] and Hu Kuo Ting [60], giving:
N1×
(cid:88)
...×Nk (cid:81)
I⊂[k];card(I)=i;i odd
p
I
I =I(X ;...;X ;P)=k p(x .....x )ln (5)
k 1 k 1 k (cid:81) p
x1,...,xk∈[N1×...×Nk] I⊂[k];card(I)=i;i even I
For example, the 3-mutual information is the function:
I =k
N1×
(cid:88)
N2×N3
p(x .x .x )ln p(x 1 )p(x 2 )p(x 3 )p(x 1 .x 2 .x 3 ) (6)
3 1 2 3 p(x .x )p(x .x )p(x .x )
1 2 1 3 2 3
x1,x2,x3∈[N1×N2×N3]
For k ≥3, I can be negative [60].
k
• The total correlation introduced by Watanabe [138], called integration by Tononi and Edelman [130]
or multi-information by Studeny´ and Vejnarova [121] and Margolin and colleagues [87], which we note
C (X ;...X ;P), is defined by:
k 1 k
k k
(cid:88) (cid:88) (cid:88)
C =C (X ;...X ;P)= H(X )−H(X ;...X )= (−1)i I (X ;P)
k k 1 k i 1 k i I
i=1 i=2 I⊂[n];card(I)=i
(7)
=k
N1×
(cid:88)
...×Nk
p(x ....x )ln p(x 1 ...x k )
1 k p(x )...p(x )
1 k
x1,...,xk∈[N1×...×Nk]
For two variables the total correlation is equal to the mutual-information (C =I ). The total correlation
2 2
has the nice property of being a relative entropy 2 between marginal and joint-variable and hence to be
always non-negative.
• The conditional entropy of X knowing (or given) X is defined as [117]:
1 2
N (cid:88)1∗N2
X .H =H(X |X ;P)=k p(x .x )lnp (x )
2 1 1 2 1 2 x2 1
x1,x2∈[N1×N2]
(cid:88)
N2 (cid:32)
(cid:88)
N1 (cid:33)
=k p(x ). p x lnp x (8)
2 x2 1 x2 1
x2∈X
2
x1∈X
1
Conditionaljoint-entropy,X .H(X ,X )or(X ,X ).H(X ),isdefinedanalogouslybyreplacingthemarginal
3 1 2 1 2 3
probabilities by the joint probabilities.
• The conditional mutual information of two variables X ,X knowing a third X is defined as [117]:
1 2 3
X .I =I(X ;X |X ;P)=k
N1×
(cid:88)
N2×N3
p(x .x .x )ln p x3 (x 1 )p x3 (x 2 ) (9)
3 2 1 2 3 1 2 3 p (x ,x )
x1,x2,x3∈[N1×N2×N3]
x3 1 2
Conditionalmutualinformationgeneratesalltheprecedinginformationfunctionsassubcases, asshownby
Yeung [147]. We have the theorem : if X = Ω then it gives the mutual information, if X = X it gives
3 2 1
conditional entropy, and if both conditions are satisfied, it gives entropy. Notably, we have I =H .
1 1
5
We now give the few information equalities and inequalities that are of central use in the homological frame-
work, in the information diagrams and for the estimation of the informations from the data.
We have the chain rules (see [35] for proofs):
H(X ;X ;P)=H(X ;P)+X .H(X ;P)=H(X ;P)+X .H(X ;P) (10)
1 2 1 1 2 2 2 1
I(X ;X ;P)=H(X ;P)−X .H(X ;P)=H(X ;P)−X .H(X ;P) (11)
1 2 1 2 1 2 1 2
That we can write more generally (where the hat denotes the omission of the variable):
H(X
1
;...;X(cid:99)i ;...;X
k+1
;P)=H(X
1
;...;X
k+1
;P)−(X
1
;...;X(cid:99)i ;...;X
k+1
).H(X
i
;P) (12)
That we can write in short H −H =(X ,...X ).H(X )
k+1 k 1 k k+1
I(X
1
;...;X(cid:99)i ;...;X
k+1
;P)=I(X
1
;...;X
k+1
;P)+X
i
.I(X
1
;...;X(cid:99)i ;...;X
k+1
;P) (13)
That we can write in short I −I =X .I , generating the chain rule 10 as special case.
k−1 k k k−1
These two equations provide recurrence relationships that give an alternative formulation of the chain rules
in terms of a chosen path on the lattice of information structures:
k
(cid:88)
H =H(X ,...,X ;P)= (X ,...,X ).H(X ;P) (14)
k 1 k 1 i−1 i
i=1
where we assume H(X ;P)=X .H(X ;P) and hence that X is the greatest element X =Ω.
1 0 1 0 0
k
(cid:88)
I =I(X ;...;X ;P)=I(X )− X .I(X ;...;X ) (15)
k 1 k 1 i 1 i−1
i=2
We have the alternated sums or inclusion-exclusion rules [60, 89, 13]:
n
(cid:88) (cid:88)
H (X ,...,X ;P)= (−1)i−1 I (X ;P) (16)
n 1 n i I
i=1 I⊂[n];card(I)=i
n
(cid:88) (cid:88)
I (X ;...;X ;P)= (−1)i−1 H (X ;P) (17)
n 1 n i I
i=1 I⊂[n];card(I)=i
Forexample: H (X ,X ,X )=I (X )+I (X )+I (X )−I (X ;X )−I (X ;X )−I (X ;X )+I (X ;X ;X )
3 1 2 3 1 1 1 2 1 3 2 1 2 2 1 3 2 2 3 3 1 2 3
The chain rule of mutual-information goes together with the following inequalities discovered by Matsuda
[89]. For all random variables X ;..;X with associated joint probability distribution P we have:
1 k
• X .I(X ;..;X ;P)≥0 if and only if I(X ;..;X ;P)≥I(X ;..;X ;P) (in short: I ≥I )
k 1 k−1 1 k−1 1 k k−1 k
• X .I(X ;..;X ;P)<0 if and only if I(X ;..;X ;P)<I(X ;..;X ;P) (in short: I <I )
k 1 k−1 1 k−1 1 k k−1 k
thatfullycharacterizethephenomenonofinformationnegativityasanincreasingordivergingsequenceofmutual
information.
2.3 Information structures and coboundaries
This section justifies the choice of functions and algorithm, the topological nature of the data analysis and the
approximations we had to concede for the computation. In the general formulation of information cohomology,
the random variables are partitions of the atomic probabilities of a finite probability space (Ω,B,P) (e.g. all
their equivalence classes). The Joint-Variable (X ,X ) is the less fine partition that is finer than X and X ;
1 2 1 2
the whole lattice of partitions Π [6] corresponds to the lattice of joint random variables [48, 13]. Then, a general
information structureisdefinedtobethetriple(Ω,Π,P). Amoremodernandgeneralexpressionincategory
theory and topos is given in [13, 133]. (X ,...,X ;P) designates the image law of the probability P by the
1 k
measurable function of joint variables (X ,...,X ). Figure 1 gives a simple example of the lattice of partitions
1 k
for 4 atomic probabilities, with the simplicial sublattice used for data analysis. Atomic probabilities are also
illustrated in a Figure the associated paper [16].
Onthisgeneralinformationstructure,weconsidertherealmoduleofallmeasurablefunctionsF(X ,...,X ;P),
1 k
andtheconditioning-expectationbyY ofmeasurablefunctionsastheactionofY onthefunctionalmodule,noted
Y.F(X ,...,X ;P), such that it corresponds to the usual definition of conditional entropy (equ. 8). We define
1 k
6
Figure 1: Example of general and simplicial information structures. a, Example of lattice of random
variables(partitions): thelatticeofpartitionsofatomic-elementaryeventsforasamplespaceof4atomicelements
|Ω|=4(forexampletwocoinsandΩ={00,01,10,11}),eachelementbeingdenotedbyablackdotinthecircles
representing the random variables. The joint operation of Random Variables noted (X,Y) or X ⊗Y of two
partitions is the less fine partition Z that is finer than X and Y (Z divides Y and X, or Z is the greatest
common divisor of Y and X). It is represented by the coincidence of two edges of the lattices. The joint
operation has an identity element noted 1 = Ω (that we will note 0 thereafter), with X,1 = X,Ω = X and is
idempotent (X,X) = X2 = X. The structure is a partially ordered set (poset) with a refinement relation. b,
Illustration of the simplicial structure (sublattice) used for the data analysis ( |Ω|=4 as previously).
our complexes of measurable functions of random variables Xk = F(X ,...,X ;P), and the cochain complexes
1 k
(Xk,∂k) as :
0→− X0 − ∂ → 0 X1 − ∂ → 1 X2 − ∂ → 2 ...Xk−1 − ∂ − k − − → 1 Xk
where ∂k is the left action co-boundary that Hochschild proposed for associative and ring structures [57]. A
similar construction of random variable complex was given by Drumond-Cole, Park and Terilla [39, 40]. We
consider also the two other directly related cohomologies that are defined by considering a trivial left action [13]
and a symmetric (left and right) action [52, 139, 67] of conditioning:
• The left action Hochschild-information coboundary and cohomology (with trivial right action):
(∂k)F(X ;X ;...;X ;P)=X .F(X ;...;X ;P)
1 2 k+1 1 2 k+1
k
(cid:88)
+ (−1)iF(X ;X ;...;(X ,X );...;X ;P) (18)
1 2 i i+1 k+1
i=1
+(−1)k+1F(X ;...;X ;P)
1 k
Thiscoboundary,withatrivialrightaction,istheusualcoboundaryofGaloiscohomology([126],p.2),and
ingeneralitisthecoboundaryofhomologicalalgebraobtainedbyCartanandEilenberg[29]andMacLane
[82] (non homogenous bar complex).
• The ”topological-trivial” Hochschild-information coboundary and cohomology: considering a trivial left
action in the preceding setting , e.g. X .F(X ;...;X ) = F(X ;...;X ). It is the subset of the
1 2 k+1 2 k+1
preceding case, which is invariant under the action of conditioning. We obtain the topological coboundary
(∂k) [13]:
t
(∂k)F(X ;X ;...;X ;P)=F(X ;...;X ;P)
t 1 2 k+1 2 k+1
k
(cid:88)
+ (−1)iF(X ;X ;...;(X ,X );...;X ;P) (19)
1 2 i i+1 k+1
i=1
+(−1)k+1F(X ;...;X ;P)
1 k
• The symmetric Hochschild-information coboundary and cohomology: as introduced by Gerstenhaber and
Shack [52], Kassel [67] (p.13) and Weibel [139] (chap.9), we consider a symmetric (left and right) action of
conditioning, that is X .F(X ;...;X ) = F(X ;...;X ).X . The left action module is essentially the
1 2 k+1 2 k+1 1
same as considering a symmetric action bimodule [52, 67, 139]. We hence obtain the following symmetric
7
coboundary (∂k):
∗
(∂k)F(X ;X ;...;X ;P)=X .F(X ;...;X ;P)
∗ 1 2 k+1 1 2 k+1
k
(cid:88)
+ (−1)iF(X ;X ;...;(X ,X );...;X ;P) (20)
1 2 i i+1 k+1
i=1
+(−1)k+1X .F(X ;...;X ;P)
k+1 1 k
Based on these definitions, Baudot and Bennequin [13] computed the first homology class in the left action
Hochschild-information cohomology case and the coboundaries in higher degrees. We introduce here the sym-
metriccase,anddetailthehigherdegreecasesbydirectspecializationoftheco-boundariesformulas,suchthatit
appears that information functions and chain rules are homological by nature. For notation clarity, we omit the
probability in the writing of the functions, and when specifically stated replace their notation F by their usual
corresponding informational function notation H,I.
2.3.1 first degree (k=1)
For the first degree k =1, we have the following results:
• The left 1-co-boundary is (∂1)F(X ;X ) = X .F(X ) − F(X ,X ) + F(X ). The 1-cocycle condition
1 2 1 2 1 2 1
(∂1)F(X ;X ) = 0 gives F(X ,X ) = F(X )+X .F(X ), which is the chain rule of information shown
1 2 1 2 1 1 2
in equation 10. Then, following Kendall [69] and Lee [78], it is possible to recover the functional equation
of information and to characterize uniquely, up to the arbitrary multiplicative constant k, the entropy
(equation 1) as the first class of cohomology [13, 133]. This main theorem allows us to obtain the other
informationfunctionsinwhatfollows. MarcolliandThorngren[86],Leinster,FritzandBaez[11,9]obtained
independentlyananalogresultusingmeasure-preservingfunctionandcharacteristiconeWittconstruction,
respectively. In these various theoretical settings, this result extends to relative entropy [86, 11, 13], and
Tsallis entropies [86, 133].
• The topological 1-coboundary (∂1) is (∂1)F(X ;X ) = F(X )−F(X ,X )+F(X ), which corresponds
t t 1 2 2 1 2 1
to the definition of mutual information (∂1)F(X ;X ) = I(X ;X ) = H(X )+H(X )−H(X ,X ), and
t 1 2 1 2 1 2 1 2
hence I is a topological 1-coboundary.
2
• The symmetric 1-coboundary (∂1) is (∂1)F(X ;X ) = X .F(X )−F(X ,X )+X .F(X ), which cor-
∗ ∗ 1 2 1 2 1 2 2 1
responds to the negative of the pairwise mutual information (∂1)F(X ;X ) = X .H(X )+X .H(X )−
∗ 1 2 2 1 1 2
H(X ,X ) = −I(X ;X ), and hence −I is a symmetric 1-coboundary. Moreover, the 1-cocycle con-
1 2 1 2 2
dition (∂1)F(X ;X ) = 0 characterizes functions satisfying F(X ,X ) = X .F(X )+X .F(X ), which
∗ 1 2 1 2 2 1 1 2
corresponds to the information pseudo-metric discovered by Shannon [116], Rajski [110], Zurek [150] and
Bennett [21], and has further been applied for hierarchical clustering and finding categories in data by
Kraskov and Grassberger [73]: H(X (cid:52)X ) = X .H(X )+X .H(X ) = H(X ,X )−I(X ;X ). There-
1 2 2 1 1 2 1 2 1 2
fore, up to an arbitrary scalar multiplicative constant k, the information pseudo-metric H(X (cid:52)X ) is the
1 2
first class of symmetric cohomology. This pseudo metric is represented in the Figure . It generalizes to
pseudo k-volumes that we define by V =H −I (particularly interesting symmetric functions computed
k k k
by the provided software).
2.3.2 Second degree (k=2)
For the second degree k =2, we have the following results:
• Theleft2-co-boundaryis∂2F(X ;X ;X )=X .F(X ;X )−F((X ,X );X )+F(X ;(X ,X ))−F(X ;X ),
1 2 3 1 2 3 1 2 3 1 2 3 1 2
whichcorrespondstominusthe3-mutualinformation∂2F(X ;X ;X )=X .I(X ;X )−I((X ,X );X )+
1 2 3 1 2 3 1 2 3
I(X ;(X ,X ))−I(X ;X )=−I(X ;X ;X ), and hence −I is left 2-coboundary.
1 2 3 1 2 1 2 3 3
• The topological 2-coboundary is (∂2)F(X ;X ;X ) = F(X ;X )−F((X ,X );X )+F(X ;(X ,X ))−
t 1 2 3 2 3 1 2 3 1 2 3
F(X ;X ),whichcorrespondsininformationto∂2F(X ;X ;X )=I(X ;X )−I((X ,X );X )+I(X ;(X ,X ))−
1 2 t 1 2 3 2 3 1 2 3 1 2 3
I(X ;X )=0, and hence the topological 2-coboundary is always null-trivial.
1 2
• Thesymmetric2-coboundaryis(∂2)F(X ;X ;X )=X .F(X ;X )−F((X ,X );X )+F(X ;(X ,X ))−
∗ 1 2 3 1 2 3 1 2 3 1 2 3
X .F(X ;X ), which corresponds in information to ∂2F(X ;X ;X )=X .I(X ;X )−I((X ,X );X )+
3 1 2 ∗ 1 2 3 1 2 3 1 2 3
I(X ;(X ,X ))−X .I(X ;X )=0, and hence the symmetric 2-coboundary is always null-trivial.
1 2 3 3 1 2
8
2.3.3 Third degree (k=3)
For the third degree k =3, we have the following results:
• Theleft3-co-boundaryis∂3F(X ;X ;X ;X )=X .F(X ;X ;X )−F((X ,X );X ;X )+F(X ;(X ,X );X )−
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
F(X ;X ;(X ,X ))+F(X ;X ;X ),whichcorrespondsininformationto∂3F(X ;X ;X ;X )=X .I(X ;X ;X )−
1 2 3 4 1 2 3 1 2 3 4 1 2 3 4
I((X ,X );X ;X )+I(X ;(X ,X );X )−I(X ;X ;(X ,X ))+I(X ;X ;X ) = 0, and hence the left
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3
3-coboundary is always null-trivial.
• Thetopological3-coboundaryis∂3F(X ;X ;X ;X )=F(X ;X ;X )−F((X ,X );X ;X )+F(X ;(X ,X );X )−
t 1 2 3 4 2 3 4 1 2 3 4 1 2 3 4
F(X ;X ;(X ,X ))+F(X ;X ;X ),whichcorrespondsininformationto∂3F(X ;X ;X ;X )=I(X ;X ;X )−
1 2 3 4 1 2 3 t 1 2 3 4 2 3 4
I((X ,X );X ;X )+I(X ;(X ,X );X )−I(X ;X ;(X ,X ))+I(X ;X ;X )=I(X ;X ;X ;X ), and
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 1 2 3 4
hence I is a topological 3-coboundary.
4
• Thesymmetric3-coboundaryis(∂3)F(X ;X ;X ;X )=X .F(X ;X ;X )−F((X ,X );X ;X )+F(X ;(X ,X );X )−
∗ 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
F(X ;X ;(X ,X )) + X .F(X ;X ;X ), which corresponds in information to ∂3F(X ;X ;X ;X ) =
1 2 3 4 4 1 2 3 ∗ 1 2 3 4
X .I(X ;X ;X )−I((X ,X );X ;X )+I(X ;(X ,X );X )−I(X ;X ;(X ,X ))+X .I(X ;X ;X )=
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 4 1 2 3
−I(X ;X ;X ;X ), and hence −I is a symmetric 3-coboundary.
1 2 3 4 4
2.3.4 Higher degrees
Fork =4,weobtain∂4F(X ;X ;X ;X ;X )=−I ,and∂5F(X ;X ;X ;X ;X )=0,and∂5F(X ;X ;X ;X ;X )=
1 2 3 4 5 5 t 1 2 3 4 5 ∗ 1 2 3 4 5
0. For arbitrary k, the symmetric coboundaries are just the opposite of the topological coboundaries ∂k =−∂k.
t ∗
It is possible to generalize to arbitrary degrees [13] by remarking that we have:
• For even degrees 2k: we have I = −∂ I and then I = ∂ ∂∂ ...∂∂ H with 2k−1 boundary terms.
2k t 2k−1 2k t t t
In conclusion, we have:
∂2kF =−I and ∂2kF =−∂2kF =0 (21)
2k+1 ∗ t
• For odd degrees 2k+1: I = −∂I and then I = −∂∂ ∂...∂∂ H with 2k boundary terms. In
2k+1 2k−1 2k+1 t t
conclusion, we have:
∂2k−1F =0 and ∂2k−1F =−∂2kF =−I (22)
∗ t 2k
In [17, 16](theorem 1.2), we show that the mutual independence of n variables is equivalent to the vanishing
of all I functions for all 2 ≤ k ≤ n. As a probabilistic interpretation and conclusion, the information coho-
k
mology hence quantifies statistical dependences at all degrees and the obstruction to factorization. Moreover,
k-independencecoincideswithcocycles. Wethereforeexpectthatthehighercocyclesofinformation,conjectured
to be polylogarithmic forms [13, 43, 42], are characterized by the functional equations I = 0, and quantify
k
statistical k-independence.
3 Simplicial information cohomology
3.1 Simplicial substructures of information
The general information structure, relying on the information functions defined on the whole lattice of parti-
tions, encompasses all possible statistical dependences and relations, since by definition it considers all possible
equivalentclassesonaprobabilityspace. Onecouldhenceexpectthisgeneralstructuretoprovidethepromising
theoretical framework for classification tasks on data, and this is probably true in theory. However, this general
casehardlyallowsanyinterestingcomputationalinvestigationasitimpliesanexhaustiveexplorationofcomputa-
tionalcomplexityfollowingBell’scombinatoricinO(exp(exp(Nn)))fornN-aryvariables. Thisfactwasalready
remarked in the study of aggregation for Artificial Intelligence by Lamarche-Perrin and colleagues [75]. At each
order k, the number of k-joint-entropy and k-mutual-information to evaluate is given by Stirling numbers of the
second kind S(n,k) that sum to Bell number B , B =
(cid:80)n
S(n,k). For example, considering 16 variables
n n k=0
thatcantake8valueseach, wehave816 =248 ≈3.1014 atomicprobabilitiesandthepartitionlatticeofvariables
exhibits around
ee248
−1 ≥ 2200 elements to compute. Such computational reef can be decreased by considering
the sample-size m, which is the number of trials, repetitions or points that is used to effectively estimate the
empirical probability. It restricts the computation to O(exp(exp(m)), which remains insurmountable in practice
withourcurrentclassicalTuringmachines. Tocircumventthiscomputationalbarrier, dataanalysisisdeveloped
on the simplest and oldest subcase of Hochschild cohomology: the simplicial cohomology, which we hence call
the simplicial information cohomology and structure, and which corresponds to a subcase of cohomology and
structure introduced previously (see Figure 1b.). It corresponds to the example 1 and 4 in [13]. For simplicity,
we note also the simplicial information structure (Ω,∆n,P), ∆n = (X ,...,X ;P), as we will not come back to
1 n
the general setting. Joint (X ,X ) and meet (X ;X ) operations on random variables are the usual joint-union
1 2 1 2
9
and meet-intersection of Boolean algebra and define two opposite-dual monoids, generating freely the lattice of
all subsets and its dual. The combinatorics of the simplicial information structure follow binomial coefficients
and, foreachdegreek inaninformationstructureofnvariables, wehave (cid:0)n(cid:1) = n! elementsthatareinone
k k!(n−k!)
to one correspondence with the k-faces (the k-tuples) of the n-simplex of random variables (or its barycentric
subdivisions). It is a (simplicial) substructure of the general structure since any finite lattice is a sub-lattice of
the partition lattice [109]. This lattice embedding and the fact that simplicial cohomology is a special case of
Hochschild cohomology can also be inferred directly from their coboundary expression and has been explicitly
formalized in homology: notably, Gerstenhaber and Shack showed that a functor, noted Σ (cid:55)→ k !, induces an
Σ
isomorphismbetweensimplicialandHochschildcohomologyH•(Σ,k)∼ =H•(k !,k !)[51]. Asimplicialcomplex
Σ Σ
Xk = F(X ,...,X ;P) of measurable functions is any subcomplex of this simplex ∆n with k ≤ n, and any
1 k
simplicial complex can be realized as a subcomplex of a simplex (see Steenrod [119] p.296). The information
landscapes presented in Figure 3 illustrate an example of such a lattice/information structure. Moreover in this
ordinary homological structure, the degree obviously coincides with the dimension of the data space (the data
spaceisingeneralRn,thespaceof”co-ordinate”valuesofthevariables). Thishomological(algebraic,geometric
and combinatorial) restriction to the simplicial subcase can have some important statistical consequences. In
practice, whereastheconsiderationofthepartitionlatticeensuredthatnoreasonable(uptologicalequivalence)
statistical dependences could be missed (since all the possible equivalence classes on the atomic probabilities
were considered), the monoidal simplicial structure unavoidably misses some possible statistical dependences as
shown and exemplified by James and Crutchfield [61].
3.2 Topological Self and Free Energy of k-body interacting system - Poincar´e-
Shannon Machine
3.2.1 Topological Self and Free Energy of k-body interacting system
The basic idea behind the development of topological quantum field theories [8, 142, 115] was to define the
action and energy functionals on a purely topological ground, independently of any metric assumptions, and
to derive from this the correlation functions or partition functions. Here, in an elementary model for applied
purposes, we define, in the special case of classical and discrete probability, the k-mutual-information I (that
k
generalize the correlation functions to nonlinear relation [111]), as the contribution of the k-body interactions
to the energy functional. Some further observations support such a definition: i) as stated in [13] (Th.D), the
signed mutual-informations (−1)kI defining energy are sub-harmonic, a kind of weak convexity ii) in the next
k
sections, we define the paths of information and show that they are equivalent to the discrete symmetry group
iii) from the empirical point of view, Figure 5 shows that these energy functionals estimated on real data behave
as expected for usual k-body homogeneous formalism such as Van-Der-Walls model, or more refined Density
Functional Theory (DFT) [58, 70]. These definitions, given in the context of simplicial structures, generalize to
thecaseofpartitionslattice,andaltogetherprovidetheusualthermodynamicalandmachine-learningexpressions
and interpretation of mutual-information quantities: some new methods free of metric assumptions. There are
two qualitatively and formally different components in the I :
k
• Self-internal information energy (definition): fork =1,I andtheirsuminaninformationstructure
1
(cid:80)
expressed in equation 16, namely I (X ;P), are a self-interaction component, since it
T⊂[n];card(T)=1 1 T
sums over marginal information-entropy I (X ) = H (X ). We call the first dimension mutual informa-
1 i 1 i
tion component U(X ,...,X ;P ) the self information or internal information energy, in analogy to usual
1 n N
statistical physics and notably DFT:
n
(cid:88)
U(X ,...,X ;P )= I (X ;P ) (23)
1 n N 1 i N
i=1
Note that in the present context, which is discrete and where the interactions do not depend on a metric,
the self-interaction does not diverge, which is a usual problem with metric continuous formalism and was
the original motivation for regularization and renormalization infinite corrections, considered by Feynman
and Dirac as the mathematical default of the formalism [47, 37].
• k-free-energy and total-free-energy (definition): for k ≥2, (−1)kI and their sum in an information
k
structure (equation 16) quantify the contribution of the k-body interactions. We call the kth dimension
mutual information component (−1)kI , given in equation 5, the k-free-information-energy. We call the
k
(cumulative) sum over dimensions of these k-free-information-energies starting at pairwise interactions
(dimension 2), the total n-free-information-energy, and note it G(X ,...,X ;P ):
1 n N
n
(cid:88) (cid:88)
G(X ,...,X ;P )= (−1)i−1 I (X ;P )=C (X ;...X ;P ) (24)
1 n N i I N n 1 n N
i=2 I⊂[n];card(I)=i
10
The total free-energy is the total correlation (equation 7) introduced by Watanabe in 1960 [138] that
quantifies statistical dependence in the work of Studeny´ and Vejnarova [121] and Margolin and colleagues
[87],andamongotherexamplesconsciousnessintheworkofTononiandEdelman[130]. Inagreementwith
the results of Baez and Pollard in their study of biological dynamics using out-of-equilibrium formalism
[10], and the appendix of the companion paper on Bayes Free Energy [16] the total free-energy is a relative
entropy. Theconsiderationthatfreeenergyisthepeculiarcaseoftotalcorrelationwithinthesetofrelative
entropies,accountsforthefactthatthefreeenergyshallbeasymmetricfunctionofthevariablesassociated
tothevariousbodies,e.gf(X;Y)=f(Y;X)inthepairwiseinteractioncase. Moreover,whereasI energy
k
component can be negative, the G total energy component is always non-negative. Each (−1)kI term in
k k
the free energy can be understood as a free energy correction accounting for the k-body interactions.
Entropy is given by the alternated sums of information (equation 16), which then read as the usual isotherm
thermodynamic relation:
H (X ,...,X ;P )=U(X ,...,X ;P )−G(X ,...,X ;P ) (25)
n 1 n N 1 n N 1 n N
ThisinformationtheoreticformulationofthermodynamicrelationfollowsJaynes[63,62],Landauer[77],Wheeler
[141], and Bennett’s[20] original work, and is general in the sense that it is finite and discrete, and holds inde-
pendently of the assumption of the system being in equilibrium or not, i.e. for whatever finite probability. In
more probabilistic terms, it does not assume that the variables are identically distributed, a condition that is
required for the application of classical central limit theorems (CLT) to obtain the normal distributions in the
asymptotic limit [136]. In the special case where one postulates that the probability follows the equilibrium
Gibbs distribution, which is also the maximum entropy distribution [41, 34], the expression of the joint-entropy
(k = −1/ln2) allows to recover the equilibrium fundamental relation, as usually achieved in statistical physics
(see Adami and Cerf [3] and Kapranov [66] for more details). Explicitly, let’s consider the Gibb’s distribution:
1
p(X
1
=x
1
,...,X
n
=x
n
)=p
ij...n
=
Z
e−βEij...n/kBT (26)
(cid:124)(cid:123)(cid:122)(cid:125)
nindices
where E is the energy of the elementary-atomic probability p , k is Boltzmann constant, T the tem-
ij...n ij...n B
perature and Z = (cid:80)N
i,j
i
,
.
.
N
.,n
j...Nne−Eij...n/kBT is the partition function, such that (cid:80)
i
N
,j
i
,
.
.
N
.,n
j...Nnp
ij...n
= 1. Since
H(X ,...,X )=k
(cid:80)Ni.Nj...Nnp
lnp , equals the thermodynamic entropy function S up to the arbitrary
1 n i,j,..,n ij...n ij...n
Landauer constant factor k ln2, S =k ln2H(X ,...,X ), the entropy for Gibbs distribution gives:
B B 1 n
N1.N (cid:88)2...Nn N1.N (cid:88)2...Nn
H(X ,...,X )/k = p E /k T + p lnZ =((cid:104)E(cid:105)−G)/k T (27)
1 n ij...n ij...n B ij...n B
i,j,..,n i,j,..,n
, which gives the expected thermodynamical relation:
k T ln2.H(X ,...,X )=(cid:104)E(cid:105)−G=U −G (28)
B 1 n
, where G is the free-energy G=−k T lnZ.
B
In the general case of arbitrary random variables (not necessarily iid) and discrete probability space, the identi-
fication of marginal informations with internal energy:
(cid:88)
n N1.N (cid:88)2...Nn
H(X )= p E (29)
k ij...n ij...n
k=1 i,j,..,n
implies by direct algebraic calculus that:
N1.N (cid:88)2...Nn N1.N (cid:88)2...Nn (cid:32)
(cid:89)
n (cid:33)
p E =− p ln p (30)
ij...n ij...n ij...n ••...k...•
i,j,..,n i,j,..,n k=i
,wherethemarginalprobabilityp isthesumoverallprobabilitiesforwhichX =x . Itishencetempting
••...k...• k k
to identify the elementary-atomic energies E with the elementary marginal informations lnp . This
ij...n ••...k...•
is achieved uniquely by considering that such an elementary energy function must satisfy the additivity axiom
(extensivity): (E(X =x ,Xj =x )=E =E =E +E ), which is the functional equation of the logarithm.
i i j i,j ij i j
TheoriginalproofgoesbackatleasttoKepler,anelementaryversionwasgivenbyErdos[45],andinInformation
theorytermscanbefoundintheproofsofuniquenessof”singleeventinformationfunction”byAczelandDarokzy
([2], p.3). It establishes the following proposition:
11
Theorem3.1. Givenasimplicialinformationstructure, theelementaryenergiessatisfyingtheextensivityaxiom
are the functions:
n
(cid:88)
E =k lnp (31)
ij...n ••...k...•
k=i
, where k is an arbitrary constant settled to k =−1/ln2 for units in bit.
The geometric meaning of these elementary energies as log of marginal elementary probability volumes (lo-
cally Euclidean) is illustrated in Figure 2 and further underlines that I are volume corrections accounting
k,k≥2
for the statistical dependences among marginal variables.
Examples: i) in the example of 3 binary random variables (n = 3,N = N = N = 2, three vari-
1 2 3
Figure 2: Elementary energy as logarithm of locally Euclidean probability volumes. Example of an
elementary energy E associated to a probability p (n = 3 variables). The histograms of the marginal
ijk ijk
distributions of each variable are plotted beside the axes.
ables of Bernoulli) illustrated in the Figure of the associated paper [16], we have E = −ln(p p p ),
000 0•• •0• ••0
E = −ln(p +p +p +p )−ln(p +p +p +p )−ln(p +p +p +p ) and in the
000 000 010 001 011 000 100 001 101 000 100 010 110
configuration of negative-entangled-Borromean information of the Figure of the associated paper [16], we obtain
E = 3 in bit units, and similarly E = E = E = E = E = E = 3, and we hence recover
000 001 010 011 101 110 111
U =
(cid:80)8
p E =
(cid:80)3
H(X ) = 3 bits. Note that the limit 0ln0 ∼ 0 avoids singularity of elementary
i,j,k ijk ijk i=1 i
energies.
ii) in the special case of identically distributed variables, p = p , we have E = nklnp
••...k...• •...j...•• ij...n ••...k...•
and hence the marginal Gibbs distribution: p ••...k...• =e Ei n j. k ..n .
iii) for independent identically distributed variable (non-interacting), we have G =0, and hence:
n
H (X ,...,X ;P )=U(X ,...,X ;P )=nH(X ) (32)
n 1 n N 1 n N i
iv) considering the variables to be the 6n variables of the phase space, with one variable of position and one
variable of momentum per body (noted (X1,X2,X3,P1,P2,P3) for the kth body), it is possible to re-express
k k k k k k
the semi-classical formalism according to which the entropy formulation is (Landau and Lifshitz [76],p.22):
(cid:18) (cid:19)
∆X∆P
H (X1,X2,X3,P1,P2,P3,...,P3;P )=log (33)
6n 1 1 1 1 1 1 n N (2π(cid:126))6n
It is achieved by identifying the internal and free energy as following:
(cid:104)E(cid:105)=−6nlog(2π(cid:126)) (34)
G=−log(∆X∆P) (35)
This identifies the elementary volumes/probabilities with the Planck constant, the quantum of action (the con-
sistency in the units is realized in section 3.4.3 by the introduction of time). The quantum of action can be
illustratedbyconsideringintheFigure2thatitisthesurfaceofthesquare/rectanglefortwoconjugatevariables
(considered as position and momentum). In this setting, ∆X∆P quantifies the non-extensivity of the volume in
the phase-space due to interactions, or in other words, the mutual-informations account for the consideration of
the dependence of the subsystems considered as opened and exchanging energy. As noted by Baez and Pollard,
the relative entropy provides a quantitative measure of how far from equilibrium the whole system is [10]. The
basic principle of such expression of information theory in physics is known at least since Jaynes’s work [62, 64].
As a conclusion, information topology applies, without imposing metric or symplectic or contact structures,
to physical formalism of n-body interacting systems relying on empirical measures. Considering the 3n or 6n
dimensions (degrees of freedom) of a configuration or a phase space as random variables, it is possible to recover
12
the (semi) classical statistical physics formalism. It is also interesting to discuss the status of the analog of
the temperature variable in the present formalism which is played by the graining, which is the size N of the
i
alphabet of a variable X . In usual thermodynamic we have H(Xn;P ) = T.S(Xn), and to stay consistent,
i N
temperature shall be a functional inverse of the graining N, lowest temperature being the finest grain (large N),
highest temperature being the coarsest graining (small N).
3.3 k-Entropy and k-Information landscapes
Information landscapes (definition):
Theinformationlandscapesarearepresentationofthe(semi)-latticeofinformationstructureswhereeachelement
is represented as a function of its corresponding value of entropy or mutual information. In abscissa are the
dimensions k and in ordinate the values of information functions of a given subset of k variables.
In data science terms, these landscapes provide a visualization of the potentially high-dimensional structure
Figure3: Entropy and information landscapes. a, illustrationoftheprincipleofentropyH landscapeand
k
b, of a mutual-information I landscape for n = 4 random variables. The lattice of the simplicial information
k
structure is depicted with grey lines.Theoretical examples of entropy and information landscapes. c,d,
H and I landscapes for n independent identically distributed variables. The degeneracy of H and I values
k k k k
is represented by a color code: the number of k-tuples having the same information value. e,f H and I
k k
landscapes for n fully redundant variables. Such variables are equivalent from the information point of view,
they are identically distributed and fully dependent.
of the data points. In information theory terms, it provides a representation of Shannon’s work on lattice [116]
further developed by Han [56]. H and I , as real continuous functions, provide a ranking of the lattices at
k k
each dimension k. It is the ranking, i.e. the relative values of information, which matters and comes out of the
homological approach, rather than the absolute values. The principle of H and I landscapes is illustrated in
k k
Figure 3 for n = 4. H and I analyse quantify the variability-randomness and statistical dependences at all
k k
dimensions k, respectively, from 1 to n, n being the total number of variables under study. The H landscape
k
represents the values of joint entropy for all k-tuples of variables as a function of the dimensions k, the number
of variables in the k-tuple, together with the associated edges-paths of the lattice (in grey). The I landscape
k
represents the values of mutual-information for all k-tuples of variables as a function of the dimension k, which
is the number of variables in the k-tuple. Figure 3 gives two theoretical extremal examples of such landscapes,
one for independent identically distributed variables (totally disordered) and one for fully dependent identically
distributed variables (totally ordered). The degeneracy of H and I values is given by the binomial coefficient
k k
(color code in Figure 3), hence allowing to derive the normal exact expression of the information landscapes
in the assymptotic infinite dimensional limit (n → ∞) by application of Laplace-Lemoivre theorem. These are
theoreticalextremalexamples: H andI landscapeseffectivelycomputedandestimatedonbiologicaldatawith
k k
finite sample are shown in the paper [16, 17, 125], and in practice the finite sample size (m) may impose some
bounds on the landscapes.
3.4 Information Paths and Minimum Free Energy Complex
Inthissectionweestablishthatinformationlandscapesandpathsencodedirectlythebasicequalities,inequalities
and functions of information theory and allow us to obtain the minimum free energy complex that we estimate
on data.
13
3.4.1 Information Paths (definition)
On the discrete simplicial information lattice ∆ , we define a path of degree k as a sequence of edges of the
k
lattice that begins at the least element of the lattice (the identity-constant ”0”), travels along edges from vertex
tovertexofincreasingdimensionandendsatthegreatestelementofthelatticeofdimensionk. Informationpaths
are defined on both joint-entropy and meet-mutual information semi-lattices, and the usual joint-entropy and
mutual-information functions are defined on each element of such paths. Entropy path and information path of
degreek arenotedHP andIP ,respectively,andthesetofallinformationpathsisnotedHP ={HP }
k k k i i∈1,...,k!
for the entropy paths, and IP ={IP } for the mutual-information paths. We have the theorem:
k i i∈1,...,k!
Theorem 3.2. The two sets of all information paths HP and IP in the simplicial information structure ∆
k k k
are both in bijection with the symmetric group S . Notably, there are k! information paths in ∆ .
k k
Proof: bysimpleenumeration, anedgeofdimensionmconnectsk−medgesofdimensionm+1, thenumber
of paths is hence (k−0).(k−1)....(k−k+2).(k−k+1)=k!, hence the conclusion (cid:3).
A given path can be identified with a permutation or a total order by extracting the missing variable in
a previous node when increasing the dimension, for example the mutual-information path in ∆ : IP = 0 →
4 i
(0,X )→(0,X ,X )→(X ,X ,X )→(0,X ,X ,X ,X ) can be noted as the permutation σ:
2 1 2 1 2 4 1 2 3 4
(cid:18) (cid:19)
0 1 2 3 4 σ
or (01234)−→(02143) (36)
0 2 1 4 3
We note an information path with arrows, giving for the previous example IP =(0→X →X →X →X ).
i 2 1 4 3
These paths shall be seen as the automorphisms of {1,2.....k} = [k] and the space of entropy and mutual
information paths can be endowed with the structure of two opposite symmetric groups S and Sopp. The
k k
equivalence of the set of paths and symmetric group only holds for the subcase of simplicial structures, and the
information paths in the lattice of partition are obviously much richer. More precisely, the subset of simplicial
informationpathsinthelatticeofpartitionscorrespondstotheautomorphismsofthelattice. Itisknownthatthe
finite symmetric group is the automorphism group of the finite partition lattice [23]. The geometrical realization
of information paths IP and HP consists in two dual permutohedron (see Postnikov [108]), and gives the
k k
informational version of the work of Matu´ˇs on conditional probability and permutohedron [90].
3.4.2 Derivatives, inequalities and conditional mutual information negativity
Derivatives of information paths: In the information landscapes, the paths HP and IP are piecewise
i i
linearfunctionsIP (k)withIP (k)=I whereI isthemutual-informationofthek-tupleofvariablespertaining
i i k k
to the path IP . We define the first derivatives of the paths for both entropy and mutual information structures
i
as piecewise linear functions:
Firstderivativeofentropypath: thefirstderivativeofanentropypathHP (k)istheconditionalinformation
i
(X ,...,X ).H(X ;P):
1 k−1 k
dHP (k)
i =H(X ,...,X ;P)−H(X ,...,X ;P)=(X ,...,X ).H(X ;P) (37)
dk 1 k 1 k−1 1 k−1 k
This derivative is illustrated in the graph of Figure 4a. It implements the chain rule of entropy H −H =
k+1 k
(X
1
;...;X(cid:99)i ;...;X
k+1
).H(X
i
) (equation 12), and in homology provides a diagram where conditional entropy is a
simplicial coface map (X
1
;...;X(cid:99)i ;...;X
k+1
).H(X
i
)=di : Xk →Xk+1, as a simplicial special case of Hochschild
coboundaries 2.3.
First derivative of mutual information path: the first derivative of an information path IP (k) is minus
i
the conditional information (X ).I(X ,...,X ;P):
k 1 k−1
dIP (k)
i =I(X ,...,X ;P)−I(X ,...,X ;P)=−X .I(X ,...,X ;P) (38)
dk 1 k 1 k−1 k 1 k−1
This derivative is illustrated in the graph of Figure 4b. It implements the chain rule of mutual-information
I −I = X .I (equation 13), and in homology provides a diagram where minus the conditional mutual-
k−1 k k k−1
information is a simplicial coface map X
i
.I(X
1
;...;X(cid:99)i ;...;X
k+1
) = di : Xk → Xk+1, introduced in section
2.3.
Bounds of the derivatives and information inequalities The slope of entropy paths is bounded by
the usual conditional entropy bounds ([147] p.27-28). Its minimum is 0 and is achieved in the case where
X is a deterministic function of (X ,...,X ) (lower dashed red line in Figure 4a). Its global upperbound is
k+1 1 k
maxH
k+1
=k.ln(N
1
...N
k+1
) and its sharp bound given by (X
1
;...;X(cid:99)i ;...;X
k+1
).H(X
i
)≤H(X
i
) is achieved in
thecasewhereX isindependentofX ,...,X (wehaveH =H +H(X )(higherdashedredlineinFigure
k+1 1 k k+1 k k+1
14
Figure 4: Entropy and information paths. Illustration of an entropy path HP = 0 → 1 → 4 → 2 → 3 (a)
i
and of a mutual information path IP =0→1→4→3→2 (b) for n=4 random variables (see text).
i
4a). Hence, any entropy path lies in the (convex) entropy cone defined by the 3 points labeled H , minH
k k+1
and maxH : the 3 vertices of the cone depicted as a red surface in Figure 4a and called the Shannonian Cone
k+1
following Yeung’s seminal work [148]. The behavior of a mutual-information path and the bounds of its slope
are richer and more complex than the preceding conditional entropy:
• For k =2, the conditional information is the conditional entropy X .I(X )=X .H(X ) and has the same
i j i j
usual bounds 0≤X .I(X )≤I(X ).
i j j
• Fork =3theconditionalmutual-informationX .I(X ;X )isalwayspositiveornullX .I(X ;X )≥0and
i j h i j h
henceI ≥I ([147]p.26,theoppositeofth.2.40p.30),whereasthehigherlimitisgivenbyX .I(X ;X )≥
2 3 i j h
min(X .H(X ),X .H(X )) ([89] th.2.17), with equality iff X and X are conditionally independent given
i j i h j h
X , and implying that the slope from k =2 to k =3 increases in the I landscape.
i k
• For k > 3, X .I(X ;..;X ) can be negative as a consequence of the preceding inequalities. In terms of
k 1 k−1
informationlandscapethisnegativitymeansthattheslopeispositive, hencethattheinformationpathhas
crossed a critical point, a minimum. As expressed by theorem 2.2, X .I(X ;..;X ) < 0 iff I < I .
k 1 k−1 k k+1
The minima correspond to zeros of conditional information (conditional independence) and hence detect
cocycles in the data. The results on information inequalities define as ”Shannonian” [145, 149, 91] the
set of inequalities that are obtained from conditional information positivity (X .I(X ;X ) ≥ 0) by linear
i j h
combination, which forms a convex ”positive” cone after closure. ”Non-Shannonian” inequalities could
also be exhibited [145][149], hence defining a new convex cone that includes and is strictly larger than
the Shannonian set. Following Yeung’s nomenclature and to underline the relation with his work, we call
the positive conditional mutual-information cone (surface colored in red in Figure 4b) the ”Shannonian”
cone and the negative conditional mutual-information cone (surface colored in blue in figure 4b) the ”non-
Shannonian” cone.
3.4.3 Information paths are random processes: topological 2nd law of thermodynamic and en-
tropy rate
Herewepresentthedynamicalaspectsofinformationstructures. Informationpathsprovidedirectlythestandard
definition of a stochastic process and it imposes how the time arrow appears in the homological framework, how
time series can be analyzed, how entropy rates can be defined (etc.).
Random (stochastic) process (definition [124]): A random process {X ,t ∈ T} is a collection of random
t
variables on the same probability space (Ω,B,P) and the index set T is a totally ordered set.
Astochasticprocessisacollectionofrandomvariablesindexedbytime,theprobabilisticversionofatimeseries.
Considering each symbol of a time series as a random variable, the definition of a random-stochastic process
corresponds to the unique information paths HP and IP which total order is the time order of the series. We
i i
have the following lemma:
Lemma 1. (Stochastic process and information paths): Let (Ω,∆k,P) be a simplicial information structure,
then the set of entropy paths HP and of mutual-information paths IP are in one to one correspondence with
k k
the set of stochastic processes {X ,t∈T,|T|=k}.
t
Proof: direct from the definitions (cid:3).
As we previously stated, these paths are also automorphisms of {1,2.....k} = [k]. We obtain immediately the
topological version of the second law of thermodynamic, which improves the result of Cover [35]:
Theorem 3.3. (Stochastic process and information paths): Let (Ω,∆k,P) be a simplicial information structure,
then the entropy of a stochastic process can only increase with time.
15
Proof: giventhecorrespondencewejustestablished,thestatementisequivalenttoH(X ,...,X )≥H(X ,...,X ),
1 k 1 k−1
whichisadirectconsequenceofconditionalentropypositivityandthechainruleofinformationwithk =−1/ln2.
The generalization with respect to the stationary Markov condition used by Cover comes from the remark that
in any case the indexing set of the variable is a total order. Note that the homological formalism imposes an
”initial” minimally low entropy state H(0) = I(0) = 0 (a usual assumption in physics), the constant and zero
degree homology, which has to have at least 1 component to talk about the cohomology (cid:3).
Remark: the meaning of this theorem in common terms was summarized by Gabor and Brillouin: ”you can’t
have something for nothing, not even an observation” [26]. This increase in entropy is illustrated in Figure a.
The usual stochastic approach of time series assumes a Markov chain structure, imposing peculiar statistical de-
pendencesthatrestrictmemoryeffects(cf. theinformationalcharacterizationofMarkovchainsintheassociated
paper[16]). Theconsiderationofstochasticprocesseswithoutrestrictionallowsanykindofdependencesandar-
bitrarylonghistoricaland”nontrivial”memory. Fromthebiologicalpointofviewitformalizesthephenomenon
of arbitrary long-lasting memory. From the physical point of view, without proof, such a framework appears as
a classical analog of the consistent or decoherent histories developed notably by Griffiths [55], Omnes [103], and
Gell-Mann and Hartle [50]. The information structures impose a stronger constraint of a totally ordered set (or
more generally a weak ordering) than the preorder imposed by Lieb and Yngvason [79] to derive the second law.
It is also interesting to note that even in this classical probability framework, the entropy cone (the topological
cone depicted in Figure 4a) imposed by information inequalities, when considered with this time ordering, is
a time-like cone (much-like the special relativity cone), but with the arguably remarkable fact that we did not
introduce any metric.
The stochastic process definition allows to define the finite and asymptotic information rate: the finite informa-
tion rate r of an information path HP
i
is r = H
k
k. The asymptotic information rate r of an information path
HP
i
isr =lim
k→∞
H
k
k. Itrequiresthegeneralizationofthepresentformalismtotheinfinitedimensionalsetting
or infinite information structures, which is not trivial and will be investigated in further work.
3.4.4 Local minima and critical dimension
Thederivativeofinformationpathsallowstoestablishthelemmaonwhichisbasedtheinformationpathanalysis.
A critical point is said to be non-trivial if at this point the sign of the derivative of the path, i.e. the conditional
information, changes.
Lemma 2. (local minima of information paths): if X .I(X ;..;X ) < 0 then all paths from 0 to I
k 1 k−1 k
passing by I have at least one local minimum. In order for an information path to have a non-trivial critical
k−1
point, it is necessary that k >3, the smallest possible dimension of a critical point being k =3.
Proof: it is a direct consequence of the definitions of paths and of conditional 2-mutual-information X .I
k 2
positivity (X .I ≥0, cf. theorem 3.4.2)(cid:3).
k 2
Notethatbydefinitionalocalminimumcanbeaglobalminimum. Wewillcall,ifitexists,thedimensionkof
the first local minimum of an information path the first informational critical dimension of the information
path IP , and note it k . This allows us to define maximal information paths:
i i1
Positive information path (definition): Apositiveinformationpathisaninformationpathfrom0toagiven
I corresponding to a given k-tuple of variables such that I <I <...<I .
k k k−1 1
Maximal Positive information path (definition): A maximal positive information path is a positive infor-
mation path of maximal length. More formally, a maximal positive information path is a positive information
path that is not a proper subset of positive information paths.
The definitions make coincide positive information paths and maximal positive information path with chains
(faces) and maximal chains (facets), respectively. The maximal positive information path stops at the first local
minimum of an information path, if it exists. The first informational critical dimension k of a time series IP ,
i1 i
whenever it exists, gives a quantification of the duration of the memory of the system.
3.4.5 Sum over paths and mean information path
As previously, for k =1, IP (1) can be identified with the self-internal energy and for k ≥2, IP (k) corresponds
i i
to the k-free-energy of a single path IP . The chain rule of mutual information (equation 15) and the derivative
i
of an IP path (equation 37) implies that the k-free-energy can be obtained from a single path:
i
k k
I =I(X ;...;X ;P)=I(X )− (cid:88) X .I(X ;...;X )=IP (1)+ (cid:88)dIP i (j) (39)
k 1 k 1 i 1 i−1 i dj
i=2 j=2
16
Hence, the global thermodynamical relation 25 can be understood as the sum over all paths, the sum over
informationalhistories: theclassical,discreteandinformationalversionofthepathintegralsinstatisticalphysics
[46]. Indeedconsideringaninverserelationbetweentimeanddimensiont= 1 intheprobabilityexpression3.2.1
n
foriidprocessesgivestheusualexpressionofaunitaryevolutionoperatorp ••...k...• =e t.Ei k j...n. Free-information-
energy integrates over the simplicial structure of the whole lattice of partitions over degrees k ≥2, which further
justifies its free energy name.
In order to obtain a single state function instead of a group of k! paths-functions, we can compute the mean
behavior of the information structure, which is achieved by defining the mean H and I , noted (cid:104)H (cid:105) and (cid:104)I (cid:105):
k k k k
(cid:80)
H (X ;P)
T⊂[n];card(T)=k k T
(cid:104)H (cid:105)= (40)
k (cid:0)n(cid:1)
k
and
(cid:80)
I (X ;P)
T⊂[n];card(T)=k k T
(cid:104)I (cid:105)= (41)
k (cid:0)n(cid:1)
k
For example, considering n=3, then (cid:104)I (cid:105)= I(X1;X2)+I(X1;X3)+I(X2;X3). This defines the mean mutual informa-
2 3
tion path and a mean entropy path noted (cid:104)HP(cid:105)(k) and (cid:104)IP(cid:105)(k) in the information landscape. The case k =2 of
those functions introduced in [17], is studied in Merkh and Montu´far [94] with a characterization of the degen-
eracy of their maxima and are called Factorized Mutual-Information. As previously, (cid:104)IP(cid:105)(1) can be identified
with the mean self-internal energy U(Xn ;P ) and for k > 1 (cid:104)IP(cid:105)(k) to the mean k-free-information-energy
hom N
G(Xn ;P ), giving the usual isotherm relation:
hom N
H(Xn ;P )=U(Xn ;P )−G(Xn ;P ) (42)
hom N hom N hom N
Figure 5: Example of mean entropy and information paths of gene expression. a, Mean entropy path
(cid:104)H (cid:105) for the 21 genes of interest for population A (green line) and population B neurons (red line). b, Mean
k
information path (cid:104)I (cid:105) for the same pool of genes. c, Mean information path (cid:104)I (cid:105) for the rest of 20 genes (”non
k k
relevant”). The undersampling dimension introduced in the associated paper [16] is depicted with arrows.
The computation of the mean paths corresponds to an idealized information structure Xn for which all
hom
the variables would be identically distributed, would have the same entropy, and would share the same mutual
information I at each dimension k: a homogeneous information structure, with homogeneous high-dimension
k
k-body interactions. Like usually achieved in physics notably in mean-field theory (for example Weiss [140] or
Hartree), it aims to provide a single function summarizing the average behavior of the system (we will see that
in practice it misses the important biological structures, pointing out the constitutive heterogeneity of biological
systems see 4.1). Using the same dataset and results presented in [16, 17, 125], the (cid:104)IP(cid:105)(k) paths estimated
on genetic expression data set are shown for two population A and population B of neurons in Figure 5. We
quantifiedthegeneexpressionlevelsfor41genesintwopopulationsofcells(AorB)aspresentedin[16,17,125].
We estimated H and I landscapes for these two populations and for two sets of genes (”genes of interest” and
k k
”nonrelevant”)accordingtothecomputationalandestimationmethodspresentedin[16,17,125]. Theavailable
computational power restricts the analysis to a maximum of n = 21 variables (or 21 dimensions), and imposed
us to divide the genes between the two classes ”genes of interest” and ”non relevant”. The 21 genes of interest
were selected within the 41 quantified genes according to their known specific involvement in the function of
population A cells.
Figure 5 exhibits the critical phenomenon usually encountered in condensed matter physics, like the example of
Van-der-Walls interactions [106]. Like any I path, (cid:104)IP(cid:105)(k) can have a first minimum with a critical dimension
k
k that could be called the homogeneous critical dimension. For the 21 genes of interest (whose expression
i1
levels, given the literature, are expected to be linked in these cell types) the (cid:104)I (cid:105) path exhibits a clear minimum
k
at the critical dimension k = 4 for population A neurons and k = 5 population B neurons, reproducing the
i1 i1
usual free-energy potential in the condensed phase for which n-body interactions are non-negligible. For the 20
17
othergenes,lessexpectedtoberelatedinthesecelltypes,the(cid:104)I (cid:105)pathexhibitsamonotonicdecreasewithouta
k
non-trivial minimum, which corresponds to the usual free-energy potential in the uncondensed-disordered phase
for which the n-body interactions are negligible. Indeed, as shown in the work of Xie and colleagues [144], the
tensor network renormalization approach of n-body interacting quantum systems gives rise to an expression of
the free-energy as a function of the dimension of the interactions, in the same way than achieved here.
3.4.6 Minimum free energy complex
The analysis of information paths that we now propose aims to determine all the first critical points of infor-
mation paths, in other words to determine all the information paths for which conditional information stays
positive, and all first local minima of the information landscape, that can also be interpreted as a conditional
independence criterion. Such an exhaustive characterization would give a good description of the landscape and
of the complexity of the measured system. The qualitative reason for considering only the first extrema for the
dataanalysisisthat,beyondthatpoint,mutualinformationdiverges(assection3.4.4explains)andthemaximal
positive information paths correspond to stable functional modules in the application to data (gene expression).
A more mathematical justification is that they define the facets of a complex in our simplicial structure, which
we will call the minimum energy complex of our information structure, underlining that this complex is the
formalization of the minimum free energy principle in a degenerate case.
We now obtain the theorem that our information path analysis aims to characterize empirically:
Theorem 3.4. (Minimum free energy complex): the set of all maximal positive information paths forms
a simplicial complex that we call the minimum free energy complex. Moreover, the dimension-degree of the
minimum free energy complex is the maximum of all the first informational critical dimensions (d = maxk ),
i1
if it exists, or the dimension of the whole simplicial structure n. The minimum free energy complex is noted
X+d. A necessary condition for this complex not to be a simplex is that its dimension is greater or equal to four
(d≥4).
Proof: It is known that there is a one to one correspondence between simplicial complexes and their set of
maximal chains (facets) (see [123] p.95 for example). The last part follows from Lemma 1. (cid:3).
In simple words, the maximal faces, e.g. the maximal positive information paths, encode all the structures of
the minimum free energy complex. Figure 6 illustrates one of the simplest examples of a minimum free energy
complex that is not a simplex, of dimension four in a five-dimensional simplicial structure of information ∆ .
5
Figure6: Example of maximal I paths in an I landscape for n=5 together with its corresponding
k k
minimum free information energy complex. a, maximal I paths in an I landscape for n = 5. The
k k
maximum positive information paths are depicted in red, for example the paths 1 → 2 → 3 → 4 but also
4 → 3 → 2 → 1, 3 → 4 → 5, and 1 → 2 → 5 are maximum positive information paths, that is facets/maximal
chains. The facet 1 → 2 → 3 → 4 is a 3-simplex while 3 → 4 → 5 is a 2-simplex with critical dimension
k = 3. The usual dimension of the simplex is used here, but we could have augmented it by one, since
3→4→5
we added the constant element ”0” to the algebra (pointed space), such that the usual simplicial dimension
and the critical dimension correspond. The maximal critical dimension of the positive information paths is
the dimension of the complex and hence d(X+k) = d(1 → 2 → 3 → 4) = 4. b, The minimum free energy
complex corresponding to the preceding maximal i paths. It is a subcomplex of the 4-simplex also called
k
the 5-cell with only one 4 dimensional cell among the five depicted as the bottom tetrahedron {1234} with
darker red volume. It has 5 vertices, 10 edges, 10 2-faces and one 3-face (cell), hence its Euler characteristic
is χ(X+k) = 5−10+10−1 = 4 and its minimum free energy characteristic characteristic is: H+k(X+k) =
(cid:80)5
I(X )−
(cid:80)10
I(X ;X )+
(cid:80)10
I(X ;X ;X )−I(X ;X ;X ;X4)
Xi∈X+k i (Xi;Xj)∈X+k i j (Xi;Xj;Xh)∈X+k i j h 1 2 3
18
We define the minimum free energy characteristic as:
k
(cid:88) (cid:88)
H+k(X+k;P)= (−1)i−1 I (X ;P) (43)
i I
i=1 I⊂X+;card(I)=i
, where the component with dimension higher than one is a free energy. In the example of Figure 6 it gives:
5 10
(cid:88) (cid:88)
H+k(X+k)= I(X )− I(X ;X )
i i j
Xi∈X+k (Xi;Xj)∈X+k
(44)
10
(cid:88)
+ I(X ;X ;X )−I(X ;X ;X ;X4)
i j h 1 2 3
(Xi;Xj;Xh)∈X+k
We propose that this complex defines a complex system:
Complex system (definition): A complex system is a minimum free energy complex.
It has the merit to provide a formal definition of complex systems as simple as the definition of an abstract
simplicial complex can be, and to be quite consensual with respect to some of the approaches in this domain,
as reviewed by Newman [98]. Notably, it provides a formal basis to define some of the important concepts in
complexsystems: emergencebeingthecoboundarymap,imergencetheboundarymap,synergybeinginformation
negativity, organization scales being the ranks of random-variable lattices, a collective interaction being a local
minimum of free-energy, diversity being the multiplicity of these minima quantified by the number of facets, a
network being a 1-complex, a network of network being a 1-complex in hyper-cohomology.
The interpretation in terms of sum over paths in the complex is direct as it sums over paths until conditional
independence. We called it the minimum free energy complex but could have called it instead the positive or
instantaneous complex because its facets appear as the boundaries of the ”present” structure, but it obviously
contains all the past-history and the memory of the structure (notably encoded in the negative I that are
k
necessarily non-Markovian). The topological formalization of the minimum energy allows the coexistence of
numerous local minima, a situation usually encountered in complex systems (slow aging) such as frustrated
glasses and K-sat problems [95, 96] which settings correspond here to the case of n binary random variables,
N = ... = N = 2. The existence of the frustration effect, due to the multiplicity of these local minima in the
1 2
free energy landscape [132], has also been one of the main difficulties of the condensed matter theory. Matsuda
couldshowthatI negativityisasignatureoffrustration[89]. ThefirstaxiomsofDFTconsiderthatprobability
k
densities of n(cid:48) elementary bodies are each in a 3-dimensional space [58, 70], defining a whole simplicial structure
of dimension n = 3n(cid:48), commonly called the configuration space. When considered with the physical axiom of
a configuration space, the theorem 3.4 implies that, while the minimum free information energy complex of an
elementary body can only be a simplex, the configuration space of n(cid:48) elementary bodies can be a complex with
(quite) arbitrary topology. In simple terms, this settles the elementary components of the configuration space
as 3-simplices, which composition can give arbitrarily complicated k-complexes. This idea is in resonance with
the triangulations of space-time that arose notably from the work of Wheeler [141] and Penrose [107], like spin
foams [113] and causal sets [118], while we only considered here classical probabilities.
4 Discussion
4.1 Complexity through finite dimension non-extensivity and non iid
Statistical physics without statistical limit? The measure of entropy and information rate on data (the
evolution of entropy H when the number of variables k increases) has a long history. Originally, in the work
k
of Strong and colleagues [120], and as usual in information theory and statistical physics, it was considered that
the”true”entropywasgivenintheasymptoticlimitlim H understationarityorstrongerassumptions. As
n→∞ n
explained in section 2.1 (see also the note of Kontsevitch [72], in the work of Baez, Fritz and Leinster [9]), and
extensively in the statistical physic works of Niven [101, 100, 99], entropy does not need asymptotic or infinite
assumptions such as Stirling approximation to be derived. Here and in the associated paper [16], we tried to
understand, explore, and exploit this observation. Rather than being interested in the asymptotic limit (the
infinite dimensional case) and absolute values of information, the present analysis focuses on the finite version of
the ”slow approach of the entropy to its extensive asymptotic limit” that Grassberger [54], and then Bialek, Ne-
menman and Tishby proposed to be ”a sign of complexity” [22], ”complexity through non-extensivity” (see also
Tsallis [131]). In short, we consider the non-extensivity of information before considering its asymptotic limit.
Considering a statistical physics without statistical limit could be pertinent for the study of ”small” systems,
which concerns biological systems. Their small size allows them to harness thermal fluctuations, and impose
their investigation with out-of equilibrium methods, as exposed in the work of Ritort and colleagues, reviewed
19
in [112]. The H and I landscapes presented here give a detailed expression of the ”signs of complexity” and
k k
non-extensivity, for such small size systems (finite dimension k), and give a finite dimensional geometric view of
the ”slow approach of the entropy to its extensive asymptotic limit”. In a sense, what replaces here the large
number limits, Avogadro number consideration (etc.), is the combinatorial explosions of the different possible
interactions: in the same way as in Van Der Walls paradigm, a combinatorial number of weak interactions can
lead to a strong global interaction. In practice, like for any empirical investigation, the finite dimensional case
imposes restrictions on the conclusions, and basically reminds us that we have not measured everything. Some
relevant random variables for the observed system may be absent from the analysis, and adding such variables
could reveal new interactions that are effectively constitutive and relevant to the system. Among all possible
structures of data, one is universal: data and empirical measures are discrete and finite, as emphasized by Born
[24], and fully justify the cohomological approach used here originating in topos (designed by Grothendieck to
hold the discrete and continuous in a single hand), which was originally constructed to handle in a common
framework the Lie and Galois theory, continuous and discrete symmetries.
Naive estimations let the data speaks, lack of iid or mean-field assumptions let differentiate the
objects One of the striking results of the data analysis as presented here and in the associated paper [16]
concerns the relatively low sample size (m = 41 and m = 111 for the analysis with cells as variable and with
genes as variables respectively) required to obtain satisfying results in relatively high dimensions (k = 10 and
k = 6 respectively). Satisfying results means here that they predict already known results reported in the
biological literature, or in agreement with experts labels. In [87], Nemenman and colleagues, who developed the
problematic of the sampling problem, state in the introduction ”entropy may be estimated reliably even when
inferences about details of the underlying probability distribution are impossible. Thus the direct estimation
of dependencies has a chance even for undersampled problems” and conclude that ”a major advantage of our
definition of statistical dependencies in terms of the MaxEnt approximations is that it can be applied even when
the underlying distributions are undersampled”. The present analysis agrees and confirms their conclusion. The
method applied here is quite elementary. It does not make assumptions of an expected or true distribution, of
maximum entropy distribution or pairwise interaction Hamiltonian, coupling constant or metric, of stationarity
or ergodicity or iid process, Markov chain, or underlying network structure (...) or whatever prior that would
speakinplaceofthedata. ItjustconsidersnumericalempiricalprobabilitiesasexpressedbyKolmogorovaxioms
([71] chap.1), which he called the ”generalized fields of probability” because it does not assume the 6th axiom
of continuity. Rather than fixing a model with priors, the present formalism allows the raw data to impose
freely their specific structure to the model, what is usually called the naive approach or naive estimation. If
one accepts that a frequentist theory and interpretation of probability is mathematically valid ([71], chap.1),
one may then conclude that a frequentist theory of entropy and information may also hold, and moreover
directly fulfills the usual requirement of observability in theoretical physics recalled by Born in his Nobel lecture
[24]. This frequentist elementary consideration is not trivial mathematically notably when considered from the
number theoretic point of view. For example, the combinatoric of integer partitions of m could be investigated
in the general information structure (partition) context, which up to our knowledge has not been achieved in
the context of probability and information. Up to our knowledge, all previous studies that tried to quantify
statisticaldependencesbyinformationmethods(withmorethan3variables)usedtotalcorrelation[129,87],and
crucially assumed that the interaction between the variables are homogeneous which corresponds to usual mean
fieldassumption, andtotheiidcaseofmeaninformation3.4.5presentedhere. Thecombinatorialdecomposition
proposed allows to identify heterogeneous classes within the set of variables which would not have been possible
using homogeneous assumptions. We hence believe that such combinatorial approach will play a key role in
future machine learning developments and automatic classification problems. Notably, we only explored here
the smallest and simplest combinatorics arising in homology, and Vigneaux already identified q-multinomial
extensions of this combinatorics associated with Tsallis entropies [134]. Notably, as stressed in introduction,
those combinatorial decomposition can be understood as providing a geometrically constrained architecture to
neural networks generalization.
4.2 Epigenetic topological learning - biological diversity
In place of the MaxEnt principle, we proposed an almost synonymous least energy principle equivalent here
to a homological complex (finite and without metric assumptions). Mathematically, we took profit of the fact
that whether the maximum of entropy functional is always unique and in a sense normative, the minima of I
k
functionals exhibit a rich structure of degeneracy, generated by the ”non-Shannonian set” [145, 149, 91] and
conjectured to be at least as rich as topological links can be. We proposed that this multiplicity of minima
accounts for biological diversity, or more precisely that the number of facets of this complex quantifies the
diversity in the system. The application to cell type identification presented in the associated paper [16] gives
a preliminary validation of such quantification. Moreover, the definition of a complex system as the minimum
20
free-energy complex given in section 3.4.6, underlining that diversity is just the multiplicity of the minima, is
in agreement with Waddington’s original work [137] (see Figure 7b). In the allegory of Waddington’s epigenetic
landscapes, whatever the ball, it will always fall down, a statement that can be assimilated to the second law
of thermodynamic. But doing so, it will be able to take different paths: diversity comes from the multiple
minima. The explanation by Waddington of such landscape is a ”complex system of interactions” that can be
formalizedbytheminimumfreeenergycomplexwithinteractionscorrespondingtotheI . Moreover,formalisms
k
assuming that the variables are identically distributed, as for the homogeneous systems described in the section
on mean paths 3.4.5, will display a single first minima (one facet, a simplex), and hence no diversity. Sharing
the same aims, Teschendorff and Enver, and then Jin and colleagues, proposed an alternative interpretation of
Waddington’s landscape in terms of signaling entropy [127] and of probability transitions [65], respectively.
Figure 7: The epigenetic landscape of Waddington. a, The epigenetic landscape of Waddington, a path of
theballinthislandscapeillustratesacelldevelopmentalfate. b,”Thecomplexsystemofinteractionsunderlying
the epigenetic landscape” with Waddington’s original legends [137].
FollowingThom’stopologicalmorphogeneticviewofWaddington’swork[128],weproposethatI landscape,
k
paths and minimum free energy complex provide a possible informational formalization of Waddington’s epige-
netic complex landscape and cell fates (cf. Figure 7). This formalization of Waddington’s epigenetic view is
consistent with the machine learning formalization of Hebbian epigenetic plasticity. From the pure formal view,
the models of Hebbian neural learning like Hopfield’s network, Boltzmann machines, the Infomax models pro-
posed by Linsker, Nadal and Parga, Bell and Sejnowski [80, 97, 19]) can be viewed as binary variables subcases
of a generic N-ary variable epigenetic developmental process. For example, Potts model were implemented for
the simulation of cell-based morphogenesis by Glazier and colleagues [31]. Hence the topological approach can
allow the treatment of neural learning and development on the ground of a common epigenetic formalism, in
agreement with biological results pointing out the continuum and ”entanglement” of the biological processes un-
derlyingdevelopmentandlearning[49]. Intermsofthecurrentproblematicsofneuroscience,suchgeneralization
allows on a formal level to consider an analog coding in place of a digital coding, and the methods developed
here can be applied to studies investigating (discrete) analog coding.
Moreover,followingalltheworkoftheselastdecadesontheapplicationofstatisticalphysicstobiologicalsystems
(someofthemcitedinthisarticle), weproposethattheepigeneticprocessimplementsthefirsttwolawsofther-
modynamics,whichweaktopologicalversionsareproposedtoholdintherawdataspace(withoutphasespaceor
symplectic structure, cf. section 3.4.3). As previously underlined, the condition for such an inscription of living
organism dynamic into classical statistical physics to be legitimate is that the considered variables correspond to
phase space variables.
Previous versions
Previous Version: a partial version of this work has been deposited in the method section of Bioarxiv 168740
in July 2017 and preprints [17].
21
Acknowledgement
This work was funded by the European Research Council (ERC consolidator grant 616827 CanaloHmics to
J.M.Goaillard) and Median Technologies, developed at Median Technologies and UNIS Inserm 1072 - Universit´e
Aix-Marseille,andatInstitutdeMathmatiquesdeJussieu-ParisRiveGauche(IMJ-PRG),andthankspreviously
tosupportsandhostingssince2007ofMaxPlanckInstituteforMathematicintheSciences(MPI-MIS)andCom-
plex System Instititute Paris-Ile-de-France (ISC-PIF). This work addresses a deep and warm acknowledgement
to the researchers who helped its realization: D.Bennequin, J.M.Goaillard, Hong Van le, G.Marrelec, M.Tapia
andJ.P.Vigneaux;orsupported-encouragedit: H.Atlan,F.Barbaresco,H.B´enali,P.Bourgine,F.Chavane,J.Jost,
A.Mohammad-Djafari,JP.Nadal,J.Petitot,A.Sarti,J.Touboul. Apartialversionofthisworkhasbeendeposited
in the method section of Bioarxiv 168740 in July 2017 and preprints [17].
Abbreviations
The following abbreviations are used in this manuscript:
iid independent identicaly distributed
H Multivariate k-joint Entropy
k
I Multivariate k-Mutual-Information
k
G Multivariate k-total-correlation or k-multi-information
k
References
[1] Ackley,D.,Hinton,G.,andSejnowski,T.J.Alearningalgorithmforboltzmannmachines.Cognitive
Science 9, 1 (1985), 147–169.
[2] Aczel, J., and Daroczy, Z. On measures of information and their characterizations. Academic Press.
Mathematics in science and engineering, 1975.
[3] Adami, C., and Cerf, N. Prolegomena to a non-equilibrium quantum statistical mechanics. Chaos,
Solitons & Fractals 10, 10 (1999), 1637–1650.
[4] Andre, Y. Symtries I. Ides galoisiennes. Ircam online courses, 2007.
[5] Andre, Y. Ambiguity theory, old and new. arXiv:0805.2568 (2008).
[6] Andrews, G. The Theory of Partitions. Cambridge University Press, Cambridge, 1998.
[7] Aomoto, K. Additiontheoremofabeltypeforhyper-logarithms. Nagoya Math. J. Vol. 88 (1982),55–71.
[8] Atiyah, M. Topologicalquantumfieldtheory. Publications mathmatiques de lI.H.E.S 68 (1988),175–186.
[9] Baez, J., Fritz, T., and Leinster, T. A characterization of entropy in terms of information loss.
Entropy 13 (2011), 1945–1957.
[10] Baez, J., and Pollard, S. Relative entropy in biological systems. Entropy 18, 2 (2016), 46.
[11] Baez, J. C., and Fritz, T. A bayesian characterization of relative entropy. Theory and Applications of
Categories, Vol. 29, No. 16 (2014), p. 422–456.
[12] Baudot. Elements of consciousness and cognition. biology, mathematic, physics and panpsychism: an
information topology perspective. arXiv:1807.04520 (2018).
[13] Baudot, P., and Bennequin, D. The homological nature of entropy. Entropy 17(5) (2015), 3253–3318.
[14] Baudot, P., and Bennequin, D. Topological forms of information. AIP Conf. Proc. 1641 (2015),
213–221.
[15] Baudot, P., and Bernardi, M. Information cohomology methods for learning the statistical structures
of data. In DS3 Data Science, Ecole Polytechnique. (2019).
[16] Baudot, P., Tapia, M., D., B., and J.M., G. Topological information data analysis. in prep. (2019).
[17] Baudot, P., Tapia, M., and Goaillard, J. Topological information data analysis: Poincare-shannon
machine and statistical physic of finite heterogeneous systems. Preprints 2018040157 (2018).
22
[18] Beilinson, A., Goncharov, A., Schechtman, V., and Varchenko, A. Aomotodilogarithms, mixed
hodge structures and motivic cohomology of pairs of triangles on the plane. The Grothendieck Festschrift,
vol. 1, in: Progr. Math., Birkhauser, vol. 86 (1990), 135–172.
[19] Bell, A.J. ; Sejnowski, T. An information maximisation approach to blind separation and blind
deconvolution. Neural Computation 7, 6 (1995), 1129–1159.
[20] Bennett, C. Notes on landauer’s principle, reversible computation and maxwell’s demon. Studies in
History and Philosophy of Modern Physics 34(3) (2003), 501–510.
[21] Bennett, C., Gacs, P., Ming Li, P., Vitanyi, M., and Zurek, W. Information distance. IEEE
Transactions on Information Theory 44, 4 (1998), 1407–1423.
[22] Bialek, W., Nemenman, I., andTishby, N. Complexitythroughnonextensivity. PhysicaA302 (2001),
89–99.
[23] Bjorner, A. Continuous partition lattice. Proc. Natl. Acad. Sci. USA 84 (1987), 6327–6329.
[24] Born, M. The statistical interpretation of quantum mechanics. Nobel Lecture. (1954).
[25] Boyom, M. Foliations-webs-hessiangeometry-informationgeometry-entropyandcohomology. Entropy 18,
12 (2016), 433.
[26] Brillouin, L. Scientific Uncertainty, and Information. Academic Press, 2014.
[27] Buchet, M., Chazal, F., Oudot, S., and Sheehy, D. Efficient and robust persistent homology for
measures. arXiv:1306.0039 (2014).
[28] Carlsson, G. Topology and data. Bull. Amer. Math. Soc. 46 (2009), p.255–308.
[29] Cartan, H., and Eilenberg, S. Homological Algebra. The Princeton University Press, Princeton, 1956.
[30] Cathelineau, J. Sur l’homologie de sl2 a coefficients dans l’action adjointe. Math. Scand. 63 (1988),
51–86.
[31] Chen, N., Glazier, J., Izaguirre, J., and Alber, M. A parallel implementation of the cellular potts
model for simulation of cell-based morphogenesis. Computer Physics Communications 176, 11 (2007),
670681.
[32] Chintakunta, H., Gentimis, T., Gonzalez-Diaz, R., Jimenez, M.-J., and Krim, H. An entropy-
based persistence barcode. Pattern Recognit. 48 (2015), 391–401.
[33] Connes, A., and Consani, C. Characteristic 1, entropy and the absolute point. preprint
arXiv:0911.3537v1. (2009).
[34] Conrad, K. Probability distributions and maximum entropy. Unpublished note.
http://www.math.uconn.edu/ kconrad/blurbs/analysis/entropypost.pdf (2005).
[35] Cover, T., and Thomas, J. Elements of Information Theory. WileySeriesinTelecommunication,1991.
[36] Dayan, P., Hinton, G., Neal, R., and Zemel, R. The helmholtz machine. Neural Computation 7
(1995), 889–904.
[37] Dirac, P. Directions in Physics. John Wiley & Sons Inc, 1978.
[38] Dreyfus, S. The numerical solution of variational problems. Journal of Mathematical Analysis and
Applications. 5, 1 (1962), 3045.
[39] Drummond-Cole, G., Park, J.-S., and Terilla, J. Homotopy probability theory i. J. Homotopy
Relat. Struct. 10, 3 (2015), 425435.
[40] Drummond-Cole, G., Park, J.-S., and Terilla, J. Homotopy probabilty theory ii. J. Homotopy
Relat. Struct. 10, 3 (2015), 623635.
[41] Ebrahimi, N., Soofi, E., and Soyer, R. Multivariatemaximumentropyidentification,transformation,
and dependence. Journal of Multivariate Analysis 99 (2008), 12171231.
[42] Elbaz-Vincent, P., and Gangl, H. On poly(ana)logs i. Compositio Mathematica 130(2) (2002),
161–214.
23
[43] Elbaz-Vincent, P., and Gangl, H. Finite polylogarithms, their multiple analogues and the shannon
entropy. To be published in (2015).
[44] Epstein, C., Carlsson, G., and Edelsbrunner, H. Topological data analysis. Inverse Probl. 27
(2011), 120201.
[45] Erdos, P. On the distribution function of additive functions. Ann. of Math. Vol. 47 (1946), pp. 1–20.
[46] Feynman, R. Space-time approach to non-relativistic quantum mechanics. Reviews of Modern Physics
20, 2 (1948), 367387.
[47] Feynman, R. QED. The Strange Theory of Light and Matter. Princeton University Press, 1985.
[48] Fresse, B. Koszul duality of operads and homology of partitionn posets. Contemp. Math. Amer. Math.
Soc. 346 (2004), pp. 115–215.
[49] Galvan, A. Neural plasticity of development and learning. Hum Brain Mapp 31, 6 (2010), 879890.
[50] Gell-Mann, M., and Hartle, J. Quantum mechanics in the light of quantum cosmology. W. H. Zurek
(ed.), Complexity, entropy and the physics of information. Redwood City, Calif.: Addison-Wesley (1990),
425–458.
[51] Gerstenhaber, M., and Schack, S. Simplicial cohomology is hochschild cohomology. Journal of Pure
and Applied Algebra 30 (1983), 143–156.
[52] Gerstenhaber, M., and Schack, S. Ahodge-typedecompositionforcommutativealgebracohomology.
Journal of Pure and Applied Algebra 48, 1-2 (1987), 229–247.
[53] Goncharov, A. Regulators. Handbook of K-theory. pringer-Verlag Berlin Heidelberg. http://k-
theory.org/handbook/ (2005), 297–324.
[54] Grassberger, P. Toward a quantitative theory of self-generated complexity. International Journal of
Theoretical Physics 25, 9 (1986), 907–938.
[55] Griffiths, R. Consistenthistoriesandtheinterpretationofquantummechanics. J.Stat.Phys.35 (1984),
219.
[56] Han, T. S. Linear dependence structure of the entropy space. Information and Control. vol. 29 (1975),
p. 337–368.
[57] Hochschild, G. On the cohomology groups of an associative algebra. Annals of Mathematics. Second
Series, 46 (1945), 5867.
[58] Hohenberg, P., and Kohn, W. Inhomogeneous electron gas. Phys. Rev. 136, 3B (1964), 864–871.
[59] Hopfield, J. Neural networks and physical systems with emergent collective computational abilities.
PNAS 79 (1982), 2554–2558.
[60] Hu, K. T. On the amount of information. Theory Probab. Appl. 7(4) (1962), 439–447.
[61] James, R., and Crutchfield, J. Multivariatedependencebeyondshannoninformation. Entropy 19, 10
(2017), 531.
[62] Jaynes, E. Information theory and statistical mechanics ii. Physical Review. Series II 108 (2) (1957), pp.
171–190.
[63] Jaynes, E. T. Information theory and statistical mechanics. Physical Review. Series II 106 (4) (1957),
pp. 620–630.
[64] Jaynes, E. T. Information Theory and Statistical Mechanics. Statistical Physics. New York: Benjamin.
Ford, K., 1963.
[65] Jin, S., MacLean, A., Peng, T., and Nie, Q. scepath: energy landscape-based inference of transition
probabilities and cellular trajectories from single-cell transcriptomic data. Bioinformatics (2018), 1–10.
[66] Kapranov, M. Thermodynamics and the moment map. arXiv:1108.3472 (2011).
[67] Kassel, C. Homology and cohomology of associative algebras- a concise introduction to cyclic homology.
Advanced Course on non-commutative geometry (2004).
24
[68] Kelley, H. Gradient theory of optimal flight paths. ARS Journal 30, 10 (1960), 947954.
[69] Kendall, D. Functional equations in information theory. Z. Wahrscheinlichkeitstheorie 2 (1964), p.
225–229.
[70] Kohn, W., and Sham, L. J. Self-consistent equations including exchange and correlation effects. Phys.
Rev. 140, 4A (1965), 1133–1138.
[71] Kolmogorov, A. N. Grundbegriffe der Wahrscheinlichkeitsrechnung.(English translation (1950): Foun-
dations of the theory of probability.). Springer, Berlin (Chelsea, New York)., 1933.
[72] Kontsevitch, M. The 11/2 logarithm. Unpublished note. Reproduced in Elbaz-Vincent & Gangl, 2002
On poly(ana)logs I. Compositio Mathematica (1995).
[73] Kraskov, A. ; Grassberger, P. Mic: Mutual information based hierarchical clustering. Information
Theory and Statistical Learning. Springer ed. http://arxiv.org/abs/q-bio/0311039 (2009), 101–123.
[74] Kullback, S., and Leibler, R. On information and sufficiency. Annals of Mathematical Statistics 22
(1951), 79–86.
[75] Lamarche-Perrin, R., Demazeau, Y., and Vincent, J. The best-partitions problem: How to build
meaningful aggregations? Research Report RR-LIG-044 ¡hal-00947934¿ (2013), 18.
[76] Landau, L., and Lifshitz, E. Statistical Physics (Course of Theoretical Physics, Volume 5).
Butterworth-Heinemann; 3 edition (1980), 1969.
[77] Landauer, R. Irreversibilityandheatgenerationinthecomputingprocess. IBM Journal of Research and
Development 5 (3) (1961), 183–191.
[78] Lee, P. Ontheaxiomsofinformationtheory. TheAnnalsofMathematicalStatisticsVol.35, No.1 (1964),
pp. 415–418.
[79] Lieb, E. H.; Yngvason, J. A guide to entropy and the second law of thermodynamics. Notices of the
AmericanMathematicalSociety.http://www.ams.org/notices/199805/lieb.pdfVol45, N5 (1998),571–581.
[80] Linsker, R. Self-organization in a perceptual network. Computer 21, 3 (1988), 105–117.
[81] Lum,P.,Singh,G.,Lehman,A.,Ishkanov,T.,Vejdemo-Johansson,M.,Alagappan,M.,Carls-
son, J., and Carlsson, G. Extractinginsightsfromtheshapeofcomplexdatausingtopology. Sci. Rep.
3, 1236 (2013).
[82] Mac Lane, S. Homology. Classic in Mathematics, Springer, Reprint of the 1975 edition, 1975.
[83] Maletic, S., and Rajkovic, M. Combinatoriallaplacianandentropyofsimplicialcomplexesassociated
with complex networks. Eur. Phys. J. 212 (2012), 77–97.
[84] Maletic, S., and Zhao, Y. Multilevel integration entropies: The case of reconstruction of structural
quasi-stability in building complex datasets. Entropy 19 (2017), 172.
[85] Marcolli,M.,andTedeschi,R.Entropyalgebrasandbirkhofffactorization.arXiv,Vol.abs/1108.2874
(2014).
[86] Marcolli, M., and Thorngren, R. Thermodynamic semirings. arXiv 10.4171/JNCG/159 Vol.
abs/1108.2874 (2011).
[87] Margolin, A., Wang, K., Califano, A., and Nemenman, I. Multivariate dependence and genetic
networks inference. IET Syst Biol. 4, 6 (2010), 428–40.
[88] Marr, D. Vision. W.H. Freeman and Co., 1982.
[89] Matsuda, H. Informationtheoreticcharacterizationoffrustratedsystems. Physica A: Statistical Mechan-
ics and its Applications. 294 (1-2) (2001), 180–190.
[90] Matus. Conditional probabilities and permutahedron. Annales de l’I.H.P. Probabilites et statistiques 39,
4 (2003), 687–701.
[91] Matus, F. Infinitely many information inequalities. IEEE International Symposium on Information
Theory. (2007).
[92] McGill, W. Multivariate information transmission. Psychometrika 19 (1954), p. 97–116.
25
[93] Merelli, E., Rucco, M., Sloot, P., and Tesei, L. Topological characterization of complex systems:
Using persistent entropy. Entropy 17 (2015), 6872–6892.
[94] Merkh, T., and Montufar, G. Factorizedmutualinformationmaximization. arXiv:1906.05460 (2019).
[95] Mezard, M. Passing messages between disciplines. science 301 (2003), 1686.
[96] Mezard, M., and Montanari, A. Information, Physics, and Computation. Oxford University Press,
2009.
[97] Nadal, J.-P. ; Parga, N. Sensory coding: information maximization and redundancy reduction. Neural
information processing, G. Burdet, P. Combe and O. Parodi Eds. World Scientific Series in Mathematical
Biology and Medecine Vol. 7 (1999), p. 164–171.
[98] Newman, M. E. J. Complex systems: A survey. http://arxiv.org/abs/1112.1440v1 (2011).
[99] Niven, R. Exactmaxwellboltzmann, boseeinsteinandfermidiracstatistics. Physics Letters A 342 (2005),
286293.
[100] Niven, R. Combinatorial entropies and statistics. Eur. Phys. J. 70 (2009), 4963.
[101] Niven, R. Non-asymptotic thermodynamic ensembles. EPL 86 (2009), 1–6.
[102] Niyogi, P., Smale, S., and Weinberger, S. A topological view of unsupervised learning from noisy
data. SIAM J. of Computing 20 (2011), 646–663.
[103] Omnes, R. Logical reformulation of quantum mechanics i. foundations. Journal of Statistical Physics 53
(1988), 893–932.
[104] Oudot, S. Persistence theory: From quiver representations to data analysis. Mathematical Surveys and
Monographs. AMS 209 (2015).
[105] Park, J.-S. Homotopy theory of probability spaces i: Classical independence and homotopy lie algebras.
arXiv:1510.08289 (2015).
[106] Parsegian, V. Van der Waals Forces: A Handbook for Biologists, Chemists, Engineers, and Physicists.
Cambridge University Press., 2006.
[107] Penrose, R. Angular momentum : an approach to combinatorial space-time. In Quantum Theory and
Beyondum. Cambridge University Press. Ted Bastin p.151-180, 1971.
[108] Postnikov, A. Permutohedra, associahedra, and beyond. Int Math Res Notices. arXiv:
math.CO/0507163. 2009(6) (2009), 1026–1106.
[109] Pudlk, P. & Tma, J. Every finite lattice can be embedded in a finite partition lattice. Algebra Univ. 10
(1980), 74–95.
[110] Rajski, C. A metric space of discrete probability distributions. Information and Control 4, 4 (1961),
371–377.
[111] Reshef, D., Reshef, Y., Finucane, H., Grossman, S., McVean, G., Turnbaugh, P., Lander,
E., Mitzenmacher, M., and Sabeti, P. Detecting novel associations in large data sets. Science 334
(2011), 1518.
[112] Ritort, F. Nonequilibrium fluctuations in small systems: from physics to biology,. Advances in Chemical
Physics Ed. Stuart. A. Rice, Wiley publications vol. 137 (2008), 31–123.
[113] Rovelli, C. Notes for a brief history of quantum gravity. arXiv:gr-qc/0006061v3 (2008).
[114] Rumelhart, D., Hinton, G., and Williams, R. Learning representations by back-propagating errors.
Nature 323, 6088 (1986), 533536.
[115] Schwarz, A. Topological quantum field theory. arXiv:hep-th/0011260v1 (2000).
[116] Shannon, C. Alatticetheoryofinformation. Trans. IRE Prof. Group Inform. Theory 1 (1953), 105–107.
[117] Shannon, C. E. Amathematicaltheoryofcommunication. The Bell System Technical Journal 27 (1948),
379–423.
26
[118] Sorkin, R. Finitary substitute for continuous topology. International Journal of Theoretical Physics 30,
7 (1991), 923–947.
[119] Steenrod, N. Products of cocycles and extensions of mapping. Annals of mathematics 2nd ser. 48, 2
(1947), 290–320.
[120] Strong, S., de Ruyter van Steveninck, R., Bialek, W., and Koberle, R. On the application of
information theory to neural spike trains. Pac Symp Biocomput (1998), 621–32.
[121] Studeny, M., and Vejnarova, J. The multiinformation function as a tool for measuring stochastic
dependence. in M I Jordan, ed., Learning in Graphical Models, MIT Press, Cambridge (1999), 261–296.
[122] Tadic, B., Andjelkovic, M., and Suvakov, M. Theinfluenceofarchitectureofnanoparticlenetworks
on collective charge transport revealed by the fractal time series and topology of phase space manifolds. J.
Coupled Syst. Multiscale Dyn. 4 (2016), 30–42.
[123] Tai Ha, H., and Van Tuyl, A. Resolutions of square-free monomial ideals via facet ideals: a survey.
Contemporary mathematics - American Mathematical Society - Algebra, Geometry and Their Interactions:
International Conference Midwest 448 (2007), 91–105.
[124] Takacs, D. Stochastic Processes problems and solutions. John Wiley and Sons Inc, 1960.
[125] Tapia, M., Baudot, P., Formizano-Treziny, C., Dufour, M., Temporal, S., Lasserre, M.,
Marqueze-Pouey, B., Gabert, J., Kobayashi, K., and J.M., G. Neurotransmitter identity and
electrophysiological phenotype are genetically coupled in midbrain dopaminergic neurons. Sci. Rep. 8, 1
(2018), 13637.
[126] Tate, J. Galois cohomology. online course (1991).
[127] Teschendorff, A., and Enver, T. Single-cellentropyforaccurateestimationofdifferentiationpotency
from a cells transcriptome. Nature communication (2017).
[128] Thom, R. Stabilite struturelle et morphogenese. deuxieme edition, InterEdition, Paris, 1977.
[129] Tkacik, G., Marre, O., Amodei, D., Schneidman, E., Bialek, W., and Berry, M. n. Searching
for collective behavior in a large network of sensory neurons. PLoS Comput Biol. 20 10, 1 (2014).
[130] Tononi, G., and Edelman, G. Consciousness and complexity. Science 282 (1998), 1846–1851.
[131] Tsallis, C. Entropic nonextensivity : a possible measure of complexity. Chaos solitons and fractals 13,
3 (2002), 371–391.
[132] Vannimenus, J., and Toulouse, G. Theory of the frustration effect. ii. ising spins on a square lattice.
Journal of Physics C: Solid State Physics 10, 18 (1977).
[133] Vigneaux, J. The structure of information: from probability to homology. arXiv:1709.07807 (2017).
[134] Vigneaux, J. Information theory with finite vector spaces. IEEE Transactions on Information Theory
99, 1 (2019), 1.
[135] Vigneaux, J. Topologyofstatisticalsystems.Acohomologicalapproachtoinformationtheory. PhDthesis,
Paris 7 Diderot University, 2019.
[136] Von Bahr, B. On the central limit theorem in rk. Ark. Mat. 7 (1967), 61–69.
[137] Waddington, C. H. The Strategy of the Genes. Routledge Library editions, 1957.
[138] Watanabe, S. Information theoretical analysis of multivariate correlation. IBM Journal of Research and
Development 4 (1960), 66–81.
[139] Weibel, C. An introduction to homological algebra. Cambridge University Press, 1995.
[140] Weiss, P. L’hypothseduchampmolculaireetlapropritferromagntique. J.Phys.Theor.Appl.6,1(1907),
661–690.
[141] Wheeler, J. Information, physics, quantum: the search for the links. Proc. 3rd Int. Symp. Foundations
of Quantum Mechanics, Tokyo (1983), 354–368.
[142] Witten, E. Topological quantum field theory. Commun. Math. Phys. 117 (1988), 353–386.
27
[143] Wu, F. The potts model. Reviews of Modern Physics. 54 (1982), 235–268.
[144] Xie, Z., Chen, J., Yu, J., Kong, X., Normand, B., and Xiang, T. Tensor renormalization of
quantum many-body systems using projected entangled simplex states. Phys. Rev. X 4 (2014), 011025–1.
[145] Yeung, R. A framework for linear information inequalities. IEEE Transactions on Information Theory
(New York) 43, 6 (1997), 19241934.
[146] Yeung, R. On entropy, information inequalities, and groups. Communications, Information and Network
Security Volume 712 of the series The Springer International Series in Engineering and Computer Science
(2003), 333–359.
[147] Yeung, R. Information Theory and Network Coding. Springer, 2007.
[148] Yeung, R. Facets of entropy. Communications in Information and Systems 15, 1 (2015), 87–117.
[149] Zang, Z., and Yeung. On characterization of entropy function via information inequalities. IEEE
transactions on information theory 44, 4 (1997), 1440–1452.
[150] Zurek, W. Thermodynamic cost of computation, algorithmic complexity and the information metric.
Nature 341 (1989), 119–125.
28