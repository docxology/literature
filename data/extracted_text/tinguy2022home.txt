Home Run: Finding Your Way Home by
Imagining Trajectories
Daria de Tinguy1, Pietro Mazzaglia1, Tim Verbelen1, and Bart Dhoedt1
IDLab, Department of Information Technology
Ghent University - imec
Technologiepark-Zwijnaarde 126, B-9052 Ghent, Belgium
firstname.lastname@ugent.be
Abstract. When studying unconstrained behavior and allowing mice to
leave their cage to navigate a complex labyrinth, the mice exhibit for-
aging behavior in the labyrinth searching for rewards, returning to their
home cage now and then, e.g. to drink. Surprisingly, when executing such
a “home run”, the mice do not follow the exact reverse path, in fact, the
entry path and home path have very little overlap. Recent work proposed
a hierarchical active inference model for navigation, where the low level
model makes inferences about hidden states and poses that explain sen-
sory inputs, whereas the high level model makes inferences about mov-
ing between locations, eﬀectively building a map of the environment.
However, using this “map” for planning, only allows the agent to ﬁnd
trajectories that it previously explored, far from the observed mice’s be-
haviour. In this paper, we explore ways of incorporating before-unvisited
paths in the planning algorithm, by using the low level generative model
to imagine potential, yet undiscovered paths. We demonstrate a proof
of concept in a grid-world environment, showing how an agent can ac-
curately predict a new, shorter path in the map leading to its starting
point, using a generative model learnt from pixel-based observations.
Keywords: Robot Navigation· Active Inference· Free Energy Principle
· Deep Learning.
1 Introduction
Humans rely on an internal representation of the environment to navigate, i.e.
they do not require precise geometric coordinates or complete mappings of the
environment; a few landmarks along the way and approximate directions are
enough to ﬁnd our way back home [1]. This reﬂects the concept of a “cognitive
map” as introduced by Tolman [2], and matches the discovery of speciﬁc place
cells ﬁring in the rodent hippocampus depending on the animal position [3] and
our representation of space [1].
Recently, Çatal et al. [4] showed how such mapping, localisation and path in-
tegration can naturally emerge from a hierarchical active inference (AIF) scheme
and are also compatible with the functions of the hippocampus and entorhinal
arXiv:2208.10914v1  [cs.LG]  19 Aug 2022
2 D. de Tinguy & al.
cortex [5]. This was implemented on a real robot to eﬀectively build a map of its
environment, which could then be used to plan its way using previously visited
locations [6].
However, while investigating the exploratory behaviour of mice in a maze,
where mice were left free to leave their home to run and explore, a peculiar
observation was made. When the mice decided to return to their home location,
instead of re-tracing their way back, the mice were seen taking fully new, shorter,
paths directly returning them home [7].
On the contrary, when given the objective to reach a home location, the hier-
archical active inference model, as proposed by [4,6], can only navigate between
known nodes of the map, unable to extrapolate possible new paths without ﬁrst
exploring the environment. To address this issue, we propose to expand the
high level map representation using the expected free energy of previously un-
explored transitions, by exploiting the learned low-level environment model. In
other worlds, we enlarge the projection capabilities of architecture [6] to unex-
plored paths.
In the remainder of this paper we will ﬁrst review the hierarchical AIF model
[4], then explain how we address planning with previously unvisited paths by
imagining novel trajectories within the model. As a proof of concept, we demon-
strate the mechanism on a Minigrid environment with a four-rooms setup. We
conclude by discussing our results, the current limitations and what is left to
improve upon the current results.
2 Navigation as hierarchical active inference
Theactiveinferenceframeworkreliesuponthenotionthatintelligentagentshave
an internal (generative) model optimising beliefs (i.e. probability distributions
over states), explaining the causes of external observations. By minimising the
surprise or prediction error, i.e, free energy (FE), agents can both update their
model as well as infer actions that yield preferred outcomes [8,9].
In the context of navigation, Çatal et al. [4] introduced a hierarchical active
inference model, where the agent reasons about the environment on two diﬀerent
levels. On the low level, the agent integrates perception and pose, whereas on
the high level the agent builds a more coarse grained, topological map. This is
depicted in Figure 1.
The low level, depicted in blue, comprises a sequence of low-level action
commands at and sensor observationsot, which are generated by hidden state
variables st and pt. Herest encodes learnable features that give rise to sensory
outcomes, whereaspt encodes the agent’s pose in terms of its position and ori-
entation. The low level transition modelp(st+1|st,pt,at) and likelihood model
p(ot|st) are jointly learnt from data using deep neural networks [10], whereas
the pose transition model p(pt+1|st,pt,at) is instantiated using a continuous
attractor network similar to [11].
At the high level, in red in the Figure, the agent reasons over more coarse
grained sequences of locationslτ, where it can execute a movemτ that gives
Home Run: Finding Your Way Home by Imagining Trajectories 3
Fig. 1.Navigation as a hierarchical generative model for active inference [4]. At the
lower level, highlighted in blue, the model entertains beliefs about hidden statesst and
pt, representing hidden causes of the observation and the pose at the current timestep
t respectively. The hidden states give rise to observationsot, whereas actionsat impact
future states. At the higher level, highlighted in red, the agent reasons about locations
l. The next locationlτ+1 is determined by executing a movemτ . Note that the higher
level operates on a coarser timescale. Grey shaded nodes are considered observed.
rise to a novel locationlτ+1. In practice, this boils down to representing the
environment as a graph-based map, where locationslτ are represented by nodes
in the graph, whereas potential movesmτ are links between those nodes. Note
that a single time step at the higher level, i.e. going fromτ to τ+1, can comprise
multiple time steps on the lower level. This enables the agent to ﬁrst ‘think’ far
ahead in the future on the higher level.
To generate motion, the agent minimizes expected free energy (EFE) under
this hierarchical generative model. To reach a preferred outcome, the agent ﬁrst
plans a sequence of moves that are expected to bring the agent to a location
rendering the preferred outcome highly plausible, after which it can infer the
action sequence that brings the agent closer to the ﬁrst location in that sequence.
For a more elaborate description of the generative model, the (expected) free
energy minimisation and implementation, we refer to [4].
3 Imagining unseen trajectories
As discussed in [4], minimising expected free energy under such a hierarchical
model induces desired behaviour for navigation. In the absence of a preferred
outcome, an epistemic term in the EFE will prevail, encouraging the agent to
explore actions that yield information on novel (hidden) states, eﬀectively ex-
panding the map while doing so. In the presence of a preferred state, the agent
will exploit the map representation to plan the shortest (known) route towards
the objective. However, crucially, the planning is restricted to previously vis-
ited locations in the map. This is not consistent with the behaviour observed in
mice [7], as these, apparently, can exploit new paths even when engaging in a
goal-directed run towards their home.
4 D. de Tinguy & al.
In order to address this issue, we hypothesize that the agent not only con-
siders previously visited links and locations in the map during planning, but
also imagines potential novel links. A potential link from a start locationlA to
a destination location lB is hence scored by the minimum EFE over all plans
π (i.e. a sequence of actions) generating such a trajectory under the (low level)
generative model, i.e.:
G(lA,lB) = min
π
H∑
k=1
DKL
[
Q(st+k,pt+k|π)Q(st|lA)∥Q(st+H,pt+H|lB)
]
  
probability reaching lB from lA
+ EQ(st+k)
[
H(P(ot+k|st+k))
]
  
observation ambiguity
.
(1)
The ﬁrst term is a KL divergence between the expected states to visit starting
at locationlA and executing planπ, and the state distribution expected at loca-
tion lB. The second term penalizes paths that are expected to yield ambiguous
observations.
Wecannowuse G(lA,lB) toweigheachmovebetweentwocloselocations(the
number of path grows exponentially the further the objective is), even through
ways not explored before, and plan for the optimal trajectory towards a goal
destination. In the next section, we work out a practical example using a grid-
world environment.
4 Experiments
4.1 MiniGrid setup
The experiments were realised in a MiniGrid environment [12] of 2×2 up to 5×5
rooms, of sizes going from 4 to 7 tiles and having a random ﬂoor color chosen
among 6 options : red, green, blue, purple, yellow and grey. Rooms are connected
by a single open tile, randomly spawned in the wall. The agent has 3 possible
actions at each time step: move one tile forward, turn 90 degrees left or turn 90
degrees right. It can’t see through walls and can only venture into an open grid
space. Note that the wall blocking vision is not really realistic and the agent can
see the whole room if there is an open door in its ﬁeld of view, thus even if part
of the room should be masked by a wall (eg. Fig 2C raw observation). It can see
ahead and around in a window of7×7 tiles, including its own occupied tile. The
observation the agent receives is a pixel rendering in RGB of shape3 ×56 ×56.
4.2 Model training and map building
Our hierarchical generative model was set up in similar fashion as [4]. To train
the lower level of the generative model, which consists of deep neural networks,
we let an agent randomly forage the MiniGrid environments, and train those
Home Run: Finding Your Way Home by Imagining Trajectories 5
A)
B)
C)
Fig. 2.MiniGrid test maze and associated ﬁgures, A) An example of the maze with
a reachable goal (door open allowing shortcut) and the agent path toward a home-
run’s starting point, the transparent grey box correspond to the agent’s ﬁeld of view at
the starting position. B) The topological map of the path executed in A as generated
by the high level of our generative model, C) The currently observed RGB image as
reconstructed by the agent’s model at the end of path and the view at the desired goal
position.
end to end by minimising the free energy on those sequences. Additional model
details and training parameters can be found in Appendix A.
The high level map is also built using the same procedure as [4]. However,
since we are dealing with a grid-world, distinct places in the grid typically yield
distinct location nodes in the map, unless these are near and actually yield
identical observations. Also, we found that predicting the eﬀect of turning left
or right was harder for neural networks to predict, yielding a higher surprise
signal. However, despite these limitations, we can still demonstrate the main
contribution of this paper.
4.3 Home run
Inspired by the mice navigation in [7], we test the following setup in which the
agent ﬁrst explores a maze, and at some point is provided with a preference of
returning to the start location. Figure 2 shows an example of a test environment
and associated trajectories realised by the agent. At the ﬁnal location, the agent
is instructed to go back home, provided by the goal observation in Fig. 2C.
Fig. 2B illustrates the map generated by the hierarchical model.
First, we test whether the agent is able to infer whether it can reach the
starting node in the experience map from the current location. We do so by
imagining all possible plansπ, and evaluating the expected free energy of each
plan over an average ofN = 3 samples from the model. Figure 3 shows the
6 D. de Tinguy & al.
Fig. 3.Lowest expected free energy of each end position after 5 steps. The right ﬁgure
shows the agent at position (0,0) facing the goal at position (0,5), as represented in
Figure 2A i). In the left ﬁgure, the door is open, therefore the goal is reachable, on the
right ﬁgure the door is closed, the goal cannot be reached in 5 steps.
EFE for all reachable locations in a 5 steps planning horizon. It is clear that
in case the door is open, the agent expects the lowest free energy when moving
forward through the door, expecting to reach the start node in the map. In case
the path is obstructed (the door as in 2A, allowing a shortcut, is closed), it can
still imagine going forward 5 steps, but this will result in the agent getting stuck
against the wall, which it correctly imagines and reﬂects on the EFE.
However, the prior model learnt by the agent is far from perfect. When
inspecting various imagined rollouts of the model, as shown in Figure 4, we
see that the model has trouble encoding and remembering the exact position of
the door, i.e. predicting the agent getting stuck (top) or incorrect room colours
and size (bottom). While not problematic in our limited proof of concept, also
due to the fact that the EFE is averaged over multiple samples, this shows that
the eﬀectiveness of the agent will be largely dependent on the accuracy of the
model.
To test the behaviour in a more general setting, we set multiple home-run
scenarios, where the agent’s end position isd = 5,6,7,9 steps away from the
start location. For eachd, we sample at least 20 runs over 4 novel2 ×2 rooms
environment, with diﬀerent room sizes and colours, similar to the train set, in
which 10 have an open door between the start and goal, and 10 have not. We
count the average number of steps required by the agent to get back home,
and compare against two baseline approaches. First is the Greedy algorithm,
inspired by [13], in which the agent greedily navigates in the direction of the
goal location, and follows obstacles in the known path direction when bumping
into one. Second is a TraceBack approach, which retraces all its steps back
home, similar to Ariadne’s thread. Our approach uses the EFE with a planning
horizon of d to decide whether or not the home node is reachable based on a
ﬁxed threshold, and falls back to planning in the hierarchical model, which boils
down to a TraceBack strategy.
Home Run: Finding Your Way Home by Imagining Trajectories 7
open closed
d Greedy TraceBack Ours Greedy TraceBack Ours
5 5 25 6.5 29.5 25 25
6 6 31 6 41 31 31
7 7 27 11.5 31.5 27 27
9 9 36 23.7 46 36 36
Table 1.Home run strategies and the resulting number of steps, for diﬀerent distances
d to home, and open versus closed scenarios. For smalld our model correctly imagines
the outcome. Ford = 9 the agent infers an open door about 27% of the time.
In case of smalld(≤6), our approach successfully identiﬁes whether the goal
is reachable or not, even when the agent is not facing it, which results in a
similar performance for a Greedy approach in the ‘open’ case, and a reverting to
TraceBack in the ‘closed’ case. There is been only one exception in our test-bench
at 5steps range issued by a reconstruction error on all samples (the occurrence
probability is 0.04% as having a sample wrongly estimating the door position at
5steps is 33%). Ford= 7our model misses some of the shortcut opportunities,
as the model’s imagination becomes more prone to errors for longer planning
horizons. Ford= 9, the rooms are larger and the wall separating the two rooms
is actually not visible to the agent. In this regime, we found the agent imagines
about 27% of the time that it will be open, and takes the gamble to move towards
the wall, immediately returning on its path if the wall is obstructed.
Fig. 4.Three imagined trajectories of a 5-steps projection moving forward. The trained
model is not perfectly predicting the future, only the middle sequence predicts the
correct dynamics.
8 D. de Tinguy & al.
5 Discussion
Our experiments show that using the EFE of imagined paths can yield more
optimal,goal-directedbehaviour.Indeed,ouragentisabletoimagineandexploit
shortcuts when planning its way home. However, our current experimental setup
is still preliminary and we plan to further expand upon this concept. For instance
we currently arbitrarily set the point at which the agent decide to home-run. In a
real experiment, the mice likely decide to go home due to some internal stimulus,
e.g., when they get thirsty and head back home where water is available. We
could further develop the experimental setup to incorporate such features and
do a more extensive evaluation.
One challenge of using the Minigrid environment as an experimental setup
[12]istheuseoftopviewvisualobservations.Usingapixel-wiseerrorforlearning
the low-level perception model can be problematic, as for example the pixel-wise
error between a closed versus an open tile in the wall is small in absolute value,
and hence it’s diﬃcult to learn for the model, as illustrated in Figure 4. A
potential approach to mitigate this is to use a contrastive objective instead, as
proposed by [14].
Another important limitation of the current model is that it depends on
the eﬀective planning horizon of the lowest level model to imagine shortcuts.
Especially in the Minigrid environment, imagining the next observation for a 90
degree turn is challenging, as it requires a form of memory of the room layout to
correctly predict the novel observation. This severely limits the planning horizon
of our current models. A potential direction of future work in this regard is to
learn a better location, state and pose mapping. For instance, instead of simply
associating locations with a certain state and pose, conditioning the transition
model on a learnt location descriptor might allow the agent to learn and encode
the shape of a complete room in a location node.
Other approaches have been proposed to address the navigation towards a
goal by the shortest way possible in a biologically plausible way. For instance,
Erdem et al. [15] reproduced the pose and place-cell principle of the rat’s hip-
pocampus with spiking neural networks and use a dense reward signal to drive
goal-directed behaviour, with more reward given the closer the agent gets to
the goal. Hence, the path with the highest reward is sought, and trajectories on
which obstacles are detected are discarded. In Vegard et al. [13], the process is
also bio-inspired, based on the combination of grid cell-based vector and topolog-
ical navigation. The objective is now explicitly represented as a target position
in space, which is reached by vector navigation mechanisms with local obstacle
avoidance mediated by border cells and place cells. Both alternatives also adopt
topological maps and path integration in order to reach their objective. However,
both exhibit more greedy and reactive behaviour, whereas our model is able to
exploit the lower level perception model to already predict potential obstacles
upfront, before bumping into those.
Home Run: Finding Your Way Home by Imagining Trajectories 9
6 Conclusion
In this paper we have proposed how a hierarchical active inference model can
be used to improve planning by predicting novel, previously unvisited paths.
We demonstrated a proof of concept using a generative model learnt from pixel
based observations in a grid-world environment.
As future work we envision a more extensive evaluation, comparing shallow
versus deep hierarchical generative models in navigation performance. Moreover,
we aim to address several of the diﬃculties of our current perception model, i.e.
thelimitationsofpixel-wisepredictionerrors,thelimitedplanninghorizon,anda
more expressive representation for locations in the high level model. Ultimately,
ourgoalistodeploythisonareal-worldrobot,autonomouslyexploring,planning
and navigating in its environment.
Acknowledgment
This research received funding from the Flemish Government under the “Onder-
zoeksprogramma Artiﬁciële Intelligentie (AI) Vlaanderen” programme.
References
1. M. Peer, I. K. Brunec, N. S. Newcombe, and R. A. Epstein, “Structuring
knowledge with cognitive maps and cognitive graphs,” Trends in Cognitive
Sciences, vol. 25, no. 1, pp. 37–54, 2021. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S1364661320302503
2. E. C. Tolman, “Cognitive maps in rats and men.”Psychological review, vol. 55 4,
pp. 189–208, 1948.
3. M. Milford, G. Wyeth, and D. Prasser, “Ratslam: a hippocampal model for simul-
taneous localization and mapping,” inIEEE International Conference on Robotics
and Automation, 2004. Proceedings. ICRA ’04. 2004, vol. 1, 2004, pp. 403–408
Vol.1.
4. O. Çatal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron, “Robot
navigation as hierarchical active inference,”Neural Networks, vol. 142, pp. 192–
204, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0893608021002021
5. A. Safron, O. Çatal, and T. Verbelen, “Generalized simultaneous localization
and mapping (g-SLAM) as uniﬁcation framework for natural and artiﬁcial
intelligences: towards reverse engineering the hippocampal/entorhinal system
and principles of high-level cognition,” Oct. 2021. [Online]. Available: https:
//doi.org/10.31234/osf.io/tdw82
6. O. Çatal, W. Jansen, T. Verbelen, B. Dhoedt, and J. Steckel, “Latentslam:
unsupervised multi-sensor representation learning for localization and mapping,”
CoRR, vol. abs/2105.03265, 2021. [Online]. Available: https://arxiv.org/abs/2105.
03265
7. M. Rosenberg, T. Zhang, P. Perona, and M. Meister, “Mice in a labyrinth show
rapid learning, sudden insight, and eﬃcient exploration,”eLife, vol. 10, p. e66175,
jul 2021. [Online]. Available: https://doi.org/10.7554/eLife.66175
10 D. de Tinguy & al.
8. K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. O. Doherty, and
G. Pezzulo, “Active inference and learning,”Neuroscience & Biobehavioral Reviews,
vol. 68, pp. 862–879, 2016. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0149763416301336
9. R. Kaplan and K. Friston, “Planning and navigation as active inference,” 12 2017.
10. O. Çatal, S. Wauthier, C. De Boom, T. Verbelen, and B. Dhoedt,
“Learning generative state space models for active inference,” Frontiers
in Computational Neuroscience , vol. 14, 2020. [Online]. Available: https:
//www.frontiersin.org/article/10.3389/fncom.2020.574372
11. M. Milford, A. Jacobson, Z. Chen, and G. Wyeth, RatSLAM: Using
Models of Rodent Hippocampus for Robot Navigation and Beyond . Cham:
Springer International Publishing, 2016, pp. 467–485. [Online]. Available:
https://doi.org/10.1007/978-3-319-28872-7_27
12. M.Chevalier-Boisvert,L.Willems,andS.Pal,“Minimalisticgridworldenvironment
for openai gym,” https://github.com/maximecb/gym-minigrid, 2018.
13. V. Edvardsen, A. Bicanski, and N. Burgess, “Navigating with grid and place cells
in cluttered environments,”Hippocampus, vol. 30, pp. 220 – 232, 2019.
14. P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Contrastive active inference,”
in Advances in Neural Information Processing Systems , A. Beygelzimer,
Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available:
https://openreview.net/forum?id=5t5FPwzE6mq
15. U. Erdem and M. Hasselmo, “A goal-directed spatial navigation model using for-
ward planning based on grid cells,”The European journal of neuroscience, vol. 35,
pp. 916–31, 03 2012.
16. D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization.” [Online].
Available: https://arxiv.org/abs/1412.6980
Home Run: Finding Your Way Home by Imagining Trajectories 11
A Model details and training
In this appendix, we provide some additional details on the training data, model
parameters, training procedure and building the hierarchical map.
A.1 Training data
To optimize the neural network models a dataset composed of sequences of
action-observation pairs was collected by human demonstrations of interaction
with the environment. The agent was made to move around from rooms to
room, circle around and turn randomly. About 12000 steps were recorded in 39
randomly created environments having diﬀerent room size, number of rooms,
open door emplacements and ﬂoor colors, as well as the agent having a random
starting pose and orientation. 2/3 of the data were used for training and 1/3 for
validation. Then a fully novel environment was used for testing.
A.2 Model parameters
The low level perception model is based on the architecture of [10], and is com-
posed of 3 neural networks that we call: prior, posterior and likelihood.
The priorneural network consists in a LSTM layer followed with a varia-
tional layer giving out a distribution (i.e mean and std).
The posteriormodel ﬁrst consists of a convolutional network to compress
sensor data. This data is then concatenated with the hot encoded action and the
previous state, all of that is then processed by a fully connected neural network
coupled with a variational layer to obtain a distribution.
The likelihoodmodel performs the inverse of the convolutional part of the
posterior, generating an image out of a given state sample.
The detailed parameters are listed in Table 2.
A.3 Training the model
The low level perception pipeline was trained end to end on time sequences of 10
steps using stochastic gradient descent with the minimization of the free energy
loss function [10]:
FE =
∑
t
DKL[Q(st|st−1,at−1,ot)||P(st|st−1,at−1)] −EQ(st)[log P(ot|st)]
The loss consists of a negative log likelihood part penalizing the error on
reconstruction, and a KL-divergence between the posterior and the prior distri-
butions on a training sequence. We trained the model for 300 epochs using the
ADAM optimizer [16] with a learning rate of 1·10–4.
12 D. de Tinguy & al.
Layer Neurons/Filters Stride
Prior
Concatenation
LSTM 200
Linear 2*30
Posterior
Convolutional 16 2
Convolutional 32 2
Convolutional 64 2
Convolutional 128 2
Convolutional 256 2
Concatenation
Linear 200
Linear 2*30
Likelihood
Linear 200
Linear 256*2*2
Upsample
Convolutional 128 1
Upsample
Convolutional 64 1
Upsample
Convolutional 32 1
Upsample
Convolutional 16 1
Upsample
Convolutional 3 1
Table 2.Models parameters
A.4 Building the map
The high level model is implemented as a topological graph representation, link-
ing pose and hidden state representation to a location in the map. Here we reuse
the LatentSLAM implementation [6] consisting of pose cells, local view cells and
an experience map.
The pose cellsare implemented as a Continuous Attractor Network (CAN),
representing the local position x, y and headingθ of the agent. Pose cells rep-
resent a ﬁnite area, therefore the ﬁring ﬁelds of a single grid cell correspond to
several periodic spatial locations.
The local view cellsare organised as a list of cell, each cell containing a
hidden state representing an observation, the pose cell excited position, and the
map’s experience node linked to this view. After each motion, the encountered
scene is compared to all previous cells observation by calculating the cosine
distance between hidden state features. If the distance is smaller than a given
threshold, then the cell corresponding to this view is activated, else a new cell
is created.
The experience mapcontains the experience of the topological map. It
gives an estimate of the agent global pose in the environment and link the pose
cell position with the local view cell active at this moment. If those elements do
not match with any existing node of the map, a new one is created and linked to
the previous experience, else a close loop is operated and the existing experiences
are linked together.