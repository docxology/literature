Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical
Planning and Control
Poppy Collis * 1 Ryan Singh * 1 2 Paul F Kinghorn 1 Christopher L Buckley 1 2
Abstract
An open problem in artificial intelligence is how
systems can flexibly learn discrete abstractions
that are useful for solving inherently continuous
problems. Previous work has demonstrated that a
class of hybrid state-space model known as recur-
rent switching linear dynamical systems (rSLDS)
discover meaningful behavioural units via the
piecewise linear decomposition of complex con-
tinuous dynamics (Linderman et al., 2016). Fur-
thermore, they model how the underlying continu-
ous states drive these discrete mode switches. We
propose that the rich representations formed by an
rSLDS can provide useful abstractions for plan-
ning and control. We present a novel hierarchical
model-based algorithm inspired by Active Infer-
ence in which a discrete MDP sits above a low-
level linear-quadratic controller. The recurrent
transition dynamics learned by the rSLDS allow
us to (1) specify temporally-abstracted sub-goals
in a method reminiscent of the options framework,
(2) lift the exploration into discrete space allow-
ing us to exploit information-theoretic exploration
bonuses and (3) ‘cache’ the approximate solutions
to low-level problems in the discrete planner. We
successfully apply our model to the sparse Con-
tinuous Mountain Car task, demonstrating fast
system identification via enhanced exploration
and non-trivial planning through the delineation
of abstract sub-goals.
1. Introduction
In a world that is inherently continuous, the brain’s apparent
capacity to distil discrete concepts from sensory data repre-
*Equal contribution 1School of Engineering and Informatics,
University of Sussex, Brighton, UK 2VERSES AI Research Lab,
Los Angeles, California, USA. Correspondence to: Poppy Collis
<pzc20@sussex.ac.uk>.
sents a highly desirable feature in the design of autonomous
systems. Humans are able to flexibly specify abstract sub-
goals during planning, thereby reducing problems into man-
ageable chunks (Newell & Simon, 1972; Gobet et al., 2001).
Furthermore, they are able to transfer this knowledge across
new tasks; a process which has proven a central challenge
in artificial intelligence (d’Avila Garcez & Lamb, 2023).
Translating problems into discrete space offers distinct ad-
vantages in decision-making. Namely, the computation-
ally feasible application of information-theoretic measures
(e.g. information-gain), as well as the direct implementa-
tion of classical techniques such as dynamic programming
(LaValle, 2006; Friston et al., 2023). One prevalent ap-
proach to tackling continuous spaces involves the simple
grid-based discretisation of the state-space, however this
becomes extremely costly as the dimensionality increases
(Coulom, 2007; Mnih et al., 2015). We therefore ask how we
might be able to smoothly handle the presence of continuous
variables whilst maintaining the benefits of decision-making
in the discrete domain.
To address this, we explore the rich representations learned
by recurrent switching linear dynamical systems (rSLDS)
in the context of planning and control. This class of hy-
brid state-space model consists of discrete latent states that
evolve via Markovian transitions, which act to index a dis-
crete set of linear dynamical systems (Linderman et al.,
2016). Importantly, a continuous dependency in the discrete
state transition probabilities is included in the generative
model. By providing an understanding of the continuous
latent causes of switches between discrete modes, this re-
current transition structure can be exploited such that a con-
troller can flexibly specify inputs to drive the system into
a desired region of the state-action space. By embracing
the established control-theoretic strategy of piecewise linear
decomposition of nonlinear dynamics, our approach lies
in contrast to the comparatively opaque solutions found by
continuous function approximators (Liberzon, 2003; Mnih
et al., 2015). Using statistical methods to fit these mod-
els provides a means by which we can effectively perform
online discovery of useful non-grid discretisations of the
state-space for system identification and control.
We describe a novel model-based algorithm inspired by Ac-
1
arXiv:2408.10970v1  [cs.AI]  20 Aug 2024
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Figure 1.HHA solves nonlinear problems via specifying abstract sub-goals in state-space. (a) Piecewise linear dynamics of the
Continuous Mountain Car state-space found by rSLDS. Reward location shown ( black triangle). While the rSLDS retrives 5 modes
in total, here we plot only the modes seen in the position-velocity ( x) space without showing the control input (u) axis. (b) Example
trajectory (segments coloured by mode) showing the HHA consistently navigating to the goal. (c) Continuous control input (coloured by
discrete action specified by planner and arrow size indicating magnitude and direction) over same example trajectory in (b).
tive Inference (Parr et al., 2022), in which a discrete MDP,
informed by the representations of an rSLDS, interfaces
with a finite horizon linear-quadratic regulator (LQR) imple-
menting closed-loop control. We demonstrate the efficacy
of this algorithm by applying it to the classic control task of
Continuous Mountain Car (OpenAI, 2021). We show that
information-theoretic exploration drive integrated with the
emergent piecewise description of the task-space facilitates
fast system identification to find successful solutions to this
non-trivial planning problem.
1.1. Contributions
• The enhancement of planning via the introduction of
temporally-abstracted sub-goals by decoupling a dis-
crete MDP from the continuous clock time using the
emergent representations from an rSLDS.
• The lifting of information-seeking decision-making
into a (discrete) abstraction of the states enabling ef-
ficient exploration and thereby reducing sensitivity to
the dimensionality of the task-space.
2. Related work
In the context of control, hybrid models in the form of piece-
wise affine (PW A) systems have been rigorously examined
and are widely applied in real-world scenarios (Bemporad
et al., 2000; Borrelli et al., 2006). Previous work by Ab-
dulsamad et. al. has applied a variant on rSLDS (recurrent
autoregressive hidden Markov models) to the optimal con-
trol of general nonlinear systems (Abdulsamad & Peters,
2023; 2020). The authors use these models to the approxi-
mate expert controllers in a closed-loop behavioural cloning
context. While their algorithm focuses on value function
approximation, in contrast, we learn online without expert
data and focus on flexible discrete planning.
3. Framework
Here, we provide a overall outline of the approach to ap-
proximate control taken with our Hybrid Hierarchical Agent
(HHA) algorithm. Consider that we have decomposed the
nonlinear dynamics into piecewise affine regions of the state-
space using an rSLDS. Should the HHA wish to navigate
to a goal specified in continuous space, the recurrent gen-
erative model parameters of the rSLDS allow it to identify
the discrete region within which the goal resides, thereby
lifting the goal into a high-level objective. The agent may
then generate a plan at a discrete level, making use of the
information-seeking bonuses that this affords. Planning
translates to specifying a sequence of abstract sub-goals.
Again using the recurrent generative model, the agent can
specify, for each sub-goal region, a continuous point in
state-space with which to drive the system into. Once in the
discrete goal region, the agent straightforwardly navigates
to the continuous goal. The following sections detail the
components of the HHA. For additional information, please
refer to Appendix. A
3.1. rSLDS(ro)
In the recurrent-only (ro) formulation of the rSLDS, the
discrete latent states zt ∈ {1, 2, ..., K} are generated as a
function of the continuous latents xt ∈ RM and the control
input ut ∈ RN via a softmax regression model
P(zt+1|xt, ut) =softmax(Wxxt + Wuut + r) (1)
whereby Wx and Wu are weight matrices with dimensions
RK×M and r is a bias of size RK. The continuous dynam-
2
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Figure 2.HHA with information-gain explored a wider range of the state-space. State-space coverage in Continuous Mountain Car
after 10,000 steps and best of 3 runs for (a) HHA with information-gain drive, (b) HHA without information gain drive and (c) randomly
sampled continuous actions baseline.
ics evolve according to a discrete linear dynamical system
indexed by zt with Gaussian diagonal noise,
xt+1|xt, ut, zt = Azt xt + Bzt ut + bzt + νt,
νt ∼ N(0, Qzt ) (2)
yt|xt = Czt xt + ωt, ωt ∼ N(0, Szt ) (3)
and identity emissions model with Gaussian diagonal noise.
In order to learn the rSLDS parameters using Bayesian up-
dates, conjugate matrix normal inverse Wishart (MNIW)
priors are placed on the parameters of the dynamical system
and recurrence weights. Inference requires approximate
methods given that the recurrent connections break conju-
gacy rendering the conditional likelihoods non-Gaussian.
Details of the Laplace Variational Expectation Maximisation
algorithm used is detailed in (Zoltowski et al., 2020).
3.2. Discrete planner
We have a Bayesian Markov Decision Process (MDP) (Vlas-
sis et al., 2012) described by MB = (S, A, Pa, R, Pθ). S
represents the set of all possible discrete states of the system
and are essentially a re-description of the discrete latents
Z found by the rSLDS. A is the set of all possible actions
which, in our case, is equal to the number of states S. The
state transition probabilities,pa(st+1 | st = s, at = a, θ) ∼
Cat(θas), and are parameterised by θ ∈ Rs×s×a for which
we maintain Dirichlet priors over, p(θas) ∼ Dir(αas), fa-
cilitating directed exploration. Due to conjugate structure,
as the agent obtains new empirical information, Bayesian
updates amount to a simple count-based update of the Dirich-
let parameters (Murphy, 2012). Importantly, the structure
of the state transition model has been constrained by the
adjacency structure of the polyhedral partitions extracted
from recurrent transition dynamics of the rSLDS: invalid
transitions are assigned zero probability while valid tran-
sitions are assigned a high probability (see A.5). R is the
reward function which, translated into the Active Inference
framework, acts as a prior distribution over rewarding states
providing the agent with an optimistic bias during policy
inference (Millidge et al., 2020; Parr et al., 2022).
The discrete planner outputs a discrete action, where the
first action is taken from a receding horizon optimisation:
a0 = arg min
a1:T
J(a1:T ) (4)
J(a1:T ) =E[
TX
t=0
R(st, at) +IGt(α) | s0, a1:T ]. (5)
This includes an explicit information-seeking incentive
IGt(α) (see A.4). This descending discrete action a0 is
translated into a continuous control prior xj via the follow-
ing link function,
xj = arg max
x
P(z = j|x, u) (6)
which represents an approximately central point in the de-
sired discrete region j requested by action a0 (see A.6). The
ascending messages from the continuous level are translated
into a categorical distribution via the rSLDS softmax link
function. Importantly, the discrete planner is only triggered
when the system switches into a new mode 1. In this sense,
discrete actions are temporally abstracted and decoupled
from continuous clock-time in a method reminiscent of the
options framework (Sutton et al., 1999; Daniel et al., 2016).
3.3. Continuous controller
Continuous closed-loop control is handled by a finite-
horizon linear-quadratic regulator (LQR) controller. For
controlling the transition from mode i to mode j (xi to
xj). The objective of the LQR controller is to minimise the
following quadratic cost function:
πij(x) = arg min
π
Jij(π) (7)
Jij(π) =Eπ,xi

(xS − xj)T Qf (xS − xj)
1Or a maximum dwell-time (hyperparameter) is reached.
3
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
+
S−1X
t=0
uT
t Rut

(8)
where S is the finite time horizon, Qf is the matrix that
penalises the terminal state deviation from xj and R is the
control cost where high control input is penalised such that
the controller only provides solutions within constraints (for
further discussion, see Sec. 5). The approximate closed-loop
solution to each of these sub-problems is computed offline
by taking in the parameters of the linear systems indexed by
the discrete modes and the continuous control priors acting
as reference points (see A.3).
4. Results
To evaluate the performance of our (HHA) model, we ap-
plied it to the classic control problem of Continuous Moun-
tain Car. This problem is particularly relevant for our pur-
poses due to the sparse nature of the rewards, necessitating
effective exploration strategies to achieve good performance.
The HHA is initialised according to the procedure outlined
in (Linderman et al., 2016). The rSLDS parameters are
then fitted to the observed trajectories every 1000 steps of
the environment unless a reward threshold within a single
episode is reached.
Figure 3.HHA both finds the reward and captilises on its expe-
rience significantly quicker than other model-free RL baselines.
Average reward (+/- std) over 6 runs for Continuous Mountain Car
(20 episodes, max episode length of 200 steps) for HHA (our
model), Soft-Actor Critic (with 2 Q-functions), and Actor-Critic
models. Note that after 20 episodes, SAC and AC are yet to find
the reward and converge on a solution.
We find that the HHA finds piecewise affine approximations
of the task-space and uses these discrete modes effectively
to solve the task. Fig.1 shows that while the rSLDS has
divided up the space according to position, velocity and
control input, the useful modes for solving the task are
those found in the position space. Once the goal and a
good approximation to the system has been found, the HHA
successfully and consistently navigates to the reward.
Fig. 2 shows that the HHA performs a comprehensive ex-
ploration of the state-space and significant gains in the state-
space coverage are observed when using information-gain
drive in policy selection compared to without. Interestingly,
even without information-gain, the area covered by the HHA
is still notably better than that of the random action control.
This is because the non-grid discretisation of the state-space
significantly reduces the dimensionality of the search space
in a behaviourally relevant way.
We compare the performance of the HHA to other reinforce-
ment learning baselines (Actor-Critic and Soft Actor-Critic)
and find that the HHA both finds the reward and captilises
on its experience significantly quicker than the other models
(see Fig. 3). Indeed, our model competes with the state-
space coverage achieved by model-based algorithms with
exploratory enhancements in the discrete Mountain Car task,
which is inherently easier to solve (see A.8).
5. Discussion
Through the application of our Hybrid Hierarchical Agent
to the Continuous Mountain Car problem, we have demon-
strated that rSLDS representations hold promise for enrich-
ing planning and control. The emergence of non-grid dis-
cretisations of the state-space allows us to perform fast sys-
tems identification via enhanced exploration, and successful
non-trivial planning through the delineation of abstract sub-
goals. Hence, the time spent exploring each region is not
equivalent in euclidean space which helps mitigate the curse
of dimensionality that other grid-based methods suffer from.
Such a piecewise affine approximation of the space will
incur some loss of optimality in the long run when pitted
against black-box approximators. This is due to the nature of
caching only approximate closed-loop solutions to control
within each piecewise region, whilst the discrete planner im-
plements open-loop control. However, this approach eases
the online computational burden for flexible re-planning.
Hence in the presence of noise or perturbations within a
region, the controller may adapt without any new compu-
tation. This is in contrast to other nonlinear model-based
algorithms like model-predictive control where reacting to
disturbances requires expensive trajectory optimisation at
every step (Schwenzer et al., 2021). By using the piece-
wise affine framework, we maintain functional simplicity
and interpretability through structured representation. This
method is amenable to future alignment with a control-
theoretic approach to safety guarantees for ensuring robust
system performance and reliability.
We acknowledge there may be better solutions to dealing
with control input constraints than the one given in Sec. 3.3.
Different approaches have been taken to the problem of im-
plementing constrained-LQR control, such as further piece-
wise approximation based on defining reachability regions
for the controller (Bemporad et al., 2002).
4
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Abdulsamad, H. and Peters, J. Hierarchical decomposition
of nonlinear dynamics and control for system identifica-
tion and policy distillation. In Bayen, A. M., Jadbabaie,
A., Pappas, G., Parrilo, P. A., Recht, B., Tomlin, C., and
Zeilinger, M. (eds.), Proceedings of the 2nd Conference
on Learning for Dynamics and Control, volume 120 of
Proceedings of Machine Learning Research, pp. 904–914.
PMLR, 10–11 Jun 2020.
Abdulsamad, H. and Peters, J. Model-based reinforce-
ment learning via stochastic hybrid models. IEEE Open
Journal of Control Systems , 2:155–170, 2023. doi:
10.1109/OJCSYS.2023.3277308.
Bemporad, A., Borrelli, F., and Morari, M. Piecewise linear
optimal controllers for hybrid systems. In Proceedings of
the 2000 American Control Conference. ACC (IEEE Cat.
No.00CH36334), volume 2, pp. 1190–1194 vol.2, 2000.
doi: 10.1109/ACC.2000.876688.
Bemporad, A., Morari, M., Dua, V ., and Pistikopoulos, E. N.
The explicit linear quadratic regulator for constrained
systems. Automatica, 38(1):3–20, 2002. ISSN 0005-1098.
doi: https://doi.org/10.1016/S0005-1098(01)00174-1.
Borrelli, F., Bemporad, A., Fodor, M., and Hrovat, D. An
mpc/hybrid system approach to traction control. IEEE
Transactions on Control Systems Technology, 14(3):541–
552, 2006. doi: 10.1109/TCST.2005.860527.
Coulom, R. Efficient selectivity and backup operators in
monte-carlo tree search. In van den Herik, H. J., Ciancar-
ini, P., and Donkers, H. H. L. M. J. (eds.),Computers and
Games, pp. 72–83, Berlin, Heidelberg, 2007. Springer
Berlin Heidelberg. ISBN 978-3-540-75538-8.
Daniel, C., van Hoof, H., Peters, J., and Neumann, G.
Probabilistic inference for determining options in re-
inforcement learning. Machine Learning, 104(2):337–
357, September 2016. ISSN 1573-0565. doi: 10.1007/
s10994-016-5580-x.
d’Avila Garcez, A. and Lamb, L. C. Neurosymbolic ai: the
3rd wave. Artificial Intelligence Review, 56(11):12387–
12406, November 2023. ISSN 1573-7462. doi: 10.1007/
s10462-023-10448-w.
Friston, K. J., Costa, L. D., Tschantz, A., Kiefer, A., Sal-
vatori, T., Neacsu, V ., Koudahl, M., Heins, C., Sajid,
N., Markovic, D., Parr, T., Verbelen, T., and Buck-
ley, C. L. Supervised structure learning, 2023. URL
https://arxiv.org/abs/2311.10300.
Gobet, F., Lane, P., Croker, S., Cheng, P., Jones, G., Oliver,
I., and Pine, J. Chunking mechanisms in human learning.
Trends in cognitive sciences, 5:236–243, 07 2001. doi:
10.1016/S1364-6613(00)01662-4.
Gou, S. Z. and Liu, Y . DQN with model-based exploration:
efficient learning on environments with sparse rewards.
CoRR, abs/1903.09295, 2019.
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K.,
Couzin, I., and Tschantz, A. pymdp: A python library for
active inference in discrete state spaces. arXiv preprint
arXiv:2201.03904, 2022.
LaValle, S. M. Planning Algorithms, chapter 2. Cambridge
University Press, Cambridge, 2006.
Liberzon, D. Switching in Systems and Control. Systems &
Control: Foundations & Applications. Birkh¨auser Boston,
2003. ISBN 9780817642976.
Linderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M.,
Paninski, L., and Johnson, M. J. Recurrent switching
linear dynamical systems, October 2016.
Millidge, B., Tschantz, A., Seth, A. K., and Buckley, C. L.
On the relationship between active inference and control
as inference, 2020.
Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidje-
land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik,
A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
Legg, S., and Hassabis, D. Human-level control through
deep reinforcement learning. Nature, 518:529–533, 2015.
Murphy, K. P.Machine learning: a probabilistic perspective.
MIT press, 2012.
Newell, A. and Simon, H. A. Human Problem Solving .
Prentice-Hall, Englewood Cliffs, NJ, 1972.
OpenAI. Continuous mountain car environment, 2021. Ac-
cessed: 2024-05-25.
Parr, T., Pezzulo, G., and Friston, K. Active Inference: The
Free Energy Principle in Mind, Brain, and Behavior. MIT
Press, 2022. ISBN 9780262045353.
Schwenzer, M., Ay, M., Bergs, T., and Abel, D. Review
on model predictive control: an engineering perspective.
The International Journal of Advanced Manufacturing
Technology, 117(5):1327–1349, November 2021. ISSN
1433-3015. doi: 10.1007/s00170-021-07682-3.
5
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Sutton, R. S., Precup, D., and Singh, S. Between mdps
and semi-mdps: A framework for temporal abstraction
in reinforcement learning. Artificial Intelligence, 112(1):
181–211, 1999. ISSN 0004-3702. doi: https://doi.org/10.
1016/S0004-3702(99)00052-1.
Vlassis, N., Ghavamzadeh, M., Mannor, S., and Poupart, P.
Bayesian Reinforcement Learning, pp. 359–386. Springer
Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-
3-642-27645-3. doi: 10.1007/978-3-642-27645-3 11.
Zoltowski, D. M., Pillow, J. W., and Linderman, S. W. Uni-
fying and generalizing models of neural dynamics during
decision-making, 2020.
6
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
A. Appendix / supplemental material
A.1. Framework
Optimal Control
We adopt the optimal control framework, specifically we consider discrete time state space dynamics of the form:
xt+1 = f(xt, ut, ηt) (9)
with known initial condition x0, and ηt drawn from some time invariant distribution ηt ∼ D, where f we assume
p(xt+1 | xt, ut) is a valid probability density throughout.
We use ct : X × U → R for the control cost function at time t and let U be the set of admissible (non-anticipative,
continuous) feedback control laws, possibly restricted by affine constraints. The optimal control law for the finite horizon
problem is given as:
J(π) =Ex0,π[
TX
t=0
ct(xt, ut)] (10)
π∗ = arg min
π∈U
J(π) (11)
PW A Optimal Control
The fact we do not have access to the true dynamical system f motivates the use of a piecewise affine (PW A) approximation.
Also known as hybrid systems:
xt+1 = Aixt + Biut + ϵt (12)
when (xt, ut) ∈ Hi (13)
Where H = {Hi : i ∈ [K]} is a polyhedral partition of the space X × U.
In the case of a quadratic cost function, it can be shown the optimal control law for such a system is peicewise linear. Further
there exist many completeness (universal approximation) type theorems for peicewise linear approximations implying
if the original system is controllable, there will exist a peicewise affine approximation through which the system is still
controllable (Bemporad et al., 2000; Borrelli et al., 2006).
Relationship to rSLDS(ro)
We perform a canonical decomposition of the control objectiveJ in terms of the components or modes of the system. By
slight abuse of notation [xt = i] := [(xt, ut) ∈ Hi] represent the Iverson bracket.
J(π) =
X
t
Z
pπ(xt | xt−1, ut)ct(xt, ut)dxtdxt−1 (14)
=
X
t
Z X
i∈[K]
[xt−1 = i]pπ(xt | xt−1, ut)ct(xt, ut)dxtdxt−1 (15)
(16)
Now let zt be the random variable on [K] induced by Zt = i if [xt = i] we can rewrite the above more concisely as,
J(π) =
X
t
Z X
i∈[K]
pπ(xt, zt−1 = i | xt−1, ut)ct(xt, ut)dxtdxt−1 (17)
=
X
i∈[K]
X
t
Z
pπ(xt, zt−1 = i | xt−1, ut)ct(xt, ut)dxtdxt−1 (18)
=
X
i∈[K]
X
t
Eπi [ct(xt, ut)] (19)
7
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
(20)
which is just the expectation under a recurrent dynamical system with deterministic switches. Later (see A.4), we exploit the
non-deterministic switches of rSLDS in order to drive exploration.
A.2. Hierarchical Decomposition
Our aim was to decouple the discrete planning problem from the fast low-level controller. In order to break down the control
objective in this manner, we first create a new discrete variable which simply tracks the transitions of z, this allows the
discrete planner to function in a temporally abstracted manner.
Decoupling from clock time Let the random variable (ζs)s>0 record the transitions of (zt)t>0 i.e. let
τs(τs−1) = min{t : zt+1 ̸= zt, t > τs−1}, τ0 = 0 (21)
be the sequence of first exit times, then ζ is given by ζs = zτs .
With these variables in hand, we frame a small section of the global problem as a first exit problem.
Low level problem Consider the first exit problem defined by,
πij(x0) = arg min
π,S
Jij(π, x0, S) (22)
Jij(π, x0, S) =Eπ,x0 [
SX
t=0
c(xt, ut)] (23)
s.t. (xt, ut) ∈ Hi (24)
s.t. c(x, u) = 0when (x, u) ∈ ∂Hij (25)
where ∂Hij is the boundary Hi
THj.
Due, to convexity of the polyhedral partition, the full objective admits the decomposition into subproblems
J(π) =
X
s
Jζ(s+1),ζ(s)(π) (26)
(27)
Slow and fast modes The goal is to tackle the decomposed objectives individually, however the hidden constraint that the
trajectories line up presents a computational challenge. Here we make the assumption that the difference in cost induced
by different starting positions, induces a relatively small change in the minimum cost Jij, intuitively this happens if the
minimum state cost in each mode is relatively uniform as compared to the difference between regions.
High level problem If the above assumption holds, we let J∗
ij = minπ
R
x0
Jij(π, x0)p(x0) be the average cost of each
low-level problem. We form a markov chain:
pik(u) =P(ζs+1 = k | ζs = i, π∗
ij, ud = j) (28)
and let pπd be the associated distribution over trajectories induced by some discrete state feedback policy, along with the
discrete state action cost cd(ud = j, η= i) =J∗
ij we may write the high level problem:
π∗
d = min
πd
Jd(π, η0) (29)
= Epπd [
SX
s=0
cd(ηs, ud
s)] (30)
Our approximate control law is then given by π∗
ij ◦ π∗
d ◦ id(x)
8
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
A.3. Offline Low Level Problems: Linear Quadratic Regulator (LQR)
Rather than solve the first-exit problem directly, we formulate an approximate problem by finding trajectories that end at
specific ‘control priors’ (see A.6). Recall the low level problem given by:
πij(x0) = arg min
π,S
Jij(π, x0, S) (31)
Jij(π, x0, S) =Eπ,x0 [
SX
t=0
c(xt, ut)] (32)
s.t. (xt, ut) ∈ Hi (33)
s.t. c(x, u) = 0when (x, u) ∈ ∂Hij (34)
In order to approximate this problem with one solvable by a finite horizon LQR controller, we adopt a fixed goal state,
x∗ ∈ Hj. Imposing costs ct(xt, ut) =uT
t Rut and cS(xS, uS) = (x − x∗)Qf (x − x∗). Formally we solve,
πij(x0) = arg min
π,S
Jij(π, x0, S) (35)
Jij(π, x0, S) =Eπ,x0 [(xS − x∗)T Qf (xS − x∗) +
S−1X
t=0
uT
t Rut] (36)
(37)
by integrating the discrete Ricatti equation backwards. Numerically, we found optimising over different time horizons
made little difference to the solution, so we opted to instead specify a fixed horizon (hyperparameter). These solutions are
recomputed offline every time the linear system matrices change.
Designing the cost matrices Instead of imposing the state constraints explicitly, we record a high cost which informs the
discrete controller to avoid them. In order to approximate the constrained input we choose a suitably large control cost
R = rI. We adopted this approach for the sake of simplicity, potentially accepting a good deal of sub-optimality. However,
we believe more involved methods for solving input constrained LQR could be used in future, e.g. (Bemporad et al., 2000),
especially because we compute these solutions offline.
A.4. Online high level problem
The high level problem is a discrete MDP with a ‘known’ model, so the usual RL techniques (approximate dynamic
programming, policy iteration) apply. Here, however we choose to use a model-based algorithm with a receding horizon
inspired by Active Inference, allowing us to easily incorporate exploration bonuses.
Let the Bayesian MDP be given by MB = (S, A, Pa, R, Pθ) be the MDP, where pa(st+1 | st, at, θ) ∼ Cat(θas) and
p(θas) ∼ Dir(α) We estimate the open loop reward plus optimistic information theoretic exploration bonuses
Active Inference conversion We adopt the Active Inference framework for dealing with exploration. Accordingly we adopt
the notation ln ˜p(st, at) =R(st, at) and refer to this ‘distribution’ as the goal prior (Millidge et al., 2020), and optimise
over open loop policies π = (a0, ..., aT ).
J(a1:T , s0) =E[
TX
t=0
R(st, at) +IGp + IGs | s0, a1:T ] (38)
where parameter information-gain is given by IGp = DKL[pt+1(θ) || pt(θ)], with pt(θ) =p(θ | s0:t). In other words, we
add a bonus when we expect the posterior to diverge from the prior, which is exactly the transitions we have observed least
(Heins et al., 2022).
We also have a state information-gain term, IGs = DKL[pt+1(st+1) || pt(st+1)]. In this case (fully observed),
pt+1(st+1) = δs is a one-hot vector. Leaving the term Et[−ln pt(st+1)] leading to a maximum entropy term (Heins
et al., 2022).
We calculate the above with Monte Carlo sampling which is possible due to the relatively small number of modes. Local
approximations such as Monte Carlo Tree Search could easily be integrated in order to scale up to more realistic problems.
9
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Alternatively, for relatively stationary environments we could instead adopt approximate dynamic programming methods for
more habitual actions.
A.5. Extracting the adjacency matrix from rSLDS
In order to generate the possible transitions from the rSLDS, we calculate the set of active constraints for each region from
the softmax representation, p(z | x) = σ(W x+ b). Specifically to check region i is adjacent to region j we verify the
solution linear program:
−bj = min(Wi − Wj)x (39)
s.t. (Wi − Wk)x ≤ (bi − bk) ∀k ∈ [K] (40)
s.t. x ∈ (xlb, xub) (41)
Where (xlb, xub) are bounds chosen to reflect realistic values for the problem. This ensures we only lift transitions to the
discrete model, if they are possible. Again, these can be calculated offline.
We initialise the entries of the transition model in the discrete MDP for possible transitions to 0.9 facilitating guided-
exploration via information-gain through a count-based updates to the transition priors.
A.6. Generating continuous control priors
In order to generate control priors for the LQR controller which correspond to each of the discrete states we must find a
continuous state xi which maximises the probability of being in a desired z:
xi = arg max
x
P(z = i|x, u) (42)
For this we perform a numerical optimisation in order to maximise this probability. Consider that this probability distribution
P(z = i|x) is a softmax function for the i-th class is defined as:
σ(vi) = exp(vi)P
j exp(vj), vi = wi · x + ri (43)
where wi is the i-th row of the weight matrix, x is the input and ri is the i-th bias term. The update function used in the
gradient descent optimisation can be described as follows:
x ← x + η∇xσ(vi) (44)
where η is the learning rate and the gradient of the softmax function with respect to the input vector x is given by:
∇xσ(vi) =∂σ(vi)
∂v · ∂v
∂x = σ(vi)(ei − σ(v)) · W (45)
in which σ(v) is the vector of softmax probabilities, and ei is the standard basis vector with 1 in the i-th position and 0
elsewhere. The gradient descent process continues until the probability P(z = i|x) exceeds a specified threshold θ which we
set to be 0.7. This threshold enforces a stopping criterion which is required for the cases in which the region z is unbounded.
10
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
A.7. Model-free RL baselines
A.7.1. S OFT-ACTOR CRITIC WITH 2 Q- FUNCTIONS
Table 1.Summary of the Soft Actor-Critic algorithm with multiple Q-functions.
COMPONENT INPUT
Q-NETWORK 3×256×256×256×2
POLICY NETWORK 2×256×256×256×2
ENTROPY REGULARIZATION COEFF 0.2
LEARNING RATES (QNET + POLNET ) 3 E-4
BATCHSIZE 60
A.7.2. A CTOR -CRITIC
Table 2.Summary of the Actor-Critic algorithm
COMPONENT INPUT
FEATURE PROCESSING STANDARD SCALER , RBF K ERNELS (4 × 100)
VALUE -NETWORK 4001 PARAMETERS (1 DENSE LAYER )
POLICY NETWORK 802 PARAMETERS (2 DENSE LAYERS )
GAMMA 0.95
LAMBDA 1E-5
LEARNING RATES (POLICY + VALUE ) 0.01
A.8. Model-based RL baseline
A.8.1. A DEEP Q-N ETWORK WITH MODEL -BASED EXPLORATION (DQN-MBE)
(a) HHA (our model) on Continuous Mountain Car
 (b) DQN-MBE on Discrete Mountain Car
Figure 4.On Continuous Mountain Car, our model (HHA) competes with the state-space coverage achieved by model-based
baselines on Discrete Mountain Car (an easier problem) State-space coverage after 10,000 timesteps on (a) Continuous Mountain Car
task using our model (HHA) and (b) Discrete Mountain Car task using a Deep Q-Network with Model-Based Exploration (DQN-MBE)
(Gou & Liu, 2019). Exact parameters in Table 3.
11
Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical Planning and Control
Table 3.Summary of DQN-MBE algorithm (Gou & Liu, 2019)
COMPONENT INPUT
Q-NETWORK 1 HIDDEN -LAYER , 48 UNITS , RELU
DYNAMICS PREDICTOR NETWORK (FULLY CONNECTED ) 2 HIDDEN -LAYERS (EACH 24 U NITS ), R ELU
ϵ MINIMUM 0.01
ϵ DECAY 0.9995
REWARD DISCOUNT 0.99
LEARNING RATES (QNET / DYNAMICS -NET ) 0.05 / 0.02
TARGET Q-NETWORK UPDATE INTERVAL 8
INITIAL EXPLORATION ONLY STEPS 10000
MINIBATCH SIZE (Q- NETWORK ) 16
MINIBATCH SIZE (DYNAMICS PREDICTOR NETWORK ) 64
NUMBER OF RECENT STATES TO FIT PROBABILITY MODEL 50
12