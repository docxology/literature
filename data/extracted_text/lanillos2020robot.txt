Robot self/other distinction: active inference meets neural
networks learning in a mirror
Pablo Lanillos1 and Jordi Pages2 and Gordon Cheng3
Abstract. Self/other distinction and self-recognition are important
skills for interacting with the world, as it allows humans to differ-
entiate own actions from others and be self-aware. However, only a
selected group of animals, mainly high order mammals such as hu-
mans, has passed the mirror test, a behavioural experiment proposed
to assess self-recognition abilities. In this paper, we describe self-
recognition as a process that is built on top of body perception uncon-
scious mechanisms. We present an algorithm that enables a robot to
perform non-appearance self-recognition on a mirror and distinguish
its simple actions from other entities, by answering the following
question: am I generating these sensations? The algorithm combines
active inference, a theoretical model of perception and action in the
brain, with neural network learning. The robot learns the relation be-
tween its actions and its body with the effect produced in the visual
ﬁeld and its body sensors. The prediction error generated between
the models and the real observations during the interaction is used to
infer the body conﬁguration through free energy minimization and to
accumulate evidence for recognizing its body. Experimental results
on a humanoid robot show the reliability of the algorithm for differ-
ent initial conditions, such as mirror recognition in any perspective,
robot-robot distinction and human-robot differentiation.
1 Introduction
“Our bodies are our gardens, to the which our wills are gar-
deners. ” Othello 1603. William Shakespeare.
Humans perceive their body in a ﬂexible manner [5] but main-
taining their identity [12]. Around the age of two, the majority of
infants are able to pass the self-recognition mirror test [31], like ele-
phants, dolphins and some non-human primates [2]. This experiment
has been argued to be hardly intertwined with self-awareness and so-
cial understanding [14] and thus consciousness. Recently, a cleaner
wrasse ﬁsh reopened the debate about the suitability of this test for
evaluating self-awareness [20]. As reported by the authors, it was the
ﬁrst ﬁsh to pass the mirror test, waving the notion of consciousness
by suggesting different evolutionary paths and showing a broad spec-
trum of self-awareness.
Self-recognition is commonly understood as a reﬂective process,
i.e. you are aware and able to think about it. However, it can be
treated in two different levels [35]: the sensorimotor [25] and the
cognitive [2]. Regarding the ﬁrst level, we hypothesize that the mir-
ror test can be partially passed without being self-conscious, open-
1 Accepted at European Conference on Artiﬁcial Intelligence (ECAI 2020)
1 Donders Institute for Brain, Cognition and Behaviour, the Netherlands.
2 PAL robotics, Pujades 77-79, 4-4 08005 Barcelona, Spain.
3 Institute for Cognitive Systems, Technical University of Munich, Arcis-
strasse 21, 80333 Munich, Germany.
ing the door for artiﬁcial agents to self-recognize and to acquire the
skill of self/other distinction. Actually, underneath self-recognition,
we need body perception and action processes, and the ﬁrst step to
achieve it is contingency-learning [23]. According to [4], humans can
learn and predict the consequence of actions and these forward mod-
els are used to determine the source of the sensory events. Thus, in
this work we extrapolated this concept to robots. The agent should
be able to answer the question “is this my body?” by answering “am
I generating those effects in the world?”.
We present an algorithm that enabled a robot with non-appearance
self-recognition capabilities, by addressing the relation between for-
ward models prediction error and its use for determining the source
of the sensory cues. In other words, the artiﬁcial agent can self-
recognize through accumulating evidence of producing effects in the
world by self-generated actions. For that purpose, the robot needs to
learn the expected sensory effect of: the changes in the visual ﬁeld
when it moves the body and where its body (e.g. hand) will appear
in the visual ﬁeld when it is in front of the mirror.
Previous works on artiﬁcial self-recognition, such as [14, 23, 34],
have been hardly criticized [1]: “Merely simulating certain features
of self-recognition through training/programming does not mean that
the underlying mechanisms are the same, similar, or even remotely
related”. Actually, we agree that experimental simpliﬁcations make
debatable that robots are currently able to self-recognize in the same
way that chimpanzees do. Therefore, instead of addressing the self-
perception awareness level, we focused on body sensorimotor learn-
ing and integration and studying how these mechanisms aid in self-
recognition and self/other distinction.
Relation to other approaches
Artiﬁcial intelligence approaches for self-recognition are quite far
from the biological plausibility [13, 24]. However, some of them are
already addressing relevant characteristics of self-perception such as
timing, spatial correlations and contingency.
On the one hand, we have methods that do not involve learning.
In [14] they used Bayesian inference to accumulate evidence. How-
ever, action and perception had to be binary variables and required
continuous stable segmentation of the body as an entity, complicating
its scalability to real scenarios. In [23] this approach was extended to
images and its relation with proprioception. The method depended
on the skin accelerometers and the image grid subsampling. On the
other hand, contingency is possible to be learnt with spiking neural
networks [29] and self/other distinction can be developed by gradu-
ally increasing spatiotemporal resolution [26].
Our approach involves both inference and learning and casts
self/other distinction and self-recognition to free energy minimiza-
tion (i.e. variational inference), in order to solve the scalability and
arXiv:2004.05473v1  [cs.RO]  11 Apr 2020
Figure 1. Self-recognition in the mirror and self/other distinction experiments
partial information sensitivity limitations of previous approaches.
Predictive coding approach
Under the predictive coding [30] approach, the robot is able to
learn (approximate) generative models of the world, especially the
effects of the body in the world. These prediction errors are used
for inferring the body state but they are also an indirect measure to
distinguish the robot generated actions.
Deﬁning the brain as an inference machine [19] that is able to
interpret the world from partial information [9, 15], body percep-
tion is presented here as an unconscious process that adapts em-
pirical information to current learned models. Based on the active
inference paradigm [11, 27], we formalized body perception and ac-
tion as an optimization problem that minimizes the prediction error,
i.e., the error between the expected sensory effect and the observed
one [10]. According to active inference, adaptation can be performed
by changing the interpretation or belief or by acting to modify the
world according to our expectations. Thus, we deployed this adap-
tation mechanism on a humanoid robot and studied the perceptual
response regarding self-recognition when executing movements in
front of a mirror or in front of other agents.
Contribution and organization
This work presents a novel active inference computational model
that combines free energy optimization from [27] with function
learning from [22]. The learning was performed by means of a Mixed
Density Networks, MDN [3], to provide scalability and interpolation.
The algorithm enables a robot to recognize itself in the mirror using
non-appearance body cues and to distinguish its body from other en-
tities. Our approach, as it incorporates learning, is robust to variations
in the robot position and the mirror location and angle. On top of this
model, we were able to compute the probability that our robot with a
given set of parameters would generate a particular observed sensor
value. The accumulation of this evidence yielded to self-recognize
itself in the mirror and perform self/other distinction.
First, Section 2 describes the experimental setup and the com-
putational model. Section 3 shows the results and Section 4 ana-
lyze the relevance of the results with respect to robotics and artiﬁ-
cial intelligence, and discusses its limitations regarding animal self-
recognition.
2 Methods
2.1 Experimental design
Figure 1 shows the humanoid robot TIAGo [28] and the three differ-
ent scenarios tested: in front of the mirror, robot-robot interaction and
human-robot differentiation. TIAGo is a mobile manipulator by PAL
Robotics that has been designed in a modular way. It is composed
of a differential drive base; a lifting torso; a 7 DoF arm and a pan-
tilt head. The robot has a laser rangeﬁnder on the base and a RGBD
camera on the head. For the experiments, we only used the monocular
RGB camera as 3D computations are distorted in the mirror setting.
In terms of manipulation capabilities, it has a 7 DoF arm and the lift-
ing torso prismatic joint, being able to reach objects on the ﬂoor up
to objects on shelves at around 1.75 m. For the self/other distinction
experiment, only the arm will be used. Moreover, the robot has stereo
microphones speakers and professional text-to-speech software. Fig-
ure 2 shows the main modular components of the robot used.
Figure 2. TIAGo robot used for the experiments
2.2 Overall algorithm
The algorithm 1 summarizes the whole process computed in the
robot for self-recognition and self/other differentiation. First, the vi-
sual input of the system is segmented using optical ﬂow to extract
the visual 2D centroid of any visual blob and the histogram of vi-
sual movement directions. This data will be used as the visual input
sv and for computing the probability of being a contingent event re-
spectively. The predicted sensor values are computed by means of an
MDN and its partial derivatives with respect to the latent space (i.e.
joint angles). Then the error between the predicted and the current
sensor input (e.g. visual errorev) is computed and added into the dif-
ferential equation weighted by its relevancy (i.e. variance Σv). The
goal attractor dynamics are also computed for the left and the right
condition. Finally, the probability of being generating that sensor in-
put is computed.
If the error between the prediction and the observed values is too
high (outlier), a learning process starts, where the robot retrain the
MDN with the new input. The outcome depends on three conditions.
(1) When there are no contingent effects the robot is not able to learn
anything and therefore it is not itself. (2) When it is able to learn an
effect but then the model cannot properly approximate the sensory
effect then it is not itself either. Finally, (3) if it is able to learn the
effect and the error is enough small then it will accumulate evidence
until it will properly answer: it is me.
Algorithm 1Self/other distinction algorithm
Require: Σp,Σv,Σµ,∆t LOGMIN, LOGMAX
1: µ←Initial joints angle estimation
2: µ′,v, L← 0
3: while ¬recognized(pself) do
4: h←of(I),I ←camera ⊿Optical ﬂow histogram
5: p(cont) ←sigmoid(BCNN.forward(a,h)) ⊿Sec. 2.3.3
6: g,µ∗,Σ∗←MDN.forward(µ) ⊿Predictor Sec. 2.3.2
7: ∂µg←MDN.backward(µ∗,Σ∗) ⊿Sec. 2.3.2
8: ˙µ←0 ⊿Active Inference. Sec 2.3.1
9: if ∃sp then ⊿Proprioception
10: ep = (sp −µ)
11: ˙µ= ˙µ+ ep/Σp
12: ˙a= ˙a−(ep/Σp)∆t
13: if ∃sv then ⊿Vision
14: ev = (sv −g)
15: ˙µ= ˙µ+ ∂µgTev/Σv
16: ˙a= ˙a−(∂µgTev/Σv)∆t
17: if ∃sρ then ⊿Goal attractor ρdynamics
18: ef = (µ′−f(µ,ρ))
19: ˙µ= ˙µ+ ∂µfTef/Σµ
20: µ= µ+ ∆t˙µ ⊿ 1st order Euler integration
21: a= a+ ∆t˙a
22: if ∃sv then ⊿Self-recognition Sec. 2.3.4
23: Li = −0.5(eT
vΣ−1
v ev + ln det Σv + nln 2π)
24: L= L+ Li + lnp(cont)
25: pnorm=(exp(L)−LOGMIN) /(LOGMAX−LOGMIN)
26: pself = pself + K(pnorm −pself)
27: if outlier then
28: while i<N s ∧¬timeout do
29: if p(cont) >δ then ⊿Learning Sec. 2.3.3
30: Qx.append(sp); Qy.append(sv)
31: if i>N s then
32: MDN.train(Qx,Qy)
2.3 Detailed computational model
We describe in detail the body perception and action model and
then the method for self/other distinction. We deﬁne body percep-
tion as inferring joint angles of the robot armµby means of different
sensory modalities. The robot approximates the belief of his body
by means of minimizing the free energy [10], computing the most
plausible solution of his arm location using the error between the
predicted sensory input and the observed one. Mathematically, per-
ception and action are interpreted by the following two equations
Figure 3. Body perception model and notation. (Left) Generative model of
the robot perception and action. (Right) Mirror and robot setup.qare the joint
measurements and µ,µ′is the inferred angle of the joints and its ﬁrst-order
dynamics. vis the visual location of the hand and ρthe goal. ai is the action
exerted in joint i.
adapted from the free energy principle approach model proposed
in [11]:
Perception µ= arg min
µ
F →˙µ= µ′−∂F(µ,ρ)
∂µ (1)
Action a= arg min
a
F →˙a= −∂F(µ,ρ)
∂a (2)
2.3.1 Active inference model
We adapted the free energy model presented in [27] to the TIAGo
robot. Using joint encoders sp and visual information sv, the graphi-
cal representation of the inference process is described in Fig. 3. The
agent encodes its belief of its body µwith the following density [6]:
p(µ,s,ρ ) = p(sp|µ)p(sv|µ)p(µ′|µ,ρ) (3)
Where the ﬁrst two likelihoods are the sensor modality contributions
(assumed independent between each other) and the third term is the
prior density for the ﬁrst-order dynamics, which also depends on the
causal variables ρ. We will control the robot in joint velocities by
deﬁning actions aas the velocity of each joint, thus only ﬁrst-order
dynamics are needed.
Under the mean-ﬁeld Laplace approximation, the free energy
bound can be approximated as4:
F ≈−ln p({s,ρ},µ) −
[1
2 ln |Σ|+ nln 2π
]
(4)
where nis the size of µ. In order to solve Eq. 1 and 2, we computed
the gradient of F with respect to the latent variables µ,a.
∂F
∂µ = (sp −µ)
Σp
+ ∂g(µ)T
∂µ
(sv −g(µ))
Σv
+ (5)
+ ∂f(µ,ρ)T
∂µ
(µ′−f(µ,ρ))
Σµ
(6)
∂F
∂a = ∂sp
∂a
T (sp −µ)
Σp
+ ∂sv
∂a
T (sv −g(µ))
Σv
(7)
where gv(µ) is the visual generative model that is learnt (Sec. 2.3.2)
and f(µ,ρ) is the model dynamics of a perceptual attractor [27].
4 In some formulations of the free energy the sensor inputssand other causal
variables are the described by the same variable. However, in this paper, we
split them for clarity purposes.
2.3.2 Generative models learning
To solve Eq. 5 and 7 the generative models for the sensor g(µ) and
its partial derivative ∂g/∂µ should be known. Conversely to previ-
ous works on active inference we enabled the robot to learn those
models taking inspiration from our latest contributions on body per-
ception with function learning [21, 22]. However, in this case, we
used a probabilistic neural network model to provide scalability and
interpolation. The visual generative model is learnt by means of a
Mixture Density Network (MDN) [3]. In this way, we encode not
only the mapping but the uncertainty associated. In principle, the for-
ward model of the robot end-effector with respect to the joint latent
space can be learnt using just one Gaussian kernel. However, we al-
lowed the robot to learn more complex visual patterns generated by
the actions or due to the lens non-linearity. The same model can gen-
eralize to forward model learning using more complex probabilistic
deep models [7] within the free energy framework. The predicted
sensation is computed through the forward pass of the MDN and
outputs the Gaussian mixture model parameters: Πi coefﬁcients, si
mean and Σi variance. To compute the prediction of the sensation
given the state s = g(µ) we select the maximum probable kernel
and get the mean and the variance as follows:
i∗= arg max
i
softmax(WT
Πi h(µ) + bΠ) (8)
g(µ) = WT
s h(µ) + bs (9)
where h(µ) is the hidden layer output and W,b are the network
weights and bias respectively. Although for this speciﬁc visual lo-
cation learning only one hidden layer is sufﬁcient, more layers can
be added for more complex non-linear function learning.
Figure 4. Forward model learning with MDN. The forward pass gives the
prediction and the backward pass gives the partial derivative with respect to
the body conﬁguration µ.
Figure 4 describes the MDN to obtain the forward and partial
derivatives through the backward pass. Figure 5 shows an instance
of the visual forward model training for the robot in front of the mir-
ror. The negative log-likelihood converges rapidly in 400 epochs. The
blue dots correspond to the visual samples acquired when the robot
moves the arm and the red dots are generated with the learnt pre-
dictor g(µ) for random joint angles within the set of the waving left
and right movement. The interpolation power of the network shown
is really crucial for the stability of free energy optimization.
The inverse mapping or partial derivative with respect to the latent
variable is computed through the backward pass of the MDN of the
most plausible Gaussian kernel i∗.
∂g(µ)
∂µ = Pi∗
(si∗ −sv)
σ2
i∗
∂zi∗
∂µ (10)
(a) Training samples and g(µ)
 (b) MDN training loss
Figure 5. Learned mapping with the MDN from the joints positions to the
2D visual point (end-effector) computed through optical ﬂow. Visual samples
(blue) used for training and g(µ) predictor learned (red). Axis ranges that
correspond to the image dimensions have been normalized between 0 and 1
dividing by the width (640px) and the height (480px) respectively.
where zi is the neuron output, si is the mean value output of the
selected kernel, svis the current observation of the end-effector in the
visual ﬁeld and Pi∗ = Πi∗ N(si∗ ,σ2
i∗ )/∑
jΠjN(sj,σ2
j) - See [3].
In order to train the neural network we use as the input the joints
estimated angles µ and as the output the 2D centroid location of
the biggest moving optical ﬂow blob. The points that are contingent
(see Sec. 2.3.3) are buffered into a batch that is then used to mod-
ify network weights through backpropagation [18]. The loss function
used is the standard negative log-likelihood of the Gaussian mixture
model: L= −ln ∑
iΠiN(si,σ2
i).
2.3.3 Contingency learning
We assume that the robot has a system that is able to classify whether
it is a contingent effect or not. This prior knowledge can be justiﬁed
by stating that the robot has learnt the potential effect of moving the
hand left and right in front of him. In essence, the robot has never
looked in a mirror but has performed actions with the arm that result
in the following contingent effect learning: when the robot moves the
hand to the left produces a change in the visual ﬁeld with a speciﬁc
direction. We computed the effect in the visual ﬁeld given the action
(i.e. velocity) by means of an optical ﬂow algorithm [33]. To learn
to classify this effect we used a deep learning classiﬁer with input
the state of the robot (i.e. estimated joint states µ) and the velocity
being exerted (i.e. a) and the output the histogram of the optical ﬂow
velocity directions. Figure 6 shows the input, the output and the ar-
chitecture of the neural network.
Figure 6. Contingent sensory event classiﬁer. Given the histogram of the
optical ﬂow velocities of the visual ﬁeld (left), the joint states and the veloci-
ties being exerted, the deep binary classiﬁer computes whether the effects are
contingent for the left and right action.
Figure 7 shows the input data, the learning curve and the clas-
siﬁcation of samples of the test set. The joint velocities (coloured
lines) for the hand wave movement show the proﬁles of left and right
movements. The associated optical ﬂow histogram (coloured dots)
varies depending on the movement. The orientations are split into
10 bins represented with different colours and the y-axis is the nor-
malized frequency. The loss function used is the binary cross-entropy
with logistic output. Then we used a sigmoid function to compute the
probability. The optimizer was Adam with 0.01 learning rate. It is im-
portant to highlight that we had to include the non-movement case to
deal with the sensor noise. We tested, in Fig. 7(c), 100 random points
from the test set where we included: self-generated actions, Gaussian
random noise and non-movement with visual input. The contingency
probability (y-axis) is shown for all those cases. Furthermore, some
random samples have some probability of being classiﬁed as self-
generated.
(a) Joint velocities and
OF histogram
(b) Training loss
 (c) Classiﬁcation with
test data
Figure 7. Contingency classiﬁer learning. The input are the 7 joint veloci-
ties (lines) and the optical ﬂow histogram with 10 bins (dots). The output is
the probability of a image velocity vector generated by an action on the joints.
2.3.4 Self-recognition: Is that me?
In order to provide non-appearance self-recognition, we transform
the question of is that me? into did I generate those sensors values?
The robot accumulates evidence of generating the sensed effects in
the world. This is computed through the marginal likelihood or sen-
sory surprise. Highly surprising events (effects in the visual ﬁeld)
drastically drop the probability of being self. Conversely, in order to
increase the probability of being self, the effect should continuously
match the expected value generated by the learnt forward model.
We compute the probability that our model with a given set of
parameters would generate a particular observed cue in the visual
ﬁeld as:
Li = −1
2(eT
vΣ−1
v ev + ln det Σv + nln 2π) (11)
L= L+ Li + lnp(contingent) (12)
where p(contingent) is computed with the deep net classiﬁer of
Sec. 2.3.3. Inspired by [17], the probability of being itself is com-
puted by normalization and smoothing as follows:
pnorm = (exp(L) −LOGMIN) /(LOGMAX −LOGMIN) (13)
pself = pself + K(pnorm −pself) (14)
where Kis a smoothing constant working as a Kalman gain.
3 Experimental results
Two experiments were designed: self-recognition in the mirror, de-
picted in Fig. 1A, and self/other distinction with other similar robot
and a human, represented in Fig. 1B. A video describing the exper-
iments is here: https://youtu.be/3l9N972xjD8. The pa-
rameters of the MDN network were ﬁxed for all experiments: µ ∈
R7, one hidden layer with 20 units, two-dimensional output si ∈R2
(i.e. hand 2D position in the image). The kernels were Gaussian dis-
tributions N with mean si and variance σi. We assumed that the
output variables (2D location in the image) have the same noise
distribution, thus we only used one variance parameter for each
Gaussian kernel. We ﬁxed the number of Gaussian mixtures to four
(i∈(0,4)). The optimizer used was Adam [18] and the training used
batches of 200 samples and 1000 epochs.
3.1 Self-recognition in the mirror
The robot was placed in 10 random positions in front of a body size
mirror (Fig. 1A) and we ran the experiment 10 times for each po-
sition. The arm was always visible in the mirror and we did not in-
clude any prior information about the mirror. All experiments were
successfully classiﬁed as itself.
The procedure went as follows. The robot moved the hand left and
right by setting an attractor of the end-effector in the visual ﬁeld. In
other words, the desired location in the image changed from left to
right of the image centre. The attractors were active for 2 seconds
each. Then, the delay between initiating the left and right waving
changed every time randomly in the range of 3 seconds. This en-
forced that actions were not periodic. Finally, the training ﬁnished if
the number of samples was more than 200 or it reached 20 seconds.
Figure 8(a) shows the prediction error dynamics for propriocep-
tion ep and visual sv cues for one trial5. The bottom plot shows the
mean and standard deviation of the probability of being self in all tri-
als with no prior training with the mirror. The ﬁrst time that the robot
looks at the mirror and starts moving the arm, the robot is not able
to predict its hand position producing a big prediction error. Before
the training stage, the robot was not able to recognize itself. Thus, it
starts learning the relation between its body state and the visual input
by means of the MDN (Sec. 2.3.2). After learning for 10 seconds,
the robot continues moving the hand left and right until it decides
whether it is him or not. After learning the forward model the er-
ror in the visual sensation drops allowing the system to accumulate
evidence of being itself (the model is correctly generating the visual
sensations). Still, there are small prediction errors related to the adap-
tation of the current learnt model and real observations. Errors in pro-
prioception are generated by the perceptual attractor created when it
has the goal to move to the left or to the right. Any disturbance in the
expected visual sensation (surprise) dropped the probability of being
self.
3.2 Self/other distinction
For the self/other experiment, we performed the same amount of tri-
als. We faced two similar robots doing the same movements but due
to the randomness of the delays between actions they were asyn-
chronous (Fig. 1B). As well as in the previous experiment, ﬁgure
8(b) shows the error in proprioceptive and visual cues of one of the
trials. The bottom plot of Fig. 8(b) shows the probability of being self
for all trials. In this experiment, the robot could not learn anything as
there was no causality between the actions (joint velocities) and the
observed sensory information. This yielded into the big visual pre-
diction errors after the training. Therefore the robot inferred that it is
not itself.
We further tested the model with an experimenter in front
(Fig. 1B) and we observed that even when trying to perform the
5 Other trials have similar proﬁles but with different delays. Thus, they don’t
bring any further information.
(a) self-recognition in the mirror
 (b) self/other distinction
Figure 8. Self-recognition and self/other distinction experiments variable dynamics. ep and ev are the proprioceptive and visual errors. During learning, the
active inference update is disabled. The probability of being self, p(self), rises when the predicted visual effect is contingent and matches the observation.
same movements the robot was able to properly distinguish between
self-generated and other generated visual cues. In all cases, the robot
correctly identiﬁed the agent in front as not itself. However, as ex-
pected, in the case of perfect synchronization the robot will identify it
as itself. From the cognition point of view interpreting another robot
as itself seems wrong, but also humans will perceive agency when
seeing an arm that moves at every instant with correlation with their
commands to the muscles.
4 Machines able to recognize themselves
We pointed out that unconscious body perception, action and learn-
ing is underneath self-recognition. Experiments showed that contin-
gent non-appearance cues can be learned through interaction and be
used to perform self/other distinction. The proposed algorithm en-
abled the robot to robustly identify itself in the mirror just by learn-
ing the forward model, i.e., the expected visual input given the in-
ferred body state. As long as the arm appeared in the mirror, the ini-
tial conditions where irrelevant. This is in line with the experiments
with non-human primates that needed learning with the mirror before
achieving the self-recognition ability.
The robot was also able to differentiate itself from other entities.
The algorithm was sensitive to any unexpected sensations as we are
using the minimization of surprise as the key mechanism for the dif-
ferentiation. Even when trying to deceive the algorithm (e.g. robot-
human experiment), any small delay or difference dropped down the
probability of being itself. Interestingly, the variance parameters (rel-
evance of the sensory modality for inferring the body) was crucial for
dealing with the robot own noise but differentiating from other enti-
ties cues. We further observed an interesting side effect of using the
action to minimize the prediction error. The human or robot hand in
the self/other distinction experiment acted sometimes as an attractor
for the body inference, generating mirroring actions similar to imita-
tion.
In summary, with these experiments, we are showing that
self/other distinction and self-recognition is possible to be imple-
mented in artiﬁcial agents. Although it will not be a conscious cog-
nitive process as described in humans or elephants, it will have some
common ground in the sensorimotor action-perception level.
Double comparator model
Within the self-perception cognitive science literature it is com-
mon knowledge the comparator model [8]. This approach determines
that humans are aware of producing an effect in the world through
their actions (i.e. agency [16]) by means of the prediction error
congruence. Some researchers criticized the simplistic vision of the
model and proposed that contingency detection and causal inference
should be also involved as processes on top of congruence [37]. This
is similar to our model. We use both prediction error and contingency
learning. In essence, we propose the “double comparator” model in-
spired by the work of [36]. The ﬁrst comparator is the prediction error
mechanism that approximate the models learnt to reality for improv-
ing interpretation. The second comparator takes into account other
processes like spatio-temporal contingency and performs the proper
distinction. In our model, we engineered this abstract level using a
probabilistic approach, i.e., by calculating the marginal likelihood to
accumulate temporal evidence and then computing the probability of
generating that observed effect in the sensory space.
Towards passing the original mirror test
We showed how non-appearance cues can be used for self-
recognition. However, robots are still not able to pass the original
self-recognition test like animals do [2]. For instance, the robot here
did not learn the mapping between the tactile and the visual reﬂec-
tion to produce the motor response. In order to fully provide robots
with a full-ﬂedged self-recognition similar to humans, there are a set
of technical challenges that should be further addressed:
• Complex visual segmentation. The algorithm was designed for
only one moving blob segmentation using optical ﬂow. Complex
inputs with multiple causal sources need improved segmentation.
• Large scale input data. Forward models learning of the end-
effector location in the image should be scaled to real sensory
input data, such as images.
• Whole body interactions. We used the arm for the experiments as
a proof-of-concept. The same approach can be extended to whole-
body self/other distinction.
• Other cues. The proprioceptive-visual tandem is just one of the
sensory inputs involved in self-recognition. We did not model ap-
pearance, tactile or auditory cues. We considered movement con-
tingency as the ﬁrst level of self-distinction but then an appearance
model similar to the body image should be included.
• Biological plausibility. Although the mathematical abstraction of
body perception has strong neuroscience-inspired notions, using
the marginal likelihood for accumulating evidence does not have
a direct relation with the mechanism observed in the brain. How-
ever, it has roots into the model evidence approach under the
Bayesian brain assumption.
5 Conclusion
We proposed an algorithm for self/other distinction using non-
appearance sensory cues by combining free energy optimization with
neural network learning. The approach showed the importance of not
only learning the models of the body but also approximating those
models to the observed reality. After learning the humanoid robot
was able to accumulate evidence during the generated movements
to discern whether it was itself or other entity in different scenar-
ios. This reliability is due to the explicit handling of the sensorimo-
tor uncertainty in the proposed model, while still performing robust
self-differentiation. When the expected sensation was similar to the
sensor values, i.e. the prediction error was low, the evidence of be-
ing itself was accumulated. Conversely, any prediction error dropped
down the probability of being itself yielding to self/other distinc-
tion. This work stresses that self-recognition and self/other distinc-
tion grounds on the body perception and action model and that it is
possible to give artiﬁcial agents, such as robots, these capabilities.
Further work will focus on scaling to raw images [32] and including
appearance cues.
ACKNOWLEDGEMENTS
This work was supported by the MSCA SELFCEPTION project
(www.selfception.eu) EU H2020 grant no. 741941 and PAL robotics.
REFERENCES
[1] James R Anderson and Gordon G Gallup, ‘Mirror self-recognition: a re-
view and critique of attempts to promote and engineer self-recognition
in primates’,Primates, 56(4), 317–326, (2015).
[2] James R Anderson and Gordon G Gallup Jr, ‘Which primates recognize
themselves in mirrors?’,PLoS Biology, 9(3), e1001024, (2011).
[3] Christopher M Bishop, ‘Mixture density networks’, Technical report,
(1994).
[4] Sarah-Jayne Blakemore and Jean Decety, ‘From the perception of ac-
tion to the understanding of intention’, Nat. Rev. Neurosci., 2(8), 561,
(2001).
[5] Matthew Botvinick and Jonathan Cohen, ‘Rubber hands ‘feel’touch that
eyes see’,Nature, 391(6669), 756, (1998).
[6] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K
Seth, ‘The free energy principle for action and perception: A mathe-
matical review’,Journal of Mathematical Psychology, (2017).
[7] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey
Levine, ‘Deep reinforcement learning in a handful of trials using proba-
bilistic dynamics models’, inAdvances in Neural Information Process-
ing Systems, pp. 4754–4765, (2018).
[8] Nicole David, Albert Newen, and Kai V ogeley, ‘The “sense of agency”
and its underlying cognitive and neural mechanisms’, Consciousness
and cognition, 17(2), 523–534, (2008).
[9] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S
Zemel, ‘The helmholtz machine’,Neural computation, 7(5), 889–904,
(1995).
[10] Karl Friston, ‘The free-energy principle: a uniﬁed brain theory?’, Na-
ture reviews neuroscience, 11(2), 127, (2010).
[11] Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel, ‘Ac-
tion and behavior: a free-energy formulation’, Biological cybernetics,
102(3), 227–260, (2010).
[12] Shaun Gallagher, ‘Philosophical conceptions of the self: implications
for cognitive science’,Trends Cogn. Sci., 4(1), 14–21, (2000).
[13] Yasmin Kim Georgie, Guido Schillaci, and Verena Vanessa Hafner,
‘An interdisciplinary overview of developmental indices and behavioral
measures of the minimal self’, in 2019 Joint IEEE 9th International
Conference on Development and Learning and Epigenetic Robotics
(ICDL-EpiRob), pp. 129–136. IEEE, (2019).
[14] Kevin Gold and Brian Scassellati, ‘Using probabilistic reasoning over
time to self-recognize’,Robotics and Autonomous Systems, 57(4), 384–
392, (2009).
[15] Hermann von Helmholtz, Handbuch der physiologischen Optik , vol-
ume 9, Leopold V oss, 1867.
[16] Bernhard Hommel, ‘Action control and the sense of agency’, The sense
of agency, 307–326, (2015).
[17] Sebastian Kahl and Stefan Kopp, ‘A predictive processing model of per-
ception and action for self-other distinction’, Frontiers in psychology,
9, 2421, (2018).
[18] Diederik P Kingma and Jimmy Ba, ‘Adam: A method for stochastic
optimization’,arXiv preprint arXiv:1412.6980, (2014).
[19] David C Knill and Alexandre Pouget, ‘The bayesian brain: the role
of uncertainty in neural coding and computation’, TRENDS in Neuro-
sciences, 27(12), 712–719, (2004).
[20] Masanori Kohda, Takashi Hotta, Tomohiro Takeyama, Satoshi Awata,
Hirokazu Tanaka, Jun-ya Asai, and Alex L Jordan, ‘If a ﬁsh can pass
the mark test, what are the implications for consciousness and self-
awareness testing in animals?’,PLoS biology, 17(2), e3000021, (2019).
[21] Pablo Lanillos and Gordon Cheng, ‘Active inference with function
learning for robot body perception’,International Workshop on Contin-
ual Unsupervised Sensorimotor Learning, IEEE Developmental Learn-
ing and Epigenetic Robotics (ICDL-Epirob), (2018).
[22] Pablo Lanillos and Gordon Cheng, ‘Adaptive robot body learning and
estimation through predictive coding’, in 2018 IEEE/RSJ Int. Conf. on
Intelligent Robots and Systems (IROS), pp. 4083–4090. IEEE, (2018).
[23] Pablo Lanillos, Emmanuel Dean-Leon, and Gordon Cheng, ‘Yielding
self-perception in robots through sensorimotor contingencies’, IEEE
Transactions on Cognitive and Developmental Systems, 9(2), 100–112,
(2016).
[24] Pablo Lanillos, Emmanuel Dean-Leon, and Gordon Cheng, ‘Enac-
tive self: a study of engineering perspectives to obtain the sensorimo-
tor self through enaction’, in Developmental Learning and Epigenetic
Robotics, Joint IEEE Int. Conf. on, (2017).
[25] Robert W Mitchell, ‘Kinesthetic-visual matching and the self-concept
as explanations of mirror-self-recognition’, Journal for the theory of
social behaviour, 27(1), 17–39, (1997).
[26] Yukie Nagai, Yuji Kawai, and Minoru Asada, ‘Emergence of mirror
neuron system: Immature vision leads to self-other correspondence’, in
Development and Learning (ICDL), IEEE Int. Conf. on , volume 2, pp.
1–6, (2011).
[27] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng, ‘Active infer-
ence body perception and action for humanoid robots’, arXiv preprint
arXiv:1906.03022, (2019).
[28] Jordi Pages, Luca Marchionni, and Francesco Ferro, ‘Tiago: the mod-
ular robot that adapts to different research needs’, in Proc. of the IROS
Workshop on Robot Modularity, (2016).
[29] Alexandre Pitti, Ganna Pugach, Philippe Gaussier, and Sotaro Shimada,
‘Spatio-temporal tolerance of visuo-tactile illusions in artiﬁcial skin by
recurrent neural network with spike-timing-dependent plasticity’, Sci-
entiﬁc reports, 7, (2017).
[30] Rajesh PN Rao and Dana H Ballard, ‘Predictive coding in the visual
cortex: a functional interpretation of some extra-classical receptive-
ﬁeld effects’,Nature neuroscience, 2(1), 79–87, (1999).
[31] Philippe Rochat, ‘Five levels of self-awareness as they unfold early in
life’,Consciousness and cognition, 12(4), 717–731, (2003).
[32] Cansu Sancaktar and Pablo Lanillos, ‘End-to-end pixel-based deep
active inference for body perception and action’, arXiv preprint
arXiv:2001.05847, (2019).
[33] Eero P Simoncelli, ‘Bayesian multi-scale differential optical ﬂow’,
Handbook of Computer Vision and Applications, 397–422, (1999).
[34] Alexander Stoytchev, ‘Self-detection in robots: a method based on de-
tecting temporal contingencies’,Robotica, 29(01), 1–21, (2011).
[35] Matthis Synofzik, Gottfried V osgerau, and Martin V oss, ‘The experi-
ence of agency: an interplay between prediction and postdiction’,Fron-
tiers in psychology, 4, 127, (2013).
[36] Daniel M Wegner, The illusion of conscious will, MIT press, 2017.
[37] Lorijn Zaadnoordijk, Tarek R Besold, and Sabine Hunnius, ‘A match
does not make a sense: on the sufﬁciency of the comparator model
for explaining the sense of agency’, Neuroscience of consciousness ,
2019(1), niz006, (2019).