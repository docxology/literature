1202
ceD
41
]GL.sc[
1v60470.2112:viXra
Branching Time Active Inference with Bayesian Filtering
Théophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grześ m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Branching Time Active Inference (Champion et al., 2021b,a) is a framework proposing to
look at planning as a form of Bayesian model expansion. Its root can be found in Active
Inference (Friston et al., 2016; Da Costa et al., 2020; Champion et al., 2021c), a neurosci-
entific framework widely used for brain modelling, as well as in Monte Carlo Tree Search
(Browne et al., 2012), a method broadly applied in the Reinforcement Learning literature.
Up to now, the inference of the latent variables was carried out by taking advantage of
the flexibility offered by Variational Message Passing (Winn and Bishop, 2005), an itera-
tive process that can be understood as sending messages along the edges of a factor graph
(Forney, 2001). In this paper, we harness the efficiency of an alternative method for in-
ference called Bayesian Filtering (Fox et al., 2003), which does not require the iteration
of the update equations until convergence of the Variational Free Energy. Instead, this
scheme alternates between two phases: integration of evidence and prediction of future
Champion et al.
states. Bothof those phases canbe performedefficiently andthis providesa seventy times
speed up over the state-of-the-art.
Keywords: Branching Time Active Inference, BayesianFiltering, Free Energy Principle
1. Introduction
Activeinferenceextendsthefreeenergyprincipletogenerativemodelswithactions(Friston et al.,
2016; Da Costa et al., 2020; Champion et al., 2021c) and can be regarded as a form of plan-
ning as inference (Botvinick and Toussaint, 2012). Over the years, this framework has suc-
cessfullyexplainedawiderange ofbrainphenomena, suchas habitformation (Friston et al.,
2016), Bayesian surprise (Itti and Baldi, 2009), curiosity (Schwartenbeck et al., 2018), and
dopaminergic discharge (FitzGerald et al., 2015). It has also been applied to a variety of
tasks such as navigation in the Animal AI environment (Fountas et al., 2020), robotic con-
trol (Pezzato et al., 2020; Sancaktar et al., 2020), the mountain car problem (Çatal et al.,
2020), the game DOOM (Cullen et al., 2018) and the cart pole problem (Millidge, 2019).
However, because active inference defines the prior over policies as a joint distribution
over the space of all possible policies, the method suffers from an exponential space and
time complexity class. In the reinforcement learning literature, this problem can be tackled
using Monte Carlo tree search (MCTS) (Browne et al., 2012), whose origins can be found
in the multi-armed bandit problem (Auer et al., 2002). More recently, MCTS has been
applied to a large number of tasks such as the game of Go (Silver et al., 2016), the Animal
AI environment (Fountas et al., 2020), and many others.
Morerecently, Branching Time ActiveInference (BTAI)(Champion et al.,2021b,a)pro-
posed that planning is a form of Bayesian model expansion guided by the upper confi-
dence bound for trees (UCT) criterion from the MCTS literature, i.e. a quantity from the
multi-armed bandit problem whose objective is to minimize the agent’s regret. And be-
cause the generative model is dynamically expanded, variational message passing (VMP)
(Winn and Bishop, 2005) was used to carry out inference over the latent variables. VMP
can be understood as a flexible iterative process that sends messages along the edges of
2
Branching Time Active Inference
a factor graph (Forney, 2001), and computes posterior beliefs by summing those messages
together.
Bayesian filtering (BF) (Fox et al., 2003) is an alternative inference method composed
of two phases. In the first phase, Bayes theorem is used to compute posterior beliefs each
time a new observation is obtained from the environment. In the second phase, posterior
beliefs over the present state (S ) are used to predict posterior beliefs over the state at the
t
next time step (S ). Importantly, this process is not iterative within a time step, i.e., it
t+1
only contains a forward pass, and therefore is much more efficient than VMP.
In Section 2, we present the theory underlying Branching Time Active Inference when
using Bayesian filtering for inference over latent variables. In Section 3, we show that using
Bayesian filtering instead of variational message passing for the inference process provides
BTAI with a seventy times speed-up while maintaining effective planning. Finally, Section
4 concludes this paper and discusses avenues for future research.
2. Branching Time Active Inference with Bayesian Filtering (BTAIBF)
In this section, we describe the theory underlying our approach. For any notational un-
certainty the reader is referred to Appendix F of Champion et al. (2021b). We let D be a
1-tensor representing the prior over initial hidden states P(S ). Let A be a 2-tensor repre-
0
senting the likelihood mapping P(O |S ), and B be a 3-tensor representing the transition
τ τ
mapping P(S |S ,U ). Additionally, we let I bethe setof multi-indices containing all the
τ+1 τ τ
policies (i.e., sequences of actions) that have been explored by the model. The generative
model of BTAI with BF can be formally written as the following joint distribution:
P(O
0
,S
0
,OI,SI)=P(O
0
|S
0
)P(S
0
) P(O
I
|S
I
)P(S
I
|S I\last)
I∈I
Y
3
Champion et al.
where S I\last is the parent of S I , and:
P(S ) = Cat(D) P(O |S ) = Cat(A)
0 τ τ
P(O
I
|S
I
) = Cat(A) P(S
I
|S I\last) = Cat(B
I
).
where B
I
= B(•,•,Ilast) is the2-tensor corresponding to I
last
(i.e., thelast action that led to
S ),andthelikelihoodmappinginthepast, i.e., P(O |S ),andinthefuture, i.e., P(O |S ),
I τ τ I I
are both categorical distributions with parameters A. This generative model is depicted in
Figure 1, where we assume that the current time step t equals zero.
PSt
S O
t POt t
PS{1} PS{2}
O S S O
{1} PO{1} {1} {2} PO{2} {2}
PS{11} PS{12} PS{22}
O S S S
{11} PO{11} {11} {12} {22}
Figure 1: This figure illustrates the expandable generative modelused by theBTAI with BF
agent. The future is a tree like generative model whose branches correspond to the policies
considered by the agent. The branches can be dynamically expanded during planning and
the nodes in light gray represent possible expansions of the current generative model.
Initially, the generative model only contains the initial state S and observation O .
0 0
The prior over the hidden state is known, i.e. P(S ) = Cat(D), as well as the likelihood,
0
i.e., P(O |S ) = Cat(A), and P(O ), the evidence, can be computed in the usual way
τ τ 0
by marginalizing over P(O ,S ) = P(O |S )P(S ). Thus, we can integrate the evidence
0 0 0 0 0
4
Branching Time Active Inference
provided to us by the initial observation O using Bayes Theorem:
0
P(O |S )P(S )
0 0 0
B(S )= , (1)
0
P(O )
0
where B(S ) are the beliefs over the initial hidden state. Then, we use the UCT criterion
0
to determine which node in the tree should be expanded. Let the tree’s root S be called
0
the current node. If the current node has no children, then it is selected for expansion.
Alternatively, the child with the highest UCT criterion becomes the new current node and
the process is iterated until we reach a leaf node (i.e. a node from which no action has
previously been selected). The UCT criterion (Browne et al., 2012) for the j-th child of the
current node is given by:
lnn
UCT = −G¯ +C , (2)
j j explore
n
s j
whereG¯ istheaverageexpectedfreeenergycalculatedwithrespectedtotheactionsselected
j
from the j-th child, C is the exploration constant that modulates the amount of
explore
exploration at the tree level, n is the number of times the current node has been visited,
and n is the number of times the j-th child has been visited.
j
Let S be the (leaf) node selected by the above selection procedure. We then expand all
I
the children of S , i.e., all the states of the form S where U ∈ {1,...,|U|} is an arbitrary
I I::U
action, and I :: U is the multi-index obtained by appending the action U at the end of the
sequence defined by I. Next, we compute the predicted beliefs over those expanded hidden
states using the transition mapping:
B(S ) =E P(S |S ) , (3)
J B(SI) J I
(cid:2) (cid:3)
where we let J = I :: U for any action U, B(S ) are the predicted posterior beliefs over S ,
I I
and according to our generative model P(S
J
|S
I
) = Cat(B
J
) with B
J
= B(•,•,Jlast). The
above equation corresponds to the second phase of Bayesian filtering, i.e., the prediction
5
Champion et al.
phase, which involves the calculation of new beliefs, using the generative model, in the
absence of new observations. Then, we need to estimate the cost of (virtually) taking each
possibleaction. Thecostinthispaperistakentobetheexpectedfreeenergy (Friston et al.,
2017):
G =∆ D [B(O )||V(O )] + E [H[P(O |S )]], (4)
J KL J J B(SJ) J J
wherethepriorpreferencesoverfutureobservationsarespecifiedbythemodellerasV(O )=
J
Cat(C), according to the generative model P(O |S ) = Cat(A), and the posterior beliefs
J J
over future observations are computed by prediction as follows:
B(O ) = E [P(O |S )].
J B(SJ) J J
Next, we assume that the agent will always perform the action with the lowest cost, and
back-propagate the cost of the best (virtual) action toward the root of the tree. Formally,
we write the update as follows:
∀K ∈A ∪{I}, G ← G + min G , (5)
I K K I::U
U∈{1,...,|U|}
where I is the multi-index of the node that was selected for (virtual) expansion, and A is
I
the set of all multi-indices corresponding to ancestors of S . During the back propagation,
I
we also update the number of visits as follows:
∀K ∈A ∪{I}, n ← n +1. (6)
I K K
IfweletGaggr
betheaggregatedcostofanarbitrarynodeS obtainedbyapplyingEquation
K K
5 after each expansion, then we are now able to express G¯ formally as:
K
Gaggr
G¯ = K .
K
n
K
6
Branching Time Active Inference
The planning procedure described above ends when the maximum number of planning iter-
ations is reached, and the action corresponding to the root’s child with the lowest average
cost is performed in the environment. At this point, the agent receives a new observation
O and needs to update its beliefs over S . First, we predict the posterior beliefs over S as
τ τ τ
follows:
B(S |U = U∗)= E P(S |S ,U = U∗) , (7)
τ τ−1 B(Sτ−1) τ τ−1 τ−1
(cid:2) (cid:3)
whereU∗istheactionperformed(fromtheroot)intheenvironment, P(S |S ,U = U∗)
τ τ−1 τ−1
is the 2-tensor B(•,•,U∗), and B(S
τ−1
) is the agent’s posterior beliefs over the state at time
τ −1, e.g., after performing the first action in the environment, τ = 1 and B(S ) = B(S )
τ−1 0
as given by Equation 1. Second, we integrate the evidence provided by the new observation
O using Bayes theorem:
τ
P(O |S )B(S |U = U∗)
τ τ τ τ−1
B(S )= , (8)
τ
P(O )
τ
7
Champion et al.
where B(S |U = U∗) is used as an empirical prior. By an empirical prior we mean a
τ τ−1
posterior distribution of the previous time step, e.g., B(S |U = U∗), that is used as a
τ τ−1
prior in Bayes theorem. Algorithm 1 concludes this section by summarizing our approach.
Algorithm 1: BTAI with BF: action-perception cycles (with relevant equations
indicated in round brackets).
Input: env the environment, O the initial observation, A the likelihood mapping,
0
B the transition mapping, C the prior preferences, D the prior over initial
states, N the number of planning iterations, M the number of
action-perception cycles.
B(S ) ← IntegrateEvidence(O , A, D) // Using (1)
0 0
root ← CreateTreeNode(beliefs = B(S ), action = -1, cost = 0, visits = 1)
0
// Where -1 in the line above is a dummy value
repeat M times
repeat N times
node← SelectNode(root) // Using (2) recursively
eNodes ← ExpandChildren(node, B) // Using (3) for each action
Evaluate(eNodes, A, C) // Compute (4) for each expanded node
Backpropagate(eNodes) // Using (5) and (6)
end
U∗ ← SelectAction(root) // Such that U∗ minimises the average cost
O ← env.Execute(U∗)
τ
B(S ) ← root.beliefs // Get beliefs of the root node
τ−1
B(S |U = U∗) ← ComputeEmpiricalPrior(B, B(S ), U∗) // Using (7)
τ τ−1 τ−1
B(S )← IntegrateEvidence(O , A, B(S |U = U∗)) // Using (8)
τ τ τ τ−1
root ← CreateTreeNode(beliefs = B(S ), action = U∗, cost = 0, visits = 1)
τ
end
3. Results
In this section, we first present the deep reward environment in which two versions of BTAI
will be compared. Then, we present experimental results comparing BTAI with VMP and
BTAI with BF in terms of running time and performance.
3.1 Deep reward environment
Thisenvironment iscalledthedeeprewardenvironment becausetheagentneedstonavigate
a tree-like graph where the graph’s nodes correspond to the states of the system, and the
agent needs to look deep into the future to diferentiate the good path from the traps. At
8
Branching Time Active Inference
the beginning of each trial, the agent is placed at the root of the tree, i.e., the initial state
of the system. From the initial state, the agent can perform n+m actions, where n and
m are the number of good and bad paths, respectively. Additionally, at any point in time,
the agent can make two observations: a pleasant one or an unpleasant one. The states of
the good paths produce pleasant observations, while the states of the bad paths produce
unpleasant ones.
If the first action selected was one of the m bad actions, then the agent will enter a
bad path in which n+m actions are available at each time step but all of them produce
unpleasant observations. If the first action selected was one of the n good actions, then the
agent will enter the associated good path. We let L be the length of the k-th good path.
k
Once the agent is engaged on the k-th path, there are still n+m actions available but only
one of them keeps theagent on the goodpath. Allthe other actions will produce unpleasant
observations, i.e., the agent will enter a bad path.
This process will continue until the agent reaches the end of the k-th path, which is
determined by the path’s length L . If the k-th path was the longest of the n good paths,
k
then the agent will from now on only receive pleasant observations independently of the
action performed. If the k-th path was not the longest path, then independently of the
action performed the agent will enter a bad path.
To summarize, at the beginning of each trial, the agent is prompted with n good paths
and m bad paths. Only the longest good path will be beneficial in the long term, the others
are traps, which will ultimately lead the agent to a bad state. Figure 2 illustrates this
environment.
9
Champion et al.
S
0
S ... S S1 S... Sn
b b 1 1 1
✳
m ❜❛❞ ♣❛t❤s ✳ ✳
S ... S S1 S ... S
b b 2 b b
m+n−1 ❜❛❞ ♣❛t❤s m+n ❜❛❞ ♣❛t❤s
S ... S
g g
m+n ❣♦♦❞ ♣❛t❤s
Figure 2: This figure illustrates a type of deep reward environment where S represents the
0
initial state, S represents a bad state, S represents a goodstate, and Si is the j-th state of
b g j
the i-th good path. Also, the longest path in the above picture is the first good path whose
length L is equal to two. Importantly, the longest path corresponds to the only good path
1
that does not turn out to be a trap.
3.2 BTAI with VMP versus BTAI with BF
In this section, we compare BTAI with VMP and BTAI with BF in terms of running time
andperformance. Therunning timereported inTables 1and2was obtained by running 100
trials each composed of 20 action-perception cycles. Also, the trial was stopped whenever
theagentreached abadstateorthegoalstate. AsshowninTables 1and2,bothapproaches
were able to solve the tasks. However, BTAI with BF ran 69±26 times faster than BTAI
with VMP.
This speed up is possible for two reasons. First, Bayesian filtering does not require the
iteration of the belief updates until convergence of the variational freeenergy. Second, when
computing the optimal posterior over a random variable X, VMP needs to compute one
message for each adjacent variable of X, add them together, and normalise using a softmax
function. In contrast, BF only performs a forward pass, which is essentially implemented as
matrix multiplications.
10
Branching Time Active Inference
n m L , L , ..., L # planning iterations P(goal) P(bad) Running time
1 2 n
2 5 5, 8 25 1 0 2.294 sec
2 5 5, 8 50 1 0 4.688 sec
2 5 5, 8 100 1 0 9.045 sec
3 5 6, 5, 8 25 1 0 2.805 sec
3 5 6, 5, 8 50 1 0 5.416 sec
3 5 6, 5, 8 100 1 0 11.288 sec
Table 1: This table presents the results of BTAI with BF on various deep reward environ-
ments. Recall, that n and m are the number of good and bad paths, respectively. L is
i
the length of the i-th good path. P(goal) reports the probability of reaching the goal state
(i.e., the agent successfully picked the longest path), and P(bad) reports the probability of
reaching the bad state (i.e., either by picking a bad action directly of by falling into a trap).
n m L , L , ..., L # planning iterations P(goal) P(bad) Running time
1 2 n
2 5 5, 8 25 1 0 4 min 16 sec
2 5 5, 8 50 1 0 4 min 42 sec
2 5 5, 8 100 1 0 6 min 26 sec
3 5 6, 5, 8 25 1 0 4 min 31 sec
3 5 6, 5, 8 50 1 0 5 min 35 sec
3 5 6, 5, 8 100 1 0 7 min 48 sec
Table 2: This table presents the results of BTAI with VMP on various deep reward envi-
ronments. Recall, that n and m are the number of good and bad paths, respectively. L is
i
the length of the i-th good path. P(goal) reports the probability of reaching the goal state
(i.e., the agent successfully picked the longest path), and P(bad) reports the probability of
reaching the bad state (i.e., either by picking a bad action directly of by falling into a trap).
4. Conclusion and future works
In this paper, we proposed a new implementation of Branching Time Active Inference
(Champion et al.,2021b,a),wheretheinferenceiscarriedoutusingBayesianfiltering(Fox et al.,
2003),insteadofusingvariationalmessagepassing(Champion et al.,2021c;Winn and Bishop,
2005).
This new approach has a few advantages. First, it achieves the same performance as
its predecessor around seventy times faster. Second, the implementation is simpler and less
data structures need to be stored in memory.
11
Champion et al.
Also,onecouldarguethatthereisatrade-off inthenatureandextentoftheinformation
inferred by classic active inference, branching-time active inference with variational message
passing (BTAIVMP) from Champion et al. (2021b,a), and branching-time active inference
with Bayesian Filtering (BTAIBF). Specifically, classic active inference exhaustively rep-
resents and updates all possible policies, while BTAIVMP will typically only represent one
policly in the past (i.e., the one undertaken by the agent) and a small subset of the possible
(future) trajectories. These will typically be the more advantageous paths for the agent
to pursue, with the less beneficial paths not represented at all. Indeed, the tree search is
based onthe expectedfreeenergy that favors policies thatmaximize information gain, while
realizing the prior preferences of the agent. BTAIBF stores even less data than BTAIVMP,
because the sequence of past hidden states is discarded as time passes, and only the beliefs
over the current and future states are stored.
Additionally, full variational inference can update the system’s understanding of past
contingencies on the basis of new observations. As a result, the system can obtain more
refined information about previous decisions, perhaps re-evaluating the optimality of these
past decisions. Because classic active inference represents a larger space of policies, this
re-evaluation could apply to more policies. When using Bayesian filtering, beliefs about
past hidden states are discarded as time progresses, which makes Bayesian belief updating
(about past hidden states) impossible.
We also know that humans engage in counterfactual reasoning (Rafetseder et al., 2013),
which, in our planning context, could involve the entertainment and evaluation of alterna-
tive (non-selected) sequences of decisions. It may be that, because of the more exhaustive
representation of possible trajectories, classic active inference can more efficiently engage in
counterfactual reasoning. In contrast, branching-time active inference would require these
alternative paststobegenerated“afresh” foreachcounterfactual deliberation. Inthissense,
one might argue that there is a trade-off: branching-time active inference provides consider-
ably more efficient planning to attain current goals, classic active inference provides a more
exhaustive assessment of paths not taken. In contrast, branching time active inference im-
12
Branching Time Active Inference
plemented with Bayesian filtering does not leave a memory at all, let alone one upon which
conterfactual reasoning could be realized.
The implementation of Branching Time Active Inference with variational message pass-
ing can be found here: https : //github.com/ChampiB/Homing − Pigeon, and the im-
plementation of Branching Time Active Inference with Bayesian Filtering is available on
Github: https: //github.com/ChampiB/Branching_Time_Active_Inference.
Even with this seventy times speed up, BTAI is still unable to deal with large scale
observations such as images. Adding deep neural networks to approximate the likelihood
mapping is therefore a compelling direction for future research.
Also, this framework is currently limited to discrete action and state spaces. Designing a
continuous extension of BTAI would enable its application to a wider range of applications
such as robotic control with continuous actions.
Finally, as the depth of the tree increases, the beliefs about future states tend to become
more and more uncertain, which can lead to a drop in performance. This suggests that
thereexistsanoptimalnumberofplanningiterations, afterwhich themodelsimplydoesnot
have enough information to keep planning. Future work could thus focus on automatically
identifying this optimal number of planning iterations, in order to improve the robustness
of the approach.
Acknowledgments
TO BE FILLED
References
Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47(2):235–256, May 2002. ISSN 1573-0565. doi:
10.1023/A:1013689704352. URL https://doi.org/10.1023/A:1013689704352.
13
Champion et al.
MatthewBotvinickandMarcToussaint. Planningasinference.TrendsinCognitiveSciences,
16(10):485 – 488, 2012. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2012.08.006.
C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,
S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of Monte Carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43,
2012.
Théophile Champion, Howard Bowman, and Marek Grześ. Branching time active inference:
empirical study. arXiv, 2021a.
Théophile Champion, Howard Bowman, and Marek Grześ. Branching time active inference:
the theory and its generality. arXiv, 2021b.
Théophile Champion, Marek Grześ, and Howard Bowman. Realizing Active Inference in
Variational Message Passing: The Outcome-Blind Certainty Seeker. Neural Computa-
tion, 33(10):2762–2826, 09 2021c. ISSN 0899-7667. doi: 10.1162/neco_a_01422. URL
https://doi.org/10.1162/neco_a_01422.
Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active inference
in OpenAI Gym: A paradigm for computational investigations into psychiatric ill-
ness. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 3(9):809 –
818, 2018. ISSN 2451-9022. doi: https://doi.org/10.1016/j.bpsc.2018.06.010. URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617. Compu-
tational Methods and Modeling in Psychiatry.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Vic-
torita Neacsu, and Karl Friston. Active inference on discrete state-
spaces: A synthesis. Journal of Mathematical Psychology, 99:102447, 2020.
ISSN 0022-2496. doi: https://doi.org/10.1016/j.jmp.2020.102447. URL
https://www.sciencedirect.com/science/article/pii/S0022249620300857.
14
Branching Time Active Inference
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine,
reward learning, and active inference. Frontiers in Computational Neuro-
science, 9:136, 2015. ISSN 1662-5188. doi: 10.3389/fncom.2015.00136. URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
G. D. Forney. Codes on graphs: normal realizations. IEEE Transactions on Information
Theory, 47(2):520–548, 2001.
ZafeiriosFountas, NoorSajid,PedroA.M.Mediano,andKarlFriston. Deepactiveinference
agents using Monte-Carlo methods. arXiv, 2020.
V. Fox, J. Hightower, Lin Liao, D. Schulz, and G. Borriello. Bayesian filtering for location
estimation. IEEE Pervasive Computing, 2(3):24–33, 2003. doi: 10.1109/MPRV.2003.
1228524.
KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck, JohnODoherty,
and Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Re-
views, 68:862 – 879, 2016. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev.
2016.06.022.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Gio-
vanni Pezzulo. Active Inference: A Process Theory. Neural Computation, 29
(1):1–49, 01 2017. ISSN 0899-7667. doi: 10.1162/NECO_a_00912. URL
https://doi.org/10.1162/NECO_a_00912.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human at-
tention. Vision Research, 49(10):1295 – 1306, 2009. ISSN 0042-
6989. doi: https://doi.org/10.1016/j.visres.2008.09.007. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Vi-
sual Attention: Psychophysics, electrophysiology and neuroimaging.
15
Champion et al.
Beren Millidge. Combining active inference and hierarchical predictive coding: A tuto-
rial introduction and case study. PsyArXiv, 2019. doi: 10.31234/osf.io/kf6wc. URL
https://doi.org/10.31234/osf.io/kf6wc.
Corrado Pezzato, Carlos Hernandez, and MartijnWisse. Active inference andbehavior trees
for reactive action planning and execution in robotics. arXiv, 2020.
Eva Rafetseder, Maria Schwitalla, and Josef Perner. Counterfactual reasoning: From child-
hood to adulthood. Journal of experimental child psychology, 114(3):389–404, 2013.
Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End-to-end pixel-based deep
active inference for body perception and action. arXiv, 2020.
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas H B FitzGer-
ald, Martin Kronbichler, and Karl Friston. Computational mechanisms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbren-
ner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks
and tree search. Nature, 529(7587):484–489, 2016. doi: 10.1038/nature16961. URL
https://doi.org/10.1038/nature16961.
John Winn and Christopher Bishop. Variational message passing. Journal of Machine
Learning Research, 6:661–694, 2005.
Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning
perception and planning with deep active inference. arXiv, 2020.
16