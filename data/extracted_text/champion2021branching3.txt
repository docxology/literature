arXiv:2112.07406v1  [cs.LG]  14 Dec 2021
Branching Time Active Inference with Bayesian Filtering
Théophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grześ m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Branching Time Active Inference (Champion et al., 2021b,a) is a framework proposing to
look at planning as a form of Bayesian model expansion. Its ro ot can be found in Active
Inference (F riston et al., 2016; Da Costa et al., 2020; Champ ion et al., 2021c), a neurosci-
entiﬁc framework widely used for brain modelling, as well as in Monte Carlo T ree Search
(Browne et al., 2012), a method broadly applied in the Reinfo rcement Learning literature.
Up to now, the inference of the latent variables was carried o ut by taking advantage of
the ﬂexibility oﬀered by V ariational Message Passing (Winn and Bishop, 2005), an itera-
tive process that can be understood as sending messages alon g the edges of a factor graph
(F orney, 2001). In this paper, we harness the eﬃciency of an a lternative method for in-
ference called Bayesian Filtering (F ox et al., 2003), which does not require the iteration
of the update equations until convergence of the V ariationa l F ree Energy . Instead, this
scheme alternates between two phases: integration of evide nce and prediction of future
Champion et al.
states. Both of those phases can be performed eﬃciently and t his provides a seventy times
speed up over the state-of-the-art.
Keywords: Branching Time Active Inference, Bayesian Filtering, F ree Energy Principle
1. Introduction
Active inference extends the free energy principle to gener ative models with actions (F riston et al.,
2016; Da Costa et al., 2020; Champion et al., 2021c) and can be regarded as a form of plan-
ning as inference (Botvinick and T oussaint, 2012). Over the y ears, this framework has suc-
cessfully explained a wide range of brain phenomena, such as habit formation (F riston et al.,
2016), Bayesian surprise (Itti and Baldi, 2009), curiosity (S chwartenbeck et al., 2018), and
dopaminergic discharge (FitzGerald et al., 2015). It has al so been applied to a variety of
tasks such as navigation in the Animal AI environment (F ount as et al., 2020), robotic con-
trol (Pezzato et al., 2020; Sancaktar et al., 2020), the moun tain car problem (Çatal et al.,
2020), the game DOOM (Cullen et al., 2018) and the cart pole pr oblem (Millidge, 2019).
However, because active inference deﬁnes the prior over pol icies as a joint distribution
over the space of all possible policies, the method suﬀers fr om an exponential space and
time complexity class. In the reinforcement learning liter ature, this problem can be tackled
using Monte Carlo tree search (MCTS) (Browne et al., 2012), wh ose origins can be found
in the multi-armed bandit problem (Auer et al., 2002). More r ecently , MCTS has been
applied to a large number of tasks such as the game of Go (Silve r et al., 2016), the Animal
AI environment (F ountas et al., 2020), and many others.
More recently , Branching Time Active Inference (BT AI) (Champ ion et al., 2021b,a) pro-
posed that planning is a form of Bayesian model expansion guid ed by the upper conﬁ-
dence bound for trees (UCT) criterion from the MCTS literatu re, i.e. a quantity from the
multi-armed bandit problem whose objective is to minimize t he agent’s regret. And be-
cause the generative model is dynamically expanded, variat ional message passing (VMP)
(Winn and Bishop, 2005) was used to carry out inference over th e latent variables. VMP
can be understood as a ﬂexible iterative process that sends m essages along the edges of
2
Branching Time Active Inference
a factor graph (F orney, 2001), and computes posterior belie fs by summing those messages
together.
Bayesian ﬁltering (BF) (F ox et al., 2003) is an alternative inf erence method composed
of two phases. In the ﬁrst phase, Bayes theorem is used to compu te posterior beliefs each
time a new observation is obtained from the environment. In t he second phase, posterior
beliefs over the present state ( St) are used to predict posterior beliefs over the state at the
next time step ( St+1). Importantly , this process is not iterative within a time s tep, i.e., it
only contains a forward pass, and therefore is much more eﬃci ent than VMP .
In Section 2, we present the theory underlying Branching Time Active Inference when
using Bayesian ﬁltering for inference over latent variables . In Section 3, we show that using
Bayesian ﬁltering instead of variational message passing fo r the inference process provides
BT AI with a seventy times speed-up while maintaining eﬀectiv e planning. Finally , Section
4 concludes this paper and discusses avenues for future rese arch.
2. Branching Time Active Inference with Bayesian Filtering (BT AIBF)
In this section, we describe the theory underlying our appro ach. F or any notational un-
certainty the reader is referred to Appendix F of Champion et al. (2021b). W e let D be a
1-tensor representing the prior over initial hidden states P (S0). Let A be a 2-tensor repre-
senting the likelihood mapping P (Oτ |Sτ ), and B be a 3-tensor representing the transition
mapping P (Sτ+1|Sτ , U τ ). Additionally , we let I be the set of multi-indices containing all the
policies (i.e., sequences of actions) that have been explor ed by the model. The generative
model of BT AI with BF can be formally written as the following jo int distribution:
P (O0, S 0, O I, S I) = P (O0|S0)P (S0)
∏
I∈ I
P (OI |SI )P (SI |SI\ last)
3
Champion et al.
where SI\ last is the parent of SI , and:
P (S0) = Cat(D) P (Oτ |Sτ ) = Cat(A)
P (OI |SI ) = Cat(A) P (SI |SI\ last) = Cat(BI ).
where BI = B(•, •, I last) is the 2-tensor corresponding to Ilast (i.e., the last action that led to
SI ), and the likelihood mapping in the past, i.e., P (Oτ |Sτ ), and in the future, i.e., P (OI |SI ),
are both categorical distributions with parameters A. This generative model is depicted in
Figure 1, where we assume that the current time step t equals zero.
St
PSt
POt Ot
PS{1}
S{1}
PS{2}
S{2}PO{1}O{1} PO{2} O{2}
PS{22}
S{22}
PS{11}
S{11}
PS{12}
S{12}PO{11}O{11}
Figure 1: This ﬁgure illustrates the expandable generative model used by the BT AI with BF
agent. The future is a tree like generative model whose branc hes correspond to the policies
considered by the agent. The branches can be dynamically exp anded during planning and
the nodes in light gray represent possible expansions of the current generative model.
Initially , the generative model only contains the initial s tate S0 and observation O0.
The prior over the hidden state is known, i.e. P (S0) = Cat(D), as well as the likelihood,
i.e., P (Oτ |Sτ ) = Cat(A), and P (O0), the evidence, can be computed in the usual way
by marginalizing over P (O0, S 0) = P (O0|S0)P (S0). Thus, we can integrate the evidence
4
Branching Time Active Inference
provided to us by the initial observation O0 using Bayes Theorem:
B(S0) = P (O0|S0)P (S0)
P (O0) , (1)
where B(S0) are the beliefs over the initial hidden state. Then, we use th e UCT criterion
to determine which node in the tree should be expanded. Let th e tree’s root S0 be called
the current node. If the current node has no children, then it is selected for expansion.
Alternatively , the child with the highest UCT criterion bec omes the new current node and
the process is iterated until we reach a leaf node (i.e. a node from which no action has
previously been selected). The UCT criterion (Browne et al., 2012) for the j-th child of the
current node is given by:
UCT j = − ¯Gj + Cexplore
√
ln n
nj
, (2)
where ¯Gj is the average expected free energy calculated with respect ed to the actions selected
from the j-th child, Cexplore is the exploration constant that modulates the amount of
exploration at the tree level, n is the number of times the current node has been visited,
and nj is the number of times the j-th child has been visited.
Let SI be the (leaf ) node selected by the above selection procedure . W e then expand all
the children of SI , i.e., all the states of the form SI::U where U ∈{1, ..., |U|}is an arbitrary
action, and I :: U is the multi-index obtained by appending the action U at the end of the
sequence deﬁned by I. Next, we compute the predicted beliefs over those expanded hidden
states using the transition mapping:
B(SJ ) = EB(SI )
[
P (SJ |SI )
]
, (3)
where we let J = I :: U for any action U, B(SI ) are the predicted posterior beliefs over SI ,
and according to our generative model P (SJ |SI ) = Cat(BJ ) with BJ = B(•, •, J last). The
above equation corresponds to the second phase of Bayesian ﬁl tering, i.e., the prediction
5
Champion et al.
phase, which involves the calculation of new beliefs, using the generative model, in the
absence of new observations. Then, we need to estimate the co st of (virtually) taking each
possible action. The cost in this paper is taken to be the expe cted free energy (F riston et al.,
2017):
GJ =∆ DKL[B(OJ )||V (OJ )] + EB(SJ )[H[P (OJ |SJ )]], (4)
where the prior preferences over future observations are sp eciﬁed by the modeller as V (OJ ) =
Cat(C), according to the generative model P (OJ |SJ ) = Cat(A), and the posterior beliefs
over future observations are computed by prediction as foll ows:
B(OJ ) = EB(SJ )[P (OJ |SJ )].
Next, we assume that the agent will always perform the action with the lowest cost, and
back-propagate the cost of the best (virtual) action toward the root of the tree. F ormally ,
we write the update as follows:
∀K ∈AI ∪{I}, GK ← GK + min
U∈{ 1,..., |U|}
GI::U , (5)
where I is the multi-index of the node that was selected for (virtual ) expansion, and AI is
the set of all multi-indices corresponding to ancestors of SI . During the back propagation,
we also update the number of visits as follows:
∀K ∈AI ∪{I}, n K ← nK + 1. (6)
If we let Gaggr
K be the aggregated cost of an arbitrary node SK obtained by applying Equation
5 after each expansion, then we are now able to express ¯GK formally as:
¯GK = Gaggr
K
nK
.
6
Branching Time Active Inference
The planning procedure described above ends when the maximu m number of planning iter-
ations is reached, and the action corresponding to the root’ s child with the lowest average
cost is performed in the environment. At this point, the agen t receives a new observation
Oτ and needs to update its beliefs over Sτ . First, we predict the posterior beliefs over Sτ as
follows:
B(Sτ |Uτ− 1 = U∗) = EB(Sτ − 1)
[
P (Sτ |Sτ− 1, U τ− 1 = U∗)
]
, (7)
where U∗ is the action performed (from the root) in the environment, P (Sτ |Sτ− 1, U τ− 1 = U∗)
is the 2-tensor B(•, •, U ∗), and B(Sτ− 1) is the agent’s posterior beliefs over the state at time
τ −1, e.g., after performing the ﬁrst action in the environment, τ = 1 and B(Sτ− 1) = B(S0)
as given by Equation 1. Second, we integrate the evidence pro vided by the new observation
Oτ using Bayes theorem:
B(Sτ ) = P (Oτ |Sτ )B(Sτ |Uτ− 1 = U∗)
P (Oτ ) , (8)
7
Champion et al.
where B(Sτ |Uτ− 1 = U∗) is used as an empirical prior. By an empirical prior we mean a
posterior distribution of the previous time step, e.g., B(Sτ |Uτ− 1 = U∗), that is used as a
prior in Bayes theorem. Algorithm 1 concludes this section by summarizing our approach.
Algorithm 1: BT AI with BF: action-perception cycles (with relevant equati ons
indicated in round brackets).
Input: env the environment, O0 the initial observation, A the likelihood mapping,
B the transition mapping, C the prior preferences, D the prior over initial
states, N the number of planning iterations, M the number of
action-perception cycles.
B(S0) ← IntegrateEvidence(O0, A, D) // Using (1)
root ← CreateT reeNode(beliefs = B(S0), action = -1, cost = 0, visits = 1)
// Where -1 in the line above is a dummy value
repeat M times
repeat N times
node ← SelectNode(root) // Using (2) recursively
eNodes ← ExpandChildren(node, B) // Using (3) for each action
Evaluate(eNodes, A, C) // Compute (4) for each expanded node
Backpropagate(eNodes) // Using (5) and (6)
end
U∗ ← SelectAction(root) // Such that U∗ minimises the average cost
Oτ ← env.Execute(U∗)
B(Sτ− 1) ← root.beliefs // Get beliefs of the root node
B(Sτ |Uτ− 1 = U∗) ← ComputeEmpiricalPrior(B, B(Sτ− 1), U∗) // Using (7)
B(Sτ ) ← IntegrateEvidence(Oτ , A, B(Sτ |Uτ− 1 = U∗)) // Using (8)
root ← CreateT reeNode(beliefs = B(Sτ ), action = U∗, cost = 0, visits = 1)
end
3. Results
In this section, we ﬁrst present the deep reward environment in which two versions of BT AI
will be compared. Then, we present experimental results com paring BT AI with VMP and
BT AI with BF in terms of running time and performance.
3.1 Deep reward environment
This environment is called the deep reward environment beca use the agent needs to navigate
a tree-like graph where the graph’s nodes correspond to the s tates of the system, and the
agent needs to look deep into the future to diferentiate the g ood path from the traps. At
8
Branching Time Active Inference
the beginning of each trial, the agent is placed at the root of the tree, i.e., the initial state
of the system. F rom the initial state, the agent can perform n + m actions, where n and
m are the number of good and bad paths, respectively . Addition ally , at any point in time,
the agent can make two observations: a pleasant one or an unpl easant one. The states of
the good paths produce pleasant observations, while the sta tes of the bad paths produce
unpleasant ones.
If the ﬁrst action selected was one of the m bad actions, then the agent will enter a
bad path in which n + m actions are available at each time step but all of them produc e
unpleasant observations. If the ﬁrst action selected was on e of the n good actions, then the
agent will enter the associated good path. W e let Lk be the length of the k-th good path.
Once the agent is engaged on the k-th path, there are still n + m actions available but only
one of them keeps the agent on the good path. All the other acti ons will produce unpleasant
observations, i.e., the agent will enter a bad path.
This process will continue until the agent reaches the end of the k-th path, which is
determined by the path’s length Lk. If the k-th path was the longest of the n good paths,
then the agent will from now on only receive pleasant observa tions independently of the
action performed. If the k-th path was not the longest path, then independently of the
action performed the agent will enter a bad path.
T o summarize, at the beginning of each trial, the agent is pro mpted with n good paths
and m bad paths. Only the longest good path will be beneﬁcial in the long term, the others
are traps, which will ultimately lead the agent to a bad state . Figure 2 illustrates this
environment.
9
Champion et al.
S0
Sb ... Sb
m ❜❛❞ ♣❛t❤s
S1
1 S...
1 Sn
1
S1
2Sb...Sb
m + n − 1 ❜❛❞ ♣❛t❤s
✳
✳
✳
...Sg Sg
m + n ❣♦ ♦ ❞ ♣❛t❤s
...Sb Sb
m + n ❜❛❞ ♣❛t❤s
Figure 2: This ﬁgure illustrates a type of deep reward enviro nment where S0 represents the
initial state, Sb represents a bad state, Sg represents a good state, and Si
j is the j-th state of
the i-th good path. Also, the longest path in the above picture is t he ﬁrst good path whose
length L1 is equal to two. Importantly , the longest path corresponds t o the only good path
that does not turn out to be a trap.
3.2 BT AI with VMP versus BT AI with BF
In this section, we compare BT AI with VMP and BT AI with BF in terms of running time
and performance. The running time reported in T ables 1 and 2 w as obtained by running 100
trials each composed of 20 action-perception cycles. Also, the trial was stopped whenever
the agent reached a bad state or the goal state. As shown in T ab les 1 and 2, both approaches
were able to solve the tasks. However, BT AI with BF ran 69 ± 26 times faster than BT AI
with VMP .
This speed up is possible for two reasons. First, Bayesian ﬁlt ering does not require the
iteration of the belief updates until convergence of the var iational free energy . Second, when
computing the optimal posterior over a random variable X, VMP needs to compute one
message for each adjacent variable of X, add them together, and normalise using a softmax
function. In contrast, BF only performs a forward pass, which is essentially implemented as
matrix multiplications.
10
Branching Time Active Inference
n m L1, L2, ..., Ln # planning iterations P(goal) P(bad) Running time
2 5 5, 8 25 1 0 2.294 sec
2 5 5, 8 50 1 0 4.688 sec
2 5 5, 8 100 1 0 9.045 sec
3 5 6, 5, 8 25 1 0 2.805 sec
3 5 6, 5, 8 50 1 0 5.416 sec
3 5 6, 5, 8 100 1 0 11.288 sec
T able 1: This table presents the results of BT AI with BF on vario us deep reward environ-
ments. Recall, that n and m are the number of good and bad paths, respectively . Li is
the length of the i-th good path. P (goal) reports the probability of reaching the goal state
(i.e., the agent successfully picked the longest path), and P (bad) reports the probability of
reaching the bad state (i.e., either by picking a bad action d irectly of by falling into a trap).
n m L1, L2, ..., Ln # planning iterations P(goal) P(bad) Running time
2 5 5, 8 25 1 0 4 min 16 sec
2 5 5, 8 50 1 0 4 min 42 sec
2 5 5, 8 100 1 0 6 min 26 sec
3 5 6, 5, 8 25 1 0 4 min 31 sec
3 5 6, 5, 8 50 1 0 5 min 35 sec
3 5 6, 5, 8 100 1 0 7 min 48 sec
T able 2: This table presents the results of BT AI with VMP on var ious deep reward envi-
ronments. Recall, that n and m are the number of good and bad paths, respectively . Li is
the length of the i-th good path. P (goal) reports the probability of reaching the goal state
(i.e., the agent successfully picked the longest path), and P (bad) reports the probability of
reaching the bad state (i.e., either by picking a bad action d irectly of by falling into a trap).
4. Conclusion and future works
In this paper, we proposed a new implementation of Branching T ime Active Inference
(Champion et al., 2021b,a), where the inference is carried o ut using Bayesian ﬁltering (F ox et al.,
2003), instead of using variational message passing (Champ ion et al., 2021c; Winn and Bishop,
2005).
This new approach has a few advantages. First, it achieves th e same performance as
its predecessor around seventy times faster. Second, the im plementation is simpler and less
data structures need to be stored in memory .
11
Champion et al.
Also, one could argue that there is a trade-oﬀ in the nature an d extent of the information
inferred by classic active inference, branching-time acti ve inference with variational message
passing (BT AI VMP) from Champion et al. (2021b,a), and branching-time active inference
with Bayesian Filtering (BT AI BF). Speciﬁcally , classic active inference exhaustively rep -
resents and updates all possible policies, while BT AI VMP will typically only represent one
policly in the past (i.e., the one undertaken by the agent) an d a small subset of the possible
(future) trajectories. These will typically be the more adv antageous paths for the agent
to pursue, with the less beneﬁcial paths not represented at a ll. Indeed, the tree search is
based on the expected free energy that favors policies that m aximize information gain, while
realizing the prior preferences of the agent. BT AI BF stores even less data than BT AI VMP,
because the sequence of past hidden states is discarded as ti me passes, and only the beliefs
over the current and future states are stored.
Additionally , full variational inference can update the sy stem’s understanding of past
contingencies on the basis of new observations. As a result, the system can obtain more
reﬁned information about previous decisions, perhaps re-e valuating the optimality of these
past decisions. Because classic active inference represent s a larger space of policies, this
re-evaluation could apply to more policies. When using Bayes ian ﬁltering, beliefs about
past hidden states are discarded as time progresses, which m akes Bayesian belief updating
(about past hidden states) impossible.
W e also know that humans engage in counterfactual reasoning (Rafetseder et al., 2013),
which, in our planning context, could involve the entertain ment and evaluation of alterna-
tive (non-selected) sequences of decisions. It may be that, because of the more exhaustive
representation of possible trajectories, classic active i nference can more eﬃciently engage in
counterfactual reasoning. In contrast, branching-time ac tive inference would require these
alternative pasts to be generated “a fresh” for each counter factual deliberation. In this sense,
one might argue that there is a trade-oﬀ: branching-time act ive inference provides consider-
ably more eﬃcient planning to attain current goals, classic active inference provides a more
exhaustive assessment of paths not taken. In contrast, bran ching time active inference im-
12
Branching Time Active Inference
plemented with Bayesian ﬁltering does not leave a memory at al l, let alone one upon which
conterfactual reasoning could be realized.
The implementation of Branching Time Active Inference with v ariational message pass-
ing can be found here: https : //github.com/ChampiB/Homing − P igeon, and the im-
plementation of Branching Time Active Inference with Bayesia n Filtering is available on
Github: https : //github.com/ChampiB/Branching _T ime_Active_Inference .
Even with this seventy times speed up, BT AI is still unable to d eal with large scale
observations such as images. Adding deep neural networks to approximate the likelihood
mapping is therefore a compelling direction for future rese arch.
Also, this framework is currently limited to discrete actio n and state spaces. Designing a
continuous extension of BT AI would enable its application to a wider range of applications
such as robotic control with continuous actions.
Finally , as the depth of the tree increases, the beliefs abou t future states tend to become
more and more uncertain, which can lead to a drop in performan ce. This suggests that
there exists an optimal number of planning iterations, afte r which the model simply does not
have enough information to keep planning. F uture work could thus focus on automatically
identifying this optimal number of planning iterations, in order to improve the robustness
of the approach.
Acknowledgments
TO BE FILLED
References
Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-t ime analysis of the multiarmed
bandit problem. Machine Learning, 47(2):235–256, May 2002. ISSN 1573-0565. doi:
10.1023/A:1013689704352. URL https://doi.org/10.1023/A:1013689704352.
13
Champion et al.
Matthew Botvinick and Marc T oussaint. Planning as inference . Trends in Cognitive Sciences,
16(10):485 – 488, 2012. ISSN 1364-6613. doi: https://doi.o rg/10.1016/j.tics.2012.08.006.
C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P . I. Cowlin g, P . Rohlfshagen,
S. T avener, D. Perez, S. Samothrakis, and S. Colton. A survey of Monte Carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43,
2012.
Théophile Champion, Howard Bowman, and Marek Grześ. Branchin g time active inference:
empirical study . arXiv, 2021a.
Théophile Champion, Howard Bowman, and Marek Grześ. Branchin g time active inference:
the theory and its generality . arXiv, 2021b.
Théophile Champion, Marek Grześ, and Howard Bowman. Realizi ng Active Inference in
V ariational Message Passing: The Outcome-Blind Certainty S eeker. Neural Computa-
tion, 33(10):2762–2826, 09 2021c. ISSN 0899-7667. doi: 10.1162 /neco_a_01422. URL
https://doi.org/10.1162/neco_a_01422.
Maell Cullen, Ben Davey , Karl J. F riston, and Rosalyn J. Moran . Active inference
in OpenAI Gym: A paradigm for computational investigations into psychiatric ill-
ness. Biological Psychiatry: Cognitive Neuroscience and Neuroi maging, 3(9):809 –
818, 2018. ISSN 2451-9022. doi: https://doi.org/10.1016/ j.bpsc.2018.06.010. URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617. Compu-
tational Methods and Modeling in Psychiatry .
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan V ese lic, Vic-
torita Neacsu, and Karl F riston. Active inference on discre te state-
spaces: A synthesis. Journal of Mathematical Psychology , 99:102447, 2020.
ISSN 0022-2496. doi: https://doi.org/10.1016/j.jmp.202 0.102447. URL
https://www.sciencedirect.com/science/article/pii/S0022249620300857.
14
Branching Time Active Inference
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl F riston. D opamine,
reward learning, and active inference. Frontiers in Computational Neuro-
science, 9:136, 2015. ISSN 1662-5188. doi: 10.3389/fncom.2015.00 136. URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
G. D. F orney. Codes on graphs: normal realizations. IEEE Transactions on Information
Theory, 47(2):520–548, 2001.
Zafeirios F ountas, Noor Sajid, Pedro A. M. Mediano, and Karl F riston. Deep active inference
agents using Monte-Carlo methods. arXiv, 2020.
V. F ox, J. Hightower, Lin Liao, D. Schulz, and G. Borriello. Bay esian ﬁltering for location
estimation. IEEE Pervasive Computing , 2(3):24–33, 2003. doi: 10.1109/MPR V.2003.
1228524.
Karl F riston, Thomas FitzGerald, F rancesco Rigoli, Philip p Schwartenbeck, John O Doherty ,
and Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Re-
views, 68:862 – 879, 2016. ISSN 0149-7634. doi: https://doi.org/ 10.1016/j.neubiorev.
2016.06.022.
Karl F riston, Thomas FitzGerald, F rancesco Rigoli, Philip p Schwartenbeck, and Gio-
vanni Pezzulo. Active Inference: A Process Theory . Neural Computation , 29
(1):1–49, 01 2017. ISSN 0899-7667. doi: 10.1162/NECO_a_00 912. URL
https://doi.org/10.1162/NECO_a_00912.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts hum an at-
tention. Vision Research , 49(10):1295 – 1306, 2009. ISSN 0042-
6989. doi: https://doi.org/10.1016/j.visres.2008.09.0 07. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Vi-
sual Attention: Psychophysics, electrophysiology and neu roimaging.
15
Champion et al.
Beren Millidge. Combining active inference and hierarchica l predictive coding: A tuto-
rial introduction and case study . PsyArXiv, 2019. doi: 10.31234/osf.io/kf6wc. URL
https://doi.org/10.31234/osf.io/kf6wc.
Corrado Pezzato, Carlos Hernandez, and Martijn Wisse. Acti ve inference and behavior trees
for reactive action planning and execution in robotics. arXiv, 2020.
Eva Rafetseder, Maria Schwitalla, and Josef Perner. Counte rfactual reasoning: F rom child-
hood to adulthood. Journal of experimental child psychology, 114(3):389–404, 2013.
Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End -to-end pixel-based deep
active inference for body perception and action. arXiv, 2020.
Philipp Schwartenbeck, Johannes Passecker, T obias U Hause r, Thomas H B FitzGer-
ald, Martin Kronbichler, and Karl F riston. Computational m echanisms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau rent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonogl ou, V edavyas Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nha m, Nal Kalchbren-
ner, Ilya Sutskever, Timothy P . Lillicrap, Madeleine Leach , Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with de ep neural networks
and tree search. Nature, 529(7587):484–489, 2016. doi: 10.1038/nature16961. URL
https://doi.org/10.1038/nature16961.
John Winn and Christopher Bishop. V ariational message passi ng. Journal of Machine
Learning Research, 6:661–694, 2005.
Ozan Çatal, Tim V erbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning
perception and planning with deep active inference. arXiv, 2020.
16