arXiv:2506.10420v1  [cs.AI]  12 Jun 2025
Multi-dimensional Autoscaling of Processing
Services: A Comparison of Agent-based Methods
Boris Sedlak1,∗ , Alireza Furutanpey1 , Zihang Wang1 , Víctor Casamayor
Pujol2 , and Schahram Dustdar1,2
1 Distributed Systems Group, TU Wien, Vienna, Austria.
2 DISL, ICREA | Universitat Pompeu Fabra, Barcelona, Spain
* Corresponding author contact:b.sedlak@dsg.tuwien.ac.at
Abstract. Edge computing breaks with traditional autoscaling due to
strict resource constraints, thus, motivating more flexible scaling behav-
iors using multiple elasticity dimensions. This work introduces an agent-
based autoscaling framework that dynamically adjusts both hardware
resources and internal service configurations to maximize requirements
fulfillmentinconstrainedenvironments.Wecomparefourtypesofscaling
agents: Active Inference, Deep Q Network, Analysis of Structural Knowl-
edge, and Deep Active Inference, using two real-world processing services
running in parallel: YOLOv8 for visual recognition and OpenCV for QR
code detection. Results show all agents achieve acceptable SLO perfor-
mance with varying convergence patterns. While the Deep Q Network
benefits from pre-training, the structural analysis converges quickly, and
the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environ-
ments and encourage future work in this research direction.
Keywords: Internet of Things· Stream Processing· Active Inference
· Autoscaling · Markov Decision Processes· Reinforcement Learning
1 Introduction
The rise of Edge Computing and the Computing Continuum (CC) addresses
the limitations of traditional Cloud infrastructures [4]. By bringing computation
closer to users and data sources (e.g., IoT devices) these paradigms significantly
reduce network latency, critical for applications that demand near real-time re-
sponses, such as autonomous driving, e-health, and virtual reality. A common
use case, as depicted in Figure 1, could be to detect entities in a video stream
(e.g., humans) or tracking objects that have a QR code attached. By running
these inference services locally, the overall network congestion is also mitigated
by minimizing long-distance data transfers.
However, Edge and CC environments introduce new challenges [1]: They rely
on resource-constrained computing hardware, and thus break with traditional
Cloud-based autoscaling. In Cloud systems, autoscaling mechanisms elastically
2 Sedlak et al.
respond to increased user demand by allocating more resources to a service or
replicating it. This is infeasible at the Edge or in the CC, where computing re-
sources are strictly limited [9]. Especially when resources are scarce, applications
require a more flexible scaling behavior that uses a wider range of adaptations
– hence, operating in multipleelasticity dimensions [3]. On the one hand, this
protectstheserviceexecutionandpromiseshigherrequirementsfulfillment–cap-
tured through a set of Service Level Objectives (SLOs). On the other hand, this
increases the complexity for choosing optimal scaling actions. What is needed,
hence, are lightweight multi-dimensional scaling mechanisms that optimize the
service execution without obstructing existing workloads.
To fill this gap, we propose an agent-based autoscaling approach tailored for
Edge and CC systems, which adjusts processing services in multiple elasticity
dimensions. Our approach employs decentralized local agents that (1) observe
the service execution and their SLO fulfillment without centralized control; thus,
we can monitor the resource allocation per service or the application through-
put. If SLOs are violated, the agents attempt to restore the desired state by (2)
adjusting the service execution; the exact scaling policy is learned by the agent
according to environmental feedback. Notably, our approach allows scaling poli-
cies tailored to the individual services, where one service could, for example,
scale down its machine learning (ML) model, while another service claim the
remaining resources. This allows building composite and customizable scaling
policies, which go further than existing approaches [13].
To show the viability of our approach, we implement four different versions of
our general agent and compare their performance in a processing environment,
where the agent needs to dynamically scale two processing services on an Edge
device. In particular, we compare an Active Inference agent (AIF), a Deep Active
Inference agent (DACI), a Deep Q-Network agent (DQN), and an agent using a
numerical solver – called Analysis of Structural Knowledge (ASK). During our
experiments, a scaling agent manages two physically executed services: one for
video stream inference (Figure 1a) using the well-known Yolov8 model [15], and
another for QR code reading (Figure 1b), implemented with OpenCV2 [11].
On the short term, our work provides well-needed baselines for comparing
the performance of different agent-based approaches – particularly needed for
emerging solutions. On the long term, our evaluation environment is extensible
and allows incorporating other agents. We summarize our contributions as:
1. Introducing an agent-based autoscaling approach within amulti-dimensional
elasticity spacethat dynamically maximize SLO fulfillment in IoT and Edge
environments by adjusting hardware and service configurations
2. Evaluating onreal-world applicationsof four distinct scaling agent architec-
tures (Active Inference, Deep Active Inference, Deep Q-Network, and Anal-
ysis of Structural Knowledge) for real-time service orchestration.
3. Providing a benchmarking environment for future autonomous research and
demonstrating viability through agent-based resource allocation for parallel
processing services (YOLOv8 and OpenCV) on constrained hardware.
Multi-dimensional Autoscaling of Processing Services: Agent-based Methods 3
(a) CV (Yolov8)
 (b) QR (OpenCV)
Fig.1: Demo output of the results produced by the two processing services
2 Preliminaries
In the following, we provide a formal description of the problem, as well as how
researchers have addressed it so far with agent-based methods – including AIF.
2.1 Problem Definition
As depicted in Figure 2, multiple services are executed within one Edge device
– sharing the device’s processing resources between them. The execution of the
individual services is affected by the amount of resources allocated (e.g., CPU, or
RAM), and the configuration of the service-internal parameters (e.g., input qual-
ity, or model size). Considering that both sets of parameters can beelastically
adjusted during runtime [3], we summarize them as elastic configurations.
The allocated resources and the service configuration influence the degree
to which the service outcome satisfies the client requirements (i.e., the SLOs).
However, and this is the core of the problem, it is not a priori known what
will be the resulting SLO fulfillment for a specific configuration. Hence, the
problem boils down to adjusting the elastic configurations in such a way, that
the SLO fulfillment is maximized. While the number of service configurations
and resource allocations is limited, it is not easily possible to brute-force the
problem by exhaustively searching the solution space. The reason is that actions
taken on the environment require a considerable amount of time to show effect.
For instance, orchestration tools like Docker and Kubernetes usually consider a
cooldown period of several minutes after taking an action.
Formal DefinitionMore formally, the problem domain and the physical pro-
cessing environment is defined as follows:
Processing HardwareAn Edge devicedis defined by its hardware constrainsH,
e.g., the physical number of CPU corescphy and RAM capacityrphy, and the
set of processing servicesS that is executed there; henced= ⟨H,S⟩.
4 Sedlak et al.
Fig.2: Services compete for limited device resources; according to SLO fulfillment, the
service execution is adjusted by elastically changing device and service configurations
Processing ServicesEach processing service (s ∈S) is characterized through a
service type t, a set of SLOsQ, and a service-internal configurationK; hence
s = ⟨t,Q,K,c s⟩, where a service (s) has a number of cores (cs) allocated to it,
for whichcs ≤cphy must hold. Each SLOsq∈Qwith q= ⟨v,t⟩tracks a variable
v and reflects whether its current assignment reaches a target (t).
Service MonitoringTo track the system state and the SLO fulfillment, the device
d and its servicess ∈S continuously monitor the execution at different levels.
This provides a set of software- or application-related metrics (Ms), and a set
of device-related metrics (Md) that capture the hardware state.
SLO FulfillmentUsingthesemetrics,wecalculatethecontinuousSLOfulfillment
(ϕ) for a metrics (m∈M) and an SLOq as shown in Eq. (1).
ϕ(q,m) =
{
m
tq
if m≤tq
1.0 if m>t q
(1)
This means, that SLOs cannot be overfulfilled; for instance, if the target is
keeping the service throughput (tp) ≥30, both assignments formtp = 40 and
mtp = 100achieve the maximum SLO fulfillment ofϕ= 1.0.
2.2 Related Work
In the following, we identify competing approaches on autoscaling that do not
use AIF, and others that use AIF agents.
Mostnotably,competingapproachesandtraditionalautoscalingmethods[17,18]
struggleinenvironments,wheremulti-dimensionaladaptationiscrucialformain-
taining service quality. In particular, RL-based systems still find no productive
use by large providers despite over a decade of research. We argue that AIF-
based agents offer a promising alternative by enabling adaptive, efficient, and
Multi-dimensional Autoscaling of Processing Services: Agent-based Methods 5
decentralized control suited to the dynamic and uncertain nature of complex
multi-tier distributed systems. Hence, our focus is on further establishing AIF
for service adaptation on resource-restricted devices.
Among existing works, Sedlak et al. [14] explore how AIF can be used to op-
timize SLO fulfillment across various use cases. Danilenka et al. [2] demonstrate
how AIF agents can adapt to dynamic and heterogenous environments. Pujol et
al. [12] adopt a representation using Partially Observable Markov Decision Pro-
cesses (POMDPs) within a multi-agent system. Lapkovskis et al. [8] presented
a comparison of AIF agents with other methods for a CC application that com-
plements our work with a different set of algorithms. Vyas et al. [16] provide an
adaptation mechanism for active sensing on Edge devices, which dynamically
adjust perception by changing the camera orientation.
3 Methodology
In this section, we first present the general design of a scaling agent that maxi-
mizes the SLO fulfillment for a set of services under limited processing resources.
Afterward, we show four approaches of how this agent can be implemented.
Namely, we present a traditional RL agent using a Deep Q-Network (DQN),
an Bayesian agent using AIF, a Deep Active Inference (DACI) agent, and an
algebraic agent based on Analysis of Structural Knowledge (ASK).
3.1 Processing Environment
To evaluate our scaling agents for a real-world problem, we need a processing
environment in which to operate. For this, we first introduce the structure of the
two processing services and how they are monitored during runtime. Lastly, we
introduce our training environment for pretraining some of the agents.
Processing ServicesIn the following, we introduce the two processing services
that will be executed in parallel on an Edge device. By limiting ourselves to two
services, we decrease the complexity of the optimization problem, while retaining
the central issue of resource limitations.
Computer Vision (CV)The CV service processes a continuous stream of video
frames to perform object detection on them. For this, it uses Yolov8 [15], a DNN
model developed by Ultralytics; Figure 1a showcases its output.
The service state for CV is composed by four features, as displayed in Table 1:
quality determines the ingested video resolution,model size describes the Yolo
model(e.g.,v8norv8m),and coresdeterminesthemaximumresourcesallocated;
throughput describes the service output in terms of frames per second.
The action state for CV is a subset of these variables – as part of the service
configuration (K), it is possible to adjustquality and model size, whilecores(cs)
is another property of the service. The exception isthroughout, which cannot be
directly set, but is statistically dependent on the three other features.
6 Sedlak et al.
variable type range actionable dependent SLO target
quality int [128, 320] ✓ (step= 32) — ≥ 288
model size int [1, 5] ✓ (step=1) — ≥ 3
cores float (0, 8) ✓ (no step) — —
throughput int [0, 100] — all others ≥ 5
Table 1: Variables of the CV service used for sensing and acting on the environment
QR Code Reader (QR) The QR service scans a continuous video stream to
detect QR codes within the individual video frames. For this, it uses the Python
wrapper of OpenCV [6]; Figure 1b showcases the service output.
The service state for QR is composed by three features, as displayed in Ta-
ble 2. The three features are defined analog to the CV service; the exception is
that QR does not use a specific model size but a fixed algorithm.
variable type range actionable dependent SLO target
quality int [300, 1000] ✓ (step= 100) — ≥ 900
cores float (0, 8) ✓ (no step) — —
throughput int [0, 100] — all others ≥ 60
Table 2: Variables of the QR service used for sensing and acting on the environment
To quantify the requirements and preferences on how the services should
operate, both Table 1 & 2 contain the precise SLO targets. Despite the fact that
both services operate on a video stream, they differ greatly in their input shape
and the expectedthroughput. Notably, QR has a high expected throughput of
60 frames per second, while the resource-heavy CV has a target of 5.
Service Monitoring The two processing services operate in batches of 1 sec-
ond: at the beginning of each iteration, 100 frames are ingested to the service;
when the processing timeframe (i.e., 1000 ms) is exceeded, the service counts the
number of processed frames – thethroughput. This information, together with
the service properties, is then collected in a time-series DB.
Later, when a scaling agent wants to resolve the service states, it can query
this information through the time-series DB. A key advantage of this approach,
is that it allows computing sliding-windows over monitored variables; thus, it
stabilizes an agent’s perception against temporary perturbations, like momen-
tary drops in throughput. Stable states are most relevant for evaluating SLOs
and avoiding overhasty scaling decisions, e.g., the Kubernetes HPA3 considers
per default a time window of 30min; in our case, we will stick to 5 seconds.
Training EnvironmentChanges to the processing environment need time to
show effect. This, however, is conflictive with contemporary ML training, which
often requires thousands of iteration to converge to a policy. Hence, sample-
efficient methods, like model-based algorithms (also evaluated in this paper)
3 https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
Multi-dimensional Autoscaling of Processing Services: Agent-based Methods 7
move into focus. However, to not exclude traditional RL algorithms, like DQN,
we supply a training environment based on the Gymnasium framework4.
To estimate the state transition probabilities for state-action pairs, we use
a part of the monitored metrics to train a Linear Gaussian Bayesian Network
(LGBN); this can be seen as a simplified replication of the real processing envi-
ronment. For an assignment of free (i.e., actionable) variables, the LGBN sam-
ples the remaining variables – in this casethroughput. Training on the LGBN
environment, scaling agents can infer expected state and reward of actions.
3.2 General Agent Design
To optimize SLO fulfillment, we supervise the service execution through a ded-
icated scaling agent, executed locally on the processing device. Generally, our
agent follows a simple two-step scheme in which it first resolve the states of
services and hardware, and then acts on the processing environment. To react
to dynamic runtime behavior, our scaling agents iterate in cycles of 5 seconds –
thus adhering to the sliding window for service monitoring.
Perception At the beginning of each iteration, the agent queries the service
states through the time-series DB (cfr. Section 3.1). Next, it iterates through the
services (s∈S) and evaluates their SLO fulfillment as in Eq. (1). This indicates
the degree to which requirements are currently fulfilled.
To determine the amount of unclaimed resources, the agent then resolves
the number of cores (cs) currently assigned to the individual services5and the
number of free cores (cfree) on the deviced as in Eq. (2).
cfree = cphy −
∑
s∈S
cs. (2)
Action After resolving the environmental state, the agent adjusts the process-
ing environment according to its preferences by changing actionable service vari-
ables. However, the exact policy depends on the implementation of the scaling
agent and will be explained in the next section. For example, some agents will
try solving this by training an accurate world model, e.g., the AIF agent, while
the DQN will use simplified state-action networks. Most importantly, this also
determines how the different scaling agents explore the solution space.
3.3 Scaling Agent Implementations
In the following, we describe four different implementations of the general scaling
agent, using different agent-based approaches. The code for the agents, as well
as for the processing services, can be found in the following repository6.
4 https://gymnasium.farama.org/api/env/
5 Internally, the services are executed in a containerized environment – Docker; the
claimed cores per service can be queried by our agent running on the same machine.
6 Repository with implementations for scaling agents and processing services
8 Sedlak et al.
Active Inference (AIF) The AIF agent is defined following the same idea
as in [12], which leveragespymdp [7]. Hence, we model the agent as a Markov
Decision Process and specifying the transition model using Dynamic Bayesian
Networks and Conditional Probability Tables (CPTs). In particular, the influ-
ence of actions over certain state factors, such asthroughput, whose transition
dynamics are unknown, is initially defined as a uniform probability distribu-
tion within the CPTs7. These uniform priors allow the agent to learn the true
dynamics through experience and improve SLO fulfillment.
The AIF agent simultaneously controls both services, which increases its
state and action spaces. To maintain practical computational times, the agent
can choose only one action per service: modifying dataquality, model size, or
number of cores. This reduces computational demands but also restricts the
agent’s capacity for more dynamic and fine-grained interventions. Additionally,
compared to the other agents in this article, the state space factors for this AIF
agent are discretized into coarser bins. While this lowers computational cost, it
results in less precise action selection and coarser estimates of SLO fulfillment.
The final state space of the agent consists of 7 factors with 3,457,440 state
combinations, and an action space of 35 different combinations.
Using this setup, the pymdp implementation (using v0.0.7.1) remained pro-
hibitively slow, requiring approximately 20 seconds per iteration for policy infer-
ence and observation model construction. We address these bottlenecks through
two algorithmic optimizations: (1) vectorized policy evaluation eliminates nested
loops in expected free energy calculations, and (2) sparse matrix operations re-
duce overhead in belief updating. These improvements achieved 20-30x speedup
in inference routines, leading to the performance describing in Figure 3b.
Deep Active Inference (DACI)While traditional AIF implementations us-
ing graphical models provide excellent interpretability, they are challenging to
scale, limiting their practicality in real-world applications with high-dimensional
input spaces. DACI addresses scalability in the CC by exploiting the abundant
availability of hardware acceleration (e.g., TPUs, NPUs) for Artificial Neural
Networks (ANNs). This work presents preliminary results on Deep Active Infer-
ence for the CC based on the work by Fountas et al [5]. The basic idea is to learn
non-linear transforms with ANNs to map the high-dimensional input space into
a compressed latent representation of the environment. Then, the agent may
operate efficiently in complex, multi-dimensional elasticity spaces that would
be intractable for discrete graphical models. Analogous to any active inference
agent, it 1) samples the environment and calibrates its internal generative model
toexplainsensoryobservationsand2)performsactionstoreducetheuncertainty
about the environment. We slightly simplify and adjust the original objective in
7 The interested reader can find the matrix definition in the following directory.
Multi-dimensional Autoscaling of Processing Services: Agent-based Methods 9
[5] to compute the variational free energy for each time-stept as:
F′
t = EQϕs(st) [log Pθo(ot|st)]
+ DKL [Qϕs(st) ∥Pθs(st|st−1,at−1)]
+ λeq ·Var[fulfillmentservices]
+ λutil ·
(
1 −ucores
tcores
)2
We have omitted the KL divergence involving the habitual network and included
regularization terms to reduce the variance between service fulfillments and en-
courageresourceutilization.Ourintuitionistoregularizetheterm −E[log P(oτ|π)]
that represents preferences about future observations by encoding the specifica-
tion for balanced services and resource efficiency to have a higher prior probabil-
ity. Lastly, we encode the objective to prioritize actions that reduce uncertainty
about the environment by minimizing the EFE with an expression that is equiv-
alent to that in the original work [5].
Deep Q Networks (DQN)The DQN [10] agent approximates Q-values for
state-action pairs in the multi-dimensional autoscaling problem. We implement
separate DQN models for each processing service (CV and QR), allowing service-
specific policy learning while maintaining coordinated resource allocation when
competing for limited resources. The models are trained jointly within the shared
processing environment to benefit from the pre-training in the simulated LGBN
environment, resulting in the DQN agent achieving stable performance from the
initial deployment phase.
Analysis of Structural Knowledge (ASK)This agent solves the orchestra-
tion problem through numerical optimization, using SLSQP. For this, it consider
the variables and their parameter boundaries from Table 1 & 2, the objective
function ϕ for calculating the SLO fulfillment of a parameter assignment, and
the SLOs (Q). Given that, it misses a continuous function that allows descend-
ing to the optimal solution. In a nutshell: how to estimatethroughput, given the
assignments of all bounded parameters – including continuous-scalecores?
We address this, similarly to the LGBN training environment, through a
regression model that captures the dependencies between service variables (K)
and the allocated cores (ck). As the agent captures more metrics, the accuracy
of the regression and the inferred parameter assignment improves. To create an
accurate model, the ASK agent needs an initial exploration time (i.e., 20 itera-
tions in the experiments), during which the parameters are assigned randomly.
After this time, the ASK is ready for the numerical optimization.
For our two services1 and s2 and all their variablesx∈{Ks∪cs}, the ASK
agent infers assignments that maximize the SLO fulfillment as in Eq. (3).
max
x
∑
s∈S
ϕs(x) subject to
∑
s∈S
cs ≤cphy, x i ∈[xmin
i ,xmax
i ] (3)
10 Sedlak et al.
Thispresentsacoupleofadvantages:First,thecontinuousactionspaceallows
agents to infer fine-grained elasticity strategies; in particularcores can be split
precisely between the devices. Also, it is possible to infer assignments for all
services and all their elasticity parameters in one operation.
4 Evaluation
4.1 Experimental Design
To analyze the performance of the different scaling agents we evaluate them, one
after the other, in the processing environment (i.e., the two processing services
executed in parallel). In each experiment we capture the following performance
metrics: (1) SLO fulfillment, as the average over the two services, and (2) com-
plexity of inference, as the duration for running one inference cycle.
Each experiment contains 50 iterations within the processing environment,
i.e., 50 times that the agent interacts with the environment by sensing its state
and acting on it. As described in Section 3.2, the agents operate with a frequency
of 5 seconds – every 5 seconds they orchestrate the services according to the
inferred policy. Thus, individual experiments take5 ×50 = 250 seconds. To
stabilize the empirical results, we repeat each experiment 10 times.
During the experiments, agents must achieve the SLO targets specified in
Table 1 & 2. To fulfill these SLOs, an agent needs to optimally adjust the services
and split the resources – in this case 8 physical cores (i.e.,cphy = 8).
4.2 Experimental Results
Figure 3a shows the SLO fulfillment rate of all agents over the 50 steps of the ex-
periment. The ASK agent requires approximately 20 iterations to train and sta-
bilize, reaching an average SLO fulfillment rate of0.868 over the final 10 steps.
In contrast, the DQN agent, having been pre-trained in the simulation environ-
ment, maintains stable performance throughout the experiment and achieves an
average of0.753 in the last 10 iterations. Regarding the active inference-based
methods, the AIF agent takes around 10 steps to stabilize its performance, ul-
timately reaching an average fulfillment rate of0.704 during the final 10 steps.
Lastly, although the DACI agent exhibits steady improvement, it only achieves
an average SLO fulfillment rate of0.724 by the end of the experiment.
In terms of computational speed (Figure 3b) there is a considerable difference
between the agents – note that the y-axis is plotted in log10 scale. The DQN
agent remains the fastest with a mean of60 milliseconds, benefiting from ma-
ture neural network optimizations. Our optimized AIF agent achieves an average
time of1.7seconds, demonstrating substantial improvement through vectorized
inference over the original implementation with roughly20 seconds. The DACI
and ASK agents exhibit execution times of 2.8 seconds and 1.2 seconds re-
spectively, highlighting the computational trade-offs inherent in their respective
approaches. When merely exploring (expl) the ASK agent also took less time
Multi-dimensional Autoscaling of Processing Services: Agent-based Methods 11
(a) SLO fulfillment of different agents
 (b) Duration of cycles (log10) in milliseconds
Fig.3: Scaling agents operating on the processing environment for 50 iterations (=250
seconds); 10 experiment repetitions per agent type to stabilize results
than during the actual numerical inference (inf). Notably, all agents (incl. the
AIF) now operate within practical time constraints for the 5-second scaling in-
terval, enabling real-time performance comparison across all four methods.
5 Conclusion & Future Work
This article highlights a key need for orchestration under resource limitations:
multi-dimensional elasticity. By optimizing resource allocation and finding qual-
ity trade-offs, it supports flexible scaling behavior that improve Service Level
Objectives (SLOs). To show the capabilities of different learning methods for
solving such problems, we developed a real-world processing environment where
two services compete for resources. We compared four different agents: an Active
Inference (AIF) agent usingpymdp, a Deep Q-Network (DQN), a Deep Active
Inference (DACI) agent, and an agent using Analysis of Structural Knowledge
(ASK). Our evaluation shows that ASK achieved the highest SLO fulfillment
(0.87), with AIF, DQN and DACI roughly taking the second spot together; AIF
and ASK, which learn online, took about 10 and 20 rounds to converge respec-
tively. In terms of execution time, agents must infer a scaling policy in less than
5 seconds – our orchestration interval. Our optimized AIF agent achieved an
average of 1.7 seconds per iteration, showcasing our improvement of the pymdp
library. However, it was the DQN agent that excelled with an average execution
under 60 ms. While DACI exhibited the longest execution time, it is arguably the
most promising approach as it combines theoretical foundations with practical
scalability, which are a necessity in large-scale distributed systems.
We emphasize that we only present early-stage results. Besides further im-
provements to the neural architecture and the objective, careful theoretical anal-
ysis, extensive experimentation, and optimizations that can exploit parallel pro-
cessing are works in progress. Moreover, future work will refine agents for dis-
tributed settings, explore service-hardware interaction, and develop tools for
easier configuration and orchestration. We also plan to extend the framework to
support a broader set of real-world use cases.
12 Sedlak et al.
References
1. Casamayor-Pujol, V., Morichetta, A., Murturi, I., Donta, P.K., Dustdar, S.: Fun-
damental Research Challenges for Distributed Computing Continuum Systems.
Information 14, 198 (Mar 2023). https://doi.org/10.3390/info14030198
2. Danilenka, A., Furutanpey, A., Pujol, V.C., Sedlak, B., Lackinger, A., Ganzha, M.,
Paprzycki, M., Dustdar, S.: Adaptive Active Inference Agents for Heterogeneous
and Lifelong Federated Learning (Oct 2024)
3. Dustdar, S., Guo, Y., Satzger, B., Truong, H.L.: Principles of Elastic Processes.
Internet Computing, IEEE15, 66–71 (Nov 2011)
4. Dustdar, S., Pujol, V.C., Donta, P.K.: On Distributed Computing Continuum Sys-
tems. IEEE Transactions on Knowledge and Data Engineering35(4), 4092–4105
(Apr 2023). https://doi.org/10.1109/TKDE.2022.3142856
5. Fountas, Z., Sajid, N., Mediano, P., Friston, K.: Deep active inference agents us-
ing monte-carlo methods. Advances in neural information processing systems33,
11662–11675 (2020)
6. Heinisuo, O.P.: opencv-python: Wrapper package for OpenCV python bindings.
(2021), https://github.com/skvark/opencv-python
7. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., Tschantz,
A.: pymdp: A Python library for active inference in discrete state spaces. Journal
of Open Source Software (May 2022)
8. Lapkovskis,A.,Sedlak,B.,Magnússon,S.,Dustdar,S.,Donta,P.K.:Benchmarking
Dynamic SLO Compliance in Distributed Computing Continuum Systems (Mar
2025). https://doi.org/10.48550/arXiv.2503.03274
9. Manaouil, K., Lebre, A.: Kubernetes and the Edge? report, Inria Rennes - Bretagne
Atlantique (Oct 2020), https://inria.hal.science/hal-02972686
10. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing atari with deep reinforcement learning (2013)
11. opencv: opencv at 4.9.0 (2024), https://github.com/opencv/opencv/tree/4.9.0
12. Pujol, V.C., Sedlak, B., Salvatori, T., Friston, K., Dustdar, S.: Distributed Intel-
ligence in the Computing Continuum with Active Inference (May 2025). https:
//doi.org/10.48550/arXiv.2505.24618
13. Sedlak, B., Morichetta, A., Raith, P., Pujol, V.C., Dustdar, S.: Towards Multi-
dimensional Elasticity for Pervasive Stream Processing Services. In: 2025 IEEE In-
ternational Conference on Pervasive Computing and Communications Workshops
and other Affiliated Events (PerCom Workshops) (2025)
14. Sedlak, B., Pujol, V.C., Donta, P.K., Dustdar, S.: Equilibrium in the Computing
Continuum through Active Inference. Future Generation Computer System160,
92–108 (2024). https://doi.org/10.1016/j.future.2024.05.056
15. Varghese, R., M., S.: YOLOv8: A Novel Object Detection Algorithm with En-
hanced Performance and Robustness. In: ADICS (2024)
16. Vyas, D., Prado, M.d., Verbelen, T.: Towards smart and adaptive agents for active
sensing on edge devices (Jan 2025). https://doi.org/10.48550/arXiv.2501.06262
17. Wang, Z., Li, P., Liang, C.J.M., Wu, F., Yan, F.Y.: Autothrottle: A Practical Bi-
Level Approach to Resource Management for SLO-Targeted Microservices (2024)
18. Zhao, Y., Uta, A.: Tiny Autoscalers for Tiny Workloads: Dynamic CPU Allocation
for Serverless Functions. IEEE Comp. Society (2022). https://doi.org/10.1109/
CCGrid54584.2022.00026