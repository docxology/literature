arXiv:1710.09118v3  [q-bio.NC]  20 Jan 2018
Recognition Dynamics in the Brain under the Free
Energy Principle
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61 186, Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. We formulate the computational processes of perception in the fr amework
of the principle of least action by postulating the theoretical action as a time integral
of the free energy in the brain sciences. The free energy principle is accordingly
rephrased as that for autopoietic grounds all viable organisms att empt to minimize
the sensory uncertainty about the unpredictable environment ov er a temporal horizon.
By varying the informational action, we derive the brain’s recognitio n dynamics
(RD) which conducts Bayesian ﬁltering of the external causes fro m noisy sensory
inputs. Consequently, we eﬀectively cast the gradient-descent s cheme of minimizing
the free energy into Hamiltonian mechanics by addressing only positio ns and momenta
of the organisms’ representations of the causal environment. T o manifest the
utility of our theory, we show how the RD may be implemented in a neuro nally
based biophysical model at a single-cell level and subsequently in a c oarse-grained,
hierarchical architecture of the brain. We also present formal so lutions to the RD for a
model brain in linear regime and analyze the perceptual trajectorie s around attractors
in neural state space.
Keywords: recognition dynamics, Bayesian ﬁltering, perception, free energ y principle,
sensory uncertainty, informational action, principle of least actio n
Recognition Dynamics in the Brain under the Free Energy Prin ciple 2
1. Introduction
The quest for a universal principle that may explain the cognitive and behavioral
operation of the brain is of great scientiﬁc interest at present. Th e apparent diﬃculty in
addressing the quest is the gap between the information processin g and the biophysics
that governs neurophysiology in the brain. However, it is evident th at the matter,
which is the ground-stuﬀ upon which the brain functions emerge, co mprises neurons
obeying the laws guided by physics principles. Thus, any biological prin ciples that
attempt to explain the brain’s large-scale workings must cope with ou r accepted physical
reality [1]. It appears that on the current approaches still prevails the classical, eﬀective
epistemology of regarding perceptions as constructing hypothes es which may hit upon
truth by producing symbolic structures matching physical reality [2 , 3, 4].
One inﬂuential candidate at present that seeks for such a rubric in neuroscience
is the free energy principle (FEP) [5, 6, 7]. For a technical appraisal of the FEP, we
refer to [8] where the theoretical assumptions and the mathemat ical structure involved
in the FEP are reviewed in great detail. We have noticed [9] suggesting ‘variational
neuroethology’ which explains, integrating the FEP with evolutionar y systems theory,
how living systems appear to resist the second law of thermodynamic s. To state
compactly, the FEP oﬀers that all viable organisms perceive and act on the external
world by instantiating a probabilistic causal model embodied in their br ain in a way to
ensure their adaptive ﬁtness or autopoiesis [10]. The biological mech anism that endows
the organism’s brain with the operation is theoretically framed into an information-
theoretic measure, which we call ‘informational free energy (IFE) ’. According to the
FEP, a living system tries to minimize the sensory surprisal when it fac es an external
cause that perturbs its spontaneous equilibrium within its physiologic al boundary by
pursuing perceptive as well as active inferences. However, the br ain does not preside
over instreaming sensory distribution; accordingly, the brain cann ot directly minimize
the sensory surprisal but, instead, minimizes its upper bound, the IFE. The probabilistic
rationale of the FEP argues that the brain’s representations of th e uncertain environment
are the suﬃcient statistics, e.g., means or variances, of a probabilit y density encoded in
the brain. The variational parameters are supposed to be encode d as physical variables
in the brain. The brain statistically infers the external causes of se nsory input by
Bayesian ﬁltering, using its internal top-down model about predict ing, or generating,
the sensory data. Filtering is a probabilistic approach to determining the external states
from noisy measurements of sensory data [11]. There is growing ex perimental support for
the brain’s maintaining internal models of the environment to predict sensory inputs and
to prepare actions, see for instance [12]. The computational oper ation of the abductive
inference is subserved by the brain variables and the resulting perc eptual mechanics is
termed as the ‘recognition dynamics (RD)’.
Although the suggestion of the FEP has been promising to account f or the brain’s
inferring mechanism of and acting upon sensory causes, we ﬁnd cer tain theoretical
subtleties in the conventional formulation:
Recognition Dynamics in the Brain under the Free Energy Prin ciple 3
First, the FEP minimizes the IFE at each point in time for successive se nsory
inputs [13]. However, precisely the objective function to be minimized is the
continuously accumulated IFE over a ﬁnite time. ; The minimization must be
performed concerning trajectories over a temporal horizon acr oss which an organism
encounters with atypical events to its natural habitat and biology .
Second, the FEP employs the gradient-descent method in practica lly executing
minimization of the IFE [15], which is widely used in machine learning theory to
solve engineering optimization problems eﬃciently. The engaged sche me allows
formulation to ﬁnd heuristically optimal solutions in the FE landscape, but it not
derived from a scientiﬁc principle.
Third, the FEP introduces the notion of the ‘generalized coordinate s’ of an inﬁnite
number of the so-called ‘generalized motions’ to account for the dy namical nature
of the environment [16]. The ensuing theoretical construct is a gen eralization of the
standard Newtonian mechanics. § With the hired theory, however, it is obscure to
decide the number of independent dynamical variables for a complet e description.
In practice, typically dynamic truncation is made at a ﬁnite embedding order by
assuming that the precision of random ﬂuctuations on higher order s of motion
disappears very quickly.
Fourth, the FEP introduces the hydrodynamics-like concepts of t he ‘path of a
mode (motion of expectation)’ and the ‘mode of a path (expected m otion)’ by
distinguishing the dynamic update from the temporal update of a tim e-dependent
state [17]. Because the distinction is essential to ensure an equilibriu m solution
to the RD in employing the dynamical generative models, further the oretical
exploration seems worthwhile.
Fifth, the FEP considers the states of the environment ‘hidden’ be cause what the
brain faces is only a probabilistic sensory mapping. Subsequently, a d istinction
is made between the hidden-state representations, responsible f or intra-level
dynamics, and causal-state representations, responsible for int er-level dynamics, in
the hierarchical brain [18]. Such a distinction must emerge as neuro nal dynamics in
the brain on diﬀerent timescales. Accordingly, a biophysically ground ed formulation
that supports the top-down idea is required.
In this paper, we present a mechanical formulation of the RD in the b rain in the
; According to the FEP, the updating or learning of the generative mo del takes places in the brain
on a longer time scale than that associated with perceptual inferen ce. To derive the RD of the slow
variables for synaptic eﬃcacy and gain, the time-integral of the IF E is taken as an objective function;
however, again the gradient descent method is executed in a pointw ise way in time [14].
§ The mechanical state of a particle is speciﬁed only by position and velo city in the Newtonian
mechanics, and no physical observables are assigned to the dynam ical orders beyond the second-order.
In some literature [19], the concept of ‘jerk’ is assigned to the third -order time-derivative of position as
a physical reality. From the mathematical perspective, such a gen eralization is not forbidden. However,
not only higher-orders are diﬃcult to measure, but more seriously it raises the question of what the
corresponding cause to jerk as the force to acceleration. And, t he same impasse in all next orders.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 4
framework of Hamilton’s principle of least action [20]. Motivated by the aforementioned
theoretical observations, we try to resolve some of the technica l complexities in the
FEP framework. To be speciﬁc, the goal is to recast the gradient- descent strategy of
minimizing the IFE into the mathematical framework that appeals to t he normative
physics rules. We do this by hypothesizing the IFE as a Lagrangian of the brain
which enters the theoretical action, being the fundamental obje ctive function to be
minimized in continuous time under the principle of least action. Conseq uently, we
reformulate the RD regarding only the canonical, physical realities t o eschew the
generalized coordinates of inﬁnitely recursive time-derivatives of t he continuous states
of the organism’s environment and brain. In the canonical descript ion, the dynamical
state of a system is speciﬁed only by positions and their ﬁrst-order derivatives.
In this work, supported by the present day evidence [22, 23], we ad mit the bi-
directional facet in informational ﬂow in the brain. The environment begets sensory
data at the brain-environment interface such as sensory recept ors or interoceptors within
an organism. The incited electro-opto-chemical interaction in sens ory neurons must
transduce forward in the anatomical structure of the brain. Whe reas complying with
the idea of perception as constructing hypotheses, there must b e backward pathway as
well in information processing in the functional hierarchy of the bra in. To understand
how such bidirectional functional architecture is emergent from t he electrophysiology
of biophysics and anatomical organization of the brain is a forefron t research interest
(see, for instance, [24] and references therein). We shall consid er a simple model that
eﬀectively incorporates the functional hierarchy while focusing ou r attention on the
brain’s perceptual mechanics of inferring of the external world, g iven sensory data. The
problem of learning of the environment via updating the internal mod el of the world
and of active inference of changing sensory input via action on the e xternal world, see
for instance [21], is deferred for an upcoming paper.
Here, we shall outline how in this work we cast Bayesian ﬁltering in the F EP using a
variational principle of least action and how we articulate the minimizat ion of the sensory
uncertainty in terms of an associated Lagrangian and Hamiltonian. F urthermore, given
a particular form for the diﬀerential equations, aﬀorded by compu tational neuroscience,
one can see relatively easily how neuronal dynamics could implement th e Bayesian
ﬁltering: (i) According to the FEP, the brain represents the enviro nmental features
statistically eﬃciently, using the suﬃcient statistics µ. We assume that µ stands for
the basic computational unit of the neural attributes of percept ion in the brain. Such
a constituent is considered a ‘perceptual particle’ which may be a sin gle neuron or
physically coarse-grained multiple neurons forming a small particle; (ii) We postulate
that the Laplace-encoded IFE in the FEP, denoted as F (Sec. 2.1), serves as an eﬀective,
informational Lagrangian (IL) of the brain, written as L. Accordingly, the informational
action (IA), which we denote by S, is deﬁned to be time-integral of the approximate
IFE (Sec. 3.1); (iii) Then, conforming to the Hamilton principle of least action, the
equations of motion of the perceptual particles are derived mathe matically by varying
the IA with respect to both µ and 9µ. The resulting Lagrange equations constitute
Recognition Dynamics in the Brain under the Free Energy Prin ciple 5
the perceptual mechanics, i.e., the RD of the brain’s inferring of the external causes
of the sensory stimuli (Sec. 3.1); (iv) In turn, we obtain the brain’s informational
Hamiltonian H from the Lagrangian via a Legendre transformation. Consequent ly,
we derive a set of coupled, ﬁrst-order diﬀerential equations for µ and its conjugate pµ ,
which are equivalent to the perceptual mechanics derived from the Lagrange formalism.
The resulting perceptual mechanics is our derived RD in the brain. Ac cordingly, the
brain performs the RD in the state space spanned by the position µ and momentum
pµ variables of the constituting neural particles. (Sec. 3.2); (v) We a dopt the Hodgkin-
Huxley (H-H) neurons as biophysical neural correlates which form the basic perceptual
units in the brain. We ﬁrst derive the RD of sensory perception at a s ingle-neuron level
where the membrane potential, ionic transport, and gating are the relevant physical
attributes. Subsequently, we scale up the cellular formulation to fu rnish a functional
hierarchical-architecture of the brain. On this coarse-grained sc ale, the perceptual states
are the averaged properties of many interacting neurons. We simp lify the hierarchical
picture with two averaged, activation and connection variables, me diating the intra- and
inter-level dynamics, respectively. According to our formulation o f the hierarchical RD
in the brain, as sensory perturbation comes in at the lowest level, i.e., sensory interface,
the brain carries out the RD in its functional network and ﬁnds an op timal trajectory
which minimizes the IA.
To summarize, we have adopted the IFE as an informational Lagran gian of
the brain and subsequently employed the principle of least action to c onstruct the
Hamiltonian mechanics of cognition. In doing so, only positions and mom enta of the
neural particles have been addressed as dynamical variables; pos itions and momenta
are the metaphorical terms for the perceptual states and the b rain’s prediction errors,
respectively. We do not distinguish the causal and hidden states, b oth of which must
emerge as biophysical neuronal activities on diﬀerent timescales. T he resulting RD
is statistically deterministic, arising from unpredictable motions of th e environmental
states and noisy sensory mapping. Furthermore, the derived RD d escribes not only the
temporal development of the brain variables but also the prediction errors. The rate
of the prediction errors is not incorporated in the conventional fo rmulation of the FEP.
The successful solutions of the RD are stable equilibrium trajector ies in the neural state
space, specifying the tightest upper bound of the sensory uncer tainty, conforming to the
rephrased FEP. Our formulation allows solutions in an analytical form in linear regimes
near ﬁxed points, expanded in terms of the eigenvectors of the Ja cobian and, thus,
provides with tractability of real-time analysis. We hope that our the ory will motivate
further investigations of some model brains with numerical simulatio ns and also of the
active inference and learning problems.
This paper is organized as follows. We ﬁrst recapitulate the FEP in Sec . 2 to support
our motivation for casting the gradient descent scheme into the st andard mechanical
formulation. In the followed Sec. 3 we present the RD reformulated in the Lagrangian
and Hamiltonian formalisms. Then, in Sec. 4 biophysical implementation s of our theory
at the cellular level and in the scaled-up hierarchical brain are formu lated, where
Recognition Dynamics in the Brain under the Free Energy Prin ciple 6
nonlinear as well as linear dynamical analyses are carried out. Finally, a discussion
is provided in Sec. 5.
2. The free energy principle
To unveil our motivation for this paper, we shall compactly digest he re the IFE and
the FEP, that are currently exercised in the brain sciences. The RD is an organism’s
organization of executing minimization of the IFE in the brain under th e FEP. In
practice, there are various ways of IFE-minimizing schemes, for ins tance, variational
message passing and belief propagation, which do not lend themselve s to treatment
regarding generalized coordinates of motion. Our treatment in this paper is more
relevant to Bayesian ﬁltering and predictive coding schemes that ha ve become a popular
metaphor for message passing in the brain. Filtering is the problem of determining the
state of a system from noisy measurements [11]. For a detailed tech nical appraisal of
the FEP, we refer to [8] from which we borrow the mathematical not ations.
2.1. The informational free energy
A living organism occupies a ﬁnite space and time in the unbounded, cha nging world
while interacting with the rest of the world, comprising its environmen t. The states of
the environment are denoted as ϑ collectively, which are ‘hidden’ from the organism’s
perspective. The signals from the environment are registered biop hysically at the
organism’s sensory interface as sensory data ϕ.
The organism’s brain faces uncertainty when it tries to predict the s ensory inputs,
the amount of which is quantiﬁed as the sensory uncertainty H. The sensory uncertainty
is deﬁned to be an average of the self-information, ´ ln ppϕq, over the probability density
ppϕq encoded at the interface,
H ”
ż
dϕt´ ln ppϕqu ppϕq. (1)
The self-information, which is also termed as the sensory ‘surprise’ or ‘surprisal’
in information theory, quantiﬁes the survival tendency of living org anisms in the
unpredictable environment. Assuming that the sensory density de scribes an ergodic
ensemble of sensory streaming }, one may convert the sensory uncertainty into a time-
average as
ż
dϕt´ ln ppϕqu ppϕq “ 1
T
żT
0
dtt´ ln ppϕptqqu ,
where T is the temporal window over which an environmental event takes pla ces, i.e., a
temporal horizon. Here, one may manipulate the right-hand side (R HS) of the preceding
} This ergodicity assumption is an essential ingredient of the FEP, whic h hypothesizes that the ensemble
average of the surprisal is equal to the time-average of that, re garding the surprisal as a statistical,
dynamical quantity.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 7
equation by adding a Kullback-Leibler divergence to the integrand to get
´ ln ppϕq `
ż
dϑqpϑq ln qpϑq
ppϑ|ϕq Ñ
ż
dϑqpϑq ln qpϑq
ppϑ,ϕ q .
The outcome brings about the mathematical deﬁnition of the IFE,
Frqpϑq,p pϑ,ϕ qs ”
ż
dϑqpϑq ln qpϑq
ppϑ,ϕ q , (2)
which is expressed as a functional of the two probability densities, qpϑq and ppϑ,ϕ q;
where qpϑq and ppϑ,ϕ q are termed the recognition density (R-density) and the generativ e
density (G-density), respectively. The R-density is the organism’s probabilistic
representation of the external world, which the organism’s brain u ses in approximately
inferring the causes ϑ of inputs ϕ. The G-density, a joint probability between ϑ and ϕ,
underlies the top-down model about how the sensory data are biop hysically generated
by interaction between the brain and the environment. By constru ction, the surprisal is
smaller than the IFE by the added positive-amount, accordingly the sensory uncertainty
is bounded from above in accordance withż
dtr´ ln ppϕqs ď
ż
dtFrqpϑq,p pϑ,ϕ qs. (3)
Note that the sensory uncertainty on the left-hand side (LHS) of Eq. (3) speciﬁes the
accumulated surprisal over a temporal horizon involved in an enviro nmental event.
Equation (3) constitutes a mathematical statement of the FEP: “ All viable
organisms try to avoid being placed in an atypical situation in their env ironmental
habitats for existence by minimizing the sensory uncertainty. Howe ver, organisms do not
possess direct control over the sensory distribution ppϕq; accordingly they, instead, are
to minimize the upper bound of Eq. (3),
ş
dtF as a proxy for the sensory uncertainty.”
The brain conducts the minimization probabilistically by updating the R- density to
approximate the posterior density ppϑ|ϕq, namely carrying out Bayesian inference of
the causes ϑ of the sensory data ϕ. In the conventional application of the FEP the
following approximate inequality is usually exercised [25, 26],
´ ln ppϕq ď F. (4)
However, note that the inequality, Eq. (4) is not equivalent to Eq. ( 3), in general. It is
only a point approximation piecewise in time.
Here, a diﬃculty arises because the functional shape of the R-den sity is not given
a priori . It may be ﬁxed after knowing all orders of its moments of the exte rnal states,
which is not possible. To circumvent the diﬃculty, usually one invokes v ariational Bayes
by assuming a Gaussian ﬁxed-form for the R-density, the ‘Laplace a pproximation’,
qpϑq “ 1?2πζ exp
␣
´pϑ´ µq2{p2ζq
(
” N pϑ; µ,ζ q, (5)
which is fully characterized simply by its means µ and variances ζ, namely ﬁrst and
second order suﬃcient statistics , respectively. Then, by substituting Eq. (5) into Eq. (2)
Recognition Dynamics in the Brain under the Free Energy Prin ciple 8
and after some technical approximations, see [8] for the details, o ne can convert the IFE
functional F into
Frqpϑq,p pϑ,ϕ qs Ñ ´ ln ppµ,ϕ q ” Fpµ,ϕ q. (6)
At the end of manipulation the outcome becomes a function of only the means µ, given
the sensory input ϕ; where the dependence of variances has been removed by their
optimal values. The resulting IFE function F in Eq. (6) is termed the ‘Laplace-encoded’
IFE in which the parameters µ, specifying the organism’s belief or expectation of the
environmental states, are the organism’s probabilistic represent ation of the external
world. In turn, it is argued that the variational parameters µ are encoded in the brain
as biophysical variables.
To proceed with minimization of the IFE in the ﬁltering scheme, a model for noisy-
data measurement and also the equations of motion of the states m ust be supplied. The
FEP assumes a formal homology between the external dynamics an d the organism’s
top-down belief: The former describes, according to physics laws, the equations of
motion of environmental states and the sensory-data registerin g process. And, the latter
prescribes the internal dynamics of the representations of the e nvironmental states and
the generative model of the sensory data in the organism’s brain [13 ]. Following this
idea, we hypothesize that the registered data ϕ are predicted by the organism according
to a linear or nonlinear morphism,
ϕ “ gpµq ` z, (7)
where gpµq is a map from µ onto ϕ and z is the involved random ﬂuctuation. Also, the
brain’s representations µ of the causes are assumed to obey the stochastic equation of
motion,
dµ
dt “ fpµq ` w (8)
where fpµq is a linear or nonlinear function of the organism’s expectation of
environmental dynamics and w is the associated random ﬂuctuation.
Assuming mutually uncorrelated Gaussian ﬂuctuations, w and z, of the organism’s
beliefs, one may furnish the models for the likelihood ppϕ|µq and the empirical prior
ppµq, which jointly enter the Laplace-encoded IFE in Eq. (6) in the facto rized form,
ppϕ,µ q “ ppϕ|µqppµq. (9)
Using the notation introduced in Eq. (5), they are given explicitly as
ppϕ|µq “ N pϕ ´ gpµq; 0,σ zq, (10)
ppµq “ N p 9µ´ fpµq; 0,σ wq, (11)
where we have set 9µ “ dµ{dt, and the normal densities are assumed to possess zero
means with variances σz and σw, respectively. When the ﬂuctuations are statistically
stationary, the variances are handled as constant.; however nonstationarity can also be
taken into account by assuming an explicit time-dependence in the va riances. Finally,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 9
by substituting Eqs. (10) and (11) into Eq. (6), one can convert t he Laplace-encoded
IFE , up to a constant, into
Fpµ,ϕ q “ 1
2σ´ 1
z ε2
z ` 1
2σ´ 1
w ε2
w ` 1
2 ln pσzσwq , (12)
where the new variables have been deﬁned as
εz ” ϕ ´ gpµq and εw ” 9µ´ fpµq.
The auxiliary variable εz speciﬁes the discrepancy between the sensory data ϕ and the
brain’s prediction gpµq. Similarly, εw speciﬁes the discrepancy between the change of
the environmental representations 9µ and the organism’s belief fpµq.
It is straightforward to extend the formulation to the multiple, cor related noisy
inputs. However, for simplicity, we shall continually work in the single- variable picture
and will promote it to the general situation later.
2.2. Gradient descent scheme of the RD
With the Laplace-encoded IFE as an instrumental tool, the organis m’s brain
searches for the tightest bound for the surprisal, conforming to Eq. (4), by varying
its internal states µ. The critical question is what machinery the brain hires for the
minimization procedure. Typically the gradient descent method in mac hine learning
theory is employed in the conventional approach.
To give an idea of the gradient-descent scheme, here we set up a sim ple gradient-
descent equation, in the usual manner, by regarding the IFE func tion F as an objective
function as
9µ “ ´ κ∇µ F. (13)
In the above 9µ implies a temporal or sequential update of the brain variable µ and ∇µ is
the gradient operator with respect to µ, and κis the learning rate that controls the speed
of optimization. In steady state, deﬁned by 9µ ” 0, the solution µp0q to the relaxation
equation, Eq. (13) must satisfy ∇µ F “ 0. Subsequently, it may be interpreted that
such a solution corresponds to an equilibrium (or ﬁxed) point of the I FE function F,
specifying a local minimum in the IFE landscape.
By inspection, however, we ﬁnd that the gradient-descent const ruct in the preceding
way bears an ambiguity when it is applied to dynamic causal models such as Eq. (8).
This is because imposing the condition, 9µ ” 0 on the LHS of Eq. (13), does not guarantee
a desired equilibrium point in the state space spanned by µ. The reason is that 9µ also
appears on the RHS of Eq. (13) via F: The gradient operation on the RHS of Eq. (13)
can be performed explicitly for given F, Eq. (12) to give
ˆµ¨ ∇µ F “ ´ σ´ 1
z pϕ ´ gq Bg
Bµ ´ σ´ 1
w p 9µ´ fq Bf
Bµ.
This subtlety does not appear in the conventional theory which inco rporates the
nonstationarity, i.e., the aspect of continually changing external states, into form ulation
using the mathematical construct of unbounded, higher-order m otion of the generalized
Recognition Dynamics in the Brain under the Free Energy Prin ciple 10
coordinates.¶ It is an attempt to allow a more precise speciﬁcation of a system’s
dynamical state. The generalized coordinates are deﬁned to be a r ow vector in the
state space spanned by all orders of time-derivatives of bare sta te µ,
˜µ “ p µ,µ 1,µ 2, ¨ ¨ ¨q ” p µr0s ,µ r1s ,µ r2s, ¨ ¨ ¨q (14)
where vector components are deﬁned, with understanding µr0s ” µ, as
µrn` 1s “ µ1
rns ” Dµrns.
Note that the notation Dµrns ” µ1
rns has been introduced to denote the dynamical update
of the component µrns, which is in contrast to the notation 9µrns for the sequential update.
Also, two components of a vector at diﬀerent dynamical orders in t he generalized
coordinates are mutually independent variables. Similarly, the senso ry-data ˜ϕ are
expressed in the generalized coordinates as a row vector,
˜ϕ “ p ϕ,ϕ 1,ϕ 2, ¨ ¨ ¨q ” p ϕr0s ,ϕ r1s ,ϕ r2s, ¨ ¨ ¨q . (15)
Each component in the vectors, ˜ µ and ˜ϕ, is to be considered as a dynamically-
independent variable. Also, assuming that the random ﬂuctuations, z and w, are
analytic, they have been written in the generalized coordinates as ˜ z and ˜w, respectively.
Then, the generalization of Eqs. (7) and (8) follows after some tec hnical approximations
as (for details, see [8])
˜ϕ “ ˜g` ˜z, (16)
D˜µ “ ˜f ` ˜w; (17)
where D˜µ “ p µ1,µ 2,µ 3, ¨ ¨ ¨q . For reference, we explicitly speel out n, Eqs. (16) and (17)
at dynamical orders as
ϕrns “ Bg
Bµµrns ` zrns ,
Dµrns “ Bf
Bµµrns ` wrns .
Note that diﬀerent dynamical orders of the noises ˜ z and ˜w may be considered
to be statistically correlated in general. Then, the Laplace-encode d IFE can be
mathematically constructed from multivariate correlated Gaussian noises with zero
means and covariance matrices Σ w and Σ z,
Fp ˜µ, ˜ϕq “ 1
2t 9˜µ´ ˜fuΣ ´ 1
w t 9˜µ´ ˜fuT ` 1
2 ln |Σ w|
` 1
2t ˜ϕ ´ ˜guΣ ´ 1
z t ˜ϕ ´ ˜guT ` 1
2 ln |Σ z|, (18)
¶ The terminology of the generalized coordinates in generalized ﬁlterin g is dissimilar from its common
usage in physics. In classical mechanics, the generalized coordinat es refer the independent coordinate
variables which are required to completely specify the conﬁguration of a system with a holonomic
constraint, not including their temporal derivatives. The number o f generalized coordinates determines
the degree of freedom in the system [20]. Accordingly, the term, ‘ge neralized states’ seems better suit
generalized ﬁltering.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 11
where t 9˜µ´ ˜fuT is the transpose of row vector t 9˜µ´ ˜fu.; |Σ w| and Σ ´ 1
w are the determinant
and the inverse of the covariance matrix Σ w, respectively, etc. In many practical
exercises, however, usually, the conditional independence among diﬀerent dynamical
orders is imposed. Consequently, the noise distribution at each dyn amical order is
assumed to be an uncorrelated Gaussian density about zero means . This simpliﬁcation
corresponds to the Wiener process or Markovian approximation [1 1]. Here, we recall
that the generalized states ˜ µ are the means of the brain’s probabilistic model of the
dynamical world, the R-density Eq. (5) after rewritten in the gene ralized coordinates.
Note Eq. (18) is a direct generalization of Eq. (12).
Furnished with the extra theoretical constructs, the IFE becom es a function of
the generalized coordinates ˜ µ, given sensory data ˜ ϕ, F “ Fp ˜µ, ˜ϕq. Accordingly, the
gradient-descent scheme must be extended to incorporate the g eneralized motions in its
formulation. This is done by the theoretical prescription that the d ynamical update D˜µ
is distinctive from the sequential update 9˜µ. Consequently, one recasts Eq. (13) into the
form,
9˜µ´ D˜µ “ ´ κ∇˜µ Fp ˜µ, ˜ϕq. (19)
With the revised gradient-descent equation, the conventional FE P claims that the IFE is
minimized by reaching a desired ﬁxed point ˜ µ˚ ” ˜µptÑ 8q in the generalized state space
spanned by ˜µ. This corresponds to the situation when the two rates of the gene ralized
brain variables, 9˜µ and D˜µ become coincident, namely 9µrns “ Dµrns at every dynamic
order n. To support the idea it is argued that the purpose of Eq. (19) is to p lace the
gradient descent in the frame of reference that moves with the me an ˜µ , see [15]. The
entire minimization procedure is compactly expressed in the literatur e as
˜µ˚ “ arg min
˜µ
Fp ˜µ, ˜ϕ|mq,
where we have inserted min F to indicate explicitly that the minimization is conditioned
on the generative model of an organism.
In brief, the brain performs the RD of perceptual inference by bio physically
implementing Eq. (19) in the gray matter. The steady-state solutio n ˜µ˚ speciﬁes
minimum value of the IFE, say Fmin “ Fp ˜µ˚ , ˜ϕq, giving the tightest bound of the
surprisal [see Eq. (4)] associated with a given sensory experience ˜ϕ. Despite its frequent
employment in practicing the FEP, we have disclosed some subtleties in volved in the
conventional formulation of the gradient descent scheme, which h as motivated our
reformulation.
3. The informational action principle
The RD condensed in Sec. 2.2 is based on the mathematical statemen t Eq. (4) of
the FEP, which is a point approximation of Eq. (3). Here we reformula te the RD by
complying with the full mathematical statement of FEP given in Eq. (3 ). Accordingly,
we need a formalism that allows minimization of the time-integral of the IFE, not at each
Recognition Dynamics in the Brain under the Free Energy Prin ciple 12
point in time. We have come to assimilate that the theoretical ‘action’ in the principle of
least action neatly serves the goal [20]. This formalism allows us to esc hew introduction
of the generalized coordinates of a dynamical state comprising an in ﬁnite number of
time-derivatives of the brain state µ. Consequently, not required is the distinctive
classiﬁcation of time-derivative of the parametric update ( 9µ) and the dynamical update
(Dµ) of the state variable. In what follows, we shall consistently use th e dot symbol to
denote time-derivative of a dynamical variable.
3.1. Lagrangian formalism
To formulate the RD from the principle of least action, the ‘Lagrangia n’ of the
system must be supplied. We deﬁne the informational Lagrangian (I L) of the brain,
denoted by L, as the Laplace-encoded IFE function,
Lpµ, 9µ; ϕq ” Fpµ, 9µ; ϕq,
where we have placed the semicolon in L to indicate that µ and 9µ are the two brain’s
dynamical variables, given sensory input ϕ. The sensory inputs are stochastic and
time-dependent, in general; ϕ “ ϕptq, reﬂecting the changing external states, of which
generative processes are to be supplied by physics laws. The propo sed IL is not a
physical quantity but an information-theoretic object. When we t ake Eq. (12) as an
explicit expression for F, the IL is written up as
Lpµ, 9µ; ϕq “ 1
2σ´ 1
w p 9µ´ fpµqq2 ` 1
2σ´ 1
z pϕ ´ gpµqq2. (20)
Note that we have dropped out the term, 1
2 ln pσzσwq in writing Eq. (20) by assuming
it as a constant, which then does not aﬀect the dynamics of µ and 9µ. This assumption
of statistical nonstationarity may be lifted by introducing time-dep endence in the
variances,
σw “ σwptq and σz “ σzptq.
Still, however, the dropped-out term does not aﬀect the dynamics because a term that
can be expressed as a total time-derivative in the Lagrangian will no t do anything [20].
Next, we postulate that the perceptual dynamics of the neural p articles conforms
to the principle of least action [20]. Accordingly, we suppose that the brain’s perceptual
operation corresponds to searching for an optimal dynamical pat h that minimizes the
informational action (IA), denoted by S,
S ”
żtf
ti
dt Lpµ, 9µ; ϕq; (21)
where tf ´ ti ” T is the temporal horizon over which a living organism encounters an
environmental event. When functional derivative of S is carried out with respect to µ
and 9µ, it gives
δS “
„BL
Bµδµ
tf
ti
´
żtf
ti
dt
ˆ d
dt
BL
B 9µ ´ BL
Bµ
˙
δµ.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 13
By imposing δS” 0 under the condition that initial and ﬁnal states are ﬁxed,
δµptiq “ 0 “ δµptf q,
we derive the Lagrangian equation as
d
dt
BL
B 9µ ´ BL
Bµ “ 0. (22)
Using the speciﬁed Lagrangian, Eq. (20), in Eq. (22), we obtain a Ne wtonian equation
of motion for the brain variable µ,
σ´ 1
w 9v“ ¯Λ 1 ` ¯Λ 2, (23)
where we have deﬁned the kinematic velocity to be
v” 9µ
and the additional notations on the RHS as
¯Λ 1 ” σ´ 1
w fBf
Bµ and ¯Λ 2 ” ´ σ´ 1
z pϕ ´ gq Bg
Bµ. (24)
Equation (23) entails the RD of the brain in the Lagrangian formulatio n. We interpret
that the inverse of the variance σ´ 1
w plays, as a metaphor, a role of inertial mass of
the neural particles. Accordingly, the LHS of Eq. (23) represent s an inertial force , i.e.
the product of ‘inertial mass’ and ‘acceleration’, :µ. Note that the inverse of variance
is interpreted as precision in the Friston formulation [8], which gives a measure for the
accuracy of the brain’s expectation or prediction of sensory data . So, the precision is the
‘informational mass’ of the neural particle metaphorically. Also, th e terms ¯Λ i, i“ 1, 2,
on the RHS are interpreted as the ‘forces’ that drive the internal ¯Λ 1 as well as sensory
¯Λ 2 excitations in the brain. The acceleration can be evaluated from :µ “ ř¯Λ i{σ´ 1
w when
the net force is known.
While the organism’s brain integrates the RD for incoming sensory inpu t, an
optimal trajectory µ˚ ptq is continuously achieved in the neural-conﬁguration space. And,
the steady-state condition in the long-time limit tÑ 8 is given by
9µ˚ “ v˚ “ const, (25)
where the net force vanishes. Note that equation (25) deﬁning an attractor, µeq “ µ˚ p8q,
is more general than the simple guess, 9µ˚ “ 0. The optimal trajectory µ˚ ptq minimizes
the IA, which, in turn, provides the organism with the tightest estim ate of the sensory
uncertainty, see equation (3).
3.2. Hamiltonian formalism
The mechanical formulation can be made more modish in terms of Hamilt onian
language which admits position and momentum as independent brain va riables, instead
of position and velocity in the Lagrangian formulation. The positions a nd the momenta
span phase space of a physical system, which deﬁnes the neural state space of the
organism’s brain.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 14
The ‘canonical’ momentum p, which is conjugate to the position µ, is deﬁned via
Lagrangian L as [20]
p” BL
B 9µ “ σ´ 1
w p 9µ´ fq , (26)
which evidently diﬀers from the ‘kinematic’ momentum σ´ 1
w v “ σ´ 1
w 9µ. Then,
the informational ‘Hamiltonian’ H may be constructed from the Lagrangian using
Legendre’s transformation [20],
Hpµ,p ; ϕq “
ÿ BL
B 9µ 9µ´ Lpµ, 9µ; ϕq. (27)
The ﬁrst term on the RHS of Eq. (27) can be further manipulated to give
ÿ BL
B 9µ 9µ “ σ´ 1
w 9µ2 ´ σ´ 1
w 9µf.
By plugging the outcome and also the Lagrangian L given in Eq. (20) into Eq. (27), we
obtain the Hamiltonian as a function of µ and p as desired, given ϕ,
Hpµ,p ; ϕq “ T ppq ` Vpµ,p ; ϕq, (28)
where Eq. (26) has been used to replace 9µ with p. The ﬁrst term on the RHS of Eq. (28)
is the ‘kinetic energy’ which depends only on momentum,
T ppq “ p2
2σ´ 1
w
. (29)
Also, the second term on the RHS of Eq. (28) is the ‘potential energ y’ which depends
on both position and momentum,
Vpµ,p ; ϕq “ Vpµ; ϕq ` pfpµq, (30)
where we have deﬁned the momentum-independent term separate ly as V,
Vpµ; ϕq “ ´ 1
2σ´ 1
z pϕ ´ gq2. (31)
We remark that the sensory stimuli ϕ enter the Hamiltonian only through the potential-
energy part V which becomes ‘conservative’ when ϕ is static. Here, we shall assume
that the variances associated with the noisy data are constant. F or time-varying
sensory inputs, in general, the Hamiltonian is nonautonomous. In Fig. 1 we depict
the conservative potential energy, using three-term approxima tions for the generative
function,
gpµq « b1 ` b2µ` b2µ2.
For convenience, we have assumed a ﬁxed sensory input, ϕ “ 15, and set parameters as
pb1,b 2,b 3q “ p 0, 1, 0.01q. We have observed numerically that the static sensory signal ϕ
changes the distance between two unstable ﬁxed points, but do no t aﬀect the location
of the stable equilibrium point. Also, the depth of the stable equilibrium valley gets
deeper as the magnitude of ϕ increases.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 15
-100 -50 50 µ
-30
-25
-20
-15
-10
-5
V(µ; )
Figure 1. The potential energy, given in Eq. (31), in arbitrary units; where t he dashed
and solid curves are for the variance σz “ 100 and 30, respectively. Both cases exhibit
a stable minimum in the central well and two unstable maxima on the side hills, which
contribute to determining the FE landscape.
Next, we take the total derivative of the Hamiltonian given in Eq. (27 ) with respect
to µ and 9µ to get
dHpµ,p ; ϕq “
ÿ
dpp9µq ´ dLpµ, 9µ; ϕq
“ 9µdp` pd9µ´
ˆBL
Bµdµ` BL
B 9µd9µ
˙
“ ´ 9pµ dµ` 9µdp.
By comparing the last expression with the formal expansion,
dH “ BH
Bµdµ` BH
Bpdp,
we identify the Hamilton equations of motion for independent variable s µ and p of a
neural particle,
9µ “ BH
Bp (32)
9p “ ´ BH
Bµ. (33)
For given H in Eq. (28), we spell out the RHS of Eq. (32) to get
9µ “ 1
σ´ 1
w
p` f (34)
which is identical to Eq. (26). Similarly, the second equation, Eq. (33 ) is spelled out
9p“ ´ BV
Bµ ´ Bf
Bµp. (35)
The ﬁrst term on the RHS of Eq. (35) speciﬁes the conservative fo rce,
´ BV
Bµ Ñ ´ σ´ 1
z pϕ ´ gq Bg
Bµ.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 16
Whereas, the second term on the RHS of Eq. (35) speciﬁes the diss ipative force, where
Bf{Bµ plays the role of damping coeﬃcient.
The derived set of coupled equations for the variables µ and pfurnish the RD of the
brain in phase space spanned by µ and p, which involve only ﬁrst order time-derivatives.
When time-derivative is taken once more for both sides of Eq. (34) w ith followed
substitution of Eq. (35) for 9p, the outcome becomes identical to the Lagrangian equation
of motion, Eq. (23). This observation conﬁrms that two mechanica l formulations, one
from the Lagrangian and the other from the Hamiltonian, are in fact equivalent.
In the Hamiltonian formulation, the brain’s fulﬁlling of the RD is equivalen t to
ﬁnding an optimal trajectory pµ˚ ptq,p ˚ ptqq in phase space. For a static sensory input,
the dynamics governed by Eqs. (34) and (35) is autonomous, and f or the time-dependent
sensory input it becomes non-autonomous. The RD can be integrat ed by providing
appropriate models for the generative functions f and g. The attractor pµ˚ p8q,p ˚ p8qq
would be a focus or center in phase space, which can be calculated by simultaneously
imposing the conditions on LHSs of Eqs. (34) and (35) ,
9µ˚ “ 0 and 9p˚ “ 0. (36)
One can readily conﬁrm that these ﬁxed-point conditions match with the Newtonian
equilibrium condition, ř
i
¯Λ i “ 0 in the Lagrangian formulation, see section 3.1. The
situation corresponds to the brain’s resting state at a local minimum on the energy
landscape deﬁned by the Hamiltonian function.
3.3. Multivariate formulation
Having established the Hamiltonian dynamics for a single brain variable µ, we now
extend our formulation to the general case of the multivariate bra in. We denote tµu as
a row vector of N brain states as done in Sec. 2.1,
tµu “ p µ1,µ 2, ¨ ¨ ¨ ,µ N q,
that respond to the multiple of sensory inputs in a general way,
tϕu “ p ϕ1,ϕ 2, ¨ ¨ ¨ ,ϕ N q.
For simplicity, we neglect the statistical correlation of the ﬂuctuat ions associated with
environmental variables and also with sensory inputs. Then, within t he independent-
particle approximation of uncorrelated brain variables, the Laplace -encoded IFE
Eq. (18) furnishes the multivariate Lagrangian,
Lptµu, t 9µu; tϕuq “ 1
2
Nÿ
α “ 1
“
σ´ 1
wα p 9µα ´ fα ptµuqq2 ` σ´ 1
zα pϕα ´ gα ptµuqq2‰
,(37)
where we have dropped out the terms which contain only the varianc es, σzα and σwα ,
assuming that the noises are statistically nonstationary. One may e xtend Eq. (37) to
interacting neural nodes in terms of covariance matrix formulation [8], which is not
Recognition Dynamics in the Brain under the Free Energy Prin ciple 17
our concern here, either. Subsequently, the conjugate moment um to the generalized
coordinate µα is determined by an explicit evaluation of
pα “ BL
B 9µα
“ σ´ 1
wα p 9µα ´ fα q . (38)
Note the momentum pα gives a measure of the discrepancy, weighted by the inverse
variance σwα , between the change of the probabilistic representation of the en vironment
9µα and the organism’s belief of it fα . The weighting factor σ´ 1
wα is called the precision
in the FEP. In turn, the Hamiltonian of the multivariate brain can be co nstructed from
Eq. (28) as
Hptµu, tpu; tϕuq “ T ptµu, tpu; tϕuq ` Vptµu, tpu; tϕuq (39)
where ﬁrst term on the RHS is the kinetic energy,
T ptpu; tϕuq ”
ÿ
α
pα 2
2σ´ 1
wα
(40)
and the potential energy V is identiﬁed as
Vptµu, tpu; tϕuq ”
ÿ
α
„
´ 1
2σ´ 1
zα pϕα ´ gα q2 ` pα fα

. (41)
Then, it is straightforward to derive the RD of the variables µα and pα , given sensory
data ϕα , as
9µα “ BH
Bpα
“ 1
σ´ 1
wα
pα ` fα , (42)
and for their conjugate momenta,
9pα “ ´ BH
Bµα
“ ´ Bgα
Bµα
pϕα ´ Bfα
Bµα
pα . (43)
In writing equation (43), for notational convenience we have intro duced an auxiliary
quantity pϕα ,
pϕα ” σ´ 1
zα pϕα ´ gα q.
Equations (42) and (43) are a coupled set of equations for the com putational units,
µα and pα , describing the brain states and their conjugate momenta, respe ctively, given
the sensory discrepancy pϕα between the observed data ϕα and their predictions gα pµα q.
With some working models for fα and gα , they shape the RD in the brain’s multi-
dimensional phase space in the Hamiltonian prescription.
In Fig. 2 we present a schematic illustration of the perceptual circu itry implied by
the RD at a neural node. The classiﬁcation of excitatory and inhibito ry activation of the
computational units is not absolute because the overall sign depen ds on the generative
function fα and map gα , which are not speciﬁed. It is admissible to assume that the brain
is, at the outset, in a resting state. As the sensory inputs ϕα come in, the organism’s
brain performs the RD online, by integrating Eqs. (42) and (43), to attain an optimal
trajectory in neural phase space,
µα “ µ˚
α ptq and pα “ p˚
α ptq,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 18
f(μ) ∂μ f(μ)
∂μ g(μ)
pφα
pαμα
g(μ)
φα
Figure 2. The perceptual circuitry at neural node α in which sensory data ϕ α stream;
where it is depicted that the computational units µα and pα are positively activated
by arrows and negatively by lines ended with ﬁlled dots. The conjugat e momenta pα ,
deﬁned in Eq. (38), to the brain variables µα mimic the precision-weighted prediction
errors in the language of predictive coding [27].
which minimize the IA, see Eq. (21). The entire minimization procedure may be stated
abstractly as
pµ˚
α ,p ˚
α q “ arg min
µ α ,p α
Spµα ,p α ; ϕ|mq, (44)
where S is the IA and m has been inserted to indicate explicitly that minimization is
conditioned on the organism as a model of the world.
Note that in our revised RD is involved not only the organism’s predictio n of the
environmental change via its representation µα but also the dynamics of its prediction-
error pα .
4. Biophysical implementation
We know that the anatomy and entire functions of an organism’s bra in develop from
single cells. In order to provide empirical Bayes in the FEP with a solid bio physical
basis, we must start with known biophysical substrates and then in troduce probabilities
to describe a neuron, neurons, and a network. Until now, howeve r, most work has
taken the reverse direction: Theory prescribes ﬁrst a conjectu ral model and then tries
to allocate possible neural correlates. At present, our knowledge remains limited on how
biophysical mechanisms of neurons implicate predictions and model a spects about the
environment, while a ‘neurocentric’ approach to the inference pro blem seems suggestive
to bridge the gap [28, 29].
Here, we regard coarse-grained Hodgkin-Huxley (H-H) neurons a s the generic, basic
building-blocks of encoding and transmitting a perceptual message in the brain. The
famous H-H model continues to be used to this day in computational neuroscience
studies of neuronal dynamics [30, 31]. In extracellular electrical re cordings, the local
ﬁeld potential and multi-unit activity result in as combined signals from a population
Recognition Dynamics in the Brain under the Free Energy Prin ciple 19
of neurons [32]. Such averaged neuronal variables must subserve the perceptual states
and conduct the cognitive computation in the brain. We shall call the m ‘small’ neural
particles and envisage that a small neural particle enacts a node that collectively forms
the whole neural network on a large scale. Before proceeding, we shall mention that there
are many biophysical eﬀorts to describe such averaged neuronal properties; for instance,
the neural mass models and neural ﬁeld theories are a few examples [33, 34, 35, 36, 37].
Also, we note the bottom-up eﬀort of trying to understand the lar ge-scale brain function
at the cortical microcircuit level based on the averaged, spikes an d synaptic inputs over
a coarse-grained time interval [38, 39].
4.1. Single cell description
We ﬁrst present how our formulation may be implemented at a single-c ell level by
hypothesizing that each neuron reﬂects the fundamentals of the perceptual computation
of the whole system. A typical neuron receives current informatio n about its
surroundings from the sensory periphery via glutamate, which exc ites or inhibits the
membrane potential V with regulating the gating variables γl and ionic concentrations
nl; where l is the ion channel index. We assume that pV, tnlu, tγluq specify the neural
states of a neuron as a neural observer in the neural conﬁgurat ional space [29]. We
encapsulate the neural states as components in a multi-dimensiona l row vector,
tµu “ p V, tnlu, tγluq “ p µ1,µ 2,µ 3, ¨ ¨ ¨q .
The H-H equation for excitation of the membrane voltage V in a spatially
homogeneous cell is given by
CdV
dt “
ÿ
l
γlGlpEl ´ Vq ` Iexptq (45)
where C is the membrane capacitance, Gl is the maximal conductance of ion channel l,
γl is the probability factor associated with opening or closing channel lwhich in general
a product of activation and inactivation gating variables, and Iex is the external driving
current. For simplicity, contributions from leakage current as well as synaptic input are
assumed to be included in the external currents. The reverse pot ential El of l-th ion
channel is given, allowing its time-dependence via nonequilibrium ion-co ncentrations, in
general, as
Elptq “ kBT
ql
ln nliptq
nloptq , (46)
where kB is the Boltzmann constant, T is the metabolic temperature of an organism,
ql is the ionic charge of channel l, and nliptq and nloptq are the instantaneous ion
concentrations inside and outside the membrane, respectively. In the steady state
without external current, Iex “ 0, V tends to the resting (Nernst) potential VptÑ 8q
with retaining ionic concentrations in electro-chemical equilibrium. Th e gating variable
Recognition Dynamics in the Brain under the Free Energy Prin ciple 20
γl of ion channels is assumed to obey the kinetics, a diﬀerent model of t hat may be
preferable,
dγl
dt “ ´ 1
τl
pγl ´ γleqq ` ηl, (47)
where the relaxation time τl and steady-state gating variable γleq depend on the
membrane potential, in general,
τl “ τlpVq and γleq “ γleqpVq,
and ηl is the noise involved in the process.
For ionic concentration dynamics, we suppose that ion concentrat ions tnlu
vary slowly compared to the membrane potential and gating-chann el kinetics, and
consequently treat them statically in our work. This restriction can be lifted when
a more detailed description is required for ion concentration dynamic s. Accordingly, the
reverse potentials El are also treated statically in below.
Then, the state equations for the multivariate neural vaiable tµu neatly map onto
the standard form suggested in the FEP,
dµα
dt “ fα pV, tγlu, tnluq ` wα ptq, (48)
where α runs 1 , 2, ¨ ¨ ¨ with implying µ1 “ V, µ2 “ γ1, and µ3 “ γ2, etc. The driving
functions fα , that are speciﬁed to be
fV pV, tγlu; tnluq “ 1
C
ÿ
l
γlGlpEl ´ Vq ` 1
CIex, (49)
fγl pV, tγlu; tnluq “ ´ 1
τl
pγl ´ γleqq. (50)
The terms wα in Eq. (48) describe the noisy synaptic and/or leakage current wV ﬂowing
into the neural cell, not the deterministic contribution Iex which is included in fV ,
and the noise wγl “ ηl associated with the activation and inactivation of ion channels,
respectively. For both noises, we assume the Gaussian distribution s N p 9µα ´ fα ; 0,σ wα q
with variances σwα about zero means.
Regarding neuronal response to the sensory stimulus ϕα , we adopt the usual
generative map in the FEP [see Eq. (7)] as
ϕα “ gα pV, tγlu, tnluq ` zα , (51)
where gα is the generative map that is unknown but must be supplied for pract ical
application and zα characterizes the stochastic nature of the sensory reading whic h
we assume the normal distribution N pϕα ´ gα ; 0,σ zα q. With the present model, we
admit that the neural observer responds to the sensory data ins tantly by means of the
neuronal states. Currently, we do not possess a ﬁrm ground on b iophysical processes of
the sensory prediction.
As a working example, here we consider a H-H neuron which allows fast relaxation,
i.e. τl ! 1, of gating variables to their steady-states, γlptq Ñ γlp8q “ γleqpVq. In this
case our neural particle is fully characterized by a single dynamical v ariable of V. Note
Recognition Dynamics in the Brain under the Free Energy Prin ciple 21
the time-dependence of the gating variables occurs only implicitly thr ough the long-
time membrane voltages in Eq. (49). Then, the RD of our neural par ticle is fulﬁlled
in a two-dimensional state space spanned by tµu “ p V,p V q ” p µ,p q, prescribed by the
Hamiltonian function, equation (28),
Hpµ,p q “ p2
2σ´ 1w
´ 1
2σ´ 1
z pϕ ´ gq2 ` pf.
While the ‘dissipative’ function f is explicitly given in the H-H model as
fpµq “ 1
C
ÿ
l
γleqpµqGlpEl ´ µq ` Iex{C, (52)
the ‘conservative’ function g must be additionally supplied. Also, one needs to
make the voltage-dependence of γleq available in practice. Note the Hamiltonian is
nonautonomous, in general, because it explicitly depends on time through both the
sensory input ϕptq and the driving current Iex in f, and also through σwptq, σzptq when
the noisy data are statistically nonstationary.
Figure 3. Hamiltonian function Hpµ, p q in arbitrary units for the chosen set of
parameters given in the main text; where the black curve on the ene rgy landscape
is the trajectory which is calculated by solving the Hamilton equations of motion for
an initial condition at pµ, p q “ p 2. 5, ´ 5. 0q. [For interpretation of the references to color
in this ﬁgure, the reader is referred to the web version of this artic le.]
In Fig. 3 we present the energy landscape described by the Hamilton ian function,
assuming static sensory data, constant driving currents, and st atistical stationarity.
Since our knowledge is limited to the functional form of gpµq and fpµq, we have taken
the algebraic approximations [40],
gpµq « a0 ` a1µ` a2µ2,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 22
fpµq « b0 ` b1µ` b2µ2 ` b3µ3.
For numerical purposes, we have speciﬁed pa0,a 1,a 2q “ p 0, 1, 1q and pb0,b 1,b 2,b 3q “
p0, 0.1, 1, 1q, and also assumed a ﬁxed sensory input with equal masses (precisio ns ) on
the brain’s internal model and belief of sensory prediction as
ϕ “ 1.0 and σ´ 1
w “ 0.1 “ σ´ 1
z .
The Hamilton equations of motion, Eqs. (42) and (43), bring about t he nonlinear
RD as
9µ “ Λ 1pµ,p ; tq, (53)
9p“ Λ 2pµ,p ; tq, (54)
where the ‘force’ functions Λ 1 and Λ 2 are speciﬁed as
Λ 1 “ fpµq ` 1
σ´ 1w
p, (55)
Λ 2 “ ´ σ´ 1
z pϕ ´ gq Bg
Bµ ´ Bf
Bµp. (56)
We have chosen an initial state and solved the equations of motion, f or the same
parameters used in Fig. 3, to obtain a trajectory in phase space. T he outcome is drawn
on the energy surface in Fig. 3. According to the present model, th e neural observer
performs the RD, given the sensory input ϕ and, consequently, obtains the optimal
trajectory pµ˚ ,p ˚ q conforming to Eq. (44). In the long-time limit the brain will reach a
ﬁxed (equilibrium) point pµ˚
eq,p ˚
eqq in the state space, that is speciﬁed by intersections
of two isoclines,
Λ ipµ,p ; 8q “ 0, i “ 1, 2.
We have determined the ﬁxed points numerically. It turns out that t here exist three
real solutions for the speciﬁed system parameters, p´1.23, 0.05q, p´0.50, ´0.01q, and
p0.07, ´0.04q, which are depicted as the blue dots in Fig. 4. By further analysis, we
have found that only the middle point is a stable equilibrium solution and t he other
two specify saddle points. Figure 4 shows a ﬂow of trajectories, ob tained from arbitrary
initial points on the red-colored circle of radius µ2 ` p2 “ 1.6, in phase space.
To gain an insight into how the system approaches to a steady state , we inspect
the optimal trajectories near an equilibrium point,
µ˚ « µ˚
eq ` δµ˚ and p˚ « p˚
eq ` δp˚ .
We expand Eqs. (53) and (54) to the linear order in the deviations δµ˚ and δp˚ and,
after rearrangement, obtain the normal form,
d
dt
˜
δµ˚
δp˚
¸
`
˜
R11 R12
R21 R22
¸˜
δµ˚
δp˚
¸
“ 0. (57)
Recognition Dynamics in the Brain under the Free Energy Prin ciple 23
-3 -1.5 1.5 3 µ
-2
-1
1
2
p
Figure 4. Optimal trajectories in phase space which are obtained by integrat ing the
RD, equations (53) and (53), from the initial conditions arbitrarily c hosen on the red-
colored circle; where the blue dots are the equilibrium points among wh ich only the
middle dot at p´ 0. 50, ´ 0. 01q is a stable ﬁxed point, and other two points are saddle
points. The stable ﬁxed point turns out to be a center, which we hav e conﬁrmed by
linear stability analysis and also numerically. [For interpretation of the references to
color in this ﬁgure, the reader is referred to the web version of this article.]
In Eq. (57) the elements of the relaxation (Jacobian) matrix R are speciﬁed to be
R11 “ ´
„Bf
Bµ

eq
, R12 “ ´ 1
σ´ 1w
R21 “ σ´ 1
z
«
´
ˆBg
Bµ
˙2
` p ϕ ´ gq B2g
Bµ2 ´ B2f
Bµ2 p
ﬀ
eq
,
R22 “
„Bf
Bµ

eq
;
where the partial derivatives are to be evaluated at the equilibrium p oints. Here, for
notational convention we denote the column vector as
δψ ”
˜
δµ˚
δp˚
¸
.
Then, the formal solution to Eq. (57) is written as
δψptq “ e´ Rtδψp0q.
One may expand the initial state ψp0q in terms of the eigenvectors of R as
δψp0q “
ÿ
cα φα ,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 24
where the eigenvalues λα and eigenvectors φα are determined by the secular equation,
Rφα “ λα φα .
Consequently, the solutions to the linear RD at a single node level is co mpleted as
δψptq “
2ÿ
α “ 1
cα e´ λα tφα , (58)
where the expansion coeﬃcients cα are ﬁxed by the initial condition.
In the linear regime, a geometrical interpretation of the equilibrium s olutions is
possible by inspecting the eigenvalues of the Jacobian matrix R. Considering that the
matrix R is not symmetric, we anticipate that the eigenvalues are not real. Fu rthermore,
because the trace of the relaxation matrix equals zero, the sum of the two eigenvalues
must be zero. Thus, when the determinant of R is positive, the two eigenvalues λ1 and
λ2 would be pure imaginary with opposite sign. Consequently, in the pres ent particular
model, the resulting equilibrium point is likely to be a center. We have conﬁrmed
numerically that the eigenvalues of the Jacobian corresponding to t he stable equilibrium
point in ﬁgure 4 meet the condition for a center.
4.2. The hierarchical neural network
Here, we suppose that there are a ﬁnite number of levels in the perceptual hierarchy
of the whole system and that for simplicity each level is characterize d eﬃciently as a
single neural node. Further, we assume that the neural node at h ierarchical level i is
described by the coarse-grained, activation and connection varia bles, denoted as Vpiq
and Spiq , respectively. The activation variable describes action potential a t a node, and
the connection variable describes inter-level synaptic input and ou tput variables. Both
variables are derived from a population of neurons and thus vary on a coarse-grained
space and time scale. The technical details of how one may derive suc h a coarse-graining
description are not our scope, for a reference see [37]. They form the coordinates in
brain’s conﬁgurational space,
µpiq “ p Vpiq ,S piq q,
where the superscript runs i“ 1, 2, ¨ ¨ ¨ ,M , with M denoting the highest level.
We assume that the activation variables Vpiq obey the eﬀective dynamics with noise
wpiq within each hierarchical level i,
dVpiq
dt “ fpiq pVpiq ,S piq q ` wpiq , (59)
which is a direct generalization of Eq. (48) with incorporating the hier archical
dependence via Spiq . For inter-level dynamics, we propose that the connection variab les
are updated by one-level higher connetion as well as activation var iables, subjected to
the stochastic equations,
dSpiq
dt “ gpi` 1q pVpi` 1q ,S pi` 1qq ` zpiq , (60)
Recognition Dynamics in the Brain under the Free Energy Prin ciple 25
where zpiq represents the noise associated with the process. The brain’s top -down
prediction functions fpiq and gpiq must be supplied in practical implementation. Note
there is only spontaneous ﬂuctuation at the top cortical level, i“ M, accordingly
gpM` 1q “ 0. (61)
Also, we constrain that the sensory data ϕ enter the interface (or boundary between)
of the brain and the environment speciﬁed as the lowest hierarchicl level, i “ 1.
Subsequently we assume that the brain’s prediction of the sensory inputs is performed
by way of an instantaneous mapping,
Sp0q “ gp1q pVp1q ,S p1q q ` zp0q , (62)
where, for notational convenience, we have set
Sp0q ” ϕptq.
We remark that the hierarchical equations Eq. (60) we propose is d issimilar to the
conventional formulation which assumes the static model in the ent ire hierarchy like the
one Eq. (62) at the sensory interface, see [8]. We treat here the c onnection variables
dynamically not statically to treat lateral and hierarchical dynamics symmetrically.
The rates of the activation and connection variables may be subjec ted to diﬀerent time-
scales, that can be incorporated, for instance, by introducing dis tinctive relaxation-times
in their generative functions. It turns out that our equations suit the formalism of the
Hamilton action principle neatly.
Having speciﬁed our hierarchical model, we write the informational L agrangian for
the constructed neural network by generalizing Eq. (37) with a sin gle sensory input for
now, as
LpV, 9V; S, 9S; ϕq “ 1
2
Mÿ
i“ 1
mpiq
w
`
εpiq
w
˘2
` 1
2
Mÿ
i“ 0
mpiq
z
`
εpiq
z
˘2
, (63)
where mpiq
w and mpiq
z are the inertia masses, associated with the Gaussian noises, wpiq
and zpiq , respectively, deﬁned to be
mpiq
w ” 1{σpiq
w and mpiq
z ” 1{σpiq
z . (64)
The auxiliary variables in the Lagrangian are deﬁned to be ( iě 1)
εpiq
w ” 9Vpiq ´ fpiq `
Vpiq ,S piq ˘
, (65)
εpiq
z ” 9Spiq ´ gpi` 1q `
Vpi` 1q ,S pi` 1q ˘
. (66)
We interpret that εpiq
w speciﬁes the discrepancy between the change in the present
lateral state and the brain’s on-level prediction, which may be cons idered as the lateral
prediction-error. On the other hand, εpiq
z measures the prediction error between the
change in the present hierarchical state and its prediction from on e higher level via the
generative map g, which may be viewed as the hierarchical prediction-error. Note εp0q
z
in the second term on the RHS of Eq. (63) is deﬁned separately as
εp0q
z ” Sp0q ´ gp1q `
Vp1q ,S p1q ˘
,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 26
which speciﬁes an error estimation in sensory prediction at the lowes t hierarchical level.
The generalized momenta, conjugate to Vpiq and Spiq , are readily calculated for
iě 1, respectively, as
ppiq
V ” BL
B 9Vpiq
“ mpiq
w εpiq
w , (67)
ppiq
S ” BL
B 9Spiq
“ mpiq
z εpiq
z , (68)
Note that the inverse variances mpiq
w , mpiq
z have been termed the informational masses,
see discussion below equation (24). The role of the inertial masses is to modulate the
discrepancy between the change of the perceptual states and t heir prediction. Thus, in
our theory, momentum ppiq
V is a measure of lateral prediction-error modulated by inertial
mass mpiq
w , and momentum ppiq
S is a measure of hierarchical prediction-error modulated
by inertial mass mpiq
z . And, the heavier the mass is, the bigger the precision becomes.
Given the Lagrangian Eq. (63), we can formulate the informational Hamiltonian
by performing a Legendre transformation,
H “
ÿ
i
´
9Vpiq ppiq
V ` 9Spiqppiq
S
¯
´ L.
After some manipulation, we obtain the outcome as
HpV,p V ; S,p S; ϕq “
Mÿ
i“ 1
`
T piq ` Vpiq ˘
, (69)
where the infromational kinetic energy is deﬁned to be ( iě 1)
T piq ppV ,p Sq “ 1
2mpiq
w
´
ppiq
V
¯2
` 1
2mpiq
z
´
ppiq
S
¯2
, (70)
and the potential energy to be ( iě 2)
Vpiq pV,p V ; S,p S; ϕq ” ppiq
V fpiq ` ppiq
S gpi` 1q . (71)
Note the potential energy at the lowest level is speciﬁed separate ly to be
Vp1q “ pp1q
V fp1q ´ 1
2mp0q
z
´
pp0q
S
¯2
; (72)
where, for notational convention, we have written the weighted p rediction-error
associated with the sensory measurement as
pp0q
S ” mp0q
z εp0q
z “ mp0q
z pϕ ´ gp1qq,
which, unlike ppiq
S for iě 1, is not a canonical momentum. Consequently, the multi-level
Hamiltonian Eq. (69) has been prescribed via the perceptual state s in the hierarchical
chain, i“ 1, 2, ¨ ¨ ¨ ,M , denoted as a four dimensional column vector ψpiq at each level,
ψpiq “ p Vpiq ,p piq
V ,S piq ,p piq
S qT ” p ψpiq
1 ,ψ piq
2 ,ψ piq
3 ,ψ piq
4 qT,
where T means the transverse operation.
Next, it is straightforward to generate the Hamiltonian equations o f motion for the
brain’s perceptual states ψpiq . The results are the coupled diﬀerential equations for the
Recognition Dynamics in the Brain under the Free Energy Prin ciple 27
four computational components at each level ( iě 1), which are, in turn, hierarchically-
connected among adjacent levels:
9Vpiq “ BH
Bppiq
V
“ 1
mpiq
w
ppiq
V ` fpiq , (73)
9Spiq “ BH
Bppiq
S
“ 1
mpiq
z
ppiq
S ` gpi` 1q, (74)
9ppiq
V “ ´ BH
BVpiq “ ´ Bfpiq
BVpiq ppiq
V ´ Bgpiq
BVpiq ppi´ 1q
S ; (75)
9ppiq
S “ ´ BH
BSpiq “ ´ Bfpiq
BSpiq ppiq
V ´ Bgpiq
BSpiq ppi´ 1q
S . (76)
According to the derived RD, the sensory inputs ϕ enter the brain-environment
interface at the level j “ 1, which are instantly predicted by the organism’s lowest-
level generative model gp1qpVp1q ,S p1q q. Subsequently, the resulting prediction-error pp0q
S
acts as a source to update the prediction-errors, pp1q
V and pp1q
S . The change of on-
level perceptual states Vp1q and Sp1q are predicted by the generative models fp1q and
gp2q with additional modulations from the perceptual momenta pp1q
V and pp1q
S , being the
lateral and hierarchical prediction-errors, respectively. At high er levels iě 2, the intra-
level dynamics of the activation state Vpiq is updated, through Eq. (73), by the on-
level generative function fpiq and prediction error ppiq
V , while the change of the current
hierarchical state Spiq is determined, through Eq. (74), by the inter-level prediction gpi` 1q
and the on-level prediction error ppiq
S . The organism’s top-down message ﬂow is mediated
by the connection state Spiq via Eq. (74) as pSpi` 1q ,V pi` 1q q Ñ Spiq . And, Eqs. (75), and
(76) govern the coupled, bottom-up propagation of the predictio n errors, mediated by
ppiq
S , ppiq
S Ñ p ppi` 1q
S ,p pi` 1q
V q. In Fig. 5 we draw a diagram that schematically illustrates
the perceptual architecture of the hierarchical network, at low est two levels, implied by
Eqs. (73)-(76).
Here, we emphasize that the dynamics of precision-weighted predic tion errors,
encapsulated in canonical momenta in which mass takes over the role of precision, are
taken into account in our Hamiltonian formulation on an equal footing with the dynamics
of prediction of the state variables. This aspect is also in contrast t o the conventional
minimization algorithm which entails diﬀerential equations only for the u pdate of the
brain states without carrying parallel ones for the prediction erro rs. Consequently, the
message passing in our model shows diﬀerent features in the details compared with the
neural circuitry from the conventional RD [41]. However, the gene ral message ﬂow, in
terms of the computational units, of feedforward, feedback, a nd lateral connections hold
the same in the hierarchical brain network. We recognize an attemp t to incorporate the
brain’s computation of prediction errors in the FEP can be found in a r ecent tutorial
model [42].
Here, for mathematical compactness, we rewrite the ﬁltering algo rithm, Eqs. (73)-
(76), as
dψpiq
α
dt “ Λ piq
α ptψpiq
α uq, (77)
Recognition Dynamics in the Brain under the Free Energy Prin ciple 28
S(1) V(1) pS
(1) pV
(1)
S(2) V(2) pS
(2) pV
(2)
φ pS
(0)
Figure 5. A schematic of the neural circuitry which conducts the RD Eqs. (73 )-
(76) in the hierarchical network of the brain; where the computat ional units
pSpiq , V piq , p piq
S , p piq
V q, i “ 1, 2, ¨ ¨ ¨ , M , are connected by arrows for excitatory (positive)
inputs and by lines ended with ﬁlled dots for inhibitory (negative) input s. Note that
the prediction error pp0q
S of incoming sensory data ϕ , at the lowest level, induces an
inhibitory change in the perceptual momenta ppp1q
S , p p1q
V q. Subsequently, the prediction
error propagates up in the hierarchy, pp1q
S Ñ p pp2q
S , p p2q
V q, etc. On the other hand, the
top-down message passing is mediated by means of the connection s tates Spiq . For
instance, the connection state Sp1q is top-down predicted by both units pSp2q , V p2q q
from one higher-level.
where the hierarchical index i runs from 1 to M, α runs from 1 to 4, and the force
function Λ piq
α is the corresponding RHS to each vector component ψpiq
α at cortical level
i. The obtained hierarchical equations are the highlight of our theor y, prescribing the
RD of the brain’s sensory inference under the FEP framework.
To apply our formulation to an empirical brain, one needs to supply th e generating
function of lateral dynamics fpiq and the hierarchical connecting function gpiq, that
enter the force functions Λ piq
α in the perceptual mechanics, Eq. (77). For the generating
function we once again use the H-H model Eq. (49) to write
fpiq pVpiq ,S piq q “
ÿ
l
γleq ˜Gl
`
El ´ Vpiq ˘
` ˜GSSpiq `
ES ´ Vpiq ˘
, (78)
where ˜Gl are the channel conductances normalized by the capacitance C. And, the
second term on the RHS accounts for other deterministic driving so urces such as leakage
and/or lateral synaptic currents with ˜GS being the normalized synaptic conductance.
The hierarchical connection function, for which we have limited bioph ysical knowledge,
shall be taken in a simple form here as
gpiqpVpiq ,S piqq “ ΓpVpiq qSpiq , (79)
Recognition Dynamics in the Brain under the Free Energy Prin ciple 29
where the function Γ specify the voltage-dependent synaptic plas ticity from hierarchical
level ito level i´ 1. In addition, as in the single-node case, one must supply approxima te
models for voltage-dependence of the gating variables γleq and the connection strength
Γ. For instance, one may take the quadratic approximations [40],
γleqpVpiq q « bl0 ` bl1Vpiq ` bl2Vpiq Vpiq ,
ΓpVpiq q « a0 ` a1Vpiq ` a2Vpiq Vpiq .
Having laid down the lateral and hierarchical generative models, the organism’s
brain can now perform the RD given a streaming of noisy inputs. While c onducting the
ﬁltering, an optimal trajectory is obtained in multi-dimensional phas e space,
ψ˚p iq
α “ ψ˚p iq
α ptq,
which, in the end, tends to a ﬁxed point, ψpiq
α,eq “ ψ˚p iq
α pt Ñ 8q . The necessary
equilibrium condition to Eq. (77) is speciﬁed by
Λ piq
α ptψpiq
α uq “ 0. (80)
Although the full time-dependent solutions must be invoked numeric ally, one may
inspect the perceptual trajectories near a ﬁxed point by linear an alysis. To this end, we
consider a small deviation of αth component of the perceptual state vector ψ˚p iq
α at the
cortical level i, δψpiq
α , from the ﬁxed point ψpiq
α,eq ,
ψ˚p iq
α « ψpiq
α 0 ` δψpiq
α .
Then, we expand Eq. (77) about the ﬁxed point to linear order in the small deviation,
and after some manipulation we get the hierarchical equations for δψpiq
α ,
dδψpiq
α
dt `
4ÿ
β “ 1
Rpiq
αβ δψpiq
β “
4ÿ
β “ 1
Mÿ
j‰ i
Φ pijq
αβ δψpjq
β ; (81)
where the αβ component of the 4 ˆ 4 Jacobian matrix at cortical level i is speciﬁed by
Rpiq
αβ “
«
BΛ piq
α
Bψpiq
β
ﬀ
eq
,
and the inter-level connection between level i and level j in the hierarchical pathway is
speciﬁed by
Φ pijq
αβ “
«
BΛ piq
α
Bψpjq
β
ﬀ
eq
;
where the subscript eq indicates that the matrix elements are to be evaluated at the
equilibrium points. To cast the inhomogeneous term into a more sugge stive form we
further inspect it in detail within the models speciﬁed: We observe ﬁr st that the matrix
elements Φ pijq
αβ do not vanish only for α “ 3 because only the force function Λ piq
3 possesses
ψpjq
β for j ‰ ias variables via gpi` 1q [see Eq. (74)]. Second, because gpi` 1q depends solely
on the hierarchical level-index i` 1, only matrix elements with the hierarchical index
j “ i` 1 survives. Combining these two observations, the source term on the RHS of
Recognition Dynamics in the Brain under the Free Energy Prin ciple 30
Eq. (81) is converted into a vector at level i` 1 with only a single nonvanishing α “ 3
component,
4ÿ
β “ 1
Mÿ
j‰ i
Φ pijq
αβ δψpjq
β ” δζpi` 1q
α
which to be complete we spell out explicitly as
δζpi` 1q
α “ δα 3
$
&
%
«
Bgpi` 1q
Bψpi` 1q
1
ﬀ
eq
δψpi` 1q
1 `
«
Bgpi` 1q
Bψpi` 1q
3
ﬀ
eq
δψpi` 1q
3
,
.
-, (82)
where δα 3 is the Kronecker delta.
Finally, we shall present a formal solution to the linearized perceptu al mechanics
Eq. (81), that can be obtained by a direct integration with respect to time. The result
takes the form
δψpiq ptq “ e´ Rpiq tδψpiq p0q `
żt
0
dt1e´ Rpiq pt´ t1 q δζpi` 1qpt1 q. (83)
We next solve the eigenvalue problem at each hierarchical level, which is deﬁned to be
Rpiq φpiq
α “ λpiq
α φpiq
α , (84)
where λpiq
α and φpiq
α are the eigenvalues and corresponding eigenvectors at level i,
respectively. Then, we expand the initial state δψpiq p0q in terms of the complete
eigenvectors:
δψpiq p0q “
ÿ
apiq
α φpiq
α . (85)
Similarly, we may expand the inhomogeneous vector δζpi` 1q as
δζpi` 1qpt1 q “
ÿ
bpi` 1q
α pt1qφpiq
α , (86)
where note the expansion coeﬃcients bpi` 1q
α are time-dependent. By substituting the
expansions Eqs. (85) and (86) into Eq. (83) we obtain the desired f ormal solution near
equilibrium points,
δψpiq ptq “
4ÿ
α “ 1
aα e´ λpiq
α tφpiq
α `
4ÿ
α “ 1
φpiq
α
żt
0
dt1 e´ λpiq
α pt´ t1 qbpi` 1q
α pt1q. (87)
The geometrical approach to a ﬁxed point is again determined by the eigenvalues λpiq
α ;
however, the details are driven by the time-dependent generative sources bpi` 1q
α ptq from
one-level higher in the hierarchy.
To sum, responding to sensory streaming ϕ “ Sp0q , at the lowest hierarchical level
(i“ 1), the brain in an initial resting state performs the hierarchical RD by integrating
Eq. (77) to infer the external causes. The ensuing brain’s comput ation corresponds
to minimizing the IA, which is an upper bound of the sensory uncertain ty, whose
mathematical statement, equation (3), is repeated compactly as
Hrppϕqs ď SrF; ϕs,
Recognition Dynamics in the Brain under the Free Energy Prin ciple 31
where the sensory uncertainty H was deﬁned in Eq. (1) and the IA on the RSH
is expressed here in terms of the hierarchical states as SrF; ϕs “
ş
dtFptψpiq
α u; ϕq.
Conforming to the FEP, the minimum value of IA speciﬁes the tightest bound of the
sensory uncertainty over a relevant biological time-scale, which pr eserves the organism’s
current model of the environment.
5. Discussion
We have recast the FEP following the principles of mechanics, which ar ticulates
that all living organisms are evolutionally self-organized to tend to min imize the sensory
uncertainty about uninhabitable environmental encounters. The sensory uncertainty is
an average of the surprisal over the sensory density registered on the brain-environment
interface, the sensory surprisal being the self-information of th e sensory probability
density. The FEP suggests that the organisms implement the minimiza tion by calling
forth the IFE in the brain. The time-integral of the IFE gives an est imate of the
upper bound of the sensory uncertainty. We have enunciated tha t the minimization of
the IFE must continually take place over a ﬁnite temporal horizon of an organism’s
unfolding environmental event. Our scheme is a generalization of th e conventional
theory which approximates minimization of the IFE at each point in time when it
performs the gradient descent. The sensory uncertainty is an inf ormation-theoretical
Shannon entropy [43]; however, in this work, we have circumvented the term, ‘sensory
entropy’ to call the sensory uncertainty. The reason is that ‘minim ization of the
sensory entropy’ is reminiscent of Erwin Schr¨ odinger’s word, ‘neg ative entropy’ which
carries a disputable connotation in implying how the living organism avoid s decay. He
subsequently suggested FE instead as a more appropriate notion in the context [1].
Conforming to the second law of thermodynamics, the organism’s ad aptive ﬁtness of
minimizing the sensory uncertainty must contribute to increasing th e total entropy of
the brain and its environment.
We have adopted the Laplace-encoded IFE as an informational Lag rangian in
implementing the FEP under the principle of least action. And, by subs cribing to
the standard Newtonian dynamics, we have considered the IFE a fu nction of position
and velocity as the metaphors for the organism’s brain variable and t heir ﬁrst-order time
derivative, respectively, in the continuous-time picture. The brain variable maps onto
the ﬁrst-order suﬃcient statistics of the recognition density laun ched in the organism’s
brain to perform the RD, Bayesian ﬁltering of the noisy sensory dat a. In the following
Hamiltonian formulation, the RD prescribes momentum, conjugate t o a position, as a
mechanical measure of prediction error weighted by mass, the pre cision. The theoretical
construct of generalized coordinates introduced in the prevailing t heory to specify the
extended states of higher-orders of motion has been eschewed in our formulation. The
features of changing world enter our theory via the sensory input s in continuous time,
and the statistical nonstationarity of the noise is embedded in time- dependence of the
variances of the Gaussian ﬂuctuations in the organism’s belief of the changing state
Recognition Dynamics in the Brain under the Free Energy Prin ciple 32
and sensory generation. The temporal correlations of the dynam ical states may be
incorporated as time-dependent covariances, but not explored in this work. Also, in
our theory all the parameters in the RD are speciﬁed in the Hamiltonia n; thus, there
require no extra parameters like learning rates in the gradient desc ent scheme to control
the speed of convergence to a steady state. According to our fo rmulation, the brain’s
Helmholtzian perception corresponds to ﬁnding an optimal traject ory in the hierarchical
functional network by minimizing the IA. When the brain completes th e RD by reaching
a desired ﬁxed-point or a limit cycle, it remains resting, i.e., being spont aneous, until
another sensory stimulus will come in.
We have applied our formalism to a biophysically grounded model for hie rarchical
neural dynamics by suggesting that the large-scale architecture of the brain be an
emergent coarse-grained description of the interacting single neu rons. We have admitted
the asymmetric top-down rationale of sensory inference in our for malism, which is an
essential facet of the FEP: The sensory inputs at the interface, the lowest hierarchical
level, were assumed to be instantaneously mapped onto the organis m’s belief, encoded
in the brain variables, about environmental causes with associated noise. Diﬀerently,
however, from the instant model at the lowest level, we have gener alized that the
inter-level ﬁltering in the brain’s functional hierarchy obeys the st ochastic dynamics,
supplied with the organism’s dynamical generative model of environm ental states. The
resulting RD is deterministic and hierarchical, which notably incorpora tes dynamics
of both predictions and prediction errors of the perceptual stat es on an equal footing.
Consequently, the details of the ensuing neural circuitry from our formulation diﬀers
from the one supported by the gradient-descent scheme which ge nerates only dynamics of
the causal and hidden states, not their prediction errors. Howev er, the general structure
of message passing, namely descending predictions and ascending p rediction-errors in
the hierarchical network, shows the same. Also, our obtained RD t enably underpins the
causality: For a speciﬁed set of the perceptual positions and corr esponding momenta,
at the outset, responding to sensory inputs that may be slow or fa st time-dependent,
the RD can be integrated online. The arbitrariness involved in deciding the number of
generalized coordinates for a complete description and also the amb iguity in specifying
unknowable initial conditions can be averted.
In short, it is still a long way to understanding how the Bayesian FEP in
neurosciences may be made congruous with the biophysical reality o f the brain. It
is far from clear how the organism embodies the generative model of the environment
in the physical brain. Our theory only delivers a hybrid of the biologica lly plausible
information-theoretic framework of the FEP and the mechanical f ormulation of the
RD under the principle of least action. To borrow what Hopﬁeld puts in words, “it
lies somewhere between a model of neurobiology and a metaphor for how the brain
computes” [44]. We hope that our eﬀort will guide a step forward to u nraveling the
challenging problem.
Recognition Dynamics in the Brain under the Free Energy Prin ciple 33
References
[1] Schr¨ odinger, E. (1967). What is Life? Mind and Matter. Cambrid ge: Cambridge University Press.
[2] von Helmholtz, H. (1962). Trietise on physiological optics. Vol. II I. Mineola: Dover Publication,
Inc.
[3] Gregory, R. L. (1980). Perceptions as hypoheses. Philosophic al Transactions of the Royal Society
of London B. 290: 181-197.
[4] Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helm holtz machine. Neural
Computation. 7: 889–904.
[5] Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in Cognitive
Science. 13: 293–301.
[6] Friston, K. (2010). The free-energy principle: a uniﬁed brain th eory? Nature Reviews.
Neuroscience. 11: 127-138.
[7] Friston, K. (2013). Life as we know it. Journal of Royal Society Interface. 10: 20130475.
[8] Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for
action and perception: A mathematical review, Journal of Mathem atical Psychology. 81: 55-79.
http://dx.doi.org/10.1016/j.jmp.2017.09.004.
[9] Ramstead, M. J. D., Badcock, P. B., & Friston, K. J. (2017).
https://doi.org/10.1016/j.plrev.2017.09.001.
[10] Maturana, H., & Varela, F. (1980). Autopoiesis and Cognition: T he Realization of the Living.
Boston: D. Reidel.
[11] Jazwinski, A. H. (1970). Stochastic Process and Filtering Theo ry. New York: Academic Press.
[12] Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontan eous cortical activity reveals
hallmakrs of an optimal internal model of the environment. Science . 331: 83-87.
[13] Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Ac tion and behavior: a free energy
formulation. Biological Cybernetics. 102(3): 227-260.
[14] Friston, K. J., & Stephan, K. E. (2007). Free-energy and the brain. Synthese. 159: 417-458.
[15] Friston. K., Stephan, K., Li, B., & Daunnizeau, J. (2010). Gener alized Filtering. Mathematical
Problems in Engineering. 261670.
[16] Friston, K. J. (2008). Variational Filtering. NeuroImage. 41: 747-766.
[17] Friston, K. (2008). Hierarchical models in the brain. PLoS Comp utational Biology. 4(11): e1000211.
[18] Friston, K. (2006). A free energy principle for the brain. Jour nal of Physiology-Paris. 100: 70-87.
[19] Sprott, J. C. (1997). Some simple chaotic jerk funcitons. Ame rical Journal of Physics. 65: 537-543.
[20] Landau, L. P. (1998) Classical Mechanics. (2nd ed.). New York : Springer-Verlag.
[21] Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcem ent learning or active inference?.
PLoS One. 4(7): e6421.
[22] Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., H uissoud, C., Lamy, C.,
Misery, P., Giroud, P., Ullman, S., Barone, P., Dehay, C., Knoblauch, K., & Kennedy, H.
(2014). Anatomy of hierarchy: Feedforward and feedback path ways in Macaque visual cortx.
Journal of Comparative Neurology. 522:225-259.
[23] Michalareas, G., Vezoli, J., van Pelt, S., Schoﬀelen, J.-M. & Kenned y, H. (2016). Alpha-beta
and gamma rhythms subserve feedback and feedforward inﬂunec es among human visual cortical
areas. Neuron. 89:384-397.
[24] Markov, N. T., & Kennedy, H. (2013). Current Opinion in Neurob iology. 23:187-194.
[25] Friston, K. & Kiebel, S. (2009). Cortical circuits for perceptu al inference. Neural Networks. 22:
1093-1104.
[26] Friston, K., Adams, R. A., Perrinet, L., & Breakspear, M. (2012 ). Frontiers in Psychology. 3: 151.
[27] Rao, R. P. N. & Ballard, D. H. (1999). Predictive coding in the visu al cortex: a functional
interpretation of some extra-classical receptive-ﬁeld eﬀects. N atue neuroscience 2(1): 79-87.
[28] Fiorillo, C. D. (2008). Towards a general theory of neural com putation based on prediction by
single neurons. PLoS One. 3(10).
Recognition Dynamics in the Brain under the Free Energy Prin ciple 34
[29] Fiorillo, C. D., Kim, J. K., & Hong, S. Z. (2014). The meaning of spike s from the neuron’s
point of view: predivtive homeostatis generates the appearance o f randomness. Frontiers in
Computational Neuroscience. 8:49.
[30] Hodgkin, A., & Huxley, A. (1952). A quantitative description of m embrane current and its
application to conduction and excitation in nerve. Journla of Physiolo gy,. 117:500-544.
[31] Hille, B. (2001). Ion Channels of Excitable Membranes. (3rd ed.) . Sunderland: Sinauer Associates,
Inc.
[32] Einevoll, G. T., Kayser, C., Logothetis, N. K., & Panzeri, S. (2013 ). Nature Reviews. Neuroscience.
14:770.
[33] Jansen, B. H., Zouridakis, G., & Brandt, E. (1993). A neurophy siologically-based mathematical
model of ﬂash visual potentials. Biological Cybernetics. 68: 275-2 83.
[34] Jirsa, V. K., & Haken, H. (1996). Field theory of electromagnet ic brain activity. Physical Review
Letter. 77: 960-963.
[35] Robinson, P. A., Rennie, C. J., & Wright, J. J. (1997). Propagat ion and stability of waves of
electrical activity in the cerebral cortex. Physical Review E. 56: 8 26-840.
[36] David, O., Friston, K. J. (2003). A neural mass model for MEG/ EEG: coupling and neuronal
dynamics. Neuroimage. 20: 1743-1755.
[37] Deco, G., Jirsa, V. K., Robinson, P. A., Breakspear, M., & Friston , K. (2008). The dyamic brain:
From spiking neurons to neural masses and cortical ﬁelds. PLoS Co mpt. Biol. 4: e1000092.
[38] Potjans, T. C., & Diesmann, M. (2014). The Cell-type speciﬁc co rtical microcircuit: Relating
structure and activity in a full-scale spiking network model. Cerebra l Cortex. 24(3):785-806.
[39] Steyn-Ross, M. L., & Steyn-Ross, D. A. (2016). From individua l spiking neurons to population
behavior: Systematic elimination of short-wavelength spatial mode ls. Physical Review E. 93:
022402.
[40] Wilson, H. R. (1999). Simpliﬁed dynamics of human and mammaian ne ocortical neurons. J. Theor.
Biol. 200: 375-388.
[41] Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries P.,& Friston, K. J. (2012).
Canonical microcircuits for predictive coding. Neuron 76: 695-711 .
[42] Bogacz, R. (2017). A tutorial on the free-energy framewor k for modelling perception and learning.
Journal of mathematical psychology, 76 (B): 198–211.
[43] Shannon, C. E. (1948). A mathematical theory of communicat ion. Bell System Technical Journal.
27: 379-423, 623-656.
[44] Hopﬁeld, J. J. (1999). Brain, neural network, and computat ion. Review of modeern physics. 71:
S431-S437.