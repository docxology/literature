5102
rpA
7
]TS.htam[
2v0750.8021:viXra
Bernoulli 21(1), 2015, 83–143
DOI: 10.3150/13-BEJ562
Lasso and probabilistic inequalities for
multivariate point processes
NIELS RICHARD HANSEN1,
PATRICIA REYNAUD-BOURET2 and VINCENT RIVOIRARD3
1Department of Mathematical Sciences, University of Copenhagen, Universitetsparken 5, 2100
Copenhagen, Denmark. E-mail: Niels.R.Hansen@math.ku.dk
2Univ. Nice Sophia Antipolis, CNRS, LJAD, UMR 7351, 06100 Nice, France.
E-mail: Patricia.Reynaud-Bouret@unice.fr
3CEREMADE, CNRS-UMR 7534, Universit´e Paris Dauphine, Place Mar´echal de Lattre de
Tassigny, 75775 Paris Cedex 16, France. INRIA Paris-Rocquencourt, projet Classic.
E-mail: Vincent.Rivoirard@dauphine.fr
Due to its low computational cost, Lasso is an attractive regularization method for high-
dimensional statistical settings. In this paper, we consider multivariate counting processes de-
pending on an unknown function parameter to be estimated by linear combinations of a fixed
dictionary. To select coefficients, we propose an adaptive ℓ -penalization methodology, where
1
data-drivenweightsofthepenaltyarederivedfrom newBernstein typeinequalities formartin-
gales. Oracle inequalities are established under assumptions on the Gram matrix of the dictio-
nary. Nonasymptotic probabilistic results for multivariate Hawkes processes are proven, which
allows us to check these assumptions by considering general dictionaries based on histograms,
Fourierorwavelet bases. Motivatedbyproblemsofneuronalactivityinference,wefinally carry
out a simulation study for multivariate Hawkes processes and compare our methodology with
the adaptive Lasso procedure proposed by Zou in (J. Amer. Statist. Assoc. 101 (2006) 1418–
1429). We observe an excellent behavior of our procedure. We rely on theoretical aspects for
the essential question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.
Assoc. 101 (2006) 1418–1429), our tuning procedure is proven to be robust with respect to all
the parameters of the problem, revealing its potential for concrete purposes, in particular in
neuroscience.
Keywords: adaptive estimation; Bernstein-typeinequalities; Hawkes processes; Lasso
procedure; multivariate countingprocess
1. Introduction
The Lasso, proposed in [58], is a well-established method that achieves sparsity of an
estimated parameter vector via ℓ -penalization. In this paper, we focus on using Lasso
1
This is an electronic reprint of theoriginal article published by theISI/BS in Bernoulli,
2015, Vol. 21, No. 1, 83–143. This reprint differs from theoriginal in pagination and
typographic detail.
1350-7265 (cid:13)c 2015ISI/BS
2 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
to select and estimate coefficients in the basis expansionof intensity processesfor multi-
variate point processes.
Recentexamplesofapplicationsofmultivariatepointprocessesincludethemodelingof
multivariate neuronspike data [42, 47],stochastic kinetic modeling [7] andthe modeling
of the distribution of ChIP-seq data along the genome [20]. In the previous examples,
the intensity of a future occurrence of a point depends on the history of all or some of
the coordinates of the point processes, and it is of particular interest to estimate this
dependence. This can be achieved using a parametric family of models, as in several of
the papers above. Our aim is to provide a nonparametric method based on the Lasso.
The statistical properties of Lasso are particularly well understood in the context of
regressionwith i.i.d. errors or for density estimation where a range of oracle inequalities
have been established. These inequalities, now widespreadin the literature, provide the-
oreticalerrorbounds that hold on events with a controllable(large) probability.See, for
instance, [5, 6, 15–18, 61]. We refer the reader to [13] for an excellent account on many
state-of-the-artresults.Onemainchallengeinthiscontextistoobtainasweakconditions
aspossibleonthedesign–orGram–matrix.Theotherimportantchallengeistobeable
to provide an ℓ -penalization procedure that provides excellent performance from both
1
theoretical and practical points of view. Standard Lasso proposed in [58] and based on
deterministicconstantweightsconstitutesamajorcontributionfromthemethodological
pointofview,butunderestimationduetoitsshrinkagenaturemayleadtopoorpractical
performance in some contexts. Alternative two step procedures have been suggested to
overcome this drawback (see [43, 60, 64]). Zou in [64] also discusses problems for stan-
dardLassotocopewithvariableselectionandconsistencysimultaneously.Heovercomes
these problems by introducing nonconstant data-drivenℓ -weights based on preliminary
1
consistent estimates.
1.1. Our contributions
In this paper, we consider an ℓ -penalized least squares criterion for the estimation of
1
coefficients in the expansion of a function parameter. As in [5, 35, 60, 64], we consider
nonconstant data-driven weights. However the setup is here that of multivariate point
processes and the function parameter that lives in a Hilbert space determines the point
processintensities. Evenin this unusualcontext, the leastsquarescriterionalso involves
a random Gram matrix as well, and in this respect, we present a fairly standard oracle
inequality with a strong condition on this Gram matrix (see Theorem 1 in Section 2).
Onemajorcontributionofthis articleis to provideprobabilisticresults thatenable us
to calibrate ℓ -weights in the most general setup (see Theorem 2 in Section 2). This is
1
naturally linked to sharp Bernstein type inequalities for martingales. In the literature,
thosekindsofinequalitiesgenerallyprovideupperboundsforthemartingalethatarede-
terministicandunobservable[57,59].Tochoosedata-drivenweights,weneedobservable
bounds.Morerecently,therehavebeensomeattemptstouseself-normalizedprocessesin
order to provide more flexible and random upper bounds [4, 25–27]. Nevertheless, those
bounds are usually not (completely) observable when dealing with counting processes.
Lasso and probabilistic inequalities for multivariate point processes 3
We prove a result that goes further in this direction by providing a completely sharp
random observable upper bound for the martingale in our counting process framework
(see Theorem 3 in Section 3).
Thesecondmaincontributionistoprovideaquitetheoreticalandabstractframework
to deal with processes whose intensity is (or is well approximated by) a linear trans-
formation of deterministic parameters to infer. This general framework also allows for
different asymptotics in terms of the number of observed processes or in terms of the
durationofthe recordingofobservations,accordingto the setup. We focus in this paper
on three main examples: the Poisson model, the Aalen multiplicative intensity model
and the multivariate Hawkesprocess,but many other situations can be expressedin the
present framework, which has the advantage of full flexibility. The first two examples
have been extensively studied in the literature as we detail hereafter, but Hawkes pro-
cesses are typical of situations where very little is known from a nonparametric point
of view, and where fully implementable adaptive methods do not exist until the present
work, to the best of our knowledge. They also constitute processes that are often used
in practice – in particular in neuroscience – as explained below.
It is also notable that we, in each of these three previous examples, can verify explic-
itly if the strong condition on the Gram matrix mentioned previously is fulfilled with
probability close to 1 (see Section 4 for the Poisson and Aalen cases and Section 5 for
the Hawkes case). For the multivariate Hawkes process, this involves novel probabilis-
tic inequalities. Even though the Hawkes processes have been studied extensively in the
literature, see [9, 24], very little is known about exponential inequalities and nonasymp-
totic tail control. Besides the univariate case [51], no exponential inequality controlling
the number ofpoints per intervalis knownto us.We derivesuchresultsandother sharp
controlsontheconvergenceintheergodictheoremtoobtaincontrolontheGrammatrix.
Finally, we carry out a simulation study in Section 6 for the most intricate process,
namely the multivariate Hawkes process,with a main aim: to convince practitioners,for
instance in neuroscience, that this method is indeed fully implementable and gives good
results in practice.Data-drivenweights for practicalpurposes are slightmodifications of
theoretical ones. These modifications essentially aim at reducing the number of tuning
parameters to one. Due to nonnegligible shrinkage that is unavoidable, in particular for
largecoefficients,weproposeatwostepprocedurewhereestimationofcoefficientsishan-
dled by using ordinary least squares estimation on the support preliminary determined
by our Lasso methodology. Tuning issues are extensively investigated in our simulation
study,andTable1inSection6.3showsthatourmethodologycaneasilyandrobustlybe
tuned by using limit values imposed by assumptions of Theorem 2. We naturally com-
pare ourprocedure with adaptive Lasso of [64] for whichweights are proportionalto the
inverse of ordinary least squares estimates. The latter is very competitive for estimation
aspects since shrinkage becomes negligible if the preliminary OLS estimates are large.
But adaptive Lasso does not incorporate random fluctuations of coefficient estimators.
So it is most of the time outperformed by our procedure. In particular, tuning adaptive
LassointheHawkessettingisadifficulttask,whichcannotbetackledbyusingstandard
cross-validation. Our simulation study shows that the performance of adaptive Lasso is
verysensitivetothechoiceofthetuningparameter.Robustnesswithrespecttotuningis
4 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
anotheradvantageofourmethodoveradaptiveLasso.Forsimulations,theframeworkof
neuronalnetworksisused.Ourshortstudyprovesthatourmethodologycanbe usedfor
solvingconcreteproblemsinneurosciencesuchastheinferenceoffunctionalconnectivity
graphs.
1.2. Multivariate counting process
The framework introduced here and used throughout the paper aims at unifying sev-
eral situations, making the reading easier. Main examples are then shortly described,
illustrating the use of this setup.
We consider an M-dimensional counting process (N(m)) , which can also be
t m=1,...,M
seen as a random point measure on R with marks in 1,...,M , and corresponding
{ }
predictable intensity processes (λ(m)) under a probability measure P (see [8] or
t m=1,...,M
[24] for precise definitions).
Classical models assume that the intensity λ(m) can be written as a linear predictable
t
transformation of a deterministic function parameter f belonging to a Hilbert space
0
H
(the structure of , and then of f , will differ according to the context, as illustrated
0
H
below). We denote this linear transformationby
ψ(f)=(ψ(1)(f),...,ψ(M)(f)). (1.1)
Therefore, for classical models, for any t,
λ(m)=ψ(m)(f ). (1.2)
t t 0
The main goal in classical settings is to estimate f based on observing (N(m))
0 t m=1,...,M
for t [0,T]. Actually, we do not require in Theorems 1 and 2 that (1.2) holds. Our
∈
aim is mainly to furnish an estimate of the best linear approximation ψ(m)(f ) of the
t 0
underlying intensity λ(m).
t
Letus illustratethe generalsetupwith threemainexamples:First,the casewithi.i.d.
observations of an inhomogeneous Poisson process on [0,1] and unknown intensity, sec-
ond,the wellknownAalenmultiplicative intensity model andthird, the centralexample
ofthe multivariateHawkesprocess.Forthefirsttwomodels,asymptoticsiswithrespect
to M (T is fixed). For the third one, M is fixed and asymptotics is with respect to T.
1.2.1. The Poisson model
Letus startwith averysimple example whichwill be somehowatoy problemherecom-
paredtothe otherexamples.Inthisexample,wetake T =1andassumethatweobserve
M i.i.d. Poisson processes on [0,1] with common intensity f :[0,1] R . Asymptotic
0 +
7−→
properties are obtained when M tends to infinity. In this case, the intensity λ(m) of the
mthprocessN(m) is f ,whichdoesnotdependonm:Therefore,foranym 1,...,M
0
∈{ }
and any t, we set
ψ(m)(f ):=f (t),
t 0 0
Lasso and probabilistic inequalities for multivariate point processes 5
and =L ([0,1]) is equipped with the classical norm defined by
2
H
1 1/2
f = f2(t)dt .
k k
(cid:18)Z0 (cid:19)
Thisframeworkhasalreadybeenextensivelystudiedfromanadaptivepointofview:see
forinstance[48,63]formodelselectionmethods,[50]forwaveletthresholdingestimation
or [53] for kernel estimates. In this context, our present general result matches with
existing minimax adaptation results where asymptotics is with respect to M.
1.2.2. The Aalen multiplicative intensity model
This is one of the most popular counting processes because of its adaptivity to various
situations(fromMarkovmodelstocensoredlifetime models)anditsvariousapplications
to biomedical data (see [2]). Given a Hilbert space,we consider f :[0,T] R ,
0 +
X ×X 7−→
and we set for any t R,
∈
λ(m)=ψ(m)(f ):=f (t,X(m))Y(m),
t t 0 0 t
where Y(m) is an observable predictable process and X(m) represents covariates. In this
case, =L ([0,T] ). Our goal is to estimate f and not to select covariates. So, to
2 0
H ×X
fix ideas one sets =[0,1] and T =1. Hence can be identified with L ([0,1]2). For
2
X H
right-censored data, f usually represents the hazard rate. The presence of covariates
0
in this pure nonparametric model is the classical generalization of the semi-parametric
model proposed by Cox (see [39], for instance). Note that the Poissonmodel is a special
case of the Aalen model.
The classical framework consists in assuming that (X(m),Y(m),N(m)) is an
m=1,...,M
i.i.d. M-sample and as for the Poisson model, it is natural to investigate asymptotic
propertieswhenM + .Iftherearenocovariates,severaladaptiveapproachesalready
→ ∞
exist: See [11, 12, 49] for various penalized least-squares contrasts and [21] for kernel
methods in special cases of censoring. In the presence of covariates, one can mention
[1, 2] for a parametric approach, [23, 39] for a model selection approach and [29] for a
Lasso approach. Let us also cite [14] where covariate selection via penalized MLE has
beenstudied.Onceagain,ourpresentgeneralresultmatcheswithexistingoracleresults.
In[21],exponentialcontrolofrandomfluctuationsleadingtoadaptiveresultsarederived
withoutusingthemartingaletheory.Inmoregeneralframeworks(asin[23],forinstance),
martingales are required and this even when i.i.d. processes are involved.
1.2.3. Hawkes processes
Hawkesprocessesarethepointprocessesequivalenttoautoregressivemodels.Inseismol-
ogy, Hawkes processes can model earthquakes and their aftershocks [62]. More recently
theyhavebeenusedtomodelfavoredoravoideddistancesbetweenoccurrencesofmotifs
[32] or Transcription Regulatory Elements [20] on the DNA. We can also mention the
6 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
use of Hawkes processes as models of social interactions [44] or financial phenomena [3].
In the univariate setting, with M =1, the intensity of a nonlinear Hawkes process
(N ) is given by
t t>0
t
−
λ =φ h(t u)dN ,
t u
−
(cid:18)Z−∞ (cid:19)
where φ:R R and h:R R (see [9]). A particular case is Hawkes’s self exciting
+ +
7→ 7→
point process, for which h is nonnegative and φ(x)=ν+x where ν>0 (see [9, 24, 34]).
Forinstance,forseismologicalpurposes,ν representsthespontaneousoccurrencesofreal
originalearthquakes.The function h models self-interaction:after a shock at time u, we
observe an aftershock at time t with large probability if h(t u) is large.
−
These notions can be easily extended to the multivariate setting and in this case the
intensity of N(m) takes the form:
M t
λ(m)=φ(m) − h(m)(t u)dN(ℓ)(u) .
t ℓ − !
X
ℓ=1Z−∞
Theorem 7 of [9] gives conditions on the functions φ(m) (namely Lipschitz properties)
and on the functions h(m) to obtain existence and uniqueness of a stationary version of
ℓ
the associated process. Throughout this paper, we assume that for any m 1,...,M ,
∈{ }
φ(m)(x)=(ν(m)+x) ,
+
where ν(m) > 0 and () denotes the positive part. Note that in [20, 22], the case
+
·
φ(m)(x)=exp(ν(m)+x)wasstudied.However,Lipschitzpropertiesrequiredin[9]arenot
satisfiedinthiscase.Byintroducing,aspreviously,thelinearpredictabletransformation
ψ(f)=(ψ(1)(f),...,ψ(M)(f)) with for any m and any t
M t
ψ(m)(f ):=ν(m)+ − h(m)(t u)dN(ℓ)(u), (1.3)
t 0 ℓ −
X
ℓ=1Z−∞
with f =(ν(m),(h(m)) ) , we have λ(m) =(ψ(m)(f )) . Note that the
0 ℓ ℓ=1,...,M m=1,...,M t t 0 +
upper integration limits in (1.3) are t , that is, the integrations are all over the open
−
interval ( ,t). This assures predictability of the intensity disregarding the values of
−∞
h(m)(0).Alternatively,itcanbe assumedthroughoutthath(m)(0)=0,inwhichcasethe
ℓ ℓ
integralsin(1.3)canbeover( ,t]withoutcompromisingpredictability.Theparameter
−∞
ν(m) is called the spontaneous rate, whereas the function h(m) is called the interaction
ℓ
function of N(ℓ) on N(m). The goal is to estimate f by using Lasso estimates. In the
0
sequel, we will assume that the support of h(m) is bounded. By rescaling, we can then
ℓ
assume that the supportis in [0,1],and we will do so throughout.Note that in this case
wewillneedtoobservetheprocesson[ 1,T]inordertocompute ψ(m)(f )for t [0,T].
− t 0 ∈
Lasso and probabilistic inequalities for multivariate point processes 7
The Hilbert space associated with this setup is
H
=(R L ([0,1]) M ) M
2
H ×
= f =((µ(m),(g(m)) ) ):
ℓ ℓ=1,...,M m=1,...,M
(cid:26)
1
g(m) with support in [0,1] and f 2= (µ(m)) 2 + g(m)(t)2dt< .
ℓ k k ℓ ∞
m m ℓ Z0 (cid:27)
X XX
Sometheoreticalresultsareestablishedinthis generalsettingbuttogofurther,weshall
consider in Section 5 the case where the functions h(m) are nonnegative and then λ(m)
ℓ t
is a linear function of f , as in Sections 1.2.1 and 1.2.2:
0
λ(m)=ψ(m)(f ).
t t 0
The multivariate point process associated with this setup is called the multivariate
Hawkes self exciting point process (see [34]). In this example, M is fixed and asymp-
totic properties are obtained when T tends to infinity.
To the best of our knowledge, nonparametric estimation for Hawkes models has
only been proposed in [52] in the univariate setting where the method is based on ℓ -
0
penalization of the least-squares contrast. However,due to ℓ -penalization, the criterion
0
is not convex and the computational cost, in particular for the memory storage of all
the potential estimators, is huge. Therefore, this method has never been adapted to the
multivariate setting. Moreover, the penalty term in this method is not data-driven and
ad-hoc tuning procedures have been used for simulations. This motivates the present
work and the use of a convex Lasso criterion combined with data-driven weights, to
provide a fully implementable and theoretically valid data-driven method, even in the
multivariate case.
Applications in neuroscience
Hawkes processes can naturally be applied to model neuronal activity. Extracellular ac-
tion potentials can be recorded by electrodes and the recorded data for the neuron m
can be seen as a point process, each point corresponding to the peak of one action po-
tential of this neuron (see [10], for instance, for precise definitions). When M neurons
are simultaneously recorded, one can assume that we are faced with a realization of
N =(N(m)) modeled by a multivariate Hawkes process. We then assume that
m=1,...,M
theintensityassociatedwiththeactivityoftheneuronmisgivenbyλ(m)=(ψ(m)(f )) ,
t t 0 +
whereψ(m)(f )isgivenin(1.3).Atanyoccurrenceu<tofN(ℓ),ψ(m)(f )increases(exci-
t 0 t 0
tation)ordecreases(inhibition)accordingtothe signofh(m)(t u).Modelinginhibition
ℓ −
is essential from the neurobiological point of view. So, we cannot assume that all inter-
actionfunctions are nonnegative,andwe cannotomitthe positivepart.Moredetails are
given in Section 6.
8 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Inneuroscience,Hawkesprocessescombinedwithmaximumlikelihoodestimationhave
beenusedintheseminalpaper[22],buttheapplicationofthemethodrequiresatoohuge
numberofobservationsforrealisticpracticalpurposes.ModelsbasedonHawkesprocesses
have nevertheless been recently discussed in neuroscience, since they constitute one of
the few simple models able to produce dependence graphs between neurons, that may
be interpretedinneuroscienceas functionalconnectivity graphs[45, 46].However,many
nonparametricstatisticalquestionsarisethatarenotsolvedyetinordertofurnishafully
applicable tool for real data analysis [38]. We think that the Lasso-based methodology
presented in this paper may furnish the first robust tool in this direction.
1.3. Notation and overview of the paper
Some notation from the general theory of stochastic integration is useful to simplify
the otherwise quite heavy notation. If H =(H(1),...,H(M)) is a multivariate process
with locally bounded coordinates, say, and X=(X(1),...,X(M)) is a multivariate semi-
martingale, we define the real valued process H X by
•
M t
H X := H(m)dX(m).
• t s s
m=1Z0
X
Given F:R R we use F(H) to denote the coordinatewise application of F, that is
7−→
F(H)=(F(H(1)),...,F(H(M))). In particular,
M t
F(H) X = F(H(m))dX(m).
• t s s
m=1Z0
X
We also define the following scalar product on the space of multivariate processes. For
any multivariate processes H =(H(1),...,H(M)) and K=(K(1),...,K(M)), we set
M T
H,K := H(m)K(m)ds,
h i proc s s
m=1Z0
X
the corresponding norm being denoted H . Since ψ introduced in (1.1) is linear,
proc
k k
the Hilbert space inherits a bilinear form from the previous scalar product, that we
H
denote, for all f,g in ,
H
M T
f,g := ψ(f),ψ(g) = ψ(m)(f)ψ(m)(g)ds,
h i T h iproc s s
m=1Z0
X
and the corresponding quadratic form is denoted f 2.
k kT
The compensator Λ=(Λ(m)) of N =(N(m)) is finally defined for all
m=1,...,M m=1,...,M
t by
t
Λ(m)= λ(m)ds.
t s
Z0
Lasso and probabilistic inequalities for multivariate point processes 9
Section 2 gives our main oracle inequality and the choice of the ℓ -weights in the
1
generalframework of counting processes.Section 3 provides the fundamental Bernstein-
type inequality.Section4 details the meaning ofthe oracleinequality inthe Poissonand
Aalen setups. The probabilistic results needed for the Hawkes processes as well as the
interpretationoftheoracleinequalityinthisframeworkisdoneinSection5.Simulations
onmultivariateHawkesprocessesareperformedinSection6.ThelastSectionisdedicated
to the proofs of our results.
2. Lasso estimate and oracle inequality
Wewishtoestimatethetrueunderlyingintensitysoourmaingoalconsistsinestimating
the parameter f . For this purpose, we assume we are given Φ a dictionary of func-
0
tions (whose cardinality is denoted Φ) and we define f as a linear combination of the
a
| |
functions of Φ, that is,
f := a ϕ,
a ϕ
ϕ Φ
X∈
where a=(a ) belongs to RΦ. Then, since ψ is linear, we get
ϕ ϕ Φ
∈
ψ(f )= a ψ(ϕ).
a ϕ
ϕ Φ
X∈
We use the following least-squares contrast defined on by
C H
(f):= 2ψ(f) N + f 2, f . (2.1)
C − • T k kT ∀ ∈H
This contrast, or some variations of , have already been used in particular setups (see,
C
for instance, [52] or [29]). The main heuristic justification lies in following arguments.
Since ψ(f) is a predictable process, the compensator at time T of (f) is given by
C
˜(f):= 2ψ(f) Λ + f 2,
C − • T k kT
whichcanalsobewrittenas ˜(f)= 2 ψ(f),λ + ψ(f) 2 .Notethat ˜isminimum
C − h i proc k kproc C
when ψ(f) λ isminimum.Ifλ=ψ(f )andif isanormontheHilbertspace
proc 0 T
k − k k·k
, then the unique minimizer of ˜is f . Therefore,to getthe best linear approximation
0
H C
of λ of the form ψ(f), it is natural to look at minimizers of (f). Restricting to linear
C
combinations of functions of Φ, since ψ is linear, we obtain
(f )= 2ab+aGa,
a ′ ′
C −
where a denotes the transpose of the vector a and for ϕ ,ϕ Φ,
′ 1 2
∈
b =ψ(ϕ ) N , G = ϕ ,ϕ . (2.2)
ϕ1 1
•
T ϕ1,ϕ2
h
1 2
i
T
10 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Note that both the vector b of dimension Φ and the Gram matrix G of dimensions
| |
Φ Φ are random but nevertheless observable.
| |×| |
To estimate a we minimize the contrast, (f ), subject to an ℓ -penalization on the
a 1
C
a-vector.That is, we introduce the following ℓ -penalized estimator
1
aˆ argmin 2a′b+a′Ga+2d′ a , (2.3)
∈ {− | |}
a RΦ
∈
where a =(a ) and d RΦ. With a good choice of d the solution of (2.3) will
achieve | b | oth | sp ϕ a | rs ϕ i ∈ ty Φ and goo ∈ d s + tatistical properties. Finally, we let fˆ=f denote the
aˆ
Lasso estimate associated with aˆ.
Our first result establishes theoretical properties of fˆ by using the classical oracle
approach. More precisely, we establish a bound on the risk of fˆif some conditions are
true. This is a nonprobabilistic result that only relies on the definition of aˆ by (2.3). In
the next section we will deal with the probabilistic aspect, which is to prove that the
conditions are fulfilled with large probability.
Theorem 1. Let c>0. If
G cI (2.4)
(cid:23)
and if for all ϕ Φ
∈
b ¯b d , (2.5)
ϕ ϕ ϕ
| − |≤
where
¯b =ψ(ϕ) Λ ,
ϕ T
•
then there exists an absolute constant C, independent of c, such that
k
ψ(fˆ)
−
λ
k
2
proc≤
C
a
in
R
f
Φ k
λ
−
ψ(f
a
)
k
2
proc
+c− 1 d2
ϕ
, (2.6)
∈ (cid:26) ϕ ∈XS(a) (cid:27)
where S(a) is the support of a. If λ=ψ(f ), the oracle inequality (2.6) can also be
0
rewritten as
fˆ f 2 C inf f f 2 +c 1 d2 . (2.7)
k − 0 kT ≤ a RΦ k 0 − a kT − ϕ
∈ (cid:26) ϕ ∈XS(a) (cid:27)
The proof of Theorem 1 is given in Section 7.1. Note that assumption (2.4) ensures
that G is invertible and then coordinates of aˆ are finite almost surely. Assumption (2.4)
alsoensuresthat f is a realnormon f atleastwhen f is a linearcombinationofthe
T
k k
functions of Φ.
Two terms are involved on the right-hand sides of (2.6) and (2.7). The first one is
an approximation term and the second one can be viewed as a variance term providing
a control of the random fluctuations of the b ’s around the ¯b ’s. Note that b ¯b =
ϕ ϕ ϕ ϕ
−
ψ(ϕ) (N Λ) isamartingale(seealsothecommentsafterTheorem2formoredetails).
T
• −
Lasso and probabilistic inequalities for multivariate point processes 11
The approximation term can be small but the price to pay may be a large support of
a, leading to large values for the second term. Conversely, a sparse a leads to a small
second term. But in this case the approximation term is potentially larger. Note that if
the function f can be approximated by a sparse linear combination of the functions of
0
Φ, then we obtain a sharp control of fˆ f 2. In particular, if f can be decomposed
k − 0 kT 0
on the dictionary, so we can write f =f for some a RΦ, then (2.7) gives
0 a0 0
∈
k
fˆ
−
f
0 k
2
T ≤
Cc− 1 d2
ϕ
.
ϕ ∈XS(a0)
In this case,the right-handside can be viewed as the sum of the estimationerrorsmade
by estimating the components of a .
0
Such oracle inequalities are now classical in the huge literature of Lasso procedures.
See, for instance, [5, 6, 15–18, 37, 61], who established oracle inequalities in the same
spirit as in Theorem 1. We bring out the paper [19], which gives technical and heuristic
arguments for justifying optimality of such oracle inequalities (see Section 1.3 of [19]).
Most of these papers deal with independent data.
In the sequel, we prove that assumption (2.4) is satisfied with large probability by
using the same approach as [55, 56] and to a lesser extent as Section 2.1 of [19] or [54].
Section 5 is in particular mainly devoted to show that (2.4) holds with large probability
for the multivariate Hawkes processes.
ForTheorem1tobeofinterest,theconditiononthemartingale,condition(2.5),needs
to hold with large probability as well. From this control, we deduce convenient data-
driven ℓ -weights that are the key parameters of our estimate. Note that our estimation
1
proceduredoesnotdependonthevalueofcin(2.4).Soknowingthelatterisnotnecessary
forimplementing ourprocedure.Therefore,oneofthe maincontributionsofthe paperis
to provide new sharp concentration inequalities that are satisfied by multivariate point
processes.This is the main goalof Theorem3 in Section 3 where we establish Bernstein
type inequalities for martingales. We apply it to the control of (2.5). This allows us to
derive the following result, which specifies the choice of the d ’s needed to obtain the
ϕ
oracle inequality with large probability.
Theorem2. LetN =(N(m)) beamultivariatecountingprocesswithpredictable
m=1,...,M
intensities λ(m) and almost surely finite corresponding compensator Λ(m). Define
t t
Ω = for any ϕ Φ, sup ψ(m)(ϕ) B and (ψ(ϕ)) 2 N V ,
V,B ∈ | t |≤ ϕ • T ≤ ϕ
t [0,T],m
n ∈ o
for positive deterministic constants B and V and
ϕ ϕ
Ω = G cI .
c
{ (cid:23) }
Let x and ε be strictly positive constants and define for all ϕ Φ,
∈
B x
d = 2(1+ε)Vˆµx+ ϕ , (2.8)
ϕ ϕ
3
q
12 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
with
µ B2x
Vˆµ= (ψ(ϕ)) 2 N + ϕ
ϕ µ φ(µ) • T µ φ(µ)
− −
for a real number µ such that µ>φ(µ), where φ(u)=exp(u) u 1.Let us consider the
− −
Lasso estimator fˆof f defined in Section 2. Then, with probability larger than
0
log(1+µV /(B2x))
1 4 ϕ ϕ +1 e x P(Ωc ) P(Ωc),
− log(1+ε) − − V,B − c
ϕ Φ(cid:18) (cid:19)
X∈
inequality (2.7) is satisfied, that is,
ψ(fˆ) λ 2 C inf λ ψ(f ) 2 +c 1 d2 .
k − kproc≤ a RΦ k − a kproc − ϕ
∈ (cid:26) ϕ ∈XS(a) (cid:27)
If moreover λ=ψ(f ), then
0
fˆ f 2 C inf f f 2 +c 1 d2 ,
k − 0 kT ≤ a RΦ k 0 − a kT − ϕ
∈ (cid:26) ϕ ∈XS(a) (cid:27)
where C is a constant independent of c, Φ, T and M.
The first oracle inequality gives a control of the difference between the true intensity
and ψ(fˆ). The equality λ=ψ(f ) is not required and we can apply this result, for
0
instance, with λ=(ψ(f )) .
0 +
Of course, the smaller the d ’s the better the oracle inequality. Therefore, when x in-
ϕ
creases,theprobabilityboundandthed ’sincreaseandwehavetorealizeacompromise
ϕ
toobtainameaningfuloracleinequalityonaneventwithlargeprobability.Thechoiceof
x is deeplydiscussedbelow,inSections 4and5for theoreticalpurposesandinSection6
for practical purposes.
Let us first discuss more deeply the definition of d (derived from subsequent The-
ϕ
orem 3) which seems intricate. Up to a constant depending on the choice of µ and ε,
d is of same order as max( x(ψ(ϕ))2 N ,B x). To give more insight on the values
ϕ T ϕ
•
of d , let us consider the very special case where for any m 1,...,M for any s,
ϕ p ∈{ }
ψ(m)(ϕ)=c 1 , where c is a positive constant and A a compact set included
in s to [0,T]. I m n t{h s i∈s A c m as}e, by natu m rally choosing B =max m c , we have:
ϕ 1 m M m
≤ ≤
M
x(ψ(ϕ)) 2 N B x c2 N (m) x max c2 ,
• T ≥ ϕ ⇐⇒ m Am ≥ 1 m M m
q m=1 ≤ ≤
X
where N(m) represents the number of points of N(m) falling in A . For more general
Am m
vector functions ψ(ϕ), the term x(ψ(ϕ))2 N will dominate B x if the number of
T ϕ
•
points of the process lying where ψ(ϕ) is large, is significant. In this case, the leading
p
Lasso and probabilistic inequalities for multivariate point processes 13
term in d is expected to be the quadratic term 2(1+ε) µ x(ψ(ϕ))2 N andthe
ϕ µ φ(µ) • T
−
linear terms in x can be viewed as residual termsq. Furthermore, note that when µ tends
to 0,
µ µ x x
=1+ +o(µ), +
µ φ(µ) 2 µ φ(µ) ∼ µ → ∞
− −
since x>0. So, if µ and ε tend to 0, the quadratic term tends to 2x(ψ(ϕ))2 N , but
T
•
the price to pay is the explosion of the linear term in x. In any case, it is possible to
p
make the quadratic term as close to 2x(ψ(ϕ))2 N as desired. Basically, this term
T
•
cannot be improved (see comments after Theorem 3 for probabilistic arguments).
p
Letusnowdiscussthechoiceofx.Inmoreclassicalcontextssuchasdensityestimation
based on an n-sample, the choice x=γlog(n) plugged in the parameters analog to the
d ’s is convenient, since it both ensures a small probability bound and a meaningful
ϕ
order of magnitude for the oracle bound (see [5] for instance). See also Sections 4 and
5 for similar evaluations in our setup. But it has also been further established that the
choice γ =1 is the best. Indeed if the components of d are chosen smaller than the
analog of 2x(ψ(ϕ))2 N in the density framework, then the resulting estimator is
T
•
definitelybadfromthetheoreticalpointofview,butsimulationsalsoshowthat,tosome
p
extent, if the components of d are larger than the analog of 2x(ψ(ϕ))2 N , then the
T
•
estimator deteriorates too. A similar result is out of reach in our setting, but similar
p
conclusions may remain valid here since density estimation often provides some clues
about what happens for more intricate heteroscedastic models. In particular, the main
heuristic justifying the optimality of this tuning result in the density setting is that the
quadraticterm(andinparticulartheconstant√2)correspondstotherateofthecentral
limittheoremandinthissense,itprovidesthe“bestapproximation”forthefluctuations.
For further discussion, see the simulation study in Section 6.
Finally, it remains to control P(Ω ) and P(Ω ). These are the goals of Section 4 for
V,B c
Poissonand Aalen models and Section 5 for multivariate Hawkes processes.
3. Bernstein type inequalities for multivariate point
processes
We establish a Bernstein type concentration inequality based on boundedness assump-
tions.Thisresult,whichhasaninterestpersefromtheprobabilisticpointofview,isthe
keyresultto derive the convenientvalues for the vectord inTheorem2and sois capital
from the statistical perspective.
Theorem3. LetN =(N(m)) beamultivariatecountingprocesswithpredictable
m=1,...,M
intensitiesλ(m) andcorrespondingcompensatorΛ(m) withrespecttosomegivenfiltration.
t t
Let B>0. Let H =(H(m)) be a multivariate predictable process such that for
m=1,...,M
all ξ (0,3), for all t,
∈
exp(ξH/B) Λ < a.s. and exp(ξH2/B2) Λ < a.s. (3.1)
t t
• ∞ • ∞
14 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let us consider the martingale defined for all t 0 by
≥
M =H (N Λ) .
t t
• −
Let v>w and x be positive constants and let τ be a bounded stopping time. Let us define
µ B2x
Vˆµ= H2 N +
τ
µ φ(µ) • µ φ(µ)
− −
for a real number µ (0,3) such that µ>φ(µ), where φ(u)=exp(u) u 1. Then, for
∈ − −
any ε>0,
Bx
P M 2(1+ε)Vˆµx+ and w Vˆµ v and sup H(m) B
τ ≥ 3 ≤ ≤ | t |≤
(cid:18) q m,t ≤ τ (cid:19) (3.2)
log(v/w)
2 +1 e− x.
≤ log(1+ε)
(cid:18) (cid:19)
This result is based on the exponential martingale for counting processes, which has
beenusedforalongtimeinthe contextofthe countingprocesstheory.See,forinstance,
[8, 57] or [59]. This basically gives a concentration inequality taking the following form
(the result is stated here in its univariate form for comparison purposes): for any x>0,
Bx τ
P M 2ρx+ and H2λ(s)ds ρ and sup H B e x. (3.3)
τ ≥ 3 s ≤ | s |≤ ≤ −
(cid:18)
p
Z0 s
∈
[0,τ] (cid:19)
In (3.3), ρ is a deterministic upper bound of v= τ H2λ(s)ds, the bracket of the mar-
0 s
tingale,andconsequentlythemartingaleequivalentofthe varianceterm.Moreover,B is
R
a deterministic upper bound of sup H . The leading term for moderate values of
s ∈ [0,τ]| s |
x and τ large enough is consequently √2ρx. The central limit theorem for martingales
states that, under some assumptions, a sequence of martingales (M ) with respective
n n
brackets (v ) tending to a deterministic value v¯, once correctly normalized, tends to a
n n
Gaussianprocesswithbracketv¯.Therefore,atermoftheform√2v¯xintheupperbound
is notimprovable,in particularthe constant √2. Howeverthe replacementofthe limit v¯
by a deterministic upper bound ρ constitutes a loss. In this sense, Theorem 3 improves
the bound and consists of plugging in the unbiased estimate vˆ= τ H2dN instead of a
0 s s
nonsharp deterministic upper bound of v. Note that we are not able to obtain exactly
R
theterm√2 butanyvaluestrictlylargerthan √2,ascloseaswewantto √2 uptosome
additive terms depending on B that are negligible for moderate values of x.
Theproofisbasedonapeelingargumentthatwasfirstintroducedin[40]forGaussian
processes and is given in Section 7.3.
Notethatthere existalsoinequalitiesthatseemnicerthan(3.3)whichconstitutesthe
basicbrickforourpurpose.Forinstance,in[27],itisestablishedthatforanydeterministic
positive real number θ, for any x>0,
τ τ
P M √2θx and H2dΛ + H2dN θ e x. (3.4)
τ ≥ s s s s ≤ ≤ −
(cid:18) Z0 Z0 (cid:19)
Lasso and probabilistic inequalities for multivariate point processes 15
At first sight, this seems better than Theorem 3 because no linear term depending on B
appears,butifwewishto usetheestimate 2 τ H2dN insteadofθ intheinequality,we
0 s s
have to bound H by some B in any case. Moreover, by doing so, the quadratic term
s
| | R
will be of order √4vˆx which is worse than the term √2vˆx derived in Theorem 3, even if
this constant √2 can only be reached asymptotically in our case.
ThereexistsabetterresultifthemartingaleM isforinstanceconditionallysymmetric
t
(see [4, 25, 27]): for any x>0,
τ
P M
τ ≥
√2κx and H
s
2dN
s ≤
κ
≤
e− x, (3.5)
(cid:18) Z0 (cid:19)
which seems close to the ideal inequality. But there are actually two major problems
with this inequality. First, one needs to assume that the martingale is conditionally
symmetric, which cannot be the case in our situation for generalcounting processes and
generaldictionaries.Second,itdepends onthedeterministicupperboundκ insteadofvˆ.
To replace κ by vˆand then to apply peeling argumentsas in the proofof Theorem3,we
needtoassumethe existenceofapositiveconstantw suchthatvˆ w.Butiftheprocess
≥
is empty, then vˆ=0, so we cannot generally find such a positive lower bound, whereas
in our theorem, we can always take w= B2x as a lower bound for Vˆµ.
µ φ(µ)
−
Finally, note that in Proposition 6 (see Section 7.3), we also derive a similar bound
whereVˆµ isreplacedby τ H2dΛ .Basically,itmeansthatthesametypeofresultshold
0 s s
for the quadratic characteristic instead of the quadratic variation. Though this result is
R
of little use here, since the quadratic characteristic is not observable, we think that it
may be of interest for readers investigating self-normalized results as in [26].
4. Applications to the Poisson and Aalen models
We nowapplyTheorem2to the PoissonandAalenmodels.The caseofthe multivariate
Hawkes process, which is much more intricate, will be the subject of the next section.
4.1. The Poisson model
Let us recall that in this case, we observe M i.i.d. Poisson processes with intensity f
0
supported by [0,1] (with M 2) and that the norm is given by f 2= 1 f2(x)dx. We
≥ k k 0
assume that Φ is an orthonormalsystem for . In this case,
k·k R
2=M 2 and G=MI,
k·k1 k·k
where I is the identity matrix. One applies Theorem 2 with c=M (so P(Ωc)=0) and
c
B = ϕ , V = ϕ 2 (1+δ)Mm ,
ϕ ϕ 1
k k∞ k k∞
16 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
for δ>0 and m = 1 f (t)dt. Note that here T =1 and therefore N(m)=N(m) is the
1 0 0 T 1
total number of observed points for the mth process. Using
R
M
ψ(ϕ)2 N ϕ 2 N(m)
• 1 ≤k k∞ 1
m=1
X
and since the distribution of M N(m) is the Poisson distribution with parameter
m=1 1
Mm , Cramer–Chernovarguments give:
1
P
M
P(Ωc ) P N(m)>(1+δ)Mm exp( (1+δ)ln(1+δ) δ Mm ).
V,B ≤ 1 1 !≤ −{ − } 1
m=1
X
For α>0, by choosing x=αlog(M), we finally obtain the following corollary derived
from Theorem 2.
Corollary 1. With probability larger than 1 − C 1| Φ | l M og α (M) − e − C2M, where C 1 is a con-
stant depending on µ, ε, α, δ and m and C is a constant depending on δ and m , we
1 2 1
have:
fˆ f 2 C inf f f 2
0 0 a
k − k ≤ a RΦ(k − k
∈
1 M 1
+ log(M) ϕ2(x)dN(m)+log2(M) ϕ 2 ,
M2
ϕ ∈XS(a) m X =1Z0
x k k∞!)
where C is a constant depending on µ, ε, α, δ and m .
1
To shed some lights on this result, consider an asymptotic perspective by assuming
that M is large. Assume also, for sake of simplicity, that f is bounded from below on
0
[0,1]. If the dictionary Φ (whose size may depend on M) satisfies
M
max ϕ =o ,
ϕ Φk k∞ slogM
∈ (cid:18) (cid:19)
then, since, almost surely,
1 M 1 1
M ϕ2(x)dN x (m)M − → → ∞ ϕ2(x)f 0 (x)dx,
m=1Z0 Z0
X
almost surely,
1 M 1
log(M) ϕ2(x)dN(m)+log2(M) ϕ 2
M2
ϕ ∈XS(a) m X =1Z0
x k k∞!
Lasso and probabilistic inequalities for multivariate point processes 17
logM 1
= ϕ2(x)f (x)dx (1+o(1)).
0
M ×
ϕ
∈XS(a)Z0
The right-hand term corresponds, up to the logarithmic term, to the sum of variance
terms when estimating 1 ϕ(x)f (x)dx with 1 M 1 ϕ(x)dN (m) for ϕ S(a). This
0 0 M m=1 0 x ∈
meansthat the estimator adaptivelyachievesthe besttrade-offbetween abias termand
R P R
a variance term. The logarithmic term is the price to pay for adaptation. Furthermore,
when M + , the inequality of Corollary 1 holds with probability that goes to 1 at a
→ ∞
polynomial rate. We refer the reader to [50] for a deep discussion on optimality of such
results.
4.2. The Aalen model
Results similar to those presented in this paragraph can be found in [29] under re-
stricted eigenvalues conditions instead of (2.4). Recall that we observe an M-sample
(X(m),Y(m),N(m)) , with Y(m) =(Y(m)) and N(m) =(N(m)) (with
m=1,...,M t t [0,1] t t [0,1]
∈ ∈
M 2).We assumethat X(m) [0,1]andthatthe intensity ofN(m) is f (t,X(m))Y(m).
≥ ∈ t 0 t
We set for any f,
1
f 2:=E f2(t,X(1))(Y(1)) 2 dt .
k ke t
(cid:18)Z0 (cid:19)
We assume that Φ is an orthonormal system for , the classical norm on L ([0,1]2),
2 2
k·k
and we assume that there exists a positive constant r such that
f L ([0,1]2), f r f , (4.1)
2 e 2
∀ ∈ k k ≥ k k
sothat isanorm.Ifweassume,forinstance,thatthedensityofthe X(m)’sislower
e
k·k
bounded by a positive constant c and there exists c >0 such that for any t,
0 1
E[(Y(1)) 2 X(1)] c
t | ≥ 1
then(4.1)holdswithr2=c c .Theempiricalversionof f ,denoted f ,isdefined
0 1 e emp
k k k k
by
1 1 M 1
f 2 := f 2 = f2(t,X(m))(Y(m)) 2 dt.
k kemp Mk kT M t
m=1Z0
X
UnlikethePoissonmodel,sincetheintensitydependsoncovariatesX(m)’sandvariables
Y(m)’s, the control of P(Ωc) is much more cumbersome for the Aalen case, even if it
c
is less intricate than for Hawkes processes (see Section 5). We have the following result
proved in Section 7.5.1.
18 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Proposition 1. We assume that (4.1) is satisfied, the density of the covariates X(m) is
bounded by D and
sup max Y(m) 1 almost surely. (4.2)
t ∈ [0,1]m ∈{ 1,...,M } t ≤
We consider an orthonormal dictionary Φ of functions of L ([0,1]2) that may depend
2
on M, and we let r denote the spectral radius of the matrix H whose components are
Φ
H ϕ,ϕ′= ϕ(t,x) ϕ
′
(t,x) dtdx. Then, if
| || |
RR logM
max ϕ 2 r Φ 0, (4.3)
Φ
ϕ Φk k∞× | |× M →
∈
when M + then, for any β>0, there exists C >0 depending on β, D and f such
1 0
→ ∞
that with c=C M, we have
1
P(Ωc)=O(Φ2M β).
c | | −
Assumption (4.2) is usually satisfied in most of the practical examples where Aalen
models are involved.See [2] for explicit examples and see, for instance, [30, 49] for other
articles where this assumptionis made. In the sequel,we also assume that there exists a
positive constant R such that
max N(m) R a.s. (4.4)
m 1,...,M 1 ≤
∈{ }
Thisassumption,consideredby[49],isobviouslysatisfiedinsurvivalanalysiswherethere
is atmostone deathper individual.It couldhavebeenrelaxedinoursetting, by consid-
ering exponential moments assumptions, to include Markovcases for instance. However,
this much simpler assumption allows us to avoid tedious and unnecessary technical as-
pects since we only wishto illustrate our results in a simple framework.Under (4.2) and
(4.4), almost surely,
M 1 M 1
ψ(ϕ)2 N = [Y(m)] 2 ϕ2(t,X(m))dN(m) ϕ2(t,X(m))dN(m) MR ϕ 2 .
• T
m=1Z0
t t ≤
m=1Z0
t ≤ k k∞
X X
So, we apply Theorem 2 with B = ϕ , V =MR ϕ 2 (so P(Ω )=1) and x=
ϕ ϕ V,B
αlog(M) for α>0. We finally obtain k th k e ∞ following coro k lla k r∞y.
Corollary 2. Assume that (4.2) and (4.4) are satisfied. With probability larger than
1 − C 1| Φ | l M og α (M) − P(Ωc c ), where C 1 is a constant depending on µ, ε, α and R, we have:
fˆ f 2 C inf f f 2
k − 0 kemp≤ a RΦ(k 0 − a kemp
∈
1 M 1
+ log(M) ϕ2(t,X(m))dN(m)+log2(M) ϕ 2 ,
M2
ϕ ∈XS(a) m X =1Z0
t k k∞!)
Lasso and probabilistic inequalities for multivariate point processes 19
where C is a constant depending on µ, ε, α and R.
To shed lights on this result, assume that the density of the X(m)’s is upper bounded
by a constant R˜. In an asymptotic perspective with M , we have almost surely,
→∞
1 M 1 1
ϕ2(t,X(m))dN (m) E ϕ2(t,X(1))f (t,X(1))Y(1)dt .
M t → 0
m=1Z0 (cid:18)Z0 (cid:19)
X
But
1 1
E ϕ2(t,X(1))f (t,X(1))Y(1)dt f E ϕ2(t,X(1))dt R˜ f .
0 0 0
(cid:18)Z0 (cid:19)
≤k k∞
(cid:18)Z0 (cid:19)
≤ k k∞
So, if the dictionary Φ satisfies
M
max ϕ =O ,
ϕ Φk k∞ slogM
∈ (cid:18) (cid:19)
which is true under (4.3) if r Φ 1, then, almost surely, the variance term is asymp-
Φ
| |≥
totically smaller than log(M)| S(a) |k f0k ∞ up to constants. So, we can draw the same
M
conclusions as for the Poisson model. We have not discussed here the choice of Φ and
Condition(4.3).ThiswillbeextensivelydoneinSection5.2wherewedealwithasimilar
condition but in a more involved setting.
5. Applications to the case of multivariate Hawkes
process
For a multivariate Hawkes model, the parameter f =(ν(m),(h(m)) ) be-
0 ℓ ℓ=1,...,M m=1,...,M
longs to
M
=HM = f =(f(m)) f(m) H and f 2= f(m) 2
H ( m=1,...,M ∈ k k k k )
(cid:12) m X =1
(cid:12)
where (cid:12)
H= f =(µ,(g ) ) µ R,g with support in [0,1]
ℓ ℓ=1,...,M ℓ
( ∈
(cid:12)
(cid:12)
M (cid:12) 1
and f 2=µ2+ g2(x)dx< .
k k
ℓ=1Z0
ℓ ∞)
X
If one defines the linear predictable transformation κ of H by
M t
κ (f)=µ+ − g (t u)dN(ℓ), (5.1)
t ℓ − u
X
ℓ=1Zt
−
1
20 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
then the transformation ψ on is given by
H
ψ(m)(f)=κ (f(m)).
t t
The first oracle inequality of Theorem 2 provides theoretical guaranties of our Lasso
methodology in full generality and in particular, even if inhibition takes place (see Sec-
tion 1.2.3). Since Ω and Ω are observable events, we know whether the oracle in-
V,B c
equality holds. However we are not able to determine P(Ω ) and P(Ω ) in the general
V,B c
case. Therefore, in Sections 5.1 and 5.2, we assume that all interaction functions are
nonnegative and that there exists f in so that for any m and any t,
0
H
λ(m)=ψ(m)(f ).
t t 0
We also assume that the process is observed on [ 1,T] with T >1.
−
5.1. Some useful probabilistic results for multivariate Hawkes
processes
In this paragraph, we present some particular exponential results and tail controls for
Hawkes processes. As far as we know, these results are new: They constitute the gener-
alization of [51] to the multivariate case. In this paper, they are used to control P(Ωc)
c
and P(Ωc ) but they may be of independent interest.
V,B
Since the functions h(m)’s are nonnegative, a cluster representation exists. We can
ℓ
indeed construct the Hawkes process by the Poisson cluster representation (see [24]) as
follows:
Distributeancestral points withmarksℓ=1,...,M accordingtohomogeneousPois-
•
son processes with intensities ν(ℓ) on R.
For each ancestral point, form a cluster of descendant points. More precisely, start-
•
ing with an ancestral point at time 0 of a certain type, we successively build new
generations as Poisson processes with intensity h(m)( T), where T is the parent
ℓ ·−
of type ℓ (the corresponding children being of type m). We will be in the situation
wherethisprocessbecomesextinguishedandwedenoteby H thelastchildrenofall
generations,whichalsorepresentsthelengthofthecluster.Notethatthenumberof
descendants is a multitype branching process (and there exists a branching cluster
representation (see [9, 24, 34])) with offspring distributions being Poisson variables
with means
1
γ = h(m)(t)dt.
ℓ,m ℓ
Z0
The essential part we need is that the expected number of offsprings of type m from
a point of type ℓ is γ . With Γ=(γ ) , the theory of multitype branching
ℓ,m ℓ,m ℓ,m=1,...,M
processes gives that the clusters are finite almost surely if the spectral radius of Γ is
Lasso and probabilistic inequalities for multivariate point processes 21
strictly smaller than 1. In this case, there is a stationary version of the Hawkes process
by the Poisson cluster representation.
Moreover,if Γ has spectralradius strictly smaller than 1, one canprovide a bound on
thenumberofpointsinacluster.Wedenoteby P thelawofthe clusterwhoseancestral
ℓ
point is of type ℓ, E is the corresponding expectation.
ℓ
The following lemma is very general and holds even if the function h(m) have infinite
ℓ
support as long as the spectral radius Γ is strictly less than 1.
Lemma 1. If W denotes the total number of points of any type in the cluster whose
ancestral point is of type ℓ, then if the spectral radius of Γ is strictly smaller than 1 there
exists ϑ >0, only depending on ℓ and on Γ, such that
ℓ
E (eϑℓW)< .
ℓ
∞
This easily leads to the following result, which provides the existence of the Laplace
transform of the total number of points in an arbitrary bounded interval, when the
functions h(m) have bounded support.
ℓ
Proposition 2. Let N be a stationary multivariate Hawkes process, with compactly sup-
ported nonnegative interactions functions and suchthat the spectralradius of Γ is strictly
smaller than 1. For any A>0, let N be the total number of points of N in [ A,0),
[ A,0)
− −
all marks included. Then there exists a constant θ>0, depending on the distribution of
the process and on A, such that
:=E(eθN[−A,0))< ,
E ∞
which implies that for all positive u
P(N u) e θu.
[ A,0) −
− ≥ ≤E
Moreover,one can strengthen the ergodic theorem in a nonasymptotic way,under the
same assumptions.
Proposition 3. Under the assumptions of Proposition 2, let A>0 and let Z(N) be a
function depending on the points of N lying in [ A,0). Assume that there exist b and η
−
nonnegative constants such that
Z(N) b(1+Nη ),
| |≤ [ A,0)
−
where N represents the total number of points of N in [ A,0), all marks included.
[ A,0)
We deno − te S the shift operator, meaning that Z S (N) dep − ends now in the same way
t
◦
as Z(N) on some points that are now the points of N lying in [t A,t).
−
We assume E[Z(N)]< and for short, we denote E(Z)=E[Z(N)]. Then, for any
| | ∞
α>0, there exists a constant (α,η,f ,A)>1 such that for T > (α,η,f ,A), there
0 0
T T
22 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
exist C , C , C and C positive constants depending on α,η,A and f such that
1 2 3 4 0
T C
P [Z S (N) E(Z)]dt C σ Tlog3(T)+C b(log(T)) 2+η 4 ,
◦ t − ≥ 1 2 ≤ Tα
(cid:18)Z0 q (cid:19)
with σ2=E([Z(N) E(Z)]21 ) and ˜ =C log(T).
− N[−A,0)≤N ˜ N 3
Finally,todealwiththecontrolofP(Ω ),weshallneedthenextresult.First,wedefine
c
a quadratic form Q on H by
1 T
Q(f,g)=E (κ (f)κ (g))=E κ (f)κ (g)dt , f,g H. (5.2)
P 1 1 P t t
T ∈
(cid:18) Z0 (cid:19)
We have:
Proposition 4. Under the assumptions of Proposition 2, if the function parameter f
0
satisfies
min ν(m)>0 and max sup h(m)(t)< (5.3)
m ∈{ 1,...,M } l,m ∈{ 1,...,M }t ∈ [0,1] ℓ ∞
then there is a constant ζ>0 such that for any f H,
∈
Q(f,f) ζ f 2.
≥ k k
We are now ready to establish oracle inequalities for multivariate Hawkes processes.
5.2. Lasso for Hawkes processes
Inthe sequel,westillconsiderthe mainassumptionsofthe previoussubsection:We deal
withastationaryHawkesprocesswhoseintensityisgivenby(1.3)suchthatthespectral
radiusofΓisstrictlysmallerthan1and(5.3)issatisfied.We recallthatthecomponents
of Γ are the γ ’s with
ℓ,m
1
(m)
γ = h (t)dt.
ℓ,m ℓ
Z0
One of the main results of this section is to link properties of the dictionary (mainly
orthonormality but also more involved assumptions) to properties of G (the control of
Ω ). To do so, let us define for all f ,
c
∈H
f =max max µ(m) , max g(m) .
k k∞ m=1,...,M| | m,ℓ=1,...,Mk ℓ k
∞
n o
Then,letusset Φ :=max ϕ ,ϕ Φ .Thenextresultisbasedontheprobabilistic
k k∞ {k k∞ ∈ }
results of Section 5.1.
Lasso and probabilistic inequalities for multivariate point processes 23
Proposition5. Assumethat theHawkes process is stationary, that(5.3) issatisfiedand
that the spectral radius of Γ is strictly smaller than 1. Let r be the spectral radius of the
Φ
matrix H defined by
M 1
H= µ(m) µ(m) + (g )(m) (g )(m) (u)du .
X m "| ϕ || ρ | X ℓ=1Z0 | ϕ ℓ || ρ ℓ | #! ϕ,ρ ∈ Φ
Assume that Φ is orthonormal and that
log5(T)
A (T):=r Φ 2 Φ[log( Φ )+log(Φ)] 0 (5.4)
Φ Φ
k k∞| | k k∞ | | T →
when T . Then, for any β>0, there exists C >0 depending on β and f such that
1 0
→∞
with c=C T, we have
1
P(Ωc
c
)=O(
|
Φ
|
2T− β).
Up to logarithmic terms, (5.4) is similar to (4.3) with M replaced with T. The dic-
tionary Φ can be built via a dictionary (Υ ) of functions of L ([0,1]) (that may
k k=1,...,K 2
depend on T) in the following way. A function ϕ=(µ(m),((g )(m)) ) belongs to Φ if
ϕ ϕ ℓ ℓ m
and only if only one of its M +M2 components is nonzero and in this case,
if µ(m)=0, then µ(m)=1,
ϕ ϕ
• 6
if (g )(m)=0, then there exists k 1,...,K such that (g )(m)=Υ .
• ϕ ℓ 6 ∈{ } ϕ ℓ k
Note that Φ =M +KM2. Furthermore, assume from now on that (Υ ) is
k k=1,...,K
| |
orthonormalin L ([0,1]). Then Φ is also orthonormal in endowed with .
2
H k·k
Before going further, let us discuss assumption (5.4). First, note that the matrix H is
blockdiagonal.Thefirstblockisthe identity matrixofsize M.Theother M2 blocksare
identical to the matrix:
H = Υ (u) Υ (u) du .
K
|
k1
||
k2
|
(cid:18)Z (cid:19)1
≤
k1,k2≤ K
So, if we denote r˜ the spectral radius of H , we have:
K K
r =max(1,r˜ ).
Φ K
We analyze the behavior of r˜ with respect to K. Note that for any k and any k ,
K 1 2
(H ) 0.
K k1,k2
≥
Therefore,
r˜ sup H x max (H ) .
K
≤ k x kℓ1 =1 k
K
k
ℓ1
≤ k1 X k2
K k1,k2
Wenowdistinguishthreetypesoforthonormaldictionaries(rememberthat M isviewed
as a constant here):
24 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let us consider regular histograms. The basis is composed of the functions Υ =
k
• δ 1/21 with Kδ=1.Therefore, Φ =δ 1/2=√K. But H is the iden-
− ((k 1)δ,kδ] − K
− k k∞
tity matrix and r˜ =1. Hence, (5.4) is satisfied as soon as
K
K2log(K)log5(T)
0
T →
when T , which is satisfied if K=o( √T ).
→∞ log3(T)
Assume that Φ is bounded by an absolute constant(Fourier dictionaries satisfy
• k k∞
this assumption). Since r˜ K, (5.4) is satisfied as soon as
K
≤
K2log(K)log5(T)
0
T →
when T , which is satisfied if K=o( √T ).
→∞ log3(T)
Assume that (Υ ) is a compactly supported wavelet dictionary where reso-
k k=1,...,K
•
lution levels belong to the set 0,1,...,J . In this case, K is of the same order as
{ }
2J, Φ is of the same order as 2J/2 and it can be established that r˜ C2J/2
K
k k∞ ≤
where C is a constant only depending on the choice of the wavelet system (see [33]
for further details). Then, (5.4) is satisfied as soon as
K5/2log(K)log5(T)
0
T →
when T , which is satisfied if K=o(
T2/5
).
→∞ log12/5(T)
To apply Theorem 2, it remains to control Ω . Note that
V,B
1 if µ(m)=1,
ϕ
ψ(m)(ϕ)= t
t  − Υ (t u)dN(ℓ) if (g )(m)=Υ .
 k − u ϕ ℓ k
Zt
−
1
Let us define 
Ω = for all t [0,T],for all m 1,...,M we have N(m) .
N { ∈ ∈{ } [t − 1,t]≤N}
We therefore set
B =1 if µ(m)=1 and B = Υ if (g )(m)=Υ . (5.5)
ϕ ϕ ϕ k k k∞N ϕ ℓ k
Note that on Ω , for any ϕ Φ,
N ∈
sup ψ(m)(ϕ) B .
| t |≤ ϕ
t [0,T],m
∈
Lasso and probabilistic inequalities for multivariate point processes 25
Now, for each ϕ Φ, let us determine V that constitutes an upper bound of
ϕ
∈
M T
M = [ψ(m)(ϕ)] 2 dN(m).
ϕ t t
m=1Z0
X
Note that only one term in this sum is nonzero. We set
V = T if µ(m)=1 and V = Υ 2 T 3 if (g )(m)=Υ , (5.6)
ϕ ⌈ ⌉N ϕ ϕ k k k∞⌈ ⌉N ϕ ℓ k
where T denotes the smallest integer larger than T. With this choice, one has that
⌈ ⌉
Ω Ω , which leads to the following result.
V,B
N ⊂
Corollary 3. Assume that the Hawkes process is stationary, that (5.3) is satisfied and
that the spectral radius of Γ is strictly smaller than 1. With the choices (5.5) and (5.6),
P(Ω ) P(Ω ) 1 C Texp( C ),
V,B 1 2
≥ N ≥ − − N
where C and C are positive constants depending on f .
1 2 0
If log(T), then for all β>0,
N ≫
P(Ωc ) P(Ωc )=o(T β).
V,B ≤ N −
We are now ready to apply Theorem 2.
Corollary 4. Assume that the Hawkes process is stationary, that (5.3) is satisfied and
that the spectral radius of Γ is strictly smaller than 1. Assume that the dictionary Φ
is built as previously from an orthonormal family (Υ ) . With the notations of
k k=1,...,K
Theorem 2, let B be defined by (5.5) and d be defined accordingly with x=αlog(T).
ϕ ϕ
Then, with probability larger than
log(1+µ T /(αlog(T)))
1
−
4(M +M2K)
l
⌈
og(
⌉
1
N
+ε)
+1 T− α
−
P(Ωc
N
)
−
P(Ωc
c
),
(cid:18) (cid:19)
1 1 log(T)(ψ(ϕ))2 N B2log2(T)
fˆ f 2 C inf f f 2 + • T + ϕ ,
Tk − 0 kT ≤ a RΦ Tk 0 − a kT T2 T2
∈ (cid:26) ϕ ∈XS(a)(cid:18) (cid:19)(cid:27)
where C is a constant depending on f , µ, ε, and α.
0
From an asymptotic point of view, if the dictionary also satisfies (5.4), and if =
log2(T) in (5.5), then for T large enough with probability larger than 1 C Klog(T) N T α
1 −
−
1 1 log3(T) 1 log7/2(T)
fˆ f 2 C inf f f 2 + ϕ 2 + Φ 2 ,
Tk − 0 kT ≤ 2 a RΦ Tk 0 − a kT T Tk kT √T k k∞
∈ (cid:26) ϕ ∈XS(a)(cid:20) (cid:21)(cid:27)
where C and C are constants depending on M, f , µ, ε, and α.
1 2 0
26 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Weexpresstheoracleinequalitybyusing 1 simplybecause,whenT goesto+ ,
Tk·k T ∞
byergodicityoftheprocess(see,forinstance,[24],andProposition3foranonasymptotic
statement),
1 M 1 T M
f 2 = (κ (f(m))) 2 dt Q(f(m),f(m))
Tk kT T t −→
m=1 Z0 m=1
X X
under assumptions of Proposition 5. Note that the right-hand side is a true norm on
H
by Proposition 4. Note also that
log7/2(T)
Φ 2 T →∞0,
√T k k∞ →
as soon as (5.4) is satisfied for the Fourier basis and compactly supported wavelets. It
is also the case for histograms as soon as K=o( √T ). Therefore, this term can be
log7/2(T)
viewed as a residual one. In those cases, the last inequality can be rewritten as
1 1 log3(T) 1
fˆ f 2 C inf f f 2 + ϕ 2 ,
Tk − 0 kT ≤ a RΦ Tk 0 − a kT T Tk kT
∈ (cid:26) ϕ ∈XS(a) (cid:27)
for a different constant C, the probability of this event tending to 1 as soon as α 1/2
≥
in the Fourier and histogram cases and α 2/5 for the compactly supported wavelet
≥
basis. Once again, as mentioned for the Poisson or Aalen models, the right-hand side
corresponds to a classical “bias-variance” trade off and we obtain a classical oracle in-
equality up to the logarithmic terms. Note that asymptotics is now with respect to T
and not with respect to M as for Poisson or Aalen models. So, the same result, namely
Theorem 2, allows to consider both asymptotics.
6. Simulations for the multivariate Hawkes process
This section is devoted to illustrations of our procedure on simulated data of multivari-
ate Hawkes processes and comparisons with the well-known adaptive Lasso procedure
proposed by [64]. We consider the general case and we do no longer assume that the
functions h(m) are nonnegative as in Section 5. However, if the parameter ν(m) is large
ℓ
withrespecttotheh(m)’s,thenψ(m)(f )isnonnegativewithlargeprobabilityandthere-
ℓ 0
fore λ(m)=ψ(m)(f ) with large probability. Hence, Theorem 2 implies that fˆis close to
0
f .
0
6.1. Description of the data
Asmentionedintheintroduction,Hawkesprocessescanbeusedinneurosciencetomodel
theactionpotentialsofindividualneurons.So,weperformsimulationswhoseparameters
Lasso and probabilistic inequalities for multivariate point processes 27
are close, to some extent, to real neuronal data. For a given neuron m 1,...,M , we
∈{ }
recall that its activity is modeled by a point process N(m) whose intensity is
M t
λ(m)= ν(m)+ − h(m)(t u)dN(ℓ)(u) .
t ℓ − !
X ℓ=1Z−∞ +
(m)
The interaction function h represents the influence of the past activity of the neuron
ℓ
ℓontheneuronm.Thespontaneous rate ν(m) maysomehowrepresenttheexternalexci-
tation linked to all the other neurons that are not recorded. It is consequently of crucial
importance not only to correctly infer the interaction functions, but also to reconstruct
the spontaneous rates accurately. Usually, activity up to 10 neurons can be recorded in
a “stationary”phase during a few seconds (sometimes up to one minute). Typically,the
points frequency is of the order of 10–80 Hz and the interaction range between points is
ofthe orderofa few milliseconds (up to 20 or40 ms).We firstleadthree experiments in
thepureexcitation case wherealltheinteractionfunctionsarenonnegativebysimulating
multivariateHawkesprocesses(two with M =2,onewith M =8)basedonthesetypical
values. More precisely, we take for any m 1,...,M , ν(m) =20 and the interaction
∈{ }
(m)
functions h are defined as follows (supports of all the functions are assumed to lie in
ℓ
the interval [0,0.04]):
Experiment 1 (M =2 and piecewise constant functions).
h (1) =30 1 , h (1) =30 1 , h (2) =30 1 , h (2) =0.
1 × (0,0.02] 2 × (0,0.01] 1 × (0.01,0.02] 2
In this case, each neuron depends on the other one. The spectral radius of the matrix Γ
is 0.725.
Experiment 2 (M =2 and “smooth” functions). In this experiment, h(1) and h(2)
1 1
are not piecewise constant.
h (
1
1) (x)=100e− 200x
×
1
(0,0.04]
(x), h (
2
1) (x)=30
×
1
(0,0.02]
(x),
h( 1 2)(x)= 0.008 1 √2π e− ( 2 x ∗ − 0 0 .0 .0 0 2 4 ) 2 2 × 1 (0,0.04] (x), h( 2 2)(x)=0.
In this case, each neuron depends on the other one as well. The spectral radius of the
matrix Γ is 0.711.
Experiment 3 (M =8 and piecewise constant functions).
h(1)=h(1)=h(2)=h(3)=h(3)=h(5)=h(6)=h(7)=h(8)=25 1
2 3 2 1 2 8 5 6 7 × (0,0.02]
andall theother 55 interaction functions areequal to0. Notein particular that this leads
to 3 independent groups of dependent neurons 1,2,3 , 4 and 5,6,7,8 . The spectral
{ } { } { }
radius of the matrix Γ is 0.5.
28 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Figure 1. Raster plots of two data sets with T =2 corresponding to Experiment 2 on the left
and Experiment 3 on the right. The x-axis correspond to the time of the experiment. Each
line with ordinate m corresponds to the points of the process N(m). From bottom to top, we
observe 124 and 103 points for Experiment 2 and 101, 60, 117, 38, 73, 75, 86 and 86 points for
Experiment 3.
We also lead one experiment in the pure inhibition case where all the interaction
functions are nonpositive:
Experiment 4 (M =2). In this experiment, the interaction functions are the opposite
of the functions introduced in Experiment 2. We take for any m 1,...,M , ν(m)=60
∈{ }
so that ψ (f ) is positive with high probability.
t 0
Foreachsimulation,welettheprocess“warmup”during1secondtoreachthestation-
ary state.1 Then the data are collected by taking recordings during the next T seconds.
For instance, we record about 100 points per neuron when T =2 and 1000 points when
T =20. Figure 1 shows two instances of data sets with T =2.
6.2. Description of the methods
To avoid approximation errors when computing the matrix G, we focus on a dictio-
nary (Υ ) whose functions are piecewise constant. More precisely, we take Υ =
k k=1,...,K k
δ 1/21 with δ=0.04/K and K, the size of the dictionary, is chosen later.
− ((k 1)δ,kδ]
−
Ourpracticalprocedurestronglyreliesonthetheoreticalonebasedonthed ’sdefined
ϕ
in (2.8), with x, µ and ε to be specified. First, using Corollary 4, we naturally take
1Notethatsincethesizeofthesupportoftheinteractionfunctionsislessorequalto0.04,the“warm
up”periodis25timestheinteractionrange.
Lasso and probabilistic inequalities for multivariate point processes 29
x=αlog(T). Then, three hyperparameters would need to be tuned, namely α, µ and ε,
ifwedirectlyusedtheLassoestimateofTheorem2.So,forsimplifications,weimplement
our procedure by replacing the Lasso parameters d with
ϕ
γlog(T)
d˜ (γ)= 2γlog(T)(ψ(ϕ)) 2 N + sup ψ(m)(ϕ),
ϕ • T 3 | t |
q t ∈ [0,T],m
where γ isaconstantto be tuned.Besidestaking x=αlog(T),ourmodificationconsists
in neglecting the linear part B ϕ 2x in Vˆµ and replacing B with sup ψ (m) (ϕ).
µ − φ(µ) ϕ t ∈ [0,T],m| t |
Then, note that, up to these modifications, the choice γ =1 corresponds to the limit
case where α 1, ε 0 and µ 0 in the definition of the d ’s (see the comments after
ϕ
→ → →
Theorem 2). Note also that, under the slight abuse consisting in identifying B with
ϕ
sup ψ(m)(ϕ), for every parameter µ, ε and α of Theorem 2 with x=αln(T),
one t ∈ca [0 n ,T fi ], n m d | tw t o par | ameters γ and γ such that
′
d˜
ϕ
(γ) d
ϕ
d˜
ϕ
(γ′).
≤ ≤
Therefore,thispracticalchoiceisconsistentwiththetheoryandtuninghyperparameters
reducestoonlytuningγ.Oursimulationstudywillprovidesoundanswerstothequestion
of tuning γ.
WecomputetheLassoestimatebyusingtheshootingmethodof[28]andtheR-package
Lassoshooting. In particular, we need to invert the matrix G. In all simulations, this
matrix was invertible, which is consistent with the fact that Ω happens with large
c
probability. Note also that the value of c, namely the smallest eigenvalue of G, can be
verysmall(about10 4)whereasthelargesteigenvalueispotentiallyaslargeas105,both
−
values highly depending on the simulation and on T. Fortunately, those values are not
needed to compute our Lasso estimate. Since it is based on Bernstein type inequalities,
our Lasso method is denoted B in the sequel.
Due to their soft thresholding nature, Lasso methods are known to underestimate the
coefficients[43,64].Toovercomebiasesinestimationduetoshrinkage,weproposeatwo
steps procedure, as usually suggested in the literature: Once the support of the vector
has been estimated by B, we compute the ordinary least-square estimator among the
vectors a having the same support, which provides the final estimate. This method is
denoted BO in the sequel.
Another popular method is adaptive Lasso proposed by Zou [64]. This method over-
comes the flaws of standard Lasso by taking ℓ -weights of the form
1
γ
da(γ)= ,
ϕ 2aˆo p
| ϕ|
where p>0, γ >0 and aˆo is a preliminary consistent estimate of the true coefficient.
ϕ
Eveniftheshapesoftheweightsaredifferent,thelatteraredata-drivenandthismethod
constitutes a natural competitive method with ours. The most usual choice, which is
adoptedinthesequel,consistsintakingp=1andtheordinaryleastsquaresestimatefor
30 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
the preliminary estimate (see [35, 60, 64]). Then, penalization is strongerfor coefficients
that are preliminary estimated by small values of the ordinary least square estimate. In
the literature, the parameter γ of adaptive Lasso is usually tuned by cross-validation,
but this does not make sense for Hawkes data that are fully dependent. Therefore, a
preliminary study has been performed to provide meaningful values for γ. Results are
given in the next section. This adaptive Lasso method is denoted A in the sequel and
AO when combined with ordinary least squares in the same way as for BO.
SimulationsareperformedinR.Thecomputationaltimeissmall(merelyafewseconds
for one estimate even when M =8, T =20 and K=8 on a classical laptop computer),
which constitutes a clear improvement with respect to existing adaptive methods for
Hawkesprocesses.Forinstance,the“Islands”method2 of[52]islimitedtotheestimation
ofoneortwodozensofcoefficientsatmost,becauseofanextremecomputationalmemory
cost whereas here when M =8 and K =8, we can easily deal with M +KM2=520
coefficients.
6.3. Results
First, we provide in Figure 2 reconstructions by using the OLS estimate on the whole
dictionary,whichcorrespondsto the casewhereallthe weightsd arenull.Asexpected,
ϕ
reconstructionsarenotsparseandalsobadduetoasmallsignaltonoiseratio(remember
that T =2).
Now let us consider methods leading to sparsity.A precise study over100 simulations
has been carried out corresponding to Experiments 1 and 3 for which we can precisely
checkifthesupportofthevectoraˆ isthe correctone.Foreachmethod,wehaveselected
3 values for the hyperparameter γ based on results of preliminary simulations. Before
studying mean squared errors, we investigate the following problems that are stated in
order of importance. We wonder whether our procedure can identify:
- the dependency groups. Recall that two neurons belong to the same group if and
only if they are connected directly or through the intermediary of one or several
neurons.This issue isessentialfromthe neurobiologicalpointofview since knowing
interactions between two neurons is of capital importance.
- the nonzero interaction functions h (m) ’s and nonzero spontaneous rates ν(m)’s. For
ℓ
ℓ,m 1,...,M , the neuron ℓ has a significative direct interaction on neuron m if
∈{ }
and only if h(m)=0;
ℓ 6
- the nonzero coefficients of nonzero interaction functions. This issue is more math-
ematical. However, it may provide information about the maximal range for direct
interactions between two given neurons or about the favored delay of interaction.
Note that the dependency groups are the only features that can be detected by classical
analysis tools of neuroscience, such as the Unitary Events method [31]. In particular,
2This method developed for M=1 couldeasilybetheoretically adapted forlargervalues of M,but
itsextremecomputational costpreventsusfromusingitinpractice.
Lasso and probabilistic inequalities for multivariate point processes 31
Figure 2. Reconstructions corresponding to Experiment 2 with the OLS estimate with T =2
and K=8. Each line m represents the function
h(m),
for ℓ=1,2. The spontaneous rates asso-
ℓ
ciatedwith each linem aregivenabovethegraphswhereS ∗ denotesthetruespontaneousrate
anditsestimatorisdenotedbySO.Thetrueinteractionsfunctionsareplottedinblackwhereas
theOLS estimates are plotted in magenta.
to the best of our knowledge, identification of the nonzero interaction functions inside a
dependency group is a problem that has not been solved yet as far as we know.
Results for our method and for adaptive Lasso can be found in Table 1. This prelim-
inary study also provides answers for tuning issues. The line “DG” gives the number of
correct identifications of dependency groups. For instance, for M =8, “DG” gives the
number of simulations for which the 3 dependency groups 1,2,3 , 4 and 5,6,7,8
{ } { } { }
arerecoveredby the methods. When M =2,both methods correctlyfindthat neurons1
and2aredependent,evenifT =2.When8neuronsareconsidered,theestimatesshould
find3dependencygroups.We seethatevenwithT =2,ourmethodwithγ=1correctly
guesses the dependency groups for 32% of the simulations. It’s close or equal to 100%
whenT =20 with γ=1 or γ=2.The adaptiveLassohastotake γ=1000for T =2 and
T =20 to obtain as convincing results. Clearly, smaller choices of γ for adaptive Lasso
leadstobadestimationsofthedependency groups.Next,letusfocusonthe detectionof
nonzero spontaneous rates. Whatever the experiment and the parameter γ, our method
is optimal whereas adaptive Lasso misses some nonzero spontaneous rates when T =2.
32
N.R.
Hansen,
P.
Reynaud-Bouret
and
V.
Rivoirard
Table 1. Numerical results of both procedures over 100 runs with K =4. Results for Experiment 1 (top) and Experiment 3
(bottom)aregivenforT =2(left)and T =20(right).“DG”givesthenumberofcorrectidentificationsofdependencygroupsover
100 runs. “S” gives the median numberof nonzero spontaneous rate estimates, “*” means that all the spontaneous rate estimates
are nonzero over all the simulations. “F+” gives the median number of additional nonzero interaction functions w.r.t. the truth.
“F−” gives the median number of missing nonzero interaction functions w.r.t. the truth. “Coeff+” and “Coeff−” are defined in
thesame way forthecoefficients. “SpontMSE”istheMean SquareErrorfor thespontaneousrates with orwithout theadditional
“ordinary least squares step”. “InterMSE” is theanalog for theinteraction functions. In bold, we give theoptimal values
M=2, T =2 OurLasso method AdaptiveLasso M=2, T =20 OurLasso method AdaptiveLasso
γ 0.5 1 2 2 200 1000 γ 0.5 1 2 2 200 1000
DG 100 100 98 100 100 98 DG 100 100 100 100 100 100
S ∗ ∗ ∗ 2 2 1 S ∗ ∗ ∗ ∗ ∗ ∗
F+ 0 0 0 1 0 0 F+ 0 0 0 1 0 0
F− 0 0 0 0 0 0 F− 0 0 0 0 0 0
Coeff+ 2 1 0 11 2 0 Coeff+ 1 0 0 11 2 0
Coeff− 0 0 0 0 0 0 Coeff− 0 0 0 0 0 0
SpontMSE 108 140 214 150 193 564 SpontMSE 22 37 69 14 12 27
+ols 104 96 95 151 154 516 +ols 11 10 9 14 12 10
InterMSE 7 9 15 13 8 11 InterMSE 2 3 6 1.4 0.6 0.5
+ols 7 7 7 14 10 10 +ols 0.6 0.5 0.4 1.4 0.9 0.4
M=2, T =2 OurLasso method AdaptiveLasso M=2, T =20 OurLasso method AdaptiveLasso
γ 0.5 1 2 2 200 1000 γ 0.5 1 2 2 200 1000
DG 0 32 24 0 0 32 DG 63 99 100 0 0 90
S ∗ ∗ ∗ 8 7 5 S ∗ ∗ ∗ ∗ ∗ ∗
F+ 17 6 1 55 13 0.5 F+ 3 1 0 55 10 0
F− 0 0 2 0 0 2 F− 0 0 0 5.5d0 0 0
Coeff+ 22 7 1 199.5 17 1 Coeff+ 4 1 0 197 13 0
Coeff− 0.5 2 7 0 2 7 Coeff− 0 0 0 0 0 0
SpontMSE 295 428 768 1445 1026 1835 SpontMSE 82 166 355 104 43 64
+ols 1327 587 859 1512 1058 1935 +ols 41 26 24 107 74 26
InterMSE 38 51 79 214 49 65 InterMSE 10 19 39 16 2.9 3.17
+ols 63 45 61 228 84 70 +ols 3 2.1 1.9 17 6.3 2
Lasso and probabilistic inequalities for multivariate point processes 33
Under this criterion, for adaptive Lasso, the choice γ=1000 is clearly bad when T =2
(the optimal value of S is S=2 when M =2 and S =8 when M =8) on both exper-
iments, whereas γ =2 or γ =200 is better. Not surprisingly, the number of additional
nonzero functions and additional nonzero coefficients decreaseswhen T growsand when
γ grows, whatever the method whereas the number of missing functions or coefficients
increases. We can conclude from these facts and from further analysis of Table 1 that
the choice γ=0.5 for our method and the choice γ=2 for the adaptive Lassoare wrong
choices of the tuning parameters. In conclusion of this preliminary study, our method
with γ=1 or γ=2 seems a good choice and is robust with respect to T. When T =20,
the optimalchoice foradaptive Lassois γ=1000.When T =2,the choiceis notso clear
and depends on the criterion we wish to favor.
Now let us look at mean squared errors (MSE). Since the spontaneous rates do not
behaveliketheothercoefficients,wesplittheMSEintwoparts:oneforthespontaneous
rates:
M
SpontMSE= (νˆ(m) ν(m)) 2 ,
−
m=1
X
and one for interactions:
M M
InterMSE= (hˆ(m)(t) h(m)(t)) 2 dt.
ℓ − ℓ
m=1ℓ=1Z
XX
WestillreporttheresultsforB,BO,AandAOinTable1.Ourcommentsmostlyfocus
oncaseswheretheresultsforthepreviousstudyaregood.First,notethatresultsonsuch
cases are better by using the second step (OLS). Furthermore, MSE is increasing with
γ for B and A, since underestimation is stronger when γ increases. This phenomenon
doesnotappearfortwostepprocedures,whichleadstoamorestableMSE.Foradaptive
Lasso, when T =2, the choice γ=200 leads to good MSE, but the MSE are smaller for
BOwithγ=1.WhenT =20,thechoiceγ=1000forAOleadstoresultsthatareofthe
samemagnitudeasthe onesobtainedby BOwith γ=1 or2.Still for T =20,resultsfor
the estimate B areworse thanresults for A. It is due to the fact that shrinkageis larger
inourmethod for the coefficients we wantto keepthanshrinkageofadaptiveLassothat
becomes negligible as soonas the true coefficients are large enough. However the second
step overcomes this problem.
Note also that a more thorough study of the tuning parameter γ has been performed
by [5] where it is mathematically proved that the choice γ <1 leads to very degener-
ate estimates in the density setting. Their method for choosing Lasso parameters being
analogous to ours, it seems coherent to obtain worse MSE for γ =0.5 than for γ =1
or γ=2, at least for BO. The boundary γ=1 in their simulation study seems to be a
robust choice there, and it seems to be the case here too.
We now provide some reconstructions by using Lasso methods. Figures 3 and 4 give
the reconstructions corresponding to Experiment 2 (M =2) with K =8 for T =2 and
34 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Figure 3. Reconstructions corresponding to Experiment 2 with T =2 and K=8. Each line
m represents the function
h(m),
for ℓ=1,2. The spontaneous rates estimation associated with
ℓ
each line m is given above thegraphs: S ∗ denotes thetrue spontaneous rate and its estimators
computedbyusingB,BOandArespectivelyaredenotedbySB,SBOandSA.Thetrueinter-
actionsfunctions(inblack)arereconstructedbyusingB,BOandAprovidingreconstructions
in green, red and blue respectively. Weuse γ=1 for B and BO and γ=200 for A.
T =20, respectively. The reconstructions are quite satisfying. Of course, the quality
improves when T grows. We also note improvements by using BO instead of B. For
adaptive Lasso, improvements by using the second step are not significative and this
is the reason why we do not represent reconstructions with AO. Graphs of the right-
hand side of Figure 3 illustrate the difficulties of adaptive Lasso to recover the exact
support of interactions functions, namely h(1) and h(2) for T =2. Figure 5 provides
2 2
another illustration in the case of Experiment 3 (M =8) with K =8 for T =20. For
the sake of clarity, we only represent reconstructions for the first 4 neurons. From the
estimationpointofview,thisillustrationprovidesaclearhierarchybetweenthemethods:
BOseemsto achievethe bestresultsandBthe worst.Finally,Figure6showsthateven
in the inhibition case, we are able to recover the negative interactions.
Lasso and probabilistic inequalities for multivariate point processes 35
Figure 4. Reconstructions corresponding to Experiment 2 with T =20 and K=8. Same con-
vention as in Figure 3. Weuse γ=1 for B and BO and γ=1000 for A.
6.4. Conclusions
WithrespecttotheproblemoftuningourmethodologybasedonBernsteintypeinequal-
ities, our simulation study is coherentwith theoretical aspects since we achieve our best
results by taking γ=1, which constitutes the limit case of assumptions of Theorem 2.
For practical aspects, we recommend the choice γ=1 even if γ=2 is acceptable. More
importantly,this choiceisrobustwithrespecttothedurationofrecordings,whichisnot
the case for adaptive Lasso. Implemented with γ=1, our method outperforms adaptive
Lassoanditisabletorecoverthedependencygroups,thenonzerospontaneousrates,the
nonzero functions and even the nonzero coefficients as soon as T is large enough. Most
of the time, the two step procedure BO seems to achieve the best results for parameter
estimation.
Itisimportanttonotethatthe questionoftuning adaptiveLassoremainsopen.Some
values of γ allow us to obtain very good results but they are not robust with respect to
T, which may constitute a serious problem for practitioners. In the standard regression
setting, this problem may be overcome by using cross-validation on independent data,
which somehow estimates random fluctuations. But in this multivariate Hawkes setup,
36
N.R.
Hansen,
P.
Reynaud-Bouret
and
V.
Rivoirard
Figure 5. Reconstructions corresponding to Experiment 3 with T =20 and K =8 and for the first 4 neurons. Each line m
represents thefunction h(m), for ℓ=1,2,3,4. Same convention as in Figure 3.We use γ=1 for B and BO and γ=1000 for A.
ℓ
Lasso and probabilistic inequalities for multivariate point processes 37
Figure 6. Reconstructions corresponding to Experiment 4 with T =20 and K=8. Same con-
ventionsas in Figure 3.We use γ=1 for B and BO and γ=1000 for A.
independence assumptions on data cannot be made and this explains the problems for
tuning adaptive Lasso. Our method based on Bernstein type concentration inequalities
takes into account those fluctuations. It also takes into account the nature of the coeffi-
cientsandthe variabilityoftheir estimates whichdiffer forspontaneousratesonthe one
hand and coefficients of interaction functions on the other hand. The shape of weights
of adaptive Lasso does not incorporatethis difference, which explains the contradictions
for tuning the method when T =2. For instance, in some cases, adaptive Lasso tends to
estimate some spontaneous rates to zero in order to achieve better performance on the
interaction functions.
7. Proofs
This section is devoted to the proofs of the results of the paper. Throughout, C is a
constant whose value may change from line to line.
38 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
7.1. Proof of Theorem 1
The proof of Theorem1 is standard (see for instance [15]), but for the sake of complete-
ness, we give it. We use for the Euclidian norm of RΦ. Given a recall that
k·k
ℓ2
f = a ϕ.
a ϕ
ϕ Φ
X∈
Then, we have fˆ=f ,
aˆ
ab=ψ(f ) N
′ a T
•
and
aGa= f 2 = ψ(f ) 2 .
′ k a kT k a kproc
Then,
2ψ(f ) N + f 2 +2d aˆ 2ψ(f ) N + f 2 +2d a.
− aˆ • T k aˆ kT ′ | |≤− a • T k a kT ′ | |
So,
ψ(f ) λ 2 = ψ(f ) 2 + λ 2 2 ψ(f ),λ
k aˆ − kproc k aˆ kproc k kproc− h aˆ iproc
ψ(f ) 2 + λ 2 +2ψ(f f ) N
≤k a kproc k kproc aˆ − a • T
+2d′(
|
a
|−|
aˆ
|
)
−
2
h
ψ(f
aˆ
),λ
iproc
2
= ψ(f ) λ +2 ψ(f f ),λ
k a − kproc h a − aˆ iproc
+2ψ(f f ) N +2d(a aˆ)
aˆ a T ′
− • | |−| |
2
= ψ(f ) λ +2ψ(f f ) (Λ N) +2d(a aˆ)
k a − kproc a − aˆ • − T ′ | |−| |
2
= ψ(f ) λ +2 (a aˆ )ψ(ϕ) (Λ N) +2d(a aˆ)
k a − kproc ϕ − ϕ • − T ′ | |−| |
ϕ Φ
X∈
ψ(f ) λ 2 +2 a aˆ ¯b b +2d(a aˆ).
≤k a − kproc | ϕ − ϕ |×| ϕ − ϕ | ′ | |−| |
ϕ Φ
X∈
Using (2.5), we obtain:
2 2
ψ(f ) λ ψ(f ) λ +2 d a aˆ +2 d (a aˆ )
k aˆ − kproc≤k a − kproc ϕ | ϕ − ϕ | ϕ | ϕ |−| ϕ |
ϕ Φ ϕ Φ
X∈ X∈
2
ψ(f ) λ +2 d (a aˆ + a aˆ ).
≤k a − kproc ϕ | ϕ − ϕ | | ϕ |−| ϕ |
ϕ Φ
X∈
Lasso and probabilistic inequalities for multivariate point processes 39
Now, if ϕ /S(a), a aˆ + a aˆ =0, and
ϕ ϕ ϕ ϕ
∈ | − | | |−| |
2 2
ψ(f ) λ ψ(f ) λ +2 d (a aˆ + a aˆ )
k aˆ − kproc≤k a − kproc ϕ | ϕ − ϕ | | ϕ |−| ϕ |
ϕ ∈XS(a)
2
ψ(f ) λ +4 d (a aˆ )
≤k a − kproc ϕ | ϕ − ϕ |
ϕ ∈XS(a)
1/2
ψ(f ) λ 2 +4 aˆ a d2 .
≤k a − kproc k − k ℓ2 ϕ
(cid:18)ϕ ∈XS(a) (cid:19)
We now use the assumption on the Gram matrix given by (2.4) and the triangular
inequality for , which yields
T
k·k
aˆ a 2 c 1(aˆ a)G(aˆ a)
k − kℓ2 ≤ − − ′ −
=c 1 f f 2
− k aˆ − a kT
2c 1( ψ(f ) λ 2 + ψ(f ) λ 2 ).
≤ − k aˆ − kproc k a − kproc
Letus take α (0,1).Since for any x R andany y R, 2xy αx2+α 1y2,we obtain:
−
∈ ∈ ∈ ≤
ψ(f ) λ 2 ψ(f ) λ 2
k aˆ − kproc≤k a − kproc
1/2
+4√2c 1/2 ψ(f ) λ 2 + ψ(f ) λ 2 d2
− k aˆ − kproc k a − kproc ϕ
q (cid:18)ϕ ∈XS(a) (cid:19)
ψ(f ) λ 2
≤k a − kproc
+α( ψ(f ) λ 2 + ψ(f ) λ 2 )+8α 1c 1 d2
k aˆ − kproc k a − kproc − − ϕ
ϕ ∈XS(a)
(1 α) 1 (1+α) ψ(f ) λ 2 +8α 1c 1 d2 .
≤ − − k a − kproc − − ϕ
(cid:18) ϕ S(a) (cid:19)
∈X
The theorem is proved just by taking an arbitrary absolute value for α (0,1).
∈
7.2. Proof of Theorem 2
Let us first define
= t 0 supψ(m)(ϕ) >B . (7.1)
T ≥ | t | ϕ
m
n . o
Let us define the stopping time τ =inf and the predictable process H by
′
T
H
t
(m)=ψ
t
(m)(ϕ)1
t
τ′.
≤
40 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let us apply Theorem 3 to this choice of H with τ =T and B=B . The choice of v
ϕ
and w will be givenlater on. To apply this result, we need to check that for all t and all
ξ ∈ (0,3), m 0 t eξH s (m)/Bϕλ ( s m) ds is a.s. finite. But if t>τ ′ , then
P R t τ′ t
eξH
s
(m)/Bϕλ(m)ds= eξH
s
(m)/Bϕλ(m)ds+ λ(m)ds,
s s s
Z0 Z0 Zτ′
where the second part is obviously finite (it is just Λ(m) Λ(m)). Hence, it remains to
t − τ′
prove that for all t τ ,
′
≤
t
eξH
s
(m)/Bϕλ(m)ds
s
Z0
is finite. But for all s<t, s<τ and consequently s / . Therefore, H(m) B . Since
′ s ϕ
∈T | |≤
weareintegratingwithrespectto theLebesguemeasure,the factthatiteventuallydoes
not hold in t is not a problem and
t
eξH
s
(m)/Bϕλ(m)ds eξΛ(m),
s ≤ t
Z0
which is obviously finite a.s. The same reasoning can be applied to show that a.s.
exp(ξH2/B2) Λ < . We can also apply Theorem 3 to H in the same way. We
t
• ∞ −
obtain at the end that for all ε>0
B x
P H (N Λ) 2(1+ε)Vˆµx+ ϕ and w Vˆµ v and sup H(m) B
| • − T |≥ 3 ≤ ≤ | t |≤ ϕ
(cid:18) q m,t ≤ T (cid:19) (7.2)
log(v/w)
4 +1 e x.
−
≤ log(1+ε)
(cid:18) (cid:19)
But on Ω it is clear that t [0,T],t / . Therefore, τ T. Therefore for all t T,
V,B ′
∀ ∈ ∈T ≥ ≤
one also has t τ and H(m)=ψ(m)(ϕ). Consequently, on Ω ,
≤ ′ t t V,B
H (N Λ) =b ¯b and Vˆµ=Vˆµ.
• − T ϕ − ϕ ϕ
Moreover,on Ω , one has that
V,B
B2x µ B2x
ϕ Vˆµ V + ϕ .
µ φ(µ) ≤ ϕ ≤ µ φ(µ) ϕ µ φ(µ)
− − −
So,wetakewandvasrespectivelytheleft-andright-handsideofthepreviousinequality.
Finally note that on Ω ,
V,B
sup H(m) = sup ψ(m)(ϕ) B .
| t | | t |≤ ϕ
m,t T m,t T
≤ ≤
Lasso and probabilistic inequalities for multivariate point processes 41
Hence, we can rewrite (7.2) as follows
P b ¯b 2(1+ε)Vˆµx+ B ϕ x and Ω 4 log(1+µV ϕ /B ϕ 2x) +1 e x.(7.3)
ϕ ϕ ϕ V,B −
| − |≥ 3 ≤ log(1+ε)
(cid:18) q (cid:19) (cid:18) (cid:19)
Apply this to all ϕ Φ, we obtain that
∈
log(1+µV /B2x)
P( ϕ Φ s.t. b ¯b d and Ω ) 4 ϕ ϕ +1 e x.
ϕ ϕ ϕ V,B −
∃ ∈ | − |≥ ≤ log(1+ε)
ϕ Φ(cid:18) (cid:19)
X∈
Now on the event Ω Ω ϕ Φ, b ¯b d , one can apply Theorem 1. To
c V,B ϕ ϕ ϕ
∩ ∩{∀ ∈ | − |≤ }
obtain Theorem 2, it remains to bound the probability of the complementary event by
P(Ωc)+P(Ωc )+P( ϕ Φ s.t. b ¯b d and Ω ).
c V,B ∃ ∈ | ϕ − ϕ |≥ ϕ V,B
7.3. Proof of Theorem 3
First, replacing H with H/B, we can always assume that B = 1. Next, let us
fix for the moment ξ (0,3). If one assumes that almost surely for all t > 0,
∈
M m=1 0 t eξH s (m)λ( s m)ds< ∞ (i.e., that the process eξH • Λ is well defined) then one
can apply Theorem 2 of [8], page 165, stating that the process (E ) defined for all t
t t 0
P R ≥
by
E =exp(ξH (N Λ) φ(ξH) Λ )
t t t
• − − •
is a supermartingale. It is also the case for E if τ is a bounded stopping time. Hence
t τ
∧
for any ξ (0,3) and for any x>0, one has that
∈
P(E >ex) e xE(E ) e x,
t τ − t τ −
∧ ≤ ∧ ≤
which means that
P(ξH (N Λ) φ(ξH) Λ >x) e x.
t τ t τ −
• − ∧ − • ∧ ≤
Therefore,
P ξH
•
(N
−
Λ)
t ∧ τ −
φ(ξH)
•
Λ
t ∧ τ
>x and
s
su
τ
p
,m |
H
s
(m)
|≤
1
≤
e− x.
(cid:16) ≤ (cid:17)
But if sup H(m) 1, then for any ξ>0 and any s,
s ≤ τ,m| s |≤
φ(ξH(m)) (H(m)) 2 φ(ξ).
s ≤ s
So, for every ξ (0,3), we obtain:
∈
P M ξ 1φ(ξ)H2 Λ +ξ 1x and sup H(m) 1 e x. (7.4)
τ ≥ − • τ − | s |≤ ≤ −
s τ,m
(cid:16) ≤ (cid:17)
42 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Now let us focus on the event H2 Λ v where v is a deterministic quantity. We have
τ
• ≤
that consequently
P M ξ 1φ(ξ)v+ξ 1x and H2 Λ v and sup H(m) 1 e x.
τ ≥ − − • τ ≤ | s |≤ ≤ −
s τ,m
(cid:16) ≤ (cid:17)
Itremains to choose ξ suchthat ξ 1φ(ξ)v+ξ 1x is minimal. But this expressionhas no
− −
simple form. However, since 0<ξ<3, one can bound φ(ξ) by ξ2(1 ξ/3) 1/2. Hence,
−
−
we can start with
ξ
P M
τ ≥ 2(1 ξ/3)
H2
•
Λ
τ
+ξ− 1x and sup
|
H
s
(m)
|≤
1
≤
e− x (7.5)
(cid:18) − s ≤ τ,m (cid:19)
and also
ξ
P M v+ξ 1x and H2 Λ v and sup H(m) 1 e x. (7.6)
τ ≥ 2(1 ξ/3) − • τ ≤ | s |≤ ≤ −
(cid:18) − s ≤ τ,m (cid:19)
It remains now to minimize ξ ξ v+ξ 1x.
7−→ 2(1 ξ/3) −
−
Lemma 2. Let a, b and x be positive constants and let us consider on (0,1/b),
aξ x
g(ξ)= + .
(1 bξ) ξ
−
Then min ξ (0,1/b) g(ξ)=2√ax+bx and the minimum is achieved in ξ(a,b,x)= x x b − b2 √a a x.
∈ −
Proof. The limits of g in 0+ and (1/b) are + . The derivative is given by
−
∞
a x
g (ξ)=
′
(1 bξ)2 − ξ2
−
whichisnullinξ(a,b,x)(remarkthattheothersolutionofthepolynomialdoesnotliein
(0,1/b)). Finally, it remains to evaluate the quantity in ξ(a,b,x) to obtain the result. (cid:3)
Now, we apply (7.6) with ξ(v/2,1/3,x) and we obtain this well known formula which
can be found in [57] for instance,
P M √2vx+x/3 and H2 Λ v and sup H(m) 1 e x. (7.7)
τ ≥ • τ ≤ | s |≤ ≤ −
s τ,m
(cid:16) ≤ (cid:17)
Now we would like first to replace v by its random version H2 Λ . Let w,v be some
τ
•
positive constants and let us concentrate on the event
w H2 Λ v. (7.8)
τ
≤ • ≤
Lasso and probabilistic inequalities for multivariate point processes 43
For all ε>0 we introduce K a positive integer depending on ε, v and w such that
(1+ε)Kw v. Note that K= log(v/w)/log(1+ε) is a possible choice. Let us denote
≥ ⌈ ⌉
v =w, v =(1+ε)w,...,v =(1+ε)Kw.Forany0<ξ<3andanyk in 0,...,K 1 ,
0 1 K
{ − }
one has, by applying (7.5),
ξ
P M H2 Λ +ξ 1x
τ τ −
≥ 2(1 ξ/3) •
(cid:18) −
and v H2 Λ v and sup H(m) 1 e x.
k ≤ • τ ≤ k+1 | s |≤ ≤ −
s ≤ τ,m (cid:19)
This implies that
ξ
P M v +ξ 1x and v H2 Λ v and sup H(m) 1 e x.
τ ≥ 2(1 ξ/3) k+1 − k ≤ • τ ≤ k+1 | s |≤ ≤ −
(cid:18) − s ≤ τ,m (cid:19)
Using Lemma 2, with ξ=ξ(v /2,1/3,x), this gives
k+1
P M 2v x+x/3 and v H2 Λ v and sup H(m) 1 e x.
τ ≥ k+1 k ≤ • τ ≤ k+1 | s |≤ ≤ −
s τ,m
(cid:16) p ≤ (cid:17)
But if v H2 Λ , v (1+ε)v (1+ε)H2 Λ , so
k τ k+1 k τ
≤ • ≤ ≤ •
P M 2(1+ε)(H2 Λ )x+x/3
τ τ
≥ •
(cid:16) q
and v
k ≤
H2
•
Λ
τ ≤
v
k+1
and sup
|
H
s
(m)
|≤
1
≤
e− x.
s τ,m
≤ (cid:17)
Finally summing on k, this gives
P M 2(1+ε)(H2 Λ )x+x/3
τ τ
≥ •
(cid:16) q (7.9)
and w
≤
H2
•
Λ
τ ≤
v and sup
|
H
s
(m)
|≤
1
≤
Ke− x.
s τ,m
≤ (cid:17)
This leads to the following result that has interest per se.
Proposition 6. Let N =(N(m)) be a multivariate counting process with pre-
m=1,...,M
dictable intensities λ(m) and corresponding compensator Λ(m) with respect to some given
t t
filtration. Let B>0. Let H=(H(m)) be a multivariate predictable process such
m=1,...,M
that for all ξ (0,3), eξH/B Λ < a.s. for all t. Let us consider the martingale defined
t
∈ • ∞
for all t by
M =H (N Λ) .
t t
• −
44 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Letv>w bepositiveconstantsandletτ beaboundedstoppingtime.Thenforanyε,x>0
Bx
P M 2(1+ε)(H2 Λ )x+ and w H2 Λ v and sup H (m) B
τ ≥ • τ 3 ≤ • τ ≤ | t |≤
(cid:18) q m,t ≤ τ (cid:19) (7.10)
log(v/w)
+1 e− x.
≤ log(1+ε)
(cid:18) (cid:19)
Next, we would like to replace H2 Λ , the quadratic characteristic of M, with its
τ
•
estimator H2 N , that is, the quadratic variation of M. For this purpose, let us con-
τ
•
sider W = H2 (N Λ) which is still a martingale since the (H(m))2’s are still
t t s
− • − −
predictable processes. We apply (7.4) with µ instead of ξ, noticing that on the event
sup H(m) 1 , one has that H4 Λ H2 Λ . This gives that
{ s ≤ τ,m| s |≤ } • τ ≤ • τ
P H2 Λ H2 N + φ(µ)/µ H2 Λ +x/µ and sup H(m) 1 e x,
• τ ≥ • τ { } • τ | s |≤ ≤ −
s τ,m
(cid:16) ≤ (cid:17)
which means that
P H2
•
Λ
τ ≥
Vˆµ and sup
|
H
s
(m)
|≤
1
≤
e− x. (7.11)
s τ,m
(cid:16) ≤ (cid:17)
So we use again (7.5) combined with (7.11) to obtain that for all ξ (0,3)
∈
ξ
P M Vˆµ+ξ 1x and sup H(m) 1
τ ≥ 2(1 ξ/3) − | s |≤
(cid:18) − s ≤ τ,m (cid:19)
ξ
≤
P M
τ ≥ 2(1 ξ/3)
Vˆµ+ξ− 1x and sup
|
H
s
(m)
|≤
1 and H2
•
Λ
τ ≤
Vˆµ
(cid:18) − s ≤ τ,m (cid:19)
+P H2 Λ Vˆµ and sup H(m) 1 2e x.
• τ ≥ | s |≤ ≤ −
s τ,m
(cid:16) ≤ (cid:17)
Thisnewinequalityreplaces(7.5)anditremainstoreplaceH2 Λ byVˆµ inthepeeling
τ
•
arguments to obtain as before that
P M 2(1+ε)Vˆµx+x/3 and w Vˆµ v and sup H(m) 1 2Ke x. (7.12)
τ ≥ ≤ ≤ | s |≤ ≤ −
s τ,m
(cid:16) q ≤ (cid:17)
7.4. Proofs of the probabilistic results for Hawkes processes
7.4.1. Proof of Lemma 1
Let K(n) denote the vector of the number of descendants in the nth generation from a
single ancestralpoint of type ℓ, define K(0)=e and let W(n)= n K(k) denote the
ℓ k=0
total number of points in the first n generations. Define for θ RM
∈ P
φ (θ)=logE eθTK(1).
ℓ ℓ
Lasso and probabilistic inequalities for multivariate point processes 45
Thus, φ (θ) is the log-Laplace transform of the distribution of K(1) given that there
ℓ
is a single initial ancestral point of type ℓ. We define the vector φ(θ) by φ(θ) =
′
(φ (θ),...,φ (θ)). Note that φ only depends on the law of the number of children per
1 M
parent, that is, it only depends on Γ. Then
E eθTW(n)=E (eθTW(n 1)E(eθTK(n) K(n 1),...,K(1)))
ℓ ℓ −
| −
=E
ℓ
(eθTW(n
−
1)eφ(θ)TK(n
−
1))
=E e(θ+φ(θ))TK(n 1)+θTW(n 2).
ℓ − −
Defining g(θ)=θ+φ(θ) we arrive by recursion at the formula
E eθTW(n)=E eg◦(n−1)(θ)TK(1)+θTW(0)
ℓ ℓ
=eφ(g◦(n−1)(θ))ℓ+θℓ
=eg◦n(θ)ℓ,
where for any n, g (n)=g g n times. Or, in other words, we have the following
◦
◦···◦
representation
logE eθTW(n)=g n(θ)
ℓ ◦ ℓ
of the log-Laplace transform of W(n).
Below we show that φ is a contraction in a neighborhood containing 0, that is, for
some r>0 and a constant C<1 (and a suitable norm), φ(s) C s for s r. If θ
k k≤ k k k k≤
is chosen such that
θ
k k r
1 C ≤
−
we have θ r, and if we assume that g k(θ) B(0,r) for k=1,...,n 1 then
◦
k k≤ ∈ −
g n(θ) θ + φ(g (n 1)(θ))
◦ ◦ −
k k≤k k k k
θ +C g (n 1)(θ)
◦ −
≤k k k k
θ (1+C+C2+ +Cn)
≤k k ···
r.
≤
Thus, by induction, g n(θ) B(0,r) for all n 1. Since n W (n) is increasing and
◦ m
∈ ≥ 7→
goes to W ( ) for n , with W ( ) the totalnumber of points in a cluster of type
m m
∞ →∞ ∞
m, and since W = W ( )=1TW( ), we have by monotone convergence that for
m m ∞ ∞
ϑ R
∈ P
logE eϑW = lim g n(ϑ1) .
ℓ ◦ ℓ
n
→∞
By the previous result, the right-hand side is bounded if ϑ is sufficiently small. This
| |
completes the proof up to proving that φ is a contraction.
46 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
To this end, we note that φ is continuously differentiable (on RM in fact, but a neigh-
borhood around 0 suffices) with derivative Dφ(0)=Γ at 0. Since the spectral radius of
Γ is strictly less than 1 there is a C<1 and, by the Householder theorem, a norm
k·k
on RM such that for the induced operator norm of Γ we have
Γ = max Γx <C.
k k x: x 1k k
k k≤
Since the norm is continuous and Dφ(s) is likewise there is an r>0 such that
Dφ(s) C<1
k k≤
for s r. This, in turn, implies that φ is Lipschitz continuous in the ball B(0,r) with
k k≤
Lipschitz constant C, and since φ(0)=0 we get
φ(s) C s
k k≤ k k
for s r. This ends the proof of the lemma.
k k≤
Notethatwe havenotatallusedthe explicitformulafor φ above,whichis obtainable
and simple since the offspring distributions are Poisson. The only thing we needed was
thefactthatφisdefinedinaneighborhoodaround0,thusthattheoffspringdistributions
are sufficiently light-tailed.
7.4.2. Proof of Proposition 2
We use the cluster representation, and we note that any cluster with ancestral point in
[ n 1, n] must have at least n+1 A points in the cluster if any of the points
− − − −⌈ ⌉
are to fall in [ A,0). This follows from the assumption that all the h(m)-functions have
− ℓ
supportin[0,1].WithN˜ thenumberofpointsin[ A,0)fromaclusterwithancestral
A,ℓ
−
points of type ℓ, we thus have the bound
An
N˜ max W n+ A ,0 ,
A,ℓ n,k
≤ { − ⌈ ⌉ }
n k=1
XX
where A is the number of ancestral points in [ n 1, n] of type ℓ and W is the
n n,k
− − −
number of points in the respective clusters. Here the A ’s and the W ’s are all inde-
n n,k
pendent, the A ’s are Poisson distributed with mean ν and the W ’s are i.i.d. with
n ℓ n,k
the same distribution as W in Lemma 1. Moreover,
H
n
(ϑ
ℓ
):=E
ℓ
eϑℓmax
{
W
−
n+
⌈
A
⌉
,0
}
P
ℓ
(W n A )+e
−
ϑℓ(n
−⌈
A
⌉
)E
ℓ
eϑℓW,
≤ ≤ −⌈ ⌉
whichis finite for ϑ sufficiently smallaccordingto Lemma 1.Thenwe cancompute an
ℓ
upper bound on th | e L | aplace transform of N˜ :
A,ℓ
An
EeϑℓN˜ A,ℓ E E(eϑℓmax { Wn,k− n+ ⌈ A ⌉ ,0 } A n )
≤ |
n k=1
Y Y
Lasso and probabilistic inequalities for multivariate point processes 47
EH (ϑ )An
n ℓ
≤
n
Y
= eνℓ(Hn(ϑℓ) − 1)
n
Y
=eνℓP
n
(Hn(ϑℓ)
−
1).
Since H
n
(ϑ
ℓ
)
−
1
≤
e
−
ϑℓ(n
−⌈
A
⌉
)E
ℓ
eϑℓW we have
n
(H
n
(ϑ
ℓ
)
−
1)<
∞
, which shows that
the upper bound is finite. To complete the proof, observe that N = N˜ where
N˜ for ℓ=1,...,M are independent. Since all P variables are po [ s − it A iv ,0 e ) , it is ℓ suffi A, c ℓ ient to
A,ℓ P
take θ=min ϑ .
ℓ ℓ
7.4.3. Proof of Proposition 3
In this paragraph, the notation (cid:3) simply denotes a generic positive absolute constant
that may change from line to line. The notation (cid:3) denotes a positive constant
θ1,θ2,...
depending on θ ,θ ,... that may change from line to line.
1 2
Let
u=C σlog3/2(T)√T +C b(log(T)) 2+η , (7.13)
1 2
where the choices of C and C will be given later. For any positive integer k such that
1 2
x:=T/(2k)>A, we have by stationarity:
T k 1 2qx+x
−
P [Z S (N) E(Z)]dt u =P [Z S (N) E(Z)]dt
t t
(cid:18)Z0
◦ − ≥
(cid:19) q=0Z2qx
◦ −
X
2qx+2x
+ [Z S (N) E(Z)]dt u
t
Z2qx+x ◦ − ≥ !
k − 1 2qx+x u
2P [Z S (N) E(Z)]dt .
t
≤ q=0Z2qx ◦ − ≥ 2 !
X
Similarly to [51], we introduce (M˜x) a sequence of independent Hawkes processes,each
q q
being stationary with intensities per mark given by ψ(m). For each q, we then introduce
t
Mx the truncated process associated with M˜x, where truncation means that we only
q q
consider the points lying in [2qx A,2qx+x]. So, if we set
−
2qx+x
F = [Z S (Mx) E(Z)]dt,
q ◦ t q −
Z2qx
(7.14)
T k − 1 u T
P [Z S (N) E(Z)]dt u 2P F +2kP T > A ,
t q e
(cid:18)Z0 ◦ − ≥ (cid:19) ≤ q=0 ≥ 2 ! (cid:18) 2k − (cid:19)
X
48 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
where T represents the time to extinction of the process. More precisely, T is the last
e e
point of the process if in the cluster representation only ancestral points before 0 are
appearing. For more details, see Section 3 of [51]. So, denoting a the ancestral points
l
withmarksl andHl thelengthofthecorrespondingclusterwhoseoriginisa ,wehave:
al l
T = max max a +Hl .
e l 1,...,M al { l al}
∈{ }
But, for any a>0,
M
P(T a)=E E[1 a ]
e ≤ " { al+H a l l≤ a }| l #
Y
l=1
Y
al
M
=E exp(log(P(Hl a a )))
" 0≤ − l #
Y
l=1
Y
al
M 0
=E exp log(P(Hl a x))dN˜(l) ,
"
Y
l=1 (cid:18)Z−∞ 0≤ − x (cid:19)#
where N˜(l) denotes the process associated with the ancestral points with marks l. So,
M 0
P(T a)=exp (exp(log(P(Hl a x))) 1)ν(l)dx
e ≤ 0≤ − − !
X
l=1Z−∞
M +
=exp ν(l) ∞ P(Hl >u)du .
− l=1 Za 0 !
X
Now, by Lemma 1, there exists some ϑ
l
>0, such that c
l
=E
ℓ
(eϑlW)<+ , where W is
∞
the number of points in the cluster. But if all the interaction functions have support in
[0,1], one always have that Hl <W. Hence,
0
P(Hl >u) E[exp(ϑHl)]exp( ϑ u)
0 ≤ l 0 − l
c exp( ϑ u).
l l
≤ −
So,
M +
P(T a) exp ν(l) ∞ c exp( ϑu)du
e l l
≤ ≥ − l=1 Za − !
X
M
=exp ν(l)c /ϑ exp( ϑ a)
l l l
− − !
l=1
X
M
1 ν(l)c /ϑ exp( ϑa).
l l l
≥ − −
l=1
X
Lasso and probabilistic inequalities for multivariate point processes 49
So, there exists a constant C depending on α,A, and f such that if we take k=
α,f0,A 0
C T/log(T) , then
⌊
α,A,f0
⌋
T
kP T
e
> A T− α.
2k − ≤
(cid:18) (cid:19)
In this case x= T log(T) is largerthan A for T largeenough(depending on A,α,f ).
2k ≈ 0
Now, let us focus on the first term B of (7.14), where
k 1
− u
B=P F .
q
≥ 2
!
q=0
X
Let us consider some ˜ where ˜ will be fixed later and let us define the measurable
N N
events
Ω = sup Mx ˜ ,
q t { q| [t − A,t) }≤N
n o
whereMx representsthesetofpointsofMx lyingin[t A,t).Letusalsoconsider
q| [t − A,t) q −
Ω= Ω . Then
1 q k q
≤ ≤
T
B P F u/2 and Ω +P(Ωc).
q
≤ ≥
(cid:18) q (cid:19)
X
We have P(Ωc) P(Ωc). Each Ω can also be easily controlled.Indeed it is sufficient
to split [2qx A ≤ ,2qx q +x] q in interval q s of size A (there are about (cid:3) log(T) of those)
− P
α,A,f0
and require that the number of points in each subinterval is smaller than ˜/2. By
N
stationarity, we obtain that
P(Ωc) (cid:3) log(T)P(N > ˜/2).
q ≤ α,A,f0 [ − A,0) N
Using Proposition 2 with u= ˜/2 +1/2, we obtain:
⌈N ⌉
P(Ωc) (cid:3) log(T)exp( (cid:3) ˜) and
q ≤ α,A,f0 − α,A,f0N (7.15)
P(Ωc) (cid:3) Texp( (cid:3) ˜).
≤
α,A,f0
−
α,A,f0N
Note that this control holds for any positive choice of ˜. Hence, this gives also the
N
following lemma that will be used later.
Lemma 3. For any >0,
R
P(there exists t [0,T] Mx > ) (cid:3) Texp( (cid:3) ).
∈ | q| [t − A,t) R ≤ α,A,f0 − α,A,f0R
Hence by taking ˜ =C log(T) for C large enough this is smaller than (cid:3) T α′ ,
N
3 3 α,A,f0 −
where α =max(α,2).
′
50 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
ItremainstoobtaintherateofD:=P( F u/2 and Ω).Foranypositiveconstant
q q ≥
θ that will be chosen later, we have:
P
D
≤
e − θu/2E eθP q Fq 1 Ωq
(cid:18) q (cid:19)
Y (7.16)
≤
e
−
θu/2 E(eθFq 1
Ωq
)
q
Y
since the variables (Mx) are independent. But
q q
θj
E(eθFq 1 )=1+θE(F 1 )+ E(Fj1 )
Ωq q Ωq j! q Ωq
j 2
X≥
and E(F 1 )=E(F ) E(F 1 )= E(F 1 ).
q Ωq q
−
q Ωc
q −
q Ωc
q
Next note that if for any integer l,
l ˜ <supMx (l+1) ˜
N t q| [t − A,t) ≤ N
then
F xb[(l+1)η ˜η+1]+xE(Z).
q
| |≤ N
Hence, cutting Ωc in slices of the type l ˜ <sup Mx (l +1) ˜ and using
Lemma 3, we obta q in by taking C large en { o N ugh, t q[t − A,t) ≤ N}
3
+
E(F 1 ) = E(F 1 ) ∞ x(b[(l+1)η ˜η+1]+ E(Z))
|
q Ωq
| |
q Ωc
q |≤ N | |
l=1
X
P(there exists t [0,T] Mx >ℓ ˜)
× ∈ |{ q| [t − A,t) } N
+
≤
(cid:3) α,A,f0 ∞ x(b[(l+1)η
N
˜η+1]+
|
E(Z)
|
)log(T)e − (cid:3) α,A,f0 l N ˜
l=1
X
+
≤
(cid:3) α,A,f0 ∞ x(b
N
˜η+
|
E(Z)
|
)log(T)2lηe− (cid:3) α,A,f0 l N ˜
l=1
X
(cid:3) ˜
(cid:3) log2(T)b ˜η e − α,A,f0N
≤ α,η,A,f0 N 1 2ηe − (cid:3) α,A,f0N ˜
−
z :=(cid:3) bT α′ .
≤
1 α,η,A,f0 −
Note that in the previous inequalities, we have bounded E(Z) by bE[Nη ]. In the
| | [ A,0)
same way, one can bound −
E(F q j1 Ωq ) ≤ E(F q 21 Ωq )z b j − 2,
Lasso and probabilistic inequalities for multivariate point processes 51
withz :=xb[ ˜η+1]+xE(Z)=(cid:3) blog(T)1+η.Onecanalsonote thatby station-
b
N
α,η,A,f0
arity,
2qx+x
E(F21 ) xE [Z θ (Mx) E(Z)] 21 ds
q Ωq ≤ (cid:20)Z2qx ◦ s q − { for all t,M q x |[t−A,t)≤N ˜ } (cid:21)
2qx+x
xE [Z θ (Mx) E(Z)] 21 ds
≤ (cid:20)Z2qx ◦ s q − { M q x |[s−A,s)≤N ˜ } (cid:21)
x2E([Z(N) E(Z)] 21 )
≤ − N[−A,0)≤N ˜
z :=(cid:3) (log(T)) 2 σ2.
≤
v α,η,A,f0
Now let us go back to (7.16). We have that
θu θj
D ≤ exp − 2 +kln 1+θz 1 + z v z b j − 2 j!
(cid:20) (cid:18) j 2 (cid:19)(cid:21)
X≥
u θj
≤ exp − θ 2 − kz 1 +k z v z b j − 2 j! ,
(cid:20) (cid:18) (cid:19) j 2 (cid:21)
X≥
using that ln(1+u) u. It is sufficient now to recognize a step of the proof of the
Bernsteininequality( ≤ weakversionsee[41],page25).Since kz =(cid:3) bT1 α′ /(log(T)),
1 α,η,s −
one can choose α >1,C and C in the definition (7.13) of u (not depending on b) such
′ 1 2
that u/2 kz √2kz z+ 1z z for some z=C log(T), where C is a constant. Hence,
− 1 ≥ v 3 b 4 4
1 θj
D ≤ exp − θ 2kz v z+ 3 z b z +k z v z b j − 2 j! .
(cid:20) (cid:18) (cid:19) j 2 (cid:21)
p X≥
One can choose accordingly θ (as for the proof of the Bernstein inequality) to obtain a
boundine z.ItremainstochooseC largeenoughandonlydependingonα,η,Aandf
− 4 0
to guarantee that D e z (cid:3) T α. This concludes the proof of the proposition.
≤
−
≤
α,η,A,f0 −
7.4.4. Proof of Proposition 4
Let Q denote a measure such that under Q the distribution of the full point process
restrictedto ( ,0] is identicalto the distributionunder P andsuchthat on (0, )the
−∞ ∞
process consists of independent components each being a homogeneous Poisson process
withrate 1.Furthermore,the Poissonprocessesshouldbe independent of the processon
( ,0]. From Corollary 5.1.2 in [36], the likelihood process is given by
−∞
t t
=exp Mt λ(m)du+ logλ(m)dN(m)
L t − u u u
(cid:18) m Z0 m Z0 (cid:19)
X X
and we have for t 0 the relation
≥
E κ (f)2=E κ (f)2 , (7.17)
P t Q t t
L
52 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
where E and E denote the expectation with respect to P and Q, respectively. Let,
P Q
furthermore, N˜ =N denote the total number of points on [ 1,0). Proposition 4
1 [ 1,0)
− −
will be an easy consequence of the following lemma.
Lemma 4. If the point process is stationary under P, if
ed λ(m) a(N +N˜ )+b
≤ t ≤ 1 1
for t [0,1] and for constants d R and a,b>0, and if E P (1+ε)N˜ 1 < for some ε>0
∈ ∈ ∞
then for any f,
Q(f,f) ζ f 2 (7.18)
≥ k k
for some constant ζ>0.
Proof. We use Ho¨lders inequality on κ 1 (f)2/p L 1 1 /p and κ 1 (f)2/q L −1 1/p to get
E Q κ 1 (f)2 ≤ (E Q κ 1 (f)2 L 1 )1/p(E Q κ 1 (f)2 L − 1 q/p)1/q=Q(f,f)1/p(E Q κ 1 (f)2 L 1 1− q)1/q,
(7.19)
where 1 + 1 =1. We choose q 1 (and thus p) below to make q 1 sufficiently small.
p q ≥ −
For the left-hand side, we have by independence of the homogeneous Poisson processes
that if f =(µ,(g ) ),
ℓ ℓ=1,...,M
E κ (f)2=(E κ (f)) 2 +V κ (f)
Q 1 Q 1 Q 1
1 2 1
= µ+ g (u)du + g (u)2du.
ℓ ℓ
(cid:18) ℓ Z0 (cid:19) ℓ Z0
X X
Exactly as on page 32 in [52] there exists c >0 such that
′
1
E κ (f)2 c µ2+ g2(u)du =c f 2. (7.20)
Q 1 ≥ ′ ℓ ′ k k
(cid:18) ℓ Z0 (cid:19)
X
To bound the second factor on the right-hand side in (7.19) we observe, by assumption,
that we have the lower bound
1 eM(1 − b)e(d − aM)N1e − aMN˜ 1
L ≥
on the likelihood process. Under Q we have that (κ (f),N ) and N˜ are independent,
1 1 1
and with ρ=e(q 1)(aM d) and ρ˜=e(q 1)(aM) we get that
− − −
E Q κ 1 (f)2
L
1 1− q
≤
e(q − 1)M(b − 1)E Q ρ˜N˜ 1E Q κ 1 (f)2ρN1.
Lasso and probabilistic inequalities for multivariate point processes 53
Herewechooseq suchthatρ˜issufficientlycloseto1tomakesurethatE
Q
ρ˜N˜ 1=E
P
ρ˜N˜ 1<
(see Proposition 2). Moreover,by Cauchy–Schwarz’inequality
∞
1
κ2(f) µ2+ − g2(1 u)dN(ℓ) (1+N ). (7.21)
1 ≤ ℓ − u 1
(cid:18) ℓ Z0 (cid:19)
X
Under Q the point processes on (0, ) are homogeneous Poisson processes with rate
∞
1 and N , the total number of points, is Poisson. This implies that conditionally on
1
(N (1) ,...,N (M) )=(n(1),...,n(M)) the n(m)-points for the mth process are uniformly
1 1
distributed on [0,1], hence
1
E Q κ 1 (f)2 L 1 1− q ≤ µ2+ g ℓ 2(u)du e(q − 1)M(b − 1)E Q ρ˜N˜ 1E Q (1+N 1 )2ρN1
(cid:18) X ℓ Z0 (cid:19) c′′
(7.22)
=c′′ f 2. | {z }
k k
Combining (7.20) and (7.22) with (7.19) we get that
c f 2 (c )1/q f 2/qQ(f,f)1/p
′ ′′
k k ≤ k k
or by rearrangingthat
Q(f,f) ζ f 2
≥ k k
with ζ=(c)p/(c )p 1. (cid:3)
′ ′′ −
For the Hawkes process, it follows that if ν(m)>0 and if
sup h(m)(t)<
ℓ ∞
t [0,1]
∈
for l,m=1,...,M then for t [0,1] we have ed λ(m) a(N +N˜ )+b with
∈ ≤ t ≤ 1 1
d=logν(m), a=max sup h(m)(t), b=ν(m).
ℓ
l t [0,1]
∈
Proposition 2 proves that there exists ε>0 such that E P (1+ε)N˜ 1 < . This completes
∞
the proof of Proposition 4.
54 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
7.5. Proofs of the results of Sections 4.2 and 5.2
7.5.1. Proof of Propositions 5 and 1
We first prove Proposition 5. As in the proof of Proposition 3, we use the notation (cid:3).
Note that for any ϕ and any ϕ belonging to Φ,
1 2
M T
G
ϕ1,ϕ2
= κ
t
(ϕ1 (m))κ
t
(ϕ2 (m))dt
m=1Z0
X
and E(G
ϕ1,ϕ2
)=T M
m=1
Q(ϕ1 (m),ϕ2 (m)) by using (5.2). This implies that
P
E(a′Ga)=a′ E(G)a=T Q(f
a
(m),f
a
(m)).
m
X
Hence by Proposition 4, E(aGa) Tζ f(m) 2=Tζ f 2 by definition of the norm
′ ≥ mk a k k a k
on . Since Φ is an orthonormal system, this implies that E(aGa) Tζ a . Hence,
H P
′
≥ k k
ℓ2
to show that Ω is a large event for some c>0, it is sufficient to show that for some
c
0<ǫ<ζ, with high probability, for any a RΦ,
∈
aGa aE(G)a Tǫ a 2 . (7.23)
| ′ − ′ |≤ k kℓ2
Indeed, (7.23) implies that, with high probability, for any a RΦ,
∈
aGa aE(G)a Tǫ a T(ζ ǫ) a ,
′
≥
′
− k k
ℓ2≥
− k k
ℓ2
and the choice c=T(ζ ǫ) is convenient. So, first one has to control all the coefficients
−
of G E(G). For all ϕ,ρ Φ, we apply Proposition 3 to
− ∈
Z(N)= ψ(m)(ϕ)ψ(m)(ρ).
0 0
m
X
NotethatZonlydependsonpointslyingin[ 1,0).Therefore, Z(N) 2M ϕ ρ (1+
N2 ). This leads to
− | |≤ k k∞k k∞
[ 1,0)
−
1
P G E(G ) x (cid:3) T β
T|
ϕ,ρ
−
ϕ,ρ
|≥
ϕ,ρ
≤
β,f0 −
(cid:18) (cid:19)
with
x
ϕ,ρ
=(cid:3)
β,f0,M
[σ
ϕ,ρ
log3/2(T)T− 1/2+
k
ϕ
k∞k
ρ
k∞
log4(T)T− 1]
and
2
σ2 =E ψ(m)(ϕ)ψ(m)(ρ) E ψ(m)(ϕ)ψ(m)(ρ) 1 .
ϕ,ρ
(cid:20)(cid:20) m
0 0 −
(cid:18) m
0 0
(cid:19)(cid:21)
N[−1,0)≤N ˜
(cid:21)
X X
Lasso and probabilistic inequalities for multivariate point processes 55
Hence, with probability larger than 1 (cid:3) Φ2T β one has that
−
β,f0|
|
−
|
a′Ga
−
a′ E(G)a
|≤
(cid:3)
β,f0
|
a
ϕ
||
a
ρ
|
[σ
ϕ,ρ
log3/2(T)T1/2+
k
ϕ
k∞k
ρ
k∞
log4(T)] .
(cid:18)ϕ,ρ Φ (cid:19)
X∈
Hence, for any positive constant δ chosen later,
a′Ga a′ E(G)a
| − |
σ2
(cid:3) T a a δ ϕ,ρ (7.24)
≤
β,f0
|
ϕ
||
ρ
| ϕ ρ
(cid:20) ϕ
X
,ρ
∈
Φ (cid:20) k k∞k k∞
1 log4(T)
+ +1 ϕ ρ .
δlog(T) k k∞k k∞ T
(cid:20) (cid:21) (cid:21)(cid:21)
σ2
Now let us focus on E:= a a ϕ,ρ . First, we have:
ϕ,ρ Φ| ϕ || ρ | ϕ ∞ ρ ∞
∈ k k k k
P
E([ ψ(m)(ϕ)ψ(m)(ρ)]21 )+(E[ ψ(m)(ϕ)ψ(m)(ρ)])2
E 2 a a
m 0 0 N[−1,0)≤N ˜ m 0 0
ϕ ρ
≤ | || | ϕ ρ
P P
ϕ
X
,ρ
∈
Φ k k∞k k∞
with ˜ :=(cid:3) log(T). Next,
N
β,f0
ψ(m)(ϕ)ψ(m)(ρ) 2M ϕ ρ (1+N2 ).
0 0 ≤ k k∞k k∞ [ − 1,0)
m
X
Hence, if N ˜ =(cid:3) log(T), for T large enough,
[
−
1,0)
≤N
β,f0
ψ(m)(ϕ)ψ(m)(ρ) (cid:3) ϕ ρ log2(T)
0 0 ≤ β,M,f0k k∞k k∞
m
X
and
E ψ(m)(ϕ)ψ(m)(ρ) (cid:3) ϕ ρ log2(T).
0 0 ≤ β,M,f0k k∞k k∞
(cid:18) m (cid:19)
X
Hence,
E (cid:3) log2(T) a a E ψ(m)(ϕ)ψ(m)(ρ) .
≤ β,M,f0 | ϕ || ρ | 0 0
ϕ,ρ Φ (cid:18)(cid:12) m (cid:12)(cid:19)
X∈ (cid:12)X (cid:12)
(cid:12) (cid:12)
Butnotethatforanyf, ψ (m) (f) ψ (m) (f )where(cid:12) f =((µ(m) ,(g (m(cid:12)) ) ) ).
| 0 |≤ 0 | | | | | | | ℓ | ℓ=1,...,M m=1,...,M
Therefore,
E (cid:3) log2(T) a a E ψ(m)(ϕ)ψ(m)(ρ)
≤ β,M,f0 | ϕ || ρ | 0 | | 0 | |
ϕ,ρ Φ (cid:18) m (cid:19)
X∈ X
56 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
2
(cid:3) log2(T) E a ψ(m)(ϕ)
≤ β,M,f0 | ϕ | 0 | |
m (cid:18)(cid:20)ϕ Φ (cid:21) (cid:19)
X X∈
2
(cid:3) log2(T) E ψ(m) a ϕ .
≤ β,M,f0 0 | ϕ || |
m (cid:18)(cid:20) (cid:18)ϕ Φ (cid:19)(cid:21) (cid:19)
X X∈
But if ϕ=(µ(m),((g )(m)) ) , then
ϕ ϕ ℓ ℓ m
2 M 0 2
ψ (m) a ϕ = a µ(m)+ − a (g ) (m) ( u)dN(ℓ) .
(cid:20) 0 (cid:18)ϕ
X∈
Φ | ϕ || | (cid:19)(cid:21) "
X
ϕ | ϕ | ϕ
X
ℓ=1Z− 1
X
ϕ | ϕ || ϕ ℓ | − u #
IfonecreatesartificiallyaprocessN(0) withonlyonepointandifwedecidethat(g )(m)
ϕ 0
is the constant function equal to µ(m), this can also be rewritten as
ϕ
2 M 0 2
ψ(m) a ϕ = − a (g )(m) ( u)dN(ℓ) .
(cid:20) 0 (cid:18)ϕ
X∈
Φ | ϕ || | (cid:19)(cid:21) "
X
ℓ=0Z− 1
X
ϕ | ϕ || ϕ ℓ | − u #
Now we apply the Cauchy–Schwarzinequality for the measure dN(ℓ), which gives
ℓ
2 M 0 P 2
ψ(m) a ϕ (N +1) − a (g )(m) ( u) dN(ℓ).
(cid:20) 0 (cid:18)ϕ
X∈
Φ | ϕ || | (cid:19)(cid:21) ≤ [ − 1,0)
X
ℓ=0Z− 1 (cid:20)
X
ϕ | ϕ || ϕ ℓ | − (cid:21) u
Consequently,
M M 0 2
E (cid:3) log2(T) E (N +1) − a (g )(m) ( u) dN(ℓ)
≤ β,M,f0 m
X
=1
X
ℓ=0 (cid:18) [ − 1,0) Z− 1 (cid:20)
X
ϕ | ϕ || ϕ ℓ | − (cid:21) u (cid:19)
M M
(cid:3) log2(T) a a
≤
β,M,f0
|
ϕ
||
ρ
|
m=1ℓ=0ϕ,ρ Φ
XX X∈
0
E − (N +1)(g )(m) ( u)(g )(m) ( u)dN(ℓ) .
× (cid:18)Z− 1 [ − 1,0) | ϕ ℓ | − | ρ ℓ | − u (cid:19)
Now let us use the fact that for every x,y 0, η,θ>0 that will be chosen later,
≥
y
xy ηeθx [log(y) log(ηθ) 1],
− ≤ θ − −
with the convention that ylog(y)=0 if y=0. Let us apply this to x=N +1 and
[ 1,0)
−
y= (g )(m) ( u)(g )(m) ( u). We obtain that
| ϕ ℓ | − | ρ ℓ | −
M
E
≤
(cid:3)
β,M,f0
ηlog2(T)
|
a
ϕ
||
a
ρ
|
E((N
[
−
1,0)
+1)eθ(N[−1,0)+1))
m=1ϕ,ρ Φ
X X∈
Lasso and probabilistic inequalities for multivariate point processes 57
M M
+(cid:3) θ 1log2(T) a a
β,M,f0 −
|
ϕ
||
ρ
|
m=1ℓ=0ϕ,ρ Φ
XX X∈
0−
E (g )(m) (g )(m) ( u)[log((g )(m) (g )(m) ( u)) log(ηθ) 1]dNℓ .
× | ϕ ℓ || ρ ℓ | − | ϕ ℓ || ρ ℓ | − − − u
(cid:18)Z− 1 (cid:19)
Since for ℓ>0, dN(ℓ) is stationary,one canreplace E(dN(ℓ)) by (cid:3) du.Moreover,since
u u f0
by Proposition 2, N has some exponential moments there exists θ=(cid:3) such that
E((N [
−
1,0) +1)eθ(N[− [ − 1, 1 0 , ) 0 + ) 1))=(cid:3) f0 . With
|
Φ
|
the size of the dictionary, this l f e 0 ads to
E (cid:3) η Φ log2(T) a 2
≤ β,M,f0 | | k kℓ2
+(cid:3) log2(T)
β,M,f0
M
a a µ(m) µ(m) [log(µ(m) µ(m) ) log(ηθ) 1]
× " | ϕ || ρ || ϕ || ρ | | ϕ || ρ | − −
m=1 ϕ,ρ Φ
X X∈
M 1
+ a a (g )(m) (g )(m) (u)
| ϕ || ρ | | ϕ ℓ || ρ ℓ |
ℓ=1ϕ,ρ Φ Z0
X X∈
[log((g )(m) (g )(m) (u)) log(ηθ) 1]du .
× | ϕ ℓ || ρ ℓ | − − #
Consequently, using Φ and r ,
Φ
k k∞
E (cid:3) η Φ log2(T) a 2 +(cid:3) log2(T)r [2log( Φ ) log(ηθ) 1] a 2 .
≤ β,M,f0 | | k kℓ2 β,M,f0 Φ k k∞ − − k kℓ2
We choose η= Φ 1 and obtain that
−
| |
E (cid:3) log2(T)r [log( Φ )+log(Φ)] a 2 .
≤ β,M,f0 Φ k k∞ | | k kℓ2
Now, let us choose δ=ω/(log2(T)r [log( Φ )+log(Φ)]) where ω depends only on
Φ
k k∞ | |
β,M and f and will be chosen later and let us go back to (7.24):
0
1
T|
a′Ga
−
a′ E(G)a
|≤
(cid:3)
β,M,f0
ω
k
a
k
2
ℓ2
+(cid:3) r [log( Φ )+log(Φ)]
β,f0,ω Φ
k k∞ | |
log5(T)
a a ϕ ρ
ϕ ρ
× | || |k k∞k k∞ T
ϕ,ρ Φ
X∈
(cid:3) ω a 2 +(cid:3) a 2 A (T).
≤ β,M,f0 k kℓ2 β,f0,ω k kℓ2 Φ
58 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Under assumptions of Proposition 5, for T large enough and T T ,
0 0
≥
1
aGa aE(G)a (cid:3) ω a 2 .
T| ′ − ′ |≤ β,M,f0 k kℓ2
It is now sufficient to take ω small enough and then T large enough to obtain (7.23)
0
with ǫ<ζ and Proposition 5 is proved.
Arguments for the proof of Proposition 1 are similar. So we just give a brief sketch of
the proof. Now,
M 1
G = (Y(m)) 2 ϕ (t,X(m))ϕ (t,X(m))dt.
ϕ1,ϕ2 t 1 2
m=1Z0
X
Let β>0. With probability larger than 1 2M β,
−
−
1 2βv logM βb logM
G E[G ] ϕ1,ϕ2 + ϕ1,ϕ2 ,
M|
ϕ1,ϕ2− ϕ1,ϕ2
|≤ M 3M
r
with
b = ϕ ϕ ,
ϕ1,ϕ2
k
1
k∞k
2
k∞
1 2
v =E (Y(m)) 2 ϕ (t,X(m))ϕ (t,X(m))dt D ϕ ϕ ϕ , ϕ ,
ϕ1,ϕ2
(cid:18)Z0
t 1 2
(cid:19)
≤ k 1 k∞k 2 k∞h| 1 | | 2 |i
where , denotesthestandardL -scalarproduct.WehavejustusedtheclassicalBern-
2
h· ·i
stein inequality combined with (4.2). So,with probability largerthan 1 2Φ2M β, for
−
− | |
any vector a and any δ>0,
aGa E[aGa] (cid:3) a a [δM ϕ , ϕ +δ 1logM ϕ ϕ ]
|
′
−
′
|≤
D,β
|
ϕ1|| ϕ2|
h|
1
| |
2
|i
−
k
1
k∞k
2
k∞
ϕ X1,ϕ2
(cid:3) (δMr +δ 1 Φ 2 Φ logM) a 2 .
≤ D,β Φ − k k∞| | k kℓ2
We choose δ= k Φ k 2∞ M | Φ r | Φ logM, so that with probability larger than 1 − 2 | Φ | 2M − β,
q
1 Φ 2 r Φ logM
M|
a′Ga
−
E[a′Ga]
|≤
(cid:3)
D,β
k k∞ Φ
M
| |
k
a
k
2
ℓ2
.
r
We use (4.1) and (4.3) to conclude as for Proposition 5 and we obtain Proposition 1.
7.5.2. Proof of Corollary 3
First,letuscut[ 1,T]in T +2intervalsI’softhetype[a,b)suchthatthefirst T +1
− ⌊ ⌋ ⌊ ⌋
intervalsareoflength1andthelastoneisoflengthstrictlysmallerthan1(eventuallyit
is just a singleton). Then, any interval of the type [t 1,t] for t in [0,T] is included into
−
Lasso and probabilistic inequalities for multivariate point processes 59
the union of two such intervals.Therefore,the event where all the N ’s are smaller than
I
u= /2isincludedintoΩ .Itremainstocontroltheprobabilityofthecomplementary
N N
of this event. By stationarity, all the first N ’s have the same distribution and satisfy
I
Proposition 2. The last one can also be viewed as the truncation of a stationary point
process to an interval of length smaller than 1. Therefore, the exponential inequality of
Proposition 2 also applies to the last interval. It remains to apply T +2 times this
⌊ ⌋
exponential inequality and to use a union bound.
7.5.3. Proof of Corollary 4
As in the proof of Proposition 3, we use the notation (cid:3). The nonasymptotic part of the
result is just a pure application of Theorem 2, with the choices of B and V given by
ϕ ϕ
(5.5)and(5.6).The nextstepconsistsincontrollingthe martingaleψ(ϕ)2 (N Λ) on
T
• −
Ω . To do so, let us apply (7.7) to H such that for any m,
V,B
H
t
(m)=ψ
t
(m)(ϕ)21
t
τ′,
≤
with B=B2 and τ =T and where τ is defined in (7.1) (see the proof of Theorem 2).
ϕ ′
TheassumptiontobefulfilledischeckedasintheproofofTheorem2.Butaspreviously,
on Ω , H (N Λ) =ψ(ϕ)2 (N Λ) and also H2 Λ =ψ(ϕ)4 Λ . Moreover,
V,B T T T T
• − • − • •
on Ω Ω
V,B
N ⊂
H2 Λ =ψ(ϕ)4 Λ v:=TM maxν(m)+ maxh(m) B4.
• T • T ≤ m N m,ℓ ℓ ϕ
(cid:16) (cid:17)
Recallthat x=αlog(T).SoonΩ ,withprobabilitylargerthan1 (M+KM2)e x=
V,B −
−
1 (M +KM2)T α, one has that for all ϕ Φ,
−
− ∈
B2x
ψ(ϕ)2 N ψ(ϕ)2 Λ +√2vx+ ϕ .
T T
• ≤ • 3
So that for all ϕ Φ,
∈
ψ(ϕ)2 N (cid:3) [ ϕ 2 + Φ 2 2 T log(T)].
• T ≤ M,f0 Nk kT k k∞N N
Also, since =log2(T), one can apply Corollary 3, with p β=α. We finally choose c as
N
in Proposition5. This leads to the result.
Acknowledgements
We are very grateful to Christine Tuleau-Malot who allowed us to use her R programs
simulating Hawkes processes. The research of Patricia Reynaud-Bouret and Vincent
Rivoirard is partly supported by the french Agence Nationale de la Recherche (ANR
2011 BS01 010 01 projet Calibration). The authors would like to thank the anonymous
Associate Editor and Referees for helpful comments and suggestions.
60 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
References
[1] Aalen, O. (1980). A model for nonparametric regression analysis of counting processes.
InMathematical Statistics and Probability Theory (Proc. Sixth Internat. Conf.,Wisl a,
1978). Lecture Notes in Statist. 21–25. New York:Springer. MR0577267
[2] Andersen, P.K., Borgan, Ø., Gill, R.D. and Keiding, N. (1993). Statistical Mod-
els Based on Counting Processes. Springer Series in Statistics. New York: Springer.
MR1198884
[3] Bacry, E., Delattre, S., Hoffmann, M. and Muzy, J.F. (2013). Some limit theorems
for Hawkes processes and application to financial statistics. Stochastic Process. Appl.
123 2475–2499. MR3054533
[4] Bercu,B.andTouati,A.(2008).Exponentialinequalitiesforself-normalizedmartingales
with applications. Ann. Appl. Probab. 18 1848–1869. MR2462551
[5] Bertin,K.,LePennec,E.andRivoirard,V.(2011).AdaptiveDantzigdensityestima-
tion. Ann. Inst. Henri Poincar´e Probab. Stat. 47 43–74. MR2779396
[6] Bickel, P.J.,Ritov, Y.andTsybakov, A.B.(2009).Simultaneousanalysisoflassoand
Dantzig selector. Ann. Statist. 37 1705–1732. MR2533469
[7] Bowsher,C.G.(2010).Stochastickineticmodels:Dynamicindependence,modularityand
graphs. Ann. Statist. 38 2242–2281. MR2676889
[8] Br´emaud, P. (1981). Point Processes and Queues. NewYork:Springer. MR0636252
[9] Br´emaud, P. and Massouli´e, L. (1996). Stability of nonlinear Hawkes processes. Ann.
Probab. 24 1563–1588. MR1411506
[10] Brette, R. and Destexhe, A., eds. (2012). Handbook of Neural Activity Measurement.
Cambridge: Cambridge Univ.Press. MR2976674
[11] Brunel, E. and Comte, F. (2005). Penalized contrast estimation of density and hazard
rate with censored data. Sankhya¯ 67 441–475. MR2235573
[12] Brunel,E.andComte,F.(2008).Adaptiveestimationofhazardratewithcensoreddata.
Comm. Statist. Theory Methods 37 1284–1305. MR2440441
[13] Bu¨hlmann, P. and van de Geer, S. (2011). Statistics for High-dimensional Data. Hei-
delberg: Springer.MR2807761
[14] Bunea, F. and McKeague, I.W. (2005). Covariate selection for semiparametric hazard
function regression models. J. Multivariate Anal. 92 186–204. MR2102251
[15] Bunea, F.,Tsybakov, A. andWegkamp, M. (2007). Sparsity oracle inequalitiesfor the
Lasso. Electron. J. Stat. 1169–194. MR2312149
[16] Bunea, F., Tsybakov, A.B. and Wegkamp, M.H. (2006). Aggregation and sparsity via
l penalized least squares. In Learning Theory. Lecture Notes in Computer Science
1
4005 379–391. Berlin: Springer. MR2280619
[17] Bunea,F.,Tsybakov,A.B.andWegkamp,M.H.(2007).Sparsedensityestimationwith
ℓ penalties. In Learning Theory. Lecture Notes in Computer Science 4539 530–543.
1
Berlin: Springer. MR2397610
[18] Bunea, F., Tsybakov, A.B. and Wegkamp, M.H. (2007). Aggregation for Gaussian
regression. Ann. Statist. 35 1674–1697. MR2351101
[19] Candes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p is
much larger than n. Ann. Statist. 35 2313–2351. MR2382644
[20] Carstensen, L., Sandelin, A., Winther, O. and Hansen, N.R. (2010). Multivariate
Hawkesprocessmodelsoftheoccurrenceofregulatoryelementsandananalysisofthe
pilot ENCODE regions. BMC Bioinformatics 11 456.
Lasso and probabilistic inequalities for multivariate point processes 61
[21] Chagny, G. (2012). Adaptive warped kernel estimators. Available at
http://hal.archives-ouvertes.fr/hal-00715184.
[22] Chornoboy, E.S., Schramm, L.P. and Karr, A.F. (1988). Maximum likelihood identi-
fication of neuralpoint process systems. Biol. Cybernet. 59 265–275. MR0961117
[23] Comte,F.,Ga¨ıffas,S.andGuilloux,A.(2011).Adaptiveestimationoftheconditional
intensity of marker-dependent counting processes. Ann. Inst. Henri Poincar´e Probab.
Stat. 47 1171–1196. MR2884230
[24] Daley, D.J. and Vere-Jones, D. (2003). An Introduction to the Theory of Point Pro-
cesses. Vol. I,2nd ed.New York:Springer. MR1950431
[25] de la Pen˜a, V.H. (1999). A general class of exponential inequalities for martingales and
ratios. Ann. Probab. 27 537–564. MR1681153
[26] de la Pen˜a, V.H.,Lai, T.L.andShao, Q.-M.(2009). Self-normalized Processes. Berlin:
Springer. MR2488094
[27] Dzhaparidze, K. and van Zanten, J.H. (2001). On Bernstein-typeinequalities for mar-
tingales. Stochastic Process. Appl. 93 109–117. MR1819486
[28] Fu, W.J. (1998). Penalized regressions: The bridge versus the lasso. J. Comput. Graph.
Statist. 7 397–416. MR1646710
[29] Ga¨ıffas, S.andGuilloux, A.(2012).High-dimensionaladditivehazardsmodelsandthe
Lasso. Electron. J. Stat. 6522–546. MR2988418
[30] Gr´egoire,G.(1993).Leastsquarescross-validationforcountingprocessintensities.Scand.
J. Statist. 20 343–360. MR1276698
[31] Gru¨n,S.,Diesmann,M.,Grammont,F.,Riehle,A.andAertsen,A.(1999).Detecting
unitary eventswithout discretization in time. J. Neurosci. Meth. 94 67–79.
[32] Gusto, G. and Schbath, S. (2005). FADO: A statistical method to detect favored or
avoided distances between occurrences of motifs using theHawkes’model. Stat. Appl.
Genet. Mol. Biol. 4Art. 24, 28 pp.(electronic). MR2170440
[33] Ha¨rdle, W., Kerkyacharian, G., Picard, D. and Tsybakov, A. (1998). Wavelets,
Approximation,andStatisticalApplications.LectureNotesinStatistics129.NewYork:
Springer. MR1618204
[34] Hawkes, A.G. (1971). Point spectra of some mutually exciting point processes. J. Roy.
Statist. Soc. Ser. B 33 438–443. MR0358976
[35] Huang, J.,Ma, S.andZhang, C.-H.(2008).AdaptiveLassoforsparsehigh-dimensional
regression models. Statist. Sinica18 1603–1618. MR2469326
[36] Jacobsen,M.(2006).PointProcessTheoryandApplications.MarkedPointandPiecewise
Deterministic Processes. Boston, MA: Birkh¨auser. MR2189574
[37] Koltchinskii, V., Lounici, K. and Tsybakov, A.B. (2011). Nuclear-norm penalization
and optimal rates for noisy low-rank matrix completion. Ann. Statist. 39 2302–2329.
MR2906869
[38] Krumin, M., Reutsky, I. and Shoham, S. (2010). Correlation-based analysis and gen-
eration of multiple spike trains using Hawkes models with an exogenous input. Front.
Comp. Neurosci 4. 147.
[39] Letue, F. (2000). Mod`ele deCox: Estimation par s´election demod`ele et mod`ele dechocs
bivari´e. Ph.D. thesis.
[40] Liptser, R. and Spokoiny, V. (2000). Deviation probability bound for martingales with
applications to statistical estimation. Statist. Probab. Lett. 46 347–357. MR1743992
[41] Massart, P. (2007). Concentration Inequalities and Model Selection. Lectures from the
33rd Summer School on Probability Theory held in Saint-Flour, July 6–23, 2003. Lec-
ture Notes in Math. 1896. Berlin: Springer. MR2319879
62 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
[42] Masud, M.S. and Borisyuk, R. (2011). Statistical technique for analysing functional
connectivity of multiplespike trains. J. Neurosci. Meth. 196 201–219.
[43] Meinshausen, N. (2007). Relaxed Lasso. Comput. Statist. Data Anal. 52 374–393.
MR2409990
[44] Mitchell, L. and Cates, M.E. (2010). Hawkes process as amodel of social interactions:
A view on video dynamics. J. Phys. A 43 045101, 11. MR2578723
[45] Pernice, V., Staude, B., Cardanobile, S. and Rotter, S. (2011). How structure
determines correlations in neuronal networks. PLoS Comput. Biol. 7 e1002059, 14.
MR2821638
[46] Pernice, V., Staude, B., Cardanobile, S. and Rotter, S. (2012). Recurrent interac-
tions in spiking networkswith arbitrary topology. Phys. Rev. E 85 031916.
[47] Pillow, J.W., Shlens, J., Paninski, L., Sher, A., Litke, A.M., Chichilnisky, E.J.
and Simoncelli, E.P. (2008). Spatio-temporal correlations and visual signalling in a
complete neuronal population. Nature 454 995–999.
[48] Reynaud-Bouret,P.(2003).AdaptiveestimationoftheintensityofinhomogeneousPois-
son processes via concentration inequalities. Probab. Theory Related Fields 126 103–
153. MR1981635
[49] Reynaud-Bouret, P. (2006). Penalized projection estimators oftheAalen multiplicative
intensity.Bernoulli 12 633–661. MR2248231
[50] Reynaud-Bouret, P. and Rivoirard, V. (2010). Near optimal thresholding estimation
of a Poisson intensity on the real line. Electron. J. Stat. 4 172–238. MR2645482
[51] Reynaud-Bouret,P.andRoy,E.(2006).SomenonasymptotictailestimatesforHawkes
processes. Bull. Belg. Math. Soc. Simon Stevin 13 883–896. MR2293215
[52] Reynaud-Bouret,P.andSchbath,S.(2010).AdaptiveestimationforHawkesprocesses;
application to genome analysis. Ann. Statist. 38 2781–2822. MR2722456
[53] Reynaud-Bouret, P., Tuleau-Malot, C., Rivoirard, V. and Grammont, F.
(2013). Spike trains as (in)homogeneous Poisson processes or Hawkes pro-
cesses: Nonparametric adaptive estimation and goodness-of-fit tests. Available at
http://hal.archives-ouvertes.fr/hal-00789127.
[54] Rudelson, M. and Vershynin, R. (2008). On sparse reconstruction from Fourier and
Gaussian measurements. Comm. Pure Appl. Math. 61 1025–1045. MR2417886
[55] Rudelson,M.andVershynin,R.(2009).Smallestsingularvalueofarandomrectangular
matrix. Comm. Pure Appl. Math. 62 1707–1739. MR2569075
[56] Rudelson, M. and Vershynin, R. (2010). Non-asymptotic theory of random matrices:
Extreme singular values. In Proceedings of the International Congress of Mathemati-
cians III 1576–1602. New Delhi: Hindustan Book Agency.MR2827856
[57] Shorack, G.R. and Wellner, J.A. (1986). Empirical Processes with Applications to
Statistics. Wiley Series in Probability and Mathematical Statistics: Probability and
Mathematical Statistics. New York:Wiley. MR0838963
[58] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Roy. Statist.
Soc. Ser. B 58 267–288. MR1379242
[59] van de Geer, S. (1995). Exponential inequalities for martingales, with application to
maximum likelihood estimation for counting processes. Ann. Statist. 23 1779–1801.
MR1370307
[60] van de Geer,S.,Bu¨hlmann, P.andZhou, S.(2011).Theadaptiveandthethresholded
Lasso for potentially misspecified models (andalower boundfortheLasso). Electron.
J. Stat. 5 688–749. MR2820636
Lasso and probabilistic inequalities for multivariate point processes 63
[61] vandeGeer,S.A.(2008).High-dimensionalgeneralizedlinearmodelsandthelasso.Ann.
Statist. 36 614–645. MR2396809
[62] Vere-Jones, D. and Ozaki, T. (1982). Some examples of statistical estimation applied
to earthquakedata I: Cyclic Poisson and self-exciting models. Ann. I. Stat. Math. 34
189–207.
[63] Willett, R.M. and Nowak, R.D. (2007). Multiscale Poisson intensity and density esti-
mation. IEEE Trans. Inform. Theory 53 3171–3187. MR2417680
[64] Zou, H.(2006). Theadaptivelasso anditsoracle properties.J. Amer. Statist. Assoc. 101
1418–1429. MR2279469
Received September 2012 and revised July 2013