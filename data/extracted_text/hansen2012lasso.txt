arXiv:1208.0570v2  [math.ST]  7 Apr 2015
Bernoulli 21(1), 2015, 83–143
DOI: 10.3150/13-BEJ562
Lasso and probabilistic inequalities for
multivariate point processes
NIELS RICHARD HANSEN
1,
PATRICIA REYNAUD-BOURET2 and VINCENT RIVOIRARD 3
1Department of Mathematical Sciences, University of Copenh agen, Universitetsparken 5, 2100
Copenhagen, Denmark. E-mail: Niels.R.Hansen@math.ku.dk
2Univ. Nice Sophia Antipolis, CNRS, LJAD, UMR 7351, 06100 Nic e, France.
E-mail: Patricia.Reynaud-Bouret@unice.fr
3CEREMADE, CNRS-UMR 7534, Universit´ e Paris Dauphine, Plac e Mar´ echal de Lattre de
Tassigny, 75775 Paris Cedex 16, France. INRIA Paris-Rocque ncourt, projet Classic.
E-mail: Vincent.Rivoirard@dauphine.fr
Due to its low computational cost, Lasso is an attractive reg ularization method for high-
dimensional statistical settings. In this paper, we consid er multivariate counting processes de-
pending on an unknown function parameter to be estimated by l inear combinations of a ﬁxed
dictionary. To select coeﬃcients, we propose an adaptive ℓ1-penalization methodology, where
data-driven weights of the penalty are derived from new Bern stein type inequalities for martin-
gales. Oracle inequalities are established under assumpti ons on the Gram matrix of the dictio-
nary. Nonasymptotic probabilistic results for multivaria te Hawkes processes are proven, which
allows us to check these assumptions by considering general dictionaries based on histograms,
Fourier or wavelet bases. Motivated by problems of neuronal activity inference, we ﬁnally carry
out a simulation study for multivariate Hawkes processes an d compare our methodology with
the adaptive Lasso procedure proposed by Zou in ( J. Amer. Statist. Assoc. 101 (2006) 1418–
1429). We observe an excellent behavior of our procedure. We rely on theoretical aspects for
the essential question of tuning our methodology. Unlike ad aptive Lasso of ( J. Amer. Statist.
Assoc. 101 (2006) 1418–1429), our tuning procedure is proven to be robu st with respect to all
the parameters of the problem, revealing its potential for c oncrete purposes, in particular in
neuroscience.
Keywords: adaptive estimation; Bernstein-type inequalities; Hawke s processes; Lasso
procedure; multivariate counting process
1. Introduction
The Lasso, proposed in [
58], is a well-established method that achieves sparsity of an
estimated parameter vector via ℓ1-penalization. In this paper, we focus on using Lasso
This is an electronic reprint of the original article publis hed by the ISI/BS in Bernoulli,
2015, Vol. 21, No. 1, 83–143 . This reprint diﬀers from the original in pagination and
typographic detail.
1350-7265 c⃝ 2015 ISI/BS
2 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
to select and estimate coeﬃcients in the basis expansion of intensity processes for multi-
variate point processes.
Recent examples of applications of multivariate point processes inclu de the modeling of
multivariate neuron spike data [ 42, 47], stochastic kinetic modeling [ 7] and the modeling
of the distribution of ChIP-seq data along the genome [ 20]. In the previous examples,
the intensity of a future occurrence of a point depends on the hist ory of all or some of
the coordinates of the point processes, and it is of particular inter est to estimate this
dependence. This can be achieved using a parametric family of models , as in several of
the papers above. Our aim is to provide a nonparametric method bas ed on the Lasso.
The statistical properties of Lasso are particularly well understo od in the context of
regression with i.i.d. errors or for density estimation where a range o f oracle inequalities
have been established. These inequalities, now widespread in the liter ature, provide the-
oretical error bounds that hold on events with a controllable (large ) probability. See, for
instance, [ 5, 6, 15–18, 61]. We refer the reader to [ 13] for an excellent account on many
state-of-the-art results. One main challenge in this context is to o btain as weak conditions
as possible on the design – or Gram – matrix. The other important cha llenge is to be able
to provide an ℓ1-penalization procedure that provides excellent performance fro m both
theoretical and practical points of view. Standard Lasso propos ed in [ 58] and based on
deterministic constant weights constitutes a major contribution f rom the methodological
point of view, but underestimation due to its shrinkage nature may le ad to poor practical
performance in some contexts. Alternative two step procedures have been suggested to
overcome this drawback (see [ 43, 60, 64]). Zou in [ 64] also discusses problems for stan-
dard Lasso to cope with variable selection and consistency simultane ously. He overcomes
these problems by introducing nonconstant data-driven ℓ1-weights based on preliminary
consistent estimates.
1.1. Our contributions
In this paper, we consider an ℓ1-penalized least squares criterion for the estimation of
coeﬃcients in the expansion of a function parameter. As in [
5, 35, 60, 64], we consider
nonconstant data-driven weights. However the setup is here tha t of multivariate point
processes and the function parameter that lives in a Hilbert space d etermines the point
process intensities. Even in this unusual context, the least squar es criterion also involves
a random Gram matrix as well, and in this respect, we present a fairly s tandard oracle
inequality with a strong condition on this Gram matrix (see Theorem 1 in Section 2).
One major contribution of this article is to provide probabilistic result s that enable us
to calibrate ℓ1-weights in the most general setup (see Theorem 2 in Section 2). This is
naturally linked to sharp Bernstein type inequalities for martingales. In the literature,
those kinds of inequalities generally provide upper bounds for the ma rtingale that are de-
terministic and unobservable [ 57, 59]. To choose data-driven weights, we need observable
bounds. More recently, there have been some attempts to use se lf-normalized processes in
order to provide more ﬂexible and random upper bounds [ 4, 25–27]. Nevertheless, those
bounds are usually not (completely) observable when dealing with cou nting processes.
Lasso and probabilistic inequalities for multivariate poi nt processes 3
We prove a result that goes further in this direction by providing a co mpletely sharp
random observable upper bound for the martingale in our counting p rocess framework
(see Theorem 3 in Section 3).
The second main contribution is to provide a quite theoretical and ab stract framework
to deal with processes whose intensity is (or is well approximated by ) a linear trans-
formation of deterministic parameters to infer. This general fram ework also allows for
diﬀerent asymptotics in terms of the number of observed process es or in terms of the
duration of the recording of observations, according to the setu p. We focus in this paper
on three main examples: the Poisson model, the Aalen multiplicative inte nsity model
and the multivariate Hawkes process, but many other situations ca n be expressed in the
present framework, which has the advantage of full ﬂexibility. The ﬁrst two examples
have been extensively studied in the literature as we detail hereaft er, but Hawkes pro-
cesses are typical of situations where very little is known from a non parametric point
of view, and where fully implementable adaptive methods do not exist u ntil the present
work, to the best of our knowledge. They also constitute process es that are often used
in practice – in particular in neuroscience – as explained below.
It is also notable that we, in each of these three previous examples, can verify explic-
itly if the strong condition on the Gram matrix mentioned previously is f ulﬁlled with
probability close to 1 (see Section 4 for the Poisson and Aalen cases and Section 5 for
the Hawkes case). For the multivariate Hawkes process, this involv es novel probabilis-
tic inequalities. Even though the Hawkes processes have been stud ied extensively in the
literature, see [ 9, 24], very little is known about exponential inequalities and nonasymp-
totic tail control. Besides the univariate case [ 51], no exponential inequality controlling
the number of points per interval is known to us. We derive such res ults and other sharp
controls on the convergence in the ergodic theorem to obtain cont rol on the Gram matrix.
Finally, we carry out a simulation study in Section 6 for the most intricate process,
namely the multivariate Hawkes process, with a main aim: to convince p ractitioners, for
instance in neuroscience, that this method is indeed fully implementab le and gives good
results in practice. Data-driven weights for practical purposes a re slight modiﬁcations of
theoretical ones. These modiﬁcations essentially aim at reducing th e number of tuning
parameters to one. Due to nonnegligible shrinkage that is unavoidab le, in particular for
large coeﬃcients, we propose a two step procedure where estimat ion of coeﬃcients is han-
dled by using ordinary least squares estimation on the support prelim inary determined
by our Lasso methodology. Tuning issues are extensively investigat ed in our simulation
study, and Table 1 in Section 6.3 shows that our methodology can easily and robustly be
tuned by using limit values imposed by assumptions of Theorem 2. We naturally com-
pare our procedure with adaptive Lasso of [ 64] for which weights are proportional to the
inverse of ordinary least squares estimates. The latter is very com petitive for estimation
aspects since shrinkage becomes negligible if the preliminary OLS estim ates are large.
But adaptive Lasso does not incorporate random ﬂuctuations of c oeﬃcient estimators.
So it is most of the time outperformed by our procedure. In particu lar, tuning adaptive
Lasso in the Hawkes setting is a diﬃcult task, which cannot be tackled by using standard
cross-validation. Our simulation study shows that the performanc e of adaptive Lasso is
very sensitive to the choice of the tuning parameter. Robustness with respect to tuning is
4 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
another advantage of our method over adaptive Lasso. For simula tions, the framework of
neuronal networks is used. Our short study proves that our met hodology can be used for
solving concrete problems in neuroscience such as the inference of functional connectivity
graphs.
1.2. Multivariate counting process
The framework introduced here and used throughout the paper a ims at unifying sev-
eral situations, making the reading easier. Main examples are then s hortly described,
illustrating the use of this setup.
We consider an M-dimensional counting process ( N(m)
t )m=1,...,M , which can also be
seen as a random point measure on R with marks in {1,...,M }, and corresponding
predictable intensity processes ( λ(m)
t )m=1,...,M under a probability measure P (see [
8] or
[24] for precise deﬁnitions).
Classical models assume that the intensity λ(m)
t can be written as a linear predictable
transformation of a deterministic function parameter f0 belonging to a Hilbert space H
(the structure of H, and then of f0, will diﬀer according to the context, as illustrated
below). We denote this linear transformation by
ψ(f) = ( ψ(1)(f),...,ψ (M)(f)). (1.1)
Therefore, for classical models, for any t,
λ(m)
t = ψ(m)
t (f0). (1.2)
The main goal in classical settings is to estimate f0 based on observing ( N(m)
t )m=1,...,M
for t∈ [0,T]. Actually, we do not require in Theorems 1 and 2 that ( 1.2) holds. Our
aim is mainly to furnish an estimate of the best linear approximation ψ(m)
t (f0) of the
underlying intensity λ(m)
t .
Let us illustrate the general setup with three main examples: First, the case with i.i.d.
observations of an inhomogeneous Poisson process on [0 ,1] and unknown intensity, sec-
ond, the well known Aalen multiplicative intensity model and third, the central example
of the multivariate Hawkes process. For the ﬁrst two models, asym ptotics is with respect
to M (T is ﬁxed). For the third one, M is ﬁxed and asymptotics is with respect to T.
1.2.1. The Poisson model
Let us start with a very simple example which will be somehow a toy prob lem here com-
pared to the other examples. In this example, we take T = 1 and assume that we observe
M i.i.d. Poisson processes on [0 ,1] with common intensity f0 : [0,1] ↦− →R+. Asymptotic
properties are obtained when M tends to inﬁnity. In this case, the intensity λ(m) of the
mth process N(m) is f0, which does not depend on m: Therefore, for any m∈ {1,...,M }
and any t, we set
ψ(m)
t (f0) := f0(t),
Lasso and probabilistic inequalities for multivariate poi nt processes 5
and H = L2([0,1]) is equipped with the classical norm deﬁned by
∥f∥ =
(∫ 1
0
f2(t) dt
)1/2
.
This framework has already been extensively studied from an adapt ive point of view: see
for instance [ 48, 63] for model selection methods, [ 50] for wavelet thresholding estimation
or [ 53] for kernel estimates. In this context, our present general re sult matches with
existing minimax adaptation results where asymptotics is with respec t to M.
1.2.2. The Aalen multiplicative intensity model
This is one of the most popular counting processes because of its ad aptivity to various
situations (from Markov models to censored lifetime models) and its v arious applications
to biomedical data (see [
2]). Given X a Hilbert space, we consider f0 : [0,T] × X ↦− →R+,
and we set for any t∈ R,
λ(m)
t = ψ(m)
t (f0) := f0(t,X(m))Y(m)
t ,
where Y(m) is an observable predictable process and X(m) represents covariates. In this
case, H = L2([0,T] × X ). Our goal is to estimate f0 and not to select covariates. So, to
ﬁx ideas one sets X = [0 ,1] and T = 1. Hence H can be identiﬁed with L2([0,1]2). For
right-censored data, f0 usually represents the hazard rate. The presence of covariates
in this pure nonparametric model is the classical generalization of th e semi-parametric
model proposed by Cox (see [ 39], for instance). Note that the Poisson model is a special
case of the Aalen model.
The classical framework consists in assuming that ( X(m),Y (m),N(m))m=1,...,M is an
i.i.d. M-sample and as for the Poisson model, it is natural to investigate asy mptotic
properties when M→ +∞. If there are no covariates, several adaptive approaches alrea dy
exist: See [ 11, 12, 49] for various penalized least-squares contrasts and [ 21] for kernel
methods in special cases of censoring. In the presence of covaria tes, one can mention
[1, 2] for a parametric approach, [ 23, 39] for a model selection approach and [ 29] for a
Lasso approach. Let us also cite [ 14] where covariate selection via penalized MLE has
been studied. Once again, our present general result matches wit h existing oracle results.
In [ 21], exponential control of random ﬂuctuations leading to adaptive r esults are derived
without using the martingale theory. In more general frameworks (as in [ 23], for instance),
martingales are required and this even when i.i.d. processes are involv ed.
1.2.3. Hawkes processes
Hawkes processes are the point processes equivalent to autoreg ressive models. In seismol-
ogy, Hawkes processes can model earthquakes and their afters hocks [
62]. More recently
they have been used to model favored or avoided distances betwe en occurrences of motifs
[32] or Transcription Regulatory Elements [ 20] on the DNA. We can also mention the
6 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
use of Hawkes processes as models of social interactions [ 44] or ﬁnancial phenomena [ 3].
In the univariate setting, with M = 1, the intensity of a nonlinear Hawkes process
(Nt)t>0 is given by
λt = φ
(∫ t−
−∞
h(t− u) dNu
)
,
where φ: R ↦→R+ and h: R+ ↦→R (see [ 9]). A particular case is Hawkes’s self exciting
point process, for which h is nonnegative and φ(x) = ν+ x where ν> 0 (see [ 9, 24, 34]).
For instance, for seismological purposes, ν represents the spontaneous occurrences of real
original earthquakes. The function h models self-interaction: after a shock at time u, we
observe an aftershock at time t with large probability if h(t− u) is large.
These notions can be easily extended to the multivariate setting and in this case the
intensity of N(m) takes the form:
λ(m)
t = φ(m)
(M∑
ℓ=1
∫ t−
−∞
h(m)
ℓ (t− u) dN(ℓ)(u)
)
.
Theorem 7 of [ 9] gives conditions on the functions φ(m) (namely Lipschitz properties)
and on the functions h(m)
ℓ to obtain existence and uniqueness of a stationary version of
the associated process. Throughout this paper, we assume that for any m∈ {1,...,M },
φ(m)(x) = ( ν(m) + x)+,
where ν(m) >0 and ( ·)+ denotes the positive part. Note that in [ 20, 22], the case
φ(m)(x) = exp( ν(m) +x) was studied. However, Lipschitz properties required in [ 9] are not
satisﬁed in this case. By introducing, as previously, the linear predic table transformation
ψ(f) = ( ψ(1)(f),...,ψ (M)(f)) with for any m and any t
ψ(m)
t (f0) := ν(m) +
M∑
ℓ=1
∫ t−
−∞
h(m)
ℓ (t− u) dN(ℓ)(u), (1.3)
with f0 = ( ν(m),(h(m)
ℓ )ℓ=1,...,M )m=1,...,M , we have λ(m)
t = ( ψ(m)
t (f0))+. Note that the
upper integration limits in ( 1.3) are t−, that is, the integrations are all over the open
interval ( −∞,t). This assures predictability of the intensity disregarding the value s of
h(m)
ℓ (0). Alternatively, it can be assumed throughout that h(m)
ℓ (0) = 0, in which case the
integrals in ( 1.3) can be over ( −∞,t] without compromising predictability. The parameter
ν(m) is called the spontaneous rate , whereas the function h(m)
ℓ is called the interaction
function of N(ℓ) on N(m). The goal is to estimate f0 by using Lasso estimates. In the
sequel, we will assume that the support of h(m)
ℓ is bounded. By rescaling, we can then
assume that the support is in [0 ,1], and we will do so throughout. Note that in this case
we will need to observe the process on [ −1,T] in order to compute ψ(m)
t (f0) for t∈ [0,T].
Lasso and probabilistic inequalities for multivariate poi nt processes 7
The Hilbert space H associated with this setup is
H = (R × L2([0,1])M )M
=
{
f= ((µ(m),(g(m)
ℓ )ℓ=1,...,M )m=1,...,M ) :
g(m)
ℓ with support in [0 ,1] and ∥f∥2 =
∑
m
(µ(m))2 +
∑
m
∑
ℓ
∫ 1
0
g(m)
ℓ (t)2 dt< ∞
}
.
Some theoretical results are established in this general setting bu t to go further, we shall
consider in Section 5 the case where the functions h(m)
ℓ are nonnegative and then λ(m)
t
is a linear function of f0, as in Sections 1.2.1 and 1.2.2:
λ(m)
t = ψ(m)
t (f0).
The multivariate point process associated with this setup is called the multivariate
Hawkes self exciting point process (see [ 34]). In this example, M is ﬁxed and asymp-
totic properties are obtained when T tends to inﬁnity.
To the best of our knowledge, nonparametric estimation for Hawke s models has
only been proposed in [ 52] in the univariate setting where the method is based on ℓ0-
penalization of the least-squares contrast. However, due to ℓ0-penalization, the criterion
is not convex and the computational cost, in particular for the mem ory storage of all
the potential estimators, is huge. Therefore, this method has ne ver been adapted to the
multivariate setting. Moreover, the penalty term in this method is no t data-driven and
ad-hoc tuning procedures have been used for simulations. This mot ivates the present
work and the use of a convex Lasso criterion combined with data-dr iven weights, to
provide a fully implementable and theoretically valid data-driven metho d, even in the
multivariate case.
Applications in neuroscience
Hawkes processes can naturally be applied to model neuronal activ ity. Extracellular ac-
tion potentials can be recorded by electrodes and the recorded da ta for the neuron m
can be seen as a point process, each point corresponding to the pe ak of one action po-
tential of this neuron (see [
10], for instance, for precise deﬁnitions). When M neurons
are simultaneously recorded, one can assume that we are faced wit h a realization of
N = (N(m))m=1,...,M modeled by a multivariate Hawkes process. We then assume that
the intensity associated with the activity of the neuron mis given by λ(m)
t = (ψ(m)
t (f0))+,
where ψ(m)
t (f0) is given in ( 1.3). At any occurrence u<t of N(ℓ), ψ(m)
t (f0) increases (exci-
tation) or decreases (inhibition) according to the sign of h(m)
ℓ (t− u). Modeling inhibition
is essential from the neurobiological point of view. So, we cannot as sume that all inter-
action functions are nonnegative, and we cannot omit the positive p art. More details are
given in Section 6.
8 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
In neuroscience, Hawkes processes combined with maximum likelihood estimation have
been used in the seminal paper [ 22], but the application of the method requires a too huge
number of observations for realistic practical purposes. Models b ased on Hawkes processes
have nevertheless been recently discussed in neuroscience, since they constitute one of
the few simple models able to produce dependence graphs between n eurons, that may
be interpreted in neuroscience as functional connectivity graphs [45, 46]. However, many
nonparametric statistical questions arise that are not solved yet in order to furnish a fully
applicable tool for real data analysis [ 38]. We think that the Lasso-based methodology
presented in this paper may furnish the ﬁrst robust tool in this dire ction.
1.3. Notation and overview of the paper
Some notation from the general theory of stochastic integration is useful to simplify
the otherwise quite heavy notation. If H = ( H(1),...,H (M)) is a multivariate process
with locally bounded coordinates, say, and X= (X(1),...,X (M)) is a multivariate semi-
martingale, we deﬁne the real valued process H• X by
H• Xt :=
M∑
m=1
∫ t
0
H(m)
s dX(m)
s .
Given F : R ↦− →R we use F(H) to denote the coordinatewise application of F, that is
F(H) = ( F(H(1)),..., F(H(M))). In particular,
F(H) • Xt =
M∑
m=1
∫ t
0
F(H(m)
s ) dX(m)
s .
We also deﬁne the following scalar product on the space of multivariat e processes. For
any multivariate processes H= (H(1),...,H (M)) and K= (K(1),...,K (M)), we set
⟨H,K⟩proc :=
M∑
m=1
∫ T
0
H(m)
s K(m)
s ds,
the corresponding norm being denoted ∥H∥proc. Since ψ introduced in (
1.1) is linear,
the Hilbert space H inherits a bilinear form from the previous scalar product, that we
denote, for all f,g in H,
⟨f,g⟩T := ⟨ψ(f),ψ(g)⟩proc =
M∑
m=1
∫ T
0
ψ(m)
s (f)ψ(m)
s (g) ds,
and the corresponding quadratic form is denoted ∥f∥2
T .
The compensator Λ = (Λ (m))m=1,...,M of N= (N(m))m=1,...,M is ﬁnally deﬁned for all
t by
Λ (m)
t =
∫ t
0
λ(m)
s ds.
Lasso and probabilistic inequalities for multivariate poi nt processes 9
Section 2 gives our main oracle inequality and the choice of the ℓ1-weights in the
general framework of counting processes. Section 3 provides the fundamental Bernstein-
type inequality. Section 4 details the meaning of the oracle inequality in the Poisson and
Aalen setups. The probabilistic results needed for the Hawkes proc esses as well as the
interpretation of the oracle inequality in this framework is done in Sec tion 5. Simulations
on multivariate Hawkes processes are performed in Section 6. The last Section is dedicated
to the proofs of our results.
2. Lasso estimate and oracle inequality
We wish to estimate the true underlying intensity so our main goal con sists in estimating
the parameter f0. For this purpose, we assume we are given Φ a dictionary of func-
tions (whose cardinality is denoted |Φ |) and we deﬁne fa as a linear combination of the
functions of Φ, that is,
fa :=
∑
ϕ∈Φ
aϕϕ,
where a= (aϕ)ϕ∈Φ belongs to RΦ . Then, since ψ is linear, we get
ψ(fa) =
∑
ϕ∈Φ
aϕψ(ϕ).
We use the following least-squares contrast C deﬁned on H by
C(f) := −2ψ(f) • NT + ∥f∥2
T , ∀f∈ H. (2.1)
This contrast, or some variations of C, have already been used in particular setups (see,
for instance, [
52] or [ 29]). The main heuristic justiﬁcation lies in following arguments.
Since ψ(f) is a predictable process, the compensator at time T of C(f) is given by
˜C(f) := −2ψ(f) • Λ T + ∥f∥2
T ,
which can also be written as ˜C(f) = −2⟨ψ(f),λ⟩proc +∥ψ(f)∥2
proc. Note that ˜C is minimum
when ∥ψ(f) − λ∥proc is minimum. If λ= ψ(f0) and if ∥ · ∥T is a norm on the Hilbert space
H,then the unique minimizer of ˜C is f0. Therefore, to get the best linear approximation
of λ of the form ψ(f), it is natural to look at minimizers of C(f). Restricting to linear
combinations of functions of Φ, since ψ is linear, we obtain
C(fa) = −2a′b+ a′Ga,
where a′ denotes the transpose of the vector a and for ϕ1,ϕ2 ∈ Φ,
bϕ1 = ψ(ϕ1) • NT , G ϕ1,ϕ2 = ⟨ϕ1,ϕ2⟩T . (2.2)
10 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Note that both the vector b of dimension |Φ | and the Gram matrix G of dimensions
|Φ | × | Φ | are random but nevertheless observable.
To estimate a we minimize the contrast, C(fa), subject to an ℓ1-penalization on the
a-vector. That is, we introduce the following ℓ1-penalized estimator
ˆa∈ arg min
a∈RΦ
{−2a′b+ a′Ga+ 2d′|a|}, (2.3)
where |a| = ( |aϕ|)ϕ∈Φ and d∈ RΦ
+. With a good choice of d the solution of (
2.3) will
achieve both sparsity and good statistical properties. Finally, we le t ˆf = fˆa denote the
Lasso estimate associated with ˆ a.
Our ﬁrst result establishes theoretical properties of ˆf by using the classical oracle
approach. More precisely, we establish a bound on the risk of ˆf if some conditions are
true. This is a nonprobabilistic result that only relies on the deﬁnition o f ˆa by ( 2.3). In
the next section we will deal with the probabilistic aspect, which is to p rove that the
conditions are fulﬁlled with large probability.
Theorem 1. Let c> 0. If
G⪰ cI (2.4)
and if for all ϕ∈ Φ
|bϕ − ¯bϕ| ≤ dϕ, (2.5)
where
¯bϕ = ψ(ϕ) • Λ T ,
then there exists an absolute constant C, independent of c, such that
∥ψ( ˆf) − λ∥2
proc ≤ C inf
a∈RΦ
{
∥λ− ψ(fa)∥2
proc + c−1 ∑
ϕ∈S(a)
d2
ϕ
}
, (2.6)
where S(a) is the support of a. If λ= ψ(f0), the oracle inequality (
2.6) can also be
rewritten as
∥ ˆf− f0∥2
T ≤ C inf
a∈RΦ
{
∥f0 − fa∥2
T + c−1 ∑
ϕ∈S(a)
d2
ϕ
}
. (2.7)
The proof of Theorem
1 is given in Section 7.1. Note that assumption ( 2.4) ensures
that G is invertible and then coordinates of ˆ a are ﬁnite almost surely. Assumption ( 2.4)
also ensures that ∥f∥T is a real norm on f at least when f is a linear combination of the
functions of Φ.
Two terms are involved on the right-hand sides of ( 2.6) and ( 2.7). The ﬁrst one is
an approximation term and the second one can be viewed as a varianc e term providing
a control of the random ﬂuctuations of the bϕ’s around the ¯bϕ’s. Note that bϕ − ¯bϕ =
ψ(ϕ)•(N−Λ) T is a martingale (see also the comments after Theorem 2 for more details).
Lasso and probabilistic inequalities for multivariate poi nt processes 11
The approximation term can be small but the price to pay may be a larg e support of
a, leading to large values for the second term. Conversely, a sparse a leads to a small
second term. But in this case the approximation term is potentially lar ger. Note that if
the function f0 can be approximated by a sparse linear combination of the functions of
Φ, then we obtain a sharp control of ∥ ˆf− f0∥2
T . In particular, if f0 can be decomposed
on the dictionary, so we can write f0 = fa0 for some a0 ∈ RΦ , then (
2.7) gives
∥ ˆf− f0∥2
T ≤ Cc−1 ∑
ϕ∈S(a0)
d2
ϕ.
In this case, the right-hand side can be viewed as the sum of the est imation errors made
by estimating the components of a0.
Such oracle inequalities are now classical in the huge literature of Las so procedures.
See, for instance, [
5, 6, 15–18, 37, 61], who established oracle inequalities in the same
spirit as in Theorem 1. We bring out the paper [ 19], which gives technical and heuristic
arguments for justifying optimality of such oracle inequalities (see S ection 1.3 of [ 19]).
Most of these papers deal with independent data.
In the sequel, we prove that assumption ( 2.4) is satisﬁed with large probability by
using the same approach as [ 55, 56] and to a lesser extent as Section 2.1 of [ 19] or [ 54].
Section 5 is in particular mainly devoted to show that ( 2.4) holds with large probability
for the multivariate Hawkes processes.
For Theorem 1 to be of interest, the condition on the martingale, condition ( 2.5), needs
to hold with large probability as well. From this control, we deduce conv enient data-
driven ℓ1-weights that are the key parameters of our estimate. Note that our estimation
procedure does not depend on the value of cin (2.4). So knowing the latter is not necessary
for implementing our procedure. Therefore, one of the main contr ibutions of the paper is
to provide new sharp concentration inequalities that are satisﬁed b y multivariate point
processes. This is the main goal of Theorem 3 in Section 3 where we establish Bernstein
type inequalities for martingales. We apply it to the control of ( 2.5). This allows us to
derive the following result, which speciﬁes the choice of the dϕ’s needed to obtain the
oracle inequality with large probability.
Theorem 2. Let N= (N(m))m=1,...,M be a multivariate counting process with predictable
intensities λ(m)
t and almost surely ﬁnite corresponding compensator Λ (m)
t . Deﬁne
Ω V,B =
{
for any ϕ∈ Φ , sup
t∈[0,T ],m
|ψ(m)
t (ϕ)| ≤ Bϕ and (ψ(ϕ))2 • NT ≤ Vϕ
}
,
for positive deterministic constants Bϕ and Vϕ and
Ω c = {G⪰ cI}.
Let x and ε be strictly positive constants and deﬁne for all ϕ∈ Φ ,
dϕ =
√
2(1 + ε) ˆVµ
ϕ x+ Bϕx
3 , (2.8)
12 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
with
ˆVµ
ϕ = µ
µ− φ(µ) (ψ(ϕ))2 • NT + B2
ϕx
µ− φ(µ)
for a real number µ such that µ>φ (µ), where φ(u) = exp( u) − u− 1. Let us consider the
Lasso estimator ˆf of f0 deﬁned in Section 2. Then, with probability larger than
1 − 4
∑
ϕ∈Φ
(log(1 + µVϕ/(B2
ϕx))
log(1 + ε) + 1
)
e−x − P(Ω c
V,B ) − P(Ω c
c),
inequality (
2.7) is satisﬁed, that is,
∥ψ( ˆf) − λ∥2
proc ≤ C inf
a∈RΦ
{
∥λ− ψ(fa)∥2
proc + c−1 ∑
ϕ∈S(a)
d2
ϕ
}
.
If moreover λ= ψ(f0), then
∥ ˆf− f0∥2
T ≤ C inf
a∈RΦ
{
∥f0 − fa∥2
T + c−1 ∑
ϕ∈S(a)
d2
ϕ
}
,
where C is a constant independent of c, Φ , T and M.
The ﬁrst oracle inequality gives a control of the diﬀerence between the true intensity
and ψ( ˆf). The equality λ= ψ(f0) is not required and we can apply this result, for
instance, with λ= (ψ(f0))+.
Of course, the smaller the dϕ’s the better the oracle inequality. Therefore, when x in-
creases, the probability bound and the dϕ’s increase and we have to realize a compromise
to obtain a meaningful oracle inequality on an event with large probab ility. The choice of
xis deeply discussed below, in Sections
4 and 5 for theoretical purposes and in Section 6
for practical purposes.
Let us ﬁrst discuss more deeply the deﬁnition of dϕ (derived from subsequent The-
orem 3) which seems intricate. Up to a constant depending on the choice of µ and ε,
dϕ is of same order as max(
√
x(ψ(ϕ))2 • NT ,Bϕx). To give more insight on the values
of dϕ, let us consider the very special case where for any m∈ { 1,...,M } for any s,
ψ(m)
s (ϕ) = cm1{s∈Am}, where cm is a positive constant and Am a compact set included
into [0 ,T]. In this case, by naturally choosing Bϕ = max1≤m≤M cm, we have:
√
x(ψ(ϕ))2 • NT ≥ Bϕx ⇐ ⇒
M∑
m=1
c2
mN(m)
Am ≥ x max
1≤m≤M
c2
m,
where N(m)
Am represents the number of points of N(m) falling in Am. For more general
vector functions ψ(ϕ), the term
√
x(ψ(ϕ))2 • NT will dominate Bϕx if the number of
points of the process lying where ψ(ϕ) is large, is signiﬁcant. In this case, the leading
Lasso and probabilistic inequalities for multivariate poi nt processes 13
term in dϕ is expected to be the quadratic term
√
2(1 + ε) µ
µ−φ(µ) x(ψ(ϕ))2 • NT and the
linear terms in x can be viewed as residual terms. Furthermore, note that when µ tends
to 0,
µ
µ− φ(µ) = 1 + µ
2 + o(µ), x
µ− φ(µ) ∼ x
µ→ +∞
since x> 0. So, if µand ε tend to 0, the quadratic term tends to
√
2x(ψ(ϕ))2 • NT , but
the price to pay is the explosion of the linear term in x. In any case, it is possible to
make the quadratic term as close to
√
2x(ψ(ϕ))2 • NT as desired. Basically, this term
cannot be improved (see comments after Theorem 3 for probabilistic arguments).
Let us now discuss the choice of x. In more classical contexts such as density estimation
based on an n-sample, the choice x= γlog(n) plugged in the parameters analog to the
dϕ’s is convenient, since it both ensures a small probability bound and a m eaningful
order of magnitude for the oracle bound (see [ 5] for instance). See also Sections 4 and
5 for similar evaluations in our setup. But it has also been further esta blished that the
choice γ = 1 is the best. Indeed if the components of d are chosen smaller than the
analog of
√
2x(ψ(ϕ))2 • NT in the density framework, then the resulting estimator is
deﬁnitely bad from the theoretical point of view, but simulations also show that, to some
extent, if the components of d are larger than the analog of
√
2x(ψ(ϕ))2 • NT , then the
estimator deteriorates too. A similar result is out of reach in our set ting, but similar
conclusions may remain valid here since density estimation often prov ides some clues
about what happens for more intricate heteroscedastic models. I n particular, the main
heuristic justifying the optimality of this tuning result in the density s etting is that the
quadratic term (and in particular the constant
√
2 ) corresponds to the rate of the central
limit theorem and in this sense, it provides the “best approximation” f or the ﬂuctuations.
For further discussion, see the simulation study in Section 6.
Finally, it remains to control P(Ω V,B ) and P(Ω c). These are the goals of Section 4 for
Poisson and Aalen models and Section 5 for multivariate Hawkes processes.
3. Bernstein type inequalities for multivariate point
processes
We establish a Bernstein type concentration inequality based on bou ndedness assump-
tions. This result, which has an interest per se from the probabilistic point of view, is the
key result to derive the convenient values for the vector din Theorem 2 and so is capital
from the statistical perspective.
Theorem 3. Let N= (N(m))m=1,...,M be a multivariate counting process with predictable
intensities λ(m)
t and corresponding compensator Λ (m)
t with respect to some given ﬁltration.
Let B> 0. Let H= (H(m))m=1,...,M be a multivariate predictable process such that for
all ξ∈ (0,3), for all t,
exp(ξH/B) • Λ t <∞ a.s. and exp(ξH2/B2) • Λ t <∞ a.s. (3.1)
14 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let us consider the martingale deﬁned for all t≥ 0 by
Mt = H• (N− Λ) t.
Let v>w and x be positive constants and let τ be a bounded stopping time. Let us deﬁne
ˆVµ = µ
µ− φ(µ) H2 • Nτ + B2x
µ− φ(µ)
for a real number µ∈ (0,3) such that µ>φ (µ), where φ(u) = exp( u) − u− 1. Then, for
any ε> 0,
P
(
Mτ ≥
√
2(1 + ε) ˆVµx+ Bx
3 and w≤ ˆVµ ≤ v and sup
m,t≤τ
|H(m)
t | ≤ B
)
(3.2)
≤ 2
(log(v/w)
log(1 + ε) + 1
)
e−x.
This result is based on the exponential martingale for counting proc esses, which has
been used for a long time in the context of the counting process the ory. See, for instance,
[8, 57] or [ 59]. This basically gives a concentration inequality taking the following for m
(the result is stated here in its univariate form for comparison purp oses): for any x> 0,
P
(
Mτ ≥
√
2ρx+ Bx
3 and
∫ τ
0
H2
s λ(s) ds≤ ρ and sup
s∈[0,τ ]
|Hs| ≤ B
)
≤ e−x. (3.3)
In ( 3.3), ρ is a deterministic upper bound of v=
∫τ
0 H2
s λ(s) ds, the bracket of the mar-
tingale, and consequently the martingale equivalent of the variance term. Moreover, B is
a deterministic upper bound of sup s∈[0,τ ] |Hs|. The leading term for moderate values of
x and τ large enough is consequently √2ρx. The central limit theorem for martingales
states that, under some assumptions, a sequence of martingales (Mn)n with respective
brackets ( vn)n tending to a deterministic value ¯ v, once correctly normalized, tends to a
Gaussian process with bracket ¯v. Therefore, a term of the form
√
2¯vxin the upper bound
is not improvable, in particular the constant
√
2. However the replacement of the limit ¯ v
by a deterministic upper bound ρ constitutes a loss. In this sense, Theorem 3 improves
the bound and consists of plugging in the unbiased estimate ˆ v= ∫τ
0 H2
s dNs instead of a
nonsharp deterministic upper bound of v. Note that we are not able to obtain exactly
the term
√
2 but any value strictly larger than
√
2, as close as we want to
√
2 up to some
additive terms depending on B that are negligible for moderate values of x.
The proof is based on a peeling argument that was ﬁrst introduced in [40] for Gaussian
processes and is given in Section 7.3.
Note that there exist also inequalities that seem nicer than ( 3.3) which constitutes the
basic brick for our purpose. For instance, in [ 27], it is established that for any deterministic
positive real number θ, for any x> 0,
P
(
Mτ ≥
√
2θx and
∫ τ
0
H2
s dΛ s +
∫ τ
0
H2
s dNs ≤ θ
)
≤ e−x. (3.4)
Lasso and probabilistic inequalities for multivariate poi nt processes 15
At ﬁrst sight, this seems better than Theorem 3 because no linear term depending on B
appears, but if we wish to use the estimate 2
∫τ
0 H2
s dNs instead of θ in the inequality, we
have to bound |Hs| by some B in any case. Moreover, by doing so, the quadratic term
will be of order
√
4ˆvx which is worse than the term
√
2ˆvx derived in Theorem 3, even if
this constant
√
2 can only be reached asymptotically in our case.
There exists a better result if the martingale Mt is for instance conditionally symmetric
(see [ 4, 25, 27]): for any x> 0,
P
(
Mτ ≥
√
2κx and
∫ τ
0
H2
s dNs ≤ κ
)
≤ e−x, (3.5)
which seems close to the ideal inequality. But there are actually two m ajor problems
with this inequality. First, one needs to assume that the martingale is conditionally
symmetric, which cannot be the case in our situation for general co unting processes and
general dictionaries. Second, it depends on the deterministic uppe r bound κinstead of ˆv.
To replace κby ˆv and then to apply peeling arguments as in the proof of Theorem 3, we
need to assume the existence of a positive constant w such that ˆv≥ w. But if the process
is empty, then ˆ v= 0, so we cannot generally ﬁnd such a positive lower bound, whereas
in our theorem, we can always take w= B2x
µ−φ(µ) as a lower bound for ˆVµ.
Finally, note that in Proposition 6 (see Section 7.3), we also derive a similar bound
where ˆVµ is replaced by
∫τ
0 H2
s dΛ s. Basically, it means that the same type of results hold
for the quadratic characteristic instead of the quadratic variatio n. Though this result is
of little use here, since the quadratic characteristic is not observa ble, we think that it
may be of interest for readers investigating self-normalized result s as in [ 26].
4. Applications to the Poisson and Aalen models
We now apply Theorem
2 to the Poisson and Aalen models. The case of the multivariate
Hawkes process, which is much more intricate, will be the subject of the next section.
4.1. The Poisson model
Let us recall that in this case, we observe M i.i.d. Poisson processes with intensity f0
supported by [0 ,1] (with M≥ 2) and that the norm is given by ∥f∥2 =
∫1
0 f2(x) dx. We
assume that Φ is an orthonormal system for ∥ · ∥ . In this case,
∥ · ∥ 2
1 = M∥ · ∥ 2 and G= MI,
where I is the identity matrix. One applies Theorem
2 with c= M (so P(Ω c
c) = 0) and
Bϕ = ∥ϕ∥∞, V ϕ = ∥ϕ∥2
∞(1 + δ)Mm1,
16 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
for δ> 0 and m1 =
∫1
0 f0(t) dt. Note that here T = 1 and therefore N(m)
T = N(m)
1 is the
total number of observed points for the mth process. Using
ψ(ϕ)2 • N1 ≤ ∥ϕ∥2
∞
M∑
m=1
N(m)
1
and since the distribution of ∑ M
m=1 N(m)
1 is the Poisson distribution with parameter
Mm1, Cramer–Chernov arguments give:
P(Ω c
V,B ) ≤ P
( M∑
m=1
N(m)
1 >(1 + δ)Mm1
)
≤ exp(−{(1 + δ) ln(1 + δ) − δ}Mm1).
For α> 0, by choosing x= αlog(M), we ﬁnally obtain the following corollary derived
from Theorem
2.
Corollary 1. With probability larger than 1 − C1
|Φ | log(M)
Mα − e−C2M , where C1 is a con-
stant depending on µ, ε, α, δ and m1 and C2 is a constant depending on δ and m1, we
have:
∥ ˆf− f0∥2 ≤ C inf
a∈RΦ
{
∥f0 − fa∥2
+ 1
M2
∑
ϕ∈S(a)
(
log(M)
M∑
m=1
∫ 1
0
ϕ2(x) dN(m)
x + log2(M)∥ϕ∥2
∞
)}
,
where C is a constant depending on µ, ε, α, δ and m1.
To shed some lights on this result, consider an asymptotic perspect ive by assuming
that M is large. Assume also, for sake of simplicity, that f0 is bounded from below on
[0,1]. If the dictionary Φ (whose size may depend on M) satisﬁes
max
ϕ∈Φ
∥ϕ∥∞ = o
(√
M
log M
)
,
then, since, almost surely,
1
M
M∑
m=1
∫ 1
0
ϕ2(x) dN(m)
x
M→∞
− →
∫ 1
0
ϕ2(x)f0(x) dx,
almost surely,
1
M2
∑
ϕ∈S(a)
(
log(M)
M∑
m=1
∫ 1
0
ϕ2(x) dN(m)
x + log2(M)∥ϕ∥2
∞
)
Lasso and probabilistic inequalities for multivariate poi nt processes 17
= log M
M
∑
ϕ∈S(a)
∫ 1
0
ϕ2(x)f0(x) dx× (1 + o(1)) .
The right-hand term corresponds, up to the logarithmic term, to t he sum of variance
terms when estimating
∫1
0 ϕ(x)f0(x) dx with 1
M
∑ M
m=1
∫1
0 ϕ(x) dN(m)
x for ϕ∈ S(a). This
means that the estimator adaptively achieves the best trade-oﬀ b etween a bias term and
a variance term. The logarithmic term is the price to pay for adaptat ion. Furthermore,
when M→ +∞, the inequality of Corollary
1 holds with probability that goes to 1 at a
polynomial rate. We refer the reader to [ 50] for a deep discussion on optimality of such
results.
4.2. The Aalen model
Results similar to those presented in this paragraph can be found in [
29] under re-
stricted eigenvalues conditions instead of ( 2.4). Recall that we observe an M-sample
(X(m),Y (m),N(m))m=1,...,M , with Y(m) = ( Y(m)
t )t∈[0,1] and N(m) = ( N(m)
t )t∈[0,1] (with
M≥ 2). We assume that X(m) ∈ [0,1] and that the intensity of N(m)
t is f0(t,X(m))Y(m)
t .
We set for any f,
∥f∥2
e := E
(∫ 1
0
f2(t,X(1))(Y(1)
t )2 dt
)
.
We assume that Φ is an orthonormal system for ∥ · ∥ 2, the classical norm on L2([0,1]2),
and we assume that there exists a positive constant r such that
∀f∈ L2([0,1]2), ∥f∥e ≥ r∥f∥2, (4.1)
so that ∥ · ∥e is a norm. If we assume, for instance, that the density of the X(m)’s is lower
bounded by a positive constant c0 and there exists c1 >0 such that for any t,
E[(Y(1)
t )2|X(1)] ≥ c1
then (
4.1) holds with r2 = c0c1. The empirical version of ∥f∥e, denoted ∥f∥emp, is deﬁned
by
∥f∥2
emp := 1
M∥f∥2
T = 1
M
M∑
m=1
∫ 1
0
f2(t,X(m))(Y(m)
t )2 dt.
Unlike the Poisson model, since the intensity depends on covariates X(m)’s and variables
Y(m)’s, the control of P(Ω c
c) is much more cumbersome for the Aalen case, even if it
is less intricate than for Hawkes processes (see Section
5). We have the following result
proved in Section 7.5.1.
18 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Proposition 1. We assume that ( 4.1) is satisﬁed, the density of the covariates X(m) is
bounded by D and
sup
t∈[0,1]
max
m∈{1,...,M}
Y(m)
t ≤ 1 almost surely . (4.2)
We consider an orthonormal dictionary Φ of functions of L2([0,1]2) that may depend
on M, and we let rΦ denote the spectral radius of the matrix H whose components are
Hϕ,ϕ′ = ∫∫ |ϕ(t,x)||ϕ′(t,x)| dtdx. Then, if
max
ϕ∈Φ
∥ϕ∥2
∞ × rΦ |Φ | × log M
M → 0, (4.3)
when M→ +∞ then, for any β> 0, there exists C1 >0 depending on β, D and f0 such
that with c= C1M, we have
P(Ω c
c) = O( |Φ |2M−β).
Assumption (
4.2) is usually satisﬁed in most of the practical examples where Aalen
models are involved. See [ 2] for explicit examples and see, for instance, [ 30, 49] for other
articles where this assumption is made. In the sequel, we also assume that there exists a
positive constant R such that
max
m∈{1,...,M}
N(m)
1 ≤ R a.s. (4.4)
This assumption, considered by [ 49], is obviously satisﬁed in survival analysis where there
is at most one death per individual. It could have been relaxed in our se tting, by consid-
ering exponential moments assumptions, to include Markov cases f or instance. However,
this much simpler assumption allows us to avoid tedious and unnecessa ry technical as-
pects since we only wish to illustrate our results in a simple framework. Under ( 4.2) and
(4.4), almost surely,
ψ(ϕ)2 •NT =
M∑
m=1
∫ 1
0
[Y(m)
t ]2ϕ2(t,X(m)) dN(m)
t ≤
M∑
m=1
∫ 1
0
ϕ2(t,X(m)) dN(m)
t ≤ MR∥ϕ∥2
∞.
So, we apply Theorem
2 with Bϕ = ∥ϕ∥∞, Vϕ = MR∥ϕ∥2
∞ (so P(Ω V,B ) = 1) and x=
αlog(M) for α> 0. We ﬁnally obtain the following corollary.
Corollary 2. Assume that (
4.2) and ( 4.4) are satisﬁed. With probability larger than
1 − C1
|Φ | log(M)
Mα − P(Ω c
c), where C1 is a constant depending on µ, ε, α and R, we have:
∥ ˆf− f0∥2
emp ≤ C inf
a∈RΦ
{
∥f0 − fa∥2
emp
+ 1
M2
∑
ϕ∈S(a)
(
log(M)
M∑
m=1
∫ 1
0
ϕ2(t,X(m)) dN(m)
t + log2(M)∥ϕ∥2
∞
)}
,
Lasso and probabilistic inequalities for multivariate poi nt processes 19
where C is a constant depending on µ, ε, α and R.
To shed lights on this result, assume that the density of the X(m)’s is upper bounded
by a constant ˜R. In an asymptotic perspective with M→ ∞, we have almost surely,
1
M
M∑
m=1
∫ 1
0
ϕ2(t,X(m)) dN(m)
t → E
(∫ 1
0
ϕ2(t,X(1))f0(t,X(1))Y(1) dt
)
.
But
E
(∫ 1
0
ϕ2(t,X(1))f0(t,X(1))Y(1) dt
)
≤ ∥f0∥∞E
(∫ 1
0
ϕ2(t,X(1)) dt
)
≤ ˜R∥f0∥∞.
So, if the dictionary Φ satisﬁes
max
ϕ∈Φ
∥ϕ∥∞ = O
(√
M
log M
)
,
which is true under ( 4.3) if rΦ |Φ | ≥ 1, then, almost surely, the variance term is asymp-
totically smaller than log( M)|S(a)|∥f0∥∞
M up to constants. So, we can draw the same
conclusions as for the Poisson model. We have not discussed here th e choice of Φ and
Condition ( 4.3). This will be extensively done in Section 5.2 where we deal with a similar
condition but in a more involved setting.
5. Applications to the case of multivariate Hawkes
process
For a multivariate Hawkes model, the parameter f0 = (ν(m),(h(m)
ℓ )ℓ=1,...,M )m=1,...,M be-
longs to
H = HM =
{
f= (f(m))m=1,...,M
⏐
⏐
⏐f(m) ∈ H and ∥f∥2 =
M∑
m=1
∥f(m)∥2
}
where
H =
{
f = (µ,(gℓ)ℓ=1,...,M )
⏐
⏐
⏐µ∈ R,gℓ with support in [0 ,1]
and ∥f∥2 = µ2 +
M∑
ℓ=1
∫ 1
0
g2
ℓ (x) dx< ∞
}
.
If one deﬁnes the linear predictable transformation κ of H by
κt(f) = µ+
M∑
ℓ=1
∫ t−
t−1
gℓ(t− u) dN(ℓ)
u , (5.1)
20 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
then the transformation ψ on H is given by
ψ(m)
t (f) = κt(f(m)).
The ﬁrst oracle inequality of Theorem 2 provides theoretical guaranties of our Lasso
methodology in full generality and in particular, even if inhibition takes place (see Sec-
tion 1.2.3). Since Ω V,B and Ω c are observable events, we know whether the oracle in-
equality holds. However we are not able to determine P(Ω V,B ) and P(Ω c) in the general
case. Therefore, in Sections 5.1 and 5.2, we assume that all interaction functions are
nonnegative and that there exists f0 in H so that for any m and any t,
λ(m)
t = ψ(m)
t (f0).
We also assume that the process is observed on [ −1,T] with T >1.
5.1. Some useful probabilistic results for multivariate Hawkes
processes
In this paragraph, we present some particular exponential result s and tail controls for
Hawkes processes. As far as we know, these results are new: The y constitute the gener-
alization of [ 51] to the multivariate case. In this paper, they are used to control P(Ω c
c)
and P(Ω c
V,B ) but they may be of independent interest.
Since the functions h(m)
ℓ ’s are nonnegative, a cluster representation exists. We can
indeed construct the Hawkes process by the Poisson cluster repr esentation (see [
24]) as
follows:
• Distribute ancestral points with marks ℓ= 1,...,M according to homogeneous Pois-
son processes with intensities ν(ℓ) on R.
• For each ancestral point, form a cluster of descendant points. M ore precisely, start-
ing with an ancestral point at time 0 of a certain type, we successive ly build new
generations as Poisson processes with intensity h(m)
ℓ (· − T), where T is the parent
of type ℓ (the corresponding children being of type m). We will be in the situation
where this process becomes extinguished and we denote by H the last children of all
generations, which also represents the length of the cluster. Not e that the number of
descendants is a multitype branching process (and there exists a b ranching cluster
representation (see [ 9, 24, 34])) with oﬀspring distributions being Poisson variables
with means
γℓ,m =
∫ 1
0
h(m)
ℓ (t) dt.
The essential part we need is that the expected number of oﬀsprin gs of type m from
a point of type ℓ is γℓ,m. With Γ = ( γℓ,m)ℓ,m=1,...,M , the theory of multitype branching
processes gives that the clusters are ﬁnite almost surely if the spe ctral radius of Γ is
Lasso and probabilistic inequalities for multivariate poi nt processes 21
strictly smaller than 1. In this case, there is a stationary version of the Hawkes process
by the Poisson cluster representation.
Moreover, if Γ has spectral radius strictly smaller than 1, one can p rovide a bound on
the number of points in a cluster. We denote by Pℓ the law of the cluster whose ancestral
point is of type ℓ, Eℓ is the corresponding expectation.
The following lemma is very general and holds even if the function h(m)
ℓ have inﬁnite
support as long as the spectral radius Γ is strictly less than 1.
Lemma 1. If W denotes the total number of points of any type in the cluster w hose
ancestral point is of type ℓ, then if the spectral radius of Γ is strictly smaller than 1 there
exists ϑℓ >0, only depending on ℓ and on Γ , such that
Eℓ(eϑℓW ) <∞.
This easily leads to the following result, which provides the existence o f the Laplace
transform of the total number of points in an arbitrary bounded in terval, when the
functions h(m)
ℓ have bounded support.
Proposition 2. Let N be a stationary multivariate Hawkes process, with compactl y sup-
ported nonnegative interactions functions and such that th e spectral radius of Γ is strictly
smaller than 1. For any A> 0, let N[−A,0) be the total number of points of N in [−A,0),
all marks included. Then there exists a constant θ> 0, depending on the distribution of
the process and on A, such that
E := E(eθN[−A,0) ) <∞,
which implies that for all positive u
P(N[−A,0) ≥ u) ≤ E e−θu.
Moreover, one can strengthen the ergodic theorem in a nonasymp totic way, under the
same assumptions.
Proposition 3. Under the assumptions of Proposition
2, let A> 0 and let Z(N) be a
function depending on the points of N lying in [−A,0). Assume that there exist b and η
nonnegative constants such that
|Z(N)| ≤ b(1 + Nη
[−A,0)),
where N[−A,0) represents the total number of points of N in [−A,0), all marks included.
We denote S the shift operator, meaning that Z◦ St(N) depends now in the same way
as Z(N) on some points that are now the points of N lying in [t− A,t).
We assume E[|Z(N)|] <∞ and for short, we denote E(Z) = E[Z(N)]. Then, for any
α> 0, there exists a constant T (α,η,f 0,A) >1 such that for T >T (α,η,f 0,A), there
22 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
exist C1, C2, C3 and C4 positive constants depending on α,η,A and f0 such that
P
(∫ T
0
[Z◦ St(N) − E(Z)] dt≥ C1σ
√
Tlog3(T) + C2b(log(T))2+η
)
≤ C4
Tα ,
with σ2 = E([Z(N) − E(Z)]21N[−A,0)≤ ˜N ) and ˜N = C3 log(T).
Finally, to deal with the control of P(Ω c), we shall need the next result. First, we deﬁne
a quadratic form Q on H by
Q(f,g) = EP(κ1(f)κ1(g)) = EP
(1
T
∫ T
0
κt(f)κt(g) dt
)
, f,g ∈ H. (5.2)
We have:
Proposition 4. Under the assumptions of Proposition
2, if the function parameter f0
satisﬁes
min
m∈{1,...,M}
ν(m) >0 and max
l,m∈{1,...,M}
sup
t∈[0,1]
h(m)
ℓ (t) <∞ (5.3)
then there is a constant ζ> 0 such that for any f ∈ H,
Q(f,f) ≥ ζ∥f∥2.
We are now ready to establish oracle inequalities for multivariate Hawk es processes.
5.2. Lasso for Hawkes processes
In the sequel, we still consider the main assumptions of the previous subsection: We deal
with a stationary Hawkes process whose intensity is given by (
1.3) such that the spectral
radius of Γ is strictly smaller than 1 and ( 5.3) is satisﬁed. We recall that the components
of Γ are the γℓ,m’s with
γℓ,m =
∫ 1
0
h(m)
ℓ (t) dt.
One of the main results of this section is to link properties of the dictio nary (mainly
orthonormality but also more involved assumptions) to properties o f G (the control of
Ω c). To do so, let us deﬁne for all f∈ H,
∥f∥∞ = max
{
max
m=1,...,M
|µ(m)|, max
m,ℓ=1,...,M
∥g(m)
ℓ ∥∞
}
.
Then, let us set ∥Φ ∥∞ := max{∥ϕ∥∞,ϕ ∈ Φ }. The next result is based on the probabilistic
results of Section 5.1.
Lasso and probabilistic inequalities for multivariate poi nt processes 23
Proposition 5. Assume that the Hawkes process is stationary, that ( 5.3) is satisﬁed and
that the spectral radius of Γ is strictly smaller than 1. Let rΦ be the spectral radius of the
matrix H deﬁned by
H =
(∑
m
[
|µ(m)
ϕ ||µ(m)
ρ | +
M∑
ℓ=1
∫ 1
0
|(gϕ)(m)
ℓ ||(gρ)(m)
ℓ |(u) du
])
ϕ,ρ∈Φ
.
Assume that Φ is orthonormal and that
AΦ (T) := rΦ ∥Φ ∥2
∞|Φ |[log(∥Φ ∥∞) + log( |Φ |)]log5(T)
T → 0 (5.4)
when T → ∞. Then, for any β> 0, there exists C1 >0 depending on β and f0 such that
with c= C1T, we have
P(Ω c
c) = O( |Φ |2T−β).
Up to logarithmic terms, (
5.4) is similar to ( 4.3) with M replaced with T. The dic-
tionary Φ can be built via a dictionary (Υ k)k=1,...,K of functions of L2([0,1]) (that may
depend on T) in the following way. A function ϕ= (µ(m)
ϕ ,((gϕ)(m)
ℓ )ℓ)m belongs to Φ if
and only if only one of its M+ M2 components is nonzero and in this case,
• if µ(m)
ϕ ̸= 0, then µ(m)
ϕ = 1,
• if ( gϕ)(m)
ℓ ̸= 0, then there exists k∈ {1,...,K } such that ( gϕ)(m)
ℓ = Υ k.
Note that |Φ | = M + KM2. Furthermore, assume from now on that (Υ k)k=1,...,K is
orthonormal in L2([0,1]). Then Φ is also orthonormal in H endowed with ∥ · ∥ .
Before going further, let us discuss assumption ( 5.4). First, note that the matrix H is
block diagonal. The ﬁrst block is the identity matrix of size M. The other M2 blocks are
identical to the matrix:
HK =
(∫
|Υ k1 (u)||Υ k2 (u)| du
)
1≤k1,k2≤K
.
So, if we denote ˜ rK the spectral radius of HK , we have:
rΦ = max(1,˜rK ).
We analyze the behavior of ˜ rK with respect to K. Note that for any k1 and any k2,
(HK )k1,k2 ≥ 0.
Therefore,
˜rK ≤ sup
∥x∥ℓ1 =1
∥HK x∥ℓ1 ≤ max
k1
∑
k2
(HK )k1,k2 .
We now distinguish three types of orthonormal dictionaries (remem ber that M is viewed
as a constant here):
24 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
• Let us consider regular histograms. The basis is composed of the fu nctions Υ k =
δ−1/21((k−1)δ,kδ] with Kδ= 1. Therefore, ∥Φ ∥∞ = δ−1/2 =
√
K. But HK is the iden-
tity matrix and ˜rK = 1. Hence, ( 5.4) is satisﬁed as soon as
K2 log(K) log5(T)
T → 0
when T → ∞, which is satisﬁed if K= o(
√
T
log3(T ) ).
• Assume that ∥Φ ∥∞ is bounded by an absolute constant (Fourier dictionaries satisfy
this assumption). Since ˜ rK ≤ K, ( 5.4) is satisﬁed as soon as
K2 log(K) log5(T)
T → 0
when T → ∞, which is satisﬁed if K= o(
√
T
log3(T ) ).
• Assume that (Υ k)k=1,...,K is a compactly supported wavelet dictionary where reso-
lution levels belong to the set {0,1,...,J }. In this case, K is of the same order as
2J , ∥Φ ∥∞ is of the same order as 2 J/2 and it can be established that ˜ rK ≤ C2J/2
where C is a constant only depending on the choice of the wavelet system (se e [ 33]
for further details). Then, ( 5.4) is satisﬁed as soon as
K5/2 log(K) log5(T)
T → 0
when T → ∞, which is satisﬁed if K= o( T 2/5
log12/5(T ) ).
To apply Theorem 2, it remains to control Ω V,B . Note that
ψ(m)
t (ϕ) =



1 if µ(m)
ϕ = 1,∫ t−
t−1
Υ k(t− u) dN(ℓ)
u if ( gϕ)(m)
ℓ = Υ k.
Let us deﬁne
Ω N = {for all t∈ [0,T],for all m∈ {1,...,M } we have N(m)
[t−1,t] ≤ N }.
We therefore set
Bϕ = 1 if µ(m)
ϕ = 1 and Bϕ = ∥Υ k∥∞N if ( gϕ)(m)
ℓ = Υ k. (5.5)
Note that on Ω N , for any ϕ∈ Φ,
sup
t∈[0,T ],m
|ψ(m)
t (ϕ)| ≤ Bϕ.
Lasso and probabilistic inequalities for multivariate poi nt processes 25
Now, for each ϕ∈ Φ, let us determine Vϕ that constitutes an upper bound of
Mϕ =
M∑
m=1
∫ T
0
[ψ(m)
t (ϕ)]2 dN(m)
t .
Note that only one term in this sum is nonzero. We set
Vϕ = ⌈T⌉N if µ(m)
ϕ = 1 and Vϕ = ∥Υ k∥2
∞⌈T⌉N 3 if ( gϕ)(m)
ℓ = Υ k, (5.6)
where ⌈T⌉ denotes the smallest integer larger than T. With this choice, one has that
Ω N ⊂ Ω V,B , which leads to the following result.
Corollary 3. Assume that the Hawkes process is stationary, that (
5.3) is satisﬁed and
that the spectral radius of Γ is strictly smaller than 1. With the choices ( 5.5) and ( 5.6),
P(Ω V,B ) ≥ P(Ω N ) ≥ 1 − C1Texp(−C2N ),
where C1 and C2 are positive constants depending on f0.
If N ≫ log(T), then for all β> 0,
P(Ω c
V,B ) ≤ P(Ω c
N ) = o( T−β).
We are now ready to apply Theorem
2.
Corollary 4. Assume that the Hawkes process is stationary, that ( 5.3) is satisﬁed and
that the spectral radius of Γ is strictly smaller than 1. Assume that the dictionary Φ
is built as previously from an orthonormal family (Υ k)k=1,...,K . With the notations of
Theorem 2, let Bϕ be deﬁned by ( 5.5) and dϕ be deﬁned accordingly with x= αlog(T).
Then, with probability larger than
1 − 4(M+ M2K)
(log(1 + µ⌈T⌉N /(αlog(T)))
log(1 + ε) + 1
)
T−α − P(Ω c
N ) − P(Ω c
c),
1
T∥ ˆf− f0∥2
T ≤ C inf
a∈RΦ
{ 1
T∥f0 − fa∥2
T +
∑
ϕ∈S(a)
(log(T)(ψ(ϕ))2 • NT
T2 + B2
ϕ log2(T)
T2
)}
,
where C is a constant depending on f0, µ, ε, and α.
From an asymptotic point of view, if the dictionary also sati sﬁes ( 5.4), and if N =
log2(T) in ( 5.5), then for T large enough with probability larger than 1 − C1Klog(T)T−α
1
T∥ ˆf− f0∥2
T ≤ C2 inf
a∈RΦ
{ 1
T∥f0 − fa∥2
T + log3(T)
T
∑
ϕ∈S(a)
[ 1
T∥ϕ∥2
T + log7/2(T)
√
T
∥Φ ∥2
∞
]}
,
where C1 and C2 are constants depending on M, f0, µ, ε, and α.
26 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
We express the oracle inequality by using 1
T ∥ · ∥T simply because, when T goes to + ∞,
by ergodicity of the process (see, for instance, [ 24], and Proposition 3 for a nonasymptotic
statement),
1
T∥f∥2
T =
M∑
m=1
1
T
∫ T
0
(κt(f(m)))2 dt− →
M∑
m=1
Q(f(m),f(m))
under assumptions of Proposition 5. Note that the right-hand side is a true norm on H
by Proposition 4. Note also that
log7/2(T)√
T
∥Φ ∥2
∞
T →∞
→ 0,
as soon as (
5.4) is satisﬁed for the Fourier basis and compactly supported wavelet s. It
is also the case for histograms as soon as K= o(
√
T
log7/2(T ) ). Therefore, this term can be
viewed as a residual one. In those cases, the last inequality can be r ewritten as
1
T∥ ˆf− f0∥2
T ≤ C inf
a∈RΦ
{ 1
T∥f0 − fa∥2
T + log3(T)
T
∑
ϕ∈S(a)
1
T∥ϕ∥2
T
}
,
for a diﬀerent constant C, the probability of this event tending to 1 as soon as α≥ 1/2
in the Fourier and histogram cases and α≥ 2/5 for the compactly supported wavelet
basis. Once again, as mentioned for the Poisson or Aalen models, the right-hand side
corresponds to a classical “bias-variance” trade oﬀ and we obtain a classical oracle in-
equality up to the logarithmic terms. Note that asymptotics is now wit h respect to T
and not with respect to M as for Poisson or Aalen models. So, the same result, namely
Theorem
2, allows to consider both asymptotics.
6. Simulations for the multivariate Hawkes process
This section is devoted to illustrations of our procedure on simulated data of multivari-
ate Hawkes processes and comparisons with the well-known adaptiv e Lasso procedure
proposed by [
64]. We consider the general case and we do no longer assume that the
functions h(m)
ℓ are nonnegative as in Section 5. However, if the parameter ν(m) is large
with respect to the h(m)
ℓ ’s, then ψ(m)(f0) is nonnegative with large probability and there-
fore λ(m) = ψ(m)(f0) with large probability. Hence, Theorem 2 implies that ˆf is close to
f0.
6.1. Description of the data
As mentioned in the introduction, Hawkes processes can be used in n euroscience to model
the action potentials of individual neurons. So, we perform simulatio ns whose parameters
Lasso and probabilistic inequalities for multivariate poi nt processes 27
are close, to some extent, to real neuronal data. For a given neu ron m∈ {1,...,M }, we
recall that its activity is modeled by a point process N(m) whose intensity is
λ(m)
t =
(
ν(m) +
M∑
ℓ=1
∫ t−
−∞
h(m)
ℓ (t− u) dN(ℓ)(u)
)
+
.
The interaction function h(m)
ℓ represents the inﬂuence of the past activity of the neuron
ℓon the neuron m. The spontaneous rate ν(m) may somehow represent the external exci-
tation linked to all the other neurons that are not recorded. It is c onsequently of crucial
importance not only to correctly infer the interaction functions, b ut also to reconstruct
the spontaneous rates accurately. Usually, activity up to 10 neur ons can be recorded in
a “stationary” phase during a few seconds (sometimes up to one min ute). Typically, the
points frequency is of the order of 10–80 Hz and the interaction ra nge between points is
of the order of a few milliseconds (up to 20 or 40 ms). We ﬁrst lead thr ee experiments in
the pure excitation case where all the interaction functions are nonnegative by simulating
multivariate Hawkes processes (two with M= 2, one with M= 8) based on these typical
values. More precisely, we take for any m∈ { 1,...,M }, ν(m) = 20 and the interaction
functions h(m)
ℓ are deﬁned as follows (supports of all the functions are assumed t o lie in
the interval [0 ,0.04]):
Experiment 1 (M= 2 and piecewise constant functions).
h(1)
1 = 30 × 1(0,0.02], h (1)
2 = 30 × 1(0,0.01], h (2)
1 = 30 × 1(0.01,0.02], h (2)
2 = 0.
In this case, each neuron depends on the other one. The spectr al radius of the matrix Γ
is 0.725.
Experiment 2 (M= 2 and “smooth” functions). In this experiment, h(1)
1 and h(2)
1
are not piecewise constant.
h(1)
1 (x) = 100e −200x × 1(0,0.04](x), h (1)
2 (x) = 30 × 1(0,0.02](x),
h(2)
1 (x) = 1
0.008
√
2π
e− (x−0.02)2
2∗0.0042 × 1(0,0.04](x), h (2)
2 (x) = 0 .
In this case, each neuron depends on the other one as well. The spectral radius of the
matrix Γ is 0.711.
Experiment 3 (M= 8 and piecewise constant functions).
h(1)
2 = h(1)
3 = h(2)
2 = h(3)
1 = h(3)
2 = h(5)
8 = h(6)
5 = h(7)
6 = h(8)
7 = 25 × 1(0,0.02]
and all the other 55 interaction functions are equal to 0. Note in particular tha t this leads
to 3 independent groups of dependent neurons {1,2,3}, {4} and {5,6,7,8}. The spectral
radius of the matrix Γ is 0.5.
28 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Figure 1. Raster plots of two data sets with T = 2 corresponding to Experiment 2 on the left
and Experiment 3 on the right. The x-axis correspond to the time of the experiment. Each
line with ordinate m corresponds to the points of the process N(m). From bottom to top, we
observe 124 and 103 points for Experiment 2 and 101, 60, 117, 38, 73, 75, 86 and 86 points for
Experiment 3.
We also lead one experiment in the pure inhibition case where all the interaction
functions are nonpositive:
Experiment 4 (M= 2). In this experiment, the interaction functions are the oppos ite
of the functions introduced in Experiment
2. We take for any m∈ {1,...,M }, ν(m) = 60
so that ψt(f0) is positive with high probability.
For each simulation, we let the process “warm up” during 1 second to reach the station-
ary state. 1 Then the data are collected by taking recordings during the next T seconds.
For instance, we record about 100 points per neuron when T = 2 and 1000 points when
T = 20. Figure 1 shows two instances of data sets with T = 2.
6.2. Description of the methods
To avoid approximation errors when computing the matrix G, we focus on a dictio-
nary (Υ k)k=1,...,K whose functions are piecewise constant. More precisely, we take Υ k =
δ−1/21((k−1)δ,kδ] with δ= 0.04/K and K, the size of the dictionary, is chosen later.
Our practical procedure strongly relies on the theoretical one ba sed on the dϕ’s deﬁned
in (
2.8), with x, µ and ε to be speciﬁed. First, using Corollary 4, we naturally take
1Note that since the size of the support of the interaction functions is less or equal to 0.04, the “warm
up” period is 25 times the interaction range.
Lasso and probabilistic inequalities for multivariate poi nt processes 29
x= αlog(T). Then, three hyperparameters would need to be tuned, namely α, µ and ε,
if we directly used the Lasso estimate of Theorem 2. So, for simpliﬁcations, we implement
our procedure by replacing the Lasso parameters dϕ with
˜dϕ(γ) =
√
2γlog(T)(ψ(ϕ))2 • NT + γlog(T)
3 sup
t∈[0,T ],m
|ψ(m)
t (ϕ)|,
where γ is a constant to be tuned. Besides taking x= αlog(T), our modiﬁcation consists
in neglecting the linear part
B2
ϕx
µ−φ(µ) in ˆVµ and replacing Bϕ with sup t∈[0,T ],m |ψ(m)
t (ϕ)|.
Then, note that, up to these modiﬁcations, the choice γ= 1 corresponds to the limit
case where α→ 1, ε→ 0 and µ→ 0 in the deﬁnition of the dϕ’s (see the comments after
Theorem 2). Note also that, under the slight abuse consisting in identifying Bϕ with
supt∈[0,T ],m |ψ(m)
t (ϕ)|, for every parameter µ, ε and α of Theorem 2 with x= αln(T),
one can ﬁnd two parameters γ and γ′ such that
˜dϕ(γ) ≤ dϕ ≤ ˜dϕ(γ′).
Therefore, this practical choice is consistent with the theory and tuning hyperparameters
reduces to only tuning γ. Our simulation study will provide sound answers to the question
of tuning γ.
We compute the Lasso estimate by using the shooting method of [ 28] and the R-package
Lassoshooting. In particular, we need to invert the matrix G. In all simulations, this
matrix was invertible, which is consistent with the fact that Ω c happens with large
probability. Note also that the value of c, namely the smallest eigenvalue of G, can be
very small (about 10 −4) whereas the largest eigenvalue is potentially as large as 10 5, both
values highly depending on the simulation and on T. Fortunately, those values are not
needed to compute our Lasso estimate. Since it is based on Bernstein type inequalities ,
our Lasso method is denoted B in the sequel.
Due to their soft thresholding nature, Lasso methods are known t o underestimate the
coeﬃcients [ 43, 64]. To overcome biases in estimation due to shrinkage, we propose a tw o
steps procedure, as usually suggested in the literature: Once the support of the vector
has been estimated by B, we compute the ordinary least-square estimator among the
vectors a having the same support, which provides the ﬁnal estimate. This me thod is
denoted BO in the sequel.
Another popular method is adaptive Lasso proposed by Zou [ 64]. This method over-
comes the ﬂaws of standard Lasso by taking ℓ1-weights of the form
da
ϕ(γ) = γ
2|ˆao
ϕ|p ,
where p> 0, γ> 0 and ˆao
ϕ is a preliminary consistent estimate of the true coeﬃcient.
Even if the shapes of the weights are diﬀerent, the latter are data -driven and this method
constitutes a natural competitive method with ours. The most usu al choice, which is
adopted in the sequel, consists in taking p= 1 and the ordinary least squares estimate for
30 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
the preliminary estimate (see [ 35, 60, 64]). Then, penalization is stronger for coeﬃcients
that are preliminary estimated by small values of the ordinary least s quare estimate. In
the literature, the parameter γ of adaptive Lasso is usually tuned by cross-validation,
but this does not make sense for Hawkes data that are fully depend ent. Therefore, a
preliminary study has been performed to provide meaningful values for γ. Results are
given in the next section. This adaptive Lasso method is denoted A in the sequel and
AO when combined with ordinary least squares in the same way as for BO.
Simulations are performed in R. The computational time is small (merely a few seconds
for one estimate even when M = 8, T = 20 and K= 8 on a classical laptop computer),
which constitutes a clear improvement with respect to existing adap tive methods for
Hawkes processes. For instance, the “Islands” method 2 of [ 52] is limited to the estimation
of one or two dozens of coeﬃcients at most, because of an extrem e computational memory
cost whereas here when M = 8 and K = 8, we can easily deal with M + KM2 = 520
coeﬃcients.
6.3. Results
First, we provide in Figure
2 reconstructions by using the OLS estimate on the whole
dictionary, which corresponds to the case where all the weights dϕ are null. As expected,
reconstructions are not sparse and also bad due to a small signal t o noise ratio (remember
that T = 2).
Now let us consider methods leading to sparsity. A precise study ove r 100 simulations
has been carried out corresponding to Experiments 1 and 3 for whic h we can precisely
check if the support of the vector ˆ ais the correct one. For each method, we have selected
3 values for the hyperparameter γ based on results of preliminary simulations. Before
studying mean squared errors, we investigate the following problem s that are stated in
order of importance. We wonder whether our procedure can ident ify:
- the dependency groups . Recall that two neurons belong to the same group if and
only if they are connected directly or through the intermediary of o ne or several
neurons. This issue is essential from the neurobiological point of vie w since knowing
interactions between two neurons is of capital importance.
- the nonzero interaction functions h(m)
ℓ ’s and nonzero spontaneous rates ν(m)’s. For
ℓ,m ∈ {1,...,M }, the neuron ℓ has a signiﬁcative direct interaction on neuron m if
and only if h(m)
ℓ ̸= 0;
- the nonzero coeﬃcients of nonzero interaction functions . This issue is more math-
ematical. However, it may provide information about the maximal ran ge for direct
interactions between two given neurons or about the favored dela y of interaction.
Note that the dependency groups are the only features that can be detected by classical
analysis tools of neuroscience, such as the Unitary Events method [31]. In particular,
2This method developed forM = 1 could easily be theoretically adapted for larger values of M, but
its extreme computational cost prevents us from using it in practice.
Lasso and probabilistic inequalities for multivariate poi nt processes 31
Figure 2. Reconstructions corresponding to Experiment 2 with the OLS estimate with T = 2
and K = 8. Each line m represents the function h(m)
ℓ , for ℓ = 1, 2. The spontaneous rates asso-
ciated with each line m are given above the graphs where S ∗ denotes the true spontaneous rate
and its estimator is denoted by SO. The true interactions fun ctions are plotted in black whereas
the OLS estimates are plotted in magenta.
to the best of our knowledge, identiﬁcation of the nonzero interac tion functions inside a
dependency group is a problem that has not been solved yet as far a s we know.
Results for our method and for adaptive Lasso can be found in Table
1. This prelim-
inary study also provides answers for tuning issues. The line “DG” giv es the number of
correct identiﬁcations of dependency groups. For instance, for M = 8, “DG” gives the
number of simulations for which the 3 dependency groups {1,2,3}, {4} and {5,6,7,8}
are recovered by the methods. When M= 2, both methods correctly ﬁnd that neurons 1
and 2 are dependent, even if T = 2. When 8 neurons are considered, the estimates should
ﬁnd 3 dependency groups. We see that even with T = 2, our method with γ= 1 correctly
guesses the dependency groups for 32% of the simulations. It’s clo se or equal to 100%
when T = 20 with γ= 1 or γ= 2. The adaptive Lasso has to take γ= 1000 for T = 2 and
T = 20 to obtain as convincing results. Clearly, smaller choices of γ for adaptive Lasso
leads to bad estimations of the dependency groups. Next, let us fo cus on the detection of
nonzero spontaneous rates. Whatever the experiment and the p arameter γ, our method
is optimal whereas adaptive Lasso misses some nonzero spontaneo us rates when T = 2.
32 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Table 1. Numerical results of both procedures over 100 runs with K = 4. Results for Experiment 1 (top) and Experiment 3
(bottom) are given for T = 2 (left) and T = 20 (right). “DG” gives the number of correct identiﬁcation s of dependency groups over
100 runs. “S” gives the median number of nonzero spontaneous rate estimates, “*” means that all the spontaneous rate esti mates
are nonzero over all the simulations. “F+” gives the median n umber of additional nonzero interaction functions w.r.t. t he truth.
“F− ” gives the median number of missing nonzero interaction fun ctions w.r.t. the truth. “Coeﬀ+” and “Coeﬀ − ” are deﬁned in
the same way for the coeﬃcients. “SpontMSE” is the Mean Squar e Error for the spontaneous rates with or without the additio nal
“ordinary least squares step”. “InterMSE” is the analog for the interaction functions. In bold, we give the optimal valu es
M = 2, T = 2 Our Lasso method Adaptive Lasso M = 2, T = 20 Our Lasso method Adaptive Lasso
γ 0.5 1 2 2 200 1000 γ 0.5 1 2 2 200 1000
DG 100 100 98 100 100 98 DG 100 100 100 100 100 100
S ∗ ∗ ∗ 2 2 1 S ∗ ∗ ∗ ∗ ∗ ∗
F+ 0 0 0 1 0 0 F+ 0 0 0 1 0 0
F− 0 0 0 0 0 0 F− 0 0 0 0 0 0
Coeﬀ+ 2 1 0 11 2 0 Coeﬀ+ 1 0 0 11 2 0
Coeﬀ− 0 0 0 0 0 0 Coeﬀ− 0 0 0 0 0 0
SpontMSE 108 140 214 150 193 564 SpontMSE 22 37 69 14 12 27
+ols 104 96 95 151 154 516 +ols 11 10 9 14 12 10
InterMSE 7 9 15 13 8 11 InterMSE 2 3 6 1 .4 0 .6 0 .5
+ols 7 7 7 14 10 10 +ols 0 .6 0 .5 0.4 1.4 0 .9 0.4
M = 2, T = 2 Our Lasso method Adaptive Lasso M = 2, T = 20 Our Lasso method Adaptive Lasso
γ 0.5 1 2 2 200 1000 γ 0.5 1 2 2 200 1000
DG 0 32 24 0 0 32 DG 63 99 100 0 0 90
S ∗ ∗ ∗ 8 7 5 S ∗ ∗ ∗ ∗ ∗ ∗
F+ 17 6 1 55 13 0.5 F+ 3 1 0 55 10 0
F− 0 0 2 0 0 2 F − 0 0 0 5.5d0 0 0
Coeﬀ+ 22 7 1 199.5 17 1 Coeﬀ+ 4 1 0 197 13 0
Coeﬀ− 0.5 2 7 0 2 7 Coeﬀ − 0 0 0 0 0 0
SpontMSE 295 428 768 1445 1026 1835 SpontMSE 82 166 355 104 43 64
+ols 1327 587 859 1512 1058 1935 +ols 41 26 24 107 74 26
InterMSE 38 51 79 214 49 65 InterMSE 10 19 39 16 2 .9 3 .17
+ols 63 45 61 228 84 70 +ols 3 2 .1 1.9 17 6 .3 2
Lasso and probabilistic inequalities for multivariate poi nt processes 33
Under this criterion, for adaptive Lasso, the choice γ= 1000 is clearly bad when T = 2
(the optimal value of S is S= 2 when M = 2 and S= 8 when M = 8) on both exper-
iments, whereas γ= 2 or γ= 200 is better. Not surprisingly, the number of additional
nonzero functions and additional nonzero coeﬃcients decreases when T grows and when
γ grows, whatever the method whereas the number of missing funct ions or coeﬃcients
increases. We can conclude from these facts and from further an alysis of Table 1 that
the choice γ= 0.5 for our method and the choice γ= 2 for the adaptive Lasso are wrong
choices of the tuning parameters. In conclusion of this preliminary s tudy, our method
with γ= 1 or γ= 2 seems a good choice and is robust with respect to T. When T = 20,
the optimal choice for adaptive Lasso is γ= 1000. When T = 2, the choice is not so clear
and depends on the criterion we wish to favor.
Now let us look at mean squared errors (MSE). Since the spontaneo us rates do not
behave like the other coeﬃcients, we split the MSE in two parts: one f or the spontaneous
rates:
SpontMSE =
M∑
m=1
(ˆν(m) − ν(m))2,
and one for interactions:
InterMSE =
M∑
m=1
M∑
ℓ=1
∫
(ˆh(m)
ℓ (t) − h(m)
ℓ (t))2 dt.
We still report the results for B, BO, A and AO in Table 1. Our comments mostly focus
on cases where the results for the previous study are good. First , note that results on such
cases are better by using the second step (OLS). Furthermore, MSE is increasing with
γ for B and A, since underestimation is stronger when γ increases. This phenomenon
does not appear for two step procedures, which leads to a more st able MSE. For adaptive
Lasso, when T = 2, the choice γ= 200 leads to good MSE, but the MSE are smaller for
BO with γ= 1. When T = 20, the choice γ= 1000 for AO leads to results that are of the
same magnitude as the ones obtained by BO with γ= 1 or 2. Still for T = 20, results for
the estimate B are worse than results for A. It is due to the fact that shrinkage is larger
in our method for the coeﬃcients we want to keep than shrinkage of adaptive Lasso that
becomes negligible as soon as the true coeﬃcients are large enough. However the second
step overcomes this problem.
Note also that a more thorough study of the tuning parameter γ has been performed
by [ 5] where it is mathematically proved that the choice γ <1 leads to very degener-
ate estimates in the density setting. Their method for choosing Las so parameters being
analogous to ours, it seems coherent to obtain worse MSE for γ= 0 .5 than for γ= 1
or γ= 2, at least for BO. The boundary γ= 1 in their simulation study seems to be a
robust choice there, and it seems to be the case here too.
We now provide some reconstructions by using Lasso methods. Figu res 3 and 4 give
the reconstructions corresponding to Experiment 2 (M = 2) with K= 8 for T = 2 and
34 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Figure 3. Reconstructions corresponding to Experiment 2 with T = 2 and K = 8. Each line
m represents the function h(m)
ℓ , for ℓ = 1, 2. The spontaneous rates estimation associated with
each line m is given above the graphs: S ∗ denotes the true spontaneous rate and its estimators
computed by using B, BO and A respectively are denoted by SB, SBO and SA. The true inter-
actions functions (in black) are reconstructed by using B, BO and A providing reconstructions
in green, red and blue respectively. We use γ = 1 for B and BO and γ = 200 for A.
T = 20, respectively. The reconstructions are quite satisfying. Of c ourse, the quality
improves when T grows. We also note improvements by using BO instead of B. For
adaptive Lasso, improvements by using the second step are not sig niﬁcative and this
is the reason why we do not represent reconstructions with AO. Graphs of the right-
hand side of Figure 3 illustrate the diﬃculties of adaptive Lasso to recover the exact
support of interactions functions, namely h(1)
2 and h(2)
2 for T = 2. Figure
5 provides
another illustration in the case of Experiment 3 (M = 8) with K = 8 for T = 20. For
the sake of clarity, we only represent reconstructions for the ﬁr st 4 neurons. From the
estimation point of view, this illustration provides a clear hierarchy be tween the methods:
BO seems to achieve the best results and B the worst. Finally, Figure 6 shows that even
in the inhibition case, we are able to recover the negative interaction s.
Lasso and probabilistic inequalities for multivariate poi nt processes 35
Figure 4. Reconstructions corresponding to Experiment 2 with T = 20 and K = 8. Same con-
vention as in Figure 3. We use γ = 1 for B and BO and γ = 1000 for A.
6.4. Conclusions
With respect to the problem of tuning our methodology based on Ber nstein type inequal-
ities, our simulation study is coherent with theoretical aspects sinc e we achieve our best
results by taking γ= 1, which constitutes the limit case of assumptions of Theorem
2.
For practical aspects, we recommend the choice γ= 1 even if γ= 2 is acceptable. More
importantly, this choice is robust with respect to the duration of re cordings, which is not
the case for adaptive Lasso. Implemented with γ= 1, our method outperforms adaptive
Lasso and it is able to recover the dependency groups, the nonzer o spontaneous rates, the
nonzero functions and even the nonzero coeﬃcients as soon as T is large enough. Most
of the time, the two step procedure BO seems to achieve the best results for parameter
estimation.
It is important to note that the question of tuning adaptive Lasso r emains open. Some
values of γ allow us to obtain very good results but they are not robust with res pect to
T, which may constitute a serious problem for practitioners. In the s tandard regression
setting, this problem may be overcome by using cross-validation on in dependent data,
which somehow estimates random ﬂuctuations. But in this multivariat e Hawkes setup,
36 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Figure 5. Reconstructions corresponding to Experiment 3 with T = 20 and K = 8 and for the ﬁrst 4 neurons. Each line m
represents the function h(m)
ℓ , for ℓ = 1, 2, 3, 4. Same convention as in Figure 3. We use γ = 1 for B and BO and γ = 1000 for A.
Lasso and probabilistic inequalities for multivariate poi nt processes 37
Figure 6. Reconstructions corresponding to Experiment 4 with T = 20 and K = 8. Same con-
ventions as in Figure 3. We use γ = 1 for B and BO and γ = 1000 for A.
independence assumptions on data cannot be made and this explains the problems for
tuning adaptive Lasso. Our method based on Bernstein type conce ntration inequalities
takes into account those ﬂuctuations. It also takes into account the nature of the coeﬃ-
cients and the variability of their estimates which diﬀer for spontane ous rates on the one
hand and coeﬃcients of interaction functions on the other hand. T he shape of weights
of adaptive Lasso does not incorporate this diﬀerence, which expla ins the contradictions
for tuning the method when T = 2. For instance, in some cases, adaptive Lasso tends to
estimate some spontaneous rates to zero in order to achieve bett er performance on the
interaction functions.
7. Proofs
This section is devoted to the proofs of the results of the paper. T hroughout, C is a
constant whose value may change from line to line.
38 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
7.1. Proof of Theorem 1
The proof of Theorem 1 is standard (see for instance [ 15]), but for the sake of complete-
ness, we give it. We use ∥ · ∥ ℓ2 for the Euclidian norm of RΦ . Given a recall that
fa =
∑
ϕ∈Φ
aϕϕ.
Then, we have ˆf= fˆa,
a′b= ψ(fa) • NT
and
a′Ga= ∥fa∥2
T = ∥ψ(fa)∥2
proc.
Then,
−2ψ(fˆa) • NT + ∥fˆa∥2
T + 2d′|ˆa| ≤ − 2ψ(fa) • NT + ∥fa∥2
T + 2d′|a|.
So,
∥ψ(fˆa) − λ∥2
proc = ∥ψ(fˆa)∥2
proc + ∥λ∥2
proc − 2⟨ψ(fˆa),λ⟩proc
≤ ∥ψ(fa)∥2
proc + ∥λ∥2
proc + 2ψ(fˆa − fa) • NT
+ 2d′(|a| − | ˆa|) − 2⟨ψ(fˆa),λ⟩proc
= ∥ψ(fa) − λ∥2
proc + 2⟨ψ(fa − fˆa),λ⟩proc
+ 2ψ(fˆa − fa) • NT + 2d′(|a| − | ˆa|)
= ∥ψ(fa) − λ∥2
proc + 2ψ(fa − fˆa) • (Λ − N)T + 2d′(|a| − | ˆa|)
= ∥ψ(fa) − λ∥2
proc + 2
∑
ϕ∈Φ
(aϕ − ˆaϕ)ψ(ϕ) • (Λ − N)T + 2d′(|a| − | ˆa|)
≤ ∥ψ(fa) − λ∥2
proc + 2
∑
ϕ∈Φ
|aϕ − ˆaϕ| × |¯bϕ − bϕ| + 2d′(|a| − | ˆa|).
Using (
2.5), we obtain:
∥ψ(fˆa) − λ∥2
proc ≤ ∥ψ(fa) − λ∥2
proc + 2
∑
ϕ∈Φ
dϕ|aϕ − ˆaϕ| + 2
∑
ϕ∈Φ
dϕ(|aϕ| − | ˆaϕ|)
≤ ∥ψ(fa) − λ∥2
proc + 2
∑
ϕ∈Φ
dϕ(|aϕ − ˆaϕ| + |aϕ| − | ˆaϕ|).
Lasso and probabilistic inequalities for multivariate poi nt processes 39
Now, if ϕ /∈ S(a), |aϕ − ˆaϕ| + |aϕ| − | ˆaϕ| = 0, and
∥ψ(fˆa) − λ∥2
proc ≤ ∥ψ(fa) − λ∥2
proc + 2
∑
ϕ∈S(a)
dϕ(|aϕ − ˆaϕ| + |aϕ| − | ˆaϕ|)
≤ ∥ψ(fa) − λ∥2
proc + 4
∑
ϕ∈S(a)
dϕ(|aϕ − ˆaϕ|)
≤ ∥ψ(fa) − λ∥2
proc + 4∥ˆa− a∥ℓ2
( ∑
ϕ∈S(a)
d2
ϕ
)1/2
.
We now use the assumption on the Gram matrix given by (
2.4) and the triangular
inequality for ∥ · ∥ T , which yields
∥ˆa− a∥2
ℓ2 ≤ c−1(ˆa− a)′G(ˆa− a)
= c−1∥fˆa − fa∥2
T
≤ 2c−1(∥ψ(fˆa) − λ∥2
proc + ∥ψ(fa) − λ∥2
proc).
Let us take α∈ (0,1). Since for any x∈ R and any y∈ R, 2 xy≤ αx2 + α−1y2, we obtain:
∥ψ(fˆa) − λ∥2
proc ≤ ∥ψ(fa) − λ∥2
proc
+ 4
√
2c−1/2
√
∥ψ(fˆa) − λ∥2
proc + ∥ψ(fa) − λ∥2
proc
( ∑
ϕ∈S(a)
d2
ϕ
)1/2
≤ ∥ψ(fa) − λ∥2
proc
+ α(∥ψ(fˆa) − λ∥2
proc + ∥ψ(fa) − λ∥2
proc) + 8 α−1c−1 ∑
ϕ∈S(a)
d2
ϕ
≤ (1 − α)−1
(
(1 + α)∥ψ(fa) − λ∥2
proc + 8α−1c−1 ∑
ϕ∈S(a)
d2
ϕ
)
.
The theorem is proved just by taking an arbitrary absolute value fo r α∈ (0,1).
7.2. Proof of Theorem
2
Let us ﬁrst deﬁne
T =
{
t≥ 0
/
sup
m
|ψ(m)
t (ϕ)| >Bϕ
}
. (7.1)
Let us deﬁne the stopping time τ′ = inf T and the predictable process H by
H(m)
t = ψ(m)
t (ϕ)1t≤τ ′ .
40 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let us apply Theorem 3 to this choice of H with τ = T and B= Bϕ. The choice of v
and w will be given later on. To apply this result, we need to check that for a ll t and all
ξ∈ (0,3), ∑
m
∫t
0 eξH(m)
s /Bϕ λ(m)
s ds is a.s. ﬁnite. But if t>τ ′, then
∫ t
0
eξH(m)
s /Bϕ λ(m)
s ds=
∫ τ ′
0
eξH(m)
s /Bϕ λ(m)
s ds+
∫ t
τ ′
λ(m)
s ds,
where the second part is obviously ﬁnite (it is just Λ (m)
t − Λ (m)
τ ′ ). Hence, it remains to
prove that for all t≤ τ′,
∫ t
0
eξH(m)
s /Bϕ λ(m)
s ds
is ﬁnite. But for all s<t , s<τ ′ and consequently s /∈ T . Therefore, |H(m)
s | ≤ Bϕ. Since
we are integrating with respect to the Lebesgue measure, the fac t that it eventually does
not hold in t is not a problem and
∫ t
0
eξH(m)
s /Bϕ λ(m)
s ds≤ eξΛ (m)
t ,
which is obviously ﬁnite a.s. The same reasoning can be applied to show t hat a.s.
exp(ξH2/B2) • Λ t <∞. We can also apply Theorem 3 to −H in the same way. We
obtain at the end that for all ε> 0
P
(
|H• (N− Λ) T | ≥
√
2(1 + ε) ˆVµx+ Bϕx
3 and w≤ ˆVµ ≤ v and sup
m,t≤T
|H(m)
t | ≤ Bϕ
)
(7.2)
≤ 4
(log(v/w)
log(1 + ε) + 1
)
e−x.
But on Ω V,B it is clear that ∀t∈ [0,T],t/∈ T . Therefore, τ′ ≥ T. Therefore for all t≤ T,
one also has t≤ τ′ and H(m)
t = ψ(m)
t (ϕ). Consequently, on Ω V,B ,
H• (N− Λ) T = bϕ − ¯bϕ and ˆVµ = ˆVµ
ϕ .
Moreover, on Ω V,B , one has that
B2
ϕx
µ− φ(µ) ≤ ˆVµ
ϕ ≤ µ
µ− φ(µ) Vϕ + B2
ϕx
µ− φ(µ) .
So, we take wand vas respectively the left- and right-hand side of the previous inequa lity.
Finally note that on Ω V,B ,
sup
m,t≤T
|H(m)
t | = sup
m,t≤T
|ψ(m)
t (ϕ)| ≤ Bϕ.
Lasso and probabilistic inequalities for multivariate poi nt processes 41
Hence, we can rewrite ( 7.2) as follows
P
(
|bϕ − ¯bϕ| ≥
√
2(1 + ε) ˆVµ
ϕ x+ Bϕx
3 and Ω V,B
)
≤ 4
(log(1 + µVϕ/B2
ϕx)
log(1 + ε) + 1
)
e−x.(7.3)
Apply this to all ϕ∈ Φ, we obtain that
P(∃ϕ∈ Φ s.t. |bϕ − ¯bϕ| ≥ dϕ and Ω V,B ) ≤ 4
∑
ϕ∈Φ
(log(1 + µVϕ/B2
ϕx)
log(1 + ε) + 1
)
e−x.
Now on the event Ω c ∩ Ω V,B ∩ {∀ϕ∈ Φ ,|bϕ − ¯bϕ| ≤ dϕ}, one can apply Theorem 1. To
obtain Theorem 2, it remains to bound the probability of the complementary event by
P(Ω c
c) + P(Ω c
V,B ) + P(∃ϕ∈ Φ s.t. |bϕ − ¯bϕ| ≥ dϕ and Ω V,B ).
7.3. Proof of Theorem
3
First, replacing H with H/B, we can always assume that B = 1. Next, let us
ﬁx for the moment ξ ∈ (0,3). If one assumes that almost surely for all t >0,∑ M
m=1
∫t
0 eξH(m)
s λ(m)
s ds< ∞ (i.e., that the process e ξH • Λ is well deﬁned) then one
can apply Theorem 2 of [
8], page 165, stating that the process ( Et)t≥0 deﬁned for all t
by
Et = exp(ξH• (N− Λ) t − φ(ξH) • Λ t)
is a supermartingale. It is also the case for Et∧τ if τ is a bounded stopping time. Hence
for any ξ∈ (0,3) and for any x> 0, one has that
P(Et∧τ >ex) ≤ e−xE(Et∧τ ) ≤ e−x,
which means that
P(ξH• (N− Λ) t∧τ − φ(ξH) • Λ t∧τ >x) ≤ e−x.
Therefore,
P
(
ξH• (N− Λ) t∧τ − φ(ξH) • Λ t∧τ >x and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
But if sup s≤τ,m |H(m)
s | ≤ 1, then for any ξ> 0 and any s,
φ(ξH(m)
s ) ≤ (H(m)
s )2φ(ξ).
So, for every ξ∈ (0,3), we obtain:
P
(
Mτ ≥ ξ−1φ(ξ)H2 • Λ τ + ξ−1x and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x. (7.4)
42 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Now let us focus on the event H2 • Λ τ ≤ v where v is a deterministic quantity. We have
that consequently
P
(
Mτ ≥ ξ−1φ(ξ)v+ ξ−1x and H2 • Λ τ ≤ v and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
It remains to choose ξ such that ξ−1φ(ξ)v+ ξ−1xis minimal. But this expression has no
simple form. However, since 0 <ξ< 3, one can bound φ(ξ) by ξ2(1 − ξ/3)−1/2. Hence,
we can start with
P
(
Mτ ≥ ξ
2(1 − ξ/3)H2 • Λ τ + ξ−1x and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x (7.5)
and also
P
(
Mτ ≥ ξ
2(1 − ξ/3)v+ ξ−1x and H2 • Λ τ ≤ v and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x. (7.6)
It remains now to minimize ξ↦− → ξ
2(1−ξ/3) v+ ξ−1x.
Lemma 2. Let a, b and x be positive constants and let us consider on (0,1/b),
g(ξ) = aξ
(1 − bξ) + x
ξ.
Then minξ∈(0,1/b) g(ξ) = 2 √ax+ bx and the minimum is achieved in ξ(a,b,x) = xb−√ax
xb2−a .
Proof. The limits of g in 0 + and (1 /b)− are + ∞. The derivative is given by
g′(ξ) = a
(1 − bξ)2 − x
ξ2
which is null in ξ(a,b,x) (remark that the other solution of the polynomial does not lie in
(0,1/b)). Finally, it remains to evaluate the quantity in ξ(a,b,x) to obtain the result. □
Now, we apply ( 7.6) with ξ(v/2,1/3,x) and we obtain this well known formula which
can be found in [ 57] for instance,
P
(
Mτ ≥
√
2vx+ x/3 and H2 • Λ τ ≤ v and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x. (7.7)
Now we would like ﬁrst to replace v by its random version H2 • Λ τ . Let w,v be some
positive constants and let us concentrate on the event
w≤ H2 • Λ τ ≤ v. (7.8)
Lasso and probabilistic inequalities for multivariate poi nt processes 43
For all ε> 0 we introduce K a positive integer depending on ε, v and w such that
(1 + ε)K w≥ v. Note that K= ⌈log(v/w)/log(1 + ε)⌉ is a possible choice. Let us denote
v0 = w, v1 = (1 + ε)w,...,v K = (1 + ε)K w. For any 0 <ξ< 3 and any kin {0,...,K − 1},
one has, by applying ( 7.5),
P
(
Mτ ≥ ξ
2(1 − ξ/3)H2 • Λ τ + ξ−1x
and vk ≤ H2 • Λ τ ≤ vk+1 and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
This implies that
P
(
Mτ ≥ ξ
2(1 − ξ/3)vk+1 + ξ−1x and vk ≤ H2 • Λ τ ≤ vk+1 and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
Using Lemma 2, with ξ= ξ(vk+1/2,1/3,x), this gives
P
(
Mτ ≥
√
2vk+1x+ x/3 and vk ≤ H2 • Λ τ ≤ vk+1 and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
But if vk ≤ H2 • Λ τ , vk+1 ≤ (1 + ε)vk ≤ (1 + ε)H2 • Λ τ , so
P
(
Mτ ≥
√
2(1 + ε)(H2 • Λ τ )x+ x/3
and vk ≤ H2 • Λ τ ≤ vk+1 and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x.
Finally summing on k, this gives
P
(
Mτ ≥
√
2(1 + ε)(H2 • Λ τ )x+ x/3
(7.9)
and w≤ H2 • Λ τ ≤ v and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ Ke−x.
This leads to the following result that has interest per se.
Proposition 6. Let N = (N(m))m=1,...,M be a multivariate counting process with pre-
dictable intensities λ(m)
t and corresponding compensator Λ (m)
t with respect to some given
ﬁltration. Let B> 0. Let H= (H(m))m=1,...,M be a multivariate predictable process such
that for all ξ∈ (0,3), eξH/B • Λ t <∞ a.s. for all t. Let us consider the martingale deﬁned
for all t by
Mt = H• (N− Λ) t.
44 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Let v>w be positive constants and let τ be a bounded stopping time. Then for any ε,x> 0
P
(
Mτ ≥
√
2(1 + ε)(H2 • Λ τ )x+ Bx
3 and w≤ H2 • Λ τ ≤ v and sup
m,t≤τ
|H(m)
t | ≤ B
)
(7.10)
≤
(log(v/w)
log(1 + ε) + 1
)
e−x.
Next, we would like to replace H2 • Λ τ , the quadratic characteristic of M, with its
estimator H2 • Nτ , that is, the quadratic variation of M. For this purpose, let us con-
sider Wt = −H2 • (N − Λ) t which is still a martingale since the −(H(m)
s )2’s are still
predictable processes. We apply ( 7.4) with µ instead of ξ, noticing that on the event
{sups≤τ,m |H(m)
s | ≤ 1}, one has that H4 • Λ τ ≤ H2 • Λ τ . This gives that
P
(
H2 • Λ τ ≥ H2 • Nτ + {φ(µ)/µ}H2 • Λ τ + x/µ and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x,
which means that
P
(
H2 • Λ τ ≥ ˆVµ and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ e−x. (7.11)
So we use again ( 7.5) combined with ( 7.11) to obtain that for all ξ∈ (0,3)
P
(
Mτ ≥ ξ
2(1 − ξ/3)
ˆVµ + ξ−1x and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ P
(
Mτ ≥ ξ
2(1 − ξ/3)
ˆVµ + ξ−1x and sup
s≤τ,m
|H(m)
s | ≤ 1 and H2 • Λ τ ≤ ˆVµ
)
+ P
(
H2 • Λ τ ≥ ˆVµ and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ 2e−x.
This new inequality replaces ( 7.5) and it remains to replace H2 • Λ τ by ˆVµ in the peeling
arguments to obtain as before that
P
(
Mτ ≥
√
2(1 + ε) ˆVµx+ x/3 and w≤ ˆVµ ≤ v and sup
s≤τ,m
|H(m)
s | ≤ 1
)
≤ 2Ke−x. (7.12)
7.4. Proofs of the probabilistic results for Hawkes processes
7.4.1. Proof of Lemma 1
Let K(n) denote the vector of the number of descendants in the nth generation from a
single ancestral point of type ℓ, deﬁne K(0) = eℓ and let W(n) = ∑ n
k=0 K(k) denote the
total number of points in the ﬁrst n generations. Deﬁne for θ∈ RM
φℓ(θ) = log EℓeθT K(1).
Lasso and probabilistic inequalities for multivariate poi nt processes 45
Thus, φℓ(θ) is the log-Laplace transform of the distribution of K(1) given that there
is a single initial ancestral point of type ℓ. We deﬁne the vector φ(θ) by φ(θ)′ =
(φ1(θ),...,φ M (θ)). Note that φ only depends on the law of the number of children per
parent, that is, it only depends on Γ. Then
EℓeθT W (n) = Eℓ(eθT W (n−1)E(eθT K(n) | K(n− 1),...,K (1)))
= Eℓ(eθT W (n−1)eφ(θ)T K(n−1))
= Eℓe(θ+φ(θ))T K(n−1)+θT W (n−2).
Deﬁning g(θ) = θ+ φ(θ) we arrive by recursion at the formula
EℓeθT W (n) = Eℓeg◦(n−1)(θ)T K(1)+θT W (0)
= eφ(g◦(n−1)(θ))ℓ+θℓ
= eg◦n(θ)ℓ ,
where for any n, g◦(n) = g◦ · · · ◦ g ntimes. Or, in other words, we have the following
representation
log EℓeθT W (n) = g◦n(θ)ℓ
of the log-Laplace transform of W(n).
Below we show that φ is a contraction in a neighborhood containing 0, that is, for
some r> 0 and a constant C< 1 (and a suitable norm), ∥φ(s)∥ ≤ C∥s∥ for ∥s∥ ≤ r. If θ
is chosen such that
∥θ∥
1 − C ≤ r
we have ∥θ∥ ≤ r, and if we assume that g◦k(θ) ∈ B(0,r) for k= 1,...,n − 1 then
∥g◦n(θ)∥ ≤ ∥ θ∥ + ∥φ(g◦(n−1)(θ))∥
≤ ∥θ∥ + C∥g◦(n−1)(θ)∥
≤ ∥θ∥(1 + C+ C2 + · · · + Cn)
≤ r.
Thus, by induction, g◦n(θ) ∈ B(0,r) for all n≥ 1. Since n↦→Wm(n) is increasing and
goes to Wm(∞) for n→ ∞, with Wm(∞) the total number of points in a cluster of type
m, and since W = ∑
m Wm(∞) = 1T W(∞), we have by monotone convergence that for
ϑ∈ R
log EℓeϑW = lim
n→∞
g◦n(ϑ1)ℓ.
By the previous result, the right-hand side is bounded if |ϑ| is suﬃciently small. This
completes the proof up to proving that φ is a contraction.
46 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
To this end, we note that φis continuously diﬀerentiable (on RM in fact, but a neigh-
borhood around 0 suﬃces) with derivative Dφ(0) = Γ at 0. Since the spectral radius of
Γ is strictly less than 1 there is a C< 1 and, by the Householder theorem, a norm ∥ · ∥
on RM such that for the induced operator norm of Γ we have
∥Γ ∥ = max
x:∥x∥≤1
∥Γ x∥ <C.
Since the norm is continuous and Dφ(s) is likewise there is an r> 0 such that
∥Dφ(s)∥ ≤ C< 1
for ∥s∥ ≤ r. This, in turn, implies that φ is Lipschitz continuous in the ball B(0,r) with
Lipschitz constant C, and since φ(0) = 0 we get
∥φ(s)∥ ≤ C∥s∥
for ∥s∥ ≤ r. This ends the proof of the lemma.
Note that we have not at all used the explicit formula for φabove, which is obtainable
and simple since the oﬀspring distributions are Poisson. The only thing we needed was
the fact that φis deﬁned in a neighborhood around 0, thus that the oﬀspring distrib utions
are suﬃciently light-tailed.
7.4.2. Proof of Proposition
2
We use the cluster representation, and we note that any cluster w ith ancestral point in
[−n− 1,−n] must have at least n+ 1 − ⌈A⌉ points in the cluster if any of the points
are to fall in [ −A,0). This follows from the assumption that all the h(m)
ℓ -functions have
support in [0 ,1]. With ˜NA,ℓ the number of points in [ −A,0) from a cluster with ancestral
points of type ℓ, we thus have the bound
˜NA,ℓ ≤
∑
n
An∑
k=1
max{Wn,k − n+ ⌈A⌉,0},
where An is the number of ancestral points in [ −n− 1,−n] of type ℓ and Wn,k is the
number of points in the respective clusters. Here the An’s and the Wn,k’s are all inde-
pendent, the An’s are Poisson distributed with mean νℓ and the Wn,k’s are i.i.d. with
the same distribution as W in Lemma
1. Moreover,
Hn(ϑℓ) := Eℓeϑℓ max{W −n+⌈A⌉,0} ≤ Pℓ(W ≤ n− ⌈A⌉) + e −ϑℓ(n−⌈A⌉)EℓeϑℓW ,
which is ﬁnite for |ϑℓ| suﬃciently small according to Lemma 1. Then we can compute an
upper bound on the Laplace transform of ˜NA,ℓ:
Eeϑℓ ˜NA,ℓ ≤
∏
n
E
An∏
k=1
E(eϑℓ max{Wn,k−n+⌈A⌉,0} | An)
Lasso and probabilistic inequalities for multivariate poi nt processes 47
≤
∏
n
EHn(ϑℓ)An
=
∏
n
eνℓ(Hn(ϑℓ)−1)
= eνℓ
∑
n(Hn(ϑℓ)−1).
Since Hn(ϑℓ) − 1 ≤ e−ϑℓ(n−⌈A⌉)EℓeϑℓW we have ∑
n(Hn(ϑℓ) − 1) <∞, which shows that
the upper bound is ﬁnite. To complete the proof, observe that N[−A,0) = ∑
ℓ ˜NA,ℓ where
˜NA,ℓ for ℓ= 1,...,M are independent. Since all variables are positive, it is suﬃcient to
take θ= minℓ ϑℓ.
7.4.3. Proof of Proposition 3
In this paragraph, the notation □ simply denotes a generic positive absolute constant
that may change from line to line. The notation □ θ1,θ2,... denotes a positive constant
depending on θ1,θ2,... that may change from line to line.
Let
u= C1σlog3/2(T)
√
T + C2b(log(T))2+η, (7.13)
where the choices of C1 and C2 will be given later. For any positive integer k such that
x:= T/(2k) >A, we have by stationarity:
P
(∫ T
0
[Z◦ St(N) − E(Z)] dt≥ u
)
= P
(k−1∑
q=0
∫ 2qx+x
2qx
[Z◦ St(N) − E(Z)] dt
+
∫ 2qx+2x
2qx+x
[Z◦ St(N) − E(Z)] dt≥ u
)
≤ 2P
(k−1∑
q=0
∫ 2qx+x
2qx
[Z◦ St(N) − E(Z)] dt≥ u
2
)
.
Similarly to [ 51], we introduce ( ˜Mx
q )q a sequence of independent Hawkes processes, each
being stationary with intensities per mark given by ψ(m)
t . For each q, we then introduce
Mx
q the truncated process associated with ˜Mx
q , where truncation means that we only
consider the points lying in [2 qx− A,2qx+ x]. So, if we set
Fq =
∫ 2qx+x
2qx
[Z◦ St(Mx
q ) − E(Z)] dt,
(7.14)
P
(∫ T
0
[Z◦ St(N) − E(Z)] dt≥ u
)
≤ 2P
(k−1∑
q=0
Fq ≥ u
2
)
+ 2kP
(
Te > T
2k − A
)
,
48 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
where Te represents the time to extinction of the process. More precisely, Te is the last
point of the process if in the cluster representation only ancestra l points before 0 are
appearing. For more details, see Section 3 of [ 51]. So, denoting al the ancestral points
with marks land Hl
al the length of the corresponding cluster whose origin is al, we have:
Te = max
l∈{1,...,M}
max
al
{al + Hl
al }.
But, for any a> 0,
P(Te ≤ a) = E
[ M∏
l=1
∏
al
E[1{al+Hlal ≤a}|al]
]
= E
[ M∏
l=1
∏
al
exp(log(P(Hl
0 ≤ a− al)))
]
= E
[ M∏
l=1
exp
(∫ 0
−∞
log(P(Hl
0 ≤ a− x)) d ˜N(l)
x
)]
,
where ˜N(l) denotes the process associated with the ancestral points with ma rks l. So,
P(Te ≤ a) = exp
(M∑
l=1
∫ 0
−∞
(exp(log(P(Hl
0 ≤ a− x))) − 1)ν(l) dx
)
= exp
(
−
M∑
l=1
ν(l)
∫ +∞
a
P(Hl
0 >u) du
)
.
Now, by Lemma 1, there exists some ϑl >0, such that cl = Eℓ(eϑlW ) <+∞, where W is
the number of points in the cluster. But if all the interaction functio ns have support in
[0,1], one always have that Hl
0 <W . Hence,
P(Hl
0 >u) ≤ E[exp(ϑlHl
0)] exp(−ϑlu)
≤ cl exp(−ϑlu).
So,
P(Te ≤ a) ≥ exp
(
−
M∑
l=1
ν(l)
∫ +∞
a
cl exp(−ϑlu) du
)
= exp
(
−
M∑
l=1
ν(l)cl/ϑl exp(−ϑla)
)
≥ 1 −
M∑
l=1
ν(l)cl/ϑl exp(−ϑla).
Lasso and probabilistic inequalities for multivariate poi nt processes 49
So, there exists a constant Cα,f0,A depending on α,A, and f0 such that if we take k=
⌊Cα,A,f0 T/log(T)⌋, then
kP
(
Te > T
2k − A
)
≤ T−α.
In this case x= T
2k ≈ log(T) is larger than Afor T large enough (depending on A,α,f0).
Now, let us focus on the ﬁrst term B of ( 7.14), where
B= P
(k−1∑
q=0
Fq ≥ u
2
)
.
Let us consider some ˜N where ˜N will be ﬁxed later and let us deﬁne the measurable
events
Ω q =
{
sup
t
{Mx
q |[t−A,t)} ≤ ˜N
}
,
where Mx
q |[t−A,t) represents the set of points of Mx
q lying in [ t− A,t). Let us also consider
Ω = ⋂
1≤q≤k Ω q. Then
B≤ P
(∑
q
Fq ≥ u/2 and Ω
)
+ P(Ω c).
We have P(Ω c) ≤ ∑
q P(Ω c
q). Each Ω q can also be easily controlled. Indeed it is suﬃcient
to split [2 qx− A,2qx+ x] in intervals of size A(there are about □ α,A,f0 log(T) of those)
and require that the number of points in each subinterval is smaller t han ˜N /2. By
stationarity, we obtain that
P(Ω c
q) ≤ □ α,A,f0 log(T)P(N[−A,0) > ˜N /2).
Using Proposition
2 with u= ⌈ ˜N /2⌉ + 1/2, we obtain:
P(Ω c
q) ≤ □ α,A,f0 log(T) exp(−□ α,A,f0
˜N ) and
(7.15)
P(Ω c) ≤ □ α,A,f0 Texp(−□ α,A,f0
˜N ).
Note that this control holds for any positive choice of ˜N . Hence, this gives also the
following lemma that will be used later.
Lemma 3. For any R >0,
P(there exists t∈ [0,T] | Mx
q |[t−A,t) >R) ≤ □ α,A,f0 Texp(−□ α,A,f0 R).
Hence by taking ˜N = C3 log(T) for C3 large enough this is smaller than □ α,A,f0 T−α′
,
where α′ = max(α,2).
50 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
It remains to obtain the rate of D:= P(∑
q Fq ≥ u/2 and Ω). For any positive constant
θ that will be chosen later, we have:
D≤ e−θu/2E
(
eθ ∑
q Fq
∏
q
1Ω q
)
(7.16)
≤ e−θu/2 ∏
q
E(eθFq 1Ω q )
since the variables ( Mx
q )q are independent. But
E(eθFq 1Ω q ) = 1 + θE(Fq 1Ω q ) +
∑
j≥2
θj
j! E(Fj
q 1Ω q )
and E(Fq 1Ω q ) = E(Fq) − E(Fq1Ω c
q ) = −E(Fq 1Ω c
q ).
Next note that if for any integer l,
l ˜N <sup
t
Mx
q |[t−A,t) ≤ (l+ 1) ˜N
then
|Fq| ≤ xb[(l+ 1)η ˜N η + 1] + xE(Z).
Hence, cutting Ω c
q in slices of the type {l ˜N <supt Mx
q [t−A,t) ≤ (l+ 1) ˜N } and using
Lemma
3, we obtain by taking C3 large enough,
|E(Fq 1Ω q )| = |E(Fq 1Ω c
q )| ≤
+∞∑
l=1
x(b[(l+ 1)η ˜N η + 1] + |E(Z)|)
× P(there exists t∈ [0,T] | {Mx
q |[t−A,t)} >ℓ ˜N )
≤ □ α,A,f0
+∞∑
l=1
x(b[(l+ 1)η ˜N η + 1] + |E(Z)|) log(T)e−□ α,A,f0 l ˜N
≤ □ α,A,f0
+∞∑
l=1
x(b˜N η + |E(Z)|) log(T)2lηe−□ α,A,f0 l ˜N
≤ □ α,η,A,f0 log2(T)b˜N η e−□ α,A,f0
˜N1 − 2ηe−□ α,A,f0
˜N
≤ z1 := □ α,η,A,f0 bT−α′
.
Note that in the previous inequalities, we have bounded |E(Z)| by bE[Nη
[−A,0)]. In the
same way, one can bound
E(Fj
q 1Ω q ) ≤ E(F2
q 1Ω q )zj−2
b ,
Lasso and probabilistic inequalities for multivariate poi nt processes 51
with zb := xb[ ˜N η + 1] + xE(Z) = □ α,η,A,f0 blog(T)1+η. One can also note that by station-
arity,
E(F2
q 1Ω q ) ≤ xE
[ ∫ 2qx+x
2qx
[Z◦ θs(Mx
q ) − E(Z)]21{for all t,Mxq |[t−A,t)≤ ˜N } ds
]
≤ xE
[ ∫ 2qx+x
2qx
[Z◦ θs(Mx
q ) − E(Z)]21{Mxq |[s−A,s)≤ ˜N } ds
]
≤ x2E([Z(N) − E(Z)]21N[−A,0)≤ ˜N )
≤ zv := □ α,η,A,f0 (log(T))2σ2.
Now let us go back to ( 7.16). We have that
D≤ exp
[
− θu
2 + kln
(
1 + θz1 +
∑
j≥2
zvzj−2
b
θj
j!
)]
≤ exp
[
−θ
(u
2 − kz1
)
+ k
∑
j≥2
zvzj−2
b
θj
j!
]
,
using that ln(1 + u) ≤ u. It is suﬃcient now to recognize a step of the proof of the
Bernstein inequality (weak version see [ 41], page 25). Since kz1 = □ α,η,sbT1−α′
/(log(T)),
one can choose α′ >1,C1 and C2 in the deﬁnition ( 7.13) of u (not depending on b) such
that u/2 − kz1 ≥ √2kzvz+ 1
3 zbz for some z= C4 log(T), where C4 is a constant. Hence,
D≤ exp
[
−θ
(√
2kzvz+ 1
3 zbz
)
+ k
∑
j≥2
zvzj−2
b
θj
j!
]
.
One can choose accordingly θ (as for the proof of the Bernstein inequality) to obtain a
bound in e −z . It remains to choose C4 large enough and only depending on α,η,A and f0
to guarantee that D≤ e−z ≤ □ α,η,A,f0 T−α. This concludes the proof of the proposition.
7.4.4. Proof of Proposition 4
Let Q denote a measure such that under Q the distribution of the full point process
restricted to ( −∞,0] is identical to the distribution under P and such that on (0 ,∞) the
process consists of independent components each being a homoge neous Poisson process
with rate 1. Furthermore, the Poisson processes should be indepe ndent of the process on
(−∞,0]. From Corollary 5.1.2 in [ 36], the likelihood process is given by
Lt = exp
(
Mt−
∑
m
∫ t
0
λ(m)
u du+
∑
m
∫ t
0
log λ(m)
u dN(m)
u
)
and we have for t≥ 0 the relation
EPκt(f)2 = EQκt(f)2Lt, (7.17)
52 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
where EP and EQ denote the expectation with respect to P and Q, respectively. Let,
furthermore, ˜N1 = N[−1,0) denote the total number of points on [ −1,0). Proposition 4
will be an easy consequence of the following lemma.
Lemma 4. If the point process is stationary under P, if
ed ≤ λ(m)
t ≤ a(N1 + ˜N1) + b
for t∈ [0,1] and for constants d∈ R and a,b> 0, and if EP(1 + ε) ˜N1 <∞ for some ε> 0
then for any f,
Q(f,f) ≥ ζ∥f∥2 (7.18)
for some constant ζ> 0.
Proof. We use H¨ olders inequality on κ1(f)2/pL1/p
1 and κ1(f)2/q L−1/p
1 to get
EQκ1(f)2 ≤ (EQκ1(f)2L1)1/p(EQκ1(f)2L−q/p
1 )1/q = Q(f,f)1/p(EQκ1(f)2L1−q
1 )1/q,
(7.19)
where 1
p + 1
q = 1. We choose q≥ 1 (and thus p) below to make q− 1 suﬃciently small.
For the left-hand side, we have by independence of the homogeneo us Poisson processes
that if f = (µ,(gℓ)ℓ=1,...,M ),
EQκ1(f)2 = (EQκ1(f))2 + VQκ1(f)
=
(
µ+
∑
ℓ
∫ 1
0
gℓ(u) du
)2
+
∑
ℓ
∫ 1
0
gℓ(u)2 du.
Exactly as on page 32 in [ 52] there exists c′ >0 such that
EQκ1(f)2 ≥ c′
(
µ2 +
∑
ℓ
∫ 1
0
g2
ℓ (u) du
)
= c′∥f∥2. (7.20)
To bound the second factor on the right-hand side in ( 7.19) we observe, by assumption,
that we have the lower bound
L1 ≥ eM(1−b)e(d−aM)N1 e−aM ˜N1
on the likelihood process. Under Q we have that ( κ1(f),N1) and ˜N1 are independent,
and with ρ= e(q−1)(aM−d) and ˜ρ= e(q−1)(aM) we get that
EQκ1(f)2L1−q
1 ≤ e(q−1)M(b−1)EQ ˜ρ
˜N1 EQκ1(f)2ρN1 .
Lasso and probabilistic inequalities for multivariate poi nt processes 53
Here we choose qsuch that ˜ρis suﬃciently close to 1 to make sure that EQ ˜ρ˜N1 = EP ˜ρ˜N1 <
∞ (see Proposition 2). Moreover, by Cauchy–Schwarz’ inequality
κ2
1(f) ≤
(
µ2 +
∑
ℓ
∫ 1−
0
g2
ℓ (1 − u) dN(ℓ)
u
)
(1 + N1). (7.21)
Under Q the point processes on (0 ,∞) are homogeneous Poisson processes with rate
1 and N1, the total number of points, is Poisson. This implies that conditionally on
(N(1)
1 ,...,N (M)
1 ) = ( n(1),...,n (M)) the n(m)-points for the mth process are uniformly
distributed on [0 ,1], hence
EQκ1(f)2L1−q
1 ≤
(
µ2 +
∑
ℓ
∫ 1
0
g2
ℓ (u) du
)
e(q−1)M(b−1)EQ ˜ρ
˜N1 EQ(1 + N1)2ρN1

 
c′′
(7.22)
= c′′∥f∥2.
Combining ( 7.20) and ( 7.22) with ( 7.19) we get that
c′∥f∥2 ≤ (c′′)1/q∥f∥2/qQ(f,f)1/p
or by rearranging that
Q(f,f) ≥ ζ∥f∥2
with ζ= (c′)p/(c′′)p−1. □
For the Hawkes process, it follows that if ν(m) >0 and if
sup
t∈[0,1]
h(m)
ℓ (t) <∞
for l,m = 1,...,M then for t∈ [0,1] we have e d ≤ λ(m)
t ≤ a(N1 + ˜N1) + b with
d= log ν(m), a = max
l
sup
t∈[0,1]
h(m)
ℓ (t), b = ν(m).
Proposition 2 proves that there exists ε> 0 such that EP(1 + ε) ˜N1 <∞. This completes
the proof of Proposition 4.
54 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
7.5. Proofs of the results of Sections4.2 and 5.2
7.5.1. Proof of Propositions 5 and 1
We ﬁrst prove Proposition 5. As in the proof of Proposition 3, we use the notation □ .
Note that for any ϕ1 and any ϕ2 belonging to Φ,
Gϕ1,ϕ2 =
M∑
m=1
∫ T
0
κt(ϕ1(m))κt(ϕ2(m)) dt
and E(Gϕ1,ϕ2 ) = T∑ M
m=1 Q(ϕ1(m),ϕ2(m)) by using (
5.2). This implies that
E(a′Ga) = a′E(G)a= T
∑
m
Q(f(m)
a ,f(m)
a ).
Hence by Proposition 4, E(a′Ga) ≥ Tζ ∑
m ∥f(m)
a ∥2 = Tζ∥fa∥2 by deﬁnition of the norm
on H. Since Φ is an orthonormal system, this implies that E(a′Ga) ≥ Tζ∥a∥ℓ2 . Hence,
to show that Ω c is a large event for some c> 0, it is suﬃcient to show that for some
0 <ǫ<ζ , with high probability, for any a∈ RΦ ,
|a′Ga− a′E(G)a| ≤ Tǫ∥a∥2
ℓ2 . (7.23)
Indeed, (
7.23) implies that, with high probability, for any a∈ RΦ ,
a′Ga≥ a′E(G)a− Tǫ∥a∥ℓ2 ≥ T(ζ− ǫ)∥a∥ℓ2 ,
and the choice c= T(ζ− ǫ) is convenient. So, ﬁrst one has to control all the coeﬃcients
of G− E(G). For all ϕ,ρ ∈ Φ, we apply Proposition 3 to
Z(N) =
∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ).
Note that Zonly depends on points lying in [ −1,0). Therefore, |Z(N)| ≤ 2M∥ϕ∥∞∥ρ∥∞(1+
N2
[−1,0)). This leads to
P
(1
T|Gϕ,ρ − E(Gϕ,ρ)| ≥ xϕ,ρ
)
≤ □ β,f0 T−β
with
xϕ,ρ = □ β,f0,M [σϕ,ρ log3/2(T)T−1/2 + ∥ϕ∥∞∥ρ∥∞ log4(T)T−1]
and
σ2
ϕ,ρ = E
[[ ∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ) − E
(∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ)
)] 2
1N[−1,0)≤ ˜N
]
.
Lasso and probabilistic inequalities for multivariate poi nt processes 55
Hence, with probability larger than 1 − □ β,f0 |Φ |2T−β one has that
|a′Ga− a′E(G)a| ≤ □ β,f0
(∑
ϕ,ρ∈Φ
|aϕ||aρ|[σϕ,ρ log3/2(T)T1/2 + ∥ϕ∥∞∥ρ∥∞ log4(T)]
)
.
Hence, for any positive constant δ chosen later,
|a′Ga− a′E(G)a|
≤ □ β,f0
[
T
∑
ϕ,ρ∈Φ
|aϕ||aρ|
[
δ σ2
ϕ,ρ
∥ϕ∥∞∥ρ∥∞
(7.24)
+
[ 1
δlog(T) + 1
]
∥ϕ∥∞∥ρ∥∞
log4(T)
T
]]
.
Now let us focus on E:= ∑
ϕ,ρ∈Φ |aϕ||aρ|
σ2
ϕ,ρ
∥ϕ∥∞∥ρ∥∞
. First, we have:
E≤ 2
∑
ϕ,ρ∈Φ
|aϕ||aρ|
E([∑
m ψ(m)
0 (ϕ)ψ(m)
0 (ρ)]21N[−1,0)≤ ˜N ) + ( E[∑
m ψ(m)
0 (ϕ)ψ(m)
0 (ρ)])2
∥ϕ∥∞∥ρ∥∞
with ˜N := □ β,f0 log(T). Next,
∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ) ≤ 2M∥ϕ∥∞∥ρ∥∞(1 + N2
[−1,0)).
Hence, if N[−1,0) ≤ ˜N = □ β,f0 log(T), for T large enough,
∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ) ≤ □ β,M,f0 ∥ϕ∥∞∥ρ∥∞ log2(T)
and
E
(∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ)
)
≤ □ β,M,f0 ∥ϕ∥∞∥ρ∥∞ log2(T).
Hence,
E≤ □ β,M,f0 log2(T)
∑
ϕ,ρ∈Φ
|aϕ||aρ|E
(⏐
⏐
⏐
⏐
∑
m
ψ(m)
0 (ϕ)ψ(m)
0 (ρ)
⏐
⏐
⏐
⏐
)
.
But note that for any f, |ψ(m)
0 (f)| ≤ ψ(m)
0 (|f|) where |f| = ((|µ(m)|,(|g(m)
ℓ |)ℓ=1,...,M )m=1,...,M ).
Therefore,
E≤ □ β,M,f0 log2(T)
∑
ϕ,ρ∈Φ
|aϕ||aρ|E
(∑
m
ψ(m)
0 (|ϕ|)ψ(m)
0 (|ρ|)
)
56 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
≤ □ β,M,f0 log2(T)
∑
m
E
([ ∑
ϕ∈Φ
|aϕ|ψ(m)
0 (|ϕ|)
] 2)
≤ □ β,M,f0 log2(T)
∑
m
E
([
ψ(m)
0
(∑
ϕ∈Φ
|aϕ||ϕ|
)] 2)
.
But if ϕ= (µ(m)
ϕ ,((gϕ)(m)
ℓ )ℓ)m, then
[
ψ(m)
0
(∑
ϕ∈Φ
|aϕ||ϕ|
)] 2
=
[ ∑
ϕ
|aϕ|µ(m)
ϕ +
M∑
ℓ=1
∫ 0−
−1
∑
ϕ
|aϕ||(gϕ)(m)
ℓ |(−u) dN(ℓ)
u
] 2
.
If one creates artiﬁcially a process N(0) with only one point and if we decide that ( gϕ)(m)
0
is the constant function equal to µ(m)
ϕ , this can also be rewritten as
[
ψ(m)
0
(∑
ϕ∈Φ
|aϕ||ϕ|
)] 2
=
[ M∑
ℓ=0
∫ 0−
−1
∑
ϕ
|aϕ||(gϕ)(m)
ℓ |(−u) dN(ℓ)
u
] 2
.
Now we apply the Cauchy–Schwarz inequality for the measure ∑
ℓ dN(ℓ), which gives
[
ψ(m)
0
(∑
ϕ∈Φ
|aϕ||ϕ|
)] 2
≤ (N[−1,0) + 1)
M∑
ℓ=0
∫ 0−
−1
[ ∑
ϕ
|aϕ||(gϕ)(m)
ℓ |(−u)
] 2
dN(ℓ)
u .
Consequently,
E≤ □ β,M,f0 log2(T)
M∑
m=1
M∑
ℓ=0
E
(
(N[−1,0) + 1)
∫ 0−
−1
[ ∑
ϕ
|aϕ||(gϕ)(m)
ℓ |(−u)
] 2
dN(ℓ)
u
)
≤ □ β,M,f0 log2(T)
M∑
m=1
M∑
ℓ=0
∑
ϕ,ρ∈Φ
|aϕ||aρ|
× E
(∫ 0−
−1
(N[−1,0) + 1)|(gϕ)(m)
ℓ |(−u)|(gρ)(m)
ℓ |(−u) dN(ℓ)
u
)
.
Now let us use the fact that for every x,y ≥ 0, η,θ> 0 that will be chosen later,
xy− ηeθx ≤ y
θ[log(y) − log(ηθ) − 1],
with the convention that ylog(y) = 0 if y= 0. Let us apply this to x= N[−1,0) + 1 and
y= |(gϕ)(m)
ℓ |(−u)|(gρ)(m)
ℓ |(−u). We obtain that
E≤ □ β,M,f0 ηlog2(T)
M∑
m=1
∑
ϕ,ρ∈Φ
|aϕ||aρ|E((N[−1,0) + 1)eθ(N[−1,0)+1))
Lasso and probabilistic inequalities for multivariate poi nt processes 57
+ □ β,M,f0 θ−1 log2(T)
M∑
m=1
M∑
ℓ=0
∑
ϕ,ρ∈Φ
|aϕ||aρ|
× E
(∫ 0−
−1
|(gϕ)(m)
ℓ ||(gρ)(m)
ℓ |(−u)[log(|(gϕ)(m)
ℓ ||(gρ)(m)
ℓ |(−u)) − log(ηθ) − 1] dNℓ
u
)
.
Since for ℓ> 0, d N(ℓ)
u is stationary, one can replace E(dN(ℓ)
u ) by □ f0 du. Moreover, since
by Proposition 2, N[−1,0) has some exponential moments there exists θ= □ f0 such that
E((N[−1,0) + 1)eθ(N[−1,0)+1)) = □ f0 . With |Φ | the size of the dictionary, this leads to
E≤ □ β,M,f0 η|Φ | log2(T)∥a∥2
ℓ2
+ □ β,M,f0 log2(T)
×
M∑
m=1
[ ∑
ϕ,ρ∈Φ
|aϕ||aρ||µ(m)
ϕ ||µ(m)
ρ |[log(|µ(m)
ϕ ||µ(m)
ρ |) − log(ηθ) − 1]
+
M∑
ℓ=1
∑
ϕ,ρ∈Φ
|aϕ||aρ|
∫ 1
0
|(gϕ)(m)
ℓ ||(gρ)(m)
ℓ |(u)
× [log(|(gϕ)(m)
ℓ ||(gρ)(m)
ℓ |(u)) − log(ηθ) − 1] du
]
.
Consequently, using ∥Φ ∥∞ and rΦ ,
E≤ □ β,M,f0 η|Φ | log2(T)∥a∥2
ℓ2 + □ β,M,f0 log2(T)rΦ [2 log(∥Φ ∥∞) − log(ηθ) − 1]∥a∥2
ℓ2 .
We choose η= |Φ |−1 and obtain that
E≤ □ β,M,f0 log2(T)rΦ [log(∥Φ ∥∞) + log( |Φ |)]∥a∥2
ℓ2 .
Now, let us choose δ= ω/(log2(T)rΦ [log(∥Φ ∥∞) + log( |Φ |)]) where ω depends only on
β,M and f0 and will be chosen later and let us go back to (
7.24):
1
T|a′Ga− a′E(G)a| ≤ □ β,M,f0 ω∥a∥2
ℓ2
+ □ β,f0,ωrΦ [log(∥Φ ∥∞) + log( |Φ |)]
×
∑
ϕ,ρ∈Φ
|aϕ||aρ|∥ϕ∥∞∥ρ∥∞
log5(T)
T
≤ □ β,M,f0 ω∥a∥2
ℓ2 + □ β,f0,ω∥a∥2
ℓ2 AΦ (T).
58 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
Under assumptions of Proposition 5, for T0 large enough and T ≥ T0,
1
T|a′Ga− a′E(G)a| ≤ □ β,M,f0 ω∥a∥2
ℓ2 .
It is now suﬃcient to take ω small enough and then T0 large enough to obtain (
7.23)
with ǫ<ζ and Proposition 5 is proved.
Arguments for the proof of Proposition 1 are similar. So we just give a brief sketch of
the proof. Now,
Gϕ1,ϕ2 =
M∑
m=1
∫ 1
0
(Y(m)
t )2ϕ1(t,X(m))ϕ2(t,X(m)) dt.
Let β> 0. With probability larger than 1 − 2M−β,
1
M|Gϕ1,ϕ2 − E[Gϕ1,ϕ2 ]| ≤
√
2βvϕ1,ϕ2 log M
M + βbϕ1,ϕ2 log M
3M ,
with
bϕ1,ϕ2 = ∥ϕ1∥∞∥ϕ2∥∞,
vϕ1,ϕ2 = E
(∫ 1
0
(Y(m)
t )2ϕ1(t,X(m))ϕ2(t,X(m)) dt
)2
≤ D∥ϕ1∥∞∥ϕ2∥∞⟨|ϕ1|,|ϕ2|⟩,
where ⟨·,·⟩ denotes the standard L2-scalar product. We have just used the classical Bern-
stein inequality combined with ( 4.2). So, with probability larger than 1 − 2|Φ |2M−β , for
any vector a and any δ> 0,
|a′Ga− E[a′Ga]| ≤ □ D,β
∑
ϕ1,ϕ2
|aϕ1 ||aϕ2 |[δM⟨|ϕ1|,|ϕ2|⟩+ δ−1 log M∥ϕ1∥∞∥ϕ2∥∞]
≤ □ D,β (δMrΦ + δ−1∥Φ ∥2
∞|Φ | log M)∥a∥2
ℓ2 .
We choose δ=
√
∥Φ ∥2
∞|Φ | log M
MrΦ
, so that with probability larger than 1 − 2|Φ |2M−β ,
1
M|a′Ga− E[a′Ga]| ≤ □ D,β
√
∥Φ ∥2
∞rΦ |Φ | log M
M ∥a∥2
ℓ2 .
We use (
4.1) and ( 4.3) to conclude as for Proposition 5 and we obtain Proposition 1.
7.5.2. Proof of Corollary 3
First, let us cut [ −1,T] in ⌊T⌋+ 2 intervals I’s of the type [ a,b) such that the ﬁrst ⌊T⌋+ 1
intervals are of length 1 and the last one is of length strictly smaller th an 1 (eventually it
is just a singleton). Then, any interval of the type [ t− 1,t] for t in [0 ,T] is included into
Lasso and probabilistic inequalities for multivariate poi nt processes 59
the union of two such intervals. Therefore, the event where all th e NI ’s are smaller than
u= N /2 is included into Ω N . It remains to control the probability of the complementary
of this event. By stationarity, all the ﬁrst NI ’s have the same distribution and satisfy
Proposition 2. The last one can also be viewed as the truncation of a stationary po int
process to an interval of length smaller than 1. Therefore, the ex ponential inequality of
Proposition 2 also applies to the last interval. It remains to apply ⌊T⌋ + 2 times this
exponential inequality and to use a union bound.
7.5.3. Proof of Corollary
4
As in the proof of Proposition 3, we use the notation □ . The nonasymptotic part of the
result is just a pure application of Theorem 2, with the choices of Bϕ and Vϕ given by
(5.5) and ( 5.6). The next step consists in controlling the martingale ψ(ϕ)2 • (N− Λ) T on
Ω V,B . To do so, let us apply ( 7.7) to H such that for any m,
H(m)
t = ψ(m)
t (ϕ)21t≤τ ′ ,
with B= B2
ϕ and τ = T and where τ′ is deﬁned in ( 7.1) (see the proof of Theorem 2).
The assumption to be fulﬁlled is checked as in the proof of Theorem 2. But as previously,
on Ω V,B , H• (N − Λ) T = ψ(ϕ)2 • (N − Λ) T and also H2 • Λ T = ψ(ϕ)4 • Λ T . Moreover,
on Ω N ⊂ Ω V,B
H2 • Λ T = ψ(ϕ)4 • Λ T ≤ v:= TM
(
max
m
ν(m) + N max
m,ℓ
h(m)
ℓ
)
B4
ϕ.
Recall that x= αlog(T). So on Ω V,B , with probability larger than 1 − (M+ KM2)e−x =
1 − (M+ KM2)T−α, one has that for all ϕ∈ Φ,
ψ(ϕ)2 • NT ≤ ψ(ϕ)2 • Λ T +
√
2vx+ B2
ϕx
3 .
So that for all ϕ∈ Φ,
ψ(ϕ)2 • NT ≤ □ M,f0 [N ∥ϕ∥2
T + ∥Φ ∥2
∞N 2√
TN log(T)].
Also, since N = log2(T), one can apply Corollary 3, with β= α. We ﬁnally choose c as
in Proposition 5. This leads to the result.
Acknowledgements
We are very grateful to Christine Tuleau-Malot who allowed us to use her R programs
simulating Hawkes processes. The research of Patricia Reynaud-B ouret and Vincent
Rivoirard is partly supported by the french Agence Nationale de la Re cherche (ANR
2011 BS01 010 01 projet Calibration). The authors would like to than k the anonymous
Associate Editor and Referees for helpful comments and suggest ions.
60 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
References
[1] Aalen, O. (1980). A model for nonparametric regression analysis of co unting processes.
In Mathematical Statistics and Probability Theory (Proc. Six th Internat. Conf., Wis/suppress la,
1978). Lecture Notes in Statist. 2 1–25. New York: Springer. MR0577267
[2] Andersen, P.K. , Borgan, Ø. , Gill, R.D. and Keiding, N. (1993). Statistical Mod-
els Based on Counting Processes . Springer Series in Statistics . New York: Springer.
MR1198884
[3] Bacry, E. , Delattre, S. , Hoffmann, M. and Muzy, J.F. (2013). Some limit theorems
for Hawkes processes and application to ﬁnancial statistic s. Stochastic Process. Appl.
123 2475–2499. MR3054533
[4] Bercu, B. and Touati, A. (2008). Exponential inequalities for self-normalized mar tingales
with applications. Ann. Appl. Probab. 18 1848–1869. MR2462551
[5] Bertin, K. , Le Pennec, E. and Rivoirard, V. (2011). Adaptive Dantzig density estima-
tion. Ann. Inst. Henri Poincar´ e Probab. Stat.47 43–74. MR2779396
[6] Bickel, P.J. , Ritov, Y. and Tsybakov, A.B. (2009). Simultaneous analysis of lasso and
Dantzig selector. Ann. Statist. 37 1705–1732. MR2533469
[7] Bowsher, C.G. (2010). Stochastic kinetic models: Dynamic independence, modularity and
graphs. Ann. Statist. 38 2242–2281. MR2676889
[8] Br´emaud, P. (1981). Point Processes and Queues . New York: Springer. MR0636252
[9] Br´emaud, P. and Massouli´e, L. (1996). Stability of nonlinear Hawkes processes. Ann.
Probab. 24 1563–1588.
MR1411506
[10] Brette, R. and Destexhe, A. , eds. (2012). Handbook of Neural Activity Measurement .
Cambridge: Cambridge Univ. Press. MR2976674
[11] Brunel, E. and Comte, F. (2005). Penalized contrast estimation of density and hazar d
rate with censored data. Sankhy¯ a67 441–475. MR2235573
[12] Brunel, E. and Comte, F. (2008). Adaptive estimation of hazard rate with censored da ta.
Comm. Statist. Theory Methods 37 1284–1305. MR2440441
[13] B¨uhlmann, P. and van de Geer, S. (2011). Statistics for High-dimensional Data . Hei-
delberg: Springer. MR2807761
[14] Bunea, F. and McKeague, I.W. (2005). Covariate selection for semiparametric hazard
function regression models. J. Multivariate Anal. 92 186–204. MR2102251
[15] Bunea, F. , Tsybakov, A. and Wegkamp, M. (2007). Sparsity oracle inequalities for the
Lasso. Electron. J. Stat. 1 169–194. MR2312149
[16] Bunea, F. , Tsybakov, A.B. and Wegkamp, M.H. (2006). Aggregation and sparsity via
l1 penalized least squares. In Learning Theory. Lecture Notes in Computer Science
4005 379–391. Berlin: Springer. MR2280619
[17] Bunea, F. , Tsybakov, A.B. and Wegkamp, M.H. (2007). Sparse density estimation with
ℓ1 penalties. In Learning Theory. Lecture Notes in Computer Science 4539 530–543.
Berlin: Springer. MR2397610
[18] Bunea, F. , Tsybakov, A.B. and Wegkamp, M.H. (2007). Aggregation for Gaussian
regression. Ann. Statist. 35 1674–1697. MR2351101
[19] Candes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p is
much larger than n. Ann. Statist. 35 2313–2351. MR2382644
[20] Carstensen, L. , Sandelin, A. , Winther, O. and Hansen, N.R. (2010). Multivariate
Hawkes process models of the occurrence of regulatory eleme nts and an analysis of the
pilot ENCODE regions. BMC Bioinformatics 11 456.
Lasso and probabilistic inequalities for multivariate poi nt processes 61
[21] Chagny, G. (2012). Adaptive warped kernel estimators. Available at
http://hal.archives-ouvertes.fr/hal-00715184.
[22] Chornoboy, E.S. , Schramm, L.P. and Karr, A.F. (1988). Maximum likelihood identi-
ﬁcation of neural point process systems. Biol. Cybernet. 59 265–275. MR0961117
[23] Comte, F. , Ga¨ıffas, S. and Guilloux, A. (2011). Adaptive estimation of the conditional
intensity of marker-dependent counting processes. Ann. Inst. Henri Poincar´ e Probab.
Stat. 47 1171–1196. MR2884230
[24] Daley, D.J. and Vere-Jones, D. (2003). An Introduction to the Theory of Point Pro-
cesses. Vol. I , 2nd ed. New York: Springer. MR1950431
[25] de la Pe ˜na, V.H. (1999). A general class of exponential inequalities for mar tingales and
ratios. Ann. Probab. 27 537–564. MR1681153
[26] de la Pe ˜na, V.H. , Lai, T.L. and Shao, Q.-M. (2009). Self-normalized Processes. Berlin:
Springer. MR2488094
[27] Dzhaparidze, K. and van Zanten, J.H. (2001). On Bernstein-type inequalities for mar-
tingales. Stochastic Process. Appl. 93 109–117. MR1819486
[28] Fu, W.J. (1998). Penalized regressions: The bridge versus the lasso . J. Comput. Graph.
Statist. 7 397–416. MR1646710
[29] Ga¨ıffas, S. and Guilloux, A. (2012). High-dimensional additive hazards models and the
Lasso. Electron. J. Stat. 6 522–546. MR2988418
[30] Gr´egoire, G. (1993). Least squares cross-validation for counting proce ss intensities. Scand.
J. Statist. 20 343–360. MR1276698
[31] Gr¨un, S. , Diesmann, M. , Grammont, F. , Riehle, A. and Aertsen, A. (1999). Detecting
unitary events without discretization in time. J. Neurosci. Meth. 94 67–79.
[32] Gusto, G. and Schbath, S. (2005). FADO: A statistical method to detect favored or
avoided distances between occurrences of motifs using the H awkes’ model. Stat. Appl.
Genet. Mol. Biol. 4 Art. 24, 28 pp. (electronic). MR2170440
[33] H¨ardle, W. , Kerkyacharian, G. , Picard, D. and Tsybakov, A. (1998). Wavelets,
Approximation, and Statistical Applications. Lecture Notes in Statistics129. New York:
Springer. MR1618204
[34] Hawkes, A.G. (1971). Point spectra of some mutually exciting point proce sses. J. Roy.
Statist. Soc. Ser. B 33 438–443. MR0358976
[35] Huang, J. , Ma, S. and Zhang, C.-H. (2008). Adaptive Lasso for sparse high-dimensional
regression models. Statist. Sinica 18 1603–1618. MR2469326
[36] Jacobsen, M. (2006). Point Process Theory and Applications. Marked Point and Piecewise
Deterministic Processes. Boston, MA: Birkh¨ auser. MR2189574
[37] Koltchinskii, V. , Lounici, K. and Tsybakov, A.B. (2011). Nuclear-norm penalization
and optimal rates for noisy low-rank matrix completion. Ann. Statist. 39 2302–2329.
MR2906869
[38] Krumin, M. , Reutsky, I. and Shoham, S. (2010). Correlation-based analysis and gen-
eration of multiple spike trains using Hawkes models with an exogenous input. Front.
Comp. Neurosci 4. 147.
[39] Letue, F. (2000). Mod` ele de Cox: Estimation par s´ election de mod` el e et mod` ele de chocs
bivari´ e. Ph.D. thesis.
[40] Liptser, R. and Spokoiny, V. (2000). Deviation probability bound for martingales with
applications to statistical estimation. Statist. Probab. Lett. 46 347–357. MR1743992
[41] Massart, P. (2007). Concentration Inequalities and Model Selection. Lectures from the
33rd Summer School on Probability Theory held in Saint-Flou r, July 6–23, 2003 . Lec-
ture Notes in Math. 1896. Berlin: Springer. MR2319879
62 N.R. Hansen, P. Reynaud-Bouret and V. Rivoirard
[42] Masud, M.S. and Borisyuk, R. (2011). Statistical technique for analysing functional
connectivity of multiple spike trains. J. Neurosci. Meth. 196 201–219.
[43] Meinshausen, N. (2007). Relaxed Lasso. Comput. Statist. Data Anal. 52 374–393.
MR2409990
[44] Mitchell, L. and Cates, M.E. (2010). Hawkes process as a model of social interactions:
A view on video dynamics. J. Phys. A 43 045101, 11. MR2578723
[45] Pernice, V. , Staude, B. , Cardanobile, S. and Rotter, S. (2011). How structure
determines correlations in neuronal networks. PLoS Comput. Biol. 7 e1002059, 14.
MR2821638
[46] Pernice, V. , Staude, B. , Cardanobile, S. and Rotter, S. (2012). Recurrent interac-
tions in spiking networks with arbitrary topology. Phys. Rev. E 85 031916.
[47] Pillow, J.W. , Shlens, J. , Paninski, L. , Sher, A. , Litke, A.M. , Chichilnisky, E.J.
and Simoncelli, E.P. (2008). Spatio-temporal correlations and visual signalli ng in a
complete neuronal population. Nature 454 995–999.
[48] Reynaud-Bouret, P. (2003). Adaptive estimation of the intensity of inhomogene ous Pois-
son processes via concentration inequalities. Probab. Theory Related Fields 126 103–
153. MR1981635
[49] Reynaud-Bouret, P. (2006). Penalized projection estimators of the Aalen multi plicative
intensity. Bernoulli 12 633–661. MR2248231
[50] Reynaud-Bouret, P. and Rivoirard, V. (2010). Near optimal thresholding estimation
of a Poisson intensity on the real line. Electron. J. Stat. 4 172–238. MR2645482
[51] Reynaud-Bouret, P. and Roy, E. (2006). Some non asymptotic tail estimates for Hawkes
processes. Bull. Belg. Math. Soc. Simon Stevin 13 883–896. MR2293215
[52] Reynaud-Bouret, P. and Schbath, S. (2010). Adaptive estimation for Hawkes processes;
application to genome analysis. Ann. Statist. 38 2781–2822. MR2722456
[53] Reynaud-Bouret, P. , Tuleau-Malot, C. , Rivoirard, V. and Grammont, F.
(2013). Spike trains as (in)homogeneous Poisson processes or Hawkes pro-
cesses: Nonparametric adaptive estimation and goodness-o f-ﬁt tests. Available at
http://hal.archives-ouvertes.fr/hal-00789127.
[54] Rudelson, M. and Vershynin, R. (2008). On sparse reconstruction from Fourier and
Gaussian measurements. Comm. Pure Appl. Math. 61 1025–1045. MR2417886
[55] Rudelson, M. and Vershynin, R. (2009). Smallest singular value of a random rectangular
matrix. Comm. Pure Appl. Math. 62 1707–1739. MR2569075
[56] Rudelson, M. and Vershynin, R. (2010). Non-asymptotic theory of random matrices:
Extreme singular values. In Proceedings of the International Congress of Mathemati-
cians III 1576–1602. New Delhi: Hindustan Book Agency. MR2827856
[57] Shorack, G.R. and Wellner, J.A. (1986). Empirical Processes with Applications to
Statistics. Wiley Series in Probability and Mathematical Statistics: P robability and
Mathematical Statistics. New York: Wiley. MR0838963
[58] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Roy. Statist.
Soc. Ser. B 58 267–288. MR1379242
[59] van de Geer, S. (1995). Exponential inequalities for martingales, with ap plication to
maximum likelihood estimation for counting processes. Ann. Statist. 23 1779–1801.
MR1370307
[60] van de Geer, S. , B¨uhlmann, P. and Zhou, S. (2011). The adaptive and the thresholded
Lasso for potentially misspeciﬁed models (and a lower bound for the Lasso). Electron.
J. Stat. 5 688–749. MR2820636
Lasso and probabilistic inequalities for multivariate poi nt processes 63
[61] van de Geer, S.A. (2008). High-dimensional generalized linear models and th e lasso. Ann.
Statist. 36 614–645. MR2396809
[62] Vere-Jones, D. and Ozaki, T. (1982). Some examples of statistical estimation applied
to earthquake data I: Cyclic Poisson and self-exciting mode ls. Ann. I. Stat. Math. 34
189–207.
[63] Willett, R.M. and Nowak, R.D. (2007). Multiscale Poisson intensity and density esti-
mation. IEEE Trans. Inform. Theory 53 3171–3187. MR2417680
[64] Zou, H. (2006). The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc. 101
1418–1429. MR2279469
Received September 2012 and revised July 2013