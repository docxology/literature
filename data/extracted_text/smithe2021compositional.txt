Compositional Active Inference I:
Bayesian Lenses. Statistical Games.
Toby St. Clere Smithe
University of Oxford
&
Topos Institute
toby@topos.institute
June 10, 2022
We introduce the concepts of Bayesian lens , characterizing the bidirectional structure of exact
Bayesian inference, and statistical game , formalizing the optimization objectives of approximate
inference problems. We prove that Bayesian inversions compose according to the compositional
lens pattern, and exemplify statistical games with a number of classic statistical concepts, from
maximum likelihood estimation to generalized variational Bayesian methods. This paper is the
/f_irst in a series laying the foundations for a compositional account of the theory of active infer-
ence, and we therefore pay particular attention to statistical games with a free-energy objective.
1. Introduction
Those systems that we might classify as ‘cybernetic’, ‘adaptive’, or ‘alive’ all display a fundamental property:
they resist perturbations that would push them away from their goals or render their existence unsustainable.
In order to do so, such systems are somehow able to sense their current state of aﬀairs (through perception)
and respond appropriately (through action). In the series of papers of which this is the /f_irst part, we seek
to supply new compositional foundations for a theory of active inference adequate to describe such systems,
with a particular focus on the framework that has come to be known in the compositional neuroscience and
arti/f_icial life communities as thefree energy principle [1], whose structures we seek to make precise.
A central feature of active inference is the use of the statistical procedure called Bayesian inference, which
supplies a recipe by which a system might invert a statistical model (say, of how causes generate observations)
in order to form beliefs about the causes of observed data. It is easy to see how such a process of inferring
causes could be understood as a process of perception, but the central dogma of active inference is that both
perception and action can be rendered as problems of Bayesian inference, with action being ‘dual’ to percep-
tion: instead of changing its internal state (its beliefs about causes) to match its observations better, a system
might act to change theexternal state (the state of the world) so that the observations that it expects or desires
obtain. In the free energy framework, both perception and action emerge through the optimization of a single
quantity, the free energy .
1
arXiv:2109.04461v2  [math.ST]  8 Jun 2022
Such processes of optimization, and perception and action more generally, are inherently dynamical pro-
cesses. In this /f_irst paper of the series, we put the dynamics temporarily aside, and lay the statistical founda-
tions, characterizing the compositional structure of the generative models instantiated by cybernetic systems
and the algebra of their inversion. This algebra is formalized by our concept ofBayesian lens, which we intro-
duce to characterize the inherently bidirectional structure of Bayesian inversion, drawing on the ‘lens’ pattern
that structures bidirectional systems from economic games [2], to databases [3], and machine-learners [4].
This lens structure serves more than an organizing purpose: we prove that the inversion of a composite
or ‘hierarchical’ statistical model is equivalently given (up to almost-equality) by the lens composition rule.
This means that cybernetic systems embodying complex composite models can simply invert each component
factor of their models and then combine these inversions, in order to obtain an inversion of the whole. In turn,
this explains the observation that hierarchical systems in the brain (such as much of the visual cortex) can
be explained as a composite of ‘local’ circuits each performing a form of approximate Bayesian inference call
predictive coding [5].
Having established the structures required to state and prove that “Bayesian updates compose optically”1,
we formalize the “algebra of statistical inference problems” as categories of statistical games . These ‘games’
consist of a lens paired with a contextual /f_itness function, which de/f_ine the quantities that we often think
of cybernetic systems as optimizing, and where the ‘context’ formalizes the system’s interaction with its
environment. In this development, we draw much inspiration from compositional game theory [2, 7]. We ex-
emplify these statistical games with a range of examples from maximum likelihood estimation to generalized
variational Bayesian methods.
This paper is the /f_irst of a series of papers. The next instalment introduces the structures necessary to supply
statistical games with “dynamical semantics”, and thus breathe some life into those systems that perform
approximate inference. A subsequent paper will then explain how such systems can perform action, and
thereby aﬀect the worlds that they inhabit.
Overview of this paper We begin in §2.1 by introducing the basics of compositional probability theory.
In §2.2, we introduce the structures necessary to describe the lens pattern, and recall that in ‘nice’ situations,
the resulting categories of lenses are monoidal. In §2.3, we introduce thePara construction, which has been
proposed [8] as foundational for categorical cybernetics, and often plays an important role for us, too. Then,
in §3, we de/f_ineBayesian lenses and prove the theorem that Bayesian inversions compose according to the lens
pattern; we also give a detailed description of the low-dimensional structure of parameterized Bayesian lenses
(§3.4). In §4, we de/f_ine contexts for Bayesian lenses, /f_itness functions, and the resulting monoidal categories
of statistical games. Finally, in §5, we give a number of examples.
Acknowledgements This series of papers is the result of a number of interactions within and around the
applied category theory and active inference communities. We thank the reviewers and organizers of the ACT
conferences in 2020, 2021, and 2022, as well as those of the SYCO series of symposia. We thank the Topos
Institute and Foundational Questions Institute for support and funding. And we particularly thank the follow-
ing individuals (in no particular order) for stimulating discussions, comments, and encouragement: Samson
Abramsky; Matteo Capucci; Bob Coecke; Lance da Costa; Brendan Fong; Karl Friston; Bruno Gavranović;
Neil Ghani; Jules Hedges; Johannes Kleiner; Tim Hosgood; Sophie Libkind; David Jaz Myers; Valeria de Paiva;
Evan Patterson; Maxwell Ramstead; Dalton Sakthivadivel; Brandon Shapiro; David Spivak; Sam Staton; Sean
Tull; Vincent Wang.
1Although we do not use the heavier machinery of ‘optics’ here; for that, see our preprint [6] and Remark 2.41 below.
2
2. Mathematical background for statistical games
2.1. Compositional probability, concretely and abstractly
In order to de/f_ine Bayesian lenses and statistical games and prove some basic results about them, we will work
at a high level of abstraction; then, to exemplify them with applications, we will need to work more concretely.
In both instances, our basic categorical setting will be acopy-delete or Markov category, whose morphisms we
will call stochastic channels or Markov kernels and which behave like functions with uncertain outputs; for
us, these model the processes by which observational data are (believed to be) generated by processes in the
world, and composite channels model composite (sequences of) processes.
We will typically be interested in applications where the sample spaces are continuous and where the
probability measures may have in/f_inite support. A stochastic channel c : XÑ‚Y will be something like a
function taking values in probability measures over a space, but in applications one will often /f_ix a reference
measure and then work with a density function pc : X ˆY Ñ r0,1srepresenting the channel c with
respect to that measure. We begin this section by introducing the category sfKrn of s-/f_inite kernels, where
these concepts have precise meanings, before generalizing graphically to the abstract setting of copy-delete
categories.
2.1.1. S-finite kernels
We sketch the basic structure of the categorysfKrn of s-/f_inite kernels between measurable spaces, and refer
the reader to Cho and Jacobs [9] and Staton [10] for elaboration of the details.
De/f_inition 2.1.Suppose pX,ΣXqand pY,ΣYqare measurable spaces, with X,Y sets and ΣX,ΣY the cor-
responding σ-algebras. A kernel kfrom Xto Y is a function k: XˆΣY Ñr0,8ssatisfying the conditions:
• for all xPX, kpx,´q: ΣY Ñr0,8sis a measure; and
• for all B PΣY, kp´,Bq: X Ñr0,8sis measurable.
A kernel k : XˆΣY Ñr0,8sis /f_initeif there exists some rPr0,8qsuch that, for all xPX, kpx,Y qď r.
And kis s-/f_initeif it is the sum of at most countably many /f_inite kernelskn, k“ř
n:N kn.
Proposition 2.2. Measurable spaces and s-/f_inite kernelspX,ΣXqÑ‚pY,ΣYqbetween them form a category,
denoted sfKrn; note that often we will just writeXfor a measurable space, leaving theσ-algebra ΣX implicit.
Identity morphisms idX : XÑ‚X in sfKrn are Dirac kernels δX : X ˆΣX Ñ r0,8s :“ xˆA ÞÑ 1 iﬀ
x PAand 0 otherwise. Composition is given by a Chapman-Kolmogorov equation: suppose c : XÑ‚Y and
d: YÑ‚Z. Then
d‚c: XˆΣZ Ñr0,8s:“xˆC ÞÑ
ż
y:Y
dpC|yqcpdy|xq
where we have used ‘conditional probability’ notationdpC|yq:“dpy,Cq.
Remark 2.3. In the scienti/f_ic literature, one often encounters the termconditional probability distribution ,
which can almost always be interpreted as indicating a probability kernel: we can think of a probability kernel
c : XÑ‚Y as a function that emits a probability distribution cpxq: ΣY Ñr0,8sover Y for each x : X. We
can then see cpxqas a probability distribution conditional on the choice or observation of x : X, hence the
notation cp´|xq. One reads this notation ´|xas “´given x”.
Proposition 2.4. There is a monoidal structurepb,1qon sfKrn, the unit of which is the singleton set1 with
its trivial sigma-algebra. On objects, XbY is the Cartesian product pXˆY,ΣXˆYqof measurable spaces,
3
with the product sigma-algebra. On morphisms, f bg: XbYÑ‚AbBis given by
f bg: pXˆYqˆ ΣAˆB :“pxˆyqˆ E ÞÑ
ż
a:A
ż
b:B
δAbBpE|x,yqfpda|xqgpdb|yq
where, as above,δAbBpE|a,bq“ 1 iﬀ pa,bqP Eand 0 otherwise. Note thatpfbgqpE|x,yq“p gbfqpE|y,xq
for all s-/f_inite kernels (and allE, xand y), by the Fubini-Tonelli theorem for s-/f_inite measures [9, 10], and so
bis symmetric on sfKrn.
Remark 2.5 (States and eﬀects). We will call kernels with domain 1 states. A kernel 1Ñ‚X is equivalently a
function ΣX Ñr0,8s, which is simply a (possibly improper) measure on the space pX,ΣXq. Occasionally
we will say distribution to mean ‘state’. We call a state on a product space, such asω: 1Ñ‚XbY, a joint state
or joint distribution .
Dually, kernels with codomain 1 will be called eﬀects. Note that although 1 is the unit of the preceding
monoidal structure, this unit is not terminal in sfKrn: s-/f_inite kernelsXÑ‚1 are equivalently measurable
functions X Ñ r0,8s, and there are of course many nontrivial examples; of which density functions will
form an important class.
De/f_inition 2.6(Probability kernel, probability measure, probability space). If a kernel k: XˆΣY Ñr0,8s
satis/f_ies the additional conditions that it takes values in the unit intervalr0,1sand, for all x: X, kpYq“ 1,
then we callka probability kernel. If kis a state (i.e., X “1), then we call it aprobability measure. A probability
space is a pair pΩ,πqof a measurable space Ω with a probabilty measure π: 1Ñ‚Ω.
Remark 2.7 (Giry monad). Probability measures 1Ñ‚Xon each Xform the points of a space which we will
denote GX. This space can be equipped with a canonicalσ-algebra, making G into a functor Meas ÑMeas
which acts on each measurable function f : X ÑY by returning its pushforward f˚ : GX ÑGY, de/f_ined
by f˚ν : ΣY Ñr0,1s: B ÞÑνpf´1pBqq. G can in turn be equipped with a monad structure pµ,ηq. Every
measurable function X ÑGY corresponds to a probability kernelXÑ‚Y, making the Kleisli category KℓpGq
a subcategory of sfKrn. Composition of probability kernels X
f
Ý Ñ‚Y
g
Ý Ñ‚Z corresponds accordingly to Kleisli
composition X
f
Ý ÑGY
Gg
ÝÑGGZ
µZ
Ý Ý ÑGZ, and the components of the monad unit correspond to the Dirac
(identity) kernels δX : XÑ‚X. We refer the reader to Fritz [11, §4] and the references therein for elaboration.
The slice 1{KℓpGqof KℓpGqunder 1 is the category of probability spaces and measure-preserving kernels
between them, called ProbStoch by Fritz [11].
Remark 2.8 (Convex spaces and expectations) . A convex space is an algebra of the Giry monad; that is, a
space Xequipped with a measurable functionGX ÑXcalled the algebra evaluation orexpected value. Each
measurable function f : X ÑX induces an expected value Ef : GX ÑX de/f_ined as
Efpπq:“
ż
x:X
fpxqπpdxq.
We will typically instead write
E
x„π
rfs:“Efpπq
where the notation x „ π should be read as “ x distributed according to π”. More generally, we have an
operator E : MeaspΩ,Xqˆ GΩ ÑX de/f_ined similarly by
E
ω„π
rps:“
ż
ω:Ω
ppωqπpdωq
where p : Ω ÑX and π : GΩ. Note that this subsumes the case where pis an X-valued random variable
de/f_ined on a probability spacepΩ,πq. Commonly, we will have X “R or X “r0,8s.
4
Remark 2.9 (Eﬀects and validities). A special case of the preceding expectation operator occurs when X “
r0,8s. In this case, maps Ω Ñr0,8sare of course eﬀects ΩÑ‚1 in sfKrn, and for each state π : 1Ñ‚Ω and
eﬀect p: ΩÑ‚1, the expectation operator simply computes the composte p‚ω. That is, we have in this case
Eω„πrps “p‚ω, and we might then call pa predicate on Ω and the expectation Eω„πrpsthe validity of p
in the state π. We refer the reader to Cho et al. [12, §5] for more on this perspective, where the validity is
written π|ùp.
Observation 2.10. Each spaceXin sfKrn is equipped with a canonical eﬀect X : XÑ‚1, the constant eﬀect
xÞÑ1. We denote this family of eﬀects by the ‘ground’ symbol and call the components discarding maps
with the intuition that they act by ‘discarding’ information (“wiring to ground”). Another way to characterize
probability kernels, and thus KℓpGq, is that they make discarding natural, satisfying ‚c“ . We call such
channels or processescausal: they cannot aﬀect the outcomes of processes earlier in a sequence of composites.
Not only can we discard information in sfKrn, but we can also copy it. In other words, each object is
equipped with a canonical copy-delete structure, making it into a comonoid, with which we can duplicate and
discard states of the corresponding type. In the terms of Fong and Spivak [13], sfKrn supplies comonoids.
Proposition 2.11 (sfKrn supplies comonoids). Each object Xis equipped with a canonical comonoid struc-
ture p X, Xq, with X : XÑ‚X bX and X : XÑ‚1 satisfying the usual comonoid laws. Discarding is
given by the family of eﬀects X : X Ñr0,8s:“xÞÑ1, and copying is Dirac-like: X : XˆΣXˆX :“
xˆE ÞÑ1 iﬀ px,xqP Eand 0 otherwise.
Discarding part of a joint state gives us marginals (or “marginal distributions”).
De/f_inition 2.12.Given a joint distribution ω : 1 Ñ‚X bY, we call ωX :“ pidX b Yq‚ ω : 1 Ñ‚X and
ωY :“p X bidYq‚ ω : 1Ñ‚Y the marginals of ω. We de/f_ine projection (ormarginalization) operators by
projX :“idX b Y : XbY ÑXb1 „Ý ÑX and projY :“ X bidY : XbY Ñ1 bY „Ý ÑY.
In this work, we are interested in the problem of inverting stochastic channels XÑ‚Y in order to obtain
channels YÑ‚X, and we are particularly interested in what is known as Bayesian inversion. As we will see,
the Bayesian inversion of a channel c: XÑ‚Y is determined in conjunction with a state π: 1Ñ‚X.
De/f_inition 2.13.We call the pairing pπ,cqof a state π : 1Ñ‚X with a channel c: XÑ‚Y a generative model
XÑ‚Y. It induces a joint distribution ωpπ,cq :“pidX bcq‚ X ‚π : 1Ñ‚XbY. The marginals of ωpπ,cq are
πand c‚π.
In the informal scienti/f_ic literature, the Bayesian inversion of a channelXÑ‚Y (typically called the ‘likeli-
hood’) with respect to a state1Ñ‚X (typically called the ‘prior’) is often written as the expression
ppx|yq“ ppy|xqppxq
ppyq “ ppy|xqppxqş
x1:X ppy|x1qppx1qdx1 , (1)
but this expression is very ill-de/f_ined: what isppy|xq, and how does it relate to a channel c : XÑ‚Y? Why
are the clearly diﬀerent terms ppx|yq,ppy|xq,ppxq,ppyqall written with the same symbol p?
To answer these questions and clarify such expressions, we use density functions.
De/f_inition 2.14(Density functions). We will say that a kernel c : XÑ‚Y is represented by the eﬀect pc :
XbYÑ‚1 with respect to the state µ: 1Ñ‚Y when
c: XˆΣY Ñr0,8s:“xˆB ÞÑ
ż
y:B
µpdyqpcpy|xq.
We call the corresponding function pc : X ˆY Ñ r0,8sa density function for c. Note that we also use
conditional probability notation for density functions, and so pcpy|xq:“pcpx,yq.
5
Remark 2.15. When a channel c is associated with a density function, we will adopt the convention of
naming the density function pc; that is, with a subscript indicating the corresponding channel. In this way,
we can rewrite Equation (1) as
pc:
π
px|yq“ pcpy|xqpπpxq
pc‚πpyq “ pcpy|xqpπpxqş
x1:X pcpy|x1qpπpx1qdx1 . (2)
Remark 2.16. We will also adopt the convention of denoting a Bayesian inversion of the channel c with
respect to the state π by the symbol c:
π. We adopt this symbol because it is known that Bayesian inversion
induces a ‘dagger’ functor [14] on (a quotient of) the category 1{KℓpGqof probability spaces and measure-
preserving functions [11, Remark 13.9]; when we ‘forget’ the measures associated with the probability spaces—
which form the ‘priors’ for the inversions—then we have to explicitly incorporate them into the structure,
which we indicate with the subscript πin c:
π.
We wrote “a Bayesian inversion” in the preceding remark since in a general measurable setting Bayesian
inversions need not always exist [15], and when they do they may only be unique up to almost-equality.
De/f_inition 2.17(Almost-equality). Given a state π : 1Ñ‚X, we say that two parallel channels c,d : XÑ‚Y
are π-almost-equal, denoted c π„d, if the joint distributions of the two generative modelspπ,cqand pπ,dqare
equal; that is, if pid bcq‚ ‚π“pid bdq‚ ‚π.
Remark 2.18. Like the notion of joint distribution for a generative model, the meaning of the de/f_inition of
π-almost equality—that the induced joint states are equal—will be rendered clearer in the graphical calculus.
We are now in a position to de/f_ine Bayesian inversions for channels in sfKrn, although we leave the
abstract de/f_inition satis/f_ied by the following until De/f_inition 2.30.
Proposition 2.19 (Cho and Jacobs [9, Example 8.4]). Suppose pπ,cqis a generative model XÑ‚Y in sfKrn,
where cis represented by the eﬀect pc with respect to the state µ: 1Ñ‚X. Then, when it exists, the channel
c:
π : YÑ‚X de/f_ined as follows is a Bayesian inversion ofcwith respect to π:
c:
π : Y ˆΣX Ñr0,8s:“yˆAÞÑ
ˆż
x:A
πpdxqpcpy|xq
˙
p´1
c pyq
“p´1
c pyq
ż
x:A
pcpy|xqπpdxq,
where p´1
c : YÑ‚I is given up to µ-almost-equality by
p´1
c : Y Ñr0,8s:“yÞÑ
ˆż
x:X
pcpy|xqπpdxq
˙´1
.
Note that from the preceding proposition we recover the informal form of Bayes’ rule (Equation (2)). Sup-
pose πis itself represented by a density function pπ with respect to the Lebesgue measure dx. Then
c:
πpA|yq“
ż
x:A
pcpy|xqpπpxqş
x1:X pcpy|x1qpπpx1qdx1 dx.
2.1.2. Copy-delete categories and their graphical calculus
While most of our examples and applications will found in sfKrn, most of our de/f_initions and results hold
more generally, and it is in such more general terms that they are most naturally expressed. Our main language
will be that of categories like sfKrn in which information can be transformed, copied, and deleted.
6
De/f_inition 2.20(Cho and Jacobs [9, Def. 2.2]) . A copy-delete category is a symmetric monoidal category
pC,b,Iqin which every object Xis supplied with a commutative comonoid structure p X, Xqcompatible
with the monoidal structure of pb,Iq. An aﬃne copy-delete category, or Markov category [11], is a copy-
delete category in which every channel c is causal in the sense that ‚c “ . Equivalently, a Markov
category is a copy-delete category in which the monoidal unit I is the terminal object.
Example 2.21. sfKrn is a copy-delete category, while KℓpGqis a Markov category.
Monoidal categories, and (co)monoids within them, admit a formal graphical calculus that substantially
simpli/f_ies many calculations involving complex morphisms: proofs of many equalities reduce to visual demon-
strations of isotopy, and structural morphisms such as the symmetry of the monoidal product acquire intuitive
topological depictions. We make substantial use of this calculus below, and summarize its features here. For
more details, see Cho and Jacobs [9, §2] or Fritz [11, §2] or the references cited therein.
Depiction 2.22 (Basic conventions). String diagrams in this paper will be read vertically, with information
/f_lowing upwards (from bottom to top); in later parts, we will have diagrams oriented left-to-right. Sequential
composition is represented by connecting strings together; and parallel composition bby placing diagrams
adjacent to one another. This way, c: XÑ‚Y, idX : XÑ‚X, d‚c: X cÝ Ñ‚Y dÝ Ñ‚Z, and f bg : XbYÑ‚AbB
are depicted respectively as:
c
X
Y
X
X
d
Z
c
X
f
X
A
g
Y
B
We represent (the identity morphism on) the monoidal unitIas an empty diagram: that is, we leave it implicit
in the graphical representation.
Depiction 2.23 (States and eﬀects). States σ: IÑ‚X and eﬀects η: XÑ‚I will be depicted as follows:
σ
X
η
X
De/f_inition 2.24(Causality). We say that a morphism is causal if it satis/f_ies the following condition, where
is the canonical discarding map (supplied by the copy-delete category structure) of the appropriate type;
compare Observation 2.10.
“c
Remark 2.25. Observe that, if the monoidal unit is terminal, then every morphism is causal.
Depiction 2.26 (Monoidal symmetry). The symmetry of the monoidal structureswapXY : XbY „Ý ÑY bX
is depicted as the swapping of wires, and satis/f_ies the equations below. The left says that swapping is an
7
isomorphism; the right says that it commutes with copying, making every object a commutative comonoid:
“ and “
Depiction 2.27 (Comonoid laws). The copy-delete structurep , qis required to satisfy the comonoid laws,
depicted below, of unitality (left depiction) and associativity (right depiction):
““ and “
Depiction 2.28 (Marginalization of joint states) . The discarding maps induce projections X bY id bÝ ÝÝÝ Ñ
XbI „Ý ÑXand XbY bidÝÝÝÑIbY „Ý ÑY, with which we can obtain the marginals of joint states; compare
De/f_inition 2.12. Suppose then that a joint stateω: IÑ‚XbY has marginals ω1 : IÑ‚Xand ω2 : IÑ‚Y. Then
we have
ω“ω1
X X
and ω “ ω2
YY
.
Depiction 2.29 (Generative models). A generative model pπ,cq: XÑ‚Y induces a joint state ωon XbY by
c
π
X Y
ω
X Y
“
with marginals πand c‚πgiven by
c
π
X
ω
X
“ “
π
X
and
c
π
Y
ω
Y
“ “
π
Y
c
.
(Compare De/f_inition 2.13.)
De/f_inition 2.30(Bayesian inversion). We say that a channelc: XÑ‚Y admits Bayesian inversion with respect
8
to π: IÑ‚X if there exists a channel c:
π : YÑ‚X satisfying the following equation [9, eq. 5]:
c
π
X Y
“
c:
π
π
c
X Y
(3)
We say that cadmits Bayesian inversion tout court if cadmits Bayesian inversion with respect to all states
π : IÑ‚X such that c‚πhas non-empty support. We say that a category C admits Bayesian inversion if all
its morphisms admit Bayesian inversion tout court .
Depiction 2.31 (Density functions). A channel c: XÑ‚Y is said to berepresented by the eﬀect pc : XbYÑ‚I
with respect to µ: IÑ‚Y if
c
X
Y
µ
p
X
Y
“ .
We call pc a density function for c; compare De/f_inition 2.14.
De/f_inition 2.32(Almost-equality). Given a state π : IÑ‚X, we say that two channels c : XÑ‚Y and d :
XÑ‚Y are π-almost-equal, denoted c π„d, if
c
π
X Y
–
d
π
X Y
.
Proposition 2.33 (Composition preserves almost-equality). If c π„d, then f ‚c π„f ‚d.
Proof. Immediate from the de/f_inition of almost-equality.
9
Proposition 2.34 (Bayesian inverses are almost-equal). Suppose α: YÑ‚Xand β : YÑ‚Xare both Bayesian
inversions of the channel c: XÑ‚Y with respect to π: IÑ‚X. Then αc‚π„ β.
Proof. Immediate from Equation (3).
2.2. Lenses for dependent bidirectional processes
The Bayesian inversion of a stochastic channel c: XÑ‚Y is a family of channels c:
π : YÑ‚X in the opposite
direction, indexed by states on X. Pairs pc,c:qof a morphism cwith a c-dependent opposite morphism c:
often fall into the compositional ‘lens’ pattern, and the Bayesian case is no exception. In this section, we
sketch the basic theory of lenses, and refer the reader to our preprint [6] for further exposition. The central
element of the structure is a (pseudo)functor picking out, for each morphismc, the category in whichc:lives.
With this piece to hand, an entire corresponding category of lenses can de/f_ined most concisely.
De/f_inition 2.35(Spivak [16, Def. 3.3]) . The category GrLensF of Grothendieck lenses for a pseudofunctor
F : C op ÑCat is the total category of the Grothendieck construction for the pointwise opposite of F.
Proposition 2.36 (GrLensF is a category). The objects pGrLensFq0 of GrLensF are (dependent) pairs
pC,Xqwith C : C and X : FpCq, and its hom-sets GrLensF
`
pC,Xq,pC1,X1q
˘
are dependent sums
GrLensF
`
pC,Xq,pC1,X1q
˘
“
ÿ
f: CpC,C1q
FpCq
`
FpfqpX1q,X
˘
so that a morphism pC,Xq ÞÑpC1,X1qis a pair pf,f :qof f : CpC,C1qand f: : FpCq
`
FpfqpX1q,X
˘
. We
call such pairs Grothendieck lenses for F or F-lenses.
Proof sketch. The identity Grothendieck lens on pC,Xqis idpC,Xq “pidC,idXq. Sequential composition is as
follows. Given pf,f :q: pC,Xq ÞÑpC1,X1qand pg,g:q: pC1,X1q ÞÑpD,Y q, their composite pg,g:qpf,f :q
is de/f_ined to be the lens
`
g˝f,F pfqpg:q
˘
: pC,Xq ÞÑ pD,Y q. Associativity and unitality of composition
follow from functoriality of F.
De/f_inition 2.37.Suppose FpCq0 “ C0, with F : C op Ñ Cat a pseudofunctor. De/f_ineSimpGrLensF
to be the full subcategory of GrLensF whose objects are duplicate pairs pC,Cqof objects C in C. We call
SimpGrLensF the category of simple F-lenses. More generally, any lens between such duplicate pairs will
be called asimple lens. Since duplicating the objects in the pairspX,Xqis redundant, we will write the objects
simply as X.
Another name for a pseudofunctor F : C op Ñ Cat is an indexed category . When C is a monoidal cat-
egory with which F is appropriately compatible, then we can ‘upgrade’ the notions of indexed category
and Grothendieck construction accordingly. In this work, the domain categories C are only trivially bicate-
gories, and the pseudofunctors F are really just functors; we will restrict ourselves to the 1-categorical case
of monoidal indexed categories, too.
De/f_inition 2.38(Moeller and Vasilakopoulou [17, §3.2]). Suppose pC,b,Iqis a monoidal category. We say
that F is a monoidal indexed category when F is a weak lax monoidal functor pF,µ,µ 0q: pC op,bop,IqÑ
pCat,ˆ,1q. This means that the laxator µis given by a natural family of functors µA,B : FA ˆFB ñ
FpAbBqalong with, for any morphisms f : A ÑA1 and g : B ÑB1 in C, a natural isomorphism µf,g :
µA1,B1 ˝pFf ˆFgqñ Fpf bgq˝ µA,B. The laxator and the unitor µ0 : 1 ÑFI together satisfy standard
axioms of associativity and unitality. All told, this structure makes pF,b,µ,I,µ 0qinto a pseudomonoid in
the monoidal 2-category of indexed categories and indexed functors.
10
Proposition 2.39 (Moeller and Vasilakopoulou [17, §6.1]). Suppose pF,µ,µ 0q: pC op,bop,IqÑp Cat,ˆ,1q
is a monoidal indexed category. Then the total category of the Grothendieck construction
ş
F obtains a
monoidal structure pbµ,Iµq. On objects, de/f_ine
pC,Xqbµ pD,Y q:“
`
CbD,µCDpX,Y q
˘
where µCD : FC ˆFD ÑFpCbDqis the component of µat pC,Dq. On morphisms pf,f :q: pC,Xq ÞÑ
pC1,X1qand pg,g:q: pD,Y q ÞÑpD1,Y 1q, de/f_ine
pf,f :qbµ pg,g:q:“
`
f bg,µCDpf:,g:q
˘
.
The monoidal unit Iµ is de/f_ined to be the objectIµ :“
`
I,µ0p˚q
˘
. Writing λ : I bp´q ñ p´qand ρ :
Cbp´qñp´q for the left and right unitors of the monoidal structure on C, the left and right unitors in
ş
F
are given by pλ,idqand pρ,idqrespectively. Writing αfor the associator of the monoidal structure on C, the
associator in
ş
F is given by pα,idq.
Corollary 2.40. When F : C op ÑCat is equipped with a (weak) lax monoidal structurepµ,µ0q, its category
of lenses GrLensF becomes a monoidal category pGrLensF,b1
µ,Iµq. On objects b1
µ is de/f_ined asbµ in
Proposition 2.39, as isIµ. On morphisms pf,f :q: pC,Xq ÞÑpC1,X1qand pg,g:q: pD,Y q ÞÑpD1,Y 1q, de/f_ine
pf,f :qb1
µ pg,g:q:“
`
f bg,µop
CDpf:,g:q
˘
where µop
CD : FpCqop ˆFpDqop Ñ FpC bDqop is the pointwise opposite of µCD. The associator and
unitors are de/f_ined as in Proposition 2.39.
Remark 2.41. An alternative perspective on lenses is given by the family of structures known asoptics [18],
which generalize lenses using the kind of actegorical machinery to which we now turn. This machinery puts
the categories of forwards and backwards maps on equal footing, unlike the /f_ibrational machinery developed
here (which privileges the base category), at the cost of the explicit dependence structure and somewhat
heavier categorical tooling. The synthesis of Grothendieck lenses and optics, dependent optics , has recently
been articulated [19–21]; while powerful, this structure demands both the machinery of /f_ibrations and of
actegories. For its relative simplicity, we therefore stick to the /f_ibrational lens perspective in this paper.
2.3. Categories with parameters
In many applications, we will be interested in cybernetic systems, where a single system might have some
freedom in the choice of forwards and backwards channel; consider the synaptic strengths or weights of a
neural network, which change as the system learns about the world, aﬀecting the predictions it makes and
actions it takes. This freedom is well modelled by equipping the morphisms of a category with parameters, and
gives rise to a notion ofparameterized category. In general, the parameterization may have diﬀerent structure
to the processes at hand, and so we describe the ‘actegorical’ situation in which a category of parametersM
acts on on a category of processes C, generating a category of parameterized processes.
De/f_inition 2.42(M-actegory). Suppose M is a monoidal category with tensor b and unit object I. We say
that C is aleft M-actegory when C is equipped with a functord: M ÑCatpC,Cqcalled theaction along with
natural unitor and associator isomorphismsλd
X : IdX „Ý ÑXand ad
M,N,X : pMbNqdX „Ý ÑMdpNdXq
compatible with the monoidal structure of pM,b,Iq.
Proposition 2.43 (Capucci et al. [8]). Let pC,d,λd,adqbe an pM,b,Iq-actegory. Then there is a bicategory
of M-parameterized morphisms in C, denoted Parapdq. Its objects are those of C. For each pair of objects
11
X,Y , the set of 1-cells is de/f_ined asParapdqpX,Y q:“ř
M:M CpMdX,Y q; we denote an element pM,f q
of this set by f : X MÝÑY. Given 1-cells f : X MÝÑY and g: Y NÝ ÑZ, their composite g˝f : X NbMÝ ÝÝÝ ÑZis
the following morphism in C:
pN b Mqd X
ad
N,M,X
Ý ÝÝÝÝ ÑN dpM dXq
idN df
ÝÝÝÝÑN dY
g
Ý ÑZ
Given 1-cells f : X MÝÑY and f1 : X M1
Ý Ý ÑY, a 2-cell α : f ñf1 is a morphism α : M ÑM1 in M such
that f “f1˝pαdidXqin C; identities and composition of 2-cells are as in C.
Proposition 2.44. When C is equipped with both a symmetric monoidal structure pb,Iqand an pM,b,Iq-
actegory structure, and there is a natural isomorphism µr
M,X,Y : MdpXbYq „Ý ÑXbpMdYqcalled the
(right) costrength2, the symmetric monoidal structure pb,Iqlifts to Parapdq. First, from the symmetry of b,
one obtains a (left) costrength,µl
M,X,Y : MdpXbYq „Ý ÑpMdXqbY. Second, using the two costrengths and
the associator of the actegory structure, one obtains a natural isomorphismιM,N,X,Y : pMbNqdpXbYq „Ý Ñ
pMdXqbp NdYqcalled the interchanger. The tensor of objects in Parapdqis then de/f_ined as the tensor
of objects in C, and the tensor of morphisms (1-cells) f : X MÝÑY and g: A NÝ ÑBis given by the composite
f bg: XbA MbNÝ ÝÝÝ ÑY bB :“ pM b Nqdp XbAq
ιM,N,X,A
Ý ÝÝÝÝÝ ÑpM dAqbp N dAq
fbg
ÝÝÑY bB.
In many simple cases, the parameters will live in the same category as the morphisms being parameterized;
this is formalized by the following proposition.
Proposition 2.45. If pC,b,Iqis a monoidal category, then it induces a parameterization Parapbqon itself.
For each M,X,Y : C, the morphisms X MÝÑY of Parapbqare the morphisms M bX ÑY in C.
Notation 2.46. When considering the self-paramterization induced by a monoidal categorypC,b,Iq, we will
often write ParapCqinstead of Parapbq.
It will frequently be the case that we do not in fact need the whole bicategory structure. The following
proposition tells us that we can also just work 1-categorically, as long as we work with equivalence classes of
isomorphically-parameterized maps, in order that composition is suﬃently strictly associative.
Proposition 2.47. Each bicategory Parapdqinduces a 1-category Parapdq1 by forgetting the bicategorical
structure. The hom setsParapdq1pX,Y qare given byUParapdqpX,Y q{„ where Uis the forgetful functor
U : Cat ÑSet and f „g if and only if there is some 2-cell α : f ñg that is an isomorphism. We call
Parapdq1 the 1-categorical truncation of Parapdq. When Parapdqis monoidal, so is Parapdq1.
3. The bidirectional structure of Bayesian updating
In this section, we de/f_ine a collection of indexed categories, each denotedStat, whose morphisms can be seen
as “generalized Bayesian inversions”. Following Proposition 2.36, these induce corresponding categories of
lenses which we callBayesian lenses. We show that, for the subcategories ofexact Bayesian lenses whose back-
ward channels correspond to ‘exact’ Bayesian inversions, the Bayesian inversion of a composite of forward
channels is given (up to almost-equality) by the lens composite of the corresponding backward channels. This
justi/f_ies calling these lenses ‘Bayesian’, and provides the foundation for the study of approximate (non-exact)
Bayesian inversion in the subsequent section.
Remark. An alternative account of Bayesian lenses, from an ‘optical’ perspective, is told in the preprint [6].
2So named for its similarity to the (co)strength of certain (co)monads; and similarly, too, our costrength should satisfy certain
standard coherence laws which we omit here.
12
3.1. State-dependent channels
A channel c: XÑ‚Y admitting a Bayesian inversion induces a family of inverse channelsc:
π : YÑ‚X, indexed
by ‘prior’ statesπ : 1 Ñ‚X. Making the state-dependence explicit, in typical cases where cis a probability
kernel we obtain a measurable function c: : GX ˆY Ñ GX. In more general situations, we obtain a
morphism c: : CpI,XqÑ CpY,Xqin the base of enrichment of the monoidal categorypC,b,Iqof c. We call
morphisms of this general type state-dependent channels , and structure the indexing as an indexed category.
De/f_inition 3.1.Let pC,b,Iqbe a monoidal category enriched in a Cartesian categoryV. De/f_ine theC-state-
indexed category Stat : C op ÑV-Cat as follows.
Stat : C op Ñ V-Cat
X ÞÑStatpXq:“
¨
˚˚˚˝
StatpXq0 :“ C0
StatpXqpA,Bq :“ VpCpI,Xq,CpA,Bqq
idA : StatpXqpA,Aq :“
#
idA : CpI,XqÑ CpA,Aq
ρ ÞÑ idA
˛
‹‹‹‚ (4)
f : CpY,XqÞÑ
¨
˚˚˚˚˚˝
Statpfq : StatpXq Ñ StatpYq
StatpXq0 “ StatpYq0
VpCpI,Xq,CpA,Bqq Ñ VpCpI,Y q,CpA,Bqq
α ÞÑ f˚α:
`
σ: CpI,Y q
˘
ÞÑ
`
αpf ‚σq: CpA,Bq
˘
˛
‹‹‹‹‹‚
Composition in each /f_ibreStatpXqis as inC. Explicitly, indicating morphismsCpI,XqÑ CpA,Bqin StatpXq
by AXÝ Ñ‚ B, and given α: AXÝ Ñ‚ Band β : BXÝ Ñ‚ C, their composite is β˝α: AXÝ Ñ‚ C :“ρÞÑβpρq‚αpρq, where
here we indicate composition in C by ‚and composition in the /f_ibresStatpXqby ˝. Given f : YÑ‚X in C,
the induced functor Statpfq: StatpXqÑ StatpYqacts by pullback.
Notation 3.2. Just as we wrote X MÝÑY for an M-parameterized morphism in CpM dX,Y q(see Proposi-
tion 2.43), we write AXÝ Ñ‚ Bfor an X-state-dependent morphism in V
`
CpI,Xq,CpA,Bq
˘
. Given a state ρin
CpI,Xqand an X-state-dependent morphism f : AXÝ Ñ‚ B, we write fρ for the resulting morphism inCpA,Bq.
Remark 3.3. We can thus think of Stat as a kind of ‘external’ parameterization of channels in C, here by
states in C. Generalizing this notion to “external parameterization by V objects” gives rise to a a cousin of
the Para construction: a notion of category by proxy , denoted Prox, of which Stat is a (/f_ibred) special case.
Correspondingly, we think of Para as capturing parameterization ‘internal’ toC. Consider for instance the
case where M acts on C by d: MˆC ÑC and suppose that, for allA: C, the functorsp´qdA: M ÑC are
left adjoint to rA,´s: C ÑM. Then we have ParapdqpA,Bq– ř
M:M M
`
M,rA,Bs
˘
, which is strongly
reminiscent of the de/f_inition ofStat, and comes close to a de/f_inition ofProx. The connections between these
constructions are a matter of on-going study; a summary is available in Capucci et al. [22].
Proposition 3.4. Stat is lax monoidal. The components µXY : StatpXqˆ StatpYqÑ StatpXbYqof the
laxator are de/f_ined on objects byµXY pA,A1q:“AbA1 and on morphisms f : AXÝ Ñ‚ Band f1 : A1YÝ Ñ‚B1 by
µXY pf,f 1q:“fbf1 : AbA1XbYÝÝÝÑ‚ BbB1, where fbf1is the V-morphism CpI,X bYqÑ CpAbA1,B b
B1q: ω ÞÑfωX bf1
ωY . Here, ωX and ωY are the X and Y marginals of ω, de/f_ined byωX :“projX ‚ωand
ωY :“ projY ‚ω (see De/f_inition 2.12 and Depiction 2.28). The unitµ0 : 1 Ñ StatpIqof the lax monoidal
structure is the functor mapping the unique object 1 : 1 to I : StatpIq.
13
Example 3.5. The category Meas of general measurable spaces is not Cartesian closed, as there is no general
way to make the evaluation maps MeaspX,Y qˆ X ÑY measurable, meaning that if we take C “KℓpGq
above, then we are forced to take V “ Set. In turn, this makes the inversion maps c: : KℓpGqp1,Xq Ñ
KℓpGqpY,Xqinto mere functions. We can salvage measurability by working instead with KℓpQq, where
Q : QBS Ñ QBS is the analogue of the Giry monad for quasi-Borel spaces [23]. The category QBS
is indeed Cartesian closed, and KℓpQqis enriched in QBS, so that we can instantiate Stat there, and the
corresponding inversion maps are accordingly measurable. Moreover, there is a quasi-Borel analogue of the
notion of s-/f_inite kernel [24, §11], with which we can de/f_ine a variant of the categorysfKrn.
Remark 3.6. When C is a Kleisli category KℓpTq, it is of course possible to de/f_ine a variant ofStat on the
other side of the product-exponential adjunction, with state-dependent morphisms AXÝ Ñ‚ Bhaving the types
TX ˆA Ñ TB. This avoids the technical diﬃculties sketched in the preceding example at the cost of
requiring a monad T. However, the exponential form makes for better exegesis, and so we will stick to that.
3.2. Bayesian lenses
We de/f_ine the category of Bayesian lenses inC to be the category of Grothendieck Stat-lenses.
De/f_inition 3.7.The categoryBayesLensC of Bayesian lenses inC is the categoryGrLensStat of Grothendieck
lenses for the functor Stat. A Bayesian lens is a morphism in BayesLensC. Where the category C is evident
from the context, we will just write BayesLens.
Unpacking this de/f_inition, we /f_ind that the objects ofBayesLensC are pairs pX,Aqof objects of C. Mor-
phisms (that is, Bayesian lenses) pX,Aq ÞÑpY,Bqare pairs pc,c:qof a channel c: XÑ‚Y and a “generalized
Bayesian inversion”c: : BXÝ Ñ‚ A; that is, elements of the hom objects
BayesLensC
`
pX,Aq,pY,Bq
˘
: “GrLensStat
`
pX,Aq,pY,Bq
˘
–CpX,Y qˆ V
`
CpI,Xq,CpB,Aq
˘
.
The identity Bayesian lens on pX,Aqis pidX,idAq, where by abuse of notation idA : CpI,Y qÑ CpA,Aqis
the constant map idA de/f_ined in Equation (4) that takes any state onY to the identity on A.
The sequential composite pd,d:qpc,c:qof pc,c:q: pX,Aq ÞÑpY,Bqand pd,d:q: pY,Bq ÞÑpZ,Cqis the
Bayesian lens
`
pd‚cq,pc:˝c˚d:q
˘
: pX,Aq ÞÑpZ,Cqwhere pc:˝c˚d:q: CXÝ Ñ‚ Atakes a state π : IÑ‚X to
the channel c:
π ‚d:
c‚π : CÑ‚A.
De/f_inition 3.8.Given a Bayesian lens pc,c1q : pX,Aq ÞÑ pY,Bq, we will call c its forwards or prediction
channel and c1its backwards or update channel (even though c1is really a family of channels).
Remark 3.9. Note that the de/f_inition ofStat and hence the de/f_inition ofBayesLensC do not require C to be
a copy-delete category, even though our motivating categories of stochastic channels are; all that is required
for the de/f_inition is thatC is monoidal.
Remark 3.10. On the other hand, the structure of C might be stronger than merely monoidal. For instance,
when C is Cartesian closed, then we can takeV “C and the monoidal structure to be the categorical product
pˆ,1q. Then a Bayesian lens pX,Aq ÞÑpY,Bqis equivalently given by a pair of a forwards mapX ÑY and
a backwards map XˆB ÑA. We call such lensesCartesian, and they characterize the original ‘lens’ notion;
see Remark 3.16.
Proposition 3.11. BayesLensC is a monoidal category, with structure
`
pb,pI,Iq
˘
inherited from C. On
objects, de/f_inepA,A1qbp B,B1q :“ pAbA1,B bB1q. On morphisms pf,f :q : pX,Aq ÞÑ pY,Bqand
14
pg,g:q: pX1,A1q ÞÑpY1,B1q, de/f_inepf,f :qbpg,g:q:“pfbg,f:bg:q, where f:bg: : BbB1XbX1
Ý ÝÝÝ Ñ‚ AbA1
acts on states ω : IÑ‚X bX1 to return the channel f:
ωX bg:
ω1
X
, following the de/f_inition of the laxatorµin
Proposition 3.4. The monoidal unit in BayesLensC is the pair pI,Iqduplicating the unit in C. When C is
moreover symmetric monoidal, so is BayesLensC.
Proof sketch. The main result is immediate from Proposition 3.4 and Corollary 2.40. When bis symmetric in
C, the symmetry lifts to the /f_ibres ofStat and hence to BayesLensC.
Remark 3.12. Although BayesLensC is a monoidal category, it does not inherit a copy-delete structure
from C, owing to the bidirectionality of its component morphisms. To see this, we can consider morphisms
into the monoidal unit pI,Iq, and /f_ind that there is generally no canonical discarding map. For instance, a
morphism pX,Aq ÞÑpI,Iqconsists in a pair of a channelXÑ‚I(which may indeed be a discarding map) and a
state-dependent channel IXÝ Ñ‚ A, for which there is generally no suitable choice satisfying the comonoid laws.
Note, however, that a lens of the type pX,Iq ÞÑpI,Bqmight indeed act by discarding, since we can choose
the constant state-dependent channel BXÝ Ñ‚ Ion the discarding map : BÑ‚I. By contrast, the Grothendieck
category
ş
Stat is a copy-delete category, as the morphisms pX,Aq Ñ pI,Iqin
ş
Stat are pairs XÑ‚I and
AXÝ Ñ‚ I, and so for both components we can choose morphisms witnessing the comonoid structure.
3.3. Bayesian updates compose optically
In this section we prove the fundamental result on which the development of statistical games rests: that
the inversion of a composite channel is given up to almost-equality by the lens composite of the backwards
components of the associated ‘exact’ Bayesian lenses.
De/f_inition 3.13.Let pc,c:q: pX,Xq ÞÑ pY,Y qbe a Bayesian lens. We say that pc,c:qis exact if cadmits
Bayesian inversion and, for each π : IÑ‚X such that c‚πhas non-empty support, cand c:
π together satisfy
equation (3). Bayesian lenses that are not exact are said to be approximate.
Theorem 3.14. Let pc,c:qand pd,d:qbe sequentially composable exact Bayesian lenses. Then the contravari-
ant component of the composite lens pd,d:qpc,c:q“p d‚c,c:˝c˚d:qis, up to d‚c‚π-almost-equality,
the Bayesian inversion of d‚cwith respect to any state πon the domain of csuch that c‚πhas non-empty
support. That is to say, Bayesian updates compose optically : pd‚cq:
π
d‚c‚π„ c:
π ‚d:
c‚π.
Proof. Suppose c:
π : YÑ‚X is the Bayesian inverse of c: XÑ‚Y with respect to π : IÑ‚X. Suppose also that
d:
c‚π : ZÑ‚Y is the Bayesian inverse of d: YÑ‚X with respect to c‚π : IÑ‚Y, and that pd‚cq:
π : ZÑ‚X is
the Bayesian inverse of d‚c: XÑ‚Zwith respect to π: IÑ‚X:
d
π
c
Y Z
“
d:c‚π
π
c
d
Y Z
and
c
π
d
X Z
“
pd‚cq:
π
π
c
d
X Z
15
The lens composite of these Bayesian inverses has the form c:
π ‚d:
c‚π : ZÑ‚X, so to establish the result it
suﬃces to show that
d:c‚π
π
c
d
c:π
X Z
“
c
π
d
X Z
.
We have
d:c‚π
π
c
d
c:π
X Z
“
d
π
c
c:π
X Z
“
c
π
d
X Z
where the /f_irst obtains becaused:
c‚π is the Bayesian inverse ofdwith respect to c‚π, and the second because
c:
π is the Bayesian inverse ofcwith respect to π. Hence, c:
π‚d:
c‚π and pd‚cq:
π are both Bayesian inversions of
d‚cwith respect toπ. Since Bayesian inversions are almost-equal (Prop. 2.34), we havec:
π‚d:
c‚π
d‚c‚π„ pd‚cq:
π,
as required.
Remark 3.15. Note that, in the context of /f_initely-supported probability (e.g., in KℓpDq, where D is the
/f_initely-supported probability distribution monad), almost-equality coincides with simple equality, and so
Bayesian inversions are then just equal.
Remark 3.16. Lenses were originally studied in the context of database systems [3], where one thinks of
the forward channel as ‘viewing’ a record in a database, and the backward channel as ‘updating’ a record by
taking a record and a new piece of data and returning the updated record. In this context, lenses have often
been subject to additional axioms characterizing well-behavedness; for example, that updating a record with
some data is idempotent (the ‘put-put’ law). Bayesian lenses do not in general satisfy these laws, and nor
even do exact Bayesian lenses. This is because Bayesian updating mixes information in the prior state (the
‘record’) with the observation (the ‘data’), rather than replacing the prior information outright. We refer the
reader to our preprint [6, §6] for a more detailed discussion of this situation.
3.4. Parameterized Bayesian lenses
Bayesian lenses for which the component channels are equipped with parameters will play an important role
in certain applications: an example which we will meet in the next section is the variational autoencoder ,
a neural network architecture originally developed for machine learning and which is well described by a
particular class of parameterized statistical games. Since BayesLensC is a monoidal category, it induces a
self-parameterization ParapBayesLensCqby Proposition 2.45, which is suﬃcient for the purposes of this
paper. Therefore, in this section, we summarize the resulting structure for later reference.
16
0-cells and 1-cells The 0-cells of the bicategoryParapBayesLensCqare pairs pX,Aqof objects in C. The
1-cells pc,c:q: pX,Aq
pΩ,Θq
ÝÝÝÑ| pY,Bqare Bayesian lenses pc,c:q: pΩ bX,Θ bAq ÞÑ pY,Bq. The forwards
component cis a channel Ω bXÑ‚Y in C, and hence also a parameterized channel XΩÝ Ñ‚Y in ParapCq; we
often think of this channel as representing a system’s model of the process by which observations (of typeY)
are generated from causes (of typeX), with the parameters (of typeΩ) representing the system’s beliefs about
the structure of this generative process.
Conversely, the backwards component c: is a state-dependent channel BΩbXÝÝÝÑ‚ Θ bA, which means a V-
morphism CpI,Ω bXq ÑCpB,Θ bAq. This is a generalized Bayesian inversion which takes a state (or
‘prior’ belief) jointly over parameters and causesΘ bXand returns a channel BÑ‚Θ bAthat we think of as
taking an observation (of possibly diﬀerent type B) and returning an updated joint belief about parameters
and causes (of possibly diﬀerent types Θ and A). Note that this ‘update’ channel is not itself parameterized:
the parameterization of the inversion is mediated through Ω, with Θ being the type of updated parameters.
Sequential composition Given parameterized Bayesian lenses pc,c:q: pX,Aq
pΩ,Θq
ÝÝÝÑ| pY,Bqand pd,d:q:
pY,Bq
pΩ1,Θ1q
ÝÝÝÝÑ| pZ,Cq, their composite pd,d:qpc,c:qis de/f_ined as the following morphism inBayesLensC:
pΩ1bΩ bX,Θ1bΘ bAq
pid,idqbpc,c:q
Ý ÝÝÝÝÝÝÝ Ñ| pΩ1bY,Θ1bBq
pd,d:q
ÝÝÝÑ| pZ,Cq.
Parallel composition Given parameterized Bayesian lenses pc,c:q : pX,Aq
pΩ,Θq
ÝÝÝÑ| pY,Bqand pd,d:q :
pX1,A1q
pΩ1,Θ1q
ÝÝÝÝÑ| pY1,B1q, their tensor pc,c:qbp d,d:qis de/f_ined as the following morphism inBayesLensC:
˜
Ω bΩ1bXbX1
Θ bΘ1bAbA1
¸
„Ý Ñ|
˜
Ω bXbΩ1bX1
Θ bAbΘ1bA1
¸ ´
c
c:
¯
b
´
d
d:
¯
Ý ÝÝÝÝÝÝÝÝ Ñ|
˜
Y bY1
BbB1
¸
where we have written the pairs vertically, so that
´
X
A
¯
:“pX,Aq.
Reparameterization Given 1-cells pf,f :q : pX,Aq
pΩ,Θq
ÝÝÝÑ| pY,Bqand pg,g:q : pX,Aq
pΩ1,Θ1q
ÝÝÝÝÑ| pY,Bq, the
2-cells α : pf,f :q ñ pg,g:qof ParapBayesLensCqare Bayesian lenses α : pΩ,Θq ÞÑ pΩ1,Θ1qsuch that
pf,f :q“p g,g:qpαbidpX,Aqq. We can think of the 2-cells asreparameterizations, or higher-order processes
that predict the parameters on the basis of yet more abstract data.
Higher structure The Paraconstruction turns a (monoidal) category into a (monoidal) bicategory, thereby
“adding a dimension”. We can consider morphisms in a parameterized category (as well as morphisms inStat;
see Remark 3.3) as processes by which processes are chosen, and reparameterizations witness the factorization
of these choice processes. In many cybernetic and statistical situations, it is of interest to consider adding
more than just one extra dimension: that is, we may be interested in the processes by which these choice
processes are chosen, and, as in the Bayesian setting, we may be interested in improving the performance of
these ‘meta-processes’. More concretely, in statistics, we may wish to describe meta-learning algorithms (such
as “learning to learn”), or in neuroscience, we may wish to describe how neuromodulation aﬀects synaptic
plasticity (which in turn aﬀects the generation of action potentials).
Since ParapBayesLensCqis itself monoidal, one has a further parameterizationParapParapBayesLensCqq,
and indeed Para has a monad structure whose multiplication collapses a doubly-parameterized morphism
to a singly-parameterized one [8, Prop. 3]. However, the full structure that emerges when iterating this con-
struction ad in/f_initumis not yet well understood3. A similar structure appears when one considers arbitrarily
3At present, we believe the resulting structure may have an opetopic shape, and constitute something like an ‘8-/f_ibration’.
17
‘nested’ dynamical systems (such as cells in an organism, organisms in societies, and societies in ecosystems;
or the ownership structure of a complex modern economy); see St. Clere Smithe [25, Remark 2.3]. Such
cybernetic systems will be treated in a future chapter of the present series.
4. Statistical games
The Bayesian lenses of Theorem 3.14 are exact, but most physically realistic cybernetic systems do not com-
pute exact inversions: the inversion of a channel cwith respect to a prior πgenerally involves evaluating the
composite c‚π, which is typically computationally costly; consequently, realistic systems typically instanti-
ate approximate Bayesian lenses. Fortunately, as a consequence of Theorem 3.14, an approximate inversion
of a composite channel will be approximately equal to this lens composite; conversely, the lens composite of
approximate inversions will be approximately equal to the exact inversion of the corresponding composite.
We can thus approximate the inversion of a composite channel by the lens composite of approximations
to the inversions of the components. But what do we mean by “approximate”? There is often substantial
freedom in the choice of approximation scheme—often manifest as some form of parameterization—and in
typical situations, the ‘/f_itness’ of a particular scheme will be context-dependent. We think of Bayesian lenses
as representing ‘open’ statistical systems, in interaction with some environment or ‘context’, and so the /f_itness
of a particular lens then depends not only on the con/f_iguration of the system (i.e., the choice of lens), but also
on the suitability of its con/f_iguration for the environment.
In this section, we introduce statistical games in order to quantify this context-dependent /f_itness: a statisti-
cal game will be a Bayesian lens paired with a contextual /f_itness function. The /f_itness function measures how
well the lens performs in each context, and the “aim of the game” is then to choose the lens or context (de-
pending on your perspective) that somehow optimizes the /f_itness. In order to de/f_ine statistical games, we /f_irst
therefore de/f_ine our notion of context, following compositional game theory [2, 7]: a context is “everything
required to make an open system closed”. In the next section, we exemplify statistical games by formalizing
a number of classic and not-so-classic problems in statistics.
4.1. Contexts for Bayesian lenses
Our /f_irst step is to de/f_ine the notion ofsimple context , with which we will be able to “close oﬀ” a lens with
respect to sequential composition , and thereby de/f_ine our categories of statistical games.
De/f_inition 4.1.A simple context for a Bayesian lens over C is an element of the V-profunctor BayesLensC
de/f_ined by
BayesLensC ˆ BayesLensop
C Ñ V
´ ˆ “ ÞÑ BayesLensCppI,Iq,´qˆ BayesLensCp“,pI,Iqq.
If the lens is pA,Sq ÞÑpB,T q, its object of simple contexts is
BayesLensC
`
pI,Iq,pA,Sq
˘
ˆBayesLensC
`
pB,T q,pI,Iq
˘
.
If we denote the lens by f, then we can denote this object of simple contexts by
Ctxpfq:“BayesLensC
`
pA,Sq,pB,T q
˘
.
When the monoidal unit I is terminal in C, or equivalently when C is semicartesian, the object of simple
contexts acquires a simpli/f_ied (and intuitive) form.
18
Proposition 4.2. When I is terminal in C, we have
BayesLensC
`
pX,Aq,pY,Bq
˘
–CpI,Xqˆ V
`
CpI,Bq,CpI,Y q
˘
.
Proof. A straightforward calculation which we omit: use causality (De/f_inition 2.24).
Remark 4.3. Proposition 4.2 means that, when C is a Markov category (such as KℓpGq), a context for a
Bayesian lens consists of a ‘prior’ on the domain of the forwards channel and a ‘continuation’: aV-morphism
(such as a function) which takes the output of the forwards channel (possibly a ‘prediction’) and returns an
observation for the update map. We think of the continuation as encoding the response of the environment
given the prediction.
In order to de/f_ine the sequential composition of statitistical games, we will need to construct, from the
context for a composite lens, contexts for each factor of the composite. The functoriality of BayesLensC
guarantees the existence of such local contexts .
De/f_inition 4.4.Given another lens g : pB,T q ÞÑ pC,Uqand a simple context for their composite gf :
pA,Sq ÞÑpC,Y q, then we can obtain a 1-local context for f by the action of the profunctor
BayesLensC
`
pA,Sq,g
˘
: BayesLensC
`
pA,Sq,pC,Uq
˘
ÑBayesLensC
`
pA,Sq,pB,T q
˘
and we can obtain a simple context for gsimilarly:
BayesLensC
`
f,pC,Uq
˘
: BayesLensC
`
pA,Sq,pC,Uq
˘
ÑBayesLensC
`
pB,T q,pC,Uq
˘
.
Note that we can write g˚ : CtxpgfqÑ Ctxpfqfor the former action and f˚ : CtxpgfqÑ Ctxpgqfor the
latter, since g˚ acts by precomposition (pullback) and f˚ acts by postcomposition (pushforwards). Note also
that, given hgf, we have h˚˝f˚ “f˚˝h˚ : CtxphgfqÑ Ctxpgqby the associativity of composition.
Remark 4.5. The local contexts of the preceding de/f_inition are local with respect to sequential composi-
tion of lenses. If we view the monoidal category BayesLensC as a one-object bicategory, then sequential
composition is composition of 1-cells, which explains their formal naming as 1-local contexts.
To lift the monoidal structure ofBayesLensC to our categories of statistical games, we will similarly need
‘2-local’ contexts: from the one-object bicategory perspective, these are local contexts with respect to 2-cell
composition. By analogy with the 1-local case, this means exhibiting maps of the formCtxpfbf1qÑ Ctxpfq
and Ctxpf bf1qÑ Ctxpf1qwhich give the ‘left’ and ‘right’ local contexts for a parallel pair of lenses.
Without requiring extra structure from C or BayesLensC
4, we /f_irst need to pass from simplex contexts to
complex ones: a 2-local context should allow for information to pass alongside a lens, through a process that
is parallel to it. Formally, we adjoin an object to the domain and codomain of the context, and quotient by
the rule that, if there is any process that ‘/f_ills the hole’ represented by the adjoined objects, then we consider
the objects equivalent: this allows us to forget about the contextually parallel processes, and keep track only
of the type of information that /f_lows. Such adjoining-and-quotienting gives us the following coend5 formula
de/f_ining complex contexts.
4An alternative is to ask for BayesLensC to be equipped with a natural family of ‘discarding’ morphismspA, Sq Ñ pI, Iq. This
in turn means asking for canonical states I Ñ S, which in general we do not have: if we are working with stochastic channels,
we could obtain such canonical states by allowing the channels to emit subdistributions, and letting the canonical states be given
by those which assign 0 density everywhere. But even though we can make the types check this way, the semantics are not quite
what we want: rather, we seek to allow information to “pass in parallel”.
5For more information about (co)ends and (co)end calculus, we refer the reader to Loregian [26].
19
De/f_inition 4.6.We de/f_ine acomplex context to be an element of the profunctor
ČBayesLensC
`
pA,Sq,pB,T q
˘
:“
żpM,Nq:BayesLensC
BayesLensC
`
pM,N qbp A,Sq,pM,N qbp B,T q
˘
.
If f : pA,Sq ÞÑpB,T qis a lens, then we will write Ctxpfq:“ ČBayesLensC
`
pA,Sq,pB,T q
˘
. As in the case
of simple contexts, we have ‘projection’ mapsg˚ : CtxpgfqÑ Ctxpfqand f˚ : CtxpgfqÑ Ctxpgq; these
are de/f_ined similarly. We will call the adjoined object, here denotedpM,N q, the residual. There is a canonical
inclusion CtxpfqãÑCtxpfqgiven by adjoining the trivial residual pI,Iqto each simple context.
We immediately have the following corollary of Proposition 4.2:
Corollary 4.7. When I is terminal in C,
ČBayesLensC
`
pA,Sq,pB,T q
˘
–
żpM,Nq
CpI,M bAqˆ V
`
CpI,M bBq,CpI,N bTq
˘
.
Remark 4.8. Since the coend denotes a quotient, its elements are equivalence classes. The preceding corol-
lary says that, when I is terminal, an equivalence class of contexts is represented by a choice of residual
pM,N q, a prior on M bAin C, and a continuation CpI,M bBqÑ CpI,N bTqin V.
Of course, when I is not terminal, then the de/f_inition says that a general complex context for a lens
pA,Sq ÞÑ pB,T qis an equivalence class represented by: as before, a choice of residual pM,N q, a prior
on M bA, and a continuation CpI,M bBq ÑCpI,N bTq; as well as an ‘eﬀect’ M bBÑ‚I in C, and
what we might call a ‘vector’CpI,IqÑ CpNbS,Iqin V. The eﬀect and vector measure the environment’s
‘internal response’ to the lens’ outputs (as opposed to representing the environment’s feedback to the lens).
Using complex contexts, it is easy to de/f_ine the ‘projections’ that give 2-local contexts.
De/f_inition 4.9.Given lenses f : Φ ÞÑΨ and f1 : Φ1 ÞÑΨ1and a complex context for their tensorfbf1, the
left 2-local context is the complex context for f given by
πf : Ctxpf bf1q“ ČBayesLensC
`
Φ bΦ1,Ψ bΨ1˘
“
żΘ:BayesLensC
BayesLensC
`
Θ bΦ bΦ1,Θ bΨ bΨ1˘
„Ý Ñ
żΘ:BayesLensC
BayesLensC
`
Θ bΦ1bΦ,Θ bΨ1bΨ
˘
“
żΘ:BayesLensC
BayesLensC
`
pI,Iq,Θ bΦ1bΦ
˘
ˆBayesLensC
`
Θ bΨ1bΨ,pI,Iq
˘
f1
Ý Ñ
żΘ:BayesLensC
BayesLensC
`
pI,Iq,Θ bΨ1bΦ
˘
ˆBayesLensC
`
Θ bΨ1bΨ,pI,Iq
˘
“
żΘ:BayesLensC
BayesLensC
`
Θ bΨ1bΦ,Θ bΨ1bΨ
˘
ãÑ
żΘ1:BayesLensC
BayesLensC
`
Θ1bΦ,Θ1bΨ
˘
“Ctxpfq.
The equalities here are just given by expanding and contracting de/f_initions; the isomorphism uses the sym-
metry of bin C; the arrow markedf1is given by composingidΘ bf1bidΦ after the ‘prior’ part of the context;
and the inclusion is given by collecting the tensor ofΘ and Ψ1together into the residual. These steps formalize
the idea of /f_illing the right-hand hole of the complex context withf1to obtain a local context for f.
20
The right 2-local context is the complex context for f1obtained similarly:
πf1 : Ctxpf bf1q“ ČBayesLensC
`
Φ bΦ1,Ψ bΨ1˘
“
żΘ:BayesLensC
BayesLensC
`
Θ bΦ bΦ1,Θ bΨ bΨ1˘
“
żΘ:BayesLensC
BayesLensC
`
pI,Iq,Θ bΦ bΦ1˘
ˆBayesLensC
`
Θ bΨ bΨ1,pI,Iq
˘
f
Ý Ñ
żΘ:BayesLensC
BayesLensC
`
pI,Iq,Θ bΨ bΦ1˘
ˆBayesLensC
`
Θ bΨ bΨ1,pI,Iq
˘
“
żΘ:BayesLensC
BayesLensC
`
Θ bΨ bΦ1,Θ bΨ bΨ1˘
ãÑ
żΘ1:BayesLensC
BayesLensC
`
Θ1bΦ1,Θ1bΨ1˘
“Ctxpf1q
Note here that we do not need to use the symmetry, as both the residual and the ‘hole’ (/f_illed withf) are on
the left of the tensor. (Strictly, we do not require symmetry of bfor πf either, only a braiding.)
Remark 4.10. It is possible to make the intuition of “/f_illing the left and right holes” more immediately precise,
at the cost of introducing another language, by rendering the 2-local context functions in the graphical cal-
culus of the monoidal bicategory of V-profunctors. We demonstrate how this works, making the hole-/f_illing
explicit, in Appendix A.
Henceforth, when we say ‘context’, we will mean ‘complex context’.
4.2. Monoidal categories of statistical games
We are now in a position to de/f_ine monoidal categories of statistical games overC. In typical examples, the
/f_itness functions will be valued in the real numbers, but this is not necessary for the categorical de/f_inition;
instead, we allow the /f_itness functions to take values in an arbitrary monoid.
Proposition 4.11. Let C be a V-category admitting Bayesian inversion and let pR,`,0qbe a monoid in V.
Then there is a category RSGameC whose objects are the objects of BayesLensC and whose morphisms
pX,AqÑp Y,Bqare statistical games : pairs pf,φqof a lens f : BayesLensC
`
pX,Aq,pY,Bq
˘
and a /f_itness
function φ: CtxpfqÑ R. When Ris the monoid of reals R, then we just denote the category by SGameC.
Proof. Suppose given statistical games pf,φq: pX,Aq Ñ pY,Bqand pg,ψq: pY,Bq Ñ pZ,Cq. We seek a
composite game pg,ψq˝p f,φq:“pgf,ψφq: pX,AqÑp Z,Cq. We have gf “gf by lens composition.
The composite /f_itness functionψφis obtained using the 1-local contexts by
ψφ:“CtxpgfqÝÑCtxpgfqˆ Ctxpgfq
pg˚,f˚q
ÝÝÝÝÑCtxpfqˆ Ctxpgq
pφ,ψq
ÝÝÝÑRˆR `Ý ÑR
The identity game pX,Aq Ñ pX,Aqis given by pid,0q, the pairing of the identity lens on pX,Aqwith the
unit 0 of the monoid R. Associativity and unitality follow from those properties of lens composition, the
coassociativity of copying in V, and the monoid laws of R.
De/f_inition 4.12.We will write SimpSGameC ãÑSGameC for the full subcategory of SGameC de/f_ined
on simple Bayesian lenses pX,Xq ÞÑpY,Y q. As the case of simple lenses (De/f_inition 2.37), we will eschew
redundancy by writing the objects pX,Xqsimply as X.
21
Proposition 4.13. RSGameC inherits a monoidal structure
`
b,pI,Iq
˘
from
`
BayesLensC,b,pI,Iq
˘
. When
pC,b,Iqis furthermore symmetric monoidal andpR,`,0qis a commutative monoid, then the monoidal struc-
ture on RSGameC is symmetric.
Proof. The structure on objects and on the lens components of games is de/f_ined as in Proposition 3.11. On
/f_itness functions, we use the monoidal structure ofR: given games pf,φq: pX,AqÑp Y,Bqand pf1,φ1q:
pX1,A1qÑp Y1,B1q, we de/f_ine the /f_itness functionφbφ1of their tensor pf,φqbp f1,φ1qas the composite
φbφ1 :“Ctxpf bf1qÝÑCtxpf bf1qˆ Ctxpf bf1q
pπf,πf1q
Ý ÝÝÝÝ ÑCtxpfqˆ Ctxpf1q
pφ,φ1q
ÝÝÝÑRˆR `Ý ÑR
That is, we form the left and right 2-local contexts, compute the local /f_itnesses, and compose them using the
monoidal operation in R. Unitality and associativity follow from that of bin BayesLensC and `in R.
Corollary 4.14. Since each category RSGameC is thus monoidal, we obtain categories ParapRSGameCq
of parameterized statistical games by Proposition 2.45, which are themselves monoidal. The structure is as
described for Bayesian lenses in Section 3.4, with some minor additions to incorporate the /f_itness functions,
the details of which we leave to the reader.
Remark 4.15 (Parameters as strategies). A parameterized statistical game of type pX,Aq
pΩ,Θq
ÝÝÝÑ pY,Bqin
ParapRSGameCqis a statistical game
´
ΩbX
ΘbA
¯
Ñ
´
Y
B
¯
in RSGameC; that is a pair of a Bayesian lens´
ΩbX
ΘbA
¯
ÞÑ
´
Y
B
¯
and a /f_itness function ČBayesLensC
´´
ΩbX
ΘbA
¯
,
´
Y
B
¯¯
Ñ R in V. If we /f_ix a choice of
parameter ω : Θ and discard the updated parameters in Θ—that is, if we reparameterize along the 2-cell
induced by the lens pω, q: pI,Iq ÞÑpΩ,Θq—then we obtain an unparameterized statistical game pX,AqÑ
pY,Bq. In this way, we can think of the parameters of a parameterized statistical game as the strategies by
which the game is to be played: each parameter ω: Ω picks out a Bayesian lens, whose forwards channel we
think of as a model by which the system predicts its observations and whose backwards channel describes
how the system updates its beliefs. And, if we don’t just discard them, then these updated beliefs may include
updated parameters (of a possibly diﬀerent type Θ). A successful strategy (a good choice of parameter) for a
statistical game is then one which optimizes the /f_itness function in the contexts of relevance to the system:
we can think of these “relevant contexts” as something like the system’s ecological niche.
Remark 4.16 (Multi-player games). In a later instalment of this series of papers, we will see how to compose
the statistical games of multiple interacting agents, so that the game-playing metaphor becomes more visceral:
the observations predicted by each system will then be generated by other systems, with each playing a game
of optimal prediction. In this paper, however, we usually think of each statistical game as representing a single
system’s model of its environment (its context), even where the games at hand are themselves sequentially
or parallelly composite. That is to say, our games here are fundamentally ‘two-player’, with the two players
being the system and the context.
Remark 4.17. Both in the case of sequential of parallel composition of statistical games, the local /f_itnesses
are computed independently and then summed. If a /f_itnesss function depends somehow on the residual, this
might lead to ‘double-counting’ the /f_itness of any overlapping factors of the residual. For our purposes, this
assumption of ‘independent /f_itness’ will suﬃce, and so we leave the question of gluing together correlated
/f_itness functions for future work.
5. Examples
In this section, we describe how a number of common concepts in statistics and particularly statistical infer-
ence /f_it into the framework of statistical games. We begin with the simple example of maximum likelihood
22
estimation and progressively generalize to include ‘variational’ [27] methods such as the variational autoen-
coder [28] and generalized variational inference [29]. Along the way, we introduce the concepts offree energy
and evidence upper bound . We do not here consider the algorithms by which statistical games may be played or
optimized; that is a matter for a subsequent paper in this series. Instead, we see statistical games as providing
an ‘algebra’ for the compositional construction of inference problems.
Remark 5.1 (The role of /f_itness functions). Before we introduce our /f_irst example, we note that the games
here are classi/f_ied by their /f_itness functions, with the choice of lens being somewhat incidental to the classi/f_i-
cation6. We note furthermore that our /f_itness functions will tend to be of the formEk‚c‚πrfs, where pπ,kqis
a context for a lens, cis a channel, and f is an appropriately typed eﬀect7. This form hints at the existence of
a compositional treatment of /f_itness functions, which seem roughly to be something like “lens functionals”.
We leave such a treatment, and its connection to Remark 4.17, to future work.
We /f_irst study the classic problem of maximum likelihood estimation, beginning by establishing an auxiliary
results about contexts.
Proposition 5.2. Let I denote the monoidal unit pI,Iqin BayesLensC, and let l: I ÞÑΨ be a lens. Then
Ctxplq“
żΘ:BayesLensC
BayesLensCpI,Θ bIqˆ BayesLensCpΘ bΨ,Iq
–
żΘ:BayesLensC
BayesLensCpI,Θqˆ BayesLensCpΘ bΨ,Iq
–BayesLensCpI bΨ,Iq
–BayesLensCpΨ,Iq.
Suppose Ψ “pA,Sq. Then Ctxplq“ CpA,Iqˆ V
`
CpI,Aq,CpI,Sq
˘
by the de/f_inition ofBayesLensC.
Proof. The /f_irst and third isomorphisms hold by unitality ofb; the second holds by the Yoneda lemma (see
Loregian [26, Prop. 2.2.1] for the argument).
Example 5.3 (Maximum likelihood). When I is terminal in C, a Bayesian lens of the form pI,Iq ÞÑpX,Xq
is determined by its forwards channel, which is simply a state π : IÑ‚X. Following Proposition 5.2, and
using that I is terminal, a context for such a lens is given simply by a continuation k : CpI,XqÑ CpI,Xq
taking states on X to states on X. A maximum likelihood game is then any statistical game π of the type
pI,Iq Ñ pX,Xqwith /f_itness functionφ : Ctxpπq ÑR given by φpkq “Ekpπqrpπs, where pπ is a density
function for π. More generally, we might consider maximum f-likelihood games for monotone functions
f : R ÑR, in which the /f_itness function is given byφpkq“ Ekpπqrf ˝pπs. A typical choice here is f :“log.
In order that there may be some freedom to optimize the /f_itness function, one typically works in the param-
eterized category: the aim of the game is then to choose the optimal parameter for the context, as quanti/f_ied
by the /f_itness function. This gives us the notion of parameterized maximum likelihood game; but /f_irst, we
de/f_ine some simplifying notation.
Notation 5.4 (Feedback). Let I be terminal in C and consider a Bayesian lens l“pl1,l1q: pA,Sq ÞÑpB,T q,
with a context represented by: a residualpM,N q; a priorπ: IÑ‚MbA; and a continuationk: CpI,M bBqÑ
CpI,N bTq. Write Lπ|l|kMto denote k
`
pidM bl1q‚ π
˘
T where pidM bl1q‚ πis the map
IπÝ Ñ‚M bAidM bl1
Ý ÝÝÝÝ Ñ‚ M bB
6This incidentality is lessened when we consider examples ofparameterized games, but even here the parameterization only induces
something of a ‘sub’-classi/f_ication; the main classi/f_ication remains due to the /f_itness functions.
7The resulting ‘optimization-centric’ perspective is in line with the aesthetic preference of [29], though we do not yet know what
this alignment might signify; we are interested to /f_ind examples of a diﬀerent /f_lavour.
23
and where p´qT denotes the projection (marginalization) onto T; here, by the channel N bTÑ‚T.
Note that Lπ|l|kMtherefore has the type IÑ‚T in C: it encodes the environment’s feedback in T to the
lens, given its output in Band the context.
Example 5.5 (Parameterized maximum likelihood) . A parameterized Bayesian lens pI,Iq
pΩ,Θq
ÝÝÝÑ| pX,Xqis
equivalently a Bayesian lens pΩ,Θq ÞÑ pX,Xq, and hence given by a pair of a channel (or “parameter-
dependent state”) ΩÑ‚X and a parameter-update XΩÝ Ñ‚Θ. When I is terminal in C, a context for the lens is
represented byπ: IÑ‚MbΩ and k: CpI,M bXqÑ CpI,N bXq. We then de/f_ine aparameterized maximum
f-likelihood game to be a parameterized statistical game of the form l “ pl1,l1q : pI,Iq
pΩ,Θq
ÝÝÝÑ| pX,Xqwith
/f_itness functionφ: CtxpπqÑ R given by φpπ,kq“ EL π|l|kM rf ˝pl‚πΩs.
Here, pl‚πΩ : X Ñr0,8sis a density function for the composite channel l‚πΩ. In applications one often
/f_ixes a single choice of parametero, with the marginal state πΩ then being a Dirac delta distribution on that
choice. One then writes the density function pl‚πΩp´qas plp´|oqor plp´|Ω “oq.
Remark 5.6. Recalling that we can think of probability density as a measure of the likelihood of an obser-
vation, we have the intuition that an “optimal strategy” ( i.e., an optimal choice of lens or parameter) for a
maximum likeihood game is one that maximizes the likelihood of the state obtained from the context, or in
other words provides the “best explanation” of the data generated by the continuation.
Considering parameterized maximum likelihood games, which are equipped with parameter-update maps,
leads one to wonder how to optimize this ‘inferential’ backwards part of the game, and not just the ‘predictive’
forwards part. Such backwards optimization is approximate Bayesian inference.
Example 5.7 (Bayesian inference). Let D : CpI,Xqˆ CpI,Xq ÑR be a measure of divergence between
states on X. Then a (simple) D-Bayesian inference game is a statistical game pc,φq: pX,XqÑp Y,Y qwith
/f_itness functionφ : Ctxpcq ÑR given by φpπ,kq “Ey„L π|c|kM
”
D
´
c1
πpyq,c:
πpyq
¯ı
, where c “ pc1,c1q
constitutes the lens part of the game and c:
π is the exact inversion of c1 with respect to π.
Note that we say thatDis a “measure of divergence between states onX”. By this we mean any function of
the given type with the semantical interpretation that it acts like a distance measure between states. But this
is not to say that Dis a metric or even pseudometric. One usually requires that Dpπ,π1q“ 0 ðñ π “π1,
but typical choices do not also satisfy symmetry nor subadditivity. An important such typical choice is the
relative entropy or Kullback-Leibler divergence, denoted DKL.
De/f_inition 5.8.The Kullback-Leibler divergence DKL : CpI,Xqˆ CpI,XqÑ R is de/f_ined by
DKLpα,βq:“ E
x„α
rlog ppxqs´ E
x„α
rlog qpxqs
where pand qare density functions corresponding to the states αand β.
In many situations, computing the exact inversion c:
πpxqis costly, and so is computing the divergence
D
´
c1
πpxq,c:
πpxq
¯
. Consequently, approximate inversion schemes typically either approximate the divergence
(as in Monte Carlo methods), or they optimize an upper bound on it (as in variational methods). In this section,
we are interested in diﬀerent choices of /f_itness function, rather than the algorithms by which the functions
are exactly or approximately evaluated; hence we here consider the latter ‘variational’ choice, leaving the
former for future work.
One widespread choice is to construct an upper bound on the divergence called the free energy or the
evidence upper bound .
24
De/f_inition 5.9(D-free energy). Let pπ,cqbe a generative model with c: XÑ‚Y. Let pc : Y ˆX ÑR`and
pπ : X ÑR`be density functions corresponding tocand π. Let pc‚π : Y ÑR`be a density function for the
composite c‚π. Let c1
π be a channel YÑ‚Xthat we take to be an approximation of the Bayesian inversion ofc
with respect to πand that admits a density functionq: XˆY ÑR`. Finally, let D: CpI,XqˆCpI,XqÑ R
be a measure of divergence between states onX. Then the D-free energy of c1
π with respect to the generative
model given an observation y: Y is the quantity
FDpc1
π,c,π,y q:“ E
x„c1πpyq
r´log pcpy|xqs` D
`
c1
πpyq,π
˘
. (5)
We will elide the dependence on the model when it is clear from the context, writing only FDpyq.
The D-free energy is an upper bound on Dwhen Dis the relative entropy DKL, as we now show.
Proposition 5.10 (Evidence upper bound). The DKL-free energy satis/f_ies the following equality:
FDKLpyq“ DKL
“
c1
πpyq,c:
πpyq
‰
´log pc‚πpyq“ E
x„c1πpyq
„
log qpx|yq
pcpy|xq¨ pπpxq

Since log pc‚πpyqis always negative, the free energy is an upper bound on DKL
”
c1
πpyq,c:
πpyq
ı
, where c:
π is
the exact Bayesian inversion of the channel c with respect to the prior π. Similarly, the free energy is an
upper bound on the negative log-likelihood ´log pc‚πpyq. Thinking of this latter quantity as a measure of
the “model evidence” gives us the alternative nameevidence upper bound for the DKL-free energy.
Proof. Let pω : Y ˆX ÑR`be the density function pωpy,xq:“pcpy|xq¨ pπpxqcorresponding to the joint
distribution of the generative model pπ,cq. We have the following equalities:
´log pc‚πpyq“ E
x„c1πpyq
r´log pc‚πpyqs
“ E
x„c1πpyq
«
´log pωpy,xq
pc:
π
px|yq
ﬀ
(by Bayes’ rule)
“ E
x„c1πpyq
«
´log pωpy,xq
qpx|yq
qpx|yq
pc:
π
px|yq
ﬀ
“´ E
x„c1πpyq
„
log pωpy,xq
qpx|yq

´DKL
“
c1
πpyq,c:
πpyq
‰
De/f_inition 5.11.We will callFDKL the variational free energy, or simplyfree energy, and denote it byF where
this will not cause confusion. We will take the result of Proposition 5.10 as a de/f_inition of the variational free
energy, writing
Fpyq“ E
x„c1πpyq
„
log qpx|yq
pcpy|xq¨ pπpxq

where each term is de/f_ined as in De/f_initions 5.9 and 5.8.
Remark 5.12. The name free energy is due to an analogy with the Helmholtz free energy in thermodynamics,
as, when D“DKL, we can write it as the diﬀerence between an (expected) energy and an entropy term:
Fpyq“ E
x„c1πpyq
„
log qpx|yq
pcpy|xq¨ pπpxq

“ E
x„c1πpyq
r´log pcpy|xq´ log pπpxqs´ SX
“
c1
πpyq
‰
“ E
x„c1πpyq
“
Epπ,cqpx,yq
‰
´SX
“
c1
πpyq
‰
“U ´TS
25
where we callEpπ,cq : XˆY ÑR`the energy of the generative modelpπ,cq, and whereSX : CpI,XqÑ R`
is the Shannon entropy on X. The last equality makes the thermodynamic analogy: U is the internal energy
of the system; T “1 is the temperature; and Sis again the entropy.
Having now de/f_ined a more tractable /f_itness function, we can construct statistical games accordingly. Since
the free energy is an upper bound on relative entropy, optimizing the former can have the side eﬀect of
optimizing the latter8. We call the resulting games autoencoder games , for reasons that will soon be clear.
Example 5.13 (Autoencoder). Let D : CpI,Xqˆ CpI,XqÑ R be a measure of divergence between states
on X. Then a simple D-autoencoder game is a simple statistical game pc,φq: pX,XqÑp Y,Y qwith /f_itness
function φ : Ctxpcq ÑR given by φpπ,kq “Ey„L π|c|kM rFDpc1
π,c,π,y qswhere c “ pc,c1q : pX,Xq ÞÑ
pY,Y qconstitutes the lens part of the game.
One also of course has parameterized versions of the autoencoder games.
Example 5.14 (Simply parameterized autoencoder). A simply parameterized D-autoencoder game is a sim-
ple parameterized statistical game pc,πq : pX,Xq
pΩ,Ωq
ÝÝÝÑ pY,Y qwith the D-autoencoder /f_itness function
φ : Ctxpcq ÑR given by φpπ,kq “Ey„L π|c|kM rFDpc1
π,c,π,y qs. That is, a simply parameterized D-
autoencoder game is just a simple D-autoencoder game with tensor product domain type.
More often in applications, one doesn’t use the same backwards channel to update both the “belief about the
causes” inXand the parameters inΩ simultaneously. Instead, the backwards channel updates only the beliefs
over X, and any updating of the parameters is left to another ‘higher-order’ process. TheX-update channel
may nonetheless still itself be parameterized in Ω: for instance, if it represents an approximate inference
algorithm, then one often wants to be able to improve the approximation, and such improvement amounts
to a change of parameters. The ‘higher-order’ process that performs the parameter updating is then often
represented as a reparameterization: a 2-cell in the bicategory ParapSGameCq. The next example tells the
/f_irst part of this story.
Example 5.15 (Parameterized autoencoder). A (simple) parameterized D-autoencoder game is a parameter-
ized statistical game pc,φq: pX,Xq
pΩ,Θq
ÝÝÝÑpY,Y qwith /f_itness functionφ: CtxpcqÑ R given by
φpπ,kq“ E
y„L π|c|kM
“
FD
`
pc1
πqX,c|πΩ,πX,y
˘‰
.
As before, the notation p´qX indicates taking the X marginal, along the projection Θ bXÑ‚X. We also
de/f_inec|πΩ :“c‚pπΩ b idXq, indicating “cgiven the parameter state πΩ”. Written out in full, the /f_itness
function is therefore given by
φpπ,kq“ E
y„L π|c|kM
“
FD
`
projX ‚c1
π,c ‚pπΩ b idXq,projX ‚π,y
˘‰
.
Note that the pair
`
c|πΩ,pc1
πΩqX
˘
de/f_ines an unparameterized simple Bayesian lenspX,Xq ÞÑpY,Y q, with
c|πΩ : XÑ‚Y and pc1
πΩqX : Y XÝ Ñ‚ X.
Remark 5.16 (Meaning of ‘autoencoder’). Why do we call autoencoder games thus? The name originates in
machine learning, where one thinks of the forwards channel as ‘decoding’ some latent state into a prediction
8Strictly speaking, one can have a decrease in free energy along with an increase in relative entropy, as long as the former remains
greater than the latter. Therefore, optimizing the free energy does not necessarily optimize the relative entropy. However, as
elaborated in Remark 5.16, the diﬀerence between the variational free energy and the relative entropy is the log-likelihood, so
optimizing the free energy corresponds to simultaneous maximum-likelihood estimation and Bayesian inference.
26
of some generated data, and the backwards channel as ‘encoding’ a latent state given an observation of the
data; typically, the latent state space is thought to have lower dimensionality than the observed data space,
justifying the use of this ‘compression’ terminology. A slightly more precise way to see this is to consider
an autoencoder game where the context and forwards channel are /f_ixed. The only free variable available
for optimization in the /f_itness function is then the backwards channel, and the optimum is obtained when
the backwards channel equals the exact inversion of the forwards channel (given the prior in the context,
and for all elements of the support of the state obtained from the continuation). Conversely, allowing only
the forwards channel to vary, it is easy to see that the autoencoder /f_itness function is then equal to the
/f_itness function of a maximum log-likelihood game (up to a constant). Consequently, optimizing the /f_itness of
an autoencoder game in general corresponds to performing approximate Bayesian inference and maximum
likelihood estimation simultaneously. The optimal strategy (lens or parameter) can then be considered as
representing an ‘optimal’ model of the process by which observations are generated, along with a recipe for
inverting that model (and hence ‘encoding’ the causes of the data). The pre/f_ixauto- indicates that this model
is learnt in an unsupervised manner, without requiring input about the ‘true’ causes of the observations.
Some authors (in particular, Knoblauch et al. [29]) take a variant of the D-autoencoder /f_itness function to
de/f_ine a generalization of Bayesian inference: in an echo of Remark 5.16, the intuition here is that Bayesian
inference simply is maximum likelihood estimation, except ‘regularized’ by the uncertainty encoded in the
prior, which stops the optimum strategy being trivially given by a Dirac delta distribution. By allowing both
the choice of likelihood function and divergence measure to vary, one obtains a family of generalized inference
methods. Moreover, when one retains the standard choices of log-density as likelihood and relative entropy as
divergence, the resulting generalized Bayesian inference games coincide with variational autoencoder games;
then, when the forwards channel (or its parameter) is /f_ixed, both types of game coincide with the Bayesian
inference games of Example 5.7 above.
Example 5.17 (Generalized Bayesian inference [29]) . Let D : CpI,Xqˆ CpI,Xq ÑR be a measure of
divergence between states on X, and let l : Y bXÑ‚I be any eﬀect on Y bX. Then a simple generalized
pl,Dq-Bayesian inference game is a simple statistical game pc,φq : pX,Xq Ñ pY,Y qwith /f_itness function
φ: CtxpcqÑ R given by
φpπ,kq“ E
y„L π|c|kM
„
E
x„c1πpyq
rlpy,xqs` Dpc1
πpyq,πq

where pc,c1q: pX,Xq ÞÑpY,Y qconstitutes the lens part of the game.
Proposition 5.18. Generalized Bayesian inversion and autoencoder games coincide when D “ DKL and
l“´ log pc, where pc is a density function for the forwards channel c.
Proof. Consider the DKL-free energy. We have
FDKL
`
c1
π,c,π,y
˘
“ E
x„c1πpyq
r´log pcpy|xq´ log pπpxqs´ SX
“
c1
πpyq
‰
by Remark 5.12
“ E
x„c1πpyq
r´log pcpy|xqs` E
x„c1πpyq
rlog qpx|yq´ log pπpxqs
“ E
x„c1πpyq
r´log pcpy|xqs` DKL
`
c1
πpyq,π
˘
“ E
x„c1πpyq
rlpy,xqs` D
`
c1
πpyq,π
˘
where qis a density function for c1
π.
27
Unsurprisingly, as in the autoencoder case, there are parameterized and simply parameterized variants of
generalized Bayesian inference games.
Finally, we remark that, in the case where C “ sfKrn, where I is not terminal and morphisms into I
correspond to functions into r0,8s, composing a Bayesian lens and its context gives a lens pI,Iq ÞÑpI,Iq:
both the forwards and backwards parts of this ‘I-endolens’ return positive reals (which in this context Jacobs
and colleagues call validities [12, 30]), and which we can think of as “the environment’s measurements of its
compatibility with the lens”. In this case, we can therefore de/f_inevalidity games , where the /f_itness function
is simply given by computing the backwards validity9. Since such a /f_itness function measures the interaction
of the lens with its environment, the corresponding statistical games may be of relevance in modelling multi-
agent or otherwise interacting statistical systems—for instance, in modelling evolutionary dynamics. We leave
the exploration of this for future work.
6. References
[1] Christopher L Buckley et al. “The free energy principle for action and perception: A mathematical re-
view”. In:Journal of Mathematical Psychology 81 (05/24/2017), pp. 55–79. arXiv:http://arxiv.org/abs/1705.09156v1 [q-bio.NC].
[2] Neil Ghani et al. “Compositional game theory”. In: Proceedings of Logic in Computer Science (LiCS) 2018
(2016). arXiv: http://arxiv.org/abs/1603.04641 [cs.GT].
[3] Aaron Bohannon, Benjamin C Pierce, and Jeﬀrey A Vaughan. “Relational lenses: a language for updat-
able views”. In:Proceedings of the twenty-/f_ifth ACM SIGMOD-SIGACT-SIGART symposium on Principles
of database systems . ACM. 2006, pp. 338–347.
[4] Brendan Fong and Michael Johnson. “Lenses and Learners”. In:In: J. Cheney, H-S. Ko (eds.): Proceedings of
the Eighth International Workshop on Bidirectional Transformations (Bx 2019), Philadelphia, PA, USA, June
4, 2019, published at http://ceur-ws.org (03/05/2019). arXiv:http://arxiv.org/abs/1903.03671v2 [cs.LG].
[5] A. M. Bastos et al. “Canonical microcircuits for predictive coding”. In: Neuron 76.4 (11/2012), pp. 695–
711. /d.sc/o.sc/i.sc: 10.1016/j.neuron.2012.10.038.
[6] Toby St. Clere Smithe. “Bayesian Updates Compose Optically”. In: (05/31/2020). arXiv:2006.01631v1 [math.CT].
[7] Joe Bolt, Jules Hedges, and Philipp Zahn. “Bayesian open games”. In: (10/08/2019). arXiv:http://arxiv.org/abs/1910.03656v1 [cs.GT].
[8] Matteo Capucci et al. “Towards foundations of categorical cybernetics”. In: (05/13/2021). arXiv:2105.06332 [math.CT].
[9] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Diagrams”. In: Math.
Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). /d.sc/o.sc/i.sc: 10.1017/S0960129518000488. arXiv:
http://arxiv.org/abs/1709.00322v3 [cs.AI].
[10] Sam Staton. “Commutative Semantics for Probabilistic Programming”. In: Programming Languages and
Systems. Springer Berlin Heidelberg, 2017, pp. 855–879./d.sc/o.sc/i.sc: 10.1007/978-3-662-54434-1_32 .
[11] Tobias Fritz. “A synthetic approach to Markov kernels, conditional independence and theorems on suﬃ-
cient statistics”. In: (08/19/2019). arXiv:http://arxiv.org/abs/1908.07021v3 [math.ST].
9Note that the backwards eﬀect, as an I-state-dependent eﬀect (or ‘vector’), already depends upon the forwards validity, so we do
not need to include the forwards validity directly in the /f_itness computation.
28
[12] Kenta Cho et al. “An Introduction to Eﬀectus Theory”. In: arXiv preprint arXiv:1512.05813 (2015). arXiv:
http://arxiv.org/abs/1512.05813v1 [cs.LO].
[13] Brendan Fong and David I Spivak. “Supplying bells and whistles in symmetric monoidal categories”.
In: (08/07/2019). arXiv: http://arxiv.org/abs/1908.02633v1 [math.CT].
[14] Martti Karvonen. “The Way of the Dagger”. In: (04/24/2019). arXiv: 1904.10805 [math.CT].
[15] J.M. Stoyanov. Counterexamples in Probability: Third Edition . Dover Books on Mathematics. Dover Publi-
cations, 2014./i.sc/s.sc/b.sc/n.sc: 9780486499987./u.sc/r.sc/l.sc: https://books.google.co.uk/books?id=xaH8AQAAQBAJ.
[16] David I. Spivak. “Generalized Lens Categories via functors Cop ÑCat”. In: (08/06/2019). arXiv:http://arxiv.org/abs/1908.02202v2 [math.CT].
[17] Joe Moeller and Christina Vasilakopoulou. “Monoidal Grothendieck Construction”. In: (09/03/2018).
arXiv: 1809.00727v2 [math.CT].
[18] Bryce Clarke et al. “Profunctor optics, a categorical update”. In: (01/21/2020). arXiv:2001.07488v1 [cs.PL].
[19] Pietro Vertechi. “Dependent Optics”. In: (04/20/2022). arXiv: 2204.09547 [math.CT].
[20] Matteo Capucci. “Seeing double through dependent optics”. In: (04/22/2022). arXiv:2204.10708 [math.CT].
[21] Dylan Braithwaite et al. “Fibre optics”. In: (12/21/2021). arXiv: 2112.11145 [math.CT].
[22] Matteo Capucci, Bruno Gavranović, and Toby St. Clere Smithe. “Parameterized Categories and Cate-
gories by Proxy”. In:Category Theory 2021 . 2021.
[23] Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In: (01/10/2017).
/d.sc/o.sc/i.sc: 10.1109/lics.2017.8005137. arXiv:http://arxiv.org/abs/1701.02547 [cs.PL].
[24] Matthijs Vákár and Luke Ong. “On S-Finite Measures and Kernels”. In: (10/03/2018). arXiv:1810.01837 [math.PR].
[25] Toby St. Clere Smithe. “Some Notions of (Open) Dynamical System on Polynomial Interfaces”. In:
(08/25/2021). arXiv: 2108.11137 [math.DS].
[26] Fosco Loregian. (Co)end Calculus . London Mathematical Society Lecture Note Series. Cambridge Uni-
versity Press, 01/11/2015./d.sc/o.sc/i.sc: 10.1017/9781108778657. arXiv:https://arxiv.org/abs/1501.02503v6 [math.CT].
[27] David M. Blei, Alp Kucukelbir, and Jon D. McAuliﬀe. “Variational Inference: A Review for Statisti-
cians”. In: Journal of the American Statistical Association, Vol. 112 , Iss. 518, 2017 (01/04/2016). /d.sc/o.sc/i.sc:
10.1080/01621459.2017.1285773. arXiv: 1601.00670v9 [stat.CO].
[28] Diederik P Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: (12/20/2013). arXiv:http://arxiv.org/abs/1312.6114v10 [stat.ML].
[29] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. “Generalized Variational Inference”. In:
(12/13/2019). arXiv: http://arxiv.org/abs/1904.02063 [stat.ML].
[30] Bart Jacobs. Structured Probabilitistic Reasoning (forthcoming) . 2019.
29
[31] Mario Román. “Open Diagrams via Coend Calculus”. In: (04/09/2020). arXiv:2004.04526v2 [math.CT].
[32] Toby St. Clere Smithe. “Cyber Kittens, or Some First Steps Towards Categorical Cybernetics”. In: Pro-
ceedings 3rd Annual International Applied Category Theory Conference 2020 (ACT 2020) . 2020.
[33] Guillaume Boisseau. “String Diagrams for Optics”. In: (02/11/2020). arXiv:2002.11480v1 [math.CT].
A. 2-local contexts, graphically
To clarify the idea that the 2-local contexts for the factors of a tensor product game (or morphism more
generally) are obtained by “/f_illing the hole” on the left or right of the tensor, one can work in the monoidal
bicategory of V-profunctors and use the associated graphical calculus [31] to depict the ‘hole’ in the context
and its /f_iller. In this section, we work with a general monoidal categoryC, which may or may not be a category
of lenses or games. Nonetheless, the basic idea is the same: a (complex) context for a morphism X ÑY is
given by a triple of a residual denoted Θ, a state I ÑΘ bX and a ‘continuation’ (or ‘eﬀect’)Θ bY ÑI,
coupled according to the coend quotient rule.
Below, we show how to obtain the object of right local 2-contexts for a tensor product morphism f bf1 :
XbX1 ÑY bY1, using the graphical calculus of V-Prof. At each stage, we depict on the left the object
named on the right. We start with a complex context for the tensor along with a ‘/f_iller’ object of morphisms
X1 ÑY1, which is shown “/f_illing the (left-hand) hole”. In the /f_irst step, we use the composition rule ofC to
connect the matching ‘ports’ on the domain X. We then couple the matching Y port using the coend and
gather Θ and Y together into a single residual. Note that these steps correspond directly to factors in the
de/f_inition ofπf1 : Ctxpf bf1qÑ Ctxpf1q(De/f_inition 4.9).
I X
X1
YX Y
Y1
Ibb
żΘ:C
CpI,Θ bXbX1qˆ CpX,Y qˆ CpΘ bY bY1,Iq
ÝÑ I
X1
YY
Y1
Ibb
żΘ:C
CpI,Θ bY bX1qˆ CpΘ bY bY1,Iq
ãÝÑ I
X1 Y1
Ibb
żΘ1:C
CpI,Θ1bX1qˆ CpΘ1bY1,Iq
We /f_ind this graphical representation to be a useful aid in comprehension, and often simpli/f_ies the symbolic
‘book-keeping’ that can complicate expressions such as those in De/f_inition 4.9. The cost of this expressivity is
the introduction of another categorical structure, and its associated cognitive load. In previous work [32], we
have made more use of this representation: there, we worked with the ‘optical’ de/f_inition of Bayesian lenses
described in St. Clere Smithe [6]; and we note that an earlier informal version of this graphical language was
30
originally used to de/f_ine local contexts for tensor product games in the compositional game theory literature
[7]. Since the V-profunctorial setting [18] is required in order to de/f_ine optics, we had already paid this extra
cognitive cost. In this paper, however, we have preferred to stick with the simpler /f_ibrational de/f_inition of
Bayesian lenses.
Remark A.1. A diﬀerent but related graphical calculus for optics is described by Boisseau [33]. However,
this alternative calculus is somewhat less general than that of Román [31], and its adoption here would not
eliminate the extra cognitive cost; we do nonetheless make use of it in [6].
31