Compositional Active Inference I:
Bayesian Lenses. Statistical Games.
Toby St. Clere Smithe
University of Oxford
&
Topos Institute
toby@topos.institute
June 10, 2022
WeintroducetheconceptsofBayesianlens,characterizingthebidirectionalstructureofexact
Bayesianinference,andstatisticalgame,formalizingtheoptimizationobjectivesofapproximate
inferenceproblems. WeprovethatBayesianinversionscomposeaccordingtothecompositional
lens pattern, and exemplify statistical games with a number of classic statistical concepts, from
maximum likelihood estimation to generalized variational Bayesian methods. This paper is the
firstinaserieslayingthefoundationsforacompositionalaccountofthetheoryofactiveinfer-
ence,andwethereforepayparticularattentiontostatisticalgameswithafree-energyobjective.
1. Introduction
Thosesystemsthatwemightclassifyas‘cybernetic’,‘adaptive’,or‘alive’alldisplayafundamentalproperty:
theyresistperturbationsthatwouldpushthemawayfromtheirgoalsorrendertheirexistenceunsustainable.
In order to do so, such systems are somehow able to sense their current state of affairs (through perception)
and respond appropriately (through action). In the series of papers of which this is the first part, we seek
tosupplynewcompositionalfoundationsforatheoryofactiveinference adequatetodescribesuchsystems,
withaparticularfocusontheframeworkthathascometobeknowninthecompositionalneuroscienceand
artificiallifecommunitiesasthefreeenergyprinciple[1],whosestructuresweseektomakeprecise.
AcentralfeatureofactiveinferenceistheuseofthestatisticalprocedurecalledBayesianinference,which
suppliesarecipebywhichasystemmightinvertastatisticalmodel(say,ofhowcausesgenerateobservations)
in order to form beliefs about the causes of observed data. It is easy to see how such a process of inferring
causescouldbeunderstoodasaprocessofperception,butthecentraldogmaofactiveinferenceisthatboth
perceptionandactioncanberenderedasproblemsofBayesianinference,withactionbeing‘dual’topercep-
tion: insteadofchangingitsinternal state(itsbeliefsaboutcauses)tomatchitsobservationsbetter,asystem
mightacttochangetheexternalstate(thestateoftheworld)sothattheobservationsthatitexpectsordesires
obtain. Inthefreeenergyframework,bothperceptionandactionemergethroughtheoptimizationofasingle
quantity,thefreeenergy.
1
2202
nuJ
8
]TS.htam[
2v16440.9012:viXra
Such processes of optimization, and perception and action more generally, are inherently dynamical pro-
cesses. Inthisfirstpaperoftheseries,weputthedynamicstemporarilyaside,andlaythestatisticalfounda-
tions,characterizingthecompositionalstructureofthegenerativemodelsinstantiatedbycyberneticsystems
andthealgebraoftheirinversion. ThisalgebraisformalizedbyourconceptofBayesianlens,whichweintro-
ducetocharacterizetheinherentlybidirectionalstructureofBayesianinversion,drawingonthe‘lens’pattern
thatstructuresbidirectionalsystemsfromeconomicgames[2],todatabases[3],andmachine-learners[4].
This lens structure serves more than an organizing purpose: we prove that the inversion of a composite
or ‘hierarchical’ statistical model is equivalently given (up to almost-equality) by the lens composition rule.
Thismeansthatcyberneticsystemsembodyingcomplexcompositemodelscansimplyinverteachcomponent
factoroftheirmodelsandthencombinetheseinversions,inordertoobtainaninversionofthewhole.Inturn,
this explains the observation that hierarchical systems in the brain (such as much of the visual cortex) can
beexplainedasacompositeof‘local’circuitseachperformingaformofapproximateBayesianinferencecall
predictivecoding[5].
Havingestablishedthestructuresrequiredtostateandprovethat“Bayesianupdatescomposeoptically”1,
we formalize the “algebra of statistical inference problems” as categories of statistical games. These ‘games’
consist of a lens paired with a contextual fitness function, which define the quantities that we often think
of cybernetic systems as optimizing, and where the ‘context’ formalizes the system’s interaction with its
environment. Inthisdevelopment,wedrawmuchinspirationfromcompositionalgametheory[2,7]. Weex-
emplifythesestatisticalgameswitharangeofexamplesfrommaximumlikelihoodestimationtogeneralized
variationalBayesianmethods.
Thispaperisthefirstofaseriesofpapers.Thenextinstalmentintroducesthestructuresnecessarytosupply
statistical games with “dynamical semantics”, and thus breathe some life into those systems that perform
approximate inference. A subsequent paper will then explain how such systems can perform action, and
therebyaffecttheworldsthattheyinhabit.
Overview of this paper We begin in §2.1 by introducing the basics of compositional probability theory.
In§2.2,weintroducethestructuresnecessarytodescribethelenspattern,andrecallthatin‘nice’situations,
theresultingcategoriesoflensesaremonoidal. In§2.3,weintroducetheParaconstruction,whichhasbeen
proposed[8]asfoundationalforcategoricalcybernetics,andoftenplaysanimportantroleforus,too. Then,
in§3,wedefineBayesianlensesandprovethetheoremthatBayesianinversionscomposeaccordingtothelens
pattern;wealsogiveadetaileddescriptionofthelow-dimensionalstructureofparameterizedBayesianlenses
(§3.4). In§4,wedefinecontextsforBayesianlenses,fitnessfunctions,andtheresultingmonoidalcategories
ofstatisticalgames. Finally,in§5,wegiveanumberofexamples.
Acknowledgements Thisseriesofpapersistheresultofanumberofinteractionswithinandaroundthe
appliedcategorytheoryandactiveinferencecommunities.WethankthereviewersandorganizersoftheACT
conferences in 2020, 2021, and 2022, as well as those of the SYCO series of symposia. We thank the Topos
InstituteandFoundationalQuestionsInstituteforsupportandfunding.Andweparticularlythankthefollow-
ing individuals (in no particular order) for stimulating discussions, comments, and encouragement: Samson
Abramsky; Matteo Capucci; Bob Coecke; Lance da Costa; Brendan Fong; Karl Friston; Bruno Gavranović;
NeilGhani;JulesHedges;JohannesKleiner;TimHosgood;SophieLibkind;DavidJazMyers;ValeriadePaiva;
EvanPatterson;MaxwellRamstead;DaltonSakthivadivel;BrandonShapiro;DavidSpivak;SamStaton;Sean
Tull;VincentWang.
1Althoughwedonotusetheheaviermachineryof‘optics’here;forthat,seeourpreprint[6]andRemark2.41below.
2
2. Mathematical background for statistical games
2.1. Compositionalprobability,concretelyandabstractly
InordertodefineBayesianlensesandstatisticalgamesandprovesomebasicresultsaboutthem,wewillwork
atahighlevelofabstraction;then,toexemplifythemwithapplications,wewillneedtoworkmoreconcretely.
Inbothinstances,ourbasiccategoricalsettingwillbeacopy-deleteorMarkovcategory,whosemorphismswe
will call stochastic channels or Markov kernels and which behave like functions with uncertain outputs; for
us,thesemodeltheprocessesbywhichobservationaldataare(believedtobe)generatedbyprocessesinthe
world,andcompositechannelsmodelcomposite(sequencesof)processes.
We will typically be interested in applications where the sample spaces are continuous and where the
probability measures may have infinite support. A stochastic channel c : XÑ‚ Y will be something like a
functiontakingvaluesinprobabilitymeasuresoveraspace,butinapplicationsonewilloftenfixareference
measure and then work with a density function p : X ˆ Y Ñ r0,1s representing the channel c with
c
respect to that measure. We begin this section by introducing the category sfKrn of s-finite kernels, where
these concepts have precise meanings, before generalizing graphically to the abstract setting of copy-delete
categories.
2.1.1. S-finitekernels
WesketchthebasicstructureofthecategorysfKrnofs-finitekernelsbetweenmeasurablespaces,andrefer
thereadertoChoandJacobs[9]andStaton[10]forelaborationofthedetails.
Definition2.1. SupposepX,Σ qandpY,Σ qaremeasurablespaces, withX,Y setsandΣ ,Σ thecor-
X Y X Y
respondingσ-algebras. Akernel kfromX toY isafunctionk : XˆΣ Ñ r0,8ssatisfyingtheconditions:
Y
• forallx P X,kpx,´q : Σ Ñ r0,8sisameasure;and
Y
• forallB P Σ ,kp´,Bq : X Ñ r0,8sismeasurable.
Y
Akernelk : X ˆΣ Ñ r0,8sisfinite ifthereexistssomer P r0,8qsuchthat,forallx P X,kpx,Yq ď r.
Y ř
Andk iss-finiteifitisthesumofatmostcountablymanyfinitekernelsk ,k “ k .
n n:N n
Proposition2.2. Measurablespacesands-finitekernelspX,Σ
X
qÑ‚ pY,Σ
Y
qbetweenthemformacategory,
denotedsfKrn;notethatoftenwewilljustwriteXforameasurablespace,leavingtheσ-algebraΣ implicit.
X
Identity morphisms id
X
: XÑ‚ X in sfKrn are Dirac kernels δ
X
: X ˆ Σ
X
Ñ r0,8s :“ x ˆ A ÞÑ 1 iff
x P A and 0 otherwise. Composition is given by a Chapman-Kolmogorov equation: suppose c : XÑ‚ Y and
d : YÑ‚ Z. Then ż
d‚c : X ˆΣ Ñ r0,8s :“ xˆC ÞÑ dpC|yqcpdy|xq
Z
y:Y
wherewehaveused‘conditionalprobability’notationdpC|yq :“ dpy,Cq.
Remark 2.3. In the scientific literature, one often encounters the term conditional probability distribution,
whichcanalmostalwaysbeinterpretedasindicatingaprobabilitykernel:wecanthinkofaprobabilitykernel
c : XÑ‚ Y asafunctionthatemitsaprobabilitydistributioncpxq : Σ
Y
Ñ r0,8soverYforeachx : X. We
can then see cpxq as a probability distribution conditional on the choice or observation of x : X, hence the
notationcp´|xq. Onereadsthisnotation´|xas“´givenx”.
Proposition2.4. Thereisamonoidalstructurepb,1qonsfKrn,theunitofwhichisthesingletonset1with
itstrivialsigma-algebra. Onobjects,X bY istheCartesianproductpX ˆY,Σ qofmeasurablespaces,
XˆY
3
withtheproductsigma-algebra. Onmorphisms,f bg : X bYÑ‚ AbB isgivenby
ż ż
f bg : pX ˆYqˆΣ :“ pxˆyqˆE ÞÑ δ pE|x,yqfpda|xqgpdb|yq
AˆB AbB
a:A b:B
where,asabove,δ pE|a,bq “ 1iffpa,bq P Eand0otherwise.NotethatpfbgqpE|x,yq “ pgbfqpE|y,xq
AbB
foralls-finitekernels(andallE,xandy),bytheFubini-Tonellitheoremfors-finitemeasures[9,10],andso
bissymmetriconsfKrn.
Remark2.5(Statesandeffects). Wewillcallkernelswithdomain1states. Akernel1Ñ‚ X isequivalentlya
function Σ Ñ r0,8s, which is simply a (possibly improper) measure on the space pX,Σ q. Occasionally
X X
wewillsaydistributiontomean‘state’. Wecallastateonaproductspace,suchasω : 1Ñ‚ XbY,ajointstate
orjointdistribution.
Dually, kernels with codomain 1 will be called effects. Note that although 1 is the unit of the preceding
monoidal structure, this unit is not terminal in sfKrn: s-finite kernels XÑ‚ 1 are equivalently measurable
functions X Ñ r0,8s, and there are of course many nontrivial examples; of which density functions will
formanimportantclass.
Definition2.6(Probabilitykernel,probabilitymeasure,probabilityspace). Ifakernelk : XˆΣ Ñ r0,8s
Y
satisfiestheadditionalconditionsthatittakesvaluesintheunitintervalr0,1sand,forallx : X,kpYq “ 1,
thenwecallkaprobabilitykernel.Ifkisastate(i.e.,X “ 1),thenwecallitaprobabilitymeasure.Aprobability
spaceisapairpΩ,πqofameasurablespaceΩwithaprobabiltymeasureπ : 1Ñ‚ Ω.
Remark2.7(Girymonad). Probabilitymeasures1Ñ‚ X oneachX formthepointsofaspacewhichwewill
denoteGX. Thisspacecanbeequippedwithacanonicalσ-algebra,makingGintoafunctorMeas Ñ Meas
which acts on each measurable function f : X Ñ Y by returning its pushforward f : GX Ñ GY, defined
˚
by f ν : Σ Ñ r0,1s : B ÞÑ νpf´1pBqq. G can in turn be equipped with a monad structure pµ,ηq. Every
˚ Y
measurablefunctionX Ñ GY correspondstoaprobabilitykernelXÑ‚ Y,makingtheKleislicategoryK(cid:96)pGq
a subcategory of sfKrn. Composition of probability kernels XÝÑ f ‚ YÑÝ g ‚ Z corresponds accordingly to Kleisli
composition X ÝÑ f GY Ý G Ñ g GGZ Ý µ ÝÑZ GZ, and the components of the monad unit correspond to the Dirac
(identity)kernelsδ
X
: XÑ‚ X. WereferthereadertoFritz[11,§4]andthereferencesthereinforelaboration.
The slice 1{K(cid:96)pGq of K(cid:96)pGq under 1 is the category of probability spaces and measure-preserving kernels
betweenthem,calledProbStochbyFritz[11].
Remark 2.8 (Convex spaces and expectations). A convex space is an algebra of the Giry monad; that is, a
spaceX equippedwithameasurablefunctionGX Ñ X calledthealgebraevaluationorexpectedvalue. Each
measurablefunctionf : X Ñ X inducesanexpectedvalueE : GX Ñ X definedas
f
ż
E pπq :“ fpxqπpdxq.
f
x:X
Wewilltypicallyinsteadwrite
E rfs :“ E pπq
f
x„π
where the notation x „ π should be read as “x distributed according to π”. More generally, we have an
operatorE : MeaspΩ,XqˆGΩ Ñ X definedsimilarlyby
ż
E rps :“ ppωqπpdωq
ω„π
ω:Ω
where p : Ω Ñ X and π : GΩ. Note that this subsumes the case where p is an X-valued random variable
definedonaprobabilityspacepΩ,πq. Commonly,wewillhaveX “ RorX “ r0,8s.
4
Remark2.9(Effectsandvalidities). AspecialcaseoftheprecedingexpectationoperatoroccurswhenX “
r0,8s. Inthiscase,mapsΩ Ñ r0,8sareofcourseeffectsΩÑ‚ 1insfKrn,andforeachstateπ : 1Ñ‚ Ωand
effectp : ΩÑ‚ 1,theexpectationoperatorsimplycomputesthecompostep‚ω. Thatis,wehaveinthiscase
E rps “ p‚ω, and we might then call p a predicate on Ω and the expectation E rps the validity of p
ω„π ω„π
in the state π. We refer the reader to Cho et al. [12, §5] for more on this perspective, where the validity is
writtenπ |ù p.
Observation2.10. EachspaceXinsfKrnisequippedwithacanonicaleffect
X
: XÑ‚ 1,theconstanteffect
x ÞÑ 1. Wedenotethisfamilyofeffectsbythe‘ground’symbol andcallthecomponentsdiscardingmaps
withtheintuitionthattheyactby‘discarding’information(“wiringtoground”). Anotherwaytocharacterize
probabilitykernels,andthusK(cid:96)pGq,isthattheymakediscardingnatural,satisfying ‚c “ . Wecallsuch
channelsorprocessescausal:theycannotaffecttheoutcomesofprocessesearlierinasequenceofcomposites.
Not only can we discard information in sfKrn, but we can also copy it. In other words, each object is
equippedwithacanonicalcopy-deletestructure,makingitintoacomonoid,withwhichwecanduplicateand
discardstatesofthecorrespondingtype. InthetermsofFongandSpivak[13],sfKrnsuppliescomonoids.
Proposition2.11(sfKrnsuppliescomonoids). EachobjectX isequippedwithacanonicalcomonoidstruc-
ture p
X
,
X
q, with
X
: XÑ‚ X bX and
X
: XÑ‚ 1 satisfying the usual comonoid laws. Discarding is
givenbythefamilyofeffects : X Ñ r0,8s :“ x ÞÑ 1,andcopyingisDirac-like: : X ˆΣ :“
X X XˆX
xˆE ÞÑ 1iffpx,xq P E and0otherwise.
Discardingpartofajointstategivesusmarginals(or“marginaldistributions”).
Definition 2.12. Given a joint distribution ω : 1Ñ‚ X b Y, we call ω
X
:“ pid
X
b
Y
q ‚ ω : 1Ñ‚ X and
ω
Y
:“ p
X
bid
Y
q‚ω : 1Ñ‚ Y the marginals of ω. We define projection (or marginalization) operators by
proj :“ id b : X bY Ñ X b1 ÝÑ „ X andproj :“ bid : X bY Ñ 1bY ÝÑ „ Y.
X X Y Y X Y
In this work, we are interested in the problem of inverting stochastic channels XÑ‚ Y in order to obtain
channelsYÑ‚ X, andweareparticularlyinterestedinwhatisknownasBayesianinversion. Aswewillsee,
theBayesianinversionofachannelc : XÑ‚ Y isdeterminedinconjunctionwithastateπ : 1Ñ‚ X.
Definition2.13. Wecallthepairingpπ,cqofastateπ : 1Ñ‚ X withachannelc : XÑ‚ Y agenerativemodel
XÑ‚ Y. Itinducesajointdistributionω
pπ,cq
:“ pid
X
bcq‚
X
‚π : 1Ñ‚ X bY. Themarginalsofω
pπ,cq
are
π andc‚π.
Intheinformalscientificliterature,theBayesianinversionofachannelXÑ‚ Y (typicallycalledthe‘likeli-
hood’)withrespecttoastate1Ñ‚ X (typicallycalledthe‘prior’)isoftenwrittenastheexpression
ppy|xqppxq ppy|xqppxq
ppx|yq “ “ ş , (1)
ppyq ppy|x1qppx1qdx1
x1:X
but this expression is very ill-defined: what is ppy|xq, and how does it relate to a channel c : XÑ‚ Y? Why
aretheclearlydifferenttermsppx|yq,ppy|xq,ppxq,ppyqallwrittenwiththesamesymbolp?
Toanswerthesequestionsandclarifysuchexpressions,weusedensityfunctions.
Definition 2.14 (Density functions). We will say that a kernel c : XÑ‚ Y is represented by the effect p
c
:
X bYÑ‚ 1withrespecttothestateµ : 1Ñ‚ Y when
ż
c : X ˆΣ Ñ r0,8s :“ xˆB ÞÑ µpdyqp py|xq.
Y c
y:B
We call the corresponding function p : X ˆ Y Ñ r0,8s a density function for c. Note that we also use
c
conditionalprobabilitynotationfordensityfunctions,andsop py|xq :“ p px,yq.
c c
5
Remark 2.15. When a channel c is associated with a density function, we will adopt the convention of
naming the density function p ; that is, with a subscript indicating the corresponding channel. In this way,
c
wecanrewriteEquation(1)as
p py|xqp pxq p py|xqp pxq
p px|yq “ c π “ ş c π . (2)
c: π p c‚π pyq x1:X p c py|x1qp π px1qdx1
Remark 2.16. We will also adopt the convention of denoting a Bayesian inversion of the channel c with
respect to the state π by the symbol c:. We adopt this symbol because it is known that Bayesian inversion
π
induces a ‘dagger’ functor [14] on (a quotient of) the category 1{K(cid:96)pGq of probability spaces and measure-
preservingfunctions[11,Remark13.9];whenwe‘forget’themeasuresassociatedwiththeprobabilityspaces—
which form the ‘priors’ for the inversions—then we have to explicitly incorporate them into the structure,
whichweindicatewiththesubscriptπ inc:.
π
We wrote “a Bayesian inversion” in the preceding remark since in a general measurable setting Bayesian
inversionsneednotalwaysexist[15],andwhentheydotheymayonlybeuniqueuptoalmost-equality.
Definition 2.17 (Almost-equality). Given a state π : 1Ñ‚ X, we say that two parallel channels c,d : XÑ‚ Y
areπ-almost-equal,denotedc „ π d,ifthejointdistributionsofthetwogenerativemodelspπ,cqandpπ,dqare
equal;thatis,ifpidbcq‚ ‚π “ pidbdq‚ ‚π.
Remark2.18. Likethenotionofjointdistributionforagenerativemodel, themeaningofthedefinitionof
π-almostequality—thattheinducedjointstatesareequal—willberenderedclearerinthegraphicalcalculus.
We are now in a position to define Bayesian inversions for channels in sfKrn, although we leave the
abstractdefinitionsatisfiedbythefollowinguntilDefinition2.30.
Proposition2.19(ChoandJacobs[9,Example8.4]). Supposepπ,cqisagenerativemodelXÑ‚ Y insfKrn,
wherecisrepresentedbytheeffectp
c
withrespecttothestateµ : 1Ñ‚ X. Then,whenitexists,thechannel
c:
π
: YÑ‚ X definedasfollowsisaBayesianinversionofcwithrespecttoπ:
ˆż ˙
c: : Y ˆΣ Ñ r0,8s :“ yˆA ÞÑ πpdxqp py|xq p´1pyq
π X c c
x:A ż
“ p´1pyq p py|xqπpdxq,
c c
x:A
wherep´
c
1 : YÑ‚ I isgivenuptoµ-almost-equalityby
ˆż ˙
´1
p´1 : Y Ñ r0,8s :“ y ÞÑ p py|xqπpdxq .
c c
x:X
NotethatfromtheprecedingpropositionwerecovertheinformalformofBayes’rule(Equation(2)). Sup-
poseπ isitselfrepresentedbyadensityfunctionp withrespecttotheLebesguemeasuredx. Then
π
ż
p py|xqp pxq
c:pA|yq “ ş c π dx.
π p py|x1qp px1qdx1
x:A x1:X c π
2.1.2. Copy-deletecategoriesandtheirgraphicalcalculus
While most of our examples and applications will found in sfKrn, most of our definitions and results hold
moregenerally,anditisinsuchmoregeneraltermsthattheyaremostnaturallyexpressed.Ourmainlanguage
willbethatofcategorieslikesfKrninwhichinformationcanbetransformed,copied,anddeleted.
6
Definition 2.20 (Cho and Jacobs [9, Def. 2.2]). A copy-delete category is a symmetric monoidal category
pC,b,IqinwhicheveryobjectX issuppliedwithacommutativecomonoidstructurep , qcompatible
X X
with the monoidal structure of pb,Iq. An affine copy-delete category, or Markov category [11], is a copy-
delete category in which every channel c is causal in the sense that ‚ c “ . Equivalently, a Markov
categoryisacopy-deletecategoryinwhichthemonoidalunitI istheterminalobject.
Example2.21. sfKrnisacopy-deletecategory,whileK(cid:96)pGqisaMarkovcategory.
Monoidal categories, and (co)monoids within them, admit a formal graphical calculus that substantially
simplifiesmanycalculationsinvolvingcomplexmorphisms:proofsofmanyequalitiesreducetovisualdemon-
strationsofisotopy,andstructuralmorphismssuchasthesymmetryofthemonoidalproductacquireintuitive
topologicaldepictions. Wemakesubstantialuseofthiscalculusbelow,andsummarizeitsfeatureshere. For
moredetails,seeChoandJacobs[9,§2]orFritz[11,§2]orthereferencescitedtherein.
Depiction 2.22 (Basic conventions). String diagrams in this paper will be read vertically, with information
flowingupwards(frombottomtotop);inlaterparts,wewillhavediagramsorientedleft-to-right. Sequential
compositionisrepresentedbyconnectingstringstogether; andparallelcompositionbbyplacingdiagrams
adjacenttooneanother. Thisway,c : XÑ‚ Y,id X : XÑ‚ X,d‚c : XÑÝ‚ c YÝÑ d ‚ Z,andf bg : X bYÑ‚ AbB
aredepictedrespectivelyas:
Y X Z A B
d
c f g
c
X X X X Y
Werepresent(theidentitymorphismon)themonoidalunitI asanemptydiagram:thatis,weleaveitimplicit
inthegraphicalrepresentation.
Depiction2.23(Statesandeffects). Statesσ : IÑ‚ X andeffectsη : XÑ‚ I willbedepictedasfollows:
X
η
σ
X
Definition 2.24 (Causality). We say that a morphism is causal if it satisfies the following condition, where
is the canonical discarding map (supplied by the copy-delete category structure) of the appropriate type;
compareObservation2.10.
c “
Remark2.25. Observethat,ifthemonoidalunitisterminal,theneverymorphismiscausal.
Depiction2.26(Monoidalsymmetry). Thesymmetryofthemonoidalstructureswap : XbY ÝÑ „ Y bX
XY
is depicted as the swapping of wires, and satisfies the equations below. The left says that swapping is an
7
isomorphism;therightsaysthatitcommuteswithcopying,makingeveryobjectacommutativecomonoid:
“ and “
Depiction2.27(Comonoidlaws). Thecopy-deletestructurep , qisrequiredtosatisfythecomonoidlaws,
depictedbelow,ofunitality(leftdepiction)andassociativity(rightdepiction):
“ “ and “
Depiction 2.28 (Marginalization of joint states). The discarding maps induce projections X b Y Ý i Ý d Ý b ÝÑ
XbI ÝÑ „ X andXbY ÝÝ b ÝÑ id IbY ÝÑ „ Y,withwhichwecanobtainthemarginalsofjointstates;compare
Definition2.12. Supposethenthatajointstateω : IÑ‚ XbY hasmarginalsω
1
: IÑ‚ X andω
2
: IÑ‚ Y. Then
wehave
X X Y Y
“ and “ .
ω ω ω ω
1 2
Depiction2.29(Generativemodels). Agenerativemodelpπ,cq : XÑ‚ Y inducesajointstateωonXbY by
X Y X Y
c
“
ω π
withmarginalsπ andc‚π givenby
X X X Y Y Y
c c c
“ “ and “ “ .
ω π π ω π π
(CompareDefinition2.13.)
Definition2.30(Bayesianinversion). Wesaythatachannelc : XÑ‚ Y admitsBayesianinversionwithrespect
8
toπ : IÑ‚ X ifthereexistsachannelc:
π
: YÑ‚ X satisfyingthefollowingequation[9,eq. 5]:
X Y X Y
c c:
π
“ (3)
c
π π
We say that c admits Bayesian inversion tout court if c admits Bayesian inversion with respect to all states
π : IÑ‚ X suchthatc‚π hasnon-emptysupport. WesaythatacategoryC admitsBayesianinversionifall
itsmorphismsadmitBayesianinversiontoutcourt.
Depiction2.31(Densityfunctions). Achannelc : XÑ‚ Y issaidtoberepresentedbytheeffectp
c
: XbYÑ‚ I
withrespecttoµ : IÑ‚ Y if
Y
Y
p
c “ .
µ
X X
Wecallp adensityfunctionforc;compareDefinition2.14.
c
Definition 2.32 (Almost-equality). Given a state π : IÑ‚ X, we say that two channels c : XÑ‚ Y and d :
XÑ‚ Y areπ-almost-equal,denotedc „ π d,if
X Y X Y
c d
– .
π π
Proposition2.33(Compositionpreservesalmost-equality). Ifc „ π d,thenf ‚c „ π f ‚d.
Proof. Immediatefromthedefinitionofalmost-equality.
9
Proposition2.34(Bayesianinversesarealmost-equal). Supposeα : YÑ‚ X andβ : YÑ‚ X arebothBayesian
inversionsofthechannelc : XÑ‚ Y withrespecttoπ : IÑ‚ X. Thenα c „ ‚π β.
Proof. ImmediatefromEquation(3).
2.2. Lensesfordependentbidirectionalprocesses
TheBayesianinversionofastochasticchannelc : XÑ‚ Y isafamilyofchannelsc:
π
: YÑ‚ X intheopposite
direction, indexed by states on X. Pairs pc,c:q of a morphism c with a c-dependent opposite morphism c:
often fall into the compositional ‘lens’ pattern, and the Bayesian case is no exception. In this section, we
sketchthebasictheoryoflenses,andreferthereadertoourpreprint[6]forfurtherexposition. Thecentral
elementofthestructureisa(pseudo)functorpickingout,foreachmorphismc,thecategoryinwhichc:lives.
Withthispiecetohand,anentirecorrespondingcategoryoflensescandefinedmostconcisely.
Definition2.35(Spivak[16, Def. 3.3]). ThecategoryGrLens ofGrothendiecklenses forapseudofunctor
F
F : Cop Ñ CatisthetotalcategoryoftheGrothendieckconstructionforthepointwiseoppositeofF.
Proposition 2.36 (GrLens F is a category). The objects pGr ` Lens F q 0 of GrL ˘ ens F are (dependent) pairs
pC,XqwithC : C andX : FpCq,anditshom-setsGrLens pC,Xq,pC1,X1q aredependentsums
F
` ˘ ÿ ` ˘
GrLens pC,Xq,pC1,X1q “ FpCq FpfqpX1q,X
F
f:CpC,C1q
` ˘
so that a morphism pC,Xq ÑÞ pC1,X1q is a pair pf,f:q of f : CpC,C1q and f: : FpCq FpfqpX1q,X . We
callsuchpairsGrothendiecklensesfor F orF-lenses.
Proofsketch. TheidentityGrothendiecklensonpC,Xqisid “ pid ,id q. Sequentialcompositionisas
pC,Xq C X
follows. Givenpf,f:q : pC ` ,Xq ÑÞ pC1,X1 ˘qandpg,g:q : pC1,X1q ÑÞ pD,Yq,theircompositepg,g:q pf,f:q
is defined to be the lens g ˝f,Fpfqpg:q : pC,Xq ÑÞ pD,Yq. Associativity and unitality of com(cid:11)position
followfromfunctorialityofF.
Definition 2.37. Suppose FpCq “ C , with F : Cop Ñ Cat a pseudofunctor. Define SimpGrLens
0 0 F
to be the full subcategory of GrLens whose objects are duplicate pairs pC,Cq of objects C in C. We call
F
SimpGrLens thecategoryofsimpleF-lenses. Moregenerally,anylensbetweensuchduplicatepairswill
F
becalledasimplelens. SinceduplicatingtheobjectsinthepairspX,Xqisredundant,wewillwritetheobjects
simplyasX.
Another name for a pseudofunctor F : Cop Ñ Cat is an indexed category. When C is a monoidal cat-
egory with which F is appropriately compatible, then we can ‘upgrade’ the notions of indexed category
and Grothendieck construction accordingly. In this work, the domain categories C are only trivially bicate-
gories,andthepseudofunctorsF arereallyjustfunctors; wewillrestrictourselvestothe1-categoricalcase
ofmonoidalindexedcategories,too.
Definition 2.38 (Moeller and Vasilakopoulou [17, §3.2]). Suppose pC,b,Iq is a monoidal category. We say
that F is a monoidal indexed category when F is a weak lax monoidal functor pF,µ,µ q : pCop,bop,Iq Ñ
0
pCat,ˆ,1q. This means that the laxator µ is given by a natural family of functors µ : FA ˆ FB ñ
A,B
FpAbBq along with, for any morphisms f : A Ñ A1 and g : B Ñ B1 in C, a natural isomorphism µ :
f,g
µ ˝pFf ˆFgq ñ Fpf bgq˝µ . Thelaxatorandtheunitorµ : 1 Ñ FI togethersatisfystandard
A1,B1 A,B 0
axioms of associativity and unitality. All told, this structure makes pF,b,µ,I,µ q into a pseudomonoid in
0
themonoidal2-categoryofindexedcategoriesandindexedfunctors.
10
Proposition2.39(MoellerandVasilakopoulou[17,§6.1]). SupposepF,µ,µ q : pCop,bop,Iq Ñ pCat,ˆ,1q
0 ş
is a monoidal indexed category. Then the total category of the Grothendieck construction F obtains a
monoidalstructurepb ,I q. Onobjects,define
µ µ
` ˘
pC,Xqb pD,Yq :“ C bD,µ pX,Yq
µ CD
whereµ : FC ˆFD Ñ FpC bDqisthecomponentofµatpC,Dq. Onmorphismspf,f:q : pC,Xq ÑÞ
CD
pC1,X1qandpg,g:q : pD,Yq ÑÞ pD1,Y1q,define
` ˘
pf,f:qb pg,g:q :“ f bg,µ pf:,g:q .
µ CD
` ˘
The monoidal unit I is defined to be the object I :“ I,µ p˚q . Writing λ : I b p´q ñ p´q and ρ :
µ µ 0 ş
Cbp´q ñ p´qfortheleftandrightunitorsofthemonoidalstructureonC,theleftandrightunitorsin F
aregivenbypλ,idqandpρ,idqrespectively. WritingαfortheassociatorofthemonoidalstructureonC,the
ş
associatorin F isgivenbypα,idq.
Corollary2.40. WhenF : Cop Ñ Catisequippedwitha(weak)laxmonoidalstructurepµ,µ q,itscategory
0
of lenses GrLens becomes a monoidal category pGrLens ,b1 ,I q. On objects b1 is defined as b in
F F µ µ µ µ
Proposition2.39,asisI . Onmorphismspf,f:q : pC,Xq ÑÞ pC1,X1qandpg,g:q : pD,Yq ÑÞ pD1,Y1q,define
µ
` ˘
pf,f:qb1 pg,g:q :“ f bg,µop pf:,g:q
µ CD
where µop : FpCqop ˆ FpDqop Ñ FpC b Dqop is the pointwise opposite of µ . The associator and
CD CD
unitorsaredefinedasinProposition2.39.
Remark2.41. Analternativeperspectiveonlensesisgivenbythefamilyofstructuresknownasoptics[18],
whichgeneralizelensesusingthekindofactegoricalmachinerytowhichwenowturn. Thismachineryputs
thecategoriesofforwardsandbackwardsmapsonequalfooting,unlikethefibrationalmachinerydeveloped
here (which privileges the base category), at the cost of the explicit dependence structure and somewhat
heavier categorical tooling. The synthesis of Grothendieck lenses and optics, dependent optics, has recently
been articulated [19–21]; while powerful, this structure demands both the machinery of fibrations and of
actegories. Foritsrelativesimplicity,wethereforesticktothefibrationallensperspectiveinthispaper.
2.3. Categorieswithparameters
In many applications, we will be interested in cybernetic systems, where a single system might have some
freedom in the choice of forwards and backwards channel; consider the synaptic strengths or weights of a
neural network, which change as the system learns about the world, affecting the predictions it makes and
actionsittakes.Thisfreedomiswellmodelledbyequippingthemorphismsofacategorywithparameters,and
givesrisetoanotionofparameterizedcategory. Ingeneral,theparameterizationmayhavedifferentstructure
totheprocessesathand,andsowedescribethe‘actegorical’situationinwhichacategoryofparametersM
actsononacategoryofprocessesC,generatingacategoryofparameterizedprocesses.
Definition2.42(M-actegory). SupposeMisamonoidalcategorywithtensorbandunitobjectI. Wesay
thatCisaleftM-actegorywhenCisequippedwithafunctord : M Ñ CatpC,Cqcalledtheactionalongwith
naturalunitorandassociatorisomorphismsλd : IdX ÝÑ „ X andad : pMbNqdX ÝÑ „ MdpNdXq
X M,N,X
compatiblewiththemonoidalstructureofpM,b,Iq.
Proposition2.43(Capuccietal.[8]). LetpC,d,λd,adqbeanpM,b,Iq-actegory.Thenthereisabicategory
of M-parameterized morphisms in C, denoted Parapdq. Its objects are those of C. For each pair of objects
11
ř
X,Y,thesetof1-cellsisdefinedasParapdqpX,Yq :“ CpM dX,Yq;wedenoteanelementpM,fq
M:M
ofthissetbyf : X Ý M Ñ Y. Given1-cellsf : X Ý M Ñ Y andg : Y Ý N Ñ Z,theircompositeg˝f : X Ý N ÝÝ b Ý M Ñ Z is
thefollowingmorphisminC:
pN bMqdX Ý a Ý d NÝ,MÝÝ,XÑ N dpM dXq Ý id ÝNÝÝ d Ñ f N dY ÑÝ g Z
Given 1-cells f : X Ý M Ñ Y and f1 : X Ý M ÝÑ 1 Y, a 2-cell α : f ñ f1 is a morphism α : M Ñ M1 in M such
thatf “ f1˝pαdid qinC;identitiesandcompositionof2-cellsareasinC.
X
Proposition2.44. WhenC isequippedwithbothasymmetricmonoidalstructurepb,IqandanpM,b,Iq-
actegorystructure,andthereisanaturalisomorphismµr : M dpX bYq ÝÑ „ X bpM dYqcalledthe
M,X,Y
(right)costrength 2,thesymmetricmonoidalstructurepb,IqliftstoParapdq. First,fromthesymmetryofb,
oneobtainsa(left)costrength,µl : MdpXbYq ÝÑ „ pMdXqbY.Second,usingthetwocostrengthsand
M,X,Y
theassociatoroftheactegorystructure,oneobtainsanaturalisomorphismι : pMbNqdpXbYq ÝÑ „
M,N,X,Y
pM dXqbpN dYqcalledtheinterchanger. ThetensorofobjectsinParapdqisthendefinedasthetensor
ofobjectsinC,andthetensorofmorphisms(1-cells)f : X Ý M Ñ Y andg : A Ý N Ñ B isgivenbythecomposite
f bg : X bA Ý M ÝÝ b Ý N Ñ Y bB :“ pM bNqdpX bAq Ý ι ÝMÝ,NÝ,ÝXÝ,ÑA pM dAqbpN dAq Ý f Ý b Ñ g Y bB.
Inmanysimplecases,theparameterswillliveinthesamecategoryasthemorphismsbeingparameterized;
thisisformalizedbythefollowingproposition.
Proposition2.45. IfpC,b,Iqisamonoidalcategory,thenitinducesaparameterizationParapbqonitself.
ForeachM,X,Y : C,themorphismsX Ý M Ñ Y ofParapbqarethemorphismsM bX Ñ Y inC.
Notation2.46. Whenconsideringtheself-paramterizationinducedbyamonoidalcategorypC,b,Iq,wewill
oftenwriteParapCqinsteadofParapbq.
It will frequently be the case that we do not in fact need the whole bicategory structure. The following
propositiontellsusthatwecanalsojustwork1-categorically,aslongasweworkwithequivalenceclassesof
isomorphically-parameterizedmaps,inorderthatcompositionissuffientlystrictlyassociative.
Proposition2.47. EachbicategoryParapdqinducesa1-categoryParapdq byforgettingthebicategorical
1
structure.ThehomsetsParapdq pX,YqaregivenbyUParapdqpX,Yq{ „whereU istheforgetfulfunctor
1
U : Cat Ñ Set and f „ g if and only if there is some 2-cell α : f ñ g that is an isomorphism. We call
Parapdq the1-categoricaltruncationofParapdq. WhenParapdqismonoidal,soisParapdq .
1 1
3. The bidirectional structure of Bayesian updating
Inthissection,wedefineacollectionofindexedcategories,eachdenotedStat,whosemorphismscanbeseen
as “generalized Bayesian inversions”. Following Proposition 2.36, these induce corresponding categories of
lenseswhichwecallBayesianlenses.Weshowthat,forthesubcategoriesofexactBayesianlenseswhoseback-
ward channels correspond to ‘exact’ Bayesian inversions, the Bayesian inversion of a composite of forward
channelsisgiven(uptoalmost-equality)bythelenscompositeofthecorrespondingbackwardchannels. This
justifiescallingtheselenses‘Bayesian’,andprovidesthefoundationforthestudyofapproximate(non-exact)
Bayesianinversioninthesubsequentsection.
Remark. AnalternativeaccountofBayesianlenses,froman‘optical’perspective,istoldinthepreprint[6].
2So named for its similarity to the (co)strength of certain (co)monads; and similarly, too, our costrength should satisfy certain
standardcoherencelawswhichweomithere.
12
3.1. State-dependentchannels
Achannelc : XÑ‚ Y admittingaBayesianinversioninducesafamilyofinversechannelsc:
π
: YÑ‚ X,indexed
by ‘prior’ states π : 1Ñ‚ X. Making the state-dependence explicit, in typical cases where c is a probability
kernel we obtain a measurable function c: : GX ˆ Y Ñ GX. In more general situations, we obtain a
morphismc: : CpI,Xq Ñ CpY,XqinthebaseofenrichmentofthemonoidalcategorypC,b,Iqofc. Wecall
morphismsofthisgeneraltypestate-dependentchannels,andstructuretheindexingasanindexedcategory.
Definition3.1. LetpC,b,IqbeamonoidalcategoryenrichedinaCartesiancategoryV. DefinetheC-state-
indexed categoryStat : Cop Ñ V-Catasfollows.
Stat : Cop Ñ V-Cat
¨ ˛
StatpXq :“ C
0 0
˚ ‹
˚ StatpXqpA,Bq :“ #VpCpI,Xq,CpA,Bqq ‹
X ÞÑ StatpXq :“ ˚ ‹ (4)
˝ id : CpI,Xq Ñ CpA,Aq‚
A
id : StatpXqpA,Aq :“
A
ρ ÞÑ id
A
¨ ˛
Statpfq : StatpXq Ñ StatpYq
˚ ‹
˚ ‹
˚ StatpXq “ StatpYq ‹
f : CpY,Xq ÞÑ ˚ 0 0 ‹
˚ ‹
˝ VpCpI,Xq,CpA,Bqq Ñ VpCpI,Yq,CpA,Bqq ‚
` ˘ ` ˘
α ÞÑ f˚α : σ : CpI,Yq ÞÑ αpf ‚σq : CpA,Bq
CompositionineachfibreStatpXqisasinC.Explicitly,indicatingmorphismsCpI,Xq Ñ CpA,BqinStatpXq
byAÝ X Ñ‚ B,andgivenα : AÝ X Ñ‚ Bandβ : BÝ X Ñ‚ C,theircompositeisβ˝α : AÝ X Ñ‚ C :“ ρ ÞÑ βpρq‚αpρq,where
here we indicate composition in C by ‚ and composition in the fibres StatpXq by ˝. Given f : YÑ‚ X in C,
theinducedfunctorStatpfq : StatpXq Ñ StatpYqactsbypullback.
Notation3.2. JustaswewroteX Ý M Ñ Y foranM-parameterizedmorphisminCpM dX,Yq(seeProposi-
` ˘
tion2.43),wewriteAÝ X Ñ‚ B foranX-state-dependentmorphisminV CpI,Xq,CpA,Bq . Givenastateρin
CpI,XqandanX-state-dependentmorphismf : AÝ X Ñ‚ B,wewritef
ρ
fortheresultingmorphisminCpA,Bq.
Remark 3.3. We can thus think of Stat as a kind of ‘external’ parameterization of channels in C, here by
states in C. Generalizing this notion to “external parameterization by V objects” gives rise to a a cousin of
theParaconstruction: anotionofcategorybyproxy,denotedProx,ofwhichStatisa(fibred)specialcase.
Correspondingly, wethinkofParaascapturingparameterization‘internal’toC. Considerforinstancethe
casewhereMactsonCbyd : MˆC Ñ Candsupposethat,forall ř A : C,the ` functorsp´˘qdA : M Ñ Care
leftadjointtorA,´s : C Ñ M. ThenwehaveParapdqpA,Bq – M M,rA,Bs ,whichisstrongly
M:M
reminiscentofthedefinitionofStat,andcomesclosetoadefinitionofProx. Theconnectionsbetweenthese
constructionsareamatterofon-goingstudy;asummaryisavailableinCapuccietal.[22].
Proposition3.4. Statislaxmonoidal. Thecomponentsµ : StatpXqˆStatpYq Ñ StatpX bYqofthe
XY
laxatoraredefinedonobjectsbyµ XY pA,A1q :“ AbA1 andonmorphismsf : AÝ X Ñ‚ B andf1 : A1ÝÑ Y ‚ B1 by
µ
XY
pf,f1q :“ fbf1 : AbA1Ý X Ý b ‚ÝÑ Y BbB1,wherefbf1istheV-morphismCpI,XbYq Ñ CpAbA1,Bb
B1q : ω ÞÑ f bf1 . Here,ω andω aretheX andY marginalsofω,definedbyω :“ proj ‚ω and
ωX ωY X Y X X
ω :“ proj ‚ω (see Definition 2.12 and Depiction 2.28). The unit µ : 1 Ñ StatpIq of the lax monoidal
Y Y 0
structureisthefunctormappingtheuniqueobject1 : 1toI : StatpIq.
13
Example3.5. ThecategoryMeasofgeneralmeasurablespacesisnotCartesianclosed,asthereisnogeneral
waytomaketheevaluationmapsMeaspX,YqˆX Ñ Y measurable, meaningthatifwetakeC “ K(cid:96)pGq
above, then we are forced to take V “ Set. In turn, this makes the inversion maps c: : K(cid:96)pGqp1,Xq Ñ
K(cid:96)pGqpY,Xq into mere functions. We can salvage measurability by working instead with K(cid:96)pQq, where
Q : QBS Ñ QBS is the analogue of the Giry monad for quasi-Borel spaces [23]. The category QBS
is indeed Cartesian closed, and K(cid:96)pQq is enriched in QBS, so that we can instantiate Stat there, and the
correspondinginversionmapsareaccordinglymeasurable. Moreover,thereisaquasi-Borelanalogueofthe
notionofs-finitekernel[24,§11],withwhichwecandefineavariantofthecategorysfKrn.
Remark 3.6. When C is a Kleisli category K(cid:96)pTq, it is of course possible to define a variant of Stat on the
othersideoftheproduct-exponentialadjunction,withstate-dependentmorphismsAÝ X Ñ‚ B havingthetypes
TX ˆ A Ñ TB. This avoids the technical difficulties sketched in the preceding example at the cost of
requiringamonadT. However,theexponentialformmakesforbetterexegesis,andsowewillsticktothat.
3.2. Bayesianlenses
WedefinethecategoryofBayesianlensesinC tobethecategoryofGrothendieckStat-lenses.
Definition3.7. ThecategoryBayesLens ofBayesianlensesinCisthecategoryGrLens ofGrothendieck
C Stat
lensesforthefunctorStat. ABayesianlensisamorphisminBayesLens . WherethecategoryC isevident
C
fromthecontext,wewilljustwriteBayesLens.
Unpackingthisdefinition,wefindthattheobjectsofBayesLens arepairspX,AqofobjectsofC. Mor-
C
phisms(thatis,Bayesianlenses)pX,Aq ÑÞ pY,Bqarepairspc,c:qofachannelc : XÑ‚ Y anda“generalized
Bayesianinversion”c: : BÝ X Ñ‚ A;thatis,elementsofthehomobjects
` ˘ ` ˘
BayesLens pX,Aq,pY,Bq : “ GrLens pX,Aq,pY,Bq
C Stat ` ˘
– CpX,YqˆV CpI,Xq,CpB,Aq .
The identity Bayesian lens on pX,Aq is pid ,id q, where by abuse of notation id : CpI,Yq Ñ CpA,Aq is
X A A
theconstantmapid definedinEquation(4)thattakesanystateonY totheidentityonA.
A
Thesequentialcompositepd,d:q pc,c:qofpc,c:q : pX,Aq ÑÞ pY,Bqandpd,d:q : pY,Bq ÑÞ pZ,Cqisthe
` ˘
Bayesianlens pd‚cq,pc: ˝c˚d:q (cid:11) : pX,Aq ÑÞ pZ,Cqwherepc: ˝c˚d:q : CÝ X Ñ‚ Atakesastateπ : IÑ‚ X to
thechannelc:
π
‚d:
c‚π
: CÑ‚ A.
Definition 3.8. Given a Bayesian lens pc,c1q : pX,Aq ÑÞ pY,Bq, we will call c its forwards or prediction
channelandc1 itsbackwardsorupdatechannel(eventhoughc1 isreallyafamilyofchannels).
Remark3.9. NotethatthedefinitionofStatandhencethedefinitionofBayesLens donotrequireCtobe
C
acopy-deletecategory,eventhoughourmotivatingcategoriesofstochasticchannelsare;allthatisrequired
forthedefinitionisthatC ismonoidal.
Remark3.10. Ontheotherhand,thestructureofC mightbestrongerthanmerelymonoidal. Forinstance,
whenC isCartesianclosed,thenwecantakeV “ C andthemonoidalstructuretobethecategoricalproduct
pˆ,1q. ThenaBayesianlenspX,Aq ÑÞ pY,BqisequivalentlygivenbyapairofaforwardsmapX Ñ Y and
abackwardsmapXˆB Ñ A. WecallsuchlensesCartesian,andtheycharacterizetheoriginal‘lens’notion;
seeRemark3.16.
` ˘
Proposition 3.11. BayesLens is a monoidal category, with structure pb,pI,Iq inherited from C. On
C
objects, define pA,A1q b pB,B1q :“ pA b A1,B b B1q. On morphisms pf,f:q : pX,Aq ÑÞ pY,Bq and
14
pg,g:q : pX1,A1q ÑÞ pY1,B1q,definepf,f:qbpg,g:q :“ pfbg,f:bg:q,wheref:bg: : BbB1Ý X ÝÝ b ‚Ý X Ñ 1 AbA1
acts on states ω : IÑ‚ X bX1 to return the channel f
ω
:
X
bg
ω
:
1
, following the definition of the laxator µ in
X
Proposition 3.4. The monoidal unit in BayesLens is the pair pI,Iq duplicating the unit in C. When C is
C
moreoversymmetricmonoidal,soisBayesLens .
C
Proofsketch. ThemainresultisimmediatefromProposition3.4andCorollary2.40. Whenbissymmetricin
C,thesymmetryliftstothefibresofStatandhencetoBayesLens .
C
Remark 3.12. Although BayesLens is a monoidal category, it does not inherit a copy-delete structure
C
from C, owing to the bidirectionality of its component morphisms. To see this, we can consider morphisms
into the monoidal unit pI,Iq, and find that there is generally no canonical discarding map. For instance, a
morphismpX,Aq ÑÞ pI,IqconsistsinapairofachannelXÑ‚ I(whichmayindeedbeadiscardingmap)anda
state-dependentchannelIÝ X Ñ‚ A,forwhichthereisgenerallynosuitablechoicesatisfyingthecomonoidlaws.
Note,however,thatalensofthetypepX,Iq ÑÞ pI,Bqmightindeedactbydiscarding,sincewecanchoose
theconsta
ş
ntstate-dependentchannelBÝ X Ñ‚ I onthediscardingmap : BÑ‚ I. Byc
ş
ontrast,theGrothendieck
category Stat is a copy-delete category, as the morphisms pX,Aq Ñ pI,Iq in Stat are pairs XÑ‚ I and
AÝ X Ñ‚ I,andsoforbothcomponentswecanchoosemorphismswitnessingthecomonoidstructure.
3.3. Bayesianupdatescomposeoptically
In this section we prove the fundamental result on which the development of statistical games rests: that
theinversionofacompositechannelisgivenuptoalmost-equalitybythelenscompositeofthebackwards
componentsoftheassociated‘exact’Bayesianlenses.
Definition 3.13. Let pc,c:q : pX,Xq ÑÞ pY,Yq be a Bayesian lens. We say that pc,c:q is exact if c admits
Bayesianinversionand,foreachπ : IÑ‚ X suchthatc‚π hasnon-emptysupport,candc:
π
togethersatisfy
equation(3). Bayesianlensesthatarenotexactaresaidtobeapproximate.
Theorem3.14. Letpc,c:qandpd,d:qbesequentiallycomposableexactBayesianlenses.Thenthecontravari-
antcomponent ofthecomposite lenspd,d:q pc,c:q “ pd‚c,c: ˝c˚d:qis, up tod‚c‚π-almost-equality,
theBayesianinversionofd‚cwithrespectto(cid:11)anystateπ onthedomainofcsuchthatc‚π hasnon-empty
support. Thatistosay,Bayesianupdatescomposeoptically: pd‚cq : d‚ „ c‚π c: ‚d: .
π π c‚π
Proof. Supposec:
π
: YÑ‚ X istheBayesianinverseofc : XÑ‚ Y withrespecttoπ : IÑ‚ X. Supposealsothat
d:
c‚π
: ZÑ‚ Y istheBayesianinverseofd : YÑ‚ X withrespecttoc‚π : IÑ‚ Y,andthatpd‚cq :
π
: ZÑ‚ X is
theBayesianinverseofd‚c : XÑ‚ Z withrespecttoπ : IÑ‚ X:
Y Z Y Z X Z X Z
d
d d: pd‚cq:
c‚π c π
“ and “
d d
c
c c
π π π π
15
ThelenscompositeoftheseBayesianinverseshastheformc:
π
‚d:
c‚π
: ZÑ‚ X,sotoestablishtheresultit
sufficestoshowthat
X Z X Z
c: d
π
d: c
c‚π
“ .
d
c
π π
Wehave
X Z X Z X Z
c: c: d
π π
d: d c
c‚π
“ “
d
c c
π π π
wherethefirstobtainsbecaused: istheBayesianinverseofdwithrespecttoc‚π,andthesecondbecause
c‚π
c: istheBayesianinverseofcwithrespecttoπ. Hence,c: ‚d: andpd‚cq : arebothBayesianinversionsof
π π c‚π π
d‚cwithrespecttoπ.SinceBayesianinversionsarealmost-equal(Prop.2.34),wehavec: ‚d: d‚ „ c‚π pd‚cq :,
π c‚π π
asrequired.
Remark 3.15. Note that, in the context of finitely-supported probability (e.g., in K(cid:96)pDq, where D is the
finitely-supported probability distribution monad), almost-equality coincides with simple equality, and so
Bayesianinversionsarethenjustequal.
Remark 3.16. Lenses were originally studied in the context of database systems [3], where one thinks of
theforwardchannelas‘viewing’arecordinadatabase,andthebackwardchannelas‘updating’arecordby
takingarecordandanewpieceofdataandreturningtheupdatedrecord. Inthiscontext,lenseshaveoften
beensubjecttoadditionalaxiomscharacterizingwell-behavedness;forexample,thatupdatingarecordwith
some data is idempotent (the ‘put-put’ law). Bayesian lenses do not in general satisfy these laws, and nor
even do exact Bayesian lenses. This is because Bayesian updating mixes information in the prior state (the
‘record’)withtheobservation(the‘data’),ratherthanreplacingthepriorinformationoutright. Wereferthe
readertoourpreprint[6,§6]foramoredetaileddiscussionofthissituation.
3.4. ParameterizedBayesianlenses
Bayesianlensesforwhichthecomponentchannelsareequippedwithparameterswillplayanimportantrole
in certain applications: an example which we will meet in the next section is the variational autoencoder,
a neural network architecture originally developed for machine learning and which is well described by a
particular class of parameterized statistical games. Since BayesLens is a monoidal category, it induces a
C
self-parameterization ParapBayesLens q by Proposition 2.45, which is sufficient for the purposes of this
C
paper. Therefore,inthissection,wesummarizetheresultingstructureforlaterreference.
16
0-cellsand1-cells The0-cellsofthebicategoryParapBayesLens qarepairspX,AqofobjectsinC. The
C
1-cells pc,c:q : pX,AqÝ p Ý Ω Ý ,Θ Ñ q pY,Bq are Bayesian lenses pc,c:q : pΩbX,ΘbAq ÑÞ pY,Bq. The forwards
|
componentcisachannelΩbXÑ‚ Y inC, andhencealsoaparameterizedchannelXÝÑ Ω ‚ Y inParapCq; we
oftenthinkofthischannelasrepresentingasystem’smodeloftheprocessbywhichobservations(oftypeY)
aregeneratedfromcauses(oftypeX),withtheparameters(oftypeΩ)representingthesystem’sbeliefsabout
thestructureofthisgenerativeprocess.
Conversely,thebackwardscomponentc: isastate-dependentchannelBÝ Ω Ý b ‚Ý X ÑΘbA,whichmeansaV-
morphism CpI,Ω b Xq Ñ CpB,Θ b Aq. This is a generalized Bayesian inversion which takes a state (or
‘prior’belief)jointlyoverparametersandcausesΘbX andreturnsachannelBÑ‚ ΘbAthatwethinkofas
taking an observation (of possibly different type B) and returning an updated joint belief about parameters
andcauses(ofpossiblydifferenttypesΘandA). Notethatthis‘update’channelisnotitselfparameterized:
theparameterizationoftheinversionismediatedthroughΩ,withΘbeingthetypeofupdated parameters.
Sequential composition Given parameterized Bayesian lenses pc,c:q : pX,AqÝ p Ý Ω Ý ,Θ Ñ q pY,Bq and pd,d:q :
|
pY,BqÝ p Ý Ω Ý 1,Θ ÝÑ 1q pZ,Cq,theircompositepd,d:q pc,c:qisdefinedasthefollowingmorphisminBayesLens :
| C
(cid:11)
pΩ1bΩbX,Θ1bΘbAqÝ p Ý id Ý ,i Ý dq Ý b Ý p Ý c, Ý c: Ñ q pΩ1bY,Θ1bBqÝ p Ý d, Ý d: Ñ q pZ,Cq.
| |
Parallel composition Given parameterized Bayesian lenses pc,c:q : pX,AqÝ p Ý Ω Ý ,Θ Ñ q pY,Bq and pd,d:q :
|
pX1,A1qÝ p Ý Ω Ý 1,Θ ÝÑ 1q pY1,B1q,theirtensorpc,c:qbpd,d:qisdefinedasthefollowingmorphisminBayesLens :
| C
´ ¯ ´ ¯
˜ ¸ ˜ ¸ ˜ ¸
c d
ΩbΩ1bX bX1 „ ΩbX bΩ1bX1 c: b d: Y bY1
ÝÑ ÝÝÝÝÝÝÝÝÝÑ
ΘbΘ1bAbA1 | ΘbAbΘ1bA1 | BbB1
´ ¯
wherewehavewrittenthepairsvertically,sothat X :“ pX,Aq.
A
Reparameterization Given 1-cells pf,f:q : pX,AqÝ p Ý Ω Ý ,Θ Ñ q pY,Bq and pg,g:q : pX,AqÝ p Ý Ω Ý 1,Θ ÝÑ 1q pY,Bq, the
| |
2-cells α : pf,f:q ñ pg,g:q of ParapBayesLens q are Bayesian lenses α : pΩ,Θq ÑÞ pΩ1,Θ1q such that
C
pf,f:q “ pg,g:q pαbid q. Wecanthinkofthe2-cellsasreparameterizations,orhigher-orderprocesses
pX,Aq
thatpredictthep(cid:11)arametersonthebasisofyetmoreabstractdata.
Higherstructure TheParaconstructionturnsa(monoidal)categoryintoa(monoidal)bicategory,thereby
“addingadimension”. Wecanconsidermorphismsinaparameterizedcategory(aswellasmorphismsinStat;
seeRemark3.3)asprocessesbywhichprocessesarechosen,andreparameterizationswitnessthefactorization
of these choice processes. In many cybernetic and statistical situations, it is of interest to consider adding
more than just one extra dimension: that is, we may be interested in the processes by which these choice
processesarechosen,and,asintheBayesiansetting,wemaybeinterestedinimprovingtheperformanceof
these‘meta-processes’.Moreconcretely,instatistics,wemaywishtodescribemeta-learningalgorithms(such
as “learning to learn”), or in neuroscience, we may wish to describe how neuromodulation affects synaptic
plasticity(whichinturnaffectsthegenerationofactionpotentials).
SinceParapBayesLens qisitselfmonoidal,onehasafurtherparameterizationParapParapBayesLens qq,
C C
and indeed Para has a monad structure whose multiplication collapses a doubly-parameterized morphism
toasingly-parameterizedone[8,Prop. 3]. However,thefullstructurethatemergeswheniteratingthiscon-
structionadinfinitumisnotyetwellunderstood3. Asimilarstructureappearswhenoneconsidersarbitrarily
3Atpresent,webelievetheresultingstructuremayhaveanopetopicshape,andconstitutesomethinglikean‘8-fibration’.
17
‘nested’dynamicalsystems(suchascellsinanorganism,organismsinsocieties,andsocietiesinecosystems;
or the ownership structure of a complex modern economy); see St. Clere Smithe [25, Remark 2.3]. Such
cyberneticsystemswillbetreatedinafuturechapterofthepresentseries.
4. Statistical games
TheBayesianlensesofTheorem3.14areexact,butmostphysicallyrealisticcyberneticsystemsdonotcom-
puteexactinversions: theinversionofachannelcwithrespecttoapriorπgenerallyinvolvesevaluatingthe
compositec‚π,whichistypicallycomputationallycostly;consequently,realisticsystemstypicallyinstanti-
ate approximate Bayesian lenses. Fortunately, as a consequence of Theorem 3.14, an approximate inversion
ofacompositechannelwillbeapproximatelyequaltothislenscomposite;conversely,thelenscompositeof
approximateinversionswillbeapproximatelyequaltotheexactinversionofthecorrespondingcomposite.
We can thus approximate the inversion of a composite channel by the lens composite of approximations
to the inversions of the components. But what do we mean by “approximate”? There is often substantial
freedom in the choice of approximation scheme—often manifest as some form of parameterization—and in
typicalsituations,the‘fitness’ofaparticularschemewillbecontext-dependent. WethinkofBayesianlenses
asrepresenting‘open’statisticalsystems,ininteractionwithsomeenvironmentor‘context’,andsothefitness
ofaparticularlensthendependsnotonlyontheconfigurationofthesystem(i.e.,thechoiceoflens),butalso
onthesuitabilityofitsconfigurationfortheenvironment.
Inthissection,weintroducestatisticalgamesinordertoquantifythiscontext-dependentfitness:astatisti-
calgamewillbeaBayesianlenspairedwithacontextualfitnessfunction. Thefitnessfunctionmeasureshow
well the lens performs in each context, and the “aim of the game” is then to choose the lens or context (de-
pendingonyourperspective)thatsomehowoptimizesthefitness. Inordertodefinestatisticalgames,wefirst
thereforedefineournotionofcontext, followingcompositionalgametheory[2, 7]: acontextis“everything
requiredtomakeanopensystemclosed”. Inthenextsection,weexemplifystatisticalgamesbyformalizing
anumberofclassicandnot-so-classicproblemsinstatistics.
4.1. ContextsforBayesianlenses
Our first step is to define the notion of simple context, with which we will be able to “close off” a lens with
respecttosequentialcomposition ,andtherebydefineourcategoriesofstatisticalgames.
(cid:11)
Definition4.1. Asimplecontext foraBayesianlensoverC isanelementoftheV-profunctorBayesLens
C
definedby
BayesLens ˆ BayesLensop Ñ V
C C
´ ˆ “ ÞÑ BayesLens ppI,Iq,´qˆBayesLens p“,pI,Iqq.
C C
IfthelensispA,Sq ÑÞ pB,Tq,itsobjectofsimplecontextsis
` ˘ ` ˘
BayesLens pI,Iq,pA,Sq ˆBayesLens pB,Tq,pI,Iq .
C C
Ifwedenotethelensbyf,thenwecandenotethisobjectofsimplecontextsby
` ˘
Ctxpfq :“ BayesLens pA,Sq,pB,Tq .
C
When the monoidal unit I is terminal in C, or equivalently when C is semicartesian, the object of simple
contextsacquiresasimplified(andintuitive)form.
18
Proposition4.2. WhenI isterminalinC,wehave
` ˘ ` ˘
BayesLens pX,Aq,pY,Bq – CpI,XqˆV CpI,Bq,CpI,Yq .
C
Proof. Astraightforwardcalculationwhichweomit: usecausality(Definition2.24).
Remark 4.3. Proposition 4.2 means that, when C is a Markov category (such as K(cid:96)pGq), a context for a
Bayesianlensconsistsofa‘prior’onthedomainoftheforwardschannelanda‘continuation’:aV-morphism
(such as a function) which takes the output of the forwards channel (possibly a ‘prediction’) and returns an
observationfortheupdatemap. Wethinkofthecontinuationasencodingtheresponseoftheenvironment
giventheprediction.
In order to define the sequential composition of statitistical games, we will need to construct, from the
context for a composite lens, contexts for each factor of the composite. The functoriality of BayesLens
C
guaranteestheexistenceofsuchlocalcontexts.
Definition 4.4. Given another lens g : pB,Tq ÑÞ pC,Uq and a simple context for their composite g f :
pA,Sq ÑÞ pC,Yq,thenwecanobtaina1-local contextforf bytheactionoftheprofunctor (cid:11)
` ˘ ` ˘ ` ˘
BayesLens pA,Sq,g : BayesLens pA,Sq,pC,Uq Ñ BayesLens pA,Sq,pB,Tq
C C C
andwecanobtainasimplecontextforg similarly:
` ˘ ` ˘ ` ˘
BayesLens f,pC,Uq : BayesLens pA,Sq,pC,Uq Ñ BayesLens pB,Tq,pC,Uq .
C C C
Notethatwecanwriteg˚ : Ctxpg fq Ñ Ctxpfqfortheformeractionandf : Ctxpg fq Ñ Ctxpgqforthe
˚
latter,sinceg˚ actsbyprecomposi(cid:11)tion(pullback)andf actsbypostcomposition(pus(cid:11)hforwards). Notealso
˚
that,givenh g f,wehaveh˚˝f “ f ˝h˚ : Ctxph g fq Ñ Ctxpgqbytheassociativityofcomposition.
˚ ˚
(cid:11) (cid:11) (cid:11) (cid:11)
Remark 4.5. The local contexts of the preceding definition are local with respect to sequential composi-
tion of lenses. If we view the monoidal category BayesLens as a one-object bicategory, then sequential
C
compositioniscompositionof1-cells,whichexplainstheirformalnamingas1-local contexts.
ToliftthemonoidalstructureofBayesLens toourcategoriesofstatisticalgames,wewillsimilarlyneed
C
‘2-local’contexts: fromtheone-objectbicategoryperspective, thesearelocalcontextswithrespectto2-cell
composition. Byanalogywiththe1-localcase,thismeansexhibitingmapsoftheformCtxpfbf1q Ñ Ctxpfq
andCtxpf bf1q Ñ Ctxpf1qwhichgivethe‘left’and‘right’localcontextsforaparallelpairoflenses.
WithoutrequiringextrastructurefromC orBayesLens 4,wefirstneedtopassfromsimplexcontextsto
C
complex ones: a2-localcontextshouldallowforinformationtopassalongsidealens,throughaprocessthat
is parallel to it. Formally, we adjoin an object to the domain and codomain of the context, and quotient by
therulethat,ifthereisanyprocessthat‘fillsthehole’representedbytheadjoinedobjects,thenweconsider
theobjectsequivalent: thisallowsustoforgetaboutthecontextuallyparallelprocesses,andkeeptrackonly
ofthetypeofinformationthatflows. Suchadjoining-and-quotientinggivesusthefollowingcoend5 formula
definingcomplexcontexts.
4AnalternativeistoaskforBayesLens tobeequippedwithanaturalfamilyof‘discarding’morphismspA,SqÑpI,Iq. This
C
inturnmeansaskingforcanonicalstatesI ÑS,whichingeneralwedonothave:ifweareworkingwithstochasticchannels,
wecouldobtainsuchcanonicalstatesbyallowingthechannelstoemitsubdistributions,andlettingthecanonicalstatesbegiven
bythosewhichassign0densityeverywhere.Buteventhoughwecanmakethetypescheckthisway,thesemanticsarenotquite
whatwewant:rather,weseektoallowinformationto“passinparallel”.
5Formoreinformationabout(co)endsand(co)endcalculus,wereferthereadertoLoregian[26].
19
Definition4.6. Wedefineacomplexcontext tobeanelementoftheprofunctor
ż
` ˘ pM,Nq:BayesLens ` ˘
Č C
BayesLens pA,Sq,pB,Tq :“ BayesLens pM,NqbpA,Sq,pM,NqbpB,Tq .
C C
` ˘
Iff : pA,Sq ÑÞ pB,Tqisalens,thenwewillwriteCtxpfq :“ Bay Č esLens pA,Sq,pB,Tq . Asinthecase
C
ofsimplecontexts,wehave‘projection’mapsg˚ : Ctxpg fq Ñ Ctxpfqandf : Ctxpg fq Ñ Ctxpgq;these
˚
aredefinedsimilarly. Wewillcalltheadjoinedobject,her(cid:11)edenotedpM,Nq,theresidual (cid:11). Thereisacanonical
inclusionCtxpfq ãÑ CtxpfqgivenbyadjoiningthetrivialresidualpI,Iqtoeachsimplecontext.
WeimmediatelyhavethefollowingcorollaryofProposition4.2:
Corollary4.7. WhenI isterminalinC,
ż
` ˘ pM,Nq ` ˘
Č
BayesLens pA,Sq,pB,Tq – CpI,M bAqˆV CpI,M bBq,CpI,N bTq .
C
Remark4.8. Sincethecoenddenotesaquotient,itselementsareequivalenceclasses. Theprecedingcorol-
lary says that, when I is terminal, an equivalence class of contexts is represented by a choice of residual
pM,Nq,aprioronM bAinC,andacontinuationCpI,M bBq Ñ CpI,N bTqinV.
Of course, when I is not terminal, then the definition says that a general complex context for a lens
pA,Sq ÑÞ pB,Tq is an equivalence class represented by: as before, a choice of residual pM,Nq, a prior
on M b A, and a continuation CpI,M b Bq Ñ CpI,N b Tq; as well as an ‘effect’ M b BÑ‚ I in C, and
whatwemightcalla‘vector’CpI,Iq Ñ CpN bS,IqinV. Theeffectandvectormeasuretheenvironment’s
‘internalresponse’tothelens’outputs(asopposedtorepresentingtheenvironment’sfeedbacktothelens).
Usingcomplexcontexts,itiseasytodefinethe‘projections’thatgive2-localcontexts.
Definition4.9. Givenlensesf : Φ ÑÞ Ψandf1 : Φ1 ÑÞ Ψ1andacomplexcontextfortheirtensorfbf1,the
left2-localcontext isthecomplexcontextforf givenby
` ˘
π : Ctxpf bf1q “ Bay Č esLens ΦbΦ1,ΨbΨ1
f C
ż
Θ:BayesLens ` ˘
C
“ BayesLens ΘbΦbΦ1,ΘbΨbΨ1
C
ż
Θ:BayesLens ` ˘
ÝÑ „ C BayesLens ΘbΦ1bΦ,ΘbΨ1bΨ
C
ż
Θ:BayesLens ` ˘ ` ˘
C
“ BayesLens pI,Iq,ΘbΦ1bΦ ˆBayesLens ΘbΨ1bΨ,pI,Iq
C C
ż
Θ:BayesLens ` ˘ ` ˘
Ý f Ñ 1 C BayesLens pI,Iq,ΘbΨ1bΦ ˆBayesLens ΘbΨ1bΨ,pI,Iq
C C
ż
Θ:BayesLens ` ˘
C
“ BayesLens ΘbΨ1bΦ,ΘbΨ1bΨ
C
ż
Θ1:BayesLens ` ˘
C
ãÑ BayesLens Θ1bΦ,Θ1bΨ “ Ctxpfq.
C
Theequalitiesherearejustgivenbyexpandingandcontractingdefinitions; theisomorphismusesthesym-
metryofbinC;thearrowmarkedf1isgivenbycomposingid bf1bid afterthe‘prior’partofthecontext;
Θ Φ
andtheinclusionisgivenbycollectingthetensorofΘandΨ1togetherintotheresidual.Thesestepsformalize
theideaoffillingtheright-handholeofthecomplexcontextwithf1 toobtainalocalcontextforf.
20
Theright2-localcontext isthecomplexcontextforf1 obtainedsimilarly:
` ˘
π : Ctxpf bf1q “ Bay Č esLens ΦbΦ1,ΨbΨ1
f1 C
ż
Θ:BayesLens ` ˘
C
“ BayesLens ΘbΦbΦ1,ΘbΨbΨ1
C
ż
Θ:BayesLens ` ˘ ` ˘
C
“ BayesLens pI,Iq,ΘbΦbΦ1 ˆBayesLens ΘbΨbΨ1,pI,Iq
C C
ż
Θ:BayesLens ` ˘ ` ˘
ÝÑ f C BayesLens pI,Iq,ΘbΨbΦ1 ˆBayesLens ΘbΨbΨ1,pI,Iq
C C
ż
Θ:BayesLens ` ˘
C
“ BayesLens ΘbΨbΦ1,ΘbΨbΨ1
C
ż
Θ1:BayesLens ` ˘
C
ãÑ BayesLens Θ1bΦ1,Θ1bΨ1 “ Ctxpf1q
C
Noteherethatwedonotneedtousethesymmetry,asboththeresidualandthe‘hole’(filledwithf)areon
theleftofthetensor. (Strictly,wedonotrequiresymmetryofbforπ either,onlyabraiding.)
f
Remark4.10. Itispossibletomaketheintuitionof“fillingtheleftandrightholes”moreimmediatelyprecise,
at the cost of introducing another language, by rendering the 2-local context functions in the graphical cal-
culusofthemonoidalbicategoryofV-profunctors. Wedemonstratehowthisworks,makingthehole-filling
explicit,inAppendixA.
Henceforth,whenwesay‘context’,wewillmean‘complexcontext’.
4.2. Monoidalcategoriesofstatisticalgames
We are now in a position to define monoidal categories of statistical games over C. In typical examples, the
fitness functions will be valued in the real numbers, but this is not necessary for the categorical definition;
instead,weallowthefitnessfunctionstotakevaluesinanarbitrarymonoid.
Proposition4.11. LetC beaV-categoryadmittingBayesianinversionandletpR,`,0qbeamonoidinV.
Then there is a category SGame whose objects are the objects of BayesLens and whose morphisms
R C ` C ˘
pX,Aq Ñ pY,Bqarestatisticalgames: pairspf,φqofalensf : BayesLens pX,Aq,pY,Bq andafitness
C
functionφ : Ctxpfq Ñ R. WhenRisthemonoidofrealsR,thenwejustdenotethecategoryby SGame .
C
Proof. Suppose given statistical games pf,φq : pX,Aq Ñ pY,Bq and pg,ψq : pY,Bq Ñ pZ,Cq. We seek a
composite game pg,ψq˝pf,φq :“ pgf,ψφq : pX,Aq Ñ pZ,Cq. We have gf “ g f by lens composition.
Thecompositefitnessfunctionψφisobtainedusingthe1-localcontextsby (cid:11)
ψφ :“ Ctxpg fq ÝÑ Ctxpg fqˆCtxpg fq Ý p Ý g˚ Ý ,f Ý˚Ñ q CtxpfqˆCtxpgq Ý p Ý φ Ý ,ψ Ñ q RˆR ÝÑ ` R
(cid:11) (cid:11) (cid:11)
The identity game pX,Aq Ñ pX,Aq is given by pid,0q, the pairing of the identity lens on pX,Aq with the
unit 0 of the monoid R. Associativity and unitality follow from those properties of lens composition, the
coassociativityofcopying inV,andthemonoidlawsofR.
Definition4.12. WewillwriteSimpSGame ãÑ SGame forthefullsubcategoryof SGame defined
C C C
on simple Bayesian lenses pX,Xq ÑÞ pY,Yq. As the case of simple lenses (Definition 2.37), we will eschew
redundancybywritingtheobjectspX,XqsimplyasX.
21
` ˘ ` ˘
Proposition4.13. SGame inheritsamonoidalstructure b,pI,Iq from BayesLens ,b,pI,Iq .When
R C C
pC,b,IqisfurthermoresymmetricmonoidalandpR,`,0qisacommutativemonoid,thenthemonoidalstruc-
tureon SGame issymmetric.
R C
Proof. The structure on objects and on the lens components of games is defined as in Proposition 3.11. On
fitness functions, we use the monoidal structure of R: given games pf,φq : pX,Aq Ñ pY,Bq and pf1,φ1q :
pX1,A1q Ñ pY1,B1q,wedefinethefitnessfunctionφbφ1 oftheirtensorpf,φqbpf1,φ1qasthecomposite
φbφ1 :“ Ctxpf bf1q ÝÑ Ctxpf bf1qˆCtxpf bf1q Ý p Ý π Ý f , Ý π ÝfÑ 1q CtxpfqˆCtxpf1q Ý p Ý φ, Ý φ Ñ 1q RˆR ÝÑ ` R
Thatis,weformtheleftandright2-localcontexts,computethelocalfitnesses,andcomposethemusingthe
monoidaloperationinR. UnitalityandassociativityfollowfromthatofbinBayesLens and`inR.
C
Corollary4.14. Sinceeachcategory SGame isthusmonoidal,weobtaincategoriesParap SGame q
R C R C
of parameterized statistical games by Proposition 2.45, which are themselves monoidal. The structure is as
describedforBayesianlensesinSection3.4,withsomeminoradditionstoincorporatethefitnessfunctions,
thedetailsofwhichweleavetothereader.
Remark 4.15 (Parameters as strategies).´ A par ¯ amete ´ rize ¯ d statistical game of type pX,Aq Ý p Ý Ω Ý ,Θ Ñ q pY,Bq in
Parap SGame q is a statistical game ΩbX Ñ Y in SGame ; that is a pair of a Bayesian lens
´ ¯R ´ ¯C ΘbA B´´ R¯ ´ ¯¯C
ΩbX ÑÞ Y and a fitness function Bay Č esLens ΩbX , Y Ñ R in V. If we fix a choice of
ΘbA B C ΘbA B
parameter ω : Θ and discard the updated parameters in Θ—that is, if we reparameterize along the 2-cell
inducedbythelenspω, q : pI,Iq ÑÞ pΩ,Θq—thenweobtainanunparameterizedstatisticalgamepX,Aq Ñ
pY,Bq. In this way, we can think of the parameters of a parameterized statistical game as the strategies by
whichthegameistobeplayed: eachparameterω : ΩpicksoutaBayesianlens,whoseforwardschannelwe
think of as a model by which the system predicts its observations and whose backwards channel describes
howthesystemupdatesitsbeliefs. And,ifwedon’tjustdiscardthem,thentheseupdatedbeliefsmayinclude
updatedparameters(ofapossiblydifferenttypeΘ). Asuccessfulstrategy(agoodchoiceofparameter)fora
statistical game is then one which optimizes the fitness function in the contexts of relevance to the system:
wecanthinkofthese“relevantcontexts”assomethinglikethesystem’secologicalniche.
Remark4.16(Multi-playergames). Inalaterinstalmentofthisseriesofpapers,wewillseehowtocompose
thestatisticalgamesofmultipleinteractingagents,sothatthegame-playingmetaphorbecomesmorevisceral:
theobservationspredictedbyeachsystemwillthenbegeneratedbyothersystems,witheachplayingagame
ofoptimalprediction.Inthispaper,however,weusuallythinkofeachstatisticalgameasrepresentingasingle
system’s model of its environment (its context), even where the games at hand are themselves sequentially
orparallellycomposite. Thatistosay,ourgamesherearefundamentally‘two-player’,withthetwoplayers
beingthesystemandthecontext.
Remark 4.17. Both in the case of sequential of parallel composition of statistical games, the local fitnesses
arecomputedindependentlyandthensummed. Ifafitnesssfunctiondependssomehowontheresidual,this
mightleadto‘double-counting’thefitnessofanyoverlappingfactorsoftheresidual. Forourpurposes,this
assumption of ‘independent fitness’ will suffice, and so we leave the question of gluing together correlated
fitnessfunctionsforfuturework.
5. Examples
Inthissection,wedescribehowanumberofcommonconceptsinstatisticsandparticularlystatisticalinfer-
ence fit into the framework of statistical games. We begin with the simple example of maximum likelihood
22
estimationandprogressivelygeneralizetoinclude‘variational’[27]methodssuchasthevariationalautoen-
coder[28]andgeneralizedvariationalinference[29]. Alongtheway,weintroducetheconceptsoffreeenergy
andevidenceupperbound.Wedonothereconsiderthealgorithmsbywhichstatisticalgamesmaybeplayedor
optimized;thatisamatterforasubsequentpaperinthisseries. Instead,weseestatisticalgamesasproviding
an‘algebra’forthecompositionalconstructionofinferenceproblems.
Remark 5.1 (The role of fitness functions). Before we introduce our first example, we note that the games
hereareclassifiedbytheirfitnessfunctions,withthechoiceoflensbeingsomewhatincidentaltotheclassifi-
cation6. WenotefurthermorethatourfitnessfunctionswilltendtobeoftheformE rfs,wherepπ,kqis
k‚c‚π
acontextforalens,cisachannel,andf isanappropriatelytypedeffect7. Thisformhintsattheexistenceof
a compositional treatment of fitness functions, which seem roughly to be something like “lens functionals”.
Weleavesuchatreatment,anditsconnectiontoRemark4.17,tofuturework.
Wefirststudytheclassicproblemofmaximumlikelihoodestimation,beginningbyestablishinganauxiliary
resultsaboutcontexts.
Proposition5.2. LetIdenotethemonoidalunitpI,IqinBayesLens ,andletl : I ÑÞ Ψbealens. Then
C
ż
Θ:BayesLens
C
Ctxplq “ BayesLens pI,ΘbIqˆBayesLens pΘbΨ,Iq
C C
ż
Θ:BayesLens
C
– BayesLens pI,ΘqˆBayesLens pΘbΨ,Iq
C C
– BayesLens pIbΨ,Iq
C
– BayesLens pΨ,Iq.
C
` ˘
SupposeΨ “ pA,Sq. ThenCtxplq “ CpA,IqˆV CpI,Aq,CpI,Sq bythedefinitionofBayesLens .
C
Proof. The first and third isomorphisms hold by unitality of b; the second holds by the Yoneda lemma (see
Loregian[26,Prop. 2.2.1]fortheargument).
Example5.3(Maximumlikelihood). WhenI isterminalinC,aBayesianlensoftheformpI,Iq ÑÞ pX,Xq
is determined by its forwards channel, which is simply a state π : IÑ‚ X. Following Proposition 5.2, and
using that I is terminal, a context for such a lens is given simply by a continuation k : CpI,Xq Ñ CpI,Xq
taking states on X to states on X. A maximum likelihood game is then any statistical game π of the type
pI,Iq Ñ pX,Xq with fitness function φ : Ctxpπq Ñ R given by φpkq “ E rp s, where p is a density
kpπq π π
function for π. More generally, we might consider maximum f-likelihood games for monotone functions
f : R Ñ R,inwhichthefitnessfunctionisgivenbyφpkq “ E rf ˝p s. Atypicalchoicehereisf :“ log.
kpπq π
Inorderthattheremaybesomefreedomtooptimizethefitnessfunction,onetypicallyworksintheparam-
eterizedcategory: theaimofthegameisthentochoosetheoptimalparameterforthecontext,asquantified
by the fitness function. This gives us the notion of parameterized maximum likelihood game; but first, we
definesomesimplifyingnotation.
Notation5.4(Feedback). LetI beterminalinC andconsideraBayesianlensl “ pl ,l1q : pA,Sq ÑÞ pB,Tq,
1
withacontextrepresentedby:aresidualpM ` ,Nq;apriorπ ˘ : IÑ‚ MbA;andacontinuationk : CpI,MbBq Ñ
CpI,N bTq. Write π|l|k todenotek pid bl q‚π wherepid bl q‚π isthemap
M 1 T M 1
(cid:76) (cid:77)
IÝÑ
π
‚ M bAÝ
i
Ý
dMÝ‚Ý b
ÝÑ
l1
M bB
6Thisincidentalityislessenedwhenweconsiderexamplesofparameterizedgames,butevenheretheparameterizationonlyinduces
somethingofa‘sub’-classification;themainclassificationremainsduetothefitnessfunctions.
7Theresulting‘optimization-centric’perspectiveisinlinewiththeaestheticpreferenceof[29],thoughwedonotyetknowwhat
thisalignmentmightsignify;weareinterestedtofindexamplesofadifferentflavour.
23
andwherep´q
T
denotestheprojection(marginalization)ontoT;here,bythechannelN bTÑ‚ T.
Note that π|l|k therefore has the type IÑ‚ T in C: it encodes the environment’s feedback in T to the
lens,givenit(cid:76)soutput(cid:77)inB andthecontext.
Example 5.5 (Parameterized maximum likelihood). A parameterized Bayesian lens pI,IqÝ p Ý Ω Ý ,Θ Ñ q pX,Xq is
|
equivalently a Bayesian lens pΩ,Θq ÑÞ pX,Xq, and hence given by a pair of a channel (or “parameter-
dependent state”) ΩÑ‚ X and a parameter-update XÝÑ Ω ‚ Θ. When I is terminal in C, a context for the lens is
representedbyπ : IÑ‚ MbΩandk : CpI,MbXq Ñ CpI,NbXq.Wethendefineaparameterizedmaximum
f-likelihood game to be a parameterized statistical game of the form l “ pl ,l1q : pI,IqÝ p Ý Ω Ý ,Θ Ñ q pX,Xq with
1 |
fitnessfunctionφ : Ctxpπq Ñ Rgivenbyφpπ,kq “ E rf ˝p s.
π|l|k l‚πΩ
(cid:76) (cid:77)
Here,p : X Ñ r0,8sisadensityfunctionforthecompositechannell‚π . Inapplicationsoneoften
l‚πΩ Ω
fixesasinglechoiceofparametero,withthemarginalstateπ thenbeingaDiracdeltadistributiononthat
Ω
choice. Onethenwritesthedensityfunctionp p´qasp p´|oqorp p´|Ω “ oq.
l‚πΩ l l
Remark 5.6. Recalling that we can think of probability density as a measure of the likelihood of an obser-
vation, we have the intuition that an “optimal strategy” (i.e., an optimal choice of lens or parameter) for a
maximum likeihood game is one that maximizes the likelihood of the state obtained from the context, or in
otherwordsprovidesthe“bestexplanation”ofthedatageneratedbythecontinuation.
Consideringparameterizedmaximumlikelihoodgames,whichareequippedwithparameter-updatemaps,
leadsonetowonderhowtooptimizethis‘inferential’backwardspartofthegame,andnotjustthe‘predictive’
forwardspart. SuchbackwardsoptimizationisapproximateBayesianinference.
Example 5.7 (Bayesian inference). Let D : CpI,XqˆCpI,Xq Ñ R be a measure of divergence between
statesonX. Thena(simple)D-Bayesianinference gameisastatisti ” calg ´ amepc,φq : p¯Xı,Xq Ñ pY,Yqwith
fitness function φ : Ctxpcq Ñ R given by φpπ,kq “ E D c1 pyq,c: pyq , where c “ pc ,c1q
y„ π|c|k π π 1
(cid:76) (cid:77)
constitutesthelenspartofthegameandc: istheexactinversionofc withrespecttoπ.
π 1
NotethatwesaythatDisa“measureofdivergencebetweenstatesonX”. Bythiswemeananyfunctionof
thegiventypewiththesemanticalinterpretationthatitactslikeadistancemeasurebetweenstates. Butthis
isnottosaythatD isametricorevenpseudometric. OneusuallyrequiresthatDpπ,π1q “ 0 ðñ π “ π1,
but typical choices do not also satisfy symmetry nor subadditivity. An important such typical choice is the
relativeentropy orKullback-Leiblerdivergence,denotedD .
KL
Definition5.8. TheKullback-LeiblerdivergenceD : CpI,XqˆCpI,Xq Ñ Risdefinedby
KL
D pα,βq :“ E rlogppxqs´ E rlogqpxqs
KL
x„α x„α
wherepandq aredensityfunctionscorrespondingtothestatesαandβ.
I ´ n many situa ¯ tions, computing the exact inversion c: π pxq is costly, and so is computing the divergence
D c1 pxq,c: pxq .Consequently,approximateinversionschemestypicallyeitherapproximatethedivergence
π π
(asinMonteCarlomethods),ortheyoptimizeanupperboundonit(asinvariationalmethods).Inthissection,
we are interested in different choices of fitness function, rather than the algorithms by which the functions
are exactly or approximately evaluated; hence we here consider the latter ‘variational’ choice, leaving the
formerforfuturework.
One widespread choice is to construct an upper bound on the divergence called the free energy or the
evidenceupperbound.
24
Definition5.9(D-freeenergy). Letpπ,cqbeagenerativemodelwithc : XÑ‚ Y. Letp
c
: Y ˆX Ñ R
`
and
p : X Ñ R bedensityfunctionscorrespondingtocandπ. Letp : Y Ñ R beadensityfunctionforthe
π ` c‚π `
compositec‚π. Letc1
π
beachannelYÑ‚ X thatwetaketobeanapproximationoftheBayesianinversionofc
withrespecttoπandthatadmitsadensityfunctionq : XˆY Ñ R . Finally,letD : CpI,XqˆCpI,Xq Ñ R
`
beameasureofdivergencebetweenstatesonX. ThentheD-freeenergyofc1 withrespecttothegenerative
π
modelgivenanobservationy : Y isthequantity
` ˘
F pc1 ,c,π,yq :“ E r´logp py|xqs`D c1 pyq,π . (5)
D π c π
x„c1 pyq
π
Wewillelidethedependenceonthemodelwhenitisclearfromthecontext,writingonlyF pyq.
D
TheD-freeenergyisanupperboundonD whenD istherelativeentropyD ,aswenowshow.
KL
Proposition5.10(Evidenceupperbound). TheD -freeenergysatisfiesthefollowingequality:
KL
„ 
“ ‰
qpx|yq
F pyq “ D c1 pyq,c:pyq ´logp pyq “ E log
DKL KL π π c‚π x„c1 π pyq p c py|xq¨p π pxq
” ı
Sincelogp pyqisalwaysnegative, thefreeenergyisanupperboundonD c1 pyq,c: pyq , wherec: is
c‚π KL π π π
the exact Bayesian inversion of the channel c with respect to the prior π. Similarly, the free energy is an
upper bound on the negative log-likelihood ´logp pyq. Thinking of this latter quantity as a measure of
c‚π
the“modelevidence”givesusthealternativenameevidenceupperbound fortheD -freeenergy.
KL
Proof. Letp : Y ˆX Ñ R bethedensityfunctionp py,xq :“ p py|xq¨p pxqcorrespondingtothejoint
ω ` ω c π
distributionofthegenerativemodelpπ,cq. Wehavethefollowingequalities:
´logp pyq “ E r´logp pyqs
c‚π c‚π
x„c1 π pyq« ff
p py,xq
“ E ´log ω (byBayes’rule)
x„c1 π pyq « p c: π px|yq ff
p py,xq qpx|yq
“ E ´log ω
x„c1 π pyq „ qpx|yq  p c: π px|yq
“ ‰
p py,xq
“ ´ E log ω ´D c1 pyq,c:pyq
x„c1 pyq qpx|yq KL π π
π
Definition5.11. WewillcallF thevariationalfreeenergy,orsimplyfreeenergy,anddenoteitbyFwhere
DKL
thiswillnotcauseconfusion. WewilltaketheresultofProposition5.10asadefinitionofthevariationalfree
energy,writing
„ 
qpx|yq
Fpyq “ E log
x„c1 π pyq p c py|xq¨p π pxq
whereeachtermisdefinedasinDefinitions5.9and5.8.
Remark5.12. ThenamefreeenergyisduetoananalogywiththeHelmholtzfreeenergyinthermodynamics,
as,whenD “ D ,wecanwriteitasthedifferencebetweenan(expected)energyandanentropyterm:
KL
„ 
qpx|yq
Fpyq “ E log
x„c1 π pyq p c py|xq¨p π pxq “ ‰
“ E r´logp py|xq´logp pxqs´S c1 pyq
c π X π
x„c1 pyq
π “ ‰ “ ‰
“ E E px,yq ´S c1 pyq “ U ´TS
pπ,cq X π
x„c1 pyq
π
25
wherewecallE : XˆY Ñ R theenergyofthegenerativemodelpπ,cq,andwhereS : CpI,Xq Ñ R
pπ,cq ` X `
istheShannonentropyonX. Thelastequalitymakesthethermodynamicanalogy: U istheinternalenergy
ofthesystem;T “ 1isthetemperature;andS isagaintheentropy.
Havingnowdefinedamoretractablefitnessfunction,wecanconstructstatisticalgamesaccordingly. Since
the free energy is an upper bound on relative entropy, optimizing the former can have the side effect of
optimizingthelatter8. Wecalltheresultinggamesautoencodergames,forreasonsthatwillsoonbeclear.
Example 5.13 (Autoencoder). Let D : CpI,XqˆCpI,Xq Ñ R be a measure of divergence between states
onX. ThenasimpleD-autoencodergame isasimplestatisticalgamepc,φq : pX,Xq Ñ pY,Yqwithfitness
function φ : Ctxpcq Ñ R given by φpπ,kq “ E rF pc1 ,c,π,yqs where c “ pc,c1q : pX,Xq ÑÞ
y„ π|c|k D π
pY,Yqconstitutesthelenspartofthegame. (cid:76) (cid:77)
Onealsoofcoursehasparameterizedversionsoftheautoencodergames.
Example 5.14 (Simply parameterized autoencoder). A simply parameterized D-autoencoder game is a sim-
ple parameterized statistical game pc,πq : pX,Xq Ý p Ý Ω Ý ,Ω Ñ q pY,Yq with the D-autoencoder fitness function
φ : Ctxpcq Ñ R given by φpπ,kq “ E rF pc1 ,c,π,yqs. That is, a simply parameterized D-
y„ π|c|k D π
autoencodergameisjustasimpleD-autoenc(cid:76)oderg(cid:77)amewithtensorproductdomaintype.
Moreofteninapplications,onedoesn’tusethesamebackwardschanneltoupdateboththe“beliefaboutthe
causes”inXandtheparametersinΩsimultaneously.Instead,thebackwardschannelupdatesonlythebeliefs
overX,andanyupdatingoftheparametersislefttoanother‘higher-order’process. TheX-updatechannel
may nonetheless still itself be parameterized in Ω: for instance, if it represents an approximate inference
algorithm, then one often wants to be able to improve the approximation, and such improvement amounts
to a change of parameters. The ‘higher-order’ process that performs the parameter updating is then often
representedasareparameterization: a2-cellinthebicategoryParapSGame q. Thenextexampletellsthe
C
firstpartofthisstory.
Example 5.15 (Parameterized autoencoder). A (simple) parameterized D-autoencoder game is a parameter-
izedstatisticalgamepc,φq : pX,Xq Ý p Ý Ω Ý ,Θ Ñ q pY,Yqwithfitnessfunctionφ : Ctxpcq Ñ Rgivenby
“ ` ˘‰
φpπ,kq “ E F pc1 q ,c|π ,π ,y .
D π X Ω X
y„ π|c|k
(cid:76) (cid:77)
As before, the notation p´q
X
indicates taking the X marginal, along the projection Θ b XÑ‚ X. We also
define c|π :“ c‚pπ b id q, indicating “c given the parameter state π ”. Written out in full, the fitness
Ω Ω X Ω
functionisthereforegivenby
“ ` ˘‰
φpπ,kq “ E F proj ‚c1 ,c‚pπ b id q,proj ‚π,y .
D X π Ω X X
y„ π|c|k
(cid:76) (cid:77)
` ˘
Notethatthepair c|π ,pc1 q definesanunparameterizedsimpleBayesianlenspX,Xq ÑÞ pY,Yq, with
Ω πΩ X
c|π
Ω
: XÑ‚ Y andpc1
πΩ
q
X
: YÝ X Ñ‚ X.
Remark5.16(Meaningof‘autoencoder’). Whydowecallautoencodergamesthus? Thenameoriginatesin
machinelearning,whereonethinksoftheforwardschannelas‘decoding’somelatentstateintoaprediction
8Strictlyspeaking,onecanhaveadecreaseinfreeenergyalongwithanincreaseinrelativeentropy,aslongastheformerremains
greaterthanthelatter. Therefore, optimizingthefreeenergydoesnotnecessarilyoptimizetherelativeentropy. However, as
elaboratedinRemark5.16,thedifferencebetweenthevariationalfreeenergyandtherelativeentropyisthelog-likelihood,so
optimizingthefreeenergycorrespondstosimultaneousmaximum-likelihoodestimationandBayesianinference.
26
of some generated data, and the backwards channel as ‘encoding’ a latent state given an observation of the
data; typically, the latent state space is thought to have lower dimensionality than the observed data space,
justifying the use of this ‘compression’ terminology. A slightly more precise way to see this is to consider
an autoencoder game where the context and forwards channel are fixed. The only free variable available
for optimization in the fitness function is then the backwards channel, and the optimum is obtained when
the backwards channel equals the exact inversion of the forwards channel (given the prior in the context,
and for all elements of the support of the state obtained from the continuation). Conversely, allowing only
the forwards channel to vary, it is easy to see that the autoencoder fitness function is then equal to the
fitnessfunctionofamaximumlog-likelihoodgame(uptoaconstant). Consequently,optimizingthefitnessof
an autoencoder game in general corresponds to performing approximate Bayesian inference and maximum
likelihood estimation simultaneously. The optimal strategy (lens or parameter) can then be considered as
representingan‘optimal’modeloftheprocessbywhichobservationsaregenerated,alongwitharecipefor
invertingthatmodel(andhence‘encoding’thecausesofthedata). Theprefixauto-indicatesthatthismodel
islearntinanunsupervisedmanner,withoutrequiringinputaboutthe‘true’causesoftheobservations.
Someauthors(inparticular,Knoblauchetal.[29])takeavariantoftheD-autoencoderfitnessfunctionto
definea generalizationofBayesian inference: inanecho ofRemark5.16, theintuitionhere isthatBayesian
inference simply is maximum likelihood estimation, except ‘regularized’ by the uncertainty encoded in the
prior,whichstopstheoptimumstrategybeingtriviallygivenbyaDiracdeltadistribution. Byallowingboth
thechoiceoflikelihoodfunctionanddivergencemeasuretovary,oneobtainsafamilyofgeneralizedinference
methods.Moreover,whenoneretainsthestandardchoicesoflog-densityaslikelihoodandrelativeentropyas
divergence,theresultinggeneralizedBayesianinferencegames coincidewithvariationalautoencodergames;
then, when the forwards channel (or its parameter) is fixed, both types of game coincide with the Bayesian
inferencegamesofExample5.7above.
Example 5.17 (Generalized Bayesian inference [29]). Let D : CpI,Xq ˆ CpI,Xq Ñ R be a measure of
divergence between states on X, and let l : Y bXÑ‚ I be any effect on Y bX. Then a simple generalized
pl,Dq-Bayesian inference game is a simple statistical game pc,φq : pX,Xq Ñ pY,Yq with fitness function
φ : Ctxpcq Ñ Rgivenby
„ 
φpπ,kq “ E E rlpy,xqs`Dpc1 pyq,πq
π
y„ π|c|k x„c1 pyq
π
(cid:76) (cid:77)
wherepc,c1q : pX,Xq ÑÞ pY,Yqconstitutesthelenspartofthegame.
Proposition 5.18. Generalized Bayesian inversion and autoencoder games coincide when D “ D and
KL
l “ ´logp ,wherep isadensityfunctionfortheforwardschannelc.
c c
Proof. ConsidertheD -freeenergy. Wehave
KL
` ˘ “ ‰
F c1 ,c,π,y “ E r´logp py|xq´logp pxqs´S c1 pyq byRemark5.12
DKL π
x„c1 pyq
c π X π
π
“ E r´logp py|xqs` E rlogqpx|yq´logp pxqs
c π
x„c1 pyq x„c1 pyq
π π ` ˘
“ E r´logp py|xqs`D c1 pyq,π
c KL π
x„c1 pyq
π ` ˘
“ E rlpy,xqs`D c1 pyq,π
π
x„c1 pyq
π
whereq isadensityfunctionforc1 .
π
27
Unsurprisingly,asintheautoencodercase,thereareparameterizedandsimplyparameterizedvariantsof
generalizedBayesianinferencegames.
Finally, we remark that, in the case where C “ sfKrn, where I is not terminal and morphisms into I
correspond to functions into r0,8s, composing a Bayesian lens and its context gives a lens pI,Iq ÑÞ pI,Iq:
boththeforwardsandbackwardspartsofthis‘I-endolens’returnpositivereals(whichinthiscontextJacobs
andcolleaguescallvalidities [12,30]),andwhichwecanthinkofas“theenvironment’smeasurementsofits
compatibility with the lens”. In this case, we can therefore define validity games, where the fitness function
issimplygivenbycomputingthebackwardsvalidity9. Sincesuchafitnessfunctionmeasurestheinteraction
ofthelenswithitsenvironment,thecorrespondingstatisticalgamesmaybeofrelevanceinmodellingmulti-
agentorotherwiseinteractingstatisticalsystems—forinstance,inmodellingevolutionarydynamics.Weleave
theexplorationofthisforfuturework.
6. References
[1] ChristopherLBuckleyetal.“Thefreeenergyprincipleforactionandperception:Amathematicalre-
view”.In:JournalofMathematicalPsychology81(05/24/2017),pp.55–79.arXiv:http://arxiv.org/abs/1705.09156v1[q-bio.NC].
[2] NeilGhanietal.“Compositionalgametheory”.In:ProceedingsofLogicinComputerScience(LiCS)2018
(2016).arXiv:http://arxiv.org/abs/1603.04641[cs.GT].
[3] AaronBohannon,BenjaminCPierce,andJeffreyAVaughan.“Relationallenses:alanguageforupdat-
ableviews”.In:Proceedingsofthetwenty-fifthACMSIGMOD-SIGACT-SIGARTsymposiumonPrinciples
ofdatabasesystems.ACM.2006,pp.338–347.
[4] BrendanFongandMichaelJohnson.“LensesandLearners”.In:In:J.Cheney,H-S.Ko(eds.):Proceedingsof
theEighthInternationalWorkshoponBidirectionalTransformations(Bx2019),Philadelphia,PA,USA,June
4,2019,publishedathttp://ceur-ws.org(03/05/2019).arXiv:http://arxiv.org/abs/1903.03671v2[cs.LG].
[5] A.M.Bastosetal.“Canonicalmicrocircuitsforpredictivecoding”.In:Neuron76.4(11/2012),pp.695–
711.doi:10.1016/j.neuron.2012.10.038.
[6] TobySt.ClereSmithe.“BayesianUpdatesComposeOptically”.In:(05/31/2020).arXiv:2006.01631v1[math.CT].
[7] JoeBolt,JulesHedges,andPhilippZahn.“Bayesianopengames”.In:(10/08/2019).arXiv:http://arxiv.org/abs/1910.03656v1[cs.GT].
[8] MatteoCapuccietal.“Towardsfoundationsofcategoricalcybernetics”.In:(05/13/2021).arXiv:2105.06332[math.CT].
[9] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Diagrams”. In: Math.
Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). doi: 10.1017/S0960129518000488. arXiv:
http://arxiv.org/abs/1709.00322v3[cs.AI].
[10] SamStaton.“CommutativeSemanticsforProbabilisticProgramming”.In:ProgrammingLanguagesand
Systems.SpringerBerlinHeidelberg,2017,pp.855–879.doi:10.1007/978-3-662-54434-1_32.
[11] TobiasFritz.“AsyntheticapproachtoMarkovkernels,conditionalindependenceandtheoremsonsuffi-
cientstatistics”.In:(08/19/2019).arXiv:http://arxiv.org/abs/1908.07021v3[math.ST].
9Notethatthebackwardseffect,asanI-state-dependenteffect(or‘vector’),alreadydependsupontheforwardsvalidity,sowedo
notneedtoincludetheforwardsvaliditydirectlyinthefitnesscomputation.
28
[12] KentaChoetal.“AnIntroductiontoEffectusTheory”.In:arXivpreprintarXiv:1512.05813(2015).arXiv:
http://arxiv.org/abs/1512.05813v1[cs.LO].
[13] Brendan Fong and David I Spivak. “Supplying bells and whistles in symmetric monoidal categories”.
In:(08/07/2019).arXiv:http://arxiv.org/abs/1908.02633v1[math.CT].
[14] MarttiKarvonen.“TheWayoftheDagger”.In:(04/24/2019).arXiv:1904.10805[math.CT].
[15] J.M.Stoyanov.CounterexamplesinProbability:ThirdEdition.DoverBooksonMathematics.DoverPubli-
cations,2014.isbn:9780486499987.url:https://books.google.co.uk/books?id=xaH8AQAAQBAJ.
[16] DavidI.Spivak.“GeneralizedLensCategoriesviafunctorsC op Ñ Cat”.In:(08/06/2019).arXiv:http://arxiv.org/abs/1908.02202v2[math.CT].
[17] Joe Moeller and Christina Vasilakopoulou. “Monoidal Grothendieck Construction”. In: (09/03/2018).
arXiv:1809.00727v2[math.CT].
[18] BryceClarkeetal.“Profunctoroptics,acategoricalupdate”.In:(01/21/2020).arXiv:2001.07488v1[cs.PL].
[19] PietroVertechi.“DependentOptics”.In:(04/20/2022).arXiv:2204.09547[math.CT].
[20] MatteoCapucci.“Seeingdoublethroughdependentoptics”.In:(04/22/2022).arXiv:2204.10708[math.CT].
[21] DylanBraithwaiteetal.“Fibreoptics”.In:(12/21/2021).arXiv:2112.11145[math.CT].
[22] Matteo Capucci, Bruno Gavranović, and Toby St. Clere Smithe. “Parameterized Categories and Cate-
goriesbyProxy”.In:CategoryTheory2021.2021.
[23] Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In: (01/10/2017).
doi:10.1109/lics.2017.8005137.arXiv:http://arxiv.org/abs/1701.02547[cs.PL].
[24] MatthijsVákárandLukeOng.“OnS-FiniteMeasuresandKernels”.In:(10/03/2018).arXiv:1810.01837[math.PR].
[25] Toby St. Clere Smithe. “Some Notions of (Open) Dynamical System on Polynomial Interfaces”. In:
(08/25/2021).arXiv:2108.11137[math.DS].
[26] Fosco Loregian. (Co)end Calculus. London Mathematical Society Lecture Note Series. Cambridge Uni-
versityPress,01/11/2015.doi:10.1017/9781108778657.arXiv:https://arxiv.org/abs/1501.02503v6[math.CT].
[27] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. “Variational Inference: A Review for Statisti-
cians”. In: Journal of the American Statistical Association, Vol. 112 , Iss. 518, 2017 (01/04/2016). doi:
10.1080/01621459.2017.1285773.arXiv:1601.00670v9[stat.CO].
[28] DiederikPKingmaandMaxWelling.“Auto-EncodingVariationalBayes”.In:(12/20/2013).arXiv:http://arxiv.org/abs/1312.6114v10[stat.ML].
[29] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. “Generalized Variational Inference”. In:
(12/13/2019).arXiv:http://arxiv.org/abs/1904.02063[stat.ML].
[30] BartJacobs.StructuredProbabilitisticReasoning(forthcoming).2019.
29
[31] MarioRomán.“OpenDiagramsviaCoendCalculus”.In:(04/09/2020).arXiv:2004.04526v2[math.CT].
[32] TobySt.ClereSmithe.“CyberKittens,orSomeFirstStepsTowardsCategoricalCybernetics”.In:Pro-
ceedings3rdAnnualInternationalAppliedCategoryTheoryConference2020(ACT2020).2020.
[33] GuillaumeBoisseau.“StringDiagramsforOptics”.In:(02/11/2020).arXiv:2002.11480v1[math.CT].
A. 2-local contexts, graphically
To clarify the idea that the 2-local contexts for the factors of a tensor product game (or morphism more
generally) are obtained by “filling the hole” on the left or right of the tensor, one can work in the monoidal
bicategoryofV-profunctorsandusetheassociatedgraphicalcalculus[31]todepictthe‘hole’inthecontext
anditsfiller.Inthissection,weworkwithageneralmonoidalcategoryC,whichmayormaynotbeacategory
of lenses or games. Nonetheless, the basic idea is the same: a (complex) context for a morphism X Ñ Y is
given by a triple of a residual denoted Θ, a state I Ñ ΘbX and a ‘continuation’ (or ‘effect’) ΘbY Ñ I,
coupledaccordingtothecoendquotientrule.
Below,weshowhowtoobtaintheobjectofrightlocal2-contextsforatensorproductmorphismf bf1 :
X bX1 Ñ Y bY1,usingthegraphicalcalculusofV-Prof. Ateachstage,wedepictonthelefttheobject
namedontheright. Westartwithacomplexcontextforthetensoralongwitha‘filler’objectofmorphisms
X1 Ñ Y1,whichisshown“fillingthe(left-hand)hole”. Inthefirststep,weusethecompositionruleofC to
connect the matching ‘ports’ on the domain X. We then couple the matching Y port using the coend and
gather Θ and Y together into a single residual. Note that these steps correspond directly to factors in the
definitionofπ : Ctxpf bf1q Ñ Ctxpf1q(Definition4.9).
f1
ż
Θ:C
I b X X Y Y b I CpI,ΘbX bX1qˆCpX,YqˆCpΘbY bY1,Iq
X1 Y1
ż
Θ:C
ÝÑ I b Y Y b I CpI,ΘbY bX1qˆCpΘbY bY1,Iq
X1 Y1
ż
Θ1:C
ãÝÑ I b b I CpI,Θ1bX1qˆCpΘ1bY1,Iq
X1 Y1
Wefindthisgraphicalrepresentationtobeausefulaidincomprehension,andoftensimplifiesthesymbolic
‘book-keeping’thatcancomplicateexpressionssuchasthoseinDefinition4.9. Thecostofthisexpressivityis
theintroductionofanothercategoricalstructure,anditsassociatedcognitiveload. Inpreviouswork[32],we
havemademoreuseofthisrepresentation: there,weworkedwiththe‘optical’definitionofBayesianlenses
describedinSt. ClereSmithe[6];andwenotethatanearlierinformalversionofthisgraphicallanguagewas
30
originallyusedtodefinelocalcontextsfortensorproductgamesinthecompositionalgametheoryliterature
[7]. SincetheV-profunctorialsetting[18]isrequiredinordertodefineoptics,wehadalreadypaidthisextra
cognitive cost. In this paper, however, we have preferred to stick with the simpler fibrational definition of
Bayesianlenses.
Remark A.1. A different but related graphical calculus for optics is described by Boisseau [33]. However,
this alternative calculus is somewhat less general than that of Román [31], and its adoption here would not
eliminatetheextracognitivecost;wedononethelessmakeuseofitin[6].
31