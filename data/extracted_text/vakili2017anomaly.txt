7102
peS
11
]GL.sc[
1v37530.9071:viXra
1
Anomaly Detection in Hierarchical Data
Streams under Unknown Models
Sattar Vakili1, Qing Zhao1, Chang Liu2 and Chen-Nee Chuah2
1School of Electrical and Computer Engineering, Cornell University, sv388,qz16 @cornell.edu
{ }
2Electrical and Computer Engineering Department, University of California, Davis, chuah,cchliu @ucdavis.edu
{ }
Abstract
We consider the problem of detecting a few targets among a large number of hierarchical data
streams. The data streams are modeledas randomprocesseswith unknownand potentiallyheavy-tailed
distributions.Theobjectiveisanactiveinferencestrategythatdetermines,sequentially,whichdatastream
to collect samples from in order to minimize the sample complexity under a reliability constraint. We
propose an active inference strategy that inducesa biased randomwalk on the tree-structuredhierarchy
based on confidenceboundsof sample statistics. We then establish its order optimalityin termsof both
thesizeofthesearchspace(i.e.,thenumberofdatastreams)andthereliabilityrequirement.Theresults
find applications in hierarchical heavy hitter detection, noisy group testing, and adaptive sampling for
active learning, classification, and stochastic root finding.
I. INTRODUCTION
We consider the problem of detecting a few targets with abnormally high mean values among
a large number of data streams. Each data stream is modeled as a stochastic process with an
unknown and potentially heavy-tailed distributions. The stochastic nature of the data streams
may be due to the inherent randomness of the underlying phenomenon or the noisy response of
the measuring process. The number of targets is unknown. The objective is to identify all targets
(if any) or declare there is no target to meet a required detection accuracy with a minimum
number of samples.
Inherent in a number of applications (see Sec. I-A) is a tree-structured hierarchy among the
large number of data streams. With each node representing a data stream, the tree structure
encodes the following relationship: the abnormal mean of a target leads to an abnormal mean
2
in every ancestor of the target (i.e., every node on the shortest path from this target to the root
of the tree). We illustrate in Fig. 1 a special case of a binary-tree hierarchy.
Targets are of two types: leaf-level targets and hierarchical targets. A leaf-level target is a leaf
(level 0) of the tree whose mean is above a given threshold. Hierarchical targets are defined
recursively in an ascending order of the level of the tree. Specifically, an upper-level node with
an anomalous mean is a (hierarchical) target if its mean remains above a given threshold after
excluding all its target descendants (if any). Otherwise, this upper-level node is only a reflecting
point for merely being an ancestor of a target (see Fig. 1).
The objective of the problem is to detect all targets quickly and reliably by fully exploiting
the hierarchical structure of the data streams. Specifically, we seek an active inference strategy
that determines, sequentially, which node on the tree to probe and when to terminate the search
in order to minimize the sample complexity for a given level of detection reliability. We are
particularly interested in strategies that achieve a sublinear scaling of the sample complexity
with respect to the number of data streams. In other words, accurate detection can be achieved
by examining only a diminishing fraction of the search space as the search space grows.
(1,3)
(l=3)
(1,2) (2,2)
(l=2)
(1,1) (2,1) (3,1) (4,1)
(l=1)
(l=0)
(1,0) (2,0) (3,0) (4,0) (5,0) (6,0) (7,0) (8,0)
Fig. 1. A tree-structured hierarchy with l denoting the level of the tree and (k,l) the kth node on the l-th level ((1,0) is a
leaf-level target, (3,1) is a hierarchical target. Nodes (1,1), (1,2), (1,3), and (2,2) are reflecting points). .
A. Applications
The above general problem of detecting abnormal mean values in hierarchical data streams
arises in a number of active inference and learning applications in networking and data analytics.
3
We give below several representative examples to make the formulation of the problem concrete.
Heavy hitter and hierarchical heavy hitter detection: In Internet and other communication
and financial networks, it is a common observation that a small number of flows, referred to as
heavy hitters (HH), account for the most of the total traffic [1]. Quickly identifying the heavy
hitters is thus crucial to network stability and security. With limited sampling resources at the
router, however, maintaining a packet count of each individual flow is highly inefficient, if not
infeasible. The key to an efficient solution is to consider prefix aggregation based on the source
or destination IP addresses. This naturally leads to a binary tree structure with all targets (HHs)
at the leaf level.
A more complex version of the problem is hierarchical heavy hitter (HHH) detection, in
which the search for flows with abnormal volume extends to aggregated flows. In other words,
there exist hierarchical targets. HHH detection is of particular interest in detecting distributed
denial-of-service attacks [2].
Noisy group testing: In group testing, the objective is to identify a few defective items in
a large population by performing tests on subsets of items. Each group test gives a binary
outcome, indicating whether the tested group contains any defective items. The problem was
first motivated by the blood-test screening of draftees during World War II, for which Robert
Dorfman originated the idea of testing blood samples pooled from a group of people [3]. Since
then, the problem has found a wide range of applications, including idle channel detection [4],
network tomography [5], detecting malicious users and attackers [6], [7], and DNA sequencing
and screening [8], [9].
Most work on group testing assumes error-free test outcomes (see Sec. I-C for a more
detailed discussion on existing work). The problem studied in this paper includes, as a special
case, adaptive group testing under general and unknown noise models for the test outcomes.
Specifically, the outcome of a group test is no longer a deterministic binary value, but rather a
Bernoulli random variable with an unknown parameter that represents the false alarm probability
(when thetested group contains no defectiveitems)or thedetection power(i.e., theprobabilityof
correct detection when the test group contains defective items). A data stream thus corresponds
to the noisy Bernoulli test outcomes of a given subset of the population. The hierarchy of the
data streams follows from the “subset” relationship among the corresponding test groups. Under
the practical assumption that false alarm and miss detection probabilities are smaller than 1/2,
targets are those leaf nodes whose mean value exceeds 1/2. A more detailed mapping of the
4
noisy group testing problem to the active inference problem studied in this paper is given in
Sec. V.
Adaptive sampling with noisy response: The problem also applies to adaptive sampling with
noisy response for estimating a step function in [0,1]. Such problems arise in active learning of
binary threshold classifiers for document classification [10] and stochastic root finding [11].
Partitioning the [0,1] interval into small intervals and sampling the boundary points of each
interval, we can map the adaptivesampling problem to the target search problem where the target
is the small interval containing the location of the step. Examining larger intervals (consisting
of several smaller intervals) induces a hierarchical structure of the noisy responses. See Sec. V
for a detailed discussion.
B. Main Results
We develop an active inference strategy for detecting an unknown number of targets among
a large number N of data streams with unknown distributions. The performance measure is the
number of samples (i.e., detection delay) required for achieving a confidence level above 1 ǫ
−
(i.e., the probability that the declared target set does not equal to the true set is bounded by
ǫ). By fully exploiting the tree-structured hierarchy, the proposed active inference strategy has a
sample complexity that is order optimal in both the size N of the search space and the reliability
constraint ǫ.
Referred to as Confidence Bounds based Random Walk (CBRW), the proposed strategy
consistsofaglobalrandom walk on thetree interwovenwith alocal confidence-bound based test.
Specifically, it induces a biased random walk that initiates at the root of the tree and eventually
arrives and terminates at a target with the required reliability. Each move in the random walk is
guided by the output of a local confidence-bound based sequential test carried on each child of
the node currently being visited by the random walk. This local sequential test module ensures
that the global random walk is more likely to move toward the target than move away from it
and that the random walk terminates at a true target with a sufficiently high probability.
The sample complexity of CBRW is analyzed using properties of biased random walk on
a tree and large deviation results on the concentration of the sample mean statistic. We show
that the sample complexity of CBRW is in the order of O(logN +log 1) provided that the gap
ǫ
between the mean value of each data stream and the given threshold is bounded away from 0. It
is thus order optimal in both N and ǫ as determined by information-theoretic lower bounds. Of
5
particular significance is that the effect on the sample complexity from an enlarged search space
(increasing N) and an enhanced reliability (decreasing ǫ) is additive rather than multiplicative.
This results from the random walk structure which effectivelyseparates two objectivesof moving
tothetargets withO(logN) samplesand declaring thetargetsat thedesired confidencelevelwith
O(log 1)samples.Theproposedstrategyappliestounknownheavy-taileddistributionmodelsand
ǫ
preserves its order optimality in both N and ǫ. Comprising of calculating confidence bounds of
the mean and performing simple comparisons, the proposed strategy is computationally efficient.
C. Related Work
The problem studied here is related to several active learning and sequential inference prob-
lems. We discuss here representative studies most pertinent to this paper and emphasize the
differences in our approach from these existing studies.
Noisy group testing: Variations of the classic group testing have been extensively studied in
the literature focusing mainly on the noisless case. There are several recent studies that consider
one-sided error (false positive or false negative) in the test outcomes [12]–[14] or symmetric
error (with equal false positive and false negative probabilities) [15]–[17] with known error
probabilities. To our best knowledge, the result in this paper is the first applicable to noisy
group testing under general and unknown noise models.
Adaptive sampling with noisy response: The main body of work on adaptive sampling is
basedonaBayesianapproach withbinarynoiseofaknownmodel.ApopularBayesian strategyis
the Probabilistic Bisection Algorithm (PBA), which updates the posterior distribution of the step
location after each sample (based on the known model of the noisy response) and chooses
the next sampling point to be the median point of the posterior distribution. Although several
variations of the method have been extensively studied in the literature [18]–[20] following the
pioneering work of [21], there is little known about the theoretical guarantees, especially when
it comes to unknown noise models. In this paper we present a non-Bayesian approach to the
adaptive sampling problem under general unknown noise models.
HH and HHH detection: Prior solutions for online detection of HHHes typically involve
adjusting which prefixes to monitor either at the arrival of each packet [22]–[24], or at periodic
intervals [25], [26]. A particularly relevant work is the adaptive monitoring algorithm proposed
by Jose et al. [25], where a fixed number of measurement rules are adjusted at periodic intervals
based on the aggregate packet counts matching to each of these rules. At each time interval,
6
the aggregate count is compared to a heuristically chosen threshold (e.g., a fraction of link
capacity), to determine whether it is an HHH, and whether the rules need to be kept in the
next interval, or expanded to monitor the children of the prefix, or collapsed and combined with
upstream nodes. While the proposed CBRW has a similar flavor of moving among parent and
children, which is very much inherent to the HHH detection problem, the decision criteria used
to adjust the prefix is different. Instead of comparing with a fixed threshold, our decision is
based on statistical metric determined by the desired detection error. Different from the heuristic
studies in the literature, the proposed strategy offers performance grantee and order optimality.
We provide a rigorous framework that succinctly captures the tradeoff between detection time
and overall detection performance.
Pure-exploration bandit problems: The problem studied here is also related to the so-called
pure-exploration bandit problems [27] where the objectiveis to search for a subset of bandit arms
with certain properties. In particular, in the Thresholding Bandit problem introduced in [28], the
objective is to determine the arms with mean above a given threshold. The key difference is that
the thresholding bandit problem does not assume any structure in the arms and has a linearly
growing sample complexity in the number of arms. The focus of this paper is on exploiting the
hierarchical structure inherent to many applications to obtain sublinear sample complexity with
the size of the search space.
Active hypothesis testing for anomaly detection: The active inference problem considered
here falls into the general class of sequential design of experiments pioneered by Chernoff in
1959 [29] with variations and extensions studied in [30]–[33]. These studies assume known
or parametric models and focus on randomized test strategies. This paper, however, adopts a
nonparametric setting and proposes a deterministic strategy. A number of studies on anomaly
detection within the sequential and active hypothesis testing framework exist in the literature
(see an excellent survey in [34] and a few recent results in [35]–[38]). These studies in general
assumeknownmodelsanddonotaddresshierarchicalstructureofthesearchspace.Aparticularly
relevant work is [40], in which a test strategy based on a random walk on a binary tree was
developed. While the CBRW policy proposed here shares a similar structure as the test strategy
developed in [40], the latter work assumes that the stochastic models of all data streams are
known and the testing strategy relies on using the likelihood ratios calculated based on the
known distributions. For the unknown model scenario considered in this paper, the confidence-
bound based statistics used for guiding the biased random walk are fundamentally different from
7
the likelihood ratio. As a result, the performance analysis also differs.
II. PROBLEM FORMULATION AND PRELIMINARIES
A. Problem Formulation
Consider a set of N data streams conforming to a binary-tree structure with K leaf nodes
as illustrated in Fig. 1 (extensions to general tree structures are discussed in Sec. V). Let (l,k)
(l = 0,1,...,L,k = 1,...,2L l) denote the kth node at level l of the tree. Let X (t)
−
{
k,l
}
∞t=1
denote the corresponding random process which is independent and identically distributed with
an unknown distribution f and an unknown mean µ .
k,l k,l
Associated with each level l of the tree is a given threshold η that defines the targets of
l
interest. Specifically, a leaf-level target is a node at level l = 0 whose mean µ exceeds η .
k,0 0
Hierarchical targets are defined recursively in terms of l. Specifically, a hierarchical target at
level l > 0 is a node whose mean value remains above the threshold η after excluding all its
l
target decedents. The tree-structured hierarchy encodes the following relationship among nodes
in terms of their mean values: for an arbitrary node (k,l), if µ k,l > η l , then µ k′,l′ > η l′ for all
(k ,l ) on the shortest path from (k,l) to the root node of the tree.
′ ′
An active inference strategy π = ( a ,T , ) consists of a sampling strategy a , a
t t 1 π π t t 1
{ } ≥ S { } ≥
stopping rule τ , and a terminal decision rule . The sampling strategy a is a sequence
π π t t 1
S { } ≥
of functions mapping from past actions and observations to a node of the tree to be sampled
at the current time t. The stopping rule T determines when to terminate the search, and the
π
decision rule declares the detected set of targets at the time of stopping. Let E and P
π
S F F
denote, respectively, the expectation and the probability measure under distribution model =
F
f . The objective is as follows:
(k,l) l=0,1,...,L
{ } k=1,...,2L−l
minimize E T ,
π π
F
s.t. P [ = ] ǫ,
π
F S 6 S ≤
where is the true set of targets.
S
B. Sub-Gaussian, Heavy-Tailed, and Concentration Inequality
We consider a general distribution f for each process. Due to different concentration behav-
k,l
iors, sub-Gaussian and heavy-tailed distributions are treated separately. Recall that a real-valued
8
random variable X is called sub-Gaussian [41] if, for all λ ( , ),
∈ −∞ ∞
E[eλ(X E[X])] eξλ2/2 (1)
−
≤
for some constant ξ > 0. We assume (an upper bound on) ξ is known. For sub-Gaussian random
variables, Chernoff-Hoeffding concentration inequalities hold. Specifically [42]:
P X(s)+ 2ξlog p 1 < µ p
s ≤
 (cid:20) q (cid:21)
 (2)



  P X(s) 2ξlog p 1 > µ p,
− s ≤
  (cid:20) q (cid:21)




where X(s) = 1 s X(t) is the sample mean of s independent samples of X.
s t=1
For heavy-tailed distributions, the moment generating functions are no longer bounded, and
P
the Chernoff-Hoeffding type of concentration inequalities do not hold. However, the following
can be said about a truncated sample mean statistic in place of the sample mean in (2). Assume
that a b-th (1 < b < 2) moment of X is bounded:
E[Xb] u, (3)
≤
for some u > 0. Define the following truncated sample mean with a parameter p (0, 1]
∈ 2
s
1
X(s,p) = X(t)
s
t=1
X
b
1
ut
X(t) ( )1/b . (4)
| | ≤ log 1
( )
p
We then have (Lemma 1 in [43]),
Pr X(s,p) 4u1/b( log p 1 ) b− b 1 > µ p
− s ≤
 (cid:20) (cid:21)
 (5)
 b


  Pr X(s,p)+4u1/b( log p 1 ) b− b 1 < µ p.
s ≤
 (cid:20) (cid:21)



 b

III. AN ACTIVE INFERENCE STRATEGY: CBRW
In this section, we present the Confidence Bounds based Random Walk (CBRW) policy. We
focus on the case of a single target and sub-Gaussian distributions. Extensions to multiple target
detection and heavy-tailed distributions are discussed in Sec. V.
9
A. Detecting Leaf-Level Targets
We first consider applications where it is known that the target is at the leaf level. This
includes, for example, HH detection, noisy group testing, and adaptive sampling as discussed in
Sec. I-A.
The basic structure of CBRW consists of a global random-walk module interwoven with a
local CB-based sequential test module at each step of the random walk. Specifically, the CBRW
policy performs a biased random walk on the tree that eventually arrives and terminates at the
target with the required reliability. Each move in the random walk (i.e., which neighboring node
to visit next) is guided by the output of the local CB-based sequential test module. This module
ensures that the random walk is more likely to move toward the target than to move away from
the target and that the random walk terminates at the true target with high probability.
Consider first the local CB-based sequential test module. This local sequential test is carried
out on a specific node (random process) X(t) , where we have omitted the node index (k,l)
{ }
∞t=1
for simplicity. The goal is to determine whether the mean value of X(t) is below a given
{ }
∞t=1
threshold η at a confidence level of 1 β or above the threshold at a confidence level of 1 α.
− −
If the former is true, the test module outputs 0, indicating this node is unlikely to be an ancestor
of a target or the target itself. If the latter is true, the output is 1. Let (α,β,η) denote this local
L
sequential test with given parameters α,β,η . It sequentially collects samples from X(t) .
{ } { }
∞t=1
After collecting each sample, it determines whether to terminate the test and if yes, which value
to output based on the following rule:
2ξlog2s3
If X(s) α > η, terminate and output 1.
• − s
q 2ξlog2s3
If X(s)+ β < η, terminate and output 0.
• s
r
Otherwise, continue taking samples,
•
where X(s) denotes the sample mean obtained form s observations and ξ is the distribution
parameter specified in (1). We now specify the random walk on the tree based on the outputs
of the local CB-based tests. Let (k,l) denote the current location of the random walk (which is
initially set at the root node). Consider first l > 1. The left child of (k,l) is first probed by the
local module (α,β,η) with parameters set to α = β = p where p can be set to any constant
0 0
L
in (0,1 1 ), and η being the threshold associated with level l 1 of the children of (k,l).
− √2 −
If the output is 1, the random walk moves to the left child of (k,l), and the procedure repeats.
Otherwise, the right child of (k,l) is tested with the same set of parameters, and the random
10
1: Initialization: Initial location of the random walk (k,l) = (1,L), p 0 (0,1 1 ), α = β = p 0 ,
∈ − √2
ǫ (0,0.5), STOP =0.
∈
2: loop while STOP=0
3: if l>1 then
4: Test the left child of (k,l) by (α,β,η l 1)
L −
5: if the output of the test on the left child is 1 then
6: Move to the left child of (k,l).
7: else if the output of the test on the left child is 0 then
8: Test the right child of (k,l) by (α,β,η l 1)
L −
9: if the output of the test on the right child is 1 then
10: Move to the right child of (k,l).
11: else if the output of the test on the right child is 0 then
12: Move to the parent of (k,l).
13: end if
14: end if
15: else if l=1 then
16: Test the left child of (k,l) by L ( 2LC ǫ p0 ,β,η l − 1).
17: if the output of the test on the left child is 1 then
18: Declare the left child of (k,l)as the target.
19: Set STOP=1.
20: else if the output of the test on the left child is 0 then
21: Test the right child of (k,l) by L ( 2LC ǫ p0 ,β,η l − 1).
22: if the output of the test on the right child is 1 then
23: Declare the right child of (k,l)as the target.
24: Set STOP=1.
25: else if the output of the test on the right child is 0 then
26: Move to the parent of (k,l).
27: end if
28: end if
29: end if
30: end loop
Fig. 2. The random walk module of CBRW for detecting a single leaf-level target.
11
walk moves to the right child if this test outputs 1. If the outputs of the tests on both children
are 0, the random walk moves back to the parent of (k,l) (the parent of the root node is defined
as itself). The values for α,β,η specified above ensure that the random walk moves toward
{ }
the target with a probability greater than 1/2. When the random walk arrives at a node on level
l = 1, the left child of (k,l) is first probed by the local module (α,β,η) with parameters set
L
to α = ǫ , β = p and η = η where
2LCp0 0 0
1
C = . (6)
p0 2
1 exp( 2(1 2(1 p )2)2)
0
− − − −
(cid:18) (cid:19)
If the output is 1, the random walk terminates and the left child of (k,l) is declared as the target.
Otherwise, the right child of (k,l) is tested with the same set of parameters, and the random
walk terminates with the right child declared as the target if this test outputs 1. If the outputs
of the tests on both children are 0, random walk moves back to the parent of (k,l). The values
for α,β,η specified above ensure that the random walk terminates at and declares the true
{ }
target with the required confidence level of 1 ǫ. A description of CBRW for detecting a single
−
leaf-level target is given in Fig. 2.
B. Detecting Hierarchical Targets
Wenowconsiderthecasewherethetargetmayresideathigherlevelsofthetree.Thefollowing
modified CBRW policy detects a potentially hierarchical target with the required confidence level
of 1 ǫ.
−
Let (k,l) denote the current location of the random walk (which is initially set at the root
node). Consider first (k,l) is a non-leaf node with l > 0. The node (k,l) is first probed by the
local module (α,β,η) with parameters set to α = β = p where p (0,1 1 ) and η = η .
L 0 0 ∈ − √32 l
If the output is 0, the random walk moves to the parent of (k,l). If the output is 1, then the
left child of (k,l) is tested by the local module (α,β,η) with parameters set to α = β = p
0
L
and η = η . If the output is 1, the random walk moves to the left child. Otherwise, the right
l 1
−
child of (k,l) is tested with the same set of parameters, and the random walk moves to the right
child if this test outputs 1. If the outputs of the tests at (k,l) and its children are 1, 0, and 0,
respectively, then (k,l) is likely to be a hierarchical target and the random walk stays at (k,l).
When the random walk stays at the same node (k,l), the same tests are repeated on (k,l) and its
children with an increased confidence level. We increase the confidence level by dividing α and
12
1: Initialization: Initial location of the random walk (k,l) = (1,L), parameters STOP = 0, p 0
∈
(0,1
− √3
1
2
), α=β =p
0
, ǫ
∈
(0,0.5)), C
p
H
0
, STOP =0.
2: loop while STOP =0
3: if l>0 then
4: Test (k,l) by (α,β,η l )
L
5: if the output of the test on (k,l) is 0 then
6: Move the parent of (k,l). Set α=β =p 0 .
7: else if the output of the test on (k,l) is 1 then
8: Test the left child of (k,l) by (α,β,η l 1).
L −
9: if the output of the test on the left child of (k,l) is 1 then
10: Move to the left child of (k,l). Set α=β =p 0 .
11: else if the output of the test on the left child of (k,l) is 0 then
12: Test the right child of (k,l) by (α,β,η l 1).
L −
13: if the output of the test on the right child of (k,l) is 1 then
14: Move to the right child of (k,l). Set α=β =p 0 .
15: else if the output of the test on the right child of (k,l) is 0 then
16: if α< ǫ then
3LCH
p0
17: Declare (k,l) as the target.
18: Set STOP =1.
19: else
20: Divide α and β by 2: α α and β β.
← 2 ← 2
21: end if
22: end if
23: end if
24: end if
25: else if l=0 then
26: Test (k,l) by L ( 3LC ǫ p H 0 ,β,η l )
27: if the output of the test on (k,l) is 0 then
28: Move to the parent of (k,l).
29: else if the output of the test on (k,l) is 1 then
30: Declare (k,l) as the target.
31: Set STOP =1.
32: end if
33: end if
34: end loop
Fig. 3. The random walk module of CBRW for detecting a single hierarchical target.
13
β by 2 iteratively. When the current value of α and β becomes smaller than ǫ , the random
3LCH
p0
walk stops and declares (k,l) as the target. The value of
1
CH = (7)
p0 2
1 exp( 2(1 2(1 p )3)2)
0
− − − −
(cid:18) (cid:19)
ensures the desired confidence level of 1 ǫ at detection of the target. If the random walk moves
−
to a new location the values of α and β is reset to p . When the random walk arrives at a leaf
0
node (k,l) with l = 0, the leaf node is tested by the local module (α,β,η) with parameters
L
set to α = ǫ , β = p and η = η . If the output is 1, the random walk stops and declares
3LCH 0 0
p0
(k,l) as the target. Otherwise, the random walk moves to the parent of (k,l). A description of
CBRW for detecting a single hierarchical target is given in Fig. 3.
IV. PERFORMANCE ANALYSIS
In this section, we analyze the sample complexity of CBRW. We again focus on the case
of a single target and sub-Gaussian distributions and leave extensions to more general cases to
Sec. V.
A. The Sample Complexity of the CB-based Sequential Test Module
To analyze the sample complexity of CBRW, we first analyze the sample complexity of the
local CB-based sequential test module (α,β,η) in the lemma below. We then analyze the
L
behavior of the random walk to establish the number of times that the local sequential test is
carried out.
Lemma 1. Let µ denote the expected value of an i.i.d. sub-Gaussian random process X(t) .
{ }
∞t=1
Let τ be the stopping time of the CB-based sequential test (α,β,η) applied to X(t) . We
L L { }
∞t=1
have, in the case of µ > η,
2ξlog 2T3
P[X(T)+ β < η] β, (8)
s T ≤
24 3 2
48
E[T] log α +2. (9)
≤ (µ η)2 (µ qη)2
− −
In the case of µ < η,
14
2ξlog 2T3
P[X(T) α > η] α, (10)
−s 2T ≤
24 3 2
E[T] 48 log β +2. (11)
≤ (µ η)2 (µ qη)2
− −
Proof. See Appendix A.
The inequalities (8) and (10) establish the confidence levels for the local sequential CB-
based test. Both results on the confidence levels and the sample complexity are based on the
concentration inequalities given in (2).
B. The Sample Complexity of CBRW
In both cases of leaf-level target detection and hierarchical target detection, the sample com-
plexity of CBRW is order optimal in both N and 1 as stated in Theorem 1 below.
ǫ
Theorem 1. Assume that there exists δ > 0 such that µ η δ for all (k,l) (l =
k,l l
− ≥
0,1,...,L,k = 1,...,2L l). We have1
−
1
E [T ] = O(log N +log ) (12)
F CBRW 2 ǫ
and
P [ = ] ǫ. (13)
CBRW
F S 6 S ≤
Proof. See Appendix B.
The gap µ η in the mean value of a random process at level l and the threshold at the
k,l l
−
respective level indicates the informativeness of the observations. It is a practical assumption
that the gap is bounded away from 0. We might also naturally assume that the higher levels are
less informative; thus, have smaller gaps. For example, in group testing, tests from larger groups
of items are less informative about the presence of defective items. Under this assumption we
can provide a finite-time upper bound on the sample complexity of CBRW.
We first introduce some auxiliary notions which are useful in understanding the trajectory
of the random walk in CBRW. Under leaf-level target setting, consider a sequence of subtrees
1Thesearchspaceforthecaseofdetectingleaf-leveltargetisofsizeK,thenumberofleafnodes.However,sinceN =2K−1
is of the same order of K, (12) satisfiesfor both K and N.
15
E [T ] 2 L C 48 log 243 p 2 0 +2 + 48 log 243 2Cp ǫ 0 L +2. (14)
F CBRW ≤ l=1 p0 (cid:18) (µ rl,l − η l )2 (µ rl,l q − η l )2 (cid:19) (µ r0,0 − η 0)2 (µ r0 q ,0 − η 0)2
X
E [T ] 3 L C p H 0 48 log 243 p 2 0 +2 + p 0 16log2
F CBRW ≤ l=
X
10+1 1 − p 0 (cid:20)(cid:18) (µ rl,l − η l )2 (µ rl,l q − η l )2 (cid:19) (1 − p 0)(µ rl,l − η l )2 (cid:21)
+3 2 C p H 0 48 log 243 p 2 0 +2 + p 0 16log2
l′=1 1 − p 0 (cid:20)(cid:18) (µ r l ′,l′ − η l′)2 (µ r l ′,l′ q − η l′)2 (cid:19) (1 − p 0)(µ r l′,l′ − η l′)2 (cid:21)
X
+log 6LC p H 0 p 0 48 log 243 4 ǫ +2 . (15)
2 ǫ (µ η )2 (µ q η )2
(cid:18) rl0 ,l0 − l0 rl0 ,l0 − l0 (cid:19)
Fig. 4. Finite-regime upper bounds on the performance of CBRW under leaf-level (14) and hierarchical (15) target settings.
, ,..., of . Subtree is obtained by removing the biggest half-tree containing the tar-
1 2 L L
{T T T } T T
get from . Subtree is iterativelyobtained by removingthebiggesthalf-tree containingthetar-
l
T T
getfromthehalf-treecontainingthetargetinthepreviousiteration.IntheexamplegiveninFig.1,
= (1,3),(2,2),(3,1),(4,1),(5,0),(6,0),(7,0),(8,0) , = (1,2),(2,1),(3,0),(4,0)
3 2
T { } T { }
and = (1,1),(2,0) . Let (r ,l) denote the child of the root node of . Under hierarchical
1 l l
T { } T
target setting with a hierarchical target at level l , let subtrees ,..., be the same as
0
{T
l0+1
T
L
}
defined for a target that is a decedent of the hierarchical target. Also, define and as subtrees
T1′ T2′
whose root nodes are children of the hierarchical target. Let (r ,l) denote the root node of .
l′ Tl′
A detailed finite-time upper bound on the sample complexity of CBRW is given in (14)
and (15) where (r ,0) and (r ,l ) denote the targets under leaf-level and hierarchical settings,
0 l0 0
respectively.
V. EXTENSIONS AND DISCUSSIONS
In this section, we discuss extensions to more general scenarios and the mapping of various
applications to the target detection problem at hand.
16
A. Detecting an Unknown Number of Targets
Detecting > 1 targets with known can be easily implemented by sequentially locating
|S| |S|
the targets one by one. We assume that each target can be removed after it is located by CBRW2.
To ensure that the reliability constraint holds, we replace ǫ with ǫ in each search of a single
|S|
target. The reliability constraint holds by union bound on the error probabilities of the searches
for a single target.
When the number of targets is unknown, but an upper bound S on the number of
max
≥ |S|
targets is known, we can similarly detect the targets one by one. To ensure that the reliability
constraint holds, we replace ǫ with ǫ in each search of a single target. The stopping rule for
2Smax
the overall search can be implemented by testing the root node. Specifically, the root node is
tested by (ǫ ,ǫ ,η ) with ǫ = ǫ every LC steps in the random walk under leaf target
L 0 0 L 0 2Smax p0
setting and every LCH steps under the hierarchical target setting. The reliability constraint holds
p0
by union bound on the error probabilities of the searches for a single target and error in stopping
the overall search before finding all targets.
Under both leaf-level and hierarchical target settings, with this modification, the sample
complexity of finding single targets simply add up to ab O( logK+ log 1) overall sample
|S| |S| ǫ
complexity.
B. Heavy-Tailed Distributions
The extension to more general distribution models can be implemented by only modifying
the local CB-based test in a way that the confidence levels remain the same. As a result, the
L
behavior of the random walk on the tree remains the same.
Specifically,forheavy-taileddistributionswithexistingb’th momentasgivenin(3), wemodify
the test
L
If X(s,α) 4u1/b( log2 α s3 ) b− b 1 > η, terminate and output 1.
• − s
If X(s,β)+4u1/b( log2 β s3 ) b− b 1 < η, terminate and output 0.
• b s
Otherwise, continue taking samples.
• b
2For example, in group testing, the detected defective item is no longer tested in any subsequent group tests or in HHH
detection, the packet count of each detected HHH can be subtracted from the packet count of the parents.
17
The resulting CBRW achieves the same O(logN + log 1) sample complexity under both leaf-
ǫ
level and hierarchical target settings. The proofs follow similar to the proofs of Theorem 1 and
Lemma 1, using confidence bounds (5) instead of (2) in the proof of Lemma 1.
C. General Tree Structures
Consider a general tree-structured hierarchy as shown in Fig. 5. The CBRW policy can be
modified as follows.
To havetherequired confidence levelin takingthesteps toward thetarget, the inputparameters
in the local CB-based sequential test are modified based on the degree d of each node (k,l)
k,l
L
in the tree. In particular, under the leaf-level target setting, we choose p (1 1 ) and
0
∈ − 2
−(dk,l −1)
α = ǫ where L is the maximum distance from the root node to a leaf node, D is the
(D 1)LC
−
maximum degree of the nodes in the tree and C is a constant independent of K and ǫ. Under
the hierarchical target setting, we choose p (1 1 ) and when increasing the confidence
0
∈ − 2
−dk,l
level iteratively to detect the hierarchical target, we terminate the search when p goes below
ǫ . The random walk moves to a child or the parent of the current location according to the
DLC
outputs of the tests.
Following similar lines as in the proof of Theorem 1, we can show a sample complexity of
O(LD)+O(log 1) under both leaf-level and hierarchical target settings.
ǫ
Fig. 5. A general tree-structured hierarchy.
18
D. Mapping from Various Applications
HH and HHH detection: The CBRW policy directly applies to leaf-level HH and HHH
detection under leaf-level and hierarchical target settings, respectively. In particular, provided
a controllable counter which can be assigned to each IP prefix, we assign the counter to the
current location which is desired to be tested according to . Based on the packet count, the
L
counter is moved on the tree according to CBRW. When there are several counters available the
tree can be partitioned to smaller subtrees and CBRW is run on each subtree separately to make
an efficient use of the available counters.
Noisy group testing: In group testing, the objective is to identify a few defective items in
a population of K items performing tests on subsets of items. Each group test gives a binary
outcome, indicating whether the tested group contains any defective items. We consider the case
ofadaptivegrouptestingundergeneralandunknownnoisemodelsfortestoutcomes.Specifically,
the outcome of a group test is a Bernoulli random variable with an unknown parameter that
represents the false alarm or missed detection probability.
The CBRW policy under leaf-level target setting directly applies to noisy group testing where
the defective items are the leaf-level targets. Each data stream corresponds to the noisy Bernoulli
test outcomes of the given subsets of the population. The parent-children relationship on the
tree represents the subset relationship among the corresponding test groups such that the group
corresponding to the parent is the union of the groups corresponding to the children. Under
the practical assumption that false alarm and miss detection probabilities are smaller than
1/2, targets are those data streams generated by a singleton subset (i.e., leaf nodes) with a
mean value exceeding 1/2. Although group testing problem does not necessarily conform to a
predetermined hierarchical structure, the proposed solution offers order optimal number of tests
in both population size and reliability constraint.
Adaptive sampling with noisy response: Consider the [0,1] interval as the input space.
We limit the input space to be one-dimensional in order to demonstrate the main idea. The
hypothesis class, denoted by , is the set of all step functions on [0,1] interval.
H
= h : [0,1] R,h (x) =
z z
H →
(cid:26)
1
(x),z (0,1) (16)
(z,1]
{ } ∈
(cid:27)
Each hypothesis h assigns a binary label to each element of the input space [0,1]. There is a
z
true hypothesis h z∗ that determines the ground truth labels for the input space.
19
The learner is allowed to make sequential observations by adaptively sampling h z∗. The
observations are however noisy. The goal is to design a sequential sampling strategy aiming
at minimizing the sample complexity required to obtain a confidence interval of length ∆ for z
∗
at a 1 ǫ confidence level. Specifically, the learner chooses the sampling point x at each time
−
t and receives a noisy sample of the true hypothesis.
We consider two noise models with unknown distribution. In the first noise model, the learner
observes a noisy sample of the threshold function in the form of
hN
z∗
(x;t) = h z∗(x;t)+n(x,t), (17)
where n(x,t) is a zero mean sampling noise that possibly depends on the sampling point x and
is generated i.i.d. over t.
In the second noise model, the binary samples can flip from zeros to ones and vice versa.
Specifically, the learner receives erroneous binary samples with an error probability of p(.) in
the form of
hB
z∗
(x;t) = h z∗(x;t)
⊕
B(x,t), (18)
where is the boolean sum and B(x,t) is a Bernoulli random variable with P[B(x,t) = 1] =
⊕
p(x) that may depend on the sampling point x and is generated i.i.d. over t.
We now present a solution to the adaptive sampling problem based on the results obtained
for CBRW strategy. For the simplicity of presentation we assume ∆ = 1 (K = 1). Let each
2L ∆
node on a binary tree represent an interval [zL ,zR ] [0,1] with zL = (k2L l 1)∆ and
T k,l k,l ⊂ k,l − −
zU = k2L l∆. The interval corresponding to each node on the tree is the union of the intervals
k,l −
corresponding to its children.
What remains to be specified is what entails when probing a node/interval (k,l). When (k,l)
is probed the boundary points of the interval are tested by (α,β,η) with parameters set to
L
η = 0.5 and α = β = p on a non-leaf node, and α = ǫ , β = p on a leaf node where p
l 0 2LCp0 0 0
can be set to any constant in (0,1 1 ). The output is 1 (indicating that the interval is likely to
− √42
contain z ) if and only if the output of is 0 on the left boundary and 1 on the right boundary.
∗
L
The output is 0 otherwise. From the results on the analysis of CBRW the above solution has a
sample complexity of O(1 logK + 1 log 1) where c is 0.5 under the first noise model and c is
c2 c2 ǫ
a lower bound on the gap in 0.5 p(.) under the second noise model.
−
20
VI. CONCLUSION
In this paper, we studied the problem of detecting a few targets among a large number of
hierarchical data streams modeled as random processes with unknown distributions.We designed
a sequential strategy to interactively choose the sampling point aiming at minimizing the sample
complexity subject to a reliability constraint. The proposed sequential sampling strategy detects
thetargets at thedesired confidence levelwith an order optimallogarithmicsamplecomplexityin
both problem size and the parameter of reliability constraint. We further showed the extensions
of the results to a number of active inference and learning problems in networking and data
analytics applications.
The results obtained in this work extend to the detection of anomaly in other statistics such as
variance. In particular, replacing the sample mean with sample variance in the local CB-based
sequential test and modifying the second term in the upper and lower confidence bounds in
the local CB-based sequential test, both the CBRW policy and its sample complexity analysis
apply to anomaly detection where the anomalies manifest in the variance. Similar results can be
obtained for other statistics assuming the existence of an efficient estimator.
REFERENCES
[1] K. Thompson, G. Miller, and R. Wilder, “Wide-area internet traffic patterns and characteristics,” IEEE Network, vol. 11,
pp. 1017, Nov 1997.
[2] P.E. Ayres, H. Sun, H. J. Chao, and W. C. Lau “Alpi: A ddos defense system for high-speed networks,” IEEEJournal on
Selected Areas in Communications, vol. 24, no. 10, pp. 18641776, 2006. 24(10):18641776, 2006.
[3] R. Dorfman, “The detection of defective members of large populations,” in Annals of Mathematical Statistics, vol. 14, pp.
436171, 1943.
[4] A.SharmaandC.Murthy,“Grouptestingbasedspectrumholesearchforcognitiveradios,”IEEETransactionsonVehicular
Technology, 2014.
[5] M. Cheraghchi, A. Karbasi, S. Mohajer, and V. Saligrama, “Graph-constrained group testing,” IEEE Transactions on
Information Theory, vol. 58, pp. 248172, Jan 2012.
[6] M. T. Thai, Y. Xuan, I. Shin, and T. Znati, “On detection of malicious users using group testing techniques,” in The 28th
International Conference on Distributed Computing Systems, pp. 206173, 2008.
[7] S. Khattab, S. Gobriel, R. Melhem, and D. Mosse, “Live baiting for service-level DoS attackers,” in The 27th IEEE
Conference on Computer Communications, April 2008.
[8] H.Q.NgoandD.-Z.Du,“AsurveyoncombinatorialgrouptestingalgorithmswithapplicationstoDNAlibraryscreening,”
Discrete mathematical problems with medical applications, vol. 55, pp. 171172, 2000.
[9] D.Balding,W.Bruno,D.Torney,andE.Knill,“Acomparativesurveyofnon-adaptivepoolingdesigns,”inGeneticmapping
and DNA sequencing, pp. 133174, Springer, 1996.
21
[10] R. Castro and R. Nowak, “Minimax Bounds for Active Learning,” IEEE Transactions on Information Theory, vol. 54,
no. 5, pp. 2339-2353, 2008.
[11] P.I.Frazier,S.G.Henderson,R.Waeber,“Probabilisticbisectionconvergesalmostasquicklyasstochasticapproximation”,
available at arXiv:1612.03964v1 [math.PR], 2016.
[12] G.AtiaandV.Saligrama,“Noisygrouptesting:Aninformationtheoreticperspective,”in47thAnnualAllertonConference
on Communication, Control, and Computing, pp. 355172, IEEE, 2009.
[13] G. K. Atia and V. Saligrama, “Boolean compressed sensing and noisy group testing,” IEEE Transactions on Information
Theory, vol. 58, no. 3, pp. 18801701, 2012.
[14] V. Y. Tan and G. Atia, “Strong impossibility results for noisy group testing,” in ICASSP, pp. 82571761, 2014.
[15] M. Cheraghchi, A. Hormati,A. Karbasi,and M. Vetterli,“Compressed sensing withprobabilistic measurements: A group
testing solution,” in 47th Annual Allerton Conference on Communication, Control, and Computing, pp. 3017, IEEE, 2009.
[16] S. Cai, M. Jahangoshahi, M. Bakshi, and S. Jaggi, “Grotesque: noisy group testing (quick and efficient),” in 51st Annual
Allerton Conference on Communication, Control, and Computing, pp. 12341741, IEEE, 2013.
[17] C.L.Chan,S.Jaggi,V.Saligrama,andS.Agnihotri,“Non-adaptivegrouptesting:Explicitboundsandnovelalgorithms,”
IEEE Transactions on Information Theory, vol. 60, no. 5, pp. 30191735, 2014.
[18] R. Waeber, P. I. Frazier, S. G. Henderson, “Bisection Search With Noisy Responses,” in SIAM Journal on Control and
Optimization, vol. 51, no. 3, pp. 22611779.
[19] M.Ben-OrandA.Hassidim,“TheBayesianlearnerisoptimalfornoisybinarysearch,”in Proceedingsofthe49thAnnual
IEEE Symposium on Foundations of Computer Science, IEEE, pp. 221170 ,2008.
[20] R.CastroandR.Nowak,“Activelearningandsampling,”inFoundationsandApplicationsofSensorManagement,Springer,
New York, pp. 177170, 2008.
[21] M. Burnashev and K. Zigangirov, “An interval estimation problem for controlled observations,” Problemy Peredachi
Informatsii, vol 10 , pp. 5117, 1974.
[22] G. Cormode, F. Korn, S. Muthukrishnan, and D. Srivastava, “Finding hierarchical heavy hitters in streaming data” ACM
Trans. Knowl. Discov. Data, vol. 1, no.4, pp 1-48, 2008.
[23] L. Yuan, C. Chuah, and P. Mohapatra, “Progme: Towards programmable network measurement” In Proceedings of the
2007 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, SIGCOMM,
ACM 2007.
[24] Y. Zhang, S. Singh, S. Sen, N. Duffield, and C. Lund, “Online identification of hierarchical heavy hitters: Algorithms,
evaluation,andapplications,”InProceedingsofthe4thACMSIGCOMMConferenceonInternetMeasurement, IMC,ACM,
2004.
[25] L. Jose, M. Yu, and J. Rexford, “Online measurement of largetrafficaggregates on commodity switches” In Proceedings
of the 11th USENIX Conference on Hot Topics in Management of Internet, Cloud, and Enterprise Networks and Services,
Hot-ICE, USENIX Association, 2011.
[26] M. Mitzenmacher, T. Steinke, and J. Thaler, “Hierarchical heavy hitterswiththe space saving algorithm,”In Proceedings
of the Meeting on Algorithm Engineering and Experiments, ALENEX,2012.
[27] S. Chen, T. Lin, I. King, M. Lyu, and W Chen, “Combinatorial pure exploration of multiarmed bandits,” in Advances in
Neural Information Processing Systems, pp. 379177, 2014
[28] A. Locatelli, M. Gutzeit, A. Carpentier, “An optimal algorithm for the Thresholding Bandit Problem,” available at:
arXiv:1605.08671v1, 2016.
[29] H.Chernoff, “Sequentialdesignof experiments,”TheAnnalsofMathematical Statistics,vol.30,no. 3,pp.755770, 1959.
22
[30] S. Nitinawarat, G. K. Atia, and V. V. Veeravalli, Controlled sensing for multihypothesis testing, IEEE Transactions on
Automatic Control, vol. 58, no. 10, pp. 24512464, 2013.
[31] S. Nitinawarat and V. V. Veeravalli, Controlled sensing for sequential multihypothesis testing with controlled markovian
observations and nonuniform control cost, Sequential Analysis, vol. 34, no. 1, pp. 124, 2015.
[32] M. Naghshvar and T. Javidi, Active sequential hypothesis testing, The Annals of Statistics, vol. 41, no. 6, pp. 27032738,
2013.
[33] M. Naghshvar and T. Javidi, Sequentiality and adaptivity gains in active hypothesis testing, IEEE Journal of Selected
Topics in Signal Processing, vol. 7, no. 5, pp. 768782, 2013.
[34] A. Tajer, V. V. Veeravalli, and H. V. Poor, Outlying sequence detection in large data sets: A data-driven approach, IEEE
Signal Processing Magazine, vol. 31, no. 5, pp. 4456, 2014.
[35] K. Cohen, Q. ZhaoActiveHypothesis Testingfor Anomaly Detection IEEETransactions on Information Theory, vol. 61,
no. 3, pp. 1432-1450, March, 2015.
[36] K. Cohen, Q. Zhao Asymptotically Optimal Anomaly Detection via Sequential Testing IEEE Transactions on Signal
Processing, vol. 63, no. 11, pp. 2929-2941, June, 2015.
[37] K.LeahyandM. Schwager,Alwayschoose secondbest:Tracking amovingtargetonagraphwithanoisybinarysensor,
in European Control Conference (ECC), 2016, pp. 17151721, IEEE, 2016.
[38] G.Fellouris,G.V.Moustakides,andV.V.Veeravalli,Multistreamquickestchangedetection:Asymptoticoptimalityundera
sparsesignal,inIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2017,pp.64446447,
2017.
[39] B. Huang, K. Cohen, Q. Zhao, ”Sequential Active Detection of Anomalies in Heterogeneous Processes,” arXiv preprint
arXiv:1704.00766
[40] C.Wang,K.Cohen,Q.Zhao,“ActiveHypothesisTestingonATree:AnomalyDetectionunderHierarchicalObservations,”
to appear in proceedings of ISIS, 2017.
[41] P. Chareka, O. Chareka, S. Kennendy, “Locally Sub-Gaussian Random Variable and the Stong Law of Large Numbers,”
Atlantic Electronic Journal of Mathematics, vol. 1, no. 1, pp. 75-81, 2006.
[42] R. Vershynin, “Introduction to the Non-Asymptotic Analysis of Random Matrices,” available at
http://arxiv.org/abs/1011.3027v6.
[43] S. Bubeck, N. Cesa-Bianchi, G. Lugosi, “Bandits with heavy tail,”arXiv:1209.1727 [stat.ML], September 2012.
APPENDIX A
Proof of Lemma 1. The proof of Lemma 1 is based on concentration inequalities for Sub-
Gaussian distributions.
23
We prove inequality (9) here. The other case, µ < η (11), can be proven similarly.
2ξlog 2T3
P X(T)+ β < η
s T
(cid:20) (cid:21)
2ξlog 2s3
P supX(s)+ β < η
≤ s s
s
(cid:20) (cid:21)
2ξlog 2s3
∞
P X(s)+ β < η
≤ s s
s=1 (cid:20) (cid:21)
X
∞
2s3
exp( log ) (19)
≤ − β
s=1
X
∞ p
=
2s3
s=1
X
β.
≤
Inequity (19) is obtained by (2).
We next analyze the E[T] for µ > η. Let s 0 = min { s ∈ N : 2log s 2 α s3 ≤ µ − 2 η,s > 1 } , for
q
n s :
0
≥
2ξlog 2s3
P[T n] P sup s : X(s)+ β > η, and
≥ ≤ s s
(cid:20) (cid:26)
2ξlog 2s3
X(s) α < η n
−s s ≥
(cid:27) (cid:21)
2ξlog 2s3
P sup s : X(s) α < η n
≤ −s s ≥
(cid:20) (cid:26) (cid:27) (cid:21)
∞ 2ξlog 2s3
P X(s) α < η
≤ −s s
s=n (cid:20) (cid:21)
X
∞ 2log 2s3
P X µ < α (20)
s
≤ − −s s
s=n (cid:20) (cid:21)
X
∞
2s3
exp( log )
≤ − α
s=n
X
∞ α
≤ 2s3
s=n
X
α
.
≤ 4(n 1)2
−
24
Notice that (20) holds because n s . We can write E[T] in terms of P[T n] as
0
≥ ≥
∞
E[T] = P[T n] (21)
≥
n=0
X
∞
= s + P[T n]
0
≥
n
X
=s0
∞ α
s +
≤ 0 4(n 1)2
n
X
=s0 −
s +1.
0
≤
For the last inequality notice that s is defined to be bigger than 1. It remains to find s . Note
0 0
that for all x > 0 we have logx < √x so loglogx < log√x = 1 logx. For s = 48 log
24√3
α
2
2 (µ η)2 (µ η)2
− −
2s3 2
log = 3log 3 s
α α
r
24 3 2
2 48
α
= 3log 3 log
α(µ η)2 (µ qη)2
r
− −
24 3 2 2
2 24
α
= 3log 3 +3loglog
α(µ η)2 (µ qη)2
r − (cid:18) − (cid:19)
2 24 2 24
3log 3 +3log 3
≤ α(µ η)2 α(µ η)2
r r
− −
2 24
= 6log 3
α(µ η)2
r
−
(µ η)2
= 6 − s.
48
Thus, for s = 48 log
24√3
α
2
,
(µ η)2 (µ η)2
− −
2log 2s3 µ η
α − .
s s ≤ 2
So, we have the following upper bound for s
0
24 3 2
48
α
s log +1. (22)
0 ≤ ⌈(µ η)2 (µ qη)2⌉
− −
The addition of 1 is because s is defined to be bigger than 1. Thus,
0
24 3 2
48
E[T] log α +2, (23)
≤ (µ η)2 (µ qη)2
− −
25
which completes the proof.
APPENDIX B
Proof of Theorem 1. Anupperboundonthesamplecomplexityoftest isprovidedinLemma1.
L
Here, we establish an upper bound on the number of times that test is called in CBRW. First,
L
consider the case of leaf-level target setting.
In order to analyze the trajectory of the random walk, we consider the last passage time T
l
of the random walk from each subtree . We prove an upper bound on E[T ] for each l which
l l
T
gives an upper bound on the total number of times that test is called. Notice that the total
L
number of times that test is called is not bigger than 2 L E[T ].
L l=1 l
The random walk initially starts at the root node at di P stance L from the target. Define the
parameters W as the steps of the random walk: W = 1 if the random walk moves one step
t t
further from the target at time t, W = 1 if the random walk moves one step closer to the
t
−
target, and W = 0 when the random walk does not move. Clearly, τ W = L where τ is
t t=1 t −
the stopping time of the random walk. The random walk stops when the policy declares a leaf
P
node as the target. For the mean value of W , from Lemma 1, we have
t
E[W ] = P[W = 1] P[W = 1]
t t t
− −
1 2(1 p )2
0
≤ − −
< 0.
Notice that if the random walk is within the subtree at step t, we have
L
T
t
W > 0. (24)
s
s=1
X
Thus, we can write
t
P[T > n] P sup t 1 : W > 0 > n (25)
L s
≤ { ≥ }
(cid:20) s=1 (cid:21)
X
t
∞
P W > 0
s
≤
t=n (cid:20) s=1 (cid:21)
X X
∞ 1
exp t(1 2(1 p )2)2 (26)
0
≤ − 2 − −
t=n (cid:18) (cid:19)
X
exp( 2n(1 2(1 p )2)2)
0
= − − − .
1 exp( 2(1 2(1 p )2)2)
0
− − − −
26
Inequity (26) is obtained by Hoeffding inequality for Bernoulli distributions. We can obtain
E[T ] from P[T > n] based on the sum of tail probabilities as
L L
∞
E[T ] = P[T > n]
L L
n=0
X
∞ exp( 2n(1 2(1 p )2)2)
0
− − −
≤ 1 exp( 2(1 2(1 p )2)2)
0
n=0 − − − −
X
1
= .
2
1 exp( 2(1 2(1 p )2)2)
0
− − − −
(cid:18) (cid:19)
Let us define
1
C = , (27)
p0 2
1 exp( 2(1 2(1 p )2)2)
0
− − − −
(cid:18) (cid:19)
which is a constant independent of K and ǫ. From the symmetry of binary tree, it can be seen
that E[T ] C for all l and the expected number of points visited by the random walk is upper
l
≤
p0
bounded by 2LC . Under the assumption that the informativeness of observations decreases in
p0
higher levels we can replace the sample complexity of test at the highest level and arrive at
L
the first term in (14). The second term in (14), is obtained by direct application of Lemma 1 on
the sample complexity of test at the target node.
L
It remains to show that CBRW satisfies the reliability constraint. We know that at each visit
of a leaf node the probability of declaring a non-target node as the target is lower than ǫ
2LCp0
by the design of the test at leaf nodes. Thus, from the upper bound on the expected number of
points visited by the random walk we have
ǫ
P[ = (k ,0) ] 2LC
S
δ
6 {
0
} ≤
p02LC
p0
= ǫ.
Order optimality of logK follows from information theoretic lower bound. Order optimality
of log 1 can be established following standard techniques in sequential testing problems.
ǫ
An upper bound on the sample complexity of CBRW under hierarchical target setting can be
obtained similarly. The trajectory of the random walk can be analyzed by considering the last
passage times T of the random walk from subtrees for l = l +1,...,L, as well as the last
l l 0
T
27
passage times T and T of the random walk from subtrees and which can be shown to
1′ 2′ T1′ T2′
not bigger than CH following the similar lines as in the proof of Theorem 1 with
p0
1
CH = . (28)
p0 2
1 exp( 2(1 2(1 p )3)2)
0
− − − −
(cid:18) (cid:19)
The analysis under hierarchical target setting differs from the analysis under leaf-level target
settinginthat theconsecutivecalls oftest on thesamenoderesultsin increasing theconfidence
L
level. We establish an upper bound on the expected total number T of observations from a
tot
node at a series of consecutive calls of test on the node where the confidence level is divided
L
by 2 iteratively at each time. Let T(k) be the number of samples taken at k’th consecutive call
of test on a node. By design of CBRW strategy under hierarchical target setting the value
L
of p in test is divided by 2 until the first time k that p0 < ǫ . Thus there are at most
L 2k − 1 3LC p H 0
log
3LC
p
H
0
p0
consecutive calls of test on one node. On a non-target node:
⌈ 2 ǫ ⌉ L
log
3LCp H
0
p0
⌈ 2 ǫ ⌉
E[T ] pk 1E[T(k)]
tot
≤
0−
k=1
X
24 3 2k
∞ pk 1 48 log p0 +2
≤
0−
(µ η)2 (µ qη)2
k=1 (cid:18) − − (cid:19)
X
24 3 2
∞
pk 1
48
log
p0
+2
≤
0−
(µ η)2 (µ qη)2
k=1 (cid:18) − − (cid:19)
X
∞ 48
+ pk 1 log√3 2k 1
0− (µ η)2 −
k=1 −
X
24 3 2
1 48 p0
= log +2
1 p (µ η)2 (µ qη)2
− 0(cid:18) − − (cid:19)
p 16log2
+ 0 . (29)
(1 p )2(µ η)2
0
− −
Upper bound on E[T ] in a conservative upper bound on each single time that the test is
tot
L
called.
28
On the target node:
log
3LCp H
0
p0
⌈ 2 ǫ ⌉
E[T ] E[T(k)]
tot
≤
k=1
X
3LCHp 48 24 3 4
log
p0 0
log
ǫ
+2
≤ ⌈ 2 ǫ ⌉ (µ η)2 (µ qη)2
(cid:18) − − (cid:19)
6LCHp 48 24 3 4
log p0 0 log ǫ +2 . (30)
≤ 2 ǫ (µ η)2 (µ qη)2
(cid:18) − − (cid:19)
From the upper bound on E[T ], the upper bound on the sample complexity of CBRW can
tot
be obtained. The satisfaction of the constraint on error probability can be shown similar to the
leaf-level target setting.