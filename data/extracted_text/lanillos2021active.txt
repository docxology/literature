IEEETEMPLATE.SUBMITTEDVERSION 1
Active Inference in Robotics and Artificial Agents:
Survey and Challenges
Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata,
Alexander Tschantz, Beren Millidge, Martijn Wisse, Christopher L. Buckley, and Jun Tani
Abstract—Activeinferenceisamathematicalframeworkwhich
originated in computational neuroscience as a theory of how the
brain implements action, perception and learning. Recently, it
has been shown to be a promising approach to the problems
of state-estimation and control under uncertainty, as well as
a foundation for the construction of goal-driven behaviours in
robotics and artificial agents in general. Here, we review the
state-of-the-art theory and implementations of active inference
for state-estimation, control, planning and learning; describing
current achievements with a particular focus on robotics. We
showcase relevant experiments that illustrate its potential in
termsofadaptation,generalizationandrobustness.Furthermore,
we connect this approach with other frameworks and discuss its
Fig. 1. Predictive brain approach. (a) Dallenbach illusion [3] and the expectedbenefitsandchallenges:aunifiedframeworkwithfunc-
importance of prior information in perception. (b) The escalator effect.
tionalbiologicalplausibilityusingvariationalBayesianinference. Involuntaryactivestrategiestriggeredbyapredictedworldstateusingprior
knowledgeacquiredthroughexperience.
Index Terms—Active Inference, Robotics, Predictive coding,
sensorystreamtoformagivenpercept.Algorithmicallyithas
Bayesian Estimation and Control.
been suggested this is achieved by minimizing the difference
(error) between top-down predictions of this internal model
I. INTRODUCTION
and incoming sensory data. Importantly, under this predictive
THIS survey presents the current work in active inference coding framework [4], it has also been suggested that agents
forroboticsandartificialagentsanddiscussesthebenefits can also act on the world to change the sensation to better fit
andchallengesitmustaddressifitistobecomeapracticaland thesameinternalmodelthusprovidingadualaccountofboth
revolutionary unified mathematical framework for estimation, perception and action [5], [6]. Again phenomenal experience
control, planning and learning. is supportive of this active component [7]. For example when
Activeinference(AIF)isabiologicallyplausiblemathemat- reachinganescalator(Fig.1b),evenifitisbroken,weperceive
ical construct based on the free energy principle (FEP) pro- it moving and we prepare our body to fit the velocity of the
posed by Karl Friston [1]. This principle describes how living stairandareoftensurprised.Oncewerealizethatitisstopped,
systemsresistanaturaltendencytodisorder.Itsbackbonecan we adapt again to the new situation.
be traced back to the work of Helmholtz on perception [2]. In Hence, adaptive behaviour can be viewed as an active
the presence of uncertain stimuli, such as the Dallenbach illu- inference process in which the agent selects those actions that
sion [3] depicted in Fig. 1a, our perceptual apparatus tends to support the maximization of model evidence, or equivalently,
fill in for missing information by utilizing prior knowledge, a the minimization of surprise. This can be performed by
processreferredtoasunconsciousinference[2].Fig.1ashows exploiting the internal model or generating exploratory be-
a cow rotated 90 degrees clockwise that is impossible to see havioursthatreducethemodelentropy:i.e.,reduceuncertainty
beforethepresenceofthispriorinformationandimpossibleto by maximizing information gain. This work describes how
un-seeafterwards.Oneimplicationofthistheoryisthatitsug- these concepts can be applied to and enrich robotic systems.
gests the brain maintains an internal model, i.e., a generative
model, of the causes of sensation which is combined with the
A. Overview
P.LanillosiswiththeDondersInsituteforBrainCognitionandBehavior, We formalize and describe AIF in the context of current
Dept.ofArtificialIntelligence,RadboudUniversity.Netherlands.
challengesinrobotics,whereadaptationtouncertain,complex
C. Meo, C. Pezzato, A. A. Meera and M. Wisse are with the Robotics
Institute,DelftUniversityofTechnology.Netherlands. and changing environment plays a major role. We survey
B. Millidge is with the MRC Brain Networks Dynamics Unit, Oxford roboticsapplicationsofAIFandprovidetheappropriatemath-
University,UK.
ematical and theoretical background. Hence, aiming to offer
M.BaioumyiswiththeOxfordRoboticsInstitute,OxfordUniversity,UK
A.TschantzandC.L.BuckleyarewiththeDept.ofInformatics,University bothareviewandatechnicalreference.TableIsummarisesthe
ofSussex,UK. most relevant works organized into: state-estimation, control,
W. Ohata and J. Tani are with with the Okinawa Institute of Science and
planning (i.e., computing actions into the future), and high-
Technology,Japan.
Thismanuscriptiscurrentlyunderreview. level cognitive skills (e.g., self-other distinction).
1202
ceD
3
]OR.sc[
1v17810.2112:viXra
2 IEEETEMPLATE.SUBMITTEDVERSION
TABLEI
OVERVIEWACTIVEINFERENCE(AIF)STATE-OF-THE-ARTINROBOTICSANDCONTROL
ResearchTopic ApproachandImplementation References
noitamitsE
Linearsystemswithcolorednoise DynamicExpectationMaximization [8],[9],[10],[11]
Multi-sensoryestimationandlearning Predictivecoding [12],[13]
Localization Laser-basedcontinuousAIF [14]
lortnoC
Continuoustorquecontrol.Lowdimensionalinput [15],[16],[17],[18]
Humanoidrobotsandmanipulators
High-dimensionalinputwithfunctionlearning(DeepAIF) [19],[20],[21]
Fault-tolerantsystems Thresholdonthesurprise,precisionlearning [22],[23]
Bio-inspiredagents Phototaxis,ContinuousAIF [24]
gninnalP
[25],[26],[27],[28],
AIFwithrewards
[29],[30]
Discrete-timestochasticcontrol DiscreteInference [31],[32]
DeepAIF [33],[34],[35],[36]
DeepAIF [37],[38],[39],[40]
Navigation
Recurrentspikingneuralnetworks [41]
evitingoC
Symbolicreasoning AIF+behaviortrees [42]
Humanrobotinteraction Imitativeinteractionsviavisio-proprioceptivesequences [43],[44],[45],[46]
Self/otherdistinction-Self-recognition Robotmirrortestusingmovementandvisualcues [47],[48]
B. Paper structure inference,e.g.,usuallyusingstochasticvariationalapproaches
and transforming the inference problem into an optimization
Section II introduces the common ground and notation.
problem.InAIFtheinternalstateisconditionallyindependent
State-estimation, control, planning, learning, and hierarchical
of the external state (world). However, they can affect each
representations are mathematically described in Sec. III, Sec.
other through sensory and action states [49]. An important
IV and Sec. V respectively. Section VIII details relevant
consequence is that the robot encodes as preferences its
robotic experiments that showcase the advantages of AIF
intentionsandthesepreferencesdriveboththestate-estimation
approaches.Finally,Sec.IXdescribestherelationofAIFwith
and the control.
other frameworks, such as classical control and reinforcement
learning, and Sec. X discusses the benefits and challenges to To help new robotic researchers in AIF to get started, we
makeAIFastandardmodellingtechniqueforroboticsystems. provide a very distilled summary of works in Table II. It
contains what the authors consider seminal papers that led
II. ACTIVEINFERENCECOMMONGROUND to the current advancements in the state of the art with the
focus on the control and robotics communities.
We introduce the standard equations and concepts from the
AIF literature, and the notation used in this paper, framed for
estimation and control of robotic systems. TABLEII
AIF solves the dual problem of estimation and control by GETTINGSTARTEDWITHACTIVEINFERENCE
optimizing a single objective: a free energy bound [1], [5],
[6]. This entails updating the internal state and generating Topic References
the control actions that minimize the error in the predicted Generalintroductionandtutorial [50],[5],
observations. Hence, AIF has the particularity that the gen- Derivationsforrobotcontrol [13],[17]
Relationshipwithclassicalcontrol [51],[18]
erative model of actions is cast into a generative model of
Relationshipwithoptimalcontrol [52],[53]
predicted observations or inputs. In contrast to other similar DiscreteActiveInference [54]
approaches, control actions are generated to make the world RLandactiveinference [55],[25]
Stateestimation [56],[9]
more predictable and the least surprising. Figure 2b sketches
Predictiveprocessing [57],[58]
the hierarchical AIF, where the lowest level is the sensory HumanRobotInteraction [45]
input. Estimation and control is solved through Bayesian Neuroscientificfoundations [1],[59]
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 3
Fig. 2. Active inference in robotics. (a) Estimation, control, planning and learning using the AIF framework. (b) Hierarchical abstract description, error
messagepassingandsubdivisionofestimation(usingpastinformation),adaptivecontrolandplanningofthefuturepredictionsandactions.
Forconsistency,wewillusenotationdescribedinTableIII. p(y) is the marginal likelihood, which corresponds to the
Figure 2b describes the AIF architecture with the sensory probability of observing y regardless of the state. The goal
observations y and the latent states x˜. Variables can be is to find the value of x which maximizes the posterior.
matrices or tensors. For instance, the sensory observation 1) Variational Free Energy: Now, we model the influence
y may have the dimension of every sensor modality (e.g. of the world on the agent as the tendency of the agent to find
visual and joints angles) and their higher-order derivatives1— an equilibrium between its internal model and the external
for rotational joints these are the angles, the joints velocities, process. This is the core of the free energy principle [1],
accelerations, etc. whichinBayesianterminologystatesthattheagentmaximizes
model evidence by approximating the internal state described
TABLEIII by the density q(x) to the world posterior p(x|y). Besides
NOTATION thebiologicalmotivation,thereisalsoacomputationalreason
for introducing this variational density. The posterior p(x|y)
Var. Definition Function[al] Definition
is typically intractable and cannot be evaluated directly in
x Processstate g Observationmodel most cases, particularly in continuous spaces. In the free
x˜ Internal/inferredstate f Dynamic/transitionmodel
energy principle, a variational Bayes approach is used to
y Observations Π=Σ−1 Precision/Inversevariance
v Generalcauses ∇x˜ Partialderivativew.r.t.state obtain a tractable solution2. Instead of computing an exact
u Controlactions q(x) Variationaldensityofthestate
posterior, it is approximated through optimization [61], [62].
z Observationnoise F Variationalfreeenergy
w Internalstatenoise KL Kullback–Leiblerdivergence This approach requires the auxiliary variational density q(x).
TheideaistominimizetheKullback-Leiblerdivergence(KL)
between q(x) and p(x|y), as the divergence will approach to
A. From Bayesian Inference to the Free Energy Principle
0 when both distributions are the same.
We will start by designing an agent that does not have (cid:90) q(x)
access to the world/body state but has to infer it from the KL[q(x)||p(x|y))]= q(x)log dx
p(x|y)
sensormeasurements.IntermsofBayesianinference,itinfers
(cid:90) q(x)
the most probable state of the world x using imperfect noisy =− q(x)log dx+logp(y)
sensory observations y. According to Bayes rule, p(y,x)
=F +logp(y)≥0. (2)
p(x,y) p(y|x)p(x)
p(x|y)= p(y) = p(y) . (1) F is defined as the variational free energy3 (VFE) and mea-
sures the divergence between the variational density q(x) and
The probability of a state x given the observed data y is
thejointdistribution(generativemodel)p(y,x).TheVFEcan
encoded in the posterior probability p(x|y). The likelihood
be evaluated because it depends on q(x) and the knowledge
p(y|x) measures the compatibility of the sensory input with
about the environment of the agent p(y,x)=p(y|x)p(x).
the state, while the prior probability p(x) is the current belief
about the state before receiving the observation y. Finally, 2Thevariationalinferenceapproachiscommoninmodernmachinelearning
forapproximatingprobabilitydensities[60].
1IntheAIFterminologythiswaytoencodeobservationandstatesiscalled 3In machine learning the negative VFE is also known as the Evidence
generalizedcoordinates—SeeAppendixDforadetailedexplanation. LowerBound(ELBO).SeeAppendixAfordemonstration.
4 IEEETEMPLATE.SUBMITTEDVERSION
Interestingly, because the KL is always positive the VFE Where the transition model p(x˜ |x˜ ) collapses to p(x˜ )
k k−1 0
is an upper bound on the surprise: F ≥ −logp(y), which when k = 0, p(y |x˜ ) is the likelihood of the observations
k k
measures the atypicality of events quantified through the given the state (observation model).
negativelogprobabilityofsensorydata.Therefore,optimizing Wecanalsodescribethegenerativemodelfromthedynam-
F is equivalent to evaluating the posterior density. In the ical systems approach using state-space equations:
ideal case (e.g., no noise), when the model is able to capture
x˜˙ =Dx˜ =f(x˜,v)+w internal state dynamics (7a)
the real generative process KL[q(x)||p(x|y))] is zero and
F becomes the marginal likelihood or surprise. The key y=g(x˜)+z observation model (7b)
advantage of this formalism is that it reduces the intractable
Both equations encode the generative model described in
BayesianinferenceproblemgiveninEq.1intoanoptimization
Eq. 6. Equation 7a describes the evolution of the internal
problem. Crucially for AIF agents, the state and action are
state—using the prior information or desired preference—and
simultaneously inferred by optimizing F.
Eq. 7b describes the generative model of the causes, usually
2) Mean-field and Laplace approximation: We did not
simplified to the likelihood of the sensory output given the
describe yet the nature of the auxiliary density q(x) that
internal states. Here x˜, y are the estimated state and output,
is encoded by the internal state of the system. Here, for
andwandzaretheprocessandobservation(Gaussian)noise.
mathematical convenience and to reflect the majority of the
Finally, Dx˜= dx˜ is the time-derivative of the state vector5.
active inference literature, we choose q(x) as a factoriza- dt
The functions that describe the generative model g(x˜) and
tion of random variables with known form, i.e., Gaussian
f(x˜,v) can be explicit or function approximators, such as
q(x)∼N(x|x˜,Σ ),andtrackthedensitieswiththesufficient
Q neuralnetworks.Accordingtothemodeltypechosen,different
statistics defined by the mean and the variance (see Appendix
behaviours of an agent can be achieved.
BandC).Otherformsofvariationalapproximationsareoutof
As a useful and particular example, in the case of a linear
thescopeofthispaper.TheVFE(F inEq.2)isbydefinition:
plant, the agent’s generative model can be described as:
(cid:90) q(x)
F = q(x)log dx (3) x˜˙ =Ax˜+Bv+w; y=Cx˜+z,
p(y,x)
(cid:90) (cid:90)
where A, B and C are the plant specific matrices and v here
= q(x)logq(x)dx− q(x)p(y,x)dx. (4)
is the control input.
The VFE under the mean-field and Laplace approximations
simplifies to4:
C. Perception and action
1
F =−logp(y,x˜)− log(2πΣ ). (5) As stated at the beginning of Sec. II, AIF agents perceive
2 Q
and act in the environment by optimizing the same objective,
wherex˜arethesufficientstatisticsofthefactorizedvariational i.e., the VFE [1], [13]. Perception involves the estimation
densitythatcodifiestheprocessstatex,andΣ Q istheoptimal of states x˜ and parameters (e.g., A,B,C), whereas action
variance that optimizes the variational free energy. has a dual role of resolving uncertainty and obtaining new
One of the advantages of using these approximations is data which concords with the agent’s belief/intention. Action
that estimation and control becomes a quadratic optimization involves the computation of the control signal u to act in
problem that explicitly minimizes the model error prediction. the environment. Together, the optimization of VFE through
We will show this explicitly in the following sections. perception and action drives the agent towards reducing the
prediction error and producing better sensory predictions.
B. Generative models
Assumingthatthevariationaldensityisdescribedbythelatent
We differentiate two functionals [63]: the generative pro- variable x˜ they solve the following equations:
cess, which defines the real system in the environment that is
x˜=argminF(x˜,y,v) (8a)
responsible for data generation—usually referred to as ’the
x˜
plant’ in control engineering— and the generative model,
u=argminF(x˜,y(u),v) (8b)
which describes the agent’s internal representation (approx- u
imation)ofthegenerativeprocess.TheAIFagent’sbehaviour Both equations are usually solved through gradient descent. It
is driven by the generative model. This model can be defined isimportanttohighlightthatoriginalworksonactiveinference
by an expert designer (operational specification) or learnt do not explicitly model the action in the generative model as
through interaction with the world. itisencodedwithintheobservationy(u).Alternativesonthis
The generative model of the system at instant k given all formulation have been left for the discussion section.
past observations and states is defined as the joint distribution
over states and observations: III. STATE-ESTIMATION
T
(cid:89) This section summarizes the mathematical formalization
p(y ,x˜ )= p(y |x˜ )p(x˜ |x˜ ) (6)
0:T 0:T k k k k−1 of the first of the four blocks treated in this paper: state-
k=0
estimation. We solve state-estimation similarly to applying
4NotethatthefirsttermofEq.(4)vanishesandthesecondtermbecomes
Eq.5.SeeAppendixCforashortexplanationofthemeanfieldandLaplace 5Anexplanationofthisoperationandthegeneralizedcoordinatesusedin
approximations. continuousAIFcanbefoundintheAppendixD.
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 5
a Bayesian filter. This involves the estimation of two com- future research can focus on the smoothness estimation of the
ponents: the mean estimate and the associated confidence coloured noise. Although a few attempts have been made in
(precision) of the estimate. Under the free energy principle providing theoretical guarantees of stability and convergence
framework,bothcanbeestimatedusingthefirsttwogradients for the estimation involving generalized coordinates [66],
of the free energy. State inference is driven by the following [9], [8], there is a huge scope for the proofs for optimality
differential equation: guarantees.
x˜˙ =Dx˜−κ ∇ F(x˜,y) (9)
x x˜
IV. CONTROL
The VFE under the Laplace and mean-field approximations
Hereweextendtheprevioussectionintroducingthecontrol
has closed form and is defined as:
actions. This is, how the robot both estimates its state and
F(x˜,y)(cid:44)−lnp(x˜,y)=−lnp(y|x˜)p(x˜) computes the control actions by filtering the information
from previous observations according to its internal model
(cid:44)(y−g(x˜))TΣ−1(y−g(x˜))
z dynamics. This can be seen as a low-level control where the
+(Dx˜−f(x˜,v))TΣ−
w
1(Dx˜−f(x˜,v))
actions correct for external and internal perturbation.
1 1 AIFrobotsusethesameobjectivefunctionforbothestima-
+ ln|Σ |+ ln|Σ | (10)
2 w 2 z tionandcontrol:theVFE.Thereby,theagentnotonlyupdates
its state influenced by the world but can also apply actions to
A. State-estimation minimizes the prediction error change the state of the world. This happens as an indirect
By defining ε = (y−g(x˜)) and ε = (Dx˜−f(x˜,v)) consequence of actively sampling sensory data that is more in
y x˜
as the sensory and state model prediction errors in Eq. 10, line with what is predicted by the internal model. Actions in
∇ F becomes the gradient of the weighted sum of squared active inference play a fundamental role in the approximation
x˜
predictionerrors.ThisisveryrelevantasitconnectsAIFwith of the real distribution, acting also in the marginal likelihood
the theory of predictive coding in the brain [64]. by changing the real robot’s configuration and modifying the
sensory input y. The AIF robot is driven by:
∇ F =∇ (ε(cid:62)Σ−1ε )+∇ (ε(cid:62)Σ−1ε ) (11)
x˜ x˜ y z y x˜ x˜ w x˜
x˜˙ =Dx˜−κ ∇ F(x˜,y) (13a)
x x˜
B. Confidence in estimation (cid:88)
u˙ =−κ ∇ y·∇ F(x˜,y) (13b)
The inverse variance or precision (Σ−1 =Π) of the state u u y
estimate represents the agent’s confidence in estimation. This y
precision also minimizes the free energy, simplified as the In AIF the control actions u steer the system towards
negativecurvatureoftheinternalenergyattheestimatedstates
minimizing the prediction errors in F. This is again achieved
[65], [8]:
through gradient descent. However, the VFE described is not
Π =−∇ F =−
(cid:0)
D−∇ f(x˜,v)
(cid:1)TΠ (cid:0)
D−∇ f(x˜,v)
(cid:1)
a function of the control actions directly, but the actions u
x˜ x˜x˜ x˜ w x˜
−∇ g(x˜)TΠ ∇ g(x˜), can influence F by modifying the sensory input. Thus, we
x˜ z x˜
(12) can differentiate, in Eq. 13b, observations w.r.t actions using
the chain rule. κ and κ are tuning parameters that define
where the generalized precision matrices (Π ,Π ) are com- x u
w z
the step size in the iterative update. The partial derivatives of
puted as given in Appendix D.
thesensoryinputwithrespecttothecontrolactionisacentral
AIF in continuous-time (with generalized coordinates) en-
point in active inference which has been tackled in different
ablesustotracktheevolutionoftheprobabilitydensityofthe
ways in past work like [13], [17].
trajectoryofstatesx˜(t)[56],insteadofjustitspointestimates,
therebyendowingthemethodwithanaccuratestateestimate6. Conversely to other control frameworks, the desired state
(goalorreference)isencodedintheinternalstatedynamics—
The key advantage of this model is its ability to leverage the
Eq. (7a)—as a preference. Thus, being away from the desired
generalized coordinates to capture the noise smoothness in
state increases the prediction error hence, generating the
data [9]. However, the information contained in the higher-
control actions that steer the system towards the goal. The
order noise derivatives is less valuable for the estimation
properties and limitations of this formulation where F does
process. Therefore, the noise precision matrices in the VFE
not depend explicitly on the actions u are analysed in the
expression should be designed such that the prediction errors
discussion, as well as possible alternatives.
coming from higher-order derivatives should be weighed less
than those coming from lower-order derivatives. This raises
the importance of noise precision modeling [9]. One of the V. PLANNING
main challenges while using generalized coordinates for real In previous sections, we considered AIF in the context of
robots is that the quality of estimation is highly sensitive continuous-time systems, where it was been cast as a gradient
to the assumed noise smoothness of the signal, especially descentoninstantaneousvariationalfreeenergy(VFE).These
for low noise smoothness [10], [11]. To resolve this issue, sections showed that active inference can naturally describe
estimation and control using a common probabilistic frame-
6When controlling a real robotic system, higher-order derivatives beyond
work. In this section, we show how active inference can be
accelerations are problematic when the noise smoothness properties are
unknown. used to explicitly model future states and observations in an
6 IEEETEMPLATE.SUBMITTEDVERSION
action plan dependent manner, thereby enabling prospective This factorization of the joint distribution over observations,
planning.Thiscapabilityiscrucialformanytasksandenviron- hidden states, and action trajectories will also mirror the
mentswhereactionshavedelayedconsequences,wheresimply structure of our active inference agent’s generative model.
doing what is best in the current moment is not necessarily
what is best in the long run.
To this end, we first introduce the concept of Partially Ob- B. Expected Free Energy
servable Markov decision process (POMDP) that we omitted
To augment active inference for this scenario we introduce
in the introduction for clarity and model the control actions
the concept of the expected free energy (EFE) [68], [69],
as a discrete-time optimization problem. Second, we define
[70]. The EFE quantifies the average free energy of a plan-
the expected free energy of the future (EFE) and finally, we
conditioned trajectory of states and observations, rather than
describe the AIF approach to find the optimal control plan.
the instantaneous free energy. Like instantaneous VFE agents
arethenmandatedtominimizethisquantitythroughbothper-
A. Discrete-time optimization under the Markov assumption ceptionandaction.Animportantterminologicalnoteconcerns
the word “plan” and its relation to the word policy. A plan
Modelling states and observations using generalized
is a sequence of actions π = [u ,u ...u ]. This differs
coordinates—as described in Sec. III—does allow the system k k+1 T
from the policy used in reinforcement learning that refers to
to incorporate knowledge of the future, since generalized
a parameterised action distribution conditioned on the current
coordinates are a Taylor expansion in time around some
state. Interestingly, these control plans naturally confer active
given time point. However, to use generalized coordinates in
inference agents with both reward seeking and exploratory
practice, it is necessary to truncate them at some small order,
behaviours.
meaning they can only model smooth and local changes over
The EFE provides a measure quantifying the notion of the
time. This is not sufficient for many complex control tasks
free energy expected over future trajectories. In effect, the
where the consequences of action may occur far into the
EFE computes the average free energy of a trajectory, taking
future. While there are alternative continuous-time methods
into account the the fact that because future observations are
to model future trajectories, these typically require substan-
unknown they must be considered as random variables to be
tially more mathematical machinery for limited algorithmic
inferred, given a specific plan π. Mathematically, the EFE (G)
gain. Therefore, for reasons of mathematical tractability and
is defined as,
computational(i.e.,statistical)efficiency,AIFagentsplansare
typically constructed in discrete time. Moreover, it is further
G =E [lnq(x |π)−lnp˜(y ,x )] (15)
assumedthattherelationshipbetweenstates,observationsand π q(y k:T ,xk:T|π) k:T k:T k:T
time are described by a POMDP [67]. Intuitively, a POMDP
Like the VFE introduced in previous sections, the EFE quan-
assumesthattherearediscrete-timesequencesofobservations
tifies the difference between a variational density q(x ) and
y and hidden states x 7. It is then assumed that, at k:T
1:T 1:T a generative model p(y ,x ). However, the distributions
some given instant point k, observations y depend only on k:T k:T
k are over trajectories of states and observations, instead of just
the hidden state x . Similarly, it is assumed that the hidden
k the state and observation at a single time-step, and there is an
states at the current time depend only on the hidden states
additional expectation over future observations, meaning that
at the previous time-step x (Markov assumption) and the
k−1 EFEisafunctionalofbothstatesandobservations,asopposed
action at the previous time-step u . Analogously to the
k−1 to only states. In the continuous-time formulation, goals are
continuous-time space state AIF approach described in the
encoded as set-points which, when compared against the cur-
previous sections, only current observation and previous state
rent state estimates, form a prediction error that is minimized
are needed to infer states and actions, instead of the entire
by action. In POMDP formalism of EFE agents, the goals
history of states and observations, improving the tractability
or ‘preferences‘ of the agent are encoded into the generative
of the control problem. Additionally, these assumptions are
modeltoformabiasedgenerativemodelp˜(y ,x ),where
not as restrictive in terms of generality as they first appear k:T k:T
the tilde notation p˜denotes that the model is biased towards
since the states are hidden, they can, by definition, include
predicting the agent’s preferred environment, or equivalent,
whatever information is necessary to ensure that the state at
rewarding states and observations.
some instant k depends only on the state and action at the
previous time-step.
Thedependencystructureentailedbytheplanningovertime
for stochastic variables is as follows, C. Finding the optimal plan
p(y ,x ,u )= One advantage of the EFE is that it allows the derivation
k:T k:T k:T
of an expression for the optimal plan over a trajectory in
T
(cid:89)
p(y |x )p(x )p(u ) p(y |x )p(y |x ,u )p(u |x ) (14) terms of the sum of the EFE’s for each individual time-step.
k k k k t t k k−1 t−1 t t
t=k+1 This is possible due to the statistical factorizations intrinsic
to the definition of the POMDP, whereby the current state
7Again,forreasonsofconceptualsimplicityandmathematicaltractability, only depends on action and the state at the previous timestep.
weonlyconsiderfuturetrajectoriesuptosometimehorizonT.Manyofthe
Specifically, assuming the variational density also factorizes
resultspresentedmayapplyintheinfinitehorizoncaseT →∞,butrequire
morenuancedmathematicalmachinerytodemonstrate. in time so that q(x k:T ) =
(cid:81)T
t=k q(x t ), the EFE of a some
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 7
trajectory can be decomposed into a sum of the EFE for each handle agents which proactively learn to explore large state
individual time-step, spaces with sparse rewards.
T
(cid:88) VI. LEARNING
G(y ,x )= G(y ,x ) (16)
k:T k:T t t
In this section, we summarize how we can learn the gen-
t=k
erative models used in previous sections without the need
where,
of an expert designer. More importantly, function approxi-
G(y ,x )=E [lnq(x )−lnp˜(y ,x )] (17) mations [75], [19], [33], [36] aid the extension of proposed
k k q(y k ,xk) k k k
frameworktocomputationallytractableactiveinferenceagents
Hence,theoptimalplan(whereaplanπdenotesasequenceof
that use high-dimensional inputs and states-spaces.
individualactionsuptosometimehorizonT)isasoftmaxdis-
We can learn the likelihood p(y |x ) and transition model
tribution over the sums of the EFE for each plan-conditioned k k
(or prior) of the generative model—depending on the for-
trajectory. Specifically,
mulation p(x |x ) or p(x |x ,u ); the variational
k k−1 k k−1 k−1
approximate posterior q(x |y ), and the desired observation
k k
G(π)=E [lnq(x ,π)−lnp˜(y ,x ,π)] distributionp˜(y ).Intheliterature,twomainapproacheshave
q(xk:T,y k:T ,π) k:T k:T k:T k
been taken to achieve this: 1) discrete-state-space [71], [54]
T
(cid:88)
=KL[q(π)||p(π) −E [lnq(x |π) (18) (Sec. VI-A) and 2) function approximation for adaptive con-
q(y
k
,xk|π) k
trol [12], [19], [20] (Sec. VI-B1) and planning schemes [36]
k
−lnp˜(y ,x |π)]] (Sec. VI-B2).
k k
T
(cid:88) A. Discrete-state-space
=KL[q(π)||p(π) −G ]
πk
Distributions are described as discrete categorical distri-
k
butions, which explicitly enumerate every possible state and
The optimal posterior is given by is argminG(π)
π explicitly assign a probability to each. In practice, these dis-
(cid:32) T (cid:33) tributions are implemented through normalized matrices and
(cid:88)
=⇒ q∗(π)=σ lnp(π)− G vectorsrepresentingeachstate,oreachstatecombination.For
πk
instance, the likelihood mapping p(y |x ) can be represented
k k k
byanY×X dimensionalmatrixwhereY isthedimensionality
To gain an understanding of the kinds of behaviours that
of the observation space and X is the dimensionality of the
activeinferenceagentswillexhibitinpractice,itisworthwhile
action space. Moreover, in a discrete state space setting, it is
studying the structure of the EFE objective in more detail.
oftentractabletoexplicitlyevaluatetheintegralovertimeand
Crucially the EFE can be decomposed into two terms: an
compute the optimal plan posterior since agents in discrete
extrinsic value term, which scores how close the agent is to
states typically exist in small enough environments such that
achieve its goals, or to maximizing its reward or utility, and
all policies can be explicitly enumerated and evaluated [71].
an intrinsic value term which scores the information gain an
Discrete state space active inference has been widely applied
agent could receive executing some plan. This information
in computational neuroscience to simulate choice behaviour,
gain, mathematically, is simply the distinction between the
e.g., saccades [76] and exploratory behaviour [68].
posterior and prior variational distribution (or the agents’
Discrete state space active inference suffers from clear lim-
‘beliefs’) about the state trajectory—and maximizing this
itations of scalability and expressiveness. First, the restriction
divergence essentially mandates the agent to seek out states
of using discrete categorical distributions means that it must
whicharemaximallyinformative,therebymaximallyreducing
be possible to model the world with a discrete and low-
uncertainty.Ineffect,activeinferenceagentspossessanatural
dimensional set of states (to be able to successfully store the
and inherent desire to seek out and explore novel states of the
full matrices on a digital computer)8. This approach renders
world which will cause them to update their world model.
The EFE can be decomposed as follows—See Appendix E operating in any kind of continuous environment with fine-
for derivation, grained discretisation unfeasible. Second, and more impor-
tantly, explicitly evaluating the path integral in Equation 16
G ≈−E [lnp˜(y )]−E D [q(x |y )||q(x |π)]
π q(yk,xk|π) k q(yk|π) KL k k k by enumerating all policies is an operation with exponential
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue computational complexity in the time-horizon as well as the
(19)
size of the state-space, since this increases with the branching
which involves both extrinsic and intrinsic value terms. The factor of the policy tree. This exponential complexity very
exploratory, information-seeking behaviour induced by the rapidly limits the scalability of this direct form of discrete
intrinsic value term has been extensively studied in the active state space active inference to relatively simple tasks such as
inference literature [68], [71] for its relationship to human the T-maze[68], althoughmore recentstudies are pushing the
notion of curiosity and intrinsic motivation that produces limit on the computational power of this method [77].
exploratory behaviours. Artificial curiosity [72], intrinsic mo-
tivation [73] and goal-directed exploration [74] is crucial for 8Thisproblemmaybeaddressedbyhavingahierarchyofstatesorbyusing
sparsematriceswhereapplicable.Weleaveittofutureworktoseewhether
learning and planning. Furthermore, it has also been applied
thiswouldenablediscrete-statespaceactiveinferencetoscaletoreal-world
productively in reward-based AIF [36], [55], [34], [35] to tasks.
8 IEEETEMPLATE.SUBMITTEDVERSION
B. Deep active inference 2) PlanningwithdeepAIF: Planningaheadcanbemodeled
with deep AIF. Here all densities (even the policy) and
Another approach, which is substantially more scalable,
parameters are learnt by optimizing the EFE. The algorithm
although at the cost of losing performance and convergence
is described in Alg. 2. This deep AIF approach draws heavily
guarantees,aswellasinterpretability,istoinsteadparametrize
fromrecentworkinmachinelearning,especiallydeepRL,and
the distributions with general function approximators [75],
has achieved significant successes at enabling active inference
such as Gaussian processes [12] or artificial neural net-
to be scaled up to challenging RL benchmark tasks, and to
works [33]. This allows, in theory, for any distribution to
meet, and potentially exceed the state of the art in deep RL
be represented faithfully since deep artificial neural networks
[36], [55], [28], [34].
(ANN)canapproximateanyarbitrarynonlinearfunctiongiven
Interestingly, if we explicitly model the parameters in the
sufficientdepthandwidth.Forinstance,thefollowingapprox-
generative and approximate posterior distributions, the EFE
imate posterior
objective also gives rise to an information gain term over the
model parameters, which encourages deliberate exploration to
q (x |y )=N(x ;µ (y ),σ (y )) (20)
φ k k k φ k φ k efficientlysearchtheparameterspace.Theensuingexploration
is usually described as one aspect of artificial curiosity or
can be parametrized as follows,
intrinsic motivation [72], [73], [74]; namely, novelty seeking.
[µ (y ),σ (y )]=f (y ) (21) While this approach allows for the expression of arbitrary
φ k φ k φ k
probability densities, it does not solve the problem of the
wheref (y )representstheprediction(forwardpassinadeep computationallyexpensiveevaluationoftheEFEpathintegral,
φ t
ANN with weights φ) which outputs a predicted mean and used to compute the optimal policy. However, in the deep
variance for the Gaussian posterior as function of the input AIF literature there have been several proposed solutions
y . The parameters φ can be straightforwardly optimized, for which draw either from model-based or model-free RL. One
t
instanceusingNNs,withthebackpropagationalgorithmwhere approach is to use the fact that the EFE satisfies a similar
the loss function is the VFE or the EFE. recursive relationship as the Bellman equation in traditional
RL to derive a bootstrapped estimator similar to a value
When the functions are approximated by deep nets the
function,whichcanbeoptimizedacrossbatches[36].Another
term coined is deep active inference (deep AIF). Here, first
approach is to utilize black-box optimization methods, such
we describe the learning for adaptive control and second we
as genetic algorithms, to directly approximate this quantity
explain how to use learning in planning. Furthermore, in the
through sampling [33]. A final approach, which is closely
next Sec. VII hierarchical learning is detailed.
related to model-based planning, and is more common in the
1) AdaptivecontrolwithdeepAIF: Estimationandadaptive
literature, is to approximate the path integral with samples of
control can be scaled to high-dimensional inputs through
future roll-outs given different policies, which can be simu-
stochasticoptimizationoftheVFEexploitingdeepANNs[6].
lated by the agent using its own generative model to imagine
Algorithm 1 describes the non-hierachical deep AIF for adap-
and score future trajectories [55]. This model-based approach
tive control. The most common approach is to learn the
has been used fairly widely in the literature, and has been
forward model q (x˜)—the sensory output y given the inter-
φ k utilized to develop powerful deep active inference algorithms
nal state x˜ —with self-supervised learning and then use the
k
capableofmatchingtheperformanceofstateoftheartmodel-
VFE optimization for the online estimation and control [19],
based RL systems in MDP problems, such as the Cartpole,
[20]. The state at instant k is inferred using the forward
Acrobot, Lunar-lander [36], the mountain car problem [78];
pass of the network to get the estimated sensory input and
and POMDPS as visual control of the cartpole [34], the car
backpropagating the weighted error (exploiting the Jacobian
racing [37], [79] and the Animal-AI environment [35].
of the network, ∇ q ). The action (e.g., torques, velocities,
x˜ φ
etc.) is computed with an analogous procedure but in this
Algorithm 2 Deep active inference for planning
case the change of the sensory input w.r.t the action should
t b h e is m p o ar d t e ia le l d de ∇ riv y a u ti . ve Th [1 e 3 re ], a [2 re 3]. se T v h e i r s al ap t p ec ro h a n c iq h u c e a s n t b o e r f e u s r o th lv e e r 2 1 : : A G p en p e ro ra x t i i m ve at m e o p d o e s l te p r ( io y r k q , ( x x k k , , π π , , φ φ ) )
3: while k≤T do
extendedtomultimodalsensoryinput[20]andcombinedwith 4: y =env.step(π) (cid:46) Update environment given policy
k
model agnostic AIFs approaches. 5: q(x k |π)←argmin q(xk|π) F(q,y k ) (cid:46) Perception
6: q(φ)←argmin F(q,y ) (cid:46) Parameters learning
q(φ) k
Algorithm 1 Deep active inference for adaptive control 7: q(π)←argmin q(π) G(q,y k ) (cid:46) Control
8: π∼q(π) (cid:46) Select most likely policy
1 2 : : G A e p n p e ro ra x t i i m ve at m e o p d o e s l te p r ( io x r k q , ( y x k k φ , ) φ) 9: return q(x k ,π,φ)
3: while true do
4: y ←env.step(u) (cid:46) Update environment given action
k
6 5 : : (cid:15) (cid:15) y x˜k k ← ← Π Π x y ˜ ( ( D y k x˜ − k − q φ f ( φ x˜ ( k x˜ ) k ) ,v k )) (cid:46) (cid:46) P P r r e e d d i i c c t t i i o o n n e e r r r r o o r r d o y b n s. . VII. HIERARCHICALREPRESENTATION
7: q(x k )≡x˜ k ←Dx˜+∇ x˜ q φ (cid:15) yk ++∇ x˜ f φ (cid:15) x˜k (cid:46) Perception Inprevioussections,wedescribedthestateinAIFasavari-
8: u←−∇ y u∇ x˜ q φ (cid:15) yk (cid:46) Control ational density. This section introduces how to represent the
stateoftherobotinahierarchicalsetting.Usingahierarchical
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 9
recurrent neural network (RNN) we can generate predictive Both learning and action generation is performed through
behaviourexploitingthefreeenergyprinciple.Murataandcol- iterative interactions between the top-down generative process
and the bottom-up inference process. The generative model
leagues [43] developed a variational hierarchically-organized
can be written as factorized:
RNN model, referred to as the Stochastic Continuous-Time
T
RNN (S-CTRNN). This model inherits the dynamic property (cid:89)
p(y ,d ,z |d )= p(y |d )p(d |d ,z )p(z |d )
of the multiple timescales RNN (MTRNN) [80] wherein the 1:T 1:T 1:T 0 k k k k−1 k k k−1
k=1
neural activation dynamics in the higher layer is dominated (22)
Although d is a deterministic latent variable, it can be con-
by slower dynamics by employing a larger time constant and
sidered to have a Dirac delta distribution centered on d˜ as
the one in the lower layer is dominated by faster dynamics
σ(d−d˜).Inthegenerativeprocess,theinformationpropagates
with a smaller time constant. Although this model was suc-
from the highest layer to the lowest layer at each time step
cessfully applied for predictive coding and active inference in the forward direction. At each layer the deterministic latent
of a physical humanoid robot that imitatively interacts with a variabled˜l andthepriordistributionp(zl)arecomputed.The
human-operatedrobot,themodelwaslimitedsincetherandom lowest lay k er computes the sensory predi k ction yl. First, d˜ is
latentvariableisallocatedonlyintheinitiallatentstateofthe computed as shown in Eq. (23) where its inte k rnal value is
MTRNN. denoted by h.
By considering that random latent variables should be
d˜l =tanh(hl)
introduced not only in the initial time step but also all time k k
(cid:18) (cid:19)
1
steps during the sequential processing, another variational hl k = 1− τl hl k−1 (23)
RNN,so-calledthepredictivecodinginspiredvariationalRNN
1 (cid:16) (cid:17)
(PV-RNN) [81] was developed. This model was inspired by + Wl,ld˜l +Wl,lzl +Wl+1,ld˜l+1+bl
τl d,d k−1 z,d k du,d k k
the sequence prior scheme [82] evinced in a model called
the variational RNN (VRNN) wherein the prior changes where zl k is a sampling of the posterior distribution q(zl k )
over time. PV-RNN using the sequence prior was extended estimated in the last iteration of the inference process. W
incorporating (1) a scheme called error regression to infer represent the learnable parameters in terms of connectivity
random latent variables using prediction error signals during weight matrices between layers and their deterministic and
action generation which is analogous to predictive coding (2) stochasticunits.brepresentsanotherlearnableparameterbias.
the multiple timescale scheme described above. A graphical
Theoutputiscomputedasmappingfromd˜1inthelowestlayer
representation of PV-RNN is shown in Figure 3. In each layer as:
y¯ =Wlld˜1 +b (24)
k yd k y
where p(b ) is a learnable bias.
y
The prior distribution p(z ) takes a Gaussian distribution
k
with mean µp and standard deviation σp. The prior depends
k k
on d˜ by following the sequence prior scheme [82].
k−1
p(z |d˜ )=N(µp,(σp)2) where k >1
k k−1 k k
µp =tanh(Wl,l d˜ ) (25)
k d,z,µp k−1
σp =exp(Wl,l d˜ )
k d,z,σp k−1
Next,theinferenceprocessusingthefreeenergyminimiza-
tion is described. The VFE F can be written as:
F =−E [lnp (y|x)]+KL[q (x|y)(cid:107)p(x)] (26)
qφ(x|y) θ φ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Accuracy Complexity
where p (y|x) is the generative model with learnable pa-
θ
rameter θ and q (x|y) is the posterior inference model with
φ
Fig.3. PV-RNNgraphicdiagram.Itillustratesacaseofactiongeneration parameter φ. The objective of the inference in learning is to
whereintheinferenceisperformedforthepastwindowfromk=1tok=3
obtain optimal values for θ and φ by minimizing the free
andthepredictionforthefuturetimestepk=4isperformed.
energy. Let us consider the posterior inference with a given
sensory observation y for the past-time window from time
l of the network we define the internal state x˜ with two latent 0:T
variables: deterministic dl and random zl, respectively with step k = 1 to T of PV-RNN. The approximate posterior for
k as time step. The rando k m variable zl k is represented by a z 0:T is represented as:
k
Gaussian distribution with mean µ and standard distribution q(z |y )=N(µq,σq)
k k:T k k
σ. The lowest layer contains the output yl k predicting sensory µq =tanh(Aµ) (27)
observation including both exteroception and proprioception. k k
σq =exp(Aσ)
In robotics applications, motor movement can be generated k k
by feeding the prediction of proprioception in terms of target The adaptation variable A represents the parameters φ
0:T
joint angles in the next time step to the motor controller. for the posterior inference model q . With considering the
φ
10 IEEETEMPLATE.SUBMITTEDVERSION
generative process described in Eq. 23 and Eq. 25, the free causes. For this reason, there is no need for precise inverse
energy F(φ,θ) for PV-RNN can be obtained as: dynamical models which might be problematic to compute.
While compared to optimal control, AIF is quite appealing
T
F(φ,θ)=− (cid:88) (E (cid:2) logp (y |d˜ ) (cid:3) for robotic applications where the dynamics of the robot or
qφ(zk|y k:T ) θx k k the task are uncertain. However, other challenges appear, e.g.,
k=1 (28) inverse dynamic modelling is shifted to the design/learning of
L
+ (cid:88) KL (cid:2) q (z |y )||p (z |d˜ ) (cid:3) ) meaningful generative models and prior beliefs (preferences).
φ k k:T θz k k−1
Despitethechallenges,AIFisshowinghugepotentialforreal
l
robotic applications [6] for estimation, adaptive control, fault-
where the first term is the accuracy and the second term
tolerant control, prospective planning and complex cognition
is the complexity as corresponding to those in Eq. 26. In
skills (i.e., human-robot collaboration, self/other distinction).
practice, the generative process—sweeping from the higher
In the following subsections we describe experiments for:
layer to the lower layer and from time step 0 to T—is
A) estimation, B) adaptive control, C) fault-tolerant control,
performed by following Eq. 23 and Eq. 25 using A and
1:T
D) planning and E) complex cognition.
the connectivity weight W updated in the previous iteration.
The inference process—sweeping from the lower layer to the
higher layer and from time step T to 1—is implemented A. Estimation
with back-propagation through time (BPTT) [83]. This uses
VFEoptimizationhasbeensuccessfullyappliedforadaptive
the error calculated between the output computed in the last
robot estimation and learning in applications, such as body
generativeprocessandthetargetforupdatingthecurrentA
1:T estimation [12], drones state tracking [9] and navigation [39],
andW.Thepairedcomputationforthegenerativeprocessand
[14].
the inference process is iterated until the free energy F(φ,θ)
In [12] the authors proposed a computational model based
isminimized.Afterthelearningprocesshasconverged,action
on predictive coding which allows generic multisensory
generation can be conducted.
integration for inferring and learning its body configuration
The basic architecture described above has been extended
by means of arbitrary sensors affected by Gaussian noise.
andimplementedinvariousroboticexperimentaltasksinclud-
Figure 4a shows the experimental setup. They learned the
ing human-robot imitative interaction [84], [85], dyadic robot
forward model using Gaussian processes. This is particularly
imitative interaction [86], goal-directed planning for robot
useful in the case an accurate model of the body and the
object manipulation [87], and goal-directed planning using
environment is not available, or when self-calibration of
activeinferenceandreinforcementlearninginnavigation[29].
different sensors is needed. Body learning was formulated
as obtaining the mapping that encodes the sensory value
VIII. ROBOTICEXPERIMENTS
dependencyonthejointvariables.Bodyperception/estimation
While the previous sections were devoted to describing the was achieved by minimizing the VFE (through stochastic
mathematical insights of AIF. This section presents selected gradient descent), which iteratively reduces the discrepancy
experiments in the literature that showcase the relevant between the belief of the robot about its configuration and the
characteristics of AIF for robotics. observed posterior. The results showed that different sensor
modalities improve the refinement of the body configuration
Historical preamble. While Friston was developing the basis and allow the body estimation to adapt to the most plausible
ofAIF,thefreeenergyprinciple[1],Taniandcolleagues[88], solution when injecting strong visual-tactile perturbation or in
[89] were investigating models similar to AIF in real robots. the case of missing sensory inputs. Interestingly, the system
In[88],ahierarchicallyorganizedRNNasagenerativemodel was prone to visual-tactile illusions, similarly to how humans
was trained to predict/generate visuo-proprioceptive sequence process body multisensory information [91].
patternsforasetofmovementprimitives.Itwasdemonstrated Based on the Dynamic Expectation Maximization (DEM)
that the robot could successfully adapt its movement pattern [65], [9] proposed a simultaneous state and input observer
to the corresponding movement primitive in real-time when designfortacklingstableLinearTimeInvariant(LTI)systems
the environment changed. However, these models are limited with coloured noise, which was tested on a real system.
because they were predicated on deterministic dynamics The use of generalized coordinates enabled this observer to
perspective instead of a Bayesian perspective which is used outperformtheKalmanfilterforstateestimationandUnknown
in the formal formulation of AIF [90]. A robotic attempt of Input Observer (UIO) for input estimation, under coloured
AIF with its exact Friston’s formalism for reaching tasks was noise. Similar ideas of perception were used to reformulate
described in [15], with a 7-DOF simulated robot arm with the DEM into a blind system identification algorithm [8] for
generative models and parameters known in advance. Finally, the simultaneous estimation of states, inputs, parameters and
Lanillos and colleagues [12], [13], were able to develop and noise hyperparameters of an LTI system. This estimator was
deploy an AIF model on a humanoid robot for estimation shown to outperform other classical estimators for parameter
and control. Concurrently, similar approaches were being estimationundercolourednoise.Ontheapplicationside,DEM
investigated for industrial manipulators [23]. was applied for the perception of a quadrotor flying under
An important concept that emerged in these works is windconditions[10],[11].TheDEMbasedlearningalgorithm
that actions in AIF realize sensory consequences of prior successfully learned the dynamic model of the quadrotor for
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 11
Fig.4. AIF estimation and control benefits.(a)Multisensoryintegration(proprioceptive,visualandtactilebodyestimation)adaptedfrom[12].(b)High-
dimensionalinput(images)state-estimationandcontrolthroughgenerativemodellearning,takenfrom[19].(c)Generativemodelshaveaphysicalmeaning
(backward neural network pass (Jacobian) visualization w.r.t. the elbow and shoulder for a virtual arm model after learning, taken from [21]. (d) Natural
behaviour through VFE minimization (upper-body reaching and head-eyes attention), taken from [13]. (e) AIF provides generative models of action and
permitstrajectoryimagination.(f)Adaptivetointernalandexternalparameter changes (e.g.,stiffness,gravity).Theplotcomparesabuiltincontrollerfrom
thePANDArobotagainsttheAIFusingproprioceptiveinformation(P-AIF,[20])andmultisensoryintegration(M-AIF),adaptedfrom[20].
accurate output predictions when compared to other state- AIF for torque-control in the joint space for industrial
of-the-art system identification methods [10]. The existence arms was shown in [17]. The authors presented an adaptive
of a mathematical proof for stable parameter estimation [66] scheme for controlling a generic n-DOFs robot manipulators,
motivatesitsreliabilityandsafetyforrealroboticapplications. namely Active Inference Controller (AIC). The control law is
These results demonstrate the applicability of DEM as a model-free, lightweight, and it uses proprioceptive sensors for
learning algorithm for future robots to learn their generative position and velocity to control a 7-DOF robot arm in joint
model from sensory data, rendering them with the capability space. By choosing the state of the system to be controlled
to make accurate predictions of the world. as the joint positions, the generative models of the sensory
input simply resulted in the identity mapping. The generative
B. Adaptive control function of the state dynamics was used to impose a specific
1) Withoutlearning: AIFrobotbodyperceptionandaction behaviour. The robot believed that each joint was moving
towards a given target following the dynamics of a chosen
were successfully deployed, for the first time, on a physical
first-order linear system. Furthermore, instead of computing
robot [13]—the iCub humanoid robot. It provided natural
the forward dynamics of the robot manipulator, the authors
behaviours for upper body reaching and head object tracking
in [17] proposed to approximate the partial derivatives of
using proprioceptive and visual sensory inputs. The AIF
F with respect to the control input by just encoding the
algorithm was validated in terms of noise robustness, and
sign of this relationship, relying on the adaptability of the
multisensory integration, and was also applied for reaching
algorithm to compensate for the modelling errors. Note that
andgraspingamovingobject.Figure4eshowsthebehavioural
byspecifyingacertaingenerativemodelinAIF,onecandefine
sequence for one of the tests. The chosen models for active
a robot that thinks it will behave in a particular way. This is
inference allowed to express goals in Cartesian or image
closelyrelatedtothemoreestablishedideaofmodelreference
spaces by specifying attractors in this space and translating
adaptive control (MRAC) [92]. The results showed that the
them into joint space by using the inverse of the Jacobian
AIC greatly outperformed the MRAC in terms of adaptability
matrix. The robot was controlled using velocity commands
to unmodelled dynamics, tuning effort, disturbance rejection,
which allow to easily define the partial derivatives of the
computational effort, and overall performance in pick and
joint position with respect to the control actions considering
place tasks with unknown object weights. A great advantage
discrete updates of the joint positions at each time step. This
of this approach was the ability to transfer from simulation
removed the need for complex inverse dynamical models to
to real robot without re-tuning of the controller while pre-
compute the control actions. The experimental results showed
serving compliant behaviour through torque commands. The
thepotentialofAIFfordualperceptionandaction,particularly
model parameters for the AIC have however been considered
forcounteractingenvironmentalormodelunexpectedchanges.
12 IEEETEMPLATE.SUBMITTEDVERSION
constants and they were seen as tuning parameters for the of actions. Figure 4f shows the M-AIF results with the panda
designer to achieve a smooth response. Also, the Mutisensory manipulator robot when changing external parameters: the
AIF presented in [20] showed better performance than the Jupiter experiment. The gravity parameter was modified to
optimizedfactorycontrollerofthePandarobot(Fig.4f)when 24.79m/s2. According to the end-effector error plot, the AIF
adapting to external and internal parameters changes, formulationachievedbetterperformancewhencomparedwith
such as gravity, stiffness, end-effector inertia and external thebuilt-incontrollerprovidedbythepandacompany.Further
perturbations. tests showed the adaptation properties to input noise and
2) With precision learning: The authors in [18] presented internal parameters changes, such as stiffness.
an evolution of [17] by including online parameters learning In all these methods the computation of the observation
for controller auto-tuning. The authors demonstrated how the model Jacobian (inverse model) is indirect and depends on
minimization of the VFE produces effective state-estimation, the proper representation learning. This Jacobian has a phys-
control for robotic manipulators. They introduced a temporal ical meaning and describes the change in perception or the
parameter τ in the dynamics generative model used in [17] action to be exerted. Figure 4c visualizes the convolutional
corresponding to a variable time constant of the first-order decoder Jacobian of a virtual arm after learning the visual
linearsystemusedasthemodelreferenceforthejointmotions. mapping [21]. The red and blue pixels defines the change on
Basically, the authors showed that the gains of the controller the arm edge w.r.t. to the elbow and shoulder joints.
correspond to the covariance matrices of the observation Other works focused on robot navigation with active in-
model, and that learning the optimal covariance matrices ference. In particular, [38], [37], [39] proposed a method to
resultsinfindingtheoptimalgainsforthecontroller[18].The learngenerativestate-spacemodelsfrompixeldata.In[38]the
results showed improved performance in terms of response authorsapproximatedthevariationalposteriordistributions,as
and robustness compared to a manually tuned AIC, but they well as the likelihood model with two deep neural networks.
also showed some limitations of the standard active inference This allows performing simple navigation tasks using a Kuka
formulation when (hyper-)parameters learning is introduced. YouBot platform in an aisle.
The belief about the current state is intentionally biased 4) Incombinationwithdiscreteplanning: Adaptivecontrol
towards the target to achieve control, which is an uncommon can also be achieved for high-level behaviour with active
thing in the control community. Thus, the state reconstruction inference, as shown in [42]. A discrete formulation of active
is not accurate unless the system reaches the target. This is inference is used in combination with behaviour trees [93] to
reflectedalsointhelearnedcovariancematricesthatconverged provide prior preferences over specific states while adapting
to a much higher value than the Gaussian noise affecting the at runtime to unforeseen contingencies during mobile manip-
controlled system. ulation in dynamic environments. The solution proposed in
3) With function learning: Instead of designing the obser- [42] blended acting and planning to provide a solution for
vationandthedynamicalmodel,asolutionforbodyestimation continual planning and hierarchical deliberation for long term
andcontrolwasproposedin[16],wherefunctionlearningwas tasks in robotics. The core idea is to use behaviour trees as
employed.Inthisapproach,onlyforwardmodelsneededtobe a graphical method to encode priors for the active inference
learned. The authors pointed out how this learning approach algorithm.Thesepriorswereusedtobiasthegenerativemodel
was not simpler than classical inverse dynamics techniques, onlineandtoguidethesearchwithactiveinferencetoprovide
because it required learning the state forward dynamics and adaptive and fast responses to changes in the environment.
theJacobianoftheobservationmodelwithrespecttothelatent The experiments were conducted both in simulation and in a
space. real robot considering two different mobile manipulators and
In [12] AIF estimation with function learning was solved twosimilartasksinaretailenvironment,forinstancestocking
using GP regression when the input is low-dimensional. It an empty shelf. The hybrid combination of active inference
exploited the closed-form equation to compute the partial and behaviour trees provides reactivity to unforeseen events,
derivatives with respect to the body state. A pixel-based deep allowing the mobile manipulators to quickly perform, repeat,
AIFcontroller[19]waspresentedtohandlehigh-dimensional or skip actions according to the state of the environment.
inputs,suchasimages.Figure4bdepictsthePixel-AIFarchi- Crucially, the method also provides safety guarantees and
tecture and a sequence of the dual perception-action inference convergence of the high-level behaviour of the robot to the
process. In the perception row, the arm overlays represent the given goal.
robot visual input and the predicted image. In the action row,
the overlays represent the visual input and the desired goal in
C. Fault-tolerant control
thevisualspace.Thisapproachincorporatedgenerativemodel
learning using convolutional neural networks for one sensor AIF can also help in case of degraded sensory input if
modality (visual) and performed velocity control. Finally, sensoryredundancyisprovidedinthesystem[12].Aspointed
a multimodal variational autoencoder AIF (M-AIF) torque out in [15], if the robot has poor proprioceptive information,
controller [20] was presented. It combined VFE optimization multimodal integration with visual data can compensate and
with generative model learning, extending the previous AIF restore effective control [12]. This property of active infer-
formulations to work with high-dimensional multimodal input ence of naturally fusing different sensory modalities for both
at the torque level. Figure 4e shows the robot imaging its state estimation and control has been taken further for fault-
future trajectory thus, describing AIF as a generative model tolerant control [23], [22].
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 13
The work in [23] proposes the use of the AIC from [17] q(π) is parameterized as a diagonal Gaussian and the cross-
with the addition of visual information on the end-effector entropy method [96] is used optimize the parameters such
position—using the GP learning approach from [12]. The that q(π)∝−G(π). The experiments focused on whether the
authorsproposedaschemeforonlinethresholdgenerationfor algorithm was able to balance exploration and exploitation.
fault detection and isolation of sensory faults based on the The performance was evaluated in domains with (i) well-
sensory prediction errors in the free-energy. Fault recovery shaped rewards (Half Cheetah), (ii) extremely sparse rewards,
when either the proprioceptive or the camera were marked as where agents only receive reward when the goal is achieved
faulty is achieved by simply setting to zero the precision ma- (Mountain Car and Cup Catch) and (iii) a complete ab-
trices (or inverse covariances) of the relative sensors. Results sence of rewards, where there are no rewards and success is
onasimulated2-DOFrobotarmshowedthatthefault-tolerant measured by the percent of the maze covered (Ant Maze).
AICcandetectandrecoverfromfreezingencodersandcamera In sparse rewards environment, deep AIF was compared to
misalignmentprovidingconvergencetoagivengoal.Themain two baselines, a reward algorithm which only selects plans
advantage with respect to standard fault-tolerant approaches based on the extrinsic term (ignores the information gain),
is that fault detection and isolation, as well as fault recovery, andavariancealgorithmthatseeksoutuncertaintransitionsby
do not require the design of additional signals to monitor or actingtomaximisetheoutputvarianceofthetransitionmodel.
alternative controllers besides what is already provided by the For environments with well-shaped rewards, deep AIF was
AIC. However, as AIF is by nature an adaptation mechanism comparedtothemaximumrewardobtainedafter100episodes
biased towards the desired state it hindered fault detection, by a oft-actor-critic (SAC) [97]—state-of-the-art model-free
producing false alarms. In fact, the sensory prediction errors RL algorithm.
in the free-energy can increase due to a changing goal for the
robot manipulator and not necessarily due to a faulty sensor.
This problem can be addressed by introducing an unbiased
AIC controller [22].
Subsequently, the authors in [94] highlighted how fault
detection and recovery can be automatically achieved through
precisionlearning.Thisprovidesamethodforstochasticfault-
detection (the probability of sensory being fault) rather than
deterministic and allowed for fault-tolerant behaviour without
needing any threshold definition.
D. Planning
As described in section V, planning can be cast as optimiz-
ingtheparametersoftheplandensityq(π)withrespecttothe
cost function EFE G(π). This process favors plans which re-
alise an agent’s prior preferences (i.e. goal-driven behaviour),
while at the same time gathering the most information from Fig. 5. AIF planning with rewards. (A) Mountain Car: Average return
after each episode on the sparse-reward Mountain Car task. (B) Cup Catch:
the environment (i.e exploration).
Average return after each episode on the sparse-reward Cup Catch task. (C
A classical illustrative example is modelling saccadic eye & D) Half Cheetah: Average return after each episode on the well-shaped
movements. In [95] they computed the saccades (using ex- Half Cheetah environment, for the running and flipping tasks, respectively.
DeepAIFiscomparedtotheaverageperformanceofSACafter100episodes
pected free energy, EFE) and then executed them through
learning.Eachlineisthemeanof5seeds.
vanilla prediction error minimization (free energy gradients).
In [55], the authors investigated the performance of deep The Mountain Car experiment is shown in Fig. 5, where
AIFinthecontextofplanningwithrewardsinstandardMDP we plot the total reward obtained for each episode over 25
RL benchmarks—See Fig. 5. Implementation-wise, the deep episodes, where each episode is at most 200 time steps.
AIF agent optimizes q(π) at each time step, and executes the These results showed that deep AIF rapidly explores and
firstactionspecifiedbythemostlikelyplan.Thisinvolveses- consistently reaches the goal, achieving optimal performance
timatingtheEFEforeachplan,whichinturninvolvesevaluat- in a single trial. In contrast, the benchmark algorithms were,
ingtheexpectedfuturebeliefsandobservations,giventheplan on average, unable to successfully explore and achieve good
(q(x ,y |π)).Thiscanbeachievedthroughthegenerative performance. Deep AIF performs comparably to benchmarks
k:T k:T
model, whereby beliefs about future hidden states (given the on the Cup Catch environment (Fig. 1B). Figure 1 C&D
current hidden state and plan) are evaluated via the transition shows that deep AIF performs substantially better than a
distribution,andbeliefsaboutobservations,givensomehidden state-of-the-art model-free algorithm after 100 episodes on
state,areevaluatedusingthelikelihooddistribution.Giventhis thechallengingHalfCheetahtasks.Thisreflectsrobustperfor-
counterfactual distribution over future states and observations, manceinenvironmentswithwell-shapedrewardsandprovides
EFE can be approximated [36], [55]. The final requirement considerable improvements in sample-efficiency. The directed
is an optimization procedure which iteratively updates the exploration afforded by minimizing the EFE proves beneficial
plan density such that EFE is minimized. In [55], the plan in environments with no reward structure. Deep AIF rate
14 IEEETEMPLATE.SUBMITTEDVERSION
of exploration was substantially higher than that of a random agent’sintentioncanbeformulatedasapredictivemodel,and
baseline in the ant-maze environment, resulting in a more the congruence between the predicted action outcomes and
substantial portion of the maze being covered. observation could be considered to contribute to the system
NewexcitingdevelopmentsandstudiesofDeepAIFagents being in control of the actions and the consequences [102].
are being under research for more complex environments Hence, when the accuracy term dominant condition, the robot
with partial observability and high-dimensional inputs and is endowed with weaker agency, and in the complexity term
actions [35], [34], [79], [98], [99]. dominant condition, the robot owns stronger agency.
E. Complex cognition
1) Intention-blended Human-robot collaboration: Ohata
and Tani [44] applied the frameworks of predictive coding
and active inference to the social cognition study in which
they investigated the dynamics of intention in human-robot
interactions. In this study, multimodal imitative interactions
betweenahumanoidrobotandahumancounterpartweresim-
ulatedinwhichtherobotandhumanimitateeachother’sbody
movement simultaneously. The imitation task was designed
to reveal the difference between strong intention and weak
intention in social interaction. During imitative interactions
sometimes there were cases where the robot tries to perform
a different movement primitive than the human. In such
conflictingsituations,itcanbeassumedthatiftherobothasa
strongintention,therobotwouldkeepperformingtheoriginal
primitive, and if it has a weak intention, it would change its
intention to adapt to the human’s primitive. Body movements
were implemented using three types of motion primitives (A,
B,andC)andtheyfollowspecificprobabilisticstatetransition
rules.EverytimetheprimitiveAcomes,eitheroftheprimitive
B or C follows with 50% chance, and the primitive A always
follows the primitive B and C (Fig. 6A). Therefore,
To model multimodal perception and action generation, a
hierarchically-organized variational RNN (see Sec. VII) was
extended. This model is comprised of three modules: the
proprioception module, the vision module, and the associative
module (Figure 6B). Fig.6. Intention-blendedHuman-robotcollaboration,adaptedfrom[44].
(A)Thetransitionruleofthemotionprimitivesduringtheimitativeinterac-
In the simulation of mutual imitative interaction, the bal- tion.(B)Thenetworkmodel.(C)Anexampleoftime-seriesplotsonneural
ance between the accuracy and complexity term in the free activities in the output layer of the proprioception module in the accuracy
termdominantcondition(a)andthecomplexitytermdominantcondition(b).
energy was the key focus. This balance determines how the
Reconstructionoftheobservationandthefuturepredictionattimestep150
approximate posterior is optimized through the free energy (top) and at time step 180 (bottom) are shown in each. Solid and dashed
minimization given the observation and the prior. Results lines represent prediction and observation, respectively. The shadowed area
indicatesthetimewindowinwhichtheapproximateposteriorisoptimized.
showed that when the complexity term was less dominant,
the robot tended to change its prediction, namely intention, 2) Self/other distinction: The work in [103] presented an
easilysothatitcouldadapttotheobservation.Inaconflicting algorithm that enables a robot to perform non-appearance
situation of motion primitives, the robot tended to change self/other distinction on a mirror by distinguishing its simple
its motion primitive to the one the human presented. When actions from other agents. Developing this visual-kinesthetic
the complexity term was more dominant, the robot tended matching is essential for safe human-robot interaction, espe-
to ignore the observation and retained its original intention. cially in social robotics [104], [48]. Using movement cues
Figure6Cdescribesthejointdynamicsdependingonthecom- the robot first learns the visual forward kinematics and then
plexity term domination. The conflicting situation in which exploitstheBayesianmodelevidencetoaccumulateevidence.
the network predicted the primitive B, but observed C. In The potential of modelling the high-order cognition needed
the less dominant condition (a), the network reconstructed the to pass the mirror test in robots makes AIF very attractive
observationandmodifiedthepredictionsuchthattheprimitive for the cognitive sciences view of robotics—See [48] for a
C persisted. In the more dominant condition (b), the network computational modelling roadmap.
ignoredtheobservationandmaintaineditsoriginalprediction.
The congruence between an agent’s intention, the action IX. CONNECTIONSWITHOTHERFRAMEWORKS
and its anticipated outcome can be linked to the psychology As one might have realised from previous sections, AIF
concept of agency [100], [101]. In the context of AIF, an shares many similarities with other more established control
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 15
schemes and theories. In this section, we summarise the main A final and very relevant relation can be also found in con-
commonalitiesbetweenactiveinferenceandotherapproaches. trol as inference [111], [112]—See [113], [114]. For instance,
performingstochasticoptimalcontrolwhenthegoalistoinfer
the input [115] can be conceptually regarded as a type of AIF
A. Relationship with classical controllers
approach.
Consider the generative model specified by Eq. (10). This
model includes the function f(x˜) which determines how the B. Relationship to Reinforcement Learning
belief state evolves over time. This can be, for example,
PlanningwithAIFusingtheexpectedfreeenergyfunctional
accordingtoafirstorderlinearsystemwhichresultsinf(x˜)=
has close relationships to the field of RL. This fact is not
(x˜ − x˜)τ−1 [5], [17], [13]. The belief state is specified
d undulysurprisingsincebothattempttosolvethesameproblem
to evolve (linearly) over time as the derivative between the
of optimal plan selection in unknown environments through
current belief x˜ and target x˜ . The term x˜ indicates the
d d largelysimilarapproaches.ThekeydifferencebetweenRLand
desired state to be achieved while τ is the time constant.
active inference relies on the properties of the EFE. Crucially,
The smaller τ, the larger the derivative. If τ approaches zero
the intrinsic and posterior divergence terms of the EFE are
(τ−1 → ∞), the value f(x˜) approaches ∞. As a result, the
not present in RL, only the extrinsic value term is. These
belief is infinitely biased towards the target and x˜ ≈ x˜ . A
d differencesarisefromthefactthatactiveinferenceisposedas
classicPIDcontrollerdefinesanerrorterm(cid:15)=(x˜ −y).The
d a variational inference procedure, thus requiring a variational
control action is then chosen as:
approximate distribution, while RL is solely an optimization
(cid:90) d(cid:15)
problem based around maximizing expected reward. Consid-
a=K (cid:15)+K (cid:15)dt+K
p i ddt
ering only the extrinsic value term, this becomes,
where K p ,K i and K d are tuning parameters. For the control E [lnp˜(y )]
law defined by active inference, our (y−x˜) is similar to the q(x˜k,y k |π) k
DefiningthedesireddistributiontobeaBoltzmanndistribution
error term. Additionally, as explained in the previous section,
when τ−1 →∞ then x˜≈x˜ d . Now the control law of active around the reward p˜(y k ) ∝ exp(−r(y k )), the extrinsic value
term simply reduces to the average reward expected under the
inference can be rewritten in terms of the error term as:
variationalbeliefdistributioninthefuture.Theonlyremaining
d(cid:15)
a˙ =κ Σ−1(cid:15)+κ Σ−1 difference to RL is that in model-free RL the reward is
u y u y(cid:48) dt
instead evaluated under the true environmental distribution
Thismeansthanifτ−1 →∞,theactiveinferencecontroller p(x˜ ,y ) instead of under the model distribution, while in
k k
is equivalent to a PI Controller i.e. PID with K d = 0, a K p model-based RL in practice the reward is evaluated under the
gainofκ u Σ− y(cid:48) 1 andanK i gainofκ u Σ− y 1.Ifoneconsidersthe model distribution so the equivalence is more exact.
generalizedmotionsuptoathirdorder,thecontrollawwould Importantly, however, active inference generalizes and ex-
include a non-zero D term. A detailed analysis of how PID tendsreinforcementlearninginseveralimportantways.Firstly,
arises in active inference under approximate linear generative the expected free energy objective generalizes the notion of
models can be found in [51], [18]. utilitybyincludinganintrinsicinformationalobjective,which
Regardingmorecomplexcontrollers,AIFsharessimilarities encourages exploration and equips agents with an ‘intrinsic
withLinearQuadraticGaussiancontrol(LQG),sincebothare curiosity’. It can be shown that this exploratory drive enables
grounded in Bayesian inference and optimal control [105]. agentstoperformbetterinmanyenvironmentswhicharehigh
However,acloserlookrevealsseveralkeydifferencesbetween dimensional with sparse rewards, where exploration is neces-
the two approaches regarding the formulation of the state sarytosolvethetask[55].Fromamathematicalperspective,it
space, the cost functions, and their minimization—See [106]. isalsopossibletogeneralizetheexpectedfreeenergytoaclass
AIF can also be considered to be a form of model predic- of objectives called divergence functionals, which all result in
tive control [53] due to the evaluation of the expected free this combination of reward-seeking and information-seeking
energy. The key difficulty is evaluating the expectation over behaviour [70], [116]. Secondly, active inference’s notion of
allpossiblefuturetrajectories,whichcanbeapproximatedvia encodinggoalsasapriordistributionoverobservationsismore
Monte-Carlo sampling of these trajectories using a forward flexible than the use of rewards in reinforcement learning,
model of the environmental dynamics. Many classical model- since, as shown above, RL implicitly assumes that the prior
predictive control planning algorithms, such as the Cross- distributionisBoltzmann,whileactiveinferenceisfreetouse
Entropy-Method (CEM) [107], and Path-Integral-Control (PI) anyotherpriordistributioninsteadwhichenablesconsiderably
[108],[109]canbeusedtoestimatethevalueofthisintegralin more flexibility in specifying goals.
the cases where explicitly enumerating every possible future
trajectory, as is commonly done in discrete-state-space AIF X. BENEFITSANDCHALLENGES
[71]. Moreover, it has recently been demonstrated that many Forrobotperception,controlandlearning,ActiveInference
of these classical planning algorithms can be interpreted as is: 1) a unified framework, 2) with functional biological
performing variational inference [110], thus linking model- plausibility, 3) using variational Bayesian inference. Each of
predictive control closely with the active inference interpreta- these three aspects lead to specific expected benefits for robot
tion of adaptive and intelligent behaviour as being fundamen- perception and control, as well as to interesting open research
tally an inference process. challenges.
16 IEEETEMPLATE.SUBMITTEDVERSION
A. Unified framework the neurological processes underlying biological perception,
control, and most importantly cognition, then it may bring
Perhaps the most exciting aspect of AIF is the natural
similar cognitive skills to our robots. Specifically, the AIF
integration of perception and action into a single objective,
approach (and the general framework of predictive coding)
namely the minimization of Variational Free Energy (or the
allows, by construction [120], to go beyond control and
Expected Free Energy when planning). A potential result is
achieve high-order cognitive and metacognitive capabilities,
that an AIF agent could use action to make its world behave
such as monitoring, self-explainability and in some degree
morepredictably.Thisintroduces,inasense,adoublemodel-
“awareness”.
bias; not only is state estimation recursively biased towards
In a broader sense of metacognition, i.e., cognition about
a Bayesian prior, even the actions help reinforce this bias. In
cognition [121], the robot should be able to evaluate and
other words, rather than trying to accurately model the world,
monitor the first-order cognitive processes [122]. One dis-
and requiring unfathomable amounts of data to do so, the
tinct characteristics of the AIF is that it uses the second-
AIF agent gets by with a strongly simplified model which
order formula in terms of the precision in prediction. The
it enforces (where possible) through its own actions. This
predictive model does not just predict sensations but also
may hold an essential key to solve the data- and experience
their predictability. This means that robots can become self-
hungriness of present-day state-of-the-art learning algorithms.
attentive and monitor its uncertainty. This is relevant for
There are several interesting research challenges still to
applications,suchashuman-robotinteractioninindustrial[44]
be solved, related to this model-bias. First, the question is
and healthcare settings.
how to avoid suboptimal convergence [117], possibly through
Furthermore,thereareinterestingconnectionsbetweenAIF
the presence of intrinsic value in the optimization criterion
and two awareness abilities in humans that may be essential
[31]. Second, if the action consists of some continuous-time
for interaction, and would bring to robotics a way to enforce
feedback signal, it is a challenge to find the update rule for
explainabilityandsafety.First,self-awareness,wheretheagent
(the parameters of) that feedback signal. For simple systems,
is able to differentiate as an independent entity. or instance,
the observation w.r.t. action derivative of the VFE can be
Taniandcolleagues[123],[124]aswellasLanillosetal.[47],
directlycomputed(e.g.[5]).However,thisisverychallenging
[48] proposed that the prediction/reconstruction error for the
for complex systems—see the closed form equation in [63,
past sensation could be related to the sense of self-awareness
appendix] when the system is known—and hence it is usually
inmachines.Second,agency,i.e.,thefeelingofcontrollingthe
approximated with a directional constant [13], [18], [23] or
actions and consequences. Hence, robotic solutions for body
by performing the control on the proprioceptive states [13],
self-awareness [6], [91], and models of agency are exciting
[20]. Potentially interesting alternative solutions exist in the
opportunities under the AIF approach.
concepts of adaptive interaction [118] or direct gradient de-
Additionally, there is an open research question of a very
scent control [119]. Finally, there is a challenging question
different nature, but also connected to the biological plau-
regarding the purpose of the belief (internal state) x˜, which in
sibility of Active Inference. Inherited from Friston’s world-
vanilla AIF is biased towards the desired state. This means,
renownedworkondynamiccausalmodeling(DCM)forbrain-
unlesstheagentisatthetargetstate,thebeliefwillbebiased,
imaging [125], AIF uses the concept of ”generalized coordi-
and therefore does not accurately represent the actual hidden
nates”, i.e. appending system states with up to the 6th order
state. A potential solution might consist of introducing the
of derivatives of those states, which is quite alien for control
action in the generative model [115], [22] in the same way
engineers.Wehaveshownthatthisimprovesestimationaccu-
that we included it in the planning.
racy in drones in wind [10], for example. The open research
Asasecondexpectedbenefit,AIFnotonlyintegratesaction
question is to what extend the use of generalized coordinates
with perception, it also provides for a natural Bayesian inte-
is worth the methodological investment for generic robotic
gration of intrinsic and extrinsic value into a single planning
applications, and whether it can be replaced by extending
objective.Thisisformallyappealing.Aninterestingremaining
system model states with noise filter model states.
research challenge is to find a principled way to balance
intrinsicandextrinsicvalue,regulatedinAIFbytheprecision
C. Variational Bayesian Inference
of the priors on desired observations [79], to completely
The fundamental mathematical operation of AIF is vari-
remove the need for a heuristic exploration/exploitation trade-
ational Bayesian Inference, an approach which has already
off.
been gaining popularity within the robotics community [126],
Finally, AIF provides a way to integrate generative models
[127], [112]. The expected benefit is that, when a proper
in a hierarchical fashion aiding the construction of complex
(hierarchical) set of Bayesian priors is in place, robots will
probabilistic controllers [95].
be able to perceive, decide, and learn with much less data
or require much fewer trials that current systems. To obtain
B. Functional biological plausibility
such a proper set of priors, we expect that AIF will provide
The concept of AIF has a very strong presence in the field a highly natural framework for humans to teach robots; the
of neuroscience with an ever-increasing list of showcases of required hierarchical set of Bayesian priors can possibly be
functional biological plausibility of the concept [71]. This obtained through a proper curriculum of demonstration and
naturally leads to great expectations for the field of robotics; training. This is still a wide open and very exciting research
if AIF is indeed an accurate mathematical description of area.
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 17
Second, we expect that the variational Bayesian inference [4] R.P.RaoandD.H.Ballard,“Predictivecodinginthevisualcortex:a
approach will lead to a leap forward in fault monitoring and functionalinterpretationofsomeextra-classicalreceptive-fieldeffects,”
Natureneuroscience,vol.2,no.1,pp.79–87,1999.
fault tolerance. If the entire control hierarchy is based on
[5] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth, “The free
prediction errors, then all unexpected sensory inputs will be energy principle for action and perception: A mathematical review,”
noticed.Bymaintainingnotonlystate-dependentexpectations JournalofMathematicalPsychology,vol.81,pp.55–79,2017.
ofthesensorvalueitself,butalsoexpectationsofthevariance [6] P. Lanillos and M. van Gerven, “Neuroscience-inspired perception-
actioninrobotics:applyingactiveinferenceforstateestimation,control
of those values, the system will only trigger on signals andself-perception,”arXivpreprintarXiv:2105.04261,2021.
that are outside regular bounds. As a follow-up from our [7] P. Lanillos, S. Franklin, A. Maselli, and D. W. Franklin, “Active
preliminary robot arm experiments, we expect eventually to strategies for multisensory conflict suppression in the virtual hand
illusion,”ScientificReports,2021.
seerobotsthatpredictallsensorsignalsatalltimes,anddetect
[8] A. Anil Meera and M. Wisse, “Dynamic expectation maximization
any unexpected behaviour, including (unexpected) collisions, algorithmforestimationoflinearsystemswithcolorednoise,”Entropy,
sensor/actuator malfunction, sample frequency hiccups, and vol.23,no.10,2021.
[9] A. A. Meera and M. Wisse, “Free energy principle based state and
even unexpected user behavior.
inputobserverdesignforlinearsystemswithcolorednoise,”in2020
Third, we expect that the variational Bayesian inference AmericanControlConference(ACC). IEEE,2020,pp.5052–5058.
approach will help alleviate the combinatorial explosion asso- [10] A. Anil Meera and M. Wisse, “A brain inspired learning algo-
ciated with making longer-term plans, and the accompanying rithm for the perception of a quadrotor in wind,” arXiv preprint
arXiv:2109.11971,2021.
deteriorationinaccuracyofpredictionswiththenumberplan-
[11] F. Bos, A. Anil Meera, D. Benders, and M. Wisse, “Free energy
ning steps. In principle this should be an emergent property principleforstateandinputestimationofaquadcopterflyinginwind,”
of AIF, in the sense the the objective to minimize free energy arXivpreprintarXiv:2109.12052,2021.
[12] P.LanillosandG.Cheng,“Adaptiverobotbodylearningandestimation
entails a minimization of complexity, both statistically and in
throughpredictivecoding,”in2018IEEE/RSJInternationalConference
termsofcomputationalcostviatheJarzynskiequality(see[32] onIntelligentRobotsandSystems(IROS). IEEE,2018,pp.4083–4090.
for an approach along these lines). AIF suggests possible [13] G. Oliver, P. Lanillos, and G. Cheng, “An empirical study of active
inferenceonahumanoidrobot,”IEEETransactionsonCognitiveand
approaches to address both issues. For example, it suggests
DevelopmentalSystems,2021.
a principled solution to combinatorial explosion of plans in
[14] D. Burghardt and P. Lanillos, “Robot localization and naviga-
terms of an approximate factorisation of the distributions over tion through predictive processing using lidar,” arXiv preprint
actions, a technique core to variational approaches in general.
arXiv:2109.04139,2021.
[15] L.Pio-Lopez,A.Nizard,K.Friston,andG.Pezzulo,“Activeinference
Uncertainty accumulated during the execution of a given pol-
androbotcontrol:acasestudy,”JournalofTheRoyalSocietyInterface,
icycanbequantifiedandthusactionsplannedaccordingly.The vol.13,no.122,p.20160616,2016.
generality of the variational Bayesian formulation provides an [16] P.LanillosandG.Cheng,“Activeinferencewithfunctionlearningfor
robotbodyperception,”inProc.Int.WorkshopContinualUnsupervised
intuitive foundation from which to develop new uncertainty-
SensorimotorLearn.,2018,pp.1–5.
sensitive approaches and gives a very appealing probabilistic [17] C.Pezzato,R.Ferrari,andC.H.Corbato,“Anoveladaptivecontroller
solution to model-predictive control. forrobotmanipulatorsbasedonactiveinference,”IEEERoboticsand
AutomationLetters,vol.5,no.2,pp.2973–2980,2020.
[18] M. Baioumy, P. Duckworth, B. Lacerda, and N. Hawes, “Active
XI. SUMMARY inference for integrated state-estimation, control, and learning,” in
InternationalconferenceonRoboticsandAutomation,ICRA,2021.
We discussed how a theory of cognition originating in
[19] C.Sancaktar,M.vanGerven,andP.Lanillos,“End-to-endpixel-based
computational neuroscience opened up opportunities to im- deep active inference for body perception and action,” in 2020 Joint
proveroboticsystems.Inparticular,wedetaileditsapplication IEEE10thInternationalConferenceonDevelopmentandLearningand
EpigeneticRobotics(ICDL-EpiRob),2020.
in estimation, adaptive control, planning and learning. We
[20] C.MeoandP.Lanillos,“Multimodalvaeactiveinferencecontroller,”
described both the mathematical formulation of AIF and the
in2021IEEE/RSJInternationalConferenceonIntelligentRobotsand
most relevant works in the literature as well as showcasing Systems(IROS). IEEE,2021.
some experiments and lessons learned from deploying AIF [21] T. Rood, M. van Gerven, and P. Lanillos, “A deep active inference
model of the rubber-hand illusion,” arXiv preprint arXiv:2008.07408,
in real robotic systems, such as industrial manipulators or
2020.
humanoids. We also described its connection with other fields [22] M. Baioumy, C. Pezzato, R. Ferrari, C. H. Corbato, and N. Hawes,
likeclassicalcontrolandRL.Finally,wediscussedthebenefits “Fault-tolerantcontrolofrobotmanipulatorswithsensoryfaultsusing
unbiased active inference,” in European Control Conference, ECC,
and challenges of this approach to transform AIF into a
2021.
standard methodology in robotics and to give robots human- [23] C. Pezzato, M. Baioumy, C. H. Corbato, N. Hawes, M. Wisse,
like interactive capabilities. and R. Ferrari, “Active inference for fault tolerant control of robot
manipulatorswithsensoryfaults,”inInternationalWorkshoponActive
Inference. Springer,2020,pp.20–27.
ACKNOWLEDGMENT [24] Anactiveinferenceimplementationofphototaxis,ser.ALIFE2021:The
2021 Conference on Artificial Life, vol. ECAL 2017, the Fourteenth
WewouldliketothankKarlFristonforhiscommentsonthe
EuropeanConferenceonArtificialLife,092017.
manuscript and his invaluable inspiration for AIF in robotics.
[25] K.J.Friston,J.Daunizeau,andS.J.Kiebel,“Reinforcementlearning
oractiveinference?”PloSone,vol.4,no.7,p.e6421,2009.
[26] B.Millidge,A.Tschantz,A.K.Seth,andC.L.Buckley,“Reinforce-
REFERENCES
mentlearningasiterativeandamortisedinference,”2020.
[1] K.Friston,“Thefree-energyprinciple:aunifiedbraintheory?”Nature [27] N. Sajid, P. J. Ball, and K. J. Friston, “Active inference: demystified
reviewsneuroscience,vol.11,no.2,pp.127–138,2010. andcompared,”arXiv,pp.arXiv–1909,2019.
[2] H.v.Helmholtz,HandbuchderphysiologischenOptik. L.Voss,1867. [28] A.Tschantz,M.Baltieri,A.K.Seth,andC.L.Buckley,“Scalingactive
[3] K.M.Dallenbach,“Apuzzle-picturewithanewprincipleofconceal- inference,”in2020InternationalJointConferenceonNeuralNetworks
ment,”TheAmericanjournalofpsychology,pp.431–433,1951. (IJCNN). IEEE,2020,pp.1–8.
18 IEEETEMPLATE.SUBMITTEDVERSION
[29] D.Han,K.Doya,andJ.Tani,“Goal-directedplanningbyreinforcement [54] L.DaCosta,T.Parr,N.Sajid,S.Veselic,V.Neacsu,andK.Friston,
learning and active inference,” arXiv preprint arxiv:2106.09938v2, “Active inference on discrete state-spaces: a synthesis,” Journal of
2021. MathematicalPsychology,vol.99,p.102447,2020.
[30] B. Millidge, “Combining active inference and hierarchical predictive [55] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley,
coding:Atutorialintroductionandcasestudy,”2019. “Reinforcement learning through active inference,” arXiv preprint
[31] A.Tschantz,A.K.Seth,andC.L.Buckley,“Learningaction-oriented arXiv:2002.12636,2020.
modelsthroughactiveinference,”PLoScomputationalbiology,vol.16, [56] K.Friston,N.Trujillo-Barreto,andJ.Daunizeau,“Dem:Avariational
no.4,p.e1007805,2020. treatment of dynamic systems,” NeuroImage, vol. 41, pp. 849–85, 08
[32] K.Friston,L.DaCosta,D.Hafner,C.Hesp,andT.Parr,“Sophisticated 2008.
inference,”arXivpreprintarXiv:2006.04120,2020. [57] A.Ciria,G.Schillaci,G.Pezzulo,V.V.Hafner,andB.Lara,“Predictive
[33] K. Ueltzho¨ffer, “Deep active inference,” Biological cybernetics, vol. processingincognitiverobotics:areview,”2021.
112,no.6,pp.547–573,2018. [58] M. Spratling, “A review of predictive coding algorithms,” Brain and
[34] O.vanderHimstandP.Lanillos,“Deepactiveinferenceforpartially Cognition, vol. 112, pp. 92–97, 2017, perspectives on Human Proba-
observable mdps,” Communications in Computer and Information bilisticInferencesandthe’BayesianBrain’.
Science,p.61–71,2020. [59] G. Pezzulo, F. Rigoli, and K. Friston, “Active inference, homeostatic
[35] Z.Fountas,N.Sajid,P.A.M.Mediano,andK.Friston,“Deepactive regulationandadaptivebehaviouralcontrol,”ProgressinNeurobiology,
inferenceagentsusingmonte-carlomethods,”2020. vol.134,pp.17–35,2015.
[36] B. Millidge, “Deep active inference as variational policy gradients,” [60] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An
JournalofMathematicalPsychology,vol.96,p.102348,2020. introduction to variational methods for graphical models,” Machine
[37] O. C¸atal, S. Wauthier, C. De Boom, T. Verbelen, and B. Dhoedt, learning,vol.37,no.2,pp.183–233,1999.
“Learninggenerativestatespacemodelsforactiveinference,”Frontiers [61] K.Friston,J.Mattout,N.Trujillo-Barreto,J.Ashburner,andW.Penny,
inComputationalNeuroscience,vol.14,p.103,2020. “Variationalfreeenergyandthelaplaceapproximation,”NeuroImage,
[38] O.C¸atal,S.Wauthier,T.Verbelen,C.DeBoom,andB.Dhoedt,“Deep vol.34,pp.220–34,022007.
activeinferenceforautonomousrobotnavigation,”inThebridgingAI [62] C. Buckley, C. Kim, S. McGregor, and A. Seth, “The free energy
andcognitivescience(BAICS)workshop,ICLR,2020. principle for action and perception: A mathematical review,” Journal
[39] O. C¸atal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron, ofMathematicalPsychology,vol.81,pp.55–79,2017.
“Robotnavigationashierarchicalactiveinference,”NeuralNetworks, [63] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
vol.142,pp.192–204,2021. behavior:afree-energyformulation,”Biologicalcybernetics,vol.102,
[40] T. Matsumoto and J. Tani, “Goal-directed planning for habituated no.3,pp.227–260,2010.
agentsbyactiveinferenceusingavariationalrecurrentneuralnetwork,”
[64] K.FristonandS.Kiebel,“Predictivecodingunderthefree-energyprin-
Entropy,vol.22,no.5,p.564,May2020.
ciple,” Philosophical transactions of the Royal Society B: Biological
[41] M. Traub, M. V. Butz, R. Legenstein, and S. Otte, “Dynamic action sciences,vol.364,no.1521,pp.1211–1221,2009.
inferencewithrecurrentspikingneuralnetworks,”inArtificialNeural
[65] K.J.Friston,N.Trujillo-Barreto,andJ.Daunizeau,“Dem:avariational
NetworksandMachineLearning–ICANN2021,I.Farkas,P.Masulli,
treatmentofdynamicsystems,”Neuroimage,vol.41,no.3,pp.849–
S.Otte,andS.Wermter,Eds. Cham:SpringerInternationalPublishing,
885,2008.
2021,pp.233–244.
[66] A. Anil Meera and M. Wisse, “On the convergence of dem’s linear
[42] C.Pezzato,C.Hernandez,S.Bonhof,andM.Wisse,“Activeinference
parameter estimator,” in International Workshop on Active Inference,
and behavior trees for reactive action planning and execution in
2021.
robotics,”arXivpreprintarXiv:2011.09756,2020.
[67] L.P.Kaelbling,M.L.Littman,andA.R.Cassandra,“Planningandact-
[43] S. Murata, Y. Yamashita, H. Arie, T. Ogata, S. Sugano, and J. Tani,
inginpartiallyobservablestochasticdomains,”Artificialintelligence,
“Learningtoperceivetheworldasprobabilisticordeterministicviain-
vol.101,no.1-2,pp.99–134,1998.
teractionwithothers:Aneuro-roboticsexperiment,”IEEEtransactions
[68] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and
onneuralnetworksandlearningsystems,vol.28,no.4,pp.830–848,
G. Pezzulo, “Active inference and epistemic value,” Cognitive neu-
2015.
roscience,vol.6,no.4,pp.187–214,2015.
[44] W. Ohata and J. Tani, “Investigation of the sense of agency in
[69] T. Parr and K. J. Friston, “Generalised free energy and active infer-
socialcognition,basedonframeworksofpredictivecodingandactive
ence,”Biologicalcybernetics,vol.113,no.5,pp.495–513,2019.
inference: A simulation study on multimodal imitative interaction,”
FrontiersinNeurorobotics,vol.14,Sep2020. [70] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the expected
freeenergy?”NeuralComputation,vol.33,no.2,pp.447–482,2021.
[45] H.F.Chame,A.Ahmadi,andJ.Tani,“Ahybridhuman-neurorobotics
approachtoprimaryintersubjectivityviaactiveinference,”Frontiersin [71] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,andG.Pezzulo,
Psychology,vol.11,p.3207,2020. “Activeinference:aprocesstheory,”Neuralcomputation,vol.29,no.1,
pp.1–49,2017.
[46] H. F. Chame and J. Tani, “Cognitive and motor compliance in inten-
tionalhuman-robotinteraction,”2020. [72] J. Schmidhuber, “Formal theory of creativity, fun, and intrinsic mo-
[47] P. Lanillos, J. Pages, and G. Cheng, “Robot self/other distinction: tivation (1990–2010),” IEEE Transactions on Autonomous Mental
Active inference meets neural networks learning in a mirror,” in Development,vol.2,no.3,pp.230–247,2010.
European Conference on Artificial Intelligence. Amsterdam: IOS [73] P.-Y.OudeyerandF.Kaplan,“Whatisintrinsicmotivation?atypology
Press,2020. ofcomputationalapproaches,”Frontiersinneurorobotics,vol.1,p.6,
[48] M.Hoffmann,S.Wang,V.Outrata,E.Alzueta,andP.Lanillos,“Robot 2009.
inthemirror:towardanembodiedcomputationalmodelofmirrorself- [74] P.Schwartenbeck,J.Passecker,T.U.Hauser,T.H.FitzGerald,M.Kro-
recognition,”KI-Ku¨nstlicheIntelligenz,vol.35,no.1,pp.37–51,2021. nbichler, and K. J. Friston, “Computational mechanisms of curiosity
[49] M.Kirchhoff,T.Parr,E.Palacios,K.Friston,andJ.Kiverstein,“The andgoal-directedexploration,”Elife,vol.8,p.e41703,2019.
markovblanketsoflife:autonomy,activeinferenceandthefreeenergy [75] P. Lanillos and G. Cheng, “Active inference with function learning
principle,”JournalofTheroyalsocietyinterface,vol.15,no.138,p. for robot body perception,” International Workshop on Continual
20170792,2018. Unsupervised Sensorimotor Learning, IEEE Developmental Learning
[50] R. Bogacz, “A tutorial on the free-energy framework for modelling andEpigeneticRobotics(ICDL-Epirob),2018.
perceptionandlearning,”Journalofmathematicalpsychology,vol.76, [76] T. Parr, N. Sajid, L. Da Costa, M. B. Mirza, and K. J. Friston,
pp.198–211,2017. “Generative models for active vision,” Frontiers in Neurorobotics,
[51] M. Baltieri and C. L. Buckley, “Pid control as a process of active vol.15,p.34,2021.
inference with linear generative models,” Entropy, vol. 21, no. 3, p. [77] N. Sajid, P. J. Ball, T. Parr, and K. J. Friston, “Active inference:
257,2019. demystified and compared,” Neural Computation, vol. 33, no. 3, pp.
[52] T. van de Laar, A. O¨zc¸elikkale, and H. Wymeersch, “Application of 674–712,2021.
thefreeenergyprincipletoestimationandcontrol,”2020. [78] O.C¸atal,J.Nauta,T.Verbelen,P.Simoens,andB.Dhoedt,“Bayesian
[53] M.Baioumy,M.Mattamala,andN.Hawes,“Variationalinferencefor policyselectionusingactiveinference,”2019.
predictive and reactive controllers,” in ICRA 2020 Workshop on New [79] N. van Hoeffelen and P. Lanillos, “Deep active inference for pixel-
advancesinBrain-inspiredPerception,InteractionandLearning,Paris, based discrete control: Evaluation on the car racing problem,” arXiv
France,2020. preprintarXiv:2109.04155,2021.
LANILLOSetal.:ACTIVEINFERENCEINROBOTICSANDARTIFICIALAGENTS:SURVEYANDCHALLENGES 19
[80] Y.YamashitaandJ.Tani,“Emergenceoffunctionalhierarchyinamul- [104] Y. Nagai, “Predictive learning: its key role in early cognitive devel-
tipletimescaleneuralnetworkmodel:ahumanoidrobotexperiment,” opment,”PhilosophicalTransactionsoftheRoyalSocietyB,vol.374,
PLoScomputationalbiology,vol.4,no.11,p.e1000220,2008. no.1771,p.20180030,2019.
[81] A.AhmadiandJ.Tani,“Anovelpredictive-coding-inspiredvariational [105] K. Friston, “What is optimal about motor control?” Neuron, vol. 72,
rnnmodelforonlinepredictionandrecognition,”Neuralcomputation, no.3,pp.488–498,2011.
vol.31,no.11,pp.2025–2074,2019.
[106] M.BaltieriandC.L.Buckley,“Onkalman-bucyfilters,linearquadratic
[82] J.Chung,K.Kastner,L.Dinh,K.Goel,A.C.Courville,andY.Bengio, controlandactiveinference,”arXivpreprintarXiv:2005.06269,2020.
“Arecurrentlatentvariablemodelforsequentialdata,”inAdvancesin
[107] R.Rubinstein,“Thecross-entropymethodforcombinatorialandcon-
neuralinformationprocessingsystems,2015,pp.2980–2988.
tinuous optimization,” Methodology and computing in applied proba-
[83] D.E.Rumelhart,G.E.Hinton,andR.J.Williams,“Learninginternal
bility,vol.1,no.2,pp.127–190,1999.
representations by error propagation,” California Univ San Diego La
JollaInstforCognitiveScience,Tech.Rep.,1985. [108] E. Theodorou, J. Buchli, and S. Schaal, “A generalized path integral
[84] H. F. Chame and J. Tani, “Cognitive and motor compliance in inten- control approach to reinforcement learning,” The Journal of Machine
tional human-robot interaction,” in Proc. of 2020 IEEE International LearningResearch,vol.11,pp.3137–3181,2010.
Conference on Robotics and Automation (ICRA), 2020, pp. 11291– [109] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive
11297. pathintegralcontrol:Fromtheorytoparallelcomputation,”Journalof
[85] H.F.Chame,A.Ahmadi,andJ.Tani,“Ahybridhuman-neurorobotics Guidance,Control,andDynamics,vol.40,no.2,pp.344–357,2017.
approachtoprimaryintersubjectivityviaactiveinference,”Frontiersin [110] M. Okada and T. Taniguchi, “Variational inference mpc for bayesian
Psychology,vol.11,p.3207,2020. model-basedreinforcementlearning,”inConferenceonRobotLearn-
[86] N.WirkuttisandJ.Tani,“Leadingorfollowing?dyadicrobotimitative ing. PMLR,2020,pp.258–272.
interactionusingtheactiveinferenceframework,”IEEERoboticsand [111] M. Botvinick and M. Toussaint, “Planning as inference,” Trends in
AutomationLetters,vol.6,no.3,pp.6024–6031,2021. cognitivesciences,vol.16,no.10,pp.485–488,2012.
[87] T. Matsumoto and J. Tani, “Goal-directed planning for habituated
[112] S.Levine,“Reinforcementlearningandcontrolasprobabilisticinfer-
agentsbyactiveinferenceusingavariationalrecurrentneuralnetwork,”
ence:Tutorialandreview,”arXivpreprintarXiv:1805.00909,2018.
Entropy,vol.22,no.5,p.564,2020.
[113] B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley, “On the
[88] J.Tani,“Learningtogeneratearticulatedbehaviorthroughthebottom-
relationshipbetweenactiveinferenceandcontrolasinference,”arXiv
upandthetop-downinteractionprocesses,”Neuralnetworks,vol.16,
preprintarXiv:2006.12964,2020.
no.1,pp.11–23,2003.
[89] J. Tani, M. Ito, and Y. Sugita, “Self-organization of distributedly [114] J.Watson,A.Imohiosen,andJ.Peters,“Activeinferenceorcontrolas
represented multiple behavior schemata in a mirror system: reviews inference?aunifyingview,”arXivpreprintarXiv:2010.00262,2020.
ofrobotexperimentsusingrnnpb,”NeuralNetworks,vol.17,no.8-9, [115] J.Watson,H.Abdulsamad,andJ.Peters,“Stochasticoptimalcontrol
pp.1273–1289,2004. as approximate input inference,” in Conference on Robot Learning.
[90] K.Friston,J.Mattout,andJ.Kilner,“Actionunderstandingandactive PMLR,2020,pp.697–716.
inference,”Biologicalcybernetics,vol.104,no.1,pp.137–160,2011. [116] B.Millidge,A.Tschantz,A.Seth,andC.Buckley,“Understandingthe
[91] N.-A.Hinz,P.Lanillos,H.Mueller,andG.Cheng,“Driftingperceptual originofinformation-seekingexplorationinprobabilisticobjectivesfor
patternssuggestpredictionerrorsfusionratherthanhypothesisselec- control,”2021.
tion: replicating the rubber-hand illusion on a robot,” in 2018 Joint
[117] M. Deisenroth and C. E. Rasmussen, “Pilco: A model-based and
IEEE8thInternationalConferenceonDevelopmentandLearningand
data-efficient approach to policy search,” in Proceedings of the 28th
EpigeneticRobotics(ICDL-EpiRob). IEEE,2018,pp.125–132. InternationalConference on machinelearning (ICML-11). Citeseer,
[92] K. Astrom, “Theory and applications of adaptive control - a survey,” 2011,pp.465–472.
Automatica,vol.Vol.19,No.5,pp.471–486,1983.
[118] F. Lin, R. D. Brandt, and G. Saikalis, “Self-tuning of pid controllers
[93] M. Colledanchise and P. Ogren, “How Behavior Trees Modularize
byadaptiveinteraction,”inProceedingsofthe2000AmericanControl
Hybrid Control Systems and Generalize Sequential Behavior Com-
Conference.ACC(IEEECat.No.00CH36334),vol.5. IEEE,2000,
positions, the Subsumption Architecture, and Decision Trees,” IEEE
pp.3676–3681.
TransactionsonRobotics,vol.33,no.2,pp.372–389,2017.
[119] J.Naiborhu,S.Nababan,R.Saragih,andI.Pranoto,“Directgradient
[94] M. Baioumy, C. Pezzato, C. H. Corbato, N. Hawes, and R. Ferrari,
descent control as a dynamic feedback control for linear system,”
“Towards stochastic fault-tolerant control usingprecision learning and
active inference,” in International Workshop on Active Inference.
BulletinoftheMalaysianMathematicalSciencesSociety,vol.29,no.2,
2006.
Springer,2021.
[95] K. J. Friston, T. Parr, and B. de Vries, “The graphical brain: belief [120] L.Sandved-Smith,C.Hesp,J.Mattout,K.Friston,A.Lutz,andM.J.
propagationandactiveinference,”NetworkNeuroscience,vol.1,no.4, Ramstead,“Towardsacomputationalphenomenologyofmentalaction:
pp.381–414,2017. modellingmeta-awarenessandattentionalcontrolwithdeepparametric
[96] R. Y. Rubinstein, “Optimization of computer simulation models with active inference,” Neuroscience of consciousness, vol. 2021, no. 1, p.
rareevents,”EuropeanJournalofOperationalResearch,vol.99,no.1, niab018,2021.
pp.89–112,1997. [121] A. Cleeremans, D. Achoui, A. Beauny, L. Keuninckx, J.-R. Martin,
[97] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine,“Softactor-critic:Off- S.Mun˜oz-Moldes,L.Vuillaume,andA.DeHeering,“Learningtobe
policymaximumentropydeepreinforcementlearningwithastochastic conscious,”Trendsincognitivesciences,vol.24,no.2,pp.112–123,
actor,”inInternationalconferenceonmachinelearning. PMLR,2018, 2020.
pp.1861–1870. [122] C.Hesp,R.Smith,T.Parr,M.Allen,K.J.Friston,andM.J.Ramstead,
[98] P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Contrastive active infer- “Deeplyfeltaffect:Theemergenceofvalenceindeepactiveinference,”
ence,” Advances in Neural Information Processing Systems, vol. 34, Neuralcomputation,vol.33,no.2,pp.398–446,2021.
2021.
[123] J. Tani, “An interpretation of the ‘self’ from the dynamical systems
[99] A. D. Noel, C. van Hoof, and B. Millidge, “Online reinforcement perspective: A constructivist approach,” Journal of Consciousness
learning with sparse rewards through an active inference capsule,” Studies,vol.5,no.5-6,pp.516–542,1998.
arXivpreprintarXiv:2106.02390,2021.
[124] J. Tani and J. White, “Cognitive neurorobotics and self in the shared
[100] S. Gallagher, “Philosophical conceptions of the self: implications for
world, a focused review of ongoing research,” Adaptive Behavior, p.
cognitivescience,”Trendsincognitivesciences,vol.4,no.1,pp.14–
1059712320962158,2020.
21,2000.
[125] K.J.Friston,L.Harrison,andW.Penny,“Dynamiccausalmodelling,”
[101] M. Synofzik, G. Vosgerau, and A. Newen, “Beyond the comparator
model: a multifactorial two-step account of agency,” Consciousness
Neuroimage,vol.19,no.4,pp.1273–1302,2003.
andcognition,vol.17,no.1,pp.219–239,2008. [126] M.Toussaint,“Robottrajectoryoptimizationusingapproximateinfer-
[102] K.Friston,“Prediction,perceptionandagency,”InternationalJournal ence,” in Proceedings of the 26th annual international conference on
ofPsychophysiology,vol.83,no.2,pp.248–252,2012. machinelearning,2009,pp.1049–1056.
[103] P.Lanillos,J.Pages,andG.Cheng,“Robotself/otherdistinction:active [127] M. P. Deisenroth, G. Neumann, J. Peters et al., “A survey on policy
inference meets neural networks learning in a mirror,” in European search for robotics,” Foundations and trends in Robotics, vol. 2, no.
ConferenceonArtificialIntelligence(ECAI2020),2020. 1-2,pp.388–403,2013.
20 IEEETEMPLATE.SUBMITTEDVERSION
APPENDIX distribution q(x). Under the mean-field assumption this could
A. Variational Free Energy correspond to the following factorization: p(x|y) ≈ q(x) =
q(x ,...,x ) = (cid:81)N q (x ). Essentially, we assume a varia-
We use the definition of the F based on [63], where the 1 N i=1 i i
action is implicit within the observation model y(u). Using tional function which partitions the variables into independent
the KL-divergence the VFE is defined as follows: parts. This greatly simplifies computations.
F =KL[q(x˜)||p(x˜|y)]−logp(y)
D. Generalized coordinates
=KL[q(x˜)||p(x˜,y)]=−ELBO
AkeyingredientbehindthesuccessofFEPwhencompared
Proof.F istheKL-divergencebetweenthevariationaldensity tootherestimatorsistheuseofgeneralizedcoordinates,which
and the true posterior minus the sensory surprise. provides a mathematical framework to model the colored
noise in data. The time dependent quantities in the generative
(cid:90) q(x˜)
KL[q(x˜)||p(x˜|y)]= q(x˜)log = modelaremodeledingeneralizedcoordinatesusingitshigher
x˜ p(x˜|y) order derivatives as x˜ = [x x(cid:48) x(cid:48)(cid:48) ...]T. Quantitatively,
(cid:90) (cid:90)
it tracks the probability density of the trajectory of states
= q(x˜)logq(x˜)− q(x˜)logp(x˜|y)=
instead of the point estimates, when compared to the Kalman
x˜ x˜
(cid:90) (cid:90) (cid:90) Filter. To embed the higher order information of the colored
= q(x˜)logq(x˜)− q(x˜)logp(x˜,y)+ q(x˜)logp(y)= noise, the process and observation noises are also modelled
x˜ x˜ x˜ in generalized coordinates as w˜ and z˜. Under a Gaussian
(cid:90) (cid:90) (cid:90)
= q(x˜)logq(x˜)− q(x˜)logp(x˜,y)+logp(y) q(x˜)= convoluted white noise assumption, the precision matrix for
x˜ x˜ x˜ noisesmoothnessthatrepresentsthecrosscorrelationbetween
(cid:90) (cid:90) each noise derivatives can be written as [65]:
= q(x˜)logq(x˜)− q(x˜)logp(x˜,y)+logp(y)=
x˜ x˜  1 0 − 1 .. −1
=F+logp(y). S(σ2)=   0 2σ 1 2 2 0 σ2 .. 
Proof. The F is the negative Evidence Lower Bound: − 1 0 3 ..
2σ2 4σ4
.. .. .. ..
F =KL[q(x˜)||p(x˜,y)] (p+1)×(p+1)
(cid:90) (cid:90) where σ is the kernel width of the Gaussian filter and p is the
= q(x˜)logq(x˜)− q(x˜)logp(x˜,y) order of generalized motion of states. The generalized noise
x˜ x˜
(cid:20) (cid:90) (cid:90) (cid:21) precision matrices for process noise and observation noise
=− − q(x˜)logq(x˜)+ q(x˜)logp(x˜,y) becomes Π˜ = S(σ2)⊗Π and Π˜ = S(σ2)⊗Π . The
w w w z z z
x˜ x˜
(cid:90) p(x˜,y) use of generalized coordinates has demonstrated a superior
=− q(x˜)log =−ELBO performanceinstateestimationwhencomparedtotheKalman
q(x˜)
x˜
Filter, and in input estimation when compared to unknown
B. Laplace Approximation input observers, for colored noise [9] . The use of generalized
coordinates has also proven to improve the state estimation
Maximum a posteriori (MAP) approximates the posterior
accuracy of a quadrotor hovering in wind [11].
p(x|y) with a point mass (delta function) by simply capturing
its mode. MAP is attractive because is fast and efficient. How
E. Expected free energy (EFE)
can we use MAP to construct a better approximation to the
The analytical expression for the optimal plan, at the
posterior?TheLaplaceapproximationisonewayofimproving
minimum of the EFE over trajectories, can be expressed as
a MAP estimate (Fig. 7) by encoding the posterior with a asoftmaxoverthepathintegraloftheEFEofeachtrajectory.
normal distribution centered at the MAP estimate. Namely, given the EFE over a plan, we have,
G(π)=E [lnq(x ,π)−lnp˜(y ,x ,π)]
q(x1:T,y1:T,π) 1:T 1:T 1:T
T
(cid:88)
=D [q(π)||p(π) −E [lnq(x |π)−lnp˜(o ,x |π)]]
KL q(oτ,xτ|π) τ τ τ
τ
T
(cid:88)
=D [q(π)||p(π) −G ]
KL πτ
τ
Thus, the optimal posterior is given by argminG(π) =⇒
π
q∗(π)=σ(lnp(π)− (cid:80)T G )Moreover, the EFE for a specific
τ πτ
timestep and plan can be decomposed into the extrinsic and
intrinsic value.
Fig.7. Comparisonbetweenthetrueposterior,andtheLaplaceapproximation G =E [lnq(x |π)−lnp˜(y ,x |π)]
π q(yt,xt) t t t
andtheMaximumAPosteriory(MAP)estimation. =E [lnq(x |π)−lnp˜(y ,x |π)
q(yt,xt|π) t t t
+lnq(x |y )−lnq(x |y )]
t t t t
C. Mean field Approximation =−E [lnp˜(y )]−E D [q(x |y )||q(x |π)]
q(yt,xt|π) t q(yt|π) KL t t t
The mean field approximation is a simplifying assumption +E D [q(x |y )||p(x |y π)]
q(yt) KL t t t t
about the for of the variational distribution q(·). Consider ≈−E [lnp˜(y )]−E D [q(x |y )||q(x |π)]
q(yt,xt|π) t q(yt|π) KL t t t
N random variables x = [x 1 ,...,x N ] and data y. Our aim (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue
is to approximate the distribution p(x|y) with a variational