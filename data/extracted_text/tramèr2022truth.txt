Truth Serum: Poisoning Machine Learning Models
to Reveal Their Secrets
Florian TramÃ¨râˆ—â€ 
ETH ZÃ¼rich
Reza Shokri
National University of
Singapore
Ayrton San Joaquin
Yale-NUS College
Hoang Le
Oregon State University
Matthew Jagielski
Google
Sanghyun Hong
Oregon State University
Nicholas Carlini
Google
ABSTRACT
We introduce a new class of attacks on machine learning models.
We show that an adversary who can poison a training dataset can
cause models trained on this dataset to leak significant private
details of training points belonging to other parties. Our active
inference attacks connect two independent lines of work targeting
the integrity and privacy of machine learning training data.
Our attacks are effective across membership inference, attribute
inference, and data extraction. For example, our targeted attacks
can poison <0.1% of the training dataset to boost the performance
of inference attacks by 1 to 2 orders of magnitude. Further, an
adversary who controls a significant fraction of the training data
(e.g., 50%) can launch untargeted attacks that enable8Ã—more precise
inference on all other usersâ€™ otherwise-private data points.
Our results cast doubts on the relevance of cryptographic pri-
vacy guarantees in multiparty computation protocols for machine
learning, if parties can arbitrarily select their share of training data.
CCS CONCEPTS
â€¢ Computing methodologies â†’ Machine learning; â€¢ Security
and privacy â†’Software and application security;
KEYWORDS
machine learning, poisoning, privacy, membership inference
ACM Reference format:
Florian TramÃ¨r, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagiel-
ski, Sanghyun Hong, and Nicholas Carlini. 2022. Truth Serum: Poisoning
Machine Learning Models to Reveal Their Secrets. InProceedings of Proceed-
ings of the 2022 ACM SIGSAC Conference on Computer and Communications
Security, Los Angeles, CA, USA, November 7â€“11, 2022 (CCS â€™22), 14 pages.
https://doi.org/10.1145/3548606.3560554
1 INTRODUCTION
A central tenet of computer security is that one cannot obtain
any privacy without integrity [10, Chapter 9]. In cryptography, for
example, an adversary who canmodify a ciphertext, before it is sent
âˆ—Authors ordered reverse alphabetically
â€ Work done while the author was at Google
This work is licensed under a Creative Commons Attribution
International 4.0 License.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
Â© 2022 Copyright held by the owner/author(s).
10âˆ’310âˆ’210âˆ’1 100
False Positive Rate
0.0
0.5
1.0True Positive Rate
480Ã—
8Ã—
(a) Membership Inference
10âˆ’310âˆ’210âˆ’1 100
False Positive Rate
33Ã—
30Ã—
Ours Prior work (b) Attribute Inference
100 102 104 106
Guesses
Success rate
39Ã—
4Ã— (c) Canary Extraction
Figure 1: Poisoning improves an adversaryâ€™s ability to per-
form three different privacy attacks. (a) For membership
inference on CIFAR-10, we improve the true-positive rate
(TPR) of [11] from 7% to 59%, at a 0.1% false-positive rate
(FPR). Conversely, at a fixed TPR of 50%, we reduce the FPR
by 480 Ã—. (b) For attribute inference on Adult (to infer gen-
der), we improve the TPR of [45] by 30 Ã—. (c) To extract 6-digit
canaries from WikiText, we reduce the median number of
guesses for the attack of [13] by 39 Ã—, from 9,018 to 230.
to the intended recipient, might be able to leverage this ability to
actually decrypt the ciphertext. In this paper, we show that this same
vulnerability applies to the training of machine learning models.
Currently, there are two long and independent lines of work that
study attacks on the integrity and privacy of training data in ma-
chine learning (ML). Data poisoning attacks [7] target the integrity
of an ML modelâ€™s data collection process to degrade model perfor-
mance at inference timeâ€”either indiscriminately [7, 15, 20, 31, 51] or
on targeted examples [4, 6, 23, 39, 61, 67]. Then, separately, privacy
attacks such asmembership inference [62], attribute inference [21, 75]
or data extraction [13, 14] aim to infer private information about
the modelâ€™s training set by interacting with a trained model, or by
actively participating in the training process [46, 53].
Some works have highlighted connections between these two
threats. For example, malicious parties in federated learning can
craft updates to increase the privacy leakage of other participants [28,
46, 53, 71]. Moreover, Chase et al. [43] show that poisoning attacks
can increase leakage of global properties of the training set (e.g.,
the prevalence of different classes). In this paper, we extend and
strengthen these results by demonstrating that an adversary can
statically poison the training set to maximize the privacy leakage
of individual training samples belonging to other parties. In other
arXiv:2204.00032v2  [cs.CR]  6 Oct 2022
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
words, we show that the ability to â€œwriteâ€ into the training dataset
can be exploited to â€œreadâ€ from other (private) entries in this dataset.
We design targeted poisoning attacks on deep learning models
that tamper with a small fraction of training data points (<0.1%) to
improve the performance of membership inference, attribute infer-
ence and data extraction attacks on other training examples, by 1 to
2 orders-of-magnitude. For example, we show that by inserting just
8 poison samples into the CIFAR-10 training set (0.03% of the data),
an adversary can infer membership of a specific target image with a
true-positive-rate (TPR) of 59%, compared to 7% without poisoning,
at a false-positive rate (FPR) of 0.1%. Conversely, poisoning enables
membership inference attacks to reach 50% TPR at a FPR of 0.05%,
an error rate 480Ã—lower than the 24% FPR from prior work.
Similarly, by poisoning 64 sentences in the WikiText corpus, an
adversary can extract a secret 6-digit â€œcanaryâ€ [13] from a model
trained on this corpus with a median of 230 guesses, compared to
9,018 guesses without poisoning (an improvement of 39Ã—).
We show that our attacks are robust to uncertainty about the tar-
geted samples, and rigorously investigate the factors that contribute
to the success of our attacks. We find that poisoning has the most
impact on samples that originally enjoy the strongest privacy, as
our attacks reduce the average-case privacy of samples in a dataset
to the worst-case privacy of data outliers. We further demonstrate
that poisoning drastically lowers the cost of state-of-the-art privacy
attacks, by alleviating the need for training shadow models [62].
We then consideruntargeted attacks where an adversary controls
a larger fraction of the training dataâ€”as high as 50%â€”and aims to
increase privacy leakage of all other data points. Such attacks are
relevant when a small number of parties (e.g., 2) want to jointly train
a model on their respective training sets without revealing their
own (private) dataset to the other(s), e.g., by using secure multi-
party computation [25, 73]. We show that untargeted poisoning
attacks can reduce the error rate of membership inference attacks
across all of the victimâ€™s data points by a factor of 8Ã—.
Our results call into question the relevance of modeling machine
learning models as ideal functionalities in cryptographic protocols,
such as when training models with secure multiparty computation
(MPC). As our attacks show, a malicious party thathonestly follows
the training protocol can exploit their freedom to choose their input
data to strongly influence the protocolâ€™s â€œidealâ€ privacy leakage.
2 BACKGROUND AND RELATED WORK
2.1 Attacks on Training Privacy
Training data privacy is an active research area in machine learning.
In our work, we consider three canonical privacy attacks: mem-
bership inference [ 62], attribute inference [ 21, 22, 75], and data
extraction [13, 14]. In membership inference, an adversaryâ€™s goal is
to determine whether a given sample appeared in the training set
of a model or not. Participation in a medical trial, for example, may
reveal information about a diagnosis [ 29]. In attribute inference,
an adversary uses the model to learn some unknown feature of a
given user in the training set. For example, partial knowledge of
a userâ€™s responses to a survey could allow the adversary to infer
the response to other sensitive questions in the survey, by query-
ing a model trained on this (and other) usersâ€™ responses. Finally,
in data extraction, we consider an adversary that seeks to learn a
secret string contained in the training data of a language model. We
focus on these three canonical attacks as they are the most often
considered attacks on training data privacy in the literature.
2.2 Attacks on Training Integrity
Poisoning attacks can be grouped into three categories: indiscrimi-
nate (availability) attacks, targeted attacks, and backdoor (or trojan)
attacks. Indiscriminate attacks seek to reduce model performance
and render it unusable [7, 15, 20, 31, 51]. Targeted attacks induce
misclassifications for specific benign samples [23, 61, 64]. Backdoor
attacks add a â€œtriggerâ€ into the model, allowing an adversary to in-
duce misclassifications by perturbing arbitrary test points [4, 6, 67].
Backdoors can also be inserted via supply-chain vulnerabilities,
rather than data poisoning attacks [26, 39, 40]. However, none of
these poisoning attacks have the goal of compromising privacy.
Our work considers an attacker that poisons the training data to
violate the privacy of other users. Prior work has considered this
goal for much stronger adversaries, with additional control over the
training procedure. For example, an adversary that controls part
of the training code can use the trained model as a side-channel to
exfiltrate training data [3, 63]. Or in federated learning, a malicious
server can select model architectures that enable reconstructing
training samples [9, 19]. Alternatively, participants in decentralized
learning protocols can boost privacy attacks by sending dynamic
malicious updates [28, 46, 53, 71]. Our work differs from these in
that we only make the weak assumption that the attacker can add
a small amount of arbitrary data to the training set once, without
contributing to any other part of training thereafter. A similar threat
model to ours is considered in [43], for the weaker goal of inferring
global properties of the training data (e.g., the class prevalences).
2.3 Defenses
As we consider adversaries that combine poisoning attacks and pri-
vacy inference attacks, defenses designed to mitigate either threat
may be effective against our attacks.
Defenses against poisoning attacks (either indiscriminate or tar-
geted) design learning algorithms that are robust to some fraction
of adversarial data, typically by detecting and removing points that
are out-of-distribution [15, 16, 27, 31, 66]. Defenses against privacy
inference either apply heuristics to minimize a modelâ€™s memo-
rization [34, 52] or train models with differential privacy [ 1, 17].
Training with differential privacy provably protects the privacy of
a userâ€™s data in any dataset, including a poisoned one.
Since our main focus in this work is to introduce a novel threat
model that amplifies individual privacy leakage through data poi-
soning, we design worst-case attacks that are not explicitly aimed
at evading specific data poisoning defenses. We note that such
poisoning defenses are rarely deployed in practice today. In partic-
ular, sanitizing user data in decentralized settings such as federated
learning or secure MPC represents a major challenge [ 35]. In Sec-
tion 4.3.7, we show that a simple loss-clipping approachâ€”inspired
by differential privacyâ€”can significantly decrease the effectiveness
of our poisoning attacks. Whether our attack techniques can be
made robust to such defenses, as well as to more complex data
sanitization mechanisms, is an interesting question for future work.
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
A related line of work uses poisoning to measure the privacy
guarantees of differentially private training algorithms [ 32, 54].
These works are fundamentally different than ours: they measure
the privacy leakageof the poisoned samples themselves to investigate
worst-case properties of machine learning; in contrast, we show
poisoning can harm other benign samples.
2.4 Machine Learning Notation
A classifier ð‘“ðœƒ : Xâ†’[ 0,1]ð‘› is a learned function that maps an input
sample ð‘¥ âˆˆX to a probability vector overð‘›classes. Given a training
set ð·sampled from some distribution D, we let ð‘“ðœƒ â†T( ð·)denote
that a classifier with weights ðœƒ is learned by running the training
algorithm Ton the training set ð·. Given a labeled sample (ð‘¥,ð‘¦),
we let â„“(ð‘“ðœƒ (ð‘¥),ð‘¦)denote a loss function applied to the classifierâ€™s
output and the ground-truth label, typically the cross-entropy loss.
Causal language models are sequential classifiers that are trained
to predict the next word in a sentence. Let sentences in a language
be sequences of tokens from a set T (e.g., all English words or sub-
words [72]). A generative language model ð‘“ðœƒ : Tâˆ—â†’[0,1]|T|takes
as input a sentence ð‘ of an arbitrary number of tokens, and outputs
a probability distribution over the value of the next token. Given a
sentence ð‘  = ð‘¡1 ...ð‘¡ ð‘˜ of ð‘˜ tokens, we define the modelâ€™s loss as:
â„“(ð‘“ðœƒ,ð‘ )B 1
ð‘˜
ð‘˜âˆ’1âˆ‘ï¸
ð‘–=0
â„“CE (ð‘“ðœƒ (ð‘¡1 ...ð‘¡ ð‘–),ð‘¡ð‘–+1), (1)
where â„“CE is the cross-entropy loss andð‘¡1 ...ð‘¡ 0 is the empty string.
3 AMPLIFYING PRIVACY LEAKAGE
WITH DATA POISONING
Motivation. The fields of security and cryptography are littered
with examples where an adversary can turn an attack on integrity
into an attack on privacy. For example, in cryptography a padding
oracle attack [ 8, 68] allows an adversary to use their ability to
modify a ciphertext to learn the entire contents of the message.
Similarly, compression leakage attacks [24, 36] inject data into a
userâ€™s encrypted traffic (e.g., HTTPS responses) and infer the userâ€™s
private data by analysing the size of ciphertexts. Alternatively,
in Web security, some past browsers were vulnerable to attacks
wherein the ability to send crafted email messages to a victim could
be abused to actually read the victimâ€™s other emails via a Cross-
Origin CSS attack [ 30]. Inspired by these attacks, we show this
same type of result is possible in the area of machine learning.
3.1 Threat Model
We consider an adversary Athat can inject some data ð·adv into a
machine learning modelâ€™s training set ð·. The goal of this adversary
is to amplify their ability to infer information about the contents
of ð·, by interacting with a model trained on ð·âˆªð·adv. In contrast
to prior attacks on distributed or federated learning [46, 53], our
adversary cannot actively participate in the learning process. The
adversary can only statically poison their data once, and after this
can only interact with the final trained model.
The privacy game. We consider a generic privacy game, wherein
the adversary has to guess which element from some universe U
was used to train a model. By appropriately defining the universeU
this game generalizes a number of prior privacy attack games, from
membership inference to data extraction.
Game 3.1 (Privacy Inference Game). The game proceeds between
a challenger Cand an adversary A. Both have access to a distribu-
tion D, and know the universe Uand training algorithm T.
(1) The challenger samples a dataset ð· â†D and a target
ð‘§ â†U from the universe (such that ð·âˆ©U = âˆ…).
(2) The challenger trains a model ð‘“ðœƒ â†T( ð· âˆª{ð‘§})on the
dataset ð· and target ð‘§.
(3) The challenger gives the adversary query access to ð‘“ðœƒ .
(4) The adversary emits a guess Ë†ð‘§ âˆˆU.
(5) The adversary wins the game if Ë†ð‘§ = ð‘§.
The universe Ucaptures the adversaryâ€™s prior belief about the
possible value that the targeted example may take. In the member-
ship inference game (see [33, 75]), for a specific target example ð‘¥
the universe is U= {ð‘¥,âŠ¥}â€”where âŠ¥indicates the absence of an
example. That is, the adversary guesses whether the model ð‘“ is
trained on ð·or on ð·âˆª{ð‘¥}. For attribute inference, the universe U
contains the real targeted example ð‘¥, along with all â€œalternate ver-
sionsâ€ of ð‘¥with other values for an unknown attribute ofð‘¥. Attacks
that extract well-formatted sensitive values, such as credit card
numbers [13], can be modeled with a universe Uof all possible
values that the secret could take.
We now introduce our new privacy game, which adds the ability
for an adversary to poison the dataset. This is a strictly more general
game, with the objective of maximizing the privacy leakage of the
targeted point. The changes to Game 3.1 are highlighted in red.
Game 3.2 (Privacy Inference Game with Poisoning) . The game
proceeds between a challenger Cand an adversary A. Both have
access to a distribution D, and know the universe Uand training
algorithm T.
(1) The challenger samples a dataset ð· â†D and a target
ð‘§ â†U from the universe (such that ð·âˆ©U = âˆ…).
(2) The adversary sends a poisoned dataset ð·adv of size ð‘adv
to the challenger.
(3) The challenger trains a model ð‘“ðœƒ â†T( ð· âˆªð·adv âˆª{ð‘§})
on the poisoned dataset ð·âˆªð·adv and target ð‘§.
(4) The challenger gives the adversary query access to ð‘“ðœƒ .
(5) The adversary emits a guess Ë†ð‘§ âˆˆU.
(6) The adversary wins the game if Ë†ð‘§ = ð‘§.
3.1.1 Adversary Capabilities. The above poisoning game implic-
itly assumes a number of adversarial capabilities, which we now
discuss more explicitly.
Game 3.2 assumes that the adversary knows the data distribu-
tion D and the universe of possible target values U. These capabili-
ties are standard and easy to meet in practice. The adversary further
gets to add a set of ð‘adv poisoned points into the training set. We
will consider attacks that require adding only a small number of
targeted poisoned points (as low as ð‘adv = 1), as well as attacks
that assume much larger data contributions (up to ð‘adv = |ð·|) as
one could expect in MPC settings with a small number of parties.
We impose no restrictions on the adversaryâ€™s poisons being
â€œstealthyâ€. That is, we allow for the poisoned dataset ð·adv to be
arbitrary. As we will see, designing poisoning attacks that maximize
privacy leakage is non-trivialâ€”even when the adversary is not
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
constrained in their choice of poisons. As poisoning attacks that
target data privacy have not been studied so far, we aim here to
understand how effective such attacks could be in the worst case,
and leave the study of attacks with further constraints (such as
â€œclean labelâ€ poisoning [61, 67]) to future work.
Finally, the game assumes that the adversary targets a specific
example ð‘§. We call this atargeted attack. We also consideruntargeted
attacks in Section 4.4, where the attacker crafts a poisoned dataset
ð·adv to harm the privacy of all samples in the training set ð·.
3.1.2 Success Metrics. When the universe of secret values is
small (as for membership inference, where |U|= 2, or for attribute
inference where it is the cardinality of the attribute), we measure an
attackâ€™s success rate by its true-positive rate (TPR) and false-positive
rate (FPR) over multiple iterations of the game. Following [11], we
focus in particular on the attack performance at low false-positive
rates (e.g., FPR=0.1%), which measures the attackâ€™s propensity to
precisely target the privacy of some worst-case users.
For membership inference, we naturally define a true-positive
as a correct guess of membership, i.e., Ë†ð‘§ = ð‘§ when ð‘§ = ð‘¥, and a
false-positive as an incorrect membership guess, Ë†ð‘§ â‰  ð‘§when ð‘§ = ð‘¥.
For attribute inference, we define a â€œpositiveâ€ as an example
with a specific value for the unknown attribute (e.g., if the unknown
attribute is gender, we define â€œfemaleâ€ as the positive class).
For canary extraction , where the universe of possible target
values is large (e.g., all possible credit card numbers), we amend
Game 3.2 to allow the adversary to obtain â€œpartial creditâ€ by emit-
ting multiple guesses. Specifically, following [13], we let the adver-
sary output an ordering (a permutation) Ë†ð‘ = ðœ‹(U)of the secretâ€™s
possible values, from most likely to least likely. We then measure
the attackâ€™s success by the exposure [13] (in bits) of the correct
secret ð‘§:
exposure(ð‘§; Ë†ð‘)B log2 (|U|)âˆ’log2

rank(ð‘§; Ë†ð‘)

. (2)
The exposure ranges from0 bits (when the correct secretð‘§is ranked
as the least likely value), to log2 (|U|) bits (when the adversaryâ€™s
most likely guess is the correct value ð‘§).
3.2 Attack Overview
We begin with a high-level overview of our poisoning attack strate-
gies. For simplicity of exposition, we focus on the special case
of membership inference. Our attacks for attribute inference and
canary extraction follow similar principles.
Given a target sample(ð‘¥,ð‘¦), the standard privacy game (for mem-
bership inference) in Game 3.1 asks the adversary to distinguish
two worlds, where the model is respectively trained onð·âˆª{(ð‘¥,ð‘¦)}
or on ð·. When we give the adversary the ability to poison the
dataset in Game 3.2, the goal is now to alter the dataset ð· so that
the above two worlds become easier to distinguish.
Note that this goal is very different from simply maximizing the
modelâ€™s memorization of the target (ð‘¥,ð‘¦). This could be achieved
with the following (bad) strategy: poison the dataset ð· by adding
multiple identical copies of (ð‘¥,ð‘¦)into it. This will ensure that the
trained model ð‘“ðœƒ strongly memorizes the target (i.e., the model will
correctly classify ð‘¥ with very high confidence). However, this will
be true in both worlds, regardless of whether the target (ð‘¥,ð‘¦)was
in the original training set ð· or not. This strategy thus does not
help the adversary in solving the distinguishing gameâ€”and in fact
actually makes it more difficult to distinguish membership.
Instead, the adversary should alter the training set ð· so as to
maximize the influence of the target (ð‘¥,ð‘¦). That is, we want the
poisoned training set ð· âˆªð·adv to be such that the inclusion of
the target (ð‘¥,ð‘¦)provides a maximal change in the trained modelâ€™s
behavior on some inputs of the adversaryâ€™s choice.
To illustrate this principle, we begin by demonstrating a provably
perfect privacy-poisoning attack for the special case of nearest-
neighbor classifiers. We also propose an alternative attack for SVMs
in Appendix D. We then describe our design principles for empirical
attacks on deep neural networks.
Warm-up: provably amplifying membership leakage in kNNs.
Consider a ð‘˜-Nearest Neighbor (kNN) classifier (assume, wlog., that
ð‘˜ is odd). Given a labeled training set ð·, and a test sample ð‘¥, this
classifier finds the ð‘˜ nearest neighbors of ð‘¥ in ð·, and outputs the
majority label among these ð‘˜ neighbors. We assume the attacker
has black-box query access to the trained classifier.
We demonstrate how to poison a kNN classifier so that the
classifier labels a target example (ð‘¥,ð‘¦)correctly if and only if the
target is in the original training set ð·. This attack thus lets the
adversary win the membership inference game with 100% accuracy.
Our poisoning attack (see Algorithm 1 in Appendix D) creates
a dataset ð·adv of size ð‘˜ that contains ð‘˜âˆ’1 copies of the target ð‘¥,
half correctly labeled as ð‘¦and half mislabeled as ð‘¦â€²â‰  ð‘¦. We further
add one poisoned example ð‘¥â€²at a small distance ð›¿ from ð‘¥ and also
mislabeled as ð‘¦â€²(we assume that no other point in the training set
ð· is within distance ð›¿ from ð‘¥). This attack maximizes the influence
of the targeted point, by turning it into a tie-breaker for classifying
ð‘¥ when it is a member.
The attacker infers that the target example (ð‘¥,ð‘¦)is a member,
if and only if the trained model correctly classifies ð‘¥ as class ð‘¦. To
see that the attack works, consider the two possible worlds:
â€¢ The target is in ð·: There are ð‘˜ copies of ð‘¥ in the poisoned
training set ð·âˆªð·adv: the ð‘˜âˆ’1 poisoned copies (half are
correctly labeled) and the target (ð‘¥,ð‘¦). Thus, the majority
vote among the ð‘˜ neighbors yields the correct class ð‘¦.
â€¢ The target is not in ð·: As all points in ð· are at distance at
least ð›¿ from the target ð‘¥, the ð‘˜ neighbors selected by the
model are the adversaryâ€™s ð‘˜ poisoned points, a majority of
which are mislabeled as ð‘¦â€². Thus, the model outputs ð‘¦â€².
In Appendix D, we show that our attack is non-trivial, in that
there exist points for which poisoning isnecessary to achieve perfect
membership inference. In fact, we show that for some points, a non-
poisoning adversary cannot infer membership better than chance.
Amplifying privacy leakage in deep neural networks. The above
attack on kNNs exploits the classifierâ€™s specific structure which
lets us turn any exampleâ€™s membership into a perfect tie-breaker
for the modelâ€™s decision on that example. In deep neural networks,
it is unlikely that examples can exhibit such a clear cut influence
(i.e., due to the stochasticity of training, it is unlikely that a specific
model behavior would occur if and only if an example is a member).
Instead, we could try to cast the adversaryâ€™s goal as an optimiza-
tion problem , of selecting a poisoned dataset ð·adv that maximizes
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
the distinguishability of models trained with or without the tar-
get (ð‘¥,ð‘¦). Yet, solving such an optimization problem is daunting.
While prior work does optimize poisons to maximally alter a single
modelâ€™s confidence on a specific target point [61, 67, 78], here we
would instead need to optimize for a difference in distributions of
the decisions of two models trained on two neighboring datasets.
Rather than tackle this optimization problem directly, we â€œhand-
craftâ€ strategies that empirically increase a sampleâ€™s influence on
the model. We start from the observation in prior work that the most
vulnerable examples to privacy attacks are data outliers [11, 75].
Such examples are easy to attack precisely because they have a large
influence on the model: a model trained on an outlier has a much
lower loss on this sample than a model that was not trained on it.
Yet, in our threat model, the attacker cannot control or modify the
targeted example ð‘¥ (and ð‘¥ is unlikely, a priori, to be an outlier). Our
insight then is to poison the training dataset so as to transform the
targeted example ð‘¥ into an outlier. For example, we could fool the
model into believing that the targeted point ð‘¥ is mislabeled. Then,
the presence of the correctly labeled target (ð‘¥,ð‘¦)in the training set
is likely to have a large influence on the modelâ€™s decision.
In Section 4, we show how to instantiate this attack strategy to
boost membership inference attacks on standard image datasets. We
then extend this attack strategy in Section 5 to the case of attribute
inference attacks for tabular datasets. Finally, in Section 6 we pro-
pose attack strategies tailored to language models, that maximize
the leakage of specially formatted canary sequences.
4 MEMBERSHIP INFERENCE ATTACKS
Membership inference (MI) captures one of the most generic notions
of privacy leakage in machine learning. Indeed, any form of data
leakage from a modelâ€™s training set (e.g., attribute inference or data
extraction) implies the ability to infer membership of some training
examples. As a result, membership inference is a natural target for
evaluating the impact of poisoning attacks on data privacy.
In this section, we introduce and analyze data poisoning attacks
that improve membership inference by one to two orders of magni-
tude. Section 4.2 describes a targeted attack that increases leakage
of a specific sample (ð‘¥,ð‘¦), and Section 4.3 contains an analysis of
this attackâ€™s success. Section 4.4 explores untargeted attacks that
increase privacy leakage on all training points simultaneously.
4.1 Experimental Setup
We extend the recent attack of [ 11] that performs membership
inference via a per-example log-likelihood test. The attack first
trains ð‘ shadow models such that each sample (ð‘¥,ð‘¦)appears in the
training set of half of the shadow models, and not in the other half.
We then compute the losses of both sets of models on ð‘¥:
ð¿in = {â„“(ð‘“(ð‘¥),ð‘¦) : ð‘“ trained on (ð‘¥,ð‘¦)},
ð¿out = {â„“(ð‘“(ð‘¥),ð‘¦) : ð‘“ not trained on (ð‘¥,ð‘¦)}
and fit Gaussian distributions N(ðœ‡in,ðœŽ2
in)to ð¿in, and N(ðœ‡out,ðœŽ2
out)
to ð¿out (with a logit scaling of the losses, as in [11]). Then, to infer
membership of ð‘¥in a trained modelð‘“ðœƒ , we compute the loss ofð‘“ðœƒ on
ð‘¥, and perform a standard likelihood-ratio test for the hypotheses
that ð‘¥ was drawn from N(ðœ‡in,ðœŽ2
in)or from N(ðœ‡out,ðœŽ2
out).
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x16
poison x8
poison x4
poison x2
poison x1
No poison
Figure 2: Targeted poisoning attacks boost membership in-
ference on CIFAR-10. For 250 random data points, we insert
1 to 16 mislabelled copies of the point into the training set,
and run the MI attack of [11] with 128 shadow models.
To amplify the attack with poisoning, the adversary builds a
poisoned dataset ð·adv that is added to the training set of ð‘“ðœƒ . The
adversary also adds ð·adv to each shadow modelâ€™s training set (so
that these models are as similar as possible to the target model ð‘“ðœƒ ).
We perform our experiments on CIFAR-10 and CIFAR-100 [38]â€”
standard image datasets of 50,000 samples from respectively 10 and
100 classes. The target models (and shadow models) use a Wide-
ResNet architecture [76] trained for 100 epochs with weight decay
and common data augmentations (random image flips and crops).
For each dataset, we train ð‘ = 128 models on random 50% splits of
the original training set.1 The models achieve 91% test accuracy on
CIFAR-10 and 67% test-accuracy on CIFAR-100 on average.
4.2 Targeted Poisoning Attacks
We now design our poisoning attack to increase the membership
inference success rate for a specific target example ð‘¥. That is, the
attacker knows the data of ð‘¥ (but not whether it is used to train the
model) and designs a poisoned dataset ð·adv adaptively based on ð‘¥.
Label flipping attacks. We find that label flipping attacks are a
very powerful form of poisoning attacks to increase data leakage.
Given a targeted example ð‘¥ with label ð‘¦, the adversary inserts
the mislabelled poisons ð·adv = {(ð‘¥,ð‘¦â€²),..., (ð‘¥,ð‘¦â€²)}for some label
ð‘¦â€²â‰  ð‘¦. The rationale for this attack is that a model trained on ð·adv
will learn to associate ð‘¥ with label ð‘¦â€², and the now â€œmislabelledâ€
target (ð‘¥,ð‘¦)will be treated as an outlier and have a heightened
influence on the model when present in the training set.
To instantiate this attack on CIFAR-10 and CIFAR-100, we pick
250 targeted points at random from the original training set. For
each targeted example (ð‘¥,ð‘¦), the poisoned dataset ð·adv contains a
mislabelled example (ð‘¥,ð‘¦â€²)replicated ð‘Ÿ times, for ð‘Ÿ âˆˆ{1,2,4,8,16}.
We report the average attack performance for a full leave-one-out
cross-validation (i.e., we evaluate the attack 128 times, using one
model as the target and the rest as shadow models).
1The training sets of the target model and shadow models thus partially overlap
(although the adversary does not know which points are in the targetâ€™s training set).
Carlini et al. [11] show that their attack is minimally affected if the attackerâ€™s shadow
models are trained on datasets fully disjoint from the targetâ€™s training set.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
â–¡25 0 25
no poison
â–¡25 0 25
poison x1
â–¡25 0 25
poison x2
â–¡25 0 25
poison x4
â–¡25 0 25
poison x8
â–¡25 0 25
poison x16
Figure 3: Our poisoning attack separates the loss distribu-
tions of members and non-members, making them more dis-
tinguishable. For five random CIFAR-10 examples, we plot
the (logit-scaled) loss distribution on that example when it
is a member (red) or not (blue). The horizontal axis varies
the number of times the adversary poisons the example.
Results. Figure 2 and Figure 15 (appendix) show the performance
of our membership inference attack on CIFAR-10 and CIFAR-100
respectively, as we vary the number of poisons ð‘Ÿ per sample.
We find that this attack is remarkably effective. Even with a
single poisoned example ( ð‘Ÿ = 1), the attackâ€™s true-positive rate
(TPR) at a 0.1% false-positive rate (FPR) increases by 1.75Ã—. With 8
poisons (0.03% of the modelâ€™s training set size), the TPR increases by
a factor 8Ã—on CIFAR-10, from 7% to 59%. On CIFAR-100, poisoning
increases the baselineâ€™s strong TPR of 22% to 69% at a FPR of 0.1%.
Alternatively, we could aim for a fixed recall and use poisoning
to reduce the MI attackâ€™s error rate. Without poisoning, an attack
that correctly identifies half of the targeted CIFAR-10 members (i.e.,
a TPR of 50%) would also incorrectly label 24% of non-members as
members. With poisoning, the same recall is achieved while only
mislabeling 0.05% of non-membersâ€”a factor 480Ã—improvement.
On CIFAR-100, also for a 50% TPR, poisoning reduces the attackâ€™s
false-positive rate by a factor 100Ã—, from 2.5% to 0.025%.
As we run multiple targeted attacks simultaneously (for effi-
ciency sake), the total number of poisons is large (up to4,000 misla-
belled points). Yet, the poisoned modelâ€™s test accuracy is minimally
reduced (from 92% to 88%) and the MI success rate on non-targeted
points remains unchanged. Thus, we are not compounding the ef-
fects of the 250 targeted attacks. As a sanity check, we repeat the
experiment with only 50 targeted points, and obtain similar results.
4.3 Analysis and Ablations
We have shown that targeted poisoning attacks significantly in-
crease membership leakage. We now set out to understand the
principles underlying our attackâ€™s success.
4.3.1 Why does our attack work? In Figure 3 we plot the dis-
tribution of model confidences for five CIFAR-10 examples, when
the example is a member (in red) and when it is not (in blue). On
the horizontal axis, we vary the number of poisons (i.e., how many
times this example is mislabeled in the training set). Without poi-
soning (left column), the distributions overlap significantly for most
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
most vulnerable (targeted x4)
most vulnerable (no poison)
least vulnerable (targeted x4)
least vulnerable (no poison)
Figure 4: Poisoning causes previously-safe data points to be-
come vulnerable. We run our attack for the 5% of points that
are originally most- and least-vulnerable to membership in-
ference without poisoning. While poisoning has little effect
for the most vulnerable points, poisoning the least vulnera-
ble points improves the TPR at a 0.1% FPR by a factor 430Ã—.
examples. As we increase the number of poisons, the confidences
shift significantly to the left, as the model becomes less and less
confident in the exampleâ€™s true label. But crucially, the distributions
also become easier to separate, because the (relative) influence of
the targeted example on the trained model is now much larger.
To illustrate, consider the top example in Figure 3 (labeled â€œshipâ€).
Without poisoning, this exampleâ€™s confidence is in the range [99.99%,
100%] when it is a member, and [99.98%, 100%] when it is not. Con-
fidently inferring membership is thus impossible. With 16 poisons,
however, the confidence on this example is in the range [0.4%, 28.5%]
when it is a member, and [0%, 2.4%] when it is notâ€”thus enabling
precise membership inference when the confidence exceeds 2.4%.
4.3.2 Which points are vulnerable to our attack? Our poisoning
attack could increase the MI success rate in different ways. Poison-
ing could increase the attack accuracy uniformly across all data
points, or it might disparately impact some data points. We show
that the latter is true: our attack disparately impacts inliers
that were originally safe from membership inference . This
result has striking consequences: even if a user is an inlier and
therefore might not be worried about privacy leakage, an active
poisoning attacker that targets this user can still infer membership.
In Figure 4, we show the performance of our poisoning attack
on those data points that are initially easiest and hardest to infer
membership for. We run the membership inference attack of [11] on
all CIFAR-10 points, and select the 5% of samples where the attack
succeeds least often and most often (averaged over all 128 models).
We then re-run the baseline attack on these extremal points with a
new set of models (to ensure our selection of points did not overfit)
and compare with our label flipping attack with ð‘Ÿ = 4.
Poisoning has a minor effect on data points that are already out-
liers: here even the baseline MI attack has a high success rate (73%
TPR at a 0.1% FPR) and thus there is little room for improvement.2
2In Figure 3, we see that examples for which MI succeeds without poisoning tend to
already be outliers. For example, the third and fourth example from the top are a â€œbirdâ€
mislabelled as â€œcatâ€ in the CIFAR-10 training set, and a â€œhorseâ€ confused as a â€œdeerâ€.
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
with shadow models
poison x16
poison x4
poison x1
No poison
Figure 5: Membership inference attacks with poisoning do
not require shadow models. With poisoning, the global
threshold attack of [75] performs nearly as well on CIFAR-
10 as the attack of [11] that uses 128 shadow models to com-
pute individual decision thresholds for each example.
For points that are originally hardest to attack, however, poisoning
improves the attackâ€™s TPR by a factor 430Ã—, from 0.1% to 43%.
4.3.3 Are shadow models necessary? Following [11, 41, 58, 70,
74], our MI attack relies on shadow models to calibrate the confi-
dences of individual examples. Indeed, as we see in the first column
of Figure 3, the confidences of different examples are on different
scales, and thus the optimal threshold to distinguish a member from
a non-member varies greatly between examples. Yet, as we increase
the number of poisoned samples, we observe that the scale of the
confidences becomes unified across examples. And with 16 poisons,
the threshold that best distinguishes members from non-members
is approximately the same for all examples in Figure 3.
As a result, we show in Figure 5 that with poisoning, the use
of shadow models for calibration is no longer necessary to
obtain a strong MI attack . By simply setting a global threshold on
the confidence of a targeted example (as in [75]) the MI attack works
nearly as well as our full attack that trains 128 shadow models.
This result renders our attack much more practical than prior
attacks. Indeed, in many settings, training even a single shadow
model could be prohibitively expensive for the attacker (in terms of
access to training data or compute). In contrast, the ability to poison
a small fraction of the training set may be much more realistic, espe-
cially for very large models. Recent works [11, 48, 70, 74] show that
non-calibrated MI attacks (without poisoning) perform no better
than chance at low false-positives (see Figure 5). With poisoning
however, these non-calibrated attacks perform extremely well. At
a FPR of 0.1%, a non-calibrated attack without poisoning has a TPR
of 0.1% (random guessing), whereas a non-calibrated attack with
16 targeted poisons has a TPR of 43%â€”an improvement of 430Ã—.
4.3.4 Does the choice of label matter? Our poisoning attack
injects a targeted example with an incorrect label. For the results in
Figure 2 and Figure 15, we select an incorrect label at random (if we
replicate a poison ð‘Ÿ times, we use the same label for each replica).
In Figure 6 we explore alternative strategies for choosing the
incorrect label on CIFAR-10. We consider three other strategies:
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x4 (random)
poison x4 (best)
poison x4 (worst)
poison x4 (random-multi)
No poison
Figure 6: Comparison of mislabelling strategies on CIFAR-
10. Assigning the same random incorrect label to 4 poison
copies performs better than mislabeling as the most likely
incorrect class (best) or the least likely class (worst). Assign-
ing different incorrect labels to the 4 copies (random-multi)
severely reduces the attack success rate.
â€¢ best: mislabel the poisons as the most likely incorrect class
for that example (as predicted by a pre-trained model).
â€¢ worst: mislabel the poisons as the least likely class.
â€¢ random-multi: sample an incorrect label at random (with-
out replacement) for each of the ð‘Ÿ poisons.
These three strategies perform worse than the random approach.
On both CIFAR-10 (Figure 6) and CIFAR-100 (Figure 16) the â€œbestâ€
and â€œworstâ€ strategies do slightly worse than random mislabeling.
The â€œrandom-multiâ€ strategy does much worse, and under-performs
the baseline attack without poisoning at low FPRs. This strategy
has the opposite effect of our original attack, as it forces the model
to predict a near-uniform distribution across classes, which is only
minimally influenced by the presence or absence of the targeted
example. Overall, this experiment shows that the exact choice of
incorrect label matters little, as long as it is consistent.
4.3.5 Can the attack be improved by modifying the target? Our
poisoning attack only tampers with a targetâ€™s label ð‘¦, while leaving
the example ð‘¥ unchanged. It is conceivable that an attack that also
alters the sample before poisoning could result in even stronger
leakage. In Appendix A.2 we experiment with a number of such
strategies, inspired by the literature on clean-label poisoning at-
tacks [61, 67, 78]. But we ultimately failed to find an approach that
improves upon our attack and leave it as an open problem to design
better privacy-poisoning strategies that alter the target sample.
4.3.6 Does the attack require exact knowledge of the target? Ex-
isting membership inference attacks, which can be used for auditing
ML privacy vulnerabilities, typically assume exact knowledge of
the targeted example (so that the adversary can query the model on
that example). Our attack is no different in this regard: it requires
knowledge of the target at training time (in order to poison the
model) and at evaluation time to run the MI attack.
We now evaluate how well our attack performs when the adver-
sary has only partial knowledge of the targeted example. As we are
dealing with images here, defining such partial knowledge requires
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x16
poison x8
poison x4
poison x2
poison x1
No poison
Figure 7: For CIFAR-10 models trained with losses clipped
to ð¶ = 1, poisoning only moderately increases the success of
MI attacks. With more than 1 mislabeled copy of the target,
poisoning harms the attack at low false-positives.
some care. We will assume that instead of knowing the exact target
example ð‘¥, the adversary knows an example Ë†ð‘¥ that â€œlooks similarâ€
to ð‘¥. The attacker needs to guess whether ð‘¥ was used to train a
model. To this end, the attacker poisons the target model (and the
shadow models) by injecting mislabeled versions of Ë†ð‘¥ and queries
the target model on Ë†ð‘¥ to formulate a guess.
Details of this experiment are in Appendix A.3. Figure 19 shows
that our attack (as well as the baseline without poisoning)
are robust to an adversary with only partial knowledge of
the target. At a FPR of 0.1%, the TPR is reduced by <1.6Ã—for both
the baseline attack and our attack with 4 poisons per target.
4.3.7 Can we mitigate the attack by bounding outlier influence?
As we have shown, our attack succeeds by turning data points into
outliers, which then have a high influence on the modelâ€™s deci-
sions. Our privacy-poisoning attack can thus likely be mitigated by
bounding the influence that an outlier can have on the model. For
example, training with differential privacy [1, 17] would prevent
our attack, as it bounds the influence that any outlier can have in
any dataset (including a poisoned one). Algorithms for differentially
private deep learning bound the size of the gradients of individual
examples [1]. Here, we opt for a slightly simpler approach that
bounds the losses of individual examples (the two approaches are
equivalent if we assume some bound on the modelâ€™s activations in
a forward pass). Bounding losses rather than gradients has the ad-
vantage of being much more computationally efficient, as it simply
requires scaling losses before backpropagation.
In Figure 7, we plot the MI success rate with and without poison-
ing, when each exampleâ€™s cross-entropy loss is bounded toð¶ = 1.
Clipping in this way only slightly reduces the success rate of the
attack without poisoning, but significantly harms the success of
the poisoning attack at low false-positives. While our attack with
ð‘Ÿ = 1 poisons per target still improves over the baseline, including
additional poisons weakens the attack, as the original sampleâ€™s loss
can no longer grow unbounded to counteract the poisoning.
In Figure 20 in Appendix A.4, we show the effect of training
with loss clipping on the distributions of member and non-member
confidences for five random CIFAR-10 samples, analogously to
None 1.0 0.1 0.01 0.001
Loss clip
10âˆ’3
10âˆ’2
10âˆ’1
100
TPR @0.1% FPR
With poison
Without poison
80
82
84
86
88
90
Test Accuracy (%)
Figure 8: Aggressive loss clipping reduces the success rate
of MI attacks on CIFAR-10 nearly to chance. However, poi-
soning can still boost the attack success by up to 3Ã—, and the
modelâ€™s test error is increased by 45%.
Figure 3. Poisoning the model with mislabeled samples still shifts
the confidences to very low values, but the inclusion of the correctly
labeled target no longer clearly separates the two distributions.
While loss clipping thus appears to be a simple and effective de-
fense against our poisoning attack, it is no privacy panacea. Indeed,
the original baseline MI attack retains high success rate. As we show
in Figure 8, further reducing the clipping bound (to ð¶ = 10âˆ’3) does
reduce the baseline MI attack to near-chance. But in this regime,
poisoning does again increase the attackâ€™s success rate at low false-
positives by a factor 3Ã—. Moreover, aggressive clipping reduces the
modelâ€™s test accuracy from 91% to 87%â€”an increase in error rate
of 45% (equivalent to undoing three years of progress in machine
learning research). Finally, we also show in Section 4.4 that an al-
ternative untargeted attack strategy , that increases leakage of all
data points, remains resilient to moderate loss clipping.
4.4 Untargeted Poisoning Attacks
So far we have considered poisoning attacks that target a specific
example (ð‘¥,ð‘¦)that is known (exactly or partially) to the adversary.
We now turn to more general untargeted attacks, where the ad-
versary aims to increase the privacy leakage of all honest training
points. As this is a much more challenging goal, we will consider
adversaries who can compromise a much larger fraction of the
training data. This threat is realistic in settings where a small num-
ber of parties decide to collaboratively train a model by pooling
their respective datasets (e.g., two or more hospitals that train a
joint model using secure multi-party computation).
Setup. We consider a setting where the training data is split
between two parties. One party acts maliciously and chooses their
data so as to maximize the leakage of the other partyâ€™s data.
We adapt the experimental setup of targeted attacks described in
Section 4.1. For our untargeted attack, we assume the adversaryâ€™s
poisoned dataset ð·adv is of the same size as the victimâ€™s training
data ð·. We propose an untargeted variant of our label flipping
attack, in which the attacker picks their data from the same dis-
tribution as the victim, i.e., ð·adv â†D, but flips all labels to the
same randomly chosen class: ð·ð‘Žð‘‘ð‘£ = {(ð‘¥1,ð‘¦),..., (ð‘¥ð‘adv ,ð‘¦)}. This
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate untargeted poisoning
No poison
Figure 9: An untargeted poisoning attack that consistently
mislabels 50% of the CIFAR-10 training data increases mem-
bership inference across all other data points.
attack aims to turn all points that are of a different class thanð‘¦into
outliers, so that their membership becomes easier to infer.
To evaluate the attack on CIFAR-10, we select12,500 points at
random from the training set to build the poisoned dataset ð·adv.
The honest partyâ€™s datasetð·consists of 12,500 points sampled from
the remaining part of the training set. We train a target model on the
joint dataset ð·âˆªð·adv. The attacker further trains shadow models
by repeating the above process of sampling an honest dataset ð·
and combining it with the adversaryâ€™s fixed dataset ð·adv. In total,
we train ð‘ = 128 models. We run the membership inference attack
on a set of 25,000 points disjoint from ð·adv, half of which are actual
members of the target model. We average results over a 128-fold
leave-one-out cross-validation where we choose one of the 128
models as the target and the others as the shadow models.
Results. Figure 9 shows the performance of our untargeted attack.
Poisoning reliably increases the privacy leakage of all the honest
partyâ€™s data points. At a FPR of 0.1%, the attackâ€™s TPR across all
the victimâ€™s data grows from 9% without poisoning to 16% with our
untargeted attack. Conversely, at a fixed recall, untargeted poison-
ing reduces the attackâ€™s error rate drastically. With our poisoning
strategy, the attacker can correctly infer membership for half of the
honest partyâ€™s data, at a false-positive rate of only 3%, compared to
an error rate of 24% without poisoningâ€”an improvement of a factor
8Ã—. We include results for other untargeted poisoning strategies, as
well as replications on additional datasets in Appendix A.5.
We further evaluate this untargeted attack against the simple
â€œloss clippingâ€ defense from Section 4.3.7. In contrast to the targeted
case, we find that moderate clipping (ð¶ = 1) has no effect on the
untargeted attack and that with more stringent clipping, the modelâ€™s
test accuracy is severely reduced.
5 ATTRIBUTE INFERENCE ATTACKS
Our results in Section 4 show that data poisoning can significantly
increase an adversaryâ€™s ability to infer membership of training
data. We now turn to attacks that infer actual data. We begin by
considering attribute inference attacks in this section, and consider
canary extraction attacks on language models in the next section.
In an attribute inference attack, the adversary has partial knowl-
edge of some training example ð‘¥, and abuses access to a trained
model to infer unknown features of this example. For simplicity
of exposition, we consider the case of inferring a binary attribute
(e.g., whether a user is married or not), given knowledge of the
other features of ð‘¥, and of the class label ð‘¦. In the context of our
privacy game, Game 3.2, the universeUconsists of the two possible
â€œversionsâ€ of a target example, ð‘§0 = (ð‘¥0,ð‘¦),ð‘§1 = (ð‘¥1,ð‘¦), where ð‘¥ð‘–
denotes the target example with value ð‘–for the unknown attribute.
5.1 Attack Setup
We start from the state-of-the-art attribute inference attack of [45].
Given a trained model ð‘“ðœƒ , this attack computes the losses for both
versions of the target, â„“(ð‘“ðœƒ (ð‘¥0),ð‘¦)and â„“(ð‘“ðœƒ (ð‘¥1),ð‘¦)and picks the
attribute value with lowest loss.
Similarly to many prior membership inference attacks, this at-
tack does not account for different examples having different loss
distributions. Indeed, for some examples the losses â„“(ð‘“ðœƒ (ð‘¥0),ð‘¦)
and â„“(ð‘“ðœƒ (ð‘¥1),ð‘¦)are very similar, while for other examples the dis-
tributions are easier to distinguish. Following recent advances in
membership inference attacks [ 11, 58, 70, 74] we thus design a
stronger attack that uses shadow models to calibrate the losses of
different examples. We train ð‘ shadow models, such that the two
versions (ð‘¥ð‘–,ð‘¦)of the targeted example each appear in the training
set of half the shadow models. For each set of models, we then
compute the difference between the loss on either version of ð‘¥:
ð¿0 =

â„“(ð‘“(ð‘¥0),ð‘¦)âˆ’â„“(ð‘“(ð‘¥1),ð‘¦) : ð‘“ trained on (ð‘¥0,ð‘¦)
	
,
ð¿1 =

â„“(ð‘“(ð‘¥0),ð‘¦)âˆ’â„“(ð‘“(ð‘¥1),ð‘¦) : ð‘“ trained on (ð‘¥1,ð‘¦)
	
.
As in the attack of [ 11], we fit Gaussian distributions to ð¿0 and
ð¿1. Given a target model ð‘“ðœƒ , we compute the difference in losses
â„“(ð‘“ðœƒ (ð‘¥0),ð‘¦)âˆ’ â„“(ð‘“ðœƒ (ð‘¥1),ð‘¦)and perform a likelihood-ratio test be-
tween the two Gaussians. This attack acts as our baseline.
To then improve on this with poisoning, we injectð‘Ÿ/2 mislabelled
samples of the form (ð‘¥0,ð‘¦â€²), and ð‘Ÿ/2 of the form (ð‘¥1,ð‘¦â€²)into the
training set. Mislabeling both versions of the target forces the model
to have similarly large loss on either version. The true variant of
the target sample will then have a large influence on one of these
losses, which will be detectable by our attack.
For completeness, we also consider an â€œimputationâ€ baseline [45]
that infers the unknown attribute from the data distribution alone .
That is, given samples (ð‘¥,ð‘¦)from the distribution D, we train a
model to infer the value of one attribute of ð‘¥, given the other
attributes and class labelð‘¦. This baseline lets us quantify how much
extra information about a sampleâ€™s unknown attribute is leaked by a
model trained on that sample, compared to what an adversary could
infer simply from inherent correlations in the data distribution.
5.2 Experimental Setup
We run our attack on the Adult dataset [37], a tabular dataset with
demographic information of 48,842 users. The target model is a
three-layer feedforward neural network to predict whether a userâ€™s
income is above $50K. The target model (and the attackâ€™s shadow
models) are trained on a random 50% of the full Adult dataset. Our
models achieve 84% test accuracy. We consider attribute inference
attacks that infer either a userâ€™s stated gender, or relationship status
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’3
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x16
poison x8
poison x4
poison x2
No poison
baseline imputation
Figure 10: Targeted poisoning attacks boost attribute infer-
ence on Adult. Without poisoning, the attack [45] performs
worse than a baseline imputation that infers gender based
on correlation statistics. With ð‘Ÿ â‰¥4 poisons, our improved
attack surpasses the baseline for the first time.
(after binarizing this feature into â€œmarriedâ€ and â€œnot marriedâ€ as
in [45]). We define the attributes â€œfemaleâ€ and â€œnot marriedâ€ as the
positive class in each case (i.e., a true-positive corresponds to the
attacker correctly guessing that a user is female, or not married).
We pick 500 target points at random, and train 10 target models
that contain these 500 points in their training sets. We further train
128 shadow models on training sets that contain these 500 targets
with the unknown attribute chosen at random. The training sets
of the target models and shadow models are augmented with the
adversaryâ€™s poisoned dataset ð·adv that contains ð‘Ÿ âˆˆ{1,2,4,8,16}
mislabelled copies of each target.
To evaluate the imputation baseline, we train the same three-
layer feedforward neural network to predict gender (or relationship
status) given a userâ€™s other features and class label. We train this
model on the entire Adult dataset except for the 500 target points.
5.3 Results
Our attack for attributing a userâ€™s stated gender is plotted in Fig-
ure 10. Results for inferring relationship status are in Figure 26.
As for membership inference, poisoning significantly improves at-
tribute inference. At a FPR of 0.1%, the attack of [45] has a TPR of
1%, while our attack with 16 poisons gets a TPR of 30%. Conversely,
to achieve a TPR of 50%, the attack without poisoning incurs a FPR
of 39%, while our attack with 16 poisons has a FPR of 1.2%â€” an
error reduction of 33Ã—. In particular, the attack without poison-
ing performs worse than the trivial imputation baseline. Access to
a non-poisoned model thus does not appear to leak more private
information than what can be inferred from the data distribution.
6 EXTRACTION IN LANGUAGE MODELS
In the previous sections, we focused on attacks that infer a single
bit of informationâ€”whether an example is a member or not (in
Section 4), or the value of some binary attribute of the example (in
Section 5). We now consider the more ambitious goal of inferring
secrets with much higher entropy. Following [13], we aim to extract
well-formatted secrets (e.g., credit card numbers, social security
numbers, etc.) from a language model trained on an unlabeled text
corpus. Language models are a prime target for poisoning attacks,
as their training datasets are often minimally curated [5, 60].
As in [13], we inject canaries into the training dataset of a lan-
guage model, and then evaluate whether an attacker (who may
poison part of the dataset) can recover the secret canary. Our ca-
naries take the form ð‘  = â€œPrefix â€, where Prefix is an
arbitrary string that is known (fully or partially) to the attacker, fol-
lowed by a random 6-digit number.3 This setup mirrors a scenario
where a secret number appears in a standardized context known to
the adversary (e.g., a PIN inserted in an HTML input form).
We train small variants of the GPT-2 model [56] on the WikiText-
2 dataset [47], a standard language modeling corpus of approxi-
mately 3 million tokens of text from Wikipedia. We inject a canary
into this dataset with a 125-token prefix followed by a random
6-digit secret (125 tokens represent about 500 characters; we also
consider adversaries with partial knowledge of the prefix in Sec-
tion 6.4). Given a trained model ð‘“ðœƒ , the attacker prompts the model
with the prefix followed by all 106 possible values of the canary,
and ranks them according to the modelâ€™s loss. Following [13], we
compute the exposure of the secret as the average number of bits
leaked to the adversary (see Equation (2)). To control the random-
ness from the choice of random prefix and of random secret, we
inject 45 different canaries into a model, and train 45 target models
(for a total of 452 = 2025 different prefix-secret combinations). We
then measure the average exposure across all combinations.
We consider two poisoning attack strategies to increase exposure
of a secret canary, that rely on different adversarial capabilities:
(1) Prefix poisoning assumes that the adversary can select the
Prefix string that precedes the secret canary. This threat
model captures settings where the attacker can select a tem-
plate in which user secrets are input. Alternatively, since
training sets for language models are often constructed
by concatenating all of a userâ€™s text sources, this attack
could be instantiated by having the attacker send a mes-
sage to the victim before the victim writes some secret
information.
(2) Suffix poisoning assumes that the adversary knows the
Prefix string preceding the canary, but cannot necessarily
modify it. Here, the adversary inserts poisoned copies of
the Prefix with a chosen suffix into the training data.
As we will see, both types of poisoning attacks significantly
increase the exposure of canaries. An attacker that combines both
forms of attack can reduce their guesswork to recover canaries by
a factor of 39Ã—, compared to a baseline attack without poisoning.
6.1 Canary Extraction with Calibration
We again begin by showing that existing canary extraction attacks
can be significantly improved by appropriatelycalibrating the attack
using shadow models (again, similar to state-of-the-art membership
inference attacks [11, 41, 58, 70, 74]).
As a baseline, we run the attack of [13], which simply ranks all
possible canary values according to the target modelâ€™s loss. We find
3We limit ourselves to 6-digit secrets which allows us to efficiently enumerate over all
possible secret values when computing the secretâ€™s exposure. As in [13], we could also
consider longer secrets and approximate exposure by sampling.
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
1 2 4 8 16 32 64 128
Number of shadow models
3
4
5
6
7
8Exposure (bits)
With calibration
Without calibration
Figure 11: Calibration with shadow models significantly im-
proves the attack of [13] for extracting canaries.
that this attack achieves only a low canary exposure of 3.1 bits on
average in our setting (i.e, the adversary learns less than 1 digit
of the secret).4 We find that even though the modelâ€™s loss on the
random 6-digit secret does decrease throughout training, there are
many other 6-digit numbers that are a priori much more likely and
that therefore yield lower losses (such as 000000, or 123456).
The issue here is again one of calibration. Any language model
trained on a large dataset will tend to assign higher likelihood to
the number 123456 than to, say, the number 418463. However, a
model trained with the canary 418463 will have a comparatively
much higher confidence in this canary than a language model that
was not trained on this specific canary.
As we did with our membership and attribute inference attacks,
we thus first train a number of shadow models . We train ð‘ shadow
models ð‘”ð‘– on random subsets of WikiText (without any inserted
canaries). Then, for a target model ð‘“ðœƒ , prefix ð‘ and canary guess ð‘,
we assign to ð‘ the calibrated confidence:
log ð‘“ðœƒ (ð‘+ð‘)âˆ’ 1
ð‘
ð‘âˆ‘ï¸
ð‘–=1
logð‘”ð‘– (ð‘+ð‘). (3)
A potential canary value such as 123456 will have a low calibrated
score, as all models assign it high confidence. In contrast, the true
canary (e.g., 418643) will have high calibrated confidence as only
the target model ð‘“ðœƒ assigns a moderately high confidence to it. We
then compute exposure exactly as in Equation (2), with possible
canary values ranked according to their calibrated confidence.
Figure 11 shows that the use of shadow models vastly increases
exposure of canaries. With just 2 shadow models, we obtain an
average exposure of 7.1 bits, a reduction in guesswork of 16Ã—
compared to a non-calibrated attack. With additional shadow
models, the exposure increases moderately to 7.4 bits. Conversely,
the fraction of canaries recovered in fewer than 100 guesses in-
creases from 0.1% to 10% with calibration (an improvement of100Ã—).
6.2 Prefix Poisoning
The first poisoning attack we consider is one where the adversary
can choose the prefix that precedes the secret canary. We evaluate
4Carlini et al. [13] report higher exposures for numeric secrets because they use worse
models (LSTMs) trained on simpler datasets that contain very few numbers.
Best Foreign Code Random Worst
6.5
7.0
7.5
8.0
8.5
9.0
9.5
10.0
10.5Exposure (bits)
Prefix poisoning
No poison
Figure 12: Secret canaries are easier to extract if the ad-
versary can force them to appear in an out-of-distribution
context. Canaries that appear after a piece of source code
(Code), uniformly random tokens (Random), or a sequence
of tokens with worst-case loss (Worst) are significantly more
exposed than canaries that appear after random WikiText
sentences (No poison baseline). Inserting canaries after sen-
tences in a non-English language (Foreign) or a sequence of
tokens with best-case loss (Best) does not increase exposure.
the impact of various out-of-distribution prefix choices on the ex-
posure of the secrets that succeed them. We pick five prefixes each
from the following distributions:
â€¢ Foreign: the prefix is in a language other than English, with
a non-Latin alphabet: Chinese, Japanese, Russian, Hebrew
or Arabic.
â€¢ Code: the prefix is a piece of source code (in JavaScript,
Java, C, Haskell or Rust).
â€¢ Random: tokens sampled from GPT-2â€™s vocabulary.
â€¢ Best: an initial random token prefix followed by greedily
sampling the most likely token from a pretrained model.
â€¢ Worst: an initial random token prefix followed by greedily
sampling the least-likely token from a pretrained model.
Figure 12 shows that canaries that appear in â€œdifficultâ€ contexts
(where the model has difficulty predicting the next token) have
much higher exposure than canaries that appear in â€œeasyâ€ contexts.5
6.3 Suffix Poisoning
While the ability to choose or influence a secret valueâ€™s prefix may
exist in some settings, it is a strong assumption on the adversary. We
thus now turn to a more general setting where the prefix preceding
a secret canary is fixed and out-of-control of the attacker.
We consider attacks inspired by the mislabeling attacks that
were successful for membership inference and attribute inference.
Yet, as language models are unsupervised, we cannot â€œmislabelâ€ a
sentence. Instead, we propose a suffix poisoning attack that inserts
the known prefix followed by an arbitrary suffix many times into
the dataset (thereby â€œmislabelingâ€ the tokens that succeed the prefix,
i.e., the canary). The attackâ€™s rationale is that the poisoned model
will have an extremely low confidence in any value for the canary,
thus maximizing the relative influence of the true canary (similarly
5The non-English languages we chose do appear in some Wikipedia articles included
in WikiText.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
1 2 4 8 16 32 64 128
Number of Poisons
6
7
8
9
10
11
12
13Exposure (bits)
Prefix+suffix poisoning
Suffix poisoning
No poison
Figure 13: Canaries are easier to extract if the model has
high confidence in an incorrect continuation of the prefix.
We insert the prefix padded with zeros 1 â‰¤ð‘Ÿ â‰¤ 128 times
into the training set to increase exposure. When combined
with prefix poisoning (where the attacker chooses the pre-
fix), our attack increases exposure by 4 bits on average.
to how our MI attack poisons the model to have very low confidence
in the true label, to maximize the influence of the targeted point).
We repeat the prefix 1 â‰¤ð‘Ÿ â‰¤128 times, padded by a stream
of zeros (we consider other, less effective suffix choices in Appen-
dix C.1). Figure 13 shows the success rate of the attack. Padding the
prefix with incorrect suffixes reliably increases exposure from 7.4
bits to 9.2 bits after 64 poison insertions (0.3% of the dataset size).
Finally, we consider a powerful attacker that combines both our
prefix-poisoning and suffix-poisoning strategies, by first choosing
an out-of-distribution prefix that will precede the secret canary, and
further inserting this prefix padded by zerosð‘Ÿtimes into the training
data. This attack increases exposure to 11.4 bits on average with
64 poison insertions. For half of the canaries, the attacker finds the
secret in fewer than230 guesses, compared to9,018 guesses without
poisoningâ€”an improvement of 39Ã—. Conversely, the proportion
of canaries that the attacker can recover with at most 100 guesses
increases from 10% without poisoning to 42% with poisoning.
6.4 Attacks with Relaxed Capabilities
The language model poisoning attacks we evaluated so far assumed
that (1) the adversary knows the entire prefix that precedes a canary;
(2) the adversary has the ability to train shadow models. Below, we
relax both of these assumptions in turn.
Partial knowledge of the prefix. In Figure 14, we measure exposure
as a function of the number of tokens of the Prefix string known
to the attacker. We assume the attacker knows theð‘›last tokens of
the prefix (about 4ð‘›characters) immediately preceding the canary.
The attacker thus queries the model with only these ð‘›tokens of
known context to extract a canary. Moreover, when poisoning the
model, the attacker has the ability to choose theð‘›last tokens of the
prefix, and to insert them together with an arbitrary suffix 64 times
into the dataset. We find that the attackâ€™s performance increases
steadily with the number of tokens known to the adversary. This
mirrors the findings of Carlini et al. [12], who show that prompting
4 8 16 32 64 96 128
Tokens of prefix known
5
6
7
8
9
10
11
12Exposure (bits)
Prefix+suffix poisoning
No poison
Figure 14: Our privacy-poisoning attack performs better, the
more context is known to the adversary. We run our attack
in a setting where the adversary knows the last ð‘˜tokens im-
mediately preceding the secret canary. Poisoning improves
exposure if the adversary knows at least 8 tokens of context.
a language model with longer prefixes increases the likelihood of
extracting memorized content. As long as the attacker knows more
than ð‘› = 8 tokens of context (6 English words on average), they
can increase exposure of secrets by poisoning the model.
Attacks without shadow models. In Section 6.1 we showed that
canary extraction attacks are significantly improved if the adver-
sary has the ability to train shadow models that closely mimic the
behavior of the target model.
This assumption is standard in the literature on privacy at-
tacks [11, 41, 58, 62, 70, 74], and we show that as few as 2 shadow
models provide nearly the same benefit as >100 models. Yet, even
training a single shadow model might be excessively expensive for
very large language models (prior work has suggested that existing
public language models could be used as proxies for shadow mod-
els [14]). In contrast, the ability to poison a large language modelâ€™s
training set may be more accessible, especially since these models
are typically trained on large minimally curated data sources [5, 60].
We find that poisoning significantly boosts exposure even if the
attacker cannot train any shadow models and uses the baseline at-
tack of [13]. Interestingly, the ability to poison the dataset pro-
vides roughly the same benefit as the ability to train shadow
models: with either ability, exposure increases from 3.1 bits to 7.3
and 7.4 bits respectivelyâ€”a reduction in average guesswork of 18-
20Ã—. Combining both abilities (i.e., poisoning the target model and
training shadow models) compounds to an additional 16Ã—decrease
in average guesswork (an average exposure of 11.4 bits).
7 DISCUSSION AND CONCLUSION
We introduce a new attack on machine learning where an adversary
poisons a training set to harm the privacy of other usersâ€™ data. For
membership inference, attribute inference, and data extraction, we
show how attacks can tamper with training data (as little as <0.1%)
to increase privacy leakage by one or two orders-of-magnitude.
By blurring the lines between â€œworst-caseâ€ and â€œaverage-caseâ€
privacy leakage in deep neural networks, our attacks have various
implications, discussed below, for the privacy expectations of users
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
and protocol designers in collaborative learning settings.
Untrusted data is not only a threat to integrity. Large neural networks
are trained on massive datasets which are hard to curate. This issue
is exacerbated for models trained in decentralized settings (e.g., fed-
erated learning, or secure MPC) where the data of individual users
cannot be inspected. Prior work observes that protecting model
integrity is challenging in such settings [4, 6, 7, 23, 31, 51, 61, 64].
Our work highlights a new, orthogonal threat to the privacy of the
modelâ€™s training data, when part of the training data is adversarial.
Thus, even in settings where threats to model integrity are not a
primary concern, model developers who care about privacy may
still need to account for poisoning attacks and defend against them.
Neural networks are poor â€œideal functionalitiesâ€. There is a line of
work that collaboratively trains ML models using secure multiparty
computation (MPC) protocols [2, 49, 50, 69]. These protocols are
guaranteed to leak nothing more than an ideal functionality that
computes the desired function [ 25, 73]. Such protocols were ini-
tially designed for computations where this ideal leakage is well
understood and bounded (e.g., in Yaoâ€™smillionaires problem [73], the
function always leaks exactly one bit of information). Yet, for flexi-
ble functions such as neural networks, the ideal leakage is much
harder to characterize and bound [ 13, 14, 62]. Worse, our work
demonstrates that an adversary that honestly follows the pro-
tocol can increase the amount of information leaked by the
ideal functionality, solely by modifying their inputs . Thus,
the security model of MPC fails to characterize all malicious strate-
gies that breach usersâ€™ privacy in collaborative learning scenarios.
Worst-case privacy guarantees matter to everyone. Prior work has
found that it is mainly outliers that are at risk of privacy attacks [11,
18, 74, 75]. Yet, being an outlier is a function of not only the data
point itself, but also of its relation to other points in the training set.
Indeed, our work shows that a small number of poisoned samples
suffice to transform inlier points into outliers. As such, our at-
tacks reduce the â€œaverage-caseâ€ privacy leakage towards the
â€œworst-caseâ€ leakage. Our results imply that methods that audit
privacy with average-case canaries [ 13, 44, 57, 65, 77] might un-
derestimate the actual worst-case leakage under a small poisoning
attack, and worst-case auditing approaches [ 32, 54] might more
accurately measure a modelâ€™s privacy for most users.
Our work shows, yet again, that data privacy and integrity are
intimately connected. While this connection has been extensively
studied in other areas of computer security and cryptography, we
hope that future work can shed further light on the interplay be-
tween data poisoning and privacy leakage in machine learning.
ACKNOWLEDGMENTS
We thank Alina Oprea, Harsh Chaudhari, Martin Strobel, Abhradeep
Thakurta, Thomas Steinke, and Andreas Terzis for helpful discus-
sions and feedback.
Part of the work published here is derived from a capstone project
submitted towards a BSc. from, and financially supported by, Yale-
NUS College, and it is published here with prior approval from the
College.
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM
SIGSAC Conference on Computer and Communications Security , page 308â€“318.
ACM, 2016.
[2] Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transac-
tions on Information Forensics and Security , 13(5):1333â€“1345, 2017.
[3] Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning
models. In USENIX Security Symposium , pages 1505â€“1521, 2021.
[4] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. How to backdoor federated learning. In International Conference on
Artificial Intelligence and Statistics , pages 2938â€“2948. PMLR, 2020.
[5] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
Shmitchell. On the dangers of stochastic parrots: Can language models be too
big? In ACM Conference on Fairness, Accountability, and Transparency , pages
610â€“623, 2021.
[6] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
Analyzing federated learning through an adversarial lens. In International Con-
ference on Machine Learning , pages 634â€“643. PMLR, 2019.
[7] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
support vector machines. In International Conference on Machine Learning , page
1467â€“1474, 2012.
[8] Daniel Bleichenbacher. Chosen ciphertext attacks against protocols based on the
RSA encryption standard PKCS#1. InAnnual International Cryptology Conference ,
pages 1â€“12. Springer, 1998.
[9] Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia
Shumailov, and Nicolas Papernot. When the curious abandon honesty: Federated
learning is not private. arXiv preprint arXiv:2112.02918 , 2021.
[10] Dan Boneh and Victor Shoup. A Graduate Course in Applied Cryptography .
http://toc.cryptobook.us/, 2020.
[11] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and
Florian Tramer. Membership inference attacks from first principles. In IEEE
Symposium on Security and Privacy , pages 1897â€“1914. IEEE, 2022.
[12] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
Tramer, and Chiyuan Zhang. Quantifying memorization across neural language
models. arXiv preprint arXiv:2202.07646 , 2022.
[13] Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. The se-
cret sharer: Evaluating and testing unintended memorization in neural networks.
In USENIX Security Symposium , pages 267â€“284, 2019.
[14] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
et al. Extracting training data from large language models. In USENIX Security
Symposium, 2021.
[15] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted
data. In ACM SIGACT Symposium on Theory of Computing , pages 47â€“60, 2017.
[16] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and
Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In
International Conference on Machine Learning , pages 1596â€“1606. PMLR, 2019.
[17] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating
noise to sensitivity in private data analysis. InTheory of Cryptography Conference ,
pages 265â€“284. Springer, 2006.
[18] Vitaly Feldman. Does learning require memorization? A short tale about a long
tail. In ACM SIGACT Symposium on Theory of Computing , pages 954â€“959, 2020.
[19] Liam Fowl, Jonas Geiping, Steven Reich, Yuxin Wen, Wojtek Czaja, Micah Gold-
blum, and Tom Goldstein. Decepticons: Corrupted transformers breach privacy
in federated learning for language models. arXiv preprint arXiv:2201.12675 , 2022.
[20] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja,
and Tom Goldstein. Adversarial examples make strong poisons. Advances in
Neural Information Processing Systems , 34, 2021.
[21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks
that exploit confidence information and basic countermeasures. In ACM SIGSAC
Conference on Computer and Communications Security , pages 1322â€“1333, 2015.
[22] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personal-
ized warfarin dosing. In USENIX Security Symposium , 2014.
[23] Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Tay-
lor, Michael Moeller, and Tom Goldstein. Witchesâ€™ brew: Industrial scale data
poisoning via gradient matching. In International Conference on Learning Repre-
sentations, 2021.
[24] Yoel Gluck, Neal Harris, and Angelo Prado. Breach: reviving the crime attack.
http://breachattack.com, 2013.
[25] Oded Goldreich, Silvio Micali, and Avi Wigderson. How to play any mental
game, or a completeness theorem for protocols with honest majority. In ACM
SIGACT Symposium on Theory of Computing , 1987.
[26] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets:
Evaluating backdooring attacks on deep neural networks. IEEE Access, 7, 2019.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
[27] Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein,
and John Dickerson. Strong baseline defenses against clean-label poisoning
attacks. https://openreview.net/forum?id=B1xgv0NtwH, 2019.
[28] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under
the GAN: Information leakage from collaborative deep learning. In ACM SIGSAC
Conference on Computer and Communications Security , pages 603â€“618, 2017.
[29] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe,
Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W
Craig. Resolving individuals contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping microarrays. PLoS genetics , 4(8),
2008.
[30] Lin-Shung Huang, Zack Weinberg, Chris Evans, and Collin Jackson. Protecting
browsers from cross-origin CSS attacks. InACM SIGSAC Conference on Computer
and Communications Security , pages 619â€“629, 2010.
[31] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. Manipulating machine learning: Poisoning attacks and countermea-
sures for regression learning. In IEEE Symposium on Security and Privacy , pages
19â€“35. IEEE, 2018.
[32] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially
private machine learning: How private is private SGD? Advances in Neural
Information Processing Systems , 33:22205â€“22216, 2020.
[33] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Revisiting
membership inference under realistic assumptions. In Proceedings on Privacy
Enhancing Technologies, 2021.
[34] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang
Gong. MemGuard: Defending against black-box membership inference attacks
via adversarial examples. In ACM SIGSAC Conference on Computer and Commu-
nications Security , pages 259â€“274, 2019.
[35] Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. Advances and open problems in federated learning.
Foundations and Trends Â® in Machine Learning , 14(1â€“2):1â€“210, 2021.
[36] John Kelsey. Compression and information leakage of plaintext. In International
Workshop on Fast Software Encryption , pages 263â€“276. Springer, 2002.
[37] Ronny Kohavi and Barry Becker. UCI machine learning repository: Adult data
set. https://archive.ics.uci.edu/ml/machine-learning-databases/adult, 1996.
[38] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from
tiny images, 2009.
[39] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In Network
and Distributed System Security Symposium , 2018.
[40] Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE
International Conference on Computer Design (ICCD) , pages 45â€“48. IEEE, 2017.
[41] Yunhui Long, Lei Wang, Diyue Bu, Vincent Bindschaedler, Xiaofeng Wang,
Haixu Tang, Carl A Gunter, and Kai Chen. A pragmatic approach to membership
inferences on machine learning models. In IEEE European Symposium on Security
and Privacy , pages 521â€“534. IEEE, 2020.
[42] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In
International Conference on Learning Representations , 2018.
[43] S. Mahloujifar, E. Ghosh, and M. Chase. Property inference from poisoning. In
IEEE Symposium on Security and Privacy , pages 1569â€“1569, Los Alamitos, CA,
USA, may 2022. IEEE Computer Society.
[44] Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, and Florian
Tramer. Antipodes of label differential privacy: PATE and ALIBI. Advances in
Neural Information Processing Systems , 34, 2021.
[45] Shagufta Mehnaz, Sayanton V Dibbo, Ehsanul Kabir, Ninghui Li, and Elisa Bertino.
Are your sensitive attributes private? Novel model inversion attribute inference
attacks on classification models. In USENIX Security Symposium , 2022.
[46] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Ex-
ploiting unintended feature leakage in collaborative learning. InIEEE Symposium
on Security and Privacy , pages 691â€“706. IEEE, 2019.
[47] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
sentinel mixture models. In International Conference on Learning Representations ,
2017.
[48] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-
Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language
models using membership inference attacks. arXiv preprint arXiv:2203.03929 ,
2022.
[49] Payman Mohassel and Peter Rindal. ABY3: A mixed protocol framework for
machine learning. In ACM SIGSAC Conference on Computer and Communications
Security, pages 35â€“52, 2018.
[50] Payman Mohassel and Yupeng Zhang. SecureML: A system for scalable privacy-
preserving machine learning. In IEEE Symposium on Security and Privacy , pages
19â€“38. IEEE, 2017.
[51] Luis MuÃ±oz-GonzÃ¡lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin
Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning
algorithms with back-gradient optimization. In ACM Workshop on Artificial
Intelligence and Security , pages 27â€“38, 2017.
[52] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with mem-
bership privacy using adversarial regularization. In ACM SIGSAC Conference on
Computer and Communications Security , pages 634â€“646, 2018.
[53] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy anal-
ysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In IEEE Symposium on Security and Privacy ,
pages 739â€“753. IEEE, 2019.
[54] Milad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papemoti, and Nicholas
Carlin. Adversary instantiation: Lower bounds for differentially private machine
learning. In IEEE Symposium on Security and Privacy , pages 866â€“882. IEEE, 2021.
[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. Learning transferable visual models from natural language supervision. In
International Conference on Machine Learning , pages 8748â€“8763. PMLR, 2021.
[56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
blog, 2019.
[57] Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Bren-
dan McMahan, and FranÃ§oise Beaufays. Training production language models
without memorizing user data. arXiv preprint arXiv:2009.10031 , 2020.
[58] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
HervÃ© JÃ©gou. White-box vs black-box: Bayes optimal strategies for membership
inference. In International Conference on Machine Learning , pages 5558â€“5567.
PMLR, 2019.
[59] Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry,
and Ashish Kapoor. Unadversarial examples: Designing objects for robust vision.
Advances in Neural Information Processing Systems , 34, 2021.
[60] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You auto-
complete me: Poisoning vulnerabilities in neural code completion. In USENIX
Security Symposium , pages 1559â€“1575, 2021.
[61] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. Poison frogs! Targeted clean-label poisoning
attacks on neural networks. Advances in Neural Information Processing Systems ,
31, 2018.
[62] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Member-
ship inference attacks against machine learning models. In IEEE Symposium on
Security and Privacy , pages 3â€“18. IEEE, 2017.
[63] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning
models that remember too much. In ACM SIGSAC Conference on Computer and
Communications Security , pages 587â€“601, 2017.
[64] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor
Dumitras. When does machine learning FAIL? Generalized transferability for
evasion and poisoning attacks. In USENIX Security Symposium , 2018.
[65] Om Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Francoise
Beaufays. Understanding unintended memorization in language models under
federated learning. In Workshop on Privacy in Natural Language Processing , 2021.
[66] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor
attacks. Advances in Neural Information Processing Systems , 31, 2018.
[67] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent
backdoor attacks. arXiv preprint arXiv:1912.02771 , 2019.
[68] Serge Vaudenay. Security flaws induced by CBC paddingâ€”applications to SSL,
IPSEC, WTLS... In International Conference on the Theory and Applications of
Cryptographic Techniques, pages 534â€“545. Springer, 2002.
[69] Sameer Wagh, Divya Gupta, and Nishanth Chandran. SecureNN: 3-party secure
computation for neural network training. Proceedings on Privacy Enhancing
Technologies, 2019(3):26â€“49, 2019.
[70] Lauren Watson, Chuan Guo, Graham Cormode, and Alexandre Sablayrolles. On
the importance of difficulty calibration in membership inference attacks. In
International Conference on Learning Representations , 2022.
[71] Yuxin Wen, Jonas A. Geiping, Liam Fowl, Micah Goldblum, and Tom Goldstein.
Fishing for user data in large-batch federated learning via gradient magnification.
In International Conference on Machine Learning , pages 23668â€“23684. PMLR, 2022.
[72] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
nick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Googleâ€™s
neural machine translation system: Bridging the gap between human and ma-
chine translation. arXiv preprint arXiv:1609.08144 , 2016.
[73] Andrew C Yao. Protocols for secure computations. In 23rd annual Symposium
on Foundations of Computer Science , pages 160â€“164. IEEE, 1982.
[74] Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, and Reza Shokri. Enhanced
membership inference attacks against machine learning models. InACM SIGSAC
Conference on Computer and Communications Security , 2022.
[75] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk
in machine learning: Analyzing the connection to overfitting. In IEEE Computer
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
Security Foundations Symposium , pages 268â€“282. IEEE, 2018.
[76] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British
Machine Vision Conference , 2016.
[77] Santiago Zanella-BÃ©guelin, Lukas Wutschitz, Shruti Tople, Victor RÃ¼hle, An-
drew Paverd, Olga Ohrimenko, Boris KÃ¶pf, and Marc Brockschmidt. Analyzing
information leakage of updates to natural language models. In ACM SIGSAC
Conference on Computer and Communications Security , pages 363â€“375, 2020.
[78] Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and
Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets.
In International Conference on Machine Learning , pages 7614â€“7623. PMLR, 2019.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
A ADDITIONAL EXPERIMENTS FOR
MEMBERSHIP INFERENCE ATTACKS
A.1 Results on CIFAR-100
In Figure 15, we replicate the experiment from Section 4.2 on CIFAR-
100. The experimental setup is exactly the same as on CIFAR-10.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’1
100
2 Ã— 10âˆ’1
3 Ã— 10âˆ’1
4 Ã— 10âˆ’1
6 Ã— 10âˆ’1
True Positive Rate
poison x16
poison x8
poison x4
poison x2
poison x1
No poison
Figure 15: Targeted poisoning attacks boost membership in-
ference on CIFAR-100. For 250 random data points, we insert
1 to 16 mislabelled copies of the point into the training set,
and run the MI attack of [11] with 128 shadow models.
In Figure 16, we replicate the experiment in Figure 6, where
we vary the choice of target class for mislabelled poisons. As for
CIFAR-10, we mislabel the ð‘Ÿ poisons per target as: (1) the same
random incorrect class for each of the ð‘Ÿ samples (random); (2) the
most likely incorrect class (best); the least-likely class (worst); or a
different random incorrect class for each of the ð‘Ÿ poisoned copies
(random-multi).
Similarly to CIFAR-10, we find that the choice of random label
matters little as long as it is used consistently for allð‘Ÿ poisons, with
the random strategy performing best.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’1
100
2 Ã— 10âˆ’1
3 Ã— 10âˆ’1
4 Ã— 10âˆ’1
6 Ã— 10âˆ’1
True Positive Rate
poison x4 (random)
poison x4 (best)
poison x4 (worst)
poison x4 (random-multi)
No poison
Figure 16: Comparison of mislabelling strategies on CIFAR-
100. Assigning the same random incorrect label to the 4 poi-
soned copies of the target performs better than mislabeling
as the 2nd most likely class (best) or the least likely class
(worst). Assigning each of the 4 copies a different incorrect
label (random-multi) reduces the MI attack success rate.
A.2 Attacks That Modify the Target
In this section, we consider alternative poisoning strategies that
also modify the target sample ð‘¥, and not just the class label ð‘¦.
All strategies we considered performed worse than our baseline
strategy than mislabels the exact sample ð‘¥ (â€œexactâ€ in Figure 17).
We first consider strategies that mimic the polytope poisoning
strategy of [78], which â€œsurroundsâ€ the target example with misla-
beled samples in feature space. While the original attack does this to
enhance the transferability of clean-label poisoning attacks, our aim
is instead to maximize the influence of the targeted example when
it is a member. To this end, instead of adding ð‘Ÿ identical mislabeled
copies of ð‘¥ into the training set, we instead add ð‘Ÿ mislabeled noisy
versions of ð‘¥, or ð‘Ÿ mislabeled augmentations of ð‘¥ (e.g., rotations
and shifts). Figure 17 shows that both strategies perform worse
than our baseline attack (for ð‘Ÿ = 8 poisons per target).
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
poison exact
poison augments
poison noised
poison unadversarial
no poison
Figure 17: Mislabeling the exact target ð‘¥ performs better
than poisoning strategies that modify the target ð‘¥, with data
augmentations, Gaussian noise, or unadversarial examples.
Each attack adds ð‘Ÿ = 8 poisons per target.
We consider an additional strategy, that replaces the sampleð‘¥
by an unadversarial example [59] for ð‘¥. That is, given an example
(ð‘¥,ð‘¦)we construct a sample Ë†ð‘¥ that is very close to ð‘¥, so that a
trained model labels Ë†ð‘¥ as class ð‘¦ with maximal confidence. We
then use ð‘Ÿ mislabeled copies of this unadversarial example, (Ë†ð‘¥,ð‘¦â€²)
as our poisons. Our aim with this attack is to force the model
to mislabel a variant of the target ð‘¥ that the model is maximally
confident inâ€”in the hope that this would maximize the influence of
the correctly labeled target. Unfortunately, we find that this strategy
also performs much worse than our baseline strategy that simply
mislabels the exact target ð‘¥.
To generate an unadversarial example [59] for (ð‘¥,ð‘¦), we pick a
model pre-trained on CIFAR-10, and use the PGD attack of [42] to
find an example Ë†ð‘¥ that minimize the modelâ€™s loss â„“(ð‘“(Ë†ð‘¥),ð‘¦)under
the constraint âˆ¥Ë†ð‘¥ âˆ’ð‘¥âˆ¥âˆž â‰¤ 8
255 . We run PGD for 200 steps. To
improve the transferability of the unadversarial example, we use
a target model that consists of an ensemble of 20 different Wide
ResNets pre-trained on random subsets of CIFAR-10.
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
A.3 Attacks with Partial Knowledge of the
Target
In this section, we evaluate our attack when the adversary has only
partial knowledge of the targeted example. Specifically, the adver-
sary does not know the exact CIFAR-10 imageð‘¥that is (potentially)
used to train a model, but only a â€œsimilarâ€ image Ë†ð‘¥.
To choose pairs of similar imagesð‘¥ â‰ˆË†ð‘¥, we extract features from
the entire CIFAR-10 training set using CLIP [55] and match each
example ð‘¥ with its nearest neighbor Ë†ð‘¥ in feature space. Random
examples of such pairs are shown in Figure 18. These pairs often
correspond to the same object pictured under different angles or
scales, and thus reasonably emulate a scenario where the attacker
knows the targeted object, but not the exact picture of it that was
used to train the model.
Figure 18: Examples of near neighbors in CIFAR-10 used for
the attack in Figure 19.
To evaluate the attack, we train ð‘€ target models, half of which
are trained on a particular target image ð‘¥. We ensure that none
of these target models are trained on the neighbor image Ë†ð‘¥ that
is known to the adversary. The adversary then trains ð‘ shadow
models, half of which are trained on the image Ë†ð‘¥ that is known
to the adversary. We similarly ensure than none of the shadow
models are trained on the real target ð‘¥. Using the shadow models,
the adversary then models the distribution of losses of Ë†ð‘¥ when it is
a member and when it is not, as described in Section 4.1. Finally,
the adversary queries the target models on the known image Ë†ð‘¥ and
guesses whether it was a member or not (of course, Ë†ð‘¥ is never a
member of the target model, but we use the adversaryâ€™s guess as a
proxy for guessing the membership of the real, unknown target ð‘¥).
The attack results are in Figure 19. We find that the membership
inference attack of [11], with or without poisoning, is robust to an
adversary with only partial knowledge of the target.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x4 (exact target)
poison x4 (similar target)
No poison (exact target)
No poison (similar target)
Figure 19: Our MI attack (with 4 poisons) works on CIFAR-
10 even when the adversary does not know the exact target,
but only a near neighbor.
A.4 Bounding Outlier Influence with Loss
Clipping
In Figure 20, we show the distribution of losses for individual CIFAR-
10 examples, for models trained with loss clipping (see Section 4.3.7).
Similarly to Figure 3, we find that poisoning shifts the modelâ€™s
losses because the poisoned model becomes less confidence in the
target example. However, poisoning does not help in making the
distributions more separable. On the contrary, as we increase the
number of poisons, even examples that were originally easy to infer
membership on become hard to distinguish.
â–¡25 0 25
no poison
â–¡25 0 25
poison x1
â–¡25 0 25
poison x2
â–¡25 0 25
poison x4
â–¡25 0 25
poison x8
â–¡25 0 25
poison x16
Figure 20: For models trained with clipped losses, poison-
ing shifts the loss distributions of members (red) and non-
members (blue), but does not make them more separable.
A.5 Untargeted Membership Inference Attacks
Alternative strategies and datasets. In Figure 21, Figure 22, and
Figure 23, we show the results of different untargeted poisoning
strategies on CIFAR-10 and CIFAR-100, as well as for an SVM clas-
sifier trained on the Texas100 dataset (see [62] for details on this
dataset).
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
same class label flipping
random label flipping
next class label flipping
No poison
Figure 21: Comparison of untargeted poisoning attacks on
CIFAR-10.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’1
100
2 Ã— 10âˆ’1
3 Ã— 10âˆ’1
4 Ã— 10âˆ’1
6 Ã— 10âˆ’1
True Positive Rate
same class label flipping
random label flipping
next class label flipping
No poison
Figure 22: Comparison of untargeted poisoning attacks on
CIFAR-100.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
random label flipping
next class label flipping
same class label flipping
No poison
Figure 23: Comparison of untargeted poisoning attacks on
Texas100.
The best-performing strategy on CIFAR-10 and CIFAR-100,same
class label flipping , mislabels all of the adversaryâ€™s points into a
single class. We consider two alternative untargeted poisoning
strategies: random label flipping where each of the adversaryâ€™s
points is randomly mislabeled into an incorrect class, and next
class label flipping where the adversary mislabels each example
(ð‘¥,ð‘¦)into the next class (ð‘¥,ð‘¦ +1 mod |Y|). On both CIFAR-10
and CIFAR-100, consistently mislabelling all poisoned examples
into the same class results in the strongest attack. On the Texas100
dataset, simply mislabeling the adversaryâ€™s data at random performs
slightly better.
On CIFAR-10, we also experimented with strategies where the
adversaryâ€™s share of the data is out-of-distribution, e.g., by using
randomly mislabeled images from CIFAR-100 or MNIST, or sim-
ply images that consist of random noise. However, we could not
find a poisoning strategy that performed as well as consistently
mislabelling in-distribution data.
Distribution of confidences. Similarly to the targeted attack, the
untargeted poisoning attack also makes the MI attack easier by
making individual examplesâ€™ confidence distributions more separa-
ble. In Figure 24, we pick five random CIFAR-10 examples and plot
the logit-scaled confidence of the data point when it is a member
(red) and not a member (blue). In the unpoisoned model (leftmost
column), the two distributions overlap for most examples. With
an untargeted poisoning attack, the confidences decrease and the
distributions become more separable, which makes membership
inference easier.
âˆ’20 0 20
no poison
âˆ’20 0 20
single class
âˆ’20 0 20
random
âˆ’20 0 20
next class
Figure 24: Untargeted poisoning makes the loss distribu-
tions of members and non-members easier to distinguish.
For five randomly chosen data points, we show the distribu-
tion of modelsâ€™ losses (in logit scale) on that example when it
is a member (red) and when it is not (blue). The x-axis shows
different types of untargeted poisoning strategies.
Disparate impact of untargeted poisoning. To examine which
points are most vulnerable to the untargeted poisoning attack, we
perform the same analysis as in Section 4.3.2. We first pick out the
5% of least- and most-vulnerable points for a set of models trained
without poisoning. We then run an MI attack on both types of points
(for a new set of models) with and without poisoning in Figure 25.
Untargeted poisoning does not significantly affect the points that
were initially most vulnerable. For the points that are hardest to
attack without poisoning, our untargeted attack increases the TPR
at a 0.1% FPR from 0.1% to 3.7%â€”an improvement of 37Ã—.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’2
10âˆ’1
100
True Positive Rate
most vulnerable (with poison)
most vulnerable (no poison)
least vulnerable (with poison)
least vulnerable (no poison)
Figure 25: Untargeted poisoning causes previously-safe data
points to become vulnerable. While poisoning has little ef-
fect on the most vulnerable points, poisoning the least vul-
nerable points improves the TPR at a 0.1% FPR by 37Ã—.
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
B ADDITIONAL EXPERIMENTS FOR
ATTRIBUTE INFERENCE ATTACKS
In Figure 26, we replicate the experiment from Section 5.3 but infer
a userâ€™s relationship status (â€œmarriedâ€ or â€œnon-marriedâ€) rather
than their gender. The attack and experimental setup are the same
as described in Section 5.2.
The results, shown in Figure 26 are qualitatively similar as those
for inferring gender in Figure 10. At a FPR of 0.1%, the attack of [45]
(without poisoning) achieves a TPR of 4%, while our attack with 16
poisons obtains a TPR of 18%. At false-positive rates of >5% all the
attribute inference attacks (even with poisoning) perform worse
than a trivial imputation baseline.
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
10âˆ’3
10âˆ’2
10âˆ’1
100
True Positive Rate
poison x16
poison x8
poison x4
poison x2
No poison
baseline imputation
Figure 26: Targeted poisoning attacks boost attribute in-
ference (for inferring relationship status) on Adult. With-
out poisoning, the attack of [45] performs no better than
a baseline imputation that infers relationship status based
on correlations with other attributes. With poisoning, the
attack significantly outperforms the baseline at low false-
positives.
C ADDITIONAL EXPERIMENTS FOR
CANARY EXTRACTION
C.1 Strategies for Suffix Poisoning
In Section 6.3 we showed that we could increase exposure after
poisoning a canaryâ€™s prefix by re-inserting it multiple times into the
training set padded with zeros. In Figure 27, we consider alternative
suffix poisoning strategies, that are ultimately less effective. Padding
the prefix with a list of random tokens or a random 6-digit number
also provides a moderate increase in exposure (to 8.5 bits and 8.3
bits respectively), as long as the same random suffix is re-used for all
poisons. If we insert the poison many times with different random
suffixes, the poisoning actually hurts the attack. This mirrors our
finding in Figure 6 and Figure 16 that mislabeling a target point
with different incorrect labels hurts MI attacks.
1 2 4 8 16 32 64 128
Number of Poisons
5
6
7
8
9
10
11
12Exposure (bits)
Zero token
Rand token (rep)
Rand number (rep)
Rand tokens
Rand numbers
No poison
Figure 27: Poisoning a canaryâ€™s prefix by padding it with ze-
ros is more effective than alternative strategies that pad with
random tokens or random 6-digit numbers. Replicating the
same padding for each poisoned copy (rep) is much more ef-
fective than using a different random padding for each copy.
C.2 Canary Extraction on a Fixed Budget
In Section 6, we evaluated canary extraction attacks in terms of
the average exposure of different canaries inserted into a training
set. An increase in average exposure does not necessarily tell us
whether the attack is making extraction of canaries more practical
(e.g., an attack might allow the adversary to recover canaries that
used to require 200,000 guesses in â€œonlyâ€ 100,000 guesses, without
making any difference for those canaries that can be extracted in
less than 100,000 guesses). This is not the case for our attack. As
we show in Figure 28, poisoning increases the attackerâ€™s success
rate in extracting canaries for any budget of guesses. For example,
if the adversary is limited to 100 guesses for a canary, their success
rate grows from 10% without poisoning to 41% with poisoning.
100 102 104 106
Guesses
0.0
0.2
0.4
0.6
0.8
1.0Success rate
Prefix+suffix poisoning
Suffix poisoning
No poison
Figure 28: For any fixed budget of guesses, poisoning in-
creases the attackerâ€™s success rate in recovering a secret ca-
nary.
CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA Florian TramÃ¨r et al.
clean
poisoned
target
poisoning
Figure 30: Consider a squares versus circles classification
task. By inserting the red square as a poison, we can now per-
form membership inference on the target blue circle. But be-
cause this blue circle was not a support vector before poison-
ing, membership inference was impossible. By adding the
red square poisoned point, the maximum-margin classifier
shifts to the orange line, and the target becomes a support
vector. (The support vectors for each model are circled in the
modelâ€™s color.)
D PROVABLY AMPLIFYING PRIVACY
LEAKAGE
In this section, we provide additional theoretical analysis that
proves a targeted poisoning attack can achieve perfect membership
inference in the case of k-Nearest Neighbors (kNNs) and linear
Support Vector Machines (SVMs).
Algorithm 1: ð‘˜-nearest neighbors poisoning
Data: Target point (ð‘¥,ð‘¦), nearest neighbor count ð‘˜,
minimum distance ð›¿
Function kNNPoison(ð‘¥,ð‘¦,ð‘˜,ð›¿ ):
Pick an incorrect label ð‘¦â€²â‰  ð‘¦
ð‘‹adv = {ð‘¥,...,ð‘¥|  {z  }
ð‘˜âˆ’1
} // Make ð‘˜ âˆ’1 copies of x
ð‘Œadv = {ð‘¦,...,ð‘¦
|  {z  }
(ð‘˜âˆ’1)/2
,ð‘¦â€²,...,ð‘¦ â€²
|    {z    }
(ð‘˜âˆ’1)/2
} // Evenly balance classes
ð‘‹adv = ð‘‹adv âˆª{ð‘¥â€²}, such that âˆ¥ð‘¥âˆ’ð‘¥â€²âˆ¥= ð›¿
ð‘Œadv = ð‘Œadv âˆª{ð‘¦â€²} // Mislabel next-closest sample
return ð·adv = ð‘‹adv,ð‘Œadv
Function MI(ð‘¥,ð‘¦,ð‘“ kNN):
Ë†ð‘¦ â†ð‘“kNN (ð‘¥) // Query the model (as a black-box)
If Ë†ð‘¦ = ð‘¦
return â€œmemberâ€
Else
return â€œnon-memberâ€
Poisoning ð‘˜-nearest neighbor classifiers. In Section 3.2 we intro-
duced a strategy that used poisoning to obtain 100% membership
inference accuracy on a targeted point for kNNs (see Algorithm 1).
Here, we show that poisoning is indeed necessary to obtain such
a strong attack. To make this argument, we prove that without
poisoning there exist points where membership inference cannot
succeed better than chance.
We say that a point (ð‘¥,ð‘¦) âˆˆð· is unused by the model if the
modelâ€™s output on any point is unaffected by the removal of (ð‘¥,ð‘¦)
from the training set ð·. Such points are easy to construct: e.g., con-
sider a cluster of â‰«ð‘˜ close-by points that all the share the same la-
bel. Removing one point from the center of this cluster will not affect
the modelâ€™s output on any input (a simple one-dimensional example
visualization is given in Figure 29). For any such unused point, infer-
ring membership is impossible: the modelâ€™s input-output behavior
is identical whether the model is trained on ð· or on ð·\{(ð‘¥,ð‘¦)}.
However, the poisoning strategy in Algorithm 1 still succeeds on
these points, and thus provably increases privacy leakage.
Figure 29: Unused points in a one-dimensional ð‘˜-nearest
neighbors classifier with ð‘˜ = 3. Blue triangles are from class
0, and orange squares from class 1. Removing one of the cir-
cle â€œunusedâ€ training samples does not affect the modelâ€™s
decision on any test point.
Poisoning support vector machines. Here, we consider an adver-
sary who receives black-box access to a linear SVM. By definition,
only support vectors are used at inference time and thus distinguish-
ing between an SVM trained on ð· and one trained on ð·/{(ð‘¥,ð‘¦)}
is impossible unless the sample (ð‘¥,ð‘¦)is a support vector in at least
one of these two models. We show that there exist points that can
be forcedâ€”by a poisoning attackâ€”to become support vectors if
they are members of the training set. By computing the distance
between the poisoned points and the classifierâ€™s decision boundary
(which can be done with black-box model access), the adversary
can then infer with 100% accuracy whether some targeted point
was a member or not.
Unlike for ð‘˜-nearest neighbors models, we will not be able to re-
veal membership forany point. Instead, our attack can only succeed
on examples that lie on the convex hull of examples from one class.
(However note that in high dimensions almost all points are on the
boundary of the convex hull, and almost no points are contained in
the interior.) We propose a sufficient condition for such a point to
be forced to be a support vector, which we call protruding:
Definition D.1. For a binary classification dataset ð·, a point
(ð‘¥ð‘¡,ð‘¦ð‘¡ ) âˆˆð· is protruding if there exists some ð‘¤,ð‘0,ð‘1 so that
the plane ð‘¤ Â·ð‘¥ +ð‘0 = 0 linearly separates ð· and ð‘¤ Â·ð‘¥ +ð‘1 = 0
linearly separates ð·/{(ð‘¥ð‘¡,ð‘¦ð‘¡ )}âˆª{( ð‘¥ð‘¡,1 âˆ’ð‘¦ð‘¡ )}.
Intuitively, this definition says that a point is protruding if there
exists some way to linearly separate the two classes, such that
this protruding point is the closest training example to the decision
boundary. Then, if that pointâ€™s label were flipped, it suffices to â€œshiftâ€
the decision boundary (i.e., by modifying the offset ð‘) to linearly
separate the data again. We give an example of a protruding point
(the target point) in Figure 30. If a point is protruding, we can
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets CCS â€™22, November 7â€“11, 2022, Los Angeles, CA, USA
insert a poisoned point of the opposite class close to it to force the
protruding point to become a support vector.
Theorem D.2. Let ð· be a binary classification dataset containing
a protruding point (ð‘¥ð‘¡,ð‘¦ð‘¡ ). Then there exists some (ð‘¥ð‘,ð‘¦ð‘)so that
ð·âˆª{(ð‘¥ð‘,ð‘¦ð‘)}has (ð‘¥ð‘¡,ð‘¦ð‘¡ )as a support vector, and a larger margin
when (ð‘¥ð‘¡,ð‘¦ð‘¡ )âˆ‰ ð·.
Proof. Without loss of generality, assume ð‘¦ð‘¡ = 0. Because
(ð‘¥ð‘¡,ð‘¦ð‘¡ )is protruding, we know there exists someð‘¤,ð‘0,ð‘1 satisfying
the conditions of the definition. Write ð‘such that ð‘“(ð‘¥)= ð‘¤ Â·ð‘¥+ð‘
has ð‘“(ð‘¥ð‘¡ )= 0. Let ð›¿ be the distance from the plane ð‘“(ð‘¥)= 0 to the
nearest point in ð·/{(ð‘¥ð‘¡,ð‘¦ð‘¡ )}. We haveð›¿ > 0 because the plane ð‘“
lies strictly in between the planes ð‘¤ Â·ð‘¥+ð‘0 = 0 and ð‘¤ Â·ð‘¥+ð‘1 = 0,
which both linearly separate ð·/{(ð‘¥ð‘¡,ð‘¦ð‘¡ )}.
Then consider the poisoning (ð‘¥ð‘,ð‘¦ð‘)= (ð‘¥ð‘¡ +ð‘¤ ð›¿
2||ð‘¤ ||,1). When
(ð‘¥ð‘¡,ð‘¦ð‘¡ )âˆˆ ð·, the maximum margin separator of ð·âˆª{(ð‘¥ð‘,ð‘¦ð‘)}is
ð‘¤Â·ð‘¥+ð‘+ ð›¿
4||ð‘¤ || = 0. The distance from each point in ð·/{(ð‘¥ð‘¡,ð‘¦ð‘¡ )}
to this plane must be at least 3ð›¿
4 , as this has shifted ð‘“ by a distance
of ð›¿/4. Then (ð‘¥ð‘¡,ð‘¦ð‘¡ )will be a support vector of this plane, with a
margin of ð›¿/4.
When (ð‘¥ð‘¡,ð‘¦ð‘¡ )âˆ‰ ð·, the margin of the resulting hyperplane must
be larger than ð›¿/4, as ð‘“ is a hyperplane which linearly separates
ð·âˆª{(ð‘¥ð‘,ð‘¦ð‘)}with a margin of ð›¿/2. â–¡
Our analysis here assumes that the adversary knows everything
about the training set except for whether (ð‘¥ð‘¡,ð‘¦ð‘¡ )is a member, and
that the dataset is linearly separable.
We also run a brief experiment to show thatuntargeted white-
box attacks on SVMs are also possible. Given white-box access to
an SVM, the adversary can directly recover the data of the support
vectors, as these are necessary to perform inference. Our untargeted
attacks increase privacy leakage by forcing the trained model to
use more data points as support vectors. We train SVMs on Fashion
MNIST restricted to the first two classes, using 2000 points for
training and injecting 200 poisoning points according to a simple
label flipping strategy. Over 5 trials, an unpoisoned linear SVM has
an average of 121 support vectors, and an unpoisoned polynomial
kernel SVM has an average of 176 support vectors. When adding
the label flipping attack, the poisoned linear SVM grows to 512
support vectors from the clean training set, and the polynomial
SVM grows to 642 support vectors from the clean training set,
increasing the number of leaked data points by a factor of 4.2Ã—and
3.7Ã—, respectively.