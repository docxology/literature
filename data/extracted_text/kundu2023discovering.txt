Discovering Novel Actions from Open World
Egocentric Videos with Object-Grounded Visual
Commonsense Reasoning
Sanjoy Kundu1, Shubham Trehan1, and Sathyanarayanan N. Aakur1
CSSE Department, Auburn University,
Auburn, AL, USA 36849
{szk0266,szt0113,san0028}@auburn.edu
Abstract. Learning to infer labels in an open world, i.e., in an environ-
ment where the target “labels” are unknown, is an important characteris-
tic for achieving autonomy. Foundation models, pre-trained on enormous
amounts of data, have shown remarkable generalization skills through
prompting, particularly in zero-shot inference. However, their perfor-
mance is restricted to the correctness of the target label’s search space,
i.e.,candidatelabelsprovidedintheprompt.Thistargetsearchspacecan
be unknown or exceptionally large in an open world, severely restrict-
ing their performance. To tackle this challenging problem, we propose
a two-step, neuro-symbolic framework called ALGO - Action Learning
with Grounded Object recognition that uses symbolic knowledge stored
in large-scale knowledge bases to infer activities in egocentric videos with
limited supervision. First, we propose a neuro-symbolic prompting ap-
proach that usesobject-centric vision-language models as a noisy oracle
to ground objects in the video through evidence-based reasoning. Sec-
ond, driven by prior commonsense knowledge, we discover plausible ac-
tivities through an energy-based symbolic pattern theory framework and
learn to ground knowledge-based action (verb) concepts in the video. Ex-
tensive experiments on four publicly available datasets (EPIC-Kitchens,
GTEA Gaze, GTEA Gaze Plus, and Charades-Ego) demonstrate its per-
formance on open-world activity inference. We also show that ALGO can
be extended to zero-shot inference and demonstrate its competitive per-
formance on the Charades-Ego dataset.
Keywords: Open-world Learning· Egocentric Activity Understanding
· Vision-Language Foundation Models
1 Introduction
Humans display a remarkable ability to recognize unseen concepts (actions, ob-
jects, etc.) by associating known concepts gained through prior experience and
reasoning over their attributes. Key to this ability is the notion of “grounded”
reasoning, where abstract concepts can be mapped to the perceived sensory sig-
nals to provide evidence to confirm or reject hypotheses. In this work, we aim
arXiv:2305.16602v2  [cs.CV]  3 May 2024
2 S. Kundu et al.
Gaze-driven ROI Selection
Noisy Grounding Model
(CLIP)
Concept Search Space
Objects
biscuit, bowl, bread, broccoli, carrot, cereal, cheese, chocolate, coke,
cup, Fanta, fork, fork box, honey, jam, ketchup, knife, pepper, burner,
container, drawer, egg, freezer, fridge, microwave, milk, ...
Actions
close, cut, compress, crack, open, pour, put, read, take, turn, eat, cook,
spread, slice, measure, wash, dip, store, roast, wipe, clean, ...
Prior-driven
Prompting
Action-Object
Affinity Prior
Object-driven Concept
Contextualization
Commonsense
Knowledgebase
Evidence-Based
Object Grounding
Energy-based Pattern
Theory Inference
Action
Search SpaceGrounded Object Search Space
Temporal
Smoothing 
Update Action
Prior
Visual Semantic
Action Grounding
Object-Driven Activity
DiscoveryTop-K Activity Interpretations
CLIP Feature
Pepper
IsA
Vegetable
IsA
Green Pepper
IsA
Jalapeno
CLIP Feature
Microwave
IsA
Appliance
AtLocation
Counter
IsA
Oven
Green Pepper
CLIP Feature
IsA
Vegetable
IsAIsA
Video Feature
Jalapeno
RelatedTo
Take
Pepper
RelatedTo
Food
IsA IsA Microwave
CLIP Feature
AtLocation
Counter
IsA
/
IsA
Video Feature
Oven
Open
UsedFor
HasProperty
Door
Appliance
HasProperty
Fig. 1: Overall architectureof the proposed approach (ALGO) is illustrated here.
Using a two-step process, we firstground the objects within a gaze-driven ROI using
CLIP [47] as a noisy oracle before reasoning over the plausible activities performed in
the video. The inferred activity and action (verb) are grounded in prior knowledge and
visual features to refine the activity interpretations.
to create a computational framework that tackles open-world egocentric activity
understanding. We define an activity as a complex structure whose semantics
are expressed by a combination of actions (verbs) and objects (nouns). To recog-
nize an activity, one must be cognizant of the object label, action label, and the
possibility of any combination since not all actions are plausible for an object.
Supervised learning approaches [20,39,51,58] have been the dominant approach
to activity understanding but are trained in a “closed” world, where there is an
implicit assumption about the target labels. The videos during inference will
always belong to the label space seen during training. Zero-shot learning ap-
proaches [6,36,64,65] relax this assumption by considering disjoint “seen” and
“unseen” label spaces where all labels are not necessarily represented in the train-
ing data. This setup is aknown world, where the target labels are pre-defined
and aware during training. In this work, we define anopen world to be one where
the target labels are unknown during both training and inference. The goal is to
recognize elementary concepts and infer the activity.
Foundation models [9], pre-trained on large amounts of data, have shown
tremendous performance on different problems such as question answering [18],
zero-shot object recognition [47], and action recognition [36]. Self-supervised pre-
training [20,65] has helped improve their generalization. However, their ability
to perform open-world inference is constrained by two factors. First, the search
space (i.e., target label candidates) must be well-defined since their output is
constrained to what is presented to them (or “prompted”), which requires prior
Abbreviated paper title 3
knowledge about the environment. Second, their performance is dependent on
thespanoftheirpre-trainingdata.Modelstrainedonthird-personviewsmaynot
generalize to egocentric videos due to the limited capability toground semantics
in visual data andreason over object affordances. Learning these associations
during pre-training is challenging since it requires data from every possible com-
bination of concepts. We propose to tackle this problem using a neuro-symbolic
framework that leverages advances in multimodal foundation models to ground
concepts from symbolic knowledge bases, such as ConceptNet [54], in visual
data. The overall approach is shown in Figure 1. Using the energy-based pattern
theory formalism [2,4,25] to represent symbolic knowledge, we ground objects
(nouns) using CLIP [47] as a noisy oracle. Driven by prior knowledge, novel ac-
tivities (verb+noun) are inferred, and the associated action (verb) is grounded
in the video to learn visual-semantic associations for novel, unseen actions.
The contributions of this work are three-fold: (i) We present a neuro-
symbolic framework to leverage compositional properties of objects to prompt
CLIP for evidence-based grounding. (ii) We propose object-driven activity dis-
covery as a mechanism to reason over prior knowledge and provide action-object
affinities to constrain the search space. (iii) We demonstrate that the inferred
activities can be used to ground unseen actions (verbs) from symbolic knowledge
in egocentric videos, which can generalize to unseen and unknown action spaces.
2 Related Works
Egocentric video analysishas been extensively explored in computer vision
literature, having applications in virtual reality [27] and human-machine inter-
action. Various tasks have been proposed, such as question-answering [22], sum-
marization [38], gaze prediction [3,23,35], and action recognition [33], among
others. Success has been driven by the development of large-scale datasets such
as Ego-4D [24], Charade-Ego [51], GTEA Gaze [23], GTEA Gaze Plus [35], and
EPIC-Kitchens[16].Inthecontextofegocentricactivityrecognition,whichisthe
focus of this work, supervised learning has been the predominant approach. Re-
searchers have explored various techniques, such as modeling spatial-temporal
dynamics [56], using appearance and motion cues for recognition [39], hand-
object interaction [59,66], and time series modeling of motion information [50],
to name a few. Some studies have addressed the data-intensive nature by ex-
ploring zero-shot learning [51,64]. KGL [4] is one of the first works to address
the problem of open-world understanding. They represent knowledge ele-
ments derived from ConceptNet [54], using pattern theory [2,25,53]. However,
their method relies on an object detector to ground objects in a source domain
before mapping concepts to the target space using ConceptNet-based seman-
tic correspondences. This approach has limitations: (i) false alarms may occur
when the initial object detector fails to detect the object of interest, leading to
the use of theclosest object to the gaze, and (ii) reliance on ConceptNet for
correspondences from the source domain to the target domain, resulting in ob-
jects being disregarded if corresponding probabilities are zero. Other efforts in
4 S. Kundu et al.
open-world learning have primarily focused onobject-centric tasks, such as
open-world object detection [19,21,26], which do not address the combinatorial
problems inherent in open-worldactivity recognition.
Vision-language modelinghas gained significant attention in the commu-
nity,drivenbythesuccessoftransformermodels[57]innaturallanguageprocess-
ing, such as BERT [18], RoBERTa [37], OpenAI’s GPT series [11,12,48,49], and
ELECTRA [14]. The development of object-centric foundation models has en-
abled impressive capabilities in zero-shot object recognition in images, as demon-
strated by CLIP [47], DeCLIP [34], and ALIGN [28]. These approaches rely on
large amounts of image-text pairs, often in the order ofbillions, to learn visual-
semantic representations using various forms of contrastive learning [13,31]. Re-
cent works, such as EGO-VLP [36], Hier-VL [6], LAVILLA [65], and CoCa [62]
have expanded the scope of multimodal foundation models to include egocentric
videos and have achieved impressive performance in zero-shot generalization.
However, these approaches require substantial amounts of curated pre-training
data to learn semantic associations among concepts for egocentric activity recog-
nition. Neuro-symbolic models[4,29,45,60] show promise in reducing the in-
creasing dependency on data. Our approach extends the idea of neuro-symbolic
reasoning to address egocentric, open-world activity recognition.
3 Proposed Framework: ALGO
Problem Formulation.We address the task of recognizing unknown activities
in egocentric videos within an open-world setting. Our objective is to develop
a framework that can learn to identify elementary concepts, establish semantic
associations, and systematically explore, evaluate, and reject combinations to
arrive at an interpretation that best describes the observed activity class. In
this context, we define the target classes as activities, which are composed of
elementary concepts such as actions (verbs) and objects (nouns). These activities
are formed by combining concepts from two distinct sets: an object search space
(Gobj) and an action search space (Gact). These sets define the pool of available
elementary concepts (objects and actions) that can be used to form an activity
(referred to as the “target label”).
Overview. We propose ALGO (Action Learning with Grounded Object
recognition), illustrated in Figure 1, to tackle the problem of discovering novel
actions in an open world. Given a search space of elementary concepts, we first
hypothesize the presence of plausible objects through evidence-based object
grounding (Section 3.1) by exploring prior knowledge from a symbolic knowl-
edge base. A noisy grounding model provides visual grounding to generate a
grounded object search space. We then use an energy-based inference mecha-
nism (Section 3.2) to discover the plausible actions that can be performed on the
ground object space, driven by prior knowledge from symbolic knowledge bases,
to recognize unseen and unknown activities (action-object combinations) with-
out supervision. A visual-semantic action grounding mechanism (Section 3.3)
then provides feedback to ground semantic concepts with video-based evidence
Abbreviated paper title 5
for discovering composite activities without explicit supervision. Although our
framework is flexible to work with any noisy grounding model and knowledge
base, we use CLIP [47] and ConceptNet [54], respectively.
Knowledge Representation.We use Grenander’s pattern theory formal-
ism [25] to represent the knowledge elements and build a contextualized activity
interpretation that integrates neural and symbolic elements in a unified, energy-
basedrepresentation.Patterntheoryprovidesaflexibleframeworktohelpreason
over variables with varying underlying dependency structures by representing
them as compositions of simpler patterns. These structures, called configura-
tions, are composed of atomic elements calledgenerators ({g1, g2, . . . gi} ∈Gs),
which connect through local connections calledbonds ({β1, β2, . . . βi} ∈gi). The
collection of all generators is called thegenerator space(Gs), with each gener-
ator possessing an arbitrary set of bonds, defined by itsarity. Bonds between
generators are constrained through local and globalregularities, as defined by
an overarching graph structure. A probability structure over the representations
captures the diversity of patterns. We refer the reader to Aakuret al.[2] and de
Souza et al. [53] for a deeper exploration of pattern theory.
3.1 Evidence-based Object Grounding with Prior-driven Prompting
The first step in our framework is to assess the plausibility of each object con-
cept (represented as generators {go
1, go
2, . . . go
i } ∈Gobj) by grounding them in
the input videoVi. We definegrounding as gathering evidence from the input
data to support a concept’s presence (or absence) in the final interpretation.
While object-centric vision-language foundation models such as CLIP [47] have
shown impressive abilities in zero-shot object recognition in images, egocentric
videos provide additional challenges such as camera motion, lens distortion, and
out-of-distribution object labels. Follow-up work [41] has focused on addressing
them to a certain extent by probing CLIP for explainable object classification.
However, they do not considercompositional properties of objects and alterna-
tive labels for verifying their presence in the video. To address this issue, we
propose a neuro-symbolicevidence-based object grounding mechanism to com-
pute the likelihood of an object in a given frame. For each object generator (go
i )
in the search space (Gobj), we first compute a set of compositionalungrounded
generatorsbyconstructinganego-graphofeachobjectlabel( Ego
i )fromConcept-
Net [54] and limiting edges to those that expresscompositional properties such
as IsA, UsedFor, HasProperty and SynonymOf. Given this set ofungrounded
generators ({¯go
i }∀go
i ∈ Gobj), we then prompt CLIP to provide likelihoods for
each ungrounded generatorp(¯go
i |It) to compute theevidence-basedlikelihood for
each grounded object generatorgo
i as defined by the probability
p(go
i |¯go
i , It, KCS ) = p(go
i |It) ∗

X
∀¯go
i
p(go
i , ¯go
i |Ego
i ) ∗ p(¯go
i )|It)

2
(1)
where p(go
i , ¯go
i |Ego
i ) is the edge weight from the edge graphEgo
i (sampled from
a knowledge graphKCS ) that acts as a prior for each ungrounded evidence gen-
6 S. Kundu et al.
erator ¯go
i and p(¯go
i |It) is the likelihood from CLIP for its presence in each frame
It. Hence the probability of the presence of agrounded object generator is de-
termined by (i) its image-based likelihood, (ii) the image-based likelihood of its
evidence generators, and (iii) support from prior knowledge for the presence of
each evidence generator. Hence, we ground the object generators in each video
frame by constructing and evaluating the evidence to support each grounding
assertion and provide an interpretable interface to video object grounding. Em-
pirically, in Section 4.2, we see that this evidence-based grounding outperforms
näive CLIP-based grounding. To navigate clutter and focus only on the object
involved in the activity (i.e., the packaging problem [40]), we use human gaze
to select a200 × 200 region centered around the gaze position (from the human
user if available, else we approximate it with center bias [35]).
3.2 Object-driven Activity Discovery
The second step in our approach is to discover plausible activities performed
in the given video. We take an object affordance-based approach to activity
inference, constraining the activity label (verb+noun) to those that conform to
affordances defined in prior knowledge. We first construct an “action-object affin-
ity” function that provides aprior probability for the validity of an activity. The
probability of each action-object combination is computed by taking a weighted
sum of the edge weights (direct and indirect) along each path that connects them
in ConceptNet. An exponential decay function is applied to each term to avoid
generating excessively long paths that can introduce noise into the reasoning
process. Finally, we filter out paths that donot contain compositional assertions
(UsedFor, HasProperty, IsA) since generic assertions (such asRelatedTo) do
not explicitly capture object affordances. The probability of an activity (defined
by an action generatorga
i and a grounded object generatorgo
j) is given by
p(ga
i , go
j|KCS ) = arg max
∀E∈KCS
X
(¯gm,¯gn)∈E
wk ∗ KCS (¯gm, ¯gn) (2)
where E is the collection of all paths between ga
i and go
j in a commonsense
knowledge graphKCS , wk is a weight drawn from an exponential decay function
based on the distance of the nodego
j from ga
i . After filtering for compositional
properties, the path with the maximum weight is chosen with the optimal action-
object affinity. The process is repeated for all activities in the search space.
Energy-based Activity Inference.To reason over the different activity
combinations, we assign an energy term to each activity label, represented as a
configuration. These are complex structures composed of individual generators
that combine through bonds dictated by their affinity functions. In our case, each
activity interpretation is a configuration composed of a grounded object genera-
tor (go
i ), its associated ungrounded evidence generators (¯go
j ), an action generator
(ga
k) and ungrounded generators from their affinity function, connected via an
underlying graph structure. This graph structure will vary for each configura-
tion depending on the presence of affinity-based bonds derived from ConceptNet.
Abbreviated paper title 7
Hence, theenergy of a configurationci is given by
E(c) = ϕ(p(go
i |¯go
j , It, KCS )) + ϕ(p(ga
k, go
i |KCS )) + ϕ(p(ga
k|It)) (3)
where the first term provides the energy of grounded object generators (from
Equation 1), the second term provides the energy from the affordance-based
affinity between the action and object generators (from Equation 2), and the
third term is the likelihood of an action generator. The probability of a configu-
ration c is given byp(c) ∝ exp(−E(c)). Hence, the lower the energy, the higher
its probability. We initially setϕ(p(ga
k|It)) = 1 to reason over all possible ac-
tions for each object and later update this using a posterior refinement process
(Section 3.3). Hence, activity inference becomes an optimization over Equation 3
to find the configuration (or activity interpretation) with the least energy. For
tractablecomputation,weusetheMCMC-basedsimulatedannealingmechanism
proposed in KGL [4] to avoid an expensive brute-force search over all verb-noun
combinations. If action priors are available from video-centric foundation mod-
els [36,65], ϕ(p(ga
k|It)) can be initialized by prompting it with plausible action
labels. Empirically, in Section 4, we can show that leverage vision-language foun-
dation models, if available, to significantly improve the performance.
3.3 Visual-Semantic Action Grounding
The third step in our framework is the idea of visual-semantic action grounding,
where we aim to learn to ground the inferred actions (verbs) from the overall
activity interpretation. While CLIP provides a general purpose, if noisy, object
grounding method, a comparable approach for actions does not exist. Hence, we
learn an action grounding model by bootstrapping a simple function (ψ(ga
i , fV ))
to map clip-level visual features to the semantic embedding space associated with
ConceptNet, called ConceptNet Numberbatch [54]. The mapping function is a
simple linear projection to go from the symbolic generator space (ga
i ∈ Gact) to
the semantic space (fa
i ), which is a 300-dimension (R1×300) vector representation
explicitly trained to capture concept-level attributes captured in ConceptNet.
While there can be many sophisticated mechanisms [6,36], including contrastive
loss-based training, we use the mean squared error (MSE) loss as the objective
function to train the mapping function since our goal is to provide a mechanism
to ground abstract concepts from the knowledge-base in the video data. We leave
the exploration of more sophisticated grounding mechanisms to future work.
Temporal SmoothingSince we predict frame-level activity interpretations
to account for gaze transitions, we first perform temporal smoothing to label
the entire video clip before training the mapping functionψ(ga
i , fV ) to reduce
noise in the learning process. For each frame in the video clip, we take the
five most common actions predicted at theactivity level (considering the top-
10 predictions) and sum their energies to consolidate activity predictions and
account for erroneous predictions. We then repeat the process for the entire clip,
i.e., get the top-5 actions based on their frequency of occurrence at the frame
level and consolidated energies across frames. These five actions provide targets
8 S. Kundu et al.
forthemappingfunction ψ(ga
i , fV ),whichisthentrainedwiththeMSEfunction.
We use the top-5 action labels as targets to limit the effect of frequency bias.
Posterior-based Activity Refinement.The final step in our framework is
an iterative refinement process that updates the action concept priors (the third
term in Equation 3) based on the predictions of the visual-semantic ground-
ing mechanism described in Section 3.3. Since our predictions are made on a
per-frame basis, it does not consider the overall temporal coherence and visual
dynamicsoftheclip.Hence,therecanbecontradictingpredictionsfortheactions
doneovertime.Hence,weiterativelyupdatetheactionpriorsfortheenergycom-
putation to re-rank the interpretations based on the clip-level visual dynamics.
We iteratively refine the activity labels and update the visual-semantic action
grounding modules simultaneously by alternating between posterior update and
action grounding until the generalization error (i.e., the performance on unseen
actions) saturates, which indicates overfitting.
Implementation Details.We use an S3D-G network pre-trained by Miech
et al. [42,43] on Howto100M [43] as our visual feature extraction for visual-
semantic action grounding. We use a CLIP model with the ViT-B/32 [20] as its
backbone network. ConceptNet was used as our source of commonsense knowl-
edge for neuro-symbolic reasoning, and ConceptNet Numberbatch [54] was used
as the semantic representation for action grounding. The mapping function, de-
fined in Section 3.3, was a 1-layer feedforward network trained with the MSE
loss for 100 epochs with a batch size of 256 and learning rate of10−3. General-
ization errors were used to pick the best model. Experiments were conducted on
a desktop with a 32-core AMD ThreadRipper and an NVIDIA TitanRTX.
4 Experimental Evaluation
Data. To evaluate the open-world inference capabilities, we evaluate the ap-
proach on GTEA Gaze [23], GTEA GazePlus [35], and EPIC-Kitchens-100 [15,
17] datasets, which contain egocentric, multi-subject videos of meal preparation
activities. GTEA Gaze and GazePlus have frame-level gaze information and ac-
tivity labels, providing an ideal test bed for our setup. EPIC-Kitchens-100 is a
much larger dataset and does not have gaze information, offering a much more
challenging evaluation of the approach. We also evaluate on Charades-Ego [51],
a larger egocentric video dataset focused on activities of daily living, to evaluate
on the zero-shot setting. The evaluation datasets, under an open-world setting,
offer a significant challenge with an increasingly larger search space. The GTEA
Gaze dataset has 10 verbs and 38 nouns (search space of 380 activities), while
GTEA GazePlus has 15 verbs and 27 nouns (search space of 405), Charades-
Ego has 33 verbs and 38 nouns (search space of 1254), and Epic-Kitchens has
97 verbs and 300 nouns (search space of 29100).
Evaluation Metrics.Following prior work in open-world activity recogni-
tion [2,4], we use accuracy to evaluate action and object recognition and use
word-level accuracy for evaluating the activity (verb+noun) recognition perfor-
mance. It provides a less-constrained measurement to measure the quality of pre-
Abbreviated paper title 9
Approach SearchVLM? GTEA Gaze GTEA GazePlus
Space ObjectActionActivityObjectActionActivity
Two-Stream CNN [52]Closed ✗ 38.05 59.54 53.08 61.87 58.65 44.89
IDT [58] Closed ✗ 45.07 75.55 40.41 53.45 66.74 51.26
Action Decomposition [64]Closed ✗ 60.01 79.39 55.67 65.62 75.07 57.79
Random Known ✗ 3.22 7.69 2.50 3.70 4.55 2.28
Action Decomposition ZSL* [64]Known ✗ 65.81 89.17 68.70 53.40 32.48 29.19
ALGO ZSL (Ours) Known ✗ 49.47 74.74 27.34 47.67 29.31 16.68
KGL [4] Open ✗ 5.12 8.04 4.91 14.78 6.73 10.87
KGL+CLIP [4] Open ✗ 10.36 8.15 9.21 20.49 9.23 14.86
ALGO (Ours) Open ✗ 13.07 17.05 15.05 26.23 11.44 18.84
EgoVLP [36] Open ✓ 10.17 8.45 9.31 29.43 17.17 23.30
LaViLa [65] Open ✓ 6.07 23.07 14.57 28.27 25.47 26.87
ALGO+LaViLa Open ✓ 17.50 26.60 22.05 30.74 27.00 28.87
Table 1: Open-world activity recognitionperformance on the GTEA Gaze and
GTEA Gaze Plus datasets. We compare approaches with a closed search space, those
with a known search space, and those with a partially open one. Accuracy is reported
for predicted objects, actions, and activities. VLM: Vision-Language Model pre-trained
on egocentric video data. * indicates training on “seen” classes from the same dataset(s)
and leave-one-action-out evaluation.
dictions beyond accuracy by considering all units without distinguishing between
insertions, deletions, or misclassifications. This helps quantify the performance
without penalizing semantically similar interpretations. We use class-wise mAP
to evaluate ALGO in the zero-shot learning setup on Charades-Ego.
Baselines. We compare against different egocentric action recognition ap-
proaches, including those with a closed-world learning setup. For open-world
inference, we compare it against Knowledge Guided Learning (KGL) [4], which
introduced the notion of open-world egocentric action recognition. We also create
a baseline called “KGL+CLIP” by augmenting KGL with CLIP-based grounding
by including CLIP’s similarity score for establishing semantic correspondences.
We compare with supervised learning models such as Action Decomposition [64],
IDT [58], and Two-Stream CNN [52], with a strong closed-world assumption and
a dependency on labeled training data. We compare against the zero-shot ver-
sion of Action Decomposition, which can work under a known world where the
final activity labels are known. Not that this is not a fair comparison since it
is evaluated under a leave-one-action-out zero-shot learning setting trained on
examples from the corresponding dataset for “seen” actions. We report it for
completeness. We also compare against large vision-language models, such as
EGO-VLP [36], HierVL [6], and LAVILA [65] in both zero-shot and open-world
settings (by prompting with all possible verb+noun combinations).
4.1 Open World Activity Recognition
Table 1 summarizes the evaluation results under the open-world inference set-
ting. Top-1 prediction results are reported for all approaches. As can be seen,
CLIP-based grounding significantly improves the performance of object recogni-
tion for KGL, as opposed to the originally proposed, prior-only correspondence
10 S. Kundu et al.
Approach VLM? Action Object Activity
Random ✗ 1.03 0.33 0.68
KGL [4] ✗ 3.89 2.56 3.23
KGL+CLIP [4] ✗ 5.32 4.67 4.99
ALGO (Ours) ✗ 10.21 6.76 8.48
EgoVLP [36] ✓ 10.77 19.51 15.14
LaViLa [65] ✓ 11.16 23.25 17.21
ALGO+LaViLa ✓ 12.54 22.84 17.69
Table 2:Evaluation on the EPIC-Kitchens-100 dataset. VLM: Vision-Language pre-
training on egocentric data. Accuracy for actions, objects, and activity are reported.
function. However, our neuro-symbolic grounding mechanism (Section 3.1) im-
proves it further, achieving an object recognition performance of13.07% on Gaze
and 26.23% on Gaze Plus. It is interesting to note that naïvely adding CLIP as
a mechanism for grounding objects, while effective, does not provide significant
gains in the overall action recognition performance (an average of2% across
Gaze and Gaze Plus). We attribute it to the fact that the inherent camera mo-
tion in egocentric videos introduces occlusions and visual variations that make
it hard to recognize actions consistently. Evidence-based grounding, as proposed
in ALGO, makes it more robust to such changes and improves object and ac-
tion recognition performance. Similarly, the posterior-based action refinement
module (Section 3.3) helps achieve a top-1 action recognition performance of
17.05% on Gaze and 11.44% on Gaze Plus, outperforming KGL (8.04% and
6.73%). Adding action priors from LaViLa (ϕ(p(ga
k|It)) in Equation 3) allows
us to improve the performance further, as indicated by ALGO+LaViLa. We see
that LaVila’s performance is consistently improved on all metrics. Note that
even without the action priors, we outperform LaViLa on GTEA Gaze and offer
competitive performance on GazePlus without pre-training on egocentric videos.
We also evaluate our approach on the Epic-Kitchens-100 dataset, a larger-
scale dataset with a significantly higher number of concepts (actions, verbs, and
activities). Table 2 summarizes the results. We significantly outperform non-
VLM models while offering competitive performance to the VLM-based models.
We see that even withoutany video-based training data, we achieve an action
accuracy of10.21% and object accuracy of6.76%, indicating that we can learn
affordance-based relationships for discovering and grounding novel actions in
egocentric data. Adding action priors from LaViLa further improves the perfor-
mance. Interestingly, the action (verb) prediction performance of both LaViLa
is improved, although it is at the cost of reduced object accuracy. Note that the
predictions are not separate for verbs and nouns but are computed from the pre-
dicted activity. These are remarkable results, considering that the search space
is open, i.e., the verb+noun combination is unknown and can be large (380 for
Gaze, 405 for Gaze Plus, and 29100 on EPIC-Kitchens).
To evaluate the generalization capabilities of ALGO, we presented videos
with unseen actions, i.e., actions not in the original training domain, and an
Abbreviated paper title 11
Approach Visual BackbonePre-Training? Pre-Training DatamAPEgo? Source Size
EGO-VLP w/o EgoNCE [36]TimeSformer [8] VisLang ✗ Howto100M [43]136M9.2
EGO-VLP w/o EgoNCE [36]TimeSformer [8] VisLang ✗ CC3M+WebVid-2m5.5M20.9
EGO-VLP + EgoNCE [36]TimeSformer [8] VisLang ✓ EgoClip [36] 3.8M23.6
HierVL [6] FrozenInTime [7]VisLang ✓ EgoClip [36] 3.8M26.0
LAVILA [65] TimeSformer [8] VisLang ✓ Ego4D [24] 4M 26.8
ALGO (Ours) S3D-G [43] Vision Only ✗ Howto100M [43]136M17.3
ALGO (Ours) S3D [61] Vision Only ✗ Kinetics-400 [30]240K16.8
Table 3: Evaluation of ALGO underzero-shot learning settings on Charades-Ego
where the search space is constrained to ground truth activity semantics. VisLang:
Vision Language Pre-Training.
open search action space, i.e., not derived from the dataset annotations. We
prompted GPT-4 [12] using the ChatGPT interface to provide100 everyday
actions and objects that can be performed in the kitchen to construct our open-
world search space and evaluated on GTEA Gaze and GazePlus. On unseen
actions and unknown search space, the performance was competitive, achieving
an accuracy of9.87% on Gaze and8.45% on Gaze Plus. Please see supplementary
for more details and results from the generalization studies. These results are
encouraging, as they significantly reduce the gap between closed-world learning
(supervised), known-world learning (zero-shot), and open-world learning.
Extension to Zero-Shot Egocentric Activity Recognition Open-world
learning involves the combinatorial search over the different, plausible compo-
sitions of elementary concepts. In activity recognition, this involves discovering
the action-object (verb-noun) combinations that make up an activity. However,
in many applications, such as zero-shot recognition, the search space is known,
and there is a need to predict pre-specified labels. To compare our approach with
such foundation models, we evaluate ALGO on the Charades-Ego dataset and
summarize the results in Table 3. We consider the top-10 interpretations made
for each clip and perform a nearest neighbor search using ConceptNet Number-
batch embedding to the set of ground-truth labels and pick the one with the least
distance. It provides a simple yet effective mechanism to extend our approach
to zero-shot settings. We achieve an mAP score of16.8% using an S3D [61]
model pre-trained on Kinetics-400 [30] and an S3D-G [42] model pre-trained on
Howto100M [43]. This significantly outperforms a comparable TimeSFormer [8]
model pre-trained with a vision-language alignment objective function and pro-
vides competitive performance to state-of-the-art vision-language models with
significantly lower training requirements - both data and time. We observe a
similar performance in the Gaze and GazePlus datasets as shown in Table 1. We
obtain 27.34% on Gaze and 16.69% on Gaze Plus, performing competitively with
the zero-shot approaches. These results are obtained without large amounts of
paired text-video pairs and a simple visual-semantic grounding approach. Note
that the performance for zero-shot ActionDecomposition is reported for leave-
one-class cross-validation, while our approach treatsall classes as unseen classes.
12 S. Kundu et al.
(a) (b) (c) (d)
Fig. 2: Ablation studiesshowing the impact of (a) the quality of object grounding
techniques, (b) posterior-based action refinement, (c) iterative action refinement on
generalization capabilities, and (d) the choice of visual and semantic representations.
4.2 Ablation Studies
We systematically examine the impact of the individual components on the over-
all performance of the approach. Specifically, we focus on three aspects: (i) the
impact of object grounding, (ii) the impact of posterior-based action refinement,
and (iii) the generalization of learned models with refinement, and (iv) the choice
of visual and semantic features. We experiment on the GTEA Gaze dataset and
present results in Figure 2 and discuss the results below.
Quality and Impact of Object Grounding.First, we evaluate the object
recognition performance of different object grounding techniques and present re-
sults in Figure 2(a). We consider 5 different techniques: the prior-based approach
proposed in KGL, updating the prior with CLIP-based likelihood (KGL+CLIP),
näively using CLIP to recognize the object in the gaze-based ROI (CLIP Only),
the proposed evidence-based object grounding (CLIP+Evidence), and using ev-
idence only without checking object-level likelihood (Evidence Only). As can
be seen, using CLIP improves performance significantly across the different ap-
proaches while using evidence provides gains over the näive CLIP Only method.
KGL+CLIP and the proposed CLIP+Evidence approaches perform similarly,
with KGL+CLIP being slightly better when considering more than the top-
5 recognized objects. However, this does not always transfer to better action
recognition because the object probabilities are much closer in KGL+CLIP
than in the proposed CLIP+Evidence. We also evaluated the performance of
CLIP+Evidence on an unknown search space by prompting GPT-4 to provide
a list of100 objects commonly found in the kitchen. The Top-3 performance is
excellent, reaching 45% before saturating, which is remarkable considering that
the unknown search space with possibly unseen objects. We anticipate using vi-
sual commonsense priors, such as from scene graphs [32], can help disambiguate
between visually similar objects.
Impact of Posterior-based Action Refinement.One of the major con-
tributions of ALGO is the use of continuous posterior-based action refinement,
where the energy of the action generator is refined based on an updated prior
from the visual-semantic action grounding to improve the activity recognition
performance. One key question is how many iterations of such refinements are
ideal before overfitting occurs. Figure 2(b) visualizes the activity recognition per-
formance with different levels of iteration, along with the results of a constrained
Abbreviated paper title 13
search space (zero-shot) approach. As can be seen, the first two iterations sig-
nificantly improved the performance, while the third iteration provided very
negligible improvement, which provided indications of overfitting. Constraining
the search space in the zero-shot setting significantly improves the performance.
Generalization of Visual-Semantic Action Grounding.To evaluate
the impact of the posterior-based refinement on the generalization capabilities,
we evaluated the trained models, at different iterations, on the GTEA Gaze Plus
dataset. As can be seen from Figure 2, each iteration improves the performance
of the model before the performance starts to stagnate (at the third iteration).
These results indicate that while iterative refinement is useful, it can lead to
overfitting to the domain-specific semantics and can hurt the generalization ca-
pabilities of the approach. To this end, we keep the termination criteria for the
iterative posterior-based refinement based on the generalization performance of
the action grounding model on unseen actions.
Impact of Visual-Semantic Features.Finally, we evaluate ALGO with
different visual and semantic features and visualize the results in Figure 2 (d).
We see that the use of ConceptNet Numberbatch (NB) considerably improves
theperformanceoftheapproachasopposedtousingGloVeembeddings[46].The
choice of visual features (S3DG vs. S3D) does not impact the performance much.
We hypothesize that the NB’s ability to capture semantic attributes [55] allows
it to generalize better than GloVe. We anticipate custom training of embedding
vectors using contextualized, ConceptNet-based pattern theory interpretations
could lead to better generalization capabilities.
4.3 Generalization of Learned Actions to Unknown Vocabulary
To measure the generalization capability of the approach to unknown actions,
we use the word similarity score (denoted as NB-WS) to measure the semantic
similarity between the predicted and ground truth actions. NB-WS has demon-
strated the ability to capture attribute-based representations when computing
similarity [55]. We evaluate ALGO’s ability to recognize actions from out of
its training distribution by presenting videos from datasets with unseen actions
and an unknown search space. Specifically, we refer to actions not in the original
training domains as “unseen” actions, following convention from zero-shot learn-
ing. Similarly, in an unknown search space, i.e.,completely open world inference,
the search space is not pre-specified but inferred from general-purpose knowledge
sources. For these experiments, we prompted GPT-4 [12] using the ChatGPT
interface to provide100 everyday actions that can be performed in the kitchen
to construct our search space. The results are summarized in Table 4, where we
present the verb accuracy and the ConceptNet Numberbatch Word Similarity
(NB-WS) score. ALGO generalizes consistently across datasets. Of particular
interest is the generalization from Gaze and Gaze Plus to Charades-Ego, where
there is a significantly higher number of unseen and unknown actions. Models
trained on GTEA Gaze, which has more variation in camera quality and actions,
generalize better than those from Gaze Plus. With unseen actions and unknown
search space, the performance was competitive, achieving an accuracy of9.87%
14 S. Kundu et al.
Training Data Evaluation DataUnknownSearchAccuracyNB-WS
Dataset # Verbs Dataset # VerbsVerbs? Space
Gaze 10 Gaze 10 ✗ K 14.11 27.24
Gaze Plus 15 Gaze Plus 15 ✗ K 11.44 24.45
Charades-Ego 33 Charades-Ego 33 ✗ K 11.92 36.02
Gaze 10 Charades-Ego 33 ✓ K 13.55 34.83
Gaze Plus 15 Charades-Ego 33 ✓ K 10.24 31.11
Gaze Plus 15 Gaze 10 ✓ K 5.27 29.68
Charades-Ego 33 Gaze 10 ✓ K 10.17 32.65
Gaze 10 Gaze Plus 15 ✓ K 10.37 23.55
Charades-Ego 33 Gaze Plus 15 ✓ K 11.22 24.25
Gaze 10 Gaze 10 ✓ U 9.87 14.51
Gaze Plus 15 Gaze Plus 15 ✓ U 8.45 11.78
Table 4: Generalization studiesto analyze the performance of the action (verb)
recognition models learned in an open-world setting. The models are trained in one
domain and evaluated in another, containing possible unknown and unseen actions.
NB-WS: ConceptNet Numberbatch Word Similarity
on Gaze and8.45% on Gaze Plus. NB-WS was higher, indicating better agree-
ment with the ground truth, i.e., the predicted verbs were similar to the ground
truth. While there is room for improvement, these results present a significant
step towards truly open-world learning without any constraints.
5 Discussion, Limitations, and Future Work
In this work, we proposed ALGO, a neuro-symbolic framework for open-world
egocentricactivityrecognitionthataimstolearnnovelactionandactivityclasses
without explicit supervision. By grounding objects and using an object-centered,
knowledge-based approach to activity inference, we reduce the need for labeled
data to learn semantic associations among elementary concepts. We demonstrate
that the open-world learning paradigm is an effective inference mechanism to dis-
till commonsense knowledge from symbolic knowledge bases for grounded action
understanding. While showing competitive performance, there are two key lim-
itations: (i) it is restricted to ego-centric videos due to the need to navigate
clutter by using human attention as a contextual cue for object grounding, and
(ii) it requires a knowledge base such as ConceptNet to learn associations be-
tween actions and objects and hence is restricted to its vocabulary. While we
demonstrated its performance on an unknown search space, much work remains
to effectively build a search space (both action and object) to move towards a
trulyopen-worldlearningparadigm.Weaimtoexploretheuseofattention-based
mechanisms [1,44] to extend the framework to third-person videos and using ab-
ductive reasoning [5,63] and neural knowledgebase completion approaches [10]
to integrate visual commonsense into the reasoning while moving beyond the
vocabulary encoded in symbolic knowledgebases.
Abbreviated paper title 15
Acknowledgements.This research was supported in part by the US Na-
tional Science Foundation grants IIS 2143150, and IIS 1955230. We thank Dr.
Anuj Srivastava (FSU) and Dr. Sudeep Sarkar (USF) for their thoughtful feed-
back during the discussion about the problem formulation and experimental
analysis phase of the project.
References
1. Aakur, S., Sarkar, S.: Actor-centered representations for action localization in
streaming videos. In: Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVIII. pp. 70–87.
Springer (2022) 14
2. Aakur, S., de Souza, F., Sarkar, S.: Generating open world descriptions of video us-
ing common sense knowledge in a pattern theory framework. Quarterly of Applied
Mathematics 77(2), 323–356 (2019) 3, 5, 8
3. Aakur, S.N., Bagavathi, A.: Unsupervised gaze prediction in egocentric videos by
energy-based surprise modeling. In: International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Applications (2021) 3
4. Aakur, S.N., Kundu, S., Gunti, N.: Knowledge guided learning: Open world ego-
centric action recognition with zero supervision. Pattern Recognition Letters156,
38–45 (2022) 3, 4, 7, 8, 9, 10, 1
5. Aakur, S.N., Sarkar, S.: Leveraging symbolic knowledge bases for commonsense
natural language inference using pattern theory. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2023) 14
6. Ashutosh, K., Girdhar, R., Torresani, L., Grauman, K.: Hiervl: Learning hierar-
chical video-language embeddings (2023) 2, 4, 7, 9, 11
7. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video
and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 1728–1738 (2021) 11
8. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
video understanding? In: International Conference on Machine Learning. pp. 813–
824. PMLR (2021) 11
9. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities
and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021) 2
10. Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., Choi, Y.: Comet:
Commonsense transformers for automatic knowledge graph construction. In: Pro-
ceedings of the 57th Annual Meeting of the Association for Computational Lin-
guistics. pp. 4762–4779 (2019) 14
11. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot
learners. Advances in Neural Information Processing Systems33, 1877–1901 (2020)
4
12. Bubeck,S.,Chandrasekaran,V.,Eldan,R.,Gehrke,J.,Horvitz,E.,Kamar,E.,Lee,
P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023) 4, 11, 13
13. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: International Conference on Machine
Learning. pp. 1597–1607. PMLR (2020) 4
16 S. Kundu et al.
14. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text en-
coders as discriminators rather than generators. arXiv preprint arXiv:2003.10555
(2020) 4
15. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: The epic-kitchens
dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence (TPAMI)43(11), 4125–4141 (2021).https://doi.
org/10.1109/TPAMI.2020.2991965 8
16. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., et al.: The epic-kitchens dataset:
Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and
Machine Intelligence43(11), 4125–4141 (2020) 3
17. Damen, D., Doughty, H., Farinella, G.M., Furnari, A., Ma, J., Kazakos, E., Molti-
santi, D., Munro, J., Perrett, T., Price, W., Wray, M.: Rescaling egocentric vision:
Collection, pipeline and challenges for epic-kitchens-100. International Journal of
Computer Vision (IJCV)130, 33–55 (2022),https://doi.org/10.1007/s11263-
021-01531-2 8
18. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:Pre-trainingofdeepbidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018) 2, 4
19. Dong, N., Zhang, Y., Ding, M., Lee, G.H.: Open world detr: transformer based
open world object detection. arXiv preprint arXiv:2212.02969 (2022) 4
20. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations 2, 8, 1
21. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for open-
vocabulary object detection with vision-language model. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14084–
14093 (2022) 4
22. Fan, C.: Egovqa-an egocentric video question answering benchmark dataset. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops. pp. 0–0 (2019) 3
23. Fathi, A., Li, Y., Rehg, J.M.: Learning to recognize daily actions using gaze. In:
European Conference on Computer Vision. pp. 314–327. Springer (2012) 3, 8
24. Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Ham-
burger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000
hours of egocentric video. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 18995–19012 (2022) 3, 11
25. Grenander, U.: Elements of pattern theory. JHU Press (1996) 3, 5
26. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision
and language knowledge distillation. In: International Conference on Learning Rep-
resentations (2021),https://api.semanticscholar.org/CorpusID:238744187 4
27. Han, S., Liu, B., Cabezas, R., Twigg, C.D., Zhang, P., Petkau, J., Yu, T.H., Tai,
C.J., Akbay, M., Wang, Z., et al.: Megatrack: monochrome egocentric articulated
hand-tracking for virtual reality. ACM Transactions on Graphics (ToG)39(4),
87–1 (2020) 3
28. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision (2021) 4
Abbreviated paper title 17
29. Jiang, J., Ahn, S.: Generative neurosymbolic machines. In: Larochelle, H., Ran-
zato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Infor-
mation Processing Systems. vol. 33, pp. 12572–12582. Curran Associates, Inc.
(2020), https://proceedings.neurips.cc/paper_files/paper/2020/file/
94c28dcfc97557df0df6d1f7222fc384-Paper.pdf 4
30. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S.,
Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video
dataset. arXiv preprint arXiv:1705.06950 (2017) 11
31. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
A., Liu, C., Krishnan, D.: Supervised contrastive learning. Advances in Neural
Information Processing Systems33, 18661–18673 (2020) 4
32. Kundu, S., Aakur, S.N.: Is-ggt: Iterative scene graph generation with generative
transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 6292–6301 (2023) 12
33. Li, H., Cai, Y., Zheng, W.S.: Deep dual relation modeling for egocentric interaction
recognition.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 7932–7941 (2019) 3
34. Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., Yan, J.: Super-
vision exists everywhere: A data efficient contrastive language-image pre-training
paradigm (2022) 4
35. Li, Y., Fathi, A., Rehg, J.M.: Learning to predict gaze in egocentric video. In:
Proceedings of the IEEE International Conference on Computer Vision. pp. 3216–
3223 (2013) 3, 6, 8, 1
36. Lin, K.Q., Wang, J., Soldan, M., Wray, M., Yan, R., XU, E.Z., Gao, D., Tu, R.C.,
Zhao, W., Kong, W., et al.: Egocentric video-language pretraining. Advances in
Neural Information Processing Systems35, 7575–7586 (2022) 2, 4, 7, 9, 10, 11, 1
37. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 (2019) 4
38. Lu, Z., Grauman, K.: Story-driven summarization for egocentric video. In: Pro-
ceedings of the IEEE conference on Computer Vision and Pattern Recognition.
pp. 2714–2721 (2013) 3
39. Ma, M., Fan, H., Kitani, K.M.: Going deeper into first-person activity recognition.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 1894–1903 (2016) 2, 3
40. Maguire, M.J., Dove, G.O.: Speaking of events: event word learning and event
representation. Understanding Events: How Humans See, Represent, and Act on
Events pp. 193–218 (2008) 6
41. Menon, S., Vondrick, C.: Visual classification via description from large language
models. arXiv preprint arXiv:2210.07183 (2022) 5
42. Miech,A.,Alayrac,J.B.,Smaira,L.,Laptev,I.,Sivic,J.,Zisserman,A.:End-to-end
learning of visual representations from uncurated instructional videos. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 9879–9889 (2020) 8, 11, 1
43. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
rated video clips. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 2630–2640 (2019) 8, 11, 1
44. Mounir, R., Shahabaz, A., Gula, R., Theuerkauf, J., Sarkar, S.: Towards au-
tomated ethogramming: Cognitively-inspired event segmentation for streaming
18 S. Kundu et al.
wildlife video monitoring. International Journal of Computer Vision pp. 1–31
(2023) 14
45. Nye, M., Tessler, M., Tenenbaum, J., Lake, B.M.: Improving coherence and con-
sistency in neural sequence models with dual-system, neuro-symbolic reasoning.
Advances in Neural Information Processing Systems34, 25192–25204 (2021) 4
46. Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word represen-
tation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pp. 1532–1543 (2014) 13
47. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International Conference on Machine Learning.
pp. 8748–8763. PMLR (2021) 2, 3, 4, 5
48. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language un-
derstanding by generative pre-training (2018) 4
49. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog1(8), 9 (2019) 4
50. Ryoo, M.S., Rothrock, B., Matthies, L.: Pooled motion features for first-person
videos. In: Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition (CVPR) (June 2015) 3
51. Sigurdsson, G.A., Gupta, A., Schmid, C., Farhadi, A., Alahari, K.: Actor and
observer: Joint modeling of first and third-person videos. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 7396–7404 (2018)
2, 3, 8
52. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-
nition in videos. Advances in Neural Information Processing Systems27 (2014)
9
53. de Souza, F.D., Sarkar, S., Srivastava, A., Su, J.: Pattern theory for representation
and inference of semantic structures in videos. Pattern Recognition Letters72,
41–51 (2016) 3, 5
54. Speer, R., Chin, J., Havasi, C.: Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In: Proceedings of the AAAI Conference on Artificial Intelligence.
vol. 31 (2017) 3, 5, 7, 8, 1
55. Speer, R., Lowry-Duda, J.: Luminoso at SemEval-2018 task 10: Distinguishing
attributes using text corpora and relational knowledge. In: Proceedings of the
12th International Workshop on Semantic Evaluation. pp. 985–989. Association
for Computational Linguistics, New Orleans, Louisiana (Jun 2018).https://doi.
org/10.18653/v1/S18-1162, https://aclanthology.org/S18-1162 13
56. Sudhakaran, S., Escalera, S., Lanz, O.: Lsta: Long short-term attention for egocen-
tric action recognition. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2019) 3
57. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in Neural Information Pro-
cessing Systems30 (2017) 4
58. Wang, H., Schmid, C.: Action recognition with improved trajectories. In:
IEEE/CVF International Conference on Computer Vision (ICCV). pp. 3551–3558
(2013) 2, 9
59. Wang, X., Zhu, L., Wang, H., Yang, Y.: Interactive prototype learning for egocen-
tric action recognition. In: Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV). pp. 8168–8177 (October 2021) 3
Abbreviated paper title 19
60. Wu,T.,Tjandrasuwita,M.,Wu,Z.,Yang,X.,Liu,K.,Sosic,R.,Leskovec,J.:Zeroc:
A neuro-symbolic model for zero-shot concept recognition and acquisition at infer-
ence time. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A.
(eds.) Advances in Neural Information Processing Systems. vol. 35, pp. 9828–9840.
Curran Associates, Inc. (2022),https://proceedings.neurips.cc/paper_files/
paper/2022/file/3ff48dde82306fe8f26f3e51dd1054d7-Paper-Conference.pdf
4
61. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature
learning: Speed-accuracy trade-offs in video classification. In: Proceedings of the
European Conference on Computer Vision. pp. 305–321 (2018) 11
62. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.: Coca:
Contrastive captioners are image-text foundation models (2022) 4
63. Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition: Visual
commonsense reasoning. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) (June 2019) 14
64. Zhang, Y.C., Li, Y., Rehg, J.M.: First-person action decomposition and zero-
shot learning. In: IEEE Winter Conference on Applications of Computer Vision
(WACV). pp. 121–129 (2017) 2, 3, 9
65. Zhao, Y., Misra, I., Krähenbühl, P., Girdhar, R.: Learning video representations
from large language models. In: CVPR (2023) 2, 4, 7, 9, 10, 11
66. Zhou, Y., Ni, B., Hong, R., Yang, X., Tian, Q.: Cascaded interactional targeting
network for egocentric video analysis. In: Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition (CVPR) (June 2016) 3
Abbreviated paper title 1
Implementation Details.
We use an S3D-G network pre-trained by Miechet al.[42,43] on Howto100M [43]
as our visual feature extraction for visual-semantic action grounding. We use a
CLIP model with the ViT-B/32 [20] as its backbone network. ConceptNet was
used as our source of commonsense knowledge for neuro-symbolic reasoning, and
ConceptNetNumberbatch[54]wasusedasthesemanticrepresentationforaction
grounding. The MCMC-based inference from KGL [4] was used as our reason-
ing mechanism. For experiments on Charades-Ego, where gaze information was
unavailable, center bias [35] was used to approximate gaze locations. The map-
ping function, defined in Section 3.3, was a 1-layer feedforward network trained
with the MSE loss for 100 epochs with a batch size of 256 and learning rate of
10−3. Generalization errors on unseen actions were used to pick the best model.
Two iterations of posterior-based action refinement were performed per video.
Experiments were conducted on a desktop with a 32-core AMD ThreadRipper
and an NVIDIA Titan RTX.
Why temporal smoothing?
Since we predict frame-level activity interpretations to account for gaze transi-
tions, we first perform temporal smoothing to label the entire video clip before
training the mapping functionψ(ga
i , fV ) to reduce noise in the learning process.
For each frame in the video clip, we take the five most common actions predicted
at theactivity level (considering the top-10 predictions) and sum their energies
to consolidate activity predictions and account for erroneous predictions. We
then repeat the process for the entire clip, i.e., get the top-5 actions based on
their frequency of occurrence at the frame level and consolidated energies across
frames. These five actions provide targets for the mapping functionψ(ga
i , fV ),
which is then trained with the MSE function. We use the top-5 action labels
as targets to limit the effect of frequency bias. For example, some actions, such
as clean, can possess a high affinity to many objects and hence be the most
commonly predicted action for a frame. Hence, temporal smoothing acts as a
regularizer to reduce overfitting by forcing the model to predict the embedding
for the top five actions for each video clip.
Why posterior-based refinement?
Since our predictions are made on a per-frame basis, it does not consider the
overall temporal coherence and visual dynamics of the clip. Hence, there can be
contradicting predictions for the actions done over time. Similarly, when setting
the action priors to1, we consider all actions equally plausible and do not re-
strict the action labels through grounding, as done for objects in Section 3.1.
Hence, we iteratively update the action priors for the energy computation to re-
rank the interpretations based on the clip-level visual dynamics. This prior could
be updated to consider predictions from other models, such as EGO-VLP [36]
2 S. Kundu et al.
through prompting mechanisms similar to our neuro-symbolic object grounding.
We iteratively refine the activity labels and update the visual-semantic action
grounding modules simultaneously by alternating between posterior update and
action grounding until the generalization error (i.e., the performance on un-
seen actions) saturates, which indicates overfitting. We empirically verify this in
Section 4.2, where we observe the impact of this posterior update on activity
recognition performance and the resulting loss of generalization.
Datasets
The GTEA Gaze dataset consists of 14 subjects performing activities composed
of 10 verbs and 38 nouns across 17 videos. The gaze information is collected
using Tobii eye-tracking glasses at 30 frames per second. The Gaze Plus dataset
has 27 nouns and 15 verbs from 6 subjects performing 7 meal preparation activ-
ities across 37 videos. The gaze information is collected at 30 frames per second
for both datasets using SMI eye-tracking glasses. Charades-Ego contains 7,860
videos containing 157 activities. Following prior work [36], we use the 785 ego-
centric clips in the test set for evaluation. EpicKitchens-100 is a large dataset
comprising several hours of egocentric videos with 300 nouns (objects) and 97
verbs (actions). We use the validation set to evaluate our approach following
prior works [65].
Visualizations of generated interpretations
Evidence-basedgrounding. Someexamplesofevidence-basedgroundingthrough
ConceptNet are shown in Figure 3. As can be seen, each concept can have multi-
ple evidence generators derived from ConceptNet using its ego-graph and limit-
ing edges to those that expresscompositional properties such asIsA, UsedFor,
HasProperty and SynonymOf. Using ego-graph helps preserve the contextual in-
formation within the semantic locality of the object to filter high-order noise in-
duced by regular k-hop neighborhoods. The derived concepts provide additional
context for verifying the presence of a given object in the video by querying
CLIP as a noisy object oracle. Note that we do not visualize the edge label to
avoid clutter. Each edge is qualified by a semantic assertion and is quantified by
a value between -2 and 2 to express its strength. This provides a more explain-
able representation that enhances the final interpretation generated, as discussed
next.
Semantically rich interpretations.The final interpretations generated
by the approach are shown in Figure 4. It can be seen that the final interpreta-
tion is semantically rich and has concepts that are not directly in the scene but
are compositionally relevant, either through affordance or object-level evidence.
We visualize two different interpretations for affordance-based reasoning for the
activity “cut fork” in (a) and (b), where it can be seen that the approach can
generate graphs of varying structures. It also shows the impact of the noise in
the knowledge graph that can introduce irrelevant concepts into the reasoning
Abbreviated paper title 3
process. We anticipate that having an additional reasoning step can reduce the
impact of noise. Interestingly, we see that combinations of similar verbs and
nouns such as “pour honey” (c) and “pour ketchup” (d) result in different un-
grounded generators, indicating that the affordance of each concept is used for
reasoning, resulting in semantically rich graphical interpretations. We anticipate
that learning customized embeddings from these graphs can result in a better
grounding of novel compositional concepts such as actions and activities.
Baselines. Since it is the most comparable, deep learning-only baseline, we
choose ActionDecomposition [65] as our primary ZSL baseline for the GTEA
datasets. It decouples verb and noun recognition and uses similar feature ex-
tractors for noun+verb prediction. However, they only report leave-one-class-
out cross-validation, which assumes access to other examples from “seen” classes
during training, which is not a fair comparison with our approach. We do not
require any labels during training. Given the list of possible objects and actions
in the videos, we ground the objects using CLIP, infer plausible actions us-
ing affordance-based reasoning, and predict the final activities driven purely by
prior knowledge encoded in ConceptNet. Besides CLIP (trained only on objects),
ALGO is not trained with any labels. Zero-shot models and “foundation models”
are trained on considerable amounts ofvideo data with action/activity/objectla-
belsandlearnverb+nounassociationsfromtheselabels.While“chain-of-thought
prompting” or other decomposition approaches can possibly improve their gener-
alization, no such models exist.We report the supervised/zero-shot/VLMs per-
formance to illustrate that ALGO performs competitively despite not requiring
large-scale video pretraining and labels (especially for actions and activities).
While there is a gap between their performance, note that we do not have ac-
cess to any data about actions (verbs) or activities, while VLMs and ZS mod-
els are trained exclusively on videos with such information. We learn to rec-
ognize actions with no labels and show that VLMs can benefit from ALGO
(ALGO+LAVILA and ALGO+EGOVLP).
Action Recognition and Metrics. While activity recognition is the focus
of the work, we would like to point out that the action (verb) recognition is also
done in an open-world, inferred without labeled data. The performances reported
for “Action” in Tables 1-3 represent ALGO’s ability to infer the verb from contex-
tualinformation.Thetop-5performanceofALGO(obj/action/activity)onGaze
(40.75/34.89/37.82 vs. KGL’s 32.39/10.73/18.78) and Gaze+ (42.77/53.88/48.33
vs. KGL’s 24.64/37.99/27.53) as well as the exact match activity accuracy (Gaze
- ALGO: 1.8, ALGO+LAVILA: 3.5, KGL: 0.3, EGOVLP: 0.9, LAVILA: 2.1,
Gaze+ - ALGO: 3.4, ALGO+LAVILA: 8.3, KGL: 1.1, EGOVLP: 4.1, LAVILA:
7.9) show consistent improvements over the baselines across all metrics. While
VLMs perform better than ALGO on exact match, which we attribute to their
ability to identify the verb correctly, augmenting them with ALGO consistently
improves their performance.
Search space and knowledge source. We use ConceptNet as the source
of knowledge, over LLM or word embedding approaches, due to its ability to sup-
port probabilistic reasoning and an interpretable internal mechanism (see [4]).
4 S. Kundu et al.
While this does limit its vocabulary, it serves us well for reasoning over action-
object affordance and affinity. We could replace ConceptNet priors with word
embedding techniques such as GloVe or GPT 4, and the framework would still
function without any modifications. Preliminary analysis with GloVe embedding
on GTEA Gaze results in object/action/activity accuracy of 12.86/14.53/13.70,
which outperforms KGL/KGL+CLIP/LAVILA/EGOVLP. More complex em-
bedding could improve this performance at the cost of interpretability. Addi-
tionally, we show in Fig 2a that ConceptNet can be augmented with ChatGPT
to move beyond its vocabulary.The vocabulary of ConceptNet is not the
sameas thesearch spacefor CLIP/LAVILA/EGOVLP.The search space
is the space of prompts one provides to VLMs to select from and is usually pre-
defined in zero-shot inference. Open-world inference requires building this search
space for VLMs to infer labels. We propose (including prior-driven prompting)
to construct such a search space without brute-force search over all combinations
of verbs/nouns.
Qualitative AnalysisOur analysis shows that the performance across the
datasets varies and primarily stems from how the verbs and nouns are defined
in the dataset. For example, Gaze and Gaze+ have activity labels with less
ambiguous verb-noun combinations than EK100. For example, “clean”, “put”,
and “take”, common verbs in EK100, can apply to every single object and have
very high affinity in ConceptNet, leading to a higher likelihood of prediction.
This is one of the failure modes of ALGO and is one of our future research
directions.
Abbreviated paper title 5
(a) (b)
(c) (d)
(e) (f)
Fig. 3:Visualization of alternative concepts that were tested for grounding concepts
in the video such as (a) fork, (b) knife, (c) table, (d) pepperoni, (e) biscuit, and (f)
chocolate. These are automatically derived from ConceptNet and have semantic asser-
tions quantifying how they are related.
6 S. Kundu et al.
(a) (b)
(c) (d)
(e) (f)
Fig. 4:Visualization of final interpretations for videos containing the activity (a) cut
fork (top interpretation), (b) cut fork (second best interpretation), (c) pour honey, (d)
pour ketchup, (e) mix ketchup, and (f) mix bowl. These are automatically derived from
ConceptNet and have semantic assertions quantifying how they are related.