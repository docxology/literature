THE FREE ENERGY PRINCIPLE FOR PERCEPTION AND ACTION:
A DEEP LEARNING PERSPECTIVE
PietroMazzaglia∗ TimVerbelen OzanÇatal BartDhoedt
GhentUniversity-imec
Ghent,Belgium
ABSTRACT
Thefreeenergyprinciple, anditscorollaryactiveinference, constituteabio-inspiredtheorythat
assumesbiologicalagentsacttoremaininarestrictedsetofpreferredstatesoftheworld,i.e.,they
minimizetheirfreeenergy. Underthisprinciple,biologicalagentslearnagenerativemodelofthe
worldandplanactionsinthefuturethatwillmaintaintheagentinanhomeostaticstatethatsatisfies
itspreferences. Thisframeworklendsitselftobeingrealizedinsilico,asitcomprehendsimportant
aspectsthatmakeitcomputationallyaffordable,suchasvariationalinferenceandamortizedplanning.
Inthiswork,weinvestigatethetoolofdeeplearningtodesignandrealizeartificialagentsbased
onactiveinference,presentingadeep-learningorientedpresentationofthefreeenergyprinciple,
surveyingworksthatarerelevantinbothmachinelearningandactiveinferenceareas,anddiscussing
thedesignchoicesthatareinvolvedintheimplementationprocess. Thismanuscriptprobesnewer
perspectivesfortheactiveinferenceframework,groundingitstheoreticalaspectsintomorepragmatic
affairs,offeringapracticalguidetoactiveinferencenewcomersandastartingpointfordeeplearning
practitionersthatwouldliketoinvestigateimplementationsofthefreeenergyprinciple.
Keywords Freeenergyprinciple·Activeinference·Deeplearning·Machinelearning
Active
Inference
Variational Bayes action
2.1 2.2
inference Variational Expected free selection
free energy energy
3.1 3.1 4.1 4.2
Prior States
Optimization Preferences Epistemics
model distribution
3.2 3.3 4.3 4.3
Plan Habit
Uncertainty Representation Amortization
selection learning
Figure1: Thefreeenergyfunctionalminimizedbyactiveinferencetakestwoforms: variationalfreeenergy, with
respecttopastexperience,andexpectedfreeenergy,forselectingfuturebehaviors. Foreachofthetwo,an(amortized)
Bayesianoptimizationschemeisfollowedthatneedstoconsiderseveralaspects,assummarizedinthediagram. The
numberingindicatesthesectionofthepaperdiscussingeachaspect.
∗Correspondence:pietro.mazzaglia@ugent.be
2202
luJ
31
]GL.sc[
1v51460.7022:viXra
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
1 Introduction
Understandingtheprocessesthatsentientandreasoningbeingsplayoutmentallyinordertoperceivetheworldthey
liveandactinisascompellingasitiscomplex. Thefreeenergyprinciplehypothesizesallbrainprocessescanbe
understoodassubservingoneunicumimperative:theminimizationoffreeenergy[1,2].Thisprinciple,anditscorollary
activeinference,assumesthatagentsacttocontrastforcesfromtheenvironmentthatobstructthemfromremainingina
restrictedsetofpreferredstatesoftheworld. Underthisassumption,biologicalagentsdevelopavarietyofskills,such
asperception,action,planning,andlearning,thatagentscontinuouslyadaptalongtheirlives.
Activeinferenceandthefreeenergyprinciplehavebeenemployedtoexplainandsimulateseveralcomplexprocesses
acrossdifferentdisciplines.Inpsychology,theyhavebeenusedtogroundacomputationalaccountofneuropsychological
syndromes[3]andtodevelopemotionalrecognitiondevices,whichallowresolvinguncertaintyaboutemotionalstates
byinteractionandlearning[4]. Ineconomics,thefreeenergyprinciplehasbeenexploitedtoreformulatetheagents’
optimization process in terms of their beliefs [5]. Variational approaches have been employed to explain niche
construction,basedonthefreeenergyprinciple[6,7].Activeinferencehasbeenusedtomodelsmoothandsaccadiceye
movements[8,9]andtoconceptualizeattention[10],salience,andmemory[11]. Inthecontextofsceneconstruction,
activeinferenceprovidesanexplanationofhowagentsinferahigher-ordervisualpatternofascenethroughvisual
foraging[12,13].
Under some sets of assumptions [14, 15], the free energy principle can also be used to explain how all biological
organismsandprocessesthatsubserveperceptionandactionnaturallyemergeandarecontinuouslyadjustedthrougha
naturalmodelselectionprocess. Onshorter(somatic)timescales,minimizingfreeenergyleadstothedevelopmentof
thesingleorganisms’brains,givingrisetolearningandmemoryfunctions. Onlonger(evolutionary)timescales,free
energyminimizationfosterstheevolutionprocessofthespecies[16,17]. Establishingastatisticalboundaryforthe
Earth’sclimatesystem,thiscanalsobeinterpretedasaself-producingsystemthatisperformingactiveinference[18].
Systemsthatarecapableofself-production,i.e.,continuouslygeneratingandmaintainingthemselvesbycreatingtheir
ownparts,arecalledautopoietic[19]. Itcanbeshownthattheprocessofautopoiesisminimizesfreeenergy,given
thatforanorganismtomaintainamodelofitselfanditsenvironmentitmustminimallyself-producethecomponents
requiredtocarryouttheprocessofpreservingitsgenerativemodel[20].
Inactiveinference,theagentminimizesavariationalfreeenergyobjectivewithrespecttopastexperiencethatcauses
ittolearnagenerativemodeloftheworld,whichallowspredictingwhatwillhappeninthefutureinordertoavoid
surprisingstates.Variationalinference[21],fromwhichoriginatesthevariationalfreeenergyfunctional,makespossible
tocasttheprocessofperceptionasanoptimizationprocess,whichsubsumesasetofchoicesintermsofmodeling
andlearning,suchasthechoiceofthestatedistributionorthewaythemodelcopeswithuncertainty. Thegenerative
worldmodelisthenusedinthefuturetoplanactionsthatwillmaintaintheagentinahomeostaticstatethatsatisfiesthe
agentpreferences. Theagentoperatesan(amortized)Bayesianselectionofactionsthatwillhavetheleastsurprisewith
respecttoitspreferredstate. Thisdecision-makingprocesstakesintoconsiderationseveralaspectsoflearning,suchas
epistemics,habitlearning,andpreferencelearning.
Recent developments in deep learning have opened new frontiers for studying and experimenting with different
perceptionandbehavioraltheories;enablingthepracticalanalysesofartificialimplementations,eitherinsimulationsor
inrealenvironments. Onepopularexampleinthisregardisreinforcementlearning(RL)[22],atheorythatlinksthe
dopaminesignalsinthebraintorewardsignalsthatcanbeusedtoreinforcecorrectbehaviors[23,24],anddescribes
howintelligentbehaviorscanbelearnedthroughrewardmaximization[25]. CombiningRLwithdeeplearningmodels
forfunctionestimation[26]hasledtoseveralempiricalsuccesses,allowingthetrainingofartificialagentstoplayvideo
games[27,28],masterboardgames[29],orexecuterobotictasks[30]. Similarly,deeplearningtechniquesarealso
startingtoariseinthecontextofactiveinference[31,32,33].
Thisworkaimstosurveythecurrentstate-of-the-artofdeeplearningmodelsforactiveinference. Atthesametime,we
wanttoprovideareferenceandastartingpointformachinelearningpractitionerstobecomeacquaintedwithactive
inference,drawingparallelsbetweenactiveinferenceandrecentadvancesinRL.Otherreviewsofactiveinferencehave
previouslybeenpresentedforexpressinggenerativemodelsincontinuousspaces[34]ordiscretespaces[35];however,
thecontext,thepracticespresented,andthescopeofourworkstronglydifferfromtheirs,inthatwefocusspecificallyon
deep-learning-basedtechniquestoscaleactiveinferencetolargecontinuousstateandactionspacesorhigh-dimensional
settings,suchasroboticsandvisualcontrol. Concurrentlytoourwork,areviewonactiveinferenceforroboticshas
beenreleased[36],includingreferencestodeeplearningmethodsforactiveinference. Ourworkdiffersfromtheirsin
thattheyspecificallyfocusonmethodsthatenableapplyingactiveinferenceinrobotics,whilewemorebroadlydiscuss
activeinferencetechniquesthatcanbeemployedtodevelopactiveinferenceartificialagents,explainingindetailshow
eachcomponentcouldbeimplemented,highlightingthechallengestoovercome,andestablishingconnectionsbetween
methodsindifferentmachinelearningareas.
2
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
Anon-exhaustivesummaryoftheaspectsthatareconsideredintheactiveinferenceframeworkforlearningperception
andactionispresentedinFigure1. Wedivideintomodellearningontheonehand,andactionselectionontheother
hand. Thefirstcomprisesminimizingvariationalfreeenergyforlearninggenerativemodelsbasedonpastexperiences,
modelingbeliefstatesandpriors,learningstaterepresentationsanduncertainty,whereasthelatterdealswithselecting
actions in the future, which trade-off epistemic foraging and realizing preferences, either by planning or learning
habits. TworecurrentpatternsinactiveinferencearevariationalinferenceandtheamortizationofBayesianselection.
Variationalinferenceallowscastinginferenceasanoptimizationproblem,i.e.,findingthedistributionclosesttothe
actual one. Amortization allows a faster computation of the inference process, by reusing previous computations
[37]. Inactiveinference,amortizedinferenceisappliedforthechoiceofvariationalparameters,inthemodellearning
process,andfortheformationofhabits,intheactionselectionprocess. Combiningthetwotechniquesstronglyreduces
thecomputationalrequirementsofactiveinferenceandmakestheframeworkpromisingforimplementationinsilico;
however,withoutadequatemodels,itisunfeasibletoscaleactiveinferenceincomplexscenarios,withcontinuous
and/orhigh-dimensionalstate/actionspaces.
Indeeplearning,generativemodelshavebeenwidelystudied,obtainingoutstandingresultsinseveraldomains,suchas
imagegeneration[38,39,40],textprediction[41,42,43],andvideomodeling[44,45,46,47]. Inparticular,temporal
deepgenerativemodelsthatallowpredictingthedynamicsofasystem,i.e.,theenvironmentorworld,havebeenstudied
forcontrol[48,49,50],curiosityandexploration[51,52,53],andanomalydetection[54]. Severalofthesemodels
havebeenusedinsettingsthataresimilartotheactiveinferenceone,andsomeofthemevensharesomesimilarities
withtheactiveinferenceobjectiveofminimizingvariationalfreeenergy. Asforactionselection,severalworksthat
usedeeplearninghaveimproveduponmoreclassicalmethods(e.g.,α−β pruning,A*,beamsearch)allowingto
searchmuchlargerstateandactionspaces. Someexamplesaredynamicprogramming-relatedtechniquesinRL[26],
evolutionarystrategies[55],andMonteCarloTreeSearch(MCTS)[29,56]. Thesemethodsallcombinemoregeneral
andclassicalplanningstrategieswithdeeplearningforfunctionestimation,enablingthescalingofbehaviorlearning
andactionselectionforcomplexenvironments.
Theremainderofthisworkisorganizedasfollows: inSection2,wepresentthefreeenergyprincipleandexplainwhat
minimizingfreeenergyentailsintermsofperceptionandaction;inSection3,weestablishtheconnectionbetween
modellearning,accordingtoactiveinference,anddeepgenerativemodels,analyzingthedifferentingredientsinvolved.
InSection4,wediscusstheimplementationanddesignchoicesthatunderlietheactionselectionprocess,relating
existingworksonhabitlearning,explorationandmodel-basedcontrol. Finally,weconcludeourdiscussion,remarking
theworkconducteduntilthismoment,addressingtheimplicationsoflearningwithdeeplearning,andproposingsome
futureperspectives.
2 TheFreeEnergyPrincipleandActiveInference
Thefreeenergyprincipleisatthecoreoftheactiveinferenceframework, asitconceptualizesthedevelopmentof
embodiedperceptionastheresultofminimizingafreeenergyobjective. Asweshowinthissection,thefreeenergy
isafunctionoftheagent’sbeliefsabouttheenvironment,representinga(variational)upperboundonsurprisalfrom
sensorialstimuli. Thisentailsthatreducingfreeenergyadditionallyreducestheagent’smodelsurprise,restricting
itsexistencetoalimitedsetofcravedbeliefs. ThefreeenergyprincipleoriginatedfromtheworkofvonHelmholtz
on‘unconsciousinference’[57],postulatingthathumansinevitablyperforminferenceinordertoperformperception.
Thisimpliesthatthehumanperceptualsystemcontinuouslyadjustsbeliefsaboutthehiddenstatesoftheworldinan
unconsciousway. Thevariationalformulationofthefreeenergy[58,1],alongwiththeintroductionofactionsaspart
oftheinferenceprocess,expandedtheoriginalfreeenergyprincipleleadingtothedevelopmentofactiveinference.
InFigure2,weillustratetheinterplaybetweenthemainfactorsthatdeterminetheembodiedperceptionprocessas
describedinactiveinference. Atanytime,theenvironmentisinacertainstateη,whichisexternaltotheagentandnot
directlyobservable. Theagentinteractswiththeenvironmentintwoways: eitherthrough(passive)sensorialperception,
whichischaracterizedbytheobservationofsensorialstateso,orbyactions,whichcanbecastasasetofactivestatesa
thattheagentimposesontheenvironment. Accordingtothefreeenergyprinciple,inordertominimizefreeenergy,
theagentlearnsaninternalmodelofpotentialstatesoftheenvironment. Crucially,theseinternalstatesdonotneed
tobeisomorphictotheexternalones,astheirpurposeisexplainingsensorialstatesinaccordancewithactivestates,
ratherthanreplicatingtheexactdynamicsoftheenvironment. Isomorphism,inthiscontext,referstoconsideringa
structure-preservingmappingofthestatespace. Accordingtoactiveinference,internalandenvironmentstatesarenot
necessarilyequalandthewaytheinternalstatesareorganizedmayevendifferfromagenttoagent,despitehavingto
dealwithsimilarconcepts/observations/sensorystates. Fromabiologicalperspective,thisfindsevidenceinthefactthat
differentlivingsystemshavedevelopeddifferentorgans/tissuesalongtheirevolutionaryprocess[16]. Theroleofthe
internalstaterepresentationis,infact,toprovidethesufficientstatisticsthatallowa‘bestguess’aboutthecausesofthe
agent’sobservationsandtheselectionofadaptiveactionpolicies[59].
3
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
Environment States Sensorial Active Model States
(external) states states (internal)
Figure 2: The external environment states η are the hidden causes of sensorial states o (observations). The agent
attemptstorepresentssuchhiddencausesthroughitsinternalmodelstatess. Crucially,internalstatesmayormaynot
correspondtoexternalstates,whichmeansthathiddencausesinthebraindonotneedtoberepresentedinthesame
wayasintheenvironment. Activestatesa(actions),whicharedevelopedaccordingtointernalstates,allowtheagentto
conditiontheenvironmentstates.
Asaconsequenceofminimizingfreeenergy,theagentpossessesbeliefsabouttheprocessgeneratingoutcomes,but
alsoaboutactionpoliciesthatleadtogeneratingthoseoutcomes[60]. Thiscorrespondstoaprobabilisticmodelof
howsensationsarecausedandhowstatesshouldbeactivelysampledtodrivetheenvironment’sgenerationofsensory
data. Becauseoftheseassumptions,theconceptof‘reward’inactiveinferenceisverydifferentfromrewardsinRL,
asrewardsarenotsignalsusedtoattracttrajectories,butrathersensorystatesthattheagentsaimstofrequentlyvisit
inordertominimizeitsfreeenergy[61]. Fromanengineeringperceptive,thisdifferenceisreflectedinthefactthat
rewardsinRLarepartoftheenvironmentand,thus,eachenvironmentshouldprovideitsuniquerewardsignal,whilein
activeinference‘rewards’areintrinsictotheagent,whichwouldpursueitspreferencesinanyenvironment,developing
asetofmostfrequentlyvisitedstates.
Intheremainderofthissection,wediscusshowtheagent’sprobabilisticmodelforperceptionandactionislearnedby
minimizingfreeenergy,providingamathematicalsynthesis. Weconsidertheenvironmentasapartiallyobservable
Markovdecisionprocess(POMDP),representedinFigure3. Usingsubscriptstospecifythediscretetimesteps,we
indicatetheobservationoroutcomeattimetwitho . Toindicatesequencesthatspanoveranundefinednumberoftime
t
steps,weusethesuperscript∼,i.e.,foroutcomeso˜={o ,o ,...,o }. Thesuccessionofstatess˜={s ,s ,...,s }
1 2 t 1 2 t
isinfluencedbysequencesofactions,orpolicies,thatweindicatewithπ =a˜={a ,a ,...,a }. Parameterizationof
1 2 t
thestate-outcomelikelihoodmappingisindicatedwithθ. Aprecisionparameterζ influencesactionselectionworking
asaninversetemperatureoverpolicies.
Thesectionisdividedintwoparts: thefirstexplainshowtheinternalmodelislearnedwithrespecttopastexperience,
minimizingavariationalfreeenergyfunctionalthatexplainsthedynamicsoftheenvironment’soutcomes,givena
sequenceofactions. Inthesecondpart,wediscusstheminimizationofexpectedfreeenergywithrespecttothefuture,
whenactionsareselectedtoreducesurprisewithrespecttotheagent’spreferredoutcomes. Importantly,ourtreatment
referstoadiscrete-timeinstantiationofactiveinference. Foradiscussiononcontinuoustime,thereadermayreferto
[34].
2.1 VariationalFreeEnergy
In order to minimize the negative log evidence (also known as “surprisal” or “surprise”) of observations from the
externalworld,theagentexploitsitspastsensoryexperiencestolearnagenerativemodeloftheenvironmentusing
variationalinference. Underthefreeenergyprinciple[62,63],anupperboundonsurpriseisestablishedas:
p(o˜,s˜,π,θ,ζ) p(o˜,s˜,π,θ,ζ)
−logp(o˜)=−logE [ ]≤E [−log ]
(cid:124) (cid:123)(cid:122) (cid:125) q(s˜,π,θ,ζ) q(s˜,π,θ,ζ) q(s˜,π,θ,ζ) q(s˜,π,θ,ζ) (1)
surprise (cid:124) (cid:123)(cid:122) (cid:125)
variationalfreeenergyF
where, left to right, the following operations are performed: (i) the surprise over the sequence of observations is
marginalized(i.e.,summedover/integrated)withrespecttotheotherfactorsofthegenerativemodel,whicharethe
4
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
1 2
Generative model
Factors
1 Precision (policy)
2 Beliefs about policies
3 3 3
3 Transition probabilities
5 5 5
4 Parameters (likelihood)
5 Likelihood mapping
4
Figure3: Thediagramillustratestheinterplaybetweenthedifferentfactorsthatcomposethegraphicalmodel. (1)
Policyprecision;(2)beliefsaboutpolicies;(3)transitionprobabilities,alsoknownasdynamics;(4)parametersofthe
likelihoodmapping;(5)likelihoodmodel.
sequenceofstates,thepolicyofactions,themodelparameters,andthepolicyprecisionparameter,(ii)avariational
posterioroverstatesandpoliciesq(s˜,π,θ,ζ)isintroducedforvariationalinference[21],(iii)Jensen’sinequalityis
appliedtopushthelogarithminsidetheexpectationoperator.
ThevariationalfreeenergyF canbereformulatedas:
F =E [F ]+D [q(π,ζ)(cid:107)p(π,ζ)]+D [q(θ)(cid:107)p(θ)],
q(π,θ) π,θ KL KL
(2)
F =E [logq(s˜|π,θ)−logp(o˜,s˜|π,θ)].
π,θ q(s˜|π,θ)
When minimizing the variational free energy with respect to the past, the second equation is generally adopted as
thetwoKullback–Leibler(KL)divergencetermsinthefirstequationcanbeneglected. TheformerKLdivergence,
referringtothepoliciesandtheprecisionparameter,canbeoverlookedbecausepoliciesinthepastareobserved. The
latterKLdivergence,referringtothemodelparameters,canbelaterconsideredtoworkontopofthemodelthrough
regularizationtechniques,e.g.,similarlytosleep,whereredundantsynapticparametersareeliminatedtominimize
modelcomplexity[64].
Omittingconditioningonπandθforbrevity,allowstheexpressionofthefreeenergyinitstwotypicalforms:
F =D [q(s˜)(cid:107)p(s˜)]−E [logp(o˜|s˜)]
π,θ KL q(s˜)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
(3)
=D [q(s˜)(cid:107)p(s˜|o˜)]− logp(o˜)
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
approxvstrueposterior logevidence
Ontheonehand,minimizingfreeenergyimpliesmaximizingtheaccuracyofalikelihoodmodelp(o˜|s˜)whilereducing
thecomplexityoftheposteriordistribution. Ontheotherhand,itimpliesoptimizingthevariationalevidencebound,
remindingthattheKLdivergenceisalwaysnon-negative,namelyD [·(cid:107)·]≥0foranydistribution. TheKLdivergence
KL
iszerowhentheagent’smodelperfectlymatchestheenvironmentdynamics,correspondingtotheoptimalscenario.
Thoughthereisnoexpectationoperatoroverpastexperiences(bothforbrevityandtocomplywiththetypicalway
of expressing this functional), it should be clear that the agent minimizes variational free with respect to known
sequencesofpastobservationsandpoliciesfromtheenvironment. Aswediscussinthefollowingparagraphs,thisis
fundamentalandisthemainaspectthatdifferentiatesthevariationalfreeenergy,computedwithrespecttopaststatesof
theenvironment,fromtheexpectedfreeenergy,whichconsidersfuturestatesandunobserveddata.
2.2 ExpectedFreeEnergy
Inordertominimizefreeenergyinthefuture,theagentshouldadaptitsbehavior,i.e. theactivestates,toconfineits
existencewithinalimitedsetofstates. Thesestatescorrespondtothesocalledpreferencesoftheagent,orpreferred
5
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
observations/outcomes/states,dependingonthecontext. Theobjectiveoftheagentistoexploititsknowledgeabout
theenvironment, availablethroughtheinternalmodel, toperpetuallyfulfillpreferredperceptionsintheupcoming
future. While minimizing free energy about future sequences, the agent imagines how the future would look like,
givenacertainsequenceofactionsorpolicy. Thisisreflectedbyanexpectationoverfuturestatesandobservations
generatedbythemodel,whichexploitsboththeinternalstatesmodelandthelikelihoodmappingfromtheenvironment
asq˜=q(o˜,s˜,θ|π)=p(o˜|s˜,θ)q(s˜|π)q(θ).
Startingfromthisassumption,theexpectedfreeenergyG canbeexpressedas:
G =E [logq(s˜,θ|π)−logp(o˜,θ,s˜|π)]
π q˜
=−E q˜ [logp(s˜|o˜,π)−logq(s˜|π)]−E q˜ [logp(θ|s˜,o˜,π)−logq(θ)]−E q˜ [logp(o˜)] (4)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
informationgain(hiddenstates) informationgain(parameters) extrinsicvalue
Hence, minimizing the expected free energy implies that the agent: (i) maximizes
epistemic value, i.e., mutual information between hidden states and sensory data,
(ii) maximizes parameter information gain, i.e., mutual information between parameters and states, and (iii)
maximizesextrinsicvalue,i.e.,theloglikelihoodofoutcomesunderapreferred,priordistributionlogp(o˜),orrewards.
Again,omittingconditioningonthepolicyforbrevity,itispossibletorewritetheexpectedfreeenergyasaminimization
ofriskandambiguity:
G =E [H[p(o˜|s˜)]+D [q(s˜)(cid:107)p(s˜)]−E [logp(θ|s˜,o˜)−logq(θ)] (5)
π q˜ KL q˜
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ambiguity statecontrol informationgain(parameters)
≈E [H[p(o˜|s˜)]+D [q(o˜)(cid:107)p(o˜)]−E [logp(θ|s˜,o˜)−logq(θ)]
q˜ KL q˜
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ambiguity risk informationgain(parameters) (6)
Here,weassumetheboundistightandtheapproximateposteriorisagoodapproximationofthetrueposteriorto
expressriskintermsofoutcomes;however,theagentmayalsoexpressitshomeostaticpreferencesintermsofinternal
statesratherthanonitssensorialperceptions,andformulatetheexpectedfreeenergyintermsofstatecontrol.
Despite the variety of objectives that can be considered for the expected free energy, by either changing some
assumptions or reordering its factors, the imperative of the functional stays the same. The goal of the agent is to
restrictitselftoitspreferredsetofstates/sensorialperceptions,whileminimizingtheambiguityofitsinternalmodel.
Maximizingepistemicvalueand/orparameterinformationgain,asforEquation(4),alsoimplicitlyadherestothis
hypothesis,asfindinginformativestatesintheenvironmentwillminimizetheuncertaintyofthemodelinthefuture
[65,63].
3 VariationalWorldModels
Inactiveinference,theobjectiveofminimizingsurpriseoftheinternalmodelwithrespecttosensoryinputsinducesa
continuousmodellearningprocessthathappensinsidethebrain. Thisassumesapredictivecodinginterpretationof
thebrain,wheretheinternalmodelisusedtogeneratepredictionsofsensoryinputsthatarecomparedtotheactual
sensoryinputs. Theinternalmodelattemptstoexplainthedynamicsoftheworldandthus,asperformedinrelatedwork
[50,66],wealsorefertoitasthe‘worldmodel’. Thereactionoftheinternalmodel,tendingtominimizefreeenergy
withrespecttothesensoryinputs,accountsforperceptionoftheagent,whichlearnstopredictthesensoryinputsand
thecausalstructureoftheirgeneration.
Inmachinelearning,suchalearningprocess,whichrequiresnohumansupervisionorlabelling,isgenerallyreferred
toasself-supervisedorunsupervisedlearning. Incontrasttobiologicalagents,deeplearningsystemstypicallyusea
batchlearningscheme,whereadatasetofpasttrajectoriesiscollected,andmodelsareparameterizedasdeepneural
networksthatareoptimizedbytrainingonbatchesofdatasampledfromthisdataset. Suchadataset,indicatedbyD ,
env
canbeseenasanorderedsetoftripletscontaininganenvironmentobservation,theagent’saction,andthefollowing
observation(causedbytheaction),namelythetriplet(o ,a ,o ). Trainingthemodelisthentypicallyalternated
t t t+1
withcollectingnewdatabyinteractingwiththeenvironment,usingthemodelforplanning[49],orusinganamortized
(habitual)policy[67,33]. Inpractice,onecanalsotrainamodelupfrontusingadatasetofcollectedtrajectoriesfroma
randomagent [68]oranexpert[32]. Thelatterisespeciallyrelevantincontextswherecollectingexperienceonline
withtheagentcanbeexpensiveorunsafe[69].
6
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
Thefreeenergylosstominimizeforonetimestep,i.e.,π =a ,canthenbewrittenundertheexpectationsofthedata
t
(observationsandactions)fromthereplaybuffer:
F =E (cid:2) D [q(s |s ,a )(cid:107)p(s |s ,a )]−E [logp(o |s )] (cid:3) (7)
at Denv KL t+1 t t t+1 t t q(st+1|st,at) t+1 t+1
Giventheaboveloss,threedistributionsormodelsneedtobeinstantiated: (i)alikelihoodmodelp(o |s )that
t+1 t+1
allows generating (also known as reconstructing) sensory data from the model’s internal states, (ii) a prior model
p(s |s ,a ),whichencodesinformationaboutthetransitionprobabilitiesofthedynamicsoftheinternalstates,(iii)
t+1 t t
aposteriormodelq(s |s ,a ),whichischosentominimizetheupperboundonsurprise,accordingtovariational
t+1 t t
inference[21]. Inmachinelearning,thisformulationisbetterknownasthe(negative)evidencelowerbound(ELBO),it
isthesameasthelossusedtotrainvariationalautoencoders(VAE)[70,71])anditisshowntooptimizeavariational
informationbottleneck,tradingoffbetweentheaccuracyandthecomplexityofsummarizingthesensoryinformationin
theinternalstaterepresentation[72,73]. Asitisassumedthatexternalstatesarenotobservedbytheagentandthat
thedynamicsoftheenvironmentisunknown,thereisnowaytoensurethattherepresentationlearnedisthesameas
externalone;however,therelationshipwithinformationcompressiontechniquesensuresthatminimizingfreeenergy
entailsoptimizingtheinformationcontainedintheinternalstatesaboutthesensorystates[62,74].
Theoretically,deeplearningmodelscanbeemployedtoapproximateanyfunctionwithanarbitrarydegreeofaccuracy
[75],whichinourcasemeanspredictingboththeinternalstatesandsensorydatadistributionswithanarbitrarydegree
ofaccuracy. Inpractice, though, obtaininganhighlyaccuratemodelisdifficultand, whileneuralmodelscanfind
usefulapproximationsofthesemodels,ifoneofthemiswell-knowninadvance,directlyusingitandadaptingtheother
modelsaccordinglycouldleadtomoresatisfactoryresults. Forinstance,iftheactuallikelihoodmodelisknownfora
certainstatespace,thestatespaceofthepriorandposteriormodelscanbeadaptedaccordingly. Thisisthecasefor
differentiablesimulators,wheretheenvironment’sobservationsaretheresultofadifferentiablegenerationprocessthat
canbeintegratedintheworldmodel[76,77]. Oragain,ifthedynamicsoftheenvironmentisknown,itispossible
tousethatasaprior,forcingtheinternalstatesofthemodelandtheexternalstatesoftheenvironmenttohavethe
samestructure. WhentheenvironmentisrepresentedasaPOMDP,havingcompleteknowledgeofthedynamicsisthe
onlycasethatensuresanoptimalbehaviorcanbefound[22,78];however,evenknowingthedynamicsinadvance,
solvingthePOMDPproblemremainscomputationallyintractable. Functionapproximationstechniquesandprocedures
toconstructanimprovedstaterepresentation,asthemodellearningapproacheswedescribehere,areoftenusedtofind
nearlyoptimalpoliciesinmorecomputationallyefficientways[79,80,81].
Inageneralscenario,alldistributionsareunknownandmustbeeitherlearnedbytheagentorassumedhavingacertain
form,accordingtosomedesignchoices. Thereareindeednumerousoptionstoconsiderwheninstantiatingthedifferent
models,andsomeofthemareimportanttocarryoutastableoptimizationand/orawell-thoughtamortizationofthe
Bayesianinferenceprocess. Otherdesignchoicesconsiderdifferentaspectsofthegenerativemodel(Figure3),suchas
theparametersofthelikelihoodmappingθandthesensitivityofthemodel,whicharecertainlyrelevantbutthatcan
oftenbeassumedfixedandneglected.
3.1 Models
Toinstantiatethedeepneuralnetworksfortheagent’sgenerativemodel,first,itisimportanttoconsiderthenatureof
thevariablesinvolved. Forthehiddenstates,activeinferenceassumesaprobabilisticmodel. Unlesstheenvironment
statespacenatureisknown,theinternalstatedistributionmayhavenopredefinedstructure,andneuralnetworkscanbe
trainedtooutputdistributionsofdifferentkinds;however,inordertocomputetheexpectations,asinEquation(7),itis
importantthatthesamplingprocessofthedistributionisdifferentiable,astheobjectiveneedstobebackpropagated
through the model for computing the gradients that update the model [82]. As the sampling process is generally
non-differentiable, the gradients of the samplesshould be estimated with adhoc techniques. Some widely known
examplesarethereparametrizationtrickforGaussiandistributions[70,71],thestraight-throughgradientmethod[83],
thelikelihood-ratiosmethod[84],alsoknownasREINFORCEgradients[85],forBernoulliandcategoricalvariables.
Intheactiveinferenceliterature,multivariateGaussian(alsoknownasnormal)distributionswithadiagonalcovariance
matrixhavebeenlargelyadoptedsincetheinitialworksonVAEs[68,32,86,33]. Similarly,numerouslatentstatespace
modelshaveadoptedaGaussianstructureoftheirlatentspace[48,49,87,88],butalsomorecomplexmixturemodels
havebeenproposed[50]. ForBernoulliandcategoricaldistributions,therehasbeenbothworkongeneral-purpose
generativemodels,suchasdiscreteVAE[89,38],onlatentdynamicsmodelsforplanning[66,90],andrecentlythey
havealsobeenusedinanactiveinferencesetting[91]. Someotheralternativestotheabovemethods,whichhaveyetto
beexploredfortrainingworldmodels,are: piecewisedistributions[92],Markovchains[93],andnormalizingflows
[94].
7
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
PosteriorModel.Thechoiceofthedistributionisparticularlyimportantfortheposteriormodel,whichisthevariational
distribution. In theory, one could search for an optimal distribution of parameters for each of the environment’s
transitions/observations, though that is a slow and difficult process. To speed up training, but also guaranteeing a
legitimatechoiceoftheposterior,itispossibletoamortizetheselectionoftheposteriorparameters,aspresentedinthe
originalVAEwork[70,71]. Theautoencodingamortizationschemeemploystheobservationcorrespondingtoacertain
statetoinfertheparametersofthevariationaldistribution,q(s |s ,a ,o ). Thisallowsoptimizingtheparameters
t+1 t t t+1
oftheposteriortocompressinformationoptimally,astheposteriorhasaccesstotheobservationthatthelikelihood
modelwantstogenerate. InVAEtermstheposteriormodelistypicallycalledthe“encoder”,whereasthelikelihood
modelisdubbedthe“decoder”.
Thechoiceoftheencoderarchitecture,whichallowstheflowofinformationfromobservationstotheposterior,depends
ontheenvironment. Forinstance,fortwo-dimensionalmatricesofdata,suchasimages,convolutionalneuralnetworks
(CNN)[95],orotherarchitecturesforcomputervisionsuchasvisiontransformers[96]arecommonchoices. Other
potentiallyusefulmodelscanbemultilayerperceptrons(MLP)forvector-structureddata[97]orgraphneuralnetworks,
forgraph-structureddata[98]. Similarly,thechoiceofthelikelihoodmodeldependsontheformatoftheobservation
data,e.g.,atransposedCNNcanbeusefulinthecaseofvisualdata.
PriorModel. Thepriormodelcaneitherbeknownorlearned. Tolearnthepriormodel,onecanadoptarecurrent
neuralnetworkarchitecture,i.e.,usingmemorycellssuchaslongshorttermmemories(LSTM)[99]orgatedrecurrent
units(GRU)[100]. Inothercases,theenvironmentdynamicsisknownupfront,orassumptionsaboutthepriorcan
bemade,suchasassumingthatthepriorisauniformprobabilitydistribution. Forinstance,anisotropicmultivariate
GaussianN(0,I),withzeromeanandanidentitycovariancematrixI,canbeemployedasafixedprior,asperformed
instandardVAEarchitectures[70,71]. Alternatively,assumingthelawsthatgovernthedynamicsareknown(e.g.,
physicslaws),theenvironment’sphysicscouldbeexploitedasastrongprior[101]. Inasimilarfashion,in[102],the
authorsusedtheinternalstateoftherobottoforceaknownpriorstructureontheposterior. Finally,thepriorcouldalso
beignored/consideredconstant,treatingthemodelasanentropy-regularizedautoencoder[103].
3.2 Uncertainty
Intheactiveinferenceperceptionmodel,precision,orsensitivity[104,105],isgenerallyassociatedtotheuncertainty
ofthetransitionsbetweenhiddenstatesoftheprior(beliefsprecision)orthemappingfromhiddenstatestooutcomes
ofthelikelihood(sensoryprecision),andcanbeexpressedastheinversevarianceofthedistribution[106]. Inactive
inferenceimplementations,precisionhasbeenemployedasaformofattention,todecideonwhichtransitionsthe
modelshouldfocusonlearningfrom[33],thoughthisaspecthasbeengenerallylessstudiedintheliterature. Similar
mechanismsofprecisionhavebeenemployedforVAEmodelstocontroldisentanglementofthelatentstatespace[107]
orposteriorcollapse[108].
Anothersourceofuncertaintyinthemodelarisesfromuncertaintyintheparameters. Inthedeeplearningcommunity,
uncertaintyaboutmodelparametershasbeenstudiedemployingBayesianneuralnetworks[109],dropout[110],or
ensembles[111],andusedinRLtostudytheexplorationproblem[53,112,113]. Inactiveinference,consideringthe
generativemodelinFigure3,uncertaintyistreatedwithrespecttothedistributionovertheparametersofthelikelihood
model. SimilarlytoRL,dropout[33]andensembles[114]havebeenstudiedintheactiveinferenceliterature,although
severalimplementationsuntilnowhaveneglectedthisaspect,assumingconfidenceoverasinglesetofparameters.
3.3 Representation
FollowingthevariationalfreeenergyformulationfromEquation(7),theagent’sgenerativemodelisassumedcapable
ofgeneratingimaginaryoutcomesthatmatchcloselywiththesensoryperceptionsthroughthelikelihoodmodel. This
ispresentedontheleftinFigure4,foranenvironmentwithvisualsensorydata,i.e.,images. Themodeldepicteduses
asequentialVAE-likesetup,withtheposteriorencodinginformationfromtheobservation(red)inthestate,andthe
likelihoodmodelgeneratingobservationsfromthestatewithadecoder(blue).
However,learningalikelihoodmodelfromhigh-dimensionalsensoryobservations,suchasinpixel-basedenvironments,
is not a trivial problem. In this case, the likelihood model needs to generate images that match with the original
observationspixelbypixel,requiringbothahigh-capacitymodelandaconsiderableaccuracy,especiallyforhigh-
resolutionimages. Mostoften, theprobabilitydistributionofanimageisrepresentedasaproductofindependent
Gaussiansovereachpixelwithfixedstandarddeviation,inwhichthelog-likelihoodlossinEquation(7)becomesthe
pixel-wisemeansquarederrorbetweentwoimages;however,thiscanbeproblematic,asitmightleadtothemodel
ignoringsmallbutimportantfeaturesintheenvironment(asthepixel-wiselossislow)andwastingalotofcapacityin
encodingpotentiallyirrelevantinformation(i.e.,theexacttexturesofawall).
8
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
(a) (b)
(c) (d)
Figure4: Representationlearningapproachesforworldmodelswithalatentdynamics. Ontheleft,thebaseapproach
withthelikelihood-modelthatreconstructssensoryinformation. Ontheright: (a)Task-orientedrepresentation;(b)
State-consistentrepresentation;(c)Memory-equippedmodel(memorycellindicatedwithM);(d)Hierarchicalstates
structure.
Onepotentialsolution(a)istotrainthemodelinsightofthefuturetasktoaccomplish,consideringonlythestates
relatedtoaccomplishingtheagent’sgoal,i.e.,rewards. Findingahiddenstaterepresentationthatallowspredicting
such information, without necessarily generating observations greatly lightens the representational burden of the
model,thoughitwouldmaketheinternaldynamicslessinformed. AnexampleisillustratedinFigure4a. Similar
representationshavebeenemployedforRL[29],andcouldperhapsbealsoadaptedforactiveinference.
Anotherproposedsolution(b)istoreplacethelikelihoodcomponentofthelosswithastate-consistencyloss. These
kinds of representations, which enforce some form of consistency between states and their corresponding sensory
observations, have increasingly gained popularity in deep learning as self-supervised learning methods, such as
contrastivelearning[115],clustering/prototypicalmethods[116],distillation/self-consistencymethods[117,118],and
redundancyreduction/conceptwhitening[119,120]. Theserepresentationtechniques,depictedinFigure4b,havealso
beenshownsuccessfulintrainingdynamicsmodelsforRL[121,122]and,recently,foractiveinferenceaswell[123].
Ratherthanreplacingthelikelihoodmodel,someotherapproaches(c–d)haveinsteadfocusedonimprovingthecapacity
ofthemodel. Thisisthecaseofthememory-equippedmodels(Figure4c)andhierarchicalmodels(Figure4d). Using
memoryallowstopreservemoreinformationaboutother(past)observations,andhasshownencouragingresultsin
traininglatentdynamicsmodelswithdeeplearningmemorymodels,suchasLSTMsandGRUs[49,48,124,47]. The
memoryincreasesthecapacityofthemodelandallowsmoreaccuratepredictionsofstatesthatarefarinthefuture,
especiallywhenthepriormodelisunknownandmustbelearned.
Hierarchicalmodels,whichintheactiveinferencecommunityarealsoreferredtoasdeepactiveinferencemodels
[125,13](withanunfortunateconfusioncausedbyusing“deepactiveinference”asatermforactiveinferencemethods
using deep neural networks in the generative model [31, 32, 102, 126]), use a multi-layer structure of the hidden
statesofthemodelthatfacilitatesthemodelingofpart-wholeortemporalhierarchies. Similarlytousingamemory,a
hierarchyofstatescanincreasetherepresentationalcapacityofthemodelandallowmoreaccuratepredictions. Some
deeplearningexampleshavealreadyimplementedthis[127,128]aswellassomeactiveinferenceimplementationsfor
long-termnavigation[69].
3.4 Summary
Thereareseveralchoicestoconsiderwhendesigningavariationalworldmodel. Inthissection,weexplainsomeof
themalongwithprovidingreferencestoavarietyofstudiesthatactuallyimplementthesemechanism,considering
9
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
Table1: Implementationanddesignchoicesforlearningthevariationalworldmodeloftheagent. Thetabledisplays
oneortwoexamplesforeachaspect–modalitypair,bothintheactiveinferenceandinthemoregeneralactiveinference
literature,whenapplicable.
Modality ActiveInference DeepLearning
Gaussian [32,114,129,33,123] [49,87]
Statesdistribution Categorical [91] [90,66]
Others - [92,93,94]
Noprior - [103]
Priormodel Knownprior [102,86,114] [101,70,71]
Learnedprior [31,68,123,91,33] [49,66,48]
Precision [33] [107,108]
Uncertainty Ensemble [114,91] [113,112]
Dropout [33] [130]
Task-oriented - [29]
Stateconsistency [123] [121,122,119,116,131]
Representation
Memory-equipped [124,32] [49,66,48,50,47]
Hierarchical [132] [128,127]
contributionsbothinthelargerdeeplearningliteratureaswellasinanactiveinferencecontext. Asummaryofthe
designchoicesispresentedinTable1.
In the future, it will be important to continue investigating these design choices and analyzing their interplay. We
believethatsynergiesbetweenadvancesinthedeeplearningcommunityandactiveinferenceadopterswillbecrucialto
furtherdevelopgenerativemodelsforawiderangeofusecases. Wealsolookforwardtowardsnovelaspectsofmodel
learning,suchastherepresentationoftime[133]orthemodelreductionhappeningduringsleep[134].
Itisalsoimportanttonotethatsomeofthedesignchoicesdiscussed,suchastheinternalstaterepresentation,havean
impactonthemodellearningpartbutalsoontheagent’sactionselection. Forinstance,ifthereisnolikelihoodmodel,
howshouldtheagentrecognizewhetherthepreferredoutcomesarebeingsatisfied? Or,again,iftheagent’smodelis
hierarchical,howshouldgranularactionsrelatetothehierarchicalstatestructure? Inthenextsection,weprovidean
overviewofthetechniquesproposedtoadoptthegenerativemodelforactionselection,andelaborateontheseissues.
4 BayesianActionSelection
Inordertoselectfutureactionsinactiveinference,theagentexploitsthelearnedmodelinordertomatchitspreferred
outcomesbyminimizingtheexpectedfreeenergy. Moreformally,theagent’sbeliefoverwhichpolicyorsequenceof
actionstofollowisgivenby:
p(π|ζ)=σ(−ζG ), (8)
π
whereσisthesoftmaxfunctionandζ aprecisionparameter. Hence,whenprecisionishigh,theagentismostlikely
toengageinthepolicywiththelowestexpectedfreeenergy,whereasfor(very)lowprecisiontheagentwillrather
randomlyexplore. AsdiscussedinSection2,theexpectedfreeenergyG iscalculatedbytakingexpectationswith
π
respecttooutcomesinthefuture,insidethemodel’spredictions. Hence,theagentminimizestheexpectedfreeenergy
byevaluatingthepredictedoutcomesagainstthepreferreddistributionbeforedeployingactionsinreality.
Whereasthegenerativemodelistrainedtomatchtherealoutcomesoftheworldwithpastexperience,futureoutcomes
arenotyetavailabletotheagent. Asanactiveinferenceagentadoptspriorexpectationsofreachingpreferredoutcomes,
onecaninterpretthisashavingabiasedgenerativemodelofthefuturetowardsone’spreferences. Theself-evidencing
behaviorthatemergesisthatofa‘crookedscientist’[7],searchingactivestatesthatwillprovideevidenceforitsbiased
hypothesis.
Fromabiologicalperspective,wecouldassumethateveryagentpossessesauniquesetofpreferences,i.e.,tomaintain
homeostasis[135]. Thesepreferencescould,forinstance,associateinternalsignals,suchasbodytemperature,hunger,
happiness, and satisfaction, to the preferred states of the world. For artificial agents, defining the correct set of
preferencescaninsteadbeproblematic. Differentwaysofaddressingthisproblemarepresentedinthefirstsubsection.
We also analyze the problem of dealing with the agent’s uncertainty and how to learn and/or amortize the action
selectionprocess.
10
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
4.1 PreferencesModeling
AssummarizedinSection2,theexpectedfreeenergyobjectivecanbefactoredinseveralways,eachhighlighting
differentemergentpropertiesoftheagent’sbehavior(Equations(4)–(6)). Whilethisaspectofactiveinferencehasbeen
thetargetofcritics[136],thisallowsforgreaterflexibilityindesigningtheagentselectionprocess.
ObservationPreferences. Iftheagent’sobjectiveistomatchasetofpreferredoutcomes,thepreferreddistributionis
overtheenvironment’sobservationsp(o). Matchingoutcomescanbeseenasaformofgoal-directedbehavior,where
theagentplansitsactionstoachievecertainoutcomesfromtheenvironment. Goal-directedbehaviorhasbeenwidely
studiedinthecontextofRL,bothinlow-dimensional[137]andvisualdomains[138,139]. Preferencesdefinedinthe
observationspacecanbehandy,astheyjustrequireobservationsfrom“snapshots”oftheenvironmentinthecorrect
state. Nevertheless,artificialactiveinferenceimplementationshaverarelyusedthem,astheyaregenerallyhardto
matchinthehigh-dimensionalsettings. Strategiesthatovercomesuchlimitations[123]couldbethesubjectoffuture
studies.
InternalStatePreferences. Insteadofdefiningpreferencesinobservationspace,thesecouldbedirectlyinstantiatedin
theinternalstatespaceoftheagent. Thisformofstatematching[140]assumesthattheagentknowsboththepreferred
statesdistributionp(s)andthemodelinadvance,orastypicalinRL,thatsensorystatesareusedasinternalstates.
Alternatively,ifasetofpreferredoutcomesisavailable,preferredstatescanbeinferredfromthoseusinganinference
modelp(s|o). Thisapproachhasbeenappliedinroboticssimulatedandrealisticsetups[68,32].
RewardsasPreferences. Anotherwaytocircumventtheproblemofdefiningpreferencesistousearewardfunction
thatrepresentstheagent’sprobabilityofobservingthepreferredoutcomes. TheRLproblemcanbecastasprobabilistic
inference, by introducing an optimality variable O , which denotes whether the time step t is optimal [141]. The
t
distribution over the optimality variable is defined in terms of rewards as p(O = 1|s ,a ) = exp(r(s ,a )). As
t t t t t
discussedin[142],RLworksalikeactiveinferencebutitencodesutilityvalueintheoptimalitylikelihoodratherthan
inaprioroverobservations. Assuminglogp(o )=logp(O |s ,a ),theenvironmentrewardscanbeusedforactive
t t t t
inferenceaswell. Thispossibilityhasallowedsomeactiveinferencework[114,33]toreuserewardfunctionsfromRL
environments[22]. Concretely,itispossibletoconsiderrewardsasapartoftheobservableaspectsoftheenvironment,
anddefinetheirmaximumvaluesasthepreferredobservations[143]. Nonetheless,definingrewardfunctionsisalso
problematic[144]astheyarenotnaturallyavailable,andthissetupworkswellonlyforwell-engineeredenvironments.
LearnedPreferences. Finally,statepreferencescanalsobelearnedfrompreviousexperienceusingconjugatepriors
[91],orfromexpertdemonstrations[68]. InaRLcontext,demonstrationscanbeusedinaninversereinforcement
learningfashion[145,146],wherearewardsignalisinferredfromcorrectbehaviors,whichisthenoptimizedusing
RLtechniques.
4.2 Epistemics,Exploration,andAmbiguity
Whileactiveinferenceagentsseektorealizetheirpreferences,theyalsoaimtoreducetheuncertaintyoftheirmodel.
Forinstance,ifanagenthastomanipulatesomeobjectsinadarkroom,itwouldfirstsearchforthelightswitchto
increasetheconfidenceofitsmodelandreducetheresultingambiguityofitsactions. AsalsoshowninSection2,
thecausesof theagent’sambiguitycanbetwofold: ontheonehand, itcanbe duetotheincapabilityofinferring
itsstatewithcertainty,referringtouncertaintyinthestate-observationmapping,e.g.,likelihoodentropyormutual
information;ontheotherhand,theuncertaintycanbecausedbytheagent’slackofconfidencewithrespecttothe
model’sparameters. AsdepictedinEquation(4),anagent’sdriveforepistemicforagingiscausedbymaximizingtwo
informationgainterms: informationgainonmodelparametersandinformationgainonhiddenstates.
Parameter-drivenExploration. MaximizingmutualinformationinparameterspacehasbeenstudiedinRLasaway
toencourageexploration,computingtheinformationgaingivenbythedistributionoverparameterswithensembles
[147, 112, 113] or Bayesian neural networks [53]. In particular, in [113], they use the model to both evaluate the
states/actionstoexploreandtoplantheexploratorybehavior,whichisclosetowhatenvisionedinactiveinference.
Ensemblemethodshavealsobeenemployedinsomeactiveinferenceworks[114,129]alongwithdropout[33].
State-drivenExploration. Maximizingmutualinformationbetweenstatesandobservationshasalsobeenstudied
inRLforexploration,usingtheBayesiansurprisesignalgivenbytheD divergencebetweenthe(autoencoding)
KL
posteriorandthepriorofthemodelasareward[51]. Alternatively,thesurprisalwithrespecttofutureobservations
hasalsobeenusedinRLtogenerateanintrinsicmotivationsignalthatrewardsexploration[148,52,149]. Inactive
inference,themajorityofworkshaveinsteadfocusedonusingmultiplesamplesfromthelikelihoodmodel[32,33].
UncertaintyTradeoffs.Itisworthmentioningthat,duringdifferentstagesoftraining,uncertaintyrelatedtoparameters
anduncertaintyrelatedtosensory/internalstatesmayoverlap. Particularly,giventhatthedistributionsthatrepresentthe
agents’statesareinferredbyemployingthemodelparameters,uncertaintyinthemodelstronglyinfluencesuncertainty
11
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
withrespecttothestate. Thishighlightstheimportanceofconsideringbothkindsofuncertainty,especiallywhenthe
modelisimperfectoritslearningprocessisincomplete.
From an engineering perspective, having to deal with multiple signals, as in the active inference objective, poses
additionaloptimizationproblems. Differentpartsoftheobjectivesmayprovidevaluesondifferentscales,depending
onthedifferentmodels,distributionsandonthesensorydataprocessedbytheagent. IntheRLlandscape,howto
combinetheenvironments’rewardswithexplorationbonusestoobtainthebestperformanceisanongoingresearch
problem. While one might consider using the ‘vanilla’ objective, with no weighting of the different components,
weightingcouldleadtodifferentbehaviorsthatmightcomeinuseforpracticalpurposes,e.g.,reducingtheweighton
theexploration/ambiguitytermsmightleadtofasterconvergencewhenthereisnoambiguity/needtoexploreinthe
environment.
4.3 Plans,Habits,andSearchOptimization
From a computational point of view, the most complex aspect of minimizing the expected free energy consists of
howselectingtheactionsthatwillaccomplishtheagent’sbelief. Inpractice,optimizingG becomesatreesearch,
π
optionallypruningawayfromthesearchallpoliciesthatfalloutsideofanOccam’swindow,whicharethepoliciesthat
haveaverylowposteriorprobability. Nonetheless,dependingonthewaypoliciesaredefined,thesearchcanstillbe
significantlyexpensive,especiallyinhigh-dimensionalandcontinuousactionsdomains.
We distinguish three ways of establishing action selection, summarized in Figure5. The first is the typical active
inference’sdefinition, withthepolicybeingasequenceofactionsπ = {a ,a ,a ,...}, andwewillrefertothese
1 2 3
policiesasplansfordistinction(Figure5a).Eachplanisevaluatedbyitsexpectedfreeenergy,andthenextactionis
selectedfromthebestplanaccordingtoEquation(8). Thesecondwayofdefiningapolicyisbylearningastate-action
mappingπ(s ),whichisamortizingpolicyselectionbyfindinganoptimalhabitpolicythatoutputstheexpectedbest
t
actionforeachstate(Figure5b). ThisisalsothenotionofapolicythatisaccustomedinatypicalRLsetting. Finally,it
ispossibletocombinebothworldsbyfirstestimatingtheexpectedfreeenergyforagivenstateandaction,andthen
performingasearchoverthereducedsearchspace(Figure5c).
Plan-basedpolicies.Assumingacompletesearchoverallpotentialsequencesofactions,theplan-basedmethodshould
yieldtheoptimalpolicy. Unfortunately,inmostdomains,consideringallsequencesofactionsisanintractableproblem
andmoreengineeredrandomshootingmethodsareusedtosearchonlyoverthemostpromisingsequencesofactions,
suchas[55]. SimilarmethodshavebeenemployedbothforRL[49]andactiveinference[68,32]. Inparticular,when
thesearchoverpoliciestakesintoaccountrecursivebeliefsaboutthefuture,thisschemeisreferredtoassophisticated
inference[74]. Sophisticationdescribesthedegreetowhichanagenthasbeliefsaboutbeliefs. Asophisticatedagent,
whenevaluatingasequenceofactions,insteadofdirectlyconsideringthesequenceofoutcomes,recursivelyevaluates
outcomesintermsofthebeliefsitwouldhavewhenapplyingeachactionofthesequence.
HabitPolicies. Forhabitpolicies,weconsideraone-actionversionoftheexpectedfreeenergyG thatcanbeobtained
byconsideringone-actionplansπ =a foralltimesteps:
t
G =E [logq(s |s ,a )−logp(o ,s |s ,a )] (9)
at q(st+1,ot+1|st,at) t+1 t t t+1 t+1 t t
Astate-actionpolicyπ(s )canthenbetrainedtomaximizetheabovesignalovermultipletimesteps, astypically
t
performedinpolicygradientmethods[150,151]. Inordertoplanforlongerhorizons,deepRLmethodsadoptvalue
functionsthatallowestimatingtheexpectedsumofrewardsovertime,overapotentiallyinfinitehorizon. Forthese
long-term estimates, the value functions utilize a dynamic programming approach, where values are continuously
updatedbootstrappingcurrentestimateswithactualdata. Fromanactiveinferenceperspective,itisalsopossibleto
estimatetheexpectedfreeenergyforalongerhorizonbyapplyingdynamicprogramming,similarlytowhatwasstudied
in[126]. Theexpectedfreeenergycanthenberewrittenandoptimizedrecursivelyasfollows:
G (s ,a )=G(s ,a )+γE [G (s ,a )], (10)
π t t t t at+1∼π(st+1) π t+1 t+1
whereG representsanestimateoftheexpectedfreeenergyfollowingthepolicyπandtheexpectationoverπmeans
π
theactionsaresampledfromthestate-actionpolicydistribution. TheaboveequationresemblestheBellmanequation
knownfromRLwithgammabeingan(optional)discountfactorthatisusedtoavoidinfinitesum. Thisoptimization
schemeleadstoanhabitpolicythatcanachieveoptimalbehavior,whenthesourcesofuncertaintyoftheenvironment
arestationary;however,habituallearningcanbeinsufficientinrealisticscenarios,whererareandunexpectedevents
arecommon. Inthisactionselectionscheme,theprecisionparameterζ controlstheentropyofthestate-actionpolicy
distribution,similarlytomaximum-entropycontrolapproaches[151,152].
HybridSearchPolicies. Finally,hybridsearchschemes(c)combinetheuseofalearnedpriorwithcomputingthe
expectedfreeenergyforsequencesofactions. Thesearchspaceisgreatlylimitedbyusingtheprior,whichinfluences
12
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
(a) (b) (c)
…
Figure5: Differentapproachesforselectingactions. Bluecirclesrepresentthepathselectedbytheagent. (a)Deep
searchviaactionplans: thepathselectedhasthelowestfreeenergy. (b)Habitlearningviastate-actionpolicies: the
agentalwayssamplesfromthesameconditionaldistribution. (c)Treesearchguidedbyvalueandpolicy: theagent
selectstheactionsaccordingtothepriorandtheexpectedfreeenergy.
Table2: Implementationanddesignchoicesfortheactionselectionprocess,minimizingtheexpectedfreeenergy. The
tabledisplaysoneortwoexamplesforeachaspect–modalitypair,bothintheactiveinferenceandinthedeeplearning
(mainly,reinforcementlearning)literature,whenapplicable.*Allactiveinferencemethodsgenerallyconsiderhidden
statesexploration.
Modality ActiveInference DeepLearning
Observations [123] [138,139,137]
States [68] [140]
Preferences
Rewards [114,126,129,33] [67,66,87,155]
Learned [91] [145,146]
Hiddenstates * [51,52,149,148]
Exploration
Likelihoodparameters [33,114,91] [113,112]
Actionplans [68,32] [55]
Actionselection State-actionpolicy [126,123] [150,151]
Amortizedsearch [33,154] [29]
the choice of the nodes to select and expand. One of the most popular applications in RL of these methods is by
employingvariantsofMonteCarloTreeSearch(MCTS)[29,153],whichusebothaprioroveractionsandestimatesof
theexpectedutilityoverlonghorizons,asinEquation(10). Similarapproacheshaverecentlybeenappliedforactive
inference[33,154]. Whilethesemethodsaregenerallyapplicableonlyfordiscreteactionspaces,extensionsofMCTS
forcontinuousdomainshavebeendevelopedaswell[56]. Theprecisionparameterζ inthesemethodscanbeusedto
controltheinfluenceofthepriorrelativetotheexpectedfreeenergy(computedaposteriori,withrespecttoacertain
action/plan).
4.4 Summary
Similaraswhendesigningtheagent’smodel,thereareseveralaspectstoconsiderforimplementingactionselection
inanactiveinferencefashion. Inthissection,wecoveredanumberofimportantaspectsandprovidedreferencesto
existingimplementations,bothintheactiveinferenceandinthedeepreinforcementlearninglandscape. Asummaryof
thesemethodsispresentedinTable2.
Therearestillseveralchallengestoovercomeinadditiontothediscussionsofar. Forinstance,definingthepreferences
foranartificialagentisstillunresolvedformanypracticalapplications. Futureworkshouldalsoaddresshierarchical
implementationsforactionselection,toaccompanyhierarchicalmodels[133],allowingtoamortizeandabstractthe
actionselectionfurther. Anotherinterestinginterestingavenueconsistsofinvestigatingepisodiccontrol(currentlyless
13
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
studiedintheactiveinferenceliterature),sincethishasplayedanimportantroleforimprovingperformanceinRL
[156].
5 DiscussionandPerspectives
DevelopingArtificialIntelligenceisacomplexandintriguingproblem. Amongthecapabilitiesthatartificialintelligent
agents should possess, the ability to sense and to act consistently is crucial. Intelligent agents should be able to
exhibit their intelligence, manipulating the environment according to their will or purpose, and to understand the
consequencesoftheiractions,inordertoprovideaclosed-loopfeedbacktotheiractingsystemandtoacknowledgethe
accomplishmentoftheirdesires.
Active Inference is a neuro-inspired framework that encompasses both a perception process, through learning a
variationalworldmodel,andaBayesianactionselectionprocess,whichconsidersbothpreferencesatisfactionand
uncertainty in the environment and in the agent’s model. The variational inference optimization scheme and the
amortizationoftheBayesianactionselectionmaketheframeworkpromisingforpracticalimplementations,however
withoutscalablemodelsitisunfeasibletoapplyactiveinferenceincomplexscenarios,withcontinuousand/orhigh-
dimensionalstate/actionspaces. Weshowedhowactiveinferencecanbecombinedwithdeeplearningmodelsfor
functionapproximationtoprovideimplementationsthatscaletomorecomplexenvironments,withthepotentialof
applyingitinrealisticscenarios.
Oneoftheintentionsofthisworkistoprovideanintroductiontoactiveinferenceandguidelinesfordeeplearning
researcherstoeasilygetstartedintothefieldbyexploitingconceptsthatareincommonbetweenthetwoareas. At
thesametime,thisarticlecanbeusedasareferenceforscientistsintendingtoaddresssomeoftheissuesthathinder
artificialimplementationsoftheactiveinferenceframework.
We presented several design choices that need to be addressed to instantiate artificial active inference agents with
deep learning models, attempting to relate them to well-established studies in both fields. In particular, we found
thatsomeaspectsofactiveinferencearewellreflectedinsomeareasofdeeplearning,suchasunsupervisedlearning,
representation learning, and reinforcement learning, whose findings can be used to push the boundaries of active
inferencefurther. Inturn, activeinferenceprovidesaframeworkforperceptionandaction, fromwhichindividual
approachescouldgetinsightstobothexpandtheirscopeorunderstandtheimplicationsoftheirworkfromalarger
perspective.
References
[1] KarlJ.FristonandKlaasE.Stephan. Free-energyandthebrain. Synthese,159(3):417–458,Dec2007. ISSN
0039-7857.
[2] KarlFriston, ThomasFitzGerald, FrancescoRigoli, PhilippSchwartenbeck, JohnODoherty, andGiovanni
Pezzulo. Activeinferenceandlearning. Neuroscience&BiobehavioralReviews, 68:862–879, 2016. ISSN
0149-7634.
[3] Thomas Parr, Geraint Rees, and Karl J. Friston. Computational neuropsychology and bayesian inference.
FrontiersinHumanNeuroscience,12,2018. ISSN1662-5161.
[4] DaphneDemekas,ThomasParr,andKarlJ.Friston. Aninvestigationofthefreeenergyprincipleforemotion
recognition. FrontiersinComputationalNeuroscience,14,2020. ISSN1662-5188.
[5] Morten Henriksen. Variational free energy and economics optimizing with biases and bounded rationality.
FrontiersinPsychology,11,2020. ISSN1664-1078.
[6] Axel Constant, Maxwell J.D. Ramstead, Samuel P.L. Veissière, John O. Campbell, andKarl J. Friston. A
variationalapproachtonicheconstruction. JournalofTheRoyalSocietyInterface,15(141):20170685,2018.
[7] JelleBruineberg,ErikRietveld,ThomasParr,LeendertvanMaanen,andKarlJFriston.Free-energyminimization
injointagent-environmentsystems: Anicheconstructionperspective. JournalofTheoreticalBiology, 455:
161–178,2018. ISSN0022-5193.
[8] LaurentU.Perrinet,RickA.Adams,andKarlJ.Friston. Activeinference,eyemovementsandoculomotor
delays. BiologicalCybernetics,108(6):777–801,Dec2014. ISSN1432-0770.
[9] ThomasParrandKarlJ.Friston. Activeinferenceandtheanatomyofoculomotion. Neuropsychologia,111:
334–343,2018. ISSN0028-3932.
[10] HarrietBrown,KarlFriston,andSvenBestmann. Activeinference,attention,andmotorpreparation. Frontiers
inPsychology,2,2011. ISSN1664-1078.
14
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[11] ThomasParrandKarlJ.Friston. Workingmemory,attention,andsalienceinactiveinference. ScientificReports,
7(1):14678,Nov2017. ISSN2045-2322.
[12] M.BerkMirza,RickAAdams,ChristophDMathys,andKarlJ.Friston. Sceneconstruction,visualforaging,
andactiveinference. FrontiersinComputationalNeuroscience,10,2016.
[13] R.ConorHeins,M.BerkMirza,ThomasParr,KarlFriston,IgorKagan,andArezooPooresmaeili. Deepactive
inferenceandsceneconstruction. FrontiersinArtificialIntelligence,3:81,2020. ISSN2624-8212.
[14] MartinBiehl,FelixA.Pollock,andRyotaKanai. Atechnicalcritiqueofsomepartsofthefreeenergyprinciple.
Entropy(Basel,Switzerland),23(3):293,Feb2021. ISSN1099-4300.
[15] KarlJ.Friston,LancelotDaCosta,andThomasParr. Someinterestingobservationsonthefreeenergyprinciple.
Entropy,23(8):1076,Aug2021. ISSN1099-4300.
[16] KarlFriston. Lifeasweknowit. JournalofTheRoyalSocietyInterface,10(86):20130475,2013.
[17] MichaelKirchhoff,ThomasParr,EnsorPalacios,KarlFriston,andJulianKiverstein. Themarkovblanketsof
life: autonomy,activeinferenceandthefreeenergyprinciple. JournalofTheRoyalSocietyInterface,15(138):
20170792,2018.
[18] SergioRubin,ThomasParr,LancelotDaCosta,andKarlFriston. Futureclimates: Markovblanketsandactive
inferenceinthebiosphere. JournalofTheRoyalSocietyInterface,17(172):20200503,2020.
[19] HRMaturana,FJVarela,andHRMaturana. Autopoiesisandcognition: Therealizationoftheliving. D.Reidel
Pub.Co,Dordrecht,Holland,1980.
[20] MichaelD.Kirchhoff.Autopoiesis,freeenergy,andthelife–mindcontinuitythesis.Synthese,195(6):2519–2540,
Jun2018. ISSN1573-0964.
[21] DavidM.Blei,AlpKucukelbir,andJonD.McAuliffe. Variationalinference: Areviewforstatisticians. Journal
oftheAmericanStatisticalAssociation,112(518):859–877,Apr2017. ISSN1537-274X.
[22] RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
[23] RoyAWise. Dopamine,learningandmotivation. Naturereviewsneuroscience,5(6):483–494,2004.
[24] PaulW.Glimcher. Understandingdopamineandreinforcementlearning: Thedopaminerewardpredictionerror
hypothesis. ProceedingsoftheNationalAcademyofSciences,108(Supplement3):15647–15654,2011. ISSN
0027-8424. doi:10.1073/pnas.1014269108.
[25] DavidSilver,SatinderSingh,DoinaPrecup,andRichardS.Sutton. Rewardisenough. ArtificialIntelligence,
299:103535,2021. ISSN0004-3702.
[26] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AlexGraves,IoannisAntonoglou,DaanWierstra,and
MartinRiedmiller. Playingatariwithdeepreinforcementlearning,2013.
[27] AdriàPuigdomènechBadia,BilalPiot,StevenKapturowski,PabloSprechmann,AlexVitvitskyi,DanielGuo,
andCharlesBlundell. Agent57: Outperformingtheatarihumanbenchmark,2020.
[28] OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaelMathieu,AndrewDudzik,JunyoungChung,
DavidH.Choi,RichardPowell,TimoEwalds,PetkoGeorgiev,JunhyukOh,DanHorgan,ManuelKroiss,Ivo
Danihelka,AjaHuang,LaurentSifre,TrevorCai,JohnP.Agapiou,MaxJaderberg,AlexanderS.Vezhnevets,
RemiLeblond,TobiasPohlen,ValentinDalibard,DavidBudden,YurySulsky,JamesMolloy,TomL.Paine,
CaglarGulcehre,ZiyuWang,TobiasPfaff,YuhuaiWu,RomanRing,DaniYogatama,DarioWunsch,Katrina
McKinney,OliverSmith,TomSchaul,TimothyLillicrap,KorayKavukcuoglu,DemisHassabis,ChrisApps,and
DavidSilver. Grandmasterlevelinstarcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):
350–354,Nov2019. ISSN1476-4687.
[29] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,
ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,andetal. Masteringatari,go,chessandshogi
byplanningwithalearnedmodel. Nature,588(7839):604–609,Dec2020. ISSN1476-4687.
[30] OpenAI,IlgeAkkaya,MarcinAndrychowicz,MaciekChociej,MateuszLitwin,BobMcGrew,ArthurPetron,
AlexPaino,MatthiasPlappert,GlennPowell,RaphaelRibas,JonasSchneider,NikolasTezak,JerryTworek,
PeterWelinder,LilianWeng,QimingYuan,WojciechZaremba,andLeiZhang. Solvingrubik’scubewitha
robothand,2019.
[31] KaiUeltzhöffer. Deepactiveinference. BiologicalCybernetics,112(6):547–573,Oct2018. ISSN1432-0770.
[32] OzanÇatal,TimVerbelen,JohannesNauta,CedricDeBoom,andBartDhoedt.Learningperceptionandplanning
withdeepactiveinference. InICASSP2020-2020IEEEInternationalConferenceonAcoustics,Speechand
SignalProcessing(ICASSP),pages3952–3956,2020.
15
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[33] ZafeiriosFountas,NoorSajid,PedroMediano,andKarlFriston. Deepactiveinferenceagentsusingmonte-carlo
methods. InH.Larochelle, M.Ranzato, R.Hadsell, M.F.Balcan, andH.Lin, editors, AdvancesinNeural
InformationProcessingSystems,volume33,pages11662–11675.CurranAssociates,Inc.,2020.
[34] ChristopherL.Buckley,ChangSubKim,SimonMcGregor,andAnilK.Seth. Thefreeenergyprinciplefor
actionandperception: Amathematicalreview. JournalofMathematicalPsychology,81:55–79,2017. ISSN
0022-2496.
[35] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,andKarlFriston. Active
inferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology,99:102447,2020. ISSN
0022-2496.
[36] PabloLanillos,CristianMeo,CorradoPezzato,AjithAnilMeera,MohamedBaioumy,WataruOhata,Alexander
Tschantz,BerenMillidge,MartijnWisse,ChristopherL.Buckley,andJunTani. Activeinferenceinroboticsand
artificialagents: Surveyandchallenges,2021.
[37] SamuelGershmanandNoahGoodman. Amortizedinferenceinprobabilisticreasoning. InProceedingsofthe
annualmeetingofthecognitivesciencesociety,volume36,2014.
[38] AliRazavi,AaronvandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimageswithvq-vae-2,
2019.
[39] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Alias-freegenerativeadversarialnetworks,2021.
[40] ArashVahdatandJanKautz. Nvae: Adeephierarchicalvariationalautoencoder,2021.
[41] Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent highway
networks,2017.
[42] GáborMelis,TomášKocˇiský,andPhilBlunsom. Mogrifierlstm,2020.
[43] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,Christopher
Berner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shot
learners,2020.
[44] SHIXingjian,ZhourongChen,HaoWang,Dit-YanYeung,Wai-KinWong,andWang-chunWoo. Convolutional
lstmnetwork: Amachinelearningapproachforprecipitationnowcasting. InAdvancesinneuralinformation
processingsystems,pages802–810,2015.
[45] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Recurrent neural
networksforpredictivelearningusingspatiotemporallstms. InI.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume30.CurranAssociates,Inc.,2017.
[46] EmilyDentonandRobFergus. Stochasticvideogenerationwithalearnedprior,2018.
[47] WilliamLotter,GabrielKreiman,andDavidCox. Deeppredictivecodingnetworksforvideopredictionand
unsupervisedlearning,2017.
[48] LarsBuesing,TheophaneWeber,SebastienRacaniere,S.M.AliEslami,DaniloRezende,DavidP.Reichert,
FabioViola,FredericBesse,KarolGregor,DemisHassabis,andDaanWierstra. Learningandqueryingfast
generativemodelsforreinforcementlearning,2018.
[49] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,andJamesDavidson.
Learninglatentdynamicsforplanningfrompixels. InKamalikaChaudhuriandRuslanSalakhutdinov,editors,
Proceedingsofthe36thInternationalConferenceonMachineLearning,volume97ofProceedingsofMachine
LearningResearch,pages2555–2565.PMLR,09–15Jun2019.
[50] DavidHaandJürgenSchmidhuber. Recurrentworldmodelsfacilitatepolicyevolution,2018.
[51] PietroMazzaglia,OzanCatal,TimVerbelen,andBartDhoedt. Self-supervisedexplorationvialatentbayesian
surprise,2021.
[52] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-
supervisedprediction,2017.
[53] ReinHouthooft,XiChen,YanDuan,JohnSchulman,FilipDeTurck,andPieterAbbeel. Vime: Variational
informationmaximizingexploration. InProceedingsofthe30thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’16,page1117–1125,2016.
16
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[54] OzanÇatal,SamLeroux,CedricDeBoom,TimVerbelen,andBartDhoedt. Anomalydetectionforautonomous
guidedvehiclesusingbayesiansurprise. In2020IEEE/RSJInternationalConferenceonIntelligentRobotsand
Systems(IROS),pages8148–8153,2020.
[55] NikolausHansen. Thecmaevolutionstrategy: Atutorial,2016.
[56] ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MohammadaminBarekatain,SimonSchmitt,and
DavidSilver. Learningandplanningincomplexactionspaces,2021.
[57] HermannVonHelmholtz. HandbuchderphysiologischenOptik: mit213indenTexteingedrucktenHolzschnitten
und11Tafeln,volume9. Voss,1867.
[58] K.Friston. Thefree-energyprinciple: aroughguidetothebrain? TrendsCognSci,13(7):293–301,Jul2009.
[59] MaxwellJDRamstead,MichaelDKirchhoff,andKarlJFriston. Ataleoftwodensities: activeinferenceis
enactiveinference. AdaptiveBehavior,28(4):225–239,2020.
[60] KarlJ.Friston,ThomasParr,andBertdeVries. Thegraphicalbrain: Beliefpropagationandactiveinference.
Networkneuroscience(Cambridge,Mass.),1(4):381–414,2017. ISSN2472-1751.
[61] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLOSONE,
4(7):1–13,072009.
[62] FristonKarl. Afreeenergyprincipleforbiologicalsystems. Entropy(Basel,Switzerland),14(11):2100–2121,
Nov2012. ISSN1099-4300.
[63] PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,MartinKronbichler,and
KarlJFriston. Computationalmechanismsofcuriosityandgoal-directedexploration. eLife,8:e41703,may
2019. ISSN2050-084X.
[64] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka.
Activeinference,curiosityandinsight. NeuralComputation,29(10):2633–2683,2017.
[65] K.Friston,F.Rigoli,D.Ognibene,C.Mathys,T.Fitzgerald,andG.Pezzulo. Activeinferenceandepistemic
value. CognNeurosci,6(4):187–214,2015.
[66] DanijarHafner,TimothyLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwithdiscreteworld
models,2021.
[67] DanijarHafner,TimothyP.Lillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol:Learningbehaviors
bylatentimagination. InICLR,2020.
[68] OzanÇatal,JohannesNauta,TimVerbelen,PieterSimoens,andBartDhoedt. Bayesianpolicyselectionusing
activeinference,2019.
[69] OzanÇatal,TimVerbelen,ToonVandeMaele,BartDhoedt,andAdamSafron. Robotnavigationashierarchical
activeinference. NeuralNetworks,142:192–204,2021. ISSN0893-6080.
[70] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes,2014.
[71] DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inferenceindeepgenerativemodels. InProceedingsofthe31stInternationalConferenceonMachineLearning
(ICML),volume32,pages1278–1286,2014.
[72] NaftaliTishby,FernandoC.Pereira,andWilliamBialek. Theinformationbottleneckmethod,2000.
[73] AlexanderA.Alemi,IanFischer,JoshuaV.Dillon,andKevinMurphy. Deepvariationalinformationbottleneck,
2019.
[74] KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. Sophisticatedinference. Neural
Computation,33(3):713–763,032021. ISSN0899-7667.
[75] KurtHornik. Approximationcapabilitiesofmultilayerfeedforwardnetworks. NeuralNetworks,4(2):251–257,
1991. ISSN0893-6080.
[76] EricHeiden,DavidMillard,ErwinCoumans,YizhouSheng,andGauravS.Sukhatme. Neuralsim: Augmenting
differentiablesimulatorswithneuralnetworks,2021.
[77] C.DanielFreeman,ErikFrey,AntonRaichuk,SertanGirgin,IgorMordatch,andOlivierBachem. Brax–a
differentiablephysicsengineforlargescalerigidbodysimulation,2021.
[78] WilliamS.Lovejoy. Asurveyofalgorithmicmethodsforpartiallyobservedmarkovdecisionprocesses. Annals
ofOperationsResearch,28(1):47–65,Dec1991. ISSN1572-9338.
[79] N.Roy,G.Gordon,andS.Thrun. Findingapproximatepomdpsolutionsthroughbeliefcompression. Journalof
ArtificialIntelligenceResearch,23:1–40,Jan2005. ISSN1076-9757.
17
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[80] HannaKurniawati,DavidHsu,andWeeSunLee.Sarsop:Efficientpoint-basedpomdpplanningbyapproximating
optimallyreachablebeliefspaces. InRobotics: Scienceandsystems,volume2008.Citeseer,2008.
[81] NicolasHeess,JonathanJHunt,TimothyPLillicrap,andDavidSilver. Memory-basedcontrolwithrecurrent
neuralnetworks,2015.
[82] DavidE.Rumelhart,GeoffreyE.Hinton,andRonaldJ.Williams. Learningrepresentationsbyback-propagating
errors. Nature,323(6088):533–536,Oct1986. ISSN1476-4687.
[83] YoshuaBengio,NicholasLéonard,andAaronCourville. Estimatingorpropagatinggradientsthroughstochastic
neuronsforconditionalcomputation,2013.
[84] PeterWGlynn. Likeliloodratiogradientestimation: anoverview. InProceedingsofthe19thconferenceon
Wintersimulation,pages366–375,1987.
[85] RonaldJ.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.
Mach.Learn.,8(3–4):229–256,1992. ISSN0885-6125.
[86] ToonVandeMaele,TimVerbelen,OzanÇatal,CedricDeBoom,andBartDhoedt. Activevisionforrobot
manipulatorsusingthefreeenergyprinciple. FrontiersinNeurorobotics,15:14,2021. ISSN1662-5218.
[87] Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep
reinforcementlearningwithalatentvariablemodel,2020.
[88] MaximilianIgl,LuisaZintgraf,TuanAnhLe,FrankWood,andShimonWhiteson.Deepvariationalreinforcement
learningforpomdps,2018.
[89] JasonTylerRolfe. Discretevariationalautoencoders. arXivpreprintarXiv:1609.02200,2016.
[90] Sherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, Aäron van den Oord, and Oriol Vinyals. Vector
quantizedmodelsforplanning,2021.
[91] NoorSajid,PanagiotisTigas,AlexeyZakharov,ZafeiriosFountas,andKarlFriston. Explorationandpreference
satisfactiontrade-offinreward-freelearning,2021.
[92] IulianVladSerban,AlexanderG.Ororbia,JoellePineau,andAaronCourville. Piecewiselatentvariablesfor
neuralvariationaltextprocessing. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages422–432,Copenhagen,Denmark,2017.AssociationforComputationalLinguistics.
[93] DaniloJimenezRezendeandShakirMohamed. Variationalinferencewithnormalizingflows,2016.
[94] TimSalimans,DiederikP.Kingma,andMaxWelling. Markovchainmontecarloandvariationalinference:
Bridgingthegap,2015.
[95] YannLeCun,LeonBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningappliedtodocument
recognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
[96] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeilHoulsby. An
imageisworth16x16words: Transformersforimagerecognitionatscale,2021.
[97] FrankRosenblatt. Theperceptron: aprobabilisticmodelforinformationstorageandorganizationinthebrain.
Psychologicalreview,65(6):386,1958.
[98] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini. Thegraph
neuralnetworkmodel. IEEETransactionsonNeuralNetworks,20(1):61–80,2009.
[99] SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. NeuralComputation,9(8):1735–1780,
1997.
[100] JunyoungChung,CaglarGulcehre,KyunghyunCho,andYoshuaBengio.Empiricalevaluationofgatedrecurrent
neuralnetworksonsequencemodeling. InNIPS2014WorkshoponDeepLearning,December2014,2014.
[101] PeterToth,DaniloJimenezRezende,AndrewJaegle,SébastienRacanière,AleksandarBotev,andIrinaHiggins.
Hamiltoniangenerativenetworks,2020.
[102] CansuSancaktar,MarcelA.J.vanGerven,andPabloLanillos. End-to-endpixel-baseddeepactiveinferencefor
bodyperceptionandaction. 2020JointIEEE10thInternationalConferenceonDevelopmentandLearningand
EpigeneticRobotics(ICDL-EpiRob),Oct2020.
[103] ParthaGhosh,MehdiS.M.Sajjadi,AntonioVergari,MichaelBlack,andBernhardSchölkopf. Fromvariational
todeterministicautoencoders,2020.
18
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[104] KarlFriston,PhilippSchwartenbeck,ThomasFitzgerald,MichaelMoutoussis,TimBehrens,andRaymond
Dolan. Theanatomyofchoice: activeinferenceandagency. FrontiersinHumanNeuroscience,7:598,2013.
ISSN1662-5161.
[105] ThomasParr,DavidA.Benrimoh,PeterVincent,andKarlJ.Friston. Precisionandfalseperceptualinference.
FrontiersinIntegrativeNeuroscience,12:39,2018. ISSN1662-5145.
[106] ThomasParrandKarlJ.Friston. Uncertainty,epistemicsandactiveinference. JournalofTheRoyalSociety
Interface,14(136):20170376,2017.
[107] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherP.Burgess,XavierGlorot,MatthewM.Botvinick,Shakir
Mohamed,andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational
framework. InICLR,2017.
[108] AliRazavi,AäronvandenOord,BenPoole,andOriolVinyals. Preventingposteriorcollapsewithdelta-vaes,
2019.
[109] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
networks,2015.
[110] YarinGalandZoubinGhahramani. Dropoutasabayesianapproximation: Representingmodeluncertaintyin
deeplearning,2016.
[111] BalajiLakshminarayanan,AlexanderPritzel,andCharlesBlundell. Simpleandscalablepredictiveuncertainty
estimationusingdeepensembles,2017.
[112] DeepakPathak,DhirajGandhi,andAbhinavGupta. Self-supervisedexplorationviadisagreement,2019.
[113] RamananSekar,OlehRybkin,KostasDaniilidis,PieterAbbeel,DanijarHafner,andDeepakPathak. Planningto
exploreviaself-supervisedworldmodels. InICML,2020.
[114] AlexanderTschantz,BerenMillidge,AnilK.Seth,andChristopherL.Buckley. Reinforcementlearningthrough
activeinference,2020.
[115] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictivecoding,
2019.
[116] MathildeCaron,IshanMisra,JulienMairal,PriyaGoyal,PiotrBojanowski,andArmandJoulin. Unsupervised
learningofvisualfeaturesbycontrastingclusterassignments,2021.
[117] Jean-BastienGrill,FlorianStrub,FlorentAltché,CorentinTallec,PierreH.Richemond,ElenaBuchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu,RémiMunos,andMichalValko. Bootstrapyourownlatent: Anewapproachtoself-supervised
learning,2020.
[118] XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning,2020.
[119] JureZbontar,LiJing,IshanMisra,YannLeCun,andStéphaneDeny. Barlowtwins: Self-supervisedlearningvia
redundancyreduction,2021.
[120] ZhiChen,YijieBei,andCynthiaRudin. Conceptwhiteningforinterpretableimagerecognition. NatureMachine
Intelligence,2(12):772–782,Dec2020. ISSN2522-5839.
[121] MaxSchwarzer,AnkeshAnand,RishabGoel,RDevonHjelm,AaronCourville,andPhilipBachman. Data-
efficientreinforcementlearningwithself-predictiverepresentations,2021.
[122] XiaoMa,SiweiChen,DavidHsu,andWeeSunLee. Contrastivevariationalmodel-basedreinforcementlearning
forcomplexobservations. InProceedingsofthe4thConferenceonRobotLearning,2020.
[123] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive active inference. In Advances in Neural
InformationProcessingSystems,2021.
[124] OzanÇatal,SamuelWauthier,CedricDeBoom,TimVerbelen,andBartDhoedt. Learninggenerativestatespace
modelsforactiveinference. FrontiersinComputationalNeuroscience,14:103,2020. ISSN1662-5188.
[125] KarlJ.Friston,RichardRosch,ThomasParr,CathyPrice,andHowardBowman. Deeptemporalmodelsand
activeinference. Neuroscience&BiobehavioralReviews,77:388–402,2017. ISSN0149-7634.
[126] BerenMillidge. Deepactiveinferenceasvariationalpolicygradients,2019.
[127] VaibhavSaxena,JimmyBa,andDanijarHafner. Clockworkvariationalautoencoders,2021.
[128] BohanWu,SurajNair,RobertoMartin-Martin,LiFei-Fei,andChelseaFinn. Greedyhierarchicalvariational
autoencodersforlarge-scalevideoprediction,2021.
19
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[129] AlexanderTschantz,ManuelBaltieri,Anil.K.Seth,andChristopherL.Buckley. Scalingactiveinference. In
2020InternationalJointConferenceonNeuralNetworks(IJCNN),pages1–8,2020.
[130] LukaszKaiser,MohammadBabaeizadeh,PiotrMilos,BlazejOsinski,RoyHCampbell,KonradCzechowski,
Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George
Tucker,andHenrykMichalewski. Model-basedreinforcementlearningforatari,2020.
[131] Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations for
reinforcementlearning,2020.
[132] GiovanniPezzulo,FrancescoRigoli,andKarlJ.Friston. Hierarchicalactiveinference: Atheoryofmotivated
control. TrendsinCognitiveSciences,22(4):294–306,2018. ISSN1364-6613.
[133] AlexeyZakharov,QinghaiGuo,andZafeiriosFountas. Variationalpredictiveroutingwithnestedsubjective
timescales,2021.
[134] SamuelT.Wauthier,OzanÇatal,CedricDeBoom,TimVerbelen,andBartDhoedt. Sleep: Modelreductionin
deepactiveinference. InTimVerbelen,PabloLanillos,ChristopherL.Buckley,andCedricDeBoom,editors,
ActiveInference,pages72–83,Cham,2020.SpringerInternationalPublishing. ISBN978-3-030-64919-7.
[135] GiovanniPezzulo,FrancescoRigoli,andKarlFriston. Activeinference,homeostaticregulationandadaptive
behaviouralcontrol. ProgressinNeurobiology,134:17–35,2015.
[136] BerenMillidge,AlexanderTschantz,andChristopherLBuckley. Whencetheexpectedfreeenergy?,2020.
[137] MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,PeterWelinder,BobMcGrew,
JoshTobin,OpenAIPieterAbbeel,andWojciechZaremba. Hindsightexperiencereplay. InI.Guyon,U.V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
InformationProcessingSystems,volume30.CurranAssociates,Inc.,2017.
[138] DavidWarde-Farley,TomVandeWiele,TejasD.Kulkarni,CatalinIonescu,StevenHansen,andVolodymyr
Mnih. Unsupervisedcontrolthroughnon-parametricdiscriminativerewards. In7thInternationalConferenceon
LearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net,2019.
[139] Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and
achievinggoalsviaworldmodels,2021.
[140] LisaLee,BenjaminEysenbach,EmilioParisotto,EricXing,SergeyLevine,andRuslanSalakhutdinov. Efficient
explorationviastatemarginalmatching,2020.
[141] SergeyLevine. Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview,2018.
[142] BerenMillidge,AlexanderTschantz,AnilKSeth,andChristopherLBuckley. Ontherelationshipbetween
activeinferenceandcontrolasinference,2020.
[143] NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. Activeinference:Demystifiedandcompared. Neural
Computation,33(3):674–712,Mar2021. ISSN1530-888X.
[144] JackClarkandDarioAmodei. Faultyrewardfunctionsinthewild,2016.
[145] BrianDZiebart,AndrewLMaas,JAndrewBagnell,AnindKDey,etal.Maximumentropyinversereinforcement
learning. InAaai,volume8,pages1433–1438.Chicago,IL,USA,2008.
[146] PieterAbbeelandAndrewY.Ng. Apprenticeshiplearningviainversereinforcementlearning. InProceedingsof
theTwenty-FirstInternationalConferenceonMachineLearning,ICML’04,page1,NewYork,NY,USA,2004.
AssociationforComputingMachinery. ISBN1581138385.
[147] PranavShyam,WojciechJas´kowski,andFaustinoGomez. Model-basedactiveexploration,2019.
[148] JoshuaAchiamandShankarSastry. Surprise-basedintrinsicmotivationfordeepreinforcementlearning,2017.
[149] YuriBurda,HarrisonEdwards,DeepakPathak,AmosJ.Storkey,TrevorDarrell,andAlexeiA.Efros.Large-scale
studyofcuriosity-drivenlearning. In7thInternationalConferenceonLearningRepresentations,ICLR,2019.
[150] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicyoptimization
algorithms,2017.
[151] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors,
Proceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachine
LearningResearch,pages1861–1870.PMLR,10–15Jul2018.
[152] BenjaminEysenbachandSergeyLevine. Maximumentropyrl(provably)solvessomerobustrlproblems,2021.
20
TheFreeEnergyPrincipleforPerceptionandAction: ADeepLearningPerspective
[153] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,TimothyLillicrap,KarenSimonyan,andDemis
Hassabis. Masteringchessandshogibyself-playwithageneralreinforcementlearningalgorithm,2017.
[154] DomenicoMaisto,FrancescoGregoretti,KarlFriston,andGiovanniPezzulo. Activetreesearchinlargepomdps,
2021.
[155] IgnasiClavera,VioletFu,andPieterAbbeel. Model-augmentedactor-critic: Backpropagatingthroughpaths,
2020.
[156] FabioPardo,ArashTavakoli,VitalyLevdik,andPetarKormushev. Timelimitsinreinforcementlearning. In
JenniferDyandAndreasKrause,editors,Proceedingsofthe35thInternationalConferenceonMachineLearning,
volume80ofProceedingsofMachineLearningResearch,pages4045–4054.PMLR,10–15Jul2018.
21