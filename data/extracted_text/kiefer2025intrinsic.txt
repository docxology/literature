Article
Intrinsic motivation as constrained entropy
maximization
Alex B. Kiefer 1,2*
1 VERSES, Los Angeles, California, USA
2 Monash Centre for Consciousness and Contemplative Studies, Monash University, Melbourne, VIC 3800, Australia
* Correspondence: alex.kiefer@monash.edu
Abstract: “Intrinsic motivation” refers to the capacity for intelligent systems to be motivated endoge-
nously, i.e. by features of agential architecture itself rather than by learned associations between action
and reward. This paper views active inference, empowerment, and other formal accounts of intrinsic
motivation as variations on the theme of constrained maximum entropy inference, providing a general
perspective on intrinsic motivation complementary to existing frameworks. The connection between
free energy and empowerment noted in previous literature is further explored, and it is argued that
the maximum-occupancy approach in practice incorporates an implicit model-evidence constraint.
Keywords: intrinsic motivation; active inference; empowerment; entropy
1. Introduction
In psychology, “intrinsic motivation” refers to the tendency for intelligent creatures to be moti-
vated to do certain things (such as explore, learn, and grow) even in the absence of specific external
reward signals [1]. This paradigm has increasingly gained traction in machine learning, where it is
operationalized as the idea that policies for action may be optimized based on structural features
of agents and agent-environment interactions, as against traditional approaches like reinforcement
learning, which optimize policies based on ad hocreward functions.
An early, and increasingly influential, formal account of intrinsic motivation is based on empower-
ment, defined as the capacity of the channel linking agents’ actions (actuator states) to sensory feedback
(observations) [2,3]. One interpretation of this objective is that empowered agents “keep their options
open”, as wide action-conditioned channel capacity entails that agents are able to realize a variety of
states (for which observations are a proxy).
The active inferenceframework [4] shares similar motivations, and provides a Bayesian method
for combining a general form of intrinsic motivation (i.e. curiosity or “epistemic drive”) [ 5] with
agent-specific prior distributions over states or outcomes [ 6], which model homeostatic set points
and can function like explicit rewards. The expected (variational) free energy (EFE), as discussed
below, guides policy selection in this framework by supplying an empirical prior over policies, given
observations.
More recently, the objective of maximum path occupancyhas been proposed as a framework for
intrinsic motivation [7]. On this account, agents are motivated to maximize future action-state path
occupancy, which can be measured in terms of both the entropy of the action distribution and the
entropy of the ensuing state distribution, given an initial state. This somewhat more radical perspective
explicitly inverts the perhaps natural assumption that drives for exploration and curiosity have evolved
as a means to achieving reward, and in effect views rewarding states as instrumentally valuable in
enabling future exploration, i.e. avoiding absorbing states that afford little or no action variability (e.g.
death).
There are many other formal treatments of intrinsic motivation in the literature on machine
learning, some closely related to those just discussed, such as pioneering work on artificial curiosity
(see e.g. [8,9]) and treatments in terms of Bayesian surprise [10,11]. Here, the focus is mainly on the
arXiv:2502.02962v3  [q-bio.NC]  13 Feb 2025
2 of 13
relationship between active inference and empowerment, and on the relationship of both to maximum
occupancy which has recently been proposed explicitly as an alternative.
While [12] conducts a comparative empirical study of these three frameworks for intrinsic moti-
vation on a toy problem and [13] considers how active inference may be formally related to broader
schemes for intrinsic motivation, comparatively little work exists on the formal and conceptual rela-
tions among these frameworks. Here, I highlight the fact that all three can be understood as variations
on the theme of constrained entropy maximization, a principle with deep connections to the free
energy principle and active inference [14]. I explore the connection between empowerment and active
inference [15] by casting the empowerment objective itself explicitly as a form of variational inference.
I also argue that the ability of occupancy-maximizing agents to exhibit apparently goal-directed be-
havior depends on a “survival instinct” or model-evidence constraint implicit in the factorization of
the overall system into actions and states. These considerations frame entropy maximization, under
local constraints, as the kernel of intelligence and agency, with particular facets of this process such as
empowerment, perception, curiosity and the “will to live” as corollaries.
The first section below unpacks the three frameworks for intrinsic motivation mentioned above
(empowerment, active inference, and maximum occupancy) in some detail, both formally and in terms
of conceptual motivation, and articulates their ties to constrained entropy maximization. Section two
looks closely at some connections among these theories, then distills a few general conclusions.
2. Three formal accounts of intrinsic motivation
2.1. Empowerment
The empowerment objective for intrinsic motivation, originally proposed in [2], is defined as the
capacity of the information channel linking an agent’s actions to its observation of the effects of those
actions. That is, given a space of possible observations OT at future timestep T and a sequence of
actions At:T from the present timestep t to the future, there is some distribution P(OT|At:T) capturing
the probabilistic dependence of the future observation on the actions taken, and the empowerment
E of an agent is measured as the capacity C of the information transmission channel defined by this
distribution:
Et = C

P(OT|At:T)

= max
P(A)
I(At:T; OT)
In this case, C is defined as the maximum mutual information between actions and future observations,
I(At:T;OT ), when the conditional distribution P(OT|At:T) is held fixed and the distribution over actions,
P(A), is allowed to vary.
It is worth taking a moment to unpack this, as a detailed understanding will be useful for
comparisons below. The mutual information is standardly defined, for two random variables X and Y,
as a double sum equivalent to the KL divergence from the joint density P(X, Y) to the product of the
marginals over X and Y:
I = ∑
y∈Y
∑
x∈X
P(x, y) log
 P(x, y)
P(x)P(y)

= DKL

P(X, Y)||P(X)P(Y)

Intuitively, this expression measures how different the actual joint distribution is from what it
would be were the two variables independent, i.e. how much information the variables carry about
one another. While this measure is symmetric (i.e. the same for X and Y), it can be broken down in
terms of conditional probabilities in either direction. Since the joint density can be factorized into a
prior and a conditional density, i.e. P(X, Y) =P(X)P(Y|X) =P(Y)P(X|Y), the mutual information
3 of 13
can also be expressed as an expected KL divergence from a conditional density P(Y|X) to the marginal
over Y:
I = ∑
y∈Y
∑
x∈X
P(x)P(y|x) log
P(x)P(y|x)
P(x)P(y)

Factorize joint distribution
= ∑
x∈X
P(x)
"
∑
y∈Y
P(y|x) log
P(y|x)
P(y)
#
Cancel out P(x)s and rearrange
= EP(X)
"
DKL

P(Y|X)||P(Y)
#
Given a fixed P(Y|X) (channel), the channel capacityC
h
P(Y|X)
i
is then the maximum value this mutual
information can take, given a free choice of P(X).
The empowerment objective is just this channel capacity, with respect to the channel linking
actions over timesteps t . . .T with observations at T.1 Intuitively, the mutual information term (i.e.
information gain expected under the action distribution) measures both the controllability of outcomes
(the influence of action selection on such outcomes) and the variety of achievable outcomes (i.e.
“keeping one’s options open”) [2]. This combination of controllability and variety is characteristic of
constrained entropy maximization, a common theme in many frameworks for intrinsic motivation
[7,16,17], and is related to Ashby’s “law of requisite variety” [18].
The “variety” aspect of empowerment can be made more explicit by considering the relation of
mutual information to entropy. Any mutual information I(X; Y) can be expressed in terms of entropy
in several ways:
I(X; Y) =H(X) − H(X|Y)
= H(Y) − H(Y|X)
= H(X) +H(Y) − H(X, Y)
Thus empowerment can be seen as maximizing the entropy of the action distribution H
 
P(A)

while
ensuring that actions are “rational” in the sense of being reliably related to observations, i.e. minimizing
H
 
P(A|O)

. At the same time, it can be viewed as maximizing the variety of observations O while
ensuring that they remain controllable, i.e. minimizing H
 
P(O|A)

.2
This objective may be read as a signal guiding model evolution or selection, as in the work just
cited (i.e. choosing a generative model of actions and outcomes P(A)P(O|A)). Given a fixed model,
agents may also choose policies (actions) so as to maximize the time-dependent empowerment Et
by seeking the position in the state-space of the overall system (where external states are implicitly
represented here by observations) in which the channel capacity is highest, since P(OT|At:T) implicitly
depends on the states at t...T.
Before moving on to consider other treatments of intrinsic motivation, we note that in [16] it is
shown (in the setting of continuous state-spaces) that generalizing the empowerment objective just
discussed, by varying the length of action and observation sequences and the time interval separating
actions from target observations, allows one to recover various extant descriptions of control in
dynamical systems. Saliently for present purposes, a generalized empowerment objective in which
1 For simplicity the discussion here focuses on the original formulation in [2], but obviously many variations on this theme are
possible, e.g. using different time indices (as explored in [16]) or swapping out observations for latent states.
2 There is a prima facieconflict here with the reasonable objective of maximizing model evidence (i.e. minimizing the surprisal
of observations), as in active inference. This matter is discussed further below, but note that in active inference treatments
the model evidence (marginal likelihood) is usually treated as fixed at the timescale of inference, and agents simply act so as
to furnish evidence for this model.
4 of 13
actions are taken only at the first time-step corresponds to a “kicked” (controlled) version of Causal
Entropic Forcing [17], a more general framework that models intelligent behavior in terms of entropy
maximization.
2.2. Active inference and expected free energy
Among the most promising approaches to intrinsic motivation are those that leverage Bayesian
Advances in cognitive (neuro)science over the past decade or so have seen the rise to prominence
of the idea that most (if not all) intelligent action can be understood in terms of Bayesian inference
[19]. This paradigm encompasses quite specific models of neuronal information processing such as
predictive coding [20,21], which has been invoked to explain perceptual inference [ 22], as well as
more abstract and general frameworks, mosty saliently the free energy principle [23,24], an account of
self-organization in terms of variational Bayesian inference, and active inference [4,25], which derives
a scheme for action (i.e. planning or policy selection) from the assumption that agents select actions
that are expected to minimize variational free energy in the future.3
Agents governed by active inference implement a specific form of planning as inference [ 27],
“reasoning backward” from preferred outcomes (cast in this context as observations that furnish
evidence for a prior generative model [28]) to the policies most likely to bring them about. In brief, this
involves inferring a (variational) posterior distribution Q(π) over policies π in which the probability
assigned to each policy is proportional to its associated model evidence. Actions are then sampled
at each timestep based on a Bayesian model average of the policies, each of which entails distinct
action-conditioned state transition probabilities.
The core quantity driving policy selection in the active inference framework is the expected
free energy (EFE, denoted G in equations), which is the cumulative variational free energy that the
agent expects to be incurred by choosing a policy (action sequence), given its generative model. The
generative model that figures in policy inference includes a state-independent distribution P(o) over
outcomes (observations o) that the agent “prefers” to see, which can be cast as the marginal likelihood
of observations [6] and models the characteristic attracting set of states that homeostatic systems must
remain within in order to persist [23]. This can be thought of as a kind of intrinsic motivation, since
it is “built in” to the agent rather than learned, though in practice (i.e. in computational models) it
functions similarly to an ad hocreward function. Crucially, however, the EFE also implements the
model-independent inductive bias that actions will minimize variational free energy in the future, and
thus subserves a more general form of intrinsic motivation.
The EFE associated with a policy Gπ is defined as the expectation of the variational free energy,
given the state-transition probabilities induced by following that policy [5]. This depends on possible
future observations, which are assumed to be generated independently by states at each timestep, so
that Gπ can be computed as a sum over timestep-specific terms Gt
π. Selection of actions or control
states u can then be summarized as follows, where P(ut = ui|πj) is 1 if policy j begins with control
state ui and 0 otherwise, and Fπ is the variational free energy (VFE) incurred by policy π:4
3 Accounts of motor control in terms of high-precision kinesthetic predictions [26] are closely related to active inference, but
here the latter term is reserved to denote the idea that policies for action are selected on the basis of expected (variational)
free energy.
4 I omit several features of active inference models that are inessential for present purposes, such as the baseline policy
or “habit” prior and the temperature parameter used in action selection. Please see [ 4,25,29] for further details. In some
treatments, F is omitted as well.
5 of 13
ut ∼ Q(ut) =∑
π
P(ut|π)Q(π) Control state sampled from marginal
Q(π) =σ

− G − F

Posterior over policies
Fπ =
T
∑
t=0
EQ(st|π)

− log P(st, ot|π)

| {z }
Energy
− H

Q(st|π)

| {z }
Entropy
VFE of policy π
Gπ =
T
∑
t=0
Gt
π EFE of policy π
Gt
π = EQ(st,ot|π)
h
log Q(st|π) − log P(st, ot|π)
i
≈ EQ(st,ot|π)
h
log Q(st|π) − log Q(st|ot, π) − log P(ot)
i
= −EQ(st,ot|π)
h
− log P(ot)
i
| {z }
Expected utility
− DKL
h
Q(st|ot, π)||Q(st|π)
i
| {z }
Information gain
Given that the EFE is defined as the VFE expected under various policies, it seems at first glance
that considering both F and G when computing Q(π) involves double-counting. The crucial difference
is that the EFE is used to compute a belief P(π) =σ
 
− G

about policies, which is used as a prior in
the full variational policy inference scheme [29]:
Q∗(π) =argmin
Q(π)
F
F =
"
DKL

P(π)||Q(π)

+ EQ(π)
h
Fπ
i#
where F is the total variational free energy and Q∗(π) is the optimal variational posterior over policies.
The role of the EFE as a prior is underwritten by two differences with respect to the VFE. First, as
discussed in [30], the variational distribution over states in the EFE, Q(st|π), is a variational empirical
prior—computed by conditioning on the most recently inferred state distribution Q(st) and rolling the
generative model out into the future—as opposed to the variational posterior Q(st) over states, which
inverts the likelihood while incorporating the prior over states. The appearance of Q(st|π) in the EFE
(together with the use of Q(st|ot, π) to approximate P(st|ot)) underwrites the EFE’s information gain
term.
A second difference is that the marginal over observations P(ot), rather than the likelihood
P(ot|st), appears in the EFE, in accord with the EFE’s role as a prior belief about which policies should
be (i.e. will be, in a planning-as-inference scheme) pursued.5 The per-policy variational free energy
F, on the other hand, takes into consideration the entropy of the posterior state distribution as well
as the expected energy of observations under the likelihood, in such a way that the entropy of the
posterior is maximized under the energy (model evidence) constraint, in accordance with the principle
of constrained maximum-entropy inference [14,23,31,32].
5 Importantly, while some formulations (e.g. [ 30]) as well as many simulations employ a “preference distribution” ˜P(ot)
specified independently of the predictive (generative) model of the world, this is not a deep feature of active inference. In [6]
for example the EFE objective is formulated solely in terms of the difference between P(o, s) and P(o, s|a), i.e. the “reward“
or preference model is the same generative model used for prediction, with actions marginalized out.
6 of 13
2.3. Maximum occupancy
The Maximum Occupancy Principle (MOP) [ 7] carries the theme of intrinsic motivation to its
logical conclusion, proposing that a traditional picture of rational agency, in which curiosity and other
intrinsic drives have evolved in order to serve reward maximization, should be inverted: we can
instead understand rewarding states as a means to the end of continuing to live, i.e. to explore (thus
maximally occupy) action-state path space.
Formally, the occupancy objective is defined in terms of a state-conditioned policy distribution
π(A|S) and transition dynamics P(S′|S, A), which can be alternately sampled from to generate action-
state paths τ. The reward function R(τ) for a given trajectory is then specified as:
R(τ) =−
∞
∑
t=0
γt log
h
πα(at|st)Pβ(st+1|st, at)
i
where γt is the standard temporal reward discount in reinforcement learning and α and β are weights
modulating the influence of action and state path occupancy. Agents select policies so as to maximize
the expected reward or “value” of states s, Vπ(s):
Vπ(s) =Eπ(A|S)P(S′|S,A)
h
R(τ)|s0 = s
i
= Eπ(A|S)P(S′|S,A)
"
∞
∑
t=0
γt

αH(A|st) +βH(S′|st, at)

|s0 = s
#
Here, the realization of τ depends on the initial state s, and H(A|st) and H(S′|st, at) denote the
conditional entropy of the action distribution given the current state, and of the distribution of the next
state given the current state and action, respectively.6 Thus, agents that maximize Vπ(s) maximize an
expectation over the (summed step-wise conditional) entropy of both action and state paths, subject to
the weights and initial condition.7
In [12], empirical studies are presented in which MOP agents aggressively explore state and action
space while still exhibiting apparently goal-directed behavior. The former is perhaps to be expected,
given the purely intrinsic, surprisal-maximizing reward function, thanks to which agents will directly
seek out improbable actions that lead to improbable states. Presumably, the ability of MOP agents
to behave in goal-oriented ways despite the absence of explicit tasks, rewards, or even preference
distributions, is underwritten by the imperative to maximize longer-term path occupancy, 8 which
balances the tendency to greedily maximize entropy at each timestep. This implicit constraint on
short-term entropy maximization in the service of increasing entropy in the long run is evocative of
the argument in [34] according to which the structured, relatively low-entropy states characteristic of
complex forms of life are favored for their ability to accelerate the dissipation of free energy within the
broader universe.
3. A unified view of intrinsic motivation
This section begins by analyzing the relationship between active inference and empowerment,
then considers the maximum-occupancy perspective in relation to both of these. It then concludes
with a discussion of some themes common across these frameworks, and a synthesis that allows us to
resolve some apparent dichotomies from a multi-scale or scale-free perspective.
6 Note that, while the expectation over conditioned variables is absorbed in the entropy terms in the third line, the expectation
over the conditioning variables st and at remains.
7 Negative weights can give rise to entropy-minimizing behavior as well, as discussed below.
8 Notably, similar emergent task-oriented behavior is demonstrated in agents governed by an empowerment objective in [33].
7 of 13
3.1. Empowerment and active inference
Maximizing the empowerment objective is closely related to minimizing expected free energy.
Most straightforwardly, in the absence of a constraint (expected utility term), the expected free
energy described above reduces to the negative information gain DKL

Q(st|ot, π)||Q(st|π)

, so that
minimizing EFE maximizes the mutual information between states and observations [35].9
While the original empowerment objective [ 2] leaves the mediation of the action-sensation
channel P(OT|At:T) by hidden states implicit, the active inference objective simply makes this explicit:
in choosing actions, agents effectively choose the transition dynamics for controllable states (in typical
implementations, discrete actions index slices of transition tensors), such that they are rendered
informative about observations. Thus effectively, states are (probabilistically) chosen so as to maximize
the mutual information between actions and observations, as in the empowerment objective.
In [15] (Appendix), it is claimed that “empowerment is a special case of active inference, when we
can ignore risk (i.e., when all policies are equally risky)”. Here, risk is a term occurring in the following
alternative breakdown of the EFE (see [25], Appendix A for a derivation):
Gt
π = DKL

Q(ot|π)||P(ot)

| {z }
Risk
+ EQ(st|π)
h
H(P(ot|st))
i
| {z }
Ambiguity
Intuitively, risk is simply a measure of expected negative reward, which in this context is how
different predicted outcomes are from those expected a priori(i.e. preferred). The entropy of the
likelihood mapping from states to observations expected under a given policy (“Ambiguity”) quantifies
how uncertain the agent will be about outcomes if that policy is pursued. Thus, minimizing expected
free energy encourages agents to choose policies (actions) that render outcomes predictable, subject to
the constraint that risk is minimized.
We can run a similar argument by considering the empowerment objective described in [2] as part
of a variational inference process. In terms of the notation used for active inference, the goal would be
to maximize It(π; oT), where as above π is a sequence of control states [u0, u1, . . ., uT]. This objective
can be expressed in terms of the entropies of posterior observation and policy distributions, and also
as a KL divergence:
It(π; oT) =H

Q(π)

− H

Q(oT|π)

= DKL

Q(π, oT)||Q(π)Q(oT)

The divergence simply states that agents maximizing empowerment should select policies that
provide information about the target observation, which in this context amounts to the former affording
control over the latter. The subscript in It indicates that, like the original empowerment objective
Et, this term is implicitly time-dependent. More specifically, in the present setting, the variational
posteriors Q at t depend on the observation ot via the state posterior Q(st).
9 Again, even though this mutual information can be interpreted as maximizing the entropy of observations (constrained by
their controllability), optimization of Q in variational inference is always constrained by the generative model P so there
is no conflict with the imperative to maximize model evidence. That said, in a multi-scale setting one may also consider
learning the parameters of P as discussed below.
8 of 13
Interestingly, defining a conditional “energy” term as the negative log probability of the observa-
tion at T given policies, the expression of the mutual information in terms of entropies can be written
in a form analogous to a free energy Ft(π, oT) simply by flipping the sign and rearranging terms:
Ft(π, oT) =−
h
H

Q(π)

− H

Q(oT|π)
i
= EQ(oT |π)
h
− log Q(oT|π)
i
| {z }
“Energy”
− H

Q(π)

| {z }
Entropy
= EQ(oT |π)
h
− log P(oT|sT)
i
| {z }
Energy of final observation
+ H

Q(sT|π)

| {z }
Conditional state entropy
− H

Q(π)

| {z }
Policy entropy
= EQ(sT |π)
h
H

P(oT|sT)
i
| {z }
Ambiguity
− It(π; sT)| {z }
State information gain
Q(sT|π) = ∑
s0∈S
· · ·∑
sT−1∈S
h T−1
∏
t=0
P(st+1|st, π)Q(s0)
i
Maximizing It(π; oT) is then equivalent to minimizing this energy. The second line lacks the form
of a proper (variational) free energy because the “energy” term is just the entropy of a variational
density Q(oT|π), rather than a joint probability (generative model) P(o, s). However, Q(oT|π) factors
into several terms some of which are distributions of the generative model. Taking this into account,
we arrive at the expression in the penultimate row, which is similar to a Helmholtz free energy with an
additional entropy term to be minimized: under this objective, agents will seek low-energy (predictable)
observations, while maximizing the entropy of policies (“keeping options open”) and also seeking
policies that minimize the entropy of the final state, i.e. seeking paths that result in controllable states.
Finally (last line), the expected energy (negative log probability under the generative model) of oT
is equivalent to the ambiguity term in the EFE mentioned above (with respect to the final observation
in a trajectory), while the two entropy terms can be combined into a state information gain term. 10
Thus from the empowerment objective alone (and ignoring additional “preference” constraints) we can
derive drives for both epistemic value(minimizing ambiguity) and control (maximizing state infogain).
Active inference agents are thus “empowered” in that they maximize the entropy of future state
distributions, under the constraint that these states or the ensuing observations be controllable. Cru-
cially, in active inference, agents are also constrained to maximize model evidence(or its tractable lower
bound, variational free energy) [28]. In fact, the latter (approximately maximizing model evidence) is
the central concept in the FEP and active inference, where (constrained) entropy maximization falls
out of variational free energy minimization, and specifically exploratory behavior emerges thanks to
the distribution-matching (KL-divergence) term in the EFE objective [37].
3.2. Constrained maximum occupancy
Prima facie, it is difficult to square the maximum occupancy objective with those just considered
in precise terms, since its objective involves only maximizing (expected) entropy, without constraints.
In fact, the MOP objective described above is general enough to encode an approximation to empow-
erment, if the β term is set to a negative value [ 7], which encourages agents to choose actions that
minimize the entropy of the state transition distribution, while still maximizing the entropy of actions.
This is clearly closely related to the empowerment objectives discussed above once the distinction
between states and observations is accommodated (i.e., it results in agents that “keep options open”
while ensuring controllable states and thus observations). However, while of practical interest, this
really amounts to a departure from the spirit of MOP .
10 A similar formulation of empowerment in terms of free energy is reached by considering action-state empowerment in the
context of the generalized free energy functional [36], in [15].
9 of 13
It is argued in [7] on both conceptual and experimental grounds that MOP agents exhibit more
robust exploratory behavior, and variety in policy selection, than agents governed by empowerment or
EFE objectives. The experiments reported in that work and in [12], however, involve full observation
of the state-space, so that the ambiguity term in the EFE does no work (and more generally, the usual
motivations for the FEP and active inference, in which agents are assumed to infer unknown states of
the environment, do not apply). Moreover, the experiments reported in [7] use a setting of β = 0 by
default, thus effectively maximizing the entropy of only the action distribution. For these reasons the
ensuing discussion focuses on the conceptual arguments surrounding entropy maximization and the
role of constraints, rather than on these experimental results.
On conceptual grounds, the MOP objective may (it is argued in [ 7]) be expected to produce a
greater variety of actions than active inference for two reasons: (a) the EFE objective contains an
explicit “preference” term which MOP lacks, and which biases action in favor of certain outcomes (thus
reducing the entropy of action-state paths); and (b) while the EFE objective maximizes the entropy of
the state-transition distribution at each timestep11, it contains no term to maximize the entropy of the
action distribution.
The maximization of action (policy) entropy does seem to fall out of the empowerment framework.
Thus, given the equivalences outlined above, the same should be true of active inference. [7] argue that
the EFE deterministically selects a single policy. However, in the context of a full variational inference
treatment (i.e. planning-as-inference), the entropy of the policy distribution should also be maximized
(under relevant constraints).
Conceptually, π is a latent variable, and ceteris paribusits entropy should be maximized during
variational inference just as the entropy of Q(s), the variational density over hidden causes, is max-
imized. This is captured formally in work on active inference exploring a formulation of expected
variational free energy that is in some ways more parsimonious than the EFE, called the generalized
free energy[36]. As shown in [ 15], this objective can (as is usual in variational inference) be written
as a Helmholtz free energy, where in this case the energy term is the expected EFE under the policy
posterior, and the entropy of the policy distribution is explicitly maximized as free energy is minimized:
F
h
Q(s, π)
i
| {z }
Generalized free energy
= EQ(π)
h
Gπ
i
| {z }
Expected EFE
− H

Q(π)

| {z }
Policy entropy
It is the generalized free energy which in [15] is shown to be equivalent to empowerment con-
strained by risk. Relatedly, the “free energy of empowerment” Ft(π, oT) defined above also contains
this policy entropy term. Thus, while a focus exclusively on the EFE is not sufficient to show this, the
main difference between active inference (viewed broadly so as to include maximum-entropy policy
inference) and MOP seems to be the presence or absence of explicit model evidence constraints.
The core concept in MOP is that maximizing path occupancy is an “intrinsic” value, from which
reward is derivative. The core claim of the FEP and active inference (which we have seen to entail
empowerment) is that maximizing model evidenceis an “intrinsic” value, and that rewards as well
as information-seeking behavior derive from this imperative. At first glance, these frameworks may
appear difficult to reconcile, since the former maximizes surprisal while the latter minimizes it (at least
with respect to sensory observations).
11 Minimizing the variational free energy F in the full variational policy inference scheme maximizes the entropy of the state
distribution at each step under an evidence constraint. The EFE at time t can be written as a Helmholtz free energy, showing
that the same is true for the policy-conditioned empirical prior over states Q(st|π):
Gt
π = EQ(st,ot|π)
h
− log P(st, ot|π)
i
| {z }
Energy
− H

Q(st|π)

| {z }
Entropy
10 of 13
One of the central claims of [7] is that intelligent, goal-directed action emerges naturally from the
MOP objective, in the presence of absorbing states together with the means to (foreseeably) avoid them
given certain courses of action. It may be wondered whether pure MOP agents would be as successful
in less predictable environments in which risk-aversion may be more important, but independently of
this, there are deep reasons to suppose that MOP agents would not produce richly intelligent behavior
without an implicit model-evidence constraint.
Occupancy-maximizing agents seek control only in order to remain alive, a goal that is argued
to flow elegantly from the desire to maximize entropy in the distant future. However, this argument
assumes that being dead corresponds to an “absorbing” state, which in the experiments is modeled
as entailing zero entropy for the rest of time. In a more physically realistic model, dying would
correspond to a breakdown of the agent-environment boundary, and so to a much higher-entropy
state (with the dissolution of individual agents corresponding to an unconstrained maximum-entropy
state, or in physical terms, thermal equilibrium). Relatedly, the “survival instinct” is encoded in active
inference agents in the fact that departures from homeostatic set points (defined by the generative
model or “preference distribution”) score high in free energy and so are aversive.
Thus, identifying a lack of action availability with a low-entropy state is plausible only in toy
scenarios in which the entropy increase induced within the overall system by the dissolution of the
agent is ignored. Death ought to be attractive to MOP agents unless they possess an a prioridistinction
between agent and environment, i.e. a “sense of self”. The upshot is that the implicit constraint
enabling the emergence of goal-directed behavior in such agents is, in the general case, not simply
long-term entropy maximization but also the existence of an agent with a repertoire of actions, encoded
in the very partitioning of the space into action and state variables. Effectively, this amounts to a
version of the “controllability” constraints that appear explicitly in active inference and empowerment,
as the agent must exert control sufficient to enable homeostasis (i.e. the maintenance of internal states
against dissipative forces).12
3.3. Model evidence and the will to live
Despite the arguments just given, the inversion of traditional assumptions about the relationship
between exploration and reward highlighted by the MOP is appealing, as entropy maximization (albeit
under constraints) appears to be an essential feature of intelligence and life [14,17,38,39], more constant
across distinct forms of life than any particular reward-seeking behavior. The idea that future path
occupancy, as measured by entropy [7], is tantamount to remaining alive is one way of understanding
the place of entropy maximization at the heart of accounts of intrinsic motivation.
We have seen however that in order to reproduce the goal-oriented behavior characteristic of
complex biological intelligence, it is necessary to maximize entropy under the constraint that the
agent’s existence, operationalized as a conditional independence between internal and external states
[23] (which appears in simple models as an action-state partition), is maintained. Taking a page from
Schopenhauer [40], intrinsic motivation may then be cast as simply the “will to live”, i.e. to persist
as a living (moving, changing) thing, a basal impulse that takes different particular forms depending
on local constraints (generative models). These constraints shape the primary motivational force of
entropy production, such that conditional independence structures are maintained.
In simpler models of intelligence, the relevant partitioning of the entire (agent-environment)
system is assumed to be fixed, but in more sophisticated treatments such as multi-scale or scale-free
active inference [41,42], model structure itself may evolve, typically at slower timescales. We may
then view the life of an agent at any given instant as seeking not only observational evidence for
the currently parameterized model, but also evidence for the parameters themselves, as well as for
12 We may note that the ability to predict the entropy of future states so as to compute state value—however this is implemented—
also corresponds to some local disequilibrium and thus imposes a de factoconstraint on entropy (here, we are explicitly
considering the entropy associated with the internal states of the agent).
11 of 13
hyperparameters (or priors over parameters, including structural priors). This structural evolution can
be understood in terms of Bayesian model selection [35].
From this perspective, there is no deep contradiction between scale-free self-evidencing (i.e. the
seeking of model evidence) [28] and maximum occupancy. Once constraints (parameters and model
structure) are themselves treated as random variables, the process of self-evidencing is seen to be data-
or observation-driven through and through, and it appears to be a property of our universe (insofar as
it is accurately modeled as a closed system) that the entropy of data-generating processes as a whole
can only increase. From this perspective, maximum-entropy inference is a ubiquitous self-fulfilling
prophecy in virtue of which the universe evolves toward thermal equilibrium.13 Thus all agents indeed
maximize occupancy on the longest timescale, though in a rather selfless way, i.e. they gather evidence
for a maximum-entropy model of the universe at large, in which boundaries between agents (Markov
blankets) and their corresponding energetic constraints have disappeared.
The idea that entropy is maximized “for its own sake” does not, of course, preclude interpretations
of this phenomenon in terms of epistemic value [5], curiosity [9], and so on, in various contexts. What
the preceding discussion does suggest is that exploratory behavior is by no means “merely” an evolved
mechanism for securing outcomes high in utility, but is at least as fundamental an aspect of agency as
the latter tendency, with the two plausibly participating in a dance of circular causality. The presence
of both goal-seeking and information-seeking drives in the expected free energy functional, regardless
of the particular generative model, points to this same conclusion [44].
4. Conclusion
Seeking common themes across contemporary accounts of intrinsic motivation has surfaced
the inevitability of constrained entropy maximization as a core principle describing motivation in
biological systems. This insight is hardly novel at a fundamental level, as entropy maximization has
long been recognized as a crucial principle both in physics generally [31] and for the physics of life
and intelligence specifically [17,23,34], and has played an explicit role in several accounts of intrinsic
motivation [9,16]. Here, the goal has been primarily to explore in detail how three accounts of intrinsic
motivation that have previously been juxtaposed in the literature [12] may nonetheless be understood
as variants of this general perspective.
Acknowledgments: The author would like to thank in particular Karl Friston, Jacqueline Hynes, and Dalton
Sakthivadivel for conversations directly relevant to this work, as well as Mahault Albarracin, Riddhi J. Pitliya,
Maxwell Ramstead, Tim Verbelen, and Ran Wei for inspiring discussions.
References
1. Domenico, S.I.D.; Ryan, R.M. The Emerging Neuroscience of Intrinsic Motivation: A New Frontier in
Self-Determination Research. Frontiers in Human Neuroscience2017, 11.
2. Klyubin, A.S.; Polani, D.; Nehaniv, C.L. Empowerment: a universal agent-centric measure of control. 2005
IEEE Congress on Evolutionary Computation2005, 1, 128–135 Vol.1.
3. Salge, C.; Glackin, C.; Polani, D. Empowerment - an Introduction. ArXiv 2013, abs/1310.1863.
4. Friston, K.J.; FitzGerald, T.H.B.; Rigoli, F.; Schwartenbeck, P .; Pezzulo, G. Active Inference: A Process Theory.
Neural Computation2017, 29, 1–49.
5. Friston, K.J.; Rigoli, F.; Ognibene, D.; Mathys, C.D.; FitzGerald, T.H.B.; Pezzulo, G. Active inference and
epistemic value. Cognitive Neuroscience2015, 6, 187 – 214.
6. Da Costa , L.; Tenka, S.; Zhao, D.; Sajid, N. Active Inference as a Model of Agency, 2024,
[arXiv:cs.AI/2401.12917].
7. Ramirez-Ruiz, J.; Grytskyy, D.; Mastrogiuseppe, C.; Habib, Y.; Moreno-Bote, R. Complex behavior from
intrinsic motivation to occupy future action-state path space. Nature Communications2022, 15.
8. Schmidhuber, J. Adaptive confidence and adaptive curiosity. Forschungsberichte, TU Munich1991, FKI 149
91, 1–9.
13 The distinction between thermodynamic and merely information-theoretic or variational free energy [32,43] needn’t concern
us here, as the entropy of observations is sufficient to drive this process.
12 of 13
9. Schmidhuber, J. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010). IEEE Transactions
on Autonomous Mental Development2010, 2, 230–247.
10. Itti, L.; Baldi, P . Bayesian surprise attracts human attention. Vision Research2009, 49, 1295–1306. Visual
Attention: Psychophysics, electrophysiology and neuroimaging, https://doi.org/https://doi.org/10.1016/j.
visres.2008.09.007.
11. Mazzaglia, P .; Çatal, O.; Verbelen, T.; Dhoedt, B. Curiosity-Driven Exploration via Latent Bayesian Surprise.
In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
12. Moreno-Bote, R.; Ramírez-Ruiz, J. Empowerment, Free Energy Principle and Maximum Occupancy Principle
Compared. In Proceedings of the NeurIPS 2023 workshop: Information-Theoretic Principles in Cognitive
Systems, 2023.
13. Biehl, M.; Guckelsberger, C.; Salge, C.; Smith, S.C.; Polani, D. Expanding the Active Inference Landscape:
More Intrinsic Motivations in the Perception-Action Loop. Frontiers in Neurorobotics2018, 12.
14. Sakthivadivel, D.A.R. Towards a Geometry and Analysis for Bayesian Mechanics, 2022, [arXiv:math-
ph/2204.11900].
15. Friston, K.; Da Costa, L.; Hafner, D.; Hesp, C.; Parr, T. Sophisticated Inference. Neural Computation2021,
33, 713–763.
16. Tiomkin, S.; Nemenman, I.; Polani, D.; Tishby, N. Intrinsic Motivation in Dynamical Control Systems. PRX
Life 2024, 2, 033009.
17. Wissner-Gross, A.D.; Freer, C.E. Causal Entropic Forces. Phys. Rev. Lett. 2013, 110, 168702. https:
//doi.org/10.1103/PhysRevLett.110.168702.
18. Ashby, W.R., Requisite Variety and Its Implications for the Control of Complex Systems. In Facets of Systems
Science; Springer US: Boston, MA, 1991; pp. 405–417. https://doi.org/10.1007/978-1-4899-0718-9_28.
19. Hohwy, J. The Predictive Mind; Oxford University Press UK: Oxford, GB, 2013.
20. Rao, R.P .N.; Ballard, D.H. Predictive coding in the visual cortex: a functional interpretation of some
extra-classical receptive-field effects. Nature Neuroscience1999, 2, 79–87.
21. Salvatori, T.; Song, Y.; Yordanov, Y.; Millidge, B.; Sha, L.; Emde, C.; Xu, Z.; Bogacz, R.; Lukasiewicz, T. A
Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks. In Proceedings of
the The Twelfth International Conference on Learning Representations, 2024.
22. Helmholtz, H.v.; Southall, J.P .C.J.P .C.Helmholtz’s Treatise on physiological optics; Dover Publications: New
York, 1962.
23. Friston, K.J. A free energy principle for a particular physics. arXiv: Neurons and Cognition2019.
24. Friston, K.; Da Costa, L.; Sakthivadivel, D.A.; Heins, C.; Pavliotis, G.A.; Ramstead, M.; Parr, T. Path integrals,
particular kinds, and strange things. Physics of Life Reviews2023, 47, 35–62.
25. Smith, R.; Friston, K.J.; Whyte, C.J. A step-by-step tutorial on active inference and its application to empirical
data. Journal of Mathematical Psychology2022, 107, 102632. https://doi.org/https://doi.org/10.1016/j.jmp.
2021.102632.
26. Brown, H.R.; Friston, K.J.; Bestmann, S. Active Inference, Attention, and Motor Preparation. Frontiers in
Psychology 2011, 2.
27. Botvinick, M.; Toussaint, M. Planning as inference. Trends in Cognitive Sciences2012, 16, 485–488. https:
//doi.org/https://doi.org/10.1016/j.tics.2012.08.006.
28. Hohwy, J. The Self-Evidencing Brain. Noûs 2014, 50, 259–285. https://doi.org/10.1111/nous.12062.
29. Heins, C.; Millidge, B.; Demekas, D.; Klein, B.; Friston, K.; Couzin, I.D.; Tschantz, A. pymdp: A Python
library for active inference in discrete state spaces. Journal of Open Source Software2022, 7, 4098. https:
//doi.org/10.21105/joss.04098.
30. Millidge, B.; Tschantz, A.; Buckley, C.L. Whence the Expected Free Energy? Neural Computation2021,
33, 447–482. https://doi.org/10.1162/neco_a_01354.
31. Jaynes, E.T. Information Theory and Statistical Mechanics. Phys. Rev.1957, 106, 620–630.
32. Kiefer, A. Psychophysical Identity and Free Energy. Journal of the Royal Society Interface2020, 17.
33. Ringstrom, T.J. Reward is not Necessary: How to Create a Modular & Compositional Self-Preserving Agent
for Life-Long Learning, 2023, [arXiv:cs.AI/2211.10851].
34. Ueltzhöffer, K. On the thermodynamics of prediction under dissipative adaptation. arXiv: Neurons and
Cognition 2020.
35. Friston, K.J.; Da Costa, L.; Tschantz, A.; Kiefer, A.; Salvatori, T.; Neacsu, V .; Koudahl, M.; Heins, C.;
Sajid, N.; Markovic, D.; et al. Supervised structure learning. Biological Psychology 2024, 193, 108891.
https://doi.org/https://doi.org/10.1016/j.biopsycho.2024.108891.
13 of 13
36. Parr, T.; Friston, K.J. Generalised free energy and active inference. Biological Cybernetics2018, 113, 495 – 513.
37. Millidge, B.; Tschantz, A.; Seth, A.K.; Buckley, C.L. Understanding the origin of information-seeking
exploration in probabilistic objectives for control. ArXiv 2021, abs/2103.06859.
38. England, J.L. Statistical physics of self-replication. The Journal of Chemical Physics2013, 139, 121923.
39. Costa, L.D. Probabilistic Principles for Biophysics and Neuroscience: Entropy Production, Bayesian Mechan-
ics & the Free-Energy Principle, 2024, [arXiv:math-ph/2410.11735].
40. Schopenhauer, A.; Payne, E.F.J. The World as Will and Representation; Dover Publications: New York„ 1958.
41. Hesp, C.; Ramstead, M.; Constant, A.; Badcock, P .; Kirchhoff, M.; Friston, K. A Multi-scale View of the
Emergent Complexity of Life: A Free-Energy Proposal. In Proceedings of the Evolution, Development and
Complexity; Georgiev, G.Y.; Smart, J.M.; Flores Martinez, C.L.; Price, M.E., Eds., Cham, 2019; pp. 195–227.
42. Friston, K.; Heins, C.; Verbelen, T.; Costa, L.D.; Salvatori, T.; Markovic, D.; Tschantz, A.; Koudahl, M.;
Buckley, C.; Parr, T. From pixels to planning: scale-free active inference, 2024, [arXiv:cs.LG/2407.20292].
43. Fields, C.; Goldstein, A.; Sandved-Smith, L. Making the Thermodynamic Cost of Active Inference Explicit.
Entropy 2024, 26. https://doi.org/10.3390/e26080622.
44. Smith, R.; Ramstead, M.J.D.; Kiefer, A. Active Inference Models Do Not Contradict Folk Psychology.Synthese
2022, 200, 1–37. https://doi.org/10.1007/s11229-022-03480-w.