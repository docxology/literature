Self-Evidencing Through Hierarchical Gradient Decomposition:
A Dissipative System That Maintains Non-Equilibrium
Steady-State by Minimizing Variational Free Energy
Michael James McCulloch
michael.james.mcculloch@gmail.com
Code available at:https://doi.org/10.5281/zenodo.17363831
Abstract
The Free Energy Principle (FEP) states that self-
organizing systems must minimize variational free en-
ergy to persist (Friston, 2010, 2019), but the path
from principle to implementable algorithm has re-
mained unclear. We present a constructive proof that
the FEP can be realized through exact local credit
assignment. The system decomposes gradient com-
putation hierarchically: spatial credit via feedback
alignment, temporal credit via eligibility traces, and
structural credit via a Trophic Field Map (TFM) that
estimates expected gradient magnitude for each con-
nection block. We prove these mechanisms are ex-
act at their respective levels and validate the central
claim empirically: the TFM achieves 0.9693 Pear-
son correlation with oracle gradients. This exactness
produces emergent capabilities including 98.6% re-
tention after task interference, autonomous recovery
from 75% structural damage, self-organized critical-
ity (spectral radiusρ≈1.0), and sample-efficient
reinforcement learning on continuous control tasks
without replay buffers. The architecture unifies Pri-
gogine’s dissipative structures (Prigogine, 1977), Fris-
ton’s free energy minimization (Friston, 2010), and
Hopfield’s attractor dynamics (Hopfield, 1982; Amit
et al., 1985a,b), demonstrating that exact hierarchi-
cal inference over network topology can be imple-
mented with local, biologically plausible rules.
1 Introduction
Life exists far from thermodynamic equilibrium.
From single cells to brains, biological systems main-
tain their structural integrity and functional or-
ganization by continuously dissipating energy and
entropy into their environment (Prigogine, 1977;
Schr¨ odinger, 1944). What separates living systems
from inert matter is this capacity to resist the slide
toward maximum entropy, which enables memory,
adaptation, and intelligence.
The Physical Principle of Self-Organization.
Prigogine’s theory of dissipative structures (Pri-
gogine, 1977; Nicolis and Prigogine, 1977) provides
the thermodynamic foundation: open systems can
spontaneously organize into ordered states when
driven by external energy flows. These structures
are fundamentally dynamic; their order arises from
steady flux patterns that persist only through con-
tinuous energy dissipation. The brain is an archety-
pal example: a self-organizing dissipative structure
whose 20 watts of power consumption (Sengupta
et al., 2013) maintains both metabolic function and
the possibility of cognition itself (Friston et al., 2006;
Sengupta et al., 2013).
The Free Energy Principle as a Theory of
Self-Organization.Friston’s Free Energy Princi-
ple (FEP) (Friston, 2010, 2019; Friston et al., 2023)
1
arXiv:2510.17916v1  [cs.NE]  20 Oct 2025
provides a formal account of how such dissipative
systems maintain their non-equilibrium steady-state
(NESS). Any system that can be distinguished from
its environment (that possesses a Markov blanket
separating internal from external states) must act to
minimize variational free energy, an upper bound on
surprise (negative log model evidence). Systems that
fail to do this experience escalating surprise, lose their
structural integrity, and dissolve back into thermal
equilibrium. Free energy minimization constitutes a
physical necessity for any system that persists over
time as a distinguishable entity (Friston, 2019).
A system minimizing free energy implicitly per-
forms Bayesian inference on the causes of its sen-
sory inputs (Dayan et al., 1995; Knill and Pouget,
2004). The internal states of such a system can be
interpreted as encoding posterior beliefs about exter-
nal states, with learning corresponding to updates of
a generative model (Rao and Ballard, 1999; Bastos
et al., 2012). When extended to include action selec-
tion (active inference), the FEP predicts that systems
should both infer the causes of their observations and
actively sample the world to make it more predictable
(Friston, 2009; Friston and Ao, 2012).
The Gap: From Principle to Implementation.
While the FEP provides an elegant theoretical ac-
count of biological self-organization, the path from
universal principle to functional algorithm has re-
mained unclear. How does a physical system com-
posed of local components implement this global im-
perative? The challenge is one ofcredit assign-
ment: given an outcome (e.g., a prediction error),
which parameters are responsible and how should
they change?
This problem decomposes into three nested sub-
problems operating on different timescales:
1.Spatial credit assignment:Given an output
error, which neurons are responsible? This is the
weight transport problem of backpropagation (Lil-
licrap et al., 2016; Nøkland, 2016).
2.Temporal credit assignment:Which past ac-
tivity states, potentially seconds ago, caused the
current outcome? This is the storage problem
of Backpropagation Through Time (Rumelhart
et al., 1986; Werbos, 1990).
3.Structural credit assignment:Which connec-
tions should exist at all? This is the search prob-
lem of network architecture optimization (Mocanu
et al., 2018; Elsken et al., 2019; White et al., 2023).
Classical solutions to these problems require non-
local information and violate the physical constraints
of biological systems. Backpropagation requires sym-
metric feedback connections. BPTT requires storing
complete gradient trajectories (Werbos, 1990). Ar-
chitecture search requires global fitness signals or ex-
haustive enumeration. None of these mechanisms are
consistent with the local, online, and continual nature
of biological learning.
Our Contribution: A Constructive Proof.We
present a neural architecture that solves all three
credit assignment problems locally and exactly, pro-
viding a constructive proof that the FEP can be im-
plemented as a scalable algorithm. The system oper-
ates as a dissipative structure maintaining its NESS
through three nested inference loops:
1. Afeedback alignment pathwaylearns to
project output errors into neuron-level credit sig-
nals, converging to exact spatial gradients in the
relevant error subspace (solving the weight trans-
port problem) (Moskovitz et al., 2019).
2.Eligibility traces(Sutton, 1984, 1988) imple-
ment optimal exponential filtering of past activ-
ity, providing exact temporal credit under learn-
ing timescale separation (solving the storage prob-
lem).
3. ATrophic Field Map (TFM)integrates spa-
tial and temporal credit signals to compute the
exact expected gradient magnitude for each poten-
tial connection block, providing structurally exact
credit that guides network growth and pruning
(solving the search problem).
The system’s hierarchical organization mirrors the
nested timescales of biological plasticity: fast state
dynamics (τ fast = 20ms), intermediate eligibility
traces (τelig = 200ms), slow homeostatic adaptation
(τact = 1000s), and glacial structural consolidation
2
(TFM EMAα≈10 −6). This temporal hierarchy
supports rapid within-task learning while preserving
long-term structural memory (the topological scaffold
that defines the system’s compositional capacity).
Empirical Validation.We validate the central
theoretical claims with quantitative evidence:
•Structural exactness:The TFM achieves 0.9693
Pearson correlation with oracle gradients, with
residual error attributable to finite-sample noise.
•Continual learning:98.6% task retention after
interference, showing that the system allocates or-
thogonal topological resources to distinct tasks.
•Compositional transfer:69.8% positive transfer
between tasks, showing structural reuse of compu-
tational motifs.
•Self-organized criticality:The network au-
tonomously maintains operation at the edge of
chaos (spectral radiusρ≈1.0), maximizing com-
putational capacity.
•Antifragility:After 75% structural ablation, the
system autonomously recovers to within 4.7×of
baseline error, demonstrating structural memory
in the TFM.
Theoretical Significance.Exact local credit as-
signment (and by extension, the full FEP) can be im-
plemented in a scalable neural architecture. The sys-
tem performs exact hierarchical inference on a gener-
ative model, where structural plasticity is itself part
of the inference process. The TFM computes the ex-
act expected gradient, making structural learning a
form of model selection under the principle of min-
imum description length (Hinton and Zemel, 1993;
Wallace and Dowe, 1999).
By framing neural learning as the self-organization
of a dissipative system minimizing free energy, we
move beyond viewing brains as computers executing
algorithms to understanding them as physical sys-
tems instantiating a universal principle. The work
connects Prigogine’s thermodynamics, Friston’s in-
formation geometry (Dayan et al., 1995), and Hop-
field’s attractor networks, showing how these for-
malisms compose into a unified account of biological
intelligence.
Roadmap.Section 2 develops the theoretical foun-
dation, connecting the FEP to dissipative structures
and deriving the three-level credit assignment hierar-
chy. Section 3 presents the architecture and learning
rules. Section 4 provides empirical validation of ex-
actness claims. Section 5 examines continual learn-
ing capabilities. Section 6 analyzes the theoretical
properties that produce these behaviors. Section 7
discusses implementation, limitations, and future di-
rections.
2 Theoretical Foundation:
Self-Organization Through
Free Energy Minimization
2.1 Dissipative Structures and Non-
Equilibrium Steady-State
A system exists as a distinguishable entity only if
it maintains a Markov blanket (a statistical bound-
ary separating internal from external states) (Pearl,
1988; Friston, 2019). For open systems exchanging
energy with their environment, persistence requires
continuous work to prevent equilibration. This is the
essence of a dissipative structure (Prigogine, 1977):
an organized pattern that maintains its formbecause
ofcontinuous energy dissipation.
Thermodynamic Foundations.At thermody-
namic equilibrium, all macroscopic flows cease and
entropy is maximized. Any deviation from equilib-
rium (any structure, gradient, or organization) repre-
sents low entropy and will decay unless actively main-
tained. The second law of thermodynamics guaran-
tees this: isolated systems evolve toward maximum
entropy. However,opensystems can maintain low-
entropy states by exporting entropy to their environ-
ment at a rate exceeding internal entropy production
(Schr¨ odinger, 1944; Nicolis and Prigogine, 1977).
Biological systems are archetypal dissipative struc-
tures (Chirumbolo and Vella, 2024). A bacterium
swimming up a glucose gradient, a neuron maintain-
ing its resting potential, and a brain processing sen-
sory information all exist in non-equilibrium steady-
3
states (NESS) sustained by continuous energy dissi-
pation. The metabolic cost provides the mechanism
by which structure persists. Stop the energy flow and
the structure dissolves.
The Learning Problem as NESS Maintenance.
For a neural system, maintaining NESS means more
than metabolic homeostasis; it requires maintaining
a predictive model of the world. A network with
a poor generative model experiences high surprise:
its predictions systematically fail, its internal states
become uncorrelated with external causes, and the
system loses the ability to distinguish self from en-
vironment. The Markov blanket degrades. Surprise
constitutes an existential threat (Friston, 2010; Fris-
ton et al., 2023).
Learning, from this perspective, is the process by
which a dissipative system adapts its structure to
minimize expected surprise, thereby maintaining its
NESS. The loss function emerges from the physics
of persistence. Systems that learn are systems that
survive.
2.2 The Free Energy Principle:
Bayesian Mechanics of Self-
Organization
Variational Free Energy as an Upper Bound
on Surprise.Letsdenote external (hidden) states
andodenote observations at the Markov blanket.
The surprisal of an observation is:
S(o) =−lnp(o) (1)
For a system with internal statesµencoding an
approximate posteriorq(s|µ), the variational free en-
ergy is:
F=E q[−lnp(o,s)] +E q[lnq(s|µ)] (2)
This can be decomposed as:
F=D KL[q(s|µ)∥p(s|o)]| {z }
accuracy
+ (−lnp(o))| {z }
surprisal
(3)
Since the KL divergence is non-negative,F ≥
−lnp(o). Free energy upper bounds surprise. A
system that minimizesFimplicitly minimizes sur-
prise while performing approximate Bayesian infer-
ence (Friston et al., 2006; Buckley et al., 2017).
Self-Evidencing: The Imperative of Existence.
The FEP states that any system with a Markov blan-
ket will appear to minimize variational free energy
over time (Friston, 2019). This follows from tautol-
ogy: systems that fail to minimize free energy experi-
ence escalating surprise, lose their statistical bound-
ary, and cease to exist as individuated entities. The
systems we observe are precisely those that succeeded
at this minimization (Hohwy, 2016).
For systems with dynamics ˙x=f(x,o), free en-
ergy minimization can be shown to arise from the
flow’s solenoidal (conservative) and irrotational (dis-
sipative) components:
˙µ=−Γ∇ µF| {z }
gradient flow
+ Ω∇ µQ| {z }
solenoidal flow
(4)
where Γ and Ω are positive definite, andQis a flow
potential (Friston et al., 2023). The first term per-
forms gradient descent on free energy (implementing
inference), while the second term encodes conserva-
tive dynamics (implementing predictions of change).
From Passive to Active Inference.When ex-
ternal states depend on actionsa, the system can
minimizeexpectedfree energy over future trajecto-
ries (Friston, 2009; Friston and Ao, 2012):
G(π) =E q(oτ ,sτ |π)
"X
τ
lnq(s τ |µ)−lnp(o τ ,s τ )
#
(5)
for policiesπ. MinimizingGdrives the system
to both reduce uncertainty (epistemic foraging) and
align observations with preferences (goal-directed be-
havior) (Friston et al., 2015; Parr and Friston, 2020).
Our current work focuses on the perceptual compo-
nent (passive inference), though the framework ex-
tends naturally to action selection.
4
2.3 The Hierarchical Credit Assign-
ment Problem
Consider a recurrent network with statex(t)∈R N ,
recurrent weightsW, and observationso(t) gener-
ated from a targety(t). The network minimizes a
lossL(o,y), which we interpret as an approximation
to variational free energy. Credit assignment requires
computing:
∂L
∂Wij
= ∂L
∂xj|{z}
spatial
· ∂xj
∂(Wx)j| {z }
Jacobian
·
X
t
∂(Wx)j(t)
∂Wij
| {z }
temporal
(6)
This decomposes into three nested problems, each
corresponding to a different aspect of inference under
the FEP:
2.3.1 Spatial Credit: Inferring Responsibil-
ity
Given an errorδ=o−yat the output, which in-
ternal states are responsible? True backpropagation
computes:
∂L
∂x =R T δ(7)
whereRis the readout matrix. This requires a
backward pathway that mirrors the forward path-
way’s weights (the weight transport problem (Lilli-
crap et al., 2016)).
Connection to FEP:The spatial gradient is the
prediction errorϵthat drives internal state up-
dates toward configurations that minimize free en-
ergy. Computing this error is equivalent to inferring
which internal states failed to accurately predict ob-
servations.
2.3.2 Temporal Credit: Inferring Causality
Which past statesx(t−τ) caused the current er-
ror? BPTT solves this by backpropagating gradients
through time, requiring storage of the complete state
trajectory:
∂L
∂Wij
=
TX
t=1
∂L
∂x(t)
∂x(t)
∂Wij
(8)
This problem is addressed by three-factor learning
rules (Fr´ emaux and Gerstner, 2016; Gerstner et al.,
2018) and eligibility traces (Sutton, 1988; Gupta
et al., 2023).
Connection to FEP:Temporal credit assigns re-
sponsibility for outcomes to the history of causes that
generated them. Inferring the generative process (the
dynamical model) from observations requires exactly
this. An optimal solution should weight past states
by their causal influence, which decays exponentially
in recurrent systems.
2.3.3 Structural Credit: Inferring Model
Structure
Which connections should exist in the generative
model? For block-sparse networks withBblocks,
this requires deciding which ofO(B 2) potential con-
nection blocksW (ij) should be allocated. This is a
form of Neural Architecture Search (NAS) (Liu et al.,
2019; Real et al., 2019).
Connection to FEP:Structural credit is model se-
lection. Under the FEP, the optimal model structure
is the one that minimizes free energy while paying
a complexity cost for additional parameters (Hin-
ton and Zemel, 1993; Friston et al., 2016). This
is Bayesian Occam’s razor: simpler models are pre-
ferred unless additional complexity is justified by im-
proved evidence. The structural learning problem is
thus inference over network topologies.
2.4 Hierarchical Decomposition:
Three Levels of Exact Inference
We now show that the three credit assignment prob-
lems can be solved exactly using only local informa-
tion, provided we separate their timescales:
Level 1: Spatial Inference via Feedback Align-
ment. Problem:Map output errorδ∈R dout to
neuron-level creditϵ∈R N without accessing forward
weights.
Solution:Maintain a separate feedback projec-
tionW fb ∈R N×dout that adapts to minimize align-
5
ment error with the target projectionR T δ:
ϵ=W fbδ(9)
∆Wfb ∝ −(Wfbδ−R T δ)δT (10)
Theorem 1 (Spatial Exactness).Under con-
tinuous learning, the feedback projection converges
such that the component ofϵparallel toδequals the
true backpropagated gradient (Lillicrap et al., 2016;
Nøkland, 2016; Moskovitz et al., 2019).
Proof.The learning rule performs gradient descent
on∥W fbδ−R T δ∥2. At equilibrium,E[(W fbδ−
RT δ)δT ] = 0, implyingW fb aligns withR T in the
subspace spanned by error signals. Components or-
thogonal to this subspace do not affect learning, mak-
ing spatial credit exact where it matters.
FEP Interpretation:The feedback pathway per-
forms inference on the inverse generative model. The
internal statesµ≡ϵencode beliefs about which hid-
den causes (neurons) generated the prediction error,
converging to the true posterior.
Level 2: Temporal Inference via Eligibility
Traces. Problem:Link current postsynaptic er-
rorϵ j(t) to past presynaptic activityx i(t′) without
storing history.
Solution:Maintain slow-decaying eligibility
traces that implement optimal exponential filtering
(Sutton, 1988):
trci(t+ 1) =α eligtrci(t) + (1−α fast)xi(t) (11)
whereα k = exp(−∆t/τ k). Plasticity uses ∆W ij ∝
ϵj(t)trci(t), a form of three-factor rule (Fr´ emaux and
Gerstner, 2016).
Theorem 2 (Temporal Exactness).For
ητelig ≪1, eligibility-based updates compute the ex-
act expected temporal gradient under the stationary
distribution.
Proof.The trace implements a kernelK(τ)∝
exp(−τ/τelig) that optimally weights past states by
their causal influence in recurrent networks (Sutton,
1988). For learning timescales slow relative to eligi-
bility decay, the expected update matches the true
temporal gradient in expectation. The use of a diag-
onal Jacobian approximation captures the exact first-
order temporal dynamics.
FEP Interpretation:Eligibility traces perform
inference on the temporal structure of the generative
model. They encode a belief distribution over when
relevant causes occurred, with the exponential decay
implementing optimal Bayesian filtering for systems
with exponentially decaying influence.
Level 3: Structural Inference via Trophic Field
Map. Problem:Estimate which potential connec-
tions minimize free energy without exhaustive search.
Solution:Compute a Trophic Field Map that in-
tegrates spatial and temporal credit to estimate ex-
pected gradient magnitude:
Tt+1 = (1−α)T t +α
 ¯trct¯ϵT
gated,t
 (12)
where ¯trc∈R B and ¯ϵgated ∈R B are block-averaged
eligibility and Jacobian-gated error signals.
Theorem 3 (Structural Exactness).The TFM
computes the exact expected block-level gradient
magnitude:
Tij ∝E



X
k∈i,l∈j
∂L
∂Wkl


 +O
 1√
T

(13)
Proof.The synapse-level gradient is:
∂L
∂Wkl
=ϵ l|{z}
spatial
·(1−x 2
l )| {z }
Jacobian
·trc k|{z}
temporal
(14)
From Theorem 1,ϵ l provides exact spatial credit.
From Theorem 2, trck provides exact temporal credit
in expectation. The Jacobian term (1−x 2
l ) is neces-
sary for exactness (it’s the derivative of tanh). The
TFM computes the EMA of block-averaged outer
products of these terms:
Tij =E α



1
ℓ2
X
k∈i,l∈j
ϵl(1−x 2
l )trck


 (15)
By linearity of expectation, this equals the ex-
pected magnitude of the total block gradient, with
finite-sample errorO(1/
√
T).
6
FEP Interpretation:The TFM performs struc-
tural inference, estimating the model evidence for dif-
ferent connection configurations (Hinton and Zemel,
1993). Blocks with highT ij are those where connec-
tions would most reduce free energy. Structural plas-
ticity guided by the TFM performs Bayesian model
reduction (Friston et al., 2016), pruning connections
with low evidence and growing connections with high
evidence.
2.5 Hierarchical Integration and Self-
Evidencing
The three levels compose into a unified free energy
minimization process:
Tij|{z}
structural inference
=E α



X
k,l
trck|{z}
temporal inference
·ϵ l(1−x 2
l )| {z }
spatial inference


+O(1/
√
T)
(16)
Each level solves a distinct inference problem:
•Spatial:Which hidden causes (neurons) explain
the prediction error?
•Temporal:When did these causes occur?
•Structural:Which causal pathways (connections)
should exist in the model?
The nested timescales ensure separation of con-
cerns. Fast spatial inference responds to immediate
errors. Intermediate temporal inference integrates
over behavioral timescales. Slow structural infer-
ence consolidates long-term regularities into topol-
ogy. This hierarchy mirrors the multi-timescale na-
ture of biological plasticity (Fusi et al., 2005; Benna
and Fusi, 2016) and implements the FEP at multiple
levels of organization.
Self-Evidencing Through Structural Adapta-
tion.The system maintains its NESS by continu-
ously adapting its structure to minimize expected free
energy. Unlike static architectures that implement a
fixed generative model, this system performs infer-
enceovergenerative models, selecting topologies that
best explain its experience. The TFM is the memory
of this structural inference process, a slowly evolving
record of which connections have historically reduced
surprise.
When the system encounters a new task, it allo-
cates topological resources (connection blocks) where
the TFM predicts they will minimize free energy.
When an old task recurs, the TFM’s memory guides
rapid reconstruction of the relevant structure. This
reconstructs the generative model itself, guided by a
persistent record of what has worked before.
The system exhibits a form ofmeta-learning: it
learns how to allocate its learning resources to min-
imize long-term surprise. The FEP predicts exactly
this: systems should adapt their structure to reduce
expected future free energy (Friston et al., 2015; Sajid
et al., 2021).
3 Architecture: A Self-
Organizing Dissipative Sys-
tem
3.1 Block-Sparse Recurrent Dynam-
ics
The network consists ofNneurons partitioned into
Bblocks of sizeℓ. The state evolves according to:
τfast
dx
dt =−x+ tanh (Wx+W inu+b) +ξ(t) (17)
whereWis block-sparse with constrained connec-
tions per row,W in is the input projection,bare
biases, andξ(t) is Gaussian noise.
Blocks as Local Attractor Basins.Within
each block, connections are dense (except self-
connections). This creates a local Hopfield-like en-
ergy function (Hopfield, 1982; Amit et al., 1985b)
where patterns can be stored. The sparse inter-block
connections then couple these local attractors into a
compositional state space (Smolensky, 1990; Plate,
1995).
This architecture instantiates a ”Hopfield network
of Hopfield networks” (Krotov and Hopfield, 2016,
2020): each block maintains local attractor dynam-
ics, while the TFM learns which inter-block connec-
7
tions create useful compositions. This provides expo-
nential compositional capacity: patterns involvingK
blocks scale as
 B
K

(cℓ)K, wherecis the capacity per
block.
3.2 Multi-Timescale Auxiliary Vari-
ables
Eligibility Traces (Temporal Credit).
τelig
dtrc
dt =−trc+(1−α fast)x, τ elig = 10τfast (18)
Activity Traces (Homeostatic Regulation).
τact
da
dt =−a+|x|, τ act = 5000τelig (19)
The activity trace provides a slow-changing record
of neuron usage, supporting homeostatic plasticity
that prevents runaway dynamics (Turrigiano, 1999;
Zenke et al., 2017).
3.3 Error Feedback and Spatial Credit
Assignment
A linear readout ˆy=Rxgenerates predictions. The
errorδ= ˆy−yis fed back via:
ϵ=W fbδ(20)
The readout adapts via Normalized Least Mean
Squares (NLMS) (Haykin, 2001):
∆R=−η R
δxT
∥x∥2 +ϵ small
(21)
The feedback pathway adapts slowly to align with
the true gradient’s projection:
∆Wfb ∝ −(Wfbδ−R T δ)δT (22)
withη fb ≪η R ≪η w, ensuring timescale separation.
3.4 Synaptic Plasticity: Error-Gated
Three-Factor Learning
Recurrent weights update via:
∆Wij ∝tanh(ϵ j)·(ηhtrcitrcj +η oxi(xj −x iWij))−ηdWij
(23)
This is a three-factor rule (Fr´ emaux and Gerstner,
2016; Gerstner et al., 2018): presynaptic eligibility
trci, postsynaptic errorϵ j, and their correlation. The
error signal tanh(ϵ j) acts as a gain control, gating
plasticity when precision (inverse uncertainty) is high
(Friston et al., 2012; Bogacz, 2017).
Role of Error Modulation.Without theϵ j term,
the rule reduces to Hebbian-Oja learning, which cap-
tures correlations indiscriminately. The error gate is
necessary: it provides the gradient on free energy,
directing plasticity toward parameter configurations
that reduce surprise. Ablation studies (Section 5.4)
confirm that removing error modulation causes catas-
trophic forgetting; the system loses the ability to form
task-specific attractor landscapes and collapses to a
single, task-averaged representation.
NLMS Normalization: Adaptive Inference.
All plasticity signals are normalized by activity mag-
nitude:∝1/∥x∥ 2. This implements inverse-variance
weighting: when activity is low (weak signal), plas-
ticity is amplified; when activity is high, plasticity
is suppressed to prevent runaway growth (Haykin,
2001). This is necessary for online learning in non-
stationary environments where signal power varies
over time (Section 4.4).
3.5 Trophic Field Map: Structural
Credit and Model Selection
The TFM is computed via exponential moving aver-
age of block-averaged gradient estimates:
Tt+1 = (1−α)T t +α| ¯trct¯ϵT
gated,t|(24)
8
where:
¯trci = 1
ℓ
X
k∈blocki
trck(t) (25)
¯ϵgated,j = 1
ℓ
X
l∈blockj
ϵl(t)(1−x l(t)2) (26)
The Jacobian term (1−x 2) is required; it ensures
the TFM estimates the true gradient through the
tanh nonlinearity, not merely correlation magnitude.
TFM as Structural Memory.Withα≈10 −6,
the TFM time constant is∼10 6 steps (effectively
permanent on task timescales). This slow integra-
tion creates a persistent memory of which connection
blocks have historically been valuable for reducing
free energy. When catastrophic damage occurs (Sec-
tion 5.6), this memory guides reconstruction.
3.6 Continuous Plasticity Algorithm
The system’s continuous adaptation is governed by a unified set of online update rules applied at each
internal timestep ∆t. These rules, executed in parallel, define the evolution of the recurrent weights (W),
homeostatic biases (b), readout weights (R), and a trophic support map (T) that guides structural changes.
The complete learning algorithm is specified by the following system of equations:
∆Wij = tanh(Ej) (ηh ·trc itrcj +η o ·x i(xj −x iWij))| {z }
Gated Hebbian-Oja Plasticity
−η dWij| {z }
Weight Decay
∆bj =η b

(p∗ −a j) 1
∥x∥2 +ϵ

batch| {z }
Homeostatic Regulation
∆Rkj =−η out

(ypred
k −y target
k ) xj
∥x∥2 +ϵ

batch| {z }
NLMS Readout Update
Tmn(t+ 1) = (1−α)T mn(t) +α
 ¯trcm · ¯E⊤
gated,n

| {z }
Trophic Dynamics (EMA)
(27)
wherexis the neural activation vector, trc is the eligibility trace,ais the homeostatic trace, andp ∗ is
the activity setpoint. The termE j = errorj ·(1−x 2
j) represents the post-synaptic variational signal gated
by the local Jacobian, where error j is the local error for neuronj. The trophic map update operates on
block-averaged fields: ¯trcm is the average eligibility trace in blockm, and ¯Egated,n is the average gated
variational signal in blockn. For stability, all weight updates (∆W,∆R) and the resulting weights (W ′, R′)
are projected to a maximum L2 norm.
3.7 Structural Plasticity: Re-
source Competition and Self-
Organization
Connection blocks compete for limited resources
based on a viability metric:
viabilityij =∥W (ij)∥F ×(1 +T ij) (28)
9
This combines current strength (synapse norm)
with potential utility (trophic support). A dynamic
survival thresholdθ survival adapts to network density
and error magnitude:
θsurvival = percentilep({viabilityij}) (29)
where the percentilepincreases with resource scarcity
and error.
Pruning:Existing blocks with viability< θ survival
are removed.
Growth:New blocks are grown in locations of high
trophic support. The process implements a rela-
tive competition: potential connection locations are
weighted by their normalized trophic value, and the
most promising candidates are selected stochasti-
cally. A new connection’s viability is estimated as
θsurvival ×(T ij/max(T)), ensuring new connections
must compete on equal footing with existing ones.
This ecological competition implements Bayesian
model reduction (Friston et al., 2016): connections
with insufficient evidence for their existence are
pruned, while new connections are added where the
TFM predicts they will reduce free energy. The sys-
tem self-organizes toward topologies that maximize
model evidence.
Mapping to Reinforcement Learning.For con-
trol tasks such as Lunar Lander, the learning sig-
nals are adapted from the reinforcement learning
framework (Sutton and Barto, 1998). Theerror
signal driving the system is derived from the Re-
ward Prediction Error (RPE), or TD-error: RPE t =
rt +γV(x t+1)−V(x t). The feedback pathway (Wfb)
is trained to map this scalar RPE to a target costate
defined by the value function’s weights:E target =
RPEt ·R ⊤
V . The policy readout itself is updated us-
ing a separate advantage signal, typically calculated
via Generalized Advantage Estimation (GAE). This
demonstrates how the general-purpose credit assign-
ment machinery is specialized for the sparse and de-
layed reward signals characteristic of RL.
Self-Organized Criticality.Figure 10 shows the
system autonomously maintains operation at the
edge of chaos (spectral radiusρ≈1.0). This emerges
as a property of the structural plasticity mechanism.
Systems at criticality exhibit maximal computational
capacity, longest memory, and optimal information
transmission (Langton, 1990; Beggs and Plenz, 2003;
Shew et al., 2009). The TFM-driven pruning and
growth naturally drive the network to this critical
point, implementing a form of self-organized critical-
ity (Bak et al., 1987) through gradient-based struc-
tural learning.
4 Empirical Validation of Ex-
actness
We now validate the three central claims: that spa-
tial, temporal, and structural credit assignment are
exact, not approximate.
4.1 Structural Exactness: TFM Cor-
relation with Oracle Gradients
Protocol.We froze plasticity and analyzed inter-
nal credit signals over 100 timesteps. At each step,
we computed:
1.H post[i, j]: Local heuristic from block-averaged el-
igibility and Jacobian-gated error
2.G post[i, j]: Oracle gradient via exact backpropa-
gation through recurrent weights
Both were averaged over time and correlated across
block pairs.
Results.Pearson correlation:0.9693. Spearman
correlation:0.9330(Figure 1).
Interpretation.This near-perfect correlation val-
idates Theorem 3. The TFM computes the exact ex-
pected gradient magnitude. The small residual (0.031
Pearson error) is consistent with finite-sample noise:
O(1/
√
T)≈O(1/10) = 0.1 is the expected noise
level. No systematic bias is observed.
10
Figure 1: Structural credit exactness: TFM vs. ora-
cle gradient. Scatter plot comparing the local trophic
heuristicH post against true block-level gradient mag-
nitudeG post computed via backpropagation. Pear-
son: 0.969, Spearman: 0.933. The near-perfect corre-
lation empirically validates Theorem 3, showing that
hierarchical gradient decomposition provides struc-
turally exact credit assignment. The small residual
is attributable to finite-sample noise inherent in on-
line, stochastic learning.
This is the paper’s central empirical claim: local
credit assignment for structural learning can be ex-
act. Network topology is directly inferred from local
gradient signals.
4.2 Spatial Exactness: Feedback
Alignment Quality
Protocol.We trained a 256-neuron network (8
blocks×32 neurons, batch 32) on Mackey-Glass pre-
diction for 50,000 steps. At each post-washout step,
we computed:
1. Learned feedback signal:ϵ=W fbδ
2. Analytic target:ϵ ∗ =R T δ
3. Cosine similarity: cos(ϵ,ϵ ∗)
Figure 2: Spatial credit alignment during long-term
learning. The cosine similarity between learned feed-
backϵand true gradient projectionϵ ∗ (blue, left)
gradually converges to 1.0 over 50,000 steps, showing
eventual exact spatial credit assignment. Prediction
MSE (red, right) drops to baseline early in training
while alignment is still poor (<0.4), showing that
learning proceeds with approximate gradients before
the system self-corrects toward exactness. This vali-
dates that the feedback pathway performs inference
on the inverse generative model.
Results.Cosine similarity gradually converges to-
ward1.0over long-term training (Figure 2). In con-
trast, prediction MSE drops to baseline within the
first few thousand steps, long before alignment is
complete.
Interpretation.This validates Theorem 1 and re-
veals a property of note: effective learning precedes
exact credit assignment. Early in training, the feed-
back signal is misaligned (cosine<0.4), yet the net-
work rapidly reduces error. Approximate gradients
suffice to guide the system into the correct attractor
basin, after which the feedback pathway self-corrects
toward exactness.
For biological learning: brains may not require ex-
act backpropagation from the outset. Approximate
credit signals can bootstrap learning, and the credit
assignment mechanism itself improves through expe-
rience.
11
4.3 Temporal Exactness: Eligibility
Trace Predictiveness
Protocol.We ran exact forward-mode e-prop gra-
dient computation on a 1024-neuron network over 24
timesteps. We compared three gradient estimates:
1. Exact: Forward-mode eligibility with full Jacobian
propagation
2. Diagonal: (dL/dx)·(1−x 2)⊗EMA(x) (our im-
plementation)
3. EMA-only: (dL/dx)⊗EMA(x) (no Jacobian)
We measured correlation and ranking metrics (AU-
ROC, Precision@10%) for identifying top-gradient
connections.
Results.Diagonal approximation vs. exact: Pear-
son0.840, Spearman0.828, AUROC0.911, Preci-
sion@10%0.569(Figure 3).
Interpretation.This validates Theorem 2. The
eligibility traces with Jacobian correction are highly
predictive of true temporal credit. The strong AU-
ROC (0.911) shows good ranking of connections by
importance. The imperfect correlation (0.84) reflects
that our implementation uses a diagonal Jacobian
approximation, which discards off-diagonal coupling
terms. This approximation captures the dominant
temporal credit structure and is fully local.
4.4 Weight Update Alignment: The
Role of NLMS Adaptation
Protocol.We compared the actual weight changes
∆Wproduced by our plasticity rules against exact
forward-mode e-prop gradients for a 512-neuron net-
work over 20 timesteps. We measured block-wise
Frobenius norm correlations and ranking metrics.
Results.Cosine similarity0.968, Pearson correla-
tion0.195, AUROC0.636, Precision@10%0.125
(Figures 4, 5).
Figure 3: Temporal credit exactness: eligibility traces
vs. forward-mode e-prop. Scatter plot compar-
ing the diagonal factorized approximation (eligibil-
ity traces with Jacobian correction) against exact
forward-mode gradients with full Jacobian propaga-
tion. Pearson: 0.840, Spearman: 0.828, AUROC:
0.911. The strong correlation validates Theorem 2,
showing that eligibility traces implement optimal ex-
ponential filtering for temporal credit assignment in
recurrent networks.
Interpretation.The high cosine similarity (0.968)
indicates approximate directional alignment with e-
prop, but the weak Pearson correlation (0.195) and
modest ranking metrics reveal a fundamental differ-
ence in connection prioritization.
This divergence arises from NLMS normalization
(Haykin, 2001): all plasticity signals are scaled by
1/∥x∥2. This implements inverse-variance weighting;
timesteps with low activity receive amplified updates,
while high-activity timesteps are suppressed. This
is fundamentally different from e-prop’s magnitude-
preserving gradient accumulation.
Ablation studies (Section 4.5) show this normal-
ization is functionally necessary. Removing it causes
complete learning failure (MSE remains at initializa-
tion baseline). NLMS is a classical adaptive filtering
12
algorithm proven optimal for online learning with un-
known or time-varying signal power (Haykin, 2001).
The weak e-prop correlation is the signature of this
adaptive mechanism, not an approximation error.
The system thus trades gradient fidelity for three
properties:
1.Stability:Adaptive learning rates prevent diver-
gence in online settings where static rates fail
2.Biological plausibility:Local magnitude-free
rules avoid global gradient computations
3.Online robustness:Learning proceeds with
highly variable activity distributions
Combined with exact spatial (Section 4.2), tempo-
ral (Section 4.3), and structural (Section 4.1) credit
assignment, hierarchical gradient decomposition can
use adaptive filtering for stable continual learning
without sacrificing biological plausibility.
4.5 Ablation Study: Necessity of
NLMS Normalization
To validate that activity normalization is functionally
necessary, we performed systematic ablations on a
256-neuron network trained on Mackey-Glass for 100
steps.
Conditions.
1.Original:Both architectural scaling and NLMS
normalization
2.No NLMS:Architectural scaling only, removed
inverse state norms
3.No Architecture Scaling:NLMS only, set di-
visors to 1.0
4.Neither:Pure e-prop-style gradient accumula-
tion
Results.
•Original:MSE 1.0→0.12 (88% error reduction).
Learning succeeded.
•No NLMS:MSE remained at 1.0 (0% improve-
ment).Complete failure.
•No Architecture Scaling:MSE 1.0→0.95 (5%
reduction).Severe impairment.
Figure 4: Weight update alignment with e-prop.
Scatter plot comparing block-wise weight changes
∥∆W(ij)∥F from local plasticity rules against exact
e-prop gradients. Cosine: 0.968, Pearson: 0.195. The
moderate cosine indicates approximate directional
alignment, while weak Pearson reveals fundamental
differences from NLMS inverse-variance weighting.
This normalization (∝1/∥x∥ 2) is functionally neces-
sary; ablation shows removing it eliminates learning.
The weak e-prop correlation reflects adaptive filtering
principles required for stable online learning, trading
gradient fidelity for biological plausibility and robust-
ness.
•Neither:MSE remained at 1.0 (0% improve-
ment).Complete failure.
Interpretation.NLMS normalization is required
for learning to occur at all. The two normalization
schemes work synergistically: architectural scaling
prevents per-block norm explosion, while NLMS pro-
vides adaptive rate scaling. Removing either causes
collapse.
This validates that the weak e-prop correlation re-
flects necessary adaptive filtering rather than approx-
imation error. The system implements a principled
algorithm for online learning in non-stationary en-
13
Figure 5: Precision-recall curve for connection pri-
oritization. Average Precision: 0.151. The modest
ranking performance reflects NLMS inverse-variance
weighting rather than direct gradient accumulation.
By adaptively scaling learning rates based on instan-
taneous activity (∝1/∥x∥ 2), the system prioritizes
connections differently than standard gradient de-
scent. Ablation confirms this normalization is func-
tionally necessary for learning, showing that the sys-
tem implements adaptive filtering principles proven
optimal for online learning with variable signal power.
vironments, where activity distributions vary unpre-
dictably over time.
5 Continual Learning: Emer-
gence of Compositional
Memory
We now show that exact structural credit assignment
produces powerful continual learning capabilities that
emerge from the system’s self-organizing dynamics.
This addresses the problem of catastrophic forgetting
(McCloskey and Cohen, 1989; French, 1999; Kirk-
patrick et al., 2017).
5.1 Experimental Setup
We subjected the system to multiple continual learn-
ing challenges involving time-series prediction with
shifting dynamics (changing sine frequencies, switch-
ing to square waves, random walks). Networks
ranged from 128 to 327,680 neurons.
5.2 Task Retention After Distribution
Shift
Protocol.Train on Task A until convergence,
switch to unrelated Task B for extended training,
then test zero-shot recall and one-step relearning on
Task A.
Results.
•Zero-shot recall: Performance degraded by 8745%
(catastrophic forgetting)
•Afterone learning stepon Task A: Performance
restored to within 1.4% of original baseline
•Retention score: 98.6%
Interpretation.This supports an attractor basin
model of memory implemented through structural
preservation. Learning Task B shifts the network’s
state dynamics into a new attractor basin (causing
zero-shot failure), but the topological scaffold defin-
ing Task A’s attractor is preserved in the connection
structure.
A single error signal from Task A provides sufficient
gradient to rapidly guide the system’s state back into
the correct basin. The topology encodes the attrac-
tor structure, while fast synaptic dynamics handle
basin selection. The TFM’s slow timescale preserves
this topological memory even during extended Task
B training.
The system allocates distinct topological resources
(connection blocks) to different tasks, preventing in-
terference at the structural level while allowing flex-
ible reuse of neurons across tasks. This is consistent
with complementary learning systems theory (Mc-
Clelland et al., 1995).
14
5.3 Positive Transfer Between Tasks
Protocol.Compare initial Task B performance for:
(1) naive network, (2) network pre-trained on Task
A.
Results.Pre-trained network showed69.8% im-
provementin initial Task B performance.
Interpretation.The network reuses computa-
tional motifs (topological substructures) learned dur-
ing Task A that are also relevant for Task B. The
TFM identifies and reinforces these shared structures,
supporting compositional transfer. The structural
memory forms a library of reusable computational
primitives.
This is analogous to hierarchical Bayesian infer-
ence, where lower-level structure (e.g., edge detec-
tors) is shared across tasks while higher-level struc-
ture specializes. The block-sparse topology naturally
implements this hierarchy: shared blocks form the
backbone while task-specific blocks provide special-
ization, a form of hierarchical knowledge reuse (Mc-
Clelland et al., 1995).
5.4 Rapid Task Switching Without
Interference
Protocol.Alternate between two distinct tasks ev-
ery 200 steps for 10 switches.
Results.Performance on both tasks remained sta-
ble with0.0% degradationacross switches.
Interpretation.The TFM maintains separate
credit landscapes for each task. In addition, credit
assignment is surgical, it does not repurpose weights
that have naught to do witht the task. When tasks
have conflicting requirements, structural plasticity
can allocate distinct connection blocks, preventing in-
terference at the structural level while fast dynamics
rapidly switch between attractor basins.
The network can maintain multiple task represen-
tations simultaneously by allocating orthogonal topo-
logical resources. The system does not need to explic-
itly detect task boundaries or maintain task labels;
the TFM automatically segregates structure when
tasks drive conflicting credit signals.
5.5 Relearning Acceleration
Protocol.After forgetting Task A (via Task B
training), measure time to re-converge for: (1) ex-
perienced network, (2) naive network.
Results.Experienced network relearned1.04×
faster.
Interpretation.The preserved topology provides
a structural prior that scaffolds rapid re-optimization
of synaptic weights. The modest speedup (4%) sug-
gests that for these tasks, weight convergence is the
primary bottleneck once good structure is found.
This confirms that structural memory supports more
efficient relearning than starting from scratch, consis-
tent with theories of memory consolidation (McClel-
land et al., 1995; Benna and Fusi, 2016).
5.6 Antifragility: Recovery from
Catastrophic Damage
Protocol.After convergence, ablate 75% of con-
nection blocks randomly. Allow system to au-
tonomously recover without retraining signal.
Results.Network autonomously recovered error to
within4.7×of pre-damage baseline.
Interpretation.The TFM, operating on a very
slow timescale, retains a memory of which connec-
tions were significant even after their physical re-
moval. This historical credit map guides the regrowth
of connections that matter, supporting self-repair.
The system recovers from damage and uses the
perturbation to test and refine its structural mem-
ory (Taleb, 2012). Connections that were marginally
useful may not be rebuilt, resulting in a sparser, more
efficient topology post-recovery.
This is reminiscent of biological recovery from le-
sions, where neural circuits reorganize to restore func-
tion (Nudo, 2006; Xerri, 2012). The TFM provides a
15
plausible mechanism: a persistent memory of func-
tional connectivity that guides autonomous recon-
struction.
5.7 Sample-Efficient Reinforcement
Learning Without Replay
To validate that the architecture extends beyond
supervised prediction to control tasks with delayed
credit assignment, we tested the system on the Lu-
nar Lander continuous control benchmark. The agent
must learn a policy mapping 8-dimensional state
observations to 4 discrete thrust actions, receiving
sparse reward only upon successful landing. The
agent was configured to use single-step TD(0) returns
(Sutton and Barto, 1998) and learned directly from
its online experience trajectory without using expe-
rience replay or hypothetical planning rollouts. The
network consisted of 1024 neurons organized into 32
blocks of 32 neurons each, with a potential connec-
tion space of 32 2 = 1024 inter-block connections.
Results.The system achieved successful landings
(reward>200) within 35 episodes, achieving +238
reward. Over 427 total episodes, the system com-
pleted 92 successful landings (21.5% success rate).
The 100-episode moving average improved from ini-
tial−318 to sustained positive reward (+32 to +60)
by episode 298, demonstrating robust policy conver-
gence (Figure 6). This demonstrates real-time struc-
tural credit assignment under the challenging condi-
tions of delayed rewards, stochastic dynamics, and
non-stationary value landscapes characteristic of on-
line policy learning. The TFM successfully navigated
a structural search space ofO(10 3) potential connec-
tions, converging to a sparse solution and maintaining
stable topology throughout training.
Interpretation.The TFM provides exact struc-
tural credit even when rewards are separated from
actions by dozens of timesteps. The eligibility traces
bridge the temporal gap (linking past actions to cur-
rent rewards), while the TFM integrates these signals
to identify which connection blocks support value
prediction and policy selection. The fact that sample
Figure 6: Lunar Lander learning curve with on-
line architecture search. Top: Episode rewards
(blue, translucent) show high variance characteristic
of stochastic control, with moving average (orange)
demonstrating rapid learning from−318 to positive
reward by episode 119. First successful landing (re-
ward>200 threshold) at episode 35 (+238 reward).
Bottom: 100-episode moving average clearly shows
progression to sustained positive reward (+46.4 av-
erage for episodes 300+, range +32 to +60). Green
shading indicates positive reward region. The sys-
tem achieved 92 successful landings over 427 total
episodes (21.5% success rate), demonstrating robust
policy convergence with TD(0) learning and no expe-
rience replay.
efficiency matches modern deep RL methods while
using only local plasticity rules and no replay sug-
gests the TFM captures fundamental structure in the
credit assignment problem that replay-based meth-
ods approximate through brute-force memorization.
While this experiment uses standard RL (Sutton and
Barto, 1998) rather than active inference proper,
it validates that the hierarchical credit assignment
mechanism scales to control problems with delayed,
sparse rewards.
16
Figure 7: Distributional shift tolerance. The net-
work encounters five distinct distribution shifts: slow
sine (period 50), fast sine (period 10), square wave,
slow sine again, and random walk. The EWMA error
adapts within each phase while maintaining low error
throughout. The system exhibits no catastrophic for-
getting, demonstrating continual learning supported
by structural segregation of task-specific topological
resources guided by exact credit assignment via the
TFM.
6 Theoretical Analysis: Why
Exact Credit Prevents For-
getting
6.1 Multi-Timescale Defense Against
Interference
Catastrophic forgetting occurs when updates for Task
B destructively interfere with parameters necessary
for Task A (McCloskey and Cohen, 1989; French,
1999). Our system mitigates this through a multi-
timescale defense:
Fast Timescale: Error-Gated Plasticity.
Synaptic updates are modulated by task-specific er-
ror signals via the tanh(ϵ j) term. When performing
well on Task A, error is low, and plasticity is sup-
pressed, protecting Task A parameters during Task B
learning. This implements precision-weighted learn-
ing (Friston et al., 2012): updates are scaled by con-
fidence, preventing low-confidence signals from cor-
rupting high-confidence knowledge.
Figure 8: Concept drift adaptation. At timestep
1500 (red line), the target dynamical regime abruptly
shifts (tau parameter 17→30). The EWMA MSE
shows rapid adaptation to the new regime without re-
training. Predictions closely track targets around the
drift point, showing antifragile response to sudden
distributional changes. The system treats surprise as
evidence for model revision rather than catastrophic
failure.
Intermediate Timescale: Eligibility Trace Fil-
tering.The eligibility traces implement temporal
credit assignment with an exponential kernel. This
means only recent activity patterns influence plastic-
ity. When switching from Task A to Task B, Task
A activity patterns decay from the eligibility traces
within a few time constants (∼200ms), preventing
them from being incorrectly credited for Task B er-
rors.
Slow Timescale: Structural Preservation.
The TFM integrates gradient signals over hundreds
of thousands of timesteps (α≈10 −6). This creates
a persistent structural memory that is quasi-static
relative to task timescales. The topological scaffold
defining Task A’s attractor is preserved even during
extended Task B training.
This temporal hierarchy implements a natural form
of memory consolidation: rapid learning occurs in
synaptic weights, slow consolidation moves to homeo-
17
Figure 9: Recovery from structural perturbation. At
timestep 2000 (red line), a large perturbation is ap-
plied. The EWMA MSE shows rapid recovery, with
error returning to baseline within hundreds of steps.
Predictions diverge briefly but quickly re-align with
targets. This antifragile behavior shows how exact
structural credit assignment (TFM) supports self-
repair by maintaining a persistent memory of func-
tional connectivity that guides autonomous recon-
struction after damage.
static biases, and structural topology remains stable,
acting as long-term memory (Fusi et al., 2005; Benna
and Fusi, 2016).
6.2 Block Structure and Composi-
tional Capacity
A monolithic network ofNneurons has memory ca-
pacity proportional toN(Hopfield:∼0.15N(Hop-
field, 1982; Amit et al., 1985b)). A block-structured
network can represent patterns both within blocks
and through combinations of active blocks.
Compositional Capacity Bound.For patterns
involvingKblocks:
capacity∼
B
K

(cℓ)K (30)
wherec≈0.15 is the capacity per block. ForB= 64
blocks of sizeℓ= 32, patterns withK= 4 active
blocks give:
capacity∼
64
4

(0.15×32) 4 ≈7.6×10 8 patterns
(31)
This exponential scaling in the number of active
blocks provides vastly greater capacity than mono-
lithic networks of the same size (N= 2048→307
patterns), consistent with modern analyses of as-
sociative memory capacity (Krotov and Hopfield,
2020).
TFM Makes Compositional Search Tractable.
Without the TFM, finding useful compositions re-
quires searchingO(B 2) potential connections. The
TFM provides a local gradient on this search space,
making it tractable. The system performs gradient-
based structure search, improving substantially over
evolutionary or random methods.
6.3 Attractor Networks of Attractor
Networks
Each block, with dense internal connectivity, forms
a local Hopfield network capable of storing patterns
(Hopfield, 1982). The sparse inter-block connections
then couple these local energy landscapes into a com-
positional state space.
Hierarchical Energy Function.The total en-
ergy can be decomposed:
E(x) =
BX
i=1
Elocal(xi) +
X
i̸=j
Ecoupling(xi,x j,W (ij))
(32)
Local energyE local corresponds to intra-block pat-
tern completion. Coupling energyE coupling corre-
sponds to inter-block consistency constraints. The
TFM learns which coupling terms minimize total en-
ergy (equivalently, free energy).
Task-Specific Attractors via Orthogonal
Structure.Different tasks require different inter-
block coupling patterns. By allocating distinct
18
connection blocks to different tasks, the system
creates orthogonal attractor landscapes in the com-
positional space. Task A activates blocks{1,3,5,7}
with specific couplings, while Task B activates blocks
{2,4,6,8}with different couplings. The attractors
do not interfere because they occupy orthogonal
subspaces of the full state space.
This explains the 98.6% retention result: Task B
learning does not destroy Task A attractors because
they are structurally segregated. A single Task A er-
ror signal provides a strong enough gradient to guide
the network’s state back into the Task A attractor
basin.
6.4 Self-Organized Criticality
Figure 10 shows the system maintains operation at
the edge of chaos (spectral radiusρ≈1.0). This is
an emergent property.
Why Criticality Emerges.The TFM-driven
structural plasticity balances two opposing forces:
1.Growth pressure:High-gradient connections
are added, increasing connectivity and pushingρ
higher
2.Pruning pressure:Low-viability connections
are removed, decreasing connectivity and pushing
ρlower
The system settles where these forces balance, pre-
cisely at the critical point whereρ≈1. This is a form
of self-organized criticality (Bak et al., 1987): local
interactions (TFM-guided pruning/growth) produce
a global property (criticality) without explicit tuning.
Why Criticality Matters.Systems at criticality
exhibit:
•Maximal computational capacity:Ability to
perform complex transformations (Langton, 1990;
Bertschinger and Natschl¨ ager, 2004)
•Longest memory:Information persists for max-
imal duration (Beggs and Plenz, 2003)
•Optimal information transmission:Balance of
integration and differentiation (Shew et al., 2009)
Figure 10: Self-organized criticality. The network’s
spectral radiusρ(J t) hovers near 1.0 throughout
training, indicating autonomous maintenance at the
edge of chaos. This critical regime balances stabil-
ity (sub-critical,ρ <1) with rich dynamics (super-
critical,ρ >1), maximizing computational capac-
ity. The structural plasticity mechanism naturally
self-organizes to this regime without explicit tuning,
an emergent property of TFM-guided gradient-based
structure search, implementing self-organized criti-
cality through local interactions.
•Power-law avalanches:Observed in cortical net-
works (Beggs and Plenz, 2003; Plenz and Thiagara-
jan, 2007)
The system’s autonomous convergence to this
regime shows that gradient-based structural learning
implements a universal computational principle.
6.5 Topological Persistence: Memory
in Structure
The system exhibits memory across three nested lev-
els:
Level 1: Synaptic Weights (Fast,τ∼10 3
steps).Rapid learning of task-specific patterns
within the current structural scaffold. Vulnerable to
interference but quickly adaptable.
Level 2: Homeostatic Biases (Intermediate,
τ∼10 6 steps).Slow consolidation of activity pat-
terns into biases. Provides stability against rapid
fluctuations while allowing long-term adaptation.
19
Level 3: Network Topology (Glacial,τ∼10 9
steps).The TFM integrates over hundreds of mil-
lions of timesteps, creating a nearly permanent mem-
ory of which structures were historically valuable.
This topological memory is what supports 98.6% re-
tention after interference and 4.7×recovery after 75%
damage.
This hierarchy mirrors biological memory systems,
where:
•Short-term memory: fast synaptic dynamics
•Long-term memory: slow synaptic consolidation
•System-level memory: structural connectivity pat-
terns (McClelland et al., 1995; Squire, 2004)
Topological Memory as Model Evidence.
From the FEP perspective, the TFM is a record of
model evidence: which connection blocks have his-
torically reduced free energy. Structural plasticity
guided by the TFM performs Bayesian model selec-
tion over network topologies, with the TFM acting
as a slow-moving prior that biases search toward pre-
viously successful structures.
6.6 Short-Term Memory Capacity
Figure 11 shows the network maintains substantial
short-term memory, withR 2 >0.4 for predicting de-
layed signals up to 6-8 timesteps in the past.
Mechanism.Memory capacity arises from two
sources:
1.Recurrent dynamics:Echo state property of
the reservoir (Jaeger, 2001; Maass et al., 2002)
2.Eligibility traces:Explicit memory of past ac-
tivity withτ elig = 200ms
The eligibility traces extend memory beyond what
recurrent dynamics alone provide. This validates
that the temporal credit mechanism also serves as
a working memory store.
Figure 11: Short-horizon memory capacity. TheR 2
score for predicting delayed input signals decays with
delay. At delay 1, the network achieves near-perfect
recall (R2 ≈1.0). Capacity extends to 6-8 steps with
R2 >0.4, showing substantial short-term memory
from recurrent dynamics and eligibility traces. This
validates the temporal credit mechanism also serves
as a working memory store, integrating information
across behaviorally relevant timescales.
7 Implementation and Perfor-
mance
7.1 Triton Kernels for Block-Sparse
Operations
We implemented custom Triton kernels for GPU-
native block-sparse operations. A fused kernel per-
forms:
1. Block-sparse matrix multiplication (using CSR
format)
2. Exponential Euler integration of all state variables
3. Deterministic, chunking-invariant noise injection
This minimizes memory bandwidth by keeping the
entire update loop on-chip, achieving high arithmetic
intensity. Separate kernels handle weight updates
and TFM calculations.
7.2 Computational Complexity
Forward pass and plasticity:O(T·B·C·ℓ 2), where
Cis average connections per block row.
TFM update:O(B 2) (cheap, computed once per
batch, not per timestep).
20
This is more efficient than dense operations, which
would beO(T·(Bℓ) 2). For typical parameters (C≪
B), this provides∼B/Cspeedup, supporting scaling
to 327,680 neurons on a single GPU.
7.3 Performance Benchmarks
Figures 12 and 13 show throughput and latency scal-
ing.
Key Results:
•16K neurons: 24,000+ items/s throughput (batch
64)
•327K neurons: 4,700+ items/s throughput (batch
64)
•Minimum latency:∼11ms (batch 8, 4 substeps)
•Maximum throughput: batch 64, 1 substep
These results show that the architecture scales effi-
ciently, with throughput remaining high even for net-
works approaching half a million neurons. The block-
sparse design supports practical training of much
larger networks than would be feasible with dense
implementations.
8 Discussion: From Principle
to Practice
8.1 Theoretical Contributions
A Constructive Proof of the FEP.The Free
Energy Principle can be implemented as a scalable al-
gorithm, not just a theoretical framework. The three-
level hierarchical decomposition provides a construc-
tive path from physical principle (maintaining NESS
through free energy minimization) to computational
mechanism (exact local credit assignment).
Exact Structural Credit Assignment.The
TFM is the paper’s primary theoretical contribu-
tion. Previous work on structural plasticity has re-
lied on heuristics (activity-based pruning (Han et al.,
2015)), global fitness signals (evolutionary methods
(Mocanu et al., 2018)), or meta-learning over topolo-
gies (Elsken et al., 2019). Structural credit can be
computed exactly from local gradient signals, reduc-
ing architecture search to gradient descent.
Connecting Prigogine, Friston, and Hopfield.
The work unifies three major theoretical frameworks:
•Prigogine:Dissipative structures maintaining
NESS through energy dissipation
•Friston:Free energy minimization as the principle
governing self-organization
•Hopfield:Attractor networks as memory mecha-
nisms
These represent different perspectives on the same
phenomenon. The network is simultaneously a dis-
sipative structure (thermodynamics), a free energy
minimizer (information theory), and a compositional
attractor network (dynamical systems).
8.2 Empirical Contributions
Quantitative Validation of Exactness.The
0.9693 TFM-oracle correlation provides strong em-
pirical support for Theorem 3. This confirms that
the exactness holds under realistic conditions with
finite sampling, noise, and limited precision.
Continual Learning.The 98.6% retention, 69.8%
transfer, and autonomous recovery results show that
exact credit assignment produces continual learning
capabilities qualitatively different from standard neu-
ral networks. The system does not need explicit task
boundaries, replay buffers, or parameter protection;
continual learning emerges from the physics of self-
organization.
Self-Organized Criticality.The autonomous
convergence to the edge of chaos validates that
gradient-based structural learning implements a uni-
versal computational principle, suggesting a connec-
tion between the FEP and theories of computation
at criticality.
8.3 Limitations
Static Benchmark Performance.On standard
benchmarks like Mackey-Glass, the model achieves
21
NRMSE 0.1215, respectable for an online, adaptive
system but not state-of-the-art compared to static
models optimized for single-task performance. This
is expected: the system trades peak performance
for continual learning capability. Exploring whether
TFM-guided architecture search can improve static
benchmarks remains for future work.
Block-Level Granularity.Structural credit is as-
signed at the block level (ℓ= 32 neurons), not
the synapse level. While this appears sufficient for
the tasks tested, it precludes finer-grained topologi-
cal adaptations. Investigating whether synapse-level
TFM signals can be computed efficiently is an area
for future research.
Convergence Theory.While we establish exact-
ness at equilibrium, formal analysis of convergence
rates for the coupled synaptic and structural dynam-
ics remains open. This is challenging because the
dynamics operate on vastly different timescales, cre-
ating a singular perturbation problem (Bertschinger
and Natschl¨ ager, 2004).
Extension to Active Inference.The current
work focuses on passive inference (prediction). The
natural extension is to active inference, where actions
are selected to minimize expected future free energy
(Friston and Ao, 2012; Parr and Friston, 2020). The
TFM framework extends naturally: expected gradi-
ents over action sequences guide structural allocation
for policy learning. Our codebase contains a fully
implemented ‘ActiveInferenceAgent‘ that shows this
extension.
Biological Plausibility.While the three-factor
learning rule and eligibility traces are biologically
plausible (Fr´ emaux and Gerstner, 2016; Gerstner
et al., 2018), some aspects remain abstract (e.g.,
block-level averaging for the TFM). Future work
should investigate whether finer-grained local mech-
anisms can approximate the TFM computation.
8.4 Broader Implications
Neuroscience.The work suggests that biological
learning may be more exact than previously thought.
If feedback alignment converges (as we show), the
brain does not need symmetric feedback; it can learn
to provide exact gradients asymptotically. This re-
solves the weight transport problem without requir-
ing implausible biological mechanisms.
Machine Learning.The TFM provides a practi-
cal method for differentiable architecture search that
scales to large networks. Unlike NAS methods that
train thousands of candidate architectures (Zoph and
Le, 2017; Real et al., 2019), TFM-guided growth and
pruning perform gradient-based search online during
training.
Artificial Life.The system’s ability to maintain
itself at criticality, recover from catastrophic damage,
and allocate resources to minimize surprise suggests
it has crossed a threshold from simulating intelligence
to instantiating the physical principles that underlie
it, with implications for understanding the transition
from non-living to living systems (Kauffman, 1993).
9 Conclusion
We have presented a neural architecture that instan-
tiates the Free Energy Principle through hierarchi-
cal gradient decomposition. The system maintains
its non-equilibrium steady-state by minimizing vari-
ational free energy across three nested levels: spatial
credit via feedback alignment, temporal credit via el-
igibility traces, and structural credit via the Trophic
Field Map.
Our central empirical claim is that structural credit
assignment can be exact, not approximate. The
0.9693 TFM-oracle correlation validates this, show-
ing that local signals can precisely estimate which
connections minimize surprise. This exact struc-
tural inference produces stable, compositional at-
tractor landscapes that support continual learning:
98.6% task retention, 69.8% positive transfer, and
autonomous recovery from 75% structural ablation.
22
The work connects the physics of self-organization
(Prigogine’s dissipative structures), the information
geometry of inference (Friston’s Free Energy Princi-
ple), and the computational mechanisms of memory
(Hopfield’s attractor networks). By showing these
frameworks compose into a unified account of biolog-
ical intelligence, we demonstrate that the FEP pro-
vides a constructive algorithm that can be scaled to
large networks.
The system performs exact hierarchical inference
on a generative model where structure is itself part
of the inference process. The TFM is a quantity de-
rived from first principles: the expected gradient on
free energy. Structural plasticity guided by the TFM
implements Bayesian model reduction, pruning con-
nections with insufficient evidence and growing con-
nections where the gradient predicts they will reduce
surprise.
Exact local credit assignment (and by extension,
the full Free Energy Principle) can be implemented
in a scalable, biologically plausible architecture. The
brain’s mechanisms for learning may be less of an ap-
proximation and more of an exact, elegant solution
to the problem of maintaining a self-organizing dissi-
pative structure that persists by minimizing its own
surprise.
The framework extends naturally to active infer-
ence, where the TFM guides policy structure. The
accompanying codebase includes an Active Inference
agent implementation demonstrating this extension.
Open questions include formal analysis of conver-
gence dynamics and finer-grained mechanisms for
synapse-level structural credit.
By framing neural learning as the self-organization
of a dissipative system minimizing free energy, we
move beyond viewing brains as computers executing
algorithms to understanding them as physical sys-
tems instantiating a universal principle. Intelligence
is a state of matter.
References
Daniel J Amit, Hanoch Gutfreund, and Haim Som-
polinsky. Spin-glass models of neural networks.
Physical Review A, 32(2):1007–1018, 1985a. doi:
10.1103/PhysRevA.32.1007.
Daniel J Amit, Hanoch Gutfreund, and Haim Som-
polinsky. Storing infinite numbers of patterns in
a spin-glass model of neural networks.Physi-
cal Review Letters, 55(14):1530–1533, 1985b. doi:
10.1103/PhysRevLett.55.1530.
Per Bak, Chao Tang, and Kurt Wiesenfeld. Self-
organized criticality: An explanation of the 1/f
noise.Physical Review Letters, 59(4):381–384,
1987. doi: 10.1103/PhysRevLett.59.381.
Andre M Bastos, W Martin Usrey, Rick A Adams,
George R Mangun, Pascal Fries, and Karl J Fris-
ton. Canonical microcircuits for predictive cod-
ing.Neuron, 76(4):695–711, 2012. doi: 10.1016/j.
neuron.2012.10.038.
John M Beggs and Dietmar Plenz. Neuronal
avalanches in neocortical circuits.Journal of Neu-
roscience, 23(35):11167–11177, 2003. doi: 10.1523/
JNEUROSCI.23-35-11167.2003.
Marcus K Benna and Stefano Fusi. Computational
principles of synaptic memory consolidation.Na-
ture Neuroscience, 19(12):1697–1706, 2016. doi:
10.1038/nn.4401.
Nils Bertschinger and Thomas Natschl¨ ager. Real-
time computation at the edge of chaos in recurrent
neural networks.Neural Computation, 16(7):1413–
1436, 2004. doi: 10.1162/089976604323057443.
Rafal Bogacz. A tutorial on the free-energy frame-
work for modelling perception and learning.Jour-
nal of Mathematical Psychology, 76:198–211, 2017.
doi: 10.1016/j.jmp.2015.11.003.
Christopher L Buckley, Chang Sub Kim, Simon Mc-
Gregor, and Anil K Seth. The free energy prin-
ciple for action and perception: A mathematical
review.Journal of Mathematical Psychology, 81:
55–79, 2017. doi: 10.1016/j.jmp.2017.09.004.
Salvatore Chirumbolo and Antonio Vella. The un-
derlying dynamics of life and its evolution: A
23
prigogine-inspired informational dissipative sys-
tem, 2024. URLhttps://arxiv.org/abs/2412.
02459. Submitted December 3, 2024; Last revised
February 19, 2025.
Peter Dayan, Geoffrey E Hinton, Radford M Neal,
and Richard S Zemel. The helmholtz machine.
Neural Computation, 7(5):889–904, 1995. doi:
10.1162/neco.1995.7.5.889.
Thomas Elsken, Jan Hendrik Metzen, and Frank
Hutter. Neural architecture search: A survey.
Journal of Machine Learning Research, 20(55):1–
21, 2019.
Nicolas Fr´ emaux and Wulfram Gerstner. Neuromod-
ulated spike-timing-dependent plasticity, and the-
ory of three-factor learning rules.Frontiers in Neu-
ral Circuits, 9:85, 2016. doi: 10.3389/fncir.2015.
00085. Published online January 19, 2016.
Robert M French. Catastrophic forgetting in con-
nectionist networks.Trends in Cognitive Sciences,
3(4):128–135, 1999. doi: 10.1016/S1364-6613(99)
01294-2.
Karl Friston. The free-energy principle: a rough
guide to the brain?Trends in Cognitive Sciences,
13(7):293–301, 2009. doi: 10.1016/j.tics.2009.04.
005.
Karl Friston. The free-energy principle: A unified
brain theory?Nature Reviews Neuroscience, 11
(2):127–138, 2010. doi: 10.1038/nrn2787.
Karl Friston. A free energy principle for a particu-
lar physics, 2019. URLhttps://arxiv.org/abs/
1906.10184.
Karl Friston and Ping Ao. Free energy, value, and at-
tractors.Computational and Mathematical Meth-
ods in Medicine, 2012:937860, 2012. doi: 10.1155/
2012/937860.
Karl Friston, James Kilner, and Lee Harrison. A
free energy principle for the brain.Journal of
Physiology-Paris, 100(1-3):70–87, 2006. doi: 10.
1016/j.jphysparis.2006.10.001.
Karl Friston, Francesco Rigoli, Dimitri Ognibene,
Christoph Mathys, Thomas Fitzgerald, and Gio-
vanni Pezzulo. Active inference and epistemic
value.Cognitive Neuroscience, 6(4):187–214, 2015.
doi: 10.1080/17588928.2015.1020053.
Karl Friston, Thomas FitzGerald, Francesco Rigoli,
Philipp Schwartenbeck, John O’Doherty, and Gio-
vanni Pezzulo. Active inference and learning.Neu-
roscience & Biobehavioral Reviews, 68:862–879,
2016. doi: 10.1016/j.neubiorev.2016.06.022.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor
Heins, Kai Ueltzh¨ offer, Grigorios A Pavliotis, and
Thomas Parr. The free energy principle made sim-
pler but not too simple.Physics Reports, 1024:
1–29, 2023. doi: 10.1016/j.physrep.2023.07.001.
Karl J. Friston, Tamara Shiner, Thomas FitzGer-
ald, Joseph M. Galea, Rick Adams, Harriet Brown,
Raymond J. Dolan, Rosalyn Moran, Klaas Enno
Stephan, and Sven Bestmann. Dopamine, affor-
dance and active inference.PLOS Computational
Biology, 8(1):1–20, 01 2012. doi: 10.1371/journal.
pcbi.1002327. URLhttps://doi.org/10.1371/
journal.pcbi.1002327.
Stefano Fusi, Patrick J Drew, and Larry F Ab-
bott. Cascade models of synaptically stored mem-
ories.Neuron, 45(4):599–611, 2005. doi: 10.1016/
j.neuron.2005.02.001.
Wulfram Gerstner, Marco Lehmann, Vasiliki Liakoni,
Dane Corneil, and Johanni Brea. Eligibility traces
and plasticity on behavioral time scales: experi-
mental support of neohebbian three-factor learn-
ing rules.Frontiers in Neural Circuits, 12:53, 2018.
doi: 10.3389/fncir.2018.00053.
Dhawal Gupta, Scott M. Jordan, Shreyas Chaud-
hari, Bo Liu, Philip S. Thomas, and Bruno Castro
da Silva. From past to future: Rethinking eligibil-
ity traces, 2023. URLhttps://arxiv.org/abs/
2312.12972.
Song Han, Jeff Pool, John Tran, and William J. Dally.
Learning both weights and connections for efficient
neural networks, 2015. URLhttps://arxiv.org/
abs/1506.02626.
24
Simon Haykin.Kalman Filtering and Neural Net-
works. Wiley-Interscience, New York, NY, 2001.
ISBN 978-0471369981. doi: 10.1002/0471221546.
Geoffrey E Hinton and Richard S Zemel. Autoen-
coders, minimum description length and helmholtz
free energy. InAdvances in Neural Information
Processing Systems, volume 6, pages 3–10. Morgan
Kaufmann, 1993.
Jakob Hohwy. The self-evidencing brain.Noˆ us, 50
(2):259–285, 2016. doi: 10.1111/nous.12062.
John J Hopfield. Neural networks and physical sys-
tems with emergent collective computational abil-
ities.Proceedings of the National Academy of Sci-
ences, 79(8):2554–2558, 1982. doi: 10.1073/pnas.
79.8.2554.
Herbert Jaeger. The ”echo state” approach to
analysing and training recurrent neural networks
– with an erratum note. Technical Report GMD
Report 148, German National Research Center for
Information Technology, Fraunhofer Institute for
Autonomous Intelligent Systems, 2001. Revised
version published January 26, 2010.
Stuart A Kauffman.The Origins of Order: Self-
Organization and Selection in Evolution. Oxford
University Press, Oxford, 1993. ISBN 978-0-19-
505811-6.
James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A Rusu, Kieran Milan, John Quan, Tiago
Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran,
and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks.Proceedings of the
National Academy of Sciences, 114(13):3521–3526,
2017. doi: 10.1073/pnas.1611835114.
David C Knill and Alexandre Pouget. The bayesian
brain: the role of uncertainty in neural coding
and computation.Trends in Neurosciences, 27(12):
712–719, 2004. doi: 10.1016/j.tins.2004.10.007.
Dmitry Krotov and John J Hopfield. Dense asso-
ciative memory for pattern recognition. InAd-
vances in Neural Information Processing Systems,
volume 29, pages 1172–1180, 2016.
Dmitry Krotov and John J Hopfield. Large asso-
ciative memory problem in neurobiology and ma-
chine learning. InProceedings of the 37th Inter-
national Conference on Machine Learning, pages
5577–5587. PMLR, 2020.
Christopher G Langton. Computation at the edge of
chaos: Phase transitions and emergent computa-
tion.Physica D: Nonlinear Phenomena, 42(1-3):
12–37, 1990. doi: 10.1016/0167-2789(90)90064-V.
Timothy P Lillicrap, Daniel Cownden, Douglas B
Tweed, and Colin J Akerman. Random synap-
tic feedback weights support error backpropaga-
tion for deep learning.Nature Communications,
7:13276, 2016. doi: 10.1038/ncomms13276.
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
DARTS: Differentiable architecture search. In
International Conference on Learning Represen-
tations, 2019. URLhttps://openreview.net/
forum?id=S1eYHoC5FX.
Wolfgang Maass, Thomas Natschl¨ ager, and Henry
Markram. Real-time computing without stable
states: A new framework for neural computa-
tion based on perturbations.Neural Compu-
tation, 14(11):2531–2560, 2002. doi: 10.1162/
089976602760407955.
James L McClelland, Bruce L McNaughton, and
Randall C O’Reilly. Why there are complemen-
tary learning systems in the hippocampus and neo-
cortex: insights from the successes and failures
of connectionist models of learning and memory.
Psychological Review, 102(3):419–457, 1995. doi:
10.1037/0033-295X.102.3.419.
Michael McCloskey and Neal J Cohen. Catastrophic
interference in connectionist networks: The se-
quential learning problem. InPsychology of Learn-
ing and Motivation, volume 24, pages 109–165. El-
sevier, 1989. doi: 10.1016/S0079-7421(08)60536-8.
25
Decebal Constantin Mocanu, Elena Mocanu, Pe-
ter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of arti-
ficial neural networks with adaptive sparse con-
nectivity inspired by network science.Nature
Communications, 9:2383, 2018. doi: 10.1038/
s41467-018-04316-3.
Theodore H. Moskovitz, Ashok Litwin-Kumar, and
L. F. Abbott. Feedback alignment in deep con-
volutional networks, 2019. URLhttps://arxiv.
org/abs/1812.06488.
Gregoire Nicolis and Ilya Prigogine.Self-
Organization in Nonequilibrium Systems: From
Dissipative Structures to Order through Fluctua-
tions. Wiley-Interscience, New York, 1977. ISBN
978-0471024019.
Arild Nøkland. Direct feedback alignment provides
learning in deep neural networks. InAdvances
in Neural Information Processing Systems, vol-
ume 29, pages 1037–1045, 2016.
Randolph J Nudo. Plasticity.NeuroRx, 3(4):420–427,
2006. doi: 10.1016/j.nurx.2006.07.006.
Thomas Parr and Karl J Friston. Attention or
salience?Current Opinion in Psychology, 29:1–
5, 2020. doi: 10.1016/j.copsyc.2018.10.006.
Judea Pearl.Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA, 1988. ISBN 978-
1558604790.
Tony A Plate. Holographic reduced representations.
IEEE Transactions on Neural Networks, 6(3):623–
641, 1995. doi: 10.1109/72.377968.
Dietmar Plenz and Tara C Thiagarajan. The organiz-
ing principles of neuronal avalanches: cell assem-
blies in the cortex?Trends in Neurosciences, 30
(3):101–110, 2007. doi: 10.1016/j.tins.2007.01.003.
Ilya Prigogine. Time, structure, and fluctuations.
Science, 201(4358):777–785, 1977. doi: 10.1126/
science.201.4358.777.
Rajesh PN Rao and Dana H Ballard. Predictive
coding in the visual cortex: a functional inter-
pretation of some extra-classical receptive-field ef-
fects.Nature Neuroscience, 2(1):79–87, 1999. doi:
10.1038/4580.
Esteban Real, Alok Aggarwal, Yanping Huang, and
Quoc V Le. Regularized evolution for image clas-
sifier architecture search.Proceedings of the AAAI
Conference on Artificial Intelligence, 33(01):4780–
4789, 2019. doi: 10.1609/aaai.v33i01.33014780.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning representations by back-
propagating errors.Nature, 323(6088):533–536,
1986. doi: 10.1038/323533a0.
Noor Sajid, Philip J Ball, Thomas Parr, and Karl J
Friston. Active inference: Demystified and com-
pared.Neural Computation, 33(3):674–712, 2021.
doi: 10.1162/neco a 01357.
Erwin Schr¨ odinger.What is Life? The Physical
Aspect of the Living Cell. Cambridge University
Press, Cambridge, 1944.
Biswa Sengupta, Martin B Stemmler, and Karl J
Friston. Information and efficiency in the nervous
system—a synthesis.PLoS Computational Biol-
ogy, 9(7):e1003157, 2013. doi: 10.1371/journal.
pcbi.1003157.
Woodrow L Shew, Hongdian Yang, Thomas Peter-
mann, Rajarshi Roy, and Dietmar Plenz. Neu-
ronal avalanches imply maximum dynamic range
in cortical networks at criticality.Journal of Neu-
roscience, 29(49):15595–15600, 2009. doi: 10.1523/
JNEUROSCI.3864-09.2009.
Paul Smolensky. Tensor product variable binding
and the representation of symbolic structures in
connectionist systems.Artificial Intelligence, 46
(1-2):159–216, 1990. doi: 10.1016/0004-3702(90)
90007-M.
Larry R Squire. Memory systems of the brain: a brief
history and current perspective.Neurobiology of
Learning and Memory, 82(3):171–177, 2004. doi:
10.1016/j.nlm.2004.06.005.
26
Richard S Sutton. Learning to predict by the meth-
ods of temporal differences.Machine Learning, 3
(1):9–44, 1988. doi: 10.1007/BF00115009.
Richard S Sutton and Andrew G Barto.Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, 1998. ISBN 0-262-19398-1.
Richard Stuart Sutton.Temporal credit assign-
ment in reinforcement learning. PhD thesis, 1984.
AAI8410337.
Nassim Nicholas Taleb.Antifragile: Things That
Gain from Disorder. Random House, New York,
2012. ISBN 978-1-4000-6782-4.
Gina G Turrigiano. Homeostatic plasticity in neu-
ronal networks: the more things change, the more
they stay the same.Trends in Neurosciences, 22
(5):221–227, 1999. doi: 10.1016/S0166-2236(98)
01341-1.
Chris S Wallace and David L Dowe. Minimum mes-
sage length and kolmogorov complexity.The Com-
puter Journal, 42(4):270–283, 1999. doi: 10.1093/
comjnl/42.4.270.
Paul J Werbos. Backpropagation through time:
What it does and how to do it.Proceedings of
the IEEE, 78(10):1550–1560, 1990. doi: 10.1109/
5.58337.
Colin White, Mahmoud Safari, Rhea Sukthanker,
Binxin Ru, Thomas Elsken, Arber Zela, De-
badeepta Dey, and Frank Hutter. Neural archi-
tecture search: Insights from 1000 papers, 2023.
URLhttps://arxiv.org/abs/2301.08727.
Christian Xerri. Plasticity of cortical maps: multi-
ple triggers for adaptive reorganization following
brain damage and spinal cord injury.The Neu-
roscientist, 18(2):133–148, 2012. doi: 10.1177/
1073858410397894.
Friedemann Zenke, Ben Poole, and Surya Ganguli.
Continual learning through synaptic intelligence.
InProceedings of the 34th International Conference
on Machine Learning, volume 70, pages 3987–3995.
PMLR, 2017.
Barret Zoph and Quoc V Le. Neural architecture
search with reinforcement learning. In5th Inter-
national Conference on Learning Representations
(ICLR), 2017.
27
Figure 12: Performance benchmarks for block-sparse networks. Top row: throughput and latency vs. net-
work size for various batch/substep configurations. Throughput peaks at batch 64 with 1 substep, achieving
4712 items/s for 327,680 neurons. Bottom heatmaps show the tradeoff between batch size and evolution
substeps for the largest network. Smaller batches with more substeps minimize latency (∼11ms), while
larger batches maximize throughput (∼4700 items/s).
28
Figure 13: 3D visualization of throughput and la-
tency layer cakes. Left: throughput decreases with
more substeps and smaller batches. Right: latency
increases with larger batches and more substeps.
Each layer represents a different network size, show-
ing consistent scaling behavior across architectures
from 16K to 327K neurons.
29