Exploiting Unintended Feature Leakage in Collaborative Learning∗
LucaMelis† CongzhengSong† EmilianoDeCristofaro VitalyShmatikov
UCL CornellUniversity UCL&AlanTuringInstitute CornellTech
luca.melis.14@alumni.ucl.ac.uk cs2296@cornell.edu e.decristofaro@ucl.ac.uk shmat@cs.cornell.edu
Abstract Collaborative training, however, does disclose information
viamodelupdatesthatarebasedonthetrainingdata. Thekey
Collaborativemachinelearningandrelatedtechniquessuchas
questionweinvestigateinthispaperis: whatcanbeinferred
federated learning allow multiple participants, each with his
about a participant’s training dataset from the model up-
own training dataset, to build a joint model by training lo-
datesrevealedduringcollaborativemodeltraining?
callyandperiodicallyexchangingmodelupdates. Wedemon-
Ofcourse,thepurposeofMListodiscovernewinformation
strate that these updates leak unintended information about
aboutthedata. AnyusefulMLmodelrevealssomethingabout
participants’ training data and develop passive and active in-
the population from which the training data was drawn. For
ference attacks to exploit this leakage. First, we show that
example,inadditiontoaccuratelyclassifyingitsinputs,aclas-
anadversarialparticipantcaninferthepresenceofexactdata
sifier model may reveal the features that characterize a given
points—for example, specific locations—in others’ training
classorhelpconstructdatapointsthatbelongtothisclass. In
data (i.e., membership inference). Then, we show how this
this paper, we focus on inferring “unintended” features, i.e.,
adversary can infer properties that hold only for a subset of
propertiesthatholdforcertainsubsetsofthetrainingdata,but
thetrainingdataandareindependentofthepropertiesthatthe
notgenericallyforallclassmembers.
jointmodelaimstocapture. Forexample,hecaninferwhena
Thebasicprivacyviolationinthissettingismembershipin-
specificpersonfirstappearsinthephotosusedtotrainabinary
ference: givenanexactdatapoint,determineifitwasusedto
genderclassifier. Weevaluateourattacksonavarietyoftasks,
trainthemodel. Priorworkdescribedpassiveandactivemem-
datasets,andlearningconfigurations,analyzetheirlimitations,
bershipinferenceattacksagainstMLmodels[24,53],butcol-
anddiscusspossibledefenses.
laborative learning presents interesting new avenues for such
inferences. Forexample, weshowthatanadversarialpartici-
1 Introduction pant can infer whether a specific location profile was used to
trainagenderclassifierontheFourSquarelocationdataset[64]
Collaborative machine learning (ML) has recently emerged with0.99precisionandperfectrecall.
as an alternative to conventional ML methodologies where We then investigate passive and active property inference
all training data is pooled and the model is trained on this attacks that allow an adversarial participant in collaborative
joint pool. It allows two or more participants, each with his learningtoinferpropertiesofotherparticipants’trainingdata
own training dataset, to construct a joint model. Each par- thatarenottrueoftheclassasawhole,orevenindependentof
ticipant trains a local model on his own data and periodically thefeaturesthatcharacterizetheclassesofthejointmodel.We
exchanges model parameters, updates to these parameters, or alsostudyvariationssuchasinferringwhenapropertyappears
partiallyconstructedmodelswiththeotherparticipants. anddisappearsinthedataduringtraining—forexample,iden-
Several architectures have been proposed for distributed, tifying when a certain person first appears in the photos used
collaborative, and federated learning [9, 11, 33, 38, 62, 68]: totrainagenericgenderclassifier.
with and without a central server, with different ways of ag- For a variety of datasets and ML tasks, we demonstrate
gregatingmodels, etc. Themaingoalistoimprovethetrain- successfulinferenceattacksagainsttwo-partyandmulti-party
ingspeedandreduceoverheads, butprotectingprivacyofthe collaborativelearningbasedon[52]andmulti-partyfederated
participants’ training data is also an important motivation for learningbasedon[35].Forexample,whenthemodelistrained
several recently proposed techniques [35, 52]. Because the ontheLFWdataset[28]torecognizegenderorrace,weinfer
training data never leave the participants’ machines, collabo- whetherpeopleinthetrainingphotoswearglasses—aproperty
rative learning may be a good match for the scenarios where thatisuncorrelatedwiththemaintask.Bycontrast,priorprop-
thisdataissensitive(e.g.,health-carerecords,privateimages, ertyinferenceattacks[2,25]inferonlypropertiesthatcharac-
personallyidentifiableinformation,etc.). Compellingapplica- terize an entire class. We discuss this critical distinction in
tionsincludetrainingofpredictivekeyboardsoncharacterse- detailinSection3.
quencesthatuserstypeontheirsmartphones[35],orusingdata Our key observation, concretely illustrated by our experi-
from multiple hospitals to develop predictive models for pa- ments,isthatmoderndeep-learningmodelscomeupwithsep-
tientsurvival[29]andsideeffectsofmedicaltreatments[30]. arateinternalrepresentationsofallkindsoffeatures, someof
whichareindependentofthetaskbeinglearned. These“unin-
∗InProceedingsof40thIEEESymposiumonSecurity&Privacy(S&P2019).
†Authorscontributedequally. tended” features leak information about participants’ training
1
8102
voN
1
]RC.sc[
3v94040.5081:viXra
data. We also demonstrate that an active adversary can use Algorithm1ParameterserverwithsynchronizedSGD
multi-tasklearningtotrickthejointmodelintolearningabet- Serverexecutes:
terinternalseparationofthefeaturesthatareofinteresttohim Initializeθ
0
andthusextractevenmoreinformation. fort=1toT do
Some of our inference attacks have direct privacy implica- foreachclientkdo
tions. Forexample,whentrainingabinarygenderclassifieron g t k ←ClientUpdate(θ t−1 )
the FaceScrub [40] dataset, we infer with high accuracy (0.9 endfor
θ ←θ −η (cid:80) gk (cid:46)synchronizedgradientupdates
AUC score) that a certain person appears in a single training t t−1 k t
endfor
batchevenifhalfofthephotosinthebatchdepictotherpeo-
ple.WhentrainingagenericsentimentanalysismodelonYelp
ClientUpdate(θ):
healthcare-relatedreviews,weinferthespecialtyofthedoctor
Selectbatchbfromclient’sdata
beingreviewedwithperfectaccuracy. OnanothersetofYelp
returnlocalgradients∇L(b;θ)
reviews, we identify the author even if their reviews account
forlessthanathirdofthebatch.
We also measure the performance of our attacks vis-a`-vis
negative gradient of the objective function with respect to θ
the number of participants (see Section 7). On the image-
and scaled by the learning rate η. Training finishes when the
classification tasks, AUC degrades once the number of par-
modelhasconvergedtoalocalminimum,wherethegradientis
ticipants exceeds a dozen or so. On sentiment-analysis tasks
closetozero. Thetrainedmodelistestedusingheld-outdata,
withYelpreviews,AUCofauthoridentificationremainshigh
whichwasnotusedduringtraining. Astandardmetricistest
formanyauthorsevenwith30participants.
accuracy, i.e., the percentage of held-out data points that are
Federated learning with model averaging [35] does not re-
classifiedcorrectly.
vealindividualgradientupdates,greatlyreducingtheinforma-
Hyperparameters. MostmodernMLalgorithmshaveasetof
tionavailabletotheadversary. Wedemonstratesuccessfulat-
tunablehyperparameters, distinctfromthemodelparameters.
tackseveninthissetting,e.g.,inferringthatphotosofacertain
Theycontrolthenumberoftrainingiterations,theratioofthe
personappearinthetrainingdata.
regularization term in the loss function (its purpose is to pre-
Finally, we evaluate possible defenses—sharing fewer
ventoverfitting,i.e.,amodelingerrorthatoccurswhenafunc-
gradients, reducing the dimensionality of the input space,
tionistoocloselyfittedtoalimitedsetofdatapoints),thesize
dropout—and find that they do not effectively thwart our at-
ofthetrainingbatches,etc.
tacks. We also attempt to use participant-level different pri-
vacy [36], which, however, is geared to work with thousands Deep learning (DL). A family of ML models known as
ofusers,andthejointmodelfailstoconvergeinoursetting. deep learning recently became very popular for many ML
tasks, especially related to computer vision and image recog-
nition [32, 51]. DL models are made of layers of non-linear
2 Background mappingsfrominputtointermediatehiddenstatesandthento
output. Each connection between layers has a floating-point
2.1 Machinelearning(ML) weightmatrixasparameters. Theseweightsareupdateddur-
AnMLmodelisafunctionf : X (cid:55)→ Y parameterizedby ing training. The topology of the connections between layers
θ
asetofparametersθ, whereX denotestheinput(orfeature) istask-dependentandimportantfortheaccuracyofthemodel.
space,andY theoutputspace.
2.2 Collaborativelearning
Inthispaper,wefocusonthesupervisedlearningofclassi-
ficationtasks. Thetrainingdataisasetofdatapointslabeled Training a deep neural network on a large dataset can be
withtheircorrectclasses.Weworkwithmodelsthattakeasin- time- and resource-consuming. A common scaling approach
putimagesortext(i.e.,sequencesofwords)andoutputaclass is to partition the training dataset, concurrently train separate
label. Tofindtheoptimalsetofparametersthatfitsthetrain- modelsoneachsubset,andexchangeparametersviaaparam-
ing data, the training algorithm optimizes the objective (loss) eterserver[9,11]. Duringtraining,eachlocalmodelpullsthe
function, which penalizes the model when it outputs a wrong parametersfromthisserver,calculatestheupdatesbasedonits
labelonadatapoint.WeuseL(x,y;θ)todenotethelosscom- currentbatchoftrainingdata,thenpushestheseupdatesback
putedonadatapoint(x,y)giventhemodelparametersθ,and totheserver,whichupdatestheglobalparameters.
L(b;θ) to denote the average loss computed on a batch b of Collaborative learning may also involve participants who
datapoints. want to hide their training data from each other. We review
twoarchitecturesforprivacy-preservingcollaborativelearning
StochasticGradientDescent(SGD). Therearemanymethods
basedon,respectively,[52]and[35].
tooptimizetheobjectivefunction. Stochasticgradientdescent
(SGD) and its variants are commonly used to train artificial Collaborative learning with synchronized gradient
neuralnetworks,butourinferencemethodologyisnotspecific updates. Algorithm 1 shows collaborative learning with
to SGD. SGD is an iterative method where at each step the synchronized gradient updates [52]. In every iteration of
optimizer receives a small batch of the training data and up- training, each participant downloads the global model from
datesthemodelparametersθaccordingtothedirectionofthe theparameterserver,locallycomputesgradientupdatesbased
2
Algorithm2Federatedlearningwithmodelaveraging 3 Reasoning about Privacy in Machine
Serverexecutes: Learning
Initializeθ
0
m←max(C·K,1) If a machine learning (ML) model is useful, it must reveal
fort=1toT do information about the data on which it was trained [13]. To
S t ←(randomsetofmclients) argue that the training process and/or the resulting model vi-
foreachclientk∈S t do olate “privacy,” it is not enough to show that the adversary
θk ←ClientUpdate(θ )
t t−1 learns something new about the training inputs. At the very
endfor
θ ← (cid:80) nkθk (cid:46)averaginglocalmodels least, the adversary must learn more about the training inputs
t k n t thanaboutothermembersoftheirrespectiveclasses. Toposi-
endfor
tionourcontributionsinthecontextofrelatedwork(surveyed
inSection10)andmotivatetheneedtostudyunintendedfea-
ClientUpdate(θ):
tureleakage,wediscussseveraltypesofadversarialinference
foreachlocaliterationdo
foreachbatchbinclient’ssplitdo previouslyconsideredintheresearchliterature.
θ←θ−η∇L(b;θ)
endfor 3.1 Inferringclassrepresentatives
endfor
Given black-box access to a classifier model, model inver-
returnlocalmodelθ
sion attacks [16] infer features that characterize each class,
makingitpossibletoconstructrepresentativesoftheseclasses.
In the special case—and only in this special case—where
on one batch of his training data, and sends the updates to allclassmembersaresimilar,theresultsofmodelinversionare
the server. The server waits for the gradient updates from all similartothetrainingdata.Forexample,inafacialrecognition
participants and then applies the aggregated updates to the modelwhereeachclasscorrespondstoasingleindividual,all
globalmodelusingstochasticgradientdescent(SGD). classmembersdepictthesameperson. Therefore,theoutputs
of model inversion are visually similar to any image of that
In [52], each client may share only a fraction of his gradi-
person,includingthetrainingphotos. Iftheclassmembersare
ents. We evaluate if this mitigates our attacks in Section 8.1.
not all visually similar, the results of model inversion do not
Furthermore, [52]suggestsdifferentialprivacytoprotectgra-
looklikethetrainingdata[53].
dient updates. We do not include differential privacy in our
If the adversary actively participates in training the model
experiments. By definition, record-level differential privacy
(as in the collaborative and federated learning scenarios con-
bounds the success of membership inference, but does not
sideredinthispaper),hecanuseGANs[22]toconstructclass
prevent property inference that applies to a group of train-
representatives,asdonebyHitajetal.[25].Onlyinthespecial
ingrecords. Participant-leveldifferentialprivacy,ontheother
case where all class members are similar, GAN-constructed
hand, bounds the success of all attacks considered in this pa-
representativesaresimilartothetrainingdata.Forexample,all
per, but we are not aware of any participant-level differential
handwrittenimagesofthedigit‘9’arevisuallysimilar. There-
privacy mechanism that enables collaborative learning of an
fore,aGAN-constructedimageforthe‘9’classlookssimilar
accurate model with a small number of participants. We dis-
toanyimageofdigit9,includingthetrainingimages. Inafa-
cussthisfurtherinSection8.4.
cialrecognitionmodel,too,allclassmembersdepictthesame
person. Hence, a GAN-constructed face looks similar to any
Federated learning with model averaging. Algorithm 2
imageofthatperson,includingthetrainingphotos.
shows the federated learning algorithm [35]. We set C, the
fraction of the participants who update the model in each Notethatneithertechniquereconstructsactualtrainingin-
round,to1(i.e.,theservertakesupdatesfromallparticipants), puts. Infact,thereisnoevidencethatGANs,asusedin[25],
to simplify our experiments and because we ignore the effi- can even distinguish between a training input and a random
ciencyofthelearningprotocolwhenanalyzingtheleakage. memberofthesameclass.
Data points produced by model inversion and GANs are
Ineachround,thek-thparticipantlocallytakesseveralsteps
similartothetraininginputsonlyifallclassmembersaresim-
ofSGDonthecurrentmodelusinghisentiretrainingdataset
ilar,asisthecaseforMNIST(thedatasetofhandwrittendigits
of size nk (i.e., the globally visible updates are based not on
used in [25]) and facial recognition. This simply shows that
batchesbutonparticipants’entiredatasets). InAlgorithm2,n
ML works as it should. A trained classifier reveals the input
isthetotalsizeofthetrainingdata,i.e.,thesumofallnk.Each
features characteristic of each class, thus enabling the adver-
participant submits the resulting model to the server, which
sary to sample from the class population. For instance, Fig-
computesaweightedaverage. Theserverevaluatestheresult-
ure 1 shows GAN-constructed images for the gender classifi-
ingjointmodelonaheld-outdatasetandstopstrainingwhen
cation task on the LFW dataset, which we use in our experi-
performancestopsimproving.
ments (see Section 6). These images show a generic female
The convergence rate of both collaborative learning ap- face,butthereisnowaytotellfromthemwhetheranimageof
proaches heavily depends on the learning task and the hyper- aspecificfemalewasusedintrainingornot.
parameters(e.g.,numberofparticipantsandbatchsize). Finally, the active attack in [25] works by overfitting the
3
ferwhetherpeopleinBob’sphotoswearglasses—eventhough
wearingglasseshasnocorrelationwithgender. Thereisnole-
gitimatereasonforamodeltoleakthisinformation;itispurely
anartifactofthelearningprocess.
A participant’s contribution to each iteration of collabora-
tive learning is based on a batch of his training data. We in-
fersingle-batchproperties,i.e.,detectthatthedatainagiven
Figure 1: Samples from a GAN attack on a gender classification
batchhasthepropertybutotherbatchesdonot. Wealsoinfer
modelwheretheclassis“female.”
whenapropertyappearsinthetrainingdata. Thishasserious
jointmodel’srepresentationofaclasstoasingleparticipant’s privacyimplications.Forinstance,wecaninferwhenacertain
trainingdata. Thisassumesthattheentiretrainingcorpusfor person starts appearing in a participant’s photos or when the
agivenclassbelongstothatparticipant. Wearenotawareof participantstartsvisitingacertaintypeofdoctors. Finally,we
anydeploymentscenarioforcollaborativelearningwherethis infer properties that characterize a participant’s entire dataset
isthecase. Bycontrast,wefocusonamorerealisticscenario (but not the entire class), e.g., authorship of the texts used to
where the training data for each class are distributed across trainasentiment-analysismodel.
multipleparticipants,althoughtheremaybesignificantdiffer-
encesbetweentheirdatasets.
4 Inference Attacks
3.2 Inferringmembershipintrainingdata
4.1 Threatmodel
The(arguably)simplestprivacybreachis,givenamodeland
WeassumethatK participants(whereK ≥ 2)jointlytrain
an exact data point, inferring whether this point was used to
an ML model using one of the collaborative learning algo-
train the model or not. Membership inference attacks against
rithmsdescribedinSection2.2. Oneoftheparticipantsisthe
aggregate statistics are well-known [14, 27, 50], and recent
adversary. His goal is to infer information about the training
work demonstrated black-box membership inference against
data of another, target participant by analyzing periodic up-
MLmodels[24,34,53,58],asdiscussedinSection10.
datestothejointmodelduringtraining. Multi-party(K > 2)
The ability of an adversary to infer the presence of a spe-
collaborative learning also involves honest participants who
cific data point in a training dataset constitutes an immediate
are neither the adversary, nor the target. In the multi-party
privacythreatifthedatasetisinitselfsensitive. Forexample,
case,theidentitiesoftheparticipantsmaynotbeknowntothe
ifamodelwastrainedontherecordsofpatientswithacertain
adversary. Eveniftheidentitiesareknownbutthemodelsare
disease, learning that an individual’s record was among them
aggregated,theadversarymayinfersomethingaboutthetrain-
directly affects his or her privacy. Membership inference can
ingdatabutnottraceittoaspecificparticipant;wediscussthis
also help demonstrate inappropriate uses of data (e.g., using
furtherinSection9.4.
health-care records to train ML models for unauthorized pur-
Theupdatesthatadversaryobservesandusesforinference
poses [4]), enforce individual rights such as the “right to be
dependonbothK andhowcollaborativetrainingisdone.
forgotten,” and/or detect violations of data-protection regula-
Asinputstohisinferencealgorithms,theadversaryusesthe
tionssuchastheGDPR[19]. Collaborativelearningpresents
model updates revealed in each round of collaborative train-
interestingnewavenuesforsuchinferences.
ing. For synchronized SGD [52] with K = 2, the adversary
observes gradient updates computed on a single batch of the
3.3 Inferringpropertiesoftrainingdata
target’s data. If K > 2, he observes an aggregation of gra-
In collaborative and federated learning, participants’ train- dientupdatesfromallotherparticipants(eachcomputedona
ingdatamaynotbeidenticallydistributed. Federatedlearning singlebatchoftherespectiveparticipant’sdata). Forfederated
isexplicitlydesignedtotakeadvantageofthefactthatpartici- learningwithmodelaveraging[35], theobservedupdatesare
pantsmayhaveprivatetrainingdatathataredifferentfromthe theresultoftwo-stepaggregation: (1)everyparticipantaggre-
publiclyavailabledataforthesameclass[35]. gatesthegradientscomputedoneachlocalbatch, and(2)the
Prior work [2, 16, 25] aimed to infer properties that char- serveraggregatestheupdatesfromallparticipants.
acterizeanentireclass: forexample,givenafacerecognition Forpropertyinference, theadversaryneedsauxiliarytrain-
model where one of the classes is Bob, infer what Bob looks ing data correctly labeled with the property he wants to infer
like (e.g., Bob wears glasses). It is not clear that hiding this (e.g., faceslabeledwithagesifthegoalistoinferages). For
informationinagoodclassifierispossibleordesirable. active property inference (Section 4.5), these auxiliary data
Bycontrast,weaimtoinferpropertiesthataretrueofasub- points must also be labeled for the main task (e.g., faces la-
set of the training inputs but not of the class as a whole. For beledwithidentitiesforafacialrecognitionmodel).
instance, when Bob’s photos are used to train a gender clas-
4.2 Overviewoftheattacks
sifier, we infer that Alice appears in some of the photos. We
especially focus on the properties that are independent of the Figure 2 provides a high-level overview of our inference
class’s characteristic features. In contrast to the face recog- attacks. At each iteration t of training, the adversary down-
nitionexample,where“Bobwearsglasses”isacharacteristic loads the current joint model, calculates gradient updates as
featureofanentireclass,inourgenderclassifierstudywein- prescribed by the collaborative learning algorithm, and sends
4
Figure2:Overviewofinferenceattacksagainstcollaborativelearning.
his own updates to the server. The adversary saves the snap- ∆θ . During training, the attacker collects a vocabulary se-
t
shotofthejointmodelparametersθ . Thedifferencebetween quence[V ,...,V ]. Givenatextrecordr,withwordsV ,he
t 1 T r
the consecutive snapshots ∆θ = θ − θ = (cid:80) ∆θk is cantestifV ⊆ V ,forsometinthevocabularysequence. If
t t t−1 k t r t
equal to the aggregated updates from all participants, hence risintarget’sdataset,thenV willbeincludedinatleastone
r
∆θ −∆θadv aretheaggregatedupdatesfromallparticipants vocabulary from the sequence. The adversary can use this to
t t
otherthantheadversary. decidewhetherrwasamemberornot.
Leakage from the embedding layer. All deep learning mod-
4.4 Passivepropertyinference
els operating on non-numeric data where the input space is
discrete and sparse (e.g., natural-language text or locations) We assume that the adversary has auxiliary data consisting
firstuseanembeddinglayertotransforminputsintoalower- ofthedatapointsthathavethepropertyofinterest(Dadv)and
prop
dimensional vector representation. For convenience, we use datapointsthatdonothavetheproperty(Dadv ). Thesedata
nonprop
word to denote discrete tokens, i.e., actual words or specific points need to be sampled from the same class as the target
locations. LetvocabularyV bethesetofallwords. Eachword participant’sdata,butotherwisecanbeunrelated.
inthetrainingdataismappedtoaword-embeddingvectorvia The intuition behind our attack is that the adversary can
anembeddingmatrixW emb ∈R|V|×d,where|V|isthesizeof leveragethesnapshotsoftheglobalmodeltogenerateaggre-
thevocabularyanddisthedimensionalityoftheembedding. gatedupdatesbasedonthedatawiththepropertyandupdates
During training, the embedding matrix is treated as a pa- basedonthedatawithouttheproperty. Thisproduceslabeled
rameterofthemodelandoptimizedcollaboratively. Thegra- examples, which enable the adversary to train a binary batch
dientoftheembeddinglayerissparsewithrespecttotheinput propertyclassifier thatdeterminesiftheobservedupdatesare
words: given a batch of text, the embedding is updated only basedonthedatawithorwithouttheproperty. Thisattackis
with the words that appear in the batch. The gradients of the passive,i.e.,theadversaryobservestheupdatesandperforms
other words are zeros. This difference directly reveals which inferencewithoutchanginganythinginthelocalorglobalcol-
wordsoccurinthetrainingbatchesusedbythehonestpartici- laborativetrainingprocedure.
pantsduringcollaborativelearning.
Batch property classifier. Algorithm 3 shows how to build a
Leakage from the gradients. In deep learning models, gra- batchpropertyclassifierduringcollaborativetraining. Givena
dientsarecomputedbyback-propagatingthelossthroughthe model snapshot θ , calculate gradients g based on a batch
t prop
entire network from the last to the first layer. Gradients of a with the property badv ⊂ Dadv and g based on a batch
prop prop nonprop
given layer are computed using this layer’s features and the withoutthepropertybadv ⊂Dadv . Onceenoughlabeled
nonprop nonprop
errorfromthelayerabove. Inthecaseofsequentialfullycon- gradientshavebeencollected,trainabinaryclassifierf .
prop
nectedlayersh ,h (h =W ·h ,whereW istheweight
l l+1 l+1 l l l For the property inference attacks that exploit the
matrix),thegradientoferrorEwithrespecttoW iscomputed
l embedding-layergradients(e.g.,theattackontheYelpdataset
as ∂E = ∂E ·h . ThegradientsofW areinnerproductsof
∂Wl ∂hl+1 l l inSection6.2), weusealogisticregressionclassifier. Forall
the error from the layer above and the features h l . Similarly, otherpropertyinferenceattacks,weexperimentedwithlogis-
foraconvolutionallayer,thegradientsoftheweightsarecon- ticregression,gradientboosting,andrandomforests. Random
volutionsoftheerrorfromthelayeraboveandthefeaturesh l . forests with 50 trees performed the best. The input features
Observationsofgradientupdatescanthusbeusedtoinferfea- inthiscasecorrespondtotheobservedgradientupdates. The
turevalues,whichareinturnbasedontheparticipants’private numberofthefeaturesisthusequaltothemodel’sparameters,
trainingdata. whichcanbeverylargeforarealisticmodel. Todownsample
the features representation, we apply the max pooling opera-
4.3 Membershipinference tor [21] on the observed gradient updates. More specifically,
max pooling performs a max filter to non-overlapping subre-
As explained above, the non-zero gradients of the embed-
gions of the initial features representation, thus reducing the
ding layer reveal which words appear in a batch. This helps
computationalcostoftheattack.
infer whether a given text or location appears in the training
dataset or not. Let V be the words included in the updates Inferencealgorithm. Ascollaborativetrainingprogresses,the
t
5
Algorithm3BatchPropertyClassifier Dataset #Records Maintasks Inferencetasks
LFW 13.2k Gender/Smile/Age Race/Eyewear
Inputs:Attacker’sauxiliarydataDadv,Dadv
prop nonprop Eyewear/Race/Hair
Outputs:Batchpropertyclassifierf
prop FaceScrub 18.8k Gender Identity
G ←∅ (cid:46)Positivetrainingdataforpropertyinference
prop PIPA 18.0k Age Gender
G ←∅ (cid:46)Negativetrainingdataforpropertyinference
nonprop CSI 1.4k Sentiment Membership,
fori=1toT do
Region/Gender/Veracity
Receiveθ t fromserver FourSquare 15.5k Gender Membership
RunClientUpdate(θ t ) Yelp-health 17.9k Reviewscore Membership,
Sampleba p d ro v p ⊂D p ad ro v p ,ba n d o v nprop ⊂D n ad o v nprop Doctorspecialty
Calculateg prop =∇L(ba p d ro v p ;θ t ),g nonprop =∇L(ba n d o v nprop ;θ t ) Yelp-author 16.2K Reviewscore Author
G ←G ∪{g }
prop prop prop Table1:Datasetsandtasksusedinourexperiments.
G ←G ∪{g }
nonprop nonprop nonprop
endfor
LabelG prop aspositiveandG nonprop asnegative Our choices of hyperparameters are based on the standard
Trainabinaryclassifierf prop givenG prop ,G nonprop modelsfromtheMLliterature.
LabeledFacesIntheWild(LFW). LFW[28]contains13,233
62x47RGBfaceimagesfor5,749individualswithlabelssuch
adversary observes gradient updates g = ∆θ − ∆θadv.
obs t t asgender,race,age,haircolor,andeyewear.
Forsingle-batchinference,theadversarysimplyfeedstheob-
FaceScrub. FaceScrub[40]contains76,54150x50RGBim-
servedgradientupdatestothebatchpropertyclassifierf .
prop
ages for 530 individuals with the gender label: 52.5% are la-
Thisattackcanbeextendedfromsingle-batchpropertiesto
beled as male, the rest as female. For our experiments, we
thetarget’sentiretrainingdataset.Thebatchpropertyclassifier
selectedasubsetof100individualswiththemostimages,for
f outputs a score in [0,1], indicating the probability that a
prop
atotalof18,809images.
batchhastheproperty.Theadversarycanusetheaveragescore
acrossalliterationstodecidewhetherthetarget’sentiredataset On both LFW and FaceScrub, the collaborative models are
hasthepropertyinquestion. convolutional neural networks (CNN) with three spatial con-
volution layers with 32, 64, and 128 filters, kernel size set to
4.5 Activepropertyinference (3,3), andmaxpoolinglayerswithpoolingsizesetto2, fol-
An active adversary can perform a more powerful attack lowedbytwofullyconnectedlayersofsize256and2. Weuse
byusingmulti-tasklearning. Theadversaryextendshislocal rectified linear unit (ReLU) as the activation function for all
copy of the collaboratively trained model with an augmented layers. Batch size is 32 (except in the experiments where we
property classifier connected to the last layer. He trains this varyit),SGDlearningrateis0.01.
model to simultaneously perform well on the main task and People in Photo Album (PIPA). PIPA [67] contains over
recognize batch properties. On the training data where each 60,000 photos of 2,000 individuals collected from public
record has a main label y and a property label p, the model’s Flickr photo albums. Each image includes one or more peo-
jointlossiscalculatedas: pleandislabeledwiththenumberofpeopleandtheirgender,
age, and race. For our experiments, we selected a subset of
L =α·L(x,y;θ)+(1−α)·L(x,p;θ)
mt 18,000 images with three or fewer people and scaled the raw
imagesto128x128.
During collaborative training, the adversary uploads the up-
The collaborative model for PIPA is a VGG-style [54] 10-
dates ∇ L based on this joint loss, causing the joint model
θ mt layerCNNwithtwoconvolutionblocksconsistingofonecon-
to learn separable representations for the data with and with-
volutional layer and max pooling, followed by three convo-
out the property. As a result, the gradients will be separable
lution blocks consisting of two convolutional layers and max
too(e.g., seeFigure 7inSection 6.5), enabling theadversary
pooling,followedbytwofullyconnectedlayers. Batchsizeis
totellifthetrainingdatahastheproperty.
32,SGDlearningrateis0.01.
This adversary is still “honest-but-curious” in the crypto-
Yelp-health. We extracted health care-related reviews from
graphicparlance. Hefaithfullyfollowsthecollaborativelearn-
the Yelp dataset1 of 5 million reviews of businesses tagged
ing protocol and does not submit any malformed messages.
withnumericratings(1-5)andattributessuchasbusinesstype
The only difference with the passive attack is that this adver-
andlocation. Oursubsetcontains17,938reviewsfor10types
sary performs additional local computations and submits the
ofmedicalspecialists.
resultingvaluesintothecollaborativelearningprotocol. Note
thatthe“honest-but-curious”modeldoesnotconstrainthepar- Yelp-author. WealsoextractedaYelpsubsetwiththereviews
ties’inputvalues,onlytheirmessages. ofthetop10mostprolificreviewers,16,207intotal.
On both Yelp datasets, the model is a recurrent neural net-
workwithaword-embeddinglayerofdimension100. Words
5 Datasets and model architectures inareviewaremappedtoasequenceofword-embeddingvec-
tors, which is fed to a gated recurrent unit (GRU [10]) layer
The datasets, collaborative learning tasks, and adversarial in-
ferencetasksusedinourexperimentsarereportedinTable1. 1https://www.yelp.com/dataset
6
that maps it to a sequence of hidden vectors. We add a fully Yelp-health FourSquare
BatchSize Precision BatchSize Precision
connected classification layer to the last hidden vector of the
32 0.92 100 0.99
sequence. SGDlearningrateis0.05.
64 0.84 200 0.98
FourSquare. In[63,64],Yangetal.collectedaglobaldataset 128 0.75 500 0.91
256 0.66 1,000 0.76
ofFourSquarelocation“check-ins”(userID,time,location,ac-
512 0.62 2,000 0.62
tivity) from April 2012 to September 2013. For our experi-
ments,weselectedasubsetof15,548userswhocheckedinat Table2:Precisionofmembershipinference(recallis1).
least10differentlocationsinNewYorkCityandforwhomwe
know their gender [65]. This yields 528,878 check-ins. The MainT. InferT. Corr. AUC MainT. InferT. Corr. AUC
modelisagenderclassifier,ataskpreviouslystudiedbyPang Gender Black -0.005 1.0 Gender Sunglasses -0.025 1.0
Gender Asian -0.018 0.93 Gender Eyeglasses 0.157 0.94
etal.[44]onsimilardatasets.
Smile Black 0.062 1.0 Smile Sunglasses -0.016 1.0
CLiPS Stylometry Investigation (CSI) Corpus. This annu- Smile Asian 0.047 0.93 Smile Eyeglasses -0.083 0.97
allyexpandeddataset[60]containsstudent-writtenessaysand Age Black -0.084 1.0 Race Sunglasses 0.026 1.0
Age Asian -0.078 0.97 Race Eyeglasses -0.116 0.96
reviews. We obtained 1,412 reviews, equally split between
Eyewear Black 0.034 1.0 Hair Sunglasses -0.013 1.0
Truthful/Deceptive or Positive/Negative and labeled with at-
Eyewear Asian -0.119 0.91 Hair Eyeglasses 0.139 0.96
tributes of the author (gender, age, sexual orientation, region
Table3: AUCscoreofsingle-batchpropertyinferenceonLFW.We
of origin, personality profile) and the document (timestamp,
also report the Pearson correlation between the main task label and
genre,topic,veracity,sentiment). 80%ofthereviewsarewrit-
thepropertylabel.
tenbyfemales,66%byauthorsfromAntwerpen,therestfrom
otherpartsofBelgiumandtheNetherlands.
On both the FourSquare and CSI datasets, the model, which eachbatchofthetarget’sdata,enablingtheadversarytobuild
is based on [31], first uses an embedding layer to turn non- abatchBoW.IfthetestBoWisasubsetofthebatchBoW,the
negative integers (locations indices and word tokens) into adversaryinfersthattheinputofinterestoccursinthebatch.
densevectorsofdimension320,thenappliesthreespatialcon- We evaluate membership inference on the Yelp-health and
volutionallayerswith100filtersandvariablekernelwindows FourSquare datasets with the vocabulary of 5,000 most fre-
ofsize(3,320),(4,320)and(5,320)andmaxpoolinglayers quent words and 30,000 most popular locations, respectively.
withpoolingsizesetto(l−3,1),(l−4,1),and(l−5,1)where Wesplitthedataevenlybetweenthetargetandtheadversary
listhefixedlengthtowhichinputsequencesarepadded. The andtrainacollaborativemodelfor3,000iterations.
hyperparameterlis300onCSIand100onFourSquare. After Table 2 shows the precision of membership inference for
this,themodelhastwofullyconnectedlayersofsize128and different batch sizes. As batch size increases, the adversary
2 for FourSquare and one fully connected layer of size 2 for observes more words in each batch BoW and the attack pro-
CSI. We use RELU as the activation function. Batch size is duces more false positives. Recall is always perfect (i.e., no
100forFourSquare,12forCSI.SGDlearningrateis0.01. falsenegatives)becauseanytruetestBoWmustbecontained
inatleastoneofthebatchBoWsobservedbytheadversary.
6 Two-Party Experiments 6.2 Single-batchpropertyinference
All experiments were performed on a workstation running We call a training batch b nonprop if none of the inputs in it
Ubuntu Server 16.04 LTS equipped with a 3.4GHz CPU i7- havetheproperty,b prop otherwise. Theadversaryaimstoiden-
6800K, 32GB RAM, and an NVIDIA TitanX GPU card. We tify which of the batches are b prop . We split the training data
useMxNet[8]andLasagne[12]toimplementdeepneuralnet- evenly between the target and the adversary and assume that
worksandScikit-learn[48]forconventionalMLmodels. The the same fraction of inputs in both subsets have the property.
sourcecodeisavailableuponrequest. Trainingourinference Duringtraining, 1 ofthetarget’sbatchesincludeonlyinputs
m
modelstakeslessthan60secondsonaverageanddoesnotre- withtheproperty(m=2inthefollowing).
quireaGPU. LFW. Table3 reportstheresults ofsingle-batchproperty in-
WeuseAUCscorestoevaluatetheperformanceofboththe ferenceontheLFWdataset. Wechosepropertiesthatareun-
collaborative model and our property inference attacks. For correlated with the main classification label that the collabo-
membership inference, we report only precision because our rative model is trying to learn. The attack has perfect AUC
decisionrulefromSection4.3isbinaryanddoesnotproduce when the main task is gender classification and the inference
aprobabilityscore. taskis“race:black”(thePearsoncorrelationbetweenthesela-
bels is -0.005). The attack also achieves almost perfect AUC
6.1 Membershipinference
when the main task is “race: black” and the inference task is
TheadversaryfirstbuildsaBagofWords(BoW)represen- “eyewear: sunglasses.” Italsoperformswellonseveralother
tationfortheinputwhosemembershipinthetarget’straining properties,including“eyewear:glasses”whenthemaintaskis
dataheaimstoinfer. WedenotethisasthetestBoW.During “race: Asian.”
training, as explained in Section 4.3, the non-zero gradients These results demonstrate that gradients observed during
of the embedding layer reveal which “words” are present in trainingleakmorethanthecharacteristicfeaturesofeachclass.
7
(a)pool1 (b)pool2 (c)pool3 (d)fc
Figure3:t-SNEprojectionofthefeaturesfromdifferentlayersofthejointmodelonLFWgenderclassification;hollowcirclepointisfemale,
solidtrianglepointismale,bluepointistheproperty“race:black”andredpointisdatawithouttheproperty.
(a)FaceScrub (b)Yelp-author
Figure4:AUCvs.thefractionofthebatchthathasthepropertyonFaceScrubandYelp-author.
Infact,collaborativelearningleakspropertiesofthetrain- HealthService TopWordsinPositiveClass
ing data that are uncorrelated with class membership. To Obstetricians pregnancy,delivery,women,birth,ultrasound
understandwhy,weplotthet-SNEprojection[59]ofthefea- Pediatricians pediatrics,sick,parents,kid,newborn
CosmeticSurgeons augmentation,plastic,breast,facial,implants
turesfromdifferentlayersofthejointmodelinFigure3. Ob-
Cardiologists cardiologist,monitor,bed,heart,ER
serve that the feature vectors are grouped by property in the
Dermatologists acne,dermatologists,mole,cancer,spots
lowerlayerspool1,pool2andpool3,andbyclasslabelinthe
Ophthalmologists vision,LASIK,contacts,lenses,frames
higherlayer.Intuitively,themodeldidnotjustlearntoseparate Orthopedists knee,orthopedic,shoulder,injury,therapy
inputsbyclass. Thelowerlayersofthemodelalsolearnedto Radiologists imaging,SimonMed,mammogram,CT,MRI
separateinputsbyvariouspropertiesthatareuncorrelatedwith Psychiatrists psychiatrist,mental,Zedek,depression,sessions
themodel’sdesignatedtask. Ourinferenceattackexploitsthis Urologists Edgepark,pump,supplies,urologist,kidney
unintendedextrafunctionality. Table4: Wordswiththelargestpositivecoefficientsintheproperty
classifierforYelp-health.
Yelp-health. On this dataset, we use review-score classifica-
tionasthemaintaskandthespecialtyofthedoctorbeingre-
viewedasthepropertyinferencetask. Obviously,thelatteris
containthatface,i.e.,theadversarycansuccessfullyinferthat
moresensitivefromtheprivacyperspective.
photos of a particular person appear in a batch even though
Weuse3,000mostfrequentwordsinthecorpusasthevo- (a) the model is trained for generic gender classification, and
cabularyandtrainfor3,000iterations. UsingBoWsfromthe (b) half of the photos in the batch are of other people. If the
embedding-layergradients,theattackachievesalmostperfect fractionishigher,AUCapproaches1.
AUC.Table4showsthewordsthathavethehighestpredictive
OnYelp-author,AUCscoresareabove0.95forallidentities
powerinourlogisticregression.
evenwhenthefractionis0.3,i.e.,theattacksuccessfullyinfers
Fractional properties. We now attempt to infer that some of theauthorsofreviewseventhough(a)themodelistrainedfor
theinputsinabatchhavetheproperty. Fortheseexperiments, genericsentimentanalysis,and(b)morethantwothirdsofthe
weuseFaceScrub’stop5faceIDsandYelp-author(thelatter reviewsinthebatcharefromotherauthors.
with the 3,000 most frequent words as the vocabulary). The
model is trained for 3,000 iterations. As before, 1/2 of the 6.3 Inferringwhenapropertyoccurs
target’s batches include inputs with the property, but here we Continuoustraining,whennewtrainingdataisaddedtothe
varythefractionofinputswiththepropertywithineachsuch process as it becomes available, presents interesting opportu-
batchamong0.1,0.3,0.5,0.7,and0.9. nitiesforinferenceattacks. Iftheoccurrencesofapropertyin
Figure4reportstheresults. OnFaceScrubforIDs0,1,and the training data can be linked to events outside the training
3, AUC scores are above 0.8 even if only 50% of the batch process, privacy violation is exacerbated. For example, sup-
8
(a)PIPA (b)FaceScrub
Figure5:Inferringoccurrenceofasingle-batchproperty.
pose a model leaks that a certain third person started appear-
inginanotherparticipant’strainingdataimmediatelyafterthat
participantuploadedhisphotosfromatrip.
PIPA. ImagesinthePIPAdatasethavebetween1to3faces.
We train the collaborative model to detect if there is a young
adult in the image; the adversary’s inference task is to deter-
mineifpeopleintheimageareofthesamegender. Thelatter
property is a stepping stone to inferring social relationships
andthussensitive. Wetrainthemodelfor2,500iterationsand
letthebatcheswiththe“samegender”propertyappeariniter-
ations500to1500.
Figure5ashows,foreachiteration,theprobabilityoutputby Figure6:Attackperformancewithrespecttothenumberofcollabo-
theadversary’sclassifierthatthebatchinthatiterationhasthe rativelearningepochs.
property. Theappearanceanddisappearanceofthepropertyin
thetrainingdataareclearlyvisibleintheplot.
6.5 Activepropertyinference
FaceScrub. ForthegenderclassificationmodelonFaceScrub,
ToshowtheadditionalpoweroftheactiveattackfromSec-
the adversary’s objective is to infer whether and when a cer-
tion 4.5, we use FaceScrub. The main task is gender classifi-
tainpersonappearsintheotherparticipant’sphotos. Thejoint
cation, theadversary’staskistoinferthepresenceofID4in
model is trained for 2,500 iterations. We arrange the target’s
the training data. We assume that this ID occurs in a single
trainingdatasothattwospecificidentitiesappearduringcer-
batch,whereitconstitutes50%ofthephotos. Weevaluatethe
tainiterations:ID0initerations0to500and1500to2000,ID
attackwithdifferentchoicesofα,whichcontrolsthebalance
1initerations 500to1000and2000 to2500. The restofthe
betweenthemain-tasklossandtheproperty-classificationloss
batches are mixtures of other identities. The adversary trains
intheadversary’sobjectivefunction.
three property classifiers, for ID 0, ID 1, and also for ID 2
Figure7ashowsthatAUCincreasesasweincreaseα. Fig-
whichdoesnotappearinthetarget’sdataset.
ure 7b and Figure 7c show the t-SNE projection of the final
Figure 5b reports the scores of all three classifiers. ID 0
fully connected layer, with α = 0 and α = 0.7, respectively.
and 1 receive the highest scores in the iterations where they
Observethatthedatawiththeproperty(bluepoints)isgrouped
appear,whereasID2,whichneverappearsinthetrainingdata,
tighterwhenα=0.7thaninthemodeltrainedunderapassive
receivesverylowscoresinalliterations.
attack (α = 0). This illustrates that as a result of the active
These experiments show that our attacks can successfully
attack,thejointmodellearnsabetterseparationfordatawith
inferdynamicpropertiesofthetrainingdatasetascollaborative
andwithouttheproperty.
learningprogresses.
6.4 Inferenceagainstwell-generalizedmodels
7 Multi-Party Experiments
To show that our attacks work with (1) relatively few ob-
served model updates and (2) against well-generalized mod- In the multi-party setting, we only consider passive property
els, we experiment with the CSI corpus. Figure 6 reports the inference attacks. We vary the number of participants be-
accuracy of inferring the author’s gender. The attack reaches tween 4 and 30 to match the deployment scenarios and ap-
0.98 AUC after only 2 epochs and improves as the training plications proposed for collaborative learning, e.g., hospitals
progressesandtheadversarycollectsmoreupdates. orbiomedicalresearchinstitutionstrainingonprivatemedical
Figure6alsoshowsthatthemodelisnotoverfitted. Itstest data [29, 30]. This is similar to prior work [25], which was
accuracy on the main sentiment-analysis task is high and im- evaluatedonMNISTwith2participantsandfacerecognition
proveswiththenumberoftheepochs. ontheAT&Tdatasetwith41participants.
9
(a)ROCfordifferentα (b)t-SNEofthefinallayerforα=0 (c)t-SNEofthefinallayerforα=0.7.
Figure7:Two-partyactivepropertyinferenceattackonFaceScrub. For(b)and(c),hollowcirclepointisfemale,solidtrianglepointismale,
bluepointistheproperty“ID4”andredpointisdatawithouttheproperty.
(a)LFW (b)Yelp-author
Figure8:Multi-partylearningwithsynchronizedSGD:attackAUCscorevs.thenumberofparticipants.
7.1 SynchronizedSGD 7.2 Modelaveraging
Asthenumberofhonestparticipantsincollaborativelearn- In every round t of federated learning with model averag-
ingincreases,theadversary’staskbecomesharderbecausethe ing (see Algorithm 2), the adversary observes θ − θ =
t t−1
observedgradientupdatesareaggregatedacrossmultiplepar- (cid:80) nkθk− (cid:80) nkθk = (cid:80) nk(θk−θk ),whereθk−θk
k n t k n t−1 k n t t−1 t t−1
ticipants. Furthermore, the inferred information may not di- aretheaggregatedgradientscomputedonthek-thparticipant’s
rectly reveal the identity of the participant to whom the data localdataset.
belongs(seeSection9.4).
Inourexperiments,wesplitthetrainingdataevenlyamong
In the following experiments, we split the training data honest participants but ensure that in the target participant’s
evenly across all participants, but so that only the target and subset, pˆ% of the inputs have the property, while none of the
theadversaryhavethedatawiththeproperty. Thejointmodel other honest participants’ data have it. During each epoch of
is trained with the same hyperparameters as in the two-party localtraining,everyhonestparticipantsplitshislocaltraining
case. SimilartoSection6.2,theadversary’sgoalistoidentify datainto10batchesandperformsoneroundoftraining.
which aggregated gradient updates are based on batches b
prop Weassumethattheadversaryhasthesamenumberofinputs
withtheproperty.
withthepropertyasthetarget. Asbefore,whentheadversary
LFW. We experiment with (1) gender classification as the trains his binary classifier, he needs to locally “emulate” the
main task and “race: black” as the inference task, and (2) collaborativetrainingprocess,i.e.,sampledatafromhislocal
smileclassificationasthemaintaskand“eyewear:sunglasses” dataset, computeaggregatedupdates, andlearntodistinguish
as the inference task. Figure 8a shows that the attack still betweentheaggregatesbasedonthedatawithouttheproperty
achievesreasonablyhighperformance,withAUCscorearound andaggregateswhereoneoftheunderlyingupdateswasbased
0.8,whenthenumberofparticipantsis12. Performancethen onthedatawiththeproperty.
degradesforbothtasks. Weperform8trialswhereasubsetofthetrainingdatahas
the property and 8 control trials where there are no training
Yelp-author. Theinferencetaskisagainauthoridentification.
inputswiththeproperty.
In the multi-party case, the gradients of the embedding layer
leakthebatchBoWsofallhonestparticipants,notjustthetar- Inferring presence of a face. We use FaceScrub and select
get. Figure 8b reports the results. For some authors, AUC two face IDs (1 and 3) whose presence we want to infer. In
scores do not degrade significantly even with many partici- the“property”case,pˆ=80%,i.e.,80%ofonehonestpartici-
pants.Thisislikelyduetosomeuniquecombinationsofwords pant’strainingdataconsistofthephotosthatdepicttheperson
usedbytheseauthors,whichidentifythemeveninmulti-party in question. In the control case, pˆ = 0%, i.e., the photos of
settings. this person do not occur in the training data. Figure 9 shows
10
(a)FaceID1,K =3 (b)FaceID1,K =5 (c)FaceID3,K =3 (d)FaceID3,K =5
Figure9: Multi-partylearningwithmodelaveraging. Boxplotsshowthedistributionoftheadversary’sscoresineachtrial: inthe8trialson
theleft,oneparticipant’sdatahastheproperty;inthe8trialsontheright,noneofthehonestparticipantshavethedatawiththeproperty.
Property/%parametersupdate 10% 50% 100%
Topregion(Antwerpen) 0.84 0.86 0.93
Gender 0.90 0.91 0.93
Veracity 0.94 0.99 0.99
Table5: InferenceattacksagainsttheCSICorpusfordifferentfrac-
tionsofgradientssharedduringtraining.
Figure10:Inferringthataparticipantwhoselocaldatahastheprop-
ertyofinteresthasjoinedthetraining. K = 2forrounds0to250,
K =3forrounds250to500.
the scores assigned by the adversary’s classifier to the aggre-
gatedupdateswith3and5totalparticipants. Whenthefacein
questionispresentinthetrainingdataset,thescoresaremuch
higherthanwhenitisabsent.
Successoftheattackdependsonthepropertybeinginferred, Figure11:Uniquenessofuserprofileswithrespecttothenumberof
distribution of the data across participants, and other factors. toplocations.
Forexample,theclassifiersforFaceIDs2and4,whichwere
trainedinthesamefashionastheclassifiersforFaceIDs1and
AUC scores: when inferring the region of the texts’ authors,
3,failedtoinferthepresenceofthecorrespondingfacesinthe
ourattackstillachieves0.84AUCwhenonly10%oftheup-
trainingdata.
datesaresharedduringeachiteration,comparedto0.93AUC
Inferringwhenafaceoccurs. Inthisexperiment,weaimto
whenallupdatesareshared.
infer when a participant whose local data has a certain prop-
erty joined collaborative training. We first let the adversary
8.2 Dimensionalityreduction
andtherestofthehonestparticipantstrainthejointmodelfor
AsdiscussedinSection4.2,iftheinputspaceofthemodelis
250 rounds. The target participant then joins the training at
sparseandinputsmustbeembeddedintoalower-dimensional
round t = 250 with the local data that consists of photos de-
space,non-zerogradientupdatesintheembeddinglayerreveal
picting ID 1. Figure 10 reports the results of the experiment:
whichinputsarepresentinthetrainingbatch.
theadversary’sAUCscoresarearound0whenfaceID1isnot
present and then increase almost to 1.0 right after the target Oneplausibledefenseistoonlyuseinputsthatoccurmany
participantjoinsthetraining. timesinthetrainingdata. Thisdoesnotworkingeneral,e.g.,
Figure 11 shows that restricting inputs to the top locations in
theFourSquaredataseteliminatesmostofthetrainingdata.
8 Defenses Asmarterdefenseistorestrictthemodelsothatitonlyuses
“words”fromapre-definedvocabularyofcommonwords.For
8.1 Sharingfewergradients
example,Google’sfederatedlearningforpredictivekeyboards
As suggested in [52], participants in collaborative learning usesafixedvocabularyof5,000words[35]. InTable6,were-
could share only a fraction of their gradients during each up- porttheaccuracyofourmembershipinferenceattackandthe
date. This reduces communication overhead and, potentially, accuracyofthejointmodelonitsmaintask—genderclassifi-
leakage,sincetheadversaryobservesfewergradients. cation for the FourSquare dataset, sentiment analysis for the
To evaluate this defense, we measure the performance of CSI Corpus—for different sizes of the common vocabulary
single-batchinferenceagainstasentimentclassifiercollabora- (locations and words, respectively). This approach partially
tivelytrainedontheCSICorpusbytwopartieswhoexchange mitigatesourattacksbutalsohasasignificantnegativeimpact
onlyafractionoftheirgradients. Table5showstheresulting onthequalityofthecollaborativelytrainedmodels.
11
CSI FourSquare DropoutProb. AttackAUC ModelAUC
TopN Attack Model TopN Attack Model
0.1 0.94 0.87
words Precision AUC locations Precision AUC
0.3 0.97 0.87
4,000 0.94 0.91 30,000 0.91 0.64 0.5 0.98 0.87
2,000 0.92 0.87 10,000 0.86 0.59 0.7 0.99 0.86
1,000 0.92 0.85 3,000 0.65 0.51
0.9 0.99 0.84
500 0.82 0.84 1,000 0.52 0.50
Table7: Inferenceofthetopregion(Antwerpen)ontheCSICorpus
Table 6: Membership inference against the CSI Corpus and
fordifferentvaluesofdropoutprobability.
FourSquarefordifferentvocabularysizes.
8.3 Dropout 9 Limitations of the attacks
Anotherpossibledefenseistoemploydropout[56],apopu-
9.1 Auxiliarydata
larregularizationtechniqueusedtomitigateoverfittinginneu-
ral networks. Dropout randomly deactivates activations be- Ourpropertyinferenceattacksassumethattheadversaryhas
tween neurons, with probability p drop ∈ [0,1]. Random de- auxiliary training data correctly labeled with the property he
activationsmayweakenourattacksbecausetheadversaryob- wants to infer. For generic properties, such data is easy to
servesfewergradientscorrespondingtotheactiveneurons. find. For example, the auxiliary data for inferring the num-
Toevaluatethisapproach,weadddropoutafterthemaxpool ber and genders of people can be any large dataset of images
layersinthejointmodel. Table7reportstheaccuracyofinfer- withmalesandfemales,singleandingroups,whereeachim-
ringtheregionofthereviewsintheCSICorpus,fordifferent ageislabeledwiththenumberofpeopleinitandtheirgenders.
valuesofp . Increasingtherandomnessofdropoutmakes Similarly,theauxiliarydataforinferringthemedicalspecialty
drop
ourattacksstronger whileslightlydecreasingtheaccuracyof ofdoctorscanconsistofanytextsthatincludewordscharac-
the joint model. Dropout stochastically removes features at teristicofdifferentspecialties(seeTable4).
everycollaborativetrainingstep, thusyieldingmoreinforma- Moretargetedinferenceattacksrequirespecializedauxiliary
tivefeatures(similartofeaturebagging[7,26])andincreasing datathatmaynotbeavailabletotheadversary. Forexample,
variancebetweenparticipants’updates. toinferthatphotosofacertainpersonoccursinanotherpartic-
ipant’sdataset,theadversaryneeds(possiblydifferent)photos
8.4 Participant-leveldifferentialprivacy
of that person to train on. To infer the authorship of training
As discussed in Section 2.2, record-level ε-differential pri- texts, the adversary needs a sufficiently large sample of texts
vacy, by definition, bounds the success of membership infer- knowntobewrittenbyaparticularauthor.
encebutdoesnotpreventpropertyinference. Anyapplication
ofdifferentialprivacyentailsapplication-specifictradeoffsbe- 9.2 Numberofparticipants
tweenprivacyofthetrainingdataandaccuracyoftheresulting
In our experiments, the number of participants in collabo-
model.Theparticipantsmustalsosomehowchoosetheparam-
rativetrainingisrelativelysmall(rangingfrom2to30),while
eters(e.g.,ε)thatcontrolthistradeoff.
somefederated-learningapplicationsinvolvethousandsormil-
In theory, participant-level differential privacy bounds the
lions of users [35, 36]. As discussed in Section 7.1, perfor-
success of inference attacks described in this paper. We
manceofourattacksdropssignificantlyasthenumberofpar-
implemented the participant-level differentially private fed-
ticipantsincreases.
erated learning algorithm by McMahan et al. [36] and at-
tempted to train a gender classifier on LFW, but the model
9.3 Undetectableproperties
did not converge for any number of participants (we tried
Itmaynotbepossibletoinfersomepropertiesfrommodel
at most 30). This is due to the magnitude of noise needed
updates. For example, our attack did not detect the presence
to achieve differential privacy with the moments accountant
ofsomefaceidentitiesinthemulti-partymodelaveragingex-
bound [1], which is inversely proportional to the number of
periments(Section7.2). Ifforwhateverreasonthemodeldoes
users (the model in [36] was trained on thousands of users).
not internally separate the features associated with the target
Anotherparticipant-leveldifferentialprivacymechanism,pre-
property,inferencewillfail.
sented in [20], also requires a very large number of partici-
pants. Moreover, these two mechanisms have been used, re-
9.4 Attributionofinferredproperties
spectively, for language modeling [36] and handwritten digit
recognition [20]. Adapting them to the specific models and In the two-party scenarios considered in Section 6, attribu-
tasksconsideredinthispapermaynotbestraightforward. tion of the inferred properties is trivial because there is only
Following[20,36], webelievethatparticipant-leveldiffer- onehonestparticipant. Inthemulti-partyscenariosconsidered
entialprivacyprovidereasonableaccuracyonlyinsettingsin- in Section 7, model updates are aggregated. Therefore, even
volving at least thousands of participants. We believe that iftheadversarysuccessfullyinfersthepresenceofinputswith
furtherworkisneededtoinvestigatewhetherparticipant-level a certain property in the training data, he may not be able to
differentialprivacycanbeadaptedtopreventourinferenceat- attributetheseinputstoaspecificparticipant. Furthermore,he
tacks and obtain high-quality models in settings that do not maynotbeabletotellifallinputswiththepropertybelongto
involvethousandsofusers. oneparticipantoraredistributedacrossmultipleparticipants.
12
Ingeneral,attributionrequiresauxiliaryinformationspecific sifier hosted by a trusted third-party and based on locally
to the leakage. For example, consider face identification. In trained classifiers held by separate, mutually distrusting par-
some applications of collaborative learning, the identities of ties. Hamm et al. [23] use knowledge transfer to combine a
allparticipantsareknownbecausetheyneedtocommunicate collectionofmodelstrainedonindividualdevicesintoasingle
witheachother.Ifcollaborativelearningleaksthataparticular model,withdifferentialprivacyguarantees.
person appears in the training images, auxiliary information Secure multi-party computation (MPC) has also been used
about the participants (e.g., their social networks) can reveal to build privacy-preserving neural networks in a distributed
whichofthemknowsthepersoninquestion. Similarly,ifcol- fashion.Forexample,SecureML[37]startswiththedataown-
laborative learning leaks the authorship of the training texts, ers (clients) distributing their private training inputs among
auxiliaryinformationcanhelpinferwhichparticipantislikely two non-colluding servers during the setup phase; the two
totrainontextswrittenbythisauthor. servers then use MPC to train a global model on the clients’
Anotherexampleofattributionbasedonauxiliaryinforma- encryptedjointdata. Bonawitzetal.[5]usesecuremulti-party
tion is described in Section 6.3. If photos of a certain per- aggregation techniques, tailored for federated learning, to let
sonfirstappearinthetrainingdataafteranewparticipanthas participantsencrypttheirupdatessothatthecentralparameter
joinedcollaborativetraining,theadversarymayattributethese server only recovers the sum of the updates. In Section 7.2,
photostothenewparticipant. weshowedthatinferenceattackscanbesuccessfulevenifthe
Note that leakage of medical conditions, locations, images adversaryonlyobservesaggregatedupdates.
of individuals, or texts written by known authors is a privacy
Membership inference. Prior work demonstrated the feasi-
breach even if it cannot be traced to a specific participant or
bilityofmembershipinferencefromaggregatestatistics, e.g.,
multipleparticipants. Leakingthatacertainpersonappearsin
in the context of genomic studies [3, 27], location time-
the photos or just the number of people in the photos reveals
series[50],ornoisystatisticsingeneral[14].
intimate relationships between people. Locations can reveal
Membership inference against black-box ML models has
people’s addresses, religion, sexual orientation, and relation-
alsobeenstudiedextensivelyinrecentwork. Shokrietal.[53]
shipswithotherpeople.
demonstrate membership inference against black-box super-
visedmodels,exploitingthedifferencesinthemodels’outputs
on training and non-training inputs. Hayes et al. [24] focus
10 Related Work
on generative models in machine-learning-as-a-service appli-
cationsandtrainGANs[22]todetectoverfittingandrecognize
Privacy-preservingdistributedlearning. Transferlearningin
traininginputs. Longetal.[34]andYeometal.[66]studythe
combination with differentially private (DP) techniques tai-
relationshipbetweenoverfittingandinformationleakage.
lored for deep learning [1] has been used in [45, 46]. These
Truex et al. [58] extend [53] to a more general setting and
techniques privately train a “student” model by transferring,
show how membership inference attacks are data-driven and
through noisy aggregation, the knowledge of an ensemble
largely transferable. They also show that an adversary who
of “teachers” trained on the disjoint subsets of training data.
participates in collaborative learning, with access to individ-
These are centralized, record-level DP mechanisms with a
ual model updates from all honest participants, can boost the
trustedaggregatoranddonotapplytofederatedorcollabora-
performanceofmembershipinferencevs.acentralizedmodel.
tivelearning. Inparticular,[45,46]assumethattheadversary
Nasr et al. [39] design a privacy mechanism to adversarially
cannotseetheindividualmodels,onlythefinalmodeltrained
train centralized machine learning models with provable pro-
by the trusted aggregator. Moreover, record-level DP by def-
tectionsagainstmembershipinference.
inition does not prevent property inference. Finally, their ef-
fectivenesshasbeendemonstratedonlyonafewspecifictasks Other attacks on machine learning models. Several tech-
(MNIST,SVHN,OCR),whicharesubstantiallydifferentfrom niques infer class features and/or construct class representa-
thetasksconsideredinthispaper. tives if the adversary has black-box [16, 17] or white-box [2]
Shokri and Shmatikov [52] propose making gradient up- accesstoaclassifiermodel.AsdiscussedindetailinSection3,
dates differentially private to protect the training data. Their thesetechniquesinferfeaturesthatcharacterizeanentireclass
approach requires extremely large values of the ε parameter and not specifically the training data, except in the cases of
(and consequently little privacy protection) to produce an ac- pathological overfitting where the training sample constitutes
curate joint model. More recently, participant-level differen- theentiremembershipoftheclass.
tiallyprivatefederatedlearningmethods[20,36]showedhow Hitaj et al. [25] show that a participant in collaborative
toprotectparticipants’trainingdatabyaddingGaussiannoise deeplearningcanuseGANstoconstructclassrepresentatives.
tolocalupdates.AsdiscussedinSection8.4,theseapproaches Theirtechniquewasevaluatedonlyonmodelswhereallmem-
require a large number of users (on the order of thousands) bers of the same class are visually similar (handwritten dig-
for the training to converge and achieve an acceptable trade- its and faces). As discussed in Section 3.1, there is no evi-
offbetweenprivacyandmodelperformance. Furthermore,the dencethatitproducesactualtrainingimagesorcandistinguish
results in [36] are reported for a specific language model and atrainingimageandanotherimagefromthesameclass.
useAccuracyTop1astheproxy,nottheactualaccuracyofthe Theinformalpropertyviolatedbytheattacksof[2,16,17,
non-privatemodel. 25] is: “a classifier should prevent users from generating an
Pathaketal.[47]presentadifferentiallyprivateglobalclas- input that belongs to a particular class or even learning what
13
suchaninputlookslike.” Itisnotcleartouswhythisproperty possibletodetectactiveattacksthatmanipulatethemodelinto
isdesirable,orwhetheritisevenachievable. learning extra features. Finally, it remains an open question
Aonoetal.[49]showthat,inthecollaborativedeeplearning if participant-level differential privacy mechanisms can pro-
protocolof[52],anhonest-but-curiousservercanpartiallyre- duceaccuratemodelswhencollaborativelearninginvolvesrel-
coverparticipants’traininginputsfromtheirgradientupdates ativelyfewparticipants.
under the (greatly simplified) assumption that the batch con-
Acknowledgments. Thisresearchwassupportedinpartbythe
sistsofasingleinput. Furthermore,thetechniqueisevaluated
NSFgrants1611770and1704296,thegenerosityofEricand
onlyonMNISTwhereallclassmembersarevisuallysimilar.
Wendy Schmidt by recommendation of the Schmidt Futures
Itisnotclearifitcandistinguishatrainingimageandanother
program, the Alan Turing Institute under the EPSRC grant
imagefromthesameMNISTclass.
EP/N510129/1,andagrantbyNokiaBellLabs.
Songetal.[55]engineeranMLmodelthatmemorizesthe
trainingdata, whichcanthenbeextractedwithblack-boxac-
References
cess to the model. Carlini et al. [6] show that deep learning-
basedgenerativesequencemodelstrainedontextdatacanun-
[1] M.Abadi,A.Chu,I.Goodfellow,H.B.McMahan,I.Mironov,
intentionallymemorizetraininginputs,whichcanthenbeex-
K. Talwar, and L. Zhang. Deep learning with differential pri-
tracted with black-box access. They do this for sequences of vacy. InCCS,2016.
digits artificially introduced into the text, which are not af-
[2] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vi-
fectedbytherelativewordfrequenciesinthelanguagemodel. tali,andG.Felici. Hackingsmartmachineswithsmarterones:
Training data that is explicitly incorporated or otherwise Howtoextractmeaningfuldatafrommachinelearningclassi-
memorizedinthemodelcanalsobeleakedbymodelstealing fiers. IJSN,10(3):137–150,2015.
attacks[41,57,61]. [3] M.Backes,P.Berrang,M.Humbert,andP.Manoharan. Mem-
Concurrently with this work, Ganju et al. [18] developed bershipPrivacyinMicroRNA-basedStudies. InCCS,2016.
property inference attacks against fully connected, relatively [4] BBC. GoogleDeepMindNHSapptestbrokeUKprivacylaw.
shallow neural networks. They focus on the post-training, https://bbc.in/2A1MftK,2017.
white-box release of models trained on sensitive data, as op- [5] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B.
posedtocollaborativetraining. Incontrasttoourattacks, the McMahan,S.Patel,D.Ramage,A.Segal,andK.Seth.Practical
properties inferred in [18] may be correlated with the main secureaggregationforprivacy-preservingmachinelearning. In
CCS,2017.
task. Evaluationislimitedtosimpledatasetsandtaskssuchas
MNIST,U.S.Censustabulardata, andhardwareperformance [6] N.Carlini,C.Liu,J.Kos,U´.Erlingsson,andD.Song. TheSe-
cret Sharer: Measuring unintended neural network memoriza-
counterswithshortfeatures.
tion&extractingsecrets. arXiv:1802.08232,2018.
[7] C.-H.Chang,L.Rampasek,andA.Goldenberg.Dropoutfeature
rankingfordeeplearningmodels. arXiv:1712.08645,2017.
11 Conclusion
[8] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
Inthispaper,weproposedandevaluatedseveralinferenceat- B.Xu,C.Zhang,andZ.Zhang.MXNet:Aflexibleandefficient
tacks against collaborative learning. These attacks enable a machinelearninglibraryforheterogeneousdistributedsystems.
malicious participant to infer not only membership, i.e., the arXiv:1512.01274,2015.
presence of exact data points in other participants’ training [9] T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman.
data,butalsopropertiesthatcharacterizesubsetsofthetrain- ProjectAdam: Buildinganefficientandscalabledeeplearning
trainingsystem. InOSDI,2014.
ing data and are independent of the properties that the joint
[10] K. Cho, B. Van Merrie¨nboer, C. Gulcehre, D. Bahdanau,
modelaimstocapture.
F.Bougares,H.Schwenk,andY.Bengio. Learningphraserep-
Deep learning models appear to internally recognize many
resentationsusingRNNencoder-decoderforstatisticalmachine
features of the data that are uncorrelated with the tasks they
translation. InEMNLP,2014.
arebeingtrainedfor. Consequently,modelupdatesduringcol-
[11] J.Dean,G.Corrado,R.Monga,K.Chen,M.Devin,M.Mao,
laborativelearningleakinformationaboutthese“unintended”
A. Senior, P. Tucker, K. Yang, Q. Le, et al. Large scale dis-
features to adversarial participants. Active attacks are poten-
tributeddeepnetworks. InNIPS,2012.
tiallyverypowerfulinthissettingbecausetheyenablethead-
[12] S.Dieleman,J.Schlu¨ter,C.Raffel,etal.Lasagne:Firstrelease.
versarytotrickthejointmodelintolearningfeaturesofthead-
http://dx.doi.org/10.5281/zenodo.27878,2015.
versary’schoosingwithoutasignificantimpactonthemodel’s
[13] C.DworkandM.Naor.Onthedifficultiesofdisclosurepreven-
performanceonitsmaintask.
tioninstatisticaldatabasesorthecasefordifferentialprivacy.
Our results suggest that leakage of unintended features ex- JournalofPrivacyandConfidentiality,2(1):93–107,2010.
posescollaborativelearningtopowerfulinferenceattacks. We
[14] C.Dwork,A.Smith,T.Steinke,J.Ullman,andS.Vadhan. Ro-
also showed that defenses such as selective gradient sharing, busttraceabilityfromtraceamounts. InFOCS,2015.
reducing dimensionality, and dropout are not effective. This [15] H.EdwardsandA.Storkey. Censoringrepresentationswithan
shouldmotivatefutureworkonbetterdefenses. Forinstance, adversary. InICLR,2016.
techniques that learn only the features relevant to a given [16] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion at-
task [15, 42, 43] can potentially serve as the basis for “least- tacksthatexploitconfidenceinformationandbasiccountermea-
privilege” collaboratively trained models. Further, it may be sures. InCCS,2015.
14
[17] M.Fredrikson,E.Lantz,S.Jha,S.Lin,D.Page,andT.Risten- differentiallyprivatelanguagemodelswithoutlosingaccuracy.
part. Privacyinpharmacogenetics:Anend-to-endcasestudyof InICLR,2018.
personalizedWarfarindosing. InUSENIXSecurity,2014. [37] P. Mohassel and Y. Zhang. SecureML: A system for scalable
[18] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. privacy-preservingmachinelearning. InS&P,2017.
Property inference attacks on fully connected neural networks [38] P.Moritz,R.Nishihara,I.Stoica,andM.I.Jordan. SparkNet:
usingpermutationinvariantrepresentations. InCCS,2018. TrainingdeepnetworksinSpark. arXiv:1511.06051,2015.
[19] General Data Protection Regulation. https://en.wikipedia.org/ [39] M.Nasr,R.Shokri,andA.Houmansadr.Machinelearningwith
wiki/General Data Protection Regulation,2018. membershipprivacy usingadversarial regularization. InCCS,
[20] R.C.Geyer,T.Klein,andM.Nabi. Differentiallyprivatefed- 2018.
eratedlearning: Aclientlevelperspective. arXiv:1712.07557, [40] H.-W.NgandS.Winkler. Adata-drivenapproachtocleaning
2017. largefacedatasets. InICIP,2014.
[21] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep [41] S. J. Oh, M. Augustin, M. Fritz, and B. Schiele. Towards
Learning. MITPress,2016. reverse-engineeringblack-boxneuralnetworks. InICLR,2018.
[22] I.Goodfellow, J.Pouget-Abadie, M.Mirza, B.Xu, D.Warde- [42] S.A.Osia,A.S.Shamsabadi,A.Taheri,K.Katevas,S.Sajad-
Farley,S.Ozair,A.Courville,andY.Bengio.Generativeadver- manesh,H.R.Rabiee,N.D.Lane,andH.Haddadi. Ahybrid
sarialnets. InNIPS,2014. deeplearningarchitectureforprivacy-preservingmobileanalyt-
[23] J.Hamm,Y.Cao,andM.Belkin. Learningprivatelyfrommul- ics. arXiv:1703.02952,2017.
tipartydata. InICML,2016. [43] S.A.Osia, A.Taheri, A.S.Shamsabadi, K.Katevas, H.Had-
[24] J.Hayes,L.Melis,G.Danezis,andE.DeCristofaro. LOGAN: dadi,andH.R.Rabiee. Deepprivate-featureextraction. TKDE,
Membership inference attacks against generative models. In 2019.
PETS,2019. [44] J.PangandY.Zhang. DeepCity:Afeaturelearningframework
[25] B.Hitaj,G.Ateniese,andF.Pe´rez-Cruz.Deepmodelsunderthe formininglocationcheck-ins.InICWSM(PosterPapers),2017.
GAN:Informationleakagefromcollaborativedeeplearning. In [45] N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and
CCS,2017. K.Talwar. Semi-supervisedknowledgetransferfordeeplearn-
[26] T.K.Ho. Randomdecisionforests. InDAR,1995. ingfromprivatetrainingdata. InICLR,2017.
[27] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, [46] N.Papernot,S.Song,I.Mironov,A.Raghunathan,K.Talwar,
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and and U´. Erlingsson. Scalable private learning with PATE. In
D.W.Craig. Resolvingindividualscontributingtraceamounts ICLR,2018.
of DNA to highly complex mixtures using high-density SNP [47] M.Pathak,S.Rane,andB.Raj. Multipartydifferentialprivacy
genotypingmicroarrays. PLoSGenetics,4(8),2008. viaaggregationoflocallytrainedclassifiers. InNIPS,2010.
[28] G.B.Huang,M.Ramesh,T.Berg,andE.Learned-Miller. La- [48] F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,
beledfacesinthewild:Adatabaseforstudyingfacerecognition O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
inunconstrainedenvironments. TechnicalReport07–49, Uni- J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,
versityofMassachusetts,Amherst,2007. and E. Duchesnay. Scikit-learn: Machine learning in Python.
[29] A. Jochems, T. M. Deist, I. El Naqa, M. Kessler, C. Mayo, JMLR,12,2011.
J.Reeves, S.Jolly, M.Matuszak, R.TenHaken, J.vanSoest, [49] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai.
etal. Developingandvalidatingasurvivalpredictionmodelfor Privacy-preservingdeeplearning: Revisitedandenhanced. In
NSCLCpatientsthroughdistributedlearningacross3countries. ATIS,2017.
IntJRadiatOncolBiolPhys,99(2):344–352,2017.
[50] A.Pyrgelis,C.Troncoso,andE.DeCristofaro. Knockknock,
[30] A. Jochems, T. M. Deist, J. Van Soest, M. Eble, P. Bulens, who’sthere? membershipinferenceonaggregatelocationdata.
P. Coucke, W. Dries, P. Lambin, and A. Dekker. Distributed InNDSS,2018.
learning: Developing a predictive model based on data from
[51] J. Schmidhuber. Deep learning in neural networks: An
multiple hospitals without data leaving the hospital–a real life
overview. NeuralNetworks,2015.
proofofconcept. RadiotherOncol,121(3):459–467,2016.
[52] R.ShokriandV.Shmatikov. Privacy-preservingdeeplearning.
[31] Y.Kim. Convolutionalneuralnetworksforsentenceclassifica-
InCCS,2015.
tion. arXiv:1408.5882,2014.
[53] R.Shokri, M.Stronati, C.Song, andV.Shmatikov. Member-
[32] Y.LeCun, Y.Bengio, andG.Hinton. Deeplearning. Nature,
shipinferenceattacksagainstmachinelearningmodels.InS&P,
2015.
2017.
[33] Y.Lin,S.Han,H.Mao,Y.Wang, andW.J.Dally. Deepgra-
[54] K.SimonyanandA.Zisserman. Verydeepconvolutionalnet-
dientcompression:Reducingthecommunicationbandwidthfor
worksforlarge-scaleimagerecognition. InICLR,2015.
distributedtraining. InICLR,2018.
[55] C. Song, T. Ristenpart, and V. Shmatikov. Machine learning
[34] Y.Long,V.Bindschaedler,L.Wang,D.Bu,X.Wang,H.Tang,
modelsthatremembertoomuch. InCCS,2017.
C.A.Gunter,andK.Chen. Understandingmembershipinfer-
[56] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
encesonwell-generalizedlearningmodels. arXiv:1802.04889,
R. Salakhutdinov. Dropout: A simple way to prevent neural
2018.
networksfromoverfitting. JMLR,2014.
[35] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al.
[57] F. Trame`r, F. Zhang, A. Juels, M. K. Reiter, and T. Risten-
Communication-efficient learning of deep networks from de-
part. StealingmachinelearningmodelsviapredictionAPIs. In
centralizeddata. InAISTATS,2017.
USENIXSecurity,2016.
[36] H.B.McMahan,D.Ramage,K.Talwar,andL.Zhang.Learning
[58] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei. Towards
15
demystifyingmembershipinferenceattacks.arXiv:1807.09173, SNs. JNCA,55:170–180,2015.
2018. [64] D.Yang,D.Zhang,andB.Qu. Participatoryculturalmapping
[59] L.vanderMaatenandG.Hinton.Visualizingdatausingt-SNE. basedoncollectivebehaviorinlocationbasedsocialnetworks.
JMLR,2008. ACMTIST,2015.
[60] B. Verhoeven and W. Daelemans. CLiPS Stylometry Investi- [65] D.Yang,D.Zhang,B.Qu,andP.Cudre-Mauroux. PrivCheck:
gation(CSI)Corpus: ADutchcorpusforthedetectionofage, Privacy-preservingcheck-indatapublishingforpersonalizedlo-
gender,personality,sentimentanddeceptionintext. InLREC, cationbasedservices. InUbiComp,2016.
2014. [66] S.Yeom,I.Giacomelli,M.Fredrikson,andS.Jha. Privacyrisk
[61] B.WangandN.Z.Gong. Stealinghyperparametersinmachine inmachinelearning:Analyzingtheconnectiontooverfitting.In
learning. InS&P,2018. CSF,2018.
[62] E.P.Xing,Q.Ho,W.Dai,J.K.Kim,J.Wei,S.Lee,X.Zheng, [67] N. Zhang, M. Paluri, Y. Taigman, R. Fergus, and L. Bourdev.
P.Xie,A.Kumar,andY.Yu. Petuum: Anewplatformfordis- Beyondfrontalfaces:Improvingpersonrecognitionusingmul-
tributed machine learning on big data. IEEE Transactions on tiplecues. InCVPR,2015.
BigData,2015. [68] M.Zinkevich,M.Weimer,L.Li,andA.J.Smola. Parallelized
[63] D.Yang,D.Zhang,L.Chen,andB.Qu.NationTelescope:Mon- stochasticgradientdescent. InNIPS,2010.
itoring and visualizing large-scale collective behavior in LB-
16