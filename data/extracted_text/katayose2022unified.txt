OU-HET-1138
A uniﬁed theory of learning
Taisuke Katayose∗
Department of Physics, Osaka University, Toyonaka 560-0043, Japan
Abstract
Recently machine learning using neural networks (NN) has been developed, and many new
methods have been suggested. These methods are optimized for the type of input data and work
very eﬀectively, but they cannot be used with any kind of input data universally. On the other
hand, the human brain is universal for any kind of problem, and we will be able to construct
artiﬁcial general intelligence if we can mimic the system of how the human brain works. We
consider how the human brain learns things uniformly, and ﬁnd that the essence of learning is
the compression of information. We suggest a toy NN model which mimics the system of the
human brain, and we show that the NN can compress the input information without ad hoc
treatment, only by setting the loss function properly. The loss function is expressed as the sum
of the self-information to remember and the loss of the information along with the compression,
and its minimum corresponds to the self-information of the original data. To evaluate the self-
information to remember, we provided the concept of memory. The memory expresses the
compressed information, and the learning proceeds by referring to previous memories. There
are many similarities between this NN and the human brain, and this NN is a realization of
the free-energy principle which is considered to be a uniﬁed theory of the human brain. This
work can be applied to any kind of data analysis and cognitive science.
∗taisuke.katayose@het.phys.sci.osaka-u.ac.jp
arXiv:2203.16941v2  [cs.LG]  24 Apr 2022
Contents
1 Introduction 2
2 Consideration about human learning 3
2.1 General deﬁnition of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Correlation and probability bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 Probability bias and redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.4 Detection of redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.5 Compression of information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 The model of neural network 5
4 The self-information and the loss function 7
4.1 The self-information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 The loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.3 The meaning of the loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 The role of memory and the hidden variables 12
5.1 The redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 The hidden variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.3 The role of memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.4 The memory as input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6 The uncertainty of the solution 15
6.1 The case with α=0, β =0.01 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.2 The case with α=0.01, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.3 The case with α=0.2, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.4 The case with α=0.5, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.5 Properties of the loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
7 Discussion 18
8 Conclusion 19
1
1 Introduction
Recently machine learning using neural networks (NN) has been developed and many new methods
have been suggested [1]. For example, convolutional NN [2] for image processing, attention model [3]
for natural language processing, and generative adversarial networks [4] for classiﬁcation problem
have achieved prominent results. These models are constructed by considering the properties of the
input data and reﬂecting these properties in the model. On the other hand, thinking about the
human brain, it does not seem to need such ad hoc treatment and can adapt to any kind of problem.
Then, what determines how the human brain works, and why is the human brain so versatile?
To answer these questions, we need to reconsider the meaning of learning and ﬁnd out the theory
behind it. In physics, there is the principle of least action, and the motion of objects is determined
by minimizing the action. There must be a corresponding concept to the theory of learning, and
this is nothing but the loss function. Learning proceeds by minimizing the loss function. From this
perspective, we consider that the essence of the problem is in the loss function and the NN model
is just a tool to ﬁnd a minimum of the loss function. In other words, the upgrade of the NN model
may improve the convergence of the calculation but cannot solve the unsolvable problem because
of the bad setting of the loss function. This situation is commonly happening in the study of the
NN, so we need not only to make a new NN model for the individual cases but also to reconsider
the essential meaning of the loss function. We claim that the loss function is not to be designed
for good results but to be deﬁned theoretically. The goal of this paper is to understand the way
how the human brain works and to derive the general loss function which enables the NN to work
similarly to the human brain by minimizing it. We emphasize that the actual implementation is not
discussed in this paper.
To achieve our goal, we need to deﬁne learning mathematically and derive the loss function in the
calculable form. By careful consideration, we found that learning is the compression of information.
We make a toy NN model and then deﬁne the loss function which enables the compression of
information. This NN is a kind of autoencoder, and we do not need teacher data or ad hoc treatment
for this NN. We referred to the system of the human brain to construct this NN model, and the
outstanding property is recording the output from the middle layer every time a new input enters.
This is called memory, and the loss function is deﬁned using these memories. The memory denotes
the compressed information, and it can express the abstract information as same as actual human
memory. We can ﬁnd many similarities between this NN and the human brain, and this work also
can be used in cognitive science.
This paper is arranged as follows. In Sec. 2, we consider the essential meaning of learning and
mention that learning is the compression of information. In Sec. 3 we deﬁne a toy NN model which
mimics the system of the human brain. This is an autoencoder type NN model, and we do not go
into the details of the model, because our goal is to derive the universal loss function. In Sec. 4,
we deﬁne the self-information to remember and loss of information and show that the sum of these
2
quantities corresponds to the self-information of the input data. We also show that the loss function
is deﬁned as the prediction of these quantities through the NN. In Sec. 5, we discuss the concept of
memory and introduce the concept of the hidden variables. We show that the memory expresses
the compressed and abstract information as similar to the memory of the human brain. In Sec. 6,
we mention the uncertainty of the solution to minimize the loss function, and we suggest a new
loss function to solve this problem. We demonstrate how this loss function works by using simple
examples. In Sec. 7, we discuss the reason why our model does not need ad hoc treatment. We also
discuss unclear part of the concept of memory and the similarity between this NN and the human
brain. In Sec. 8, we summarize our study.
2 Consideration about human learning
In this section, we consider how the human brain learns things. To make it easy to understand, we
discuss the concepts step by step.
2.1 General deﬁnition of learning
The word learning has a wide range of meaning, and this word is used when we acquire some skills
through training. For thinking about learning, we need to deﬁne what is learning mathematically.
Let us give two examples of learning and ﬁnd out the common point. For the ﬁrst example, we can
say that a baby learned the word cat when he understands what is a cat. For the second example,
we can say that a dog learned tricks, after repeating the training such as feeding him if he follows
what the owner says. Then, what is the thing these learning have in common? Our answer to
the question is that learning is the discovery of the correlation between information. In the ﬁrst
example, the baby notices that he tends to hear the word cat pronounced when he is seeing a cat,
and he discovers the correlation between hearing the pronounced word cat and seeing a cat. In the
second example, the dog notice that he can get food after following what the owner says, and he
discovers the correlation between having food and following what the owner says. This statement
applies to any kind of learning and we can identify the learning as the discovery of the correlation
between the information.
2.2 Correlation and probability bias
Next, let us explain the correlation between the information as discussed in Sec. 2.1. To reveal the
essence of the correlation between information, let us explain it using coding theory. Suppose the
two bits binary system, which can take the following states deﬁned as
E1 ≡(0,0), E2 ≡(0,1), E3 ≡(1,0), E4 ≡(1,1). (1)
In this system, the ﬁrst bit takes a 0/1 value and the second bit also takes a 0/1 value, and we
consider each bit expresses diﬀerent information. Making the correspondence to the case with the
3
baby learns the word cat, the ﬁrst bit expresses whether the baby hears the word cat and the second
bit expresses whether the baby sees a cat. In such a case, there emerges the correlation between the
ﬁrst bit and the second bit, and we can think that the probabilities of getting E1 and E4 become
higher. This probability bias is caused by the correlation between the information, and we can say
that the learning is the detection of the probability bias.
2.3 Probability bias and redundancy
The next question is how the brain detects such probability bias automatically. To clarify what
happens when the probability distribution is biased, let us consider the sample case as in Sec. 2.2.
We set the probability distribution of the states as follows:
P(E1) = 0.6, P(E2) = 0.1, P(E3) = 0.1, P(E4) = 0.2. (2)
where P(·) denotes the probability that the argument happens, and we set the probabilities of E1
and E4 higher than others. If the probability is biased, it is known that redundancy emerge. The
redundancy R is deﬁned as
R≡Hmax −H, (3)
where H is the information entropy deﬁned as
H ≡−
∑
i
P(Ei) logP(Ei) , (4)
and Hmax is the maximum value of the information entropy when we change the probability distri-
bution. For the case with Eq. (2), H becomes maximum when Ei happens with the same probability,
and Hmax is calculated as
Hmax = −
4∑
i=1
(1/4) log (1/4)
= 1.386 ··· .
(5)
The information entropy H is calculated as
H = −
4∑
i=1
P(Ei) logP(Ei)
= 1.088 ··· .
(6)
Then the redundancy for this case is calculated as
R= Hmax −H = 0.298 ··· . (7)
The redundancy R always has a positive value for a biased probability distribution, and we can say
that learning is the detection of redundancy.
4
2.4 Detection of redundancy
The next question is how our brain detects such redundancy. Let us consider the coded information
which has redundancy. For example, let us consider the following binary code:
C = 111000111111000111000111000000 . (8)
Then we can compress this information with following replacement:
000 →0
111 →1
C →C′= 1011010100 .
(9)
Even for general cases, using some methods, we can compress the information. Then, where does the
redundancy go? The answer is that redundancy is embedded in how we compress the information.
If there is redundancy in the information, we can ﬁnd some way to compress it, and the redundancy
corresponds to the rule of how we compress it. Summarizing the statement of Sec. 2 up to now,
the learning is the discovery of the correlation, the correlation makes the probability bias, the
probability bias makes the redundancy, and the redundancy can be compressed by the proper rule
for the compression. We can conclude that learning is the discovery of the rule for the compression
of information.
2.5 Compression of information
The next question is how our brain compresses the information. This is nothing but the theme of
this paper, and our answer is explained as follows. The ultimate motivation of the human brain is
to survive, and for this purpose, the human brain tries to record incoming information as much as
possible. However, the amount of incoming information is very huge, and the human brain is limited,
so it can record only compressed information. For example, when we see the image of an apple,
millions of our optic neurons carry the information to our brain, but we only remember very abstract
information such as “the image was an apple.”, and we forget the details of the image. This is lossy
compression, and there is the loss of information by forgetting the detail of the information. In this
process, the human brain tries to reduce the amount of information to remember and also tries to
reduce the loss of information, as a survival strategy. We develop the quantitative evaluation of these
amounts of information and ﬁnd that the ideal data compression can be achieved by minimizing these
values. In the following paper, we discuss the evaluation of these values and how the compression is
achieved by considering the NN model as a toy model of our brain.
3 The model of neural network
In this section, we deﬁne a new NN model as a toy model of the human brain to evaluate the amount
of information to remember and the loss of the information. Our goal is to derive these quantities,
5
*OQVUMBZFS
&ODPEJOHMBZFST %FDPEJOHMBZFST
0VUQVUMBZFS.JEEMFMBZFS
Figure 1: The conceptual diagram of autoencoder NN model. The autoencoder has input layer, encoding
layer, middle layer, decoding layers and output layer. The dimension of the middle layer is less than that
of the input layer. The output of the NN tries to reproduce the input layer from the middle layer with less
dimension.
and we do not care about the actual implementation of the model.
First, as an example of the NN model which compresses the input data without ad hoc treatment,
let us explain the overview of autoencoder NN [5]. The autoencoder has the input layer, encoding
layers, middle layer, decoding layers, and output layer as shown in Fig. 1. Here, the dimension of
the middle layer is less than that of the input layer, and the dimension of the output layer is the
same as that of the input layer. The loss function is deﬁned as the diﬀerence between the input
and output, so the NN tries to restore the input data from the reduced information in the middle
layer. We can consider this system as the compression and decompression of the input data, and
the middle layer can extract the features of the input data.
However, this model has two problems to achieve our goal. First, there is no guideline to choose
the dimension of the middle layer, so we choose it looking at the result. We have to step out of such
ad hoc treatment. The second problem is there is no evaluation of the amount of information to
remember, and the NN only tries to reduce the loss of the information. We need a new system to
treat the amount of information to remember and the loss of the information on the same ground.
We suggest a new NN model to solve these problems. For the ﬁrst problem, we do not constrain
the dimension of the middle layer if it is large enough to contain the features of the input. For the
second problem, we record the value of variables in the middle layer to the storage every time a new
input enters, and use them to evaluate the amount of information to remember. We call the value
of the middle layer the memory in the following paper, and we need to calculate the self-information
of the memory. The conceptual diagram for this model is shown in Fig. 2. We will discuss how the
self-information of the memory is evaluated in Sec. 4.
6
*OQVUMBZFS
&ODPEJOHMBZFST %FDPEJOHMBZFST
0VUQVUMBZFS.JEEMFMBZFS
5PTUPSBHF 4UPSBHF
.FNPSJFT
Figure 2: The conceptual diagram of our NN model. Diﬀerent from the usual autoencoder model, the
dimension of the middle layer does not need to be less than that of the input layer. Instead, we record the
value of the middle layer to the storage every time when new input enters and the calculation is done. We
call the output from the middle layer the memory.
To clarify the situation, we explain it more concretely. Suppose the dimension of the input layer
is 4, the dimension of the middle layer is 3, and the dimension of the input layer is 4. The input E
has the form of a vector with dimension 4, and it is written as
E = (e1,e2,e3,e4) . (10)
The output from the middle layer, or the memory M, has the form of a vector with dimension 3,
and it is written as
M = (m1,m2,m3) , (11)
where M is calculated from the input E and we record M to the storage. The output O has the
form of a vector with dimension 4, and it is written as
O= (o1,o2,o3,o4) , (12)
where Ois calculated from the memory M. If a new input E′enters, the NN calculate a new memory
M′and a new output O′, and we record M′to the storage. Repeating this process, we have a lot of
memories in the storage. Here, we consider that ei and mi take discrete values, namely, E and M
are discrete vectors. This is a very important assumption, and we will mention it in Sec. 4.
4 The self-information and the loss function
Our goal is to formularize the amount of information to remember and the loss of information. The
amount of information is usually discussed in the framework of self-information. In this framework,
7
we mention how to deﬁne the amount of information of the memory and the loss of information.
Finally, we derive the loss function which can be used generally.
4.1 The self-information
Suppose that E is the event that occurs with the probability P(E). We can consider that the rare
event has large information, and we can formularize the self-information I(E) as
I(E) = −log P(E) . (13)
Let us consider E as the input of the NN deﬁned in Sec. 3. Here, we do not restrict the type of E.
We consider that E corresponds to the raw data of our optic nerve or auditory nerve when we see or
hear something. For example, if E is the event of seeing a cat, we enter E as a form of pixel image
data of a cat. Strictly speaking, our nerves carry the chemical substance, and the number of them
must be discrete not continuous. Hence, we consider E as a discrete vector. For the usual event, we
cannot deﬁne P(E) because there are so many types of inputs and the same input does not occur
twice. However, if we could prepare the inﬁnite number of input data sets, the same events occur
many times for the discrete vector, and we can deﬁne P(E) well. In this case, P(E) takes a very
small but non-zero value, and in the following, we take P(E) as this meaning.
Next, we will derive the amount of information to remember. We deﬁned the memory as the
value of variables in the middle layer in Sec. 3, and let us consider the the amount of information of
the memory M. We also take M as a discrete vector. By the deﬁnition of the self-information, we
can write the self-information of M as
I(M) = −log P(M) . (14)
Here, the question is what P(M) is. This is the probability of getting the memory M, and we can
deﬁne P(M) by using the recorded memories as
P(M) = n(M)
N , (15)
where n(M) is the number of the memory M recorded up to now and N is the total number of
memories in the storage. We emphasize that the NN may record the same memory from diﬀerent
inputs because the memory is a discrete vector, and we count how many times the same memory M
was recorded, which corresponds to n(M). However, if the parameter space of the memory is much
larger than N, n(M) equals 0 usually, and it becomes diﬃcult to deﬁne P(M) well. The prescription
for this problem will be mentioned in Sec. 4.2.
Next, let us consider the loss of information along with the compression. When E enters as the
input, we have all the information of E, but after recording M, we only have the information that
M is recorded. In this situation, there is a possibility that M is recorded from the diﬀerent input
E′ and we cannot tell which was the actual input only from the memory M. For example, suppose
8
drawing one card from playing cards, such as king of hearts. Then, suppose that we only remember
that the suit of the card was the heart and forget the number. For this case, we lose the information
of number and this loss of information L can be evaluated as
L= −log P(King of hearts|Hearts)
= −log (1/13) ,
(16)
where P(A|B) denote the conditional probability where Ahappens under the condition of B. Gener-
alizing this statement to our NN model, the loss of information Lwhen E enters under the condition
that M is recorded is written as
L= −log P(E|M). (17)
Here, we can observe interesting equations from these quantities. From the Bayes’ theorem, we can
write the probability of input E as
P(E) = P(M)P(E|M) . (18)
Then taking the log of both sides and converting the sign, we get
I(E) =
(
−log P(M) −log P(E|M)
)
= I(M) + L.
(19)
This corresponds to the conservation of self-information, which means that some portion of the input
information will be embedded in the memory and the other portion of the input information will be
lost.
4.2 The loss function
Next, we explain how to estimate I(M) and Lusing the NN deﬁned in Sec. 3. Then, we will see this
estimation is nothing but the loss function that we want to derive.
Let us start from the estimation of I(M). For this purpose, we need to estimate n(M). However,
we have the problem that n(M) tends to be 0 when the parameter space of the memory is large. For
example, when the middle layer has 100 dimensions and each node takes a binary value, the number
of cases is 2 100. This value must be larger than N, and exactly the same memory will not appear.
To solve this problem, we add the condition that a similar memory should express similar data, and
take the average among the near region. From this condition, we can consider P(M) ≃P(M′) if
M ≃M′. Then we deﬁne the estimation of P(M) as
P(M) ≡avg
(
P(X)|X∈S
)
= avg
(
n(X)|X∈S
)
N ,
(20)
9
.
.
. .
.
.
E
E E
/FJHICPSIPPES
Figure 3: The conceptual diagram of neighborhood S from the memory M. The memories can be recorded
as the lattice points, and black dots denote the memories in the storage. This diagram corresponds to the
case of taking n = 3 in Eq. (22), and we take the average among the lattice points in the blue circle to
calculate P(M).
where P(M) denotes the estimation of P(M), avg(·) is the function to take the average of the argu-
ment, and S is the neighborhood of the memory M. By taking the average among the neighborhood
S, we can get a non-zero value for P(M) if we set S as large enough to contain a non-zero number
of memories. Here, the problem is how to deﬁne S. One suggestion is as follows. Let us name the
memories in the storage as M1,M2,...,M N , then we calculate the distance between a new memory
M and a recorded memory Mi as
di =
√
(M −Mi)2 . (21)
Next, we rename di, arranging in ascending order as d1 ≤d2 ≤···≤ dN is satisﬁed. Then we deﬁne
S using the distance between memories as
S =
{
X
⏐⏐(X−M)2 ≤(dn)2}
, (22)
where nis the number of the memories that are contained in the neighborhood S, and we can choose
an arbitrary number for n. By doing this, P(M) takes the average in the neighborhood within the
radius dn. We show the conceptual image of the neighborhood S in Fig. 3. This suggestion for the
deﬁnition of S is just an example, and it will be improved in future work.
Next, we mention the estimation Lusing this NN. First, let us discuss the case of Eq. (16) more
10
deeply. The conditional probabilities under the condition of getting hearts are written as
P(Ace of spades|Hearts) = 0 ,
...
P(King of spades|Hearts) = 0 ,
P(Ace of hearts|Hearts) = 1/13 ,
...
P(King of hearts|Hearts) = 1/13 ,
P(Ace of diamonds|Hearts) = 0 ,
...
P(King of diamonds|Hearts) = 0 ,
P(Ace of clubs|Hearts) = 0 ,
...
P(King of clubs|Hearts) = 0 .
(23)
This is the list of P(X|Hearts) where X can take any kind of playing cards, and we can think that
P(King of hearts|Hearts) corresponds to the case with X = King of hearts. By the analogy of this
case, we ﬁrst consider P(X|M) to get P(E|M) where X can be any type of input data. Hence, the
output of the NN tries to estimate the probability distribution P(X|M), and let us call this output
P(X|M). We give an example of such an output. Let us take the input E as binary data set with
dimension 3, such as (0 ,1,0). Through the NN, this input is compressed into the memory M and
the output predicts the probability of the input from the compressed information M. The output
has 3 nodes and each node tries to predict the probability of the corresponding node of input data
to have the value 1. For example, if the output is (0 .1,0.9,0.2), this output predicts that the ﬁrst
node of the input takes 1 with 10 %, the second node of the input takes 1 with 90 %, and the third
node of the input takes 1 with 20 %. In this case, the P(E|M) is calculated as
P(E|M) = (1 −0.1) ×0.9 ×(1 −0.2) . (24)
Though we just explained the case with dimension 3, we can generalize this for any number of input
dimensions. Moreover, any type of input data can be recast into binary data, and it means that we
can always use this method in principle.
Finally, we set the loss function L(E) to be the sum of the estimation of I(M) and L as
L(E) ≡−log P(M) −log P(E|M) . (25)
11
4.3 The meaning of the loss function
To discuss the meaning of the loss function, we need to mention the important property of the
information entropy. Suppose P(X) and Q(X) is the probability distribution about X and
∑
X
P(X) =
∑
X
Q(X) = 1 (26)
is satisﬁed. In this case, we can write the following inequality as
−
∑
X
P(X) logP(X) ≤−
∑
X
P(X) logQ(X) , (27)
where left-hand side corresponds to the information entropy, or the expectation value of self-
information for P(X). This inequality always holds for any probability distribution P(X) and
Q(X), and the equal sign holds only when P(X) = Q(X) is satisﬁed.
Using this inequality for the true probability distribution of input P(E) = P(M)P(E|M) and
the prediction of the probability distribution of input P(E) = P(M)P(E|M), we get
∑
E
P(E)(−log P(M) −log P(E|M)) ≤
∑
E
P(E)(−log P(M) −log P(E|M)) . (28)
Here, the left-hand side corresponds to the expectation value of I(E), and the right-hand side
corresponds to the expectation value of L(E). Then we can write the following inequality as
I(E)|exp ≤L(E)|exp , (29)
where ·|exp means the expectation value of the function. This inequality has a very important
meaning. As the learning proceeds, the loss function L(E) gets closer to the true self-information
of the input I(E), and the minimum point corresponds to I(E). Using this loss function, we can
estimate the actual self-information I(E) without knowing the true probability distribution P(E).
5 The role of memory and the hidden variables
In Sec. 2, we mentioned that learning is the detection of redundancy. In this section, we discuss how
the redundancy is compressed, and we also consider the role of the memory.
5.1 The redundancy
Although we brieﬂy explained about the redundancy in Sec. 2.3, we will explain the more general
and complicated cases here. Suppose that we take the animal images as the input of the NN, and
each image is expressed as the pixel binary data. Let us discuss what is redundancy in this case.
Taking the number of the pixels of the input data as l, the number of the cases of the input data is
12
2l. If there is no probability bias, each data will appear completely randomly, and we can calculate
the maximum of the information entropy Hmax as
Hmax = −log 2−l , (30)
Among the random inputs, only a very tiny parameter region can be considered to be an animal
image. Hence, the input of animal image have very peaky probability bias on the parameter space
of the input, and the expectation value of self-information is written as
I(E)|exp = −
∑
E
P(E) logP(E)
= −log 2−l −R,
(31)
where E denotes the input of animal image, and R is the redundancy deﬁned in Eq. (3). From the
eq. (29), the NN compresses R as the learning proceed.
5.2 The hidden variables
The input of the animal image has l variables, and these variables are strongly correlated and form
probability bias. We can consider this situation as follows. There are hidden variables such as
the kind of the animal, the size of the animal, and the posture of the animal, and the input data
is decided by these hidden variables. Here, the dimension of the hidden variables is less than l.
Considering the meaning of the redundancy R from this perspective, we ﬁnd there are two types of
redundancy. The ﬁrst is the redundancy from the mapping of the hidden variables to input data.
For example, if the hidden variables specify the kind of animal as a cat, there must be mapping
to input data that looks like a cat, and it makes the correlation between the input variables. The
second is the redundancy from the prior probability distribution of hidden variables. For example,
if the probability of the hidden variables which denote a cat is much higher than that of a kangaroo,
it also can be a source of redundancy.
5.3 The role of memory
The NN tries to reduce −log P(M), which means the NN tries to increase P(M), and from Eq. (20),
the NN tries to record the memories as near as possible. We show the conceptual diagram in Fig. 4.
Namely, the memory tries to record the information with fewer variables, and this is nothing but
the hidden variables. From this consideration, we guess that the redundancy from mapping of the
hidden variables to input data is embedded in the encoding layer, and the redundancy from the prior
probability distribution of the hidden variables is embedded in the probability distribution of the
memory. The hidden variables are considered to have abstract information, such as “this is a cat”
or “this is sitting”, and recording the memory corresponds to recording such abstract information.
This is very similar to the human brain which only remembers abstract information and forgets
trivial information.
13
1BSBNFUFSTQBDFPGUIFNFNPSZ
1SPCBCJMJUZEJTUSJCVUJPO
1BSBNFUFSTQBDFPGUIFNFNPSZ
1SPCBCJMJUZEJTUSJCVUJPO
1BSBNFUFSTQBDFPGUIFNFNPSZ
1SPCBCJMJUZEJTUSJCVUJPO
Figure 4: The conceptual diagram which shows how the memories are recorded and how the probability
distribution of the memory is updated. Each vertical black line denotes each memory.
Next, let us discuss the probability distribution of the memory. Suppose the learning of the
animal image, there considered to be many hidden variables, such as “having hair”, “having a tail”,
or “having slippery skin”. Among the combination of these hidden variables, some combinations will
appear frequently. For example, the combination of “having slippery skin” and “having a streamlined
shape” means the characteristics of ﬁsh, and other animals also have such a combination of the hidden
variables. These combinations have a higher probability than random combinations, and it means
that there emerge some peaks of the probability distribution in the parameter space of memory. We
can consider that each peak corresponds to the species of animals. If the learning proceed more,
there emerge minor peaks which correspond to a detailed classiﬁcation of the animals. We illustrate
this situation in Fig. 5. In this ﬁgure, the darker region has a higher probability distribution, which
means the density of the recorded memories is higher in such region. The important point is that
such a structure of the probability distribution of memory automatically emerges without ad hoc
treatment. We guess that this is exactly the way how the human brain remembers and learns things.
5.4 The memory as input
Next, let us consider what happens if we set the memories as input. In Sec. 5.3, we mentioned that
memories also have their probability distribution. This means that memories are also controlled by
other hidden variables, and we can compress the information again. Repeating this process many
times, we can ﬁnd deeper hidden variables which express more abstract information. We can also set
diﬀerent types of memories as input simultaneously, then the NN will learn the correlation between
diﬀerent types of information. For example, if we enter the memories from voice data of the name of
animals and the memories from image data of animals simultaneously, the NN will learn the name
of animals. The essence of learning is ﬁnding the hidden variables by compressing the input data,
and the NN will learn more deeply by repeating this process.
14
1BSBNFUFSTQBDFPGNFNPSZ
<BOJNBM>
<CJSE>
<pTI>
<NBNNBM>
<QJHFPO>
<TQBSSPX>
<TIBSL>
<TBMNPO>
<DBU>
<EPH>
Figure 5: The conceptual diagram of the probability distribution in the parameter space of memory. We
considered the case of learning the image of animals, and a darker region denote higher probability, in other
words, memories are distributed with a higher density in a darker region. The peaks of the probability
distribution correspond to the species of animals.
6 The uncertainty of the solution
Here, we mention the uncertainty of the solution which minimizes this loss function. The self-
information of the memory and the loss of information are inextricably linked, and there are several
types of realization to obtain the minimum of the loss function whether the NN tends to record
information as much as possible or abandon information as much as possible. To control this
uncertainty, we suggest a new loss function as
L(E; α,β) = −(1 + α) logP(M) −(1 + β) logP(E|M) (32)
where α and β is a small non-negative constant. In the following, we demonstrate how this loss
function works by changing the value of α and β, using a very simple example. To help understand
how this loss function works, we consider two-dimensional binary input data and name them as
E1 ≡(0,0), E2 ≡(0,1), E3 ≡(1,0), E4 ≡(1,1). (33)
We also set the memory to have the shape of two-dimensional binary data and name them as
M1 ≡(0,0), M2 ≡(0,1), M3 ≡(1,0), M4 ≡(1,1). (34)
Then, for instance, let us consider the case mentioned in Sec. 2.2 and set the probability as same as
Eq.(2)
P(E1) = 0.6, P(E2) = 0.1, P(E3) = 0.1, P(E4) = 0.2 . (35)
The expectation value of the self-information of input data is calculated as
I(E)|exp = −
4∑
i=1
P(Ei) logP(Ei)
= 1.088 ··· .
(36)
15
Next, we consider when the loss function takes its minimum, changing the value of α and β. Here,
there are only four types of memories, and we do not need to average over the neighborhood to
calculate P(M).
6.1 The case with α=0, β =0.01
Let us take L(E; 0,0.01) as the loss function. The minimum of the loss function is achieved when
the encoding layers work as
E1 →M1,
E2 →M2,
E3 →M3,
E4 →M4,
(37)
and the decoding layers work as
M1 →(p1,p2) = (0,0),
M2 →(p1,p2) = (0,1),
M3 →(p1,p2) = (1,0),
M4 →(p1,p2) = (1,1).
(38)
Then the expectation value of the loss function is
L(E,0,0.01)|exp = −0.6 ×[(1 + 0) log 0.6 + (1 + 0.01) log 1]
−0.1 ×[(1 + 0) log 0.1 + (1 + 0.01) log 1]
−0.1 ×[(1 + 0) log 0.1 + (1 + 0.01) log 1]
−0.2 ×[(1 + 0) log 0.2 + (1 + 0.01) log 1]
=1.088 ··· .
(39)
6.2 The case with α=0.01, β =0
Next, let us take L(E; 0.01,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E1 →M1,
E2 →M1,
E3 →M2,
E4 →M2,
(40)
and the decoding layers work as
M1 →(p1,p2) = (0,1/7),
M2 →(p1,p2) = (1,2/3),
(41)
16
Then the expectation value of the loss function is
L(E,0.01,0)|exp = −0.6 ×[(1 + 0.01) log 0.7 + (1 + 0) log 6/7]
−0.1 ×[(1 + 0.01) log 0.7 + (1 + 0) log 1/7]
−0.1 ×[(1 + 0.01) log 0.3 + (1 + 0) log 1/3]
−0.2 ×[(1 + 0.01) log 0.3 + (1 + 0) log 2/3]
=1.095 ··· .
(42)
6.3 The case with α=0.2, β =0
Next, let us take L(E; 0.2,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E1 →M1,
E2 →M1,
E3 →M1,
E4 →M2,
(43)
and the decoding layers work as
M1 →(p1,p2) = (1/8,1/8),
M2 →(p1,p2) = (1,1),
(44)
Then the expectation value of the loss function is
L(E,0.2,0)|exp = −0.6 ×[(1 + 0.2) log 0.8 + (1 + 0) log 49/64]
−0.1 ×[(1 + 0.2) log 0.8 + (1 + 0) log 7/64]
−0.1 ×[(1 + 0.2) log 0.8 + (1 + 0) log 7/64]
−0.2 ×[(1 + 0.2) log 0.2 + (1 + 0) log 1]
=1.203 ··· .
(45)
6.4 The case with α=0.5, β =0
Next, let us take L(E; 0.5,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E1 →M1,
E2 →M1,
E3 →M1,
E4 →M1,
(46)
17
and the decoding layers work as
M1 →(p1,p2) = (0.3,0.3), (47)
In this case, there are only four type of memories, and we do not need to average over the neighbor-
hood to calculate P(M). Then the expectation value of the loss function is
L(E,0.5,0)|exp = −0.6 ×[(1 + 0.5) log 1 + (1 + 0) log 0.49]
−0.1 ×[(1 + 0.5) log 1 + (1 + 0) log 0.21]
−0.1 ×[(1 + 0.5) log 1 + (1 + 0) log 0.21]
−0.2 ×[(1 + 0.5) log 1 + (1 + 0) log 0.09]
=1.221 ··· .
(48)
6.5 Properties of the loss functions
From these observations, we can state as follows. If α is zero, the NN tries to record as much
information as possible, and if β is zero, the NN tries to abandon as much information as possible.
As α becomes bigger, the NN starts to reduce the information to remember, and abandons the
information of the correlation between two nodes. For the case with α = 0 .01,β = 0 and α =
0.2,β = 0, the NN uses two memories M1 and M2. Considering the case mentioned in Sec. 2.2, the
memory M2 corresponds to the concept which connects the word cat and the visual image of a cat.
7 Discussion
First, we discuss how we solved the problem mentioned in Sec. 1. We wondered about the trend of
the current study of the NN, in which people focus on how to get good results by ad hoc treatment.
People are trying to improve the NN model, the loss function, or the style of input data. However,
considering the human brain, it does not need such treatments and can learn everything automat-
ically in daily life. There must be a uniﬁed theory that controls the human brain, and we do not
need ad hoc treatment under such a theory. This theory is nothing but the loss function we deﬁned
in Eq.(25). Let us mention that our theory includes previous ad hoc treatments. For example,
convolutional NN is doing coarse-graining, and this can be considered as some kind of compression
of the input data. We know the fact coarse-graining does not lose so much information, and this
fact is implemented in the NN ad hoc. The situation for other good-looking models are more or less
same. People consider the property of the input data and try to use this property and extract the
features of input data by improving the NN models. This corresponds to teaching the NN how to
compress the input data ad hoc. Our theory oﬀers a uniﬁed treatment for any kind of input data.
Next, we discuss the memory. We mentioned that the memory expresses the hidden variable
in Sec. 5.2, but the realization is not unique. The probability distribution of the memory would
18
be diﬀerent if we clean up all the memories and restart the learning. Moreover, if we change the
deﬁnition of the neighborhood S considered in Sec. 4.1 or change the value of α and β considered
in Sec. 6, the probability distribution of the memory will change more drastically. There is a lot of
uncertainty, and we need to improve the theory about the hidden variables. Let us also mention
about the treatment of old memories. As the learning proceeds, many memories are recorded in the
storage, but the memories recorded in the early stage of learning do not follow the desired probability
distribution. To improve the convergence of the calculation, we should remove such memories from
the storage. This also remains as future work.
Finally, we discuss the similarity between this NN and the human brain. This NN is suggested
as a toy model of the human brain, and we do not need ad hoc treatment for the diﬀerent inputs.
We can use the same type of NN for a diﬀerent types of input data, and this is very similar to
our brain which can adapt to any kind of problem. This is achieved only by minimizing the loss
function, which is deﬁned as the sum of the self-information to remember and the loss of information.
This situation amazingly agrees with the free-energy principle [6]. The free-energy principle is the
theory in cognitive science proposed by Karl Friston, in which the cognition, learning, and action
of the organism are determined by minimizing the function called free-energy. Our loss function
corresponds to this free-energy, and it is formularized in a calculable way. To calculate the self-
information to remember, we introduced the concept of memory. This is the core of our theory,
and it has many similarities with our actual memory. The memory denotes compressed information,
and it can express abstract information. The human brain also remembers only compressed and
abstract information. In the paragraph above, we discussed the beneﬁt to forget old memories, and
this situation is also similar to our brain, as the human brain tends to forget old memories which
are not often referred to.
8 Conclusion
In this paper, we proposed a uniﬁed theory of learning. Learning is the discovery of the correlation
between the information, and this can be achieved through the compression of the information. To
explain how this compression is carried out, we treat a NN model as a toy model of the human
brain and derived a new loss function. In this process, we proposed the concept of memory, and
this is the core of our theory. The loss function is expressed as the sum of the self-information of
the memory and the loss of information. This reﬂects the self-information of the input data, and
we can estimate it without knowing the actual probability distribution of the input data only by
calculating the minimum of the loss function. To estimate the self-information of the memory, the
NN refers to the recorded memories. Usually, the density of the memories is very low, and we need
to refer to similar memories.
We can think that the correlation of the information is caused by the hidden variables, and
the memory expresses these hidden variables. The hidden variables also have their probability
19
distribution, and it emerges as the density distribution of the memories. The peaks of the density
distribution correspond to the features of the input data. The memories have their probability
distribution, and it means we can ﬁnd deeper hidden variables by setting the memories as input.
We can also set the diﬀerent tyeps of memories as input simultaneously, then the NN will ﬁnd the
correlation between diﬀerent data. This means the NN can learn the abstract concept, such as the
meaning of the words, by setting the memories of audio data and image data as input. Deeper
hidden variables express more abstract information, and the reputation of this process is nothing
but how the human brain learns.
We also found many common features in this NN model and the human brain. We do not need
ad hoc treatment for the diﬀerent types of inputs, and this is the same as the human brain which
learns by itself. This is achieved only by minimizing the loss function which denotes the sum of the
information to remember and the loss of the information, and this corresponds to the free-energy
theorem which is considered to be the uniﬁed theory of the human brain. The memory of this NN
works very similarly to our actual memory, and it can express very abstract information. The similar
information is recorded as a similar memory. We also mentioned the beneﬁt to forget old memories.
This work has a lot of implications for a very wide range of ﬁelds, and it can be applied to any
kind of data analysis. However, there will be many diﬃculties in the implementation of this work in
the actual NN system. We only oﬀered the deﬁnition of the loss function, and we have not checked
the NN numerically. To improve the convergence of the calculation, we will need a lot of study.
We also oﬀered the concepts of memory and hidden variables, but these concepts have so many
theoretical uncertainties. They are totally new concepts and we do not know the properties of them
at all. They will be understood through the calculation of concrete examples. We need not only
experimental approach but also theoretical approach to the NN.
Acknowledgments
The author would like to thank Prof. Haruhiro Katayose for the useful discussion and suggestions.
This work is supported in part by the JSPS KAKENHI Grant No. 20H00160.
References
[1] J. Schmidhuber, Neural Networks 61, 85 (2015).
[2] K. O’Shea and R. Nash, CoRR abs/1511.08458 (2015), 1511.08458 .
[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin, CoRR abs/1706.03762 (2017), 1706.03762 .
[4] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, CoRR
abs/1606.03498 (2016), 1606.03498 .
20
[5] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 (2006).
[6] K. Friston, Nature Reviews Neuroscience 11, 127 (2010).
21