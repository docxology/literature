rspa.royalsocietypublishing.org
Research
Article submitted to journal
Subject Areas:
Stochastic processes, mathematical
biology, Bayesian statistics,
stochastic control, mathematical
modelling, statistical physics
Keywords:
Markov blanket, variational Bayesian
inference, active inference,
non-equilibrium steady-state,
predictive processing, free-energy
principle
Author for correspondence:
Lancelot Da Costa
e-mail: l.da-costa@imperial.ac.uk
Bayesian Mechanics for
Stationary Processes
Lancelot Da Costa1,2, Karl Friston2, Conor
Heins3,4,5 and Grigorios A. Pavliotis1
1Department of Mathematics, Imperial College
London, London SW7 2AZ, UK
2Wellcome Centre for Human Neuroimaging,
University College London, London WC1N 3AR, UK
3Department of Collective Behaviour, Max Planck
Institute of Animal Behavior, Konstanz D-78457,
Germany
4Centre for the Advanced Study of Collective
Behaviour, University of Konstanz, Konstanz D-78457,
Germany
5Department of Biology, University of Konstanz,
Konstanz D-78457, Germany
This paper develops a Bayesian mechanics for
adaptive systems.
Firstly, we model the interface between a system
and its environment with a Markov blanket. This
affords conditions under which states internal to the
blanket encode information about external states.
Second, we introduce dynamics and represent
adaptive systems as Markov blankets at steady-state.
This allows us to identify a wide class of systems
whose internal states appear to infer external states,
consistent with variational inference in Bayesian
statistics and theoretical neuroscience.
Finally, we partition the blanket into sensory and
active states. It follows that active states can be seen
as performing active inference and well-known forms
of stochastic control (such as PID control), which
are prominent formulations of adaptive behaviour in
theoretical biology and engineering.
1 Introduction
Any object of study must be, implicitly or explicitly,
separated from its environment. This implies a boundary
that separates it from its surroundings, and which
persists for at least as long as the system exists.
© The Authors. Published by the Royal Society under the terms of the
Creative Commons Attribution License http://creativecommons.org/licenses/
by/4.0/, which permits unrestricted use, provided the original author and
source are credited.
arXiv:2106.13830v3  [math-ph]  26 Oct 2021
2rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
In this article, we explore the consequences of a boundary mediating interactions between
states internal and external to a system. This provides a useful metaphor to think about biological
systems, which comprise spatially bounded, interacting components, nested at several spatial
scales [1,2]: for example, the membrane of a cell acts as a boundary through which the cell
communicates with its environment, and the same can be said of the sensory receptors and
muscles that bound the nervous system.
By examining the dynamics of persistent, bounded systems, we identify a wide class of
systems wherein the states internal to a boundary appear to infer those states outside the
boundary—a description which we refer to as Bayesian mechanics. Moreover, if we assume that
the boundary comprises sensory and active states, we can identify the dynamics of active states
with well-known descriptions of adaptive behaviour from theoretical biology and stochastic
control.
In what follows, we link a purely mathematical formulation of interfaces and dynamics with
descriptions of belief updating and behaviour found in the biological sciences and engineering.
Altogether, this can be seen as a model of adaptive agents, as these interface with their
environment through sensory and active states and furthermore behave so as to preserve a target
steady-state.
(a) Outline of paper
This paper has three parts, each of which introduces a simple, but fundamental, move.
(i) The ﬁrst is to partition the world into internal and external states whose boundary is
modelled with a Markov blanket [3,4]. This allows us to identify conditions under which
internal states encode information about external states.
(ii) The second move is to equip this partition with stochastic dynamics. The key
consequence of this is that internal states can be seen as continuously inferring external
states, consistent with variational inference in Bayesian statistics and with predictive
processing accounts of biological neural networks in theoretical neuroscience.
(iii) The third move is to partition the boundary into sensory and active states. It follows that
active states can be seen as performing active inference and stochastic control, which are
prominent descriptions of adaptive behaviour in biological agents, machine learning and
robotics.
(b) Related work
The emergence and sustaining of complex (dissipative) structures have been subjects of
long-standing research starting from the work of Prigogine [5,6], followed notably by Haken’s
synergetics [7], and in recent years, the statistical physics of adaptation [8]. A central theme of
these works is that complex systems can only emerge and sustain themselves far from equilibrium
[9–11].
Information processing has long been recognised as a hallmark of cognition in biological
systems. In light of this, theoretical physicists have identiﬁed basic instances of information
processing in systems far from equilibrium using tools from information theory, such as how
a drive for metabolic efﬁciency can lead a system to become predictive [12–15].
A fundamental aspect of biological systems is a self-organisation of various interacting
components at several spatial scales [1,2]. Much research currently focuses on multipartite
processes—modelling interactions between various sub-components that form biological
systems—and how their interactions constrain the thermodynamics of the whole [16–20].
At the conﬂuence of these efforts, researchers have sought to explain cognition in biological
systems. Since the advent of the 20th century, Bayesian inference has been used to describe various
cognitive processes in the brain [21–25]. In particular, the free energy principle [23], a prominent
3rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
theory of self-organisation from the neurosciences, postulates that Bayesian inference can be used
to describe the dynamics of multipartite, persistent systems modelled as Markov blankets at non-
equilibrium steady-state [26–30].
This paper connects and develops some of the key themes from this literature. Starting
from fundamental considerations about adaptive systems, we develop a physics of things that
hold beliefs about other things–consistently with Bayesian inference–and explore how it relates
to known descriptions of action and behaviour from the neurosciences and engineering. Our
contribution is theoretical: from a biophysicist’s perspective, this paper describes how Bayesian
descriptions of biological cognition and behaviour can emerge from standard accounts of physics.
From an engineer’s perspective this paper contextualises some of the most common stochastic
control methods and reminds us how these can be extended to suit more sophisticated control
problems.
(c) Notation
Let Π∈Rd×dbe a square matrix with real coefﬁcients. Letη,b,µ denote a partition of the states
[ [1,d] ], so that
Π=


Πη Πηb Πηµ
Πbη Πb Πbµ
Πµη Πµb Πµ

.
We denote principal submatrices with one index only (i.e., we use Πη instead of Πηη). Similarly,
principal submatrices involving various indices are denoted with a colon
Πη:b:=
[
Πη Πηb
Πbη Πb
]
.
When a square matrix Π is symmetric positive-deﬁnite we write Π≻0. ker, Im and ·−
respectively denote the kernel, image and Moore-Penrose pseudo-inverse of a linear map or
matrix, e.g., a non-necessarily square matrix such as Πµb. In our notation, indexing takes
precedence over (pseudo) inversion, for example,
Π−
µb:=
(
Πµb
)−̸= (Π−)µb.
2 Markov blankets
The section formalises the notion of boundary between a system and its environment as a Markov
blanket [3,4], depicted graphically in Figure 1. Intuitive examples of a Markov blanket are that of
a cell membrane, mediating all interactions between the inside and the outside of the cell, or that
of sensory receptors and muscles that bound the nervous system.
To formalise this intuition, we model the world’s state as a random variable x with
corresponding probability distribution pover a state-space X= Rd. We partition the state-space
of xinto external, blanket and internal states:
x= (η,b,µ )
X= E×B×I .
External, blanket and internal state-spaces (E,B,I) are taken to be Euclidean spaces for simplicity.
A Markov blanket is a statement of conditional independence between internal and external
states given blanket states.
Deﬁnition 2.1 (Markov blanket). A Markov blanket is deﬁned as
4rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 1. Markov blanketdepicted graphically as an undirected graphical model, also known as a Markov random
ﬁeld [4,31]. (A Markov random ﬁeld is a Bayesian network whose directed arrows are replaced by undirected arrows). The
circles represent random variables. The lines represent conditional dependencies between random variables. The Markov
blanket condition means that there is no line between µand η. This means that, µand ηare conditionally independent
given b. In other words, knowing the internal state µ, does not afford additional information about the external state η
when the blanket state bis known. Thus blanket states act as an informational boundary between internal and external
states.
η⊥µ|b (M.B.)
That is, blanket states are a Markov blanket separating µ,η [3,4].
The existence of a Markov blanket can be expressed in several equivalent ways
(M.B.) ⇐⇒p(η,µ|b) =p(η|b)p(µ|b) ⇐⇒p(η|b,µ) =p(η|b) ⇐⇒p(µ|b,η) =p(µ|b). (2.1)
For now, we will consider a (non-degenerate) Gaussian distributionpencoding the distribution
of states of the world
p(x) :=N(x; 0,Π−1), Π ≻0,
with associated precision (i.e., inverse covariance) matrix Π. Throughout, we will denote the
(positive deﬁnite) covariance by Σ:= Π−1. Unpacking (2.1) in terms of Gaussian densities, we
ﬁnd that a Markov blanket is equivalent to a sparsity in the precision matrix
(M.B.) ⇐⇒Πηµ= Πµη= 0. (2.2)
Example 2.1. For example,
Π=


2 1 0
1 2 1
0 1 2

⇒Σ−1
η:b =
[
2 1
1 1 .5
]
,Σ−1
b:µ=
[
1.5 1
1 2
]
Then,
p(η,µ|b) ∝p(η,µ,b ) ∝exp
(
−1
2x·Πx
)
∝exp
(
−1
2
[
η,b
]
Σ−1
η:b
[
η
b
]
−1
2
[
b,µ
]
Σ−1
b:µ
[
b
µ
])
∝p(η,b)p(b,µ) ∝p(η|b)p(µ|b).
Thus, the Markov blanket condition (2.1) holds.
5rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(a) Expected internal and external states
Blanket states act as an information boundary between external and internal states. Given
a blanket state, we can express the conditional probability densities over external and internal
states (using (2.1) and [32, Prop. 3.13])1
p(η|b) =N(η; ΣηbΣ−1
b b, Π−1
η ),
p(µ|b) =N(µ; ΣµbΣ−1
b b, Π−1
µ ).
(2.3)
This enables us to associate to any blanket state its corresponding expected external and
expected internal states:
η(b) =E[η|b] =Ep(η|b)[η] =ΣηbΣ−1
b b∈E
µ(b) =E[µ|b] =Ep(µ|b)[µ] =ΣµbΣ−1
b b∈I.
Pursuing the example of the nervous system, each sensory impression on the retina and
oculomotor orientation (blanket state) is associated with an expected scene that caused sensory
input (expected external state) and an expected pattern of neural activity in the visual cortex
(expected internal state) [33].
(b) Synchronisation map
A central question is whether and how expected internal states encode information about
expected external states. For this, we need to characterise a synchronisation function σ, mapping
the expected internal state to the expected external state, given a blanket state σ(µ(b)) =η(b).
This is summarised in the following commutative diagram:
b∈B
Image(η) Image(µ)
η µ
σ
The existence of σ is guaranteed, for instance, if the expected internal state completely
determines the blanket state—that is, when no information is lost in the mapping b↦→µ(b) in
virtue of it being one-to-one. In general, however, many blanket states may correspond to an
unique expected internal state. Intuitively, consider the various neural pathways that compress
the signal arriving from retinal photoreceptors [34], thus many different (hopefully similar) retinal
impressions lead to the same signal arriving in the visual cortex.
i Existence
The key for the existence of a functionσmapping expected internal states to expected external
states given blanket states, is that for any two blanket states associated with the same expected
internal state, these be associated with the same expected external state. This non-degeneracy
means that the internal states (e.g., patterns of activity in the visual cortex) have enough capacity
to represent all possible expected external states (e.g., 3D scenes of the environment). We formalise
this in the following Lemma:
Lemma 2.1. The following are equivalent:
(i) There exists a function σ: Image(µ) →Image(η) such that for any blanket state b∈B
σ(µ(b)) =η(b).
1Note that Πη,Πµare invertible as principal submatrices of a positive deﬁnite matrix.
6rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(ii) For any two blanket states b1,b2 ∈B
µ(b1) =µ(b2) ⇒η(b1) =η(b2).
(iii) ker Σµb⊂ker Σηb.
(iv) ker Πµb⊂ker Πηb.
See Appendix A for a proof of Lemma 2.1.
Example 2.2. • When external, blanket and internal states are one-dimensional, the existence of a
synchronisation map is equivalent to Πµb̸= 0or Πµb= Πηb= 0.
• If Πµb is chosen at random–its entries sampled from a non-degenerate Gaussian or uniform
distribution–then Πµb has full rank with probability 1. If furthermore the blanket state-space
Bhas lower or equal dimensionality than the internal state-space I, we obtain that Πµb is one-
to-one (i.e., ker Πµb= 0) with probability 1. Thus, in this case, the conditions of Lemma 2.1 are
fulﬁlled with probability 1.
ii Construction
The key idea to map an expected internal state µ(b) to an expected external state η(b) is to: 1)
ﬁnd a blanket state that maps to this expected internal state (i.e., by inverting µ) and 2) from this
blanket state, ﬁnd the corresponding expected external state (i.e., by applying η):
b∈B
Image(η) Image(µ)
η µ
σ=η◦µ−
µ−
We now proceed to solving this problem. Given an internal stateµ, we study the set of blanket
states bsuch that µ(b) =µ
µ(b) =ΣµbΣ−1
b b= µ ⇐⇒b∈µ−1(µ) =ΣbΣ−1
µb µ. (2.4)
Here the inverse on the right hand side of (2.4) is understood as the preimage of a linear map. We
know that this system of linear equations has a vector space of solutions given by [35]
µ−1(µ) =
{
ΣbΣ−
µbµ+
(
Id −ΣbΣ−
µbΣµbΣ−1
b
)
b: b∈B
}
. (2.5)
Among these, we choose
µ−(µ) =ΣbΣ−
µbµ.
Deﬁnition 2.2 (Synchronisation map). We deﬁne a synchronisation function that maps to an internal
state a corresponding most likely internal state23
σ: Imµ→Im η
µ↦→η(µ−(µ)) =ΣηbΣ−
µbµ= Π−1
η ΠηbΠ−
µbΠµµ.
The expression in terms of the precision matrix is a byproduct of Appendix A.
Note that we can always deﬁne such σ, however, it is only when the conditions of Lemma
2.1 are fulﬁlled that σ maps expected internal states to expected external states σ(µ(b)) =η(b).
When this is not the case, the internal states do not fully represent external states, which leads to
a partly degenerate type of representation, see Figure 2 for a numerical illustration obtained by
2This mapping was derived independently of our work in [36, Section 3.2].
3Replacing µ−(µ) by any other element of (2.5) would lead to the same synchronisation map provided that the conditions
of Lemma 2.1 are satisﬁed.
7rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 2. Synchronisation map: example and non-example.This ﬁgure plots expected external states given blanket
states η(b) (in orange), and the corresponding prediction encoded by internal states σ(µ(b)) (in blue). In this example,
external, blanket and internal state-spaces are taken to be one dimensional. We show the correspondence under the
conditions of Lemma 2.1 (left panel) and when these are not satisﬁed (right panel). To generate these data, 1) we drew
106 samples from a Gaussian distribution with a Markov blanket, 2) we partitioned the blanket state-space into several
bins, 3) we obtained the expected external and internal states given blanket states empirically by averaging samples
from each bin, and ﬁnally, 4) we applied the synchronisation map to the (empirical) expected internal states given blanket
states.
sampling from a Gaussian distribution, in the non-degenerate (left) and degenerate cases (right),
respectively.
3 Bayesian mechanics
In order to study the time-evolution of systems with a Markov blanket, we introduce dynamics
into the external, blanket and internal states. Henceforth, we assume a synchronisation map under
the conditions of Lemma 2.1.
(a) Processes at a Gaussian steady-state
We consider stochastic processes at a Gaussian steady-state p with a Markov blanket. The
steady-state assumption means that the system’s overall conﬁguration persists over time (e.g.,
it does not dissipate). In other words, we have a Gaussian density p= N(0,Π−1) with a Markov
blanket (2.2) and a stochastic process distributed according to pat every point in time
xt∼p= N(0,Π−1) for any t.
Recalling our partition into external, blanket and internal states, this affords a Markov blanket
that persists over time, see Figure 3
xt= (ηt,bt,µt) ∼p⇒ηt⊥µt|bt. (3.1)
Note that we do not requirextto be independent samples from the steady-state distributionp.
On the contrary, xt may be generated by extremely complex, non-linear, and possibly stochastic
equations of motion. See Example 3.1 and Figure 4 for details.
Example 3.1. The dynamics of xt are described by a stochastic process at a Gaussian steady-state p=
N(0,Π−1). There is a large class of such processes, for example:
8rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 3. Markov blanket evolving in time.We use a bacillus to depict an intuitive example of a Markov blanket that
persists over time. Here, the blanket states represent the membrane and actin ﬁlaments of the cytoskeleton, which mediate
all interactions between internal states and the external medium (external states).
Figure 4. Processes at a Gaussian steady-state. This ﬁgure illustrates the synchronisation map and transition
probabilities of processes at a Gaussian steady-state.Left: We plot the synchronisation map as in Figure 2, only, here, the
samples are drawn from trajectories of a diffusion process(3.2) with a Markov blanket. Although this is not the case here,
one might obtain a slightly noisier correspondence between predictions σ(µ(bt)) and expected external states η(bt)—
compared to Figure 2—in numerical discretisations of a diffusion process. This is because the steady-state of a numerical
discretisation usually differs slightly from the steady-state of the continuous-time process [37]. Right: This panel plots the
transition probabilities of the same diffusion process(3.2), for the blanket state at two different times. The joint distribution
(depicted as a heat map) is not Gaussian but its marginals—the steady-state density—are Gaussian. This shows that in
general, processes at a Gaussian steady-state are not Gaussian processes. In fact, the Ornstein-Uhlenbeck process is
the only stationary diffusion process (3.2) that is a Gaussian process, so the transition probabilities of non-linear diffusion
processes (3.2) are never multivariate Gaussians.
• Stationary diffusion processes, with initial condition x0 ∼p. Their time-evolution is given by an
Itô stochastic differential equation (see Appendix B):
dxt= (Γ + Q)(xt)∇log p(xt)dt+ ∇·(Γ + Q)(xt)dt+ ς(xt)dWt,
= −(Γ + Q)(xt)Πxtdt+ ∇·(Γ + Q)(xt)dt+ ς(xt)dWt
Γ := ςς⊤/2, Q = −Q⊤.
(3.2)
Here, Wt is a standard Brownian motion (a.k.a., Wiener process) [38,39] and ς,Γ,Q are
sufﬁciently well-behaved matrix ﬁelds (see AppendixB). Namely,Γ is the diffusion tensor (half the
covariance of random ﬂuctuations), which drives dissipative ﬂow;Qis an arbitrary antisymmetric
matrix ﬁeld which drives conservative (i.e., solenoidal) ﬂow. We emphasise that there are no non-
degeneracy conditions on the matrix ﬁeldς—in particular, the process is allowed to be non-ergodic
9rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
or even completely deterministic (i.e., ς≡0). Also, ∇·denotes the divergence of a matrix ﬁeld
deﬁned as (∇·(Γ + Q))i:= ∑
j
∂
∂xj
(Γ + Q)ij.
• More generally, xtcould be generated by any Markov process at steady-statep, such as the zig-zag
process or the bouncy particle sampler [40–42], by any mean-zero Gaussian process at steady-state
p[43] or by any random dynamical system at steady-state p[44].
Remark 3.1. When the dynamics are given by an Itô stochastic differential equation (3.2), a Markov
blanket of the steady-state density (2.2) does not preclude reciprocal inﬂuences between internal and
external states [45,46]. For example,
Π=


2 1 0
1 2 1
0 1 2

, Q ≡


0 0 1
0 0 0
−1 0 0

, ς ≡Id3
⇒d


ηt
bt
µt

= −


1 1 .5 2
0.5 1 0 .5
−2 −0.5 1




ηt
bt
µt

dt+ ςdWt.
Conversely, the absence of reciprocal coupling between two states in the drift in some instances, though not
always, leads to conditional independence [30,36,45].
(b) Maximum a posteriori estimation
The Markov blanket (3.1) allows us to harness the construction of Section 2 to determine
expected external and internal states given blanket states
ηt:= η(bt) µt:= µ(bt).
Note that η,µ are linear functions of blanket states; since bt generally exhibits rough sample
paths, ηt,µt will also exhibit very rough sample paths.
We can view the steady-state density pas specifying the relationship between external states
(η, causes) and particular states (b,µ, consequences). In statistics, this corresponds to a generative
model, a probabilistic speciﬁcation of how (external) causes generate (particular) consequences.
By construction, the expected internal states encode expected external states via the
synchronisation map
σ(µt) =ηt,
which manifests a form of generalised synchrony across the Markov blanket [47–49]. Moreover,
the expected internal state µt effectively follows the most likely cause of its sensations
σ(µt) = arg maxp(ηt|bt) for any t.
This has an interesting statistical interpretation as expected internal states perform maximum a
posteriori (MAP) inference over external states.
(c) Predictive processing
We can go further and associate to each internal stateµa probability distribution over external
states, such that each internal state encodes beliefs about external states
qµ(η) :=N(η; σ(µ),Π−1
η ). (3.3)
We will call qµ the approximate posterior belief associated with the internal state µ due to
the forecoming connection to inference. Under this speciﬁcation, the mean of the approximate
posterior depends upon the internal state, while its covariance equals that of the true posterior
w.r.t. external states (2.3). It follows that the approximate posterior equals the true posterior when
the internal state µequals the expected internal state µ(b) (given blanket states):
10rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
qµ(η) =p(η|b) ⇐⇒µ= µ(b). (3.4)
Note a potential connection with epistemic accounts of quantum mechanics; namely, a world
governed by classical mechanics ( σ≡0 in (3.2)) in which each agent encodes Gaussian beliefs
about external states could appear to the agents as reproducing many features of quantum
mechanics [50].
Under this speciﬁcation (3.4), expected internal states are the unique minimiser of a Kullback-
Leibler divergence [51]
µt= arg minµ DKL[qµ(η)∥p(η|b)]
that measures the discrepancy between beliefs about the external world qµ(η) and the posterior
distribution over external variables. Computing the KL divergence (see Appendix C), we obtain
µt= arg minµ (σ(µ) −ηt)Πη(σ(µ) −ηt) (3.5)
In the neurosciences, the right hand side of (3.5) is commonly known as a (squared) precision-
weighted prediction error: the discrepancy between the prediction and the (expected) state of
the environment is weighted with a precision matrix [24,52,53] that derives from the steady-
state density. This equation is formally similar to that found in predictive coding formulations
of biological function [24,54–56], which stipulate that organisms minimise prediction errors, and
in doing so optimise their beliefs to match the distribution of external states.
(d) Variational Bayesian inference
We can go further and associate expected internal states to the solution to the classical
variational inference problem from statistical machine learning [59] and theoretical neurobiology
[52,60]. Expected internal states are the unique minimiser of a free energy functional (i.e., an
evidence bound [59,61])
F(bt,µt) ≥F(bt,µt)
F(b,µ) = DKL[qµ(η)∥p(η|b)] −log p(b,µ)
= Eqµ(η)[−log p(x)]
  
Energy
−H[qµ]
Entropy
.
(3.6)
The last line expresses the free energy as a difference between energy and entropy: energy or
accuracy measures to what extent predicted external states are close to the true external states,
while entropy penalises beliefs that are overly precise.
At ﬁrst sight, variational inference and predictive processing are solely useful to characterise
the average internal state given blanket states at steady-state. It is then surprising to see that the
free energy says a great deal about a system’s expected trajectories as it relaxes to steady-state.
Figure 5 and 6 illustrate the time-evolution of the free energy and prediction errors after exposure
to a surprising stimulus. In particular, Figure 5 averages internal variables for any blanket state:
In the neurosciences, perhaps the closest analogy is the event-triggered averaging protocol, where
neurophysiological responses are averaged following a ﬁxed perturbation, such a predictable
neural input or an experimentally-controlled sensory stimulus (e.g., spike-triggered averaging,
event-related potentials) [62–64].
The most striking observation is the nearly monotonic decrease of the free energy as the system
relaxes to steady-state. This simply follows from the fact that regions of high density under the
steady-state distribution have a low free energy. This overall decrease in free energy is the essence
of the free-energy principle, which describes self-organisation at non-equilibrium steady-state
[23,28,29]. Note that the free energy, even after averaging internal variables, may decrease non-
monotonically. See the explanation in Figure 5.
11rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 5. Variational inference and predictive processing, averaging internal variables for any blanket state. This
ﬁgure illustrates a system’s behaviour after experiencing a surprising blanket state, averaging internal variables for any
blanket state. This is a multidimensional Ornstein-Uhlenbeck process, with two external, blanket and internal variables,
initialised at the steady-state density conditioned upon an improbable blanket statep(x0|b0). Upper left:we plot a sample
trajectory of the blanket states as these relax to steady-state over a contour plot of the free energy (up to a constant).
Upper right: this plots the free energy (up to a constant) over time, averaged over multiple trajectories. In this example,
the rare ﬂuctuations that climb the free energy landscape vanish on average, so that the average free energy decreases
monotonically. This need not always be the case: conservative systems (i.e., ς≡0 in (3.2)) are deterministic ﬂows
along the contours of the steady-state density (see Appendix B). Since these contours do not generally coincide with
those of F(b,µ) it follows that the free energy oscillates between its maximum and minimum value over the system’s
periodic trajectory. Luckily, conservative systems are not representative of dissipative, living systems. Y et, it follows that
the average free energy of expected internal variables may increase, albeit only momentarily, in dissipative systems(3.2)
whose solenoidal ﬂow dominates dissipative ﬂow.Lower left: we illustrate the accuracy of predictions over external states
of the sample path from the upper left panel. At steady-state (from timestep ∼100), the predictions become accurate.
The prediction of the second component is offset by four units for greater visibility, as can be seen from the longtime
behaviour converging to four instead of zero. Lower right:We show the evolution of precision-weighted prediction errors
ξt:= Πη(ηt−σ(µt)) over time. These are normally distributed with zero mean at steady-state.
4 Active inference and stochastic control
In order to model agents that interact with their environment, we now partition blanket states
into sensory and active states
bt= (st,at)
xt= (ηt,st,at,µt).
Intuitively, sensory states are the sensory receptors of the system (e.g., olfactory or visual
receptors) while active states correspond to actuators through which the system inﬂuences the
environment (e.g., muscles). See Figure 7. The goal of this section is to explain how autonomous
12rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 6. Variational inference and predictive processing . This ﬁgure illustrates a system’s behaviour after
experiencing a surprising blanket state. This is a multidimensional Ornstein-Uhlenbeck process, with one external, blanket
and internal variable, initialised at the steady-state density conditioned upon an improbable blanket statep(x0|b0). Upper
left: this plots a sample trajectory of particular states as these relax to steady-state over a contour plot of the free energy.
The white line shows the expected internal state given blanket states, at which point inference is exact. After starting
close to this line, the process is driven by solenoidal ﬂow to regions where inference is inaccurate. Y et, solenoidal ﬂow
makes the system converge faster to steady-state [57,58] at which point inference becomes accurate again. Upper right:
this plots the free energy (up to a constant) over time, averaged over multiple trajectories. Lower left: we illustrate the
accuracy of predictions over external states of the sample path from the upper left panel. These predictions are accurate
at steady-state (from timestep ∼100). Lower right:we illustrate the (precision weighted) prediction errors over time. In
orange we plot the prediction error corresponding to the sample path in the upper left panel; the other sample paths are
summarised as a heat map in blue.
states (i.e., active and internal states) respond adaptively to sensory perturbations in order to
maintain the steady-state, which we interpret as the agent’s preferences or goal. This allows us
to relate the dynamics of autonomous states to active inference and stochastic control, which are
well-known formulations of adaptive behaviour in theoretical biology and engineering.
(a) Active inference
We now proceed to characterise autonomous states, given sensory states, using the free energy.
Unpacking blanket states, the free energy (3.6) reads
F(s,a,µ ) = DKL[qµ(η)∥p(η|s,a)] −log p(µ|s,a) −log p(a|s) −log p(s).
Crucially, it follows that the expected autonomous states minimise free energy
F(st,at,µt) ≥F(st,at,µt),
at:= a(st) :=Ep(at|st)[at] =ΣasΣ−1
s st,
13rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 7. Markov blanket evolving in time comprising sensory and active states.We continue the intuitive example
from Figure 3 of the bacillus as representing a Markov blanket that persists over time. The only difference is that we
partition blanket states into sensory and active states. In this example, the sensory states can be seen as the bacillus’
membrane, while the active states correspond to the actin ﬁlaments of the cytoskeleton.
Figure 8. Active inference. This ﬁgure illustrates a system’s behaviour after experiencing a surprising sensory state,
averaging internal variables for any blanket state. We simulated an Ornstein-Uhlenbeck process with two external, one
sensory, one active and two internal variables, initialised at the steady-state density conditioned upon an improbable
sensory state p(x0|s0). Left: The white line shows the expected active state given sensory states: this is the action that
performs active inference and optimal stochastic control. As the process experiences a surprising sensory state, it initially
relaxes to steady-state in a winding manner due to the presence of solenoidal ﬂow. Even though solenoidal ﬂow drives the
actions away from the optimal action initially, it allows the process to converge faster to steady-state [57,58,73] where the
actions are again close to the optimal action from optimal control. Right: We plot the free energy of the expected internal
state, averaged over multiple trajectories. In this example, the average free energy does not decrease monotonically—see
Figure 5 for an explanation.
where at denotes the expected active states given sensory states, which is the mean of p(at|st).
This result forms the basis of active inference, a well-known framework to describe and generate
adaptive behaviour in neuroscience, machine learning and robotics [25,60,65–72]. See Figure 8.
(b) Multivariate control
Active inference is used in various domains to simulate control [65,69,71,72,74–77], thus, it is
natural that we can relate the dynamics of active states to well-known forms of stochastic control.
By computing the free energy explicitly (see Appendix C), we obtain that
14rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 9. Stochastic control. This ﬁgure plots a sample path of the system’s particular states after it experiences a
surprising sensory state. This is the same sample path as shown in Figure 8 (left panel), however, here the link with
stochastic control is easier to see. Indeed, it looks as if active states (in red) are actively compensating for sensory states
(in green): rises in the active state-space lead to plunges in the sensory state-space and vice-versa. Notice the initial
rise in active states to compensate for the perturbation in the sensory states. Active states follow a similar trajectory as
sensory states, with a slight delay, which can be interpreted as a reaction time [78]. In fact, the correspondence between
sensory and active states is a consequence of the solenoidal ﬂow–see Figure 8 (left panel). The damped oscillations
as the particular states approach their target value of 0 (in grey) is analogous to that found in basic implementations of
stochastic control, e.g., [79, Figure 4.9].
(at,µt) minimises (a,µ) ↦→
[
st,a,µ
]
K


st
a
µ

 (4.1)
K:= Σ−1
b:µ
where we denoted by K the concentration (i.e., precision) matrix of p(s,a,µ ). We may interpret
(a,µ) as controlling how far particular states [s,a,µ ] are from their target set-point of [0,0,0],
where the error is weighted by the precision matrix K. See Figure 9. (Note that we could choose
any other set-point by translating the frame of reference or equivalently choosing a Gaussian
steady-state centred away from zero). In other words, there is a cost associated to how far away
s,a,µ are from the origin and this cost is weighed by the precision matrix, which derives from the
stationary covariance of the steady-state. In summary, the expected internal and active states can
be seen as performing multivariate stochastic control, where the matrix Kencodes control gains.
From a biologist’s perspective, this corresponds to a simple instance of homeostatic regulation:
maintaining physiological variables within their preferred range.
(c) Stochastic control in an extended state-space
More sophisticated control methods, such as PID (Proportional-Integral-Derivative) control
[77,80], involve controlling a process and its higher orders of motion (e.g., integral or derivative
terms). So how can we relate the dynamics of autonomous states to these more sophisticated
control methods? The basic idea involves extending the sensory state-space to replace the sensory
process st by its various orders of motion ˜st=
(
s(0)
t ,...,s (n)
t
)
(integral, position, velocity, jerk
etc, up to order n). To ﬁnd these orders of motion, one must solve the stochastic realisation
problem.
15rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
i The stochastic realisation problem
Recall that the sensory process st is a stationary stochastic process (with a Gaussian steady-
state). The following is a central problem in stochastic systems theory: Given a stationary
stochastic process st, ﬁnd a Markov process ˜st, called the state process, and a function f such
that
st= f(˜st) for all t. (4.2)
Moreover, ﬁnd an Itô stochastic differential equation whose unique solution is the state process
˜st. The problem of characterising the family of all such representations is known as the stochastic
realisation problem [81].
What kind of processes st can be expressed as a function of a Markov process (4.2)?
There is a rather comprehensive theory of stochastic realisation for the case where st is
a Gaussian process (which occurs, for example, when xt is a Gaussian process). This theory
expresses st as a linear map of an Ornstein-Uhlenbeck process [39,82,83]. The idea is as follows:
as a mean-zero Gaussian process, st is completely determined by its autocovariance function
C(t−r) =E[st⊗sr], which by stationarity only depends on |t−r|. It is well known that any
mean-zero stationary Gaussian process with exponentially decaying autocovariance function is
an Ornstein-Uhlenbeck process (a result sometimes known as Doob’s theorem) [39,84–86]. Thus if
C equals a ﬁnite sum of exponentially decaying functions, we can express st as a linear function
of several nested Ornstein-Uhlenbeck processes, i.e., as an integrator chain from control theory
[87,88]
st= f(s(0)
t )
ds(0)
t = f0(s(0)
t ,s(1)
t )dt+ ς0dW(0)
t
ds(1)
t = f1(s(1)
t ,s(2)
t )dt+ ς1dW(1)
t
...
...
...
ds(n−1)
t = fn−1(s(n−1)
t ,s(n)
t )dt+ ςn−1dW(n−1)
t
ds(n)
t = fn(s(n)
t )dt+ ςndW(n)
t .
(4.3)
In this example,f,fiare suitably chosen linear functions,ςiare matrices andW(i) are standard
Brownian motions. Thus, we can seestas the output of a continuous-time hidden Markov model,
whose (hidden) states s(i)
t encode its various orders of motion: position, velocity, jerk etc. These
are known as generalised coordinates of motion in the Bayesian ﬁltering literature [89–91]. See
Figure 10.
More generally, the state process ˜st and the function f need not be linear, which enables to
realise non-linear, non-Gaussian processes st [89,92,93]. Technically, this follows as Ornstein-
Uhlenbeck processes are the only stationary Gaussian Markov processes. Note that stochastic
realisation theory is not as well developed in this general case [81,89,93–95].
ii Stochastic control of integrator chains
Henceforth, we assume that we can express st as a function of a Markov process ˜st (4.2).
Inserting (4.2) into (4.1), we now see that the expected autonomous states minimise how far
themselves and f(˜st) are from their target value of zero
(at,µt) minimises (a,µ) ↦→
[
f(˜st),a,µ
]
K


f(˜st)
a
µ

. (4.4)
16rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 10. Continuous-time Hidden Markov model. This ﬁgure depicts (4.3) in a graphical format, as a Bayesian
network [3,31]. The encircled variables are random variables—the processes indexed at an arbitrary sequence
of subsequent times t1 <t2 <...<t 9. The arrows represent relationships of causality. In this hidden Markov
model, the (hidden) state process ˜st is given by an integrator chain—i.e., nested stochastic differential equations
s(0)
t ,s(1)
t ,...,s (n)
t . These processes s(i)
t ,i ≥0, can respectively be seen as encoding the position, velocity, jerk etc,
of the process st.
Furthermore, if the state process ˜st can be expressed as an integrator chain, as in (4.3), then
we can interpret expected active and internal states as controlling each order of motion s(i)
t . For
example, if f is linear, these processes control each order of motion s(i)
t towards its target value
of zero.
iii PID-like control
Proportional-integral-derivative (PID) control is a well-known control method in engineering
[77,80]. More than 90% of controllers in engineered systems implement either PID or PI (no
derivative) control. The goal of PID control is to control a signal s(1)
t , its integral s(0)
t , and its
derivative s(2)
t close to a pre-speciﬁed target value [77].
This turns out to be exactly what happens here when we consider the stochastic control of an
integrator chain (4.4) with three orders of motion(n= 2). When f is linear, expected autonomous
states control integral, proportional and derivative processes s(0)
t ,s(1)
t ,s(2)
t towards their target
value of zero. Furthermore, from f and K one can derive integral, proportional and derivative
gains, which penalise deviations of s(0)
t ,s(1)
t ,s(2)
t , respectively, from their target value of zero.
Crucially, these control gains are simple by-products of the steady-state density and the stochastic
realisation problem.
Why restrict ourselves to PID control when stochastic control of integrator chains is available?
It turns out that when sensory states st are expressed as a function of an integrator chain (4.3),
one may get away by controlling an approximation of the true (sensory) process, obtained by
truncating high orders of motion as these have less effect on the dynamics, though knowing when
this is warranted is a problem in approximation theory. This may explain why integral feedback
control (n= 0), PI control (n= 1) and PID control (n= 2) are the most ubiquitous control methods
in engineering applications. However, when simulating biological control—usually with highly
non-linear dynamics—it is not uncommon to consider generalised motion to fourth ( n= 4) or
sixth (n= 6) order [92,96].
It is worth mentioning that PID control has been shown to be implemented in simple
molecular systems and is becoming a popular mechanistic explanation of behaviours such as
bacterial chemotaxis and robust homeostatic algorithms in biochemical networks [77,97,98]. We
17rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
suggest that this kind of behaviour emerges in Markov blankets at non-equilibrium steady-
state. Indeed, stationarity means that autonomous states will look as if they respond adaptively
to external perturbations to preserve the steady-state, and we can identify these dynamics as
implementations of various forms of stochastic control (including PID-like control).
5 Discussion
In this paper, we considered the consequences of a boundary mediating interactions between
states internal and external to a system. On unpacking this notion, we found that the states
internal to a Markov blanket look as if they perform variational Bayesian inference, optimising
beliefs about their external counterparts. When subdividing the blanket into sensory and active
states, we found that autonomous states perform active inference and various forms of stochastic
control (i.e., generalisations of PID control).
Interacting Markov blankets: The sort of inference we have described could be nuanced by
partitioning the external state-space into several systems that are themselves Markov blankets
(such as Markov blankets nested at several different scales [1]). From the perspective of internal
states, this leads to a more interesting inference problem, with a more complex generative model.
It may be that the distinction between the sorts of systems we generally think of as engaging in
cognitive, inferential, dynamics [99] and simpler systems rest upon the level of structure of the
generative models (i.e., steady-state densities) that describe their inferential dynamics.
Temporally deep inference: This distinction may speak to a straightforward extension of the
treatment on offer, from simply inferring an external state to inferring the trajectories of external
states. This may be achieved by representing the external process in terms of its higher orders
of motion by solving the stochastic realisation problem. By repeating the analysis above, internal
states may be seen as inferring the position, velocity, jerk, etc of the external process, consistently
with temporally deep inference in the sense of a Bayesian ﬁlter [91] (a special case of which is an
extended Kalman–Bucy ﬁlter [100]).
Bayesian mechanics in non-Gaussian steady-states: The treatment from this paper extends
easily to non-Gaussian steady-states, in which internal states appear to perform approximate
Bayesian inference over external states. Indeed, any arbitrary (smooth) steady-state density
may be approximated by a Gaussian density at one of its modes using a so-called Laplace
approximation. This Gaussian density affords one with a synchronisation map in closed form 4
that maps the expected internal state to an approximation of the expected external state. It
follows that the system can be seen as performing approximate Bayesian inference over external
states—precisely, an inferential scheme known as variational Laplace [101]. We refer the interested
reader to a worked-out example involving two sparsely coupled Lorenz systems [30]. Note that
variational Laplace has been proposed as an implementation of various cognitive processes in
biological systems [25,52,60] accounting for several features of the brain’s functional anatomy
and neural message passing [53,70,99,102,103].
Modelling real systems: The simulations presented here are as simple as possible and are
intended to illustrate general principles that apply to all stationary processes with a Markov
blanket (3.1). These principles have been used to account for synthetic data arising in more
reﬁned (and more speciﬁc) simulations of an interacting particle system [27] and synchronisation
between two sparsely coupled stochastic Lorenz systems [30]. Clearly, an outstanding challenge
is to account for empirical data arising from more interesting and complex structures. To do this,
one would have to collect time-series from an organism’s internal states (e.g., neural activity), its
surrounding external states, and its interface, including sensory receptors and actuators. Then,
one could test for conditional independence between internal, external and blanket states (3.1)
[104]. One might then test for the existence of a synchronisation map (using Lemma 2.1). This
speaks to modelling systemic dynamics using stochastic processes with a Markov blanket. For
example, one could learn the volatility, solenoidal ﬂow and steady-state density in a stochastic
differential equation (3.2) from data, using supervised learning [105].
4Another option is to empirically ﬁt a synchronisation map to data [27].
18rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6 Conclusion
This paper outlines some of the key relationships between stationary processes, inference and
control. These relationships rest upon partitioning the world into those things that are internal or
external to a (statistical) boundary, known as a Markov blanket. When equipped with dynamics,
the expected internal states appear to engage in variational inference, while the expected active
states appear to be performing active inference and various forms of stochastic control.
The rationale behind these ﬁndings is rather simple: if a Markov blanket derives from a steady-
state density, the states of the system will look as if they are responding adaptively to external
perturbations in order to recover the steady-state. Conversely, well-known methods used to build
adaptive systems implement the same kind of dynamics, implicitly so that the system maintains
a steady-state with its environment.
Data Accessibility. All data and numerical simulations can be reproduced with code freely available at
https://github.com/conorheins/bayesian-mechanics-sdes.
Authors’ Contributions. Conceptualization: LD, KF, CH, GAP; Formal analysis: LD, KF, GAP; Software:
LD, CH; Supervision: KF, GAP; Writing – original draft: LD; Writing – review & editing: KF, CH, GAP . All
authors gave ﬁnal approval for publication and agree to be held accountable for the work performed therein.
Competing Interests. We have no competing interests.
Funding. LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875).
This publication is based on work partially supported by the EPSRC Centre for Doctoral Training in
Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1). KF was a Wellcome
Principal Research Fellow (Ref: 088130/Z/09/Z). CH is supported by the U.S. Ofﬁce of Naval Research
(N00014-19-1-2556). The work of GAP was partially funded by the EPSRC, grant number EP/P031587/1,
and by JPMorgan Chase & Co. Any views or opinions expressed herein are solely those of the authors listed,
and may differ from the views and opinions expressed by JPMorgan Chase & Co. or its afﬁliates. This material
is not a product of the Research Department of J.P . Morgan Securities LLC. This material does not constitute
a solicitation or offer in any jurisdiction.
Acknowledgements. LD would like to thank Kai Ueltzhöffer, Toby St Clere Smithe and Thomas Parr for
interesting discussions. We are grateful to our two anonymous reviewers for feedback which substantially
improved the manuscript.
A Existence of synchronisation map: proof
We prove Lemma 2.1.
Proof. (i) ⇐⇒(ii) follows by deﬁnition of a function.
(ii) ⇐⇒(iii) is as follows
∀b1,b2 ∈B: µ(b1) =µ(b2) ⇒η(b1) =η(b2)
⇐⇒
(
∀b1,b2 ∈B: ΣµbΣ−1
b b1 = ΣµbΣ−1
b b2 ⇒ΣηbΣ−1
b b1 = ΣηbΣ−1
b b2
)
⇐⇒
(
∀b∈B: ΣµbΣ−1
b b= 0⇒ΣηbΣ−1
b b= 0
)
⇐⇒ker Σµb⊂ker Σηb
(iii) ⇐⇒(iv) From [106, Section 0.7.3], using the Markov blanket condition (2.2), we can
verify that
ΠµΣµb= −ΠµbΣb
ΠηΣηb= −ΠηbΣb.
19rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Since Πµ,Πη,Σb are invertible, we deduce
ker Σµb⊂ker Σηb
⇐⇒ker ΠµΣµb⊂ker ΠηΣηb
⇐⇒ker −ΠµbΣb⊂ker −ΠηbΣb
⇐⇒ker Πµb⊂ker Πηb.
B The Helmholtz decomposition
We consider a diffusion process xt on Rd satisfying an Itô stochastic differential equation (SDE)
[39,107,108],
dxt= f(xt) dt+ ς(xt) dWt, (A 1)
where Wt is an m-dimensional standard Brownian motion (a.k.a., Wiener process) [38,39] and
f: Rd→Rd,ς : Rd→Rd×m are smooth functions satisfying for all x,y ∈Rd:
• Bounded, linear growth condition: |f(x)|+ |ς(x)|≤K(1 +|x|),
• Lipschitz condition: |f(x) −f(y)|+ |ς(x) −ς(y)|≤K|x−y|,
for some constant Kand |ς|= ∑
ij
⏐⏐ςij
⏐⏐. These are standard regularity conditions that ensure the
existence and uniqueness of a solution to the SDE (A 1) [108, Theorem 5.2.1].
We now recall an important result from the theory of stationary diffusion processes, known
as the Helmholtz decomposition. It consists of splitting the dynamic into time-reversible (i.e.,
dissipative) and time-irreversible (i.e., conservative) components. The importance of this result
in non-equilibrium thermodynamics was originally recognised by Graham in 1977 [109] and has
been of great interest in the ﬁeld since [39,110–112]. Furthermore, the Helmholtz decomposition
is widely used in statistical machine learning to generate Monte-Carlo sampling schemes [39,73,
113–116].
Lemma B.1 (Helmholtz decomposition). For a diffusion process(A 1) and a smooth probability density
p> 0, the following are equivalent:
(i) pis a steady-state for xt.
(ii) We can write the drift as
f= frev + firrev
frev := Γ∇log p+ ∇·Γ
firrev := Q∇log p+ ∇·Q.
(A 2)
where Γ = ςς⊤/2 is the diffusion tensor and Q= −Q⊤is a smooth antisymmetric matrix ﬁeld.
∇·denotes the divergence of a matrix ﬁeld deﬁned as (∇·Q)i:= ∑
j
∂
∂xj
Qij.
Furthermore, frev is invariant under time-reversal, while firrev changes sign under time-reversal.
In the Helmholtz decomposition of the drift (A 2), the diffusion tensor Γ mediates the
dissipative ﬂow, which ﬂows towards the modes of the steady-state density, but is counteracted
by random ﬂuctuations Wt, so that the system’s distribution remains unchanged—together these
form the time-reversible part of the dynamics. In contrast,Qmediates the solenoidal ﬂow—whose
direction is reversed under time-reversal—which consists of conservative (i.e., Hamiltonian)
dynamics that ﬂow on the level sets of the steady-state. See Figure 11 for an illustration. Note
that the terms time-reversible and time-irreversible are meant in a probabilistic sense, in the
20rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
sense that time-reversibility denotes invariance under time-reversal. This is opposite to reversible
and irreversible in a classical physics sense, which respectively mean energy preserving (i.e.,
conservative) and entropy creating (i.e., dissipative).
Figure 11. Helmholtz decomposition.The upper left panel shows a sample trajectory of a two-dimensional diffusion
process (A 1) on a heat map of the (Gaussian) steady-state density. The upper right panel illustrates the Helmholtz
decomposition of the drift into time-reversible and time-irreversible parts: the time-reversible part of the drift ﬂows towards
the peak of the steady-state density, while the time-irreversible part ﬂows along the contours of the probability distribution.
The lower panels plot sample paths of the time-reversible (lower left) and time-irreversible (lower right) parts of the
dynamics. Purely conservative dynamics (lower right panel) are reminiscent of the trajectories of massive bodies (e.g.,
planets) whose random ﬂuctuations are negligible, as in Newtonian mechanics. The lower panels help illustrate the
meaning of time-irreversibility: If we were to reverse time (c.f., (A 3)), the trajectories the time-reversible process would
be, on average, no different, while the trajectories of the time-irreversible process would ﬂow, say, clockwise instead of
counterclockwise, which would clearly be distinguishable. Here, the full process (upper left panel) is a combination of both
dynamics. As we can see the time-reversible part affords the stochasticity while the time-irreversible part characterises
non-equilibria and the accompanying wandering behaviour that characterises life-like systems [11,117].
Proof. "⇒" It is well-known that when xt is stationary at p, its time-reversal is also a diffusion
process that solves the following Itô SDE [118]
dx−
t = f−(x−
t )dt+ ς(x−
t )dWt
f−:= −b+ p−1∇·(2Γp) .
(A 3)
This enables us to write the driftfas a sum of two terms: one that is invariant under time
reversal, another that changes sign under time-reversal
f= f + f−
2 + f −f−
2 =: frev + firrev.
21rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
It is straightforward to identify the time-reversible term
frev = p−1∇·(Γp) =p−1Γ∇p+ p−1p∇·Γ = Γ∇log p+ ∇·Γ. (A 4)
For the remaining term, we ﬁrst note that the steady-state psolves the stationary Fokker-
Planck equation [39,119]
∇·(−fp + ∇·(Γp)) = 0.
Decomposing the drift into time-reversible and time-irreversible parts, we obtain that the
time-irreversible part produces divergence-free (i.e., conservative) ﬂow w.r.t. the steady-
state distribution
∇·(firrevp) = 0.
Now recall that any (smooth) divergence-free vector ﬁeld is the divergence of a (smooth)
antisymmetric matrix ﬁeld A= −AT [109,110,120]
firrevp= ∇·A.
We deﬁne a new antisymmetric matrix ﬁeld Q:= p−1A. It follows from the product rule
for divergences that we can rewrite the time-irreversible drift as required
firrev = Q∇log p+ ∇·Q.
"⇐" From (A 4) we can rewrite the time-reversible part of the drift as
frev = p−1∇·(Γp). (A 5)
In addition, we deﬁne the auxiliary antisymmetric matrix ﬁeld A:= pQ and use the
product rule for divergences to simplify the expression of the time-irreversible part
firrev = p−1∇·A.
Note that
∇·(firrevp) = 0
as the matrix ﬁeld Ais smooth and antisymmetric. It follows that the distributionpsolves
the stationary Fokker-Planck equation
∇·(−fp + ∇·(Γp)) =∇·(−frevp−firrevp+ ∇·(Γp)) =∇·(−firrevp) = 0.
C Free energy computations
The free energy reads (3.6)
F(b,µ) = DKL[qµ(η)∥p(η|b)] −log p(b,µ).
Recalling from (2.3), (3.3) that qµ(η) and p(η|b) are Gaussian, the KL divergence between
multivariate Gaussians is well-known
qµ(η) =N(η; σ(µ),Π−1
η ), p (η|b) =N(η; η(b),Π−1
η ),
⇒DKL[qµ(η)∥p(η|b)] =1
2(σ(µ) −η(b))Πη(σ(µ) −η(b)).
Furthermore, we can compute the log partition
−log p(b,µ) =1
2
[
b,µ
]
Σ−1
b:µ
[
b
µ
]
(up to a constant).
22rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Note that Σ−1
b:µ is the inverse of a principal submatrix of Σ, which in general differs from Πb:µ, a
principal submatrix of Π. Finally,
F(b,µ) =1
2(σ(µ) −η(b))Πη(σ(µ) −η(b)) +1
2
[
b,µ
]
Σ−1
b:µ
[
b
µ
]
(up to a constant).
References
1. Casper Hesp, Maxwell Ramstead, Axel Constant, Paul Badcock, Michael Kirchhoff, and Karl
Friston.
A Multi-scale View of the Emergent Complexity of Life: A Free-Energy Proposal.
In Georgi Yordanov Georgiev, John M. Smart, Claudio L. Flores Martinez, and Michael E.
Price, editors, Evolution, Development and Complexity , Springer Proceedings in Complexity,
pages 195–227, Cham, 2019. Springer International Publishing.
2. Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein.
The Markov blankets of life: Autonomy, active inference and the free energy principle.
Journal of The Royal Society Interface, 15(138):20170792, January 2018.
3. Judea Pearl.
Graphical Models for Probabilistic and Causal Reasoning.
In Philippe Smets, editor, Quantiﬁed Representation of Uncertainty and Imprecision , Handbook
of Defeasible Reasoning and Uncertainty Management Systems, pages 367–389. Springer
Netherlands, Dordrecht, 1998.
4. Christopher M. Bishop.
Pattern Recognition and Machine Learning.
Information Science and Statistics. Springer, New York, 2006.
5. G. Nicolis and I. Prigogine.
Self-Organization in Nonequilibrium Systems: From Dissipative Structures to Order Through
Fluctuations.
Wiley-Blackwell, New York, June 1977.
6. Albert Goldbeter.
Dissipative structures in biological systems: Bistability, oscillations, spatial patterns and
waves.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
376(2124):20170376, July 2018.
7. Hermann Haken.
Synergetics: An Introduction Nonequilibrium Phase Transitions and Self-Organization in Physics,
Chemistry and Biology.
Springer Series in Synergetics. Springer-Verlag, Berlin Heidelberg, second edition, 1978.
8. Nikolay Perunov, Robert A. Marsland, and Jeremy L. England.
Statistical Physics of Adaptation.
Physical Review X, 6(2):021036, June 2016.
9. Kate Jeffery, Robert Pollack, and Carlo Rovelli.
On the statistical mechanics of life: Schr\"odinger revisited.
arXiv:1908.08374 [physics], August 2019.
10. Jeremy L. England.
Statistical physics of self-replication.
The Journal of Chemical Physics, 139(12):121923, August 2013.
11. Dominic J. Skinner and Jörn Dunkel.
Improved bounds on entropy production in living systems.
Proceedings of the National Academy of Sciences, 118(18), May 2021.
12. Benjamin Dunn and Yasser Roudi.
Learning and inference in a nonequilibrium Ising model with hidden nodes.
Physical Review E, 87(2):022127, February 2013.
13. Susanne Still.
Thermodynamic Cost and Beneﬁt of Memory.
Physical Review Letters, 124(5):050601, February 2020.
14. Susanne Still, David A. Sivak, Anthony J. Bell, and Gavin E. Crooks.
23rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Thermodynamics of Prediction.
Physical Review Letters, 109(12):120604, September 2012.
15. Kai Ueltzhöffer.
On the thermodynamics of prediction under dissipative adaptation.
arXiv:2009.04006 [cond-mat, q-bio], September 2020.
16. Gülce Karde¸ s and David H. Wolpert.
Thermodynamic Uncertainty Relations for Multipartite Processes.
arXiv:2101.01610 [cond-mat], March 2021.
17. David H. Wolpert.
Minimal entropy production rate of interacting systems.
New Journal of Physics, 22(11):113013, November 2020.
18. David H. Wolpert.
Uncertainty Relations and Fluctuation Theorems for Bayes Nets.
Physical Review Letters, 125(20):200602, November 2020.
19. Gavin E. Crooks and Susanne Still.
Marginal and conditional second laws of thermodynamics.
EPL (Europhysics Letters), 125(4):40005, March 2019.
20. Jordan M. Horowitz and Massimiliano Esposito.
Thermodynamics with Continuous Information Flow.
Physical Review X, 4(3):031015, July 2014.
21. Alexandre Pouget, Peter Dayan, and Richard S. Zemel.
Inference and computation with population codes.
Annual Review of Neuroscience, 26(1):381–410, March 2003.
22. David C. Knill and Alexandre Pouget.
The Bayesian brain: The role of uncertainty in neural coding and computation.
Trends in Neurosciences, 27(12):712–719, December 2004.
23. Karl Friston.
The free-energy principle: A uniﬁed brain theory?
Nature Reviews Neuroscience, 11(2):127–138, February 2010.
24. Rajesh P . N. Rao and Dana H. Ballard.
Predictive coding in the visual cortex: A functional interpretation of some extra-classical
receptive-ﬁeld effects.
Nature Neuroscience, 2(1):79–87, January 1999.
25. Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel.
Action and behavior: A free-energy formulation.
Biological Cybernetics, 102(3):227–260, March 2010.
26. Karl J. Friston, Erik D. Fagerholm, Tahereh S. Zarghami, Thomas Parr, Inês Hipólito, Loïc
Magrou, and Adeel Razi.
Parcels and particles: Markov blankets in the brain.
arXiv:2007.09704 [q-bio], July 2020.
27. Karl Friston.
Life as we know it.
Journal of The Royal Society Interface, 10(86):20130475, September 2013.
28. Thomas Parr, Lancelot Da Costa, and Karl Friston.
Markov blankets, information geometry and stochastic thermodynamics.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
378(2164):20190159, February 2020.
29. Karl Friston.
A free energy principle for a particular physics.
arXiv:1906.10184 [q-bio], June 2019.
30. Karl Friston, Conor Heins, Kai Ueltzhöffer, Lancelot Da Costa, and Thomas Parr.
Stochastic Chaos and Markov Blankets.
Entropy, 23(9):1220, September 2021.
31. Martin J. Wainwright and Michael I. Jordan.
Graphical Models, Exponential Families, and Variational Inference.
Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2007.
32. Morris L. Eaton.
Multivariate Statistics: A Vector Space Approach.
Institute of Mathematical Statistics, 2007.
24rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33. Thomas Parr.
The Computational Neurology of Active Vision.
Ph.D. Thesis, University College London, London, 2019.
34. Markus Meister and Michael J. Berry.
The Neural Code of the Retina.
Neuron, 22(3):435–450, March 1999.
35. M. James.
The generalised inverse.
The Mathematical Gazette, 62(420):109–114, June 1978.
36. Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L. Buckley.
How particular is the physics of the Free Energy Principle?
arXiv:2105.11203 [q-bio], May 2021.
37. Jonathan C. Mattingly, Andrew M. Stuart, and M. V . Tretyakov.
Convergence of Numerical Time-Averaging and Stationary Measures via Poisson Equations.
SIAM Journal on Numerical Analysis, 48(2):552–577, January 2010.
38. L. C. G. Rogers and David Williams.
Diffusions, Markov Processes, and Martingales: Volume 1: Foundations , volume 1 of Cambridge
Mathematical Library.
Cambridge University Press, Cambridge, second edition, 2000.
39. Grigorios A. Pavliotis.
Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin
Equations.
Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014.
40. Joris Bierkens, Paul Fearnhead, and Gareth Roberts.
The Zig-Zag process and super-efﬁcient sampling for Bayesian analysis of big data.
The Annals of Statistics, 47(3):1288–1320, June 2019.
41. Joris Bierkens and Gareth Roberts.
A piecewise deterministic scaling limit of lifted Metropolis–Hastings in the Curie–Weiss
model.
The Annals of Applied Probability, 27(2):846–882, April 2017.
42. Alexandre Bouchard-Côté, Sebastian J. Vollmer, and Arnaud Doucet.
The Bouncy Particle Sampler: A Nonreversible Rejection-Free Markov Chain Monte Carlo
Method.
Journal of the American Statistical Association, 113(522):855–867, April 2018.
43. Carl Edward Rasmussen.
Gaussian Processes in Machine Learning.
In Olivier Bousquet, Ulrike von Luxburg, and Gunnar Rätsch, editors, Advanced Lectures on
Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, Tübingen,
Germany, August 4 - 16, 2003, Revised Lectures , Lecture Notes in Computer Science, pages
63–71. Springer, Berlin, Heidelberg, 2004.
44. Ludwig Arnold.
Random Dynamical Systems.
Springer Monographs in Mathematics. Springer-Verlag, Berlin Heidelberg, 1998.
45. Martin Biehl, Felix A. Pollock, and Ryota Kanai.
A Technical Critique of Some Parts of the Free Energy Principle.
Entropy, 23(3):293, March 2021.
46. Karl J. Friston, Lancelot Da Costa, and Thomas Parr.
Some Interesting Observations on the Free Energy Principle.
Entropy, 23(8):1076, August 2021.
47. Haider Hasan Jafri, R. K. Brojen Singh, and Ramakrishna Ramaswamy.
Generalized synchrony of coupled stochastic processes with multiplicative noise.
Physical Review E, 94(5):052216, November 2016.
48. D. Cumin and C. P . Unsworth.
Generalising the Kuramoto model for the study of neuronal synchronisation in the brain.
Physica D: Nonlinear Phenomena, 226(2):181–196, February 2007.
49. Ensor Rafael Palacios, Takuya Isomura, Thomas Parr, and Karl Friston.
The emergence of synchrony in networks of mutually inferring neurons.
Scientiﬁc Reports, 9(1):6412, April 2019.
25rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50. Stephen D. Bartlett, Terry Rudolph, and Robert W. Spekkens.
Reconstruction of Gaussian quantum mechanics from Liouville mechanics with an epistemic
restriction.
Physical Review A, 86(1):012103, July 2012.
51. S. Kullback and R. A. Leibler.
On Information and Sufﬁciency.
The Annals of Mathematical Statistics, 22(1):79–86, March 1951.
52. Rafal Bogacz.
A tutorial on the free-energy framework for modelling perception and learning.
Journal of Mathematical Psychology, 76:198–211, February 2017.
53. Karl Friston and Stefan Kiebel.
Predictive coding under the free-energy principle.
Philosophical Transactions of the Royal Society B: Biological Sciences , 364(1521):1211–1221, May
2009.
54. Zenas C. Chao, Kana Takaura, Liping Wang, Naotaka Fujii, and Stanislas Dehaene.
Large-Scale Cortical Networks for Hierarchical Prediction and Prediction Error in the
Primate Brain.
Neuron, 100(5):1252–1266.e3, May 2018.
55. Sandra Iglesias, Christoph Mathys, Kay H. Brodersen, Lars Kasper, Marco Piccirelli,
Hanneke E. M. den Ouden, and Klaas E. Stephan.
Hierarchical Prediction Errors in Midbrain and Basal Forebrain during Sensory Learning.
Neuron, 80(2):519–530, October 2013.
56. Nathaniel D. Daw, Samuel J. Gershman, Ben Seymour, Peter Dayan, and Raymond J. Dolan.
Model-Based Inﬂuences on Humans’ Choices and Striatal Prediction Errors.
Neuron, 69(6):1204–1215, March 2011.
57. Michela Ottobre.
Markov Chain Monte Carlo and Irreversibility.
Reports on Mathematical Physics, 77:267–292, June 2016.
58. Luc Rey-Bellet and Kostantinos Spiliopoulos.
Irreversible Langevin samplers and variance reduction: A large deviation approach.
Nonlinearity, 28(7):2081–2103, July 2015.
59. David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.
Variational Inference: A Review for Statisticians.
Journal of the American Statistical Association, 112(518):859–877, April 2017.
60. Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth.
The free energy principle for action and perception: A mathematical review.
Journal of Mathematical Psychology, 81:55–79, December 2017.
61. Matthew James Beal.
Variational Algorithms for Approximate Bayesian Inference.
Ph.D. Thesis, University of London, 2003.
62. Odelia Schwartz, Jonathan W. Pillow, Nicole C. Rust, and Eero P . Simoncelli.
Spike-triggered neural characterization.
Journal of Vision, 6(4):484–507, July 2006.
63. R. J. Sayer, M. J. Friedlander, and S. J. Redman.
The time course and amplitude of EPSPs evoked at synapses between pairs of CA3/CA1
neurons in the hippocampal slice.
The Journal of Neuroscience: The Ofﬁcial Journal of the Society for Neuroscience , 10(3):826–836,
March 1990.
64. Steven J. Luck.
An Introduction to the Event-Related Potential Technique.
A Bradford Book, Cambridge, MA, USA, second edition, May 2014.
65. Kai Ueltzhöffer.
Deep Active Inference.
Biological Cybernetics, 112(6):547–573, December 2018.
66. Beren Millidge.
Deep active inference as variational policy gradients.
Journal of Mathematical Psychology, 96:102348, June 2020.
67. R. Conor Heins, M. Berk Mirza, Thomas Parr, Karl Friston, Igor Kagan, and Arezoo
Pooresmaeili.
26rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Deep Active Inference and Scene Construction.
Frontiers in Artiﬁcial Intelligence, 3:81, 2020.
68. Pablo Lanillos, Jordi Pages, and Gordon Cheng.
Robot self/other distinction: Active inference meets neural networks learning in a mirror.
In European Conference on Artiﬁcial Intelligence. IOS press, April 2020.
69. Tim Verbelen, Pablo Lanillos, Christopher Buckley, and Cedric De Boom, editors.
Active Inference: First International Workshop, IWAI 2020, Co-Located with ECML/PKDD 2020,
Ghent, Belgium, September 14, 2020, Proceedings.
Communications in Computer and Information Science. Springer International Publishing,
2020.
70. Rick A. Adams, Stewart Shipp, and Karl J. Friston.
Predictions not commands: Active inference in the motor system.
Brain Structure & Function, 218(3):611–643, May 2013.
71. Corrado Pezzato, Riccardo Ferrari, and Carlos Hernández Corbato.
A Novel Adaptive Controller for Robot Manipulators Based on Active Inference.
IEEE Robotics and Automation Letters, 5(2):2973–2980, April 2020.
72. Guillermo Oliver, Pablo Lanillos, and Gordon Cheng.
An empirical study of active inference on a humanoid robot.
IEEE Transactions on Cognitive and Developmental Systems, pages 1–1, 2021.
73. Tony Lelièvre, Francis Nier, and Grigorios A. Pavliotis.
Optimal non-reversible linear drift for the convergence to equilibrium of a diffusion.
Journal of Statistical Physics, 152(2):237–274, July 2013.
74. Magnus T. Koudahl and Bert de Vries.
A Worked Example of Fokker-Planck-Based Active Inference.
In Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom, editors,Active
Inference, Communications in Computer and Information Science, pages 28–34, Cham, 2020.
Springer International Publishing.
75. Karl Friston.
What Is Optimal about Motor Control?
Neuron, 72(3):488–498, November 2011.
76. Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos.
End-to-End Pixel-Based Deep Active Inference for Body Perception and Action.
arXiv:2001.05847 [cs, q-bio], May 2020.
77. Manuel Baltieri and Christopher L. Buckley.
PID Control as a Process of Active Inference with Linear Generative Models.
Entropy, 21(3):257, March 2019.
78. Robert J Kosinski.
A literature review on reaction time.
Clemson University, 10(1), 2008.
79. Tony Roskilly and Dr Rikard Mikalsen.
Marine Systems Identiﬁcation, Modeling and Control.
Butterworth-Heinemann, Amsterdam ; Boston, illustrated edition edition, March 2015.
80. Karl Johan Åström.
Pid Controllers.
International Society for Measurement and Control, January 1995.
81. Sanjoy Mitter, Giorgio Picci, and Anders Lindquist.
Toward a theory of nonlinear stochastic realization.
In Feedback and Synthesis of Linear and Nonlinear Systems, 1981.
82. Anders Lindquist and Giorgio Picci.
Linear Stochastic Systems: A Geometric Approach to Modeling, Estimation and Identiﬁcation.
Series in Contemporary Mathematics. Springer-Verlag, Berlin Heidelberg, 2015.
83. Anders Lindquist and Giorgio Picci.
Realization Theory for Multivariate Stationary Gaussian Processes.
SIAM Journal on Control and Optimization, 23(6):809–857, November 1985.
84. J. L. Doob.
The Brownian Movement and Stochastic Equations.
Annals of Mathematics, 43(2):351–369, 1942.
85. Ming Chen Wang and G. E. Uhlenbeck.
27rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
On the Theory of the Brownian Motion II.
In Selected Papers on Noise and Stochastic Processes. Dover, 2014.
86. Luc Rey-Bellet.
Open Classical Systems.
In Stéphane Attal, Alain Joye, and Claude-Alain Pillet, editors, Open Quantum Systems
II: The Markovian Approach , Lecture Notes in Mathematics, pages 41–78. Springer, Berlin,
Heidelberg, 2006.
87. Mikhail Kryachkov, Andrey Polyakov, and Vadim Strygin.
Finite-time stabilization of an integrator chain using only signs of the state variables.
In 2010 11th International Workshop on Variable Structure Systems (VSS) , pages 510–515, June
2010.
88. Konstantin Zimenko, Andrey Polyakov, Denis Eﬁmo, and Wilfrid Perruquetti.
Finite-time and ﬁxed-time stabilization for integrator chain of arbitrary order*.
In 2018 European Control Conference (ECC), pages 1631–1635, June 2018.
89. K. J. Friston.
Variational ﬁltering.
NeuroImage, 41(3):747–766, July 2008.
90. K. J. Friston, N. Trujillo-Barreto, and J. Daunizeau.
DEM: A variational treatment of dynamic systems.
NeuroImage, 41(3):849–885, July 2008.
91. Karl Friston, Klaas Stephan, Baojuan Li, and Jean Daunizeau.
Generalised Filtering.
Mathematical Problems in Engineering, 2010:1–34, 2010.
92. Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston.
The computational neurology of movement under active inference.
Brain, (awab085), March 2021.
93. S. N. Gomes, G. A. Pavliotis, and U. Vaes.
Mean Field Limits for Interacting Diffusions with Colored Noise: Phase Transitions and
Spectral Numerical Methods.
Multiscale Modeling & Simulation, 18(3):1343–1370, January 2020.
94. T. J. S. Tayor and M. Pavon.
On the nonlinear stochastic realization problem.
Stochastics and Stochastic Reports, 26(2):65–79, February 1989.
95. A. E. Frazho.
On stochastic realization theory.
Stochastics, 7(1-2):1–27, January 1982.
96. Karl J. Friston, Thomas Parr, and Bert de Vries.
The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4):381–414, December 2017.
97. Michael Chevalier, Mariana Gómez-Schiavon, Andrew H. Ng, and Hana El-Samad.
Design and Analysis of a Proportional-Integral-Derivative Controller with Biological
Molecules.
Cell Systems, 9(4):338–353.e10, October 2019.
98. Tau-Mu Yi, Yun Huang, Melvin I. Simon, and John Doyle.
Robust perfect adaptation in bacterial chemotaxis through integral feedback control.
Proceedings of the National Academy of Sciences, 97(9):4649–4653, April 2000.
99. Karl Friston.
Hierarchical Models in the Brain.
PLoS Computational Biology, 4(11):e1000211, November 2008.
100. R. E. Kalman.
A New Approach to Linear Filtering and Prediction Problems.
Journal of Basic Engineering, 82(1):35–45, March 1960.
101. Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny.
Variational free energy and the Laplace approximation.
NeuroImage, 34(1):220–234, January 2007.
102. Karl Friston.
A theory of cortical responses.
Philosophical Transactions of the Royal Society B: Biological Sciences , 360(1456):815–836, April
2005.
28rspa.royalsocietypublishing.org Proc R Soc A 0000000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103. Giovanni Pezzulo.
An Active Inference view of cognitive control.
Frontiers in Psychology, 3, 2012.
104. Jean-Philippe Pellet and André Elisseeff.
Using Markov Blankets for Causal Structure Learning.
Journal of Machine Learning Research, 9(43):1295–1342, 2008.
105. Belinda Tzen and M. Raginsky.
Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion
Limit.
ArXiv, 2019.
106. Roger A. Horn.
Matrix Analysis: Second Edition.
Cambridge University Press, New York, NY, 2nd edition edition, December 2012.
107. L. C. G. Rogers and David Williams.
Diffusions, Markov Processes and Martingales: Volume 2: Itô Calculus , volume 2 of Cambridge
Mathematical Library.
Cambridge University Press, Cambridge, second edition, 2000.
108. Bernt Øksendal.
Stochastic Differential Equations: An Introduction with Applications.
Universitext. Springer-Verlag, Berlin Heidelberg, sixth edition, 2003.
109. Robert Graham.
Covariant formulation of non-equilibrium statistical thermodynamics.
Zeitschrift für Physik B Condensed Matter, 26(4):397–405, December 1977.
110. Gregory L. Eyink, Joel L. Lebowitz, and Herbert Spohn.
Hydrodynamics and ﬂuctuations outside of local equilibrium: Driven diffusive systems.
Journal of Statistical Physics, 83(3):385–472, May 1996.
111. P . Ao.
Potential in stochastic differential equations: Novel construction.
Journal of Physics A: Mathematical and General, 37(3):L25–L30, January 2004.
112. Hong Qian.
A decomposition of irreversible diffusion processes without detailed balance.
Journal of Mathematical Physics, 54(5):053302, May 2013.
113. Yi-An Ma, Tianqi Chen, and Emily B. Fox.
A Complete Recipe for Stochastic Gradient MCMC.
arXiv:1506.04696 [math, stat], October 2015.
114. Alessandro Barp, So Takao, Michael Betancourt, Alexis Arnaudon, and Mark Girolami.
A Unifying and Canonical Description of Measure-Preserving Diffusions.
arXiv:2105.02845 [math, stat], May 2021.
115. Pratik Chaudhari and Stefano Soatto.
Stochastic gradient descent performs variational inference, converges to limit cycles for deep
networks.
In International Conference on Learning Representations, February 2018.
116. Xiaowu Dai and Yuhua Zhu.
On Large Batch Training and Sharp Minima: A Fokker–Planck Perspective.
Journal of Statistical Theory and Practice, 14(3):53, July 2020.
117. Ichiro Aoki.
Entropy production in living systems: From organisms to ecosystems.
Thermochimica Acta, 250(2):359–370, February 1995.
118. U. G. Haussmann and E. Pardoux.
Time Reversal of Diffusions.
Annals of Probability, 14(4):1188–1205, October 1986.
119. Hannes Risken and Till Frank.
The Fokker-Planck Equation: Methods of Solution and Applications.
Springer Series in Synergetics. Springer-Verlag, Berlin Heidelberg, second edition, 1996.
120. Real analysis - Every divergence-free vector ﬁeld generated from skew-symmetric matrix.
https://math.stackexchange.com/questions/578898/every-divergence-free-vector-ﬁeld-
generated-from-skew-symmetric-matrix.